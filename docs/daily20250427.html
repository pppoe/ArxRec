<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250424.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "When Gaussian Meets Surfel: Ultra-fast High-fidelity Radiance Field\n  Rendering", "author": "Keyang Ye and Tianjia Shao and Kun Zhou", "abstract": "  We introduce Gaussian-enhanced Surfels (GESs), a bi-scale representation for\nradiance field rendering, wherein a set of 2D opaque surfels with\nview-dependent colors represent the coarse-scale geometry and appearance of\nscenes, and a few 3D Gaussians surrounding the surfels supplement fine-scale\nappearance details. The rendering with GESs consists of two passes -- surfels\nare first rasterized through a standard graphics pipeline to produce depth and\ncolor maps, and then Gaussians are splatted with depth testing and color\naccumulation on each pixel order independently. The optimization of GESs from\nmulti-view images is performed through an elaborate coarse-to-fine procedure,\nfaithfully capturing rich scene appearance. The entirely sorting-free rendering\nof GESs not only achieves very fast rates, but also produces view-consistent\nimages, successfully avoiding popping artifacts under view changes. The basic\nGES representation can be easily extended to achieve anti-aliasing in rendering\n(Mip-GES), boosted rendering speeds (Speedy-GES) and compact storage\n(Compact-GES), and reconstruct better scene geometries by replacing 3D\nGaussians with 2D Gaussians (2D-GES). Experimental results show that GESs\nadvance the state-of-the-arts as a compelling representation for ultra-fast\nhigh-fidelity radiance field rendering.\n", "link": "http://arxiv.org/abs/2504.17545v1", "date": "2025-04-24", "relevancy": 3.0883, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6409}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.623}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Gaussian%20Meets%20Surfel%3A%20Ultra-fast%20High-fidelity%20Radiance%20Field%0A%20%20Rendering&body=Title%3A%20When%20Gaussian%20Meets%20Surfel%3A%20Ultra-fast%20High-fidelity%20Radiance%20Field%0A%20%20Rendering%0AAuthor%3A%20Keyang%20Ye%20and%20Tianjia%20Shao%20and%20Kun%20Zhou%0AAbstract%3A%20%20%20We%20introduce%20Gaussian-enhanced%20Surfels%20%28GESs%29%2C%20a%20bi-scale%20representation%20for%0Aradiance%20field%20rendering%2C%20wherein%20a%20set%20of%202D%20opaque%20surfels%20with%0Aview-dependent%20colors%20represent%20the%20coarse-scale%20geometry%20and%20appearance%20of%0Ascenes%2C%20and%20a%20few%203D%20Gaussians%20surrounding%20the%20surfels%20supplement%20fine-scale%0Aappearance%20details.%20The%20rendering%20with%20GESs%20consists%20of%20two%20passes%20--%20surfels%0Aare%20first%20rasterized%20through%20a%20standard%20graphics%20pipeline%20to%20produce%20depth%20and%0Acolor%20maps%2C%20and%20then%20Gaussians%20are%20splatted%20with%20depth%20testing%20and%20color%0Aaccumulation%20on%20each%20pixel%20order%20independently.%20The%20optimization%20of%20GESs%20from%0Amulti-view%20images%20is%20performed%20through%20an%20elaborate%20coarse-to-fine%20procedure%2C%0Afaithfully%20capturing%20rich%20scene%20appearance.%20The%20entirely%20sorting-free%20rendering%0Aof%20GESs%20not%20only%20achieves%20very%20fast%20rates%2C%20but%20also%20produces%20view-consistent%0Aimages%2C%20successfully%20avoiding%20popping%20artifacts%20under%20view%20changes.%20The%20basic%0AGES%20representation%20can%20be%20easily%20extended%20to%20achieve%20anti-aliasing%20in%20rendering%0A%28Mip-GES%29%2C%20boosted%20rendering%20speeds%20%28Speedy-GES%29%20and%20compact%20storage%0A%28Compact-GES%29%2C%20and%20reconstruct%20better%20scene%20geometries%20by%20replacing%203D%0AGaussians%20with%202D%20Gaussians%20%282D-GES%29.%20Experimental%20results%20show%20that%20GESs%0Aadvance%20the%20state-of-the-arts%20as%20a%20compelling%20representation%20for%20ultra-fast%0Ahigh-fidelity%20radiance%20field%20rendering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17545v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Gaussian%2520Meets%2520Surfel%253A%2520Ultra-fast%2520High-fidelity%2520Radiance%2520Field%250A%2520%2520Rendering%26entry.906535625%3DKeyang%2520Ye%2520and%2520Tianjia%2520Shao%2520and%2520Kun%2520Zhou%26entry.1292438233%3D%2520%2520We%2520introduce%2520Gaussian-enhanced%2520Surfels%2520%2528GESs%2529%252C%2520a%2520bi-scale%2520representation%2520for%250Aradiance%2520field%2520rendering%252C%2520wherein%2520a%2520set%2520of%25202D%2520opaque%2520surfels%2520with%250Aview-dependent%2520colors%2520represent%2520the%2520coarse-scale%2520geometry%2520and%2520appearance%2520of%250Ascenes%252C%2520and%2520a%2520few%25203D%2520Gaussians%2520surrounding%2520the%2520surfels%2520supplement%2520fine-scale%250Aappearance%2520details.%2520The%2520rendering%2520with%2520GESs%2520consists%2520of%2520two%2520passes%2520--%2520surfels%250Aare%2520first%2520rasterized%2520through%2520a%2520standard%2520graphics%2520pipeline%2520to%2520produce%2520depth%2520and%250Acolor%2520maps%252C%2520and%2520then%2520Gaussians%2520are%2520splatted%2520with%2520depth%2520testing%2520and%2520color%250Aaccumulation%2520on%2520each%2520pixel%2520order%2520independently.%2520The%2520optimization%2520of%2520GESs%2520from%250Amulti-view%2520images%2520is%2520performed%2520through%2520an%2520elaborate%2520coarse-to-fine%2520procedure%252C%250Afaithfully%2520capturing%2520rich%2520scene%2520appearance.%2520The%2520entirely%2520sorting-free%2520rendering%250Aof%2520GESs%2520not%2520only%2520achieves%2520very%2520fast%2520rates%252C%2520but%2520also%2520produces%2520view-consistent%250Aimages%252C%2520successfully%2520avoiding%2520popping%2520artifacts%2520under%2520view%2520changes.%2520The%2520basic%250AGES%2520representation%2520can%2520be%2520easily%2520extended%2520to%2520achieve%2520anti-aliasing%2520in%2520rendering%250A%2528Mip-GES%2529%252C%2520boosted%2520rendering%2520speeds%2520%2528Speedy-GES%2529%2520and%2520compact%2520storage%250A%2528Compact-GES%2529%252C%2520and%2520reconstruct%2520better%2520scene%2520geometries%2520by%2520replacing%25203D%250AGaussians%2520with%25202D%2520Gaussians%2520%25282D-GES%2529.%2520Experimental%2520results%2520show%2520that%2520GESs%250Aadvance%2520the%2520state-of-the-arts%2520as%2520a%2520compelling%2520representation%2520for%2520ultra-fast%250Ahigh-fidelity%2520radiance%2520field%2520rendering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17545v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Gaussian%20Meets%20Surfel%3A%20Ultra-fast%20High-fidelity%20Radiance%20Field%0A%20%20Rendering&entry.906535625=Keyang%20Ye%20and%20Tianjia%20Shao%20and%20Kun%20Zhou&entry.1292438233=%20%20We%20introduce%20Gaussian-enhanced%20Surfels%20%28GESs%29%2C%20a%20bi-scale%20representation%20for%0Aradiance%20field%20rendering%2C%20wherein%20a%20set%20of%202D%20opaque%20surfels%20with%0Aview-dependent%20colors%20represent%20the%20coarse-scale%20geometry%20and%20appearance%20of%0Ascenes%2C%20and%20a%20few%203D%20Gaussians%20surrounding%20the%20surfels%20supplement%20fine-scale%0Aappearance%20details.%20The%20rendering%20with%20GESs%20consists%20of%20two%20passes%20--%20surfels%0Aare%20first%20rasterized%20through%20a%20standard%20graphics%20pipeline%20to%20produce%20depth%20and%0Acolor%20maps%2C%20and%20then%20Gaussians%20are%20splatted%20with%20depth%20testing%20and%20color%0Aaccumulation%20on%20each%20pixel%20order%20independently.%20The%20optimization%20of%20GESs%20from%0Amulti-view%20images%20is%20performed%20through%20an%20elaborate%20coarse-to-fine%20procedure%2C%0Afaithfully%20capturing%20rich%20scene%20appearance.%20The%20entirely%20sorting-free%20rendering%0Aof%20GESs%20not%20only%20achieves%20very%20fast%20rates%2C%20but%20also%20produces%20view-consistent%0Aimages%2C%20successfully%20avoiding%20popping%20artifacts%20under%20view%20changes.%20The%20basic%0AGES%20representation%20can%20be%20easily%20extended%20to%20achieve%20anti-aliasing%20in%20rendering%0A%28Mip-GES%29%2C%20boosted%20rendering%20speeds%20%28Speedy-GES%29%20and%20compact%20storage%0A%28Compact-GES%29%2C%20and%20reconstruct%20better%20scene%20geometries%20by%20replacing%203D%0AGaussians%20with%202D%20Gaussians%20%282D-GES%29.%20Experimental%20results%20show%20that%20GESs%0Aadvance%20the%20state-of-the-arts%20as%20a%20compelling%20representation%20for%20ultra-fast%0Ahigh-fidelity%20radiance%20field%20rendering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17545v1&entry.124074799=Read"},
{"title": "Breaking the Modality Barrier: Universal Embedding Learning with\n  Multimodal LLMs", "author": "Tiancheng Gu and Kaicheng Yang and Ziyong Feng and Xingjun Wang and Yanzhao Zhang and Dingkun Long and Yingda Chen and Weidong Cai and Jiankang Deng", "abstract": "  The Contrastive Language-Image Pre-training (CLIP) framework has become a\nwidely used approach for multimodal representation learning, particularly in\nimage-text retrieval and clustering. However, its efficacy is constrained by\nthree key limitations: (1) text token truncation, (2) isolated image-text\nencoding, and (3) deficient compositionality due to bag-of-words behavior.\nWhile recent Multimodal Large Language Models (MLLMs) have demonstrated\nsignificant advances in generalized vision-language understanding, their\npotential for learning transferable multimodal representations remains\nunderexplored.In this work, we present UniME (Universal Multimodal Embedding),\na novel two-stage framework that leverages MLLMs to learn discriminative\nrepresentations for diverse downstream tasks. In the first stage, we perform\ntextual discriminative knowledge distillation from a powerful LLM-based teacher\nmodel to enhance the embedding capability of the MLLM\\'s language component. In\nthe second stage, we introduce hard negative enhanced instruction tuning to\nfurther advance discriminative representation learning. Specifically, we\ninitially mitigate false negative contamination and then sample multiple hard\nnegatives per instance within each batch, forcing the model to focus on\nchallenging samples. This approach not only improves discriminative power but\nalso enhances instruction-following ability in downstream tasks. We conduct\nextensive experiments on the MMEB benchmark and multiple retrieval tasks,\nincluding short and long caption retrieval and compositional retrieval. Results\ndemonstrate that UniME achieves consistent performance improvement across all\ntasks, exhibiting superior discriminative and compositional capabilities.\n", "link": "http://arxiv.org/abs/2504.17432v1", "date": "2025-04-24", "relevancy": 2.9992, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6623}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5686}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breaking%20the%20Modality%20Barrier%3A%20Universal%20Embedding%20Learning%20with%0A%20%20Multimodal%20LLMs&body=Title%3A%20Breaking%20the%20Modality%20Barrier%3A%20Universal%20Embedding%20Learning%20with%0A%20%20Multimodal%20LLMs%0AAuthor%3A%20Tiancheng%20Gu%20and%20Kaicheng%20Yang%20and%20Ziyong%20Feng%20and%20Xingjun%20Wang%20and%20Yanzhao%20Zhang%20and%20Dingkun%20Long%20and%20Yingda%20Chen%20and%20Weidong%20Cai%20and%20Jiankang%20Deng%0AAbstract%3A%20%20%20The%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20framework%20has%20become%20a%0Awidely%20used%20approach%20for%20multimodal%20representation%20learning%2C%20particularly%20in%0Aimage-text%20retrieval%20and%20clustering.%20However%2C%20its%20efficacy%20is%20constrained%20by%0Athree%20key%20limitations%3A%20%281%29%20text%20token%20truncation%2C%20%282%29%20isolated%20image-text%0Aencoding%2C%20and%20%283%29%20deficient%20compositionality%20due%20to%20bag-of-words%20behavior.%0AWhile%20recent%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%0Asignificant%20advances%20in%20generalized%20vision-language%20understanding%2C%20their%0Apotential%20for%20learning%20transferable%20multimodal%20representations%20remains%0Aunderexplored.In%20this%20work%2C%20we%20present%20UniME%20%28Universal%20Multimodal%20Embedding%29%2C%0Aa%20novel%20two-stage%20framework%20that%20leverages%20MLLMs%20to%20learn%20discriminative%0Arepresentations%20for%20diverse%20downstream%20tasks.%20In%20the%20first%20stage%2C%20we%20perform%0Atextual%20discriminative%20knowledge%20distillation%20from%20a%20powerful%20LLM-based%20teacher%0Amodel%20to%20enhance%20the%20embedding%20capability%20of%20the%20MLLM%5C%27s%20language%20component.%20In%0Athe%20second%20stage%2C%20we%20introduce%20hard%20negative%20enhanced%20instruction%20tuning%20to%0Afurther%20advance%20discriminative%20representation%20learning.%20Specifically%2C%20we%0Ainitially%20mitigate%20false%20negative%20contamination%20and%20then%20sample%20multiple%20hard%0Anegatives%20per%20instance%20within%20each%20batch%2C%20forcing%20the%20model%20to%20focus%20on%0Achallenging%20samples.%20This%20approach%20not%20only%20improves%20discriminative%20power%20but%0Aalso%20enhances%20instruction-following%20ability%20in%20downstream%20tasks.%20We%20conduct%0Aextensive%20experiments%20on%20the%20MMEB%20benchmark%20and%20multiple%20retrieval%20tasks%2C%0Aincluding%20short%20and%20long%20caption%20retrieval%20and%20compositional%20retrieval.%20Results%0Ademonstrate%20that%20UniME%20achieves%20consistent%20performance%20improvement%20across%20all%0Atasks%2C%20exhibiting%20superior%20discriminative%20and%20compositional%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17432v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreaking%2520the%2520Modality%2520Barrier%253A%2520Universal%2520Embedding%2520Learning%2520with%250A%2520%2520Multimodal%2520LLMs%26entry.906535625%3DTiancheng%2520Gu%2520and%2520Kaicheng%2520Yang%2520and%2520Ziyong%2520Feng%2520and%2520Xingjun%2520Wang%2520and%2520Yanzhao%2520Zhang%2520and%2520Dingkun%2520Long%2520and%2520Yingda%2520Chen%2520and%2520Weidong%2520Cai%2520and%2520Jiankang%2520Deng%26entry.1292438233%3D%2520%2520The%2520Contrastive%2520Language-Image%2520Pre-training%2520%2528CLIP%2529%2520framework%2520has%2520become%2520a%250Awidely%2520used%2520approach%2520for%2520multimodal%2520representation%2520learning%252C%2520particularly%2520in%250Aimage-text%2520retrieval%2520and%2520clustering.%2520However%252C%2520its%2520efficacy%2520is%2520constrained%2520by%250Athree%2520key%2520limitations%253A%2520%25281%2529%2520text%2520token%2520truncation%252C%2520%25282%2529%2520isolated%2520image-text%250Aencoding%252C%2520and%2520%25283%2529%2520deficient%2520compositionality%2520due%2520to%2520bag-of-words%2520behavior.%250AWhile%2520recent%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520demonstrated%250Asignificant%2520advances%2520in%2520generalized%2520vision-language%2520understanding%252C%2520their%250Apotential%2520for%2520learning%2520transferable%2520multimodal%2520representations%2520remains%250Aunderexplored.In%2520this%2520work%252C%2520we%2520present%2520UniME%2520%2528Universal%2520Multimodal%2520Embedding%2529%252C%250Aa%2520novel%2520two-stage%2520framework%2520that%2520leverages%2520MLLMs%2520to%2520learn%2520discriminative%250Arepresentations%2520for%2520diverse%2520downstream%2520tasks.%2520In%2520the%2520first%2520stage%252C%2520we%2520perform%250Atextual%2520discriminative%2520knowledge%2520distillation%2520from%2520a%2520powerful%2520LLM-based%2520teacher%250Amodel%2520to%2520enhance%2520the%2520embedding%2520capability%2520of%2520the%2520MLLM%255C%2527s%2520language%2520component.%2520In%250Athe%2520second%2520stage%252C%2520we%2520introduce%2520hard%2520negative%2520enhanced%2520instruction%2520tuning%2520to%250Afurther%2520advance%2520discriminative%2520representation%2520learning.%2520Specifically%252C%2520we%250Ainitially%2520mitigate%2520false%2520negative%2520contamination%2520and%2520then%2520sample%2520multiple%2520hard%250Anegatives%2520per%2520instance%2520within%2520each%2520batch%252C%2520forcing%2520the%2520model%2520to%2520focus%2520on%250Achallenging%2520samples.%2520This%2520approach%2520not%2520only%2520improves%2520discriminative%2520power%2520but%250Aalso%2520enhances%2520instruction-following%2520ability%2520in%2520downstream%2520tasks.%2520We%2520conduct%250Aextensive%2520experiments%2520on%2520the%2520MMEB%2520benchmark%2520and%2520multiple%2520retrieval%2520tasks%252C%250Aincluding%2520short%2520and%2520long%2520caption%2520retrieval%2520and%2520compositional%2520retrieval.%2520Results%250Ademonstrate%2520that%2520UniME%2520achieves%2520consistent%2520performance%2520improvement%2520across%2520all%250Atasks%252C%2520exhibiting%2520superior%2520discriminative%2520and%2520compositional%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17432v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breaking%20the%20Modality%20Barrier%3A%20Universal%20Embedding%20Learning%20with%0A%20%20Multimodal%20LLMs&entry.906535625=Tiancheng%20Gu%20and%20Kaicheng%20Yang%20and%20Ziyong%20Feng%20and%20Xingjun%20Wang%20and%20Yanzhao%20Zhang%20and%20Dingkun%20Long%20and%20Yingda%20Chen%20and%20Weidong%20Cai%20and%20Jiankang%20Deng&entry.1292438233=%20%20The%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20framework%20has%20become%20a%0Awidely%20used%20approach%20for%20multimodal%20representation%20learning%2C%20particularly%20in%0Aimage-text%20retrieval%20and%20clustering.%20However%2C%20its%20efficacy%20is%20constrained%20by%0Athree%20key%20limitations%3A%20%281%29%20text%20token%20truncation%2C%20%282%29%20isolated%20image-text%0Aencoding%2C%20and%20%283%29%20deficient%20compositionality%20due%20to%20bag-of-words%20behavior.%0AWhile%20recent%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%0Asignificant%20advances%20in%20generalized%20vision-language%20understanding%2C%20their%0Apotential%20for%20learning%20transferable%20multimodal%20representations%20remains%0Aunderexplored.In%20this%20work%2C%20we%20present%20UniME%20%28Universal%20Multimodal%20Embedding%29%2C%0Aa%20novel%20two-stage%20framework%20that%20leverages%20MLLMs%20to%20learn%20discriminative%0Arepresentations%20for%20diverse%20downstream%20tasks.%20In%20the%20first%20stage%2C%20we%20perform%0Atextual%20discriminative%20knowledge%20distillation%20from%20a%20powerful%20LLM-based%20teacher%0Amodel%20to%20enhance%20the%20embedding%20capability%20of%20the%20MLLM%5C%27s%20language%20component.%20In%0Athe%20second%20stage%2C%20we%20introduce%20hard%20negative%20enhanced%20instruction%20tuning%20to%0Afurther%20advance%20discriminative%20representation%20learning.%20Specifically%2C%20we%0Ainitially%20mitigate%20false%20negative%20contamination%20and%20then%20sample%20multiple%20hard%0Anegatives%20per%20instance%20within%20each%20batch%2C%20forcing%20the%20model%20to%20focus%20on%0Achallenging%20samples.%20This%20approach%20not%20only%20improves%20discriminative%20power%20but%0Aalso%20enhances%20instruction-following%20ability%20in%20downstream%20tasks.%20We%20conduct%0Aextensive%20experiments%20on%20the%20MMEB%20benchmark%20and%20multiple%20retrieval%20tasks%2C%0Aincluding%20short%20and%20long%20caption%20retrieval%20and%20compositional%20retrieval.%20Results%0Ademonstrate%20that%20UniME%20achieves%20consistent%20performance%20improvement%20across%20all%0Atasks%2C%20exhibiting%20superior%20discriminative%20and%20compositional%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17432v1&entry.124074799=Read"},
{"title": "PICO: Reconstructing 3D People In Contact with Objects", "author": "Alp\u00e1r Cseke and Shashank Tripathi and Sai Kumar Dwivedi and Arjun Lakshmipathy and Agniv Chatterjee and Michael J. Black and Dimitrios Tzionas", "abstract": "  Recovering 3D Human-Object Interaction (HOI) from single color images is\nchallenging due to depth ambiguities, occlusions, and the huge variation in\nobject shape and appearance. Thus, past work requires controlled settings such\nas known object shapes and contacts, and tackles only limited object classes.\nInstead, we need methods that generalize to natural images and novel object\nclasses. We tackle this in two main ways: (1) We collect PICO-db, a new dataset\nof natural images uniquely paired with dense 3D contact on both body and object\nmeshes. To this end, we use images from the recent DAMON dataset that are\npaired with contacts, but these contacts are only annotated on a canonical 3D\nbody. In contrast, we seek contact labels on both the body and the object. To\ninfer these given an image, we retrieve an appropriate 3D object mesh from a\ndatabase by leveraging vision foundation models. Then, we project DAMON's body\ncontact patches onto the object via a novel method needing only 2 clicks per\npatch. This minimal human input establishes rich contact correspondences\nbetween bodies and objects. (2) We exploit our new dataset of contact\ncorrespondences in a novel render-and-compare fitting method, called PICO-fit,\nto recover 3D body and object meshes in interaction. PICO-fit infers contact\nfor the SMPL-X body, retrieves a likely 3D object mesh and contact from PICO-db\nfor that object, and uses the contact to iteratively fit the 3D body and object\nmeshes to image evidence via optimization. Uniquely, PICO-fit works well for\nmany object categories that no existing method can tackle. This is crucial to\nenable HOI understanding to scale in the wild. Our data and code are available\nat https://pico.is.tue.mpg.de.\n", "link": "http://arxiv.org/abs/2504.17695v1", "date": "2025-04-24", "relevancy": 2.9446, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6068}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5862}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PICO%3A%20Reconstructing%203D%20People%20In%20Contact%20with%20Objects&body=Title%3A%20PICO%3A%20Reconstructing%203D%20People%20In%20Contact%20with%20Objects%0AAuthor%3A%20Alp%C3%A1r%20Cseke%20and%20Shashank%20Tripathi%20and%20Sai%20Kumar%20Dwivedi%20and%20Arjun%20Lakshmipathy%20and%20Agniv%20Chatterjee%20and%20Michael%20J.%20Black%20and%20Dimitrios%20Tzionas%0AAbstract%3A%20%20%20Recovering%203D%20Human-Object%20Interaction%20%28HOI%29%20from%20single%20color%20images%20is%0Achallenging%20due%20to%20depth%20ambiguities%2C%20occlusions%2C%20and%20the%20huge%20variation%20in%0Aobject%20shape%20and%20appearance.%20Thus%2C%20past%20work%20requires%20controlled%20settings%20such%0Aas%20known%20object%20shapes%20and%20contacts%2C%20and%20tackles%20only%20limited%20object%20classes.%0AInstead%2C%20we%20need%20methods%20that%20generalize%20to%20natural%20images%20and%20novel%20object%0Aclasses.%20We%20tackle%20this%20in%20two%20main%20ways%3A%20%281%29%20We%20collect%20PICO-db%2C%20a%20new%20dataset%0Aof%20natural%20images%20uniquely%20paired%20with%20dense%203D%20contact%20on%20both%20body%20and%20object%0Ameshes.%20To%20this%20end%2C%20we%20use%20images%20from%20the%20recent%20DAMON%20dataset%20that%20are%0Apaired%20with%20contacts%2C%20but%20these%20contacts%20are%20only%20annotated%20on%20a%20canonical%203D%0Abody.%20In%20contrast%2C%20we%20seek%20contact%20labels%20on%20both%20the%20body%20and%20the%20object.%20To%0Ainfer%20these%20given%20an%20image%2C%20we%20retrieve%20an%20appropriate%203D%20object%20mesh%20from%20a%0Adatabase%20by%20leveraging%20vision%20foundation%20models.%20Then%2C%20we%20project%20DAMON%27s%20body%0Acontact%20patches%20onto%20the%20object%20via%20a%20novel%20method%20needing%20only%202%20clicks%20per%0Apatch.%20This%20minimal%20human%20input%20establishes%20rich%20contact%20correspondences%0Abetween%20bodies%20and%20objects.%20%282%29%20We%20exploit%20our%20new%20dataset%20of%20contact%0Acorrespondences%20in%20a%20novel%20render-and-compare%20fitting%20method%2C%20called%20PICO-fit%2C%0Ato%20recover%203D%20body%20and%20object%20meshes%20in%20interaction.%20PICO-fit%20infers%20contact%0Afor%20the%20SMPL-X%20body%2C%20retrieves%20a%20likely%203D%20object%20mesh%20and%20contact%20from%20PICO-db%0Afor%20that%20object%2C%20and%20uses%20the%20contact%20to%20iteratively%20fit%20the%203D%20body%20and%20object%0Ameshes%20to%20image%20evidence%20via%20optimization.%20Uniquely%2C%20PICO-fit%20works%20well%20for%0Amany%20object%20categories%20that%20no%20existing%20method%20can%20tackle.%20This%20is%20crucial%20to%0Aenable%20HOI%20understanding%20to%20scale%20in%20the%20wild.%20Our%20data%20and%20code%20are%20available%0Aat%20https%3A//pico.is.tue.mpg.de.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17695v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPICO%253A%2520Reconstructing%25203D%2520People%2520In%2520Contact%2520with%2520Objects%26entry.906535625%3DAlp%25C3%25A1r%2520Cseke%2520and%2520Shashank%2520Tripathi%2520and%2520Sai%2520Kumar%2520Dwivedi%2520and%2520Arjun%2520Lakshmipathy%2520and%2520Agniv%2520Chatterjee%2520and%2520Michael%2520J.%2520Black%2520and%2520Dimitrios%2520Tzionas%26entry.1292438233%3D%2520%2520Recovering%25203D%2520Human-Object%2520Interaction%2520%2528HOI%2529%2520from%2520single%2520color%2520images%2520is%250Achallenging%2520due%2520to%2520depth%2520ambiguities%252C%2520occlusions%252C%2520and%2520the%2520huge%2520variation%2520in%250Aobject%2520shape%2520and%2520appearance.%2520Thus%252C%2520past%2520work%2520requires%2520controlled%2520settings%2520such%250Aas%2520known%2520object%2520shapes%2520and%2520contacts%252C%2520and%2520tackles%2520only%2520limited%2520object%2520classes.%250AInstead%252C%2520we%2520need%2520methods%2520that%2520generalize%2520to%2520natural%2520images%2520and%2520novel%2520object%250Aclasses.%2520We%2520tackle%2520this%2520in%2520two%2520main%2520ways%253A%2520%25281%2529%2520We%2520collect%2520PICO-db%252C%2520a%2520new%2520dataset%250Aof%2520natural%2520images%2520uniquely%2520paired%2520with%2520dense%25203D%2520contact%2520on%2520both%2520body%2520and%2520object%250Ameshes.%2520To%2520this%2520end%252C%2520we%2520use%2520images%2520from%2520the%2520recent%2520DAMON%2520dataset%2520that%2520are%250Apaired%2520with%2520contacts%252C%2520but%2520these%2520contacts%2520are%2520only%2520annotated%2520on%2520a%2520canonical%25203D%250Abody.%2520In%2520contrast%252C%2520we%2520seek%2520contact%2520labels%2520on%2520both%2520the%2520body%2520and%2520the%2520object.%2520To%250Ainfer%2520these%2520given%2520an%2520image%252C%2520we%2520retrieve%2520an%2520appropriate%25203D%2520object%2520mesh%2520from%2520a%250Adatabase%2520by%2520leveraging%2520vision%2520foundation%2520models.%2520Then%252C%2520we%2520project%2520DAMON%2527s%2520body%250Acontact%2520patches%2520onto%2520the%2520object%2520via%2520a%2520novel%2520method%2520needing%2520only%25202%2520clicks%2520per%250Apatch.%2520This%2520minimal%2520human%2520input%2520establishes%2520rich%2520contact%2520correspondences%250Abetween%2520bodies%2520and%2520objects.%2520%25282%2529%2520We%2520exploit%2520our%2520new%2520dataset%2520of%2520contact%250Acorrespondences%2520in%2520a%2520novel%2520render-and-compare%2520fitting%2520method%252C%2520called%2520PICO-fit%252C%250Ato%2520recover%25203D%2520body%2520and%2520object%2520meshes%2520in%2520interaction.%2520PICO-fit%2520infers%2520contact%250Afor%2520the%2520SMPL-X%2520body%252C%2520retrieves%2520a%2520likely%25203D%2520object%2520mesh%2520and%2520contact%2520from%2520PICO-db%250Afor%2520that%2520object%252C%2520and%2520uses%2520the%2520contact%2520to%2520iteratively%2520fit%2520the%25203D%2520body%2520and%2520object%250Ameshes%2520to%2520image%2520evidence%2520via%2520optimization.%2520Uniquely%252C%2520PICO-fit%2520works%2520well%2520for%250Amany%2520object%2520categories%2520that%2520no%2520existing%2520method%2520can%2520tackle.%2520This%2520is%2520crucial%2520to%250Aenable%2520HOI%2520understanding%2520to%2520scale%2520in%2520the%2520wild.%2520Our%2520data%2520and%2520code%2520are%2520available%250Aat%2520https%253A//pico.is.tue.mpg.de.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17695v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PICO%3A%20Reconstructing%203D%20People%20In%20Contact%20with%20Objects&entry.906535625=Alp%C3%A1r%20Cseke%20and%20Shashank%20Tripathi%20and%20Sai%20Kumar%20Dwivedi%20and%20Arjun%20Lakshmipathy%20and%20Agniv%20Chatterjee%20and%20Michael%20J.%20Black%20and%20Dimitrios%20Tzionas&entry.1292438233=%20%20Recovering%203D%20Human-Object%20Interaction%20%28HOI%29%20from%20single%20color%20images%20is%0Achallenging%20due%20to%20depth%20ambiguities%2C%20occlusions%2C%20and%20the%20huge%20variation%20in%0Aobject%20shape%20and%20appearance.%20Thus%2C%20past%20work%20requires%20controlled%20settings%20such%0Aas%20known%20object%20shapes%20and%20contacts%2C%20and%20tackles%20only%20limited%20object%20classes.%0AInstead%2C%20we%20need%20methods%20that%20generalize%20to%20natural%20images%20and%20novel%20object%0Aclasses.%20We%20tackle%20this%20in%20two%20main%20ways%3A%20%281%29%20We%20collect%20PICO-db%2C%20a%20new%20dataset%0Aof%20natural%20images%20uniquely%20paired%20with%20dense%203D%20contact%20on%20both%20body%20and%20object%0Ameshes.%20To%20this%20end%2C%20we%20use%20images%20from%20the%20recent%20DAMON%20dataset%20that%20are%0Apaired%20with%20contacts%2C%20but%20these%20contacts%20are%20only%20annotated%20on%20a%20canonical%203D%0Abody.%20In%20contrast%2C%20we%20seek%20contact%20labels%20on%20both%20the%20body%20and%20the%20object.%20To%0Ainfer%20these%20given%20an%20image%2C%20we%20retrieve%20an%20appropriate%203D%20object%20mesh%20from%20a%0Adatabase%20by%20leveraging%20vision%20foundation%20models.%20Then%2C%20we%20project%20DAMON%27s%20body%0Acontact%20patches%20onto%20the%20object%20via%20a%20novel%20method%20needing%20only%202%20clicks%20per%0Apatch.%20This%20minimal%20human%20input%20establishes%20rich%20contact%20correspondences%0Abetween%20bodies%20and%20objects.%20%282%29%20We%20exploit%20our%20new%20dataset%20of%20contact%0Acorrespondences%20in%20a%20novel%20render-and-compare%20fitting%20method%2C%20called%20PICO-fit%2C%0Ato%20recover%203D%20body%20and%20object%20meshes%20in%20interaction.%20PICO-fit%20infers%20contact%0Afor%20the%20SMPL-X%20body%2C%20retrieves%20a%20likely%203D%20object%20mesh%20and%20contact%20from%20PICO-db%0Afor%20that%20object%2C%20and%20uses%20the%20contact%20to%20iteratively%20fit%20the%203D%20body%20and%20object%0Ameshes%20to%20image%20evidence%20via%20optimization.%20Uniquely%2C%20PICO-fit%20works%20well%20for%0Amany%20object%20categories%20that%20no%20existing%20method%20can%20tackle.%20This%20is%20crucial%20to%0Aenable%20HOI%20understanding%20to%20scale%20in%20the%20wild.%20Our%20data%20and%20code%20are%20available%0Aat%20https%3A//pico.is.tue.mpg.de.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17695v1&entry.124074799=Read"},
{"title": "A Guide to Structureless Visual Localization", "author": "Vojtech Panek and Qunjie Zhou and Yaqing Ding and S\u00e9rgio Agostinho and Zuzana Kukelova and Torsten Sattler and Laura Leal-Taix\u00e9", "abstract": "  Visual localization algorithms, i.e., methods that estimate the camera pose\nof a query image in a known scene, are core components of many applications,\nincluding self-driving cars and augmented / mixed reality systems.\nState-of-the-art visual localization algorithms are structure-based, i.e., they\nstore a 3D model of the scene and use 2D-3D correspondences between the query\nimage and 3D points in the model for camera pose estimation. While such\napproaches are highly accurate, they are also rather inflexible when it comes\nto adjusting the underlying 3D model after changes in the scene. Structureless\nlocalization approaches represent the scene as a database of images with known\nposes and thus offer a much more flexible representation that can be easily\nupdated by adding or removing images. Although there is a large amount of\nliterature on structure-based approaches, there is significantly less work on\nstructureless methods. Hence, this paper is dedicated to providing the, to the\nbest of our knowledge, first comprehensive discussion and comparison of\nstructureless methods. Extensive experiments show that approaches that use a\nhigher degree of classical geometric reasoning generally achieve higher pose\naccuracy. In particular, approaches based on classical absolute or\nsemi-generalized relative pose estimation outperform very recent methods based\non pose regression by a wide margin. Compared with state-of-the-art\nstructure-based approaches, the flexibility of structureless methods comes at\nthe cost of (slightly) lower pose accuracy, indicating an interesting direction\nfor future work.\n", "link": "http://arxiv.org/abs/2504.17636v1", "date": "2025-04-24", "relevancy": 2.9238, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6068}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5737}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Guide%20to%20Structureless%20Visual%20Localization&body=Title%3A%20A%20Guide%20to%20Structureless%20Visual%20Localization%0AAuthor%3A%20Vojtech%20Panek%20and%20Qunjie%20Zhou%20and%20Yaqing%20Ding%20and%20S%C3%A9rgio%20Agostinho%20and%20Zuzana%20Kukelova%20and%20Torsten%20Sattler%20and%20Laura%20Leal-Taix%C3%A9%0AAbstract%3A%20%20%20Visual%20localization%20algorithms%2C%20i.e.%2C%20methods%20that%20estimate%20the%20camera%20pose%0Aof%20a%20query%20image%20in%20a%20known%20scene%2C%20are%20core%20components%20of%20many%20applications%2C%0Aincluding%20self-driving%20cars%20and%20augmented%20/%20mixed%20reality%20systems.%0AState-of-the-art%20visual%20localization%20algorithms%20are%20structure-based%2C%20i.e.%2C%20they%0Astore%20a%203D%20model%20of%20the%20scene%20and%20use%202D-3D%20correspondences%20between%20the%20query%0Aimage%20and%203D%20points%20in%20the%20model%20for%20camera%20pose%20estimation.%20While%20such%0Aapproaches%20are%20highly%20accurate%2C%20they%20are%20also%20rather%20inflexible%20when%20it%20comes%0Ato%20adjusting%20the%20underlying%203D%20model%20after%20changes%20in%20the%20scene.%20Structureless%0Alocalization%20approaches%20represent%20the%20scene%20as%20a%20database%20of%20images%20with%20known%0Aposes%20and%20thus%20offer%20a%20much%20more%20flexible%20representation%20that%20can%20be%20easily%0Aupdated%20by%20adding%20or%20removing%20images.%20Although%20there%20is%20a%20large%20amount%20of%0Aliterature%20on%20structure-based%20approaches%2C%20there%20is%20significantly%20less%20work%20on%0Astructureless%20methods.%20Hence%2C%20this%20paper%20is%20dedicated%20to%20providing%20the%2C%20to%20the%0Abest%20of%20our%20knowledge%2C%20first%20comprehensive%20discussion%20and%20comparison%20of%0Astructureless%20methods.%20Extensive%20experiments%20show%20that%20approaches%20that%20use%20a%0Ahigher%20degree%20of%20classical%20geometric%20reasoning%20generally%20achieve%20higher%20pose%0Aaccuracy.%20In%20particular%2C%20approaches%20based%20on%20classical%20absolute%20or%0Asemi-generalized%20relative%20pose%20estimation%20outperform%20very%20recent%20methods%20based%0Aon%20pose%20regression%20by%20a%20wide%20margin.%20Compared%20with%20state-of-the-art%0Astructure-based%20approaches%2C%20the%20flexibility%20of%20structureless%20methods%20comes%20at%0Athe%20cost%20of%20%28slightly%29%20lower%20pose%20accuracy%2C%20indicating%20an%20interesting%20direction%0Afor%20future%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17636v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Guide%2520to%2520Structureless%2520Visual%2520Localization%26entry.906535625%3DVojtech%2520Panek%2520and%2520Qunjie%2520Zhou%2520and%2520Yaqing%2520Ding%2520and%2520S%25C3%25A9rgio%2520Agostinho%2520and%2520Zuzana%2520Kukelova%2520and%2520Torsten%2520Sattler%2520and%2520Laura%2520Leal-Taix%25C3%25A9%26entry.1292438233%3D%2520%2520Visual%2520localization%2520algorithms%252C%2520i.e.%252C%2520methods%2520that%2520estimate%2520the%2520camera%2520pose%250Aof%2520a%2520query%2520image%2520in%2520a%2520known%2520scene%252C%2520are%2520core%2520components%2520of%2520many%2520applications%252C%250Aincluding%2520self-driving%2520cars%2520and%2520augmented%2520/%2520mixed%2520reality%2520systems.%250AState-of-the-art%2520visual%2520localization%2520algorithms%2520are%2520structure-based%252C%2520i.e.%252C%2520they%250Astore%2520a%25203D%2520model%2520of%2520the%2520scene%2520and%2520use%25202D-3D%2520correspondences%2520between%2520the%2520query%250Aimage%2520and%25203D%2520points%2520in%2520the%2520model%2520for%2520camera%2520pose%2520estimation.%2520While%2520such%250Aapproaches%2520are%2520highly%2520accurate%252C%2520they%2520are%2520also%2520rather%2520inflexible%2520when%2520it%2520comes%250Ato%2520adjusting%2520the%2520underlying%25203D%2520model%2520after%2520changes%2520in%2520the%2520scene.%2520Structureless%250Alocalization%2520approaches%2520represent%2520the%2520scene%2520as%2520a%2520database%2520of%2520images%2520with%2520known%250Aposes%2520and%2520thus%2520offer%2520a%2520much%2520more%2520flexible%2520representation%2520that%2520can%2520be%2520easily%250Aupdated%2520by%2520adding%2520or%2520removing%2520images.%2520Although%2520there%2520is%2520a%2520large%2520amount%2520of%250Aliterature%2520on%2520structure-based%2520approaches%252C%2520there%2520is%2520significantly%2520less%2520work%2520on%250Astructureless%2520methods.%2520Hence%252C%2520this%2520paper%2520is%2520dedicated%2520to%2520providing%2520the%252C%2520to%2520the%250Abest%2520of%2520our%2520knowledge%252C%2520first%2520comprehensive%2520discussion%2520and%2520comparison%2520of%250Astructureless%2520methods.%2520Extensive%2520experiments%2520show%2520that%2520approaches%2520that%2520use%2520a%250Ahigher%2520degree%2520of%2520classical%2520geometric%2520reasoning%2520generally%2520achieve%2520higher%2520pose%250Aaccuracy.%2520In%2520particular%252C%2520approaches%2520based%2520on%2520classical%2520absolute%2520or%250Asemi-generalized%2520relative%2520pose%2520estimation%2520outperform%2520very%2520recent%2520methods%2520based%250Aon%2520pose%2520regression%2520by%2520a%2520wide%2520margin.%2520Compared%2520with%2520state-of-the-art%250Astructure-based%2520approaches%252C%2520the%2520flexibility%2520of%2520structureless%2520methods%2520comes%2520at%250Athe%2520cost%2520of%2520%2528slightly%2529%2520lower%2520pose%2520accuracy%252C%2520indicating%2520an%2520interesting%2520direction%250Afor%2520future%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17636v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Guide%20to%20Structureless%20Visual%20Localization&entry.906535625=Vojtech%20Panek%20and%20Qunjie%20Zhou%20and%20Yaqing%20Ding%20and%20S%C3%A9rgio%20Agostinho%20and%20Zuzana%20Kukelova%20and%20Torsten%20Sattler%20and%20Laura%20Leal-Taix%C3%A9&entry.1292438233=%20%20Visual%20localization%20algorithms%2C%20i.e.%2C%20methods%20that%20estimate%20the%20camera%20pose%0Aof%20a%20query%20image%20in%20a%20known%20scene%2C%20are%20core%20components%20of%20many%20applications%2C%0Aincluding%20self-driving%20cars%20and%20augmented%20/%20mixed%20reality%20systems.%0AState-of-the-art%20visual%20localization%20algorithms%20are%20structure-based%2C%20i.e.%2C%20they%0Astore%20a%203D%20model%20of%20the%20scene%20and%20use%202D-3D%20correspondences%20between%20the%20query%0Aimage%20and%203D%20points%20in%20the%20model%20for%20camera%20pose%20estimation.%20While%20such%0Aapproaches%20are%20highly%20accurate%2C%20they%20are%20also%20rather%20inflexible%20when%20it%20comes%0Ato%20adjusting%20the%20underlying%203D%20model%20after%20changes%20in%20the%20scene.%20Structureless%0Alocalization%20approaches%20represent%20the%20scene%20as%20a%20database%20of%20images%20with%20known%0Aposes%20and%20thus%20offer%20a%20much%20more%20flexible%20representation%20that%20can%20be%20easily%0Aupdated%20by%20adding%20or%20removing%20images.%20Although%20there%20is%20a%20large%20amount%20of%0Aliterature%20on%20structure-based%20approaches%2C%20there%20is%20significantly%20less%20work%20on%0Astructureless%20methods.%20Hence%2C%20this%20paper%20is%20dedicated%20to%20providing%20the%2C%20to%20the%0Abest%20of%20our%20knowledge%2C%20first%20comprehensive%20discussion%20and%20comparison%20of%0Astructureless%20methods.%20Extensive%20experiments%20show%20that%20approaches%20that%20use%20a%0Ahigher%20degree%20of%20classical%20geometric%20reasoning%20generally%20achieve%20higher%20pose%0Aaccuracy.%20In%20particular%2C%20approaches%20based%20on%20classical%20absolute%20or%0Asemi-generalized%20relative%20pose%20estimation%20outperform%20very%20recent%20methods%20based%0Aon%20pose%20regression%20by%20a%20wide%20margin.%20Compared%20with%20state-of-the-art%0Astructure-based%20approaches%2C%20the%20flexibility%20of%20structureless%20methods%20comes%20at%0Athe%20cost%20of%20%28slightly%29%20lower%20pose%20accuracy%2C%20indicating%20an%20interesting%20direction%0Afor%20future%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17636v1&entry.124074799=Read"},
{"title": "Variational Self-Supervised Learning", "author": "Mehmet Can Yavuz and Berrin Yanikoglu", "abstract": "  We present Variational Self-Supervised Learning (VSSL), a novel framework\nthat combines variational inference with self-supervised learning to enable\nefficient, decoder-free representation learning. Unlike traditional VAEs that\nrely on input reconstruction via a decoder, VSSL symmetrically couples two\nencoders with Gaussian outputs. A momentum-updated teacher network defines a\ndynamic, data-dependent prior, while the student encoder produces an\napproximate posterior from augmented views. The reconstruction term in the ELBO\nis replaced with a cross-view denoising objective, preserving the analytical\ntractability of Gaussian KL divergence. We further introduce cosine-based\nformulations of KL and log-likelihood terms to enhance semantic alignment in\nhigh-dimensional latent spaces. Experiments on CIFAR-10, CIFAR-100, and\nImageNet-100 show that VSSL achieves competitive or superior performance to\nleading self-supervised methods, including BYOL and MoCo V3. VSSL offers a\nscalable, probabilistically grounded approach to learning transferable\nrepresentations without generative reconstruction, bridging the gap between\nvariational modeling and modern self-supervised techniques.\n", "link": "http://arxiv.org/abs/2504.04318v2", "date": "2025-04-24", "relevancy": 2.8222, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6028}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5847}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Variational%20Self-Supervised%20Learning&body=Title%3A%20Variational%20Self-Supervised%20Learning%0AAuthor%3A%20Mehmet%20Can%20Yavuz%20and%20Berrin%20Yanikoglu%0AAbstract%3A%20%20%20We%20present%20Variational%20Self-Supervised%20Learning%20%28VSSL%29%2C%20a%20novel%20framework%0Athat%20combines%20variational%20inference%20with%20self-supervised%20learning%20to%20enable%0Aefficient%2C%20decoder-free%20representation%20learning.%20Unlike%20traditional%20VAEs%20that%0Arely%20on%20input%20reconstruction%20via%20a%20decoder%2C%20VSSL%20symmetrically%20couples%20two%0Aencoders%20with%20Gaussian%20outputs.%20A%20momentum-updated%20teacher%20network%20defines%20a%0Adynamic%2C%20data-dependent%20prior%2C%20while%20the%20student%20encoder%20produces%20an%0Aapproximate%20posterior%20from%20augmented%20views.%20The%20reconstruction%20term%20in%20the%20ELBO%0Ais%20replaced%20with%20a%20cross-view%20denoising%20objective%2C%20preserving%20the%20analytical%0Atractability%20of%20Gaussian%20KL%20divergence.%20We%20further%20introduce%20cosine-based%0Aformulations%20of%20KL%20and%20log-likelihood%20terms%20to%20enhance%20semantic%20alignment%20in%0Ahigh-dimensional%20latent%20spaces.%20Experiments%20on%20CIFAR-10%2C%20CIFAR-100%2C%20and%0AImageNet-100%20show%20that%20VSSL%20achieves%20competitive%20or%20superior%20performance%20to%0Aleading%20self-supervised%20methods%2C%20including%20BYOL%20and%20MoCo%20V3.%20VSSL%20offers%20a%0Ascalable%2C%20probabilistically%20grounded%20approach%20to%20learning%20transferable%0Arepresentations%20without%20generative%20reconstruction%2C%20bridging%20the%20gap%20between%0Avariational%20modeling%20and%20modern%20self-supervised%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.04318v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVariational%2520Self-Supervised%2520Learning%26entry.906535625%3DMehmet%2520Can%2520Yavuz%2520and%2520Berrin%2520Yanikoglu%26entry.1292438233%3D%2520%2520We%2520present%2520Variational%2520Self-Supervised%2520Learning%2520%2528VSSL%2529%252C%2520a%2520novel%2520framework%250Athat%2520combines%2520variational%2520inference%2520with%2520self-supervised%2520learning%2520to%2520enable%250Aefficient%252C%2520decoder-free%2520representation%2520learning.%2520Unlike%2520traditional%2520VAEs%2520that%250Arely%2520on%2520input%2520reconstruction%2520via%2520a%2520decoder%252C%2520VSSL%2520symmetrically%2520couples%2520two%250Aencoders%2520with%2520Gaussian%2520outputs.%2520A%2520momentum-updated%2520teacher%2520network%2520defines%2520a%250Adynamic%252C%2520data-dependent%2520prior%252C%2520while%2520the%2520student%2520encoder%2520produces%2520an%250Aapproximate%2520posterior%2520from%2520augmented%2520views.%2520The%2520reconstruction%2520term%2520in%2520the%2520ELBO%250Ais%2520replaced%2520with%2520a%2520cross-view%2520denoising%2520objective%252C%2520preserving%2520the%2520analytical%250Atractability%2520of%2520Gaussian%2520KL%2520divergence.%2520We%2520further%2520introduce%2520cosine-based%250Aformulations%2520of%2520KL%2520and%2520log-likelihood%2520terms%2520to%2520enhance%2520semantic%2520alignment%2520in%250Ahigh-dimensional%2520latent%2520spaces.%2520Experiments%2520on%2520CIFAR-10%252C%2520CIFAR-100%252C%2520and%250AImageNet-100%2520show%2520that%2520VSSL%2520achieves%2520competitive%2520or%2520superior%2520performance%2520to%250Aleading%2520self-supervised%2520methods%252C%2520including%2520BYOL%2520and%2520MoCo%2520V3.%2520VSSL%2520offers%2520a%250Ascalable%252C%2520probabilistically%2520grounded%2520approach%2520to%2520learning%2520transferable%250Arepresentations%2520without%2520generative%2520reconstruction%252C%2520bridging%2520the%2520gap%2520between%250Avariational%2520modeling%2520and%2520modern%2520self-supervised%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.04318v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variational%20Self-Supervised%20Learning&entry.906535625=Mehmet%20Can%20Yavuz%20and%20Berrin%20Yanikoglu&entry.1292438233=%20%20We%20present%20Variational%20Self-Supervised%20Learning%20%28VSSL%29%2C%20a%20novel%20framework%0Athat%20combines%20variational%20inference%20with%20self-supervised%20learning%20to%20enable%0Aefficient%2C%20decoder-free%20representation%20learning.%20Unlike%20traditional%20VAEs%20that%0Arely%20on%20input%20reconstruction%20via%20a%20decoder%2C%20VSSL%20symmetrically%20couples%20two%0Aencoders%20with%20Gaussian%20outputs.%20A%20momentum-updated%20teacher%20network%20defines%20a%0Adynamic%2C%20data-dependent%20prior%2C%20while%20the%20student%20encoder%20produces%20an%0Aapproximate%20posterior%20from%20augmented%20views.%20The%20reconstruction%20term%20in%20the%20ELBO%0Ais%20replaced%20with%20a%20cross-view%20denoising%20objective%2C%20preserving%20the%20analytical%0Atractability%20of%20Gaussian%20KL%20divergence.%20We%20further%20introduce%20cosine-based%0Aformulations%20of%20KL%20and%20log-likelihood%20terms%20to%20enhance%20semantic%20alignment%20in%0Ahigh-dimensional%20latent%20spaces.%20Experiments%20on%20CIFAR-10%2C%20CIFAR-100%2C%20and%0AImageNet-100%20show%20that%20VSSL%20achieves%20competitive%20or%20superior%20performance%20to%0Aleading%20self-supervised%20methods%2C%20including%20BYOL%20and%20MoCo%20V3.%20VSSL%20offers%20a%0Ascalable%2C%20probabilistically%20grounded%20approach%20to%20learning%20transferable%0Arepresentations%20without%20generative%20reconstruction%2C%20bridging%20the%20gap%20between%0Avariational%20modeling%20and%20modern%20self-supervised%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.04318v2&entry.124074799=Read"},
{"title": "BIM-Constrained Optimization for Accurate Localization and Deviation\n  Correction in Construction Monitoring", "author": "Asier Bikandi and Muhammad Shaheer and Hriday Bavle and Jayan Jevanesan and Holger Voos and Jose Luis Sanchez-Lopez", "abstract": "  Augmented reality (AR) applications for construction monitoring rely on\nreal-time environmental tracking to visualize architectural elements. However,\nconstruction sites present significant challenges for traditional tracking\nmethods due to featureless surfaces, dynamic changes, and drift accumulation,\nleading to misalignment between digital models and the physical world. This\npaper proposes a BIM-aware drift correction method to address these challenges.\nInstead of relying solely on SLAM-based localization, we align ``as-built\"\ndetected planes from the real-world environment with ``as-planned\"\narchitectural planes in BIM. Our method performs robust plane matching and\ncomputes a transformation (TF) between SLAM (S) and BIM (B) origin frames using\noptimization techniques, minimizing drift over time. By incorporating BIM as\nprior structural knowledge, we can achieve improved long-term localization and\nenhanced AR visualization accuracy in noisy construction environments. The\nmethod is evaluated through real-world experiments, showing significant\nreductions in drift-induced errors and optimized alignment consistency. On\naverage, our system achieves a reduction of 52.24% in angular deviations and a\nreduction of 60.8% in the distance error of the matched walls compared to the\ninitial manual alignment by the user.\n", "link": "http://arxiv.org/abs/2504.17693v1", "date": "2025-04-24", "relevancy": 2.814, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5804}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5544}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BIM-Constrained%20Optimization%20for%20Accurate%20Localization%20and%20Deviation%0A%20%20Correction%20in%20Construction%20Monitoring&body=Title%3A%20BIM-Constrained%20Optimization%20for%20Accurate%20Localization%20and%20Deviation%0A%20%20Correction%20in%20Construction%20Monitoring%0AAuthor%3A%20Asier%20Bikandi%20and%20Muhammad%20Shaheer%20and%20Hriday%20Bavle%20and%20Jayan%20Jevanesan%20and%20Holger%20Voos%20and%20Jose%20Luis%20Sanchez-Lopez%0AAbstract%3A%20%20%20Augmented%20reality%20%28AR%29%20applications%20for%20construction%20monitoring%20rely%20on%0Areal-time%20environmental%20tracking%20to%20visualize%20architectural%20elements.%20However%2C%0Aconstruction%20sites%20present%20significant%20challenges%20for%20traditional%20tracking%0Amethods%20due%20to%20featureless%20surfaces%2C%20dynamic%20changes%2C%20and%20drift%20accumulation%2C%0Aleading%20to%20misalignment%20between%20digital%20models%20and%20the%20physical%20world.%20This%0Apaper%20proposes%20a%20BIM-aware%20drift%20correction%20method%20to%20address%20these%20challenges.%0AInstead%20of%20relying%20solely%20on%20SLAM-based%20localization%2C%20we%20align%20%60%60as-built%22%0Adetected%20planes%20from%20the%20real-world%20environment%20with%20%60%60as-planned%22%0Aarchitectural%20planes%20in%20BIM.%20Our%20method%20performs%20robust%20plane%20matching%20and%0Acomputes%20a%20transformation%20%28TF%29%20between%20SLAM%20%28S%29%20and%20BIM%20%28B%29%20origin%20frames%20using%0Aoptimization%20techniques%2C%20minimizing%20drift%20over%20time.%20By%20incorporating%20BIM%20as%0Aprior%20structural%20knowledge%2C%20we%20can%20achieve%20improved%20long-term%20localization%20and%0Aenhanced%20AR%20visualization%20accuracy%20in%20noisy%20construction%20environments.%20The%0Amethod%20is%20evaluated%20through%20real-world%20experiments%2C%20showing%20significant%0Areductions%20in%20drift-induced%20errors%20and%20optimized%20alignment%20consistency.%20On%0Aaverage%2C%20our%20system%20achieves%20a%20reduction%20of%2052.24%25%20in%20angular%20deviations%20and%20a%0Areduction%20of%2060.8%25%20in%20the%20distance%20error%20of%20the%20matched%20walls%20compared%20to%20the%0Ainitial%20manual%20alignment%20by%20the%20user.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17693v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBIM-Constrained%2520Optimization%2520for%2520Accurate%2520Localization%2520and%2520Deviation%250A%2520%2520Correction%2520in%2520Construction%2520Monitoring%26entry.906535625%3DAsier%2520Bikandi%2520and%2520Muhammad%2520Shaheer%2520and%2520Hriday%2520Bavle%2520and%2520Jayan%2520Jevanesan%2520and%2520Holger%2520Voos%2520and%2520Jose%2520Luis%2520Sanchez-Lopez%26entry.1292438233%3D%2520%2520Augmented%2520reality%2520%2528AR%2529%2520applications%2520for%2520construction%2520monitoring%2520rely%2520on%250Areal-time%2520environmental%2520tracking%2520to%2520visualize%2520architectural%2520elements.%2520However%252C%250Aconstruction%2520sites%2520present%2520significant%2520challenges%2520for%2520traditional%2520tracking%250Amethods%2520due%2520to%2520featureless%2520surfaces%252C%2520dynamic%2520changes%252C%2520and%2520drift%2520accumulation%252C%250Aleading%2520to%2520misalignment%2520between%2520digital%2520models%2520and%2520the%2520physical%2520world.%2520This%250Apaper%2520proposes%2520a%2520BIM-aware%2520drift%2520correction%2520method%2520to%2520address%2520these%2520challenges.%250AInstead%2520of%2520relying%2520solely%2520on%2520SLAM-based%2520localization%252C%2520we%2520align%2520%2560%2560as-built%2522%250Adetected%2520planes%2520from%2520the%2520real-world%2520environment%2520with%2520%2560%2560as-planned%2522%250Aarchitectural%2520planes%2520in%2520BIM.%2520Our%2520method%2520performs%2520robust%2520plane%2520matching%2520and%250Acomputes%2520a%2520transformation%2520%2528TF%2529%2520between%2520SLAM%2520%2528S%2529%2520and%2520BIM%2520%2528B%2529%2520origin%2520frames%2520using%250Aoptimization%2520techniques%252C%2520minimizing%2520drift%2520over%2520time.%2520By%2520incorporating%2520BIM%2520as%250Aprior%2520structural%2520knowledge%252C%2520we%2520can%2520achieve%2520improved%2520long-term%2520localization%2520and%250Aenhanced%2520AR%2520visualization%2520accuracy%2520in%2520noisy%2520construction%2520environments.%2520The%250Amethod%2520is%2520evaluated%2520through%2520real-world%2520experiments%252C%2520showing%2520significant%250Areductions%2520in%2520drift-induced%2520errors%2520and%2520optimized%2520alignment%2520consistency.%2520On%250Aaverage%252C%2520our%2520system%2520achieves%2520a%2520reduction%2520of%252052.24%2525%2520in%2520angular%2520deviations%2520and%2520a%250Areduction%2520of%252060.8%2525%2520in%2520the%2520distance%2520error%2520of%2520the%2520matched%2520walls%2520compared%2520to%2520the%250Ainitial%2520manual%2520alignment%2520by%2520the%2520user.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17693v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BIM-Constrained%20Optimization%20for%20Accurate%20Localization%20and%20Deviation%0A%20%20Correction%20in%20Construction%20Monitoring&entry.906535625=Asier%20Bikandi%20and%20Muhammad%20Shaheer%20and%20Hriday%20Bavle%20and%20Jayan%20Jevanesan%20and%20Holger%20Voos%20and%20Jose%20Luis%20Sanchez-Lopez&entry.1292438233=%20%20Augmented%20reality%20%28AR%29%20applications%20for%20construction%20monitoring%20rely%20on%0Areal-time%20environmental%20tracking%20to%20visualize%20architectural%20elements.%20However%2C%0Aconstruction%20sites%20present%20significant%20challenges%20for%20traditional%20tracking%0Amethods%20due%20to%20featureless%20surfaces%2C%20dynamic%20changes%2C%20and%20drift%20accumulation%2C%0Aleading%20to%20misalignment%20between%20digital%20models%20and%20the%20physical%20world.%20This%0Apaper%20proposes%20a%20BIM-aware%20drift%20correction%20method%20to%20address%20these%20challenges.%0AInstead%20of%20relying%20solely%20on%20SLAM-based%20localization%2C%20we%20align%20%60%60as-built%22%0Adetected%20planes%20from%20the%20real-world%20environment%20with%20%60%60as-planned%22%0Aarchitectural%20planes%20in%20BIM.%20Our%20method%20performs%20robust%20plane%20matching%20and%0Acomputes%20a%20transformation%20%28TF%29%20between%20SLAM%20%28S%29%20and%20BIM%20%28B%29%20origin%20frames%20using%0Aoptimization%20techniques%2C%20minimizing%20drift%20over%20time.%20By%20incorporating%20BIM%20as%0Aprior%20structural%20knowledge%2C%20we%20can%20achieve%20improved%20long-term%20localization%20and%0Aenhanced%20AR%20visualization%20accuracy%20in%20noisy%20construction%20environments.%20The%0Amethod%20is%20evaluated%20through%20real-world%20experiments%2C%20showing%20significant%0Areductions%20in%20drift-induced%20errors%20and%20optimized%20alignment%20consistency.%20On%0Aaverage%2C%20our%20system%20achieves%20a%20reduction%20of%2052.24%25%20in%20angular%20deviations%20and%20a%0Areduction%20of%2060.8%25%20in%20the%20distance%20error%20of%20the%20matched%20walls%20compared%20to%20the%0Ainitial%20manual%20alignment%20by%20the%20user.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17693v1&entry.124074799=Read"},
{"title": "jina-clip-v2: Multilingual Multimodal Embeddings for Text and Images", "author": "Andreas Koukounas and Georgios Mastrapas and Sedigheh Eslami and Bo Wang and Mohammad Kalim Akram and Michael G\u00fcnther and Isabelle Mohr and Saba Sturua and Nan Wang and Han Xiao", "abstract": "  Contrastive Language-Image Pretraining (CLIP) has been widely used for\ncrossmodal information retrieval and multimodal understanding tasks. However,\nCLIP models are mainly optimized for crossmodal vision-language tasks and\nunderperform in single-mode text tasks. Moreover, these models are often\ntrained on English datasets and therefore lack multilingual understanding.\nAdditionally, from a visual understanding perspective, previous CLIP-based\nmodels exhibit insufficient understanding of visually rich documents. In this\nwork, we propose jina-clip-v2, a contrastive vision-language model trained on\ntext pairs, triplets and image-text pairs via a multi-task and multi-stage\ncontrastive learning paradigm in order to support both text-only and crossmodal\ntasks. We employ a multilingual text encoder and expand the training dataset to\ninclude multilingual texts from 29 non-English languages, including Hindi,\nChinese, German, French, and others, as well as images of visually rich\ndocuments. We evaluate the model's performance and show that jina-clip-v2\nachieves notable improvements over state-of-the-art CLIP-based models in\nzero-shot text-only retrieval, semantic textual similarity, and crossmodal\nretrieval tasks in both English and multilingual settings. jina-clip-v2 also\nprovides for flexibility in embedding dimensionality, enabling users to select\nthe granularity of the representations. jina-clip-v2 is publicly available at\nhttps://huggingface.co/jinaai/jina-clip-v2.\n", "link": "http://arxiv.org/abs/2412.08802v2", "date": "2025-04-24", "relevancy": 2.792, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5984}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5384}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20jina-clip-v2%3A%20Multilingual%20Multimodal%20Embeddings%20for%20Text%20and%20Images&body=Title%3A%20jina-clip-v2%3A%20Multilingual%20Multimodal%20Embeddings%20for%20Text%20and%20Images%0AAuthor%3A%20Andreas%20Koukounas%20and%20Georgios%20Mastrapas%20and%20Sedigheh%20Eslami%20and%20Bo%20Wang%20and%20Mohammad%20Kalim%20Akram%20and%20Michael%20G%C3%BCnther%20and%20Isabelle%20Mohr%20and%20Saba%20Sturua%20and%20Nan%20Wang%20and%20Han%20Xiao%0AAbstract%3A%20%20%20Contrastive%20Language-Image%20Pretraining%20%28CLIP%29%20has%20been%20widely%20used%20for%0Acrossmodal%20information%20retrieval%20and%20multimodal%20understanding%20tasks.%20However%2C%0ACLIP%20models%20are%20mainly%20optimized%20for%20crossmodal%20vision-language%20tasks%20and%0Aunderperform%20in%20single-mode%20text%20tasks.%20Moreover%2C%20these%20models%20are%20often%0Atrained%20on%20English%20datasets%20and%20therefore%20lack%20multilingual%20understanding.%0AAdditionally%2C%20from%20a%20visual%20understanding%20perspective%2C%20previous%20CLIP-based%0Amodels%20exhibit%20insufficient%20understanding%20of%20visually%20rich%20documents.%20In%20this%0Awork%2C%20we%20propose%20jina-clip-v2%2C%20a%20contrastive%20vision-language%20model%20trained%20on%0Atext%20pairs%2C%20triplets%20and%20image-text%20pairs%20via%20a%20multi-task%20and%20multi-stage%0Acontrastive%20learning%20paradigm%20in%20order%20to%20support%20both%20text-only%20and%20crossmodal%0Atasks.%20We%20employ%20a%20multilingual%20text%20encoder%20and%20expand%20the%20training%20dataset%20to%0Ainclude%20multilingual%20texts%20from%2029%20non-English%20languages%2C%20including%20Hindi%2C%0AChinese%2C%20German%2C%20French%2C%20and%20others%2C%20as%20well%20as%20images%20of%20visually%20rich%0Adocuments.%20We%20evaluate%20the%20model%27s%20performance%20and%20show%20that%20jina-clip-v2%0Aachieves%20notable%20improvements%20over%20state-of-the-art%20CLIP-based%20models%20in%0Azero-shot%20text-only%20retrieval%2C%20semantic%20textual%20similarity%2C%20and%20crossmodal%0Aretrieval%20tasks%20in%20both%20English%20and%20multilingual%20settings.%20jina-clip-v2%20also%0Aprovides%20for%20flexibility%20in%20embedding%20dimensionality%2C%20enabling%20users%20to%20select%0Athe%20granularity%20of%20the%20representations.%20jina-clip-v2%20is%20publicly%20available%20at%0Ahttps%3A//huggingface.co/jinaai/jina-clip-v2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08802v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Djina-clip-v2%253A%2520Multilingual%2520Multimodal%2520Embeddings%2520for%2520Text%2520and%2520Images%26entry.906535625%3DAndreas%2520Koukounas%2520and%2520Georgios%2520Mastrapas%2520and%2520Sedigheh%2520Eslami%2520and%2520Bo%2520Wang%2520and%2520Mohammad%2520Kalim%2520Akram%2520and%2520Michael%2520G%25C3%25BCnther%2520and%2520Isabelle%2520Mohr%2520and%2520Saba%2520Sturua%2520and%2520Nan%2520Wang%2520and%2520Han%2520Xiao%26entry.1292438233%3D%2520%2520Contrastive%2520Language-Image%2520Pretraining%2520%2528CLIP%2529%2520has%2520been%2520widely%2520used%2520for%250Acrossmodal%2520information%2520retrieval%2520and%2520multimodal%2520understanding%2520tasks.%2520However%252C%250ACLIP%2520models%2520are%2520mainly%2520optimized%2520for%2520crossmodal%2520vision-language%2520tasks%2520and%250Aunderperform%2520in%2520single-mode%2520text%2520tasks.%2520Moreover%252C%2520these%2520models%2520are%2520often%250Atrained%2520on%2520English%2520datasets%2520and%2520therefore%2520lack%2520multilingual%2520understanding.%250AAdditionally%252C%2520from%2520a%2520visual%2520understanding%2520perspective%252C%2520previous%2520CLIP-based%250Amodels%2520exhibit%2520insufficient%2520understanding%2520of%2520visually%2520rich%2520documents.%2520In%2520this%250Awork%252C%2520we%2520propose%2520jina-clip-v2%252C%2520a%2520contrastive%2520vision-language%2520model%2520trained%2520on%250Atext%2520pairs%252C%2520triplets%2520and%2520image-text%2520pairs%2520via%2520a%2520multi-task%2520and%2520multi-stage%250Acontrastive%2520learning%2520paradigm%2520in%2520order%2520to%2520support%2520both%2520text-only%2520and%2520crossmodal%250Atasks.%2520We%2520employ%2520a%2520multilingual%2520text%2520encoder%2520and%2520expand%2520the%2520training%2520dataset%2520to%250Ainclude%2520multilingual%2520texts%2520from%252029%2520non-English%2520languages%252C%2520including%2520Hindi%252C%250AChinese%252C%2520German%252C%2520French%252C%2520and%2520others%252C%2520as%2520well%2520as%2520images%2520of%2520visually%2520rich%250Adocuments.%2520We%2520evaluate%2520the%2520model%2527s%2520performance%2520and%2520show%2520that%2520jina-clip-v2%250Aachieves%2520notable%2520improvements%2520over%2520state-of-the-art%2520CLIP-based%2520models%2520in%250Azero-shot%2520text-only%2520retrieval%252C%2520semantic%2520textual%2520similarity%252C%2520and%2520crossmodal%250Aretrieval%2520tasks%2520in%2520both%2520English%2520and%2520multilingual%2520settings.%2520jina-clip-v2%2520also%250Aprovides%2520for%2520flexibility%2520in%2520embedding%2520dimensionality%252C%2520enabling%2520users%2520to%2520select%250Athe%2520granularity%2520of%2520the%2520representations.%2520jina-clip-v2%2520is%2520publicly%2520available%2520at%250Ahttps%253A//huggingface.co/jinaai/jina-clip-v2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08802v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=jina-clip-v2%3A%20Multilingual%20Multimodal%20Embeddings%20for%20Text%20and%20Images&entry.906535625=Andreas%20Koukounas%20and%20Georgios%20Mastrapas%20and%20Sedigheh%20Eslami%20and%20Bo%20Wang%20and%20Mohammad%20Kalim%20Akram%20and%20Michael%20G%C3%BCnther%20and%20Isabelle%20Mohr%20and%20Saba%20Sturua%20and%20Nan%20Wang%20and%20Han%20Xiao&entry.1292438233=%20%20Contrastive%20Language-Image%20Pretraining%20%28CLIP%29%20has%20been%20widely%20used%20for%0Acrossmodal%20information%20retrieval%20and%20multimodal%20understanding%20tasks.%20However%2C%0ACLIP%20models%20are%20mainly%20optimized%20for%20crossmodal%20vision-language%20tasks%20and%0Aunderperform%20in%20single-mode%20text%20tasks.%20Moreover%2C%20these%20models%20are%20often%0Atrained%20on%20English%20datasets%20and%20therefore%20lack%20multilingual%20understanding.%0AAdditionally%2C%20from%20a%20visual%20understanding%20perspective%2C%20previous%20CLIP-based%0Amodels%20exhibit%20insufficient%20understanding%20of%20visually%20rich%20documents.%20In%20this%0Awork%2C%20we%20propose%20jina-clip-v2%2C%20a%20contrastive%20vision-language%20model%20trained%20on%0Atext%20pairs%2C%20triplets%20and%20image-text%20pairs%20via%20a%20multi-task%20and%20multi-stage%0Acontrastive%20learning%20paradigm%20in%20order%20to%20support%20both%20text-only%20and%20crossmodal%0Atasks.%20We%20employ%20a%20multilingual%20text%20encoder%20and%20expand%20the%20training%20dataset%20to%0Ainclude%20multilingual%20texts%20from%2029%20non-English%20languages%2C%20including%20Hindi%2C%0AChinese%2C%20German%2C%20French%2C%20and%20others%2C%20as%20well%20as%20images%20of%20visually%20rich%0Adocuments.%20We%20evaluate%20the%20model%27s%20performance%20and%20show%20that%20jina-clip-v2%0Aachieves%20notable%20improvements%20over%20state-of-the-art%20CLIP-based%20models%20in%0Azero-shot%20text-only%20retrieval%2C%20semantic%20textual%20similarity%2C%20and%20crossmodal%0Aretrieval%20tasks%20in%20both%20English%20and%20multilingual%20settings.%20jina-clip-v2%20also%0Aprovides%20for%20flexibility%20in%20embedding%20dimensionality%2C%20enabling%20users%20to%20select%0Athe%20granularity%20of%20the%20representations.%20jina-clip-v2%20is%20publicly%20available%20at%0Ahttps%3A//huggingface.co/jinaai/jina-clip-v2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08802v2&entry.124074799=Read"},
{"title": "Unsupervised Urban Land Use Mapping with Street View Contrastive\n  Clustering and a Geographical Prior", "author": "Lin Che and Yizi Chen and Tanhua Jin and Martin Raubal and Konrad Schindler and Peter Kiefer", "abstract": "  Urban land use classification and mapping are critical for urban planning,\nresource management, and environmental monitoring. Existing remote sensing\ntechniques often lack precision in complex urban environments due to the\nabsence of ground-level details. Unlike aerial perspectives, street view images\nprovide a ground-level view that captures more human and social activities\nrelevant to land use in complex urban scenes. Existing street view-based\nmethods primarily rely on supervised classification, which is challenged by the\nscarcity of high-quality labeled data and the difficulty of generalizing across\ndiverse urban landscapes. This study introduces an unsupervised contrastive\nclustering model for street view images with a built-in geographical prior, to\nenhance clustering performance. When combined with a simple visual assignment\nof the clusters, our approach offers a flexible and customizable solution to\nland use mapping, tailored to the specific needs of urban planners. We\nexperimentally show that our method can generate land use maps from geotagged\nstreet view image datasets of two cities. As our methodology relies on the\nuniversal spatial coherence of geospatial data (\"Tobler's law\"), it can be\nadapted to various settings where street view images are available, to enable\nscalable, unsupervised land use mapping and updating. The code will be\navailable at https://github.com/lin102/CCGP.\n", "link": "http://arxiv.org/abs/2504.17551v1", "date": "2025-04-24", "relevancy": 2.7608, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5967}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5343}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Urban%20Land%20Use%20Mapping%20with%20Street%20View%20Contrastive%0A%20%20Clustering%20and%20a%20Geographical%20Prior&body=Title%3A%20Unsupervised%20Urban%20Land%20Use%20Mapping%20with%20Street%20View%20Contrastive%0A%20%20Clustering%20and%20a%20Geographical%20Prior%0AAuthor%3A%20Lin%20Che%20and%20Yizi%20Chen%20and%20Tanhua%20Jin%20and%20Martin%20Raubal%20and%20Konrad%20Schindler%20and%20Peter%20Kiefer%0AAbstract%3A%20%20%20Urban%20land%20use%20classification%20and%20mapping%20are%20critical%20for%20urban%20planning%2C%0Aresource%20management%2C%20and%20environmental%20monitoring.%20Existing%20remote%20sensing%0Atechniques%20often%20lack%20precision%20in%20complex%20urban%20environments%20due%20to%20the%0Aabsence%20of%20ground-level%20details.%20Unlike%20aerial%20perspectives%2C%20street%20view%20images%0Aprovide%20a%20ground-level%20view%20that%20captures%20more%20human%20and%20social%20activities%0Arelevant%20to%20land%20use%20in%20complex%20urban%20scenes.%20Existing%20street%20view-based%0Amethods%20primarily%20rely%20on%20supervised%20classification%2C%20which%20is%20challenged%20by%20the%0Ascarcity%20of%20high-quality%20labeled%20data%20and%20the%20difficulty%20of%20generalizing%20across%0Adiverse%20urban%20landscapes.%20This%20study%20introduces%20an%20unsupervised%20contrastive%0Aclustering%20model%20for%20street%20view%20images%20with%20a%20built-in%20geographical%20prior%2C%20to%0Aenhance%20clustering%20performance.%20When%20combined%20with%20a%20simple%20visual%20assignment%0Aof%20the%20clusters%2C%20our%20approach%20offers%20a%20flexible%20and%20customizable%20solution%20to%0Aland%20use%20mapping%2C%20tailored%20to%20the%20specific%20needs%20of%20urban%20planners.%20We%0Aexperimentally%20show%20that%20our%20method%20can%20generate%20land%20use%20maps%20from%20geotagged%0Astreet%20view%20image%20datasets%20of%20two%20cities.%20As%20our%20methodology%20relies%20on%20the%0Auniversal%20spatial%20coherence%20of%20geospatial%20data%20%28%22Tobler%27s%20law%22%29%2C%20it%20can%20be%0Aadapted%20to%20various%20settings%20where%20street%20view%20images%20are%20available%2C%20to%20enable%0Ascalable%2C%20unsupervised%20land%20use%20mapping%20and%20updating.%20The%20code%20will%20be%0Aavailable%20at%20https%3A//github.com/lin102/CCGP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17551v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Urban%2520Land%2520Use%2520Mapping%2520with%2520Street%2520View%2520Contrastive%250A%2520%2520Clustering%2520and%2520a%2520Geographical%2520Prior%26entry.906535625%3DLin%2520Che%2520and%2520Yizi%2520Chen%2520and%2520Tanhua%2520Jin%2520and%2520Martin%2520Raubal%2520and%2520Konrad%2520Schindler%2520and%2520Peter%2520Kiefer%26entry.1292438233%3D%2520%2520Urban%2520land%2520use%2520classification%2520and%2520mapping%2520are%2520critical%2520for%2520urban%2520planning%252C%250Aresource%2520management%252C%2520and%2520environmental%2520monitoring.%2520Existing%2520remote%2520sensing%250Atechniques%2520often%2520lack%2520precision%2520in%2520complex%2520urban%2520environments%2520due%2520to%2520the%250Aabsence%2520of%2520ground-level%2520details.%2520Unlike%2520aerial%2520perspectives%252C%2520street%2520view%2520images%250Aprovide%2520a%2520ground-level%2520view%2520that%2520captures%2520more%2520human%2520and%2520social%2520activities%250Arelevant%2520to%2520land%2520use%2520in%2520complex%2520urban%2520scenes.%2520Existing%2520street%2520view-based%250Amethods%2520primarily%2520rely%2520on%2520supervised%2520classification%252C%2520which%2520is%2520challenged%2520by%2520the%250Ascarcity%2520of%2520high-quality%2520labeled%2520data%2520and%2520the%2520difficulty%2520of%2520generalizing%2520across%250Adiverse%2520urban%2520landscapes.%2520This%2520study%2520introduces%2520an%2520unsupervised%2520contrastive%250Aclustering%2520model%2520for%2520street%2520view%2520images%2520with%2520a%2520built-in%2520geographical%2520prior%252C%2520to%250Aenhance%2520clustering%2520performance.%2520When%2520combined%2520with%2520a%2520simple%2520visual%2520assignment%250Aof%2520the%2520clusters%252C%2520our%2520approach%2520offers%2520a%2520flexible%2520and%2520customizable%2520solution%2520to%250Aland%2520use%2520mapping%252C%2520tailored%2520to%2520the%2520specific%2520needs%2520of%2520urban%2520planners.%2520We%250Aexperimentally%2520show%2520that%2520our%2520method%2520can%2520generate%2520land%2520use%2520maps%2520from%2520geotagged%250Astreet%2520view%2520image%2520datasets%2520of%2520two%2520cities.%2520As%2520our%2520methodology%2520relies%2520on%2520the%250Auniversal%2520spatial%2520coherence%2520of%2520geospatial%2520data%2520%2528%2522Tobler%2527s%2520law%2522%2529%252C%2520it%2520can%2520be%250Aadapted%2520to%2520various%2520settings%2520where%2520street%2520view%2520images%2520are%2520available%252C%2520to%2520enable%250Ascalable%252C%2520unsupervised%2520land%2520use%2520mapping%2520and%2520updating.%2520The%2520code%2520will%2520be%250Aavailable%2520at%2520https%253A//github.com/lin102/CCGP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17551v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Urban%20Land%20Use%20Mapping%20with%20Street%20View%20Contrastive%0A%20%20Clustering%20and%20a%20Geographical%20Prior&entry.906535625=Lin%20Che%20and%20Yizi%20Chen%20and%20Tanhua%20Jin%20and%20Martin%20Raubal%20and%20Konrad%20Schindler%20and%20Peter%20Kiefer&entry.1292438233=%20%20Urban%20land%20use%20classification%20and%20mapping%20are%20critical%20for%20urban%20planning%2C%0Aresource%20management%2C%20and%20environmental%20monitoring.%20Existing%20remote%20sensing%0Atechniques%20often%20lack%20precision%20in%20complex%20urban%20environments%20due%20to%20the%0Aabsence%20of%20ground-level%20details.%20Unlike%20aerial%20perspectives%2C%20street%20view%20images%0Aprovide%20a%20ground-level%20view%20that%20captures%20more%20human%20and%20social%20activities%0Arelevant%20to%20land%20use%20in%20complex%20urban%20scenes.%20Existing%20street%20view-based%0Amethods%20primarily%20rely%20on%20supervised%20classification%2C%20which%20is%20challenged%20by%20the%0Ascarcity%20of%20high-quality%20labeled%20data%20and%20the%20difficulty%20of%20generalizing%20across%0Adiverse%20urban%20landscapes.%20This%20study%20introduces%20an%20unsupervised%20contrastive%0Aclustering%20model%20for%20street%20view%20images%20with%20a%20built-in%20geographical%20prior%2C%20to%0Aenhance%20clustering%20performance.%20When%20combined%20with%20a%20simple%20visual%20assignment%0Aof%20the%20clusters%2C%20our%20approach%20offers%20a%20flexible%20and%20customizable%20solution%20to%0Aland%20use%20mapping%2C%20tailored%20to%20the%20specific%20needs%20of%20urban%20planners.%20We%0Aexperimentally%20show%20that%20our%20method%20can%20generate%20land%20use%20maps%20from%20geotagged%0Astreet%20view%20image%20datasets%20of%20two%20cities.%20As%20our%20methodology%20relies%20on%20the%0Auniversal%20spatial%20coherence%20of%20geospatial%20data%20%28%22Tobler%27s%20law%22%29%2C%20it%20can%20be%0Aadapted%20to%20various%20settings%20where%20street%20view%20images%20are%20available%2C%20to%20enable%0Ascalable%2C%20unsupervised%20land%20use%20mapping%20and%20updating.%20The%20code%20will%20be%0Aavailable%20at%20https%3A//github.com/lin102/CCGP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17551v1&entry.124074799=Read"},
{"title": "RGB-D Tracking via Hierarchical Modality Aggregation and Distribution\n  Network", "author": "Boyue Xu and Yi Xu and Ruichao Hou and Jia Bei and Tongwei Ren and Gangshan Wu", "abstract": "  The integration of dual-modal features has been pivotal in advancing\nRGB-Depth (RGB-D) tracking. However, current trackers are less efficient and\nfocus solely on single-level features, resulting in weaker robustness in fusion\nand slower speeds that fail to meet the demands of real-world applications. In\nthis paper, we introduce a novel network, denoted as HMAD (Hierarchical\nModality Aggregation and Distribution), which addresses these challenges. HMAD\nleverages the distinct feature representation strengths of RGB and depth\nmodalities, giving prominence to a hierarchical approach for feature\ndistribution and fusion, thereby enhancing the robustness of RGB-D tracking.\nExperimental results on various RGB-D datasets demonstrate that HMAD achieves\nstate-of-the-art performance. Moreover, real-world experiments further validate\nHMAD's capacity to effectively handle a spectrum of tracking challenges in\nreal-time scenarios.\n", "link": "http://arxiv.org/abs/2504.17595v1", "date": "2025-04-24", "relevancy": 2.7445, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5597}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5534}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RGB-D%20Tracking%20via%20Hierarchical%20Modality%20Aggregation%20and%20Distribution%0A%20%20Network&body=Title%3A%20RGB-D%20Tracking%20via%20Hierarchical%20Modality%20Aggregation%20and%20Distribution%0A%20%20Network%0AAuthor%3A%20Boyue%20Xu%20and%20Yi%20Xu%20and%20Ruichao%20Hou%20and%20Jia%20Bei%20and%20Tongwei%20Ren%20and%20Gangshan%20Wu%0AAbstract%3A%20%20%20The%20integration%20of%20dual-modal%20features%20has%20been%20pivotal%20in%20advancing%0ARGB-Depth%20%28RGB-D%29%20tracking.%20However%2C%20current%20trackers%20are%20less%20efficient%20and%0Afocus%20solely%20on%20single-level%20features%2C%20resulting%20in%20weaker%20robustness%20in%20fusion%0Aand%20slower%20speeds%20that%20fail%20to%20meet%20the%20demands%20of%20real-world%20applications.%20In%0Athis%20paper%2C%20we%20introduce%20a%20novel%20network%2C%20denoted%20as%20HMAD%20%28Hierarchical%0AModality%20Aggregation%20and%20Distribution%29%2C%20which%20addresses%20these%20challenges.%20HMAD%0Aleverages%20the%20distinct%20feature%20representation%20strengths%20of%20RGB%20and%20depth%0Amodalities%2C%20giving%20prominence%20to%20a%20hierarchical%20approach%20for%20feature%0Adistribution%20and%20fusion%2C%20thereby%20enhancing%20the%20robustness%20of%20RGB-D%20tracking.%0AExperimental%20results%20on%20various%20RGB-D%20datasets%20demonstrate%20that%20HMAD%20achieves%0Astate-of-the-art%20performance.%20Moreover%2C%20real-world%20experiments%20further%20validate%0AHMAD%27s%20capacity%20to%20effectively%20handle%20a%20spectrum%20of%20tracking%20challenges%20in%0Areal-time%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17595v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRGB-D%2520Tracking%2520via%2520Hierarchical%2520Modality%2520Aggregation%2520and%2520Distribution%250A%2520%2520Network%26entry.906535625%3DBoyue%2520Xu%2520and%2520Yi%2520Xu%2520and%2520Ruichao%2520Hou%2520and%2520Jia%2520Bei%2520and%2520Tongwei%2520Ren%2520and%2520Gangshan%2520Wu%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520dual-modal%2520features%2520has%2520been%2520pivotal%2520in%2520advancing%250ARGB-Depth%2520%2528RGB-D%2529%2520tracking.%2520However%252C%2520current%2520trackers%2520are%2520less%2520efficient%2520and%250Afocus%2520solely%2520on%2520single-level%2520features%252C%2520resulting%2520in%2520weaker%2520robustness%2520in%2520fusion%250Aand%2520slower%2520speeds%2520that%2520fail%2520to%2520meet%2520the%2520demands%2520of%2520real-world%2520applications.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520network%252C%2520denoted%2520as%2520HMAD%2520%2528Hierarchical%250AModality%2520Aggregation%2520and%2520Distribution%2529%252C%2520which%2520addresses%2520these%2520challenges.%2520HMAD%250Aleverages%2520the%2520distinct%2520feature%2520representation%2520strengths%2520of%2520RGB%2520and%2520depth%250Amodalities%252C%2520giving%2520prominence%2520to%2520a%2520hierarchical%2520approach%2520for%2520feature%250Adistribution%2520and%2520fusion%252C%2520thereby%2520enhancing%2520the%2520robustness%2520of%2520RGB-D%2520tracking.%250AExperimental%2520results%2520on%2520various%2520RGB-D%2520datasets%2520demonstrate%2520that%2520HMAD%2520achieves%250Astate-of-the-art%2520performance.%2520Moreover%252C%2520real-world%2520experiments%2520further%2520validate%250AHMAD%2527s%2520capacity%2520to%2520effectively%2520handle%2520a%2520spectrum%2520of%2520tracking%2520challenges%2520in%250Areal-time%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17595v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RGB-D%20Tracking%20via%20Hierarchical%20Modality%20Aggregation%20and%20Distribution%0A%20%20Network&entry.906535625=Boyue%20Xu%20and%20Yi%20Xu%20and%20Ruichao%20Hou%20and%20Jia%20Bei%20and%20Tongwei%20Ren%20and%20Gangshan%20Wu&entry.1292438233=%20%20The%20integration%20of%20dual-modal%20features%20has%20been%20pivotal%20in%20advancing%0ARGB-Depth%20%28RGB-D%29%20tracking.%20However%2C%20current%20trackers%20are%20less%20efficient%20and%0Afocus%20solely%20on%20single-level%20features%2C%20resulting%20in%20weaker%20robustness%20in%20fusion%0Aand%20slower%20speeds%20that%20fail%20to%20meet%20the%20demands%20of%20real-world%20applications.%20In%0Athis%20paper%2C%20we%20introduce%20a%20novel%20network%2C%20denoted%20as%20HMAD%20%28Hierarchical%0AModality%20Aggregation%20and%20Distribution%29%2C%20which%20addresses%20these%20challenges.%20HMAD%0Aleverages%20the%20distinct%20feature%20representation%20strengths%20of%20RGB%20and%20depth%0Amodalities%2C%20giving%20prominence%20to%20a%20hierarchical%20approach%20for%20feature%0Adistribution%20and%20fusion%2C%20thereby%20enhancing%20the%20robustness%20of%20RGB-D%20tracking.%0AExperimental%20results%20on%20various%20RGB-D%20datasets%20demonstrate%20that%20HMAD%20achieves%0Astate-of-the-art%20performance.%20Moreover%2C%20real-world%20experiments%20further%20validate%0AHMAD%27s%20capacity%20to%20effectively%20handle%20a%20spectrum%20of%20tracking%20challenges%20in%0Areal-time%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17595v1&entry.124074799=Read"},
{"title": "3DV-TON: Textured 3D-Guided Consistent Video Try-on via Diffusion Models", "author": "Min Wei and Chaohui Yu and Jingkai Zhou and Fan Wang", "abstract": "  Video try-on replaces clothing in videos with target garments. Existing\nmethods struggle to generate high-quality and temporally consistent results\nwhen handling complex clothing patterns and diverse body poses. We present\n3DV-TON, a novel diffusion-based framework for generating high-fidelity and\ntemporally consistent video try-on results. Our approach employs generated\nanimatable textured 3D meshes as explicit frame-level guidance, alleviating the\nissue of models over-focusing on appearance fidelity at the expanse of motion\ncoherence. This is achieved by enabling direct reference to consistent garment\ntexture movements throughout video sequences. The proposed method features an\nadaptive pipeline for generating dynamic 3D guidance: (1) selecting a keyframe\nfor initial 2D image try-on, followed by (2) reconstructing and animating a\ntextured 3D mesh synchronized with original video poses. We further introduce a\nrobust rectangular masking strategy that successfully mitigates artifact\npropagation caused by leaking clothing information during dynamic human and\ngarment movements. To advance video try-on research, we introduce HR-VVT, a\nhigh-resolution benchmark dataset containing 130 videos with diverse clothing\ntypes and scenarios. Quantitative and qualitative results demonstrate our\nsuperior performance over existing methods. The project page is at this link\nhttps://2y7c3.github.io/3DV-TON/\n", "link": "http://arxiv.org/abs/2504.17414v1", "date": "2025-04-24", "relevancy": 2.7441, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6864}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6858}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DV-TON%3A%20Textured%203D-Guided%20Consistent%20Video%20Try-on%20via%20Diffusion%20Models&body=Title%3A%203DV-TON%3A%20Textured%203D-Guided%20Consistent%20Video%20Try-on%20via%20Diffusion%20Models%0AAuthor%3A%20Min%20Wei%20and%20Chaohui%20Yu%20and%20Jingkai%20Zhou%20and%20Fan%20Wang%0AAbstract%3A%20%20%20Video%20try-on%20replaces%20clothing%20in%20videos%20with%20target%20garments.%20Existing%0Amethods%20struggle%20to%20generate%20high-quality%20and%20temporally%20consistent%20results%0Awhen%20handling%20complex%20clothing%20patterns%20and%20diverse%20body%20poses.%20We%20present%0A3DV-TON%2C%20a%20novel%20diffusion-based%20framework%20for%20generating%20high-fidelity%20and%0Atemporally%20consistent%20video%20try-on%20results.%20Our%20approach%20employs%20generated%0Aanimatable%20textured%203D%20meshes%20as%20explicit%20frame-level%20guidance%2C%20alleviating%20the%0Aissue%20of%20models%20over-focusing%20on%20appearance%20fidelity%20at%20the%20expanse%20of%20motion%0Acoherence.%20This%20is%20achieved%20by%20enabling%20direct%20reference%20to%20consistent%20garment%0Atexture%20movements%20throughout%20video%20sequences.%20The%20proposed%20method%20features%20an%0Aadaptive%20pipeline%20for%20generating%20dynamic%203D%20guidance%3A%20%281%29%20selecting%20a%20keyframe%0Afor%20initial%202D%20image%20try-on%2C%20followed%20by%20%282%29%20reconstructing%20and%20animating%20a%0Atextured%203D%20mesh%20synchronized%20with%20original%20video%20poses.%20We%20further%20introduce%20a%0Arobust%20rectangular%20masking%20strategy%20that%20successfully%20mitigates%20artifact%0Apropagation%20caused%20by%20leaking%20clothing%20information%20during%20dynamic%20human%20and%0Agarment%20movements.%20To%20advance%20video%20try-on%20research%2C%20we%20introduce%20HR-VVT%2C%20a%0Ahigh-resolution%20benchmark%20dataset%20containing%20130%20videos%20with%20diverse%20clothing%0Atypes%20and%20scenarios.%20Quantitative%20and%20qualitative%20results%20demonstrate%20our%0Asuperior%20performance%20over%20existing%20methods.%20The%20project%20page%20is%20at%20this%20link%0Ahttps%3A//2y7c3.github.io/3DV-TON/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17414v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DV-TON%253A%2520Textured%25203D-Guided%2520Consistent%2520Video%2520Try-on%2520via%2520Diffusion%2520Models%26entry.906535625%3DMin%2520Wei%2520and%2520Chaohui%2520Yu%2520and%2520Jingkai%2520Zhou%2520and%2520Fan%2520Wang%26entry.1292438233%3D%2520%2520Video%2520try-on%2520replaces%2520clothing%2520in%2520videos%2520with%2520target%2520garments.%2520Existing%250Amethods%2520struggle%2520to%2520generate%2520high-quality%2520and%2520temporally%2520consistent%2520results%250Awhen%2520handling%2520complex%2520clothing%2520patterns%2520and%2520diverse%2520body%2520poses.%2520We%2520present%250A3DV-TON%252C%2520a%2520novel%2520diffusion-based%2520framework%2520for%2520generating%2520high-fidelity%2520and%250Atemporally%2520consistent%2520video%2520try-on%2520results.%2520Our%2520approach%2520employs%2520generated%250Aanimatable%2520textured%25203D%2520meshes%2520as%2520explicit%2520frame-level%2520guidance%252C%2520alleviating%2520the%250Aissue%2520of%2520models%2520over-focusing%2520on%2520appearance%2520fidelity%2520at%2520the%2520expanse%2520of%2520motion%250Acoherence.%2520This%2520is%2520achieved%2520by%2520enabling%2520direct%2520reference%2520to%2520consistent%2520garment%250Atexture%2520movements%2520throughout%2520video%2520sequences.%2520The%2520proposed%2520method%2520features%2520an%250Aadaptive%2520pipeline%2520for%2520generating%2520dynamic%25203D%2520guidance%253A%2520%25281%2529%2520selecting%2520a%2520keyframe%250Afor%2520initial%25202D%2520image%2520try-on%252C%2520followed%2520by%2520%25282%2529%2520reconstructing%2520and%2520animating%2520a%250Atextured%25203D%2520mesh%2520synchronized%2520with%2520original%2520video%2520poses.%2520We%2520further%2520introduce%2520a%250Arobust%2520rectangular%2520masking%2520strategy%2520that%2520successfully%2520mitigates%2520artifact%250Apropagation%2520caused%2520by%2520leaking%2520clothing%2520information%2520during%2520dynamic%2520human%2520and%250Agarment%2520movements.%2520To%2520advance%2520video%2520try-on%2520research%252C%2520we%2520introduce%2520HR-VVT%252C%2520a%250Ahigh-resolution%2520benchmark%2520dataset%2520containing%2520130%2520videos%2520with%2520diverse%2520clothing%250Atypes%2520and%2520scenarios.%2520Quantitative%2520and%2520qualitative%2520results%2520demonstrate%2520our%250Asuperior%2520performance%2520over%2520existing%2520methods.%2520The%2520project%2520page%2520is%2520at%2520this%2520link%250Ahttps%253A//2y7c3.github.io/3DV-TON/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17414v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DV-TON%3A%20Textured%203D-Guided%20Consistent%20Video%20Try-on%20via%20Diffusion%20Models&entry.906535625=Min%20Wei%20and%20Chaohui%20Yu%20and%20Jingkai%20Zhou%20and%20Fan%20Wang&entry.1292438233=%20%20Video%20try-on%20replaces%20clothing%20in%20videos%20with%20target%20garments.%20Existing%0Amethods%20struggle%20to%20generate%20high-quality%20and%20temporally%20consistent%20results%0Awhen%20handling%20complex%20clothing%20patterns%20and%20diverse%20body%20poses.%20We%20present%0A3DV-TON%2C%20a%20novel%20diffusion-based%20framework%20for%20generating%20high-fidelity%20and%0Atemporally%20consistent%20video%20try-on%20results.%20Our%20approach%20employs%20generated%0Aanimatable%20textured%203D%20meshes%20as%20explicit%20frame-level%20guidance%2C%20alleviating%20the%0Aissue%20of%20models%20over-focusing%20on%20appearance%20fidelity%20at%20the%20expanse%20of%20motion%0Acoherence.%20This%20is%20achieved%20by%20enabling%20direct%20reference%20to%20consistent%20garment%0Atexture%20movements%20throughout%20video%20sequences.%20The%20proposed%20method%20features%20an%0Aadaptive%20pipeline%20for%20generating%20dynamic%203D%20guidance%3A%20%281%29%20selecting%20a%20keyframe%0Afor%20initial%202D%20image%20try-on%2C%20followed%20by%20%282%29%20reconstructing%20and%20animating%20a%0Atextured%203D%20mesh%20synchronized%20with%20original%20video%20poses.%20We%20further%20introduce%20a%0Arobust%20rectangular%20masking%20strategy%20that%20successfully%20mitigates%20artifact%0Apropagation%20caused%20by%20leaking%20clothing%20information%20during%20dynamic%20human%20and%0Agarment%20movements.%20To%20advance%20video%20try-on%20research%2C%20we%20introduce%20HR-VVT%2C%20a%0Ahigh-resolution%20benchmark%20dataset%20containing%20130%20videos%20with%20diverse%20clothing%0Atypes%20and%20scenarios.%20Quantitative%20and%20qualitative%20results%20demonstrate%20our%0Asuperior%20performance%20over%20existing%20methods.%20The%20project%20page%20is%20at%20this%20link%0Ahttps%3A//2y7c3.github.io/3DV-TON/%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17414v1&entry.124074799=Read"},
{"title": "Interpretable non-linear dimensionality reduction using gaussian\n  weighted linear transformation", "author": "Erik Bergh", "abstract": "  Dimensionality reduction techniques are fundamental for analyzing and\nvisualizing high-dimensional data. With established methods like t-SNE and PCA\npresenting a trade-off between representational power and interpretability.\nThis paper introduces a novel approach that bridges this gap by combining the\ninterpretability of linear methods with the expressiveness of non-linear\ntransformations. The proposed algorithm constructs a non-linear mapping between\nhigh-dimensional and low-dimensional spaces through a combination of linear\ntransformations, each weighted by Gaussian functions. This architecture enables\ncomplex non-linear transformations while preserving the interpretability\nadvantages of linear methods, as each transformation can be analyzed\nindependently. The resulting model provides both powerful dimensionality\nreduction and transparent insights into the transformed space. Techniques for\ninterpreting the learned transformations are presented, including methods for\nidentifying suppressed dimensions and how space is expanded and contracted.\nThese tools enable practitioners to understand how the algorithm preserves and\nmodifies geometric relationships during dimensionality reduction. To ensure the\npractical utility of this algorithm, the creation of user-friendly software\npackages is emphasized, facilitating its adoption in both academia and\nindustry.\n", "link": "http://arxiv.org/abs/2504.17601v1", "date": "2025-04-24", "relevancy": 2.7083, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5594}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5375}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5281}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20non-linear%20dimensionality%20reduction%20using%20gaussian%0A%20%20weighted%20linear%20transformation&body=Title%3A%20Interpretable%20non-linear%20dimensionality%20reduction%20using%20gaussian%0A%20%20weighted%20linear%20transformation%0AAuthor%3A%20Erik%20Bergh%0AAbstract%3A%20%20%20Dimensionality%20reduction%20techniques%20are%20fundamental%20for%20analyzing%20and%0Avisualizing%20high-dimensional%20data.%20With%20established%20methods%20like%20t-SNE%20and%20PCA%0Apresenting%20a%20trade-off%20between%20representational%20power%20and%20interpretability.%0AThis%20paper%20introduces%20a%20novel%20approach%20that%20bridges%20this%20gap%20by%20combining%20the%0Ainterpretability%20of%20linear%20methods%20with%20the%20expressiveness%20of%20non-linear%0Atransformations.%20The%20proposed%20algorithm%20constructs%20a%20non-linear%20mapping%20between%0Ahigh-dimensional%20and%20low-dimensional%20spaces%20through%20a%20combination%20of%20linear%0Atransformations%2C%20each%20weighted%20by%20Gaussian%20functions.%20This%20architecture%20enables%0Acomplex%20non-linear%20transformations%20while%20preserving%20the%20interpretability%0Aadvantages%20of%20linear%20methods%2C%20as%20each%20transformation%20can%20be%20analyzed%0Aindependently.%20The%20resulting%20model%20provides%20both%20powerful%20dimensionality%0Areduction%20and%20transparent%20insights%20into%20the%20transformed%20space.%20Techniques%20for%0Ainterpreting%20the%20learned%20transformations%20are%20presented%2C%20including%20methods%20for%0Aidentifying%20suppressed%20dimensions%20and%20how%20space%20is%20expanded%20and%20contracted.%0AThese%20tools%20enable%20practitioners%20to%20understand%20how%20the%20algorithm%20preserves%20and%0Amodifies%20geometric%20relationships%20during%20dimensionality%20reduction.%20To%20ensure%20the%0Apractical%20utility%20of%20this%20algorithm%2C%20the%20creation%20of%20user-friendly%20software%0Apackages%20is%20emphasized%2C%20facilitating%20its%20adoption%20in%20both%20academia%20and%0Aindustry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17601v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520non-linear%2520dimensionality%2520reduction%2520using%2520gaussian%250A%2520%2520weighted%2520linear%2520transformation%26entry.906535625%3DErik%2520Bergh%26entry.1292438233%3D%2520%2520Dimensionality%2520reduction%2520techniques%2520are%2520fundamental%2520for%2520analyzing%2520and%250Avisualizing%2520high-dimensional%2520data.%2520With%2520established%2520methods%2520like%2520t-SNE%2520and%2520PCA%250Apresenting%2520a%2520trade-off%2520between%2520representational%2520power%2520and%2520interpretability.%250AThis%2520paper%2520introduces%2520a%2520novel%2520approach%2520that%2520bridges%2520this%2520gap%2520by%2520combining%2520the%250Ainterpretability%2520of%2520linear%2520methods%2520with%2520the%2520expressiveness%2520of%2520non-linear%250Atransformations.%2520The%2520proposed%2520algorithm%2520constructs%2520a%2520non-linear%2520mapping%2520between%250Ahigh-dimensional%2520and%2520low-dimensional%2520spaces%2520through%2520a%2520combination%2520of%2520linear%250Atransformations%252C%2520each%2520weighted%2520by%2520Gaussian%2520functions.%2520This%2520architecture%2520enables%250Acomplex%2520non-linear%2520transformations%2520while%2520preserving%2520the%2520interpretability%250Aadvantages%2520of%2520linear%2520methods%252C%2520as%2520each%2520transformation%2520can%2520be%2520analyzed%250Aindependently.%2520The%2520resulting%2520model%2520provides%2520both%2520powerful%2520dimensionality%250Areduction%2520and%2520transparent%2520insights%2520into%2520the%2520transformed%2520space.%2520Techniques%2520for%250Ainterpreting%2520the%2520learned%2520transformations%2520are%2520presented%252C%2520including%2520methods%2520for%250Aidentifying%2520suppressed%2520dimensions%2520and%2520how%2520space%2520is%2520expanded%2520and%2520contracted.%250AThese%2520tools%2520enable%2520practitioners%2520to%2520understand%2520how%2520the%2520algorithm%2520preserves%2520and%250Amodifies%2520geometric%2520relationships%2520during%2520dimensionality%2520reduction.%2520To%2520ensure%2520the%250Apractical%2520utility%2520of%2520this%2520algorithm%252C%2520the%2520creation%2520of%2520user-friendly%2520software%250Apackages%2520is%2520emphasized%252C%2520facilitating%2520its%2520adoption%2520in%2520both%2520academia%2520and%250Aindustry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17601v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20non-linear%20dimensionality%20reduction%20using%20gaussian%0A%20%20weighted%20linear%20transformation&entry.906535625=Erik%20Bergh&entry.1292438233=%20%20Dimensionality%20reduction%20techniques%20are%20fundamental%20for%20analyzing%20and%0Avisualizing%20high-dimensional%20data.%20With%20established%20methods%20like%20t-SNE%20and%20PCA%0Apresenting%20a%20trade-off%20between%20representational%20power%20and%20interpretability.%0AThis%20paper%20introduces%20a%20novel%20approach%20that%20bridges%20this%20gap%20by%20combining%20the%0Ainterpretability%20of%20linear%20methods%20with%20the%20expressiveness%20of%20non-linear%0Atransformations.%20The%20proposed%20algorithm%20constructs%20a%20non-linear%20mapping%20between%0Ahigh-dimensional%20and%20low-dimensional%20spaces%20through%20a%20combination%20of%20linear%0Atransformations%2C%20each%20weighted%20by%20Gaussian%20functions.%20This%20architecture%20enables%0Acomplex%20non-linear%20transformations%20while%20preserving%20the%20interpretability%0Aadvantages%20of%20linear%20methods%2C%20as%20each%20transformation%20can%20be%20analyzed%0Aindependently.%20The%20resulting%20model%20provides%20both%20powerful%20dimensionality%0Areduction%20and%20transparent%20insights%20into%20the%20transformed%20space.%20Techniques%20for%0Ainterpreting%20the%20learned%20transformations%20are%20presented%2C%20including%20methods%20for%0Aidentifying%20suppressed%20dimensions%20and%20how%20space%20is%20expanded%20and%20contracted.%0AThese%20tools%20enable%20practitioners%20to%20understand%20how%20the%20algorithm%20preserves%20and%0Amodifies%20geometric%20relationships%20during%20dimensionality%20reduction.%20To%20ensure%20the%0Apractical%20utility%20of%20this%20algorithm%2C%20the%20creation%20of%20user-friendly%20software%0Apackages%20is%20emphasized%2C%20facilitating%20its%20adoption%20in%20both%20academia%20and%0Aindustry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17601v1&entry.124074799=Read"},
{"title": "CasualHDRSplat: Robust High Dynamic Range 3D Gaussian Splatting from\n  Casually Captured Videos", "author": "Shucheng Gong and Lingzhe Zhao and Wenpu Li and Hong Xie and Yin Zhang and Shiyu Zhao and Peidong Liu", "abstract": "  Recently, photo-realistic novel view synthesis from multi-view images, such\nas neural radiance field (NeRF) and 3D Gaussian Splatting (3DGS), have garnered\nwidespread attention due to their superior performance. However, most works\nrely on low dynamic range (LDR) images, which limits the capturing of richer\nscene details. Some prior works have focused on high dynamic range (HDR) scene\nreconstruction, typically require capturing of multi-view sharp images with\ndifferent exposure times at fixed camera positions during exposure times, which\nis time-consuming and challenging in practice. For a more flexible data\nacquisition, we propose a one-stage method: \\textbf{CasualHDRSplat} to easily\nand robustly reconstruct the 3D HDR scene from casually captured videos with\nauto-exposure enabled, even in the presence of severe motion blur and varying\nunknown exposure time. \\textbf{CasualHDRSplat} contains a unified\ndifferentiable physical imaging model which first applies continuous-time\ntrajectory constraint to imaging process so that we can jointly optimize\nexposure time, camera response function (CRF), camera poses, and sharp 3D HDR\nscene. Extensive experiments demonstrate that our approach outperforms existing\nmethods in terms of robustness and rendering quality. Our source code will be\navailable at https://github.com/WU-CVGL/CasualHDRSplat\n", "link": "http://arxiv.org/abs/2504.17728v1", "date": "2025-04-24", "relevancy": 2.6995, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7137}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6577}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6206}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CasualHDRSplat%3A%20Robust%20High%20Dynamic%20Range%203D%20Gaussian%20Splatting%20from%0A%20%20Casually%20Captured%20Videos&body=Title%3A%20CasualHDRSplat%3A%20Robust%20High%20Dynamic%20Range%203D%20Gaussian%20Splatting%20from%0A%20%20Casually%20Captured%20Videos%0AAuthor%3A%20Shucheng%20Gong%20and%20Lingzhe%20Zhao%20and%20Wenpu%20Li%20and%20Hong%20Xie%20and%20Yin%20Zhang%20and%20Shiyu%20Zhao%20and%20Peidong%20Liu%0AAbstract%3A%20%20%20Recently%2C%20photo-realistic%20novel%20view%20synthesis%20from%20multi-view%20images%2C%20such%0Aas%20neural%20radiance%20field%20%28NeRF%29%20and%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20have%20garnered%0Awidespread%20attention%20due%20to%20their%20superior%20performance.%20However%2C%20most%20works%0Arely%20on%20low%20dynamic%20range%20%28LDR%29%20images%2C%20which%20limits%20the%20capturing%20of%20richer%0Ascene%20details.%20Some%20prior%20works%20have%20focused%20on%20high%20dynamic%20range%20%28HDR%29%20scene%0Areconstruction%2C%20typically%20require%20capturing%20of%20multi-view%20sharp%20images%20with%0Adifferent%20exposure%20times%20at%20fixed%20camera%20positions%20during%20exposure%20times%2C%20which%0Ais%20time-consuming%20and%20challenging%20in%20practice.%20For%20a%20more%20flexible%20data%0Aacquisition%2C%20we%20propose%20a%20one-stage%20method%3A%20%5Ctextbf%7BCasualHDRSplat%7D%20to%20easily%0Aand%20robustly%20reconstruct%20the%203D%20HDR%20scene%20from%20casually%20captured%20videos%20with%0Aauto-exposure%20enabled%2C%20even%20in%20the%20presence%20of%20severe%20motion%20blur%20and%20varying%0Aunknown%20exposure%20time.%20%5Ctextbf%7BCasualHDRSplat%7D%20contains%20a%20unified%0Adifferentiable%20physical%20imaging%20model%20which%20first%20applies%20continuous-time%0Atrajectory%20constraint%20to%20imaging%20process%20so%20that%20we%20can%20jointly%20optimize%0Aexposure%20time%2C%20camera%20response%20function%20%28CRF%29%2C%20camera%20poses%2C%20and%20sharp%203D%20HDR%0Ascene.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20outperforms%20existing%0Amethods%20in%20terms%20of%20robustness%20and%20rendering%20quality.%20Our%20source%20code%20will%20be%0Aavailable%20at%20https%3A//github.com/WU-CVGL/CasualHDRSplat%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17728v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCasualHDRSplat%253A%2520Robust%2520High%2520Dynamic%2520Range%25203D%2520Gaussian%2520Splatting%2520from%250A%2520%2520Casually%2520Captured%2520Videos%26entry.906535625%3DShucheng%2520Gong%2520and%2520Lingzhe%2520Zhao%2520and%2520Wenpu%2520Li%2520and%2520Hong%2520Xie%2520and%2520Yin%2520Zhang%2520and%2520Shiyu%2520Zhao%2520and%2520Peidong%2520Liu%26entry.1292438233%3D%2520%2520Recently%252C%2520photo-realistic%2520novel%2520view%2520synthesis%2520from%2520multi-view%2520images%252C%2520such%250Aas%2520neural%2520radiance%2520field%2520%2528NeRF%2529%2520and%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%252C%2520have%2520garnered%250Awidespread%2520attention%2520due%2520to%2520their%2520superior%2520performance.%2520However%252C%2520most%2520works%250Arely%2520on%2520low%2520dynamic%2520range%2520%2528LDR%2529%2520images%252C%2520which%2520limits%2520the%2520capturing%2520of%2520richer%250Ascene%2520details.%2520Some%2520prior%2520works%2520have%2520focused%2520on%2520high%2520dynamic%2520range%2520%2528HDR%2529%2520scene%250Areconstruction%252C%2520typically%2520require%2520capturing%2520of%2520multi-view%2520sharp%2520images%2520with%250Adifferent%2520exposure%2520times%2520at%2520fixed%2520camera%2520positions%2520during%2520exposure%2520times%252C%2520which%250Ais%2520time-consuming%2520and%2520challenging%2520in%2520practice.%2520For%2520a%2520more%2520flexible%2520data%250Aacquisition%252C%2520we%2520propose%2520a%2520one-stage%2520method%253A%2520%255Ctextbf%257BCasualHDRSplat%257D%2520to%2520easily%250Aand%2520robustly%2520reconstruct%2520the%25203D%2520HDR%2520scene%2520from%2520casually%2520captured%2520videos%2520with%250Aauto-exposure%2520enabled%252C%2520even%2520in%2520the%2520presence%2520of%2520severe%2520motion%2520blur%2520and%2520varying%250Aunknown%2520exposure%2520time.%2520%255Ctextbf%257BCasualHDRSplat%257D%2520contains%2520a%2520unified%250Adifferentiable%2520physical%2520imaging%2520model%2520which%2520first%2520applies%2520continuous-time%250Atrajectory%2520constraint%2520to%2520imaging%2520process%2520so%2520that%2520we%2520can%2520jointly%2520optimize%250Aexposure%2520time%252C%2520camera%2520response%2520function%2520%2528CRF%2529%252C%2520camera%2520poses%252C%2520and%2520sharp%25203D%2520HDR%250Ascene.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520approach%2520outperforms%2520existing%250Amethods%2520in%2520terms%2520of%2520robustness%2520and%2520rendering%2520quality.%2520Our%2520source%2520code%2520will%2520be%250Aavailable%2520at%2520https%253A//github.com/WU-CVGL/CasualHDRSplat%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17728v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CasualHDRSplat%3A%20Robust%20High%20Dynamic%20Range%203D%20Gaussian%20Splatting%20from%0A%20%20Casually%20Captured%20Videos&entry.906535625=Shucheng%20Gong%20and%20Lingzhe%20Zhao%20and%20Wenpu%20Li%20and%20Hong%20Xie%20and%20Yin%20Zhang%20and%20Shiyu%20Zhao%20and%20Peidong%20Liu&entry.1292438233=%20%20Recently%2C%20photo-realistic%20novel%20view%20synthesis%20from%20multi-view%20images%2C%20such%0Aas%20neural%20radiance%20field%20%28NeRF%29%20and%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20have%20garnered%0Awidespread%20attention%20due%20to%20their%20superior%20performance.%20However%2C%20most%20works%0Arely%20on%20low%20dynamic%20range%20%28LDR%29%20images%2C%20which%20limits%20the%20capturing%20of%20richer%0Ascene%20details.%20Some%20prior%20works%20have%20focused%20on%20high%20dynamic%20range%20%28HDR%29%20scene%0Areconstruction%2C%20typically%20require%20capturing%20of%20multi-view%20sharp%20images%20with%0Adifferent%20exposure%20times%20at%20fixed%20camera%20positions%20during%20exposure%20times%2C%20which%0Ais%20time-consuming%20and%20challenging%20in%20practice.%20For%20a%20more%20flexible%20data%0Aacquisition%2C%20we%20propose%20a%20one-stage%20method%3A%20%5Ctextbf%7BCasualHDRSplat%7D%20to%20easily%0Aand%20robustly%20reconstruct%20the%203D%20HDR%20scene%20from%20casually%20captured%20videos%20with%0Aauto-exposure%20enabled%2C%20even%20in%20the%20presence%20of%20severe%20motion%20blur%20and%20varying%0Aunknown%20exposure%20time.%20%5Ctextbf%7BCasualHDRSplat%7D%20contains%20a%20unified%0Adifferentiable%20physical%20imaging%20model%20which%20first%20applies%20continuous-time%0Atrajectory%20constraint%20to%20imaging%20process%20so%20that%20we%20can%20jointly%20optimize%0Aexposure%20time%2C%20camera%20response%20function%20%28CRF%29%2C%20camera%20poses%2C%20and%20sharp%203D%20HDR%0Ascene.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20outperforms%20existing%0Amethods%20in%20terms%20of%20robustness%20and%20rendering%20quality.%20Our%20source%20code%20will%20be%0Aavailable%20at%20https%3A//github.com/WU-CVGL/CasualHDRSplat%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17728v1&entry.124074799=Read"},
{"title": "Unlocking Large Language Model's Planning Capabilities with Maximum\n  Diversity Fine-tuning", "author": "Wenjun Li and Changyu Chen and Pradeep Varakantham", "abstract": "  Large language models (LLMs) have demonstrated impressive task-solving\ncapabilities through prompting techniques and system designs, including solving\nplanning tasks (e.g., math proofs, basic travel planning) when sufficient data\nis available online and used during pre-training. However, for planning tasks\nwith limited prior data (e.g., blocks world, advanced travel planning), the\nperformance of LLMs, including proprietary models like GPT and Gemini, is poor.\nThis paper investigates the impact of fine-tuning on the planning capabilities\nof LLMs, revealing that LLMs can achieve strong performance in planning through\nsubstantial (tens of thousands of specific examples) fine-tuning. Yet, this\nprocess incurs high economic, time, and computational costs for each planning\nproblem variation. To address this, we propose Clustering-Based Maximum\nDiversity Sampling (CMDS), which selects diverse and representative data to\nenhance sample efficiency and the model's generalization capability. Extensive\nevaluations demonstrate that CMDS-l, a baseline method combining CMDS with\nlanguage embeddings, outperforms random sampling. Furthermore, we introduce a\nnovel algorithm, CMDS-g, which encodes planning task instances with their graph\nrepresentations into the embedding space. Empirical results show that CMDS-g\nconsistently outperforms baseline methods across various scales and multiple\nbenchmark domains.\n", "link": "http://arxiv.org/abs/2406.10479v2", "date": "2025-04-24", "relevancy": 2.6833, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.544}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.544}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5221}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unlocking%20Large%20Language%20Model%27s%20Planning%20Capabilities%20with%20Maximum%0A%20%20Diversity%20Fine-tuning&body=Title%3A%20Unlocking%20Large%20Language%20Model%27s%20Planning%20Capabilities%20with%20Maximum%0A%20%20Diversity%20Fine-tuning%0AAuthor%3A%20Wenjun%20Li%20and%20Changyu%20Chen%20and%20Pradeep%20Varakantham%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20impressive%20task-solving%0Acapabilities%20through%20prompting%20techniques%20and%20system%20designs%2C%20including%20solving%0Aplanning%20tasks%20%28e.g.%2C%20math%20proofs%2C%20basic%20travel%20planning%29%20when%20sufficient%20data%0Ais%20available%20online%20and%20used%20during%20pre-training.%20However%2C%20for%20planning%20tasks%0Awith%20limited%20prior%20data%20%28e.g.%2C%20blocks%20world%2C%20advanced%20travel%20planning%29%2C%20the%0Aperformance%20of%20LLMs%2C%20including%20proprietary%20models%20like%20GPT%20and%20Gemini%2C%20is%20poor.%0AThis%20paper%20investigates%20the%20impact%20of%20fine-tuning%20on%20the%20planning%20capabilities%0Aof%20LLMs%2C%20revealing%20that%20LLMs%20can%20achieve%20strong%20performance%20in%20planning%20through%0Asubstantial%20%28tens%20of%20thousands%20of%20specific%20examples%29%20fine-tuning.%20Yet%2C%20this%0Aprocess%20incurs%20high%20economic%2C%20time%2C%20and%20computational%20costs%20for%20each%20planning%0Aproblem%20variation.%20To%20address%20this%2C%20we%20propose%20Clustering-Based%20Maximum%0ADiversity%20Sampling%20%28CMDS%29%2C%20which%20selects%20diverse%20and%20representative%20data%20to%0Aenhance%20sample%20efficiency%20and%20the%20model%27s%20generalization%20capability.%20Extensive%0Aevaluations%20demonstrate%20that%20CMDS-l%2C%20a%20baseline%20method%20combining%20CMDS%20with%0Alanguage%20embeddings%2C%20outperforms%20random%20sampling.%20Furthermore%2C%20we%20introduce%20a%0Anovel%20algorithm%2C%20CMDS-g%2C%20which%20encodes%20planning%20task%20instances%20with%20their%20graph%0Arepresentations%20into%20the%20embedding%20space.%20Empirical%20results%20show%20that%20CMDS-g%0Aconsistently%20outperforms%20baseline%20methods%20across%20various%20scales%20and%20multiple%0Abenchmark%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10479v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnlocking%2520Large%2520Language%2520Model%2527s%2520Planning%2520Capabilities%2520with%2520Maximum%250A%2520%2520Diversity%2520Fine-tuning%26entry.906535625%3DWenjun%2520Li%2520and%2520Changyu%2520Chen%2520and%2520Pradeep%2520Varakantham%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520impressive%2520task-solving%250Acapabilities%2520through%2520prompting%2520techniques%2520and%2520system%2520designs%252C%2520including%2520solving%250Aplanning%2520tasks%2520%2528e.g.%252C%2520math%2520proofs%252C%2520basic%2520travel%2520planning%2529%2520when%2520sufficient%2520data%250Ais%2520available%2520online%2520and%2520used%2520during%2520pre-training.%2520However%252C%2520for%2520planning%2520tasks%250Awith%2520limited%2520prior%2520data%2520%2528e.g.%252C%2520blocks%2520world%252C%2520advanced%2520travel%2520planning%2529%252C%2520the%250Aperformance%2520of%2520LLMs%252C%2520including%2520proprietary%2520models%2520like%2520GPT%2520and%2520Gemini%252C%2520is%2520poor.%250AThis%2520paper%2520investigates%2520the%2520impact%2520of%2520fine-tuning%2520on%2520the%2520planning%2520capabilities%250Aof%2520LLMs%252C%2520revealing%2520that%2520LLMs%2520can%2520achieve%2520strong%2520performance%2520in%2520planning%2520through%250Asubstantial%2520%2528tens%2520of%2520thousands%2520of%2520specific%2520examples%2529%2520fine-tuning.%2520Yet%252C%2520this%250Aprocess%2520incurs%2520high%2520economic%252C%2520time%252C%2520and%2520computational%2520costs%2520for%2520each%2520planning%250Aproblem%2520variation.%2520To%2520address%2520this%252C%2520we%2520propose%2520Clustering-Based%2520Maximum%250ADiversity%2520Sampling%2520%2528CMDS%2529%252C%2520which%2520selects%2520diverse%2520and%2520representative%2520data%2520to%250Aenhance%2520sample%2520efficiency%2520and%2520the%2520model%2527s%2520generalization%2520capability.%2520Extensive%250Aevaluations%2520demonstrate%2520that%2520CMDS-l%252C%2520a%2520baseline%2520method%2520combining%2520CMDS%2520with%250Alanguage%2520embeddings%252C%2520outperforms%2520random%2520sampling.%2520Furthermore%252C%2520we%2520introduce%2520a%250Anovel%2520algorithm%252C%2520CMDS-g%252C%2520which%2520encodes%2520planning%2520task%2520instances%2520with%2520their%2520graph%250Arepresentations%2520into%2520the%2520embedding%2520space.%2520Empirical%2520results%2520show%2520that%2520CMDS-g%250Aconsistently%2520outperforms%2520baseline%2520methods%2520across%2520various%2520scales%2520and%2520multiple%250Abenchmark%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10479v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unlocking%20Large%20Language%20Model%27s%20Planning%20Capabilities%20with%20Maximum%0A%20%20Diversity%20Fine-tuning&entry.906535625=Wenjun%20Li%20and%20Changyu%20Chen%20and%20Pradeep%20Varakantham&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20impressive%20task-solving%0Acapabilities%20through%20prompting%20techniques%20and%20system%20designs%2C%20including%20solving%0Aplanning%20tasks%20%28e.g.%2C%20math%20proofs%2C%20basic%20travel%20planning%29%20when%20sufficient%20data%0Ais%20available%20online%20and%20used%20during%20pre-training.%20However%2C%20for%20planning%20tasks%0Awith%20limited%20prior%20data%20%28e.g.%2C%20blocks%20world%2C%20advanced%20travel%20planning%29%2C%20the%0Aperformance%20of%20LLMs%2C%20including%20proprietary%20models%20like%20GPT%20and%20Gemini%2C%20is%20poor.%0AThis%20paper%20investigates%20the%20impact%20of%20fine-tuning%20on%20the%20planning%20capabilities%0Aof%20LLMs%2C%20revealing%20that%20LLMs%20can%20achieve%20strong%20performance%20in%20planning%20through%0Asubstantial%20%28tens%20of%20thousands%20of%20specific%20examples%29%20fine-tuning.%20Yet%2C%20this%0Aprocess%20incurs%20high%20economic%2C%20time%2C%20and%20computational%20costs%20for%20each%20planning%0Aproblem%20variation.%20To%20address%20this%2C%20we%20propose%20Clustering-Based%20Maximum%0ADiversity%20Sampling%20%28CMDS%29%2C%20which%20selects%20diverse%20and%20representative%20data%20to%0Aenhance%20sample%20efficiency%20and%20the%20model%27s%20generalization%20capability.%20Extensive%0Aevaluations%20demonstrate%20that%20CMDS-l%2C%20a%20baseline%20method%20combining%20CMDS%20with%0Alanguage%20embeddings%2C%20outperforms%20random%20sampling.%20Furthermore%2C%20we%20introduce%20a%0Anovel%20algorithm%2C%20CMDS-g%2C%20which%20encodes%20planning%20task%20instances%20with%20their%20graph%0Arepresentations%20into%20the%20embedding%20space.%20Empirical%20results%20show%20that%20CMDS-g%0Aconsistently%20outperforms%20baseline%20methods%20across%20various%20scales%20and%20multiple%0Abenchmark%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10479v2&entry.124074799=Read"},
{"title": "PTCL: Pseudo-Label Temporal Curriculum Learning for Label-Limited\n  Dynamic Graph", "author": "Shengtao Zhang and Haokai Zhang and Shiqi Lou and Zicheng Wang and Zinan Zeng and Yilin Wang and Minnan Luo", "abstract": "  Dynamic node classification is critical for modeling evolving systems like\nfinancial transactions and academic collaborations. In such systems,\ndynamically capturing node information changes is critical for dynamic node\nclassification, which usually requires all labels at every timestamp. However,\nit is difficult to collect all dynamic labels in real-world scenarios due to\nhigh annotation costs and label uncertainty (e.g., ambiguous or delayed labels\nin fraud detection). In contrast, final timestamp labels are easier to obtain\nas they rely on complete temporal patterns and are usually maintained as a\nunique label for each user in many open platforms, without tracking the history\ndata. To bridge this gap, we propose PTCL(Pseudo-label Temporal Curriculum\nLearning), a pioneering method addressing label-limited dynamic node\nclassification where only final labels are available. PTCL introduces: (1) a\ntemporal decoupling architecture separating the backbone (learning time-aware\nrepresentations) and decoder (strictly aligned with final labels), which\ngenerate pseudo-labels, and (2) a Temporal Curriculum Learning strategy that\nprioritizes pseudo-labels closer to the final timestamp by assigning them\nhigher weights using an exponentially decaying function. We contribute a new\nacademic dataset (CoOAG), capturing long-range research interest in dynamic\ngraph. Experiments across real-world scenarios demonstrate PTCL's consistent\nsuperiority over other methods adapted to this task. Beyond methodology, we\npropose a unified framework FLiD (Framework for Label-Limited Dynamic Node\nClassification), consisting of a complete preparation workflow, training\npipeline, and evaluation standards, and supporting various models and datasets.\nThe code can be found at https://github.com/3205914485/FLiD.\n", "link": "http://arxiv.org/abs/2504.17641v1", "date": "2025-04-24", "relevancy": 2.6672, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5375}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5361}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PTCL%3A%20Pseudo-Label%20Temporal%20Curriculum%20Learning%20for%20Label-Limited%0A%20%20Dynamic%20Graph&body=Title%3A%20PTCL%3A%20Pseudo-Label%20Temporal%20Curriculum%20Learning%20for%20Label-Limited%0A%20%20Dynamic%20Graph%0AAuthor%3A%20Shengtao%20Zhang%20and%20Haokai%20Zhang%20and%20Shiqi%20Lou%20and%20Zicheng%20Wang%20and%20Zinan%20Zeng%20and%20Yilin%20Wang%20and%20Minnan%20Luo%0AAbstract%3A%20%20%20Dynamic%20node%20classification%20is%20critical%20for%20modeling%20evolving%20systems%20like%0Afinancial%20transactions%20and%20academic%20collaborations.%20In%20such%20systems%2C%0Adynamically%20capturing%20node%20information%20changes%20is%20critical%20for%20dynamic%20node%0Aclassification%2C%20which%20usually%20requires%20all%20labels%20at%20every%20timestamp.%20However%2C%0Ait%20is%20difficult%20to%20collect%20all%20dynamic%20labels%20in%20real-world%20scenarios%20due%20to%0Ahigh%20annotation%20costs%20and%20label%20uncertainty%20%28e.g.%2C%20ambiguous%20or%20delayed%20labels%0Ain%20fraud%20detection%29.%20In%20contrast%2C%20final%20timestamp%20labels%20are%20easier%20to%20obtain%0Aas%20they%20rely%20on%20complete%20temporal%20patterns%20and%20are%20usually%20maintained%20as%20a%0Aunique%20label%20for%20each%20user%20in%20many%20open%20platforms%2C%20without%20tracking%20the%20history%0Adata.%20To%20bridge%20this%20gap%2C%20we%20propose%20PTCL%28Pseudo-label%20Temporal%20Curriculum%0ALearning%29%2C%20a%20pioneering%20method%20addressing%20label-limited%20dynamic%20node%0Aclassification%20where%20only%20final%20labels%20are%20available.%20PTCL%20introduces%3A%20%281%29%20a%0Atemporal%20decoupling%20architecture%20separating%20the%20backbone%20%28learning%20time-aware%0Arepresentations%29%20and%20decoder%20%28strictly%20aligned%20with%20final%20labels%29%2C%20which%0Agenerate%20pseudo-labels%2C%20and%20%282%29%20a%20Temporal%20Curriculum%20Learning%20strategy%20that%0Aprioritizes%20pseudo-labels%20closer%20to%20the%20final%20timestamp%20by%20assigning%20them%0Ahigher%20weights%20using%20an%20exponentially%20decaying%20function.%20We%20contribute%20a%20new%0Aacademic%20dataset%20%28CoOAG%29%2C%20capturing%20long-range%20research%20interest%20in%20dynamic%0Agraph.%20Experiments%20across%20real-world%20scenarios%20demonstrate%20PTCL%27s%20consistent%0Asuperiority%20over%20other%20methods%20adapted%20to%20this%20task.%20Beyond%20methodology%2C%20we%0Apropose%20a%20unified%20framework%20FLiD%20%28Framework%20for%20Label-Limited%20Dynamic%20Node%0AClassification%29%2C%20consisting%20of%20a%20complete%20preparation%20workflow%2C%20training%0Apipeline%2C%20and%20evaluation%20standards%2C%20and%20supporting%20various%20models%20and%20datasets.%0AThe%20code%20can%20be%20found%20at%20https%3A//github.com/3205914485/FLiD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17641v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPTCL%253A%2520Pseudo-Label%2520Temporal%2520Curriculum%2520Learning%2520for%2520Label-Limited%250A%2520%2520Dynamic%2520Graph%26entry.906535625%3DShengtao%2520Zhang%2520and%2520Haokai%2520Zhang%2520and%2520Shiqi%2520Lou%2520and%2520Zicheng%2520Wang%2520and%2520Zinan%2520Zeng%2520and%2520Yilin%2520Wang%2520and%2520Minnan%2520Luo%26entry.1292438233%3D%2520%2520Dynamic%2520node%2520classification%2520is%2520critical%2520for%2520modeling%2520evolving%2520systems%2520like%250Afinancial%2520transactions%2520and%2520academic%2520collaborations.%2520In%2520such%2520systems%252C%250Adynamically%2520capturing%2520node%2520information%2520changes%2520is%2520critical%2520for%2520dynamic%2520node%250Aclassification%252C%2520which%2520usually%2520requires%2520all%2520labels%2520at%2520every%2520timestamp.%2520However%252C%250Ait%2520is%2520difficult%2520to%2520collect%2520all%2520dynamic%2520labels%2520in%2520real-world%2520scenarios%2520due%2520to%250Ahigh%2520annotation%2520costs%2520and%2520label%2520uncertainty%2520%2528e.g.%252C%2520ambiguous%2520or%2520delayed%2520labels%250Ain%2520fraud%2520detection%2529.%2520In%2520contrast%252C%2520final%2520timestamp%2520labels%2520are%2520easier%2520to%2520obtain%250Aas%2520they%2520rely%2520on%2520complete%2520temporal%2520patterns%2520and%2520are%2520usually%2520maintained%2520as%2520a%250Aunique%2520label%2520for%2520each%2520user%2520in%2520many%2520open%2520platforms%252C%2520without%2520tracking%2520the%2520history%250Adata.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520PTCL%2528Pseudo-label%2520Temporal%2520Curriculum%250ALearning%2529%252C%2520a%2520pioneering%2520method%2520addressing%2520label-limited%2520dynamic%2520node%250Aclassification%2520where%2520only%2520final%2520labels%2520are%2520available.%2520PTCL%2520introduces%253A%2520%25281%2529%2520a%250Atemporal%2520decoupling%2520architecture%2520separating%2520the%2520backbone%2520%2528learning%2520time-aware%250Arepresentations%2529%2520and%2520decoder%2520%2528strictly%2520aligned%2520with%2520final%2520labels%2529%252C%2520which%250Agenerate%2520pseudo-labels%252C%2520and%2520%25282%2529%2520a%2520Temporal%2520Curriculum%2520Learning%2520strategy%2520that%250Aprioritizes%2520pseudo-labels%2520closer%2520to%2520the%2520final%2520timestamp%2520by%2520assigning%2520them%250Ahigher%2520weights%2520using%2520an%2520exponentially%2520decaying%2520function.%2520We%2520contribute%2520a%2520new%250Aacademic%2520dataset%2520%2528CoOAG%2529%252C%2520capturing%2520long-range%2520research%2520interest%2520in%2520dynamic%250Agraph.%2520Experiments%2520across%2520real-world%2520scenarios%2520demonstrate%2520PTCL%2527s%2520consistent%250Asuperiority%2520over%2520other%2520methods%2520adapted%2520to%2520this%2520task.%2520Beyond%2520methodology%252C%2520we%250Apropose%2520a%2520unified%2520framework%2520FLiD%2520%2528Framework%2520for%2520Label-Limited%2520Dynamic%2520Node%250AClassification%2529%252C%2520consisting%2520of%2520a%2520complete%2520preparation%2520workflow%252C%2520training%250Apipeline%252C%2520and%2520evaluation%2520standards%252C%2520and%2520supporting%2520various%2520models%2520and%2520datasets.%250AThe%2520code%2520can%2520be%2520found%2520at%2520https%253A//github.com/3205914485/FLiD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17641v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PTCL%3A%20Pseudo-Label%20Temporal%20Curriculum%20Learning%20for%20Label-Limited%0A%20%20Dynamic%20Graph&entry.906535625=Shengtao%20Zhang%20and%20Haokai%20Zhang%20and%20Shiqi%20Lou%20and%20Zicheng%20Wang%20and%20Zinan%20Zeng%20and%20Yilin%20Wang%20and%20Minnan%20Luo&entry.1292438233=%20%20Dynamic%20node%20classification%20is%20critical%20for%20modeling%20evolving%20systems%20like%0Afinancial%20transactions%20and%20academic%20collaborations.%20In%20such%20systems%2C%0Adynamically%20capturing%20node%20information%20changes%20is%20critical%20for%20dynamic%20node%0Aclassification%2C%20which%20usually%20requires%20all%20labels%20at%20every%20timestamp.%20However%2C%0Ait%20is%20difficult%20to%20collect%20all%20dynamic%20labels%20in%20real-world%20scenarios%20due%20to%0Ahigh%20annotation%20costs%20and%20label%20uncertainty%20%28e.g.%2C%20ambiguous%20or%20delayed%20labels%0Ain%20fraud%20detection%29.%20In%20contrast%2C%20final%20timestamp%20labels%20are%20easier%20to%20obtain%0Aas%20they%20rely%20on%20complete%20temporal%20patterns%20and%20are%20usually%20maintained%20as%20a%0Aunique%20label%20for%20each%20user%20in%20many%20open%20platforms%2C%20without%20tracking%20the%20history%0Adata.%20To%20bridge%20this%20gap%2C%20we%20propose%20PTCL%28Pseudo-label%20Temporal%20Curriculum%0ALearning%29%2C%20a%20pioneering%20method%20addressing%20label-limited%20dynamic%20node%0Aclassification%20where%20only%20final%20labels%20are%20available.%20PTCL%20introduces%3A%20%281%29%20a%0Atemporal%20decoupling%20architecture%20separating%20the%20backbone%20%28learning%20time-aware%0Arepresentations%29%20and%20decoder%20%28strictly%20aligned%20with%20final%20labels%29%2C%20which%0Agenerate%20pseudo-labels%2C%20and%20%282%29%20a%20Temporal%20Curriculum%20Learning%20strategy%20that%0Aprioritizes%20pseudo-labels%20closer%20to%20the%20final%20timestamp%20by%20assigning%20them%0Ahigher%20weights%20using%20an%20exponentially%20decaying%20function.%20We%20contribute%20a%20new%0Aacademic%20dataset%20%28CoOAG%29%2C%20capturing%20long-range%20research%20interest%20in%20dynamic%0Agraph.%20Experiments%20across%20real-world%20scenarios%20demonstrate%20PTCL%27s%20consistent%0Asuperiority%20over%20other%20methods%20adapted%20to%20this%20task.%20Beyond%20methodology%2C%20we%0Apropose%20a%20unified%20framework%20FLiD%20%28Framework%20for%20Label-Limited%20Dynamic%20Node%0AClassification%29%2C%20consisting%20of%20a%20complete%20preparation%20workflow%2C%20training%0Apipeline%2C%20and%20evaluation%20standards%2C%20and%20supporting%20various%20models%20and%20datasets.%0AThe%20code%20can%20be%20found%20at%20https%3A//github.com/3205914485/FLiD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17641v1&entry.124074799=Read"},
{"title": "Sparse Gaussian Neural Processes", "author": "Tommy Rochussen and Vincent Fortuin", "abstract": "  Despite significant recent advances in probabilistic meta-learning, it is\ncommon for practitioners to avoid using deep learning models due to a\ncomparative lack of interpretability. Instead, many practitioners simply use\nnon-meta-models such as Gaussian processes with interpretable priors, and\nconduct the tedious procedure of training their model from scratch for each\ntask they encounter. While this is justifiable for tasks with a limited number\nof data points, the cubic computational cost of exact Gaussian process\ninference renders this prohibitive when each task has many observations. To\nremedy this, we introduce a family of models that meta-learn sparse Gaussian\nprocess inference. Not only does this enable rapid prediction on new tasks with\nsparse Gaussian processes, but since our models have clear interpretations as\nmembers of the neural process family, it also allows manual elicitation of\npriors in a neural process for the first time. In meta-learning regimes for\nwhich the number of observed tasks is small or for which expert domain\nknowledge is available, this offers a crucial advantage.\n", "link": "http://arxiv.org/abs/2504.01650v2", "date": "2025-04-24", "relevancy": 2.6482, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5511}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5215}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Gaussian%20Neural%20Processes&body=Title%3A%20Sparse%20Gaussian%20Neural%20Processes%0AAuthor%3A%20Tommy%20Rochussen%20and%20Vincent%20Fortuin%0AAbstract%3A%20%20%20Despite%20significant%20recent%20advances%20in%20probabilistic%20meta-learning%2C%20it%20is%0Acommon%20for%20practitioners%20to%20avoid%20using%20deep%20learning%20models%20due%20to%20a%0Acomparative%20lack%20of%20interpretability.%20Instead%2C%20many%20practitioners%20simply%20use%0Anon-meta-models%20such%20as%20Gaussian%20processes%20with%20interpretable%20priors%2C%20and%0Aconduct%20the%20tedious%20procedure%20of%20training%20their%20model%20from%20scratch%20for%20each%0Atask%20they%20encounter.%20While%20this%20is%20justifiable%20for%20tasks%20with%20a%20limited%20number%0Aof%20data%20points%2C%20the%20cubic%20computational%20cost%20of%20exact%20Gaussian%20process%0Ainference%20renders%20this%20prohibitive%20when%20each%20task%20has%20many%20observations.%20To%0Aremedy%20this%2C%20we%20introduce%20a%20family%20of%20models%20that%20meta-learn%20sparse%20Gaussian%0Aprocess%20inference.%20Not%20only%20does%20this%20enable%20rapid%20prediction%20on%20new%20tasks%20with%0Asparse%20Gaussian%20processes%2C%20but%20since%20our%20models%20have%20clear%20interpretations%20as%0Amembers%20of%20the%20neural%20process%20family%2C%20it%20also%20allows%20manual%20elicitation%20of%0Apriors%20in%20a%20neural%20process%20for%20the%20first%20time.%20In%20meta-learning%20regimes%20for%0Awhich%20the%20number%20of%20observed%20tasks%20is%20small%20or%20for%20which%20expert%20domain%0Aknowledge%20is%20available%2C%20this%20offers%20a%20crucial%20advantage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.01650v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Gaussian%2520Neural%2520Processes%26entry.906535625%3DTommy%2520Rochussen%2520and%2520Vincent%2520Fortuin%26entry.1292438233%3D%2520%2520Despite%2520significant%2520recent%2520advances%2520in%2520probabilistic%2520meta-learning%252C%2520it%2520is%250Acommon%2520for%2520practitioners%2520to%2520avoid%2520using%2520deep%2520learning%2520models%2520due%2520to%2520a%250Acomparative%2520lack%2520of%2520interpretability.%2520Instead%252C%2520many%2520practitioners%2520simply%2520use%250Anon-meta-models%2520such%2520as%2520Gaussian%2520processes%2520with%2520interpretable%2520priors%252C%2520and%250Aconduct%2520the%2520tedious%2520procedure%2520of%2520training%2520their%2520model%2520from%2520scratch%2520for%2520each%250Atask%2520they%2520encounter.%2520While%2520this%2520is%2520justifiable%2520for%2520tasks%2520with%2520a%2520limited%2520number%250Aof%2520data%2520points%252C%2520the%2520cubic%2520computational%2520cost%2520of%2520exact%2520Gaussian%2520process%250Ainference%2520renders%2520this%2520prohibitive%2520when%2520each%2520task%2520has%2520many%2520observations.%2520To%250Aremedy%2520this%252C%2520we%2520introduce%2520a%2520family%2520of%2520models%2520that%2520meta-learn%2520sparse%2520Gaussian%250Aprocess%2520inference.%2520Not%2520only%2520does%2520this%2520enable%2520rapid%2520prediction%2520on%2520new%2520tasks%2520with%250Asparse%2520Gaussian%2520processes%252C%2520but%2520since%2520our%2520models%2520have%2520clear%2520interpretations%2520as%250Amembers%2520of%2520the%2520neural%2520process%2520family%252C%2520it%2520also%2520allows%2520manual%2520elicitation%2520of%250Apriors%2520in%2520a%2520neural%2520process%2520for%2520the%2520first%2520time.%2520In%2520meta-learning%2520regimes%2520for%250Awhich%2520the%2520number%2520of%2520observed%2520tasks%2520is%2520small%2520or%2520for%2520which%2520expert%2520domain%250Aknowledge%2520is%2520available%252C%2520this%2520offers%2520a%2520crucial%2520advantage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.01650v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Gaussian%20Neural%20Processes&entry.906535625=Tommy%20Rochussen%20and%20Vincent%20Fortuin&entry.1292438233=%20%20Despite%20significant%20recent%20advances%20in%20probabilistic%20meta-learning%2C%20it%20is%0Acommon%20for%20practitioners%20to%20avoid%20using%20deep%20learning%20models%20due%20to%20a%0Acomparative%20lack%20of%20interpretability.%20Instead%2C%20many%20practitioners%20simply%20use%0Anon-meta-models%20such%20as%20Gaussian%20processes%20with%20interpretable%20priors%2C%20and%0Aconduct%20the%20tedious%20procedure%20of%20training%20their%20model%20from%20scratch%20for%20each%0Atask%20they%20encounter.%20While%20this%20is%20justifiable%20for%20tasks%20with%20a%20limited%20number%0Aof%20data%20points%2C%20the%20cubic%20computational%20cost%20of%20exact%20Gaussian%20process%0Ainference%20renders%20this%20prohibitive%20when%20each%20task%20has%20many%20observations.%20To%0Aremedy%20this%2C%20we%20introduce%20a%20family%20of%20models%20that%20meta-learn%20sparse%20Gaussian%0Aprocess%20inference.%20Not%20only%20does%20this%20enable%20rapid%20prediction%20on%20new%20tasks%20with%0Asparse%20Gaussian%20processes%2C%20but%20since%20our%20models%20have%20clear%20interpretations%20as%0Amembers%20of%20the%20neural%20process%20family%2C%20it%20also%20allows%20manual%20elicitation%20of%0Apriors%20in%20a%20neural%20process%20for%20the%20first%20time.%20In%20meta-learning%20regimes%20for%0Awhich%20the%20number%20of%20observed%20tasks%20is%20small%20or%20for%20which%20expert%20domain%0Aknowledge%20is%20available%2C%20this%20offers%20a%20crucial%20advantage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.01650v2&entry.124074799=Read"},
{"title": "DiffKillR: Killing and Recreating Diffeomorphisms for Cell Annotation in\n  Dense Microscopy Images", "author": "Chen Liu and Danqi Liao and Alejandro Parada-Mayorga and Alejandro Ribeiro and Marcello DiStasio and Smita Krishnaswamy", "abstract": "  The proliferation of digital microscopy images, driven by advances in\nautomated whole slide scanning, presents significant opportunities for\nbiomedical research and clinical diagnostics. However, accurately annotating\ndensely packed information in these images remains a major challenge. To\naddress this, we introduce DiffKillR, a novel framework that reframes cell\nannotation as the combination of archetype matching and image registration\ntasks. DiffKillR employs two complementary neural networks: one that learns a\ndiffeomorphism-invariant feature space for robust cell matching and another\nthat computes the precise warping field between cells for annotation mapping.\nUsing a small set of annotated archetypes, DiffKillR efficiently propagates\nannotations across large microscopy images, reducing the need for extensive\nmanual labeling. More importantly, it is suitable for any type of pixel-level\nannotation. We will discuss the theoretical properties of DiffKillR and\nvalidate it on three microscopy tasks, demonstrating its advantages over\nexisting supervised, semi-supervised, and unsupervised methods. The code is\navailable at https://github.com/KrishnaswamyLab/DiffKillR.\n", "link": "http://arxiv.org/abs/2410.03058v2", "date": "2025-04-24", "relevancy": 2.6058, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5218}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5208}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffKillR%3A%20Killing%20and%20Recreating%20Diffeomorphisms%20for%20Cell%20Annotation%20in%0A%20%20Dense%20Microscopy%20Images&body=Title%3A%20DiffKillR%3A%20Killing%20and%20Recreating%20Diffeomorphisms%20for%20Cell%20Annotation%20in%0A%20%20Dense%20Microscopy%20Images%0AAuthor%3A%20Chen%20Liu%20and%20Danqi%20Liao%20and%20Alejandro%20Parada-Mayorga%20and%20Alejandro%20Ribeiro%20and%20Marcello%20DiStasio%20and%20Smita%20Krishnaswamy%0AAbstract%3A%20%20%20The%20proliferation%20of%20digital%20microscopy%20images%2C%20driven%20by%20advances%20in%0Aautomated%20whole%20slide%20scanning%2C%20presents%20significant%20opportunities%20for%0Abiomedical%20research%20and%20clinical%20diagnostics.%20However%2C%20accurately%20annotating%0Adensely%20packed%20information%20in%20these%20images%20remains%20a%20major%20challenge.%20To%0Aaddress%20this%2C%20we%20introduce%20DiffKillR%2C%20a%20novel%20framework%20that%20reframes%20cell%0Aannotation%20as%20the%20combination%20of%20archetype%20matching%20and%20image%20registration%0Atasks.%20DiffKillR%20employs%20two%20complementary%20neural%20networks%3A%20one%20that%20learns%20a%0Adiffeomorphism-invariant%20feature%20space%20for%20robust%20cell%20matching%20and%20another%0Athat%20computes%20the%20precise%20warping%20field%20between%20cells%20for%20annotation%20mapping.%0AUsing%20a%20small%20set%20of%20annotated%20archetypes%2C%20DiffKillR%20efficiently%20propagates%0Aannotations%20across%20large%20microscopy%20images%2C%20reducing%20the%20need%20for%20extensive%0Amanual%20labeling.%20More%20importantly%2C%20it%20is%20suitable%20for%20any%20type%20of%20pixel-level%0Aannotation.%20We%20will%20discuss%20the%20theoretical%20properties%20of%20DiffKillR%20and%0Avalidate%20it%20on%20three%20microscopy%20tasks%2C%20demonstrating%20its%20advantages%20over%0Aexisting%20supervised%2C%20semi-supervised%2C%20and%20unsupervised%20methods.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/KrishnaswamyLab/DiffKillR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03058v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffKillR%253A%2520Killing%2520and%2520Recreating%2520Diffeomorphisms%2520for%2520Cell%2520Annotation%2520in%250A%2520%2520Dense%2520Microscopy%2520Images%26entry.906535625%3DChen%2520Liu%2520and%2520Danqi%2520Liao%2520and%2520Alejandro%2520Parada-Mayorga%2520and%2520Alejandro%2520Ribeiro%2520and%2520Marcello%2520DiStasio%2520and%2520Smita%2520Krishnaswamy%26entry.1292438233%3D%2520%2520The%2520proliferation%2520of%2520digital%2520microscopy%2520images%252C%2520driven%2520by%2520advances%2520in%250Aautomated%2520whole%2520slide%2520scanning%252C%2520presents%2520significant%2520opportunities%2520for%250Abiomedical%2520research%2520and%2520clinical%2520diagnostics.%2520However%252C%2520accurately%2520annotating%250Adensely%2520packed%2520information%2520in%2520these%2520images%2520remains%2520a%2520major%2520challenge.%2520To%250Aaddress%2520this%252C%2520we%2520introduce%2520DiffKillR%252C%2520a%2520novel%2520framework%2520that%2520reframes%2520cell%250Aannotation%2520as%2520the%2520combination%2520of%2520archetype%2520matching%2520and%2520image%2520registration%250Atasks.%2520DiffKillR%2520employs%2520two%2520complementary%2520neural%2520networks%253A%2520one%2520that%2520learns%2520a%250Adiffeomorphism-invariant%2520feature%2520space%2520for%2520robust%2520cell%2520matching%2520and%2520another%250Athat%2520computes%2520the%2520precise%2520warping%2520field%2520between%2520cells%2520for%2520annotation%2520mapping.%250AUsing%2520a%2520small%2520set%2520of%2520annotated%2520archetypes%252C%2520DiffKillR%2520efficiently%2520propagates%250Aannotations%2520across%2520large%2520microscopy%2520images%252C%2520reducing%2520the%2520need%2520for%2520extensive%250Amanual%2520labeling.%2520More%2520importantly%252C%2520it%2520is%2520suitable%2520for%2520any%2520type%2520of%2520pixel-level%250Aannotation.%2520We%2520will%2520discuss%2520the%2520theoretical%2520properties%2520of%2520DiffKillR%2520and%250Avalidate%2520it%2520on%2520three%2520microscopy%2520tasks%252C%2520demonstrating%2520its%2520advantages%2520over%250Aexisting%2520supervised%252C%2520semi-supervised%252C%2520and%2520unsupervised%2520methods.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/KrishnaswamyLab/DiffKillR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03058v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffKillR%3A%20Killing%20and%20Recreating%20Diffeomorphisms%20for%20Cell%20Annotation%20in%0A%20%20Dense%20Microscopy%20Images&entry.906535625=Chen%20Liu%20and%20Danqi%20Liao%20and%20Alejandro%20Parada-Mayorga%20and%20Alejandro%20Ribeiro%20and%20Marcello%20DiStasio%20and%20Smita%20Krishnaswamy&entry.1292438233=%20%20The%20proliferation%20of%20digital%20microscopy%20images%2C%20driven%20by%20advances%20in%0Aautomated%20whole%20slide%20scanning%2C%20presents%20significant%20opportunities%20for%0Abiomedical%20research%20and%20clinical%20diagnostics.%20However%2C%20accurately%20annotating%0Adensely%20packed%20information%20in%20these%20images%20remains%20a%20major%20challenge.%20To%0Aaddress%20this%2C%20we%20introduce%20DiffKillR%2C%20a%20novel%20framework%20that%20reframes%20cell%0Aannotation%20as%20the%20combination%20of%20archetype%20matching%20and%20image%20registration%0Atasks.%20DiffKillR%20employs%20two%20complementary%20neural%20networks%3A%20one%20that%20learns%20a%0Adiffeomorphism-invariant%20feature%20space%20for%20robust%20cell%20matching%20and%20another%0Athat%20computes%20the%20precise%20warping%20field%20between%20cells%20for%20annotation%20mapping.%0AUsing%20a%20small%20set%20of%20annotated%20archetypes%2C%20DiffKillR%20efficiently%20propagates%0Aannotations%20across%20large%20microscopy%20images%2C%20reducing%20the%20need%20for%20extensive%0Amanual%20labeling.%20More%20importantly%2C%20it%20is%20suitable%20for%20any%20type%20of%20pixel-level%0Aannotation.%20We%20will%20discuss%20the%20theoretical%20properties%20of%20DiffKillR%20and%0Avalidate%20it%20on%20three%20microscopy%20tasks%2C%20demonstrating%20its%20advantages%20over%0Aexisting%20supervised%2C%20semi-supervised%2C%20and%20unsupervised%20methods.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/KrishnaswamyLab/DiffKillR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03058v2&entry.124074799=Read"},
{"title": "A Comprehensive Survey of Knowledge-Based Vision Question Answering\n  Systems: The Lifecycle of Knowledge in Visual Reasoning Task", "author": "Jiaqi Deng and Zonghan Wu and Huan Huo and Guandong Xu", "abstract": "  Knowledge-based Vision Question Answering (KB-VQA) extends general Vision\nQuestion Answering (VQA) by not only requiring the understanding of visual and\ntextual inputs but also extensive range of knowledge, enabling significant\nadvancements across various real-world applications. KB-VQA introduces unique\nchallenges, including the alignment of heterogeneous information from diverse\nmodalities and sources, the retrieval of relevant knowledge from noisy or\nlarge-scale repositories, and the execution of complex reasoning to infer\nanswers from the combined context. With the advancement of Large Language\nModels (LLMs), KB-VQA systems have also undergone a notable transformation,\nwhere LLMs serve as powerful knowledge repositories, retrieval-augmented\ngenerators and strong reasoners. Despite substantial progress, no comprehensive\nsurvey currently exists that systematically organizes and reviews the existing\nKB-VQA methods. This survey aims to fill this gap by establishing a structured\ntaxonomy of KB-VQA approaches, and categorizing the systems into main stages:\nknowledge representation, knowledge retrieval, and knowledge reasoning. By\nexploring various knowledge integration techniques and identifying persistent\nchallenges, this work also outlines promising future research directions,\nproviding a foundation for advancing KB-VQA models and their applications.\n", "link": "http://arxiv.org/abs/2504.17547v1", "date": "2025-04-24", "relevancy": 2.6042, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5468}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5468}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Survey%20of%20Knowledge-Based%20Vision%20Question%20Answering%0A%20%20Systems%3A%20The%20Lifecycle%20of%20Knowledge%20in%20Visual%20Reasoning%20Task&body=Title%3A%20A%20Comprehensive%20Survey%20of%20Knowledge-Based%20Vision%20Question%20Answering%0A%20%20Systems%3A%20The%20Lifecycle%20of%20Knowledge%20in%20Visual%20Reasoning%20Task%0AAuthor%3A%20Jiaqi%20Deng%20and%20Zonghan%20Wu%20and%20Huan%20Huo%20and%20Guandong%20Xu%0AAbstract%3A%20%20%20Knowledge-based%20Vision%20Question%20Answering%20%28KB-VQA%29%20extends%20general%20Vision%0AQuestion%20Answering%20%28VQA%29%20by%20not%20only%20requiring%20the%20understanding%20of%20visual%20and%0Atextual%20inputs%20but%20also%20extensive%20range%20of%20knowledge%2C%20enabling%20significant%0Aadvancements%20across%20various%20real-world%20applications.%20KB-VQA%20introduces%20unique%0Achallenges%2C%20including%20the%20alignment%20of%20heterogeneous%20information%20from%20diverse%0Amodalities%20and%20sources%2C%20the%20retrieval%20of%20relevant%20knowledge%20from%20noisy%20or%0Alarge-scale%20repositories%2C%20and%20the%20execution%20of%20complex%20reasoning%20to%20infer%0Aanswers%20from%20the%20combined%20context.%20With%20the%20advancement%20of%20Large%20Language%0AModels%20%28LLMs%29%2C%20KB-VQA%20systems%20have%20also%20undergone%20a%20notable%20transformation%2C%0Awhere%20LLMs%20serve%20as%20powerful%20knowledge%20repositories%2C%20retrieval-augmented%0Agenerators%20and%20strong%20reasoners.%20Despite%20substantial%20progress%2C%20no%20comprehensive%0Asurvey%20currently%20exists%20that%20systematically%20organizes%20and%20reviews%20the%20existing%0AKB-VQA%20methods.%20This%20survey%20aims%20to%20fill%20this%20gap%20by%20establishing%20a%20structured%0Ataxonomy%20of%20KB-VQA%20approaches%2C%20and%20categorizing%20the%20systems%20into%20main%20stages%3A%0Aknowledge%20representation%2C%20knowledge%20retrieval%2C%20and%20knowledge%20reasoning.%20By%0Aexploring%20various%20knowledge%20integration%20techniques%20and%20identifying%20persistent%0Achallenges%2C%20this%20work%20also%20outlines%20promising%20future%20research%20directions%2C%0Aproviding%20a%20foundation%20for%20advancing%20KB-VQA%20models%20and%20their%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17547v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Survey%2520of%2520Knowledge-Based%2520Vision%2520Question%2520Answering%250A%2520%2520Systems%253A%2520The%2520Lifecycle%2520of%2520Knowledge%2520in%2520Visual%2520Reasoning%2520Task%26entry.906535625%3DJiaqi%2520Deng%2520and%2520Zonghan%2520Wu%2520and%2520Huan%2520Huo%2520and%2520Guandong%2520Xu%26entry.1292438233%3D%2520%2520Knowledge-based%2520Vision%2520Question%2520Answering%2520%2528KB-VQA%2529%2520extends%2520general%2520Vision%250AQuestion%2520Answering%2520%2528VQA%2529%2520by%2520not%2520only%2520requiring%2520the%2520understanding%2520of%2520visual%2520and%250Atextual%2520inputs%2520but%2520also%2520extensive%2520range%2520of%2520knowledge%252C%2520enabling%2520significant%250Aadvancements%2520across%2520various%2520real-world%2520applications.%2520KB-VQA%2520introduces%2520unique%250Achallenges%252C%2520including%2520the%2520alignment%2520of%2520heterogeneous%2520information%2520from%2520diverse%250Amodalities%2520and%2520sources%252C%2520the%2520retrieval%2520of%2520relevant%2520knowledge%2520from%2520noisy%2520or%250Alarge-scale%2520repositories%252C%2520and%2520the%2520execution%2520of%2520complex%2520reasoning%2520to%2520infer%250Aanswers%2520from%2520the%2520combined%2520context.%2520With%2520the%2520advancement%2520of%2520Large%2520Language%250AModels%2520%2528LLMs%2529%252C%2520KB-VQA%2520systems%2520have%2520also%2520undergone%2520a%2520notable%2520transformation%252C%250Awhere%2520LLMs%2520serve%2520as%2520powerful%2520knowledge%2520repositories%252C%2520retrieval-augmented%250Agenerators%2520and%2520strong%2520reasoners.%2520Despite%2520substantial%2520progress%252C%2520no%2520comprehensive%250Asurvey%2520currently%2520exists%2520that%2520systematically%2520organizes%2520and%2520reviews%2520the%2520existing%250AKB-VQA%2520methods.%2520This%2520survey%2520aims%2520to%2520fill%2520this%2520gap%2520by%2520establishing%2520a%2520structured%250Ataxonomy%2520of%2520KB-VQA%2520approaches%252C%2520and%2520categorizing%2520the%2520systems%2520into%2520main%2520stages%253A%250Aknowledge%2520representation%252C%2520knowledge%2520retrieval%252C%2520and%2520knowledge%2520reasoning.%2520By%250Aexploring%2520various%2520knowledge%2520integration%2520techniques%2520and%2520identifying%2520persistent%250Achallenges%252C%2520this%2520work%2520also%2520outlines%2520promising%2520future%2520research%2520directions%252C%250Aproviding%2520a%2520foundation%2520for%2520advancing%2520KB-VQA%2520models%2520and%2520their%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17547v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Survey%20of%20Knowledge-Based%20Vision%20Question%20Answering%0A%20%20Systems%3A%20The%20Lifecycle%20of%20Knowledge%20in%20Visual%20Reasoning%20Task&entry.906535625=Jiaqi%20Deng%20and%20Zonghan%20Wu%20and%20Huan%20Huo%20and%20Guandong%20Xu&entry.1292438233=%20%20Knowledge-based%20Vision%20Question%20Answering%20%28KB-VQA%29%20extends%20general%20Vision%0AQuestion%20Answering%20%28VQA%29%20by%20not%20only%20requiring%20the%20understanding%20of%20visual%20and%0Atextual%20inputs%20but%20also%20extensive%20range%20of%20knowledge%2C%20enabling%20significant%0Aadvancements%20across%20various%20real-world%20applications.%20KB-VQA%20introduces%20unique%0Achallenges%2C%20including%20the%20alignment%20of%20heterogeneous%20information%20from%20diverse%0Amodalities%20and%20sources%2C%20the%20retrieval%20of%20relevant%20knowledge%20from%20noisy%20or%0Alarge-scale%20repositories%2C%20and%20the%20execution%20of%20complex%20reasoning%20to%20infer%0Aanswers%20from%20the%20combined%20context.%20With%20the%20advancement%20of%20Large%20Language%0AModels%20%28LLMs%29%2C%20KB-VQA%20systems%20have%20also%20undergone%20a%20notable%20transformation%2C%0Awhere%20LLMs%20serve%20as%20powerful%20knowledge%20repositories%2C%20retrieval-augmented%0Agenerators%20and%20strong%20reasoners.%20Despite%20substantial%20progress%2C%20no%20comprehensive%0Asurvey%20currently%20exists%20that%20systematically%20organizes%20and%20reviews%20the%20existing%0AKB-VQA%20methods.%20This%20survey%20aims%20to%20fill%20this%20gap%20by%20establishing%20a%20structured%0Ataxonomy%20of%20KB-VQA%20approaches%2C%20and%20categorizing%20the%20systems%20into%20main%20stages%3A%0Aknowledge%20representation%2C%20knowledge%20retrieval%2C%20and%20knowledge%20reasoning.%20By%0Aexploring%20various%20knowledge%20integration%20techniques%20and%20identifying%20persistent%0Achallenges%2C%20this%20work%20also%20outlines%20promising%20future%20research%20directions%2C%0Aproviding%20a%20foundation%20for%20advancing%20KB-VQA%20models%20and%20their%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17547v1&entry.124074799=Read"},
{"title": "Exploring How LLMs Capture and Represent Domain-Specific Knowledge", "author": "Mirian Hipolito Garcia and Camille Couturier and Daniel Madrigal Diaz and Ankur Mallick and Anastasios Kyrillidis and Robert Sim and Victor Ruhle and Saravan Rajmohan", "abstract": "  We study whether Large Language Models (LLMs) inherently capture\ndomain-specific nuances in natural language. Our experiments probe the domain\nsensitivity of LLMs by examining their ability to distinguish queries from\ndifferent domains using hidden states generated during the prefill phase. We\nreveal latent domain-related trajectories that indicate the model's internal\nrecognition of query domains. We also study the robustness of these domain\nrepresentations to variations in prompt styles and sources. Our approach\nleverages these representations for model selection, mapping the LLM that best\nmatches the domain trace of the input query (i.e., the model with the highest\nperformance on similar traces). Our findings show that LLMs can differentiate\nqueries for related domains, and that the fine-tuned model is not always the\nmost accurate. Unlike previous work, our interpretations apply to both closed\nand open-ended generative tasks\n", "link": "http://arxiv.org/abs/2504.16871v2", "date": "2025-04-24", "relevancy": 2.5781, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5312}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5312}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20How%20LLMs%20Capture%20and%20Represent%20Domain-Specific%20Knowledge&body=Title%3A%20Exploring%20How%20LLMs%20Capture%20and%20Represent%20Domain-Specific%20Knowledge%0AAuthor%3A%20Mirian%20Hipolito%20Garcia%20and%20Camille%20Couturier%20and%20Daniel%20Madrigal%20Diaz%20and%20Ankur%20Mallick%20and%20Anastasios%20Kyrillidis%20and%20Robert%20Sim%20and%20Victor%20Ruhle%20and%20Saravan%20Rajmohan%0AAbstract%3A%20%20%20We%20study%20whether%20Large%20Language%20Models%20%28LLMs%29%20inherently%20capture%0Adomain-specific%20nuances%20in%20natural%20language.%20Our%20experiments%20probe%20the%20domain%0Asensitivity%20of%20LLMs%20by%20examining%20their%20ability%20to%20distinguish%20queries%20from%0Adifferent%20domains%20using%20hidden%20states%20generated%20during%20the%20prefill%20phase.%20We%0Areveal%20latent%20domain-related%20trajectories%20that%20indicate%20the%20model%27s%20internal%0Arecognition%20of%20query%20domains.%20We%20also%20study%20the%20robustness%20of%20these%20domain%0Arepresentations%20to%20variations%20in%20prompt%20styles%20and%20sources.%20Our%20approach%0Aleverages%20these%20representations%20for%20model%20selection%2C%20mapping%20the%20LLM%20that%20best%0Amatches%20the%20domain%20trace%20of%20the%20input%20query%20%28i.e.%2C%20the%20model%20with%20the%20highest%0Aperformance%20on%20similar%20traces%29.%20Our%20findings%20show%20that%20LLMs%20can%20differentiate%0Aqueries%20for%20related%20domains%2C%20and%20that%20the%20fine-tuned%20model%20is%20not%20always%20the%0Amost%20accurate.%20Unlike%20previous%20work%2C%20our%20interpretations%20apply%20to%20both%20closed%0Aand%20open-ended%20generative%20tasks%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16871v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520How%2520LLMs%2520Capture%2520and%2520Represent%2520Domain-Specific%2520Knowledge%26entry.906535625%3DMirian%2520Hipolito%2520Garcia%2520and%2520Camille%2520Couturier%2520and%2520Daniel%2520Madrigal%2520Diaz%2520and%2520Ankur%2520Mallick%2520and%2520Anastasios%2520Kyrillidis%2520and%2520Robert%2520Sim%2520and%2520Victor%2520Ruhle%2520and%2520Saravan%2520Rajmohan%26entry.1292438233%3D%2520%2520We%2520study%2520whether%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520inherently%2520capture%250Adomain-specific%2520nuances%2520in%2520natural%2520language.%2520Our%2520experiments%2520probe%2520the%2520domain%250Asensitivity%2520of%2520LLMs%2520by%2520examining%2520their%2520ability%2520to%2520distinguish%2520queries%2520from%250Adifferent%2520domains%2520using%2520hidden%2520states%2520generated%2520during%2520the%2520prefill%2520phase.%2520We%250Areveal%2520latent%2520domain-related%2520trajectories%2520that%2520indicate%2520the%2520model%2527s%2520internal%250Arecognition%2520of%2520query%2520domains.%2520We%2520also%2520study%2520the%2520robustness%2520of%2520these%2520domain%250Arepresentations%2520to%2520variations%2520in%2520prompt%2520styles%2520and%2520sources.%2520Our%2520approach%250Aleverages%2520these%2520representations%2520for%2520model%2520selection%252C%2520mapping%2520the%2520LLM%2520that%2520best%250Amatches%2520the%2520domain%2520trace%2520of%2520the%2520input%2520query%2520%2528i.e.%252C%2520the%2520model%2520with%2520the%2520highest%250Aperformance%2520on%2520similar%2520traces%2529.%2520Our%2520findings%2520show%2520that%2520LLMs%2520can%2520differentiate%250Aqueries%2520for%2520related%2520domains%252C%2520and%2520that%2520the%2520fine-tuned%2520model%2520is%2520not%2520always%2520the%250Amost%2520accurate.%2520Unlike%2520previous%2520work%252C%2520our%2520interpretations%2520apply%2520to%2520both%2520closed%250Aand%2520open-ended%2520generative%2520tasks%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16871v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20How%20LLMs%20Capture%20and%20Represent%20Domain-Specific%20Knowledge&entry.906535625=Mirian%20Hipolito%20Garcia%20and%20Camille%20Couturier%20and%20Daniel%20Madrigal%20Diaz%20and%20Ankur%20Mallick%20and%20Anastasios%20Kyrillidis%20and%20Robert%20Sim%20and%20Victor%20Ruhle%20and%20Saravan%20Rajmohan&entry.1292438233=%20%20We%20study%20whether%20Large%20Language%20Models%20%28LLMs%29%20inherently%20capture%0Adomain-specific%20nuances%20in%20natural%20language.%20Our%20experiments%20probe%20the%20domain%0Asensitivity%20of%20LLMs%20by%20examining%20their%20ability%20to%20distinguish%20queries%20from%0Adifferent%20domains%20using%20hidden%20states%20generated%20during%20the%20prefill%20phase.%20We%0Areveal%20latent%20domain-related%20trajectories%20that%20indicate%20the%20model%27s%20internal%0Arecognition%20of%20query%20domains.%20We%20also%20study%20the%20robustness%20of%20these%20domain%0Arepresentations%20to%20variations%20in%20prompt%20styles%20and%20sources.%20Our%20approach%0Aleverages%20these%20representations%20for%20model%20selection%2C%20mapping%20the%20LLM%20that%20best%0Amatches%20the%20domain%20trace%20of%20the%20input%20query%20%28i.e.%2C%20the%20model%20with%20the%20highest%0Aperformance%20on%20similar%20traces%29.%20Our%20findings%20show%20that%20LLMs%20can%20differentiate%0Aqueries%20for%20related%20domains%2C%20and%20that%20the%20fine-tuned%20model%20is%20not%20always%20the%0Amost%20accurate.%20Unlike%20previous%20work%2C%20our%20interpretations%20apply%20to%20both%20closed%0Aand%20open-ended%20generative%20tasks%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16871v2&entry.124074799=Read"},
{"title": "Towards One-Stage End-to-End Table Structure Recognition with Parallel\n  Regression for Diverse Scenarios", "author": "Anyi Xiao and Cihui Yang", "abstract": "  Table structure recognition aims to parse tables in unstructured data into\nmachine-understandable formats. Recent methods address this problem through a\ntwo-stage process or optimized one-stage approaches. However, these methods\neither require multiple networks to be serially trained and perform more\ntime-consuming sequential decoding, or rely on complex post-processing\nalgorithms to parse the logical structure of tables. They struggle to balance\ncross-scenario adaptability, robustness, and computational efficiency. In this\npaper, we propose a one-stage end-to-end table structure parsing network called\nTableCenterNet. This network unifies the prediction of table spatial and\nlogical structure into a parallel regression task for the first time, and\nimplicitly learns the spatial-logical location mapping laws of cells through a\nsynergistic architecture of shared feature extraction layers and task-specific\ndecoding. Compared with two-stage methods, our method is easier to train and\nfaster to infer. Experiments on benchmark datasets show that TableCenterNet can\neffectively parse table structures in diverse scenarios and achieve\nstate-of-the-art performance on the TableGraph-24k dataset. Code is available\nat https://github.com/dreamy-xay/TableCenterNet.\n", "link": "http://arxiv.org/abs/2504.17522v1", "date": "2025-04-24", "relevancy": 2.566, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5146}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5135}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5115}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20One-Stage%20End-to-End%20Table%20Structure%20Recognition%20with%20Parallel%0A%20%20Regression%20for%20Diverse%20Scenarios&body=Title%3A%20Towards%20One-Stage%20End-to-End%20Table%20Structure%20Recognition%20with%20Parallel%0A%20%20Regression%20for%20Diverse%20Scenarios%0AAuthor%3A%20Anyi%20Xiao%20and%20Cihui%20Yang%0AAbstract%3A%20%20%20Table%20structure%20recognition%20aims%20to%20parse%20tables%20in%20unstructured%20data%20into%0Amachine-understandable%20formats.%20Recent%20methods%20address%20this%20problem%20through%20a%0Atwo-stage%20process%20or%20optimized%20one-stage%20approaches.%20However%2C%20these%20methods%0Aeither%20require%20multiple%20networks%20to%20be%20serially%20trained%20and%20perform%20more%0Atime-consuming%20sequential%20decoding%2C%20or%20rely%20on%20complex%20post-processing%0Aalgorithms%20to%20parse%20the%20logical%20structure%20of%20tables.%20They%20struggle%20to%20balance%0Across-scenario%20adaptability%2C%20robustness%2C%20and%20computational%20efficiency.%20In%20this%0Apaper%2C%20we%20propose%20a%20one-stage%20end-to-end%20table%20structure%20parsing%20network%20called%0ATableCenterNet.%20This%20network%20unifies%20the%20prediction%20of%20table%20spatial%20and%0Alogical%20structure%20into%20a%20parallel%20regression%20task%20for%20the%20first%20time%2C%20and%0Aimplicitly%20learns%20the%20spatial-logical%20location%20mapping%20laws%20of%20cells%20through%20a%0Asynergistic%20architecture%20of%20shared%20feature%20extraction%20layers%20and%20task-specific%0Adecoding.%20Compared%20with%20two-stage%20methods%2C%20our%20method%20is%20easier%20to%20train%20and%0Afaster%20to%20infer.%20Experiments%20on%20benchmark%20datasets%20show%20that%20TableCenterNet%20can%0Aeffectively%20parse%20table%20structures%20in%20diverse%20scenarios%20and%20achieve%0Astate-of-the-art%20performance%20on%20the%20TableGraph-24k%20dataset.%20Code%20is%20available%0Aat%20https%3A//github.com/dreamy-xay/TableCenterNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17522v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520One-Stage%2520End-to-End%2520Table%2520Structure%2520Recognition%2520with%2520Parallel%250A%2520%2520Regression%2520for%2520Diverse%2520Scenarios%26entry.906535625%3DAnyi%2520Xiao%2520and%2520Cihui%2520Yang%26entry.1292438233%3D%2520%2520Table%2520structure%2520recognition%2520aims%2520to%2520parse%2520tables%2520in%2520unstructured%2520data%2520into%250Amachine-understandable%2520formats.%2520Recent%2520methods%2520address%2520this%2520problem%2520through%2520a%250Atwo-stage%2520process%2520or%2520optimized%2520one-stage%2520approaches.%2520However%252C%2520these%2520methods%250Aeither%2520require%2520multiple%2520networks%2520to%2520be%2520serially%2520trained%2520and%2520perform%2520more%250Atime-consuming%2520sequential%2520decoding%252C%2520or%2520rely%2520on%2520complex%2520post-processing%250Aalgorithms%2520to%2520parse%2520the%2520logical%2520structure%2520of%2520tables.%2520They%2520struggle%2520to%2520balance%250Across-scenario%2520adaptability%252C%2520robustness%252C%2520and%2520computational%2520efficiency.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520one-stage%2520end-to-end%2520table%2520structure%2520parsing%2520network%2520called%250ATableCenterNet.%2520This%2520network%2520unifies%2520the%2520prediction%2520of%2520table%2520spatial%2520and%250Alogical%2520structure%2520into%2520a%2520parallel%2520regression%2520task%2520for%2520the%2520first%2520time%252C%2520and%250Aimplicitly%2520learns%2520the%2520spatial-logical%2520location%2520mapping%2520laws%2520of%2520cells%2520through%2520a%250Asynergistic%2520architecture%2520of%2520shared%2520feature%2520extraction%2520layers%2520and%2520task-specific%250Adecoding.%2520Compared%2520with%2520two-stage%2520methods%252C%2520our%2520method%2520is%2520easier%2520to%2520train%2520and%250Afaster%2520to%2520infer.%2520Experiments%2520on%2520benchmark%2520datasets%2520show%2520that%2520TableCenterNet%2520can%250Aeffectively%2520parse%2520table%2520structures%2520in%2520diverse%2520scenarios%2520and%2520achieve%250Astate-of-the-art%2520performance%2520on%2520the%2520TableGraph-24k%2520dataset.%2520Code%2520is%2520available%250Aat%2520https%253A//github.com/dreamy-xay/TableCenterNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17522v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20One-Stage%20End-to-End%20Table%20Structure%20Recognition%20with%20Parallel%0A%20%20Regression%20for%20Diverse%20Scenarios&entry.906535625=Anyi%20Xiao%20and%20Cihui%20Yang&entry.1292438233=%20%20Table%20structure%20recognition%20aims%20to%20parse%20tables%20in%20unstructured%20data%20into%0Amachine-understandable%20formats.%20Recent%20methods%20address%20this%20problem%20through%20a%0Atwo-stage%20process%20or%20optimized%20one-stage%20approaches.%20However%2C%20these%20methods%0Aeither%20require%20multiple%20networks%20to%20be%20serially%20trained%20and%20perform%20more%0Atime-consuming%20sequential%20decoding%2C%20or%20rely%20on%20complex%20post-processing%0Aalgorithms%20to%20parse%20the%20logical%20structure%20of%20tables.%20They%20struggle%20to%20balance%0Across-scenario%20adaptability%2C%20robustness%2C%20and%20computational%20efficiency.%20In%20this%0Apaper%2C%20we%20propose%20a%20one-stage%20end-to-end%20table%20structure%20parsing%20network%20called%0ATableCenterNet.%20This%20network%20unifies%20the%20prediction%20of%20table%20spatial%20and%0Alogical%20structure%20into%20a%20parallel%20regression%20task%20for%20the%20first%20time%2C%20and%0Aimplicitly%20learns%20the%20spatial-logical%20location%20mapping%20laws%20of%20cells%20through%20a%0Asynergistic%20architecture%20of%20shared%20feature%20extraction%20layers%20and%20task-specific%0Adecoding.%20Compared%20with%20two-stage%20methods%2C%20our%20method%20is%20easier%20to%20train%20and%0Afaster%20to%20infer.%20Experiments%20on%20benchmark%20datasets%20show%20that%20TableCenterNet%20can%0Aeffectively%20parse%20table%20structures%20in%20diverse%20scenarios%20and%20achieve%0Astate-of-the-art%20performance%20on%20the%20TableGraph-24k%20dataset.%20Code%20is%20available%0Aat%20https%3A//github.com/dreamy-xay/TableCenterNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17522v1&entry.124074799=Read"},
{"title": "Likelihood-Free Variational Autoencoders", "author": "Chen Xu and Qiang Wang and Lijun Sun", "abstract": "  Variational Autoencoders (VAEs) typically rely on a probabilistic decoder\nwith a predefined likelihood, most commonly an isotropic Gaussian, to model the\ndata conditional on latent variables. While convenient for optimization, this\nchoice often leads to likelihood misspecification, resulting in blurry\nreconstructions and poor data fidelity, especially for high-dimensional data\nsuch as images. In this work, we propose \\textit{EnVAE}, a novel\nlikelihood-free generative framework that has a deterministic decoder and\nemploys the energy score -- a proper scoring rule -- to build the\nreconstruction loss. This enables likelihood-free inference without requiring\nexplicit parametric density functions. To address the computational\ninefficiency of the energy score, we introduce a fast variant, \\textit{FEnVAE},\nbased on the local smoothness of the decoder and the sharpness of the posterior\ndistribution of latent variables. This yields an efficient single-sample\ntraining objective that integrates seamlessly into existing VAE pipelines with\nminimal overhead. Empirical results on standard benchmarks demonstrate that\n\\textit{EnVAE} achieves superior reconstruction and generation quality compared\nto likelihood-based baselines. Our framework offers a general, scalable, and\nstatistically principled alternative for flexible and nonparametric\ndistribution learning in generative modeling.\n", "link": "http://arxiv.org/abs/2504.17622v1", "date": "2025-04-24", "relevancy": 2.5587, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5398}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5125}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4828}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Likelihood-Free%20Variational%20Autoencoders&body=Title%3A%20Likelihood-Free%20Variational%20Autoencoders%0AAuthor%3A%20Chen%20Xu%20and%20Qiang%20Wang%20and%20Lijun%20Sun%0AAbstract%3A%20%20%20Variational%20Autoencoders%20%28VAEs%29%20typically%20rely%20on%20a%20probabilistic%20decoder%0Awith%20a%20predefined%20likelihood%2C%20most%20commonly%20an%20isotropic%20Gaussian%2C%20to%20model%20the%0Adata%20conditional%20on%20latent%20variables.%20While%20convenient%20for%20optimization%2C%20this%0Achoice%20often%20leads%20to%20likelihood%20misspecification%2C%20resulting%20in%20blurry%0Areconstructions%20and%20poor%20data%20fidelity%2C%20especially%20for%20high-dimensional%20data%0Asuch%20as%20images.%20In%20this%20work%2C%20we%20propose%20%5Ctextit%7BEnVAE%7D%2C%20a%20novel%0Alikelihood-free%20generative%20framework%20that%20has%20a%20deterministic%20decoder%20and%0Aemploys%20the%20energy%20score%20--%20a%20proper%20scoring%20rule%20--%20to%20build%20the%0Areconstruction%20loss.%20This%20enables%20likelihood-free%20inference%20without%20requiring%0Aexplicit%20parametric%20density%20functions.%20To%20address%20the%20computational%0Ainefficiency%20of%20the%20energy%20score%2C%20we%20introduce%20a%20fast%20variant%2C%20%5Ctextit%7BFEnVAE%7D%2C%0Abased%20on%20the%20local%20smoothness%20of%20the%20decoder%20and%20the%20sharpness%20of%20the%20posterior%0Adistribution%20of%20latent%20variables.%20This%20yields%20an%20efficient%20single-sample%0Atraining%20objective%20that%20integrates%20seamlessly%20into%20existing%20VAE%20pipelines%20with%0Aminimal%20overhead.%20Empirical%20results%20on%20standard%20benchmarks%20demonstrate%20that%0A%5Ctextit%7BEnVAE%7D%20achieves%20superior%20reconstruction%20and%20generation%20quality%20compared%0Ato%20likelihood-based%20baselines.%20Our%20framework%20offers%20a%20general%2C%20scalable%2C%20and%0Astatistically%20principled%20alternative%20for%20flexible%20and%20nonparametric%0Adistribution%20learning%20in%20generative%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17622v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLikelihood-Free%2520Variational%2520Autoencoders%26entry.906535625%3DChen%2520Xu%2520and%2520Qiang%2520Wang%2520and%2520Lijun%2520Sun%26entry.1292438233%3D%2520%2520Variational%2520Autoencoders%2520%2528VAEs%2529%2520typically%2520rely%2520on%2520a%2520probabilistic%2520decoder%250Awith%2520a%2520predefined%2520likelihood%252C%2520most%2520commonly%2520an%2520isotropic%2520Gaussian%252C%2520to%2520model%2520the%250Adata%2520conditional%2520on%2520latent%2520variables.%2520While%2520convenient%2520for%2520optimization%252C%2520this%250Achoice%2520often%2520leads%2520to%2520likelihood%2520misspecification%252C%2520resulting%2520in%2520blurry%250Areconstructions%2520and%2520poor%2520data%2520fidelity%252C%2520especially%2520for%2520high-dimensional%2520data%250Asuch%2520as%2520images.%2520In%2520this%2520work%252C%2520we%2520propose%2520%255Ctextit%257BEnVAE%257D%252C%2520a%2520novel%250Alikelihood-free%2520generative%2520framework%2520that%2520has%2520a%2520deterministic%2520decoder%2520and%250Aemploys%2520the%2520energy%2520score%2520--%2520a%2520proper%2520scoring%2520rule%2520--%2520to%2520build%2520the%250Areconstruction%2520loss.%2520This%2520enables%2520likelihood-free%2520inference%2520without%2520requiring%250Aexplicit%2520parametric%2520density%2520functions.%2520To%2520address%2520the%2520computational%250Ainefficiency%2520of%2520the%2520energy%2520score%252C%2520we%2520introduce%2520a%2520fast%2520variant%252C%2520%255Ctextit%257BFEnVAE%257D%252C%250Abased%2520on%2520the%2520local%2520smoothness%2520of%2520the%2520decoder%2520and%2520the%2520sharpness%2520of%2520the%2520posterior%250Adistribution%2520of%2520latent%2520variables.%2520This%2520yields%2520an%2520efficient%2520single-sample%250Atraining%2520objective%2520that%2520integrates%2520seamlessly%2520into%2520existing%2520VAE%2520pipelines%2520with%250Aminimal%2520overhead.%2520Empirical%2520results%2520on%2520standard%2520benchmarks%2520demonstrate%2520that%250A%255Ctextit%257BEnVAE%257D%2520achieves%2520superior%2520reconstruction%2520and%2520generation%2520quality%2520compared%250Ato%2520likelihood-based%2520baselines.%2520Our%2520framework%2520offers%2520a%2520general%252C%2520scalable%252C%2520and%250Astatistically%2520principled%2520alternative%2520for%2520flexible%2520and%2520nonparametric%250Adistribution%2520learning%2520in%2520generative%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17622v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Likelihood-Free%20Variational%20Autoencoders&entry.906535625=Chen%20Xu%20and%20Qiang%20Wang%20and%20Lijun%20Sun&entry.1292438233=%20%20Variational%20Autoencoders%20%28VAEs%29%20typically%20rely%20on%20a%20probabilistic%20decoder%0Awith%20a%20predefined%20likelihood%2C%20most%20commonly%20an%20isotropic%20Gaussian%2C%20to%20model%20the%0Adata%20conditional%20on%20latent%20variables.%20While%20convenient%20for%20optimization%2C%20this%0Achoice%20often%20leads%20to%20likelihood%20misspecification%2C%20resulting%20in%20blurry%0Areconstructions%20and%20poor%20data%20fidelity%2C%20especially%20for%20high-dimensional%20data%0Asuch%20as%20images.%20In%20this%20work%2C%20we%20propose%20%5Ctextit%7BEnVAE%7D%2C%20a%20novel%0Alikelihood-free%20generative%20framework%20that%20has%20a%20deterministic%20decoder%20and%0Aemploys%20the%20energy%20score%20--%20a%20proper%20scoring%20rule%20--%20to%20build%20the%0Areconstruction%20loss.%20This%20enables%20likelihood-free%20inference%20without%20requiring%0Aexplicit%20parametric%20density%20functions.%20To%20address%20the%20computational%0Ainefficiency%20of%20the%20energy%20score%2C%20we%20introduce%20a%20fast%20variant%2C%20%5Ctextit%7BFEnVAE%7D%2C%0Abased%20on%20the%20local%20smoothness%20of%20the%20decoder%20and%20the%20sharpness%20of%20the%20posterior%0Adistribution%20of%20latent%20variables.%20This%20yields%20an%20efficient%20single-sample%0Atraining%20objective%20that%20integrates%20seamlessly%20into%20existing%20VAE%20pipelines%20with%0Aminimal%20overhead.%20Empirical%20results%20on%20standard%20benchmarks%20demonstrate%20that%0A%5Ctextit%7BEnVAE%7D%20achieves%20superior%20reconstruction%20and%20generation%20quality%20compared%0Ato%20likelihood-based%20baselines.%20Our%20framework%20offers%20a%20general%2C%20scalable%2C%20and%0Astatistically%20principled%20alternative%20for%20flexible%20and%20nonparametric%0Adistribution%20learning%20in%20generative%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17622v1&entry.124074799=Read"},
{"title": "PhysFlow: Unleashing the Potential of Multi-modal Foundation Models and\n  Video Diffusion for 4D Dynamic Physical Scene Simulation", "author": "Zhuoman Liu and Weicai Ye and Yan Luximon and Pengfei Wan and Di Zhang", "abstract": "  Realistic simulation of dynamic scenes requires accurately capturing diverse\nmaterial properties and modeling complex object interactions grounded in\nphysical principles. However, existing methods are constrained to basic\nmaterial types with limited predictable parameters, making them insufficient to\nrepresent the complexity of real-world materials. We introduce PhysFlow, a\nnovel approach that leverages multi-modal foundation models and video diffusion\nto achieve enhanced 4D dynamic scene simulation. Our method utilizes\nmulti-modal models to identify material types and initialize material\nparameters through image queries, while simultaneously inferring 3D Gaussian\nsplats for detailed scene representation. We further refine these material\nparameters using video diffusion with a differentiable Material Point Method\n(MPM) and optical flow guidance rather than render loss or Score Distillation\nSampling (SDS) loss. This integrated framework enables accurate prediction and\nrealistic simulation of dynamic interactions in real-world scenarios, advancing\nboth accuracy and flexibility in physics-based simulations.\n", "link": "http://arxiv.org/abs/2411.14423v3", "date": "2025-04-24", "relevancy": 2.5175, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6382}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6357}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5914}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhysFlow%3A%20Unleashing%20the%20Potential%20of%20Multi-modal%20Foundation%20Models%20and%0A%20%20Video%20Diffusion%20for%204D%20Dynamic%20Physical%20Scene%20Simulation&body=Title%3A%20PhysFlow%3A%20Unleashing%20the%20Potential%20of%20Multi-modal%20Foundation%20Models%20and%0A%20%20Video%20Diffusion%20for%204D%20Dynamic%20Physical%20Scene%20Simulation%0AAuthor%3A%20Zhuoman%20Liu%20and%20Weicai%20Ye%20and%20Yan%20Luximon%20and%20Pengfei%20Wan%20and%20Di%20Zhang%0AAbstract%3A%20%20%20Realistic%20simulation%20of%20dynamic%20scenes%20requires%20accurately%20capturing%20diverse%0Amaterial%20properties%20and%20modeling%20complex%20object%20interactions%20grounded%20in%0Aphysical%20principles.%20However%2C%20existing%20methods%20are%20constrained%20to%20basic%0Amaterial%20types%20with%20limited%20predictable%20parameters%2C%20making%20them%20insufficient%20to%0Arepresent%20the%20complexity%20of%20real-world%20materials.%20We%20introduce%20PhysFlow%2C%20a%0Anovel%20approach%20that%20leverages%20multi-modal%20foundation%20models%20and%20video%20diffusion%0Ato%20achieve%20enhanced%204D%20dynamic%20scene%20simulation.%20Our%20method%20utilizes%0Amulti-modal%20models%20to%20identify%20material%20types%20and%20initialize%20material%0Aparameters%20through%20image%20queries%2C%20while%20simultaneously%20inferring%203D%20Gaussian%0Asplats%20for%20detailed%20scene%20representation.%20We%20further%20refine%20these%20material%0Aparameters%20using%20video%20diffusion%20with%20a%20differentiable%20Material%20Point%20Method%0A%28MPM%29%20and%20optical%20flow%20guidance%20rather%20than%20render%20loss%20or%20Score%20Distillation%0ASampling%20%28SDS%29%20loss.%20This%20integrated%20framework%20enables%20accurate%20prediction%20and%0Arealistic%20simulation%20of%20dynamic%20interactions%20in%20real-world%20scenarios%2C%20advancing%0Aboth%20accuracy%20and%20flexibility%20in%20physics-based%20simulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14423v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysFlow%253A%2520Unleashing%2520the%2520Potential%2520of%2520Multi-modal%2520Foundation%2520Models%2520and%250A%2520%2520Video%2520Diffusion%2520for%25204D%2520Dynamic%2520Physical%2520Scene%2520Simulation%26entry.906535625%3DZhuoman%2520Liu%2520and%2520Weicai%2520Ye%2520and%2520Yan%2520Luximon%2520and%2520Pengfei%2520Wan%2520and%2520Di%2520Zhang%26entry.1292438233%3D%2520%2520Realistic%2520simulation%2520of%2520dynamic%2520scenes%2520requires%2520accurately%2520capturing%2520diverse%250Amaterial%2520properties%2520and%2520modeling%2520complex%2520object%2520interactions%2520grounded%2520in%250Aphysical%2520principles.%2520However%252C%2520existing%2520methods%2520are%2520constrained%2520to%2520basic%250Amaterial%2520types%2520with%2520limited%2520predictable%2520parameters%252C%2520making%2520them%2520insufficient%2520to%250Arepresent%2520the%2520complexity%2520of%2520real-world%2520materials.%2520We%2520introduce%2520PhysFlow%252C%2520a%250Anovel%2520approach%2520that%2520leverages%2520multi-modal%2520foundation%2520models%2520and%2520video%2520diffusion%250Ato%2520achieve%2520enhanced%25204D%2520dynamic%2520scene%2520simulation.%2520Our%2520method%2520utilizes%250Amulti-modal%2520models%2520to%2520identify%2520material%2520types%2520and%2520initialize%2520material%250Aparameters%2520through%2520image%2520queries%252C%2520while%2520simultaneously%2520inferring%25203D%2520Gaussian%250Asplats%2520for%2520detailed%2520scene%2520representation.%2520We%2520further%2520refine%2520these%2520material%250Aparameters%2520using%2520video%2520diffusion%2520with%2520a%2520differentiable%2520Material%2520Point%2520Method%250A%2528MPM%2529%2520and%2520optical%2520flow%2520guidance%2520rather%2520than%2520render%2520loss%2520or%2520Score%2520Distillation%250ASampling%2520%2528SDS%2529%2520loss.%2520This%2520integrated%2520framework%2520enables%2520accurate%2520prediction%2520and%250Arealistic%2520simulation%2520of%2520dynamic%2520interactions%2520in%2520real-world%2520scenarios%252C%2520advancing%250Aboth%2520accuracy%2520and%2520flexibility%2520in%2520physics-based%2520simulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14423v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhysFlow%3A%20Unleashing%20the%20Potential%20of%20Multi-modal%20Foundation%20Models%20and%0A%20%20Video%20Diffusion%20for%204D%20Dynamic%20Physical%20Scene%20Simulation&entry.906535625=Zhuoman%20Liu%20and%20Weicai%20Ye%20and%20Yan%20Luximon%20and%20Pengfei%20Wan%20and%20Di%20Zhang&entry.1292438233=%20%20Realistic%20simulation%20of%20dynamic%20scenes%20requires%20accurately%20capturing%20diverse%0Amaterial%20properties%20and%20modeling%20complex%20object%20interactions%20grounded%20in%0Aphysical%20principles.%20However%2C%20existing%20methods%20are%20constrained%20to%20basic%0Amaterial%20types%20with%20limited%20predictable%20parameters%2C%20making%20them%20insufficient%20to%0Arepresent%20the%20complexity%20of%20real-world%20materials.%20We%20introduce%20PhysFlow%2C%20a%0Anovel%20approach%20that%20leverages%20multi-modal%20foundation%20models%20and%20video%20diffusion%0Ato%20achieve%20enhanced%204D%20dynamic%20scene%20simulation.%20Our%20method%20utilizes%0Amulti-modal%20models%20to%20identify%20material%20types%20and%20initialize%20material%0Aparameters%20through%20image%20queries%2C%20while%20simultaneously%20inferring%203D%20Gaussian%0Asplats%20for%20detailed%20scene%20representation.%20We%20further%20refine%20these%20material%0Aparameters%20using%20video%20diffusion%20with%20a%20differentiable%20Material%20Point%20Method%0A%28MPM%29%20and%20optical%20flow%20guidance%20rather%20than%20render%20loss%20or%20Score%20Distillation%0ASampling%20%28SDS%29%20loss.%20This%20integrated%20framework%20enables%20accurate%20prediction%20and%0Arealistic%20simulation%20of%20dynamic%20interactions%20in%20real-world%20scenarios%2C%20advancing%0Aboth%20accuracy%20and%20flexibility%20in%20physics-based%20simulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14423v3&entry.124074799=Read"},
{"title": "Auditing the Ethical Logic of Generative AI Models", "author": "W. Russell Neuman and Chad Coleman and Ali Dasdan and Safinah Ali and Manan Shah", "abstract": "  As generative AI models become increasingly integrated into high-stakes\ndomains, the need for robust methods to evaluate their ethical reasoning\nbecomes increasingly important. This paper introduces a five-dimensional audit\nmodel -- assessing Analytic Quality, Breadth of Ethical Considerations, Depth\nof Explanation, Consistency, and Decisiveness -- to evaluate the ethical logic\nof leading large language models (LLMs). Drawing on traditions from applied\nethics and higher-order thinking, we present a multi-battery prompt approach,\nincluding novel ethical dilemmas, to probe the models' reasoning across diverse\ncontexts. We benchmark seven major LLMs finding that while models generally\nconverge on ethical decisions, they vary in explanatory rigor and moral\nprioritization. Chain-of-Thought prompting and reasoning-optimized models\nsignificantly enhance performance on our audit metrics. This study introduces a\nscalable methodology for ethical benchmarking of AI systems and highlights the\npotential for AI to complement human moral reasoning in complex decision-making\ncontexts.\n", "link": "http://arxiv.org/abs/2504.17544v1", "date": "2025-04-24", "relevancy": 2.5121, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5102}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4985}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Auditing%20the%20Ethical%20Logic%20of%20Generative%20AI%20Models&body=Title%3A%20Auditing%20the%20Ethical%20Logic%20of%20Generative%20AI%20Models%0AAuthor%3A%20W.%20Russell%20Neuman%20and%20Chad%20Coleman%20and%20Ali%20Dasdan%20and%20Safinah%20Ali%20and%20Manan%20Shah%0AAbstract%3A%20%20%20As%20generative%20AI%20models%20become%20increasingly%20integrated%20into%20high-stakes%0Adomains%2C%20the%20need%20for%20robust%20methods%20to%20evaluate%20their%20ethical%20reasoning%0Abecomes%20increasingly%20important.%20This%20paper%20introduces%20a%20five-dimensional%20audit%0Amodel%20--%20assessing%20Analytic%20Quality%2C%20Breadth%20of%20Ethical%20Considerations%2C%20Depth%0Aof%20Explanation%2C%20Consistency%2C%20and%20Decisiveness%20--%20to%20evaluate%20the%20ethical%20logic%0Aof%20leading%20large%20language%20models%20%28LLMs%29.%20Drawing%20on%20traditions%20from%20applied%0Aethics%20and%20higher-order%20thinking%2C%20we%20present%20a%20multi-battery%20prompt%20approach%2C%0Aincluding%20novel%20ethical%20dilemmas%2C%20to%20probe%20the%20models%27%20reasoning%20across%20diverse%0Acontexts.%20We%20benchmark%20seven%20major%20LLMs%20finding%20that%20while%20models%20generally%0Aconverge%20on%20ethical%20decisions%2C%20they%20vary%20in%20explanatory%20rigor%20and%20moral%0Aprioritization.%20Chain-of-Thought%20prompting%20and%20reasoning-optimized%20models%0Asignificantly%20enhance%20performance%20on%20our%20audit%20metrics.%20This%20study%20introduces%20a%0Ascalable%20methodology%20for%20ethical%20benchmarking%20of%20AI%20systems%20and%20highlights%20the%0Apotential%20for%20AI%20to%20complement%20human%20moral%20reasoning%20in%20complex%20decision-making%0Acontexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17544v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuditing%2520the%2520Ethical%2520Logic%2520of%2520Generative%2520AI%2520Models%26entry.906535625%3DW.%2520Russell%2520Neuman%2520and%2520Chad%2520Coleman%2520and%2520Ali%2520Dasdan%2520and%2520Safinah%2520Ali%2520and%2520Manan%2520Shah%26entry.1292438233%3D%2520%2520As%2520generative%2520AI%2520models%2520become%2520increasingly%2520integrated%2520into%2520high-stakes%250Adomains%252C%2520the%2520need%2520for%2520robust%2520methods%2520to%2520evaluate%2520their%2520ethical%2520reasoning%250Abecomes%2520increasingly%2520important.%2520This%2520paper%2520introduces%2520a%2520five-dimensional%2520audit%250Amodel%2520--%2520assessing%2520Analytic%2520Quality%252C%2520Breadth%2520of%2520Ethical%2520Considerations%252C%2520Depth%250Aof%2520Explanation%252C%2520Consistency%252C%2520and%2520Decisiveness%2520--%2520to%2520evaluate%2520the%2520ethical%2520logic%250Aof%2520leading%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Drawing%2520on%2520traditions%2520from%2520applied%250Aethics%2520and%2520higher-order%2520thinking%252C%2520we%2520present%2520a%2520multi-battery%2520prompt%2520approach%252C%250Aincluding%2520novel%2520ethical%2520dilemmas%252C%2520to%2520probe%2520the%2520models%2527%2520reasoning%2520across%2520diverse%250Acontexts.%2520We%2520benchmark%2520seven%2520major%2520LLMs%2520finding%2520that%2520while%2520models%2520generally%250Aconverge%2520on%2520ethical%2520decisions%252C%2520they%2520vary%2520in%2520explanatory%2520rigor%2520and%2520moral%250Aprioritization.%2520Chain-of-Thought%2520prompting%2520and%2520reasoning-optimized%2520models%250Asignificantly%2520enhance%2520performance%2520on%2520our%2520audit%2520metrics.%2520This%2520study%2520introduces%2520a%250Ascalable%2520methodology%2520for%2520ethical%2520benchmarking%2520of%2520AI%2520systems%2520and%2520highlights%2520the%250Apotential%2520for%2520AI%2520to%2520complement%2520human%2520moral%2520reasoning%2520in%2520complex%2520decision-making%250Acontexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17544v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Auditing%20the%20Ethical%20Logic%20of%20Generative%20AI%20Models&entry.906535625=W.%20Russell%20Neuman%20and%20Chad%20Coleman%20and%20Ali%20Dasdan%20and%20Safinah%20Ali%20and%20Manan%20Shah&entry.1292438233=%20%20As%20generative%20AI%20models%20become%20increasingly%20integrated%20into%20high-stakes%0Adomains%2C%20the%20need%20for%20robust%20methods%20to%20evaluate%20their%20ethical%20reasoning%0Abecomes%20increasingly%20important.%20This%20paper%20introduces%20a%20five-dimensional%20audit%0Amodel%20--%20assessing%20Analytic%20Quality%2C%20Breadth%20of%20Ethical%20Considerations%2C%20Depth%0Aof%20Explanation%2C%20Consistency%2C%20and%20Decisiveness%20--%20to%20evaluate%20the%20ethical%20logic%0Aof%20leading%20large%20language%20models%20%28LLMs%29.%20Drawing%20on%20traditions%20from%20applied%0Aethics%20and%20higher-order%20thinking%2C%20we%20present%20a%20multi-battery%20prompt%20approach%2C%0Aincluding%20novel%20ethical%20dilemmas%2C%20to%20probe%20the%20models%27%20reasoning%20across%20diverse%0Acontexts.%20We%20benchmark%20seven%20major%20LLMs%20finding%20that%20while%20models%20generally%0Aconverge%20on%20ethical%20decisions%2C%20they%20vary%20in%20explanatory%20rigor%20and%20moral%0Aprioritization.%20Chain-of-Thought%20prompting%20and%20reasoning-optimized%20models%0Asignificantly%20enhance%20performance%20on%20our%20audit%20metrics.%20This%20study%20introduces%20a%0Ascalable%20methodology%20for%20ethical%20benchmarking%20of%20AI%20systems%20and%20highlights%20the%0Apotential%20for%20AI%20to%20complement%20human%20moral%20reasoning%20in%20complex%20decision-making%0Acontexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17544v1&entry.124074799=Read"},
{"title": "Combining GCN Structural Learning with LLM Chemical Knowledge for or\n  Enhanced Virtual Screening", "author": "Radia Berreziga and Mohammed Brahimi and Khairedine Kraim and Hamid Azzoune", "abstract": "  Virtual screening plays a critical role in modern drug discovery by enabling\nthe identification of promising candidate molecules for experimental\nvalidation. Traditional machine learning methods such as support vector\nmachines (SVM) and XGBoost rely on predefined molecular representations, often\nleading to information loss and potential bias. In contrast, deep learning\napproaches-particularly Graph Convolutional Networks (GCNs)-offer a more\nexpressive and unbiased alternative by operating directly on molecular graphs.\nMeanwhile, Large Language Models (LLMs) have recently demonstrated\nstate-of-the-art performance in drug design, thanks to their capacity to\ncapture complex chemical patterns from large-scale data via attention\nmechanisms.\n  In this paper, we propose a hybrid architecture that integrates GCNs with\nLLM-derived embeddings to combine localized structural learning with global\nchemical knowledge. The LLM embeddings can be precomputed and stored in a\nmolecular feature library, removing the need to rerun the LLM during training\nor inference and thus maintaining computational efficiency. We found that\nconcatenating the LLM embeddings after each GCN layer-rather than only at the\nfinal layer-significantly improves performance, enabling deeper integration of\nglobal context throughout the network. The resulting model achieves superior\nresults, with an F1-score of (88.8%), outperforming standalone GCN (87.9%),\nXGBoost (85.5%), and SVM (85.4%) baselines.\n", "link": "http://arxiv.org/abs/2504.17497v1", "date": "2025-04-24", "relevancy": 2.4847, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4973}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4973}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Combining%20GCN%20Structural%20Learning%20with%20LLM%20Chemical%20Knowledge%20for%20or%0A%20%20Enhanced%20Virtual%20Screening&body=Title%3A%20Combining%20GCN%20Structural%20Learning%20with%20LLM%20Chemical%20Knowledge%20for%20or%0A%20%20Enhanced%20Virtual%20Screening%0AAuthor%3A%20Radia%20Berreziga%20and%20Mohammed%20Brahimi%20and%20Khairedine%20Kraim%20and%20Hamid%20Azzoune%0AAbstract%3A%20%20%20Virtual%20screening%20plays%20a%20critical%20role%20in%20modern%20drug%20discovery%20by%20enabling%0Athe%20identification%20of%20promising%20candidate%20molecules%20for%20experimental%0Avalidation.%20Traditional%20machine%20learning%20methods%20such%20as%20support%20vector%0Amachines%20%28SVM%29%20and%20XGBoost%20rely%20on%20predefined%20molecular%20representations%2C%20often%0Aleading%20to%20information%20loss%20and%20potential%20bias.%20In%20contrast%2C%20deep%20learning%0Aapproaches-particularly%20Graph%20Convolutional%20Networks%20%28GCNs%29-offer%20a%20more%0Aexpressive%20and%20unbiased%20alternative%20by%20operating%20directly%20on%20molecular%20graphs.%0AMeanwhile%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20recently%20demonstrated%0Astate-of-the-art%20performance%20in%20drug%20design%2C%20thanks%20to%20their%20capacity%20to%0Acapture%20complex%20chemical%20patterns%20from%20large-scale%20data%20via%20attention%0Amechanisms.%0A%20%20In%20this%20paper%2C%20we%20propose%20a%20hybrid%20architecture%20that%20integrates%20GCNs%20with%0ALLM-derived%20embeddings%20to%20combine%20localized%20structural%20learning%20with%20global%0Achemical%20knowledge.%20The%20LLM%20embeddings%20can%20be%20precomputed%20and%20stored%20in%20a%0Amolecular%20feature%20library%2C%20removing%20the%20need%20to%20rerun%20the%20LLM%20during%20training%0Aor%20inference%20and%20thus%20maintaining%20computational%20efficiency.%20We%20found%20that%0Aconcatenating%20the%20LLM%20embeddings%20after%20each%20GCN%20layer-rather%20than%20only%20at%20the%0Afinal%20layer-significantly%20improves%20performance%2C%20enabling%20deeper%20integration%20of%0Aglobal%20context%20throughout%20the%20network.%20The%20resulting%20model%20achieves%20superior%0Aresults%2C%20with%20an%20F1-score%20of%20%2888.8%25%29%2C%20outperforming%20standalone%20GCN%20%2887.9%25%29%2C%0AXGBoost%20%2885.5%25%29%2C%20and%20SVM%20%2885.4%25%29%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17497v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCombining%2520GCN%2520Structural%2520Learning%2520with%2520LLM%2520Chemical%2520Knowledge%2520for%2520or%250A%2520%2520Enhanced%2520Virtual%2520Screening%26entry.906535625%3DRadia%2520Berreziga%2520and%2520Mohammed%2520Brahimi%2520and%2520Khairedine%2520Kraim%2520and%2520Hamid%2520Azzoune%26entry.1292438233%3D%2520%2520Virtual%2520screening%2520plays%2520a%2520critical%2520role%2520in%2520modern%2520drug%2520discovery%2520by%2520enabling%250Athe%2520identification%2520of%2520promising%2520candidate%2520molecules%2520for%2520experimental%250Avalidation.%2520Traditional%2520machine%2520learning%2520methods%2520such%2520as%2520support%2520vector%250Amachines%2520%2528SVM%2529%2520and%2520XGBoost%2520rely%2520on%2520predefined%2520molecular%2520representations%252C%2520often%250Aleading%2520to%2520information%2520loss%2520and%2520potential%2520bias.%2520In%2520contrast%252C%2520deep%2520learning%250Aapproaches-particularly%2520Graph%2520Convolutional%2520Networks%2520%2528GCNs%2529-offer%2520a%2520more%250Aexpressive%2520and%2520unbiased%2520alternative%2520by%2520operating%2520directly%2520on%2520molecular%2520graphs.%250AMeanwhile%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520recently%2520demonstrated%250Astate-of-the-art%2520performance%2520in%2520drug%2520design%252C%2520thanks%2520to%2520their%2520capacity%2520to%250Acapture%2520complex%2520chemical%2520patterns%2520from%2520large-scale%2520data%2520via%2520attention%250Amechanisms.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520hybrid%2520architecture%2520that%2520integrates%2520GCNs%2520with%250ALLM-derived%2520embeddings%2520to%2520combine%2520localized%2520structural%2520learning%2520with%2520global%250Achemical%2520knowledge.%2520The%2520LLM%2520embeddings%2520can%2520be%2520precomputed%2520and%2520stored%2520in%2520a%250Amolecular%2520feature%2520library%252C%2520removing%2520the%2520need%2520to%2520rerun%2520the%2520LLM%2520during%2520training%250Aor%2520inference%2520and%2520thus%2520maintaining%2520computational%2520efficiency.%2520We%2520found%2520that%250Aconcatenating%2520the%2520LLM%2520embeddings%2520after%2520each%2520GCN%2520layer-rather%2520than%2520only%2520at%2520the%250Afinal%2520layer-significantly%2520improves%2520performance%252C%2520enabling%2520deeper%2520integration%2520of%250Aglobal%2520context%2520throughout%2520the%2520network.%2520The%2520resulting%2520model%2520achieves%2520superior%250Aresults%252C%2520with%2520an%2520F1-score%2520of%2520%252888.8%2525%2529%252C%2520outperforming%2520standalone%2520GCN%2520%252887.9%2525%2529%252C%250AXGBoost%2520%252885.5%2525%2529%252C%2520and%2520SVM%2520%252885.4%2525%2529%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17497v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Combining%20GCN%20Structural%20Learning%20with%20LLM%20Chemical%20Knowledge%20for%20or%0A%20%20Enhanced%20Virtual%20Screening&entry.906535625=Radia%20Berreziga%20and%20Mohammed%20Brahimi%20and%20Khairedine%20Kraim%20and%20Hamid%20Azzoune&entry.1292438233=%20%20Virtual%20screening%20plays%20a%20critical%20role%20in%20modern%20drug%20discovery%20by%20enabling%0Athe%20identification%20of%20promising%20candidate%20molecules%20for%20experimental%0Avalidation.%20Traditional%20machine%20learning%20methods%20such%20as%20support%20vector%0Amachines%20%28SVM%29%20and%20XGBoost%20rely%20on%20predefined%20molecular%20representations%2C%20often%0Aleading%20to%20information%20loss%20and%20potential%20bias.%20In%20contrast%2C%20deep%20learning%0Aapproaches-particularly%20Graph%20Convolutional%20Networks%20%28GCNs%29-offer%20a%20more%0Aexpressive%20and%20unbiased%20alternative%20by%20operating%20directly%20on%20molecular%20graphs.%0AMeanwhile%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20recently%20demonstrated%0Astate-of-the-art%20performance%20in%20drug%20design%2C%20thanks%20to%20their%20capacity%20to%0Acapture%20complex%20chemical%20patterns%20from%20large-scale%20data%20via%20attention%0Amechanisms.%0A%20%20In%20this%20paper%2C%20we%20propose%20a%20hybrid%20architecture%20that%20integrates%20GCNs%20with%0ALLM-derived%20embeddings%20to%20combine%20localized%20structural%20learning%20with%20global%0Achemical%20knowledge.%20The%20LLM%20embeddings%20can%20be%20precomputed%20and%20stored%20in%20a%0Amolecular%20feature%20library%2C%20removing%20the%20need%20to%20rerun%20the%20LLM%20during%20training%0Aor%20inference%20and%20thus%20maintaining%20computational%20efficiency.%20We%20found%20that%0Aconcatenating%20the%20LLM%20embeddings%20after%20each%20GCN%20layer-rather%20than%20only%20at%20the%0Afinal%20layer-significantly%20improves%20performance%2C%20enabling%20deeper%20integration%20of%0Aglobal%20context%20throughout%20the%20network.%20The%20resulting%20model%20achieves%20superior%0Aresults%2C%20with%20an%20F1-score%20of%20%2888.8%25%29%2C%20outperforming%20standalone%20GCN%20%2887.9%25%29%2C%0AXGBoost%20%2885.5%25%29%2C%20and%20SVM%20%2885.4%25%29%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17497v1&entry.124074799=Read"},
{"title": "TACO: Tackling Over-correction in Federated Learning with Tailored\n  Adaptive Correction", "author": "Weijie Liu and Ziwei Zhan and Carlee Joe-Wong and Edith Ngai and Jingpu Duan and Deke Guo and Xu Chen and Xiaoxi Zhang", "abstract": "  Non-independent and identically distributed (Non-IID) data across edge\nclients have long posed significant challenges to federated learning (FL)\ntraining in edge computing environments. Prior works have proposed various\nmethods to mitigate this statistical heterogeneity. While these works can\nachieve good theoretical performance, in this work we provide the first\ninvestigation into a hidden over-correction phenomenon brought by the uniform\nmodel correction coefficients across clients adopted by existing methods. Such\nover-correction could degrade model performance and even cause failures in\nmodel convergence. To address this, we propose TACO, a novel algorithm that\naddresses the non-IID nature of clients' data by implementing fine-grained,\nclient-specific gradient correction and model aggregation, steering local\nmodels towards a more accurate global optimum. Moreover, we verify that leading\nFL algorithms generally have better model accuracy in terms of communication\nrounds rather than wall-clock time, resulting from their extra computation\noverhead imposed on clients. To enhance the training efficiency, TACO deploys a\nlightweight model correction and tailored aggregation approach that requires\nminimum computation overhead and no extra information beyond the synchronized\nmodel parameters. To validate TACO's effectiveness, we present the first FL\nconvergence analysis that reveals the root cause of over-correction. Extensive\nexperiments across various datasets confirm TACO's superior and stable\nperformance in practice.\n", "link": "http://arxiv.org/abs/2504.17528v1", "date": "2025-04-24", "relevancy": 2.4831, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.512}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4994}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TACO%3A%20Tackling%20Over-correction%20in%20Federated%20Learning%20with%20Tailored%0A%20%20Adaptive%20Correction&body=Title%3A%20TACO%3A%20Tackling%20Over-correction%20in%20Federated%20Learning%20with%20Tailored%0A%20%20Adaptive%20Correction%0AAuthor%3A%20Weijie%20Liu%20and%20Ziwei%20Zhan%20and%20Carlee%20Joe-Wong%20and%20Edith%20Ngai%20and%20Jingpu%20Duan%20and%20Deke%20Guo%20and%20Xu%20Chen%20and%20Xiaoxi%20Zhang%0AAbstract%3A%20%20%20Non-independent%20and%20identically%20distributed%20%28Non-IID%29%20data%20across%20edge%0Aclients%20have%20long%20posed%20significant%20challenges%20to%20federated%20learning%20%28FL%29%0Atraining%20in%20edge%20computing%20environments.%20Prior%20works%20have%20proposed%20various%0Amethods%20to%20mitigate%20this%20statistical%20heterogeneity.%20While%20these%20works%20can%0Aachieve%20good%20theoretical%20performance%2C%20in%20this%20work%20we%20provide%20the%20first%0Ainvestigation%20into%20a%20hidden%20over-correction%20phenomenon%20brought%20by%20the%20uniform%0Amodel%20correction%20coefficients%20across%20clients%20adopted%20by%20existing%20methods.%20Such%0Aover-correction%20could%20degrade%20model%20performance%20and%20even%20cause%20failures%20in%0Amodel%20convergence.%20To%20address%20this%2C%20we%20propose%20TACO%2C%20a%20novel%20algorithm%20that%0Aaddresses%20the%20non-IID%20nature%20of%20clients%27%20data%20by%20implementing%20fine-grained%2C%0Aclient-specific%20gradient%20correction%20and%20model%20aggregation%2C%20steering%20local%0Amodels%20towards%20a%20more%20accurate%20global%20optimum.%20Moreover%2C%20we%20verify%20that%20leading%0AFL%20algorithms%20generally%20have%20better%20model%20accuracy%20in%20terms%20of%20communication%0Arounds%20rather%20than%20wall-clock%20time%2C%20resulting%20from%20their%20extra%20computation%0Aoverhead%20imposed%20on%20clients.%20To%20enhance%20the%20training%20efficiency%2C%20TACO%20deploys%20a%0Alightweight%20model%20correction%20and%20tailored%20aggregation%20approach%20that%20requires%0Aminimum%20computation%20overhead%20and%20no%20extra%20information%20beyond%20the%20synchronized%0Amodel%20parameters.%20To%20validate%20TACO%27s%20effectiveness%2C%20we%20present%20the%20first%20FL%0Aconvergence%20analysis%20that%20reveals%20the%20root%20cause%20of%20over-correction.%20Extensive%0Aexperiments%20across%20various%20datasets%20confirm%20TACO%27s%20superior%20and%20stable%0Aperformance%20in%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17528v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTACO%253A%2520Tackling%2520Over-correction%2520in%2520Federated%2520Learning%2520with%2520Tailored%250A%2520%2520Adaptive%2520Correction%26entry.906535625%3DWeijie%2520Liu%2520and%2520Ziwei%2520Zhan%2520and%2520Carlee%2520Joe-Wong%2520and%2520Edith%2520Ngai%2520and%2520Jingpu%2520Duan%2520and%2520Deke%2520Guo%2520and%2520Xu%2520Chen%2520and%2520Xiaoxi%2520Zhang%26entry.1292438233%3D%2520%2520Non-independent%2520and%2520identically%2520distributed%2520%2528Non-IID%2529%2520data%2520across%2520edge%250Aclients%2520have%2520long%2520posed%2520significant%2520challenges%2520to%2520federated%2520learning%2520%2528FL%2529%250Atraining%2520in%2520edge%2520computing%2520environments.%2520Prior%2520works%2520have%2520proposed%2520various%250Amethods%2520to%2520mitigate%2520this%2520statistical%2520heterogeneity.%2520While%2520these%2520works%2520can%250Aachieve%2520good%2520theoretical%2520performance%252C%2520in%2520this%2520work%2520we%2520provide%2520the%2520first%250Ainvestigation%2520into%2520a%2520hidden%2520over-correction%2520phenomenon%2520brought%2520by%2520the%2520uniform%250Amodel%2520correction%2520coefficients%2520across%2520clients%2520adopted%2520by%2520existing%2520methods.%2520Such%250Aover-correction%2520could%2520degrade%2520model%2520performance%2520and%2520even%2520cause%2520failures%2520in%250Amodel%2520convergence.%2520To%2520address%2520this%252C%2520we%2520propose%2520TACO%252C%2520a%2520novel%2520algorithm%2520that%250Aaddresses%2520the%2520non-IID%2520nature%2520of%2520clients%2527%2520data%2520by%2520implementing%2520fine-grained%252C%250Aclient-specific%2520gradient%2520correction%2520and%2520model%2520aggregation%252C%2520steering%2520local%250Amodels%2520towards%2520a%2520more%2520accurate%2520global%2520optimum.%2520Moreover%252C%2520we%2520verify%2520that%2520leading%250AFL%2520algorithms%2520generally%2520have%2520better%2520model%2520accuracy%2520in%2520terms%2520of%2520communication%250Arounds%2520rather%2520than%2520wall-clock%2520time%252C%2520resulting%2520from%2520their%2520extra%2520computation%250Aoverhead%2520imposed%2520on%2520clients.%2520To%2520enhance%2520the%2520training%2520efficiency%252C%2520TACO%2520deploys%2520a%250Alightweight%2520model%2520correction%2520and%2520tailored%2520aggregation%2520approach%2520that%2520requires%250Aminimum%2520computation%2520overhead%2520and%2520no%2520extra%2520information%2520beyond%2520the%2520synchronized%250Amodel%2520parameters.%2520To%2520validate%2520TACO%2527s%2520effectiveness%252C%2520we%2520present%2520the%2520first%2520FL%250Aconvergence%2520analysis%2520that%2520reveals%2520the%2520root%2520cause%2520of%2520over-correction.%2520Extensive%250Aexperiments%2520across%2520various%2520datasets%2520confirm%2520TACO%2527s%2520superior%2520and%2520stable%250Aperformance%2520in%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17528v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TACO%3A%20Tackling%20Over-correction%20in%20Federated%20Learning%20with%20Tailored%0A%20%20Adaptive%20Correction&entry.906535625=Weijie%20Liu%20and%20Ziwei%20Zhan%20and%20Carlee%20Joe-Wong%20and%20Edith%20Ngai%20and%20Jingpu%20Duan%20and%20Deke%20Guo%20and%20Xu%20Chen%20and%20Xiaoxi%20Zhang&entry.1292438233=%20%20Non-independent%20and%20identically%20distributed%20%28Non-IID%29%20data%20across%20edge%0Aclients%20have%20long%20posed%20significant%20challenges%20to%20federated%20learning%20%28FL%29%0Atraining%20in%20edge%20computing%20environments.%20Prior%20works%20have%20proposed%20various%0Amethods%20to%20mitigate%20this%20statistical%20heterogeneity.%20While%20these%20works%20can%0Aachieve%20good%20theoretical%20performance%2C%20in%20this%20work%20we%20provide%20the%20first%0Ainvestigation%20into%20a%20hidden%20over-correction%20phenomenon%20brought%20by%20the%20uniform%0Amodel%20correction%20coefficients%20across%20clients%20adopted%20by%20existing%20methods.%20Such%0Aover-correction%20could%20degrade%20model%20performance%20and%20even%20cause%20failures%20in%0Amodel%20convergence.%20To%20address%20this%2C%20we%20propose%20TACO%2C%20a%20novel%20algorithm%20that%0Aaddresses%20the%20non-IID%20nature%20of%20clients%27%20data%20by%20implementing%20fine-grained%2C%0Aclient-specific%20gradient%20correction%20and%20model%20aggregation%2C%20steering%20local%0Amodels%20towards%20a%20more%20accurate%20global%20optimum.%20Moreover%2C%20we%20verify%20that%20leading%0AFL%20algorithms%20generally%20have%20better%20model%20accuracy%20in%20terms%20of%20communication%0Arounds%20rather%20than%20wall-clock%20time%2C%20resulting%20from%20their%20extra%20computation%0Aoverhead%20imposed%20on%20clients.%20To%20enhance%20the%20training%20efficiency%2C%20TACO%20deploys%20a%0Alightweight%20model%20correction%20and%20tailored%20aggregation%20approach%20that%20requires%0Aminimum%20computation%20overhead%20and%20no%20extra%20information%20beyond%20the%20synchronized%0Amodel%20parameters.%20To%20validate%20TACO%27s%20effectiveness%2C%20we%20present%20the%20first%20FL%0Aconvergence%20analysis%20that%20reveals%20the%20root%20cause%20of%20over-correction.%20Extensive%0Aexperiments%20across%20various%20datasets%20confirm%20TACO%27s%20superior%20and%20stable%0Aperformance%20in%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17528v1&entry.124074799=Read"},
{"title": "Replay to Remember: Retaining Domain Knowledge in Streaming Language\n  Models", "author": "Sneh Pillai", "abstract": "  Continual learning in large language models (LLMs) typically encounters the\ncritical challenge of catastrophic forgetting, where previously acquired\nknowledge deteriorates upon exposure to new data. While techniques like replay\nbuffers and parameter-efficient tuning (e.g., Low-Rank Adaptation or LoRA) have\nbeen proposed, few studies investigate real-time domain adaptation under strict\ncomputational and data-stream constraints. In this paper, we demonstrate a\nlightweight method combining LoRA and a minimal replay mechanism in a realistic\nstreaming setting across three diverse knowledge domains: medical question\nanswering, genetics, and law. Using perplexity, semantic similarity, and\nGPT-based human-like evaluation metrics, we quantify the model's adaptation,\nforgetting, and recovery over time. Our experiments reveal that while\ncatastrophic forgetting naturally occurs, even minimal replay significantly\nstabilizes and partially restores domain-specific knowledge. This study\ncontributes practical insights for deploying adaptable LLMs in\nresource-constrained, real-world scenarios.\n", "link": "http://arxiv.org/abs/2504.17780v1", "date": "2025-04-24", "relevancy": 2.4592, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.496}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4898}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Replay%20to%20Remember%3A%20Retaining%20Domain%20Knowledge%20in%20Streaming%20Language%0A%20%20Models&body=Title%3A%20Replay%20to%20Remember%3A%20Retaining%20Domain%20Knowledge%20in%20Streaming%20Language%0A%20%20Models%0AAuthor%3A%20Sneh%20Pillai%0AAbstract%3A%20%20%20Continual%20learning%20in%20large%20language%20models%20%28LLMs%29%20typically%20encounters%20the%0Acritical%20challenge%20of%20catastrophic%20forgetting%2C%20where%20previously%20acquired%0Aknowledge%20deteriorates%20upon%20exposure%20to%20new%20data.%20While%20techniques%20like%20replay%0Abuffers%20and%20parameter-efficient%20tuning%20%28e.g.%2C%20Low-Rank%20Adaptation%20or%20LoRA%29%20have%0Abeen%20proposed%2C%20few%20studies%20investigate%20real-time%20domain%20adaptation%20under%20strict%0Acomputational%20and%20data-stream%20constraints.%20In%20this%20paper%2C%20we%20demonstrate%20a%0Alightweight%20method%20combining%20LoRA%20and%20a%20minimal%20replay%20mechanism%20in%20a%20realistic%0Astreaming%20setting%20across%20three%20diverse%20knowledge%20domains%3A%20medical%20question%0Aanswering%2C%20genetics%2C%20and%20law.%20Using%20perplexity%2C%20semantic%20similarity%2C%20and%0AGPT-based%20human-like%20evaluation%20metrics%2C%20we%20quantify%20the%20model%27s%20adaptation%2C%0Aforgetting%2C%20and%20recovery%20over%20time.%20Our%20experiments%20reveal%20that%20while%0Acatastrophic%20forgetting%20naturally%20occurs%2C%20even%20minimal%20replay%20significantly%0Astabilizes%20and%20partially%20restores%20domain-specific%20knowledge.%20This%20study%0Acontributes%20practical%20insights%20for%20deploying%20adaptable%20LLMs%20in%0Aresource-constrained%2C%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17780v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReplay%2520to%2520Remember%253A%2520Retaining%2520Domain%2520Knowledge%2520in%2520Streaming%2520Language%250A%2520%2520Models%26entry.906535625%3DSneh%2520Pillai%26entry.1292438233%3D%2520%2520Continual%2520learning%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520typically%2520encounters%2520the%250Acritical%2520challenge%2520of%2520catastrophic%2520forgetting%252C%2520where%2520previously%2520acquired%250Aknowledge%2520deteriorates%2520upon%2520exposure%2520to%2520new%2520data.%2520While%2520techniques%2520like%2520replay%250Abuffers%2520and%2520parameter-efficient%2520tuning%2520%2528e.g.%252C%2520Low-Rank%2520Adaptation%2520or%2520LoRA%2529%2520have%250Abeen%2520proposed%252C%2520few%2520studies%2520investigate%2520real-time%2520domain%2520adaptation%2520under%2520strict%250Acomputational%2520and%2520data-stream%2520constraints.%2520In%2520this%2520paper%252C%2520we%2520demonstrate%2520a%250Alightweight%2520method%2520combining%2520LoRA%2520and%2520a%2520minimal%2520replay%2520mechanism%2520in%2520a%2520realistic%250Astreaming%2520setting%2520across%2520three%2520diverse%2520knowledge%2520domains%253A%2520medical%2520question%250Aanswering%252C%2520genetics%252C%2520and%2520law.%2520Using%2520perplexity%252C%2520semantic%2520similarity%252C%2520and%250AGPT-based%2520human-like%2520evaluation%2520metrics%252C%2520we%2520quantify%2520the%2520model%2527s%2520adaptation%252C%250Aforgetting%252C%2520and%2520recovery%2520over%2520time.%2520Our%2520experiments%2520reveal%2520that%2520while%250Acatastrophic%2520forgetting%2520naturally%2520occurs%252C%2520even%2520minimal%2520replay%2520significantly%250Astabilizes%2520and%2520partially%2520restores%2520domain-specific%2520knowledge.%2520This%2520study%250Acontributes%2520practical%2520insights%2520for%2520deploying%2520adaptable%2520LLMs%2520in%250Aresource-constrained%252C%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17780v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Replay%20to%20Remember%3A%20Retaining%20Domain%20Knowledge%20in%20Streaming%20Language%0A%20%20Models&entry.906535625=Sneh%20Pillai&entry.1292438233=%20%20Continual%20learning%20in%20large%20language%20models%20%28LLMs%29%20typically%20encounters%20the%0Acritical%20challenge%20of%20catastrophic%20forgetting%2C%20where%20previously%20acquired%0Aknowledge%20deteriorates%20upon%20exposure%20to%20new%20data.%20While%20techniques%20like%20replay%0Abuffers%20and%20parameter-efficient%20tuning%20%28e.g.%2C%20Low-Rank%20Adaptation%20or%20LoRA%29%20have%0Abeen%20proposed%2C%20few%20studies%20investigate%20real-time%20domain%20adaptation%20under%20strict%0Acomputational%20and%20data-stream%20constraints.%20In%20this%20paper%2C%20we%20demonstrate%20a%0Alightweight%20method%20combining%20LoRA%20and%20a%20minimal%20replay%20mechanism%20in%20a%20realistic%0Astreaming%20setting%20across%20three%20diverse%20knowledge%20domains%3A%20medical%20question%0Aanswering%2C%20genetics%2C%20and%20law.%20Using%20perplexity%2C%20semantic%20similarity%2C%20and%0AGPT-based%20human-like%20evaluation%20metrics%2C%20we%20quantify%20the%20model%27s%20adaptation%2C%0Aforgetting%2C%20and%20recovery%20over%20time.%20Our%20experiments%20reveal%20that%20while%0Acatastrophic%20forgetting%20naturally%20occurs%2C%20even%20minimal%20replay%20significantly%0Astabilizes%20and%20partially%20restores%20domain-specific%20knowledge.%20This%20study%0Acontributes%20practical%20insights%20for%20deploying%20adaptable%20LLMs%20in%0Aresource-constrained%2C%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17780v1&entry.124074799=Read"},
{"title": "DiMeR: Disentangled Mesh Reconstruction Model", "author": "Lutao Jiang and Jiantao Lin and Kanghao Chen and Wenhang Ge and Xin Yang and Yifan Jiang and Yuanhuiyi Lyu and Xu Zheng and Yingcong Chen", "abstract": "  With the advent of large-scale 3D datasets, feed-forward 3D generative\nmodels, such as the Large Reconstruction Model (LRM), have gained significant\nattention and achieved remarkable success. However, we observe that RGB images\noften lead to conflicting training objectives and lack the necessary clarity\nfor geometry reconstruction. In this paper, we revisit the inductive biases\nassociated with mesh reconstruction and introduce DiMeR, a novel disentangled\ndual-stream feed-forward model for sparse-view mesh reconstruction. The key\nidea is to disentangle both the input and framework into geometry and texture\nparts, thereby reducing the training difficulty for each part according to the\nPrinciple of Occam's Razor. Given that normal maps are strictly consistent with\ngeometry and accurately capture surface variations, we utilize normal maps as\nexclusive input for the geometry branch to reduce the complexity between the\nnetwork's input and output. Moreover, we improve the mesh extraction algorithm\nto introduce 3D ground truth supervision. As for texture branch, we use RGB\nimages as input to obtain the textured mesh. Overall, DiMeR demonstrates robust\ncapabilities across various tasks, including sparse-view reconstruction,\nsingle-image-to-3D, and text-to-3D. Numerous experiments show that DiMeR\nsignificantly outperforms previous methods, achieving over 30% improvement in\nChamfer Distance on the GSO and OmniObject3D dataset.\n", "link": "http://arxiv.org/abs/2504.17670v1", "date": "2025-04-24", "relevancy": 2.4273, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6442}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5803}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.58}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiMeR%3A%20Disentangled%20Mesh%20Reconstruction%20Model&body=Title%3A%20DiMeR%3A%20Disentangled%20Mesh%20Reconstruction%20Model%0AAuthor%3A%20Lutao%20Jiang%20and%20Jiantao%20Lin%20and%20Kanghao%20Chen%20and%20Wenhang%20Ge%20and%20Xin%20Yang%20and%20Yifan%20Jiang%20and%20Yuanhuiyi%20Lyu%20and%20Xu%20Zheng%20and%20Yingcong%20Chen%0AAbstract%3A%20%20%20With%20the%20advent%20of%20large-scale%203D%20datasets%2C%20feed-forward%203D%20generative%0Amodels%2C%20such%20as%20the%20Large%20Reconstruction%20Model%20%28LRM%29%2C%20have%20gained%20significant%0Aattention%20and%20achieved%20remarkable%20success.%20However%2C%20we%20observe%20that%20RGB%20images%0Aoften%20lead%20to%20conflicting%20training%20objectives%20and%20lack%20the%20necessary%20clarity%0Afor%20geometry%20reconstruction.%20In%20this%20paper%2C%20we%20revisit%20the%20inductive%20biases%0Aassociated%20with%20mesh%20reconstruction%20and%20introduce%20DiMeR%2C%20a%20novel%20disentangled%0Adual-stream%20feed-forward%20model%20for%20sparse-view%20mesh%20reconstruction.%20The%20key%0Aidea%20is%20to%20disentangle%20both%20the%20input%20and%20framework%20into%20geometry%20and%20texture%0Aparts%2C%20thereby%20reducing%20the%20training%20difficulty%20for%20each%20part%20according%20to%20the%0APrinciple%20of%20Occam%27s%20Razor.%20Given%20that%20normal%20maps%20are%20strictly%20consistent%20with%0Ageometry%20and%20accurately%20capture%20surface%20variations%2C%20we%20utilize%20normal%20maps%20as%0Aexclusive%20input%20for%20the%20geometry%20branch%20to%20reduce%20the%20complexity%20between%20the%0Anetwork%27s%20input%20and%20output.%20Moreover%2C%20we%20improve%20the%20mesh%20extraction%20algorithm%0Ato%20introduce%203D%20ground%20truth%20supervision.%20As%20for%20texture%20branch%2C%20we%20use%20RGB%0Aimages%20as%20input%20to%20obtain%20the%20textured%20mesh.%20Overall%2C%20DiMeR%20demonstrates%20robust%0Acapabilities%20across%20various%20tasks%2C%20including%20sparse-view%20reconstruction%2C%0Asingle-image-to-3D%2C%20and%20text-to-3D.%20Numerous%20experiments%20show%20that%20DiMeR%0Asignificantly%20outperforms%20previous%20methods%2C%20achieving%20over%2030%25%20improvement%20in%0AChamfer%20Distance%20on%20the%20GSO%20and%20OmniObject3D%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17670v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiMeR%253A%2520Disentangled%2520Mesh%2520Reconstruction%2520Model%26entry.906535625%3DLutao%2520Jiang%2520and%2520Jiantao%2520Lin%2520and%2520Kanghao%2520Chen%2520and%2520Wenhang%2520Ge%2520and%2520Xin%2520Yang%2520and%2520Yifan%2520Jiang%2520and%2520Yuanhuiyi%2520Lyu%2520and%2520Xu%2520Zheng%2520and%2520Yingcong%2520Chen%26entry.1292438233%3D%2520%2520With%2520the%2520advent%2520of%2520large-scale%25203D%2520datasets%252C%2520feed-forward%25203D%2520generative%250Amodels%252C%2520such%2520as%2520the%2520Large%2520Reconstruction%2520Model%2520%2528LRM%2529%252C%2520have%2520gained%2520significant%250Aattention%2520and%2520achieved%2520remarkable%2520success.%2520However%252C%2520we%2520observe%2520that%2520RGB%2520images%250Aoften%2520lead%2520to%2520conflicting%2520training%2520objectives%2520and%2520lack%2520the%2520necessary%2520clarity%250Afor%2520geometry%2520reconstruction.%2520In%2520this%2520paper%252C%2520we%2520revisit%2520the%2520inductive%2520biases%250Aassociated%2520with%2520mesh%2520reconstruction%2520and%2520introduce%2520DiMeR%252C%2520a%2520novel%2520disentangled%250Adual-stream%2520feed-forward%2520model%2520for%2520sparse-view%2520mesh%2520reconstruction.%2520The%2520key%250Aidea%2520is%2520to%2520disentangle%2520both%2520the%2520input%2520and%2520framework%2520into%2520geometry%2520and%2520texture%250Aparts%252C%2520thereby%2520reducing%2520the%2520training%2520difficulty%2520for%2520each%2520part%2520according%2520to%2520the%250APrinciple%2520of%2520Occam%2527s%2520Razor.%2520Given%2520that%2520normal%2520maps%2520are%2520strictly%2520consistent%2520with%250Ageometry%2520and%2520accurately%2520capture%2520surface%2520variations%252C%2520we%2520utilize%2520normal%2520maps%2520as%250Aexclusive%2520input%2520for%2520the%2520geometry%2520branch%2520to%2520reduce%2520the%2520complexity%2520between%2520the%250Anetwork%2527s%2520input%2520and%2520output.%2520Moreover%252C%2520we%2520improve%2520the%2520mesh%2520extraction%2520algorithm%250Ato%2520introduce%25203D%2520ground%2520truth%2520supervision.%2520As%2520for%2520texture%2520branch%252C%2520we%2520use%2520RGB%250Aimages%2520as%2520input%2520to%2520obtain%2520the%2520textured%2520mesh.%2520Overall%252C%2520DiMeR%2520demonstrates%2520robust%250Acapabilities%2520across%2520various%2520tasks%252C%2520including%2520sparse-view%2520reconstruction%252C%250Asingle-image-to-3D%252C%2520and%2520text-to-3D.%2520Numerous%2520experiments%2520show%2520that%2520DiMeR%250Asignificantly%2520outperforms%2520previous%2520methods%252C%2520achieving%2520over%252030%2525%2520improvement%2520in%250AChamfer%2520Distance%2520on%2520the%2520GSO%2520and%2520OmniObject3D%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17670v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiMeR%3A%20Disentangled%20Mesh%20Reconstruction%20Model&entry.906535625=Lutao%20Jiang%20and%20Jiantao%20Lin%20and%20Kanghao%20Chen%20and%20Wenhang%20Ge%20and%20Xin%20Yang%20and%20Yifan%20Jiang%20and%20Yuanhuiyi%20Lyu%20and%20Xu%20Zheng%20and%20Yingcong%20Chen&entry.1292438233=%20%20With%20the%20advent%20of%20large-scale%203D%20datasets%2C%20feed-forward%203D%20generative%0Amodels%2C%20such%20as%20the%20Large%20Reconstruction%20Model%20%28LRM%29%2C%20have%20gained%20significant%0Aattention%20and%20achieved%20remarkable%20success.%20However%2C%20we%20observe%20that%20RGB%20images%0Aoften%20lead%20to%20conflicting%20training%20objectives%20and%20lack%20the%20necessary%20clarity%0Afor%20geometry%20reconstruction.%20In%20this%20paper%2C%20we%20revisit%20the%20inductive%20biases%0Aassociated%20with%20mesh%20reconstruction%20and%20introduce%20DiMeR%2C%20a%20novel%20disentangled%0Adual-stream%20feed-forward%20model%20for%20sparse-view%20mesh%20reconstruction.%20The%20key%0Aidea%20is%20to%20disentangle%20both%20the%20input%20and%20framework%20into%20geometry%20and%20texture%0Aparts%2C%20thereby%20reducing%20the%20training%20difficulty%20for%20each%20part%20according%20to%20the%0APrinciple%20of%20Occam%27s%20Razor.%20Given%20that%20normal%20maps%20are%20strictly%20consistent%20with%0Ageometry%20and%20accurately%20capture%20surface%20variations%2C%20we%20utilize%20normal%20maps%20as%0Aexclusive%20input%20for%20the%20geometry%20branch%20to%20reduce%20the%20complexity%20between%20the%0Anetwork%27s%20input%20and%20output.%20Moreover%2C%20we%20improve%20the%20mesh%20extraction%20algorithm%0Ato%20introduce%203D%20ground%20truth%20supervision.%20As%20for%20texture%20branch%2C%20we%20use%20RGB%0Aimages%20as%20input%20to%20obtain%20the%20textured%20mesh.%20Overall%2C%20DiMeR%20demonstrates%20robust%0Acapabilities%20across%20various%20tasks%2C%20including%20sparse-view%20reconstruction%2C%0Asingle-image-to-3D%2C%20and%20text-to-3D.%20Numerous%20experiments%20show%20that%20DiMeR%0Asignificantly%20outperforms%20previous%20methods%2C%20achieving%20over%2030%25%20improvement%20in%0AChamfer%20Distance%20on%20the%20GSO%20and%20OmniObject3D%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17670v1&entry.124074799=Read"},
{"title": "Transfer Learning with Foundational Models for Time Series Forecasting\n  using Low-Rank Adaptations", "author": "M. Germ\u00e1n-Morales and A. J. Rivera-Rivas and M. J. del Jesus D\u00edaz and C. J. Carmona", "abstract": "  Foundational Models are an emerging widely used technique of GenAI. These\nmodels are distinguished by their scalability and the ease with which they can\nbe adapted through the exploitation of Transfer Learning. The availability of\nhigh computational power and large datasets have supported their development,\nachieving a high generalization capacity due to the enormous and heterogeneous\namounts of data used in their initial training. These characteristics\ncontribute to a solid base that can be adapted or adjusted to a wide range of\ntasks, increasing their applicability. This study proposes the methodology\nLLIAM, a straightforward adaptation of a kind of FM, Large Language Models, for\nthe Time Series Forecasting task. An adequate time-series prompting schema and\nLow-Rank Adaptations are used to enhance the knowledge of the model with\ndiverse time series datasets, known as the fine-tuning phase. A study divided\nin two stages has been performed for evaluating the effectiveness of the\nproposed methodology. Initially, a comparison was made between the performance\nof LLIAM and different state-of-the-art DL algorithms, including Recurrent\nNeural Networks and Temporal Convolutional Networks, as well as a LLM-based\nmethod, TimeLLM. Following this, a zero-shot study is presented in order to\nevaluate the generalization capacity of the proposed methodology with time\nseries datasets from unknown domains not considered in the model training. The\noutcomes of this investigation demonstrate the efficacy of LLIAM, highlighting\nthat this straightforward and general approach can attain competent results\nwithout the necessity for applying complex modifications. This work also\nencourages the use of available resources (such as these pre-trained models)\nand efficient fine-tuning techniques to avoid unnecessary and costly training,\nnarrowing the gap between the goals of traditional AI and Green AI.\n", "link": "http://arxiv.org/abs/2410.11539v2", "date": "2025-04-24", "relevancy": 2.4083, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4905}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4772}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transfer%20Learning%20with%20Foundational%20Models%20for%20Time%20Series%20Forecasting%0A%20%20using%20Low-Rank%20Adaptations&body=Title%3A%20Transfer%20Learning%20with%20Foundational%20Models%20for%20Time%20Series%20Forecasting%0A%20%20using%20Low-Rank%20Adaptations%0AAuthor%3A%20M.%20Germ%C3%A1n-Morales%20and%20A.%20J.%20Rivera-Rivas%20and%20M.%20J.%20del%20Jesus%20D%C3%ADaz%20and%20C.%20J.%20Carmona%0AAbstract%3A%20%20%20Foundational%20Models%20are%20an%20emerging%20widely%20used%20technique%20of%20GenAI.%20These%0Amodels%20are%20distinguished%20by%20their%20scalability%20and%20the%20ease%20with%20which%20they%20can%0Abe%20adapted%20through%20the%20exploitation%20of%20Transfer%20Learning.%20The%20availability%20of%0Ahigh%20computational%20power%20and%20large%20datasets%20have%20supported%20their%20development%2C%0Aachieving%20a%20high%20generalization%20capacity%20due%20to%20the%20enormous%20and%20heterogeneous%0Aamounts%20of%20data%20used%20in%20their%20initial%20training.%20These%20characteristics%0Acontribute%20to%20a%20solid%20base%20that%20can%20be%20adapted%20or%20adjusted%20to%20a%20wide%20range%20of%0Atasks%2C%20increasing%20their%20applicability.%20This%20study%20proposes%20the%20methodology%0ALLIAM%2C%20a%20straightforward%20adaptation%20of%20a%20kind%20of%20FM%2C%20Large%20Language%20Models%2C%20for%0Athe%20Time%20Series%20Forecasting%20task.%20An%20adequate%20time-series%20prompting%20schema%20and%0ALow-Rank%20Adaptations%20are%20used%20to%20enhance%20the%20knowledge%20of%20the%20model%20with%0Adiverse%20time%20series%20datasets%2C%20known%20as%20the%20fine-tuning%20phase.%20A%20study%20divided%0Ain%20two%20stages%20has%20been%20performed%20for%20evaluating%20the%20effectiveness%20of%20the%0Aproposed%20methodology.%20Initially%2C%20a%20comparison%20was%20made%20between%20the%20performance%0Aof%20LLIAM%20and%20different%20state-of-the-art%20DL%20algorithms%2C%20including%20Recurrent%0ANeural%20Networks%20and%20Temporal%20Convolutional%20Networks%2C%20as%20well%20as%20a%20LLM-based%0Amethod%2C%20TimeLLM.%20Following%20this%2C%20a%20zero-shot%20study%20is%20presented%20in%20order%20to%0Aevaluate%20the%20generalization%20capacity%20of%20the%20proposed%20methodology%20with%20time%0Aseries%20datasets%20from%20unknown%20domains%20not%20considered%20in%20the%20model%20training.%20The%0Aoutcomes%20of%20this%20investigation%20demonstrate%20the%20efficacy%20of%20LLIAM%2C%20highlighting%0Athat%20this%20straightforward%20and%20general%20approach%20can%20attain%20competent%20results%0Awithout%20the%20necessity%20for%20applying%20complex%20modifications.%20This%20work%20also%0Aencourages%20the%20use%20of%20available%20resources%20%28such%20as%20these%20pre-trained%20models%29%0Aand%20efficient%20fine-tuning%20techniques%20to%20avoid%20unnecessary%20and%20costly%20training%2C%0Anarrowing%20the%20gap%20between%20the%20goals%20of%20traditional%20AI%20and%20Green%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11539v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransfer%2520Learning%2520with%2520Foundational%2520Models%2520for%2520Time%2520Series%2520Forecasting%250A%2520%2520using%2520Low-Rank%2520Adaptations%26entry.906535625%3DM.%2520Germ%25C3%25A1n-Morales%2520and%2520A.%2520J.%2520Rivera-Rivas%2520and%2520M.%2520J.%2520del%2520Jesus%2520D%25C3%25ADaz%2520and%2520C.%2520J.%2520Carmona%26entry.1292438233%3D%2520%2520Foundational%2520Models%2520are%2520an%2520emerging%2520widely%2520used%2520technique%2520of%2520GenAI.%2520These%250Amodels%2520are%2520distinguished%2520by%2520their%2520scalability%2520and%2520the%2520ease%2520with%2520which%2520they%2520can%250Abe%2520adapted%2520through%2520the%2520exploitation%2520of%2520Transfer%2520Learning.%2520The%2520availability%2520of%250Ahigh%2520computational%2520power%2520and%2520large%2520datasets%2520have%2520supported%2520their%2520development%252C%250Aachieving%2520a%2520high%2520generalization%2520capacity%2520due%2520to%2520the%2520enormous%2520and%2520heterogeneous%250Aamounts%2520of%2520data%2520used%2520in%2520their%2520initial%2520training.%2520These%2520characteristics%250Acontribute%2520to%2520a%2520solid%2520base%2520that%2520can%2520be%2520adapted%2520or%2520adjusted%2520to%2520a%2520wide%2520range%2520of%250Atasks%252C%2520increasing%2520their%2520applicability.%2520This%2520study%2520proposes%2520the%2520methodology%250ALLIAM%252C%2520a%2520straightforward%2520adaptation%2520of%2520a%2520kind%2520of%2520FM%252C%2520Large%2520Language%2520Models%252C%2520for%250Athe%2520Time%2520Series%2520Forecasting%2520task.%2520An%2520adequate%2520time-series%2520prompting%2520schema%2520and%250ALow-Rank%2520Adaptations%2520are%2520used%2520to%2520enhance%2520the%2520knowledge%2520of%2520the%2520model%2520with%250Adiverse%2520time%2520series%2520datasets%252C%2520known%2520as%2520the%2520fine-tuning%2520phase.%2520A%2520study%2520divided%250Ain%2520two%2520stages%2520has%2520been%2520performed%2520for%2520evaluating%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520methodology.%2520Initially%252C%2520a%2520comparison%2520was%2520made%2520between%2520the%2520performance%250Aof%2520LLIAM%2520and%2520different%2520state-of-the-art%2520DL%2520algorithms%252C%2520including%2520Recurrent%250ANeural%2520Networks%2520and%2520Temporal%2520Convolutional%2520Networks%252C%2520as%2520well%2520as%2520a%2520LLM-based%250Amethod%252C%2520TimeLLM.%2520Following%2520this%252C%2520a%2520zero-shot%2520study%2520is%2520presented%2520in%2520order%2520to%250Aevaluate%2520the%2520generalization%2520capacity%2520of%2520the%2520proposed%2520methodology%2520with%2520time%250Aseries%2520datasets%2520from%2520unknown%2520domains%2520not%2520considered%2520in%2520the%2520model%2520training.%2520The%250Aoutcomes%2520of%2520this%2520investigation%2520demonstrate%2520the%2520efficacy%2520of%2520LLIAM%252C%2520highlighting%250Athat%2520this%2520straightforward%2520and%2520general%2520approach%2520can%2520attain%2520competent%2520results%250Awithout%2520the%2520necessity%2520for%2520applying%2520complex%2520modifications.%2520This%2520work%2520also%250Aencourages%2520the%2520use%2520of%2520available%2520resources%2520%2528such%2520as%2520these%2520pre-trained%2520models%2529%250Aand%2520efficient%2520fine-tuning%2520techniques%2520to%2520avoid%2520unnecessary%2520and%2520costly%2520training%252C%250Anarrowing%2520the%2520gap%2520between%2520the%2520goals%2520of%2520traditional%2520AI%2520and%2520Green%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11539v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transfer%20Learning%20with%20Foundational%20Models%20for%20Time%20Series%20Forecasting%0A%20%20using%20Low-Rank%20Adaptations&entry.906535625=M.%20Germ%C3%A1n-Morales%20and%20A.%20J.%20Rivera-Rivas%20and%20M.%20J.%20del%20Jesus%20D%C3%ADaz%20and%20C.%20J.%20Carmona&entry.1292438233=%20%20Foundational%20Models%20are%20an%20emerging%20widely%20used%20technique%20of%20GenAI.%20These%0Amodels%20are%20distinguished%20by%20their%20scalability%20and%20the%20ease%20with%20which%20they%20can%0Abe%20adapted%20through%20the%20exploitation%20of%20Transfer%20Learning.%20The%20availability%20of%0Ahigh%20computational%20power%20and%20large%20datasets%20have%20supported%20their%20development%2C%0Aachieving%20a%20high%20generalization%20capacity%20due%20to%20the%20enormous%20and%20heterogeneous%0Aamounts%20of%20data%20used%20in%20their%20initial%20training.%20These%20characteristics%0Acontribute%20to%20a%20solid%20base%20that%20can%20be%20adapted%20or%20adjusted%20to%20a%20wide%20range%20of%0Atasks%2C%20increasing%20their%20applicability.%20This%20study%20proposes%20the%20methodology%0ALLIAM%2C%20a%20straightforward%20adaptation%20of%20a%20kind%20of%20FM%2C%20Large%20Language%20Models%2C%20for%0Athe%20Time%20Series%20Forecasting%20task.%20An%20adequate%20time-series%20prompting%20schema%20and%0ALow-Rank%20Adaptations%20are%20used%20to%20enhance%20the%20knowledge%20of%20the%20model%20with%0Adiverse%20time%20series%20datasets%2C%20known%20as%20the%20fine-tuning%20phase.%20A%20study%20divided%0Ain%20two%20stages%20has%20been%20performed%20for%20evaluating%20the%20effectiveness%20of%20the%0Aproposed%20methodology.%20Initially%2C%20a%20comparison%20was%20made%20between%20the%20performance%0Aof%20LLIAM%20and%20different%20state-of-the-art%20DL%20algorithms%2C%20including%20Recurrent%0ANeural%20Networks%20and%20Temporal%20Convolutional%20Networks%2C%20as%20well%20as%20a%20LLM-based%0Amethod%2C%20TimeLLM.%20Following%20this%2C%20a%20zero-shot%20study%20is%20presented%20in%20order%20to%0Aevaluate%20the%20generalization%20capacity%20of%20the%20proposed%20methodology%20with%20time%0Aseries%20datasets%20from%20unknown%20domains%20not%20considered%20in%20the%20model%20training.%20The%0Aoutcomes%20of%20this%20investigation%20demonstrate%20the%20efficacy%20of%20LLIAM%2C%20highlighting%0Athat%20this%20straightforward%20and%20general%20approach%20can%20attain%20competent%20results%0Awithout%20the%20necessity%20for%20applying%20complex%20modifications.%20This%20work%20also%0Aencourages%20the%20use%20of%20available%20resources%20%28such%20as%20these%20pre-trained%20models%29%0Aand%20efficient%20fine-tuning%20techniques%20to%20avoid%20unnecessary%20and%20costly%20training%2C%0Anarrowing%20the%20gap%20between%20the%20goals%20of%20traditional%20AI%20and%20Green%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11539v2&entry.124074799=Read"},
{"title": "Predict-Optimize-Distill: A Self-Improving Cycle for 4D Object\n  Understanding", "author": "Mingxuan Wu and Huang Huang and Justin Kerr and Chung Min Kim and Anthony Zhang and Brent Yi and Angjoo Kanazawa", "abstract": "  Humans can resort to long-form inspection to build intuition on predicting\nthe 3D configurations of unseen objects. The more we observe the object motion,\nthe better we get at predicting its 3D state immediately. Existing systems\neither optimize underlying representations from multi-view observations or\ntrain a feed-forward predictor from supervised datasets. We introduce\nPredict-Optimize-Distill (POD), a self-improving framework that interleaves\nprediction and optimization in a mutually reinforcing cycle to achieve better\n4D object understanding with increasing observation time. Given a multi-view\nobject scan and a long-form monocular video of human-object interaction, POD\niteratively trains a neural network to predict local part poses from RGB\nframes, uses this predictor to initialize a global optimization which refines\noutput poses through inverse rendering, then finally distills the results of\noptimization back into the model by generating synthetic self-labeled training\ndata from novel viewpoints. Each iteration improves both the predictive model\nand the optimized motion trajectory, creating a virtuous cycle that bootstraps\nits own training data to learn about the pose configurations of an object. We\nalso introduce a quasi-multiview mining strategy for reducing depth ambiguity\nby leveraging long video. We evaluate POD on 14 real-world and 5 synthetic\nobjects with various joint types, including revolute and prismatic joints as\nwell as multi-body configurations where parts detach or reattach independently.\nPOD demonstrates significant improvement over a pure optimization baseline\nwhich gets stuck in local minima, particularly for longer videos. We also find\nthat POD's performance improves with both video length and successive\niterations of the self-improving cycle, highlighting its ability to scale\nperformance with additional observations and looped refinement.\n", "link": "http://arxiv.org/abs/2504.17441v1", "date": "2025-04-24", "relevancy": 2.4072, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6161}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5943}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predict-Optimize-Distill%3A%20A%20Self-Improving%20Cycle%20for%204D%20Object%0A%20%20Understanding&body=Title%3A%20Predict-Optimize-Distill%3A%20A%20Self-Improving%20Cycle%20for%204D%20Object%0A%20%20Understanding%0AAuthor%3A%20Mingxuan%20Wu%20and%20Huang%20Huang%20and%20Justin%20Kerr%20and%20Chung%20Min%20Kim%20and%20Anthony%20Zhang%20and%20Brent%20Yi%20and%20Angjoo%20Kanazawa%0AAbstract%3A%20%20%20Humans%20can%20resort%20to%20long-form%20inspection%20to%20build%20intuition%20on%20predicting%0Athe%203D%20configurations%20of%20unseen%20objects.%20The%20more%20we%20observe%20the%20object%20motion%2C%0Athe%20better%20we%20get%20at%20predicting%20its%203D%20state%20immediately.%20Existing%20systems%0Aeither%20optimize%20underlying%20representations%20from%20multi-view%20observations%20or%0Atrain%20a%20feed-forward%20predictor%20from%20supervised%20datasets.%20We%20introduce%0APredict-Optimize-Distill%20%28POD%29%2C%20a%20self-improving%20framework%20that%20interleaves%0Aprediction%20and%20optimization%20in%20a%20mutually%20reinforcing%20cycle%20to%20achieve%20better%0A4D%20object%20understanding%20with%20increasing%20observation%20time.%20Given%20a%20multi-view%0Aobject%20scan%20and%20a%20long-form%20monocular%20video%20of%20human-object%20interaction%2C%20POD%0Aiteratively%20trains%20a%20neural%20network%20to%20predict%20local%20part%20poses%20from%20RGB%0Aframes%2C%20uses%20this%20predictor%20to%20initialize%20a%20global%20optimization%20which%20refines%0Aoutput%20poses%20through%20inverse%20rendering%2C%20then%20finally%20distills%20the%20results%20of%0Aoptimization%20back%20into%20the%20model%20by%20generating%20synthetic%20self-labeled%20training%0Adata%20from%20novel%20viewpoints.%20Each%20iteration%20improves%20both%20the%20predictive%20model%0Aand%20the%20optimized%20motion%20trajectory%2C%20creating%20a%20virtuous%20cycle%20that%20bootstraps%0Aits%20own%20training%20data%20to%20learn%20about%20the%20pose%20configurations%20of%20an%20object.%20We%0Aalso%20introduce%20a%20quasi-multiview%20mining%20strategy%20for%20reducing%20depth%20ambiguity%0Aby%20leveraging%20long%20video.%20We%20evaluate%20POD%20on%2014%20real-world%20and%205%20synthetic%0Aobjects%20with%20various%20joint%20types%2C%20including%20revolute%20and%20prismatic%20joints%20as%0Awell%20as%20multi-body%20configurations%20where%20parts%20detach%20or%20reattach%20independently.%0APOD%20demonstrates%20significant%20improvement%20over%20a%20pure%20optimization%20baseline%0Awhich%20gets%20stuck%20in%20local%20minima%2C%20particularly%20for%20longer%20videos.%20We%20also%20find%0Athat%20POD%27s%20performance%20improves%20with%20both%20video%20length%20and%20successive%0Aiterations%20of%20the%20self-improving%20cycle%2C%20highlighting%20its%20ability%20to%20scale%0Aperformance%20with%20additional%20observations%20and%20looped%20refinement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17441v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredict-Optimize-Distill%253A%2520A%2520Self-Improving%2520Cycle%2520for%25204D%2520Object%250A%2520%2520Understanding%26entry.906535625%3DMingxuan%2520Wu%2520and%2520Huang%2520Huang%2520and%2520Justin%2520Kerr%2520and%2520Chung%2520Min%2520Kim%2520and%2520Anthony%2520Zhang%2520and%2520Brent%2520Yi%2520and%2520Angjoo%2520Kanazawa%26entry.1292438233%3D%2520%2520Humans%2520can%2520resort%2520to%2520long-form%2520inspection%2520to%2520build%2520intuition%2520on%2520predicting%250Athe%25203D%2520configurations%2520of%2520unseen%2520objects.%2520The%2520more%2520we%2520observe%2520the%2520object%2520motion%252C%250Athe%2520better%2520we%2520get%2520at%2520predicting%2520its%25203D%2520state%2520immediately.%2520Existing%2520systems%250Aeither%2520optimize%2520underlying%2520representations%2520from%2520multi-view%2520observations%2520or%250Atrain%2520a%2520feed-forward%2520predictor%2520from%2520supervised%2520datasets.%2520We%2520introduce%250APredict-Optimize-Distill%2520%2528POD%2529%252C%2520a%2520self-improving%2520framework%2520that%2520interleaves%250Aprediction%2520and%2520optimization%2520in%2520a%2520mutually%2520reinforcing%2520cycle%2520to%2520achieve%2520better%250A4D%2520object%2520understanding%2520with%2520increasing%2520observation%2520time.%2520Given%2520a%2520multi-view%250Aobject%2520scan%2520and%2520a%2520long-form%2520monocular%2520video%2520of%2520human-object%2520interaction%252C%2520POD%250Aiteratively%2520trains%2520a%2520neural%2520network%2520to%2520predict%2520local%2520part%2520poses%2520from%2520RGB%250Aframes%252C%2520uses%2520this%2520predictor%2520to%2520initialize%2520a%2520global%2520optimization%2520which%2520refines%250Aoutput%2520poses%2520through%2520inverse%2520rendering%252C%2520then%2520finally%2520distills%2520the%2520results%2520of%250Aoptimization%2520back%2520into%2520the%2520model%2520by%2520generating%2520synthetic%2520self-labeled%2520training%250Adata%2520from%2520novel%2520viewpoints.%2520Each%2520iteration%2520improves%2520both%2520the%2520predictive%2520model%250Aand%2520the%2520optimized%2520motion%2520trajectory%252C%2520creating%2520a%2520virtuous%2520cycle%2520that%2520bootstraps%250Aits%2520own%2520training%2520data%2520to%2520learn%2520about%2520the%2520pose%2520configurations%2520of%2520an%2520object.%2520We%250Aalso%2520introduce%2520a%2520quasi-multiview%2520mining%2520strategy%2520for%2520reducing%2520depth%2520ambiguity%250Aby%2520leveraging%2520long%2520video.%2520We%2520evaluate%2520POD%2520on%252014%2520real-world%2520and%25205%2520synthetic%250Aobjects%2520with%2520various%2520joint%2520types%252C%2520including%2520revolute%2520and%2520prismatic%2520joints%2520as%250Awell%2520as%2520multi-body%2520configurations%2520where%2520parts%2520detach%2520or%2520reattach%2520independently.%250APOD%2520demonstrates%2520significant%2520improvement%2520over%2520a%2520pure%2520optimization%2520baseline%250Awhich%2520gets%2520stuck%2520in%2520local%2520minima%252C%2520particularly%2520for%2520longer%2520videos.%2520We%2520also%2520find%250Athat%2520POD%2527s%2520performance%2520improves%2520with%2520both%2520video%2520length%2520and%2520successive%250Aiterations%2520of%2520the%2520self-improving%2520cycle%252C%2520highlighting%2520its%2520ability%2520to%2520scale%250Aperformance%2520with%2520additional%2520observations%2520and%2520looped%2520refinement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17441v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predict-Optimize-Distill%3A%20A%20Self-Improving%20Cycle%20for%204D%20Object%0A%20%20Understanding&entry.906535625=Mingxuan%20Wu%20and%20Huang%20Huang%20and%20Justin%20Kerr%20and%20Chung%20Min%20Kim%20and%20Anthony%20Zhang%20and%20Brent%20Yi%20and%20Angjoo%20Kanazawa&entry.1292438233=%20%20Humans%20can%20resort%20to%20long-form%20inspection%20to%20build%20intuition%20on%20predicting%0Athe%203D%20configurations%20of%20unseen%20objects.%20The%20more%20we%20observe%20the%20object%20motion%2C%0Athe%20better%20we%20get%20at%20predicting%20its%203D%20state%20immediately.%20Existing%20systems%0Aeither%20optimize%20underlying%20representations%20from%20multi-view%20observations%20or%0Atrain%20a%20feed-forward%20predictor%20from%20supervised%20datasets.%20We%20introduce%0APredict-Optimize-Distill%20%28POD%29%2C%20a%20self-improving%20framework%20that%20interleaves%0Aprediction%20and%20optimization%20in%20a%20mutually%20reinforcing%20cycle%20to%20achieve%20better%0A4D%20object%20understanding%20with%20increasing%20observation%20time.%20Given%20a%20multi-view%0Aobject%20scan%20and%20a%20long-form%20monocular%20video%20of%20human-object%20interaction%2C%20POD%0Aiteratively%20trains%20a%20neural%20network%20to%20predict%20local%20part%20poses%20from%20RGB%0Aframes%2C%20uses%20this%20predictor%20to%20initialize%20a%20global%20optimization%20which%20refines%0Aoutput%20poses%20through%20inverse%20rendering%2C%20then%20finally%20distills%20the%20results%20of%0Aoptimization%20back%20into%20the%20model%20by%20generating%20synthetic%20self-labeled%20training%0Adata%20from%20novel%20viewpoints.%20Each%20iteration%20improves%20both%20the%20predictive%20model%0Aand%20the%20optimized%20motion%20trajectory%2C%20creating%20a%20virtuous%20cycle%20that%20bootstraps%0Aits%20own%20training%20data%20to%20learn%20about%20the%20pose%20configurations%20of%20an%20object.%20We%0Aalso%20introduce%20a%20quasi-multiview%20mining%20strategy%20for%20reducing%20depth%20ambiguity%0Aby%20leveraging%20long%20video.%20We%20evaluate%20POD%20on%2014%20real-world%20and%205%20synthetic%0Aobjects%20with%20various%20joint%20types%2C%20including%20revolute%20and%20prismatic%20joints%20as%0Awell%20as%20multi-body%20configurations%20where%20parts%20detach%20or%20reattach%20independently.%0APOD%20demonstrates%20significant%20improvement%20over%20a%20pure%20optimization%20baseline%0Awhich%20gets%20stuck%20in%20local%20minima%2C%20particularly%20for%20longer%20videos.%20We%20also%20find%0Athat%20POD%27s%20performance%20improves%20with%20both%20video%20length%20and%20successive%0Aiterations%20of%20the%20self-improving%20cycle%2C%20highlighting%20its%20ability%20to%20scale%0Aperformance%20with%20additional%20observations%20and%20looped%20refinement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17441v1&entry.124074799=Read"},
{"title": "Unleashing the Power of Natural Audio Featuring Multiple Sound Sources", "author": "Xize Cheng and Slytherin Wang and Zehan Wang and Rongjie Huang and Tao Jin and Zhou Zhao", "abstract": "  Universal sound separation aims to extract clean audio tracks corresponding\nto distinct events from mixed audio, which is critical for artificial auditory\nperception. However, current methods heavily rely on artificially mixed audio\nfor training, which limits their ability to generalize to naturally mixed audio\ncollected in real-world environments. To overcome this limitation, we propose\nClearSep, an innovative framework that employs a data engine to decompose\ncomplex naturally mixed audio into multiple independent tracks, thereby\nallowing effective sound separation in real-world scenarios. We introduce two\nremix-based evaluation metrics to quantitatively assess separation quality and\nuse these metrics as thresholds to iteratively apply the data engine alongside\nmodel training, progressively optimizing separation performance. In addition,\nwe propose a series of training strategies tailored to these separated\nindependent tracks to make the best use of them. Extensive experiments\ndemonstrate that ClearSep achieves state-of-the-art performance across multiple\nsound separation tasks, highlighting its potential for advancing sound\nseparation in natural audio scenarios. For more examples and detailed results,\nplease visit our demo page at https://clearsep.github.io.\n", "link": "http://arxiv.org/abs/2504.17782v1", "date": "2025-04-24", "relevancy": 2.4026, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4881}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4767}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unleashing%20the%20Power%20of%20Natural%20Audio%20Featuring%20Multiple%20Sound%20Sources&body=Title%3A%20Unleashing%20the%20Power%20of%20Natural%20Audio%20Featuring%20Multiple%20Sound%20Sources%0AAuthor%3A%20Xize%20Cheng%20and%20Slytherin%20Wang%20and%20Zehan%20Wang%20and%20Rongjie%20Huang%20and%20Tao%20Jin%20and%20Zhou%20Zhao%0AAbstract%3A%20%20%20Universal%20sound%20separation%20aims%20to%20extract%20clean%20audio%20tracks%20corresponding%0Ato%20distinct%20events%20from%20mixed%20audio%2C%20which%20is%20critical%20for%20artificial%20auditory%0Aperception.%20However%2C%20current%20methods%20heavily%20rely%20on%20artificially%20mixed%20audio%0Afor%20training%2C%20which%20limits%20their%20ability%20to%20generalize%20to%20naturally%20mixed%20audio%0Acollected%20in%20real-world%20environments.%20To%20overcome%20this%20limitation%2C%20we%20propose%0AClearSep%2C%20an%20innovative%20framework%20that%20employs%20a%20data%20engine%20to%20decompose%0Acomplex%20naturally%20mixed%20audio%20into%20multiple%20independent%20tracks%2C%20thereby%0Aallowing%20effective%20sound%20separation%20in%20real-world%20scenarios.%20We%20introduce%20two%0Aremix-based%20evaluation%20metrics%20to%20quantitatively%20assess%20separation%20quality%20and%0Ause%20these%20metrics%20as%20thresholds%20to%20iteratively%20apply%20the%20data%20engine%20alongside%0Amodel%20training%2C%20progressively%20optimizing%20separation%20performance.%20In%20addition%2C%0Awe%20propose%20a%20series%20of%20training%20strategies%20tailored%20to%20these%20separated%0Aindependent%20tracks%20to%20make%20the%20best%20use%20of%20them.%20Extensive%20experiments%0Ademonstrate%20that%20ClearSep%20achieves%20state-of-the-art%20performance%20across%20multiple%0Asound%20separation%20tasks%2C%20highlighting%20its%20potential%20for%20advancing%20sound%0Aseparation%20in%20natural%20audio%20scenarios.%20For%20more%20examples%20and%20detailed%20results%2C%0Aplease%20visit%20our%20demo%20page%20at%20https%3A//clearsep.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17782v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnleashing%2520the%2520Power%2520of%2520Natural%2520Audio%2520Featuring%2520Multiple%2520Sound%2520Sources%26entry.906535625%3DXize%2520Cheng%2520and%2520Slytherin%2520Wang%2520and%2520Zehan%2520Wang%2520and%2520Rongjie%2520Huang%2520and%2520Tao%2520Jin%2520and%2520Zhou%2520Zhao%26entry.1292438233%3D%2520%2520Universal%2520sound%2520separation%2520aims%2520to%2520extract%2520clean%2520audio%2520tracks%2520corresponding%250Ato%2520distinct%2520events%2520from%2520mixed%2520audio%252C%2520which%2520is%2520critical%2520for%2520artificial%2520auditory%250Aperception.%2520However%252C%2520current%2520methods%2520heavily%2520rely%2520on%2520artificially%2520mixed%2520audio%250Afor%2520training%252C%2520which%2520limits%2520their%2520ability%2520to%2520generalize%2520to%2520naturally%2520mixed%2520audio%250Acollected%2520in%2520real-world%2520environments.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%250AClearSep%252C%2520an%2520innovative%2520framework%2520that%2520employs%2520a%2520data%2520engine%2520to%2520decompose%250Acomplex%2520naturally%2520mixed%2520audio%2520into%2520multiple%2520independent%2520tracks%252C%2520thereby%250Aallowing%2520effective%2520sound%2520separation%2520in%2520real-world%2520scenarios.%2520We%2520introduce%2520two%250Aremix-based%2520evaluation%2520metrics%2520to%2520quantitatively%2520assess%2520separation%2520quality%2520and%250Ause%2520these%2520metrics%2520as%2520thresholds%2520to%2520iteratively%2520apply%2520the%2520data%2520engine%2520alongside%250Amodel%2520training%252C%2520progressively%2520optimizing%2520separation%2520performance.%2520In%2520addition%252C%250Awe%2520propose%2520a%2520series%2520of%2520training%2520strategies%2520tailored%2520to%2520these%2520separated%250Aindependent%2520tracks%2520to%2520make%2520the%2520best%2520use%2520of%2520them.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520ClearSep%2520achieves%2520state-of-the-art%2520performance%2520across%2520multiple%250Asound%2520separation%2520tasks%252C%2520highlighting%2520its%2520potential%2520for%2520advancing%2520sound%250Aseparation%2520in%2520natural%2520audio%2520scenarios.%2520For%2520more%2520examples%2520and%2520detailed%2520results%252C%250Aplease%2520visit%2520our%2520demo%2520page%2520at%2520https%253A//clearsep.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17782v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unleashing%20the%20Power%20of%20Natural%20Audio%20Featuring%20Multiple%20Sound%20Sources&entry.906535625=Xize%20Cheng%20and%20Slytherin%20Wang%20and%20Zehan%20Wang%20and%20Rongjie%20Huang%20and%20Tao%20Jin%20and%20Zhou%20Zhao&entry.1292438233=%20%20Universal%20sound%20separation%20aims%20to%20extract%20clean%20audio%20tracks%20corresponding%0Ato%20distinct%20events%20from%20mixed%20audio%2C%20which%20is%20critical%20for%20artificial%20auditory%0Aperception.%20However%2C%20current%20methods%20heavily%20rely%20on%20artificially%20mixed%20audio%0Afor%20training%2C%20which%20limits%20their%20ability%20to%20generalize%20to%20naturally%20mixed%20audio%0Acollected%20in%20real-world%20environments.%20To%20overcome%20this%20limitation%2C%20we%20propose%0AClearSep%2C%20an%20innovative%20framework%20that%20employs%20a%20data%20engine%20to%20decompose%0Acomplex%20naturally%20mixed%20audio%20into%20multiple%20independent%20tracks%2C%20thereby%0Aallowing%20effective%20sound%20separation%20in%20real-world%20scenarios.%20We%20introduce%20two%0Aremix-based%20evaluation%20metrics%20to%20quantitatively%20assess%20separation%20quality%20and%0Ause%20these%20metrics%20as%20thresholds%20to%20iteratively%20apply%20the%20data%20engine%20alongside%0Amodel%20training%2C%20progressively%20optimizing%20separation%20performance.%20In%20addition%2C%0Awe%20propose%20a%20series%20of%20training%20strategies%20tailored%20to%20these%20separated%0Aindependent%20tracks%20to%20make%20the%20best%20use%20of%20them.%20Extensive%20experiments%0Ademonstrate%20that%20ClearSep%20achieves%20state-of-the-art%20performance%20across%20multiple%0Asound%20separation%20tasks%2C%20highlighting%20its%20potential%20for%20advancing%20sound%0Aseparation%20in%20natural%20audio%20scenarios.%20For%20more%20examples%20and%20detailed%20results%2C%0Aplease%20visit%20our%20demo%20page%20at%20https%3A//clearsep.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17782v1&entry.124074799=Read"},
{"title": "Text-to-Image Alignment in Denoising-Based Models through Step Selection", "author": "Paul Grimal and Herv\u00e9 Le Borgne and Olivier Ferret", "abstract": "  Visual generative AI models often encounter challenges related to text-image\nalignment and reasoning limitations. This paper presents a novel method for\nselectively enhancing the signal at critical denoising steps, optimizing image\ngeneration based on input semantics. Our approach addresses the shortcomings of\nearly-stage signal modifications, demonstrating that adjustments made at later\nstages yield superior results. We conduct extensive experiments to validate the\neffectiveness of our method in producing semantically aligned images on\nDiffusion and Flow Matching model, achieving state-of-the-art performance. Our\nresults highlight the importance of a judicious choice of sampling stage to\nimprove performance and overall image alignment.\n", "link": "http://arxiv.org/abs/2504.17525v1", "date": "2025-04-24", "relevancy": 2.4015, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6097}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6082}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-to-Image%20Alignment%20in%20Denoising-Based%20Models%20through%20Step%20Selection&body=Title%3A%20Text-to-Image%20Alignment%20in%20Denoising-Based%20Models%20through%20Step%20Selection%0AAuthor%3A%20Paul%20Grimal%20and%20Herv%C3%A9%20Le%20Borgne%20and%20Olivier%20Ferret%0AAbstract%3A%20%20%20Visual%20generative%20AI%20models%20often%20encounter%20challenges%20related%20to%20text-image%0Aalignment%20and%20reasoning%20limitations.%20This%20paper%20presents%20a%20novel%20method%20for%0Aselectively%20enhancing%20the%20signal%20at%20critical%20denoising%20steps%2C%20optimizing%20image%0Ageneration%20based%20on%20input%20semantics.%20Our%20approach%20addresses%20the%20shortcomings%20of%0Aearly-stage%20signal%20modifications%2C%20demonstrating%20that%20adjustments%20made%20at%20later%0Astages%20yield%20superior%20results.%20We%20conduct%20extensive%20experiments%20to%20validate%20the%0Aeffectiveness%20of%20our%20method%20in%20producing%20semantically%20aligned%20images%20on%0ADiffusion%20and%20Flow%20Matching%20model%2C%20achieving%20state-of-the-art%20performance.%20Our%0Aresults%20highlight%20the%20importance%20of%20a%20judicious%20choice%20of%20sampling%20stage%20to%0Aimprove%20performance%20and%20overall%20image%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17525v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-to-Image%2520Alignment%2520in%2520Denoising-Based%2520Models%2520through%2520Step%2520Selection%26entry.906535625%3DPaul%2520Grimal%2520and%2520Herv%25C3%25A9%2520Le%2520Borgne%2520and%2520Olivier%2520Ferret%26entry.1292438233%3D%2520%2520Visual%2520generative%2520AI%2520models%2520often%2520encounter%2520challenges%2520related%2520to%2520text-image%250Aalignment%2520and%2520reasoning%2520limitations.%2520This%2520paper%2520presents%2520a%2520novel%2520method%2520for%250Aselectively%2520enhancing%2520the%2520signal%2520at%2520critical%2520denoising%2520steps%252C%2520optimizing%2520image%250Ageneration%2520based%2520on%2520input%2520semantics.%2520Our%2520approach%2520addresses%2520the%2520shortcomings%2520of%250Aearly-stage%2520signal%2520modifications%252C%2520demonstrating%2520that%2520adjustments%2520made%2520at%2520later%250Astages%2520yield%2520superior%2520results.%2520We%2520conduct%2520extensive%2520experiments%2520to%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520method%2520in%2520producing%2520semantically%2520aligned%2520images%2520on%250ADiffusion%2520and%2520Flow%2520Matching%2520model%252C%2520achieving%2520state-of-the-art%2520performance.%2520Our%250Aresults%2520highlight%2520the%2520importance%2520of%2520a%2520judicious%2520choice%2520of%2520sampling%2520stage%2520to%250Aimprove%2520performance%2520and%2520overall%2520image%2520alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17525v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-to-Image%20Alignment%20in%20Denoising-Based%20Models%20through%20Step%20Selection&entry.906535625=Paul%20Grimal%20and%20Herv%C3%A9%20Le%20Borgne%20and%20Olivier%20Ferret&entry.1292438233=%20%20Visual%20generative%20AI%20models%20often%20encounter%20challenges%20related%20to%20text-image%0Aalignment%20and%20reasoning%20limitations.%20This%20paper%20presents%20a%20novel%20method%20for%0Aselectively%20enhancing%20the%20signal%20at%20critical%20denoising%20steps%2C%20optimizing%20image%0Ageneration%20based%20on%20input%20semantics.%20Our%20approach%20addresses%20the%20shortcomings%20of%0Aearly-stage%20signal%20modifications%2C%20demonstrating%20that%20adjustments%20made%20at%20later%0Astages%20yield%20superior%20results.%20We%20conduct%20extensive%20experiments%20to%20validate%20the%0Aeffectiveness%20of%20our%20method%20in%20producing%20semantically%20aligned%20images%20on%0ADiffusion%20and%20Flow%20Matching%20model%2C%20achieving%20state-of-the-art%20performance.%20Our%0Aresults%20highlight%20the%20importance%20of%20a%20judicious%20choice%20of%20sampling%20stage%20to%0Aimprove%20performance%20and%20overall%20image%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17525v1&entry.124074799=Read"},
{"title": "Latent Representations for Visual Proprioception in Inexpensive Robots", "author": "Sahara Sheikholeslami and Ladislau B\u00f6l\u00f6ni", "abstract": "  Robotic manipulation requires explicit or implicit knowledge of the robot's\njoint positions. Precise proprioception is standard in high-quality industrial\nrobots but is often unavailable in inexpensive robots operating in unstructured\nenvironments. In this paper, we ask: to what extent can a fast, single-pass\nregression architecture perform visual proprioception from a single external\ncamera image, available even in the simplest manipulation settings? We explore\nseveral latent representations, including CNNs, VAEs, ViTs, and bags of\nuncalibrated fiducial markers, using fine-tuning techniques adapted to the\nlimited data available. We evaluate the achievable accuracy through experiments\non an inexpensive 6-DoF robot.\n", "link": "http://arxiv.org/abs/2504.14634v2", "date": "2025-04-24", "relevancy": 2.39, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.685}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5894}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20Representations%20for%20Visual%20Proprioception%20in%20Inexpensive%20Robots&body=Title%3A%20Latent%20Representations%20for%20Visual%20Proprioception%20in%20Inexpensive%20Robots%0AAuthor%3A%20Sahara%20Sheikholeslami%20and%20Ladislau%20B%C3%B6l%C3%B6ni%0AAbstract%3A%20%20%20Robotic%20manipulation%20requires%20explicit%20or%20implicit%20knowledge%20of%20the%20robot%27s%0Ajoint%20positions.%20Precise%20proprioception%20is%20standard%20in%20high-quality%20industrial%0Arobots%20but%20is%20often%20unavailable%20in%20inexpensive%20robots%20operating%20in%20unstructured%0Aenvironments.%20In%20this%20paper%2C%20we%20ask%3A%20to%20what%20extent%20can%20a%20fast%2C%20single-pass%0Aregression%20architecture%20perform%20visual%20proprioception%20from%20a%20single%20external%0Acamera%20image%2C%20available%20even%20in%20the%20simplest%20manipulation%20settings%3F%20We%20explore%0Aseveral%20latent%20representations%2C%20including%20CNNs%2C%20VAEs%2C%20ViTs%2C%20and%20bags%20of%0Auncalibrated%20fiducial%20markers%2C%20using%20fine-tuning%20techniques%20adapted%20to%20the%0Alimited%20data%20available.%20We%20evaluate%20the%20achievable%20accuracy%20through%20experiments%0Aon%20an%20inexpensive%206-DoF%20robot.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.14634v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520Representations%2520for%2520Visual%2520Proprioception%2520in%2520Inexpensive%2520Robots%26entry.906535625%3DSahara%2520Sheikholeslami%2520and%2520Ladislau%2520B%25C3%25B6l%25C3%25B6ni%26entry.1292438233%3D%2520%2520Robotic%2520manipulation%2520requires%2520explicit%2520or%2520implicit%2520knowledge%2520of%2520the%2520robot%2527s%250Ajoint%2520positions.%2520Precise%2520proprioception%2520is%2520standard%2520in%2520high-quality%2520industrial%250Arobots%2520but%2520is%2520often%2520unavailable%2520in%2520inexpensive%2520robots%2520operating%2520in%2520unstructured%250Aenvironments.%2520In%2520this%2520paper%252C%2520we%2520ask%253A%2520to%2520what%2520extent%2520can%2520a%2520fast%252C%2520single-pass%250Aregression%2520architecture%2520perform%2520visual%2520proprioception%2520from%2520a%2520single%2520external%250Acamera%2520image%252C%2520available%2520even%2520in%2520the%2520simplest%2520manipulation%2520settings%253F%2520We%2520explore%250Aseveral%2520latent%2520representations%252C%2520including%2520CNNs%252C%2520VAEs%252C%2520ViTs%252C%2520and%2520bags%2520of%250Auncalibrated%2520fiducial%2520markers%252C%2520using%2520fine-tuning%2520techniques%2520adapted%2520to%2520the%250Alimited%2520data%2520available.%2520We%2520evaluate%2520the%2520achievable%2520accuracy%2520through%2520experiments%250Aon%2520an%2520inexpensive%25206-DoF%2520robot.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.14634v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Representations%20for%20Visual%20Proprioception%20in%20Inexpensive%20Robots&entry.906535625=Sahara%20Sheikholeslami%20and%20Ladislau%20B%C3%B6l%C3%B6ni&entry.1292438233=%20%20Robotic%20manipulation%20requires%20explicit%20or%20implicit%20knowledge%20of%20the%20robot%27s%0Ajoint%20positions.%20Precise%20proprioception%20is%20standard%20in%20high-quality%20industrial%0Arobots%20but%20is%20often%20unavailable%20in%20inexpensive%20robots%20operating%20in%20unstructured%0Aenvironments.%20In%20this%20paper%2C%20we%20ask%3A%20to%20what%20extent%20can%20a%20fast%2C%20single-pass%0Aregression%20architecture%20perform%20visual%20proprioception%20from%20a%20single%20external%0Acamera%20image%2C%20available%20even%20in%20the%20simplest%20manipulation%20settings%3F%20We%20explore%0Aseveral%20latent%20representations%2C%20including%20CNNs%2C%20VAEs%2C%20ViTs%2C%20and%20bags%20of%0Auncalibrated%20fiducial%20markers%2C%20using%20fine-tuning%20techniques%20adapted%20to%20the%0Alimited%20data%20available.%20We%20evaluate%20the%20achievable%20accuracy%20through%20experiments%0Aon%20an%20inexpensive%206-DoF%20robot.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.14634v2&entry.124074799=Read"},
{"title": "GRANITE : a Byzantine-Resilient Dynamic Gossip Learning Framework", "author": "Yacine Belal and Mohamed Maouche and Sonia Ben Mokhtar and Anthony Simonet-Boulogne", "abstract": "  Gossip Learning (GL) is a decentralized learning paradigm where users\niteratively exchange and aggregate models with a small set of neighboring\npeers. Recent GL approaches rely on dynamic communication graphs built and\nmaintained using Random Peer Sampling (RPS) protocols. Thanks to graph\ndynamics, GL can achieve fast convergence even over extremely sparse\ntopologies. However, the robustness of GL over dy- namic graphs to Byzantine\n(model poisoning) attacks remains unaddressed especially when Byzantine nodes\nattack the RPS protocol to scale up model poisoning. We address this issue by\nintroducing GRANITE, a framework for robust learning over sparse, dynamic\ngraphs in the presence of a fraction of Byzantine nodes. GRANITE relies on two\nkey components (i) a History-aware Byzantine-resilient Peer Sampling protocol\n(HaPS), which tracks previously encountered identifiers to reduce adversarial\ninfluence over time, and (ii) an Adaptive Probabilistic Threshold (APT), which\nleverages an estimate of Byzantine presence to set aggregation thresholds with\nformal guarantees. Empirical results confirm that GRANITE maintains convergence\nwith up to 30% Byzantine nodes, improves learning speed via adaptive filtering\nof poisoned models and obtains these results in up to 9 times sparser graphs\nthan dictated by current theory.\n", "link": "http://arxiv.org/abs/2504.17471v1", "date": "2025-04-24", "relevancy": 2.3898, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.491}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4757}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRANITE%20%3A%20a%20Byzantine-Resilient%20Dynamic%20Gossip%20Learning%20Framework&body=Title%3A%20GRANITE%20%3A%20a%20Byzantine-Resilient%20Dynamic%20Gossip%20Learning%20Framework%0AAuthor%3A%20Yacine%20Belal%20and%20Mohamed%20Maouche%20and%20Sonia%20Ben%20Mokhtar%20and%20Anthony%20Simonet-Boulogne%0AAbstract%3A%20%20%20Gossip%20Learning%20%28GL%29%20is%20a%20decentralized%20learning%20paradigm%20where%20users%0Aiteratively%20exchange%20and%20aggregate%20models%20with%20a%20small%20set%20of%20neighboring%0Apeers.%20Recent%20GL%20approaches%20rely%20on%20dynamic%20communication%20graphs%20built%20and%0Amaintained%20using%20Random%20Peer%20Sampling%20%28RPS%29%20protocols.%20Thanks%20to%20graph%0Adynamics%2C%20GL%20can%20achieve%20fast%20convergence%20even%20over%20extremely%20sparse%0Atopologies.%20However%2C%20the%20robustness%20of%20GL%20over%20dy-%20namic%20graphs%20to%20Byzantine%0A%28model%20poisoning%29%20attacks%20remains%20unaddressed%20especially%20when%20Byzantine%20nodes%0Aattack%20the%20RPS%20protocol%20to%20scale%20up%20model%20poisoning.%20We%20address%20this%20issue%20by%0Aintroducing%20GRANITE%2C%20a%20framework%20for%20robust%20learning%20over%20sparse%2C%20dynamic%0Agraphs%20in%20the%20presence%20of%20a%20fraction%20of%20Byzantine%20nodes.%20GRANITE%20relies%20on%20two%0Akey%20components%20%28i%29%20a%20History-aware%20Byzantine-resilient%20Peer%20Sampling%20protocol%0A%28HaPS%29%2C%20which%20tracks%20previously%20encountered%20identifiers%20to%20reduce%20adversarial%0Ainfluence%20over%20time%2C%20and%20%28ii%29%20an%20Adaptive%20Probabilistic%20Threshold%20%28APT%29%2C%20which%0Aleverages%20an%20estimate%20of%20Byzantine%20presence%20to%20set%20aggregation%20thresholds%20with%0Aformal%20guarantees.%20Empirical%20results%20confirm%20that%20GRANITE%20maintains%20convergence%0Awith%20up%20to%2030%25%20Byzantine%20nodes%2C%20improves%20learning%20speed%20via%20adaptive%20filtering%0Aof%20poisoned%20models%20and%20obtains%20these%20results%20in%20up%20to%209%20times%20sparser%20graphs%0Athan%20dictated%20by%20current%20theory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17471v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRANITE%2520%253A%2520a%2520Byzantine-Resilient%2520Dynamic%2520Gossip%2520Learning%2520Framework%26entry.906535625%3DYacine%2520Belal%2520and%2520Mohamed%2520Maouche%2520and%2520Sonia%2520Ben%2520Mokhtar%2520and%2520Anthony%2520Simonet-Boulogne%26entry.1292438233%3D%2520%2520Gossip%2520Learning%2520%2528GL%2529%2520is%2520a%2520decentralized%2520learning%2520paradigm%2520where%2520users%250Aiteratively%2520exchange%2520and%2520aggregate%2520models%2520with%2520a%2520small%2520set%2520of%2520neighboring%250Apeers.%2520Recent%2520GL%2520approaches%2520rely%2520on%2520dynamic%2520communication%2520graphs%2520built%2520and%250Amaintained%2520using%2520Random%2520Peer%2520Sampling%2520%2528RPS%2529%2520protocols.%2520Thanks%2520to%2520graph%250Adynamics%252C%2520GL%2520can%2520achieve%2520fast%2520convergence%2520even%2520over%2520extremely%2520sparse%250Atopologies.%2520However%252C%2520the%2520robustness%2520of%2520GL%2520over%2520dy-%2520namic%2520graphs%2520to%2520Byzantine%250A%2528model%2520poisoning%2529%2520attacks%2520remains%2520unaddressed%2520especially%2520when%2520Byzantine%2520nodes%250Aattack%2520the%2520RPS%2520protocol%2520to%2520scale%2520up%2520model%2520poisoning.%2520We%2520address%2520this%2520issue%2520by%250Aintroducing%2520GRANITE%252C%2520a%2520framework%2520for%2520robust%2520learning%2520over%2520sparse%252C%2520dynamic%250Agraphs%2520in%2520the%2520presence%2520of%2520a%2520fraction%2520of%2520Byzantine%2520nodes.%2520GRANITE%2520relies%2520on%2520two%250Akey%2520components%2520%2528i%2529%2520a%2520History-aware%2520Byzantine-resilient%2520Peer%2520Sampling%2520protocol%250A%2528HaPS%2529%252C%2520which%2520tracks%2520previously%2520encountered%2520identifiers%2520to%2520reduce%2520adversarial%250Ainfluence%2520over%2520time%252C%2520and%2520%2528ii%2529%2520an%2520Adaptive%2520Probabilistic%2520Threshold%2520%2528APT%2529%252C%2520which%250Aleverages%2520an%2520estimate%2520of%2520Byzantine%2520presence%2520to%2520set%2520aggregation%2520thresholds%2520with%250Aformal%2520guarantees.%2520Empirical%2520results%2520confirm%2520that%2520GRANITE%2520maintains%2520convergence%250Awith%2520up%2520to%252030%2525%2520Byzantine%2520nodes%252C%2520improves%2520learning%2520speed%2520via%2520adaptive%2520filtering%250Aof%2520poisoned%2520models%2520and%2520obtains%2520these%2520results%2520in%2520up%2520to%25209%2520times%2520sparser%2520graphs%250Athan%2520dictated%2520by%2520current%2520theory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17471v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRANITE%20%3A%20a%20Byzantine-Resilient%20Dynamic%20Gossip%20Learning%20Framework&entry.906535625=Yacine%20Belal%20and%20Mohamed%20Maouche%20and%20Sonia%20Ben%20Mokhtar%20and%20Anthony%20Simonet-Boulogne&entry.1292438233=%20%20Gossip%20Learning%20%28GL%29%20is%20a%20decentralized%20learning%20paradigm%20where%20users%0Aiteratively%20exchange%20and%20aggregate%20models%20with%20a%20small%20set%20of%20neighboring%0Apeers.%20Recent%20GL%20approaches%20rely%20on%20dynamic%20communication%20graphs%20built%20and%0Amaintained%20using%20Random%20Peer%20Sampling%20%28RPS%29%20protocols.%20Thanks%20to%20graph%0Adynamics%2C%20GL%20can%20achieve%20fast%20convergence%20even%20over%20extremely%20sparse%0Atopologies.%20However%2C%20the%20robustness%20of%20GL%20over%20dy-%20namic%20graphs%20to%20Byzantine%0A%28model%20poisoning%29%20attacks%20remains%20unaddressed%20especially%20when%20Byzantine%20nodes%0Aattack%20the%20RPS%20protocol%20to%20scale%20up%20model%20poisoning.%20We%20address%20this%20issue%20by%0Aintroducing%20GRANITE%2C%20a%20framework%20for%20robust%20learning%20over%20sparse%2C%20dynamic%0Agraphs%20in%20the%20presence%20of%20a%20fraction%20of%20Byzantine%20nodes.%20GRANITE%20relies%20on%20two%0Akey%20components%20%28i%29%20a%20History-aware%20Byzantine-resilient%20Peer%20Sampling%20protocol%0A%28HaPS%29%2C%20which%20tracks%20previously%20encountered%20identifiers%20to%20reduce%20adversarial%0Ainfluence%20over%20time%2C%20and%20%28ii%29%20an%20Adaptive%20Probabilistic%20Threshold%20%28APT%29%2C%20which%0Aleverages%20an%20estimate%20of%20Byzantine%20presence%20to%20set%20aggregation%20thresholds%20with%0Aformal%20guarantees.%20Empirical%20results%20confirm%20that%20GRANITE%20maintains%20convergence%0Awith%20up%20to%2030%25%20Byzantine%20nodes%2C%20improves%20learning%20speed%20via%20adaptive%20filtering%0Aof%20poisoned%20models%20and%20obtains%20these%20results%20in%20up%20to%209%20times%20sparser%20graphs%0Athan%20dictated%20by%20current%20theory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17471v1&entry.124074799=Read"},
{"title": "Artifact detection and localization in single-channel mobile EEG for\n  sleep research using deep learning and attention mechanisms", "author": "Khrystyna Semkiv and Jia Zhang and Maria Laura Ferster and Walter Karlen", "abstract": "  Artifacts in the electroencephalogram (EEG) degrade signal quality and impact\nthe analysis of brain activity. Current methods for detecting artifacts in\nsleep EEG rely on simple threshold-based algorithms that require manual\nintervention, which is time-consuming and impractical due to the vast volume of\ndata that novel mobile recording systems generate. We propose a convolutional\nneural network (CNN) model incorporating a convolutional block attention module\n(CNN-CBAM) to detect and identify the location of artifacts in the sleep EEG\nwith attention maps. We benchmarked this model against six other machine\nlearning and signal processing approaches. We trained/tuned all models on 72\nmanually annotated EEG recordings obtained during home-based monitoring from 18\nhealthy participants with a mean (SD) age of 68.05 y ($\\pm$5.02). We tested\nthem on 26 separate recordings from 6 healthy participants with a mean (SD) age\nof 68.33 y ($\\pm$4.08), with contained artifacts in 4\\% of epochs. CNN-CBAM\nachieved the highest area under the receiver operating characteristic curve\n(0.88), sensitivity (0.81), and specificity (0.86) when compared to the other\napproaches. The attention maps from CNN-CBAM localized artifacts within the\nepoch with a sensitivity of 0.71 and specificity of 0.67. This work\ndemonstrates the feasibility of automating the detection and localization of\nartifacts in wearable sleep EEG.\n", "link": "http://arxiv.org/abs/2504.08469v2", "date": "2025-04-24", "relevancy": 2.3801, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4836}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4748}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Artifact%20detection%20and%20localization%20in%20single-channel%20mobile%20EEG%20for%0A%20%20sleep%20research%20using%20deep%20learning%20and%20attention%20mechanisms&body=Title%3A%20Artifact%20detection%20and%20localization%20in%20single-channel%20mobile%20EEG%20for%0A%20%20sleep%20research%20using%20deep%20learning%20and%20attention%20mechanisms%0AAuthor%3A%20Khrystyna%20Semkiv%20and%20Jia%20Zhang%20and%20Maria%20Laura%20Ferster%20and%20Walter%20Karlen%0AAbstract%3A%20%20%20Artifacts%20in%20the%20electroencephalogram%20%28EEG%29%20degrade%20signal%20quality%20and%20impact%0Athe%20analysis%20of%20brain%20activity.%20Current%20methods%20for%20detecting%20artifacts%20in%0Asleep%20EEG%20rely%20on%20simple%20threshold-based%20algorithms%20that%20require%20manual%0Aintervention%2C%20which%20is%20time-consuming%20and%20impractical%20due%20to%20the%20vast%20volume%20of%0Adata%20that%20novel%20mobile%20recording%20systems%20generate.%20We%20propose%20a%20convolutional%0Aneural%20network%20%28CNN%29%20model%20incorporating%20a%20convolutional%20block%20attention%20module%0A%28CNN-CBAM%29%20to%20detect%20and%20identify%20the%20location%20of%20artifacts%20in%20the%20sleep%20EEG%0Awith%20attention%20maps.%20We%20benchmarked%20this%20model%20against%20six%20other%20machine%0Alearning%20and%20signal%20processing%20approaches.%20We%20trained/tuned%20all%20models%20on%2072%0Amanually%20annotated%20EEG%20recordings%20obtained%20during%20home-based%20monitoring%20from%2018%0Ahealthy%20participants%20with%20a%20mean%20%28SD%29%20age%20of%2068.05%20y%20%28%24%5Cpm%245.02%29.%20We%20tested%0Athem%20on%2026%20separate%20recordings%20from%206%20healthy%20participants%20with%20a%20mean%20%28SD%29%20age%0Aof%2068.33%20y%20%28%24%5Cpm%244.08%29%2C%20with%20contained%20artifacts%20in%204%5C%25%20of%20epochs.%20CNN-CBAM%0Aachieved%20the%20highest%20area%20under%20the%20receiver%20operating%20characteristic%20curve%0A%280.88%29%2C%20sensitivity%20%280.81%29%2C%20and%20specificity%20%280.86%29%20when%20compared%20to%20the%20other%0Aapproaches.%20The%20attention%20maps%20from%20CNN-CBAM%20localized%20artifacts%20within%20the%0Aepoch%20with%20a%20sensitivity%20of%200.71%20and%20specificity%20of%200.67.%20This%20work%0Ademonstrates%20the%20feasibility%20of%20automating%20the%20detection%20and%20localization%20of%0Aartifacts%20in%20wearable%20sleep%20EEG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.08469v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArtifact%2520detection%2520and%2520localization%2520in%2520single-channel%2520mobile%2520EEG%2520for%250A%2520%2520sleep%2520research%2520using%2520deep%2520learning%2520and%2520attention%2520mechanisms%26entry.906535625%3DKhrystyna%2520Semkiv%2520and%2520Jia%2520Zhang%2520and%2520Maria%2520Laura%2520Ferster%2520and%2520Walter%2520Karlen%26entry.1292438233%3D%2520%2520Artifacts%2520in%2520the%2520electroencephalogram%2520%2528EEG%2529%2520degrade%2520signal%2520quality%2520and%2520impact%250Athe%2520analysis%2520of%2520brain%2520activity.%2520Current%2520methods%2520for%2520detecting%2520artifacts%2520in%250Asleep%2520EEG%2520rely%2520on%2520simple%2520threshold-based%2520algorithms%2520that%2520require%2520manual%250Aintervention%252C%2520which%2520is%2520time-consuming%2520and%2520impractical%2520due%2520to%2520the%2520vast%2520volume%2520of%250Adata%2520that%2520novel%2520mobile%2520recording%2520systems%2520generate.%2520We%2520propose%2520a%2520convolutional%250Aneural%2520network%2520%2528CNN%2529%2520model%2520incorporating%2520a%2520convolutional%2520block%2520attention%2520module%250A%2528CNN-CBAM%2529%2520to%2520detect%2520and%2520identify%2520the%2520location%2520of%2520artifacts%2520in%2520the%2520sleep%2520EEG%250Awith%2520attention%2520maps.%2520We%2520benchmarked%2520this%2520model%2520against%2520six%2520other%2520machine%250Alearning%2520and%2520signal%2520processing%2520approaches.%2520We%2520trained/tuned%2520all%2520models%2520on%252072%250Amanually%2520annotated%2520EEG%2520recordings%2520obtained%2520during%2520home-based%2520monitoring%2520from%252018%250Ahealthy%2520participants%2520with%2520a%2520mean%2520%2528SD%2529%2520age%2520of%252068.05%2520y%2520%2528%2524%255Cpm%25245.02%2529.%2520We%2520tested%250Athem%2520on%252026%2520separate%2520recordings%2520from%25206%2520healthy%2520participants%2520with%2520a%2520mean%2520%2528SD%2529%2520age%250Aof%252068.33%2520y%2520%2528%2524%255Cpm%25244.08%2529%252C%2520with%2520contained%2520artifacts%2520in%25204%255C%2525%2520of%2520epochs.%2520CNN-CBAM%250Aachieved%2520the%2520highest%2520area%2520under%2520the%2520receiver%2520operating%2520characteristic%2520curve%250A%25280.88%2529%252C%2520sensitivity%2520%25280.81%2529%252C%2520and%2520specificity%2520%25280.86%2529%2520when%2520compared%2520to%2520the%2520other%250Aapproaches.%2520The%2520attention%2520maps%2520from%2520CNN-CBAM%2520localized%2520artifacts%2520within%2520the%250Aepoch%2520with%2520a%2520sensitivity%2520of%25200.71%2520and%2520specificity%2520of%25200.67.%2520This%2520work%250Ademonstrates%2520the%2520feasibility%2520of%2520automating%2520the%2520detection%2520and%2520localization%2520of%250Aartifacts%2520in%2520wearable%2520sleep%2520EEG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.08469v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Artifact%20detection%20and%20localization%20in%20single-channel%20mobile%20EEG%20for%0A%20%20sleep%20research%20using%20deep%20learning%20and%20attention%20mechanisms&entry.906535625=Khrystyna%20Semkiv%20and%20Jia%20Zhang%20and%20Maria%20Laura%20Ferster%20and%20Walter%20Karlen&entry.1292438233=%20%20Artifacts%20in%20the%20electroencephalogram%20%28EEG%29%20degrade%20signal%20quality%20and%20impact%0Athe%20analysis%20of%20brain%20activity.%20Current%20methods%20for%20detecting%20artifacts%20in%0Asleep%20EEG%20rely%20on%20simple%20threshold-based%20algorithms%20that%20require%20manual%0Aintervention%2C%20which%20is%20time-consuming%20and%20impractical%20due%20to%20the%20vast%20volume%20of%0Adata%20that%20novel%20mobile%20recording%20systems%20generate.%20We%20propose%20a%20convolutional%0Aneural%20network%20%28CNN%29%20model%20incorporating%20a%20convolutional%20block%20attention%20module%0A%28CNN-CBAM%29%20to%20detect%20and%20identify%20the%20location%20of%20artifacts%20in%20the%20sleep%20EEG%0Awith%20attention%20maps.%20We%20benchmarked%20this%20model%20against%20six%20other%20machine%0Alearning%20and%20signal%20processing%20approaches.%20We%20trained/tuned%20all%20models%20on%2072%0Amanually%20annotated%20EEG%20recordings%20obtained%20during%20home-based%20monitoring%20from%2018%0Ahealthy%20participants%20with%20a%20mean%20%28SD%29%20age%20of%2068.05%20y%20%28%24%5Cpm%245.02%29.%20We%20tested%0Athem%20on%2026%20separate%20recordings%20from%206%20healthy%20participants%20with%20a%20mean%20%28SD%29%20age%0Aof%2068.33%20y%20%28%24%5Cpm%244.08%29%2C%20with%20contained%20artifacts%20in%204%5C%25%20of%20epochs.%20CNN-CBAM%0Aachieved%20the%20highest%20area%20under%20the%20receiver%20operating%20characteristic%20curve%0A%280.88%29%2C%20sensitivity%20%280.81%29%2C%20and%20specificity%20%280.86%29%20when%20compared%20to%20the%20other%0Aapproaches.%20The%20attention%20maps%20from%20CNN-CBAM%20localized%20artifacts%20within%20the%0Aepoch%20with%20a%20sensitivity%20of%200.71%20and%20specificity%20of%200.67.%20This%20work%0Ademonstrates%20the%20feasibility%20of%20automating%20the%20detection%20and%20localization%20of%0Aartifacts%20in%20wearable%20sleep%20EEG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.08469v2&entry.124074799=Read"},
{"title": "Bias-Eliminated PnP for Stereo Visual Odometry: Provably Consistent and\n  Large-Scale Localization", "author": "Guangyang Zeng and Yuan Shen and Ziyang Hong and Yuze Hong and Viorela Ila and Guodong Shi and Junfeng Wu", "abstract": "  In this paper, we first present a bias-eliminated weighted (Bias-Eli-W)\nperspective-n-point (PnP) estimator for stereo visual odometry (VO) with\nprovable consistency. Specifically, leveraging statistical theory, we develop\nan asymptotically unbiased and $\\sqrt {n}$-consistent PnP estimator that\naccounts for varying 3D triangulation uncertainties, ensuring that the relative\npose estimate converges to the ground truth as the number of features\nincreases. Next, on the stereo VO pipeline side, we propose a framework that\ncontinuously triangulates contemporary features for tracking new frames,\neffectively decoupling temporal dependencies between pose and 3D point errors.\nWe integrate the Bias-Eli-W PnP estimator into the proposed stereo VO pipeline,\ncreating a synergistic effect that enhances the suppression of pose estimation\nerrors. We validate the performance of our method on the KITTI and Oxford\nRobotCar datasets. Experimental results demonstrate that our method: 1)\nachieves significant improvements in both relative pose error and absolute\ntrajectory error in large-scale environments; 2) provides reliable localization\nunder erratic and unpredictable robot motions. The successful implementation of\nthe Bias-Eli-W PnP in stereo VO indicates the importance of information\nscreening in robotic estimation tasks with high-uncertainty measurements,\nshedding light on diverse applications where PnP is a key ingredient.\n", "link": "http://arxiv.org/abs/2504.17410v1", "date": "2025-04-24", "relevancy": 2.3788, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6174}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5831}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bias-Eliminated%20PnP%20for%20Stereo%20Visual%20Odometry%3A%20Provably%20Consistent%20and%0A%20%20Large-Scale%20Localization&body=Title%3A%20Bias-Eliminated%20PnP%20for%20Stereo%20Visual%20Odometry%3A%20Provably%20Consistent%20and%0A%20%20Large-Scale%20Localization%0AAuthor%3A%20Guangyang%20Zeng%20and%20Yuan%20Shen%20and%20Ziyang%20Hong%20and%20Yuze%20Hong%20and%20Viorela%20Ila%20and%20Guodong%20Shi%20and%20Junfeng%20Wu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20first%20present%20a%20bias-eliminated%20weighted%20%28Bias-Eli-W%29%0Aperspective-n-point%20%28PnP%29%20estimator%20for%20stereo%20visual%20odometry%20%28VO%29%20with%0Aprovable%20consistency.%20Specifically%2C%20leveraging%20statistical%20theory%2C%20we%20develop%0Aan%20asymptotically%20unbiased%20and%20%24%5Csqrt%20%7Bn%7D%24-consistent%20PnP%20estimator%20that%0Aaccounts%20for%20varying%203D%20triangulation%20uncertainties%2C%20ensuring%20that%20the%20relative%0Apose%20estimate%20converges%20to%20the%20ground%20truth%20as%20the%20number%20of%20features%0Aincreases.%20Next%2C%20on%20the%20stereo%20VO%20pipeline%20side%2C%20we%20propose%20a%20framework%20that%0Acontinuously%20triangulates%20contemporary%20features%20for%20tracking%20new%20frames%2C%0Aeffectively%20decoupling%20temporal%20dependencies%20between%20pose%20and%203D%20point%20errors.%0AWe%20integrate%20the%20Bias-Eli-W%20PnP%20estimator%20into%20the%20proposed%20stereo%20VO%20pipeline%2C%0Acreating%20a%20synergistic%20effect%20that%20enhances%20the%20suppression%20of%20pose%20estimation%0Aerrors.%20We%20validate%20the%20performance%20of%20our%20method%20on%20the%20KITTI%20and%20Oxford%0ARobotCar%20datasets.%20Experimental%20results%20demonstrate%20that%20our%20method%3A%201%29%0Aachieves%20significant%20improvements%20in%20both%20relative%20pose%20error%20and%20absolute%0Atrajectory%20error%20in%20large-scale%20environments%3B%202%29%20provides%20reliable%20localization%0Aunder%20erratic%20and%20unpredictable%20robot%20motions.%20The%20successful%20implementation%20of%0Athe%20Bias-Eli-W%20PnP%20in%20stereo%20VO%20indicates%20the%20importance%20of%20information%0Ascreening%20in%20robotic%20estimation%20tasks%20with%20high-uncertainty%20measurements%2C%0Ashedding%20light%20on%20diverse%20applications%20where%20PnP%20is%20a%20key%20ingredient.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17410v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBias-Eliminated%2520PnP%2520for%2520Stereo%2520Visual%2520Odometry%253A%2520Provably%2520Consistent%2520and%250A%2520%2520Large-Scale%2520Localization%26entry.906535625%3DGuangyang%2520Zeng%2520and%2520Yuan%2520Shen%2520and%2520Ziyang%2520Hong%2520and%2520Yuze%2520Hong%2520and%2520Viorela%2520Ila%2520and%2520Guodong%2520Shi%2520and%2520Junfeng%2520Wu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520first%2520present%2520a%2520bias-eliminated%2520weighted%2520%2528Bias-Eli-W%2529%250Aperspective-n-point%2520%2528PnP%2529%2520estimator%2520for%2520stereo%2520visual%2520odometry%2520%2528VO%2529%2520with%250Aprovable%2520consistency.%2520Specifically%252C%2520leveraging%2520statistical%2520theory%252C%2520we%2520develop%250Aan%2520asymptotically%2520unbiased%2520and%2520%2524%255Csqrt%2520%257Bn%257D%2524-consistent%2520PnP%2520estimator%2520that%250Aaccounts%2520for%2520varying%25203D%2520triangulation%2520uncertainties%252C%2520ensuring%2520that%2520the%2520relative%250Apose%2520estimate%2520converges%2520to%2520the%2520ground%2520truth%2520as%2520the%2520number%2520of%2520features%250Aincreases.%2520Next%252C%2520on%2520the%2520stereo%2520VO%2520pipeline%2520side%252C%2520we%2520propose%2520a%2520framework%2520that%250Acontinuously%2520triangulates%2520contemporary%2520features%2520for%2520tracking%2520new%2520frames%252C%250Aeffectively%2520decoupling%2520temporal%2520dependencies%2520between%2520pose%2520and%25203D%2520point%2520errors.%250AWe%2520integrate%2520the%2520Bias-Eli-W%2520PnP%2520estimator%2520into%2520the%2520proposed%2520stereo%2520VO%2520pipeline%252C%250Acreating%2520a%2520synergistic%2520effect%2520that%2520enhances%2520the%2520suppression%2520of%2520pose%2520estimation%250Aerrors.%2520We%2520validate%2520the%2520performance%2520of%2520our%2520method%2520on%2520the%2520KITTI%2520and%2520Oxford%250ARobotCar%2520datasets.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520method%253A%25201%2529%250Aachieves%2520significant%2520improvements%2520in%2520both%2520relative%2520pose%2520error%2520and%2520absolute%250Atrajectory%2520error%2520in%2520large-scale%2520environments%253B%25202%2529%2520provides%2520reliable%2520localization%250Aunder%2520erratic%2520and%2520unpredictable%2520robot%2520motions.%2520The%2520successful%2520implementation%2520of%250Athe%2520Bias-Eli-W%2520PnP%2520in%2520stereo%2520VO%2520indicates%2520the%2520importance%2520of%2520information%250Ascreening%2520in%2520robotic%2520estimation%2520tasks%2520with%2520high-uncertainty%2520measurements%252C%250Ashedding%2520light%2520on%2520diverse%2520applications%2520where%2520PnP%2520is%2520a%2520key%2520ingredient.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17410v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bias-Eliminated%20PnP%20for%20Stereo%20Visual%20Odometry%3A%20Provably%20Consistent%20and%0A%20%20Large-Scale%20Localization&entry.906535625=Guangyang%20Zeng%20and%20Yuan%20Shen%20and%20Ziyang%20Hong%20and%20Yuze%20Hong%20and%20Viorela%20Ila%20and%20Guodong%20Shi%20and%20Junfeng%20Wu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20first%20present%20a%20bias-eliminated%20weighted%20%28Bias-Eli-W%29%0Aperspective-n-point%20%28PnP%29%20estimator%20for%20stereo%20visual%20odometry%20%28VO%29%20with%0Aprovable%20consistency.%20Specifically%2C%20leveraging%20statistical%20theory%2C%20we%20develop%0Aan%20asymptotically%20unbiased%20and%20%24%5Csqrt%20%7Bn%7D%24-consistent%20PnP%20estimator%20that%0Aaccounts%20for%20varying%203D%20triangulation%20uncertainties%2C%20ensuring%20that%20the%20relative%0Apose%20estimate%20converges%20to%20the%20ground%20truth%20as%20the%20number%20of%20features%0Aincreases.%20Next%2C%20on%20the%20stereo%20VO%20pipeline%20side%2C%20we%20propose%20a%20framework%20that%0Acontinuously%20triangulates%20contemporary%20features%20for%20tracking%20new%20frames%2C%0Aeffectively%20decoupling%20temporal%20dependencies%20between%20pose%20and%203D%20point%20errors.%0AWe%20integrate%20the%20Bias-Eli-W%20PnP%20estimator%20into%20the%20proposed%20stereo%20VO%20pipeline%2C%0Acreating%20a%20synergistic%20effect%20that%20enhances%20the%20suppression%20of%20pose%20estimation%0Aerrors.%20We%20validate%20the%20performance%20of%20our%20method%20on%20the%20KITTI%20and%20Oxford%0ARobotCar%20datasets.%20Experimental%20results%20demonstrate%20that%20our%20method%3A%201%29%0Aachieves%20significant%20improvements%20in%20both%20relative%20pose%20error%20and%20absolute%0Atrajectory%20error%20in%20large-scale%20environments%3B%202%29%20provides%20reliable%20localization%0Aunder%20erratic%20and%20unpredictable%20robot%20motions.%20The%20successful%20implementation%20of%0Athe%20Bias-Eli-W%20PnP%20in%20stereo%20VO%20indicates%20the%20importance%20of%20information%0Ascreening%20in%20robotic%20estimation%20tasks%20with%20high-uncertainty%20measurements%2C%0Ashedding%20light%20on%20diverse%20applications%20where%20PnP%20is%20a%20key%20ingredient.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17410v1&entry.124074799=Read"},
{"title": "Dynamic Camera Poses and Where to Find Them", "author": "Chris Rockwell and Joseph Tung and Tsung-Yi Lin and Ming-Yu Liu and David F. Fouhey and Chen-Hsuan Lin", "abstract": "  Annotating camera poses on dynamic Internet videos at scale is critical for\nadvancing fields like realistic video generation and simulation. However,\ncollecting such a dataset is difficult, as most Internet videos are unsuitable\nfor pose estimation. Furthermore, annotating dynamic Internet videos present\nsignificant challenges even for state-of-theart methods. In this paper, we\nintroduce DynPose-100K, a large-scale dataset of dynamic Internet videos\nannotated with camera poses. Our collection pipeline addresses filtering using\na carefully combined set of task-specific and generalist models. For pose\nestimation, we combine the latest techniques of point tracking, dynamic\nmasking, and structure-from-motion to achieve improvements over the\nstate-of-the-art approaches. Our analysis and experiments demonstrate that\nDynPose-100K is both large-scale and diverse across several key attributes,\nopening up avenues for advancements in various downstream applications.\n", "link": "http://arxiv.org/abs/2504.17788v1", "date": "2025-04-24", "relevancy": 2.3787, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6001}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5994}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Camera%20Poses%20and%20Where%20to%20Find%20Them&body=Title%3A%20Dynamic%20Camera%20Poses%20and%20Where%20to%20Find%20Them%0AAuthor%3A%20Chris%20Rockwell%20and%20Joseph%20Tung%20and%20Tsung-Yi%20Lin%20and%20Ming-Yu%20Liu%20and%20David%20F.%20Fouhey%20and%20Chen-Hsuan%20Lin%0AAbstract%3A%20%20%20Annotating%20camera%20poses%20on%20dynamic%20Internet%20videos%20at%20scale%20is%20critical%20for%0Aadvancing%20fields%20like%20realistic%20video%20generation%20and%20simulation.%20However%2C%0Acollecting%20such%20a%20dataset%20is%20difficult%2C%20as%20most%20Internet%20videos%20are%20unsuitable%0Afor%20pose%20estimation.%20Furthermore%2C%20annotating%20dynamic%20Internet%20videos%20present%0Asignificant%20challenges%20even%20for%20state-of-theart%20methods.%20In%20this%20paper%2C%20we%0Aintroduce%20DynPose-100K%2C%20a%20large-scale%20dataset%20of%20dynamic%20Internet%20videos%0Aannotated%20with%20camera%20poses.%20Our%20collection%20pipeline%20addresses%20filtering%20using%0Aa%20carefully%20combined%20set%20of%20task-specific%20and%20generalist%20models.%20For%20pose%0Aestimation%2C%20we%20combine%20the%20latest%20techniques%20of%20point%20tracking%2C%20dynamic%0Amasking%2C%20and%20structure-from-motion%20to%20achieve%20improvements%20over%20the%0Astate-of-the-art%20approaches.%20Our%20analysis%20and%20experiments%20demonstrate%20that%0ADynPose-100K%20is%20both%20large-scale%20and%20diverse%20across%20several%20key%20attributes%2C%0Aopening%20up%20avenues%20for%20advancements%20in%20various%20downstream%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17788v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Camera%2520Poses%2520and%2520Where%2520to%2520Find%2520Them%26entry.906535625%3DChris%2520Rockwell%2520and%2520Joseph%2520Tung%2520and%2520Tsung-Yi%2520Lin%2520and%2520Ming-Yu%2520Liu%2520and%2520David%2520F.%2520Fouhey%2520and%2520Chen-Hsuan%2520Lin%26entry.1292438233%3D%2520%2520Annotating%2520camera%2520poses%2520on%2520dynamic%2520Internet%2520videos%2520at%2520scale%2520is%2520critical%2520for%250Aadvancing%2520fields%2520like%2520realistic%2520video%2520generation%2520and%2520simulation.%2520However%252C%250Acollecting%2520such%2520a%2520dataset%2520is%2520difficult%252C%2520as%2520most%2520Internet%2520videos%2520are%2520unsuitable%250Afor%2520pose%2520estimation.%2520Furthermore%252C%2520annotating%2520dynamic%2520Internet%2520videos%2520present%250Asignificant%2520challenges%2520even%2520for%2520state-of-theart%2520methods.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520DynPose-100K%252C%2520a%2520large-scale%2520dataset%2520of%2520dynamic%2520Internet%2520videos%250Aannotated%2520with%2520camera%2520poses.%2520Our%2520collection%2520pipeline%2520addresses%2520filtering%2520using%250Aa%2520carefully%2520combined%2520set%2520of%2520task-specific%2520and%2520generalist%2520models.%2520For%2520pose%250Aestimation%252C%2520we%2520combine%2520the%2520latest%2520techniques%2520of%2520point%2520tracking%252C%2520dynamic%250Amasking%252C%2520and%2520structure-from-motion%2520to%2520achieve%2520improvements%2520over%2520the%250Astate-of-the-art%2520approaches.%2520Our%2520analysis%2520and%2520experiments%2520demonstrate%2520that%250ADynPose-100K%2520is%2520both%2520large-scale%2520and%2520diverse%2520across%2520several%2520key%2520attributes%252C%250Aopening%2520up%2520avenues%2520for%2520advancements%2520in%2520various%2520downstream%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17788v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Camera%20Poses%20and%20Where%20to%20Find%20Them&entry.906535625=Chris%20Rockwell%20and%20Joseph%20Tung%20and%20Tsung-Yi%20Lin%20and%20Ming-Yu%20Liu%20and%20David%20F.%20Fouhey%20and%20Chen-Hsuan%20Lin&entry.1292438233=%20%20Annotating%20camera%20poses%20on%20dynamic%20Internet%20videos%20at%20scale%20is%20critical%20for%0Aadvancing%20fields%20like%20realistic%20video%20generation%20and%20simulation.%20However%2C%0Acollecting%20such%20a%20dataset%20is%20difficult%2C%20as%20most%20Internet%20videos%20are%20unsuitable%0Afor%20pose%20estimation.%20Furthermore%2C%20annotating%20dynamic%20Internet%20videos%20present%0Asignificant%20challenges%20even%20for%20state-of-theart%20methods.%20In%20this%20paper%2C%20we%0Aintroduce%20DynPose-100K%2C%20a%20large-scale%20dataset%20of%20dynamic%20Internet%20videos%0Aannotated%20with%20camera%20poses.%20Our%20collection%20pipeline%20addresses%20filtering%20using%0Aa%20carefully%20combined%20set%20of%20task-specific%20and%20generalist%20models.%20For%20pose%0Aestimation%2C%20we%20combine%20the%20latest%20techniques%20of%20point%20tracking%2C%20dynamic%0Amasking%2C%20and%20structure-from-motion%20to%20achieve%20improvements%20over%20the%0Astate-of-the-art%20approaches.%20Our%20analysis%20and%20experiments%20demonstrate%20that%0ADynPose-100K%20is%20both%20large-scale%20and%20diverse%20across%20several%20key%20attributes%2C%0Aopening%20up%20avenues%20for%20advancements%20in%20various%20downstream%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17788v1&entry.124074799=Read"},
{"title": "The Malicious Technical Ecosystem: Exposing Limitations in Technical\n  Governance of AI-Generated Non-Consensual Intimate Images of Adults", "author": "Michelle L. Ding and Harini Suresh", "abstract": "  In this paper, we adopt a survivor-centered approach to locate and dissect\nthe role of sociotechnical AI governance in preventing AI-Generated\nNon-Consensual Intimate Images (AIG-NCII) of adults, colloquially known as\n\"deep fake pornography.\" We identify a \"malicious technical ecosystem\" or\n\"MTE,\" comprising of open-source face-swapping models and nearly 200\n\"nudifying\" software programs that allow non-technical users to create AIG-NCII\nwithin minutes. Then, using the National Institute of Standards and Technology\n(NIST) AI 100-4 report as a reflection of current synthetic content governance\nmethods, we show how the current landscape of practices fails to effectively\nregulate the MTE for adult AIG-NCII, as well as flawed assumptions explaining\nthese gaps.\n", "link": "http://arxiv.org/abs/2504.17663v1", "date": "2025-04-24", "relevancy": 2.3785, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4851}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4795}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Malicious%20Technical%20Ecosystem%3A%20Exposing%20Limitations%20in%20Technical%0A%20%20Governance%20of%20AI-Generated%20Non-Consensual%20Intimate%20Images%20of%20Adults&body=Title%3A%20The%20Malicious%20Technical%20Ecosystem%3A%20Exposing%20Limitations%20in%20Technical%0A%20%20Governance%20of%20AI-Generated%20Non-Consensual%20Intimate%20Images%20of%20Adults%0AAuthor%3A%20Michelle%20L.%20Ding%20and%20Harini%20Suresh%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20adopt%20a%20survivor-centered%20approach%20to%20locate%20and%20dissect%0Athe%20role%20of%20sociotechnical%20AI%20governance%20in%20preventing%20AI-Generated%0ANon-Consensual%20Intimate%20Images%20%28AIG-NCII%29%20of%20adults%2C%20colloquially%20known%20as%0A%22deep%20fake%20pornography.%22%20We%20identify%20a%20%22malicious%20technical%20ecosystem%22%20or%0A%22MTE%2C%22%20comprising%20of%20open-source%20face-swapping%20models%20and%20nearly%20200%0A%22nudifying%22%20software%20programs%20that%20allow%20non-technical%20users%20to%20create%20AIG-NCII%0Awithin%20minutes.%20Then%2C%20using%20the%20National%20Institute%20of%20Standards%20and%20Technology%0A%28NIST%29%20AI%20100-4%20report%20as%20a%20reflection%20of%20current%20synthetic%20content%20governance%0Amethods%2C%20we%20show%20how%20the%20current%20landscape%20of%20practices%20fails%20to%20effectively%0Aregulate%20the%20MTE%20for%20adult%20AIG-NCII%2C%20as%20well%20as%20flawed%20assumptions%20explaining%0Athese%20gaps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17663v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Malicious%2520Technical%2520Ecosystem%253A%2520Exposing%2520Limitations%2520in%2520Technical%250A%2520%2520Governance%2520of%2520AI-Generated%2520Non-Consensual%2520Intimate%2520Images%2520of%2520Adults%26entry.906535625%3DMichelle%2520L.%2520Ding%2520and%2520Harini%2520Suresh%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520adopt%2520a%2520survivor-centered%2520approach%2520to%2520locate%2520and%2520dissect%250Athe%2520role%2520of%2520sociotechnical%2520AI%2520governance%2520in%2520preventing%2520AI-Generated%250ANon-Consensual%2520Intimate%2520Images%2520%2528AIG-NCII%2529%2520of%2520adults%252C%2520colloquially%2520known%2520as%250A%2522deep%2520fake%2520pornography.%2522%2520We%2520identify%2520a%2520%2522malicious%2520technical%2520ecosystem%2522%2520or%250A%2522MTE%252C%2522%2520comprising%2520of%2520open-source%2520face-swapping%2520models%2520and%2520nearly%2520200%250A%2522nudifying%2522%2520software%2520programs%2520that%2520allow%2520non-technical%2520users%2520to%2520create%2520AIG-NCII%250Awithin%2520minutes.%2520Then%252C%2520using%2520the%2520National%2520Institute%2520of%2520Standards%2520and%2520Technology%250A%2528NIST%2529%2520AI%2520100-4%2520report%2520as%2520a%2520reflection%2520of%2520current%2520synthetic%2520content%2520governance%250Amethods%252C%2520we%2520show%2520how%2520the%2520current%2520landscape%2520of%2520practices%2520fails%2520to%2520effectively%250Aregulate%2520the%2520MTE%2520for%2520adult%2520AIG-NCII%252C%2520as%2520well%2520as%2520flawed%2520assumptions%2520explaining%250Athese%2520gaps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17663v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Malicious%20Technical%20Ecosystem%3A%20Exposing%20Limitations%20in%20Technical%0A%20%20Governance%20of%20AI-Generated%20Non-Consensual%20Intimate%20Images%20of%20Adults&entry.906535625=Michelle%20L.%20Ding%20and%20Harini%20Suresh&entry.1292438233=%20%20In%20this%20paper%2C%20we%20adopt%20a%20survivor-centered%20approach%20to%20locate%20and%20dissect%0Athe%20role%20of%20sociotechnical%20AI%20governance%20in%20preventing%20AI-Generated%0ANon-Consensual%20Intimate%20Images%20%28AIG-NCII%29%20of%20adults%2C%20colloquially%20known%20as%0A%22deep%20fake%20pornography.%22%20We%20identify%20a%20%22malicious%20technical%20ecosystem%22%20or%0A%22MTE%2C%22%20comprising%20of%20open-source%20face-swapping%20models%20and%20nearly%20200%0A%22nudifying%22%20software%20programs%20that%20allow%20non-technical%20users%20to%20create%20AIG-NCII%0Awithin%20minutes.%20Then%2C%20using%20the%20National%20Institute%20of%20Standards%20and%20Technology%0A%28NIST%29%20AI%20100-4%20report%20as%20a%20reflection%20of%20current%20synthetic%20content%20governance%0Amethods%2C%20we%20show%20how%20the%20current%20landscape%20of%20practices%20fails%20to%20effectively%0Aregulate%20the%20MTE%20for%20adult%20AIG-NCII%2C%20as%20well%20as%20flawed%20assumptions%20explaining%0Athese%20gaps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17663v1&entry.124074799=Read"},
{"title": "Improving Open-World Object Localization by Discovering Background", "author": "Ashish Singh and Michael J. Jones and Kuan-Chuan Peng and Anoop Cherian and Moitreya Chatterjee and Erik Learned-Miller", "abstract": "  Our work addresses the problem of learning to localize objects in an\nopen-world setting, i.e., given the bounding box information of a limited\nnumber of object classes during training, the goal is to localize all objects,\nbelonging to both the training and unseen classes in an image, during\ninference. Towards this end, recent work in this area has focused on improving\nthe characterization of objects either explicitly by proposing new objective\nfunctions (localization quality) or implicitly using object-centric\nauxiliary-information, such as depth information, pixel/region affinity map\netc. In this work, we address this problem by incorporating background\ninformation to guide the learning of the notion of objectness. Specifically, we\npropose a novel framework to discover background regions in an image and train\nan object proposal network to not detect any objects in these regions. We\nformulate the background discovery task as that of identifying image regions\nthat are not discriminative, i.e., those that are redundant and constitute low\ninformation content. We conduct experiments on standard benchmarks to showcase\nthe effectiveness of our proposed approach and observe significant improvements\nover the previous state-of-the-art approaches for this task.\n", "link": "http://arxiv.org/abs/2504.17626v1", "date": "2025-04-24", "relevancy": 2.3505, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6057}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5894}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Open-World%20Object%20Localization%20by%20Discovering%20Background&body=Title%3A%20Improving%20Open-World%20Object%20Localization%20by%20Discovering%20Background%0AAuthor%3A%20Ashish%20Singh%20and%20Michael%20J.%20Jones%20and%20Kuan-Chuan%20Peng%20and%20Anoop%20Cherian%20and%20Moitreya%20Chatterjee%20and%20Erik%20Learned-Miller%0AAbstract%3A%20%20%20Our%20work%20addresses%20the%20problem%20of%20learning%20to%20localize%20objects%20in%20an%0Aopen-world%20setting%2C%20i.e.%2C%20given%20the%20bounding%20box%20information%20of%20a%20limited%0Anumber%20of%20object%20classes%20during%20training%2C%20the%20goal%20is%20to%20localize%20all%20objects%2C%0Abelonging%20to%20both%20the%20training%20and%20unseen%20classes%20in%20an%20image%2C%20during%0Ainference.%20Towards%20this%20end%2C%20recent%20work%20in%20this%20area%20has%20focused%20on%20improving%0Athe%20characterization%20of%20objects%20either%20explicitly%20by%20proposing%20new%20objective%0Afunctions%20%28localization%20quality%29%20or%20implicitly%20using%20object-centric%0Aauxiliary-information%2C%20such%20as%20depth%20information%2C%20pixel/region%20affinity%20map%0Aetc.%20In%20this%20work%2C%20we%20address%20this%20problem%20by%20incorporating%20background%0Ainformation%20to%20guide%20the%20learning%20of%20the%20notion%20of%20objectness.%20Specifically%2C%20we%0Apropose%20a%20novel%20framework%20to%20discover%20background%20regions%20in%20an%20image%20and%20train%0Aan%20object%20proposal%20network%20to%20not%20detect%20any%20objects%20in%20these%20regions.%20We%0Aformulate%20the%20background%20discovery%20task%20as%20that%20of%20identifying%20image%20regions%0Athat%20are%20not%20discriminative%2C%20i.e.%2C%20those%20that%20are%20redundant%20and%20constitute%20low%0Ainformation%20content.%20We%20conduct%20experiments%20on%20standard%20benchmarks%20to%20showcase%0Athe%20effectiveness%20of%20our%20proposed%20approach%20and%20observe%20significant%20improvements%0Aover%20the%20previous%20state-of-the-art%20approaches%20for%20this%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17626v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Open-World%2520Object%2520Localization%2520by%2520Discovering%2520Background%26entry.906535625%3DAshish%2520Singh%2520and%2520Michael%2520J.%2520Jones%2520and%2520Kuan-Chuan%2520Peng%2520and%2520Anoop%2520Cherian%2520and%2520Moitreya%2520Chatterjee%2520and%2520Erik%2520Learned-Miller%26entry.1292438233%3D%2520%2520Our%2520work%2520addresses%2520the%2520problem%2520of%2520learning%2520to%2520localize%2520objects%2520in%2520an%250Aopen-world%2520setting%252C%2520i.e.%252C%2520given%2520the%2520bounding%2520box%2520information%2520of%2520a%2520limited%250Anumber%2520of%2520object%2520classes%2520during%2520training%252C%2520the%2520goal%2520is%2520to%2520localize%2520all%2520objects%252C%250Abelonging%2520to%2520both%2520the%2520training%2520and%2520unseen%2520classes%2520in%2520an%2520image%252C%2520during%250Ainference.%2520Towards%2520this%2520end%252C%2520recent%2520work%2520in%2520this%2520area%2520has%2520focused%2520on%2520improving%250Athe%2520characterization%2520of%2520objects%2520either%2520explicitly%2520by%2520proposing%2520new%2520objective%250Afunctions%2520%2528localization%2520quality%2529%2520or%2520implicitly%2520using%2520object-centric%250Aauxiliary-information%252C%2520such%2520as%2520depth%2520information%252C%2520pixel/region%2520affinity%2520map%250Aetc.%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520problem%2520by%2520incorporating%2520background%250Ainformation%2520to%2520guide%2520the%2520learning%2520of%2520the%2520notion%2520of%2520objectness.%2520Specifically%252C%2520we%250Apropose%2520a%2520novel%2520framework%2520to%2520discover%2520background%2520regions%2520in%2520an%2520image%2520and%2520train%250Aan%2520object%2520proposal%2520network%2520to%2520not%2520detect%2520any%2520objects%2520in%2520these%2520regions.%2520We%250Aformulate%2520the%2520background%2520discovery%2520task%2520as%2520that%2520of%2520identifying%2520image%2520regions%250Athat%2520are%2520not%2520discriminative%252C%2520i.e.%252C%2520those%2520that%2520are%2520redundant%2520and%2520constitute%2520low%250Ainformation%2520content.%2520We%2520conduct%2520experiments%2520on%2520standard%2520benchmarks%2520to%2520showcase%250Athe%2520effectiveness%2520of%2520our%2520proposed%2520approach%2520and%2520observe%2520significant%2520improvements%250Aover%2520the%2520previous%2520state-of-the-art%2520approaches%2520for%2520this%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17626v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Open-World%20Object%20Localization%20by%20Discovering%20Background&entry.906535625=Ashish%20Singh%20and%20Michael%20J.%20Jones%20and%20Kuan-Chuan%20Peng%20and%20Anoop%20Cherian%20and%20Moitreya%20Chatterjee%20and%20Erik%20Learned-Miller&entry.1292438233=%20%20Our%20work%20addresses%20the%20problem%20of%20learning%20to%20localize%20objects%20in%20an%0Aopen-world%20setting%2C%20i.e.%2C%20given%20the%20bounding%20box%20information%20of%20a%20limited%0Anumber%20of%20object%20classes%20during%20training%2C%20the%20goal%20is%20to%20localize%20all%20objects%2C%0Abelonging%20to%20both%20the%20training%20and%20unseen%20classes%20in%20an%20image%2C%20during%0Ainference.%20Towards%20this%20end%2C%20recent%20work%20in%20this%20area%20has%20focused%20on%20improving%0Athe%20characterization%20of%20objects%20either%20explicitly%20by%20proposing%20new%20objective%0Afunctions%20%28localization%20quality%29%20or%20implicitly%20using%20object-centric%0Aauxiliary-information%2C%20such%20as%20depth%20information%2C%20pixel/region%20affinity%20map%0Aetc.%20In%20this%20work%2C%20we%20address%20this%20problem%20by%20incorporating%20background%0Ainformation%20to%20guide%20the%20learning%20of%20the%20notion%20of%20objectness.%20Specifically%2C%20we%0Apropose%20a%20novel%20framework%20to%20discover%20background%20regions%20in%20an%20image%20and%20train%0Aan%20object%20proposal%20network%20to%20not%20detect%20any%20objects%20in%20these%20regions.%20We%0Aformulate%20the%20background%20discovery%20task%20as%20that%20of%20identifying%20image%20regions%0Athat%20are%20not%20discriminative%2C%20i.e.%2C%20those%20that%20are%20redundant%20and%20constitute%20low%0Ainformation%20content.%20We%20conduct%20experiments%20on%20standard%20benchmarks%20to%20showcase%0Athe%20effectiveness%20of%20our%20proposed%20approach%20and%20observe%20significant%20improvements%0Aover%20the%20previous%20state-of-the-art%20approaches%20for%20this%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17626v1&entry.124074799=Read"},
{"title": "RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image\n  Generation", "author": "Aviv Slobodkin and Hagai Taitelbaum and Yonatan Bitton and Brian Gordon and Michal Sokolik and Nitzan Bitton Guetta and Almog Gueta and Royi Rassin and Itay Laish and Dani Lischinski and Idan Szpektor", "abstract": "  Subject-driven text-to-image (T2I) generation aims to produce images that\nalign with a given textual description, while preserving the visual identity\nfrom a referenced subject image. Despite its broad downstream applicability --\nranging from enhanced personalization in image generation to consistent\ncharacter representation in video rendering -- progress in this field is\nlimited by the lack of reliable automatic evaluation. Existing methods either\nassess only one aspect of the task (i.e., textual alignment or subject\npreservation), misalign with human judgments, or rely on costly API-based\nevaluation. To address this, we introduce RefVNLI, a cost-effective metric that\nevaluates both textual alignment and subject preservation in a single\nprediction. Trained on a large-scale dataset derived from video-reasoning\nbenchmarks and image perturbations, RefVNLI outperforms or matches existing\nbaselines across multiple benchmarks and subject categories (e.g.,\n\\emph{Animal}, \\emph{Object}), achieving up to 6.4-point gains in textual\nalignment and 8.5-point gains in subject consistency. It also excels with\nlesser-known concepts, aligning with human preferences at over 87\\% accuracy.\n", "link": "http://arxiv.org/abs/2504.17502v1", "date": "2025-04-24", "relevancy": 2.3488, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6034}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5785}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RefVNLI%3A%20Towards%20Scalable%20Evaluation%20of%20Subject-driven%20Text-to-image%0A%20%20Generation&body=Title%3A%20RefVNLI%3A%20Towards%20Scalable%20Evaluation%20of%20Subject-driven%20Text-to-image%0A%20%20Generation%0AAuthor%3A%20Aviv%20Slobodkin%20and%20Hagai%20Taitelbaum%20and%20Yonatan%20Bitton%20and%20Brian%20Gordon%20and%20Michal%20Sokolik%20and%20Nitzan%20Bitton%20Guetta%20and%20Almog%20Gueta%20and%20Royi%20Rassin%20and%20Itay%20Laish%20and%20Dani%20Lischinski%20and%20Idan%20Szpektor%0AAbstract%3A%20%20%20Subject-driven%20text-to-image%20%28T2I%29%20generation%20aims%20to%20produce%20images%20that%0Aalign%20with%20a%20given%20textual%20description%2C%20while%20preserving%20the%20visual%20identity%0Afrom%20a%20referenced%20subject%20image.%20Despite%20its%20broad%20downstream%20applicability%20--%0Aranging%20from%20enhanced%20personalization%20in%20image%20generation%20to%20consistent%0Acharacter%20representation%20in%20video%20rendering%20--%20progress%20in%20this%20field%20is%0Alimited%20by%20the%20lack%20of%20reliable%20automatic%20evaluation.%20Existing%20methods%20either%0Aassess%20only%20one%20aspect%20of%20the%20task%20%28i.e.%2C%20textual%20alignment%20or%20subject%0Apreservation%29%2C%20misalign%20with%20human%20judgments%2C%20or%20rely%20on%20costly%20API-based%0Aevaluation.%20To%20address%20this%2C%20we%20introduce%20RefVNLI%2C%20a%20cost-effective%20metric%20that%0Aevaluates%20both%20textual%20alignment%20and%20subject%20preservation%20in%20a%20single%0Aprediction.%20Trained%20on%20a%20large-scale%20dataset%20derived%20from%20video-reasoning%0Abenchmarks%20and%20image%20perturbations%2C%20RefVNLI%20outperforms%20or%20matches%20existing%0Abaselines%20across%20multiple%20benchmarks%20and%20subject%20categories%20%28e.g.%2C%0A%5Cemph%7BAnimal%7D%2C%20%5Cemph%7BObject%7D%29%2C%20achieving%20up%20to%206.4-point%20gains%20in%20textual%0Aalignment%20and%208.5-point%20gains%20in%20subject%20consistency.%20It%20also%20excels%20with%0Alesser-known%20concepts%2C%20aligning%20with%20human%20preferences%20at%20over%2087%5C%25%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17502v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRefVNLI%253A%2520Towards%2520Scalable%2520Evaluation%2520of%2520Subject-driven%2520Text-to-image%250A%2520%2520Generation%26entry.906535625%3DAviv%2520Slobodkin%2520and%2520Hagai%2520Taitelbaum%2520and%2520Yonatan%2520Bitton%2520and%2520Brian%2520Gordon%2520and%2520Michal%2520Sokolik%2520and%2520Nitzan%2520Bitton%2520Guetta%2520and%2520Almog%2520Gueta%2520and%2520Royi%2520Rassin%2520and%2520Itay%2520Laish%2520and%2520Dani%2520Lischinski%2520and%2520Idan%2520Szpektor%26entry.1292438233%3D%2520%2520Subject-driven%2520text-to-image%2520%2528T2I%2529%2520generation%2520aims%2520to%2520produce%2520images%2520that%250Aalign%2520with%2520a%2520given%2520textual%2520description%252C%2520while%2520preserving%2520the%2520visual%2520identity%250Afrom%2520a%2520referenced%2520subject%2520image.%2520Despite%2520its%2520broad%2520downstream%2520applicability%2520--%250Aranging%2520from%2520enhanced%2520personalization%2520in%2520image%2520generation%2520to%2520consistent%250Acharacter%2520representation%2520in%2520video%2520rendering%2520--%2520progress%2520in%2520this%2520field%2520is%250Alimited%2520by%2520the%2520lack%2520of%2520reliable%2520automatic%2520evaluation.%2520Existing%2520methods%2520either%250Aassess%2520only%2520one%2520aspect%2520of%2520the%2520task%2520%2528i.e.%252C%2520textual%2520alignment%2520or%2520subject%250Apreservation%2529%252C%2520misalign%2520with%2520human%2520judgments%252C%2520or%2520rely%2520on%2520costly%2520API-based%250Aevaluation.%2520To%2520address%2520this%252C%2520we%2520introduce%2520RefVNLI%252C%2520a%2520cost-effective%2520metric%2520that%250Aevaluates%2520both%2520textual%2520alignment%2520and%2520subject%2520preservation%2520in%2520a%2520single%250Aprediction.%2520Trained%2520on%2520a%2520large-scale%2520dataset%2520derived%2520from%2520video-reasoning%250Abenchmarks%2520and%2520image%2520perturbations%252C%2520RefVNLI%2520outperforms%2520or%2520matches%2520existing%250Abaselines%2520across%2520multiple%2520benchmarks%2520and%2520subject%2520categories%2520%2528e.g.%252C%250A%255Cemph%257BAnimal%257D%252C%2520%255Cemph%257BObject%257D%2529%252C%2520achieving%2520up%2520to%25206.4-point%2520gains%2520in%2520textual%250Aalignment%2520and%25208.5-point%2520gains%2520in%2520subject%2520consistency.%2520It%2520also%2520excels%2520with%250Alesser-known%2520concepts%252C%2520aligning%2520with%2520human%2520preferences%2520at%2520over%252087%255C%2525%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17502v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RefVNLI%3A%20Towards%20Scalable%20Evaluation%20of%20Subject-driven%20Text-to-image%0A%20%20Generation&entry.906535625=Aviv%20Slobodkin%20and%20Hagai%20Taitelbaum%20and%20Yonatan%20Bitton%20and%20Brian%20Gordon%20and%20Michal%20Sokolik%20and%20Nitzan%20Bitton%20Guetta%20and%20Almog%20Gueta%20and%20Royi%20Rassin%20and%20Itay%20Laish%20and%20Dani%20Lischinski%20and%20Idan%20Szpektor&entry.1292438233=%20%20Subject-driven%20text-to-image%20%28T2I%29%20generation%20aims%20to%20produce%20images%20that%0Aalign%20with%20a%20given%20textual%20description%2C%20while%20preserving%20the%20visual%20identity%0Afrom%20a%20referenced%20subject%20image.%20Despite%20its%20broad%20downstream%20applicability%20--%0Aranging%20from%20enhanced%20personalization%20in%20image%20generation%20to%20consistent%0Acharacter%20representation%20in%20video%20rendering%20--%20progress%20in%20this%20field%20is%0Alimited%20by%20the%20lack%20of%20reliable%20automatic%20evaluation.%20Existing%20methods%20either%0Aassess%20only%20one%20aspect%20of%20the%20task%20%28i.e.%2C%20textual%20alignment%20or%20subject%0Apreservation%29%2C%20misalign%20with%20human%20judgments%2C%20or%20rely%20on%20costly%20API-based%0Aevaluation.%20To%20address%20this%2C%20we%20introduce%20RefVNLI%2C%20a%20cost-effective%20metric%20that%0Aevaluates%20both%20textual%20alignment%20and%20subject%20preservation%20in%20a%20single%0Aprediction.%20Trained%20on%20a%20large-scale%20dataset%20derived%20from%20video-reasoning%0Abenchmarks%20and%20image%20perturbations%2C%20RefVNLI%20outperforms%20or%20matches%20existing%0Abaselines%20across%20multiple%20benchmarks%20and%20subject%20categories%20%28e.g.%2C%0A%5Cemph%7BAnimal%7D%2C%20%5Cemph%7BObject%7D%29%2C%20achieving%20up%20to%206.4-point%20gains%20in%20textual%0Aalignment%20and%208.5-point%20gains%20in%20subject%20consistency.%20It%20also%20excels%20with%0Alesser-known%20concepts%2C%20aligning%20with%20human%20preferences%20at%20over%2087%5C%25%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17502v1&entry.124074799=Read"},
{"title": "Towards Harnessing the Collaborative Power of Large and Small Models for\n  Domain Tasks", "author": "Yang Liu and Bingjie Yan and Tianyuan Zou and Jianqing Zhang and Zixuan Gu and Jianbing Ding and Xidong Wang and Jingyi Li and Xiaozhou Ye and Ye Ouyang and Qiang Yang and Ya-Qin Zhang", "abstract": "  Large language models (LLMs) have demonstrated remarkable capabilities, but\nthey require vast amounts of data and computational resources. In contrast,\nsmaller models (SMs), while less powerful, can be more efficient and tailored\nto specific domains. In this position paper, we argue that taking a\ncollaborative approach, where large and small models work synergistically, can\naccelerate the adaptation of LLMs to private domains and unlock new potential\nin AI. We explore various strategies for model collaboration and identify\npotential challenges and opportunities. Building upon this, we advocate for\nindustry-driven research that prioritizes multi-objective benchmarks on\nreal-world private datasets and applications.\n", "link": "http://arxiv.org/abs/2504.17421v1", "date": "2025-04-24", "relevancy": 2.3485, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4697}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4697}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Harnessing%20the%20Collaborative%20Power%20of%20Large%20and%20Small%20Models%20for%0A%20%20Domain%20Tasks&body=Title%3A%20Towards%20Harnessing%20the%20Collaborative%20Power%20of%20Large%20and%20Small%20Models%20for%0A%20%20Domain%20Tasks%0AAuthor%3A%20Yang%20Liu%20and%20Bingjie%20Yan%20and%20Tianyuan%20Zou%20and%20Jianqing%20Zhang%20and%20Zixuan%20Gu%20and%20Jianbing%20Ding%20and%20Xidong%20Wang%20and%20Jingyi%20Li%20and%20Xiaozhou%20Ye%20and%20Ye%20Ouyang%20and%20Qiang%20Yang%20and%20Ya-Qin%20Zhang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%2C%20but%0Athey%20require%20vast%20amounts%20of%20data%20and%20computational%20resources.%20In%20contrast%2C%0Asmaller%20models%20%28SMs%29%2C%20while%20less%20powerful%2C%20can%20be%20more%20efficient%20and%20tailored%0Ato%20specific%20domains.%20In%20this%20position%20paper%2C%20we%20argue%20that%20taking%20a%0Acollaborative%20approach%2C%20where%20large%20and%20small%20models%20work%20synergistically%2C%20can%0Aaccelerate%20the%20adaptation%20of%20LLMs%20to%20private%20domains%20and%20unlock%20new%20potential%0Ain%20AI.%20We%20explore%20various%20strategies%20for%20model%20collaboration%20and%20identify%0Apotential%20challenges%20and%20opportunities.%20Building%20upon%20this%2C%20we%20advocate%20for%0Aindustry-driven%20research%20that%20prioritizes%20multi-objective%20benchmarks%20on%0Areal-world%20private%20datasets%20and%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17421v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Harnessing%2520the%2520Collaborative%2520Power%2520of%2520Large%2520and%2520Small%2520Models%2520for%250A%2520%2520Domain%2520Tasks%26entry.906535625%3DYang%2520Liu%2520and%2520Bingjie%2520Yan%2520and%2520Tianyuan%2520Zou%2520and%2520Jianqing%2520Zhang%2520and%2520Zixuan%2520Gu%2520and%2520Jianbing%2520Ding%2520and%2520Xidong%2520Wang%2520and%2520Jingyi%2520Li%2520and%2520Xiaozhou%2520Ye%2520and%2520Ye%2520Ouyang%2520and%2520Qiang%2520Yang%2520and%2520Ya-Qin%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%252C%2520but%250Athey%2520require%2520vast%2520amounts%2520of%2520data%2520and%2520computational%2520resources.%2520In%2520contrast%252C%250Asmaller%2520models%2520%2528SMs%2529%252C%2520while%2520less%2520powerful%252C%2520can%2520be%2520more%2520efficient%2520and%2520tailored%250Ato%2520specific%2520domains.%2520In%2520this%2520position%2520paper%252C%2520we%2520argue%2520that%2520taking%2520a%250Acollaborative%2520approach%252C%2520where%2520large%2520and%2520small%2520models%2520work%2520synergistically%252C%2520can%250Aaccelerate%2520the%2520adaptation%2520of%2520LLMs%2520to%2520private%2520domains%2520and%2520unlock%2520new%2520potential%250Ain%2520AI.%2520We%2520explore%2520various%2520strategies%2520for%2520model%2520collaboration%2520and%2520identify%250Apotential%2520challenges%2520and%2520opportunities.%2520Building%2520upon%2520this%252C%2520we%2520advocate%2520for%250Aindustry-driven%2520research%2520that%2520prioritizes%2520multi-objective%2520benchmarks%2520on%250Areal-world%2520private%2520datasets%2520and%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17421v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Harnessing%20the%20Collaborative%20Power%20of%20Large%20and%20Small%20Models%20for%0A%20%20Domain%20Tasks&entry.906535625=Yang%20Liu%20and%20Bingjie%20Yan%20and%20Tianyuan%20Zou%20and%20Jianqing%20Zhang%20and%20Zixuan%20Gu%20and%20Jianbing%20Ding%20and%20Xidong%20Wang%20and%20Jingyi%20Li%20and%20Xiaozhou%20Ye%20and%20Ye%20Ouyang%20and%20Qiang%20Yang%20and%20Ya-Qin%20Zhang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%2C%20but%0Athey%20require%20vast%20amounts%20of%20data%20and%20computational%20resources.%20In%20contrast%2C%0Asmaller%20models%20%28SMs%29%2C%20while%20less%20powerful%2C%20can%20be%20more%20efficient%20and%20tailored%0Ato%20specific%20domains.%20In%20this%20position%20paper%2C%20we%20argue%20that%20taking%20a%0Acollaborative%20approach%2C%20where%20large%20and%20small%20models%20work%20synergistically%2C%20can%0Aaccelerate%20the%20adaptation%20of%20LLMs%20to%20private%20domains%20and%20unlock%20new%20potential%0Ain%20AI.%20We%20explore%20various%20strategies%20for%20model%20collaboration%20and%20identify%0Apotential%20challenges%20and%20opportunities.%20Building%20upon%20this%2C%20we%20advocate%20for%0Aindustry-driven%20research%20that%20prioritizes%20multi-objective%20benchmarks%20on%0Areal-world%20private%20datasets%20and%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17421v1&entry.124074799=Read"},
{"title": "Keyframe-oriented Vision Token Pruning: Enhancing Efficiency of Large\n  Vision Language Models on Long-Form Video Processing", "author": "Yudong Liu and Jingwei Sun and Yueqian Lin and Jingyang Zhang and Ming Yin and Qinsi Wang and Jianyi Zhang and Hai Li and Yiran Chen", "abstract": "  Vision language models (VLMs) demonstrate strong capabilities in jointly\nprocessing visual and textual data. However, they often incur substantial\ncomputational overhead due to redundant visual information, particularly in\nlong-form video scenarios. Existing approaches predominantly focus on either\nvision token pruning, which may overlook spatio-temporal dependencies, or\nkeyframe selection, which identifies informative frames but discards others,\nthus disrupting contextual continuity. In this work, we propose KVTP\n(Keyframe-oriented Vision Token Pruning), a novel framework that overcomes the\ndrawbacks of token pruning and keyframe selection. By adaptively assigning\npruning rates based on frame relevance to the query, KVTP effectively retains\nessential contextual information while significantly reducing redundant\ncomputation. To thoroughly evaluate the long-form video understanding\ncapacities of VLMs, we curated and reorganized subsets from VideoMME,\nEgoSchema, and NextQA into a unified benchmark named SparseKV-QA that\nhighlights real-world scenarios with sparse but crucial events. Our experiments\nwith VLMs of various scales show that KVTP can reduce token usage by 80%\nwithout compromising spatiotemporal and contextual consistency, significantly\ncutting computation while maintaining the performance. These results\ndemonstrate our approach's effectiveness in efficient long-video processing,\nfacilitating more scalable VLM deployment.\n", "link": "http://arxiv.org/abs/2503.10742v2", "date": "2025-04-24", "relevancy": 2.3411, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5902}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5902}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Keyframe-oriented%20Vision%20Token%20Pruning%3A%20Enhancing%20Efficiency%20of%20Large%0A%20%20Vision%20Language%20Models%20on%20Long-Form%20Video%20Processing&body=Title%3A%20Keyframe-oriented%20Vision%20Token%20Pruning%3A%20Enhancing%20Efficiency%20of%20Large%0A%20%20Vision%20Language%20Models%20on%20Long-Form%20Video%20Processing%0AAuthor%3A%20Yudong%20Liu%20and%20Jingwei%20Sun%20and%20Yueqian%20Lin%20and%20Jingyang%20Zhang%20and%20Ming%20Yin%20and%20Qinsi%20Wang%20and%20Jianyi%20Zhang%20and%20Hai%20Li%20and%20Yiran%20Chen%0AAbstract%3A%20%20%20Vision%20language%20models%20%28VLMs%29%20demonstrate%20strong%20capabilities%20in%20jointly%0Aprocessing%20visual%20and%20textual%20data.%20However%2C%20they%20often%20incur%20substantial%0Acomputational%20overhead%20due%20to%20redundant%20visual%20information%2C%20particularly%20in%0Along-form%20video%20scenarios.%20Existing%20approaches%20predominantly%20focus%20on%20either%0Avision%20token%20pruning%2C%20which%20may%20overlook%20spatio-temporal%20dependencies%2C%20or%0Akeyframe%20selection%2C%20which%20identifies%20informative%20frames%20but%20discards%20others%2C%0Athus%20disrupting%20contextual%20continuity.%20In%20this%20work%2C%20we%20propose%20KVTP%0A%28Keyframe-oriented%20Vision%20Token%20Pruning%29%2C%20a%20novel%20framework%20that%20overcomes%20the%0Adrawbacks%20of%20token%20pruning%20and%20keyframe%20selection.%20By%20adaptively%20assigning%0Apruning%20rates%20based%20on%20frame%20relevance%20to%20the%20query%2C%20KVTP%20effectively%20retains%0Aessential%20contextual%20information%20while%20significantly%20reducing%20redundant%0Acomputation.%20To%20thoroughly%20evaluate%20the%20long-form%20video%20understanding%0Acapacities%20of%20VLMs%2C%20we%20curated%20and%20reorganized%20subsets%20from%20VideoMME%2C%0AEgoSchema%2C%20and%20NextQA%20into%20a%20unified%20benchmark%20named%20SparseKV-QA%20that%0Ahighlights%20real-world%20scenarios%20with%20sparse%20but%20crucial%20events.%20Our%20experiments%0Awith%20VLMs%20of%20various%20scales%20show%20that%20KVTP%20can%20reduce%20token%20usage%20by%2080%25%0Awithout%20compromising%20spatiotemporal%20and%20contextual%20consistency%2C%20significantly%0Acutting%20computation%20while%20maintaining%20the%20performance.%20These%20results%0Ademonstrate%20our%20approach%27s%20effectiveness%20in%20efficient%20long-video%20processing%2C%0Afacilitating%20more%20scalable%20VLM%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.10742v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKeyframe-oriented%2520Vision%2520Token%2520Pruning%253A%2520Enhancing%2520Efficiency%2520of%2520Large%250A%2520%2520Vision%2520Language%2520Models%2520on%2520Long-Form%2520Video%2520Processing%26entry.906535625%3DYudong%2520Liu%2520and%2520Jingwei%2520Sun%2520and%2520Yueqian%2520Lin%2520and%2520Jingyang%2520Zhang%2520and%2520Ming%2520Yin%2520and%2520Qinsi%2520Wang%2520and%2520Jianyi%2520Zhang%2520and%2520Hai%2520Li%2520and%2520Yiran%2520Chen%26entry.1292438233%3D%2520%2520Vision%2520language%2520models%2520%2528VLMs%2529%2520demonstrate%2520strong%2520capabilities%2520in%2520jointly%250Aprocessing%2520visual%2520and%2520textual%2520data.%2520However%252C%2520they%2520often%2520incur%2520substantial%250Acomputational%2520overhead%2520due%2520to%2520redundant%2520visual%2520information%252C%2520particularly%2520in%250Along-form%2520video%2520scenarios.%2520Existing%2520approaches%2520predominantly%2520focus%2520on%2520either%250Avision%2520token%2520pruning%252C%2520which%2520may%2520overlook%2520spatio-temporal%2520dependencies%252C%2520or%250Akeyframe%2520selection%252C%2520which%2520identifies%2520informative%2520frames%2520but%2520discards%2520others%252C%250Athus%2520disrupting%2520contextual%2520continuity.%2520In%2520this%2520work%252C%2520we%2520propose%2520KVTP%250A%2528Keyframe-oriented%2520Vision%2520Token%2520Pruning%2529%252C%2520a%2520novel%2520framework%2520that%2520overcomes%2520the%250Adrawbacks%2520of%2520token%2520pruning%2520and%2520keyframe%2520selection.%2520By%2520adaptively%2520assigning%250Apruning%2520rates%2520based%2520on%2520frame%2520relevance%2520to%2520the%2520query%252C%2520KVTP%2520effectively%2520retains%250Aessential%2520contextual%2520information%2520while%2520significantly%2520reducing%2520redundant%250Acomputation.%2520To%2520thoroughly%2520evaluate%2520the%2520long-form%2520video%2520understanding%250Acapacities%2520of%2520VLMs%252C%2520we%2520curated%2520and%2520reorganized%2520subsets%2520from%2520VideoMME%252C%250AEgoSchema%252C%2520and%2520NextQA%2520into%2520a%2520unified%2520benchmark%2520named%2520SparseKV-QA%2520that%250Ahighlights%2520real-world%2520scenarios%2520with%2520sparse%2520but%2520crucial%2520events.%2520Our%2520experiments%250Awith%2520VLMs%2520of%2520various%2520scales%2520show%2520that%2520KVTP%2520can%2520reduce%2520token%2520usage%2520by%252080%2525%250Awithout%2520compromising%2520spatiotemporal%2520and%2520contextual%2520consistency%252C%2520significantly%250Acutting%2520computation%2520while%2520maintaining%2520the%2520performance.%2520These%2520results%250Ademonstrate%2520our%2520approach%2527s%2520effectiveness%2520in%2520efficient%2520long-video%2520processing%252C%250Afacilitating%2520more%2520scalable%2520VLM%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.10742v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Keyframe-oriented%20Vision%20Token%20Pruning%3A%20Enhancing%20Efficiency%20of%20Large%0A%20%20Vision%20Language%20Models%20on%20Long-Form%20Video%20Processing&entry.906535625=Yudong%20Liu%20and%20Jingwei%20Sun%20and%20Yueqian%20Lin%20and%20Jingyang%20Zhang%20and%20Ming%20Yin%20and%20Qinsi%20Wang%20and%20Jianyi%20Zhang%20and%20Hai%20Li%20and%20Yiran%20Chen&entry.1292438233=%20%20Vision%20language%20models%20%28VLMs%29%20demonstrate%20strong%20capabilities%20in%20jointly%0Aprocessing%20visual%20and%20textual%20data.%20However%2C%20they%20often%20incur%20substantial%0Acomputational%20overhead%20due%20to%20redundant%20visual%20information%2C%20particularly%20in%0Along-form%20video%20scenarios.%20Existing%20approaches%20predominantly%20focus%20on%20either%0Avision%20token%20pruning%2C%20which%20may%20overlook%20spatio-temporal%20dependencies%2C%20or%0Akeyframe%20selection%2C%20which%20identifies%20informative%20frames%20but%20discards%20others%2C%0Athus%20disrupting%20contextual%20continuity.%20In%20this%20work%2C%20we%20propose%20KVTP%0A%28Keyframe-oriented%20Vision%20Token%20Pruning%29%2C%20a%20novel%20framework%20that%20overcomes%20the%0Adrawbacks%20of%20token%20pruning%20and%20keyframe%20selection.%20By%20adaptively%20assigning%0Apruning%20rates%20based%20on%20frame%20relevance%20to%20the%20query%2C%20KVTP%20effectively%20retains%0Aessential%20contextual%20information%20while%20significantly%20reducing%20redundant%0Acomputation.%20To%20thoroughly%20evaluate%20the%20long-form%20video%20understanding%0Acapacities%20of%20VLMs%2C%20we%20curated%20and%20reorganized%20subsets%20from%20VideoMME%2C%0AEgoSchema%2C%20and%20NextQA%20into%20a%20unified%20benchmark%20named%20SparseKV-QA%20that%0Ahighlights%20real-world%20scenarios%20with%20sparse%20but%20crucial%20events.%20Our%20experiments%0Awith%20VLMs%20of%20various%20scales%20show%20that%20KVTP%20can%20reduce%20token%20usage%20by%2080%25%0Awithout%20compromising%20spatiotemporal%20and%20contextual%20consistency%2C%20significantly%0Acutting%20computation%20while%20maintaining%20the%20performance.%20These%20results%0Ademonstrate%20our%20approach%27s%20effectiveness%20in%20efficient%20long-video%20processing%2C%0Afacilitating%20more%20scalable%20VLM%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.10742v2&entry.124074799=Read"},
{"title": "MUVO: A Multimodal Generative World Model for Autonomous Driving with\n  Geometric Representations", "author": "Daniel Bogdoll and Yitian Yang and Tim Joseph and Melih Yazgan and J. Marius Z\u00f6llner", "abstract": "  World models for autonomous driving have the potential to dramatically\nimprove the reasoning capabilities of today's systems. However, most works\nfocus on camera data, with only a few that leverage lidar data or combine both\nto better represent autonomous vehicle sensor setups. In addition, raw sensor\npredictions are less actionable than 3D occupancy predictions, but there are no\nworks examining the effects of combining both multimodal sensor data and 3D\noccupancy prediction. In this work, we perform a set of experiments with a\nMUltimodal World Model with Geometric VOxel representations (MUVO) to evaluate\ndifferent sensor fusion strategies to better understand the effects on sensor\ndata prediction. We also analyze potential weaknesses of current sensor fusion\napproaches and examine the benefits of additionally predicting 3D occupancy.\n", "link": "http://arxiv.org/abs/2311.11762v4", "date": "2025-04-24", "relevancy": 2.3316, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6106}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5849}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MUVO%3A%20A%20Multimodal%20Generative%20World%20Model%20for%20Autonomous%20Driving%20with%0A%20%20Geometric%20Representations&body=Title%3A%20MUVO%3A%20A%20Multimodal%20Generative%20World%20Model%20for%20Autonomous%20Driving%20with%0A%20%20Geometric%20Representations%0AAuthor%3A%20Daniel%20Bogdoll%20and%20Yitian%20Yang%20and%20Tim%20Joseph%20and%20Melih%20Yazgan%20and%20J.%20Marius%20Z%C3%B6llner%0AAbstract%3A%20%20%20World%20models%20for%20autonomous%20driving%20have%20the%20potential%20to%20dramatically%0Aimprove%20the%20reasoning%20capabilities%20of%20today%27s%20systems.%20However%2C%20most%20works%0Afocus%20on%20camera%20data%2C%20with%20only%20a%20few%20that%20leverage%20lidar%20data%20or%20combine%20both%0Ato%20better%20represent%20autonomous%20vehicle%20sensor%20setups.%20In%20addition%2C%20raw%20sensor%0Apredictions%20are%20less%20actionable%20than%203D%20occupancy%20predictions%2C%20but%20there%20are%20no%0Aworks%20examining%20the%20effects%20of%20combining%20both%20multimodal%20sensor%20data%20and%203D%0Aoccupancy%20prediction.%20In%20this%20work%2C%20we%20perform%20a%20set%20of%20experiments%20with%20a%0AMUltimodal%20World%20Model%20with%20Geometric%20VOxel%20representations%20%28MUVO%29%20to%20evaluate%0Adifferent%20sensor%20fusion%20strategies%20to%20better%20understand%20the%20effects%20on%20sensor%0Adata%20prediction.%20We%20also%20analyze%20potential%20weaknesses%20of%20current%20sensor%20fusion%0Aapproaches%20and%20examine%20the%20benefits%20of%20additionally%20predicting%203D%20occupancy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.11762v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMUVO%253A%2520A%2520Multimodal%2520Generative%2520World%2520Model%2520for%2520Autonomous%2520Driving%2520with%250A%2520%2520Geometric%2520Representations%26entry.906535625%3DDaniel%2520Bogdoll%2520and%2520Yitian%2520Yang%2520and%2520Tim%2520Joseph%2520and%2520Melih%2520Yazgan%2520and%2520J.%2520Marius%2520Z%25C3%25B6llner%26entry.1292438233%3D%2520%2520World%2520models%2520for%2520autonomous%2520driving%2520have%2520the%2520potential%2520to%2520dramatically%250Aimprove%2520the%2520reasoning%2520capabilities%2520of%2520today%2527s%2520systems.%2520However%252C%2520most%2520works%250Afocus%2520on%2520camera%2520data%252C%2520with%2520only%2520a%2520few%2520that%2520leverage%2520lidar%2520data%2520or%2520combine%2520both%250Ato%2520better%2520represent%2520autonomous%2520vehicle%2520sensor%2520setups.%2520In%2520addition%252C%2520raw%2520sensor%250Apredictions%2520are%2520less%2520actionable%2520than%25203D%2520occupancy%2520predictions%252C%2520but%2520there%2520are%2520no%250Aworks%2520examining%2520the%2520effects%2520of%2520combining%2520both%2520multimodal%2520sensor%2520data%2520and%25203D%250Aoccupancy%2520prediction.%2520In%2520this%2520work%252C%2520we%2520perform%2520a%2520set%2520of%2520experiments%2520with%2520a%250AMUltimodal%2520World%2520Model%2520with%2520Geometric%2520VOxel%2520representations%2520%2528MUVO%2529%2520to%2520evaluate%250Adifferent%2520sensor%2520fusion%2520strategies%2520to%2520better%2520understand%2520the%2520effects%2520on%2520sensor%250Adata%2520prediction.%2520We%2520also%2520analyze%2520potential%2520weaknesses%2520of%2520current%2520sensor%2520fusion%250Aapproaches%2520and%2520examine%2520the%2520benefits%2520of%2520additionally%2520predicting%25203D%2520occupancy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.11762v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MUVO%3A%20A%20Multimodal%20Generative%20World%20Model%20for%20Autonomous%20Driving%20with%0A%20%20Geometric%20Representations&entry.906535625=Daniel%20Bogdoll%20and%20Yitian%20Yang%20and%20Tim%20Joseph%20and%20Melih%20Yazgan%20and%20J.%20Marius%20Z%C3%B6llner&entry.1292438233=%20%20World%20models%20for%20autonomous%20driving%20have%20the%20potential%20to%20dramatically%0Aimprove%20the%20reasoning%20capabilities%20of%20today%27s%20systems.%20However%2C%20most%20works%0Afocus%20on%20camera%20data%2C%20with%20only%20a%20few%20that%20leverage%20lidar%20data%20or%20combine%20both%0Ato%20better%20represent%20autonomous%20vehicle%20sensor%20setups.%20In%20addition%2C%20raw%20sensor%0Apredictions%20are%20less%20actionable%20than%203D%20occupancy%20predictions%2C%20but%20there%20are%20no%0Aworks%20examining%20the%20effects%20of%20combining%20both%20multimodal%20sensor%20data%20and%203D%0Aoccupancy%20prediction.%20In%20this%20work%2C%20we%20perform%20a%20set%20of%20experiments%20with%20a%0AMUltimodal%20World%20Model%20with%20Geometric%20VOxel%20representations%20%28MUVO%29%20to%20evaluate%0Adifferent%20sensor%20fusion%20strategies%20to%20better%20understand%20the%20effects%20on%20sensor%0Adata%20prediction.%20We%20also%20analyze%20potential%20weaknesses%20of%20current%20sensor%20fusion%0Aapproaches%20and%20examine%20the%20benefits%20of%20additionally%20predicting%203D%20occupancy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.11762v4&entry.124074799=Read"},
{"title": "HierarQ: Task-Aware Hierarchical Q-Former for Enhanced Video\n  Understanding", "author": "Shehreen Azad and Vibhav Vineet and Yogesh Singh Rawat", "abstract": "  Despite advancements in multimodal large language models (MLLMs), current\napproaches struggle in medium-to-long video understanding due to frame and\ncontext length limitations. As a result, these models often depend on frame\nsampling, which risks missing key information over time and lacks task-specific\nrelevance. To address these challenges, we introduce HierarQ, a task-aware\nhierarchical Q-Former based framework that sequentially processes frames to\nbypass the need for frame sampling, while avoiding LLM's context length\nlimitations. We introduce a lightweight two-stream language-guided feature\nmodulator to incorporate task awareness in video understanding, with the entity\nstream capturing frame-level object information within a short context and the\nscene stream identifying their broader interactions over longer period of time.\nEach stream is supported by dedicated memory banks which enables our proposed\nHierachical Querying transformer (HierarQ) to effectively capture short and\nlong-term context. Extensive evaluations on 10 video benchmarks across video\nunderstanding, question answering, and captioning tasks demonstrate HierarQ's\nstate-of-the-art performance across most datasets, proving its robustness and\nefficiency for comprehensive video analysis.\n", "link": "http://arxiv.org/abs/2503.08585v2", "date": "2025-04-24", "relevancy": 2.2967, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5838}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5838}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HierarQ%3A%20Task-Aware%20Hierarchical%20Q-Former%20for%20Enhanced%20Video%0A%20%20Understanding&body=Title%3A%20HierarQ%3A%20Task-Aware%20Hierarchical%20Q-Former%20for%20Enhanced%20Video%0A%20%20Understanding%0AAuthor%3A%20Shehreen%20Azad%20and%20Vibhav%20Vineet%20and%20Yogesh%20Singh%20Rawat%0AAbstract%3A%20%20%20Despite%20advancements%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%2C%20current%0Aapproaches%20struggle%20in%20medium-to-long%20video%20understanding%20due%20to%20frame%20and%0Acontext%20length%20limitations.%20As%20a%20result%2C%20these%20models%20often%20depend%20on%20frame%0Asampling%2C%20which%20risks%20missing%20key%20information%20over%20time%20and%20lacks%20task-specific%0Arelevance.%20To%20address%20these%20challenges%2C%20we%20introduce%20HierarQ%2C%20a%20task-aware%0Ahierarchical%20Q-Former%20based%20framework%20that%20sequentially%20processes%20frames%20to%0Abypass%20the%20need%20for%20frame%20sampling%2C%20while%20avoiding%20LLM%27s%20context%20length%0Alimitations.%20We%20introduce%20a%20lightweight%20two-stream%20language-guided%20feature%0Amodulator%20to%20incorporate%20task%20awareness%20in%20video%20understanding%2C%20with%20the%20entity%0Astream%20capturing%20frame-level%20object%20information%20within%20a%20short%20context%20and%20the%0Ascene%20stream%20identifying%20their%20broader%20interactions%20over%20longer%20period%20of%20time.%0AEach%20stream%20is%20supported%20by%20dedicated%20memory%20banks%20which%20enables%20our%20proposed%0AHierachical%20Querying%20transformer%20%28HierarQ%29%20to%20effectively%20capture%20short%20and%0Along-term%20context.%20Extensive%20evaluations%20on%2010%20video%20benchmarks%20across%20video%0Aunderstanding%2C%20question%20answering%2C%20and%20captioning%20tasks%20demonstrate%20HierarQ%27s%0Astate-of-the-art%20performance%20across%20most%20datasets%2C%20proving%20its%20robustness%20and%0Aefficiency%20for%20comprehensive%20video%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.08585v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarQ%253A%2520Task-Aware%2520Hierarchical%2520Q-Former%2520for%2520Enhanced%2520Video%250A%2520%2520Understanding%26entry.906535625%3DShehreen%2520Azad%2520and%2520Vibhav%2520Vineet%2520and%2520Yogesh%2520Singh%2520Rawat%26entry.1292438233%3D%2520%2520Despite%2520advancements%2520in%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%252C%2520current%250Aapproaches%2520struggle%2520in%2520medium-to-long%2520video%2520understanding%2520due%2520to%2520frame%2520and%250Acontext%2520length%2520limitations.%2520As%2520a%2520result%252C%2520these%2520models%2520often%2520depend%2520on%2520frame%250Asampling%252C%2520which%2520risks%2520missing%2520key%2520information%2520over%2520time%2520and%2520lacks%2520task-specific%250Arelevance.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520HierarQ%252C%2520a%2520task-aware%250Ahierarchical%2520Q-Former%2520based%2520framework%2520that%2520sequentially%2520processes%2520frames%2520to%250Abypass%2520the%2520need%2520for%2520frame%2520sampling%252C%2520while%2520avoiding%2520LLM%2527s%2520context%2520length%250Alimitations.%2520We%2520introduce%2520a%2520lightweight%2520two-stream%2520language-guided%2520feature%250Amodulator%2520to%2520incorporate%2520task%2520awareness%2520in%2520video%2520understanding%252C%2520with%2520the%2520entity%250Astream%2520capturing%2520frame-level%2520object%2520information%2520within%2520a%2520short%2520context%2520and%2520the%250Ascene%2520stream%2520identifying%2520their%2520broader%2520interactions%2520over%2520longer%2520period%2520of%2520time.%250AEach%2520stream%2520is%2520supported%2520by%2520dedicated%2520memory%2520banks%2520which%2520enables%2520our%2520proposed%250AHierachical%2520Querying%2520transformer%2520%2528HierarQ%2529%2520to%2520effectively%2520capture%2520short%2520and%250Along-term%2520context.%2520Extensive%2520evaluations%2520on%252010%2520video%2520benchmarks%2520across%2520video%250Aunderstanding%252C%2520question%2520answering%252C%2520and%2520captioning%2520tasks%2520demonstrate%2520HierarQ%2527s%250Astate-of-the-art%2520performance%2520across%2520most%2520datasets%252C%2520proving%2520its%2520robustness%2520and%250Aefficiency%2520for%2520comprehensive%2520video%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.08585v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HierarQ%3A%20Task-Aware%20Hierarchical%20Q-Former%20for%20Enhanced%20Video%0A%20%20Understanding&entry.906535625=Shehreen%20Azad%20and%20Vibhav%20Vineet%20and%20Yogesh%20Singh%20Rawat&entry.1292438233=%20%20Despite%20advancements%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%2C%20current%0Aapproaches%20struggle%20in%20medium-to-long%20video%20understanding%20due%20to%20frame%20and%0Acontext%20length%20limitations.%20As%20a%20result%2C%20these%20models%20often%20depend%20on%20frame%0Asampling%2C%20which%20risks%20missing%20key%20information%20over%20time%20and%20lacks%20task-specific%0Arelevance.%20To%20address%20these%20challenges%2C%20we%20introduce%20HierarQ%2C%20a%20task-aware%0Ahierarchical%20Q-Former%20based%20framework%20that%20sequentially%20processes%20frames%20to%0Abypass%20the%20need%20for%20frame%20sampling%2C%20while%20avoiding%20LLM%27s%20context%20length%0Alimitations.%20We%20introduce%20a%20lightweight%20two-stream%20language-guided%20feature%0Amodulator%20to%20incorporate%20task%20awareness%20in%20video%20understanding%2C%20with%20the%20entity%0Astream%20capturing%20frame-level%20object%20information%20within%20a%20short%20context%20and%20the%0Ascene%20stream%20identifying%20their%20broader%20interactions%20over%20longer%20period%20of%20time.%0AEach%20stream%20is%20supported%20by%20dedicated%20memory%20banks%20which%20enables%20our%20proposed%0AHierachical%20Querying%20transformer%20%28HierarQ%29%20to%20effectively%20capture%20short%20and%0Along-term%20context.%20Extensive%20evaluations%20on%2010%20video%20benchmarks%20across%20video%0Aunderstanding%2C%20question%20answering%2C%20and%20captioning%20tasks%20demonstrate%20HierarQ%27s%0Astate-of-the-art%20performance%20across%20most%20datasets%2C%20proving%20its%20robustness%20and%0Aefficiency%20for%20comprehensive%20video%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.08585v2&entry.124074799=Read"},
{"title": "LiDPM: Rethinking Point Diffusion for Lidar Scene Completion", "author": "Tetiana Martyniuk and Gilles Puy and Alexandre Boulch and Renaud Marlet and Raoul de Charette", "abstract": "  Training diffusion models that work directly on lidar points at the scale of\noutdoor scenes is challenging due to the difficulty of generating fine-grained\ndetails from white noise over a broad field of view. The latest works\naddressing scene completion with diffusion models tackle this problem by\nreformulating the original DDPM as a local diffusion process. It contrasts with\nthe common practice of operating at the level of objects, where vanilla DDPMs\nare currently used. In this work, we close the gap between these two lines of\nwork. We identify approximations in the local diffusion formulation, show that\nthey are not required to operate at the scene level, and that a vanilla DDPM\nwith a well-chosen starting point is enough for completion. Finally, we\ndemonstrate that our method, LiDPM, leads to better results in scene completion\non SemanticKITTI. The project page is https://astra-vision.github.io/LiDPM .\n", "link": "http://arxiv.org/abs/2504.17791v1", "date": "2025-04-24", "relevancy": 2.2812, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6041}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5707}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiDPM%3A%20Rethinking%20Point%20Diffusion%20for%20Lidar%20Scene%20Completion&body=Title%3A%20LiDPM%3A%20Rethinking%20Point%20Diffusion%20for%20Lidar%20Scene%20Completion%0AAuthor%3A%20Tetiana%20Martyniuk%20and%20Gilles%20Puy%20and%20Alexandre%20Boulch%20and%20Renaud%20Marlet%20and%20Raoul%20de%20Charette%0AAbstract%3A%20%20%20Training%20diffusion%20models%20that%20work%20directly%20on%20lidar%20points%20at%20the%20scale%20of%0Aoutdoor%20scenes%20is%20challenging%20due%20to%20the%20difficulty%20of%20generating%20fine-grained%0Adetails%20from%20white%20noise%20over%20a%20broad%20field%20of%20view.%20The%20latest%20works%0Aaddressing%20scene%20completion%20with%20diffusion%20models%20tackle%20this%20problem%20by%0Areformulating%20the%20original%20DDPM%20as%20a%20local%20diffusion%20process.%20It%20contrasts%20with%0Athe%20common%20practice%20of%20operating%20at%20the%20level%20of%20objects%2C%20where%20vanilla%20DDPMs%0Aare%20currently%20used.%20In%20this%20work%2C%20we%20close%20the%20gap%20between%20these%20two%20lines%20of%0Awork.%20We%20identify%20approximations%20in%20the%20local%20diffusion%20formulation%2C%20show%20that%0Athey%20are%20not%20required%20to%20operate%20at%20the%20scene%20level%2C%20and%20that%20a%20vanilla%20DDPM%0Awith%20a%20well-chosen%20starting%20point%20is%20enough%20for%20completion.%20Finally%2C%20we%0Ademonstrate%20that%20our%20method%2C%20LiDPM%2C%20leads%20to%20better%20results%20in%20scene%20completion%0Aon%20SemanticKITTI.%20The%20project%20page%20is%20https%3A//astra-vision.github.io/LiDPM%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17791v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiDPM%253A%2520Rethinking%2520Point%2520Diffusion%2520for%2520Lidar%2520Scene%2520Completion%26entry.906535625%3DTetiana%2520Martyniuk%2520and%2520Gilles%2520Puy%2520and%2520Alexandre%2520Boulch%2520and%2520Renaud%2520Marlet%2520and%2520Raoul%2520de%2520Charette%26entry.1292438233%3D%2520%2520Training%2520diffusion%2520models%2520that%2520work%2520directly%2520on%2520lidar%2520points%2520at%2520the%2520scale%2520of%250Aoutdoor%2520scenes%2520is%2520challenging%2520due%2520to%2520the%2520difficulty%2520of%2520generating%2520fine-grained%250Adetails%2520from%2520white%2520noise%2520over%2520a%2520broad%2520field%2520of%2520view.%2520The%2520latest%2520works%250Aaddressing%2520scene%2520completion%2520with%2520diffusion%2520models%2520tackle%2520this%2520problem%2520by%250Areformulating%2520the%2520original%2520DDPM%2520as%2520a%2520local%2520diffusion%2520process.%2520It%2520contrasts%2520with%250Athe%2520common%2520practice%2520of%2520operating%2520at%2520the%2520level%2520of%2520objects%252C%2520where%2520vanilla%2520DDPMs%250Aare%2520currently%2520used.%2520In%2520this%2520work%252C%2520we%2520close%2520the%2520gap%2520between%2520these%2520two%2520lines%2520of%250Awork.%2520We%2520identify%2520approximations%2520in%2520the%2520local%2520diffusion%2520formulation%252C%2520show%2520that%250Athey%2520are%2520not%2520required%2520to%2520operate%2520at%2520the%2520scene%2520level%252C%2520and%2520that%2520a%2520vanilla%2520DDPM%250Awith%2520a%2520well-chosen%2520starting%2520point%2520is%2520enough%2520for%2520completion.%2520Finally%252C%2520we%250Ademonstrate%2520that%2520our%2520method%252C%2520LiDPM%252C%2520leads%2520to%2520better%2520results%2520in%2520scene%2520completion%250Aon%2520SemanticKITTI.%2520The%2520project%2520page%2520is%2520https%253A//astra-vision.github.io/LiDPM%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17791v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiDPM%3A%20Rethinking%20Point%20Diffusion%20for%20Lidar%20Scene%20Completion&entry.906535625=Tetiana%20Martyniuk%20and%20Gilles%20Puy%20and%20Alexandre%20Boulch%20and%20Renaud%20Marlet%20and%20Raoul%20de%20Charette&entry.1292438233=%20%20Training%20diffusion%20models%20that%20work%20directly%20on%20lidar%20points%20at%20the%20scale%20of%0Aoutdoor%20scenes%20is%20challenging%20due%20to%20the%20difficulty%20of%20generating%20fine-grained%0Adetails%20from%20white%20noise%20over%20a%20broad%20field%20of%20view.%20The%20latest%20works%0Aaddressing%20scene%20completion%20with%20diffusion%20models%20tackle%20this%20problem%20by%0Areformulating%20the%20original%20DDPM%20as%20a%20local%20diffusion%20process.%20It%20contrasts%20with%0Athe%20common%20practice%20of%20operating%20at%20the%20level%20of%20objects%2C%20where%20vanilla%20DDPMs%0Aare%20currently%20used.%20In%20this%20work%2C%20we%20close%20the%20gap%20between%20these%20two%20lines%20of%0Awork.%20We%20identify%20approximations%20in%20the%20local%20diffusion%20formulation%2C%20show%20that%0Athey%20are%20not%20required%20to%20operate%20at%20the%20scene%20level%2C%20and%20that%20a%20vanilla%20DDPM%0Awith%20a%20well-chosen%20starting%20point%20is%20enough%20for%20completion.%20Finally%2C%20we%0Ademonstrate%20that%20our%20method%2C%20LiDPM%2C%20leads%20to%20better%20results%20in%20scene%20completion%0Aon%20SemanticKITTI.%20The%20project%20page%20is%20https%3A//astra-vision.github.io/LiDPM%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17791v1&entry.124074799=Read"},
{"title": "Dexterous Manipulation through Imitation Learning: A Survey", "author": "Shan An and Ziyu Meng and Chao Tang and Yuning Zhou and Tengyu Liu and Fangqiang Ding and Shufang Zhang and Yao Mu and Ran Song and Wei Zhang and Zeng-Guang Hou and Hong Zhang", "abstract": "  Dexterous manipulation, which refers to the ability of a robotic hand or\nmulti-fingered end-effector to skillfully control, reorient, and manipulate\nobjects through precise, coordinated finger movements and adaptive force\nmodulation, enables complex interactions similar to human hand dexterity. With\nrecent advances in robotics and machine learning, there is a growing demand for\nthese systems to operate in complex and unstructured environments. Traditional\nmodel-based approaches struggle to generalize across tasks and object\nvariations due to the high dimensionality and complex contact dynamics of\ndexterous manipulation. Although model-free methods such as reinforcement\nlearning (RL) show promise, they require extensive training, large-scale\ninteraction data, and carefully designed rewards for stability and\neffectiveness. Imitation learning (IL) offers an alternative by allowing robots\nto acquire dexterous manipulation skills directly from expert demonstrations,\ncapturing fine-grained coordination and contact dynamics while bypassing the\nneed for explicit modeling and large-scale trial-and-error. This survey\nprovides an overview of dexterous manipulation methods based on imitation\nlearning, details recent advances, and addresses key challenges in the field.\nAdditionally, it explores potential research directions to enhance IL-driven\ndexterous manipulation. Our goal is to offer researchers and practitioners a\ncomprehensive introduction to this rapidly evolving domain.\n", "link": "http://arxiv.org/abs/2504.03515v2", "date": "2025-04-24", "relevancy": 2.2765, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6004}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5893}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dexterous%20Manipulation%20through%20Imitation%20Learning%3A%20A%20Survey&body=Title%3A%20Dexterous%20Manipulation%20through%20Imitation%20Learning%3A%20A%20Survey%0AAuthor%3A%20Shan%20An%20and%20Ziyu%20Meng%20and%20Chao%20Tang%20and%20Yuning%20Zhou%20and%20Tengyu%20Liu%20and%20Fangqiang%20Ding%20and%20Shufang%20Zhang%20and%20Yao%20Mu%20and%20Ran%20Song%20and%20Wei%20Zhang%20and%20Zeng-Guang%20Hou%20and%20Hong%20Zhang%0AAbstract%3A%20%20%20Dexterous%20manipulation%2C%20which%20refers%20to%20the%20ability%20of%20a%20robotic%20hand%20or%0Amulti-fingered%20end-effector%20to%20skillfully%20control%2C%20reorient%2C%20and%20manipulate%0Aobjects%20through%20precise%2C%20coordinated%20finger%20movements%20and%20adaptive%20force%0Amodulation%2C%20enables%20complex%20interactions%20similar%20to%20human%20hand%20dexterity.%20With%0Arecent%20advances%20in%20robotics%20and%20machine%20learning%2C%20there%20is%20a%20growing%20demand%20for%0Athese%20systems%20to%20operate%20in%20complex%20and%20unstructured%20environments.%20Traditional%0Amodel-based%20approaches%20struggle%20to%20generalize%20across%20tasks%20and%20object%0Avariations%20due%20to%20the%20high%20dimensionality%20and%20complex%20contact%20dynamics%20of%0Adexterous%20manipulation.%20Although%20model-free%20methods%20such%20as%20reinforcement%0Alearning%20%28RL%29%20show%20promise%2C%20they%20require%20extensive%20training%2C%20large-scale%0Ainteraction%20data%2C%20and%20carefully%20designed%20rewards%20for%20stability%20and%0Aeffectiveness.%20Imitation%20learning%20%28IL%29%20offers%20an%20alternative%20by%20allowing%20robots%0Ato%20acquire%20dexterous%20manipulation%20skills%20directly%20from%20expert%20demonstrations%2C%0Acapturing%20fine-grained%20coordination%20and%20contact%20dynamics%20while%20bypassing%20the%0Aneed%20for%20explicit%20modeling%20and%20large-scale%20trial-and-error.%20This%20survey%0Aprovides%20an%20overview%20of%20dexterous%20manipulation%20methods%20based%20on%20imitation%0Alearning%2C%20details%20recent%20advances%2C%20and%20addresses%20key%20challenges%20in%20the%20field.%0AAdditionally%2C%20it%20explores%20potential%20research%20directions%20to%20enhance%20IL-driven%0Adexterous%20manipulation.%20Our%20goal%20is%20to%20offer%20researchers%20and%20practitioners%20a%0Acomprehensive%20introduction%20to%20this%20rapidly%20evolving%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.03515v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDexterous%2520Manipulation%2520through%2520Imitation%2520Learning%253A%2520A%2520Survey%26entry.906535625%3DShan%2520An%2520and%2520Ziyu%2520Meng%2520and%2520Chao%2520Tang%2520and%2520Yuning%2520Zhou%2520and%2520Tengyu%2520Liu%2520and%2520Fangqiang%2520Ding%2520and%2520Shufang%2520Zhang%2520and%2520Yao%2520Mu%2520and%2520Ran%2520Song%2520and%2520Wei%2520Zhang%2520and%2520Zeng-Guang%2520Hou%2520and%2520Hong%2520Zhang%26entry.1292438233%3D%2520%2520Dexterous%2520manipulation%252C%2520which%2520refers%2520to%2520the%2520ability%2520of%2520a%2520robotic%2520hand%2520or%250Amulti-fingered%2520end-effector%2520to%2520skillfully%2520control%252C%2520reorient%252C%2520and%2520manipulate%250Aobjects%2520through%2520precise%252C%2520coordinated%2520finger%2520movements%2520and%2520adaptive%2520force%250Amodulation%252C%2520enables%2520complex%2520interactions%2520similar%2520to%2520human%2520hand%2520dexterity.%2520With%250Arecent%2520advances%2520in%2520robotics%2520and%2520machine%2520learning%252C%2520there%2520is%2520a%2520growing%2520demand%2520for%250Athese%2520systems%2520to%2520operate%2520in%2520complex%2520and%2520unstructured%2520environments.%2520Traditional%250Amodel-based%2520approaches%2520struggle%2520to%2520generalize%2520across%2520tasks%2520and%2520object%250Avariations%2520due%2520to%2520the%2520high%2520dimensionality%2520and%2520complex%2520contact%2520dynamics%2520of%250Adexterous%2520manipulation.%2520Although%2520model-free%2520methods%2520such%2520as%2520reinforcement%250Alearning%2520%2528RL%2529%2520show%2520promise%252C%2520they%2520require%2520extensive%2520training%252C%2520large-scale%250Ainteraction%2520data%252C%2520and%2520carefully%2520designed%2520rewards%2520for%2520stability%2520and%250Aeffectiveness.%2520Imitation%2520learning%2520%2528IL%2529%2520offers%2520an%2520alternative%2520by%2520allowing%2520robots%250Ato%2520acquire%2520dexterous%2520manipulation%2520skills%2520directly%2520from%2520expert%2520demonstrations%252C%250Acapturing%2520fine-grained%2520coordination%2520and%2520contact%2520dynamics%2520while%2520bypassing%2520the%250Aneed%2520for%2520explicit%2520modeling%2520and%2520large-scale%2520trial-and-error.%2520This%2520survey%250Aprovides%2520an%2520overview%2520of%2520dexterous%2520manipulation%2520methods%2520based%2520on%2520imitation%250Alearning%252C%2520details%2520recent%2520advances%252C%2520and%2520addresses%2520key%2520challenges%2520in%2520the%2520field.%250AAdditionally%252C%2520it%2520explores%2520potential%2520research%2520directions%2520to%2520enhance%2520IL-driven%250Adexterous%2520manipulation.%2520Our%2520goal%2520is%2520to%2520offer%2520researchers%2520and%2520practitioners%2520a%250Acomprehensive%2520introduction%2520to%2520this%2520rapidly%2520evolving%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.03515v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dexterous%20Manipulation%20through%20Imitation%20Learning%3A%20A%20Survey&entry.906535625=Shan%20An%20and%20Ziyu%20Meng%20and%20Chao%20Tang%20and%20Yuning%20Zhou%20and%20Tengyu%20Liu%20and%20Fangqiang%20Ding%20and%20Shufang%20Zhang%20and%20Yao%20Mu%20and%20Ran%20Song%20and%20Wei%20Zhang%20and%20Zeng-Guang%20Hou%20and%20Hong%20Zhang&entry.1292438233=%20%20Dexterous%20manipulation%2C%20which%20refers%20to%20the%20ability%20of%20a%20robotic%20hand%20or%0Amulti-fingered%20end-effector%20to%20skillfully%20control%2C%20reorient%2C%20and%20manipulate%0Aobjects%20through%20precise%2C%20coordinated%20finger%20movements%20and%20adaptive%20force%0Amodulation%2C%20enables%20complex%20interactions%20similar%20to%20human%20hand%20dexterity.%20With%0Arecent%20advances%20in%20robotics%20and%20machine%20learning%2C%20there%20is%20a%20growing%20demand%20for%0Athese%20systems%20to%20operate%20in%20complex%20and%20unstructured%20environments.%20Traditional%0Amodel-based%20approaches%20struggle%20to%20generalize%20across%20tasks%20and%20object%0Avariations%20due%20to%20the%20high%20dimensionality%20and%20complex%20contact%20dynamics%20of%0Adexterous%20manipulation.%20Although%20model-free%20methods%20such%20as%20reinforcement%0Alearning%20%28RL%29%20show%20promise%2C%20they%20require%20extensive%20training%2C%20large-scale%0Ainteraction%20data%2C%20and%20carefully%20designed%20rewards%20for%20stability%20and%0Aeffectiveness.%20Imitation%20learning%20%28IL%29%20offers%20an%20alternative%20by%20allowing%20robots%0Ato%20acquire%20dexterous%20manipulation%20skills%20directly%20from%20expert%20demonstrations%2C%0Acapturing%20fine-grained%20coordination%20and%20contact%20dynamics%20while%20bypassing%20the%0Aneed%20for%20explicit%20modeling%20and%20large-scale%20trial-and-error.%20This%20survey%0Aprovides%20an%20overview%20of%20dexterous%20manipulation%20methods%20based%20on%20imitation%0Alearning%2C%20details%20recent%20advances%2C%20and%20addresses%20key%20challenges%20in%20the%20field.%0AAdditionally%2C%20it%20explores%20potential%20research%20directions%20to%20enhance%20IL-driven%0Adexterous%20manipulation.%20Our%20goal%20is%20to%20offer%20researchers%20and%20practitioners%20a%0Acomprehensive%20introduction%20to%20this%20rapidly%20evolving%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.03515v2&entry.124074799=Read"},
{"title": "Empirical Comparison of Four Stereoscopic Depth Sensing Cameras for\n  Robotics Applications", "author": "Lukas Rustler and Vojtech Volprecht and Matej Hoffmann", "abstract": "  Depth sensing is an essential technology in robotics and many other fields.\nMany depth sensing (or RGB-D) cameras are available on the market and selecting\nthe best one for your application can be challenging. In this work, we tested\nfour stereoscopic RGB-D cameras that sense the distance by using two images\nfrom slightly different views. We empirically compared four cameras (Intel\nRealSense D435, Intel RealSense D455, StereoLabs ZED 2, and Luxonis OAK-D Pro)\nin three scenarios: (i) planar surface perception, (ii) plastic doll\nperception, (iii) household object perception (YCB dataset). We recorded and\nevaluated more than 3,000 RGB-D frames for each camera. For table-top robotics\nscenarios with distance to objects up to one meter, the best performance is\nprovided by the D435 camera that is able to perceive with an error under 1 cm\nin all of the tested scenarios. For longer distances, the other three models\nperform better, making them more suitable for some mobile robotics\napplications. OAK-D Pro additionally offers integrated AI modules (e.g., object\nand human keypoint detection). ZED 2 is overall the best camera which is able\nto keep the error under 3 cm even at 4 meters. However, it is not a standalone\ndevice and requires a computer with a GPU for depth data acquisition. All data\n(more than 12,000 RGB-D frames) are publicly available at\nhttps://rustlluk.github.io/rgbd-comparison.\n", "link": "http://arxiv.org/abs/2501.07421v2", "date": "2025-04-24", "relevancy": 2.2692, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5762}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5694}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Empirical%20Comparison%20of%20Four%20Stereoscopic%20Depth%20Sensing%20Cameras%20for%0A%20%20Robotics%20Applications&body=Title%3A%20Empirical%20Comparison%20of%20Four%20Stereoscopic%20Depth%20Sensing%20Cameras%20for%0A%20%20Robotics%20Applications%0AAuthor%3A%20Lukas%20Rustler%20and%20Vojtech%20Volprecht%20and%20Matej%20Hoffmann%0AAbstract%3A%20%20%20Depth%20sensing%20is%20an%20essential%20technology%20in%20robotics%20and%20many%20other%20fields.%0AMany%20depth%20sensing%20%28or%20RGB-D%29%20cameras%20are%20available%20on%20the%20market%20and%20selecting%0Athe%20best%20one%20for%20your%20application%20can%20be%20challenging.%20In%20this%20work%2C%20we%20tested%0Afour%20stereoscopic%20RGB-D%20cameras%20that%20sense%20the%20distance%20by%20using%20two%20images%0Afrom%20slightly%20different%20views.%20We%20empirically%20compared%20four%20cameras%20%28Intel%0ARealSense%20D435%2C%20Intel%20RealSense%20D455%2C%20StereoLabs%20ZED%202%2C%20and%20Luxonis%20OAK-D%20Pro%29%0Ain%20three%20scenarios%3A%20%28i%29%20planar%20surface%20perception%2C%20%28ii%29%20plastic%20doll%0Aperception%2C%20%28iii%29%20household%20object%20perception%20%28YCB%20dataset%29.%20We%20recorded%20and%0Aevaluated%20more%20than%203%2C000%20RGB-D%20frames%20for%20each%20camera.%20For%20table-top%20robotics%0Ascenarios%20with%20distance%20to%20objects%20up%20to%20one%20meter%2C%20the%20best%20performance%20is%0Aprovided%20by%20the%20D435%20camera%20that%20is%20able%20to%20perceive%20with%20an%20error%20under%201%20cm%0Ain%20all%20of%20the%20tested%20scenarios.%20For%20longer%20distances%2C%20the%20other%20three%20models%0Aperform%20better%2C%20making%20them%20more%20suitable%20for%20some%20mobile%20robotics%0Aapplications.%20OAK-D%20Pro%20additionally%20offers%20integrated%20AI%20modules%20%28e.g.%2C%20object%0Aand%20human%20keypoint%20detection%29.%20ZED%202%20is%20overall%20the%20best%20camera%20which%20is%20able%0Ato%20keep%20the%20error%20under%203%20cm%20even%20at%204%20meters.%20However%2C%20it%20is%20not%20a%20standalone%0Adevice%20and%20requires%20a%20computer%20with%20a%20GPU%20for%20depth%20data%20acquisition.%20All%20data%0A%28more%20than%2012%2C000%20RGB-D%20frames%29%20are%20publicly%20available%20at%0Ahttps%3A//rustlluk.github.io/rgbd-comparison.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07421v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmpirical%2520Comparison%2520of%2520Four%2520Stereoscopic%2520Depth%2520Sensing%2520Cameras%2520for%250A%2520%2520Robotics%2520Applications%26entry.906535625%3DLukas%2520Rustler%2520and%2520Vojtech%2520Volprecht%2520and%2520Matej%2520Hoffmann%26entry.1292438233%3D%2520%2520Depth%2520sensing%2520is%2520an%2520essential%2520technology%2520in%2520robotics%2520and%2520many%2520other%2520fields.%250AMany%2520depth%2520sensing%2520%2528or%2520RGB-D%2529%2520cameras%2520are%2520available%2520on%2520the%2520market%2520and%2520selecting%250Athe%2520best%2520one%2520for%2520your%2520application%2520can%2520be%2520challenging.%2520In%2520this%2520work%252C%2520we%2520tested%250Afour%2520stereoscopic%2520RGB-D%2520cameras%2520that%2520sense%2520the%2520distance%2520by%2520using%2520two%2520images%250Afrom%2520slightly%2520different%2520views.%2520We%2520empirically%2520compared%2520four%2520cameras%2520%2528Intel%250ARealSense%2520D435%252C%2520Intel%2520RealSense%2520D455%252C%2520StereoLabs%2520ZED%25202%252C%2520and%2520Luxonis%2520OAK-D%2520Pro%2529%250Ain%2520three%2520scenarios%253A%2520%2528i%2529%2520planar%2520surface%2520perception%252C%2520%2528ii%2529%2520plastic%2520doll%250Aperception%252C%2520%2528iii%2529%2520household%2520object%2520perception%2520%2528YCB%2520dataset%2529.%2520We%2520recorded%2520and%250Aevaluated%2520more%2520than%25203%252C000%2520RGB-D%2520frames%2520for%2520each%2520camera.%2520For%2520table-top%2520robotics%250Ascenarios%2520with%2520distance%2520to%2520objects%2520up%2520to%2520one%2520meter%252C%2520the%2520best%2520performance%2520is%250Aprovided%2520by%2520the%2520D435%2520camera%2520that%2520is%2520able%2520to%2520perceive%2520with%2520an%2520error%2520under%25201%2520cm%250Ain%2520all%2520of%2520the%2520tested%2520scenarios.%2520For%2520longer%2520distances%252C%2520the%2520other%2520three%2520models%250Aperform%2520better%252C%2520making%2520them%2520more%2520suitable%2520for%2520some%2520mobile%2520robotics%250Aapplications.%2520OAK-D%2520Pro%2520additionally%2520offers%2520integrated%2520AI%2520modules%2520%2528e.g.%252C%2520object%250Aand%2520human%2520keypoint%2520detection%2529.%2520ZED%25202%2520is%2520overall%2520the%2520best%2520camera%2520which%2520is%2520able%250Ato%2520keep%2520the%2520error%2520under%25203%2520cm%2520even%2520at%25204%2520meters.%2520However%252C%2520it%2520is%2520not%2520a%2520standalone%250Adevice%2520and%2520requires%2520a%2520computer%2520with%2520a%2520GPU%2520for%2520depth%2520data%2520acquisition.%2520All%2520data%250A%2528more%2520than%252012%252C000%2520RGB-D%2520frames%2529%2520are%2520publicly%2520available%2520at%250Ahttps%253A//rustlluk.github.io/rgbd-comparison.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07421v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empirical%20Comparison%20of%20Four%20Stereoscopic%20Depth%20Sensing%20Cameras%20for%0A%20%20Robotics%20Applications&entry.906535625=Lukas%20Rustler%20and%20Vojtech%20Volprecht%20and%20Matej%20Hoffmann&entry.1292438233=%20%20Depth%20sensing%20is%20an%20essential%20technology%20in%20robotics%20and%20many%20other%20fields.%0AMany%20depth%20sensing%20%28or%20RGB-D%29%20cameras%20are%20available%20on%20the%20market%20and%20selecting%0Athe%20best%20one%20for%20your%20application%20can%20be%20challenging.%20In%20this%20work%2C%20we%20tested%0Afour%20stereoscopic%20RGB-D%20cameras%20that%20sense%20the%20distance%20by%20using%20two%20images%0Afrom%20slightly%20different%20views.%20We%20empirically%20compared%20four%20cameras%20%28Intel%0ARealSense%20D435%2C%20Intel%20RealSense%20D455%2C%20StereoLabs%20ZED%202%2C%20and%20Luxonis%20OAK-D%20Pro%29%0Ain%20three%20scenarios%3A%20%28i%29%20planar%20surface%20perception%2C%20%28ii%29%20plastic%20doll%0Aperception%2C%20%28iii%29%20household%20object%20perception%20%28YCB%20dataset%29.%20We%20recorded%20and%0Aevaluated%20more%20than%203%2C000%20RGB-D%20frames%20for%20each%20camera.%20For%20table-top%20robotics%0Ascenarios%20with%20distance%20to%20objects%20up%20to%20one%20meter%2C%20the%20best%20performance%20is%0Aprovided%20by%20the%20D435%20camera%20that%20is%20able%20to%20perceive%20with%20an%20error%20under%201%20cm%0Ain%20all%20of%20the%20tested%20scenarios.%20For%20longer%20distances%2C%20the%20other%20three%20models%0Aperform%20better%2C%20making%20them%20more%20suitable%20for%20some%20mobile%20robotics%0Aapplications.%20OAK-D%20Pro%20additionally%20offers%20integrated%20AI%20modules%20%28e.g.%2C%20object%0Aand%20human%20keypoint%20detection%29.%20ZED%202%20is%20overall%20the%20best%20camera%20which%20is%20able%0Ato%20keep%20the%20error%20under%203%20cm%20even%20at%204%20meters.%20However%2C%20it%20is%20not%20a%20standalone%0Adevice%20and%20requires%20a%20computer%20with%20a%20GPU%20for%20depth%20data%20acquisition.%20All%20data%0A%28more%20than%2012%2C000%20RGB-D%20frames%29%20are%20publicly%20available%20at%0Ahttps%3A//rustlluk.github.io/rgbd-comparison.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07421v2&entry.124074799=Read"},
{"title": "FedMerge: Federated Personalization via Model Merging", "author": "Shutong Chen and Tianyi Zhou and Guodong Long and Jing Jiang and Chengqi Zhang", "abstract": "  One global model in federated learning (FL) might not be sufficient to serve\nmany clients with non-IID tasks and distributions. While there has been\nadvances in FL to train multiple global models for better personalization, they\nonly provide limited choices to clients so local finetuning is still\nindispensable. In this paper, we propose a novel ``FedMerge'' approach that can\ncreate a personalized model per client by simply merging multiple global models\nwith automatically optimized and customized weights. In FedMerge, a few global\nmodels can serve many non-IID clients, even without further local finetuning.\nWe formulate this problem as a joint optimization of global models and the\nmerging weights for each client. Unlike existing FL approaches where the server\nbroadcasts one or multiple global models to all clients, the server only needs\nto send a customized, merged model to each client. Moreover, instead of\nperiodically interrupting the local training and re-initializing it to a global\nmodel, the merged model aligns better with each client's task and data\ndistribution, smoothening the local-global gap between consecutive rounds\ncaused by client drift. We evaluate FedMerge on three different non-IID\nsettings applied to different domains with diverse tasks and data types, in\nwhich FedMerge consistently outperforms existing FL approaches, including\nclustering-based and mixture-of-experts (MoE) based methods.\n", "link": "http://arxiv.org/abs/2504.06768v2", "date": "2025-04-24", "relevancy": 2.2649, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4584}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4512}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedMerge%3A%20Federated%20Personalization%20via%20Model%20Merging&body=Title%3A%20FedMerge%3A%20Federated%20Personalization%20via%20Model%20Merging%0AAuthor%3A%20Shutong%20Chen%20and%20Tianyi%20Zhou%20and%20Guodong%20Long%20and%20Jing%20Jiang%20and%20Chengqi%20Zhang%0AAbstract%3A%20%20%20One%20global%20model%20in%20federated%20learning%20%28FL%29%20might%20not%20be%20sufficient%20to%20serve%0Amany%20clients%20with%20non-IID%20tasks%20and%20distributions.%20While%20there%20has%20been%0Aadvances%20in%20FL%20to%20train%20multiple%20global%20models%20for%20better%20personalization%2C%20they%0Aonly%20provide%20limited%20choices%20to%20clients%20so%20local%20finetuning%20is%20still%0Aindispensable.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20%60%60FedMerge%27%27%20approach%20that%20can%0Acreate%20a%20personalized%20model%20per%20client%20by%20simply%20merging%20multiple%20global%20models%0Awith%20automatically%20optimized%20and%20customized%20weights.%20In%20FedMerge%2C%20a%20few%20global%0Amodels%20can%20serve%20many%20non-IID%20clients%2C%20even%20without%20further%20local%20finetuning.%0AWe%20formulate%20this%20problem%20as%20a%20joint%20optimization%20of%20global%20models%20and%20the%0Amerging%20weights%20for%20each%20client.%20Unlike%20existing%20FL%20approaches%20where%20the%20server%0Abroadcasts%20one%20or%20multiple%20global%20models%20to%20all%20clients%2C%20the%20server%20only%20needs%0Ato%20send%20a%20customized%2C%20merged%20model%20to%20each%20client.%20Moreover%2C%20instead%20of%0Aperiodically%20interrupting%20the%20local%20training%20and%20re-initializing%20it%20to%20a%20global%0Amodel%2C%20the%20merged%20model%20aligns%20better%20with%20each%20client%27s%20task%20and%20data%0Adistribution%2C%20smoothening%20the%20local-global%20gap%20between%20consecutive%20rounds%0Acaused%20by%20client%20drift.%20We%20evaluate%20FedMerge%20on%20three%20different%20non-IID%0Asettings%20applied%20to%20different%20domains%20with%20diverse%20tasks%20and%20data%20types%2C%20in%0Awhich%20FedMerge%20consistently%20outperforms%20existing%20FL%20approaches%2C%20including%0Aclustering-based%20and%20mixture-of-experts%20%28MoE%29%20based%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.06768v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedMerge%253A%2520Federated%2520Personalization%2520via%2520Model%2520Merging%26entry.906535625%3DShutong%2520Chen%2520and%2520Tianyi%2520Zhou%2520and%2520Guodong%2520Long%2520and%2520Jing%2520Jiang%2520and%2520Chengqi%2520Zhang%26entry.1292438233%3D%2520%2520One%2520global%2520model%2520in%2520federated%2520learning%2520%2528FL%2529%2520might%2520not%2520be%2520sufficient%2520to%2520serve%250Amany%2520clients%2520with%2520non-IID%2520tasks%2520and%2520distributions.%2520While%2520there%2520has%2520been%250Aadvances%2520in%2520FL%2520to%2520train%2520multiple%2520global%2520models%2520for%2520better%2520personalization%252C%2520they%250Aonly%2520provide%2520limited%2520choices%2520to%2520clients%2520so%2520local%2520finetuning%2520is%2520still%250Aindispensable.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520%2560%2560FedMerge%2527%2527%2520approach%2520that%2520can%250Acreate%2520a%2520personalized%2520model%2520per%2520client%2520by%2520simply%2520merging%2520multiple%2520global%2520models%250Awith%2520automatically%2520optimized%2520and%2520customized%2520weights.%2520In%2520FedMerge%252C%2520a%2520few%2520global%250Amodels%2520can%2520serve%2520many%2520non-IID%2520clients%252C%2520even%2520without%2520further%2520local%2520finetuning.%250AWe%2520formulate%2520this%2520problem%2520as%2520a%2520joint%2520optimization%2520of%2520global%2520models%2520and%2520the%250Amerging%2520weights%2520for%2520each%2520client.%2520Unlike%2520existing%2520FL%2520approaches%2520where%2520the%2520server%250Abroadcasts%2520one%2520or%2520multiple%2520global%2520models%2520to%2520all%2520clients%252C%2520the%2520server%2520only%2520needs%250Ato%2520send%2520a%2520customized%252C%2520merged%2520model%2520to%2520each%2520client.%2520Moreover%252C%2520instead%2520of%250Aperiodically%2520interrupting%2520the%2520local%2520training%2520and%2520re-initializing%2520it%2520to%2520a%2520global%250Amodel%252C%2520the%2520merged%2520model%2520aligns%2520better%2520with%2520each%2520client%2527s%2520task%2520and%2520data%250Adistribution%252C%2520smoothening%2520the%2520local-global%2520gap%2520between%2520consecutive%2520rounds%250Acaused%2520by%2520client%2520drift.%2520We%2520evaluate%2520FedMerge%2520on%2520three%2520different%2520non-IID%250Asettings%2520applied%2520to%2520different%2520domains%2520with%2520diverse%2520tasks%2520and%2520data%2520types%252C%2520in%250Awhich%2520FedMerge%2520consistently%2520outperforms%2520existing%2520FL%2520approaches%252C%2520including%250Aclustering-based%2520and%2520mixture-of-experts%2520%2528MoE%2529%2520based%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.06768v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedMerge%3A%20Federated%20Personalization%20via%20Model%20Merging&entry.906535625=Shutong%20Chen%20and%20Tianyi%20Zhou%20and%20Guodong%20Long%20and%20Jing%20Jiang%20and%20Chengqi%20Zhang&entry.1292438233=%20%20One%20global%20model%20in%20federated%20learning%20%28FL%29%20might%20not%20be%20sufficient%20to%20serve%0Amany%20clients%20with%20non-IID%20tasks%20and%20distributions.%20While%20there%20has%20been%0Aadvances%20in%20FL%20to%20train%20multiple%20global%20models%20for%20better%20personalization%2C%20they%0Aonly%20provide%20limited%20choices%20to%20clients%20so%20local%20finetuning%20is%20still%0Aindispensable.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20%60%60FedMerge%27%27%20approach%20that%20can%0Acreate%20a%20personalized%20model%20per%20client%20by%20simply%20merging%20multiple%20global%20models%0Awith%20automatically%20optimized%20and%20customized%20weights.%20In%20FedMerge%2C%20a%20few%20global%0Amodels%20can%20serve%20many%20non-IID%20clients%2C%20even%20without%20further%20local%20finetuning.%0AWe%20formulate%20this%20problem%20as%20a%20joint%20optimization%20of%20global%20models%20and%20the%0Amerging%20weights%20for%20each%20client.%20Unlike%20existing%20FL%20approaches%20where%20the%20server%0Abroadcasts%20one%20or%20multiple%20global%20models%20to%20all%20clients%2C%20the%20server%20only%20needs%0Ato%20send%20a%20customized%2C%20merged%20model%20to%20each%20client.%20Moreover%2C%20instead%20of%0Aperiodically%20interrupting%20the%20local%20training%20and%20re-initializing%20it%20to%20a%20global%0Amodel%2C%20the%20merged%20model%20aligns%20better%20with%20each%20client%27s%20task%20and%20data%0Adistribution%2C%20smoothening%20the%20local-global%20gap%20between%20consecutive%20rounds%0Acaused%20by%20client%20drift.%20We%20evaluate%20FedMerge%20on%20three%20different%20non-IID%0Asettings%20applied%20to%20different%20domains%20with%20diverse%20tasks%20and%20data%20types%2C%20in%0Awhich%20FedMerge%20consistently%20outperforms%20existing%20FL%20approaches%2C%20including%0Aclustering-based%20and%20mixture-of-experts%20%28MoE%29%20based%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.06768v2&entry.124074799=Read"},
{"title": "Towards Spatially-Lucid AI Classification in Non-Euclidean Space: An\n  Application for MxIF Oncology Data", "author": "Majid Farhadloo and Arun Sharma and Jayant Gupta and Alexey Leontovich and Svetomir N. Markovic and Shashi Shekhar", "abstract": "  Given multi-category point sets from different place-types, our goal is to\ndevelop a spatially-lucid classifier that can distinguish between two classes\nbased on the arrangements of their points. This problem is important for many\napplications, such as oncology, for analyzing immune-tumor relationships and\ndesigning new immunotherapies. It is challenging due to spatial variability and\ninterpretability needs. Previously proposed techniques require dense training\ndata or have limited ability to handle significant spatial variability within a\nsingle place-type. Most importantly, these deep neural network (DNN) approaches\nare not designed to work in non-Euclidean space, particularly point sets.\nExisting non-Euclidean DNN methods are limited to one-size-fits-all approaches.\nWe explore a spatial ensemble framework that explicitly uses different training\nstrategies, including weighted-distance learning rate and spatial domain\nadaptation, on various place-types for spatially-lucid classification.\nExperimental results on real-world datasets (e.g., MxIF oncology data) show\nthat the proposed framework provides higher prediction accuracy than baseline\nmethods.\n", "link": "http://arxiv.org/abs/2402.14974v2", "date": "2025-04-24", "relevancy": 2.2626, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5956}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5469}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Spatially-Lucid%20AI%20Classification%20in%20Non-Euclidean%20Space%3A%20An%0A%20%20Application%20for%20MxIF%20Oncology%20Data&body=Title%3A%20Towards%20Spatially-Lucid%20AI%20Classification%20in%20Non-Euclidean%20Space%3A%20An%0A%20%20Application%20for%20MxIF%20Oncology%20Data%0AAuthor%3A%20Majid%20Farhadloo%20and%20Arun%20Sharma%20and%20Jayant%20Gupta%20and%20Alexey%20Leontovich%20and%20Svetomir%20N.%20Markovic%20and%20Shashi%20Shekhar%0AAbstract%3A%20%20%20Given%20multi-category%20point%20sets%20from%20different%20place-types%2C%20our%20goal%20is%20to%0Adevelop%20a%20spatially-lucid%20classifier%20that%20can%20distinguish%20between%20two%20classes%0Abased%20on%20the%20arrangements%20of%20their%20points.%20This%20problem%20is%20important%20for%20many%0Aapplications%2C%20such%20as%20oncology%2C%20for%20analyzing%20immune-tumor%20relationships%20and%0Adesigning%20new%20immunotherapies.%20It%20is%20challenging%20due%20to%20spatial%20variability%20and%0Ainterpretability%20needs.%20Previously%20proposed%20techniques%20require%20dense%20training%0Adata%20or%20have%20limited%20ability%20to%20handle%20significant%20spatial%20variability%20within%20a%0Asingle%20place-type.%20Most%20importantly%2C%20these%20deep%20neural%20network%20%28DNN%29%20approaches%0Aare%20not%20designed%20to%20work%20in%20non-Euclidean%20space%2C%20particularly%20point%20sets.%0AExisting%20non-Euclidean%20DNN%20methods%20are%20limited%20to%20one-size-fits-all%20approaches.%0AWe%20explore%20a%20spatial%20ensemble%20framework%20that%20explicitly%20uses%20different%20training%0Astrategies%2C%20including%20weighted-distance%20learning%20rate%20and%20spatial%20domain%0Aadaptation%2C%20on%20various%20place-types%20for%20spatially-lucid%20classification.%0AExperimental%20results%20on%20real-world%20datasets%20%28e.g.%2C%20MxIF%20oncology%20data%29%20show%0Athat%20the%20proposed%20framework%20provides%20higher%20prediction%20accuracy%20than%20baseline%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14974v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Spatially-Lucid%2520AI%2520Classification%2520in%2520Non-Euclidean%2520Space%253A%2520An%250A%2520%2520Application%2520for%2520MxIF%2520Oncology%2520Data%26entry.906535625%3DMajid%2520Farhadloo%2520and%2520Arun%2520Sharma%2520and%2520Jayant%2520Gupta%2520and%2520Alexey%2520Leontovich%2520and%2520Svetomir%2520N.%2520Markovic%2520and%2520Shashi%2520Shekhar%26entry.1292438233%3D%2520%2520Given%2520multi-category%2520point%2520sets%2520from%2520different%2520place-types%252C%2520our%2520goal%2520is%2520to%250Adevelop%2520a%2520spatially-lucid%2520classifier%2520that%2520can%2520distinguish%2520between%2520two%2520classes%250Abased%2520on%2520the%2520arrangements%2520of%2520their%2520points.%2520This%2520problem%2520is%2520important%2520for%2520many%250Aapplications%252C%2520such%2520as%2520oncology%252C%2520for%2520analyzing%2520immune-tumor%2520relationships%2520and%250Adesigning%2520new%2520immunotherapies.%2520It%2520is%2520challenging%2520due%2520to%2520spatial%2520variability%2520and%250Ainterpretability%2520needs.%2520Previously%2520proposed%2520techniques%2520require%2520dense%2520training%250Adata%2520or%2520have%2520limited%2520ability%2520to%2520handle%2520significant%2520spatial%2520variability%2520within%2520a%250Asingle%2520place-type.%2520Most%2520importantly%252C%2520these%2520deep%2520neural%2520network%2520%2528DNN%2529%2520approaches%250Aare%2520not%2520designed%2520to%2520work%2520in%2520non-Euclidean%2520space%252C%2520particularly%2520point%2520sets.%250AExisting%2520non-Euclidean%2520DNN%2520methods%2520are%2520limited%2520to%2520one-size-fits-all%2520approaches.%250AWe%2520explore%2520a%2520spatial%2520ensemble%2520framework%2520that%2520explicitly%2520uses%2520different%2520training%250Astrategies%252C%2520including%2520weighted-distance%2520learning%2520rate%2520and%2520spatial%2520domain%250Aadaptation%252C%2520on%2520various%2520place-types%2520for%2520spatially-lucid%2520classification.%250AExperimental%2520results%2520on%2520real-world%2520datasets%2520%2528e.g.%252C%2520MxIF%2520oncology%2520data%2529%2520show%250Athat%2520the%2520proposed%2520framework%2520provides%2520higher%2520prediction%2520accuracy%2520than%2520baseline%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14974v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Spatially-Lucid%20AI%20Classification%20in%20Non-Euclidean%20Space%3A%20An%0A%20%20Application%20for%20MxIF%20Oncology%20Data&entry.906535625=Majid%20Farhadloo%20and%20Arun%20Sharma%20and%20Jayant%20Gupta%20and%20Alexey%20Leontovich%20and%20Svetomir%20N.%20Markovic%20and%20Shashi%20Shekhar&entry.1292438233=%20%20Given%20multi-category%20point%20sets%20from%20different%20place-types%2C%20our%20goal%20is%20to%0Adevelop%20a%20spatially-lucid%20classifier%20that%20can%20distinguish%20between%20two%20classes%0Abased%20on%20the%20arrangements%20of%20their%20points.%20This%20problem%20is%20important%20for%20many%0Aapplications%2C%20such%20as%20oncology%2C%20for%20analyzing%20immune-tumor%20relationships%20and%0Adesigning%20new%20immunotherapies.%20It%20is%20challenging%20due%20to%20spatial%20variability%20and%0Ainterpretability%20needs.%20Previously%20proposed%20techniques%20require%20dense%20training%0Adata%20or%20have%20limited%20ability%20to%20handle%20significant%20spatial%20variability%20within%20a%0Asingle%20place-type.%20Most%20importantly%2C%20these%20deep%20neural%20network%20%28DNN%29%20approaches%0Aare%20not%20designed%20to%20work%20in%20non-Euclidean%20space%2C%20particularly%20point%20sets.%0AExisting%20non-Euclidean%20DNN%20methods%20are%20limited%20to%20one-size-fits-all%20approaches.%0AWe%20explore%20a%20spatial%20ensemble%20framework%20that%20explicitly%20uses%20different%20training%0Astrategies%2C%20including%20weighted-distance%20learning%20rate%20and%20spatial%20domain%0Aadaptation%2C%20on%20various%20place-types%20for%20spatially-lucid%20classification.%0AExperimental%20results%20on%20real-world%20datasets%20%28e.g.%2C%20MxIF%20oncology%20data%29%20show%0Athat%20the%20proposed%20framework%20provides%20higher%20prediction%20accuracy%20than%20baseline%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14974v2&entry.124074799=Read"},
{"title": "DDU-Net: A Domain Decomposition-Based CNN for High-Resolution Image\n  Segmentation on Multiple GPUs", "author": "Corn\u00e9 Verburg and Alexander Heinlein and Eric C. Cyr", "abstract": "  The segmentation of ultra-high resolution images poses challenges such as\nloss of spatial information or computational inefficiency. In this work, a\nnovel approach that combines encoder-decoder architectures with domain\ndecomposition strategies to address these challenges is proposed. Specifically,\na domain decomposition-based U-Net (DDU-Net) architecture is introduced, which\npartitions input images into non-overlapping patches that can be processed\nindependently on separate devices. A communication network is added to\nfacilitate inter-patch information exchange to enhance the understanding of\nspatial context. Experimental validation is performed on a synthetic dataset\nthat is designed to measure the effectiveness of the communication network.\nThen, the performance is tested on the DeepGlobe land cover classification\ndataset as a real-world benchmark data set. The results demonstrate that the\napproach, which includes inter-patch communication for images divided into\n$16\\times16$ non-overlapping subimages, achieves a $2-3\\,\\%$ higher\nintersection over union (IoU) score compared to the same network without\ninter-patch communication. The performance of the network which includes\ncommunication is equivalent to that of a baseline U-Net trained on the full\nimage, showing that our model provides an effective solution for segmenting\nultra-high-resolution images while preserving spatial context. The code is\navailable at https://github.com/corne00/DDU-Net.\n", "link": "http://arxiv.org/abs/2407.21266v3", "date": "2025-04-24", "relevancy": 2.2618, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5945}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5683}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DDU-Net%3A%20A%20Domain%20Decomposition-Based%20CNN%20for%20High-Resolution%20Image%0A%20%20Segmentation%20on%20Multiple%20GPUs&body=Title%3A%20DDU-Net%3A%20A%20Domain%20Decomposition-Based%20CNN%20for%20High-Resolution%20Image%0A%20%20Segmentation%20on%20Multiple%20GPUs%0AAuthor%3A%20Corn%C3%A9%20Verburg%20and%20Alexander%20Heinlein%20and%20Eric%20C.%20Cyr%0AAbstract%3A%20%20%20The%20segmentation%20of%20ultra-high%20resolution%20images%20poses%20challenges%20such%20as%0Aloss%20of%20spatial%20information%20or%20computational%20inefficiency.%20In%20this%20work%2C%20a%0Anovel%20approach%20that%20combines%20encoder-decoder%20architectures%20with%20domain%0Adecomposition%20strategies%20to%20address%20these%20challenges%20is%20proposed.%20Specifically%2C%0Aa%20domain%20decomposition-based%20U-Net%20%28DDU-Net%29%20architecture%20is%20introduced%2C%20which%0Apartitions%20input%20images%20into%20non-overlapping%20patches%20that%20can%20be%20processed%0Aindependently%20on%20separate%20devices.%20A%20communication%20network%20is%20added%20to%0Afacilitate%20inter-patch%20information%20exchange%20to%20enhance%20the%20understanding%20of%0Aspatial%20context.%20Experimental%20validation%20is%20performed%20on%20a%20synthetic%20dataset%0Athat%20is%20designed%20to%20measure%20the%20effectiveness%20of%20the%20communication%20network.%0AThen%2C%20the%20performance%20is%20tested%20on%20the%20DeepGlobe%20land%20cover%20classification%0Adataset%20as%20a%20real-world%20benchmark%20data%20set.%20The%20results%20demonstrate%20that%20the%0Aapproach%2C%20which%20includes%20inter-patch%20communication%20for%20images%20divided%20into%0A%2416%5Ctimes16%24%20non-overlapping%20subimages%2C%20achieves%20a%20%242-3%5C%2C%5C%25%24%20higher%0Aintersection%20over%20union%20%28IoU%29%20score%20compared%20to%20the%20same%20network%20without%0Ainter-patch%20communication.%20The%20performance%20of%20the%20network%20which%20includes%0Acommunication%20is%20equivalent%20to%20that%20of%20a%20baseline%20U-Net%20trained%20on%20the%20full%0Aimage%2C%20showing%20that%20our%20model%20provides%20an%20effective%20solution%20for%20segmenting%0Aultra-high-resolution%20images%20while%20preserving%20spatial%20context.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/corne00/DDU-Net.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21266v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDDU-Net%253A%2520A%2520Domain%2520Decomposition-Based%2520CNN%2520for%2520High-Resolution%2520Image%250A%2520%2520Segmentation%2520on%2520Multiple%2520GPUs%26entry.906535625%3DCorn%25C3%25A9%2520Verburg%2520and%2520Alexander%2520Heinlein%2520and%2520Eric%2520C.%2520Cyr%26entry.1292438233%3D%2520%2520The%2520segmentation%2520of%2520ultra-high%2520resolution%2520images%2520poses%2520challenges%2520such%2520as%250Aloss%2520of%2520spatial%2520information%2520or%2520computational%2520inefficiency.%2520In%2520this%2520work%252C%2520a%250Anovel%2520approach%2520that%2520combines%2520encoder-decoder%2520architectures%2520with%2520domain%250Adecomposition%2520strategies%2520to%2520address%2520these%2520challenges%2520is%2520proposed.%2520Specifically%252C%250Aa%2520domain%2520decomposition-based%2520U-Net%2520%2528DDU-Net%2529%2520architecture%2520is%2520introduced%252C%2520which%250Apartitions%2520input%2520images%2520into%2520non-overlapping%2520patches%2520that%2520can%2520be%2520processed%250Aindependently%2520on%2520separate%2520devices.%2520A%2520communication%2520network%2520is%2520added%2520to%250Afacilitate%2520inter-patch%2520information%2520exchange%2520to%2520enhance%2520the%2520understanding%2520of%250Aspatial%2520context.%2520Experimental%2520validation%2520is%2520performed%2520on%2520a%2520synthetic%2520dataset%250Athat%2520is%2520designed%2520to%2520measure%2520the%2520effectiveness%2520of%2520the%2520communication%2520network.%250AThen%252C%2520the%2520performance%2520is%2520tested%2520on%2520the%2520DeepGlobe%2520land%2520cover%2520classification%250Adataset%2520as%2520a%2520real-world%2520benchmark%2520data%2520set.%2520The%2520results%2520demonstrate%2520that%2520the%250Aapproach%252C%2520which%2520includes%2520inter-patch%2520communication%2520for%2520images%2520divided%2520into%250A%252416%255Ctimes16%2524%2520non-overlapping%2520subimages%252C%2520achieves%2520a%2520%25242-3%255C%252C%255C%2525%2524%2520higher%250Aintersection%2520over%2520union%2520%2528IoU%2529%2520score%2520compared%2520to%2520the%2520same%2520network%2520without%250Ainter-patch%2520communication.%2520The%2520performance%2520of%2520the%2520network%2520which%2520includes%250Acommunication%2520is%2520equivalent%2520to%2520that%2520of%2520a%2520baseline%2520U-Net%2520trained%2520on%2520the%2520full%250Aimage%252C%2520showing%2520that%2520our%2520model%2520provides%2520an%2520effective%2520solution%2520for%2520segmenting%250Aultra-high-resolution%2520images%2520while%2520preserving%2520spatial%2520context.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/corne00/DDU-Net.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21266v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DDU-Net%3A%20A%20Domain%20Decomposition-Based%20CNN%20for%20High-Resolution%20Image%0A%20%20Segmentation%20on%20Multiple%20GPUs&entry.906535625=Corn%C3%A9%20Verburg%20and%20Alexander%20Heinlein%20and%20Eric%20C.%20Cyr&entry.1292438233=%20%20The%20segmentation%20of%20ultra-high%20resolution%20images%20poses%20challenges%20such%20as%0Aloss%20of%20spatial%20information%20or%20computational%20inefficiency.%20In%20this%20work%2C%20a%0Anovel%20approach%20that%20combines%20encoder-decoder%20architectures%20with%20domain%0Adecomposition%20strategies%20to%20address%20these%20challenges%20is%20proposed.%20Specifically%2C%0Aa%20domain%20decomposition-based%20U-Net%20%28DDU-Net%29%20architecture%20is%20introduced%2C%20which%0Apartitions%20input%20images%20into%20non-overlapping%20patches%20that%20can%20be%20processed%0Aindependently%20on%20separate%20devices.%20A%20communication%20network%20is%20added%20to%0Afacilitate%20inter-patch%20information%20exchange%20to%20enhance%20the%20understanding%20of%0Aspatial%20context.%20Experimental%20validation%20is%20performed%20on%20a%20synthetic%20dataset%0Athat%20is%20designed%20to%20measure%20the%20effectiveness%20of%20the%20communication%20network.%0AThen%2C%20the%20performance%20is%20tested%20on%20the%20DeepGlobe%20land%20cover%20classification%0Adataset%20as%20a%20real-world%20benchmark%20data%20set.%20The%20results%20demonstrate%20that%20the%0Aapproach%2C%20which%20includes%20inter-patch%20communication%20for%20images%20divided%20into%0A%2416%5Ctimes16%24%20non-overlapping%20subimages%2C%20achieves%20a%20%242-3%5C%2C%5C%25%24%20higher%0Aintersection%20over%20union%20%28IoU%29%20score%20compared%20to%20the%20same%20network%20without%0Ainter-patch%20communication.%20The%20performance%20of%20the%20network%20which%20includes%0Acommunication%20is%20equivalent%20to%20that%20of%20a%20baseline%20U-Net%20trained%20on%20the%20full%0Aimage%2C%20showing%20that%20our%20model%20provides%20an%20effective%20solution%20for%20segmenting%0Aultra-high-resolution%20images%20while%20preserving%20spatial%20context.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/corne00/DDU-Net.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21266v3&entry.124074799=Read"},
{"title": "Tamper-evident Image using JPEG Fixed Points", "author": "Zhaofeng Si and Siwei Lyu", "abstract": "  An intriguing phenomenon about JPEG compression has been observed since two\ndecades ago- after repeating JPEG compression and decompression, it leads to a\nstable image that does not change anymore, which is a fixed point. In this\nwork, we prove the existence of fixed points in the essential JPEG procedures.\nWe analyze JPEG compression and decompression processes, revealing the\nexistence of fixed points that can be reached within a few iterations. These\nfixed points are diverse and preserve the image's visual quality, ensuring\nminimal distortion. This result is used to develop a method to create a\ntamper-evident image from the original authentic image, which can expose\ntampering operations by showing deviations from the fixed point image.\n", "link": "http://arxiv.org/abs/2504.17594v1", "date": "2025-04-24", "relevancy": 2.2463, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4658}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4458}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tamper-evident%20Image%20using%20JPEG%20Fixed%20Points&body=Title%3A%20Tamper-evident%20Image%20using%20JPEG%20Fixed%20Points%0AAuthor%3A%20Zhaofeng%20Si%20and%20Siwei%20Lyu%0AAbstract%3A%20%20%20An%20intriguing%20phenomenon%20about%20JPEG%20compression%20has%20been%20observed%20since%20two%0Adecades%20ago-%20after%20repeating%20JPEG%20compression%20and%20decompression%2C%20it%20leads%20to%20a%0Astable%20image%20that%20does%20not%20change%20anymore%2C%20which%20is%20a%20fixed%20point.%20In%20this%0Awork%2C%20we%20prove%20the%20existence%20of%20fixed%20points%20in%20the%20essential%20JPEG%20procedures.%0AWe%20analyze%20JPEG%20compression%20and%20decompression%20processes%2C%20revealing%20the%0Aexistence%20of%20fixed%20points%20that%20can%20be%20reached%20within%20a%20few%20iterations.%20These%0Afixed%20points%20are%20diverse%20and%20preserve%20the%20image%27s%20visual%20quality%2C%20ensuring%0Aminimal%20distortion.%20This%20result%20is%20used%20to%20develop%20a%20method%20to%20create%20a%0Atamper-evident%20image%20from%20the%20original%20authentic%20image%2C%20which%20can%20expose%0Atampering%20operations%20by%20showing%20deviations%20from%20the%20fixed%20point%20image.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17594v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTamper-evident%2520Image%2520using%2520JPEG%2520Fixed%2520Points%26entry.906535625%3DZhaofeng%2520Si%2520and%2520Siwei%2520Lyu%26entry.1292438233%3D%2520%2520An%2520intriguing%2520phenomenon%2520about%2520JPEG%2520compression%2520has%2520been%2520observed%2520since%2520two%250Adecades%2520ago-%2520after%2520repeating%2520JPEG%2520compression%2520and%2520decompression%252C%2520it%2520leads%2520to%2520a%250Astable%2520image%2520that%2520does%2520not%2520change%2520anymore%252C%2520which%2520is%2520a%2520fixed%2520point.%2520In%2520this%250Awork%252C%2520we%2520prove%2520the%2520existence%2520of%2520fixed%2520points%2520in%2520the%2520essential%2520JPEG%2520procedures.%250AWe%2520analyze%2520JPEG%2520compression%2520and%2520decompression%2520processes%252C%2520revealing%2520the%250Aexistence%2520of%2520fixed%2520points%2520that%2520can%2520be%2520reached%2520within%2520a%2520few%2520iterations.%2520These%250Afixed%2520points%2520are%2520diverse%2520and%2520preserve%2520the%2520image%2527s%2520visual%2520quality%252C%2520ensuring%250Aminimal%2520distortion.%2520This%2520result%2520is%2520used%2520to%2520develop%2520a%2520method%2520to%2520create%2520a%250Atamper-evident%2520image%2520from%2520the%2520original%2520authentic%2520image%252C%2520which%2520can%2520expose%250Atampering%2520operations%2520by%2520showing%2520deviations%2520from%2520the%2520fixed%2520point%2520image.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17594v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tamper-evident%20Image%20using%20JPEG%20Fixed%20Points&entry.906535625=Zhaofeng%20Si%20and%20Siwei%20Lyu&entry.1292438233=%20%20An%20intriguing%20phenomenon%20about%20JPEG%20compression%20has%20been%20observed%20since%20two%0Adecades%20ago-%20after%20repeating%20JPEG%20compression%20and%20decompression%2C%20it%20leads%20to%20a%0Astable%20image%20that%20does%20not%20change%20anymore%2C%20which%20is%20a%20fixed%20point.%20In%20this%0Awork%2C%20we%20prove%20the%20existence%20of%20fixed%20points%20in%20the%20essential%20JPEG%20procedures.%0AWe%20analyze%20JPEG%20compression%20and%20decompression%20processes%2C%20revealing%20the%0Aexistence%20of%20fixed%20points%20that%20can%20be%20reached%20within%20a%20few%20iterations.%20These%0Afixed%20points%20are%20diverse%20and%20preserve%20the%20image%27s%20visual%20quality%2C%20ensuring%0Aminimal%20distortion.%20This%20result%20is%20used%20to%20develop%20a%20method%20to%20create%20a%0Atamper-evident%20image%20from%20the%20original%20authentic%20image%2C%20which%20can%20expose%0Atampering%20operations%20by%20showing%20deviations%20from%20the%20fixed%20point%20image.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17594v1&entry.124074799=Read"},
{"title": "Learning Isometric Embeddings of Road Networks using Multidimensional\n  Scaling", "author": "Juan Carlos Climent Pardo", "abstract": "  The lack of generalization in learning-based autonomous driving applications\nis shown by the narrow range of road scenarios that vehicles can currently\ncover. A generalizable approach should capture many distinct road structures\nand topologies, as well as consider traffic participants, and dynamic changes\nin the environment, so that vehicles can navigate and perform motion planning\ntasks even in the most difficult situations. Designing suitable feature spaces\nfor neural network-based motion planers that encapsulate all kinds of road\nscenarios is still an open research challenge. This paper tackles this\nlearning-based generalization challenge and shows how graph representations of\nroad networks can be leveraged by using multidimensional scaling (MDS)\ntechniques in order to obtain such feature spaces. State-of-the-art graph\nrepresentations and MDS approaches are analyzed for the autonomous driving use\ncase. Finally, the option of embedding graph nodes is discussed in order to\nperform easier learning procedures and obtain dimensionality reduction.\n", "link": "http://arxiv.org/abs/2504.17534v1", "date": "2025-04-24", "relevancy": 2.2091, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5926}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5289}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Isometric%20Embeddings%20of%20Road%20Networks%20using%20Multidimensional%0A%20%20Scaling&body=Title%3A%20Learning%20Isometric%20Embeddings%20of%20Road%20Networks%20using%20Multidimensional%0A%20%20Scaling%0AAuthor%3A%20Juan%20Carlos%20Climent%20Pardo%0AAbstract%3A%20%20%20The%20lack%20of%20generalization%20in%20learning-based%20autonomous%20driving%20applications%0Ais%20shown%20by%20the%20narrow%20range%20of%20road%20scenarios%20that%20vehicles%20can%20currently%0Acover.%20A%20generalizable%20approach%20should%20capture%20many%20distinct%20road%20structures%0Aand%20topologies%2C%20as%20well%20as%20consider%20traffic%20participants%2C%20and%20dynamic%20changes%0Ain%20the%20environment%2C%20so%20that%20vehicles%20can%20navigate%20and%20perform%20motion%20planning%0Atasks%20even%20in%20the%20most%20difficult%20situations.%20Designing%20suitable%20feature%20spaces%0Afor%20neural%20network-based%20motion%20planers%20that%20encapsulate%20all%20kinds%20of%20road%0Ascenarios%20is%20still%20an%20open%20research%20challenge.%20This%20paper%20tackles%20this%0Alearning-based%20generalization%20challenge%20and%20shows%20how%20graph%20representations%20of%0Aroad%20networks%20can%20be%20leveraged%20by%20using%20multidimensional%20scaling%20%28MDS%29%0Atechniques%20in%20order%20to%20obtain%20such%20feature%20spaces.%20State-of-the-art%20graph%0Arepresentations%20and%20MDS%20approaches%20are%20analyzed%20for%20the%20autonomous%20driving%20use%0Acase.%20Finally%2C%20the%20option%20of%20embedding%20graph%20nodes%20is%20discussed%20in%20order%20to%0Aperform%20easier%20learning%20procedures%20and%20obtain%20dimensionality%20reduction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17534v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Isometric%2520Embeddings%2520of%2520Road%2520Networks%2520using%2520Multidimensional%250A%2520%2520Scaling%26entry.906535625%3DJuan%2520Carlos%2520Climent%2520Pardo%26entry.1292438233%3D%2520%2520The%2520lack%2520of%2520generalization%2520in%2520learning-based%2520autonomous%2520driving%2520applications%250Ais%2520shown%2520by%2520the%2520narrow%2520range%2520of%2520road%2520scenarios%2520that%2520vehicles%2520can%2520currently%250Acover.%2520A%2520generalizable%2520approach%2520should%2520capture%2520many%2520distinct%2520road%2520structures%250Aand%2520topologies%252C%2520as%2520well%2520as%2520consider%2520traffic%2520participants%252C%2520and%2520dynamic%2520changes%250Ain%2520the%2520environment%252C%2520so%2520that%2520vehicles%2520can%2520navigate%2520and%2520perform%2520motion%2520planning%250Atasks%2520even%2520in%2520the%2520most%2520difficult%2520situations.%2520Designing%2520suitable%2520feature%2520spaces%250Afor%2520neural%2520network-based%2520motion%2520planers%2520that%2520encapsulate%2520all%2520kinds%2520of%2520road%250Ascenarios%2520is%2520still%2520an%2520open%2520research%2520challenge.%2520This%2520paper%2520tackles%2520this%250Alearning-based%2520generalization%2520challenge%2520and%2520shows%2520how%2520graph%2520representations%2520of%250Aroad%2520networks%2520can%2520be%2520leveraged%2520by%2520using%2520multidimensional%2520scaling%2520%2528MDS%2529%250Atechniques%2520in%2520order%2520to%2520obtain%2520such%2520feature%2520spaces.%2520State-of-the-art%2520graph%250Arepresentations%2520and%2520MDS%2520approaches%2520are%2520analyzed%2520for%2520the%2520autonomous%2520driving%2520use%250Acase.%2520Finally%252C%2520the%2520option%2520of%2520embedding%2520graph%2520nodes%2520is%2520discussed%2520in%2520order%2520to%250Aperform%2520easier%2520learning%2520procedures%2520and%2520obtain%2520dimensionality%2520reduction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17534v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Isometric%20Embeddings%20of%20Road%20Networks%20using%20Multidimensional%0A%20%20Scaling&entry.906535625=Juan%20Carlos%20Climent%20Pardo&entry.1292438233=%20%20The%20lack%20of%20generalization%20in%20learning-based%20autonomous%20driving%20applications%0Ais%20shown%20by%20the%20narrow%20range%20of%20road%20scenarios%20that%20vehicles%20can%20currently%0Acover.%20A%20generalizable%20approach%20should%20capture%20many%20distinct%20road%20structures%0Aand%20topologies%2C%20as%20well%20as%20consider%20traffic%20participants%2C%20and%20dynamic%20changes%0Ain%20the%20environment%2C%20so%20that%20vehicles%20can%20navigate%20and%20perform%20motion%20planning%0Atasks%20even%20in%20the%20most%20difficult%20situations.%20Designing%20suitable%20feature%20spaces%0Afor%20neural%20network-based%20motion%20planers%20that%20encapsulate%20all%20kinds%20of%20road%0Ascenarios%20is%20still%20an%20open%20research%20challenge.%20This%20paper%20tackles%20this%0Alearning-based%20generalization%20challenge%20and%20shows%20how%20graph%20representations%20of%0Aroad%20networks%20can%20be%20leveraged%20by%20using%20multidimensional%20scaling%20%28MDS%29%0Atechniques%20in%20order%20to%20obtain%20such%20feature%20spaces.%20State-of-the-art%20graph%0Arepresentations%20and%20MDS%20approaches%20are%20analyzed%20for%20the%20autonomous%20driving%20use%0Acase.%20Finally%2C%20the%20option%20of%20embedding%20graph%20nodes%20is%20discussed%20in%20order%20to%0Aperform%20easier%20learning%20procedures%20and%20obtain%20dimensionality%20reduction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17534v1&entry.124074799=Read"},
{"title": "Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation\n  with Multimodal Generative Pretraining", "author": "Dongyang Liu and Shitian Zhao and Le Zhuo and Weifeng Lin and Yi Xin and Xinyue Li and Qi Qin and Yu Qiao and Hongsheng Li and Peng Gao", "abstract": "  We present Lumina-mGPT, a family of multimodal autoregressive models capable\nof various vision and language tasks, particularly excelling in generating\nflexible photorealistic images from text descriptions. By initializing from\nmultimodal Generative PreTraining (mGPT), we demonstrate that decoder-only\nAutoregressive (AR) model can achieve image generation performance comparable\nto modern diffusion models with high efficiency through Flexible Progressive\nSupervised Fine-tuning (FP-SFT). Equipped with our proposed Unambiguous image\nRepresentation (UniRep), Lumina-mGPT can flexibly generate high-quality images\nof varying aspect ratios. Building on the strong image generation capabilities,\nwe further explore Ominiponent Supervised Fine-tuning (Omni-SFT), an initial\nattempt to elevate Lumina-mGPT into a unified multi-modal generalist. The\nresulting model demonstrates versatile multimodal capabilities, including\nvisual generation tasks like text-to-image/multiview generation and\ncontrollable generation, visual recognition tasks like segmentation and depth\nestimation, and vision-language tasks like multi-turn visual question\nanswering, showing the rosy potential of the technical direction. Codes and\ncheckpoints are available at https://github.com/Alpha-VLLM/Lumina-mGPT.\n", "link": "http://arxiv.org/abs/2408.02657v3", "date": "2025-04-24", "relevancy": 2.2071, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5535}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5514}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lumina-mGPT%3A%20Illuminate%20Flexible%20Photorealistic%20Text-to-Image%20Generation%0A%20%20with%20Multimodal%20Generative%20Pretraining&body=Title%3A%20Lumina-mGPT%3A%20Illuminate%20Flexible%20Photorealistic%20Text-to-Image%20Generation%0A%20%20with%20Multimodal%20Generative%20Pretraining%0AAuthor%3A%20Dongyang%20Liu%20and%20Shitian%20Zhao%20and%20Le%20Zhuo%20and%20Weifeng%20Lin%20and%20Yi%20Xin%20and%20Xinyue%20Li%20and%20Qi%20Qin%20and%20Yu%20Qiao%20and%20Hongsheng%20Li%20and%20Peng%20Gao%0AAbstract%3A%20%20%20We%20present%20Lumina-mGPT%2C%20a%20family%20of%20multimodal%20autoregressive%20models%20capable%0Aof%20various%20vision%20and%20language%20tasks%2C%20particularly%20excelling%20in%20generating%0Aflexible%20photorealistic%20images%20from%20text%20descriptions.%20By%20initializing%20from%0Amultimodal%20Generative%20PreTraining%20%28mGPT%29%2C%20we%20demonstrate%20that%20decoder-only%0AAutoregressive%20%28AR%29%20model%20can%20achieve%20image%20generation%20performance%20comparable%0Ato%20modern%20diffusion%20models%20with%20high%20efficiency%20through%20Flexible%20Progressive%0ASupervised%20Fine-tuning%20%28FP-SFT%29.%20Equipped%20with%20our%20proposed%20Unambiguous%20image%0ARepresentation%20%28UniRep%29%2C%20Lumina-mGPT%20can%20flexibly%20generate%20high-quality%20images%0Aof%20varying%20aspect%20ratios.%20Building%20on%20the%20strong%20image%20generation%20capabilities%2C%0Awe%20further%20explore%20Ominiponent%20Supervised%20Fine-tuning%20%28Omni-SFT%29%2C%20an%20initial%0Aattempt%20to%20elevate%20Lumina-mGPT%20into%20a%20unified%20multi-modal%20generalist.%20The%0Aresulting%20model%20demonstrates%20versatile%20multimodal%20capabilities%2C%20including%0Avisual%20generation%20tasks%20like%20text-to-image/multiview%20generation%20and%0Acontrollable%20generation%2C%20visual%20recognition%20tasks%20like%20segmentation%20and%20depth%0Aestimation%2C%20and%20vision-language%20tasks%20like%20multi-turn%20visual%20question%0Aanswering%2C%20showing%20the%20rosy%20potential%20of%20the%20technical%20direction.%20Codes%20and%0Acheckpoints%20are%20available%20at%20https%3A//github.com/Alpha-VLLM/Lumina-mGPT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02657v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLumina-mGPT%253A%2520Illuminate%2520Flexible%2520Photorealistic%2520Text-to-Image%2520Generation%250A%2520%2520with%2520Multimodal%2520Generative%2520Pretraining%26entry.906535625%3DDongyang%2520Liu%2520and%2520Shitian%2520Zhao%2520and%2520Le%2520Zhuo%2520and%2520Weifeng%2520Lin%2520and%2520Yi%2520Xin%2520and%2520Xinyue%2520Li%2520and%2520Qi%2520Qin%2520and%2520Yu%2520Qiao%2520and%2520Hongsheng%2520Li%2520and%2520Peng%2520Gao%26entry.1292438233%3D%2520%2520We%2520present%2520Lumina-mGPT%252C%2520a%2520family%2520of%2520multimodal%2520autoregressive%2520models%2520capable%250Aof%2520various%2520vision%2520and%2520language%2520tasks%252C%2520particularly%2520excelling%2520in%2520generating%250Aflexible%2520photorealistic%2520images%2520from%2520text%2520descriptions.%2520By%2520initializing%2520from%250Amultimodal%2520Generative%2520PreTraining%2520%2528mGPT%2529%252C%2520we%2520demonstrate%2520that%2520decoder-only%250AAutoregressive%2520%2528AR%2529%2520model%2520can%2520achieve%2520image%2520generation%2520performance%2520comparable%250Ato%2520modern%2520diffusion%2520models%2520with%2520high%2520efficiency%2520through%2520Flexible%2520Progressive%250ASupervised%2520Fine-tuning%2520%2528FP-SFT%2529.%2520Equipped%2520with%2520our%2520proposed%2520Unambiguous%2520image%250ARepresentation%2520%2528UniRep%2529%252C%2520Lumina-mGPT%2520can%2520flexibly%2520generate%2520high-quality%2520images%250Aof%2520varying%2520aspect%2520ratios.%2520Building%2520on%2520the%2520strong%2520image%2520generation%2520capabilities%252C%250Awe%2520further%2520explore%2520Ominiponent%2520Supervised%2520Fine-tuning%2520%2528Omni-SFT%2529%252C%2520an%2520initial%250Aattempt%2520to%2520elevate%2520Lumina-mGPT%2520into%2520a%2520unified%2520multi-modal%2520generalist.%2520The%250Aresulting%2520model%2520demonstrates%2520versatile%2520multimodal%2520capabilities%252C%2520including%250Avisual%2520generation%2520tasks%2520like%2520text-to-image/multiview%2520generation%2520and%250Acontrollable%2520generation%252C%2520visual%2520recognition%2520tasks%2520like%2520segmentation%2520and%2520depth%250Aestimation%252C%2520and%2520vision-language%2520tasks%2520like%2520multi-turn%2520visual%2520question%250Aanswering%252C%2520showing%2520the%2520rosy%2520potential%2520of%2520the%2520technical%2520direction.%2520Codes%2520and%250Acheckpoints%2520are%2520available%2520at%2520https%253A//github.com/Alpha-VLLM/Lumina-mGPT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02657v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lumina-mGPT%3A%20Illuminate%20Flexible%20Photorealistic%20Text-to-Image%20Generation%0A%20%20with%20Multimodal%20Generative%20Pretraining&entry.906535625=Dongyang%20Liu%20and%20Shitian%20Zhao%20and%20Le%20Zhuo%20and%20Weifeng%20Lin%20and%20Yi%20Xin%20and%20Xinyue%20Li%20and%20Qi%20Qin%20and%20Yu%20Qiao%20and%20Hongsheng%20Li%20and%20Peng%20Gao&entry.1292438233=%20%20We%20present%20Lumina-mGPT%2C%20a%20family%20of%20multimodal%20autoregressive%20models%20capable%0Aof%20various%20vision%20and%20language%20tasks%2C%20particularly%20excelling%20in%20generating%0Aflexible%20photorealistic%20images%20from%20text%20descriptions.%20By%20initializing%20from%0Amultimodal%20Generative%20PreTraining%20%28mGPT%29%2C%20we%20demonstrate%20that%20decoder-only%0AAutoregressive%20%28AR%29%20model%20can%20achieve%20image%20generation%20performance%20comparable%0Ato%20modern%20diffusion%20models%20with%20high%20efficiency%20through%20Flexible%20Progressive%0ASupervised%20Fine-tuning%20%28FP-SFT%29.%20Equipped%20with%20our%20proposed%20Unambiguous%20image%0ARepresentation%20%28UniRep%29%2C%20Lumina-mGPT%20can%20flexibly%20generate%20high-quality%20images%0Aof%20varying%20aspect%20ratios.%20Building%20on%20the%20strong%20image%20generation%20capabilities%2C%0Awe%20further%20explore%20Ominiponent%20Supervised%20Fine-tuning%20%28Omni-SFT%29%2C%20an%20initial%0Aattempt%20to%20elevate%20Lumina-mGPT%20into%20a%20unified%20multi-modal%20generalist.%20The%0Aresulting%20model%20demonstrates%20versatile%20multimodal%20capabilities%2C%20including%0Avisual%20generation%20tasks%20like%20text-to-image/multiview%20generation%20and%0Acontrollable%20generation%2C%20visual%20recognition%20tasks%20like%20segmentation%20and%20depth%0Aestimation%2C%20and%20vision-language%20tasks%20like%20multi-turn%20visual%20question%0Aanswering%2C%20showing%20the%20rosy%20potential%20of%20the%20technical%20direction.%20Codes%20and%0Acheckpoints%20are%20available%20at%20https%3A//github.com/Alpha-VLLM/Lumina-mGPT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02657v3&entry.124074799=Read"},
{"title": "Integrating Learning-Based Manipulation and Physics-Based Locomotion for\n  Whole-Body Badminton Robot Control", "author": "Haochen Wang and Zhiwei Shi and Chengxi Zhu and Yafei Qiao and Cheng Zhang and Fan Yang and Pengjie Ren and Lan Lu and Dong Xuan", "abstract": "  Learning-based methods, such as imitation learning (IL) and reinforcement\nlearning (RL), can produce excel control policies over challenging agile robot\ntasks, such as sports robot. However, no existing work has harmonized\nlearning-based policy with model-based methods to reduce training complexity\nand ensure the safety and stability for agile badminton robot control. In this\npaper, we introduce \\ourmethod, a novel hybrid control system for agile\nbadminton robots. Specifically, we propose a model-based strategy for chassis\nlocomotion which provides a base for arm policy. We introduce a\nphysics-informed ``IL+RL'' training framework for learning-based arm policy. In\nthis train framework, a model-based strategy with privileged information is\nused to guide arm policy training during both IL and RL phases. In addition, we\ntrain the critic model during IL phase to alleviate the performance drop issue\nwhen transitioning from IL to RL. We present results on our self-engineered\nbadminton robot, achieving 94.5% success rate against the serving machine and\n90.7% success rate against human players. Our system can be easily generalized\nto other agile mobile manipulation tasks such as agile catching and table\ntennis. Our project website: https://dreamstarring.github.io/HAMLET/.\n", "link": "http://arxiv.org/abs/2504.17771v1", "date": "2025-04-24", "relevancy": 2.2006, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.582}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5446}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20Learning-Based%20Manipulation%20and%20Physics-Based%20Locomotion%20for%0A%20%20Whole-Body%20Badminton%20Robot%20Control&body=Title%3A%20Integrating%20Learning-Based%20Manipulation%20and%20Physics-Based%20Locomotion%20for%0A%20%20Whole-Body%20Badminton%20Robot%20Control%0AAuthor%3A%20Haochen%20Wang%20and%20Zhiwei%20Shi%20and%20Chengxi%20Zhu%20and%20Yafei%20Qiao%20and%20Cheng%20Zhang%20and%20Fan%20Yang%20and%20Pengjie%20Ren%20and%20Lan%20Lu%20and%20Dong%20Xuan%0AAbstract%3A%20%20%20Learning-based%20methods%2C%20such%20as%20imitation%20learning%20%28IL%29%20and%20reinforcement%0Alearning%20%28RL%29%2C%20can%20produce%20excel%20control%20policies%20over%20challenging%20agile%20robot%0Atasks%2C%20such%20as%20sports%20robot.%20However%2C%20no%20existing%20work%20has%20harmonized%0Alearning-based%20policy%20with%20model-based%20methods%20to%20reduce%20training%20complexity%0Aand%20ensure%20the%20safety%20and%20stability%20for%20agile%20badminton%20robot%20control.%20In%20this%0Apaper%2C%20we%20introduce%20%5Courmethod%2C%20a%20novel%20hybrid%20control%20system%20for%20agile%0Abadminton%20robots.%20Specifically%2C%20we%20propose%20a%20model-based%20strategy%20for%20chassis%0Alocomotion%20which%20provides%20a%20base%20for%20arm%20policy.%20We%20introduce%20a%0Aphysics-informed%20%60%60IL%2BRL%27%27%20training%20framework%20for%20learning-based%20arm%20policy.%20In%0Athis%20train%20framework%2C%20a%20model-based%20strategy%20with%20privileged%20information%20is%0Aused%20to%20guide%20arm%20policy%20training%20during%20both%20IL%20and%20RL%20phases.%20In%20addition%2C%20we%0Atrain%20the%20critic%20model%20during%20IL%20phase%20to%20alleviate%20the%20performance%20drop%20issue%0Awhen%20transitioning%20from%20IL%20to%20RL.%20We%20present%20results%20on%20our%20self-engineered%0Abadminton%20robot%2C%20achieving%2094.5%25%20success%20rate%20against%20the%20serving%20machine%20and%0A90.7%25%20success%20rate%20against%20human%20players.%20Our%20system%20can%20be%20easily%20generalized%0Ato%20other%20agile%20mobile%20manipulation%20tasks%20such%20as%20agile%20catching%20and%20table%0Atennis.%20Our%20project%20website%3A%20https%3A//dreamstarring.github.io/HAMLET/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17771v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520Learning-Based%2520Manipulation%2520and%2520Physics-Based%2520Locomotion%2520for%250A%2520%2520Whole-Body%2520Badminton%2520Robot%2520Control%26entry.906535625%3DHaochen%2520Wang%2520and%2520Zhiwei%2520Shi%2520and%2520Chengxi%2520Zhu%2520and%2520Yafei%2520Qiao%2520and%2520Cheng%2520Zhang%2520and%2520Fan%2520Yang%2520and%2520Pengjie%2520Ren%2520and%2520Lan%2520Lu%2520and%2520Dong%2520Xuan%26entry.1292438233%3D%2520%2520Learning-based%2520methods%252C%2520such%2520as%2520imitation%2520learning%2520%2528IL%2529%2520and%2520reinforcement%250Alearning%2520%2528RL%2529%252C%2520can%2520produce%2520excel%2520control%2520policies%2520over%2520challenging%2520agile%2520robot%250Atasks%252C%2520such%2520as%2520sports%2520robot.%2520However%252C%2520no%2520existing%2520work%2520has%2520harmonized%250Alearning-based%2520policy%2520with%2520model-based%2520methods%2520to%2520reduce%2520training%2520complexity%250Aand%2520ensure%2520the%2520safety%2520and%2520stability%2520for%2520agile%2520badminton%2520robot%2520control.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520%255Courmethod%252C%2520a%2520novel%2520hybrid%2520control%2520system%2520for%2520agile%250Abadminton%2520robots.%2520Specifically%252C%2520we%2520propose%2520a%2520model-based%2520strategy%2520for%2520chassis%250Alocomotion%2520which%2520provides%2520a%2520base%2520for%2520arm%2520policy.%2520We%2520introduce%2520a%250Aphysics-informed%2520%2560%2560IL%252BRL%2527%2527%2520training%2520framework%2520for%2520learning-based%2520arm%2520policy.%2520In%250Athis%2520train%2520framework%252C%2520a%2520model-based%2520strategy%2520with%2520privileged%2520information%2520is%250Aused%2520to%2520guide%2520arm%2520policy%2520training%2520during%2520both%2520IL%2520and%2520RL%2520phases.%2520In%2520addition%252C%2520we%250Atrain%2520the%2520critic%2520model%2520during%2520IL%2520phase%2520to%2520alleviate%2520the%2520performance%2520drop%2520issue%250Awhen%2520transitioning%2520from%2520IL%2520to%2520RL.%2520We%2520present%2520results%2520on%2520our%2520self-engineered%250Abadminton%2520robot%252C%2520achieving%252094.5%2525%2520success%2520rate%2520against%2520the%2520serving%2520machine%2520and%250A90.7%2525%2520success%2520rate%2520against%2520human%2520players.%2520Our%2520system%2520can%2520be%2520easily%2520generalized%250Ato%2520other%2520agile%2520mobile%2520manipulation%2520tasks%2520such%2520as%2520agile%2520catching%2520and%2520table%250Atennis.%2520Our%2520project%2520website%253A%2520https%253A//dreamstarring.github.io/HAMLET/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17771v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Learning-Based%20Manipulation%20and%20Physics-Based%20Locomotion%20for%0A%20%20Whole-Body%20Badminton%20Robot%20Control&entry.906535625=Haochen%20Wang%20and%20Zhiwei%20Shi%20and%20Chengxi%20Zhu%20and%20Yafei%20Qiao%20and%20Cheng%20Zhang%20and%20Fan%20Yang%20and%20Pengjie%20Ren%20and%20Lan%20Lu%20and%20Dong%20Xuan&entry.1292438233=%20%20Learning-based%20methods%2C%20such%20as%20imitation%20learning%20%28IL%29%20and%20reinforcement%0Alearning%20%28RL%29%2C%20can%20produce%20excel%20control%20policies%20over%20challenging%20agile%20robot%0Atasks%2C%20such%20as%20sports%20robot.%20However%2C%20no%20existing%20work%20has%20harmonized%0Alearning-based%20policy%20with%20model-based%20methods%20to%20reduce%20training%20complexity%0Aand%20ensure%20the%20safety%20and%20stability%20for%20agile%20badminton%20robot%20control.%20In%20this%0Apaper%2C%20we%20introduce%20%5Courmethod%2C%20a%20novel%20hybrid%20control%20system%20for%20agile%0Abadminton%20robots.%20Specifically%2C%20we%20propose%20a%20model-based%20strategy%20for%20chassis%0Alocomotion%20which%20provides%20a%20base%20for%20arm%20policy.%20We%20introduce%20a%0Aphysics-informed%20%60%60IL%2BRL%27%27%20training%20framework%20for%20learning-based%20arm%20policy.%20In%0Athis%20train%20framework%2C%20a%20model-based%20strategy%20with%20privileged%20information%20is%0Aused%20to%20guide%20arm%20policy%20training%20during%20both%20IL%20and%20RL%20phases.%20In%20addition%2C%20we%0Atrain%20the%20critic%20model%20during%20IL%20phase%20to%20alleviate%20the%20performance%20drop%20issue%0Awhen%20transitioning%20from%20IL%20to%20RL.%20We%20present%20results%20on%20our%20self-engineered%0Abadminton%20robot%2C%20achieving%2094.5%25%20success%20rate%20against%20the%20serving%20machine%20and%0A90.7%25%20success%20rate%20against%20human%20players.%20Our%20system%20can%20be%20easily%20generalized%0Ato%20other%20agile%20mobile%20manipulation%20tasks%20such%20as%20agile%20catching%20and%20table%0Atennis.%20Our%20project%20website%3A%20https%3A//dreamstarring.github.io/HAMLET/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17771v1&entry.124074799=Read"},
{"title": "FRAG: Frame Selection Augmented Generation for Long Video and Long\n  Document Understanding", "author": "De-An Huang and Subhashree Radhakrishnan and Zhiding Yu and Jan Kautz", "abstract": "  There has been impressive progress in Large Multimodal Models (LMMs). Recent\nworks extend these models to long inputs, including multi-page documents and\nlong videos. However, the model size and performance of these long context\nmodels are still limited due to the computational cost in both training and\ninference. In this work, we explore an orthogonal direction and process long\ninputs without long context LMMs. We propose Frame Selection Augmented\nGeneration (FRAG), where the model first selects relevant frames within the\ninput, and then only generates the final outputs based on the selected frames.\nThe core of the selection process is done by scoring each frame independently,\nwhich does not require long context processing. The frames with the highest\nscores are then selected by a simple Top-K selection. We show that this\nfrustratingly simple framework is applicable to both long videos and multi-page\ndocuments using existing LMMs without any fine-tuning. We consider two models,\nLLaVA-OneVision and InternVL2, in our experiments and show that FRAG\nconsistently improves the performance and achieves state-of-the-art\nperformances for both long video and long document understanding. For videos,\nFRAG substantially improves InternVL2-76B by 5.8% on MLVU and 3.7% on\nVideo-MME. For documents, FRAG achieves over 20% improvements on MP-DocVQA\ncompared with recent LMMs specialized in long document understanding. Code is\navailable at: https://github.com/NVlabs/FRAG\n", "link": "http://arxiv.org/abs/2504.17447v1", "date": "2025-04-24", "relevancy": 2.1964, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5603}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5468}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FRAG%3A%20Frame%20Selection%20Augmented%20Generation%20for%20Long%20Video%20and%20Long%0A%20%20Document%20Understanding&body=Title%3A%20FRAG%3A%20Frame%20Selection%20Augmented%20Generation%20for%20Long%20Video%20and%20Long%0A%20%20Document%20Understanding%0AAuthor%3A%20De-An%20Huang%20and%20Subhashree%20Radhakrishnan%20and%20Zhiding%20Yu%20and%20Jan%20Kautz%0AAbstract%3A%20%20%20There%20has%20been%20impressive%20progress%20in%20Large%20Multimodal%20Models%20%28LMMs%29.%20Recent%0Aworks%20extend%20these%20models%20to%20long%20inputs%2C%20including%20multi-page%20documents%20and%0Along%20videos.%20However%2C%20the%20model%20size%20and%20performance%20of%20these%20long%20context%0Amodels%20are%20still%20limited%20due%20to%20the%20computational%20cost%20in%20both%20training%20and%0Ainference.%20In%20this%20work%2C%20we%20explore%20an%20orthogonal%20direction%20and%20process%20long%0Ainputs%20without%20long%20context%20LMMs.%20We%20propose%20Frame%20Selection%20Augmented%0AGeneration%20%28FRAG%29%2C%20where%20the%20model%20first%20selects%20relevant%20frames%20within%20the%0Ainput%2C%20and%20then%20only%20generates%20the%20final%20outputs%20based%20on%20the%20selected%20frames.%0AThe%20core%20of%20the%20selection%20process%20is%20done%20by%20scoring%20each%20frame%20independently%2C%0Awhich%20does%20not%20require%20long%20context%20processing.%20The%20frames%20with%20the%20highest%0Ascores%20are%20then%20selected%20by%20a%20simple%20Top-K%20selection.%20We%20show%20that%20this%0Afrustratingly%20simple%20framework%20is%20applicable%20to%20both%20long%20videos%20and%20multi-page%0Adocuments%20using%20existing%20LMMs%20without%20any%20fine-tuning.%20We%20consider%20two%20models%2C%0ALLaVA-OneVision%20and%20InternVL2%2C%20in%20our%20experiments%20and%20show%20that%20FRAG%0Aconsistently%20improves%20the%20performance%20and%20achieves%20state-of-the-art%0Aperformances%20for%20both%20long%20video%20and%20long%20document%20understanding.%20For%20videos%2C%0AFRAG%20substantially%20improves%20InternVL2-76B%20by%205.8%25%20on%20MLVU%20and%203.7%25%20on%0AVideo-MME.%20For%20documents%2C%20FRAG%20achieves%20over%2020%25%20improvements%20on%20MP-DocVQA%0Acompared%20with%20recent%20LMMs%20specialized%20in%20long%20document%20understanding.%20Code%20is%0Aavailable%20at%3A%20https%3A//github.com/NVlabs/FRAG%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17447v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFRAG%253A%2520Frame%2520Selection%2520Augmented%2520Generation%2520for%2520Long%2520Video%2520and%2520Long%250A%2520%2520Document%2520Understanding%26entry.906535625%3DDe-An%2520Huang%2520and%2520Subhashree%2520Radhakrishnan%2520and%2520Zhiding%2520Yu%2520and%2520Jan%2520Kautz%26entry.1292438233%3D%2520%2520There%2520has%2520been%2520impressive%2520progress%2520in%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529.%2520Recent%250Aworks%2520extend%2520these%2520models%2520to%2520long%2520inputs%252C%2520including%2520multi-page%2520documents%2520and%250Along%2520videos.%2520However%252C%2520the%2520model%2520size%2520and%2520performance%2520of%2520these%2520long%2520context%250Amodels%2520are%2520still%2520limited%2520due%2520to%2520the%2520computational%2520cost%2520in%2520both%2520training%2520and%250Ainference.%2520In%2520this%2520work%252C%2520we%2520explore%2520an%2520orthogonal%2520direction%2520and%2520process%2520long%250Ainputs%2520without%2520long%2520context%2520LMMs.%2520We%2520propose%2520Frame%2520Selection%2520Augmented%250AGeneration%2520%2528FRAG%2529%252C%2520where%2520the%2520model%2520first%2520selects%2520relevant%2520frames%2520within%2520the%250Ainput%252C%2520and%2520then%2520only%2520generates%2520the%2520final%2520outputs%2520based%2520on%2520the%2520selected%2520frames.%250AThe%2520core%2520of%2520the%2520selection%2520process%2520is%2520done%2520by%2520scoring%2520each%2520frame%2520independently%252C%250Awhich%2520does%2520not%2520require%2520long%2520context%2520processing.%2520The%2520frames%2520with%2520the%2520highest%250Ascores%2520are%2520then%2520selected%2520by%2520a%2520simple%2520Top-K%2520selection.%2520We%2520show%2520that%2520this%250Afrustratingly%2520simple%2520framework%2520is%2520applicable%2520to%2520both%2520long%2520videos%2520and%2520multi-page%250Adocuments%2520using%2520existing%2520LMMs%2520without%2520any%2520fine-tuning.%2520We%2520consider%2520two%2520models%252C%250ALLaVA-OneVision%2520and%2520InternVL2%252C%2520in%2520our%2520experiments%2520and%2520show%2520that%2520FRAG%250Aconsistently%2520improves%2520the%2520performance%2520and%2520achieves%2520state-of-the-art%250Aperformances%2520for%2520both%2520long%2520video%2520and%2520long%2520document%2520understanding.%2520For%2520videos%252C%250AFRAG%2520substantially%2520improves%2520InternVL2-76B%2520by%25205.8%2525%2520on%2520MLVU%2520and%25203.7%2525%2520on%250AVideo-MME.%2520For%2520documents%252C%2520FRAG%2520achieves%2520over%252020%2525%2520improvements%2520on%2520MP-DocVQA%250Acompared%2520with%2520recent%2520LMMs%2520specialized%2520in%2520long%2520document%2520understanding.%2520Code%2520is%250Aavailable%2520at%253A%2520https%253A//github.com/NVlabs/FRAG%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17447v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FRAG%3A%20Frame%20Selection%20Augmented%20Generation%20for%20Long%20Video%20and%20Long%0A%20%20Document%20Understanding&entry.906535625=De-An%20Huang%20and%20Subhashree%20Radhakrishnan%20and%20Zhiding%20Yu%20and%20Jan%20Kautz&entry.1292438233=%20%20There%20has%20been%20impressive%20progress%20in%20Large%20Multimodal%20Models%20%28LMMs%29.%20Recent%0Aworks%20extend%20these%20models%20to%20long%20inputs%2C%20including%20multi-page%20documents%20and%0Along%20videos.%20However%2C%20the%20model%20size%20and%20performance%20of%20these%20long%20context%0Amodels%20are%20still%20limited%20due%20to%20the%20computational%20cost%20in%20both%20training%20and%0Ainference.%20In%20this%20work%2C%20we%20explore%20an%20orthogonal%20direction%20and%20process%20long%0Ainputs%20without%20long%20context%20LMMs.%20We%20propose%20Frame%20Selection%20Augmented%0AGeneration%20%28FRAG%29%2C%20where%20the%20model%20first%20selects%20relevant%20frames%20within%20the%0Ainput%2C%20and%20then%20only%20generates%20the%20final%20outputs%20based%20on%20the%20selected%20frames.%0AThe%20core%20of%20the%20selection%20process%20is%20done%20by%20scoring%20each%20frame%20independently%2C%0Awhich%20does%20not%20require%20long%20context%20processing.%20The%20frames%20with%20the%20highest%0Ascores%20are%20then%20selected%20by%20a%20simple%20Top-K%20selection.%20We%20show%20that%20this%0Afrustratingly%20simple%20framework%20is%20applicable%20to%20both%20long%20videos%20and%20multi-page%0Adocuments%20using%20existing%20LMMs%20without%20any%20fine-tuning.%20We%20consider%20two%20models%2C%0ALLaVA-OneVision%20and%20InternVL2%2C%20in%20our%20experiments%20and%20show%20that%20FRAG%0Aconsistently%20improves%20the%20performance%20and%20achieves%20state-of-the-art%0Aperformances%20for%20both%20long%20video%20and%20long%20document%20understanding.%20For%20videos%2C%0AFRAG%20substantially%20improves%20InternVL2-76B%20by%205.8%25%20on%20MLVU%20and%203.7%25%20on%0AVideo-MME.%20For%20documents%2C%20FRAG%20achieves%20over%2020%25%20improvements%20on%20MP-DocVQA%0Acompared%20with%20recent%20LMMs%20specialized%20in%20long%20document%20understanding.%20Code%20is%0Aavailable%20at%3A%20https%3A//github.com/NVlabs/FRAG%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17447v1&entry.124074799=Read"},
{"title": "Unveiling Hidden Vulnerabilities in Digital Human Generation via\n  Adversarial Attacks", "author": "Zhiying Li and Yeying Jin and Fan Shen and Zhi Liu and Weibin Chen and Pengju Zhang and Xiaomei Zhang and Boyu Chen and Michael Shen and Kejian Wu and Zhaoxin Fan and Jin Dong", "abstract": "  Expressive human pose and shape estimation (EHPS) is crucial for digital\nhuman generation, especially in applications like live streaming. While\nexisting research primarily focuses on reducing estimation errors, it largely\nneglects robustness and security aspects, leaving these systems vulnerable to\nadversarial attacks. To address this significant challenge, we propose the\n\\textbf{Tangible Attack (TBA)}, a novel framework designed to generate\nadversarial examples capable of effectively compromising any digital human\ngeneration model. Our approach introduces a \\textbf{Dual Heterogeneous Noise\nGenerator (DHNG)}, which leverages Variational Autoencoders (VAE) and\nControlNet to produce diverse, targeted noise tailored to the original image\nfeatures. Additionally, we design a custom \\textbf{adversarial loss function}\nto optimize the noise, ensuring both high controllability and potent\ndisruption. By iteratively refining the adversarial sample through\nmulti-gradient signals from both the noise and the state-of-the-art EHPS model,\nTBA substantially improves the effectiveness of adversarial attacks. Extensive\nexperiments demonstrate TBA's superiority, achieving a remarkable 41.0\\%\nincrease in estimation error, with an average improvement of approximately\n17.0\\%. These findings expose significant security vulnerabilities in current\nEHPS models and highlight the need for stronger defenses in digital human\ngeneration systems.\n", "link": "http://arxiv.org/abs/2504.17457v1", "date": "2025-04-24", "relevancy": 2.1943, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5494}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5491}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20Hidden%20Vulnerabilities%20in%20Digital%20Human%20Generation%20via%0A%20%20Adversarial%20Attacks&body=Title%3A%20Unveiling%20Hidden%20Vulnerabilities%20in%20Digital%20Human%20Generation%20via%0A%20%20Adversarial%20Attacks%0AAuthor%3A%20Zhiying%20Li%20and%20Yeying%20Jin%20and%20Fan%20Shen%20and%20Zhi%20Liu%20and%20Weibin%20Chen%20and%20Pengju%20Zhang%20and%20Xiaomei%20Zhang%20and%20Boyu%20Chen%20and%20Michael%20Shen%20and%20Kejian%20Wu%20and%20Zhaoxin%20Fan%20and%20Jin%20Dong%0AAbstract%3A%20%20%20Expressive%20human%20pose%20and%20shape%20estimation%20%28EHPS%29%20is%20crucial%20for%20digital%0Ahuman%20generation%2C%20especially%20in%20applications%20like%20live%20streaming.%20While%0Aexisting%20research%20primarily%20focuses%20on%20reducing%20estimation%20errors%2C%20it%20largely%0Aneglects%20robustness%20and%20security%20aspects%2C%20leaving%20these%20systems%20vulnerable%20to%0Aadversarial%20attacks.%20To%20address%20this%20significant%20challenge%2C%20we%20propose%20the%0A%5Ctextbf%7BTangible%20Attack%20%28TBA%29%7D%2C%20a%20novel%20framework%20designed%20to%20generate%0Aadversarial%20examples%20capable%20of%20effectively%20compromising%20any%20digital%20human%0Ageneration%20model.%20Our%20approach%20introduces%20a%20%5Ctextbf%7BDual%20Heterogeneous%20Noise%0AGenerator%20%28DHNG%29%7D%2C%20which%20leverages%20Variational%20Autoencoders%20%28VAE%29%20and%0AControlNet%20to%20produce%20diverse%2C%20targeted%20noise%20tailored%20to%20the%20original%20image%0Afeatures.%20Additionally%2C%20we%20design%20a%20custom%20%5Ctextbf%7Badversarial%20loss%20function%7D%0Ato%20optimize%20the%20noise%2C%20ensuring%20both%20high%20controllability%20and%20potent%0Adisruption.%20By%20iteratively%20refining%20the%20adversarial%20sample%20through%0Amulti-gradient%20signals%20from%20both%20the%20noise%20and%20the%20state-of-the-art%20EHPS%20model%2C%0ATBA%20substantially%20improves%20the%20effectiveness%20of%20adversarial%20attacks.%20Extensive%0Aexperiments%20demonstrate%20TBA%27s%20superiority%2C%20achieving%20a%20remarkable%2041.0%5C%25%0Aincrease%20in%20estimation%20error%2C%20with%20an%20average%20improvement%20of%20approximately%0A17.0%5C%25.%20These%20findings%20expose%20significant%20security%20vulnerabilities%20in%20current%0AEHPS%20models%20and%20highlight%20the%20need%20for%20stronger%20defenses%20in%20digital%20human%0Ageneration%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17457v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520Hidden%2520Vulnerabilities%2520in%2520Digital%2520Human%2520Generation%2520via%250A%2520%2520Adversarial%2520Attacks%26entry.906535625%3DZhiying%2520Li%2520and%2520Yeying%2520Jin%2520and%2520Fan%2520Shen%2520and%2520Zhi%2520Liu%2520and%2520Weibin%2520Chen%2520and%2520Pengju%2520Zhang%2520and%2520Xiaomei%2520Zhang%2520and%2520Boyu%2520Chen%2520and%2520Michael%2520Shen%2520and%2520Kejian%2520Wu%2520and%2520Zhaoxin%2520Fan%2520and%2520Jin%2520Dong%26entry.1292438233%3D%2520%2520Expressive%2520human%2520pose%2520and%2520shape%2520estimation%2520%2528EHPS%2529%2520is%2520crucial%2520for%2520digital%250Ahuman%2520generation%252C%2520especially%2520in%2520applications%2520like%2520live%2520streaming.%2520While%250Aexisting%2520research%2520primarily%2520focuses%2520on%2520reducing%2520estimation%2520errors%252C%2520it%2520largely%250Aneglects%2520robustness%2520and%2520security%2520aspects%252C%2520leaving%2520these%2520systems%2520vulnerable%2520to%250Aadversarial%2520attacks.%2520To%2520address%2520this%2520significant%2520challenge%252C%2520we%2520propose%2520the%250A%255Ctextbf%257BTangible%2520Attack%2520%2528TBA%2529%257D%252C%2520a%2520novel%2520framework%2520designed%2520to%2520generate%250Aadversarial%2520examples%2520capable%2520of%2520effectively%2520compromising%2520any%2520digital%2520human%250Ageneration%2520model.%2520Our%2520approach%2520introduces%2520a%2520%255Ctextbf%257BDual%2520Heterogeneous%2520Noise%250AGenerator%2520%2528DHNG%2529%257D%252C%2520which%2520leverages%2520Variational%2520Autoencoders%2520%2528VAE%2529%2520and%250AControlNet%2520to%2520produce%2520diverse%252C%2520targeted%2520noise%2520tailored%2520to%2520the%2520original%2520image%250Afeatures.%2520Additionally%252C%2520we%2520design%2520a%2520custom%2520%255Ctextbf%257Badversarial%2520loss%2520function%257D%250Ato%2520optimize%2520the%2520noise%252C%2520ensuring%2520both%2520high%2520controllability%2520and%2520potent%250Adisruption.%2520By%2520iteratively%2520refining%2520the%2520adversarial%2520sample%2520through%250Amulti-gradient%2520signals%2520from%2520both%2520the%2520noise%2520and%2520the%2520state-of-the-art%2520EHPS%2520model%252C%250ATBA%2520substantially%2520improves%2520the%2520effectiveness%2520of%2520adversarial%2520attacks.%2520Extensive%250Aexperiments%2520demonstrate%2520TBA%2527s%2520superiority%252C%2520achieving%2520a%2520remarkable%252041.0%255C%2525%250Aincrease%2520in%2520estimation%2520error%252C%2520with%2520an%2520average%2520improvement%2520of%2520approximately%250A17.0%255C%2525.%2520These%2520findings%2520expose%2520significant%2520security%2520vulnerabilities%2520in%2520current%250AEHPS%2520models%2520and%2520highlight%2520the%2520need%2520for%2520stronger%2520defenses%2520in%2520digital%2520human%250Ageneration%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17457v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20Hidden%20Vulnerabilities%20in%20Digital%20Human%20Generation%20via%0A%20%20Adversarial%20Attacks&entry.906535625=Zhiying%20Li%20and%20Yeying%20Jin%20and%20Fan%20Shen%20and%20Zhi%20Liu%20and%20Weibin%20Chen%20and%20Pengju%20Zhang%20and%20Xiaomei%20Zhang%20and%20Boyu%20Chen%20and%20Michael%20Shen%20and%20Kejian%20Wu%20and%20Zhaoxin%20Fan%20and%20Jin%20Dong&entry.1292438233=%20%20Expressive%20human%20pose%20and%20shape%20estimation%20%28EHPS%29%20is%20crucial%20for%20digital%0Ahuman%20generation%2C%20especially%20in%20applications%20like%20live%20streaming.%20While%0Aexisting%20research%20primarily%20focuses%20on%20reducing%20estimation%20errors%2C%20it%20largely%0Aneglects%20robustness%20and%20security%20aspects%2C%20leaving%20these%20systems%20vulnerable%20to%0Aadversarial%20attacks.%20To%20address%20this%20significant%20challenge%2C%20we%20propose%20the%0A%5Ctextbf%7BTangible%20Attack%20%28TBA%29%7D%2C%20a%20novel%20framework%20designed%20to%20generate%0Aadversarial%20examples%20capable%20of%20effectively%20compromising%20any%20digital%20human%0Ageneration%20model.%20Our%20approach%20introduces%20a%20%5Ctextbf%7BDual%20Heterogeneous%20Noise%0AGenerator%20%28DHNG%29%7D%2C%20which%20leverages%20Variational%20Autoencoders%20%28VAE%29%20and%0AControlNet%20to%20produce%20diverse%2C%20targeted%20noise%20tailored%20to%20the%20original%20image%0Afeatures.%20Additionally%2C%20we%20design%20a%20custom%20%5Ctextbf%7Badversarial%20loss%20function%7D%0Ato%20optimize%20the%20noise%2C%20ensuring%20both%20high%20controllability%20and%20potent%0Adisruption.%20By%20iteratively%20refining%20the%20adversarial%20sample%20through%0Amulti-gradient%20signals%20from%20both%20the%20noise%20and%20the%20state-of-the-art%20EHPS%20model%2C%0ATBA%20substantially%20improves%20the%20effectiveness%20of%20adversarial%20attacks.%20Extensive%0Aexperiments%20demonstrate%20TBA%27s%20superiority%2C%20achieving%20a%20remarkable%2041.0%5C%25%0Aincrease%20in%20estimation%20error%2C%20with%20an%20average%20improvement%20of%20approximately%0A17.0%5C%25.%20These%20findings%20expose%20significant%20security%20vulnerabilities%20in%20current%0AEHPS%20models%20and%20highlight%20the%20need%20for%20stronger%20defenses%20in%20digital%20human%0Ageneration%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17457v1&entry.124074799=Read"},
{"title": "Enhanced Sample Selection with Confidence Tracking: Identifying\n  Correctly Labeled yet Hard-to-Learn Samples in Noisy Data", "author": "Weiran Pan and Wei Wei and Feida Zhu and Yong Deng", "abstract": "  We propose a novel sample selection method for image classification in the\npresence of noisy labels. Existing methods typically consider small-loss\nsamples as correctly labeled. However, some correctly labeled samples are\ninherently difficult for the model to learn and can exhibit high loss similar\nto mislabeled samples in the early stages of training. Consequently, setting a\nthreshold on per-sample loss to select correct labels results in a trade-off\nbetween precision and recall in sample selection: a lower threshold may miss\nmany correctly labeled hard-to-learn samples (low recall), while a higher\nthreshold may include many mislabeled samples (low precision). To address this\nissue, our goal is to accurately distinguish correctly labeled yet\nhard-to-learn samples from mislabeled ones, thus alleviating the trade-off\ndilemma. We achieve this by considering the trends in model prediction\nconfidence rather than relying solely on loss values. Empirical observations\nshow that only for correctly labeled samples, the model's prediction confidence\nfor the annotated labels typically increases faster than for any other classes.\nBased on this insight, we propose tracking the confidence gaps between the\nannotated labels and other classes during training and evaluating their trends\nusing the Mann-Kendall Test. A sample is considered potentially correctly\nlabeled if all its confidence gaps tend to increase. Our method functions as a\nplug-and-play component that can be seamlessly integrated into existing sample\nselection techniques. Experiments on several standard benchmarks and real-world\ndatasets demonstrate that our method enhances the performance of existing\nmethods for learning with noisy labels.\n", "link": "http://arxiv.org/abs/2504.17474v1", "date": "2025-04-24", "relevancy": 2.1933, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5823}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5452}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5156}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Sample%20Selection%20with%20Confidence%20Tracking%3A%20Identifying%0A%20%20Correctly%20Labeled%20yet%20Hard-to-Learn%20Samples%20in%20Noisy%20Data&body=Title%3A%20Enhanced%20Sample%20Selection%20with%20Confidence%20Tracking%3A%20Identifying%0A%20%20Correctly%20Labeled%20yet%20Hard-to-Learn%20Samples%20in%20Noisy%20Data%0AAuthor%3A%20Weiran%20Pan%20and%20Wei%20Wei%20and%20Feida%20Zhu%20and%20Yong%20Deng%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20sample%20selection%20method%20for%20image%20classification%20in%20the%0Apresence%20of%20noisy%20labels.%20Existing%20methods%20typically%20consider%20small-loss%0Asamples%20as%20correctly%20labeled.%20However%2C%20some%20correctly%20labeled%20samples%20are%0Ainherently%20difficult%20for%20the%20model%20to%20learn%20and%20can%20exhibit%20high%20loss%20similar%0Ato%20mislabeled%20samples%20in%20the%20early%20stages%20of%20training.%20Consequently%2C%20setting%20a%0Athreshold%20on%20per-sample%20loss%20to%20select%20correct%20labels%20results%20in%20a%20trade-off%0Abetween%20precision%20and%20recall%20in%20sample%20selection%3A%20a%20lower%20threshold%20may%20miss%0Amany%20correctly%20labeled%20hard-to-learn%20samples%20%28low%20recall%29%2C%20while%20a%20higher%0Athreshold%20may%20include%20many%20mislabeled%20samples%20%28low%20precision%29.%20To%20address%20this%0Aissue%2C%20our%20goal%20is%20to%20accurately%20distinguish%20correctly%20labeled%20yet%0Ahard-to-learn%20samples%20from%20mislabeled%20ones%2C%20thus%20alleviating%20the%20trade-off%0Adilemma.%20We%20achieve%20this%20by%20considering%20the%20trends%20in%20model%20prediction%0Aconfidence%20rather%20than%20relying%20solely%20on%20loss%20values.%20Empirical%20observations%0Ashow%20that%20only%20for%20correctly%20labeled%20samples%2C%20the%20model%27s%20prediction%20confidence%0Afor%20the%20annotated%20labels%20typically%20increases%20faster%20than%20for%20any%20other%20classes.%0ABased%20on%20this%20insight%2C%20we%20propose%20tracking%20the%20confidence%20gaps%20between%20the%0Aannotated%20labels%20and%20other%20classes%20during%20training%20and%20evaluating%20their%20trends%0Ausing%20the%20Mann-Kendall%20Test.%20A%20sample%20is%20considered%20potentially%20correctly%0Alabeled%20if%20all%20its%20confidence%20gaps%20tend%20to%20increase.%20Our%20method%20functions%20as%20a%0Aplug-and-play%20component%20that%20can%20be%20seamlessly%20integrated%20into%20existing%20sample%0Aselection%20techniques.%20Experiments%20on%20several%20standard%20benchmarks%20and%20real-world%0Adatasets%20demonstrate%20that%20our%20method%20enhances%20the%20performance%20of%20existing%0Amethods%20for%20learning%20with%20noisy%20labels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17474v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Sample%2520Selection%2520with%2520Confidence%2520Tracking%253A%2520Identifying%250A%2520%2520Correctly%2520Labeled%2520yet%2520Hard-to-Learn%2520Samples%2520in%2520Noisy%2520Data%26entry.906535625%3DWeiran%2520Pan%2520and%2520Wei%2520Wei%2520and%2520Feida%2520Zhu%2520and%2520Yong%2520Deng%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520sample%2520selection%2520method%2520for%2520image%2520classification%2520in%2520the%250Apresence%2520of%2520noisy%2520labels.%2520Existing%2520methods%2520typically%2520consider%2520small-loss%250Asamples%2520as%2520correctly%2520labeled.%2520However%252C%2520some%2520correctly%2520labeled%2520samples%2520are%250Ainherently%2520difficult%2520for%2520the%2520model%2520to%2520learn%2520and%2520can%2520exhibit%2520high%2520loss%2520similar%250Ato%2520mislabeled%2520samples%2520in%2520the%2520early%2520stages%2520of%2520training.%2520Consequently%252C%2520setting%2520a%250Athreshold%2520on%2520per-sample%2520loss%2520to%2520select%2520correct%2520labels%2520results%2520in%2520a%2520trade-off%250Abetween%2520precision%2520and%2520recall%2520in%2520sample%2520selection%253A%2520a%2520lower%2520threshold%2520may%2520miss%250Amany%2520correctly%2520labeled%2520hard-to-learn%2520samples%2520%2528low%2520recall%2529%252C%2520while%2520a%2520higher%250Athreshold%2520may%2520include%2520many%2520mislabeled%2520samples%2520%2528low%2520precision%2529.%2520To%2520address%2520this%250Aissue%252C%2520our%2520goal%2520is%2520to%2520accurately%2520distinguish%2520correctly%2520labeled%2520yet%250Ahard-to-learn%2520samples%2520from%2520mislabeled%2520ones%252C%2520thus%2520alleviating%2520the%2520trade-off%250Adilemma.%2520We%2520achieve%2520this%2520by%2520considering%2520the%2520trends%2520in%2520model%2520prediction%250Aconfidence%2520rather%2520than%2520relying%2520solely%2520on%2520loss%2520values.%2520Empirical%2520observations%250Ashow%2520that%2520only%2520for%2520correctly%2520labeled%2520samples%252C%2520the%2520model%2527s%2520prediction%2520confidence%250Afor%2520the%2520annotated%2520labels%2520typically%2520increases%2520faster%2520than%2520for%2520any%2520other%2520classes.%250ABased%2520on%2520this%2520insight%252C%2520we%2520propose%2520tracking%2520the%2520confidence%2520gaps%2520between%2520the%250Aannotated%2520labels%2520and%2520other%2520classes%2520during%2520training%2520and%2520evaluating%2520their%2520trends%250Ausing%2520the%2520Mann-Kendall%2520Test.%2520A%2520sample%2520is%2520considered%2520potentially%2520correctly%250Alabeled%2520if%2520all%2520its%2520confidence%2520gaps%2520tend%2520to%2520increase.%2520Our%2520method%2520functions%2520as%2520a%250Aplug-and-play%2520component%2520that%2520can%2520be%2520seamlessly%2520integrated%2520into%2520existing%2520sample%250Aselection%2520techniques.%2520Experiments%2520on%2520several%2520standard%2520benchmarks%2520and%2520real-world%250Adatasets%2520demonstrate%2520that%2520our%2520method%2520enhances%2520the%2520performance%2520of%2520existing%250Amethods%2520for%2520learning%2520with%2520noisy%2520labels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17474v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Sample%20Selection%20with%20Confidence%20Tracking%3A%20Identifying%0A%20%20Correctly%20Labeled%20yet%20Hard-to-Learn%20Samples%20in%20Noisy%20Data&entry.906535625=Weiran%20Pan%20and%20Wei%20Wei%20and%20Feida%20Zhu%20and%20Yong%20Deng&entry.1292438233=%20%20We%20propose%20a%20novel%20sample%20selection%20method%20for%20image%20classification%20in%20the%0Apresence%20of%20noisy%20labels.%20Existing%20methods%20typically%20consider%20small-loss%0Asamples%20as%20correctly%20labeled.%20However%2C%20some%20correctly%20labeled%20samples%20are%0Ainherently%20difficult%20for%20the%20model%20to%20learn%20and%20can%20exhibit%20high%20loss%20similar%0Ato%20mislabeled%20samples%20in%20the%20early%20stages%20of%20training.%20Consequently%2C%20setting%20a%0Athreshold%20on%20per-sample%20loss%20to%20select%20correct%20labels%20results%20in%20a%20trade-off%0Abetween%20precision%20and%20recall%20in%20sample%20selection%3A%20a%20lower%20threshold%20may%20miss%0Amany%20correctly%20labeled%20hard-to-learn%20samples%20%28low%20recall%29%2C%20while%20a%20higher%0Athreshold%20may%20include%20many%20mislabeled%20samples%20%28low%20precision%29.%20To%20address%20this%0Aissue%2C%20our%20goal%20is%20to%20accurately%20distinguish%20correctly%20labeled%20yet%0Ahard-to-learn%20samples%20from%20mislabeled%20ones%2C%20thus%20alleviating%20the%20trade-off%0Adilemma.%20We%20achieve%20this%20by%20considering%20the%20trends%20in%20model%20prediction%0Aconfidence%20rather%20than%20relying%20solely%20on%20loss%20values.%20Empirical%20observations%0Ashow%20that%20only%20for%20correctly%20labeled%20samples%2C%20the%20model%27s%20prediction%20confidence%0Afor%20the%20annotated%20labels%20typically%20increases%20faster%20than%20for%20any%20other%20classes.%0ABased%20on%20this%20insight%2C%20we%20propose%20tracking%20the%20confidence%20gaps%20between%20the%0Aannotated%20labels%20and%20other%20classes%20during%20training%20and%20evaluating%20their%20trends%0Ausing%20the%20Mann-Kendall%20Test.%20A%20sample%20is%20considered%20potentially%20correctly%0Alabeled%20if%20all%20its%20confidence%20gaps%20tend%20to%20increase.%20Our%20method%20functions%20as%20a%0Aplug-and-play%20component%20that%20can%20be%20seamlessly%20integrated%20into%20existing%20sample%0Aselection%20techniques.%20Experiments%20on%20several%20standard%20benchmarks%20and%20real-world%0Adatasets%20demonstrate%20that%20our%20method%20enhances%20the%20performance%20of%20existing%0Amethods%20for%20learning%20with%20noisy%20labels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17474v1&entry.124074799=Read"},
{"title": "The Fourth Monocular Depth Estimation Challenge", "author": "Anton Obukhov and Matteo Poggi and Fabio Tosi and Ripudaman Singh Arora and Jaime Spencer and Chris Russell and Simon Hadfield and Richard Bowden and Shuaihang Wang and Zhenxin Ma and Weijie Chen and Baobei Xu and Fengyu Sun and Di Xie and Jiang Zhu and Mykola Lavreniuk and Haining Guan and Qun Wu and Yupei Zeng and Chao Lu and Huanran Wang and Guangyuan Zhou and Haotian Zhang and Jianxiong Wang and Qiang Rao and Chunjie Wang and Xiao Liu and Zhiqiang Lou and Hualie Jiang and Yihao Chen and Rui Xu and Minglang Tan and Zihan Qin and Yifan Mao and Jiayang Liu and Jialei Xu and Yifan Yang and Wenbo Zhao and Junjun Jiang and Xianming Liu and Mingshuai Zhao and Anlong Ming and Wu Chen and Feng Xue and Mengying Yu and Shida Gao and Xiangfeng Wang and Gbenga Omotara and Ramy Farag and Jacket Demby and Seyed Mohamad Ali Tousi and Guilherme N DeSouza and Tuan-Anh Yang and Minh-Quang Nguyen and Thien-Phuc Tran and Albert Luginov and Muhammad Shahzad", "abstract": "  This paper presents the results of the fourth edition of the Monocular Depth\nEstimation Challenge (MDEC), which focuses on zero-shot generalization to the\nSYNS-Patches benchmark, a dataset featuring challenging environments in both\nnatural and indoor settings. In this edition, we revised the evaluation\nprotocol to use least-squares alignment with two degrees of freedom to support\ndisparity and affine-invariant predictions. We also revised the baselines and\nincluded popular off-the-shelf methods: Depth Anything v2 and Marigold. The\nchallenge received a total of 24 submissions that outperformed the baselines on\nthe test set; 10 of these included a report describing their approach, with\nmost leading methods relying on affine-invariant predictions. The challenge\nwinners improved the 3D F-Score over the previous edition's best result,\nraising it from 22.58% to 23.05%.\n", "link": "http://arxiv.org/abs/2504.17787v1", "date": "2025-04-24", "relevancy": 2.1811, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5564}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5431}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Fourth%20Monocular%20Depth%20Estimation%20Challenge&body=Title%3A%20The%20Fourth%20Monocular%20Depth%20Estimation%20Challenge%0AAuthor%3A%20Anton%20Obukhov%20and%20Matteo%20Poggi%20and%20Fabio%20Tosi%20and%20Ripudaman%20Singh%20Arora%20and%20Jaime%20Spencer%20and%20Chris%20Russell%20and%20Simon%20Hadfield%20and%20Richard%20Bowden%20and%20Shuaihang%20Wang%20and%20Zhenxin%20Ma%20and%20Weijie%20Chen%20and%20Baobei%20Xu%20and%20Fengyu%20Sun%20and%20Di%20Xie%20and%20Jiang%20Zhu%20and%20Mykola%20Lavreniuk%20and%20Haining%20Guan%20and%20Qun%20Wu%20and%20Yupei%20Zeng%20and%20Chao%20Lu%20and%20Huanran%20Wang%20and%20Guangyuan%20Zhou%20and%20Haotian%20Zhang%20and%20Jianxiong%20Wang%20and%20Qiang%20Rao%20and%20Chunjie%20Wang%20and%20Xiao%20Liu%20and%20Zhiqiang%20Lou%20and%20Hualie%20Jiang%20and%20Yihao%20Chen%20and%20Rui%20Xu%20and%20Minglang%20Tan%20and%20Zihan%20Qin%20and%20Yifan%20Mao%20and%20Jiayang%20Liu%20and%20Jialei%20Xu%20and%20Yifan%20Yang%20and%20Wenbo%20Zhao%20and%20Junjun%20Jiang%20and%20Xianming%20Liu%20and%20Mingshuai%20Zhao%20and%20Anlong%20Ming%20and%20Wu%20Chen%20and%20Feng%20Xue%20and%20Mengying%20Yu%20and%20Shida%20Gao%20and%20Xiangfeng%20Wang%20and%20Gbenga%20Omotara%20and%20Ramy%20Farag%20and%20Jacket%20Demby%20and%20Seyed%20Mohamad%20Ali%20Tousi%20and%20Guilherme%20N%20DeSouza%20and%20Tuan-Anh%20Yang%20and%20Minh-Quang%20Nguyen%20and%20Thien-Phuc%20Tran%20and%20Albert%20Luginov%20and%20Muhammad%20Shahzad%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20results%20of%20the%20fourth%20edition%20of%20the%20Monocular%20Depth%0AEstimation%20Challenge%20%28MDEC%29%2C%20which%20focuses%20on%20zero-shot%20generalization%20to%20the%0ASYNS-Patches%20benchmark%2C%20a%20dataset%20featuring%20challenging%20environments%20in%20both%0Anatural%20and%20indoor%20settings.%20In%20this%20edition%2C%20we%20revised%20the%20evaluation%0Aprotocol%20to%20use%20least-squares%20alignment%20with%20two%20degrees%20of%20freedom%20to%20support%0Adisparity%20and%20affine-invariant%20predictions.%20We%20also%20revised%20the%20baselines%20and%0Aincluded%20popular%20off-the-shelf%20methods%3A%20Depth%20Anything%20v2%20and%20Marigold.%20The%0Achallenge%20received%20a%20total%20of%2024%20submissions%20that%20outperformed%20the%20baselines%20on%0Athe%20test%20set%3B%2010%20of%20these%20included%20a%20report%20describing%20their%20approach%2C%20with%0Amost%20leading%20methods%20relying%20on%20affine-invariant%20predictions.%20The%20challenge%0Awinners%20improved%20the%203D%20F-Score%20over%20the%20previous%20edition%27s%20best%20result%2C%0Araising%20it%20from%2022.58%25%20to%2023.05%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17787v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Fourth%2520Monocular%2520Depth%2520Estimation%2520Challenge%26entry.906535625%3DAnton%2520Obukhov%2520and%2520Matteo%2520Poggi%2520and%2520Fabio%2520Tosi%2520and%2520Ripudaman%2520Singh%2520Arora%2520and%2520Jaime%2520Spencer%2520and%2520Chris%2520Russell%2520and%2520Simon%2520Hadfield%2520and%2520Richard%2520Bowden%2520and%2520Shuaihang%2520Wang%2520and%2520Zhenxin%2520Ma%2520and%2520Weijie%2520Chen%2520and%2520Baobei%2520Xu%2520and%2520Fengyu%2520Sun%2520and%2520Di%2520Xie%2520and%2520Jiang%2520Zhu%2520and%2520Mykola%2520Lavreniuk%2520and%2520Haining%2520Guan%2520and%2520Qun%2520Wu%2520and%2520Yupei%2520Zeng%2520and%2520Chao%2520Lu%2520and%2520Huanran%2520Wang%2520and%2520Guangyuan%2520Zhou%2520and%2520Haotian%2520Zhang%2520and%2520Jianxiong%2520Wang%2520and%2520Qiang%2520Rao%2520and%2520Chunjie%2520Wang%2520and%2520Xiao%2520Liu%2520and%2520Zhiqiang%2520Lou%2520and%2520Hualie%2520Jiang%2520and%2520Yihao%2520Chen%2520and%2520Rui%2520Xu%2520and%2520Minglang%2520Tan%2520and%2520Zihan%2520Qin%2520and%2520Yifan%2520Mao%2520and%2520Jiayang%2520Liu%2520and%2520Jialei%2520Xu%2520and%2520Yifan%2520Yang%2520and%2520Wenbo%2520Zhao%2520and%2520Junjun%2520Jiang%2520and%2520Xianming%2520Liu%2520and%2520Mingshuai%2520Zhao%2520and%2520Anlong%2520Ming%2520and%2520Wu%2520Chen%2520and%2520Feng%2520Xue%2520and%2520Mengying%2520Yu%2520and%2520Shida%2520Gao%2520and%2520Xiangfeng%2520Wang%2520and%2520Gbenga%2520Omotara%2520and%2520Ramy%2520Farag%2520and%2520Jacket%2520Demby%2520and%2520Seyed%2520Mohamad%2520Ali%2520Tousi%2520and%2520Guilherme%2520N%2520DeSouza%2520and%2520Tuan-Anh%2520Yang%2520and%2520Minh-Quang%2520Nguyen%2520and%2520Thien-Phuc%2520Tran%2520and%2520Albert%2520Luginov%2520and%2520Muhammad%2520Shahzad%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520results%2520of%2520the%2520fourth%2520edition%2520of%2520the%2520Monocular%2520Depth%250AEstimation%2520Challenge%2520%2528MDEC%2529%252C%2520which%2520focuses%2520on%2520zero-shot%2520generalization%2520to%2520the%250ASYNS-Patches%2520benchmark%252C%2520a%2520dataset%2520featuring%2520challenging%2520environments%2520in%2520both%250Anatural%2520and%2520indoor%2520settings.%2520In%2520this%2520edition%252C%2520we%2520revised%2520the%2520evaluation%250Aprotocol%2520to%2520use%2520least-squares%2520alignment%2520with%2520two%2520degrees%2520of%2520freedom%2520to%2520support%250Adisparity%2520and%2520affine-invariant%2520predictions.%2520We%2520also%2520revised%2520the%2520baselines%2520and%250Aincluded%2520popular%2520off-the-shelf%2520methods%253A%2520Depth%2520Anything%2520v2%2520and%2520Marigold.%2520The%250Achallenge%2520received%2520a%2520total%2520of%252024%2520submissions%2520that%2520outperformed%2520the%2520baselines%2520on%250Athe%2520test%2520set%253B%252010%2520of%2520these%2520included%2520a%2520report%2520describing%2520their%2520approach%252C%2520with%250Amost%2520leading%2520methods%2520relying%2520on%2520affine-invariant%2520predictions.%2520The%2520challenge%250Awinners%2520improved%2520the%25203D%2520F-Score%2520over%2520the%2520previous%2520edition%2527s%2520best%2520result%252C%250Araising%2520it%2520from%252022.58%2525%2520to%252023.05%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17787v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Fourth%20Monocular%20Depth%20Estimation%20Challenge&entry.906535625=Anton%20Obukhov%20and%20Matteo%20Poggi%20and%20Fabio%20Tosi%20and%20Ripudaman%20Singh%20Arora%20and%20Jaime%20Spencer%20and%20Chris%20Russell%20and%20Simon%20Hadfield%20and%20Richard%20Bowden%20and%20Shuaihang%20Wang%20and%20Zhenxin%20Ma%20and%20Weijie%20Chen%20and%20Baobei%20Xu%20and%20Fengyu%20Sun%20and%20Di%20Xie%20and%20Jiang%20Zhu%20and%20Mykola%20Lavreniuk%20and%20Haining%20Guan%20and%20Qun%20Wu%20and%20Yupei%20Zeng%20and%20Chao%20Lu%20and%20Huanran%20Wang%20and%20Guangyuan%20Zhou%20and%20Haotian%20Zhang%20and%20Jianxiong%20Wang%20and%20Qiang%20Rao%20and%20Chunjie%20Wang%20and%20Xiao%20Liu%20and%20Zhiqiang%20Lou%20and%20Hualie%20Jiang%20and%20Yihao%20Chen%20and%20Rui%20Xu%20and%20Minglang%20Tan%20and%20Zihan%20Qin%20and%20Yifan%20Mao%20and%20Jiayang%20Liu%20and%20Jialei%20Xu%20and%20Yifan%20Yang%20and%20Wenbo%20Zhao%20and%20Junjun%20Jiang%20and%20Xianming%20Liu%20and%20Mingshuai%20Zhao%20and%20Anlong%20Ming%20and%20Wu%20Chen%20and%20Feng%20Xue%20and%20Mengying%20Yu%20and%20Shida%20Gao%20and%20Xiangfeng%20Wang%20and%20Gbenga%20Omotara%20and%20Ramy%20Farag%20and%20Jacket%20Demby%20and%20Seyed%20Mohamad%20Ali%20Tousi%20and%20Guilherme%20N%20DeSouza%20and%20Tuan-Anh%20Yang%20and%20Minh-Quang%20Nguyen%20and%20Thien-Phuc%20Tran%20and%20Albert%20Luginov%20and%20Muhammad%20Shahzad&entry.1292438233=%20%20This%20paper%20presents%20the%20results%20of%20the%20fourth%20edition%20of%20the%20Monocular%20Depth%0AEstimation%20Challenge%20%28MDEC%29%2C%20which%20focuses%20on%20zero-shot%20generalization%20to%20the%0ASYNS-Patches%20benchmark%2C%20a%20dataset%20featuring%20challenging%20environments%20in%20both%0Anatural%20and%20indoor%20settings.%20In%20this%20edition%2C%20we%20revised%20the%20evaluation%0Aprotocol%20to%20use%20least-squares%20alignment%20with%20two%20degrees%20of%20freedom%20to%20support%0Adisparity%20and%20affine-invariant%20predictions.%20We%20also%20revised%20the%20baselines%20and%0Aincluded%20popular%20off-the-shelf%20methods%3A%20Depth%20Anything%20v2%20and%20Marigold.%20The%0Achallenge%20received%20a%20total%20of%2024%20submissions%20that%20outperformed%20the%20baselines%20on%0Athe%20test%20set%3B%2010%20of%20these%20included%20a%20report%20describing%20their%20approach%2C%20with%0Amost%20leading%20methods%20relying%20on%20affine-invariant%20predictions.%20The%20challenge%0Awinners%20improved%20the%203D%20F-Score%20over%20the%20previous%20edition%27s%20best%20result%2C%0Araising%20it%20from%2022.58%25%20to%2023.05%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17787v1&entry.124074799=Read"},
{"title": "Building Trustworthy Multimodal AI: A Review of Fairness, Transparency,\n  and Ethics in Vision-Language Tasks", "author": "Mohammad Saleh and Azadeh Tabatabaei", "abstract": "  Objective: This review explores the trustworthiness of multimodal artificial\nintelligence (AI) systems, specifically focusing on vision-language tasks. It\naddresses critical challenges related to fairness, transparency, and ethical\nimplications in these systems, providing a comparative analysis of key tasks\nsuch as Visual Question Answering (VQA), image captioning, and visual dialogue.\nBackground: Multimodal models, particularly vision-language models, enhance\nartificial intelligence (AI) capabilities by integrating visual and textual\ndata, mimicking human learning processes. Despite significant advancements, the\ntrustworthiness of these models remains a crucial concern, particularly as AI\nsystems increasingly confront issues regarding fairness, transparency, and\nethics. Methods: This review examines research conducted from 2017 to 2024\nfocusing on forenamed core vision-language tasks. It employs a comparative\napproach to analyze these tasks through the lens of trustworthiness,\nunderlining fairness, explainability, and ethics. This study synthesizes\nfindings from recent literature to identify trends, challenges, and\nstate-of-the-art solutions. Results: Several key findings were highlighted.\nTransparency: Explainability of vision language tasks is important for user\ntrust. Techniques, such as attention maps and gradient-based methods, have\nsuccessfully addressed this issue. Fairness: Bias mitigation in VQA and visual\ndialogue systems is essential for ensuring unbiased outcomes across diverse\ndemographic groups. Ethical Implications: Addressing biases in multilingual\nmodels and ensuring ethical data handling is critical for the responsible\ndeployment of vision-language systems. Conclusion: This study underscores the\nimportance of integrating fairness, transparency, and ethical considerations in\ndeveloping vision-language models within a unified framework.\n", "link": "http://arxiv.org/abs/2504.13199v2", "date": "2025-04-24", "relevancy": 2.1742, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5497}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5497}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Building%20Trustworthy%20Multimodal%20AI%3A%20A%20Review%20of%20Fairness%2C%20Transparency%2C%0A%20%20and%20Ethics%20in%20Vision-Language%20Tasks&body=Title%3A%20Building%20Trustworthy%20Multimodal%20AI%3A%20A%20Review%20of%20Fairness%2C%20Transparency%2C%0A%20%20and%20Ethics%20in%20Vision-Language%20Tasks%0AAuthor%3A%20Mohammad%20Saleh%20and%20Azadeh%20Tabatabaei%0AAbstract%3A%20%20%20Objective%3A%20This%20review%20explores%20the%20trustworthiness%20of%20multimodal%20artificial%0Aintelligence%20%28AI%29%20systems%2C%20specifically%20focusing%20on%20vision-language%20tasks.%20It%0Aaddresses%20critical%20challenges%20related%20to%20fairness%2C%20transparency%2C%20and%20ethical%0Aimplications%20in%20these%20systems%2C%20providing%20a%20comparative%20analysis%20of%20key%20tasks%0Asuch%20as%20Visual%20Question%20Answering%20%28VQA%29%2C%20image%20captioning%2C%20and%20visual%20dialogue.%0ABackground%3A%20Multimodal%20models%2C%20particularly%20vision-language%20models%2C%20enhance%0Aartificial%20intelligence%20%28AI%29%20capabilities%20by%20integrating%20visual%20and%20textual%0Adata%2C%20mimicking%20human%20learning%20processes.%20Despite%20significant%20advancements%2C%20the%0Atrustworthiness%20of%20these%20models%20remains%20a%20crucial%20concern%2C%20particularly%20as%20AI%0Asystems%20increasingly%20confront%20issues%20regarding%20fairness%2C%20transparency%2C%20and%0Aethics.%20Methods%3A%20This%20review%20examines%20research%20conducted%20from%202017%20to%202024%0Afocusing%20on%20forenamed%20core%20vision-language%20tasks.%20It%20employs%20a%20comparative%0Aapproach%20to%20analyze%20these%20tasks%20through%20the%20lens%20of%20trustworthiness%2C%0Aunderlining%20fairness%2C%20explainability%2C%20and%20ethics.%20This%20study%20synthesizes%0Afindings%20from%20recent%20literature%20to%20identify%20trends%2C%20challenges%2C%20and%0Astate-of-the-art%20solutions.%20Results%3A%20Several%20key%20findings%20were%20highlighted.%0ATransparency%3A%20Explainability%20of%20vision%20language%20tasks%20is%20important%20for%20user%0Atrust.%20Techniques%2C%20such%20as%20attention%20maps%20and%20gradient-based%20methods%2C%20have%0Asuccessfully%20addressed%20this%20issue.%20Fairness%3A%20Bias%20mitigation%20in%20VQA%20and%20visual%0Adialogue%20systems%20is%20essential%20for%20ensuring%20unbiased%20outcomes%20across%20diverse%0Ademographic%20groups.%20Ethical%20Implications%3A%20Addressing%20biases%20in%20multilingual%0Amodels%20and%20ensuring%20ethical%20data%20handling%20is%20critical%20for%20the%20responsible%0Adeployment%20of%20vision-language%20systems.%20Conclusion%3A%20This%20study%20underscores%20the%0Aimportance%20of%20integrating%20fairness%2C%20transparency%2C%20and%20ethical%20considerations%20in%0Adeveloping%20vision-language%20models%20within%20a%20unified%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13199v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBuilding%2520Trustworthy%2520Multimodal%2520AI%253A%2520A%2520Review%2520of%2520Fairness%252C%2520Transparency%252C%250A%2520%2520and%2520Ethics%2520in%2520Vision-Language%2520Tasks%26entry.906535625%3DMohammad%2520Saleh%2520and%2520Azadeh%2520Tabatabaei%26entry.1292438233%3D%2520%2520Objective%253A%2520This%2520review%2520explores%2520the%2520trustworthiness%2520of%2520multimodal%2520artificial%250Aintelligence%2520%2528AI%2529%2520systems%252C%2520specifically%2520focusing%2520on%2520vision-language%2520tasks.%2520It%250Aaddresses%2520critical%2520challenges%2520related%2520to%2520fairness%252C%2520transparency%252C%2520and%2520ethical%250Aimplications%2520in%2520these%2520systems%252C%2520providing%2520a%2520comparative%2520analysis%2520of%2520key%2520tasks%250Asuch%2520as%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%252C%2520image%2520captioning%252C%2520and%2520visual%2520dialogue.%250ABackground%253A%2520Multimodal%2520models%252C%2520particularly%2520vision-language%2520models%252C%2520enhance%250Aartificial%2520intelligence%2520%2528AI%2529%2520capabilities%2520by%2520integrating%2520visual%2520and%2520textual%250Adata%252C%2520mimicking%2520human%2520learning%2520processes.%2520Despite%2520significant%2520advancements%252C%2520the%250Atrustworthiness%2520of%2520these%2520models%2520remains%2520a%2520crucial%2520concern%252C%2520particularly%2520as%2520AI%250Asystems%2520increasingly%2520confront%2520issues%2520regarding%2520fairness%252C%2520transparency%252C%2520and%250Aethics.%2520Methods%253A%2520This%2520review%2520examines%2520research%2520conducted%2520from%25202017%2520to%25202024%250Afocusing%2520on%2520forenamed%2520core%2520vision-language%2520tasks.%2520It%2520employs%2520a%2520comparative%250Aapproach%2520to%2520analyze%2520these%2520tasks%2520through%2520the%2520lens%2520of%2520trustworthiness%252C%250Aunderlining%2520fairness%252C%2520explainability%252C%2520and%2520ethics.%2520This%2520study%2520synthesizes%250Afindings%2520from%2520recent%2520literature%2520to%2520identify%2520trends%252C%2520challenges%252C%2520and%250Astate-of-the-art%2520solutions.%2520Results%253A%2520Several%2520key%2520findings%2520were%2520highlighted.%250ATransparency%253A%2520Explainability%2520of%2520vision%2520language%2520tasks%2520is%2520important%2520for%2520user%250Atrust.%2520Techniques%252C%2520such%2520as%2520attention%2520maps%2520and%2520gradient-based%2520methods%252C%2520have%250Asuccessfully%2520addressed%2520this%2520issue.%2520Fairness%253A%2520Bias%2520mitigation%2520in%2520VQA%2520and%2520visual%250Adialogue%2520systems%2520is%2520essential%2520for%2520ensuring%2520unbiased%2520outcomes%2520across%2520diverse%250Ademographic%2520groups.%2520Ethical%2520Implications%253A%2520Addressing%2520biases%2520in%2520multilingual%250Amodels%2520and%2520ensuring%2520ethical%2520data%2520handling%2520is%2520critical%2520for%2520the%2520responsible%250Adeployment%2520of%2520vision-language%2520systems.%2520Conclusion%253A%2520This%2520study%2520underscores%2520the%250Aimportance%2520of%2520integrating%2520fairness%252C%2520transparency%252C%2520and%2520ethical%2520considerations%2520in%250Adeveloping%2520vision-language%2520models%2520within%2520a%2520unified%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13199v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Building%20Trustworthy%20Multimodal%20AI%3A%20A%20Review%20of%20Fairness%2C%20Transparency%2C%0A%20%20and%20Ethics%20in%20Vision-Language%20Tasks&entry.906535625=Mohammad%20Saleh%20and%20Azadeh%20Tabatabaei&entry.1292438233=%20%20Objective%3A%20This%20review%20explores%20the%20trustworthiness%20of%20multimodal%20artificial%0Aintelligence%20%28AI%29%20systems%2C%20specifically%20focusing%20on%20vision-language%20tasks.%20It%0Aaddresses%20critical%20challenges%20related%20to%20fairness%2C%20transparency%2C%20and%20ethical%0Aimplications%20in%20these%20systems%2C%20providing%20a%20comparative%20analysis%20of%20key%20tasks%0Asuch%20as%20Visual%20Question%20Answering%20%28VQA%29%2C%20image%20captioning%2C%20and%20visual%20dialogue.%0ABackground%3A%20Multimodal%20models%2C%20particularly%20vision-language%20models%2C%20enhance%0Aartificial%20intelligence%20%28AI%29%20capabilities%20by%20integrating%20visual%20and%20textual%0Adata%2C%20mimicking%20human%20learning%20processes.%20Despite%20significant%20advancements%2C%20the%0Atrustworthiness%20of%20these%20models%20remains%20a%20crucial%20concern%2C%20particularly%20as%20AI%0Asystems%20increasingly%20confront%20issues%20regarding%20fairness%2C%20transparency%2C%20and%0Aethics.%20Methods%3A%20This%20review%20examines%20research%20conducted%20from%202017%20to%202024%0Afocusing%20on%20forenamed%20core%20vision-language%20tasks.%20It%20employs%20a%20comparative%0Aapproach%20to%20analyze%20these%20tasks%20through%20the%20lens%20of%20trustworthiness%2C%0Aunderlining%20fairness%2C%20explainability%2C%20and%20ethics.%20This%20study%20synthesizes%0Afindings%20from%20recent%20literature%20to%20identify%20trends%2C%20challenges%2C%20and%0Astate-of-the-art%20solutions.%20Results%3A%20Several%20key%20findings%20were%20highlighted.%0ATransparency%3A%20Explainability%20of%20vision%20language%20tasks%20is%20important%20for%20user%0Atrust.%20Techniques%2C%20such%20as%20attention%20maps%20and%20gradient-based%20methods%2C%20have%0Asuccessfully%20addressed%20this%20issue.%20Fairness%3A%20Bias%20mitigation%20in%20VQA%20and%20visual%0Adialogue%20systems%20is%20essential%20for%20ensuring%20unbiased%20outcomes%20across%20diverse%0Ademographic%20groups.%20Ethical%20Implications%3A%20Addressing%20biases%20in%20multilingual%0Amodels%20and%20ensuring%20ethical%20data%20handling%20is%20critical%20for%20the%20responsible%0Adeployment%20of%20vision-language%20systems.%20Conclusion%3A%20This%20study%20underscores%20the%0Aimportance%20of%20integrating%20fairness%2C%20transparency%2C%20and%20ethical%20considerations%20in%0Adeveloping%20vision-language%20models%20within%20a%20unified%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13199v2&entry.124074799=Read"},
{"title": "Efficient Iterative Proximal Variational Inference Motion Planning", "author": "Zinuo Chang and Hongzhe Yu and Patricio Vela and Yongxin Chen", "abstract": "  Motion planning under uncertainty can be cast as a stochastic optimal control\nproblem where the optimal posterior distribution has an explicit form. To\napproximate this posterior, this work frames an optimization problem in the\nspace of Gaussian distributions by solving a Variational Inference (VI) in the\npath distribution space. For linear-Gaussian stochastic dynamics, we propose a\nproximal algorithm to solve for an optimal Gaussian proposal iteratively. The\ncomputational bottleneck is evaluating the gradients with respect to the\nproposal over a dense trajectory. We exploit the sparse motion planning factor\ngraph and Gaussian Belief Propagation (GBP), allowing for parallel computing of\nthese gradients on Graphics Processing Units (GPUs). We term the novel paradigm\nas the Parallel Gaussian Variational Inference Motion Planning (P-GVIMP).\nBuilding on the efficient algorithm for linear Gaussian systems, we then\npropose an iterative paradigm based on Statistical Linear Regression (SLR)\ntechniques to solve motion planning for nonlinear stochastic systems, where the\nP-GVIMP serves as a sub-routine for the linearized time-varying system. We\nvalidate the proposed framework on various robotic systems, demonstrating\nsignificant speed acceleration achieved by leveraging parallel computation and\nsuccessful planning solutions for nonlinear systems under uncertainty. An\nopen-sourced implementation is presented at https://github.com/hzyu17/VIMP.\n", "link": "http://arxiv.org/abs/2411.03416v3", "date": "2025-04-24", "relevancy": 2.1708, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5716}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.541}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Iterative%20Proximal%20Variational%20Inference%20Motion%20Planning&body=Title%3A%20Efficient%20Iterative%20Proximal%20Variational%20Inference%20Motion%20Planning%0AAuthor%3A%20Zinuo%20Chang%20and%20Hongzhe%20Yu%20and%20Patricio%20Vela%20and%20Yongxin%20Chen%0AAbstract%3A%20%20%20Motion%20planning%20under%20uncertainty%20can%20be%20cast%20as%20a%20stochastic%20optimal%20control%0Aproblem%20where%20the%20optimal%20posterior%20distribution%20has%20an%20explicit%20form.%20To%0Aapproximate%20this%20posterior%2C%20this%20work%20frames%20an%20optimization%20problem%20in%20the%0Aspace%20of%20Gaussian%20distributions%20by%20solving%20a%20Variational%20Inference%20%28VI%29%20in%20the%0Apath%20distribution%20space.%20For%20linear-Gaussian%20stochastic%20dynamics%2C%20we%20propose%20a%0Aproximal%20algorithm%20to%20solve%20for%20an%20optimal%20Gaussian%20proposal%20iteratively.%20The%0Acomputational%20bottleneck%20is%20evaluating%20the%20gradients%20with%20respect%20to%20the%0Aproposal%20over%20a%20dense%20trajectory.%20We%20exploit%20the%20sparse%20motion%20planning%20factor%0Agraph%20and%20Gaussian%20Belief%20Propagation%20%28GBP%29%2C%20allowing%20for%20parallel%20computing%20of%0Athese%20gradients%20on%20Graphics%20Processing%20Units%20%28GPUs%29.%20We%20term%20the%20novel%20paradigm%0Aas%20the%20Parallel%20Gaussian%20Variational%20Inference%20Motion%20Planning%20%28P-GVIMP%29.%0ABuilding%20on%20the%20efficient%20algorithm%20for%20linear%20Gaussian%20systems%2C%20we%20then%0Apropose%20an%20iterative%20paradigm%20based%20on%20Statistical%20Linear%20Regression%20%28SLR%29%0Atechniques%20to%20solve%20motion%20planning%20for%20nonlinear%20stochastic%20systems%2C%20where%20the%0AP-GVIMP%20serves%20as%20a%20sub-routine%20for%20the%20linearized%20time-varying%20system.%20We%0Avalidate%20the%20proposed%20framework%20on%20various%20robotic%20systems%2C%20demonstrating%0Asignificant%20speed%20acceleration%20achieved%20by%20leveraging%20parallel%20computation%20and%0Asuccessful%20planning%20solutions%20for%20nonlinear%20systems%20under%20uncertainty.%20An%0Aopen-sourced%20implementation%20is%20presented%20at%20https%3A//github.com/hzyu17/VIMP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03416v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Iterative%2520Proximal%2520Variational%2520Inference%2520Motion%2520Planning%26entry.906535625%3DZinuo%2520Chang%2520and%2520Hongzhe%2520Yu%2520and%2520Patricio%2520Vela%2520and%2520Yongxin%2520Chen%26entry.1292438233%3D%2520%2520Motion%2520planning%2520under%2520uncertainty%2520can%2520be%2520cast%2520as%2520a%2520stochastic%2520optimal%2520control%250Aproblem%2520where%2520the%2520optimal%2520posterior%2520distribution%2520has%2520an%2520explicit%2520form.%2520To%250Aapproximate%2520this%2520posterior%252C%2520this%2520work%2520frames%2520an%2520optimization%2520problem%2520in%2520the%250Aspace%2520of%2520Gaussian%2520distributions%2520by%2520solving%2520a%2520Variational%2520Inference%2520%2528VI%2529%2520in%2520the%250Apath%2520distribution%2520space.%2520For%2520linear-Gaussian%2520stochastic%2520dynamics%252C%2520we%2520propose%2520a%250Aproximal%2520algorithm%2520to%2520solve%2520for%2520an%2520optimal%2520Gaussian%2520proposal%2520iteratively.%2520The%250Acomputational%2520bottleneck%2520is%2520evaluating%2520the%2520gradients%2520with%2520respect%2520to%2520the%250Aproposal%2520over%2520a%2520dense%2520trajectory.%2520We%2520exploit%2520the%2520sparse%2520motion%2520planning%2520factor%250Agraph%2520and%2520Gaussian%2520Belief%2520Propagation%2520%2528GBP%2529%252C%2520allowing%2520for%2520parallel%2520computing%2520of%250Athese%2520gradients%2520on%2520Graphics%2520Processing%2520Units%2520%2528GPUs%2529.%2520We%2520term%2520the%2520novel%2520paradigm%250Aas%2520the%2520Parallel%2520Gaussian%2520Variational%2520Inference%2520Motion%2520Planning%2520%2528P-GVIMP%2529.%250ABuilding%2520on%2520the%2520efficient%2520algorithm%2520for%2520linear%2520Gaussian%2520systems%252C%2520we%2520then%250Apropose%2520an%2520iterative%2520paradigm%2520based%2520on%2520Statistical%2520Linear%2520Regression%2520%2528SLR%2529%250Atechniques%2520to%2520solve%2520motion%2520planning%2520for%2520nonlinear%2520stochastic%2520systems%252C%2520where%2520the%250AP-GVIMP%2520serves%2520as%2520a%2520sub-routine%2520for%2520the%2520linearized%2520time-varying%2520system.%2520We%250Avalidate%2520the%2520proposed%2520framework%2520on%2520various%2520robotic%2520systems%252C%2520demonstrating%250Asignificant%2520speed%2520acceleration%2520achieved%2520by%2520leveraging%2520parallel%2520computation%2520and%250Asuccessful%2520planning%2520solutions%2520for%2520nonlinear%2520systems%2520under%2520uncertainty.%2520An%250Aopen-sourced%2520implementation%2520is%2520presented%2520at%2520https%253A//github.com/hzyu17/VIMP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03416v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Iterative%20Proximal%20Variational%20Inference%20Motion%20Planning&entry.906535625=Zinuo%20Chang%20and%20Hongzhe%20Yu%20and%20Patricio%20Vela%20and%20Yongxin%20Chen&entry.1292438233=%20%20Motion%20planning%20under%20uncertainty%20can%20be%20cast%20as%20a%20stochastic%20optimal%20control%0Aproblem%20where%20the%20optimal%20posterior%20distribution%20has%20an%20explicit%20form.%20To%0Aapproximate%20this%20posterior%2C%20this%20work%20frames%20an%20optimization%20problem%20in%20the%0Aspace%20of%20Gaussian%20distributions%20by%20solving%20a%20Variational%20Inference%20%28VI%29%20in%20the%0Apath%20distribution%20space.%20For%20linear-Gaussian%20stochastic%20dynamics%2C%20we%20propose%20a%0Aproximal%20algorithm%20to%20solve%20for%20an%20optimal%20Gaussian%20proposal%20iteratively.%20The%0Acomputational%20bottleneck%20is%20evaluating%20the%20gradients%20with%20respect%20to%20the%0Aproposal%20over%20a%20dense%20trajectory.%20We%20exploit%20the%20sparse%20motion%20planning%20factor%0Agraph%20and%20Gaussian%20Belief%20Propagation%20%28GBP%29%2C%20allowing%20for%20parallel%20computing%20of%0Athese%20gradients%20on%20Graphics%20Processing%20Units%20%28GPUs%29.%20We%20term%20the%20novel%20paradigm%0Aas%20the%20Parallel%20Gaussian%20Variational%20Inference%20Motion%20Planning%20%28P-GVIMP%29.%0ABuilding%20on%20the%20efficient%20algorithm%20for%20linear%20Gaussian%20systems%2C%20we%20then%0Apropose%20an%20iterative%20paradigm%20based%20on%20Statistical%20Linear%20Regression%20%28SLR%29%0Atechniques%20to%20solve%20motion%20planning%20for%20nonlinear%20stochastic%20systems%2C%20where%20the%0AP-GVIMP%20serves%20as%20a%20sub-routine%20for%20the%20linearized%20time-varying%20system.%20We%0Avalidate%20the%20proposed%20framework%20on%20various%20robotic%20systems%2C%20demonstrating%0Asignificant%20speed%20acceleration%20achieved%20by%20leveraging%20parallel%20computation%20and%0Asuccessful%20planning%20solutions%20for%20nonlinear%20systems%20under%20uncertainty.%20An%0Aopen-sourced%20implementation%20is%20presented%20at%20https%3A//github.com/hzyu17/VIMP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03416v3&entry.124074799=Read"},
{"title": "Evaluating Uncertainty in Deep Gaussian Processes", "author": "Matthijs van der Lende and Jeremias Lino Ferrao and Niclas M\u00fcller-Hof", "abstract": "  Reliable uncertainty estimates are crucial in modern machine learning. Deep\nGaussian Processes (DGPs) and Deep Sigma Point Processes (DSPPs) extend GPs\nhierarchically, offering promising methods for uncertainty quantification\ngrounded in Bayesian principles. However, their empirical calibration and\nrobustness under distribution shift relative to baselines like Deep Ensembles\nremain understudied. This work evaluates these models on regression (CASP\ndataset) and classification (ESR dataset) tasks, assessing predictive\nperformance (MAE, Accu- racy), calibration using Negative Log-Likelihood (NLL)\nand Expected Calibration Error (ECE), alongside robustness under various\nsynthetic feature-level distribution shifts. Results indicate DSPPs provide\nstrong in-distribution calibration leveraging their sigma point approximations.\nHowever, compared to Deep Ensembles, which demonstrated superior robustness in\nboth per- formance and calibration under the tested shifts, the GP-based\nmethods showed vulnerabilities, exhibiting particular sensitivity in the\nobserved metrics. Our findings underscore ensembles as a robust baseline,\nsuggesting that while deep GP methods offer good in-distribution calibration,\ntheir practical robustness under distribution shift requires careful\nevaluation. To facilitate reproducibility, we make our code available at\nhttps://github.com/matthjs/xai-gp.\n", "link": "http://arxiv.org/abs/2504.17719v1", "date": "2025-04-24", "relevancy": 2.1507, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5537}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5429}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5196}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Uncertainty%20in%20Deep%20Gaussian%20Processes&body=Title%3A%20Evaluating%20Uncertainty%20in%20Deep%20Gaussian%20Processes%0AAuthor%3A%20Matthijs%20van%20der%20Lende%20and%20Jeremias%20Lino%20Ferrao%20and%20Niclas%20M%C3%BCller-Hof%0AAbstract%3A%20%20%20Reliable%20uncertainty%20estimates%20are%20crucial%20in%20modern%20machine%20learning.%20Deep%0AGaussian%20Processes%20%28DGPs%29%20and%20Deep%20Sigma%20Point%20Processes%20%28DSPPs%29%20extend%20GPs%0Ahierarchically%2C%20offering%20promising%20methods%20for%20uncertainty%20quantification%0Agrounded%20in%20Bayesian%20principles.%20However%2C%20their%20empirical%20calibration%20and%0Arobustness%20under%20distribution%20shift%20relative%20to%20baselines%20like%20Deep%20Ensembles%0Aremain%20understudied.%20This%20work%20evaluates%20these%20models%20on%20regression%20%28CASP%0Adataset%29%20and%20classification%20%28ESR%20dataset%29%20tasks%2C%20assessing%20predictive%0Aperformance%20%28MAE%2C%20Accu-%20racy%29%2C%20calibration%20using%20Negative%20Log-Likelihood%20%28NLL%29%0Aand%20Expected%20Calibration%20Error%20%28ECE%29%2C%20alongside%20robustness%20under%20various%0Asynthetic%20feature-level%20distribution%20shifts.%20Results%20indicate%20DSPPs%20provide%0Astrong%20in-distribution%20calibration%20leveraging%20their%20sigma%20point%20approximations.%0AHowever%2C%20compared%20to%20Deep%20Ensembles%2C%20which%20demonstrated%20superior%20robustness%20in%0Aboth%20per-%20formance%20and%20calibration%20under%20the%20tested%20shifts%2C%20the%20GP-based%0Amethods%20showed%20vulnerabilities%2C%20exhibiting%20particular%20sensitivity%20in%20the%0Aobserved%20metrics.%20Our%20findings%20underscore%20ensembles%20as%20a%20robust%20baseline%2C%0Asuggesting%20that%20while%20deep%20GP%20methods%20offer%20good%20in-distribution%20calibration%2C%0Atheir%20practical%20robustness%20under%20distribution%20shift%20requires%20careful%0Aevaluation.%20To%20facilitate%20reproducibility%2C%20we%20make%20our%20code%20available%20at%0Ahttps%3A//github.com/matthjs/xai-gp.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17719v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Uncertainty%2520in%2520Deep%2520Gaussian%2520Processes%26entry.906535625%3DMatthijs%2520van%2520der%2520Lende%2520and%2520Jeremias%2520Lino%2520Ferrao%2520and%2520Niclas%2520M%25C3%25BCller-Hof%26entry.1292438233%3D%2520%2520Reliable%2520uncertainty%2520estimates%2520are%2520crucial%2520in%2520modern%2520machine%2520learning.%2520Deep%250AGaussian%2520Processes%2520%2528DGPs%2529%2520and%2520Deep%2520Sigma%2520Point%2520Processes%2520%2528DSPPs%2529%2520extend%2520GPs%250Ahierarchically%252C%2520offering%2520promising%2520methods%2520for%2520uncertainty%2520quantification%250Agrounded%2520in%2520Bayesian%2520principles.%2520However%252C%2520their%2520empirical%2520calibration%2520and%250Arobustness%2520under%2520distribution%2520shift%2520relative%2520to%2520baselines%2520like%2520Deep%2520Ensembles%250Aremain%2520understudied.%2520This%2520work%2520evaluates%2520these%2520models%2520on%2520regression%2520%2528CASP%250Adataset%2529%2520and%2520classification%2520%2528ESR%2520dataset%2529%2520tasks%252C%2520assessing%2520predictive%250Aperformance%2520%2528MAE%252C%2520Accu-%2520racy%2529%252C%2520calibration%2520using%2520Negative%2520Log-Likelihood%2520%2528NLL%2529%250Aand%2520Expected%2520Calibration%2520Error%2520%2528ECE%2529%252C%2520alongside%2520robustness%2520under%2520various%250Asynthetic%2520feature-level%2520distribution%2520shifts.%2520Results%2520indicate%2520DSPPs%2520provide%250Astrong%2520in-distribution%2520calibration%2520leveraging%2520their%2520sigma%2520point%2520approximations.%250AHowever%252C%2520compared%2520to%2520Deep%2520Ensembles%252C%2520which%2520demonstrated%2520superior%2520robustness%2520in%250Aboth%2520per-%2520formance%2520and%2520calibration%2520under%2520the%2520tested%2520shifts%252C%2520the%2520GP-based%250Amethods%2520showed%2520vulnerabilities%252C%2520exhibiting%2520particular%2520sensitivity%2520in%2520the%250Aobserved%2520metrics.%2520Our%2520findings%2520underscore%2520ensembles%2520as%2520a%2520robust%2520baseline%252C%250Asuggesting%2520that%2520while%2520deep%2520GP%2520methods%2520offer%2520good%2520in-distribution%2520calibration%252C%250Atheir%2520practical%2520robustness%2520under%2520distribution%2520shift%2520requires%2520careful%250Aevaluation.%2520To%2520facilitate%2520reproducibility%252C%2520we%2520make%2520our%2520code%2520available%2520at%250Ahttps%253A//github.com/matthjs/xai-gp.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17719v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Uncertainty%20in%20Deep%20Gaussian%20Processes&entry.906535625=Matthijs%20van%20der%20Lende%20and%20Jeremias%20Lino%20Ferrao%20and%20Niclas%20M%C3%BCller-Hof&entry.1292438233=%20%20Reliable%20uncertainty%20estimates%20are%20crucial%20in%20modern%20machine%20learning.%20Deep%0AGaussian%20Processes%20%28DGPs%29%20and%20Deep%20Sigma%20Point%20Processes%20%28DSPPs%29%20extend%20GPs%0Ahierarchically%2C%20offering%20promising%20methods%20for%20uncertainty%20quantification%0Agrounded%20in%20Bayesian%20principles.%20However%2C%20their%20empirical%20calibration%20and%0Arobustness%20under%20distribution%20shift%20relative%20to%20baselines%20like%20Deep%20Ensembles%0Aremain%20understudied.%20This%20work%20evaluates%20these%20models%20on%20regression%20%28CASP%0Adataset%29%20and%20classification%20%28ESR%20dataset%29%20tasks%2C%20assessing%20predictive%0Aperformance%20%28MAE%2C%20Accu-%20racy%29%2C%20calibration%20using%20Negative%20Log-Likelihood%20%28NLL%29%0Aand%20Expected%20Calibration%20Error%20%28ECE%29%2C%20alongside%20robustness%20under%20various%0Asynthetic%20feature-level%20distribution%20shifts.%20Results%20indicate%20DSPPs%20provide%0Astrong%20in-distribution%20calibration%20leveraging%20their%20sigma%20point%20approximations.%0AHowever%2C%20compared%20to%20Deep%20Ensembles%2C%20which%20demonstrated%20superior%20robustness%20in%0Aboth%20per-%20formance%20and%20calibration%20under%20the%20tested%20shifts%2C%20the%20GP-based%0Amethods%20showed%20vulnerabilities%2C%20exhibiting%20particular%20sensitivity%20in%20the%0Aobserved%20metrics.%20Our%20findings%20underscore%20ensembles%20as%20a%20robust%20baseline%2C%0Asuggesting%20that%20while%20deep%20GP%20methods%20offer%20good%20in-distribution%20calibration%2C%0Atheir%20practical%20robustness%20under%20distribution%20shift%20requires%20careful%0Aevaluation.%20To%20facilitate%20reproducibility%2C%20we%20make%20our%20code%20available%20at%0Ahttps%3A//github.com/matthjs/xai-gp.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17719v1&entry.124074799=Read"},
{"title": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs", "author": "Piotr Nawrot and Robert Li and Renjie Huang and Sebastian Ruder and Kelly Marchisio and Edoardo M. Ponti", "abstract": "  Sparse attention offers a promising strategy to extend long-context\ncapabilities in Transformer LLMs, yet its viability, its efficiency-accuracy\ntrade-offs, and systematic scaling studies remain unexplored. To address this\ngap, we perform a careful comparison of training-free sparse attention methods\nat varying model scales, sequence lengths, and sparsity levels on a diverse\ncollection of long-sequence tasks-including novel ones that rely on natural\nlanguage while remaining controllable and easy to evaluate. Based on our\nexperiments, we report a series of key findings: 1) an isoFLOPS analysis\nreveals that for very long sequences, larger and highly sparse models are\npreferable to smaller and dense ones. 2) The level of sparsity attainable while\nstatistically guaranteeing accuracy preservation is higher during decoding than\nprefilling, and correlates with model size in the former. 3) There is no clear\nstrategy that performs best across tasks and phases, with different units of\nsparsification or budget adaptivity needed for different scenarios. Even\nmoderate sparsity levels often result in significant performance degradation on\nat least one task, highlighting that sparse attention is not a universal\nsolution. 4) We introduce and validate novel scaling laws specifically tailored\nfor sparse attention, providing evidence that our findings are likely to hold\ntrue beyond our range of experiments. Through these insights, we demonstrate\nthat sparse attention is a key tool to enhance the capabilities of Transformer\nLLMs for processing longer sequences, but requires careful evaluation of\ntrade-offs for performance-sensitive applications.\n", "link": "http://arxiv.org/abs/2504.17768v1", "date": "2025-04-24", "relevancy": 2.149, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5586}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5403}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Sparse%20Frontier%3A%20Sparse%20Attention%20Trade-offs%20in%20Transformer%20LLMs&body=Title%3A%20The%20Sparse%20Frontier%3A%20Sparse%20Attention%20Trade-offs%20in%20Transformer%20LLMs%0AAuthor%3A%20Piotr%20Nawrot%20and%20Robert%20Li%20and%20Renjie%20Huang%20and%20Sebastian%20Ruder%20and%20Kelly%20Marchisio%20and%20Edoardo%20M.%20Ponti%0AAbstract%3A%20%20%20Sparse%20attention%20offers%20a%20promising%20strategy%20to%20extend%20long-context%0Acapabilities%20in%20Transformer%20LLMs%2C%20yet%20its%20viability%2C%20its%20efficiency-accuracy%0Atrade-offs%2C%20and%20systematic%20scaling%20studies%20remain%20unexplored.%20To%20address%20this%0Agap%2C%20we%20perform%20a%20careful%20comparison%20of%20training-free%20sparse%20attention%20methods%0Aat%20varying%20model%20scales%2C%20sequence%20lengths%2C%20and%20sparsity%20levels%20on%20a%20diverse%0Acollection%20of%20long-sequence%20tasks-including%20novel%20ones%20that%20rely%20on%20natural%0Alanguage%20while%20remaining%20controllable%20and%20easy%20to%20evaluate.%20Based%20on%20our%0Aexperiments%2C%20we%20report%20a%20series%20of%20key%20findings%3A%201%29%20an%20isoFLOPS%20analysis%0Areveals%20that%20for%20very%20long%20sequences%2C%20larger%20and%20highly%20sparse%20models%20are%0Apreferable%20to%20smaller%20and%20dense%20ones.%202%29%20The%20level%20of%20sparsity%20attainable%20while%0Astatistically%20guaranteeing%20accuracy%20preservation%20is%20higher%20during%20decoding%20than%0Aprefilling%2C%20and%20correlates%20with%20model%20size%20in%20the%20former.%203%29%20There%20is%20no%20clear%0Astrategy%20that%20performs%20best%20across%20tasks%20and%20phases%2C%20with%20different%20units%20of%0Asparsification%20or%20budget%20adaptivity%20needed%20for%20different%20scenarios.%20Even%0Amoderate%20sparsity%20levels%20often%20result%20in%20significant%20performance%20degradation%20on%0Aat%20least%20one%20task%2C%20highlighting%20that%20sparse%20attention%20is%20not%20a%20universal%0Asolution.%204%29%20We%20introduce%20and%20validate%20novel%20scaling%20laws%20specifically%20tailored%0Afor%20sparse%20attention%2C%20providing%20evidence%20that%20our%20findings%20are%20likely%20to%20hold%0Atrue%20beyond%20our%20range%20of%20experiments.%20Through%20these%20insights%2C%20we%20demonstrate%0Athat%20sparse%20attention%20is%20a%20key%20tool%20to%20enhance%20the%20capabilities%20of%20Transformer%0ALLMs%20for%20processing%20longer%20sequences%2C%20but%20requires%20careful%20evaluation%20of%0Atrade-offs%20for%20performance-sensitive%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Sparse%2520Frontier%253A%2520Sparse%2520Attention%2520Trade-offs%2520in%2520Transformer%2520LLMs%26entry.906535625%3DPiotr%2520Nawrot%2520and%2520Robert%2520Li%2520and%2520Renjie%2520Huang%2520and%2520Sebastian%2520Ruder%2520and%2520Kelly%2520Marchisio%2520and%2520Edoardo%2520M.%2520Ponti%26entry.1292438233%3D%2520%2520Sparse%2520attention%2520offers%2520a%2520promising%2520strategy%2520to%2520extend%2520long-context%250Acapabilities%2520in%2520Transformer%2520LLMs%252C%2520yet%2520its%2520viability%252C%2520its%2520efficiency-accuracy%250Atrade-offs%252C%2520and%2520systematic%2520scaling%2520studies%2520remain%2520unexplored.%2520To%2520address%2520this%250Agap%252C%2520we%2520perform%2520a%2520careful%2520comparison%2520of%2520training-free%2520sparse%2520attention%2520methods%250Aat%2520varying%2520model%2520scales%252C%2520sequence%2520lengths%252C%2520and%2520sparsity%2520levels%2520on%2520a%2520diverse%250Acollection%2520of%2520long-sequence%2520tasks-including%2520novel%2520ones%2520that%2520rely%2520on%2520natural%250Alanguage%2520while%2520remaining%2520controllable%2520and%2520easy%2520to%2520evaluate.%2520Based%2520on%2520our%250Aexperiments%252C%2520we%2520report%2520a%2520series%2520of%2520key%2520findings%253A%25201%2529%2520an%2520isoFLOPS%2520analysis%250Areveals%2520that%2520for%2520very%2520long%2520sequences%252C%2520larger%2520and%2520highly%2520sparse%2520models%2520are%250Apreferable%2520to%2520smaller%2520and%2520dense%2520ones.%25202%2529%2520The%2520level%2520of%2520sparsity%2520attainable%2520while%250Astatistically%2520guaranteeing%2520accuracy%2520preservation%2520is%2520higher%2520during%2520decoding%2520than%250Aprefilling%252C%2520and%2520correlates%2520with%2520model%2520size%2520in%2520the%2520former.%25203%2529%2520There%2520is%2520no%2520clear%250Astrategy%2520that%2520performs%2520best%2520across%2520tasks%2520and%2520phases%252C%2520with%2520different%2520units%2520of%250Asparsification%2520or%2520budget%2520adaptivity%2520needed%2520for%2520different%2520scenarios.%2520Even%250Amoderate%2520sparsity%2520levels%2520often%2520result%2520in%2520significant%2520performance%2520degradation%2520on%250Aat%2520least%2520one%2520task%252C%2520highlighting%2520that%2520sparse%2520attention%2520is%2520not%2520a%2520universal%250Asolution.%25204%2529%2520We%2520introduce%2520and%2520validate%2520novel%2520scaling%2520laws%2520specifically%2520tailored%250Afor%2520sparse%2520attention%252C%2520providing%2520evidence%2520that%2520our%2520findings%2520are%2520likely%2520to%2520hold%250Atrue%2520beyond%2520our%2520range%2520of%2520experiments.%2520Through%2520these%2520insights%252C%2520we%2520demonstrate%250Athat%2520sparse%2520attention%2520is%2520a%2520key%2520tool%2520to%2520enhance%2520the%2520capabilities%2520of%2520Transformer%250ALLMs%2520for%2520processing%2520longer%2520sequences%252C%2520but%2520requires%2520careful%2520evaluation%2520of%250Atrade-offs%2520for%2520performance-sensitive%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Sparse%20Frontier%3A%20Sparse%20Attention%20Trade-offs%20in%20Transformer%20LLMs&entry.906535625=Piotr%20Nawrot%20and%20Robert%20Li%20and%20Renjie%20Huang%20and%20Sebastian%20Ruder%20and%20Kelly%20Marchisio%20and%20Edoardo%20M.%20Ponti&entry.1292438233=%20%20Sparse%20attention%20offers%20a%20promising%20strategy%20to%20extend%20long-context%0Acapabilities%20in%20Transformer%20LLMs%2C%20yet%20its%20viability%2C%20its%20efficiency-accuracy%0Atrade-offs%2C%20and%20systematic%20scaling%20studies%20remain%20unexplored.%20To%20address%20this%0Agap%2C%20we%20perform%20a%20careful%20comparison%20of%20training-free%20sparse%20attention%20methods%0Aat%20varying%20model%20scales%2C%20sequence%20lengths%2C%20and%20sparsity%20levels%20on%20a%20diverse%0Acollection%20of%20long-sequence%20tasks-including%20novel%20ones%20that%20rely%20on%20natural%0Alanguage%20while%20remaining%20controllable%20and%20easy%20to%20evaluate.%20Based%20on%20our%0Aexperiments%2C%20we%20report%20a%20series%20of%20key%20findings%3A%201%29%20an%20isoFLOPS%20analysis%0Areveals%20that%20for%20very%20long%20sequences%2C%20larger%20and%20highly%20sparse%20models%20are%0Apreferable%20to%20smaller%20and%20dense%20ones.%202%29%20The%20level%20of%20sparsity%20attainable%20while%0Astatistically%20guaranteeing%20accuracy%20preservation%20is%20higher%20during%20decoding%20than%0Aprefilling%2C%20and%20correlates%20with%20model%20size%20in%20the%20former.%203%29%20There%20is%20no%20clear%0Astrategy%20that%20performs%20best%20across%20tasks%20and%20phases%2C%20with%20different%20units%20of%0Asparsification%20or%20budget%20adaptivity%20needed%20for%20different%20scenarios.%20Even%0Amoderate%20sparsity%20levels%20often%20result%20in%20significant%20performance%20degradation%20on%0Aat%20least%20one%20task%2C%20highlighting%20that%20sparse%20attention%20is%20not%20a%20universal%0Asolution.%204%29%20We%20introduce%20and%20validate%20novel%20scaling%20laws%20specifically%20tailored%0Afor%20sparse%20attention%2C%20providing%20evidence%20that%20our%20findings%20are%20likely%20to%20hold%0Atrue%20beyond%20our%20range%20of%20experiments.%20Through%20these%20insights%2C%20we%20demonstrate%0Athat%20sparse%20attention%20is%20a%20key%20tool%20to%20enhance%20the%20capabilities%20of%20Transformer%0ALLMs%20for%20processing%20longer%20sequences%2C%20but%20requires%20careful%20evaluation%20of%0Atrade-offs%20for%20performance-sensitive%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17768v1&entry.124074799=Read"},
{"title": "CLIPSE -- a minimalistic CLIP-based image search engine for research", "author": "Steve G\u00f6ring", "abstract": "  A brief overview of CLIPSE, a self-hosted image search engine with the main\napplication of research, is provided. In general, CLIPSE uses CLIP embeddings\nto process the images and also the text queries. The overall framework is\ndesigned with simplicity to enable easy extension and usage. Two benchmark\nscenarios are described and evaluated, covering indexing and querying time. It\nis shown that CLIPSE is capable of handling smaller datasets; for larger\ndatasets, a distributed approach with several instances should be considered.\n", "link": "http://arxiv.org/abs/2504.17643v1", "date": "2025-04-24", "relevancy": 2.147, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4311}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4286}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4286}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIPSE%20--%20a%20minimalistic%20CLIP-based%20image%20search%20engine%20for%20research&body=Title%3A%20CLIPSE%20--%20a%20minimalistic%20CLIP-based%20image%20search%20engine%20for%20research%0AAuthor%3A%20Steve%20G%C3%B6ring%0AAbstract%3A%20%20%20A%20brief%20overview%20of%20CLIPSE%2C%20a%20self-hosted%20image%20search%20engine%20with%20the%20main%0Aapplication%20of%20research%2C%20is%20provided.%20In%20general%2C%20CLIPSE%20uses%20CLIP%20embeddings%0Ato%20process%20the%20images%20and%20also%20the%20text%20queries.%20The%20overall%20framework%20is%0Adesigned%20with%20simplicity%20to%20enable%20easy%20extension%20and%20usage.%20Two%20benchmark%0Ascenarios%20are%20described%20and%20evaluated%2C%20covering%20indexing%20and%20querying%20time.%20It%0Ais%20shown%20that%20CLIPSE%20is%20capable%20of%20handling%20smaller%20datasets%3B%20for%20larger%0Adatasets%2C%20a%20distributed%20approach%20with%20several%20instances%20should%20be%20considered.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIPSE%2520--%2520a%2520minimalistic%2520CLIP-based%2520image%2520search%2520engine%2520for%2520research%26entry.906535625%3DSteve%2520G%25C3%25B6ring%26entry.1292438233%3D%2520%2520A%2520brief%2520overview%2520of%2520CLIPSE%252C%2520a%2520self-hosted%2520image%2520search%2520engine%2520with%2520the%2520main%250Aapplication%2520of%2520research%252C%2520is%2520provided.%2520In%2520general%252C%2520CLIPSE%2520uses%2520CLIP%2520embeddings%250Ato%2520process%2520the%2520images%2520and%2520also%2520the%2520text%2520queries.%2520The%2520overall%2520framework%2520is%250Adesigned%2520with%2520simplicity%2520to%2520enable%2520easy%2520extension%2520and%2520usage.%2520Two%2520benchmark%250Ascenarios%2520are%2520described%2520and%2520evaluated%252C%2520covering%2520indexing%2520and%2520querying%2520time.%2520It%250Ais%2520shown%2520that%2520CLIPSE%2520is%2520capable%2520of%2520handling%2520smaller%2520datasets%253B%2520for%2520larger%250Adatasets%252C%2520a%2520distributed%2520approach%2520with%2520several%2520instances%2520should%2520be%2520considered.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIPSE%20--%20a%20minimalistic%20CLIP-based%20image%20search%20engine%20for%20research&entry.906535625=Steve%20G%C3%B6ring&entry.1292438233=%20%20A%20brief%20overview%20of%20CLIPSE%2C%20a%20self-hosted%20image%20search%20engine%20with%20the%20main%0Aapplication%20of%20research%2C%20is%20provided.%20In%20general%2C%20CLIPSE%20uses%20CLIP%20embeddings%0Ato%20process%20the%20images%20and%20also%20the%20text%20queries.%20The%20overall%20framework%20is%0Adesigned%20with%20simplicity%20to%20enable%20easy%20extension%20and%20usage.%20Two%20benchmark%0Ascenarios%20are%20described%20and%20evaluated%2C%20covering%20indexing%20and%20querying%20time.%20It%0Ais%20shown%20that%20CLIPSE%20is%20capable%20of%20handling%20smaller%20datasets%3B%20for%20larger%0Adatasets%2C%20a%20distributed%20approach%20with%20several%20instances%20should%20be%20considered.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17643v1&entry.124074799=Read"},
{"title": "A Machine Learning Approach for Denoising and Upsampling HRTFs", "author": "Xuyi Hu and Jian Li and Lorenzo Picinali and Aidan O. T. Hogg", "abstract": "  The demand for realistic virtual immersive audio continues to grow, with\nHead-Related Transfer Functions (HRTFs) playing a key role. HRTFs capture how\nsound reaches our ears, reflecting unique anatomical features and enhancing\nspatial perception. It has been shown that personalized HRTFs improve\nlocalization accuracy, but their measurement remains time-consuming and\nrequires a noise-free environment. Although machine learning has been shown to\nreduce the required measurement points and, thus, the measurement time, a\ncontrolled environment is still necessary. This paper proposes a method to\naddress this constraint by presenting a novel technique that can upsample\nsparse, noisy HRTF measurements. The proposed approach combines an HRTF Denoisy\nU-Net for denoising and an Autoencoding Generative Adversarial Network (AE-GAN)\nfor upsampling from three measurement points. The proposed method achieves a\nlog-spectral distortion (LSD) error of 5.41 dB and a cosine similarity loss of\n0.0070, demonstrating the method's effectiveness in HRTF upsampling.\n", "link": "http://arxiv.org/abs/2504.17586v1", "date": "2025-04-24", "relevancy": 2.1387, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5594}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5211}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Machine%20Learning%20Approach%20for%20Denoising%20and%20Upsampling%20HRTFs&body=Title%3A%20A%20Machine%20Learning%20Approach%20for%20Denoising%20and%20Upsampling%20HRTFs%0AAuthor%3A%20Xuyi%20Hu%20and%20Jian%20Li%20and%20Lorenzo%20Picinali%20and%20Aidan%20O.%20T.%20Hogg%0AAbstract%3A%20%20%20The%20demand%20for%20realistic%20virtual%20immersive%20audio%20continues%20to%20grow%2C%20with%0AHead-Related%20Transfer%20Functions%20%28HRTFs%29%20playing%20a%20key%20role.%20HRTFs%20capture%20how%0Asound%20reaches%20our%20ears%2C%20reflecting%20unique%20anatomical%20features%20and%20enhancing%0Aspatial%20perception.%20It%20has%20been%20shown%20that%20personalized%20HRTFs%20improve%0Alocalization%20accuracy%2C%20but%20their%20measurement%20remains%20time-consuming%20and%0Arequires%20a%20noise-free%20environment.%20Although%20machine%20learning%20has%20been%20shown%20to%0Areduce%20the%20required%20measurement%20points%20and%2C%20thus%2C%20the%20measurement%20time%2C%20a%0Acontrolled%20environment%20is%20still%20necessary.%20This%20paper%20proposes%20a%20method%20to%0Aaddress%20this%20constraint%20by%20presenting%20a%20novel%20technique%20that%20can%20upsample%0Asparse%2C%20noisy%20HRTF%20measurements.%20The%20proposed%20approach%20combines%20an%20HRTF%20Denoisy%0AU-Net%20for%20denoising%20and%20an%20Autoencoding%20Generative%20Adversarial%20Network%20%28AE-GAN%29%0Afor%20upsampling%20from%20three%20measurement%20points.%20The%20proposed%20method%20achieves%20a%0Alog-spectral%20distortion%20%28LSD%29%20error%20of%205.41%20dB%20and%20a%20cosine%20similarity%20loss%20of%0A0.0070%2C%20demonstrating%20the%20method%27s%20effectiveness%20in%20HRTF%20upsampling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17586v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Machine%2520Learning%2520Approach%2520for%2520Denoising%2520and%2520Upsampling%2520HRTFs%26entry.906535625%3DXuyi%2520Hu%2520and%2520Jian%2520Li%2520and%2520Lorenzo%2520Picinali%2520and%2520Aidan%2520O.%2520T.%2520Hogg%26entry.1292438233%3D%2520%2520The%2520demand%2520for%2520realistic%2520virtual%2520immersive%2520audio%2520continues%2520to%2520grow%252C%2520with%250AHead-Related%2520Transfer%2520Functions%2520%2528HRTFs%2529%2520playing%2520a%2520key%2520role.%2520HRTFs%2520capture%2520how%250Asound%2520reaches%2520our%2520ears%252C%2520reflecting%2520unique%2520anatomical%2520features%2520and%2520enhancing%250Aspatial%2520perception.%2520It%2520has%2520been%2520shown%2520that%2520personalized%2520HRTFs%2520improve%250Alocalization%2520accuracy%252C%2520but%2520their%2520measurement%2520remains%2520time-consuming%2520and%250Arequires%2520a%2520noise-free%2520environment.%2520Although%2520machine%2520learning%2520has%2520been%2520shown%2520to%250Areduce%2520the%2520required%2520measurement%2520points%2520and%252C%2520thus%252C%2520the%2520measurement%2520time%252C%2520a%250Acontrolled%2520environment%2520is%2520still%2520necessary.%2520This%2520paper%2520proposes%2520a%2520method%2520to%250Aaddress%2520this%2520constraint%2520by%2520presenting%2520a%2520novel%2520technique%2520that%2520can%2520upsample%250Asparse%252C%2520noisy%2520HRTF%2520measurements.%2520The%2520proposed%2520approach%2520combines%2520an%2520HRTF%2520Denoisy%250AU-Net%2520for%2520denoising%2520and%2520an%2520Autoencoding%2520Generative%2520Adversarial%2520Network%2520%2528AE-GAN%2529%250Afor%2520upsampling%2520from%2520three%2520measurement%2520points.%2520The%2520proposed%2520method%2520achieves%2520a%250Alog-spectral%2520distortion%2520%2528LSD%2529%2520error%2520of%25205.41%2520dB%2520and%2520a%2520cosine%2520similarity%2520loss%2520of%250A0.0070%252C%2520demonstrating%2520the%2520method%2527s%2520effectiveness%2520in%2520HRTF%2520upsampling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17586v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Machine%20Learning%20Approach%20for%20Denoising%20and%20Upsampling%20HRTFs&entry.906535625=Xuyi%20Hu%20and%20Jian%20Li%20and%20Lorenzo%20Picinali%20and%20Aidan%20O.%20T.%20Hogg&entry.1292438233=%20%20The%20demand%20for%20realistic%20virtual%20immersive%20audio%20continues%20to%20grow%2C%20with%0AHead-Related%20Transfer%20Functions%20%28HRTFs%29%20playing%20a%20key%20role.%20HRTFs%20capture%20how%0Asound%20reaches%20our%20ears%2C%20reflecting%20unique%20anatomical%20features%20and%20enhancing%0Aspatial%20perception.%20It%20has%20been%20shown%20that%20personalized%20HRTFs%20improve%0Alocalization%20accuracy%2C%20but%20their%20measurement%20remains%20time-consuming%20and%0Arequires%20a%20noise-free%20environment.%20Although%20machine%20learning%20has%20been%20shown%20to%0Areduce%20the%20required%20measurement%20points%20and%2C%20thus%2C%20the%20measurement%20time%2C%20a%0Acontrolled%20environment%20is%20still%20necessary.%20This%20paper%20proposes%20a%20method%20to%0Aaddress%20this%20constraint%20by%20presenting%20a%20novel%20technique%20that%20can%20upsample%0Asparse%2C%20noisy%20HRTF%20measurements.%20The%20proposed%20approach%20combines%20an%20HRTF%20Denoisy%0AU-Net%20for%20denoising%20and%20an%20Autoencoding%20Generative%20Adversarial%20Network%20%28AE-GAN%29%0Afor%20upsampling%20from%20three%20measurement%20points.%20The%20proposed%20method%20achieves%20a%0Alog-spectral%20distortion%20%28LSD%29%20error%20of%205.41%20dB%20and%20a%20cosine%20similarity%20loss%20of%0A0.0070%2C%20demonstrating%20the%20method%27s%20effectiveness%20in%20HRTF%20upsampling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17586v1&entry.124074799=Read"},
{"title": "Mamba-Sea: A Mamba-based Framework with Global-to-Local Sequence\n  Augmentation for Generalizable Medical Image Segmentation", "author": "Zihan Cheng and Jintao Guo and Jian Zhang and Lei Qi and Luping Zhou and Yinghuan Shi and Yang Gao", "abstract": "  To segment medical images with distribution shifts, domain generalization\n(DG) has emerged as a promising setting to train models on source domains that\ncan generalize to unseen target domains. Existing DG methods are mainly based\non CNN or ViT architectures. Recently, advanced state space models, represented\nby Mamba, have shown promising results in various supervised medical image\nsegmentation. The success of Mamba is primarily owing to its ability to capture\nlong-range dependencies while keeping linear complexity with input sequence\nlength, making it a promising alternative to CNNs and ViTs. Inspired by the\nsuccess, in the paper, we explore the potential of the Mamba architecture to\naddress distribution shifts in DG for medical image segmentation. Specifically,\nwe propose a novel Mamba-based framework, Mamba-Sea, incorporating\nglobal-to-local sequence augmentation to improve the model's generalizability\nunder domain shift issues. Our Mamba-Sea introduces a global augmentation\nmechanism designed to simulate potential variations in appearance across\ndifferent sites, aiming to suppress the model's learning of domain-specific\ninformation. At the local level, we propose a sequence-wise augmentation along\ninput sequences, which perturbs the style of tokens within random continuous\nsub-sequences by modeling and resampling style statistics associated with\ndomain shifts. To our best knowledge, Mamba-Sea is the first work to explore\nthe generalization of Mamba for medical image segmentation, providing an\nadvanced and promising Mamba-based architecture with strong robustness to\ndomain shifts. Remarkably, our proposed method is the first to surpass a Dice\ncoefficient of 90% on the Prostate dataset, which exceeds previous SOTA of\n88.61%. The code is available at https://github.com/orange-czh/Mamba-Sea.\n", "link": "http://arxiv.org/abs/2504.17515v1", "date": "2025-04-24", "relevancy": 2.1368, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5412}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5314}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mamba-Sea%3A%20A%20Mamba-based%20Framework%20with%20Global-to-Local%20Sequence%0A%20%20Augmentation%20for%20Generalizable%20Medical%20Image%20Segmentation&body=Title%3A%20Mamba-Sea%3A%20A%20Mamba-based%20Framework%20with%20Global-to-Local%20Sequence%0A%20%20Augmentation%20for%20Generalizable%20Medical%20Image%20Segmentation%0AAuthor%3A%20Zihan%20Cheng%20and%20Jintao%20Guo%20and%20Jian%20Zhang%20and%20Lei%20Qi%20and%20Luping%20Zhou%20and%20Yinghuan%20Shi%20and%20Yang%20Gao%0AAbstract%3A%20%20%20To%20segment%20medical%20images%20with%20distribution%20shifts%2C%20domain%20generalization%0A%28DG%29%20has%20emerged%20as%20a%20promising%20setting%20to%20train%20models%20on%20source%20domains%20that%0Acan%20generalize%20to%20unseen%20target%20domains.%20Existing%20DG%20methods%20are%20mainly%20based%0Aon%20CNN%20or%20ViT%20architectures.%20Recently%2C%20advanced%20state%20space%20models%2C%20represented%0Aby%20Mamba%2C%20have%20shown%20promising%20results%20in%20various%20supervised%20medical%20image%0Asegmentation.%20The%20success%20of%20Mamba%20is%20primarily%20owing%20to%20its%20ability%20to%20capture%0Along-range%20dependencies%20while%20keeping%20linear%20complexity%20with%20input%20sequence%0Alength%2C%20making%20it%20a%20promising%20alternative%20to%20CNNs%20and%20ViTs.%20Inspired%20by%20the%0Asuccess%2C%20in%20the%20paper%2C%20we%20explore%20the%20potential%20of%20the%20Mamba%20architecture%20to%0Aaddress%20distribution%20shifts%20in%20DG%20for%20medical%20image%20segmentation.%20Specifically%2C%0Awe%20propose%20a%20novel%20Mamba-based%20framework%2C%20Mamba-Sea%2C%20incorporating%0Aglobal-to-local%20sequence%20augmentation%20to%20improve%20the%20model%27s%20generalizability%0Aunder%20domain%20shift%20issues.%20Our%20Mamba-Sea%20introduces%20a%20global%20augmentation%0Amechanism%20designed%20to%20simulate%20potential%20variations%20in%20appearance%20across%0Adifferent%20sites%2C%20aiming%20to%20suppress%20the%20model%27s%20learning%20of%20domain-specific%0Ainformation.%20At%20the%20local%20level%2C%20we%20propose%20a%20sequence-wise%20augmentation%20along%0Ainput%20sequences%2C%20which%20perturbs%20the%20style%20of%20tokens%20within%20random%20continuous%0Asub-sequences%20by%20modeling%20and%20resampling%20style%20statistics%20associated%20with%0Adomain%20shifts.%20To%20our%20best%20knowledge%2C%20Mamba-Sea%20is%20the%20first%20work%20to%20explore%0Athe%20generalization%20of%20Mamba%20for%20medical%20image%20segmentation%2C%20providing%20an%0Aadvanced%20and%20promising%20Mamba-based%20architecture%20with%20strong%20robustness%20to%0Adomain%20shifts.%20Remarkably%2C%20our%20proposed%20method%20is%20the%20first%20to%20surpass%20a%20Dice%0Acoefficient%20of%2090%25%20on%20the%20Prostate%20dataset%2C%20which%20exceeds%20previous%20SOTA%20of%0A88.61%25.%20The%20code%20is%20available%20at%20https%3A//github.com/orange-czh/Mamba-Sea.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17515v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMamba-Sea%253A%2520A%2520Mamba-based%2520Framework%2520with%2520Global-to-Local%2520Sequence%250A%2520%2520Augmentation%2520for%2520Generalizable%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DZihan%2520Cheng%2520and%2520Jintao%2520Guo%2520and%2520Jian%2520Zhang%2520and%2520Lei%2520Qi%2520and%2520Luping%2520Zhou%2520and%2520Yinghuan%2520Shi%2520and%2520Yang%2520Gao%26entry.1292438233%3D%2520%2520To%2520segment%2520medical%2520images%2520with%2520distribution%2520shifts%252C%2520domain%2520generalization%250A%2528DG%2529%2520has%2520emerged%2520as%2520a%2520promising%2520setting%2520to%2520train%2520models%2520on%2520source%2520domains%2520that%250Acan%2520generalize%2520to%2520unseen%2520target%2520domains.%2520Existing%2520DG%2520methods%2520are%2520mainly%2520based%250Aon%2520CNN%2520or%2520ViT%2520architectures.%2520Recently%252C%2520advanced%2520state%2520space%2520models%252C%2520represented%250Aby%2520Mamba%252C%2520have%2520shown%2520promising%2520results%2520in%2520various%2520supervised%2520medical%2520image%250Asegmentation.%2520The%2520success%2520of%2520Mamba%2520is%2520primarily%2520owing%2520to%2520its%2520ability%2520to%2520capture%250Along-range%2520dependencies%2520while%2520keeping%2520linear%2520complexity%2520with%2520input%2520sequence%250Alength%252C%2520making%2520it%2520a%2520promising%2520alternative%2520to%2520CNNs%2520and%2520ViTs.%2520Inspired%2520by%2520the%250Asuccess%252C%2520in%2520the%2520paper%252C%2520we%2520explore%2520the%2520potential%2520of%2520the%2520Mamba%2520architecture%2520to%250Aaddress%2520distribution%2520shifts%2520in%2520DG%2520for%2520medical%2520image%2520segmentation.%2520Specifically%252C%250Awe%2520propose%2520a%2520novel%2520Mamba-based%2520framework%252C%2520Mamba-Sea%252C%2520incorporating%250Aglobal-to-local%2520sequence%2520augmentation%2520to%2520improve%2520the%2520model%2527s%2520generalizability%250Aunder%2520domain%2520shift%2520issues.%2520Our%2520Mamba-Sea%2520introduces%2520a%2520global%2520augmentation%250Amechanism%2520designed%2520to%2520simulate%2520potential%2520variations%2520in%2520appearance%2520across%250Adifferent%2520sites%252C%2520aiming%2520to%2520suppress%2520the%2520model%2527s%2520learning%2520of%2520domain-specific%250Ainformation.%2520At%2520the%2520local%2520level%252C%2520we%2520propose%2520a%2520sequence-wise%2520augmentation%2520along%250Ainput%2520sequences%252C%2520which%2520perturbs%2520the%2520style%2520of%2520tokens%2520within%2520random%2520continuous%250Asub-sequences%2520by%2520modeling%2520and%2520resampling%2520style%2520statistics%2520associated%2520with%250Adomain%2520shifts.%2520To%2520our%2520best%2520knowledge%252C%2520Mamba-Sea%2520is%2520the%2520first%2520work%2520to%2520explore%250Athe%2520generalization%2520of%2520Mamba%2520for%2520medical%2520image%2520segmentation%252C%2520providing%2520an%250Aadvanced%2520and%2520promising%2520Mamba-based%2520architecture%2520with%2520strong%2520robustness%2520to%250Adomain%2520shifts.%2520Remarkably%252C%2520our%2520proposed%2520method%2520is%2520the%2520first%2520to%2520surpass%2520a%2520Dice%250Acoefficient%2520of%252090%2525%2520on%2520the%2520Prostate%2520dataset%252C%2520which%2520exceeds%2520previous%2520SOTA%2520of%250A88.61%2525.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/orange-czh/Mamba-Sea.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17515v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mamba-Sea%3A%20A%20Mamba-based%20Framework%20with%20Global-to-Local%20Sequence%0A%20%20Augmentation%20for%20Generalizable%20Medical%20Image%20Segmentation&entry.906535625=Zihan%20Cheng%20and%20Jintao%20Guo%20and%20Jian%20Zhang%20and%20Lei%20Qi%20and%20Luping%20Zhou%20and%20Yinghuan%20Shi%20and%20Yang%20Gao&entry.1292438233=%20%20To%20segment%20medical%20images%20with%20distribution%20shifts%2C%20domain%20generalization%0A%28DG%29%20has%20emerged%20as%20a%20promising%20setting%20to%20train%20models%20on%20source%20domains%20that%0Acan%20generalize%20to%20unseen%20target%20domains.%20Existing%20DG%20methods%20are%20mainly%20based%0Aon%20CNN%20or%20ViT%20architectures.%20Recently%2C%20advanced%20state%20space%20models%2C%20represented%0Aby%20Mamba%2C%20have%20shown%20promising%20results%20in%20various%20supervised%20medical%20image%0Asegmentation.%20The%20success%20of%20Mamba%20is%20primarily%20owing%20to%20its%20ability%20to%20capture%0Along-range%20dependencies%20while%20keeping%20linear%20complexity%20with%20input%20sequence%0Alength%2C%20making%20it%20a%20promising%20alternative%20to%20CNNs%20and%20ViTs.%20Inspired%20by%20the%0Asuccess%2C%20in%20the%20paper%2C%20we%20explore%20the%20potential%20of%20the%20Mamba%20architecture%20to%0Aaddress%20distribution%20shifts%20in%20DG%20for%20medical%20image%20segmentation.%20Specifically%2C%0Awe%20propose%20a%20novel%20Mamba-based%20framework%2C%20Mamba-Sea%2C%20incorporating%0Aglobal-to-local%20sequence%20augmentation%20to%20improve%20the%20model%27s%20generalizability%0Aunder%20domain%20shift%20issues.%20Our%20Mamba-Sea%20introduces%20a%20global%20augmentation%0Amechanism%20designed%20to%20simulate%20potential%20variations%20in%20appearance%20across%0Adifferent%20sites%2C%20aiming%20to%20suppress%20the%20model%27s%20learning%20of%20domain-specific%0Ainformation.%20At%20the%20local%20level%2C%20we%20propose%20a%20sequence-wise%20augmentation%20along%0Ainput%20sequences%2C%20which%20perturbs%20the%20style%20of%20tokens%20within%20random%20continuous%0Asub-sequences%20by%20modeling%20and%20resampling%20style%20statistics%20associated%20with%0Adomain%20shifts.%20To%20our%20best%20knowledge%2C%20Mamba-Sea%20is%20the%20first%20work%20to%20explore%0Athe%20generalization%20of%20Mamba%20for%20medical%20image%20segmentation%2C%20providing%20an%0Aadvanced%20and%20promising%20Mamba-based%20architecture%20with%20strong%20robustness%20to%0Adomain%20shifts.%20Remarkably%2C%20our%20proposed%20method%20is%20the%20first%20to%20surpass%20a%20Dice%0Acoefficient%20of%2090%25%20on%20the%20Prostate%20dataset%2C%20which%20exceeds%20previous%20SOTA%20of%0A88.61%25.%20The%20code%20is%20available%20at%20https%3A//github.com/orange-czh/Mamba-Sea.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17515v1&entry.124074799=Read"},
{"title": "polyGen: A Learning Framework for Atomic-level Polymer Structure\n  Generation", "author": "Ayush Jain and Rampi Ramprasad", "abstract": "  Synthetic polymeric materials underpin fundamental technologies in the\nenergy, electronics, consumer goods, and medical sectors, yet their development\nstill suffers from prolonged design timelines. Although polymer informatics\ntools have supported speedup, polymer simulation protocols continue to face\nsignificant challenges: on-demand generation of realistic 3D atomic structures\nthat respect the conformational diversity of polymer structures. Generative\nalgorithms for 3D structures of inorganic crystals, bio-polymers, and small\nmolecules exist, but have not addressed synthetic polymers. In this work, we\nintroduce polyGen, the first latent diffusion model designed specifically to\ngenerate realistic polymer structures from minimal inputs such as the repeat\nunit chemistry alone, leveraging a molecular encoding that captures polymer\nconnectivity throughout the architecture. Due to a scarce dataset of only 3855\nDFT-optimized polymer structures, we augment our training with DFT-optimized\nmolecular structures, showing improvement in joint learning between similar\nchemical structures. We also establish structure matching criteria to benchmark\nour approach on this novel problem. polyGen effectively generates diverse\nconformations of both linear chains and complex branched structures, though its\nperformance decreases when handling repeat units with a high atom count. Given\nthese initial results, polyGen represents a paradigm shift in atomic-level\nstructure generation for polymer science-the first proof-of-concept for\npredicting realistic atomic-level polymer conformations while accounting for\ntheir intrinsic structural flexibility.\n", "link": "http://arxiv.org/abs/2504.17656v1", "date": "2025-04-24", "relevancy": 2.1342, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5515}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.538}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20polyGen%3A%20A%20Learning%20Framework%20for%20Atomic-level%20Polymer%20Structure%0A%20%20Generation&body=Title%3A%20polyGen%3A%20A%20Learning%20Framework%20for%20Atomic-level%20Polymer%20Structure%0A%20%20Generation%0AAuthor%3A%20Ayush%20Jain%20and%20Rampi%20Ramprasad%0AAbstract%3A%20%20%20Synthetic%20polymeric%20materials%20underpin%20fundamental%20technologies%20in%20the%0Aenergy%2C%20electronics%2C%20consumer%20goods%2C%20and%20medical%20sectors%2C%20yet%20their%20development%0Astill%20suffers%20from%20prolonged%20design%20timelines.%20Although%20polymer%20informatics%0Atools%20have%20supported%20speedup%2C%20polymer%20simulation%20protocols%20continue%20to%20face%0Asignificant%20challenges%3A%20on-demand%20generation%20of%20realistic%203D%20atomic%20structures%0Athat%20respect%20the%20conformational%20diversity%20of%20polymer%20structures.%20Generative%0Aalgorithms%20for%203D%20structures%20of%20inorganic%20crystals%2C%20bio-polymers%2C%20and%20small%0Amolecules%20exist%2C%20but%20have%20not%20addressed%20synthetic%20polymers.%20In%20this%20work%2C%20we%0Aintroduce%20polyGen%2C%20the%20first%20latent%20diffusion%20model%20designed%20specifically%20to%0Agenerate%20realistic%20polymer%20structures%20from%20minimal%20inputs%20such%20as%20the%20repeat%0Aunit%20chemistry%20alone%2C%20leveraging%20a%20molecular%20encoding%20that%20captures%20polymer%0Aconnectivity%20throughout%20the%20architecture.%20Due%20to%20a%20scarce%20dataset%20of%20only%203855%0ADFT-optimized%20polymer%20structures%2C%20we%20augment%20our%20training%20with%20DFT-optimized%0Amolecular%20structures%2C%20showing%20improvement%20in%20joint%20learning%20between%20similar%0Achemical%20structures.%20We%20also%20establish%20structure%20matching%20criteria%20to%20benchmark%0Aour%20approach%20on%20this%20novel%20problem.%20polyGen%20effectively%20generates%20diverse%0Aconformations%20of%20both%20linear%20chains%20and%20complex%20branched%20structures%2C%20though%20its%0Aperformance%20decreases%20when%20handling%20repeat%20units%20with%20a%20high%20atom%20count.%20Given%0Athese%20initial%20results%2C%20polyGen%20represents%20a%20paradigm%20shift%20in%20atomic-level%0Astructure%20generation%20for%20polymer%20science-the%20first%20proof-of-concept%20for%0Apredicting%20realistic%20atomic-level%20polymer%20conformations%20while%20accounting%20for%0Atheir%20intrinsic%20structural%20flexibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17656v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DpolyGen%253A%2520A%2520Learning%2520Framework%2520for%2520Atomic-level%2520Polymer%2520Structure%250A%2520%2520Generation%26entry.906535625%3DAyush%2520Jain%2520and%2520Rampi%2520Ramprasad%26entry.1292438233%3D%2520%2520Synthetic%2520polymeric%2520materials%2520underpin%2520fundamental%2520technologies%2520in%2520the%250Aenergy%252C%2520electronics%252C%2520consumer%2520goods%252C%2520and%2520medical%2520sectors%252C%2520yet%2520their%2520development%250Astill%2520suffers%2520from%2520prolonged%2520design%2520timelines.%2520Although%2520polymer%2520informatics%250Atools%2520have%2520supported%2520speedup%252C%2520polymer%2520simulation%2520protocols%2520continue%2520to%2520face%250Asignificant%2520challenges%253A%2520on-demand%2520generation%2520of%2520realistic%25203D%2520atomic%2520structures%250Athat%2520respect%2520the%2520conformational%2520diversity%2520of%2520polymer%2520structures.%2520Generative%250Aalgorithms%2520for%25203D%2520structures%2520of%2520inorganic%2520crystals%252C%2520bio-polymers%252C%2520and%2520small%250Amolecules%2520exist%252C%2520but%2520have%2520not%2520addressed%2520synthetic%2520polymers.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520polyGen%252C%2520the%2520first%2520latent%2520diffusion%2520model%2520designed%2520specifically%2520to%250Agenerate%2520realistic%2520polymer%2520structures%2520from%2520minimal%2520inputs%2520such%2520as%2520the%2520repeat%250Aunit%2520chemistry%2520alone%252C%2520leveraging%2520a%2520molecular%2520encoding%2520that%2520captures%2520polymer%250Aconnectivity%2520throughout%2520the%2520architecture.%2520Due%2520to%2520a%2520scarce%2520dataset%2520of%2520only%25203855%250ADFT-optimized%2520polymer%2520structures%252C%2520we%2520augment%2520our%2520training%2520with%2520DFT-optimized%250Amolecular%2520structures%252C%2520showing%2520improvement%2520in%2520joint%2520learning%2520between%2520similar%250Achemical%2520structures.%2520We%2520also%2520establish%2520structure%2520matching%2520criteria%2520to%2520benchmark%250Aour%2520approach%2520on%2520this%2520novel%2520problem.%2520polyGen%2520effectively%2520generates%2520diverse%250Aconformations%2520of%2520both%2520linear%2520chains%2520and%2520complex%2520branched%2520structures%252C%2520though%2520its%250Aperformance%2520decreases%2520when%2520handling%2520repeat%2520units%2520with%2520a%2520high%2520atom%2520count.%2520Given%250Athese%2520initial%2520results%252C%2520polyGen%2520represents%2520a%2520paradigm%2520shift%2520in%2520atomic-level%250Astructure%2520generation%2520for%2520polymer%2520science-the%2520first%2520proof-of-concept%2520for%250Apredicting%2520realistic%2520atomic-level%2520polymer%2520conformations%2520while%2520accounting%2520for%250Atheir%2520intrinsic%2520structural%2520flexibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17656v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=polyGen%3A%20A%20Learning%20Framework%20for%20Atomic-level%20Polymer%20Structure%0A%20%20Generation&entry.906535625=Ayush%20Jain%20and%20Rampi%20Ramprasad&entry.1292438233=%20%20Synthetic%20polymeric%20materials%20underpin%20fundamental%20technologies%20in%20the%0Aenergy%2C%20electronics%2C%20consumer%20goods%2C%20and%20medical%20sectors%2C%20yet%20their%20development%0Astill%20suffers%20from%20prolonged%20design%20timelines.%20Although%20polymer%20informatics%0Atools%20have%20supported%20speedup%2C%20polymer%20simulation%20protocols%20continue%20to%20face%0Asignificant%20challenges%3A%20on-demand%20generation%20of%20realistic%203D%20atomic%20structures%0Athat%20respect%20the%20conformational%20diversity%20of%20polymer%20structures.%20Generative%0Aalgorithms%20for%203D%20structures%20of%20inorganic%20crystals%2C%20bio-polymers%2C%20and%20small%0Amolecules%20exist%2C%20but%20have%20not%20addressed%20synthetic%20polymers.%20In%20this%20work%2C%20we%0Aintroduce%20polyGen%2C%20the%20first%20latent%20diffusion%20model%20designed%20specifically%20to%0Agenerate%20realistic%20polymer%20structures%20from%20minimal%20inputs%20such%20as%20the%20repeat%0Aunit%20chemistry%20alone%2C%20leveraging%20a%20molecular%20encoding%20that%20captures%20polymer%0Aconnectivity%20throughout%20the%20architecture.%20Due%20to%20a%20scarce%20dataset%20of%20only%203855%0ADFT-optimized%20polymer%20structures%2C%20we%20augment%20our%20training%20with%20DFT-optimized%0Amolecular%20structures%2C%20showing%20improvement%20in%20joint%20learning%20between%20similar%0Achemical%20structures.%20We%20also%20establish%20structure%20matching%20criteria%20to%20benchmark%0Aour%20approach%20on%20this%20novel%20problem.%20polyGen%20effectively%20generates%20diverse%0Aconformations%20of%20both%20linear%20chains%20and%20complex%20branched%20structures%2C%20though%20its%0Aperformance%20decreases%20when%20handling%20repeat%20units%20with%20a%20high%20atom%20count.%20Given%0Athese%20initial%20results%2C%20polyGen%20represents%20a%20paradigm%20shift%20in%20atomic-level%0Astructure%20generation%20for%20polymer%20science-the%20first%20proof-of-concept%20for%0Apredicting%20realistic%20atomic-level%20polymer%20conformations%20while%20accounting%20for%0Atheir%20intrinsic%20structural%20flexibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17656v1&entry.124074799=Read"},
{"title": "Deployment-friendly Lane-changing Intention Prediction Powered by\n  Brain-inspired Spiking Neural Networks", "author": "Shuqi Shen and Junjie Yang and Hui Zhong and Hongliang Lu and Xinhu Zheng and Hai Yang", "abstract": "  Accurate and real-time prediction of surrounding vehicles' lane-changing\nintentions is a critical challenge in deploying safe and efficient autonomous\ndriving systems in open-world scenarios. Existing high-performing methods\nremain hard to deploy due to their high computational cost, long training\ntimes, and excessive memory requirements. Here, we propose an efficient\nlane-changing intention prediction approach based on brain-inspired Spiking\nNeural Networks (SNN). By leveraging the event-driven nature of SNN, the\nproposed approach enables us to encode the vehicle's states in a more efficient\nmanner. Comparison experiments conducted on HighD and NGSIM datasets\ndemonstrate that our method significantly improves training efficiency and\nreduces deployment costs while maintaining comparable prediction accuracy.\nParticularly, compared to the baseline, our approach reduces training time by\n75% and memory usage by 99.9%. These results validate the efficiency and\nreliability of our method in lane-changing predictions, highlighting its\npotential for safe and efficient autonomous driving systems while offering\nsignificant advantages in deployment, including reduced training time, lower\nmemory usage, and faster inference.\n", "link": "http://arxiv.org/abs/2502.08659v3", "date": "2025-04-24", "relevancy": 2.1264, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5619}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5365}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deployment-friendly%20Lane-changing%20Intention%20Prediction%20Powered%20by%0A%20%20Brain-inspired%20Spiking%20Neural%20Networks&body=Title%3A%20Deployment-friendly%20Lane-changing%20Intention%20Prediction%20Powered%20by%0A%20%20Brain-inspired%20Spiking%20Neural%20Networks%0AAuthor%3A%20Shuqi%20Shen%20and%20Junjie%20Yang%20and%20Hui%20Zhong%20and%20Hongliang%20Lu%20and%20Xinhu%20Zheng%20and%20Hai%20Yang%0AAbstract%3A%20%20%20Accurate%20and%20real-time%20prediction%20of%20surrounding%20vehicles%27%20lane-changing%0Aintentions%20is%20a%20critical%20challenge%20in%20deploying%20safe%20and%20efficient%20autonomous%0Adriving%20systems%20in%20open-world%20scenarios.%20Existing%20high-performing%20methods%0Aremain%20hard%20to%20deploy%20due%20to%20their%20high%20computational%20cost%2C%20long%20training%0Atimes%2C%20and%20excessive%20memory%20requirements.%20Here%2C%20we%20propose%20an%20efficient%0Alane-changing%20intention%20prediction%20approach%20based%20on%20brain-inspired%20Spiking%0ANeural%20Networks%20%28SNN%29.%20By%20leveraging%20the%20event-driven%20nature%20of%20SNN%2C%20the%0Aproposed%20approach%20enables%20us%20to%20encode%20the%20vehicle%27s%20states%20in%20a%20more%20efficient%0Amanner.%20Comparison%20experiments%20conducted%20on%20HighD%20and%20NGSIM%20datasets%0Ademonstrate%20that%20our%20method%20significantly%20improves%20training%20efficiency%20and%0Areduces%20deployment%20costs%20while%20maintaining%20comparable%20prediction%20accuracy.%0AParticularly%2C%20compared%20to%20the%20baseline%2C%20our%20approach%20reduces%20training%20time%20by%0A75%25%20and%20memory%20usage%20by%2099.9%25.%20These%20results%20validate%20the%20efficiency%20and%0Areliability%20of%20our%20method%20in%20lane-changing%20predictions%2C%20highlighting%20its%0Apotential%20for%20safe%20and%20efficient%20autonomous%20driving%20systems%20while%20offering%0Asignificant%20advantages%20in%20deployment%2C%20including%20reduced%20training%20time%2C%20lower%0Amemory%20usage%2C%20and%20faster%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08659v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeployment-friendly%2520Lane-changing%2520Intention%2520Prediction%2520Powered%2520by%250A%2520%2520Brain-inspired%2520Spiking%2520Neural%2520Networks%26entry.906535625%3DShuqi%2520Shen%2520and%2520Junjie%2520Yang%2520and%2520Hui%2520Zhong%2520and%2520Hongliang%2520Lu%2520and%2520Xinhu%2520Zheng%2520and%2520Hai%2520Yang%26entry.1292438233%3D%2520%2520Accurate%2520and%2520real-time%2520prediction%2520of%2520surrounding%2520vehicles%2527%2520lane-changing%250Aintentions%2520is%2520a%2520critical%2520challenge%2520in%2520deploying%2520safe%2520and%2520efficient%2520autonomous%250Adriving%2520systems%2520in%2520open-world%2520scenarios.%2520Existing%2520high-performing%2520methods%250Aremain%2520hard%2520to%2520deploy%2520due%2520to%2520their%2520high%2520computational%2520cost%252C%2520long%2520training%250Atimes%252C%2520and%2520excessive%2520memory%2520requirements.%2520Here%252C%2520we%2520propose%2520an%2520efficient%250Alane-changing%2520intention%2520prediction%2520approach%2520based%2520on%2520brain-inspired%2520Spiking%250ANeural%2520Networks%2520%2528SNN%2529.%2520By%2520leveraging%2520the%2520event-driven%2520nature%2520of%2520SNN%252C%2520the%250Aproposed%2520approach%2520enables%2520us%2520to%2520encode%2520the%2520vehicle%2527s%2520states%2520in%2520a%2520more%2520efficient%250Amanner.%2520Comparison%2520experiments%2520conducted%2520on%2520HighD%2520and%2520NGSIM%2520datasets%250Ademonstrate%2520that%2520our%2520method%2520significantly%2520improves%2520training%2520efficiency%2520and%250Areduces%2520deployment%2520costs%2520while%2520maintaining%2520comparable%2520prediction%2520accuracy.%250AParticularly%252C%2520compared%2520to%2520the%2520baseline%252C%2520our%2520approach%2520reduces%2520training%2520time%2520by%250A75%2525%2520and%2520memory%2520usage%2520by%252099.9%2525.%2520These%2520results%2520validate%2520the%2520efficiency%2520and%250Areliability%2520of%2520our%2520method%2520in%2520lane-changing%2520predictions%252C%2520highlighting%2520its%250Apotential%2520for%2520safe%2520and%2520efficient%2520autonomous%2520driving%2520systems%2520while%2520offering%250Asignificant%2520advantages%2520in%2520deployment%252C%2520including%2520reduced%2520training%2520time%252C%2520lower%250Amemory%2520usage%252C%2520and%2520faster%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08659v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deployment-friendly%20Lane-changing%20Intention%20Prediction%20Powered%20by%0A%20%20Brain-inspired%20Spiking%20Neural%20Networks&entry.906535625=Shuqi%20Shen%20and%20Junjie%20Yang%20and%20Hui%20Zhong%20and%20Hongliang%20Lu%20and%20Xinhu%20Zheng%20and%20Hai%20Yang&entry.1292438233=%20%20Accurate%20and%20real-time%20prediction%20of%20surrounding%20vehicles%27%20lane-changing%0Aintentions%20is%20a%20critical%20challenge%20in%20deploying%20safe%20and%20efficient%20autonomous%0Adriving%20systems%20in%20open-world%20scenarios.%20Existing%20high-performing%20methods%0Aremain%20hard%20to%20deploy%20due%20to%20their%20high%20computational%20cost%2C%20long%20training%0Atimes%2C%20and%20excessive%20memory%20requirements.%20Here%2C%20we%20propose%20an%20efficient%0Alane-changing%20intention%20prediction%20approach%20based%20on%20brain-inspired%20Spiking%0ANeural%20Networks%20%28SNN%29.%20By%20leveraging%20the%20event-driven%20nature%20of%20SNN%2C%20the%0Aproposed%20approach%20enables%20us%20to%20encode%20the%20vehicle%27s%20states%20in%20a%20more%20efficient%0Amanner.%20Comparison%20experiments%20conducted%20on%20HighD%20and%20NGSIM%20datasets%0Ademonstrate%20that%20our%20method%20significantly%20improves%20training%20efficiency%20and%0Areduces%20deployment%20costs%20while%20maintaining%20comparable%20prediction%20accuracy.%0AParticularly%2C%20compared%20to%20the%20baseline%2C%20our%20approach%20reduces%20training%20time%20by%0A75%25%20and%20memory%20usage%20by%2099.9%25.%20These%20results%20validate%20the%20efficiency%20and%0Areliability%20of%20our%20method%20in%20lane-changing%20predictions%2C%20highlighting%20its%0Apotential%20for%20safe%20and%20efficient%20autonomous%20driving%20systems%20while%20offering%0Asignificant%20advantages%20in%20deployment%2C%20including%20reduced%20training%20time%2C%20lower%0Amemory%20usage%2C%20and%20faster%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08659v3&entry.124074799=Read"},
{"title": "Hierarchical and Multimodal Data for Daily Activity Understanding", "author": "Ghazal Kaviani and Yavuz Yarici and Seulgi Kim and Mohit Prabhushankar and Ghassan AlRegib and Mashhour Solh and Ameya Patil", "abstract": "  Daily Activity Recordings for Artificial Intelligence (DARai, pronounced\n\"Dahr-ree\") is a multimodal, hierarchically annotated dataset constructed to\nunderstand human activities in real-world settings. DARai consists of\ncontinuous scripted and unscripted recordings of 50 participants in 10\ndifferent environments, totaling over 200 hours of data from 20 sensors\nincluding multiple camera views, depth and radar sensors, wearable inertial\nmeasurement units (IMUs), electromyography (EMG), insole pressure sensors,\nbiomonitor sensors, and gaze tracker.\n  To capture the complexity in human activities, DARai is annotated at three\nlevels of hierarchy: (i) high-level activities (L1) that are independent tasks,\n(ii) lower-level actions (L2) that are patterns shared between activities, and\n(iii) fine-grained procedures (L3) that detail the exact execution steps for\nactions. The dataset annotations and recordings are designed so that 22.7% of\nL2 actions are shared between L1 activities and 14.2% of L3 procedures are\nshared between L2 actions. The overlap and unscripted nature of DARai allows\ncounterfactual activities in the dataset.\n  Experiments with various machine learning models showcase the value of DARai\nin uncovering important challenges in human-centered applications.\nSpecifically, we conduct unimodal and multimodal sensor fusion experiments for\nrecognition, temporal localization, and future action anticipation across all\nhierarchical annotation levels. To highlight the limitations of individual\nsensors, we also conduct domain-variant experiments that are enabled by DARai's\nmulti-sensor and counterfactual activity design setup.\n  The code, documentation, and dataset are available at the dedicated DARai\nwebsite:\nhttps://alregib.ece.gatech.edu/software-and-datasets/darai-daily-activity-recordings-for-artificial-intelligence-and-machine-learning/\n", "link": "http://arxiv.org/abs/2504.17696v1", "date": "2025-04-24", "relevancy": 2.0957, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5326}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5308}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5136}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20and%20Multimodal%20Data%20for%20Daily%20Activity%20Understanding&body=Title%3A%20Hierarchical%20and%20Multimodal%20Data%20for%20Daily%20Activity%20Understanding%0AAuthor%3A%20Ghazal%20Kaviani%20and%20Yavuz%20Yarici%20and%20Seulgi%20Kim%20and%20Mohit%20Prabhushankar%20and%20Ghassan%20AlRegib%20and%20Mashhour%20Solh%20and%20Ameya%20Patil%0AAbstract%3A%20%20%20Daily%20Activity%20Recordings%20for%20Artificial%20Intelligence%20%28DARai%2C%20pronounced%0A%22Dahr-ree%22%29%20is%20a%20multimodal%2C%20hierarchically%20annotated%20dataset%20constructed%20to%0Aunderstand%20human%20activities%20in%20real-world%20settings.%20DARai%20consists%20of%0Acontinuous%20scripted%20and%20unscripted%20recordings%20of%2050%20participants%20in%2010%0Adifferent%20environments%2C%20totaling%20over%20200%20hours%20of%20data%20from%2020%20sensors%0Aincluding%20multiple%20camera%20views%2C%20depth%20and%20radar%20sensors%2C%20wearable%20inertial%0Ameasurement%20units%20%28IMUs%29%2C%20electromyography%20%28EMG%29%2C%20insole%20pressure%20sensors%2C%0Abiomonitor%20sensors%2C%20and%20gaze%20tracker.%0A%20%20To%20capture%20the%20complexity%20in%20human%20activities%2C%20DARai%20is%20annotated%20at%20three%0Alevels%20of%20hierarchy%3A%20%28i%29%20high-level%20activities%20%28L1%29%20that%20are%20independent%20tasks%2C%0A%28ii%29%20lower-level%20actions%20%28L2%29%20that%20are%20patterns%20shared%20between%20activities%2C%20and%0A%28iii%29%20fine-grained%20procedures%20%28L3%29%20that%20detail%20the%20exact%20execution%20steps%20for%0Aactions.%20The%20dataset%20annotations%20and%20recordings%20are%20designed%20so%20that%2022.7%25%20of%0AL2%20actions%20are%20shared%20between%20L1%20activities%20and%2014.2%25%20of%20L3%20procedures%20are%0Ashared%20between%20L2%20actions.%20The%20overlap%20and%20unscripted%20nature%20of%20DARai%20allows%0Acounterfactual%20activities%20in%20the%20dataset.%0A%20%20Experiments%20with%20various%20machine%20learning%20models%20showcase%20the%20value%20of%20DARai%0Ain%20uncovering%20important%20challenges%20in%20human-centered%20applications.%0ASpecifically%2C%20we%20conduct%20unimodal%20and%20multimodal%20sensor%20fusion%20experiments%20for%0Arecognition%2C%20temporal%20localization%2C%20and%20future%20action%20anticipation%20across%20all%0Ahierarchical%20annotation%20levels.%20To%20highlight%20the%20limitations%20of%20individual%0Asensors%2C%20we%20also%20conduct%20domain-variant%20experiments%20that%20are%20enabled%20by%20DARai%27s%0Amulti-sensor%20and%20counterfactual%20activity%20design%20setup.%0A%20%20The%20code%2C%20documentation%2C%20and%20dataset%20are%20available%20at%20the%20dedicated%20DARai%0Awebsite%3A%0Ahttps%3A//alregib.ece.gatech.edu/software-and-datasets/darai-daily-activity-recordings-for-artificial-intelligence-and-machine-learning/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17696v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520and%2520Multimodal%2520Data%2520for%2520Daily%2520Activity%2520Understanding%26entry.906535625%3DGhazal%2520Kaviani%2520and%2520Yavuz%2520Yarici%2520and%2520Seulgi%2520Kim%2520and%2520Mohit%2520Prabhushankar%2520and%2520Ghassan%2520AlRegib%2520and%2520Mashhour%2520Solh%2520and%2520Ameya%2520Patil%26entry.1292438233%3D%2520%2520Daily%2520Activity%2520Recordings%2520for%2520Artificial%2520Intelligence%2520%2528DARai%252C%2520pronounced%250A%2522Dahr-ree%2522%2529%2520is%2520a%2520multimodal%252C%2520hierarchically%2520annotated%2520dataset%2520constructed%2520to%250Aunderstand%2520human%2520activities%2520in%2520real-world%2520settings.%2520DARai%2520consists%2520of%250Acontinuous%2520scripted%2520and%2520unscripted%2520recordings%2520of%252050%2520participants%2520in%252010%250Adifferent%2520environments%252C%2520totaling%2520over%2520200%2520hours%2520of%2520data%2520from%252020%2520sensors%250Aincluding%2520multiple%2520camera%2520views%252C%2520depth%2520and%2520radar%2520sensors%252C%2520wearable%2520inertial%250Ameasurement%2520units%2520%2528IMUs%2529%252C%2520electromyography%2520%2528EMG%2529%252C%2520insole%2520pressure%2520sensors%252C%250Abiomonitor%2520sensors%252C%2520and%2520gaze%2520tracker.%250A%2520%2520To%2520capture%2520the%2520complexity%2520in%2520human%2520activities%252C%2520DARai%2520is%2520annotated%2520at%2520three%250Alevels%2520of%2520hierarchy%253A%2520%2528i%2529%2520high-level%2520activities%2520%2528L1%2529%2520that%2520are%2520independent%2520tasks%252C%250A%2528ii%2529%2520lower-level%2520actions%2520%2528L2%2529%2520that%2520are%2520patterns%2520shared%2520between%2520activities%252C%2520and%250A%2528iii%2529%2520fine-grained%2520procedures%2520%2528L3%2529%2520that%2520detail%2520the%2520exact%2520execution%2520steps%2520for%250Aactions.%2520The%2520dataset%2520annotations%2520and%2520recordings%2520are%2520designed%2520so%2520that%252022.7%2525%2520of%250AL2%2520actions%2520are%2520shared%2520between%2520L1%2520activities%2520and%252014.2%2525%2520of%2520L3%2520procedures%2520are%250Ashared%2520between%2520L2%2520actions.%2520The%2520overlap%2520and%2520unscripted%2520nature%2520of%2520DARai%2520allows%250Acounterfactual%2520activities%2520in%2520the%2520dataset.%250A%2520%2520Experiments%2520with%2520various%2520machine%2520learning%2520models%2520showcase%2520the%2520value%2520of%2520DARai%250Ain%2520uncovering%2520important%2520challenges%2520in%2520human-centered%2520applications.%250ASpecifically%252C%2520we%2520conduct%2520unimodal%2520and%2520multimodal%2520sensor%2520fusion%2520experiments%2520for%250Arecognition%252C%2520temporal%2520localization%252C%2520and%2520future%2520action%2520anticipation%2520across%2520all%250Ahierarchical%2520annotation%2520levels.%2520To%2520highlight%2520the%2520limitations%2520of%2520individual%250Asensors%252C%2520we%2520also%2520conduct%2520domain-variant%2520experiments%2520that%2520are%2520enabled%2520by%2520DARai%2527s%250Amulti-sensor%2520and%2520counterfactual%2520activity%2520design%2520setup.%250A%2520%2520The%2520code%252C%2520documentation%252C%2520and%2520dataset%2520are%2520available%2520at%2520the%2520dedicated%2520DARai%250Awebsite%253A%250Ahttps%253A//alregib.ece.gatech.edu/software-and-datasets/darai-daily-activity-recordings-for-artificial-intelligence-and-machine-learning/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17696v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20and%20Multimodal%20Data%20for%20Daily%20Activity%20Understanding&entry.906535625=Ghazal%20Kaviani%20and%20Yavuz%20Yarici%20and%20Seulgi%20Kim%20and%20Mohit%20Prabhushankar%20and%20Ghassan%20AlRegib%20and%20Mashhour%20Solh%20and%20Ameya%20Patil&entry.1292438233=%20%20Daily%20Activity%20Recordings%20for%20Artificial%20Intelligence%20%28DARai%2C%20pronounced%0A%22Dahr-ree%22%29%20is%20a%20multimodal%2C%20hierarchically%20annotated%20dataset%20constructed%20to%0Aunderstand%20human%20activities%20in%20real-world%20settings.%20DARai%20consists%20of%0Acontinuous%20scripted%20and%20unscripted%20recordings%20of%2050%20participants%20in%2010%0Adifferent%20environments%2C%20totaling%20over%20200%20hours%20of%20data%20from%2020%20sensors%0Aincluding%20multiple%20camera%20views%2C%20depth%20and%20radar%20sensors%2C%20wearable%20inertial%0Ameasurement%20units%20%28IMUs%29%2C%20electromyography%20%28EMG%29%2C%20insole%20pressure%20sensors%2C%0Abiomonitor%20sensors%2C%20and%20gaze%20tracker.%0A%20%20To%20capture%20the%20complexity%20in%20human%20activities%2C%20DARai%20is%20annotated%20at%20three%0Alevels%20of%20hierarchy%3A%20%28i%29%20high-level%20activities%20%28L1%29%20that%20are%20independent%20tasks%2C%0A%28ii%29%20lower-level%20actions%20%28L2%29%20that%20are%20patterns%20shared%20between%20activities%2C%20and%0A%28iii%29%20fine-grained%20procedures%20%28L3%29%20that%20detail%20the%20exact%20execution%20steps%20for%0Aactions.%20The%20dataset%20annotations%20and%20recordings%20are%20designed%20so%20that%2022.7%25%20of%0AL2%20actions%20are%20shared%20between%20L1%20activities%20and%2014.2%25%20of%20L3%20procedures%20are%0Ashared%20between%20L2%20actions.%20The%20overlap%20and%20unscripted%20nature%20of%20DARai%20allows%0Acounterfactual%20activities%20in%20the%20dataset.%0A%20%20Experiments%20with%20various%20machine%20learning%20models%20showcase%20the%20value%20of%20DARai%0Ain%20uncovering%20important%20challenges%20in%20human-centered%20applications.%0ASpecifically%2C%20we%20conduct%20unimodal%20and%20multimodal%20sensor%20fusion%20experiments%20for%0Arecognition%2C%20temporal%20localization%2C%20and%20future%20action%20anticipation%20across%20all%0Ahierarchical%20annotation%20levels.%20To%20highlight%20the%20limitations%20of%20individual%0Asensors%2C%20we%20also%20conduct%20domain-variant%20experiments%20that%20are%20enabled%20by%20DARai%27s%0Amulti-sensor%20and%20counterfactual%20activity%20design%20setup.%0A%20%20The%20code%2C%20documentation%2C%20and%20dataset%20are%20available%20at%20the%20dedicated%20DARai%0Awebsite%3A%0Ahttps%3A//alregib.ece.gatech.edu/software-and-datasets/darai-daily-activity-recordings-for-artificial-intelligence-and-machine-learning/%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17696v1&entry.124074799=Read"},
{"title": "Putting the Segment Anything Model to the Test with 3D Knee MRI - A\n  Comparison with State-of-the-Art Performance", "author": "Oliver Mills and Philip Conaghan and Nishant Ravikumar and Samuel Relton", "abstract": "  Menisci are cartilaginous tissue found within the knee that contribute to\njoint lubrication and weight dispersal. Damage to menisci can lead to onset and\nprogression of knee osteoarthritis (OA), a condition that is a leading cause of\ndisability, and for which there are few effective therapies. Accurate automated\nsegmentation of menisci would allow for earlier detection and treatment of\nmeniscal abnormalities, as well as shedding more light on the role the menisci\nplay in OA pathogenesis. Focus in this area has mainly used variants of\nconvolutional networks, but there has been no attempt to utilise recent large\nvision transformer segmentation models. The Segment Anything Model (SAM) is a\nso-called foundation segmentation model, which has been found useful across a\nrange of different tasks due to the large volume of data used for training the\nmodel. In this study, SAM was adapted to perform fully-automated segmentation\nof menisci from 3D knee magnetic resonance images. A 3D U-Net was also trained\nas a baseline. It was found that, when fine-tuning only the decoder, SAM was\nunable to compete with 3D U-Net, achieving a Dice score of $0.81\\pm0.03$,\ncompared to $0.87\\pm0.03$, on a held-out test set. When fine-tuning SAM\nend-to-end, a Dice score of $0.87\\pm0.03$ was achieved. The performance of both\nthe end-to-end trained SAM configuration and the 3D U-Net were comparable to\nthe winning Dice score ($0.88\\pm0.03$) in the IWOAI Knee MRI Segmentation\nChallenge 2019. Performance in terms of the Hausdorff Distance showed that both\nconfigurations of SAM were inferior to 3D U-Net in matching the meniscus\nmorphology. Results demonstrated that, despite its generalisability, SAM was\nunable to outperform a basic 3D U-Net in meniscus segmentation, and may not be\nsuitable for similar 3D medical image segmentation tasks also involving fine\nanatomical structures with low contrast and poorly-defined boundaries.\n", "link": "http://arxiv.org/abs/2504.13340v3", "date": "2025-04-24", "relevancy": 2.0957, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5504}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5256}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Putting%20the%20Segment%20Anything%20Model%20to%20the%20Test%20with%203D%20Knee%20MRI%20-%20A%0A%20%20Comparison%20with%20State-of-the-Art%20Performance&body=Title%3A%20Putting%20the%20Segment%20Anything%20Model%20to%20the%20Test%20with%203D%20Knee%20MRI%20-%20A%0A%20%20Comparison%20with%20State-of-the-Art%20Performance%0AAuthor%3A%20Oliver%20Mills%20and%20Philip%20Conaghan%20and%20Nishant%20Ravikumar%20and%20Samuel%20Relton%0AAbstract%3A%20%20%20Menisci%20are%20cartilaginous%20tissue%20found%20within%20the%20knee%20that%20contribute%20to%0Ajoint%20lubrication%20and%20weight%20dispersal.%20Damage%20to%20menisci%20can%20lead%20to%20onset%20and%0Aprogression%20of%20knee%20osteoarthritis%20%28OA%29%2C%20a%20condition%20that%20is%20a%20leading%20cause%20of%0Adisability%2C%20and%20for%20which%20there%20are%20few%20effective%20therapies.%20Accurate%20automated%0Asegmentation%20of%20menisci%20would%20allow%20for%20earlier%20detection%20and%20treatment%20of%0Ameniscal%20abnormalities%2C%20as%20well%20as%20shedding%20more%20light%20on%20the%20role%20the%20menisci%0Aplay%20in%20OA%20pathogenesis.%20Focus%20in%20this%20area%20has%20mainly%20used%20variants%20of%0Aconvolutional%20networks%2C%20but%20there%20has%20been%20no%20attempt%20to%20utilise%20recent%20large%0Avision%20transformer%20segmentation%20models.%20The%20Segment%20Anything%20Model%20%28SAM%29%20is%20a%0Aso-called%20foundation%20segmentation%20model%2C%20which%20has%20been%20found%20useful%20across%20a%0Arange%20of%20different%20tasks%20due%20to%20the%20large%20volume%20of%20data%20used%20for%20training%20the%0Amodel.%20In%20this%20study%2C%20SAM%20was%20adapted%20to%20perform%20fully-automated%20segmentation%0Aof%20menisci%20from%203D%20knee%20magnetic%20resonance%20images.%20A%203D%20U-Net%20was%20also%20trained%0Aas%20a%20baseline.%20It%20was%20found%20that%2C%20when%20fine-tuning%20only%20the%20decoder%2C%20SAM%20was%0Aunable%20to%20compete%20with%203D%20U-Net%2C%20achieving%20a%20Dice%20score%20of%20%240.81%5Cpm0.03%24%2C%0Acompared%20to%20%240.87%5Cpm0.03%24%2C%20on%20a%20held-out%20test%20set.%20When%20fine-tuning%20SAM%0Aend-to-end%2C%20a%20Dice%20score%20of%20%240.87%5Cpm0.03%24%20was%20achieved.%20The%20performance%20of%20both%0Athe%20end-to-end%20trained%20SAM%20configuration%20and%20the%203D%20U-Net%20were%20comparable%20to%0Athe%20winning%20Dice%20score%20%28%240.88%5Cpm0.03%24%29%20in%20the%20IWOAI%20Knee%20MRI%20Segmentation%0AChallenge%202019.%20Performance%20in%20terms%20of%20the%20Hausdorff%20Distance%20showed%20that%20both%0Aconfigurations%20of%20SAM%20were%20inferior%20to%203D%20U-Net%20in%20matching%20the%20meniscus%0Amorphology.%20Results%20demonstrated%20that%2C%20despite%20its%20generalisability%2C%20SAM%20was%0Aunable%20to%20outperform%20a%20basic%203D%20U-Net%20in%20meniscus%20segmentation%2C%20and%20may%20not%20be%0Asuitable%20for%20similar%203D%20medical%20image%20segmentation%20tasks%20also%20involving%20fine%0Aanatomical%20structures%20with%20low%20contrast%20and%20poorly-defined%20boundaries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13340v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPutting%2520the%2520Segment%2520Anything%2520Model%2520to%2520the%2520Test%2520with%25203D%2520Knee%2520MRI%2520-%2520A%250A%2520%2520Comparison%2520with%2520State-of-the-Art%2520Performance%26entry.906535625%3DOliver%2520Mills%2520and%2520Philip%2520Conaghan%2520and%2520Nishant%2520Ravikumar%2520and%2520Samuel%2520Relton%26entry.1292438233%3D%2520%2520Menisci%2520are%2520cartilaginous%2520tissue%2520found%2520within%2520the%2520knee%2520that%2520contribute%2520to%250Ajoint%2520lubrication%2520and%2520weight%2520dispersal.%2520Damage%2520to%2520menisci%2520can%2520lead%2520to%2520onset%2520and%250Aprogression%2520of%2520knee%2520osteoarthritis%2520%2528OA%2529%252C%2520a%2520condition%2520that%2520is%2520a%2520leading%2520cause%2520of%250Adisability%252C%2520and%2520for%2520which%2520there%2520are%2520few%2520effective%2520therapies.%2520Accurate%2520automated%250Asegmentation%2520of%2520menisci%2520would%2520allow%2520for%2520earlier%2520detection%2520and%2520treatment%2520of%250Ameniscal%2520abnormalities%252C%2520as%2520well%2520as%2520shedding%2520more%2520light%2520on%2520the%2520role%2520the%2520menisci%250Aplay%2520in%2520OA%2520pathogenesis.%2520Focus%2520in%2520this%2520area%2520has%2520mainly%2520used%2520variants%2520of%250Aconvolutional%2520networks%252C%2520but%2520there%2520has%2520been%2520no%2520attempt%2520to%2520utilise%2520recent%2520large%250Avision%2520transformer%2520segmentation%2520models.%2520The%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520is%2520a%250Aso-called%2520foundation%2520segmentation%2520model%252C%2520which%2520has%2520been%2520found%2520useful%2520across%2520a%250Arange%2520of%2520different%2520tasks%2520due%2520to%2520the%2520large%2520volume%2520of%2520data%2520used%2520for%2520training%2520the%250Amodel.%2520In%2520this%2520study%252C%2520SAM%2520was%2520adapted%2520to%2520perform%2520fully-automated%2520segmentation%250Aof%2520menisci%2520from%25203D%2520knee%2520magnetic%2520resonance%2520images.%2520A%25203D%2520U-Net%2520was%2520also%2520trained%250Aas%2520a%2520baseline.%2520It%2520was%2520found%2520that%252C%2520when%2520fine-tuning%2520only%2520the%2520decoder%252C%2520SAM%2520was%250Aunable%2520to%2520compete%2520with%25203D%2520U-Net%252C%2520achieving%2520a%2520Dice%2520score%2520of%2520%25240.81%255Cpm0.03%2524%252C%250Acompared%2520to%2520%25240.87%255Cpm0.03%2524%252C%2520on%2520a%2520held-out%2520test%2520set.%2520When%2520fine-tuning%2520SAM%250Aend-to-end%252C%2520a%2520Dice%2520score%2520of%2520%25240.87%255Cpm0.03%2524%2520was%2520achieved.%2520The%2520performance%2520of%2520both%250Athe%2520end-to-end%2520trained%2520SAM%2520configuration%2520and%2520the%25203D%2520U-Net%2520were%2520comparable%2520to%250Athe%2520winning%2520Dice%2520score%2520%2528%25240.88%255Cpm0.03%2524%2529%2520in%2520the%2520IWOAI%2520Knee%2520MRI%2520Segmentation%250AChallenge%25202019.%2520Performance%2520in%2520terms%2520of%2520the%2520Hausdorff%2520Distance%2520showed%2520that%2520both%250Aconfigurations%2520of%2520SAM%2520were%2520inferior%2520to%25203D%2520U-Net%2520in%2520matching%2520the%2520meniscus%250Amorphology.%2520Results%2520demonstrated%2520that%252C%2520despite%2520its%2520generalisability%252C%2520SAM%2520was%250Aunable%2520to%2520outperform%2520a%2520basic%25203D%2520U-Net%2520in%2520meniscus%2520segmentation%252C%2520and%2520may%2520not%2520be%250Asuitable%2520for%2520similar%25203D%2520medical%2520image%2520segmentation%2520tasks%2520also%2520involving%2520fine%250Aanatomical%2520structures%2520with%2520low%2520contrast%2520and%2520poorly-defined%2520boundaries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13340v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Putting%20the%20Segment%20Anything%20Model%20to%20the%20Test%20with%203D%20Knee%20MRI%20-%20A%0A%20%20Comparison%20with%20State-of-the-Art%20Performance&entry.906535625=Oliver%20Mills%20and%20Philip%20Conaghan%20and%20Nishant%20Ravikumar%20and%20Samuel%20Relton&entry.1292438233=%20%20Menisci%20are%20cartilaginous%20tissue%20found%20within%20the%20knee%20that%20contribute%20to%0Ajoint%20lubrication%20and%20weight%20dispersal.%20Damage%20to%20menisci%20can%20lead%20to%20onset%20and%0Aprogression%20of%20knee%20osteoarthritis%20%28OA%29%2C%20a%20condition%20that%20is%20a%20leading%20cause%20of%0Adisability%2C%20and%20for%20which%20there%20are%20few%20effective%20therapies.%20Accurate%20automated%0Asegmentation%20of%20menisci%20would%20allow%20for%20earlier%20detection%20and%20treatment%20of%0Ameniscal%20abnormalities%2C%20as%20well%20as%20shedding%20more%20light%20on%20the%20role%20the%20menisci%0Aplay%20in%20OA%20pathogenesis.%20Focus%20in%20this%20area%20has%20mainly%20used%20variants%20of%0Aconvolutional%20networks%2C%20but%20there%20has%20been%20no%20attempt%20to%20utilise%20recent%20large%0Avision%20transformer%20segmentation%20models.%20The%20Segment%20Anything%20Model%20%28SAM%29%20is%20a%0Aso-called%20foundation%20segmentation%20model%2C%20which%20has%20been%20found%20useful%20across%20a%0Arange%20of%20different%20tasks%20due%20to%20the%20large%20volume%20of%20data%20used%20for%20training%20the%0Amodel.%20In%20this%20study%2C%20SAM%20was%20adapted%20to%20perform%20fully-automated%20segmentation%0Aof%20menisci%20from%203D%20knee%20magnetic%20resonance%20images.%20A%203D%20U-Net%20was%20also%20trained%0Aas%20a%20baseline.%20It%20was%20found%20that%2C%20when%20fine-tuning%20only%20the%20decoder%2C%20SAM%20was%0Aunable%20to%20compete%20with%203D%20U-Net%2C%20achieving%20a%20Dice%20score%20of%20%240.81%5Cpm0.03%24%2C%0Acompared%20to%20%240.87%5Cpm0.03%24%2C%20on%20a%20held-out%20test%20set.%20When%20fine-tuning%20SAM%0Aend-to-end%2C%20a%20Dice%20score%20of%20%240.87%5Cpm0.03%24%20was%20achieved.%20The%20performance%20of%20both%0Athe%20end-to-end%20trained%20SAM%20configuration%20and%20the%203D%20U-Net%20were%20comparable%20to%0Athe%20winning%20Dice%20score%20%28%240.88%5Cpm0.03%24%29%20in%20the%20IWOAI%20Knee%20MRI%20Segmentation%0AChallenge%202019.%20Performance%20in%20terms%20of%20the%20Hausdorff%20Distance%20showed%20that%20both%0Aconfigurations%20of%20SAM%20were%20inferior%20to%203D%20U-Net%20in%20matching%20the%20meniscus%0Amorphology.%20Results%20demonstrated%20that%2C%20despite%20its%20generalisability%2C%20SAM%20was%0Aunable%20to%20outperform%20a%20basic%203D%20U-Net%20in%20meniscus%20segmentation%2C%20and%20may%20not%20be%0Asuitable%20for%20similar%203D%20medical%20image%20segmentation%20tasks%20also%20involving%20fine%0Aanatomical%20structures%20with%20low%20contrast%20and%20poorly-defined%20boundaries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13340v3&entry.124074799=Read"},
{"title": "Analysing Multiscale Clusterings with Persistent Homology", "author": "Juni Schindler and Mauricio Barahona", "abstract": "  In data clustering, it is often desirable to find not just a single partition\ninto clusters but a sequence of partitions that describes the data at different\nscales (or levels of coarseness). A natural problem then is to analyse and\ncompare the (not necessarily hierarchical) sequences of partitions that\nunderpin such multiscale descriptions. Here, we use tools from topological data\nanalysis and introduce the Multiscale Clustering Filtration (MCF), a\nwell-defined and stable filtration of abstract simplicial complexes that\nencodes arbitrary cluster assignments in a sequence of partitions across scales\nof increasing coarseness. We show that the zero-dimensional persistent homology\nof the MCF measures the degree of hierarchy of this sequence, and the\nhigher-dimensional persistent homology tracks the emergence and resolution of\nconflicts between cluster assignments across the sequence of partitions. To\nbroaden the theoretical foundations of the MCF, we provide an equivalent\nconstruction via a nerve complex filtration, and we show that, in the\nhierarchical case, the MCF reduces to a Vietoris-Rips filtration of an\nultrametric space. Using synthetic data, we then illustrate how the persistence\ndiagram of the MCF provides a feature map that can serve to characterise and\nclassify multiscale clusterings.\n", "link": "http://arxiv.org/abs/2305.04281v5", "date": "2025-04-24", "relevancy": 2.0839, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.425}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4216}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analysing%20Multiscale%20Clusterings%20with%20Persistent%20Homology&body=Title%3A%20Analysing%20Multiscale%20Clusterings%20with%20Persistent%20Homology%0AAuthor%3A%20Juni%20Schindler%20and%20Mauricio%20Barahona%0AAbstract%3A%20%20%20In%20data%20clustering%2C%20it%20is%20often%20desirable%20to%20find%20not%20just%20a%20single%20partition%0Ainto%20clusters%20but%20a%20sequence%20of%20partitions%20that%20describes%20the%20data%20at%20different%0Ascales%20%28or%20levels%20of%20coarseness%29.%20A%20natural%20problem%20then%20is%20to%20analyse%20and%0Acompare%20the%20%28not%20necessarily%20hierarchical%29%20sequences%20of%20partitions%20that%0Aunderpin%20such%20multiscale%20descriptions.%20Here%2C%20we%20use%20tools%20from%20topological%20data%0Aanalysis%20and%20introduce%20the%20Multiscale%20Clustering%20Filtration%20%28MCF%29%2C%20a%0Awell-defined%20and%20stable%20filtration%20of%20abstract%20simplicial%20complexes%20that%0Aencodes%20arbitrary%20cluster%20assignments%20in%20a%20sequence%20of%20partitions%20across%20scales%0Aof%20increasing%20coarseness.%20We%20show%20that%20the%20zero-dimensional%20persistent%20homology%0Aof%20the%20MCF%20measures%20the%20degree%20of%20hierarchy%20of%20this%20sequence%2C%20and%20the%0Ahigher-dimensional%20persistent%20homology%20tracks%20the%20emergence%20and%20resolution%20of%0Aconflicts%20between%20cluster%20assignments%20across%20the%20sequence%20of%20partitions.%20To%0Abroaden%20the%20theoretical%20foundations%20of%20the%20MCF%2C%20we%20provide%20an%20equivalent%0Aconstruction%20via%20a%20nerve%20complex%20filtration%2C%20and%20we%20show%20that%2C%20in%20the%0Ahierarchical%20case%2C%20the%20MCF%20reduces%20to%20a%20Vietoris-Rips%20filtration%20of%20an%0Aultrametric%20space.%20Using%20synthetic%20data%2C%20we%20then%20illustrate%20how%20the%20persistence%0Adiagram%20of%20the%20MCF%20provides%20a%20feature%20map%20that%20can%20serve%20to%20characterise%20and%0Aclassify%20multiscale%20clusterings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.04281v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalysing%2520Multiscale%2520Clusterings%2520with%2520Persistent%2520Homology%26entry.906535625%3DJuni%2520Schindler%2520and%2520Mauricio%2520Barahona%26entry.1292438233%3D%2520%2520In%2520data%2520clustering%252C%2520it%2520is%2520often%2520desirable%2520to%2520find%2520not%2520just%2520a%2520single%2520partition%250Ainto%2520clusters%2520but%2520a%2520sequence%2520of%2520partitions%2520that%2520describes%2520the%2520data%2520at%2520different%250Ascales%2520%2528or%2520levels%2520of%2520coarseness%2529.%2520A%2520natural%2520problem%2520then%2520is%2520to%2520analyse%2520and%250Acompare%2520the%2520%2528not%2520necessarily%2520hierarchical%2529%2520sequences%2520of%2520partitions%2520that%250Aunderpin%2520such%2520multiscale%2520descriptions.%2520Here%252C%2520we%2520use%2520tools%2520from%2520topological%2520data%250Aanalysis%2520and%2520introduce%2520the%2520Multiscale%2520Clustering%2520Filtration%2520%2528MCF%2529%252C%2520a%250Awell-defined%2520and%2520stable%2520filtration%2520of%2520abstract%2520simplicial%2520complexes%2520that%250Aencodes%2520arbitrary%2520cluster%2520assignments%2520in%2520a%2520sequence%2520of%2520partitions%2520across%2520scales%250Aof%2520increasing%2520coarseness.%2520We%2520show%2520that%2520the%2520zero-dimensional%2520persistent%2520homology%250Aof%2520the%2520MCF%2520measures%2520the%2520degree%2520of%2520hierarchy%2520of%2520this%2520sequence%252C%2520and%2520the%250Ahigher-dimensional%2520persistent%2520homology%2520tracks%2520the%2520emergence%2520and%2520resolution%2520of%250Aconflicts%2520between%2520cluster%2520assignments%2520across%2520the%2520sequence%2520of%2520partitions.%2520To%250Abroaden%2520the%2520theoretical%2520foundations%2520of%2520the%2520MCF%252C%2520we%2520provide%2520an%2520equivalent%250Aconstruction%2520via%2520a%2520nerve%2520complex%2520filtration%252C%2520and%2520we%2520show%2520that%252C%2520in%2520the%250Ahierarchical%2520case%252C%2520the%2520MCF%2520reduces%2520to%2520a%2520Vietoris-Rips%2520filtration%2520of%2520an%250Aultrametric%2520space.%2520Using%2520synthetic%2520data%252C%2520we%2520then%2520illustrate%2520how%2520the%2520persistence%250Adiagram%2520of%2520the%2520MCF%2520provides%2520a%2520feature%2520map%2520that%2520can%2520serve%2520to%2520characterise%2520and%250Aclassify%2520multiscale%2520clusterings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.04281v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analysing%20Multiscale%20Clusterings%20with%20Persistent%20Homology&entry.906535625=Juni%20Schindler%20and%20Mauricio%20Barahona&entry.1292438233=%20%20In%20data%20clustering%2C%20it%20is%20often%20desirable%20to%20find%20not%20just%20a%20single%20partition%0Ainto%20clusters%20but%20a%20sequence%20of%20partitions%20that%20describes%20the%20data%20at%20different%0Ascales%20%28or%20levels%20of%20coarseness%29.%20A%20natural%20problem%20then%20is%20to%20analyse%20and%0Acompare%20the%20%28not%20necessarily%20hierarchical%29%20sequences%20of%20partitions%20that%0Aunderpin%20such%20multiscale%20descriptions.%20Here%2C%20we%20use%20tools%20from%20topological%20data%0Aanalysis%20and%20introduce%20the%20Multiscale%20Clustering%20Filtration%20%28MCF%29%2C%20a%0Awell-defined%20and%20stable%20filtration%20of%20abstract%20simplicial%20complexes%20that%0Aencodes%20arbitrary%20cluster%20assignments%20in%20a%20sequence%20of%20partitions%20across%20scales%0Aof%20increasing%20coarseness.%20We%20show%20that%20the%20zero-dimensional%20persistent%20homology%0Aof%20the%20MCF%20measures%20the%20degree%20of%20hierarchy%20of%20this%20sequence%2C%20and%20the%0Ahigher-dimensional%20persistent%20homology%20tracks%20the%20emergence%20and%20resolution%20of%0Aconflicts%20between%20cluster%20assignments%20across%20the%20sequence%20of%20partitions.%20To%0Abroaden%20the%20theoretical%20foundations%20of%20the%20MCF%2C%20we%20provide%20an%20equivalent%0Aconstruction%20via%20a%20nerve%20complex%20filtration%2C%20and%20we%20show%20that%2C%20in%20the%0Ahierarchical%20case%2C%20the%20MCF%20reduces%20to%20a%20Vietoris-Rips%20filtration%20of%20an%0Aultrametric%20space.%20Using%20synthetic%20data%2C%20we%20then%20illustrate%20how%20the%20persistence%0Adiagram%20of%20the%20MCF%20provides%20a%20feature%20map%20that%20can%20serve%20to%20characterise%20and%0Aclassify%20multiscale%20clusterings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.04281v5&entry.124074799=Read"},
{"title": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference", "author": "Qingyuan Liu and Liyan Chen and Yanning Yang and Haocheng Wang and Dong Du and Zhigang Mao and Naifeng Jing and Yubin Xia and Haibo Chen", "abstract": "  Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes.\n", "link": "http://arxiv.org/abs/2504.17584v1", "date": "2025-04-24", "relevancy": 2.0777, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5442}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5145}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20L3%3A%20DIMM-PIM%20Integrated%20Architecture%20and%20Coordination%20for%20Scalable%0A%20%20Long-Context%20LLM%20Inference&body=Title%3A%20L3%3A%20DIMM-PIM%20Integrated%20Architecture%20and%20Coordination%20for%20Scalable%0A%20%20Long-Context%20LLM%20Inference%0AAuthor%3A%20Qingyuan%20Liu%20and%20Liyan%20Chen%20and%20Yanning%20Yang%20and%20Haocheng%20Wang%20and%20Dong%20Du%20and%20Zhigang%20Mao%20and%20Naifeng%20Jing%20and%20Yubin%20Xia%20and%20Haibo%20Chen%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20increasingly%20require%20processing%20long%20text%0Asequences%2C%20but%20GPU%20memory%20limitations%20force%20difficult%20trade-offs%20between%20memory%0Acapacity%20and%20bandwidth.%20While%20HBM-based%20acceleration%20offers%20high%20bandwidth%2C%20its%0Acapacity%20remains%20constrained.%20Offloading%20data%20to%20host-side%20DIMMs%20improves%0Acapacity%20but%20introduces%20costly%20data%20swapping%20overhead.%20We%20identify%20that%20the%0Acritical%20memory%20bottleneck%20lies%20in%20the%20decoding%20phase%20of%20multi-head%20attention%0A%28MHA%29%20exclusively%2C%20which%20demands%20substantial%20capacity%20for%20storing%20KV%20caches%20and%0Ahigh%20bandwidth%20for%20attention%20computation.%20Our%20key%20insight%20reveals%20this%0Aoperation%20uniquely%20aligns%20with%20modern%20DIMM-based%20processing-in-memory%20%28PIM%29%0Aarchitectures%2C%20which%20offers%20scalability%20of%20both%20capacity%20and%20bandwidth.%0A%20%20Based%20on%20this%20observation%20and%20insight%2C%20we%20propose%20L3%2C%20a%20hardware-software%0Aco-designed%20system%20integrating%20DIMM-PIM%20and%20GPU%20devices.%20L3%20introduces%20three%0Ainnovations%3A%20First%2C%20hardware%20redesigns%20resolve%20data%20layout%20mismatches%20and%0Acomputational%20element%20mismatches%20in%20DIMM-PIM%2C%20enhancing%20LLM%20inference%0Autilization.%20Second%2C%20communication%20optimization%20enables%20hiding%20the%20data%0Atransfer%20overhead%20with%20the%20computation.%20Third%2C%20an%20adaptive%20scheduler%0Acoordinates%20GPU-DIMM-PIM%20operations%20to%20maximize%20parallelism%20between%20devices.%0AEvaluations%20using%20real-world%20traces%20show%20L3%20achieves%20up%20to%206.1%24%5Ctimes%24%20speedup%0Aover%20state-of-the-art%20HBM-PIM%20solutions%20while%20significantly%20improving%20batch%0Asizes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17584v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DL3%253A%2520DIMM-PIM%2520Integrated%2520Architecture%2520and%2520Coordination%2520for%2520Scalable%250A%2520%2520Long-Context%2520LLM%2520Inference%26entry.906535625%3DQingyuan%2520Liu%2520and%2520Liyan%2520Chen%2520and%2520Yanning%2520Yang%2520and%2520Haocheng%2520Wang%2520and%2520Dong%2520Du%2520and%2520Zhigang%2520Mao%2520and%2520Naifeng%2520Jing%2520and%2520Yubin%2520Xia%2520and%2520Haibo%2520Chen%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520increasingly%2520require%2520processing%2520long%2520text%250Asequences%252C%2520but%2520GPU%2520memory%2520limitations%2520force%2520difficult%2520trade-offs%2520between%2520memory%250Acapacity%2520and%2520bandwidth.%2520While%2520HBM-based%2520acceleration%2520offers%2520high%2520bandwidth%252C%2520its%250Acapacity%2520remains%2520constrained.%2520Offloading%2520data%2520to%2520host-side%2520DIMMs%2520improves%250Acapacity%2520but%2520introduces%2520costly%2520data%2520swapping%2520overhead.%2520We%2520identify%2520that%2520the%250Acritical%2520memory%2520bottleneck%2520lies%2520in%2520the%2520decoding%2520phase%2520of%2520multi-head%2520attention%250A%2528MHA%2529%2520exclusively%252C%2520which%2520demands%2520substantial%2520capacity%2520for%2520storing%2520KV%2520caches%2520and%250Ahigh%2520bandwidth%2520for%2520attention%2520computation.%2520Our%2520key%2520insight%2520reveals%2520this%250Aoperation%2520uniquely%2520aligns%2520with%2520modern%2520DIMM-based%2520processing-in-memory%2520%2528PIM%2529%250Aarchitectures%252C%2520which%2520offers%2520scalability%2520of%2520both%2520capacity%2520and%2520bandwidth.%250A%2520%2520Based%2520on%2520this%2520observation%2520and%2520insight%252C%2520we%2520propose%2520L3%252C%2520a%2520hardware-software%250Aco-designed%2520system%2520integrating%2520DIMM-PIM%2520and%2520GPU%2520devices.%2520L3%2520introduces%2520three%250Ainnovations%253A%2520First%252C%2520hardware%2520redesigns%2520resolve%2520data%2520layout%2520mismatches%2520and%250Acomputational%2520element%2520mismatches%2520in%2520DIMM-PIM%252C%2520enhancing%2520LLM%2520inference%250Autilization.%2520Second%252C%2520communication%2520optimization%2520enables%2520hiding%2520the%2520data%250Atransfer%2520overhead%2520with%2520the%2520computation.%2520Third%252C%2520an%2520adaptive%2520scheduler%250Acoordinates%2520GPU-DIMM-PIM%2520operations%2520to%2520maximize%2520parallelism%2520between%2520devices.%250AEvaluations%2520using%2520real-world%2520traces%2520show%2520L3%2520achieves%2520up%2520to%25206.1%2524%255Ctimes%2524%2520speedup%250Aover%2520state-of-the-art%2520HBM-PIM%2520solutions%2520while%2520significantly%2520improving%2520batch%250Asizes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17584v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=L3%3A%20DIMM-PIM%20Integrated%20Architecture%20and%20Coordination%20for%20Scalable%0A%20%20Long-Context%20LLM%20Inference&entry.906535625=Qingyuan%20Liu%20and%20Liyan%20Chen%20and%20Yanning%20Yang%20and%20Haocheng%20Wang%20and%20Dong%20Du%20and%20Zhigang%20Mao%20and%20Naifeng%20Jing%20and%20Yubin%20Xia%20and%20Haibo%20Chen&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20increasingly%20require%20processing%20long%20text%0Asequences%2C%20but%20GPU%20memory%20limitations%20force%20difficult%20trade-offs%20between%20memory%0Acapacity%20and%20bandwidth.%20While%20HBM-based%20acceleration%20offers%20high%20bandwidth%2C%20its%0Acapacity%20remains%20constrained.%20Offloading%20data%20to%20host-side%20DIMMs%20improves%0Acapacity%20but%20introduces%20costly%20data%20swapping%20overhead.%20We%20identify%20that%20the%0Acritical%20memory%20bottleneck%20lies%20in%20the%20decoding%20phase%20of%20multi-head%20attention%0A%28MHA%29%20exclusively%2C%20which%20demands%20substantial%20capacity%20for%20storing%20KV%20caches%20and%0Ahigh%20bandwidth%20for%20attention%20computation.%20Our%20key%20insight%20reveals%20this%0Aoperation%20uniquely%20aligns%20with%20modern%20DIMM-based%20processing-in-memory%20%28PIM%29%0Aarchitectures%2C%20which%20offers%20scalability%20of%20both%20capacity%20and%20bandwidth.%0A%20%20Based%20on%20this%20observation%20and%20insight%2C%20we%20propose%20L3%2C%20a%20hardware-software%0Aco-designed%20system%20integrating%20DIMM-PIM%20and%20GPU%20devices.%20L3%20introduces%20three%0Ainnovations%3A%20First%2C%20hardware%20redesigns%20resolve%20data%20layout%20mismatches%20and%0Acomputational%20element%20mismatches%20in%20DIMM-PIM%2C%20enhancing%20LLM%20inference%0Autilization.%20Second%2C%20communication%20optimization%20enables%20hiding%20the%20data%0Atransfer%20overhead%20with%20the%20computation.%20Third%2C%20an%20adaptive%20scheduler%0Acoordinates%20GPU-DIMM-PIM%20operations%20to%20maximize%20parallelism%20between%20devices.%0AEvaluations%20using%20real-world%20traces%20show%20L3%20achieves%20up%20to%206.1%24%5Ctimes%24%20speedup%0Aover%20state-of-the-art%20HBM-PIM%20solutions%20while%20significantly%20improving%20batch%0Asizes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17584v1&entry.124074799=Read"},
{"title": "STCL:Curriculum learning Strategies for deep learning image\n  steganography models", "author": "Fengchun Liu and Tong Zhang and Chunying Zhang", "abstract": "  Aiming at the problems of poor quality of steganographic images and slow\nnetwork convergence of image steganography models based on deep learning, this\npaper proposes a Steganography Curriculum Learning training strategy (STCL) for\ndeep learning image steganography models. So that only easy images are selected\nfor training when the model has poor fitting ability at the initial stage, and\ngradually expand to more difficult images, the strategy includes a difficulty\nevaluation strategy based on the teacher model and an knee point-based training\nscheduling strategy. Firstly, multiple teacher models are trained, and the\nconsistency of the quality of steganographic images under multiple teacher\nmodels is used as the difficulty score to construct the training subsets from\neasy to difficult. Secondly, a training control strategy based on knee points\nis proposed to reduce the possibility of overfitting on small training sets and\naccelerate the training process. Experimental results on three large public\ndatasets, ALASKA2, VOC2012 and ImageNet, show that the proposed image\nsteganography scheme is able to improve the model performance under multiple\nalgorithmic frameworks, which not only has a high PSNR, SSIM score, and\ndecoding accuracy, but also the steganographic images generated by the model\nunder the training of the STCL strategy have a low steganography analysis\nscores. You can find our code at\n\\href{https://github.com/chaos-boops/STCL}{https://github.com/chaos-boops/STCL}.\n", "link": "http://arxiv.org/abs/2504.17609v1", "date": "2025-04-24", "relevancy": 2.068, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5263}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5151}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5151}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STCL%3ACurriculum%20learning%20Strategies%20for%20deep%20learning%20image%0A%20%20steganography%20models&body=Title%3A%20STCL%3ACurriculum%20learning%20Strategies%20for%20deep%20learning%20image%0A%20%20steganography%20models%0AAuthor%3A%20Fengchun%20Liu%20and%20Tong%20Zhang%20and%20Chunying%20Zhang%0AAbstract%3A%20%20%20Aiming%20at%20the%20problems%20of%20poor%20quality%20of%20steganographic%20images%20and%20slow%0Anetwork%20convergence%20of%20image%20steganography%20models%20based%20on%20deep%20learning%2C%20this%0Apaper%20proposes%20a%20Steganography%20Curriculum%20Learning%20training%20strategy%20%28STCL%29%20for%0Adeep%20learning%20image%20steganography%20models.%20So%20that%20only%20easy%20images%20are%20selected%0Afor%20training%20when%20the%20model%20has%20poor%20fitting%20ability%20at%20the%20initial%20stage%2C%20and%0Agradually%20expand%20to%20more%20difficult%20images%2C%20the%20strategy%20includes%20a%20difficulty%0Aevaluation%20strategy%20based%20on%20the%20teacher%20model%20and%20an%20knee%20point-based%20training%0Ascheduling%20strategy.%20Firstly%2C%20multiple%20teacher%20models%20are%20trained%2C%20and%20the%0Aconsistency%20of%20the%20quality%20of%20steganographic%20images%20under%20multiple%20teacher%0Amodels%20is%20used%20as%20the%20difficulty%20score%20to%20construct%20the%20training%20subsets%20from%0Aeasy%20to%20difficult.%20Secondly%2C%20a%20training%20control%20strategy%20based%20on%20knee%20points%0Ais%20proposed%20to%20reduce%20the%20possibility%20of%20overfitting%20on%20small%20training%20sets%20and%0Aaccelerate%20the%20training%20process.%20Experimental%20results%20on%20three%20large%20public%0Adatasets%2C%20ALASKA2%2C%20VOC2012%20and%20ImageNet%2C%20show%20that%20the%20proposed%20image%0Asteganography%20scheme%20is%20able%20to%20improve%20the%20model%20performance%20under%20multiple%0Aalgorithmic%20frameworks%2C%20which%20not%20only%20has%20a%20high%20PSNR%2C%20SSIM%20score%2C%20and%0Adecoding%20accuracy%2C%20but%20also%20the%20steganographic%20images%20generated%20by%20the%20model%0Aunder%20the%20training%20of%20the%20STCL%20strategy%20have%20a%20low%20steganography%20analysis%0Ascores.%20You%20can%20find%20our%20code%20at%0A%5Chref%7Bhttps%3A//github.com/chaos-boops/STCL%7D%7Bhttps%3A//github.com/chaos-boops/STCL%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTCL%253ACurriculum%2520learning%2520Strategies%2520for%2520deep%2520learning%2520image%250A%2520%2520steganography%2520models%26entry.906535625%3DFengchun%2520Liu%2520and%2520Tong%2520Zhang%2520and%2520Chunying%2520Zhang%26entry.1292438233%3D%2520%2520Aiming%2520at%2520the%2520problems%2520of%2520poor%2520quality%2520of%2520steganographic%2520images%2520and%2520slow%250Anetwork%2520convergence%2520of%2520image%2520steganography%2520models%2520based%2520on%2520deep%2520learning%252C%2520this%250Apaper%2520proposes%2520a%2520Steganography%2520Curriculum%2520Learning%2520training%2520strategy%2520%2528STCL%2529%2520for%250Adeep%2520learning%2520image%2520steganography%2520models.%2520So%2520that%2520only%2520easy%2520images%2520are%2520selected%250Afor%2520training%2520when%2520the%2520model%2520has%2520poor%2520fitting%2520ability%2520at%2520the%2520initial%2520stage%252C%2520and%250Agradually%2520expand%2520to%2520more%2520difficult%2520images%252C%2520the%2520strategy%2520includes%2520a%2520difficulty%250Aevaluation%2520strategy%2520based%2520on%2520the%2520teacher%2520model%2520and%2520an%2520knee%2520point-based%2520training%250Ascheduling%2520strategy.%2520Firstly%252C%2520multiple%2520teacher%2520models%2520are%2520trained%252C%2520and%2520the%250Aconsistency%2520of%2520the%2520quality%2520of%2520steganographic%2520images%2520under%2520multiple%2520teacher%250Amodels%2520is%2520used%2520as%2520the%2520difficulty%2520score%2520to%2520construct%2520the%2520training%2520subsets%2520from%250Aeasy%2520to%2520difficult.%2520Secondly%252C%2520a%2520training%2520control%2520strategy%2520based%2520on%2520knee%2520points%250Ais%2520proposed%2520to%2520reduce%2520the%2520possibility%2520of%2520overfitting%2520on%2520small%2520training%2520sets%2520and%250Aaccelerate%2520the%2520training%2520process.%2520Experimental%2520results%2520on%2520three%2520large%2520public%250Adatasets%252C%2520ALASKA2%252C%2520VOC2012%2520and%2520ImageNet%252C%2520show%2520that%2520the%2520proposed%2520image%250Asteganography%2520scheme%2520is%2520able%2520to%2520improve%2520the%2520model%2520performance%2520under%2520multiple%250Aalgorithmic%2520frameworks%252C%2520which%2520not%2520only%2520has%2520a%2520high%2520PSNR%252C%2520SSIM%2520score%252C%2520and%250Adecoding%2520accuracy%252C%2520but%2520also%2520the%2520steganographic%2520images%2520generated%2520by%2520the%2520model%250Aunder%2520the%2520training%2520of%2520the%2520STCL%2520strategy%2520have%2520a%2520low%2520steganography%2520analysis%250Ascores.%2520You%2520can%2520find%2520our%2520code%2520at%250A%255Chref%257Bhttps%253A//github.com/chaos-boops/STCL%257D%257Bhttps%253A//github.com/chaos-boops/STCL%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STCL%3ACurriculum%20learning%20Strategies%20for%20deep%20learning%20image%0A%20%20steganography%20models&entry.906535625=Fengchun%20Liu%20and%20Tong%20Zhang%20and%20Chunying%20Zhang&entry.1292438233=%20%20Aiming%20at%20the%20problems%20of%20poor%20quality%20of%20steganographic%20images%20and%20slow%0Anetwork%20convergence%20of%20image%20steganography%20models%20based%20on%20deep%20learning%2C%20this%0Apaper%20proposes%20a%20Steganography%20Curriculum%20Learning%20training%20strategy%20%28STCL%29%20for%0Adeep%20learning%20image%20steganography%20models.%20So%20that%20only%20easy%20images%20are%20selected%0Afor%20training%20when%20the%20model%20has%20poor%20fitting%20ability%20at%20the%20initial%20stage%2C%20and%0Agradually%20expand%20to%20more%20difficult%20images%2C%20the%20strategy%20includes%20a%20difficulty%0Aevaluation%20strategy%20based%20on%20the%20teacher%20model%20and%20an%20knee%20point-based%20training%0Ascheduling%20strategy.%20Firstly%2C%20multiple%20teacher%20models%20are%20trained%2C%20and%20the%0Aconsistency%20of%20the%20quality%20of%20steganographic%20images%20under%20multiple%20teacher%0Amodels%20is%20used%20as%20the%20difficulty%20score%20to%20construct%20the%20training%20subsets%20from%0Aeasy%20to%20difficult.%20Secondly%2C%20a%20training%20control%20strategy%20based%20on%20knee%20points%0Ais%20proposed%20to%20reduce%20the%20possibility%20of%20overfitting%20on%20small%20training%20sets%20and%0Aaccelerate%20the%20training%20process.%20Experimental%20results%20on%20three%20large%20public%0Adatasets%2C%20ALASKA2%2C%20VOC2012%20and%20ImageNet%2C%20show%20that%20the%20proposed%20image%0Asteganography%20scheme%20is%20able%20to%20improve%20the%20model%20performance%20under%20multiple%0Aalgorithmic%20frameworks%2C%20which%20not%20only%20has%20a%20high%20PSNR%2C%20SSIM%20score%2C%20and%0Adecoding%20accuracy%2C%20but%20also%20the%20steganographic%20images%20generated%20by%20the%20model%0Aunder%20the%20training%20of%20the%20STCL%20strategy%20have%20a%20low%20steganography%20analysis%0Ascores.%20You%20can%20find%20our%20code%20at%0A%5Chref%7Bhttps%3A//github.com/chaos-boops/STCL%7D%7Bhttps%3A//github.com/chaos-boops/STCL%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17609v1&entry.124074799=Read"},
{"title": "Disentangling Visual Transformers: Patch-level Interpretability for\n  Image Classification", "author": "Guillaume Jeanneret and Lo\u00efc Simon and Fr\u00e9d\u00e9ric Jurie", "abstract": "  Visual transformers have achieved remarkable performance in image\nclassification tasks, but this performance gain has come at the cost of\ninterpretability. One of the main obstacles to the interpretation of\ntransformers is the self-attention mechanism, which mixes visual information\nacross the whole image in a complex way. In this paper, we propose Hindered\nTransformer (HiT), a novel interpretable by design architecture inspired by\nvisual transformers. Our proposed architecture rethinks the design of\ntransformers to better disentangle patch influences at the classification\nstage. Ultimately, HiT can be interpreted as a linear combination of\npatch-level information. We show that the advantages of our approach in terms\nof explicability come with a reasonable trade-off in performance, making it an\nattractive alternative for applications where interpretability is paramount.\n", "link": "http://arxiv.org/abs/2502.17196v2", "date": "2025-04-24", "relevancy": 2.0677, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5532}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.518}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangling%20Visual%20Transformers%3A%20Patch-level%20Interpretability%20for%0A%20%20Image%20Classification&body=Title%3A%20Disentangling%20Visual%20Transformers%3A%20Patch-level%20Interpretability%20for%0A%20%20Image%20Classification%0AAuthor%3A%20Guillaume%20Jeanneret%20and%20Lo%C3%AFc%20Simon%20and%20Fr%C3%A9d%C3%A9ric%20Jurie%0AAbstract%3A%20%20%20Visual%20transformers%20have%20achieved%20remarkable%20performance%20in%20image%0Aclassification%20tasks%2C%20but%20this%20performance%20gain%20has%20come%20at%20the%20cost%20of%0Ainterpretability.%20One%20of%20the%20main%20obstacles%20to%20the%20interpretation%20of%0Atransformers%20is%20the%20self-attention%20mechanism%2C%20which%20mixes%20visual%20information%0Aacross%20the%20whole%20image%20in%20a%20complex%20way.%20In%20this%20paper%2C%20we%20propose%20Hindered%0ATransformer%20%28HiT%29%2C%20a%20novel%20interpretable%20by%20design%20architecture%20inspired%20by%0Avisual%20transformers.%20Our%20proposed%20architecture%20rethinks%20the%20design%20of%0Atransformers%20to%20better%20disentangle%20patch%20influences%20at%20the%20classification%0Astage.%20Ultimately%2C%20HiT%20can%20be%20interpreted%20as%20a%20linear%20combination%20of%0Apatch-level%20information.%20We%20show%20that%20the%20advantages%20of%20our%20approach%20in%20terms%0Aof%20explicability%20come%20with%20a%20reasonable%20trade-off%20in%20performance%2C%20making%20it%20an%0Aattractive%20alternative%20for%20applications%20where%20interpretability%20is%20paramount.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17196v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangling%2520Visual%2520Transformers%253A%2520Patch-level%2520Interpretability%2520for%250A%2520%2520Image%2520Classification%26entry.906535625%3DGuillaume%2520Jeanneret%2520and%2520Lo%25C3%25AFc%2520Simon%2520and%2520Fr%25C3%25A9d%25C3%25A9ric%2520Jurie%26entry.1292438233%3D%2520%2520Visual%2520transformers%2520have%2520achieved%2520remarkable%2520performance%2520in%2520image%250Aclassification%2520tasks%252C%2520but%2520this%2520performance%2520gain%2520has%2520come%2520at%2520the%2520cost%2520of%250Ainterpretability.%2520One%2520of%2520the%2520main%2520obstacles%2520to%2520the%2520interpretation%2520of%250Atransformers%2520is%2520the%2520self-attention%2520mechanism%252C%2520which%2520mixes%2520visual%2520information%250Aacross%2520the%2520whole%2520image%2520in%2520a%2520complex%2520way.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Hindered%250ATransformer%2520%2528HiT%2529%252C%2520a%2520novel%2520interpretable%2520by%2520design%2520architecture%2520inspired%2520by%250Avisual%2520transformers.%2520Our%2520proposed%2520architecture%2520rethinks%2520the%2520design%2520of%250Atransformers%2520to%2520better%2520disentangle%2520patch%2520influences%2520at%2520the%2520classification%250Astage.%2520Ultimately%252C%2520HiT%2520can%2520be%2520interpreted%2520as%2520a%2520linear%2520combination%2520of%250Apatch-level%2520information.%2520We%2520show%2520that%2520the%2520advantages%2520of%2520our%2520approach%2520in%2520terms%250Aof%2520explicability%2520come%2520with%2520a%2520reasonable%2520trade-off%2520in%2520performance%252C%2520making%2520it%2520an%250Aattractive%2520alternative%2520for%2520applications%2520where%2520interpretability%2520is%2520paramount.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17196v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangling%20Visual%20Transformers%3A%20Patch-level%20Interpretability%20for%0A%20%20Image%20Classification&entry.906535625=Guillaume%20Jeanneret%20and%20Lo%C3%AFc%20Simon%20and%20Fr%C3%A9d%C3%A9ric%20Jurie&entry.1292438233=%20%20Visual%20transformers%20have%20achieved%20remarkable%20performance%20in%20image%0Aclassification%20tasks%2C%20but%20this%20performance%20gain%20has%20come%20at%20the%20cost%20of%0Ainterpretability.%20One%20of%20the%20main%20obstacles%20to%20the%20interpretation%20of%0Atransformers%20is%20the%20self-attention%20mechanism%2C%20which%20mixes%20visual%20information%0Aacross%20the%20whole%20image%20in%20a%20complex%20way.%20In%20this%20paper%2C%20we%20propose%20Hindered%0ATransformer%20%28HiT%29%2C%20a%20novel%20interpretable%20by%20design%20architecture%20inspired%20by%0Avisual%20transformers.%20Our%20proposed%20architecture%20rethinks%20the%20design%20of%0Atransformers%20to%20better%20disentangle%20patch%20influences%20at%20the%20classification%0Astage.%20Ultimately%2C%20HiT%20can%20be%20interpreted%20as%20a%20linear%20combination%20of%0Apatch-level%20information.%20We%20show%20that%20the%20advantages%20of%20our%20approach%20in%20terms%0Aof%20explicability%20come%20with%20a%20reasonable%20trade-off%20in%20performance%2C%20making%20it%20an%0Aattractive%20alternative%20for%20applications%20where%20interpretability%20is%20paramount.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17196v2&entry.124074799=Read"},
{"title": "Contrastive Learning with Synthetic Positives", "author": "Dewen Zeng and Yawen Wu and Xinrong Hu and Xiaowei Xu and Yiyu Shi", "abstract": "  Contrastive learning with the nearest neighbor has proved to be one of the\nmost efficient self-supervised learning (SSL) techniques by utilizing the\nsimilarity of multiple instances within the same class. However, its efficacy\nis constrained as the nearest neighbor algorithm primarily identifies \"easy\"\npositive pairs, where the representations are already closely located in the\nembedding space. In this paper, we introduce a novel approach called\nContrastive Learning with Synthetic Positives (CLSP) that utilizes synthetic\nimages, generated by an unconditional diffusion model, as the additional\npositives to help the model learn from diverse positives. Through feature\ninterpolation in the diffusion model sampling process, we generate images with\ndistinct backgrounds yet similar semantic content to the anchor image. These\nimages are considered \"hard\" positives for the anchor image, and when included\nas supplementary positives in the contrastive loss, they contribute to a\nperformance improvement of over 2% and 1% in linear evaluation compared to the\nprevious NNCLR and All4One methods across multiple benchmark datasets such as\nCIFAR10, achieving state-of-the-art methods. On transfer learning benchmarks,\nCLSP outperforms existing SSL frameworks on 6 out of 8 downstream datasets. We\nbelieve CLSP establishes a valuable baseline for future SSL studies\nincorporating synthetic data in the training process.\n", "link": "http://arxiv.org/abs/2408.16965v2", "date": "2025-04-24", "relevancy": 2.065, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5207}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.514}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrastive%20Learning%20with%20Synthetic%20Positives&body=Title%3A%20Contrastive%20Learning%20with%20Synthetic%20Positives%0AAuthor%3A%20Dewen%20Zeng%20and%20Yawen%20Wu%20and%20Xinrong%20Hu%20and%20Xiaowei%20Xu%20and%20Yiyu%20Shi%0AAbstract%3A%20%20%20Contrastive%20learning%20with%20the%20nearest%20neighbor%20has%20proved%20to%20be%20one%20of%20the%0Amost%20efficient%20self-supervised%20learning%20%28SSL%29%20techniques%20by%20utilizing%20the%0Asimilarity%20of%20multiple%20instances%20within%20the%20same%20class.%20However%2C%20its%20efficacy%0Ais%20constrained%20as%20the%20nearest%20neighbor%20algorithm%20primarily%20identifies%20%22easy%22%0Apositive%20pairs%2C%20where%20the%20representations%20are%20already%20closely%20located%20in%20the%0Aembedding%20space.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20approach%20called%0AContrastive%20Learning%20with%20Synthetic%20Positives%20%28CLSP%29%20that%20utilizes%20synthetic%0Aimages%2C%20generated%20by%20an%20unconditional%20diffusion%20model%2C%20as%20the%20additional%0Apositives%20to%20help%20the%20model%20learn%20from%20diverse%20positives.%20Through%20feature%0Ainterpolation%20in%20the%20diffusion%20model%20sampling%20process%2C%20we%20generate%20images%20with%0Adistinct%20backgrounds%20yet%20similar%20semantic%20content%20to%20the%20anchor%20image.%20These%0Aimages%20are%20considered%20%22hard%22%20positives%20for%20the%20anchor%20image%2C%20and%20when%20included%0Aas%20supplementary%20positives%20in%20the%20contrastive%20loss%2C%20they%20contribute%20to%20a%0Aperformance%20improvement%20of%20over%202%25%20and%201%25%20in%20linear%20evaluation%20compared%20to%20the%0Aprevious%20NNCLR%20and%20All4One%20methods%20across%20multiple%20benchmark%20datasets%20such%20as%0ACIFAR10%2C%20achieving%20state-of-the-art%20methods.%20On%20transfer%20learning%20benchmarks%2C%0ACLSP%20outperforms%20existing%20SSL%20frameworks%20on%206%20out%20of%208%20downstream%20datasets.%20We%0Abelieve%20CLSP%20establishes%20a%20valuable%20baseline%20for%20future%20SSL%20studies%0Aincorporating%20synthetic%20data%20in%20the%20training%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16965v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrastive%2520Learning%2520with%2520Synthetic%2520Positives%26entry.906535625%3DDewen%2520Zeng%2520and%2520Yawen%2520Wu%2520and%2520Xinrong%2520Hu%2520and%2520Xiaowei%2520Xu%2520and%2520Yiyu%2520Shi%26entry.1292438233%3D%2520%2520Contrastive%2520learning%2520with%2520the%2520nearest%2520neighbor%2520has%2520proved%2520to%2520be%2520one%2520of%2520the%250Amost%2520efficient%2520self-supervised%2520learning%2520%2528SSL%2529%2520techniques%2520by%2520utilizing%2520the%250Asimilarity%2520of%2520multiple%2520instances%2520within%2520the%2520same%2520class.%2520However%252C%2520its%2520efficacy%250Ais%2520constrained%2520as%2520the%2520nearest%2520neighbor%2520algorithm%2520primarily%2520identifies%2520%2522easy%2522%250Apositive%2520pairs%252C%2520where%2520the%2520representations%2520are%2520already%2520closely%2520located%2520in%2520the%250Aembedding%2520space.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520called%250AContrastive%2520Learning%2520with%2520Synthetic%2520Positives%2520%2528CLSP%2529%2520that%2520utilizes%2520synthetic%250Aimages%252C%2520generated%2520by%2520an%2520unconditional%2520diffusion%2520model%252C%2520as%2520the%2520additional%250Apositives%2520to%2520help%2520the%2520model%2520learn%2520from%2520diverse%2520positives.%2520Through%2520feature%250Ainterpolation%2520in%2520the%2520diffusion%2520model%2520sampling%2520process%252C%2520we%2520generate%2520images%2520with%250Adistinct%2520backgrounds%2520yet%2520similar%2520semantic%2520content%2520to%2520the%2520anchor%2520image.%2520These%250Aimages%2520are%2520considered%2520%2522hard%2522%2520positives%2520for%2520the%2520anchor%2520image%252C%2520and%2520when%2520included%250Aas%2520supplementary%2520positives%2520in%2520the%2520contrastive%2520loss%252C%2520they%2520contribute%2520to%2520a%250Aperformance%2520improvement%2520of%2520over%25202%2525%2520and%25201%2525%2520in%2520linear%2520evaluation%2520compared%2520to%2520the%250Aprevious%2520NNCLR%2520and%2520All4One%2520methods%2520across%2520multiple%2520benchmark%2520datasets%2520such%2520as%250ACIFAR10%252C%2520achieving%2520state-of-the-art%2520methods.%2520On%2520transfer%2520learning%2520benchmarks%252C%250ACLSP%2520outperforms%2520existing%2520SSL%2520frameworks%2520on%25206%2520out%2520of%25208%2520downstream%2520datasets.%2520We%250Abelieve%2520CLSP%2520establishes%2520a%2520valuable%2520baseline%2520for%2520future%2520SSL%2520studies%250Aincorporating%2520synthetic%2520data%2520in%2520the%2520training%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16965v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Learning%20with%20Synthetic%20Positives&entry.906535625=Dewen%20Zeng%20and%20Yawen%20Wu%20and%20Xinrong%20Hu%20and%20Xiaowei%20Xu%20and%20Yiyu%20Shi&entry.1292438233=%20%20Contrastive%20learning%20with%20the%20nearest%20neighbor%20has%20proved%20to%20be%20one%20of%20the%0Amost%20efficient%20self-supervised%20learning%20%28SSL%29%20techniques%20by%20utilizing%20the%0Asimilarity%20of%20multiple%20instances%20within%20the%20same%20class.%20However%2C%20its%20efficacy%0Ais%20constrained%20as%20the%20nearest%20neighbor%20algorithm%20primarily%20identifies%20%22easy%22%0Apositive%20pairs%2C%20where%20the%20representations%20are%20already%20closely%20located%20in%20the%0Aembedding%20space.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20approach%20called%0AContrastive%20Learning%20with%20Synthetic%20Positives%20%28CLSP%29%20that%20utilizes%20synthetic%0Aimages%2C%20generated%20by%20an%20unconditional%20diffusion%20model%2C%20as%20the%20additional%0Apositives%20to%20help%20the%20model%20learn%20from%20diverse%20positives.%20Through%20feature%0Ainterpolation%20in%20the%20diffusion%20model%20sampling%20process%2C%20we%20generate%20images%20with%0Adistinct%20backgrounds%20yet%20similar%20semantic%20content%20to%20the%20anchor%20image.%20These%0Aimages%20are%20considered%20%22hard%22%20positives%20for%20the%20anchor%20image%2C%20and%20when%20included%0Aas%20supplementary%20positives%20in%20the%20contrastive%20loss%2C%20they%20contribute%20to%20a%0Aperformance%20improvement%20of%20over%202%25%20and%201%25%20in%20linear%20evaluation%20compared%20to%20the%0Aprevious%20NNCLR%20and%20All4One%20methods%20across%20multiple%20benchmark%20datasets%20such%20as%0ACIFAR10%2C%20achieving%20state-of-the-art%20methods.%20On%20transfer%20learning%20benchmarks%2C%0ACLSP%20outperforms%20existing%20SSL%20frameworks%20on%206%20out%20of%208%20downstream%20datasets.%20We%0Abelieve%20CLSP%20establishes%20a%20valuable%20baseline%20for%20future%20SSL%20studies%0Aincorporating%20synthetic%20data%20in%20the%20training%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16965v2&entry.124074799=Read"},
{"title": "Towards Machine-Generated Code for the Resolution of User Intentions", "author": "Justus Flerlage and Ilja Behnke and Odej Kao", "abstract": "  The growing capabilities of Artificial Intelligence (AI), particularly Large\nLanguage Models (LLMs), prompt a reassessment of the interaction mechanisms\nbetween users and their devices. Currently, users are required to use a set of\nhigh-level applications to achieve their desired results. However, the advent\nof AI may signal a shift in this regard, as its capabilities have generated\nnovel prospects for user-provided intent resolution through the deployment of\nmodel-generated code, which is tantamount to the generation of workflows\ncomprising a multitude of interdependent steps. This development represents a\nsignificant progression in the realm of hybrid workflows, where human and\nartificial intelligence collaborate to address user intentions, with the former\nresponsible for defining these intentions and the latter for implementing the\nsolutions to address them. In this paper, we investigate the feasibility of\ngenerating and executing workflows through code generation that results from\nprompting an LLM with a concrete user intention, such as \\emph{Please send my\ncar title to my insurance company}, and a simplified application programming\ninterface for a GUI-less operating system. We provide in-depth analysis and\ncomparison of various user intentions, the resulting code, and its execution.\nThe findings demonstrate a general feasibility of our approach and that the\nemployed LLM, GPT-4o-mini, exhibits remarkable proficiency in the generation of\ncode-oriented workflows in accordance with provided user intentions.\n", "link": "http://arxiv.org/abs/2504.17531v1", "date": "2025-04-24", "relevancy": 2.0604, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5413}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5077}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Machine-Generated%20Code%20for%20the%20Resolution%20of%20User%20Intentions&body=Title%3A%20Towards%20Machine-Generated%20Code%20for%20the%20Resolution%20of%20User%20Intentions%0AAuthor%3A%20Justus%20Flerlage%20and%20Ilja%20Behnke%20and%20Odej%20Kao%0AAbstract%3A%20%20%20The%20growing%20capabilities%20of%20Artificial%20Intelligence%20%28AI%29%2C%20particularly%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20prompt%20a%20reassessment%20of%20the%20interaction%20mechanisms%0Abetween%20users%20and%20their%20devices.%20Currently%2C%20users%20are%20required%20to%20use%20a%20set%20of%0Ahigh-level%20applications%20to%20achieve%20their%20desired%20results.%20However%2C%20the%20advent%0Aof%20AI%20may%20signal%20a%20shift%20in%20this%20regard%2C%20as%20its%20capabilities%20have%20generated%0Anovel%20prospects%20for%20user-provided%20intent%20resolution%20through%20the%20deployment%20of%0Amodel-generated%20code%2C%20which%20is%20tantamount%20to%20the%20generation%20of%20workflows%0Acomprising%20a%20multitude%20of%20interdependent%20steps.%20This%20development%20represents%20a%0Asignificant%20progression%20in%20the%20realm%20of%20hybrid%20workflows%2C%20where%20human%20and%0Aartificial%20intelligence%20collaborate%20to%20address%20user%20intentions%2C%20with%20the%20former%0Aresponsible%20for%20defining%20these%20intentions%20and%20the%20latter%20for%20implementing%20the%0Asolutions%20to%20address%20them.%20In%20this%20paper%2C%20we%20investigate%20the%20feasibility%20of%0Agenerating%20and%20executing%20workflows%20through%20code%20generation%20that%20results%20from%0Aprompting%20an%20LLM%20with%20a%20concrete%20user%20intention%2C%20such%20as%20%5Cemph%7BPlease%20send%20my%0Acar%20title%20to%20my%20insurance%20company%7D%2C%20and%20a%20simplified%20application%20programming%0Ainterface%20for%20a%20GUI-less%20operating%20system.%20We%20provide%20in-depth%20analysis%20and%0Acomparison%20of%20various%20user%20intentions%2C%20the%20resulting%20code%2C%20and%20its%20execution.%0AThe%20findings%20demonstrate%20a%20general%20feasibility%20of%20our%20approach%20and%20that%20the%0Aemployed%20LLM%2C%20GPT-4o-mini%2C%20exhibits%20remarkable%20proficiency%20in%20the%20generation%20of%0Acode-oriented%20workflows%20in%20accordance%20with%20provided%20user%20intentions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Machine-Generated%2520Code%2520for%2520the%2520Resolution%2520of%2520User%2520Intentions%26entry.906535625%3DJustus%2520Flerlage%2520and%2520Ilja%2520Behnke%2520and%2520Odej%2520Kao%26entry.1292438233%3D%2520%2520The%2520growing%2520capabilities%2520of%2520Artificial%2520Intelligence%2520%2528AI%2529%252C%2520particularly%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%252C%2520prompt%2520a%2520reassessment%2520of%2520the%2520interaction%2520mechanisms%250Abetween%2520users%2520and%2520their%2520devices.%2520Currently%252C%2520users%2520are%2520required%2520to%2520use%2520a%2520set%2520of%250Ahigh-level%2520applications%2520to%2520achieve%2520their%2520desired%2520results.%2520However%252C%2520the%2520advent%250Aof%2520AI%2520may%2520signal%2520a%2520shift%2520in%2520this%2520regard%252C%2520as%2520its%2520capabilities%2520have%2520generated%250Anovel%2520prospects%2520for%2520user-provided%2520intent%2520resolution%2520through%2520the%2520deployment%2520of%250Amodel-generated%2520code%252C%2520which%2520is%2520tantamount%2520to%2520the%2520generation%2520of%2520workflows%250Acomprising%2520a%2520multitude%2520of%2520interdependent%2520steps.%2520This%2520development%2520represents%2520a%250Asignificant%2520progression%2520in%2520the%2520realm%2520of%2520hybrid%2520workflows%252C%2520where%2520human%2520and%250Aartificial%2520intelligence%2520collaborate%2520to%2520address%2520user%2520intentions%252C%2520with%2520the%2520former%250Aresponsible%2520for%2520defining%2520these%2520intentions%2520and%2520the%2520latter%2520for%2520implementing%2520the%250Asolutions%2520to%2520address%2520them.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520feasibility%2520of%250Agenerating%2520and%2520executing%2520workflows%2520through%2520code%2520generation%2520that%2520results%2520from%250Aprompting%2520an%2520LLM%2520with%2520a%2520concrete%2520user%2520intention%252C%2520such%2520as%2520%255Cemph%257BPlease%2520send%2520my%250Acar%2520title%2520to%2520my%2520insurance%2520company%257D%252C%2520and%2520a%2520simplified%2520application%2520programming%250Ainterface%2520for%2520a%2520GUI-less%2520operating%2520system.%2520We%2520provide%2520in-depth%2520analysis%2520and%250Acomparison%2520of%2520various%2520user%2520intentions%252C%2520the%2520resulting%2520code%252C%2520and%2520its%2520execution.%250AThe%2520findings%2520demonstrate%2520a%2520general%2520feasibility%2520of%2520our%2520approach%2520and%2520that%2520the%250Aemployed%2520LLM%252C%2520GPT-4o-mini%252C%2520exhibits%2520remarkable%2520proficiency%2520in%2520the%2520generation%2520of%250Acode-oriented%2520workflows%2520in%2520accordance%2520with%2520provided%2520user%2520intentions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Machine-Generated%20Code%20for%20the%20Resolution%20of%20User%20Intentions&entry.906535625=Justus%20Flerlage%20and%20Ilja%20Behnke%20and%20Odej%20Kao&entry.1292438233=%20%20The%20growing%20capabilities%20of%20Artificial%20Intelligence%20%28AI%29%2C%20particularly%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20prompt%20a%20reassessment%20of%20the%20interaction%20mechanisms%0Abetween%20users%20and%20their%20devices.%20Currently%2C%20users%20are%20required%20to%20use%20a%20set%20of%0Ahigh-level%20applications%20to%20achieve%20their%20desired%20results.%20However%2C%20the%20advent%0Aof%20AI%20may%20signal%20a%20shift%20in%20this%20regard%2C%20as%20its%20capabilities%20have%20generated%0Anovel%20prospects%20for%20user-provided%20intent%20resolution%20through%20the%20deployment%20of%0Amodel-generated%20code%2C%20which%20is%20tantamount%20to%20the%20generation%20of%20workflows%0Acomprising%20a%20multitude%20of%20interdependent%20steps.%20This%20development%20represents%20a%0Asignificant%20progression%20in%20the%20realm%20of%20hybrid%20workflows%2C%20where%20human%20and%0Aartificial%20intelligence%20collaborate%20to%20address%20user%20intentions%2C%20with%20the%20former%0Aresponsible%20for%20defining%20these%20intentions%20and%20the%20latter%20for%20implementing%20the%0Asolutions%20to%20address%20them.%20In%20this%20paper%2C%20we%20investigate%20the%20feasibility%20of%0Agenerating%20and%20executing%20workflows%20through%20code%20generation%20that%20results%20from%0Aprompting%20an%20LLM%20with%20a%20concrete%20user%20intention%2C%20such%20as%20%5Cemph%7BPlease%20send%20my%0Acar%20title%20to%20my%20insurance%20company%7D%2C%20and%20a%20simplified%20application%20programming%0Ainterface%20for%20a%20GUI-less%20operating%20system.%20We%20provide%20in-depth%20analysis%20and%0Acomparison%20of%20various%20user%20intentions%2C%20the%20resulting%20code%2C%20and%20its%20execution.%0AThe%20findings%20demonstrate%20a%20general%20feasibility%20of%20our%20approach%20and%20that%20the%0Aemployed%20LLM%2C%20GPT-4o-mini%2C%20exhibits%20remarkable%20proficiency%20in%20the%20generation%20of%0Acode-oriented%20workflows%20in%20accordance%20with%20provided%20user%20intentions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17531v1&entry.124074799=Read"},
{"title": "HalluLens: LLM Hallucination Benchmark", "author": "Yejin Bang and Ziwei Ji and Alan Schelten and Anthony Hartshorn and Tara Fowler and Cheng Zhang and Nicola Cancedda and Pascale Fung", "abstract": "  Large language models (LLMs) often generate responses that deviate from user\ninput or training data, a phenomenon known as \"hallucination.\" These\nhallucinations undermine user trust and hinder the adoption of generative AI\nsystems. Addressing hallucinations is essential for the advancement of LLMs.\nThis paper introduces a comprehensive hallucination benchmark, incorporating\nboth new extrinsic and existing intrinsic evaluation tasks, built upon clear\ntaxonomy of hallucination. A major challenge in benchmarking hallucinations is\nthe lack of a unified framework due to inconsistent definitions and\ncategorizations. We disentangle LLM hallucination from \"factuality,\" proposing\na clear taxonomy that distinguishes between extrinsic and intrinsic\nhallucinations, to promote consistency and facilitate research. Extrinsic\nhallucinations, where the generated content is not consistent with the training\ndata, are increasingly important as LLMs evolve. Our benchmark includes dynamic\ntest set generation to mitigate data leakage and ensure robustness against such\nleakage. We also analyze existing benchmarks, highlighting their limitations\nand saturation. The work aims to: (1) establish a clear taxonomy of\nhallucinations, (2) introduce new extrinsic hallucination tasks, with data that\ncan be dynamically regenerated to prevent saturation by leakage, (3) provide a\ncomprehensive analysis of existing benchmarks, distinguishing them from\nfactuality evaluations.\n", "link": "http://arxiv.org/abs/2504.17550v1", "date": "2025-04-24", "relevancy": 2.0598, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5316}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5116}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HalluLens%3A%20LLM%20Hallucination%20Benchmark&body=Title%3A%20HalluLens%3A%20LLM%20Hallucination%20Benchmark%0AAuthor%3A%20Yejin%20Bang%20and%20Ziwei%20Ji%20and%20Alan%20Schelten%20and%20Anthony%20Hartshorn%20and%20Tara%20Fowler%20and%20Cheng%20Zhang%20and%20Nicola%20Cancedda%20and%20Pascale%20Fung%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20often%20generate%20responses%20that%20deviate%20from%20user%0Ainput%20or%20training%20data%2C%20a%20phenomenon%20known%20as%20%22hallucination.%22%20These%0Ahallucinations%20undermine%20user%20trust%20and%20hinder%20the%20adoption%20of%20generative%20AI%0Asystems.%20Addressing%20hallucinations%20is%20essential%20for%20the%20advancement%20of%20LLMs.%0AThis%20paper%20introduces%20a%20comprehensive%20hallucination%20benchmark%2C%20incorporating%0Aboth%20new%20extrinsic%20and%20existing%20intrinsic%20evaluation%20tasks%2C%20built%20upon%20clear%0Ataxonomy%20of%20hallucination.%20A%20major%20challenge%20in%20benchmarking%20hallucinations%20is%0Athe%20lack%20of%20a%20unified%20framework%20due%20to%20inconsistent%20definitions%20and%0Acategorizations.%20We%20disentangle%20LLM%20hallucination%20from%20%22factuality%2C%22%20proposing%0Aa%20clear%20taxonomy%20that%20distinguishes%20between%20extrinsic%20and%20intrinsic%0Ahallucinations%2C%20to%20promote%20consistency%20and%20facilitate%20research.%20Extrinsic%0Ahallucinations%2C%20where%20the%20generated%20content%20is%20not%20consistent%20with%20the%20training%0Adata%2C%20are%20increasingly%20important%20as%20LLMs%20evolve.%20Our%20benchmark%20includes%20dynamic%0Atest%20set%20generation%20to%20mitigate%20data%20leakage%20and%20ensure%20robustness%20against%20such%0Aleakage.%20We%20also%20analyze%20existing%20benchmarks%2C%20highlighting%20their%20limitations%0Aand%20saturation.%20The%20work%20aims%20to%3A%20%281%29%20establish%20a%20clear%20taxonomy%20of%0Ahallucinations%2C%20%282%29%20introduce%20new%20extrinsic%20hallucination%20tasks%2C%20with%20data%20that%0Acan%20be%20dynamically%20regenerated%20to%20prevent%20saturation%20by%20leakage%2C%20%283%29%20provide%20a%0Acomprehensive%20analysis%20of%20existing%20benchmarks%2C%20distinguishing%20them%20from%0Afactuality%20evaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17550v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHalluLens%253A%2520LLM%2520Hallucination%2520Benchmark%26entry.906535625%3DYejin%2520Bang%2520and%2520Ziwei%2520Ji%2520and%2520Alan%2520Schelten%2520and%2520Anthony%2520Hartshorn%2520and%2520Tara%2520Fowler%2520and%2520Cheng%2520Zhang%2520and%2520Nicola%2520Cancedda%2520and%2520Pascale%2520Fung%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520often%2520generate%2520responses%2520that%2520deviate%2520from%2520user%250Ainput%2520or%2520training%2520data%252C%2520a%2520phenomenon%2520known%2520as%2520%2522hallucination.%2522%2520These%250Ahallucinations%2520undermine%2520user%2520trust%2520and%2520hinder%2520the%2520adoption%2520of%2520generative%2520AI%250Asystems.%2520Addressing%2520hallucinations%2520is%2520essential%2520for%2520the%2520advancement%2520of%2520LLMs.%250AThis%2520paper%2520introduces%2520a%2520comprehensive%2520hallucination%2520benchmark%252C%2520incorporating%250Aboth%2520new%2520extrinsic%2520and%2520existing%2520intrinsic%2520evaluation%2520tasks%252C%2520built%2520upon%2520clear%250Ataxonomy%2520of%2520hallucination.%2520A%2520major%2520challenge%2520in%2520benchmarking%2520hallucinations%2520is%250Athe%2520lack%2520of%2520a%2520unified%2520framework%2520due%2520to%2520inconsistent%2520definitions%2520and%250Acategorizations.%2520We%2520disentangle%2520LLM%2520hallucination%2520from%2520%2522factuality%252C%2522%2520proposing%250Aa%2520clear%2520taxonomy%2520that%2520distinguishes%2520between%2520extrinsic%2520and%2520intrinsic%250Ahallucinations%252C%2520to%2520promote%2520consistency%2520and%2520facilitate%2520research.%2520Extrinsic%250Ahallucinations%252C%2520where%2520the%2520generated%2520content%2520is%2520not%2520consistent%2520with%2520the%2520training%250Adata%252C%2520are%2520increasingly%2520important%2520as%2520LLMs%2520evolve.%2520Our%2520benchmark%2520includes%2520dynamic%250Atest%2520set%2520generation%2520to%2520mitigate%2520data%2520leakage%2520and%2520ensure%2520robustness%2520against%2520such%250Aleakage.%2520We%2520also%2520analyze%2520existing%2520benchmarks%252C%2520highlighting%2520their%2520limitations%250Aand%2520saturation.%2520The%2520work%2520aims%2520to%253A%2520%25281%2529%2520establish%2520a%2520clear%2520taxonomy%2520of%250Ahallucinations%252C%2520%25282%2529%2520introduce%2520new%2520extrinsic%2520hallucination%2520tasks%252C%2520with%2520data%2520that%250Acan%2520be%2520dynamically%2520regenerated%2520to%2520prevent%2520saturation%2520by%2520leakage%252C%2520%25283%2529%2520provide%2520a%250Acomprehensive%2520analysis%2520of%2520existing%2520benchmarks%252C%2520distinguishing%2520them%2520from%250Afactuality%2520evaluations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17550v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HalluLens%3A%20LLM%20Hallucination%20Benchmark&entry.906535625=Yejin%20Bang%20and%20Ziwei%20Ji%20and%20Alan%20Schelten%20and%20Anthony%20Hartshorn%20and%20Tara%20Fowler%20and%20Cheng%20Zhang%20and%20Nicola%20Cancedda%20and%20Pascale%20Fung&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20often%20generate%20responses%20that%20deviate%20from%20user%0Ainput%20or%20training%20data%2C%20a%20phenomenon%20known%20as%20%22hallucination.%22%20These%0Ahallucinations%20undermine%20user%20trust%20and%20hinder%20the%20adoption%20of%20generative%20AI%0Asystems.%20Addressing%20hallucinations%20is%20essential%20for%20the%20advancement%20of%20LLMs.%0AThis%20paper%20introduces%20a%20comprehensive%20hallucination%20benchmark%2C%20incorporating%0Aboth%20new%20extrinsic%20and%20existing%20intrinsic%20evaluation%20tasks%2C%20built%20upon%20clear%0Ataxonomy%20of%20hallucination.%20A%20major%20challenge%20in%20benchmarking%20hallucinations%20is%0Athe%20lack%20of%20a%20unified%20framework%20due%20to%20inconsistent%20definitions%20and%0Acategorizations.%20We%20disentangle%20LLM%20hallucination%20from%20%22factuality%2C%22%20proposing%0Aa%20clear%20taxonomy%20that%20distinguishes%20between%20extrinsic%20and%20intrinsic%0Ahallucinations%2C%20to%20promote%20consistency%20and%20facilitate%20research.%20Extrinsic%0Ahallucinations%2C%20where%20the%20generated%20content%20is%20not%20consistent%20with%20the%20training%0Adata%2C%20are%20increasingly%20important%20as%20LLMs%20evolve.%20Our%20benchmark%20includes%20dynamic%0Atest%20set%20generation%20to%20mitigate%20data%20leakage%20and%20ensure%20robustness%20against%20such%0Aleakage.%20We%20also%20analyze%20existing%20benchmarks%2C%20highlighting%20their%20limitations%0Aand%20saturation.%20The%20work%20aims%20to%3A%20%281%29%20establish%20a%20clear%20taxonomy%20of%0Ahallucinations%2C%20%282%29%20introduce%20new%20extrinsic%20hallucination%20tasks%2C%20with%20data%20that%0Acan%20be%20dynamically%20regenerated%20to%20prevent%20saturation%20by%20leakage%2C%20%283%29%20provide%20a%0Acomprehensive%20analysis%20of%20existing%20benchmarks%2C%20distinguishing%20them%20from%0Afactuality%20evaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17550v1&entry.124074799=Read"},
{"title": "INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language\n  Models", "author": "Jarne Thys and Sebe Vanbrabant and Davy Vanacken and Gustavo Rovelo Ruiz", "abstract": "  The rise of AI, especially Large Language Models, presents challenges and\nopportunities to integrate such technology into the classroom. AI has the\npotential to revolutionize education by helping teaching staff with various\ntasks, such as personalizing their teaching methods, but it also raises\nconcerns, for example, about the degradation of student-teacher interactions\nand user privacy. This paper introduces INSIGHT, a proof of concept to combine\nvarious AI tools to assist teaching staff and students in the process of\nsolving exercises. INSIGHT has a modular design that allows it to be integrated\ninto various higher education courses. We analyze students' questions to an LLM\nby extracting keywords, which we use to dynamically build an FAQ from students'\nquestions and provide new insights for the teaching staff to use for more\npersonalized face-to-face support. Future work could build upon INSIGHT by\nusing the collected data to provide adaptive learning and adjust content based\non student progress and learning styles to offer a more interactive and\ninclusive learning experience.\n", "link": "http://arxiv.org/abs/2504.17677v1", "date": "2025-04-24", "relevancy": 2.0578, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5204}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5204}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20INSIGHT%3A%20Bridging%20the%20Student-Teacher%20Gap%20in%20Times%20of%20Large%20Language%0A%20%20Models&body=Title%3A%20INSIGHT%3A%20Bridging%20the%20Student-Teacher%20Gap%20in%20Times%20of%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Jarne%20Thys%20and%20Sebe%20Vanbrabant%20and%20Davy%20Vanacken%20and%20Gustavo%20Rovelo%20Ruiz%0AAbstract%3A%20%20%20The%20rise%20of%20AI%2C%20especially%20Large%20Language%20Models%2C%20presents%20challenges%20and%0Aopportunities%20to%20integrate%20such%20technology%20into%20the%20classroom.%20AI%20has%20the%0Apotential%20to%20revolutionize%20education%20by%20helping%20teaching%20staff%20with%20various%0Atasks%2C%20such%20as%20personalizing%20their%20teaching%20methods%2C%20but%20it%20also%20raises%0Aconcerns%2C%20for%20example%2C%20about%20the%20degradation%20of%20student-teacher%20interactions%0Aand%20user%20privacy.%20This%20paper%20introduces%20INSIGHT%2C%20a%20proof%20of%20concept%20to%20combine%0Avarious%20AI%20tools%20to%20assist%20teaching%20staff%20and%20students%20in%20the%20process%20of%0Asolving%20exercises.%20INSIGHT%20has%20a%20modular%20design%20that%20allows%20it%20to%20be%20integrated%0Ainto%20various%20higher%20education%20courses.%20We%20analyze%20students%27%20questions%20to%20an%20LLM%0Aby%20extracting%20keywords%2C%20which%20we%20use%20to%20dynamically%20build%20an%20FAQ%20from%20students%27%0Aquestions%20and%20provide%20new%20insights%20for%20the%20teaching%20staff%20to%20use%20for%20more%0Apersonalized%20face-to-face%20support.%20Future%20work%20could%20build%20upon%20INSIGHT%20by%0Ausing%20the%20collected%20data%20to%20provide%20adaptive%20learning%20and%20adjust%20content%20based%0Aon%20student%20progress%20and%20learning%20styles%20to%20offer%20a%20more%20interactive%20and%0Ainclusive%20learning%20experience.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17677v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DINSIGHT%253A%2520Bridging%2520the%2520Student-Teacher%2520Gap%2520in%2520Times%2520of%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DJarne%2520Thys%2520and%2520Sebe%2520Vanbrabant%2520and%2520Davy%2520Vanacken%2520and%2520Gustavo%2520Rovelo%2520Ruiz%26entry.1292438233%3D%2520%2520The%2520rise%2520of%2520AI%252C%2520especially%2520Large%2520Language%2520Models%252C%2520presents%2520challenges%2520and%250Aopportunities%2520to%2520integrate%2520such%2520technology%2520into%2520the%2520classroom.%2520AI%2520has%2520the%250Apotential%2520to%2520revolutionize%2520education%2520by%2520helping%2520teaching%2520staff%2520with%2520various%250Atasks%252C%2520such%2520as%2520personalizing%2520their%2520teaching%2520methods%252C%2520but%2520it%2520also%2520raises%250Aconcerns%252C%2520for%2520example%252C%2520about%2520the%2520degradation%2520of%2520student-teacher%2520interactions%250Aand%2520user%2520privacy.%2520This%2520paper%2520introduces%2520INSIGHT%252C%2520a%2520proof%2520of%2520concept%2520to%2520combine%250Avarious%2520AI%2520tools%2520to%2520assist%2520teaching%2520staff%2520and%2520students%2520in%2520the%2520process%2520of%250Asolving%2520exercises.%2520INSIGHT%2520has%2520a%2520modular%2520design%2520that%2520allows%2520it%2520to%2520be%2520integrated%250Ainto%2520various%2520higher%2520education%2520courses.%2520We%2520analyze%2520students%2527%2520questions%2520to%2520an%2520LLM%250Aby%2520extracting%2520keywords%252C%2520which%2520we%2520use%2520to%2520dynamically%2520build%2520an%2520FAQ%2520from%2520students%2527%250Aquestions%2520and%2520provide%2520new%2520insights%2520for%2520the%2520teaching%2520staff%2520to%2520use%2520for%2520more%250Apersonalized%2520face-to-face%2520support.%2520Future%2520work%2520could%2520build%2520upon%2520INSIGHT%2520by%250Ausing%2520the%2520collected%2520data%2520to%2520provide%2520adaptive%2520learning%2520and%2520adjust%2520content%2520based%250Aon%2520student%2520progress%2520and%2520learning%2520styles%2520to%2520offer%2520a%2520more%2520interactive%2520and%250Ainclusive%2520learning%2520experience.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17677v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=INSIGHT%3A%20Bridging%20the%20Student-Teacher%20Gap%20in%20Times%20of%20Large%20Language%0A%20%20Models&entry.906535625=Jarne%20Thys%20and%20Sebe%20Vanbrabant%20and%20Davy%20Vanacken%20and%20Gustavo%20Rovelo%20Ruiz&entry.1292438233=%20%20The%20rise%20of%20AI%2C%20especially%20Large%20Language%20Models%2C%20presents%20challenges%20and%0Aopportunities%20to%20integrate%20such%20technology%20into%20the%20classroom.%20AI%20has%20the%0Apotential%20to%20revolutionize%20education%20by%20helping%20teaching%20staff%20with%20various%0Atasks%2C%20such%20as%20personalizing%20their%20teaching%20methods%2C%20but%20it%20also%20raises%0Aconcerns%2C%20for%20example%2C%20about%20the%20degradation%20of%20student-teacher%20interactions%0Aand%20user%20privacy.%20This%20paper%20introduces%20INSIGHT%2C%20a%20proof%20of%20concept%20to%20combine%0Avarious%20AI%20tools%20to%20assist%20teaching%20staff%20and%20students%20in%20the%20process%20of%0Asolving%20exercises.%20INSIGHT%20has%20a%20modular%20design%20that%20allows%20it%20to%20be%20integrated%0Ainto%20various%20higher%20education%20courses.%20We%20analyze%20students%27%20questions%20to%20an%20LLM%0Aby%20extracting%20keywords%2C%20which%20we%20use%20to%20dynamically%20build%20an%20FAQ%20from%20students%27%0Aquestions%20and%20provide%20new%20insights%20for%20the%20teaching%20staff%20to%20use%20for%20more%0Apersonalized%20face-to-face%20support.%20Future%20work%20could%20build%20upon%20INSIGHT%20by%0Ausing%20the%20collected%20data%20to%20provide%20adaptive%20learning%20and%20adjust%20content%20based%0Aon%20student%20progress%20and%20learning%20styles%20to%20offer%20a%20more%20interactive%20and%0Ainclusive%20learning%20experience.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17677v1&entry.124074799=Read"},
{"title": "AgentsCoMerge: Large Language Model Empowered Collaborative Decision\n  Making for Ramp Merging", "author": "Senkang Hu and Zhengru Fang and Zihan Fang and Yiqin Deng and Xianhao Chen and Yuguang Fang and Sam Kwong", "abstract": "  Ramp merging is one of the bottlenecks in traffic systems, which commonly\ncause traffic congestion, accidents, and severe carbon emissions. In order to\naddress this essential issue and enhance the safety and efficiency of connected\nand autonomous vehicles (CAVs) at multi-lane merging zones, we propose a novel\ncollaborative decision-making framework, named AgentsCoMerge, to leverage large\nlanguage models (LLMs). Specifically, we first design a scene observation and\nunderstanding module to allow an agent to capture the traffic environment. Then\nwe propose a hierarchical planning module to enable the agent to make decisions\nand plan trajectories based on the observation and the agent's own state. In\naddition, in order to facilitate collaboration among multiple agents, we\nintroduce a communication module to enable the surrounding agents to exchange\nnecessary information and coordinate their actions. Finally, we develop a\nreinforcement reflection guided training paradigm to further enhance the\ndecision-making capability of the framework. Extensive experiments are\nconducted to evaluate the performance of our proposed method, demonstrating its\nsuperior efficiency and effectiveness for multi-agent collaborative\ndecision-making under various ramp merging scenarios.\n", "link": "http://arxiv.org/abs/2408.03624v2", "date": "2025-04-24", "relevancy": 2.0517, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5205}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5114}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AgentsCoMerge%3A%20Large%20Language%20Model%20Empowered%20Collaborative%20Decision%0A%20%20Making%20for%20Ramp%20Merging&body=Title%3A%20AgentsCoMerge%3A%20Large%20Language%20Model%20Empowered%20Collaborative%20Decision%0A%20%20Making%20for%20Ramp%20Merging%0AAuthor%3A%20Senkang%20Hu%20and%20Zhengru%20Fang%20and%20Zihan%20Fang%20and%20Yiqin%20Deng%20and%20Xianhao%20Chen%20and%20Yuguang%20Fang%20and%20Sam%20Kwong%0AAbstract%3A%20%20%20Ramp%20merging%20is%20one%20of%20the%20bottlenecks%20in%20traffic%20systems%2C%20which%20commonly%0Acause%20traffic%20congestion%2C%20accidents%2C%20and%20severe%20carbon%20emissions.%20In%20order%20to%0Aaddress%20this%20essential%20issue%20and%20enhance%20the%20safety%20and%20efficiency%20of%20connected%0Aand%20autonomous%20vehicles%20%28CAVs%29%20at%20multi-lane%20merging%20zones%2C%20we%20propose%20a%20novel%0Acollaborative%20decision-making%20framework%2C%20named%20AgentsCoMerge%2C%20to%20leverage%20large%0Alanguage%20models%20%28LLMs%29.%20Specifically%2C%20we%20first%20design%20a%20scene%20observation%20and%0Aunderstanding%20module%20to%20allow%20an%20agent%20to%20capture%20the%20traffic%20environment.%20Then%0Awe%20propose%20a%20hierarchical%20planning%20module%20to%20enable%20the%20agent%20to%20make%20decisions%0Aand%20plan%20trajectories%20based%20on%20the%20observation%20and%20the%20agent%27s%20own%20state.%20In%0Aaddition%2C%20in%20order%20to%20facilitate%20collaboration%20among%20multiple%20agents%2C%20we%0Aintroduce%20a%20communication%20module%20to%20enable%20the%20surrounding%20agents%20to%20exchange%0Anecessary%20information%20and%20coordinate%20their%20actions.%20Finally%2C%20we%20develop%20a%0Areinforcement%20reflection%20guided%20training%20paradigm%20to%20further%20enhance%20the%0Adecision-making%20capability%20of%20the%20framework.%20Extensive%20experiments%20are%0Aconducted%20to%20evaluate%20the%20performance%20of%20our%20proposed%20method%2C%20demonstrating%20its%0Asuperior%20efficiency%20and%20effectiveness%20for%20multi-agent%20collaborative%0Adecision-making%20under%20various%20ramp%20merging%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03624v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentsCoMerge%253A%2520Large%2520Language%2520Model%2520Empowered%2520Collaborative%2520Decision%250A%2520%2520Making%2520for%2520Ramp%2520Merging%26entry.906535625%3DSenkang%2520Hu%2520and%2520Zhengru%2520Fang%2520and%2520Zihan%2520Fang%2520and%2520Yiqin%2520Deng%2520and%2520Xianhao%2520Chen%2520and%2520Yuguang%2520Fang%2520and%2520Sam%2520Kwong%26entry.1292438233%3D%2520%2520Ramp%2520merging%2520is%2520one%2520of%2520the%2520bottlenecks%2520in%2520traffic%2520systems%252C%2520which%2520commonly%250Acause%2520traffic%2520congestion%252C%2520accidents%252C%2520and%2520severe%2520carbon%2520emissions.%2520In%2520order%2520to%250Aaddress%2520this%2520essential%2520issue%2520and%2520enhance%2520the%2520safety%2520and%2520efficiency%2520of%2520connected%250Aand%2520autonomous%2520vehicles%2520%2528CAVs%2529%2520at%2520multi-lane%2520merging%2520zones%252C%2520we%2520propose%2520a%2520novel%250Acollaborative%2520decision-making%2520framework%252C%2520named%2520AgentsCoMerge%252C%2520to%2520leverage%2520large%250Alanguage%2520models%2520%2528LLMs%2529.%2520Specifically%252C%2520we%2520first%2520design%2520a%2520scene%2520observation%2520and%250Aunderstanding%2520module%2520to%2520allow%2520an%2520agent%2520to%2520capture%2520the%2520traffic%2520environment.%2520Then%250Awe%2520propose%2520a%2520hierarchical%2520planning%2520module%2520to%2520enable%2520the%2520agent%2520to%2520make%2520decisions%250Aand%2520plan%2520trajectories%2520based%2520on%2520the%2520observation%2520and%2520the%2520agent%2527s%2520own%2520state.%2520In%250Aaddition%252C%2520in%2520order%2520to%2520facilitate%2520collaboration%2520among%2520multiple%2520agents%252C%2520we%250Aintroduce%2520a%2520communication%2520module%2520to%2520enable%2520the%2520surrounding%2520agents%2520to%2520exchange%250Anecessary%2520information%2520and%2520coordinate%2520their%2520actions.%2520Finally%252C%2520we%2520develop%2520a%250Areinforcement%2520reflection%2520guided%2520training%2520paradigm%2520to%2520further%2520enhance%2520the%250Adecision-making%2520capability%2520of%2520the%2520framework.%2520Extensive%2520experiments%2520are%250Aconducted%2520to%2520evaluate%2520the%2520performance%2520of%2520our%2520proposed%2520method%252C%2520demonstrating%2520its%250Asuperior%2520efficiency%2520and%2520effectiveness%2520for%2520multi-agent%2520collaborative%250Adecision-making%2520under%2520various%2520ramp%2520merging%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03624v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AgentsCoMerge%3A%20Large%20Language%20Model%20Empowered%20Collaborative%20Decision%0A%20%20Making%20for%20Ramp%20Merging&entry.906535625=Senkang%20Hu%20and%20Zhengru%20Fang%20and%20Zihan%20Fang%20and%20Yiqin%20Deng%20and%20Xianhao%20Chen%20and%20Yuguang%20Fang%20and%20Sam%20Kwong&entry.1292438233=%20%20Ramp%20merging%20is%20one%20of%20the%20bottlenecks%20in%20traffic%20systems%2C%20which%20commonly%0Acause%20traffic%20congestion%2C%20accidents%2C%20and%20severe%20carbon%20emissions.%20In%20order%20to%0Aaddress%20this%20essential%20issue%20and%20enhance%20the%20safety%20and%20efficiency%20of%20connected%0Aand%20autonomous%20vehicles%20%28CAVs%29%20at%20multi-lane%20merging%20zones%2C%20we%20propose%20a%20novel%0Acollaborative%20decision-making%20framework%2C%20named%20AgentsCoMerge%2C%20to%20leverage%20large%0Alanguage%20models%20%28LLMs%29.%20Specifically%2C%20we%20first%20design%20a%20scene%20observation%20and%0Aunderstanding%20module%20to%20allow%20an%20agent%20to%20capture%20the%20traffic%20environment.%20Then%0Awe%20propose%20a%20hierarchical%20planning%20module%20to%20enable%20the%20agent%20to%20make%20decisions%0Aand%20plan%20trajectories%20based%20on%20the%20observation%20and%20the%20agent%27s%20own%20state.%20In%0Aaddition%2C%20in%20order%20to%20facilitate%20collaboration%20among%20multiple%20agents%2C%20we%0Aintroduce%20a%20communication%20module%20to%20enable%20the%20surrounding%20agents%20to%20exchange%0Anecessary%20information%20and%20coordinate%20their%20actions.%20Finally%2C%20we%20develop%20a%0Areinforcement%20reflection%20guided%20training%20paradigm%20to%20further%20enhance%20the%0Adecision-making%20capability%20of%20the%20framework.%20Extensive%20experiments%20are%0Aconducted%20to%20evaluate%20the%20performance%20of%20our%20proposed%20method%2C%20demonstrating%20its%0Asuperior%20efficiency%20and%20effectiveness%20for%20multi-agent%20collaborative%0Adecision-making%20under%20various%20ramp%20merging%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03624v2&entry.124074799=Read"},
{"title": "Towards Leveraging Large Language Model Summaries for Topic Modeling in\n  Source Code", "author": "Michele Carissimi and Martina Saletta and Claudio Ferretti", "abstract": "  Understanding source code is a topic of great interest in the software\nengineering community, since it can help programmers in various tasks such as\nsoftware maintenance and reuse. Recent advances in large language models (LLMs)\nhave demonstrated remarkable program comprehension capabilities, while\ntransformer-based topic modeling techniques offer effective ways to extract\nsemantic information from text. This paper proposes and explores a novel\napproach that combines these strengths to automatically identify meaningful\ntopics in a corpus of Python programs. Our method consists in applying topic\nmodeling on the descriptions obtained by asking an LLM to summarize the code.\nTo assess the internal consistency of the extracted topics, we compare them\nagainst topics inferred from function names alone, and those derived from\nexisting docstrings. Experimental results suggest that leveraging LLM-generated\nsummaries provides interpretable and semantically rich representation of code\nstructure. The promising results suggest that our approach can be fruitfully\napplied in various software engineering tasks such as automatic documentation\nand tagging, code search, software reorganization and knowledge discovery in\nlarge repositories.\n", "link": "http://arxiv.org/abs/2504.17426v1", "date": "2025-04-24", "relevancy": 2.0471, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5224}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5224}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Leveraging%20Large%20Language%20Model%20Summaries%20for%20Topic%20Modeling%20in%0A%20%20Source%20Code&body=Title%3A%20Towards%20Leveraging%20Large%20Language%20Model%20Summaries%20for%20Topic%20Modeling%20in%0A%20%20Source%20Code%0AAuthor%3A%20Michele%20Carissimi%20and%20Martina%20Saletta%20and%20Claudio%20Ferretti%0AAbstract%3A%20%20%20Understanding%20source%20code%20is%20a%20topic%20of%20great%20interest%20in%20the%20software%0Aengineering%20community%2C%20since%20it%20can%20help%20programmers%20in%20various%20tasks%20such%20as%0Asoftware%20maintenance%20and%20reuse.%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%0Ahave%20demonstrated%20remarkable%20program%20comprehension%20capabilities%2C%20while%0Atransformer-based%20topic%20modeling%20techniques%20offer%20effective%20ways%20to%20extract%0Asemantic%20information%20from%20text.%20This%20paper%20proposes%20and%20explores%20a%20novel%0Aapproach%20that%20combines%20these%20strengths%20to%20automatically%20identify%20meaningful%0Atopics%20in%20a%20corpus%20of%20Python%20programs.%20Our%20method%20consists%20in%20applying%20topic%0Amodeling%20on%20the%20descriptions%20obtained%20by%20asking%20an%20LLM%20to%20summarize%20the%20code.%0ATo%20assess%20the%20internal%20consistency%20of%20the%20extracted%20topics%2C%20we%20compare%20them%0Aagainst%20topics%20inferred%20from%20function%20names%20alone%2C%20and%20those%20derived%20from%0Aexisting%20docstrings.%20Experimental%20results%20suggest%20that%20leveraging%20LLM-generated%0Asummaries%20provides%20interpretable%20and%20semantically%20rich%20representation%20of%20code%0Astructure.%20The%20promising%20results%20suggest%20that%20our%20approach%20can%20be%20fruitfully%0Aapplied%20in%20various%20software%20engineering%20tasks%20such%20as%20automatic%20documentation%0Aand%20tagging%2C%20code%20search%2C%20software%20reorganization%20and%20knowledge%20discovery%20in%0Alarge%20repositories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17426v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Leveraging%2520Large%2520Language%2520Model%2520Summaries%2520for%2520Topic%2520Modeling%2520in%250A%2520%2520Source%2520Code%26entry.906535625%3DMichele%2520Carissimi%2520and%2520Martina%2520Saletta%2520and%2520Claudio%2520Ferretti%26entry.1292438233%3D%2520%2520Understanding%2520source%2520code%2520is%2520a%2520topic%2520of%2520great%2520interest%2520in%2520the%2520software%250Aengineering%2520community%252C%2520since%2520it%2520can%2520help%2520programmers%2520in%2520various%2520tasks%2520such%2520as%250Asoftware%2520maintenance%2520and%2520reuse.%2520Recent%2520advances%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%250Ahave%2520demonstrated%2520remarkable%2520program%2520comprehension%2520capabilities%252C%2520while%250Atransformer-based%2520topic%2520modeling%2520techniques%2520offer%2520effective%2520ways%2520to%2520extract%250Asemantic%2520information%2520from%2520text.%2520This%2520paper%2520proposes%2520and%2520explores%2520a%2520novel%250Aapproach%2520that%2520combines%2520these%2520strengths%2520to%2520automatically%2520identify%2520meaningful%250Atopics%2520in%2520a%2520corpus%2520of%2520Python%2520programs.%2520Our%2520method%2520consists%2520in%2520applying%2520topic%250Amodeling%2520on%2520the%2520descriptions%2520obtained%2520by%2520asking%2520an%2520LLM%2520to%2520summarize%2520the%2520code.%250ATo%2520assess%2520the%2520internal%2520consistency%2520of%2520the%2520extracted%2520topics%252C%2520we%2520compare%2520them%250Aagainst%2520topics%2520inferred%2520from%2520function%2520names%2520alone%252C%2520and%2520those%2520derived%2520from%250Aexisting%2520docstrings.%2520Experimental%2520results%2520suggest%2520that%2520leveraging%2520LLM-generated%250Asummaries%2520provides%2520interpretable%2520and%2520semantically%2520rich%2520representation%2520of%2520code%250Astructure.%2520The%2520promising%2520results%2520suggest%2520that%2520our%2520approach%2520can%2520be%2520fruitfully%250Aapplied%2520in%2520various%2520software%2520engineering%2520tasks%2520such%2520as%2520automatic%2520documentation%250Aand%2520tagging%252C%2520code%2520search%252C%2520software%2520reorganization%2520and%2520knowledge%2520discovery%2520in%250Alarge%2520repositories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17426v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Leveraging%20Large%20Language%20Model%20Summaries%20for%20Topic%20Modeling%20in%0A%20%20Source%20Code&entry.906535625=Michele%20Carissimi%20and%20Martina%20Saletta%20and%20Claudio%20Ferretti&entry.1292438233=%20%20Understanding%20source%20code%20is%20a%20topic%20of%20great%20interest%20in%20the%20software%0Aengineering%20community%2C%20since%20it%20can%20help%20programmers%20in%20various%20tasks%20such%20as%0Asoftware%20maintenance%20and%20reuse.%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%0Ahave%20demonstrated%20remarkable%20program%20comprehension%20capabilities%2C%20while%0Atransformer-based%20topic%20modeling%20techniques%20offer%20effective%20ways%20to%20extract%0Asemantic%20information%20from%20text.%20This%20paper%20proposes%20and%20explores%20a%20novel%0Aapproach%20that%20combines%20these%20strengths%20to%20automatically%20identify%20meaningful%0Atopics%20in%20a%20corpus%20of%20Python%20programs.%20Our%20method%20consists%20in%20applying%20topic%0Amodeling%20on%20the%20descriptions%20obtained%20by%20asking%20an%20LLM%20to%20summarize%20the%20code.%0ATo%20assess%20the%20internal%20consistency%20of%20the%20extracted%20topics%2C%20we%20compare%20them%0Aagainst%20topics%20inferred%20from%20function%20names%20alone%2C%20and%20those%20derived%20from%0Aexisting%20docstrings.%20Experimental%20results%20suggest%20that%20leveraging%20LLM-generated%0Asummaries%20provides%20interpretable%20and%20semantically%20rich%20representation%20of%20code%0Astructure.%20The%20promising%20results%20suggest%20that%20our%20approach%20can%20be%20fruitfully%0Aapplied%20in%20various%20software%20engineering%20tasks%20such%20as%20automatic%20documentation%0Aand%20tagging%2C%20code%20search%2C%20software%20reorganization%20and%20knowledge%20discovery%20in%0Alarge%20repositories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17426v1&entry.124074799=Read"},
{"title": "When Does Metadata Conditioning (NOT) Work for Language Model\n  Pre-Training? A Study with Context-Free Grammars", "author": "Rei Higuchi and Ryotaro Kawata and Naoki Nishikawa and Kazusato Oko and Shoichiro Yamaguchi and Sosuke Kobayashi and Seiya Tokui and Kohei Hayashi and Daisuke Okanohara and Taiji Suzuki", "abstract": "  The ability to acquire latent semantics is one of the key properties that\ndetermines the performance of language models. One convenient approach to\ninvoke this ability is to prepend metadata (e.g. URLs, domains, and styles) at\nthe beginning of texts in the pre-training data, making it easier for the model\nto access latent semantics before observing the entire text. Previous studies\nhave reported that this technique actually improves the performance of trained\nmodels in downstream tasks; however, this improvement has been observed only in\nspecific downstream tasks, without consistent enhancement in average next-token\nprediction loss. To understand this phenomenon, we closely investigate how\nprepending metadata during pre-training affects model performance by examining\nits behavior using artificial data. Interestingly, we found that this approach\nproduces both positive and negative effects on the downstream tasks. We\ndemonstrate that the effectiveness of the approach depends on whether latent\nsemantics can be inferred from the downstream task's prompt. Specifically,\nthrough investigations using data generated by probabilistic context-free\ngrammars, we show that training with metadata helps improve model's performance\nwhen the given context is long enough to infer the latent semantics. In\ncontrast, the technique negatively impacts performance when the context lacks\nthe necessary information to make an accurate posterior inference.\n", "link": "http://arxiv.org/abs/2504.17562v1", "date": "2025-04-24", "relevancy": 2.0358, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5143}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5143}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Does%20Metadata%20Conditioning%20%28NOT%29%20Work%20for%20Language%20Model%0A%20%20Pre-Training%3F%20A%20Study%20with%20Context-Free%20Grammars&body=Title%3A%20When%20Does%20Metadata%20Conditioning%20%28NOT%29%20Work%20for%20Language%20Model%0A%20%20Pre-Training%3F%20A%20Study%20with%20Context-Free%20Grammars%0AAuthor%3A%20Rei%20Higuchi%20and%20Ryotaro%20Kawata%20and%20Naoki%20Nishikawa%20and%20Kazusato%20Oko%20and%20Shoichiro%20Yamaguchi%20and%20Sosuke%20Kobayashi%20and%20Seiya%20Tokui%20and%20Kohei%20Hayashi%20and%20Daisuke%20Okanohara%20and%20Taiji%20Suzuki%0AAbstract%3A%20%20%20The%20ability%20to%20acquire%20latent%20semantics%20is%20one%20of%20the%20key%20properties%20that%0Adetermines%20the%20performance%20of%20language%20models.%20One%20convenient%20approach%20to%0Ainvoke%20this%20ability%20is%20to%20prepend%20metadata%20%28e.g.%20URLs%2C%20domains%2C%20and%20styles%29%20at%0Athe%20beginning%20of%20texts%20in%20the%20pre-training%20data%2C%20making%20it%20easier%20for%20the%20model%0Ato%20access%20latent%20semantics%20before%20observing%20the%20entire%20text.%20Previous%20studies%0Ahave%20reported%20that%20this%20technique%20actually%20improves%20the%20performance%20of%20trained%0Amodels%20in%20downstream%20tasks%3B%20however%2C%20this%20improvement%20has%20been%20observed%20only%20in%0Aspecific%20downstream%20tasks%2C%20without%20consistent%20enhancement%20in%20average%20next-token%0Aprediction%20loss.%20To%20understand%20this%20phenomenon%2C%20we%20closely%20investigate%20how%0Aprepending%20metadata%20during%20pre-training%20affects%20model%20performance%20by%20examining%0Aits%20behavior%20using%20artificial%20data.%20Interestingly%2C%20we%20found%20that%20this%20approach%0Aproduces%20both%20positive%20and%20negative%20effects%20on%20the%20downstream%20tasks.%20We%0Ademonstrate%20that%20the%20effectiveness%20of%20the%20approach%20depends%20on%20whether%20latent%0Asemantics%20can%20be%20inferred%20from%20the%20downstream%20task%27s%20prompt.%20Specifically%2C%0Athrough%20investigations%20using%20data%20generated%20by%20probabilistic%20context-free%0Agrammars%2C%20we%20show%20that%20training%20with%20metadata%20helps%20improve%20model%27s%20performance%0Awhen%20the%20given%20context%20is%20long%20enough%20to%20infer%20the%20latent%20semantics.%20In%0Acontrast%2C%20the%20technique%20negatively%20impacts%20performance%20when%20the%20context%20lacks%0Athe%20necessary%20information%20to%20make%20an%20accurate%20posterior%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17562v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Does%2520Metadata%2520Conditioning%2520%2528NOT%2529%2520Work%2520for%2520Language%2520Model%250A%2520%2520Pre-Training%253F%2520A%2520Study%2520with%2520Context-Free%2520Grammars%26entry.906535625%3DRei%2520Higuchi%2520and%2520Ryotaro%2520Kawata%2520and%2520Naoki%2520Nishikawa%2520and%2520Kazusato%2520Oko%2520and%2520Shoichiro%2520Yamaguchi%2520and%2520Sosuke%2520Kobayashi%2520and%2520Seiya%2520Tokui%2520and%2520Kohei%2520Hayashi%2520and%2520Daisuke%2520Okanohara%2520and%2520Taiji%2520Suzuki%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520acquire%2520latent%2520semantics%2520is%2520one%2520of%2520the%2520key%2520properties%2520that%250Adetermines%2520the%2520performance%2520of%2520language%2520models.%2520One%2520convenient%2520approach%2520to%250Ainvoke%2520this%2520ability%2520is%2520to%2520prepend%2520metadata%2520%2528e.g.%2520URLs%252C%2520domains%252C%2520and%2520styles%2529%2520at%250Athe%2520beginning%2520of%2520texts%2520in%2520the%2520pre-training%2520data%252C%2520making%2520it%2520easier%2520for%2520the%2520model%250Ato%2520access%2520latent%2520semantics%2520before%2520observing%2520the%2520entire%2520text.%2520Previous%2520studies%250Ahave%2520reported%2520that%2520this%2520technique%2520actually%2520improves%2520the%2520performance%2520of%2520trained%250Amodels%2520in%2520downstream%2520tasks%253B%2520however%252C%2520this%2520improvement%2520has%2520been%2520observed%2520only%2520in%250Aspecific%2520downstream%2520tasks%252C%2520without%2520consistent%2520enhancement%2520in%2520average%2520next-token%250Aprediction%2520loss.%2520To%2520understand%2520this%2520phenomenon%252C%2520we%2520closely%2520investigate%2520how%250Aprepending%2520metadata%2520during%2520pre-training%2520affects%2520model%2520performance%2520by%2520examining%250Aits%2520behavior%2520using%2520artificial%2520data.%2520Interestingly%252C%2520we%2520found%2520that%2520this%2520approach%250Aproduces%2520both%2520positive%2520and%2520negative%2520effects%2520on%2520the%2520downstream%2520tasks.%2520We%250Ademonstrate%2520that%2520the%2520effectiveness%2520of%2520the%2520approach%2520depends%2520on%2520whether%2520latent%250Asemantics%2520can%2520be%2520inferred%2520from%2520the%2520downstream%2520task%2527s%2520prompt.%2520Specifically%252C%250Athrough%2520investigations%2520using%2520data%2520generated%2520by%2520probabilistic%2520context-free%250Agrammars%252C%2520we%2520show%2520that%2520training%2520with%2520metadata%2520helps%2520improve%2520model%2527s%2520performance%250Awhen%2520the%2520given%2520context%2520is%2520long%2520enough%2520to%2520infer%2520the%2520latent%2520semantics.%2520In%250Acontrast%252C%2520the%2520technique%2520negatively%2520impacts%2520performance%2520when%2520the%2520context%2520lacks%250Athe%2520necessary%2520information%2520to%2520make%2520an%2520accurate%2520posterior%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17562v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Does%20Metadata%20Conditioning%20%28NOT%29%20Work%20for%20Language%20Model%0A%20%20Pre-Training%3F%20A%20Study%20with%20Context-Free%20Grammars&entry.906535625=Rei%20Higuchi%20and%20Ryotaro%20Kawata%20and%20Naoki%20Nishikawa%20and%20Kazusato%20Oko%20and%20Shoichiro%20Yamaguchi%20and%20Sosuke%20Kobayashi%20and%20Seiya%20Tokui%20and%20Kohei%20Hayashi%20and%20Daisuke%20Okanohara%20and%20Taiji%20Suzuki&entry.1292438233=%20%20The%20ability%20to%20acquire%20latent%20semantics%20is%20one%20of%20the%20key%20properties%20that%0Adetermines%20the%20performance%20of%20language%20models.%20One%20convenient%20approach%20to%0Ainvoke%20this%20ability%20is%20to%20prepend%20metadata%20%28e.g.%20URLs%2C%20domains%2C%20and%20styles%29%20at%0Athe%20beginning%20of%20texts%20in%20the%20pre-training%20data%2C%20making%20it%20easier%20for%20the%20model%0Ato%20access%20latent%20semantics%20before%20observing%20the%20entire%20text.%20Previous%20studies%0Ahave%20reported%20that%20this%20technique%20actually%20improves%20the%20performance%20of%20trained%0Amodels%20in%20downstream%20tasks%3B%20however%2C%20this%20improvement%20has%20been%20observed%20only%20in%0Aspecific%20downstream%20tasks%2C%20without%20consistent%20enhancement%20in%20average%20next-token%0Aprediction%20loss.%20To%20understand%20this%20phenomenon%2C%20we%20closely%20investigate%20how%0Aprepending%20metadata%20during%20pre-training%20affects%20model%20performance%20by%20examining%0Aits%20behavior%20using%20artificial%20data.%20Interestingly%2C%20we%20found%20that%20this%20approach%0Aproduces%20both%20positive%20and%20negative%20effects%20on%20the%20downstream%20tasks.%20We%0Ademonstrate%20that%20the%20effectiveness%20of%20the%20approach%20depends%20on%20whether%20latent%0Asemantics%20can%20be%20inferred%20from%20the%20downstream%20task%27s%20prompt.%20Specifically%2C%0Athrough%20investigations%20using%20data%20generated%20by%20probabilistic%20context-free%0Agrammars%2C%20we%20show%20that%20training%20with%20metadata%20helps%20improve%20model%27s%20performance%0Awhen%20the%20given%20context%20is%20long%20enough%20to%20infer%20the%20latent%20semantics.%20In%0Acontrast%2C%20the%20technique%20negatively%20impacts%20performance%20when%20the%20context%20lacks%0Athe%20necessary%20information%20to%20make%20an%20accurate%20posterior%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17562v1&entry.124074799=Read"},
{"title": "EgoCHARM: Resource-Efficient Hierarchical Activity Recognition using an\n  Egocentric IMU Sensor", "author": "Akhil Padmanabha and Saravanan Govindarajan and Hwanmun Kim and Sergio Ortiz and Rahul Rajan and Doruk Senkal and Sneha Kadetotad", "abstract": "  Human activity recognition (HAR) on smartglasses has various use cases,\nincluding health/fitness tracking and input for context-aware AI assistants.\nHowever, current approaches for egocentric activity recognition suffer from low\nperformance or are resource-intensive. In this work, we introduce a resource\n(memory, compute, power, sample) efficient machine learning algorithm,\nEgoCHARM, for recognizing both high level and low level activities using a\nsingle egocentric (head-mounted) Inertial Measurement Unit (IMU). Our\nhierarchical algorithm employs a semi-supervised learning strategy, requiring\nprimarily high level activity labels for training, to learn generalizable low\nlevel motion embeddings that can be effectively utilized for low level activity\nrecognition. We evaluate our method on 9 high level and 3 low level activities\nachieving 0.826 and 0.855 F1 scores on high level and low level activity\nrecognition respectively, with just 63k high level and 22k low level model\nparameters, allowing the low level encoder to be deployed directly on current\nIMU chips with compute. Lastly, we present results and insights from a\nsensitivity analysis and highlight the opportunities and limitations of\nactivity recognition using egocentric IMUs.\n", "link": "http://arxiv.org/abs/2504.17735v1", "date": "2025-04-24", "relevancy": 2.033, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.524}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5064}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EgoCHARM%3A%20Resource-Efficient%20Hierarchical%20Activity%20Recognition%20using%20an%0A%20%20Egocentric%20IMU%20Sensor&body=Title%3A%20EgoCHARM%3A%20Resource-Efficient%20Hierarchical%20Activity%20Recognition%20using%20an%0A%20%20Egocentric%20IMU%20Sensor%0AAuthor%3A%20Akhil%20Padmanabha%20and%20Saravanan%20Govindarajan%20and%20Hwanmun%20Kim%20and%20Sergio%20Ortiz%20and%20Rahul%20Rajan%20and%20Doruk%20Senkal%20and%20Sneha%20Kadetotad%0AAbstract%3A%20%20%20Human%20activity%20recognition%20%28HAR%29%20on%20smartglasses%20has%20various%20use%20cases%2C%0Aincluding%20health/fitness%20tracking%20and%20input%20for%20context-aware%20AI%20assistants.%0AHowever%2C%20current%20approaches%20for%20egocentric%20activity%20recognition%20suffer%20from%20low%0Aperformance%20or%20are%20resource-intensive.%20In%20this%20work%2C%20we%20introduce%20a%20resource%0A%28memory%2C%20compute%2C%20power%2C%20sample%29%20efficient%20machine%20learning%20algorithm%2C%0AEgoCHARM%2C%20for%20recognizing%20both%20high%20level%20and%20low%20level%20activities%20using%20a%0Asingle%20egocentric%20%28head-mounted%29%20Inertial%20Measurement%20Unit%20%28IMU%29.%20Our%0Ahierarchical%20algorithm%20employs%20a%20semi-supervised%20learning%20strategy%2C%20requiring%0Aprimarily%20high%20level%20activity%20labels%20for%20training%2C%20to%20learn%20generalizable%20low%0Alevel%20motion%20embeddings%20that%20can%20be%20effectively%20utilized%20for%20low%20level%20activity%0Arecognition.%20We%20evaluate%20our%20method%20on%209%20high%20level%20and%203%20low%20level%20activities%0Aachieving%200.826%20and%200.855%20F1%20scores%20on%20high%20level%20and%20low%20level%20activity%0Arecognition%20respectively%2C%20with%20just%2063k%20high%20level%20and%2022k%20low%20level%20model%0Aparameters%2C%20allowing%20the%20low%20level%20encoder%20to%20be%20deployed%20directly%20on%20current%0AIMU%20chips%20with%20compute.%20Lastly%2C%20we%20present%20results%20and%20insights%20from%20a%0Asensitivity%20analysis%20and%20highlight%20the%20opportunities%20and%20limitations%20of%0Aactivity%20recognition%20using%20egocentric%20IMUs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17735v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgoCHARM%253A%2520Resource-Efficient%2520Hierarchical%2520Activity%2520Recognition%2520using%2520an%250A%2520%2520Egocentric%2520IMU%2520Sensor%26entry.906535625%3DAkhil%2520Padmanabha%2520and%2520Saravanan%2520Govindarajan%2520and%2520Hwanmun%2520Kim%2520and%2520Sergio%2520Ortiz%2520and%2520Rahul%2520Rajan%2520and%2520Doruk%2520Senkal%2520and%2520Sneha%2520Kadetotad%26entry.1292438233%3D%2520%2520Human%2520activity%2520recognition%2520%2528HAR%2529%2520on%2520smartglasses%2520has%2520various%2520use%2520cases%252C%250Aincluding%2520health/fitness%2520tracking%2520and%2520input%2520for%2520context-aware%2520AI%2520assistants.%250AHowever%252C%2520current%2520approaches%2520for%2520egocentric%2520activity%2520recognition%2520suffer%2520from%2520low%250Aperformance%2520or%2520are%2520resource-intensive.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520resource%250A%2528memory%252C%2520compute%252C%2520power%252C%2520sample%2529%2520efficient%2520machine%2520learning%2520algorithm%252C%250AEgoCHARM%252C%2520for%2520recognizing%2520both%2520high%2520level%2520and%2520low%2520level%2520activities%2520using%2520a%250Asingle%2520egocentric%2520%2528head-mounted%2529%2520Inertial%2520Measurement%2520Unit%2520%2528IMU%2529.%2520Our%250Ahierarchical%2520algorithm%2520employs%2520a%2520semi-supervised%2520learning%2520strategy%252C%2520requiring%250Aprimarily%2520high%2520level%2520activity%2520labels%2520for%2520training%252C%2520to%2520learn%2520generalizable%2520low%250Alevel%2520motion%2520embeddings%2520that%2520can%2520be%2520effectively%2520utilized%2520for%2520low%2520level%2520activity%250Arecognition.%2520We%2520evaluate%2520our%2520method%2520on%25209%2520high%2520level%2520and%25203%2520low%2520level%2520activities%250Aachieving%25200.826%2520and%25200.855%2520F1%2520scores%2520on%2520high%2520level%2520and%2520low%2520level%2520activity%250Arecognition%2520respectively%252C%2520with%2520just%252063k%2520high%2520level%2520and%252022k%2520low%2520level%2520model%250Aparameters%252C%2520allowing%2520the%2520low%2520level%2520encoder%2520to%2520be%2520deployed%2520directly%2520on%2520current%250AIMU%2520chips%2520with%2520compute.%2520Lastly%252C%2520we%2520present%2520results%2520and%2520insights%2520from%2520a%250Asensitivity%2520analysis%2520and%2520highlight%2520the%2520opportunities%2520and%2520limitations%2520of%250Aactivity%2520recognition%2520using%2520egocentric%2520IMUs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17735v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EgoCHARM%3A%20Resource-Efficient%20Hierarchical%20Activity%20Recognition%20using%20an%0A%20%20Egocentric%20IMU%20Sensor&entry.906535625=Akhil%20Padmanabha%20and%20Saravanan%20Govindarajan%20and%20Hwanmun%20Kim%20and%20Sergio%20Ortiz%20and%20Rahul%20Rajan%20and%20Doruk%20Senkal%20and%20Sneha%20Kadetotad&entry.1292438233=%20%20Human%20activity%20recognition%20%28HAR%29%20on%20smartglasses%20has%20various%20use%20cases%2C%0Aincluding%20health/fitness%20tracking%20and%20input%20for%20context-aware%20AI%20assistants.%0AHowever%2C%20current%20approaches%20for%20egocentric%20activity%20recognition%20suffer%20from%20low%0Aperformance%20or%20are%20resource-intensive.%20In%20this%20work%2C%20we%20introduce%20a%20resource%0A%28memory%2C%20compute%2C%20power%2C%20sample%29%20efficient%20machine%20learning%20algorithm%2C%0AEgoCHARM%2C%20for%20recognizing%20both%20high%20level%20and%20low%20level%20activities%20using%20a%0Asingle%20egocentric%20%28head-mounted%29%20Inertial%20Measurement%20Unit%20%28IMU%29.%20Our%0Ahierarchical%20algorithm%20employs%20a%20semi-supervised%20learning%20strategy%2C%20requiring%0Aprimarily%20high%20level%20activity%20labels%20for%20training%2C%20to%20learn%20generalizable%20low%0Alevel%20motion%20embeddings%20that%20can%20be%20effectively%20utilized%20for%20low%20level%20activity%0Arecognition.%20We%20evaluate%20our%20method%20on%209%20high%20level%20and%203%20low%20level%20activities%0Aachieving%200.826%20and%200.855%20F1%20scores%20on%20high%20level%20and%20low%20level%20activity%0Arecognition%20respectively%2C%20with%20just%2063k%20high%20level%20and%2022k%20low%20level%20model%0Aparameters%2C%20allowing%20the%20low%20level%20encoder%20to%20be%20deployed%20directly%20on%20current%0AIMU%20chips%20with%20compute.%20Lastly%2C%20we%20present%20results%20and%20insights%20from%20a%0Asensitivity%20analysis%20and%20highlight%20the%20opportunities%20and%20limitations%20of%0Aactivity%20recognition%20using%20egocentric%20IMUs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17735v1&entry.124074799=Read"},
{"title": "Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve\n  LLM-level Accuracy in Profile Matching Tasks", "author": "Haru-Tada Sato and Fuka Matsuzaki and Jun-ichiro Takahashi", "abstract": "  This study explores the potential of small language model(SLM) ensembles to\nachieve accuracy comparable to proprietary large language models (LLMs). We\npropose Ensemble Bayesian Inference (EBI), a novel approach that applies\nBayesian estimation to combine judgments from multiple SLMs, allowing them to\nexceed the performance limitations of individual models. Our experiments on\ndiverse tasks(aptitude assessments and consumer profile analysis in both\nJapanese and English) demonstrate EBI's effectiveness. Notably, we analyze\ncases where incorporating models with negative Lift values into ensembles\nimproves overall performance, and we examine the method's efficacy across\ndifferent languages. These findings suggest new possibilities for constructing\nhigh-performance AI systems with limited computational resources and for\neffectively utilizing models with individually lower performance. Building on\nexisting research on LLM performance evaluation, ensemble methods, and\nopen-source LLM utilization, we discuss the novelty and significance of our\napproach.\n", "link": "http://arxiv.org/abs/2504.17685v1", "date": "2025-04-24", "relevancy": 2.0301, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.51}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.507}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ensemble%20Bayesian%20Inference%3A%20Leveraging%20Small%20Language%20Models%20to%20Achieve%0A%20%20LLM-level%20Accuracy%20in%20Profile%20Matching%20Tasks&body=Title%3A%20Ensemble%20Bayesian%20Inference%3A%20Leveraging%20Small%20Language%20Models%20to%20Achieve%0A%20%20LLM-level%20Accuracy%20in%20Profile%20Matching%20Tasks%0AAuthor%3A%20Haru-Tada%20Sato%20and%20Fuka%20Matsuzaki%20and%20Jun-ichiro%20Takahashi%0AAbstract%3A%20%20%20This%20study%20explores%20the%20potential%20of%20small%20language%20model%28SLM%29%20ensembles%20to%0Aachieve%20accuracy%20comparable%20to%20proprietary%20large%20language%20models%20%28LLMs%29.%20We%0Apropose%20Ensemble%20Bayesian%20Inference%20%28EBI%29%2C%20a%20novel%20approach%20that%20applies%0ABayesian%20estimation%20to%20combine%20judgments%20from%20multiple%20SLMs%2C%20allowing%20them%20to%0Aexceed%20the%20performance%20limitations%20of%20individual%20models.%20Our%20experiments%20on%0Adiverse%20tasks%28aptitude%20assessments%20and%20consumer%20profile%20analysis%20in%20both%0AJapanese%20and%20English%29%20demonstrate%20EBI%27s%20effectiveness.%20Notably%2C%20we%20analyze%0Acases%20where%20incorporating%20models%20with%20negative%20Lift%20values%20into%20ensembles%0Aimproves%20overall%20performance%2C%20and%20we%20examine%20the%20method%27s%20efficacy%20across%0Adifferent%20languages.%20These%20findings%20suggest%20new%20possibilities%20for%20constructing%0Ahigh-performance%20AI%20systems%20with%20limited%20computational%20resources%20and%20for%0Aeffectively%20utilizing%20models%20with%20individually%20lower%20performance.%20Building%20on%0Aexisting%20research%20on%20LLM%20performance%20evaluation%2C%20ensemble%20methods%2C%20and%0Aopen-source%20LLM%20utilization%2C%20we%20discuss%20the%20novelty%20and%20significance%20of%20our%0Aapproach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17685v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnsemble%2520Bayesian%2520Inference%253A%2520Leveraging%2520Small%2520Language%2520Models%2520to%2520Achieve%250A%2520%2520LLM-level%2520Accuracy%2520in%2520Profile%2520Matching%2520Tasks%26entry.906535625%3DHaru-Tada%2520Sato%2520and%2520Fuka%2520Matsuzaki%2520and%2520Jun-ichiro%2520Takahashi%26entry.1292438233%3D%2520%2520This%2520study%2520explores%2520the%2520potential%2520of%2520small%2520language%2520model%2528SLM%2529%2520ensembles%2520to%250Aachieve%2520accuracy%2520comparable%2520to%2520proprietary%2520large%2520language%2520models%2520%2528LLMs%2529.%2520We%250Apropose%2520Ensemble%2520Bayesian%2520Inference%2520%2528EBI%2529%252C%2520a%2520novel%2520approach%2520that%2520applies%250ABayesian%2520estimation%2520to%2520combine%2520judgments%2520from%2520multiple%2520SLMs%252C%2520allowing%2520them%2520to%250Aexceed%2520the%2520performance%2520limitations%2520of%2520individual%2520models.%2520Our%2520experiments%2520on%250Adiverse%2520tasks%2528aptitude%2520assessments%2520and%2520consumer%2520profile%2520analysis%2520in%2520both%250AJapanese%2520and%2520English%2529%2520demonstrate%2520EBI%2527s%2520effectiveness.%2520Notably%252C%2520we%2520analyze%250Acases%2520where%2520incorporating%2520models%2520with%2520negative%2520Lift%2520values%2520into%2520ensembles%250Aimproves%2520overall%2520performance%252C%2520and%2520we%2520examine%2520the%2520method%2527s%2520efficacy%2520across%250Adifferent%2520languages.%2520These%2520findings%2520suggest%2520new%2520possibilities%2520for%2520constructing%250Ahigh-performance%2520AI%2520systems%2520with%2520limited%2520computational%2520resources%2520and%2520for%250Aeffectively%2520utilizing%2520models%2520with%2520individually%2520lower%2520performance.%2520Building%2520on%250Aexisting%2520research%2520on%2520LLM%2520performance%2520evaluation%252C%2520ensemble%2520methods%252C%2520and%250Aopen-source%2520LLM%2520utilization%252C%2520we%2520discuss%2520the%2520novelty%2520and%2520significance%2520of%2520our%250Aapproach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17685v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ensemble%20Bayesian%20Inference%3A%20Leveraging%20Small%20Language%20Models%20to%20Achieve%0A%20%20LLM-level%20Accuracy%20in%20Profile%20Matching%20Tasks&entry.906535625=Haru-Tada%20Sato%20and%20Fuka%20Matsuzaki%20and%20Jun-ichiro%20Takahashi&entry.1292438233=%20%20This%20study%20explores%20the%20potential%20of%20small%20language%20model%28SLM%29%20ensembles%20to%0Aachieve%20accuracy%20comparable%20to%20proprietary%20large%20language%20models%20%28LLMs%29.%20We%0Apropose%20Ensemble%20Bayesian%20Inference%20%28EBI%29%2C%20a%20novel%20approach%20that%20applies%0ABayesian%20estimation%20to%20combine%20judgments%20from%20multiple%20SLMs%2C%20allowing%20them%20to%0Aexceed%20the%20performance%20limitations%20of%20individual%20models.%20Our%20experiments%20on%0Adiverse%20tasks%28aptitude%20assessments%20and%20consumer%20profile%20analysis%20in%20both%0AJapanese%20and%20English%29%20demonstrate%20EBI%27s%20effectiveness.%20Notably%2C%20we%20analyze%0Acases%20where%20incorporating%20models%20with%20negative%20Lift%20values%20into%20ensembles%0Aimproves%20overall%20performance%2C%20and%20we%20examine%20the%20method%27s%20efficacy%20across%0Adifferent%20languages.%20These%20findings%20suggest%20new%20possibilities%20for%20constructing%0Ahigh-performance%20AI%20systems%20with%20limited%20computational%20resources%20and%20for%0Aeffectively%20utilizing%20models%20with%20individually%20lower%20performance.%20Building%20on%0Aexisting%20research%20on%20LLM%20performance%20evaluation%2C%20ensemble%20methods%2C%20and%0Aopen-source%20LLM%20utilization%2C%20we%20discuss%20the%20novelty%20and%20significance%20of%20our%0Aapproach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17685v1&entry.124074799=Read"},
{"title": "Advancing CMA-ES with Learning-Based Cooperative Coevolution for\n  Scalable Optimization", "author": "Hongshu Guo and Wenjie Qiu and Zeyuan Ma and Xinglin Zhang and Jun Zhang and Yue-Jiao Gong", "abstract": "  Recent research in Cooperative Coevolution~(CC) have achieved promising\nprogress in solving large-scale global optimization problems. However, existing\nCC paradigms have a primary limitation in that they require deep expertise for\nselecting or designing effective variable decomposition strategies. Inspired by\nadvancements in Meta-Black-Box Optimization, this paper introduces LCC, a\npioneering learning-based cooperative coevolution framework that dynamically\nschedules decomposition strategies during optimization processes. The\ndecomposition strategy selector is parameterized through a neural network,\nwhich processes a meticulously crafted set of optimization status features to\ndetermine the optimal strategy for each optimization step. The network is\ntrained via the Proximal Policy Optimization method in a reinforcement learning\nmanner across a collection of representative problems, aiming to maximize the\nexpected optimization performance. Extensive experimental results demonstrate\nthat LCC not only offers certain advantages over state-of-the-art baselines in\nterms of optimization effectiveness and resource consumption, but it also\nexhibits promising transferability towards unseen problems.\n", "link": "http://arxiv.org/abs/2504.17578v1", "date": "2025-04-24", "relevancy": 2.0206, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5242}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5045}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4982}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20CMA-ES%20with%20Learning-Based%20Cooperative%20Coevolution%20for%0A%20%20Scalable%20Optimization&body=Title%3A%20Advancing%20CMA-ES%20with%20Learning-Based%20Cooperative%20Coevolution%20for%0A%20%20Scalable%20Optimization%0AAuthor%3A%20Hongshu%20Guo%20and%20Wenjie%20Qiu%20and%20Zeyuan%20Ma%20and%20Xinglin%20Zhang%20and%20Jun%20Zhang%20and%20Yue-Jiao%20Gong%0AAbstract%3A%20%20%20Recent%20research%20in%20Cooperative%20Coevolution~%28CC%29%20have%20achieved%20promising%0Aprogress%20in%20solving%20large-scale%20global%20optimization%20problems.%20However%2C%20existing%0ACC%20paradigms%20have%20a%20primary%20limitation%20in%20that%20they%20require%20deep%20expertise%20for%0Aselecting%20or%20designing%20effective%20variable%20decomposition%20strategies.%20Inspired%20by%0Aadvancements%20in%20Meta-Black-Box%20Optimization%2C%20this%20paper%20introduces%20LCC%2C%20a%0Apioneering%20learning-based%20cooperative%20coevolution%20framework%20that%20dynamically%0Aschedules%20decomposition%20strategies%20during%20optimization%20processes.%20The%0Adecomposition%20strategy%20selector%20is%20parameterized%20through%20a%20neural%20network%2C%0Awhich%20processes%20a%20meticulously%20crafted%20set%20of%20optimization%20status%20features%20to%0Adetermine%20the%20optimal%20strategy%20for%20each%20optimization%20step.%20The%20network%20is%0Atrained%20via%20the%20Proximal%20Policy%20Optimization%20method%20in%20a%20reinforcement%20learning%0Amanner%20across%20a%20collection%20of%20representative%20problems%2C%20aiming%20to%20maximize%20the%0Aexpected%20optimization%20performance.%20Extensive%20experimental%20results%20demonstrate%0Athat%20LCC%20not%20only%20offers%20certain%20advantages%20over%20state-of-the-art%20baselines%20in%0Aterms%20of%20optimization%20effectiveness%20and%20resource%20consumption%2C%20but%20it%20also%0Aexhibits%20promising%20transferability%20towards%20unseen%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17578v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520CMA-ES%2520with%2520Learning-Based%2520Cooperative%2520Coevolution%2520for%250A%2520%2520Scalable%2520Optimization%26entry.906535625%3DHongshu%2520Guo%2520and%2520Wenjie%2520Qiu%2520and%2520Zeyuan%2520Ma%2520and%2520Xinglin%2520Zhang%2520and%2520Jun%2520Zhang%2520and%2520Yue-Jiao%2520Gong%26entry.1292438233%3D%2520%2520Recent%2520research%2520in%2520Cooperative%2520Coevolution~%2528CC%2529%2520have%2520achieved%2520promising%250Aprogress%2520in%2520solving%2520large-scale%2520global%2520optimization%2520problems.%2520However%252C%2520existing%250ACC%2520paradigms%2520have%2520a%2520primary%2520limitation%2520in%2520that%2520they%2520require%2520deep%2520expertise%2520for%250Aselecting%2520or%2520designing%2520effective%2520variable%2520decomposition%2520strategies.%2520Inspired%2520by%250Aadvancements%2520in%2520Meta-Black-Box%2520Optimization%252C%2520this%2520paper%2520introduces%2520LCC%252C%2520a%250Apioneering%2520learning-based%2520cooperative%2520coevolution%2520framework%2520that%2520dynamically%250Aschedules%2520decomposition%2520strategies%2520during%2520optimization%2520processes.%2520The%250Adecomposition%2520strategy%2520selector%2520is%2520parameterized%2520through%2520a%2520neural%2520network%252C%250Awhich%2520processes%2520a%2520meticulously%2520crafted%2520set%2520of%2520optimization%2520status%2520features%2520to%250Adetermine%2520the%2520optimal%2520strategy%2520for%2520each%2520optimization%2520step.%2520The%2520network%2520is%250Atrained%2520via%2520the%2520Proximal%2520Policy%2520Optimization%2520method%2520in%2520a%2520reinforcement%2520learning%250Amanner%2520across%2520a%2520collection%2520of%2520representative%2520problems%252C%2520aiming%2520to%2520maximize%2520the%250Aexpected%2520optimization%2520performance.%2520Extensive%2520experimental%2520results%2520demonstrate%250Athat%2520LCC%2520not%2520only%2520offers%2520certain%2520advantages%2520over%2520state-of-the-art%2520baselines%2520in%250Aterms%2520of%2520optimization%2520effectiveness%2520and%2520resource%2520consumption%252C%2520but%2520it%2520also%250Aexhibits%2520promising%2520transferability%2520towards%2520unseen%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17578v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20CMA-ES%20with%20Learning-Based%20Cooperative%20Coevolution%20for%0A%20%20Scalable%20Optimization&entry.906535625=Hongshu%20Guo%20and%20Wenjie%20Qiu%20and%20Zeyuan%20Ma%20and%20Xinglin%20Zhang%20and%20Jun%20Zhang%20and%20Yue-Jiao%20Gong&entry.1292438233=%20%20Recent%20research%20in%20Cooperative%20Coevolution~%28CC%29%20have%20achieved%20promising%0Aprogress%20in%20solving%20large-scale%20global%20optimization%20problems.%20However%2C%20existing%0ACC%20paradigms%20have%20a%20primary%20limitation%20in%20that%20they%20require%20deep%20expertise%20for%0Aselecting%20or%20designing%20effective%20variable%20decomposition%20strategies.%20Inspired%20by%0Aadvancements%20in%20Meta-Black-Box%20Optimization%2C%20this%20paper%20introduces%20LCC%2C%20a%0Apioneering%20learning-based%20cooperative%20coevolution%20framework%20that%20dynamically%0Aschedules%20decomposition%20strategies%20during%20optimization%20processes.%20The%0Adecomposition%20strategy%20selector%20is%20parameterized%20through%20a%20neural%20network%2C%0Awhich%20processes%20a%20meticulously%20crafted%20set%20of%20optimization%20status%20features%20to%0Adetermine%20the%20optimal%20strategy%20for%20each%20optimization%20step.%20The%20network%20is%0Atrained%20via%20the%20Proximal%20Policy%20Optimization%20method%20in%20a%20reinforcement%20learning%0Amanner%20across%20a%20collection%20of%20representative%20problems%2C%20aiming%20to%20maximize%20the%0Aexpected%20optimization%20performance.%20Extensive%20experimental%20results%20demonstrate%0Athat%20LCC%20not%20only%20offers%20certain%20advantages%20over%20state-of-the-art%20baselines%20in%0Aterms%20of%20optimization%20effectiveness%20and%20resource%20consumption%2C%20but%20it%20also%0Aexhibits%20promising%20transferability%20towards%20unseen%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17578v1&entry.124074799=Read"},
{"title": "Embedding Empirical Distributions for Computing Optimal Transport Maps", "author": "Mingchen Jiang and Peng Xu and Xichen Ye and Xiaohui Chen and Yun Yang and Yifan Chen", "abstract": "  Distributional data have become increasingly prominent in modern signal\nprocessing, highlighting the necessity of computing optimal transport (OT) maps\nacross multiple probability distributions. Nevertheless, recent studies on\nneural OT methods predominantly focused on the efficient computation of a\nsingle map between two distributions. To address this challenge, we introduce a\nnovel approach to learning transport maps for new empirical distributions.\nSpecifically, we employ the transformer architecture to produce embeddings from\ndistributional data of varying length; these embeddings are then fed into a\nhypernetwork to generate neural OT maps. Various numerical experiments were\nconducted to validate the embeddings and the generated OT maps. The model\nimplementation and the code are provided on\nhttps://github.com/jiangmingchen/HOTET.\n", "link": "http://arxiv.org/abs/2504.17740v1", "date": "2025-04-24", "relevancy": 2.0123, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5347}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5027}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embedding%20Empirical%20Distributions%20for%20Computing%20Optimal%20Transport%20Maps&body=Title%3A%20Embedding%20Empirical%20Distributions%20for%20Computing%20Optimal%20Transport%20Maps%0AAuthor%3A%20Mingchen%20Jiang%20and%20Peng%20Xu%20and%20Xichen%20Ye%20and%20Xiaohui%20Chen%20and%20Yun%20Yang%20and%20Yifan%20Chen%0AAbstract%3A%20%20%20Distributional%20data%20have%20become%20increasingly%20prominent%20in%20modern%20signal%0Aprocessing%2C%20highlighting%20the%20necessity%20of%20computing%20optimal%20transport%20%28OT%29%20maps%0Aacross%20multiple%20probability%20distributions.%20Nevertheless%2C%20recent%20studies%20on%0Aneural%20OT%20methods%20predominantly%20focused%20on%20the%20efficient%20computation%20of%20a%0Asingle%20map%20between%20two%20distributions.%20To%20address%20this%20challenge%2C%20we%20introduce%20a%0Anovel%20approach%20to%20learning%20transport%20maps%20for%20new%20empirical%20distributions.%0ASpecifically%2C%20we%20employ%20the%20transformer%20architecture%20to%20produce%20embeddings%20from%0Adistributional%20data%20of%20varying%20length%3B%20these%20embeddings%20are%20then%20fed%20into%20a%0Ahypernetwork%20to%20generate%20neural%20OT%20maps.%20Various%20numerical%20experiments%20were%0Aconducted%20to%20validate%20the%20embeddings%20and%20the%20generated%20OT%20maps.%20The%20model%0Aimplementation%20and%20the%20code%20are%20provided%20on%0Ahttps%3A//github.com/jiangmingchen/HOTET.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17740v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbedding%2520Empirical%2520Distributions%2520for%2520Computing%2520Optimal%2520Transport%2520Maps%26entry.906535625%3DMingchen%2520Jiang%2520and%2520Peng%2520Xu%2520and%2520Xichen%2520Ye%2520and%2520Xiaohui%2520Chen%2520and%2520Yun%2520Yang%2520and%2520Yifan%2520Chen%26entry.1292438233%3D%2520%2520Distributional%2520data%2520have%2520become%2520increasingly%2520prominent%2520in%2520modern%2520signal%250Aprocessing%252C%2520highlighting%2520the%2520necessity%2520of%2520computing%2520optimal%2520transport%2520%2528OT%2529%2520maps%250Aacross%2520multiple%2520probability%2520distributions.%2520Nevertheless%252C%2520recent%2520studies%2520on%250Aneural%2520OT%2520methods%2520predominantly%2520focused%2520on%2520the%2520efficient%2520computation%2520of%2520a%250Asingle%2520map%2520between%2520two%2520distributions.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520a%250Anovel%2520approach%2520to%2520learning%2520transport%2520maps%2520for%2520new%2520empirical%2520distributions.%250ASpecifically%252C%2520we%2520employ%2520the%2520transformer%2520architecture%2520to%2520produce%2520embeddings%2520from%250Adistributional%2520data%2520of%2520varying%2520length%253B%2520these%2520embeddings%2520are%2520then%2520fed%2520into%2520a%250Ahypernetwork%2520to%2520generate%2520neural%2520OT%2520maps.%2520Various%2520numerical%2520experiments%2520were%250Aconducted%2520to%2520validate%2520the%2520embeddings%2520and%2520the%2520generated%2520OT%2520maps.%2520The%2520model%250Aimplementation%2520and%2520the%2520code%2520are%2520provided%2520on%250Ahttps%253A//github.com/jiangmingchen/HOTET.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17740v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embedding%20Empirical%20Distributions%20for%20Computing%20Optimal%20Transport%20Maps&entry.906535625=Mingchen%20Jiang%20and%20Peng%20Xu%20and%20Xichen%20Ye%20and%20Xiaohui%20Chen%20and%20Yun%20Yang%20and%20Yifan%20Chen&entry.1292438233=%20%20Distributional%20data%20have%20become%20increasingly%20prominent%20in%20modern%20signal%0Aprocessing%2C%20highlighting%20the%20necessity%20of%20computing%20optimal%20transport%20%28OT%29%20maps%0Aacross%20multiple%20probability%20distributions.%20Nevertheless%2C%20recent%20studies%20on%0Aneural%20OT%20methods%20predominantly%20focused%20on%20the%20efficient%20computation%20of%20a%0Asingle%20map%20between%20two%20distributions.%20To%20address%20this%20challenge%2C%20we%20introduce%20a%0Anovel%20approach%20to%20learning%20transport%20maps%20for%20new%20empirical%20distributions.%0ASpecifically%2C%20we%20employ%20the%20transformer%20architecture%20to%20produce%20embeddings%20from%0Adistributional%20data%20of%20varying%20length%3B%20these%20embeddings%20are%20then%20fed%20into%20a%0Ahypernetwork%20to%20generate%20neural%20OT%20maps.%20Various%20numerical%20experiments%20were%0Aconducted%20to%20validate%20the%20embeddings%20and%20the%20generated%20OT%20maps.%20The%20model%0Aimplementation%20and%20the%20code%20are%20provided%20on%0Ahttps%3A//github.com/jiangmingchen/HOTET.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17740v1&entry.124074799=Read"},
{"title": "Energy Considerations of Large Language Model Inference and Efficiency\n  Optimizations", "author": "Jared Fernandez and Clara Na and Vashisth Tiwari and Yonatan Bisk and Sasha Luccioni and Emma Strubell", "abstract": "  As large language models (LLMs) scale in size and adoption, their\ncomputational and environmental costs continue to rise. Prior benchmarking\nefforts have primarily focused on latency reduction in idealized settings,\noften overlooking the diverse real-world inference workloads that shape energy\nuse. In this work, we systematically analyze the energy implications of common\ninference efficiency optimizations across diverse Natural Language Processing\n(NLP) and generative Artificial Intelligence (AI) workloads, including\nconversational AI and code generation. We introduce a modeling approach that\napproximates real-world LLM workflows through a binning strategy for\ninput-output token distributions and batch size variations. Our empirical\nanalysis spans software frameworks, decoding strategies, GPU architectures,\nonline and offline serving settings, and model parallelism configurations. We\nshow that the effectiveness of inference optimizations is highly sensitive to\nworkload geometry, software stack, and hardware accelerators, demonstrating\nthat naive energy estimates based on FLOPs or theoretical GPU utilization\nsignificantly underestimate real-world energy consumption. Our findings reveal\nthat the proper application of relevant inference efficiency optimizations can\nreduce total energy use by up to 73% from unoptimized baselines. These insights\nprovide a foundation for sustainable LLM deployment and inform energy-efficient\ndesign strategies for future AI infrastructure.\n", "link": "http://arxiv.org/abs/2504.17674v1", "date": "2025-04-24", "relevancy": 2.007, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5034}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5034}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Energy%20Considerations%20of%20Large%20Language%20Model%20Inference%20and%20Efficiency%0A%20%20Optimizations&body=Title%3A%20Energy%20Considerations%20of%20Large%20Language%20Model%20Inference%20and%20Efficiency%0A%20%20Optimizations%0AAuthor%3A%20Jared%20Fernandez%20and%20Clara%20Na%20and%20Vashisth%20Tiwari%20and%20Yonatan%20Bisk%20and%20Sasha%20Luccioni%20and%20Emma%20Strubell%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LLMs%29%20scale%20in%20size%20and%20adoption%2C%20their%0Acomputational%20and%20environmental%20costs%20continue%20to%20rise.%20Prior%20benchmarking%0Aefforts%20have%20primarily%20focused%20on%20latency%20reduction%20in%20idealized%20settings%2C%0Aoften%20overlooking%20the%20diverse%20real-world%20inference%20workloads%20that%20shape%20energy%0Ause.%20In%20this%20work%2C%20we%20systematically%20analyze%20the%20energy%20implications%20of%20common%0Ainference%20efficiency%20optimizations%20across%20diverse%20Natural%20Language%20Processing%0A%28NLP%29%20and%20generative%20Artificial%20Intelligence%20%28AI%29%20workloads%2C%20including%0Aconversational%20AI%20and%20code%20generation.%20We%20introduce%20a%20modeling%20approach%20that%0Aapproximates%20real-world%20LLM%20workflows%20through%20a%20binning%20strategy%20for%0Ainput-output%20token%20distributions%20and%20batch%20size%20variations.%20Our%20empirical%0Aanalysis%20spans%20software%20frameworks%2C%20decoding%20strategies%2C%20GPU%20architectures%2C%0Aonline%20and%20offline%20serving%20settings%2C%20and%20model%20parallelism%20configurations.%20We%0Ashow%20that%20the%20effectiveness%20of%20inference%20optimizations%20is%20highly%20sensitive%20to%0Aworkload%20geometry%2C%20software%20stack%2C%20and%20hardware%20accelerators%2C%20demonstrating%0Athat%20naive%20energy%20estimates%20based%20on%20FLOPs%20or%20theoretical%20GPU%20utilization%0Asignificantly%20underestimate%20real-world%20energy%20consumption.%20Our%20findings%20reveal%0Athat%20the%20proper%20application%20of%20relevant%20inference%20efficiency%20optimizations%20can%0Areduce%20total%20energy%20use%20by%20up%20to%2073%25%20from%20unoptimized%20baselines.%20These%20insights%0Aprovide%20a%20foundation%20for%20sustainable%20LLM%20deployment%20and%20inform%20energy-efficient%0Adesign%20strategies%20for%20future%20AI%20infrastructure.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17674v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnergy%2520Considerations%2520of%2520Large%2520Language%2520Model%2520Inference%2520and%2520Efficiency%250A%2520%2520Optimizations%26entry.906535625%3DJared%2520Fernandez%2520and%2520Clara%2520Na%2520and%2520Vashisth%2520Tiwari%2520and%2520Yonatan%2520Bisk%2520and%2520Sasha%2520Luccioni%2520and%2520Emma%2520Strubell%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520%2528LLMs%2529%2520scale%2520in%2520size%2520and%2520adoption%252C%2520their%250Acomputational%2520and%2520environmental%2520costs%2520continue%2520to%2520rise.%2520Prior%2520benchmarking%250Aefforts%2520have%2520primarily%2520focused%2520on%2520latency%2520reduction%2520in%2520idealized%2520settings%252C%250Aoften%2520overlooking%2520the%2520diverse%2520real-world%2520inference%2520workloads%2520that%2520shape%2520energy%250Ause.%2520In%2520this%2520work%252C%2520we%2520systematically%2520analyze%2520the%2520energy%2520implications%2520of%2520common%250Ainference%2520efficiency%2520optimizations%2520across%2520diverse%2520Natural%2520Language%2520Processing%250A%2528NLP%2529%2520and%2520generative%2520Artificial%2520Intelligence%2520%2528AI%2529%2520workloads%252C%2520including%250Aconversational%2520AI%2520and%2520code%2520generation.%2520We%2520introduce%2520a%2520modeling%2520approach%2520that%250Aapproximates%2520real-world%2520LLM%2520workflows%2520through%2520a%2520binning%2520strategy%2520for%250Ainput-output%2520token%2520distributions%2520and%2520batch%2520size%2520variations.%2520Our%2520empirical%250Aanalysis%2520spans%2520software%2520frameworks%252C%2520decoding%2520strategies%252C%2520GPU%2520architectures%252C%250Aonline%2520and%2520offline%2520serving%2520settings%252C%2520and%2520model%2520parallelism%2520configurations.%2520We%250Ashow%2520that%2520the%2520effectiveness%2520of%2520inference%2520optimizations%2520is%2520highly%2520sensitive%2520to%250Aworkload%2520geometry%252C%2520software%2520stack%252C%2520and%2520hardware%2520accelerators%252C%2520demonstrating%250Athat%2520naive%2520energy%2520estimates%2520based%2520on%2520FLOPs%2520or%2520theoretical%2520GPU%2520utilization%250Asignificantly%2520underestimate%2520real-world%2520energy%2520consumption.%2520Our%2520findings%2520reveal%250Athat%2520the%2520proper%2520application%2520of%2520relevant%2520inference%2520efficiency%2520optimizations%2520can%250Areduce%2520total%2520energy%2520use%2520by%2520up%2520to%252073%2525%2520from%2520unoptimized%2520baselines.%2520These%2520insights%250Aprovide%2520a%2520foundation%2520for%2520sustainable%2520LLM%2520deployment%2520and%2520inform%2520energy-efficient%250Adesign%2520strategies%2520for%2520future%2520AI%2520infrastructure.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17674v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Energy%20Considerations%20of%20Large%20Language%20Model%20Inference%20and%20Efficiency%0A%20%20Optimizations&entry.906535625=Jared%20Fernandez%20and%20Clara%20Na%20and%20Vashisth%20Tiwari%20and%20Yonatan%20Bisk%20and%20Sasha%20Luccioni%20and%20Emma%20Strubell&entry.1292438233=%20%20As%20large%20language%20models%20%28LLMs%29%20scale%20in%20size%20and%20adoption%2C%20their%0Acomputational%20and%20environmental%20costs%20continue%20to%20rise.%20Prior%20benchmarking%0Aefforts%20have%20primarily%20focused%20on%20latency%20reduction%20in%20idealized%20settings%2C%0Aoften%20overlooking%20the%20diverse%20real-world%20inference%20workloads%20that%20shape%20energy%0Ause.%20In%20this%20work%2C%20we%20systematically%20analyze%20the%20energy%20implications%20of%20common%0Ainference%20efficiency%20optimizations%20across%20diverse%20Natural%20Language%20Processing%0A%28NLP%29%20and%20generative%20Artificial%20Intelligence%20%28AI%29%20workloads%2C%20including%0Aconversational%20AI%20and%20code%20generation.%20We%20introduce%20a%20modeling%20approach%20that%0Aapproximates%20real-world%20LLM%20workflows%20through%20a%20binning%20strategy%20for%0Ainput-output%20token%20distributions%20and%20batch%20size%20variations.%20Our%20empirical%0Aanalysis%20spans%20software%20frameworks%2C%20decoding%20strategies%2C%20GPU%20architectures%2C%0Aonline%20and%20offline%20serving%20settings%2C%20and%20model%20parallelism%20configurations.%20We%0Ashow%20that%20the%20effectiveness%20of%20inference%20optimizations%20is%20highly%20sensitive%20to%0Aworkload%20geometry%2C%20software%20stack%2C%20and%20hardware%20accelerators%2C%20demonstrating%0Athat%20naive%20energy%20estimates%20based%20on%20FLOPs%20or%20theoretical%20GPU%20utilization%0Asignificantly%20underestimate%20real-world%20energy%20consumption.%20Our%20findings%20reveal%0Athat%20the%20proper%20application%20of%20relevant%20inference%20efficiency%20optimizations%20can%0Areduce%20total%20energy%20use%20by%20up%20to%2073%25%20from%20unoptimized%20baselines.%20These%20insights%0Aprovide%20a%20foundation%20for%20sustainable%20LLM%20deployment%20and%20inform%20energy-efficient%0Adesign%20strategies%20for%20future%20AI%20infrastructure.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17674v1&entry.124074799=Read"},
{"title": "Towards Robust LLMs: an Adversarial Robustness Measurement Framework", "author": "Natan Levy and Adiel Ashrov and Guy Katz", "abstract": "  The rise of Large Language Models (LLMs) has revolutionized artificial\nintelligence, yet these models remain vulnerable to adversarial perturbations,\nundermining their reliability in high-stakes applications. While adversarial\nrobustness in vision-based neural networks has been extensively studied, LLM\nrobustness remains under-explored. We adapt the Robustness Measurement and\nAssessment (RoMA) framework to quantify LLM resilience against adversarial\ninputs without requiring access to model parameters. By comparing RoMA's\nestimates to those of formal verification methods, we demonstrate its accuracy\nwith minimal error margins while maintaining computational efficiency. Our\nempirical evaluation reveals that robustness varies significantly not only\nbetween different models but also across categories within the same task and\nbetween various types of perturbations. This non-uniformity underscores the\nneed for task-specific robustness evaluations, enabling practitioners to\ncompare and select models based on application-specific robustness\nrequirements. Our work provides a systematic methodology to assess LLM\nrobustness, advancing the development of more reliable language models for\nreal-world deployment.\n", "link": "http://arxiv.org/abs/2504.17723v1", "date": "2025-04-24", "relevancy": 1.989, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5073}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5006}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Robust%20LLMs%3A%20an%20Adversarial%20Robustness%20Measurement%20Framework&body=Title%3A%20Towards%20Robust%20LLMs%3A%20an%20Adversarial%20Robustness%20Measurement%20Framework%0AAuthor%3A%20Natan%20Levy%20and%20Adiel%20Ashrov%20and%20Guy%20Katz%0AAbstract%3A%20%20%20The%20rise%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20revolutionized%20artificial%0Aintelligence%2C%20yet%20these%20models%20remain%20vulnerable%20to%20adversarial%20perturbations%2C%0Aundermining%20their%20reliability%20in%20high-stakes%20applications.%20While%20adversarial%0Arobustness%20in%20vision-based%20neural%20networks%20has%20been%20extensively%20studied%2C%20LLM%0Arobustness%20remains%20under-explored.%20We%20adapt%20the%20Robustness%20Measurement%20and%0AAssessment%20%28RoMA%29%20framework%20to%20quantify%20LLM%20resilience%20against%20adversarial%0Ainputs%20without%20requiring%20access%20to%20model%20parameters.%20By%20comparing%20RoMA%27s%0Aestimates%20to%20those%20of%20formal%20verification%20methods%2C%20we%20demonstrate%20its%20accuracy%0Awith%20minimal%20error%20margins%20while%20maintaining%20computational%20efficiency.%20Our%0Aempirical%20evaluation%20reveals%20that%20robustness%20varies%20significantly%20not%20only%0Abetween%20different%20models%20but%20also%20across%20categories%20within%20the%20same%20task%20and%0Abetween%20various%20types%20of%20perturbations.%20This%20non-uniformity%20underscores%20the%0Aneed%20for%20task-specific%20robustness%20evaluations%2C%20enabling%20practitioners%20to%0Acompare%20and%20select%20models%20based%20on%20application-specific%20robustness%0Arequirements.%20Our%20work%20provides%20a%20systematic%20methodology%20to%20assess%20LLM%0Arobustness%2C%20advancing%20the%20development%20of%20more%20reliable%20language%20models%20for%0Areal-world%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17723v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Robust%2520LLMs%253A%2520an%2520Adversarial%2520Robustness%2520Measurement%2520Framework%26entry.906535625%3DNatan%2520Levy%2520and%2520Adiel%2520Ashrov%2520and%2520Guy%2520Katz%26entry.1292438233%3D%2520%2520The%2520rise%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520revolutionized%2520artificial%250Aintelligence%252C%2520yet%2520these%2520models%2520remain%2520vulnerable%2520to%2520adversarial%2520perturbations%252C%250Aundermining%2520their%2520reliability%2520in%2520high-stakes%2520applications.%2520While%2520adversarial%250Arobustness%2520in%2520vision-based%2520neural%2520networks%2520has%2520been%2520extensively%2520studied%252C%2520LLM%250Arobustness%2520remains%2520under-explored.%2520We%2520adapt%2520the%2520Robustness%2520Measurement%2520and%250AAssessment%2520%2528RoMA%2529%2520framework%2520to%2520quantify%2520LLM%2520resilience%2520against%2520adversarial%250Ainputs%2520without%2520requiring%2520access%2520to%2520model%2520parameters.%2520By%2520comparing%2520RoMA%2527s%250Aestimates%2520to%2520those%2520of%2520formal%2520verification%2520methods%252C%2520we%2520demonstrate%2520its%2520accuracy%250Awith%2520minimal%2520error%2520margins%2520while%2520maintaining%2520computational%2520efficiency.%2520Our%250Aempirical%2520evaluation%2520reveals%2520that%2520robustness%2520varies%2520significantly%2520not%2520only%250Abetween%2520different%2520models%2520but%2520also%2520across%2520categories%2520within%2520the%2520same%2520task%2520and%250Abetween%2520various%2520types%2520of%2520perturbations.%2520This%2520non-uniformity%2520underscores%2520the%250Aneed%2520for%2520task-specific%2520robustness%2520evaluations%252C%2520enabling%2520practitioners%2520to%250Acompare%2520and%2520select%2520models%2520based%2520on%2520application-specific%2520robustness%250Arequirements.%2520Our%2520work%2520provides%2520a%2520systematic%2520methodology%2520to%2520assess%2520LLM%250Arobustness%252C%2520advancing%2520the%2520development%2520of%2520more%2520reliable%2520language%2520models%2520for%250Areal-world%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17723v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Robust%20LLMs%3A%20an%20Adversarial%20Robustness%20Measurement%20Framework&entry.906535625=Natan%20Levy%20and%20Adiel%20Ashrov%20and%20Guy%20Katz&entry.1292438233=%20%20The%20rise%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20revolutionized%20artificial%0Aintelligence%2C%20yet%20these%20models%20remain%20vulnerable%20to%20adversarial%20perturbations%2C%0Aundermining%20their%20reliability%20in%20high-stakes%20applications.%20While%20adversarial%0Arobustness%20in%20vision-based%20neural%20networks%20has%20been%20extensively%20studied%2C%20LLM%0Arobustness%20remains%20under-explored.%20We%20adapt%20the%20Robustness%20Measurement%20and%0AAssessment%20%28RoMA%29%20framework%20to%20quantify%20LLM%20resilience%20against%20adversarial%0Ainputs%20without%20requiring%20access%20to%20model%20parameters.%20By%20comparing%20RoMA%27s%0Aestimates%20to%20those%20of%20formal%20verification%20methods%2C%20we%20demonstrate%20its%20accuracy%0Awith%20minimal%20error%20margins%20while%20maintaining%20computational%20efficiency.%20Our%0Aempirical%20evaluation%20reveals%20that%20robustness%20varies%20significantly%20not%20only%0Abetween%20different%20models%20but%20also%20across%20categories%20within%20the%20same%20task%20and%0Abetween%20various%20types%20of%20perturbations.%20This%20non-uniformity%20underscores%20the%0Aneed%20for%20task-specific%20robustness%20evaluations%2C%20enabling%20practitioners%20to%0Acompare%20and%20select%20models%20based%20on%20application-specific%20robustness%0Arequirements.%20Our%20work%20provides%20a%20systematic%20methodology%20to%20assess%20LLM%0Arobustness%2C%20advancing%20the%20development%20of%20more%20reliable%20language%20models%20for%0Areal-world%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17723v1&entry.124074799=Read"},
{"title": "Communication-Efficient Personalized Distributed Learning with Data and\n  Node Heterogeneity", "author": "Zhuojun Tian and Zhaoyang Zhang and Yiwei Li and Mehdi Bennis", "abstract": "  To jointly tackle the challenges of data and node heterogeneity in\ndecentralized learning, we propose a distributed strong lottery ticket\nhypothesis (DSLTH), based on which a communication-efficient personalized\nlearning algorithm is developed. In the proposed method, each local model is\nrepresented as the Hadamard product of global real-valued parameters and a\npersonalized binary mask for pruning. The local model is learned by updating\nand fusing the personalized binary masks while the real-valued parameters are\nfixed among different agents. To further reduce the complexity of hardware\nimplementation, we incorporate a group sparse regularization term in the loss\nfunction, enabling the learned local model to achieve structured sparsity.\nThen, a binary mask aggregation algorithm is designed by introducing an\nintermediate aggregation tensor and adding a personalized fine-tuning step in\neach iteration, which constrains model updates towards the local data\ndistribution. The proposed method effectively leverages the relativity among\nagents while meeting personalized requirements in heterogeneous node\nconditions. We also provide a theoretical proof for the DSLTH, establishing it\nas the foundation of the proposed method. Numerical simulations confirm the\nvalidity of the DSLTH and demonstrate the effectiveness of the proposed\nalgorithm.\n", "link": "http://arxiv.org/abs/2504.17520v1", "date": "2025-04-24", "relevancy": 1.9853, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5126}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5031}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Communication-Efficient%20Personalized%20Distributed%20Learning%20with%20Data%20and%0A%20%20Node%20Heterogeneity&body=Title%3A%20Communication-Efficient%20Personalized%20Distributed%20Learning%20with%20Data%20and%0A%20%20Node%20Heterogeneity%0AAuthor%3A%20Zhuojun%20Tian%20and%20Zhaoyang%20Zhang%20and%20Yiwei%20Li%20and%20Mehdi%20Bennis%0AAbstract%3A%20%20%20To%20jointly%20tackle%20the%20challenges%20of%20data%20and%20node%20heterogeneity%20in%0Adecentralized%20learning%2C%20we%20propose%20a%20distributed%20strong%20lottery%20ticket%0Ahypothesis%20%28DSLTH%29%2C%20based%20on%20which%20a%20communication-efficient%20personalized%0Alearning%20algorithm%20is%20developed.%20In%20the%20proposed%20method%2C%20each%20local%20model%20is%0Arepresented%20as%20the%20Hadamard%20product%20of%20global%20real-valued%20parameters%20and%20a%0Apersonalized%20binary%20mask%20for%20pruning.%20The%20local%20model%20is%20learned%20by%20updating%0Aand%20fusing%20the%20personalized%20binary%20masks%20while%20the%20real-valued%20parameters%20are%0Afixed%20among%20different%20agents.%20To%20further%20reduce%20the%20complexity%20of%20hardware%0Aimplementation%2C%20we%20incorporate%20a%20group%20sparse%20regularization%20term%20in%20the%20loss%0Afunction%2C%20enabling%20the%20learned%20local%20model%20to%20achieve%20structured%20sparsity.%0AThen%2C%20a%20binary%20mask%20aggregation%20algorithm%20is%20designed%20by%20introducing%20an%0Aintermediate%20aggregation%20tensor%20and%20adding%20a%20personalized%20fine-tuning%20step%20in%0Aeach%20iteration%2C%20which%20constrains%20model%20updates%20towards%20the%20local%20data%0Adistribution.%20The%20proposed%20method%20effectively%20leverages%20the%20relativity%20among%0Aagents%20while%20meeting%20personalized%20requirements%20in%20heterogeneous%20node%0Aconditions.%20We%20also%20provide%20a%20theoretical%20proof%20for%20the%20DSLTH%2C%20establishing%20it%0Aas%20the%20foundation%20of%20the%20proposed%20method.%20Numerical%20simulations%20confirm%20the%0Avalidity%20of%20the%20DSLTH%20and%20demonstrate%20the%20effectiveness%20of%20the%20proposed%0Aalgorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17520v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommunication-Efficient%2520Personalized%2520Distributed%2520Learning%2520with%2520Data%2520and%250A%2520%2520Node%2520Heterogeneity%26entry.906535625%3DZhuojun%2520Tian%2520and%2520Zhaoyang%2520Zhang%2520and%2520Yiwei%2520Li%2520and%2520Mehdi%2520Bennis%26entry.1292438233%3D%2520%2520To%2520jointly%2520tackle%2520the%2520challenges%2520of%2520data%2520and%2520node%2520heterogeneity%2520in%250Adecentralized%2520learning%252C%2520we%2520propose%2520a%2520distributed%2520strong%2520lottery%2520ticket%250Ahypothesis%2520%2528DSLTH%2529%252C%2520based%2520on%2520which%2520a%2520communication-efficient%2520personalized%250Alearning%2520algorithm%2520is%2520developed.%2520In%2520the%2520proposed%2520method%252C%2520each%2520local%2520model%2520is%250Arepresented%2520as%2520the%2520Hadamard%2520product%2520of%2520global%2520real-valued%2520parameters%2520and%2520a%250Apersonalized%2520binary%2520mask%2520for%2520pruning.%2520The%2520local%2520model%2520is%2520learned%2520by%2520updating%250Aand%2520fusing%2520the%2520personalized%2520binary%2520masks%2520while%2520the%2520real-valued%2520parameters%2520are%250Afixed%2520among%2520different%2520agents.%2520To%2520further%2520reduce%2520the%2520complexity%2520of%2520hardware%250Aimplementation%252C%2520we%2520incorporate%2520a%2520group%2520sparse%2520regularization%2520term%2520in%2520the%2520loss%250Afunction%252C%2520enabling%2520the%2520learned%2520local%2520model%2520to%2520achieve%2520structured%2520sparsity.%250AThen%252C%2520a%2520binary%2520mask%2520aggregation%2520algorithm%2520is%2520designed%2520by%2520introducing%2520an%250Aintermediate%2520aggregation%2520tensor%2520and%2520adding%2520a%2520personalized%2520fine-tuning%2520step%2520in%250Aeach%2520iteration%252C%2520which%2520constrains%2520model%2520updates%2520towards%2520the%2520local%2520data%250Adistribution.%2520The%2520proposed%2520method%2520effectively%2520leverages%2520the%2520relativity%2520among%250Aagents%2520while%2520meeting%2520personalized%2520requirements%2520in%2520heterogeneous%2520node%250Aconditions.%2520We%2520also%2520provide%2520a%2520theoretical%2520proof%2520for%2520the%2520DSLTH%252C%2520establishing%2520it%250Aas%2520the%2520foundation%2520of%2520the%2520proposed%2520method.%2520Numerical%2520simulations%2520confirm%2520the%250Avalidity%2520of%2520the%2520DSLTH%2520and%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%250Aalgorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17520v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Communication-Efficient%20Personalized%20Distributed%20Learning%20with%20Data%20and%0A%20%20Node%20Heterogeneity&entry.906535625=Zhuojun%20Tian%20and%20Zhaoyang%20Zhang%20and%20Yiwei%20Li%20and%20Mehdi%20Bennis&entry.1292438233=%20%20To%20jointly%20tackle%20the%20challenges%20of%20data%20and%20node%20heterogeneity%20in%0Adecentralized%20learning%2C%20we%20propose%20a%20distributed%20strong%20lottery%20ticket%0Ahypothesis%20%28DSLTH%29%2C%20based%20on%20which%20a%20communication-efficient%20personalized%0Alearning%20algorithm%20is%20developed.%20In%20the%20proposed%20method%2C%20each%20local%20model%20is%0Arepresented%20as%20the%20Hadamard%20product%20of%20global%20real-valued%20parameters%20and%20a%0Apersonalized%20binary%20mask%20for%20pruning.%20The%20local%20model%20is%20learned%20by%20updating%0Aand%20fusing%20the%20personalized%20binary%20masks%20while%20the%20real-valued%20parameters%20are%0Afixed%20among%20different%20agents.%20To%20further%20reduce%20the%20complexity%20of%20hardware%0Aimplementation%2C%20we%20incorporate%20a%20group%20sparse%20regularization%20term%20in%20the%20loss%0Afunction%2C%20enabling%20the%20learned%20local%20model%20to%20achieve%20structured%20sparsity.%0AThen%2C%20a%20binary%20mask%20aggregation%20algorithm%20is%20designed%20by%20introducing%20an%0Aintermediate%20aggregation%20tensor%20and%20adding%20a%20personalized%20fine-tuning%20step%20in%0Aeach%20iteration%2C%20which%20constrains%20model%20updates%20towards%20the%20local%20data%0Adistribution.%20The%20proposed%20method%20effectively%20leverages%20the%20relativity%20among%0Aagents%20while%20meeting%20personalized%20requirements%20in%20heterogeneous%20node%0Aconditions.%20We%20also%20provide%20a%20theoretical%20proof%20for%20the%20DSLTH%2C%20establishing%20it%0Aas%20the%20foundation%20of%20the%20proposed%20method.%20Numerical%20simulations%20confirm%20the%0Avalidity%20of%20the%20DSLTH%20and%20demonstrate%20the%20effectiveness%20of%20the%20proposed%0Aalgorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17520v1&entry.124074799=Read"},
{"title": "An introduction to R package `mvs`", "author": "Wouter van Loon", "abstract": "  In biomedical science, a set of objects or persons can often be described by\nmultiple distinct sets of features obtained from different data sources or\nmodalities (called \"multi-view data\"). Classical machine learning methods\nignore the multi-view structure of such data, limiting model interpretability\nand performance. The R package `mvs` provides methods that were designed\nspecifically for dealing with multi-view data, based on the multi-view stacking\n(MVS) framework. MVS is a form of supervised (machine) learning used to train\nmulti-view classification or prediction models. MVS works by training a\nlearning algorithm on each view separately, estimating the predictive power of\neach view-specific model through cross-validation, and then using another\nlearning algorithm to assign weights to the view-specific models based on their\nestimated predictions. MVS is a form of ensemble learning, dividing the large\nmulti-view learning problem into smaller sub-problems. Most of these\nsub-problems can be solved in parallel, making it computationally attractive.\nAdditionally, the number of features of the sub-problems is greatly reduced\ncompared with the full multi-view learning problem. This makes MVS especially\nuseful when the total number of features is larger than the number of\nobservations (i.e., high-dimensional data). MVS can still be applied even if\nthe sub-problems are themselves high-dimensional by adding suitable penalty\nterms to the learning algorithms. Furthermore, MVS can be used to automatically\nselect the views which are most important for prediction. The R package `mvs`\nmakes fitting MVS models, including such penalty terms, easily and openly\naccessible. `mvs` allows for the fitting of stacked models with any number of\nlevels, with different penalty terms, different outcome distributions, and\nprovides several options for missing data handling.\n", "link": "http://arxiv.org/abs/2504.17546v1", "date": "2025-04-24", "relevancy": 1.9688, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4996}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4906}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20introduction%20to%20R%20package%20%60mvs%60&body=Title%3A%20An%20introduction%20to%20R%20package%20%60mvs%60%0AAuthor%3A%20Wouter%20van%20Loon%0AAbstract%3A%20%20%20In%20biomedical%20science%2C%20a%20set%20of%20objects%20or%20persons%20can%20often%20be%20described%20by%0Amultiple%20distinct%20sets%20of%20features%20obtained%20from%20different%20data%20sources%20or%0Amodalities%20%28called%20%22multi-view%20data%22%29.%20Classical%20machine%20learning%20methods%0Aignore%20the%20multi-view%20structure%20of%20such%20data%2C%20limiting%20model%20interpretability%0Aand%20performance.%20The%20R%20package%20%60mvs%60%20provides%20methods%20that%20were%20designed%0Aspecifically%20for%20dealing%20with%20multi-view%20data%2C%20based%20on%20the%20multi-view%20stacking%0A%28MVS%29%20framework.%20MVS%20is%20a%20form%20of%20supervised%20%28machine%29%20learning%20used%20to%20train%0Amulti-view%20classification%20or%20prediction%20models.%20MVS%20works%20by%20training%20a%0Alearning%20algorithm%20on%20each%20view%20separately%2C%20estimating%20the%20predictive%20power%20of%0Aeach%20view-specific%20model%20through%20cross-validation%2C%20and%20then%20using%20another%0Alearning%20algorithm%20to%20assign%20weights%20to%20the%20view-specific%20models%20based%20on%20their%0Aestimated%20predictions.%20MVS%20is%20a%20form%20of%20ensemble%20learning%2C%20dividing%20the%20large%0Amulti-view%20learning%20problem%20into%20smaller%20sub-problems.%20Most%20of%20these%0Asub-problems%20can%20be%20solved%20in%20parallel%2C%20making%20it%20computationally%20attractive.%0AAdditionally%2C%20the%20number%20of%20features%20of%20the%20sub-problems%20is%20greatly%20reduced%0Acompared%20with%20the%20full%20multi-view%20learning%20problem.%20This%20makes%20MVS%20especially%0Auseful%20when%20the%20total%20number%20of%20features%20is%20larger%20than%20the%20number%20of%0Aobservations%20%28i.e.%2C%20high-dimensional%20data%29.%20MVS%20can%20still%20be%20applied%20even%20if%0Athe%20sub-problems%20are%20themselves%20high-dimensional%20by%20adding%20suitable%20penalty%0Aterms%20to%20the%20learning%20algorithms.%20Furthermore%2C%20MVS%20can%20be%20used%20to%20automatically%0Aselect%20the%20views%20which%20are%20most%20important%20for%20prediction.%20The%20R%20package%20%60mvs%60%0Amakes%20fitting%20MVS%20models%2C%20including%20such%20penalty%20terms%2C%20easily%20and%20openly%0Aaccessible.%20%60mvs%60%20allows%20for%20the%20fitting%20of%20stacked%20models%20with%20any%20number%20of%0Alevels%2C%20with%20different%20penalty%20terms%2C%20different%20outcome%20distributions%2C%20and%0Aprovides%20several%20options%20for%20missing%20data%20handling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17546v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520introduction%2520to%2520R%2520package%2520%2560mvs%2560%26entry.906535625%3DWouter%2520van%2520Loon%26entry.1292438233%3D%2520%2520In%2520biomedical%2520science%252C%2520a%2520set%2520of%2520objects%2520or%2520persons%2520can%2520often%2520be%2520described%2520by%250Amultiple%2520distinct%2520sets%2520of%2520features%2520obtained%2520from%2520different%2520data%2520sources%2520or%250Amodalities%2520%2528called%2520%2522multi-view%2520data%2522%2529.%2520Classical%2520machine%2520learning%2520methods%250Aignore%2520the%2520multi-view%2520structure%2520of%2520such%2520data%252C%2520limiting%2520model%2520interpretability%250Aand%2520performance.%2520The%2520R%2520package%2520%2560mvs%2560%2520provides%2520methods%2520that%2520were%2520designed%250Aspecifically%2520for%2520dealing%2520with%2520multi-view%2520data%252C%2520based%2520on%2520the%2520multi-view%2520stacking%250A%2528MVS%2529%2520framework.%2520MVS%2520is%2520a%2520form%2520of%2520supervised%2520%2528machine%2529%2520learning%2520used%2520to%2520train%250Amulti-view%2520classification%2520or%2520prediction%2520models.%2520MVS%2520works%2520by%2520training%2520a%250Alearning%2520algorithm%2520on%2520each%2520view%2520separately%252C%2520estimating%2520the%2520predictive%2520power%2520of%250Aeach%2520view-specific%2520model%2520through%2520cross-validation%252C%2520and%2520then%2520using%2520another%250Alearning%2520algorithm%2520to%2520assign%2520weights%2520to%2520the%2520view-specific%2520models%2520based%2520on%2520their%250Aestimated%2520predictions.%2520MVS%2520is%2520a%2520form%2520of%2520ensemble%2520learning%252C%2520dividing%2520the%2520large%250Amulti-view%2520learning%2520problem%2520into%2520smaller%2520sub-problems.%2520Most%2520of%2520these%250Asub-problems%2520can%2520be%2520solved%2520in%2520parallel%252C%2520making%2520it%2520computationally%2520attractive.%250AAdditionally%252C%2520the%2520number%2520of%2520features%2520of%2520the%2520sub-problems%2520is%2520greatly%2520reduced%250Acompared%2520with%2520the%2520full%2520multi-view%2520learning%2520problem.%2520This%2520makes%2520MVS%2520especially%250Auseful%2520when%2520the%2520total%2520number%2520of%2520features%2520is%2520larger%2520than%2520the%2520number%2520of%250Aobservations%2520%2528i.e.%252C%2520high-dimensional%2520data%2529.%2520MVS%2520can%2520still%2520be%2520applied%2520even%2520if%250Athe%2520sub-problems%2520are%2520themselves%2520high-dimensional%2520by%2520adding%2520suitable%2520penalty%250Aterms%2520to%2520the%2520learning%2520algorithms.%2520Furthermore%252C%2520MVS%2520can%2520be%2520used%2520to%2520automatically%250Aselect%2520the%2520views%2520which%2520are%2520most%2520important%2520for%2520prediction.%2520The%2520R%2520package%2520%2560mvs%2560%250Amakes%2520fitting%2520MVS%2520models%252C%2520including%2520such%2520penalty%2520terms%252C%2520easily%2520and%2520openly%250Aaccessible.%2520%2560mvs%2560%2520allows%2520for%2520the%2520fitting%2520of%2520stacked%2520models%2520with%2520any%2520number%2520of%250Alevels%252C%2520with%2520different%2520penalty%2520terms%252C%2520different%2520outcome%2520distributions%252C%2520and%250Aprovides%2520several%2520options%2520for%2520missing%2520data%2520handling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17546v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20introduction%20to%20R%20package%20%60mvs%60&entry.906535625=Wouter%20van%20Loon&entry.1292438233=%20%20In%20biomedical%20science%2C%20a%20set%20of%20objects%20or%20persons%20can%20often%20be%20described%20by%0Amultiple%20distinct%20sets%20of%20features%20obtained%20from%20different%20data%20sources%20or%0Amodalities%20%28called%20%22multi-view%20data%22%29.%20Classical%20machine%20learning%20methods%0Aignore%20the%20multi-view%20structure%20of%20such%20data%2C%20limiting%20model%20interpretability%0Aand%20performance.%20The%20R%20package%20%60mvs%60%20provides%20methods%20that%20were%20designed%0Aspecifically%20for%20dealing%20with%20multi-view%20data%2C%20based%20on%20the%20multi-view%20stacking%0A%28MVS%29%20framework.%20MVS%20is%20a%20form%20of%20supervised%20%28machine%29%20learning%20used%20to%20train%0Amulti-view%20classification%20or%20prediction%20models.%20MVS%20works%20by%20training%20a%0Alearning%20algorithm%20on%20each%20view%20separately%2C%20estimating%20the%20predictive%20power%20of%0Aeach%20view-specific%20model%20through%20cross-validation%2C%20and%20then%20using%20another%0Alearning%20algorithm%20to%20assign%20weights%20to%20the%20view-specific%20models%20based%20on%20their%0Aestimated%20predictions.%20MVS%20is%20a%20form%20of%20ensemble%20learning%2C%20dividing%20the%20large%0Amulti-view%20learning%20problem%20into%20smaller%20sub-problems.%20Most%20of%20these%0Asub-problems%20can%20be%20solved%20in%20parallel%2C%20making%20it%20computationally%20attractive.%0AAdditionally%2C%20the%20number%20of%20features%20of%20the%20sub-problems%20is%20greatly%20reduced%0Acompared%20with%20the%20full%20multi-view%20learning%20problem.%20This%20makes%20MVS%20especially%0Auseful%20when%20the%20total%20number%20of%20features%20is%20larger%20than%20the%20number%20of%0Aobservations%20%28i.e.%2C%20high-dimensional%20data%29.%20MVS%20can%20still%20be%20applied%20even%20if%0Athe%20sub-problems%20are%20themselves%20high-dimensional%20by%20adding%20suitable%20penalty%0Aterms%20to%20the%20learning%20algorithms.%20Furthermore%2C%20MVS%20can%20be%20used%20to%20automatically%0Aselect%20the%20views%20which%20are%20most%20important%20for%20prediction.%20The%20R%20package%20%60mvs%60%0Amakes%20fitting%20MVS%20models%2C%20including%20such%20penalty%20terms%2C%20easily%20and%20openly%0Aaccessible.%20%60mvs%60%20allows%20for%20the%20fitting%20of%20stacked%20models%20with%20any%20number%20of%0Alevels%2C%20with%20different%20penalty%20terms%2C%20different%20outcome%20distributions%2C%20and%0Aprovides%20several%20options%20for%20missing%20data%20handling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17546v1&entry.124074799=Read"},
{"title": "Longitudinal Control for Autonomous Racing with Combustion Engine\n  Vehicles", "author": "Phillip Pitschi and Simon Sagmeister and Sven Goblirsch and Markus Lienkamp and Boris Lohmann", "abstract": "  Usually, a controller for path- or trajectory tracking is employed in\nautonomous driving. Typically, these controllers generate high-level commands\nlike longitudinal acceleration or force. However, vehicles with combustion\nengines expect different actuation inputs. This paper proposes a longitudinal\ncontrol concept that translates high-level trajectory-tracking commands to the\nrequired low-level vehicle commands such as throttle, brake pressure and a\ndesired gear. We chose a modular structure to easily integrate different\ntrajectory-tracking control algorithms and vehicles. The proposed control\nconcept enables a close tracking of the high-level control command. An\nanti-lock braking system, traction control, and brake warmup control also\nensure a safe operation during real-world tests. We provide experimental\nvalidation of our concept using real world data with longitudinal accelerations\nreaching up to $25 \\, \\frac{\\mathrm{m}}{\\mathrm{s}^2}$. The experiments were\nconducted using the EAV24 racecar during the first event of the Abu Dhabi\nAutonomous Racing League on the Yas Marina Formula 1 Circuit.\n", "link": "http://arxiv.org/abs/2504.17418v1", "date": "2025-04-24", "relevancy": 1.9673, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4988}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4906}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4775}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Longitudinal%20Control%20for%20Autonomous%20Racing%20with%20Combustion%20Engine%0A%20%20Vehicles&body=Title%3A%20Longitudinal%20Control%20for%20Autonomous%20Racing%20with%20Combustion%20Engine%0A%20%20Vehicles%0AAuthor%3A%20Phillip%20Pitschi%20and%20Simon%20Sagmeister%20and%20Sven%20Goblirsch%20and%20Markus%20Lienkamp%20and%20Boris%20Lohmann%0AAbstract%3A%20%20%20Usually%2C%20a%20controller%20for%20path-%20or%20trajectory%20tracking%20is%20employed%20in%0Aautonomous%20driving.%20Typically%2C%20these%20controllers%20generate%20high-level%20commands%0Alike%20longitudinal%20acceleration%20or%20force.%20However%2C%20vehicles%20with%20combustion%0Aengines%20expect%20different%20actuation%20inputs.%20This%20paper%20proposes%20a%20longitudinal%0Acontrol%20concept%20that%20translates%20high-level%20trajectory-tracking%20commands%20to%20the%0Arequired%20low-level%20vehicle%20commands%20such%20as%20throttle%2C%20brake%20pressure%20and%20a%0Adesired%20gear.%20We%20chose%20a%20modular%20structure%20to%20easily%20integrate%20different%0Atrajectory-tracking%20control%20algorithms%20and%20vehicles.%20The%20proposed%20control%0Aconcept%20enables%20a%20close%20tracking%20of%20the%20high-level%20control%20command.%20An%0Aanti-lock%20braking%20system%2C%20traction%20control%2C%20and%20brake%20warmup%20control%20also%0Aensure%20a%20safe%20operation%20during%20real-world%20tests.%20We%20provide%20experimental%0Avalidation%20of%20our%20concept%20using%20real%20world%20data%20with%20longitudinal%20accelerations%0Areaching%20up%20to%20%2425%20%5C%2C%20%5Cfrac%7B%5Cmathrm%7Bm%7D%7D%7B%5Cmathrm%7Bs%7D%5E2%7D%24.%20The%20experiments%20were%0Aconducted%20using%20the%20EAV24%20racecar%20during%20the%20first%20event%20of%20the%20Abu%20Dhabi%0AAutonomous%20Racing%20League%20on%20the%20Yas%20Marina%20Formula%201%20Circuit.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17418v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongitudinal%2520Control%2520for%2520Autonomous%2520Racing%2520with%2520Combustion%2520Engine%250A%2520%2520Vehicles%26entry.906535625%3DPhillip%2520Pitschi%2520and%2520Simon%2520Sagmeister%2520and%2520Sven%2520Goblirsch%2520and%2520Markus%2520Lienkamp%2520and%2520Boris%2520Lohmann%26entry.1292438233%3D%2520%2520Usually%252C%2520a%2520controller%2520for%2520path-%2520or%2520trajectory%2520tracking%2520is%2520employed%2520in%250Aautonomous%2520driving.%2520Typically%252C%2520these%2520controllers%2520generate%2520high-level%2520commands%250Alike%2520longitudinal%2520acceleration%2520or%2520force.%2520However%252C%2520vehicles%2520with%2520combustion%250Aengines%2520expect%2520different%2520actuation%2520inputs.%2520This%2520paper%2520proposes%2520a%2520longitudinal%250Acontrol%2520concept%2520that%2520translates%2520high-level%2520trajectory-tracking%2520commands%2520to%2520the%250Arequired%2520low-level%2520vehicle%2520commands%2520such%2520as%2520throttle%252C%2520brake%2520pressure%2520and%2520a%250Adesired%2520gear.%2520We%2520chose%2520a%2520modular%2520structure%2520to%2520easily%2520integrate%2520different%250Atrajectory-tracking%2520control%2520algorithms%2520and%2520vehicles.%2520The%2520proposed%2520control%250Aconcept%2520enables%2520a%2520close%2520tracking%2520of%2520the%2520high-level%2520control%2520command.%2520An%250Aanti-lock%2520braking%2520system%252C%2520traction%2520control%252C%2520and%2520brake%2520warmup%2520control%2520also%250Aensure%2520a%2520safe%2520operation%2520during%2520real-world%2520tests.%2520We%2520provide%2520experimental%250Avalidation%2520of%2520our%2520concept%2520using%2520real%2520world%2520data%2520with%2520longitudinal%2520accelerations%250Areaching%2520up%2520to%2520%252425%2520%255C%252C%2520%255Cfrac%257B%255Cmathrm%257Bm%257D%257D%257B%255Cmathrm%257Bs%257D%255E2%257D%2524.%2520The%2520experiments%2520were%250Aconducted%2520using%2520the%2520EAV24%2520racecar%2520during%2520the%2520first%2520event%2520of%2520the%2520Abu%2520Dhabi%250AAutonomous%2520Racing%2520League%2520on%2520the%2520Yas%2520Marina%2520Formula%25201%2520Circuit.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17418v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Longitudinal%20Control%20for%20Autonomous%20Racing%20with%20Combustion%20Engine%0A%20%20Vehicles&entry.906535625=Phillip%20Pitschi%20and%20Simon%20Sagmeister%20and%20Sven%20Goblirsch%20and%20Markus%20Lienkamp%20and%20Boris%20Lohmann&entry.1292438233=%20%20Usually%2C%20a%20controller%20for%20path-%20or%20trajectory%20tracking%20is%20employed%20in%0Aautonomous%20driving.%20Typically%2C%20these%20controllers%20generate%20high-level%20commands%0Alike%20longitudinal%20acceleration%20or%20force.%20However%2C%20vehicles%20with%20combustion%0Aengines%20expect%20different%20actuation%20inputs.%20This%20paper%20proposes%20a%20longitudinal%0Acontrol%20concept%20that%20translates%20high-level%20trajectory-tracking%20commands%20to%20the%0Arequired%20low-level%20vehicle%20commands%20such%20as%20throttle%2C%20brake%20pressure%20and%20a%0Adesired%20gear.%20We%20chose%20a%20modular%20structure%20to%20easily%20integrate%20different%0Atrajectory-tracking%20control%20algorithms%20and%20vehicles.%20The%20proposed%20control%0Aconcept%20enables%20a%20close%20tracking%20of%20the%20high-level%20control%20command.%20An%0Aanti-lock%20braking%20system%2C%20traction%20control%2C%20and%20brake%20warmup%20control%20also%0Aensure%20a%20safe%20operation%20during%20real-world%20tests.%20We%20provide%20experimental%0Avalidation%20of%20our%20concept%20using%20real%20world%20data%20with%20longitudinal%20accelerations%0Areaching%20up%20to%20%2425%20%5C%2C%20%5Cfrac%7B%5Cmathrm%7Bm%7D%7D%7B%5Cmathrm%7Bs%7D%5E2%7D%24.%20The%20experiments%20were%0Aconducted%20using%20the%20EAV24%20racecar%20during%20the%20first%20event%20of%20the%20Abu%20Dhabi%0AAutonomous%20Racing%20League%20on%20the%20Yas%20Marina%20Formula%201%20Circuit.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17418v1&entry.124074799=Read"},
{"title": "Cooperative Task Offloading through Asynchronous Deep Reinforcement\n  Learning in Mobile Edge Computing for Future Networks", "author": "Yuelin Liu and Haiyuan Li and Xenofon Vasilakos and Rasheed Hussain and Dimitra Simeonidou", "abstract": "  Future networks (including 6G) are poised to accelerate the realisation of\nInternet of Everything. However, it will result in a high demand for computing\nresources to support new services. Mobile Edge Computing (MEC) is a promising\nsolution, enabling to offload computation-intensive tasks to nearby edge\nservers from the end-user devices, thereby reducing latency and energy\nconsumption. However, relying solely on a single MEC server for task offloading\ncan lead to uneven resource utilisation and suboptimal performance in complex\nscenarios. Additionally, traditional task offloading strategies specialise in\ncentralised policy decisions, which unavoidably entail extreme transmission\nlatency and reach computational bottleneck. To fill the gaps, we propose a\nlatency and energy efficient Cooperative Task Offloading framework with\nTransformer-driven Prediction (CTO-TP), leveraging asynchronous multi-agent\ndeep reinforcement learning to address these challenges. This approach fosters\nedge-edge cooperation and decreases the synchronous waiting time by performing\nasynchronous training, optimising task offloading, and resource allocation\nacross distributed networks. The performance evaluation demonstrates that the\nproposed CTO-TP algorithm reduces up to 80% overall system latency and 87%\nenergy consumption compared to the baseline schemes.\n", "link": "http://arxiv.org/abs/2504.17526v1", "date": "2025-04-24", "relevancy": 1.9665, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5021}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4846}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cooperative%20Task%20Offloading%20through%20Asynchronous%20Deep%20Reinforcement%0A%20%20Learning%20in%20Mobile%20Edge%20Computing%20for%20Future%20Networks&body=Title%3A%20Cooperative%20Task%20Offloading%20through%20Asynchronous%20Deep%20Reinforcement%0A%20%20Learning%20in%20Mobile%20Edge%20Computing%20for%20Future%20Networks%0AAuthor%3A%20Yuelin%20Liu%20and%20Haiyuan%20Li%20and%20Xenofon%20Vasilakos%20and%20Rasheed%20Hussain%20and%20Dimitra%20Simeonidou%0AAbstract%3A%20%20%20Future%20networks%20%28including%206G%29%20are%20poised%20to%20accelerate%20the%20realisation%20of%0AInternet%20of%20Everything.%20However%2C%20it%20will%20result%20in%20a%20high%20demand%20for%20computing%0Aresources%20to%20support%20new%20services.%20Mobile%20Edge%20Computing%20%28MEC%29%20is%20a%20promising%0Asolution%2C%20enabling%20to%20offload%20computation-intensive%20tasks%20to%20nearby%20edge%0Aservers%20from%20the%20end-user%20devices%2C%20thereby%20reducing%20latency%20and%20energy%0Aconsumption.%20However%2C%20relying%20solely%20on%20a%20single%20MEC%20server%20for%20task%20offloading%0Acan%20lead%20to%20uneven%20resource%20utilisation%20and%20suboptimal%20performance%20in%20complex%0Ascenarios.%20Additionally%2C%20traditional%20task%20offloading%20strategies%20specialise%20in%0Acentralised%20policy%20decisions%2C%20which%20unavoidably%20entail%20extreme%20transmission%0Alatency%20and%20reach%20computational%20bottleneck.%20To%20fill%20the%20gaps%2C%20we%20propose%20a%0Alatency%20and%20energy%20efficient%20Cooperative%20Task%20Offloading%20framework%20with%0ATransformer-driven%20Prediction%20%28CTO-TP%29%2C%20leveraging%20asynchronous%20multi-agent%0Adeep%20reinforcement%20learning%20to%20address%20these%20challenges.%20This%20approach%20fosters%0Aedge-edge%20cooperation%20and%20decreases%20the%20synchronous%20waiting%20time%20by%20performing%0Aasynchronous%20training%2C%20optimising%20task%20offloading%2C%20and%20resource%20allocation%0Aacross%20distributed%20networks.%20The%20performance%20evaluation%20demonstrates%20that%20the%0Aproposed%20CTO-TP%20algorithm%20reduces%20up%20to%2080%25%20overall%20system%20latency%20and%2087%25%0Aenergy%20consumption%20compared%20to%20the%20baseline%20schemes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17526v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCooperative%2520Task%2520Offloading%2520through%2520Asynchronous%2520Deep%2520Reinforcement%250A%2520%2520Learning%2520in%2520Mobile%2520Edge%2520Computing%2520for%2520Future%2520Networks%26entry.906535625%3DYuelin%2520Liu%2520and%2520Haiyuan%2520Li%2520and%2520Xenofon%2520Vasilakos%2520and%2520Rasheed%2520Hussain%2520and%2520Dimitra%2520Simeonidou%26entry.1292438233%3D%2520%2520Future%2520networks%2520%2528including%25206G%2529%2520are%2520poised%2520to%2520accelerate%2520the%2520realisation%2520of%250AInternet%2520of%2520Everything.%2520However%252C%2520it%2520will%2520result%2520in%2520a%2520high%2520demand%2520for%2520computing%250Aresources%2520to%2520support%2520new%2520services.%2520Mobile%2520Edge%2520Computing%2520%2528MEC%2529%2520is%2520a%2520promising%250Asolution%252C%2520enabling%2520to%2520offload%2520computation-intensive%2520tasks%2520to%2520nearby%2520edge%250Aservers%2520from%2520the%2520end-user%2520devices%252C%2520thereby%2520reducing%2520latency%2520and%2520energy%250Aconsumption.%2520However%252C%2520relying%2520solely%2520on%2520a%2520single%2520MEC%2520server%2520for%2520task%2520offloading%250Acan%2520lead%2520to%2520uneven%2520resource%2520utilisation%2520and%2520suboptimal%2520performance%2520in%2520complex%250Ascenarios.%2520Additionally%252C%2520traditional%2520task%2520offloading%2520strategies%2520specialise%2520in%250Acentralised%2520policy%2520decisions%252C%2520which%2520unavoidably%2520entail%2520extreme%2520transmission%250Alatency%2520and%2520reach%2520computational%2520bottleneck.%2520To%2520fill%2520the%2520gaps%252C%2520we%2520propose%2520a%250Alatency%2520and%2520energy%2520efficient%2520Cooperative%2520Task%2520Offloading%2520framework%2520with%250ATransformer-driven%2520Prediction%2520%2528CTO-TP%2529%252C%2520leveraging%2520asynchronous%2520multi-agent%250Adeep%2520reinforcement%2520learning%2520to%2520address%2520these%2520challenges.%2520This%2520approach%2520fosters%250Aedge-edge%2520cooperation%2520and%2520decreases%2520the%2520synchronous%2520waiting%2520time%2520by%2520performing%250Aasynchronous%2520training%252C%2520optimising%2520task%2520offloading%252C%2520and%2520resource%2520allocation%250Aacross%2520distributed%2520networks.%2520The%2520performance%2520evaluation%2520demonstrates%2520that%2520the%250Aproposed%2520CTO-TP%2520algorithm%2520reduces%2520up%2520to%252080%2525%2520overall%2520system%2520latency%2520and%252087%2525%250Aenergy%2520consumption%2520compared%2520to%2520the%2520baseline%2520schemes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17526v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cooperative%20Task%20Offloading%20through%20Asynchronous%20Deep%20Reinforcement%0A%20%20Learning%20in%20Mobile%20Edge%20Computing%20for%20Future%20Networks&entry.906535625=Yuelin%20Liu%20and%20Haiyuan%20Li%20and%20Xenofon%20Vasilakos%20and%20Rasheed%20Hussain%20and%20Dimitra%20Simeonidou&entry.1292438233=%20%20Future%20networks%20%28including%206G%29%20are%20poised%20to%20accelerate%20the%20realisation%20of%0AInternet%20of%20Everything.%20However%2C%20it%20will%20result%20in%20a%20high%20demand%20for%20computing%0Aresources%20to%20support%20new%20services.%20Mobile%20Edge%20Computing%20%28MEC%29%20is%20a%20promising%0Asolution%2C%20enabling%20to%20offload%20computation-intensive%20tasks%20to%20nearby%20edge%0Aservers%20from%20the%20end-user%20devices%2C%20thereby%20reducing%20latency%20and%20energy%0Aconsumption.%20However%2C%20relying%20solely%20on%20a%20single%20MEC%20server%20for%20task%20offloading%0Acan%20lead%20to%20uneven%20resource%20utilisation%20and%20suboptimal%20performance%20in%20complex%0Ascenarios.%20Additionally%2C%20traditional%20task%20offloading%20strategies%20specialise%20in%0Acentralised%20policy%20decisions%2C%20which%20unavoidably%20entail%20extreme%20transmission%0Alatency%20and%20reach%20computational%20bottleneck.%20To%20fill%20the%20gaps%2C%20we%20propose%20a%0Alatency%20and%20energy%20efficient%20Cooperative%20Task%20Offloading%20framework%20with%0ATransformer-driven%20Prediction%20%28CTO-TP%29%2C%20leveraging%20asynchronous%20multi-agent%0Adeep%20reinforcement%20learning%20to%20address%20these%20challenges.%20This%20approach%20fosters%0Aedge-edge%20cooperation%20and%20decreases%20the%20synchronous%20waiting%20time%20by%20performing%0Aasynchronous%20training%2C%20optimising%20task%20offloading%2C%20and%20resource%20allocation%0Aacross%20distributed%20networks.%20The%20performance%20evaluation%20demonstrates%20that%20the%0Aproposed%20CTO-TP%20algorithm%20reduces%20up%20to%2080%25%20overall%20system%20latency%20and%2087%25%0Aenergy%20consumption%20compared%20to%20the%20baseline%20schemes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17526v1&entry.124074799=Read"},
{"title": "Token-Shuffle: Towards High-Resolution Image Generation with\n  Autoregressive Models", "author": "Xu Ma and Peize Sun and Haoyu Ma and Hao Tang and Chih-Yao Ma and Jialiang Wang and Kunpeng Li and Xiaoliang Dai and Yujun Shi and Xuan Ju and Yushi Hu and Artsiom Sanakoyeu and Felix Juefei-Xu and Ji Hou and Junjiao Tian and Tao Xu and Tingbo Hou and Yen-Cheng Liu and Zecheng He and Zijian He and Matt Feiszli and Peizhao Zhang and Peter Vajda and Sam Tsai and Yun Fu", "abstract": "  Autoregressive (AR) models, long dominant in language generation, are\nincreasingly applied to image synthesis but are often considered less\ncompetitive than Diffusion-based models. A primary limitation is the\nsubstantial number of image tokens required for AR models, which constrains\nboth training and inference efficiency, as well as image resolution. To address\nthis, we present Token-Shuffle, a novel yet simple method that reduces the\nnumber of image tokens in Transformer. Our key insight is the dimensional\nredundancy of visual vocabularies in Multimodal Large Language Models (MLLMs),\nwhere low-dimensional visual codes from visual encoder are directly mapped to\nhigh-dimensional language vocabularies. Leveraging this, we consider two key\noperations: token-shuffle, which merges spatially local tokens along channel\ndimension to decrease the input token number, and token-unshuffle, which\nuntangles the inferred tokens after Transformer blocks to restore the spatial\narrangement for output. Jointly training with textual prompts, our strategy\nrequires no additional pretrained text-encoder and enables MLLMs to support\nextremely high-resolution image synthesis in a unified next-token prediction\nway while maintaining efficient training and inference. For the first time, we\npush the boundary of AR text-to-image generation to a resolution of 2048x2048\nwith gratifying generation performance. In GenAI-benchmark, our 2.7B model\nachieves 0.77 overall score on hard prompts, outperforming AR models LlamaGen\nby 0.18 and diffusion models LDM by 0.15. Exhaustive large-scale human\nevaluations also demonstrate our prominent image generation ability in terms of\ntext-alignment, visual flaw, and visual appearance. We hope that Token-Shuffle\ncan serve as a foundational design for efficient high-resolution image\ngeneration within MLLMs.\n", "link": "http://arxiv.org/abs/2504.17789v1", "date": "2025-04-24", "relevancy": 1.959, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6849}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.647}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5793}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Token-Shuffle%3A%20Towards%20High-Resolution%20Image%20Generation%20with%0A%20%20Autoregressive%20Models&body=Title%3A%20Token-Shuffle%3A%20Towards%20High-Resolution%20Image%20Generation%20with%0A%20%20Autoregressive%20Models%0AAuthor%3A%20Xu%20Ma%20and%20Peize%20Sun%20and%20Haoyu%20Ma%20and%20Hao%20Tang%20and%20Chih-Yao%20Ma%20and%20Jialiang%20Wang%20and%20Kunpeng%20Li%20and%20Xiaoliang%20Dai%20and%20Yujun%20Shi%20and%20Xuan%20Ju%20and%20Yushi%20Hu%20and%20Artsiom%20Sanakoyeu%20and%20Felix%20Juefei-Xu%20and%20Ji%20Hou%20and%20Junjiao%20Tian%20and%20Tao%20Xu%20and%20Tingbo%20Hou%20and%20Yen-Cheng%20Liu%20and%20Zecheng%20He%20and%20Zijian%20He%20and%20Matt%20Feiszli%20and%20Peizhao%20Zhang%20and%20Peter%20Vajda%20and%20Sam%20Tsai%20and%20Yun%20Fu%0AAbstract%3A%20%20%20Autoregressive%20%28AR%29%20models%2C%20long%20dominant%20in%20language%20generation%2C%20are%0Aincreasingly%20applied%20to%20image%20synthesis%20but%20are%20often%20considered%20less%0Acompetitive%20than%20Diffusion-based%20models.%20A%20primary%20limitation%20is%20the%0Asubstantial%20number%20of%20image%20tokens%20required%20for%20AR%20models%2C%20which%20constrains%0Aboth%20training%20and%20inference%20efficiency%2C%20as%20well%20as%20image%20resolution.%20To%20address%0Athis%2C%20we%20present%20Token-Shuffle%2C%20a%20novel%20yet%20simple%20method%20that%20reduces%20the%0Anumber%20of%20image%20tokens%20in%20Transformer.%20Our%20key%20insight%20is%20the%20dimensional%0Aredundancy%20of%20visual%20vocabularies%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%0Awhere%20low-dimensional%20visual%20codes%20from%20visual%20encoder%20are%20directly%20mapped%20to%0Ahigh-dimensional%20language%20vocabularies.%20Leveraging%20this%2C%20we%20consider%20two%20key%0Aoperations%3A%20token-shuffle%2C%20which%20merges%20spatially%20local%20tokens%20along%20channel%0Adimension%20to%20decrease%20the%20input%20token%20number%2C%20and%20token-unshuffle%2C%20which%0Auntangles%20the%20inferred%20tokens%20after%20Transformer%20blocks%20to%20restore%20the%20spatial%0Aarrangement%20for%20output.%20Jointly%20training%20with%20textual%20prompts%2C%20our%20strategy%0Arequires%20no%20additional%20pretrained%20text-encoder%20and%20enables%20MLLMs%20to%20support%0Aextremely%20high-resolution%20image%20synthesis%20in%20a%20unified%20next-token%20prediction%0Away%20while%20maintaining%20efficient%20training%20and%20inference.%20For%20the%20first%20time%2C%20we%0Apush%20the%20boundary%20of%20AR%20text-to-image%20generation%20to%20a%20resolution%20of%202048x2048%0Awith%20gratifying%20generation%20performance.%20In%20GenAI-benchmark%2C%20our%202.7B%20model%0Aachieves%200.77%20overall%20score%20on%20hard%20prompts%2C%20outperforming%20AR%20models%20LlamaGen%0Aby%200.18%20and%20diffusion%20models%20LDM%20by%200.15.%20Exhaustive%20large-scale%20human%0Aevaluations%20also%20demonstrate%20our%20prominent%20image%20generation%20ability%20in%20terms%20of%0Atext-alignment%2C%20visual%20flaw%2C%20and%20visual%20appearance.%20We%20hope%20that%20Token-Shuffle%0Acan%20serve%20as%20a%20foundational%20design%20for%20efficient%20high-resolution%20image%0Ageneration%20within%20MLLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17789v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToken-Shuffle%253A%2520Towards%2520High-Resolution%2520Image%2520Generation%2520with%250A%2520%2520Autoregressive%2520Models%26entry.906535625%3DXu%2520Ma%2520and%2520Peize%2520Sun%2520and%2520Haoyu%2520Ma%2520and%2520Hao%2520Tang%2520and%2520Chih-Yao%2520Ma%2520and%2520Jialiang%2520Wang%2520and%2520Kunpeng%2520Li%2520and%2520Xiaoliang%2520Dai%2520and%2520Yujun%2520Shi%2520and%2520Xuan%2520Ju%2520and%2520Yushi%2520Hu%2520and%2520Artsiom%2520Sanakoyeu%2520and%2520Felix%2520Juefei-Xu%2520and%2520Ji%2520Hou%2520and%2520Junjiao%2520Tian%2520and%2520Tao%2520Xu%2520and%2520Tingbo%2520Hou%2520and%2520Yen-Cheng%2520Liu%2520and%2520Zecheng%2520He%2520and%2520Zijian%2520He%2520and%2520Matt%2520Feiszli%2520and%2520Peizhao%2520Zhang%2520and%2520Peter%2520Vajda%2520and%2520Sam%2520Tsai%2520and%2520Yun%2520Fu%26entry.1292438233%3D%2520%2520Autoregressive%2520%2528AR%2529%2520models%252C%2520long%2520dominant%2520in%2520language%2520generation%252C%2520are%250Aincreasingly%2520applied%2520to%2520image%2520synthesis%2520but%2520are%2520often%2520considered%2520less%250Acompetitive%2520than%2520Diffusion-based%2520models.%2520A%2520primary%2520limitation%2520is%2520the%250Asubstantial%2520number%2520of%2520image%2520tokens%2520required%2520for%2520AR%2520models%252C%2520which%2520constrains%250Aboth%2520training%2520and%2520inference%2520efficiency%252C%2520as%2520well%2520as%2520image%2520resolution.%2520To%2520address%250Athis%252C%2520we%2520present%2520Token-Shuffle%252C%2520a%2520novel%2520yet%2520simple%2520method%2520that%2520reduces%2520the%250Anumber%2520of%2520image%2520tokens%2520in%2520Transformer.%2520Our%2520key%2520insight%2520is%2520the%2520dimensional%250Aredundancy%2520of%2520visual%2520vocabularies%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%250Awhere%2520low-dimensional%2520visual%2520codes%2520from%2520visual%2520encoder%2520are%2520directly%2520mapped%2520to%250Ahigh-dimensional%2520language%2520vocabularies.%2520Leveraging%2520this%252C%2520we%2520consider%2520two%2520key%250Aoperations%253A%2520token-shuffle%252C%2520which%2520merges%2520spatially%2520local%2520tokens%2520along%2520channel%250Adimension%2520to%2520decrease%2520the%2520input%2520token%2520number%252C%2520and%2520token-unshuffle%252C%2520which%250Auntangles%2520the%2520inferred%2520tokens%2520after%2520Transformer%2520blocks%2520to%2520restore%2520the%2520spatial%250Aarrangement%2520for%2520output.%2520Jointly%2520training%2520with%2520textual%2520prompts%252C%2520our%2520strategy%250Arequires%2520no%2520additional%2520pretrained%2520text-encoder%2520and%2520enables%2520MLLMs%2520to%2520support%250Aextremely%2520high-resolution%2520image%2520synthesis%2520in%2520a%2520unified%2520next-token%2520prediction%250Away%2520while%2520maintaining%2520efficient%2520training%2520and%2520inference.%2520For%2520the%2520first%2520time%252C%2520we%250Apush%2520the%2520boundary%2520of%2520AR%2520text-to-image%2520generation%2520to%2520a%2520resolution%2520of%25202048x2048%250Awith%2520gratifying%2520generation%2520performance.%2520In%2520GenAI-benchmark%252C%2520our%25202.7B%2520model%250Aachieves%25200.77%2520overall%2520score%2520on%2520hard%2520prompts%252C%2520outperforming%2520AR%2520models%2520LlamaGen%250Aby%25200.18%2520and%2520diffusion%2520models%2520LDM%2520by%25200.15.%2520Exhaustive%2520large-scale%2520human%250Aevaluations%2520also%2520demonstrate%2520our%2520prominent%2520image%2520generation%2520ability%2520in%2520terms%2520of%250Atext-alignment%252C%2520visual%2520flaw%252C%2520and%2520visual%2520appearance.%2520We%2520hope%2520that%2520Token-Shuffle%250Acan%2520serve%2520as%2520a%2520foundational%2520design%2520for%2520efficient%2520high-resolution%2520image%250Ageneration%2520within%2520MLLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17789v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Token-Shuffle%3A%20Towards%20High-Resolution%20Image%20Generation%20with%0A%20%20Autoregressive%20Models&entry.906535625=Xu%20Ma%20and%20Peize%20Sun%20and%20Haoyu%20Ma%20and%20Hao%20Tang%20and%20Chih-Yao%20Ma%20and%20Jialiang%20Wang%20and%20Kunpeng%20Li%20and%20Xiaoliang%20Dai%20and%20Yujun%20Shi%20and%20Xuan%20Ju%20and%20Yushi%20Hu%20and%20Artsiom%20Sanakoyeu%20and%20Felix%20Juefei-Xu%20and%20Ji%20Hou%20and%20Junjiao%20Tian%20and%20Tao%20Xu%20and%20Tingbo%20Hou%20and%20Yen-Cheng%20Liu%20and%20Zecheng%20He%20and%20Zijian%20He%20and%20Matt%20Feiszli%20and%20Peizhao%20Zhang%20and%20Peter%20Vajda%20and%20Sam%20Tsai%20and%20Yun%20Fu&entry.1292438233=%20%20Autoregressive%20%28AR%29%20models%2C%20long%20dominant%20in%20language%20generation%2C%20are%0Aincreasingly%20applied%20to%20image%20synthesis%20but%20are%20often%20considered%20less%0Acompetitive%20than%20Diffusion-based%20models.%20A%20primary%20limitation%20is%20the%0Asubstantial%20number%20of%20image%20tokens%20required%20for%20AR%20models%2C%20which%20constrains%0Aboth%20training%20and%20inference%20efficiency%2C%20as%20well%20as%20image%20resolution.%20To%20address%0Athis%2C%20we%20present%20Token-Shuffle%2C%20a%20novel%20yet%20simple%20method%20that%20reduces%20the%0Anumber%20of%20image%20tokens%20in%20Transformer.%20Our%20key%20insight%20is%20the%20dimensional%0Aredundancy%20of%20visual%20vocabularies%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%0Awhere%20low-dimensional%20visual%20codes%20from%20visual%20encoder%20are%20directly%20mapped%20to%0Ahigh-dimensional%20language%20vocabularies.%20Leveraging%20this%2C%20we%20consider%20two%20key%0Aoperations%3A%20token-shuffle%2C%20which%20merges%20spatially%20local%20tokens%20along%20channel%0Adimension%20to%20decrease%20the%20input%20token%20number%2C%20and%20token-unshuffle%2C%20which%0Auntangles%20the%20inferred%20tokens%20after%20Transformer%20blocks%20to%20restore%20the%20spatial%0Aarrangement%20for%20output.%20Jointly%20training%20with%20textual%20prompts%2C%20our%20strategy%0Arequires%20no%20additional%20pretrained%20text-encoder%20and%20enables%20MLLMs%20to%20support%0Aextremely%20high-resolution%20image%20synthesis%20in%20a%20unified%20next-token%20prediction%0Away%20while%20maintaining%20efficient%20training%20and%20inference.%20For%20the%20first%20time%2C%20we%0Apush%20the%20boundary%20of%20AR%20text-to-image%20generation%20to%20a%20resolution%20of%202048x2048%0Awith%20gratifying%20generation%20performance.%20In%20GenAI-benchmark%2C%20our%202.7B%20model%0Aachieves%200.77%20overall%20score%20on%20hard%20prompts%2C%20outperforming%20AR%20models%20LlamaGen%0Aby%200.18%20and%20diffusion%20models%20LDM%20by%200.15.%20Exhaustive%20large-scale%20human%0Aevaluations%20also%20demonstrate%20our%20prominent%20image%20generation%20ability%20in%20terms%20of%0Atext-alignment%2C%20visual%20flaw%2C%20and%20visual%20appearance.%20We%20hope%20that%20Token-Shuffle%0Acan%20serve%20as%20a%20foundational%20design%20for%20efficient%20high-resolution%20image%0Ageneration%20within%20MLLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17789v1&entry.124074799=Read"},
{"title": "LSEAttention is All You Need for Time Series Forecasting", "author": "Dizhen Liang", "abstract": "  Transformer-based architectures have achieved remarkable success in natural\nlanguage processing and computer vision. However, their performance in\nmultivariate long-term forecasting often falls short compared to simpler linear\nbaselines. Previous research has identified the traditional attention mechanism\nas a key factor limiting their effectiveness in this domain. To bridge this\ngap, we introduce LATST, a novel approach designed to mitigate entropy collapse\nand training instability common challenges in Transformer-based time series\nforecasting. We rigorously evaluate LATST across multiple real-world\nmultivariate time series datasets, demonstrating its ability to outperform\nexisting state-of-the-art Transformer models. Notably, LATST manages to achieve\ncompetitive performance with fewer parameters than some linear models on\ncertain datasets, highlighting its efficiency and effectiveness.\n", "link": "http://arxiv.org/abs/2410.23749v7", "date": "2025-04-24", "relevancy": 1.9464, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4992}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4917}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4765}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LSEAttention%20is%20All%20You%20Need%20for%20Time%20Series%20Forecasting&body=Title%3A%20LSEAttention%20is%20All%20You%20Need%20for%20Time%20Series%20Forecasting%0AAuthor%3A%20Dizhen%20Liang%0AAbstract%3A%20%20%20Transformer-based%20architectures%20have%20achieved%20remarkable%20success%20in%20natural%0Alanguage%20processing%20and%20computer%20vision.%20However%2C%20their%20performance%20in%0Amultivariate%20long-term%20forecasting%20often%20falls%20short%20compared%20to%20simpler%20linear%0Abaselines.%20Previous%20research%20has%20identified%20the%20traditional%20attention%20mechanism%0Aas%20a%20key%20factor%20limiting%20their%20effectiveness%20in%20this%20domain.%20To%20bridge%20this%0Agap%2C%20we%20introduce%20LATST%2C%20a%20novel%20approach%20designed%20to%20mitigate%20entropy%20collapse%0Aand%20training%20instability%20common%20challenges%20in%20Transformer-based%20time%20series%0Aforecasting.%20We%20rigorously%20evaluate%20LATST%20across%20multiple%20real-world%0Amultivariate%20time%20series%20datasets%2C%20demonstrating%20its%20ability%20to%20outperform%0Aexisting%20state-of-the-art%20Transformer%20models.%20Notably%2C%20LATST%20manages%20to%20achieve%0Acompetitive%20performance%20with%20fewer%20parameters%20than%20some%20linear%20models%20on%0Acertain%20datasets%2C%20highlighting%20its%20efficiency%20and%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23749v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLSEAttention%2520is%2520All%2520You%2520Need%2520for%2520Time%2520Series%2520Forecasting%26entry.906535625%3DDizhen%2520Liang%26entry.1292438233%3D%2520%2520Transformer-based%2520architectures%2520have%2520achieved%2520remarkable%2520success%2520in%2520natural%250Alanguage%2520processing%2520and%2520computer%2520vision.%2520However%252C%2520their%2520performance%2520in%250Amultivariate%2520long-term%2520forecasting%2520often%2520falls%2520short%2520compared%2520to%2520simpler%2520linear%250Abaselines.%2520Previous%2520research%2520has%2520identified%2520the%2520traditional%2520attention%2520mechanism%250Aas%2520a%2520key%2520factor%2520limiting%2520their%2520effectiveness%2520in%2520this%2520domain.%2520To%2520bridge%2520this%250Agap%252C%2520we%2520introduce%2520LATST%252C%2520a%2520novel%2520approach%2520designed%2520to%2520mitigate%2520entropy%2520collapse%250Aand%2520training%2520instability%2520common%2520challenges%2520in%2520Transformer-based%2520time%2520series%250Aforecasting.%2520We%2520rigorously%2520evaluate%2520LATST%2520across%2520multiple%2520real-world%250Amultivariate%2520time%2520series%2520datasets%252C%2520demonstrating%2520its%2520ability%2520to%2520outperform%250Aexisting%2520state-of-the-art%2520Transformer%2520models.%2520Notably%252C%2520LATST%2520manages%2520to%2520achieve%250Acompetitive%2520performance%2520with%2520fewer%2520parameters%2520than%2520some%2520linear%2520models%2520on%250Acertain%2520datasets%252C%2520highlighting%2520its%2520efficiency%2520and%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23749v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LSEAttention%20is%20All%20You%20Need%20for%20Time%20Series%20Forecasting&entry.906535625=Dizhen%20Liang&entry.1292438233=%20%20Transformer-based%20architectures%20have%20achieved%20remarkable%20success%20in%20natural%0Alanguage%20processing%20and%20computer%20vision.%20However%2C%20their%20performance%20in%0Amultivariate%20long-term%20forecasting%20often%20falls%20short%20compared%20to%20simpler%20linear%0Abaselines.%20Previous%20research%20has%20identified%20the%20traditional%20attention%20mechanism%0Aas%20a%20key%20factor%20limiting%20their%20effectiveness%20in%20this%20domain.%20To%20bridge%20this%0Agap%2C%20we%20introduce%20LATST%2C%20a%20novel%20approach%20designed%20to%20mitigate%20entropy%20collapse%0Aand%20training%20instability%20common%20challenges%20in%20Transformer-based%20time%20series%0Aforecasting.%20We%20rigorously%20evaluate%20LATST%20across%20multiple%20real-world%0Amultivariate%20time%20series%20datasets%2C%20demonstrating%20its%20ability%20to%20outperform%0Aexisting%20state-of-the-art%20Transformer%20models.%20Notably%2C%20LATST%20manages%20to%20achieve%0Acompetitive%20performance%20with%20fewer%20parameters%20than%20some%20linear%20models%20on%0Acertain%20datasets%2C%20highlighting%20its%20efficiency%20and%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23749v7&entry.124074799=Read"},
{"title": "Prototype-enhanced prediction in graph neural networks for climate\n  applications", "author": "Nawid Keshtmand and Elena Fillola and Jeffrey Nicholas Clark and Raul Santos-Rodriguez and Matthew Rigby", "abstract": "  Data-driven emulators are increasingly being used to learn and emulate\nphysics-based simulations, reducing computational expense and run time. Here,\nwe present a structured way to improve the quality of these high-dimensional\nemulated outputs, through the use of prototypes: an approximation of the\nemulator's output passed as an input, which informs the model and leads to\nbetter predictions. We demonstrate our approach to emulate atmospheric\ndispersion, key for greenhouse gas emissions monitoring, by comparing a\nbaseline model to models trained using prototypes as an additional input. The\nprototype models achieve better performance, even with few prototypes and even\nif they are chosen at random, but we show that choosing the prototypes through\ndata-driven methods (k-means) can lead to almost 10\\% increased performance in\nsome metrics.\n", "link": "http://arxiv.org/abs/2504.17492v1", "date": "2025-04-24", "relevancy": 1.0085, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5208}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.505}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prototype-enhanced%20prediction%20in%20graph%20neural%20networks%20for%20climate%0A%20%20applications&body=Title%3A%20Prototype-enhanced%20prediction%20in%20graph%20neural%20networks%20for%20climate%0A%20%20applications%0AAuthor%3A%20Nawid%20Keshtmand%20and%20Elena%20Fillola%20and%20Jeffrey%20Nicholas%20Clark%20and%20Raul%20Santos-Rodriguez%20and%20Matthew%20Rigby%0AAbstract%3A%20%20%20Data-driven%20emulators%20are%20increasingly%20being%20used%20to%20learn%20and%20emulate%0Aphysics-based%20simulations%2C%20reducing%20computational%20expense%20and%20run%20time.%20Here%2C%0Awe%20present%20a%20structured%20way%20to%20improve%20the%20quality%20of%20these%20high-dimensional%0Aemulated%20outputs%2C%20through%20the%20use%20of%20prototypes%3A%20an%20approximation%20of%20the%0Aemulator%27s%20output%20passed%20as%20an%20input%2C%20which%20informs%20the%20model%20and%20leads%20to%0Abetter%20predictions.%20We%20demonstrate%20our%20approach%20to%20emulate%20atmospheric%0Adispersion%2C%20key%20for%20greenhouse%20gas%20emissions%20monitoring%2C%20by%20comparing%20a%0Abaseline%20model%20to%20models%20trained%20using%20prototypes%20as%20an%20additional%20input.%20The%0Aprototype%20models%20achieve%20better%20performance%2C%20even%20with%20few%20prototypes%20and%20even%0Aif%20they%20are%20chosen%20at%20random%2C%20but%20we%20show%20that%20choosing%20the%20prototypes%20through%0Adata-driven%20methods%20%28k-means%29%20can%20lead%20to%20almost%2010%5C%25%20increased%20performance%20in%0Asome%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17492v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrototype-enhanced%2520prediction%2520in%2520graph%2520neural%2520networks%2520for%2520climate%250A%2520%2520applications%26entry.906535625%3DNawid%2520Keshtmand%2520and%2520Elena%2520Fillola%2520and%2520Jeffrey%2520Nicholas%2520Clark%2520and%2520Raul%2520Santos-Rodriguez%2520and%2520Matthew%2520Rigby%26entry.1292438233%3D%2520%2520Data-driven%2520emulators%2520are%2520increasingly%2520being%2520used%2520to%2520learn%2520and%2520emulate%250Aphysics-based%2520simulations%252C%2520reducing%2520computational%2520expense%2520and%2520run%2520time.%2520Here%252C%250Awe%2520present%2520a%2520structured%2520way%2520to%2520improve%2520the%2520quality%2520of%2520these%2520high-dimensional%250Aemulated%2520outputs%252C%2520through%2520the%2520use%2520of%2520prototypes%253A%2520an%2520approximation%2520of%2520the%250Aemulator%2527s%2520output%2520passed%2520as%2520an%2520input%252C%2520which%2520informs%2520the%2520model%2520and%2520leads%2520to%250Abetter%2520predictions.%2520We%2520demonstrate%2520our%2520approach%2520to%2520emulate%2520atmospheric%250Adispersion%252C%2520key%2520for%2520greenhouse%2520gas%2520emissions%2520monitoring%252C%2520by%2520comparing%2520a%250Abaseline%2520model%2520to%2520models%2520trained%2520using%2520prototypes%2520as%2520an%2520additional%2520input.%2520The%250Aprototype%2520models%2520achieve%2520better%2520performance%252C%2520even%2520with%2520few%2520prototypes%2520and%2520even%250Aif%2520they%2520are%2520chosen%2520at%2520random%252C%2520but%2520we%2520show%2520that%2520choosing%2520the%2520prototypes%2520through%250Adata-driven%2520methods%2520%2528k-means%2529%2520can%2520lead%2520to%2520almost%252010%255C%2525%2520increased%2520performance%2520in%250Asome%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17492v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prototype-enhanced%20prediction%20in%20graph%20neural%20networks%20for%20climate%0A%20%20applications&entry.906535625=Nawid%20Keshtmand%20and%20Elena%20Fillola%20and%20Jeffrey%20Nicholas%20Clark%20and%20Raul%20Santos-Rodriguez%20and%20Matthew%20Rigby&entry.1292438233=%20%20Data-driven%20emulators%20are%20increasingly%20being%20used%20to%20learn%20and%20emulate%0Aphysics-based%20simulations%2C%20reducing%20computational%20expense%20and%20run%20time.%20Here%2C%0Awe%20present%20a%20structured%20way%20to%20improve%20the%20quality%20of%20these%20high-dimensional%0Aemulated%20outputs%2C%20through%20the%20use%20of%20prototypes%3A%20an%20approximation%20of%20the%0Aemulator%27s%20output%20passed%20as%20an%20input%2C%20which%20informs%20the%20model%20and%20leads%20to%0Abetter%20predictions.%20We%20demonstrate%20our%20approach%20to%20emulate%20atmospheric%0Adispersion%2C%20key%20for%20greenhouse%20gas%20emissions%20monitoring%2C%20by%20comparing%20a%0Abaseline%20model%20to%20models%20trained%20using%20prototypes%20as%20an%20additional%20input.%20The%0Aprototype%20models%20achieve%20better%20performance%2C%20even%20with%20few%20prototypes%20and%20even%0Aif%20they%20are%20chosen%20at%20random%2C%20but%20we%20show%20that%20choosing%20the%20prototypes%20through%0Adata-driven%20methods%20%28k-means%29%20can%20lead%20to%20almost%2010%5C%25%20increased%20performance%20in%0Asome%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17492v1&entry.124074799=Read"},
{"title": "Object Pose Estimation by Camera Arm Control Based on the Next Viewpoint\n  Estimation", "author": "Tomoki Mizuno and Kazuya Yabashi and Tsuyoshi Tasaki", "abstract": "  We have developed a new method to estimate a Next Viewpoint (NV) which is\neffective for pose estimation of simple-shaped products for product display\nrobots in retail stores. Pose estimation methods using Neural Networks (NN)\nbased on an RGBD camera are highly accurate, but their accuracy significantly\ndecreases when the camera acquires few texture and shape features at a current\nview point. However, it is difficult for previous mathematical model-based\nmethods to estimate effective NV which is because the simple shaped objects\nhave few shape features. Therefore, we focus on the relationship between the\npose estimation and NV estimation. When the pose estimation is more accurate,\nthe NV estimation is more accurate. Therefore, we develop a new pose estimation\nNN that estimates NV simultaneously. Experimental results showed that our NV\nestimation realized a pose estimation success rate 77.3\\%, which was 7.4pt\nhigher than the mathematical model-based NV calculation did. Moreover, we\nverified that the robot using our method displayed 84.2\\% of products.\n", "link": "http://arxiv.org/abs/2504.17424v1", "date": "2025-04-24", "relevancy": 1.5502, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5246}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5149}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4989}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Object%20Pose%20Estimation%20by%20Camera%20Arm%20Control%20Based%20on%20the%20Next%20Viewpoint%0A%20%20Estimation&body=Title%3A%20Object%20Pose%20Estimation%20by%20Camera%20Arm%20Control%20Based%20on%20the%20Next%20Viewpoint%0A%20%20Estimation%0AAuthor%3A%20Tomoki%20Mizuno%20and%20Kazuya%20Yabashi%20and%20Tsuyoshi%20Tasaki%0AAbstract%3A%20%20%20We%20have%20developed%20a%20new%20method%20to%20estimate%20a%20Next%20Viewpoint%20%28NV%29%20which%20is%0Aeffective%20for%20pose%20estimation%20of%20simple-shaped%20products%20for%20product%20display%0Arobots%20in%20retail%20stores.%20Pose%20estimation%20methods%20using%20Neural%20Networks%20%28NN%29%0Abased%20on%20an%20RGBD%20camera%20are%20highly%20accurate%2C%20but%20their%20accuracy%20significantly%0Adecreases%20when%20the%20camera%20acquires%20few%20texture%20and%20shape%20features%20at%20a%20current%0Aview%20point.%20However%2C%20it%20is%20difficult%20for%20previous%20mathematical%20model-based%0Amethods%20to%20estimate%20effective%20NV%20which%20is%20because%20the%20simple%20shaped%20objects%0Ahave%20few%20shape%20features.%20Therefore%2C%20we%20focus%20on%20the%20relationship%20between%20the%0Apose%20estimation%20and%20NV%20estimation.%20When%20the%20pose%20estimation%20is%20more%20accurate%2C%0Athe%20NV%20estimation%20is%20more%20accurate.%20Therefore%2C%20we%20develop%20a%20new%20pose%20estimation%0ANN%20that%20estimates%20NV%20simultaneously.%20Experimental%20results%20showed%20that%20our%20NV%0Aestimation%20realized%20a%20pose%20estimation%20success%20rate%2077.3%5C%25%2C%20which%20was%207.4pt%0Ahigher%20than%20the%20mathematical%20model-based%20NV%20calculation%20did.%20Moreover%2C%20we%0Averified%20that%20the%20robot%20using%20our%20method%20displayed%2084.2%5C%25%20of%20products.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17424v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObject%2520Pose%2520Estimation%2520by%2520Camera%2520Arm%2520Control%2520Based%2520on%2520the%2520Next%2520Viewpoint%250A%2520%2520Estimation%26entry.906535625%3DTomoki%2520Mizuno%2520and%2520Kazuya%2520Yabashi%2520and%2520Tsuyoshi%2520Tasaki%26entry.1292438233%3D%2520%2520We%2520have%2520developed%2520a%2520new%2520method%2520to%2520estimate%2520a%2520Next%2520Viewpoint%2520%2528NV%2529%2520which%2520is%250Aeffective%2520for%2520pose%2520estimation%2520of%2520simple-shaped%2520products%2520for%2520product%2520display%250Arobots%2520in%2520retail%2520stores.%2520Pose%2520estimation%2520methods%2520using%2520Neural%2520Networks%2520%2528NN%2529%250Abased%2520on%2520an%2520RGBD%2520camera%2520are%2520highly%2520accurate%252C%2520but%2520their%2520accuracy%2520significantly%250Adecreases%2520when%2520the%2520camera%2520acquires%2520few%2520texture%2520and%2520shape%2520features%2520at%2520a%2520current%250Aview%2520point.%2520However%252C%2520it%2520is%2520difficult%2520for%2520previous%2520mathematical%2520model-based%250Amethods%2520to%2520estimate%2520effective%2520NV%2520which%2520is%2520because%2520the%2520simple%2520shaped%2520objects%250Ahave%2520few%2520shape%2520features.%2520Therefore%252C%2520we%2520focus%2520on%2520the%2520relationship%2520between%2520the%250Apose%2520estimation%2520and%2520NV%2520estimation.%2520When%2520the%2520pose%2520estimation%2520is%2520more%2520accurate%252C%250Athe%2520NV%2520estimation%2520is%2520more%2520accurate.%2520Therefore%252C%2520we%2520develop%2520a%2520new%2520pose%2520estimation%250ANN%2520that%2520estimates%2520NV%2520simultaneously.%2520Experimental%2520results%2520showed%2520that%2520our%2520NV%250Aestimation%2520realized%2520a%2520pose%2520estimation%2520success%2520rate%252077.3%255C%2525%252C%2520which%2520was%25207.4pt%250Ahigher%2520than%2520the%2520mathematical%2520model-based%2520NV%2520calculation%2520did.%2520Moreover%252C%2520we%250Averified%2520that%2520the%2520robot%2520using%2520our%2520method%2520displayed%252084.2%255C%2525%2520of%2520products.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17424v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Object%20Pose%20Estimation%20by%20Camera%20Arm%20Control%20Based%20on%20the%20Next%20Viewpoint%0A%20%20Estimation&entry.906535625=Tomoki%20Mizuno%20and%20Kazuya%20Yabashi%20and%20Tsuyoshi%20Tasaki&entry.1292438233=%20%20We%20have%20developed%20a%20new%20method%20to%20estimate%20a%20Next%20Viewpoint%20%28NV%29%20which%20is%0Aeffective%20for%20pose%20estimation%20of%20simple-shaped%20products%20for%20product%20display%0Arobots%20in%20retail%20stores.%20Pose%20estimation%20methods%20using%20Neural%20Networks%20%28NN%29%0Abased%20on%20an%20RGBD%20camera%20are%20highly%20accurate%2C%20but%20their%20accuracy%20significantly%0Adecreases%20when%20the%20camera%20acquires%20few%20texture%20and%20shape%20features%20at%20a%20current%0Aview%20point.%20However%2C%20it%20is%20difficult%20for%20previous%20mathematical%20model-based%0Amethods%20to%20estimate%20effective%20NV%20which%20is%20because%20the%20simple%20shaped%20objects%0Ahave%20few%20shape%20features.%20Therefore%2C%20we%20focus%20on%20the%20relationship%20between%20the%0Apose%20estimation%20and%20NV%20estimation.%20When%20the%20pose%20estimation%20is%20more%20accurate%2C%0Athe%20NV%20estimation%20is%20more%20accurate.%20Therefore%2C%20we%20develop%20a%20new%20pose%20estimation%0ANN%20that%20estimates%20NV%20simultaneously.%20Experimental%20results%20showed%20that%20our%20NV%0Aestimation%20realized%20a%20pose%20estimation%20success%20rate%2077.3%5C%25%2C%20which%20was%207.4pt%0Ahigher%20than%20the%20mathematical%20model-based%20NV%20calculation%20did.%20Moreover%2C%20we%0Averified%20that%20the%20robot%20using%20our%20method%20displayed%2084.2%5C%25%20of%20products.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17424v1&entry.124074799=Read"},
{"title": "Generative Fields: Uncovering Hierarchical Feature Control for StyleGAN\n  via Inverted Receptive Fields", "author": "Zhuo He and Paul Henderson and Nicolas Pugeault", "abstract": "  StyleGAN has demonstrated the ability of GANs to synthesize highly-realistic\nfaces of imaginary people from random noise. One limitation of GAN-based image\ngeneration is the difficulty of controlling the features of the generated\nimage, due to the strong entanglement of the low-dimensional latent space.\nPrevious work that aimed to control StyleGAN with image or text prompts\nmodulated sampling in W latent space, which is more expressive than Z latent\nspace. However, W space still has restricted expressivity since it does not\ncontrol the feature synthesis directly; also the feature embedding in W space\nrequires a pre-training process to reconstruct the style signal, limiting its\napplication. This paper introduces the concept of \"generative fields\" to\nexplain the hierarchical feature synthesis in StyleGAN, inspired by the\nreceptive fields of convolution neural networks (CNNs). Additionally, we\npropose a new image editing pipeline for StyleGAN using generative field theory\nand the channel-wise style latent space S, utilizing the intrinsic structural\nfeature of CNNs to achieve disentangled control of feature synthesis at\nsynthesis time.\n", "link": "http://arxiv.org/abs/2504.17712v1", "date": "2025-04-24", "relevancy": 1.6082, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5603}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5341}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Fields%3A%20Uncovering%20Hierarchical%20Feature%20Control%20for%20StyleGAN%0A%20%20via%20Inverted%20Receptive%20Fields&body=Title%3A%20Generative%20Fields%3A%20Uncovering%20Hierarchical%20Feature%20Control%20for%20StyleGAN%0A%20%20via%20Inverted%20Receptive%20Fields%0AAuthor%3A%20Zhuo%20He%20and%20Paul%20Henderson%20and%20Nicolas%20Pugeault%0AAbstract%3A%20%20%20StyleGAN%20has%20demonstrated%20the%20ability%20of%20GANs%20to%20synthesize%20highly-realistic%0Afaces%20of%20imaginary%20people%20from%20random%20noise.%20One%20limitation%20of%20GAN-based%20image%0Ageneration%20is%20the%20difficulty%20of%20controlling%20the%20features%20of%20the%20generated%0Aimage%2C%20due%20to%20the%20strong%20entanglement%20of%20the%20low-dimensional%20latent%20space.%0APrevious%20work%20that%20aimed%20to%20control%20StyleGAN%20with%20image%20or%20text%20prompts%0Amodulated%20sampling%20in%20W%20latent%20space%2C%20which%20is%20more%20expressive%20than%20Z%20latent%0Aspace.%20However%2C%20W%20space%20still%20has%20restricted%20expressivity%20since%20it%20does%20not%0Acontrol%20the%20feature%20synthesis%20directly%3B%20also%20the%20feature%20embedding%20in%20W%20space%0Arequires%20a%20pre-training%20process%20to%20reconstruct%20the%20style%20signal%2C%20limiting%20its%0Aapplication.%20This%20paper%20introduces%20the%20concept%20of%20%22generative%20fields%22%20to%0Aexplain%20the%20hierarchical%20feature%20synthesis%20in%20StyleGAN%2C%20inspired%20by%20the%0Areceptive%20fields%20of%20convolution%20neural%20networks%20%28CNNs%29.%20Additionally%2C%20we%0Apropose%20a%20new%20image%20editing%20pipeline%20for%20StyleGAN%20using%20generative%20field%20theory%0Aand%20the%20channel-wise%20style%20latent%20space%20S%2C%20utilizing%20the%20intrinsic%20structural%0Afeature%20of%20CNNs%20to%20achieve%20disentangled%20control%20of%20feature%20synthesis%20at%0Asynthesis%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17712v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Fields%253A%2520Uncovering%2520Hierarchical%2520Feature%2520Control%2520for%2520StyleGAN%250A%2520%2520via%2520Inverted%2520Receptive%2520Fields%26entry.906535625%3DZhuo%2520He%2520and%2520Paul%2520Henderson%2520and%2520Nicolas%2520Pugeault%26entry.1292438233%3D%2520%2520StyleGAN%2520has%2520demonstrated%2520the%2520ability%2520of%2520GANs%2520to%2520synthesize%2520highly-realistic%250Afaces%2520of%2520imaginary%2520people%2520from%2520random%2520noise.%2520One%2520limitation%2520of%2520GAN-based%2520image%250Ageneration%2520is%2520the%2520difficulty%2520of%2520controlling%2520the%2520features%2520of%2520the%2520generated%250Aimage%252C%2520due%2520to%2520the%2520strong%2520entanglement%2520of%2520the%2520low-dimensional%2520latent%2520space.%250APrevious%2520work%2520that%2520aimed%2520to%2520control%2520StyleGAN%2520with%2520image%2520or%2520text%2520prompts%250Amodulated%2520sampling%2520in%2520W%2520latent%2520space%252C%2520which%2520is%2520more%2520expressive%2520than%2520Z%2520latent%250Aspace.%2520However%252C%2520W%2520space%2520still%2520has%2520restricted%2520expressivity%2520since%2520it%2520does%2520not%250Acontrol%2520the%2520feature%2520synthesis%2520directly%253B%2520also%2520the%2520feature%2520embedding%2520in%2520W%2520space%250Arequires%2520a%2520pre-training%2520process%2520to%2520reconstruct%2520the%2520style%2520signal%252C%2520limiting%2520its%250Aapplication.%2520This%2520paper%2520introduces%2520the%2520concept%2520of%2520%2522generative%2520fields%2522%2520to%250Aexplain%2520the%2520hierarchical%2520feature%2520synthesis%2520in%2520StyleGAN%252C%2520inspired%2520by%2520the%250Areceptive%2520fields%2520of%2520convolution%2520neural%2520networks%2520%2528CNNs%2529.%2520Additionally%252C%2520we%250Apropose%2520a%2520new%2520image%2520editing%2520pipeline%2520for%2520StyleGAN%2520using%2520generative%2520field%2520theory%250Aand%2520the%2520channel-wise%2520style%2520latent%2520space%2520S%252C%2520utilizing%2520the%2520intrinsic%2520structural%250Afeature%2520of%2520CNNs%2520to%2520achieve%2520disentangled%2520control%2520of%2520feature%2520synthesis%2520at%250Asynthesis%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17712v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Fields%3A%20Uncovering%20Hierarchical%20Feature%20Control%20for%20StyleGAN%0A%20%20via%20Inverted%20Receptive%20Fields&entry.906535625=Zhuo%20He%20and%20Paul%20Henderson%20and%20Nicolas%20Pugeault&entry.1292438233=%20%20StyleGAN%20has%20demonstrated%20the%20ability%20of%20GANs%20to%20synthesize%20highly-realistic%0Afaces%20of%20imaginary%20people%20from%20random%20noise.%20One%20limitation%20of%20GAN-based%20image%0Ageneration%20is%20the%20difficulty%20of%20controlling%20the%20features%20of%20the%20generated%0Aimage%2C%20due%20to%20the%20strong%20entanglement%20of%20the%20low-dimensional%20latent%20space.%0APrevious%20work%20that%20aimed%20to%20control%20StyleGAN%20with%20image%20or%20text%20prompts%0Amodulated%20sampling%20in%20W%20latent%20space%2C%20which%20is%20more%20expressive%20than%20Z%20latent%0Aspace.%20However%2C%20W%20space%20still%20has%20restricted%20expressivity%20since%20it%20does%20not%0Acontrol%20the%20feature%20synthesis%20directly%3B%20also%20the%20feature%20embedding%20in%20W%20space%0Arequires%20a%20pre-training%20process%20to%20reconstruct%20the%20style%20signal%2C%20limiting%20its%0Aapplication.%20This%20paper%20introduces%20the%20concept%20of%20%22generative%20fields%22%20to%0Aexplain%20the%20hierarchical%20feature%20synthesis%20in%20StyleGAN%2C%20inspired%20by%20the%0Areceptive%20fields%20of%20convolution%20neural%20networks%20%28CNNs%29.%20Additionally%2C%20we%0Apropose%20a%20new%20image%20editing%20pipeline%20for%20StyleGAN%20using%20generative%20field%20theory%0Aand%20the%20channel-wise%20style%20latent%20space%20S%2C%20utilizing%20the%20intrinsic%20structural%0Afeature%20of%20CNNs%20to%20achieve%20disentangled%20control%20of%20feature%20synthesis%20at%0Asynthesis%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17712v1&entry.124074799=Read"},
{"title": "HydroStartML: A combined machine learning and physics-based approach to\n  reduce hydrological model spin-up time", "author": "Louisa Pawusch and Stefania Scheurer and Wolfgang Nowak and Reed Maxwell", "abstract": "  Finding the initial depth-to-water table (DTWT) configuration of a catchment\nis a critical challenge when simulating the hydrological cycle with integrated\nmodels, significantly impacting simulation outcomes. Traditionally, this\ninvolves iterative spin-up computations, where the model runs under constant\natmospheric settings until steady-state is achieved. These so-called model\nspin-ups are computationally expensive, often requiring many years of simulated\ntime, particularly when the initial DTWT configuration is far from steady\nstate.\n  To accelerate the model spin-up process we developed HydroStartML, a machine\nlearning emulator trained on steady-state DTWT configurations across the\ncontiguous United States. HydroStartML predicts, based on available data like\nconductivity and surface slopes, a DTWT configuration of the respective\nwatershed, which can be used as an initial DTWT.\n  Our results show that initializing spin-up computations with HydroStartML\npredictions leads to faster convergence than with other initial configurations\nlike spatially constant DTWTs. The emulator accurately predicts configurations\nclose to steady state, even for terrain configurations not seen in training,\nand allows especially significant reductions in computational spin-up effort in\nregions with deep DTWTs. This work opens the door for hybrid approaches that\nblend machine learning and traditional simulation, enhancing predictive\naccuracy and efficiency in hydrology for improving water resource management\nand understanding complex environmental interactions.\n", "link": "http://arxiv.org/abs/2504.17420v1", "date": "2025-04-24", "relevancy": 0.9173, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4844}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.449}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HydroStartML%3A%20A%20combined%20machine%20learning%20and%20physics-based%20approach%20to%0A%20%20reduce%20hydrological%20model%20spin-up%20time&body=Title%3A%20HydroStartML%3A%20A%20combined%20machine%20learning%20and%20physics-based%20approach%20to%0A%20%20reduce%20hydrological%20model%20spin-up%20time%0AAuthor%3A%20Louisa%20Pawusch%20and%20Stefania%20Scheurer%20and%20Wolfgang%20Nowak%20and%20Reed%20Maxwell%0AAbstract%3A%20%20%20Finding%20the%20initial%20depth-to-water%20table%20%28DTWT%29%20configuration%20of%20a%20catchment%0Ais%20a%20critical%20challenge%20when%20simulating%20the%20hydrological%20cycle%20with%20integrated%0Amodels%2C%20significantly%20impacting%20simulation%20outcomes.%20Traditionally%2C%20this%0Ainvolves%20iterative%20spin-up%20computations%2C%20where%20the%20model%20runs%20under%20constant%0Aatmospheric%20settings%20until%20steady-state%20is%20achieved.%20These%20so-called%20model%0Aspin-ups%20are%20computationally%20expensive%2C%20often%20requiring%20many%20years%20of%20simulated%0Atime%2C%20particularly%20when%20the%20initial%20DTWT%20configuration%20is%20far%20from%20steady%0Astate.%0A%20%20To%20accelerate%20the%20model%20spin-up%20process%20we%20developed%20HydroStartML%2C%20a%20machine%0Alearning%20emulator%20trained%20on%20steady-state%20DTWT%20configurations%20across%20the%0Acontiguous%20United%20States.%20HydroStartML%20predicts%2C%20based%20on%20available%20data%20like%0Aconductivity%20and%20surface%20slopes%2C%20a%20DTWT%20configuration%20of%20the%20respective%0Awatershed%2C%20which%20can%20be%20used%20as%20an%20initial%20DTWT.%0A%20%20Our%20results%20show%20that%20initializing%20spin-up%20computations%20with%20HydroStartML%0Apredictions%20leads%20to%20faster%20convergence%20than%20with%20other%20initial%20configurations%0Alike%20spatially%20constant%20DTWTs.%20The%20emulator%20accurately%20predicts%20configurations%0Aclose%20to%20steady%20state%2C%20even%20for%20terrain%20configurations%20not%20seen%20in%20training%2C%0Aand%20allows%20especially%20significant%20reductions%20in%20computational%20spin-up%20effort%20in%0Aregions%20with%20deep%20DTWTs.%20This%20work%20opens%20the%20door%20for%20hybrid%20approaches%20that%0Ablend%20machine%20learning%20and%20traditional%20simulation%2C%20enhancing%20predictive%0Aaccuracy%20and%20efficiency%20in%20hydrology%20for%20improving%20water%20resource%20management%0Aand%20understanding%20complex%20environmental%20interactions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17420v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHydroStartML%253A%2520A%2520combined%2520machine%2520learning%2520and%2520physics-based%2520approach%2520to%250A%2520%2520reduce%2520hydrological%2520model%2520spin-up%2520time%26entry.906535625%3DLouisa%2520Pawusch%2520and%2520Stefania%2520Scheurer%2520and%2520Wolfgang%2520Nowak%2520and%2520Reed%2520Maxwell%26entry.1292438233%3D%2520%2520Finding%2520the%2520initial%2520depth-to-water%2520table%2520%2528DTWT%2529%2520configuration%2520of%2520a%2520catchment%250Ais%2520a%2520critical%2520challenge%2520when%2520simulating%2520the%2520hydrological%2520cycle%2520with%2520integrated%250Amodels%252C%2520significantly%2520impacting%2520simulation%2520outcomes.%2520Traditionally%252C%2520this%250Ainvolves%2520iterative%2520spin-up%2520computations%252C%2520where%2520the%2520model%2520runs%2520under%2520constant%250Aatmospheric%2520settings%2520until%2520steady-state%2520is%2520achieved.%2520These%2520so-called%2520model%250Aspin-ups%2520are%2520computationally%2520expensive%252C%2520often%2520requiring%2520many%2520years%2520of%2520simulated%250Atime%252C%2520particularly%2520when%2520the%2520initial%2520DTWT%2520configuration%2520is%2520far%2520from%2520steady%250Astate.%250A%2520%2520To%2520accelerate%2520the%2520model%2520spin-up%2520process%2520we%2520developed%2520HydroStartML%252C%2520a%2520machine%250Alearning%2520emulator%2520trained%2520on%2520steady-state%2520DTWT%2520configurations%2520across%2520the%250Acontiguous%2520United%2520States.%2520HydroStartML%2520predicts%252C%2520based%2520on%2520available%2520data%2520like%250Aconductivity%2520and%2520surface%2520slopes%252C%2520a%2520DTWT%2520configuration%2520of%2520the%2520respective%250Awatershed%252C%2520which%2520can%2520be%2520used%2520as%2520an%2520initial%2520DTWT.%250A%2520%2520Our%2520results%2520show%2520that%2520initializing%2520spin-up%2520computations%2520with%2520HydroStartML%250Apredictions%2520leads%2520to%2520faster%2520convergence%2520than%2520with%2520other%2520initial%2520configurations%250Alike%2520spatially%2520constant%2520DTWTs.%2520The%2520emulator%2520accurately%2520predicts%2520configurations%250Aclose%2520to%2520steady%2520state%252C%2520even%2520for%2520terrain%2520configurations%2520not%2520seen%2520in%2520training%252C%250Aand%2520allows%2520especially%2520significant%2520reductions%2520in%2520computational%2520spin-up%2520effort%2520in%250Aregions%2520with%2520deep%2520DTWTs.%2520This%2520work%2520opens%2520the%2520door%2520for%2520hybrid%2520approaches%2520that%250Ablend%2520machine%2520learning%2520and%2520traditional%2520simulation%252C%2520enhancing%2520predictive%250Aaccuracy%2520and%2520efficiency%2520in%2520hydrology%2520for%2520improving%2520water%2520resource%2520management%250Aand%2520understanding%2520complex%2520environmental%2520interactions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17420v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HydroStartML%3A%20A%20combined%20machine%20learning%20and%20physics-based%20approach%20to%0A%20%20reduce%20hydrological%20model%20spin-up%20time&entry.906535625=Louisa%20Pawusch%20and%20Stefania%20Scheurer%20and%20Wolfgang%20Nowak%20and%20Reed%20Maxwell&entry.1292438233=%20%20Finding%20the%20initial%20depth-to-water%20table%20%28DTWT%29%20configuration%20of%20a%20catchment%0Ais%20a%20critical%20challenge%20when%20simulating%20the%20hydrological%20cycle%20with%20integrated%0Amodels%2C%20significantly%20impacting%20simulation%20outcomes.%20Traditionally%2C%20this%0Ainvolves%20iterative%20spin-up%20computations%2C%20where%20the%20model%20runs%20under%20constant%0Aatmospheric%20settings%20until%20steady-state%20is%20achieved.%20These%20so-called%20model%0Aspin-ups%20are%20computationally%20expensive%2C%20often%20requiring%20many%20years%20of%20simulated%0Atime%2C%20particularly%20when%20the%20initial%20DTWT%20configuration%20is%20far%20from%20steady%0Astate.%0A%20%20To%20accelerate%20the%20model%20spin-up%20process%20we%20developed%20HydroStartML%2C%20a%20machine%0Alearning%20emulator%20trained%20on%20steady-state%20DTWT%20configurations%20across%20the%0Acontiguous%20United%20States.%20HydroStartML%20predicts%2C%20based%20on%20available%20data%20like%0Aconductivity%20and%20surface%20slopes%2C%20a%20DTWT%20configuration%20of%20the%20respective%0Awatershed%2C%20which%20can%20be%20used%20as%20an%20initial%20DTWT.%0A%20%20Our%20results%20show%20that%20initializing%20spin-up%20computations%20with%20HydroStartML%0Apredictions%20leads%20to%20faster%20convergence%20than%20with%20other%20initial%20configurations%0Alike%20spatially%20constant%20DTWTs.%20The%20emulator%20accurately%20predicts%20configurations%0Aclose%20to%20steady%20state%2C%20even%20for%20terrain%20configurations%20not%20seen%20in%20training%2C%0Aand%20allows%20especially%20significant%20reductions%20in%20computational%20spin-up%20effort%20in%0Aregions%20with%20deep%20DTWTs.%20This%20work%20opens%20the%20door%20for%20hybrid%20approaches%20that%0Ablend%20machine%20learning%20and%20traditional%20simulation%2C%20enhancing%20predictive%0Aaccuracy%20and%20efficiency%20in%20hydrology%20for%20improving%20water%20resource%20management%0Aand%20understanding%20complex%20environmental%20interactions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17420v1&entry.124074799=Read"},
{"title": "Fault Diagnosis in New Wind Turbines using Knowledge from Existing\n  Turbines by Generative Domain Adaptation", "author": "Stefan Jonas and Angela Meyer", "abstract": "  Intelligent condition monitoring of wind turbines is essential for reducing\ndowntimes. Machine learning models trained on wind turbine operation data are\ncommonly used to detect anomalies and, eventually, operation faults. However,\ndata-driven normal behavior models (NBMs) require a substantial amount of\ntraining data, as NBMs trained with scarce data may result in unreliable fault\ndiagnosis. To overcome this limitation, we present a novel generative deep\nlearning approach to make SCADA samples from one wind turbine lacking training\ndata resemble SCADA data from wind turbines with representative training data.\nThrough CycleGAN-based domain mapping, our method enables the application of an\nNBM trained on an existing wind turbine to one with severely limited data. We\ndemonstrate our approach on field data mapping SCADA samples across 7\nsubstantially different WTs. Our findings show significantly improved fault\ndiagnosis in wind turbines with scarce data. Our method achieves the most\nsimilar anomaly scores to an NBM trained with abundant data, outperforming NBMs\ntrained on scarce training data with improvements of +10.3% in F1-score when 1\nmonth of training data is available and +16.8% when 2 weeks are available. The\ndomain mapping approach outperforms conventional fine-tuning at all considered\ndegrees of data scarcity, ranging from 1 to 8 weeks of training data. The\nproposed technique enables earlier and more reliable fault diagnosis in newly\ninstalled wind farms, demonstrating a novel and promising research direction to\nimprove anomaly detection when faced with training data scarcity.\n", "link": "http://arxiv.org/abs/2504.17709v1", "date": "2025-04-24", "relevancy": 1.8471, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.49}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.471}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fault%20Diagnosis%20in%20New%20Wind%20Turbines%20using%20Knowledge%20from%20Existing%0A%20%20Turbines%20by%20Generative%20Domain%20Adaptation&body=Title%3A%20Fault%20Diagnosis%20in%20New%20Wind%20Turbines%20using%20Knowledge%20from%20Existing%0A%20%20Turbines%20by%20Generative%20Domain%20Adaptation%0AAuthor%3A%20Stefan%20Jonas%20and%20Angela%20Meyer%0AAbstract%3A%20%20%20Intelligent%20condition%20monitoring%20of%20wind%20turbines%20is%20essential%20for%20reducing%0Adowntimes.%20Machine%20learning%20models%20trained%20on%20wind%20turbine%20operation%20data%20are%0Acommonly%20used%20to%20detect%20anomalies%20and%2C%20eventually%2C%20operation%20faults.%20However%2C%0Adata-driven%20normal%20behavior%20models%20%28NBMs%29%20require%20a%20substantial%20amount%20of%0Atraining%20data%2C%20as%20NBMs%20trained%20with%20scarce%20data%20may%20result%20in%20unreliable%20fault%0Adiagnosis.%20To%20overcome%20this%20limitation%2C%20we%20present%20a%20novel%20generative%20deep%0Alearning%20approach%20to%20make%20SCADA%20samples%20from%20one%20wind%20turbine%20lacking%20training%0Adata%20resemble%20SCADA%20data%20from%20wind%20turbines%20with%20representative%20training%20data.%0AThrough%20CycleGAN-based%20domain%20mapping%2C%20our%20method%20enables%20the%20application%20of%20an%0ANBM%20trained%20on%20an%20existing%20wind%20turbine%20to%20one%20with%20severely%20limited%20data.%20We%0Ademonstrate%20our%20approach%20on%20field%20data%20mapping%20SCADA%20samples%20across%207%0Asubstantially%20different%20WTs.%20Our%20findings%20show%20significantly%20improved%20fault%0Adiagnosis%20in%20wind%20turbines%20with%20scarce%20data.%20Our%20method%20achieves%20the%20most%0Asimilar%20anomaly%20scores%20to%20an%20NBM%20trained%20with%20abundant%20data%2C%20outperforming%20NBMs%0Atrained%20on%20scarce%20training%20data%20with%20improvements%20of%20%2B10.3%25%20in%20F1-score%20when%201%0Amonth%20of%20training%20data%20is%20available%20and%20%2B16.8%25%20when%202%20weeks%20are%20available.%20The%0Adomain%20mapping%20approach%20outperforms%20conventional%20fine-tuning%20at%20all%20considered%0Adegrees%20of%20data%20scarcity%2C%20ranging%20from%201%20to%208%20weeks%20of%20training%20data.%20The%0Aproposed%20technique%20enables%20earlier%20and%20more%20reliable%20fault%20diagnosis%20in%20newly%0Ainstalled%20wind%20farms%2C%20demonstrating%20a%20novel%20and%20promising%20research%20direction%20to%0Aimprove%20anomaly%20detection%20when%20faced%20with%20training%20data%20scarcity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17709v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFault%2520Diagnosis%2520in%2520New%2520Wind%2520Turbines%2520using%2520Knowledge%2520from%2520Existing%250A%2520%2520Turbines%2520by%2520Generative%2520Domain%2520Adaptation%26entry.906535625%3DStefan%2520Jonas%2520and%2520Angela%2520Meyer%26entry.1292438233%3D%2520%2520Intelligent%2520condition%2520monitoring%2520of%2520wind%2520turbines%2520is%2520essential%2520for%2520reducing%250Adowntimes.%2520Machine%2520learning%2520models%2520trained%2520on%2520wind%2520turbine%2520operation%2520data%2520are%250Acommonly%2520used%2520to%2520detect%2520anomalies%2520and%252C%2520eventually%252C%2520operation%2520faults.%2520However%252C%250Adata-driven%2520normal%2520behavior%2520models%2520%2528NBMs%2529%2520require%2520a%2520substantial%2520amount%2520of%250Atraining%2520data%252C%2520as%2520NBMs%2520trained%2520with%2520scarce%2520data%2520may%2520result%2520in%2520unreliable%2520fault%250Adiagnosis.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520present%2520a%2520novel%2520generative%2520deep%250Alearning%2520approach%2520to%2520make%2520SCADA%2520samples%2520from%2520one%2520wind%2520turbine%2520lacking%2520training%250Adata%2520resemble%2520SCADA%2520data%2520from%2520wind%2520turbines%2520with%2520representative%2520training%2520data.%250AThrough%2520CycleGAN-based%2520domain%2520mapping%252C%2520our%2520method%2520enables%2520the%2520application%2520of%2520an%250ANBM%2520trained%2520on%2520an%2520existing%2520wind%2520turbine%2520to%2520one%2520with%2520severely%2520limited%2520data.%2520We%250Ademonstrate%2520our%2520approach%2520on%2520field%2520data%2520mapping%2520SCADA%2520samples%2520across%25207%250Asubstantially%2520different%2520WTs.%2520Our%2520findings%2520show%2520significantly%2520improved%2520fault%250Adiagnosis%2520in%2520wind%2520turbines%2520with%2520scarce%2520data.%2520Our%2520method%2520achieves%2520the%2520most%250Asimilar%2520anomaly%2520scores%2520to%2520an%2520NBM%2520trained%2520with%2520abundant%2520data%252C%2520outperforming%2520NBMs%250Atrained%2520on%2520scarce%2520training%2520data%2520with%2520improvements%2520of%2520%252B10.3%2525%2520in%2520F1-score%2520when%25201%250Amonth%2520of%2520training%2520data%2520is%2520available%2520and%2520%252B16.8%2525%2520when%25202%2520weeks%2520are%2520available.%2520The%250Adomain%2520mapping%2520approach%2520outperforms%2520conventional%2520fine-tuning%2520at%2520all%2520considered%250Adegrees%2520of%2520data%2520scarcity%252C%2520ranging%2520from%25201%2520to%25208%2520weeks%2520of%2520training%2520data.%2520The%250Aproposed%2520technique%2520enables%2520earlier%2520and%2520more%2520reliable%2520fault%2520diagnosis%2520in%2520newly%250Ainstalled%2520wind%2520farms%252C%2520demonstrating%2520a%2520novel%2520and%2520promising%2520research%2520direction%2520to%250Aimprove%2520anomaly%2520detection%2520when%2520faced%2520with%2520training%2520data%2520scarcity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17709v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fault%20Diagnosis%20in%20New%20Wind%20Turbines%20using%20Knowledge%20from%20Existing%0A%20%20Turbines%20by%20Generative%20Domain%20Adaptation&entry.906535625=Stefan%20Jonas%20and%20Angela%20Meyer&entry.1292438233=%20%20Intelligent%20condition%20monitoring%20of%20wind%20turbines%20is%20essential%20for%20reducing%0Adowntimes.%20Machine%20learning%20models%20trained%20on%20wind%20turbine%20operation%20data%20are%0Acommonly%20used%20to%20detect%20anomalies%20and%2C%20eventually%2C%20operation%20faults.%20However%2C%0Adata-driven%20normal%20behavior%20models%20%28NBMs%29%20require%20a%20substantial%20amount%20of%0Atraining%20data%2C%20as%20NBMs%20trained%20with%20scarce%20data%20may%20result%20in%20unreliable%20fault%0Adiagnosis.%20To%20overcome%20this%20limitation%2C%20we%20present%20a%20novel%20generative%20deep%0Alearning%20approach%20to%20make%20SCADA%20samples%20from%20one%20wind%20turbine%20lacking%20training%0Adata%20resemble%20SCADA%20data%20from%20wind%20turbines%20with%20representative%20training%20data.%0AThrough%20CycleGAN-based%20domain%20mapping%2C%20our%20method%20enables%20the%20application%20of%20an%0ANBM%20trained%20on%20an%20existing%20wind%20turbine%20to%20one%20with%20severely%20limited%20data.%20We%0Ademonstrate%20our%20approach%20on%20field%20data%20mapping%20SCADA%20samples%20across%207%0Asubstantially%20different%20WTs.%20Our%20findings%20show%20significantly%20improved%20fault%0Adiagnosis%20in%20wind%20turbines%20with%20scarce%20data.%20Our%20method%20achieves%20the%20most%0Asimilar%20anomaly%20scores%20to%20an%20NBM%20trained%20with%20abundant%20data%2C%20outperforming%20NBMs%0Atrained%20on%20scarce%20training%20data%20with%20improvements%20of%20%2B10.3%25%20in%20F1-score%20when%201%0Amonth%20of%20training%20data%20is%20available%20and%20%2B16.8%25%20when%202%20weeks%20are%20available.%20The%0Adomain%20mapping%20approach%20outperforms%20conventional%20fine-tuning%20at%20all%20considered%0Adegrees%20of%20data%20scarcity%2C%20ranging%20from%201%20to%208%20weeks%20of%20training%20data.%20The%0Aproposed%20technique%20enables%20earlier%20and%20more%20reliable%20fault%20diagnosis%20in%20newly%0Ainstalled%20wind%20farms%2C%20demonstrating%20a%20novel%20and%20promising%20research%20direction%20to%0Aimprove%20anomaly%20detection%20when%20faced%20with%20training%20data%20scarcity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17709v1&entry.124074799=Read"},
{"title": "Gripper Keypose and Object Pointflow as Interfaces for Bimanual Robotic\n  Manipulation", "author": "Yuyin Yang and Zetao Cai and Yang Tian and Jia Zeng and Jiangmiao Pang", "abstract": "  Bimanual manipulation is a challenging yet crucial robotic capability,\ndemanding precise spatial localization and versatile motion trajectories, which\npose significant challenges to existing approaches. Existing approaches fall\ninto two categories: keyframe-based strategies, which predict gripper poses in\nkeyframes and execute them via motion planners, and continuous control methods,\nwhich estimate actions sequentially at each timestep. The keyframe-based method\nlacks inter-frame supervision, struggling to perform consistently or execute\ncurved motions, while the continuous method suffers from weaker spatial\nperception. To address these issues, this paper introduces an end-to-end\nframework PPI (keyPose and Pointflow Interface), which integrates the\nprediction of target gripper poses and object pointflow with the continuous\nactions estimation. These interfaces enable the model to effectively attend to\nthe target manipulation area, while the overall framework guides diverse and\ncollision-free trajectories. By combining interface predictions with continuous\nactions estimation, PPI demonstrates superior performance in diverse bimanual\nmanipulation tasks, providing enhanced spatial localization and satisfying\nflexibility in handling movement restrictions. In extensive evaluations, PPI\nsignificantly outperforms prior methods in both simulated and real-world\nexperiments, achieving state-of-the-art performance with a +16.1% improvement\non the RLBench2 simulation benchmark and an average of +27.5% gain across four\nchallenging real-world tasks. Notably, PPI exhibits strong stability, high\nprecision, and remarkable generalization capabilities in real-world scenarios.\nProject page: https://yuyinyang3y.github.io/PPI/\n", "link": "http://arxiv.org/abs/2504.17784v1", "date": "2025-04-24", "relevancy": 1.7458, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5839}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5832}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gripper%20Keypose%20and%20Object%20Pointflow%20as%20Interfaces%20for%20Bimanual%20Robotic%0A%20%20Manipulation&body=Title%3A%20Gripper%20Keypose%20and%20Object%20Pointflow%20as%20Interfaces%20for%20Bimanual%20Robotic%0A%20%20Manipulation%0AAuthor%3A%20Yuyin%20Yang%20and%20Zetao%20Cai%20and%20Yang%20Tian%20and%20Jia%20Zeng%20and%20Jiangmiao%20Pang%0AAbstract%3A%20%20%20Bimanual%20manipulation%20is%20a%20challenging%20yet%20crucial%20robotic%20capability%2C%0Ademanding%20precise%20spatial%20localization%20and%20versatile%20motion%20trajectories%2C%20which%0Apose%20significant%20challenges%20to%20existing%20approaches.%20Existing%20approaches%20fall%0Ainto%20two%20categories%3A%20keyframe-based%20strategies%2C%20which%20predict%20gripper%20poses%20in%0Akeyframes%20and%20execute%20them%20via%20motion%20planners%2C%20and%20continuous%20control%20methods%2C%0Awhich%20estimate%20actions%20sequentially%20at%20each%20timestep.%20The%20keyframe-based%20method%0Alacks%20inter-frame%20supervision%2C%20struggling%20to%20perform%20consistently%20or%20execute%0Acurved%20motions%2C%20while%20the%20continuous%20method%20suffers%20from%20weaker%20spatial%0Aperception.%20To%20address%20these%20issues%2C%20this%20paper%20introduces%20an%20end-to-end%0Aframework%20PPI%20%28keyPose%20and%20Pointflow%20Interface%29%2C%20which%20integrates%20the%0Aprediction%20of%20target%20gripper%20poses%20and%20object%20pointflow%20with%20the%20continuous%0Aactions%20estimation.%20These%20interfaces%20enable%20the%20model%20to%20effectively%20attend%20to%0Athe%20target%20manipulation%20area%2C%20while%20the%20overall%20framework%20guides%20diverse%20and%0Acollision-free%20trajectories.%20By%20combining%20interface%20predictions%20with%20continuous%0Aactions%20estimation%2C%20PPI%20demonstrates%20superior%20performance%20in%20diverse%20bimanual%0Amanipulation%20tasks%2C%20providing%20enhanced%20spatial%20localization%20and%20satisfying%0Aflexibility%20in%20handling%20movement%20restrictions.%20In%20extensive%20evaluations%2C%20PPI%0Asignificantly%20outperforms%20prior%20methods%20in%20both%20simulated%20and%20real-world%0Aexperiments%2C%20achieving%20state-of-the-art%20performance%20with%20a%20%2B16.1%25%20improvement%0Aon%20the%20RLBench2%20simulation%20benchmark%20and%20an%20average%20of%20%2B27.5%25%20gain%20across%20four%0Achallenging%20real-world%20tasks.%20Notably%2C%20PPI%20exhibits%20strong%20stability%2C%20high%0Aprecision%2C%20and%20remarkable%20generalization%20capabilities%20in%20real-world%20scenarios.%0AProject%20page%3A%20https%3A//yuyinyang3y.github.io/PPI/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGripper%2520Keypose%2520and%2520Object%2520Pointflow%2520as%2520Interfaces%2520for%2520Bimanual%2520Robotic%250A%2520%2520Manipulation%26entry.906535625%3DYuyin%2520Yang%2520and%2520Zetao%2520Cai%2520and%2520Yang%2520Tian%2520and%2520Jia%2520Zeng%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3D%2520%2520Bimanual%2520manipulation%2520is%2520a%2520challenging%2520yet%2520crucial%2520robotic%2520capability%252C%250Ademanding%2520precise%2520spatial%2520localization%2520and%2520versatile%2520motion%2520trajectories%252C%2520which%250Apose%2520significant%2520challenges%2520to%2520existing%2520approaches.%2520Existing%2520approaches%2520fall%250Ainto%2520two%2520categories%253A%2520keyframe-based%2520strategies%252C%2520which%2520predict%2520gripper%2520poses%2520in%250Akeyframes%2520and%2520execute%2520them%2520via%2520motion%2520planners%252C%2520and%2520continuous%2520control%2520methods%252C%250Awhich%2520estimate%2520actions%2520sequentially%2520at%2520each%2520timestep.%2520The%2520keyframe-based%2520method%250Alacks%2520inter-frame%2520supervision%252C%2520struggling%2520to%2520perform%2520consistently%2520or%2520execute%250Acurved%2520motions%252C%2520while%2520the%2520continuous%2520method%2520suffers%2520from%2520weaker%2520spatial%250Aperception.%2520To%2520address%2520these%2520issues%252C%2520this%2520paper%2520introduces%2520an%2520end-to-end%250Aframework%2520PPI%2520%2528keyPose%2520and%2520Pointflow%2520Interface%2529%252C%2520which%2520integrates%2520the%250Aprediction%2520of%2520target%2520gripper%2520poses%2520and%2520object%2520pointflow%2520with%2520the%2520continuous%250Aactions%2520estimation.%2520These%2520interfaces%2520enable%2520the%2520model%2520to%2520effectively%2520attend%2520to%250Athe%2520target%2520manipulation%2520area%252C%2520while%2520the%2520overall%2520framework%2520guides%2520diverse%2520and%250Acollision-free%2520trajectories.%2520By%2520combining%2520interface%2520predictions%2520with%2520continuous%250Aactions%2520estimation%252C%2520PPI%2520demonstrates%2520superior%2520performance%2520in%2520diverse%2520bimanual%250Amanipulation%2520tasks%252C%2520providing%2520enhanced%2520spatial%2520localization%2520and%2520satisfying%250Aflexibility%2520in%2520handling%2520movement%2520restrictions.%2520In%2520extensive%2520evaluations%252C%2520PPI%250Asignificantly%2520outperforms%2520prior%2520methods%2520in%2520both%2520simulated%2520and%2520real-world%250Aexperiments%252C%2520achieving%2520state-of-the-art%2520performance%2520with%2520a%2520%252B16.1%2525%2520improvement%250Aon%2520the%2520RLBench2%2520simulation%2520benchmark%2520and%2520an%2520average%2520of%2520%252B27.5%2525%2520gain%2520across%2520four%250Achallenging%2520real-world%2520tasks.%2520Notably%252C%2520PPI%2520exhibits%2520strong%2520stability%252C%2520high%250Aprecision%252C%2520and%2520remarkable%2520generalization%2520capabilities%2520in%2520real-world%2520scenarios.%250AProject%2520page%253A%2520https%253A//yuyinyang3y.github.io/PPI/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gripper%20Keypose%20and%20Object%20Pointflow%20as%20Interfaces%20for%20Bimanual%20Robotic%0A%20%20Manipulation&entry.906535625=Yuyin%20Yang%20and%20Zetao%20Cai%20and%20Yang%20Tian%20and%20Jia%20Zeng%20and%20Jiangmiao%20Pang&entry.1292438233=%20%20Bimanual%20manipulation%20is%20a%20challenging%20yet%20crucial%20robotic%20capability%2C%0Ademanding%20precise%20spatial%20localization%20and%20versatile%20motion%20trajectories%2C%20which%0Apose%20significant%20challenges%20to%20existing%20approaches.%20Existing%20approaches%20fall%0Ainto%20two%20categories%3A%20keyframe-based%20strategies%2C%20which%20predict%20gripper%20poses%20in%0Akeyframes%20and%20execute%20them%20via%20motion%20planners%2C%20and%20continuous%20control%20methods%2C%0Awhich%20estimate%20actions%20sequentially%20at%20each%20timestep.%20The%20keyframe-based%20method%0Alacks%20inter-frame%20supervision%2C%20struggling%20to%20perform%20consistently%20or%20execute%0Acurved%20motions%2C%20while%20the%20continuous%20method%20suffers%20from%20weaker%20spatial%0Aperception.%20To%20address%20these%20issues%2C%20this%20paper%20introduces%20an%20end-to-end%0Aframework%20PPI%20%28keyPose%20and%20Pointflow%20Interface%29%2C%20which%20integrates%20the%0Aprediction%20of%20target%20gripper%20poses%20and%20object%20pointflow%20with%20the%20continuous%0Aactions%20estimation.%20These%20interfaces%20enable%20the%20model%20to%20effectively%20attend%20to%0Athe%20target%20manipulation%20area%2C%20while%20the%20overall%20framework%20guides%20diverse%20and%0Acollision-free%20trajectories.%20By%20combining%20interface%20predictions%20with%20continuous%0Aactions%20estimation%2C%20PPI%20demonstrates%20superior%20performance%20in%20diverse%20bimanual%0Amanipulation%20tasks%2C%20providing%20enhanced%20spatial%20localization%20and%20satisfying%0Aflexibility%20in%20handling%20movement%20restrictions.%20In%20extensive%20evaluations%2C%20PPI%0Asignificantly%20outperforms%20prior%20methods%20in%20both%20simulated%20and%20real-world%0Aexperiments%2C%20achieving%20state-of-the-art%20performance%20with%20a%20%2B16.1%25%20improvement%0Aon%20the%20RLBench2%20simulation%20benchmark%20and%20an%20average%20of%20%2B27.5%25%20gain%20across%20four%0Achallenging%20real-world%20tasks.%20Notably%2C%20PPI%20exhibits%20strong%20stability%2C%20high%0Aprecision%2C%20and%20remarkable%20generalization%20capabilities%20in%20real-world%20scenarios.%0AProject%20page%3A%20https%3A//yuyinyang3y.github.io/PPI/%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17784v1&entry.124074799=Read"},
{"title": "Aerial Image Classification in Scarce and Unconstrained Environments via\n  Conformal Prediction", "author": "Farhad Pourkamali-Anaraki", "abstract": "  This paper presents a comprehensive empirical analysis of conformal\nprediction methods on a challenging aerial image dataset featuring diverse\nevents in unconstrained environments. Conformal prediction is a powerful\npost-hoc technique that takes the output of any classifier and transforms it\ninto a set of likely labels, providing a statistical guarantee on the coverage\nof the true label. Unlike evaluations on standard benchmarks, our study\naddresses the complexities of data-scarce and highly variable real-world\nsettings. We investigate the effectiveness of leveraging pretrained models\n(MobileNet, DenseNet, and ResNet), fine-tuned with limited labeled data, to\ngenerate informative prediction sets. To further evaluate the impact of\ncalibration, we consider two parallel pipelines (with and without temperature\nscaling) and assess performance using two key metrics: empirical coverage and\naverage prediction set size. This setup allows us to systematically examine how\ncalibration choices influence the trade-off between reliability and efficiency.\nOur findings demonstrate that even with relatively small labeled samples and\nsimple nonconformity scores, conformal prediction can yield valuable\nuncertainty estimates for complex tasks. Moreover, our analysis reveals that\nwhile temperature scaling is often employed for calibration, it does not\nconsistently lead to smaller prediction sets, underscoring the importance of\ncareful consideration in its application. Furthermore, our results highlight\nthe significant potential of model compression techniques within the conformal\nprediction pipeline for deployment in resource-constrained environments. Based\non our observations, we advocate for future research to delve into the impact\nof noisy or ambiguous labels on conformal prediction performance and to explore\neffective model reduction strategies.\n", "link": "http://arxiv.org/abs/2504.17655v1", "date": "2025-04-24", "relevancy": 1.7041, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5778}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5709}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aerial%20Image%20Classification%20in%20Scarce%20and%20Unconstrained%20Environments%20via%0A%20%20Conformal%20Prediction&body=Title%3A%20Aerial%20Image%20Classification%20in%20Scarce%20and%20Unconstrained%20Environments%20via%0A%20%20Conformal%20Prediction%0AAuthor%3A%20Farhad%20Pourkamali-Anaraki%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20comprehensive%20empirical%20analysis%20of%20conformal%0Aprediction%20methods%20on%20a%20challenging%20aerial%20image%20dataset%20featuring%20diverse%0Aevents%20in%20unconstrained%20environments.%20Conformal%20prediction%20is%20a%20powerful%0Apost-hoc%20technique%20that%20takes%20the%20output%20of%20any%20classifier%20and%20transforms%20it%0Ainto%20a%20set%20of%20likely%20labels%2C%20providing%20a%20statistical%20guarantee%20on%20the%20coverage%0Aof%20the%20true%20label.%20Unlike%20evaluations%20on%20standard%20benchmarks%2C%20our%20study%0Aaddresses%20the%20complexities%20of%20data-scarce%20and%20highly%20variable%20real-world%0Asettings.%20We%20investigate%20the%20effectiveness%20of%20leveraging%20pretrained%20models%0A%28MobileNet%2C%20DenseNet%2C%20and%20ResNet%29%2C%20fine-tuned%20with%20limited%20labeled%20data%2C%20to%0Agenerate%20informative%20prediction%20sets.%20To%20further%20evaluate%20the%20impact%20of%0Acalibration%2C%20we%20consider%20two%20parallel%20pipelines%20%28with%20and%20without%20temperature%0Ascaling%29%20and%20assess%20performance%20using%20two%20key%20metrics%3A%20empirical%20coverage%20and%0Aaverage%20prediction%20set%20size.%20This%20setup%20allows%20us%20to%20systematically%20examine%20how%0Acalibration%20choices%20influence%20the%20trade-off%20between%20reliability%20and%20efficiency.%0AOur%20findings%20demonstrate%20that%20even%20with%20relatively%20small%20labeled%20samples%20and%0Asimple%20nonconformity%20scores%2C%20conformal%20prediction%20can%20yield%20valuable%0Auncertainty%20estimates%20for%20complex%20tasks.%20Moreover%2C%20our%20analysis%20reveals%20that%0Awhile%20temperature%20scaling%20is%20often%20employed%20for%20calibration%2C%20it%20does%20not%0Aconsistently%20lead%20to%20smaller%20prediction%20sets%2C%20underscoring%20the%20importance%20of%0Acareful%20consideration%20in%20its%20application.%20Furthermore%2C%20our%20results%20highlight%0Athe%20significant%20potential%20of%20model%20compression%20techniques%20within%20the%20conformal%0Aprediction%20pipeline%20for%20deployment%20in%20resource-constrained%20environments.%20Based%0Aon%20our%20observations%2C%20we%20advocate%20for%20future%20research%20to%20delve%20into%20the%20impact%0Aof%20noisy%20or%20ambiguous%20labels%20on%20conformal%20prediction%20performance%20and%20to%20explore%0Aeffective%20model%20reduction%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17655v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAerial%2520Image%2520Classification%2520in%2520Scarce%2520and%2520Unconstrained%2520Environments%2520via%250A%2520%2520Conformal%2520Prediction%26entry.906535625%3DFarhad%2520Pourkamali-Anaraki%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520comprehensive%2520empirical%2520analysis%2520of%2520conformal%250Aprediction%2520methods%2520on%2520a%2520challenging%2520aerial%2520image%2520dataset%2520featuring%2520diverse%250Aevents%2520in%2520unconstrained%2520environments.%2520Conformal%2520prediction%2520is%2520a%2520powerful%250Apost-hoc%2520technique%2520that%2520takes%2520the%2520output%2520of%2520any%2520classifier%2520and%2520transforms%2520it%250Ainto%2520a%2520set%2520of%2520likely%2520labels%252C%2520providing%2520a%2520statistical%2520guarantee%2520on%2520the%2520coverage%250Aof%2520the%2520true%2520label.%2520Unlike%2520evaluations%2520on%2520standard%2520benchmarks%252C%2520our%2520study%250Aaddresses%2520the%2520complexities%2520of%2520data-scarce%2520and%2520highly%2520variable%2520real-world%250Asettings.%2520We%2520investigate%2520the%2520effectiveness%2520of%2520leveraging%2520pretrained%2520models%250A%2528MobileNet%252C%2520DenseNet%252C%2520and%2520ResNet%2529%252C%2520fine-tuned%2520with%2520limited%2520labeled%2520data%252C%2520to%250Agenerate%2520informative%2520prediction%2520sets.%2520To%2520further%2520evaluate%2520the%2520impact%2520of%250Acalibration%252C%2520we%2520consider%2520two%2520parallel%2520pipelines%2520%2528with%2520and%2520without%2520temperature%250Ascaling%2529%2520and%2520assess%2520performance%2520using%2520two%2520key%2520metrics%253A%2520empirical%2520coverage%2520and%250Aaverage%2520prediction%2520set%2520size.%2520This%2520setup%2520allows%2520us%2520to%2520systematically%2520examine%2520how%250Acalibration%2520choices%2520influence%2520the%2520trade-off%2520between%2520reliability%2520and%2520efficiency.%250AOur%2520findings%2520demonstrate%2520that%2520even%2520with%2520relatively%2520small%2520labeled%2520samples%2520and%250Asimple%2520nonconformity%2520scores%252C%2520conformal%2520prediction%2520can%2520yield%2520valuable%250Auncertainty%2520estimates%2520for%2520complex%2520tasks.%2520Moreover%252C%2520our%2520analysis%2520reveals%2520that%250Awhile%2520temperature%2520scaling%2520is%2520often%2520employed%2520for%2520calibration%252C%2520it%2520does%2520not%250Aconsistently%2520lead%2520to%2520smaller%2520prediction%2520sets%252C%2520underscoring%2520the%2520importance%2520of%250Acareful%2520consideration%2520in%2520its%2520application.%2520Furthermore%252C%2520our%2520results%2520highlight%250Athe%2520significant%2520potential%2520of%2520model%2520compression%2520techniques%2520within%2520the%2520conformal%250Aprediction%2520pipeline%2520for%2520deployment%2520in%2520resource-constrained%2520environments.%2520Based%250Aon%2520our%2520observations%252C%2520we%2520advocate%2520for%2520future%2520research%2520to%2520delve%2520into%2520the%2520impact%250Aof%2520noisy%2520or%2520ambiguous%2520labels%2520on%2520conformal%2520prediction%2520performance%2520and%2520to%2520explore%250Aeffective%2520model%2520reduction%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17655v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aerial%20Image%20Classification%20in%20Scarce%20and%20Unconstrained%20Environments%20via%0A%20%20Conformal%20Prediction&entry.906535625=Farhad%20Pourkamali-Anaraki&entry.1292438233=%20%20This%20paper%20presents%20a%20comprehensive%20empirical%20analysis%20of%20conformal%0Aprediction%20methods%20on%20a%20challenging%20aerial%20image%20dataset%20featuring%20diverse%0Aevents%20in%20unconstrained%20environments.%20Conformal%20prediction%20is%20a%20powerful%0Apost-hoc%20technique%20that%20takes%20the%20output%20of%20any%20classifier%20and%20transforms%20it%0Ainto%20a%20set%20of%20likely%20labels%2C%20providing%20a%20statistical%20guarantee%20on%20the%20coverage%0Aof%20the%20true%20label.%20Unlike%20evaluations%20on%20standard%20benchmarks%2C%20our%20study%0Aaddresses%20the%20complexities%20of%20data-scarce%20and%20highly%20variable%20real-world%0Asettings.%20We%20investigate%20the%20effectiveness%20of%20leveraging%20pretrained%20models%0A%28MobileNet%2C%20DenseNet%2C%20and%20ResNet%29%2C%20fine-tuned%20with%20limited%20labeled%20data%2C%20to%0Agenerate%20informative%20prediction%20sets.%20To%20further%20evaluate%20the%20impact%20of%0Acalibration%2C%20we%20consider%20two%20parallel%20pipelines%20%28with%20and%20without%20temperature%0Ascaling%29%20and%20assess%20performance%20using%20two%20key%20metrics%3A%20empirical%20coverage%20and%0Aaverage%20prediction%20set%20size.%20This%20setup%20allows%20us%20to%20systematically%20examine%20how%0Acalibration%20choices%20influence%20the%20trade-off%20between%20reliability%20and%20efficiency.%0AOur%20findings%20demonstrate%20that%20even%20with%20relatively%20small%20labeled%20samples%20and%0Asimple%20nonconformity%20scores%2C%20conformal%20prediction%20can%20yield%20valuable%0Auncertainty%20estimates%20for%20complex%20tasks.%20Moreover%2C%20our%20analysis%20reveals%20that%0Awhile%20temperature%20scaling%20is%20often%20employed%20for%20calibration%2C%20it%20does%20not%0Aconsistently%20lead%20to%20smaller%20prediction%20sets%2C%20underscoring%20the%20importance%20of%0Acareful%20consideration%20in%20its%20application.%20Furthermore%2C%20our%20results%20highlight%0Athe%20significant%20potential%20of%20model%20compression%20techniques%20within%20the%20conformal%0Aprediction%20pipeline%20for%20deployment%20in%20resource-constrained%20environments.%20Based%0Aon%20our%20observations%2C%20we%20advocate%20for%20future%20research%20to%20delve%20into%20the%20impact%0Aof%20noisy%20or%20ambiguous%20labels%20on%20conformal%20prediction%20performance%20and%20to%20explore%0Aeffective%20model%20reduction%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17655v1&entry.124074799=Read"},
{"title": "Data-Driven Calibration of Prediction Sets in Large Vision-Language\n  Models Based on Inductive Conformal Prediction", "author": "Yuanchang Ye and Weiyan Wen", "abstract": "  This study addresses the critical challenge of hallucination mitigation in\nLarge Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks\nthrough a Split Conformal Prediction (SCP) framework. While LVLMs excel in\nmulti-modal reasoning, their outputs often exhibit hallucinated content with\nhigh confidence, posing risks in safety-critical applications. We propose a\nmodel-agnostic uncertainty quantification method that integrates dynamic\nthreshold calibration and cross-modal consistency verification. By partitioning\ndata into calibration and test sets, the framework computes nonconformity\nscores to construct prediction sets with statistical guarantees under\nuser-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous\ncontrol of \\textbf{marginal coverage} to ensure empirical error rates remain\nstrictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes\ninversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of\nprior distribution assumptions and retraining requirements. Evaluations on\nbenchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces\ntheoretical guarantees across all $\\alpha$ values. The framework achieves\nstable performance across varying calibration-to-test split ratios,\nunderscoring its robustness for real-world deployment in healthcare, autonomous\nsystems, and other safety-sensitive domains. This work bridges the gap between\ntheoretical reliability and practical applicability in multi-modal AI systems,\noffering a scalable solution for hallucination detection and uncertainty-aware\ndecision-making.\n", "link": "http://arxiv.org/abs/2504.17671v1", "date": "2025-04-24", "relevancy": 1.7292, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5816}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5785}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Driven%20Calibration%20of%20Prediction%20Sets%20in%20Large%20Vision-Language%0A%20%20Models%20Based%20on%20Inductive%20Conformal%20Prediction&body=Title%3A%20Data-Driven%20Calibration%20of%20Prediction%20Sets%20in%20Large%20Vision-Language%0A%20%20Models%20Based%20on%20Inductive%20Conformal%20Prediction%0AAuthor%3A%20Yuanchang%20Ye%20and%20Weiyan%20Wen%0AAbstract%3A%20%20%20This%20study%20addresses%20the%20critical%20challenge%20of%20hallucination%20mitigation%20in%0ALarge%20Vision-Language%20Models%20%28LVLMs%29%20for%20Visual%20Question%20Answering%20%28VQA%29%20tasks%0Athrough%20a%20Split%20Conformal%20Prediction%20%28SCP%29%20framework.%20While%20LVLMs%20excel%20in%0Amulti-modal%20reasoning%2C%20their%20outputs%20often%20exhibit%20hallucinated%20content%20with%0Ahigh%20confidence%2C%20posing%20risks%20in%20safety-critical%20applications.%20We%20propose%20a%0Amodel-agnostic%20uncertainty%20quantification%20method%20that%20integrates%20dynamic%0Athreshold%20calibration%20and%20cross-modal%20consistency%20verification.%20By%20partitioning%0Adata%20into%20calibration%20and%20test%20sets%2C%20the%20framework%20computes%20nonconformity%0Ascores%20to%20construct%20prediction%20sets%20with%20statistical%20guarantees%20under%0Auser-defined%20risk%20levels%20%28%24%5Calpha%24%29.%20Key%20innovations%20include%3A%20%281%29%20rigorous%0Acontrol%20of%20%5Ctextbf%7Bmarginal%20coverage%7D%20to%20ensure%20empirical%20error%20rates%20remain%0Astrictly%20below%20%24%5Calpha%24%3B%20%282%29%20dynamic%20adjustment%20of%20prediction%20set%20sizes%0Ainversely%20with%20%24%5Calpha%24%2C%20filtering%20low-confidence%20outputs%3B%20%283%29%20elimination%20of%0Aprior%20distribution%20assumptions%20and%20retraining%20requirements.%20Evaluations%20on%0Abenchmarks%20%28ScienceQA%2C%20MMMU%29%20with%20eight%20LVLMs%20demonstrate%20that%20SCP%20enforces%0Atheoretical%20guarantees%20across%20all%20%24%5Calpha%24%20values.%20The%20framework%20achieves%0Astable%20performance%20across%20varying%20calibration-to-test%20split%20ratios%2C%0Aunderscoring%20its%20robustness%20for%20real-world%20deployment%20in%20healthcare%2C%20autonomous%0Asystems%2C%20and%20other%20safety-sensitive%20domains.%20This%20work%20bridges%20the%20gap%20between%0Atheoretical%20reliability%20and%20practical%20applicability%20in%20multi-modal%20AI%20systems%2C%0Aoffering%20a%20scalable%20solution%20for%20hallucination%20detection%20and%20uncertainty-aware%0Adecision-making.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17671v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Driven%2520Calibration%2520of%2520Prediction%2520Sets%2520in%2520Large%2520Vision-Language%250A%2520%2520Models%2520Based%2520on%2520Inductive%2520Conformal%2520Prediction%26entry.906535625%3DYuanchang%2520Ye%2520and%2520Weiyan%2520Wen%26entry.1292438233%3D%2520%2520This%2520study%2520addresses%2520the%2520critical%2520challenge%2520of%2520hallucination%2520mitigation%2520in%250ALarge%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520for%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520tasks%250Athrough%2520a%2520Split%2520Conformal%2520Prediction%2520%2528SCP%2529%2520framework.%2520While%2520LVLMs%2520excel%2520in%250Amulti-modal%2520reasoning%252C%2520their%2520outputs%2520often%2520exhibit%2520hallucinated%2520content%2520with%250Ahigh%2520confidence%252C%2520posing%2520risks%2520in%2520safety-critical%2520applications.%2520We%2520propose%2520a%250Amodel-agnostic%2520uncertainty%2520quantification%2520method%2520that%2520integrates%2520dynamic%250Athreshold%2520calibration%2520and%2520cross-modal%2520consistency%2520verification.%2520By%2520partitioning%250Adata%2520into%2520calibration%2520and%2520test%2520sets%252C%2520the%2520framework%2520computes%2520nonconformity%250Ascores%2520to%2520construct%2520prediction%2520sets%2520with%2520statistical%2520guarantees%2520under%250Auser-defined%2520risk%2520levels%2520%2528%2524%255Calpha%2524%2529.%2520Key%2520innovations%2520include%253A%2520%25281%2529%2520rigorous%250Acontrol%2520of%2520%255Ctextbf%257Bmarginal%2520coverage%257D%2520to%2520ensure%2520empirical%2520error%2520rates%2520remain%250Astrictly%2520below%2520%2524%255Calpha%2524%253B%2520%25282%2529%2520dynamic%2520adjustment%2520of%2520prediction%2520set%2520sizes%250Ainversely%2520with%2520%2524%255Calpha%2524%252C%2520filtering%2520low-confidence%2520outputs%253B%2520%25283%2529%2520elimination%2520of%250Aprior%2520distribution%2520assumptions%2520and%2520retraining%2520requirements.%2520Evaluations%2520on%250Abenchmarks%2520%2528ScienceQA%252C%2520MMMU%2529%2520with%2520eight%2520LVLMs%2520demonstrate%2520that%2520SCP%2520enforces%250Atheoretical%2520guarantees%2520across%2520all%2520%2524%255Calpha%2524%2520values.%2520The%2520framework%2520achieves%250Astable%2520performance%2520across%2520varying%2520calibration-to-test%2520split%2520ratios%252C%250Aunderscoring%2520its%2520robustness%2520for%2520real-world%2520deployment%2520in%2520healthcare%252C%2520autonomous%250Asystems%252C%2520and%2520other%2520safety-sensitive%2520domains.%2520This%2520work%2520bridges%2520the%2520gap%2520between%250Atheoretical%2520reliability%2520and%2520practical%2520applicability%2520in%2520multi-modal%2520AI%2520systems%252C%250Aoffering%2520a%2520scalable%2520solution%2520for%2520hallucination%2520detection%2520and%2520uncertainty-aware%250Adecision-making.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17671v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Driven%20Calibration%20of%20Prediction%20Sets%20in%20Large%20Vision-Language%0A%20%20Models%20Based%20on%20Inductive%20Conformal%20Prediction&entry.906535625=Yuanchang%20Ye%20and%20Weiyan%20Wen&entry.1292438233=%20%20This%20study%20addresses%20the%20critical%20challenge%20of%20hallucination%20mitigation%20in%0ALarge%20Vision-Language%20Models%20%28LVLMs%29%20for%20Visual%20Question%20Answering%20%28VQA%29%20tasks%0Athrough%20a%20Split%20Conformal%20Prediction%20%28SCP%29%20framework.%20While%20LVLMs%20excel%20in%0Amulti-modal%20reasoning%2C%20their%20outputs%20often%20exhibit%20hallucinated%20content%20with%0Ahigh%20confidence%2C%20posing%20risks%20in%20safety-critical%20applications.%20We%20propose%20a%0Amodel-agnostic%20uncertainty%20quantification%20method%20that%20integrates%20dynamic%0Athreshold%20calibration%20and%20cross-modal%20consistency%20verification.%20By%20partitioning%0Adata%20into%20calibration%20and%20test%20sets%2C%20the%20framework%20computes%20nonconformity%0Ascores%20to%20construct%20prediction%20sets%20with%20statistical%20guarantees%20under%0Auser-defined%20risk%20levels%20%28%24%5Calpha%24%29.%20Key%20innovations%20include%3A%20%281%29%20rigorous%0Acontrol%20of%20%5Ctextbf%7Bmarginal%20coverage%7D%20to%20ensure%20empirical%20error%20rates%20remain%0Astrictly%20below%20%24%5Calpha%24%3B%20%282%29%20dynamic%20adjustment%20of%20prediction%20set%20sizes%0Ainversely%20with%20%24%5Calpha%24%2C%20filtering%20low-confidence%20outputs%3B%20%283%29%20elimination%20of%0Aprior%20distribution%20assumptions%20and%20retraining%20requirements.%20Evaluations%20on%0Abenchmarks%20%28ScienceQA%2C%20MMMU%29%20with%20eight%20LVLMs%20demonstrate%20that%20SCP%20enforces%0Atheoretical%20guarantees%20across%20all%20%24%5Calpha%24%20values.%20The%20framework%20achieves%0Astable%20performance%20across%20varying%20calibration-to-test%20split%20ratios%2C%0Aunderscoring%20its%20robustness%20for%20real-world%20deployment%20in%20healthcare%2C%20autonomous%0Asystems%2C%20and%20other%20safety-sensitive%20domains.%20This%20work%20bridges%20the%20gap%20between%0Atheoretical%20reliability%20and%20practical%20applicability%20in%20multi-modal%20AI%20systems%2C%0Aoffering%20a%20scalable%20solution%20for%20hallucination%20detection%20and%20uncertainty-aware%0Adecision-making.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17671v1&entry.124074799=Read"},
{"title": "Early Detection of Multidrug Resistance Using Multivariate Time Series\n  Analysis and Interpretable Patient-Similarity Representations", "author": "\u00d3scar Escudero-Arnanz and Antonio G. Marques and Inmaculada Mora-Jim\u00e9nez and Joaqu\u00edn \u00c1lvarez-Rodr\u00edguez and Cristina Soguero-Ruiz", "abstract": "  Background and Objectives: Multidrug Resistance (MDR) is a critical global\nhealth issue, causing increased hospital stays, healthcare costs, and\nmortality. This study proposes an interpretable Machine Learning (ML) framework\nfor MDR prediction, aiming for both accurate inference and enhanced\nexplainability.\n  Methods: Patients are modeled as Multivariate Time Series (MTS), capturing\nclinical progression and patient-to-patient interactions. Similarity among\npatients is quantified using MTS-based methods: descriptive statistics, Dynamic\nTime Warping, and Time Cluster Kernel. These similarity measures serve as\ninputs for MDR classification via Logistic Regression, Random Forest, and\nSupport Vector Machines, with dimensionality reduction and kernel\ntransformations improving model performance. For explainability, patient\nsimilarity networks are constructed from these metrics. Spectral clustering and\nt-SNE are applied to identify MDR-related subgroups and visualize high-risk\nclusters, enabling insight into clinically relevant patterns.\n  Results: The framework was validated on ICU Electronic Health Records from\nthe University Hospital of Fuenlabrada, achieving an AUC of 81%. It outperforms\nbaseline ML and deep learning models by leveraging graph-based patient\nsimilarity. The approach identifies key risk factors -- prolonged antibiotic\nuse, invasive procedures, co-infections, and extended ICU stays -- and reveals\nclinically meaningful clusters. Code and results are available at\n\\https://github.com/oscarescuderoarnanz/DM4MTS.\n  Conclusions: Patient similarity representations combined with graph-based\nanalysis provide accurate MDR prediction and interpretable insights. This\nmethod supports early detection, risk factor identification, and patient\nstratification, highlighting the potential of explainable ML in critical care.\n", "link": "http://arxiv.org/abs/2504.17717v1", "date": "2025-04-24", "relevancy": 1.3843, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4855}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4831}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Early%20Detection%20of%20Multidrug%20Resistance%20Using%20Multivariate%20Time%20Series%0A%20%20Analysis%20and%20Interpretable%20Patient-Similarity%20Representations&body=Title%3A%20Early%20Detection%20of%20Multidrug%20Resistance%20Using%20Multivariate%20Time%20Series%0A%20%20Analysis%20and%20Interpretable%20Patient-Similarity%20Representations%0AAuthor%3A%20%C3%93scar%20Escudero-Arnanz%20and%20Antonio%20G.%20Marques%20and%20Inmaculada%20Mora-Jim%C3%A9nez%20and%20Joaqu%C3%ADn%20%C3%81lvarez-Rodr%C3%ADguez%20and%20Cristina%20Soguero-Ruiz%0AAbstract%3A%20%20%20Background%20and%20Objectives%3A%20Multidrug%20Resistance%20%28MDR%29%20is%20a%20critical%20global%0Ahealth%20issue%2C%20causing%20increased%20hospital%20stays%2C%20healthcare%20costs%2C%20and%0Amortality.%20This%20study%20proposes%20an%20interpretable%20Machine%20Learning%20%28ML%29%20framework%0Afor%20MDR%20prediction%2C%20aiming%20for%20both%20accurate%20inference%20and%20enhanced%0Aexplainability.%0A%20%20Methods%3A%20Patients%20are%20modeled%20as%20Multivariate%20Time%20Series%20%28MTS%29%2C%20capturing%0Aclinical%20progression%20and%20patient-to-patient%20interactions.%20Similarity%20among%0Apatients%20is%20quantified%20using%20MTS-based%20methods%3A%20descriptive%20statistics%2C%20Dynamic%0ATime%20Warping%2C%20and%20Time%20Cluster%20Kernel.%20These%20similarity%20measures%20serve%20as%0Ainputs%20for%20MDR%20classification%20via%20Logistic%20Regression%2C%20Random%20Forest%2C%20and%0ASupport%20Vector%20Machines%2C%20with%20dimensionality%20reduction%20and%20kernel%0Atransformations%20improving%20model%20performance.%20For%20explainability%2C%20patient%0Asimilarity%20networks%20are%20constructed%20from%20these%20metrics.%20Spectral%20clustering%20and%0At-SNE%20are%20applied%20to%20identify%20MDR-related%20subgroups%20and%20visualize%20high-risk%0Aclusters%2C%20enabling%20insight%20into%20clinically%20relevant%20patterns.%0A%20%20Results%3A%20The%20framework%20was%20validated%20on%20ICU%20Electronic%20Health%20Records%20from%0Athe%20University%20Hospital%20of%20Fuenlabrada%2C%20achieving%20an%20AUC%20of%2081%25.%20It%20outperforms%0Abaseline%20ML%20and%20deep%20learning%20models%20by%20leveraging%20graph-based%20patient%0Asimilarity.%20The%20approach%20identifies%20key%20risk%20factors%20--%20prolonged%20antibiotic%0Ause%2C%20invasive%20procedures%2C%20co-infections%2C%20and%20extended%20ICU%20stays%20--%20and%20reveals%0Aclinically%20meaningful%20clusters.%20Code%20and%20results%20are%20available%20at%0A%5Chttps%3A//github.com/oscarescuderoarnanz/DM4MTS.%0A%20%20Conclusions%3A%20Patient%20similarity%20representations%20combined%20with%20graph-based%0Aanalysis%20provide%20accurate%20MDR%20prediction%20and%20interpretable%20insights.%20This%0Amethod%20supports%20early%20detection%2C%20risk%20factor%20identification%2C%20and%20patient%0Astratification%2C%20highlighting%20the%20potential%20of%20explainable%20ML%20in%20critical%20care.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17717v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEarly%2520Detection%2520of%2520Multidrug%2520Resistance%2520Using%2520Multivariate%2520Time%2520Series%250A%2520%2520Analysis%2520and%2520Interpretable%2520Patient-Similarity%2520Representations%26entry.906535625%3D%25C3%2593scar%2520Escudero-Arnanz%2520and%2520Antonio%2520G.%2520Marques%2520and%2520Inmaculada%2520Mora-Jim%25C3%25A9nez%2520and%2520Joaqu%25C3%25ADn%2520%25C3%2581lvarez-Rodr%25C3%25ADguez%2520and%2520Cristina%2520Soguero-Ruiz%26entry.1292438233%3D%2520%2520Background%2520and%2520Objectives%253A%2520Multidrug%2520Resistance%2520%2528MDR%2529%2520is%2520a%2520critical%2520global%250Ahealth%2520issue%252C%2520causing%2520increased%2520hospital%2520stays%252C%2520healthcare%2520costs%252C%2520and%250Amortality.%2520This%2520study%2520proposes%2520an%2520interpretable%2520Machine%2520Learning%2520%2528ML%2529%2520framework%250Afor%2520MDR%2520prediction%252C%2520aiming%2520for%2520both%2520accurate%2520inference%2520and%2520enhanced%250Aexplainability.%250A%2520%2520Methods%253A%2520Patients%2520are%2520modeled%2520as%2520Multivariate%2520Time%2520Series%2520%2528MTS%2529%252C%2520capturing%250Aclinical%2520progression%2520and%2520patient-to-patient%2520interactions.%2520Similarity%2520among%250Apatients%2520is%2520quantified%2520using%2520MTS-based%2520methods%253A%2520descriptive%2520statistics%252C%2520Dynamic%250ATime%2520Warping%252C%2520and%2520Time%2520Cluster%2520Kernel.%2520These%2520similarity%2520measures%2520serve%2520as%250Ainputs%2520for%2520MDR%2520classification%2520via%2520Logistic%2520Regression%252C%2520Random%2520Forest%252C%2520and%250ASupport%2520Vector%2520Machines%252C%2520with%2520dimensionality%2520reduction%2520and%2520kernel%250Atransformations%2520improving%2520model%2520performance.%2520For%2520explainability%252C%2520patient%250Asimilarity%2520networks%2520are%2520constructed%2520from%2520these%2520metrics.%2520Spectral%2520clustering%2520and%250At-SNE%2520are%2520applied%2520to%2520identify%2520MDR-related%2520subgroups%2520and%2520visualize%2520high-risk%250Aclusters%252C%2520enabling%2520insight%2520into%2520clinically%2520relevant%2520patterns.%250A%2520%2520Results%253A%2520The%2520framework%2520was%2520validated%2520on%2520ICU%2520Electronic%2520Health%2520Records%2520from%250Athe%2520University%2520Hospital%2520of%2520Fuenlabrada%252C%2520achieving%2520an%2520AUC%2520of%252081%2525.%2520It%2520outperforms%250Abaseline%2520ML%2520and%2520deep%2520learning%2520models%2520by%2520leveraging%2520graph-based%2520patient%250Asimilarity.%2520The%2520approach%2520identifies%2520key%2520risk%2520factors%2520--%2520prolonged%2520antibiotic%250Ause%252C%2520invasive%2520procedures%252C%2520co-infections%252C%2520and%2520extended%2520ICU%2520stays%2520--%2520and%2520reveals%250Aclinically%2520meaningful%2520clusters.%2520Code%2520and%2520results%2520are%2520available%2520at%250A%255Chttps%253A//github.com/oscarescuderoarnanz/DM4MTS.%250A%2520%2520Conclusions%253A%2520Patient%2520similarity%2520representations%2520combined%2520with%2520graph-based%250Aanalysis%2520provide%2520accurate%2520MDR%2520prediction%2520and%2520interpretable%2520insights.%2520This%250Amethod%2520supports%2520early%2520detection%252C%2520risk%2520factor%2520identification%252C%2520and%2520patient%250Astratification%252C%2520highlighting%2520the%2520potential%2520of%2520explainable%2520ML%2520in%2520critical%2520care.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17717v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Early%20Detection%20of%20Multidrug%20Resistance%20Using%20Multivariate%20Time%20Series%0A%20%20Analysis%20and%20Interpretable%20Patient-Similarity%20Representations&entry.906535625=%C3%93scar%20Escudero-Arnanz%20and%20Antonio%20G.%20Marques%20and%20Inmaculada%20Mora-Jim%C3%A9nez%20and%20Joaqu%C3%ADn%20%C3%81lvarez-Rodr%C3%ADguez%20and%20Cristina%20Soguero-Ruiz&entry.1292438233=%20%20Background%20and%20Objectives%3A%20Multidrug%20Resistance%20%28MDR%29%20is%20a%20critical%20global%0Ahealth%20issue%2C%20causing%20increased%20hospital%20stays%2C%20healthcare%20costs%2C%20and%0Amortality.%20This%20study%20proposes%20an%20interpretable%20Machine%20Learning%20%28ML%29%20framework%0Afor%20MDR%20prediction%2C%20aiming%20for%20both%20accurate%20inference%20and%20enhanced%0Aexplainability.%0A%20%20Methods%3A%20Patients%20are%20modeled%20as%20Multivariate%20Time%20Series%20%28MTS%29%2C%20capturing%0Aclinical%20progression%20and%20patient-to-patient%20interactions.%20Similarity%20among%0Apatients%20is%20quantified%20using%20MTS-based%20methods%3A%20descriptive%20statistics%2C%20Dynamic%0ATime%20Warping%2C%20and%20Time%20Cluster%20Kernel.%20These%20similarity%20measures%20serve%20as%0Ainputs%20for%20MDR%20classification%20via%20Logistic%20Regression%2C%20Random%20Forest%2C%20and%0ASupport%20Vector%20Machines%2C%20with%20dimensionality%20reduction%20and%20kernel%0Atransformations%20improving%20model%20performance.%20For%20explainability%2C%20patient%0Asimilarity%20networks%20are%20constructed%20from%20these%20metrics.%20Spectral%20clustering%20and%0At-SNE%20are%20applied%20to%20identify%20MDR-related%20subgroups%20and%20visualize%20high-risk%0Aclusters%2C%20enabling%20insight%20into%20clinically%20relevant%20patterns.%0A%20%20Results%3A%20The%20framework%20was%20validated%20on%20ICU%20Electronic%20Health%20Records%20from%0Athe%20University%20Hospital%20of%20Fuenlabrada%2C%20achieving%20an%20AUC%20of%2081%25.%20It%20outperforms%0Abaseline%20ML%20and%20deep%20learning%20models%20by%20leveraging%20graph-based%20patient%0Asimilarity.%20The%20approach%20identifies%20key%20risk%20factors%20--%20prolonged%20antibiotic%0Ause%2C%20invasive%20procedures%2C%20co-infections%2C%20and%20extended%20ICU%20stays%20--%20and%20reveals%0Aclinically%20meaningful%20clusters.%20Code%20and%20results%20are%20available%20at%0A%5Chttps%3A//github.com/oscarescuderoarnanz/DM4MTS.%0A%20%20Conclusions%3A%20Patient%20similarity%20representations%20combined%20with%20graph-based%0Aanalysis%20provide%20accurate%20MDR%20prediction%20and%20interpretable%20insights.%20This%0Amethod%20supports%20early%20detection%2C%20risk%20factor%20identification%2C%20and%20patient%0Astratification%2C%20highlighting%20the%20potential%20of%20explainable%20ML%20in%20critical%20care.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17717v1&entry.124074799=Read"},
{"title": "On Multivariate Financial Time Series Classification", "author": "Gr\u00e9gory Bournassenko", "abstract": "  This article investigates the use of Machine Learning and Deep Learning\nmodels in multivariate time series analysis within financial markets. It\ncompares small and big data approaches, focusing on their distinct challenges\nand the benefits of scaling. Traditional methods such as SVMs are contrasted\nwith modern architectures like ConvTimeNet. The results show the importance of\nusing and understanding Big Data in depth in the analysis and prediction of\nfinancial time series.\n", "link": "http://arxiv.org/abs/2504.17664v1", "date": "2025-04-24", "relevancy": 1.3226, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4509}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4342}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Multivariate%20Financial%20Time%20Series%20Classification&body=Title%3A%20On%20Multivariate%20Financial%20Time%20Series%20Classification%0AAuthor%3A%20Gr%C3%A9gory%20Bournassenko%0AAbstract%3A%20%20%20This%20article%20investigates%20the%20use%20of%20Machine%20Learning%20and%20Deep%20Learning%0Amodels%20in%20multivariate%20time%20series%20analysis%20within%20financial%20markets.%20It%0Acompares%20small%20and%20big%20data%20approaches%2C%20focusing%20on%20their%20distinct%20challenges%0Aand%20the%20benefits%20of%20scaling.%20Traditional%20methods%20such%20as%20SVMs%20are%20contrasted%0Awith%20modern%20architectures%20like%20ConvTimeNet.%20The%20results%20show%20the%20importance%20of%0Ausing%20and%20understanding%20Big%20Data%20in%20depth%20in%20the%20analysis%20and%20prediction%20of%0Afinancial%20time%20series.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17664v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Multivariate%2520Financial%2520Time%2520Series%2520Classification%26entry.906535625%3DGr%25C3%25A9gory%2520Bournassenko%26entry.1292438233%3D%2520%2520This%2520article%2520investigates%2520the%2520use%2520of%2520Machine%2520Learning%2520and%2520Deep%2520Learning%250Amodels%2520in%2520multivariate%2520time%2520series%2520analysis%2520within%2520financial%2520markets.%2520It%250Acompares%2520small%2520and%2520big%2520data%2520approaches%252C%2520focusing%2520on%2520their%2520distinct%2520challenges%250Aand%2520the%2520benefits%2520of%2520scaling.%2520Traditional%2520methods%2520such%2520as%2520SVMs%2520are%2520contrasted%250Awith%2520modern%2520architectures%2520like%2520ConvTimeNet.%2520The%2520results%2520show%2520the%2520importance%2520of%250Ausing%2520and%2520understanding%2520Big%2520Data%2520in%2520depth%2520in%2520the%2520analysis%2520and%2520prediction%2520of%250Afinancial%2520time%2520series.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17664v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Multivariate%20Financial%20Time%20Series%20Classification&entry.906535625=Gr%C3%A9gory%20Bournassenko&entry.1292438233=%20%20This%20article%20investigates%20the%20use%20of%20Machine%20Learning%20and%20Deep%20Learning%0Amodels%20in%20multivariate%20time%20series%20analysis%20within%20financial%20markets.%20It%0Acompares%20small%20and%20big%20data%20approaches%2C%20focusing%20on%20their%20distinct%20challenges%0Aand%20the%20benefits%20of%20scaling.%20Traditional%20methods%20such%20as%20SVMs%20are%20contrasted%0Awith%20modern%20architectures%20like%20ConvTimeNet.%20The%20results%20show%20the%20importance%20of%0Ausing%20and%20understanding%20Big%20Data%20in%20depth%20in%20the%20analysis%20and%20prediction%20of%0Afinancial%20time%20series.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17664v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


