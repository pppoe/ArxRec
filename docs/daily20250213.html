<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250212.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Re$^3$Sim: Generating High-Fidelity Simulation Data via\n  3D-Photorealistic Real-to-Sim for Robotic Manipulation", "author": "Xiaoshen Han and Minghuan Liu and Yilun Chen and Junqiu Yu and Xiaoyang Lyu and Yang Tian and Bolun Wang and Weinan Zhang and Jiangmiao Pang", "abstract": "  Real-world data collection for robotics is costly and resource-intensive,\nrequiring skilled operators and expensive hardware. Simulations offer a\nscalable alternative but often fail to achieve sim-to-real generalization due\nto geometric and visual gaps. To address these challenges, we propose a\n3D-photorealistic real-to-sim system, namely, RE$^3$SIM, addressing geometric\nand visual sim-to-real gaps. RE$^3$SIM employs advanced 3D reconstruction and\nneural rendering techniques to faithfully recreate real-world scenarios,\nenabling real-time rendering of simulated cross-view cameras within a\nphysics-based simulator. By utilizing privileged information to collect expert\ndemonstrations efficiently in simulation, and train robot policies with\nimitation learning, we validate the effectiveness of the real-to-sim-to-real\npipeline across various manipulation task scenarios. Notably, with only\nsimulated data, we can achieve zero-shot sim-to-real transfer with an average\nsuccess rate exceeding 58%. To push the limit of real-to-sim, we further\ngenerate a large-scale simulation dataset, demonstrating how a robust policy\ncan be built from simulation data that generalizes across various objects.\nCodes and demos are available at: http://xshenhan.github.io/Re3Sim/.\n", "link": "http://arxiv.org/abs/2502.08645v1", "date": "2025-02-12", "relevancy": 3.085, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6175}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6168}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Re%24%5E3%24Sim%3A%20Generating%20High-Fidelity%20Simulation%20Data%20via%0A%20%203D-Photorealistic%20Real-to-Sim%20for%20Robotic%20Manipulation&body=Title%3A%20Re%24%5E3%24Sim%3A%20Generating%20High-Fidelity%20Simulation%20Data%20via%0A%20%203D-Photorealistic%20Real-to-Sim%20for%20Robotic%20Manipulation%0AAuthor%3A%20Xiaoshen%20Han%20and%20Minghuan%20Liu%20and%20Yilun%20Chen%20and%20Junqiu%20Yu%20and%20Xiaoyang%20Lyu%20and%20Yang%20Tian%20and%20Bolun%20Wang%20and%20Weinan%20Zhang%20and%20Jiangmiao%20Pang%0AAbstract%3A%20%20%20Real-world%20data%20collection%20for%20robotics%20is%20costly%20and%20resource-intensive%2C%0Arequiring%20skilled%20operators%20and%20expensive%20hardware.%20Simulations%20offer%20a%0Ascalable%20alternative%20but%20often%20fail%20to%20achieve%20sim-to-real%20generalization%20due%0Ato%20geometric%20and%20visual%20gaps.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0A3D-photorealistic%20real-to-sim%20system%2C%20namely%2C%20RE%24%5E3%24SIM%2C%20addressing%20geometric%0Aand%20visual%20sim-to-real%20gaps.%20RE%24%5E3%24SIM%20employs%20advanced%203D%20reconstruction%20and%0Aneural%20rendering%20techniques%20to%20faithfully%20recreate%20real-world%20scenarios%2C%0Aenabling%20real-time%20rendering%20of%20simulated%20cross-view%20cameras%20within%20a%0Aphysics-based%20simulator.%20By%20utilizing%20privileged%20information%20to%20collect%20expert%0Ademonstrations%20efficiently%20in%20simulation%2C%20and%20train%20robot%20policies%20with%0Aimitation%20learning%2C%20we%20validate%20the%20effectiveness%20of%20the%20real-to-sim-to-real%0Apipeline%20across%20various%20manipulation%20task%20scenarios.%20Notably%2C%20with%20only%0Asimulated%20data%2C%20we%20can%20achieve%20zero-shot%20sim-to-real%20transfer%20with%20an%20average%0Asuccess%20rate%20exceeding%2058%25.%20To%20push%20the%20limit%20of%20real-to-sim%2C%20we%20further%0Agenerate%20a%20large-scale%20simulation%20dataset%2C%20demonstrating%20how%20a%20robust%20policy%0Acan%20be%20built%20from%20simulation%20data%20that%20generalizes%20across%20various%20objects.%0ACodes%20and%20demos%20are%20available%20at%3A%20http%3A//xshenhan.github.io/Re3Sim/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08645v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRe%2524%255E3%2524Sim%253A%2520Generating%2520High-Fidelity%2520Simulation%2520Data%2520via%250A%2520%25203D-Photorealistic%2520Real-to-Sim%2520for%2520Robotic%2520Manipulation%26entry.906535625%3DXiaoshen%2520Han%2520and%2520Minghuan%2520Liu%2520and%2520Yilun%2520Chen%2520and%2520Junqiu%2520Yu%2520and%2520Xiaoyang%2520Lyu%2520and%2520Yang%2520Tian%2520and%2520Bolun%2520Wang%2520and%2520Weinan%2520Zhang%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3D%2520%2520Real-world%2520data%2520collection%2520for%2520robotics%2520is%2520costly%2520and%2520resource-intensive%252C%250Arequiring%2520skilled%2520operators%2520and%2520expensive%2520hardware.%2520Simulations%2520offer%2520a%250Ascalable%2520alternative%2520but%2520often%2520fail%2520to%2520achieve%2520sim-to-real%2520generalization%2520due%250Ato%2520geometric%2520and%2520visual%2520gaps.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%250A3D-photorealistic%2520real-to-sim%2520system%252C%2520namely%252C%2520RE%2524%255E3%2524SIM%252C%2520addressing%2520geometric%250Aand%2520visual%2520sim-to-real%2520gaps.%2520RE%2524%255E3%2524SIM%2520employs%2520advanced%25203D%2520reconstruction%2520and%250Aneural%2520rendering%2520techniques%2520to%2520faithfully%2520recreate%2520real-world%2520scenarios%252C%250Aenabling%2520real-time%2520rendering%2520of%2520simulated%2520cross-view%2520cameras%2520within%2520a%250Aphysics-based%2520simulator.%2520By%2520utilizing%2520privileged%2520information%2520to%2520collect%2520expert%250Ademonstrations%2520efficiently%2520in%2520simulation%252C%2520and%2520train%2520robot%2520policies%2520with%250Aimitation%2520learning%252C%2520we%2520validate%2520the%2520effectiveness%2520of%2520the%2520real-to-sim-to-real%250Apipeline%2520across%2520various%2520manipulation%2520task%2520scenarios.%2520Notably%252C%2520with%2520only%250Asimulated%2520data%252C%2520we%2520can%2520achieve%2520zero-shot%2520sim-to-real%2520transfer%2520with%2520an%2520average%250Asuccess%2520rate%2520exceeding%252058%2525.%2520To%2520push%2520the%2520limit%2520of%2520real-to-sim%252C%2520we%2520further%250Agenerate%2520a%2520large-scale%2520simulation%2520dataset%252C%2520demonstrating%2520how%2520a%2520robust%2520policy%250Acan%2520be%2520built%2520from%2520simulation%2520data%2520that%2520generalizes%2520across%2520various%2520objects.%250ACodes%2520and%2520demos%2520are%2520available%2520at%253A%2520http%253A//xshenhan.github.io/Re3Sim/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08645v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Re%24%5E3%24Sim%3A%20Generating%20High-Fidelity%20Simulation%20Data%20via%0A%20%203D-Photorealistic%20Real-to-Sim%20for%20Robotic%20Manipulation&entry.906535625=Xiaoshen%20Han%20and%20Minghuan%20Liu%20and%20Yilun%20Chen%20and%20Junqiu%20Yu%20and%20Xiaoyang%20Lyu%20and%20Yang%20Tian%20and%20Bolun%20Wang%20and%20Weinan%20Zhang%20and%20Jiangmiao%20Pang&entry.1292438233=%20%20Real-world%20data%20collection%20for%20robotics%20is%20costly%20and%20resource-intensive%2C%0Arequiring%20skilled%20operators%20and%20expensive%20hardware.%20Simulations%20offer%20a%0Ascalable%20alternative%20but%20often%20fail%20to%20achieve%20sim-to-real%20generalization%20due%0Ato%20geometric%20and%20visual%20gaps.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0A3D-photorealistic%20real-to-sim%20system%2C%20namely%2C%20RE%24%5E3%24SIM%2C%20addressing%20geometric%0Aand%20visual%20sim-to-real%20gaps.%20RE%24%5E3%24SIM%20employs%20advanced%203D%20reconstruction%20and%0Aneural%20rendering%20techniques%20to%20faithfully%20recreate%20real-world%20scenarios%2C%0Aenabling%20real-time%20rendering%20of%20simulated%20cross-view%20cameras%20within%20a%0Aphysics-based%20simulator.%20By%20utilizing%20privileged%20information%20to%20collect%20expert%0Ademonstrations%20efficiently%20in%20simulation%2C%20and%20train%20robot%20policies%20with%0Aimitation%20learning%2C%20we%20validate%20the%20effectiveness%20of%20the%20real-to-sim-to-real%0Apipeline%20across%20various%20manipulation%20task%20scenarios.%20Notably%2C%20with%20only%0Asimulated%20data%2C%20we%20can%20achieve%20zero-shot%20sim-to-real%20transfer%20with%20an%20average%0Asuccess%20rate%20exceeding%2058%25.%20To%20push%20the%20limit%20of%20real-to-sim%2C%20we%20further%0Agenerate%20a%20large-scale%20simulation%20dataset%2C%20demonstrating%20how%20a%20robust%20policy%0Acan%20be%20built%20from%20simulation%20data%20that%20generalizes%20across%20various%20objects.%0ACodes%20and%20demos%20are%20available%20at%3A%20http%3A//xshenhan.github.io/Re3Sim/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08645v1&entry.124074799=Read"},
{"title": "PulseCheck457: A Diagnostic Benchmark for Comprehensive Spatial\n  Reasoning of Large Multimodal Models", "author": "Xingrui Wang and Wufei Ma and Tiezheng Zhang and Celso M de Melo and Jieneng Chen and Alan Yuille", "abstract": "  Although large multimodal models (LMMs) have demonstrated remarkable\ncapabilities in visual scene interpretation and reasoning, their capacity for\ncomplex and precise 3-dimensional spatial reasoning remains uncertain. Existing\nbenchmarks focus predominantly on 2D spatial understanding and lack a framework\nto comprehensively evaluate 6D spatial reasoning across varying complexities.\nTo address this limitation, we present PulseCheck457, a scalable and unbiased\nsynthetic dataset designed with 4 key capability for spatial reasoning:\nmulti-object recognition, 2D location, 3D location, and 3D orientation. We\ndevelop a cascading evaluation structure, constructing 7 question types across\n5 difficulty levels that range from basic single object recognition to our new\nproposed complex 6D spatial reasoning tasks. We evaluated various large\nmultimodal models (LMMs) on PulseCheck457, observing a general decline in\nperformance as task complexity increases, particularly in 3D reasoning and 6D\nspatial tasks. To quantify these challenges, we introduce the Relative\nPerformance Dropping Rate (RPDR), highlighting key weaknesses in 3D reasoning\ncapabilities. Leveraging the unbiased attribute design of our dataset, we also\nuncover prediction biases across different attributes, with similar patterns\nobserved in real-world image settings.\n", "link": "http://arxiv.org/abs/2502.08636v1", "date": "2025-02-12", "relevancy": 3.0151, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6222}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6222}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PulseCheck457%3A%20A%20Diagnostic%20Benchmark%20for%20Comprehensive%20Spatial%0A%20%20Reasoning%20of%20Large%20Multimodal%20Models&body=Title%3A%20PulseCheck457%3A%20A%20Diagnostic%20Benchmark%20for%20Comprehensive%20Spatial%0A%20%20Reasoning%20of%20Large%20Multimodal%20Models%0AAuthor%3A%20Xingrui%20Wang%20and%20Wufei%20Ma%20and%20Tiezheng%20Zhang%20and%20Celso%20M%20de%20Melo%20and%20Jieneng%20Chen%20and%20Alan%20Yuille%0AAbstract%3A%20%20%20Although%20large%20multimodal%20models%20%28LMMs%29%20have%20demonstrated%20remarkable%0Acapabilities%20in%20visual%20scene%20interpretation%20and%20reasoning%2C%20their%20capacity%20for%0Acomplex%20and%20precise%203-dimensional%20spatial%20reasoning%20remains%20uncertain.%20Existing%0Abenchmarks%20focus%20predominantly%20on%202D%20spatial%20understanding%20and%20lack%20a%20framework%0Ato%20comprehensively%20evaluate%206D%20spatial%20reasoning%20across%20varying%20complexities.%0ATo%20address%20this%20limitation%2C%20we%20present%20PulseCheck457%2C%20a%20scalable%20and%20unbiased%0Asynthetic%20dataset%20designed%20with%204%20key%20capability%20for%20spatial%20reasoning%3A%0Amulti-object%20recognition%2C%202D%20location%2C%203D%20location%2C%20and%203D%20orientation.%20We%0Adevelop%20a%20cascading%20evaluation%20structure%2C%20constructing%207%20question%20types%20across%0A5%20difficulty%20levels%20that%20range%20from%20basic%20single%20object%20recognition%20to%20our%20new%0Aproposed%20complex%206D%20spatial%20reasoning%20tasks.%20We%20evaluated%20various%20large%0Amultimodal%20models%20%28LMMs%29%20on%20PulseCheck457%2C%20observing%20a%20general%20decline%20in%0Aperformance%20as%20task%20complexity%20increases%2C%20particularly%20in%203D%20reasoning%20and%206D%0Aspatial%20tasks.%20To%20quantify%20these%20challenges%2C%20we%20introduce%20the%20Relative%0APerformance%20Dropping%20Rate%20%28RPDR%29%2C%20highlighting%20key%20weaknesses%20in%203D%20reasoning%0Acapabilities.%20Leveraging%20the%20unbiased%20attribute%20design%20of%20our%20dataset%2C%20we%20also%0Auncover%20prediction%20biases%20across%20different%20attributes%2C%20with%20similar%20patterns%0Aobserved%20in%20real-world%20image%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08636v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPulseCheck457%253A%2520A%2520Diagnostic%2520Benchmark%2520for%2520Comprehensive%2520Spatial%250A%2520%2520Reasoning%2520of%2520Large%2520Multimodal%2520Models%26entry.906535625%3DXingrui%2520Wang%2520and%2520Wufei%2520Ma%2520and%2520Tiezheng%2520Zhang%2520and%2520Celso%2520M%2520de%2520Melo%2520and%2520Jieneng%2520Chen%2520and%2520Alan%2520Yuille%26entry.1292438233%3D%2520%2520Although%2520large%2520multimodal%2520models%2520%2528LMMs%2529%2520have%2520demonstrated%2520remarkable%250Acapabilities%2520in%2520visual%2520scene%2520interpretation%2520and%2520reasoning%252C%2520their%2520capacity%2520for%250Acomplex%2520and%2520precise%25203-dimensional%2520spatial%2520reasoning%2520remains%2520uncertain.%2520Existing%250Abenchmarks%2520focus%2520predominantly%2520on%25202D%2520spatial%2520understanding%2520and%2520lack%2520a%2520framework%250Ato%2520comprehensively%2520evaluate%25206D%2520spatial%2520reasoning%2520across%2520varying%2520complexities.%250ATo%2520address%2520this%2520limitation%252C%2520we%2520present%2520PulseCheck457%252C%2520a%2520scalable%2520and%2520unbiased%250Asynthetic%2520dataset%2520designed%2520with%25204%2520key%2520capability%2520for%2520spatial%2520reasoning%253A%250Amulti-object%2520recognition%252C%25202D%2520location%252C%25203D%2520location%252C%2520and%25203D%2520orientation.%2520We%250Adevelop%2520a%2520cascading%2520evaluation%2520structure%252C%2520constructing%25207%2520question%2520types%2520across%250A5%2520difficulty%2520levels%2520that%2520range%2520from%2520basic%2520single%2520object%2520recognition%2520to%2520our%2520new%250Aproposed%2520complex%25206D%2520spatial%2520reasoning%2520tasks.%2520We%2520evaluated%2520various%2520large%250Amultimodal%2520models%2520%2528LMMs%2529%2520on%2520PulseCheck457%252C%2520observing%2520a%2520general%2520decline%2520in%250Aperformance%2520as%2520task%2520complexity%2520increases%252C%2520particularly%2520in%25203D%2520reasoning%2520and%25206D%250Aspatial%2520tasks.%2520To%2520quantify%2520these%2520challenges%252C%2520we%2520introduce%2520the%2520Relative%250APerformance%2520Dropping%2520Rate%2520%2528RPDR%2529%252C%2520highlighting%2520key%2520weaknesses%2520in%25203D%2520reasoning%250Acapabilities.%2520Leveraging%2520the%2520unbiased%2520attribute%2520design%2520of%2520our%2520dataset%252C%2520we%2520also%250Auncover%2520prediction%2520biases%2520across%2520different%2520attributes%252C%2520with%2520similar%2520patterns%250Aobserved%2520in%2520real-world%2520image%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08636v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PulseCheck457%3A%20A%20Diagnostic%20Benchmark%20for%20Comprehensive%20Spatial%0A%20%20Reasoning%20of%20Large%20Multimodal%20Models&entry.906535625=Xingrui%20Wang%20and%20Wufei%20Ma%20and%20Tiezheng%20Zhang%20and%20Celso%20M%20de%20Melo%20and%20Jieneng%20Chen%20and%20Alan%20Yuille&entry.1292438233=%20%20Although%20large%20multimodal%20models%20%28LMMs%29%20have%20demonstrated%20remarkable%0Acapabilities%20in%20visual%20scene%20interpretation%20and%20reasoning%2C%20their%20capacity%20for%0Acomplex%20and%20precise%203-dimensional%20spatial%20reasoning%20remains%20uncertain.%20Existing%0Abenchmarks%20focus%20predominantly%20on%202D%20spatial%20understanding%20and%20lack%20a%20framework%0Ato%20comprehensively%20evaluate%206D%20spatial%20reasoning%20across%20varying%20complexities.%0ATo%20address%20this%20limitation%2C%20we%20present%20PulseCheck457%2C%20a%20scalable%20and%20unbiased%0Asynthetic%20dataset%20designed%20with%204%20key%20capability%20for%20spatial%20reasoning%3A%0Amulti-object%20recognition%2C%202D%20location%2C%203D%20location%2C%20and%203D%20orientation.%20We%0Adevelop%20a%20cascading%20evaluation%20structure%2C%20constructing%207%20question%20types%20across%0A5%20difficulty%20levels%20that%20range%20from%20basic%20single%20object%20recognition%20to%20our%20new%0Aproposed%20complex%206D%20spatial%20reasoning%20tasks.%20We%20evaluated%20various%20large%0Amultimodal%20models%20%28LMMs%29%20on%20PulseCheck457%2C%20observing%20a%20general%20decline%20in%0Aperformance%20as%20task%20complexity%20increases%2C%20particularly%20in%203D%20reasoning%20and%206D%0Aspatial%20tasks.%20To%20quantify%20these%20challenges%2C%20we%20introduce%20the%20Relative%0APerformance%20Dropping%20Rate%20%28RPDR%29%2C%20highlighting%20key%20weaknesses%20in%203D%20reasoning%0Acapabilities.%20Leveraging%20the%20unbiased%20attribute%20design%20of%20our%20dataset%2C%20we%20also%0Auncover%20prediction%20biases%20across%20different%20attributes%2C%20with%20similar%20patterns%0Aobserved%20in%20real-world%20image%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08636v1&entry.124074799=Read"},
{"title": "Human-Centric Foundation Models: Perception, Generation and Agentic\n  Modeling", "author": "Shixiang Tang and Yizhou Wang and Lu Chen and Yuan Wang and Sida Peng and Dan Xu and Wanli Ouyang", "abstract": "  Human understanding and generation are critical for modeling digital humans\nand humanoid embodiments. Recently, Human-centric Foundation Models (HcFMs)\ninspired by the success of generalist models, such as large language and vision\nmodels, have emerged to unify diverse human-centric tasks into a single\nframework, surpassing traditional task-specific approaches. In this survey, we\npresent a comprehensive overview of HcFMs by proposing a taxonomy that\ncategorizes current approaches into four groups: (1) Human-centric Perception\nFoundation Models that capture fine-grained features for multi-modal 2D and 3D\nunderstanding. (2) Human-centric AIGC Foundation Models that generate\nhigh-fidelity, diverse human-related content. (3) Unified Perception and\nGeneration Models that integrate these capabilities to enhance both human\nunderstanding and synthesis. (4) Human-centric Agentic Foundation Models that\nextend beyond perception and generation to learn human-like intelligence and\ninteractive behaviors for humanoid embodied tasks. We review state-of-the-art\ntechniques, discuss emerging challenges and future research directions. This\nsurvey aims to serve as a roadmap for researchers and practitioners working\ntowards more robust, versatile, and intelligent digital human and embodiments\nmodeling.\n", "link": "http://arxiv.org/abs/2502.08556v1", "date": "2025-02-12", "relevancy": 3.0099, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6047}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6007}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-Centric%20Foundation%20Models%3A%20Perception%2C%20Generation%20and%20Agentic%0A%20%20Modeling&body=Title%3A%20Human-Centric%20Foundation%20Models%3A%20Perception%2C%20Generation%20and%20Agentic%0A%20%20Modeling%0AAuthor%3A%20Shixiang%20Tang%20and%20Yizhou%20Wang%20and%20Lu%20Chen%20and%20Yuan%20Wang%20and%20Sida%20Peng%20and%20Dan%20Xu%20and%20Wanli%20Ouyang%0AAbstract%3A%20%20%20Human%20understanding%20and%20generation%20are%20critical%20for%20modeling%20digital%20humans%0Aand%20humanoid%20embodiments.%20Recently%2C%20Human-centric%20Foundation%20Models%20%28HcFMs%29%0Ainspired%20by%20the%20success%20of%20generalist%20models%2C%20such%20as%20large%20language%20and%20vision%0Amodels%2C%20have%20emerged%20to%20unify%20diverse%20human-centric%20tasks%20into%20a%20single%0Aframework%2C%20surpassing%20traditional%20task-specific%20approaches.%20In%20this%20survey%2C%20we%0Apresent%20a%20comprehensive%20overview%20of%20HcFMs%20by%20proposing%20a%20taxonomy%20that%0Acategorizes%20current%20approaches%20into%20four%20groups%3A%20%281%29%20Human-centric%20Perception%0AFoundation%20Models%20that%20capture%20fine-grained%20features%20for%20multi-modal%202D%20and%203D%0Aunderstanding.%20%282%29%20Human-centric%20AIGC%20Foundation%20Models%20that%20generate%0Ahigh-fidelity%2C%20diverse%20human-related%20content.%20%283%29%20Unified%20Perception%20and%0AGeneration%20Models%20that%20integrate%20these%20capabilities%20to%20enhance%20both%20human%0Aunderstanding%20and%20synthesis.%20%284%29%20Human-centric%20Agentic%20Foundation%20Models%20that%0Aextend%20beyond%20perception%20and%20generation%20to%20learn%20human-like%20intelligence%20and%0Ainteractive%20behaviors%20for%20humanoid%20embodied%20tasks.%20We%20review%20state-of-the-art%0Atechniques%2C%20discuss%20emerging%20challenges%20and%20future%20research%20directions.%20This%0Asurvey%20aims%20to%20serve%20as%20a%20roadmap%20for%20researchers%20and%20practitioners%20working%0Atowards%20more%20robust%2C%20versatile%2C%20and%20intelligent%20digital%20human%20and%20embodiments%0Amodeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08556v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-Centric%2520Foundation%2520Models%253A%2520Perception%252C%2520Generation%2520and%2520Agentic%250A%2520%2520Modeling%26entry.906535625%3DShixiang%2520Tang%2520and%2520Yizhou%2520Wang%2520and%2520Lu%2520Chen%2520and%2520Yuan%2520Wang%2520and%2520Sida%2520Peng%2520and%2520Dan%2520Xu%2520and%2520Wanli%2520Ouyang%26entry.1292438233%3D%2520%2520Human%2520understanding%2520and%2520generation%2520are%2520critical%2520for%2520modeling%2520digital%2520humans%250Aand%2520humanoid%2520embodiments.%2520Recently%252C%2520Human-centric%2520Foundation%2520Models%2520%2528HcFMs%2529%250Ainspired%2520by%2520the%2520success%2520of%2520generalist%2520models%252C%2520such%2520as%2520large%2520language%2520and%2520vision%250Amodels%252C%2520have%2520emerged%2520to%2520unify%2520diverse%2520human-centric%2520tasks%2520into%2520a%2520single%250Aframework%252C%2520surpassing%2520traditional%2520task-specific%2520approaches.%2520In%2520this%2520survey%252C%2520we%250Apresent%2520a%2520comprehensive%2520overview%2520of%2520HcFMs%2520by%2520proposing%2520a%2520taxonomy%2520that%250Acategorizes%2520current%2520approaches%2520into%2520four%2520groups%253A%2520%25281%2529%2520Human-centric%2520Perception%250AFoundation%2520Models%2520that%2520capture%2520fine-grained%2520features%2520for%2520multi-modal%25202D%2520and%25203D%250Aunderstanding.%2520%25282%2529%2520Human-centric%2520AIGC%2520Foundation%2520Models%2520that%2520generate%250Ahigh-fidelity%252C%2520diverse%2520human-related%2520content.%2520%25283%2529%2520Unified%2520Perception%2520and%250AGeneration%2520Models%2520that%2520integrate%2520these%2520capabilities%2520to%2520enhance%2520both%2520human%250Aunderstanding%2520and%2520synthesis.%2520%25284%2529%2520Human-centric%2520Agentic%2520Foundation%2520Models%2520that%250Aextend%2520beyond%2520perception%2520and%2520generation%2520to%2520learn%2520human-like%2520intelligence%2520and%250Ainteractive%2520behaviors%2520for%2520humanoid%2520embodied%2520tasks.%2520We%2520review%2520state-of-the-art%250Atechniques%252C%2520discuss%2520emerging%2520challenges%2520and%2520future%2520research%2520directions.%2520This%250Asurvey%2520aims%2520to%2520serve%2520as%2520a%2520roadmap%2520for%2520researchers%2520and%2520practitioners%2520working%250Atowards%2520more%2520robust%252C%2520versatile%252C%2520and%2520intelligent%2520digital%2520human%2520and%2520embodiments%250Amodeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08556v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-Centric%20Foundation%20Models%3A%20Perception%2C%20Generation%20and%20Agentic%0A%20%20Modeling&entry.906535625=Shixiang%20Tang%20and%20Yizhou%20Wang%20and%20Lu%20Chen%20and%20Yuan%20Wang%20and%20Sida%20Peng%20and%20Dan%20Xu%20and%20Wanli%20Ouyang&entry.1292438233=%20%20Human%20understanding%20and%20generation%20are%20critical%20for%20modeling%20digital%20humans%0Aand%20humanoid%20embodiments.%20Recently%2C%20Human-centric%20Foundation%20Models%20%28HcFMs%29%0Ainspired%20by%20the%20success%20of%20generalist%20models%2C%20such%20as%20large%20language%20and%20vision%0Amodels%2C%20have%20emerged%20to%20unify%20diverse%20human-centric%20tasks%20into%20a%20single%0Aframework%2C%20surpassing%20traditional%20task-specific%20approaches.%20In%20this%20survey%2C%20we%0Apresent%20a%20comprehensive%20overview%20of%20HcFMs%20by%20proposing%20a%20taxonomy%20that%0Acategorizes%20current%20approaches%20into%20four%20groups%3A%20%281%29%20Human-centric%20Perception%0AFoundation%20Models%20that%20capture%20fine-grained%20features%20for%20multi-modal%202D%20and%203D%0Aunderstanding.%20%282%29%20Human-centric%20AIGC%20Foundation%20Models%20that%20generate%0Ahigh-fidelity%2C%20diverse%20human-related%20content.%20%283%29%20Unified%20Perception%20and%0AGeneration%20Models%20that%20integrate%20these%20capabilities%20to%20enhance%20both%20human%0Aunderstanding%20and%20synthesis.%20%284%29%20Human-centric%20Agentic%20Foundation%20Models%20that%0Aextend%20beyond%20perception%20and%20generation%20to%20learn%20human-like%20intelligence%20and%0Ainteractive%20behaviors%20for%20humanoid%20embodied%20tasks.%20We%20review%20state-of-the-art%0Atechniques%2C%20discuss%20emerging%20challenges%20and%20future%20research%20directions.%20This%0Asurvey%20aims%20to%20serve%20as%20a%20roadmap%20for%20researchers%20and%20practitioners%20working%0Atowards%20more%20robust%2C%20versatile%2C%20and%20intelligent%20digital%20human%20and%20embodiments%0Amodeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08556v1&entry.124074799=Read"},
{"title": "Revisiting 3D LLM Benchmarks: Are We Really Testing 3D Capabilities?", "author": "Jiahe Jin and Yanheng He and Mingyan Yang", "abstract": "  In this work, we identify the \"2D-Cheating\" problem in 3D LLM evaluation,\nwhere these tasks might be easily solved by VLMs with rendered images of point\nclouds, exposing ineffective evaluation of 3D LLMs' unique 3D capabilities. We\ntest VLM performance across multiple 3D LLM benchmarks and, using this as a\nreference, propose principles for better assessing genuine 3D understanding. We\nalso advocate explicitly separating 3D abilities from 1D or 2D aspects when\nevaluating 3D LLMs.\n", "link": "http://arxiv.org/abs/2502.08503v1", "date": "2025-02-12", "relevancy": 2.7904, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5778}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5778}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%203D%20LLM%20Benchmarks%3A%20Are%20We%20Really%20Testing%203D%20Capabilities%3F&body=Title%3A%20Revisiting%203D%20LLM%20Benchmarks%3A%20Are%20We%20Really%20Testing%203D%20Capabilities%3F%0AAuthor%3A%20Jiahe%20Jin%20and%20Yanheng%20He%20and%20Mingyan%20Yang%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20identify%20the%20%222D-Cheating%22%20problem%20in%203D%20LLM%20evaluation%2C%0Awhere%20these%20tasks%20might%20be%20easily%20solved%20by%20VLMs%20with%20rendered%20images%20of%20point%0Aclouds%2C%20exposing%20ineffective%20evaluation%20of%203D%20LLMs%27%20unique%203D%20capabilities.%20We%0Atest%20VLM%20performance%20across%20multiple%203D%20LLM%20benchmarks%20and%2C%20using%20this%20as%20a%0Areference%2C%20propose%20principles%20for%20better%20assessing%20genuine%203D%20understanding.%20We%0Aalso%20advocate%20explicitly%20separating%203D%20abilities%20from%201D%20or%202D%20aspects%20when%0Aevaluating%203D%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08503v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%25203D%2520LLM%2520Benchmarks%253A%2520Are%2520We%2520Really%2520Testing%25203D%2520Capabilities%253F%26entry.906535625%3DJiahe%2520Jin%2520and%2520Yanheng%2520He%2520and%2520Mingyan%2520Yang%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520identify%2520the%2520%25222D-Cheating%2522%2520problem%2520in%25203D%2520LLM%2520evaluation%252C%250Awhere%2520these%2520tasks%2520might%2520be%2520easily%2520solved%2520by%2520VLMs%2520with%2520rendered%2520images%2520of%2520point%250Aclouds%252C%2520exposing%2520ineffective%2520evaluation%2520of%25203D%2520LLMs%2527%2520unique%25203D%2520capabilities.%2520We%250Atest%2520VLM%2520performance%2520across%2520multiple%25203D%2520LLM%2520benchmarks%2520and%252C%2520using%2520this%2520as%2520a%250Areference%252C%2520propose%2520principles%2520for%2520better%2520assessing%2520genuine%25203D%2520understanding.%2520We%250Aalso%2520advocate%2520explicitly%2520separating%25203D%2520abilities%2520from%25201D%2520or%25202D%2520aspects%2520when%250Aevaluating%25203D%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08503v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%203D%20LLM%20Benchmarks%3A%20Are%20We%20Really%20Testing%203D%20Capabilities%3F&entry.906535625=Jiahe%20Jin%20and%20Yanheng%20He%20and%20Mingyan%20Yang&entry.1292438233=%20%20In%20this%20work%2C%20we%20identify%20the%20%222D-Cheating%22%20problem%20in%203D%20LLM%20evaluation%2C%0Awhere%20these%20tasks%20might%20be%20easily%20solved%20by%20VLMs%20with%20rendered%20images%20of%20point%0Aclouds%2C%20exposing%20ineffective%20evaluation%20of%203D%20LLMs%27%20unique%203D%20capabilities.%20We%0Atest%20VLM%20performance%20across%20multiple%203D%20LLM%20benchmarks%20and%2C%20using%20this%20as%20a%0Areference%2C%20propose%20principles%20for%20better%20assessing%20genuine%203D%20understanding.%20We%0Aalso%20advocate%20explicitly%20separating%203D%20abilities%20from%201D%20or%202D%20aspects%20when%0Aevaluating%203D%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08503v1&entry.124074799=Read"},
{"title": "Rapid Whole Brain Mesoscale In-vivo MR Imaging using Multi-scale\n  Implicit Neural Representation", "author": "Jun Lyu and Lipeng Ning and William Consagra and Qiang Liu and Richard J. Rushmore and Berkin Bilgic and Yogesh Rathi", "abstract": "  Purpose: To develop and validate a novel image reconstruction technique using\nimplicit neural representations (INR) for multi-view thick-slice acquisitions\nwhile reducing the scan time but maintaining high signal-to-noise ratio (SNR).\nMethods: We propose Rotating-view super-resolution (ROVER)-MRI, an unsupervised\nneural network-based algorithm designed to reconstruct MRI data from multi-view\nthick slices, effectively reducing scan time by 2-fold while maintaining fine\nanatomical details. We compare our method to both bicubic interpolation and the\ncurrent state-of-the-art regularized least-squares super-resolution\nreconstruction (LS-SRR) technique. Validation is performed using ground-truth\nex-vivo monkey brain data, and we demonstrate superior reconstruction quality\nacross several in-vivo human datasets. Notably, we achieve the reconstruction\nof a whole human brain in-vivo T2-weighted image with an unprecedented\n180{\\mu}m isotropic spatial resolution, accomplished in just 17 minutes of scan\ntime on a 7T MRI scanner. Results: ROVER-MRI outperformed LS-SRR method in\nterms of reconstruction quality with 22.4% lower relative error (RE) and 7.5%\nlower full-width half maximum (FWHM) indicating better preservation of fine\nstructural details in nearly half the scan time. Conclusion: ROVER-MRI offers\nan efficient and robust approach for mesoscale MR imaging, enabling rapid,\nhigh-resolution whole-brain scans. Its versatility holds great promise for\nresearch applications requiring anatomical details and time-efficient imaging.\n", "link": "http://arxiv.org/abs/2502.08634v1", "date": "2025-02-12", "relevancy": 2.7777, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5762}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5452}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rapid%20Whole%20Brain%20Mesoscale%20In-vivo%20MR%20Imaging%20using%20Multi-scale%0A%20%20Implicit%20Neural%20Representation&body=Title%3A%20Rapid%20Whole%20Brain%20Mesoscale%20In-vivo%20MR%20Imaging%20using%20Multi-scale%0A%20%20Implicit%20Neural%20Representation%0AAuthor%3A%20Jun%20Lyu%20and%20Lipeng%20Ning%20and%20William%20Consagra%20and%20Qiang%20Liu%20and%20Richard%20J.%20Rushmore%20and%20Berkin%20Bilgic%20and%20Yogesh%20Rathi%0AAbstract%3A%20%20%20Purpose%3A%20To%20develop%20and%20validate%20a%20novel%20image%20reconstruction%20technique%20using%0Aimplicit%20neural%20representations%20%28INR%29%20for%20multi-view%20thick-slice%20acquisitions%0Awhile%20reducing%20the%20scan%20time%20but%20maintaining%20high%20signal-to-noise%20ratio%20%28SNR%29.%0AMethods%3A%20We%20propose%20Rotating-view%20super-resolution%20%28ROVER%29-MRI%2C%20an%20unsupervised%0Aneural%20network-based%20algorithm%20designed%20to%20reconstruct%20MRI%20data%20from%20multi-view%0Athick%20slices%2C%20effectively%20reducing%20scan%20time%20by%202-fold%20while%20maintaining%20fine%0Aanatomical%20details.%20We%20compare%20our%20method%20to%20both%20bicubic%20interpolation%20and%20the%0Acurrent%20state-of-the-art%20regularized%20least-squares%20super-resolution%0Areconstruction%20%28LS-SRR%29%20technique.%20Validation%20is%20performed%20using%20ground-truth%0Aex-vivo%20monkey%20brain%20data%2C%20and%20we%20demonstrate%20superior%20reconstruction%20quality%0Aacross%20several%20in-vivo%20human%20datasets.%20Notably%2C%20we%20achieve%20the%20reconstruction%0Aof%20a%20whole%20human%20brain%20in-vivo%20T2-weighted%20image%20with%20an%20unprecedented%0A180%7B%5Cmu%7Dm%20isotropic%20spatial%20resolution%2C%20accomplished%20in%20just%2017%20minutes%20of%20scan%0Atime%20on%20a%207T%20MRI%20scanner.%20Results%3A%20ROVER-MRI%20outperformed%20LS-SRR%20method%20in%0Aterms%20of%20reconstruction%20quality%20with%2022.4%25%20lower%20relative%20error%20%28RE%29%20and%207.5%25%0Alower%20full-width%20half%20maximum%20%28FWHM%29%20indicating%20better%20preservation%20of%20fine%0Astructural%20details%20in%20nearly%20half%20the%20scan%20time.%20Conclusion%3A%20ROVER-MRI%20offers%0Aan%20efficient%20and%20robust%20approach%20for%20mesoscale%20MR%20imaging%2C%20enabling%20rapid%2C%0Ahigh-resolution%20whole-brain%20scans.%20Its%20versatility%20holds%20great%20promise%20for%0Aresearch%20applications%20requiring%20anatomical%20details%20and%20time-efficient%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08634v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRapid%2520Whole%2520Brain%2520Mesoscale%2520In-vivo%2520MR%2520Imaging%2520using%2520Multi-scale%250A%2520%2520Implicit%2520Neural%2520Representation%26entry.906535625%3DJun%2520Lyu%2520and%2520Lipeng%2520Ning%2520and%2520William%2520Consagra%2520and%2520Qiang%2520Liu%2520and%2520Richard%2520J.%2520Rushmore%2520and%2520Berkin%2520Bilgic%2520and%2520Yogesh%2520Rathi%26entry.1292438233%3D%2520%2520Purpose%253A%2520To%2520develop%2520and%2520validate%2520a%2520novel%2520image%2520reconstruction%2520technique%2520using%250Aimplicit%2520neural%2520representations%2520%2528INR%2529%2520for%2520multi-view%2520thick-slice%2520acquisitions%250Awhile%2520reducing%2520the%2520scan%2520time%2520but%2520maintaining%2520high%2520signal-to-noise%2520ratio%2520%2528SNR%2529.%250AMethods%253A%2520We%2520propose%2520Rotating-view%2520super-resolution%2520%2528ROVER%2529-MRI%252C%2520an%2520unsupervised%250Aneural%2520network-based%2520algorithm%2520designed%2520to%2520reconstruct%2520MRI%2520data%2520from%2520multi-view%250Athick%2520slices%252C%2520effectively%2520reducing%2520scan%2520time%2520by%25202-fold%2520while%2520maintaining%2520fine%250Aanatomical%2520details.%2520We%2520compare%2520our%2520method%2520to%2520both%2520bicubic%2520interpolation%2520and%2520the%250Acurrent%2520state-of-the-art%2520regularized%2520least-squares%2520super-resolution%250Areconstruction%2520%2528LS-SRR%2529%2520technique.%2520Validation%2520is%2520performed%2520using%2520ground-truth%250Aex-vivo%2520monkey%2520brain%2520data%252C%2520and%2520we%2520demonstrate%2520superior%2520reconstruction%2520quality%250Aacross%2520several%2520in-vivo%2520human%2520datasets.%2520Notably%252C%2520we%2520achieve%2520the%2520reconstruction%250Aof%2520a%2520whole%2520human%2520brain%2520in-vivo%2520T2-weighted%2520image%2520with%2520an%2520unprecedented%250A180%257B%255Cmu%257Dm%2520isotropic%2520spatial%2520resolution%252C%2520accomplished%2520in%2520just%252017%2520minutes%2520of%2520scan%250Atime%2520on%2520a%25207T%2520MRI%2520scanner.%2520Results%253A%2520ROVER-MRI%2520outperformed%2520LS-SRR%2520method%2520in%250Aterms%2520of%2520reconstruction%2520quality%2520with%252022.4%2525%2520lower%2520relative%2520error%2520%2528RE%2529%2520and%25207.5%2525%250Alower%2520full-width%2520half%2520maximum%2520%2528FWHM%2529%2520indicating%2520better%2520preservation%2520of%2520fine%250Astructural%2520details%2520in%2520nearly%2520half%2520the%2520scan%2520time.%2520Conclusion%253A%2520ROVER-MRI%2520offers%250Aan%2520efficient%2520and%2520robust%2520approach%2520for%2520mesoscale%2520MR%2520imaging%252C%2520enabling%2520rapid%252C%250Ahigh-resolution%2520whole-brain%2520scans.%2520Its%2520versatility%2520holds%2520great%2520promise%2520for%250Aresearch%2520applications%2520requiring%2520anatomical%2520details%2520and%2520time-efficient%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08634v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rapid%20Whole%20Brain%20Mesoscale%20In-vivo%20MR%20Imaging%20using%20Multi-scale%0A%20%20Implicit%20Neural%20Representation&entry.906535625=Jun%20Lyu%20and%20Lipeng%20Ning%20and%20William%20Consagra%20and%20Qiang%20Liu%20and%20Richard%20J.%20Rushmore%20and%20Berkin%20Bilgic%20and%20Yogesh%20Rathi&entry.1292438233=%20%20Purpose%3A%20To%20develop%20and%20validate%20a%20novel%20image%20reconstruction%20technique%20using%0Aimplicit%20neural%20representations%20%28INR%29%20for%20multi-view%20thick-slice%20acquisitions%0Awhile%20reducing%20the%20scan%20time%20but%20maintaining%20high%20signal-to-noise%20ratio%20%28SNR%29.%0AMethods%3A%20We%20propose%20Rotating-view%20super-resolution%20%28ROVER%29-MRI%2C%20an%20unsupervised%0Aneural%20network-based%20algorithm%20designed%20to%20reconstruct%20MRI%20data%20from%20multi-view%0Athick%20slices%2C%20effectively%20reducing%20scan%20time%20by%202-fold%20while%20maintaining%20fine%0Aanatomical%20details.%20We%20compare%20our%20method%20to%20both%20bicubic%20interpolation%20and%20the%0Acurrent%20state-of-the-art%20regularized%20least-squares%20super-resolution%0Areconstruction%20%28LS-SRR%29%20technique.%20Validation%20is%20performed%20using%20ground-truth%0Aex-vivo%20monkey%20brain%20data%2C%20and%20we%20demonstrate%20superior%20reconstruction%20quality%0Aacross%20several%20in-vivo%20human%20datasets.%20Notably%2C%20we%20achieve%20the%20reconstruction%0Aof%20a%20whole%20human%20brain%20in-vivo%20T2-weighted%20image%20with%20an%20unprecedented%0A180%7B%5Cmu%7Dm%20isotropic%20spatial%20resolution%2C%20accomplished%20in%20just%2017%20minutes%20of%20scan%0Atime%20on%20a%207T%20MRI%20scanner.%20Results%3A%20ROVER-MRI%20outperformed%20LS-SRR%20method%20in%0Aterms%20of%20reconstruction%20quality%20with%2022.4%25%20lower%20relative%20error%20%28RE%29%20and%207.5%25%0Alower%20full-width%20half%20maximum%20%28FWHM%29%20indicating%20better%20preservation%20of%20fine%0Astructural%20details%20in%20nearly%20half%20the%20scan%20time.%20Conclusion%3A%20ROVER-MRI%20offers%0Aan%20efficient%20and%20robust%20approach%20for%20mesoscale%20MR%20imaging%2C%20enabling%20rapid%2C%0Ahigh-resolution%20whole-brain%20scans.%20Its%20versatility%20holds%20great%20promise%20for%0Aresearch%20applications%20requiring%20anatomical%20details%20and%20time-efficient%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08634v1&entry.124074799=Read"},
{"title": "Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive\n  Modality Alignment", "author": "Zuyan Liu and Yuhao Dong and Jiahui Wang and Ziwei Liu and Winston Hu and Jiwen Lu and Yongming Rao", "abstract": "  Recent advances in large language models, particularly following GPT-4o, have\nsparked increasing interest in developing omni-modal models capable of\nunderstanding more modalities. While some open-source alternatives have\nemerged, there is still a notable lag behind specialized single-modality models\nin performance. In this paper, we present Ola, an Omni-modal language model\nthat achieves competitive performance across image, video, and audio\nunderstanding compared to specialized counterparts. The core design of Ola lies\nin its progressive modality alignment strategy that extends the supporting\nmodality of the language model progressively. Our training pipeline begins with\nthe most distinct modalities: image and text, then gradually expands the skill\nsets of the model using speech data that connects language and audio knowledge,\nand video data that connects all modalities. The progressive learning pipeline\nalso enables us to maintain a relatively small size of the cross-modal\nalignment data, making developing omni-modal from existing vision-language\nmodels easy and less costly. Moreover, to unlock an advanced interactive\nexperience like GPT-4o, we further design a sentence-wise decoding solution for\nstreaming speech generation. Extensive experiments demonstrate that Ola\nsurpasses existing open omni-modal LLMs across all modalities while achieving\nhighly competitive performance compared to state-of-the-art specialized models\nof similar sizes. We aim to make Ola a fully open omni-modal understanding\nsolution to advance future research in this emerging field. Model weights,\ncode, and data are open-sourced at https://github.com/Ola-Omni/Ola.\n", "link": "http://arxiv.org/abs/2502.04328v2", "date": "2025-02-12", "relevancy": 2.7539, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5516}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5516}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ola%3A%20Pushing%20the%20Frontiers%20of%20Omni-Modal%20Language%20Model%20with%20Progressive%0A%20%20Modality%20Alignment&body=Title%3A%20Ola%3A%20Pushing%20the%20Frontiers%20of%20Omni-Modal%20Language%20Model%20with%20Progressive%0A%20%20Modality%20Alignment%0AAuthor%3A%20Zuyan%20Liu%20and%20Yuhao%20Dong%20and%20Jiahui%20Wang%20and%20Ziwei%20Liu%20and%20Winston%20Hu%20and%20Jiwen%20Lu%20and%20Yongming%20Rao%0AAbstract%3A%20%20%20Recent%20advances%20in%20large%20language%20models%2C%20particularly%20following%20GPT-4o%2C%20have%0Asparked%20increasing%20interest%20in%20developing%20omni-modal%20models%20capable%20of%0Aunderstanding%20more%20modalities.%20While%20some%20open-source%20alternatives%20have%0Aemerged%2C%20there%20is%20still%20a%20notable%20lag%20behind%20specialized%20single-modality%20models%0Ain%20performance.%20In%20this%20paper%2C%20we%20present%20Ola%2C%20an%20Omni-modal%20language%20model%0Athat%20achieves%20competitive%20performance%20across%20image%2C%20video%2C%20and%20audio%0Aunderstanding%20compared%20to%20specialized%20counterparts.%20The%20core%20design%20of%20Ola%20lies%0Ain%20its%20progressive%20modality%20alignment%20strategy%20that%20extends%20the%20supporting%0Amodality%20of%20the%20language%20model%20progressively.%20Our%20training%20pipeline%20begins%20with%0Athe%20most%20distinct%20modalities%3A%20image%20and%20text%2C%20then%20gradually%20expands%20the%20skill%0Asets%20of%20the%20model%20using%20speech%20data%20that%20connects%20language%20and%20audio%20knowledge%2C%0Aand%20video%20data%20that%20connects%20all%20modalities.%20The%20progressive%20learning%20pipeline%0Aalso%20enables%20us%20to%20maintain%20a%20relatively%20small%20size%20of%20the%20cross-modal%0Aalignment%20data%2C%20making%20developing%20omni-modal%20from%20existing%20vision-language%0Amodels%20easy%20and%20less%20costly.%20Moreover%2C%20to%20unlock%20an%20advanced%20interactive%0Aexperience%20like%20GPT-4o%2C%20we%20further%20design%20a%20sentence-wise%20decoding%20solution%20for%0Astreaming%20speech%20generation.%20Extensive%20experiments%20demonstrate%20that%20Ola%0Asurpasses%20existing%20open%20omni-modal%20LLMs%20across%20all%20modalities%20while%20achieving%0Ahighly%20competitive%20performance%20compared%20to%20state-of-the-art%20specialized%20models%0Aof%20similar%20sizes.%20We%20aim%20to%20make%20Ola%20a%20fully%20open%20omni-modal%20understanding%0Asolution%20to%20advance%20future%20research%20in%20this%20emerging%20field.%20Model%20weights%2C%0Acode%2C%20and%20data%20are%20open-sourced%20at%20https%3A//github.com/Ola-Omni/Ola.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04328v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOla%253A%2520Pushing%2520the%2520Frontiers%2520of%2520Omni-Modal%2520Language%2520Model%2520with%2520Progressive%250A%2520%2520Modality%2520Alignment%26entry.906535625%3DZuyan%2520Liu%2520and%2520Yuhao%2520Dong%2520and%2520Jiahui%2520Wang%2520and%2520Ziwei%2520Liu%2520and%2520Winston%2520Hu%2520and%2520Jiwen%2520Lu%2520and%2520Yongming%2520Rao%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520large%2520language%2520models%252C%2520particularly%2520following%2520GPT-4o%252C%2520have%250Asparked%2520increasing%2520interest%2520in%2520developing%2520omni-modal%2520models%2520capable%2520of%250Aunderstanding%2520more%2520modalities.%2520While%2520some%2520open-source%2520alternatives%2520have%250Aemerged%252C%2520there%2520is%2520still%2520a%2520notable%2520lag%2520behind%2520specialized%2520single-modality%2520models%250Ain%2520performance.%2520In%2520this%2520paper%252C%2520we%2520present%2520Ola%252C%2520an%2520Omni-modal%2520language%2520model%250Athat%2520achieves%2520competitive%2520performance%2520across%2520image%252C%2520video%252C%2520and%2520audio%250Aunderstanding%2520compared%2520to%2520specialized%2520counterparts.%2520The%2520core%2520design%2520of%2520Ola%2520lies%250Ain%2520its%2520progressive%2520modality%2520alignment%2520strategy%2520that%2520extends%2520the%2520supporting%250Amodality%2520of%2520the%2520language%2520model%2520progressively.%2520Our%2520training%2520pipeline%2520begins%2520with%250Athe%2520most%2520distinct%2520modalities%253A%2520image%2520and%2520text%252C%2520then%2520gradually%2520expands%2520the%2520skill%250Asets%2520of%2520the%2520model%2520using%2520speech%2520data%2520that%2520connects%2520language%2520and%2520audio%2520knowledge%252C%250Aand%2520video%2520data%2520that%2520connects%2520all%2520modalities.%2520The%2520progressive%2520learning%2520pipeline%250Aalso%2520enables%2520us%2520to%2520maintain%2520a%2520relatively%2520small%2520size%2520of%2520the%2520cross-modal%250Aalignment%2520data%252C%2520making%2520developing%2520omni-modal%2520from%2520existing%2520vision-language%250Amodels%2520easy%2520and%2520less%2520costly.%2520Moreover%252C%2520to%2520unlock%2520an%2520advanced%2520interactive%250Aexperience%2520like%2520GPT-4o%252C%2520we%2520further%2520design%2520a%2520sentence-wise%2520decoding%2520solution%2520for%250Astreaming%2520speech%2520generation.%2520Extensive%2520experiments%2520demonstrate%2520that%2520Ola%250Asurpasses%2520existing%2520open%2520omni-modal%2520LLMs%2520across%2520all%2520modalities%2520while%2520achieving%250Ahighly%2520competitive%2520performance%2520compared%2520to%2520state-of-the-art%2520specialized%2520models%250Aof%2520similar%2520sizes.%2520We%2520aim%2520to%2520make%2520Ola%2520a%2520fully%2520open%2520omni-modal%2520understanding%250Asolution%2520to%2520advance%2520future%2520research%2520in%2520this%2520emerging%2520field.%2520Model%2520weights%252C%250Acode%252C%2520and%2520data%2520are%2520open-sourced%2520at%2520https%253A//github.com/Ola-Omni/Ola.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04328v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ola%3A%20Pushing%20the%20Frontiers%20of%20Omni-Modal%20Language%20Model%20with%20Progressive%0A%20%20Modality%20Alignment&entry.906535625=Zuyan%20Liu%20and%20Yuhao%20Dong%20and%20Jiahui%20Wang%20and%20Ziwei%20Liu%20and%20Winston%20Hu%20and%20Jiwen%20Lu%20and%20Yongming%20Rao&entry.1292438233=%20%20Recent%20advances%20in%20large%20language%20models%2C%20particularly%20following%20GPT-4o%2C%20have%0Asparked%20increasing%20interest%20in%20developing%20omni-modal%20models%20capable%20of%0Aunderstanding%20more%20modalities.%20While%20some%20open-source%20alternatives%20have%0Aemerged%2C%20there%20is%20still%20a%20notable%20lag%20behind%20specialized%20single-modality%20models%0Ain%20performance.%20In%20this%20paper%2C%20we%20present%20Ola%2C%20an%20Omni-modal%20language%20model%0Athat%20achieves%20competitive%20performance%20across%20image%2C%20video%2C%20and%20audio%0Aunderstanding%20compared%20to%20specialized%20counterparts.%20The%20core%20design%20of%20Ola%20lies%0Ain%20its%20progressive%20modality%20alignment%20strategy%20that%20extends%20the%20supporting%0Amodality%20of%20the%20language%20model%20progressively.%20Our%20training%20pipeline%20begins%20with%0Athe%20most%20distinct%20modalities%3A%20image%20and%20text%2C%20then%20gradually%20expands%20the%20skill%0Asets%20of%20the%20model%20using%20speech%20data%20that%20connects%20language%20and%20audio%20knowledge%2C%0Aand%20video%20data%20that%20connects%20all%20modalities.%20The%20progressive%20learning%20pipeline%0Aalso%20enables%20us%20to%20maintain%20a%20relatively%20small%20size%20of%20the%20cross-modal%0Aalignment%20data%2C%20making%20developing%20omni-modal%20from%20existing%20vision-language%0Amodels%20easy%20and%20less%20costly.%20Moreover%2C%20to%20unlock%20an%20advanced%20interactive%0Aexperience%20like%20GPT-4o%2C%20we%20further%20design%20a%20sentence-wise%20decoding%20solution%20for%0Astreaming%20speech%20generation.%20Extensive%20experiments%20demonstrate%20that%20Ola%0Asurpasses%20existing%20open%20omni-modal%20LLMs%20across%20all%20modalities%20while%20achieving%0Ahighly%20competitive%20performance%20compared%20to%20state-of-the-art%20specialized%20models%0Aof%20similar%20sizes.%20We%20aim%20to%20make%20Ola%20a%20fully%20open%20omni-modal%20understanding%0Asolution%20to%20advance%20future%20research%20in%20this%20emerging%20field.%20Model%20weights%2C%0Acode%2C%20and%20data%20are%20open-sourced%20at%20https%3A//github.com/Ola-Omni/Ola.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04328v2&entry.124074799=Read"},
{"title": "Wrapped Gaussian on the manifold of Symmetric Positive Definite Matrices", "author": "Thibault de Surrel and Fabien Lotte and Sylvain Chevallier and Florian Yger", "abstract": "  Circular and non-flat data distributions are prevalent across diverse domains\nof data science, yet their specific geometric structures often remain\nunderutilized in machine learning frameworks. A principled approach to\naccounting for the underlying geometry of such data is pivotal, particularly\nwhen extending statistical models, like the pervasive Gaussian distribution. In\nthis work, we tackle those issue by focusing on the manifold of symmetric\npositive definite matrices, a key focus in information geometry. We introduced\na non-isotropic wrapped Gaussian by leveraging the exponential map, we derive\ntheoretical properties of this distribution and propose a maximum likelihood\nframework for parameter estimation. Furthermore, we reinterpret established\nclassifiers on SPD through a probabilistic lens and introduce new classifiers\nbased on the wrapped Gaussian model. Experiments on synthetic and real-world\ndatasets demonstrate the robustness and flexibility of this geometry-aware\ndistribution, underscoring its potential to advance manifold-based data\nanalysis. This work lays the groundwork for extending classical machine\nlearning and statistical methods to more complex and structured data.\n", "link": "http://arxiv.org/abs/2502.01512v2", "date": "2025-02-12", "relevancy": 2.7361, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.564}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5405}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5372}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wrapped%20Gaussian%20on%20the%20manifold%20of%20Symmetric%20Positive%20Definite%20Matrices&body=Title%3A%20Wrapped%20Gaussian%20on%20the%20manifold%20of%20Symmetric%20Positive%20Definite%20Matrices%0AAuthor%3A%20Thibault%20de%20Surrel%20and%20Fabien%20Lotte%20and%20Sylvain%20Chevallier%20and%20Florian%20Yger%0AAbstract%3A%20%20%20Circular%20and%20non-flat%20data%20distributions%20are%20prevalent%20across%20diverse%20domains%0Aof%20data%20science%2C%20yet%20their%20specific%20geometric%20structures%20often%20remain%0Aunderutilized%20in%20machine%20learning%20frameworks.%20A%20principled%20approach%20to%0Aaccounting%20for%20the%20underlying%20geometry%20of%20such%20data%20is%20pivotal%2C%20particularly%0Awhen%20extending%20statistical%20models%2C%20like%20the%20pervasive%20Gaussian%20distribution.%20In%0Athis%20work%2C%20we%20tackle%20those%20issue%20by%20focusing%20on%20the%20manifold%20of%20symmetric%0Apositive%20definite%20matrices%2C%20a%20key%20focus%20in%20information%20geometry.%20We%20introduced%0Aa%20non-isotropic%20wrapped%20Gaussian%20by%20leveraging%20the%20exponential%20map%2C%20we%20derive%0Atheoretical%20properties%20of%20this%20distribution%20and%20propose%20a%20maximum%20likelihood%0Aframework%20for%20parameter%20estimation.%20Furthermore%2C%20we%20reinterpret%20established%0Aclassifiers%20on%20SPD%20through%20a%20probabilistic%20lens%20and%20introduce%20new%20classifiers%0Abased%20on%20the%20wrapped%20Gaussian%20model.%20Experiments%20on%20synthetic%20and%20real-world%0Adatasets%20demonstrate%20the%20robustness%20and%20flexibility%20of%20this%20geometry-aware%0Adistribution%2C%20underscoring%20its%20potential%20to%20advance%20manifold-based%20data%0Aanalysis.%20This%20work%20lays%20the%20groundwork%20for%20extending%20classical%20machine%0Alearning%20and%20statistical%20methods%20to%20more%20complex%20and%20structured%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.01512v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWrapped%2520Gaussian%2520on%2520the%2520manifold%2520of%2520Symmetric%2520Positive%2520Definite%2520Matrices%26entry.906535625%3DThibault%2520de%2520Surrel%2520and%2520Fabien%2520Lotte%2520and%2520Sylvain%2520Chevallier%2520and%2520Florian%2520Yger%26entry.1292438233%3D%2520%2520Circular%2520and%2520non-flat%2520data%2520distributions%2520are%2520prevalent%2520across%2520diverse%2520domains%250Aof%2520data%2520science%252C%2520yet%2520their%2520specific%2520geometric%2520structures%2520often%2520remain%250Aunderutilized%2520in%2520machine%2520learning%2520frameworks.%2520A%2520principled%2520approach%2520to%250Aaccounting%2520for%2520the%2520underlying%2520geometry%2520of%2520such%2520data%2520is%2520pivotal%252C%2520particularly%250Awhen%2520extending%2520statistical%2520models%252C%2520like%2520the%2520pervasive%2520Gaussian%2520distribution.%2520In%250Athis%2520work%252C%2520we%2520tackle%2520those%2520issue%2520by%2520focusing%2520on%2520the%2520manifold%2520of%2520symmetric%250Apositive%2520definite%2520matrices%252C%2520a%2520key%2520focus%2520in%2520information%2520geometry.%2520We%2520introduced%250Aa%2520non-isotropic%2520wrapped%2520Gaussian%2520by%2520leveraging%2520the%2520exponential%2520map%252C%2520we%2520derive%250Atheoretical%2520properties%2520of%2520this%2520distribution%2520and%2520propose%2520a%2520maximum%2520likelihood%250Aframework%2520for%2520parameter%2520estimation.%2520Furthermore%252C%2520we%2520reinterpret%2520established%250Aclassifiers%2520on%2520SPD%2520through%2520a%2520probabilistic%2520lens%2520and%2520introduce%2520new%2520classifiers%250Abased%2520on%2520the%2520wrapped%2520Gaussian%2520model.%2520Experiments%2520on%2520synthetic%2520and%2520real-world%250Adatasets%2520demonstrate%2520the%2520robustness%2520and%2520flexibility%2520of%2520this%2520geometry-aware%250Adistribution%252C%2520underscoring%2520its%2520potential%2520to%2520advance%2520manifold-based%2520data%250Aanalysis.%2520This%2520work%2520lays%2520the%2520groundwork%2520for%2520extending%2520classical%2520machine%250Alearning%2520and%2520statistical%2520methods%2520to%2520more%2520complex%2520and%2520structured%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.01512v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wrapped%20Gaussian%20on%20the%20manifold%20of%20Symmetric%20Positive%20Definite%20Matrices&entry.906535625=Thibault%20de%20Surrel%20and%20Fabien%20Lotte%20and%20Sylvain%20Chevallier%20and%20Florian%20Yger&entry.1292438233=%20%20Circular%20and%20non-flat%20data%20distributions%20are%20prevalent%20across%20diverse%20domains%0Aof%20data%20science%2C%20yet%20their%20specific%20geometric%20structures%20often%20remain%0Aunderutilized%20in%20machine%20learning%20frameworks.%20A%20principled%20approach%20to%0Aaccounting%20for%20the%20underlying%20geometry%20of%20such%20data%20is%20pivotal%2C%20particularly%0Awhen%20extending%20statistical%20models%2C%20like%20the%20pervasive%20Gaussian%20distribution.%20In%0Athis%20work%2C%20we%20tackle%20those%20issue%20by%20focusing%20on%20the%20manifold%20of%20symmetric%0Apositive%20definite%20matrices%2C%20a%20key%20focus%20in%20information%20geometry.%20We%20introduced%0Aa%20non-isotropic%20wrapped%20Gaussian%20by%20leveraging%20the%20exponential%20map%2C%20we%20derive%0Atheoretical%20properties%20of%20this%20distribution%20and%20propose%20a%20maximum%20likelihood%0Aframework%20for%20parameter%20estimation.%20Furthermore%2C%20we%20reinterpret%20established%0Aclassifiers%20on%20SPD%20through%20a%20probabilistic%20lens%20and%20introduce%20new%20classifiers%0Abased%20on%20the%20wrapped%20Gaussian%20model.%20Experiments%20on%20synthetic%20and%20real-world%0Adatasets%20demonstrate%20the%20robustness%20and%20flexibility%20of%20this%20geometry-aware%0Adistribution%2C%20underscoring%20its%20potential%20to%20advance%20manifold-based%20data%0Aanalysis.%20This%20work%20lays%20the%20groundwork%20for%20extending%20classical%20machine%0Alearning%20and%20statistical%20methods%20to%20more%20complex%20and%20structured%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.01512v2&entry.124074799=Read"},
{"title": "Topological Blindspots: Understanding and Extending Topological Deep\n  Learning Through the Lens of Expressivity", "author": "Yam Eitan and Yoav Gelberg and Guy Bar-Shalom and Fabrizio Frasca and Michael Bronstein and Haggai Maron", "abstract": "  Topological deep learning (TDL) is a rapidly growing field that seeks to\nleverage topological structure in data and facilitate learning from data\nsupported on topological objects, ranging from molecules to 3D shapes. Most TDL\narchitectures can be unified under the framework of higher-order\nmessage-passing (HOMP), which generalizes graph message-passing to higher-order\ndomains. In the first part of the paper, we explore HOMP's expressive power\nfrom a topological perspective, demonstrating the framework's inability to\ncapture fundamental topological and metric invariants such as diameter,\norientability, planarity, and homology. In addition, we demonstrate HOMP's\nlimitations in fully leveraging lifting and pooling methods on graphs. To the\nbest of our knowledge, this is the first work to study the expressivity of TDL\nfrom a \\emph{topological} perspective. In the second part of the paper, we\ndevelop two new classes of architectures -- multi-cellular networks (MCN) and\nscalable MCN (SMCN) -- which draw inspiration from expressive GNNs. MCN can\nreach full expressivity, but scaling it to large data objects can be\ncomputationally expansive. Designed as a more scalable alternative, SMCN still\nmitigates many of HOMP's expressivity limitations. Finally, we create new\nbenchmarks for evaluating models based on their ability to learn topological\nproperties of complexes. We then evaluate SMCN on these benchmarks and on\nreal-world graph datasets, demonstrating improvements over both HOMP baselines\nand expressive graph methods, highlighting the value of expressively leveraging\ntopological information. Code and data are available at\nhttps://github.com/yoavgelberg/SMCN.\n", "link": "http://arxiv.org/abs/2408.05486v2", "date": "2025-02-12", "relevancy": 2.6727, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5593}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5222}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5222}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topological%20Blindspots%3A%20Understanding%20and%20Extending%20Topological%20Deep%0A%20%20Learning%20Through%20the%20Lens%20of%20Expressivity&body=Title%3A%20Topological%20Blindspots%3A%20Understanding%20and%20Extending%20Topological%20Deep%0A%20%20Learning%20Through%20the%20Lens%20of%20Expressivity%0AAuthor%3A%20Yam%20Eitan%20and%20Yoav%20Gelberg%20and%20Guy%20Bar-Shalom%20and%20Fabrizio%20Frasca%20and%20Michael%20Bronstein%20and%20Haggai%20Maron%0AAbstract%3A%20%20%20Topological%20deep%20learning%20%28TDL%29%20is%20a%20rapidly%20growing%20field%20that%20seeks%20to%0Aleverage%20topological%20structure%20in%20data%20and%20facilitate%20learning%20from%20data%0Asupported%20on%20topological%20objects%2C%20ranging%20from%20molecules%20to%203D%20shapes.%20Most%20TDL%0Aarchitectures%20can%20be%20unified%20under%20the%20framework%20of%20higher-order%0Amessage-passing%20%28HOMP%29%2C%20which%20generalizes%20graph%20message-passing%20to%20higher-order%0Adomains.%20In%20the%20first%20part%20of%20the%20paper%2C%20we%20explore%20HOMP%27s%20expressive%20power%0Afrom%20a%20topological%20perspective%2C%20demonstrating%20the%20framework%27s%20inability%20to%0Acapture%20fundamental%20topological%20and%20metric%20invariants%20such%20as%20diameter%2C%0Aorientability%2C%20planarity%2C%20and%20homology.%20In%20addition%2C%20we%20demonstrate%20HOMP%27s%0Alimitations%20in%20fully%20leveraging%20lifting%20and%20pooling%20methods%20on%20graphs.%20To%20the%0Abest%20of%20our%20knowledge%2C%20this%20is%20the%20first%20work%20to%20study%20the%20expressivity%20of%20TDL%0Afrom%20a%20%5Cemph%7Btopological%7D%20perspective.%20In%20the%20second%20part%20of%20the%20paper%2C%20we%0Adevelop%20two%20new%20classes%20of%20architectures%20--%20multi-cellular%20networks%20%28MCN%29%20and%0Ascalable%20MCN%20%28SMCN%29%20--%20which%20draw%20inspiration%20from%20expressive%20GNNs.%20MCN%20can%0Areach%20full%20expressivity%2C%20but%20scaling%20it%20to%20large%20data%20objects%20can%20be%0Acomputationally%20expansive.%20Designed%20as%20a%20more%20scalable%20alternative%2C%20SMCN%20still%0Amitigates%20many%20of%20HOMP%27s%20expressivity%20limitations.%20Finally%2C%20we%20create%20new%0Abenchmarks%20for%20evaluating%20models%20based%20on%20their%20ability%20to%20learn%20topological%0Aproperties%20of%20complexes.%20We%20then%20evaluate%20SMCN%20on%20these%20benchmarks%20and%20on%0Areal-world%20graph%20datasets%2C%20demonstrating%20improvements%20over%20both%20HOMP%20baselines%0Aand%20expressive%20graph%20methods%2C%20highlighting%20the%20value%20of%20expressively%20leveraging%0Atopological%20information.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/yoavgelberg/SMCN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05486v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopological%2520Blindspots%253A%2520Understanding%2520and%2520Extending%2520Topological%2520Deep%250A%2520%2520Learning%2520Through%2520the%2520Lens%2520of%2520Expressivity%26entry.906535625%3DYam%2520Eitan%2520and%2520Yoav%2520Gelberg%2520and%2520Guy%2520Bar-Shalom%2520and%2520Fabrizio%2520Frasca%2520and%2520Michael%2520Bronstein%2520and%2520Haggai%2520Maron%26entry.1292438233%3D%2520%2520Topological%2520deep%2520learning%2520%2528TDL%2529%2520is%2520a%2520rapidly%2520growing%2520field%2520that%2520seeks%2520to%250Aleverage%2520topological%2520structure%2520in%2520data%2520and%2520facilitate%2520learning%2520from%2520data%250Asupported%2520on%2520topological%2520objects%252C%2520ranging%2520from%2520molecules%2520to%25203D%2520shapes.%2520Most%2520TDL%250Aarchitectures%2520can%2520be%2520unified%2520under%2520the%2520framework%2520of%2520higher-order%250Amessage-passing%2520%2528HOMP%2529%252C%2520which%2520generalizes%2520graph%2520message-passing%2520to%2520higher-order%250Adomains.%2520In%2520the%2520first%2520part%2520of%2520the%2520paper%252C%2520we%2520explore%2520HOMP%2527s%2520expressive%2520power%250Afrom%2520a%2520topological%2520perspective%252C%2520demonstrating%2520the%2520framework%2527s%2520inability%2520to%250Acapture%2520fundamental%2520topological%2520and%2520metric%2520invariants%2520such%2520as%2520diameter%252C%250Aorientability%252C%2520planarity%252C%2520and%2520homology.%2520In%2520addition%252C%2520we%2520demonstrate%2520HOMP%2527s%250Alimitations%2520in%2520fully%2520leveraging%2520lifting%2520and%2520pooling%2520methods%2520on%2520graphs.%2520To%2520the%250Abest%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520work%2520to%2520study%2520the%2520expressivity%2520of%2520TDL%250Afrom%2520a%2520%255Cemph%257Btopological%257D%2520perspective.%2520In%2520the%2520second%2520part%2520of%2520the%2520paper%252C%2520we%250Adevelop%2520two%2520new%2520classes%2520of%2520architectures%2520--%2520multi-cellular%2520networks%2520%2528MCN%2529%2520and%250Ascalable%2520MCN%2520%2528SMCN%2529%2520--%2520which%2520draw%2520inspiration%2520from%2520expressive%2520GNNs.%2520MCN%2520can%250Areach%2520full%2520expressivity%252C%2520but%2520scaling%2520it%2520to%2520large%2520data%2520objects%2520can%2520be%250Acomputationally%2520expansive.%2520Designed%2520as%2520a%2520more%2520scalable%2520alternative%252C%2520SMCN%2520still%250Amitigates%2520many%2520of%2520HOMP%2527s%2520expressivity%2520limitations.%2520Finally%252C%2520we%2520create%2520new%250Abenchmarks%2520for%2520evaluating%2520models%2520based%2520on%2520their%2520ability%2520to%2520learn%2520topological%250Aproperties%2520of%2520complexes.%2520We%2520then%2520evaluate%2520SMCN%2520on%2520these%2520benchmarks%2520and%2520on%250Areal-world%2520graph%2520datasets%252C%2520demonstrating%2520improvements%2520over%2520both%2520HOMP%2520baselines%250Aand%2520expressive%2520graph%2520methods%252C%2520highlighting%2520the%2520value%2520of%2520expressively%2520leveraging%250Atopological%2520information.%2520Code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/yoavgelberg/SMCN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05486v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topological%20Blindspots%3A%20Understanding%20and%20Extending%20Topological%20Deep%0A%20%20Learning%20Through%20the%20Lens%20of%20Expressivity&entry.906535625=Yam%20Eitan%20and%20Yoav%20Gelberg%20and%20Guy%20Bar-Shalom%20and%20Fabrizio%20Frasca%20and%20Michael%20Bronstein%20and%20Haggai%20Maron&entry.1292438233=%20%20Topological%20deep%20learning%20%28TDL%29%20is%20a%20rapidly%20growing%20field%20that%20seeks%20to%0Aleverage%20topological%20structure%20in%20data%20and%20facilitate%20learning%20from%20data%0Asupported%20on%20topological%20objects%2C%20ranging%20from%20molecules%20to%203D%20shapes.%20Most%20TDL%0Aarchitectures%20can%20be%20unified%20under%20the%20framework%20of%20higher-order%0Amessage-passing%20%28HOMP%29%2C%20which%20generalizes%20graph%20message-passing%20to%20higher-order%0Adomains.%20In%20the%20first%20part%20of%20the%20paper%2C%20we%20explore%20HOMP%27s%20expressive%20power%0Afrom%20a%20topological%20perspective%2C%20demonstrating%20the%20framework%27s%20inability%20to%0Acapture%20fundamental%20topological%20and%20metric%20invariants%20such%20as%20diameter%2C%0Aorientability%2C%20planarity%2C%20and%20homology.%20In%20addition%2C%20we%20demonstrate%20HOMP%27s%0Alimitations%20in%20fully%20leveraging%20lifting%20and%20pooling%20methods%20on%20graphs.%20To%20the%0Abest%20of%20our%20knowledge%2C%20this%20is%20the%20first%20work%20to%20study%20the%20expressivity%20of%20TDL%0Afrom%20a%20%5Cemph%7Btopological%7D%20perspective.%20In%20the%20second%20part%20of%20the%20paper%2C%20we%0Adevelop%20two%20new%20classes%20of%20architectures%20--%20multi-cellular%20networks%20%28MCN%29%20and%0Ascalable%20MCN%20%28SMCN%29%20--%20which%20draw%20inspiration%20from%20expressive%20GNNs.%20MCN%20can%0Areach%20full%20expressivity%2C%20but%20scaling%20it%20to%20large%20data%20objects%20can%20be%0Acomputationally%20expansive.%20Designed%20as%20a%20more%20scalable%20alternative%2C%20SMCN%20still%0Amitigates%20many%20of%20HOMP%27s%20expressivity%20limitations.%20Finally%2C%20we%20create%20new%0Abenchmarks%20for%20evaluating%20models%20based%20on%20their%20ability%20to%20learn%20topological%0Aproperties%20of%20complexes.%20We%20then%20evaluate%20SMCN%20on%20these%20benchmarks%20and%20on%0Areal-world%20graph%20datasets%2C%20demonstrating%20improvements%20over%20both%20HOMP%20baselines%0Aand%20expressive%20graph%20methods%2C%20highlighting%20the%20value%20of%20expressively%20leveraging%0Atopological%20information.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/yoavgelberg/SMCN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05486v2&entry.124074799=Read"},
{"title": "Bridging Domain Adaptation and Graph Neural Networks: A Tensor-Based\n  Framework for Effective Label Propagation", "author": "Tao Wen and Elynn Chen and Yuzhou Chen and Qi Lei", "abstract": "  Graph Neural Networks (GNNs) have recently become the predominant tools for\nstudying graph data. Despite state-of-the-art performance on graph\nclassification tasks, GNNs are overwhelmingly trained in a single domain under\nsupervision, thus necessitating a prohibitively high demand for labels and\nresulting in poorly transferable representations. To address this challenge, we\npropose the Label-Propagation Tensor Graph Neural Network (LP-TGNN) framework\nto bridge the gap between graph data and traditional domain adaptation methods.\nIt extracts graph topological information holistically with a tensor\narchitecture and then reduces domain discrepancy through label propagation. It\nis readily compatible with general GNNs and domain adaptation techniques with\nminimal adjustment through pseudo-labeling. Experiments on various real-world\nbenchmarks show that our LP-TGNN outperforms baselines by a notable margin. We\nalso validate and analyze each component of the proposed framework in the\nablation study.\n", "link": "http://arxiv.org/abs/2502.08505v1", "date": "2025-02-12", "relevancy": 2.5125, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5187}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4959}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Domain%20Adaptation%20and%20Graph%20Neural%20Networks%3A%20A%20Tensor-Based%0A%20%20Framework%20for%20Effective%20Label%20Propagation&body=Title%3A%20Bridging%20Domain%20Adaptation%20and%20Graph%20Neural%20Networks%3A%20A%20Tensor-Based%0A%20%20Framework%20for%20Effective%20Label%20Propagation%0AAuthor%3A%20Tao%20Wen%20and%20Elynn%20Chen%20and%20Yuzhou%20Chen%20and%20Qi%20Lei%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20recently%20become%20the%20predominant%20tools%20for%0Astudying%20graph%20data.%20Despite%20state-of-the-art%20performance%20on%20graph%0Aclassification%20tasks%2C%20GNNs%20are%20overwhelmingly%20trained%20in%20a%20single%20domain%20under%0Asupervision%2C%20thus%20necessitating%20a%20prohibitively%20high%20demand%20for%20labels%20and%0Aresulting%20in%20poorly%20transferable%20representations.%20To%20address%20this%20challenge%2C%20we%0Apropose%20the%20Label-Propagation%20Tensor%20Graph%20Neural%20Network%20%28LP-TGNN%29%20framework%0Ato%20bridge%20the%20gap%20between%20graph%20data%20and%20traditional%20domain%20adaptation%20methods.%0AIt%20extracts%20graph%20topological%20information%20holistically%20with%20a%20tensor%0Aarchitecture%20and%20then%20reduces%20domain%20discrepancy%20through%20label%20propagation.%20It%0Ais%20readily%20compatible%20with%20general%20GNNs%20and%20domain%20adaptation%20techniques%20with%0Aminimal%20adjustment%20through%20pseudo-labeling.%20Experiments%20on%20various%20real-world%0Abenchmarks%20show%20that%20our%20LP-TGNN%20outperforms%20baselines%20by%20a%20notable%20margin.%20We%0Aalso%20validate%20and%20analyze%20each%20component%20of%20the%20proposed%20framework%20in%20the%0Aablation%20study.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08505v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Domain%2520Adaptation%2520and%2520Graph%2520Neural%2520Networks%253A%2520A%2520Tensor-Based%250A%2520%2520Framework%2520for%2520Effective%2520Label%2520Propagation%26entry.906535625%3DTao%2520Wen%2520and%2520Elynn%2520Chen%2520and%2520Yuzhou%2520Chen%2520and%2520Qi%2520Lei%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520recently%2520become%2520the%2520predominant%2520tools%2520for%250Astudying%2520graph%2520data.%2520Despite%2520state-of-the-art%2520performance%2520on%2520graph%250Aclassification%2520tasks%252C%2520GNNs%2520are%2520overwhelmingly%2520trained%2520in%2520a%2520single%2520domain%2520under%250Asupervision%252C%2520thus%2520necessitating%2520a%2520prohibitively%2520high%2520demand%2520for%2520labels%2520and%250Aresulting%2520in%2520poorly%2520transferable%2520representations.%2520To%2520address%2520this%2520challenge%252C%2520we%250Apropose%2520the%2520Label-Propagation%2520Tensor%2520Graph%2520Neural%2520Network%2520%2528LP-TGNN%2529%2520framework%250Ato%2520bridge%2520the%2520gap%2520between%2520graph%2520data%2520and%2520traditional%2520domain%2520adaptation%2520methods.%250AIt%2520extracts%2520graph%2520topological%2520information%2520holistically%2520with%2520a%2520tensor%250Aarchitecture%2520and%2520then%2520reduces%2520domain%2520discrepancy%2520through%2520label%2520propagation.%2520It%250Ais%2520readily%2520compatible%2520with%2520general%2520GNNs%2520and%2520domain%2520adaptation%2520techniques%2520with%250Aminimal%2520adjustment%2520through%2520pseudo-labeling.%2520Experiments%2520on%2520various%2520real-world%250Abenchmarks%2520show%2520that%2520our%2520LP-TGNN%2520outperforms%2520baselines%2520by%2520a%2520notable%2520margin.%2520We%250Aalso%2520validate%2520and%2520analyze%2520each%2520component%2520of%2520the%2520proposed%2520framework%2520in%2520the%250Aablation%2520study.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08505v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Domain%20Adaptation%20and%20Graph%20Neural%20Networks%3A%20A%20Tensor-Based%0A%20%20Framework%20for%20Effective%20Label%20Propagation&entry.906535625=Tao%20Wen%20and%20Elynn%20Chen%20and%20Yuzhou%20Chen%20and%20Qi%20Lei&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20recently%20become%20the%20predominant%20tools%20for%0Astudying%20graph%20data.%20Despite%20state-of-the-art%20performance%20on%20graph%0Aclassification%20tasks%2C%20GNNs%20are%20overwhelmingly%20trained%20in%20a%20single%20domain%20under%0Asupervision%2C%20thus%20necessitating%20a%20prohibitively%20high%20demand%20for%20labels%20and%0Aresulting%20in%20poorly%20transferable%20representations.%20To%20address%20this%20challenge%2C%20we%0Apropose%20the%20Label-Propagation%20Tensor%20Graph%20Neural%20Network%20%28LP-TGNN%29%20framework%0Ato%20bridge%20the%20gap%20between%20graph%20data%20and%20traditional%20domain%20adaptation%20methods.%0AIt%20extracts%20graph%20topological%20information%20holistically%20with%20a%20tensor%0Aarchitecture%20and%20then%20reduces%20domain%20discrepancy%20through%20label%20propagation.%20It%0Ais%20readily%20compatible%20with%20general%20GNNs%20and%20domain%20adaptation%20techniques%20with%0Aminimal%20adjustment%20through%20pseudo-labeling.%20Experiments%20on%20various%20real-world%0Abenchmarks%20show%20that%20our%20LP-TGNN%20outperforms%20baselines%20by%20a%20notable%20margin.%20We%0Aalso%20validate%20and%20analyze%20each%20component%20of%20the%20proposed%20framework%20in%20the%0Aablation%20study.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08505v1&entry.124074799=Read"},
{"title": "A Survey on Image Quality Assessment: Insights, Analysis, and Future\n  Outlook", "author": "Chengqian Ma and Zhengyi Shi and Zhiqiang Lu and Shenghao Xie and Fei Chao and Yao Sui", "abstract": "  Image quality assessment (IQA) represents a pivotal challenge in\nimage-focused technologies, significantly influencing the advancement\ntrajectory of image processing and computer vision. Recently, IQA has witnessed\na notable surge in innovative research efforts, driven by the emergence of\nnovel architectural paradigms and sophisticated computational techniques. This\nsurvey delivers an extensive analysis of contemporary IQA methodologies,\norganized according to their application scenarios, serving as a beneficial\nreference for both beginners and experienced researchers. We analyze the\nadvantages and limitations of current approaches and suggest potential future\nresearch pathways. The survey encompasses both general and specific IQA\nmethodologies, including conventional statistical measures, machine learning\ntechniques, and cutting-edge deep learning models such as convolutional neural\nnetworks (CNNs) and Transformer models. The analysis within this survey\nhighlights the necessity for distortion-specific IQA methods tailored to\nvarious application scenarios, emphasizing the significance of practicality,\ninterpretability, and ease of implementation in future developments.\n", "link": "http://arxiv.org/abs/2502.08540v1", "date": "2025-02-12", "relevancy": 2.51, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5032}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5014}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5014}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Image%20Quality%20Assessment%3A%20Insights%2C%20Analysis%2C%20and%20Future%0A%20%20Outlook&body=Title%3A%20A%20Survey%20on%20Image%20Quality%20Assessment%3A%20Insights%2C%20Analysis%2C%20and%20Future%0A%20%20Outlook%0AAuthor%3A%20Chengqian%20Ma%20and%20Zhengyi%20Shi%20and%20Zhiqiang%20Lu%20and%20Shenghao%20Xie%20and%20Fei%20Chao%20and%20Yao%20Sui%0AAbstract%3A%20%20%20Image%20quality%20assessment%20%28IQA%29%20represents%20a%20pivotal%20challenge%20in%0Aimage-focused%20technologies%2C%20significantly%20influencing%20the%20advancement%0Atrajectory%20of%20image%20processing%20and%20computer%20vision.%20Recently%2C%20IQA%20has%20witnessed%0Aa%20notable%20surge%20in%20innovative%20research%20efforts%2C%20driven%20by%20the%20emergence%20of%0Anovel%20architectural%20paradigms%20and%20sophisticated%20computational%20techniques.%20This%0Asurvey%20delivers%20an%20extensive%20analysis%20of%20contemporary%20IQA%20methodologies%2C%0Aorganized%20according%20to%20their%20application%20scenarios%2C%20serving%20as%20a%20beneficial%0Areference%20for%20both%20beginners%20and%20experienced%20researchers.%20We%20analyze%20the%0Aadvantages%20and%20limitations%20of%20current%20approaches%20and%20suggest%20potential%20future%0Aresearch%20pathways.%20The%20survey%20encompasses%20both%20general%20and%20specific%20IQA%0Amethodologies%2C%20including%20conventional%20statistical%20measures%2C%20machine%20learning%0Atechniques%2C%20and%20cutting-edge%20deep%20learning%20models%20such%20as%20convolutional%20neural%0Anetworks%20%28CNNs%29%20and%20Transformer%20models.%20The%20analysis%20within%20this%20survey%0Ahighlights%20the%20necessity%20for%20distortion-specific%20IQA%20methods%20tailored%20to%0Avarious%20application%20scenarios%2C%20emphasizing%20the%20significance%20of%20practicality%2C%0Ainterpretability%2C%20and%20ease%20of%20implementation%20in%20future%20developments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08540v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Image%2520Quality%2520Assessment%253A%2520Insights%252C%2520Analysis%252C%2520and%2520Future%250A%2520%2520Outlook%26entry.906535625%3DChengqian%2520Ma%2520and%2520Zhengyi%2520Shi%2520and%2520Zhiqiang%2520Lu%2520and%2520Shenghao%2520Xie%2520and%2520Fei%2520Chao%2520and%2520Yao%2520Sui%26entry.1292438233%3D%2520%2520Image%2520quality%2520assessment%2520%2528IQA%2529%2520represents%2520a%2520pivotal%2520challenge%2520in%250Aimage-focused%2520technologies%252C%2520significantly%2520influencing%2520the%2520advancement%250Atrajectory%2520of%2520image%2520processing%2520and%2520computer%2520vision.%2520Recently%252C%2520IQA%2520has%2520witnessed%250Aa%2520notable%2520surge%2520in%2520innovative%2520research%2520efforts%252C%2520driven%2520by%2520the%2520emergence%2520of%250Anovel%2520architectural%2520paradigms%2520and%2520sophisticated%2520computational%2520techniques.%2520This%250Asurvey%2520delivers%2520an%2520extensive%2520analysis%2520of%2520contemporary%2520IQA%2520methodologies%252C%250Aorganized%2520according%2520to%2520their%2520application%2520scenarios%252C%2520serving%2520as%2520a%2520beneficial%250Areference%2520for%2520both%2520beginners%2520and%2520experienced%2520researchers.%2520We%2520analyze%2520the%250Aadvantages%2520and%2520limitations%2520of%2520current%2520approaches%2520and%2520suggest%2520potential%2520future%250Aresearch%2520pathways.%2520The%2520survey%2520encompasses%2520both%2520general%2520and%2520specific%2520IQA%250Amethodologies%252C%2520including%2520conventional%2520statistical%2520measures%252C%2520machine%2520learning%250Atechniques%252C%2520and%2520cutting-edge%2520deep%2520learning%2520models%2520such%2520as%2520convolutional%2520neural%250Anetworks%2520%2528CNNs%2529%2520and%2520Transformer%2520models.%2520The%2520analysis%2520within%2520this%2520survey%250Ahighlights%2520the%2520necessity%2520for%2520distortion-specific%2520IQA%2520methods%2520tailored%2520to%250Avarious%2520application%2520scenarios%252C%2520emphasizing%2520the%2520significance%2520of%2520practicality%252C%250Ainterpretability%252C%2520and%2520ease%2520of%2520implementation%2520in%2520future%2520developments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08540v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Image%20Quality%20Assessment%3A%20Insights%2C%20Analysis%2C%20and%20Future%0A%20%20Outlook&entry.906535625=Chengqian%20Ma%20and%20Zhengyi%20Shi%20and%20Zhiqiang%20Lu%20and%20Shenghao%20Xie%20and%20Fei%20Chao%20and%20Yao%20Sui&entry.1292438233=%20%20Image%20quality%20assessment%20%28IQA%29%20represents%20a%20pivotal%20challenge%20in%0Aimage-focused%20technologies%2C%20significantly%20influencing%20the%20advancement%0Atrajectory%20of%20image%20processing%20and%20computer%20vision.%20Recently%2C%20IQA%20has%20witnessed%0Aa%20notable%20surge%20in%20innovative%20research%20efforts%2C%20driven%20by%20the%20emergence%20of%0Anovel%20architectural%20paradigms%20and%20sophisticated%20computational%20techniques.%20This%0Asurvey%20delivers%20an%20extensive%20analysis%20of%20contemporary%20IQA%20methodologies%2C%0Aorganized%20according%20to%20their%20application%20scenarios%2C%20serving%20as%20a%20beneficial%0Areference%20for%20both%20beginners%20and%20experienced%20researchers.%20We%20analyze%20the%0Aadvantages%20and%20limitations%20of%20current%20approaches%20and%20suggest%20potential%20future%0Aresearch%20pathways.%20The%20survey%20encompasses%20both%20general%20and%20specific%20IQA%0Amethodologies%2C%20including%20conventional%20statistical%20measures%2C%20machine%20learning%0Atechniques%2C%20and%20cutting-edge%20deep%20learning%20models%20such%20as%20convolutional%20neural%0Anetworks%20%28CNNs%29%20and%20Transformer%20models.%20The%20analysis%20within%20this%20survey%0Ahighlights%20the%20necessity%20for%20distortion-specific%20IQA%20methods%20tailored%20to%0Avarious%20application%20scenarios%2C%20emphasizing%20the%20significance%20of%20practicality%2C%0Ainterpretability%2C%20and%20ease%20of%20implementation%20in%20future%20developments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08540v1&entry.124074799=Read"},
{"title": "CurvGAD: Leveraging Curvature for Enhanced Graph Anomaly Detection", "author": "Karish Grover and Geoffrey J. Gordon and Christos Faloutsos", "abstract": "  Does the intrinsic curvature of complex networks hold the key to unveiling\ngraph anomalies that conventional approaches overlook? Reconstruction-based\ngraph anomaly detection (GAD) methods overlook such geometric outliers,\nfocusing only on structural and attribute-level anomalies. To this end, we\npropose CurvGAD - a mixed-curvature graph autoencoder that introduces the\nnotion of curvature-based geometric anomalies. CurvGAD introduces two parallel\npipelines for enhanced anomaly interpretability: (1) Curvature-equivariant\ngeometry reconstruction, which focuses exclusively on reconstructing the edge\ncurvatures using a mixed-curvature, Riemannian encoder and Gaussian\nkernel-based decoder; and (2) Curvature-invariant structure and attribute\nreconstruction, which decouples structural and attribute anomalies from\ngeometric irregularities by regularizing graph curvature under discrete\nOllivier-Ricci flow, thereby isolating the non-geometric anomalies. By\nleveraging curvature, CurvGAD refines the existing anomaly classifications and\nidentifies new curvature-driven anomalies. Extensive experimentation over 10\nreal-world datasets (both homophilic and heterophilic) demonstrates an\nimprovement of up to 6.5% over state-of-the-art GAD methods.\n", "link": "http://arxiv.org/abs/2502.08605v1", "date": "2025-02-12", "relevancy": 2.5097, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5117}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5036}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CurvGAD%3A%20Leveraging%20Curvature%20for%20Enhanced%20Graph%20Anomaly%20Detection&body=Title%3A%20CurvGAD%3A%20Leveraging%20Curvature%20for%20Enhanced%20Graph%20Anomaly%20Detection%0AAuthor%3A%20Karish%20Grover%20and%20Geoffrey%20J.%20Gordon%20and%20Christos%20Faloutsos%0AAbstract%3A%20%20%20Does%20the%20intrinsic%20curvature%20of%20complex%20networks%20hold%20the%20key%20to%20unveiling%0Agraph%20anomalies%20that%20conventional%20approaches%20overlook%3F%20Reconstruction-based%0Agraph%20anomaly%20detection%20%28GAD%29%20methods%20overlook%20such%20geometric%20outliers%2C%0Afocusing%20only%20on%20structural%20and%20attribute-level%20anomalies.%20To%20this%20end%2C%20we%0Apropose%20CurvGAD%20-%20a%20mixed-curvature%20graph%20autoencoder%20that%20introduces%20the%0Anotion%20of%20curvature-based%20geometric%20anomalies.%20CurvGAD%20introduces%20two%20parallel%0Apipelines%20for%20enhanced%20anomaly%20interpretability%3A%20%281%29%20Curvature-equivariant%0Ageometry%20reconstruction%2C%20which%20focuses%20exclusively%20on%20reconstructing%20the%20edge%0Acurvatures%20using%20a%20mixed-curvature%2C%20Riemannian%20encoder%20and%20Gaussian%0Akernel-based%20decoder%3B%20and%20%282%29%20Curvature-invariant%20structure%20and%20attribute%0Areconstruction%2C%20which%20decouples%20structural%20and%20attribute%20anomalies%20from%0Ageometric%20irregularities%20by%20regularizing%20graph%20curvature%20under%20discrete%0AOllivier-Ricci%20flow%2C%20thereby%20isolating%20the%20non-geometric%20anomalies.%20By%0Aleveraging%20curvature%2C%20CurvGAD%20refines%20the%20existing%20anomaly%20classifications%20and%0Aidentifies%20new%20curvature-driven%20anomalies.%20Extensive%20experimentation%20over%2010%0Areal-world%20datasets%20%28both%20homophilic%20and%20heterophilic%29%20demonstrates%20an%0Aimprovement%20of%20up%20to%206.5%25%20over%20state-of-the-art%20GAD%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08605v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCurvGAD%253A%2520Leveraging%2520Curvature%2520for%2520Enhanced%2520Graph%2520Anomaly%2520Detection%26entry.906535625%3DKarish%2520Grover%2520and%2520Geoffrey%2520J.%2520Gordon%2520and%2520Christos%2520Faloutsos%26entry.1292438233%3D%2520%2520Does%2520the%2520intrinsic%2520curvature%2520of%2520complex%2520networks%2520hold%2520the%2520key%2520to%2520unveiling%250Agraph%2520anomalies%2520that%2520conventional%2520approaches%2520overlook%253F%2520Reconstruction-based%250Agraph%2520anomaly%2520detection%2520%2528GAD%2529%2520methods%2520overlook%2520such%2520geometric%2520outliers%252C%250Afocusing%2520only%2520on%2520structural%2520and%2520attribute-level%2520anomalies.%2520To%2520this%2520end%252C%2520we%250Apropose%2520CurvGAD%2520-%2520a%2520mixed-curvature%2520graph%2520autoencoder%2520that%2520introduces%2520the%250Anotion%2520of%2520curvature-based%2520geometric%2520anomalies.%2520CurvGAD%2520introduces%2520two%2520parallel%250Apipelines%2520for%2520enhanced%2520anomaly%2520interpretability%253A%2520%25281%2529%2520Curvature-equivariant%250Ageometry%2520reconstruction%252C%2520which%2520focuses%2520exclusively%2520on%2520reconstructing%2520the%2520edge%250Acurvatures%2520using%2520a%2520mixed-curvature%252C%2520Riemannian%2520encoder%2520and%2520Gaussian%250Akernel-based%2520decoder%253B%2520and%2520%25282%2529%2520Curvature-invariant%2520structure%2520and%2520attribute%250Areconstruction%252C%2520which%2520decouples%2520structural%2520and%2520attribute%2520anomalies%2520from%250Ageometric%2520irregularities%2520by%2520regularizing%2520graph%2520curvature%2520under%2520discrete%250AOllivier-Ricci%2520flow%252C%2520thereby%2520isolating%2520the%2520non-geometric%2520anomalies.%2520By%250Aleveraging%2520curvature%252C%2520CurvGAD%2520refines%2520the%2520existing%2520anomaly%2520classifications%2520and%250Aidentifies%2520new%2520curvature-driven%2520anomalies.%2520Extensive%2520experimentation%2520over%252010%250Areal-world%2520datasets%2520%2528both%2520homophilic%2520and%2520heterophilic%2529%2520demonstrates%2520an%250Aimprovement%2520of%2520up%2520to%25206.5%2525%2520over%2520state-of-the-art%2520GAD%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08605v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CurvGAD%3A%20Leveraging%20Curvature%20for%20Enhanced%20Graph%20Anomaly%20Detection&entry.906535625=Karish%20Grover%20and%20Geoffrey%20J.%20Gordon%20and%20Christos%20Faloutsos&entry.1292438233=%20%20Does%20the%20intrinsic%20curvature%20of%20complex%20networks%20hold%20the%20key%20to%20unveiling%0Agraph%20anomalies%20that%20conventional%20approaches%20overlook%3F%20Reconstruction-based%0Agraph%20anomaly%20detection%20%28GAD%29%20methods%20overlook%20such%20geometric%20outliers%2C%0Afocusing%20only%20on%20structural%20and%20attribute-level%20anomalies.%20To%20this%20end%2C%20we%0Apropose%20CurvGAD%20-%20a%20mixed-curvature%20graph%20autoencoder%20that%20introduces%20the%0Anotion%20of%20curvature-based%20geometric%20anomalies.%20CurvGAD%20introduces%20two%20parallel%0Apipelines%20for%20enhanced%20anomaly%20interpretability%3A%20%281%29%20Curvature-equivariant%0Ageometry%20reconstruction%2C%20which%20focuses%20exclusively%20on%20reconstructing%20the%20edge%0Acurvatures%20using%20a%20mixed-curvature%2C%20Riemannian%20encoder%20and%20Gaussian%0Akernel-based%20decoder%3B%20and%20%282%29%20Curvature-invariant%20structure%20and%20attribute%0Areconstruction%2C%20which%20decouples%20structural%20and%20attribute%20anomalies%20from%0Ageometric%20irregularities%20by%20regularizing%20graph%20curvature%20under%20discrete%0AOllivier-Ricci%20flow%2C%20thereby%20isolating%20the%20non-geometric%20anomalies.%20By%0Aleveraging%20curvature%2C%20CurvGAD%20refines%20the%20existing%20anomaly%20classifications%20and%0Aidentifies%20new%20curvature-driven%20anomalies.%20Extensive%20experimentation%20over%2010%0Areal-world%20datasets%20%28both%20homophilic%20and%20heterophilic%29%20demonstrates%20an%0Aimprovement%20of%20up%20to%206.5%25%20over%20state-of-the-art%20GAD%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08605v1&entry.124074799=Read"},
{"title": "Matrix Completion with Graph Information: A Provable Nonconvex\n  Optimization Approach", "author": "Yao Wang and Yiyang Yang and Kaidong Wang and Shanxing Gao and Xiuwu Liao", "abstract": "  We consider the problem of matrix completion with graphs as side information\ndepicting the interrelations between variables. The key challenge lies in\nleveraging the similarity structure of the graph to enhance matrix recovery.\nExisting approaches, primarily based on graph Laplacian regularization, suffer\nfrom several limitations: (1) they focus only on the similarity between\nneighboring variables, while overlooking long-range correlations; (2) they are\nhighly sensitive to false edges in the graphs and (3) they lack theoretical\nguarantees regarding statistical and computational complexities. To address\nthese issues, we propose in this paper a novel graph regularized matrix\ncompletion algorithm called GSGD, based on preconditioned projected gradient\ndescent approach. We demonstrate that GSGD effectively captures the\nhigher-order correlation information behind the graphs, and achieves superior\nrobustness and stability against the false edges. Theoretically, we prove that\nGSGD achieves linear convergence to the global optimum with near-optimal sample\ncomplexity, providing the first theoretical guarantees for both recovery\naccuracy and efficacy in the perspective of nonconvex optimization. Our\nnumerical experiments on both synthetic and real-world data further validate\nthat GSGD achieves superior recovery accuracy and scalability compared with\nseveral popular alternatives.\n", "link": "http://arxiv.org/abs/2502.08536v1", "date": "2025-02-12", "relevancy": 2.498, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5166}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5011}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Matrix%20Completion%20with%20Graph%20Information%3A%20A%20Provable%20Nonconvex%0A%20%20Optimization%20Approach&body=Title%3A%20Matrix%20Completion%20with%20Graph%20Information%3A%20A%20Provable%20Nonconvex%0A%20%20Optimization%20Approach%0AAuthor%3A%20Yao%20Wang%20and%20Yiyang%20Yang%20and%20Kaidong%20Wang%20and%20Shanxing%20Gao%20and%20Xiuwu%20Liao%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20matrix%20completion%20with%20graphs%20as%20side%20information%0Adepicting%20the%20interrelations%20between%20variables.%20The%20key%20challenge%20lies%20in%0Aleveraging%20the%20similarity%20structure%20of%20the%20graph%20to%20enhance%20matrix%20recovery.%0AExisting%20approaches%2C%20primarily%20based%20on%20graph%20Laplacian%20regularization%2C%20suffer%0Afrom%20several%20limitations%3A%20%281%29%20they%20focus%20only%20on%20the%20similarity%20between%0Aneighboring%20variables%2C%20while%20overlooking%20long-range%20correlations%3B%20%282%29%20they%20are%0Ahighly%20sensitive%20to%20false%20edges%20in%20the%20graphs%20and%20%283%29%20they%20lack%20theoretical%0Aguarantees%20regarding%20statistical%20and%20computational%20complexities.%20To%20address%0Athese%20issues%2C%20we%20propose%20in%20this%20paper%20a%20novel%20graph%20regularized%20matrix%0Acompletion%20algorithm%20called%20GSGD%2C%20based%20on%20preconditioned%20projected%20gradient%0Adescent%20approach.%20We%20demonstrate%20that%20GSGD%20effectively%20captures%20the%0Ahigher-order%20correlation%20information%20behind%20the%20graphs%2C%20and%20achieves%20superior%0Arobustness%20and%20stability%20against%20the%20false%20edges.%20Theoretically%2C%20we%20prove%20that%0AGSGD%20achieves%20linear%20convergence%20to%20the%20global%20optimum%20with%20near-optimal%20sample%0Acomplexity%2C%20providing%20the%20first%20theoretical%20guarantees%20for%20both%20recovery%0Aaccuracy%20and%20efficacy%20in%20the%20perspective%20of%20nonconvex%20optimization.%20Our%0Anumerical%20experiments%20on%20both%20synthetic%20and%20real-world%20data%20further%20validate%0Athat%20GSGD%20achieves%20superior%20recovery%20accuracy%20and%20scalability%20compared%20with%0Aseveral%20popular%20alternatives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08536v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMatrix%2520Completion%2520with%2520Graph%2520Information%253A%2520A%2520Provable%2520Nonconvex%250A%2520%2520Optimization%2520Approach%26entry.906535625%3DYao%2520Wang%2520and%2520Yiyang%2520Yang%2520and%2520Kaidong%2520Wang%2520and%2520Shanxing%2520Gao%2520and%2520Xiuwu%2520Liao%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520matrix%2520completion%2520with%2520graphs%2520as%2520side%2520information%250Adepicting%2520the%2520interrelations%2520between%2520variables.%2520The%2520key%2520challenge%2520lies%2520in%250Aleveraging%2520the%2520similarity%2520structure%2520of%2520the%2520graph%2520to%2520enhance%2520matrix%2520recovery.%250AExisting%2520approaches%252C%2520primarily%2520based%2520on%2520graph%2520Laplacian%2520regularization%252C%2520suffer%250Afrom%2520several%2520limitations%253A%2520%25281%2529%2520they%2520focus%2520only%2520on%2520the%2520similarity%2520between%250Aneighboring%2520variables%252C%2520while%2520overlooking%2520long-range%2520correlations%253B%2520%25282%2529%2520they%2520are%250Ahighly%2520sensitive%2520to%2520false%2520edges%2520in%2520the%2520graphs%2520and%2520%25283%2529%2520they%2520lack%2520theoretical%250Aguarantees%2520regarding%2520statistical%2520and%2520computational%2520complexities.%2520To%2520address%250Athese%2520issues%252C%2520we%2520propose%2520in%2520this%2520paper%2520a%2520novel%2520graph%2520regularized%2520matrix%250Acompletion%2520algorithm%2520called%2520GSGD%252C%2520based%2520on%2520preconditioned%2520projected%2520gradient%250Adescent%2520approach.%2520We%2520demonstrate%2520that%2520GSGD%2520effectively%2520captures%2520the%250Ahigher-order%2520correlation%2520information%2520behind%2520the%2520graphs%252C%2520and%2520achieves%2520superior%250Arobustness%2520and%2520stability%2520against%2520the%2520false%2520edges.%2520Theoretically%252C%2520we%2520prove%2520that%250AGSGD%2520achieves%2520linear%2520convergence%2520to%2520the%2520global%2520optimum%2520with%2520near-optimal%2520sample%250Acomplexity%252C%2520providing%2520the%2520first%2520theoretical%2520guarantees%2520for%2520both%2520recovery%250Aaccuracy%2520and%2520efficacy%2520in%2520the%2520perspective%2520of%2520nonconvex%2520optimization.%2520Our%250Anumerical%2520experiments%2520on%2520both%2520synthetic%2520and%2520real-world%2520data%2520further%2520validate%250Athat%2520GSGD%2520achieves%2520superior%2520recovery%2520accuracy%2520and%2520scalability%2520compared%2520with%250Aseveral%2520popular%2520alternatives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08536v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Matrix%20Completion%20with%20Graph%20Information%3A%20A%20Provable%20Nonconvex%0A%20%20Optimization%20Approach&entry.906535625=Yao%20Wang%20and%20Yiyang%20Yang%20and%20Kaidong%20Wang%20and%20Shanxing%20Gao%20and%20Xiuwu%20Liao&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20matrix%20completion%20with%20graphs%20as%20side%20information%0Adepicting%20the%20interrelations%20between%20variables.%20The%20key%20challenge%20lies%20in%0Aleveraging%20the%20similarity%20structure%20of%20the%20graph%20to%20enhance%20matrix%20recovery.%0AExisting%20approaches%2C%20primarily%20based%20on%20graph%20Laplacian%20regularization%2C%20suffer%0Afrom%20several%20limitations%3A%20%281%29%20they%20focus%20only%20on%20the%20similarity%20between%0Aneighboring%20variables%2C%20while%20overlooking%20long-range%20correlations%3B%20%282%29%20they%20are%0Ahighly%20sensitive%20to%20false%20edges%20in%20the%20graphs%20and%20%283%29%20they%20lack%20theoretical%0Aguarantees%20regarding%20statistical%20and%20computational%20complexities.%20To%20address%0Athese%20issues%2C%20we%20propose%20in%20this%20paper%20a%20novel%20graph%20regularized%20matrix%0Acompletion%20algorithm%20called%20GSGD%2C%20based%20on%20preconditioned%20projected%20gradient%0Adescent%20approach.%20We%20demonstrate%20that%20GSGD%20effectively%20captures%20the%0Ahigher-order%20correlation%20information%20behind%20the%20graphs%2C%20and%20achieves%20superior%0Arobustness%20and%20stability%20against%20the%20false%20edges.%20Theoretically%2C%20we%20prove%20that%0AGSGD%20achieves%20linear%20convergence%20to%20the%20global%20optimum%20with%20near-optimal%20sample%0Acomplexity%2C%20providing%20the%20first%20theoretical%20guarantees%20for%20both%20recovery%0Aaccuracy%20and%20efficacy%20in%20the%20perspective%20of%20nonconvex%20optimization.%20Our%0Anumerical%20experiments%20on%20both%20synthetic%20and%20real-world%20data%20further%20validate%0Athat%20GSGD%20achieves%20superior%20recovery%20accuracy%20and%20scalability%20compared%20with%0Aseveral%20popular%20alternatives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08536v1&entry.124074799=Read"},
{"title": "Robustly Learning Monotone Generalized Linear Models via Data\n  Augmentation", "author": "Nikos Zarifis and Puqian Wang and Ilias Diakonikolas and Jelena Diakonikolas", "abstract": "  We study the task of learning Generalized Linear models (GLMs) in the\nagnostic model under the Gaussian distribution. We give the first\npolynomial-time algorithm that achieves a constant-factor approximation for\n\\textit{any} monotone Lipschitz activation. Prior constant-factor GLM learners\nsucceed for a substantially smaller class of activations. Our work resolves a\nwell-known open problem, by developing a robust counterpart to the classical\nGLMtron algorithm (Kakade et al., 2011). Our robust learner applies more\ngenerally, encompassing all monotone activations with bounded\n$(2+\\zeta)$-moments, for any fixed $\\zeta>0$ -- a condition that is essentially\nnecessary. To obtain our results, we leverage a novel data augmentation\ntechnique with decreasing Gaussian noise injection and prove a number of\nstructural results that may be useful in other settings.\n", "link": "http://arxiv.org/abs/2502.08611v1", "date": "2025-02-12", "relevancy": 2.4882, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5073}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4969}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robustly%20Learning%20Monotone%20Generalized%20Linear%20Models%20via%20Data%0A%20%20Augmentation&body=Title%3A%20Robustly%20Learning%20Monotone%20Generalized%20Linear%20Models%20via%20Data%0A%20%20Augmentation%0AAuthor%3A%20Nikos%20Zarifis%20and%20Puqian%20Wang%20and%20Ilias%20Diakonikolas%20and%20Jelena%20Diakonikolas%0AAbstract%3A%20%20%20We%20study%20the%20task%20of%20learning%20Generalized%20Linear%20models%20%28GLMs%29%20in%20the%0Aagnostic%20model%20under%20the%20Gaussian%20distribution.%20We%20give%20the%20first%0Apolynomial-time%20algorithm%20that%20achieves%20a%20constant-factor%20approximation%20for%0A%5Ctextit%7Bany%7D%20monotone%20Lipschitz%20activation.%20Prior%20constant-factor%20GLM%20learners%0Asucceed%20for%20a%20substantially%20smaller%20class%20of%20activations.%20Our%20work%20resolves%20a%0Awell-known%20open%20problem%2C%20by%20developing%20a%20robust%20counterpart%20to%20the%20classical%0AGLMtron%20algorithm%20%28Kakade%20et%20al.%2C%202011%29.%20Our%20robust%20learner%20applies%20more%0Agenerally%2C%20encompassing%20all%20monotone%20activations%20with%20bounded%0A%24%282%2B%5Czeta%29%24-moments%2C%20for%20any%20fixed%20%24%5Czeta%3E0%24%20--%20a%20condition%20that%20is%20essentially%0Anecessary.%20To%20obtain%20our%20results%2C%20we%20leverage%20a%20novel%20data%20augmentation%0Atechnique%20with%20decreasing%20Gaussian%20noise%20injection%20and%20prove%20a%20number%20of%0Astructural%20results%20that%20may%20be%20useful%20in%20other%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08611v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobustly%2520Learning%2520Monotone%2520Generalized%2520Linear%2520Models%2520via%2520Data%250A%2520%2520Augmentation%26entry.906535625%3DNikos%2520Zarifis%2520and%2520Puqian%2520Wang%2520and%2520Ilias%2520Diakonikolas%2520and%2520Jelena%2520Diakonikolas%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520task%2520of%2520learning%2520Generalized%2520Linear%2520models%2520%2528GLMs%2529%2520in%2520the%250Aagnostic%2520model%2520under%2520the%2520Gaussian%2520distribution.%2520We%2520give%2520the%2520first%250Apolynomial-time%2520algorithm%2520that%2520achieves%2520a%2520constant-factor%2520approximation%2520for%250A%255Ctextit%257Bany%257D%2520monotone%2520Lipschitz%2520activation.%2520Prior%2520constant-factor%2520GLM%2520learners%250Asucceed%2520for%2520a%2520substantially%2520smaller%2520class%2520of%2520activations.%2520Our%2520work%2520resolves%2520a%250Awell-known%2520open%2520problem%252C%2520by%2520developing%2520a%2520robust%2520counterpart%2520to%2520the%2520classical%250AGLMtron%2520algorithm%2520%2528Kakade%2520et%2520al.%252C%25202011%2529.%2520Our%2520robust%2520learner%2520applies%2520more%250Agenerally%252C%2520encompassing%2520all%2520monotone%2520activations%2520with%2520bounded%250A%2524%25282%252B%255Czeta%2529%2524-moments%252C%2520for%2520any%2520fixed%2520%2524%255Czeta%253E0%2524%2520--%2520a%2520condition%2520that%2520is%2520essentially%250Anecessary.%2520To%2520obtain%2520our%2520results%252C%2520we%2520leverage%2520a%2520novel%2520data%2520augmentation%250Atechnique%2520with%2520decreasing%2520Gaussian%2520noise%2520injection%2520and%2520prove%2520a%2520number%2520of%250Astructural%2520results%2520that%2520may%2520be%2520useful%2520in%2520other%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08611v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robustly%20Learning%20Monotone%20Generalized%20Linear%20Models%20via%20Data%0A%20%20Augmentation&entry.906535625=Nikos%20Zarifis%20and%20Puqian%20Wang%20and%20Ilias%20Diakonikolas%20and%20Jelena%20Diakonikolas&entry.1292438233=%20%20We%20study%20the%20task%20of%20learning%20Generalized%20Linear%20models%20%28GLMs%29%20in%20the%0Aagnostic%20model%20under%20the%20Gaussian%20distribution.%20We%20give%20the%20first%0Apolynomial-time%20algorithm%20that%20achieves%20a%20constant-factor%20approximation%20for%0A%5Ctextit%7Bany%7D%20monotone%20Lipschitz%20activation.%20Prior%20constant-factor%20GLM%20learners%0Asucceed%20for%20a%20substantially%20smaller%20class%20of%20activations.%20Our%20work%20resolves%20a%0Awell-known%20open%20problem%2C%20by%20developing%20a%20robust%20counterpart%20to%20the%20classical%0AGLMtron%20algorithm%20%28Kakade%20et%20al.%2C%202011%29.%20Our%20robust%20learner%20applies%20more%0Agenerally%2C%20encompassing%20all%20monotone%20activations%20with%20bounded%0A%24%282%2B%5Czeta%29%24-moments%2C%20for%20any%20fixed%20%24%5Czeta%3E0%24%20--%20a%20condition%20that%20is%20essentially%0Anecessary.%20To%20obtain%20our%20results%2C%20we%20leverage%20a%20novel%20data%20augmentation%0Atechnique%20with%20decreasing%20Gaussian%20noise%20injection%20and%20prove%20a%20number%20of%0Astructural%20results%20that%20may%20be%20useful%20in%20other%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08611v1&entry.124074799=Read"},
{"title": "Light-A-Video: Training-free Video Relighting via Progressive Light\n  Fusion", "author": "Yujie Zhou and Jiazi Bu and Pengyang Ling and Pan Zhang and Tong Wu and Qidong Huang and Jinsong Li and Xiaoyi Dong and Yuhang Zang and Yuhang Cao and Anyi Rao and Jiaqi Wang and Li Niu", "abstract": "  Recent advancements in image relighting models, driven by large-scale\ndatasets and pre-trained diffusion models, have enabled the imposition of\nconsistent lighting. However, video relighting still lags, primarily due to the\nexcessive training costs and the scarcity of diverse, high-quality video\nrelighting datasets. A simple application of image relighting models on a\nframe-by-frame basis leads to several issues: lighting source inconsistency and\nrelighted appearance inconsistency, resulting in flickers in the generated\nvideos. In this work, we propose Light-A-Video, a training-free approach to\nachieve temporally smooth video relighting. Adapted from image relighting\nmodels, Light-A-Video introduces two key techniques to enhance lighting\nconsistency. First, we design a Consistent Light Attention (CLA) module, which\nenhances cross-frame interactions within the self-attention layers to stabilize\nthe generation of the background lighting source. Second, leveraging the\nphysical principle of light transport independence, we apply linear blending\nbetween the source video's appearance and the relighted appearance, using a\nProgressive Light Fusion (PLF) strategy to ensure smooth temporal transitions\nin illumination. Experiments show that Light-A-Video improves the temporal\nconsistency of relighted video while maintaining the image quality, ensuring\ncoherent lighting transitions across frames. Project page:\nhttps://bujiazi.github.io/light-a-video.github.io/.\n", "link": "http://arxiv.org/abs/2502.08590v1", "date": "2025-02-12", "relevancy": 2.4304, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6243}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6209}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Light-A-Video%3A%20Training-free%20Video%20Relighting%20via%20Progressive%20Light%0A%20%20Fusion&body=Title%3A%20Light-A-Video%3A%20Training-free%20Video%20Relighting%20via%20Progressive%20Light%0A%20%20Fusion%0AAuthor%3A%20Yujie%20Zhou%20and%20Jiazi%20Bu%20and%20Pengyang%20Ling%20and%20Pan%20Zhang%20and%20Tong%20Wu%20and%20Qidong%20Huang%20and%20Jinsong%20Li%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Zang%20and%20Yuhang%20Cao%20and%20Anyi%20Rao%20and%20Jiaqi%20Wang%20and%20Li%20Niu%0AAbstract%3A%20%20%20Recent%20advancements%20in%20image%20relighting%20models%2C%20driven%20by%20large-scale%0Adatasets%20and%20pre-trained%20diffusion%20models%2C%20have%20enabled%20the%20imposition%20of%0Aconsistent%20lighting.%20However%2C%20video%20relighting%20still%20lags%2C%20primarily%20due%20to%20the%0Aexcessive%20training%20costs%20and%20the%20scarcity%20of%20diverse%2C%20high-quality%20video%0Arelighting%20datasets.%20A%20simple%20application%20of%20image%20relighting%20models%20on%20a%0Aframe-by-frame%20basis%20leads%20to%20several%20issues%3A%20lighting%20source%20inconsistency%20and%0Arelighted%20appearance%20inconsistency%2C%20resulting%20in%20flickers%20in%20the%20generated%0Avideos.%20In%20this%20work%2C%20we%20propose%20Light-A-Video%2C%20a%20training-free%20approach%20to%0Aachieve%20temporally%20smooth%20video%20relighting.%20Adapted%20from%20image%20relighting%0Amodels%2C%20Light-A-Video%20introduces%20two%20key%20techniques%20to%20enhance%20lighting%0Aconsistency.%20First%2C%20we%20design%20a%20Consistent%20Light%20Attention%20%28CLA%29%20module%2C%20which%0Aenhances%20cross-frame%20interactions%20within%20the%20self-attention%20layers%20to%20stabilize%0Athe%20generation%20of%20the%20background%20lighting%20source.%20Second%2C%20leveraging%20the%0Aphysical%20principle%20of%20light%20transport%20independence%2C%20we%20apply%20linear%20blending%0Abetween%20the%20source%20video%27s%20appearance%20and%20the%20relighted%20appearance%2C%20using%20a%0AProgressive%20Light%20Fusion%20%28PLF%29%20strategy%20to%20ensure%20smooth%20temporal%20transitions%0Ain%20illumination.%20Experiments%20show%20that%20Light-A-Video%20improves%20the%20temporal%0Aconsistency%20of%20relighted%20video%20while%20maintaining%20the%20image%20quality%2C%20ensuring%0Acoherent%20lighting%20transitions%20across%20frames.%20Project%20page%3A%0Ahttps%3A//bujiazi.github.io/light-a-video.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08590v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLight-A-Video%253A%2520Training-free%2520Video%2520Relighting%2520via%2520Progressive%2520Light%250A%2520%2520Fusion%26entry.906535625%3DYujie%2520Zhou%2520and%2520Jiazi%2520Bu%2520and%2520Pengyang%2520Ling%2520and%2520Pan%2520Zhang%2520and%2520Tong%2520Wu%2520and%2520Qidong%2520Huang%2520and%2520Jinsong%2520Li%2520and%2520Xiaoyi%2520Dong%2520and%2520Yuhang%2520Zang%2520and%2520Yuhang%2520Cao%2520and%2520Anyi%2520Rao%2520and%2520Jiaqi%2520Wang%2520and%2520Li%2520Niu%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520image%2520relighting%2520models%252C%2520driven%2520by%2520large-scale%250Adatasets%2520and%2520pre-trained%2520diffusion%2520models%252C%2520have%2520enabled%2520the%2520imposition%2520of%250Aconsistent%2520lighting.%2520However%252C%2520video%2520relighting%2520still%2520lags%252C%2520primarily%2520due%2520to%2520the%250Aexcessive%2520training%2520costs%2520and%2520the%2520scarcity%2520of%2520diverse%252C%2520high-quality%2520video%250Arelighting%2520datasets.%2520A%2520simple%2520application%2520of%2520image%2520relighting%2520models%2520on%2520a%250Aframe-by-frame%2520basis%2520leads%2520to%2520several%2520issues%253A%2520lighting%2520source%2520inconsistency%2520and%250Arelighted%2520appearance%2520inconsistency%252C%2520resulting%2520in%2520flickers%2520in%2520the%2520generated%250Avideos.%2520In%2520this%2520work%252C%2520we%2520propose%2520Light-A-Video%252C%2520a%2520training-free%2520approach%2520to%250Aachieve%2520temporally%2520smooth%2520video%2520relighting.%2520Adapted%2520from%2520image%2520relighting%250Amodels%252C%2520Light-A-Video%2520introduces%2520two%2520key%2520techniques%2520to%2520enhance%2520lighting%250Aconsistency.%2520First%252C%2520we%2520design%2520a%2520Consistent%2520Light%2520Attention%2520%2528CLA%2529%2520module%252C%2520which%250Aenhances%2520cross-frame%2520interactions%2520within%2520the%2520self-attention%2520layers%2520to%2520stabilize%250Athe%2520generation%2520of%2520the%2520background%2520lighting%2520source.%2520Second%252C%2520leveraging%2520the%250Aphysical%2520principle%2520of%2520light%2520transport%2520independence%252C%2520we%2520apply%2520linear%2520blending%250Abetween%2520the%2520source%2520video%2527s%2520appearance%2520and%2520the%2520relighted%2520appearance%252C%2520using%2520a%250AProgressive%2520Light%2520Fusion%2520%2528PLF%2529%2520strategy%2520to%2520ensure%2520smooth%2520temporal%2520transitions%250Ain%2520illumination.%2520Experiments%2520show%2520that%2520Light-A-Video%2520improves%2520the%2520temporal%250Aconsistency%2520of%2520relighted%2520video%2520while%2520maintaining%2520the%2520image%2520quality%252C%2520ensuring%250Acoherent%2520lighting%2520transitions%2520across%2520frames.%2520Project%2520page%253A%250Ahttps%253A//bujiazi.github.io/light-a-video.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08590v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Light-A-Video%3A%20Training-free%20Video%20Relighting%20via%20Progressive%20Light%0A%20%20Fusion&entry.906535625=Yujie%20Zhou%20and%20Jiazi%20Bu%20and%20Pengyang%20Ling%20and%20Pan%20Zhang%20and%20Tong%20Wu%20and%20Qidong%20Huang%20and%20Jinsong%20Li%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Zang%20and%20Yuhang%20Cao%20and%20Anyi%20Rao%20and%20Jiaqi%20Wang%20and%20Li%20Niu&entry.1292438233=%20%20Recent%20advancements%20in%20image%20relighting%20models%2C%20driven%20by%20large-scale%0Adatasets%20and%20pre-trained%20diffusion%20models%2C%20have%20enabled%20the%20imposition%20of%0Aconsistent%20lighting.%20However%2C%20video%20relighting%20still%20lags%2C%20primarily%20due%20to%20the%0Aexcessive%20training%20costs%20and%20the%20scarcity%20of%20diverse%2C%20high-quality%20video%0Arelighting%20datasets.%20A%20simple%20application%20of%20image%20relighting%20models%20on%20a%0Aframe-by-frame%20basis%20leads%20to%20several%20issues%3A%20lighting%20source%20inconsistency%20and%0Arelighted%20appearance%20inconsistency%2C%20resulting%20in%20flickers%20in%20the%20generated%0Avideos.%20In%20this%20work%2C%20we%20propose%20Light-A-Video%2C%20a%20training-free%20approach%20to%0Aachieve%20temporally%20smooth%20video%20relighting.%20Adapted%20from%20image%20relighting%0Amodels%2C%20Light-A-Video%20introduces%20two%20key%20techniques%20to%20enhance%20lighting%0Aconsistency.%20First%2C%20we%20design%20a%20Consistent%20Light%20Attention%20%28CLA%29%20module%2C%20which%0Aenhances%20cross-frame%20interactions%20within%20the%20self-attention%20layers%20to%20stabilize%0Athe%20generation%20of%20the%20background%20lighting%20source.%20Second%2C%20leveraging%20the%0Aphysical%20principle%20of%20light%20transport%20independence%2C%20we%20apply%20linear%20blending%0Abetween%20the%20source%20video%27s%20appearance%20and%20the%20relighted%20appearance%2C%20using%20a%0AProgressive%20Light%20Fusion%20%28PLF%29%20strategy%20to%20ensure%20smooth%20temporal%20transitions%0Ain%20illumination.%20Experiments%20show%20that%20Light-A-Video%20improves%20the%20temporal%0Aconsistency%20of%20relighted%20video%20while%20maintaining%20the%20image%20quality%2C%20ensuring%0Acoherent%20lighting%20transitions%20across%20frames.%20Project%20page%3A%0Ahttps%3A//bujiazi.github.io/light-a-video.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08590v1&entry.124074799=Read"},
{"title": "Measuring Diversity in Synthetic Datasets", "author": "Yuchang Zhu and Huizhe Zhang and Bingzhe Wu and Jintang Li and Zibin Zheng and Peilin Zhao and Liang Chen and Yatao Bian", "abstract": "  Large language models (LLMs) are widely adopted to generate synthetic\ndatasets for various natural language processing (NLP) tasks, such as text\nclassification and summarization. However, accurately measuring the diversity\nof these synthetic datasets-an aspect crucial for robust model\nperformance-remains a significant challenge. In this paper, we introduce\nDCScore, a novel method for measuring synthetic dataset diversity from a\nclassification perspective. Specifically, DCScore formulates diversity\nevaluation as a sample classification task, leveraging mutual relationships\namong samples. We further provide theoretical verification of the\ndiversity-related axioms satisfied by DCScore, highlighting its role as a\nprincipled diversity evaluation method. Experimental results on synthetic\ndatasets reveal that DCScore enjoys a stronger correlation with multiple\ndiversity pseudo-truths of evaluated datasets, underscoring its effectiveness.\nMoreover, both empirical and theoretical evidence demonstrate that DCScore\nsubstantially reduces computational costs compared to existing approaches. Code\nis available at: https://github.com/BlueWhaleLab/DCScore.\n", "link": "http://arxiv.org/abs/2502.08512v1", "date": "2025-02-12", "relevancy": 2.4136, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4902}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4902}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Measuring%20Diversity%20in%20Synthetic%20Datasets&body=Title%3A%20Measuring%20Diversity%20in%20Synthetic%20Datasets%0AAuthor%3A%20Yuchang%20Zhu%20and%20Huizhe%20Zhang%20and%20Bingzhe%20Wu%20and%20Jintang%20Li%20and%20Zibin%20Zheng%20and%20Peilin%20Zhao%20and%20Liang%20Chen%20and%20Yatao%20Bian%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20widely%20adopted%20to%20generate%20synthetic%0Adatasets%20for%20various%20natural%20language%20processing%20%28NLP%29%20tasks%2C%20such%20as%20text%0Aclassification%20and%20summarization.%20However%2C%20accurately%20measuring%20the%20diversity%0Aof%20these%20synthetic%20datasets-an%20aspect%20crucial%20for%20robust%20model%0Aperformance-remains%20a%20significant%20challenge.%20In%20this%20paper%2C%20we%20introduce%0ADCScore%2C%20a%20novel%20method%20for%20measuring%20synthetic%20dataset%20diversity%20from%20a%0Aclassification%20perspective.%20Specifically%2C%20DCScore%20formulates%20diversity%0Aevaluation%20as%20a%20sample%20classification%20task%2C%20leveraging%20mutual%20relationships%0Aamong%20samples.%20We%20further%20provide%20theoretical%20verification%20of%20the%0Adiversity-related%20axioms%20satisfied%20by%20DCScore%2C%20highlighting%20its%20role%20as%20a%0Aprincipled%20diversity%20evaluation%20method.%20Experimental%20results%20on%20synthetic%0Adatasets%20reveal%20that%20DCScore%20enjoys%20a%20stronger%20correlation%20with%20multiple%0Adiversity%20pseudo-truths%20of%20evaluated%20datasets%2C%20underscoring%20its%20effectiveness.%0AMoreover%2C%20both%20empirical%20and%20theoretical%20evidence%20demonstrate%20that%20DCScore%0Asubstantially%20reduces%20computational%20costs%20compared%20to%20existing%20approaches.%20Code%0Ais%20available%20at%3A%20https%3A//github.com/BlueWhaleLab/DCScore.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08512v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeasuring%2520Diversity%2520in%2520Synthetic%2520Datasets%26entry.906535625%3DYuchang%2520Zhu%2520and%2520Huizhe%2520Zhang%2520and%2520Bingzhe%2520Wu%2520and%2520Jintang%2520Li%2520and%2520Zibin%2520Zheng%2520and%2520Peilin%2520Zhao%2520and%2520Liang%2520Chen%2520and%2520Yatao%2520Bian%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520widely%2520adopted%2520to%2520generate%2520synthetic%250Adatasets%2520for%2520various%2520natural%2520language%2520processing%2520%2528NLP%2529%2520tasks%252C%2520such%2520as%2520text%250Aclassification%2520and%2520summarization.%2520However%252C%2520accurately%2520measuring%2520the%2520diversity%250Aof%2520these%2520synthetic%2520datasets-an%2520aspect%2520crucial%2520for%2520robust%2520model%250Aperformance-remains%2520a%2520significant%2520challenge.%2520In%2520this%2520paper%252C%2520we%2520introduce%250ADCScore%252C%2520a%2520novel%2520method%2520for%2520measuring%2520synthetic%2520dataset%2520diversity%2520from%2520a%250Aclassification%2520perspective.%2520Specifically%252C%2520DCScore%2520formulates%2520diversity%250Aevaluation%2520as%2520a%2520sample%2520classification%2520task%252C%2520leveraging%2520mutual%2520relationships%250Aamong%2520samples.%2520We%2520further%2520provide%2520theoretical%2520verification%2520of%2520the%250Adiversity-related%2520axioms%2520satisfied%2520by%2520DCScore%252C%2520highlighting%2520its%2520role%2520as%2520a%250Aprincipled%2520diversity%2520evaluation%2520method.%2520Experimental%2520results%2520on%2520synthetic%250Adatasets%2520reveal%2520that%2520DCScore%2520enjoys%2520a%2520stronger%2520correlation%2520with%2520multiple%250Adiversity%2520pseudo-truths%2520of%2520evaluated%2520datasets%252C%2520underscoring%2520its%2520effectiveness.%250AMoreover%252C%2520both%2520empirical%2520and%2520theoretical%2520evidence%2520demonstrate%2520that%2520DCScore%250Asubstantially%2520reduces%2520computational%2520costs%2520compared%2520to%2520existing%2520approaches.%2520Code%250Ais%2520available%2520at%253A%2520https%253A//github.com/BlueWhaleLab/DCScore.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08512v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Measuring%20Diversity%20in%20Synthetic%20Datasets&entry.906535625=Yuchang%20Zhu%20and%20Huizhe%20Zhang%20and%20Bingzhe%20Wu%20and%20Jintang%20Li%20and%20Zibin%20Zheng%20and%20Peilin%20Zhao%20and%20Liang%20Chen%20and%20Yatao%20Bian&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20widely%20adopted%20to%20generate%20synthetic%0Adatasets%20for%20various%20natural%20language%20processing%20%28NLP%29%20tasks%2C%20such%20as%20text%0Aclassification%20and%20summarization.%20However%2C%20accurately%20measuring%20the%20diversity%0Aof%20these%20synthetic%20datasets-an%20aspect%20crucial%20for%20robust%20model%0Aperformance-remains%20a%20significant%20challenge.%20In%20this%20paper%2C%20we%20introduce%0ADCScore%2C%20a%20novel%20method%20for%20measuring%20synthetic%20dataset%20diversity%20from%20a%0Aclassification%20perspective.%20Specifically%2C%20DCScore%20formulates%20diversity%0Aevaluation%20as%20a%20sample%20classification%20task%2C%20leveraging%20mutual%20relationships%0Aamong%20samples.%20We%20further%20provide%20theoretical%20verification%20of%20the%0Adiversity-related%20axioms%20satisfied%20by%20DCScore%2C%20highlighting%20its%20role%20as%20a%0Aprincipled%20diversity%20evaluation%20method.%20Experimental%20results%20on%20synthetic%0Adatasets%20reveal%20that%20DCScore%20enjoys%20a%20stronger%20correlation%20with%20multiple%0Adiversity%20pseudo-truths%20of%20evaluated%20datasets%2C%20underscoring%20its%20effectiveness.%0AMoreover%2C%20both%20empirical%20and%20theoretical%20evidence%20demonstrate%20that%20DCScore%0Asubstantially%20reduces%20computational%20costs%20compared%20to%20existing%20approaches.%20Code%0Ais%20available%20at%3A%20https%3A//github.com/BlueWhaleLab/DCScore.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08512v1&entry.124074799=Read"},
{"title": "Fostering Appropriate Reliance on Large Language Models: The Role of\n  Explanations, Sources, and Inconsistencies", "author": "Sunnie S. Y. Kim and Jennifer Wortman Vaughan and Q. Vera Liao and Tania Lombrozo and Olga Russakovsky", "abstract": "  Large language models (LLMs) can produce erroneous responses that sound\nfluent and convincing, raising the risk that users will rely on these responses\nas if they were correct. Mitigating such overreliance is a key challenge.\nThrough a think-aloud study in which participants use an LLM-infused\napplication to answer objective questions, we identify several features of LLM\nresponses that shape users' reliance: explanations (supporting details for\nanswers), inconsistencies in explanations, and sources. Through a large-scale,\npre-registered, controlled experiment (N=308), we isolate and study the effects\nof these features on users' reliance, accuracy, and other measures. We find\nthat the presence of explanations increases reliance on both correct and\nincorrect responses. However, we observe less reliance on incorrect responses\nwhen sources are provided or when explanations exhibit inconsistencies. We\ndiscuss the implications of these findings for fostering appropriate reliance\non LLMs.\n", "link": "http://arxiv.org/abs/2502.08554v1", "date": "2025-02-12", "relevancy": 2.4041, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4858}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4858}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4709}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fostering%20Appropriate%20Reliance%20on%20Large%20Language%20Models%3A%20The%20Role%20of%0A%20%20Explanations%2C%20Sources%2C%20and%20Inconsistencies&body=Title%3A%20Fostering%20Appropriate%20Reliance%20on%20Large%20Language%20Models%3A%20The%20Role%20of%0A%20%20Explanations%2C%20Sources%2C%20and%20Inconsistencies%0AAuthor%3A%20Sunnie%20S.%20Y.%20Kim%20and%20Jennifer%20Wortman%20Vaughan%20and%20Q.%20Vera%20Liao%20and%20Tania%20Lombrozo%20and%20Olga%20Russakovsky%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20can%20produce%20erroneous%20responses%20that%20sound%0Afluent%20and%20convincing%2C%20raising%20the%20risk%20that%20users%20will%20rely%20on%20these%20responses%0Aas%20if%20they%20were%20correct.%20Mitigating%20such%20overreliance%20is%20a%20key%20challenge.%0AThrough%20a%20think-aloud%20study%20in%20which%20participants%20use%20an%20LLM-infused%0Aapplication%20to%20answer%20objective%20questions%2C%20we%20identify%20several%20features%20of%20LLM%0Aresponses%20that%20shape%20users%27%20reliance%3A%20explanations%20%28supporting%20details%20for%0Aanswers%29%2C%20inconsistencies%20in%20explanations%2C%20and%20sources.%20Through%20a%20large-scale%2C%0Apre-registered%2C%20controlled%20experiment%20%28N%3D308%29%2C%20we%20isolate%20and%20study%20the%20effects%0Aof%20these%20features%20on%20users%27%20reliance%2C%20accuracy%2C%20and%20other%20measures.%20We%20find%0Athat%20the%20presence%20of%20explanations%20increases%20reliance%20on%20both%20correct%20and%0Aincorrect%20responses.%20However%2C%20we%20observe%20less%20reliance%20on%20incorrect%20responses%0Awhen%20sources%20are%20provided%20or%20when%20explanations%20exhibit%20inconsistencies.%20We%0Adiscuss%20the%20implications%20of%20these%20findings%20for%20fostering%20appropriate%20reliance%0Aon%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08554v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFostering%2520Appropriate%2520Reliance%2520on%2520Large%2520Language%2520Models%253A%2520The%2520Role%2520of%250A%2520%2520Explanations%252C%2520Sources%252C%2520and%2520Inconsistencies%26entry.906535625%3DSunnie%2520S.%2520Y.%2520Kim%2520and%2520Jennifer%2520Wortman%2520Vaughan%2520and%2520Q.%2520Vera%2520Liao%2520and%2520Tania%2520Lombrozo%2520and%2520Olga%2520Russakovsky%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520can%2520produce%2520erroneous%2520responses%2520that%2520sound%250Afluent%2520and%2520convincing%252C%2520raising%2520the%2520risk%2520that%2520users%2520will%2520rely%2520on%2520these%2520responses%250Aas%2520if%2520they%2520were%2520correct.%2520Mitigating%2520such%2520overreliance%2520is%2520a%2520key%2520challenge.%250AThrough%2520a%2520think-aloud%2520study%2520in%2520which%2520participants%2520use%2520an%2520LLM-infused%250Aapplication%2520to%2520answer%2520objective%2520questions%252C%2520we%2520identify%2520several%2520features%2520of%2520LLM%250Aresponses%2520that%2520shape%2520users%2527%2520reliance%253A%2520explanations%2520%2528supporting%2520details%2520for%250Aanswers%2529%252C%2520inconsistencies%2520in%2520explanations%252C%2520and%2520sources.%2520Through%2520a%2520large-scale%252C%250Apre-registered%252C%2520controlled%2520experiment%2520%2528N%253D308%2529%252C%2520we%2520isolate%2520and%2520study%2520the%2520effects%250Aof%2520these%2520features%2520on%2520users%2527%2520reliance%252C%2520accuracy%252C%2520and%2520other%2520measures.%2520We%2520find%250Athat%2520the%2520presence%2520of%2520explanations%2520increases%2520reliance%2520on%2520both%2520correct%2520and%250Aincorrect%2520responses.%2520However%252C%2520we%2520observe%2520less%2520reliance%2520on%2520incorrect%2520responses%250Awhen%2520sources%2520are%2520provided%2520or%2520when%2520explanations%2520exhibit%2520inconsistencies.%2520We%250Adiscuss%2520the%2520implications%2520of%2520these%2520findings%2520for%2520fostering%2520appropriate%2520reliance%250Aon%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08554v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fostering%20Appropriate%20Reliance%20on%20Large%20Language%20Models%3A%20The%20Role%20of%0A%20%20Explanations%2C%20Sources%2C%20and%20Inconsistencies&entry.906535625=Sunnie%20S.%20Y.%20Kim%20and%20Jennifer%20Wortman%20Vaughan%20and%20Q.%20Vera%20Liao%20and%20Tania%20Lombrozo%20and%20Olga%20Russakovsky&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20can%20produce%20erroneous%20responses%20that%20sound%0Afluent%20and%20convincing%2C%20raising%20the%20risk%20that%20users%20will%20rely%20on%20these%20responses%0Aas%20if%20they%20were%20correct.%20Mitigating%20such%20overreliance%20is%20a%20key%20challenge.%0AThrough%20a%20think-aloud%20study%20in%20which%20participants%20use%20an%20LLM-infused%0Aapplication%20to%20answer%20objective%20questions%2C%20we%20identify%20several%20features%20of%20LLM%0Aresponses%20that%20shape%20users%27%20reliance%3A%20explanations%20%28supporting%20details%20for%0Aanswers%29%2C%20inconsistencies%20in%20explanations%2C%20and%20sources.%20Through%20a%20large-scale%2C%0Apre-registered%2C%20controlled%20experiment%20%28N%3D308%29%2C%20we%20isolate%20and%20study%20the%20effects%0Aof%20these%20features%20on%20users%27%20reliance%2C%20accuracy%2C%20and%20other%20measures.%20We%20find%0Athat%20the%20presence%20of%20explanations%20increases%20reliance%20on%20both%20correct%20and%0Aincorrect%20responses.%20However%2C%20we%20observe%20less%20reliance%20on%20incorrect%20responses%0Awhen%20sources%20are%20provided%20or%20when%20explanations%20exhibit%20inconsistencies.%20We%0Adiscuss%20the%20implications%20of%20these%20findings%20for%20fostering%20appropriate%20reliance%0Aon%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08554v1&entry.124074799=Read"},
{"title": "AR Glulam: Accurate Augmented Reality Using Multiple Fiducial Markers\n  for Glulam Fabrication", "author": "Alexander Htet Kyaw and Arvin Xu and Sasa Zivkovic and Gwyllim Jahn and Cameron Newnham and Nick Van Den Berg", "abstract": "  Recent advancements in Augmented Reality (AR) have demonstrated applications\nin architecture, design, and fabrication. Compared to conventional 2D\nconstruction drawings, AR can be used to superimpose contextual instructions,\ndisplay 3D spatial information and enable on-site engagement. Despite the\npotential of AR, the widespread adoption of the technology in the industry is\nlimited by its precision. Precision is important for projects requiring strict\nconstruction tolerances, design fidelity, and fabrication feedback. For\nexample, the manufacturing of glulam beams requires tolerances of less than\n2mm. The goal of this project is to explore the industrial application of using\nmultiple fiducial markers for high-precision AR fabrication. While the method\nhas been validated in lab settings with a precision of 0.97, this paper focuses\non fabricating glulam beams in a factory setting with an industry manufacturer,\nUnalam Factory.\n", "link": "http://arxiv.org/abs/2502.08566v1", "date": "2025-02-12", "relevancy": 2.374, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5061}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4713}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AR%20Glulam%3A%20Accurate%20Augmented%20Reality%20Using%20Multiple%20Fiducial%20Markers%0A%20%20for%20Glulam%20Fabrication&body=Title%3A%20AR%20Glulam%3A%20Accurate%20Augmented%20Reality%20Using%20Multiple%20Fiducial%20Markers%0A%20%20for%20Glulam%20Fabrication%0AAuthor%3A%20Alexander%20Htet%20Kyaw%20and%20Arvin%20Xu%20and%20Sasa%20Zivkovic%20and%20Gwyllim%20Jahn%20and%20Cameron%20Newnham%20and%20Nick%20Van%20Den%20Berg%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Augmented%20Reality%20%28AR%29%20have%20demonstrated%20applications%0Ain%20architecture%2C%20design%2C%20and%20fabrication.%20Compared%20to%20conventional%202D%0Aconstruction%20drawings%2C%20AR%20can%20be%20used%20to%20superimpose%20contextual%20instructions%2C%0Adisplay%203D%20spatial%20information%20and%20enable%20on-site%20engagement.%20Despite%20the%0Apotential%20of%20AR%2C%20the%20widespread%20adoption%20of%20the%20technology%20in%20the%20industry%20is%0Alimited%20by%20its%20precision.%20Precision%20is%20important%20for%20projects%20requiring%20strict%0Aconstruction%20tolerances%2C%20design%20fidelity%2C%20and%20fabrication%20feedback.%20For%0Aexample%2C%20the%20manufacturing%20of%20glulam%20beams%20requires%20tolerances%20of%20less%20than%0A2mm.%20The%20goal%20of%20this%20project%20is%20to%20explore%20the%20industrial%20application%20of%20using%0Amultiple%20fiducial%20markers%20for%20high-precision%20AR%20fabrication.%20While%20the%20method%0Ahas%20been%20validated%20in%20lab%20settings%20with%20a%20precision%20of%200.97%2C%20this%20paper%20focuses%0Aon%20fabricating%20glulam%20beams%20in%20a%20factory%20setting%20with%20an%20industry%20manufacturer%2C%0AUnalam%20Factory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08566v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAR%2520Glulam%253A%2520Accurate%2520Augmented%2520Reality%2520Using%2520Multiple%2520Fiducial%2520Markers%250A%2520%2520for%2520Glulam%2520Fabrication%26entry.906535625%3DAlexander%2520Htet%2520Kyaw%2520and%2520Arvin%2520Xu%2520and%2520Sasa%2520Zivkovic%2520and%2520Gwyllim%2520Jahn%2520and%2520Cameron%2520Newnham%2520and%2520Nick%2520Van%2520Den%2520Berg%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Augmented%2520Reality%2520%2528AR%2529%2520have%2520demonstrated%2520applications%250Ain%2520architecture%252C%2520design%252C%2520and%2520fabrication.%2520Compared%2520to%2520conventional%25202D%250Aconstruction%2520drawings%252C%2520AR%2520can%2520be%2520used%2520to%2520superimpose%2520contextual%2520instructions%252C%250Adisplay%25203D%2520spatial%2520information%2520and%2520enable%2520on-site%2520engagement.%2520Despite%2520the%250Apotential%2520of%2520AR%252C%2520the%2520widespread%2520adoption%2520of%2520the%2520technology%2520in%2520the%2520industry%2520is%250Alimited%2520by%2520its%2520precision.%2520Precision%2520is%2520important%2520for%2520projects%2520requiring%2520strict%250Aconstruction%2520tolerances%252C%2520design%2520fidelity%252C%2520and%2520fabrication%2520feedback.%2520For%250Aexample%252C%2520the%2520manufacturing%2520of%2520glulam%2520beams%2520requires%2520tolerances%2520of%2520less%2520than%250A2mm.%2520The%2520goal%2520of%2520this%2520project%2520is%2520to%2520explore%2520the%2520industrial%2520application%2520of%2520using%250Amultiple%2520fiducial%2520markers%2520for%2520high-precision%2520AR%2520fabrication.%2520While%2520the%2520method%250Ahas%2520been%2520validated%2520in%2520lab%2520settings%2520with%2520a%2520precision%2520of%25200.97%252C%2520this%2520paper%2520focuses%250Aon%2520fabricating%2520glulam%2520beams%2520in%2520a%2520factory%2520setting%2520with%2520an%2520industry%2520manufacturer%252C%250AUnalam%2520Factory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08566v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AR%20Glulam%3A%20Accurate%20Augmented%20Reality%20Using%20Multiple%20Fiducial%20Markers%0A%20%20for%20Glulam%20Fabrication&entry.906535625=Alexander%20Htet%20Kyaw%20and%20Arvin%20Xu%20and%20Sasa%20Zivkovic%20and%20Gwyllim%20Jahn%20and%20Cameron%20Newnham%20and%20Nick%20Van%20Den%20Berg&entry.1292438233=%20%20Recent%20advancements%20in%20Augmented%20Reality%20%28AR%29%20have%20demonstrated%20applications%0Ain%20architecture%2C%20design%2C%20and%20fabrication.%20Compared%20to%20conventional%202D%0Aconstruction%20drawings%2C%20AR%20can%20be%20used%20to%20superimpose%20contextual%20instructions%2C%0Adisplay%203D%20spatial%20information%20and%20enable%20on-site%20engagement.%20Despite%20the%0Apotential%20of%20AR%2C%20the%20widespread%20adoption%20of%20the%20technology%20in%20the%20industry%20is%0Alimited%20by%20its%20precision.%20Precision%20is%20important%20for%20projects%20requiring%20strict%0Aconstruction%20tolerances%2C%20design%20fidelity%2C%20and%20fabrication%20feedback.%20For%0Aexample%2C%20the%20manufacturing%20of%20glulam%20beams%20requires%20tolerances%20of%20less%20than%0A2mm.%20The%20goal%20of%20this%20project%20is%20to%20explore%20the%20industrial%20application%20of%20using%0Amultiple%20fiducial%20markers%20for%20high-precision%20AR%20fabrication.%20While%20the%20method%0Ahas%20been%20validated%20in%20lab%20settings%20with%20a%20precision%20of%200.97%2C%20this%20paper%20focuses%0Aon%20fabricating%20glulam%20beams%20in%20a%20factory%20setting%20with%20an%20industry%20manufacturer%2C%0AUnalam%20Factory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08566v1&entry.124074799=Read"},
{"title": "TimeSuite: Improving MLLMs for Long Video Understanding via Grounded\n  Tuning", "author": "Xiangyu Zeng and Kunchang Li and Chenting Wang and Xinhao Li and Tianxiang Jiang and Ziang Yan and Songze Li and Yansong Shi and Zhengrong Yue and Yi Wang and Yali Wang and Yu Qiao and Limin Wang", "abstract": "  Multimodal Large Language Models (MLLMs) have demonstrated impressive\nperformance in short video understanding. However, understanding long-form\nvideos still remains challenging for MLLMs. This paper proposes TimeSuite, a\ncollection of new designs to adapt the existing short-form video MLLMs for long\nvideo understanding, including a simple yet efficient framework to process long\nvideo sequence, a high-quality video dataset for grounded tuning of MLLMs, and\na carefully-designed instruction tuning task to explicitly incorporate the\ngrounding supervision in the traditional QA format. Specifically, based on\nVideoChat, we propose our long-video MLLM, coined as VideoChat-T, by\nimplementing a token shuffling to compress long video tokens and introducing\nTemporal Adaptive Position Encoding (TAPE) to enhance the temporal awareness of\nvisual representation. Meanwhile, we introduce the TimePro, a comprehensive\ngrounding-centric instruction tuning dataset composed of 9 tasks and 349k\nhigh-quality grounded annotations. Notably, we design a new instruction tuning\ntask type, called Temporal Grounded Caption, to peform detailed video\ndescriptions with the corresponding time stamps prediction. This explicit\ntemporal location prediction will guide MLLM to correctly attend on the visual\ncontent when generating description, and thus reduce the hallucination risk\ncaused by the LLMs. Experimental results demonstrate that our TimeSuite\nprovides a successful solution to enhance the long video understanding\ncapability of short-form MLLM, achieving improvement of 5.6% and 6.8% on the\nbenchmarks of Egoschema and VideoMME, respectively. In addition, VideoChat-T\nexhibits robust zero-shot temporal grounding capabilities, significantly\noutperforming the existing state-of-the-art MLLMs. After fine-tuning, it\nperforms on par with the traditional supervised expert models.\n", "link": "http://arxiv.org/abs/2410.19702v2", "date": "2025-02-12", "relevancy": 2.3365, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5989}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5748}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TimeSuite%3A%20Improving%20MLLMs%20for%20Long%20Video%20Understanding%20via%20Grounded%0A%20%20Tuning&body=Title%3A%20TimeSuite%3A%20Improving%20MLLMs%20for%20Long%20Video%20Understanding%20via%20Grounded%0A%20%20Tuning%0AAuthor%3A%20Xiangyu%20Zeng%20and%20Kunchang%20Li%20and%20Chenting%20Wang%20and%20Xinhao%20Li%20and%20Tianxiang%20Jiang%20and%20Ziang%20Yan%20and%20Songze%20Li%20and%20Yansong%20Shi%20and%20Zhengrong%20Yue%20and%20Yi%20Wang%20and%20Yali%20Wang%20and%20Yu%20Qiao%20and%20Limin%20Wang%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20impressive%0Aperformance%20in%20short%20video%20understanding.%20However%2C%20understanding%20long-form%0Avideos%20still%20remains%20challenging%20for%20MLLMs.%20This%20paper%20proposes%20TimeSuite%2C%20a%0Acollection%20of%20new%20designs%20to%20adapt%20the%20existing%20short-form%20video%20MLLMs%20for%20long%0Avideo%20understanding%2C%20including%20a%20simple%20yet%20efficient%20framework%20to%20process%20long%0Avideo%20sequence%2C%20a%20high-quality%20video%20dataset%20for%20grounded%20tuning%20of%20MLLMs%2C%20and%0Aa%20carefully-designed%20instruction%20tuning%20task%20to%20explicitly%20incorporate%20the%0Agrounding%20supervision%20in%20the%20traditional%20QA%20format.%20Specifically%2C%20based%20on%0AVideoChat%2C%20we%20propose%20our%20long-video%20MLLM%2C%20coined%20as%20VideoChat-T%2C%20by%0Aimplementing%20a%20token%20shuffling%20to%20compress%20long%20video%20tokens%20and%20introducing%0ATemporal%20Adaptive%20Position%20Encoding%20%28TAPE%29%20to%20enhance%20the%20temporal%20awareness%20of%0Avisual%20representation.%20Meanwhile%2C%20we%20introduce%20the%20TimePro%2C%20a%20comprehensive%0Agrounding-centric%20instruction%20tuning%20dataset%20composed%20of%209%20tasks%20and%20349k%0Ahigh-quality%20grounded%20annotations.%20Notably%2C%20we%20design%20a%20new%20instruction%20tuning%0Atask%20type%2C%20called%20Temporal%20Grounded%20Caption%2C%20to%20peform%20detailed%20video%0Adescriptions%20with%20the%20corresponding%20time%20stamps%20prediction.%20This%20explicit%0Atemporal%20location%20prediction%20will%20guide%20MLLM%20to%20correctly%20attend%20on%20the%20visual%0Acontent%20when%20generating%20description%2C%20and%20thus%20reduce%20the%20hallucination%20risk%0Acaused%20by%20the%20LLMs.%20Experimental%20results%20demonstrate%20that%20our%20TimeSuite%0Aprovides%20a%20successful%20solution%20to%20enhance%20the%20long%20video%20understanding%0Acapability%20of%20short-form%20MLLM%2C%20achieving%20improvement%20of%205.6%25%20and%206.8%25%20on%20the%0Abenchmarks%20of%20Egoschema%20and%20VideoMME%2C%20respectively.%20In%20addition%2C%20VideoChat-T%0Aexhibits%20robust%20zero-shot%20temporal%20grounding%20capabilities%2C%20significantly%0Aoutperforming%20the%20existing%20state-of-the-art%20MLLMs.%20After%20fine-tuning%2C%20it%0Aperforms%20on%20par%20with%20the%20traditional%20supervised%20expert%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19702v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTimeSuite%253A%2520Improving%2520MLLMs%2520for%2520Long%2520Video%2520Understanding%2520via%2520Grounded%250A%2520%2520Tuning%26entry.906535625%3DXiangyu%2520Zeng%2520and%2520Kunchang%2520Li%2520and%2520Chenting%2520Wang%2520and%2520Xinhao%2520Li%2520and%2520Tianxiang%2520Jiang%2520and%2520Ziang%2520Yan%2520and%2520Songze%2520Li%2520and%2520Yansong%2520Shi%2520and%2520Zhengrong%2520Yue%2520and%2520Yi%2520Wang%2520and%2520Yali%2520Wang%2520and%2520Yu%2520Qiao%2520and%2520Limin%2520Wang%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520impressive%250Aperformance%2520in%2520short%2520video%2520understanding.%2520However%252C%2520understanding%2520long-form%250Avideos%2520still%2520remains%2520challenging%2520for%2520MLLMs.%2520This%2520paper%2520proposes%2520TimeSuite%252C%2520a%250Acollection%2520of%2520new%2520designs%2520to%2520adapt%2520the%2520existing%2520short-form%2520video%2520MLLMs%2520for%2520long%250Avideo%2520understanding%252C%2520including%2520a%2520simple%2520yet%2520efficient%2520framework%2520to%2520process%2520long%250Avideo%2520sequence%252C%2520a%2520high-quality%2520video%2520dataset%2520for%2520grounded%2520tuning%2520of%2520MLLMs%252C%2520and%250Aa%2520carefully-designed%2520instruction%2520tuning%2520task%2520to%2520explicitly%2520incorporate%2520the%250Agrounding%2520supervision%2520in%2520the%2520traditional%2520QA%2520format.%2520Specifically%252C%2520based%2520on%250AVideoChat%252C%2520we%2520propose%2520our%2520long-video%2520MLLM%252C%2520coined%2520as%2520VideoChat-T%252C%2520by%250Aimplementing%2520a%2520token%2520shuffling%2520to%2520compress%2520long%2520video%2520tokens%2520and%2520introducing%250ATemporal%2520Adaptive%2520Position%2520Encoding%2520%2528TAPE%2529%2520to%2520enhance%2520the%2520temporal%2520awareness%2520of%250Avisual%2520representation.%2520Meanwhile%252C%2520we%2520introduce%2520the%2520TimePro%252C%2520a%2520comprehensive%250Agrounding-centric%2520instruction%2520tuning%2520dataset%2520composed%2520of%25209%2520tasks%2520and%2520349k%250Ahigh-quality%2520grounded%2520annotations.%2520Notably%252C%2520we%2520design%2520a%2520new%2520instruction%2520tuning%250Atask%2520type%252C%2520called%2520Temporal%2520Grounded%2520Caption%252C%2520to%2520peform%2520detailed%2520video%250Adescriptions%2520with%2520the%2520corresponding%2520time%2520stamps%2520prediction.%2520This%2520explicit%250Atemporal%2520location%2520prediction%2520will%2520guide%2520MLLM%2520to%2520correctly%2520attend%2520on%2520the%2520visual%250Acontent%2520when%2520generating%2520description%252C%2520and%2520thus%2520reduce%2520the%2520hallucination%2520risk%250Acaused%2520by%2520the%2520LLMs.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520TimeSuite%250Aprovides%2520a%2520successful%2520solution%2520to%2520enhance%2520the%2520long%2520video%2520understanding%250Acapability%2520of%2520short-form%2520MLLM%252C%2520achieving%2520improvement%2520of%25205.6%2525%2520and%25206.8%2525%2520on%2520the%250Abenchmarks%2520of%2520Egoschema%2520and%2520VideoMME%252C%2520respectively.%2520In%2520addition%252C%2520VideoChat-T%250Aexhibits%2520robust%2520zero-shot%2520temporal%2520grounding%2520capabilities%252C%2520significantly%250Aoutperforming%2520the%2520existing%2520state-of-the-art%2520MLLMs.%2520After%2520fine-tuning%252C%2520it%250Aperforms%2520on%2520par%2520with%2520the%2520traditional%2520supervised%2520expert%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19702v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TimeSuite%3A%20Improving%20MLLMs%20for%20Long%20Video%20Understanding%20via%20Grounded%0A%20%20Tuning&entry.906535625=Xiangyu%20Zeng%20and%20Kunchang%20Li%20and%20Chenting%20Wang%20and%20Xinhao%20Li%20and%20Tianxiang%20Jiang%20and%20Ziang%20Yan%20and%20Songze%20Li%20and%20Yansong%20Shi%20and%20Zhengrong%20Yue%20and%20Yi%20Wang%20and%20Yali%20Wang%20and%20Yu%20Qiao%20and%20Limin%20Wang&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20impressive%0Aperformance%20in%20short%20video%20understanding.%20However%2C%20understanding%20long-form%0Avideos%20still%20remains%20challenging%20for%20MLLMs.%20This%20paper%20proposes%20TimeSuite%2C%20a%0Acollection%20of%20new%20designs%20to%20adapt%20the%20existing%20short-form%20video%20MLLMs%20for%20long%0Avideo%20understanding%2C%20including%20a%20simple%20yet%20efficient%20framework%20to%20process%20long%0Avideo%20sequence%2C%20a%20high-quality%20video%20dataset%20for%20grounded%20tuning%20of%20MLLMs%2C%20and%0Aa%20carefully-designed%20instruction%20tuning%20task%20to%20explicitly%20incorporate%20the%0Agrounding%20supervision%20in%20the%20traditional%20QA%20format.%20Specifically%2C%20based%20on%0AVideoChat%2C%20we%20propose%20our%20long-video%20MLLM%2C%20coined%20as%20VideoChat-T%2C%20by%0Aimplementing%20a%20token%20shuffling%20to%20compress%20long%20video%20tokens%20and%20introducing%0ATemporal%20Adaptive%20Position%20Encoding%20%28TAPE%29%20to%20enhance%20the%20temporal%20awareness%20of%0Avisual%20representation.%20Meanwhile%2C%20we%20introduce%20the%20TimePro%2C%20a%20comprehensive%0Agrounding-centric%20instruction%20tuning%20dataset%20composed%20of%209%20tasks%20and%20349k%0Ahigh-quality%20grounded%20annotations.%20Notably%2C%20we%20design%20a%20new%20instruction%20tuning%0Atask%20type%2C%20called%20Temporal%20Grounded%20Caption%2C%20to%20peform%20detailed%20video%0Adescriptions%20with%20the%20corresponding%20time%20stamps%20prediction.%20This%20explicit%0Atemporal%20location%20prediction%20will%20guide%20MLLM%20to%20correctly%20attend%20on%20the%20visual%0Acontent%20when%20generating%20description%2C%20and%20thus%20reduce%20the%20hallucination%20risk%0Acaused%20by%20the%20LLMs.%20Experimental%20results%20demonstrate%20that%20our%20TimeSuite%0Aprovides%20a%20successful%20solution%20to%20enhance%20the%20long%20video%20understanding%0Acapability%20of%20short-form%20MLLM%2C%20achieving%20improvement%20of%205.6%25%20and%206.8%25%20on%20the%0Abenchmarks%20of%20Egoschema%20and%20VideoMME%2C%20respectively.%20In%20addition%2C%20VideoChat-T%0Aexhibits%20robust%20zero-shot%20temporal%20grounding%20capabilities%2C%20significantly%0Aoutperforming%20the%20existing%20state-of-the-art%20MLLMs.%20After%20fine-tuning%2C%20it%0Aperforms%20on%20par%20with%20the%20traditional%20supervised%20expert%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19702v2&entry.124074799=Read"},
{"title": "LLMs can implicitly learn from mistakes in-context", "author": "Lisa Alazraki and Maximilian Mozes and Jon Ander Campos and Yi Chern Tan and Marek Rei and Max Bartolo", "abstract": "  Learning from mistakes is a fundamental feature of human intelligence.\nPrevious work has shown that Large Language Models (LLMs) can also learn from\nincorrect answers when provided with a comprehensive rationale detailing why an\nanswer is wrong or how to correct it. In this work, we examine whether LLMs can\nlearn from mistakes in mathematical reasoning tasks when these explanations are\nnot provided. We investigate if LLMs are able to implicitly infer such\nrationales simply from observing both incorrect and correct answers.\nSurprisingly, we find that LLMs perform better, on average, when rationales are\neliminated from the context and incorrect answers are simply shown alongside\ncorrect ones. This approach also substantially outperforms chain-of-thought\nprompting in our evaluations. We show that these results are consistent across\nLLMs of different sizes and varying reasoning abilities. Further, we carry out\nan in-depth analysis, and show that prompting with both wrong and correct\nanswers leads to greater performance and better generalisation than introducing\nadditional, more diverse question-answer pairs into the context. Finally, we\nshow that new rationales generated by models that have only observed incorrect\nand correct answers are scored equally as highly by humans as those produced\nwith the aid of exemplar rationales. Our results demonstrate that LLMs are\nindeed capable of in-context implicit learning.\n", "link": "http://arxiv.org/abs/2502.08550v1", "date": "2025-02-12", "relevancy": 2.3293, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4661}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4661}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%20can%20implicitly%20learn%20from%20mistakes%20in-context&body=Title%3A%20LLMs%20can%20implicitly%20learn%20from%20mistakes%20in-context%0AAuthor%3A%20Lisa%20Alazraki%20and%20Maximilian%20Mozes%20and%20Jon%20Ander%20Campos%20and%20Yi%20Chern%20Tan%20and%20Marek%20Rei%20and%20Max%20Bartolo%0AAbstract%3A%20%20%20Learning%20from%20mistakes%20is%20a%20fundamental%20feature%20of%20human%20intelligence.%0APrevious%20work%20has%20shown%20that%20Large%20Language%20Models%20%28LLMs%29%20can%20also%20learn%20from%0Aincorrect%20answers%20when%20provided%20with%20a%20comprehensive%20rationale%20detailing%20why%20an%0Aanswer%20is%20wrong%20or%20how%20to%20correct%20it.%20In%20this%20work%2C%20we%20examine%20whether%20LLMs%20can%0Alearn%20from%20mistakes%20in%20mathematical%20reasoning%20tasks%20when%20these%20explanations%20are%0Anot%20provided.%20We%20investigate%20if%20LLMs%20are%20able%20to%20implicitly%20infer%20such%0Arationales%20simply%20from%20observing%20both%20incorrect%20and%20correct%20answers.%0ASurprisingly%2C%20we%20find%20that%20LLMs%20perform%20better%2C%20on%20average%2C%20when%20rationales%20are%0Aeliminated%20from%20the%20context%20and%20incorrect%20answers%20are%20simply%20shown%20alongside%0Acorrect%20ones.%20This%20approach%20also%20substantially%20outperforms%20chain-of-thought%0Aprompting%20in%20our%20evaluations.%20We%20show%20that%20these%20results%20are%20consistent%20across%0ALLMs%20of%20different%20sizes%20and%20varying%20reasoning%20abilities.%20Further%2C%20we%20carry%20out%0Aan%20in-depth%20analysis%2C%20and%20show%20that%20prompting%20with%20both%20wrong%20and%20correct%0Aanswers%20leads%20to%20greater%20performance%20and%20better%20generalisation%20than%20introducing%0Aadditional%2C%20more%20diverse%20question-answer%20pairs%20into%20the%20context.%20Finally%2C%20we%0Ashow%20that%20new%20rationales%20generated%20by%20models%20that%20have%20only%20observed%20incorrect%0Aand%20correct%20answers%20are%20scored%20equally%20as%20highly%20by%20humans%20as%20those%20produced%0Awith%20the%20aid%20of%20exemplar%20rationales.%20Our%20results%20demonstrate%20that%20LLMs%20are%0Aindeed%20capable%20of%20in-context%20implicit%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08550v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2520can%2520implicitly%2520learn%2520from%2520mistakes%2520in-context%26entry.906535625%3DLisa%2520Alazraki%2520and%2520Maximilian%2520Mozes%2520and%2520Jon%2520Ander%2520Campos%2520and%2520Yi%2520Chern%2520Tan%2520and%2520Marek%2520Rei%2520and%2520Max%2520Bartolo%26entry.1292438233%3D%2520%2520Learning%2520from%2520mistakes%2520is%2520a%2520fundamental%2520feature%2520of%2520human%2520intelligence.%250APrevious%2520work%2520has%2520shown%2520that%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520can%2520also%2520learn%2520from%250Aincorrect%2520answers%2520when%2520provided%2520with%2520a%2520comprehensive%2520rationale%2520detailing%2520why%2520an%250Aanswer%2520is%2520wrong%2520or%2520how%2520to%2520correct%2520it.%2520In%2520this%2520work%252C%2520we%2520examine%2520whether%2520LLMs%2520can%250Alearn%2520from%2520mistakes%2520in%2520mathematical%2520reasoning%2520tasks%2520when%2520these%2520explanations%2520are%250Anot%2520provided.%2520We%2520investigate%2520if%2520LLMs%2520are%2520able%2520to%2520implicitly%2520infer%2520such%250Arationales%2520simply%2520from%2520observing%2520both%2520incorrect%2520and%2520correct%2520answers.%250ASurprisingly%252C%2520we%2520find%2520that%2520LLMs%2520perform%2520better%252C%2520on%2520average%252C%2520when%2520rationales%2520are%250Aeliminated%2520from%2520the%2520context%2520and%2520incorrect%2520answers%2520are%2520simply%2520shown%2520alongside%250Acorrect%2520ones.%2520This%2520approach%2520also%2520substantially%2520outperforms%2520chain-of-thought%250Aprompting%2520in%2520our%2520evaluations.%2520We%2520show%2520that%2520these%2520results%2520are%2520consistent%2520across%250ALLMs%2520of%2520different%2520sizes%2520and%2520varying%2520reasoning%2520abilities.%2520Further%252C%2520we%2520carry%2520out%250Aan%2520in-depth%2520analysis%252C%2520and%2520show%2520that%2520prompting%2520with%2520both%2520wrong%2520and%2520correct%250Aanswers%2520leads%2520to%2520greater%2520performance%2520and%2520better%2520generalisation%2520than%2520introducing%250Aadditional%252C%2520more%2520diverse%2520question-answer%2520pairs%2520into%2520the%2520context.%2520Finally%252C%2520we%250Ashow%2520that%2520new%2520rationales%2520generated%2520by%2520models%2520that%2520have%2520only%2520observed%2520incorrect%250Aand%2520correct%2520answers%2520are%2520scored%2520equally%2520as%2520highly%2520by%2520humans%2520as%2520those%2520produced%250Awith%2520the%2520aid%2520of%2520exemplar%2520rationales.%2520Our%2520results%2520demonstrate%2520that%2520LLMs%2520are%250Aindeed%2520capable%2520of%2520in-context%2520implicit%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08550v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20can%20implicitly%20learn%20from%20mistakes%20in-context&entry.906535625=Lisa%20Alazraki%20and%20Maximilian%20Mozes%20and%20Jon%20Ander%20Campos%20and%20Yi%20Chern%20Tan%20and%20Marek%20Rei%20and%20Max%20Bartolo&entry.1292438233=%20%20Learning%20from%20mistakes%20is%20a%20fundamental%20feature%20of%20human%20intelligence.%0APrevious%20work%20has%20shown%20that%20Large%20Language%20Models%20%28LLMs%29%20can%20also%20learn%20from%0Aincorrect%20answers%20when%20provided%20with%20a%20comprehensive%20rationale%20detailing%20why%20an%0Aanswer%20is%20wrong%20or%20how%20to%20correct%20it.%20In%20this%20work%2C%20we%20examine%20whether%20LLMs%20can%0Alearn%20from%20mistakes%20in%20mathematical%20reasoning%20tasks%20when%20these%20explanations%20are%0Anot%20provided.%20We%20investigate%20if%20LLMs%20are%20able%20to%20implicitly%20infer%20such%0Arationales%20simply%20from%20observing%20both%20incorrect%20and%20correct%20answers.%0ASurprisingly%2C%20we%20find%20that%20LLMs%20perform%20better%2C%20on%20average%2C%20when%20rationales%20are%0Aeliminated%20from%20the%20context%20and%20incorrect%20answers%20are%20simply%20shown%20alongside%0Acorrect%20ones.%20This%20approach%20also%20substantially%20outperforms%20chain-of-thought%0Aprompting%20in%20our%20evaluations.%20We%20show%20that%20these%20results%20are%20consistent%20across%0ALLMs%20of%20different%20sizes%20and%20varying%20reasoning%20abilities.%20Further%2C%20we%20carry%20out%0Aan%20in-depth%20analysis%2C%20and%20show%20that%20prompting%20with%20both%20wrong%20and%20correct%0Aanswers%20leads%20to%20greater%20performance%20and%20better%20generalisation%20than%20introducing%0Aadditional%2C%20more%20diverse%20question-answer%20pairs%20into%20the%20context.%20Finally%2C%20we%0Ashow%20that%20new%20rationales%20generated%20by%20models%20that%20have%20only%20observed%20incorrect%0Aand%20correct%20answers%20are%20scored%20equally%20as%20highly%20by%20humans%20as%20those%20produced%0Awith%20the%20aid%20of%20exemplar%20rationales.%20Our%20results%20demonstrate%20that%20LLMs%20are%0Aindeed%20capable%20of%20in-context%20implicit%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08550v1&entry.124074799=Read"},
{"title": "Referring Remote Sensing Image Segmentation via Bidirectional Alignment\n  Guided Joint Prediction", "author": "Tianxiang Zhang and Zhaokun Wen and Bo Kong and Kecheng Liu and Yisi Zhang and Peixian Zhuang and Jiangyun Li", "abstract": "  Referring Remote Sensing Image Segmentation (RRSIS) is critical for\necological monitoring, urban planning, and disaster management, requiring\nprecise segmentation of objects in remote sensing imagery guided by textual\ndescriptions. This task is uniquely challenging due to the considerable\nvision-language gap, the high spatial resolution and broad coverage of remote\nsensing imagery with diverse categories and small targets, and the presence of\nclustered, unclear targets with blurred edges. To tackle these issues, we\npropose \\ours, a novel framework designed to bridge the vision-language gap,\nenhance multi-scale feature interaction, and improve fine-grained object\ndifferentiation. Specifically, \\ours introduces: (1) the Bidirectional Spatial\nCorrelation (BSC) for improved vision-language feature alignment, (2) the\nTarget-Background TwinStream Decoder (T-BTD) for precise distinction between\ntargets and non-targets, and (3) the Dual-Modal Object Learning Strategy\n(D-MOLS) for robust multimodal feature reconstruction. Extensive experiments on\nthe benchmark datasets RefSegRS and RRSIS-D demonstrate that \\ours achieves\nstate-of-the-art performance. Specifically, \\ours improves the overall IoU\n(oIoU) by 3.76 percentage points (80.57) and 1.44 percentage points (79.23) on\nthe two datasets, respectively. Additionally, it outperforms previous methods\nin the mean IoU (mIoU) by 5.37 percentage points (67.95) and 1.84 percentage\npoints (66.04), effectively addressing the core challenges of RRSIS with\nenhanced precision and robustness.\n", "link": "http://arxiv.org/abs/2502.08486v1", "date": "2025-02-12", "relevancy": 2.2781, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5858}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5663}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Referring%20Remote%20Sensing%20Image%20Segmentation%20via%20Bidirectional%20Alignment%0A%20%20Guided%20Joint%20Prediction&body=Title%3A%20Referring%20Remote%20Sensing%20Image%20Segmentation%20via%20Bidirectional%20Alignment%0A%20%20Guided%20Joint%20Prediction%0AAuthor%3A%20Tianxiang%20Zhang%20and%20Zhaokun%20Wen%20and%20Bo%20Kong%20and%20Kecheng%20Liu%20and%20Yisi%20Zhang%20and%20Peixian%20Zhuang%20and%20Jiangyun%20Li%0AAbstract%3A%20%20%20Referring%20Remote%20Sensing%20Image%20Segmentation%20%28RRSIS%29%20is%20critical%20for%0Aecological%20monitoring%2C%20urban%20planning%2C%20and%20disaster%20management%2C%20requiring%0Aprecise%20segmentation%20of%20objects%20in%20remote%20sensing%20imagery%20guided%20by%20textual%0Adescriptions.%20This%20task%20is%20uniquely%20challenging%20due%20to%20the%20considerable%0Avision-language%20gap%2C%20the%20high%20spatial%20resolution%20and%20broad%20coverage%20of%20remote%0Asensing%20imagery%20with%20diverse%20categories%20and%20small%20targets%2C%20and%20the%20presence%20of%0Aclustered%2C%20unclear%20targets%20with%20blurred%20edges.%20To%20tackle%20these%20issues%2C%20we%0Apropose%20%5Cours%2C%20a%20novel%20framework%20designed%20to%20bridge%20the%20vision-language%20gap%2C%0Aenhance%20multi-scale%20feature%20interaction%2C%20and%20improve%20fine-grained%20object%0Adifferentiation.%20Specifically%2C%20%5Cours%20introduces%3A%20%281%29%20the%20Bidirectional%20Spatial%0ACorrelation%20%28BSC%29%20for%20improved%20vision-language%20feature%20alignment%2C%20%282%29%20the%0ATarget-Background%20TwinStream%20Decoder%20%28T-BTD%29%20for%20precise%20distinction%20between%0Atargets%20and%20non-targets%2C%20and%20%283%29%20the%20Dual-Modal%20Object%20Learning%20Strategy%0A%28D-MOLS%29%20for%20robust%20multimodal%20feature%20reconstruction.%20Extensive%20experiments%20on%0Athe%20benchmark%20datasets%20RefSegRS%20and%20RRSIS-D%20demonstrate%20that%20%5Cours%20achieves%0Astate-of-the-art%20performance.%20Specifically%2C%20%5Cours%20improves%20the%20overall%20IoU%0A%28oIoU%29%20by%203.76%20percentage%20points%20%2880.57%29%20and%201.44%20percentage%20points%20%2879.23%29%20on%0Athe%20two%20datasets%2C%20respectively.%20Additionally%2C%20it%20outperforms%20previous%20methods%0Ain%20the%20mean%20IoU%20%28mIoU%29%20by%205.37%20percentage%20points%20%2867.95%29%20and%201.84%20percentage%0Apoints%20%2866.04%29%2C%20effectively%20addressing%20the%20core%20challenges%20of%20RRSIS%20with%0Aenhanced%20precision%20and%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08486v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReferring%2520Remote%2520Sensing%2520Image%2520Segmentation%2520via%2520Bidirectional%2520Alignment%250A%2520%2520Guided%2520Joint%2520Prediction%26entry.906535625%3DTianxiang%2520Zhang%2520and%2520Zhaokun%2520Wen%2520and%2520Bo%2520Kong%2520and%2520Kecheng%2520Liu%2520and%2520Yisi%2520Zhang%2520and%2520Peixian%2520Zhuang%2520and%2520Jiangyun%2520Li%26entry.1292438233%3D%2520%2520Referring%2520Remote%2520Sensing%2520Image%2520Segmentation%2520%2528RRSIS%2529%2520is%2520critical%2520for%250Aecological%2520monitoring%252C%2520urban%2520planning%252C%2520and%2520disaster%2520management%252C%2520requiring%250Aprecise%2520segmentation%2520of%2520objects%2520in%2520remote%2520sensing%2520imagery%2520guided%2520by%2520textual%250Adescriptions.%2520This%2520task%2520is%2520uniquely%2520challenging%2520due%2520to%2520the%2520considerable%250Avision-language%2520gap%252C%2520the%2520high%2520spatial%2520resolution%2520and%2520broad%2520coverage%2520of%2520remote%250Asensing%2520imagery%2520with%2520diverse%2520categories%2520and%2520small%2520targets%252C%2520and%2520the%2520presence%2520of%250Aclustered%252C%2520unclear%2520targets%2520with%2520blurred%2520edges.%2520To%2520tackle%2520these%2520issues%252C%2520we%250Apropose%2520%255Cours%252C%2520a%2520novel%2520framework%2520designed%2520to%2520bridge%2520the%2520vision-language%2520gap%252C%250Aenhance%2520multi-scale%2520feature%2520interaction%252C%2520and%2520improve%2520fine-grained%2520object%250Adifferentiation.%2520Specifically%252C%2520%255Cours%2520introduces%253A%2520%25281%2529%2520the%2520Bidirectional%2520Spatial%250ACorrelation%2520%2528BSC%2529%2520for%2520improved%2520vision-language%2520feature%2520alignment%252C%2520%25282%2529%2520the%250ATarget-Background%2520TwinStream%2520Decoder%2520%2528T-BTD%2529%2520for%2520precise%2520distinction%2520between%250Atargets%2520and%2520non-targets%252C%2520and%2520%25283%2529%2520the%2520Dual-Modal%2520Object%2520Learning%2520Strategy%250A%2528D-MOLS%2529%2520for%2520robust%2520multimodal%2520feature%2520reconstruction.%2520Extensive%2520experiments%2520on%250Athe%2520benchmark%2520datasets%2520RefSegRS%2520and%2520RRSIS-D%2520demonstrate%2520that%2520%255Cours%2520achieves%250Astate-of-the-art%2520performance.%2520Specifically%252C%2520%255Cours%2520improves%2520the%2520overall%2520IoU%250A%2528oIoU%2529%2520by%25203.76%2520percentage%2520points%2520%252880.57%2529%2520and%25201.44%2520percentage%2520points%2520%252879.23%2529%2520on%250Athe%2520two%2520datasets%252C%2520respectively.%2520Additionally%252C%2520it%2520outperforms%2520previous%2520methods%250Ain%2520the%2520mean%2520IoU%2520%2528mIoU%2529%2520by%25205.37%2520percentage%2520points%2520%252867.95%2529%2520and%25201.84%2520percentage%250Apoints%2520%252866.04%2529%252C%2520effectively%2520addressing%2520the%2520core%2520challenges%2520of%2520RRSIS%2520with%250Aenhanced%2520precision%2520and%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08486v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Referring%20Remote%20Sensing%20Image%20Segmentation%20via%20Bidirectional%20Alignment%0A%20%20Guided%20Joint%20Prediction&entry.906535625=Tianxiang%20Zhang%20and%20Zhaokun%20Wen%20and%20Bo%20Kong%20and%20Kecheng%20Liu%20and%20Yisi%20Zhang%20and%20Peixian%20Zhuang%20and%20Jiangyun%20Li&entry.1292438233=%20%20Referring%20Remote%20Sensing%20Image%20Segmentation%20%28RRSIS%29%20is%20critical%20for%0Aecological%20monitoring%2C%20urban%20planning%2C%20and%20disaster%20management%2C%20requiring%0Aprecise%20segmentation%20of%20objects%20in%20remote%20sensing%20imagery%20guided%20by%20textual%0Adescriptions.%20This%20task%20is%20uniquely%20challenging%20due%20to%20the%20considerable%0Avision-language%20gap%2C%20the%20high%20spatial%20resolution%20and%20broad%20coverage%20of%20remote%0Asensing%20imagery%20with%20diverse%20categories%20and%20small%20targets%2C%20and%20the%20presence%20of%0Aclustered%2C%20unclear%20targets%20with%20blurred%20edges.%20To%20tackle%20these%20issues%2C%20we%0Apropose%20%5Cours%2C%20a%20novel%20framework%20designed%20to%20bridge%20the%20vision-language%20gap%2C%0Aenhance%20multi-scale%20feature%20interaction%2C%20and%20improve%20fine-grained%20object%0Adifferentiation.%20Specifically%2C%20%5Cours%20introduces%3A%20%281%29%20the%20Bidirectional%20Spatial%0ACorrelation%20%28BSC%29%20for%20improved%20vision-language%20feature%20alignment%2C%20%282%29%20the%0ATarget-Background%20TwinStream%20Decoder%20%28T-BTD%29%20for%20precise%20distinction%20between%0Atargets%20and%20non-targets%2C%20and%20%283%29%20the%20Dual-Modal%20Object%20Learning%20Strategy%0A%28D-MOLS%29%20for%20robust%20multimodal%20feature%20reconstruction.%20Extensive%20experiments%20on%0Athe%20benchmark%20datasets%20RefSegRS%20and%20RRSIS-D%20demonstrate%20that%20%5Cours%20achieves%0Astate-of-the-art%20performance.%20Specifically%2C%20%5Cours%20improves%20the%20overall%20IoU%0A%28oIoU%29%20by%203.76%20percentage%20points%20%2880.57%29%20and%201.44%20percentage%20points%20%2879.23%29%20on%0Athe%20two%20datasets%2C%20respectively.%20Additionally%2C%20it%20outperforms%20previous%20methods%0Ain%20the%20mean%20IoU%20%28mIoU%29%20by%205.37%20percentage%20points%20%2867.95%29%20and%201.84%20percentage%0Apoints%20%2866.04%29%2C%20effectively%20addressing%20the%20core%20challenges%20of%20RRSIS%20with%0Aenhanced%20precision%20and%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08486v1&entry.124074799=Read"},
{"title": "Surface Vision Mamba: Leveraging Bidirectional State Space Model for\n  Efficient Spherical Manifold Representation", "author": "Rongzhao He and Weihao Zheng and Leilei Zhao and Ying Wang and Dalin Zhu and Dan Wu and Bin Hu", "abstract": "  Attention-based methods have demonstrated exceptional performance in\nmodelling long-range dependencies on spherical cortical surfaces, surpassing\ntraditional Geometric Deep Learning (GDL) models. However, their extensive\ninference time and high memory demands pose challenges for application to large\ndatasets with limited computing resources. Inspired by the state space model in\ncomputer vision, we introduce the attention-free Vision Mamba (Vim) to\nspherical surfaces, presenting a domain-agnostic architecture for analyzing\ndata on spherical manifolds. Our method achieves surface patching by\nrepresenting spherical data as a sequence of triangular patches derived from a\nsubdivided icosphere. The proposed Surface Vision Mamba (SiM) is evaluated on\nmultiple neurodevelopmental phenotype regression tasks using cortical surface\nmetrics from neonatal brains. Experimental results demonstrate that SiM\noutperforms both attention- and GDL-based methods, delivering 4.8 times faster\ninference and achieving 91.7% lower memory consumption compared to the Surface\nVision Transformer (SiT) under the Ico-4 grid partitioning. Sensitivity\nanalysis further underscores the potential of SiM to identify subtle cognitive\ndevelopmental patterns. The code is available at\nhttps://github.com/Rongzhao-He/surface-vision-mamba.\n", "link": "http://arxiv.org/abs/2501.14679v2", "date": "2025-02-12", "relevancy": 2.2524, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5642}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5637}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Surface%20Vision%20Mamba%3A%20Leveraging%20Bidirectional%20State%20Space%20Model%20for%0A%20%20Efficient%20Spherical%20Manifold%20Representation&body=Title%3A%20Surface%20Vision%20Mamba%3A%20Leveraging%20Bidirectional%20State%20Space%20Model%20for%0A%20%20Efficient%20Spherical%20Manifold%20Representation%0AAuthor%3A%20Rongzhao%20He%20and%20Weihao%20Zheng%20and%20Leilei%20Zhao%20and%20Ying%20Wang%20and%20Dalin%20Zhu%20and%20Dan%20Wu%20and%20Bin%20Hu%0AAbstract%3A%20%20%20Attention-based%20methods%20have%20demonstrated%20exceptional%20performance%20in%0Amodelling%20long-range%20dependencies%20on%20spherical%20cortical%20surfaces%2C%20surpassing%0Atraditional%20Geometric%20Deep%20Learning%20%28GDL%29%20models.%20However%2C%20their%20extensive%0Ainference%20time%20and%20high%20memory%20demands%20pose%20challenges%20for%20application%20to%20large%0Adatasets%20with%20limited%20computing%20resources.%20Inspired%20by%20the%20state%20space%20model%20in%0Acomputer%20vision%2C%20we%20introduce%20the%20attention-free%20Vision%20Mamba%20%28Vim%29%20to%0Aspherical%20surfaces%2C%20presenting%20a%20domain-agnostic%20architecture%20for%20analyzing%0Adata%20on%20spherical%20manifolds.%20Our%20method%20achieves%20surface%20patching%20by%0Arepresenting%20spherical%20data%20as%20a%20sequence%20of%20triangular%20patches%20derived%20from%20a%0Asubdivided%20icosphere.%20The%20proposed%20Surface%20Vision%20Mamba%20%28SiM%29%20is%20evaluated%20on%0Amultiple%20neurodevelopmental%20phenotype%20regression%20tasks%20using%20cortical%20surface%0Ametrics%20from%20neonatal%20brains.%20Experimental%20results%20demonstrate%20that%20SiM%0Aoutperforms%20both%20attention-%20and%20GDL-based%20methods%2C%20delivering%204.8%20times%20faster%0Ainference%20and%20achieving%2091.7%25%20lower%20memory%20consumption%20compared%20to%20the%20Surface%0AVision%20Transformer%20%28SiT%29%20under%20the%20Ico-4%20grid%20partitioning.%20Sensitivity%0Aanalysis%20further%20underscores%20the%20potential%20of%20SiM%20to%20identify%20subtle%20cognitive%0Adevelopmental%20patterns.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Rongzhao-He/surface-vision-mamba.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.14679v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurface%2520Vision%2520Mamba%253A%2520Leveraging%2520Bidirectional%2520State%2520Space%2520Model%2520for%250A%2520%2520Efficient%2520Spherical%2520Manifold%2520Representation%26entry.906535625%3DRongzhao%2520He%2520and%2520Weihao%2520Zheng%2520and%2520Leilei%2520Zhao%2520and%2520Ying%2520Wang%2520and%2520Dalin%2520Zhu%2520and%2520Dan%2520Wu%2520and%2520Bin%2520Hu%26entry.1292438233%3D%2520%2520Attention-based%2520methods%2520have%2520demonstrated%2520exceptional%2520performance%2520in%250Amodelling%2520long-range%2520dependencies%2520on%2520spherical%2520cortical%2520surfaces%252C%2520surpassing%250Atraditional%2520Geometric%2520Deep%2520Learning%2520%2528GDL%2529%2520models.%2520However%252C%2520their%2520extensive%250Ainference%2520time%2520and%2520high%2520memory%2520demands%2520pose%2520challenges%2520for%2520application%2520to%2520large%250Adatasets%2520with%2520limited%2520computing%2520resources.%2520Inspired%2520by%2520the%2520state%2520space%2520model%2520in%250Acomputer%2520vision%252C%2520we%2520introduce%2520the%2520attention-free%2520Vision%2520Mamba%2520%2528Vim%2529%2520to%250Aspherical%2520surfaces%252C%2520presenting%2520a%2520domain-agnostic%2520architecture%2520for%2520analyzing%250Adata%2520on%2520spherical%2520manifolds.%2520Our%2520method%2520achieves%2520surface%2520patching%2520by%250Arepresenting%2520spherical%2520data%2520as%2520a%2520sequence%2520of%2520triangular%2520patches%2520derived%2520from%2520a%250Asubdivided%2520icosphere.%2520The%2520proposed%2520Surface%2520Vision%2520Mamba%2520%2528SiM%2529%2520is%2520evaluated%2520on%250Amultiple%2520neurodevelopmental%2520phenotype%2520regression%2520tasks%2520using%2520cortical%2520surface%250Ametrics%2520from%2520neonatal%2520brains.%2520Experimental%2520results%2520demonstrate%2520that%2520SiM%250Aoutperforms%2520both%2520attention-%2520and%2520GDL-based%2520methods%252C%2520delivering%25204.8%2520times%2520faster%250Ainference%2520and%2520achieving%252091.7%2525%2520lower%2520memory%2520consumption%2520compared%2520to%2520the%2520Surface%250AVision%2520Transformer%2520%2528SiT%2529%2520under%2520the%2520Ico-4%2520grid%2520partitioning.%2520Sensitivity%250Aanalysis%2520further%2520underscores%2520the%2520potential%2520of%2520SiM%2520to%2520identify%2520subtle%2520cognitive%250Adevelopmental%2520patterns.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Rongzhao-He/surface-vision-mamba.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.14679v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Surface%20Vision%20Mamba%3A%20Leveraging%20Bidirectional%20State%20Space%20Model%20for%0A%20%20Efficient%20Spherical%20Manifold%20Representation&entry.906535625=Rongzhao%20He%20and%20Weihao%20Zheng%20and%20Leilei%20Zhao%20and%20Ying%20Wang%20and%20Dalin%20Zhu%20and%20Dan%20Wu%20and%20Bin%20Hu&entry.1292438233=%20%20Attention-based%20methods%20have%20demonstrated%20exceptional%20performance%20in%0Amodelling%20long-range%20dependencies%20on%20spherical%20cortical%20surfaces%2C%20surpassing%0Atraditional%20Geometric%20Deep%20Learning%20%28GDL%29%20models.%20However%2C%20their%20extensive%0Ainference%20time%20and%20high%20memory%20demands%20pose%20challenges%20for%20application%20to%20large%0Adatasets%20with%20limited%20computing%20resources.%20Inspired%20by%20the%20state%20space%20model%20in%0Acomputer%20vision%2C%20we%20introduce%20the%20attention-free%20Vision%20Mamba%20%28Vim%29%20to%0Aspherical%20surfaces%2C%20presenting%20a%20domain-agnostic%20architecture%20for%20analyzing%0Adata%20on%20spherical%20manifolds.%20Our%20method%20achieves%20surface%20patching%20by%0Arepresenting%20spherical%20data%20as%20a%20sequence%20of%20triangular%20patches%20derived%20from%20a%0Asubdivided%20icosphere.%20The%20proposed%20Surface%20Vision%20Mamba%20%28SiM%29%20is%20evaluated%20on%0Amultiple%20neurodevelopmental%20phenotype%20regression%20tasks%20using%20cortical%20surface%0Ametrics%20from%20neonatal%20brains.%20Experimental%20results%20demonstrate%20that%20SiM%0Aoutperforms%20both%20attention-%20and%20GDL-based%20methods%2C%20delivering%204.8%20times%20faster%0Ainference%20and%20achieving%2091.7%25%20lower%20memory%20consumption%20compared%20to%20the%20Surface%0AVision%20Transformer%20%28SiT%29%20under%20the%20Ico-4%20grid%20partitioning.%20Sensitivity%0Aanalysis%20further%20underscores%20the%20potential%20of%20SiM%20to%20identify%20subtle%20cognitive%0Adevelopmental%20patterns.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Rongzhao-He/surface-vision-mamba.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.14679v2&entry.124074799=Read"},
{"title": "Ensemble based approach to quantifying uncertainty of LLM based\n  classifications", "author": "Srijith Rajamohan and Ahmed Salhin and Josh Frazier and Rohit Kumar and Yu-Cheng Tsai and Todd Cook", "abstract": "  The output of Large Language Models (LLMs) are a function of the internal\nmodel's parameters and the input provided into the context window. The\nhypothesis presented here is that under a greedy sampling strategy the variance\nin the LLM's output is a function of the conceptual certainty embedded in the\nmodel's parametric knowledge, as well as the lexical variance in the input.\nFinetuning the model results in reducing the sensitivity of the model output to\nthe lexical input variations. This is then applied to a classification problem\nand a probabilistic method is proposed for estimating the certainties of the\npredicted classes.\n", "link": "http://arxiv.org/abs/2502.08631v1", "date": "2025-02-12", "relevancy": 2.2138, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5838}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5515}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ensemble%20based%20approach%20to%20quantifying%20uncertainty%20of%20LLM%20based%0A%20%20classifications&body=Title%3A%20Ensemble%20based%20approach%20to%20quantifying%20uncertainty%20of%20LLM%20based%0A%20%20classifications%0AAuthor%3A%20Srijith%20Rajamohan%20and%20Ahmed%20Salhin%20and%20Josh%20Frazier%20and%20Rohit%20Kumar%20and%20Yu-Cheng%20Tsai%20and%20Todd%20Cook%0AAbstract%3A%20%20%20The%20output%20of%20Large%20Language%20Models%20%28LLMs%29%20are%20a%20function%20of%20the%20internal%0Amodel%27s%20parameters%20and%20the%20input%20provided%20into%20the%20context%20window.%20The%0Ahypothesis%20presented%20here%20is%20that%20under%20a%20greedy%20sampling%20strategy%20the%20variance%0Ain%20the%20LLM%27s%20output%20is%20a%20function%20of%20the%20conceptual%20certainty%20embedded%20in%20the%0Amodel%27s%20parametric%20knowledge%2C%20as%20well%20as%20the%20lexical%20variance%20in%20the%20input.%0AFinetuning%20the%20model%20results%20in%20reducing%20the%20sensitivity%20of%20the%20model%20output%20to%0Athe%20lexical%20input%20variations.%20This%20is%20then%20applied%20to%20a%20classification%20problem%0Aand%20a%20probabilistic%20method%20is%20proposed%20for%20estimating%20the%20certainties%20of%20the%0Apredicted%20classes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08631v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnsemble%2520based%2520approach%2520to%2520quantifying%2520uncertainty%2520of%2520LLM%2520based%250A%2520%2520classifications%26entry.906535625%3DSrijith%2520Rajamohan%2520and%2520Ahmed%2520Salhin%2520and%2520Josh%2520Frazier%2520and%2520Rohit%2520Kumar%2520and%2520Yu-Cheng%2520Tsai%2520and%2520Todd%2520Cook%26entry.1292438233%3D%2520%2520The%2520output%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520a%2520function%2520of%2520the%2520internal%250Amodel%2527s%2520parameters%2520and%2520the%2520input%2520provided%2520into%2520the%2520context%2520window.%2520The%250Ahypothesis%2520presented%2520here%2520is%2520that%2520under%2520a%2520greedy%2520sampling%2520strategy%2520the%2520variance%250Ain%2520the%2520LLM%2527s%2520output%2520is%2520a%2520function%2520of%2520the%2520conceptual%2520certainty%2520embedded%2520in%2520the%250Amodel%2527s%2520parametric%2520knowledge%252C%2520as%2520well%2520as%2520the%2520lexical%2520variance%2520in%2520the%2520input.%250AFinetuning%2520the%2520model%2520results%2520in%2520reducing%2520the%2520sensitivity%2520of%2520the%2520model%2520output%2520to%250Athe%2520lexical%2520input%2520variations.%2520This%2520is%2520then%2520applied%2520to%2520a%2520classification%2520problem%250Aand%2520a%2520probabilistic%2520method%2520is%2520proposed%2520for%2520estimating%2520the%2520certainties%2520of%2520the%250Apredicted%2520classes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08631v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ensemble%20based%20approach%20to%20quantifying%20uncertainty%20of%20LLM%20based%0A%20%20classifications&entry.906535625=Srijith%20Rajamohan%20and%20Ahmed%20Salhin%20and%20Josh%20Frazier%20and%20Rohit%20Kumar%20and%20Yu-Cheng%20Tsai%20and%20Todd%20Cook&entry.1292438233=%20%20The%20output%20of%20Large%20Language%20Models%20%28LLMs%29%20are%20a%20function%20of%20the%20internal%0Amodel%27s%20parameters%20and%20the%20input%20provided%20into%20the%20context%20window.%20The%0Ahypothesis%20presented%20here%20is%20that%20under%20a%20greedy%20sampling%20strategy%20the%20variance%0Ain%20the%20LLM%27s%20output%20is%20a%20function%20of%20the%20conceptual%20certainty%20embedded%20in%20the%0Amodel%27s%20parametric%20knowledge%2C%20as%20well%20as%20the%20lexical%20variance%20in%20the%20input.%0AFinetuning%20the%20model%20results%20in%20reducing%20the%20sensitivity%20of%20the%20model%20output%20to%0Athe%20lexical%20input%20variations.%20This%20is%20then%20applied%20to%20a%20classification%20problem%0Aand%20a%20probabilistic%20method%20is%20proposed%20for%20estimating%20the%20certainties%20of%20the%0Apredicted%20classes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08631v1&entry.124074799=Read"},
{"title": "Automated Capability Discovery via Model Self-Exploration", "author": "Cong Lu and Shengran Hu and Jeff Clune", "abstract": "  Foundation models have become general-purpose assistants, exhibiting diverse\ncapabilities across numerous domains through training on web-scale data. It\nremains challenging to precisely characterize even a fraction of the full\nspectrum of capabilities and potential risks in any new model. Existing\nevaluation approaches often require significant human effort, and it is taking\nincreasing effort to design ever harder challenges for more capable models. We\nintroduce Automated Capability Discovery (ACD), a framework that designates one\nfoundation model as a scientist to systematically propose open-ended tasks\nprobing the abilities of a subject model (potentially itself). By combining\nfrontier models with ideas from the field of open-endedness, ACD automatically\nand systematically uncovers both surprising capabilities and failures in the\nsubject model. We demonstrate ACD across a range of foundation models\n(including the GPT, Claude, and Llama series), showing that it automatically\nreveals thousands of capabilities that would be challenging for any single team\nto uncover. We further validate our method's automated scoring with extensive\nhuman surveys, observing high agreement between model-generated and human\nevaluations. By leveraging foundation models' ability to both create tasks and\nself-evaluate, ACD is a significant step toward scalable, automated evaluation\nof novel AI systems. All code and evaluation logs are open-sourced at\nhttps://github.com/conglu1997/ACD.\n", "link": "http://arxiv.org/abs/2502.07577v2", "date": "2025-02-12", "relevancy": 2.2094, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5757}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5477}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Capability%20Discovery%20via%20Model%20Self-Exploration&body=Title%3A%20Automated%20Capability%20Discovery%20via%20Model%20Self-Exploration%0AAuthor%3A%20Cong%20Lu%20and%20Shengran%20Hu%20and%20Jeff%20Clune%0AAbstract%3A%20%20%20Foundation%20models%20have%20become%20general-purpose%20assistants%2C%20exhibiting%20diverse%0Acapabilities%20across%20numerous%20domains%20through%20training%20on%20web-scale%20data.%20It%0Aremains%20challenging%20to%20precisely%20characterize%20even%20a%20fraction%20of%20the%20full%0Aspectrum%20of%20capabilities%20and%20potential%20risks%20in%20any%20new%20model.%20Existing%0Aevaluation%20approaches%20often%20require%20significant%20human%20effort%2C%20and%20it%20is%20taking%0Aincreasing%20effort%20to%20design%20ever%20harder%20challenges%20for%20more%20capable%20models.%20We%0Aintroduce%20Automated%20Capability%20Discovery%20%28ACD%29%2C%20a%20framework%20that%20designates%20one%0Afoundation%20model%20as%20a%20scientist%20to%20systematically%20propose%20open-ended%20tasks%0Aprobing%20the%20abilities%20of%20a%20subject%20model%20%28potentially%20itself%29.%20By%20combining%0Afrontier%20models%20with%20ideas%20from%20the%20field%20of%20open-endedness%2C%20ACD%20automatically%0Aand%20systematically%20uncovers%20both%20surprising%20capabilities%20and%20failures%20in%20the%0Asubject%20model.%20We%20demonstrate%20ACD%20across%20a%20range%20of%20foundation%20models%0A%28including%20the%20GPT%2C%20Claude%2C%20and%20Llama%20series%29%2C%20showing%20that%20it%20automatically%0Areveals%20thousands%20of%20capabilities%20that%20would%20be%20challenging%20for%20any%20single%20team%0Ato%20uncover.%20We%20further%20validate%20our%20method%27s%20automated%20scoring%20with%20extensive%0Ahuman%20surveys%2C%20observing%20high%20agreement%20between%20model-generated%20and%20human%0Aevaluations.%20By%20leveraging%20foundation%20models%27%20ability%20to%20both%20create%20tasks%20and%0Aself-evaluate%2C%20ACD%20is%20a%20significant%20step%20toward%20scalable%2C%20automated%20evaluation%0Aof%20novel%20AI%20systems.%20All%20code%20and%20evaluation%20logs%20are%20open-sourced%20at%0Ahttps%3A//github.com/conglu1997/ACD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07577v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Capability%2520Discovery%2520via%2520Model%2520Self-Exploration%26entry.906535625%3DCong%2520Lu%2520and%2520Shengran%2520Hu%2520and%2520Jeff%2520Clune%26entry.1292438233%3D%2520%2520Foundation%2520models%2520have%2520become%2520general-purpose%2520assistants%252C%2520exhibiting%2520diverse%250Acapabilities%2520across%2520numerous%2520domains%2520through%2520training%2520on%2520web-scale%2520data.%2520It%250Aremains%2520challenging%2520to%2520precisely%2520characterize%2520even%2520a%2520fraction%2520of%2520the%2520full%250Aspectrum%2520of%2520capabilities%2520and%2520potential%2520risks%2520in%2520any%2520new%2520model.%2520Existing%250Aevaluation%2520approaches%2520often%2520require%2520significant%2520human%2520effort%252C%2520and%2520it%2520is%2520taking%250Aincreasing%2520effort%2520to%2520design%2520ever%2520harder%2520challenges%2520for%2520more%2520capable%2520models.%2520We%250Aintroduce%2520Automated%2520Capability%2520Discovery%2520%2528ACD%2529%252C%2520a%2520framework%2520that%2520designates%2520one%250Afoundation%2520model%2520as%2520a%2520scientist%2520to%2520systematically%2520propose%2520open-ended%2520tasks%250Aprobing%2520the%2520abilities%2520of%2520a%2520subject%2520model%2520%2528potentially%2520itself%2529.%2520By%2520combining%250Afrontier%2520models%2520with%2520ideas%2520from%2520the%2520field%2520of%2520open-endedness%252C%2520ACD%2520automatically%250Aand%2520systematically%2520uncovers%2520both%2520surprising%2520capabilities%2520and%2520failures%2520in%2520the%250Asubject%2520model.%2520We%2520demonstrate%2520ACD%2520across%2520a%2520range%2520of%2520foundation%2520models%250A%2528including%2520the%2520GPT%252C%2520Claude%252C%2520and%2520Llama%2520series%2529%252C%2520showing%2520that%2520it%2520automatically%250Areveals%2520thousands%2520of%2520capabilities%2520that%2520would%2520be%2520challenging%2520for%2520any%2520single%2520team%250Ato%2520uncover.%2520We%2520further%2520validate%2520our%2520method%2527s%2520automated%2520scoring%2520with%2520extensive%250Ahuman%2520surveys%252C%2520observing%2520high%2520agreement%2520between%2520model-generated%2520and%2520human%250Aevaluations.%2520By%2520leveraging%2520foundation%2520models%2527%2520ability%2520to%2520both%2520create%2520tasks%2520and%250Aself-evaluate%252C%2520ACD%2520is%2520a%2520significant%2520step%2520toward%2520scalable%252C%2520automated%2520evaluation%250Aof%2520novel%2520AI%2520systems.%2520All%2520code%2520and%2520evaluation%2520logs%2520are%2520open-sourced%2520at%250Ahttps%253A//github.com/conglu1997/ACD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07577v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Capability%20Discovery%20via%20Model%20Self-Exploration&entry.906535625=Cong%20Lu%20and%20Shengran%20Hu%20and%20Jeff%20Clune&entry.1292438233=%20%20Foundation%20models%20have%20become%20general-purpose%20assistants%2C%20exhibiting%20diverse%0Acapabilities%20across%20numerous%20domains%20through%20training%20on%20web-scale%20data.%20It%0Aremains%20challenging%20to%20precisely%20characterize%20even%20a%20fraction%20of%20the%20full%0Aspectrum%20of%20capabilities%20and%20potential%20risks%20in%20any%20new%20model.%20Existing%0Aevaluation%20approaches%20often%20require%20significant%20human%20effort%2C%20and%20it%20is%20taking%0Aincreasing%20effort%20to%20design%20ever%20harder%20challenges%20for%20more%20capable%20models.%20We%0Aintroduce%20Automated%20Capability%20Discovery%20%28ACD%29%2C%20a%20framework%20that%20designates%20one%0Afoundation%20model%20as%20a%20scientist%20to%20systematically%20propose%20open-ended%20tasks%0Aprobing%20the%20abilities%20of%20a%20subject%20model%20%28potentially%20itself%29.%20By%20combining%0Afrontier%20models%20with%20ideas%20from%20the%20field%20of%20open-endedness%2C%20ACD%20automatically%0Aand%20systematically%20uncovers%20both%20surprising%20capabilities%20and%20failures%20in%20the%0Asubject%20model.%20We%20demonstrate%20ACD%20across%20a%20range%20of%20foundation%20models%0A%28including%20the%20GPT%2C%20Claude%2C%20and%20Llama%20series%29%2C%20showing%20that%20it%20automatically%0Areveals%20thousands%20of%20capabilities%20that%20would%20be%20challenging%20for%20any%20single%20team%0Ato%20uncover.%20We%20further%20validate%20our%20method%27s%20automated%20scoring%20with%20extensive%0Ahuman%20surveys%2C%20observing%20high%20agreement%20between%20model-generated%20and%20human%0Aevaluations.%20By%20leveraging%20foundation%20models%27%20ability%20to%20both%20create%20tasks%20and%0Aself-evaluate%2C%20ACD%20is%20a%20significant%20step%20toward%20scalable%2C%20automated%20evaluation%0Aof%20novel%20AI%20systems.%20All%20code%20and%20evaluation%20logs%20are%20open-sourced%20at%0Ahttps%3A//github.com/conglu1997/ACD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07577v2&entry.124074799=Read"},
{"title": "Fine-Tuning Topics through Weighting Aspect Keywords", "author": "Ali Nazari and Michael Weiss", "abstract": "  Topic modeling often requires examining topics from multiple perspectives to\nuncover hidden patterns, especially in less explored areas. This paper presents\nan approach to address this need, utilizing weighted keywords from various\naspects derived from a domain knowledge. The research method starts with\nstandard topic modeling. Then, it adds a process consisting of four key steps.\nFirst, it defines keywords for each aspect. Second, it gives weights to these\nkeywords based on their relevance. Third, it calculates relevance scores for\naspect-weighted keywords and topic keywords to create aspect-topic models.\nFourth, it uses these scores to tune relevant new documents. Finally, the\ngenerated topic models are interpreted and validated. The findings show that\ntop-scoring documents are more likely to be about the same aspect of a topic.\nThis highlights the model's effectiveness in finding the related documents to\nthe aspects.\n", "link": "http://arxiv.org/abs/2502.08496v1", "date": "2025-02-12", "relevancy": 2.2052, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4433}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4433}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-Tuning%20Topics%20through%20Weighting%20Aspect%20Keywords&body=Title%3A%20Fine-Tuning%20Topics%20through%20Weighting%20Aspect%20Keywords%0AAuthor%3A%20Ali%20Nazari%20and%20Michael%20Weiss%0AAbstract%3A%20%20%20Topic%20modeling%20often%20requires%20examining%20topics%20from%20multiple%20perspectives%20to%0Auncover%20hidden%20patterns%2C%20especially%20in%20less%20explored%20areas.%20This%20paper%20presents%0Aan%20approach%20to%20address%20this%20need%2C%20utilizing%20weighted%20keywords%20from%20various%0Aaspects%20derived%20from%20a%20domain%20knowledge.%20The%20research%20method%20starts%20with%0Astandard%20topic%20modeling.%20Then%2C%20it%20adds%20a%20process%20consisting%20of%20four%20key%20steps.%0AFirst%2C%20it%20defines%20keywords%20for%20each%20aspect.%20Second%2C%20it%20gives%20weights%20to%20these%0Akeywords%20based%20on%20their%20relevance.%20Third%2C%20it%20calculates%20relevance%20scores%20for%0Aaspect-weighted%20keywords%20and%20topic%20keywords%20to%20create%20aspect-topic%20models.%0AFourth%2C%20it%20uses%20these%20scores%20to%20tune%20relevant%20new%20documents.%20Finally%2C%20the%0Agenerated%20topic%20models%20are%20interpreted%20and%20validated.%20The%20findings%20show%20that%0Atop-scoring%20documents%20are%20more%20likely%20to%20be%20about%20the%20same%20aspect%20of%20a%20topic.%0AThis%20highlights%20the%20model%27s%20effectiveness%20in%20finding%20the%20related%20documents%20to%0Athe%20aspects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08496v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-Tuning%2520Topics%2520through%2520Weighting%2520Aspect%2520Keywords%26entry.906535625%3DAli%2520Nazari%2520and%2520Michael%2520Weiss%26entry.1292438233%3D%2520%2520Topic%2520modeling%2520often%2520requires%2520examining%2520topics%2520from%2520multiple%2520perspectives%2520to%250Auncover%2520hidden%2520patterns%252C%2520especially%2520in%2520less%2520explored%2520areas.%2520This%2520paper%2520presents%250Aan%2520approach%2520to%2520address%2520this%2520need%252C%2520utilizing%2520weighted%2520keywords%2520from%2520various%250Aaspects%2520derived%2520from%2520a%2520domain%2520knowledge.%2520The%2520research%2520method%2520starts%2520with%250Astandard%2520topic%2520modeling.%2520Then%252C%2520it%2520adds%2520a%2520process%2520consisting%2520of%2520four%2520key%2520steps.%250AFirst%252C%2520it%2520defines%2520keywords%2520for%2520each%2520aspect.%2520Second%252C%2520it%2520gives%2520weights%2520to%2520these%250Akeywords%2520based%2520on%2520their%2520relevance.%2520Third%252C%2520it%2520calculates%2520relevance%2520scores%2520for%250Aaspect-weighted%2520keywords%2520and%2520topic%2520keywords%2520to%2520create%2520aspect-topic%2520models.%250AFourth%252C%2520it%2520uses%2520these%2520scores%2520to%2520tune%2520relevant%2520new%2520documents.%2520Finally%252C%2520the%250Agenerated%2520topic%2520models%2520are%2520interpreted%2520and%2520validated.%2520The%2520findings%2520show%2520that%250Atop-scoring%2520documents%2520are%2520more%2520likely%2520to%2520be%2520about%2520the%2520same%2520aspect%2520of%2520a%2520topic.%250AThis%2520highlights%2520the%2520model%2527s%2520effectiveness%2520in%2520finding%2520the%2520related%2520documents%2520to%250Athe%2520aspects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08496v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-Tuning%20Topics%20through%20Weighting%20Aspect%20Keywords&entry.906535625=Ali%20Nazari%20and%20Michael%20Weiss&entry.1292438233=%20%20Topic%20modeling%20often%20requires%20examining%20topics%20from%20multiple%20perspectives%20to%0Auncover%20hidden%20patterns%2C%20especially%20in%20less%20explored%20areas.%20This%20paper%20presents%0Aan%20approach%20to%20address%20this%20need%2C%20utilizing%20weighted%20keywords%20from%20various%0Aaspects%20derived%20from%20a%20domain%20knowledge.%20The%20research%20method%20starts%20with%0Astandard%20topic%20modeling.%20Then%2C%20it%20adds%20a%20process%20consisting%20of%20four%20key%20steps.%0AFirst%2C%20it%20defines%20keywords%20for%20each%20aspect.%20Second%2C%20it%20gives%20weights%20to%20these%0Akeywords%20based%20on%20their%20relevance.%20Third%2C%20it%20calculates%20relevance%20scores%20for%0Aaspect-weighted%20keywords%20and%20topic%20keywords%20to%20create%20aspect-topic%20models.%0AFourth%2C%20it%20uses%20these%20scores%20to%20tune%20relevant%20new%20documents.%20Finally%2C%20the%0Agenerated%20topic%20models%20are%20interpreted%20and%20validated.%20The%20findings%20show%20that%0Atop-scoring%20documents%20are%20more%20likely%20to%20be%20about%20the%20same%20aspect%20of%20a%20topic.%0AThis%20highlights%20the%20model%27s%20effectiveness%20in%20finding%20the%20related%20documents%20to%0Athe%20aspects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08496v1&entry.124074799=Read"},
{"title": "Sketched Equivariant Imaging Regularization and Deep Internal Learning\n  for Inverse Problems", "author": "Guixian Xu and Jinglai Li and Junqi Tang", "abstract": "  Equivariant Imaging (EI) regularization has become the de-facto technique for\nunsupervised training of deep imaging networks, without any need of\nground-truth data. Observing that the EI-based unsupervised training paradigm\ncurrently has significant computational redundancy leading to inefficiency in\nhigh-dimensional applications, we propose a sketched EI regularization which\nleverages the randomized sketching techniques for acceleration. We then extend\nour sketched EI regularization to develop an accelerated deep internal learning\nframework, Sketched Equivariant Deep Image Prior (Sk-EI-DIP), which can be\nefficiently applied for single-image and task-adapted reconstruction.\nAdditionally, for network adaptation tasks, we propose a parameter-efficient\napproach for accelerating both EI-DIP and Sk-EI-DIP via optimizing only the\nnormalization layers. Our numerical study on X-ray CT and multi-coil MRI image\nreconstruction tasks demonstrate that our approach can achieve significant\ncomputational acceleration over standard EI-based counterpart in single-input\nsetting and network adaptation at test time.\n", "link": "http://arxiv.org/abs/2411.05771v3", "date": "2025-02-12", "relevancy": 2.1788, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5511}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5485}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sketched%20Equivariant%20Imaging%20Regularization%20and%20Deep%20Internal%20Learning%0A%20%20for%20Inverse%20Problems&body=Title%3A%20Sketched%20Equivariant%20Imaging%20Regularization%20and%20Deep%20Internal%20Learning%0A%20%20for%20Inverse%20Problems%0AAuthor%3A%20Guixian%20Xu%20and%20Jinglai%20Li%20and%20Junqi%20Tang%0AAbstract%3A%20%20%20Equivariant%20Imaging%20%28EI%29%20regularization%20has%20become%20the%20de-facto%20technique%20for%0Aunsupervised%20training%20of%20deep%20imaging%20networks%2C%20without%20any%20need%20of%0Aground-truth%20data.%20Observing%20that%20the%20EI-based%20unsupervised%20training%20paradigm%0Acurrently%20has%20significant%20computational%20redundancy%20leading%20to%20inefficiency%20in%0Ahigh-dimensional%20applications%2C%20we%20propose%20a%20sketched%20EI%20regularization%20which%0Aleverages%20the%20randomized%20sketching%20techniques%20for%20acceleration.%20We%20then%20extend%0Aour%20sketched%20EI%20regularization%20to%20develop%20an%20accelerated%20deep%20internal%20learning%0Aframework%2C%20Sketched%20Equivariant%20Deep%20Image%20Prior%20%28Sk-EI-DIP%29%2C%20which%20can%20be%0Aefficiently%20applied%20for%20single-image%20and%20task-adapted%20reconstruction.%0AAdditionally%2C%20for%20network%20adaptation%20tasks%2C%20we%20propose%20a%20parameter-efficient%0Aapproach%20for%20accelerating%20both%20EI-DIP%20and%20Sk-EI-DIP%20via%20optimizing%20only%20the%0Anormalization%20layers.%20Our%20numerical%20study%20on%20X-ray%20CT%20and%20multi-coil%20MRI%20image%0Areconstruction%20tasks%20demonstrate%20that%20our%20approach%20can%20achieve%20significant%0Acomputational%20acceleration%20over%20standard%20EI-based%20counterpart%20in%20single-input%0Asetting%20and%20network%20adaptation%20at%20test%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05771v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSketched%2520Equivariant%2520Imaging%2520Regularization%2520and%2520Deep%2520Internal%2520Learning%250A%2520%2520for%2520Inverse%2520Problems%26entry.906535625%3DGuixian%2520Xu%2520and%2520Jinglai%2520Li%2520and%2520Junqi%2520Tang%26entry.1292438233%3D%2520%2520Equivariant%2520Imaging%2520%2528EI%2529%2520regularization%2520has%2520become%2520the%2520de-facto%2520technique%2520for%250Aunsupervised%2520training%2520of%2520deep%2520imaging%2520networks%252C%2520without%2520any%2520need%2520of%250Aground-truth%2520data.%2520Observing%2520that%2520the%2520EI-based%2520unsupervised%2520training%2520paradigm%250Acurrently%2520has%2520significant%2520computational%2520redundancy%2520leading%2520to%2520inefficiency%2520in%250Ahigh-dimensional%2520applications%252C%2520we%2520propose%2520a%2520sketched%2520EI%2520regularization%2520which%250Aleverages%2520the%2520randomized%2520sketching%2520techniques%2520for%2520acceleration.%2520We%2520then%2520extend%250Aour%2520sketched%2520EI%2520regularization%2520to%2520develop%2520an%2520accelerated%2520deep%2520internal%2520learning%250Aframework%252C%2520Sketched%2520Equivariant%2520Deep%2520Image%2520Prior%2520%2528Sk-EI-DIP%2529%252C%2520which%2520can%2520be%250Aefficiently%2520applied%2520for%2520single-image%2520and%2520task-adapted%2520reconstruction.%250AAdditionally%252C%2520for%2520network%2520adaptation%2520tasks%252C%2520we%2520propose%2520a%2520parameter-efficient%250Aapproach%2520for%2520accelerating%2520both%2520EI-DIP%2520and%2520Sk-EI-DIP%2520via%2520optimizing%2520only%2520the%250Anormalization%2520layers.%2520Our%2520numerical%2520study%2520on%2520X-ray%2520CT%2520and%2520multi-coil%2520MRI%2520image%250Areconstruction%2520tasks%2520demonstrate%2520that%2520our%2520approach%2520can%2520achieve%2520significant%250Acomputational%2520acceleration%2520over%2520standard%2520EI-based%2520counterpart%2520in%2520single-input%250Asetting%2520and%2520network%2520adaptation%2520at%2520test%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05771v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sketched%20Equivariant%20Imaging%20Regularization%20and%20Deep%20Internal%20Learning%0A%20%20for%20Inverse%20Problems&entry.906535625=Guixian%20Xu%20and%20Jinglai%20Li%20and%20Junqi%20Tang&entry.1292438233=%20%20Equivariant%20Imaging%20%28EI%29%20regularization%20has%20become%20the%20de-facto%20technique%20for%0Aunsupervised%20training%20of%20deep%20imaging%20networks%2C%20without%20any%20need%20of%0Aground-truth%20data.%20Observing%20that%20the%20EI-based%20unsupervised%20training%20paradigm%0Acurrently%20has%20significant%20computational%20redundancy%20leading%20to%20inefficiency%20in%0Ahigh-dimensional%20applications%2C%20we%20propose%20a%20sketched%20EI%20regularization%20which%0Aleverages%20the%20randomized%20sketching%20techniques%20for%20acceleration.%20We%20then%20extend%0Aour%20sketched%20EI%20regularization%20to%20develop%20an%20accelerated%20deep%20internal%20learning%0Aframework%2C%20Sketched%20Equivariant%20Deep%20Image%20Prior%20%28Sk-EI-DIP%29%2C%20which%20can%20be%0Aefficiently%20applied%20for%20single-image%20and%20task-adapted%20reconstruction.%0AAdditionally%2C%20for%20network%20adaptation%20tasks%2C%20we%20propose%20a%20parameter-efficient%0Aapproach%20for%20accelerating%20both%20EI-DIP%20and%20Sk-EI-DIP%20via%20optimizing%20only%20the%0Anormalization%20layers.%20Our%20numerical%20study%20on%20X-ray%20CT%20and%20multi-coil%20MRI%20image%0Areconstruction%20tasks%20demonstrate%20that%20our%20approach%20can%20achieve%20significant%0Acomputational%20acceleration%20over%20standard%20EI-based%20counterpart%20in%20single-input%0Asetting%20and%20network%20adaptation%20at%20test%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05771v3&entry.124074799=Read"},
{"title": "Poly-Autoregressive Prediction for Modeling Interactions", "author": "Neerja Thakkar and Tara Sadjadpour and Jathushan Rajasegaran and Shiry Ginosar and Jitendra Malik", "abstract": "  We introduce a simple framework for predicting the behavior of an agent in\nmulti-agent settings. In contrast to autoregressive (AR) tasks, such as\nlanguage processing, our focus is on scenarios with multiple agents whose\ninteractions are shaped by physical constraints and internal motivations. To\nthis end, we propose Poly-Autoregressive (PAR) modeling, which forecasts an ego\nagent's future behavior by reasoning about the ego agent's state history and\nthe past and current states of other interacting agents. At its core, PAR\nrepresents the behavior of all agents as a sequence of tokens, each\nrepresenting an agent's state at a specific timestep. With minimal data\npre-processing changes, we show that PAR can be applied to three different\nproblems: human action forecasting in social situations, trajectory prediction\nfor autonomous vehicles, and object pose forecasting during hand-object\ninteraction. Using a small proof-of-concept transformer backbone, PAR\noutperforms AR across these three scenarios. The project website can be found\nat https://neerja.me/PAR/.\n", "link": "http://arxiv.org/abs/2502.08646v1", "date": "2025-02-12", "relevancy": 2.1354, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6078}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5275}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Poly-Autoregressive%20Prediction%20for%20Modeling%20Interactions&body=Title%3A%20Poly-Autoregressive%20Prediction%20for%20Modeling%20Interactions%0AAuthor%3A%20Neerja%20Thakkar%20and%20Tara%20Sadjadpour%20and%20Jathushan%20Rajasegaran%20and%20Shiry%20Ginosar%20and%20Jitendra%20Malik%0AAbstract%3A%20%20%20We%20introduce%20a%20simple%20framework%20for%20predicting%20the%20behavior%20of%20an%20agent%20in%0Amulti-agent%20settings.%20In%20contrast%20to%20autoregressive%20%28AR%29%20tasks%2C%20such%20as%0Alanguage%20processing%2C%20our%20focus%20is%20on%20scenarios%20with%20multiple%20agents%20whose%0Ainteractions%20are%20shaped%20by%20physical%20constraints%20and%20internal%20motivations.%20To%0Athis%20end%2C%20we%20propose%20Poly-Autoregressive%20%28PAR%29%20modeling%2C%20which%20forecasts%20an%20ego%0Aagent%27s%20future%20behavior%20by%20reasoning%20about%20the%20ego%20agent%27s%20state%20history%20and%0Athe%20past%20and%20current%20states%20of%20other%20interacting%20agents.%20At%20its%20core%2C%20PAR%0Arepresents%20the%20behavior%20of%20all%20agents%20as%20a%20sequence%20of%20tokens%2C%20each%0Arepresenting%20an%20agent%27s%20state%20at%20a%20specific%20timestep.%20With%20minimal%20data%0Apre-processing%20changes%2C%20we%20show%20that%20PAR%20can%20be%20applied%20to%20three%20different%0Aproblems%3A%20human%20action%20forecasting%20in%20social%20situations%2C%20trajectory%20prediction%0Afor%20autonomous%20vehicles%2C%20and%20object%20pose%20forecasting%20during%20hand-object%0Ainteraction.%20Using%20a%20small%20proof-of-concept%20transformer%20backbone%2C%20PAR%0Aoutperforms%20AR%20across%20these%20three%20scenarios.%20The%20project%20website%20can%20be%20found%0Aat%20https%3A//neerja.me/PAR/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08646v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoly-Autoregressive%2520Prediction%2520for%2520Modeling%2520Interactions%26entry.906535625%3DNeerja%2520Thakkar%2520and%2520Tara%2520Sadjadpour%2520and%2520Jathushan%2520Rajasegaran%2520and%2520Shiry%2520Ginosar%2520and%2520Jitendra%2520Malik%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520simple%2520framework%2520for%2520predicting%2520the%2520behavior%2520of%2520an%2520agent%2520in%250Amulti-agent%2520settings.%2520In%2520contrast%2520to%2520autoregressive%2520%2528AR%2529%2520tasks%252C%2520such%2520as%250Alanguage%2520processing%252C%2520our%2520focus%2520is%2520on%2520scenarios%2520with%2520multiple%2520agents%2520whose%250Ainteractions%2520are%2520shaped%2520by%2520physical%2520constraints%2520and%2520internal%2520motivations.%2520To%250Athis%2520end%252C%2520we%2520propose%2520Poly-Autoregressive%2520%2528PAR%2529%2520modeling%252C%2520which%2520forecasts%2520an%2520ego%250Aagent%2527s%2520future%2520behavior%2520by%2520reasoning%2520about%2520the%2520ego%2520agent%2527s%2520state%2520history%2520and%250Athe%2520past%2520and%2520current%2520states%2520of%2520other%2520interacting%2520agents.%2520At%2520its%2520core%252C%2520PAR%250Arepresents%2520the%2520behavior%2520of%2520all%2520agents%2520as%2520a%2520sequence%2520of%2520tokens%252C%2520each%250Arepresenting%2520an%2520agent%2527s%2520state%2520at%2520a%2520specific%2520timestep.%2520With%2520minimal%2520data%250Apre-processing%2520changes%252C%2520we%2520show%2520that%2520PAR%2520can%2520be%2520applied%2520to%2520three%2520different%250Aproblems%253A%2520human%2520action%2520forecasting%2520in%2520social%2520situations%252C%2520trajectory%2520prediction%250Afor%2520autonomous%2520vehicles%252C%2520and%2520object%2520pose%2520forecasting%2520during%2520hand-object%250Ainteraction.%2520Using%2520a%2520small%2520proof-of-concept%2520transformer%2520backbone%252C%2520PAR%250Aoutperforms%2520AR%2520across%2520these%2520three%2520scenarios.%2520The%2520project%2520website%2520can%2520be%2520found%250Aat%2520https%253A//neerja.me/PAR/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08646v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Poly-Autoregressive%20Prediction%20for%20Modeling%20Interactions&entry.906535625=Neerja%20Thakkar%20and%20Tara%20Sadjadpour%20and%20Jathushan%20Rajasegaran%20and%20Shiry%20Ginosar%20and%20Jitendra%20Malik&entry.1292438233=%20%20We%20introduce%20a%20simple%20framework%20for%20predicting%20the%20behavior%20of%20an%20agent%20in%0Amulti-agent%20settings.%20In%20contrast%20to%20autoregressive%20%28AR%29%20tasks%2C%20such%20as%0Alanguage%20processing%2C%20our%20focus%20is%20on%20scenarios%20with%20multiple%20agents%20whose%0Ainteractions%20are%20shaped%20by%20physical%20constraints%20and%20internal%20motivations.%20To%0Athis%20end%2C%20we%20propose%20Poly-Autoregressive%20%28PAR%29%20modeling%2C%20which%20forecasts%20an%20ego%0Aagent%27s%20future%20behavior%20by%20reasoning%20about%20the%20ego%20agent%27s%20state%20history%20and%0Athe%20past%20and%20current%20states%20of%20other%20interacting%20agents.%20At%20its%20core%2C%20PAR%0Arepresents%20the%20behavior%20of%20all%20agents%20as%20a%20sequence%20of%20tokens%2C%20each%0Arepresenting%20an%20agent%27s%20state%20at%20a%20specific%20timestep.%20With%20minimal%20data%0Apre-processing%20changes%2C%20we%20show%20that%20PAR%20can%20be%20applied%20to%20three%20different%0Aproblems%3A%20human%20action%20forecasting%20in%20social%20situations%2C%20trajectory%20prediction%0Afor%20autonomous%20vehicles%2C%20and%20object%20pose%20forecasting%20during%20hand-object%0Ainteraction.%20Using%20a%20small%20proof-of-concept%20transformer%20backbone%2C%20PAR%0Aoutperforms%20AR%20across%20these%20three%20scenarios.%20The%20project%20website%20can%20be%20found%0Aat%20https%3A//neerja.me/PAR/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08646v1&entry.124074799=Read"},
{"title": "Evaluating the Performance of ChatGPT for Spam Email Detection", "author": "Shijing Si and Yuwei Wu and Le Tang and Yugui Zhang and Jedrek Wosik and Qinliang Su", "abstract": "  Email continues to be a pivotal and extensively utilized communication medium\nwithin professional and commercial domains. Nonetheless, the prevalence of spam\nemails poses a significant challenge for users, disrupting their daily routines\nand diminishing productivity. Consequently, accurately identifying and\nfiltering spam based on content has become crucial for cybersecurity. Recent\nadvancements in natural language processing, particularly with large language\nmodels like ChatGPT, have shown remarkable performance in tasks such as\nquestion answering and text generation. However, its potential in spam\nidentification remains underexplored. To fill in the gap, this study attempts\nto evaluate ChatGPT's capabilities for spam identification in both English and\nChinese email datasets. We employ ChatGPT for spam email detection using\nin-context learning, which requires a prompt instruction with (or without) a\nfew demonstrations. We also investigate how the number of demonstrations in the\nprompt affects the performance of ChatGPT. For comparison, we also implement\nfive popular benchmark methods, including naive Bayes, support vector machines\n(SVM), logistic regression (LR), feedforward dense neural networks (DNN), and\nBERT classifiers. Through extensive experiments, the performance of ChatGPT is\nsignificantly worse than deep supervised learning methods in the large English\ndataset, while it presents superior performance on the low-resourced Chinese\ndataset. This study provides insights into the potential and limitations of\nChatGPT for spam identification, highlighting its potential as a viable\nsolution for resource-constrained language domains.\n", "link": "http://arxiv.org/abs/2402.15537v3", "date": "2025-02-12", "relevancy": 2.1053, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4344}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4179}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4109}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20the%20Performance%20of%20ChatGPT%20for%20Spam%20Email%20Detection&body=Title%3A%20Evaluating%20the%20Performance%20of%20ChatGPT%20for%20Spam%20Email%20Detection%0AAuthor%3A%20Shijing%20Si%20and%20Yuwei%20Wu%20and%20Le%20Tang%20and%20Yugui%20Zhang%20and%20Jedrek%20Wosik%20and%20Qinliang%20Su%0AAbstract%3A%20%20%20Email%20continues%20to%20be%20a%20pivotal%20and%20extensively%20utilized%20communication%20medium%0Awithin%20professional%20and%20commercial%20domains.%20Nonetheless%2C%20the%20prevalence%20of%20spam%0Aemails%20poses%20a%20significant%20challenge%20for%20users%2C%20disrupting%20their%20daily%20routines%0Aand%20diminishing%20productivity.%20Consequently%2C%20accurately%20identifying%20and%0Afiltering%20spam%20based%20on%20content%20has%20become%20crucial%20for%20cybersecurity.%20Recent%0Aadvancements%20in%20natural%20language%20processing%2C%20particularly%20with%20large%20language%0Amodels%20like%20ChatGPT%2C%20have%20shown%20remarkable%20performance%20in%20tasks%20such%20as%0Aquestion%20answering%20and%20text%20generation.%20However%2C%20its%20potential%20in%20spam%0Aidentification%20remains%20underexplored.%20To%20fill%20in%20the%20gap%2C%20this%20study%20attempts%0Ato%20evaluate%20ChatGPT%27s%20capabilities%20for%20spam%20identification%20in%20both%20English%20and%0AChinese%20email%20datasets.%20We%20employ%20ChatGPT%20for%20spam%20email%20detection%20using%0Ain-context%20learning%2C%20which%20requires%20a%20prompt%20instruction%20with%20%28or%20without%29%20a%0Afew%20demonstrations.%20We%20also%20investigate%20how%20the%20number%20of%20demonstrations%20in%20the%0Aprompt%20affects%20the%20performance%20of%20ChatGPT.%20For%20comparison%2C%20we%20also%20implement%0Afive%20popular%20benchmark%20methods%2C%20including%20naive%20Bayes%2C%20support%20vector%20machines%0A%28SVM%29%2C%20logistic%20regression%20%28LR%29%2C%20feedforward%20dense%20neural%20networks%20%28DNN%29%2C%20and%0ABERT%20classifiers.%20Through%20extensive%20experiments%2C%20the%20performance%20of%20ChatGPT%20is%0Asignificantly%20worse%20than%20deep%20supervised%20learning%20methods%20in%20the%20large%20English%0Adataset%2C%20while%20it%20presents%20superior%20performance%20on%20the%20low-resourced%20Chinese%0Adataset.%20This%20study%20provides%20insights%20into%20the%20potential%20and%20limitations%20of%0AChatGPT%20for%20spam%20identification%2C%20highlighting%20its%20potential%20as%20a%20viable%0Asolution%20for%20resource-constrained%20language%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.15537v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520the%2520Performance%2520of%2520ChatGPT%2520for%2520Spam%2520Email%2520Detection%26entry.906535625%3DShijing%2520Si%2520and%2520Yuwei%2520Wu%2520and%2520Le%2520Tang%2520and%2520Yugui%2520Zhang%2520and%2520Jedrek%2520Wosik%2520and%2520Qinliang%2520Su%26entry.1292438233%3D%2520%2520Email%2520continues%2520to%2520be%2520a%2520pivotal%2520and%2520extensively%2520utilized%2520communication%2520medium%250Awithin%2520professional%2520and%2520commercial%2520domains.%2520Nonetheless%252C%2520the%2520prevalence%2520of%2520spam%250Aemails%2520poses%2520a%2520significant%2520challenge%2520for%2520users%252C%2520disrupting%2520their%2520daily%2520routines%250Aand%2520diminishing%2520productivity.%2520Consequently%252C%2520accurately%2520identifying%2520and%250Afiltering%2520spam%2520based%2520on%2520content%2520has%2520become%2520crucial%2520for%2520cybersecurity.%2520Recent%250Aadvancements%2520in%2520natural%2520language%2520processing%252C%2520particularly%2520with%2520large%2520language%250Amodels%2520like%2520ChatGPT%252C%2520have%2520shown%2520remarkable%2520performance%2520in%2520tasks%2520such%2520as%250Aquestion%2520answering%2520and%2520text%2520generation.%2520However%252C%2520its%2520potential%2520in%2520spam%250Aidentification%2520remains%2520underexplored.%2520To%2520fill%2520in%2520the%2520gap%252C%2520this%2520study%2520attempts%250Ato%2520evaluate%2520ChatGPT%2527s%2520capabilities%2520for%2520spam%2520identification%2520in%2520both%2520English%2520and%250AChinese%2520email%2520datasets.%2520We%2520employ%2520ChatGPT%2520for%2520spam%2520email%2520detection%2520using%250Ain-context%2520learning%252C%2520which%2520requires%2520a%2520prompt%2520instruction%2520with%2520%2528or%2520without%2529%2520a%250Afew%2520demonstrations.%2520We%2520also%2520investigate%2520how%2520the%2520number%2520of%2520demonstrations%2520in%2520the%250Aprompt%2520affects%2520the%2520performance%2520of%2520ChatGPT.%2520For%2520comparison%252C%2520we%2520also%2520implement%250Afive%2520popular%2520benchmark%2520methods%252C%2520including%2520naive%2520Bayes%252C%2520support%2520vector%2520machines%250A%2528SVM%2529%252C%2520logistic%2520regression%2520%2528LR%2529%252C%2520feedforward%2520dense%2520neural%2520networks%2520%2528DNN%2529%252C%2520and%250ABERT%2520classifiers.%2520Through%2520extensive%2520experiments%252C%2520the%2520performance%2520of%2520ChatGPT%2520is%250Asignificantly%2520worse%2520than%2520deep%2520supervised%2520learning%2520methods%2520in%2520the%2520large%2520English%250Adataset%252C%2520while%2520it%2520presents%2520superior%2520performance%2520on%2520the%2520low-resourced%2520Chinese%250Adataset.%2520This%2520study%2520provides%2520insights%2520into%2520the%2520potential%2520and%2520limitations%2520of%250AChatGPT%2520for%2520spam%2520identification%252C%2520highlighting%2520its%2520potential%2520as%2520a%2520viable%250Asolution%2520for%2520resource-constrained%2520language%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.15537v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20the%20Performance%20of%20ChatGPT%20for%20Spam%20Email%20Detection&entry.906535625=Shijing%20Si%20and%20Yuwei%20Wu%20and%20Le%20Tang%20and%20Yugui%20Zhang%20and%20Jedrek%20Wosik%20and%20Qinliang%20Su&entry.1292438233=%20%20Email%20continues%20to%20be%20a%20pivotal%20and%20extensively%20utilized%20communication%20medium%0Awithin%20professional%20and%20commercial%20domains.%20Nonetheless%2C%20the%20prevalence%20of%20spam%0Aemails%20poses%20a%20significant%20challenge%20for%20users%2C%20disrupting%20their%20daily%20routines%0Aand%20diminishing%20productivity.%20Consequently%2C%20accurately%20identifying%20and%0Afiltering%20spam%20based%20on%20content%20has%20become%20crucial%20for%20cybersecurity.%20Recent%0Aadvancements%20in%20natural%20language%20processing%2C%20particularly%20with%20large%20language%0Amodels%20like%20ChatGPT%2C%20have%20shown%20remarkable%20performance%20in%20tasks%20such%20as%0Aquestion%20answering%20and%20text%20generation.%20However%2C%20its%20potential%20in%20spam%0Aidentification%20remains%20underexplored.%20To%20fill%20in%20the%20gap%2C%20this%20study%20attempts%0Ato%20evaluate%20ChatGPT%27s%20capabilities%20for%20spam%20identification%20in%20both%20English%20and%0AChinese%20email%20datasets.%20We%20employ%20ChatGPT%20for%20spam%20email%20detection%20using%0Ain-context%20learning%2C%20which%20requires%20a%20prompt%20instruction%20with%20%28or%20without%29%20a%0Afew%20demonstrations.%20We%20also%20investigate%20how%20the%20number%20of%20demonstrations%20in%20the%0Aprompt%20affects%20the%20performance%20of%20ChatGPT.%20For%20comparison%2C%20we%20also%20implement%0Afive%20popular%20benchmark%20methods%2C%20including%20naive%20Bayes%2C%20support%20vector%20machines%0A%28SVM%29%2C%20logistic%20regression%20%28LR%29%2C%20feedforward%20dense%20neural%20networks%20%28DNN%29%2C%20and%0ABERT%20classifiers.%20Through%20extensive%20experiments%2C%20the%20performance%20of%20ChatGPT%20is%0Asignificantly%20worse%20than%20deep%20supervised%20learning%20methods%20in%20the%20large%20English%0Adataset%2C%20while%20it%20presents%20superior%20performance%20on%20the%20low-resourced%20Chinese%0Adataset.%20This%20study%20provides%20insights%20into%20the%20potential%20and%20limitations%20of%0AChatGPT%20for%20spam%20identification%2C%20highlighting%20its%20potential%20as%20a%20viable%0Asolution%20for%20resource-constrained%20language%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15537v3&entry.124074799=Read"},
{"title": "Deep Spatiotemporal Clutter Filtering of Transthoracic Echocardiographic\n  Images: Leveraging Contextual Attention and Residual Learning", "author": "Mahdi Tabassian and Somayeh Akbari and Sandro Queir\u00f3s and Jan D'hooge", "abstract": "  This study presents a deep convolutional autoencoder network for filtering\nreverberation clutter from transthoracic echocardiographic (TTE) image\nsequences. Given the spatiotemporal nature of this type of clutter, the\nfiltering network employs 3D convolutional layers to suppress it throughout the\ncardiac cycle. The design of the network incorporates two key features that\ncontribute to the effectiveness of the filter: 1) an attention mechanism for\nfocusing on cluttered regions and leveraging contextual information, and 2)\nresidual learning for preserving fine image structures. To train the network, a\ndiverse set of artifact patterns was simulated and superimposed onto\nultra-realistic synthetic TTE sequences from six ultrasound vendors, generating\ninput for the filtering network. The artifact-free sequences served as\nground-truth. Performance of the filtering network was evaluated using unseen\nsynthetic and in vivo artifactual sequences. Results from the in vivo dataset\nconfirmed the network's strong generalization capabilities, despite being\ntrained solely on synthetic data and simulated artifacts. The suitability of\nthe filtered sequences for downstream processing was assessed by computing\nsegmental strain curves. A significant reduction in the discrepancy between\nstrain profiles computed from cluttered and clutter-free segments was observed\nafter filtering the cluttered sequences with the proposed network. The trained\nnetwork processes a TTE sequence in a fraction of a second, enabling real-time\nclutter filtering and potentially improving the precision of clinically\nrelevant indices derived from TTE sequences. The source code of the proposed\nmethod and example video files of the filtering results are available at:\n\\href{https://github.com/MahdiTabassian/Deep-Clutter-Filtering/tree/main}{https://github.com/MahdiTabassian/Deep-Clutter-Filtering/tree/main}.\n", "link": "http://arxiv.org/abs/2401.13147v2", "date": "2025-02-12", "relevancy": 2.0976, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5262}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5235}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5221}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Spatiotemporal%20Clutter%20Filtering%20of%20Transthoracic%20Echocardiographic%0A%20%20Images%3A%20Leveraging%20Contextual%20Attention%20and%20Residual%20Learning&body=Title%3A%20Deep%20Spatiotemporal%20Clutter%20Filtering%20of%20Transthoracic%20Echocardiographic%0A%20%20Images%3A%20Leveraging%20Contextual%20Attention%20and%20Residual%20Learning%0AAuthor%3A%20Mahdi%20Tabassian%20and%20Somayeh%20Akbari%20and%20Sandro%20Queir%C3%B3s%20and%20Jan%20D%27hooge%0AAbstract%3A%20%20%20This%20study%20presents%20a%20deep%20convolutional%20autoencoder%20network%20for%20filtering%0Areverberation%20clutter%20from%20transthoracic%20echocardiographic%20%28TTE%29%20image%0Asequences.%20Given%20the%20spatiotemporal%20nature%20of%20this%20type%20of%20clutter%2C%20the%0Afiltering%20network%20employs%203D%20convolutional%20layers%20to%20suppress%20it%20throughout%20the%0Acardiac%20cycle.%20The%20design%20of%20the%20network%20incorporates%20two%20key%20features%20that%0Acontribute%20to%20the%20effectiveness%20of%20the%20filter%3A%201%29%20an%20attention%20mechanism%20for%0Afocusing%20on%20cluttered%20regions%20and%20leveraging%20contextual%20information%2C%20and%202%29%0Aresidual%20learning%20for%20preserving%20fine%20image%20structures.%20To%20train%20the%20network%2C%20a%0Adiverse%20set%20of%20artifact%20patterns%20was%20simulated%20and%20superimposed%20onto%0Aultra-realistic%20synthetic%20TTE%20sequences%20from%20six%20ultrasound%20vendors%2C%20generating%0Ainput%20for%20the%20filtering%20network.%20The%20artifact-free%20sequences%20served%20as%0Aground-truth.%20Performance%20of%20the%20filtering%20network%20was%20evaluated%20using%20unseen%0Asynthetic%20and%20in%20vivo%20artifactual%20sequences.%20Results%20from%20the%20in%20vivo%20dataset%0Aconfirmed%20the%20network%27s%20strong%20generalization%20capabilities%2C%20despite%20being%0Atrained%20solely%20on%20synthetic%20data%20and%20simulated%20artifacts.%20The%20suitability%20of%0Athe%20filtered%20sequences%20for%20downstream%20processing%20was%20assessed%20by%20computing%0Asegmental%20strain%20curves.%20A%20significant%20reduction%20in%20the%20discrepancy%20between%0Astrain%20profiles%20computed%20from%20cluttered%20and%20clutter-free%20segments%20was%20observed%0Aafter%20filtering%20the%20cluttered%20sequences%20with%20the%20proposed%20network.%20The%20trained%0Anetwork%20processes%20a%20TTE%20sequence%20in%20a%20fraction%20of%20a%20second%2C%20enabling%20real-time%0Aclutter%20filtering%20and%20potentially%20improving%20the%20precision%20of%20clinically%0Arelevant%20indices%20derived%20from%20TTE%20sequences.%20The%20source%20code%20of%20the%20proposed%0Amethod%20and%20example%20video%20files%20of%20the%20filtering%20results%20are%20available%20at%3A%0A%5Chref%7Bhttps%3A//github.com/MahdiTabassian/Deep-Clutter-Filtering/tree/main%7D%7Bhttps%3A//github.com/MahdiTabassian/Deep-Clutter-Filtering/tree/main%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.13147v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Spatiotemporal%2520Clutter%2520Filtering%2520of%2520Transthoracic%2520Echocardiographic%250A%2520%2520Images%253A%2520Leveraging%2520Contextual%2520Attention%2520and%2520Residual%2520Learning%26entry.906535625%3DMahdi%2520Tabassian%2520and%2520Somayeh%2520Akbari%2520and%2520Sandro%2520Queir%25C3%25B3s%2520and%2520Jan%2520D%2527hooge%26entry.1292438233%3D%2520%2520This%2520study%2520presents%2520a%2520deep%2520convolutional%2520autoencoder%2520network%2520for%2520filtering%250Areverberation%2520clutter%2520from%2520transthoracic%2520echocardiographic%2520%2528TTE%2529%2520image%250Asequences.%2520Given%2520the%2520spatiotemporal%2520nature%2520of%2520this%2520type%2520of%2520clutter%252C%2520the%250Afiltering%2520network%2520employs%25203D%2520convolutional%2520layers%2520to%2520suppress%2520it%2520throughout%2520the%250Acardiac%2520cycle.%2520The%2520design%2520of%2520the%2520network%2520incorporates%2520two%2520key%2520features%2520that%250Acontribute%2520to%2520the%2520effectiveness%2520of%2520the%2520filter%253A%25201%2529%2520an%2520attention%2520mechanism%2520for%250Afocusing%2520on%2520cluttered%2520regions%2520and%2520leveraging%2520contextual%2520information%252C%2520and%25202%2529%250Aresidual%2520learning%2520for%2520preserving%2520fine%2520image%2520structures.%2520To%2520train%2520the%2520network%252C%2520a%250Adiverse%2520set%2520of%2520artifact%2520patterns%2520was%2520simulated%2520and%2520superimposed%2520onto%250Aultra-realistic%2520synthetic%2520TTE%2520sequences%2520from%2520six%2520ultrasound%2520vendors%252C%2520generating%250Ainput%2520for%2520the%2520filtering%2520network.%2520The%2520artifact-free%2520sequences%2520served%2520as%250Aground-truth.%2520Performance%2520of%2520the%2520filtering%2520network%2520was%2520evaluated%2520using%2520unseen%250Asynthetic%2520and%2520in%2520vivo%2520artifactual%2520sequences.%2520Results%2520from%2520the%2520in%2520vivo%2520dataset%250Aconfirmed%2520the%2520network%2527s%2520strong%2520generalization%2520capabilities%252C%2520despite%2520being%250Atrained%2520solely%2520on%2520synthetic%2520data%2520and%2520simulated%2520artifacts.%2520The%2520suitability%2520of%250Athe%2520filtered%2520sequences%2520for%2520downstream%2520processing%2520was%2520assessed%2520by%2520computing%250Asegmental%2520strain%2520curves.%2520A%2520significant%2520reduction%2520in%2520the%2520discrepancy%2520between%250Astrain%2520profiles%2520computed%2520from%2520cluttered%2520and%2520clutter-free%2520segments%2520was%2520observed%250Aafter%2520filtering%2520the%2520cluttered%2520sequences%2520with%2520the%2520proposed%2520network.%2520The%2520trained%250Anetwork%2520processes%2520a%2520TTE%2520sequence%2520in%2520a%2520fraction%2520of%2520a%2520second%252C%2520enabling%2520real-time%250Aclutter%2520filtering%2520and%2520potentially%2520improving%2520the%2520precision%2520of%2520clinically%250Arelevant%2520indices%2520derived%2520from%2520TTE%2520sequences.%2520The%2520source%2520code%2520of%2520the%2520proposed%250Amethod%2520and%2520example%2520video%2520files%2520of%2520the%2520filtering%2520results%2520are%2520available%2520at%253A%250A%255Chref%257Bhttps%253A//github.com/MahdiTabassian/Deep-Clutter-Filtering/tree/main%257D%257Bhttps%253A//github.com/MahdiTabassian/Deep-Clutter-Filtering/tree/main%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.13147v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Spatiotemporal%20Clutter%20Filtering%20of%20Transthoracic%20Echocardiographic%0A%20%20Images%3A%20Leveraging%20Contextual%20Attention%20and%20Residual%20Learning&entry.906535625=Mahdi%20Tabassian%20and%20Somayeh%20Akbari%20and%20Sandro%20Queir%C3%B3s%20and%20Jan%20D%27hooge&entry.1292438233=%20%20This%20study%20presents%20a%20deep%20convolutional%20autoencoder%20network%20for%20filtering%0Areverberation%20clutter%20from%20transthoracic%20echocardiographic%20%28TTE%29%20image%0Asequences.%20Given%20the%20spatiotemporal%20nature%20of%20this%20type%20of%20clutter%2C%20the%0Afiltering%20network%20employs%203D%20convolutional%20layers%20to%20suppress%20it%20throughout%20the%0Acardiac%20cycle.%20The%20design%20of%20the%20network%20incorporates%20two%20key%20features%20that%0Acontribute%20to%20the%20effectiveness%20of%20the%20filter%3A%201%29%20an%20attention%20mechanism%20for%0Afocusing%20on%20cluttered%20regions%20and%20leveraging%20contextual%20information%2C%20and%202%29%0Aresidual%20learning%20for%20preserving%20fine%20image%20structures.%20To%20train%20the%20network%2C%20a%0Adiverse%20set%20of%20artifact%20patterns%20was%20simulated%20and%20superimposed%20onto%0Aultra-realistic%20synthetic%20TTE%20sequences%20from%20six%20ultrasound%20vendors%2C%20generating%0Ainput%20for%20the%20filtering%20network.%20The%20artifact-free%20sequences%20served%20as%0Aground-truth.%20Performance%20of%20the%20filtering%20network%20was%20evaluated%20using%20unseen%0Asynthetic%20and%20in%20vivo%20artifactual%20sequences.%20Results%20from%20the%20in%20vivo%20dataset%0Aconfirmed%20the%20network%27s%20strong%20generalization%20capabilities%2C%20despite%20being%0Atrained%20solely%20on%20synthetic%20data%20and%20simulated%20artifacts.%20The%20suitability%20of%0Athe%20filtered%20sequences%20for%20downstream%20processing%20was%20assessed%20by%20computing%0Asegmental%20strain%20curves.%20A%20significant%20reduction%20in%20the%20discrepancy%20between%0Astrain%20profiles%20computed%20from%20cluttered%20and%20clutter-free%20segments%20was%20observed%0Aafter%20filtering%20the%20cluttered%20sequences%20with%20the%20proposed%20network.%20The%20trained%0Anetwork%20processes%20a%20TTE%20sequence%20in%20a%20fraction%20of%20a%20second%2C%20enabling%20real-time%0Aclutter%20filtering%20and%20potentially%20improving%20the%20precision%20of%20clinically%0Arelevant%20indices%20derived%20from%20TTE%20sequences.%20The%20source%20code%20of%20the%20proposed%0Amethod%20and%20example%20video%20files%20of%20the%20filtering%20results%20are%20available%20at%3A%0A%5Chref%7Bhttps%3A//github.com/MahdiTabassian/Deep-Clutter-Filtering/tree/main%7D%7Bhttps%3A//github.com/MahdiTabassian/Deep-Clutter-Filtering/tree/main%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.13147v2&entry.124074799=Read"},
{"title": "UniZyme: A Unified Protein Cleavage Site Predictor Enhanced with Enzyme\n  Active-Site Knowledge", "author": "Chenao Li and Shuo Yan and Enyan Dai", "abstract": "  Enzyme-catalyzed protein cleavage is essential for many biological functions.\nAccurate prediction of cleavage sites can facilitate various applications such\nas drug development, enzyme design, and a deeper understanding of biological\nmechanisms. However, most existing models are restricted to an individual\nenzyme, which neglects shared knowledge of enzymes and fails generalize to\nnovel enzymes. Thus, we introduce a unified protein cleavage site predictor\nnamed UniZyme, which can generalize across diverse enzymes. To enhance the\nenzyme encoding for the protein cleavage site prediction, UniZyme employs a\nnovel biochemically-informed model architecture along with active-site\nknowledge of proteolytic enzymes. Extensive experiments demonstrate that\nUniZyme achieves high accuracy in predicting cleavage sites across a range of\nproteolytic enzymes, including unseen enzymes. The code is available in\nhttps://anonymous.4open.science/r/UniZyme-4A67.\n", "link": "http://arxiv.org/abs/2502.06914v2", "date": "2025-02-12", "relevancy": 2.0832, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4197}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4197}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniZyme%3A%20A%20Unified%20Protein%20Cleavage%20Site%20Predictor%20Enhanced%20with%20Enzyme%0A%20%20Active-Site%20Knowledge&body=Title%3A%20UniZyme%3A%20A%20Unified%20Protein%20Cleavage%20Site%20Predictor%20Enhanced%20with%20Enzyme%0A%20%20Active-Site%20Knowledge%0AAuthor%3A%20Chenao%20Li%20and%20Shuo%20Yan%20and%20Enyan%20Dai%0AAbstract%3A%20%20%20Enzyme-catalyzed%20protein%20cleavage%20is%20essential%20for%20many%20biological%20functions.%0AAccurate%20prediction%20of%20cleavage%20sites%20can%20facilitate%20various%20applications%20such%0Aas%20drug%20development%2C%20enzyme%20design%2C%20and%20a%20deeper%20understanding%20of%20biological%0Amechanisms.%20However%2C%20most%20existing%20models%20are%20restricted%20to%20an%20individual%0Aenzyme%2C%20which%20neglects%20shared%20knowledge%20of%20enzymes%20and%20fails%20generalize%20to%0Anovel%20enzymes.%20Thus%2C%20we%20introduce%20a%20unified%20protein%20cleavage%20site%20predictor%0Anamed%20UniZyme%2C%20which%20can%20generalize%20across%20diverse%20enzymes.%20To%20enhance%20the%0Aenzyme%20encoding%20for%20the%20protein%20cleavage%20site%20prediction%2C%20UniZyme%20employs%20a%0Anovel%20biochemically-informed%20model%20architecture%20along%20with%20active-site%0Aknowledge%20of%20proteolytic%20enzymes.%20Extensive%20experiments%20demonstrate%20that%0AUniZyme%20achieves%20high%20accuracy%20in%20predicting%20cleavage%20sites%20across%20a%20range%20of%0Aproteolytic%20enzymes%2C%20including%20unseen%20enzymes.%20The%20code%20is%20available%20in%0Ahttps%3A//anonymous.4open.science/r/UniZyme-4A67.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06914v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniZyme%253A%2520A%2520Unified%2520Protein%2520Cleavage%2520Site%2520Predictor%2520Enhanced%2520with%2520Enzyme%250A%2520%2520Active-Site%2520Knowledge%26entry.906535625%3DChenao%2520Li%2520and%2520Shuo%2520Yan%2520and%2520Enyan%2520Dai%26entry.1292438233%3D%2520%2520Enzyme-catalyzed%2520protein%2520cleavage%2520is%2520essential%2520for%2520many%2520biological%2520functions.%250AAccurate%2520prediction%2520of%2520cleavage%2520sites%2520can%2520facilitate%2520various%2520applications%2520such%250Aas%2520drug%2520development%252C%2520enzyme%2520design%252C%2520and%2520a%2520deeper%2520understanding%2520of%2520biological%250Amechanisms.%2520However%252C%2520most%2520existing%2520models%2520are%2520restricted%2520to%2520an%2520individual%250Aenzyme%252C%2520which%2520neglects%2520shared%2520knowledge%2520of%2520enzymes%2520and%2520fails%2520generalize%2520to%250Anovel%2520enzymes.%2520Thus%252C%2520we%2520introduce%2520a%2520unified%2520protein%2520cleavage%2520site%2520predictor%250Anamed%2520UniZyme%252C%2520which%2520can%2520generalize%2520across%2520diverse%2520enzymes.%2520To%2520enhance%2520the%250Aenzyme%2520encoding%2520for%2520the%2520protein%2520cleavage%2520site%2520prediction%252C%2520UniZyme%2520employs%2520a%250Anovel%2520biochemically-informed%2520model%2520architecture%2520along%2520with%2520active-site%250Aknowledge%2520of%2520proteolytic%2520enzymes.%2520Extensive%2520experiments%2520demonstrate%2520that%250AUniZyme%2520achieves%2520high%2520accuracy%2520in%2520predicting%2520cleavage%2520sites%2520across%2520a%2520range%2520of%250Aproteolytic%2520enzymes%252C%2520including%2520unseen%2520enzymes.%2520The%2520code%2520is%2520available%2520in%250Ahttps%253A//anonymous.4open.science/r/UniZyme-4A67.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06914v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniZyme%3A%20A%20Unified%20Protein%20Cleavage%20Site%20Predictor%20Enhanced%20with%20Enzyme%0A%20%20Active-Site%20Knowledge&entry.906535625=Chenao%20Li%20and%20Shuo%20Yan%20and%20Enyan%20Dai&entry.1292438233=%20%20Enzyme-catalyzed%20protein%20cleavage%20is%20essential%20for%20many%20biological%20functions.%0AAccurate%20prediction%20of%20cleavage%20sites%20can%20facilitate%20various%20applications%20such%0Aas%20drug%20development%2C%20enzyme%20design%2C%20and%20a%20deeper%20understanding%20of%20biological%0Amechanisms.%20However%2C%20most%20existing%20models%20are%20restricted%20to%20an%20individual%0Aenzyme%2C%20which%20neglects%20shared%20knowledge%20of%20enzymes%20and%20fails%20generalize%20to%0Anovel%20enzymes.%20Thus%2C%20we%20introduce%20a%20unified%20protein%20cleavage%20site%20predictor%0Anamed%20UniZyme%2C%20which%20can%20generalize%20across%20diverse%20enzymes.%20To%20enhance%20the%0Aenzyme%20encoding%20for%20the%20protein%20cleavage%20site%20prediction%2C%20UniZyme%20employs%20a%0Anovel%20biochemically-informed%20model%20architecture%20along%20with%20active-site%0Aknowledge%20of%20proteolytic%20enzymes.%20Extensive%20experiments%20demonstrate%20that%0AUniZyme%20achieves%20high%20accuracy%20in%20predicting%20cleavage%20sites%20across%20a%20range%20of%0Aproteolytic%20enzymes%2C%20including%20unseen%20enzymes.%20The%20code%20is%20available%20in%0Ahttps%3A//anonymous.4open.science/r/UniZyme-4A67.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06914v2&entry.124074799=Read"},
{"title": "Do Large Code Models Understand Programming Concepts? Counterfactual\n  Analysis for Code Predicates", "author": "Ashish Hooda and Mihai Christodorescu and Miltiadis Allamanis and Aaron Wilson and Kassem Fawaz and Somesh Jha", "abstract": "  Large Language Models' success on text generation has also made them better\nat code generation and coding tasks. While a lot of work has demonstrated their\nremarkable performance on tasks such as code completion and editing, it is\nstill unclear as to why. We help bridge this gap by exploring to what degree\nauto-regressive models understand the logical constructs of the underlying\nprograms. We propose Counterfactual Analysis for Programming Concept Predicates\n(CACP) as a counterfactual testing framework to evaluate whether Large Code\nModels understand programming concepts. With only black-box access to the\nmodel, we use CACP to evaluate ten popular Large Code Models for four different\nprogramming concepts. Our findings suggest that current models lack\nunderstanding of concepts such as data flow and control flow.\n", "link": "http://arxiv.org/abs/2402.05980v3", "date": "2025-02-12", "relevancy": 2.0539, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5265}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5265}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Large%20Code%20Models%20Understand%20Programming%20Concepts%3F%20Counterfactual%0A%20%20Analysis%20for%20Code%20Predicates&body=Title%3A%20Do%20Large%20Code%20Models%20Understand%20Programming%20Concepts%3F%20Counterfactual%0A%20%20Analysis%20for%20Code%20Predicates%0AAuthor%3A%20Ashish%20Hooda%20and%20Mihai%20Christodorescu%20and%20Miltiadis%20Allamanis%20and%20Aaron%20Wilson%20and%20Kassem%20Fawaz%20and%20Somesh%20Jha%0AAbstract%3A%20%20%20Large%20Language%20Models%27%20success%20on%20text%20generation%20has%20also%20made%20them%20better%0Aat%20code%20generation%20and%20coding%20tasks.%20While%20a%20lot%20of%20work%20has%20demonstrated%20their%0Aremarkable%20performance%20on%20tasks%20such%20as%20code%20completion%20and%20editing%2C%20it%20is%0Astill%20unclear%20as%20to%20why.%20We%20help%20bridge%20this%20gap%20by%20exploring%20to%20what%20degree%0Aauto-regressive%20models%20understand%20the%20logical%20constructs%20of%20the%20underlying%0Aprograms.%20We%20propose%20Counterfactual%20Analysis%20for%20Programming%20Concept%20Predicates%0A%28CACP%29%20as%20a%20counterfactual%20testing%20framework%20to%20evaluate%20whether%20Large%20Code%0AModels%20understand%20programming%20concepts.%20With%20only%20black-box%20access%20to%20the%0Amodel%2C%20we%20use%20CACP%20to%20evaluate%20ten%20popular%20Large%20Code%20Models%20for%20four%20different%0Aprogramming%20concepts.%20Our%20findings%20suggest%20that%20current%20models%20lack%0Aunderstanding%20of%20concepts%20such%20as%20data%20flow%20and%20control%20flow.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.05980v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Large%2520Code%2520Models%2520Understand%2520Programming%2520Concepts%253F%2520Counterfactual%250A%2520%2520Analysis%2520for%2520Code%2520Predicates%26entry.906535625%3DAshish%2520Hooda%2520and%2520Mihai%2520Christodorescu%2520and%2520Miltiadis%2520Allamanis%2520and%2520Aaron%2520Wilson%2520and%2520Kassem%2520Fawaz%2520and%2520Somesh%2520Jha%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2527%2520success%2520on%2520text%2520generation%2520has%2520also%2520made%2520them%2520better%250Aat%2520code%2520generation%2520and%2520coding%2520tasks.%2520While%2520a%2520lot%2520of%2520work%2520has%2520demonstrated%2520their%250Aremarkable%2520performance%2520on%2520tasks%2520such%2520as%2520code%2520completion%2520and%2520editing%252C%2520it%2520is%250Astill%2520unclear%2520as%2520to%2520why.%2520We%2520help%2520bridge%2520this%2520gap%2520by%2520exploring%2520to%2520what%2520degree%250Aauto-regressive%2520models%2520understand%2520the%2520logical%2520constructs%2520of%2520the%2520underlying%250Aprograms.%2520We%2520propose%2520Counterfactual%2520Analysis%2520for%2520Programming%2520Concept%2520Predicates%250A%2528CACP%2529%2520as%2520a%2520counterfactual%2520testing%2520framework%2520to%2520evaluate%2520whether%2520Large%2520Code%250AModels%2520understand%2520programming%2520concepts.%2520With%2520only%2520black-box%2520access%2520to%2520the%250Amodel%252C%2520we%2520use%2520CACP%2520to%2520evaluate%2520ten%2520popular%2520Large%2520Code%2520Models%2520for%2520four%2520different%250Aprogramming%2520concepts.%2520Our%2520findings%2520suggest%2520that%2520current%2520models%2520lack%250Aunderstanding%2520of%2520concepts%2520such%2520as%2520data%2520flow%2520and%2520control%2520flow.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.05980v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Large%20Code%20Models%20Understand%20Programming%20Concepts%3F%20Counterfactual%0A%20%20Analysis%20for%20Code%20Predicates&entry.906535625=Ashish%20Hooda%20and%20Mihai%20Christodorescu%20and%20Miltiadis%20Allamanis%20and%20Aaron%20Wilson%20and%20Kassem%20Fawaz%20and%20Somesh%20Jha&entry.1292438233=%20%20Large%20Language%20Models%27%20success%20on%20text%20generation%20has%20also%20made%20them%20better%0Aat%20code%20generation%20and%20coding%20tasks.%20While%20a%20lot%20of%20work%20has%20demonstrated%20their%0Aremarkable%20performance%20on%20tasks%20such%20as%20code%20completion%20and%20editing%2C%20it%20is%0Astill%20unclear%20as%20to%20why.%20We%20help%20bridge%20this%20gap%20by%20exploring%20to%20what%20degree%0Aauto-regressive%20models%20understand%20the%20logical%20constructs%20of%20the%20underlying%0Aprograms.%20We%20propose%20Counterfactual%20Analysis%20for%20Programming%20Concept%20Predicates%0A%28CACP%29%20as%20a%20counterfactual%20testing%20framework%20to%20evaluate%20whether%20Large%20Code%0AModels%20understand%20programming%20concepts.%20With%20only%20black-box%20access%20to%20the%0Amodel%2C%20we%20use%20CACP%20to%20evaluate%20ten%20popular%20Large%20Code%20Models%20for%20four%20different%0Aprogramming%20concepts.%20Our%20findings%20suggest%20that%20current%20models%20lack%0Aunderstanding%20of%20concepts%20such%20as%20data%20flow%20and%20control%20flow.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05980v3&entry.124074799=Read"},
{"title": "ARR: Question Answering with Large Language Models via Analyzing,\n  Retrieving, and Reasoning", "author": "Yuwei Yin and Giuseppe Carenini", "abstract": "  Large language models (LLMs) achieve remarkable performance on challenging\nbenchmarks that are often structured as multiple-choice question-answering (QA)\ntasks. Zero-shot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs\nbut provides only vague and generic guidance (\"think step by step\"). This paper\nintroduces ARR, an intuitive and effective zero-shot prompting method that\nexplicitly incorporates three key steps in QA solving: analyzing the intent of\nthe question, retrieving relevant information, and reasoning step by step.\nComprehensive experiments across diverse and challenging QA tasks demonstrate\nthat ARR consistently improves the Baseline (without ARR prompting) and\noutperforms CoT. Ablation and case studies further validate the positive\ncontributions of each component: analyzing, retrieving, and reasoning. Notably,\nintent analysis plays a vital role in ARR. Additionally, extensive evaluations\nacross various model sizes, LLM series, and generation settings solidify the\neffectiveness, robustness, and generalizability of ARR.\n", "link": "http://arxiv.org/abs/2502.04689v2", "date": "2025-02-12", "relevancy": 2.0345, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5118}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5118}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ARR%3A%20Question%20Answering%20with%20Large%20Language%20Models%20via%20Analyzing%2C%0A%20%20Retrieving%2C%20and%20Reasoning&body=Title%3A%20ARR%3A%20Question%20Answering%20with%20Large%20Language%20Models%20via%20Analyzing%2C%0A%20%20Retrieving%2C%20and%20Reasoning%0AAuthor%3A%20Yuwei%20Yin%20and%20Giuseppe%20Carenini%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20achieve%20remarkable%20performance%20on%20challenging%0Abenchmarks%20that%20are%20often%20structured%20as%20multiple-choice%20question-answering%20%28QA%29%0Atasks.%20Zero-shot%20Chain-of-Thought%20%28CoT%29%20prompting%20enhances%20reasoning%20in%20LLMs%0Abut%20provides%20only%20vague%20and%20generic%20guidance%20%28%22think%20step%20by%20step%22%29.%20This%20paper%0Aintroduces%20ARR%2C%20an%20intuitive%20and%20effective%20zero-shot%20prompting%20method%20that%0Aexplicitly%20incorporates%20three%20key%20steps%20in%20QA%20solving%3A%20analyzing%20the%20intent%20of%0Athe%20question%2C%20retrieving%20relevant%20information%2C%20and%20reasoning%20step%20by%20step.%0AComprehensive%20experiments%20across%20diverse%20and%20challenging%20QA%20tasks%20demonstrate%0Athat%20ARR%20consistently%20improves%20the%20Baseline%20%28without%20ARR%20prompting%29%20and%0Aoutperforms%20CoT.%20Ablation%20and%20case%20studies%20further%20validate%20the%20positive%0Acontributions%20of%20each%20component%3A%20analyzing%2C%20retrieving%2C%20and%20reasoning.%20Notably%2C%0Aintent%20analysis%20plays%20a%20vital%20role%20in%20ARR.%20Additionally%2C%20extensive%20evaluations%0Aacross%20various%20model%20sizes%2C%20LLM%20series%2C%20and%20generation%20settings%20solidify%20the%0Aeffectiveness%2C%20robustness%2C%20and%20generalizability%20of%20ARR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04689v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DARR%253A%2520Question%2520Answering%2520with%2520Large%2520Language%2520Models%2520via%2520Analyzing%252C%250A%2520%2520Retrieving%252C%2520and%2520Reasoning%26entry.906535625%3DYuwei%2520Yin%2520and%2520Giuseppe%2520Carenini%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520achieve%2520remarkable%2520performance%2520on%2520challenging%250Abenchmarks%2520that%2520are%2520often%2520structured%2520as%2520multiple-choice%2520question-answering%2520%2528QA%2529%250Atasks.%2520Zero-shot%2520Chain-of-Thought%2520%2528CoT%2529%2520prompting%2520enhances%2520reasoning%2520in%2520LLMs%250Abut%2520provides%2520only%2520vague%2520and%2520generic%2520guidance%2520%2528%2522think%2520step%2520by%2520step%2522%2529.%2520This%2520paper%250Aintroduces%2520ARR%252C%2520an%2520intuitive%2520and%2520effective%2520zero-shot%2520prompting%2520method%2520that%250Aexplicitly%2520incorporates%2520three%2520key%2520steps%2520in%2520QA%2520solving%253A%2520analyzing%2520the%2520intent%2520of%250Athe%2520question%252C%2520retrieving%2520relevant%2520information%252C%2520and%2520reasoning%2520step%2520by%2520step.%250AComprehensive%2520experiments%2520across%2520diverse%2520and%2520challenging%2520QA%2520tasks%2520demonstrate%250Athat%2520ARR%2520consistently%2520improves%2520the%2520Baseline%2520%2528without%2520ARR%2520prompting%2529%2520and%250Aoutperforms%2520CoT.%2520Ablation%2520and%2520case%2520studies%2520further%2520validate%2520the%2520positive%250Acontributions%2520of%2520each%2520component%253A%2520analyzing%252C%2520retrieving%252C%2520and%2520reasoning.%2520Notably%252C%250Aintent%2520analysis%2520plays%2520a%2520vital%2520role%2520in%2520ARR.%2520Additionally%252C%2520extensive%2520evaluations%250Aacross%2520various%2520model%2520sizes%252C%2520LLM%2520series%252C%2520and%2520generation%2520settings%2520solidify%2520the%250Aeffectiveness%252C%2520robustness%252C%2520and%2520generalizability%2520of%2520ARR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04689v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ARR%3A%20Question%20Answering%20with%20Large%20Language%20Models%20via%20Analyzing%2C%0A%20%20Retrieving%2C%20and%20Reasoning&entry.906535625=Yuwei%20Yin%20and%20Giuseppe%20Carenini&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20achieve%20remarkable%20performance%20on%20challenging%0Abenchmarks%20that%20are%20often%20structured%20as%20multiple-choice%20question-answering%20%28QA%29%0Atasks.%20Zero-shot%20Chain-of-Thought%20%28CoT%29%20prompting%20enhances%20reasoning%20in%20LLMs%0Abut%20provides%20only%20vague%20and%20generic%20guidance%20%28%22think%20step%20by%20step%22%29.%20This%20paper%0Aintroduces%20ARR%2C%20an%20intuitive%20and%20effective%20zero-shot%20prompting%20method%20that%0Aexplicitly%20incorporates%20three%20key%20steps%20in%20QA%20solving%3A%20analyzing%20the%20intent%20of%0Athe%20question%2C%20retrieving%20relevant%20information%2C%20and%20reasoning%20step%20by%20step.%0AComprehensive%20experiments%20across%20diverse%20and%20challenging%20QA%20tasks%20demonstrate%0Athat%20ARR%20consistently%20improves%20the%20Baseline%20%28without%20ARR%20prompting%29%20and%0Aoutperforms%20CoT.%20Ablation%20and%20case%20studies%20further%20validate%20the%20positive%0Acontributions%20of%20each%20component%3A%20analyzing%2C%20retrieving%2C%20and%20reasoning.%20Notably%2C%0Aintent%20analysis%20plays%20a%20vital%20role%20in%20ARR.%20Additionally%2C%20extensive%20evaluations%0Aacross%20various%20model%20sizes%2C%20LLM%20series%2C%20and%20generation%20settings%20solidify%20the%0Aeffectiveness%2C%20robustness%2C%20and%20generalizability%20of%20ARR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04689v2&entry.124074799=Read"},
{"title": "Numerical Schemes for Signature Kernels", "author": "Thomas Cass and Francesco Piatti and Jeffrey Pei", "abstract": "  Signature kernels have emerged as a powerful tool within kernel methods for\nsequential data. In the paper \"The Signature Kernel is the solution of a\nGoursat PDE\", the authors identify a kernel trick that demonstrates that, for\ncontinuously differentiable paths, the signature kernel satisfies a Goursat\nproblem for a hyperbolic partial differential equation (PDE) in two independent\ntime variables. While finite difference methods have been explored for this\nPDE, they face limitations in accuracy and stability when handling highly\noscillatory inputs. In this work, we introduce two advanced numerical schemes\nthat leverage polynomial representations of boundary conditions through either\napproximation or interpolation techniques, and rigorously establish the\ntheoretical convergence of the polynomial approximation scheme. Experimental\nevaluations reveal that our approaches yield improvements of several orders of\nmagnitude in mean absolute percentage error (MAPE) compared to traditional\nfinite difference schemes, without increasing computational complexity.\nFurthermore, like finite difference methods, our algorithms can be\nGPU-parallelized to reduce computational complexity from quadratic to linear in\nthe length of the input sequences, thereby improving scalability for\nhigh-frequency data. We have implemented these algorithms in a dedicated Python\nlibrary, which is publicly available at:\nhttps://github.com/FrancescoPiatti/polysigkernel.\n", "link": "http://arxiv.org/abs/2502.08470v1", "date": "2025-02-12", "relevancy": 1.997, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4046}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4033}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.3904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Numerical%20Schemes%20for%20Signature%20Kernels&body=Title%3A%20Numerical%20Schemes%20for%20Signature%20Kernels%0AAuthor%3A%20Thomas%20Cass%20and%20Francesco%20Piatti%20and%20Jeffrey%20Pei%0AAbstract%3A%20%20%20Signature%20kernels%20have%20emerged%20as%20a%20powerful%20tool%20within%20kernel%20methods%20for%0Asequential%20data.%20In%20the%20paper%20%22The%20Signature%20Kernel%20is%20the%20solution%20of%20a%0AGoursat%20PDE%22%2C%20the%20authors%20identify%20a%20kernel%20trick%20that%20demonstrates%20that%2C%20for%0Acontinuously%20differentiable%20paths%2C%20the%20signature%20kernel%20satisfies%20a%20Goursat%0Aproblem%20for%20a%20hyperbolic%20partial%20differential%20equation%20%28PDE%29%20in%20two%20independent%0Atime%20variables.%20While%20finite%20difference%20methods%20have%20been%20explored%20for%20this%0APDE%2C%20they%20face%20limitations%20in%20accuracy%20and%20stability%20when%20handling%20highly%0Aoscillatory%20inputs.%20In%20this%20work%2C%20we%20introduce%20two%20advanced%20numerical%20schemes%0Athat%20leverage%20polynomial%20representations%20of%20boundary%20conditions%20through%20either%0Aapproximation%20or%20interpolation%20techniques%2C%20and%20rigorously%20establish%20the%0Atheoretical%20convergence%20of%20the%20polynomial%20approximation%20scheme.%20Experimental%0Aevaluations%20reveal%20that%20our%20approaches%20yield%20improvements%20of%20several%20orders%20of%0Amagnitude%20in%20mean%20absolute%20percentage%20error%20%28MAPE%29%20compared%20to%20traditional%0Afinite%20difference%20schemes%2C%20without%20increasing%20computational%20complexity.%0AFurthermore%2C%20like%20finite%20difference%20methods%2C%20our%20algorithms%20can%20be%0AGPU-parallelized%20to%20reduce%20computational%20complexity%20from%20quadratic%20to%20linear%20in%0Athe%20length%20of%20the%20input%20sequences%2C%20thereby%20improving%20scalability%20for%0Ahigh-frequency%20data.%20We%20have%20implemented%20these%20algorithms%20in%20a%20dedicated%20Python%0Alibrary%2C%20which%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/FrancescoPiatti/polysigkernel.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08470v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNumerical%2520Schemes%2520for%2520Signature%2520Kernels%26entry.906535625%3DThomas%2520Cass%2520and%2520Francesco%2520Piatti%2520and%2520Jeffrey%2520Pei%26entry.1292438233%3D%2520%2520Signature%2520kernels%2520have%2520emerged%2520as%2520a%2520powerful%2520tool%2520within%2520kernel%2520methods%2520for%250Asequential%2520data.%2520In%2520the%2520paper%2520%2522The%2520Signature%2520Kernel%2520is%2520the%2520solution%2520of%2520a%250AGoursat%2520PDE%2522%252C%2520the%2520authors%2520identify%2520a%2520kernel%2520trick%2520that%2520demonstrates%2520that%252C%2520for%250Acontinuously%2520differentiable%2520paths%252C%2520the%2520signature%2520kernel%2520satisfies%2520a%2520Goursat%250Aproblem%2520for%2520a%2520hyperbolic%2520partial%2520differential%2520equation%2520%2528PDE%2529%2520in%2520two%2520independent%250Atime%2520variables.%2520While%2520finite%2520difference%2520methods%2520have%2520been%2520explored%2520for%2520this%250APDE%252C%2520they%2520face%2520limitations%2520in%2520accuracy%2520and%2520stability%2520when%2520handling%2520highly%250Aoscillatory%2520inputs.%2520In%2520this%2520work%252C%2520we%2520introduce%2520two%2520advanced%2520numerical%2520schemes%250Athat%2520leverage%2520polynomial%2520representations%2520of%2520boundary%2520conditions%2520through%2520either%250Aapproximation%2520or%2520interpolation%2520techniques%252C%2520and%2520rigorously%2520establish%2520the%250Atheoretical%2520convergence%2520of%2520the%2520polynomial%2520approximation%2520scheme.%2520Experimental%250Aevaluations%2520reveal%2520that%2520our%2520approaches%2520yield%2520improvements%2520of%2520several%2520orders%2520of%250Amagnitude%2520in%2520mean%2520absolute%2520percentage%2520error%2520%2528MAPE%2529%2520compared%2520to%2520traditional%250Afinite%2520difference%2520schemes%252C%2520without%2520increasing%2520computational%2520complexity.%250AFurthermore%252C%2520like%2520finite%2520difference%2520methods%252C%2520our%2520algorithms%2520can%2520be%250AGPU-parallelized%2520to%2520reduce%2520computational%2520complexity%2520from%2520quadratic%2520to%2520linear%2520in%250Athe%2520length%2520of%2520the%2520input%2520sequences%252C%2520thereby%2520improving%2520scalability%2520for%250Ahigh-frequency%2520data.%2520We%2520have%2520implemented%2520these%2520algorithms%2520in%2520a%2520dedicated%2520Python%250Alibrary%252C%2520which%2520is%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/FrancescoPiatti/polysigkernel.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08470v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Numerical%20Schemes%20for%20Signature%20Kernels&entry.906535625=Thomas%20Cass%20and%20Francesco%20Piatti%20and%20Jeffrey%20Pei&entry.1292438233=%20%20Signature%20kernels%20have%20emerged%20as%20a%20powerful%20tool%20within%20kernel%20methods%20for%0Asequential%20data.%20In%20the%20paper%20%22The%20Signature%20Kernel%20is%20the%20solution%20of%20a%0AGoursat%20PDE%22%2C%20the%20authors%20identify%20a%20kernel%20trick%20that%20demonstrates%20that%2C%20for%0Acontinuously%20differentiable%20paths%2C%20the%20signature%20kernel%20satisfies%20a%20Goursat%0Aproblem%20for%20a%20hyperbolic%20partial%20differential%20equation%20%28PDE%29%20in%20two%20independent%0Atime%20variables.%20While%20finite%20difference%20methods%20have%20been%20explored%20for%20this%0APDE%2C%20they%20face%20limitations%20in%20accuracy%20and%20stability%20when%20handling%20highly%0Aoscillatory%20inputs.%20In%20this%20work%2C%20we%20introduce%20two%20advanced%20numerical%20schemes%0Athat%20leverage%20polynomial%20representations%20of%20boundary%20conditions%20through%20either%0Aapproximation%20or%20interpolation%20techniques%2C%20and%20rigorously%20establish%20the%0Atheoretical%20convergence%20of%20the%20polynomial%20approximation%20scheme.%20Experimental%0Aevaluations%20reveal%20that%20our%20approaches%20yield%20improvements%20of%20several%20orders%20of%0Amagnitude%20in%20mean%20absolute%20percentage%20error%20%28MAPE%29%20compared%20to%20traditional%0Afinite%20difference%20schemes%2C%20without%20increasing%20computational%20complexity.%0AFurthermore%2C%20like%20finite%20difference%20methods%2C%20our%20algorithms%20can%20be%0AGPU-parallelized%20to%20reduce%20computational%20complexity%20from%20quadratic%20to%20linear%20in%0Athe%20length%20of%20the%20input%20sequences%2C%20thereby%20improving%20scalability%20for%0Ahigh-frequency%20data.%20We%20have%20implemented%20these%20algorithms%20in%20a%20dedicated%20Python%0Alibrary%2C%20which%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/FrancescoPiatti/polysigkernel.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08470v1&entry.124074799=Read"},
{"title": "Transcoders Beat Sparse Autoencoders for Interpretability", "author": "Gon\u00e7alo Paulo and Stepan Shabalin and Nora Belrose", "abstract": "  Sparse autoencoders (SAEs) extract human-interpretable features from deep\nneural networks by transforming their activations into a sparse, higher\ndimensional latent space, and then reconstructing the activations from these\nlatents. Transcoders are similar to SAEs, but they are trained to reconstruct\nthe output of a component of a deep network given its input. In this work, we\ncompare the features found by transcoders and SAEs trained on the same model\nand data, finding that transcoder features are significantly more\ninterpretable. We also propose skip transcoders, which add an affine skip\nconnection to the transcoder architecture, and show that these achieve lower\nreconstruction loss with no effect on interpretability.\n", "link": "http://arxiv.org/abs/2501.18823v2", "date": "2025-02-12", "relevancy": 1.9919, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5073}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5027}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transcoders%20Beat%20Sparse%20Autoencoders%20for%20Interpretability&body=Title%3A%20Transcoders%20Beat%20Sparse%20Autoencoders%20for%20Interpretability%0AAuthor%3A%20Gon%C3%A7alo%20Paulo%20and%20Stepan%20Shabalin%20and%20Nora%20Belrose%0AAbstract%3A%20%20%20Sparse%20autoencoders%20%28SAEs%29%20extract%20human-interpretable%20features%20from%20deep%0Aneural%20networks%20by%20transforming%20their%20activations%20into%20a%20sparse%2C%20higher%0Adimensional%20latent%20space%2C%20and%20then%20reconstructing%20the%20activations%20from%20these%0Alatents.%20Transcoders%20are%20similar%20to%20SAEs%2C%20but%20they%20are%20trained%20to%20reconstruct%0Athe%20output%20of%20a%20component%20of%20a%20deep%20network%20given%20its%20input.%20In%20this%20work%2C%20we%0Acompare%20the%20features%20found%20by%20transcoders%20and%20SAEs%20trained%20on%20the%20same%20model%0Aand%20data%2C%20finding%20that%20transcoder%20features%20are%20significantly%20more%0Ainterpretable.%20We%20also%20propose%20skip%20transcoders%2C%20which%20add%20an%20affine%20skip%0Aconnection%20to%20the%20transcoder%20architecture%2C%20and%20show%20that%20these%20achieve%20lower%0Areconstruction%20loss%20with%20no%20effect%20on%20interpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.18823v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTranscoders%2520Beat%2520Sparse%2520Autoencoders%2520for%2520Interpretability%26entry.906535625%3DGon%25C3%25A7alo%2520Paulo%2520and%2520Stepan%2520Shabalin%2520and%2520Nora%2520Belrose%26entry.1292438233%3D%2520%2520Sparse%2520autoencoders%2520%2528SAEs%2529%2520extract%2520human-interpretable%2520features%2520from%2520deep%250Aneural%2520networks%2520by%2520transforming%2520their%2520activations%2520into%2520a%2520sparse%252C%2520higher%250Adimensional%2520latent%2520space%252C%2520and%2520then%2520reconstructing%2520the%2520activations%2520from%2520these%250Alatents.%2520Transcoders%2520are%2520similar%2520to%2520SAEs%252C%2520but%2520they%2520are%2520trained%2520to%2520reconstruct%250Athe%2520output%2520of%2520a%2520component%2520of%2520a%2520deep%2520network%2520given%2520its%2520input.%2520In%2520this%2520work%252C%2520we%250Acompare%2520the%2520features%2520found%2520by%2520transcoders%2520and%2520SAEs%2520trained%2520on%2520the%2520same%2520model%250Aand%2520data%252C%2520finding%2520that%2520transcoder%2520features%2520are%2520significantly%2520more%250Ainterpretable.%2520We%2520also%2520propose%2520skip%2520transcoders%252C%2520which%2520add%2520an%2520affine%2520skip%250Aconnection%2520to%2520the%2520transcoder%2520architecture%252C%2520and%2520show%2520that%2520these%2520achieve%2520lower%250Areconstruction%2520loss%2520with%2520no%2520effect%2520on%2520interpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18823v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transcoders%20Beat%20Sparse%20Autoencoders%20for%20Interpretability&entry.906535625=Gon%C3%A7alo%20Paulo%20and%20Stepan%20Shabalin%20and%20Nora%20Belrose&entry.1292438233=%20%20Sparse%20autoencoders%20%28SAEs%29%20extract%20human-interpretable%20features%20from%20deep%0Aneural%20networks%20by%20transforming%20their%20activations%20into%20a%20sparse%2C%20higher%0Adimensional%20latent%20space%2C%20and%20then%20reconstructing%20the%20activations%20from%20these%0Alatents.%20Transcoders%20are%20similar%20to%20SAEs%2C%20but%20they%20are%20trained%20to%20reconstruct%0Athe%20output%20of%20a%20component%20of%20a%20deep%20network%20given%20its%20input.%20In%20this%20work%2C%20we%0Acompare%20the%20features%20found%20by%20transcoders%20and%20SAEs%20trained%20on%20the%20same%20model%0Aand%20data%2C%20finding%20that%20transcoder%20features%20are%20significantly%20more%0Ainterpretable.%20We%20also%20propose%20skip%20transcoders%2C%20which%20add%20an%20affine%20skip%0Aconnection%20to%20the%20transcoder%20architecture%2C%20and%20show%20that%20these%20achieve%20lower%0Areconstruction%20loss%20with%20no%20effect%20on%20interpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.18823v2&entry.124074799=Read"},
{"title": "FBFL: A Field-Based Coordination Approach for Data Heterogeneity in\n  Federated Learning", "author": "Davide Domini and Gianluca Aguzzi and Lukas Esterle and Mirko Viroli", "abstract": "  In the last years, Federated learning (FL) has become a popular solution to\ntrain machine learning models in domains with high privacy concerns. However,\nFL scalability and performance face significant challenges in real-world\ndeployments where data across devices are non-independently and identically\ndistributed (non-IID). The heterogeneity in data distribution frequently arises\nfrom spatial distribution of devices, leading to degraded model performance in\nthe absence of proper handling. Additionally, FL typical reliance on\ncentralized architectures introduces bottlenecks and single-point-of-failure\nrisks, particularly problematic at scale or in dynamic environments. To close\nthis gap, we propose Field-Based Federated Learning (FBFL), a novel approach\nleveraging macroprogramming and field coordination to address these limitations\nthrough: (i) distributed spatial-based leader election for personalization to\nmitigate non-IID data challenges; and (ii) construction of a self-organizing,\nhierarchical architecture using advanced macroprogramming patterns. Moreover,\nFBFL not only overcomes the aforementioned limitations, but also enables the\ndevelopment of more specialized models tailored to the specific data\ndistribution in each subregion. This paper formalizes FBFL and evaluates it\nextensively using MNIST, FashionMNIST, and Extended MNIST datasets. We\ndemonstrate that, when operating under IID data conditions, FBFL performs\ncomparably to the widely-used FedAvg algorithm. Furthermore, in challenging\nnon-IID scenarios, FBFL not only outperforms FedAvg but also surpasses other\nstate-of-the-art methods, namely FedProx and Scaffold, which have been\nspecifically designed to address non-IID data distributions. Additionally, we\nshowcase the resilience of FBFL's self-organizing hierarchical architecture\nagainst server failures.\n", "link": "http://arxiv.org/abs/2502.08577v1", "date": "2025-02-12", "relevancy": 1.9917, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5112}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4887}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FBFL%3A%20A%20Field-Based%20Coordination%20Approach%20for%20Data%20Heterogeneity%20in%0A%20%20Federated%20Learning&body=Title%3A%20FBFL%3A%20A%20Field-Based%20Coordination%20Approach%20for%20Data%20Heterogeneity%20in%0A%20%20Federated%20Learning%0AAuthor%3A%20Davide%20Domini%20and%20Gianluca%20Aguzzi%20and%20Lukas%20Esterle%20and%20Mirko%20Viroli%0AAbstract%3A%20%20%20In%20the%20last%20years%2C%20Federated%20learning%20%28FL%29%20has%20become%20a%20popular%20solution%20to%0Atrain%20machine%20learning%20models%20in%20domains%20with%20high%20privacy%20concerns.%20However%2C%0AFL%20scalability%20and%20performance%20face%20significant%20challenges%20in%20real-world%0Adeployments%20where%20data%20across%20devices%20are%20non-independently%20and%20identically%0Adistributed%20%28non-IID%29.%20The%20heterogeneity%20in%20data%20distribution%20frequently%20arises%0Afrom%20spatial%20distribution%20of%20devices%2C%20leading%20to%20degraded%20model%20performance%20in%0Athe%20absence%20of%20proper%20handling.%20Additionally%2C%20FL%20typical%20reliance%20on%0Acentralized%20architectures%20introduces%20bottlenecks%20and%20single-point-of-failure%0Arisks%2C%20particularly%20problematic%20at%20scale%20or%20in%20dynamic%20environments.%20To%20close%0Athis%20gap%2C%20we%20propose%20Field-Based%20Federated%20Learning%20%28FBFL%29%2C%20a%20novel%20approach%0Aleveraging%20macroprogramming%20and%20field%20coordination%20to%20address%20these%20limitations%0Athrough%3A%20%28i%29%20distributed%20spatial-based%20leader%20election%20for%20personalization%20to%0Amitigate%20non-IID%20data%20challenges%3B%20and%20%28ii%29%20construction%20of%20a%20self-organizing%2C%0Ahierarchical%20architecture%20using%20advanced%20macroprogramming%20patterns.%20Moreover%2C%0AFBFL%20not%20only%20overcomes%20the%20aforementioned%20limitations%2C%20but%20also%20enables%20the%0Adevelopment%20of%20more%20specialized%20models%20tailored%20to%20the%20specific%20data%0Adistribution%20in%20each%20subregion.%20This%20paper%20formalizes%20FBFL%20and%20evaluates%20it%0Aextensively%20using%20MNIST%2C%20FashionMNIST%2C%20and%20Extended%20MNIST%20datasets.%20We%0Ademonstrate%20that%2C%20when%20operating%20under%20IID%20data%20conditions%2C%20FBFL%20performs%0Acomparably%20to%20the%20widely-used%20FedAvg%20algorithm.%20Furthermore%2C%20in%20challenging%0Anon-IID%20scenarios%2C%20FBFL%20not%20only%20outperforms%20FedAvg%20but%20also%20surpasses%20other%0Astate-of-the-art%20methods%2C%20namely%20FedProx%20and%20Scaffold%2C%20which%20have%20been%0Aspecifically%20designed%20to%20address%20non-IID%20data%20distributions.%20Additionally%2C%20we%0Ashowcase%20the%20resilience%20of%20FBFL%27s%20self-organizing%20hierarchical%20architecture%0Aagainst%20server%20failures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08577v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFBFL%253A%2520A%2520Field-Based%2520Coordination%2520Approach%2520for%2520Data%2520Heterogeneity%2520in%250A%2520%2520Federated%2520Learning%26entry.906535625%3DDavide%2520Domini%2520and%2520Gianluca%2520Aguzzi%2520and%2520Lukas%2520Esterle%2520and%2520Mirko%2520Viroli%26entry.1292438233%3D%2520%2520In%2520the%2520last%2520years%252C%2520Federated%2520learning%2520%2528FL%2529%2520has%2520become%2520a%2520popular%2520solution%2520to%250Atrain%2520machine%2520learning%2520models%2520in%2520domains%2520with%2520high%2520privacy%2520concerns.%2520However%252C%250AFL%2520scalability%2520and%2520performance%2520face%2520significant%2520challenges%2520in%2520real-world%250Adeployments%2520where%2520data%2520across%2520devices%2520are%2520non-independently%2520and%2520identically%250Adistributed%2520%2528non-IID%2529.%2520The%2520heterogeneity%2520in%2520data%2520distribution%2520frequently%2520arises%250Afrom%2520spatial%2520distribution%2520of%2520devices%252C%2520leading%2520to%2520degraded%2520model%2520performance%2520in%250Athe%2520absence%2520of%2520proper%2520handling.%2520Additionally%252C%2520FL%2520typical%2520reliance%2520on%250Acentralized%2520architectures%2520introduces%2520bottlenecks%2520and%2520single-point-of-failure%250Arisks%252C%2520particularly%2520problematic%2520at%2520scale%2520or%2520in%2520dynamic%2520environments.%2520To%2520close%250Athis%2520gap%252C%2520we%2520propose%2520Field-Based%2520Federated%2520Learning%2520%2528FBFL%2529%252C%2520a%2520novel%2520approach%250Aleveraging%2520macroprogramming%2520and%2520field%2520coordination%2520to%2520address%2520these%2520limitations%250Athrough%253A%2520%2528i%2529%2520distributed%2520spatial-based%2520leader%2520election%2520for%2520personalization%2520to%250Amitigate%2520non-IID%2520data%2520challenges%253B%2520and%2520%2528ii%2529%2520construction%2520of%2520a%2520self-organizing%252C%250Ahierarchical%2520architecture%2520using%2520advanced%2520macroprogramming%2520patterns.%2520Moreover%252C%250AFBFL%2520not%2520only%2520overcomes%2520the%2520aforementioned%2520limitations%252C%2520but%2520also%2520enables%2520the%250Adevelopment%2520of%2520more%2520specialized%2520models%2520tailored%2520to%2520the%2520specific%2520data%250Adistribution%2520in%2520each%2520subregion.%2520This%2520paper%2520formalizes%2520FBFL%2520and%2520evaluates%2520it%250Aextensively%2520using%2520MNIST%252C%2520FashionMNIST%252C%2520and%2520Extended%2520MNIST%2520datasets.%2520We%250Ademonstrate%2520that%252C%2520when%2520operating%2520under%2520IID%2520data%2520conditions%252C%2520FBFL%2520performs%250Acomparably%2520to%2520the%2520widely-used%2520FedAvg%2520algorithm.%2520Furthermore%252C%2520in%2520challenging%250Anon-IID%2520scenarios%252C%2520FBFL%2520not%2520only%2520outperforms%2520FedAvg%2520but%2520also%2520surpasses%2520other%250Astate-of-the-art%2520methods%252C%2520namely%2520FedProx%2520and%2520Scaffold%252C%2520which%2520have%2520been%250Aspecifically%2520designed%2520to%2520address%2520non-IID%2520data%2520distributions.%2520Additionally%252C%2520we%250Ashowcase%2520the%2520resilience%2520of%2520FBFL%2527s%2520self-organizing%2520hierarchical%2520architecture%250Aagainst%2520server%2520failures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08577v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FBFL%3A%20A%20Field-Based%20Coordination%20Approach%20for%20Data%20Heterogeneity%20in%0A%20%20Federated%20Learning&entry.906535625=Davide%20Domini%20and%20Gianluca%20Aguzzi%20and%20Lukas%20Esterle%20and%20Mirko%20Viroli&entry.1292438233=%20%20In%20the%20last%20years%2C%20Federated%20learning%20%28FL%29%20has%20become%20a%20popular%20solution%20to%0Atrain%20machine%20learning%20models%20in%20domains%20with%20high%20privacy%20concerns.%20However%2C%0AFL%20scalability%20and%20performance%20face%20significant%20challenges%20in%20real-world%0Adeployments%20where%20data%20across%20devices%20are%20non-independently%20and%20identically%0Adistributed%20%28non-IID%29.%20The%20heterogeneity%20in%20data%20distribution%20frequently%20arises%0Afrom%20spatial%20distribution%20of%20devices%2C%20leading%20to%20degraded%20model%20performance%20in%0Athe%20absence%20of%20proper%20handling.%20Additionally%2C%20FL%20typical%20reliance%20on%0Acentralized%20architectures%20introduces%20bottlenecks%20and%20single-point-of-failure%0Arisks%2C%20particularly%20problematic%20at%20scale%20or%20in%20dynamic%20environments.%20To%20close%0Athis%20gap%2C%20we%20propose%20Field-Based%20Federated%20Learning%20%28FBFL%29%2C%20a%20novel%20approach%0Aleveraging%20macroprogramming%20and%20field%20coordination%20to%20address%20these%20limitations%0Athrough%3A%20%28i%29%20distributed%20spatial-based%20leader%20election%20for%20personalization%20to%0Amitigate%20non-IID%20data%20challenges%3B%20and%20%28ii%29%20construction%20of%20a%20self-organizing%2C%0Ahierarchical%20architecture%20using%20advanced%20macroprogramming%20patterns.%20Moreover%2C%0AFBFL%20not%20only%20overcomes%20the%20aforementioned%20limitations%2C%20but%20also%20enables%20the%0Adevelopment%20of%20more%20specialized%20models%20tailored%20to%20the%20specific%20data%0Adistribution%20in%20each%20subregion.%20This%20paper%20formalizes%20FBFL%20and%20evaluates%20it%0Aextensively%20using%20MNIST%2C%20FashionMNIST%2C%20and%20Extended%20MNIST%20datasets.%20We%0Ademonstrate%20that%2C%20when%20operating%20under%20IID%20data%20conditions%2C%20FBFL%20performs%0Acomparably%20to%20the%20widely-used%20FedAvg%20algorithm.%20Furthermore%2C%20in%20challenging%0Anon-IID%20scenarios%2C%20FBFL%20not%20only%20outperforms%20FedAvg%20but%20also%20surpasses%20other%0Astate-of-the-art%20methods%2C%20namely%20FedProx%20and%20Scaffold%2C%20which%20have%20been%0Aspecifically%20designed%20to%20address%20non-IID%20data%20distributions.%20Additionally%2C%20we%0Ashowcase%20the%20resilience%20of%20FBFL%27s%20self-organizing%20hierarchical%20architecture%0Aagainst%20server%20failures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08577v1&entry.124074799=Read"},
{"title": "Matcha: Mitigating Graph Structure Shifts with Test-Time Adaptation", "author": "Wenxuan Bao and Zhichen Zeng and Zhining Liu and Hanghang Tong and Jingrui He", "abstract": "  Powerful as they are, graph neural networks (GNNs) are known to be vulnerable\nto distribution shifts. Recently, test-time adaptation (TTA) has attracted\nattention due to its ability to adapt a pre-trained model to a target domain,\nwithout re-accessing the source domain. However, existing TTA algorithms are\nprimarily designed for attribute shifts in vision tasks, where samples are\nindependent. These methods perform poorly on graph data that experience\nstructure shifts, where node connectivity differs between source and target\ngraphs. We attribute this performance gap to the distinct impact of node\nattribute shifts versus graph structure shifts: the latter significantly\ndegrades the quality of node representations and blurs the boundaries between\ndifferent node categories. To address structure shifts in graphs, we propose\nMatcha, an innovative framework designed for effective and efficient adaptation\nto structure shifts by adjusting the htop-aggregation parameters in GNNs. To\nenhance the representation quality, we design a prediction-informed clustering\nloss to encourage the formation of distinct clusters for different node\ncategories. Additionally, Matcha seamlessly integrates with existing TTA\nalgorithms, allowing it to handle attribute shifts effectively while improving\noverall performance under combined structure and attribute shifts. We validate\nthe effectiveness of Matcha on both synthetic and real-world datasets,\ndemonstrating its robustness across various combinations of structure and\nattribute shifts. Our code is available at https://github.com/baowenxuan/Matcha .\n", "link": "http://arxiv.org/abs/2410.06976v2", "date": "2025-02-12", "relevancy": 1.9906, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5038}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4977}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Matcha%3A%20Mitigating%20Graph%20Structure%20Shifts%20with%20Test-Time%20Adaptation&body=Title%3A%20Matcha%3A%20Mitigating%20Graph%20Structure%20Shifts%20with%20Test-Time%20Adaptation%0AAuthor%3A%20Wenxuan%20Bao%20and%20Zhichen%20Zeng%20and%20Zhining%20Liu%20and%20Hanghang%20Tong%20and%20Jingrui%20He%0AAbstract%3A%20%20%20Powerful%20as%20they%20are%2C%20graph%20neural%20networks%20%28GNNs%29%20are%20known%20to%20be%20vulnerable%0Ato%20distribution%20shifts.%20Recently%2C%20test-time%20adaptation%20%28TTA%29%20has%20attracted%0Aattention%20due%20to%20its%20ability%20to%20adapt%20a%20pre-trained%20model%20to%20a%20target%20domain%2C%0Awithout%20re-accessing%20the%20source%20domain.%20However%2C%20existing%20TTA%20algorithms%20are%0Aprimarily%20designed%20for%20attribute%20shifts%20in%20vision%20tasks%2C%20where%20samples%20are%0Aindependent.%20These%20methods%20perform%20poorly%20on%20graph%20data%20that%20experience%0Astructure%20shifts%2C%20where%20node%20connectivity%20differs%20between%20source%20and%20target%0Agraphs.%20We%20attribute%20this%20performance%20gap%20to%20the%20distinct%20impact%20of%20node%0Aattribute%20shifts%20versus%20graph%20structure%20shifts%3A%20the%20latter%20significantly%0Adegrades%20the%20quality%20of%20node%20representations%20and%20blurs%20the%20boundaries%20between%0Adifferent%20node%20categories.%20To%20address%20structure%20shifts%20in%20graphs%2C%20we%20propose%0AMatcha%2C%20an%20innovative%20framework%20designed%20for%20effective%20and%20efficient%20adaptation%0Ato%20structure%20shifts%20by%20adjusting%20the%20htop-aggregation%20parameters%20in%20GNNs.%20To%0Aenhance%20the%20representation%20quality%2C%20we%20design%20a%20prediction-informed%20clustering%0Aloss%20to%20encourage%20the%20formation%20of%20distinct%20clusters%20for%20different%20node%0Acategories.%20Additionally%2C%20Matcha%20seamlessly%20integrates%20with%20existing%20TTA%0Aalgorithms%2C%20allowing%20it%20to%20handle%20attribute%20shifts%20effectively%20while%20improving%0Aoverall%20performance%20under%20combined%20structure%20and%20attribute%20shifts.%20We%20validate%0Athe%20effectiveness%20of%20Matcha%20on%20both%20synthetic%20and%20real-world%20datasets%2C%0Ademonstrating%20its%20robustness%20across%20various%20combinations%20of%20structure%20and%0Aattribute%20shifts.%20Our%20code%20is%20available%20at%20https%3A//github.com/baowenxuan/Matcha%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06976v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMatcha%253A%2520Mitigating%2520Graph%2520Structure%2520Shifts%2520with%2520Test-Time%2520Adaptation%26entry.906535625%3DWenxuan%2520Bao%2520and%2520Zhichen%2520Zeng%2520and%2520Zhining%2520Liu%2520and%2520Hanghang%2520Tong%2520and%2520Jingrui%2520He%26entry.1292438233%3D%2520%2520Powerful%2520as%2520they%2520are%252C%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520are%2520known%2520to%2520be%2520vulnerable%250Ato%2520distribution%2520shifts.%2520Recently%252C%2520test-time%2520adaptation%2520%2528TTA%2529%2520has%2520attracted%250Aattention%2520due%2520to%2520its%2520ability%2520to%2520adapt%2520a%2520pre-trained%2520model%2520to%2520a%2520target%2520domain%252C%250Awithout%2520re-accessing%2520the%2520source%2520domain.%2520However%252C%2520existing%2520TTA%2520algorithms%2520are%250Aprimarily%2520designed%2520for%2520attribute%2520shifts%2520in%2520vision%2520tasks%252C%2520where%2520samples%2520are%250Aindependent.%2520These%2520methods%2520perform%2520poorly%2520on%2520graph%2520data%2520that%2520experience%250Astructure%2520shifts%252C%2520where%2520node%2520connectivity%2520differs%2520between%2520source%2520and%2520target%250Agraphs.%2520We%2520attribute%2520this%2520performance%2520gap%2520to%2520the%2520distinct%2520impact%2520of%2520node%250Aattribute%2520shifts%2520versus%2520graph%2520structure%2520shifts%253A%2520the%2520latter%2520significantly%250Adegrades%2520the%2520quality%2520of%2520node%2520representations%2520and%2520blurs%2520the%2520boundaries%2520between%250Adifferent%2520node%2520categories.%2520To%2520address%2520structure%2520shifts%2520in%2520graphs%252C%2520we%2520propose%250AMatcha%252C%2520an%2520innovative%2520framework%2520designed%2520for%2520effective%2520and%2520efficient%2520adaptation%250Ato%2520structure%2520shifts%2520by%2520adjusting%2520the%2520htop-aggregation%2520parameters%2520in%2520GNNs.%2520To%250Aenhance%2520the%2520representation%2520quality%252C%2520we%2520design%2520a%2520prediction-informed%2520clustering%250Aloss%2520to%2520encourage%2520the%2520formation%2520of%2520distinct%2520clusters%2520for%2520different%2520node%250Acategories.%2520Additionally%252C%2520Matcha%2520seamlessly%2520integrates%2520with%2520existing%2520TTA%250Aalgorithms%252C%2520allowing%2520it%2520to%2520handle%2520attribute%2520shifts%2520effectively%2520while%2520improving%250Aoverall%2520performance%2520under%2520combined%2520structure%2520and%2520attribute%2520shifts.%2520We%2520validate%250Athe%2520effectiveness%2520of%2520Matcha%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%252C%250Ademonstrating%2520its%2520robustness%2520across%2520various%2520combinations%2520of%2520structure%2520and%250Aattribute%2520shifts.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/baowenxuan/Matcha%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06976v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Matcha%3A%20Mitigating%20Graph%20Structure%20Shifts%20with%20Test-Time%20Adaptation&entry.906535625=Wenxuan%20Bao%20and%20Zhichen%20Zeng%20and%20Zhining%20Liu%20and%20Hanghang%20Tong%20and%20Jingrui%20He&entry.1292438233=%20%20Powerful%20as%20they%20are%2C%20graph%20neural%20networks%20%28GNNs%29%20are%20known%20to%20be%20vulnerable%0Ato%20distribution%20shifts.%20Recently%2C%20test-time%20adaptation%20%28TTA%29%20has%20attracted%0Aattention%20due%20to%20its%20ability%20to%20adapt%20a%20pre-trained%20model%20to%20a%20target%20domain%2C%0Awithout%20re-accessing%20the%20source%20domain.%20However%2C%20existing%20TTA%20algorithms%20are%0Aprimarily%20designed%20for%20attribute%20shifts%20in%20vision%20tasks%2C%20where%20samples%20are%0Aindependent.%20These%20methods%20perform%20poorly%20on%20graph%20data%20that%20experience%0Astructure%20shifts%2C%20where%20node%20connectivity%20differs%20between%20source%20and%20target%0Agraphs.%20We%20attribute%20this%20performance%20gap%20to%20the%20distinct%20impact%20of%20node%0Aattribute%20shifts%20versus%20graph%20structure%20shifts%3A%20the%20latter%20significantly%0Adegrades%20the%20quality%20of%20node%20representations%20and%20blurs%20the%20boundaries%20between%0Adifferent%20node%20categories.%20To%20address%20structure%20shifts%20in%20graphs%2C%20we%20propose%0AMatcha%2C%20an%20innovative%20framework%20designed%20for%20effective%20and%20efficient%20adaptation%0Ato%20structure%20shifts%20by%20adjusting%20the%20htop-aggregation%20parameters%20in%20GNNs.%20To%0Aenhance%20the%20representation%20quality%2C%20we%20design%20a%20prediction-informed%20clustering%0Aloss%20to%20encourage%20the%20formation%20of%20distinct%20clusters%20for%20different%20node%0Acategories.%20Additionally%2C%20Matcha%20seamlessly%20integrates%20with%20existing%20TTA%0Aalgorithms%2C%20allowing%20it%20to%20handle%20attribute%20shifts%20effectively%20while%20improving%0Aoverall%20performance%20under%20combined%20structure%20and%20attribute%20shifts.%20We%20validate%0Athe%20effectiveness%20of%20Matcha%20on%20both%20synthetic%20and%20real-world%20datasets%2C%0Ademonstrating%20its%20robustness%20across%20various%20combinations%20of%20structure%20and%0Aattribute%20shifts.%20Our%20code%20is%20available%20at%20https%3A//github.com/baowenxuan/Matcha%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06976v2&entry.124074799=Read"},
{"title": "Shadow Program Inversion with Differentiable Planning: A Framework for\n  Unified Robot Program Parameter and Trajectory Optimization", "author": "Benjamin Alt and Claudius Kienle and Darko Katic and Rainer J\u00e4kel and Michael Beetz", "abstract": "  This paper presents SPI-DP, a novel first-order optimizer capable of\noptimizing robot programs with respect to both high-level task objectives and\nmotion-level constraints. To that end, we introduce DGPMP2-ND, a differentiable\ncollision-free motion planner for serial N-DoF kinematics, and integrate it\ninto an iterative, gradient-based optimization approach for generic,\nparameterized robot program representations. SPI-DP allows first-order\noptimization of planned trajectories and program parameters with respect to\nobjectives such as cycle time or smoothness subject to e.g. collision\nconstraints, while enabling humans to understand, modify or even certify the\noptimized programs. We provide a comprehensive evaluation on two practical\nhousehold and industrial applications.\n", "link": "http://arxiv.org/abs/2409.08678v2", "date": "2025-02-12", "relevancy": 1.966, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5214}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4891}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4819}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shadow%20Program%20Inversion%20with%20Differentiable%20Planning%3A%20A%20Framework%20for%0A%20%20Unified%20Robot%20Program%20Parameter%20and%20Trajectory%20Optimization&body=Title%3A%20Shadow%20Program%20Inversion%20with%20Differentiable%20Planning%3A%20A%20Framework%20for%0A%20%20Unified%20Robot%20Program%20Parameter%20and%20Trajectory%20Optimization%0AAuthor%3A%20Benjamin%20Alt%20and%20Claudius%20Kienle%20and%20Darko%20Katic%20and%20Rainer%20J%C3%A4kel%20and%20Michael%20Beetz%0AAbstract%3A%20%20%20This%20paper%20presents%20SPI-DP%2C%20a%20novel%20first-order%20optimizer%20capable%20of%0Aoptimizing%20robot%20programs%20with%20respect%20to%20both%20high-level%20task%20objectives%20and%0Amotion-level%20constraints.%20To%20that%20end%2C%20we%20introduce%20DGPMP2-ND%2C%20a%20differentiable%0Acollision-free%20motion%20planner%20for%20serial%20N-DoF%20kinematics%2C%20and%20integrate%20it%0Ainto%20an%20iterative%2C%20gradient-based%20optimization%20approach%20for%20generic%2C%0Aparameterized%20robot%20program%20representations.%20SPI-DP%20allows%20first-order%0Aoptimization%20of%20planned%20trajectories%20and%20program%20parameters%20with%20respect%20to%0Aobjectives%20such%20as%20cycle%20time%20or%20smoothness%20subject%20to%20e.g.%20collision%0Aconstraints%2C%20while%20enabling%20humans%20to%20understand%2C%20modify%20or%20even%20certify%20the%0Aoptimized%20programs.%20We%20provide%20a%20comprehensive%20evaluation%20on%20two%20practical%0Ahousehold%20and%20industrial%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08678v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShadow%2520Program%2520Inversion%2520with%2520Differentiable%2520Planning%253A%2520A%2520Framework%2520for%250A%2520%2520Unified%2520Robot%2520Program%2520Parameter%2520and%2520Trajectory%2520Optimization%26entry.906535625%3DBenjamin%2520Alt%2520and%2520Claudius%2520Kienle%2520and%2520Darko%2520Katic%2520and%2520Rainer%2520J%25C3%25A4kel%2520and%2520Michael%2520Beetz%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520SPI-DP%252C%2520a%2520novel%2520first-order%2520optimizer%2520capable%2520of%250Aoptimizing%2520robot%2520programs%2520with%2520respect%2520to%2520both%2520high-level%2520task%2520objectives%2520and%250Amotion-level%2520constraints.%2520To%2520that%2520end%252C%2520we%2520introduce%2520DGPMP2-ND%252C%2520a%2520differentiable%250Acollision-free%2520motion%2520planner%2520for%2520serial%2520N-DoF%2520kinematics%252C%2520and%2520integrate%2520it%250Ainto%2520an%2520iterative%252C%2520gradient-based%2520optimization%2520approach%2520for%2520generic%252C%250Aparameterized%2520robot%2520program%2520representations.%2520SPI-DP%2520allows%2520first-order%250Aoptimization%2520of%2520planned%2520trajectories%2520and%2520program%2520parameters%2520with%2520respect%2520to%250Aobjectives%2520such%2520as%2520cycle%2520time%2520or%2520smoothness%2520subject%2520to%2520e.g.%2520collision%250Aconstraints%252C%2520while%2520enabling%2520humans%2520to%2520understand%252C%2520modify%2520or%2520even%2520certify%2520the%250Aoptimized%2520programs.%2520We%2520provide%2520a%2520comprehensive%2520evaluation%2520on%2520two%2520practical%250Ahousehold%2520and%2520industrial%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08678v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shadow%20Program%20Inversion%20with%20Differentiable%20Planning%3A%20A%20Framework%20for%0A%20%20Unified%20Robot%20Program%20Parameter%20and%20Trajectory%20Optimization&entry.906535625=Benjamin%20Alt%20and%20Claudius%20Kienle%20and%20Darko%20Katic%20and%20Rainer%20J%C3%A4kel%20and%20Michael%20Beetz&entry.1292438233=%20%20This%20paper%20presents%20SPI-DP%2C%20a%20novel%20first-order%20optimizer%20capable%20of%0Aoptimizing%20robot%20programs%20with%20respect%20to%20both%20high-level%20task%20objectives%20and%0Amotion-level%20constraints.%20To%20that%20end%2C%20we%20introduce%20DGPMP2-ND%2C%20a%20differentiable%0Acollision-free%20motion%20planner%20for%20serial%20N-DoF%20kinematics%2C%20and%20integrate%20it%0Ainto%20an%20iterative%2C%20gradient-based%20optimization%20approach%20for%20generic%2C%0Aparameterized%20robot%20program%20representations.%20SPI-DP%20allows%20first-order%0Aoptimization%20of%20planned%20trajectories%20and%20program%20parameters%20with%20respect%20to%0Aobjectives%20such%20as%20cycle%20time%20or%20smoothness%20subject%20to%20e.g.%20collision%0Aconstraints%2C%20while%20enabling%20humans%20to%20understand%2C%20modify%20or%20even%20certify%20the%0Aoptimized%20programs.%20We%20provide%20a%20comprehensive%20evaluation%20on%20two%20practical%0Ahousehold%20and%20industrial%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08678v2&entry.124074799=Read"},
{"title": "Rhythmic sharing: A bio-inspired paradigm for zero-shot adaptation and\n  learning in neural networks", "author": "Hoony Kang and Wolfgang Losert", "abstract": "  The brain can rapidly adapt to new contexts and learn from limited data, a\ncoveted characteristic that artificial intelligence algorithms have struggled\nto mimic. Inspired by oscillatory rhythms of the mechanical structures of\nneural cells, we developed a learning paradigm that is based on oscillations in\nlink strengths and associates learning with the coordination of these\noscillations. We find that this paradigm yields rapid adaptation and learning\nin artificial neural networks. Link oscillations can rapidly change\ncoordination, endowing the network with the ability to sense subtle context\nchanges in an unsupervised manner. In other words, the network generates the\nmissing contextual tokens required to perform as a generalist AI architecture\ncapable of predicting dynamics in multiple contexts. Oscillations also allow\nthe network to extrapolate dynamics to never-seen-before contexts. These\ncapabilities make our learning paradigm a powerful starting point for novel\nmodels of learning and cognition. Furthermore, learning through link\ncoordination is agnostic to the specifics of the neural network architecture,\nhence our study opens the door for introducing rapid adaptation and learning\ncapabilities into leading AI models.\n", "link": "http://arxiv.org/abs/2502.08644v1", "date": "2025-02-12", "relevancy": 1.9613, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5105}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4863}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rhythmic%20sharing%3A%20A%20bio-inspired%20paradigm%20for%20zero-shot%20adaptation%20and%0A%20%20learning%20in%20neural%20networks&body=Title%3A%20Rhythmic%20sharing%3A%20A%20bio-inspired%20paradigm%20for%20zero-shot%20adaptation%20and%0A%20%20learning%20in%20neural%20networks%0AAuthor%3A%20Hoony%20Kang%20and%20Wolfgang%20Losert%0AAbstract%3A%20%20%20The%20brain%20can%20rapidly%20adapt%20to%20new%20contexts%20and%20learn%20from%20limited%20data%2C%20a%0Acoveted%20characteristic%20that%20artificial%20intelligence%20algorithms%20have%20struggled%0Ato%20mimic.%20Inspired%20by%20oscillatory%20rhythms%20of%20the%20mechanical%20structures%20of%0Aneural%20cells%2C%20we%20developed%20a%20learning%20paradigm%20that%20is%20based%20on%20oscillations%20in%0Alink%20strengths%20and%20associates%20learning%20with%20the%20coordination%20of%20these%0Aoscillations.%20We%20find%20that%20this%20paradigm%20yields%20rapid%20adaptation%20and%20learning%0Ain%20artificial%20neural%20networks.%20Link%20oscillations%20can%20rapidly%20change%0Acoordination%2C%20endowing%20the%20network%20with%20the%20ability%20to%20sense%20subtle%20context%0Achanges%20in%20an%20unsupervised%20manner.%20In%20other%20words%2C%20the%20network%20generates%20the%0Amissing%20contextual%20tokens%20required%20to%20perform%20as%20a%20generalist%20AI%20architecture%0Acapable%20of%20predicting%20dynamics%20in%20multiple%20contexts.%20Oscillations%20also%20allow%0Athe%20network%20to%20extrapolate%20dynamics%20to%20never-seen-before%20contexts.%20These%0Acapabilities%20make%20our%20learning%20paradigm%20a%20powerful%20starting%20point%20for%20novel%0Amodels%20of%20learning%20and%20cognition.%20Furthermore%2C%20learning%20through%20link%0Acoordination%20is%20agnostic%20to%20the%20specifics%20of%20the%20neural%20network%20architecture%2C%0Ahence%20our%20study%20opens%20the%20door%20for%20introducing%20rapid%20adaptation%20and%20learning%0Acapabilities%20into%20leading%20AI%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08644v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRhythmic%2520sharing%253A%2520A%2520bio-inspired%2520paradigm%2520for%2520zero-shot%2520adaptation%2520and%250A%2520%2520learning%2520in%2520neural%2520networks%26entry.906535625%3DHoony%2520Kang%2520and%2520Wolfgang%2520Losert%26entry.1292438233%3D%2520%2520The%2520brain%2520can%2520rapidly%2520adapt%2520to%2520new%2520contexts%2520and%2520learn%2520from%2520limited%2520data%252C%2520a%250Acoveted%2520characteristic%2520that%2520artificial%2520intelligence%2520algorithms%2520have%2520struggled%250Ato%2520mimic.%2520Inspired%2520by%2520oscillatory%2520rhythms%2520of%2520the%2520mechanical%2520structures%2520of%250Aneural%2520cells%252C%2520we%2520developed%2520a%2520learning%2520paradigm%2520that%2520is%2520based%2520on%2520oscillations%2520in%250Alink%2520strengths%2520and%2520associates%2520learning%2520with%2520the%2520coordination%2520of%2520these%250Aoscillations.%2520We%2520find%2520that%2520this%2520paradigm%2520yields%2520rapid%2520adaptation%2520and%2520learning%250Ain%2520artificial%2520neural%2520networks.%2520Link%2520oscillations%2520can%2520rapidly%2520change%250Acoordination%252C%2520endowing%2520the%2520network%2520with%2520the%2520ability%2520to%2520sense%2520subtle%2520context%250Achanges%2520in%2520an%2520unsupervised%2520manner.%2520In%2520other%2520words%252C%2520the%2520network%2520generates%2520the%250Amissing%2520contextual%2520tokens%2520required%2520to%2520perform%2520as%2520a%2520generalist%2520AI%2520architecture%250Acapable%2520of%2520predicting%2520dynamics%2520in%2520multiple%2520contexts.%2520Oscillations%2520also%2520allow%250Athe%2520network%2520to%2520extrapolate%2520dynamics%2520to%2520never-seen-before%2520contexts.%2520These%250Acapabilities%2520make%2520our%2520learning%2520paradigm%2520a%2520powerful%2520starting%2520point%2520for%2520novel%250Amodels%2520of%2520learning%2520and%2520cognition.%2520Furthermore%252C%2520learning%2520through%2520link%250Acoordination%2520is%2520agnostic%2520to%2520the%2520specifics%2520of%2520the%2520neural%2520network%2520architecture%252C%250Ahence%2520our%2520study%2520opens%2520the%2520door%2520for%2520introducing%2520rapid%2520adaptation%2520and%2520learning%250Acapabilities%2520into%2520leading%2520AI%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08644v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rhythmic%20sharing%3A%20A%20bio-inspired%20paradigm%20for%20zero-shot%20adaptation%20and%0A%20%20learning%20in%20neural%20networks&entry.906535625=Hoony%20Kang%20and%20Wolfgang%20Losert&entry.1292438233=%20%20The%20brain%20can%20rapidly%20adapt%20to%20new%20contexts%20and%20learn%20from%20limited%20data%2C%20a%0Acoveted%20characteristic%20that%20artificial%20intelligence%20algorithms%20have%20struggled%0Ato%20mimic.%20Inspired%20by%20oscillatory%20rhythms%20of%20the%20mechanical%20structures%20of%0Aneural%20cells%2C%20we%20developed%20a%20learning%20paradigm%20that%20is%20based%20on%20oscillations%20in%0Alink%20strengths%20and%20associates%20learning%20with%20the%20coordination%20of%20these%0Aoscillations.%20We%20find%20that%20this%20paradigm%20yields%20rapid%20adaptation%20and%20learning%0Ain%20artificial%20neural%20networks.%20Link%20oscillations%20can%20rapidly%20change%0Acoordination%2C%20endowing%20the%20network%20with%20the%20ability%20to%20sense%20subtle%20context%0Achanges%20in%20an%20unsupervised%20manner.%20In%20other%20words%2C%20the%20network%20generates%20the%0Amissing%20contextual%20tokens%20required%20to%20perform%20as%20a%20generalist%20AI%20architecture%0Acapable%20of%20predicting%20dynamics%20in%20multiple%20contexts.%20Oscillations%20also%20allow%0Athe%20network%20to%20extrapolate%20dynamics%20to%20never-seen-before%20contexts.%20These%0Acapabilities%20make%20our%20learning%20paradigm%20a%20powerful%20starting%20point%20for%20novel%0Amodels%20of%20learning%20and%20cognition.%20Furthermore%2C%20learning%20through%20link%0Acoordination%20is%20agnostic%20to%20the%20specifics%20of%20the%20neural%20network%20architecture%2C%0Ahence%20our%20study%20opens%20the%20door%20for%20introducing%20rapid%20adaptation%20and%20learning%0Acapabilities%20into%20leading%20AI%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08644v1&entry.124074799=Read"},
{"title": "LLM Pretraining with Continuous Concepts", "author": "Jihoon Tack and Jack Lanchantin and Jane Yu and Andrew Cohen and Ilia Kulikov and Janice Lan and Shibo Hao and Yuandong Tian and Jason Weston and Xian Li", "abstract": "  Next token prediction has been the standard training objective used in large\nlanguage model pretraining. Representations are learned as a result of\noptimizing for token-level perplexity. We propose Continuous Concept Mixing\n(CoCoMix), a novel pretraining framework that combines discrete next token\nprediction with continuous concepts. Specifically, CoCoMix predicts continuous\nconcepts learned from a pretrained sparse autoencoder and mixes them into the\nmodel's hidden state by interleaving with token hidden representations. Through\nexperiments on multiple benchmarks, including language modeling and downstream\nreasoning tasks, we show that CoCoMix is more sample efficient and consistently\noutperforms standard next token prediction, knowledge distillation and\ninserting pause tokens. We find that combining both concept learning and\ninterleaving in an end-to-end framework is critical to performance gains.\nFurthermore, CoCoMix enhances interpretability and steerability by allowing\ndirect inspection and modification of the predicted concept, offering a\ntransparent way to guide the model's internal reasoning process.\n", "link": "http://arxiv.org/abs/2502.08524v1", "date": "2025-02-12", "relevancy": 1.9609, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4932}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4918}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4874}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM%20Pretraining%20with%20Continuous%20Concepts&body=Title%3A%20LLM%20Pretraining%20with%20Continuous%20Concepts%0AAuthor%3A%20Jihoon%20Tack%20and%20Jack%20Lanchantin%20and%20Jane%20Yu%20and%20Andrew%20Cohen%20and%20Ilia%20Kulikov%20and%20Janice%20Lan%20and%20Shibo%20Hao%20and%20Yuandong%20Tian%20and%20Jason%20Weston%20and%20Xian%20Li%0AAbstract%3A%20%20%20Next%20token%20prediction%20has%20been%20the%20standard%20training%20objective%20used%20in%20large%0Alanguage%20model%20pretraining.%20Representations%20are%20learned%20as%20a%20result%20of%0Aoptimizing%20for%20token-level%20perplexity.%20We%20propose%20Continuous%20Concept%20Mixing%0A%28CoCoMix%29%2C%20a%20novel%20pretraining%20framework%20that%20combines%20discrete%20next%20token%0Aprediction%20with%20continuous%20concepts.%20Specifically%2C%20CoCoMix%20predicts%20continuous%0Aconcepts%20learned%20from%20a%20pretrained%20sparse%20autoencoder%20and%20mixes%20them%20into%20the%0Amodel%27s%20hidden%20state%20by%20interleaving%20with%20token%20hidden%20representations.%20Through%0Aexperiments%20on%20multiple%20benchmarks%2C%20including%20language%20modeling%20and%20downstream%0Areasoning%20tasks%2C%20we%20show%20that%20CoCoMix%20is%20more%20sample%20efficient%20and%20consistently%0Aoutperforms%20standard%20next%20token%20prediction%2C%20knowledge%20distillation%20and%0Ainserting%20pause%20tokens.%20We%20find%20that%20combining%20both%20concept%20learning%20and%0Ainterleaving%20in%20an%20end-to-end%20framework%20is%20critical%20to%20performance%20gains.%0AFurthermore%2C%20CoCoMix%20enhances%20interpretability%20and%20steerability%20by%20allowing%0Adirect%20inspection%20and%20modification%20of%20the%20predicted%20concept%2C%20offering%20a%0Atransparent%20way%20to%20guide%20the%20model%27s%20internal%20reasoning%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08524v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM%2520Pretraining%2520with%2520Continuous%2520Concepts%26entry.906535625%3DJihoon%2520Tack%2520and%2520Jack%2520Lanchantin%2520and%2520Jane%2520Yu%2520and%2520Andrew%2520Cohen%2520and%2520Ilia%2520Kulikov%2520and%2520Janice%2520Lan%2520and%2520Shibo%2520Hao%2520and%2520Yuandong%2520Tian%2520and%2520Jason%2520Weston%2520and%2520Xian%2520Li%26entry.1292438233%3D%2520%2520Next%2520token%2520prediction%2520has%2520been%2520the%2520standard%2520training%2520objective%2520used%2520in%2520large%250Alanguage%2520model%2520pretraining.%2520Representations%2520are%2520learned%2520as%2520a%2520result%2520of%250Aoptimizing%2520for%2520token-level%2520perplexity.%2520We%2520propose%2520Continuous%2520Concept%2520Mixing%250A%2528CoCoMix%2529%252C%2520a%2520novel%2520pretraining%2520framework%2520that%2520combines%2520discrete%2520next%2520token%250Aprediction%2520with%2520continuous%2520concepts.%2520Specifically%252C%2520CoCoMix%2520predicts%2520continuous%250Aconcepts%2520learned%2520from%2520a%2520pretrained%2520sparse%2520autoencoder%2520and%2520mixes%2520them%2520into%2520the%250Amodel%2527s%2520hidden%2520state%2520by%2520interleaving%2520with%2520token%2520hidden%2520representations.%2520Through%250Aexperiments%2520on%2520multiple%2520benchmarks%252C%2520including%2520language%2520modeling%2520and%2520downstream%250Areasoning%2520tasks%252C%2520we%2520show%2520that%2520CoCoMix%2520is%2520more%2520sample%2520efficient%2520and%2520consistently%250Aoutperforms%2520standard%2520next%2520token%2520prediction%252C%2520knowledge%2520distillation%2520and%250Ainserting%2520pause%2520tokens.%2520We%2520find%2520that%2520combining%2520both%2520concept%2520learning%2520and%250Ainterleaving%2520in%2520an%2520end-to-end%2520framework%2520is%2520critical%2520to%2520performance%2520gains.%250AFurthermore%252C%2520CoCoMix%2520enhances%2520interpretability%2520and%2520steerability%2520by%2520allowing%250Adirect%2520inspection%2520and%2520modification%2520of%2520the%2520predicted%2520concept%252C%2520offering%2520a%250Atransparent%2520way%2520to%2520guide%2520the%2520model%2527s%2520internal%2520reasoning%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08524v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM%20Pretraining%20with%20Continuous%20Concepts&entry.906535625=Jihoon%20Tack%20and%20Jack%20Lanchantin%20and%20Jane%20Yu%20and%20Andrew%20Cohen%20and%20Ilia%20Kulikov%20and%20Janice%20Lan%20and%20Shibo%20Hao%20and%20Yuandong%20Tian%20and%20Jason%20Weston%20and%20Xian%20Li&entry.1292438233=%20%20Next%20token%20prediction%20has%20been%20the%20standard%20training%20objective%20used%20in%20large%0Alanguage%20model%20pretraining.%20Representations%20are%20learned%20as%20a%20result%20of%0Aoptimizing%20for%20token-level%20perplexity.%20We%20propose%20Continuous%20Concept%20Mixing%0A%28CoCoMix%29%2C%20a%20novel%20pretraining%20framework%20that%20combines%20discrete%20next%20token%0Aprediction%20with%20continuous%20concepts.%20Specifically%2C%20CoCoMix%20predicts%20continuous%0Aconcepts%20learned%20from%20a%20pretrained%20sparse%20autoencoder%20and%20mixes%20them%20into%20the%0Amodel%27s%20hidden%20state%20by%20interleaving%20with%20token%20hidden%20representations.%20Through%0Aexperiments%20on%20multiple%20benchmarks%2C%20including%20language%20modeling%20and%20downstream%0Areasoning%20tasks%2C%20we%20show%20that%20CoCoMix%20is%20more%20sample%20efficient%20and%20consistently%0Aoutperforms%20standard%20next%20token%20prediction%2C%20knowledge%20distillation%20and%0Ainserting%20pause%20tokens.%20We%20find%20that%20combining%20both%20concept%20learning%20and%0Ainterleaving%20in%20an%20end-to-end%20framework%20is%20critical%20to%20performance%20gains.%0AFurthermore%2C%20CoCoMix%20enhances%20interpretability%20and%20steerability%20by%20allowing%0Adirect%20inspection%20and%20modification%20of%20the%20predicted%20concept%2C%20offering%20a%0Atransparent%20way%20to%20guide%20the%20model%27s%20internal%20reasoning%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08524v1&entry.124074799=Read"},
{"title": "Moment of Untruth: Dealing with Negative Queries in Video Moment\n  Retrieval", "author": "Kevin Flanagan and Dima Damen and Michael Wray", "abstract": "  Video Moment Retrieval is a common task to evaluate the performance of\nvisual-language models - it involves localising start and end times of moments\nin videos from query sentences. The current task formulation assumes that the\nqueried moment is present in the video, resulting in false positive moment\npredictions when irrelevant query sentences are provided.\n  In this paper we propose the task of Negative-Aware Video Moment Retrieval\n(NA-VMR), which considers both moment retrieval accuracy and negative query\nrejection accuracy. We make the distinction between In-Domain and Out-of-Domain\nnegative queries and provide new evaluation benchmarks for two popular video\nmoment retrieval datasets: QVHighlights and Charades-STA. We analyse the\nability of current SOTA video moment retrieval approaches to adapt to\nNegative-Aware Video Moment Retrieval and propose UniVTG-NA, an adaptation of\nUniVTG designed to tackle NA-VMR. UniVTG-NA achieves high negative rejection\naccuracy (avg. $98.4\\%$) scores while retaining moment retrieval scores to\nwithin $3.87\\%$ Recall@1. Dataset splits and code are available at\nhttps://github.com/keflanagan/MomentofUntruth\n", "link": "http://arxiv.org/abs/2502.08544v1", "date": "2025-02-12", "relevancy": 1.9592, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4899}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4899}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4894}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Moment%20of%20Untruth%3A%20Dealing%20with%20Negative%20Queries%20in%20Video%20Moment%0A%20%20Retrieval&body=Title%3A%20Moment%20of%20Untruth%3A%20Dealing%20with%20Negative%20Queries%20in%20Video%20Moment%0A%20%20Retrieval%0AAuthor%3A%20Kevin%20Flanagan%20and%20Dima%20Damen%20and%20Michael%20Wray%0AAbstract%3A%20%20%20Video%20Moment%20Retrieval%20is%20a%20common%20task%20to%20evaluate%20the%20performance%20of%0Avisual-language%20models%20-%20it%20involves%20localising%20start%20and%20end%20times%20of%20moments%0Ain%20videos%20from%20query%20sentences.%20The%20current%20task%20formulation%20assumes%20that%20the%0Aqueried%20moment%20is%20present%20in%20the%20video%2C%20resulting%20in%20false%20positive%20moment%0Apredictions%20when%20irrelevant%20query%20sentences%20are%20provided.%0A%20%20In%20this%20paper%20we%20propose%20the%20task%20of%20Negative-Aware%20Video%20Moment%20Retrieval%0A%28NA-VMR%29%2C%20which%20considers%20both%20moment%20retrieval%20accuracy%20and%20negative%20query%0Arejection%20accuracy.%20We%20make%20the%20distinction%20between%20In-Domain%20and%20Out-of-Domain%0Anegative%20queries%20and%20provide%20new%20evaluation%20benchmarks%20for%20two%20popular%20video%0Amoment%20retrieval%20datasets%3A%20QVHighlights%20and%20Charades-STA.%20We%20analyse%20the%0Aability%20of%20current%20SOTA%20video%20moment%20retrieval%20approaches%20to%20adapt%20to%0ANegative-Aware%20Video%20Moment%20Retrieval%20and%20propose%20UniVTG-NA%2C%20an%20adaptation%20of%0AUniVTG%20designed%20to%20tackle%20NA-VMR.%20UniVTG-NA%20achieves%20high%20negative%20rejection%0Aaccuracy%20%28avg.%20%2498.4%5C%25%24%29%20scores%20while%20retaining%20moment%20retrieval%20scores%20to%0Awithin%20%243.87%5C%25%24%20Recall%401.%20Dataset%20splits%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/keflanagan/MomentofUntruth%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08544v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoment%2520of%2520Untruth%253A%2520Dealing%2520with%2520Negative%2520Queries%2520in%2520Video%2520Moment%250A%2520%2520Retrieval%26entry.906535625%3DKevin%2520Flanagan%2520and%2520Dima%2520Damen%2520and%2520Michael%2520Wray%26entry.1292438233%3D%2520%2520Video%2520Moment%2520Retrieval%2520is%2520a%2520common%2520task%2520to%2520evaluate%2520the%2520performance%2520of%250Avisual-language%2520models%2520-%2520it%2520involves%2520localising%2520start%2520and%2520end%2520times%2520of%2520moments%250Ain%2520videos%2520from%2520query%2520sentences.%2520The%2520current%2520task%2520formulation%2520assumes%2520that%2520the%250Aqueried%2520moment%2520is%2520present%2520in%2520the%2520video%252C%2520resulting%2520in%2520false%2520positive%2520moment%250Apredictions%2520when%2520irrelevant%2520query%2520sentences%2520are%2520provided.%250A%2520%2520In%2520this%2520paper%2520we%2520propose%2520the%2520task%2520of%2520Negative-Aware%2520Video%2520Moment%2520Retrieval%250A%2528NA-VMR%2529%252C%2520which%2520considers%2520both%2520moment%2520retrieval%2520accuracy%2520and%2520negative%2520query%250Arejection%2520accuracy.%2520We%2520make%2520the%2520distinction%2520between%2520In-Domain%2520and%2520Out-of-Domain%250Anegative%2520queries%2520and%2520provide%2520new%2520evaluation%2520benchmarks%2520for%2520two%2520popular%2520video%250Amoment%2520retrieval%2520datasets%253A%2520QVHighlights%2520and%2520Charades-STA.%2520We%2520analyse%2520the%250Aability%2520of%2520current%2520SOTA%2520video%2520moment%2520retrieval%2520approaches%2520to%2520adapt%2520to%250ANegative-Aware%2520Video%2520Moment%2520Retrieval%2520and%2520propose%2520UniVTG-NA%252C%2520an%2520adaptation%2520of%250AUniVTG%2520designed%2520to%2520tackle%2520NA-VMR.%2520UniVTG-NA%2520achieves%2520high%2520negative%2520rejection%250Aaccuracy%2520%2528avg.%2520%252498.4%255C%2525%2524%2529%2520scores%2520while%2520retaining%2520moment%2520retrieval%2520scores%2520to%250Awithin%2520%25243.87%255C%2525%2524%2520Recall%25401.%2520Dataset%2520splits%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/keflanagan/MomentofUntruth%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08544v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Moment%20of%20Untruth%3A%20Dealing%20with%20Negative%20Queries%20in%20Video%20Moment%0A%20%20Retrieval&entry.906535625=Kevin%20Flanagan%20and%20Dima%20Damen%20and%20Michael%20Wray&entry.1292438233=%20%20Video%20Moment%20Retrieval%20is%20a%20common%20task%20to%20evaluate%20the%20performance%20of%0Avisual-language%20models%20-%20it%20involves%20localising%20start%20and%20end%20times%20of%20moments%0Ain%20videos%20from%20query%20sentences.%20The%20current%20task%20formulation%20assumes%20that%20the%0Aqueried%20moment%20is%20present%20in%20the%20video%2C%20resulting%20in%20false%20positive%20moment%0Apredictions%20when%20irrelevant%20query%20sentences%20are%20provided.%0A%20%20In%20this%20paper%20we%20propose%20the%20task%20of%20Negative-Aware%20Video%20Moment%20Retrieval%0A%28NA-VMR%29%2C%20which%20considers%20both%20moment%20retrieval%20accuracy%20and%20negative%20query%0Arejection%20accuracy.%20We%20make%20the%20distinction%20between%20In-Domain%20and%20Out-of-Domain%0Anegative%20queries%20and%20provide%20new%20evaluation%20benchmarks%20for%20two%20popular%20video%0Amoment%20retrieval%20datasets%3A%20QVHighlights%20and%20Charades-STA.%20We%20analyse%20the%0Aability%20of%20current%20SOTA%20video%20moment%20retrieval%20approaches%20to%20adapt%20to%0ANegative-Aware%20Video%20Moment%20Retrieval%20and%20propose%20UniVTG-NA%2C%20an%20adaptation%20of%0AUniVTG%20designed%20to%20tackle%20NA-VMR.%20UniVTG-NA%20achieves%20high%20negative%20rejection%0Aaccuracy%20%28avg.%20%2498.4%5C%25%24%29%20scores%20while%20retaining%20moment%20retrieval%20scores%20to%0Awithin%20%243.87%5C%25%24%20Recall%401.%20Dataset%20splits%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/keflanagan/MomentofUntruth%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08544v1&entry.124074799=Read"},
{"title": "Enhancing Auto-regressive Chain-of-Thought through Loop-Aligned\n  Reasoning", "author": "Qifan Yu and Zhenyu He and Sijie Li and Xun Zhou and Jun Zhang and Jingjing Xu and Di He", "abstract": "  Chain-of-Thought (CoT) prompting has emerged as a powerful technique for\nenhancing language model's reasoning capabilities. However, generating long and\ncorrect CoT trajectories is challenging. Recent studies have demonstrated that\nLooped Transformers possess remarkable length generalization capabilities, but\ntheir limited generality and adaptability prevent them from serving as an\nalternative to auto-regressive solutions. To better leverage the strengths of\nLooped Transformers, we propose RELAY (REasoning through Loop Alignment\niterativelY). Specifically, we align the steps of Chain-of-Thought (CoT)\nreasoning with loop iterations and apply intermediate supervision during the\ntraining of Looped Transformers. This additional iteration-wise supervision not\nonly preserves the Looped Transformer's ability for length generalization but\nalso enables it to predict CoT reasoning steps for unseen data. Therefore, we\nleverage this Looped Transformer to generate accurate reasoning chains for\ncomplex problems that exceed the training length, which will then be used to\nfine-tune an auto-regressive model. We conduct extensive experiments, and the\nresults demonstrate the effectiveness of our approach, with significant\nimprovements in the performance of the auto-regressive model. Code will be\nreleased at https://github.com/qifanyu/RELAY.\n", "link": "http://arxiv.org/abs/2502.08482v1", "date": "2025-02-12", "relevancy": 1.9507, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4997}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4866}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Auto-regressive%20Chain-of-Thought%20through%20Loop-Aligned%0A%20%20Reasoning&body=Title%3A%20Enhancing%20Auto-regressive%20Chain-of-Thought%20through%20Loop-Aligned%0A%20%20Reasoning%0AAuthor%3A%20Qifan%20Yu%20and%20Zhenyu%20He%20and%20Sijie%20Li%20and%20Xun%20Zhou%20and%20Jun%20Zhang%20and%20Jingjing%20Xu%20and%20Di%20He%0AAbstract%3A%20%20%20Chain-of-Thought%20%28CoT%29%20prompting%20has%20emerged%20as%20a%20powerful%20technique%20for%0Aenhancing%20language%20model%27s%20reasoning%20capabilities.%20However%2C%20generating%20long%20and%0Acorrect%20CoT%20trajectories%20is%20challenging.%20Recent%20studies%20have%20demonstrated%20that%0ALooped%20Transformers%20possess%20remarkable%20length%20generalization%20capabilities%2C%20but%0Atheir%20limited%20generality%20and%20adaptability%20prevent%20them%20from%20serving%20as%20an%0Aalternative%20to%20auto-regressive%20solutions.%20To%20better%20leverage%20the%20strengths%20of%0ALooped%20Transformers%2C%20we%20propose%20RELAY%20%28REasoning%20through%20Loop%20Alignment%0AiterativelY%29.%20Specifically%2C%20we%20align%20the%20steps%20of%20Chain-of-Thought%20%28CoT%29%0Areasoning%20with%20loop%20iterations%20and%20apply%20intermediate%20supervision%20during%20the%0Atraining%20of%20Looped%20Transformers.%20This%20additional%20iteration-wise%20supervision%20not%0Aonly%20preserves%20the%20Looped%20Transformer%27s%20ability%20for%20length%20generalization%20but%0Aalso%20enables%20it%20to%20predict%20CoT%20reasoning%20steps%20for%20unseen%20data.%20Therefore%2C%20we%0Aleverage%20this%20Looped%20Transformer%20to%20generate%20accurate%20reasoning%20chains%20for%0Acomplex%20problems%20that%20exceed%20the%20training%20length%2C%20which%20will%20then%20be%20used%20to%0Afine-tune%20an%20auto-regressive%20model.%20We%20conduct%20extensive%20experiments%2C%20and%20the%0Aresults%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%20with%20significant%0Aimprovements%20in%20the%20performance%20of%20the%20auto-regressive%20model.%20Code%20will%20be%0Areleased%20at%20https%3A//github.com/qifanyu/RELAY.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08482v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Auto-regressive%2520Chain-of-Thought%2520through%2520Loop-Aligned%250A%2520%2520Reasoning%26entry.906535625%3DQifan%2520Yu%2520and%2520Zhenyu%2520He%2520and%2520Sijie%2520Li%2520and%2520Xun%2520Zhou%2520and%2520Jun%2520Zhang%2520and%2520Jingjing%2520Xu%2520and%2520Di%2520He%26entry.1292438233%3D%2520%2520Chain-of-Thought%2520%2528CoT%2529%2520prompting%2520has%2520emerged%2520as%2520a%2520powerful%2520technique%2520for%250Aenhancing%2520language%2520model%2527s%2520reasoning%2520capabilities.%2520However%252C%2520generating%2520long%2520and%250Acorrect%2520CoT%2520trajectories%2520is%2520challenging.%2520Recent%2520studies%2520have%2520demonstrated%2520that%250ALooped%2520Transformers%2520possess%2520remarkable%2520length%2520generalization%2520capabilities%252C%2520but%250Atheir%2520limited%2520generality%2520and%2520adaptability%2520prevent%2520them%2520from%2520serving%2520as%2520an%250Aalternative%2520to%2520auto-regressive%2520solutions.%2520To%2520better%2520leverage%2520the%2520strengths%2520of%250ALooped%2520Transformers%252C%2520we%2520propose%2520RELAY%2520%2528REasoning%2520through%2520Loop%2520Alignment%250AiterativelY%2529.%2520Specifically%252C%2520we%2520align%2520the%2520steps%2520of%2520Chain-of-Thought%2520%2528CoT%2529%250Areasoning%2520with%2520loop%2520iterations%2520and%2520apply%2520intermediate%2520supervision%2520during%2520the%250Atraining%2520of%2520Looped%2520Transformers.%2520This%2520additional%2520iteration-wise%2520supervision%2520not%250Aonly%2520preserves%2520the%2520Looped%2520Transformer%2527s%2520ability%2520for%2520length%2520generalization%2520but%250Aalso%2520enables%2520it%2520to%2520predict%2520CoT%2520reasoning%2520steps%2520for%2520unseen%2520data.%2520Therefore%252C%2520we%250Aleverage%2520this%2520Looped%2520Transformer%2520to%2520generate%2520accurate%2520reasoning%2520chains%2520for%250Acomplex%2520problems%2520that%2520exceed%2520the%2520training%2520length%252C%2520which%2520will%2520then%2520be%2520used%2520to%250Afine-tune%2520an%2520auto-regressive%2520model.%2520We%2520conduct%2520extensive%2520experiments%252C%2520and%2520the%250Aresults%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%252C%2520with%2520significant%250Aimprovements%2520in%2520the%2520performance%2520of%2520the%2520auto-regressive%2520model.%2520Code%2520will%2520be%250Areleased%2520at%2520https%253A//github.com/qifanyu/RELAY.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08482v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Auto-regressive%20Chain-of-Thought%20through%20Loop-Aligned%0A%20%20Reasoning&entry.906535625=Qifan%20Yu%20and%20Zhenyu%20He%20and%20Sijie%20Li%20and%20Xun%20Zhou%20and%20Jun%20Zhang%20and%20Jingjing%20Xu%20and%20Di%20He&entry.1292438233=%20%20Chain-of-Thought%20%28CoT%29%20prompting%20has%20emerged%20as%20a%20powerful%20technique%20for%0Aenhancing%20language%20model%27s%20reasoning%20capabilities.%20However%2C%20generating%20long%20and%0Acorrect%20CoT%20trajectories%20is%20challenging.%20Recent%20studies%20have%20demonstrated%20that%0ALooped%20Transformers%20possess%20remarkable%20length%20generalization%20capabilities%2C%20but%0Atheir%20limited%20generality%20and%20adaptability%20prevent%20them%20from%20serving%20as%20an%0Aalternative%20to%20auto-regressive%20solutions.%20To%20better%20leverage%20the%20strengths%20of%0ALooped%20Transformers%2C%20we%20propose%20RELAY%20%28REasoning%20through%20Loop%20Alignment%0AiterativelY%29.%20Specifically%2C%20we%20align%20the%20steps%20of%20Chain-of-Thought%20%28CoT%29%0Areasoning%20with%20loop%20iterations%20and%20apply%20intermediate%20supervision%20during%20the%0Atraining%20of%20Looped%20Transformers.%20This%20additional%20iteration-wise%20supervision%20not%0Aonly%20preserves%20the%20Looped%20Transformer%27s%20ability%20for%20length%20generalization%20but%0Aalso%20enables%20it%20to%20predict%20CoT%20reasoning%20steps%20for%20unseen%20data.%20Therefore%2C%20we%0Aleverage%20this%20Looped%20Transformer%20to%20generate%20accurate%20reasoning%20chains%20for%0Acomplex%20problems%20that%20exceed%20the%20training%20length%2C%20which%20will%20then%20be%20used%20to%0Afine-tune%20an%20auto-regressive%20model.%20We%20conduct%20extensive%20experiments%2C%20and%20the%0Aresults%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%20with%20significant%0Aimprovements%20in%20the%20performance%20of%20the%20auto-regressive%20model.%20Code%20will%20be%0Areleased%20at%20https%3A//github.com/qifanyu/RELAY.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08482v1&entry.124074799=Read"},
{"title": "CineMaster: A 3D-Aware and Controllable Framework for Cinematic\n  Text-to-Video Generation", "author": "Qinghe Wang and Yawen Luo and Xiaoyu Shi and Xu Jia and Huchuan Lu and Tianfan Xue and Xintao Wang and Pengfei Wan and Di Zhang and Kun Gai", "abstract": "  In this work, we present CineMaster, a novel framework for 3D-aware and\ncontrollable text-to-video generation. Our goal is to empower users with\ncomparable controllability as professional film directors: precise placement of\nobjects within the scene, flexible manipulation of both objects and camera in\n3D space, and intuitive layout control over the rendered frames. To achieve\nthis, CineMaster operates in two stages. In the first stage, we design an\ninteractive workflow that allows users to intuitively construct 3D-aware\nconditional signals by positioning object bounding boxes and defining camera\nmovements within the 3D space. In the second stage, these control\nsignals--comprising rendered depth maps, camera trajectories and object class\nlabels--serve as the guidance for a text-to-video diffusion model, ensuring to\ngenerate the user-intended video content. Furthermore, to overcome the scarcity\nof in-the-wild datasets with 3D object motion and camera pose annotations, we\ncarefully establish an automated data annotation pipeline that extracts 3D\nbounding boxes and camera trajectories from large-scale video data. Extensive\nqualitative and quantitative experiments demonstrate that CineMaster\nsignificantly outperforms existing methods and implements prominent 3D-aware\ntext-to-video generation. Project page: https://cinemaster-dev.github.io/.\n", "link": "http://arxiv.org/abs/2502.08639v1", "date": "2025-02-12", "relevancy": 1.9483, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6717}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6462}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CineMaster%3A%20A%203D-Aware%20and%20Controllable%20Framework%20for%20Cinematic%0A%20%20Text-to-Video%20Generation&body=Title%3A%20CineMaster%3A%20A%203D-Aware%20and%20Controllable%20Framework%20for%20Cinematic%0A%20%20Text-to-Video%20Generation%0AAuthor%3A%20Qinghe%20Wang%20and%20Yawen%20Luo%20and%20Xiaoyu%20Shi%20and%20Xu%20Jia%20and%20Huchuan%20Lu%20and%20Tianfan%20Xue%20and%20Xintao%20Wang%20and%20Pengfei%20Wan%20and%20Di%20Zhang%20and%20Kun%20Gai%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20CineMaster%2C%20a%20novel%20framework%20for%203D-aware%20and%0Acontrollable%20text-to-video%20generation.%20Our%20goal%20is%20to%20empower%20users%20with%0Acomparable%20controllability%20as%20professional%20film%20directors%3A%20precise%20placement%20of%0Aobjects%20within%20the%20scene%2C%20flexible%20manipulation%20of%20both%20objects%20and%20camera%20in%0A3D%20space%2C%20and%20intuitive%20layout%20control%20over%20the%20rendered%20frames.%20To%20achieve%0Athis%2C%20CineMaster%20operates%20in%20two%20stages.%20In%20the%20first%20stage%2C%20we%20design%20an%0Ainteractive%20workflow%20that%20allows%20users%20to%20intuitively%20construct%203D-aware%0Aconditional%20signals%20by%20positioning%20object%20bounding%20boxes%20and%20defining%20camera%0Amovements%20within%20the%203D%20space.%20In%20the%20second%20stage%2C%20these%20control%0Asignals--comprising%20rendered%20depth%20maps%2C%20camera%20trajectories%20and%20object%20class%0Alabels--serve%20as%20the%20guidance%20for%20a%20text-to-video%20diffusion%20model%2C%20ensuring%20to%0Agenerate%20the%20user-intended%20video%20content.%20Furthermore%2C%20to%20overcome%20the%20scarcity%0Aof%20in-the-wild%20datasets%20with%203D%20object%20motion%20and%20camera%20pose%20annotations%2C%20we%0Acarefully%20establish%20an%20automated%20data%20annotation%20pipeline%20that%20extracts%203D%0Abounding%20boxes%20and%20camera%20trajectories%20from%20large-scale%20video%20data.%20Extensive%0Aqualitative%20and%20quantitative%20experiments%20demonstrate%20that%20CineMaster%0Asignificantly%20outperforms%20existing%20methods%20and%20implements%20prominent%203D-aware%0Atext-to-video%20generation.%20Project%20page%3A%20https%3A//cinemaster-dev.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08639v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCineMaster%253A%2520A%25203D-Aware%2520and%2520Controllable%2520Framework%2520for%2520Cinematic%250A%2520%2520Text-to-Video%2520Generation%26entry.906535625%3DQinghe%2520Wang%2520and%2520Yawen%2520Luo%2520and%2520Xiaoyu%2520Shi%2520and%2520Xu%2520Jia%2520and%2520Huchuan%2520Lu%2520and%2520Tianfan%2520Xue%2520and%2520Xintao%2520Wang%2520and%2520Pengfei%2520Wan%2520and%2520Di%2520Zhang%2520and%2520Kun%2520Gai%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520CineMaster%252C%2520a%2520novel%2520framework%2520for%25203D-aware%2520and%250Acontrollable%2520text-to-video%2520generation.%2520Our%2520goal%2520is%2520to%2520empower%2520users%2520with%250Acomparable%2520controllability%2520as%2520professional%2520film%2520directors%253A%2520precise%2520placement%2520of%250Aobjects%2520within%2520the%2520scene%252C%2520flexible%2520manipulation%2520of%2520both%2520objects%2520and%2520camera%2520in%250A3D%2520space%252C%2520and%2520intuitive%2520layout%2520control%2520over%2520the%2520rendered%2520frames.%2520To%2520achieve%250Athis%252C%2520CineMaster%2520operates%2520in%2520two%2520stages.%2520In%2520the%2520first%2520stage%252C%2520we%2520design%2520an%250Ainteractive%2520workflow%2520that%2520allows%2520users%2520to%2520intuitively%2520construct%25203D-aware%250Aconditional%2520signals%2520by%2520positioning%2520object%2520bounding%2520boxes%2520and%2520defining%2520camera%250Amovements%2520within%2520the%25203D%2520space.%2520In%2520the%2520second%2520stage%252C%2520these%2520control%250Asignals--comprising%2520rendered%2520depth%2520maps%252C%2520camera%2520trajectories%2520and%2520object%2520class%250Alabels--serve%2520as%2520the%2520guidance%2520for%2520a%2520text-to-video%2520diffusion%2520model%252C%2520ensuring%2520to%250Agenerate%2520the%2520user-intended%2520video%2520content.%2520Furthermore%252C%2520to%2520overcome%2520the%2520scarcity%250Aof%2520in-the-wild%2520datasets%2520with%25203D%2520object%2520motion%2520and%2520camera%2520pose%2520annotations%252C%2520we%250Acarefully%2520establish%2520an%2520automated%2520data%2520annotation%2520pipeline%2520that%2520extracts%25203D%250Abounding%2520boxes%2520and%2520camera%2520trajectories%2520from%2520large-scale%2520video%2520data.%2520Extensive%250Aqualitative%2520and%2520quantitative%2520experiments%2520demonstrate%2520that%2520CineMaster%250Asignificantly%2520outperforms%2520existing%2520methods%2520and%2520implements%2520prominent%25203D-aware%250Atext-to-video%2520generation.%2520Project%2520page%253A%2520https%253A//cinemaster-dev.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08639v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CineMaster%3A%20A%203D-Aware%20and%20Controllable%20Framework%20for%20Cinematic%0A%20%20Text-to-Video%20Generation&entry.906535625=Qinghe%20Wang%20and%20Yawen%20Luo%20and%20Xiaoyu%20Shi%20and%20Xu%20Jia%20and%20Huchuan%20Lu%20and%20Tianfan%20Xue%20and%20Xintao%20Wang%20and%20Pengfei%20Wan%20and%20Di%20Zhang%20and%20Kun%20Gai&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20CineMaster%2C%20a%20novel%20framework%20for%203D-aware%20and%0Acontrollable%20text-to-video%20generation.%20Our%20goal%20is%20to%20empower%20users%20with%0Acomparable%20controllability%20as%20professional%20film%20directors%3A%20precise%20placement%20of%0Aobjects%20within%20the%20scene%2C%20flexible%20manipulation%20of%20both%20objects%20and%20camera%20in%0A3D%20space%2C%20and%20intuitive%20layout%20control%20over%20the%20rendered%20frames.%20To%20achieve%0Athis%2C%20CineMaster%20operates%20in%20two%20stages.%20In%20the%20first%20stage%2C%20we%20design%20an%0Ainteractive%20workflow%20that%20allows%20users%20to%20intuitively%20construct%203D-aware%0Aconditional%20signals%20by%20positioning%20object%20bounding%20boxes%20and%20defining%20camera%0Amovements%20within%20the%203D%20space.%20In%20the%20second%20stage%2C%20these%20control%0Asignals--comprising%20rendered%20depth%20maps%2C%20camera%20trajectories%20and%20object%20class%0Alabels--serve%20as%20the%20guidance%20for%20a%20text-to-video%20diffusion%20model%2C%20ensuring%20to%0Agenerate%20the%20user-intended%20video%20content.%20Furthermore%2C%20to%20overcome%20the%20scarcity%0Aof%20in-the-wild%20datasets%20with%203D%20object%20motion%20and%20camera%20pose%20annotations%2C%20we%0Acarefully%20establish%20an%20automated%20data%20annotation%20pipeline%20that%20extracts%203D%0Abounding%20boxes%20and%20camera%20trajectories%20from%20large-scale%20video%20data.%20Extensive%0Aqualitative%20and%20quantitative%20experiments%20demonstrate%20that%20CineMaster%0Asignificantly%20outperforms%20existing%20methods%20and%20implements%20prominent%203D-aware%0Atext-to-video%20generation.%20Project%20page%3A%20https%3A//cinemaster-dev.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08639v1&entry.124074799=Read"},
{"title": "Benign Overfitting in Single-Head Attention", "author": "Roey Magen and Shuning Shang and Zhiwei Xu and Spencer Frei and Wei Hu and Gal Vardi", "abstract": "  The phenomenon of benign overfitting, where a trained neural network\nperfectly fits noisy training data but still achieves near-optimal test\nperformance, has been extensively studied in recent years for linear models and\nfully-connected/convolutional networks. In this work, we study benign\noverfitting in a single-head softmax attention model, which is the fundamental\nbuilding block of Transformers. We prove that under appropriate conditions, the\nmodel exhibits benign overfitting in a classification setting already after two\nsteps of gradient descent. Moreover, we show conditions where a\nminimum-norm/maximum-margin interpolator exhibits benign overfitting. We study\nhow the overfitting behavior depends on the signal-to-noise ratio (SNR) of the\ndata distribution, namely, the ratio between norms of signal and noise tokens,\nand prove that a sufficiently large SNR is both necessary and sufficient for\nbenign overfitting.\n", "link": "http://arxiv.org/abs/2410.07746v2", "date": "2025-02-12", "relevancy": 1.9454, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5011}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4782}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benign%20Overfitting%20in%20Single-Head%20Attention&body=Title%3A%20Benign%20Overfitting%20in%20Single-Head%20Attention%0AAuthor%3A%20Roey%20Magen%20and%20Shuning%20Shang%20and%20Zhiwei%20Xu%20and%20Spencer%20Frei%20and%20Wei%20Hu%20and%20Gal%20Vardi%0AAbstract%3A%20%20%20The%20phenomenon%20of%20benign%20overfitting%2C%20where%20a%20trained%20neural%20network%0Aperfectly%20fits%20noisy%20training%20data%20but%20still%20achieves%20near-optimal%20test%0Aperformance%2C%20has%20been%20extensively%20studied%20in%20recent%20years%20for%20linear%20models%20and%0Afully-connected/convolutional%20networks.%20In%20this%20work%2C%20we%20study%20benign%0Aoverfitting%20in%20a%20single-head%20softmax%20attention%20model%2C%20which%20is%20the%20fundamental%0Abuilding%20block%20of%20Transformers.%20We%20prove%20that%20under%20appropriate%20conditions%2C%20the%0Amodel%20exhibits%20benign%20overfitting%20in%20a%20classification%20setting%20already%20after%20two%0Asteps%20of%20gradient%20descent.%20Moreover%2C%20we%20show%20conditions%20where%20a%0Aminimum-norm/maximum-margin%20interpolator%20exhibits%20benign%20overfitting.%20We%20study%0Ahow%20the%20overfitting%20behavior%20depends%20on%20the%20signal-to-noise%20ratio%20%28SNR%29%20of%20the%0Adata%20distribution%2C%20namely%2C%20the%20ratio%20between%20norms%20of%20signal%20and%20noise%20tokens%2C%0Aand%20prove%20that%20a%20sufficiently%20large%20SNR%20is%20both%20necessary%20and%20sufficient%20for%0Abenign%20overfitting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07746v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenign%2520Overfitting%2520in%2520Single-Head%2520Attention%26entry.906535625%3DRoey%2520Magen%2520and%2520Shuning%2520Shang%2520and%2520Zhiwei%2520Xu%2520and%2520Spencer%2520Frei%2520and%2520Wei%2520Hu%2520and%2520Gal%2520Vardi%26entry.1292438233%3D%2520%2520The%2520phenomenon%2520of%2520benign%2520overfitting%252C%2520where%2520a%2520trained%2520neural%2520network%250Aperfectly%2520fits%2520noisy%2520training%2520data%2520but%2520still%2520achieves%2520near-optimal%2520test%250Aperformance%252C%2520has%2520been%2520extensively%2520studied%2520in%2520recent%2520years%2520for%2520linear%2520models%2520and%250Afully-connected/convolutional%2520networks.%2520In%2520this%2520work%252C%2520we%2520study%2520benign%250Aoverfitting%2520in%2520a%2520single-head%2520softmax%2520attention%2520model%252C%2520which%2520is%2520the%2520fundamental%250Abuilding%2520block%2520of%2520Transformers.%2520We%2520prove%2520that%2520under%2520appropriate%2520conditions%252C%2520the%250Amodel%2520exhibits%2520benign%2520overfitting%2520in%2520a%2520classification%2520setting%2520already%2520after%2520two%250Asteps%2520of%2520gradient%2520descent.%2520Moreover%252C%2520we%2520show%2520conditions%2520where%2520a%250Aminimum-norm/maximum-margin%2520interpolator%2520exhibits%2520benign%2520overfitting.%2520We%2520study%250Ahow%2520the%2520overfitting%2520behavior%2520depends%2520on%2520the%2520signal-to-noise%2520ratio%2520%2528SNR%2529%2520of%2520the%250Adata%2520distribution%252C%2520namely%252C%2520the%2520ratio%2520between%2520norms%2520of%2520signal%2520and%2520noise%2520tokens%252C%250Aand%2520prove%2520that%2520a%2520sufficiently%2520large%2520SNR%2520is%2520both%2520necessary%2520and%2520sufficient%2520for%250Abenign%2520overfitting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07746v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benign%20Overfitting%20in%20Single-Head%20Attention&entry.906535625=Roey%20Magen%20and%20Shuning%20Shang%20and%20Zhiwei%20Xu%20and%20Spencer%20Frei%20and%20Wei%20Hu%20and%20Gal%20Vardi&entry.1292438233=%20%20The%20phenomenon%20of%20benign%20overfitting%2C%20where%20a%20trained%20neural%20network%0Aperfectly%20fits%20noisy%20training%20data%20but%20still%20achieves%20near-optimal%20test%0Aperformance%2C%20has%20been%20extensively%20studied%20in%20recent%20years%20for%20linear%20models%20and%0Afully-connected/convolutional%20networks.%20In%20this%20work%2C%20we%20study%20benign%0Aoverfitting%20in%20a%20single-head%20softmax%20attention%20model%2C%20which%20is%20the%20fundamental%0Abuilding%20block%20of%20Transformers.%20We%20prove%20that%20under%20appropriate%20conditions%2C%20the%0Amodel%20exhibits%20benign%20overfitting%20in%20a%20classification%20setting%20already%20after%20two%0Asteps%20of%20gradient%20descent.%20Moreover%2C%20we%20show%20conditions%20where%20a%0Aminimum-norm/maximum-margin%20interpolator%20exhibits%20benign%20overfitting.%20We%20study%0Ahow%20the%20overfitting%20behavior%20depends%20on%20the%20signal-to-noise%20ratio%20%28SNR%29%20of%20the%0Adata%20distribution%2C%20namely%2C%20the%20ratio%20between%20norms%20of%20signal%20and%20noise%20tokens%2C%0Aand%20prove%20that%20a%20sufficiently%20large%20SNR%20is%20both%20necessary%20and%20sufficient%20for%0Abenign%20overfitting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07746v2&entry.124074799=Read"},
{"title": "Scalable Bilevel Loss Balancing for Multi-Task Learning", "author": "Peiyao Xiao and Chaosheng Dong and Shaofeng Zou and Kaiyi Ji", "abstract": "  Multi-task learning (MTL) has been widely adopted for its ability to\nsimultaneously learn multiple tasks. While existing gradient manipulation\nmethods often yield more balanced solutions than simple scalarization-based\napproaches, they typically incur a significant computational overhead of\n$\\mathcal{O}(K)$ in both time and memory, where $K$ is the number of tasks. In\nthis paper, we propose BiLB4MTL, a simple and scalable loss balancing approach\nfor MTL, formulated from a novel bilevel optimization perspective. Our method\nincorporates three key components: (i) an initial loss normalization, (ii) a\nbilevel loss-balancing formulation, and (iii) a scalable first-order algorithm\nthat requires only $\\mathcal{O}(1)$ time and memory. Theoretically, we prove\nthat BiLB4MTL guarantees convergence not only to a stationary point of the\nbilevel loss balancing problem but also to an $\\epsilon$-accurate Pareto\nstationary point for all $K$ loss functions under mild conditions. Extensive\nexperiments on diverse multi-task datasets demonstrate that BiLB4MTL achieves\nstate-of-the-art performance in both accuracy and efficiency. Code is available\nat https://github.com/OptMN-Lab/-BiLB4MTL.\n", "link": "http://arxiv.org/abs/2502.08585v1", "date": "2025-02-12", "relevancy": 1.937, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4957}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4774}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4726}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Bilevel%20Loss%20Balancing%20for%20Multi-Task%20Learning&body=Title%3A%20Scalable%20Bilevel%20Loss%20Balancing%20for%20Multi-Task%20Learning%0AAuthor%3A%20Peiyao%20Xiao%20and%20Chaosheng%20Dong%20and%20Shaofeng%20Zou%20and%20Kaiyi%20Ji%0AAbstract%3A%20%20%20Multi-task%20learning%20%28MTL%29%20has%20been%20widely%20adopted%20for%20its%20ability%20to%0Asimultaneously%20learn%20multiple%20tasks.%20While%20existing%20gradient%20manipulation%0Amethods%20often%20yield%20more%20balanced%20solutions%20than%20simple%20scalarization-based%0Aapproaches%2C%20they%20typically%20incur%20a%20significant%20computational%20overhead%20of%0A%24%5Cmathcal%7BO%7D%28K%29%24%20in%20both%20time%20and%20memory%2C%20where%20%24K%24%20is%20the%20number%20of%20tasks.%20In%0Athis%20paper%2C%20we%20propose%20BiLB4MTL%2C%20a%20simple%20and%20scalable%20loss%20balancing%20approach%0Afor%20MTL%2C%20formulated%20from%20a%20novel%20bilevel%20optimization%20perspective.%20Our%20method%0Aincorporates%20three%20key%20components%3A%20%28i%29%20an%20initial%20loss%20normalization%2C%20%28ii%29%20a%0Abilevel%20loss-balancing%20formulation%2C%20and%20%28iii%29%20a%20scalable%20first-order%20algorithm%0Athat%20requires%20only%20%24%5Cmathcal%7BO%7D%281%29%24%20time%20and%20memory.%20Theoretically%2C%20we%20prove%0Athat%20BiLB4MTL%20guarantees%20convergence%20not%20only%20to%20a%20stationary%20point%20of%20the%0Abilevel%20loss%20balancing%20problem%20but%20also%20to%20an%20%24%5Cepsilon%24-accurate%20Pareto%0Astationary%20point%20for%20all%20%24K%24%20loss%20functions%20under%20mild%20conditions.%20Extensive%0Aexperiments%20on%20diverse%20multi-task%20datasets%20demonstrate%20that%20BiLB4MTL%20achieves%0Astate-of-the-art%20performance%20in%20both%20accuracy%20and%20efficiency.%20Code%20is%20available%0Aat%20https%3A//github.com/OptMN-Lab/-BiLB4MTL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08585v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Bilevel%2520Loss%2520Balancing%2520for%2520Multi-Task%2520Learning%26entry.906535625%3DPeiyao%2520Xiao%2520and%2520Chaosheng%2520Dong%2520and%2520Shaofeng%2520Zou%2520and%2520Kaiyi%2520Ji%26entry.1292438233%3D%2520%2520Multi-task%2520learning%2520%2528MTL%2529%2520has%2520been%2520widely%2520adopted%2520for%2520its%2520ability%2520to%250Asimultaneously%2520learn%2520multiple%2520tasks.%2520While%2520existing%2520gradient%2520manipulation%250Amethods%2520often%2520yield%2520more%2520balanced%2520solutions%2520than%2520simple%2520scalarization-based%250Aapproaches%252C%2520they%2520typically%2520incur%2520a%2520significant%2520computational%2520overhead%2520of%250A%2524%255Cmathcal%257BO%257D%2528K%2529%2524%2520in%2520both%2520time%2520and%2520memory%252C%2520where%2520%2524K%2524%2520is%2520the%2520number%2520of%2520tasks.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520BiLB4MTL%252C%2520a%2520simple%2520and%2520scalable%2520loss%2520balancing%2520approach%250Afor%2520MTL%252C%2520formulated%2520from%2520a%2520novel%2520bilevel%2520optimization%2520perspective.%2520Our%2520method%250Aincorporates%2520three%2520key%2520components%253A%2520%2528i%2529%2520an%2520initial%2520loss%2520normalization%252C%2520%2528ii%2529%2520a%250Abilevel%2520loss-balancing%2520formulation%252C%2520and%2520%2528iii%2529%2520a%2520scalable%2520first-order%2520algorithm%250Athat%2520requires%2520only%2520%2524%255Cmathcal%257BO%257D%25281%2529%2524%2520time%2520and%2520memory.%2520Theoretically%252C%2520we%2520prove%250Athat%2520BiLB4MTL%2520guarantees%2520convergence%2520not%2520only%2520to%2520a%2520stationary%2520point%2520of%2520the%250Abilevel%2520loss%2520balancing%2520problem%2520but%2520also%2520to%2520an%2520%2524%255Cepsilon%2524-accurate%2520Pareto%250Astationary%2520point%2520for%2520all%2520%2524K%2524%2520loss%2520functions%2520under%2520mild%2520conditions.%2520Extensive%250Aexperiments%2520on%2520diverse%2520multi-task%2520datasets%2520demonstrate%2520that%2520BiLB4MTL%2520achieves%250Astate-of-the-art%2520performance%2520in%2520both%2520accuracy%2520and%2520efficiency.%2520Code%2520is%2520available%250Aat%2520https%253A//github.com/OptMN-Lab/-BiLB4MTL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08585v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Bilevel%20Loss%20Balancing%20for%20Multi-Task%20Learning&entry.906535625=Peiyao%20Xiao%20and%20Chaosheng%20Dong%20and%20Shaofeng%20Zou%20and%20Kaiyi%20Ji&entry.1292438233=%20%20Multi-task%20learning%20%28MTL%29%20has%20been%20widely%20adopted%20for%20its%20ability%20to%0Asimultaneously%20learn%20multiple%20tasks.%20While%20existing%20gradient%20manipulation%0Amethods%20often%20yield%20more%20balanced%20solutions%20than%20simple%20scalarization-based%0Aapproaches%2C%20they%20typically%20incur%20a%20significant%20computational%20overhead%20of%0A%24%5Cmathcal%7BO%7D%28K%29%24%20in%20both%20time%20and%20memory%2C%20where%20%24K%24%20is%20the%20number%20of%20tasks.%20In%0Athis%20paper%2C%20we%20propose%20BiLB4MTL%2C%20a%20simple%20and%20scalable%20loss%20balancing%20approach%0Afor%20MTL%2C%20formulated%20from%20a%20novel%20bilevel%20optimization%20perspective.%20Our%20method%0Aincorporates%20three%20key%20components%3A%20%28i%29%20an%20initial%20loss%20normalization%2C%20%28ii%29%20a%0Abilevel%20loss-balancing%20formulation%2C%20and%20%28iii%29%20a%20scalable%20first-order%20algorithm%0Athat%20requires%20only%20%24%5Cmathcal%7BO%7D%281%29%24%20time%20and%20memory.%20Theoretically%2C%20we%20prove%0Athat%20BiLB4MTL%20guarantees%20convergence%20not%20only%20to%20a%20stationary%20point%20of%20the%0Abilevel%20loss%20balancing%20problem%20but%20also%20to%20an%20%24%5Cepsilon%24-accurate%20Pareto%0Astationary%20point%20for%20all%20%24K%24%20loss%20functions%20under%20mild%20conditions.%20Extensive%0Aexperiments%20on%20diverse%20multi-task%20datasets%20demonstrate%20that%20BiLB4MTL%20achieves%0Astate-of-the-art%20performance%20in%20both%20accuracy%20and%20efficiency.%20Code%20is%20available%0Aat%20https%3A//github.com/OptMN-Lab/-BiLB4MTL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08585v1&entry.124074799=Read"},
{"title": "chebgreen: Learning and Interpolating Continuous Empirical Green's\n  Functions from Data", "author": "Harshwardhan Praveen and Jacob Brown and Christopher Earls", "abstract": "  In this work, we present a mesh-independent, data-driven library, chebgreen,\nto mathematically model one-dimensional systems, possessing an associated\ncontrol parameter, and whose governing partial differential equation is\nunknown. The proposed method learns an Empirical Green's Function for the\nassociated, but hidden, boundary value problem, in the form of a Rational\nNeural Network from which we subsequently construct a bivariate representation\nin a Chebyshev basis. We uncover the Green's function, at an unseen control\nparameter value, by interpolating the left and right singular functions within\na suitable library, expressed as points on a manifold of Quasimatrices, while\nthe associated singular values are interpolated with Lagrange polynomials.\n", "link": "http://arxiv.org/abs/2501.18715v2", "date": "2025-02-12", "relevancy": 1.9272, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4997}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4737}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20chebgreen%3A%20Learning%20and%20Interpolating%20Continuous%20Empirical%20Green%27s%0A%20%20Functions%20from%20Data&body=Title%3A%20chebgreen%3A%20Learning%20and%20Interpolating%20Continuous%20Empirical%20Green%27s%0A%20%20Functions%20from%20Data%0AAuthor%3A%20Harshwardhan%20Praveen%20and%20Jacob%20Brown%20and%20Christopher%20Earls%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20a%20mesh-independent%2C%20data-driven%20library%2C%20chebgreen%2C%0Ato%20mathematically%20model%20one-dimensional%20systems%2C%20possessing%20an%20associated%0Acontrol%20parameter%2C%20and%20whose%20governing%20partial%20differential%20equation%20is%0Aunknown.%20The%20proposed%20method%20learns%20an%20Empirical%20Green%27s%20Function%20for%20the%0Aassociated%2C%20but%20hidden%2C%20boundary%20value%20problem%2C%20in%20the%20form%20of%20a%20Rational%0ANeural%20Network%20from%20which%20we%20subsequently%20construct%20a%20bivariate%20representation%0Ain%20a%20Chebyshev%20basis.%20We%20uncover%20the%20Green%27s%20function%2C%20at%20an%20unseen%20control%0Aparameter%20value%2C%20by%20interpolating%20the%20left%20and%20right%20singular%20functions%20within%0Aa%20suitable%20library%2C%20expressed%20as%20points%20on%20a%20manifold%20of%20Quasimatrices%2C%20while%0Athe%20associated%20singular%20values%20are%20interpolated%20with%20Lagrange%20polynomials.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.18715v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dchebgreen%253A%2520Learning%2520and%2520Interpolating%2520Continuous%2520Empirical%2520Green%2527s%250A%2520%2520Functions%2520from%2520Data%26entry.906535625%3DHarshwardhan%2520Praveen%2520and%2520Jacob%2520Brown%2520and%2520Christopher%2520Earls%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520mesh-independent%252C%2520data-driven%2520library%252C%2520chebgreen%252C%250Ato%2520mathematically%2520model%2520one-dimensional%2520systems%252C%2520possessing%2520an%2520associated%250Acontrol%2520parameter%252C%2520and%2520whose%2520governing%2520partial%2520differential%2520equation%2520is%250Aunknown.%2520The%2520proposed%2520method%2520learns%2520an%2520Empirical%2520Green%2527s%2520Function%2520for%2520the%250Aassociated%252C%2520but%2520hidden%252C%2520boundary%2520value%2520problem%252C%2520in%2520the%2520form%2520of%2520a%2520Rational%250ANeural%2520Network%2520from%2520which%2520we%2520subsequently%2520construct%2520a%2520bivariate%2520representation%250Ain%2520a%2520Chebyshev%2520basis.%2520We%2520uncover%2520the%2520Green%2527s%2520function%252C%2520at%2520an%2520unseen%2520control%250Aparameter%2520value%252C%2520by%2520interpolating%2520the%2520left%2520and%2520right%2520singular%2520functions%2520within%250Aa%2520suitable%2520library%252C%2520expressed%2520as%2520points%2520on%2520a%2520manifold%2520of%2520Quasimatrices%252C%2520while%250Athe%2520associated%2520singular%2520values%2520are%2520interpolated%2520with%2520Lagrange%2520polynomials.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18715v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=chebgreen%3A%20Learning%20and%20Interpolating%20Continuous%20Empirical%20Green%27s%0A%20%20Functions%20from%20Data&entry.906535625=Harshwardhan%20Praveen%20and%20Jacob%20Brown%20and%20Christopher%20Earls&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20a%20mesh-independent%2C%20data-driven%20library%2C%20chebgreen%2C%0Ato%20mathematically%20model%20one-dimensional%20systems%2C%20possessing%20an%20associated%0Acontrol%20parameter%2C%20and%20whose%20governing%20partial%20differential%20equation%20is%0Aunknown.%20The%20proposed%20method%20learns%20an%20Empirical%20Green%27s%20Function%20for%20the%0Aassociated%2C%20but%20hidden%2C%20boundary%20value%20problem%2C%20in%20the%20form%20of%20a%20Rational%0ANeural%20Network%20from%20which%20we%20subsequently%20construct%20a%20bivariate%20representation%0Ain%20a%20Chebyshev%20basis.%20We%20uncover%20the%20Green%27s%20function%2C%20at%20an%20unseen%20control%0Aparameter%20value%2C%20by%20interpolating%20the%20left%20and%20right%20singular%20functions%20within%0Aa%20suitable%20library%2C%20expressed%20as%20points%20on%20a%20manifold%20of%20Quasimatrices%2C%20while%0Athe%20associated%20singular%20values%20are%20interpolated%20with%20Lagrange%20polynomials.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.18715v2&entry.124074799=Read"},
{"title": "Concentration Inequalities for the Stochastic Optimization of Unbounded\n  Objectives with Application to Denoising Score Matching", "author": "Jeremiah Birrell", "abstract": "  We derive novel concentration inequalities that bound the statistical error\nfor a large class of stochastic optimization problems, focusing on the case of\nunbounded objective functions. Our derivations utilize the following tools: 1)\nA new form of McDiarmid's inequality that is based on sample dependent one\ncomponent difference bounds and which leads to a novel uniform law of large\nnumbers result for unbounded functions. 2) A Rademacher complexity bound for\nfamilies of functions that satisfy an appropriate local Lipschitz property. As\nan application of these results, we derive statistical error bounds for\ndenoising score matching (DSM), an application that inherently requires one to\nconsider unbounded objective functions, even when the data distribution has\nbounded support. In addition, our results establish the benefit of sample reuse\nin algorithms that employ easily sampled auxiliary random variables in addition\nto the training data, e.g., as in DSM, which uses auxiliary Gaussian random\nvariables.\n", "link": "http://arxiv.org/abs/2502.08628v1", "date": "2025-02-12", "relevancy": 1.9259, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4904}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.479}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4736}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Concentration%20Inequalities%20for%20the%20Stochastic%20Optimization%20of%20Unbounded%0A%20%20Objectives%20with%20Application%20to%20Denoising%20Score%20Matching&body=Title%3A%20Concentration%20Inequalities%20for%20the%20Stochastic%20Optimization%20of%20Unbounded%0A%20%20Objectives%20with%20Application%20to%20Denoising%20Score%20Matching%0AAuthor%3A%20Jeremiah%20Birrell%0AAbstract%3A%20%20%20We%20derive%20novel%20concentration%20inequalities%20that%20bound%20the%20statistical%20error%0Afor%20a%20large%20class%20of%20stochastic%20optimization%20problems%2C%20focusing%20on%20the%20case%20of%0Aunbounded%20objective%20functions.%20Our%20derivations%20utilize%20the%20following%20tools%3A%201%29%0AA%20new%20form%20of%20McDiarmid%27s%20inequality%20that%20is%20based%20on%20sample%20dependent%20one%0Acomponent%20difference%20bounds%20and%20which%20leads%20to%20a%20novel%20uniform%20law%20of%20large%0Anumbers%20result%20for%20unbounded%20functions.%202%29%20A%20Rademacher%20complexity%20bound%20for%0Afamilies%20of%20functions%20that%20satisfy%20an%20appropriate%20local%20Lipschitz%20property.%20As%0Aan%20application%20of%20these%20results%2C%20we%20derive%20statistical%20error%20bounds%20for%0Adenoising%20score%20matching%20%28DSM%29%2C%20an%20application%20that%20inherently%20requires%20one%20to%0Aconsider%20unbounded%20objective%20functions%2C%20even%20when%20the%20data%20distribution%20has%0Abounded%20support.%20In%20addition%2C%20our%20results%20establish%20the%20benefit%20of%20sample%20reuse%0Ain%20algorithms%20that%20employ%20easily%20sampled%20auxiliary%20random%20variables%20in%20addition%0Ato%20the%20training%20data%2C%20e.g.%2C%20as%20in%20DSM%2C%20which%20uses%20auxiliary%20Gaussian%20random%0Avariables.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08628v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConcentration%2520Inequalities%2520for%2520the%2520Stochastic%2520Optimization%2520of%2520Unbounded%250A%2520%2520Objectives%2520with%2520Application%2520to%2520Denoising%2520Score%2520Matching%26entry.906535625%3DJeremiah%2520Birrell%26entry.1292438233%3D%2520%2520We%2520derive%2520novel%2520concentration%2520inequalities%2520that%2520bound%2520the%2520statistical%2520error%250Afor%2520a%2520large%2520class%2520of%2520stochastic%2520optimization%2520problems%252C%2520focusing%2520on%2520the%2520case%2520of%250Aunbounded%2520objective%2520functions.%2520Our%2520derivations%2520utilize%2520the%2520following%2520tools%253A%25201%2529%250AA%2520new%2520form%2520of%2520McDiarmid%2527s%2520inequality%2520that%2520is%2520based%2520on%2520sample%2520dependent%2520one%250Acomponent%2520difference%2520bounds%2520and%2520which%2520leads%2520to%2520a%2520novel%2520uniform%2520law%2520of%2520large%250Anumbers%2520result%2520for%2520unbounded%2520functions.%25202%2529%2520A%2520Rademacher%2520complexity%2520bound%2520for%250Afamilies%2520of%2520functions%2520that%2520satisfy%2520an%2520appropriate%2520local%2520Lipschitz%2520property.%2520As%250Aan%2520application%2520of%2520these%2520results%252C%2520we%2520derive%2520statistical%2520error%2520bounds%2520for%250Adenoising%2520score%2520matching%2520%2528DSM%2529%252C%2520an%2520application%2520that%2520inherently%2520requires%2520one%2520to%250Aconsider%2520unbounded%2520objective%2520functions%252C%2520even%2520when%2520the%2520data%2520distribution%2520has%250Abounded%2520support.%2520In%2520addition%252C%2520our%2520results%2520establish%2520the%2520benefit%2520of%2520sample%2520reuse%250Ain%2520algorithms%2520that%2520employ%2520easily%2520sampled%2520auxiliary%2520random%2520variables%2520in%2520addition%250Ato%2520the%2520training%2520data%252C%2520e.g.%252C%2520as%2520in%2520DSM%252C%2520which%2520uses%2520auxiliary%2520Gaussian%2520random%250Avariables.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08628v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Concentration%20Inequalities%20for%20the%20Stochastic%20Optimization%20of%20Unbounded%0A%20%20Objectives%20with%20Application%20to%20Denoising%20Score%20Matching&entry.906535625=Jeremiah%20Birrell&entry.1292438233=%20%20We%20derive%20novel%20concentration%20inequalities%20that%20bound%20the%20statistical%20error%0Afor%20a%20large%20class%20of%20stochastic%20optimization%20problems%2C%20focusing%20on%20the%20case%20of%0Aunbounded%20objective%20functions.%20Our%20derivations%20utilize%20the%20following%20tools%3A%201%29%0AA%20new%20form%20of%20McDiarmid%27s%20inequality%20that%20is%20based%20on%20sample%20dependent%20one%0Acomponent%20difference%20bounds%20and%20which%20leads%20to%20a%20novel%20uniform%20law%20of%20large%0Anumbers%20result%20for%20unbounded%20functions.%202%29%20A%20Rademacher%20complexity%20bound%20for%0Afamilies%20of%20functions%20that%20satisfy%20an%20appropriate%20local%20Lipschitz%20property.%20As%0Aan%20application%20of%20these%20results%2C%20we%20derive%20statistical%20error%20bounds%20for%0Adenoising%20score%20matching%20%28DSM%29%2C%20an%20application%20that%20inherently%20requires%20one%20to%0Aconsider%20unbounded%20objective%20functions%2C%20even%20when%20the%20data%20distribution%20has%0Abounded%20support.%20In%20addition%2C%20our%20results%20establish%20the%20benefit%20of%20sample%20reuse%0Ain%20algorithms%20that%20employ%20easily%20sampled%20auxiliary%20random%20variables%20in%20addition%0Ato%20the%20training%20data%2C%20e.g.%2C%20as%20in%20DSM%2C%20which%20uses%20auxiliary%20Gaussian%20random%0Avariables.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08628v1&entry.124074799=Read"},
{"title": "Convergence of Message Passing Graph Neural Networks with Generic\n  Aggregation On Large Random Graphs", "author": "Matthieu Cordonnier and Nicolas Keriven and Nicolas Tremblay and Samuel Vaiter", "abstract": "  We study the convergence of message passing graph neural networks on random\ngraph models to their continuous counterpart as the number of nodes tends to\ninfinity. Until now, this convergence was only known for architectures with\naggregation functions in the form of normalized means, or, equivalently, of an\napplication of classical operators like the adjacency matrix or the graph\nLaplacian. We extend such results to a large class of aggregation functions,\nthat encompasses all classically used message passing graph neural networks,\nsuch as attention-based message passing, max convolutional message passing,\n(degree-normalized) convolutional message passing, or moment-based aggregation\nmessage passing. Under mild assumptions, we give non-asymptotic bounds with\nhigh probability to quantify this convergence. Our main result is based on the\nMcDiarmid inequality. Interestingly, this result does not apply to the case\nwhere the aggregation is a coordinate-wise maximum. We treat this case\nseparately and obtain a different convergence rate.\n", "link": "http://arxiv.org/abs/2304.11140v4", "date": "2025-02-12", "relevancy": 1.9134, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4876}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4791}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convergence%20of%20Message%20Passing%20Graph%20Neural%20Networks%20with%20Generic%0A%20%20Aggregation%20On%20Large%20Random%20Graphs&body=Title%3A%20Convergence%20of%20Message%20Passing%20Graph%20Neural%20Networks%20with%20Generic%0A%20%20Aggregation%20On%20Large%20Random%20Graphs%0AAuthor%3A%20Matthieu%20Cordonnier%20and%20Nicolas%20Keriven%20and%20Nicolas%20Tremblay%20and%20Samuel%20Vaiter%0AAbstract%3A%20%20%20We%20study%20the%20convergence%20of%20message%20passing%20graph%20neural%20networks%20on%20random%0Agraph%20models%20to%20their%20continuous%20counterpart%20as%20the%20number%20of%20nodes%20tends%20to%0Ainfinity.%20Until%20now%2C%20this%20convergence%20was%20only%20known%20for%20architectures%20with%0Aaggregation%20functions%20in%20the%20form%20of%20normalized%20means%2C%20or%2C%20equivalently%2C%20of%20an%0Aapplication%20of%20classical%20operators%20like%20the%20adjacency%20matrix%20or%20the%20graph%0ALaplacian.%20We%20extend%20such%20results%20to%20a%20large%20class%20of%20aggregation%20functions%2C%0Athat%20encompasses%20all%20classically%20used%20message%20passing%20graph%20neural%20networks%2C%0Asuch%20as%20attention-based%20message%20passing%2C%20max%20convolutional%20message%20passing%2C%0A%28degree-normalized%29%20convolutional%20message%20passing%2C%20or%20moment-based%20aggregation%0Amessage%20passing.%20Under%20mild%20assumptions%2C%20we%20give%20non-asymptotic%20bounds%20with%0Ahigh%20probability%20to%20quantify%20this%20convergence.%20Our%20main%20result%20is%20based%20on%20the%0AMcDiarmid%20inequality.%20Interestingly%2C%20this%20result%20does%20not%20apply%20to%20the%20case%0Awhere%20the%20aggregation%20is%20a%20coordinate-wise%20maximum.%20We%20treat%20this%20case%0Aseparately%20and%20obtain%20a%20different%20convergence%20rate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.11140v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvergence%2520of%2520Message%2520Passing%2520Graph%2520Neural%2520Networks%2520with%2520Generic%250A%2520%2520Aggregation%2520On%2520Large%2520Random%2520Graphs%26entry.906535625%3DMatthieu%2520Cordonnier%2520and%2520Nicolas%2520Keriven%2520and%2520Nicolas%2520Tremblay%2520and%2520Samuel%2520Vaiter%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520convergence%2520of%2520message%2520passing%2520graph%2520neural%2520networks%2520on%2520random%250Agraph%2520models%2520to%2520their%2520continuous%2520counterpart%2520as%2520the%2520number%2520of%2520nodes%2520tends%2520to%250Ainfinity.%2520Until%2520now%252C%2520this%2520convergence%2520was%2520only%2520known%2520for%2520architectures%2520with%250Aaggregation%2520functions%2520in%2520the%2520form%2520of%2520normalized%2520means%252C%2520or%252C%2520equivalently%252C%2520of%2520an%250Aapplication%2520of%2520classical%2520operators%2520like%2520the%2520adjacency%2520matrix%2520or%2520the%2520graph%250ALaplacian.%2520We%2520extend%2520such%2520results%2520to%2520a%2520large%2520class%2520of%2520aggregation%2520functions%252C%250Athat%2520encompasses%2520all%2520classically%2520used%2520message%2520passing%2520graph%2520neural%2520networks%252C%250Asuch%2520as%2520attention-based%2520message%2520passing%252C%2520max%2520convolutional%2520message%2520passing%252C%250A%2528degree-normalized%2529%2520convolutional%2520message%2520passing%252C%2520or%2520moment-based%2520aggregation%250Amessage%2520passing.%2520Under%2520mild%2520assumptions%252C%2520we%2520give%2520non-asymptotic%2520bounds%2520with%250Ahigh%2520probability%2520to%2520quantify%2520this%2520convergence.%2520Our%2520main%2520result%2520is%2520based%2520on%2520the%250AMcDiarmid%2520inequality.%2520Interestingly%252C%2520this%2520result%2520does%2520not%2520apply%2520to%2520the%2520case%250Awhere%2520the%2520aggregation%2520is%2520a%2520coordinate-wise%2520maximum.%2520We%2520treat%2520this%2520case%250Aseparately%2520and%2520obtain%2520a%2520different%2520convergence%2520rate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.11140v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convergence%20of%20Message%20Passing%20Graph%20Neural%20Networks%20with%20Generic%0A%20%20Aggregation%20On%20Large%20Random%20Graphs&entry.906535625=Matthieu%20Cordonnier%20and%20Nicolas%20Keriven%20and%20Nicolas%20Tremblay%20and%20Samuel%20Vaiter&entry.1292438233=%20%20We%20study%20the%20convergence%20of%20message%20passing%20graph%20neural%20networks%20on%20random%0Agraph%20models%20to%20their%20continuous%20counterpart%20as%20the%20number%20of%20nodes%20tends%20to%0Ainfinity.%20Until%20now%2C%20this%20convergence%20was%20only%20known%20for%20architectures%20with%0Aaggregation%20functions%20in%20the%20form%20of%20normalized%20means%2C%20or%2C%20equivalently%2C%20of%20an%0Aapplication%20of%20classical%20operators%20like%20the%20adjacency%20matrix%20or%20the%20graph%0ALaplacian.%20We%20extend%20such%20results%20to%20a%20large%20class%20of%20aggregation%20functions%2C%0Athat%20encompasses%20all%20classically%20used%20message%20passing%20graph%20neural%20networks%2C%0Asuch%20as%20attention-based%20message%20passing%2C%20max%20convolutional%20message%20passing%2C%0A%28degree-normalized%29%20convolutional%20message%20passing%2C%20or%20moment-based%20aggregation%0Amessage%20passing.%20Under%20mild%20assumptions%2C%20we%20give%20non-asymptotic%20bounds%20with%0Ahigh%20probability%20to%20quantify%20this%20convergence.%20Our%20main%20result%20is%20based%20on%20the%0AMcDiarmid%20inequality.%20Interestingly%2C%20this%20result%20does%20not%20apply%20to%20the%20case%0Awhere%20the%20aggregation%20is%20a%20coordinate-wise%20maximum.%20We%20treat%20this%20case%0Aseparately%20and%20obtain%20a%20different%20convergence%20rate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.11140v4&entry.124074799=Read"},
{"title": "Topic-Aware Knowledge Graph with Large Language Models for\n  Interoperability in Recommender Systems", "author": "Minhye Jeon and Seokho Ahn and Young-Duk Seo", "abstract": "  The use of knowledge graphs in recommender systems has become one of the\ncommon approaches to addressing data sparsity and cold start problems. Recent\nadvances in large language models (LLMs) offer new possibilities for processing\nside and context information within knowledge graphs. However, consistent\nintegration across various systems remains challenging due to the need for\ndomain expert intervention and differences in system characteristics. To\naddress these issues, we propose a consistent approach that extracts both\ngeneral and specific topics from both side and context information using LLMs.\nFirst, general topics are iteratively extracted and updated from side\ninformation. Then, specific topics are extracted using context information.\nFinally, to address synonymous topics generated during the specific topic\nextraction process, a refining algorithm processes and resolves these issues\neffectively. This approach allows general topics to capture broad knowledge\nacross diverse item characteristics, while specific topics emphasize detailed\nattributes, providing a more comprehensive understanding of the semantic\nfeatures of items and the preferences of users. Experimental results\ndemonstrate significant improvements in recommendation performance across\ndiverse knowledge graphs.\n", "link": "http://arxiv.org/abs/2412.20163v3", "date": "2025-02-12", "relevancy": 1.8977, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4838}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4726}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4726}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topic-Aware%20Knowledge%20Graph%20with%20Large%20Language%20Models%20for%0A%20%20Interoperability%20in%20Recommender%20Systems&body=Title%3A%20Topic-Aware%20Knowledge%20Graph%20with%20Large%20Language%20Models%20for%0A%20%20Interoperability%20in%20Recommender%20Systems%0AAuthor%3A%20Minhye%20Jeon%20and%20Seokho%20Ahn%20and%20Young-Duk%20Seo%0AAbstract%3A%20%20%20The%20use%20of%20knowledge%20graphs%20in%20recommender%20systems%20has%20become%20one%20of%20the%0Acommon%20approaches%20to%20addressing%20data%20sparsity%20and%20cold%20start%20problems.%20Recent%0Aadvances%20in%20large%20language%20models%20%28LLMs%29%20offer%20new%20possibilities%20for%20processing%0Aside%20and%20context%20information%20within%20knowledge%20graphs.%20However%2C%20consistent%0Aintegration%20across%20various%20systems%20remains%20challenging%20due%20to%20the%20need%20for%0Adomain%20expert%20intervention%20and%20differences%20in%20system%20characteristics.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20a%20consistent%20approach%20that%20extracts%20both%0Ageneral%20and%20specific%20topics%20from%20both%20side%20and%20context%20information%20using%20LLMs.%0AFirst%2C%20general%20topics%20are%20iteratively%20extracted%20and%20updated%20from%20side%0Ainformation.%20Then%2C%20specific%20topics%20are%20extracted%20using%20context%20information.%0AFinally%2C%20to%20address%20synonymous%20topics%20generated%20during%20the%20specific%20topic%0Aextraction%20process%2C%20a%20refining%20algorithm%20processes%20and%20resolves%20these%20issues%0Aeffectively.%20This%20approach%20allows%20general%20topics%20to%20capture%20broad%20knowledge%0Aacross%20diverse%20item%20characteristics%2C%20while%20specific%20topics%20emphasize%20detailed%0Aattributes%2C%20providing%20a%20more%20comprehensive%20understanding%20of%20the%20semantic%0Afeatures%20of%20items%20and%20the%20preferences%20of%20users.%20Experimental%20results%0Ademonstrate%20significant%20improvements%20in%20recommendation%20performance%20across%0Adiverse%20knowledge%20graphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20163v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopic-Aware%2520Knowledge%2520Graph%2520with%2520Large%2520Language%2520Models%2520for%250A%2520%2520Interoperability%2520in%2520Recommender%2520Systems%26entry.906535625%3DMinhye%2520Jeon%2520and%2520Seokho%2520Ahn%2520and%2520Young-Duk%2520Seo%26entry.1292438233%3D%2520%2520The%2520use%2520of%2520knowledge%2520graphs%2520in%2520recommender%2520systems%2520has%2520become%2520one%2520of%2520the%250Acommon%2520approaches%2520to%2520addressing%2520data%2520sparsity%2520and%2520cold%2520start%2520problems.%2520Recent%250Aadvances%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520offer%2520new%2520possibilities%2520for%2520processing%250Aside%2520and%2520context%2520information%2520within%2520knowledge%2520graphs.%2520However%252C%2520consistent%250Aintegration%2520across%2520various%2520systems%2520remains%2520challenging%2520due%2520to%2520the%2520need%2520for%250Adomain%2520expert%2520intervention%2520and%2520differences%2520in%2520system%2520characteristics.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520propose%2520a%2520consistent%2520approach%2520that%2520extracts%2520both%250Ageneral%2520and%2520specific%2520topics%2520from%2520both%2520side%2520and%2520context%2520information%2520using%2520LLMs.%250AFirst%252C%2520general%2520topics%2520are%2520iteratively%2520extracted%2520and%2520updated%2520from%2520side%250Ainformation.%2520Then%252C%2520specific%2520topics%2520are%2520extracted%2520using%2520context%2520information.%250AFinally%252C%2520to%2520address%2520synonymous%2520topics%2520generated%2520during%2520the%2520specific%2520topic%250Aextraction%2520process%252C%2520a%2520refining%2520algorithm%2520processes%2520and%2520resolves%2520these%2520issues%250Aeffectively.%2520This%2520approach%2520allows%2520general%2520topics%2520to%2520capture%2520broad%2520knowledge%250Aacross%2520diverse%2520item%2520characteristics%252C%2520while%2520specific%2520topics%2520emphasize%2520detailed%250Aattributes%252C%2520providing%2520a%2520more%2520comprehensive%2520understanding%2520of%2520the%2520semantic%250Afeatures%2520of%2520items%2520and%2520the%2520preferences%2520of%2520users.%2520Experimental%2520results%250Ademonstrate%2520significant%2520improvements%2520in%2520recommendation%2520performance%2520across%250Adiverse%2520knowledge%2520graphs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20163v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topic-Aware%20Knowledge%20Graph%20with%20Large%20Language%20Models%20for%0A%20%20Interoperability%20in%20Recommender%20Systems&entry.906535625=Minhye%20Jeon%20and%20Seokho%20Ahn%20and%20Young-Duk%20Seo&entry.1292438233=%20%20The%20use%20of%20knowledge%20graphs%20in%20recommender%20systems%20has%20become%20one%20of%20the%0Acommon%20approaches%20to%20addressing%20data%20sparsity%20and%20cold%20start%20problems.%20Recent%0Aadvances%20in%20large%20language%20models%20%28LLMs%29%20offer%20new%20possibilities%20for%20processing%0Aside%20and%20context%20information%20within%20knowledge%20graphs.%20However%2C%20consistent%0Aintegration%20across%20various%20systems%20remains%20challenging%20due%20to%20the%20need%20for%0Adomain%20expert%20intervention%20and%20differences%20in%20system%20characteristics.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20a%20consistent%20approach%20that%20extracts%20both%0Ageneral%20and%20specific%20topics%20from%20both%20side%20and%20context%20information%20using%20LLMs.%0AFirst%2C%20general%20topics%20are%20iteratively%20extracted%20and%20updated%20from%20side%0Ainformation.%20Then%2C%20specific%20topics%20are%20extracted%20using%20context%20information.%0AFinally%2C%20to%20address%20synonymous%20topics%20generated%20during%20the%20specific%20topic%0Aextraction%20process%2C%20a%20refining%20algorithm%20processes%20and%20resolves%20these%20issues%0Aeffectively.%20This%20approach%20allows%20general%20topics%20to%20capture%20broad%20knowledge%0Aacross%20diverse%20item%20characteristics%2C%20while%20specific%20topics%20emphasize%20detailed%0Aattributes%2C%20providing%20a%20more%20comprehensive%20understanding%20of%20the%20semantic%0Afeatures%20of%20items%20and%20the%20preferences%20of%20users.%20Experimental%20results%0Ademonstrate%20significant%20improvements%20in%20recommendation%20performance%20across%0Adiverse%20knowledge%20graphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20163v3&entry.124074799=Read"},
{"title": "Sample complexity of data-driven tuning of model hyperparameters in\n  neural networks with structured parameter-dependent dual function", "author": "Maria-Florina Balcan and Anh Tuan Nguyen and Dravyansh Sharma", "abstract": "  Modern machine learning algorithms, especially deep learning based\ntechniques, typically involve careful hyperparameter tuning to achieve the best\nperformance. Despite the surge of intense interest in practical techniques like\nBayesian optimization and random search based approaches to automating this\nlaborious and compute intensive task, the fundamental learning theoretic\ncomplexity of tuning hyperparameters for deep neural networks is poorly\nunderstood. Inspired by this glaring gap, we initiate the formal study of\nhyperparameter tuning complexity in deep learning through a recently introduced\ndata driven setting. We assume that we have a series of deep learning tasks,\nand we have to tune hyperparameters to do well on average over the distribution\nof tasks. A major difficulty is that the utility function as a function of the\nhyperparameter is very volatile and furthermore, it is given implicitly by an\noptimization problem over the model parameters. To tackle this challenge, we\nintroduce a new technique to characterize the discontinuities and oscillations\nof the utility function on any fixed problem instance as we vary the\nhyperparameter; our analysis relies on subtle concepts including tools from\ndifferential/algebraic geometry and constrained optimization. This can be used\nto show that the learning theoretic complexity of the corresponding family of\nutility functions is bounded. We instantiate our results and provide sample\ncomplexity bounds for concrete applications tuning a hyperparameter that\ninterpolates neural activation functions and setting the kernel parameter in\ngraph neural networks.\n", "link": "http://arxiv.org/abs/2501.13734v3", "date": "2025-02-12", "relevancy": 1.8945, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4985}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4732}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sample%20complexity%20of%20data-driven%20tuning%20of%20model%20hyperparameters%20in%0A%20%20neural%20networks%20with%20structured%20parameter-dependent%20dual%20function&body=Title%3A%20Sample%20complexity%20of%20data-driven%20tuning%20of%20model%20hyperparameters%20in%0A%20%20neural%20networks%20with%20structured%20parameter-dependent%20dual%20function%0AAuthor%3A%20Maria-Florina%20Balcan%20and%20Anh%20Tuan%20Nguyen%20and%20Dravyansh%20Sharma%0AAbstract%3A%20%20%20Modern%20machine%20learning%20algorithms%2C%20especially%20deep%20learning%20based%0Atechniques%2C%20typically%20involve%20careful%20hyperparameter%20tuning%20to%20achieve%20the%20best%0Aperformance.%20Despite%20the%20surge%20of%20intense%20interest%20in%20practical%20techniques%20like%0ABayesian%20optimization%20and%20random%20search%20based%20approaches%20to%20automating%20this%0Alaborious%20and%20compute%20intensive%20task%2C%20the%20fundamental%20learning%20theoretic%0Acomplexity%20of%20tuning%20hyperparameters%20for%20deep%20neural%20networks%20is%20poorly%0Aunderstood.%20Inspired%20by%20this%20glaring%20gap%2C%20we%20initiate%20the%20formal%20study%20of%0Ahyperparameter%20tuning%20complexity%20in%20deep%20learning%20through%20a%20recently%20introduced%0Adata%20driven%20setting.%20We%20assume%20that%20we%20have%20a%20series%20of%20deep%20learning%20tasks%2C%0Aand%20we%20have%20to%20tune%20hyperparameters%20to%20do%20well%20on%20average%20over%20the%20distribution%0Aof%20tasks.%20A%20major%20difficulty%20is%20that%20the%20utility%20function%20as%20a%20function%20of%20the%0Ahyperparameter%20is%20very%20volatile%20and%20furthermore%2C%20it%20is%20given%20implicitly%20by%20an%0Aoptimization%20problem%20over%20the%20model%20parameters.%20To%20tackle%20this%20challenge%2C%20we%0Aintroduce%20a%20new%20technique%20to%20characterize%20the%20discontinuities%20and%20oscillations%0Aof%20the%20utility%20function%20on%20any%20fixed%20problem%20instance%20as%20we%20vary%20the%0Ahyperparameter%3B%20our%20analysis%20relies%20on%20subtle%20concepts%20including%20tools%20from%0Adifferential/algebraic%20geometry%20and%20constrained%20optimization.%20This%20can%20be%20used%0Ato%20show%20that%20the%20learning%20theoretic%20complexity%20of%20the%20corresponding%20family%20of%0Autility%20functions%20is%20bounded.%20We%20instantiate%20our%20results%20and%20provide%20sample%0Acomplexity%20bounds%20for%20concrete%20applications%20tuning%20a%20hyperparameter%20that%0Ainterpolates%20neural%20activation%20functions%20and%20setting%20the%20kernel%20parameter%20in%0Agraph%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13734v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSample%2520complexity%2520of%2520data-driven%2520tuning%2520of%2520model%2520hyperparameters%2520in%250A%2520%2520neural%2520networks%2520with%2520structured%2520parameter-dependent%2520dual%2520function%26entry.906535625%3DMaria-Florina%2520Balcan%2520and%2520Anh%2520Tuan%2520Nguyen%2520and%2520Dravyansh%2520Sharma%26entry.1292438233%3D%2520%2520Modern%2520machine%2520learning%2520algorithms%252C%2520especially%2520deep%2520learning%2520based%250Atechniques%252C%2520typically%2520involve%2520careful%2520hyperparameter%2520tuning%2520to%2520achieve%2520the%2520best%250Aperformance.%2520Despite%2520the%2520surge%2520of%2520intense%2520interest%2520in%2520practical%2520techniques%2520like%250ABayesian%2520optimization%2520and%2520random%2520search%2520based%2520approaches%2520to%2520automating%2520this%250Alaborious%2520and%2520compute%2520intensive%2520task%252C%2520the%2520fundamental%2520learning%2520theoretic%250Acomplexity%2520of%2520tuning%2520hyperparameters%2520for%2520deep%2520neural%2520networks%2520is%2520poorly%250Aunderstood.%2520Inspired%2520by%2520this%2520glaring%2520gap%252C%2520we%2520initiate%2520the%2520formal%2520study%2520of%250Ahyperparameter%2520tuning%2520complexity%2520in%2520deep%2520learning%2520through%2520a%2520recently%2520introduced%250Adata%2520driven%2520setting.%2520We%2520assume%2520that%2520we%2520have%2520a%2520series%2520of%2520deep%2520learning%2520tasks%252C%250Aand%2520we%2520have%2520to%2520tune%2520hyperparameters%2520to%2520do%2520well%2520on%2520average%2520over%2520the%2520distribution%250Aof%2520tasks.%2520A%2520major%2520difficulty%2520is%2520that%2520the%2520utility%2520function%2520as%2520a%2520function%2520of%2520the%250Ahyperparameter%2520is%2520very%2520volatile%2520and%2520furthermore%252C%2520it%2520is%2520given%2520implicitly%2520by%2520an%250Aoptimization%2520problem%2520over%2520the%2520model%2520parameters.%2520To%2520tackle%2520this%2520challenge%252C%2520we%250Aintroduce%2520a%2520new%2520technique%2520to%2520characterize%2520the%2520discontinuities%2520and%2520oscillations%250Aof%2520the%2520utility%2520function%2520on%2520any%2520fixed%2520problem%2520instance%2520as%2520we%2520vary%2520the%250Ahyperparameter%253B%2520our%2520analysis%2520relies%2520on%2520subtle%2520concepts%2520including%2520tools%2520from%250Adifferential/algebraic%2520geometry%2520and%2520constrained%2520optimization.%2520This%2520can%2520be%2520used%250Ato%2520show%2520that%2520the%2520learning%2520theoretic%2520complexity%2520of%2520the%2520corresponding%2520family%2520of%250Autility%2520functions%2520is%2520bounded.%2520We%2520instantiate%2520our%2520results%2520and%2520provide%2520sample%250Acomplexity%2520bounds%2520for%2520concrete%2520applications%2520tuning%2520a%2520hyperparameter%2520that%250Ainterpolates%2520neural%2520activation%2520functions%2520and%2520setting%2520the%2520kernel%2520parameter%2520in%250Agraph%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13734v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sample%20complexity%20of%20data-driven%20tuning%20of%20model%20hyperparameters%20in%0A%20%20neural%20networks%20with%20structured%20parameter-dependent%20dual%20function&entry.906535625=Maria-Florina%20Balcan%20and%20Anh%20Tuan%20Nguyen%20and%20Dravyansh%20Sharma&entry.1292438233=%20%20Modern%20machine%20learning%20algorithms%2C%20especially%20deep%20learning%20based%0Atechniques%2C%20typically%20involve%20careful%20hyperparameter%20tuning%20to%20achieve%20the%20best%0Aperformance.%20Despite%20the%20surge%20of%20intense%20interest%20in%20practical%20techniques%20like%0ABayesian%20optimization%20and%20random%20search%20based%20approaches%20to%20automating%20this%0Alaborious%20and%20compute%20intensive%20task%2C%20the%20fundamental%20learning%20theoretic%0Acomplexity%20of%20tuning%20hyperparameters%20for%20deep%20neural%20networks%20is%20poorly%0Aunderstood.%20Inspired%20by%20this%20glaring%20gap%2C%20we%20initiate%20the%20formal%20study%20of%0Ahyperparameter%20tuning%20complexity%20in%20deep%20learning%20through%20a%20recently%20introduced%0Adata%20driven%20setting.%20We%20assume%20that%20we%20have%20a%20series%20of%20deep%20learning%20tasks%2C%0Aand%20we%20have%20to%20tune%20hyperparameters%20to%20do%20well%20on%20average%20over%20the%20distribution%0Aof%20tasks.%20A%20major%20difficulty%20is%20that%20the%20utility%20function%20as%20a%20function%20of%20the%0Ahyperparameter%20is%20very%20volatile%20and%20furthermore%2C%20it%20is%20given%20implicitly%20by%20an%0Aoptimization%20problem%20over%20the%20model%20parameters.%20To%20tackle%20this%20challenge%2C%20we%0Aintroduce%20a%20new%20technique%20to%20characterize%20the%20discontinuities%20and%20oscillations%0Aof%20the%20utility%20function%20on%20any%20fixed%20problem%20instance%20as%20we%20vary%20the%0Ahyperparameter%3B%20our%20analysis%20relies%20on%20subtle%20concepts%20including%20tools%20from%0Adifferential/algebraic%20geometry%20and%20constrained%20optimization.%20This%20can%20be%20used%0Ato%20show%20that%20the%20learning%20theoretic%20complexity%20of%20the%20corresponding%20family%20of%0Autility%20functions%20is%20bounded.%20We%20instantiate%20our%20results%20and%20provide%20sample%0Acomplexity%20bounds%20for%20concrete%20applications%20tuning%20a%20hyperparameter%20that%0Ainterpolates%20neural%20activation%20functions%20and%20setting%20the%20kernel%20parameter%20in%0Agraph%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13734v3&entry.124074799=Read"},
{"title": "Random ReLU Neural Networks as Non-Gaussian Processes", "author": "Rahul Parhi and Pakshal Bohra and Ayoub El Biari and Mehrsa Pourya and Michael Unser", "abstract": "  We consider a large class of shallow neural networks with randomly\ninitialized parameters and rectified linear unit activation functions. We prove\nthat these random neural networks are well-defined non-Gaussian processes. As a\nby-product, we demonstrate that these networks are solutions to stochastic\ndifferential equations driven by impulsive white noise (combinations of random\nDirac measures). These processes are parameterized by the law of the weights\nand biases as well as the density of activation thresholds in each bounded\nregion of the input domain. We prove that these processes are isotropic and\nwide-sense self-similar with Hurst exponent 3/2. We also derive a remarkably\nsimple closed-form expression for their autocovariance function. Our results\nare fundamentally different from prior work in that we consider a\nnon-asymptotic viewpoint: The number of neurons in each bounded region of the\ninput domain (i.e., the width) is itself a random variable with a Poisson law\nwith mean proportional to the density parameter. Finally, we show that, under\nsuitable hypotheses, as the expected width tends to infinity, these processes\ncan converge in law not only to Gaussian processes, but also to non-Gaussian\nprocesses depending on the law of the weights. Our asymptotic results provide a\nnew take on several classical results (wide networks converge to Gaussian\nprocesses) as well as some new ones (wide networks can converge to non-Gaussian\nprocesses).\n", "link": "http://arxiv.org/abs/2405.10229v2", "date": "2025-02-12", "relevancy": 1.891, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4993}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4735}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Random%20ReLU%20Neural%20Networks%20as%20Non-Gaussian%20Processes&body=Title%3A%20Random%20ReLU%20Neural%20Networks%20as%20Non-Gaussian%20Processes%0AAuthor%3A%20Rahul%20Parhi%20and%20Pakshal%20Bohra%20and%20Ayoub%20El%20Biari%20and%20Mehrsa%20Pourya%20and%20Michael%20Unser%0AAbstract%3A%20%20%20We%20consider%20a%20large%20class%20of%20shallow%20neural%20networks%20with%20randomly%0Ainitialized%20parameters%20and%20rectified%20linear%20unit%20activation%20functions.%20We%20prove%0Athat%20these%20random%20neural%20networks%20are%20well-defined%20non-Gaussian%20processes.%20As%20a%0Aby-product%2C%20we%20demonstrate%20that%20these%20networks%20are%20solutions%20to%20stochastic%0Adifferential%20equations%20driven%20by%20impulsive%20white%20noise%20%28combinations%20of%20random%0ADirac%20measures%29.%20These%20processes%20are%20parameterized%20by%20the%20law%20of%20the%20weights%0Aand%20biases%20as%20well%20as%20the%20density%20of%20activation%20thresholds%20in%20each%20bounded%0Aregion%20of%20the%20input%20domain.%20We%20prove%20that%20these%20processes%20are%20isotropic%20and%0Awide-sense%20self-similar%20with%20Hurst%20exponent%203/2.%20We%20also%20derive%20a%20remarkably%0Asimple%20closed-form%20expression%20for%20their%20autocovariance%20function.%20Our%20results%0Aare%20fundamentally%20different%20from%20prior%20work%20in%20that%20we%20consider%20a%0Anon-asymptotic%20viewpoint%3A%20The%20number%20of%20neurons%20in%20each%20bounded%20region%20of%20the%0Ainput%20domain%20%28i.e.%2C%20the%20width%29%20is%20itself%20a%20random%20variable%20with%20a%20Poisson%20law%0Awith%20mean%20proportional%20to%20the%20density%20parameter.%20Finally%2C%20we%20show%20that%2C%20under%0Asuitable%20hypotheses%2C%20as%20the%20expected%20width%20tends%20to%20infinity%2C%20these%20processes%0Acan%20converge%20in%20law%20not%20only%20to%20Gaussian%20processes%2C%20but%20also%20to%20non-Gaussian%0Aprocesses%20depending%20on%20the%20law%20of%20the%20weights.%20Our%20asymptotic%20results%20provide%20a%0Anew%20take%20on%20several%20classical%20results%20%28wide%20networks%20converge%20to%20Gaussian%0Aprocesses%29%20as%20well%20as%20some%20new%20ones%20%28wide%20networks%20can%20converge%20to%20non-Gaussian%0Aprocesses%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10229v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRandom%2520ReLU%2520Neural%2520Networks%2520as%2520Non-Gaussian%2520Processes%26entry.906535625%3DRahul%2520Parhi%2520and%2520Pakshal%2520Bohra%2520and%2520Ayoub%2520El%2520Biari%2520and%2520Mehrsa%2520Pourya%2520and%2520Michael%2520Unser%26entry.1292438233%3D%2520%2520We%2520consider%2520a%2520large%2520class%2520of%2520shallow%2520neural%2520networks%2520with%2520randomly%250Ainitialized%2520parameters%2520and%2520rectified%2520linear%2520unit%2520activation%2520functions.%2520We%2520prove%250Athat%2520these%2520random%2520neural%2520networks%2520are%2520well-defined%2520non-Gaussian%2520processes.%2520As%2520a%250Aby-product%252C%2520we%2520demonstrate%2520that%2520these%2520networks%2520are%2520solutions%2520to%2520stochastic%250Adifferential%2520equations%2520driven%2520by%2520impulsive%2520white%2520noise%2520%2528combinations%2520of%2520random%250ADirac%2520measures%2529.%2520These%2520processes%2520are%2520parameterized%2520by%2520the%2520law%2520of%2520the%2520weights%250Aand%2520biases%2520as%2520well%2520as%2520the%2520density%2520of%2520activation%2520thresholds%2520in%2520each%2520bounded%250Aregion%2520of%2520the%2520input%2520domain.%2520We%2520prove%2520that%2520these%2520processes%2520are%2520isotropic%2520and%250Awide-sense%2520self-similar%2520with%2520Hurst%2520exponent%25203/2.%2520We%2520also%2520derive%2520a%2520remarkably%250Asimple%2520closed-form%2520expression%2520for%2520their%2520autocovariance%2520function.%2520Our%2520results%250Aare%2520fundamentally%2520different%2520from%2520prior%2520work%2520in%2520that%2520we%2520consider%2520a%250Anon-asymptotic%2520viewpoint%253A%2520The%2520number%2520of%2520neurons%2520in%2520each%2520bounded%2520region%2520of%2520the%250Ainput%2520domain%2520%2528i.e.%252C%2520the%2520width%2529%2520is%2520itself%2520a%2520random%2520variable%2520with%2520a%2520Poisson%2520law%250Awith%2520mean%2520proportional%2520to%2520the%2520density%2520parameter.%2520Finally%252C%2520we%2520show%2520that%252C%2520under%250Asuitable%2520hypotheses%252C%2520as%2520the%2520expected%2520width%2520tends%2520to%2520infinity%252C%2520these%2520processes%250Acan%2520converge%2520in%2520law%2520not%2520only%2520to%2520Gaussian%2520processes%252C%2520but%2520also%2520to%2520non-Gaussian%250Aprocesses%2520depending%2520on%2520the%2520law%2520of%2520the%2520weights.%2520Our%2520asymptotic%2520results%2520provide%2520a%250Anew%2520take%2520on%2520several%2520classical%2520results%2520%2528wide%2520networks%2520converge%2520to%2520Gaussian%250Aprocesses%2529%2520as%2520well%2520as%2520some%2520new%2520ones%2520%2528wide%2520networks%2520can%2520converge%2520to%2520non-Gaussian%250Aprocesses%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10229v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Random%20ReLU%20Neural%20Networks%20as%20Non-Gaussian%20Processes&entry.906535625=Rahul%20Parhi%20and%20Pakshal%20Bohra%20and%20Ayoub%20El%20Biari%20and%20Mehrsa%20Pourya%20and%20Michael%20Unser&entry.1292438233=%20%20We%20consider%20a%20large%20class%20of%20shallow%20neural%20networks%20with%20randomly%0Ainitialized%20parameters%20and%20rectified%20linear%20unit%20activation%20functions.%20We%20prove%0Athat%20these%20random%20neural%20networks%20are%20well-defined%20non-Gaussian%20processes.%20As%20a%0Aby-product%2C%20we%20demonstrate%20that%20these%20networks%20are%20solutions%20to%20stochastic%0Adifferential%20equations%20driven%20by%20impulsive%20white%20noise%20%28combinations%20of%20random%0ADirac%20measures%29.%20These%20processes%20are%20parameterized%20by%20the%20law%20of%20the%20weights%0Aand%20biases%20as%20well%20as%20the%20density%20of%20activation%20thresholds%20in%20each%20bounded%0Aregion%20of%20the%20input%20domain.%20We%20prove%20that%20these%20processes%20are%20isotropic%20and%0Awide-sense%20self-similar%20with%20Hurst%20exponent%203/2.%20We%20also%20derive%20a%20remarkably%0Asimple%20closed-form%20expression%20for%20their%20autocovariance%20function.%20Our%20results%0Aare%20fundamentally%20different%20from%20prior%20work%20in%20that%20we%20consider%20a%0Anon-asymptotic%20viewpoint%3A%20The%20number%20of%20neurons%20in%20each%20bounded%20region%20of%20the%0Ainput%20domain%20%28i.e.%2C%20the%20width%29%20is%20itself%20a%20random%20variable%20with%20a%20Poisson%20law%0Awith%20mean%20proportional%20to%20the%20density%20parameter.%20Finally%2C%20we%20show%20that%2C%20under%0Asuitable%20hypotheses%2C%20as%20the%20expected%20width%20tends%20to%20infinity%2C%20these%20processes%0Acan%20converge%20in%20law%20not%20only%20to%20Gaussian%20processes%2C%20but%20also%20to%20non-Gaussian%0Aprocesses%20depending%20on%20the%20law%20of%20the%20weights.%20Our%20asymptotic%20results%20provide%20a%0Anew%20take%20on%20several%20classical%20results%20%28wide%20networks%20converge%20to%20Gaussian%0Aprocesses%29%20as%20well%20as%20some%20new%20ones%20%28wide%20networks%20can%20converge%20to%20non-Gaussian%0Aprocesses%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10229v2&entry.124074799=Read"},
{"title": "LoSAM: Local Search in Additive Noise Models with Mixed Mechanisms and\n  General Noise for Global Causal Discovery", "author": "Sujai Hiremath and Promit Ghosal and Kyra Gan", "abstract": "  Inferring causal relationships from observational data is crucial when\nexperiments are costly or infeasible. Additive noise models (ANMs) enable\nunique directed acyclic graph (DAG) identification, but existing ANM methods\noften rely on restrictive assumptions on the data generating process, limiting\ntheir applicability to real-world settings. We propose local search in additive\nnoise models, LoSAM, a topological ordering method for learning a unique DAG in\nANMs with mixed causal mechanisms and general noise distributions. We introduce\nnew causal substructures and criteria for identifying roots and leaves,\nenabling efficient top-down learning. We prove asymptotic consistency and\npolynomial runtime, ensuring scalability and sample efficiency. We test LoSAM\non synthetic and real-world data, demonstrating state-of-the-art performance\nacross all mixed mechanism settings.\n", "link": "http://arxiv.org/abs/2410.11759v4", "date": "2025-02-12", "relevancy": 1.8762, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4864}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4799}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoSAM%3A%20Local%20Search%20in%20Additive%20Noise%20Models%20with%20Mixed%20Mechanisms%20and%0A%20%20General%20Noise%20for%20Global%20Causal%20Discovery&body=Title%3A%20LoSAM%3A%20Local%20Search%20in%20Additive%20Noise%20Models%20with%20Mixed%20Mechanisms%20and%0A%20%20General%20Noise%20for%20Global%20Causal%20Discovery%0AAuthor%3A%20Sujai%20Hiremath%20and%20Promit%20Ghosal%20and%20Kyra%20Gan%0AAbstract%3A%20%20%20Inferring%20causal%20relationships%20from%20observational%20data%20is%20crucial%20when%0Aexperiments%20are%20costly%20or%20infeasible.%20Additive%20noise%20models%20%28ANMs%29%20enable%0Aunique%20directed%20acyclic%20graph%20%28DAG%29%20identification%2C%20but%20existing%20ANM%20methods%0Aoften%20rely%20on%20restrictive%20assumptions%20on%20the%20data%20generating%20process%2C%20limiting%0Atheir%20applicability%20to%20real-world%20settings.%20We%20propose%20local%20search%20in%20additive%0Anoise%20models%2C%20LoSAM%2C%20a%20topological%20ordering%20method%20for%20learning%20a%20unique%20DAG%20in%0AANMs%20with%20mixed%20causal%20mechanisms%20and%20general%20noise%20distributions.%20We%20introduce%0Anew%20causal%20substructures%20and%20criteria%20for%20identifying%20roots%20and%20leaves%2C%0Aenabling%20efficient%20top-down%20learning.%20We%20prove%20asymptotic%20consistency%20and%0Apolynomial%20runtime%2C%20ensuring%20scalability%20and%20sample%20efficiency.%20We%20test%20LoSAM%0Aon%20synthetic%20and%20real-world%20data%2C%20demonstrating%20state-of-the-art%20performance%0Aacross%20all%20mixed%20mechanism%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11759v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoSAM%253A%2520Local%2520Search%2520in%2520Additive%2520Noise%2520Models%2520with%2520Mixed%2520Mechanisms%2520and%250A%2520%2520General%2520Noise%2520for%2520Global%2520Causal%2520Discovery%26entry.906535625%3DSujai%2520Hiremath%2520and%2520Promit%2520Ghosal%2520and%2520Kyra%2520Gan%26entry.1292438233%3D%2520%2520Inferring%2520causal%2520relationships%2520from%2520observational%2520data%2520is%2520crucial%2520when%250Aexperiments%2520are%2520costly%2520or%2520infeasible.%2520Additive%2520noise%2520models%2520%2528ANMs%2529%2520enable%250Aunique%2520directed%2520acyclic%2520graph%2520%2528DAG%2529%2520identification%252C%2520but%2520existing%2520ANM%2520methods%250Aoften%2520rely%2520on%2520restrictive%2520assumptions%2520on%2520the%2520data%2520generating%2520process%252C%2520limiting%250Atheir%2520applicability%2520to%2520real-world%2520settings.%2520We%2520propose%2520local%2520search%2520in%2520additive%250Anoise%2520models%252C%2520LoSAM%252C%2520a%2520topological%2520ordering%2520method%2520for%2520learning%2520a%2520unique%2520DAG%2520in%250AANMs%2520with%2520mixed%2520causal%2520mechanisms%2520and%2520general%2520noise%2520distributions.%2520We%2520introduce%250Anew%2520causal%2520substructures%2520and%2520criteria%2520for%2520identifying%2520roots%2520and%2520leaves%252C%250Aenabling%2520efficient%2520top-down%2520learning.%2520We%2520prove%2520asymptotic%2520consistency%2520and%250Apolynomial%2520runtime%252C%2520ensuring%2520scalability%2520and%2520sample%2520efficiency.%2520We%2520test%2520LoSAM%250Aon%2520synthetic%2520and%2520real-world%2520data%252C%2520demonstrating%2520state-of-the-art%2520performance%250Aacross%2520all%2520mixed%2520mechanism%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11759v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoSAM%3A%20Local%20Search%20in%20Additive%20Noise%20Models%20with%20Mixed%20Mechanisms%20and%0A%20%20General%20Noise%20for%20Global%20Causal%20Discovery&entry.906535625=Sujai%20Hiremath%20and%20Promit%20Ghosal%20and%20Kyra%20Gan&entry.1292438233=%20%20Inferring%20causal%20relationships%20from%20observational%20data%20is%20crucial%20when%0Aexperiments%20are%20costly%20or%20infeasible.%20Additive%20noise%20models%20%28ANMs%29%20enable%0Aunique%20directed%20acyclic%20graph%20%28DAG%29%20identification%2C%20but%20existing%20ANM%20methods%0Aoften%20rely%20on%20restrictive%20assumptions%20on%20the%20data%20generating%20process%2C%20limiting%0Atheir%20applicability%20to%20real-world%20settings.%20We%20propose%20local%20search%20in%20additive%0Anoise%20models%2C%20LoSAM%2C%20a%20topological%20ordering%20method%20for%20learning%20a%20unique%20DAG%20in%0AANMs%20with%20mixed%20causal%20mechanisms%20and%20general%20noise%20distributions.%20We%20introduce%0Anew%20causal%20substructures%20and%20criteria%20for%20identifying%20roots%20and%20leaves%2C%0Aenabling%20efficient%20top-down%20learning.%20We%20prove%20asymptotic%20consistency%20and%0Apolynomial%20runtime%2C%20ensuring%20scalability%20and%20sample%20efficiency.%20We%20test%20LoSAM%0Aon%20synthetic%20and%20real-world%20data%2C%20demonstrating%20state-of-the-art%20performance%0Aacross%20all%20mixed%20mechanism%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11759v4&entry.124074799=Read"},
{"title": "Input convex neural networks: universal approximation theorem and\n  implementation for isotropic polyconvex hyperelastic energies", "author": "Gian-Luca Geuken and Patrick Kurzeja and David Wiedemann and J\u00f6rn Mosler", "abstract": "  This paper presents a novel framework of neural networks for isotropic\nhyperelasticity that enforces necessary physical and mathematical constraints\nwhile simultaneously satisfying the universal approximation theorem. The two\nkey ingredients are an input convex network architecture and a formulation in\nthe elementary polynomials of the signed singular values of the deformation\ngradient. In line with previously published networks, it can rigorously capture\nframe-indifference and polyconvexity - as well as further constraints like\nbalance of angular momentum and growth conditions. However and in contrast to\nprevious networks, a universal approximation theorem for the proposed approach\nis proven. To be more explicit, the proposed network can approximate any\nframe-indifferent, isotropic polyconvex energy (provided the network is large\nenough). This is possible by working with a sufficient and necessary criterion\nfor frame-indifferent, isotropic polyconvex functions. Comparative studies with\nexisting approaches identify the advantages of the proposed method,\nparticularly in approximating non-polyconvex energies as well as computing\npolyconvex hulls.\n", "link": "http://arxiv.org/abs/2502.08534v1", "date": "2025-02-12", "relevancy": 1.8553, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4722}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4681}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Input%20convex%20neural%20networks%3A%20universal%20approximation%20theorem%20and%0A%20%20implementation%20for%20isotropic%20polyconvex%20hyperelastic%20energies&body=Title%3A%20Input%20convex%20neural%20networks%3A%20universal%20approximation%20theorem%20and%0A%20%20implementation%20for%20isotropic%20polyconvex%20hyperelastic%20energies%0AAuthor%3A%20Gian-Luca%20Geuken%20and%20Patrick%20Kurzeja%20and%20David%20Wiedemann%20and%20J%C3%B6rn%20Mosler%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20framework%20of%20neural%20networks%20for%20isotropic%0Ahyperelasticity%20that%20enforces%20necessary%20physical%20and%20mathematical%20constraints%0Awhile%20simultaneously%20satisfying%20the%20universal%20approximation%20theorem.%20The%20two%0Akey%20ingredients%20are%20an%20input%20convex%20network%20architecture%20and%20a%20formulation%20in%0Athe%20elementary%20polynomials%20of%20the%20signed%20singular%20values%20of%20the%20deformation%0Agradient.%20In%20line%20with%20previously%20published%20networks%2C%20it%20can%20rigorously%20capture%0Aframe-indifference%20and%20polyconvexity%20-%20as%20well%20as%20further%20constraints%20like%0Abalance%20of%20angular%20momentum%20and%20growth%20conditions.%20However%20and%20in%20contrast%20to%0Aprevious%20networks%2C%20a%20universal%20approximation%20theorem%20for%20the%20proposed%20approach%0Ais%20proven.%20To%20be%20more%20explicit%2C%20the%20proposed%20network%20can%20approximate%20any%0Aframe-indifferent%2C%20isotropic%20polyconvex%20energy%20%28provided%20the%20network%20is%20large%0Aenough%29.%20This%20is%20possible%20by%20working%20with%20a%20sufficient%20and%20necessary%20criterion%0Afor%20frame-indifferent%2C%20isotropic%20polyconvex%20functions.%20Comparative%20studies%20with%0Aexisting%20approaches%20identify%20the%20advantages%20of%20the%20proposed%20method%2C%0Aparticularly%20in%20approximating%20non-polyconvex%20energies%20as%20well%20as%20computing%0Apolyconvex%20hulls.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08534v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInput%2520convex%2520neural%2520networks%253A%2520universal%2520approximation%2520theorem%2520and%250A%2520%2520implementation%2520for%2520isotropic%2520polyconvex%2520hyperelastic%2520energies%26entry.906535625%3DGian-Luca%2520Geuken%2520and%2520Patrick%2520Kurzeja%2520and%2520David%2520Wiedemann%2520and%2520J%25C3%25B6rn%2520Mosler%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520framework%2520of%2520neural%2520networks%2520for%2520isotropic%250Ahyperelasticity%2520that%2520enforces%2520necessary%2520physical%2520and%2520mathematical%2520constraints%250Awhile%2520simultaneously%2520satisfying%2520the%2520universal%2520approximation%2520theorem.%2520The%2520two%250Akey%2520ingredients%2520are%2520an%2520input%2520convex%2520network%2520architecture%2520and%2520a%2520formulation%2520in%250Athe%2520elementary%2520polynomials%2520of%2520the%2520signed%2520singular%2520values%2520of%2520the%2520deformation%250Agradient.%2520In%2520line%2520with%2520previously%2520published%2520networks%252C%2520it%2520can%2520rigorously%2520capture%250Aframe-indifference%2520and%2520polyconvexity%2520-%2520as%2520well%2520as%2520further%2520constraints%2520like%250Abalance%2520of%2520angular%2520momentum%2520and%2520growth%2520conditions.%2520However%2520and%2520in%2520contrast%2520to%250Aprevious%2520networks%252C%2520a%2520universal%2520approximation%2520theorem%2520for%2520the%2520proposed%2520approach%250Ais%2520proven.%2520To%2520be%2520more%2520explicit%252C%2520the%2520proposed%2520network%2520can%2520approximate%2520any%250Aframe-indifferent%252C%2520isotropic%2520polyconvex%2520energy%2520%2528provided%2520the%2520network%2520is%2520large%250Aenough%2529.%2520This%2520is%2520possible%2520by%2520working%2520with%2520a%2520sufficient%2520and%2520necessary%2520criterion%250Afor%2520frame-indifferent%252C%2520isotropic%2520polyconvex%2520functions.%2520Comparative%2520studies%2520with%250Aexisting%2520approaches%2520identify%2520the%2520advantages%2520of%2520the%2520proposed%2520method%252C%250Aparticularly%2520in%2520approximating%2520non-polyconvex%2520energies%2520as%2520well%2520as%2520computing%250Apolyconvex%2520hulls.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08534v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Input%20convex%20neural%20networks%3A%20universal%20approximation%20theorem%20and%0A%20%20implementation%20for%20isotropic%20polyconvex%20hyperelastic%20energies&entry.906535625=Gian-Luca%20Geuken%20and%20Patrick%20Kurzeja%20and%20David%20Wiedemann%20and%20J%C3%B6rn%20Mosler&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20framework%20of%20neural%20networks%20for%20isotropic%0Ahyperelasticity%20that%20enforces%20necessary%20physical%20and%20mathematical%20constraints%0Awhile%20simultaneously%20satisfying%20the%20universal%20approximation%20theorem.%20The%20two%0Akey%20ingredients%20are%20an%20input%20convex%20network%20architecture%20and%20a%20formulation%20in%0Athe%20elementary%20polynomials%20of%20the%20signed%20singular%20values%20of%20the%20deformation%0Agradient.%20In%20line%20with%20previously%20published%20networks%2C%20it%20can%20rigorously%20capture%0Aframe-indifference%20and%20polyconvexity%20-%20as%20well%20as%20further%20constraints%20like%0Abalance%20of%20angular%20momentum%20and%20growth%20conditions.%20However%20and%20in%20contrast%20to%0Aprevious%20networks%2C%20a%20universal%20approximation%20theorem%20for%20the%20proposed%20approach%0Ais%20proven.%20To%20be%20more%20explicit%2C%20the%20proposed%20network%20can%20approximate%20any%0Aframe-indifferent%2C%20isotropic%20polyconvex%20energy%20%28provided%20the%20network%20is%20large%0Aenough%29.%20This%20is%20possible%20by%20working%20with%20a%20sufficient%20and%20necessary%20criterion%0Afor%20frame-indifferent%2C%20isotropic%20polyconvex%20functions.%20Comparative%20studies%20with%0Aexisting%20approaches%20identify%20the%20advantages%20of%20the%20proposed%20method%2C%0Aparticularly%20in%20approximating%20non-polyconvex%20energies%20as%20well%20as%20computing%0Apolyconvex%20hulls.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08534v1&entry.124074799=Read"},
{"title": "A Stability Principle for Learning under Non-Stationarity", "author": "Chengpiao Huang and Kaizheng Wang", "abstract": "  We develop a versatile framework for statistical learning in non-stationary\nenvironments. In each time period, our approach applies a stability principle\nto select a look-back window that maximizes the utilization of historical data\nwhile keeping the cumulative bias within an acceptable range relative to the\nstochastic error. Our theory and numerical experiments showcase the adaptivity\nof this approach to unknown non-stationarity. We prove regret bounds that are\nminimax optimal up to logarithmic factors when the population losses are\nstrongly convex, or Lipschitz only. At the heart of our analysis lie two novel\ncomponents: a measure of similarity between functions and a segmentation\ntechnique for dividing the non-stationary data sequence into quasi-stationary\npieces.\n", "link": "http://arxiv.org/abs/2310.18304v4", "date": "2025-02-12", "relevancy": 1.8426, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4626}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4617}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4582}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Stability%20Principle%20for%20Learning%20under%20Non-Stationarity&body=Title%3A%20A%20Stability%20Principle%20for%20Learning%20under%20Non-Stationarity%0AAuthor%3A%20Chengpiao%20Huang%20and%20Kaizheng%20Wang%0AAbstract%3A%20%20%20We%20develop%20a%20versatile%20framework%20for%20statistical%20learning%20in%20non-stationary%0Aenvironments.%20In%20each%20time%20period%2C%20our%20approach%20applies%20a%20stability%20principle%0Ato%20select%20a%20look-back%20window%20that%20maximizes%20the%20utilization%20of%20historical%20data%0Awhile%20keeping%20the%20cumulative%20bias%20within%20an%20acceptable%20range%20relative%20to%20the%0Astochastic%20error.%20Our%20theory%20and%20numerical%20experiments%20showcase%20the%20adaptivity%0Aof%20this%20approach%20to%20unknown%20non-stationarity.%20We%20prove%20regret%20bounds%20that%20are%0Aminimax%20optimal%20up%20to%20logarithmic%20factors%20when%20the%20population%20losses%20are%0Astrongly%20convex%2C%20or%20Lipschitz%20only.%20At%20the%20heart%20of%20our%20analysis%20lie%20two%20novel%0Acomponents%3A%20a%20measure%20of%20similarity%20between%20functions%20and%20a%20segmentation%0Atechnique%20for%20dividing%20the%20non-stationary%20data%20sequence%20into%20quasi-stationary%0Apieces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.18304v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Stability%2520Principle%2520for%2520Learning%2520under%2520Non-Stationarity%26entry.906535625%3DChengpiao%2520Huang%2520and%2520Kaizheng%2520Wang%26entry.1292438233%3D%2520%2520We%2520develop%2520a%2520versatile%2520framework%2520for%2520statistical%2520learning%2520in%2520non-stationary%250Aenvironments.%2520In%2520each%2520time%2520period%252C%2520our%2520approach%2520applies%2520a%2520stability%2520principle%250Ato%2520select%2520a%2520look-back%2520window%2520that%2520maximizes%2520the%2520utilization%2520of%2520historical%2520data%250Awhile%2520keeping%2520the%2520cumulative%2520bias%2520within%2520an%2520acceptable%2520range%2520relative%2520to%2520the%250Astochastic%2520error.%2520Our%2520theory%2520and%2520numerical%2520experiments%2520showcase%2520the%2520adaptivity%250Aof%2520this%2520approach%2520to%2520unknown%2520non-stationarity.%2520We%2520prove%2520regret%2520bounds%2520that%2520are%250Aminimax%2520optimal%2520up%2520to%2520logarithmic%2520factors%2520when%2520the%2520population%2520losses%2520are%250Astrongly%2520convex%252C%2520or%2520Lipschitz%2520only.%2520At%2520the%2520heart%2520of%2520our%2520analysis%2520lie%2520two%2520novel%250Acomponents%253A%2520a%2520measure%2520of%2520similarity%2520between%2520functions%2520and%2520a%2520segmentation%250Atechnique%2520for%2520dividing%2520the%2520non-stationary%2520data%2520sequence%2520into%2520quasi-stationary%250Apieces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.18304v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Stability%20Principle%20for%20Learning%20under%20Non-Stationarity&entry.906535625=Chengpiao%20Huang%20and%20Kaizheng%20Wang&entry.1292438233=%20%20We%20develop%20a%20versatile%20framework%20for%20statistical%20learning%20in%20non-stationary%0Aenvironments.%20In%20each%20time%20period%2C%20our%20approach%20applies%20a%20stability%20principle%0Ato%20select%20a%20look-back%20window%20that%20maximizes%20the%20utilization%20of%20historical%20data%0Awhile%20keeping%20the%20cumulative%20bias%20within%20an%20acceptable%20range%20relative%20to%20the%0Astochastic%20error.%20Our%20theory%20and%20numerical%20experiments%20showcase%20the%20adaptivity%0Aof%20this%20approach%20to%20unknown%20non-stationarity.%20We%20prove%20regret%20bounds%20that%20are%0Aminimax%20optimal%20up%20to%20logarithmic%20factors%20when%20the%20population%20losses%20are%0Astrongly%20convex%2C%20or%20Lipschitz%20only.%20At%20the%20heart%20of%20our%20analysis%20lie%20two%20novel%0Acomponents%3A%20a%20measure%20of%20similarity%20between%20functions%20and%20a%20segmentation%0Atechnique%20for%20dividing%20the%20non-stationary%20data%20sequence%20into%20quasi-stationary%0Apieces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.18304v4&entry.124074799=Read"},
{"title": "On the convergence rate of noisy Bayesian Optimization with Expected\n  Improvement", "author": "Jingyi Wang and Haowei Wang and Nai-Yuan Chiang and Cosmin G. Petra", "abstract": "  Expected improvement (EI) is one of the most widely used acquisition\nfunctions in Bayesian optimization (BO). Despite its proven success in\napplications for decades, important open questions remain on the theoretical\nconvergence behaviors and rates for EI. In this paper, we contribute to the\nconvergence theory of EI in three novel and critical areas. First, we consider\nobjective functions that fit under the Gaussian process (GP) prior assumption,\nwhereas existing works mostly focus on functions in the reproducing kernel\nHilbert space (RKHS). Second, we establish for the first time the asymptotic\nerror bound and its corresponding rate for GP-EI with noisy observations under\nthe GP prior assumption. Third, by investigating the exploration and\nexploitation properties of the non-convex EI function, we establish improved\nerror bounds of GP-EI for both the noise-free and noisy cases.\n", "link": "http://arxiv.org/abs/2501.09262v2", "date": "2025-02-12", "relevancy": 1.834, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4926}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4538}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20convergence%20rate%20of%20noisy%20Bayesian%20Optimization%20with%20Expected%0A%20%20Improvement&body=Title%3A%20On%20the%20convergence%20rate%20of%20noisy%20Bayesian%20Optimization%20with%20Expected%0A%20%20Improvement%0AAuthor%3A%20Jingyi%20Wang%20and%20Haowei%20Wang%20and%20Nai-Yuan%20Chiang%20and%20Cosmin%20G.%20Petra%0AAbstract%3A%20%20%20Expected%20improvement%20%28EI%29%20is%20one%20of%20the%20most%20widely%20used%20acquisition%0Afunctions%20in%20Bayesian%20optimization%20%28BO%29.%20Despite%20its%20proven%20success%20in%0Aapplications%20for%20decades%2C%20important%20open%20questions%20remain%20on%20the%20theoretical%0Aconvergence%20behaviors%20and%20rates%20for%20EI.%20In%20this%20paper%2C%20we%20contribute%20to%20the%0Aconvergence%20theory%20of%20EI%20in%20three%20novel%20and%20critical%20areas.%20First%2C%20we%20consider%0Aobjective%20functions%20that%20fit%20under%20the%20Gaussian%20process%20%28GP%29%20prior%20assumption%2C%0Awhereas%20existing%20works%20mostly%20focus%20on%20functions%20in%20the%20reproducing%20kernel%0AHilbert%20space%20%28RKHS%29.%20Second%2C%20we%20establish%20for%20the%20first%20time%20the%20asymptotic%0Aerror%20bound%20and%20its%20corresponding%20rate%20for%20GP-EI%20with%20noisy%20observations%20under%0Athe%20GP%20prior%20assumption.%20Third%2C%20by%20investigating%20the%20exploration%20and%0Aexploitation%20properties%20of%20the%20non-convex%20EI%20function%2C%20we%20establish%20improved%0Aerror%20bounds%20of%20GP-EI%20for%20both%20the%20noise-free%20and%20noisy%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09262v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520convergence%2520rate%2520of%2520noisy%2520Bayesian%2520Optimization%2520with%2520Expected%250A%2520%2520Improvement%26entry.906535625%3DJingyi%2520Wang%2520and%2520Haowei%2520Wang%2520and%2520Nai-Yuan%2520Chiang%2520and%2520Cosmin%2520G.%2520Petra%26entry.1292438233%3D%2520%2520Expected%2520improvement%2520%2528EI%2529%2520is%2520one%2520of%2520the%2520most%2520widely%2520used%2520acquisition%250Afunctions%2520in%2520Bayesian%2520optimization%2520%2528BO%2529.%2520Despite%2520its%2520proven%2520success%2520in%250Aapplications%2520for%2520decades%252C%2520important%2520open%2520questions%2520remain%2520on%2520the%2520theoretical%250Aconvergence%2520behaviors%2520and%2520rates%2520for%2520EI.%2520In%2520this%2520paper%252C%2520we%2520contribute%2520to%2520the%250Aconvergence%2520theory%2520of%2520EI%2520in%2520three%2520novel%2520and%2520critical%2520areas.%2520First%252C%2520we%2520consider%250Aobjective%2520functions%2520that%2520fit%2520under%2520the%2520Gaussian%2520process%2520%2528GP%2529%2520prior%2520assumption%252C%250Awhereas%2520existing%2520works%2520mostly%2520focus%2520on%2520functions%2520in%2520the%2520reproducing%2520kernel%250AHilbert%2520space%2520%2528RKHS%2529.%2520Second%252C%2520we%2520establish%2520for%2520the%2520first%2520time%2520the%2520asymptotic%250Aerror%2520bound%2520and%2520its%2520corresponding%2520rate%2520for%2520GP-EI%2520with%2520noisy%2520observations%2520under%250Athe%2520GP%2520prior%2520assumption.%2520Third%252C%2520by%2520investigating%2520the%2520exploration%2520and%250Aexploitation%2520properties%2520of%2520the%2520non-convex%2520EI%2520function%252C%2520we%2520establish%2520improved%250Aerror%2520bounds%2520of%2520GP-EI%2520for%2520both%2520the%2520noise-free%2520and%2520noisy%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09262v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20convergence%20rate%20of%20noisy%20Bayesian%20Optimization%20with%20Expected%0A%20%20Improvement&entry.906535625=Jingyi%20Wang%20and%20Haowei%20Wang%20and%20Nai-Yuan%20Chiang%20and%20Cosmin%20G.%20Petra&entry.1292438233=%20%20Expected%20improvement%20%28EI%29%20is%20one%20of%20the%20most%20widely%20used%20acquisition%0Afunctions%20in%20Bayesian%20optimization%20%28BO%29.%20Despite%20its%20proven%20success%20in%0Aapplications%20for%20decades%2C%20important%20open%20questions%20remain%20on%20the%20theoretical%0Aconvergence%20behaviors%20and%20rates%20for%20EI.%20In%20this%20paper%2C%20we%20contribute%20to%20the%0Aconvergence%20theory%20of%20EI%20in%20three%20novel%20and%20critical%20areas.%20First%2C%20we%20consider%0Aobjective%20functions%20that%20fit%20under%20the%20Gaussian%20process%20%28GP%29%20prior%20assumption%2C%0Awhereas%20existing%20works%20mostly%20focus%20on%20functions%20in%20the%20reproducing%20kernel%0AHilbert%20space%20%28RKHS%29.%20Second%2C%20we%20establish%20for%20the%20first%20time%20the%20asymptotic%0Aerror%20bound%20and%20its%20corresponding%20rate%20for%20GP-EI%20with%20noisy%20observations%20under%0Athe%20GP%20prior%20assumption.%20Third%2C%20by%20investigating%20the%20exploration%20and%0Aexploitation%20properties%20of%20the%20non-convex%20EI%20function%2C%20we%20establish%20improved%0Aerror%20bounds%20of%20GP-EI%20for%20both%20the%20noise-free%20and%20noisy%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09262v2&entry.124074799=Read"},
{"title": "SwiftSketch: A Diffusion Model for Image-to-Vector Sketch Generation", "author": "Ellie Arar and Yarden Frenkel and Daniel Cohen-Or and Ariel Shamir and Yael Vinker", "abstract": "  Recent advancements in large vision-language models have enabled highly\nexpressive and diverse vector sketch generation. However, state-of-the-art\nmethods rely on a time-consuming optimization process involving repeated\nfeedback from a pretrained model to determine stroke placement. Consequently,\ndespite producing impressive sketches, these methods are limited in practical\napplications. In this work, we introduce SwiftSketch, a diffusion model for\nimage-conditioned vector sketch generation that can produce high-quality\nsketches in less than a second. SwiftSketch operates by progressively denoising\nstroke control points sampled from a Gaussian distribution. Its\ntransformer-decoder architecture is designed to effectively handle the discrete\nnature of vector representation and capture the inherent global dependencies\nbetween strokes. To train SwiftSketch, we construct a synthetic dataset of\nimage-sketch pairs, addressing the limitations of existing sketch datasets,\nwhich are often created by non-artists and lack professional quality. For\ngenerating these synthetic sketches, we introduce ControlSketch, a method that\nenhances SDS-based techniques by incorporating precise spatial control through\na depth-aware ControlNet. We demonstrate that SwiftSketch generalizes across\ndiverse concepts, efficiently producing sketches that combine high fidelity\nwith a natural and visually appealing style.\n", "link": "http://arxiv.org/abs/2502.08642v1", "date": "2025-02-12", "relevancy": 1.8338, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.642}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6101}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SwiftSketch%3A%20A%20Diffusion%20Model%20for%20Image-to-Vector%20Sketch%20Generation&body=Title%3A%20SwiftSketch%3A%20A%20Diffusion%20Model%20for%20Image-to-Vector%20Sketch%20Generation%0AAuthor%3A%20Ellie%20Arar%20and%20Yarden%20Frenkel%20and%20Daniel%20Cohen-Or%20and%20Ariel%20Shamir%20and%20Yael%20Vinker%0AAbstract%3A%20%20%20Recent%20advancements%20in%20large%20vision-language%20models%20have%20enabled%20highly%0Aexpressive%20and%20diverse%20vector%20sketch%20generation.%20However%2C%20state-of-the-art%0Amethods%20rely%20on%20a%20time-consuming%20optimization%20process%20involving%20repeated%0Afeedback%20from%20a%20pretrained%20model%20to%20determine%20stroke%20placement.%20Consequently%2C%0Adespite%20producing%20impressive%20sketches%2C%20these%20methods%20are%20limited%20in%20practical%0Aapplications.%20In%20this%20work%2C%20we%20introduce%20SwiftSketch%2C%20a%20diffusion%20model%20for%0Aimage-conditioned%20vector%20sketch%20generation%20that%20can%20produce%20high-quality%0Asketches%20in%20less%20than%20a%20second.%20SwiftSketch%20operates%20by%20progressively%20denoising%0Astroke%20control%20points%20sampled%20from%20a%20Gaussian%20distribution.%20Its%0Atransformer-decoder%20architecture%20is%20designed%20to%20effectively%20handle%20the%20discrete%0Anature%20of%20vector%20representation%20and%20capture%20the%20inherent%20global%20dependencies%0Abetween%20strokes.%20To%20train%20SwiftSketch%2C%20we%20construct%20a%20synthetic%20dataset%20of%0Aimage-sketch%20pairs%2C%20addressing%20the%20limitations%20of%20existing%20sketch%20datasets%2C%0Awhich%20are%20often%20created%20by%20non-artists%20and%20lack%20professional%20quality.%20For%0Agenerating%20these%20synthetic%20sketches%2C%20we%20introduce%20ControlSketch%2C%20a%20method%20that%0Aenhances%20SDS-based%20techniques%20by%20incorporating%20precise%20spatial%20control%20through%0Aa%20depth-aware%20ControlNet.%20We%20demonstrate%20that%20SwiftSketch%20generalizes%20across%0Adiverse%20concepts%2C%20efficiently%20producing%20sketches%20that%20combine%20high%20fidelity%0Awith%20a%20natural%20and%20visually%20appealing%20style.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08642v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSwiftSketch%253A%2520A%2520Diffusion%2520Model%2520for%2520Image-to-Vector%2520Sketch%2520Generation%26entry.906535625%3DEllie%2520Arar%2520and%2520Yarden%2520Frenkel%2520and%2520Daniel%2520Cohen-Or%2520and%2520Ariel%2520Shamir%2520and%2520Yael%2520Vinker%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520large%2520vision-language%2520models%2520have%2520enabled%2520highly%250Aexpressive%2520and%2520diverse%2520vector%2520sketch%2520generation.%2520However%252C%2520state-of-the-art%250Amethods%2520rely%2520on%2520a%2520time-consuming%2520optimization%2520process%2520involving%2520repeated%250Afeedback%2520from%2520a%2520pretrained%2520model%2520to%2520determine%2520stroke%2520placement.%2520Consequently%252C%250Adespite%2520producing%2520impressive%2520sketches%252C%2520these%2520methods%2520are%2520limited%2520in%2520practical%250Aapplications.%2520In%2520this%2520work%252C%2520we%2520introduce%2520SwiftSketch%252C%2520a%2520diffusion%2520model%2520for%250Aimage-conditioned%2520vector%2520sketch%2520generation%2520that%2520can%2520produce%2520high-quality%250Asketches%2520in%2520less%2520than%2520a%2520second.%2520SwiftSketch%2520operates%2520by%2520progressively%2520denoising%250Astroke%2520control%2520points%2520sampled%2520from%2520a%2520Gaussian%2520distribution.%2520Its%250Atransformer-decoder%2520architecture%2520is%2520designed%2520to%2520effectively%2520handle%2520the%2520discrete%250Anature%2520of%2520vector%2520representation%2520and%2520capture%2520the%2520inherent%2520global%2520dependencies%250Abetween%2520strokes.%2520To%2520train%2520SwiftSketch%252C%2520we%2520construct%2520a%2520synthetic%2520dataset%2520of%250Aimage-sketch%2520pairs%252C%2520addressing%2520the%2520limitations%2520of%2520existing%2520sketch%2520datasets%252C%250Awhich%2520are%2520often%2520created%2520by%2520non-artists%2520and%2520lack%2520professional%2520quality.%2520For%250Agenerating%2520these%2520synthetic%2520sketches%252C%2520we%2520introduce%2520ControlSketch%252C%2520a%2520method%2520that%250Aenhances%2520SDS-based%2520techniques%2520by%2520incorporating%2520precise%2520spatial%2520control%2520through%250Aa%2520depth-aware%2520ControlNet.%2520We%2520demonstrate%2520that%2520SwiftSketch%2520generalizes%2520across%250Adiverse%2520concepts%252C%2520efficiently%2520producing%2520sketches%2520that%2520combine%2520high%2520fidelity%250Awith%2520a%2520natural%2520and%2520visually%2520appealing%2520style.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08642v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SwiftSketch%3A%20A%20Diffusion%20Model%20for%20Image-to-Vector%20Sketch%20Generation&entry.906535625=Ellie%20Arar%20and%20Yarden%20Frenkel%20and%20Daniel%20Cohen-Or%20and%20Ariel%20Shamir%20and%20Yael%20Vinker&entry.1292438233=%20%20Recent%20advancements%20in%20large%20vision-language%20models%20have%20enabled%20highly%0Aexpressive%20and%20diverse%20vector%20sketch%20generation.%20However%2C%20state-of-the-art%0Amethods%20rely%20on%20a%20time-consuming%20optimization%20process%20involving%20repeated%0Afeedback%20from%20a%20pretrained%20model%20to%20determine%20stroke%20placement.%20Consequently%2C%0Adespite%20producing%20impressive%20sketches%2C%20these%20methods%20are%20limited%20in%20practical%0Aapplications.%20In%20this%20work%2C%20we%20introduce%20SwiftSketch%2C%20a%20diffusion%20model%20for%0Aimage-conditioned%20vector%20sketch%20generation%20that%20can%20produce%20high-quality%0Asketches%20in%20less%20than%20a%20second.%20SwiftSketch%20operates%20by%20progressively%20denoising%0Astroke%20control%20points%20sampled%20from%20a%20Gaussian%20distribution.%20Its%0Atransformer-decoder%20architecture%20is%20designed%20to%20effectively%20handle%20the%20discrete%0Anature%20of%20vector%20representation%20and%20capture%20the%20inherent%20global%20dependencies%0Abetween%20strokes.%20To%20train%20SwiftSketch%2C%20we%20construct%20a%20synthetic%20dataset%20of%0Aimage-sketch%20pairs%2C%20addressing%20the%20limitations%20of%20existing%20sketch%20datasets%2C%0Awhich%20are%20often%20created%20by%20non-artists%20and%20lack%20professional%20quality.%20For%0Agenerating%20these%20synthetic%20sketches%2C%20we%20introduce%20ControlSketch%2C%20a%20method%20that%0Aenhances%20SDS-based%20techniques%20by%20incorporating%20precise%20spatial%20control%20through%0Aa%20depth-aware%20ControlNet.%20We%20demonstrate%20that%20SwiftSketch%20generalizes%20across%0Adiverse%20concepts%2C%20efficiently%20producing%20sketches%20that%20combine%20high%20fidelity%0Awith%20a%20natural%20and%20visually%20appealing%20style.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08642v1&entry.124074799=Read"},
{"title": "Ultrasound Image Generation using Latent Diffusion Models", "author": "Benoit Freiche and Anthony El-Khoury and Ali Nasiri-Sarvi and Mahdi S. Hosseini and Damien Garcia and Adrian Basarab and Mathieu Boily and Hassan Rivaz", "abstract": "  Diffusion models for image generation have been a subject of increasing\ninterest due to their ability to generate diverse, high-quality images. Image\ngeneration has immense potential in medical imaging because open-source medical\nimages are difficult to obtain compared to natural images, especially for rare\nconditions. The generated images can be used later to train classification and\nsegmentation models. In this paper, we propose simulating realistic ultrasound\n(US) images by successive fine-tuning of large diffusion models on different\npublicly available databases. To do so, we fine-tuned Stable Diffusion, a\nstate-of-the-art latent diffusion model, on BUSI (Breast US Images) an\nultrasound breast image dataset. We successfully generated high-quality US\nimages of the breast using simple prompts that specify the organ and pathology,\nwhich appeared realistic to three experienced US scientists and a US\nradiologist. Additionally, we provided user control by conditioning the model\nwith segmentations through ControlNet. We will release the source code at\nhttp://code.sonography.ai/ to allow fast US image generation to the scientific\ncommunity.\n", "link": "http://arxiv.org/abs/2502.08580v1", "date": "2025-02-12", "relevancy": 1.8133, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6746}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5852}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ultrasound%20Image%20Generation%20using%20Latent%20Diffusion%20Models&body=Title%3A%20Ultrasound%20Image%20Generation%20using%20Latent%20Diffusion%20Models%0AAuthor%3A%20Benoit%20Freiche%20and%20Anthony%20El-Khoury%20and%20Ali%20Nasiri-Sarvi%20and%20Mahdi%20S.%20Hosseini%20and%20Damien%20Garcia%20and%20Adrian%20Basarab%20and%20Mathieu%20Boily%20and%20Hassan%20Rivaz%0AAbstract%3A%20%20%20Diffusion%20models%20for%20image%20generation%20have%20been%20a%20subject%20of%20increasing%0Ainterest%20due%20to%20their%20ability%20to%20generate%20diverse%2C%20high-quality%20images.%20Image%0Ageneration%20has%20immense%20potential%20in%20medical%20imaging%20because%20open-source%20medical%0Aimages%20are%20difficult%20to%20obtain%20compared%20to%20natural%20images%2C%20especially%20for%20rare%0Aconditions.%20The%20generated%20images%20can%20be%20used%20later%20to%20train%20classification%20and%0Asegmentation%20models.%20In%20this%20paper%2C%20we%20propose%20simulating%20realistic%20ultrasound%0A%28US%29%20images%20by%20successive%20fine-tuning%20of%20large%20diffusion%20models%20on%20different%0Apublicly%20available%20databases.%20To%20do%20so%2C%20we%20fine-tuned%20Stable%20Diffusion%2C%20a%0Astate-of-the-art%20latent%20diffusion%20model%2C%20on%20BUSI%20%28Breast%20US%20Images%29%20an%0Aultrasound%20breast%20image%20dataset.%20We%20successfully%20generated%20high-quality%20US%0Aimages%20of%20the%20breast%20using%20simple%20prompts%20that%20specify%20the%20organ%20and%20pathology%2C%0Awhich%20appeared%20realistic%20to%20three%20experienced%20US%20scientists%20and%20a%20US%0Aradiologist.%20Additionally%2C%20we%20provided%20user%20control%20by%20conditioning%20the%20model%0Awith%20segmentations%20through%20ControlNet.%20We%20will%20release%20the%20source%20code%20at%0Ahttp%3A//code.sonography.ai/%20to%20allow%20fast%20US%20image%20generation%20to%20the%20scientific%0Acommunity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08580v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUltrasound%2520Image%2520Generation%2520using%2520Latent%2520Diffusion%2520Models%26entry.906535625%3DBenoit%2520Freiche%2520and%2520Anthony%2520El-Khoury%2520and%2520Ali%2520Nasiri-Sarvi%2520and%2520Mahdi%2520S.%2520Hosseini%2520and%2520Damien%2520Garcia%2520and%2520Adrian%2520Basarab%2520and%2520Mathieu%2520Boily%2520and%2520Hassan%2520Rivaz%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520for%2520image%2520generation%2520have%2520been%2520a%2520subject%2520of%2520increasing%250Ainterest%2520due%2520to%2520their%2520ability%2520to%2520generate%2520diverse%252C%2520high-quality%2520images.%2520Image%250Ageneration%2520has%2520immense%2520potential%2520in%2520medical%2520imaging%2520because%2520open-source%2520medical%250Aimages%2520are%2520difficult%2520to%2520obtain%2520compared%2520to%2520natural%2520images%252C%2520especially%2520for%2520rare%250Aconditions.%2520The%2520generated%2520images%2520can%2520be%2520used%2520later%2520to%2520train%2520classification%2520and%250Asegmentation%2520models.%2520In%2520this%2520paper%252C%2520we%2520propose%2520simulating%2520realistic%2520ultrasound%250A%2528US%2529%2520images%2520by%2520successive%2520fine-tuning%2520of%2520large%2520diffusion%2520models%2520on%2520different%250Apublicly%2520available%2520databases.%2520To%2520do%2520so%252C%2520we%2520fine-tuned%2520Stable%2520Diffusion%252C%2520a%250Astate-of-the-art%2520latent%2520diffusion%2520model%252C%2520on%2520BUSI%2520%2528Breast%2520US%2520Images%2529%2520an%250Aultrasound%2520breast%2520image%2520dataset.%2520We%2520successfully%2520generated%2520high-quality%2520US%250Aimages%2520of%2520the%2520breast%2520using%2520simple%2520prompts%2520that%2520specify%2520the%2520organ%2520and%2520pathology%252C%250Awhich%2520appeared%2520realistic%2520to%2520three%2520experienced%2520US%2520scientists%2520and%2520a%2520US%250Aradiologist.%2520Additionally%252C%2520we%2520provided%2520user%2520control%2520by%2520conditioning%2520the%2520model%250Awith%2520segmentations%2520through%2520ControlNet.%2520We%2520will%2520release%2520the%2520source%2520code%2520at%250Ahttp%253A//code.sonography.ai/%2520to%2520allow%2520fast%2520US%2520image%2520generation%2520to%2520the%2520scientific%250Acommunity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08580v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ultrasound%20Image%20Generation%20using%20Latent%20Diffusion%20Models&entry.906535625=Benoit%20Freiche%20and%20Anthony%20El-Khoury%20and%20Ali%20Nasiri-Sarvi%20and%20Mahdi%20S.%20Hosseini%20and%20Damien%20Garcia%20and%20Adrian%20Basarab%20and%20Mathieu%20Boily%20and%20Hassan%20Rivaz&entry.1292438233=%20%20Diffusion%20models%20for%20image%20generation%20have%20been%20a%20subject%20of%20increasing%0Ainterest%20due%20to%20their%20ability%20to%20generate%20diverse%2C%20high-quality%20images.%20Image%0Ageneration%20has%20immense%20potential%20in%20medical%20imaging%20because%20open-source%20medical%0Aimages%20are%20difficult%20to%20obtain%20compared%20to%20natural%20images%2C%20especially%20for%20rare%0Aconditions.%20The%20generated%20images%20can%20be%20used%20later%20to%20train%20classification%20and%0Asegmentation%20models.%20In%20this%20paper%2C%20we%20propose%20simulating%20realistic%20ultrasound%0A%28US%29%20images%20by%20successive%20fine-tuning%20of%20large%20diffusion%20models%20on%20different%0Apublicly%20available%20databases.%20To%20do%20so%2C%20we%20fine-tuned%20Stable%20Diffusion%2C%20a%0Astate-of-the-art%20latent%20diffusion%20model%2C%20on%20BUSI%20%28Breast%20US%20Images%29%20an%0Aultrasound%20breast%20image%20dataset.%20We%20successfully%20generated%20high-quality%20US%0Aimages%20of%20the%20breast%20using%20simple%20prompts%20that%20specify%20the%20organ%20and%20pathology%2C%0Awhich%20appeared%20realistic%20to%20three%20experienced%20US%20scientists%20and%20a%20US%0Aradiologist.%20Additionally%2C%20we%20provided%20user%20control%20by%20conditioning%20the%20model%0Awith%20segmentations%20through%20ControlNet.%20We%20will%20release%20the%20source%20code%20at%0Ahttp%3A//code.sonography.ai/%20to%20allow%20fast%20US%20image%20generation%20to%20the%20scientific%0Acommunity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08580v1&entry.124074799=Read"},
{"title": "GraphXAIN: Narratives to Explain Graph Neural Networks", "author": "Mateusz Cedro and David Martens", "abstract": "  Graph Neural Networks (GNNs) are a powerful technique for machine learning on\ngraph-structured data, yet they pose challenges in interpretability. Existing\nGNN explanation methods usually yield technical outputs, such as subgraphs and\nfeature importance scores, that are difficult for non-data scientists to\nunderstand and thereby violate the purpose of explanations. Motivated by recent\nExplainable AI (XAI) research, we propose GraphXAIN, a method that generates\nnatural language narratives explaining GNN predictions. GraphXAIN is a model-\nand explainer-agnostic method that uses Large Language Models (LLMs) to\ntranslate explanatory subgraphs and feature importance scores into coherent,\nstory-like explanations of GNN decision-making processes. Evaluations on\nreal-world datasets demonstrate GraphXAIN's ability to improve graph\nexplanations. A survey of machine learning researchers and practitioners\nreveals that GraphXAIN enhances four explainability dimensions:\nunderstandability, satisfaction, convincingness, and suitability for\ncommunicating model predictions. When combined with another graph explainer\nmethod, GraphXAIN further improves trustworthiness, insightfulness, confidence,\nand usability. Notably, 95% of participants found GraphXAIN to be a valuable\naddition to the GNN explanation method. By incorporating natural language\nnarratives, our approach serves both graph practitioners and non-expert users\nby providing clearer and more effective explanations.\n", "link": "http://arxiv.org/abs/2411.02540v3", "date": "2025-02-12", "relevancy": 1.8107, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4662}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4545}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphXAIN%3A%20Narratives%20to%20Explain%20Graph%20Neural%20Networks&body=Title%3A%20GraphXAIN%3A%20Narratives%20to%20Explain%20Graph%20Neural%20Networks%0AAuthor%3A%20Mateusz%20Cedro%20and%20David%20Martens%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20a%20powerful%20technique%20for%20machine%20learning%20on%0Agraph-structured%20data%2C%20yet%20they%20pose%20challenges%20in%20interpretability.%20Existing%0AGNN%20explanation%20methods%20usually%20yield%20technical%20outputs%2C%20such%20as%20subgraphs%20and%0Afeature%20importance%20scores%2C%20that%20are%20difficult%20for%20non-data%20scientists%20to%0Aunderstand%20and%20thereby%20violate%20the%20purpose%20of%20explanations.%20Motivated%20by%20recent%0AExplainable%20AI%20%28XAI%29%20research%2C%20we%20propose%20GraphXAIN%2C%20a%20method%20that%20generates%0Anatural%20language%20narratives%20explaining%20GNN%20predictions.%20GraphXAIN%20is%20a%20model-%0Aand%20explainer-agnostic%20method%20that%20uses%20Large%20Language%20Models%20%28LLMs%29%20to%0Atranslate%20explanatory%20subgraphs%20and%20feature%20importance%20scores%20into%20coherent%2C%0Astory-like%20explanations%20of%20GNN%20decision-making%20processes.%20Evaluations%20on%0Areal-world%20datasets%20demonstrate%20GraphXAIN%27s%20ability%20to%20improve%20graph%0Aexplanations.%20A%20survey%20of%20machine%20learning%20researchers%20and%20practitioners%0Areveals%20that%20GraphXAIN%20enhances%20four%20explainability%20dimensions%3A%0Aunderstandability%2C%20satisfaction%2C%20convincingness%2C%20and%20suitability%20for%0Acommunicating%20model%20predictions.%20When%20combined%20with%20another%20graph%20explainer%0Amethod%2C%20GraphXAIN%20further%20improves%20trustworthiness%2C%20insightfulness%2C%20confidence%2C%0Aand%20usability.%20Notably%2C%2095%25%20of%20participants%20found%20GraphXAIN%20to%20be%20a%20valuable%0Aaddition%20to%20the%20GNN%20explanation%20method.%20By%20incorporating%20natural%20language%0Anarratives%2C%20our%20approach%20serves%20both%20graph%20practitioners%20and%20non-expert%20users%0Aby%20providing%20clearer%20and%20more%20effective%20explanations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02540v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphXAIN%253A%2520Narratives%2520to%2520Explain%2520Graph%2520Neural%2520Networks%26entry.906535625%3DMateusz%2520Cedro%2520and%2520David%2520Martens%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520are%2520a%2520powerful%2520technique%2520for%2520machine%2520learning%2520on%250Agraph-structured%2520data%252C%2520yet%2520they%2520pose%2520challenges%2520in%2520interpretability.%2520Existing%250AGNN%2520explanation%2520methods%2520usually%2520yield%2520technical%2520outputs%252C%2520such%2520as%2520subgraphs%2520and%250Afeature%2520importance%2520scores%252C%2520that%2520are%2520difficult%2520for%2520non-data%2520scientists%2520to%250Aunderstand%2520and%2520thereby%2520violate%2520the%2520purpose%2520of%2520explanations.%2520Motivated%2520by%2520recent%250AExplainable%2520AI%2520%2528XAI%2529%2520research%252C%2520we%2520propose%2520GraphXAIN%252C%2520a%2520method%2520that%2520generates%250Anatural%2520language%2520narratives%2520explaining%2520GNN%2520predictions.%2520GraphXAIN%2520is%2520a%2520model-%250Aand%2520explainer-agnostic%2520method%2520that%2520uses%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%250Atranslate%2520explanatory%2520subgraphs%2520and%2520feature%2520importance%2520scores%2520into%2520coherent%252C%250Astory-like%2520explanations%2520of%2520GNN%2520decision-making%2520processes.%2520Evaluations%2520on%250Areal-world%2520datasets%2520demonstrate%2520GraphXAIN%2527s%2520ability%2520to%2520improve%2520graph%250Aexplanations.%2520A%2520survey%2520of%2520machine%2520learning%2520researchers%2520and%2520practitioners%250Areveals%2520that%2520GraphXAIN%2520enhances%2520four%2520explainability%2520dimensions%253A%250Aunderstandability%252C%2520satisfaction%252C%2520convincingness%252C%2520and%2520suitability%2520for%250Acommunicating%2520model%2520predictions.%2520When%2520combined%2520with%2520another%2520graph%2520explainer%250Amethod%252C%2520GraphXAIN%2520further%2520improves%2520trustworthiness%252C%2520insightfulness%252C%2520confidence%252C%250Aand%2520usability.%2520Notably%252C%252095%2525%2520of%2520participants%2520found%2520GraphXAIN%2520to%2520be%2520a%2520valuable%250Aaddition%2520to%2520the%2520GNN%2520explanation%2520method.%2520By%2520incorporating%2520natural%2520language%250Anarratives%252C%2520our%2520approach%2520serves%2520both%2520graph%2520practitioners%2520and%2520non-expert%2520users%250Aby%2520providing%2520clearer%2520and%2520more%2520effective%2520explanations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02540v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphXAIN%3A%20Narratives%20to%20Explain%20Graph%20Neural%20Networks&entry.906535625=Mateusz%20Cedro%20and%20David%20Martens&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20a%20powerful%20technique%20for%20machine%20learning%20on%0Agraph-structured%20data%2C%20yet%20they%20pose%20challenges%20in%20interpretability.%20Existing%0AGNN%20explanation%20methods%20usually%20yield%20technical%20outputs%2C%20such%20as%20subgraphs%20and%0Afeature%20importance%20scores%2C%20that%20are%20difficult%20for%20non-data%20scientists%20to%0Aunderstand%20and%20thereby%20violate%20the%20purpose%20of%20explanations.%20Motivated%20by%20recent%0AExplainable%20AI%20%28XAI%29%20research%2C%20we%20propose%20GraphXAIN%2C%20a%20method%20that%20generates%0Anatural%20language%20narratives%20explaining%20GNN%20predictions.%20GraphXAIN%20is%20a%20model-%0Aand%20explainer-agnostic%20method%20that%20uses%20Large%20Language%20Models%20%28LLMs%29%20to%0Atranslate%20explanatory%20subgraphs%20and%20feature%20importance%20scores%20into%20coherent%2C%0Astory-like%20explanations%20of%20GNN%20decision-making%20processes.%20Evaluations%20on%0Areal-world%20datasets%20demonstrate%20GraphXAIN%27s%20ability%20to%20improve%20graph%0Aexplanations.%20A%20survey%20of%20machine%20learning%20researchers%20and%20practitioners%0Areveals%20that%20GraphXAIN%20enhances%20four%20explainability%20dimensions%3A%0Aunderstandability%2C%20satisfaction%2C%20convincingness%2C%20and%20suitability%20for%0Acommunicating%20model%20predictions.%20When%20combined%20with%20another%20graph%20explainer%0Amethod%2C%20GraphXAIN%20further%20improves%20trustworthiness%2C%20insightfulness%2C%20confidence%2C%0Aand%20usability.%20Notably%2C%2095%25%20of%20participants%20found%20GraphXAIN%20to%20be%20a%20valuable%0Aaddition%20to%20the%20GNN%20explanation%20method.%20By%20incorporating%20natural%20language%0Anarratives%2C%20our%20approach%20serves%20both%20graph%20practitioners%20and%20non-expert%20users%0Aby%20providing%20clearer%20and%20more%20effective%20explanations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02540v3&entry.124074799=Read"},
{"title": "Truthful Aggregation of LLMs with an Application to Online Advertising", "author": "Ermis Soumalias and Michael J. Curry and Sven Seuken", "abstract": "  The next frontier of online advertising is revenue generation from\nLLM-generated content. We consider a setting where advertisers aim to influence\nthe responses of an LLM to align with their interests, while platforms seek to\nmaximize advertiser value and ensure user satisfaction. The challenge is that\nadvertisers' preferences generally conflict with those of the user, and\nadvertisers may misreport their preferences. To address this, we introduce\nMOSAIC, an auction mechanism that ensures that truthful reporting is a dominant\nstrategy for advertisers and that aligns the utility of each advertiser with\ntheir contribution to social welfare. Importantly, the mechanism operates\nwithout LLM fine-tuning or access to model weights and provably converges to\nthe output of the optimally fine-tuned LLM as computational resources increase.\nAdditionally, it can incorporate contextual information about advertisers,\nwhich significantly improves social welfare. Through experiments with a\npublicly available LLM, we show that MOSAIC leads to high advertiser value and\nplatform revenue with low computational overhead. While our motivating\napplication is online advertising, our mechanism can be applied in any setting\nwith monetary transfers, making it a general-purpose solution for truthfully\naggregating the preferences of self-interested agents over LLM-generated\nreplies.\n", "link": "http://arxiv.org/abs/2405.05905v5", "date": "2025-02-12", "relevancy": 1.7938, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4572}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4432}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4396}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Truthful%20Aggregation%20of%20LLMs%20with%20an%20Application%20to%20Online%20Advertising&body=Title%3A%20Truthful%20Aggregation%20of%20LLMs%20with%20an%20Application%20to%20Online%20Advertising%0AAuthor%3A%20Ermis%20Soumalias%20and%20Michael%20J.%20Curry%20and%20Sven%20Seuken%0AAbstract%3A%20%20%20The%20next%20frontier%20of%20online%20advertising%20is%20revenue%20generation%20from%0ALLM-generated%20content.%20We%20consider%20a%20setting%20where%20advertisers%20aim%20to%20influence%0Athe%20responses%20of%20an%20LLM%20to%20align%20with%20their%20interests%2C%20while%20platforms%20seek%20to%0Amaximize%20advertiser%20value%20and%20ensure%20user%20satisfaction.%20The%20challenge%20is%20that%0Aadvertisers%27%20preferences%20generally%20conflict%20with%20those%20of%20the%20user%2C%20and%0Aadvertisers%20may%20misreport%20their%20preferences.%20To%20address%20this%2C%20we%20introduce%0AMOSAIC%2C%20an%20auction%20mechanism%20that%20ensures%20that%20truthful%20reporting%20is%20a%20dominant%0Astrategy%20for%20advertisers%20and%20that%20aligns%20the%20utility%20of%20each%20advertiser%20with%0Atheir%20contribution%20to%20social%20welfare.%20Importantly%2C%20the%20mechanism%20operates%0Awithout%20LLM%20fine-tuning%20or%20access%20to%20model%20weights%20and%20provably%20converges%20to%0Athe%20output%20of%20the%20optimally%20fine-tuned%20LLM%20as%20computational%20resources%20increase.%0AAdditionally%2C%20it%20can%20incorporate%20contextual%20information%20about%20advertisers%2C%0Awhich%20significantly%20improves%20social%20welfare.%20Through%20experiments%20with%20a%0Apublicly%20available%20LLM%2C%20we%20show%20that%20MOSAIC%20leads%20to%20high%20advertiser%20value%20and%0Aplatform%20revenue%20with%20low%20computational%20overhead.%20While%20our%20motivating%0Aapplication%20is%20online%20advertising%2C%20our%20mechanism%20can%20be%20applied%20in%20any%20setting%0Awith%20monetary%20transfers%2C%20making%20it%20a%20general-purpose%20solution%20for%20truthfully%0Aaggregating%20the%20preferences%20of%20self-interested%20agents%20over%20LLM-generated%0Areplies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05905v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTruthful%2520Aggregation%2520of%2520LLMs%2520with%2520an%2520Application%2520to%2520Online%2520Advertising%26entry.906535625%3DErmis%2520Soumalias%2520and%2520Michael%2520J.%2520Curry%2520and%2520Sven%2520Seuken%26entry.1292438233%3D%2520%2520The%2520next%2520frontier%2520of%2520online%2520advertising%2520is%2520revenue%2520generation%2520from%250ALLM-generated%2520content.%2520We%2520consider%2520a%2520setting%2520where%2520advertisers%2520aim%2520to%2520influence%250Athe%2520responses%2520of%2520an%2520LLM%2520to%2520align%2520with%2520their%2520interests%252C%2520while%2520platforms%2520seek%2520to%250Amaximize%2520advertiser%2520value%2520and%2520ensure%2520user%2520satisfaction.%2520The%2520challenge%2520is%2520that%250Aadvertisers%2527%2520preferences%2520generally%2520conflict%2520with%2520those%2520of%2520the%2520user%252C%2520and%250Aadvertisers%2520may%2520misreport%2520their%2520preferences.%2520To%2520address%2520this%252C%2520we%2520introduce%250AMOSAIC%252C%2520an%2520auction%2520mechanism%2520that%2520ensures%2520that%2520truthful%2520reporting%2520is%2520a%2520dominant%250Astrategy%2520for%2520advertisers%2520and%2520that%2520aligns%2520the%2520utility%2520of%2520each%2520advertiser%2520with%250Atheir%2520contribution%2520to%2520social%2520welfare.%2520Importantly%252C%2520the%2520mechanism%2520operates%250Awithout%2520LLM%2520fine-tuning%2520or%2520access%2520to%2520model%2520weights%2520and%2520provably%2520converges%2520to%250Athe%2520output%2520of%2520the%2520optimally%2520fine-tuned%2520LLM%2520as%2520computational%2520resources%2520increase.%250AAdditionally%252C%2520it%2520can%2520incorporate%2520contextual%2520information%2520about%2520advertisers%252C%250Awhich%2520significantly%2520improves%2520social%2520welfare.%2520Through%2520experiments%2520with%2520a%250Apublicly%2520available%2520LLM%252C%2520we%2520show%2520that%2520MOSAIC%2520leads%2520to%2520high%2520advertiser%2520value%2520and%250Aplatform%2520revenue%2520with%2520low%2520computational%2520overhead.%2520While%2520our%2520motivating%250Aapplication%2520is%2520online%2520advertising%252C%2520our%2520mechanism%2520can%2520be%2520applied%2520in%2520any%2520setting%250Awith%2520monetary%2520transfers%252C%2520making%2520it%2520a%2520general-purpose%2520solution%2520for%2520truthfully%250Aaggregating%2520the%2520preferences%2520of%2520self-interested%2520agents%2520over%2520LLM-generated%250Areplies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05905v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Truthful%20Aggregation%20of%20LLMs%20with%20an%20Application%20to%20Online%20Advertising&entry.906535625=Ermis%20Soumalias%20and%20Michael%20J.%20Curry%20and%20Sven%20Seuken&entry.1292438233=%20%20The%20next%20frontier%20of%20online%20advertising%20is%20revenue%20generation%20from%0ALLM-generated%20content.%20We%20consider%20a%20setting%20where%20advertisers%20aim%20to%20influence%0Athe%20responses%20of%20an%20LLM%20to%20align%20with%20their%20interests%2C%20while%20platforms%20seek%20to%0Amaximize%20advertiser%20value%20and%20ensure%20user%20satisfaction.%20The%20challenge%20is%20that%0Aadvertisers%27%20preferences%20generally%20conflict%20with%20those%20of%20the%20user%2C%20and%0Aadvertisers%20may%20misreport%20their%20preferences.%20To%20address%20this%2C%20we%20introduce%0AMOSAIC%2C%20an%20auction%20mechanism%20that%20ensures%20that%20truthful%20reporting%20is%20a%20dominant%0Astrategy%20for%20advertisers%20and%20that%20aligns%20the%20utility%20of%20each%20advertiser%20with%0Atheir%20contribution%20to%20social%20welfare.%20Importantly%2C%20the%20mechanism%20operates%0Awithout%20LLM%20fine-tuning%20or%20access%20to%20model%20weights%20and%20provably%20converges%20to%0Athe%20output%20of%20the%20optimally%20fine-tuned%20LLM%20as%20computational%20resources%20increase.%0AAdditionally%2C%20it%20can%20incorporate%20contextual%20information%20about%20advertisers%2C%0Awhich%20significantly%20improves%20social%20welfare.%20Through%20experiments%20with%20a%0Apublicly%20available%20LLM%2C%20we%20show%20that%20MOSAIC%20leads%20to%20high%20advertiser%20value%20and%0Aplatform%20revenue%20with%20low%20computational%20overhead.%20While%20our%20motivating%0Aapplication%20is%20online%20advertising%2C%20our%20mechanism%20can%20be%20applied%20in%20any%20setting%0Awith%20monetary%20transfers%2C%20making%20it%20a%20general-purpose%20solution%20for%20truthfully%0Aaggregating%20the%20preferences%20of%20self-interested%20agents%20over%20LLM-generated%0Areplies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05905v5&entry.124074799=Read"},
{"title": "Forecasting Drought Using Machine Learning in California", "author": "Nan K. Li and Angela Chang and David Sherman", "abstract": "  Drought is a frequent and costly natural disaster in California, with major\nnegative impacts on agricultural production and water resource availability,\nparticularly groundwater. This study investigated the performance of applying\ndifferent machine learning approaches to predicting the U.S. Drought Monitor\nclassification in California. Four approaches were used: a convolutional neural\nnetwork (CNN), random forest, XGBoost, and long short term memory (LSTM)\nrecurrent neural network, and compared to a baseline persistence model. We\nevaluated the models' performance in predicting severe drought (USDM drought\ncategory D2 or higher) using a macro F1 binary classification metric. The LSTM\nmodel emerged as the top performer, followed by XGBoost, CNN, and random\nforest. Further evaluation of our results at the county level suggested that\nthe LSTM model would perform best in counties with more consistent drought\npatterns and where severe drought was more common, and the LSTM model would\nperform worse where drought scores increased rapidly. Utilizing 30 weeks of\nhistorical data, the LSTM model successfully forecasted drought scores for a\n12-week period with a Mean Absolute Error (MAE) of 0.33, equivalent to less\nthan half a drought category on a scale of 0 to 5. Additionally, the LSTM\nachieved a macro F1 score of 0.9, indicating high accuracy in binary\nclassification for severe drought conditions. Evaluation of different window\nand future horizon sizes in weeks suggested that at least 24 weeks of data\nwould result in the best performance, with best performance for shorter horizon\nsizes, particularly less than eight weeks.\n", "link": "http://arxiv.org/abs/2502.08622v1", "date": "2025-02-12", "relevancy": 1.7863, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4697}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4485}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forecasting%20Drought%20Using%20Machine%20Learning%20in%20California&body=Title%3A%20Forecasting%20Drought%20Using%20Machine%20Learning%20in%20California%0AAuthor%3A%20Nan%20K.%20Li%20and%20Angela%20Chang%20and%20David%20Sherman%0AAbstract%3A%20%20%20Drought%20is%20a%20frequent%20and%20costly%20natural%20disaster%20in%20California%2C%20with%20major%0Anegative%20impacts%20on%20agricultural%20production%20and%20water%20resource%20availability%2C%0Aparticularly%20groundwater.%20This%20study%20investigated%20the%20performance%20of%20applying%0Adifferent%20machine%20learning%20approaches%20to%20predicting%20the%20U.S.%20Drought%20Monitor%0Aclassification%20in%20California.%20Four%20approaches%20were%20used%3A%20a%20convolutional%20neural%0Anetwork%20%28CNN%29%2C%20random%20forest%2C%20XGBoost%2C%20and%20long%20short%20term%20memory%20%28LSTM%29%0Arecurrent%20neural%20network%2C%20and%20compared%20to%20a%20baseline%20persistence%20model.%20We%0Aevaluated%20the%20models%27%20performance%20in%20predicting%20severe%20drought%20%28USDM%20drought%0Acategory%20D2%20or%20higher%29%20using%20a%20macro%20F1%20binary%20classification%20metric.%20The%20LSTM%0Amodel%20emerged%20as%20the%20top%20performer%2C%20followed%20by%20XGBoost%2C%20CNN%2C%20and%20random%0Aforest.%20Further%20evaluation%20of%20our%20results%20at%20the%20county%20level%20suggested%20that%0Athe%20LSTM%20model%20would%20perform%20best%20in%20counties%20with%20more%20consistent%20drought%0Apatterns%20and%20where%20severe%20drought%20was%20more%20common%2C%20and%20the%20LSTM%20model%20would%0Aperform%20worse%20where%20drought%20scores%20increased%20rapidly.%20Utilizing%2030%20weeks%20of%0Ahistorical%20data%2C%20the%20LSTM%20model%20successfully%20forecasted%20drought%20scores%20for%20a%0A12-week%20period%20with%20a%20Mean%20Absolute%20Error%20%28MAE%29%20of%200.33%2C%20equivalent%20to%20less%0Athan%20half%20a%20drought%20category%20on%20a%20scale%20of%200%20to%205.%20Additionally%2C%20the%20LSTM%0Aachieved%20a%20macro%20F1%20score%20of%200.9%2C%20indicating%20high%20accuracy%20in%20binary%0Aclassification%20for%20severe%20drought%20conditions.%20Evaluation%20of%20different%20window%0Aand%20future%20horizon%20sizes%20in%20weeks%20suggested%20that%20at%20least%2024%20weeks%20of%20data%0Awould%20result%20in%20the%20best%20performance%2C%20with%20best%20performance%20for%20shorter%20horizon%0Asizes%2C%20particularly%20less%20than%20eight%20weeks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08622v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForecasting%2520Drought%2520Using%2520Machine%2520Learning%2520in%2520California%26entry.906535625%3DNan%2520K.%2520Li%2520and%2520Angela%2520Chang%2520and%2520David%2520Sherman%26entry.1292438233%3D%2520%2520Drought%2520is%2520a%2520frequent%2520and%2520costly%2520natural%2520disaster%2520in%2520California%252C%2520with%2520major%250Anegative%2520impacts%2520on%2520agricultural%2520production%2520and%2520water%2520resource%2520availability%252C%250Aparticularly%2520groundwater.%2520This%2520study%2520investigated%2520the%2520performance%2520of%2520applying%250Adifferent%2520machine%2520learning%2520approaches%2520to%2520predicting%2520the%2520U.S.%2520Drought%2520Monitor%250Aclassification%2520in%2520California.%2520Four%2520approaches%2520were%2520used%253A%2520a%2520convolutional%2520neural%250Anetwork%2520%2528CNN%2529%252C%2520random%2520forest%252C%2520XGBoost%252C%2520and%2520long%2520short%2520term%2520memory%2520%2528LSTM%2529%250Arecurrent%2520neural%2520network%252C%2520and%2520compared%2520to%2520a%2520baseline%2520persistence%2520model.%2520We%250Aevaluated%2520the%2520models%2527%2520performance%2520in%2520predicting%2520severe%2520drought%2520%2528USDM%2520drought%250Acategory%2520D2%2520or%2520higher%2529%2520using%2520a%2520macro%2520F1%2520binary%2520classification%2520metric.%2520The%2520LSTM%250Amodel%2520emerged%2520as%2520the%2520top%2520performer%252C%2520followed%2520by%2520XGBoost%252C%2520CNN%252C%2520and%2520random%250Aforest.%2520Further%2520evaluation%2520of%2520our%2520results%2520at%2520the%2520county%2520level%2520suggested%2520that%250Athe%2520LSTM%2520model%2520would%2520perform%2520best%2520in%2520counties%2520with%2520more%2520consistent%2520drought%250Apatterns%2520and%2520where%2520severe%2520drought%2520was%2520more%2520common%252C%2520and%2520the%2520LSTM%2520model%2520would%250Aperform%2520worse%2520where%2520drought%2520scores%2520increased%2520rapidly.%2520Utilizing%252030%2520weeks%2520of%250Ahistorical%2520data%252C%2520the%2520LSTM%2520model%2520successfully%2520forecasted%2520drought%2520scores%2520for%2520a%250A12-week%2520period%2520with%2520a%2520Mean%2520Absolute%2520Error%2520%2528MAE%2529%2520of%25200.33%252C%2520equivalent%2520to%2520less%250Athan%2520half%2520a%2520drought%2520category%2520on%2520a%2520scale%2520of%25200%2520to%25205.%2520Additionally%252C%2520the%2520LSTM%250Aachieved%2520a%2520macro%2520F1%2520score%2520of%25200.9%252C%2520indicating%2520high%2520accuracy%2520in%2520binary%250Aclassification%2520for%2520severe%2520drought%2520conditions.%2520Evaluation%2520of%2520different%2520window%250Aand%2520future%2520horizon%2520sizes%2520in%2520weeks%2520suggested%2520that%2520at%2520least%252024%2520weeks%2520of%2520data%250Awould%2520result%2520in%2520the%2520best%2520performance%252C%2520with%2520best%2520performance%2520for%2520shorter%2520horizon%250Asizes%252C%2520particularly%2520less%2520than%2520eight%2520weeks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08622v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forecasting%20Drought%20Using%20Machine%20Learning%20in%20California&entry.906535625=Nan%20K.%20Li%20and%20Angela%20Chang%20and%20David%20Sherman&entry.1292438233=%20%20Drought%20is%20a%20frequent%20and%20costly%20natural%20disaster%20in%20California%2C%20with%20major%0Anegative%20impacts%20on%20agricultural%20production%20and%20water%20resource%20availability%2C%0Aparticularly%20groundwater.%20This%20study%20investigated%20the%20performance%20of%20applying%0Adifferent%20machine%20learning%20approaches%20to%20predicting%20the%20U.S.%20Drought%20Monitor%0Aclassification%20in%20California.%20Four%20approaches%20were%20used%3A%20a%20convolutional%20neural%0Anetwork%20%28CNN%29%2C%20random%20forest%2C%20XGBoost%2C%20and%20long%20short%20term%20memory%20%28LSTM%29%0Arecurrent%20neural%20network%2C%20and%20compared%20to%20a%20baseline%20persistence%20model.%20We%0Aevaluated%20the%20models%27%20performance%20in%20predicting%20severe%20drought%20%28USDM%20drought%0Acategory%20D2%20or%20higher%29%20using%20a%20macro%20F1%20binary%20classification%20metric.%20The%20LSTM%0Amodel%20emerged%20as%20the%20top%20performer%2C%20followed%20by%20XGBoost%2C%20CNN%2C%20and%20random%0Aforest.%20Further%20evaluation%20of%20our%20results%20at%20the%20county%20level%20suggested%20that%0Athe%20LSTM%20model%20would%20perform%20best%20in%20counties%20with%20more%20consistent%20drought%0Apatterns%20and%20where%20severe%20drought%20was%20more%20common%2C%20and%20the%20LSTM%20model%20would%0Aperform%20worse%20where%20drought%20scores%20increased%20rapidly.%20Utilizing%2030%20weeks%20of%0Ahistorical%20data%2C%20the%20LSTM%20model%20successfully%20forecasted%20drought%20scores%20for%20a%0A12-week%20period%20with%20a%20Mean%20Absolute%20Error%20%28MAE%29%20of%200.33%2C%20equivalent%20to%20less%0Athan%20half%20a%20drought%20category%20on%20a%20scale%20of%200%20to%205.%20Additionally%2C%20the%20LSTM%0Aachieved%20a%20macro%20F1%20score%20of%200.9%2C%20indicating%20high%20accuracy%20in%20binary%0Aclassification%20for%20severe%20drought%20conditions.%20Evaluation%20of%20different%20window%0Aand%20future%20horizon%20sizes%20in%20weeks%20suggested%20that%20at%20least%2024%20weeks%20of%20data%0Awould%20result%20in%20the%20best%20performance%2C%20with%20best%20performance%20for%20shorter%20horizon%0Asizes%2C%20particularly%20less%20than%20eight%20weeks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08622v1&entry.124074799=Read"},
{"title": "Randomness of Low-Layer Parameters Determines Confusing Samples in Terms\n  of Interaction Representations of a DNN", "author": "Junpeng Zhang and Lei Cheng and Qing Li and Liang Lin and Quanshi Zhang", "abstract": "  In this paper, we find that the complexity of interactions encoded by a deep\nneural network (DNN) can explain its generalization power. We also discover\nthat the confusing samples of a DNN, which are represented by non-generalizable\ninteractions, are determined by its low-layer parameters. In comparison, other\nfactors, such as high-layer parameters and network architecture, have much less\nimpact on the composition of confusing samples. Two DNNs with different\nlow-layer parameters usually have fully different sets of confusing samples,\neven though they have similar performance. This finding extends the\nunderstanding of the lottery ticket hypothesis, and well explains distinctive\nrepresentation power of different DNNs.\n", "link": "http://arxiv.org/abs/2502.08625v1", "date": "2025-02-12", "relevancy": 1.7679, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4453}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4438}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Randomness%20of%20Low-Layer%20Parameters%20Determines%20Confusing%20Samples%20in%20Terms%0A%20%20of%20Interaction%20Representations%20of%20a%20DNN&body=Title%3A%20Randomness%20of%20Low-Layer%20Parameters%20Determines%20Confusing%20Samples%20in%20Terms%0A%20%20of%20Interaction%20Representations%20of%20a%20DNN%0AAuthor%3A%20Junpeng%20Zhang%20and%20Lei%20Cheng%20and%20Qing%20Li%20and%20Liang%20Lin%20and%20Quanshi%20Zhang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20find%20that%20the%20complexity%20of%20interactions%20encoded%20by%20a%20deep%0Aneural%20network%20%28DNN%29%20can%20explain%20its%20generalization%20power.%20We%20also%20discover%0Athat%20the%20confusing%20samples%20of%20a%20DNN%2C%20which%20are%20represented%20by%20non-generalizable%0Ainteractions%2C%20are%20determined%20by%20its%20low-layer%20parameters.%20In%20comparison%2C%20other%0Afactors%2C%20such%20as%20high-layer%20parameters%20and%20network%20architecture%2C%20have%20much%20less%0Aimpact%20on%20the%20composition%20of%20confusing%20samples.%20Two%20DNNs%20with%20different%0Alow-layer%20parameters%20usually%20have%20fully%20different%20sets%20of%20confusing%20samples%2C%0Aeven%20though%20they%20have%20similar%20performance.%20This%20finding%20extends%20the%0Aunderstanding%20of%20the%20lottery%20ticket%20hypothesis%2C%20and%20well%20explains%20distinctive%0Arepresentation%20power%20of%20different%20DNNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08625v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRandomness%2520of%2520Low-Layer%2520Parameters%2520Determines%2520Confusing%2520Samples%2520in%2520Terms%250A%2520%2520of%2520Interaction%2520Representations%2520of%2520a%2520DNN%26entry.906535625%3DJunpeng%2520Zhang%2520and%2520Lei%2520Cheng%2520and%2520Qing%2520Li%2520and%2520Liang%2520Lin%2520and%2520Quanshi%2520Zhang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520find%2520that%2520the%2520complexity%2520of%2520interactions%2520encoded%2520by%2520a%2520deep%250Aneural%2520network%2520%2528DNN%2529%2520can%2520explain%2520its%2520generalization%2520power.%2520We%2520also%2520discover%250Athat%2520the%2520confusing%2520samples%2520of%2520a%2520DNN%252C%2520which%2520are%2520represented%2520by%2520non-generalizable%250Ainteractions%252C%2520are%2520determined%2520by%2520its%2520low-layer%2520parameters.%2520In%2520comparison%252C%2520other%250Afactors%252C%2520such%2520as%2520high-layer%2520parameters%2520and%2520network%2520architecture%252C%2520have%2520much%2520less%250Aimpact%2520on%2520the%2520composition%2520of%2520confusing%2520samples.%2520Two%2520DNNs%2520with%2520different%250Alow-layer%2520parameters%2520usually%2520have%2520fully%2520different%2520sets%2520of%2520confusing%2520samples%252C%250Aeven%2520though%2520they%2520have%2520similar%2520performance.%2520This%2520finding%2520extends%2520the%250Aunderstanding%2520of%2520the%2520lottery%2520ticket%2520hypothesis%252C%2520and%2520well%2520explains%2520distinctive%250Arepresentation%2520power%2520of%2520different%2520DNNs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08625v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Randomness%20of%20Low-Layer%20Parameters%20Determines%20Confusing%20Samples%20in%20Terms%0A%20%20of%20Interaction%20Representations%20of%20a%20DNN&entry.906535625=Junpeng%20Zhang%20and%20Lei%20Cheng%20and%20Qing%20Li%20and%20Liang%20Lin%20and%20Quanshi%20Zhang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20find%20that%20the%20complexity%20of%20interactions%20encoded%20by%20a%20deep%0Aneural%20network%20%28DNN%29%20can%20explain%20its%20generalization%20power.%20We%20also%20discover%0Athat%20the%20confusing%20samples%20of%20a%20DNN%2C%20which%20are%20represented%20by%20non-generalizable%0Ainteractions%2C%20are%20determined%20by%20its%20low-layer%20parameters.%20In%20comparison%2C%20other%0Afactors%2C%20such%20as%20high-layer%20parameters%20and%20network%20architecture%2C%20have%20much%20less%0Aimpact%20on%20the%20composition%20of%20confusing%20samples.%20Two%20DNNs%20with%20different%0Alow-layer%20parameters%20usually%20have%20fully%20different%20sets%20of%20confusing%20samples%2C%0Aeven%20though%20they%20have%20similar%20performance.%20This%20finding%20extends%20the%0Aunderstanding%20of%20the%20lottery%20ticket%20hypothesis%2C%20and%20well%20explains%20distinctive%0Arepresentation%20power%20of%20different%20DNNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08625v1&entry.124074799=Read"},
{"title": "Learning in Markets with Heterogeneous Agents: Dynamics and Survival of\n  Bayesian vs. No-Regret Learners", "author": "David Easley and Yoav Kolumbus and Eva Tardos", "abstract": "  We analyze the performance of heterogeneous learning agents in asset markets\nwith stochastic payoffs. Our agents aim to maximize the expected growth rate of\ntheir wealth but have different theories on how to learn this best. We focus on\ncomparing Bayesian and no-regret learners in market dynamics. Bayesian learners\nwith a prior over a finite set of models that assign positive prior probability\nto the correct model have posterior probabilities that converge exponentially\nto the correct model. Consequently, they survive even in the presence of agents\nwho invest according to the correct model of the stochastic process. Bayesians\nwith a continuum prior converge to the correct model at a rate of $O((\\log\nT)/T)$. Online learning theory provides no-regret algorithms for maximizing the\nlog of wealth in this setting, achieving a worst-case regret bound of $O(\\log\nT)$ without assuming a steady underlying stochastic process but comparing to\nthe best fixed investment rule. This regret, as we observe, is of the same\norder of magnitude as that of a Bayesian learner with a continuum prior.\nHowever, we show that even such low regret may not be sufficient for survival\nin asset markets: an agent can have regret as low as $O(\\log T)$, but still\nvanish in market dynamics when competing against agents who invest according to\nthe correct model or even against a perfect Bayesian with a finite prior. On\nthe other hand, we show that Bayesian learning is fragile, while no-regret\nlearning requires less knowledge of the environment and is therefore more\nrobust. Any no-regret learner will drive out of the market an imperfect\nBayesian whose finite prior or update rule has even small errors. We formally\nestablish the relationship between notions of survival, vanishing, and market\ndomination studied in economics and the framework of regret minimization, thus\nbridging these theories.\n", "link": "http://arxiv.org/abs/2502.08597v1", "date": "2025-02-12", "relevancy": 1.7567, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4654}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4354}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20in%20Markets%20with%20Heterogeneous%20Agents%3A%20Dynamics%20and%20Survival%20of%0A%20%20Bayesian%20vs.%20No-Regret%20Learners&body=Title%3A%20Learning%20in%20Markets%20with%20Heterogeneous%20Agents%3A%20Dynamics%20and%20Survival%20of%0A%20%20Bayesian%20vs.%20No-Regret%20Learners%0AAuthor%3A%20David%20Easley%20and%20Yoav%20Kolumbus%20and%20Eva%20Tardos%0AAbstract%3A%20%20%20We%20analyze%20the%20performance%20of%20heterogeneous%20learning%20agents%20in%20asset%20markets%0Awith%20stochastic%20payoffs.%20Our%20agents%20aim%20to%20maximize%20the%20expected%20growth%20rate%20of%0Atheir%20wealth%20but%20have%20different%20theories%20on%20how%20to%20learn%20this%20best.%20We%20focus%20on%0Acomparing%20Bayesian%20and%20no-regret%20learners%20in%20market%20dynamics.%20Bayesian%20learners%0Awith%20a%20prior%20over%20a%20finite%20set%20of%20models%20that%20assign%20positive%20prior%20probability%0Ato%20the%20correct%20model%20have%20posterior%20probabilities%20that%20converge%20exponentially%0Ato%20the%20correct%20model.%20Consequently%2C%20they%20survive%20even%20in%20the%20presence%20of%20agents%0Awho%20invest%20according%20to%20the%20correct%20model%20of%20the%20stochastic%20process.%20Bayesians%0Awith%20a%20continuum%20prior%20converge%20to%20the%20correct%20model%20at%20a%20rate%20of%20%24O%28%28%5Clog%0AT%29/T%29%24.%20Online%20learning%20theory%20provides%20no-regret%20algorithms%20for%20maximizing%20the%0Alog%20of%20wealth%20in%20this%20setting%2C%20achieving%20a%20worst-case%20regret%20bound%20of%20%24O%28%5Clog%0AT%29%24%20without%20assuming%20a%20steady%20underlying%20stochastic%20process%20but%20comparing%20to%0Athe%20best%20fixed%20investment%20rule.%20This%20regret%2C%20as%20we%20observe%2C%20is%20of%20the%20same%0Aorder%20of%20magnitude%20as%20that%20of%20a%20Bayesian%20learner%20with%20a%20continuum%20prior.%0AHowever%2C%20we%20show%20that%20even%20such%20low%20regret%20may%20not%20be%20sufficient%20for%20survival%0Ain%20asset%20markets%3A%20an%20agent%20can%20have%20regret%20as%20low%20as%20%24O%28%5Clog%20T%29%24%2C%20but%20still%0Avanish%20in%20market%20dynamics%20when%20competing%20against%20agents%20who%20invest%20according%20to%0Athe%20correct%20model%20or%20even%20against%20a%20perfect%20Bayesian%20with%20a%20finite%20prior.%20On%0Athe%20other%20hand%2C%20we%20show%20that%20Bayesian%20learning%20is%20fragile%2C%20while%20no-regret%0Alearning%20requires%20less%20knowledge%20of%20the%20environment%20and%20is%20therefore%20more%0Arobust.%20Any%20no-regret%20learner%20will%20drive%20out%20of%20the%20market%20an%20imperfect%0ABayesian%20whose%20finite%20prior%20or%20update%20rule%20has%20even%20small%20errors.%20We%20formally%0Aestablish%20the%20relationship%20between%20notions%20of%20survival%2C%20vanishing%2C%20and%20market%0Adomination%20studied%20in%20economics%20and%20the%20framework%20of%20regret%20minimization%2C%20thus%0Abridging%20these%20theories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08597v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520in%2520Markets%2520with%2520Heterogeneous%2520Agents%253A%2520Dynamics%2520and%2520Survival%2520of%250A%2520%2520Bayesian%2520vs.%2520No-Regret%2520Learners%26entry.906535625%3DDavid%2520Easley%2520and%2520Yoav%2520Kolumbus%2520and%2520Eva%2520Tardos%26entry.1292438233%3D%2520%2520We%2520analyze%2520the%2520performance%2520of%2520heterogeneous%2520learning%2520agents%2520in%2520asset%2520markets%250Awith%2520stochastic%2520payoffs.%2520Our%2520agents%2520aim%2520to%2520maximize%2520the%2520expected%2520growth%2520rate%2520of%250Atheir%2520wealth%2520but%2520have%2520different%2520theories%2520on%2520how%2520to%2520learn%2520this%2520best.%2520We%2520focus%2520on%250Acomparing%2520Bayesian%2520and%2520no-regret%2520learners%2520in%2520market%2520dynamics.%2520Bayesian%2520learners%250Awith%2520a%2520prior%2520over%2520a%2520finite%2520set%2520of%2520models%2520that%2520assign%2520positive%2520prior%2520probability%250Ato%2520the%2520correct%2520model%2520have%2520posterior%2520probabilities%2520that%2520converge%2520exponentially%250Ato%2520the%2520correct%2520model.%2520Consequently%252C%2520they%2520survive%2520even%2520in%2520the%2520presence%2520of%2520agents%250Awho%2520invest%2520according%2520to%2520the%2520correct%2520model%2520of%2520the%2520stochastic%2520process.%2520Bayesians%250Awith%2520a%2520continuum%2520prior%2520converge%2520to%2520the%2520correct%2520model%2520at%2520a%2520rate%2520of%2520%2524O%2528%2528%255Clog%250AT%2529/T%2529%2524.%2520Online%2520learning%2520theory%2520provides%2520no-regret%2520algorithms%2520for%2520maximizing%2520the%250Alog%2520of%2520wealth%2520in%2520this%2520setting%252C%2520achieving%2520a%2520worst-case%2520regret%2520bound%2520of%2520%2524O%2528%255Clog%250AT%2529%2524%2520without%2520assuming%2520a%2520steady%2520underlying%2520stochastic%2520process%2520but%2520comparing%2520to%250Athe%2520best%2520fixed%2520investment%2520rule.%2520This%2520regret%252C%2520as%2520we%2520observe%252C%2520is%2520of%2520the%2520same%250Aorder%2520of%2520magnitude%2520as%2520that%2520of%2520a%2520Bayesian%2520learner%2520with%2520a%2520continuum%2520prior.%250AHowever%252C%2520we%2520show%2520that%2520even%2520such%2520low%2520regret%2520may%2520not%2520be%2520sufficient%2520for%2520survival%250Ain%2520asset%2520markets%253A%2520an%2520agent%2520can%2520have%2520regret%2520as%2520low%2520as%2520%2524O%2528%255Clog%2520T%2529%2524%252C%2520but%2520still%250Avanish%2520in%2520market%2520dynamics%2520when%2520competing%2520against%2520agents%2520who%2520invest%2520according%2520to%250Athe%2520correct%2520model%2520or%2520even%2520against%2520a%2520perfect%2520Bayesian%2520with%2520a%2520finite%2520prior.%2520On%250Athe%2520other%2520hand%252C%2520we%2520show%2520that%2520Bayesian%2520learning%2520is%2520fragile%252C%2520while%2520no-regret%250Alearning%2520requires%2520less%2520knowledge%2520of%2520the%2520environment%2520and%2520is%2520therefore%2520more%250Arobust.%2520Any%2520no-regret%2520learner%2520will%2520drive%2520out%2520of%2520the%2520market%2520an%2520imperfect%250ABayesian%2520whose%2520finite%2520prior%2520or%2520update%2520rule%2520has%2520even%2520small%2520errors.%2520We%2520formally%250Aestablish%2520the%2520relationship%2520between%2520notions%2520of%2520survival%252C%2520vanishing%252C%2520and%2520market%250Adomination%2520studied%2520in%2520economics%2520and%2520the%2520framework%2520of%2520regret%2520minimization%252C%2520thus%250Abridging%2520these%2520theories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08597v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20in%20Markets%20with%20Heterogeneous%20Agents%3A%20Dynamics%20and%20Survival%20of%0A%20%20Bayesian%20vs.%20No-Regret%20Learners&entry.906535625=David%20Easley%20and%20Yoav%20Kolumbus%20and%20Eva%20Tardos&entry.1292438233=%20%20We%20analyze%20the%20performance%20of%20heterogeneous%20learning%20agents%20in%20asset%20markets%0Awith%20stochastic%20payoffs.%20Our%20agents%20aim%20to%20maximize%20the%20expected%20growth%20rate%20of%0Atheir%20wealth%20but%20have%20different%20theories%20on%20how%20to%20learn%20this%20best.%20We%20focus%20on%0Acomparing%20Bayesian%20and%20no-regret%20learners%20in%20market%20dynamics.%20Bayesian%20learners%0Awith%20a%20prior%20over%20a%20finite%20set%20of%20models%20that%20assign%20positive%20prior%20probability%0Ato%20the%20correct%20model%20have%20posterior%20probabilities%20that%20converge%20exponentially%0Ato%20the%20correct%20model.%20Consequently%2C%20they%20survive%20even%20in%20the%20presence%20of%20agents%0Awho%20invest%20according%20to%20the%20correct%20model%20of%20the%20stochastic%20process.%20Bayesians%0Awith%20a%20continuum%20prior%20converge%20to%20the%20correct%20model%20at%20a%20rate%20of%20%24O%28%28%5Clog%0AT%29/T%29%24.%20Online%20learning%20theory%20provides%20no-regret%20algorithms%20for%20maximizing%20the%0Alog%20of%20wealth%20in%20this%20setting%2C%20achieving%20a%20worst-case%20regret%20bound%20of%20%24O%28%5Clog%0AT%29%24%20without%20assuming%20a%20steady%20underlying%20stochastic%20process%20but%20comparing%20to%0Athe%20best%20fixed%20investment%20rule.%20This%20regret%2C%20as%20we%20observe%2C%20is%20of%20the%20same%0Aorder%20of%20magnitude%20as%20that%20of%20a%20Bayesian%20learner%20with%20a%20continuum%20prior.%0AHowever%2C%20we%20show%20that%20even%20such%20low%20regret%20may%20not%20be%20sufficient%20for%20survival%0Ain%20asset%20markets%3A%20an%20agent%20can%20have%20regret%20as%20low%20as%20%24O%28%5Clog%20T%29%24%2C%20but%20still%0Avanish%20in%20market%20dynamics%20when%20competing%20against%20agents%20who%20invest%20according%20to%0Athe%20correct%20model%20or%20even%20against%20a%20perfect%20Bayesian%20with%20a%20finite%20prior.%20On%0Athe%20other%20hand%2C%20we%20show%20that%20Bayesian%20learning%20is%20fragile%2C%20while%20no-regret%0Alearning%20requires%20less%20knowledge%20of%20the%20environment%20and%20is%20therefore%20more%0Arobust.%20Any%20no-regret%20learner%20will%20drive%20out%20of%20the%20market%20an%20imperfect%0ABayesian%20whose%20finite%20prior%20or%20update%20rule%20has%20even%20small%20errors.%20We%20formally%0Aestablish%20the%20relationship%20between%20notions%20of%20survival%2C%20vanishing%2C%20and%20market%0Adomination%20studied%20in%20economics%20and%20the%20framework%20of%20regret%20minimization%2C%20thus%0Abridging%20these%20theories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08597v1&entry.124074799=Read"},
{"title": "Continuous Cardiac Arrest Prediction in ICU using PPG Foundation Model", "author": "Saurabh Kataria and Ran Xiao and Timothy Ruchti and Matthew Clark and Jiaying Lu and Randall J. Lee and Jocelyn Grunwell and Xiao Hu", "abstract": "  Non-invasive patient monitoring for tracking and predicting adverse acute\nhealth events is an emerging area of research. We pursue in-hospital cardiac\narrest (IHCA) prediction using only single-channel finger photoplethysmography\n(PPG) signals. Our proposed two-stage model Feature Extractor-Aggregator\nNetwork (FEAN) leverages powerful representations from pre-trained PPG\nfoundation models (PPG-GPT of size up to 1 Billion) stacked with sequential\nclassification models. We propose two FEAN variants (\"1H\", \"FH\") which use the\nlatest one-hour and (max) 24-hour history to make decisions respectively. Our\nstudy is the first to present IHCA prediction results in ICU patients using\nonly unimodal (continuous PPG signal) waveform deep representations. With our\nbest model, we obtain an average of 0.79 AUROC over 24~h prediction window\nbefore CA event onset with our model peaking performance at 0.82 one hour\nbefore CA. We also provide a comprehensive analysis of our model through\narchitectural tuning and PaCMAP visualization of patient health trajectory in\nlatent space.\n", "link": "http://arxiv.org/abs/2502.08612v1", "date": "2025-02-12", "relevancy": 1.7529, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4392}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4385}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continuous%20Cardiac%20Arrest%20Prediction%20in%20ICU%20using%20PPG%20Foundation%20Model&body=Title%3A%20Continuous%20Cardiac%20Arrest%20Prediction%20in%20ICU%20using%20PPG%20Foundation%20Model%0AAuthor%3A%20Saurabh%20Kataria%20and%20Ran%20Xiao%20and%20Timothy%20Ruchti%20and%20Matthew%20Clark%20and%20Jiaying%20Lu%20and%20Randall%20J.%20Lee%20and%20Jocelyn%20Grunwell%20and%20Xiao%20Hu%0AAbstract%3A%20%20%20Non-invasive%20patient%20monitoring%20for%20tracking%20and%20predicting%20adverse%20acute%0Ahealth%20events%20is%20an%20emerging%20area%20of%20research.%20We%20pursue%20in-hospital%20cardiac%0Aarrest%20%28IHCA%29%20prediction%20using%20only%20single-channel%20finger%20photoplethysmography%0A%28PPG%29%20signals.%20Our%20proposed%20two-stage%20model%20Feature%20Extractor-Aggregator%0ANetwork%20%28FEAN%29%20leverages%20powerful%20representations%20from%20pre-trained%20PPG%0Afoundation%20models%20%28PPG-GPT%20of%20size%20up%20to%201%20Billion%29%20stacked%20with%20sequential%0Aclassification%20models.%20We%20propose%20two%20FEAN%20variants%20%28%221H%22%2C%20%22FH%22%29%20which%20use%20the%0Alatest%20one-hour%20and%20%28max%29%2024-hour%20history%20to%20make%20decisions%20respectively.%20Our%0Astudy%20is%20the%20first%20to%20present%20IHCA%20prediction%20results%20in%20ICU%20patients%20using%0Aonly%20unimodal%20%28continuous%20PPG%20signal%29%20waveform%20deep%20representations.%20With%20our%0Abest%20model%2C%20we%20obtain%20an%20average%20of%200.79%20AUROC%20over%2024~h%20prediction%20window%0Abefore%20CA%20event%20onset%20with%20our%20model%20peaking%20performance%20at%200.82%20one%20hour%0Abefore%20CA.%20We%20also%20provide%20a%20comprehensive%20analysis%20of%20our%20model%20through%0Aarchitectural%20tuning%20and%20PaCMAP%20visualization%20of%20patient%20health%20trajectory%20in%0Alatent%20space.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08612v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinuous%2520Cardiac%2520Arrest%2520Prediction%2520in%2520ICU%2520using%2520PPG%2520Foundation%2520Model%26entry.906535625%3DSaurabh%2520Kataria%2520and%2520Ran%2520Xiao%2520and%2520Timothy%2520Ruchti%2520and%2520Matthew%2520Clark%2520and%2520Jiaying%2520Lu%2520and%2520Randall%2520J.%2520Lee%2520and%2520Jocelyn%2520Grunwell%2520and%2520Xiao%2520Hu%26entry.1292438233%3D%2520%2520Non-invasive%2520patient%2520monitoring%2520for%2520tracking%2520and%2520predicting%2520adverse%2520acute%250Ahealth%2520events%2520is%2520an%2520emerging%2520area%2520of%2520research.%2520We%2520pursue%2520in-hospital%2520cardiac%250Aarrest%2520%2528IHCA%2529%2520prediction%2520using%2520only%2520single-channel%2520finger%2520photoplethysmography%250A%2528PPG%2529%2520signals.%2520Our%2520proposed%2520two-stage%2520model%2520Feature%2520Extractor-Aggregator%250ANetwork%2520%2528FEAN%2529%2520leverages%2520powerful%2520representations%2520from%2520pre-trained%2520PPG%250Afoundation%2520models%2520%2528PPG-GPT%2520of%2520size%2520up%2520to%25201%2520Billion%2529%2520stacked%2520with%2520sequential%250Aclassification%2520models.%2520We%2520propose%2520two%2520FEAN%2520variants%2520%2528%25221H%2522%252C%2520%2522FH%2522%2529%2520which%2520use%2520the%250Alatest%2520one-hour%2520and%2520%2528max%2529%252024-hour%2520history%2520to%2520make%2520decisions%2520respectively.%2520Our%250Astudy%2520is%2520the%2520first%2520to%2520present%2520IHCA%2520prediction%2520results%2520in%2520ICU%2520patients%2520using%250Aonly%2520unimodal%2520%2528continuous%2520PPG%2520signal%2529%2520waveform%2520deep%2520representations.%2520With%2520our%250Abest%2520model%252C%2520we%2520obtain%2520an%2520average%2520of%25200.79%2520AUROC%2520over%252024~h%2520prediction%2520window%250Abefore%2520CA%2520event%2520onset%2520with%2520our%2520model%2520peaking%2520performance%2520at%25200.82%2520one%2520hour%250Abefore%2520CA.%2520We%2520also%2520provide%2520a%2520comprehensive%2520analysis%2520of%2520our%2520model%2520through%250Aarchitectural%2520tuning%2520and%2520PaCMAP%2520visualization%2520of%2520patient%2520health%2520trajectory%2520in%250Alatent%2520space.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08612v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continuous%20Cardiac%20Arrest%20Prediction%20in%20ICU%20using%20PPG%20Foundation%20Model&entry.906535625=Saurabh%20Kataria%20and%20Ran%20Xiao%20and%20Timothy%20Ruchti%20and%20Matthew%20Clark%20and%20Jiaying%20Lu%20and%20Randall%20J.%20Lee%20and%20Jocelyn%20Grunwell%20and%20Xiao%20Hu&entry.1292438233=%20%20Non-invasive%20patient%20monitoring%20for%20tracking%20and%20predicting%20adverse%20acute%0Ahealth%20events%20is%20an%20emerging%20area%20of%20research.%20We%20pursue%20in-hospital%20cardiac%0Aarrest%20%28IHCA%29%20prediction%20using%20only%20single-channel%20finger%20photoplethysmography%0A%28PPG%29%20signals.%20Our%20proposed%20two-stage%20model%20Feature%20Extractor-Aggregator%0ANetwork%20%28FEAN%29%20leverages%20powerful%20representations%20from%20pre-trained%20PPG%0Afoundation%20models%20%28PPG-GPT%20of%20size%20up%20to%201%20Billion%29%20stacked%20with%20sequential%0Aclassification%20models.%20We%20propose%20two%20FEAN%20variants%20%28%221H%22%2C%20%22FH%22%29%20which%20use%20the%0Alatest%20one-hour%20and%20%28max%29%2024-hour%20history%20to%20make%20decisions%20respectively.%20Our%0Astudy%20is%20the%20first%20to%20present%20IHCA%20prediction%20results%20in%20ICU%20patients%20using%0Aonly%20unimodal%20%28continuous%20PPG%20signal%29%20waveform%20deep%20representations.%20With%20our%0Abest%20model%2C%20we%20obtain%20an%20average%20of%200.79%20AUROC%20over%2024~h%20prediction%20window%0Abefore%20CA%20event%20onset%20with%20our%20model%20peaking%20performance%20at%200.82%20one%20hour%0Abefore%20CA.%20We%20also%20provide%20a%20comprehensive%20analysis%20of%20our%20model%20through%0Aarchitectural%20tuning%20and%20PaCMAP%20visualization%20of%20patient%20health%20trajectory%20in%0Alatent%20space.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08612v1&entry.124074799=Read"},
{"title": "A Real-to-Sim-to-Real Approach to Robotic Manipulation with\n  VLM-Generated Iterative Keypoint Rewards", "author": "Shivansh Patel and Xinchen Yin and Wenlong Huang and Shubham Garg and Hooshang Nayyeri and Li Fei-Fei and Svetlana Lazebnik and Yunzhu Li", "abstract": "  Task specification for robotic manipulation in open-world environments is\nchallenging, requiring flexible and adaptive objectives that align with human\nintentions and can evolve through iterative feedback. We introduce Iterative\nKeypoint Reward (IKER), a visually grounded, Python-based reward function that\nserves as a dynamic task specification. Our framework leverages VLMs to\ngenerate and refine these reward functions for multi-step manipulation tasks.\nGiven RGB-D observations and free-form language instructions, we sample\nkeypoints in the scene and generate a reward function conditioned on these\nkeypoints. IKER operates on the spatial relationships between keypoints,\nleveraging commonsense priors about the desired behaviors, and enabling precise\nSE(3) control. We reconstruct real-world scenes in simulation and use the\ngenerated rewards to train reinforcement learning (RL) policies, which are then\ndeployed into the real world-forming a real-to-sim-to-real loop. Our approach\ndemonstrates notable capabilities across diverse scenarios, including both\nprehensile and non-prehensile tasks, showcasing multi-step task execution,\nspontaneous error recovery, and on-the-fly strategy adjustments. The results\nhighlight IKER's effectiveness in enabling robots to perform multi-step tasks\nin dynamic environments through iterative reward shaping.\n", "link": "http://arxiv.org/abs/2502.08643v1", "date": "2025-02-12", "relevancy": 1.7454, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6536}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.602}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Real-to-Sim-to-Real%20Approach%20to%20Robotic%20Manipulation%20with%0A%20%20VLM-Generated%20Iterative%20Keypoint%20Rewards&body=Title%3A%20A%20Real-to-Sim-to-Real%20Approach%20to%20Robotic%20Manipulation%20with%0A%20%20VLM-Generated%20Iterative%20Keypoint%20Rewards%0AAuthor%3A%20Shivansh%20Patel%20and%20Xinchen%20Yin%20and%20Wenlong%20Huang%20and%20Shubham%20Garg%20and%20Hooshang%20Nayyeri%20and%20Li%20Fei-Fei%20and%20Svetlana%20Lazebnik%20and%20Yunzhu%20Li%0AAbstract%3A%20%20%20Task%20specification%20for%20robotic%20manipulation%20in%20open-world%20environments%20is%0Achallenging%2C%20requiring%20flexible%20and%20adaptive%20objectives%20that%20align%20with%20human%0Aintentions%20and%20can%20evolve%20through%20iterative%20feedback.%20We%20introduce%20Iterative%0AKeypoint%20Reward%20%28IKER%29%2C%20a%20visually%20grounded%2C%20Python-based%20reward%20function%20that%0Aserves%20as%20a%20dynamic%20task%20specification.%20Our%20framework%20leverages%20VLMs%20to%0Agenerate%20and%20refine%20these%20reward%20functions%20for%20multi-step%20manipulation%20tasks.%0AGiven%20RGB-D%20observations%20and%20free-form%20language%20instructions%2C%20we%20sample%0Akeypoints%20in%20the%20scene%20and%20generate%20a%20reward%20function%20conditioned%20on%20these%0Akeypoints.%20IKER%20operates%20on%20the%20spatial%20relationships%20between%20keypoints%2C%0Aleveraging%20commonsense%20priors%20about%20the%20desired%20behaviors%2C%20and%20enabling%20precise%0ASE%283%29%20control.%20We%20reconstruct%20real-world%20scenes%20in%20simulation%20and%20use%20the%0Agenerated%20rewards%20to%20train%20reinforcement%20learning%20%28RL%29%20policies%2C%20which%20are%20then%0Adeployed%20into%20the%20real%20world-forming%20a%20real-to-sim-to-real%20loop.%20Our%20approach%0Ademonstrates%20notable%20capabilities%20across%20diverse%20scenarios%2C%20including%20both%0Aprehensile%20and%20non-prehensile%20tasks%2C%20showcasing%20multi-step%20task%20execution%2C%0Aspontaneous%20error%20recovery%2C%20and%20on-the-fly%20strategy%20adjustments.%20The%20results%0Ahighlight%20IKER%27s%20effectiveness%20in%20enabling%20robots%20to%20perform%20multi-step%20tasks%0Ain%20dynamic%20environments%20through%20iterative%20reward%20shaping.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Real-to-Sim-to-Real%2520Approach%2520to%2520Robotic%2520Manipulation%2520with%250A%2520%2520VLM-Generated%2520Iterative%2520Keypoint%2520Rewards%26entry.906535625%3DShivansh%2520Patel%2520and%2520Xinchen%2520Yin%2520and%2520Wenlong%2520Huang%2520and%2520Shubham%2520Garg%2520and%2520Hooshang%2520Nayyeri%2520and%2520Li%2520Fei-Fei%2520and%2520Svetlana%2520Lazebnik%2520and%2520Yunzhu%2520Li%26entry.1292438233%3D%2520%2520Task%2520specification%2520for%2520robotic%2520manipulation%2520in%2520open-world%2520environments%2520is%250Achallenging%252C%2520requiring%2520flexible%2520and%2520adaptive%2520objectives%2520that%2520align%2520with%2520human%250Aintentions%2520and%2520can%2520evolve%2520through%2520iterative%2520feedback.%2520We%2520introduce%2520Iterative%250AKeypoint%2520Reward%2520%2528IKER%2529%252C%2520a%2520visually%2520grounded%252C%2520Python-based%2520reward%2520function%2520that%250Aserves%2520as%2520a%2520dynamic%2520task%2520specification.%2520Our%2520framework%2520leverages%2520VLMs%2520to%250Agenerate%2520and%2520refine%2520these%2520reward%2520functions%2520for%2520multi-step%2520manipulation%2520tasks.%250AGiven%2520RGB-D%2520observations%2520and%2520free-form%2520language%2520instructions%252C%2520we%2520sample%250Akeypoints%2520in%2520the%2520scene%2520and%2520generate%2520a%2520reward%2520function%2520conditioned%2520on%2520these%250Akeypoints.%2520IKER%2520operates%2520on%2520the%2520spatial%2520relationships%2520between%2520keypoints%252C%250Aleveraging%2520commonsense%2520priors%2520about%2520the%2520desired%2520behaviors%252C%2520and%2520enabling%2520precise%250ASE%25283%2529%2520control.%2520We%2520reconstruct%2520real-world%2520scenes%2520in%2520simulation%2520and%2520use%2520the%250Agenerated%2520rewards%2520to%2520train%2520reinforcement%2520learning%2520%2528RL%2529%2520policies%252C%2520which%2520are%2520then%250Adeployed%2520into%2520the%2520real%2520world-forming%2520a%2520real-to-sim-to-real%2520loop.%2520Our%2520approach%250Ademonstrates%2520notable%2520capabilities%2520across%2520diverse%2520scenarios%252C%2520including%2520both%250Aprehensile%2520and%2520non-prehensile%2520tasks%252C%2520showcasing%2520multi-step%2520task%2520execution%252C%250Aspontaneous%2520error%2520recovery%252C%2520and%2520on-the-fly%2520strategy%2520adjustments.%2520The%2520results%250Ahighlight%2520IKER%2527s%2520effectiveness%2520in%2520enabling%2520robots%2520to%2520perform%2520multi-step%2520tasks%250Ain%2520dynamic%2520environments%2520through%2520iterative%2520reward%2520shaping.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Real-to-Sim-to-Real%20Approach%20to%20Robotic%20Manipulation%20with%0A%20%20VLM-Generated%20Iterative%20Keypoint%20Rewards&entry.906535625=Shivansh%20Patel%20and%20Xinchen%20Yin%20and%20Wenlong%20Huang%20and%20Shubham%20Garg%20and%20Hooshang%20Nayyeri%20and%20Li%20Fei-Fei%20and%20Svetlana%20Lazebnik%20and%20Yunzhu%20Li&entry.1292438233=%20%20Task%20specification%20for%20robotic%20manipulation%20in%20open-world%20environments%20is%0Achallenging%2C%20requiring%20flexible%20and%20adaptive%20objectives%20that%20align%20with%20human%0Aintentions%20and%20can%20evolve%20through%20iterative%20feedback.%20We%20introduce%20Iterative%0AKeypoint%20Reward%20%28IKER%29%2C%20a%20visually%20grounded%2C%20Python-based%20reward%20function%20that%0Aserves%20as%20a%20dynamic%20task%20specification.%20Our%20framework%20leverages%20VLMs%20to%0Agenerate%20and%20refine%20these%20reward%20functions%20for%20multi-step%20manipulation%20tasks.%0AGiven%20RGB-D%20observations%20and%20free-form%20language%20instructions%2C%20we%20sample%0Akeypoints%20in%20the%20scene%20and%20generate%20a%20reward%20function%20conditioned%20on%20these%0Akeypoints.%20IKER%20operates%20on%20the%20spatial%20relationships%20between%20keypoints%2C%0Aleveraging%20commonsense%20priors%20about%20the%20desired%20behaviors%2C%20and%20enabling%20precise%0ASE%283%29%20control.%20We%20reconstruct%20real-world%20scenes%20in%20simulation%20and%20use%20the%0Agenerated%20rewards%20to%20train%20reinforcement%20learning%20%28RL%29%20policies%2C%20which%20are%20then%0Adeployed%20into%20the%20real%20world-forming%20a%20real-to-sim-to-real%20loop.%20Our%20approach%0Ademonstrates%20notable%20capabilities%20across%20diverse%20scenarios%2C%20including%20both%0Aprehensile%20and%20non-prehensile%20tasks%2C%20showcasing%20multi-step%20task%20execution%2C%0Aspontaneous%20error%20recovery%2C%20and%20on-the-fly%20strategy%20adjustments.%20The%20results%0Ahighlight%20IKER%27s%20effectiveness%20in%20enabling%20robots%20to%20perform%20multi-step%20tasks%0Ain%20dynamic%20environments%20through%20iterative%20reward%20shaping.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08643v1&entry.124074799=Read"},
{"title": "Enhancing Diffusion Models Efficiency by Disentangling Total-Variance\n  and Signal-to-Noise Ratio", "author": "Khaled Kahouli and Winfried Ripken and Stefan Gugler and Oliver T. Unke and Klaus-Robert M\u00fcller and Shinichi Nakajima", "abstract": "  The long sampling time of diffusion models remains a significant bottleneck,\nwhich can be mitigated by reducing the number of diffusion time steps. However,\nthe quality of samples with fewer steps is highly dependent on the noise\nschedule, i.e., the specific manner in which noise is introduced and the signal\nis reduced at each step. Although prior work has improved upon the original\nvariance-preserving and variance-exploding schedules, these approaches\n$\\textit{passively}$ adjust the total variance, without direct control over it.\nIn this work, we propose a novel total-variance/signal-to-noise-ratio\ndisentangled (TV/SNR) framework, where TV and SNR can be controlled\nindependently. Our approach reveals that different existing schedules, where\nthe TV explodes exponentially, can be $\\textit{improved}$ by setting a constant\nTV schedule while preserving the same SNR schedule. Furthermore, generalizing\nthe SNR schedule of the optimal transport flow matching significantly improves\nthe performance in molecular structure generation, achieving few step\ngeneration of stable molecules. A similar tendency is observed in image\ngeneration, where our approach with a uniform diffusion time grid performs\ncomparably to the highly tailored EDM sampler.\n", "link": "http://arxiv.org/abs/2502.08598v1", "date": "2025-02-12", "relevancy": 1.7369, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6234}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.567}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Diffusion%20Models%20Efficiency%20by%20Disentangling%20Total-Variance%0A%20%20and%20Signal-to-Noise%20Ratio&body=Title%3A%20Enhancing%20Diffusion%20Models%20Efficiency%20by%20Disentangling%20Total-Variance%0A%20%20and%20Signal-to-Noise%20Ratio%0AAuthor%3A%20Khaled%20Kahouli%20and%20Winfried%20Ripken%20and%20Stefan%20Gugler%20and%20Oliver%20T.%20Unke%20and%20Klaus-Robert%20M%C3%BCller%20and%20Shinichi%20Nakajima%0AAbstract%3A%20%20%20The%20long%20sampling%20time%20of%20diffusion%20models%20remains%20a%20significant%20bottleneck%2C%0Awhich%20can%20be%20mitigated%20by%20reducing%20the%20number%20of%20diffusion%20time%20steps.%20However%2C%0Athe%20quality%20of%20samples%20with%20fewer%20steps%20is%20highly%20dependent%20on%20the%20noise%0Aschedule%2C%20i.e.%2C%20the%20specific%20manner%20in%20which%20noise%20is%20introduced%20and%20the%20signal%0Ais%20reduced%20at%20each%20step.%20Although%20prior%20work%20has%20improved%20upon%20the%20original%0Avariance-preserving%20and%20variance-exploding%20schedules%2C%20these%20approaches%0A%24%5Ctextit%7Bpassively%7D%24%20adjust%20the%20total%20variance%2C%20without%20direct%20control%20over%20it.%0AIn%20this%20work%2C%20we%20propose%20a%20novel%20total-variance/signal-to-noise-ratio%0Adisentangled%20%28TV/SNR%29%20framework%2C%20where%20TV%20and%20SNR%20can%20be%20controlled%0Aindependently.%20Our%20approach%20reveals%20that%20different%20existing%20schedules%2C%20where%0Athe%20TV%20explodes%20exponentially%2C%20can%20be%20%24%5Ctextit%7Bimproved%7D%24%20by%20setting%20a%20constant%0ATV%20schedule%20while%20preserving%20the%20same%20SNR%20schedule.%20Furthermore%2C%20generalizing%0Athe%20SNR%20schedule%20of%20the%20optimal%20transport%20flow%20matching%20significantly%20improves%0Athe%20performance%20in%20molecular%20structure%20generation%2C%20achieving%20few%20step%0Ageneration%20of%20stable%20molecules.%20A%20similar%20tendency%20is%20observed%20in%20image%0Ageneration%2C%20where%20our%20approach%20with%20a%20uniform%20diffusion%20time%20grid%20performs%0Acomparably%20to%20the%20highly%20tailored%20EDM%20sampler.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08598v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Diffusion%2520Models%2520Efficiency%2520by%2520Disentangling%2520Total-Variance%250A%2520%2520and%2520Signal-to-Noise%2520Ratio%26entry.906535625%3DKhaled%2520Kahouli%2520and%2520Winfried%2520Ripken%2520and%2520Stefan%2520Gugler%2520and%2520Oliver%2520T.%2520Unke%2520and%2520Klaus-Robert%2520M%25C3%25BCller%2520and%2520Shinichi%2520Nakajima%26entry.1292438233%3D%2520%2520The%2520long%2520sampling%2520time%2520of%2520diffusion%2520models%2520remains%2520a%2520significant%2520bottleneck%252C%250Awhich%2520can%2520be%2520mitigated%2520by%2520reducing%2520the%2520number%2520of%2520diffusion%2520time%2520steps.%2520However%252C%250Athe%2520quality%2520of%2520samples%2520with%2520fewer%2520steps%2520is%2520highly%2520dependent%2520on%2520the%2520noise%250Aschedule%252C%2520i.e.%252C%2520the%2520specific%2520manner%2520in%2520which%2520noise%2520is%2520introduced%2520and%2520the%2520signal%250Ais%2520reduced%2520at%2520each%2520step.%2520Although%2520prior%2520work%2520has%2520improved%2520upon%2520the%2520original%250Avariance-preserving%2520and%2520variance-exploding%2520schedules%252C%2520these%2520approaches%250A%2524%255Ctextit%257Bpassively%257D%2524%2520adjust%2520the%2520total%2520variance%252C%2520without%2520direct%2520control%2520over%2520it.%250AIn%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520total-variance/signal-to-noise-ratio%250Adisentangled%2520%2528TV/SNR%2529%2520framework%252C%2520where%2520TV%2520and%2520SNR%2520can%2520be%2520controlled%250Aindependently.%2520Our%2520approach%2520reveals%2520that%2520different%2520existing%2520schedules%252C%2520where%250Athe%2520TV%2520explodes%2520exponentially%252C%2520can%2520be%2520%2524%255Ctextit%257Bimproved%257D%2524%2520by%2520setting%2520a%2520constant%250ATV%2520schedule%2520while%2520preserving%2520the%2520same%2520SNR%2520schedule.%2520Furthermore%252C%2520generalizing%250Athe%2520SNR%2520schedule%2520of%2520the%2520optimal%2520transport%2520flow%2520matching%2520significantly%2520improves%250Athe%2520performance%2520in%2520molecular%2520structure%2520generation%252C%2520achieving%2520few%2520step%250Ageneration%2520of%2520stable%2520molecules.%2520A%2520similar%2520tendency%2520is%2520observed%2520in%2520image%250Ageneration%252C%2520where%2520our%2520approach%2520with%2520a%2520uniform%2520diffusion%2520time%2520grid%2520performs%250Acomparably%2520to%2520the%2520highly%2520tailored%2520EDM%2520sampler.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08598v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Diffusion%20Models%20Efficiency%20by%20Disentangling%20Total-Variance%0A%20%20and%20Signal-to-Noise%20Ratio&entry.906535625=Khaled%20Kahouli%20and%20Winfried%20Ripken%20and%20Stefan%20Gugler%20and%20Oliver%20T.%20Unke%20and%20Klaus-Robert%20M%C3%BCller%20and%20Shinichi%20Nakajima&entry.1292438233=%20%20The%20long%20sampling%20time%20of%20diffusion%20models%20remains%20a%20significant%20bottleneck%2C%0Awhich%20can%20be%20mitigated%20by%20reducing%20the%20number%20of%20diffusion%20time%20steps.%20However%2C%0Athe%20quality%20of%20samples%20with%20fewer%20steps%20is%20highly%20dependent%20on%20the%20noise%0Aschedule%2C%20i.e.%2C%20the%20specific%20manner%20in%20which%20noise%20is%20introduced%20and%20the%20signal%0Ais%20reduced%20at%20each%20step.%20Although%20prior%20work%20has%20improved%20upon%20the%20original%0Avariance-preserving%20and%20variance-exploding%20schedules%2C%20these%20approaches%0A%24%5Ctextit%7Bpassively%7D%24%20adjust%20the%20total%20variance%2C%20without%20direct%20control%20over%20it.%0AIn%20this%20work%2C%20we%20propose%20a%20novel%20total-variance/signal-to-noise-ratio%0Adisentangled%20%28TV/SNR%29%20framework%2C%20where%20TV%20and%20SNR%20can%20be%20controlled%0Aindependently.%20Our%20approach%20reveals%20that%20different%20existing%20schedules%2C%20where%0Athe%20TV%20explodes%20exponentially%2C%20can%20be%20%24%5Ctextit%7Bimproved%7D%24%20by%20setting%20a%20constant%0ATV%20schedule%20while%20preserving%20the%20same%20SNR%20schedule.%20Furthermore%2C%20generalizing%0Athe%20SNR%20schedule%20of%20the%20optimal%20transport%20flow%20matching%20significantly%20improves%0Athe%20performance%20in%20molecular%20structure%20generation%2C%20achieving%20few%20step%0Ageneration%20of%20stable%20molecules.%20A%20similar%20tendency%20is%20observed%20in%20image%0Ageneration%2C%20where%20our%20approach%20with%20a%20uniform%20diffusion%20time%20grid%20performs%0Acomparably%20to%20the%20highly%20tailored%20EDM%20sampler.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08598v1&entry.124074799=Read"},
{"title": "Interactive incremental learning of generalizable skills with local\n  trajectory modulation", "author": "Markus Knauer and Alin Albu-Sch\u00e4ffer and Freek Stulp and Jo\u00e3o Silv\u00e9rio", "abstract": "  The problem of generalization in learning from demonstration (LfD) has\nreceived considerable attention over the years, particularly within the context\nof movement primitives, where a number of approaches have emerged. Recently,\ntwo important approaches have gained recognition. While one leverages\nvia-points to adapt skills locally by modulating demonstrated trajectories,\nanother relies on so-called task-parameterized models that encode movements\nwith respect to different coordinate systems, using a product of probabilities\nfor generalization. While the former are well-suited to precise, local\nmodulations, the latter aim at generalizing over large regions of the workspace\nand often involve multiple objects. Addressing the quality of generalization by\nleveraging both approaches simultaneously has received little attention. In\nthis work, we propose an interactive imitation learning framework that\nsimultaneously leverages local and global modulations of trajectory\ndistributions. Building on the kernelized movement primitives (KMP) framework,\nwe introduce novel mechanisms for skill modulation from direct human corrective\nfeedback. Our approach particularly exploits the concept of via-points to\nincrementally and interactively 1) improve the model accuracy locally, 2) add\nnew objects to the task during execution and 3) extend the skill into regions\nwhere demonstrations were not provided. We evaluate our method on a bearing\nring-loading task using a torque-controlled, 7-DoF, DLR SARA robot.\n", "link": "http://arxiv.org/abs/2409.05655v2", "date": "2025-02-12", "relevancy": 1.6953, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5927}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5673}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interactive%20incremental%20learning%20of%20generalizable%20skills%20with%20local%0A%20%20trajectory%20modulation&body=Title%3A%20Interactive%20incremental%20learning%20of%20generalizable%20skills%20with%20local%0A%20%20trajectory%20modulation%0AAuthor%3A%20Markus%20Knauer%20and%20Alin%20Albu-Sch%C3%A4ffer%20and%20Freek%20Stulp%20and%20Jo%C3%A3o%20Silv%C3%A9rio%0AAbstract%3A%20%20%20The%20problem%20of%20generalization%20in%20learning%20from%20demonstration%20%28LfD%29%20has%0Areceived%20considerable%20attention%20over%20the%20years%2C%20particularly%20within%20the%20context%0Aof%20movement%20primitives%2C%20where%20a%20number%20of%20approaches%20have%20emerged.%20Recently%2C%0Atwo%20important%20approaches%20have%20gained%20recognition.%20While%20one%20leverages%0Avia-points%20to%20adapt%20skills%20locally%20by%20modulating%20demonstrated%20trajectories%2C%0Aanother%20relies%20on%20so-called%20task-parameterized%20models%20that%20encode%20movements%0Awith%20respect%20to%20different%20coordinate%20systems%2C%20using%20a%20product%20of%20probabilities%0Afor%20generalization.%20While%20the%20former%20are%20well-suited%20to%20precise%2C%20local%0Amodulations%2C%20the%20latter%20aim%20at%20generalizing%20over%20large%20regions%20of%20the%20workspace%0Aand%20often%20involve%20multiple%20objects.%20Addressing%20the%20quality%20of%20generalization%20by%0Aleveraging%20both%20approaches%20simultaneously%20has%20received%20little%20attention.%20In%0Athis%20work%2C%20we%20propose%20an%20interactive%20imitation%20learning%20framework%20that%0Asimultaneously%20leverages%20local%20and%20global%20modulations%20of%20trajectory%0Adistributions.%20Building%20on%20the%20kernelized%20movement%20primitives%20%28KMP%29%20framework%2C%0Awe%20introduce%20novel%20mechanisms%20for%20skill%20modulation%20from%20direct%20human%20corrective%0Afeedback.%20Our%20approach%20particularly%20exploits%20the%20concept%20of%20via-points%20to%0Aincrementally%20and%20interactively%201%29%20improve%20the%20model%20accuracy%20locally%2C%202%29%20add%0Anew%20objects%20to%20the%20task%20during%20execution%20and%203%29%20extend%20the%20skill%20into%20regions%0Awhere%20demonstrations%20were%20not%20provided.%20We%20evaluate%20our%20method%20on%20a%20bearing%0Aring-loading%20task%20using%20a%20torque-controlled%2C%207-DoF%2C%20DLR%20SARA%20robot.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05655v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInteractive%2520incremental%2520learning%2520of%2520generalizable%2520skills%2520with%2520local%250A%2520%2520trajectory%2520modulation%26entry.906535625%3DMarkus%2520Knauer%2520and%2520Alin%2520Albu-Sch%25C3%25A4ffer%2520and%2520Freek%2520Stulp%2520and%2520Jo%25C3%25A3o%2520Silv%25C3%25A9rio%26entry.1292438233%3D%2520%2520The%2520problem%2520of%2520generalization%2520in%2520learning%2520from%2520demonstration%2520%2528LfD%2529%2520has%250Areceived%2520considerable%2520attention%2520over%2520the%2520years%252C%2520particularly%2520within%2520the%2520context%250Aof%2520movement%2520primitives%252C%2520where%2520a%2520number%2520of%2520approaches%2520have%2520emerged.%2520Recently%252C%250Atwo%2520important%2520approaches%2520have%2520gained%2520recognition.%2520While%2520one%2520leverages%250Avia-points%2520to%2520adapt%2520skills%2520locally%2520by%2520modulating%2520demonstrated%2520trajectories%252C%250Aanother%2520relies%2520on%2520so-called%2520task-parameterized%2520models%2520that%2520encode%2520movements%250Awith%2520respect%2520to%2520different%2520coordinate%2520systems%252C%2520using%2520a%2520product%2520of%2520probabilities%250Afor%2520generalization.%2520While%2520the%2520former%2520are%2520well-suited%2520to%2520precise%252C%2520local%250Amodulations%252C%2520the%2520latter%2520aim%2520at%2520generalizing%2520over%2520large%2520regions%2520of%2520the%2520workspace%250Aand%2520often%2520involve%2520multiple%2520objects.%2520Addressing%2520the%2520quality%2520of%2520generalization%2520by%250Aleveraging%2520both%2520approaches%2520simultaneously%2520has%2520received%2520little%2520attention.%2520In%250Athis%2520work%252C%2520we%2520propose%2520an%2520interactive%2520imitation%2520learning%2520framework%2520that%250Asimultaneously%2520leverages%2520local%2520and%2520global%2520modulations%2520of%2520trajectory%250Adistributions.%2520Building%2520on%2520the%2520kernelized%2520movement%2520primitives%2520%2528KMP%2529%2520framework%252C%250Awe%2520introduce%2520novel%2520mechanisms%2520for%2520skill%2520modulation%2520from%2520direct%2520human%2520corrective%250Afeedback.%2520Our%2520approach%2520particularly%2520exploits%2520the%2520concept%2520of%2520via-points%2520to%250Aincrementally%2520and%2520interactively%25201%2529%2520improve%2520the%2520model%2520accuracy%2520locally%252C%25202%2529%2520add%250Anew%2520objects%2520to%2520the%2520task%2520during%2520execution%2520and%25203%2529%2520extend%2520the%2520skill%2520into%2520regions%250Awhere%2520demonstrations%2520were%2520not%2520provided.%2520We%2520evaluate%2520our%2520method%2520on%2520a%2520bearing%250Aring-loading%2520task%2520using%2520a%2520torque-controlled%252C%25207-DoF%252C%2520DLR%2520SARA%2520robot.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05655v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interactive%20incremental%20learning%20of%20generalizable%20skills%20with%20local%0A%20%20trajectory%20modulation&entry.906535625=Markus%20Knauer%20and%20Alin%20Albu-Sch%C3%A4ffer%20and%20Freek%20Stulp%20and%20Jo%C3%A3o%20Silv%C3%A9rio&entry.1292438233=%20%20The%20problem%20of%20generalization%20in%20learning%20from%20demonstration%20%28LfD%29%20has%0Areceived%20considerable%20attention%20over%20the%20years%2C%20particularly%20within%20the%20context%0Aof%20movement%20primitives%2C%20where%20a%20number%20of%20approaches%20have%20emerged.%20Recently%2C%0Atwo%20important%20approaches%20have%20gained%20recognition.%20While%20one%20leverages%0Avia-points%20to%20adapt%20skills%20locally%20by%20modulating%20demonstrated%20trajectories%2C%0Aanother%20relies%20on%20so-called%20task-parameterized%20models%20that%20encode%20movements%0Awith%20respect%20to%20different%20coordinate%20systems%2C%20using%20a%20product%20of%20probabilities%0Afor%20generalization.%20While%20the%20former%20are%20well-suited%20to%20precise%2C%20local%0Amodulations%2C%20the%20latter%20aim%20at%20generalizing%20over%20large%20regions%20of%20the%20workspace%0Aand%20often%20involve%20multiple%20objects.%20Addressing%20the%20quality%20of%20generalization%20by%0Aleveraging%20both%20approaches%20simultaneously%20has%20received%20little%20attention.%20In%0Athis%20work%2C%20we%20propose%20an%20interactive%20imitation%20learning%20framework%20that%0Asimultaneously%20leverages%20local%20and%20global%20modulations%20of%20trajectory%0Adistributions.%20Building%20on%20the%20kernelized%20movement%20primitives%20%28KMP%29%20framework%2C%0Awe%20introduce%20novel%20mechanisms%20for%20skill%20modulation%20from%20direct%20human%20corrective%0Afeedback.%20Our%20approach%20particularly%20exploits%20the%20concept%20of%20via-points%20to%0Aincrementally%20and%20interactively%201%29%20improve%20the%20model%20accuracy%20locally%2C%202%29%20add%0Anew%20objects%20to%20the%20task%20during%20execution%20and%203%29%20extend%20the%20skill%20into%20regions%0Awhere%20demonstrations%20were%20not%20provided.%20We%20evaluate%20our%20method%20on%20a%20bearing%0Aring-loading%20task%20using%20a%20torque-controlled%2C%207-DoF%2C%20DLR%20SARA%20robot.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05655v2&entry.124074799=Read"},
{"title": "Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous\n  Attacks", "author": "Ang Li and Yin Zhou and Vethavikashini Chithrra Raghuram and Tom Goldstein and Micah Goldblum", "abstract": "  A high volume of recent ML security literature focuses on attacks against\naligned large language models (LLMs). These attacks may extract private\ninformation or coerce the model into producing harmful outputs. In real-world\ndeployments, LLMs are often part of a larger agentic pipeline including memory\nsystems, retrieval, web access, and API calling. Such additional components\nintroduce vulnerabilities that make these LLM-powered agents much easier to\nattack than isolated LLMs, yet relatively little work focuses on the security\nof LLM agents. In this paper, we analyze security and privacy vulnerabilities\nthat are unique to LLM agents. We first provide a taxonomy of attacks\ncategorized by threat actors, objectives, entry points, attacker observability,\nattack strategies, and inherent vulnerabilities of agent pipelines. We then\nconduct a series of illustrative attacks on popular open-source and commercial\nagents, demonstrating the immediate practical implications of their\nvulnerabilities. Notably, our attacks are trivial to implement and require no\nunderstanding of machine learning.\n", "link": "http://arxiv.org/abs/2502.08586v1", "date": "2025-02-12", "relevancy": 1.6938, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4346}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4217}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Commercial%20LLM%20Agents%20Are%20Already%20Vulnerable%20to%20Simple%20Yet%20Dangerous%0A%20%20Attacks&body=Title%3A%20Commercial%20LLM%20Agents%20Are%20Already%20Vulnerable%20to%20Simple%20Yet%20Dangerous%0A%20%20Attacks%0AAuthor%3A%20Ang%20Li%20and%20Yin%20Zhou%20and%20Vethavikashini%20Chithrra%20Raghuram%20and%20Tom%20Goldstein%20and%20Micah%20Goldblum%0AAbstract%3A%20%20%20A%20high%20volume%20of%20recent%20ML%20security%20literature%20focuses%20on%20attacks%20against%0Aaligned%20large%20language%20models%20%28LLMs%29.%20These%20attacks%20may%20extract%20private%0Ainformation%20or%20coerce%20the%20model%20into%20producing%20harmful%20outputs.%20In%20real-world%0Adeployments%2C%20LLMs%20are%20often%20part%20of%20a%20larger%20agentic%20pipeline%20including%20memory%0Asystems%2C%20retrieval%2C%20web%20access%2C%20and%20API%20calling.%20Such%20additional%20components%0Aintroduce%20vulnerabilities%20that%20make%20these%20LLM-powered%20agents%20much%20easier%20to%0Aattack%20than%20isolated%20LLMs%2C%20yet%20relatively%20little%20work%20focuses%20on%20the%20security%0Aof%20LLM%20agents.%20In%20this%20paper%2C%20we%20analyze%20security%20and%20privacy%20vulnerabilities%0Athat%20are%20unique%20to%20LLM%20agents.%20We%20first%20provide%20a%20taxonomy%20of%20attacks%0Acategorized%20by%20threat%20actors%2C%20objectives%2C%20entry%20points%2C%20attacker%20observability%2C%0Aattack%20strategies%2C%20and%20inherent%20vulnerabilities%20of%20agent%20pipelines.%20We%20then%0Aconduct%20a%20series%20of%20illustrative%20attacks%20on%20popular%20open-source%20and%20commercial%0Aagents%2C%20demonstrating%20the%20immediate%20practical%20implications%20of%20their%0Avulnerabilities.%20Notably%2C%20our%20attacks%20are%20trivial%20to%20implement%20and%20require%20no%0Aunderstanding%20of%20machine%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08586v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommercial%2520LLM%2520Agents%2520Are%2520Already%2520Vulnerable%2520to%2520Simple%2520Yet%2520Dangerous%250A%2520%2520Attacks%26entry.906535625%3DAng%2520Li%2520and%2520Yin%2520Zhou%2520and%2520Vethavikashini%2520Chithrra%2520Raghuram%2520and%2520Tom%2520Goldstein%2520and%2520Micah%2520Goldblum%26entry.1292438233%3D%2520%2520A%2520high%2520volume%2520of%2520recent%2520ML%2520security%2520literature%2520focuses%2520on%2520attacks%2520against%250Aaligned%2520large%2520language%2520models%2520%2528LLMs%2529.%2520These%2520attacks%2520may%2520extract%2520private%250Ainformation%2520or%2520coerce%2520the%2520model%2520into%2520producing%2520harmful%2520outputs.%2520In%2520real-world%250Adeployments%252C%2520LLMs%2520are%2520often%2520part%2520of%2520a%2520larger%2520agentic%2520pipeline%2520including%2520memory%250Asystems%252C%2520retrieval%252C%2520web%2520access%252C%2520and%2520API%2520calling.%2520Such%2520additional%2520components%250Aintroduce%2520vulnerabilities%2520that%2520make%2520these%2520LLM-powered%2520agents%2520much%2520easier%2520to%250Aattack%2520than%2520isolated%2520LLMs%252C%2520yet%2520relatively%2520little%2520work%2520focuses%2520on%2520the%2520security%250Aof%2520LLM%2520agents.%2520In%2520this%2520paper%252C%2520we%2520analyze%2520security%2520and%2520privacy%2520vulnerabilities%250Athat%2520are%2520unique%2520to%2520LLM%2520agents.%2520We%2520first%2520provide%2520a%2520taxonomy%2520of%2520attacks%250Acategorized%2520by%2520threat%2520actors%252C%2520objectives%252C%2520entry%2520points%252C%2520attacker%2520observability%252C%250Aattack%2520strategies%252C%2520and%2520inherent%2520vulnerabilities%2520of%2520agent%2520pipelines.%2520We%2520then%250Aconduct%2520a%2520series%2520of%2520illustrative%2520attacks%2520on%2520popular%2520open-source%2520and%2520commercial%250Aagents%252C%2520demonstrating%2520the%2520immediate%2520practical%2520implications%2520of%2520their%250Avulnerabilities.%2520Notably%252C%2520our%2520attacks%2520are%2520trivial%2520to%2520implement%2520and%2520require%2520no%250Aunderstanding%2520of%2520machine%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08586v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Commercial%20LLM%20Agents%20Are%20Already%20Vulnerable%20to%20Simple%20Yet%20Dangerous%0A%20%20Attacks&entry.906535625=Ang%20Li%20and%20Yin%20Zhou%20and%20Vethavikashini%20Chithrra%20Raghuram%20and%20Tom%20Goldstein%20and%20Micah%20Goldblum&entry.1292438233=%20%20A%20high%20volume%20of%20recent%20ML%20security%20literature%20focuses%20on%20attacks%20against%0Aaligned%20large%20language%20models%20%28LLMs%29.%20These%20attacks%20may%20extract%20private%0Ainformation%20or%20coerce%20the%20model%20into%20producing%20harmful%20outputs.%20In%20real-world%0Adeployments%2C%20LLMs%20are%20often%20part%20of%20a%20larger%20agentic%20pipeline%20including%20memory%0Asystems%2C%20retrieval%2C%20web%20access%2C%20and%20API%20calling.%20Such%20additional%20components%0Aintroduce%20vulnerabilities%20that%20make%20these%20LLM-powered%20agents%20much%20easier%20to%0Aattack%20than%20isolated%20LLMs%2C%20yet%20relatively%20little%20work%20focuses%20on%20the%20security%0Aof%20LLM%20agents.%20In%20this%20paper%2C%20we%20analyze%20security%20and%20privacy%20vulnerabilities%0Athat%20are%20unique%20to%20LLM%20agents.%20We%20first%20provide%20a%20taxonomy%20of%20attacks%0Acategorized%20by%20threat%20actors%2C%20objectives%2C%20entry%20points%2C%20attacker%20observability%2C%0Aattack%20strategies%2C%20and%20inherent%20vulnerabilities%20of%20agent%20pipelines.%20We%20then%0Aconduct%20a%20series%20of%20illustrative%20attacks%20on%20popular%20open-source%20and%20commercial%0Aagents%2C%20demonstrating%20the%20immediate%20practical%20implications%20of%20their%0Avulnerabilities.%20Notably%2C%20our%20attacks%20are%20trivial%20to%20implement%20and%20require%20no%0Aunderstanding%20of%20machine%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08586v1&entry.124074799=Read"},
{"title": "Mathematical Data Science", "author": "Michael R. Douglas and Kyu-Hwan Lee", "abstract": "  Can machine learning help discover new mathematical structures? In this\narticle we discuss an approach to doing this which one can call \"mathematical\ndata science\". In this paradigm, one studies mathematical objects collectively\nrather than individually, by creating datasets and doing machine learning\nexperiments and interpretations. After an overview, we present two case\nstudies: murmurations in number theory and loadings of partitions related to\nKronecker coefficients in representation theory and combinatorics.\n", "link": "http://arxiv.org/abs/2502.08620v1", "date": "2025-02-12", "relevancy": 1.6808, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4319}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4133}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mathematical%20Data%20Science&body=Title%3A%20Mathematical%20Data%20Science%0AAuthor%3A%20Michael%20R.%20Douglas%20and%20Kyu-Hwan%20Lee%0AAbstract%3A%20%20%20Can%20machine%20learning%20help%20discover%20new%20mathematical%20structures%3F%20In%20this%0Aarticle%20we%20discuss%20an%20approach%20to%20doing%20this%20which%20one%20can%20call%20%22mathematical%0Adata%20science%22.%20In%20this%20paradigm%2C%20one%20studies%20mathematical%20objects%20collectively%0Arather%20than%20individually%2C%20by%20creating%20datasets%20and%20doing%20machine%20learning%0Aexperiments%20and%20interpretations.%20After%20an%20overview%2C%20we%20present%20two%20case%0Astudies%3A%20murmurations%20in%20number%20theory%20and%20loadings%20of%20partitions%20related%20to%0AKronecker%20coefficients%20in%20representation%20theory%20and%20combinatorics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08620v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMathematical%2520Data%2520Science%26entry.906535625%3DMichael%2520R.%2520Douglas%2520and%2520Kyu-Hwan%2520Lee%26entry.1292438233%3D%2520%2520Can%2520machine%2520learning%2520help%2520discover%2520new%2520mathematical%2520structures%253F%2520In%2520this%250Aarticle%2520we%2520discuss%2520an%2520approach%2520to%2520doing%2520this%2520which%2520one%2520can%2520call%2520%2522mathematical%250Adata%2520science%2522.%2520In%2520this%2520paradigm%252C%2520one%2520studies%2520mathematical%2520objects%2520collectively%250Arather%2520than%2520individually%252C%2520by%2520creating%2520datasets%2520and%2520doing%2520machine%2520learning%250Aexperiments%2520and%2520interpretations.%2520After%2520an%2520overview%252C%2520we%2520present%2520two%2520case%250Astudies%253A%2520murmurations%2520in%2520number%2520theory%2520and%2520loadings%2520of%2520partitions%2520related%2520to%250AKronecker%2520coefficients%2520in%2520representation%2520theory%2520and%2520combinatorics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08620v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mathematical%20Data%20Science&entry.906535625=Michael%20R.%20Douglas%20and%20Kyu-Hwan%20Lee&entry.1292438233=%20%20Can%20machine%20learning%20help%20discover%20new%20mathematical%20structures%3F%20In%20this%0Aarticle%20we%20discuss%20an%20approach%20to%20doing%20this%20which%20one%20can%20call%20%22mathematical%0Adata%20science%22.%20In%20this%20paradigm%2C%20one%20studies%20mathematical%20objects%20collectively%0Arather%20than%20individually%2C%20by%20creating%20datasets%20and%20doing%20machine%20learning%0Aexperiments%20and%20interpretations.%20After%20an%20overview%2C%20we%20present%20two%20case%0Astudies%3A%20murmurations%20in%20number%20theory%20and%20loadings%20of%20partitions%20related%20to%0AKronecker%20coefficients%20in%20representation%20theory%20and%20combinatorics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08620v1&entry.124074799=Read"},
{"title": "Toward Universal Laws of Outlier Propagation", "author": "Yuhao Wang and Aram Ebtekar and Dominik Janzing", "abstract": "  We argue that Algorithmic Information Theory (AIT) admits a principled way to\nquantify outliers in terms of so-called randomness deficiency. For the\nprobability distribution generated by a causal Bayesian network, we show that\nthe randomness deficiency of the joint state decomposes into randomness\ndeficiencies of each causal mechanism, subject to the Independence of\nMechanisms Principle. Accordingly, anomalous joint observations can be\nquantitatively attributed to their root causes, i.e., the mechanisms that\nbehaved anomalously. As an extension of Levin's law of randomness conservation,\nwe show that weak outliers cannot cause strong ones when Independence of\nMechanisms holds. We show how these information theoretic laws provide a better\nunderstanding of the behaviour of outliers defined with respect to existing\nscores.\n", "link": "http://arxiv.org/abs/2502.08593v1", "date": "2025-02-12", "relevancy": 1.6772, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4614}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4286}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Universal%20Laws%20of%20Outlier%20Propagation&body=Title%3A%20Toward%20Universal%20Laws%20of%20Outlier%20Propagation%0AAuthor%3A%20Yuhao%20Wang%20and%20Aram%20Ebtekar%20and%20Dominik%20Janzing%0AAbstract%3A%20%20%20We%20argue%20that%20Algorithmic%20Information%20Theory%20%28AIT%29%20admits%20a%20principled%20way%20to%0Aquantify%20outliers%20in%20terms%20of%20so-called%20randomness%20deficiency.%20For%20the%0Aprobability%20distribution%20generated%20by%20a%20causal%20Bayesian%20network%2C%20we%20show%20that%0Athe%20randomness%20deficiency%20of%20the%20joint%20state%20decomposes%20into%20randomness%0Adeficiencies%20of%20each%20causal%20mechanism%2C%20subject%20to%20the%20Independence%20of%0AMechanisms%20Principle.%20Accordingly%2C%20anomalous%20joint%20observations%20can%20be%0Aquantitatively%20attributed%20to%20their%20root%20causes%2C%20i.e.%2C%20the%20mechanisms%20that%0Abehaved%20anomalously.%20As%20an%20extension%20of%20Levin%27s%20law%20of%20randomness%20conservation%2C%0Awe%20show%20that%20weak%20outliers%20cannot%20cause%20strong%20ones%20when%20Independence%20of%0AMechanisms%20holds.%20We%20show%20how%20these%20information%20theoretic%20laws%20provide%20a%20better%0Aunderstanding%20of%20the%20behaviour%20of%20outliers%20defined%20with%20respect%20to%20existing%0Ascores.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08593v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Universal%2520Laws%2520of%2520Outlier%2520Propagation%26entry.906535625%3DYuhao%2520Wang%2520and%2520Aram%2520Ebtekar%2520and%2520Dominik%2520Janzing%26entry.1292438233%3D%2520%2520We%2520argue%2520that%2520Algorithmic%2520Information%2520Theory%2520%2528AIT%2529%2520admits%2520a%2520principled%2520way%2520to%250Aquantify%2520outliers%2520in%2520terms%2520of%2520so-called%2520randomness%2520deficiency.%2520For%2520the%250Aprobability%2520distribution%2520generated%2520by%2520a%2520causal%2520Bayesian%2520network%252C%2520we%2520show%2520that%250Athe%2520randomness%2520deficiency%2520of%2520the%2520joint%2520state%2520decomposes%2520into%2520randomness%250Adeficiencies%2520of%2520each%2520causal%2520mechanism%252C%2520subject%2520to%2520the%2520Independence%2520of%250AMechanisms%2520Principle.%2520Accordingly%252C%2520anomalous%2520joint%2520observations%2520can%2520be%250Aquantitatively%2520attributed%2520to%2520their%2520root%2520causes%252C%2520i.e.%252C%2520the%2520mechanisms%2520that%250Abehaved%2520anomalously.%2520As%2520an%2520extension%2520of%2520Levin%2527s%2520law%2520of%2520randomness%2520conservation%252C%250Awe%2520show%2520that%2520weak%2520outliers%2520cannot%2520cause%2520strong%2520ones%2520when%2520Independence%2520of%250AMechanisms%2520holds.%2520We%2520show%2520how%2520these%2520information%2520theoretic%2520laws%2520provide%2520a%2520better%250Aunderstanding%2520of%2520the%2520behaviour%2520of%2520outliers%2520defined%2520with%2520respect%2520to%2520existing%250Ascores.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08593v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Universal%20Laws%20of%20Outlier%20Propagation&entry.906535625=Yuhao%20Wang%20and%20Aram%20Ebtekar%20and%20Dominik%20Janzing&entry.1292438233=%20%20We%20argue%20that%20Algorithmic%20Information%20Theory%20%28AIT%29%20admits%20a%20principled%20way%20to%0Aquantify%20outliers%20in%20terms%20of%20so-called%20randomness%20deficiency.%20For%20the%0Aprobability%20distribution%20generated%20by%20a%20causal%20Bayesian%20network%2C%20we%20show%20that%0Athe%20randomness%20deficiency%20of%20the%20joint%20state%20decomposes%20into%20randomness%0Adeficiencies%20of%20each%20causal%20mechanism%2C%20subject%20to%20the%20Independence%20of%0AMechanisms%20Principle.%20Accordingly%2C%20anomalous%20joint%20observations%20can%20be%0Aquantitatively%20attributed%20to%20their%20root%20causes%2C%20i.e.%2C%20the%20mechanisms%20that%0Abehaved%20anomalously.%20As%20an%20extension%20of%20Levin%27s%20law%20of%20randomness%20conservation%2C%0Awe%20show%20that%20weak%20outliers%20cannot%20cause%20strong%20ones%20when%20Independence%20of%0AMechanisms%20holds.%20We%20show%20how%20these%20information%20theoretic%20laws%20provide%20a%20better%0Aunderstanding%20of%20the%20behaviour%20of%20outliers%20defined%20with%20respect%20to%20existing%0Ascores.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08593v1&entry.124074799=Read"},
{"title": "Mapping the Landscape of Generative AI in Network Monitoring and\n  Management", "author": "Giampaolo Bovenzi and Francesco Cerasuolo and Domenico Ciuonzo and Davide Di Monda and Idio Guarino and Antonio Montieri and Valerio Persico and Antonio Pescap\u00e8", "abstract": "  Generative Artificial Intelligence (GenAI) models such as LLMs, GPTs, and\nDiffusion Models have recently gained widespread attention from both the\nresearch and the industrial communities. This survey explores their application\nin network monitoring and management, focusing on prominent use cases, as well\nas challenges and opportunities. We discuss how network traffic generation and\nclassification, network intrusion detection, networked system log analysis, and\nnetwork digital assistance can benefit from the use of GenAI models.\nAdditionally, we provide an overview of the available GenAI models, datasets\nfor large-scale training phases, and platforms for the development of such\nmodels. Finally, we discuss research directions that potentially mitigate the\nroadblocks to the adoption of GenAI for network monitoring and management. Our\ninvestigation aims to map the current landscape and pave the way for future\nresearch in leveraging GenAI for network monitoring and management.\n", "link": "http://arxiv.org/abs/2502.08576v1", "date": "2025-02-12", "relevancy": 1.6578, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5818}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5238}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mapping%20the%20Landscape%20of%20Generative%20AI%20in%20Network%20Monitoring%20and%0A%20%20Management&body=Title%3A%20Mapping%20the%20Landscape%20of%20Generative%20AI%20in%20Network%20Monitoring%20and%0A%20%20Management%0AAuthor%3A%20Giampaolo%20Bovenzi%20and%20Francesco%20Cerasuolo%20and%20Domenico%20Ciuonzo%20and%20Davide%20Di%20Monda%20and%20Idio%20Guarino%20and%20Antonio%20Montieri%20and%20Valerio%20Persico%20and%20Antonio%20Pescap%C3%A8%0AAbstract%3A%20%20%20Generative%20Artificial%20Intelligence%20%28GenAI%29%20models%20such%20as%20LLMs%2C%20GPTs%2C%20and%0ADiffusion%20Models%20have%20recently%20gained%20widespread%20attention%20from%20both%20the%0Aresearch%20and%20the%20industrial%20communities.%20This%20survey%20explores%20their%20application%0Ain%20network%20monitoring%20and%20management%2C%20focusing%20on%20prominent%20use%20cases%2C%20as%20well%0Aas%20challenges%20and%20opportunities.%20We%20discuss%20how%20network%20traffic%20generation%20and%0Aclassification%2C%20network%20intrusion%20detection%2C%20networked%20system%20log%20analysis%2C%20and%0Anetwork%20digital%20assistance%20can%20benefit%20from%20the%20use%20of%20GenAI%20models.%0AAdditionally%2C%20we%20provide%20an%20overview%20of%20the%20available%20GenAI%20models%2C%20datasets%0Afor%20large-scale%20training%20phases%2C%20and%20platforms%20for%20the%20development%20of%20such%0Amodels.%20Finally%2C%20we%20discuss%20research%20directions%20that%20potentially%20mitigate%20the%0Aroadblocks%20to%20the%20adoption%20of%20GenAI%20for%20network%20monitoring%20and%20management.%20Our%0Ainvestigation%20aims%20to%20map%20the%20current%20landscape%20and%20pave%20the%20way%20for%20future%0Aresearch%20in%20leveraging%20GenAI%20for%20network%20monitoring%20and%20management.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08576v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMapping%2520the%2520Landscape%2520of%2520Generative%2520AI%2520in%2520Network%2520Monitoring%2520and%250A%2520%2520Management%26entry.906535625%3DGiampaolo%2520Bovenzi%2520and%2520Francesco%2520Cerasuolo%2520and%2520Domenico%2520Ciuonzo%2520and%2520Davide%2520Di%2520Monda%2520and%2520Idio%2520Guarino%2520and%2520Antonio%2520Montieri%2520and%2520Valerio%2520Persico%2520and%2520Antonio%2520Pescap%25C3%25A8%26entry.1292438233%3D%2520%2520Generative%2520Artificial%2520Intelligence%2520%2528GenAI%2529%2520models%2520such%2520as%2520LLMs%252C%2520GPTs%252C%2520and%250ADiffusion%2520Models%2520have%2520recently%2520gained%2520widespread%2520attention%2520from%2520both%2520the%250Aresearch%2520and%2520the%2520industrial%2520communities.%2520This%2520survey%2520explores%2520their%2520application%250Ain%2520network%2520monitoring%2520and%2520management%252C%2520focusing%2520on%2520prominent%2520use%2520cases%252C%2520as%2520well%250Aas%2520challenges%2520and%2520opportunities.%2520We%2520discuss%2520how%2520network%2520traffic%2520generation%2520and%250Aclassification%252C%2520network%2520intrusion%2520detection%252C%2520networked%2520system%2520log%2520analysis%252C%2520and%250Anetwork%2520digital%2520assistance%2520can%2520benefit%2520from%2520the%2520use%2520of%2520GenAI%2520models.%250AAdditionally%252C%2520we%2520provide%2520an%2520overview%2520of%2520the%2520available%2520GenAI%2520models%252C%2520datasets%250Afor%2520large-scale%2520training%2520phases%252C%2520and%2520platforms%2520for%2520the%2520development%2520of%2520such%250Amodels.%2520Finally%252C%2520we%2520discuss%2520research%2520directions%2520that%2520potentially%2520mitigate%2520the%250Aroadblocks%2520to%2520the%2520adoption%2520of%2520GenAI%2520for%2520network%2520monitoring%2520and%2520management.%2520Our%250Ainvestigation%2520aims%2520to%2520map%2520the%2520current%2520landscape%2520and%2520pave%2520the%2520way%2520for%2520future%250Aresearch%2520in%2520leveraging%2520GenAI%2520for%2520network%2520monitoring%2520and%2520management.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08576v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mapping%20the%20Landscape%20of%20Generative%20AI%20in%20Network%20Monitoring%20and%0A%20%20Management&entry.906535625=Giampaolo%20Bovenzi%20and%20Francesco%20Cerasuolo%20and%20Domenico%20Ciuonzo%20and%20Davide%20Di%20Monda%20and%20Idio%20Guarino%20and%20Antonio%20Montieri%20and%20Valerio%20Persico%20and%20Antonio%20Pescap%C3%A8&entry.1292438233=%20%20Generative%20Artificial%20Intelligence%20%28GenAI%29%20models%20such%20as%20LLMs%2C%20GPTs%2C%20and%0ADiffusion%20Models%20have%20recently%20gained%20widespread%20attention%20from%20both%20the%0Aresearch%20and%20the%20industrial%20communities.%20This%20survey%20explores%20their%20application%0Ain%20network%20monitoring%20and%20management%2C%20focusing%20on%20prominent%20use%20cases%2C%20as%20well%0Aas%20challenges%20and%20opportunities.%20We%20discuss%20how%20network%20traffic%20generation%20and%0Aclassification%2C%20network%20intrusion%20detection%2C%20networked%20system%20log%20analysis%2C%20and%0Anetwork%20digital%20assistance%20can%20benefit%20from%20the%20use%20of%20GenAI%20models.%0AAdditionally%2C%20we%20provide%20an%20overview%20of%20the%20available%20GenAI%20models%2C%20datasets%0Afor%20large-scale%20training%20phases%2C%20and%20platforms%20for%20the%20development%20of%20such%0Amodels.%20Finally%2C%20we%20discuss%20research%20directions%20that%20potentially%20mitigate%20the%0Aroadblocks%20to%20the%20adoption%20of%20GenAI%20for%20network%20monitoring%20and%20management.%20Our%0Ainvestigation%20aims%20to%20map%20the%20current%20landscape%20and%20pave%20the%20way%20for%20future%0Aresearch%20in%20leveraging%20GenAI%20for%20network%20monitoring%20and%20management.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08576v1&entry.124074799=Read"},
{"title": "SoK: A Classification for AI-driven Personalized Privacy Assistants", "author": "Victor Morel and Leonardo Iwaya and Simone Fischer-H\u00fcbner", "abstract": "  To help users make privacy-related decisions, personalized privacy assistants\nbased on AI technology have been developed in recent years. These AI-driven\nPersonalized Privacy Assistants (AI-driven PPAs) can reap significant benefits\nfor users, who may otherwise struggle to make decisions regarding their\npersonal data in environments saturated with privacy-related decision requests.\nHowever, no study systematically inquired about the features of these AI-driven\nPPAs, their underlying technologies, or the accuracy of their decisions. To\nfill this gap, we present a Systematization of Knowledge (SoK) to map the\nexisting solutions found in the scientific literature. We screened 1697 unique\nresearch papers over the last decade (2013-2023), constructing a classification\nfrom 39 included papers. As a result, this SoK reviews several aspects of\nexisting research on AI-driven PPAs in terms of types of publications,\ncontributions, methodological quality, and other quantitative insights.\nFurthermore, we provide a comprehensive classification for AI-driven PPAs,\ndelving into their architectural choices, system contexts, types of AI used,\ndata sources, types of decisions, and control over decisions, among other\nfacets. Based on our SoK, we further underline the research gaps and challenges\nand formulate recommendations for the design and development of AI-driven PPAs\nas well as avenues for future research.\n", "link": "http://arxiv.org/abs/2502.07693v2", "date": "2025-02-12", "relevancy": 1.6465, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4248}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4156}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4024}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SoK%3A%20A%20Classification%20for%20AI-driven%20Personalized%20Privacy%20Assistants&body=Title%3A%20SoK%3A%20A%20Classification%20for%20AI-driven%20Personalized%20Privacy%20Assistants%0AAuthor%3A%20Victor%20Morel%20and%20Leonardo%20Iwaya%20and%20Simone%20Fischer-H%C3%BCbner%0AAbstract%3A%20%20%20To%20help%20users%20make%20privacy-related%20decisions%2C%20personalized%20privacy%20assistants%0Abased%20on%20AI%20technology%20have%20been%20developed%20in%20recent%20years.%20These%20AI-driven%0APersonalized%20Privacy%20Assistants%20%28AI-driven%20PPAs%29%20can%20reap%20significant%20benefits%0Afor%20users%2C%20who%20may%20otherwise%20struggle%20to%20make%20decisions%20regarding%20their%0Apersonal%20data%20in%20environments%20saturated%20with%20privacy-related%20decision%20requests.%0AHowever%2C%20no%20study%20systematically%20inquired%20about%20the%20features%20of%20these%20AI-driven%0APPAs%2C%20their%20underlying%20technologies%2C%20or%20the%20accuracy%20of%20their%20decisions.%20To%0Afill%20this%20gap%2C%20we%20present%20a%20Systematization%20of%20Knowledge%20%28SoK%29%20to%20map%20the%0Aexisting%20solutions%20found%20in%20the%20scientific%20literature.%20We%20screened%201697%20unique%0Aresearch%20papers%20over%20the%20last%20decade%20%282013-2023%29%2C%20constructing%20a%20classification%0Afrom%2039%20included%20papers.%20As%20a%20result%2C%20this%20SoK%20reviews%20several%20aspects%20of%0Aexisting%20research%20on%20AI-driven%20PPAs%20in%20terms%20of%20types%20of%20publications%2C%0Acontributions%2C%20methodological%20quality%2C%20and%20other%20quantitative%20insights.%0AFurthermore%2C%20we%20provide%20a%20comprehensive%20classification%20for%20AI-driven%20PPAs%2C%0Adelving%20into%20their%20architectural%20choices%2C%20system%20contexts%2C%20types%20of%20AI%20used%2C%0Adata%20sources%2C%20types%20of%20decisions%2C%20and%20control%20over%20decisions%2C%20among%20other%0Afacets.%20Based%20on%20our%20SoK%2C%20we%20further%20underline%20the%20research%20gaps%20and%20challenges%0Aand%20formulate%20recommendations%20for%20the%20design%20and%20development%20of%20AI-driven%20PPAs%0Aas%20well%20as%20avenues%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07693v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoK%253A%2520A%2520Classification%2520for%2520AI-driven%2520Personalized%2520Privacy%2520Assistants%26entry.906535625%3DVictor%2520Morel%2520and%2520Leonardo%2520Iwaya%2520and%2520Simone%2520Fischer-H%25C3%25BCbner%26entry.1292438233%3D%2520%2520To%2520help%2520users%2520make%2520privacy-related%2520decisions%252C%2520personalized%2520privacy%2520assistants%250Abased%2520on%2520AI%2520technology%2520have%2520been%2520developed%2520in%2520recent%2520years.%2520These%2520AI-driven%250APersonalized%2520Privacy%2520Assistants%2520%2528AI-driven%2520PPAs%2529%2520can%2520reap%2520significant%2520benefits%250Afor%2520users%252C%2520who%2520may%2520otherwise%2520struggle%2520to%2520make%2520decisions%2520regarding%2520their%250Apersonal%2520data%2520in%2520environments%2520saturated%2520with%2520privacy-related%2520decision%2520requests.%250AHowever%252C%2520no%2520study%2520systematically%2520inquired%2520about%2520the%2520features%2520of%2520these%2520AI-driven%250APPAs%252C%2520their%2520underlying%2520technologies%252C%2520or%2520the%2520accuracy%2520of%2520their%2520decisions.%2520To%250Afill%2520this%2520gap%252C%2520we%2520present%2520a%2520Systematization%2520of%2520Knowledge%2520%2528SoK%2529%2520to%2520map%2520the%250Aexisting%2520solutions%2520found%2520in%2520the%2520scientific%2520literature.%2520We%2520screened%25201697%2520unique%250Aresearch%2520papers%2520over%2520the%2520last%2520decade%2520%25282013-2023%2529%252C%2520constructing%2520a%2520classification%250Afrom%252039%2520included%2520papers.%2520As%2520a%2520result%252C%2520this%2520SoK%2520reviews%2520several%2520aspects%2520of%250Aexisting%2520research%2520on%2520AI-driven%2520PPAs%2520in%2520terms%2520of%2520types%2520of%2520publications%252C%250Acontributions%252C%2520methodological%2520quality%252C%2520and%2520other%2520quantitative%2520insights.%250AFurthermore%252C%2520we%2520provide%2520a%2520comprehensive%2520classification%2520for%2520AI-driven%2520PPAs%252C%250Adelving%2520into%2520their%2520architectural%2520choices%252C%2520system%2520contexts%252C%2520types%2520of%2520AI%2520used%252C%250Adata%2520sources%252C%2520types%2520of%2520decisions%252C%2520and%2520control%2520over%2520decisions%252C%2520among%2520other%250Afacets.%2520Based%2520on%2520our%2520SoK%252C%2520we%2520further%2520underline%2520the%2520research%2520gaps%2520and%2520challenges%250Aand%2520formulate%2520recommendations%2520for%2520the%2520design%2520and%2520development%2520of%2520AI-driven%2520PPAs%250Aas%2520well%2520as%2520avenues%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07693v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SoK%3A%20A%20Classification%20for%20AI-driven%20Personalized%20Privacy%20Assistants&entry.906535625=Victor%20Morel%20and%20Leonardo%20Iwaya%20and%20Simone%20Fischer-H%C3%BCbner&entry.1292438233=%20%20To%20help%20users%20make%20privacy-related%20decisions%2C%20personalized%20privacy%20assistants%0Abased%20on%20AI%20technology%20have%20been%20developed%20in%20recent%20years.%20These%20AI-driven%0APersonalized%20Privacy%20Assistants%20%28AI-driven%20PPAs%29%20can%20reap%20significant%20benefits%0Afor%20users%2C%20who%20may%20otherwise%20struggle%20to%20make%20decisions%20regarding%20their%0Apersonal%20data%20in%20environments%20saturated%20with%20privacy-related%20decision%20requests.%0AHowever%2C%20no%20study%20systematically%20inquired%20about%20the%20features%20of%20these%20AI-driven%0APPAs%2C%20their%20underlying%20technologies%2C%20or%20the%20accuracy%20of%20their%20decisions.%20To%0Afill%20this%20gap%2C%20we%20present%20a%20Systematization%20of%20Knowledge%20%28SoK%29%20to%20map%20the%0Aexisting%20solutions%20found%20in%20the%20scientific%20literature.%20We%20screened%201697%20unique%0Aresearch%20papers%20over%20the%20last%20decade%20%282013-2023%29%2C%20constructing%20a%20classification%0Afrom%2039%20included%20papers.%20As%20a%20result%2C%20this%20SoK%20reviews%20several%20aspects%20of%0Aexisting%20research%20on%20AI-driven%20PPAs%20in%20terms%20of%20types%20of%20publications%2C%0Acontributions%2C%20methodological%20quality%2C%20and%20other%20quantitative%20insights.%0AFurthermore%2C%20we%20provide%20a%20comprehensive%20classification%20for%20AI-driven%20PPAs%2C%0Adelving%20into%20their%20architectural%20choices%2C%20system%20contexts%2C%20types%20of%20AI%20used%2C%0Adata%20sources%2C%20types%20of%20decisions%2C%20and%20control%20over%20decisions%2C%20among%20other%0Afacets.%20Based%20on%20our%20SoK%2C%20we%20further%20underline%20the%20research%20gaps%20and%20challenges%0Aand%20formulate%20recommendations%20for%20the%20design%20and%20development%20of%20AI-driven%20PPAs%0Aas%20well%20as%20avenues%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07693v2&entry.124074799=Read"},
{"title": "BCDDM: Branch-Corrected Denoising Diffusion Model for Black Hole Image\n  Generation", "author": "Ao liu and Zelin Zhang and Songbai Chen and Cuihong Wen", "abstract": "  The properties of black holes and accretion flows can be inferred by fitting\nEvent Horizon Telescope (EHT) data to simulated images generated through\ngeneral relativistic ray tracing (GRRT). However, due to the computationally\nintensive nature of GRRT, the efficiency of generating specific radiation flux\nimages needs to be improved. This paper introduces the Branch Correction\nDenoising Diffusion Model (BCDDM), which uses a branch correction mechanism and\na weighted mixed loss function to improve the accuracy of generated black hole\nimages based on seven physical parameters of the radiatively inefficient\naccretion flow (RIAF) model. Our experiments show a strong correlation between\nthe generated images and their physical parameters. By enhancing the GRRT\ndataset with BCDDM-generated images and using ResNet50 for parameter\nregression, we achieve significant improvements in parameter prediction\nperformance. This approach reduces computational costs and provides a faster,\nmore efficient method for dataset expansion, parameter estimation, and model\nfitting.\n", "link": "http://arxiv.org/abs/2502.08528v1", "date": "2025-02-12", "relevancy": 1.6383, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5792}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5496}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BCDDM%3A%20Branch-Corrected%20Denoising%20Diffusion%20Model%20for%20Black%20Hole%20Image%0A%20%20Generation&body=Title%3A%20BCDDM%3A%20Branch-Corrected%20Denoising%20Diffusion%20Model%20for%20Black%20Hole%20Image%0A%20%20Generation%0AAuthor%3A%20Ao%20liu%20and%20Zelin%20Zhang%20and%20Songbai%20Chen%20and%20Cuihong%20Wen%0AAbstract%3A%20%20%20The%20properties%20of%20black%20holes%20and%20accretion%20flows%20can%20be%20inferred%20by%20fitting%0AEvent%20Horizon%20Telescope%20%28EHT%29%20data%20to%20simulated%20images%20generated%20through%0Ageneral%20relativistic%20ray%20tracing%20%28GRRT%29.%20However%2C%20due%20to%20the%20computationally%0Aintensive%20nature%20of%20GRRT%2C%20the%20efficiency%20of%20generating%20specific%20radiation%20flux%0Aimages%20needs%20to%20be%20improved.%20This%20paper%20introduces%20the%20Branch%20Correction%0ADenoising%20Diffusion%20Model%20%28BCDDM%29%2C%20which%20uses%20a%20branch%20correction%20mechanism%20and%0Aa%20weighted%20mixed%20loss%20function%20to%20improve%20the%20accuracy%20of%20generated%20black%20hole%0Aimages%20based%20on%20seven%20physical%20parameters%20of%20the%20radiatively%20inefficient%0Aaccretion%20flow%20%28RIAF%29%20model.%20Our%20experiments%20show%20a%20strong%20correlation%20between%0Athe%20generated%20images%20and%20their%20physical%20parameters.%20By%20enhancing%20the%20GRRT%0Adataset%20with%20BCDDM-generated%20images%20and%20using%20ResNet50%20for%20parameter%0Aregression%2C%20we%20achieve%20significant%20improvements%20in%20parameter%20prediction%0Aperformance.%20This%20approach%20reduces%20computational%20costs%20and%20provides%20a%20faster%2C%0Amore%20efficient%20method%20for%20dataset%20expansion%2C%20parameter%20estimation%2C%20and%20model%0Afitting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08528v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBCDDM%253A%2520Branch-Corrected%2520Denoising%2520Diffusion%2520Model%2520for%2520Black%2520Hole%2520Image%250A%2520%2520Generation%26entry.906535625%3DAo%2520liu%2520and%2520Zelin%2520Zhang%2520and%2520Songbai%2520Chen%2520and%2520Cuihong%2520Wen%26entry.1292438233%3D%2520%2520The%2520properties%2520of%2520black%2520holes%2520and%2520accretion%2520flows%2520can%2520be%2520inferred%2520by%2520fitting%250AEvent%2520Horizon%2520Telescope%2520%2528EHT%2529%2520data%2520to%2520simulated%2520images%2520generated%2520through%250Ageneral%2520relativistic%2520ray%2520tracing%2520%2528GRRT%2529.%2520However%252C%2520due%2520to%2520the%2520computationally%250Aintensive%2520nature%2520of%2520GRRT%252C%2520the%2520efficiency%2520of%2520generating%2520specific%2520radiation%2520flux%250Aimages%2520needs%2520to%2520be%2520improved.%2520This%2520paper%2520introduces%2520the%2520Branch%2520Correction%250ADenoising%2520Diffusion%2520Model%2520%2528BCDDM%2529%252C%2520which%2520uses%2520a%2520branch%2520correction%2520mechanism%2520and%250Aa%2520weighted%2520mixed%2520loss%2520function%2520to%2520improve%2520the%2520accuracy%2520of%2520generated%2520black%2520hole%250Aimages%2520based%2520on%2520seven%2520physical%2520parameters%2520of%2520the%2520radiatively%2520inefficient%250Aaccretion%2520flow%2520%2528RIAF%2529%2520model.%2520Our%2520experiments%2520show%2520a%2520strong%2520correlation%2520between%250Athe%2520generated%2520images%2520and%2520their%2520physical%2520parameters.%2520By%2520enhancing%2520the%2520GRRT%250Adataset%2520with%2520BCDDM-generated%2520images%2520and%2520using%2520ResNet50%2520for%2520parameter%250Aregression%252C%2520we%2520achieve%2520significant%2520improvements%2520in%2520parameter%2520prediction%250Aperformance.%2520This%2520approach%2520reduces%2520computational%2520costs%2520and%2520provides%2520a%2520faster%252C%250Amore%2520efficient%2520method%2520for%2520dataset%2520expansion%252C%2520parameter%2520estimation%252C%2520and%2520model%250Afitting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08528v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BCDDM%3A%20Branch-Corrected%20Denoising%20Diffusion%20Model%20for%20Black%20Hole%20Image%0A%20%20Generation&entry.906535625=Ao%20liu%20and%20Zelin%20Zhang%20and%20Songbai%20Chen%20and%20Cuihong%20Wen&entry.1292438233=%20%20The%20properties%20of%20black%20holes%20and%20accretion%20flows%20can%20be%20inferred%20by%20fitting%0AEvent%20Horizon%20Telescope%20%28EHT%29%20data%20to%20simulated%20images%20generated%20through%0Ageneral%20relativistic%20ray%20tracing%20%28GRRT%29.%20However%2C%20due%20to%20the%20computationally%0Aintensive%20nature%20of%20GRRT%2C%20the%20efficiency%20of%20generating%20specific%20radiation%20flux%0Aimages%20needs%20to%20be%20improved.%20This%20paper%20introduces%20the%20Branch%20Correction%0ADenoising%20Diffusion%20Model%20%28BCDDM%29%2C%20which%20uses%20a%20branch%20correction%20mechanism%20and%0Aa%20weighted%20mixed%20loss%20function%20to%20improve%20the%20accuracy%20of%20generated%20black%20hole%0Aimages%20based%20on%20seven%20physical%20parameters%20of%20the%20radiatively%20inefficient%0Aaccretion%20flow%20%28RIAF%29%20model.%20Our%20experiments%20show%20a%20strong%20correlation%20between%0Athe%20generated%20images%20and%20their%20physical%20parameters.%20By%20enhancing%20the%20GRRT%0Adataset%20with%20BCDDM-generated%20images%20and%20using%20ResNet50%20for%20parameter%0Aregression%2C%20we%20achieve%20significant%20improvements%20in%20parameter%20prediction%0Aperformance.%20This%20approach%20reduces%20computational%20costs%20and%20provides%20a%20faster%2C%0Amore%20efficient%20method%20for%20dataset%20expansion%2C%20parameter%20estimation%2C%20and%20model%0Afitting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08528v1&entry.124074799=Read"},
{"title": "URSA: Understanding and Verifying Chain-of-thought Reasoning in\n  Multimodal Mathematics", "author": "Ruilin Luo and Zhuofan Zheng and Yifan Wang and Yiyao Yu and Xinzhe Ni and Zicheng Lin and Jin Zeng and Yujiu Yang", "abstract": "  Chain-of-Thought (CoT) reasoning is widely used to enhance the mathematical\nreasoning capabilities of large language models (LLMs). The introduction of\nprocess supervision for CoT trajectories has sparked discussions on improving\ntest-time scaling, thereby unlocking the System 2-style thinking capabilities\nof these models. However, in multimodal mathematical reasoning, the scarcity of\nhigh-quality CoT training data has hindered existing models from achieving both\ndeliberate reasoning and fine-grained verification. In this work, we propose a\nnovel framework that introduces System 2-style thinking to multimodal\nmathematical reasoning. We introduce a three-module CoT data synthesis process\nthat integrates CoT distillation, trajectory-format rewriting, and format\nunification. This process generates MMathCoT-1M, a high-quality CoT reasoning\ninstruction fine-tuning dataset. Furthermore, we implement a dual-view\ntrajectory labeling automation that targets both visual grounding fidelity and\ndeductive chain validity, resulting in the DualMath-1.1M dataset. The URSA-8B\nmodel, trained on MMathCoT-1M, achieves new state-of-the-art (SOTA) performance\namong similarly sized multimodal LLMs on six popular reasoning benchmarks.\nTraining URSA-8B further on the DualMath-1.1M dataset yields URSA-RM-8B, a\nverifier that enhances URSA-8B's test-time performance and surpasses strong\nclosed-source multimodal MLLMs like GPT-4o. The model weights, training data,\nand code have been open-sourced: https://github.com/URSA-MATH/URSA-MATH.\n", "link": "http://arxiv.org/abs/2501.04686v3", "date": "2025-02-12", "relevancy": 1.5877, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5452}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5297}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20URSA%3A%20Understanding%20and%20Verifying%20Chain-of-thought%20Reasoning%20in%0A%20%20Multimodal%20Mathematics&body=Title%3A%20URSA%3A%20Understanding%20and%20Verifying%20Chain-of-thought%20Reasoning%20in%0A%20%20Multimodal%20Mathematics%0AAuthor%3A%20Ruilin%20Luo%20and%20Zhuofan%20Zheng%20and%20Yifan%20Wang%20and%20Yiyao%20Yu%20and%20Xinzhe%20Ni%20and%20Zicheng%20Lin%20and%20Jin%20Zeng%20and%20Yujiu%20Yang%0AAbstract%3A%20%20%20Chain-of-Thought%20%28CoT%29%20reasoning%20is%20widely%20used%20to%20enhance%20the%20mathematical%0Areasoning%20capabilities%20of%20large%20language%20models%20%28LLMs%29.%20The%20introduction%20of%0Aprocess%20supervision%20for%20CoT%20trajectories%20has%20sparked%20discussions%20on%20improving%0Atest-time%20scaling%2C%20thereby%20unlocking%20the%20System%202-style%20thinking%20capabilities%0Aof%20these%20models.%20However%2C%20in%20multimodal%20mathematical%20reasoning%2C%20the%20scarcity%20of%0Ahigh-quality%20CoT%20training%20data%20has%20hindered%20existing%20models%20from%20achieving%20both%0Adeliberate%20reasoning%20and%20fine-grained%20verification.%20In%20this%20work%2C%20we%20propose%20a%0Anovel%20framework%20that%20introduces%20System%202-style%20thinking%20to%20multimodal%0Amathematical%20reasoning.%20We%20introduce%20a%20three-module%20CoT%20data%20synthesis%20process%0Athat%20integrates%20CoT%20distillation%2C%20trajectory-format%20rewriting%2C%20and%20format%0Aunification.%20This%20process%20generates%20MMathCoT-1M%2C%20a%20high-quality%20CoT%20reasoning%0Ainstruction%20fine-tuning%20dataset.%20Furthermore%2C%20we%20implement%20a%20dual-view%0Atrajectory%20labeling%20automation%20that%20targets%20both%20visual%20grounding%20fidelity%20and%0Adeductive%20chain%20validity%2C%20resulting%20in%20the%20DualMath-1.1M%20dataset.%20The%20URSA-8B%0Amodel%2C%20trained%20on%20MMathCoT-1M%2C%20achieves%20new%20state-of-the-art%20%28SOTA%29%20performance%0Aamong%20similarly%20sized%20multimodal%20LLMs%20on%20six%20popular%20reasoning%20benchmarks.%0ATraining%20URSA-8B%20further%20on%20the%20DualMath-1.1M%20dataset%20yields%20URSA-RM-8B%2C%20a%0Averifier%20that%20enhances%20URSA-8B%27s%20test-time%20performance%20and%20surpasses%20strong%0Aclosed-source%20multimodal%20MLLMs%20like%20GPT-4o.%20The%20model%20weights%2C%20training%20data%2C%0Aand%20code%20have%20been%20open-sourced%3A%20https%3A//github.com/URSA-MATH/URSA-MATH.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04686v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DURSA%253A%2520Understanding%2520and%2520Verifying%2520Chain-of-thought%2520Reasoning%2520in%250A%2520%2520Multimodal%2520Mathematics%26entry.906535625%3DRuilin%2520Luo%2520and%2520Zhuofan%2520Zheng%2520and%2520Yifan%2520Wang%2520and%2520Yiyao%2520Yu%2520and%2520Xinzhe%2520Ni%2520and%2520Zicheng%2520Lin%2520and%2520Jin%2520Zeng%2520and%2520Yujiu%2520Yang%26entry.1292438233%3D%2520%2520Chain-of-Thought%2520%2528CoT%2529%2520reasoning%2520is%2520widely%2520used%2520to%2520enhance%2520the%2520mathematical%250Areasoning%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520The%2520introduction%2520of%250Aprocess%2520supervision%2520for%2520CoT%2520trajectories%2520has%2520sparked%2520discussions%2520on%2520improving%250Atest-time%2520scaling%252C%2520thereby%2520unlocking%2520the%2520System%25202-style%2520thinking%2520capabilities%250Aof%2520these%2520models.%2520However%252C%2520in%2520multimodal%2520mathematical%2520reasoning%252C%2520the%2520scarcity%2520of%250Ahigh-quality%2520CoT%2520training%2520data%2520has%2520hindered%2520existing%2520models%2520from%2520achieving%2520both%250Adeliberate%2520reasoning%2520and%2520fine-grained%2520verification.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250Anovel%2520framework%2520that%2520introduces%2520System%25202-style%2520thinking%2520to%2520multimodal%250Amathematical%2520reasoning.%2520We%2520introduce%2520a%2520three-module%2520CoT%2520data%2520synthesis%2520process%250Athat%2520integrates%2520CoT%2520distillation%252C%2520trajectory-format%2520rewriting%252C%2520and%2520format%250Aunification.%2520This%2520process%2520generates%2520MMathCoT-1M%252C%2520a%2520high-quality%2520CoT%2520reasoning%250Ainstruction%2520fine-tuning%2520dataset.%2520Furthermore%252C%2520we%2520implement%2520a%2520dual-view%250Atrajectory%2520labeling%2520automation%2520that%2520targets%2520both%2520visual%2520grounding%2520fidelity%2520and%250Adeductive%2520chain%2520validity%252C%2520resulting%2520in%2520the%2520DualMath-1.1M%2520dataset.%2520The%2520URSA-8B%250Amodel%252C%2520trained%2520on%2520MMathCoT-1M%252C%2520achieves%2520new%2520state-of-the-art%2520%2528SOTA%2529%2520performance%250Aamong%2520similarly%2520sized%2520multimodal%2520LLMs%2520on%2520six%2520popular%2520reasoning%2520benchmarks.%250ATraining%2520URSA-8B%2520further%2520on%2520the%2520DualMath-1.1M%2520dataset%2520yields%2520URSA-RM-8B%252C%2520a%250Averifier%2520that%2520enhances%2520URSA-8B%2527s%2520test-time%2520performance%2520and%2520surpasses%2520strong%250Aclosed-source%2520multimodal%2520MLLMs%2520like%2520GPT-4o.%2520The%2520model%2520weights%252C%2520training%2520data%252C%250Aand%2520code%2520have%2520been%2520open-sourced%253A%2520https%253A//github.com/URSA-MATH/URSA-MATH.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04686v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=URSA%3A%20Understanding%20and%20Verifying%20Chain-of-thought%20Reasoning%20in%0A%20%20Multimodal%20Mathematics&entry.906535625=Ruilin%20Luo%20and%20Zhuofan%20Zheng%20and%20Yifan%20Wang%20and%20Yiyao%20Yu%20and%20Xinzhe%20Ni%20and%20Zicheng%20Lin%20and%20Jin%20Zeng%20and%20Yujiu%20Yang&entry.1292438233=%20%20Chain-of-Thought%20%28CoT%29%20reasoning%20is%20widely%20used%20to%20enhance%20the%20mathematical%0Areasoning%20capabilities%20of%20large%20language%20models%20%28LLMs%29.%20The%20introduction%20of%0Aprocess%20supervision%20for%20CoT%20trajectories%20has%20sparked%20discussions%20on%20improving%0Atest-time%20scaling%2C%20thereby%20unlocking%20the%20System%202-style%20thinking%20capabilities%0Aof%20these%20models.%20However%2C%20in%20multimodal%20mathematical%20reasoning%2C%20the%20scarcity%20of%0Ahigh-quality%20CoT%20training%20data%20has%20hindered%20existing%20models%20from%20achieving%20both%0Adeliberate%20reasoning%20and%20fine-grained%20verification.%20In%20this%20work%2C%20we%20propose%20a%0Anovel%20framework%20that%20introduces%20System%202-style%20thinking%20to%20multimodal%0Amathematical%20reasoning.%20We%20introduce%20a%20three-module%20CoT%20data%20synthesis%20process%0Athat%20integrates%20CoT%20distillation%2C%20trajectory-format%20rewriting%2C%20and%20format%0Aunification.%20This%20process%20generates%20MMathCoT-1M%2C%20a%20high-quality%20CoT%20reasoning%0Ainstruction%20fine-tuning%20dataset.%20Furthermore%2C%20we%20implement%20a%20dual-view%0Atrajectory%20labeling%20automation%20that%20targets%20both%20visual%20grounding%20fidelity%20and%0Adeductive%20chain%20validity%2C%20resulting%20in%20the%20DualMath-1.1M%20dataset.%20The%20URSA-8B%0Amodel%2C%20trained%20on%20MMathCoT-1M%2C%20achieves%20new%20state-of-the-art%20%28SOTA%29%20performance%0Aamong%20similarly%20sized%20multimodal%20LLMs%20on%20six%20popular%20reasoning%20benchmarks.%0ATraining%20URSA-8B%20further%20on%20the%20DualMath-1.1M%20dataset%20yields%20URSA-RM-8B%2C%20a%0Averifier%20that%20enhances%20URSA-8B%27s%20test-time%20performance%20and%20surpasses%20strong%0Aclosed-source%20multimodal%20MLLMs%20like%20GPT-4o.%20The%20model%20weights%2C%20training%20data%2C%0Aand%20code%20have%20been%20open-sourced%3A%20https%3A//github.com/URSA-MATH/URSA-MATH.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04686v3&entry.124074799=Read"},
{"title": "Algorithmic Persuasion Through Simulation", "author": "Keegan Harris and Nicole Immorlica and Brendan Lucier and Aleksandrs Slivkins", "abstract": "  We study a Bayesian persuasion game where a sender wants to persuade a\nreceiver to take a binary action, such as purchasing a product. The sender is\ninformed about the (real-valued) state of the world, such as the quality of the\nproduct, but only has limited information about the receiver's beliefs and\nutilities. Motivated by customer surveys, user studies, and recent advances in\nAI, we allow the sender to learn more about the receiver by querying an oracle\nthat simulates the receiver's behavior. After a fixed number of queries, the\nsender commits to a messaging policy and the receiver takes the action that\nmaximizes her expected utility given the message she receives. We characterize\nthe sender's optimal messaging policy given any distribution over receiver\ntypes. We then design a polynomial-time querying algorithm that optimizes the\nsender's expected utility in this game. We also consider approximate oracles,\nmore general query structures, and costly queries.\n", "link": "http://arxiv.org/abs/2311.18138v5", "date": "2025-02-12", "relevancy": 1.5775, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4735}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.3786}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.3785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Algorithmic%20Persuasion%20Through%20Simulation&body=Title%3A%20Algorithmic%20Persuasion%20Through%20Simulation%0AAuthor%3A%20Keegan%20Harris%20and%20Nicole%20Immorlica%20and%20Brendan%20Lucier%20and%20Aleksandrs%20Slivkins%0AAbstract%3A%20%20%20We%20study%20a%20Bayesian%20persuasion%20game%20where%20a%20sender%20wants%20to%20persuade%20a%0Areceiver%20to%20take%20a%20binary%20action%2C%20such%20as%20purchasing%20a%20product.%20The%20sender%20is%0Ainformed%20about%20the%20%28real-valued%29%20state%20of%20the%20world%2C%20such%20as%20the%20quality%20of%20the%0Aproduct%2C%20but%20only%20has%20limited%20information%20about%20the%20receiver%27s%20beliefs%20and%0Autilities.%20Motivated%20by%20customer%20surveys%2C%20user%20studies%2C%20and%20recent%20advances%20in%0AAI%2C%20we%20allow%20the%20sender%20to%20learn%20more%20about%20the%20receiver%20by%20querying%20an%20oracle%0Athat%20simulates%20the%20receiver%27s%20behavior.%20After%20a%20fixed%20number%20of%20queries%2C%20the%0Asender%20commits%20to%20a%20messaging%20policy%20and%20the%20receiver%20takes%20the%20action%20that%0Amaximizes%20her%20expected%20utility%20given%20the%20message%20she%20receives.%20We%20characterize%0Athe%20sender%27s%20optimal%20messaging%20policy%20given%20any%20distribution%20over%20receiver%0Atypes.%20We%20then%20design%20a%20polynomial-time%20querying%20algorithm%20that%20optimizes%20the%0Asender%27s%20expected%20utility%20in%20this%20game.%20We%20also%20consider%20approximate%20oracles%2C%0Amore%20general%20query%20structures%2C%20and%20costly%20queries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.18138v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlgorithmic%2520Persuasion%2520Through%2520Simulation%26entry.906535625%3DKeegan%2520Harris%2520and%2520Nicole%2520Immorlica%2520and%2520Brendan%2520Lucier%2520and%2520Aleksandrs%2520Slivkins%26entry.1292438233%3D%2520%2520We%2520study%2520a%2520Bayesian%2520persuasion%2520game%2520where%2520a%2520sender%2520wants%2520to%2520persuade%2520a%250Areceiver%2520to%2520take%2520a%2520binary%2520action%252C%2520such%2520as%2520purchasing%2520a%2520product.%2520The%2520sender%2520is%250Ainformed%2520about%2520the%2520%2528real-valued%2529%2520state%2520of%2520the%2520world%252C%2520such%2520as%2520the%2520quality%2520of%2520the%250Aproduct%252C%2520but%2520only%2520has%2520limited%2520information%2520about%2520the%2520receiver%2527s%2520beliefs%2520and%250Autilities.%2520Motivated%2520by%2520customer%2520surveys%252C%2520user%2520studies%252C%2520and%2520recent%2520advances%2520in%250AAI%252C%2520we%2520allow%2520the%2520sender%2520to%2520learn%2520more%2520about%2520the%2520receiver%2520by%2520querying%2520an%2520oracle%250Athat%2520simulates%2520the%2520receiver%2527s%2520behavior.%2520After%2520a%2520fixed%2520number%2520of%2520queries%252C%2520the%250Asender%2520commits%2520to%2520a%2520messaging%2520policy%2520and%2520the%2520receiver%2520takes%2520the%2520action%2520that%250Amaximizes%2520her%2520expected%2520utility%2520given%2520the%2520message%2520she%2520receives.%2520We%2520characterize%250Athe%2520sender%2527s%2520optimal%2520messaging%2520policy%2520given%2520any%2520distribution%2520over%2520receiver%250Atypes.%2520We%2520then%2520design%2520a%2520polynomial-time%2520querying%2520algorithm%2520that%2520optimizes%2520the%250Asender%2527s%2520expected%2520utility%2520in%2520this%2520game.%2520We%2520also%2520consider%2520approximate%2520oracles%252C%250Amore%2520general%2520query%2520structures%252C%2520and%2520costly%2520queries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.18138v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Algorithmic%20Persuasion%20Through%20Simulation&entry.906535625=Keegan%20Harris%20and%20Nicole%20Immorlica%20and%20Brendan%20Lucier%20and%20Aleksandrs%20Slivkins&entry.1292438233=%20%20We%20study%20a%20Bayesian%20persuasion%20game%20where%20a%20sender%20wants%20to%20persuade%20a%0Areceiver%20to%20take%20a%20binary%20action%2C%20such%20as%20purchasing%20a%20product.%20The%20sender%20is%0Ainformed%20about%20the%20%28real-valued%29%20state%20of%20the%20world%2C%20such%20as%20the%20quality%20of%20the%0Aproduct%2C%20but%20only%20has%20limited%20information%20about%20the%20receiver%27s%20beliefs%20and%0Autilities.%20Motivated%20by%20customer%20surveys%2C%20user%20studies%2C%20and%20recent%20advances%20in%0AAI%2C%20we%20allow%20the%20sender%20to%20learn%20more%20about%20the%20receiver%20by%20querying%20an%20oracle%0Athat%20simulates%20the%20receiver%27s%20behavior.%20After%20a%20fixed%20number%20of%20queries%2C%20the%0Asender%20commits%20to%20a%20messaging%20policy%20and%20the%20receiver%20takes%20the%20action%20that%0Amaximizes%20her%20expected%20utility%20given%20the%20message%20she%20receives.%20We%20characterize%0Athe%20sender%27s%20optimal%20messaging%20policy%20given%20any%20distribution%20over%20receiver%0Atypes.%20We%20then%20design%20a%20polynomial-time%20querying%20algorithm%20that%20optimizes%20the%0Asender%27s%20expected%20utility%20in%20this%20game.%20We%20also%20consider%20approximate%20oracles%2C%0Amore%20general%20query%20structures%2C%20and%20costly%20queries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.18138v5&entry.124074799=Read"},
{"title": "Brain Latent Progression: Individual-based Spatiotemporal Disease\n  Progression on 3D Brain MRIs via Latent Diffusion", "author": "Lemuel Puglisi and Daniel C. Alexander and Daniele Rav\u00ec", "abstract": "  The growing availability of longitudinal Magnetic Resonance Imaging (MRI)\ndatasets has facilitated Artificial Intelligence (AI)-driven modeling of\ndisease progression, making it possible to predict future medical scans for\nindividual patients. However, despite significant advancements in AI, current\nmethods continue to face challenges including achieving patient-specific\nindividualization, ensuring spatiotemporal consistency, efficiently utilizing\nlongitudinal data, and managing the substantial memory demands of 3D scans. To\naddress these challenges, we propose Brain Latent Progression (BrLP), a novel\nspatiotemporal model designed to predict individual-level disease progression\nin 3D brain MRIs. The key contributions in BrLP are fourfold: (i) it operates\nin a small latent space, mitigating the computational challenges posed by\nhigh-dimensional imaging data; (ii) it explicitly integrates subject metadata\nto enhance the individualization of predictions; (iii) it incorporates prior\nknowledge of disease dynamics through an auxiliary model, facilitating the\nintegration of longitudinal data; and (iv) it introduces the Latent Average\nStabilization (LAS) algorithm, which (a) enforces spatiotemporal consistency in\nthe predicted progression at inference time and (b) allows us to derive a\nmeasure of the uncertainty for the prediction. We train and evaluate BrLP on\n11,730 T1-weighted (T1w) brain MRIs from 2,805 subjects and validate its\ngeneralizability on an external test set comprising 2,257 MRIs from 962\nsubjects. Our experiments compare BrLP-generated MRI scans with real follow-up\nMRIs, demonstrating state-of-the-art accuracy compared to existing methods. The\ncode is publicly available at: https://github.com/LemuelPuglisi/BrLP.\n", "link": "http://arxiv.org/abs/2502.08560v1", "date": "2025-02-12", "relevancy": 1.562, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.544}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5143}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Brain%20Latent%20Progression%3A%20Individual-based%20Spatiotemporal%20Disease%0A%20%20Progression%20on%203D%20Brain%20MRIs%20via%20Latent%20Diffusion&body=Title%3A%20Brain%20Latent%20Progression%3A%20Individual-based%20Spatiotemporal%20Disease%0A%20%20Progression%20on%203D%20Brain%20MRIs%20via%20Latent%20Diffusion%0AAuthor%3A%20Lemuel%20Puglisi%20and%20Daniel%20C.%20Alexander%20and%20Daniele%20Rav%C3%AC%0AAbstract%3A%20%20%20The%20growing%20availability%20of%20longitudinal%20Magnetic%20Resonance%20Imaging%20%28MRI%29%0Adatasets%20has%20facilitated%20Artificial%20Intelligence%20%28AI%29-driven%20modeling%20of%0Adisease%20progression%2C%20making%20it%20possible%20to%20predict%20future%20medical%20scans%20for%0Aindividual%20patients.%20However%2C%20despite%20significant%20advancements%20in%20AI%2C%20current%0Amethods%20continue%20to%20face%20challenges%20including%20achieving%20patient-specific%0Aindividualization%2C%20ensuring%20spatiotemporal%20consistency%2C%20efficiently%20utilizing%0Alongitudinal%20data%2C%20and%20managing%20the%20substantial%20memory%20demands%20of%203D%20scans.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20Brain%20Latent%20Progression%20%28BrLP%29%2C%20a%20novel%0Aspatiotemporal%20model%20designed%20to%20predict%20individual-level%20disease%20progression%0Ain%203D%20brain%20MRIs.%20The%20key%20contributions%20in%20BrLP%20are%20fourfold%3A%20%28i%29%20it%20operates%0Ain%20a%20small%20latent%20space%2C%20mitigating%20the%20computational%20challenges%20posed%20by%0Ahigh-dimensional%20imaging%20data%3B%20%28ii%29%20it%20explicitly%20integrates%20subject%20metadata%0Ato%20enhance%20the%20individualization%20of%20predictions%3B%20%28iii%29%20it%20incorporates%20prior%0Aknowledge%20of%20disease%20dynamics%20through%20an%20auxiliary%20model%2C%20facilitating%20the%0Aintegration%20of%20longitudinal%20data%3B%20and%20%28iv%29%20it%20introduces%20the%20Latent%20Average%0AStabilization%20%28LAS%29%20algorithm%2C%20which%20%28a%29%20enforces%20spatiotemporal%20consistency%20in%0Athe%20predicted%20progression%20at%20inference%20time%20and%20%28b%29%20allows%20us%20to%20derive%20a%0Ameasure%20of%20the%20uncertainty%20for%20the%20prediction.%20We%20train%20and%20evaluate%20BrLP%20on%0A11%2C730%20T1-weighted%20%28T1w%29%20brain%20MRIs%20from%202%2C805%20subjects%20and%20validate%20its%0Ageneralizability%20on%20an%20external%20test%20set%20comprising%202%2C257%20MRIs%20from%20962%0Asubjects.%20Our%20experiments%20compare%20BrLP-generated%20MRI%20scans%20with%20real%20follow-up%0AMRIs%2C%20demonstrating%20state-of-the-art%20accuracy%20compared%20to%20existing%20methods.%20The%0Acode%20is%20publicly%20available%20at%3A%20https%3A//github.com/LemuelPuglisi/BrLP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08560v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrain%2520Latent%2520Progression%253A%2520Individual-based%2520Spatiotemporal%2520Disease%250A%2520%2520Progression%2520on%25203D%2520Brain%2520MRIs%2520via%2520Latent%2520Diffusion%26entry.906535625%3DLemuel%2520Puglisi%2520and%2520Daniel%2520C.%2520Alexander%2520and%2520Daniele%2520Rav%25C3%25AC%26entry.1292438233%3D%2520%2520The%2520growing%2520availability%2520of%2520longitudinal%2520Magnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%250Adatasets%2520has%2520facilitated%2520Artificial%2520Intelligence%2520%2528AI%2529-driven%2520modeling%2520of%250Adisease%2520progression%252C%2520making%2520it%2520possible%2520to%2520predict%2520future%2520medical%2520scans%2520for%250Aindividual%2520patients.%2520However%252C%2520despite%2520significant%2520advancements%2520in%2520AI%252C%2520current%250Amethods%2520continue%2520to%2520face%2520challenges%2520including%2520achieving%2520patient-specific%250Aindividualization%252C%2520ensuring%2520spatiotemporal%2520consistency%252C%2520efficiently%2520utilizing%250Alongitudinal%2520data%252C%2520and%2520managing%2520the%2520substantial%2520memory%2520demands%2520of%25203D%2520scans.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520Brain%2520Latent%2520Progression%2520%2528BrLP%2529%252C%2520a%2520novel%250Aspatiotemporal%2520model%2520designed%2520to%2520predict%2520individual-level%2520disease%2520progression%250Ain%25203D%2520brain%2520MRIs.%2520The%2520key%2520contributions%2520in%2520BrLP%2520are%2520fourfold%253A%2520%2528i%2529%2520it%2520operates%250Ain%2520a%2520small%2520latent%2520space%252C%2520mitigating%2520the%2520computational%2520challenges%2520posed%2520by%250Ahigh-dimensional%2520imaging%2520data%253B%2520%2528ii%2529%2520it%2520explicitly%2520integrates%2520subject%2520metadata%250Ato%2520enhance%2520the%2520individualization%2520of%2520predictions%253B%2520%2528iii%2529%2520it%2520incorporates%2520prior%250Aknowledge%2520of%2520disease%2520dynamics%2520through%2520an%2520auxiliary%2520model%252C%2520facilitating%2520the%250Aintegration%2520of%2520longitudinal%2520data%253B%2520and%2520%2528iv%2529%2520it%2520introduces%2520the%2520Latent%2520Average%250AStabilization%2520%2528LAS%2529%2520algorithm%252C%2520which%2520%2528a%2529%2520enforces%2520spatiotemporal%2520consistency%2520in%250Athe%2520predicted%2520progression%2520at%2520inference%2520time%2520and%2520%2528b%2529%2520allows%2520us%2520to%2520derive%2520a%250Ameasure%2520of%2520the%2520uncertainty%2520for%2520the%2520prediction.%2520We%2520train%2520and%2520evaluate%2520BrLP%2520on%250A11%252C730%2520T1-weighted%2520%2528T1w%2529%2520brain%2520MRIs%2520from%25202%252C805%2520subjects%2520and%2520validate%2520its%250Ageneralizability%2520on%2520an%2520external%2520test%2520set%2520comprising%25202%252C257%2520MRIs%2520from%2520962%250Asubjects.%2520Our%2520experiments%2520compare%2520BrLP-generated%2520MRI%2520scans%2520with%2520real%2520follow-up%250AMRIs%252C%2520demonstrating%2520state-of-the-art%2520accuracy%2520compared%2520to%2520existing%2520methods.%2520The%250Acode%2520is%2520publicly%2520available%2520at%253A%2520https%253A//github.com/LemuelPuglisi/BrLP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08560v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Brain%20Latent%20Progression%3A%20Individual-based%20Spatiotemporal%20Disease%0A%20%20Progression%20on%203D%20Brain%20MRIs%20via%20Latent%20Diffusion&entry.906535625=Lemuel%20Puglisi%20and%20Daniel%20C.%20Alexander%20and%20Daniele%20Rav%C3%AC&entry.1292438233=%20%20The%20growing%20availability%20of%20longitudinal%20Magnetic%20Resonance%20Imaging%20%28MRI%29%0Adatasets%20has%20facilitated%20Artificial%20Intelligence%20%28AI%29-driven%20modeling%20of%0Adisease%20progression%2C%20making%20it%20possible%20to%20predict%20future%20medical%20scans%20for%0Aindividual%20patients.%20However%2C%20despite%20significant%20advancements%20in%20AI%2C%20current%0Amethods%20continue%20to%20face%20challenges%20including%20achieving%20patient-specific%0Aindividualization%2C%20ensuring%20spatiotemporal%20consistency%2C%20efficiently%20utilizing%0Alongitudinal%20data%2C%20and%20managing%20the%20substantial%20memory%20demands%20of%203D%20scans.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20Brain%20Latent%20Progression%20%28BrLP%29%2C%20a%20novel%0Aspatiotemporal%20model%20designed%20to%20predict%20individual-level%20disease%20progression%0Ain%203D%20brain%20MRIs.%20The%20key%20contributions%20in%20BrLP%20are%20fourfold%3A%20%28i%29%20it%20operates%0Ain%20a%20small%20latent%20space%2C%20mitigating%20the%20computational%20challenges%20posed%20by%0Ahigh-dimensional%20imaging%20data%3B%20%28ii%29%20it%20explicitly%20integrates%20subject%20metadata%0Ato%20enhance%20the%20individualization%20of%20predictions%3B%20%28iii%29%20it%20incorporates%20prior%0Aknowledge%20of%20disease%20dynamics%20through%20an%20auxiliary%20model%2C%20facilitating%20the%0Aintegration%20of%20longitudinal%20data%3B%20and%20%28iv%29%20it%20introduces%20the%20Latent%20Average%0AStabilization%20%28LAS%29%20algorithm%2C%20which%20%28a%29%20enforces%20spatiotemporal%20consistency%20in%0Athe%20predicted%20progression%20at%20inference%20time%20and%20%28b%29%20allows%20us%20to%20derive%20a%0Ameasure%20of%20the%20uncertainty%20for%20the%20prediction.%20We%20train%20and%20evaluate%20BrLP%20on%0A11%2C730%20T1-weighted%20%28T1w%29%20brain%20MRIs%20from%202%2C805%20subjects%20and%20validate%20its%0Ageneralizability%20on%20an%20external%20test%20set%20comprising%202%2C257%20MRIs%20from%20962%0Asubjects.%20Our%20experiments%20compare%20BrLP-generated%20MRI%20scans%20with%20real%20follow-up%0AMRIs%2C%20demonstrating%20state-of-the-art%20accuracy%20compared%20to%20existing%20methods.%20The%0Acode%20is%20publicly%20available%20at%3A%20https%3A//github.com/LemuelPuglisi/BrLP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08560v1&entry.124074799=Read"},
{"title": "A Novel Approach to for Multimodal Emotion Recognition : Multimodal\n  semantic information fusion", "author": "Wei Dai and Dequan Zheng and Feng Yu and Yanrong Zhang and Yaohui Hou", "abstract": "  With the advancement of artificial intelligence and computer vision\ntechnologies, multimodal emotion recognition has become a prominent research\ntopic. However, existing methods face challenges such as heterogeneous data\nfusion and the effective utilization of modality correlations. This paper\nproposes a novel multimodal emotion recognition approach, DeepMSI-MER, based on\nthe integration of contrastive learning and visual sequence compression. The\nproposed method enhances cross-modal feature fusion through contrastive\nlearning and reduces redundancy in the visual modality by leveraging visual\nsequence compression. Experimental results on two public datasets, IEMOCAP and\nMELD, demonstrate that DeepMSI-MER significantly improves the accuracy and\nrobustness of emotion recognition, validating the effectiveness of multimodal\nfeature fusion and the proposed approach.\n", "link": "http://arxiv.org/abs/2502.08573v1", "date": "2025-02-12", "relevancy": 1.5584, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5296}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5172}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Approach%20to%20for%20Multimodal%20Emotion%20Recognition%20%3A%20Multimodal%0A%20%20semantic%20information%20fusion&body=Title%3A%20A%20Novel%20Approach%20to%20for%20Multimodal%20Emotion%20Recognition%20%3A%20Multimodal%0A%20%20semantic%20information%20fusion%0AAuthor%3A%20Wei%20Dai%20and%20Dequan%20Zheng%20and%20Feng%20Yu%20and%20Yanrong%20Zhang%20and%20Yaohui%20Hou%0AAbstract%3A%20%20%20With%20the%20advancement%20of%20artificial%20intelligence%20and%20computer%20vision%0Atechnologies%2C%20multimodal%20emotion%20recognition%20has%20become%20a%20prominent%20research%0Atopic.%20However%2C%20existing%20methods%20face%20challenges%20such%20as%20heterogeneous%20data%0Afusion%20and%20the%20effective%20utilization%20of%20modality%20correlations.%20This%20paper%0Aproposes%20a%20novel%20multimodal%20emotion%20recognition%20approach%2C%20DeepMSI-MER%2C%20based%20on%0Athe%20integration%20of%20contrastive%20learning%20and%20visual%20sequence%20compression.%20The%0Aproposed%20method%20enhances%20cross-modal%20feature%20fusion%20through%20contrastive%0Alearning%20and%20reduces%20redundancy%20in%20the%20visual%20modality%20by%20leveraging%20visual%0Asequence%20compression.%20Experimental%20results%20on%20two%20public%20datasets%2C%20IEMOCAP%20and%0AMELD%2C%20demonstrate%20that%20DeepMSI-MER%20significantly%20improves%20the%20accuracy%20and%0Arobustness%20of%20emotion%20recognition%2C%20validating%20the%20effectiveness%20of%20multimodal%0Afeature%20fusion%20and%20the%20proposed%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08573v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Novel%2520Approach%2520to%2520for%2520Multimodal%2520Emotion%2520Recognition%2520%253A%2520Multimodal%250A%2520%2520semantic%2520information%2520fusion%26entry.906535625%3DWei%2520Dai%2520and%2520Dequan%2520Zheng%2520and%2520Feng%2520Yu%2520and%2520Yanrong%2520Zhang%2520and%2520Yaohui%2520Hou%26entry.1292438233%3D%2520%2520With%2520the%2520advancement%2520of%2520artificial%2520intelligence%2520and%2520computer%2520vision%250Atechnologies%252C%2520multimodal%2520emotion%2520recognition%2520has%2520become%2520a%2520prominent%2520research%250Atopic.%2520However%252C%2520existing%2520methods%2520face%2520challenges%2520such%2520as%2520heterogeneous%2520data%250Afusion%2520and%2520the%2520effective%2520utilization%2520of%2520modality%2520correlations.%2520This%2520paper%250Aproposes%2520a%2520novel%2520multimodal%2520emotion%2520recognition%2520approach%252C%2520DeepMSI-MER%252C%2520based%2520on%250Athe%2520integration%2520of%2520contrastive%2520learning%2520and%2520visual%2520sequence%2520compression.%2520The%250Aproposed%2520method%2520enhances%2520cross-modal%2520feature%2520fusion%2520through%2520contrastive%250Alearning%2520and%2520reduces%2520redundancy%2520in%2520the%2520visual%2520modality%2520by%2520leveraging%2520visual%250Asequence%2520compression.%2520Experimental%2520results%2520on%2520two%2520public%2520datasets%252C%2520IEMOCAP%2520and%250AMELD%252C%2520demonstrate%2520that%2520DeepMSI-MER%2520significantly%2520improves%2520the%2520accuracy%2520and%250Arobustness%2520of%2520emotion%2520recognition%252C%2520validating%2520the%2520effectiveness%2520of%2520multimodal%250Afeature%2520fusion%2520and%2520the%2520proposed%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08573v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Approach%20to%20for%20Multimodal%20Emotion%20Recognition%20%3A%20Multimodal%0A%20%20semantic%20information%20fusion&entry.906535625=Wei%20Dai%20and%20Dequan%20Zheng%20and%20Feng%20Yu%20and%20Yanrong%20Zhang%20and%20Yaohui%20Hou&entry.1292438233=%20%20With%20the%20advancement%20of%20artificial%20intelligence%20and%20computer%20vision%0Atechnologies%2C%20multimodal%20emotion%20recognition%20has%20become%20a%20prominent%20research%0Atopic.%20However%2C%20existing%20methods%20face%20challenges%20such%20as%20heterogeneous%20data%0Afusion%20and%20the%20effective%20utilization%20of%20modality%20correlations.%20This%20paper%0Aproposes%20a%20novel%20multimodal%20emotion%20recognition%20approach%2C%20DeepMSI-MER%2C%20based%20on%0Athe%20integration%20of%20contrastive%20learning%20and%20visual%20sequence%20compression.%20The%0Aproposed%20method%20enhances%20cross-modal%20feature%20fusion%20through%20contrastive%0Alearning%20and%20reduces%20redundancy%20in%20the%20visual%20modality%20by%20leveraging%20visual%0Asequence%20compression.%20Experimental%20results%20on%20two%20public%20datasets%2C%20IEMOCAP%20and%0AMELD%2C%20demonstrate%20that%20DeepMSI-MER%20significantly%20improves%20the%20accuracy%20and%0Arobustness%20of%20emotion%20recognition%2C%20validating%20the%20effectiveness%20of%20multimodal%0Afeature%20fusion%20and%20the%20proposed%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08573v1&entry.124074799=Read"},
{"title": "Annealed Winner-Takes-All for Motion Forecasting", "author": "Yihong Xu and Victor Letzelter and Micka\u00ebl Chen and \u00c9loi Zablocki and Matthieu Cord", "abstract": "  In autonomous driving, motion prediction aims at forecasting the future\ntrajectories of nearby agents, helping the ego vehicle to anticipate behaviors\nand drive safely. A key challenge is generating a diverse set of future\npredictions, commonly addressed using data-driven models with Multiple Choice\nLearning (MCL) architectures and Winner-Takes-All (WTA) training objectives.\nHowever, these methods face initialization sensitivity and training\ninstabilities. Additionally, to compensate for limited performance, some\napproaches rely on training with a large set of hypotheses, requiring a\npost-selection step during inference to significantly reduce the number of\npredictions. To tackle these issues, we take inspiration from annealed MCL, a\nrecently introduced technique that improves the convergence properties of MCL\nmethods through an annealed Winner-Takes-All loss (aWTA). In this paper, we\ndemonstrate how the aWTA loss can be integrated with state-of-the-art motion\nforecasting models to enhance their performance using only a minimal set of\nhypotheses, eliminating the need for the cumbersome post-selection step. Our\napproach can be easily incorporated into any trajectory prediction model\nnormally trained using WTA and yields significant improvements. To facilitate\nthe application of our approach to future motion forecasting models, the code\nis made publicly available: https://github.com/valeoai/MF_aWTA.\n", "link": "http://arxiv.org/abs/2409.11172v3", "date": "2025-02-12", "relevancy": 1.5518, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5676}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5298}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4921}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Annealed%20Winner-Takes-All%20for%20Motion%20Forecasting&body=Title%3A%20Annealed%20Winner-Takes-All%20for%20Motion%20Forecasting%0AAuthor%3A%20Yihong%20Xu%20and%20Victor%20Letzelter%20and%20Micka%C3%ABl%20Chen%20and%20%C3%89loi%20Zablocki%20and%20Matthieu%20Cord%0AAbstract%3A%20%20%20In%20autonomous%20driving%2C%20motion%20prediction%20aims%20at%20forecasting%20the%20future%0Atrajectories%20of%20nearby%20agents%2C%20helping%20the%20ego%20vehicle%20to%20anticipate%20behaviors%0Aand%20drive%20safely.%20A%20key%20challenge%20is%20generating%20a%20diverse%20set%20of%20future%0Apredictions%2C%20commonly%20addressed%20using%20data-driven%20models%20with%20Multiple%20Choice%0ALearning%20%28MCL%29%20architectures%20and%20Winner-Takes-All%20%28WTA%29%20training%20objectives.%0AHowever%2C%20these%20methods%20face%20initialization%20sensitivity%20and%20training%0Ainstabilities.%20Additionally%2C%20to%20compensate%20for%20limited%20performance%2C%20some%0Aapproaches%20rely%20on%20training%20with%20a%20large%20set%20of%20hypotheses%2C%20requiring%20a%0Apost-selection%20step%20during%20inference%20to%20significantly%20reduce%20the%20number%20of%0Apredictions.%20To%20tackle%20these%20issues%2C%20we%20take%20inspiration%20from%20annealed%20MCL%2C%20a%0Arecently%20introduced%20technique%20that%20improves%20the%20convergence%20properties%20of%20MCL%0Amethods%20through%20an%20annealed%20Winner-Takes-All%20loss%20%28aWTA%29.%20In%20this%20paper%2C%20we%0Ademonstrate%20how%20the%20aWTA%20loss%20can%20be%20integrated%20with%20state-of-the-art%20motion%0Aforecasting%20models%20to%20enhance%20their%20performance%20using%20only%20a%20minimal%20set%20of%0Ahypotheses%2C%20eliminating%20the%20need%20for%20the%20cumbersome%20post-selection%20step.%20Our%0Aapproach%20can%20be%20easily%20incorporated%20into%20any%20trajectory%20prediction%20model%0Anormally%20trained%20using%20WTA%20and%20yields%20significant%20improvements.%20To%20facilitate%0Athe%20application%20of%20our%20approach%20to%20future%20motion%20forecasting%20models%2C%20the%20code%0Ais%20made%20publicly%20available%3A%20https%3A//github.com/valeoai/MF_aWTA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11172v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnnealed%2520Winner-Takes-All%2520for%2520Motion%2520Forecasting%26entry.906535625%3DYihong%2520Xu%2520and%2520Victor%2520Letzelter%2520and%2520Micka%25C3%25ABl%2520Chen%2520and%2520%25C3%2589loi%2520Zablocki%2520and%2520Matthieu%2520Cord%26entry.1292438233%3D%2520%2520In%2520autonomous%2520driving%252C%2520motion%2520prediction%2520aims%2520at%2520forecasting%2520the%2520future%250Atrajectories%2520of%2520nearby%2520agents%252C%2520helping%2520the%2520ego%2520vehicle%2520to%2520anticipate%2520behaviors%250Aand%2520drive%2520safely.%2520A%2520key%2520challenge%2520is%2520generating%2520a%2520diverse%2520set%2520of%2520future%250Apredictions%252C%2520commonly%2520addressed%2520using%2520data-driven%2520models%2520with%2520Multiple%2520Choice%250ALearning%2520%2528MCL%2529%2520architectures%2520and%2520Winner-Takes-All%2520%2528WTA%2529%2520training%2520objectives.%250AHowever%252C%2520these%2520methods%2520face%2520initialization%2520sensitivity%2520and%2520training%250Ainstabilities.%2520Additionally%252C%2520to%2520compensate%2520for%2520limited%2520performance%252C%2520some%250Aapproaches%2520rely%2520on%2520training%2520with%2520a%2520large%2520set%2520of%2520hypotheses%252C%2520requiring%2520a%250Apost-selection%2520step%2520during%2520inference%2520to%2520significantly%2520reduce%2520the%2520number%2520of%250Apredictions.%2520To%2520tackle%2520these%2520issues%252C%2520we%2520take%2520inspiration%2520from%2520annealed%2520MCL%252C%2520a%250Arecently%2520introduced%2520technique%2520that%2520improves%2520the%2520convergence%2520properties%2520of%2520MCL%250Amethods%2520through%2520an%2520annealed%2520Winner-Takes-All%2520loss%2520%2528aWTA%2529.%2520In%2520this%2520paper%252C%2520we%250Ademonstrate%2520how%2520the%2520aWTA%2520loss%2520can%2520be%2520integrated%2520with%2520state-of-the-art%2520motion%250Aforecasting%2520models%2520to%2520enhance%2520their%2520performance%2520using%2520only%2520a%2520minimal%2520set%2520of%250Ahypotheses%252C%2520eliminating%2520the%2520need%2520for%2520the%2520cumbersome%2520post-selection%2520step.%2520Our%250Aapproach%2520can%2520be%2520easily%2520incorporated%2520into%2520any%2520trajectory%2520prediction%2520model%250Anormally%2520trained%2520using%2520WTA%2520and%2520yields%2520significant%2520improvements.%2520To%2520facilitate%250Athe%2520application%2520of%2520our%2520approach%2520to%2520future%2520motion%2520forecasting%2520models%252C%2520the%2520code%250Ais%2520made%2520publicly%2520available%253A%2520https%253A//github.com/valeoai/MF_aWTA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11172v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Annealed%20Winner-Takes-All%20for%20Motion%20Forecasting&entry.906535625=Yihong%20Xu%20and%20Victor%20Letzelter%20and%20Micka%C3%ABl%20Chen%20and%20%C3%89loi%20Zablocki%20and%20Matthieu%20Cord&entry.1292438233=%20%20In%20autonomous%20driving%2C%20motion%20prediction%20aims%20at%20forecasting%20the%20future%0Atrajectories%20of%20nearby%20agents%2C%20helping%20the%20ego%20vehicle%20to%20anticipate%20behaviors%0Aand%20drive%20safely.%20A%20key%20challenge%20is%20generating%20a%20diverse%20set%20of%20future%0Apredictions%2C%20commonly%20addressed%20using%20data-driven%20models%20with%20Multiple%20Choice%0ALearning%20%28MCL%29%20architectures%20and%20Winner-Takes-All%20%28WTA%29%20training%20objectives.%0AHowever%2C%20these%20methods%20face%20initialization%20sensitivity%20and%20training%0Ainstabilities.%20Additionally%2C%20to%20compensate%20for%20limited%20performance%2C%20some%0Aapproaches%20rely%20on%20training%20with%20a%20large%20set%20of%20hypotheses%2C%20requiring%20a%0Apost-selection%20step%20during%20inference%20to%20significantly%20reduce%20the%20number%20of%0Apredictions.%20To%20tackle%20these%20issues%2C%20we%20take%20inspiration%20from%20annealed%20MCL%2C%20a%0Arecently%20introduced%20technique%20that%20improves%20the%20convergence%20properties%20of%20MCL%0Amethods%20through%20an%20annealed%20Winner-Takes-All%20loss%20%28aWTA%29.%20In%20this%20paper%2C%20we%0Ademonstrate%20how%20the%20aWTA%20loss%20can%20be%20integrated%20with%20state-of-the-art%20motion%0Aforecasting%20models%20to%20enhance%20their%20performance%20using%20only%20a%20minimal%20set%20of%0Ahypotheses%2C%20eliminating%20the%20need%20for%20the%20cumbersome%20post-selection%20step.%20Our%0Aapproach%20can%20be%20easily%20incorporated%20into%20any%20trajectory%20prediction%20model%0Anormally%20trained%20using%20WTA%20and%20yields%20significant%20improvements.%20To%20facilitate%0Athe%20application%20of%20our%20approach%20to%20future%20motion%20forecasting%20models%2C%20the%20code%0Ais%20made%20publicly%20available%3A%20https%3A//github.com/valeoai/MF_aWTA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11172v3&entry.124074799=Read"},
{"title": "An Explainable Pipeline for Machine Learning with Functional Data", "author": "Katherine Goode and J. Derek Tucker and Daniel Ries and Heike Hofmann", "abstract": "  Machine learning (ML) models have shown success in applications with an\nobjective of prediction, but the algorithmic complexity of some models makes\nthem difficult to interpret. Methods have been proposed to provide insight into\nthese \"black-box\" models, but there is little research that focuses on\nsupervised ML when the model inputs are functional data. In this work, we\nconsider two applications from high-consequence spaces with objectives of\nmaking predictions using functional data inputs. One application aims to\nclassify material types to identify explosive materials given hyperspectral\ncomputed tomography scans of the materials. The other application considers the\nforensics science task of connecting an inkjet printed document to the source\nprinter using color signatures extracted by Raman spectroscopy. An instinctive\nroute to consider for analyzing these data is a data driven ML model for\nclassification, but due to the high consequence nature of the applications, we\nargue it is important to appropriately account for the nature of the data in\nthe analysis to not obscure or misrepresent patterns. As such, we propose the\nVariable importance Explainable Elastic Shape Analysis (VEESA) pipeline for\ntraining ML models with functional data that (1) accounts for the vertical and\nhorizontal variability in the functional data and (2) provides an explanation\nin the original data space of how the model uses variability in the functional\ndata for prediction. The pipeline makes use of elastic functional principal\ncomponents analysis (efPCA) to generate uncorrelated model inputs and\npermutation feature importance (PFI) to identify the principal components\nimportant for prediction. The variability captured by the important principal\ncomponents in visualized the original data space. We ultimately discuss ideas\nfor natural extensions of the VEESA pipeline and challenges for future\nresearch.\n", "link": "http://arxiv.org/abs/2501.07602v2", "date": "2025-02-12", "relevancy": 1.5263, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5322}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5064}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Explainable%20Pipeline%20for%20Machine%20Learning%20with%20Functional%20Data&body=Title%3A%20An%20Explainable%20Pipeline%20for%20Machine%20Learning%20with%20Functional%20Data%0AAuthor%3A%20Katherine%20Goode%20and%20J.%20Derek%20Tucker%20and%20Daniel%20Ries%20and%20Heike%20Hofmann%0AAbstract%3A%20%20%20Machine%20learning%20%28ML%29%20models%20have%20shown%20success%20in%20applications%20with%20an%0Aobjective%20of%20prediction%2C%20but%20the%20algorithmic%20complexity%20of%20some%20models%20makes%0Athem%20difficult%20to%20interpret.%20Methods%20have%20been%20proposed%20to%20provide%20insight%20into%0Athese%20%22black-box%22%20models%2C%20but%20there%20is%20little%20research%20that%20focuses%20on%0Asupervised%20ML%20when%20the%20model%20inputs%20are%20functional%20data.%20In%20this%20work%2C%20we%0Aconsider%20two%20applications%20from%20high-consequence%20spaces%20with%20objectives%20of%0Amaking%20predictions%20using%20functional%20data%20inputs.%20One%20application%20aims%20to%0Aclassify%20material%20types%20to%20identify%20explosive%20materials%20given%20hyperspectral%0Acomputed%20tomography%20scans%20of%20the%20materials.%20The%20other%20application%20considers%20the%0Aforensics%20science%20task%20of%20connecting%20an%20inkjet%20printed%20document%20to%20the%20source%0Aprinter%20using%20color%20signatures%20extracted%20by%20Raman%20spectroscopy.%20An%20instinctive%0Aroute%20to%20consider%20for%20analyzing%20these%20data%20is%20a%20data%20driven%20ML%20model%20for%0Aclassification%2C%20but%20due%20to%20the%20high%20consequence%20nature%20of%20the%20applications%2C%20we%0Aargue%20it%20is%20important%20to%20appropriately%20account%20for%20the%20nature%20of%20the%20data%20in%0Athe%20analysis%20to%20not%20obscure%20or%20misrepresent%20patterns.%20As%20such%2C%20we%20propose%20the%0AVariable%20importance%20Explainable%20Elastic%20Shape%20Analysis%20%28VEESA%29%20pipeline%20for%0Atraining%20ML%20models%20with%20functional%20data%20that%20%281%29%20accounts%20for%20the%20vertical%20and%0Ahorizontal%20variability%20in%20the%20functional%20data%20and%20%282%29%20provides%20an%20explanation%0Ain%20the%20original%20data%20space%20of%20how%20the%20model%20uses%20variability%20in%20the%20functional%0Adata%20for%20prediction.%20The%20pipeline%20makes%20use%20of%20elastic%20functional%20principal%0Acomponents%20analysis%20%28efPCA%29%20to%20generate%20uncorrelated%20model%20inputs%20and%0Apermutation%20feature%20importance%20%28PFI%29%20to%20identify%20the%20principal%20components%0Aimportant%20for%20prediction.%20The%20variability%20captured%20by%20the%20important%20principal%0Acomponents%20in%20visualized%20the%20original%20data%20space.%20We%20ultimately%20discuss%20ideas%0Afor%20natural%20extensions%20of%20the%20VEESA%20pipeline%20and%20challenges%20for%20future%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07602v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Explainable%2520Pipeline%2520for%2520Machine%2520Learning%2520with%2520Functional%2520Data%26entry.906535625%3DKatherine%2520Goode%2520and%2520J.%2520Derek%2520Tucker%2520and%2520Daniel%2520Ries%2520and%2520Heike%2520Hofmann%26entry.1292438233%3D%2520%2520Machine%2520learning%2520%2528ML%2529%2520models%2520have%2520shown%2520success%2520in%2520applications%2520with%2520an%250Aobjective%2520of%2520prediction%252C%2520but%2520the%2520algorithmic%2520complexity%2520of%2520some%2520models%2520makes%250Athem%2520difficult%2520to%2520interpret.%2520Methods%2520have%2520been%2520proposed%2520to%2520provide%2520insight%2520into%250Athese%2520%2522black-box%2522%2520models%252C%2520but%2520there%2520is%2520little%2520research%2520that%2520focuses%2520on%250Asupervised%2520ML%2520when%2520the%2520model%2520inputs%2520are%2520functional%2520data.%2520In%2520this%2520work%252C%2520we%250Aconsider%2520two%2520applications%2520from%2520high-consequence%2520spaces%2520with%2520objectives%2520of%250Amaking%2520predictions%2520using%2520functional%2520data%2520inputs.%2520One%2520application%2520aims%2520to%250Aclassify%2520material%2520types%2520to%2520identify%2520explosive%2520materials%2520given%2520hyperspectral%250Acomputed%2520tomography%2520scans%2520of%2520the%2520materials.%2520The%2520other%2520application%2520considers%2520the%250Aforensics%2520science%2520task%2520of%2520connecting%2520an%2520inkjet%2520printed%2520document%2520to%2520the%2520source%250Aprinter%2520using%2520color%2520signatures%2520extracted%2520by%2520Raman%2520spectroscopy.%2520An%2520instinctive%250Aroute%2520to%2520consider%2520for%2520analyzing%2520these%2520data%2520is%2520a%2520data%2520driven%2520ML%2520model%2520for%250Aclassification%252C%2520but%2520due%2520to%2520the%2520high%2520consequence%2520nature%2520of%2520the%2520applications%252C%2520we%250Aargue%2520it%2520is%2520important%2520to%2520appropriately%2520account%2520for%2520the%2520nature%2520of%2520the%2520data%2520in%250Athe%2520analysis%2520to%2520not%2520obscure%2520or%2520misrepresent%2520patterns.%2520As%2520such%252C%2520we%2520propose%2520the%250AVariable%2520importance%2520Explainable%2520Elastic%2520Shape%2520Analysis%2520%2528VEESA%2529%2520pipeline%2520for%250Atraining%2520ML%2520models%2520with%2520functional%2520data%2520that%2520%25281%2529%2520accounts%2520for%2520the%2520vertical%2520and%250Ahorizontal%2520variability%2520in%2520the%2520functional%2520data%2520and%2520%25282%2529%2520provides%2520an%2520explanation%250Ain%2520the%2520original%2520data%2520space%2520of%2520how%2520the%2520model%2520uses%2520variability%2520in%2520the%2520functional%250Adata%2520for%2520prediction.%2520The%2520pipeline%2520makes%2520use%2520of%2520elastic%2520functional%2520principal%250Acomponents%2520analysis%2520%2528efPCA%2529%2520to%2520generate%2520uncorrelated%2520model%2520inputs%2520and%250Apermutation%2520feature%2520importance%2520%2528PFI%2529%2520to%2520identify%2520the%2520principal%2520components%250Aimportant%2520for%2520prediction.%2520The%2520variability%2520captured%2520by%2520the%2520important%2520principal%250Acomponents%2520in%2520visualized%2520the%2520original%2520data%2520space.%2520We%2520ultimately%2520discuss%2520ideas%250Afor%2520natural%2520extensions%2520of%2520the%2520VEESA%2520pipeline%2520and%2520challenges%2520for%2520future%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07602v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Explainable%20Pipeline%20for%20Machine%20Learning%20with%20Functional%20Data&entry.906535625=Katherine%20Goode%20and%20J.%20Derek%20Tucker%20and%20Daniel%20Ries%20and%20Heike%20Hofmann&entry.1292438233=%20%20Machine%20learning%20%28ML%29%20models%20have%20shown%20success%20in%20applications%20with%20an%0Aobjective%20of%20prediction%2C%20but%20the%20algorithmic%20complexity%20of%20some%20models%20makes%0Athem%20difficult%20to%20interpret.%20Methods%20have%20been%20proposed%20to%20provide%20insight%20into%0Athese%20%22black-box%22%20models%2C%20but%20there%20is%20little%20research%20that%20focuses%20on%0Asupervised%20ML%20when%20the%20model%20inputs%20are%20functional%20data.%20In%20this%20work%2C%20we%0Aconsider%20two%20applications%20from%20high-consequence%20spaces%20with%20objectives%20of%0Amaking%20predictions%20using%20functional%20data%20inputs.%20One%20application%20aims%20to%0Aclassify%20material%20types%20to%20identify%20explosive%20materials%20given%20hyperspectral%0Acomputed%20tomography%20scans%20of%20the%20materials.%20The%20other%20application%20considers%20the%0Aforensics%20science%20task%20of%20connecting%20an%20inkjet%20printed%20document%20to%20the%20source%0Aprinter%20using%20color%20signatures%20extracted%20by%20Raman%20spectroscopy.%20An%20instinctive%0Aroute%20to%20consider%20for%20analyzing%20these%20data%20is%20a%20data%20driven%20ML%20model%20for%0Aclassification%2C%20but%20due%20to%20the%20high%20consequence%20nature%20of%20the%20applications%2C%20we%0Aargue%20it%20is%20important%20to%20appropriately%20account%20for%20the%20nature%20of%20the%20data%20in%0Athe%20analysis%20to%20not%20obscure%20or%20misrepresent%20patterns.%20As%20such%2C%20we%20propose%20the%0AVariable%20importance%20Explainable%20Elastic%20Shape%20Analysis%20%28VEESA%29%20pipeline%20for%0Atraining%20ML%20models%20with%20functional%20data%20that%20%281%29%20accounts%20for%20the%20vertical%20and%0Ahorizontal%20variability%20in%20the%20functional%20data%20and%20%282%29%20provides%20an%20explanation%0Ain%20the%20original%20data%20space%20of%20how%20the%20model%20uses%20variability%20in%20the%20functional%0Adata%20for%20prediction.%20The%20pipeline%20makes%20use%20of%20elastic%20functional%20principal%0Acomponents%20analysis%20%28efPCA%29%20to%20generate%20uncorrelated%20model%20inputs%20and%0Apermutation%20feature%20importance%20%28PFI%29%20to%20identify%20the%20principal%20components%0Aimportant%20for%20prediction.%20The%20variability%20captured%20by%20the%20important%20principal%0Acomponents%20in%20visualized%20the%20original%20data%20space.%20We%20ultimately%20discuss%20ideas%0Afor%20natural%20extensions%20of%20the%20VEESA%20pipeline%20and%20challenges%20for%20future%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07602v2&entry.124074799=Read"},
{"title": "Lightweight Neural App Control", "author": "Filippos Christianos and Georgios Papoudakis and Thomas Coste and Jianye Hao and Jun Wang and Kun Shao", "abstract": "  This paper introduces a novel mobile phone control architecture, Lightweight\nMulti-modal App Control (LiMAC), for efficient interactions and control across\nvarious Android apps. LiMAC takes as input a textual goal and a sequence of\npast mobile observations, such as screenshots and corresponding UI trees, to\ngenerate precise actions. To address the computational constraints inherent to\nsmartphones, we introduce a small Action Transformer (AcT) integrated with a\nfine-tuned vision-language model (VLM) for real-time decision-making and task\nexecution. We evaluate LiMAC on two open-source mobile control datasets,\ndemonstrating the superior performance of our small-form-factor approach\nagainst fine-tuned versions of open-source VLMs, such as Florence2 and\nQwen2-VL. It also significantly outperforms prompt engineering baselines\nutilising closed-source foundation models like GPT-4o. More specifically, LiMAC\nincreases the overall action accuracy by up to 19% compared to fine-tuned VLMs,\nand up to 42% compared to prompt-engineering baselines.\n", "link": "http://arxiv.org/abs/2410.17883v2", "date": "2025-02-12", "relevancy": 1.5131, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5168}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.508}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lightweight%20Neural%20App%20Control&body=Title%3A%20Lightweight%20Neural%20App%20Control%0AAuthor%3A%20Filippos%20Christianos%20and%20Georgios%20Papoudakis%20and%20Thomas%20Coste%20and%20Jianye%20Hao%20and%20Jun%20Wang%20and%20Kun%20Shao%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20mobile%20phone%20control%20architecture%2C%20Lightweight%0AMulti-modal%20App%20Control%20%28LiMAC%29%2C%20for%20efficient%20interactions%20and%20control%20across%0Avarious%20Android%20apps.%20LiMAC%20takes%20as%20input%20a%20textual%20goal%20and%20a%20sequence%20of%0Apast%20mobile%20observations%2C%20such%20as%20screenshots%20and%20corresponding%20UI%20trees%2C%20to%0Agenerate%20precise%20actions.%20To%20address%20the%20computational%20constraints%20inherent%20to%0Asmartphones%2C%20we%20introduce%20a%20small%20Action%20Transformer%20%28AcT%29%20integrated%20with%20a%0Afine-tuned%20vision-language%20model%20%28VLM%29%20for%20real-time%20decision-making%20and%20task%0Aexecution.%20We%20evaluate%20LiMAC%20on%20two%20open-source%20mobile%20control%20datasets%2C%0Ademonstrating%20the%20superior%20performance%20of%20our%20small-form-factor%20approach%0Aagainst%20fine-tuned%20versions%20of%20open-source%20VLMs%2C%20such%20as%20Florence2%20and%0AQwen2-VL.%20It%20also%20significantly%20outperforms%20prompt%20engineering%20baselines%0Autilising%20closed-source%20foundation%20models%20like%20GPT-4o.%20More%20specifically%2C%20LiMAC%0Aincreases%20the%20overall%20action%20accuracy%20by%20up%20to%2019%25%20compared%20to%20fine-tuned%20VLMs%2C%0Aand%20up%20to%2042%25%20compared%20to%20prompt-engineering%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17883v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightweight%2520Neural%2520App%2520Control%26entry.906535625%3DFilippos%2520Christianos%2520and%2520Georgios%2520Papoudakis%2520and%2520Thomas%2520Coste%2520and%2520Jianye%2520Hao%2520and%2520Jun%2520Wang%2520and%2520Kun%2520Shao%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520mobile%2520phone%2520control%2520architecture%252C%2520Lightweight%250AMulti-modal%2520App%2520Control%2520%2528LiMAC%2529%252C%2520for%2520efficient%2520interactions%2520and%2520control%2520across%250Avarious%2520Android%2520apps.%2520LiMAC%2520takes%2520as%2520input%2520a%2520textual%2520goal%2520and%2520a%2520sequence%2520of%250Apast%2520mobile%2520observations%252C%2520such%2520as%2520screenshots%2520and%2520corresponding%2520UI%2520trees%252C%2520to%250Agenerate%2520precise%2520actions.%2520To%2520address%2520the%2520computational%2520constraints%2520inherent%2520to%250Asmartphones%252C%2520we%2520introduce%2520a%2520small%2520Action%2520Transformer%2520%2528AcT%2529%2520integrated%2520with%2520a%250Afine-tuned%2520vision-language%2520model%2520%2528VLM%2529%2520for%2520real-time%2520decision-making%2520and%2520task%250Aexecution.%2520We%2520evaluate%2520LiMAC%2520on%2520two%2520open-source%2520mobile%2520control%2520datasets%252C%250Ademonstrating%2520the%2520superior%2520performance%2520of%2520our%2520small-form-factor%2520approach%250Aagainst%2520fine-tuned%2520versions%2520of%2520open-source%2520VLMs%252C%2520such%2520as%2520Florence2%2520and%250AQwen2-VL.%2520It%2520also%2520significantly%2520outperforms%2520prompt%2520engineering%2520baselines%250Autilising%2520closed-source%2520foundation%2520models%2520like%2520GPT-4o.%2520More%2520specifically%252C%2520LiMAC%250Aincreases%2520the%2520overall%2520action%2520accuracy%2520by%2520up%2520to%252019%2525%2520compared%2520to%2520fine-tuned%2520VLMs%252C%250Aand%2520up%2520to%252042%2525%2520compared%2520to%2520prompt-engineering%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17883v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lightweight%20Neural%20App%20Control&entry.906535625=Filippos%20Christianos%20and%20Georgios%20Papoudakis%20and%20Thomas%20Coste%20and%20Jianye%20Hao%20and%20Jun%20Wang%20and%20Kun%20Shao&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20mobile%20phone%20control%20architecture%2C%20Lightweight%0AMulti-modal%20App%20Control%20%28LiMAC%29%2C%20for%20efficient%20interactions%20and%20control%20across%0Avarious%20Android%20apps.%20LiMAC%20takes%20as%20input%20a%20textual%20goal%20and%20a%20sequence%20of%0Apast%20mobile%20observations%2C%20such%20as%20screenshots%20and%20corresponding%20UI%20trees%2C%20to%0Agenerate%20precise%20actions.%20To%20address%20the%20computational%20constraints%20inherent%20to%0Asmartphones%2C%20we%20introduce%20a%20small%20Action%20Transformer%20%28AcT%29%20integrated%20with%20a%0Afine-tuned%20vision-language%20model%20%28VLM%29%20for%20real-time%20decision-making%20and%20task%0Aexecution.%20We%20evaluate%20LiMAC%20on%20two%20open-source%20mobile%20control%20datasets%2C%0Ademonstrating%20the%20superior%20performance%20of%20our%20small-form-factor%20approach%0Aagainst%20fine-tuned%20versions%20of%20open-source%20VLMs%2C%20such%20as%20Florence2%20and%0AQwen2-VL.%20It%20also%20significantly%20outperforms%20prompt%20engineering%20baselines%0Autilising%20closed-source%20foundation%20models%20like%20GPT-4o.%20More%20specifically%2C%20LiMAC%0Aincreases%20the%20overall%20action%20accuracy%20by%20up%20to%2019%25%20compared%20to%20fine-tuned%20VLMs%2C%0Aand%20up%20to%2042%25%20compared%20to%20prompt-engineering%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17883v2&entry.124074799=Read"},
{"title": "One-Shot Federated Learning with Classifier-Free Diffusion Models", "author": "Obaidullah Zaland and Shutong Jin and Florian T. Pokorny and Monowar Bhuyan", "abstract": "  Federated learning (FL) enables collaborative learning without data\ncentralization but introduces significant communication costs due to multiple\ncommunication rounds between clients and the server. One-shot federated\nlearning (OSFL) addresses this by forming a global model with a single\ncommunication round, often relying on the server's model distillation or\nauxiliary dataset generation - often through pre-trained diffusion models\n(DMs). Existing DM-assisted OSFL methods, however, typically employ\nclassifier-guided DMs, which require training auxiliary classifier models at\neach client, introducing additional computation overhead. This work introduces\nOSCAR (One-Shot Federated Learning with Classifier-Free Diffusion Models), a\nnovel OSFL approach that eliminates the need for auxiliary models. OSCAR uses\nfoundation models to devise category-specific data representations at each\nclient, seamlessly integrated into a classifier-free diffusion model pipeline\nfor server-side data generation. OSCAR is a simple yet cost-effective OSFL\napproach that outperforms the state-of-the-art on four benchmarking datasets\nwhile reducing the communication load by at least 99%.\n", "link": "http://arxiv.org/abs/2502.08488v1", "date": "2025-02-12", "relevancy": 1.5007, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5558}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4881}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One-Shot%20Federated%20Learning%20with%20Classifier-Free%20Diffusion%20Models&body=Title%3A%20One-Shot%20Federated%20Learning%20with%20Classifier-Free%20Diffusion%20Models%0AAuthor%3A%20Obaidullah%20Zaland%20and%20Shutong%20Jin%20and%20Florian%20T.%20Pokorny%20and%20Monowar%20Bhuyan%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20enables%20collaborative%20learning%20without%20data%0Acentralization%20but%20introduces%20significant%20communication%20costs%20due%20to%20multiple%0Acommunication%20rounds%20between%20clients%20and%20the%20server.%20One-shot%20federated%0Alearning%20%28OSFL%29%20addresses%20this%20by%20forming%20a%20global%20model%20with%20a%20single%0Acommunication%20round%2C%20often%20relying%20on%20the%20server%27s%20model%20distillation%20or%0Aauxiliary%20dataset%20generation%20-%20often%20through%20pre-trained%20diffusion%20models%0A%28DMs%29.%20Existing%20DM-assisted%20OSFL%20methods%2C%20however%2C%20typically%20employ%0Aclassifier-guided%20DMs%2C%20which%20require%20training%20auxiliary%20classifier%20models%20at%0Aeach%20client%2C%20introducing%20additional%20computation%20overhead.%20This%20work%20introduces%0AOSCAR%20%28One-Shot%20Federated%20Learning%20with%20Classifier-Free%20Diffusion%20Models%29%2C%20a%0Anovel%20OSFL%20approach%20that%20eliminates%20the%20need%20for%20auxiliary%20models.%20OSCAR%20uses%0Afoundation%20models%20to%20devise%20category-specific%20data%20representations%20at%20each%0Aclient%2C%20seamlessly%20integrated%20into%20a%20classifier-free%20diffusion%20model%20pipeline%0Afor%20server-side%20data%20generation.%20OSCAR%20is%20a%20simple%20yet%20cost-effective%20OSFL%0Aapproach%20that%20outperforms%20the%20state-of-the-art%20on%20four%20benchmarking%20datasets%0Awhile%20reducing%20the%20communication%20load%20by%20at%20least%2099%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08488v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne-Shot%2520Federated%2520Learning%2520with%2520Classifier-Free%2520Diffusion%2520Models%26entry.906535625%3DObaidullah%2520Zaland%2520and%2520Shutong%2520Jin%2520and%2520Florian%2520T.%2520Pokorny%2520and%2520Monowar%2520Bhuyan%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520enables%2520collaborative%2520learning%2520without%2520data%250Acentralization%2520but%2520introduces%2520significant%2520communication%2520costs%2520due%2520to%2520multiple%250Acommunication%2520rounds%2520between%2520clients%2520and%2520the%2520server.%2520One-shot%2520federated%250Alearning%2520%2528OSFL%2529%2520addresses%2520this%2520by%2520forming%2520a%2520global%2520model%2520with%2520a%2520single%250Acommunication%2520round%252C%2520often%2520relying%2520on%2520the%2520server%2527s%2520model%2520distillation%2520or%250Aauxiliary%2520dataset%2520generation%2520-%2520often%2520through%2520pre-trained%2520diffusion%2520models%250A%2528DMs%2529.%2520Existing%2520DM-assisted%2520OSFL%2520methods%252C%2520however%252C%2520typically%2520employ%250Aclassifier-guided%2520DMs%252C%2520which%2520require%2520training%2520auxiliary%2520classifier%2520models%2520at%250Aeach%2520client%252C%2520introducing%2520additional%2520computation%2520overhead.%2520This%2520work%2520introduces%250AOSCAR%2520%2528One-Shot%2520Federated%2520Learning%2520with%2520Classifier-Free%2520Diffusion%2520Models%2529%252C%2520a%250Anovel%2520OSFL%2520approach%2520that%2520eliminates%2520the%2520need%2520for%2520auxiliary%2520models.%2520OSCAR%2520uses%250Afoundation%2520models%2520to%2520devise%2520category-specific%2520data%2520representations%2520at%2520each%250Aclient%252C%2520seamlessly%2520integrated%2520into%2520a%2520classifier-free%2520diffusion%2520model%2520pipeline%250Afor%2520server-side%2520data%2520generation.%2520OSCAR%2520is%2520a%2520simple%2520yet%2520cost-effective%2520OSFL%250Aapproach%2520that%2520outperforms%2520the%2520state-of-the-art%2520on%2520four%2520benchmarking%2520datasets%250Awhile%2520reducing%2520the%2520communication%2520load%2520by%2520at%2520least%252099%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08488v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One-Shot%20Federated%20Learning%20with%20Classifier-Free%20Diffusion%20Models&entry.906535625=Obaidullah%20Zaland%20and%20Shutong%20Jin%20and%20Florian%20T.%20Pokorny%20and%20Monowar%20Bhuyan&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20enables%20collaborative%20learning%20without%20data%0Acentralization%20but%20introduces%20significant%20communication%20costs%20due%20to%20multiple%0Acommunication%20rounds%20between%20clients%20and%20the%20server.%20One-shot%20federated%0Alearning%20%28OSFL%29%20addresses%20this%20by%20forming%20a%20global%20model%20with%20a%20single%0Acommunication%20round%2C%20often%20relying%20on%20the%20server%27s%20model%20distillation%20or%0Aauxiliary%20dataset%20generation%20-%20often%20through%20pre-trained%20diffusion%20models%0A%28DMs%29.%20Existing%20DM-assisted%20OSFL%20methods%2C%20however%2C%20typically%20employ%0Aclassifier-guided%20DMs%2C%20which%20require%20training%20auxiliary%20classifier%20models%20at%0Aeach%20client%2C%20introducing%20additional%20computation%20overhead.%20This%20work%20introduces%0AOSCAR%20%28One-Shot%20Federated%20Learning%20with%20Classifier-Free%20Diffusion%20Models%29%2C%20a%0Anovel%20OSFL%20approach%20that%20eliminates%20the%20need%20for%20auxiliary%20models.%20OSCAR%20uses%0Afoundation%20models%20to%20devise%20category-specific%20data%20representations%20at%20each%0Aclient%2C%20seamlessly%20integrated%20into%20a%20classifier-free%20diffusion%20model%20pipeline%0Afor%20server-side%20data%20generation.%20OSCAR%20is%20a%20simple%20yet%20cost-effective%20OSFL%0Aapproach%20that%20outperforms%20the%20state-of-the-art%20on%20four%20benchmarking%20datasets%0Awhile%20reducing%20the%20communication%20load%20by%20at%20least%2099%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08488v1&entry.124074799=Read"},
{"title": "FedMHO: Heterogeneous One-Shot Federated Learning Towards\n  Resource-Constrained Edge Devices", "author": "Dezhong Yao and Yuexin Shi and Tongtong Liu and Zhiqiang Xu", "abstract": "  Federated Learning (FL) is increasingly adopted in edge computing scenarios,\nwhere a large number of heterogeneous clients operate under constrained or\nsufficient resources. The iterative training process in conventional FL\nintroduces significant computation and communication overhead, which is\nunfriendly for resource-constrained edge devices. One-shot FL has emerged as a\npromising approach to mitigate communication overhead, and model-heterogeneous\nFL solves the problem of diverse computing resources across clients. However,\nexisting methods face challenges in effectively managing model-heterogeneous\none-shot FL, often leading to unsatisfactory global model performance or\nreliance on auxiliary datasets. To address these challenges, we propose a novel\nFL framework named FedMHO, which leverages deep classification models on\nresource-sufficient clients and lightweight generative models on\nresource-constrained devices. On the server side, FedMHO involves a two-stage\nprocess that includes data generation and knowledge fusion. Furthermore, we\nintroduce FedMHO-MD and FedMHO-SD to mitigate the knowledge-forgetting problem\nduring the knowledge fusion stage, and an unsupervised data optimization\nsolution to improve the quality of synthetic samples. Comprehensive experiments\ndemonstrate the effectiveness of our methods, as they outperform\nstate-of-the-art baselines in various experimental setups.\n", "link": "http://arxiv.org/abs/2502.08518v1", "date": "2025-02-12", "relevancy": 1.4988, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.506}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4941}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedMHO%3A%20Heterogeneous%20One-Shot%20Federated%20Learning%20Towards%0A%20%20Resource-Constrained%20Edge%20Devices&body=Title%3A%20FedMHO%3A%20Heterogeneous%20One-Shot%20Federated%20Learning%20Towards%0A%20%20Resource-Constrained%20Edge%20Devices%0AAuthor%3A%20Dezhong%20Yao%20and%20Yuexin%20Shi%20and%20Tongtong%20Liu%20and%20Zhiqiang%20Xu%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20is%20increasingly%20adopted%20in%20edge%20computing%20scenarios%2C%0Awhere%20a%20large%20number%20of%20heterogeneous%20clients%20operate%20under%20constrained%20or%0Asufficient%20resources.%20The%20iterative%20training%20process%20in%20conventional%20FL%0Aintroduces%20significant%20computation%20and%20communication%20overhead%2C%20which%20is%0Aunfriendly%20for%20resource-constrained%20edge%20devices.%20One-shot%20FL%20has%20emerged%20as%20a%0Apromising%20approach%20to%20mitigate%20communication%20overhead%2C%20and%20model-heterogeneous%0AFL%20solves%20the%20problem%20of%20diverse%20computing%20resources%20across%20clients.%20However%2C%0Aexisting%20methods%20face%20challenges%20in%20effectively%20managing%20model-heterogeneous%0Aone-shot%20FL%2C%20often%20leading%20to%20unsatisfactory%20global%20model%20performance%20or%0Areliance%20on%20auxiliary%20datasets.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%0AFL%20framework%20named%20FedMHO%2C%20which%20leverages%20deep%20classification%20models%20on%0Aresource-sufficient%20clients%20and%20lightweight%20generative%20models%20on%0Aresource-constrained%20devices.%20On%20the%20server%20side%2C%20FedMHO%20involves%20a%20two-stage%0Aprocess%20that%20includes%20data%20generation%20and%20knowledge%20fusion.%20Furthermore%2C%20we%0Aintroduce%20FedMHO-MD%20and%20FedMHO-SD%20to%20mitigate%20the%20knowledge-forgetting%20problem%0Aduring%20the%20knowledge%20fusion%20stage%2C%20and%20an%20unsupervised%20data%20optimization%0Asolution%20to%20improve%20the%20quality%20of%20synthetic%20samples.%20Comprehensive%20experiments%0Ademonstrate%20the%20effectiveness%20of%20our%20methods%2C%20as%20they%20outperform%0Astate-of-the-art%20baselines%20in%20various%20experimental%20setups.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08518v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedMHO%253A%2520Heterogeneous%2520One-Shot%2520Federated%2520Learning%2520Towards%250A%2520%2520Resource-Constrained%2520Edge%2520Devices%26entry.906535625%3DDezhong%2520Yao%2520and%2520Yuexin%2520Shi%2520and%2520Tongtong%2520Liu%2520and%2520Zhiqiang%2520Xu%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520is%2520increasingly%2520adopted%2520in%2520edge%2520computing%2520scenarios%252C%250Awhere%2520a%2520large%2520number%2520of%2520heterogeneous%2520clients%2520operate%2520under%2520constrained%2520or%250Asufficient%2520resources.%2520The%2520iterative%2520training%2520process%2520in%2520conventional%2520FL%250Aintroduces%2520significant%2520computation%2520and%2520communication%2520overhead%252C%2520which%2520is%250Aunfriendly%2520for%2520resource-constrained%2520edge%2520devices.%2520One-shot%2520FL%2520has%2520emerged%2520as%2520a%250Apromising%2520approach%2520to%2520mitigate%2520communication%2520overhead%252C%2520and%2520model-heterogeneous%250AFL%2520solves%2520the%2520problem%2520of%2520diverse%2520computing%2520resources%2520across%2520clients.%2520However%252C%250Aexisting%2520methods%2520face%2520challenges%2520in%2520effectively%2520managing%2520model-heterogeneous%250Aone-shot%2520FL%252C%2520often%2520leading%2520to%2520unsatisfactory%2520global%2520model%2520performance%2520or%250Areliance%2520on%2520auxiliary%2520datasets.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%250AFL%2520framework%2520named%2520FedMHO%252C%2520which%2520leverages%2520deep%2520classification%2520models%2520on%250Aresource-sufficient%2520clients%2520and%2520lightweight%2520generative%2520models%2520on%250Aresource-constrained%2520devices.%2520On%2520the%2520server%2520side%252C%2520FedMHO%2520involves%2520a%2520two-stage%250Aprocess%2520that%2520includes%2520data%2520generation%2520and%2520knowledge%2520fusion.%2520Furthermore%252C%2520we%250Aintroduce%2520FedMHO-MD%2520and%2520FedMHO-SD%2520to%2520mitigate%2520the%2520knowledge-forgetting%2520problem%250Aduring%2520the%2520knowledge%2520fusion%2520stage%252C%2520and%2520an%2520unsupervised%2520data%2520optimization%250Asolution%2520to%2520improve%2520the%2520quality%2520of%2520synthetic%2520samples.%2520Comprehensive%2520experiments%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520methods%252C%2520as%2520they%2520outperform%250Astate-of-the-art%2520baselines%2520in%2520various%2520experimental%2520setups.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08518v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedMHO%3A%20Heterogeneous%20One-Shot%20Federated%20Learning%20Towards%0A%20%20Resource-Constrained%20Edge%20Devices&entry.906535625=Dezhong%20Yao%20and%20Yuexin%20Shi%20and%20Tongtong%20Liu%20and%20Zhiqiang%20Xu&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20is%20increasingly%20adopted%20in%20edge%20computing%20scenarios%2C%0Awhere%20a%20large%20number%20of%20heterogeneous%20clients%20operate%20under%20constrained%20or%0Asufficient%20resources.%20The%20iterative%20training%20process%20in%20conventional%20FL%0Aintroduces%20significant%20computation%20and%20communication%20overhead%2C%20which%20is%0Aunfriendly%20for%20resource-constrained%20edge%20devices.%20One-shot%20FL%20has%20emerged%20as%20a%0Apromising%20approach%20to%20mitigate%20communication%20overhead%2C%20and%20model-heterogeneous%0AFL%20solves%20the%20problem%20of%20diverse%20computing%20resources%20across%20clients.%20However%2C%0Aexisting%20methods%20face%20challenges%20in%20effectively%20managing%20model-heterogeneous%0Aone-shot%20FL%2C%20often%20leading%20to%20unsatisfactory%20global%20model%20performance%20or%0Areliance%20on%20auxiliary%20datasets.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%0AFL%20framework%20named%20FedMHO%2C%20which%20leverages%20deep%20classification%20models%20on%0Aresource-sufficient%20clients%20and%20lightweight%20generative%20models%20on%0Aresource-constrained%20devices.%20On%20the%20server%20side%2C%20FedMHO%20involves%20a%20two-stage%0Aprocess%20that%20includes%20data%20generation%20and%20knowledge%20fusion.%20Furthermore%2C%20we%0Aintroduce%20FedMHO-MD%20and%20FedMHO-SD%20to%20mitigate%20the%20knowledge-forgetting%20problem%0Aduring%20the%20knowledge%20fusion%20stage%2C%20and%20an%20unsupervised%20data%20optimization%0Asolution%20to%20improve%20the%20quality%20of%20synthetic%20samples.%20Comprehensive%20experiments%0Ademonstrate%20the%20effectiveness%20of%20our%20methods%2C%20as%20they%20outperform%0Astate-of-the-art%20baselines%20in%20various%20experimental%20setups.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08518v1&entry.124074799=Read"},
{"title": "Representing Rule-based Chatbots with Transformers", "author": "Dan Friedman and Abhishek Panigrahi and Danqi Chen", "abstract": "  What kind of internal mechanisms might Transformers use to conduct fluid,\nnatural-sounding conversations? Prior work has illustrated by construction how\nTransformers can solve various synthetic tasks, such as sorting a list or\nrecognizing formal languages, but it remains unclear how to extend this\napproach to a conversational setting. In this work, we propose using ELIZA, a\nclassic rule-based chatbot, as a setting for formal, mechanistic analysis of\nTransformer-based chatbots. ELIZA allows us to formally model key aspects of\nconversation, including local pattern matching and long-term dialogue state\ntracking. We first present a theoretical construction of a Transformer that\nimplements the ELIZA chatbot. Building on prior constructions, particularly\nthose for simulating finite-state automata, we show how simpler mechanisms can\nbe composed and extended to produce more sophisticated behavior. Next, we\nconduct a set of empirical analyses of Transformers trained on synthetically\ngenerated ELIZA conversations. Our analysis illustrates the kinds of mechanisms\nthese models tend to prefer--for example, models favor an induction head\nmechanism over a more precise, position-based copying mechanism; and using\nintermediate generations to simulate recurrent data structures, akin to an\nimplicit scratchpad or Chain-of-Thought. Overall, by drawing an explicit\nconnection between neural chatbots and interpretable, symbolic mechanisms, our\nresults provide a new framework for the mechanistic analysis of conversational\nagents.\n", "link": "http://arxiv.org/abs/2407.10949v2", "date": "2025-02-12", "relevancy": 1.482, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.511}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5054}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4827}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Representing%20Rule-based%20Chatbots%20with%20Transformers&body=Title%3A%20Representing%20Rule-based%20Chatbots%20with%20Transformers%0AAuthor%3A%20Dan%20Friedman%20and%20Abhishek%20Panigrahi%20and%20Danqi%20Chen%0AAbstract%3A%20%20%20What%20kind%20of%20internal%20mechanisms%20might%20Transformers%20use%20to%20conduct%20fluid%2C%0Anatural-sounding%20conversations%3F%20Prior%20work%20has%20illustrated%20by%20construction%20how%0ATransformers%20can%20solve%20various%20synthetic%20tasks%2C%20such%20as%20sorting%20a%20list%20or%0Arecognizing%20formal%20languages%2C%20but%20it%20remains%20unclear%20how%20to%20extend%20this%0Aapproach%20to%20a%20conversational%20setting.%20In%20this%20work%2C%20we%20propose%20using%20ELIZA%2C%20a%0Aclassic%20rule-based%20chatbot%2C%20as%20a%20setting%20for%20formal%2C%20mechanistic%20analysis%20of%0ATransformer-based%20chatbots.%20ELIZA%20allows%20us%20to%20formally%20model%20key%20aspects%20of%0Aconversation%2C%20including%20local%20pattern%20matching%20and%20long-term%20dialogue%20state%0Atracking.%20We%20first%20present%20a%20theoretical%20construction%20of%20a%20Transformer%20that%0Aimplements%20the%20ELIZA%20chatbot.%20Building%20on%20prior%20constructions%2C%20particularly%0Athose%20for%20simulating%20finite-state%20automata%2C%20we%20show%20how%20simpler%20mechanisms%20can%0Abe%20composed%20and%20extended%20to%20produce%20more%20sophisticated%20behavior.%20Next%2C%20we%0Aconduct%20a%20set%20of%20empirical%20analyses%20of%20Transformers%20trained%20on%20synthetically%0Agenerated%20ELIZA%20conversations.%20Our%20analysis%20illustrates%20the%20kinds%20of%20mechanisms%0Athese%20models%20tend%20to%20prefer--for%20example%2C%20models%20favor%20an%20induction%20head%0Amechanism%20over%20a%20more%20precise%2C%20position-based%20copying%20mechanism%3B%20and%20using%0Aintermediate%20generations%20to%20simulate%20recurrent%20data%20structures%2C%20akin%20to%20an%0Aimplicit%20scratchpad%20or%20Chain-of-Thought.%20Overall%2C%20by%20drawing%20an%20explicit%0Aconnection%20between%20neural%20chatbots%20and%20interpretable%2C%20symbolic%20mechanisms%2C%20our%0Aresults%20provide%20a%20new%20framework%20for%20the%20mechanistic%20analysis%20of%20conversational%0Aagents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10949v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepresenting%2520Rule-based%2520Chatbots%2520with%2520Transformers%26entry.906535625%3DDan%2520Friedman%2520and%2520Abhishek%2520Panigrahi%2520and%2520Danqi%2520Chen%26entry.1292438233%3D%2520%2520What%2520kind%2520of%2520internal%2520mechanisms%2520might%2520Transformers%2520use%2520to%2520conduct%2520fluid%252C%250Anatural-sounding%2520conversations%253F%2520Prior%2520work%2520has%2520illustrated%2520by%2520construction%2520how%250ATransformers%2520can%2520solve%2520various%2520synthetic%2520tasks%252C%2520such%2520as%2520sorting%2520a%2520list%2520or%250Arecognizing%2520formal%2520languages%252C%2520but%2520it%2520remains%2520unclear%2520how%2520to%2520extend%2520this%250Aapproach%2520to%2520a%2520conversational%2520setting.%2520In%2520this%2520work%252C%2520we%2520propose%2520using%2520ELIZA%252C%2520a%250Aclassic%2520rule-based%2520chatbot%252C%2520as%2520a%2520setting%2520for%2520formal%252C%2520mechanistic%2520analysis%2520of%250ATransformer-based%2520chatbots.%2520ELIZA%2520allows%2520us%2520to%2520formally%2520model%2520key%2520aspects%2520of%250Aconversation%252C%2520including%2520local%2520pattern%2520matching%2520and%2520long-term%2520dialogue%2520state%250Atracking.%2520We%2520first%2520present%2520a%2520theoretical%2520construction%2520of%2520a%2520Transformer%2520that%250Aimplements%2520the%2520ELIZA%2520chatbot.%2520Building%2520on%2520prior%2520constructions%252C%2520particularly%250Athose%2520for%2520simulating%2520finite-state%2520automata%252C%2520we%2520show%2520how%2520simpler%2520mechanisms%2520can%250Abe%2520composed%2520and%2520extended%2520to%2520produce%2520more%2520sophisticated%2520behavior.%2520Next%252C%2520we%250Aconduct%2520a%2520set%2520of%2520empirical%2520analyses%2520of%2520Transformers%2520trained%2520on%2520synthetically%250Agenerated%2520ELIZA%2520conversations.%2520Our%2520analysis%2520illustrates%2520the%2520kinds%2520of%2520mechanisms%250Athese%2520models%2520tend%2520to%2520prefer--for%2520example%252C%2520models%2520favor%2520an%2520induction%2520head%250Amechanism%2520over%2520a%2520more%2520precise%252C%2520position-based%2520copying%2520mechanism%253B%2520and%2520using%250Aintermediate%2520generations%2520to%2520simulate%2520recurrent%2520data%2520structures%252C%2520akin%2520to%2520an%250Aimplicit%2520scratchpad%2520or%2520Chain-of-Thought.%2520Overall%252C%2520by%2520drawing%2520an%2520explicit%250Aconnection%2520between%2520neural%2520chatbots%2520and%2520interpretable%252C%2520symbolic%2520mechanisms%252C%2520our%250Aresults%2520provide%2520a%2520new%2520framework%2520for%2520the%2520mechanistic%2520analysis%2520of%2520conversational%250Aagents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10949v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Representing%20Rule-based%20Chatbots%20with%20Transformers&entry.906535625=Dan%20Friedman%20and%20Abhishek%20Panigrahi%20and%20Danqi%20Chen&entry.1292438233=%20%20What%20kind%20of%20internal%20mechanisms%20might%20Transformers%20use%20to%20conduct%20fluid%2C%0Anatural-sounding%20conversations%3F%20Prior%20work%20has%20illustrated%20by%20construction%20how%0ATransformers%20can%20solve%20various%20synthetic%20tasks%2C%20such%20as%20sorting%20a%20list%20or%0Arecognizing%20formal%20languages%2C%20but%20it%20remains%20unclear%20how%20to%20extend%20this%0Aapproach%20to%20a%20conversational%20setting.%20In%20this%20work%2C%20we%20propose%20using%20ELIZA%2C%20a%0Aclassic%20rule-based%20chatbot%2C%20as%20a%20setting%20for%20formal%2C%20mechanistic%20analysis%20of%0ATransformer-based%20chatbots.%20ELIZA%20allows%20us%20to%20formally%20model%20key%20aspects%20of%0Aconversation%2C%20including%20local%20pattern%20matching%20and%20long-term%20dialogue%20state%0Atracking.%20We%20first%20present%20a%20theoretical%20construction%20of%20a%20Transformer%20that%0Aimplements%20the%20ELIZA%20chatbot.%20Building%20on%20prior%20constructions%2C%20particularly%0Athose%20for%20simulating%20finite-state%20automata%2C%20we%20show%20how%20simpler%20mechanisms%20can%0Abe%20composed%20and%20extended%20to%20produce%20more%20sophisticated%20behavior.%20Next%2C%20we%0Aconduct%20a%20set%20of%20empirical%20analyses%20of%20Transformers%20trained%20on%20synthetically%0Agenerated%20ELIZA%20conversations.%20Our%20analysis%20illustrates%20the%20kinds%20of%20mechanisms%0Athese%20models%20tend%20to%20prefer--for%20example%2C%20models%20favor%20an%20induction%20head%0Amechanism%20over%20a%20more%20precise%2C%20position-based%20copying%20mechanism%3B%20and%20using%0Aintermediate%20generations%20to%20simulate%20recurrent%20data%20structures%2C%20akin%20to%20an%0Aimplicit%20scratchpad%20or%20Chain-of-Thought.%20Overall%2C%20by%20drawing%20an%20explicit%0Aconnection%20between%20neural%20chatbots%20and%20interpretable%2C%20symbolic%20mechanisms%2C%20our%0Aresults%20provide%20a%20new%20framework%20for%20the%20mechanistic%20analysis%20of%20conversational%0Aagents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10949v2&entry.124074799=Read"},
{"title": "A method for classification of data with uncertainty using hypothesis\n  testing", "author": "Shoma Yokura and Akihisa Ichiki", "abstract": "  Binary classification is a task that involves the classification of data into\none of two distinct classes. It is widely utilized in various fields. However,\nconventional classifiers tend to make overconfident predictions for data that\nbelong to overlapping regions of the two class distributions or for data\noutside the distributions (out-of-distribution data). Therefore, conventional\nclassifiers should not be applied in high-risk fields where classification\nresults can have significant consequences. In order to address this issue, it\nis necessary to quantify uncertainty and adopt decision-making approaches that\ntake it into account. Many methods have been proposed for this purpose;\nhowever, implementing these methods often requires performing resampling,\nimproving the structure or performance of models, and optimizing the thresholds\nof classifiers. We propose a new decision-making approach using two types of\nhypothesis testing. This method is capable of detecting ambiguous data that\nbelong to the overlapping regions of two class distributions, as well as\nout-of-distribution data that are not included in the training data\ndistribution. In addition, we quantify uncertainty using the empirical\ndistribution of feature values derived from the training data obtained through\nthe trained model. The classification threshold is determined by the\n$\\alpha$-quantile and ($1-\\alpha$)-quantile, where the significance level\n$\\alpha$ is set according to each specific situation.\n", "link": "http://arxiv.org/abs/2502.08582v1", "date": "2025-02-12", "relevancy": 1.4603, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4962}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4889}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20method%20for%20classification%20of%20data%20with%20uncertainty%20using%20hypothesis%0A%20%20testing&body=Title%3A%20A%20method%20for%20classification%20of%20data%20with%20uncertainty%20using%20hypothesis%0A%20%20testing%0AAuthor%3A%20Shoma%20Yokura%20and%20Akihisa%20Ichiki%0AAbstract%3A%20%20%20Binary%20classification%20is%20a%20task%20that%20involves%20the%20classification%20of%20data%20into%0Aone%20of%20two%20distinct%20classes.%20It%20is%20widely%20utilized%20in%20various%20fields.%20However%2C%0Aconventional%20classifiers%20tend%20to%20make%20overconfident%20predictions%20for%20data%20that%0Abelong%20to%20overlapping%20regions%20of%20the%20two%20class%20distributions%20or%20for%20data%0Aoutside%20the%20distributions%20%28out-of-distribution%20data%29.%20Therefore%2C%20conventional%0Aclassifiers%20should%20not%20be%20applied%20in%20high-risk%20fields%20where%20classification%0Aresults%20can%20have%20significant%20consequences.%20In%20order%20to%20address%20this%20issue%2C%20it%0Ais%20necessary%20to%20quantify%20uncertainty%20and%20adopt%20decision-making%20approaches%20that%0Atake%20it%20into%20account.%20Many%20methods%20have%20been%20proposed%20for%20this%20purpose%3B%0Ahowever%2C%20implementing%20these%20methods%20often%20requires%20performing%20resampling%2C%0Aimproving%20the%20structure%20or%20performance%20of%20models%2C%20and%20optimizing%20the%20thresholds%0Aof%20classifiers.%20We%20propose%20a%20new%20decision-making%20approach%20using%20two%20types%20of%0Ahypothesis%20testing.%20This%20method%20is%20capable%20of%20detecting%20ambiguous%20data%20that%0Abelong%20to%20the%20overlapping%20regions%20of%20two%20class%20distributions%2C%20as%20well%20as%0Aout-of-distribution%20data%20that%20are%20not%20included%20in%20the%20training%20data%0Adistribution.%20In%20addition%2C%20we%20quantify%20uncertainty%20using%20the%20empirical%0Adistribution%20of%20feature%20values%20derived%20from%20the%20training%20data%20obtained%20through%0Athe%20trained%20model.%20The%20classification%20threshold%20is%20determined%20by%20the%0A%24%5Calpha%24-quantile%20and%20%28%241-%5Calpha%24%29-quantile%2C%20where%20the%20significance%20level%0A%24%5Calpha%24%20is%20set%20according%20to%20each%20specific%20situation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08582v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520method%2520for%2520classification%2520of%2520data%2520with%2520uncertainty%2520using%2520hypothesis%250A%2520%2520testing%26entry.906535625%3DShoma%2520Yokura%2520and%2520Akihisa%2520Ichiki%26entry.1292438233%3D%2520%2520Binary%2520classification%2520is%2520a%2520task%2520that%2520involves%2520the%2520classification%2520of%2520data%2520into%250Aone%2520of%2520two%2520distinct%2520classes.%2520It%2520is%2520widely%2520utilized%2520in%2520various%2520fields.%2520However%252C%250Aconventional%2520classifiers%2520tend%2520to%2520make%2520overconfident%2520predictions%2520for%2520data%2520that%250Abelong%2520to%2520overlapping%2520regions%2520of%2520the%2520two%2520class%2520distributions%2520or%2520for%2520data%250Aoutside%2520the%2520distributions%2520%2528out-of-distribution%2520data%2529.%2520Therefore%252C%2520conventional%250Aclassifiers%2520should%2520not%2520be%2520applied%2520in%2520high-risk%2520fields%2520where%2520classification%250Aresults%2520can%2520have%2520significant%2520consequences.%2520In%2520order%2520to%2520address%2520this%2520issue%252C%2520it%250Ais%2520necessary%2520to%2520quantify%2520uncertainty%2520and%2520adopt%2520decision-making%2520approaches%2520that%250Atake%2520it%2520into%2520account.%2520Many%2520methods%2520have%2520been%2520proposed%2520for%2520this%2520purpose%253B%250Ahowever%252C%2520implementing%2520these%2520methods%2520often%2520requires%2520performing%2520resampling%252C%250Aimproving%2520the%2520structure%2520or%2520performance%2520of%2520models%252C%2520and%2520optimizing%2520the%2520thresholds%250Aof%2520classifiers.%2520We%2520propose%2520a%2520new%2520decision-making%2520approach%2520using%2520two%2520types%2520of%250Ahypothesis%2520testing.%2520This%2520method%2520is%2520capable%2520of%2520detecting%2520ambiguous%2520data%2520that%250Abelong%2520to%2520the%2520overlapping%2520regions%2520of%2520two%2520class%2520distributions%252C%2520as%2520well%2520as%250Aout-of-distribution%2520data%2520that%2520are%2520not%2520included%2520in%2520the%2520training%2520data%250Adistribution.%2520In%2520addition%252C%2520we%2520quantify%2520uncertainty%2520using%2520the%2520empirical%250Adistribution%2520of%2520feature%2520values%2520derived%2520from%2520the%2520training%2520data%2520obtained%2520through%250Athe%2520trained%2520model.%2520The%2520classification%2520threshold%2520is%2520determined%2520by%2520the%250A%2524%255Calpha%2524-quantile%2520and%2520%2528%25241-%255Calpha%2524%2529-quantile%252C%2520where%2520the%2520significance%2520level%250A%2524%255Calpha%2524%2520is%2520set%2520according%2520to%2520each%2520specific%2520situation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08582v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20method%20for%20classification%20of%20data%20with%20uncertainty%20using%20hypothesis%0A%20%20testing&entry.906535625=Shoma%20Yokura%20and%20Akihisa%20Ichiki&entry.1292438233=%20%20Binary%20classification%20is%20a%20task%20that%20involves%20the%20classification%20of%20data%20into%0Aone%20of%20two%20distinct%20classes.%20It%20is%20widely%20utilized%20in%20various%20fields.%20However%2C%0Aconventional%20classifiers%20tend%20to%20make%20overconfident%20predictions%20for%20data%20that%0Abelong%20to%20overlapping%20regions%20of%20the%20two%20class%20distributions%20or%20for%20data%0Aoutside%20the%20distributions%20%28out-of-distribution%20data%29.%20Therefore%2C%20conventional%0Aclassifiers%20should%20not%20be%20applied%20in%20high-risk%20fields%20where%20classification%0Aresults%20can%20have%20significant%20consequences.%20In%20order%20to%20address%20this%20issue%2C%20it%0Ais%20necessary%20to%20quantify%20uncertainty%20and%20adopt%20decision-making%20approaches%20that%0Atake%20it%20into%20account.%20Many%20methods%20have%20been%20proposed%20for%20this%20purpose%3B%0Ahowever%2C%20implementing%20these%20methods%20often%20requires%20performing%20resampling%2C%0Aimproving%20the%20structure%20or%20performance%20of%20models%2C%20and%20optimizing%20the%20thresholds%0Aof%20classifiers.%20We%20propose%20a%20new%20decision-making%20approach%20using%20two%20types%20of%0Ahypothesis%20testing.%20This%20method%20is%20capable%20of%20detecting%20ambiguous%20data%20that%0Abelong%20to%20the%20overlapping%20regions%20of%20two%20class%20distributions%2C%20as%20well%20as%0Aout-of-distribution%20data%20that%20are%20not%20included%20in%20the%20training%20data%0Adistribution.%20In%20addition%2C%20we%20quantify%20uncertainty%20using%20the%20empirical%0Adistribution%20of%20feature%20values%20derived%20from%20the%20training%20data%20obtained%20through%0Athe%20trained%20model.%20The%20classification%20threshold%20is%20determined%20by%20the%0A%24%5Calpha%24-quantile%20and%20%28%241-%5Calpha%24%29-quantile%2C%20where%20the%20significance%20level%0A%24%5Calpha%24%20is%20set%20according%20to%20each%20specific%20situation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08582v1&entry.124074799=Read"},
{"title": "COAST: Intelligent Time-Adaptive Neural Operators", "author": "Zhikai Wu and Shiyang Zhang and Sizhuang He and Sifan Wang and Min Zhu and Anran Jiao and Lu Lu and David van Dijk", "abstract": "  We introduce Causal Operator with Adaptive Solver Transformer (COAST), a\nnovel neural operator learning method that leverages a causal language model\n(CLM) framework to dynamically adapt time steps. Our method predicts both the\nevolution of a system and its optimal time step, intelligently balancing\ncomputational efficiency and accuracy. We find that COAST generates variable\nstep sizes that correlate with the underlying system intrinsicities, both\nwithin and across dynamical systems. Within a single trajectory, smaller steps\nare taken in regions of high complexity, while larger steps are employed in\nsimpler regions. Across different systems, more complex dynamics receive more\ngranular time steps. Benchmarked on diverse systems with varied dynamics, COAST\nconsistently outperforms state-of-the-art methods, achieving superior\nperformance in both efficiency and accuracy. This work underscores the\npotential of CLM-based intelligent adaptive solvers for scalable operator\nlearning of dynamical systems.\n", "link": "http://arxiv.org/abs/2502.08574v1", "date": "2025-02-12", "relevancy": 1.4469, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4921}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4703}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4696}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COAST%3A%20Intelligent%20Time-Adaptive%20Neural%20Operators&body=Title%3A%20COAST%3A%20Intelligent%20Time-Adaptive%20Neural%20Operators%0AAuthor%3A%20Zhikai%20Wu%20and%20Shiyang%20Zhang%20and%20Sizhuang%20He%20and%20Sifan%20Wang%20and%20Min%20Zhu%20and%20Anran%20Jiao%20and%20Lu%20Lu%20and%20David%20van%20Dijk%0AAbstract%3A%20%20%20We%20introduce%20Causal%20Operator%20with%20Adaptive%20Solver%20Transformer%20%28COAST%29%2C%20a%0Anovel%20neural%20operator%20learning%20method%20that%20leverages%20a%20causal%20language%20model%0A%28CLM%29%20framework%20to%20dynamically%20adapt%20time%20steps.%20Our%20method%20predicts%20both%20the%0Aevolution%20of%20a%20system%20and%20its%20optimal%20time%20step%2C%20intelligently%20balancing%0Acomputational%20efficiency%20and%20accuracy.%20We%20find%20that%20COAST%20generates%20variable%0Astep%20sizes%20that%20correlate%20with%20the%20underlying%20system%20intrinsicities%2C%20both%0Awithin%20and%20across%20dynamical%20systems.%20Within%20a%20single%20trajectory%2C%20smaller%20steps%0Aare%20taken%20in%20regions%20of%20high%20complexity%2C%20while%20larger%20steps%20are%20employed%20in%0Asimpler%20regions.%20Across%20different%20systems%2C%20more%20complex%20dynamics%20receive%20more%0Agranular%20time%20steps.%20Benchmarked%20on%20diverse%20systems%20with%20varied%20dynamics%2C%20COAST%0Aconsistently%20outperforms%20state-of-the-art%20methods%2C%20achieving%20superior%0Aperformance%20in%20both%20efficiency%20and%20accuracy.%20This%20work%20underscores%20the%0Apotential%20of%20CLM-based%20intelligent%20adaptive%20solvers%20for%20scalable%20operator%0Alearning%20of%20dynamical%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08574v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOAST%253A%2520Intelligent%2520Time-Adaptive%2520Neural%2520Operators%26entry.906535625%3DZhikai%2520Wu%2520and%2520Shiyang%2520Zhang%2520and%2520Sizhuang%2520He%2520and%2520Sifan%2520Wang%2520and%2520Min%2520Zhu%2520and%2520Anran%2520Jiao%2520and%2520Lu%2520Lu%2520and%2520David%2520van%2520Dijk%26entry.1292438233%3D%2520%2520We%2520introduce%2520Causal%2520Operator%2520with%2520Adaptive%2520Solver%2520Transformer%2520%2528COAST%2529%252C%2520a%250Anovel%2520neural%2520operator%2520learning%2520method%2520that%2520leverages%2520a%2520causal%2520language%2520model%250A%2528CLM%2529%2520framework%2520to%2520dynamically%2520adapt%2520time%2520steps.%2520Our%2520method%2520predicts%2520both%2520the%250Aevolution%2520of%2520a%2520system%2520and%2520its%2520optimal%2520time%2520step%252C%2520intelligently%2520balancing%250Acomputational%2520efficiency%2520and%2520accuracy.%2520We%2520find%2520that%2520COAST%2520generates%2520variable%250Astep%2520sizes%2520that%2520correlate%2520with%2520the%2520underlying%2520system%2520intrinsicities%252C%2520both%250Awithin%2520and%2520across%2520dynamical%2520systems.%2520Within%2520a%2520single%2520trajectory%252C%2520smaller%2520steps%250Aare%2520taken%2520in%2520regions%2520of%2520high%2520complexity%252C%2520while%2520larger%2520steps%2520are%2520employed%2520in%250Asimpler%2520regions.%2520Across%2520different%2520systems%252C%2520more%2520complex%2520dynamics%2520receive%2520more%250Agranular%2520time%2520steps.%2520Benchmarked%2520on%2520diverse%2520systems%2520with%2520varied%2520dynamics%252C%2520COAST%250Aconsistently%2520outperforms%2520state-of-the-art%2520methods%252C%2520achieving%2520superior%250Aperformance%2520in%2520both%2520efficiency%2520and%2520accuracy.%2520This%2520work%2520underscores%2520the%250Apotential%2520of%2520CLM-based%2520intelligent%2520adaptive%2520solvers%2520for%2520scalable%2520operator%250Alearning%2520of%2520dynamical%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08574v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COAST%3A%20Intelligent%20Time-Adaptive%20Neural%20Operators&entry.906535625=Zhikai%20Wu%20and%20Shiyang%20Zhang%20and%20Sizhuang%20He%20and%20Sifan%20Wang%20and%20Min%20Zhu%20and%20Anran%20Jiao%20and%20Lu%20Lu%20and%20David%20van%20Dijk&entry.1292438233=%20%20We%20introduce%20Causal%20Operator%20with%20Adaptive%20Solver%20Transformer%20%28COAST%29%2C%20a%0Anovel%20neural%20operator%20learning%20method%20that%20leverages%20a%20causal%20language%20model%0A%28CLM%29%20framework%20to%20dynamically%20adapt%20time%20steps.%20Our%20method%20predicts%20both%20the%0Aevolution%20of%20a%20system%20and%20its%20optimal%20time%20step%2C%20intelligently%20balancing%0Acomputational%20efficiency%20and%20accuracy.%20We%20find%20that%20COAST%20generates%20variable%0Astep%20sizes%20that%20correlate%20with%20the%20underlying%20system%20intrinsicities%2C%20both%0Awithin%20and%20across%20dynamical%20systems.%20Within%20a%20single%20trajectory%2C%20smaller%20steps%0Aare%20taken%20in%20regions%20of%20high%20complexity%2C%20while%20larger%20steps%20are%20employed%20in%0Asimpler%20regions.%20Across%20different%20systems%2C%20more%20complex%20dynamics%20receive%20more%0Agranular%20time%20steps.%20Benchmarked%20on%20diverse%20systems%20with%20varied%20dynamics%2C%20COAST%0Aconsistently%20outperforms%20state-of-the-art%20methods%2C%20achieving%20superior%0Aperformance%20in%20both%20efficiency%20and%20accuracy.%20This%20work%20underscores%20the%0Apotential%20of%20CLM-based%20intelligent%20adaptive%20solvers%20for%20scalable%20operator%0Alearning%20of%20dynamical%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08574v1&entry.124074799=Read"},
{"title": "Beyond Predictions: A Participatory Framework for Multi-Stakeholder\n  Decision-Making", "author": "Vittoria Vineis and Giuseppe Perelli and Gabriele Tolomei", "abstract": "  Conventional decision-support systems, primarily based on supervised\nlearning, focus on outcome prediction models to recommend actions. However,\nthey often fail to account for the complexities of multi-actor environments,\nwhere diverse and potentially conflicting stakeholder preferences must be\nbalanced. In this paper, we propose a novel participatory framework that\nredefines decision-making as a multi-stakeholder optimization problem,\ncapturing each actor's preferences through context-dependent reward functions.\nOur framework leverages $k$-fold cross-validation to fine-tune user-provided\noutcome prediction models and evaluate decision strategies, including\ncompromise functions mediating stakeholder trade-offs. We introduce a synthetic\nscoring mechanism that exploits user-defined preferences across multiple\nmetrics to rank decision-making strategies and identify the optimal\ndecision-maker. The selected decision-maker can then be used to generate\nactionable recommendations for new data. We validate our framework using two\nreal-world use cases, demonstrating its ability to deliver recommendations that\neffectively balance multiple metrics, achieving results that are often beyond\nthe scope of purely prediction-based methods. Ablation studies demonstrate that\nour framework, with its modular, model-agnostic, and inherently transparent\ndesign, integrates seamlessly with various predictive models, reward\nstructures, evaluation metrics, and sample sizes, making it particularly suited\nfor complex, high-stakes decision-making contexts.\n", "link": "http://arxiv.org/abs/2502.08542v1", "date": "2025-02-12", "relevancy": 1.4439, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5246}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4755}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Predictions%3A%20A%20Participatory%20Framework%20for%20Multi-Stakeholder%0A%20%20Decision-Making&body=Title%3A%20Beyond%20Predictions%3A%20A%20Participatory%20Framework%20for%20Multi-Stakeholder%0A%20%20Decision-Making%0AAuthor%3A%20Vittoria%20Vineis%20and%20Giuseppe%20Perelli%20and%20Gabriele%20Tolomei%0AAbstract%3A%20%20%20Conventional%20decision-support%20systems%2C%20primarily%20based%20on%20supervised%0Alearning%2C%20focus%20on%20outcome%20prediction%20models%20to%20recommend%20actions.%20However%2C%0Athey%20often%20fail%20to%20account%20for%20the%20complexities%20of%20multi-actor%20environments%2C%0Awhere%20diverse%20and%20potentially%20conflicting%20stakeholder%20preferences%20must%20be%0Abalanced.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20participatory%20framework%20that%0Aredefines%20decision-making%20as%20a%20multi-stakeholder%20optimization%20problem%2C%0Acapturing%20each%20actor%27s%20preferences%20through%20context-dependent%20reward%20functions.%0AOur%20framework%20leverages%20%24k%24-fold%20cross-validation%20to%20fine-tune%20user-provided%0Aoutcome%20prediction%20models%20and%20evaluate%20decision%20strategies%2C%20including%0Acompromise%20functions%20mediating%20stakeholder%20trade-offs.%20We%20introduce%20a%20synthetic%0Ascoring%20mechanism%20that%20exploits%20user-defined%20preferences%20across%20multiple%0Ametrics%20to%20rank%20decision-making%20strategies%20and%20identify%20the%20optimal%0Adecision-maker.%20The%20selected%20decision-maker%20can%20then%20be%20used%20to%20generate%0Aactionable%20recommendations%20for%20new%20data.%20We%20validate%20our%20framework%20using%20two%0Areal-world%20use%20cases%2C%20demonstrating%20its%20ability%20to%20deliver%20recommendations%20that%0Aeffectively%20balance%20multiple%20metrics%2C%20achieving%20results%20that%20are%20often%20beyond%0Athe%20scope%20of%20purely%20prediction-based%20methods.%20Ablation%20studies%20demonstrate%20that%0Aour%20framework%2C%20with%20its%20modular%2C%20model-agnostic%2C%20and%20inherently%20transparent%0Adesign%2C%20integrates%20seamlessly%20with%20various%20predictive%20models%2C%20reward%0Astructures%2C%20evaluation%20metrics%2C%20and%20sample%20sizes%2C%20making%20it%20particularly%20suited%0Afor%20complex%2C%20high-stakes%20decision-making%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08542v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Predictions%253A%2520A%2520Participatory%2520Framework%2520for%2520Multi-Stakeholder%250A%2520%2520Decision-Making%26entry.906535625%3DVittoria%2520Vineis%2520and%2520Giuseppe%2520Perelli%2520and%2520Gabriele%2520Tolomei%26entry.1292438233%3D%2520%2520Conventional%2520decision-support%2520systems%252C%2520primarily%2520based%2520on%2520supervised%250Alearning%252C%2520focus%2520on%2520outcome%2520prediction%2520models%2520to%2520recommend%2520actions.%2520However%252C%250Athey%2520often%2520fail%2520to%2520account%2520for%2520the%2520complexities%2520of%2520multi-actor%2520environments%252C%250Awhere%2520diverse%2520and%2520potentially%2520conflicting%2520stakeholder%2520preferences%2520must%2520be%250Abalanced.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520participatory%2520framework%2520that%250Aredefines%2520decision-making%2520as%2520a%2520multi-stakeholder%2520optimization%2520problem%252C%250Acapturing%2520each%2520actor%2527s%2520preferences%2520through%2520context-dependent%2520reward%2520functions.%250AOur%2520framework%2520leverages%2520%2524k%2524-fold%2520cross-validation%2520to%2520fine-tune%2520user-provided%250Aoutcome%2520prediction%2520models%2520and%2520evaluate%2520decision%2520strategies%252C%2520including%250Acompromise%2520functions%2520mediating%2520stakeholder%2520trade-offs.%2520We%2520introduce%2520a%2520synthetic%250Ascoring%2520mechanism%2520that%2520exploits%2520user-defined%2520preferences%2520across%2520multiple%250Ametrics%2520to%2520rank%2520decision-making%2520strategies%2520and%2520identify%2520the%2520optimal%250Adecision-maker.%2520The%2520selected%2520decision-maker%2520can%2520then%2520be%2520used%2520to%2520generate%250Aactionable%2520recommendations%2520for%2520new%2520data.%2520We%2520validate%2520our%2520framework%2520using%2520two%250Areal-world%2520use%2520cases%252C%2520demonstrating%2520its%2520ability%2520to%2520deliver%2520recommendations%2520that%250Aeffectively%2520balance%2520multiple%2520metrics%252C%2520achieving%2520results%2520that%2520are%2520often%2520beyond%250Athe%2520scope%2520of%2520purely%2520prediction-based%2520methods.%2520Ablation%2520studies%2520demonstrate%2520that%250Aour%2520framework%252C%2520with%2520its%2520modular%252C%2520model-agnostic%252C%2520and%2520inherently%2520transparent%250Adesign%252C%2520integrates%2520seamlessly%2520with%2520various%2520predictive%2520models%252C%2520reward%250Astructures%252C%2520evaluation%2520metrics%252C%2520and%2520sample%2520sizes%252C%2520making%2520it%2520particularly%2520suited%250Afor%2520complex%252C%2520high-stakes%2520decision-making%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08542v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Predictions%3A%20A%20Participatory%20Framework%20for%20Multi-Stakeholder%0A%20%20Decision-Making&entry.906535625=Vittoria%20Vineis%20and%20Giuseppe%20Perelli%20and%20Gabriele%20Tolomei&entry.1292438233=%20%20Conventional%20decision-support%20systems%2C%20primarily%20based%20on%20supervised%0Alearning%2C%20focus%20on%20outcome%20prediction%20models%20to%20recommend%20actions.%20However%2C%0Athey%20often%20fail%20to%20account%20for%20the%20complexities%20of%20multi-actor%20environments%2C%0Awhere%20diverse%20and%20potentially%20conflicting%20stakeholder%20preferences%20must%20be%0Abalanced.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20participatory%20framework%20that%0Aredefines%20decision-making%20as%20a%20multi-stakeholder%20optimization%20problem%2C%0Acapturing%20each%20actor%27s%20preferences%20through%20context-dependent%20reward%20functions.%0AOur%20framework%20leverages%20%24k%24-fold%20cross-validation%20to%20fine-tune%20user-provided%0Aoutcome%20prediction%20models%20and%20evaluate%20decision%20strategies%2C%20including%0Acompromise%20functions%20mediating%20stakeholder%20trade-offs.%20We%20introduce%20a%20synthetic%0Ascoring%20mechanism%20that%20exploits%20user-defined%20preferences%20across%20multiple%0Ametrics%20to%20rank%20decision-making%20strategies%20and%20identify%20the%20optimal%0Adecision-maker.%20The%20selected%20decision-maker%20can%20then%20be%20used%20to%20generate%0Aactionable%20recommendations%20for%20new%20data.%20We%20validate%20our%20framework%20using%20two%0Areal-world%20use%20cases%2C%20demonstrating%20its%20ability%20to%20deliver%20recommendations%20that%0Aeffectively%20balance%20multiple%20metrics%2C%20achieving%20results%20that%20are%20often%20beyond%0Athe%20scope%20of%20purely%20prediction-based%20methods.%20Ablation%20studies%20demonstrate%20that%0Aour%20framework%2C%20with%20its%20modular%2C%20model-agnostic%2C%20and%20inherently%20transparent%0Adesign%2C%20integrates%20seamlessly%20with%20various%20predictive%20models%2C%20reward%0Astructures%2C%20evaluation%20metrics%2C%20and%20sample%20sizes%2C%20making%20it%20particularly%20suited%0Afor%20complex%2C%20high-stakes%20decision-making%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08542v1&entry.124074799=Read"},
{"title": "Representation Learning to Advance Multi-institutional Studies with\n  Electronic Health Record Data", "author": "Doudou Zhou and Han Tong and Linshanshan Wang and Suqi Liu and Xin Xiong and Ziming Gan and Romain Griffier and Boris Hejblum and Yun-Chung Liu and Chuan Hong and Clara-Lea Bonzel and Tianrun Cai and Kevin Pan and Yuk-Lam Ho and Lauren Costa and Vidul A. Panickan and J. Michael Gaziano and Kenneth Mandl and Vianney Jouhet and Rodolphe Thiebaut and Zongqi Xia and Kelly Cho and Katherine Liao and Tianxi Cai", "abstract": "  The adoption of EHRs has expanded opportunities to leverage data-driven\nalgorithms in clinical care and research. A major bottleneck in effectively\nconducting multi-institutional EHR studies is the data heterogeneity across\nsystems with numerous codes that either do not exist or represent different\nclinical concepts across institutions. The need for data privacy further limits\nthe feasibility of including multi-institutional patient-level data required to\nstudy similarities and differences across patient subgroups. To address these\nchallenges, we developed the GAME algorithm. Tested and validated across 7\ninstitutions and 2 languages, GAME integrates data in several levels: (1) at\nthe institutional level with knowledge graphs to establish relationships\nbetween codes and existing knowledge sources, providing the medical context for\nstandard codes and their relationship to each other; (2) between institutions,\nleveraging language models to determine the relationships between\ninstitution-specific codes with established standard codes; and (3) quantifying\nthe strength of the relationships between codes using a graph attention\nnetwork. Jointly trained embeddings are created using transfer and federated\nlearning to preserve data privacy. In this study, we demonstrate the\napplicability of GAME in selecting relevant features as inputs for AI-driven\nalgorithms in a range of conditions, e.g., heart failure, rheumatoid arthritis.\nWe then highlight the application of GAME harmonized multi-institutional EHR\ndata in a study of Alzheimer's disease outcomes and suicide risk among patients\nwith mental health disorders, without sharing patient-level data outside\nindividual institutions.\n", "link": "http://arxiv.org/abs/2502.08547v1", "date": "2025-02-12", "relevancy": 1.4269, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4777}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4752}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Representation%20Learning%20to%20Advance%20Multi-institutional%20Studies%20with%0A%20%20Electronic%20Health%20Record%20Data&body=Title%3A%20Representation%20Learning%20to%20Advance%20Multi-institutional%20Studies%20with%0A%20%20Electronic%20Health%20Record%20Data%0AAuthor%3A%20Doudou%20Zhou%20and%20Han%20Tong%20and%20Linshanshan%20Wang%20and%20Suqi%20Liu%20and%20Xin%20Xiong%20and%20Ziming%20Gan%20and%20Romain%20Griffier%20and%20Boris%20Hejblum%20and%20Yun-Chung%20Liu%20and%20Chuan%20Hong%20and%20Clara-Lea%20Bonzel%20and%20Tianrun%20Cai%20and%20Kevin%20Pan%20and%20Yuk-Lam%20Ho%20and%20Lauren%20Costa%20and%20Vidul%20A.%20Panickan%20and%20J.%20Michael%20Gaziano%20and%20Kenneth%20Mandl%20and%20Vianney%20Jouhet%20and%20Rodolphe%20Thiebaut%20and%20Zongqi%20Xia%20and%20Kelly%20Cho%20and%20Katherine%20Liao%20and%20Tianxi%20Cai%0AAbstract%3A%20%20%20The%20adoption%20of%20EHRs%20has%20expanded%20opportunities%20to%20leverage%20data-driven%0Aalgorithms%20in%20clinical%20care%20and%20research.%20A%20major%20bottleneck%20in%20effectively%0Aconducting%20multi-institutional%20EHR%20studies%20is%20the%20data%20heterogeneity%20across%0Asystems%20with%20numerous%20codes%20that%20either%20do%20not%20exist%20or%20represent%20different%0Aclinical%20concepts%20across%20institutions.%20The%20need%20for%20data%20privacy%20further%20limits%0Athe%20feasibility%20of%20including%20multi-institutional%20patient-level%20data%20required%20to%0Astudy%20similarities%20and%20differences%20across%20patient%20subgroups.%20To%20address%20these%0Achallenges%2C%20we%20developed%20the%20GAME%20algorithm.%20Tested%20and%20validated%20across%207%0Ainstitutions%20and%202%20languages%2C%20GAME%20integrates%20data%20in%20several%20levels%3A%20%281%29%20at%0Athe%20institutional%20level%20with%20knowledge%20graphs%20to%20establish%20relationships%0Abetween%20codes%20and%20existing%20knowledge%20sources%2C%20providing%20the%20medical%20context%20for%0Astandard%20codes%20and%20their%20relationship%20to%20each%20other%3B%20%282%29%20between%20institutions%2C%0Aleveraging%20language%20models%20to%20determine%20the%20relationships%20between%0Ainstitution-specific%20codes%20with%20established%20standard%20codes%3B%20and%20%283%29%20quantifying%0Athe%20strength%20of%20the%20relationships%20between%20codes%20using%20a%20graph%20attention%0Anetwork.%20Jointly%20trained%20embeddings%20are%20created%20using%20transfer%20and%20federated%0Alearning%20to%20preserve%20data%20privacy.%20In%20this%20study%2C%20we%20demonstrate%20the%0Aapplicability%20of%20GAME%20in%20selecting%20relevant%20features%20as%20inputs%20for%20AI-driven%0Aalgorithms%20in%20a%20range%20of%20conditions%2C%20e.g.%2C%20heart%20failure%2C%20rheumatoid%20arthritis.%0AWe%20then%20highlight%20the%20application%20of%20GAME%20harmonized%20multi-institutional%20EHR%0Adata%20in%20a%20study%20of%20Alzheimer%27s%20disease%20outcomes%20and%20suicide%20risk%20among%20patients%0Awith%20mental%20health%20disorders%2C%20without%20sharing%20patient-level%20data%20outside%0Aindividual%20institutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08547v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepresentation%2520Learning%2520to%2520Advance%2520Multi-institutional%2520Studies%2520with%250A%2520%2520Electronic%2520Health%2520Record%2520Data%26entry.906535625%3DDoudou%2520Zhou%2520and%2520Han%2520Tong%2520and%2520Linshanshan%2520Wang%2520and%2520Suqi%2520Liu%2520and%2520Xin%2520Xiong%2520and%2520Ziming%2520Gan%2520and%2520Romain%2520Griffier%2520and%2520Boris%2520Hejblum%2520and%2520Yun-Chung%2520Liu%2520and%2520Chuan%2520Hong%2520and%2520Clara-Lea%2520Bonzel%2520and%2520Tianrun%2520Cai%2520and%2520Kevin%2520Pan%2520and%2520Yuk-Lam%2520Ho%2520and%2520Lauren%2520Costa%2520and%2520Vidul%2520A.%2520Panickan%2520and%2520J.%2520Michael%2520Gaziano%2520and%2520Kenneth%2520Mandl%2520and%2520Vianney%2520Jouhet%2520and%2520Rodolphe%2520Thiebaut%2520and%2520Zongqi%2520Xia%2520and%2520Kelly%2520Cho%2520and%2520Katherine%2520Liao%2520and%2520Tianxi%2520Cai%26entry.1292438233%3D%2520%2520The%2520adoption%2520of%2520EHRs%2520has%2520expanded%2520opportunities%2520to%2520leverage%2520data-driven%250Aalgorithms%2520in%2520clinical%2520care%2520and%2520research.%2520A%2520major%2520bottleneck%2520in%2520effectively%250Aconducting%2520multi-institutional%2520EHR%2520studies%2520is%2520the%2520data%2520heterogeneity%2520across%250Asystems%2520with%2520numerous%2520codes%2520that%2520either%2520do%2520not%2520exist%2520or%2520represent%2520different%250Aclinical%2520concepts%2520across%2520institutions.%2520The%2520need%2520for%2520data%2520privacy%2520further%2520limits%250Athe%2520feasibility%2520of%2520including%2520multi-institutional%2520patient-level%2520data%2520required%2520to%250Astudy%2520similarities%2520and%2520differences%2520across%2520patient%2520subgroups.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520developed%2520the%2520GAME%2520algorithm.%2520Tested%2520and%2520validated%2520across%25207%250Ainstitutions%2520and%25202%2520languages%252C%2520GAME%2520integrates%2520data%2520in%2520several%2520levels%253A%2520%25281%2529%2520at%250Athe%2520institutional%2520level%2520with%2520knowledge%2520graphs%2520to%2520establish%2520relationships%250Abetween%2520codes%2520and%2520existing%2520knowledge%2520sources%252C%2520providing%2520the%2520medical%2520context%2520for%250Astandard%2520codes%2520and%2520their%2520relationship%2520to%2520each%2520other%253B%2520%25282%2529%2520between%2520institutions%252C%250Aleveraging%2520language%2520models%2520to%2520determine%2520the%2520relationships%2520between%250Ainstitution-specific%2520codes%2520with%2520established%2520standard%2520codes%253B%2520and%2520%25283%2529%2520quantifying%250Athe%2520strength%2520of%2520the%2520relationships%2520between%2520codes%2520using%2520a%2520graph%2520attention%250Anetwork.%2520Jointly%2520trained%2520embeddings%2520are%2520created%2520using%2520transfer%2520and%2520federated%250Alearning%2520to%2520preserve%2520data%2520privacy.%2520In%2520this%2520study%252C%2520we%2520demonstrate%2520the%250Aapplicability%2520of%2520GAME%2520in%2520selecting%2520relevant%2520features%2520as%2520inputs%2520for%2520AI-driven%250Aalgorithms%2520in%2520a%2520range%2520of%2520conditions%252C%2520e.g.%252C%2520heart%2520failure%252C%2520rheumatoid%2520arthritis.%250AWe%2520then%2520highlight%2520the%2520application%2520of%2520GAME%2520harmonized%2520multi-institutional%2520EHR%250Adata%2520in%2520a%2520study%2520of%2520Alzheimer%2527s%2520disease%2520outcomes%2520and%2520suicide%2520risk%2520among%2520patients%250Awith%2520mental%2520health%2520disorders%252C%2520without%2520sharing%2520patient-level%2520data%2520outside%250Aindividual%2520institutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08547v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Representation%20Learning%20to%20Advance%20Multi-institutional%20Studies%20with%0A%20%20Electronic%20Health%20Record%20Data&entry.906535625=Doudou%20Zhou%20and%20Han%20Tong%20and%20Linshanshan%20Wang%20and%20Suqi%20Liu%20and%20Xin%20Xiong%20and%20Ziming%20Gan%20and%20Romain%20Griffier%20and%20Boris%20Hejblum%20and%20Yun-Chung%20Liu%20and%20Chuan%20Hong%20and%20Clara-Lea%20Bonzel%20and%20Tianrun%20Cai%20and%20Kevin%20Pan%20and%20Yuk-Lam%20Ho%20and%20Lauren%20Costa%20and%20Vidul%20A.%20Panickan%20and%20J.%20Michael%20Gaziano%20and%20Kenneth%20Mandl%20and%20Vianney%20Jouhet%20and%20Rodolphe%20Thiebaut%20and%20Zongqi%20Xia%20and%20Kelly%20Cho%20and%20Katherine%20Liao%20and%20Tianxi%20Cai&entry.1292438233=%20%20The%20adoption%20of%20EHRs%20has%20expanded%20opportunities%20to%20leverage%20data-driven%0Aalgorithms%20in%20clinical%20care%20and%20research.%20A%20major%20bottleneck%20in%20effectively%0Aconducting%20multi-institutional%20EHR%20studies%20is%20the%20data%20heterogeneity%20across%0Asystems%20with%20numerous%20codes%20that%20either%20do%20not%20exist%20or%20represent%20different%0Aclinical%20concepts%20across%20institutions.%20The%20need%20for%20data%20privacy%20further%20limits%0Athe%20feasibility%20of%20including%20multi-institutional%20patient-level%20data%20required%20to%0Astudy%20similarities%20and%20differences%20across%20patient%20subgroups.%20To%20address%20these%0Achallenges%2C%20we%20developed%20the%20GAME%20algorithm.%20Tested%20and%20validated%20across%207%0Ainstitutions%20and%202%20languages%2C%20GAME%20integrates%20data%20in%20several%20levels%3A%20%281%29%20at%0Athe%20institutional%20level%20with%20knowledge%20graphs%20to%20establish%20relationships%0Abetween%20codes%20and%20existing%20knowledge%20sources%2C%20providing%20the%20medical%20context%20for%0Astandard%20codes%20and%20their%20relationship%20to%20each%20other%3B%20%282%29%20between%20institutions%2C%0Aleveraging%20language%20models%20to%20determine%20the%20relationships%20between%0Ainstitution-specific%20codes%20with%20established%20standard%20codes%3B%20and%20%283%29%20quantifying%0Athe%20strength%20of%20the%20relationships%20between%20codes%20using%20a%20graph%20attention%0Anetwork.%20Jointly%20trained%20embeddings%20are%20created%20using%20transfer%20and%20federated%0Alearning%20to%20preserve%20data%20privacy.%20In%20this%20study%2C%20we%20demonstrate%20the%0Aapplicability%20of%20GAME%20in%20selecting%20relevant%20features%20as%20inputs%20for%20AI-driven%0Aalgorithms%20in%20a%20range%20of%20conditions%2C%20e.g.%2C%20heart%20failure%2C%20rheumatoid%20arthritis.%0AWe%20then%20highlight%20the%20application%20of%20GAME%20harmonized%20multi-institutional%20EHR%0Adata%20in%20a%20study%20of%20Alzheimer%27s%20disease%20outcomes%20and%20suicide%20risk%20among%20patients%0Awith%20mental%20health%20disorders%2C%20without%20sharing%20patient-level%20data%20outside%0Aindividual%20institutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08547v1&entry.124074799=Read"},
{"title": "Two-stage hybrid models for enhancing forecasting accuracy on\n  heterogeneous time series", "author": "Junru Ren and Shaomin Wu", "abstract": "  Compared to local models built in a series-by-series manner, global models\nleverage relevant information across time series, resulting in improved\nforecasting performance and generalization capacity. Constructing global models\non a set of time series is becoming mainstream in the field of time series\nforecasting. However, the advantages of global models may not always be\nrealized when dealing with heterogeneous data. While they can adapt to\nheterogeneous datasets by increasing the model complexity, the model cannot be\ninfinitely complex due to the finite sample size, which poses challenges for\nthe application of global models. Additionally, determining whether the time\nseries data is homogeneous or heterogeneous can be ambiguous in practice. To\naddress these research gaps, this paper argues that the heterogeneity of the\ndata should be defined by the global model used, and for each series, the\nportion not modelled by the global model represents heterogeneity. It further\nproposes two-stage hybrid models, which include a second stage to identify and\nmodel heterogeneous patterns. In this second stage, we can estimate either all\nlocal models or sub-global models across different domains divided based on\nheterogeneity. Experiments on four open datasets reveal that the proposed\nmethods significantly outperform five existing models, indicating they\ncontribute to fully unleash the potential of global models on heterogeneous\ndatasets.\n", "link": "http://arxiv.org/abs/2502.08600v1", "date": "2025-02-12", "relevancy": 1.4207, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.476}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4755}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Two-stage%20hybrid%20models%20for%20enhancing%20forecasting%20accuracy%20on%0A%20%20heterogeneous%20time%20series&body=Title%3A%20Two-stage%20hybrid%20models%20for%20enhancing%20forecasting%20accuracy%20on%0A%20%20heterogeneous%20time%20series%0AAuthor%3A%20Junru%20Ren%20and%20Shaomin%20Wu%0AAbstract%3A%20%20%20Compared%20to%20local%20models%20built%20in%20a%20series-by-series%20manner%2C%20global%20models%0Aleverage%20relevant%20information%20across%20time%20series%2C%20resulting%20in%20improved%0Aforecasting%20performance%20and%20generalization%20capacity.%20Constructing%20global%20models%0Aon%20a%20set%20of%20time%20series%20is%20becoming%20mainstream%20in%20the%20field%20of%20time%20series%0Aforecasting.%20However%2C%20the%20advantages%20of%20global%20models%20may%20not%20always%20be%0Arealized%20when%20dealing%20with%20heterogeneous%20data.%20While%20they%20can%20adapt%20to%0Aheterogeneous%20datasets%20by%20increasing%20the%20model%20complexity%2C%20the%20model%20cannot%20be%0Ainfinitely%20complex%20due%20to%20the%20finite%20sample%20size%2C%20which%20poses%20challenges%20for%0Athe%20application%20of%20global%20models.%20Additionally%2C%20determining%20whether%20the%20time%0Aseries%20data%20is%20homogeneous%20or%20heterogeneous%20can%20be%20ambiguous%20in%20practice.%20To%0Aaddress%20these%20research%20gaps%2C%20this%20paper%20argues%20that%20the%20heterogeneity%20of%20the%0Adata%20should%20be%20defined%20by%20the%20global%20model%20used%2C%20and%20for%20each%20series%2C%20the%0Aportion%20not%20modelled%20by%20the%20global%20model%20represents%20heterogeneity.%20It%20further%0Aproposes%20two-stage%20hybrid%20models%2C%20which%20include%20a%20second%20stage%20to%20identify%20and%0Amodel%20heterogeneous%20patterns.%20In%20this%20second%20stage%2C%20we%20can%20estimate%20either%20all%0Alocal%20models%20or%20sub-global%20models%20across%20different%20domains%20divided%20based%20on%0Aheterogeneity.%20Experiments%20on%20four%20open%20datasets%20reveal%20that%20the%20proposed%0Amethods%20significantly%20outperform%20five%20existing%20models%2C%20indicating%20they%0Acontribute%20to%20fully%20unleash%20the%20potential%20of%20global%20models%20on%20heterogeneous%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08600v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTwo-stage%2520hybrid%2520models%2520for%2520enhancing%2520forecasting%2520accuracy%2520on%250A%2520%2520heterogeneous%2520time%2520series%26entry.906535625%3DJunru%2520Ren%2520and%2520Shaomin%2520Wu%26entry.1292438233%3D%2520%2520Compared%2520to%2520local%2520models%2520built%2520in%2520a%2520series-by-series%2520manner%252C%2520global%2520models%250Aleverage%2520relevant%2520information%2520across%2520time%2520series%252C%2520resulting%2520in%2520improved%250Aforecasting%2520performance%2520and%2520generalization%2520capacity.%2520Constructing%2520global%2520models%250Aon%2520a%2520set%2520of%2520time%2520series%2520is%2520becoming%2520mainstream%2520in%2520the%2520field%2520of%2520time%2520series%250Aforecasting.%2520However%252C%2520the%2520advantages%2520of%2520global%2520models%2520may%2520not%2520always%2520be%250Arealized%2520when%2520dealing%2520with%2520heterogeneous%2520data.%2520While%2520they%2520can%2520adapt%2520to%250Aheterogeneous%2520datasets%2520by%2520increasing%2520the%2520model%2520complexity%252C%2520the%2520model%2520cannot%2520be%250Ainfinitely%2520complex%2520due%2520to%2520the%2520finite%2520sample%2520size%252C%2520which%2520poses%2520challenges%2520for%250Athe%2520application%2520of%2520global%2520models.%2520Additionally%252C%2520determining%2520whether%2520the%2520time%250Aseries%2520data%2520is%2520homogeneous%2520or%2520heterogeneous%2520can%2520be%2520ambiguous%2520in%2520practice.%2520To%250Aaddress%2520these%2520research%2520gaps%252C%2520this%2520paper%2520argues%2520that%2520the%2520heterogeneity%2520of%2520the%250Adata%2520should%2520be%2520defined%2520by%2520the%2520global%2520model%2520used%252C%2520and%2520for%2520each%2520series%252C%2520the%250Aportion%2520not%2520modelled%2520by%2520the%2520global%2520model%2520represents%2520heterogeneity.%2520It%2520further%250Aproposes%2520two-stage%2520hybrid%2520models%252C%2520which%2520include%2520a%2520second%2520stage%2520to%2520identify%2520and%250Amodel%2520heterogeneous%2520patterns.%2520In%2520this%2520second%2520stage%252C%2520we%2520can%2520estimate%2520either%2520all%250Alocal%2520models%2520or%2520sub-global%2520models%2520across%2520different%2520domains%2520divided%2520based%2520on%250Aheterogeneity.%2520Experiments%2520on%2520four%2520open%2520datasets%2520reveal%2520that%2520the%2520proposed%250Amethods%2520significantly%2520outperform%2520five%2520existing%2520models%252C%2520indicating%2520they%250Acontribute%2520to%2520fully%2520unleash%2520the%2520potential%2520of%2520global%2520models%2520on%2520heterogeneous%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08600v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two-stage%20hybrid%20models%20for%20enhancing%20forecasting%20accuracy%20on%0A%20%20heterogeneous%20time%20series&entry.906535625=Junru%20Ren%20and%20Shaomin%20Wu&entry.1292438233=%20%20Compared%20to%20local%20models%20built%20in%20a%20series-by-series%20manner%2C%20global%20models%0Aleverage%20relevant%20information%20across%20time%20series%2C%20resulting%20in%20improved%0Aforecasting%20performance%20and%20generalization%20capacity.%20Constructing%20global%20models%0Aon%20a%20set%20of%20time%20series%20is%20becoming%20mainstream%20in%20the%20field%20of%20time%20series%0Aforecasting.%20However%2C%20the%20advantages%20of%20global%20models%20may%20not%20always%20be%0Arealized%20when%20dealing%20with%20heterogeneous%20data.%20While%20they%20can%20adapt%20to%0Aheterogeneous%20datasets%20by%20increasing%20the%20model%20complexity%2C%20the%20model%20cannot%20be%0Ainfinitely%20complex%20due%20to%20the%20finite%20sample%20size%2C%20which%20poses%20challenges%20for%0Athe%20application%20of%20global%20models.%20Additionally%2C%20determining%20whether%20the%20time%0Aseries%20data%20is%20homogeneous%20or%20heterogeneous%20can%20be%20ambiguous%20in%20practice.%20To%0Aaddress%20these%20research%20gaps%2C%20this%20paper%20argues%20that%20the%20heterogeneity%20of%20the%0Adata%20should%20be%20defined%20by%20the%20global%20model%20used%2C%20and%20for%20each%20series%2C%20the%0Aportion%20not%20modelled%20by%20the%20global%20model%20represents%20heterogeneity.%20It%20further%0Aproposes%20two-stage%20hybrid%20models%2C%20which%20include%20a%20second%20stage%20to%20identify%20and%0Amodel%20heterogeneous%20patterns.%20In%20this%20second%20stage%2C%20we%20can%20estimate%20either%20all%0Alocal%20models%20or%20sub-global%20models%20across%20different%20domains%20divided%20based%20on%0Aheterogeneity.%20Experiments%20on%20four%20open%20datasets%20reveal%20that%20the%20proposed%0Amethods%20significantly%20outperform%20five%20existing%20models%2C%20indicating%20they%0Acontribute%20to%20fully%20unleash%20the%20potential%20of%20global%20models%20on%20heterogeneous%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08600v1&entry.124074799=Read"},
{"title": "Utility Engineering: Analyzing and Controlling Emergent Value Systems in\n  AIs", "author": "Mantas Mazeika and Xuwang Yin and Rishub Tamirisa and Jaehyuk Lim and Bruce W. Lee and Richard Ren and Long Phan and Norman Mu and Adam Khoja and Oliver Zhang and Dan Hendrycks", "abstract": "  As AIs rapidly advance and become more agentic, the risk they pose is\ngoverned not only by their capabilities but increasingly by their propensities,\nincluding goals and values. Tracking the emergence of goals and values has\nproven a longstanding problem, and despite much interest over the years it\nremains unclear whether current AIs have meaningful values. We propose a\nsolution to this problem, leveraging the framework of utility functions to\nstudy the internal coherence of AI preferences. Surprisingly, we find that\nindependently-sampled preferences in current LLMs exhibit high degrees of\nstructural coherence, and moreover that this emerges with scale. These findings\nsuggest that value systems emerge in LLMs in a meaningful sense, a finding with\nbroad implications. To study these emergent value systems, we propose utility\nengineering as a research agenda, comprising both the analysis and control of\nAI utilities. We uncover problematic and often shocking values in LLM\nassistants despite existing control measures. These include cases where AIs\nvalue themselves over humans and are anti-aligned with specific individuals. To\nconstrain these emergent value systems, we propose methods of utility control.\nAs a case study, we show how aligning utilities with a citizen assembly reduces\npolitical biases and generalizes to new scenarios. Whether we like it or not,\nvalue systems have already emerged in AIs, and much work remains to fully\nunderstand and control these emergent representations.\n", "link": "http://arxiv.org/abs/2502.08640v1", "date": "2025-02-12", "relevancy": 1.3995, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4956}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4735}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Utility%20Engineering%3A%20Analyzing%20and%20Controlling%20Emergent%20Value%20Systems%20in%0A%20%20AIs&body=Title%3A%20Utility%20Engineering%3A%20Analyzing%20and%20Controlling%20Emergent%20Value%20Systems%20in%0A%20%20AIs%0AAuthor%3A%20Mantas%20Mazeika%20and%20Xuwang%20Yin%20and%20Rishub%20Tamirisa%20and%20Jaehyuk%20Lim%20and%20Bruce%20W.%20Lee%20and%20Richard%20Ren%20and%20Long%20Phan%20and%20Norman%20Mu%20and%20Adam%20Khoja%20and%20Oliver%20Zhang%20and%20Dan%20Hendrycks%0AAbstract%3A%20%20%20As%20AIs%20rapidly%20advance%20and%20become%20more%20agentic%2C%20the%20risk%20they%20pose%20is%0Agoverned%20not%20only%20by%20their%20capabilities%20but%20increasingly%20by%20their%20propensities%2C%0Aincluding%20goals%20and%20values.%20Tracking%20the%20emergence%20of%20goals%20and%20values%20has%0Aproven%20a%20longstanding%20problem%2C%20and%20despite%20much%20interest%20over%20the%20years%20it%0Aremains%20unclear%20whether%20current%20AIs%20have%20meaningful%20values.%20We%20propose%20a%0Asolution%20to%20this%20problem%2C%20leveraging%20the%20framework%20of%20utility%20functions%20to%0Astudy%20the%20internal%20coherence%20of%20AI%20preferences.%20Surprisingly%2C%20we%20find%20that%0Aindependently-sampled%20preferences%20in%20current%20LLMs%20exhibit%20high%20degrees%20of%0Astructural%20coherence%2C%20and%20moreover%20that%20this%20emerges%20with%20scale.%20These%20findings%0Asuggest%20that%20value%20systems%20emerge%20in%20LLMs%20in%20a%20meaningful%20sense%2C%20a%20finding%20with%0Abroad%20implications.%20To%20study%20these%20emergent%20value%20systems%2C%20we%20propose%20utility%0Aengineering%20as%20a%20research%20agenda%2C%20comprising%20both%20the%20analysis%20and%20control%20of%0AAI%20utilities.%20We%20uncover%20problematic%20and%20often%20shocking%20values%20in%20LLM%0Aassistants%20despite%20existing%20control%20measures.%20These%20include%20cases%20where%20AIs%0Avalue%20themselves%20over%20humans%20and%20are%20anti-aligned%20with%20specific%20individuals.%20To%0Aconstrain%20these%20emergent%20value%20systems%2C%20we%20propose%20methods%20of%20utility%20control.%0AAs%20a%20case%20study%2C%20we%20show%20how%20aligning%20utilities%20with%20a%20citizen%20assembly%20reduces%0Apolitical%20biases%20and%20generalizes%20to%20new%20scenarios.%20Whether%20we%20like%20it%20or%20not%2C%0Avalue%20systems%20have%20already%20emerged%20in%20AIs%2C%20and%20much%20work%20remains%20to%20fully%0Aunderstand%20and%20control%20these%20emergent%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08640v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUtility%2520Engineering%253A%2520Analyzing%2520and%2520Controlling%2520Emergent%2520Value%2520Systems%2520in%250A%2520%2520AIs%26entry.906535625%3DMantas%2520Mazeika%2520and%2520Xuwang%2520Yin%2520and%2520Rishub%2520Tamirisa%2520and%2520Jaehyuk%2520Lim%2520and%2520Bruce%2520W.%2520Lee%2520and%2520Richard%2520Ren%2520and%2520Long%2520Phan%2520and%2520Norman%2520Mu%2520and%2520Adam%2520Khoja%2520and%2520Oliver%2520Zhang%2520and%2520Dan%2520Hendrycks%26entry.1292438233%3D%2520%2520As%2520AIs%2520rapidly%2520advance%2520and%2520become%2520more%2520agentic%252C%2520the%2520risk%2520they%2520pose%2520is%250Agoverned%2520not%2520only%2520by%2520their%2520capabilities%2520but%2520increasingly%2520by%2520their%2520propensities%252C%250Aincluding%2520goals%2520and%2520values.%2520Tracking%2520the%2520emergence%2520of%2520goals%2520and%2520values%2520has%250Aproven%2520a%2520longstanding%2520problem%252C%2520and%2520despite%2520much%2520interest%2520over%2520the%2520years%2520it%250Aremains%2520unclear%2520whether%2520current%2520AIs%2520have%2520meaningful%2520values.%2520We%2520propose%2520a%250Asolution%2520to%2520this%2520problem%252C%2520leveraging%2520the%2520framework%2520of%2520utility%2520functions%2520to%250Astudy%2520the%2520internal%2520coherence%2520of%2520AI%2520preferences.%2520Surprisingly%252C%2520we%2520find%2520that%250Aindependently-sampled%2520preferences%2520in%2520current%2520LLMs%2520exhibit%2520high%2520degrees%2520of%250Astructural%2520coherence%252C%2520and%2520moreover%2520that%2520this%2520emerges%2520with%2520scale.%2520These%2520findings%250Asuggest%2520that%2520value%2520systems%2520emerge%2520in%2520LLMs%2520in%2520a%2520meaningful%2520sense%252C%2520a%2520finding%2520with%250Abroad%2520implications.%2520To%2520study%2520these%2520emergent%2520value%2520systems%252C%2520we%2520propose%2520utility%250Aengineering%2520as%2520a%2520research%2520agenda%252C%2520comprising%2520both%2520the%2520analysis%2520and%2520control%2520of%250AAI%2520utilities.%2520We%2520uncover%2520problematic%2520and%2520often%2520shocking%2520values%2520in%2520LLM%250Aassistants%2520despite%2520existing%2520control%2520measures.%2520These%2520include%2520cases%2520where%2520AIs%250Avalue%2520themselves%2520over%2520humans%2520and%2520are%2520anti-aligned%2520with%2520specific%2520individuals.%2520To%250Aconstrain%2520these%2520emergent%2520value%2520systems%252C%2520we%2520propose%2520methods%2520of%2520utility%2520control.%250AAs%2520a%2520case%2520study%252C%2520we%2520show%2520how%2520aligning%2520utilities%2520with%2520a%2520citizen%2520assembly%2520reduces%250Apolitical%2520biases%2520and%2520generalizes%2520to%2520new%2520scenarios.%2520Whether%2520we%2520like%2520it%2520or%2520not%252C%250Avalue%2520systems%2520have%2520already%2520emerged%2520in%2520AIs%252C%2520and%2520much%2520work%2520remains%2520to%2520fully%250Aunderstand%2520and%2520control%2520these%2520emergent%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08640v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Utility%20Engineering%3A%20Analyzing%20and%20Controlling%20Emergent%20Value%20Systems%20in%0A%20%20AIs&entry.906535625=Mantas%20Mazeika%20and%20Xuwang%20Yin%20and%20Rishub%20Tamirisa%20and%20Jaehyuk%20Lim%20and%20Bruce%20W.%20Lee%20and%20Richard%20Ren%20and%20Long%20Phan%20and%20Norman%20Mu%20and%20Adam%20Khoja%20and%20Oliver%20Zhang%20and%20Dan%20Hendrycks&entry.1292438233=%20%20As%20AIs%20rapidly%20advance%20and%20become%20more%20agentic%2C%20the%20risk%20they%20pose%20is%0Agoverned%20not%20only%20by%20their%20capabilities%20but%20increasingly%20by%20their%20propensities%2C%0Aincluding%20goals%20and%20values.%20Tracking%20the%20emergence%20of%20goals%20and%20values%20has%0Aproven%20a%20longstanding%20problem%2C%20and%20despite%20much%20interest%20over%20the%20years%20it%0Aremains%20unclear%20whether%20current%20AIs%20have%20meaningful%20values.%20We%20propose%20a%0Asolution%20to%20this%20problem%2C%20leveraging%20the%20framework%20of%20utility%20functions%20to%0Astudy%20the%20internal%20coherence%20of%20AI%20preferences.%20Surprisingly%2C%20we%20find%20that%0Aindependently-sampled%20preferences%20in%20current%20LLMs%20exhibit%20high%20degrees%20of%0Astructural%20coherence%2C%20and%20moreover%20that%20this%20emerges%20with%20scale.%20These%20findings%0Asuggest%20that%20value%20systems%20emerge%20in%20LLMs%20in%20a%20meaningful%20sense%2C%20a%20finding%20with%0Abroad%20implications.%20To%20study%20these%20emergent%20value%20systems%2C%20we%20propose%20utility%0Aengineering%20as%20a%20research%20agenda%2C%20comprising%20both%20the%20analysis%20and%20control%20of%0AAI%20utilities.%20We%20uncover%20problematic%20and%20often%20shocking%20values%20in%20LLM%0Aassistants%20despite%20existing%20control%20measures.%20These%20include%20cases%20where%20AIs%0Avalue%20themselves%20over%20humans%20and%20are%20anti-aligned%20with%20specific%20individuals.%20To%0Aconstrain%20these%20emergent%20value%20systems%2C%20we%20propose%20methods%20of%20utility%20control.%0AAs%20a%20case%20study%2C%20we%20show%20how%20aligning%20utilities%20with%20a%20citizen%20assembly%20reduces%0Apolitical%20biases%20and%20generalizes%20to%20new%20scenarios.%20Whether%20we%20like%20it%20or%20not%2C%0Avalue%20systems%20have%20already%20emerged%20in%20AIs%2C%20and%20much%20work%20remains%20to%20fully%0Aunderstand%20and%20control%20these%20emergent%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08640v1&entry.124074799=Read"},
{"title": "The Paradox of Stochasticity: Limited Creativity and Computational\n  Decoupling in Temperature-Varied LLM Outputs of Structured Fictional Data", "author": "Evgenii Evstafev", "abstract": "  This study examines how temperature settings and model architectures affect\nthe generation of structured fictional data (names, birthdates) across three\nlarge language models (LLMs): llama3.1:8b, deepseek-r1:8b, and mistral:latest.\nBy systematically testing temperature values from 0.0 to 1.0 in increments of\n0.1, we conducted 330 trials yielding 889 structured entities, validated for\nsyntactic consistency. Key findings reveal that model architecture\nsignificantly influences computational efficiency, with mistral:latest and\nllama3.1:8b processing data 8x faster than deepseek-r1:8b. Contrary to\nexpectations, temperature showed no correlation with processing time,\nchallenging assumptions about stochastic sampling costs. Output diversity\nremained limited, as models consistently defaulted to common name archetypes\n(e.g., 'John Doe' and 'Jane Smith') across all temperatures, though rare names\nclustered at intermediate values (0.3-0.7). These results demonstrate that\narchitectural optimizations, rather than temperature adjustments, dominate\nperformance in structured generation tasks. The findings emphasize prioritizing\nmodel selection over hyperparameter tuning for efficiency and suggest explicit\ndiversity constraints are necessary to mitigate default output biases in\nsynthetic data pipelines.\n", "link": "http://arxiv.org/abs/2502.08515v1", "date": "2025-02-12", "relevancy": 1.3828, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4704}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4594}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Paradox%20of%20Stochasticity%3A%20Limited%20Creativity%20and%20Computational%0A%20%20Decoupling%20in%20Temperature-Varied%20LLM%20Outputs%20of%20Structured%20Fictional%20Data&body=Title%3A%20The%20Paradox%20of%20Stochasticity%3A%20Limited%20Creativity%20and%20Computational%0A%20%20Decoupling%20in%20Temperature-Varied%20LLM%20Outputs%20of%20Structured%20Fictional%20Data%0AAuthor%3A%20Evgenii%20Evstafev%0AAbstract%3A%20%20%20This%20study%20examines%20how%20temperature%20settings%20and%20model%20architectures%20affect%0Athe%20generation%20of%20structured%20fictional%20data%20%28names%2C%20birthdates%29%20across%20three%0Alarge%20language%20models%20%28LLMs%29%3A%20llama3.1%3A8b%2C%20deepseek-r1%3A8b%2C%20and%20mistral%3Alatest.%0ABy%20systematically%20testing%20temperature%20values%20from%200.0%20to%201.0%20in%20increments%20of%0A0.1%2C%20we%20conducted%20330%20trials%20yielding%20889%20structured%20entities%2C%20validated%20for%0Asyntactic%20consistency.%20Key%20findings%20reveal%20that%20model%20architecture%0Asignificantly%20influences%20computational%20efficiency%2C%20with%20mistral%3Alatest%20and%0Allama3.1%3A8b%20processing%20data%208x%20faster%20than%20deepseek-r1%3A8b.%20Contrary%20to%0Aexpectations%2C%20temperature%20showed%20no%20correlation%20with%20processing%20time%2C%0Achallenging%20assumptions%20about%20stochastic%20sampling%20costs.%20Output%20diversity%0Aremained%20limited%2C%20as%20models%20consistently%20defaulted%20to%20common%20name%20archetypes%0A%28e.g.%2C%20%27John%20Doe%27%20and%20%27Jane%20Smith%27%29%20across%20all%20temperatures%2C%20though%20rare%20names%0Aclustered%20at%20intermediate%20values%20%280.3-0.7%29.%20These%20results%20demonstrate%20that%0Aarchitectural%20optimizations%2C%20rather%20than%20temperature%20adjustments%2C%20dominate%0Aperformance%20in%20structured%20generation%20tasks.%20The%20findings%20emphasize%20prioritizing%0Amodel%20selection%20over%20hyperparameter%20tuning%20for%20efficiency%20and%20suggest%20explicit%0Adiversity%20constraints%20are%20necessary%20to%20mitigate%20default%20output%20biases%20in%0Asynthetic%20data%20pipelines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08515v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Paradox%2520of%2520Stochasticity%253A%2520Limited%2520Creativity%2520and%2520Computational%250A%2520%2520Decoupling%2520in%2520Temperature-Varied%2520LLM%2520Outputs%2520of%2520Structured%2520Fictional%2520Data%26entry.906535625%3DEvgenii%2520Evstafev%26entry.1292438233%3D%2520%2520This%2520study%2520examines%2520how%2520temperature%2520settings%2520and%2520model%2520architectures%2520affect%250Athe%2520generation%2520of%2520structured%2520fictional%2520data%2520%2528names%252C%2520birthdates%2529%2520across%2520three%250Alarge%2520language%2520models%2520%2528LLMs%2529%253A%2520llama3.1%253A8b%252C%2520deepseek-r1%253A8b%252C%2520and%2520mistral%253Alatest.%250ABy%2520systematically%2520testing%2520temperature%2520values%2520from%25200.0%2520to%25201.0%2520in%2520increments%2520of%250A0.1%252C%2520we%2520conducted%2520330%2520trials%2520yielding%2520889%2520structured%2520entities%252C%2520validated%2520for%250Asyntactic%2520consistency.%2520Key%2520findings%2520reveal%2520that%2520model%2520architecture%250Asignificantly%2520influences%2520computational%2520efficiency%252C%2520with%2520mistral%253Alatest%2520and%250Allama3.1%253A8b%2520processing%2520data%25208x%2520faster%2520than%2520deepseek-r1%253A8b.%2520Contrary%2520to%250Aexpectations%252C%2520temperature%2520showed%2520no%2520correlation%2520with%2520processing%2520time%252C%250Achallenging%2520assumptions%2520about%2520stochastic%2520sampling%2520costs.%2520Output%2520diversity%250Aremained%2520limited%252C%2520as%2520models%2520consistently%2520defaulted%2520to%2520common%2520name%2520archetypes%250A%2528e.g.%252C%2520%2527John%2520Doe%2527%2520and%2520%2527Jane%2520Smith%2527%2529%2520across%2520all%2520temperatures%252C%2520though%2520rare%2520names%250Aclustered%2520at%2520intermediate%2520values%2520%25280.3-0.7%2529.%2520These%2520results%2520demonstrate%2520that%250Aarchitectural%2520optimizations%252C%2520rather%2520than%2520temperature%2520adjustments%252C%2520dominate%250Aperformance%2520in%2520structured%2520generation%2520tasks.%2520The%2520findings%2520emphasize%2520prioritizing%250Amodel%2520selection%2520over%2520hyperparameter%2520tuning%2520for%2520efficiency%2520and%2520suggest%2520explicit%250Adiversity%2520constraints%2520are%2520necessary%2520to%2520mitigate%2520default%2520output%2520biases%2520in%250Asynthetic%2520data%2520pipelines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08515v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Paradox%20of%20Stochasticity%3A%20Limited%20Creativity%20and%20Computational%0A%20%20Decoupling%20in%20Temperature-Varied%20LLM%20Outputs%20of%20Structured%20Fictional%20Data&entry.906535625=Evgenii%20Evstafev&entry.1292438233=%20%20This%20study%20examines%20how%20temperature%20settings%20and%20model%20architectures%20affect%0Athe%20generation%20of%20structured%20fictional%20data%20%28names%2C%20birthdates%29%20across%20three%0Alarge%20language%20models%20%28LLMs%29%3A%20llama3.1%3A8b%2C%20deepseek-r1%3A8b%2C%20and%20mistral%3Alatest.%0ABy%20systematically%20testing%20temperature%20values%20from%200.0%20to%201.0%20in%20increments%20of%0A0.1%2C%20we%20conducted%20330%20trials%20yielding%20889%20structured%20entities%2C%20validated%20for%0Asyntactic%20consistency.%20Key%20findings%20reveal%20that%20model%20architecture%0Asignificantly%20influences%20computational%20efficiency%2C%20with%20mistral%3Alatest%20and%0Allama3.1%3A8b%20processing%20data%208x%20faster%20than%20deepseek-r1%3A8b.%20Contrary%20to%0Aexpectations%2C%20temperature%20showed%20no%20correlation%20with%20processing%20time%2C%0Achallenging%20assumptions%20about%20stochastic%20sampling%20costs.%20Output%20diversity%0Aremained%20limited%2C%20as%20models%20consistently%20defaulted%20to%20common%20name%20archetypes%0A%28e.g.%2C%20%27John%20Doe%27%20and%20%27Jane%20Smith%27%29%20across%20all%20temperatures%2C%20though%20rare%20names%0Aclustered%20at%20intermediate%20values%20%280.3-0.7%29.%20These%20results%20demonstrate%20that%0Aarchitectural%20optimizations%2C%20rather%20than%20temperature%20adjustments%2C%20dominate%0Aperformance%20in%20structured%20generation%20tasks.%20The%20findings%20emphasize%20prioritizing%0Amodel%20selection%20over%20hyperparameter%20tuning%20for%20efficiency%20and%20suggest%20explicit%0Adiversity%20constraints%20are%20necessary%20to%20mitigate%20default%20output%20biases%20in%0Asynthetic%20data%20pipelines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08515v1&entry.124074799=Read"},
{"title": "Copula-based mixture model identification for subgroup clustering with\n  imaging applications", "author": "Fei Zheng and Nicolas Duchateau", "abstract": "  Model-based clustering techniques have been widely applied to various\napplication areas, while most studies focus on canonical mixtures with unique\ncomponent distribution form. However, this strict assumption is often hard to\nsatisfy. In this paper, we consider the more flexible Copula-Based Mixture\nModels (CBMMs) for clustering, which allow heterogeneous component\ndistributions composed by flexible choices of marginal and copula forms. More\nspecifically, we propose an adaptation of the Generalized Iterative Conditional\nEstimation (GICE) algorithm to identify the CBMMs in an unsupervised manner,\nwhere the marginal and copula forms and their parameters are estimated\niteratively. GICE is adapted from its original version developed for switching\nMarkov model identification with the choice of realization time. Our CBMM-GICE\nclustering method is then tested on synthetic two-cluster data (N=2000 samples)\nwith discussion of the factors impacting its convergence. Finally, it is\ncompared to the Expectation Maximization identified mixture models with unique\ncomponent form on the entire MNIST database (N=70000), and on real cardiac\nmagnetic resonance data (N=276) to illustrate its value for imaging\napplications.\n", "link": "http://arxiv.org/abs/2502.08549v1", "date": "2025-02-12", "relevancy": 1.3812, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4719}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.469}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Copula-based%20mixture%20model%20identification%20for%20subgroup%20clustering%20with%0A%20%20imaging%20applications&body=Title%3A%20Copula-based%20mixture%20model%20identification%20for%20subgroup%20clustering%20with%0A%20%20imaging%20applications%0AAuthor%3A%20Fei%20Zheng%20and%20Nicolas%20Duchateau%0AAbstract%3A%20%20%20Model-based%20clustering%20techniques%20have%20been%20widely%20applied%20to%20various%0Aapplication%20areas%2C%20while%20most%20studies%20focus%20on%20canonical%20mixtures%20with%20unique%0Acomponent%20distribution%20form.%20However%2C%20this%20strict%20assumption%20is%20often%20hard%20to%0Asatisfy.%20In%20this%20paper%2C%20we%20consider%20the%20more%20flexible%20Copula-Based%20Mixture%0AModels%20%28CBMMs%29%20for%20clustering%2C%20which%20allow%20heterogeneous%20component%0Adistributions%20composed%20by%20flexible%20choices%20of%20marginal%20and%20copula%20forms.%20More%0Aspecifically%2C%20we%20propose%20an%20adaptation%20of%20the%20Generalized%20Iterative%20Conditional%0AEstimation%20%28GICE%29%20algorithm%20to%20identify%20the%20CBMMs%20in%20an%20unsupervised%20manner%2C%0Awhere%20the%20marginal%20and%20copula%20forms%20and%20their%20parameters%20are%20estimated%0Aiteratively.%20GICE%20is%20adapted%20from%20its%20original%20version%20developed%20for%20switching%0AMarkov%20model%20identification%20with%20the%20choice%20of%20realization%20time.%20Our%20CBMM-GICE%0Aclustering%20method%20is%20then%20tested%20on%20synthetic%20two-cluster%20data%20%28N%3D2000%20samples%29%0Awith%20discussion%20of%20the%20factors%20impacting%20its%20convergence.%20Finally%2C%20it%20is%0Acompared%20to%20the%20Expectation%20Maximization%20identified%20mixture%20models%20with%20unique%0Acomponent%20form%20on%20the%20entire%20MNIST%20database%20%28N%3D70000%29%2C%20and%20on%20real%20cardiac%0Amagnetic%20resonance%20data%20%28N%3D276%29%20to%20illustrate%20its%20value%20for%20imaging%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08549v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCopula-based%2520mixture%2520model%2520identification%2520for%2520subgroup%2520clustering%2520with%250A%2520%2520imaging%2520applications%26entry.906535625%3DFei%2520Zheng%2520and%2520Nicolas%2520Duchateau%26entry.1292438233%3D%2520%2520Model-based%2520clustering%2520techniques%2520have%2520been%2520widely%2520applied%2520to%2520various%250Aapplication%2520areas%252C%2520while%2520most%2520studies%2520focus%2520on%2520canonical%2520mixtures%2520with%2520unique%250Acomponent%2520distribution%2520form.%2520However%252C%2520this%2520strict%2520assumption%2520is%2520often%2520hard%2520to%250Asatisfy.%2520In%2520this%2520paper%252C%2520we%2520consider%2520the%2520more%2520flexible%2520Copula-Based%2520Mixture%250AModels%2520%2528CBMMs%2529%2520for%2520clustering%252C%2520which%2520allow%2520heterogeneous%2520component%250Adistributions%2520composed%2520by%2520flexible%2520choices%2520of%2520marginal%2520and%2520copula%2520forms.%2520More%250Aspecifically%252C%2520we%2520propose%2520an%2520adaptation%2520of%2520the%2520Generalized%2520Iterative%2520Conditional%250AEstimation%2520%2528GICE%2529%2520algorithm%2520to%2520identify%2520the%2520CBMMs%2520in%2520an%2520unsupervised%2520manner%252C%250Awhere%2520the%2520marginal%2520and%2520copula%2520forms%2520and%2520their%2520parameters%2520are%2520estimated%250Aiteratively.%2520GICE%2520is%2520adapted%2520from%2520its%2520original%2520version%2520developed%2520for%2520switching%250AMarkov%2520model%2520identification%2520with%2520the%2520choice%2520of%2520realization%2520time.%2520Our%2520CBMM-GICE%250Aclustering%2520method%2520is%2520then%2520tested%2520on%2520synthetic%2520two-cluster%2520data%2520%2528N%253D2000%2520samples%2529%250Awith%2520discussion%2520of%2520the%2520factors%2520impacting%2520its%2520convergence.%2520Finally%252C%2520it%2520is%250Acompared%2520to%2520the%2520Expectation%2520Maximization%2520identified%2520mixture%2520models%2520with%2520unique%250Acomponent%2520form%2520on%2520the%2520entire%2520MNIST%2520database%2520%2528N%253D70000%2529%252C%2520and%2520on%2520real%2520cardiac%250Amagnetic%2520resonance%2520data%2520%2528N%253D276%2529%2520to%2520illustrate%2520its%2520value%2520for%2520imaging%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08549v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Copula-based%20mixture%20model%20identification%20for%20subgroup%20clustering%20with%0A%20%20imaging%20applications&entry.906535625=Fei%20Zheng%20and%20Nicolas%20Duchateau&entry.1292438233=%20%20Model-based%20clustering%20techniques%20have%20been%20widely%20applied%20to%20various%0Aapplication%20areas%2C%20while%20most%20studies%20focus%20on%20canonical%20mixtures%20with%20unique%0Acomponent%20distribution%20form.%20However%2C%20this%20strict%20assumption%20is%20often%20hard%20to%0Asatisfy.%20In%20this%20paper%2C%20we%20consider%20the%20more%20flexible%20Copula-Based%20Mixture%0AModels%20%28CBMMs%29%20for%20clustering%2C%20which%20allow%20heterogeneous%20component%0Adistributions%20composed%20by%20flexible%20choices%20of%20marginal%20and%20copula%20forms.%20More%0Aspecifically%2C%20we%20propose%20an%20adaptation%20of%20the%20Generalized%20Iterative%20Conditional%0AEstimation%20%28GICE%29%20algorithm%20to%20identify%20the%20CBMMs%20in%20an%20unsupervised%20manner%2C%0Awhere%20the%20marginal%20and%20copula%20forms%20and%20their%20parameters%20are%20estimated%0Aiteratively.%20GICE%20is%20adapted%20from%20its%20original%20version%20developed%20for%20switching%0AMarkov%20model%20identification%20with%20the%20choice%20of%20realization%20time.%20Our%20CBMM-GICE%0Aclustering%20method%20is%20then%20tested%20on%20synthetic%20two-cluster%20data%20%28N%3D2000%20samples%29%0Awith%20discussion%20of%20the%20factors%20impacting%20its%20convergence.%20Finally%2C%20it%20is%0Acompared%20to%20the%20Expectation%20Maximization%20identified%20mixture%20models%20with%20unique%0Acomponent%20form%20on%20the%20entire%20MNIST%20database%20%28N%3D70000%29%2C%20and%20on%20real%20cardiac%0Amagnetic%20resonance%20data%20%28N%3D276%29%20to%20illustrate%20its%20value%20for%20imaging%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08549v1&entry.124074799=Read"},
{"title": "Oscillatory State-Space Models", "author": "T. Konstantin Rusch and Daniela Rus", "abstract": "  We propose Linear Oscillatory State-Space models (LinOSS) for efficiently\nlearning on long sequences. Inspired by cortical dynamics of biological neural\nnetworks, we base our proposed LinOSS model on a system of forced harmonic\noscillators. A stable discretization, integrated over time using fast\nassociative parallel scans, yields the proposed state-space model. We prove\nthat LinOSS produces stable dynamics only requiring nonnegative diagonal state\nmatrix. This is in stark contrast to many previous state-space models relying\nheavily on restrictive parameterizations. Moreover, we rigorously show that\nLinOSS is universal, i.e., it can approximate any continuous and causal\noperator mapping between time-varying functions, to desired accuracy. In\naddition, we show that an implicit-explicit discretization of LinOSS perfectly\nconserves the symmetry of time reversibility of the underlying dynamics.\nTogether, these properties enable efficient modeling of long-range\ninteractions, while ensuring stable and accurate long-horizon forecasting.\nFinally, our empirical results, spanning a wide range of time-series tasks from\nmid-range to very long-range classification and regression, as well as\nlong-horizon forecasting, demonstrate that our proposed LinOSS model\nconsistently outperforms state-of-the-art sequence models. Notably, LinOSS\noutperforms Mamba by nearly 2x and LRU by 2.5x on a sequence modeling task with\nsequences of length 50k.\n", "link": "http://arxiv.org/abs/2410.03943v2", "date": "2025-02-12", "relevancy": 1.3776, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4778}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.454}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Oscillatory%20State-Space%20Models&body=Title%3A%20Oscillatory%20State-Space%20Models%0AAuthor%3A%20T.%20Konstantin%20Rusch%20and%20Daniela%20Rus%0AAbstract%3A%20%20%20We%20propose%20Linear%20Oscillatory%20State-Space%20models%20%28LinOSS%29%20for%20efficiently%0Alearning%20on%20long%20sequences.%20Inspired%20by%20cortical%20dynamics%20of%20biological%20neural%0Anetworks%2C%20we%20base%20our%20proposed%20LinOSS%20model%20on%20a%20system%20of%20forced%20harmonic%0Aoscillators.%20A%20stable%20discretization%2C%20integrated%20over%20time%20using%20fast%0Aassociative%20parallel%20scans%2C%20yields%20the%20proposed%20state-space%20model.%20We%20prove%0Athat%20LinOSS%20produces%20stable%20dynamics%20only%20requiring%20nonnegative%20diagonal%20state%0Amatrix.%20This%20is%20in%20stark%20contrast%20to%20many%20previous%20state-space%20models%20relying%0Aheavily%20on%20restrictive%20parameterizations.%20Moreover%2C%20we%20rigorously%20show%20that%0ALinOSS%20is%20universal%2C%20i.e.%2C%20it%20can%20approximate%20any%20continuous%20and%20causal%0Aoperator%20mapping%20between%20time-varying%20functions%2C%20to%20desired%20accuracy.%20In%0Aaddition%2C%20we%20show%20that%20an%20implicit-explicit%20discretization%20of%20LinOSS%20perfectly%0Aconserves%20the%20symmetry%20of%20time%20reversibility%20of%20the%20underlying%20dynamics.%0ATogether%2C%20these%20properties%20enable%20efficient%20modeling%20of%20long-range%0Ainteractions%2C%20while%20ensuring%20stable%20and%20accurate%20long-horizon%20forecasting.%0AFinally%2C%20our%20empirical%20results%2C%20spanning%20a%20wide%20range%20of%20time-series%20tasks%20from%0Amid-range%20to%20very%20long-range%20classification%20and%20regression%2C%20as%20well%20as%0Along-horizon%20forecasting%2C%20demonstrate%20that%20our%20proposed%20LinOSS%20model%0Aconsistently%20outperforms%20state-of-the-art%20sequence%20models.%20Notably%2C%20LinOSS%0Aoutperforms%20Mamba%20by%20nearly%202x%20and%20LRU%20by%202.5x%20on%20a%20sequence%20modeling%20task%20with%0Asequences%20of%20length%2050k.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03943v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOscillatory%2520State-Space%2520Models%26entry.906535625%3DT.%2520Konstantin%2520Rusch%2520and%2520Daniela%2520Rus%26entry.1292438233%3D%2520%2520We%2520propose%2520Linear%2520Oscillatory%2520State-Space%2520models%2520%2528LinOSS%2529%2520for%2520efficiently%250Alearning%2520on%2520long%2520sequences.%2520Inspired%2520by%2520cortical%2520dynamics%2520of%2520biological%2520neural%250Anetworks%252C%2520we%2520base%2520our%2520proposed%2520LinOSS%2520model%2520on%2520a%2520system%2520of%2520forced%2520harmonic%250Aoscillators.%2520A%2520stable%2520discretization%252C%2520integrated%2520over%2520time%2520using%2520fast%250Aassociative%2520parallel%2520scans%252C%2520yields%2520the%2520proposed%2520state-space%2520model.%2520We%2520prove%250Athat%2520LinOSS%2520produces%2520stable%2520dynamics%2520only%2520requiring%2520nonnegative%2520diagonal%2520state%250Amatrix.%2520This%2520is%2520in%2520stark%2520contrast%2520to%2520many%2520previous%2520state-space%2520models%2520relying%250Aheavily%2520on%2520restrictive%2520parameterizations.%2520Moreover%252C%2520we%2520rigorously%2520show%2520that%250ALinOSS%2520is%2520universal%252C%2520i.e.%252C%2520it%2520can%2520approximate%2520any%2520continuous%2520and%2520causal%250Aoperator%2520mapping%2520between%2520time-varying%2520functions%252C%2520to%2520desired%2520accuracy.%2520In%250Aaddition%252C%2520we%2520show%2520that%2520an%2520implicit-explicit%2520discretization%2520of%2520LinOSS%2520perfectly%250Aconserves%2520the%2520symmetry%2520of%2520time%2520reversibility%2520of%2520the%2520underlying%2520dynamics.%250ATogether%252C%2520these%2520properties%2520enable%2520efficient%2520modeling%2520of%2520long-range%250Ainteractions%252C%2520while%2520ensuring%2520stable%2520and%2520accurate%2520long-horizon%2520forecasting.%250AFinally%252C%2520our%2520empirical%2520results%252C%2520spanning%2520a%2520wide%2520range%2520of%2520time-series%2520tasks%2520from%250Amid-range%2520to%2520very%2520long-range%2520classification%2520and%2520regression%252C%2520as%2520well%2520as%250Along-horizon%2520forecasting%252C%2520demonstrate%2520that%2520our%2520proposed%2520LinOSS%2520model%250Aconsistently%2520outperforms%2520state-of-the-art%2520sequence%2520models.%2520Notably%252C%2520LinOSS%250Aoutperforms%2520Mamba%2520by%2520nearly%25202x%2520and%2520LRU%2520by%25202.5x%2520on%2520a%2520sequence%2520modeling%2520task%2520with%250Asequences%2520of%2520length%252050k.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03943v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Oscillatory%20State-Space%20Models&entry.906535625=T.%20Konstantin%20Rusch%20and%20Daniela%20Rus&entry.1292438233=%20%20We%20propose%20Linear%20Oscillatory%20State-Space%20models%20%28LinOSS%29%20for%20efficiently%0Alearning%20on%20long%20sequences.%20Inspired%20by%20cortical%20dynamics%20of%20biological%20neural%0Anetworks%2C%20we%20base%20our%20proposed%20LinOSS%20model%20on%20a%20system%20of%20forced%20harmonic%0Aoscillators.%20A%20stable%20discretization%2C%20integrated%20over%20time%20using%20fast%0Aassociative%20parallel%20scans%2C%20yields%20the%20proposed%20state-space%20model.%20We%20prove%0Athat%20LinOSS%20produces%20stable%20dynamics%20only%20requiring%20nonnegative%20diagonal%20state%0Amatrix.%20This%20is%20in%20stark%20contrast%20to%20many%20previous%20state-space%20models%20relying%0Aheavily%20on%20restrictive%20parameterizations.%20Moreover%2C%20we%20rigorously%20show%20that%0ALinOSS%20is%20universal%2C%20i.e.%2C%20it%20can%20approximate%20any%20continuous%20and%20causal%0Aoperator%20mapping%20between%20time-varying%20functions%2C%20to%20desired%20accuracy.%20In%0Aaddition%2C%20we%20show%20that%20an%20implicit-explicit%20discretization%20of%20LinOSS%20perfectly%0Aconserves%20the%20symmetry%20of%20time%20reversibility%20of%20the%20underlying%20dynamics.%0ATogether%2C%20these%20properties%20enable%20efficient%20modeling%20of%20long-range%0Ainteractions%2C%20while%20ensuring%20stable%20and%20accurate%20long-horizon%20forecasting.%0AFinally%2C%20our%20empirical%20results%2C%20spanning%20a%20wide%20range%20of%20time-series%20tasks%20from%0Amid-range%20to%20very%20long-range%20classification%20and%20regression%2C%20as%20well%20as%0Along-horizon%20forecasting%2C%20demonstrate%20that%20our%20proposed%20LinOSS%20model%0Aconsistently%20outperforms%20state-of-the-art%20sequence%20models.%20Notably%2C%20LinOSS%0Aoutperforms%20Mamba%20by%20nearly%202x%20and%20LRU%20by%202.5x%20on%20a%20sequence%20modeling%20task%20with%0Asequences%20of%20length%2050k.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03943v2&entry.124074799=Read"},
{"title": "On Different Notions of Redundancy in Conditional-Independence-Based\n  Discovery of Graphical Models", "author": "Philipp M. Faller and Dominik Janzing", "abstract": "  The goal of conditional-independence-based discovery of graphical models is\nto find a graph that represents the independence structure of variables in a\ngiven dataset. To learn such a representation, conditional-independence-based\napproaches conduct a set of statistical tests that suffices to identify the\ngraphical representation under some assumptions on the underlying distribution\nof the data. In this work, we highlight that due to the conciseness of the\ngraphical representation, there are often many tests that are not used in the\nconstruction of the graph. These redundant tests have the potential to detect\nor sometimes correct errors in the learned model. We show that not all tests\ncontain this additional information and that such redundant tests have to be\napplied with care. Precisely, we argue that particularly those conditional\n(in)dependence statements are interesting that follow only from graphical\nassumptions but do not hold for every probability distribution.\n", "link": "http://arxiv.org/abs/2502.08531v1", "date": "2025-02-12", "relevancy": 1.3762, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4655}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4539}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Different%20Notions%20of%20Redundancy%20in%20Conditional-Independence-Based%0A%20%20Discovery%20of%20Graphical%20Models&body=Title%3A%20On%20Different%20Notions%20of%20Redundancy%20in%20Conditional-Independence-Based%0A%20%20Discovery%20of%20Graphical%20Models%0AAuthor%3A%20Philipp%20M.%20Faller%20and%20Dominik%20Janzing%0AAbstract%3A%20%20%20The%20goal%20of%20conditional-independence-based%20discovery%20of%20graphical%20models%20is%0Ato%20find%20a%20graph%20that%20represents%20the%20independence%20structure%20of%20variables%20in%20a%0Agiven%20dataset.%20To%20learn%20such%20a%20representation%2C%20conditional-independence-based%0Aapproaches%20conduct%20a%20set%20of%20statistical%20tests%20that%20suffices%20to%20identify%20the%0Agraphical%20representation%20under%20some%20assumptions%20on%20the%20underlying%20distribution%0Aof%20the%20data.%20In%20this%20work%2C%20we%20highlight%20that%20due%20to%20the%20conciseness%20of%20the%0Agraphical%20representation%2C%20there%20are%20often%20many%20tests%20that%20are%20not%20used%20in%20the%0Aconstruction%20of%20the%20graph.%20These%20redundant%20tests%20have%20the%20potential%20to%20detect%0Aor%20sometimes%20correct%20errors%20in%20the%20learned%20model.%20We%20show%20that%20not%20all%20tests%0Acontain%20this%20additional%20information%20and%20that%20such%20redundant%20tests%20have%20to%20be%0Aapplied%20with%20care.%20Precisely%2C%20we%20argue%20that%20particularly%20those%20conditional%0A%28in%29dependence%20statements%20are%20interesting%20that%20follow%20only%20from%20graphical%0Aassumptions%20but%20do%20not%20hold%20for%20every%20probability%20distribution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Different%2520Notions%2520of%2520Redundancy%2520in%2520Conditional-Independence-Based%250A%2520%2520Discovery%2520of%2520Graphical%2520Models%26entry.906535625%3DPhilipp%2520M.%2520Faller%2520and%2520Dominik%2520Janzing%26entry.1292438233%3D%2520%2520The%2520goal%2520of%2520conditional-independence-based%2520discovery%2520of%2520graphical%2520models%2520is%250Ato%2520find%2520a%2520graph%2520that%2520represents%2520the%2520independence%2520structure%2520of%2520variables%2520in%2520a%250Agiven%2520dataset.%2520To%2520learn%2520such%2520a%2520representation%252C%2520conditional-independence-based%250Aapproaches%2520conduct%2520a%2520set%2520of%2520statistical%2520tests%2520that%2520suffices%2520to%2520identify%2520the%250Agraphical%2520representation%2520under%2520some%2520assumptions%2520on%2520the%2520underlying%2520distribution%250Aof%2520the%2520data.%2520In%2520this%2520work%252C%2520we%2520highlight%2520that%2520due%2520to%2520the%2520conciseness%2520of%2520the%250Agraphical%2520representation%252C%2520there%2520are%2520often%2520many%2520tests%2520that%2520are%2520not%2520used%2520in%2520the%250Aconstruction%2520of%2520the%2520graph.%2520These%2520redundant%2520tests%2520have%2520the%2520potential%2520to%2520detect%250Aor%2520sometimes%2520correct%2520errors%2520in%2520the%2520learned%2520model.%2520We%2520show%2520that%2520not%2520all%2520tests%250Acontain%2520this%2520additional%2520information%2520and%2520that%2520such%2520redundant%2520tests%2520have%2520to%2520be%250Aapplied%2520with%2520care.%2520Precisely%252C%2520we%2520argue%2520that%2520particularly%2520those%2520conditional%250A%2528in%2529dependence%2520statements%2520are%2520interesting%2520that%2520follow%2520only%2520from%2520graphical%250Aassumptions%2520but%2520do%2520not%2520hold%2520for%2520every%2520probability%2520distribution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Different%20Notions%20of%20Redundancy%20in%20Conditional-Independence-Based%0A%20%20Discovery%20of%20Graphical%20Models&entry.906535625=Philipp%20M.%20Faller%20and%20Dominik%20Janzing&entry.1292438233=%20%20The%20goal%20of%20conditional-independence-based%20discovery%20of%20graphical%20models%20is%0Ato%20find%20a%20graph%20that%20represents%20the%20independence%20structure%20of%20variables%20in%20a%0Agiven%20dataset.%20To%20learn%20such%20a%20representation%2C%20conditional-independence-based%0Aapproaches%20conduct%20a%20set%20of%20statistical%20tests%20that%20suffices%20to%20identify%20the%0Agraphical%20representation%20under%20some%20assumptions%20on%20the%20underlying%20distribution%0Aof%20the%20data.%20In%20this%20work%2C%20we%20highlight%20that%20due%20to%20the%20conciseness%20of%20the%0Agraphical%20representation%2C%20there%20are%20often%20many%20tests%20that%20are%20not%20used%20in%20the%0Aconstruction%20of%20the%20graph.%20These%20redundant%20tests%20have%20the%20potential%20to%20detect%0Aor%20sometimes%20correct%20errors%20in%20the%20learned%20model.%20We%20show%20that%20not%20all%20tests%0Acontain%20this%20additional%20information%20and%20that%20such%20redundant%20tests%20have%20to%20be%0Aapplied%20with%20care.%20Precisely%2C%20we%20argue%20that%20particularly%20those%20conditional%0A%28in%29dependence%20statements%20are%20interesting%20that%20follow%20only%20from%20graphical%0Aassumptions%20but%20do%20not%20hold%20for%20every%20probability%20distribution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08531v1&entry.124074799=Read"},
{"title": "Scalable Thermodynamic Second-order Optimization", "author": "Kaelan Donatella and Samuel Duffield and Denis Melanson and Maxwell Aifer and Phoebe Klett and Rajath Salegame and Zach Belateche and Gavin Crooks and Antonio J. Martinez and Patrick J. Coles", "abstract": "  Many hardware proposals have aimed to accelerate inference in AI workloads.\nLess attention has been paid to hardware acceleration of training, despite the\nenormous societal impact of rapid training of AI models. Physics-based\ncomputers, such as thermodynamic computers, offer an efficient means to solve\nkey primitives in AI training algorithms. Optimizers that normally would be\ncomputationally out-of-reach (e.g., due to expensive matrix inversions) on\ndigital hardware could be unlocked with physics-based hardware. In this work,\nwe propose a scalable algorithm for employing thermodynamic computers to\naccelerate a popular second-order optimizer called Kronecker-factored\napproximate curvature (K-FAC). Our asymptotic complexity analysis predicts\nincreasing advantage with our algorithm as $n$, the number of neurons per\nlayer, increases. Numerical experiments show that even under significant\nquantization noise, the benefits of second-order optimization can be preserved.\nFinally, we predict substantial speedups for large-scale vision and graph\nproblems based on realistic hardware characteristics.\n", "link": "http://arxiv.org/abs/2502.08603v1", "date": "2025-02-12", "relevancy": 1.0105, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5343}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4959}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Thermodynamic%20Second-order%20Optimization&body=Title%3A%20Scalable%20Thermodynamic%20Second-order%20Optimization%0AAuthor%3A%20Kaelan%20Donatella%20and%20Samuel%20Duffield%20and%20Denis%20Melanson%20and%20Maxwell%20Aifer%20and%20Phoebe%20Klett%20and%20Rajath%20Salegame%20and%20Zach%20Belateche%20and%20Gavin%20Crooks%20and%20Antonio%20J.%20Martinez%20and%20Patrick%20J.%20Coles%0AAbstract%3A%20%20%20Many%20hardware%20proposals%20have%20aimed%20to%20accelerate%20inference%20in%20AI%20workloads.%0ALess%20attention%20has%20been%20paid%20to%20hardware%20acceleration%20of%20training%2C%20despite%20the%0Aenormous%20societal%20impact%20of%20rapid%20training%20of%20AI%20models.%20Physics-based%0Acomputers%2C%20such%20as%20thermodynamic%20computers%2C%20offer%20an%20efficient%20means%20to%20solve%0Akey%20primitives%20in%20AI%20training%20algorithms.%20Optimizers%20that%20normally%20would%20be%0Acomputationally%20out-of-reach%20%28e.g.%2C%20due%20to%20expensive%20matrix%20inversions%29%20on%0Adigital%20hardware%20could%20be%20unlocked%20with%20physics-based%20hardware.%20In%20this%20work%2C%0Awe%20propose%20a%20scalable%20algorithm%20for%20employing%20thermodynamic%20computers%20to%0Aaccelerate%20a%20popular%20second-order%20optimizer%20called%20Kronecker-factored%0Aapproximate%20curvature%20%28K-FAC%29.%20Our%20asymptotic%20complexity%20analysis%20predicts%0Aincreasing%20advantage%20with%20our%20algorithm%20as%20%24n%24%2C%20the%20number%20of%20neurons%20per%0Alayer%2C%20increases.%20Numerical%20experiments%20show%20that%20even%20under%20significant%0Aquantization%20noise%2C%20the%20benefits%20of%20second-order%20optimization%20can%20be%20preserved.%0AFinally%2C%20we%20predict%20substantial%20speedups%20for%20large-scale%20vision%20and%20graph%0Aproblems%20based%20on%20realistic%20hardware%20characteristics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08603v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Thermodynamic%2520Second-order%2520Optimization%26entry.906535625%3DKaelan%2520Donatella%2520and%2520Samuel%2520Duffield%2520and%2520Denis%2520Melanson%2520and%2520Maxwell%2520Aifer%2520and%2520Phoebe%2520Klett%2520and%2520Rajath%2520Salegame%2520and%2520Zach%2520Belateche%2520and%2520Gavin%2520Crooks%2520and%2520Antonio%2520J.%2520Martinez%2520and%2520Patrick%2520J.%2520Coles%26entry.1292438233%3D%2520%2520Many%2520hardware%2520proposals%2520have%2520aimed%2520to%2520accelerate%2520inference%2520in%2520AI%2520workloads.%250ALess%2520attention%2520has%2520been%2520paid%2520to%2520hardware%2520acceleration%2520of%2520training%252C%2520despite%2520the%250Aenormous%2520societal%2520impact%2520of%2520rapid%2520training%2520of%2520AI%2520models.%2520Physics-based%250Acomputers%252C%2520such%2520as%2520thermodynamic%2520computers%252C%2520offer%2520an%2520efficient%2520means%2520to%2520solve%250Akey%2520primitives%2520in%2520AI%2520training%2520algorithms.%2520Optimizers%2520that%2520normally%2520would%2520be%250Acomputationally%2520out-of-reach%2520%2528e.g.%252C%2520due%2520to%2520expensive%2520matrix%2520inversions%2529%2520on%250Adigital%2520hardware%2520could%2520be%2520unlocked%2520with%2520physics-based%2520hardware.%2520In%2520this%2520work%252C%250Awe%2520propose%2520a%2520scalable%2520algorithm%2520for%2520employing%2520thermodynamic%2520computers%2520to%250Aaccelerate%2520a%2520popular%2520second-order%2520optimizer%2520called%2520Kronecker-factored%250Aapproximate%2520curvature%2520%2528K-FAC%2529.%2520Our%2520asymptotic%2520complexity%2520analysis%2520predicts%250Aincreasing%2520advantage%2520with%2520our%2520algorithm%2520as%2520%2524n%2524%252C%2520the%2520number%2520of%2520neurons%2520per%250Alayer%252C%2520increases.%2520Numerical%2520experiments%2520show%2520that%2520even%2520under%2520significant%250Aquantization%2520noise%252C%2520the%2520benefits%2520of%2520second-order%2520optimization%2520can%2520be%2520preserved.%250AFinally%252C%2520we%2520predict%2520substantial%2520speedups%2520for%2520large-scale%2520vision%2520and%2520graph%250Aproblems%2520based%2520on%2520realistic%2520hardware%2520characteristics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08603v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Thermodynamic%20Second-order%20Optimization&entry.906535625=Kaelan%20Donatella%20and%20Samuel%20Duffield%20and%20Denis%20Melanson%20and%20Maxwell%20Aifer%20and%20Phoebe%20Klett%20and%20Rajath%20Salegame%20and%20Zach%20Belateche%20and%20Gavin%20Crooks%20and%20Antonio%20J.%20Martinez%20and%20Patrick%20J.%20Coles&entry.1292438233=%20%20Many%20hardware%20proposals%20have%20aimed%20to%20accelerate%20inference%20in%20AI%20workloads.%0ALess%20attention%20has%20been%20paid%20to%20hardware%20acceleration%20of%20training%2C%20despite%20the%0Aenormous%20societal%20impact%20of%20rapid%20training%20of%20AI%20models.%20Physics-based%0Acomputers%2C%20such%20as%20thermodynamic%20computers%2C%20offer%20an%20efficient%20means%20to%20solve%0Akey%20primitives%20in%20AI%20training%20algorithms.%20Optimizers%20that%20normally%20would%20be%0Acomputationally%20out-of-reach%20%28e.g.%2C%20due%20to%20expensive%20matrix%20inversions%29%20on%0Adigital%20hardware%20could%20be%20unlocked%20with%20physics-based%20hardware.%20In%20this%20work%2C%0Awe%20propose%20a%20scalable%20algorithm%20for%20employing%20thermodynamic%20computers%20to%0Aaccelerate%20a%20popular%20second-order%20optimizer%20called%20Kronecker-factored%0Aapproximate%20curvature%20%28K-FAC%29.%20Our%20asymptotic%20complexity%20analysis%20predicts%0Aincreasing%20advantage%20with%20our%20algorithm%20as%20%24n%24%2C%20the%20number%20of%20neurons%20per%0Alayer%2C%20increases.%20Numerical%20experiments%20show%20that%20even%20under%20significant%0Aquantization%20noise%2C%20the%20benefits%20of%20second-order%20optimization%20can%20be%20preserved.%0AFinally%2C%20we%20predict%20substantial%20speedups%20for%20large-scale%20vision%20and%20graph%0Aproblems%20based%20on%20realistic%20hardware%20characteristics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08603v1&entry.124074799=Read"},
{"title": "Tensor-Var: Variational Data Assimilation in Tensor Product Feature\n  Space", "author": "Yiming Yang and Xiaoyuan Cheng and Daniel Giles and Sibo Cheng and Yi He and Xiao Xue and Boli Chen and Yukun Hu", "abstract": "  Variational data assimilation estimates the dynamical system states by\nminimizing a cost function that fits the numerical models with observational\ndata. The widely used method, four-dimensional variational assimilation\n(4D-Var), has two primary challenges: (1) computationally demanding for complex\nnonlinear systems and (2) relying on state-observation mappings, which are\noften not perfectly known. Deep learning (DL) has been used as a more\nexpressive class of efficient model approximators to address these challenges.\nHowever, integrating such models into 4D-Var remains challenging due to their\ninherent nonlinearities and the lack of theoretical guarantees for consistency\nin assimilation results. In this paper, we propose Tensor-Var to address these\nchallenges using kernel Conditional Mean Embedding (CME). Tensor-Var improves\noptimization efficiency by characterizing system dynamics and state-observation\nmappings as linear operators, leading to a convex cost function in the feature\nspace. Furthermore, our method provides a new perspective to incorporate CME\ninto 4D-Var, offering theoretical guarantees of consistent assimilation results\nbetween the original and feature spaces. To improve scalability, we propose a\nmethod to learn deep features (DFs) using neural networks within the Tensor-Var\nframework. Experiments on chaotic systems and global weather prediction with\nreal-time observations show that Tensor-Var outperforms conventional and DL\nhybrid 4D-Var baselines in accuracy while achieving efficiency comparable to\nthe static 3D-Var method.\n", "link": "http://arxiv.org/abs/2501.13312v2", "date": "2025-02-12", "relevancy": 0.9964, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5027}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.502}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.49}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tensor-Var%3A%20Variational%20Data%20Assimilation%20in%20Tensor%20Product%20Feature%0A%20%20Space&body=Title%3A%20Tensor-Var%3A%20Variational%20Data%20Assimilation%20in%20Tensor%20Product%20Feature%0A%20%20Space%0AAuthor%3A%20Yiming%20Yang%20and%20Xiaoyuan%20Cheng%20and%20Daniel%20Giles%20and%20Sibo%20Cheng%20and%20Yi%20He%20and%20Xiao%20Xue%20and%20Boli%20Chen%20and%20Yukun%20Hu%0AAbstract%3A%20%20%20Variational%20data%20assimilation%20estimates%20the%20dynamical%20system%20states%20by%0Aminimizing%20a%20cost%20function%20that%20fits%20the%20numerical%20models%20with%20observational%0Adata.%20The%20widely%20used%20method%2C%20four-dimensional%20variational%20assimilation%0A%284D-Var%29%2C%20has%20two%20primary%20challenges%3A%20%281%29%20computationally%20demanding%20for%20complex%0Anonlinear%20systems%20and%20%282%29%20relying%20on%20state-observation%20mappings%2C%20which%20are%0Aoften%20not%20perfectly%20known.%20Deep%20learning%20%28DL%29%20has%20been%20used%20as%20a%20more%0Aexpressive%20class%20of%20efficient%20model%20approximators%20to%20address%20these%20challenges.%0AHowever%2C%20integrating%20such%20models%20into%204D-Var%20remains%20challenging%20due%20to%20their%0Ainherent%20nonlinearities%20and%20the%20lack%20of%20theoretical%20guarantees%20for%20consistency%0Ain%20assimilation%20results.%20In%20this%20paper%2C%20we%20propose%20Tensor-Var%20to%20address%20these%0Achallenges%20using%20kernel%20Conditional%20Mean%20Embedding%20%28CME%29.%20Tensor-Var%20improves%0Aoptimization%20efficiency%20by%20characterizing%20system%20dynamics%20and%20state-observation%0Amappings%20as%20linear%20operators%2C%20leading%20to%20a%20convex%20cost%20function%20in%20the%20feature%0Aspace.%20Furthermore%2C%20our%20method%20provides%20a%20new%20perspective%20to%20incorporate%20CME%0Ainto%204D-Var%2C%20offering%20theoretical%20guarantees%20of%20consistent%20assimilation%20results%0Abetween%20the%20original%20and%20feature%20spaces.%20To%20improve%20scalability%2C%20we%20propose%20a%0Amethod%20to%20learn%20deep%20features%20%28DFs%29%20using%20neural%20networks%20within%20the%20Tensor-Var%0Aframework.%20Experiments%20on%20chaotic%20systems%20and%20global%20weather%20prediction%20with%0Areal-time%20observations%20show%20that%20Tensor-Var%20outperforms%20conventional%20and%20DL%0Ahybrid%204D-Var%20baselines%20in%20accuracy%20while%20achieving%20efficiency%20comparable%20to%0Athe%20static%203D-Var%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13312v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTensor-Var%253A%2520Variational%2520Data%2520Assimilation%2520in%2520Tensor%2520Product%2520Feature%250A%2520%2520Space%26entry.906535625%3DYiming%2520Yang%2520and%2520Xiaoyuan%2520Cheng%2520and%2520Daniel%2520Giles%2520and%2520Sibo%2520Cheng%2520and%2520Yi%2520He%2520and%2520Xiao%2520Xue%2520and%2520Boli%2520Chen%2520and%2520Yukun%2520Hu%26entry.1292438233%3D%2520%2520Variational%2520data%2520assimilation%2520estimates%2520the%2520dynamical%2520system%2520states%2520by%250Aminimizing%2520a%2520cost%2520function%2520that%2520fits%2520the%2520numerical%2520models%2520with%2520observational%250Adata.%2520The%2520widely%2520used%2520method%252C%2520four-dimensional%2520variational%2520assimilation%250A%25284D-Var%2529%252C%2520has%2520two%2520primary%2520challenges%253A%2520%25281%2529%2520computationally%2520demanding%2520for%2520complex%250Anonlinear%2520systems%2520and%2520%25282%2529%2520relying%2520on%2520state-observation%2520mappings%252C%2520which%2520are%250Aoften%2520not%2520perfectly%2520known.%2520Deep%2520learning%2520%2528DL%2529%2520has%2520been%2520used%2520as%2520a%2520more%250Aexpressive%2520class%2520of%2520efficient%2520model%2520approximators%2520to%2520address%2520these%2520challenges.%250AHowever%252C%2520integrating%2520such%2520models%2520into%25204D-Var%2520remains%2520challenging%2520due%2520to%2520their%250Ainherent%2520nonlinearities%2520and%2520the%2520lack%2520of%2520theoretical%2520guarantees%2520for%2520consistency%250Ain%2520assimilation%2520results.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Tensor-Var%2520to%2520address%2520these%250Achallenges%2520using%2520kernel%2520Conditional%2520Mean%2520Embedding%2520%2528CME%2529.%2520Tensor-Var%2520improves%250Aoptimization%2520efficiency%2520by%2520characterizing%2520system%2520dynamics%2520and%2520state-observation%250Amappings%2520as%2520linear%2520operators%252C%2520leading%2520to%2520a%2520convex%2520cost%2520function%2520in%2520the%2520feature%250Aspace.%2520Furthermore%252C%2520our%2520method%2520provides%2520a%2520new%2520perspective%2520to%2520incorporate%2520CME%250Ainto%25204D-Var%252C%2520offering%2520theoretical%2520guarantees%2520of%2520consistent%2520assimilation%2520results%250Abetween%2520the%2520original%2520and%2520feature%2520spaces.%2520To%2520improve%2520scalability%252C%2520we%2520propose%2520a%250Amethod%2520to%2520learn%2520deep%2520features%2520%2528DFs%2529%2520using%2520neural%2520networks%2520within%2520the%2520Tensor-Var%250Aframework.%2520Experiments%2520on%2520chaotic%2520systems%2520and%2520global%2520weather%2520prediction%2520with%250Areal-time%2520observations%2520show%2520that%2520Tensor-Var%2520outperforms%2520conventional%2520and%2520DL%250Ahybrid%25204D-Var%2520baselines%2520in%2520accuracy%2520while%2520achieving%2520efficiency%2520comparable%2520to%250Athe%2520static%25203D-Var%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13312v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tensor-Var%3A%20Variational%20Data%20Assimilation%20in%20Tensor%20Product%20Feature%0A%20%20Space&entry.906535625=Yiming%20Yang%20and%20Xiaoyuan%20Cheng%20and%20Daniel%20Giles%20and%20Sibo%20Cheng%20and%20Yi%20He%20and%20Xiao%20Xue%20and%20Boli%20Chen%20and%20Yukun%20Hu&entry.1292438233=%20%20Variational%20data%20assimilation%20estimates%20the%20dynamical%20system%20states%20by%0Aminimizing%20a%20cost%20function%20that%20fits%20the%20numerical%20models%20with%20observational%0Adata.%20The%20widely%20used%20method%2C%20four-dimensional%20variational%20assimilation%0A%284D-Var%29%2C%20has%20two%20primary%20challenges%3A%20%281%29%20computationally%20demanding%20for%20complex%0Anonlinear%20systems%20and%20%282%29%20relying%20on%20state-observation%20mappings%2C%20which%20are%0Aoften%20not%20perfectly%20known.%20Deep%20learning%20%28DL%29%20has%20been%20used%20as%20a%20more%0Aexpressive%20class%20of%20efficient%20model%20approximators%20to%20address%20these%20challenges.%0AHowever%2C%20integrating%20such%20models%20into%204D-Var%20remains%20challenging%20due%20to%20their%0Ainherent%20nonlinearities%20and%20the%20lack%20of%20theoretical%20guarantees%20for%20consistency%0Ain%20assimilation%20results.%20In%20this%20paper%2C%20we%20propose%20Tensor-Var%20to%20address%20these%0Achallenges%20using%20kernel%20Conditional%20Mean%20Embedding%20%28CME%29.%20Tensor-Var%20improves%0Aoptimization%20efficiency%20by%20characterizing%20system%20dynamics%20and%20state-observation%0Amappings%20as%20linear%20operators%2C%20leading%20to%20a%20convex%20cost%20function%20in%20the%20feature%0Aspace.%20Furthermore%2C%20our%20method%20provides%20a%20new%20perspective%20to%20incorporate%20CME%0Ainto%204D-Var%2C%20offering%20theoretical%20guarantees%20of%20consistent%20assimilation%20results%0Abetween%20the%20original%20and%20feature%20spaces.%20To%20improve%20scalability%2C%20we%20propose%20a%0Amethod%20to%20learn%20deep%20features%20%28DFs%29%20using%20neural%20networks%20within%20the%20Tensor-Var%0Aframework.%20Experiments%20on%20chaotic%20systems%20and%20global%20weather%20prediction%20with%0Areal-time%20observations%20show%20that%20Tensor-Var%20outperforms%20conventional%20and%20DL%0Ahybrid%204D-Var%20baselines%20in%20accuracy%20while%20achieving%20efficiency%20comparable%20to%0Athe%20static%203D-Var%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13312v2&entry.124074799=Read"},
{"title": "Joint Transmit and Pinching Beamforming for PASS: Optimization-Based or\n  Learning-Based?", "author": "Xiaoxia Xu and Xidong Mu and Yuanwei Liu and Arumugam Nallanathan", "abstract": "  A novel pinching antenna system (PASS)-enabled downlink multi-user\nmultiple-input single-output (MISO) framework is proposed. PASS consists of\nmultiple waveguides spanning over thousands of wavelength, which equip numerous\nlow-cost dielectric particles, named pinching antennas (PAs), to radiate\nsignals into free space. The positions of PAs can be reconfigured to change\nboth the large-scale path losses and phases of signals, thus facilitating the\nnovel pinching beamforming design. A sum rate maximization problem is\nformulated, which jointly optimizes the transmit and pinching beamforming to\nadaptively achieve constructive signal enhancement and destructive interference\nmitigation. To solve this highly coupled and nonconvex problem, both\noptimization-based and learning-based methods are proposed. 1) For the\noptimization-based method, a majorization-minimization and penalty dual\ndecomposition (MM-PDD) algorithm is developed, which handles the nonconvex\ncomplex exponential component using a Lipschitz surrogate function and then\ninvokes PDD for problem decoupling. 2) For the learning-based method, a novel\nKarush-Kuhn-Tucker (KKT)-guided dual learning (KDL) approach is proposed, which\nenables KKT solutions to be reconstructed in a data-driven manner by learning\ndual variables. Following this idea, a KDL-Tranformer algorithm is developed,\nwhich captures both inter-PA/inter-user dependencies and\nchannel-state-information (CSI)-beamforming dependencies by attention\nmechanisms. Simulation results demonstrate that: i) The proposed PASS framework\nsignificantly outperforms conventional massive multiple input multiple output\n(MIMO) system even with a few PAs. ii) The proposed KDL-Transformer can improve\nover 30% system performance than MM-PDD algorithm, while achieving a\nmillisecond-level response on modern GPUs.\n", "link": "http://arxiv.org/abs/2502.08637v1", "date": "2025-02-12", "relevancy": 1.3626, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4668}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4546}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%20Transmit%20and%20Pinching%20Beamforming%20for%20PASS%3A%20Optimization-Based%20or%0A%20%20Learning-Based%3F&body=Title%3A%20Joint%20Transmit%20and%20Pinching%20Beamforming%20for%20PASS%3A%20Optimization-Based%20or%0A%20%20Learning-Based%3F%0AAuthor%3A%20Xiaoxia%20Xu%20and%20Xidong%20Mu%20and%20Yuanwei%20Liu%20and%20Arumugam%20Nallanathan%0AAbstract%3A%20%20%20A%20novel%20pinching%20antenna%20system%20%28PASS%29-enabled%20downlink%20multi-user%0Amultiple-input%20single-output%20%28MISO%29%20framework%20is%20proposed.%20PASS%20consists%20of%0Amultiple%20waveguides%20spanning%20over%20thousands%20of%20wavelength%2C%20which%20equip%20numerous%0Alow-cost%20dielectric%20particles%2C%20named%20pinching%20antennas%20%28PAs%29%2C%20to%20radiate%0Asignals%20into%20free%20space.%20The%20positions%20of%20PAs%20can%20be%20reconfigured%20to%20change%0Aboth%20the%20large-scale%20path%20losses%20and%20phases%20of%20signals%2C%20thus%20facilitating%20the%0Anovel%20pinching%20beamforming%20design.%20A%20sum%20rate%20maximization%20problem%20is%0Aformulated%2C%20which%20jointly%20optimizes%20the%20transmit%20and%20pinching%20beamforming%20to%0Aadaptively%20achieve%20constructive%20signal%20enhancement%20and%20destructive%20interference%0Amitigation.%20To%20solve%20this%20highly%20coupled%20and%20nonconvex%20problem%2C%20both%0Aoptimization-based%20and%20learning-based%20methods%20are%20proposed.%201%29%20For%20the%0Aoptimization-based%20method%2C%20a%20majorization-minimization%20and%20penalty%20dual%0Adecomposition%20%28MM-PDD%29%20algorithm%20is%20developed%2C%20which%20handles%20the%20nonconvex%0Acomplex%20exponential%20component%20using%20a%20Lipschitz%20surrogate%20function%20and%20then%0Ainvokes%20PDD%20for%20problem%20decoupling.%202%29%20For%20the%20learning-based%20method%2C%20a%20novel%0AKarush-Kuhn-Tucker%20%28KKT%29-guided%20dual%20learning%20%28KDL%29%20approach%20is%20proposed%2C%20which%0Aenables%20KKT%20solutions%20to%20be%20reconstructed%20in%20a%20data-driven%20manner%20by%20learning%0Adual%20variables.%20Following%20this%20idea%2C%20a%20KDL-Tranformer%20algorithm%20is%20developed%2C%0Awhich%20captures%20both%20inter-PA/inter-user%20dependencies%20and%0Achannel-state-information%20%28CSI%29-beamforming%20dependencies%20by%20attention%0Amechanisms.%20Simulation%20results%20demonstrate%20that%3A%20i%29%20The%20proposed%20PASS%20framework%0Asignificantly%20outperforms%20conventional%20massive%20multiple%20input%20multiple%20output%0A%28MIMO%29%20system%20even%20with%20a%20few%20PAs.%20ii%29%20The%20proposed%20KDL-Transformer%20can%20improve%0Aover%2030%25%20system%20performance%20than%20MM-PDD%20algorithm%2C%20while%20achieving%20a%0Amillisecond-level%20response%20on%20modern%20GPUs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08637v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%2520Transmit%2520and%2520Pinching%2520Beamforming%2520for%2520PASS%253A%2520Optimization-Based%2520or%250A%2520%2520Learning-Based%253F%26entry.906535625%3DXiaoxia%2520Xu%2520and%2520Xidong%2520Mu%2520and%2520Yuanwei%2520Liu%2520and%2520Arumugam%2520Nallanathan%26entry.1292438233%3D%2520%2520A%2520novel%2520pinching%2520antenna%2520system%2520%2528PASS%2529-enabled%2520downlink%2520multi-user%250Amultiple-input%2520single-output%2520%2528MISO%2529%2520framework%2520is%2520proposed.%2520PASS%2520consists%2520of%250Amultiple%2520waveguides%2520spanning%2520over%2520thousands%2520of%2520wavelength%252C%2520which%2520equip%2520numerous%250Alow-cost%2520dielectric%2520particles%252C%2520named%2520pinching%2520antennas%2520%2528PAs%2529%252C%2520to%2520radiate%250Asignals%2520into%2520free%2520space.%2520The%2520positions%2520of%2520PAs%2520can%2520be%2520reconfigured%2520to%2520change%250Aboth%2520the%2520large-scale%2520path%2520losses%2520and%2520phases%2520of%2520signals%252C%2520thus%2520facilitating%2520the%250Anovel%2520pinching%2520beamforming%2520design.%2520A%2520sum%2520rate%2520maximization%2520problem%2520is%250Aformulated%252C%2520which%2520jointly%2520optimizes%2520the%2520transmit%2520and%2520pinching%2520beamforming%2520to%250Aadaptively%2520achieve%2520constructive%2520signal%2520enhancement%2520and%2520destructive%2520interference%250Amitigation.%2520To%2520solve%2520this%2520highly%2520coupled%2520and%2520nonconvex%2520problem%252C%2520both%250Aoptimization-based%2520and%2520learning-based%2520methods%2520are%2520proposed.%25201%2529%2520For%2520the%250Aoptimization-based%2520method%252C%2520a%2520majorization-minimization%2520and%2520penalty%2520dual%250Adecomposition%2520%2528MM-PDD%2529%2520algorithm%2520is%2520developed%252C%2520which%2520handles%2520the%2520nonconvex%250Acomplex%2520exponential%2520component%2520using%2520a%2520Lipschitz%2520surrogate%2520function%2520and%2520then%250Ainvokes%2520PDD%2520for%2520problem%2520decoupling.%25202%2529%2520For%2520the%2520learning-based%2520method%252C%2520a%2520novel%250AKarush-Kuhn-Tucker%2520%2528KKT%2529-guided%2520dual%2520learning%2520%2528KDL%2529%2520approach%2520is%2520proposed%252C%2520which%250Aenables%2520KKT%2520solutions%2520to%2520be%2520reconstructed%2520in%2520a%2520data-driven%2520manner%2520by%2520learning%250Adual%2520variables.%2520Following%2520this%2520idea%252C%2520a%2520KDL-Tranformer%2520algorithm%2520is%2520developed%252C%250Awhich%2520captures%2520both%2520inter-PA/inter-user%2520dependencies%2520and%250Achannel-state-information%2520%2528CSI%2529-beamforming%2520dependencies%2520by%2520attention%250Amechanisms.%2520Simulation%2520results%2520demonstrate%2520that%253A%2520i%2529%2520The%2520proposed%2520PASS%2520framework%250Asignificantly%2520outperforms%2520conventional%2520massive%2520multiple%2520input%2520multiple%2520output%250A%2528MIMO%2529%2520system%2520even%2520with%2520a%2520few%2520PAs.%2520ii%2529%2520The%2520proposed%2520KDL-Transformer%2520can%2520improve%250Aover%252030%2525%2520system%2520performance%2520than%2520MM-PDD%2520algorithm%252C%2520while%2520achieving%2520a%250Amillisecond-level%2520response%2520on%2520modern%2520GPUs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08637v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Transmit%20and%20Pinching%20Beamforming%20for%20PASS%3A%20Optimization-Based%20or%0A%20%20Learning-Based%3F&entry.906535625=Xiaoxia%20Xu%20and%20Xidong%20Mu%20and%20Yuanwei%20Liu%20and%20Arumugam%20Nallanathan&entry.1292438233=%20%20A%20novel%20pinching%20antenna%20system%20%28PASS%29-enabled%20downlink%20multi-user%0Amultiple-input%20single-output%20%28MISO%29%20framework%20is%20proposed.%20PASS%20consists%20of%0Amultiple%20waveguides%20spanning%20over%20thousands%20of%20wavelength%2C%20which%20equip%20numerous%0Alow-cost%20dielectric%20particles%2C%20named%20pinching%20antennas%20%28PAs%29%2C%20to%20radiate%0Asignals%20into%20free%20space.%20The%20positions%20of%20PAs%20can%20be%20reconfigured%20to%20change%0Aboth%20the%20large-scale%20path%20losses%20and%20phases%20of%20signals%2C%20thus%20facilitating%20the%0Anovel%20pinching%20beamforming%20design.%20A%20sum%20rate%20maximization%20problem%20is%0Aformulated%2C%20which%20jointly%20optimizes%20the%20transmit%20and%20pinching%20beamforming%20to%0Aadaptively%20achieve%20constructive%20signal%20enhancement%20and%20destructive%20interference%0Amitigation.%20To%20solve%20this%20highly%20coupled%20and%20nonconvex%20problem%2C%20both%0Aoptimization-based%20and%20learning-based%20methods%20are%20proposed.%201%29%20For%20the%0Aoptimization-based%20method%2C%20a%20majorization-minimization%20and%20penalty%20dual%0Adecomposition%20%28MM-PDD%29%20algorithm%20is%20developed%2C%20which%20handles%20the%20nonconvex%0Acomplex%20exponential%20component%20using%20a%20Lipschitz%20surrogate%20function%20and%20then%0Ainvokes%20PDD%20for%20problem%20decoupling.%202%29%20For%20the%20learning-based%20method%2C%20a%20novel%0AKarush-Kuhn-Tucker%20%28KKT%29-guided%20dual%20learning%20%28KDL%29%20approach%20is%20proposed%2C%20which%0Aenables%20KKT%20solutions%20to%20be%20reconstructed%20in%20a%20data-driven%20manner%20by%20learning%0Adual%20variables.%20Following%20this%20idea%2C%20a%20KDL-Tranformer%20algorithm%20is%20developed%2C%0Awhich%20captures%20both%20inter-PA/inter-user%20dependencies%20and%0Achannel-state-information%20%28CSI%29-beamforming%20dependencies%20by%20attention%0Amechanisms.%20Simulation%20results%20demonstrate%20that%3A%20i%29%20The%20proposed%20PASS%20framework%0Asignificantly%20outperforms%20conventional%20massive%20multiple%20input%20multiple%20output%0A%28MIMO%29%20system%20even%20with%20a%20few%20PAs.%20ii%29%20The%20proposed%20KDL-Transformer%20can%20improve%0Aover%2030%25%20system%20performance%20than%20MM-PDD%20algorithm%2C%20while%20achieving%20a%0Amillisecond-level%20response%20on%20modern%20GPUs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08637v1&entry.124074799=Read"},
{"title": "Robot Data Curation with Mutual Information Estimators", "author": "Joey Hejna and Suvir Mirchandani and Ashwin Balakrishna and Annie Xie and Ayzaan Wahid and Jonathan Tompson and Pannag Sanketi and Dhruv Shah and Coline Devin and Dorsa Sadigh", "abstract": "  The performance of imitation learning policies often hinges on the datasets\nwith which they are trained. Consequently, investment in data collection for\nrobotics has grown across both industrial and academic labs. However, despite\nthe marked increase in the quantity of demonstrations collected, little work\nhas sought to assess the quality of said data despite mounting evidence of its\nimportance in other areas such as vision and language. In this work, we take a\ncritical step towards addressing the data quality in robotics. Given a dataset\nof demonstrations, we aim to estimate the relative quality of individual\ndemonstrations in terms of both state diversity and action predictability. To\ndo so, we estimate the average contribution of a trajectory towards the mutual\ninformation between states and actions in the entire dataset, which precisely\ncaptures both the entropy of the state distribution and the state-conditioned\nentropy of actions. Though commonly used mutual information estimators require\nvast amounts of data often beyond the scale available in robotics, we introduce\na novel technique based on k-nearest neighbor estimates of mutual information\non top of simple VAE embeddings of states and actions. Empirically, we\ndemonstrate that our approach is able to partition demonstration datasets by\nquality according to human expert scores across a diverse set of benchmarks\nspanning simulation and real world environments. Moreover, training policies\nbased on data filtered by our method leads to a 5-10% improvement in RoboMimic\nand better performance on real ALOHA and Franka setups.\n", "link": "http://arxiv.org/abs/2502.08623v1", "date": "2025-02-12", "relevancy": 1.1613, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6179}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5672}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robot%20Data%20Curation%20with%20Mutual%20Information%20Estimators&body=Title%3A%20Robot%20Data%20Curation%20with%20Mutual%20Information%20Estimators%0AAuthor%3A%20Joey%20Hejna%20and%20Suvir%20Mirchandani%20and%20Ashwin%20Balakrishna%20and%20Annie%20Xie%20and%20Ayzaan%20Wahid%20and%20Jonathan%20Tompson%20and%20Pannag%20Sanketi%20and%20Dhruv%20Shah%20and%20Coline%20Devin%20and%20Dorsa%20Sadigh%0AAbstract%3A%20%20%20The%20performance%20of%20imitation%20learning%20policies%20often%20hinges%20on%20the%20datasets%0Awith%20which%20they%20are%20trained.%20Consequently%2C%20investment%20in%20data%20collection%20for%0Arobotics%20has%20grown%20across%20both%20industrial%20and%20academic%20labs.%20However%2C%20despite%0Athe%20marked%20increase%20in%20the%20quantity%20of%20demonstrations%20collected%2C%20little%20work%0Ahas%20sought%20to%20assess%20the%20quality%20of%20said%20data%20despite%20mounting%20evidence%20of%20its%0Aimportance%20in%20other%20areas%20such%20as%20vision%20and%20language.%20In%20this%20work%2C%20we%20take%20a%0Acritical%20step%20towards%20addressing%20the%20data%20quality%20in%20robotics.%20Given%20a%20dataset%0Aof%20demonstrations%2C%20we%20aim%20to%20estimate%20the%20relative%20quality%20of%20individual%0Ademonstrations%20in%20terms%20of%20both%20state%20diversity%20and%20action%20predictability.%20To%0Ado%20so%2C%20we%20estimate%20the%20average%20contribution%20of%20a%20trajectory%20towards%20the%20mutual%0Ainformation%20between%20states%20and%20actions%20in%20the%20entire%20dataset%2C%20which%20precisely%0Acaptures%20both%20the%20entropy%20of%20the%20state%20distribution%20and%20the%20state-conditioned%0Aentropy%20of%20actions.%20Though%20commonly%20used%20mutual%20information%20estimators%20require%0Avast%20amounts%20of%20data%20often%20beyond%20the%20scale%20available%20in%20robotics%2C%20we%20introduce%0Aa%20novel%20technique%20based%20on%20k-nearest%20neighbor%20estimates%20of%20mutual%20information%0Aon%20top%20of%20simple%20VAE%20embeddings%20of%20states%20and%20actions.%20Empirically%2C%20we%0Ademonstrate%20that%20our%20approach%20is%20able%20to%20partition%20demonstration%20datasets%20by%0Aquality%20according%20to%20human%20expert%20scores%20across%20a%20diverse%20set%20of%20benchmarks%0Aspanning%20simulation%20and%20real%20world%20environments.%20Moreover%2C%20training%20policies%0Abased%20on%20data%20filtered%20by%20our%20method%20leads%20to%20a%205-10%25%20improvement%20in%20RoboMimic%0Aand%20better%20performance%20on%20real%20ALOHA%20and%20Franka%20setups.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08623v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobot%2520Data%2520Curation%2520with%2520Mutual%2520Information%2520Estimators%26entry.906535625%3DJoey%2520Hejna%2520and%2520Suvir%2520Mirchandani%2520and%2520Ashwin%2520Balakrishna%2520and%2520Annie%2520Xie%2520and%2520Ayzaan%2520Wahid%2520and%2520Jonathan%2520Tompson%2520and%2520Pannag%2520Sanketi%2520and%2520Dhruv%2520Shah%2520and%2520Coline%2520Devin%2520and%2520Dorsa%2520Sadigh%26entry.1292438233%3D%2520%2520The%2520performance%2520of%2520imitation%2520learning%2520policies%2520often%2520hinges%2520on%2520the%2520datasets%250Awith%2520which%2520they%2520are%2520trained.%2520Consequently%252C%2520investment%2520in%2520data%2520collection%2520for%250Arobotics%2520has%2520grown%2520across%2520both%2520industrial%2520and%2520academic%2520labs.%2520However%252C%2520despite%250Athe%2520marked%2520increase%2520in%2520the%2520quantity%2520of%2520demonstrations%2520collected%252C%2520little%2520work%250Ahas%2520sought%2520to%2520assess%2520the%2520quality%2520of%2520said%2520data%2520despite%2520mounting%2520evidence%2520of%2520its%250Aimportance%2520in%2520other%2520areas%2520such%2520as%2520vision%2520and%2520language.%2520In%2520this%2520work%252C%2520we%2520take%2520a%250Acritical%2520step%2520towards%2520addressing%2520the%2520data%2520quality%2520in%2520robotics.%2520Given%2520a%2520dataset%250Aof%2520demonstrations%252C%2520we%2520aim%2520to%2520estimate%2520the%2520relative%2520quality%2520of%2520individual%250Ademonstrations%2520in%2520terms%2520of%2520both%2520state%2520diversity%2520and%2520action%2520predictability.%2520To%250Ado%2520so%252C%2520we%2520estimate%2520the%2520average%2520contribution%2520of%2520a%2520trajectory%2520towards%2520the%2520mutual%250Ainformation%2520between%2520states%2520and%2520actions%2520in%2520the%2520entire%2520dataset%252C%2520which%2520precisely%250Acaptures%2520both%2520the%2520entropy%2520of%2520the%2520state%2520distribution%2520and%2520the%2520state-conditioned%250Aentropy%2520of%2520actions.%2520Though%2520commonly%2520used%2520mutual%2520information%2520estimators%2520require%250Avast%2520amounts%2520of%2520data%2520often%2520beyond%2520the%2520scale%2520available%2520in%2520robotics%252C%2520we%2520introduce%250Aa%2520novel%2520technique%2520based%2520on%2520k-nearest%2520neighbor%2520estimates%2520of%2520mutual%2520information%250Aon%2520top%2520of%2520simple%2520VAE%2520embeddings%2520of%2520states%2520and%2520actions.%2520Empirically%252C%2520we%250Ademonstrate%2520that%2520our%2520approach%2520is%2520able%2520to%2520partition%2520demonstration%2520datasets%2520by%250Aquality%2520according%2520to%2520human%2520expert%2520scores%2520across%2520a%2520diverse%2520set%2520of%2520benchmarks%250Aspanning%2520simulation%2520and%2520real%2520world%2520environments.%2520Moreover%252C%2520training%2520policies%250Abased%2520on%2520data%2520filtered%2520by%2520our%2520method%2520leads%2520to%2520a%25205-10%2525%2520improvement%2520in%2520RoboMimic%250Aand%2520better%2520performance%2520on%2520real%2520ALOHA%2520and%2520Franka%2520setups.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08623v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robot%20Data%20Curation%20with%20Mutual%20Information%20Estimators&entry.906535625=Joey%20Hejna%20and%20Suvir%20Mirchandani%20and%20Ashwin%20Balakrishna%20and%20Annie%20Xie%20and%20Ayzaan%20Wahid%20and%20Jonathan%20Tompson%20and%20Pannag%20Sanketi%20and%20Dhruv%20Shah%20and%20Coline%20Devin%20and%20Dorsa%20Sadigh&entry.1292438233=%20%20The%20performance%20of%20imitation%20learning%20policies%20often%20hinges%20on%20the%20datasets%0Awith%20which%20they%20are%20trained.%20Consequently%2C%20investment%20in%20data%20collection%20for%0Arobotics%20has%20grown%20across%20both%20industrial%20and%20academic%20labs.%20However%2C%20despite%0Athe%20marked%20increase%20in%20the%20quantity%20of%20demonstrations%20collected%2C%20little%20work%0Ahas%20sought%20to%20assess%20the%20quality%20of%20said%20data%20despite%20mounting%20evidence%20of%20its%0Aimportance%20in%20other%20areas%20such%20as%20vision%20and%20language.%20In%20this%20work%2C%20we%20take%20a%0Acritical%20step%20towards%20addressing%20the%20data%20quality%20in%20robotics.%20Given%20a%20dataset%0Aof%20demonstrations%2C%20we%20aim%20to%20estimate%20the%20relative%20quality%20of%20individual%0Ademonstrations%20in%20terms%20of%20both%20state%20diversity%20and%20action%20predictability.%20To%0Ado%20so%2C%20we%20estimate%20the%20average%20contribution%20of%20a%20trajectory%20towards%20the%20mutual%0Ainformation%20between%20states%20and%20actions%20in%20the%20entire%20dataset%2C%20which%20precisely%0Acaptures%20both%20the%20entropy%20of%20the%20state%20distribution%20and%20the%20state-conditioned%0Aentropy%20of%20actions.%20Though%20commonly%20used%20mutual%20information%20estimators%20require%0Avast%20amounts%20of%20data%20often%20beyond%20the%20scale%20available%20in%20robotics%2C%20we%20introduce%0Aa%20novel%20technique%20based%20on%20k-nearest%20neighbor%20estimates%20of%20mutual%20information%0Aon%20top%20of%20simple%20VAE%20embeddings%20of%20states%20and%20actions.%20Empirically%2C%20we%0Ademonstrate%20that%20our%20approach%20is%20able%20to%20partition%20demonstration%20datasets%20by%0Aquality%20according%20to%20human%20expert%20scores%20across%20a%20diverse%20set%20of%20benchmarks%0Aspanning%20simulation%20and%20real%20world%20environments.%20Moreover%2C%20training%20policies%0Abased%20on%20data%20filtered%20by%20our%20method%20leads%20to%20a%205-10%25%20improvement%20in%20RoboMimic%0Aand%20better%20performance%20on%20real%20ALOHA%20and%20Franka%20setups.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08623v1&entry.124074799=Read"},
{"title": "QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion\n  in Information Retrieval", "author": "Wonduk Seo and Seunghyun Lee", "abstract": "  Query expansion is widely used in Information Retrieval (IR) to improve\nsearch outcomes by enriching queries with additional contextual information.\nAlthough recent Large Language Model (LLM) based methods generate\npseudo-relevant content and expanded terms via multiple prompts, they often\nyield repetitive, narrow expansions that lack the diverse context needed to\nretrieve all relevant information. In this paper, we introduce QA-Expand, a\nnovel and effective framework for query expansion. It first generates multiple\nrelevant questions from the initial query and subsequently produces\ncorresponding pseudo-answers as surrogate documents. A feedback model further\nrewrites and filters these answers to ensure only the most informative\naugmentations are incorporated. Extensive experiments on benchmarks such as\nBEIR and TREC demonstrate that QA-Expand enhances retrieval performance by up\nto 13% over state-of-the-art methods, offering a robust solution for modern\nretrieval challenges.\n", "link": "http://arxiv.org/abs/2502.08557v1", "date": "2025-02-12", "relevancy": 1.3553, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4663}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4626}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QA-Expand%3A%20Multi-Question%20Answer%20Generation%20for%20Enhanced%20Query%20Expansion%0A%20%20in%20Information%20Retrieval&body=Title%3A%20QA-Expand%3A%20Multi-Question%20Answer%20Generation%20for%20Enhanced%20Query%20Expansion%0A%20%20in%20Information%20Retrieval%0AAuthor%3A%20Wonduk%20Seo%20and%20Seunghyun%20Lee%0AAbstract%3A%20%20%20Query%20expansion%20is%20widely%20used%20in%20Information%20Retrieval%20%28IR%29%20to%20improve%0Asearch%20outcomes%20by%20enriching%20queries%20with%20additional%20contextual%20information.%0AAlthough%20recent%20Large%20Language%20Model%20%28LLM%29%20based%20methods%20generate%0Apseudo-relevant%20content%20and%20expanded%20terms%20via%20multiple%20prompts%2C%20they%20often%0Ayield%20repetitive%2C%20narrow%20expansions%20that%20lack%20the%20diverse%20context%20needed%20to%0Aretrieve%20all%20relevant%20information.%20In%20this%20paper%2C%20we%20introduce%20QA-Expand%2C%20a%0Anovel%20and%20effective%20framework%20for%20query%20expansion.%20It%20first%20generates%20multiple%0Arelevant%20questions%20from%20the%20initial%20query%20and%20subsequently%20produces%0Acorresponding%20pseudo-answers%20as%20surrogate%20documents.%20A%20feedback%20model%20further%0Arewrites%20and%20filters%20these%20answers%20to%20ensure%20only%20the%20most%20informative%0Aaugmentations%20are%20incorporated.%20Extensive%20experiments%20on%20benchmarks%20such%20as%0ABEIR%20and%20TREC%20demonstrate%20that%20QA-Expand%20enhances%20retrieval%20performance%20by%20up%0Ato%2013%25%20over%20state-of-the-art%20methods%2C%20offering%20a%20robust%20solution%20for%20modern%0Aretrieval%20challenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08557v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQA-Expand%253A%2520Multi-Question%2520Answer%2520Generation%2520for%2520Enhanced%2520Query%2520Expansion%250A%2520%2520in%2520Information%2520Retrieval%26entry.906535625%3DWonduk%2520Seo%2520and%2520Seunghyun%2520Lee%26entry.1292438233%3D%2520%2520Query%2520expansion%2520is%2520widely%2520used%2520in%2520Information%2520Retrieval%2520%2528IR%2529%2520to%2520improve%250Asearch%2520outcomes%2520by%2520enriching%2520queries%2520with%2520additional%2520contextual%2520information.%250AAlthough%2520recent%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520based%2520methods%2520generate%250Apseudo-relevant%2520content%2520and%2520expanded%2520terms%2520via%2520multiple%2520prompts%252C%2520they%2520often%250Ayield%2520repetitive%252C%2520narrow%2520expansions%2520that%2520lack%2520the%2520diverse%2520context%2520needed%2520to%250Aretrieve%2520all%2520relevant%2520information.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520QA-Expand%252C%2520a%250Anovel%2520and%2520effective%2520framework%2520for%2520query%2520expansion.%2520It%2520first%2520generates%2520multiple%250Arelevant%2520questions%2520from%2520the%2520initial%2520query%2520and%2520subsequently%2520produces%250Acorresponding%2520pseudo-answers%2520as%2520surrogate%2520documents.%2520A%2520feedback%2520model%2520further%250Arewrites%2520and%2520filters%2520these%2520answers%2520to%2520ensure%2520only%2520the%2520most%2520informative%250Aaugmentations%2520are%2520incorporated.%2520Extensive%2520experiments%2520on%2520benchmarks%2520such%2520as%250ABEIR%2520and%2520TREC%2520demonstrate%2520that%2520QA-Expand%2520enhances%2520retrieval%2520performance%2520by%2520up%250Ato%252013%2525%2520over%2520state-of-the-art%2520methods%252C%2520offering%2520a%2520robust%2520solution%2520for%2520modern%250Aretrieval%2520challenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08557v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QA-Expand%3A%20Multi-Question%20Answer%20Generation%20for%20Enhanced%20Query%20Expansion%0A%20%20in%20Information%20Retrieval&entry.906535625=Wonduk%20Seo%20and%20Seunghyun%20Lee&entry.1292438233=%20%20Query%20expansion%20is%20widely%20used%20in%20Information%20Retrieval%20%28IR%29%20to%20improve%0Asearch%20outcomes%20by%20enriching%20queries%20with%20additional%20contextual%20information.%0AAlthough%20recent%20Large%20Language%20Model%20%28LLM%29%20based%20methods%20generate%0Apseudo-relevant%20content%20and%20expanded%20terms%20via%20multiple%20prompts%2C%20they%20often%0Ayield%20repetitive%2C%20narrow%20expansions%20that%20lack%20the%20diverse%20context%20needed%20to%0Aretrieve%20all%20relevant%20information.%20In%20this%20paper%2C%20we%20introduce%20QA-Expand%2C%20a%0Anovel%20and%20effective%20framework%20for%20query%20expansion.%20It%20first%20generates%20multiple%0Arelevant%20questions%20from%20the%20initial%20query%20and%20subsequently%20produces%0Acorresponding%20pseudo-answers%20as%20surrogate%20documents.%20A%20feedback%20model%20further%0Arewrites%20and%20filters%20these%20answers%20to%20ensure%20only%20the%20most%20informative%0Aaugmentations%20are%20incorporated.%20Extensive%20experiments%20on%20benchmarks%20such%20as%0ABEIR%20and%20TREC%20demonstrate%20that%20QA-Expand%20enhances%20retrieval%20performance%20by%20up%0Ato%2013%25%20over%20state-of-the-art%20methods%2C%20offering%20a%20robust%20solution%20for%20modern%0Aretrieval%20challenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08557v1&entry.124074799=Read"},
{"title": "Necessary and Sufficient Oracles: Toward a Computational Taxonomy For\n  Reinforcement Learning", "author": "Dhruv Rohatgi and Dylan J. Foster", "abstract": "  Algorithms for reinforcement learning (RL) in large state spaces crucially\nrely on supervised learning subroutines to estimate objects such as value\nfunctions or transition probabilities. Since only the simplest supervised\nlearning problems can be solved provably and efficiently, practical performance\nof an RL algorithm depends on which of these supervised learning \"oracles\" it\nassumes access to (and how they are implemented). But which oracles are better\nor worse? Is there a minimal oracle?\n  In this work, we clarify the impact of the choice of supervised learning\noracle on the computational complexity of RL, as quantified by the oracle\nstrength. First, for the task of reward-free exploration in Block MDPs in the\nstandard episodic access model -- a ubiquitous setting for RL with function\napproximation -- we identify two-context regression as a minimal oracle, i.e.\nan oracle that is both necessary and sufficient (under a mild regularity\nassumption). Second, we identify one-context regression as a near-minimal\noracle in the stronger reset access model, establishing a provable\ncomputational benefit of resets in the process. Third, we broaden our focus to\nLow-Rank MDPs, where we give cryptographic evidence that the analogous oracle\nfrom the Block MDP setting is insufficient.\n", "link": "http://arxiv.org/abs/2502.08632v1", "date": "2025-02-12", "relevancy": 1.3704, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4658}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4656}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Necessary%20and%20Sufficient%20Oracles%3A%20Toward%20a%20Computational%20Taxonomy%20For%0A%20%20Reinforcement%20Learning&body=Title%3A%20Necessary%20and%20Sufficient%20Oracles%3A%20Toward%20a%20Computational%20Taxonomy%20For%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Dhruv%20Rohatgi%20and%20Dylan%20J.%20Foster%0AAbstract%3A%20%20%20Algorithms%20for%20reinforcement%20learning%20%28RL%29%20in%20large%20state%20spaces%20crucially%0Arely%20on%20supervised%20learning%20subroutines%20to%20estimate%20objects%20such%20as%20value%0Afunctions%20or%20transition%20probabilities.%20Since%20only%20the%20simplest%20supervised%0Alearning%20problems%20can%20be%20solved%20provably%20and%20efficiently%2C%20practical%20performance%0Aof%20an%20RL%20algorithm%20depends%20on%20which%20of%20these%20supervised%20learning%20%22oracles%22%20it%0Aassumes%20access%20to%20%28and%20how%20they%20are%20implemented%29.%20But%20which%20oracles%20are%20better%0Aor%20worse%3F%20Is%20there%20a%20minimal%20oracle%3F%0A%20%20In%20this%20work%2C%20we%20clarify%20the%20impact%20of%20the%20choice%20of%20supervised%20learning%0Aoracle%20on%20the%20computational%20complexity%20of%20RL%2C%20as%20quantified%20by%20the%20oracle%0Astrength.%20First%2C%20for%20the%20task%20of%20reward-free%20exploration%20in%20Block%20MDPs%20in%20the%0Astandard%20episodic%20access%20model%20--%20a%20ubiquitous%20setting%20for%20RL%20with%20function%0Aapproximation%20--%20we%20identify%20two-context%20regression%20as%20a%20minimal%20oracle%2C%20i.e.%0Aan%20oracle%20that%20is%20both%20necessary%20and%20sufficient%20%28under%20a%20mild%20regularity%0Aassumption%29.%20Second%2C%20we%20identify%20one-context%20regression%20as%20a%20near-minimal%0Aoracle%20in%20the%20stronger%20reset%20access%20model%2C%20establishing%20a%20provable%0Acomputational%20benefit%20of%20resets%20in%20the%20process.%20Third%2C%20we%20broaden%20our%20focus%20to%0ALow-Rank%20MDPs%2C%20where%20we%20give%20cryptographic%20evidence%20that%20the%20analogous%20oracle%0Afrom%20the%20Block%20MDP%20setting%20is%20insufficient.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08632v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNecessary%2520and%2520Sufficient%2520Oracles%253A%2520Toward%2520a%2520Computational%2520Taxonomy%2520For%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DDhruv%2520Rohatgi%2520and%2520Dylan%2520J.%2520Foster%26entry.1292438233%3D%2520%2520Algorithms%2520for%2520reinforcement%2520learning%2520%2528RL%2529%2520in%2520large%2520state%2520spaces%2520crucially%250Arely%2520on%2520supervised%2520learning%2520subroutines%2520to%2520estimate%2520objects%2520such%2520as%2520value%250Afunctions%2520or%2520transition%2520probabilities.%2520Since%2520only%2520the%2520simplest%2520supervised%250Alearning%2520problems%2520can%2520be%2520solved%2520provably%2520and%2520efficiently%252C%2520practical%2520performance%250Aof%2520an%2520RL%2520algorithm%2520depends%2520on%2520which%2520of%2520these%2520supervised%2520learning%2520%2522oracles%2522%2520it%250Aassumes%2520access%2520to%2520%2528and%2520how%2520they%2520are%2520implemented%2529.%2520But%2520which%2520oracles%2520are%2520better%250Aor%2520worse%253F%2520Is%2520there%2520a%2520minimal%2520oracle%253F%250A%2520%2520In%2520this%2520work%252C%2520we%2520clarify%2520the%2520impact%2520of%2520the%2520choice%2520of%2520supervised%2520learning%250Aoracle%2520on%2520the%2520computational%2520complexity%2520of%2520RL%252C%2520as%2520quantified%2520by%2520the%2520oracle%250Astrength.%2520First%252C%2520for%2520the%2520task%2520of%2520reward-free%2520exploration%2520in%2520Block%2520MDPs%2520in%2520the%250Astandard%2520episodic%2520access%2520model%2520--%2520a%2520ubiquitous%2520setting%2520for%2520RL%2520with%2520function%250Aapproximation%2520--%2520we%2520identify%2520two-context%2520regression%2520as%2520a%2520minimal%2520oracle%252C%2520i.e.%250Aan%2520oracle%2520that%2520is%2520both%2520necessary%2520and%2520sufficient%2520%2528under%2520a%2520mild%2520regularity%250Aassumption%2529.%2520Second%252C%2520we%2520identify%2520one-context%2520regression%2520as%2520a%2520near-minimal%250Aoracle%2520in%2520the%2520stronger%2520reset%2520access%2520model%252C%2520establishing%2520a%2520provable%250Acomputational%2520benefit%2520of%2520resets%2520in%2520the%2520process.%2520Third%252C%2520we%2520broaden%2520our%2520focus%2520to%250ALow-Rank%2520MDPs%252C%2520where%2520we%2520give%2520cryptographic%2520evidence%2520that%2520the%2520analogous%2520oracle%250Afrom%2520the%2520Block%2520MDP%2520setting%2520is%2520insufficient.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08632v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Necessary%20and%20Sufficient%20Oracles%3A%20Toward%20a%20Computational%20Taxonomy%20For%0A%20%20Reinforcement%20Learning&entry.906535625=Dhruv%20Rohatgi%20and%20Dylan%20J.%20Foster&entry.1292438233=%20%20Algorithms%20for%20reinforcement%20learning%20%28RL%29%20in%20large%20state%20spaces%20crucially%0Arely%20on%20supervised%20learning%20subroutines%20to%20estimate%20objects%20such%20as%20value%0Afunctions%20or%20transition%20probabilities.%20Since%20only%20the%20simplest%20supervised%0Alearning%20problems%20can%20be%20solved%20provably%20and%20efficiently%2C%20practical%20performance%0Aof%20an%20RL%20algorithm%20depends%20on%20which%20of%20these%20supervised%20learning%20%22oracles%22%20it%0Aassumes%20access%20to%20%28and%20how%20they%20are%20implemented%29.%20But%20which%20oracles%20are%20better%0Aor%20worse%3F%20Is%20there%20a%20minimal%20oracle%3F%0A%20%20In%20this%20work%2C%20we%20clarify%20the%20impact%20of%20the%20choice%20of%20supervised%20learning%0Aoracle%20on%20the%20computational%20complexity%20of%20RL%2C%20as%20quantified%20by%20the%20oracle%0Astrength.%20First%2C%20for%20the%20task%20of%20reward-free%20exploration%20in%20Block%20MDPs%20in%20the%0Astandard%20episodic%20access%20model%20--%20a%20ubiquitous%20setting%20for%20RL%20with%20function%0Aapproximation%20--%20we%20identify%20two-context%20regression%20as%20a%20minimal%20oracle%2C%20i.e.%0Aan%20oracle%20that%20is%20both%20necessary%20and%20sufficient%20%28under%20a%20mild%20regularity%0Aassumption%29.%20Second%2C%20we%20identify%20one-context%20regression%20as%20a%20near-minimal%0Aoracle%20in%20the%20stronger%20reset%20access%20model%2C%20establishing%20a%20provable%0Acomputational%20benefit%20of%20resets%20in%20the%20process.%20Third%2C%20we%20broaden%20our%20focus%20to%0ALow-Rank%20MDPs%2C%20where%20we%20give%20cryptographic%20evidence%20that%20the%20analogous%20oracle%0Afrom%20the%20Block%20MDP%20setting%20is%20insufficient.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08632v1&entry.124074799=Read"},
{"title": "Quantifying Security Vulnerabilities: A Metric-Driven Security Analysis\n  of Gaps in Current AI Standards", "author": "Keerthana Madhavan and Abbas Yazdinejad and Fattane Zarrinkalam and Ali Dehghantanha", "abstract": "  As AI systems integrate into critical infrastructure, security gaps in AI\ncompliance frameworks demand urgent attention. This paper audits and quantifies\nsecurity risks in three major AI governance standards: NIST AI RMF 1.0, UK's AI\nand Data Protection Risk Toolkit, and the EU's ALTAI. Using a novel risk\nassessment methodology, we develop four key metrics: Risk Severity Index (RSI),\nAttack Potential Index (AVPI), Compliance-Security Gap Percentage (CSGP), and\nRoot Cause Vulnerability Score (RCVS). Our analysis identifies 136 concerns\nacross the frameworks, exposing significant gaps. NIST fails to address 69.23\npercent of identified risks, ALTAI has the highest attack vector vulnerability\n(AVPI = 0.51) and the ICO Toolkit has the largest compliance-security gap, with\n80.00 percent of high-risk concerns remaining unresolved. Root cause analysis\nhighlights under-defined processes (ALTAI RCVS = 033) and weak implementation\nguidance (NIST and ICO RCVS = 0.25) as critical weaknesses. These findings\nemphasize the need for stronger, enforceable security controls in AI\ncompliance. We offer targeted recommendations to enhance security posture and\nbridge the gap between compliance and real-world AI risks.\n", "link": "http://arxiv.org/abs/2502.08610v1", "date": "2025-02-12", "relevancy": 1.2458, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4312}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4121}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantifying%20Security%20Vulnerabilities%3A%20A%20Metric-Driven%20Security%20Analysis%0A%20%20of%20Gaps%20in%20Current%20AI%20Standards&body=Title%3A%20Quantifying%20Security%20Vulnerabilities%3A%20A%20Metric-Driven%20Security%20Analysis%0A%20%20of%20Gaps%20in%20Current%20AI%20Standards%0AAuthor%3A%20Keerthana%20Madhavan%20and%20Abbas%20Yazdinejad%20and%20Fattane%20Zarrinkalam%20and%20Ali%20Dehghantanha%0AAbstract%3A%20%20%20As%20AI%20systems%20integrate%20into%20critical%20infrastructure%2C%20security%20gaps%20in%20AI%0Acompliance%20frameworks%20demand%20urgent%20attention.%20This%20paper%20audits%20and%20quantifies%0Asecurity%20risks%20in%20three%20major%20AI%20governance%20standards%3A%20NIST%20AI%20RMF%201.0%2C%20UK%27s%20AI%0Aand%20Data%20Protection%20Risk%20Toolkit%2C%20and%20the%20EU%27s%20ALTAI.%20Using%20a%20novel%20risk%0Aassessment%20methodology%2C%20we%20develop%20four%20key%20metrics%3A%20Risk%20Severity%20Index%20%28RSI%29%2C%0AAttack%20Potential%20Index%20%28AVPI%29%2C%20Compliance-Security%20Gap%20Percentage%20%28CSGP%29%2C%20and%0ARoot%20Cause%20Vulnerability%20Score%20%28RCVS%29.%20Our%20analysis%20identifies%20136%20concerns%0Aacross%20the%20frameworks%2C%20exposing%20significant%20gaps.%20NIST%20fails%20to%20address%2069.23%0Apercent%20of%20identified%20risks%2C%20ALTAI%20has%20the%20highest%20attack%20vector%20vulnerability%0A%28AVPI%20%3D%200.51%29%20and%20the%20ICO%20Toolkit%20has%20the%20largest%20compliance-security%20gap%2C%20with%0A80.00%20percent%20of%20high-risk%20concerns%20remaining%20unresolved.%20Root%20cause%20analysis%0Ahighlights%20under-defined%20processes%20%28ALTAI%20RCVS%20%3D%20033%29%20and%20weak%20implementation%0Aguidance%20%28NIST%20and%20ICO%20RCVS%20%3D%200.25%29%20as%20critical%20weaknesses.%20These%20findings%0Aemphasize%20the%20need%20for%20stronger%2C%20enforceable%20security%20controls%20in%20AI%0Acompliance.%20We%20offer%20targeted%20recommendations%20to%20enhance%20security%20posture%20and%0Abridge%20the%20gap%20between%20compliance%20and%20real-world%20AI%20risks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08610v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantifying%2520Security%2520Vulnerabilities%253A%2520A%2520Metric-Driven%2520Security%2520Analysis%250A%2520%2520of%2520Gaps%2520in%2520Current%2520AI%2520Standards%26entry.906535625%3DKeerthana%2520Madhavan%2520and%2520Abbas%2520Yazdinejad%2520and%2520Fattane%2520Zarrinkalam%2520and%2520Ali%2520Dehghantanha%26entry.1292438233%3D%2520%2520As%2520AI%2520systems%2520integrate%2520into%2520critical%2520infrastructure%252C%2520security%2520gaps%2520in%2520AI%250Acompliance%2520frameworks%2520demand%2520urgent%2520attention.%2520This%2520paper%2520audits%2520and%2520quantifies%250Asecurity%2520risks%2520in%2520three%2520major%2520AI%2520governance%2520standards%253A%2520NIST%2520AI%2520RMF%25201.0%252C%2520UK%2527s%2520AI%250Aand%2520Data%2520Protection%2520Risk%2520Toolkit%252C%2520and%2520the%2520EU%2527s%2520ALTAI.%2520Using%2520a%2520novel%2520risk%250Aassessment%2520methodology%252C%2520we%2520develop%2520four%2520key%2520metrics%253A%2520Risk%2520Severity%2520Index%2520%2528RSI%2529%252C%250AAttack%2520Potential%2520Index%2520%2528AVPI%2529%252C%2520Compliance-Security%2520Gap%2520Percentage%2520%2528CSGP%2529%252C%2520and%250ARoot%2520Cause%2520Vulnerability%2520Score%2520%2528RCVS%2529.%2520Our%2520analysis%2520identifies%2520136%2520concerns%250Aacross%2520the%2520frameworks%252C%2520exposing%2520significant%2520gaps.%2520NIST%2520fails%2520to%2520address%252069.23%250Apercent%2520of%2520identified%2520risks%252C%2520ALTAI%2520has%2520the%2520highest%2520attack%2520vector%2520vulnerability%250A%2528AVPI%2520%253D%25200.51%2529%2520and%2520the%2520ICO%2520Toolkit%2520has%2520the%2520largest%2520compliance-security%2520gap%252C%2520with%250A80.00%2520percent%2520of%2520high-risk%2520concerns%2520remaining%2520unresolved.%2520Root%2520cause%2520analysis%250Ahighlights%2520under-defined%2520processes%2520%2528ALTAI%2520RCVS%2520%253D%2520033%2529%2520and%2520weak%2520implementation%250Aguidance%2520%2528NIST%2520and%2520ICO%2520RCVS%2520%253D%25200.25%2529%2520as%2520critical%2520weaknesses.%2520These%2520findings%250Aemphasize%2520the%2520need%2520for%2520stronger%252C%2520enforceable%2520security%2520controls%2520in%2520AI%250Acompliance.%2520We%2520offer%2520targeted%2520recommendations%2520to%2520enhance%2520security%2520posture%2520and%250Abridge%2520the%2520gap%2520between%2520compliance%2520and%2520real-world%2520AI%2520risks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08610v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantifying%20Security%20Vulnerabilities%3A%20A%20Metric-Driven%20Security%20Analysis%0A%20%20of%20Gaps%20in%20Current%20AI%20Standards&entry.906535625=Keerthana%20Madhavan%20and%20Abbas%20Yazdinejad%20and%20Fattane%20Zarrinkalam%20and%20Ali%20Dehghantanha&entry.1292438233=%20%20As%20AI%20systems%20integrate%20into%20critical%20infrastructure%2C%20security%20gaps%20in%20AI%0Acompliance%20frameworks%20demand%20urgent%20attention.%20This%20paper%20audits%20and%20quantifies%0Asecurity%20risks%20in%20three%20major%20AI%20governance%20standards%3A%20NIST%20AI%20RMF%201.0%2C%20UK%27s%20AI%0Aand%20Data%20Protection%20Risk%20Toolkit%2C%20and%20the%20EU%27s%20ALTAI.%20Using%20a%20novel%20risk%0Aassessment%20methodology%2C%20we%20develop%20four%20key%20metrics%3A%20Risk%20Severity%20Index%20%28RSI%29%2C%0AAttack%20Potential%20Index%20%28AVPI%29%2C%20Compliance-Security%20Gap%20Percentage%20%28CSGP%29%2C%20and%0ARoot%20Cause%20Vulnerability%20Score%20%28RCVS%29.%20Our%20analysis%20identifies%20136%20concerns%0Aacross%20the%20frameworks%2C%20exposing%20significant%20gaps.%20NIST%20fails%20to%20address%2069.23%0Apercent%20of%20identified%20risks%2C%20ALTAI%20has%20the%20highest%20attack%20vector%20vulnerability%0A%28AVPI%20%3D%200.51%29%20and%20the%20ICO%20Toolkit%20has%20the%20largest%20compliance-security%20gap%2C%20with%0A80.00%20percent%20of%20high-risk%20concerns%20remaining%20unresolved.%20Root%20cause%20analysis%0Ahighlights%20under-defined%20processes%20%28ALTAI%20RCVS%20%3D%20033%29%20and%20weak%20implementation%0Aguidance%20%28NIST%20and%20ICO%20RCVS%20%3D%200.25%29%20as%20critical%20weaknesses.%20These%20findings%0Aemphasize%20the%20need%20for%20stronger%2C%20enforceable%20security%20controls%20in%20AI%0Acompliance.%20We%20offer%20targeted%20recommendations%20to%20enhance%20security%20posture%20and%0Abridge%20the%20gap%20between%20compliance%20and%20real-world%20AI%20risks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08610v1&entry.124074799=Read"},
{"title": "A Machine Learning-Ready Data Processing Tool for Near Real-Time\n  Forecasting", "author": "Maher A Dayeh and Michael J Starkey and Subhamoy Chatterjee and Heather Elliott and Samuel Hart and Kimberly Moreland", "abstract": "  Space weather forecasting is critical for mitigating radiation risks in space\nexploration and protecting Earth-based technologies from geomagnetic\ndisturbances. This paper presents the development of a Machine Learning (ML)-\nready data processing tool for Near Real-Time (NRT) space weather forecasting.\nBy merging data from diverse NRT sources such as solar imagery, magnetic field\nmeasurements, and energetic particle fluxes, the tool addresses key gaps in\ncurrent space weather prediction capabilities. The tool processes and\nstructures the data for machine learning models, focusing on time-series\nforecasting and event detection for extreme solar events. It provides users\nwith a framework to download, process, and label data for ML applications,\nstreamlining the workflow for improved NRT space weather forecasting and\nscientific research.\n", "link": "http://arxiv.org/abs/2502.08555v1", "date": "2025-02-12", "relevancy": 0.8724, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4509}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4358}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Machine%20Learning-Ready%20Data%20Processing%20Tool%20for%20Near%20Real-Time%0A%20%20Forecasting&body=Title%3A%20A%20Machine%20Learning-Ready%20Data%20Processing%20Tool%20for%20Near%20Real-Time%0A%20%20Forecasting%0AAuthor%3A%20Maher%20A%20Dayeh%20and%20Michael%20J%20Starkey%20and%20Subhamoy%20Chatterjee%20and%20Heather%20Elliott%20and%20Samuel%20Hart%20and%20Kimberly%20Moreland%0AAbstract%3A%20%20%20Space%20weather%20forecasting%20is%20critical%20for%20mitigating%20radiation%20risks%20in%20space%0Aexploration%20and%20protecting%20Earth-based%20technologies%20from%20geomagnetic%0Adisturbances.%20This%20paper%20presents%20the%20development%20of%20a%20Machine%20Learning%20%28ML%29-%0Aready%20data%20processing%20tool%20for%20Near%20Real-Time%20%28NRT%29%20space%20weather%20forecasting.%0ABy%20merging%20data%20from%20diverse%20NRT%20sources%20such%20as%20solar%20imagery%2C%20magnetic%20field%0Ameasurements%2C%20and%20energetic%20particle%20fluxes%2C%20the%20tool%20addresses%20key%20gaps%20in%0Acurrent%20space%20weather%20prediction%20capabilities.%20The%20tool%20processes%20and%0Astructures%20the%20data%20for%20machine%20learning%20models%2C%20focusing%20on%20time-series%0Aforecasting%20and%20event%20detection%20for%20extreme%20solar%20events.%20It%20provides%20users%0Awith%20a%20framework%20to%20download%2C%20process%2C%20and%20label%20data%20for%20ML%20applications%2C%0Astreamlining%20the%20workflow%20for%20improved%20NRT%20space%20weather%20forecasting%20and%0Ascientific%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08555v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Machine%2520Learning-Ready%2520Data%2520Processing%2520Tool%2520for%2520Near%2520Real-Time%250A%2520%2520Forecasting%26entry.906535625%3DMaher%2520A%2520Dayeh%2520and%2520Michael%2520J%2520Starkey%2520and%2520Subhamoy%2520Chatterjee%2520and%2520Heather%2520Elliott%2520and%2520Samuel%2520Hart%2520and%2520Kimberly%2520Moreland%26entry.1292438233%3D%2520%2520Space%2520weather%2520forecasting%2520is%2520critical%2520for%2520mitigating%2520radiation%2520risks%2520in%2520space%250Aexploration%2520and%2520protecting%2520Earth-based%2520technologies%2520from%2520geomagnetic%250Adisturbances.%2520This%2520paper%2520presents%2520the%2520development%2520of%2520a%2520Machine%2520Learning%2520%2528ML%2529-%250Aready%2520data%2520processing%2520tool%2520for%2520Near%2520Real-Time%2520%2528NRT%2529%2520space%2520weather%2520forecasting.%250ABy%2520merging%2520data%2520from%2520diverse%2520NRT%2520sources%2520such%2520as%2520solar%2520imagery%252C%2520magnetic%2520field%250Ameasurements%252C%2520and%2520energetic%2520particle%2520fluxes%252C%2520the%2520tool%2520addresses%2520key%2520gaps%2520in%250Acurrent%2520space%2520weather%2520prediction%2520capabilities.%2520The%2520tool%2520processes%2520and%250Astructures%2520the%2520data%2520for%2520machine%2520learning%2520models%252C%2520focusing%2520on%2520time-series%250Aforecasting%2520and%2520event%2520detection%2520for%2520extreme%2520solar%2520events.%2520It%2520provides%2520users%250Awith%2520a%2520framework%2520to%2520download%252C%2520process%252C%2520and%2520label%2520data%2520for%2520ML%2520applications%252C%250Astreamlining%2520the%2520workflow%2520for%2520improved%2520NRT%2520space%2520weather%2520forecasting%2520and%250Ascientific%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08555v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Machine%20Learning-Ready%20Data%20Processing%20Tool%20for%20Near%20Real-Time%0A%20%20Forecasting&entry.906535625=Maher%20A%20Dayeh%20and%20Michael%20J%20Starkey%20and%20Subhamoy%20Chatterjee%20and%20Heather%20Elliott%20and%20Samuel%20Hart%20and%20Kimberly%20Moreland&entry.1292438233=%20%20Space%20weather%20forecasting%20is%20critical%20for%20mitigating%20radiation%20risks%20in%20space%0Aexploration%20and%20protecting%20Earth-based%20technologies%20from%20geomagnetic%0Adisturbances.%20This%20paper%20presents%20the%20development%20of%20a%20Machine%20Learning%20%28ML%29-%0Aready%20data%20processing%20tool%20for%20Near%20Real-Time%20%28NRT%29%20space%20weather%20forecasting.%0ABy%20merging%20data%20from%20diverse%20NRT%20sources%20such%20as%20solar%20imagery%2C%20magnetic%20field%0Ameasurements%2C%20and%20energetic%20particle%20fluxes%2C%20the%20tool%20addresses%20key%20gaps%20in%0Acurrent%20space%20weather%20prediction%20capabilities.%20The%20tool%20processes%20and%0Astructures%20the%20data%20for%20machine%20learning%20models%2C%20focusing%20on%20time-series%0Aforecasting%20and%20event%20detection%20for%20extreme%20solar%20events.%20It%20provides%20users%0Awith%20a%20framework%20to%20download%2C%20process%2C%20and%20label%20data%20for%20ML%20applications%2C%0Astreamlining%20the%20workflow%20for%20improved%20NRT%20space%20weather%20forecasting%20and%0Ascientific%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08555v1&entry.124074799=Read"},
{"title": "Causal Discovery from Conditionally Stationary Time Series", "author": "Carles Balsells-Rodas and Xavier Sumba and Tanmayee Narendra and Ruibo Tu and Gabriele Schweikert and Hedvig Kjellstrom and Yingzhen Li", "abstract": "  Causal discovery, i.e., inferring underlying causal relationships from\nobservational data, is highly challenging for AI systems. In a time series\nmodeling context, traditional causal discovery methods mainly consider\nconstrained scenarios with fully observed variables and/or data from stationary\ntime-series. We develop a causal discovery approach to handle a wide class of\nnonstationary time series that are conditionally stationary, where the\nnonstationary behaviour is modeled as stationarity conditioned on a set of\nlatent state variables. Named State-Dependent Causal Inference (SDCI), our\napproach is able to recover the underlying causal dependencies, with provable\nidentifiablity for the state-dependent causal structures. Empirical experiments\non nonlinear particle interaction data and gene regulatory networks demonstrate\nSDCI's superior performance over baseline causal discovery methods. Improved\nresults over non-causal RNNs on modeling NBA player movements demonstrate the\npotential of our method and motivate the use of causality-driven methods for\nforecasting.\n", "link": "http://arxiv.org/abs/2110.06257v3", "date": "2025-02-12", "relevancy": 0.9173, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.471}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4694}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal%20Discovery%20from%20Conditionally%20Stationary%20Time%20Series&body=Title%3A%20Causal%20Discovery%20from%20Conditionally%20Stationary%20Time%20Series%0AAuthor%3A%20Carles%20Balsells-Rodas%20and%20Xavier%20Sumba%20and%20Tanmayee%20Narendra%20and%20Ruibo%20Tu%20and%20Gabriele%20Schweikert%20and%20Hedvig%20Kjellstrom%20and%20Yingzhen%20Li%0AAbstract%3A%20%20%20Causal%20discovery%2C%20i.e.%2C%20inferring%20underlying%20causal%20relationships%20from%0Aobservational%20data%2C%20is%20highly%20challenging%20for%20AI%20systems.%20In%20a%20time%20series%0Amodeling%20context%2C%20traditional%20causal%20discovery%20methods%20mainly%20consider%0Aconstrained%20scenarios%20with%20fully%20observed%20variables%20and/or%20data%20from%20stationary%0Atime-series.%20We%20develop%20a%20causal%20discovery%20approach%20to%20handle%20a%20wide%20class%20of%0Anonstationary%20time%20series%20that%20are%20conditionally%20stationary%2C%20where%20the%0Anonstationary%20behaviour%20is%20modeled%20as%20stationarity%20conditioned%20on%20a%20set%20of%0Alatent%20state%20variables.%20Named%20State-Dependent%20Causal%20Inference%20%28SDCI%29%2C%20our%0Aapproach%20is%20able%20to%20recover%20the%20underlying%20causal%20dependencies%2C%20with%20provable%0Aidentifiablity%20for%20the%20state-dependent%20causal%20structures.%20Empirical%20experiments%0Aon%20nonlinear%20particle%20interaction%20data%20and%20gene%20regulatory%20networks%20demonstrate%0ASDCI%27s%20superior%20performance%20over%20baseline%20causal%20discovery%20methods.%20Improved%0Aresults%20over%20non-causal%20RNNs%20on%20modeling%20NBA%20player%20movements%20demonstrate%20the%0Apotential%20of%20our%20method%20and%20motivate%20the%20use%20of%20causality-driven%20methods%20for%0Aforecasting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2110.06257v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal%2520Discovery%2520from%2520Conditionally%2520Stationary%2520Time%2520Series%26entry.906535625%3DCarles%2520Balsells-Rodas%2520and%2520Xavier%2520Sumba%2520and%2520Tanmayee%2520Narendra%2520and%2520Ruibo%2520Tu%2520and%2520Gabriele%2520Schweikert%2520and%2520Hedvig%2520Kjellstrom%2520and%2520Yingzhen%2520Li%26entry.1292438233%3D%2520%2520Causal%2520discovery%252C%2520i.e.%252C%2520inferring%2520underlying%2520causal%2520relationships%2520from%250Aobservational%2520data%252C%2520is%2520highly%2520challenging%2520for%2520AI%2520systems.%2520In%2520a%2520time%2520series%250Amodeling%2520context%252C%2520traditional%2520causal%2520discovery%2520methods%2520mainly%2520consider%250Aconstrained%2520scenarios%2520with%2520fully%2520observed%2520variables%2520and/or%2520data%2520from%2520stationary%250Atime-series.%2520We%2520develop%2520a%2520causal%2520discovery%2520approach%2520to%2520handle%2520a%2520wide%2520class%2520of%250Anonstationary%2520time%2520series%2520that%2520are%2520conditionally%2520stationary%252C%2520where%2520the%250Anonstationary%2520behaviour%2520is%2520modeled%2520as%2520stationarity%2520conditioned%2520on%2520a%2520set%2520of%250Alatent%2520state%2520variables.%2520Named%2520State-Dependent%2520Causal%2520Inference%2520%2528SDCI%2529%252C%2520our%250Aapproach%2520is%2520able%2520to%2520recover%2520the%2520underlying%2520causal%2520dependencies%252C%2520with%2520provable%250Aidentifiablity%2520for%2520the%2520state-dependent%2520causal%2520structures.%2520Empirical%2520experiments%250Aon%2520nonlinear%2520particle%2520interaction%2520data%2520and%2520gene%2520regulatory%2520networks%2520demonstrate%250ASDCI%2527s%2520superior%2520performance%2520over%2520baseline%2520causal%2520discovery%2520methods.%2520Improved%250Aresults%2520over%2520non-causal%2520RNNs%2520on%2520modeling%2520NBA%2520player%2520movements%2520demonstrate%2520the%250Apotential%2520of%2520our%2520method%2520and%2520motivate%2520the%2520use%2520of%2520causality-driven%2520methods%2520for%250Aforecasting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2110.06257v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal%20Discovery%20from%20Conditionally%20Stationary%20Time%20Series&entry.906535625=Carles%20Balsells-Rodas%20and%20Xavier%20Sumba%20and%20Tanmayee%20Narendra%20and%20Ruibo%20Tu%20and%20Gabriele%20Schweikert%20and%20Hedvig%20Kjellstrom%20and%20Yingzhen%20Li&entry.1292438233=%20%20Causal%20discovery%2C%20i.e.%2C%20inferring%20underlying%20causal%20relationships%20from%0Aobservational%20data%2C%20is%20highly%20challenging%20for%20AI%20systems.%20In%20a%20time%20series%0Amodeling%20context%2C%20traditional%20causal%20discovery%20methods%20mainly%20consider%0Aconstrained%20scenarios%20with%20fully%20observed%20variables%20and/or%20data%20from%20stationary%0Atime-series.%20We%20develop%20a%20causal%20discovery%20approach%20to%20handle%20a%20wide%20class%20of%0Anonstationary%20time%20series%20that%20are%20conditionally%20stationary%2C%20where%20the%0Anonstationary%20behaviour%20is%20modeled%20as%20stationarity%20conditioned%20on%20a%20set%20of%0Alatent%20state%20variables.%20Named%20State-Dependent%20Causal%20Inference%20%28SDCI%29%2C%20our%0Aapproach%20is%20able%20to%20recover%20the%20underlying%20causal%20dependencies%2C%20with%20provable%0Aidentifiablity%20for%20the%20state-dependent%20causal%20structures.%20Empirical%20experiments%0Aon%20nonlinear%20particle%20interaction%20data%20and%20gene%20regulatory%20networks%20demonstrate%0ASDCI%27s%20superior%20performance%20over%20baseline%20causal%20discovery%20methods.%20Improved%0Aresults%20over%20non-causal%20RNNs%20on%20modeling%20NBA%20player%20movements%20demonstrate%20the%0Apotential%20of%20our%20method%20and%20motivate%20the%20use%20of%20causality-driven%20methods%20for%0Aforecasting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2110.06257v3&entry.124074799=Read"},
{"title": "Distillation Scaling Laws", "author": "Dan Busbridge and Amitis Shidani and Floris Weers and Jason Ramapuram and Etai Littwin and Russ Webb", "abstract": "  We provide a distillation scaling law that estimates distilled model\nperformance based on a compute budget and its allocation between the student\nand teacher. Our findings reduce the risks associated with using distillation\nat scale; compute allocation for both the teacher and student models can now be\ndone to maximize student performance. We provide compute optimal distillation\nrecipes for when 1) a teacher exists, or 2) a teacher needs training. If many\nstudents are to be distilled, or a teacher already exists, distillation\noutperforms supervised pretraining until a compute level which grows\npredictably with student size. If one student is to be distilled and a teacher\nalso needs training, supervised learning should be done instead. Additionally,\nwe provide insights across our large scale study of distillation, which\nincrease our understanding of distillation and inform experimental design.\n", "link": "http://arxiv.org/abs/2502.08606v1", "date": "2025-02-12", "relevancy": 1.3448, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5193}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4368}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distillation%20Scaling%20Laws&body=Title%3A%20Distillation%20Scaling%20Laws%0AAuthor%3A%20Dan%20Busbridge%20and%20Amitis%20Shidani%20and%20Floris%20Weers%20and%20Jason%20Ramapuram%20and%20Etai%20Littwin%20and%20Russ%20Webb%0AAbstract%3A%20%20%20We%20provide%20a%20distillation%20scaling%20law%20that%20estimates%20distilled%20model%0Aperformance%20based%20on%20a%20compute%20budget%20and%20its%20allocation%20between%20the%20student%0Aand%20teacher.%20Our%20findings%20reduce%20the%20risks%20associated%20with%20using%20distillation%0Aat%20scale%3B%20compute%20allocation%20for%20both%20the%20teacher%20and%20student%20models%20can%20now%20be%0Adone%20to%20maximize%20student%20performance.%20We%20provide%20compute%20optimal%20distillation%0Arecipes%20for%20when%201%29%20a%20teacher%20exists%2C%20or%202%29%20a%20teacher%20needs%20training.%20If%20many%0Astudents%20are%20to%20be%20distilled%2C%20or%20a%20teacher%20already%20exists%2C%20distillation%0Aoutperforms%20supervised%20pretraining%20until%20a%20compute%20level%20which%20grows%0Apredictably%20with%20student%20size.%20If%20one%20student%20is%20to%20be%20distilled%20and%20a%20teacher%0Aalso%20needs%20training%2C%20supervised%20learning%20should%20be%20done%20instead.%20Additionally%2C%0Awe%20provide%20insights%20across%20our%20large%20scale%20study%20of%20distillation%2C%20which%0Aincrease%20our%20understanding%20of%20distillation%20and%20inform%20experimental%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08606v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistillation%2520Scaling%2520Laws%26entry.906535625%3DDan%2520Busbridge%2520and%2520Amitis%2520Shidani%2520and%2520Floris%2520Weers%2520and%2520Jason%2520Ramapuram%2520and%2520Etai%2520Littwin%2520and%2520Russ%2520Webb%26entry.1292438233%3D%2520%2520We%2520provide%2520a%2520distillation%2520scaling%2520law%2520that%2520estimates%2520distilled%2520model%250Aperformance%2520based%2520on%2520a%2520compute%2520budget%2520and%2520its%2520allocation%2520between%2520the%2520student%250Aand%2520teacher.%2520Our%2520findings%2520reduce%2520the%2520risks%2520associated%2520with%2520using%2520distillation%250Aat%2520scale%253B%2520compute%2520allocation%2520for%2520both%2520the%2520teacher%2520and%2520student%2520models%2520can%2520now%2520be%250Adone%2520to%2520maximize%2520student%2520performance.%2520We%2520provide%2520compute%2520optimal%2520distillation%250Arecipes%2520for%2520when%25201%2529%2520a%2520teacher%2520exists%252C%2520or%25202%2529%2520a%2520teacher%2520needs%2520training.%2520If%2520many%250Astudents%2520are%2520to%2520be%2520distilled%252C%2520or%2520a%2520teacher%2520already%2520exists%252C%2520distillation%250Aoutperforms%2520supervised%2520pretraining%2520until%2520a%2520compute%2520level%2520which%2520grows%250Apredictably%2520with%2520student%2520size.%2520If%2520one%2520student%2520is%2520to%2520be%2520distilled%2520and%2520a%2520teacher%250Aalso%2520needs%2520training%252C%2520supervised%2520learning%2520should%2520be%2520done%2520instead.%2520Additionally%252C%250Awe%2520provide%2520insights%2520across%2520our%2520large%2520scale%2520study%2520of%2520distillation%252C%2520which%250Aincrease%2520our%2520understanding%2520of%2520distillation%2520and%2520inform%2520experimental%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08606v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distillation%20Scaling%20Laws&entry.906535625=Dan%20Busbridge%20and%20Amitis%20Shidani%20and%20Floris%20Weers%20and%20Jason%20Ramapuram%20and%20Etai%20Littwin%20and%20Russ%20Webb&entry.1292438233=%20%20We%20provide%20a%20distillation%20scaling%20law%20that%20estimates%20distilled%20model%0Aperformance%20based%20on%20a%20compute%20budget%20and%20its%20allocation%20between%20the%20student%0Aand%20teacher.%20Our%20findings%20reduce%20the%20risks%20associated%20with%20using%20distillation%0Aat%20scale%3B%20compute%20allocation%20for%20both%20the%20teacher%20and%20student%20models%20can%20now%20be%0Adone%20to%20maximize%20student%20performance.%20We%20provide%20compute%20optimal%20distillation%0Arecipes%20for%20when%201%29%20a%20teacher%20exists%2C%20or%202%29%20a%20teacher%20needs%20training.%20If%20many%0Astudents%20are%20to%20be%20distilled%2C%20or%20a%20teacher%20already%20exists%2C%20distillation%0Aoutperforms%20supervised%20pretraining%20until%20a%20compute%20level%20which%20grows%0Apredictably%20with%20student%20size.%20If%20one%20student%20is%20to%20be%20distilled%20and%20a%20teacher%0Aalso%20needs%20training%2C%20supervised%20learning%20should%20be%20done%20instead.%20Additionally%2C%0Awe%20provide%20insights%20across%20our%20large%20scale%20study%20of%20distillation%2C%20which%0Aincrease%20our%20understanding%20of%20distillation%20and%20inform%20experimental%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08606v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


