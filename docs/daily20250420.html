<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="#"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GSAC: Leveraging Gaussian Splatting for Photorealistic Avatar Creation\n  with Unity Integration", "author": "Rendong Zhang and Alexandra Watkins and Nilanjan Sarkar", "abstract": "  Photorealistic avatars have become essential for immersive applications in\nvirtual reality (VR) and augmented reality (AR), enabling lifelike interactions\nin areas such as training simulations, telemedicine, and virtual collaboration.\nThese avatars bridge the gap between the physical and digital worlds, improving\nthe user experience through realistic human representation. However, existing\navatar creation techniques face significant challenges, including high costs,\nlong creation times, and limited utility in virtual applications. Manual\nmethods, such as MetaHuman, require extensive time and expertise, while\nautomatic approaches, such as NeRF-based pipelines often lack efficiency,\ndetailed facial expression fidelity, and are unable to be rendered at a speed\nsufficent for real-time applications. By involving several cutting-edge modern\ntechniques, we introduce an end-to-end 3D Gaussian Splatting (3DGS) avatar\ncreation pipeline that leverages monocular video input to create a scalable and\nefficient photorealistic avatar directly compatible with the Unity game engine.\nOur pipeline incorporates a novel Gaussian splatting technique with customized\npreprocessing that enables the user of \"in the wild\" monocular video capture,\ndetailed facial expression reconstruction and embedding within a fully rigged\navatar model. Additionally, we present a Unity-integrated Gaussian Splatting\nAvatar Editor, offering a user-friendly environment for VR/AR application\ndevelopment. Experimental results validate the effectiveness of our\npreprocessing pipeline in standardizing custom data for 3DGS training and\ndemonstrate the versatility of Gaussian avatars in Unity, highlighting the\nscalability and practicality of our approach.\n", "link": "http://arxiv.org/abs/2504.12999v1", "date": "2025-04-17", "relevancy": 3.5381, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7139}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7139}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GSAC%3A%20Leveraging%20Gaussian%20Splatting%20for%20Photorealistic%20Avatar%20Creation%0A%20%20with%20Unity%20Integration&body=Title%3A%20GSAC%3A%20Leveraging%20Gaussian%20Splatting%20for%20Photorealistic%20Avatar%20Creation%0A%20%20with%20Unity%20Integration%0AAuthor%3A%20Rendong%20Zhang%20and%20Alexandra%20Watkins%20and%20Nilanjan%20Sarkar%0AAbstract%3A%20%20%20Photorealistic%20avatars%20have%20become%20essential%20for%20immersive%20applications%20in%0Avirtual%20reality%20%28VR%29%20and%20augmented%20reality%20%28AR%29%2C%20enabling%20lifelike%20interactions%0Ain%20areas%20such%20as%20training%20simulations%2C%20telemedicine%2C%20and%20virtual%20collaboration.%0AThese%20avatars%20bridge%20the%20gap%20between%20the%20physical%20and%20digital%20worlds%2C%20improving%0Athe%20user%20experience%20through%20realistic%20human%20representation.%20However%2C%20existing%0Aavatar%20creation%20techniques%20face%20significant%20challenges%2C%20including%20high%20costs%2C%0Along%20creation%20times%2C%20and%20limited%20utility%20in%20virtual%20applications.%20Manual%0Amethods%2C%20such%20as%20MetaHuman%2C%20require%20extensive%20time%20and%20expertise%2C%20while%0Aautomatic%20approaches%2C%20such%20as%20NeRF-based%20pipelines%20often%20lack%20efficiency%2C%0Adetailed%20facial%20expression%20fidelity%2C%20and%20are%20unable%20to%20be%20rendered%20at%20a%20speed%0Asufficent%20for%20real-time%20applications.%20By%20involving%20several%20cutting-edge%20modern%0Atechniques%2C%20we%20introduce%20an%20end-to-end%203D%20Gaussian%20Splatting%20%283DGS%29%20avatar%0Acreation%20pipeline%20that%20leverages%20monocular%20video%20input%20to%20create%20a%20scalable%20and%0Aefficient%20photorealistic%20avatar%20directly%20compatible%20with%20the%20Unity%20game%20engine.%0AOur%20pipeline%20incorporates%20a%20novel%20Gaussian%20splatting%20technique%20with%20customized%0Apreprocessing%20that%20enables%20the%20user%20of%20%22in%20the%20wild%22%20monocular%20video%20capture%2C%0Adetailed%20facial%20expression%20reconstruction%20and%20embedding%20within%20a%20fully%20rigged%0Aavatar%20model.%20Additionally%2C%20we%20present%20a%20Unity-integrated%20Gaussian%20Splatting%0AAvatar%20Editor%2C%20offering%20a%20user-friendly%20environment%20for%20VR/AR%20application%0Adevelopment.%20Experimental%20results%20validate%20the%20effectiveness%20of%20our%0Apreprocessing%20pipeline%20in%20standardizing%20custom%20data%20for%203DGS%20training%20and%0Ademonstrate%20the%20versatility%20of%20Gaussian%20avatars%20in%20Unity%2C%20highlighting%20the%0Ascalability%20and%20practicality%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12999v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGSAC%253A%2520Leveraging%2520Gaussian%2520Splatting%2520for%2520Photorealistic%2520Avatar%2520Creation%250A%2520%2520with%2520Unity%2520Integration%26entry.906535625%3DRendong%2520Zhang%2520and%2520Alexandra%2520Watkins%2520and%2520Nilanjan%2520Sarkar%26entry.1292438233%3D%2520%2520Photorealistic%2520avatars%2520have%2520become%2520essential%2520for%2520immersive%2520applications%2520in%250Avirtual%2520reality%2520%2528VR%2529%2520and%2520augmented%2520reality%2520%2528AR%2529%252C%2520enabling%2520lifelike%2520interactions%250Ain%2520areas%2520such%2520as%2520training%2520simulations%252C%2520telemedicine%252C%2520and%2520virtual%2520collaboration.%250AThese%2520avatars%2520bridge%2520the%2520gap%2520between%2520the%2520physical%2520and%2520digital%2520worlds%252C%2520improving%250Athe%2520user%2520experience%2520through%2520realistic%2520human%2520representation.%2520However%252C%2520existing%250Aavatar%2520creation%2520techniques%2520face%2520significant%2520challenges%252C%2520including%2520high%2520costs%252C%250Along%2520creation%2520times%252C%2520and%2520limited%2520utility%2520in%2520virtual%2520applications.%2520Manual%250Amethods%252C%2520such%2520as%2520MetaHuman%252C%2520require%2520extensive%2520time%2520and%2520expertise%252C%2520while%250Aautomatic%2520approaches%252C%2520such%2520as%2520NeRF-based%2520pipelines%2520often%2520lack%2520efficiency%252C%250Adetailed%2520facial%2520expression%2520fidelity%252C%2520and%2520are%2520unable%2520to%2520be%2520rendered%2520at%2520a%2520speed%250Asufficent%2520for%2520real-time%2520applications.%2520By%2520involving%2520several%2520cutting-edge%2520modern%250Atechniques%252C%2520we%2520introduce%2520an%2520end-to-end%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520avatar%250Acreation%2520pipeline%2520that%2520leverages%2520monocular%2520video%2520input%2520to%2520create%2520a%2520scalable%2520and%250Aefficient%2520photorealistic%2520avatar%2520directly%2520compatible%2520with%2520the%2520Unity%2520game%2520engine.%250AOur%2520pipeline%2520incorporates%2520a%2520novel%2520Gaussian%2520splatting%2520technique%2520with%2520customized%250Apreprocessing%2520that%2520enables%2520the%2520user%2520of%2520%2522in%2520the%2520wild%2522%2520monocular%2520video%2520capture%252C%250Adetailed%2520facial%2520expression%2520reconstruction%2520and%2520embedding%2520within%2520a%2520fully%2520rigged%250Aavatar%2520model.%2520Additionally%252C%2520we%2520present%2520a%2520Unity-integrated%2520Gaussian%2520Splatting%250AAvatar%2520Editor%252C%2520offering%2520a%2520user-friendly%2520environment%2520for%2520VR/AR%2520application%250Adevelopment.%2520Experimental%2520results%2520validate%2520the%2520effectiveness%2520of%2520our%250Apreprocessing%2520pipeline%2520in%2520standardizing%2520custom%2520data%2520for%25203DGS%2520training%2520and%250Ademonstrate%2520the%2520versatility%2520of%2520Gaussian%2520avatars%2520in%2520Unity%252C%2520highlighting%2520the%250Ascalability%2520and%2520practicality%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12999v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GSAC%3A%20Leveraging%20Gaussian%20Splatting%20for%20Photorealistic%20Avatar%20Creation%0A%20%20with%20Unity%20Integration&entry.906535625=Rendong%20Zhang%20and%20Alexandra%20Watkins%20and%20Nilanjan%20Sarkar&entry.1292438233=%20%20Photorealistic%20avatars%20have%20become%20essential%20for%20immersive%20applications%20in%0Avirtual%20reality%20%28VR%29%20and%20augmented%20reality%20%28AR%29%2C%20enabling%20lifelike%20interactions%0Ain%20areas%20such%20as%20training%20simulations%2C%20telemedicine%2C%20and%20virtual%20collaboration.%0AThese%20avatars%20bridge%20the%20gap%20between%20the%20physical%20and%20digital%20worlds%2C%20improving%0Athe%20user%20experience%20through%20realistic%20human%20representation.%20However%2C%20existing%0Aavatar%20creation%20techniques%20face%20significant%20challenges%2C%20including%20high%20costs%2C%0Along%20creation%20times%2C%20and%20limited%20utility%20in%20virtual%20applications.%20Manual%0Amethods%2C%20such%20as%20MetaHuman%2C%20require%20extensive%20time%20and%20expertise%2C%20while%0Aautomatic%20approaches%2C%20such%20as%20NeRF-based%20pipelines%20often%20lack%20efficiency%2C%0Adetailed%20facial%20expression%20fidelity%2C%20and%20are%20unable%20to%20be%20rendered%20at%20a%20speed%0Asufficent%20for%20real-time%20applications.%20By%20involving%20several%20cutting-edge%20modern%0Atechniques%2C%20we%20introduce%20an%20end-to-end%203D%20Gaussian%20Splatting%20%283DGS%29%20avatar%0Acreation%20pipeline%20that%20leverages%20monocular%20video%20input%20to%20create%20a%20scalable%20and%0Aefficient%20photorealistic%20avatar%20directly%20compatible%20with%20the%20Unity%20game%20engine.%0AOur%20pipeline%20incorporates%20a%20novel%20Gaussian%20splatting%20technique%20with%20customized%0Apreprocessing%20that%20enables%20the%20user%20of%20%22in%20the%20wild%22%20monocular%20video%20capture%2C%0Adetailed%20facial%20expression%20reconstruction%20and%20embedding%20within%20a%20fully%20rigged%0Aavatar%20model.%20Additionally%2C%20we%20present%20a%20Unity-integrated%20Gaussian%20Splatting%0AAvatar%20Editor%2C%20offering%20a%20user-friendly%20environment%20for%20VR/AR%20application%0Adevelopment.%20Experimental%20results%20validate%20the%20effectiveness%20of%20our%0Apreprocessing%20pipeline%20in%20standardizing%20custom%20data%20for%203DGS%20training%20and%0Ademonstrate%20the%20versatility%20of%20Gaussian%20avatars%20in%20Unity%2C%20highlighting%20the%0Ascalability%20and%20practicality%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12999v1&entry.124074799=Read"},
{"title": "Real-time High-fidelity Gaussian Human Avatars with Position-based\n  Interpolation of Spatially Distributed MLPs", "author": "Youyi Zhan and Tianjia Shao and Yin Yang and Kun Zhou", "abstract": "  Many works have succeeded in reconstructing Gaussian human avatars from\nmulti-view videos. However, they either struggle to capture pose-dependent\nappearance details with a single MLP, or rely on a computationally intensive\nneural network to reconstruct high-fidelity appearance but with rendering\nperformance degraded to non-real-time. We propose a novel Gaussian human avatar\nrepresentation that can reconstruct high-fidelity pose-dependence appearance\nwith details and meanwhile can be rendered in real time. Our Gaussian avatar is\nempowered by spatially distributed MLPs which are explicitly located on\ndifferent positions on human body. The parameters stored in each Gaussian are\nobtained by interpolating from the outputs of its nearby MLPs based on their\ndistances. To avoid undesired smooth Gaussian property changing during\ninterpolation, for each Gaussian we define a set of Gaussian offset basis, and\na linear combination of basis represents the Gaussian property offsets relative\nto the neutral properties. Then we propose to let the MLPs output a set of\ncoefficients corresponding to the basis. In this way, although Gaussian\ncoefficients are derived from interpolation and change smoothly, the Gaussian\noffset basis is learned freely without constraints. The smoothly varying\ncoefficients combined with freely learned basis can still produce distinctly\ndifferent Gaussian property offsets, allowing the ability to learn\nhigh-frequency spatial signals. We further use control points to constrain the\nGaussians distributed on a surface layer rather than allowing them to be\nirregularly distributed inside the body, to help the human avatar generalize\nbetter when animated under novel poses. Compared to the state-of-the-art\nmethod, our method achieves better appearance quality with finer details while\nthe rendering speed is significantly faster under novel views and novel poses.\n", "link": "http://arxiv.org/abs/2504.12909v1", "date": "2025-04-17", "relevancy": 3.4695, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7181}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7181}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-time%20High-fidelity%20Gaussian%20Human%20Avatars%20with%20Position-based%0A%20%20Interpolation%20of%20Spatially%20Distributed%20MLPs&body=Title%3A%20Real-time%20High-fidelity%20Gaussian%20Human%20Avatars%20with%20Position-based%0A%20%20Interpolation%20of%20Spatially%20Distributed%20MLPs%0AAuthor%3A%20Youyi%20Zhan%20and%20Tianjia%20Shao%20and%20Yin%20Yang%20and%20Kun%20Zhou%0AAbstract%3A%20%20%20Many%20works%20have%20succeeded%20in%20reconstructing%20Gaussian%20human%20avatars%20from%0Amulti-view%20videos.%20However%2C%20they%20either%20struggle%20to%20capture%20pose-dependent%0Aappearance%20details%20with%20a%20single%20MLP%2C%20or%20rely%20on%20a%20computationally%20intensive%0Aneural%20network%20to%20reconstruct%20high-fidelity%20appearance%20but%20with%20rendering%0Aperformance%20degraded%20to%20non-real-time.%20We%20propose%20a%20novel%20Gaussian%20human%20avatar%0Arepresentation%20that%20can%20reconstruct%20high-fidelity%20pose-dependence%20appearance%0Awith%20details%20and%20meanwhile%20can%20be%20rendered%20in%20real%20time.%20Our%20Gaussian%20avatar%20is%0Aempowered%20by%20spatially%20distributed%20MLPs%20which%20are%20explicitly%20located%20on%0Adifferent%20positions%20on%20human%20body.%20The%20parameters%20stored%20in%20each%20Gaussian%20are%0Aobtained%20by%20interpolating%20from%20the%20outputs%20of%20its%20nearby%20MLPs%20based%20on%20their%0Adistances.%20To%20avoid%20undesired%20smooth%20Gaussian%20property%20changing%20during%0Ainterpolation%2C%20for%20each%20Gaussian%20we%20define%20a%20set%20of%20Gaussian%20offset%20basis%2C%20and%0Aa%20linear%20combination%20of%20basis%20represents%20the%20Gaussian%20property%20offsets%20relative%0Ato%20the%20neutral%20properties.%20Then%20we%20propose%20to%20let%20the%20MLPs%20output%20a%20set%20of%0Acoefficients%20corresponding%20to%20the%20basis.%20In%20this%20way%2C%20although%20Gaussian%0Acoefficients%20are%20derived%20from%20interpolation%20and%20change%20smoothly%2C%20the%20Gaussian%0Aoffset%20basis%20is%20learned%20freely%20without%20constraints.%20The%20smoothly%20varying%0Acoefficients%20combined%20with%20freely%20learned%20basis%20can%20still%20produce%20distinctly%0Adifferent%20Gaussian%20property%20offsets%2C%20allowing%20the%20ability%20to%20learn%0Ahigh-frequency%20spatial%20signals.%20We%20further%20use%20control%20points%20to%20constrain%20the%0AGaussians%20distributed%20on%20a%20surface%20layer%20rather%20than%20allowing%20them%20to%20be%0Airregularly%20distributed%20inside%20the%20body%2C%20to%20help%20the%20human%20avatar%20generalize%0Abetter%20when%20animated%20under%20novel%20poses.%20Compared%20to%20the%20state-of-the-art%0Amethod%2C%20our%20method%20achieves%20better%20appearance%20quality%20with%20finer%20details%20while%0Athe%20rendering%20speed%20is%20significantly%20faster%20under%20novel%20views%20and%20novel%20poses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12909v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-time%2520High-fidelity%2520Gaussian%2520Human%2520Avatars%2520with%2520Position-based%250A%2520%2520Interpolation%2520of%2520Spatially%2520Distributed%2520MLPs%26entry.906535625%3DYouyi%2520Zhan%2520and%2520Tianjia%2520Shao%2520and%2520Yin%2520Yang%2520and%2520Kun%2520Zhou%26entry.1292438233%3D%2520%2520Many%2520works%2520have%2520succeeded%2520in%2520reconstructing%2520Gaussian%2520human%2520avatars%2520from%250Amulti-view%2520videos.%2520However%252C%2520they%2520either%2520struggle%2520to%2520capture%2520pose-dependent%250Aappearance%2520details%2520with%2520a%2520single%2520MLP%252C%2520or%2520rely%2520on%2520a%2520computationally%2520intensive%250Aneural%2520network%2520to%2520reconstruct%2520high-fidelity%2520appearance%2520but%2520with%2520rendering%250Aperformance%2520degraded%2520to%2520non-real-time.%2520We%2520propose%2520a%2520novel%2520Gaussian%2520human%2520avatar%250Arepresentation%2520that%2520can%2520reconstruct%2520high-fidelity%2520pose-dependence%2520appearance%250Awith%2520details%2520and%2520meanwhile%2520can%2520be%2520rendered%2520in%2520real%2520time.%2520Our%2520Gaussian%2520avatar%2520is%250Aempowered%2520by%2520spatially%2520distributed%2520MLPs%2520which%2520are%2520explicitly%2520located%2520on%250Adifferent%2520positions%2520on%2520human%2520body.%2520The%2520parameters%2520stored%2520in%2520each%2520Gaussian%2520are%250Aobtained%2520by%2520interpolating%2520from%2520the%2520outputs%2520of%2520its%2520nearby%2520MLPs%2520based%2520on%2520their%250Adistances.%2520To%2520avoid%2520undesired%2520smooth%2520Gaussian%2520property%2520changing%2520during%250Ainterpolation%252C%2520for%2520each%2520Gaussian%2520we%2520define%2520a%2520set%2520of%2520Gaussian%2520offset%2520basis%252C%2520and%250Aa%2520linear%2520combination%2520of%2520basis%2520represents%2520the%2520Gaussian%2520property%2520offsets%2520relative%250Ato%2520the%2520neutral%2520properties.%2520Then%2520we%2520propose%2520to%2520let%2520the%2520MLPs%2520output%2520a%2520set%2520of%250Acoefficients%2520corresponding%2520to%2520the%2520basis.%2520In%2520this%2520way%252C%2520although%2520Gaussian%250Acoefficients%2520are%2520derived%2520from%2520interpolation%2520and%2520change%2520smoothly%252C%2520the%2520Gaussian%250Aoffset%2520basis%2520is%2520learned%2520freely%2520without%2520constraints.%2520The%2520smoothly%2520varying%250Acoefficients%2520combined%2520with%2520freely%2520learned%2520basis%2520can%2520still%2520produce%2520distinctly%250Adifferent%2520Gaussian%2520property%2520offsets%252C%2520allowing%2520the%2520ability%2520to%2520learn%250Ahigh-frequency%2520spatial%2520signals.%2520We%2520further%2520use%2520control%2520points%2520to%2520constrain%2520the%250AGaussians%2520distributed%2520on%2520a%2520surface%2520layer%2520rather%2520than%2520allowing%2520them%2520to%2520be%250Airregularly%2520distributed%2520inside%2520the%2520body%252C%2520to%2520help%2520the%2520human%2520avatar%2520generalize%250Abetter%2520when%2520animated%2520under%2520novel%2520poses.%2520Compared%2520to%2520the%2520state-of-the-art%250Amethod%252C%2520our%2520method%2520achieves%2520better%2520appearance%2520quality%2520with%2520finer%2520details%2520while%250Athe%2520rendering%2520speed%2520is%2520significantly%2520faster%2520under%2520novel%2520views%2520and%2520novel%2520poses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12909v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-time%20High-fidelity%20Gaussian%20Human%20Avatars%20with%20Position-based%0A%20%20Interpolation%20of%20Spatially%20Distributed%20MLPs&entry.906535625=Youyi%20Zhan%20and%20Tianjia%20Shao%20and%20Yin%20Yang%20and%20Kun%20Zhou&entry.1292438233=%20%20Many%20works%20have%20succeeded%20in%20reconstructing%20Gaussian%20human%20avatars%20from%0Amulti-view%20videos.%20However%2C%20they%20either%20struggle%20to%20capture%20pose-dependent%0Aappearance%20details%20with%20a%20single%20MLP%2C%20or%20rely%20on%20a%20computationally%20intensive%0Aneural%20network%20to%20reconstruct%20high-fidelity%20appearance%20but%20with%20rendering%0Aperformance%20degraded%20to%20non-real-time.%20We%20propose%20a%20novel%20Gaussian%20human%20avatar%0Arepresentation%20that%20can%20reconstruct%20high-fidelity%20pose-dependence%20appearance%0Awith%20details%20and%20meanwhile%20can%20be%20rendered%20in%20real%20time.%20Our%20Gaussian%20avatar%20is%0Aempowered%20by%20spatially%20distributed%20MLPs%20which%20are%20explicitly%20located%20on%0Adifferent%20positions%20on%20human%20body.%20The%20parameters%20stored%20in%20each%20Gaussian%20are%0Aobtained%20by%20interpolating%20from%20the%20outputs%20of%20its%20nearby%20MLPs%20based%20on%20their%0Adistances.%20To%20avoid%20undesired%20smooth%20Gaussian%20property%20changing%20during%0Ainterpolation%2C%20for%20each%20Gaussian%20we%20define%20a%20set%20of%20Gaussian%20offset%20basis%2C%20and%0Aa%20linear%20combination%20of%20basis%20represents%20the%20Gaussian%20property%20offsets%20relative%0Ato%20the%20neutral%20properties.%20Then%20we%20propose%20to%20let%20the%20MLPs%20output%20a%20set%20of%0Acoefficients%20corresponding%20to%20the%20basis.%20In%20this%20way%2C%20although%20Gaussian%0Acoefficients%20are%20derived%20from%20interpolation%20and%20change%20smoothly%2C%20the%20Gaussian%0Aoffset%20basis%20is%20learned%20freely%20without%20constraints.%20The%20smoothly%20varying%0Acoefficients%20combined%20with%20freely%20learned%20basis%20can%20still%20produce%20distinctly%0Adifferent%20Gaussian%20property%20offsets%2C%20allowing%20the%20ability%20to%20learn%0Ahigh-frequency%20spatial%20signals.%20We%20further%20use%20control%20points%20to%20constrain%20the%0AGaussians%20distributed%20on%20a%20surface%20layer%20rather%20than%20allowing%20them%20to%20be%0Airregularly%20distributed%20inside%20the%20body%2C%20to%20help%20the%20human%20avatar%20generalize%0Abetter%20when%20animated%20under%20novel%20poses.%20Compared%20to%20the%20state-of-the-art%0Amethod%2C%20our%20method%20achieves%20better%20appearance%20quality%20with%20finer%20details%20while%0Athe%20rendering%20speed%20is%20significantly%20faster%20under%20novel%20views%20and%20novel%20poses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12909v1&entry.124074799=Read"},
{"title": "TSGS: Improving Gaussian Splatting for Transparent Surface\n  Reconstruction via Normal and De-lighting Priors", "author": "Mingwei Li and Pu Pang and Hehe Fan and Hua Huang and Yi Yang", "abstract": "  Reconstructing transparent surfaces is essential for tasks such as robotic\nmanipulation in labs, yet it poses a significant challenge for 3D\nreconstruction techniques like 3D Gaussian Splatting (3DGS). These methods\noften encounter a transparency-depth dilemma, where the pursuit of\nphotorealistic rendering through standard $\\alpha$-blending undermines\ngeometric precision, resulting in considerable depth estimation errors for\ntransparent materials. To address this issue, we introduce Transparent Surface\nGaussian Splatting (TSGS), a new framework that separates geometry learning\nfrom appearance refinement. In the geometry learning stage, TSGS focuses on\ngeometry by using specular-suppressed inputs to accurately represent surfaces.\nIn the second stage, TSGS improves visual fidelity through anisotropic specular\nmodeling, crucially maintaining the established opacity to ensure geometric\naccuracy. To enhance depth inference, TSGS employs a first-surface depth\nextraction method. This technique uses a sliding window over $\\alpha$-blending\nweights to pinpoint the most likely surface location and calculates a robust\nweighted average depth. To evaluate the transparent surface reconstruction task\nunder realistic conditions, we collect a TransLab dataset that includes complex\ntransparent laboratory glassware. Extensive experiments on TransLab show that\nTSGS achieves accurate geometric reconstruction and realistic rendering of\ntransparent objects simultaneously within the efficient 3DGS framework.\nSpecifically, TSGS significantly surpasses current leading methods, achieving a\n37.3% reduction in chamfer distance and an 8.0% improvement in F1 score\ncompared to the top baseline. The code and dataset will be released at\nhttps://longxiang-ai.github.io/TSGS/.\n", "link": "http://arxiv.org/abs/2504.12799v1", "date": "2025-04-17", "relevancy": 3.4681, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7418}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7058}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TSGS%3A%20Improving%20Gaussian%20Splatting%20for%20Transparent%20Surface%0A%20%20Reconstruction%20via%20Normal%20and%20De-lighting%20Priors&body=Title%3A%20TSGS%3A%20Improving%20Gaussian%20Splatting%20for%20Transparent%20Surface%0A%20%20Reconstruction%20via%20Normal%20and%20De-lighting%20Priors%0AAuthor%3A%20Mingwei%20Li%20and%20Pu%20Pang%20and%20Hehe%20Fan%20and%20Hua%20Huang%20and%20Yi%20Yang%0AAbstract%3A%20%20%20Reconstructing%20transparent%20surfaces%20is%20essential%20for%20tasks%20such%20as%20robotic%0Amanipulation%20in%20labs%2C%20yet%20it%20poses%20a%20significant%20challenge%20for%203D%0Areconstruction%20techniques%20like%203D%20Gaussian%20Splatting%20%283DGS%29.%20These%20methods%0Aoften%20encounter%20a%20transparency-depth%20dilemma%2C%20where%20the%20pursuit%20of%0Aphotorealistic%20rendering%20through%20standard%20%24%5Calpha%24-blending%20undermines%0Ageometric%20precision%2C%20resulting%20in%20considerable%20depth%20estimation%20errors%20for%0Atransparent%20materials.%20To%20address%20this%20issue%2C%20we%20introduce%20Transparent%20Surface%0AGaussian%20Splatting%20%28TSGS%29%2C%20a%20new%20framework%20that%20separates%20geometry%20learning%0Afrom%20appearance%20refinement.%20In%20the%20geometry%20learning%20stage%2C%20TSGS%20focuses%20on%0Ageometry%20by%20using%20specular-suppressed%20inputs%20to%20accurately%20represent%20surfaces.%0AIn%20the%20second%20stage%2C%20TSGS%20improves%20visual%20fidelity%20through%20anisotropic%20specular%0Amodeling%2C%20crucially%20maintaining%20the%20established%20opacity%20to%20ensure%20geometric%0Aaccuracy.%20To%20enhance%20depth%20inference%2C%20TSGS%20employs%20a%20first-surface%20depth%0Aextraction%20method.%20This%20technique%20uses%20a%20sliding%20window%20over%20%24%5Calpha%24-blending%0Aweights%20to%20pinpoint%20the%20most%20likely%20surface%20location%20and%20calculates%20a%20robust%0Aweighted%20average%20depth.%20To%20evaluate%20the%20transparent%20surface%20reconstruction%20task%0Aunder%20realistic%20conditions%2C%20we%20collect%20a%20TransLab%20dataset%20that%20includes%20complex%0Atransparent%20laboratory%20glassware.%20Extensive%20experiments%20on%20TransLab%20show%20that%0ATSGS%20achieves%20accurate%20geometric%20reconstruction%20and%20realistic%20rendering%20of%0Atransparent%20objects%20simultaneously%20within%20the%20efficient%203DGS%20framework.%0ASpecifically%2C%20TSGS%20significantly%20surpasses%20current%20leading%20methods%2C%20achieving%20a%0A37.3%25%20reduction%20in%20chamfer%20distance%20and%20an%208.0%25%20improvement%20in%20F1%20score%0Acompared%20to%20the%20top%20baseline.%20The%20code%20and%20dataset%20will%20be%20released%20at%0Ahttps%3A//longxiang-ai.github.io/TSGS/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12799v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTSGS%253A%2520Improving%2520Gaussian%2520Splatting%2520for%2520Transparent%2520Surface%250A%2520%2520Reconstruction%2520via%2520Normal%2520and%2520De-lighting%2520Priors%26entry.906535625%3DMingwei%2520Li%2520and%2520Pu%2520Pang%2520and%2520Hehe%2520Fan%2520and%2520Hua%2520Huang%2520and%2520Yi%2520Yang%26entry.1292438233%3D%2520%2520Reconstructing%2520transparent%2520surfaces%2520is%2520essential%2520for%2520tasks%2520such%2520as%2520robotic%250Amanipulation%2520in%2520labs%252C%2520yet%2520it%2520poses%2520a%2520significant%2520challenge%2520for%25203D%250Areconstruction%2520techniques%2520like%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529.%2520These%2520methods%250Aoften%2520encounter%2520a%2520transparency-depth%2520dilemma%252C%2520where%2520the%2520pursuit%2520of%250Aphotorealistic%2520rendering%2520through%2520standard%2520%2524%255Calpha%2524-blending%2520undermines%250Ageometric%2520precision%252C%2520resulting%2520in%2520considerable%2520depth%2520estimation%2520errors%2520for%250Atransparent%2520materials.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520Transparent%2520Surface%250AGaussian%2520Splatting%2520%2528TSGS%2529%252C%2520a%2520new%2520framework%2520that%2520separates%2520geometry%2520learning%250Afrom%2520appearance%2520refinement.%2520In%2520the%2520geometry%2520learning%2520stage%252C%2520TSGS%2520focuses%2520on%250Ageometry%2520by%2520using%2520specular-suppressed%2520inputs%2520to%2520accurately%2520represent%2520surfaces.%250AIn%2520the%2520second%2520stage%252C%2520TSGS%2520improves%2520visual%2520fidelity%2520through%2520anisotropic%2520specular%250Amodeling%252C%2520crucially%2520maintaining%2520the%2520established%2520opacity%2520to%2520ensure%2520geometric%250Aaccuracy.%2520To%2520enhance%2520depth%2520inference%252C%2520TSGS%2520employs%2520a%2520first-surface%2520depth%250Aextraction%2520method.%2520This%2520technique%2520uses%2520a%2520sliding%2520window%2520over%2520%2524%255Calpha%2524-blending%250Aweights%2520to%2520pinpoint%2520the%2520most%2520likely%2520surface%2520location%2520and%2520calculates%2520a%2520robust%250Aweighted%2520average%2520depth.%2520To%2520evaluate%2520the%2520transparent%2520surface%2520reconstruction%2520task%250Aunder%2520realistic%2520conditions%252C%2520we%2520collect%2520a%2520TransLab%2520dataset%2520that%2520includes%2520complex%250Atransparent%2520laboratory%2520glassware.%2520Extensive%2520experiments%2520on%2520TransLab%2520show%2520that%250ATSGS%2520achieves%2520accurate%2520geometric%2520reconstruction%2520and%2520realistic%2520rendering%2520of%250Atransparent%2520objects%2520simultaneously%2520within%2520the%2520efficient%25203DGS%2520framework.%250ASpecifically%252C%2520TSGS%2520significantly%2520surpasses%2520current%2520leading%2520methods%252C%2520achieving%2520a%250A37.3%2525%2520reduction%2520in%2520chamfer%2520distance%2520and%2520an%25208.0%2525%2520improvement%2520in%2520F1%2520score%250Acompared%2520to%2520the%2520top%2520baseline.%2520The%2520code%2520and%2520dataset%2520will%2520be%2520released%2520at%250Ahttps%253A//longxiang-ai.github.io/TSGS/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12799v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TSGS%3A%20Improving%20Gaussian%20Splatting%20for%20Transparent%20Surface%0A%20%20Reconstruction%20via%20Normal%20and%20De-lighting%20Priors&entry.906535625=Mingwei%20Li%20and%20Pu%20Pang%20and%20Hehe%20Fan%20and%20Hua%20Huang%20and%20Yi%20Yang&entry.1292438233=%20%20Reconstructing%20transparent%20surfaces%20is%20essential%20for%20tasks%20such%20as%20robotic%0Amanipulation%20in%20labs%2C%20yet%20it%20poses%20a%20significant%20challenge%20for%203D%0Areconstruction%20techniques%20like%203D%20Gaussian%20Splatting%20%283DGS%29.%20These%20methods%0Aoften%20encounter%20a%20transparency-depth%20dilemma%2C%20where%20the%20pursuit%20of%0Aphotorealistic%20rendering%20through%20standard%20%24%5Calpha%24-blending%20undermines%0Ageometric%20precision%2C%20resulting%20in%20considerable%20depth%20estimation%20errors%20for%0Atransparent%20materials.%20To%20address%20this%20issue%2C%20we%20introduce%20Transparent%20Surface%0AGaussian%20Splatting%20%28TSGS%29%2C%20a%20new%20framework%20that%20separates%20geometry%20learning%0Afrom%20appearance%20refinement.%20In%20the%20geometry%20learning%20stage%2C%20TSGS%20focuses%20on%0Ageometry%20by%20using%20specular-suppressed%20inputs%20to%20accurately%20represent%20surfaces.%0AIn%20the%20second%20stage%2C%20TSGS%20improves%20visual%20fidelity%20through%20anisotropic%20specular%0Amodeling%2C%20crucially%20maintaining%20the%20established%20opacity%20to%20ensure%20geometric%0Aaccuracy.%20To%20enhance%20depth%20inference%2C%20TSGS%20employs%20a%20first-surface%20depth%0Aextraction%20method.%20This%20technique%20uses%20a%20sliding%20window%20over%20%24%5Calpha%24-blending%0Aweights%20to%20pinpoint%20the%20most%20likely%20surface%20location%20and%20calculates%20a%20robust%0Aweighted%20average%20depth.%20To%20evaluate%20the%20transparent%20surface%20reconstruction%20task%0Aunder%20realistic%20conditions%2C%20we%20collect%20a%20TransLab%20dataset%20that%20includes%20complex%0Atransparent%20laboratory%20glassware.%20Extensive%20experiments%20on%20TransLab%20show%20that%0ATSGS%20achieves%20accurate%20geometric%20reconstruction%20and%20realistic%20rendering%20of%0Atransparent%20objects%20simultaneously%20within%20the%20efficient%203DGS%20framework.%0ASpecifically%2C%20TSGS%20significantly%20surpasses%20current%20leading%20methods%2C%20achieving%20a%0A37.3%25%20reduction%20in%20chamfer%20distance%20and%20an%208.0%25%20improvement%20in%20F1%20score%0Acompared%20to%20the%20top%20baseline.%20The%20code%20and%20dataset%20will%20be%20released%20at%0Ahttps%3A//longxiang-ai.github.io/TSGS/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12799v1&entry.124074799=Read"},
{"title": "AAA-Gaussians: Anti-Aliased and Artifact-Free 3D Gaussian Rendering", "author": "Michael Steiner and Thomas K\u00f6hler and Lukas Radl and Felix Windisch and Dieter Schmalstieg and Markus Steinberger", "abstract": "  Although 3D Gaussian Splatting (3DGS) has revolutionized 3D reconstruction,\nit still faces challenges such as aliasing, projection artifacts, and view\ninconsistencies, primarily due to the simplification of treating splats as 2D\nentities. We argue that incorporating full 3D evaluation of Gaussians\nthroughout the 3DGS pipeline can effectively address these issues while\npreserving rasterization efficiency. Specifically, we introduce an adaptive 3D\nsmoothing filter to mitigate aliasing and present a stable view-space bounding\nmethod that eliminates popping artifacts when Gaussians extend beyond the view\nfrustum. Furthermore, we promote tile-based culling to 3D with screen-space\nplanes, accelerating rendering and reducing sorting costs for hierarchical\nrasterization. Our method achieves state-of-the-art quality on in-distribution\nevaluation sets and significantly outperforms other approaches for\nout-of-distribution views. Our qualitative evaluations further demonstrate the\neffective removal of aliasing, distortions, and popping artifacts, ensuring\nreal-time, artifact-free rendering.\n", "link": "http://arxiv.org/abs/2504.12811v1", "date": "2025-04-17", "relevancy": 3.4388, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7195}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6857}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AAA-Gaussians%3A%20Anti-Aliased%20and%20Artifact-Free%203D%20Gaussian%20Rendering&body=Title%3A%20AAA-Gaussians%3A%20Anti-Aliased%20and%20Artifact-Free%203D%20Gaussian%20Rendering%0AAuthor%3A%20Michael%20Steiner%20and%20Thomas%20K%C3%B6hler%20and%20Lukas%20Radl%20and%20Felix%20Windisch%20and%20Dieter%20Schmalstieg%20and%20Markus%20Steinberger%0AAbstract%3A%20%20%20Although%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20revolutionized%203D%20reconstruction%2C%0Ait%20still%20faces%20challenges%20such%20as%20aliasing%2C%20projection%20artifacts%2C%20and%20view%0Ainconsistencies%2C%20primarily%20due%20to%20the%20simplification%20of%20treating%20splats%20as%202D%0Aentities.%20We%20argue%20that%20incorporating%20full%203D%20evaluation%20of%20Gaussians%0Athroughout%20the%203DGS%20pipeline%20can%20effectively%20address%20these%20issues%20while%0Apreserving%20rasterization%20efficiency.%20Specifically%2C%20we%20introduce%20an%20adaptive%203D%0Asmoothing%20filter%20to%20mitigate%20aliasing%20and%20present%20a%20stable%20view-space%20bounding%0Amethod%20that%20eliminates%20popping%20artifacts%20when%20Gaussians%20extend%20beyond%20the%20view%0Afrustum.%20Furthermore%2C%20we%20promote%20tile-based%20culling%20to%203D%20with%20screen-space%0Aplanes%2C%20accelerating%20rendering%20and%20reducing%20sorting%20costs%20for%20hierarchical%0Arasterization.%20Our%20method%20achieves%20state-of-the-art%20quality%20on%20in-distribution%0Aevaluation%20sets%20and%20significantly%20outperforms%20other%20approaches%20for%0Aout-of-distribution%20views.%20Our%20qualitative%20evaluations%20further%20demonstrate%20the%0Aeffective%20removal%20of%20aliasing%2C%20distortions%2C%20and%20popping%20artifacts%2C%20ensuring%0Areal-time%2C%20artifact-free%20rendering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12811v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAAA-Gaussians%253A%2520Anti-Aliased%2520and%2520Artifact-Free%25203D%2520Gaussian%2520Rendering%26entry.906535625%3DMichael%2520Steiner%2520and%2520Thomas%2520K%25C3%25B6hler%2520and%2520Lukas%2520Radl%2520and%2520Felix%2520Windisch%2520and%2520Dieter%2520Schmalstieg%2520and%2520Markus%2520Steinberger%26entry.1292438233%3D%2520%2520Although%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520revolutionized%25203D%2520reconstruction%252C%250Ait%2520still%2520faces%2520challenges%2520such%2520as%2520aliasing%252C%2520projection%2520artifacts%252C%2520and%2520view%250Ainconsistencies%252C%2520primarily%2520due%2520to%2520the%2520simplification%2520of%2520treating%2520splats%2520as%25202D%250Aentities.%2520We%2520argue%2520that%2520incorporating%2520full%25203D%2520evaluation%2520of%2520Gaussians%250Athroughout%2520the%25203DGS%2520pipeline%2520can%2520effectively%2520address%2520these%2520issues%2520while%250Apreserving%2520rasterization%2520efficiency.%2520Specifically%252C%2520we%2520introduce%2520an%2520adaptive%25203D%250Asmoothing%2520filter%2520to%2520mitigate%2520aliasing%2520and%2520present%2520a%2520stable%2520view-space%2520bounding%250Amethod%2520that%2520eliminates%2520popping%2520artifacts%2520when%2520Gaussians%2520extend%2520beyond%2520the%2520view%250Afrustum.%2520Furthermore%252C%2520we%2520promote%2520tile-based%2520culling%2520to%25203D%2520with%2520screen-space%250Aplanes%252C%2520accelerating%2520rendering%2520and%2520reducing%2520sorting%2520costs%2520for%2520hierarchical%250Arasterization.%2520Our%2520method%2520achieves%2520state-of-the-art%2520quality%2520on%2520in-distribution%250Aevaluation%2520sets%2520and%2520significantly%2520outperforms%2520other%2520approaches%2520for%250Aout-of-distribution%2520views.%2520Our%2520qualitative%2520evaluations%2520further%2520demonstrate%2520the%250Aeffective%2520removal%2520of%2520aliasing%252C%2520distortions%252C%2520and%2520popping%2520artifacts%252C%2520ensuring%250Areal-time%252C%2520artifact-free%2520rendering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12811v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AAA-Gaussians%3A%20Anti-Aliased%20and%20Artifact-Free%203D%20Gaussian%20Rendering&entry.906535625=Michael%20Steiner%20and%20Thomas%20K%C3%B6hler%20and%20Lukas%20Radl%20and%20Felix%20Windisch%20and%20Dieter%20Schmalstieg%20and%20Markus%20Steinberger&entry.1292438233=%20%20Although%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20revolutionized%203D%20reconstruction%2C%0Ait%20still%20faces%20challenges%20such%20as%20aliasing%2C%20projection%20artifacts%2C%20and%20view%0Ainconsistencies%2C%20primarily%20due%20to%20the%20simplification%20of%20treating%20splats%20as%202D%0Aentities.%20We%20argue%20that%20incorporating%20full%203D%20evaluation%20of%20Gaussians%0Athroughout%20the%203DGS%20pipeline%20can%20effectively%20address%20these%20issues%20while%0Apreserving%20rasterization%20efficiency.%20Specifically%2C%20we%20introduce%20an%20adaptive%203D%0Asmoothing%20filter%20to%20mitigate%20aliasing%20and%20present%20a%20stable%20view-space%20bounding%0Amethod%20that%20eliminates%20popping%20artifacts%20when%20Gaussians%20extend%20beyond%20the%20view%0Afrustum.%20Furthermore%2C%20we%20promote%20tile-based%20culling%20to%203D%20with%20screen-space%0Aplanes%2C%20accelerating%20rendering%20and%20reducing%20sorting%20costs%20for%20hierarchical%0Arasterization.%20Our%20method%20achieves%20state-of-the-art%20quality%20on%20in-distribution%0Aevaluation%20sets%20and%20significantly%20outperforms%20other%20approaches%20for%0Aout-of-distribution%20views.%20Our%20qualitative%20evaluations%20further%20demonstrate%20the%0Aeffective%20removal%20of%20aliasing%2C%20distortions%2C%20and%20popping%20artifacts%2C%20ensuring%0Areal-time%2C%20artifact-free%20rendering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12811v1&entry.124074799=Read"},
{"title": "ODHSR: Online Dense 3D Reconstruction of Humans and Scenes from\n  Monocular Videos", "author": "Zetong Zhang and Manuel kaufmann and Lixin Xue and Jie Song and Martin R. Oswald", "abstract": "  Creating a photorealistic scene and human reconstruction from a single\nmonocular in-the-wild video figures prominently in the perception of a\nhuman-centric 3D world. Recent neural rendering advances have enabled holistic\nhuman-scene reconstruction but require pre-calibrated camera and human poses,\nand days of training time. In this work, we introduce a novel unified framework\nthat simultaneously performs camera tracking, human pose estimation and\nhuman-scene reconstruction in an online fashion. 3D Gaussian Splatting is\nutilized to learn Gaussian primitives for humans and scenes efficiently, and\nreconstruction-based camera tracking and human pose estimation modules are\ndesigned to enable holistic understanding and effective disentanglement of pose\nand appearance. Specifically, we design a human deformation module to\nreconstruct the details and enhance generalizability to out-of-distribution\nposes faithfully. Aiming to learn the spatial correlation between human and\nscene accurately, we introduce occlusion-aware human silhouette rendering and\nmonocular geometric priors, which further improve reconstruction quality.\nExperiments on the EMDB and NeuMan datasets demonstrate superior or on-par\nperformance with existing methods in camera tracking, human pose estimation,\nnovel view synthesis and runtime. Our project page is at\nhttps://eth-ait.github.io/ODHSR.\n", "link": "http://arxiv.org/abs/2504.13167v1", "date": "2025-04-17", "relevancy": 3.3794, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6831}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.675}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ODHSR%3A%20Online%20Dense%203D%20Reconstruction%20of%20Humans%20and%20Scenes%20from%0A%20%20Monocular%20Videos&body=Title%3A%20ODHSR%3A%20Online%20Dense%203D%20Reconstruction%20of%20Humans%20and%20Scenes%20from%0A%20%20Monocular%20Videos%0AAuthor%3A%20Zetong%20Zhang%20and%20Manuel%20kaufmann%20and%20Lixin%20Xue%20and%20Jie%20Song%20and%20Martin%20R.%20Oswald%0AAbstract%3A%20%20%20Creating%20a%20photorealistic%20scene%20and%20human%20reconstruction%20from%20a%20single%0Amonocular%20in-the-wild%20video%20figures%20prominently%20in%20the%20perception%20of%20a%0Ahuman-centric%203D%20world.%20Recent%20neural%20rendering%20advances%20have%20enabled%20holistic%0Ahuman-scene%20reconstruction%20but%20require%20pre-calibrated%20camera%20and%20human%20poses%2C%0Aand%20days%20of%20training%20time.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20unified%20framework%0Athat%20simultaneously%20performs%20camera%20tracking%2C%20human%20pose%20estimation%20and%0Ahuman-scene%20reconstruction%20in%20an%20online%20fashion.%203D%20Gaussian%20Splatting%20is%0Autilized%20to%20learn%20Gaussian%20primitives%20for%20humans%20and%20scenes%20efficiently%2C%20and%0Areconstruction-based%20camera%20tracking%20and%20human%20pose%20estimation%20modules%20are%0Adesigned%20to%20enable%20holistic%20understanding%20and%20effective%20disentanglement%20of%20pose%0Aand%20appearance.%20Specifically%2C%20we%20design%20a%20human%20deformation%20module%20to%0Areconstruct%20the%20details%20and%20enhance%20generalizability%20to%20out-of-distribution%0Aposes%20faithfully.%20Aiming%20to%20learn%20the%20spatial%20correlation%20between%20human%20and%0Ascene%20accurately%2C%20we%20introduce%20occlusion-aware%20human%20silhouette%20rendering%20and%0Amonocular%20geometric%20priors%2C%20which%20further%20improve%20reconstruction%20quality.%0AExperiments%20on%20the%20EMDB%20and%20NeuMan%20datasets%20demonstrate%20superior%20or%20on-par%0Aperformance%20with%20existing%20methods%20in%20camera%20tracking%2C%20human%20pose%20estimation%2C%0Anovel%20view%20synthesis%20and%20runtime.%20Our%20project%20page%20is%20at%0Ahttps%3A//eth-ait.github.io/ODHSR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13167v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DODHSR%253A%2520Online%2520Dense%25203D%2520Reconstruction%2520of%2520Humans%2520and%2520Scenes%2520from%250A%2520%2520Monocular%2520Videos%26entry.906535625%3DZetong%2520Zhang%2520and%2520Manuel%2520kaufmann%2520and%2520Lixin%2520Xue%2520and%2520Jie%2520Song%2520and%2520Martin%2520R.%2520Oswald%26entry.1292438233%3D%2520%2520Creating%2520a%2520photorealistic%2520scene%2520and%2520human%2520reconstruction%2520from%2520a%2520single%250Amonocular%2520in-the-wild%2520video%2520figures%2520prominently%2520in%2520the%2520perception%2520of%2520a%250Ahuman-centric%25203D%2520world.%2520Recent%2520neural%2520rendering%2520advances%2520have%2520enabled%2520holistic%250Ahuman-scene%2520reconstruction%2520but%2520require%2520pre-calibrated%2520camera%2520and%2520human%2520poses%252C%250Aand%2520days%2520of%2520training%2520time.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520unified%2520framework%250Athat%2520simultaneously%2520performs%2520camera%2520tracking%252C%2520human%2520pose%2520estimation%2520and%250Ahuman-scene%2520reconstruction%2520in%2520an%2520online%2520fashion.%25203D%2520Gaussian%2520Splatting%2520is%250Autilized%2520to%2520learn%2520Gaussian%2520primitives%2520for%2520humans%2520and%2520scenes%2520efficiently%252C%2520and%250Areconstruction-based%2520camera%2520tracking%2520and%2520human%2520pose%2520estimation%2520modules%2520are%250Adesigned%2520to%2520enable%2520holistic%2520understanding%2520and%2520effective%2520disentanglement%2520of%2520pose%250Aand%2520appearance.%2520Specifically%252C%2520we%2520design%2520a%2520human%2520deformation%2520module%2520to%250Areconstruct%2520the%2520details%2520and%2520enhance%2520generalizability%2520to%2520out-of-distribution%250Aposes%2520faithfully.%2520Aiming%2520to%2520learn%2520the%2520spatial%2520correlation%2520between%2520human%2520and%250Ascene%2520accurately%252C%2520we%2520introduce%2520occlusion-aware%2520human%2520silhouette%2520rendering%2520and%250Amonocular%2520geometric%2520priors%252C%2520which%2520further%2520improve%2520reconstruction%2520quality.%250AExperiments%2520on%2520the%2520EMDB%2520and%2520NeuMan%2520datasets%2520demonstrate%2520superior%2520or%2520on-par%250Aperformance%2520with%2520existing%2520methods%2520in%2520camera%2520tracking%252C%2520human%2520pose%2520estimation%252C%250Anovel%2520view%2520synthesis%2520and%2520runtime.%2520Our%2520project%2520page%2520is%2520at%250Ahttps%253A//eth-ait.github.io/ODHSR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13167v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ODHSR%3A%20Online%20Dense%203D%20Reconstruction%20of%20Humans%20and%20Scenes%20from%0A%20%20Monocular%20Videos&entry.906535625=Zetong%20Zhang%20and%20Manuel%20kaufmann%20and%20Lixin%20Xue%20and%20Jie%20Song%20and%20Martin%20R.%20Oswald&entry.1292438233=%20%20Creating%20a%20photorealistic%20scene%20and%20human%20reconstruction%20from%20a%20single%0Amonocular%20in-the-wild%20video%20figures%20prominently%20in%20the%20perception%20of%20a%0Ahuman-centric%203D%20world.%20Recent%20neural%20rendering%20advances%20have%20enabled%20holistic%0Ahuman-scene%20reconstruction%20but%20require%20pre-calibrated%20camera%20and%20human%20poses%2C%0Aand%20days%20of%20training%20time.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20unified%20framework%0Athat%20simultaneously%20performs%20camera%20tracking%2C%20human%20pose%20estimation%20and%0Ahuman-scene%20reconstruction%20in%20an%20online%20fashion.%203D%20Gaussian%20Splatting%20is%0Autilized%20to%20learn%20Gaussian%20primitives%20for%20humans%20and%20scenes%20efficiently%2C%20and%0Areconstruction-based%20camera%20tracking%20and%20human%20pose%20estimation%20modules%20are%0Adesigned%20to%20enable%20holistic%20understanding%20and%20effective%20disentanglement%20of%20pose%0Aand%20appearance.%20Specifically%2C%20we%20design%20a%20human%20deformation%20module%20to%0Areconstruct%20the%20details%20and%20enhance%20generalizability%20to%20out-of-distribution%0Aposes%20faithfully.%20Aiming%20to%20learn%20the%20spatial%20correlation%20between%20human%20and%0Ascene%20accurately%2C%20we%20introduce%20occlusion-aware%20human%20silhouette%20rendering%20and%0Amonocular%20geometric%20priors%2C%20which%20further%20improve%20reconstruction%20quality.%0AExperiments%20on%20the%20EMDB%20and%20NeuMan%20datasets%20demonstrate%20superior%20or%20on-par%0Aperformance%20with%20existing%20methods%20in%20camera%20tracking%2C%20human%20pose%20estimation%2C%0Anovel%20view%20synthesis%20and%20runtime.%20Our%20project%20page%20is%20at%0Ahttps%3A//eth-ait.github.io/ODHSR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13167v1&entry.124074799=Read"},
{"title": "Training-Free Hierarchical Scene Understanding for Gaussian Splatting\n  with Superpoint Graphs", "author": "Shaohui Dai and Yansong Qu and Zheyan Li and Xinyang Li and Shengchuan Zhang and Liujuan Cao", "abstract": "  Bridging natural language and 3D geometry is a crucial step toward flexible,\nlanguage-driven scene understanding. While recent advances in 3D Gaussian\nSplatting (3DGS) have enabled fast and high-quality scene reconstruction,\nresearch has also explored incorporating open-vocabulary understanding into\n3DGS. However, most existing methods require iterative optimization over\nper-view 2D semantic feature maps, which not only results in inefficiencies but\nalso leads to inconsistent 3D semantics across views. To address these\nlimitations, we introduce a training-free framework that constructs a\nsuperpoint graph directly from Gaussian primitives. The superpoint graph\npartitions the scene into spatially compact and semantically coherent regions,\nforming view-consistent 3D entities and providing a structured foundation for\nopen-vocabulary understanding. Based on the graph structure, we design an\nefficient reprojection strategy that lifts 2D semantic features onto the\nsuperpoints, avoiding costly multi-view iterative training. The resulting\nrepresentation ensures strong 3D semantic coherence and naturally supports\nhierarchical understanding, enabling both coarse- and fine-grained\nopen-vocabulary perception within a unified semantic field. Extensive\nexperiments demonstrate that our method achieves state-of-the-art\nopen-vocabulary segmentation performance, with semantic field reconstruction\ncompleted over $30\\times$ faster. Our code will be available at\nhttps://github.com/Atrovast/THGS.\n", "link": "http://arxiv.org/abs/2504.13153v1", "date": "2025-04-17", "relevancy": 3.3722, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6983}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6865}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-Free%20Hierarchical%20Scene%20Understanding%20for%20Gaussian%20Splatting%0A%20%20with%20Superpoint%20Graphs&body=Title%3A%20Training-Free%20Hierarchical%20Scene%20Understanding%20for%20Gaussian%20Splatting%0A%20%20with%20Superpoint%20Graphs%0AAuthor%3A%20Shaohui%20Dai%20and%20Yansong%20Qu%20and%20Zheyan%20Li%20and%20Xinyang%20Li%20and%20Shengchuan%20Zhang%20and%20Liujuan%20Cao%0AAbstract%3A%20%20%20Bridging%20natural%20language%20and%203D%20geometry%20is%20a%20crucial%20step%20toward%20flexible%2C%0Alanguage-driven%20scene%20understanding.%20While%20recent%20advances%20in%203D%20Gaussian%0ASplatting%20%283DGS%29%20have%20enabled%20fast%20and%20high-quality%20scene%20reconstruction%2C%0Aresearch%20has%20also%20explored%20incorporating%20open-vocabulary%20understanding%20into%0A3DGS.%20However%2C%20most%20existing%20methods%20require%20iterative%20optimization%20over%0Aper-view%202D%20semantic%20feature%20maps%2C%20which%20not%20only%20results%20in%20inefficiencies%20but%0Aalso%20leads%20to%20inconsistent%203D%20semantics%20across%20views.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20a%20training-free%20framework%20that%20constructs%20a%0Asuperpoint%20graph%20directly%20from%20Gaussian%20primitives.%20The%20superpoint%20graph%0Apartitions%20the%20scene%20into%20spatially%20compact%20and%20semantically%20coherent%20regions%2C%0Aforming%20view-consistent%203D%20entities%20and%20providing%20a%20structured%20foundation%20for%0Aopen-vocabulary%20understanding.%20Based%20on%20the%20graph%20structure%2C%20we%20design%20an%0Aefficient%20reprojection%20strategy%20that%20lifts%202D%20semantic%20features%20onto%20the%0Asuperpoints%2C%20avoiding%20costly%20multi-view%20iterative%20training.%20The%20resulting%0Arepresentation%20ensures%20strong%203D%20semantic%20coherence%20and%20naturally%20supports%0Ahierarchical%20understanding%2C%20enabling%20both%20coarse-%20and%20fine-grained%0Aopen-vocabulary%20perception%20within%20a%20unified%20semantic%20field.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%0Aopen-vocabulary%20segmentation%20performance%2C%20with%20semantic%20field%20reconstruction%0Acompleted%20over%20%2430%5Ctimes%24%20faster.%20Our%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/Atrovast/THGS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13153v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-Free%2520Hierarchical%2520Scene%2520Understanding%2520for%2520Gaussian%2520Splatting%250A%2520%2520with%2520Superpoint%2520Graphs%26entry.906535625%3DShaohui%2520Dai%2520and%2520Yansong%2520Qu%2520and%2520Zheyan%2520Li%2520and%2520Xinyang%2520Li%2520and%2520Shengchuan%2520Zhang%2520and%2520Liujuan%2520Cao%26entry.1292438233%3D%2520%2520Bridging%2520natural%2520language%2520and%25203D%2520geometry%2520is%2520a%2520crucial%2520step%2520toward%2520flexible%252C%250Alanguage-driven%2520scene%2520understanding.%2520While%2520recent%2520advances%2520in%25203D%2520Gaussian%250ASplatting%2520%25283DGS%2529%2520have%2520enabled%2520fast%2520and%2520high-quality%2520scene%2520reconstruction%252C%250Aresearch%2520has%2520also%2520explored%2520incorporating%2520open-vocabulary%2520understanding%2520into%250A3DGS.%2520However%252C%2520most%2520existing%2520methods%2520require%2520iterative%2520optimization%2520over%250Aper-view%25202D%2520semantic%2520feature%2520maps%252C%2520which%2520not%2520only%2520results%2520in%2520inefficiencies%2520but%250Aalso%2520leads%2520to%2520inconsistent%25203D%2520semantics%2520across%2520views.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520introduce%2520a%2520training-free%2520framework%2520that%2520constructs%2520a%250Asuperpoint%2520graph%2520directly%2520from%2520Gaussian%2520primitives.%2520The%2520superpoint%2520graph%250Apartitions%2520the%2520scene%2520into%2520spatially%2520compact%2520and%2520semantically%2520coherent%2520regions%252C%250Aforming%2520view-consistent%25203D%2520entities%2520and%2520providing%2520a%2520structured%2520foundation%2520for%250Aopen-vocabulary%2520understanding.%2520Based%2520on%2520the%2520graph%2520structure%252C%2520we%2520design%2520an%250Aefficient%2520reprojection%2520strategy%2520that%2520lifts%25202D%2520semantic%2520features%2520onto%2520the%250Asuperpoints%252C%2520avoiding%2520costly%2520multi-view%2520iterative%2520training.%2520The%2520resulting%250Arepresentation%2520ensures%2520strong%25203D%2520semantic%2520coherence%2520and%2520naturally%2520supports%250Ahierarchical%2520understanding%252C%2520enabling%2520both%2520coarse-%2520and%2520fine-grained%250Aopen-vocabulary%2520perception%2520within%2520a%2520unified%2520semantic%2520field.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%250Aopen-vocabulary%2520segmentation%2520performance%252C%2520with%2520semantic%2520field%2520reconstruction%250Acompleted%2520over%2520%252430%255Ctimes%2524%2520faster.%2520Our%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/Atrovast/THGS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13153v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-Free%20Hierarchical%20Scene%20Understanding%20for%20Gaussian%20Splatting%0A%20%20with%20Superpoint%20Graphs&entry.906535625=Shaohui%20Dai%20and%20Yansong%20Qu%20and%20Zheyan%20Li%20and%20Xinyang%20Li%20and%20Shengchuan%20Zhang%20and%20Liujuan%20Cao&entry.1292438233=%20%20Bridging%20natural%20language%20and%203D%20geometry%20is%20a%20crucial%20step%20toward%20flexible%2C%0Alanguage-driven%20scene%20understanding.%20While%20recent%20advances%20in%203D%20Gaussian%0ASplatting%20%283DGS%29%20have%20enabled%20fast%20and%20high-quality%20scene%20reconstruction%2C%0Aresearch%20has%20also%20explored%20incorporating%20open-vocabulary%20understanding%20into%0A3DGS.%20However%2C%20most%20existing%20methods%20require%20iterative%20optimization%20over%0Aper-view%202D%20semantic%20feature%20maps%2C%20which%20not%20only%20results%20in%20inefficiencies%20but%0Aalso%20leads%20to%20inconsistent%203D%20semantics%20across%20views.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20a%20training-free%20framework%20that%20constructs%20a%0Asuperpoint%20graph%20directly%20from%20Gaussian%20primitives.%20The%20superpoint%20graph%0Apartitions%20the%20scene%20into%20spatially%20compact%20and%20semantically%20coherent%20regions%2C%0Aforming%20view-consistent%203D%20entities%20and%20providing%20a%20structured%20foundation%20for%0Aopen-vocabulary%20understanding.%20Based%20on%20the%20graph%20structure%2C%20we%20design%20an%0Aefficient%20reprojection%20strategy%20that%20lifts%202D%20semantic%20features%20onto%20the%0Asuperpoints%2C%20avoiding%20costly%20multi-view%20iterative%20training.%20The%20resulting%0Arepresentation%20ensures%20strong%203D%20semantic%20coherence%20and%20naturally%20supports%0Ahierarchical%20understanding%2C%20enabling%20both%20coarse-%20and%20fine-grained%0Aopen-vocabulary%20perception%20within%20a%20unified%20semantic%20field.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%0Aopen-vocabulary%20segmentation%20performance%2C%20with%20semantic%20field%20reconstruction%0Acompleted%20over%20%2430%5Ctimes%24%20faster.%20Our%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/Atrovast/THGS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13153v1&entry.124074799=Read"},
{"title": "Second-order Optimization of Gaussian Splats with Importance Sampling", "author": "Hamza Pehlivan and Andrea Boscolo Camiletto and Lin Geng Foo and Marc Habermann and Christian Theobalt", "abstract": "  3D Gaussian Splatting (3DGS) is widely used for novel view synthesis due to\nits high rendering quality and fast inference time. However, 3DGS predominantly\nrelies on first-order optimizers such as Adam, which leads to long training\ntimes. To address this limitation, we propose a novel second-order optimization\nstrategy based on Levenberg-Marquardt (LM) and Conjugate Gradient (CG), which\nwe specifically tailor towards Gaussian Splatting. Our key insight is that the\nJacobian in 3DGS exhibits significant sparsity since each Gaussian affects only\na limited number of pixels. We exploit this sparsity by proposing a matrix-free\nand GPU-parallelized LM optimization. To further improve its efficiency, we\npropose sampling strategies for both the camera views and loss function and,\nconsequently, the normal equation, significantly reducing the computational\ncomplexity. In addition, we increase the convergence rate of the second-order\napproximation by introducing an effective heuristic to determine the learning\nrate that avoids the expensive computation cost of line search methods. As a\nresult, our method achieves a $3\\times$ speedup over standard LM and\noutperforms Adam by $~6\\times$ when the Gaussian count is low while remaining\ncompetitive for moderate counts. Project Page:\nhttps://vcai.mpi-inf.mpg.de/projects/LM-IS\n", "link": "http://arxiv.org/abs/2504.12905v1", "date": "2025-04-17", "relevancy": 3.3131, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6899}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6574}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Second-order%20Optimization%20of%20Gaussian%20Splats%20with%20Importance%20Sampling&body=Title%3A%20Second-order%20Optimization%20of%20Gaussian%20Splats%20with%20Importance%20Sampling%0AAuthor%3A%20Hamza%20Pehlivan%20and%20Andrea%20Boscolo%20Camiletto%20and%20Lin%20Geng%20Foo%20and%20Marc%20Habermann%20and%20Christian%20Theobalt%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20is%20widely%20used%20for%20novel%20view%20synthesis%20due%20to%0Aits%20high%20rendering%20quality%20and%20fast%20inference%20time.%20However%2C%203DGS%20predominantly%0Arelies%20on%20first-order%20optimizers%20such%20as%20Adam%2C%20which%20leads%20to%20long%20training%0Atimes.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20novel%20second-order%20optimization%0Astrategy%20based%20on%20Levenberg-Marquardt%20%28LM%29%20and%20Conjugate%20Gradient%20%28CG%29%2C%20which%0Awe%20specifically%20tailor%20towards%20Gaussian%20Splatting.%20Our%20key%20insight%20is%20that%20the%0AJacobian%20in%203DGS%20exhibits%20significant%20sparsity%20since%20each%20Gaussian%20affects%20only%0Aa%20limited%20number%20of%20pixels.%20We%20exploit%20this%20sparsity%20by%20proposing%20a%20matrix-free%0Aand%20GPU-parallelized%20LM%20optimization.%20To%20further%20improve%20its%20efficiency%2C%20we%0Apropose%20sampling%20strategies%20for%20both%20the%20camera%20views%20and%20loss%20function%20and%2C%0Aconsequently%2C%20the%20normal%20equation%2C%20significantly%20reducing%20the%20computational%0Acomplexity.%20In%20addition%2C%20we%20increase%20the%20convergence%20rate%20of%20the%20second-order%0Aapproximation%20by%20introducing%20an%20effective%20heuristic%20to%20determine%20the%20learning%0Arate%20that%20avoids%20the%20expensive%20computation%20cost%20of%20line%20search%20methods.%20As%20a%0Aresult%2C%20our%20method%20achieves%20a%20%243%5Ctimes%24%20speedup%20over%20standard%20LM%20and%0Aoutperforms%20Adam%20by%20%24~6%5Ctimes%24%20when%20the%20Gaussian%20count%20is%20low%20while%20remaining%0Acompetitive%20for%20moderate%20counts.%20Project%20Page%3A%0Ahttps%3A//vcai.mpi-inf.mpg.de/projects/LM-IS%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12905v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSecond-order%2520Optimization%2520of%2520Gaussian%2520Splats%2520with%2520Importance%2520Sampling%26entry.906535625%3DHamza%2520Pehlivan%2520and%2520Andrea%2520Boscolo%2520Camiletto%2520and%2520Lin%2520Geng%2520Foo%2520and%2520Marc%2520Habermann%2520and%2520Christian%2520Theobalt%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520is%2520widely%2520used%2520for%2520novel%2520view%2520synthesis%2520due%2520to%250Aits%2520high%2520rendering%2520quality%2520and%2520fast%2520inference%2520time.%2520However%252C%25203DGS%2520predominantly%250Arelies%2520on%2520first-order%2520optimizers%2520such%2520as%2520Adam%252C%2520which%2520leads%2520to%2520long%2520training%250Atimes.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%2520novel%2520second-order%2520optimization%250Astrategy%2520based%2520on%2520Levenberg-Marquardt%2520%2528LM%2529%2520and%2520Conjugate%2520Gradient%2520%2528CG%2529%252C%2520which%250Awe%2520specifically%2520tailor%2520towards%2520Gaussian%2520Splatting.%2520Our%2520key%2520insight%2520is%2520that%2520the%250AJacobian%2520in%25203DGS%2520exhibits%2520significant%2520sparsity%2520since%2520each%2520Gaussian%2520affects%2520only%250Aa%2520limited%2520number%2520of%2520pixels.%2520We%2520exploit%2520this%2520sparsity%2520by%2520proposing%2520a%2520matrix-free%250Aand%2520GPU-parallelized%2520LM%2520optimization.%2520To%2520further%2520improve%2520its%2520efficiency%252C%2520we%250Apropose%2520sampling%2520strategies%2520for%2520both%2520the%2520camera%2520views%2520and%2520loss%2520function%2520and%252C%250Aconsequently%252C%2520the%2520normal%2520equation%252C%2520significantly%2520reducing%2520the%2520computational%250Acomplexity.%2520In%2520addition%252C%2520we%2520increase%2520the%2520convergence%2520rate%2520of%2520the%2520second-order%250Aapproximation%2520by%2520introducing%2520an%2520effective%2520heuristic%2520to%2520determine%2520the%2520learning%250Arate%2520that%2520avoids%2520the%2520expensive%2520computation%2520cost%2520of%2520line%2520search%2520methods.%2520As%2520a%250Aresult%252C%2520our%2520method%2520achieves%2520a%2520%25243%255Ctimes%2524%2520speedup%2520over%2520standard%2520LM%2520and%250Aoutperforms%2520Adam%2520by%2520%2524~6%255Ctimes%2524%2520when%2520the%2520Gaussian%2520count%2520is%2520low%2520while%2520remaining%250Acompetitive%2520for%2520moderate%2520counts.%2520Project%2520Page%253A%250Ahttps%253A//vcai.mpi-inf.mpg.de/projects/LM-IS%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12905v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Second-order%20Optimization%20of%20Gaussian%20Splats%20with%20Importance%20Sampling&entry.906535625=Hamza%20Pehlivan%20and%20Andrea%20Boscolo%20Camiletto%20and%20Lin%20Geng%20Foo%20and%20Marc%20Habermann%20and%20Christian%20Theobalt&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20is%20widely%20used%20for%20novel%20view%20synthesis%20due%20to%0Aits%20high%20rendering%20quality%20and%20fast%20inference%20time.%20However%2C%203DGS%20predominantly%0Arelies%20on%20first-order%20optimizers%20such%20as%20Adam%2C%20which%20leads%20to%20long%20training%0Atimes.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20novel%20second-order%20optimization%0Astrategy%20based%20on%20Levenberg-Marquardt%20%28LM%29%20and%20Conjugate%20Gradient%20%28CG%29%2C%20which%0Awe%20specifically%20tailor%20towards%20Gaussian%20Splatting.%20Our%20key%20insight%20is%20that%20the%0AJacobian%20in%203DGS%20exhibits%20significant%20sparsity%20since%20each%20Gaussian%20affects%20only%0Aa%20limited%20number%20of%20pixels.%20We%20exploit%20this%20sparsity%20by%20proposing%20a%20matrix-free%0Aand%20GPU-parallelized%20LM%20optimization.%20To%20further%20improve%20its%20efficiency%2C%20we%0Apropose%20sampling%20strategies%20for%20both%20the%20camera%20views%20and%20loss%20function%20and%2C%0Aconsequently%2C%20the%20normal%20equation%2C%20significantly%20reducing%20the%20computational%0Acomplexity.%20In%20addition%2C%20we%20increase%20the%20convergence%20rate%20of%20the%20second-order%0Aapproximation%20by%20introducing%20an%20effective%20heuristic%20to%20determine%20the%20learning%0Arate%20that%20avoids%20the%20expensive%20computation%20cost%20of%20line%20search%20methods.%20As%20a%0Aresult%2C%20our%20method%20achieves%20a%20%243%5Ctimes%24%20speedup%20over%20standard%20LM%20and%0Aoutperforms%20Adam%20by%20%24~6%5Ctimes%24%20when%20the%20Gaussian%20count%20is%20low%20while%20remaining%0Acompetitive%20for%20moderate%20counts.%20Project%20Page%3A%0Ahttps%3A//vcai.mpi-inf.mpg.de/projects/LM-IS%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12905v1&entry.124074799=Read"},
{"title": "CompGS++: Compressed Gaussian Splatting for Static and Dynamic Scene\n  Representation", "author": "Xiangrui Liu and Xinju Wu and Shiqi Wang and Zhu Li and Sam Kwong", "abstract": "  Gaussian splatting demonstrates proficiency for 3D scene modeling but suffers\nfrom substantial data volume due to inherent primitive redundancy. To enable\nfuture photorealistic 3D immersive visual communication applications,\nsignificant compression is essential for transmission over the existing\nInternet infrastructure. Hence, we propose Compressed Gaussian Splatting\n(CompGS++), a novel framework that leverages compact Gaussian primitives to\nachieve accurate 3D modeling with substantial size reduction for both static\nand dynamic scenes. Our design is based on the principle of eliminating\nredundancy both between and within primitives. Specifically, we develop a\ncomprehensive prediction paradigm to address inter-primitive redundancy through\nspatial and temporal primitive prediction modules. The spatial primitive\nprediction module establishes predictive relationships for scene primitives and\nenables most primitives to be encoded as compact residuals, substantially\nreducing the spatial redundancy. We further devise a temporal primitive\nprediction module to handle dynamic scenes, which exploits primitive\ncorrelations across timestamps to effectively reduce temporal redundancy.\nMoreover, we devise a rate-constrained optimization module that jointly\nminimizes reconstruction error and rate consumption. This module effectively\neliminates parameter redundancy within primitives and enhances the overall\ncompactness of scene representations. Comprehensive evaluations across multiple\nbenchmark datasets demonstrate that CompGS++ significantly outperforms existing\nmethods, achieving superior compression performance while preserving accurate\nscene modeling. Our implementation will be made publicly available on GitHub to\nfacilitate further research.\n", "link": "http://arxiv.org/abs/2504.13022v1", "date": "2025-04-17", "relevancy": 3.3038, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6943}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6613}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CompGS%2B%2B%3A%20Compressed%20Gaussian%20Splatting%20for%20Static%20and%20Dynamic%20Scene%0A%20%20Representation&body=Title%3A%20CompGS%2B%2B%3A%20Compressed%20Gaussian%20Splatting%20for%20Static%20and%20Dynamic%20Scene%0A%20%20Representation%0AAuthor%3A%20Xiangrui%20Liu%20and%20Xinju%20Wu%20and%20Shiqi%20Wang%20and%20Zhu%20Li%20and%20Sam%20Kwong%0AAbstract%3A%20%20%20Gaussian%20splatting%20demonstrates%20proficiency%20for%203D%20scene%20modeling%20but%20suffers%0Afrom%20substantial%20data%20volume%20due%20to%20inherent%20primitive%20redundancy.%20To%20enable%0Afuture%20photorealistic%203D%20immersive%20visual%20communication%20applications%2C%0Asignificant%20compression%20is%20essential%20for%20transmission%20over%20the%20existing%0AInternet%20infrastructure.%20Hence%2C%20we%20propose%20Compressed%20Gaussian%20Splatting%0A%28CompGS%2B%2B%29%2C%20a%20novel%20framework%20that%20leverages%20compact%20Gaussian%20primitives%20to%0Aachieve%20accurate%203D%20modeling%20with%20substantial%20size%20reduction%20for%20both%20static%0Aand%20dynamic%20scenes.%20Our%20design%20is%20based%20on%20the%20principle%20of%20eliminating%0Aredundancy%20both%20between%20and%20within%20primitives.%20Specifically%2C%20we%20develop%20a%0Acomprehensive%20prediction%20paradigm%20to%20address%20inter-primitive%20redundancy%20through%0Aspatial%20and%20temporal%20primitive%20prediction%20modules.%20The%20spatial%20primitive%0Aprediction%20module%20establishes%20predictive%20relationships%20for%20scene%20primitives%20and%0Aenables%20most%20primitives%20to%20be%20encoded%20as%20compact%20residuals%2C%20substantially%0Areducing%20the%20spatial%20redundancy.%20We%20further%20devise%20a%20temporal%20primitive%0Aprediction%20module%20to%20handle%20dynamic%20scenes%2C%20which%20exploits%20primitive%0Acorrelations%20across%20timestamps%20to%20effectively%20reduce%20temporal%20redundancy.%0AMoreover%2C%20we%20devise%20a%20rate-constrained%20optimization%20module%20that%20jointly%0Aminimizes%20reconstruction%20error%20and%20rate%20consumption.%20This%20module%20effectively%0Aeliminates%20parameter%20redundancy%20within%20primitives%20and%20enhances%20the%20overall%0Acompactness%20of%20scene%20representations.%20Comprehensive%20evaluations%20across%20multiple%0Abenchmark%20datasets%20demonstrate%20that%20CompGS%2B%2B%20significantly%20outperforms%20existing%0Amethods%2C%20achieving%20superior%20compression%20performance%20while%20preserving%20accurate%0Ascene%20modeling.%20Our%20implementation%20will%20be%20made%20publicly%20available%20on%20GitHub%20to%0Afacilitate%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13022v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompGS%252B%252B%253A%2520Compressed%2520Gaussian%2520Splatting%2520for%2520Static%2520and%2520Dynamic%2520Scene%250A%2520%2520Representation%26entry.906535625%3DXiangrui%2520Liu%2520and%2520Xinju%2520Wu%2520and%2520Shiqi%2520Wang%2520and%2520Zhu%2520Li%2520and%2520Sam%2520Kwong%26entry.1292438233%3D%2520%2520Gaussian%2520splatting%2520demonstrates%2520proficiency%2520for%25203D%2520scene%2520modeling%2520but%2520suffers%250Afrom%2520substantial%2520data%2520volume%2520due%2520to%2520inherent%2520primitive%2520redundancy.%2520To%2520enable%250Afuture%2520photorealistic%25203D%2520immersive%2520visual%2520communication%2520applications%252C%250Asignificant%2520compression%2520is%2520essential%2520for%2520transmission%2520over%2520the%2520existing%250AInternet%2520infrastructure.%2520Hence%252C%2520we%2520propose%2520Compressed%2520Gaussian%2520Splatting%250A%2528CompGS%252B%252B%2529%252C%2520a%2520novel%2520framework%2520that%2520leverages%2520compact%2520Gaussian%2520primitives%2520to%250Aachieve%2520accurate%25203D%2520modeling%2520with%2520substantial%2520size%2520reduction%2520for%2520both%2520static%250Aand%2520dynamic%2520scenes.%2520Our%2520design%2520is%2520based%2520on%2520the%2520principle%2520of%2520eliminating%250Aredundancy%2520both%2520between%2520and%2520within%2520primitives.%2520Specifically%252C%2520we%2520develop%2520a%250Acomprehensive%2520prediction%2520paradigm%2520to%2520address%2520inter-primitive%2520redundancy%2520through%250Aspatial%2520and%2520temporal%2520primitive%2520prediction%2520modules.%2520The%2520spatial%2520primitive%250Aprediction%2520module%2520establishes%2520predictive%2520relationships%2520for%2520scene%2520primitives%2520and%250Aenables%2520most%2520primitives%2520to%2520be%2520encoded%2520as%2520compact%2520residuals%252C%2520substantially%250Areducing%2520the%2520spatial%2520redundancy.%2520We%2520further%2520devise%2520a%2520temporal%2520primitive%250Aprediction%2520module%2520to%2520handle%2520dynamic%2520scenes%252C%2520which%2520exploits%2520primitive%250Acorrelations%2520across%2520timestamps%2520to%2520effectively%2520reduce%2520temporal%2520redundancy.%250AMoreover%252C%2520we%2520devise%2520a%2520rate-constrained%2520optimization%2520module%2520that%2520jointly%250Aminimizes%2520reconstruction%2520error%2520and%2520rate%2520consumption.%2520This%2520module%2520effectively%250Aeliminates%2520parameter%2520redundancy%2520within%2520primitives%2520and%2520enhances%2520the%2520overall%250Acompactness%2520of%2520scene%2520representations.%2520Comprehensive%2520evaluations%2520across%2520multiple%250Abenchmark%2520datasets%2520demonstrate%2520that%2520CompGS%252B%252B%2520significantly%2520outperforms%2520existing%250Amethods%252C%2520achieving%2520superior%2520compression%2520performance%2520while%2520preserving%2520accurate%250Ascene%2520modeling.%2520Our%2520implementation%2520will%2520be%2520made%2520publicly%2520available%2520on%2520GitHub%2520to%250Afacilitate%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13022v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CompGS%2B%2B%3A%20Compressed%20Gaussian%20Splatting%20for%20Static%20and%20Dynamic%20Scene%0A%20%20Representation&entry.906535625=Xiangrui%20Liu%20and%20Xinju%20Wu%20and%20Shiqi%20Wang%20and%20Zhu%20Li%20and%20Sam%20Kwong&entry.1292438233=%20%20Gaussian%20splatting%20demonstrates%20proficiency%20for%203D%20scene%20modeling%20but%20suffers%0Afrom%20substantial%20data%20volume%20due%20to%20inherent%20primitive%20redundancy.%20To%20enable%0Afuture%20photorealistic%203D%20immersive%20visual%20communication%20applications%2C%0Asignificant%20compression%20is%20essential%20for%20transmission%20over%20the%20existing%0AInternet%20infrastructure.%20Hence%2C%20we%20propose%20Compressed%20Gaussian%20Splatting%0A%28CompGS%2B%2B%29%2C%20a%20novel%20framework%20that%20leverages%20compact%20Gaussian%20primitives%20to%0Aachieve%20accurate%203D%20modeling%20with%20substantial%20size%20reduction%20for%20both%20static%0Aand%20dynamic%20scenes.%20Our%20design%20is%20based%20on%20the%20principle%20of%20eliminating%0Aredundancy%20both%20between%20and%20within%20primitives.%20Specifically%2C%20we%20develop%20a%0Acomprehensive%20prediction%20paradigm%20to%20address%20inter-primitive%20redundancy%20through%0Aspatial%20and%20temporal%20primitive%20prediction%20modules.%20The%20spatial%20primitive%0Aprediction%20module%20establishes%20predictive%20relationships%20for%20scene%20primitives%20and%0Aenables%20most%20primitives%20to%20be%20encoded%20as%20compact%20residuals%2C%20substantially%0Areducing%20the%20spatial%20redundancy.%20We%20further%20devise%20a%20temporal%20primitive%0Aprediction%20module%20to%20handle%20dynamic%20scenes%2C%20which%20exploits%20primitive%0Acorrelations%20across%20timestamps%20to%20effectively%20reduce%20temporal%20redundancy.%0AMoreover%2C%20we%20devise%20a%20rate-constrained%20optimization%20module%20that%20jointly%0Aminimizes%20reconstruction%20error%20and%20rate%20consumption.%20This%20module%20effectively%0Aeliminates%20parameter%20redundancy%20within%20primitives%20and%20enhances%20the%20overall%0Acompactness%20of%20scene%20representations.%20Comprehensive%20evaluations%20across%20multiple%0Abenchmark%20datasets%20demonstrate%20that%20CompGS%2B%2B%20significantly%20outperforms%20existing%0Amethods%2C%20achieving%20superior%20compression%20performance%20while%20preserving%20accurate%0Ascene%20modeling.%20Our%20implementation%20will%20be%20made%20publicly%20available%20on%20GitHub%20to%0Afacilitate%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13022v1&entry.124074799=Read"},
{"title": "CAGE-GS: High-fidelity Cage Based 3D Gaussian Splatting Deformation", "author": "Yifei Tong and Runze Tian and Xiao Han and Dingyao Liu and Fenggen Yu and Yan Zhang", "abstract": "  As 3D Gaussian Splatting (3DGS) gains popularity as a 3D representation of\nreal scenes, enabling user-friendly deformation to create novel scenes while\npreserving fine details from the original 3DGS has attracted significant\nresearch attention. We introduce CAGE-GS, a cage-based 3DGS deformation method\nthat seamlessly aligns a source 3DGS scene with a user-defined target shape.\nOur approach learns a deformation cage from the target, which guides the\ngeometric transformation of the source scene. While the cages effectively\ncontrol structural alignment, preserving the textural appearance of 3DGS\nremains challenging due to the complexity of covariance parameters. To address\nthis, we employ a Jacobian matrix-based strategy to update the covariance\nparameters of each Gaussian, ensuring texture fidelity post-deformation. Our\nmethod is highly flexible, accommodating various target shape representations,\nincluding texts, images, point clouds, meshes and 3DGS models. Extensive\nexperiments and ablation studies on both public datasets and newly proposed\nscenes demonstrate that our method significantly outperforms existing\ntechniques in both efficiency and deformation quality.\n", "link": "http://arxiv.org/abs/2504.12800v1", "date": "2025-04-17", "relevancy": 3.2798, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6809}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6497}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAGE-GS%3A%20High-fidelity%20Cage%20Based%203D%20Gaussian%20Splatting%20Deformation&body=Title%3A%20CAGE-GS%3A%20High-fidelity%20Cage%20Based%203D%20Gaussian%20Splatting%20Deformation%0AAuthor%3A%20Yifei%20Tong%20and%20Runze%20Tian%20and%20Xiao%20Han%20and%20Dingyao%20Liu%20and%20Fenggen%20Yu%20and%20Yan%20Zhang%0AAbstract%3A%20%20%20As%203D%20Gaussian%20Splatting%20%283DGS%29%20gains%20popularity%20as%20a%203D%20representation%20of%0Areal%20scenes%2C%20enabling%20user-friendly%20deformation%20to%20create%20novel%20scenes%20while%0Apreserving%20fine%20details%20from%20the%20original%203DGS%20has%20attracted%20significant%0Aresearch%20attention.%20We%20introduce%20CAGE-GS%2C%20a%20cage-based%203DGS%20deformation%20method%0Athat%20seamlessly%20aligns%20a%20source%203DGS%20scene%20with%20a%20user-defined%20target%20shape.%0AOur%20approach%20learns%20a%20deformation%20cage%20from%20the%20target%2C%20which%20guides%20the%0Ageometric%20transformation%20of%20the%20source%20scene.%20While%20the%20cages%20effectively%0Acontrol%20structural%20alignment%2C%20preserving%20the%20textural%20appearance%20of%203DGS%0Aremains%20challenging%20due%20to%20the%20complexity%20of%20covariance%20parameters.%20To%20address%0Athis%2C%20we%20employ%20a%20Jacobian%20matrix-based%20strategy%20to%20update%20the%20covariance%0Aparameters%20of%20each%20Gaussian%2C%20ensuring%20texture%20fidelity%20post-deformation.%20Our%0Amethod%20is%20highly%20flexible%2C%20accommodating%20various%20target%20shape%20representations%2C%0Aincluding%20texts%2C%20images%2C%20point%20clouds%2C%20meshes%20and%203DGS%20models.%20Extensive%0Aexperiments%20and%20ablation%20studies%20on%20both%20public%20datasets%20and%20newly%20proposed%0Ascenes%20demonstrate%20that%20our%20method%20significantly%20outperforms%20existing%0Atechniques%20in%20both%20efficiency%20and%20deformation%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12800v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAGE-GS%253A%2520High-fidelity%2520Cage%2520Based%25203D%2520Gaussian%2520Splatting%2520Deformation%26entry.906535625%3DYifei%2520Tong%2520and%2520Runze%2520Tian%2520and%2520Xiao%2520Han%2520and%2520Dingyao%2520Liu%2520and%2520Fenggen%2520Yu%2520and%2520Yan%2520Zhang%26entry.1292438233%3D%2520%2520As%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520gains%2520popularity%2520as%2520a%25203D%2520representation%2520of%250Areal%2520scenes%252C%2520enabling%2520user-friendly%2520deformation%2520to%2520create%2520novel%2520scenes%2520while%250Apreserving%2520fine%2520details%2520from%2520the%2520original%25203DGS%2520has%2520attracted%2520significant%250Aresearch%2520attention.%2520We%2520introduce%2520CAGE-GS%252C%2520a%2520cage-based%25203DGS%2520deformation%2520method%250Athat%2520seamlessly%2520aligns%2520a%2520source%25203DGS%2520scene%2520with%2520a%2520user-defined%2520target%2520shape.%250AOur%2520approach%2520learns%2520a%2520deformation%2520cage%2520from%2520the%2520target%252C%2520which%2520guides%2520the%250Ageometric%2520transformation%2520of%2520the%2520source%2520scene.%2520While%2520the%2520cages%2520effectively%250Acontrol%2520structural%2520alignment%252C%2520preserving%2520the%2520textural%2520appearance%2520of%25203DGS%250Aremains%2520challenging%2520due%2520to%2520the%2520complexity%2520of%2520covariance%2520parameters.%2520To%2520address%250Athis%252C%2520we%2520employ%2520a%2520Jacobian%2520matrix-based%2520strategy%2520to%2520update%2520the%2520covariance%250Aparameters%2520of%2520each%2520Gaussian%252C%2520ensuring%2520texture%2520fidelity%2520post-deformation.%2520Our%250Amethod%2520is%2520highly%2520flexible%252C%2520accommodating%2520various%2520target%2520shape%2520representations%252C%250Aincluding%2520texts%252C%2520images%252C%2520point%2520clouds%252C%2520meshes%2520and%25203DGS%2520models.%2520Extensive%250Aexperiments%2520and%2520ablation%2520studies%2520on%2520both%2520public%2520datasets%2520and%2520newly%2520proposed%250Ascenes%2520demonstrate%2520that%2520our%2520method%2520significantly%2520outperforms%2520existing%250Atechniques%2520in%2520both%2520efficiency%2520and%2520deformation%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12800v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAGE-GS%3A%20High-fidelity%20Cage%20Based%203D%20Gaussian%20Splatting%20Deformation&entry.906535625=Yifei%20Tong%20and%20Runze%20Tian%20and%20Xiao%20Han%20and%20Dingyao%20Liu%20and%20Fenggen%20Yu%20and%20Yan%20Zhang&entry.1292438233=%20%20As%203D%20Gaussian%20Splatting%20%283DGS%29%20gains%20popularity%20as%20a%203D%20representation%20of%0Areal%20scenes%2C%20enabling%20user-friendly%20deformation%20to%20create%20novel%20scenes%20while%0Apreserving%20fine%20details%20from%20the%20original%203DGS%20has%20attracted%20significant%0Aresearch%20attention.%20We%20introduce%20CAGE-GS%2C%20a%20cage-based%203DGS%20deformation%20method%0Athat%20seamlessly%20aligns%20a%20source%203DGS%20scene%20with%20a%20user-defined%20target%20shape.%0AOur%20approach%20learns%20a%20deformation%20cage%20from%20the%20target%2C%20which%20guides%20the%0Ageometric%20transformation%20of%20the%20source%20scene.%20While%20the%20cages%20effectively%0Acontrol%20structural%20alignment%2C%20preserving%20the%20textural%20appearance%20of%203DGS%0Aremains%20challenging%20due%20to%20the%20complexity%20of%20covariance%20parameters.%20To%20address%0Athis%2C%20we%20employ%20a%20Jacobian%20matrix-based%20strategy%20to%20update%20the%20covariance%0Aparameters%20of%20each%20Gaussian%2C%20ensuring%20texture%20fidelity%20post-deformation.%20Our%0Amethod%20is%20highly%20flexible%2C%20accommodating%20various%20target%20shape%20representations%2C%0Aincluding%20texts%2C%20images%2C%20point%20clouds%2C%20meshes%20and%203DGS%20models.%20Extensive%0Aexperiments%20and%20ablation%20studies%20on%20both%20public%20datasets%20and%20newly%20proposed%0Ascenes%20demonstrate%20that%20our%20method%20significantly%20outperforms%20existing%0Atechniques%20in%20both%20efficiency%20and%20deformation%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12800v1&entry.124074799=Read"},
{"title": "Novel Demonstration Generation with Gaussian Splatting Enables Robust\n  One-Shot Manipulation", "author": "Sizhe Yang and Wenye Yu and Jia Zeng and Jun Lv and Kerui Ren and Cewu Lu and Dahua Lin and Jiangmiao Pang", "abstract": "  Visuomotor policies learned from teleoperated demonstrations face challenges\nsuch as lengthy data collection, high costs, and limited data diversity.\nExisting approaches address these issues by augmenting image observations in\nRGB space or employing Real-to-Sim-to-Real pipelines based on physical\nsimulators. However, the former is constrained to 2D data augmentation, while\nthe latter suffers from imprecise physical simulation caused by inaccurate\ngeometric reconstruction. This paper introduces RoboSplat, a novel method that\ngenerates diverse, visually realistic demonstrations by directly manipulating\n3D Gaussians. Specifically, we reconstruct the scene through 3D Gaussian\nSplatting (3DGS), directly edit the reconstructed scene, and augment data\nacross six types of generalization with five techniques: 3D Gaussian\nreplacement for varying object types, scene appearance, and robot embodiments;\nequivariant transformations for different object poses; visual attribute\nediting for various lighting conditions; novel view synthesis for new camera\nperspectives; and 3D content generation for diverse object types. Comprehensive\nreal-world experiments demonstrate that RoboSplat significantly enhances the\ngeneralization of visuomotor policies under diverse disturbances. Notably,\nwhile policies trained on hundreds of real-world demonstrations with additional\n2D data augmentation achieve an average success rate of 57.2%, RoboSplat\nattains 87.8% in one-shot settings across six types of generalization in the\nreal world.\n", "link": "http://arxiv.org/abs/2504.13175v1", "date": "2025-04-17", "relevancy": 3.2468, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.673}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6625}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Novel%20Demonstration%20Generation%20with%20Gaussian%20Splatting%20Enables%20Robust%0A%20%20One-Shot%20Manipulation&body=Title%3A%20Novel%20Demonstration%20Generation%20with%20Gaussian%20Splatting%20Enables%20Robust%0A%20%20One-Shot%20Manipulation%0AAuthor%3A%20Sizhe%20Yang%20and%20Wenye%20Yu%20and%20Jia%20Zeng%20and%20Jun%20Lv%20and%20Kerui%20Ren%20and%20Cewu%20Lu%20and%20Dahua%20Lin%20and%20Jiangmiao%20Pang%0AAbstract%3A%20%20%20Visuomotor%20policies%20learned%20from%20teleoperated%20demonstrations%20face%20challenges%0Asuch%20as%20lengthy%20data%20collection%2C%20high%20costs%2C%20and%20limited%20data%20diversity.%0AExisting%20approaches%20address%20these%20issues%20by%20augmenting%20image%20observations%20in%0ARGB%20space%20or%20employing%20Real-to-Sim-to-Real%20pipelines%20based%20on%20physical%0Asimulators.%20However%2C%20the%20former%20is%20constrained%20to%202D%20data%20augmentation%2C%20while%0Athe%20latter%20suffers%20from%20imprecise%20physical%20simulation%20caused%20by%20inaccurate%0Ageometric%20reconstruction.%20This%20paper%20introduces%20RoboSplat%2C%20a%20novel%20method%20that%0Agenerates%20diverse%2C%20visually%20realistic%20demonstrations%20by%20directly%20manipulating%0A3D%20Gaussians.%20Specifically%2C%20we%20reconstruct%20the%20scene%20through%203D%20Gaussian%0ASplatting%20%283DGS%29%2C%20directly%20edit%20the%20reconstructed%20scene%2C%20and%20augment%20data%0Aacross%20six%20types%20of%20generalization%20with%20five%20techniques%3A%203D%20Gaussian%0Areplacement%20for%20varying%20object%20types%2C%20scene%20appearance%2C%20and%20robot%20embodiments%3B%0Aequivariant%20transformations%20for%20different%20object%20poses%3B%20visual%20attribute%0Aediting%20for%20various%20lighting%20conditions%3B%20novel%20view%20synthesis%20for%20new%20camera%0Aperspectives%3B%20and%203D%20content%20generation%20for%20diverse%20object%20types.%20Comprehensive%0Areal-world%20experiments%20demonstrate%20that%20RoboSplat%20significantly%20enhances%20the%0Ageneralization%20of%20visuomotor%20policies%20under%20diverse%20disturbances.%20Notably%2C%0Awhile%20policies%20trained%20on%20hundreds%20of%20real-world%20demonstrations%20with%20additional%0A2D%20data%20augmentation%20achieve%20an%20average%20success%20rate%20of%2057.2%25%2C%20RoboSplat%0Aattains%2087.8%25%20in%20one-shot%20settings%20across%20six%20types%20of%20generalization%20in%20the%0Areal%20world.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13175v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNovel%2520Demonstration%2520Generation%2520with%2520Gaussian%2520Splatting%2520Enables%2520Robust%250A%2520%2520One-Shot%2520Manipulation%26entry.906535625%3DSizhe%2520Yang%2520and%2520Wenye%2520Yu%2520and%2520Jia%2520Zeng%2520and%2520Jun%2520Lv%2520and%2520Kerui%2520Ren%2520and%2520Cewu%2520Lu%2520and%2520Dahua%2520Lin%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3D%2520%2520Visuomotor%2520policies%2520learned%2520from%2520teleoperated%2520demonstrations%2520face%2520challenges%250Asuch%2520as%2520lengthy%2520data%2520collection%252C%2520high%2520costs%252C%2520and%2520limited%2520data%2520diversity.%250AExisting%2520approaches%2520address%2520these%2520issues%2520by%2520augmenting%2520image%2520observations%2520in%250ARGB%2520space%2520or%2520employing%2520Real-to-Sim-to-Real%2520pipelines%2520based%2520on%2520physical%250Asimulators.%2520However%252C%2520the%2520former%2520is%2520constrained%2520to%25202D%2520data%2520augmentation%252C%2520while%250Athe%2520latter%2520suffers%2520from%2520imprecise%2520physical%2520simulation%2520caused%2520by%2520inaccurate%250Ageometric%2520reconstruction.%2520This%2520paper%2520introduces%2520RoboSplat%252C%2520a%2520novel%2520method%2520that%250Agenerates%2520diverse%252C%2520visually%2520realistic%2520demonstrations%2520by%2520directly%2520manipulating%250A3D%2520Gaussians.%2520Specifically%252C%2520we%2520reconstruct%2520the%2520scene%2520through%25203D%2520Gaussian%250ASplatting%2520%25283DGS%2529%252C%2520directly%2520edit%2520the%2520reconstructed%2520scene%252C%2520and%2520augment%2520data%250Aacross%2520six%2520types%2520of%2520generalization%2520with%2520five%2520techniques%253A%25203D%2520Gaussian%250Areplacement%2520for%2520varying%2520object%2520types%252C%2520scene%2520appearance%252C%2520and%2520robot%2520embodiments%253B%250Aequivariant%2520transformations%2520for%2520different%2520object%2520poses%253B%2520visual%2520attribute%250Aediting%2520for%2520various%2520lighting%2520conditions%253B%2520novel%2520view%2520synthesis%2520for%2520new%2520camera%250Aperspectives%253B%2520and%25203D%2520content%2520generation%2520for%2520diverse%2520object%2520types.%2520Comprehensive%250Areal-world%2520experiments%2520demonstrate%2520that%2520RoboSplat%2520significantly%2520enhances%2520the%250Ageneralization%2520of%2520visuomotor%2520policies%2520under%2520diverse%2520disturbances.%2520Notably%252C%250Awhile%2520policies%2520trained%2520on%2520hundreds%2520of%2520real-world%2520demonstrations%2520with%2520additional%250A2D%2520data%2520augmentation%2520achieve%2520an%2520average%2520success%2520rate%2520of%252057.2%2525%252C%2520RoboSplat%250Aattains%252087.8%2525%2520in%2520one-shot%2520settings%2520across%2520six%2520types%2520of%2520generalization%2520in%2520the%250Areal%2520world.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13175v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Novel%20Demonstration%20Generation%20with%20Gaussian%20Splatting%20Enables%20Robust%0A%20%20One-Shot%20Manipulation&entry.906535625=Sizhe%20Yang%20and%20Wenye%20Yu%20and%20Jia%20Zeng%20and%20Jun%20Lv%20and%20Kerui%20Ren%20and%20Cewu%20Lu%20and%20Dahua%20Lin%20and%20Jiangmiao%20Pang&entry.1292438233=%20%20Visuomotor%20policies%20learned%20from%20teleoperated%20demonstrations%20face%20challenges%0Asuch%20as%20lengthy%20data%20collection%2C%20high%20costs%2C%20and%20limited%20data%20diversity.%0AExisting%20approaches%20address%20these%20issues%20by%20augmenting%20image%20observations%20in%0ARGB%20space%20or%20employing%20Real-to-Sim-to-Real%20pipelines%20based%20on%20physical%0Asimulators.%20However%2C%20the%20former%20is%20constrained%20to%202D%20data%20augmentation%2C%20while%0Athe%20latter%20suffers%20from%20imprecise%20physical%20simulation%20caused%20by%20inaccurate%0Ageometric%20reconstruction.%20This%20paper%20introduces%20RoboSplat%2C%20a%20novel%20method%20that%0Agenerates%20diverse%2C%20visually%20realistic%20demonstrations%20by%20directly%20manipulating%0A3D%20Gaussians.%20Specifically%2C%20we%20reconstruct%20the%20scene%20through%203D%20Gaussian%0ASplatting%20%283DGS%29%2C%20directly%20edit%20the%20reconstructed%20scene%2C%20and%20augment%20data%0Aacross%20six%20types%20of%20generalization%20with%20five%20techniques%3A%203D%20Gaussian%0Areplacement%20for%20varying%20object%20types%2C%20scene%20appearance%2C%20and%20robot%20embodiments%3B%0Aequivariant%20transformations%20for%20different%20object%20poses%3B%20visual%20attribute%0Aediting%20for%20various%20lighting%20conditions%3B%20novel%20view%20synthesis%20for%20new%20camera%0Aperspectives%3B%20and%203D%20content%20generation%20for%20diverse%20object%20types.%20Comprehensive%0Areal-world%20experiments%20demonstrate%20that%20RoboSplat%20significantly%20enhances%20the%0Ageneralization%20of%20visuomotor%20policies%20under%20diverse%20disturbances.%20Notably%2C%0Awhile%20policies%20trained%20on%20hundreds%20of%20real-world%20demonstrations%20with%20additional%0A2D%20data%20augmentation%20achieve%20an%20average%20success%20rate%20of%2057.2%25%2C%20RoboSplat%0Aattains%2087.8%25%20in%20one-shot%20settings%20across%20six%20types%20of%20generalization%20in%20the%0Areal%20world.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13175v1&entry.124074799=Read"},
{"title": "ARAP-GS: Drag-driven As-Rigid-As-Possible 3D Gaussian Splatting Editing\n  with Diffusion Prior", "author": "Xiao Han and Runze Tian and Yifei Tong and Fenggen Yu and Dingyao Liu and Yan Zhang", "abstract": "  Drag-driven editing has become popular among designers for its ability to\nmodify complex geometric structures through simple and intuitive manipulation,\nallowing users to adjust and reshape content with minimal technical skill. This\ndrag operation has been incorporated into numerous methods to facilitate the\nediting of 2D images and 3D meshes in design. However, few studies have\nexplored drag-driven editing for the widely-used 3D Gaussian Splatting (3DGS)\nrepresentation, as deforming 3DGS while preserving shape coherence and visual\ncontinuity remains challenging. In this paper, we introduce ARAP-GS, a\ndrag-driven 3DGS editing framework based on As-Rigid-As-Possible (ARAP)\ndeformation. Unlike previous 3DGS editing methods, we are the first to apply\nARAP deformation directly to 3D Gaussians, enabling flexible, drag-driven\ngeometric transformations. To preserve scene appearance after deformation, we\nincorporate an advanced diffusion prior for image super-resolution within our\niterative optimization process. This approach enhances visual quality while\nmaintaining multi-view consistency in the edited results. Experiments show that\nARAP-GS outperforms current methods across diverse 3D scenes, demonstrating its\neffectiveness and superiority for drag-driven 3DGS editing. Additionally, our\nmethod is highly efficient, requiring only 10 to 20 minutes to edit a scene on\na single RTX 3090 GPU.\n", "link": "http://arxiv.org/abs/2504.12788v1", "date": "2025-04-17", "relevancy": 3.1811, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.664}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6337}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ARAP-GS%3A%20Drag-driven%20As-Rigid-As-Possible%203D%20Gaussian%20Splatting%20Editing%0A%20%20with%20Diffusion%20Prior&body=Title%3A%20ARAP-GS%3A%20Drag-driven%20As-Rigid-As-Possible%203D%20Gaussian%20Splatting%20Editing%0A%20%20with%20Diffusion%20Prior%0AAuthor%3A%20Xiao%20Han%20and%20Runze%20Tian%20and%20Yifei%20Tong%20and%20Fenggen%20Yu%20and%20Dingyao%20Liu%20and%20Yan%20Zhang%0AAbstract%3A%20%20%20Drag-driven%20editing%20has%20become%20popular%20among%20designers%20for%20its%20ability%20to%0Amodify%20complex%20geometric%20structures%20through%20simple%20and%20intuitive%20manipulation%2C%0Aallowing%20users%20to%20adjust%20and%20reshape%20content%20with%20minimal%20technical%20skill.%20This%0Adrag%20operation%20has%20been%20incorporated%20into%20numerous%20methods%20to%20facilitate%20the%0Aediting%20of%202D%20images%20and%203D%20meshes%20in%20design.%20However%2C%20few%20studies%20have%0Aexplored%20drag-driven%20editing%20for%20the%20widely-used%203D%20Gaussian%20Splatting%20%283DGS%29%0Arepresentation%2C%20as%20deforming%203DGS%20while%20preserving%20shape%20coherence%20and%20visual%0Acontinuity%20remains%20challenging.%20In%20this%20paper%2C%20we%20introduce%20ARAP-GS%2C%20a%0Adrag-driven%203DGS%20editing%20framework%20based%20on%20As-Rigid-As-Possible%20%28ARAP%29%0Adeformation.%20Unlike%20previous%203DGS%20editing%20methods%2C%20we%20are%20the%20first%20to%20apply%0AARAP%20deformation%20directly%20to%203D%20Gaussians%2C%20enabling%20flexible%2C%20drag-driven%0Ageometric%20transformations.%20To%20preserve%20scene%20appearance%20after%20deformation%2C%20we%0Aincorporate%20an%20advanced%20diffusion%20prior%20for%20image%20super-resolution%20within%20our%0Aiterative%20optimization%20process.%20This%20approach%20enhances%20visual%20quality%20while%0Amaintaining%20multi-view%20consistency%20in%20the%20edited%20results.%20Experiments%20show%20that%0AARAP-GS%20outperforms%20current%20methods%20across%20diverse%203D%20scenes%2C%20demonstrating%20its%0Aeffectiveness%20and%20superiority%20for%20drag-driven%203DGS%20editing.%20Additionally%2C%20our%0Amethod%20is%20highly%20efficient%2C%20requiring%20only%2010%20to%2020%20minutes%20to%20edit%20a%20scene%20on%0Aa%20single%20RTX%203090%20GPU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12788v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DARAP-GS%253A%2520Drag-driven%2520As-Rigid-As-Possible%25203D%2520Gaussian%2520Splatting%2520Editing%250A%2520%2520with%2520Diffusion%2520Prior%26entry.906535625%3DXiao%2520Han%2520and%2520Runze%2520Tian%2520and%2520Yifei%2520Tong%2520and%2520Fenggen%2520Yu%2520and%2520Dingyao%2520Liu%2520and%2520Yan%2520Zhang%26entry.1292438233%3D%2520%2520Drag-driven%2520editing%2520has%2520become%2520popular%2520among%2520designers%2520for%2520its%2520ability%2520to%250Amodify%2520complex%2520geometric%2520structures%2520through%2520simple%2520and%2520intuitive%2520manipulation%252C%250Aallowing%2520users%2520to%2520adjust%2520and%2520reshape%2520content%2520with%2520minimal%2520technical%2520skill.%2520This%250Adrag%2520operation%2520has%2520been%2520incorporated%2520into%2520numerous%2520methods%2520to%2520facilitate%2520the%250Aediting%2520of%25202D%2520images%2520and%25203D%2520meshes%2520in%2520design.%2520However%252C%2520few%2520studies%2520have%250Aexplored%2520drag-driven%2520editing%2520for%2520the%2520widely-used%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%250Arepresentation%252C%2520as%2520deforming%25203DGS%2520while%2520preserving%2520shape%2520coherence%2520and%2520visual%250Acontinuity%2520remains%2520challenging.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520ARAP-GS%252C%2520a%250Adrag-driven%25203DGS%2520editing%2520framework%2520based%2520on%2520As-Rigid-As-Possible%2520%2528ARAP%2529%250Adeformation.%2520Unlike%2520previous%25203DGS%2520editing%2520methods%252C%2520we%2520are%2520the%2520first%2520to%2520apply%250AARAP%2520deformation%2520directly%2520to%25203D%2520Gaussians%252C%2520enabling%2520flexible%252C%2520drag-driven%250Ageometric%2520transformations.%2520To%2520preserve%2520scene%2520appearance%2520after%2520deformation%252C%2520we%250Aincorporate%2520an%2520advanced%2520diffusion%2520prior%2520for%2520image%2520super-resolution%2520within%2520our%250Aiterative%2520optimization%2520process.%2520This%2520approach%2520enhances%2520visual%2520quality%2520while%250Amaintaining%2520multi-view%2520consistency%2520in%2520the%2520edited%2520results.%2520Experiments%2520show%2520that%250AARAP-GS%2520outperforms%2520current%2520methods%2520across%2520diverse%25203D%2520scenes%252C%2520demonstrating%2520its%250Aeffectiveness%2520and%2520superiority%2520for%2520drag-driven%25203DGS%2520editing.%2520Additionally%252C%2520our%250Amethod%2520is%2520highly%2520efficient%252C%2520requiring%2520only%252010%2520to%252020%2520minutes%2520to%2520edit%2520a%2520scene%2520on%250Aa%2520single%2520RTX%25203090%2520GPU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12788v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ARAP-GS%3A%20Drag-driven%20As-Rigid-As-Possible%203D%20Gaussian%20Splatting%20Editing%0A%20%20with%20Diffusion%20Prior&entry.906535625=Xiao%20Han%20and%20Runze%20Tian%20and%20Yifei%20Tong%20and%20Fenggen%20Yu%20and%20Dingyao%20Liu%20and%20Yan%20Zhang&entry.1292438233=%20%20Drag-driven%20editing%20has%20become%20popular%20among%20designers%20for%20its%20ability%20to%0Amodify%20complex%20geometric%20structures%20through%20simple%20and%20intuitive%20manipulation%2C%0Aallowing%20users%20to%20adjust%20and%20reshape%20content%20with%20minimal%20technical%20skill.%20This%0Adrag%20operation%20has%20been%20incorporated%20into%20numerous%20methods%20to%20facilitate%20the%0Aediting%20of%202D%20images%20and%203D%20meshes%20in%20design.%20However%2C%20few%20studies%20have%0Aexplored%20drag-driven%20editing%20for%20the%20widely-used%203D%20Gaussian%20Splatting%20%283DGS%29%0Arepresentation%2C%20as%20deforming%203DGS%20while%20preserving%20shape%20coherence%20and%20visual%0Acontinuity%20remains%20challenging.%20In%20this%20paper%2C%20we%20introduce%20ARAP-GS%2C%20a%0Adrag-driven%203DGS%20editing%20framework%20based%20on%20As-Rigid-As-Possible%20%28ARAP%29%0Adeformation.%20Unlike%20previous%203DGS%20editing%20methods%2C%20we%20are%20the%20first%20to%20apply%0AARAP%20deformation%20directly%20to%203D%20Gaussians%2C%20enabling%20flexible%2C%20drag-driven%0Ageometric%20transformations.%20To%20preserve%20scene%20appearance%20after%20deformation%2C%20we%0Aincorporate%20an%20advanced%20diffusion%20prior%20for%20image%20super-resolution%20within%20our%0Aiterative%20optimization%20process.%20This%20approach%20enhances%20visual%20quality%20while%0Amaintaining%20multi-view%20consistency%20in%20the%20edited%20results.%20Experiments%20show%20that%0AARAP-GS%20outperforms%20current%20methods%20across%20diverse%203D%20scenes%2C%20demonstrating%20its%0Aeffectiveness%20and%20superiority%20for%20drag-driven%203DGS%20editing.%20Additionally%2C%20our%0Amethod%20is%20highly%20efficient%2C%20requiring%20only%2010%20to%2020%20minutes%20to%20edit%20a%20scene%20on%0Aa%20single%20RTX%203090%20GPU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12788v1&entry.124074799=Read"},
{"title": "Structured 3D Latents for Scalable and Versatile 3D Generation", "author": "Jianfeng Xiang and Zelong Lv and Sicheng Xu and Yu Deng and Ruicheng Wang and Bowen Zhang and Dong Chen and Xin Tong and Jiaolong Yang", "abstract": "  We introduce a novel 3D generation method for versatile and high-quality 3D\nasset creation. The cornerstone is a unified Structured LATent (SLAT)\nrepresentation which allows decoding to different output formats, such as\nRadiance Fields, 3D Gaussians, and meshes. This is achieved by integrating a\nsparsely-populated 3D grid with dense multiview visual features extracted from\na powerful vision foundation model, comprehensively capturing both structural\n(geometry) and textural (appearance) information while maintaining flexibility\nduring decoding. We employ rectified flow transformers tailored for SLAT as our\n3D generation models and train models with up to 2 billion parameters on a\nlarge 3D asset dataset of 500K diverse objects. Our model generates\nhigh-quality results with text or image conditions, significantly surpassing\nexisting methods, including recent ones at similar scales. We showcase flexible\noutput format selection and local 3D editing capabilities which were not\noffered by previous models. Code, model, and data will be released.\n", "link": "http://arxiv.org/abs/2412.01506v2", "date": "2025-04-17", "relevancy": 3.1488, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6443}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6225}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structured%203D%20Latents%20for%20Scalable%20and%20Versatile%203D%20Generation&body=Title%3A%20Structured%203D%20Latents%20for%20Scalable%20and%20Versatile%203D%20Generation%0AAuthor%3A%20Jianfeng%20Xiang%20and%20Zelong%20Lv%20and%20Sicheng%20Xu%20and%20Yu%20Deng%20and%20Ruicheng%20Wang%20and%20Bowen%20Zhang%20and%20Dong%20Chen%20and%20Xin%20Tong%20and%20Jiaolong%20Yang%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%203D%20generation%20method%20for%20versatile%20and%20high-quality%203D%0Aasset%20creation.%20The%20cornerstone%20is%20a%20unified%20Structured%20LATent%20%28SLAT%29%0Arepresentation%20which%20allows%20decoding%20to%20different%20output%20formats%2C%20such%20as%0ARadiance%20Fields%2C%203D%20Gaussians%2C%20and%20meshes.%20This%20is%20achieved%20by%20integrating%20a%0Asparsely-populated%203D%20grid%20with%20dense%20multiview%20visual%20features%20extracted%20from%0Aa%20powerful%20vision%20foundation%20model%2C%20comprehensively%20capturing%20both%20structural%0A%28geometry%29%20and%20textural%20%28appearance%29%20information%20while%20maintaining%20flexibility%0Aduring%20decoding.%20We%20employ%20rectified%20flow%20transformers%20tailored%20for%20SLAT%20as%20our%0A3D%20generation%20models%20and%20train%20models%20with%20up%20to%202%20billion%20parameters%20on%20a%0Alarge%203D%20asset%20dataset%20of%20500K%20diverse%20objects.%20Our%20model%20generates%0Ahigh-quality%20results%20with%20text%20or%20image%20conditions%2C%20significantly%20surpassing%0Aexisting%20methods%2C%20including%20recent%20ones%20at%20similar%20scales.%20We%20showcase%20flexible%0Aoutput%20format%20selection%20and%20local%203D%20editing%20capabilities%20which%20were%20not%0Aoffered%20by%20previous%20models.%20Code%2C%20model%2C%20and%20data%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.01506v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructured%25203D%2520Latents%2520for%2520Scalable%2520and%2520Versatile%25203D%2520Generation%26entry.906535625%3DJianfeng%2520Xiang%2520and%2520Zelong%2520Lv%2520and%2520Sicheng%2520Xu%2520and%2520Yu%2520Deng%2520and%2520Ruicheng%2520Wang%2520and%2520Bowen%2520Zhang%2520and%2520Dong%2520Chen%2520and%2520Xin%2520Tong%2520and%2520Jiaolong%2520Yang%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%25203D%2520generation%2520method%2520for%2520versatile%2520and%2520high-quality%25203D%250Aasset%2520creation.%2520The%2520cornerstone%2520is%2520a%2520unified%2520Structured%2520LATent%2520%2528SLAT%2529%250Arepresentation%2520which%2520allows%2520decoding%2520to%2520different%2520output%2520formats%252C%2520such%2520as%250ARadiance%2520Fields%252C%25203D%2520Gaussians%252C%2520and%2520meshes.%2520This%2520is%2520achieved%2520by%2520integrating%2520a%250Asparsely-populated%25203D%2520grid%2520with%2520dense%2520multiview%2520visual%2520features%2520extracted%2520from%250Aa%2520powerful%2520vision%2520foundation%2520model%252C%2520comprehensively%2520capturing%2520both%2520structural%250A%2528geometry%2529%2520and%2520textural%2520%2528appearance%2529%2520information%2520while%2520maintaining%2520flexibility%250Aduring%2520decoding.%2520We%2520employ%2520rectified%2520flow%2520transformers%2520tailored%2520for%2520SLAT%2520as%2520our%250A3D%2520generation%2520models%2520and%2520train%2520models%2520with%2520up%2520to%25202%2520billion%2520parameters%2520on%2520a%250Alarge%25203D%2520asset%2520dataset%2520of%2520500K%2520diverse%2520objects.%2520Our%2520model%2520generates%250Ahigh-quality%2520results%2520with%2520text%2520or%2520image%2520conditions%252C%2520significantly%2520surpassing%250Aexisting%2520methods%252C%2520including%2520recent%2520ones%2520at%2520similar%2520scales.%2520We%2520showcase%2520flexible%250Aoutput%2520format%2520selection%2520and%2520local%25203D%2520editing%2520capabilities%2520which%2520were%2520not%250Aoffered%2520by%2520previous%2520models.%2520Code%252C%2520model%252C%2520and%2520data%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01506v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structured%203D%20Latents%20for%20Scalable%20and%20Versatile%203D%20Generation&entry.906535625=Jianfeng%20Xiang%20and%20Zelong%20Lv%20and%20Sicheng%20Xu%20and%20Yu%20Deng%20and%20Ruicheng%20Wang%20and%20Bowen%20Zhang%20and%20Dong%20Chen%20and%20Xin%20Tong%20and%20Jiaolong%20Yang&entry.1292438233=%20%20We%20introduce%20a%20novel%203D%20generation%20method%20for%20versatile%20and%20high-quality%203D%0Aasset%20creation.%20The%20cornerstone%20is%20a%20unified%20Structured%20LATent%20%28SLAT%29%0Arepresentation%20which%20allows%20decoding%20to%20different%20output%20formats%2C%20such%20as%0ARadiance%20Fields%2C%203D%20Gaussians%2C%20and%20meshes.%20This%20is%20achieved%20by%20integrating%20a%0Asparsely-populated%203D%20grid%20with%20dense%20multiview%20visual%20features%20extracted%20from%0Aa%20powerful%20vision%20foundation%20model%2C%20comprehensively%20capturing%20both%20structural%0A%28geometry%29%20and%20textural%20%28appearance%29%20information%20while%20maintaining%20flexibility%0Aduring%20decoding.%20We%20employ%20rectified%20flow%20transformers%20tailored%20for%20SLAT%20as%20our%0A3D%20generation%20models%20and%20train%20models%20with%20up%20to%202%20billion%20parameters%20on%20a%0Alarge%203D%20asset%20dataset%20of%20500K%20diverse%20objects.%20Our%20model%20generates%0Ahigh-quality%20results%20with%20text%20or%20image%20conditions%2C%20significantly%20surpassing%0Aexisting%20methods%2C%20including%20recent%20ones%20at%20similar%20scales.%20We%20showcase%20flexible%0Aoutput%20format%20selection%20and%20local%203D%20editing%20capabilities%20which%20were%20not%0Aoffered%20by%20previous%20models.%20Code%2C%20model%2C%20and%20data%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.01506v2&entry.124074799=Read"},
{"title": "PerceptionLM: Open-Access Data and Models for Detailed Visual\n  Understanding", "author": "Jang Hyun Cho and Andrea Madotto and Effrosyni Mavroudi and Triantafyllos Afouras and Tushar Nagarajan and Muhammad Maaz and Yale Song and Tengyu Ma and Shuming Hu and Suyog Jain and Miguel Martin and Huiyu Wang and Hanoona Rasheed and Peize Sun and Po-Yao Huang and Daniel Bolya and Nikhila Ravi and Shashank Jain and Tammy Stark and Shane Moon and Babak Damavandi and Vivian Lee and Andrew Westbury and Salman Khan and Philipp Kr\u00e4henb\u00fchl and Piotr Doll\u00e1r and Lorenzo Torresani and Kristen Grauman and Christoph Feichtenhofer", "abstract": "  Vision-language models are integral to computer vision research, yet many\nhigh-performing models remain closed-source, obscuring their data, design and\ntraining recipe. The research community has responded by using distillation\nfrom black-box models to label training data, achieving strong benchmark\nresults, at the cost of measurable scientific progress. However, without\nknowing the details of the teacher model and its data sources, scientific\nprogress remains difficult to measure. In this paper, we study building a\nPerception Language Model (PLM) in a fully open and reproducible framework for\ntransparent research in image and video understanding. We analyze standard\ntraining pipelines without distillation from proprietary models and explore\nlarge-scale synthetic data to identify critical data gaps, particularly in\ndetailed video understanding. To bridge these gaps, we release 2.8M\nhuman-labeled instances of fine-grained video question-answer pairs and\nspatio-temporally grounded video captions. Additionally, we introduce\nPLM-VideoBench, a suite for evaluating challenging video understanding tasks\nfocusing on the ability to reason about \"what\", \"where\", \"when\", and \"how\" of a\nvideo. We make our work fully reproducible by providing data, training recipes,\ncode & models.\n", "link": "http://arxiv.org/abs/2504.13180v1", "date": "2025-04-17", "relevancy": 3.113, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.639}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.639}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PerceptionLM%3A%20Open-Access%20Data%20and%20Models%20for%20Detailed%20Visual%0A%20%20Understanding&body=Title%3A%20PerceptionLM%3A%20Open-Access%20Data%20and%20Models%20for%20Detailed%20Visual%0A%20%20Understanding%0AAuthor%3A%20Jang%20Hyun%20Cho%20and%20Andrea%20Madotto%20and%20Effrosyni%20Mavroudi%20and%20Triantafyllos%20Afouras%20and%20Tushar%20Nagarajan%20and%20Muhammad%20Maaz%20and%20Yale%20Song%20and%20Tengyu%20Ma%20and%20Shuming%20Hu%20and%20Suyog%20Jain%20and%20Miguel%20Martin%20and%20Huiyu%20Wang%20and%20Hanoona%20Rasheed%20and%20Peize%20Sun%20and%20Po-Yao%20Huang%20and%20Daniel%20Bolya%20and%20Nikhila%20Ravi%20and%20Shashank%20Jain%20and%20Tammy%20Stark%20and%20Shane%20Moon%20and%20Babak%20Damavandi%20and%20Vivian%20Lee%20and%20Andrew%20Westbury%20and%20Salman%20Khan%20and%20Philipp%20Kr%C3%A4henb%C3%BChl%20and%20Piotr%20Doll%C3%A1r%20and%20Lorenzo%20Torresani%20and%20Kristen%20Grauman%20and%20Christoph%20Feichtenhofer%0AAbstract%3A%20%20%20Vision-language%20models%20are%20integral%20to%20computer%20vision%20research%2C%20yet%20many%0Ahigh-performing%20models%20remain%20closed-source%2C%20obscuring%20their%20data%2C%20design%20and%0Atraining%20recipe.%20The%20research%20community%20has%20responded%20by%20using%20distillation%0Afrom%20black-box%20models%20to%20label%20training%20data%2C%20achieving%20strong%20benchmark%0Aresults%2C%20at%20the%20cost%20of%20measurable%20scientific%20progress.%20However%2C%20without%0Aknowing%20the%20details%20of%20the%20teacher%20model%20and%20its%20data%20sources%2C%20scientific%0Aprogress%20remains%20difficult%20to%20measure.%20In%20this%20paper%2C%20we%20study%20building%20a%0APerception%20Language%20Model%20%28PLM%29%20in%20a%20fully%20open%20and%20reproducible%20framework%20for%0Atransparent%20research%20in%20image%20and%20video%20understanding.%20We%20analyze%20standard%0Atraining%20pipelines%20without%20distillation%20from%20proprietary%20models%20and%20explore%0Alarge-scale%20synthetic%20data%20to%20identify%20critical%20data%20gaps%2C%20particularly%20in%0Adetailed%20video%20understanding.%20To%20bridge%20these%20gaps%2C%20we%20release%202.8M%0Ahuman-labeled%20instances%20of%20fine-grained%20video%20question-answer%20pairs%20and%0Aspatio-temporally%20grounded%20video%20captions.%20Additionally%2C%20we%20introduce%0APLM-VideoBench%2C%20a%20suite%20for%20evaluating%20challenging%20video%20understanding%20tasks%0Afocusing%20on%20the%20ability%20to%20reason%20about%20%22what%22%2C%20%22where%22%2C%20%22when%22%2C%20and%20%22how%22%20of%20a%0Avideo.%20We%20make%20our%20work%20fully%20reproducible%20by%20providing%20data%2C%20training%20recipes%2C%0Acode%20%26%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13180v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerceptionLM%253A%2520Open-Access%2520Data%2520and%2520Models%2520for%2520Detailed%2520Visual%250A%2520%2520Understanding%26entry.906535625%3DJang%2520Hyun%2520Cho%2520and%2520Andrea%2520Madotto%2520and%2520Effrosyni%2520Mavroudi%2520and%2520Triantafyllos%2520Afouras%2520and%2520Tushar%2520Nagarajan%2520and%2520Muhammad%2520Maaz%2520and%2520Yale%2520Song%2520and%2520Tengyu%2520Ma%2520and%2520Shuming%2520Hu%2520and%2520Suyog%2520Jain%2520and%2520Miguel%2520Martin%2520and%2520Huiyu%2520Wang%2520and%2520Hanoona%2520Rasheed%2520and%2520Peize%2520Sun%2520and%2520Po-Yao%2520Huang%2520and%2520Daniel%2520Bolya%2520and%2520Nikhila%2520Ravi%2520and%2520Shashank%2520Jain%2520and%2520Tammy%2520Stark%2520and%2520Shane%2520Moon%2520and%2520Babak%2520Damavandi%2520and%2520Vivian%2520Lee%2520and%2520Andrew%2520Westbury%2520and%2520Salman%2520Khan%2520and%2520Philipp%2520Kr%25C3%25A4henb%25C3%25BChl%2520and%2520Piotr%2520Doll%25C3%25A1r%2520and%2520Lorenzo%2520Torresani%2520and%2520Kristen%2520Grauman%2520and%2520Christoph%2520Feichtenhofer%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520are%2520integral%2520to%2520computer%2520vision%2520research%252C%2520yet%2520many%250Ahigh-performing%2520models%2520remain%2520closed-source%252C%2520obscuring%2520their%2520data%252C%2520design%2520and%250Atraining%2520recipe.%2520The%2520research%2520community%2520has%2520responded%2520by%2520using%2520distillation%250Afrom%2520black-box%2520models%2520to%2520label%2520training%2520data%252C%2520achieving%2520strong%2520benchmark%250Aresults%252C%2520at%2520the%2520cost%2520of%2520measurable%2520scientific%2520progress.%2520However%252C%2520without%250Aknowing%2520the%2520details%2520of%2520the%2520teacher%2520model%2520and%2520its%2520data%2520sources%252C%2520scientific%250Aprogress%2520remains%2520difficult%2520to%2520measure.%2520In%2520this%2520paper%252C%2520we%2520study%2520building%2520a%250APerception%2520Language%2520Model%2520%2528PLM%2529%2520in%2520a%2520fully%2520open%2520and%2520reproducible%2520framework%2520for%250Atransparent%2520research%2520in%2520image%2520and%2520video%2520understanding.%2520We%2520analyze%2520standard%250Atraining%2520pipelines%2520without%2520distillation%2520from%2520proprietary%2520models%2520and%2520explore%250Alarge-scale%2520synthetic%2520data%2520to%2520identify%2520critical%2520data%2520gaps%252C%2520particularly%2520in%250Adetailed%2520video%2520understanding.%2520To%2520bridge%2520these%2520gaps%252C%2520we%2520release%25202.8M%250Ahuman-labeled%2520instances%2520of%2520fine-grained%2520video%2520question-answer%2520pairs%2520and%250Aspatio-temporally%2520grounded%2520video%2520captions.%2520Additionally%252C%2520we%2520introduce%250APLM-VideoBench%252C%2520a%2520suite%2520for%2520evaluating%2520challenging%2520video%2520understanding%2520tasks%250Afocusing%2520on%2520the%2520ability%2520to%2520reason%2520about%2520%2522what%2522%252C%2520%2522where%2522%252C%2520%2522when%2522%252C%2520and%2520%2522how%2522%2520of%2520a%250Avideo.%2520We%2520make%2520our%2520work%2520fully%2520reproducible%2520by%2520providing%2520data%252C%2520training%2520recipes%252C%250Acode%2520%2526%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13180v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PerceptionLM%3A%20Open-Access%20Data%20and%20Models%20for%20Detailed%20Visual%0A%20%20Understanding&entry.906535625=Jang%20Hyun%20Cho%20and%20Andrea%20Madotto%20and%20Effrosyni%20Mavroudi%20and%20Triantafyllos%20Afouras%20and%20Tushar%20Nagarajan%20and%20Muhammad%20Maaz%20and%20Yale%20Song%20and%20Tengyu%20Ma%20and%20Shuming%20Hu%20and%20Suyog%20Jain%20and%20Miguel%20Martin%20and%20Huiyu%20Wang%20and%20Hanoona%20Rasheed%20and%20Peize%20Sun%20and%20Po-Yao%20Huang%20and%20Daniel%20Bolya%20and%20Nikhila%20Ravi%20and%20Shashank%20Jain%20and%20Tammy%20Stark%20and%20Shane%20Moon%20and%20Babak%20Damavandi%20and%20Vivian%20Lee%20and%20Andrew%20Westbury%20and%20Salman%20Khan%20and%20Philipp%20Kr%C3%A4henb%C3%BChl%20and%20Piotr%20Doll%C3%A1r%20and%20Lorenzo%20Torresani%20and%20Kristen%20Grauman%20and%20Christoph%20Feichtenhofer&entry.1292438233=%20%20Vision-language%20models%20are%20integral%20to%20computer%20vision%20research%2C%20yet%20many%0Ahigh-performing%20models%20remain%20closed-source%2C%20obscuring%20their%20data%2C%20design%20and%0Atraining%20recipe.%20The%20research%20community%20has%20responded%20by%20using%20distillation%0Afrom%20black-box%20models%20to%20label%20training%20data%2C%20achieving%20strong%20benchmark%0Aresults%2C%20at%20the%20cost%20of%20measurable%20scientific%20progress.%20However%2C%20without%0Aknowing%20the%20details%20of%20the%20teacher%20model%20and%20its%20data%20sources%2C%20scientific%0Aprogress%20remains%20difficult%20to%20measure.%20In%20this%20paper%2C%20we%20study%20building%20a%0APerception%20Language%20Model%20%28PLM%29%20in%20a%20fully%20open%20and%20reproducible%20framework%20for%0Atransparent%20research%20in%20image%20and%20video%20understanding.%20We%20analyze%20standard%0Atraining%20pipelines%20without%20distillation%20from%20proprietary%20models%20and%20explore%0Alarge-scale%20synthetic%20data%20to%20identify%20critical%20data%20gaps%2C%20particularly%20in%0Adetailed%20video%20understanding.%20To%20bridge%20these%20gaps%2C%20we%20release%202.8M%0Ahuman-labeled%20instances%20of%20fine-grained%20video%20question-answer%20pairs%20and%0Aspatio-temporally%20grounded%20video%20captions.%20Additionally%2C%20we%20introduce%0APLM-VideoBench%2C%20a%20suite%20for%20evaluating%20challenging%20video%20understanding%20tasks%0Afocusing%20on%20the%20ability%20to%20reason%20about%20%22what%22%2C%20%22where%22%2C%20%22when%22%2C%20and%20%22how%22%20of%20a%0Avideo.%20We%20make%20our%20work%20fully%20reproducible%20by%20providing%20data%2C%20training%20recipes%2C%0Acode%20%26%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13180v1&entry.124074799=Read"},
{"title": "HiScene: Creating Hierarchical 3D Scenes with Isometric View Generation", "author": "Wenqi Dong and Bangbang Yang and Zesong Yang and Yuan Li and Tao Hu and Hujun Bao and Yuewen Ma and Zhaopeng Cui", "abstract": "  Scene-level 3D generation represents a critical frontier in multimedia and\ncomputer graphics, yet existing approaches either suffer from limited object\ncategories or lack editing flexibility for interactive applications. In this\npaper, we present HiScene, a novel hierarchical framework that bridges the gap\nbetween 2D image generation and 3D object generation and delivers high-fidelity\nscenes with compositional identities and aesthetic scene content. Our key\ninsight is treating scenes as hierarchical \"objects\" under isometric views,\nwhere a room functions as a complex object that can be further decomposed into\nmanipulatable items. This hierarchical approach enables us to generate 3D\ncontent that aligns with 2D representations while maintaining compositional\nstructure. To ensure completeness and spatial alignment of each decomposed\ninstance, we develop a video-diffusion-based amodal completion technique that\neffectively handles occlusions and shadows between objects, and introduce shape\nprior injection to ensure spatial coherence within the scene. Experimental\nresults demonstrate that our method produces more natural object arrangements\nand complete object instances suitable for interactive applications, while\nmaintaining physical plausibility and alignment with user inputs.\n", "link": "http://arxiv.org/abs/2504.13072v1", "date": "2025-04-17", "relevancy": 3.0888, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6369}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6369}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HiScene%3A%20Creating%20Hierarchical%203D%20Scenes%20with%20Isometric%20View%20Generation&body=Title%3A%20HiScene%3A%20Creating%20Hierarchical%203D%20Scenes%20with%20Isometric%20View%20Generation%0AAuthor%3A%20Wenqi%20Dong%20and%20Bangbang%20Yang%20and%20Zesong%20Yang%20and%20Yuan%20Li%20and%20Tao%20Hu%20and%20Hujun%20Bao%20and%20Yuewen%20Ma%20and%20Zhaopeng%20Cui%0AAbstract%3A%20%20%20Scene-level%203D%20generation%20represents%20a%20critical%20frontier%20in%20multimedia%20and%0Acomputer%20graphics%2C%20yet%20existing%20approaches%20either%20suffer%20from%20limited%20object%0Acategories%20or%20lack%20editing%20flexibility%20for%20interactive%20applications.%20In%20this%0Apaper%2C%20we%20present%20HiScene%2C%20a%20novel%20hierarchical%20framework%20that%20bridges%20the%20gap%0Abetween%202D%20image%20generation%20and%203D%20object%20generation%20and%20delivers%20high-fidelity%0Ascenes%20with%20compositional%20identities%20and%20aesthetic%20scene%20content.%20Our%20key%0Ainsight%20is%20treating%20scenes%20as%20hierarchical%20%22objects%22%20under%20isometric%20views%2C%0Awhere%20a%20room%20functions%20as%20a%20complex%20object%20that%20can%20be%20further%20decomposed%20into%0Amanipulatable%20items.%20This%20hierarchical%20approach%20enables%20us%20to%20generate%203D%0Acontent%20that%20aligns%20with%202D%20representations%20while%20maintaining%20compositional%0Astructure.%20To%20ensure%20completeness%20and%20spatial%20alignment%20of%20each%20decomposed%0Ainstance%2C%20we%20develop%20a%20video-diffusion-based%20amodal%20completion%20technique%20that%0Aeffectively%20handles%20occlusions%20and%20shadows%20between%20objects%2C%20and%20introduce%20shape%0Aprior%20injection%20to%20ensure%20spatial%20coherence%20within%20the%20scene.%20Experimental%0Aresults%20demonstrate%20that%20our%20method%20produces%20more%20natural%20object%20arrangements%0Aand%20complete%20object%20instances%20suitable%20for%20interactive%20applications%2C%20while%0Amaintaining%20physical%20plausibility%20and%20alignment%20with%20user%20inputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13072v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHiScene%253A%2520Creating%2520Hierarchical%25203D%2520Scenes%2520with%2520Isometric%2520View%2520Generation%26entry.906535625%3DWenqi%2520Dong%2520and%2520Bangbang%2520Yang%2520and%2520Zesong%2520Yang%2520and%2520Yuan%2520Li%2520and%2520Tao%2520Hu%2520and%2520Hujun%2520Bao%2520and%2520Yuewen%2520Ma%2520and%2520Zhaopeng%2520Cui%26entry.1292438233%3D%2520%2520Scene-level%25203D%2520generation%2520represents%2520a%2520critical%2520frontier%2520in%2520multimedia%2520and%250Acomputer%2520graphics%252C%2520yet%2520existing%2520approaches%2520either%2520suffer%2520from%2520limited%2520object%250Acategories%2520or%2520lack%2520editing%2520flexibility%2520for%2520interactive%2520applications.%2520In%2520this%250Apaper%252C%2520we%2520present%2520HiScene%252C%2520a%2520novel%2520hierarchical%2520framework%2520that%2520bridges%2520the%2520gap%250Abetween%25202D%2520image%2520generation%2520and%25203D%2520object%2520generation%2520and%2520delivers%2520high-fidelity%250Ascenes%2520with%2520compositional%2520identities%2520and%2520aesthetic%2520scene%2520content.%2520Our%2520key%250Ainsight%2520is%2520treating%2520scenes%2520as%2520hierarchical%2520%2522objects%2522%2520under%2520isometric%2520views%252C%250Awhere%2520a%2520room%2520functions%2520as%2520a%2520complex%2520object%2520that%2520can%2520be%2520further%2520decomposed%2520into%250Amanipulatable%2520items.%2520This%2520hierarchical%2520approach%2520enables%2520us%2520to%2520generate%25203D%250Acontent%2520that%2520aligns%2520with%25202D%2520representations%2520while%2520maintaining%2520compositional%250Astructure.%2520To%2520ensure%2520completeness%2520and%2520spatial%2520alignment%2520of%2520each%2520decomposed%250Ainstance%252C%2520we%2520develop%2520a%2520video-diffusion-based%2520amodal%2520completion%2520technique%2520that%250Aeffectively%2520handles%2520occlusions%2520and%2520shadows%2520between%2520objects%252C%2520and%2520introduce%2520shape%250Aprior%2520injection%2520to%2520ensure%2520spatial%2520coherence%2520within%2520the%2520scene.%2520Experimental%250Aresults%2520demonstrate%2520that%2520our%2520method%2520produces%2520more%2520natural%2520object%2520arrangements%250Aand%2520complete%2520object%2520instances%2520suitable%2520for%2520interactive%2520applications%252C%2520while%250Amaintaining%2520physical%2520plausibility%2520and%2520alignment%2520with%2520user%2520inputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13072v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiScene%3A%20Creating%20Hierarchical%203D%20Scenes%20with%20Isometric%20View%20Generation&entry.906535625=Wenqi%20Dong%20and%20Bangbang%20Yang%20and%20Zesong%20Yang%20and%20Yuan%20Li%20and%20Tao%20Hu%20and%20Hujun%20Bao%20and%20Yuewen%20Ma%20and%20Zhaopeng%20Cui&entry.1292438233=%20%20Scene-level%203D%20generation%20represents%20a%20critical%20frontier%20in%20multimedia%20and%0Acomputer%20graphics%2C%20yet%20existing%20approaches%20either%20suffer%20from%20limited%20object%0Acategories%20or%20lack%20editing%20flexibility%20for%20interactive%20applications.%20In%20this%0Apaper%2C%20we%20present%20HiScene%2C%20a%20novel%20hierarchical%20framework%20that%20bridges%20the%20gap%0Abetween%202D%20image%20generation%20and%203D%20object%20generation%20and%20delivers%20high-fidelity%0Ascenes%20with%20compositional%20identities%20and%20aesthetic%20scene%20content.%20Our%20key%0Ainsight%20is%20treating%20scenes%20as%20hierarchical%20%22objects%22%20under%20isometric%20views%2C%0Awhere%20a%20room%20functions%20as%20a%20complex%20object%20that%20can%20be%20further%20decomposed%20into%0Amanipulatable%20items.%20This%20hierarchical%20approach%20enables%20us%20to%20generate%203D%0Acontent%20that%20aligns%20with%202D%20representations%20while%20maintaining%20compositional%0Astructure.%20To%20ensure%20completeness%20and%20spatial%20alignment%20of%20each%20decomposed%0Ainstance%2C%20we%20develop%20a%20video-diffusion-based%20amodal%20completion%20technique%20that%0Aeffectively%20handles%20occlusions%20and%20shadows%20between%20objects%2C%20and%20introduce%20shape%0Aprior%20injection%20to%20ensure%20spatial%20coherence%20within%20the%20scene.%20Experimental%0Aresults%20demonstrate%20that%20our%20method%20produces%20more%20natural%20object%20arrangements%0Aand%20complete%20object%20instances%20suitable%20for%20interactive%20applications%2C%20while%0Amaintaining%20physical%20plausibility%20and%20alignment%20with%20user%20inputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13072v1&entry.124074799=Read"},
{"title": "Stronger, Steadier & Superior: Geometric Consistency in Depth VFM Forges\n  Domain Generalized Semantic Segmentation", "author": "Siyu Chen and Ting Han and Changshe Zhang and Xin Luo and Meiliu Wu and Guorong Cai and Jinhe Su", "abstract": "  Vision Foundation Models (VFMs) have delivered remarkable performance in\nDomain Generalized Semantic Segmentation (DGSS). However, recent methods often\noverlook the fact that visual cues are susceptible, whereas the underlying\ngeometry remains stable, rendering depth information more robust. In this\npaper, we investigate the potential of integrating depth information with\nfeatures from VFMs, to improve the geometric consistency within an image and\nboost the generalization performance of VFMs. We propose a novel fine-tuning\nDGSS framework, named DepthForge, which integrates the visual cues from frozen\nDINOv2 or EVA02 and depth cues from frozen Depth Anything V2. In each layer of\nthe VFMs, we incorporate depth-aware learnable tokens to continuously decouple\ndomain-invariant visual and spatial information, thereby enhancing depth\nawareness and attention of the VFMs. Finally, we develop a depth refinement\ndecoder and integrate it into the model architecture to adaptively refine\nmulti-layer VFM features and depth-aware learnable tokens. Extensive\nexperiments are conducted based on various DGSS settings and five different\ndatsets as unseen target domains. The qualitative and quantitative results\ndemonstrate that our method significantly outperforms alternative approaches\nwith stronger performance, steadier visual-spatial attention, and superior\ngeneralization ability. In particular, DepthForge exhibits outstanding\nperformance under extreme conditions (e.g., night and snow). Code is available\nat https://github.com/anonymouse-xzrptkvyqc/DepthForge.\n", "link": "http://arxiv.org/abs/2504.12753v1", "date": "2025-04-17", "relevancy": 3.0498, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6184}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6184}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stronger%2C%20Steadier%20%26%20Superior%3A%20Geometric%20Consistency%20in%20Depth%20VFM%20Forges%0A%20%20Domain%20Generalized%20Semantic%20Segmentation&body=Title%3A%20Stronger%2C%20Steadier%20%26%20Superior%3A%20Geometric%20Consistency%20in%20Depth%20VFM%20Forges%0A%20%20Domain%20Generalized%20Semantic%20Segmentation%0AAuthor%3A%20Siyu%20Chen%20and%20Ting%20Han%20and%20Changshe%20Zhang%20and%20Xin%20Luo%20and%20Meiliu%20Wu%20and%20Guorong%20Cai%20and%20Jinhe%20Su%0AAbstract%3A%20%20%20Vision%20Foundation%20Models%20%28VFMs%29%20have%20delivered%20remarkable%20performance%20in%0ADomain%20Generalized%20Semantic%20Segmentation%20%28DGSS%29.%20However%2C%20recent%20methods%20often%0Aoverlook%20the%20fact%20that%20visual%20cues%20are%20susceptible%2C%20whereas%20the%20underlying%0Ageometry%20remains%20stable%2C%20rendering%20depth%20information%20more%20robust.%20In%20this%0Apaper%2C%20we%20investigate%20the%20potential%20of%20integrating%20depth%20information%20with%0Afeatures%20from%20VFMs%2C%20to%20improve%20the%20geometric%20consistency%20within%20an%20image%20and%0Aboost%20the%20generalization%20performance%20of%20VFMs.%20We%20propose%20a%20novel%20fine-tuning%0ADGSS%20framework%2C%20named%20DepthForge%2C%20which%20integrates%20the%20visual%20cues%20from%20frozen%0ADINOv2%20or%20EVA02%20and%20depth%20cues%20from%20frozen%20Depth%20Anything%20V2.%20In%20each%20layer%20of%0Athe%20VFMs%2C%20we%20incorporate%20depth-aware%20learnable%20tokens%20to%20continuously%20decouple%0Adomain-invariant%20visual%20and%20spatial%20information%2C%20thereby%20enhancing%20depth%0Aawareness%20and%20attention%20of%20the%20VFMs.%20Finally%2C%20we%20develop%20a%20depth%20refinement%0Adecoder%20and%20integrate%20it%20into%20the%20model%20architecture%20to%20adaptively%20refine%0Amulti-layer%20VFM%20features%20and%20depth-aware%20learnable%20tokens.%20Extensive%0Aexperiments%20are%20conducted%20based%20on%20various%20DGSS%20settings%20and%20five%20different%0Adatsets%20as%20unseen%20target%20domains.%20The%20qualitative%20and%20quantitative%20results%0Ademonstrate%20that%20our%20method%20significantly%20outperforms%20alternative%20approaches%0Awith%20stronger%20performance%2C%20steadier%20visual-spatial%20attention%2C%20and%20superior%0Ageneralization%20ability.%20In%20particular%2C%20DepthForge%20exhibits%20outstanding%0Aperformance%20under%20extreme%20conditions%20%28e.g.%2C%20night%20and%20snow%29.%20Code%20is%20available%0Aat%20https%3A//github.com/anonymouse-xzrptkvyqc/DepthForge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12753v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStronger%252C%2520Steadier%2520%2526%2520Superior%253A%2520Geometric%2520Consistency%2520in%2520Depth%2520VFM%2520Forges%250A%2520%2520Domain%2520Generalized%2520Semantic%2520Segmentation%26entry.906535625%3DSiyu%2520Chen%2520and%2520Ting%2520Han%2520and%2520Changshe%2520Zhang%2520and%2520Xin%2520Luo%2520and%2520Meiliu%2520Wu%2520and%2520Guorong%2520Cai%2520and%2520Jinhe%2520Su%26entry.1292438233%3D%2520%2520Vision%2520Foundation%2520Models%2520%2528VFMs%2529%2520have%2520delivered%2520remarkable%2520performance%2520in%250ADomain%2520Generalized%2520Semantic%2520Segmentation%2520%2528DGSS%2529.%2520However%252C%2520recent%2520methods%2520often%250Aoverlook%2520the%2520fact%2520that%2520visual%2520cues%2520are%2520susceptible%252C%2520whereas%2520the%2520underlying%250Ageometry%2520remains%2520stable%252C%2520rendering%2520depth%2520information%2520more%2520robust.%2520In%2520this%250Apaper%252C%2520we%2520investigate%2520the%2520potential%2520of%2520integrating%2520depth%2520information%2520with%250Afeatures%2520from%2520VFMs%252C%2520to%2520improve%2520the%2520geometric%2520consistency%2520within%2520an%2520image%2520and%250Aboost%2520the%2520generalization%2520performance%2520of%2520VFMs.%2520We%2520propose%2520a%2520novel%2520fine-tuning%250ADGSS%2520framework%252C%2520named%2520DepthForge%252C%2520which%2520integrates%2520the%2520visual%2520cues%2520from%2520frozen%250ADINOv2%2520or%2520EVA02%2520and%2520depth%2520cues%2520from%2520frozen%2520Depth%2520Anything%2520V2.%2520In%2520each%2520layer%2520of%250Athe%2520VFMs%252C%2520we%2520incorporate%2520depth-aware%2520learnable%2520tokens%2520to%2520continuously%2520decouple%250Adomain-invariant%2520visual%2520and%2520spatial%2520information%252C%2520thereby%2520enhancing%2520depth%250Aawareness%2520and%2520attention%2520of%2520the%2520VFMs.%2520Finally%252C%2520we%2520develop%2520a%2520depth%2520refinement%250Adecoder%2520and%2520integrate%2520it%2520into%2520the%2520model%2520architecture%2520to%2520adaptively%2520refine%250Amulti-layer%2520VFM%2520features%2520and%2520depth-aware%2520learnable%2520tokens.%2520Extensive%250Aexperiments%2520are%2520conducted%2520based%2520on%2520various%2520DGSS%2520settings%2520and%2520five%2520different%250Adatsets%2520as%2520unseen%2520target%2520domains.%2520The%2520qualitative%2520and%2520quantitative%2520results%250Ademonstrate%2520that%2520our%2520method%2520significantly%2520outperforms%2520alternative%2520approaches%250Awith%2520stronger%2520performance%252C%2520steadier%2520visual-spatial%2520attention%252C%2520and%2520superior%250Ageneralization%2520ability.%2520In%2520particular%252C%2520DepthForge%2520exhibits%2520outstanding%250Aperformance%2520under%2520extreme%2520conditions%2520%2528e.g.%252C%2520night%2520and%2520snow%2529.%2520Code%2520is%2520available%250Aat%2520https%253A//github.com/anonymouse-xzrptkvyqc/DepthForge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12753v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stronger%2C%20Steadier%20%26%20Superior%3A%20Geometric%20Consistency%20in%20Depth%20VFM%20Forges%0A%20%20Domain%20Generalized%20Semantic%20Segmentation&entry.906535625=Siyu%20Chen%20and%20Ting%20Han%20and%20Changshe%20Zhang%20and%20Xin%20Luo%20and%20Meiliu%20Wu%20and%20Guorong%20Cai%20and%20Jinhe%20Su&entry.1292438233=%20%20Vision%20Foundation%20Models%20%28VFMs%29%20have%20delivered%20remarkable%20performance%20in%0ADomain%20Generalized%20Semantic%20Segmentation%20%28DGSS%29.%20However%2C%20recent%20methods%20often%0Aoverlook%20the%20fact%20that%20visual%20cues%20are%20susceptible%2C%20whereas%20the%20underlying%0Ageometry%20remains%20stable%2C%20rendering%20depth%20information%20more%20robust.%20In%20this%0Apaper%2C%20we%20investigate%20the%20potential%20of%20integrating%20depth%20information%20with%0Afeatures%20from%20VFMs%2C%20to%20improve%20the%20geometric%20consistency%20within%20an%20image%20and%0Aboost%20the%20generalization%20performance%20of%20VFMs.%20We%20propose%20a%20novel%20fine-tuning%0ADGSS%20framework%2C%20named%20DepthForge%2C%20which%20integrates%20the%20visual%20cues%20from%20frozen%0ADINOv2%20or%20EVA02%20and%20depth%20cues%20from%20frozen%20Depth%20Anything%20V2.%20In%20each%20layer%20of%0Athe%20VFMs%2C%20we%20incorporate%20depth-aware%20learnable%20tokens%20to%20continuously%20decouple%0Adomain-invariant%20visual%20and%20spatial%20information%2C%20thereby%20enhancing%20depth%0Aawareness%20and%20attention%20of%20the%20VFMs.%20Finally%2C%20we%20develop%20a%20depth%20refinement%0Adecoder%20and%20integrate%20it%20into%20the%20model%20architecture%20to%20adaptively%20refine%0Amulti-layer%20VFM%20features%20and%20depth-aware%20learnable%20tokens.%20Extensive%0Aexperiments%20are%20conducted%20based%20on%20various%20DGSS%20settings%20and%20five%20different%0Adatsets%20as%20unseen%20target%20domains.%20The%20qualitative%20and%20quantitative%20results%0Ademonstrate%20that%20our%20method%20significantly%20outperforms%20alternative%20approaches%0Awith%20stronger%20performance%2C%20steadier%20visual-spatial%20attention%2C%20and%20superior%0Ageneralization%20ability.%20In%20particular%2C%20DepthForge%20exhibits%20outstanding%0Aperformance%20under%20extreme%20conditions%20%28e.g.%2C%20night%20and%20snow%29.%20Code%20is%20available%0Aat%20https%3A//github.com/anonymouse-xzrptkvyqc/DepthForge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12753v1&entry.124074799=Read"},
{"title": "Probing and Inducing Combinational Creativity in Vision-Language Models", "author": "Yongqian Peng and Yuxi Ma and Mengmeng Wang and Yuxuan Wang and Yizhou Wang and Chi Zhang and Yixin Zhu and Zilong Zheng", "abstract": "  The ability to combine existing concepts into novel ideas stands as a\nfundamental hallmark of human intelligence. Recent advances in Vision-Language\nModels (VLMs) like GPT-4V and DALLE-3 have sparked debate about whether their\noutputs reflect combinational creativity--defined by M. A. Boden (1998) as\nsynthesizing novel ideas through combining existing concepts--or sophisticated\npattern matching of training data. Drawing inspiration from cognitive science,\nwe investigate the combinational creativity of VLMs from the lens of concept\nblending. We propose the Identification-Explanation-Implication (IEI)\nframework, which decomposes creative processes into three levels: identifying\ninput spaces, extracting shared attributes, and deriving novel semantic\nimplications. To validate this framework, we curate CreativeMashup, a\nhigh-quality dataset of 666 artist-generated visual mashups annotated according\nto the IEI framework. Through extensive experiments, we demonstrate that in\ncomprehension tasks, best VLMs have surpassed average human performance while\nfalling short of expert-level understanding; in generation tasks, incorporating\nour IEI framework into the generation pipeline significantly enhances the\ncreative quality of VLMs outputs. Our findings establish both a theoretical\nfoundation for evaluating artificial creativity and practical guidelines for\nimproving creative generation in VLMs.\n", "link": "http://arxiv.org/abs/2504.13120v1", "date": "2025-04-17", "relevancy": 3.0404, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6258}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6258}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probing%20and%20Inducing%20Combinational%20Creativity%20in%20Vision-Language%20Models&body=Title%3A%20Probing%20and%20Inducing%20Combinational%20Creativity%20in%20Vision-Language%20Models%0AAuthor%3A%20Yongqian%20Peng%20and%20Yuxi%20Ma%20and%20Mengmeng%20Wang%20and%20Yuxuan%20Wang%20and%20Yizhou%20Wang%20and%20Chi%20Zhang%20and%20Yixin%20Zhu%20and%20Zilong%20Zheng%0AAbstract%3A%20%20%20The%20ability%20to%20combine%20existing%20concepts%20into%20novel%20ideas%20stands%20as%20a%0Afundamental%20hallmark%20of%20human%20intelligence.%20Recent%20advances%20in%20Vision-Language%0AModels%20%28VLMs%29%20like%20GPT-4V%20and%20DALLE-3%20have%20sparked%20debate%20about%20whether%20their%0Aoutputs%20reflect%20combinational%20creativity--defined%20by%20M.%20A.%20Boden%20%281998%29%20as%0Asynthesizing%20novel%20ideas%20through%20combining%20existing%20concepts--or%20sophisticated%0Apattern%20matching%20of%20training%20data.%20Drawing%20inspiration%20from%20cognitive%20science%2C%0Awe%20investigate%20the%20combinational%20creativity%20of%20VLMs%20from%20the%20lens%20of%20concept%0Ablending.%20We%20propose%20the%20Identification-Explanation-Implication%20%28IEI%29%0Aframework%2C%20which%20decomposes%20creative%20processes%20into%20three%20levels%3A%20identifying%0Ainput%20spaces%2C%20extracting%20shared%20attributes%2C%20and%20deriving%20novel%20semantic%0Aimplications.%20To%20validate%20this%20framework%2C%20we%20curate%20CreativeMashup%2C%20a%0Ahigh-quality%20dataset%20of%20666%20artist-generated%20visual%20mashups%20annotated%20according%0Ato%20the%20IEI%20framework.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20that%20in%0Acomprehension%20tasks%2C%20best%20VLMs%20have%20surpassed%20average%20human%20performance%20while%0Afalling%20short%20of%20expert-level%20understanding%3B%20in%20generation%20tasks%2C%20incorporating%0Aour%20IEI%20framework%20into%20the%20generation%20pipeline%20significantly%20enhances%20the%0Acreative%20quality%20of%20VLMs%20outputs.%20Our%20findings%20establish%20both%20a%20theoretical%0Afoundation%20for%20evaluating%20artificial%20creativity%20and%20practical%20guidelines%20for%0Aimproving%20creative%20generation%20in%20VLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13120v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbing%2520and%2520Inducing%2520Combinational%2520Creativity%2520in%2520Vision-Language%2520Models%26entry.906535625%3DYongqian%2520Peng%2520and%2520Yuxi%2520Ma%2520and%2520Mengmeng%2520Wang%2520and%2520Yuxuan%2520Wang%2520and%2520Yizhou%2520Wang%2520and%2520Chi%2520Zhang%2520and%2520Yixin%2520Zhu%2520and%2520Zilong%2520Zheng%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520combine%2520existing%2520concepts%2520into%2520novel%2520ideas%2520stands%2520as%2520a%250Afundamental%2520hallmark%2520of%2520human%2520intelligence.%2520Recent%2520advances%2520in%2520Vision-Language%250AModels%2520%2528VLMs%2529%2520like%2520GPT-4V%2520and%2520DALLE-3%2520have%2520sparked%2520debate%2520about%2520whether%2520their%250Aoutputs%2520reflect%2520combinational%2520creativity--defined%2520by%2520M.%2520A.%2520Boden%2520%25281998%2529%2520as%250Asynthesizing%2520novel%2520ideas%2520through%2520combining%2520existing%2520concepts--or%2520sophisticated%250Apattern%2520matching%2520of%2520training%2520data.%2520Drawing%2520inspiration%2520from%2520cognitive%2520science%252C%250Awe%2520investigate%2520the%2520combinational%2520creativity%2520of%2520VLMs%2520from%2520the%2520lens%2520of%2520concept%250Ablending.%2520We%2520propose%2520the%2520Identification-Explanation-Implication%2520%2528IEI%2529%250Aframework%252C%2520which%2520decomposes%2520creative%2520processes%2520into%2520three%2520levels%253A%2520identifying%250Ainput%2520spaces%252C%2520extracting%2520shared%2520attributes%252C%2520and%2520deriving%2520novel%2520semantic%250Aimplications.%2520To%2520validate%2520this%2520framework%252C%2520we%2520curate%2520CreativeMashup%252C%2520a%250Ahigh-quality%2520dataset%2520of%2520666%2520artist-generated%2520visual%2520mashups%2520annotated%2520according%250Ato%2520the%2520IEI%2520framework.%2520Through%2520extensive%2520experiments%252C%2520we%2520demonstrate%2520that%2520in%250Acomprehension%2520tasks%252C%2520best%2520VLMs%2520have%2520surpassed%2520average%2520human%2520performance%2520while%250Afalling%2520short%2520of%2520expert-level%2520understanding%253B%2520in%2520generation%2520tasks%252C%2520incorporating%250Aour%2520IEI%2520framework%2520into%2520the%2520generation%2520pipeline%2520significantly%2520enhances%2520the%250Acreative%2520quality%2520of%2520VLMs%2520outputs.%2520Our%2520findings%2520establish%2520both%2520a%2520theoretical%250Afoundation%2520for%2520evaluating%2520artificial%2520creativity%2520and%2520practical%2520guidelines%2520for%250Aimproving%2520creative%2520generation%2520in%2520VLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13120v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probing%20and%20Inducing%20Combinational%20Creativity%20in%20Vision-Language%20Models&entry.906535625=Yongqian%20Peng%20and%20Yuxi%20Ma%20and%20Mengmeng%20Wang%20and%20Yuxuan%20Wang%20and%20Yizhou%20Wang%20and%20Chi%20Zhang%20and%20Yixin%20Zhu%20and%20Zilong%20Zheng&entry.1292438233=%20%20The%20ability%20to%20combine%20existing%20concepts%20into%20novel%20ideas%20stands%20as%20a%0Afundamental%20hallmark%20of%20human%20intelligence.%20Recent%20advances%20in%20Vision-Language%0AModels%20%28VLMs%29%20like%20GPT-4V%20and%20DALLE-3%20have%20sparked%20debate%20about%20whether%20their%0Aoutputs%20reflect%20combinational%20creativity--defined%20by%20M.%20A.%20Boden%20%281998%29%20as%0Asynthesizing%20novel%20ideas%20through%20combining%20existing%20concepts--or%20sophisticated%0Apattern%20matching%20of%20training%20data.%20Drawing%20inspiration%20from%20cognitive%20science%2C%0Awe%20investigate%20the%20combinational%20creativity%20of%20VLMs%20from%20the%20lens%20of%20concept%0Ablending.%20We%20propose%20the%20Identification-Explanation-Implication%20%28IEI%29%0Aframework%2C%20which%20decomposes%20creative%20processes%20into%20three%20levels%3A%20identifying%0Ainput%20spaces%2C%20extracting%20shared%20attributes%2C%20and%20deriving%20novel%20semantic%0Aimplications.%20To%20validate%20this%20framework%2C%20we%20curate%20CreativeMashup%2C%20a%0Ahigh-quality%20dataset%20of%20666%20artist-generated%20visual%20mashups%20annotated%20according%0Ato%20the%20IEI%20framework.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20that%20in%0Acomprehension%20tasks%2C%20best%20VLMs%20have%20surpassed%20average%20human%20performance%20while%0Afalling%20short%20of%20expert-level%20understanding%3B%20in%20generation%20tasks%2C%20incorporating%0Aour%20IEI%20framework%20into%20the%20generation%20pipeline%20significantly%20enhances%20the%0Acreative%20quality%20of%20VLMs%20outputs.%20Our%20findings%20establish%20both%20a%20theoretical%0Afoundation%20for%20evaluating%20artificial%20creativity%20and%20practical%20guidelines%20for%0Aimproving%20creative%20generation%20in%20VLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13120v1&entry.124074799=Read"},
{"title": "Near, far: Patch-ordering enhances vision foundation models' scene\n  understanding", "author": "Valentinos Pariza and Mohammadreza Salehi and Gertjan Burghouts and Francesco Locatello and Yuki M. Asano", "abstract": "  We introduce NeCo: Patch Neighbor Consistency, a novel self-supervised\ntraining loss that enforces patch-level nearest neighbor consistency across a\nstudent and teacher model. Compared to contrastive approaches that only yield\nbinary learning signals, i.e., 'attract' and 'repel', this approach benefits\nfrom the more fine-grained learning signal of sorting spatially dense features\nrelative to reference patches. Our method leverages differentiable sorting\napplied on top of pretrained representations, such as DINOv2-registers to\nbootstrap the learning signal and further improve upon them. This dense\npost-pretraining leads to superior performance across various models and\ndatasets, despite requiring only 19 hours on a single GPU. This method\ngenerates high-quality dense feature encoders and establishes several new\nstate-of-the-art results such as +5.5% and +6% for non-parametric in-context\nsemantic segmentation on ADE20k and Pascal VOC, +7.2% and +5.7% for linear\nsegmentation evaluations on COCO-Things and -Stuff and improvements in the 3D\nunderstanding of multi-view consistency on SPair-71k, by more than 1.5%.\n", "link": "http://arxiv.org/abs/2408.11054v3", "date": "2025-04-17", "relevancy": 3.0325, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6158}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6158}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Near%2C%20far%3A%20Patch-ordering%20enhances%20vision%20foundation%20models%27%20scene%0A%20%20understanding&body=Title%3A%20Near%2C%20far%3A%20Patch-ordering%20enhances%20vision%20foundation%20models%27%20scene%0A%20%20understanding%0AAuthor%3A%20Valentinos%20Pariza%20and%20Mohammadreza%20Salehi%20and%20Gertjan%20Burghouts%20and%20Francesco%20Locatello%20and%20Yuki%20M.%20Asano%0AAbstract%3A%20%20%20We%20introduce%20NeCo%3A%20Patch%20Neighbor%20Consistency%2C%20a%20novel%20self-supervised%0Atraining%20loss%20that%20enforces%20patch-level%20nearest%20neighbor%20consistency%20across%20a%0Astudent%20and%20teacher%20model.%20Compared%20to%20contrastive%20approaches%20that%20only%20yield%0Abinary%20learning%20signals%2C%20i.e.%2C%20%27attract%27%20and%20%27repel%27%2C%20this%20approach%20benefits%0Afrom%20the%20more%20fine-grained%20learning%20signal%20of%20sorting%20spatially%20dense%20features%0Arelative%20to%20reference%20patches.%20Our%20method%20leverages%20differentiable%20sorting%0Aapplied%20on%20top%20of%20pretrained%20representations%2C%20such%20as%20DINOv2-registers%20to%0Abootstrap%20the%20learning%20signal%20and%20further%20improve%20upon%20them.%20This%20dense%0Apost-pretraining%20leads%20to%20superior%20performance%20across%20various%20models%20and%0Adatasets%2C%20despite%20requiring%20only%2019%20hours%20on%20a%20single%20GPU.%20This%20method%0Agenerates%20high-quality%20dense%20feature%20encoders%20and%20establishes%20several%20new%0Astate-of-the-art%20results%20such%20as%20%2B5.5%25%20and%20%2B6%25%20for%20non-parametric%20in-context%0Asemantic%20segmentation%20on%20ADE20k%20and%20Pascal%20VOC%2C%20%2B7.2%25%20and%20%2B5.7%25%20for%20linear%0Asegmentation%20evaluations%20on%20COCO-Things%20and%20-Stuff%20and%20improvements%20in%20the%203D%0Aunderstanding%20of%20multi-view%20consistency%20on%20SPair-71k%2C%20by%20more%20than%201.5%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11054v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNear%252C%2520far%253A%2520Patch-ordering%2520enhances%2520vision%2520foundation%2520models%2527%2520scene%250A%2520%2520understanding%26entry.906535625%3DValentinos%2520Pariza%2520and%2520Mohammadreza%2520Salehi%2520and%2520Gertjan%2520Burghouts%2520and%2520Francesco%2520Locatello%2520and%2520Yuki%2520M.%2520Asano%26entry.1292438233%3D%2520%2520We%2520introduce%2520NeCo%253A%2520Patch%2520Neighbor%2520Consistency%252C%2520a%2520novel%2520self-supervised%250Atraining%2520loss%2520that%2520enforces%2520patch-level%2520nearest%2520neighbor%2520consistency%2520across%2520a%250Astudent%2520and%2520teacher%2520model.%2520Compared%2520to%2520contrastive%2520approaches%2520that%2520only%2520yield%250Abinary%2520learning%2520signals%252C%2520i.e.%252C%2520%2527attract%2527%2520and%2520%2527repel%2527%252C%2520this%2520approach%2520benefits%250Afrom%2520the%2520more%2520fine-grained%2520learning%2520signal%2520of%2520sorting%2520spatially%2520dense%2520features%250Arelative%2520to%2520reference%2520patches.%2520Our%2520method%2520leverages%2520differentiable%2520sorting%250Aapplied%2520on%2520top%2520of%2520pretrained%2520representations%252C%2520such%2520as%2520DINOv2-registers%2520to%250Abootstrap%2520the%2520learning%2520signal%2520and%2520further%2520improve%2520upon%2520them.%2520This%2520dense%250Apost-pretraining%2520leads%2520to%2520superior%2520performance%2520across%2520various%2520models%2520and%250Adatasets%252C%2520despite%2520requiring%2520only%252019%2520hours%2520on%2520a%2520single%2520GPU.%2520This%2520method%250Agenerates%2520high-quality%2520dense%2520feature%2520encoders%2520and%2520establishes%2520several%2520new%250Astate-of-the-art%2520results%2520such%2520as%2520%252B5.5%2525%2520and%2520%252B6%2525%2520for%2520non-parametric%2520in-context%250Asemantic%2520segmentation%2520on%2520ADE20k%2520and%2520Pascal%2520VOC%252C%2520%252B7.2%2525%2520and%2520%252B5.7%2525%2520for%2520linear%250Asegmentation%2520evaluations%2520on%2520COCO-Things%2520and%2520-Stuff%2520and%2520improvements%2520in%2520the%25203D%250Aunderstanding%2520of%2520multi-view%2520consistency%2520on%2520SPair-71k%252C%2520by%2520more%2520than%25201.5%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11054v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Near%2C%20far%3A%20Patch-ordering%20enhances%20vision%20foundation%20models%27%20scene%0A%20%20understanding&entry.906535625=Valentinos%20Pariza%20and%20Mohammadreza%20Salehi%20and%20Gertjan%20Burghouts%20and%20Francesco%20Locatello%20and%20Yuki%20M.%20Asano&entry.1292438233=%20%20We%20introduce%20NeCo%3A%20Patch%20Neighbor%20Consistency%2C%20a%20novel%20self-supervised%0Atraining%20loss%20that%20enforces%20patch-level%20nearest%20neighbor%20consistency%20across%20a%0Astudent%20and%20teacher%20model.%20Compared%20to%20contrastive%20approaches%20that%20only%20yield%0Abinary%20learning%20signals%2C%20i.e.%2C%20%27attract%27%20and%20%27repel%27%2C%20this%20approach%20benefits%0Afrom%20the%20more%20fine-grained%20learning%20signal%20of%20sorting%20spatially%20dense%20features%0Arelative%20to%20reference%20patches.%20Our%20method%20leverages%20differentiable%20sorting%0Aapplied%20on%20top%20of%20pretrained%20representations%2C%20such%20as%20DINOv2-registers%20to%0Abootstrap%20the%20learning%20signal%20and%20further%20improve%20upon%20them.%20This%20dense%0Apost-pretraining%20leads%20to%20superior%20performance%20across%20various%20models%20and%0Adatasets%2C%20despite%20requiring%20only%2019%20hours%20on%20a%20single%20GPU.%20This%20method%0Agenerates%20high-quality%20dense%20feature%20encoders%20and%20establishes%20several%20new%0Astate-of-the-art%20results%20such%20as%20%2B5.5%25%20and%20%2B6%25%20for%20non-parametric%20in-context%0Asemantic%20segmentation%20on%20ADE20k%20and%20Pascal%20VOC%2C%20%2B7.2%25%20and%20%2B5.7%25%20for%20linear%0Asegmentation%20evaluations%20on%20COCO-Things%20and%20-Stuff%20and%20improvements%20in%20the%203D%0Aunderstanding%20of%20multi-view%20consistency%20on%20SPair-71k%2C%20by%20more%20than%201.5%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11054v3&entry.124074799=Read"},
{"title": "Post-pre-training for Modality Alignment in Vision-Language Foundation\n  Models", "author": "Shin'ya Yamaguchi and Dewei Feng and Sekitoshi Kanai and Kazuki Adachi and Daiki Chijiwa", "abstract": "  Contrastive language image pre-training (CLIP) is an essential component of\nbuilding modern vision-language foundation models. While CLIP demonstrates\nremarkable zero-shot performance on downstream tasks, the multi-modal feature\nspaces still suffer from a modality gap, which is a gap between image and text\nfeature clusters and limits downstream task performance. Although existing\nworks attempt to address the modality gap by modifying pre-training or\nfine-tuning, they struggle with heavy training costs with large datasets or\ndegradations of zero-shot performance. This paper presents CLIP-Refine, a\npost-pre-training method for CLIP models at a phase between pre-training and\nfine-tuning. CLIP-Refine aims to align the feature space with 1 epoch training\non small image-text datasets without zero-shot performance degradations. To\nthis end, we introduce two techniques: random feature alignment (RaFA) and\nhybrid contrastive-distillation (HyCD). RaFA aligns the image and text features\nto follow a shared prior distribution by minimizing the distance to random\nreference vectors sampled from the prior. HyCD updates the model with hybrid\nsoft labels generated by combining ground-truth image-text pair labels and\noutputs from the pre-trained CLIP model. This contributes to achieving both\nmaintaining the past knowledge and learning new knowledge to align features.\nOur extensive experiments with multiple classification and retrieval tasks show\nthat CLIP-Refine succeeds in mitigating the modality gap and improving the\nzero-shot performance.\n", "link": "http://arxiv.org/abs/2504.12717v1", "date": "2025-04-17", "relevancy": 3.0277, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6394}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5886}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Post-pre-training%20for%20Modality%20Alignment%20in%20Vision-Language%20Foundation%0A%20%20Models&body=Title%3A%20Post-pre-training%20for%20Modality%20Alignment%20in%20Vision-Language%20Foundation%0A%20%20Models%0AAuthor%3A%20Shin%27ya%20Yamaguchi%20and%20Dewei%20Feng%20and%20Sekitoshi%20Kanai%20and%20Kazuki%20Adachi%20and%20Daiki%20Chijiwa%0AAbstract%3A%20%20%20Contrastive%20language%20image%20pre-training%20%28CLIP%29%20is%20an%20essential%20component%20of%0Abuilding%20modern%20vision-language%20foundation%20models.%20While%20CLIP%20demonstrates%0Aremarkable%20zero-shot%20performance%20on%20downstream%20tasks%2C%20the%20multi-modal%20feature%0Aspaces%20still%20suffer%20from%20a%20modality%20gap%2C%20which%20is%20a%20gap%20between%20image%20and%20text%0Afeature%20clusters%20and%20limits%20downstream%20task%20performance.%20Although%20existing%0Aworks%20attempt%20to%20address%20the%20modality%20gap%20by%20modifying%20pre-training%20or%0Afine-tuning%2C%20they%20struggle%20with%20heavy%20training%20costs%20with%20large%20datasets%20or%0Adegradations%20of%20zero-shot%20performance.%20This%20paper%20presents%20CLIP-Refine%2C%20a%0Apost-pre-training%20method%20for%20CLIP%20models%20at%20a%20phase%20between%20pre-training%20and%0Afine-tuning.%20CLIP-Refine%20aims%20to%20align%20the%20feature%20space%20with%201%20epoch%20training%0Aon%20small%20image-text%20datasets%20without%20zero-shot%20performance%20degradations.%20To%0Athis%20end%2C%20we%20introduce%20two%20techniques%3A%20random%20feature%20alignment%20%28RaFA%29%20and%0Ahybrid%20contrastive-distillation%20%28HyCD%29.%20RaFA%20aligns%20the%20image%20and%20text%20features%0Ato%20follow%20a%20shared%20prior%20distribution%20by%20minimizing%20the%20distance%20to%20random%0Areference%20vectors%20sampled%20from%20the%20prior.%20HyCD%20updates%20the%20model%20with%20hybrid%0Asoft%20labels%20generated%20by%20combining%20ground-truth%20image-text%20pair%20labels%20and%0Aoutputs%20from%20the%20pre-trained%20CLIP%20model.%20This%20contributes%20to%20achieving%20both%0Amaintaining%20the%20past%20knowledge%20and%20learning%20new%20knowledge%20to%20align%20features.%0AOur%20extensive%20experiments%20with%20multiple%20classification%20and%20retrieval%20tasks%20show%0Athat%20CLIP-Refine%20succeeds%20in%20mitigating%20the%20modality%20gap%20and%20improving%20the%0Azero-shot%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12717v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPost-pre-training%2520for%2520Modality%2520Alignment%2520in%2520Vision-Language%2520Foundation%250A%2520%2520Models%26entry.906535625%3DShin%2527ya%2520Yamaguchi%2520and%2520Dewei%2520Feng%2520and%2520Sekitoshi%2520Kanai%2520and%2520Kazuki%2520Adachi%2520and%2520Daiki%2520Chijiwa%26entry.1292438233%3D%2520%2520Contrastive%2520language%2520image%2520pre-training%2520%2528CLIP%2529%2520is%2520an%2520essential%2520component%2520of%250Abuilding%2520modern%2520vision-language%2520foundation%2520models.%2520While%2520CLIP%2520demonstrates%250Aremarkable%2520zero-shot%2520performance%2520on%2520downstream%2520tasks%252C%2520the%2520multi-modal%2520feature%250Aspaces%2520still%2520suffer%2520from%2520a%2520modality%2520gap%252C%2520which%2520is%2520a%2520gap%2520between%2520image%2520and%2520text%250Afeature%2520clusters%2520and%2520limits%2520downstream%2520task%2520performance.%2520Although%2520existing%250Aworks%2520attempt%2520to%2520address%2520the%2520modality%2520gap%2520by%2520modifying%2520pre-training%2520or%250Afine-tuning%252C%2520they%2520struggle%2520with%2520heavy%2520training%2520costs%2520with%2520large%2520datasets%2520or%250Adegradations%2520of%2520zero-shot%2520performance.%2520This%2520paper%2520presents%2520CLIP-Refine%252C%2520a%250Apost-pre-training%2520method%2520for%2520CLIP%2520models%2520at%2520a%2520phase%2520between%2520pre-training%2520and%250Afine-tuning.%2520CLIP-Refine%2520aims%2520to%2520align%2520the%2520feature%2520space%2520with%25201%2520epoch%2520training%250Aon%2520small%2520image-text%2520datasets%2520without%2520zero-shot%2520performance%2520degradations.%2520To%250Athis%2520end%252C%2520we%2520introduce%2520two%2520techniques%253A%2520random%2520feature%2520alignment%2520%2528RaFA%2529%2520and%250Ahybrid%2520contrastive-distillation%2520%2528HyCD%2529.%2520RaFA%2520aligns%2520the%2520image%2520and%2520text%2520features%250Ato%2520follow%2520a%2520shared%2520prior%2520distribution%2520by%2520minimizing%2520the%2520distance%2520to%2520random%250Areference%2520vectors%2520sampled%2520from%2520the%2520prior.%2520HyCD%2520updates%2520the%2520model%2520with%2520hybrid%250Asoft%2520labels%2520generated%2520by%2520combining%2520ground-truth%2520image-text%2520pair%2520labels%2520and%250Aoutputs%2520from%2520the%2520pre-trained%2520CLIP%2520model.%2520This%2520contributes%2520to%2520achieving%2520both%250Amaintaining%2520the%2520past%2520knowledge%2520and%2520learning%2520new%2520knowledge%2520to%2520align%2520features.%250AOur%2520extensive%2520experiments%2520with%2520multiple%2520classification%2520and%2520retrieval%2520tasks%2520show%250Athat%2520CLIP-Refine%2520succeeds%2520in%2520mitigating%2520the%2520modality%2520gap%2520and%2520improving%2520the%250Azero-shot%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12717v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Post-pre-training%20for%20Modality%20Alignment%20in%20Vision-Language%20Foundation%0A%20%20Models&entry.906535625=Shin%27ya%20Yamaguchi%20and%20Dewei%20Feng%20and%20Sekitoshi%20Kanai%20and%20Kazuki%20Adachi%20and%20Daiki%20Chijiwa&entry.1292438233=%20%20Contrastive%20language%20image%20pre-training%20%28CLIP%29%20is%20an%20essential%20component%20of%0Abuilding%20modern%20vision-language%20foundation%20models.%20While%20CLIP%20demonstrates%0Aremarkable%20zero-shot%20performance%20on%20downstream%20tasks%2C%20the%20multi-modal%20feature%0Aspaces%20still%20suffer%20from%20a%20modality%20gap%2C%20which%20is%20a%20gap%20between%20image%20and%20text%0Afeature%20clusters%20and%20limits%20downstream%20task%20performance.%20Although%20existing%0Aworks%20attempt%20to%20address%20the%20modality%20gap%20by%20modifying%20pre-training%20or%0Afine-tuning%2C%20they%20struggle%20with%20heavy%20training%20costs%20with%20large%20datasets%20or%0Adegradations%20of%20zero-shot%20performance.%20This%20paper%20presents%20CLIP-Refine%2C%20a%0Apost-pre-training%20method%20for%20CLIP%20models%20at%20a%20phase%20between%20pre-training%20and%0Afine-tuning.%20CLIP-Refine%20aims%20to%20align%20the%20feature%20space%20with%201%20epoch%20training%0Aon%20small%20image-text%20datasets%20without%20zero-shot%20performance%20degradations.%20To%0Athis%20end%2C%20we%20introduce%20two%20techniques%3A%20random%20feature%20alignment%20%28RaFA%29%20and%0Ahybrid%20contrastive-distillation%20%28HyCD%29.%20RaFA%20aligns%20the%20image%20and%20text%20features%0Ato%20follow%20a%20shared%20prior%20distribution%20by%20minimizing%20the%20distance%20to%20random%0Areference%20vectors%20sampled%20from%20the%20prior.%20HyCD%20updates%20the%20model%20with%20hybrid%0Asoft%20labels%20generated%20by%20combining%20ground-truth%20image-text%20pair%20labels%20and%0Aoutputs%20from%20the%20pre-trained%20CLIP%20model.%20This%20contributes%20to%20achieving%20both%0Amaintaining%20the%20past%20knowledge%20and%20learning%20new%20knowledge%20to%20align%20features.%0AOur%20extensive%20experiments%20with%20multiple%20classification%20and%20retrieval%20tasks%20show%0Athat%20CLIP-Refine%20succeeds%20in%20mitigating%20the%20modality%20gap%20and%20improving%20the%0Azero-shot%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12717v1&entry.124074799=Read"},
{"title": "RGB-Phase Speckle: Cross-Scene Stereo 3D Reconstruction via Wrapped\n  Pre-Normalization", "author": "Kai Yang and Zijian Bai and Yang Xiao and Xinyu Li and Xiaohan Shi", "abstract": "  3D reconstruction garners increasing attention alongside the advancement of\nhigh-level image applications, where dense stereo matching (DSM) serves as a\npivotal technique. Previous studies often rely on publicly available datasets\nfor training, focusing on modifying network architectures or incorporating\nspecialized modules to extract domain-invariant features and thus improve model\nrobustness. In contrast, inspired by single-frame structured-light\nphase-shifting encoding, this study introduces RGB-Speckle, a cross-scene 3D\nreconstruction framework based on an active stereo camera system, designed to\nenhance robustness. Specifically, we propose a novel phase pre-normalization\nencoding-decoding method: first, we randomly perturb phase-shift maps and embed\nthem into the three RGB channels to generate color speckle patterns;\nsubsequently, the camera captures phase-encoded images modulated by objects as\ninput to a stereo matching network. This technique effectively mitigates\nexternal interference and ensures consistent input data for RGB-Speckle,\nthereby bolstering cross-domain 3D reconstruction stability. To validate the\nproposed method, we conduct complex experiments: (1) construct a color speckle\ndataset for complex scenarios based on the proposed encoding scheme; (2)\nevaluate the impact of the phase pre-normalization encoding-decoding technique\non 3D reconstruction accuracy; and (3) further investigate its robustness\nacross diverse conditions. Experimental results demonstrate that the proposed\nRGB-Speckle model offers significant advantages in cross-domain and cross-scene\n3D reconstruction tasks, enhancing model generalization and reinforcing\nrobustness in challenging environments, thus providing a novel solution for\nrobust 3D reconstruction research.\n", "link": "http://arxiv.org/abs/2503.06125v2", "date": "2025-04-17", "relevancy": 3.0119, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6171}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.595}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RGB-Phase%20Speckle%3A%20Cross-Scene%20Stereo%203D%20Reconstruction%20via%20Wrapped%0A%20%20Pre-Normalization&body=Title%3A%20RGB-Phase%20Speckle%3A%20Cross-Scene%20Stereo%203D%20Reconstruction%20via%20Wrapped%0A%20%20Pre-Normalization%0AAuthor%3A%20Kai%20Yang%20and%20Zijian%20Bai%20and%20Yang%20Xiao%20and%20Xinyu%20Li%20and%20Xiaohan%20Shi%0AAbstract%3A%20%20%203D%20reconstruction%20garners%20increasing%20attention%20alongside%20the%20advancement%20of%0Ahigh-level%20image%20applications%2C%20where%20dense%20stereo%20matching%20%28DSM%29%20serves%20as%20a%0Apivotal%20technique.%20Previous%20studies%20often%20rely%20on%20publicly%20available%20datasets%0Afor%20training%2C%20focusing%20on%20modifying%20network%20architectures%20or%20incorporating%0Aspecialized%20modules%20to%20extract%20domain-invariant%20features%20and%20thus%20improve%20model%0Arobustness.%20In%20contrast%2C%20inspired%20by%20single-frame%20structured-light%0Aphase-shifting%20encoding%2C%20this%20study%20introduces%20RGB-Speckle%2C%20a%20cross-scene%203D%0Areconstruction%20framework%20based%20on%20an%20active%20stereo%20camera%20system%2C%20designed%20to%0Aenhance%20robustness.%20Specifically%2C%20we%20propose%20a%20novel%20phase%20pre-normalization%0Aencoding-decoding%20method%3A%20first%2C%20we%20randomly%20perturb%20phase-shift%20maps%20and%20embed%0Athem%20into%20the%20three%20RGB%20channels%20to%20generate%20color%20speckle%20patterns%3B%0Asubsequently%2C%20the%20camera%20captures%20phase-encoded%20images%20modulated%20by%20objects%20as%0Ainput%20to%20a%20stereo%20matching%20network.%20This%20technique%20effectively%20mitigates%0Aexternal%20interference%20and%20ensures%20consistent%20input%20data%20for%20RGB-Speckle%2C%0Athereby%20bolstering%20cross-domain%203D%20reconstruction%20stability.%20To%20validate%20the%0Aproposed%20method%2C%20we%20conduct%20complex%20experiments%3A%20%281%29%20construct%20a%20color%20speckle%0Adataset%20for%20complex%20scenarios%20based%20on%20the%20proposed%20encoding%20scheme%3B%20%282%29%0Aevaluate%20the%20impact%20of%20the%20phase%20pre-normalization%20encoding-decoding%20technique%0Aon%203D%20reconstruction%20accuracy%3B%20and%20%283%29%20further%20investigate%20its%20robustness%0Aacross%20diverse%20conditions.%20Experimental%20results%20demonstrate%20that%20the%20proposed%0ARGB-Speckle%20model%20offers%20significant%20advantages%20in%20cross-domain%20and%20cross-scene%0A3D%20reconstruction%20tasks%2C%20enhancing%20model%20generalization%20and%20reinforcing%0Arobustness%20in%20challenging%20environments%2C%20thus%20providing%20a%20novel%20solution%20for%0Arobust%203D%20reconstruction%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.06125v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRGB-Phase%2520Speckle%253A%2520Cross-Scene%2520Stereo%25203D%2520Reconstruction%2520via%2520Wrapped%250A%2520%2520Pre-Normalization%26entry.906535625%3DKai%2520Yang%2520and%2520Zijian%2520Bai%2520and%2520Yang%2520Xiao%2520and%2520Xinyu%2520Li%2520and%2520Xiaohan%2520Shi%26entry.1292438233%3D%2520%25203D%2520reconstruction%2520garners%2520increasing%2520attention%2520alongside%2520the%2520advancement%2520of%250Ahigh-level%2520image%2520applications%252C%2520where%2520dense%2520stereo%2520matching%2520%2528DSM%2529%2520serves%2520as%2520a%250Apivotal%2520technique.%2520Previous%2520studies%2520often%2520rely%2520on%2520publicly%2520available%2520datasets%250Afor%2520training%252C%2520focusing%2520on%2520modifying%2520network%2520architectures%2520or%2520incorporating%250Aspecialized%2520modules%2520to%2520extract%2520domain-invariant%2520features%2520and%2520thus%2520improve%2520model%250Arobustness.%2520In%2520contrast%252C%2520inspired%2520by%2520single-frame%2520structured-light%250Aphase-shifting%2520encoding%252C%2520this%2520study%2520introduces%2520RGB-Speckle%252C%2520a%2520cross-scene%25203D%250Areconstruction%2520framework%2520based%2520on%2520an%2520active%2520stereo%2520camera%2520system%252C%2520designed%2520to%250Aenhance%2520robustness.%2520Specifically%252C%2520we%2520propose%2520a%2520novel%2520phase%2520pre-normalization%250Aencoding-decoding%2520method%253A%2520first%252C%2520we%2520randomly%2520perturb%2520phase-shift%2520maps%2520and%2520embed%250Athem%2520into%2520the%2520three%2520RGB%2520channels%2520to%2520generate%2520color%2520speckle%2520patterns%253B%250Asubsequently%252C%2520the%2520camera%2520captures%2520phase-encoded%2520images%2520modulated%2520by%2520objects%2520as%250Ainput%2520to%2520a%2520stereo%2520matching%2520network.%2520This%2520technique%2520effectively%2520mitigates%250Aexternal%2520interference%2520and%2520ensures%2520consistent%2520input%2520data%2520for%2520RGB-Speckle%252C%250Athereby%2520bolstering%2520cross-domain%25203D%2520reconstruction%2520stability.%2520To%2520validate%2520the%250Aproposed%2520method%252C%2520we%2520conduct%2520complex%2520experiments%253A%2520%25281%2529%2520construct%2520a%2520color%2520speckle%250Adataset%2520for%2520complex%2520scenarios%2520based%2520on%2520the%2520proposed%2520encoding%2520scheme%253B%2520%25282%2529%250Aevaluate%2520the%2520impact%2520of%2520the%2520phase%2520pre-normalization%2520encoding-decoding%2520technique%250Aon%25203D%2520reconstruction%2520accuracy%253B%2520and%2520%25283%2529%2520further%2520investigate%2520its%2520robustness%250Aacross%2520diverse%2520conditions.%2520Experimental%2520results%2520demonstrate%2520that%2520the%2520proposed%250ARGB-Speckle%2520model%2520offers%2520significant%2520advantages%2520in%2520cross-domain%2520and%2520cross-scene%250A3D%2520reconstruction%2520tasks%252C%2520enhancing%2520model%2520generalization%2520and%2520reinforcing%250Arobustness%2520in%2520challenging%2520environments%252C%2520thus%2520providing%2520a%2520novel%2520solution%2520for%250Arobust%25203D%2520reconstruction%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.06125v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RGB-Phase%20Speckle%3A%20Cross-Scene%20Stereo%203D%20Reconstruction%20via%20Wrapped%0A%20%20Pre-Normalization&entry.906535625=Kai%20Yang%20and%20Zijian%20Bai%20and%20Yang%20Xiao%20and%20Xinyu%20Li%20and%20Xiaohan%20Shi&entry.1292438233=%20%203D%20reconstruction%20garners%20increasing%20attention%20alongside%20the%20advancement%20of%0Ahigh-level%20image%20applications%2C%20where%20dense%20stereo%20matching%20%28DSM%29%20serves%20as%20a%0Apivotal%20technique.%20Previous%20studies%20often%20rely%20on%20publicly%20available%20datasets%0Afor%20training%2C%20focusing%20on%20modifying%20network%20architectures%20or%20incorporating%0Aspecialized%20modules%20to%20extract%20domain-invariant%20features%20and%20thus%20improve%20model%0Arobustness.%20In%20contrast%2C%20inspired%20by%20single-frame%20structured-light%0Aphase-shifting%20encoding%2C%20this%20study%20introduces%20RGB-Speckle%2C%20a%20cross-scene%203D%0Areconstruction%20framework%20based%20on%20an%20active%20stereo%20camera%20system%2C%20designed%20to%0Aenhance%20robustness.%20Specifically%2C%20we%20propose%20a%20novel%20phase%20pre-normalization%0Aencoding-decoding%20method%3A%20first%2C%20we%20randomly%20perturb%20phase-shift%20maps%20and%20embed%0Athem%20into%20the%20three%20RGB%20channels%20to%20generate%20color%20speckle%20patterns%3B%0Asubsequently%2C%20the%20camera%20captures%20phase-encoded%20images%20modulated%20by%20objects%20as%0Ainput%20to%20a%20stereo%20matching%20network.%20This%20technique%20effectively%20mitigates%0Aexternal%20interference%20and%20ensures%20consistent%20input%20data%20for%20RGB-Speckle%2C%0Athereby%20bolstering%20cross-domain%203D%20reconstruction%20stability.%20To%20validate%20the%0Aproposed%20method%2C%20we%20conduct%20complex%20experiments%3A%20%281%29%20construct%20a%20color%20speckle%0Adataset%20for%20complex%20scenarios%20based%20on%20the%20proposed%20encoding%20scheme%3B%20%282%29%0Aevaluate%20the%20impact%20of%20the%20phase%20pre-normalization%20encoding-decoding%20technique%0Aon%203D%20reconstruction%20accuracy%3B%20and%20%283%29%20further%20investigate%20its%20robustness%0Aacross%20diverse%20conditions.%20Experimental%20results%20demonstrate%20that%20the%20proposed%0ARGB-Speckle%20model%20offers%20significant%20advantages%20in%20cross-domain%20and%20cross-scene%0A3D%20reconstruction%20tasks%2C%20enhancing%20model%20generalization%20and%20reinforcing%0Arobustness%20in%20challenging%20environments%2C%20thus%20providing%20a%20novel%20solution%20for%0Arobust%203D%20reconstruction%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.06125v2&entry.124074799=Read"},
{"title": "All-in-One Transferring Image Compression from Human Perception to\n  Multi-Machine Perception", "author": "Jiancheng Zhao and Xiang Ji and Zhuoxiao Li and Zunian Wan and Weihang Ran and Mingze Ma and Muyao Niu and Yifan Zhan and Cheng-Ching Tseng and Yinqiang Zheng", "abstract": "  Efficiently transferring Learned Image Compression (LIC) model from human\nperception to machine perception is an emerging challenge in vision-centric\nrepresentation learning. Existing approaches typically adapt LIC to downstream\ntasks in a single-task manner, which is inefficient, lacks task interaction,\nand results in multiple task-specific bitstreams. To address these limitations,\nwe propose an asymmetric adaptor framework that supports multi-task adaptation\nwithin a single model. Our method introduces a shared adaptor to learn general\nsemantic features and task-specific adaptors to preserve task-level\ndistinctions. With only lightweight plug-in modules and a frozen base codec,\nour method achieves strong performance across multiple tasks while maintaining\ncompression efficiency. Experiments on the PASCAL-Context benchmark demonstrate\nthat our method outperforms both Fully Fine-Tuned and other Parameter Efficient\nFine-Tuned (PEFT) baselines, and validating the effectiveness of multi-vision\ntransferring.\n", "link": "http://arxiv.org/abs/2504.12997v1", "date": "2025-04-17", "relevancy": 3.0039, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6354}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5878}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20All-in-One%20Transferring%20Image%20Compression%20from%20Human%20Perception%20to%0A%20%20Multi-Machine%20Perception&body=Title%3A%20All-in-One%20Transferring%20Image%20Compression%20from%20Human%20Perception%20to%0A%20%20Multi-Machine%20Perception%0AAuthor%3A%20Jiancheng%20Zhao%20and%20Xiang%20Ji%20and%20Zhuoxiao%20Li%20and%20Zunian%20Wan%20and%20Weihang%20Ran%20and%20Mingze%20Ma%20and%20Muyao%20Niu%20and%20Yifan%20Zhan%20and%20Cheng-Ching%20Tseng%20and%20Yinqiang%20Zheng%0AAbstract%3A%20%20%20Efficiently%20transferring%20Learned%20Image%20Compression%20%28LIC%29%20model%20from%20human%0Aperception%20to%20machine%20perception%20is%20an%20emerging%20challenge%20in%20vision-centric%0Arepresentation%20learning.%20Existing%20approaches%20typically%20adapt%20LIC%20to%20downstream%0Atasks%20in%20a%20single-task%20manner%2C%20which%20is%20inefficient%2C%20lacks%20task%20interaction%2C%0Aand%20results%20in%20multiple%20task-specific%20bitstreams.%20To%20address%20these%20limitations%2C%0Awe%20propose%20an%20asymmetric%20adaptor%20framework%20that%20supports%20multi-task%20adaptation%0Awithin%20a%20single%20model.%20Our%20method%20introduces%20a%20shared%20adaptor%20to%20learn%20general%0Asemantic%20features%20and%20task-specific%20adaptors%20to%20preserve%20task-level%0Adistinctions.%20With%20only%20lightweight%20plug-in%20modules%20and%20a%20frozen%20base%20codec%2C%0Aour%20method%20achieves%20strong%20performance%20across%20multiple%20tasks%20while%20maintaining%0Acompression%20efficiency.%20Experiments%20on%20the%20PASCAL-Context%20benchmark%20demonstrate%0Athat%20our%20method%20outperforms%20both%20Fully%20Fine-Tuned%20and%20other%20Parameter%20Efficient%0AFine-Tuned%20%28PEFT%29%20baselines%2C%20and%20validating%20the%20effectiveness%20of%20multi-vision%0Atransferring.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12997v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAll-in-One%2520Transferring%2520Image%2520Compression%2520from%2520Human%2520Perception%2520to%250A%2520%2520Multi-Machine%2520Perception%26entry.906535625%3DJiancheng%2520Zhao%2520and%2520Xiang%2520Ji%2520and%2520Zhuoxiao%2520Li%2520and%2520Zunian%2520Wan%2520and%2520Weihang%2520Ran%2520and%2520Mingze%2520Ma%2520and%2520Muyao%2520Niu%2520and%2520Yifan%2520Zhan%2520and%2520Cheng-Ching%2520Tseng%2520and%2520Yinqiang%2520Zheng%26entry.1292438233%3D%2520%2520Efficiently%2520transferring%2520Learned%2520Image%2520Compression%2520%2528LIC%2529%2520model%2520from%2520human%250Aperception%2520to%2520machine%2520perception%2520is%2520an%2520emerging%2520challenge%2520in%2520vision-centric%250Arepresentation%2520learning.%2520Existing%2520approaches%2520typically%2520adapt%2520LIC%2520to%2520downstream%250Atasks%2520in%2520a%2520single-task%2520manner%252C%2520which%2520is%2520inefficient%252C%2520lacks%2520task%2520interaction%252C%250Aand%2520results%2520in%2520multiple%2520task-specific%2520bitstreams.%2520To%2520address%2520these%2520limitations%252C%250Awe%2520propose%2520an%2520asymmetric%2520adaptor%2520framework%2520that%2520supports%2520multi-task%2520adaptation%250Awithin%2520a%2520single%2520model.%2520Our%2520method%2520introduces%2520a%2520shared%2520adaptor%2520to%2520learn%2520general%250Asemantic%2520features%2520and%2520task-specific%2520adaptors%2520to%2520preserve%2520task-level%250Adistinctions.%2520With%2520only%2520lightweight%2520plug-in%2520modules%2520and%2520a%2520frozen%2520base%2520codec%252C%250Aour%2520method%2520achieves%2520strong%2520performance%2520across%2520multiple%2520tasks%2520while%2520maintaining%250Acompression%2520efficiency.%2520Experiments%2520on%2520the%2520PASCAL-Context%2520benchmark%2520demonstrate%250Athat%2520our%2520method%2520outperforms%2520both%2520Fully%2520Fine-Tuned%2520and%2520other%2520Parameter%2520Efficient%250AFine-Tuned%2520%2528PEFT%2529%2520baselines%252C%2520and%2520validating%2520the%2520effectiveness%2520of%2520multi-vision%250Atransferring.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12997v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=All-in-One%20Transferring%20Image%20Compression%20from%20Human%20Perception%20to%0A%20%20Multi-Machine%20Perception&entry.906535625=Jiancheng%20Zhao%20and%20Xiang%20Ji%20and%20Zhuoxiao%20Li%20and%20Zunian%20Wan%20and%20Weihang%20Ran%20and%20Mingze%20Ma%20and%20Muyao%20Niu%20and%20Yifan%20Zhan%20and%20Cheng-Ching%20Tseng%20and%20Yinqiang%20Zheng&entry.1292438233=%20%20Efficiently%20transferring%20Learned%20Image%20Compression%20%28LIC%29%20model%20from%20human%0Aperception%20to%20machine%20perception%20is%20an%20emerging%20challenge%20in%20vision-centric%0Arepresentation%20learning.%20Existing%20approaches%20typically%20adapt%20LIC%20to%20downstream%0Atasks%20in%20a%20single-task%20manner%2C%20which%20is%20inefficient%2C%20lacks%20task%20interaction%2C%0Aand%20results%20in%20multiple%20task-specific%20bitstreams.%20To%20address%20these%20limitations%2C%0Awe%20propose%20an%20asymmetric%20adaptor%20framework%20that%20supports%20multi-task%20adaptation%0Awithin%20a%20single%20model.%20Our%20method%20introduces%20a%20shared%20adaptor%20to%20learn%20general%0Asemantic%20features%20and%20task-specific%20adaptors%20to%20preserve%20task-level%0Adistinctions.%20With%20only%20lightweight%20plug-in%20modules%20and%20a%20frozen%20base%20codec%2C%0Aour%20method%20achieves%20strong%20performance%20across%20multiple%20tasks%20while%20maintaining%0Acompression%20efficiency.%20Experiments%20on%20the%20PASCAL-Context%20benchmark%20demonstrate%0Athat%20our%20method%20outperforms%20both%20Fully%20Fine-Tuned%20and%20other%20Parameter%20Efficient%0AFine-Tuned%20%28PEFT%29%20baselines%2C%20and%20validating%20the%20effectiveness%20of%20multi-vision%0Atransferring.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12997v1&entry.124074799=Read"},
{"title": "Perception Encoder: The best visual embeddings are not at the output of\n  the network", "author": "Daniel Bolya and Po-Yao Huang and Peize Sun and Jang Hyun Cho and Andrea Madotto and Chen Wei and Tengyu Ma and Jiale Zhi and Jathushan Rajasegaran and Hanoona Rasheed and Junke Wang and Marco Monteiro and Hu Xu and Shiyu Dong and Nikhila Ravi and Daniel Li and Piotr Doll\u00e1r and Christoph Feichtenhofer", "abstract": "  We introduce Perception Encoder (PE), a state-of-the-art encoder for image\nand video understanding trained via simple vision-language learning.\nTraditionally, vision encoders have relied on a variety of pretraining\nobjectives, each tailored to specific downstream tasks such as classification,\ncaptioning, or localization. Surprisingly, after scaling our carefully tuned\nimage pretraining recipe and refining with our robust video data engine, we\nfind that contrastive vision-language training alone can produce strong,\ngeneral embeddings for all of these downstream tasks. There is only one caveat:\nthese embeddings are hidden within the intermediate layers of the network. To\ndraw them out, we introduce two alignment methods, language alignment for\nmultimodal language modeling, and spatial alignment for dense prediction.\nTogether with the core contrastive checkpoint, our PE family of models achieves\nstate-of-the-art performance on a wide variety of tasks, including zero-shot\nimage and video classification and retrieval; document, image, and video Q&A;\nand spatial tasks such as detection, depth estimation, and tracking. To foster\nfurther research, we are releasing our models, code, and a novel dataset of\nsynthetically and human-annotated videos.\n", "link": "http://arxiv.org/abs/2504.13181v1", "date": "2025-04-17", "relevancy": 3.0012, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6251}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6251}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perception%20Encoder%3A%20The%20best%20visual%20embeddings%20are%20not%20at%20the%20output%20of%0A%20%20the%20network&body=Title%3A%20Perception%20Encoder%3A%20The%20best%20visual%20embeddings%20are%20not%20at%20the%20output%20of%0A%20%20the%20network%0AAuthor%3A%20Daniel%20Bolya%20and%20Po-Yao%20Huang%20and%20Peize%20Sun%20and%20Jang%20Hyun%20Cho%20and%20Andrea%20Madotto%20and%20Chen%20Wei%20and%20Tengyu%20Ma%20and%20Jiale%20Zhi%20and%20Jathushan%20Rajasegaran%20and%20Hanoona%20Rasheed%20and%20Junke%20Wang%20and%20Marco%20Monteiro%20and%20Hu%20Xu%20and%20Shiyu%20Dong%20and%20Nikhila%20Ravi%20and%20Daniel%20Li%20and%20Piotr%20Doll%C3%A1r%20and%20Christoph%20Feichtenhofer%0AAbstract%3A%20%20%20We%20introduce%20Perception%20Encoder%20%28PE%29%2C%20a%20state-of-the-art%20encoder%20for%20image%0Aand%20video%20understanding%20trained%20via%20simple%20vision-language%20learning.%0ATraditionally%2C%20vision%20encoders%20have%20relied%20on%20a%20variety%20of%20pretraining%0Aobjectives%2C%20each%20tailored%20to%20specific%20downstream%20tasks%20such%20as%20classification%2C%0Acaptioning%2C%20or%20localization.%20Surprisingly%2C%20after%20scaling%20our%20carefully%20tuned%0Aimage%20pretraining%20recipe%20and%20refining%20with%20our%20robust%20video%20data%20engine%2C%20we%0Afind%20that%20contrastive%20vision-language%20training%20alone%20can%20produce%20strong%2C%0Ageneral%20embeddings%20for%20all%20of%20these%20downstream%20tasks.%20There%20is%20only%20one%20caveat%3A%0Athese%20embeddings%20are%20hidden%20within%20the%20intermediate%20layers%20of%20the%20network.%20To%0Adraw%20them%20out%2C%20we%20introduce%20two%20alignment%20methods%2C%20language%20alignment%20for%0Amultimodal%20language%20modeling%2C%20and%20spatial%20alignment%20for%20dense%20prediction.%0ATogether%20with%20the%20core%20contrastive%20checkpoint%2C%20our%20PE%20family%20of%20models%20achieves%0Astate-of-the-art%20performance%20on%20a%20wide%20variety%20of%20tasks%2C%20including%20zero-shot%0Aimage%20and%20video%20classification%20and%20retrieval%3B%20document%2C%20image%2C%20and%20video%20Q%26A%3B%0Aand%20spatial%20tasks%20such%20as%20detection%2C%20depth%20estimation%2C%20and%20tracking.%20To%20foster%0Afurther%20research%2C%20we%20are%20releasing%20our%20models%2C%20code%2C%20and%20a%20novel%20dataset%20of%0Asynthetically%20and%20human-annotated%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13181v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerception%2520Encoder%253A%2520The%2520best%2520visual%2520embeddings%2520are%2520not%2520at%2520the%2520output%2520of%250A%2520%2520the%2520network%26entry.906535625%3DDaniel%2520Bolya%2520and%2520Po-Yao%2520Huang%2520and%2520Peize%2520Sun%2520and%2520Jang%2520Hyun%2520Cho%2520and%2520Andrea%2520Madotto%2520and%2520Chen%2520Wei%2520and%2520Tengyu%2520Ma%2520and%2520Jiale%2520Zhi%2520and%2520Jathushan%2520Rajasegaran%2520and%2520Hanoona%2520Rasheed%2520and%2520Junke%2520Wang%2520and%2520Marco%2520Monteiro%2520and%2520Hu%2520Xu%2520and%2520Shiyu%2520Dong%2520and%2520Nikhila%2520Ravi%2520and%2520Daniel%2520Li%2520and%2520Piotr%2520Doll%25C3%25A1r%2520and%2520Christoph%2520Feichtenhofer%26entry.1292438233%3D%2520%2520We%2520introduce%2520Perception%2520Encoder%2520%2528PE%2529%252C%2520a%2520state-of-the-art%2520encoder%2520for%2520image%250Aand%2520video%2520understanding%2520trained%2520via%2520simple%2520vision-language%2520learning.%250ATraditionally%252C%2520vision%2520encoders%2520have%2520relied%2520on%2520a%2520variety%2520of%2520pretraining%250Aobjectives%252C%2520each%2520tailored%2520to%2520specific%2520downstream%2520tasks%2520such%2520as%2520classification%252C%250Acaptioning%252C%2520or%2520localization.%2520Surprisingly%252C%2520after%2520scaling%2520our%2520carefully%2520tuned%250Aimage%2520pretraining%2520recipe%2520and%2520refining%2520with%2520our%2520robust%2520video%2520data%2520engine%252C%2520we%250Afind%2520that%2520contrastive%2520vision-language%2520training%2520alone%2520can%2520produce%2520strong%252C%250Ageneral%2520embeddings%2520for%2520all%2520of%2520these%2520downstream%2520tasks.%2520There%2520is%2520only%2520one%2520caveat%253A%250Athese%2520embeddings%2520are%2520hidden%2520within%2520the%2520intermediate%2520layers%2520of%2520the%2520network.%2520To%250Adraw%2520them%2520out%252C%2520we%2520introduce%2520two%2520alignment%2520methods%252C%2520language%2520alignment%2520for%250Amultimodal%2520language%2520modeling%252C%2520and%2520spatial%2520alignment%2520for%2520dense%2520prediction.%250ATogether%2520with%2520the%2520core%2520contrastive%2520checkpoint%252C%2520our%2520PE%2520family%2520of%2520models%2520achieves%250Astate-of-the-art%2520performance%2520on%2520a%2520wide%2520variety%2520of%2520tasks%252C%2520including%2520zero-shot%250Aimage%2520and%2520video%2520classification%2520and%2520retrieval%253B%2520document%252C%2520image%252C%2520and%2520video%2520Q%2526A%253B%250Aand%2520spatial%2520tasks%2520such%2520as%2520detection%252C%2520depth%2520estimation%252C%2520and%2520tracking.%2520To%2520foster%250Afurther%2520research%252C%2520we%2520are%2520releasing%2520our%2520models%252C%2520code%252C%2520and%2520a%2520novel%2520dataset%2520of%250Asynthetically%2520and%2520human-annotated%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13181v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perception%20Encoder%3A%20The%20best%20visual%20embeddings%20are%20not%20at%20the%20output%20of%0A%20%20the%20network&entry.906535625=Daniel%20Bolya%20and%20Po-Yao%20Huang%20and%20Peize%20Sun%20and%20Jang%20Hyun%20Cho%20and%20Andrea%20Madotto%20and%20Chen%20Wei%20and%20Tengyu%20Ma%20and%20Jiale%20Zhi%20and%20Jathushan%20Rajasegaran%20and%20Hanoona%20Rasheed%20and%20Junke%20Wang%20and%20Marco%20Monteiro%20and%20Hu%20Xu%20and%20Shiyu%20Dong%20and%20Nikhila%20Ravi%20and%20Daniel%20Li%20and%20Piotr%20Doll%C3%A1r%20and%20Christoph%20Feichtenhofer&entry.1292438233=%20%20We%20introduce%20Perception%20Encoder%20%28PE%29%2C%20a%20state-of-the-art%20encoder%20for%20image%0Aand%20video%20understanding%20trained%20via%20simple%20vision-language%20learning.%0ATraditionally%2C%20vision%20encoders%20have%20relied%20on%20a%20variety%20of%20pretraining%0Aobjectives%2C%20each%20tailored%20to%20specific%20downstream%20tasks%20such%20as%20classification%2C%0Acaptioning%2C%20or%20localization.%20Surprisingly%2C%20after%20scaling%20our%20carefully%20tuned%0Aimage%20pretraining%20recipe%20and%20refining%20with%20our%20robust%20video%20data%20engine%2C%20we%0Afind%20that%20contrastive%20vision-language%20training%20alone%20can%20produce%20strong%2C%0Ageneral%20embeddings%20for%20all%20of%20these%20downstream%20tasks.%20There%20is%20only%20one%20caveat%3A%0Athese%20embeddings%20are%20hidden%20within%20the%20intermediate%20layers%20of%20the%20network.%20To%0Adraw%20them%20out%2C%20we%20introduce%20two%20alignment%20methods%2C%20language%20alignment%20for%0Amultimodal%20language%20modeling%2C%20and%20spatial%20alignment%20for%20dense%20prediction.%0ATogether%20with%20the%20core%20contrastive%20checkpoint%2C%20our%20PE%20family%20of%20models%20achieves%0Astate-of-the-art%20performance%20on%20a%20wide%20variety%20of%20tasks%2C%20including%20zero-shot%0Aimage%20and%20video%20classification%20and%20retrieval%3B%20document%2C%20image%2C%20and%20video%20Q%26A%3B%0Aand%20spatial%20tasks%20such%20as%20detection%2C%20depth%20estimation%2C%20and%20tracking.%20To%20foster%0Afurther%20research%2C%20we%20are%20releasing%20our%20models%2C%20code%2C%20and%20a%20novel%20dataset%20of%0Asynthetically%20and%20human-annotated%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13181v1&entry.124074799=Read"},
{"title": "Do Vision-Language Models Represent Space and How? Evaluating Spatial\n  Frame of Reference Under Ambiguities", "author": "Zheyuan Zhang and Fengyuan Hu and Jayjun Lee and Freda Shi and Parisa Kordjamshidi and Joyce Chai and Ziqiao Ma", "abstract": "  Spatial expressions in situated communication can be ambiguous, as their\nmeanings vary depending on the frames of reference (FoR) adopted by speakers\nand listeners. While spatial language understanding and reasoning by\nvision-language models (VLMs) have gained increasing attention, potential\nambiguities in these models are still under-explored. To address this issue, we\npresent the COnsistent Multilingual Frame Of Reference Test (COMFORT), an\nevaluation protocol to systematically assess the spatial reasoning capabilities\nof VLMs. We evaluate nine state-of-the-art VLMs using COMFORT. Despite showing\nsome alignment with English conventions in resolving ambiguities, our\nexperiments reveal significant shortcomings of VLMs: notably, the models (1)\nexhibit poor robustness and consistency, (2) lack the flexibility to\naccommodate multiple FoRs, and (3) fail to adhere to language-specific or\nculture-specific conventions in cross-lingual tests, as English tends to\ndominate other languages. With a growing effort to align vision-language models\nwith human cognitive intuitions, we call for more attention to the ambiguous\nnature and cross-cultural diversity of spatial reasoning.\n", "link": "http://arxiv.org/abs/2410.17385v2", "date": "2025-04-17", "relevancy": 2.9978, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6354}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6354}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Vision-Language%20Models%20Represent%20Space%20and%20How%3F%20Evaluating%20Spatial%0A%20%20Frame%20of%20Reference%20Under%20Ambiguities&body=Title%3A%20Do%20Vision-Language%20Models%20Represent%20Space%20and%20How%3F%20Evaluating%20Spatial%0A%20%20Frame%20of%20Reference%20Under%20Ambiguities%0AAuthor%3A%20Zheyuan%20Zhang%20and%20Fengyuan%20Hu%20and%20Jayjun%20Lee%20and%20Freda%20Shi%20and%20Parisa%20Kordjamshidi%20and%20Joyce%20Chai%20and%20Ziqiao%20Ma%0AAbstract%3A%20%20%20Spatial%20expressions%20in%20situated%20communication%20can%20be%20ambiguous%2C%20as%20their%0Ameanings%20vary%20depending%20on%20the%20frames%20of%20reference%20%28FoR%29%20adopted%20by%20speakers%0Aand%20listeners.%20While%20spatial%20language%20understanding%20and%20reasoning%20by%0Avision-language%20models%20%28VLMs%29%20have%20gained%20increasing%20attention%2C%20potential%0Aambiguities%20in%20these%20models%20are%20still%20under-explored.%20To%20address%20this%20issue%2C%20we%0Apresent%20the%20COnsistent%20Multilingual%20Frame%20Of%20Reference%20Test%20%28COMFORT%29%2C%20an%0Aevaluation%20protocol%20to%20systematically%20assess%20the%20spatial%20reasoning%20capabilities%0Aof%20VLMs.%20We%20evaluate%20nine%20state-of-the-art%20VLMs%20using%20COMFORT.%20Despite%20showing%0Asome%20alignment%20with%20English%20conventions%20in%20resolving%20ambiguities%2C%20our%0Aexperiments%20reveal%20significant%20shortcomings%20of%20VLMs%3A%20notably%2C%20the%20models%20%281%29%0Aexhibit%20poor%20robustness%20and%20consistency%2C%20%282%29%20lack%20the%20flexibility%20to%0Aaccommodate%20multiple%20FoRs%2C%20and%20%283%29%20fail%20to%20adhere%20to%20language-specific%20or%0Aculture-specific%20conventions%20in%20cross-lingual%20tests%2C%20as%20English%20tends%20to%0Adominate%20other%20languages.%20With%20a%20growing%20effort%20to%20align%20vision-language%20models%0Awith%20human%20cognitive%20intuitions%2C%20we%20call%20for%20more%20attention%20to%20the%20ambiguous%0Anature%20and%20cross-cultural%20diversity%20of%20spatial%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17385v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Vision-Language%2520Models%2520Represent%2520Space%2520and%2520How%253F%2520Evaluating%2520Spatial%250A%2520%2520Frame%2520of%2520Reference%2520Under%2520Ambiguities%26entry.906535625%3DZheyuan%2520Zhang%2520and%2520Fengyuan%2520Hu%2520and%2520Jayjun%2520Lee%2520and%2520Freda%2520Shi%2520and%2520Parisa%2520Kordjamshidi%2520and%2520Joyce%2520Chai%2520and%2520Ziqiao%2520Ma%26entry.1292438233%3D%2520%2520Spatial%2520expressions%2520in%2520situated%2520communication%2520can%2520be%2520ambiguous%252C%2520as%2520their%250Ameanings%2520vary%2520depending%2520on%2520the%2520frames%2520of%2520reference%2520%2528FoR%2529%2520adopted%2520by%2520speakers%250Aand%2520listeners.%2520While%2520spatial%2520language%2520understanding%2520and%2520reasoning%2520by%250Avision-language%2520models%2520%2528VLMs%2529%2520have%2520gained%2520increasing%2520attention%252C%2520potential%250Aambiguities%2520in%2520these%2520models%2520are%2520still%2520under-explored.%2520To%2520address%2520this%2520issue%252C%2520we%250Apresent%2520the%2520COnsistent%2520Multilingual%2520Frame%2520Of%2520Reference%2520Test%2520%2528COMFORT%2529%252C%2520an%250Aevaluation%2520protocol%2520to%2520systematically%2520assess%2520the%2520spatial%2520reasoning%2520capabilities%250Aof%2520VLMs.%2520We%2520evaluate%2520nine%2520state-of-the-art%2520VLMs%2520using%2520COMFORT.%2520Despite%2520showing%250Asome%2520alignment%2520with%2520English%2520conventions%2520in%2520resolving%2520ambiguities%252C%2520our%250Aexperiments%2520reveal%2520significant%2520shortcomings%2520of%2520VLMs%253A%2520notably%252C%2520the%2520models%2520%25281%2529%250Aexhibit%2520poor%2520robustness%2520and%2520consistency%252C%2520%25282%2529%2520lack%2520the%2520flexibility%2520to%250Aaccommodate%2520multiple%2520FoRs%252C%2520and%2520%25283%2529%2520fail%2520to%2520adhere%2520to%2520language-specific%2520or%250Aculture-specific%2520conventions%2520in%2520cross-lingual%2520tests%252C%2520as%2520English%2520tends%2520to%250Adominate%2520other%2520languages.%2520With%2520a%2520growing%2520effort%2520to%2520align%2520vision-language%2520models%250Awith%2520human%2520cognitive%2520intuitions%252C%2520we%2520call%2520for%2520more%2520attention%2520to%2520the%2520ambiguous%250Anature%2520and%2520cross-cultural%2520diversity%2520of%2520spatial%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17385v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Vision-Language%20Models%20Represent%20Space%20and%20How%3F%20Evaluating%20Spatial%0A%20%20Frame%20of%20Reference%20Under%20Ambiguities&entry.906535625=Zheyuan%20Zhang%20and%20Fengyuan%20Hu%20and%20Jayjun%20Lee%20and%20Freda%20Shi%20and%20Parisa%20Kordjamshidi%20and%20Joyce%20Chai%20and%20Ziqiao%20Ma&entry.1292438233=%20%20Spatial%20expressions%20in%20situated%20communication%20can%20be%20ambiguous%2C%20as%20their%0Ameanings%20vary%20depending%20on%20the%20frames%20of%20reference%20%28FoR%29%20adopted%20by%20speakers%0Aand%20listeners.%20While%20spatial%20language%20understanding%20and%20reasoning%20by%0Avision-language%20models%20%28VLMs%29%20have%20gained%20increasing%20attention%2C%20potential%0Aambiguities%20in%20these%20models%20are%20still%20under-explored.%20To%20address%20this%20issue%2C%20we%0Apresent%20the%20COnsistent%20Multilingual%20Frame%20Of%20Reference%20Test%20%28COMFORT%29%2C%20an%0Aevaluation%20protocol%20to%20systematically%20assess%20the%20spatial%20reasoning%20capabilities%0Aof%20VLMs.%20We%20evaluate%20nine%20state-of-the-art%20VLMs%20using%20COMFORT.%20Despite%20showing%0Asome%20alignment%20with%20English%20conventions%20in%20resolving%20ambiguities%2C%20our%0Aexperiments%20reveal%20significant%20shortcomings%20of%20VLMs%3A%20notably%2C%20the%20models%20%281%29%0Aexhibit%20poor%20robustness%20and%20consistency%2C%20%282%29%20lack%20the%20flexibility%20to%0Aaccommodate%20multiple%20FoRs%2C%20and%20%283%29%20fail%20to%20adhere%20to%20language-specific%20or%0Aculture-specific%20conventions%20in%20cross-lingual%20tests%2C%20as%20English%20tends%20to%0Adominate%20other%20languages.%20With%20a%20growing%20effort%20to%20align%20vision-language%20models%0Awith%20human%20cognitive%20intuitions%2C%20we%20call%20for%20more%20attention%20to%20the%20ambiguous%0Anature%20and%20cross-cultural%20diversity%20of%20spatial%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17385v2&entry.124074799=Read"},
{"title": "DG-MVP: 3D Domain Generalization via Multiple Views of Point Clouds for\n  Classification", "author": "Huantao Ren and Minmin Yang and Senem Velipasalar", "abstract": "  Deep neural networks have achieved significant success in 3D point cloud\nclassification while relying on large-scale, annotated point cloud datasets,\nwhich are labor-intensive to build. Compared to capturing data with LiDAR\nsensors and then performing annotation, it is relatively easier to sample point\nclouds from CAD models. Yet, data sampled from CAD models is regular, and does\nnot suffer from occlusion and missing points, which are very common for LiDAR\ndata, creating a large domain shift. Therefore, it is critical to develop\nmethods that can generalize well across different point cloud domains. %In this\npaper, we focus on the 3D point cloud domain generalization problem. Existing\n3D domain generalization methods employ point-based backbones to extract point\ncloud features. Yet, by analyzing point utilization of point-based methods and\nobserving the geometry of point clouds from different domains, we have found\nthat a large number of point features are discarded by point-based methods\nthrough the max-pooling operation. This is a significant waste especially\nconsidering the fact that domain generalization is more challenging than\nsupervised learning, and point clouds are already affected by missing points\nand occlusion to begin with. To address these issues, we propose a novel method\nfor 3D point cloud domain generalization, which can generalize to unseen\ndomains of point clouds. Our proposed method employs multiple 2D projections of\na 3D point cloud to alleviate the issue of missing points and involves a simple\nyet effective convolution-based model to extract features. The experiments,\nperformed on the PointDA-10 and Sim-to-Real benchmarks, demonstrate the\neffectiveness of our proposed method, which outperforms different baselines,\nand can transfer well from synthetic domain to real-world domain.\n", "link": "http://arxiv.org/abs/2504.12456v1", "date": "2025-04-16", "relevancy": 2.9877, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6114}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5909}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DG-MVP%3A%203D%20Domain%20Generalization%20via%20Multiple%20Views%20of%20Point%20Clouds%20for%0A%20%20Classification&body=Title%3A%20DG-MVP%3A%203D%20Domain%20Generalization%20via%20Multiple%20Views%20of%20Point%20Clouds%20for%0A%20%20Classification%0AAuthor%3A%20Huantao%20Ren%20and%20Minmin%20Yang%20and%20Senem%20Velipasalar%0AAbstract%3A%20%20%20Deep%20neural%20networks%20have%20achieved%20significant%20success%20in%203D%20point%20cloud%0Aclassification%20while%20relying%20on%20large-scale%2C%20annotated%20point%20cloud%20datasets%2C%0Awhich%20are%20labor-intensive%20to%20build.%20Compared%20to%20capturing%20data%20with%20LiDAR%0Asensors%20and%20then%20performing%20annotation%2C%20it%20is%20relatively%20easier%20to%20sample%20point%0Aclouds%20from%20CAD%20models.%20Yet%2C%20data%20sampled%20from%20CAD%20models%20is%20regular%2C%20and%20does%0Anot%20suffer%20from%20occlusion%20and%20missing%20points%2C%20which%20are%20very%20common%20for%20LiDAR%0Adata%2C%20creating%20a%20large%20domain%20shift.%20Therefore%2C%20it%20is%20critical%20to%20develop%0Amethods%20that%20can%20generalize%20well%20across%20different%20point%20cloud%20domains.%20%25In%20this%0Apaper%2C%20we%20focus%20on%20the%203D%20point%20cloud%20domain%20generalization%20problem.%20Existing%0A3D%20domain%20generalization%20methods%20employ%20point-based%20backbones%20to%20extract%20point%0Acloud%20features.%20Yet%2C%20by%20analyzing%20point%20utilization%20of%20point-based%20methods%20and%0Aobserving%20the%20geometry%20of%20point%20clouds%20from%20different%20domains%2C%20we%20have%20found%0Athat%20a%20large%20number%20of%20point%20features%20are%20discarded%20by%20point-based%20methods%0Athrough%20the%20max-pooling%20operation.%20This%20is%20a%20significant%20waste%20especially%0Aconsidering%20the%20fact%20that%20domain%20generalization%20is%20more%20challenging%20than%0Asupervised%20learning%2C%20and%20point%20clouds%20are%20already%20affected%20by%20missing%20points%0Aand%20occlusion%20to%20begin%20with.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20method%0Afor%203D%20point%20cloud%20domain%20generalization%2C%20which%20can%20generalize%20to%20unseen%0Adomains%20of%20point%20clouds.%20Our%20proposed%20method%20employs%20multiple%202D%20projections%20of%0Aa%203D%20point%20cloud%20to%20alleviate%20the%20issue%20of%20missing%20points%20and%20involves%20a%20simple%0Ayet%20effective%20convolution-based%20model%20to%20extract%20features.%20The%20experiments%2C%0Aperformed%20on%20the%20PointDA-10%20and%20Sim-to-Real%20benchmarks%2C%20demonstrate%20the%0Aeffectiveness%20of%20our%20proposed%20method%2C%20which%20outperforms%20different%20baselines%2C%0Aand%20can%20transfer%20well%20from%20synthetic%20domain%20to%20real-world%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12456v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDG-MVP%253A%25203D%2520Domain%2520Generalization%2520via%2520Multiple%2520Views%2520of%2520Point%2520Clouds%2520for%250A%2520%2520Classification%26entry.906535625%3DHuantao%2520Ren%2520and%2520Minmin%2520Yang%2520and%2520Senem%2520Velipasalar%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520have%2520achieved%2520significant%2520success%2520in%25203D%2520point%2520cloud%250Aclassification%2520while%2520relying%2520on%2520large-scale%252C%2520annotated%2520point%2520cloud%2520datasets%252C%250Awhich%2520are%2520labor-intensive%2520to%2520build.%2520Compared%2520to%2520capturing%2520data%2520with%2520LiDAR%250Asensors%2520and%2520then%2520performing%2520annotation%252C%2520it%2520is%2520relatively%2520easier%2520to%2520sample%2520point%250Aclouds%2520from%2520CAD%2520models.%2520Yet%252C%2520data%2520sampled%2520from%2520CAD%2520models%2520is%2520regular%252C%2520and%2520does%250Anot%2520suffer%2520from%2520occlusion%2520and%2520missing%2520points%252C%2520which%2520are%2520very%2520common%2520for%2520LiDAR%250Adata%252C%2520creating%2520a%2520large%2520domain%2520shift.%2520Therefore%252C%2520it%2520is%2520critical%2520to%2520develop%250Amethods%2520that%2520can%2520generalize%2520well%2520across%2520different%2520point%2520cloud%2520domains.%2520%2525In%2520this%250Apaper%252C%2520we%2520focus%2520on%2520the%25203D%2520point%2520cloud%2520domain%2520generalization%2520problem.%2520Existing%250A3D%2520domain%2520generalization%2520methods%2520employ%2520point-based%2520backbones%2520to%2520extract%2520point%250Acloud%2520features.%2520Yet%252C%2520by%2520analyzing%2520point%2520utilization%2520of%2520point-based%2520methods%2520and%250Aobserving%2520the%2520geometry%2520of%2520point%2520clouds%2520from%2520different%2520domains%252C%2520we%2520have%2520found%250Athat%2520a%2520large%2520number%2520of%2520point%2520features%2520are%2520discarded%2520by%2520point-based%2520methods%250Athrough%2520the%2520max-pooling%2520operation.%2520This%2520is%2520a%2520significant%2520waste%2520especially%250Aconsidering%2520the%2520fact%2520that%2520domain%2520generalization%2520is%2520more%2520challenging%2520than%250Asupervised%2520learning%252C%2520and%2520point%2520clouds%2520are%2520already%2520affected%2520by%2520missing%2520points%250Aand%2520occlusion%2520to%2520begin%2520with.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%2520method%250Afor%25203D%2520point%2520cloud%2520domain%2520generalization%252C%2520which%2520can%2520generalize%2520to%2520unseen%250Adomains%2520of%2520point%2520clouds.%2520Our%2520proposed%2520method%2520employs%2520multiple%25202D%2520projections%2520of%250Aa%25203D%2520point%2520cloud%2520to%2520alleviate%2520the%2520issue%2520of%2520missing%2520points%2520and%2520involves%2520a%2520simple%250Ayet%2520effective%2520convolution-based%2520model%2520to%2520extract%2520features.%2520The%2520experiments%252C%250Aperformed%2520on%2520the%2520PointDA-10%2520and%2520Sim-to-Real%2520benchmarks%252C%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520proposed%2520method%252C%2520which%2520outperforms%2520different%2520baselines%252C%250Aand%2520can%2520transfer%2520well%2520from%2520synthetic%2520domain%2520to%2520real-world%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12456v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DG-MVP%3A%203D%20Domain%20Generalization%20via%20Multiple%20Views%20of%20Point%20Clouds%20for%0A%20%20Classification&entry.906535625=Huantao%20Ren%20and%20Minmin%20Yang%20and%20Senem%20Velipasalar&entry.1292438233=%20%20Deep%20neural%20networks%20have%20achieved%20significant%20success%20in%203D%20point%20cloud%0Aclassification%20while%20relying%20on%20large-scale%2C%20annotated%20point%20cloud%20datasets%2C%0Awhich%20are%20labor-intensive%20to%20build.%20Compared%20to%20capturing%20data%20with%20LiDAR%0Asensors%20and%20then%20performing%20annotation%2C%20it%20is%20relatively%20easier%20to%20sample%20point%0Aclouds%20from%20CAD%20models.%20Yet%2C%20data%20sampled%20from%20CAD%20models%20is%20regular%2C%20and%20does%0Anot%20suffer%20from%20occlusion%20and%20missing%20points%2C%20which%20are%20very%20common%20for%20LiDAR%0Adata%2C%20creating%20a%20large%20domain%20shift.%20Therefore%2C%20it%20is%20critical%20to%20develop%0Amethods%20that%20can%20generalize%20well%20across%20different%20point%20cloud%20domains.%20%25In%20this%0Apaper%2C%20we%20focus%20on%20the%203D%20point%20cloud%20domain%20generalization%20problem.%20Existing%0A3D%20domain%20generalization%20methods%20employ%20point-based%20backbones%20to%20extract%20point%0Acloud%20features.%20Yet%2C%20by%20analyzing%20point%20utilization%20of%20point-based%20methods%20and%0Aobserving%20the%20geometry%20of%20point%20clouds%20from%20different%20domains%2C%20we%20have%20found%0Athat%20a%20large%20number%20of%20point%20features%20are%20discarded%20by%20point-based%20methods%0Athrough%20the%20max-pooling%20operation.%20This%20is%20a%20significant%20waste%20especially%0Aconsidering%20the%20fact%20that%20domain%20generalization%20is%20more%20challenging%20than%0Asupervised%20learning%2C%20and%20point%20clouds%20are%20already%20affected%20by%20missing%20points%0Aand%20occlusion%20to%20begin%20with.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20method%0Afor%203D%20point%20cloud%20domain%20generalization%2C%20which%20can%20generalize%20to%20unseen%0Adomains%20of%20point%20clouds.%20Our%20proposed%20method%20employs%20multiple%202D%20projections%20of%0Aa%203D%20point%20cloud%20to%20alleviate%20the%20issue%20of%20missing%20points%20and%20involves%20a%20simple%0Ayet%20effective%20convolution-based%20model%20to%20extract%20features.%20The%20experiments%2C%0Aperformed%20on%20the%20PointDA-10%20and%20Sim-to-Real%20benchmarks%2C%20demonstrate%20the%0Aeffectiveness%20of%20our%20proposed%20method%2C%20which%20outperforms%20different%20baselines%2C%0Aand%20can%20transfer%20well%20from%20synthetic%20domain%20to%20real-world%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12456v1&entry.124074799=Read"},
{"title": "AerialMegaDepth: Learning Aerial-Ground Reconstruction and View\n  Synthesis", "author": "Khiem Vuong and Anurag Ghosh and Deva Ramanan and Srinivasa Narasimhan and Shubham Tulsiani", "abstract": "  We explore the task of geometric reconstruction of images captured from a\nmixture of ground and aerial views. Current state-of-the-art learning-based\napproaches fail to handle the extreme viewpoint variation between aerial-ground\nimage pairs. Our hypothesis is that the lack of high-quality, co-registered\naerial-ground datasets for training is a key reason for this failure. Such data\nis difficult to assemble precisely because it is difficult to reconstruct in a\nscalable way. To overcome this challenge, we propose a scalable framework\ncombining pseudo-synthetic renderings from 3D city-wide meshes (e.g., Google\nEarth) with real, ground-level crowd-sourced images (e.g., MegaDepth). The\npseudo-synthetic data simulates a wide range of aerial viewpoints, while the\nreal, crowd-sourced images help improve visual fidelity for ground-level images\nwhere mesh-based renderings lack sufficient detail, effectively bridging the\ndomain gap between real images and pseudo-synthetic renderings. Using this\nhybrid dataset, we fine-tune several state-of-the-art algorithms and achieve\nsignificant improvements on real-world, zero-shot aerial-ground tasks. For\nexample, we observe that baseline DUSt3R localizes fewer than 5% of\naerial-ground pairs within 5 degrees of camera rotation error, while\nfine-tuning with our data raises accuracy to nearly 56%, addressing a major\nfailure point in handling large viewpoint changes. Beyond camera estimation and\nscene reconstruction, our dataset also improves performance on downstream tasks\nlike novel-view synthesis in challenging aerial-ground scenarios, demonstrating\nthe practical value of our approach in real-world applications.\n", "link": "http://arxiv.org/abs/2504.13157v1", "date": "2025-04-17", "relevancy": 2.9814, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6008}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5957}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AerialMegaDepth%3A%20Learning%20Aerial-Ground%20Reconstruction%20and%20View%0A%20%20Synthesis&body=Title%3A%20AerialMegaDepth%3A%20Learning%20Aerial-Ground%20Reconstruction%20and%20View%0A%20%20Synthesis%0AAuthor%3A%20Khiem%20Vuong%20and%20Anurag%20Ghosh%20and%20Deva%20Ramanan%20and%20Srinivasa%20Narasimhan%20and%20Shubham%20Tulsiani%0AAbstract%3A%20%20%20We%20explore%20the%20task%20of%20geometric%20reconstruction%20of%20images%20captured%20from%20a%0Amixture%20of%20ground%20and%20aerial%20views.%20Current%20state-of-the-art%20learning-based%0Aapproaches%20fail%20to%20handle%20the%20extreme%20viewpoint%20variation%20between%20aerial-ground%0Aimage%20pairs.%20Our%20hypothesis%20is%20that%20the%20lack%20of%20high-quality%2C%20co-registered%0Aaerial-ground%20datasets%20for%20training%20is%20a%20key%20reason%20for%20this%20failure.%20Such%20data%0Ais%20difficult%20to%20assemble%20precisely%20because%20it%20is%20difficult%20to%20reconstruct%20in%20a%0Ascalable%20way.%20To%20overcome%20this%20challenge%2C%20we%20propose%20a%20scalable%20framework%0Acombining%20pseudo-synthetic%20renderings%20from%203D%20city-wide%20meshes%20%28e.g.%2C%20Google%0AEarth%29%20with%20real%2C%20ground-level%20crowd-sourced%20images%20%28e.g.%2C%20MegaDepth%29.%20The%0Apseudo-synthetic%20data%20simulates%20a%20wide%20range%20of%20aerial%20viewpoints%2C%20while%20the%0Areal%2C%20crowd-sourced%20images%20help%20improve%20visual%20fidelity%20for%20ground-level%20images%0Awhere%20mesh-based%20renderings%20lack%20sufficient%20detail%2C%20effectively%20bridging%20the%0Adomain%20gap%20between%20real%20images%20and%20pseudo-synthetic%20renderings.%20Using%20this%0Ahybrid%20dataset%2C%20we%20fine-tune%20several%20state-of-the-art%20algorithms%20and%20achieve%0Asignificant%20improvements%20on%20real-world%2C%20zero-shot%20aerial-ground%20tasks.%20For%0Aexample%2C%20we%20observe%20that%20baseline%20DUSt3R%20localizes%20fewer%20than%205%25%20of%0Aaerial-ground%20pairs%20within%205%20degrees%20of%20camera%20rotation%20error%2C%20while%0Afine-tuning%20with%20our%20data%20raises%20accuracy%20to%20nearly%2056%25%2C%20addressing%20a%20major%0Afailure%20point%20in%20handling%20large%20viewpoint%20changes.%20Beyond%20camera%20estimation%20and%0Ascene%20reconstruction%2C%20our%20dataset%20also%20improves%20performance%20on%20downstream%20tasks%0Alike%20novel-view%20synthesis%20in%20challenging%20aerial-ground%20scenarios%2C%20demonstrating%0Athe%20practical%20value%20of%20our%20approach%20in%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13157v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAerialMegaDepth%253A%2520Learning%2520Aerial-Ground%2520Reconstruction%2520and%2520View%250A%2520%2520Synthesis%26entry.906535625%3DKhiem%2520Vuong%2520and%2520Anurag%2520Ghosh%2520and%2520Deva%2520Ramanan%2520and%2520Srinivasa%2520Narasimhan%2520and%2520Shubham%2520Tulsiani%26entry.1292438233%3D%2520%2520We%2520explore%2520the%2520task%2520of%2520geometric%2520reconstruction%2520of%2520images%2520captured%2520from%2520a%250Amixture%2520of%2520ground%2520and%2520aerial%2520views.%2520Current%2520state-of-the-art%2520learning-based%250Aapproaches%2520fail%2520to%2520handle%2520the%2520extreme%2520viewpoint%2520variation%2520between%2520aerial-ground%250Aimage%2520pairs.%2520Our%2520hypothesis%2520is%2520that%2520the%2520lack%2520of%2520high-quality%252C%2520co-registered%250Aaerial-ground%2520datasets%2520for%2520training%2520is%2520a%2520key%2520reason%2520for%2520this%2520failure.%2520Such%2520data%250Ais%2520difficult%2520to%2520assemble%2520precisely%2520because%2520it%2520is%2520difficult%2520to%2520reconstruct%2520in%2520a%250Ascalable%2520way.%2520To%2520overcome%2520this%2520challenge%252C%2520we%2520propose%2520a%2520scalable%2520framework%250Acombining%2520pseudo-synthetic%2520renderings%2520from%25203D%2520city-wide%2520meshes%2520%2528e.g.%252C%2520Google%250AEarth%2529%2520with%2520real%252C%2520ground-level%2520crowd-sourced%2520images%2520%2528e.g.%252C%2520MegaDepth%2529.%2520The%250Apseudo-synthetic%2520data%2520simulates%2520a%2520wide%2520range%2520of%2520aerial%2520viewpoints%252C%2520while%2520the%250Areal%252C%2520crowd-sourced%2520images%2520help%2520improve%2520visual%2520fidelity%2520for%2520ground-level%2520images%250Awhere%2520mesh-based%2520renderings%2520lack%2520sufficient%2520detail%252C%2520effectively%2520bridging%2520the%250Adomain%2520gap%2520between%2520real%2520images%2520and%2520pseudo-synthetic%2520renderings.%2520Using%2520this%250Ahybrid%2520dataset%252C%2520we%2520fine-tune%2520several%2520state-of-the-art%2520algorithms%2520and%2520achieve%250Asignificant%2520improvements%2520on%2520real-world%252C%2520zero-shot%2520aerial-ground%2520tasks.%2520For%250Aexample%252C%2520we%2520observe%2520that%2520baseline%2520DUSt3R%2520localizes%2520fewer%2520than%25205%2525%2520of%250Aaerial-ground%2520pairs%2520within%25205%2520degrees%2520of%2520camera%2520rotation%2520error%252C%2520while%250Afine-tuning%2520with%2520our%2520data%2520raises%2520accuracy%2520to%2520nearly%252056%2525%252C%2520addressing%2520a%2520major%250Afailure%2520point%2520in%2520handling%2520large%2520viewpoint%2520changes.%2520Beyond%2520camera%2520estimation%2520and%250Ascene%2520reconstruction%252C%2520our%2520dataset%2520also%2520improves%2520performance%2520on%2520downstream%2520tasks%250Alike%2520novel-view%2520synthesis%2520in%2520challenging%2520aerial-ground%2520scenarios%252C%2520demonstrating%250Athe%2520practical%2520value%2520of%2520our%2520approach%2520in%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13157v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AerialMegaDepth%3A%20Learning%20Aerial-Ground%20Reconstruction%20and%20View%0A%20%20Synthesis&entry.906535625=Khiem%20Vuong%20and%20Anurag%20Ghosh%20and%20Deva%20Ramanan%20and%20Srinivasa%20Narasimhan%20and%20Shubham%20Tulsiani&entry.1292438233=%20%20We%20explore%20the%20task%20of%20geometric%20reconstruction%20of%20images%20captured%20from%20a%0Amixture%20of%20ground%20and%20aerial%20views.%20Current%20state-of-the-art%20learning-based%0Aapproaches%20fail%20to%20handle%20the%20extreme%20viewpoint%20variation%20between%20aerial-ground%0Aimage%20pairs.%20Our%20hypothesis%20is%20that%20the%20lack%20of%20high-quality%2C%20co-registered%0Aaerial-ground%20datasets%20for%20training%20is%20a%20key%20reason%20for%20this%20failure.%20Such%20data%0Ais%20difficult%20to%20assemble%20precisely%20because%20it%20is%20difficult%20to%20reconstruct%20in%20a%0Ascalable%20way.%20To%20overcome%20this%20challenge%2C%20we%20propose%20a%20scalable%20framework%0Acombining%20pseudo-synthetic%20renderings%20from%203D%20city-wide%20meshes%20%28e.g.%2C%20Google%0AEarth%29%20with%20real%2C%20ground-level%20crowd-sourced%20images%20%28e.g.%2C%20MegaDepth%29.%20The%0Apseudo-synthetic%20data%20simulates%20a%20wide%20range%20of%20aerial%20viewpoints%2C%20while%20the%0Areal%2C%20crowd-sourced%20images%20help%20improve%20visual%20fidelity%20for%20ground-level%20images%0Awhere%20mesh-based%20renderings%20lack%20sufficient%20detail%2C%20effectively%20bridging%20the%0Adomain%20gap%20between%20real%20images%20and%20pseudo-synthetic%20renderings.%20Using%20this%0Ahybrid%20dataset%2C%20we%20fine-tune%20several%20state-of-the-art%20algorithms%20and%20achieve%0Asignificant%20improvements%20on%20real-world%2C%20zero-shot%20aerial-ground%20tasks.%20For%0Aexample%2C%20we%20observe%20that%20baseline%20DUSt3R%20localizes%20fewer%20than%205%25%20of%0Aaerial-ground%20pairs%20within%205%20degrees%20of%20camera%20rotation%20error%2C%20while%0Afine-tuning%20with%20our%20data%20raises%20accuracy%20to%20nearly%2056%25%2C%20addressing%20a%20major%0Afailure%20point%20in%20handling%20large%20viewpoint%20changes.%20Beyond%20camera%20estimation%20and%0Ascene%20reconstruction%2C%20our%20dataset%20also%20improves%20performance%20on%20downstream%20tasks%0Alike%20novel-view%20synthesis%20in%20challenging%20aerial-ground%20scenarios%2C%20demonstrating%0Athe%20practical%20value%20of%20our%20approach%20in%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13157v1&entry.124074799=Read"},
{"title": "Online Video Understanding: OVBench and VideoChat-Online", "author": "Zhenpeng Huang and Xinhao Li and Jiaqi Li and Jing Wang and Xiangyu Zeng and Cheng Liang and Tao Wu and Xi Chen and Liang Li and Limin Wang", "abstract": "  Multimodal Large Language Models (MLLMs) have significantly progressed in\noffline video understanding. However, applying these models to real-world\nscenarios, such as autonomous driving and human-computer interaction, presents\nunique challenges due to the need for real-time processing of continuous online\nvideo streams. To this end, this paper presents systematic efforts from three\nperspectives: evaluation benchmark, model architecture, and training strategy.\nFirst, we introduce OVBench, a comprehensive question-answering benchmark\ndesigned to evaluate models' ability to perceive, memorize, and reason within\nonline video contexts. It features 6 core task types across three temporal\ncontexts-past, current, and future-forming 16 subtasks from diverse datasets.\nSecond, we propose a new Pyramid Memory Bank (PMB) that effectively retains key\nspatiotemporal information in video streams. Third, we proposed an\noffline-to-online learning paradigm, designing an interleaved dialogue format\nfor online video data and constructing an instruction-tuning dataset tailored\nfor online video training. This framework led to the development of\nVideoChat-Online, a robust and efficient model for online video understanding.\nDespite the lower computational cost and higher efficiency, VideoChat-Online\noutperforms existing state-of-the-art offline and online models across popular\noffline video benchmarks and OVBench, demonstrating the effectiveness of our\nmodel architecture and training strategy. % Our approach surpasses existing\nstate-of-the-art offline models Qwen2-VL 7B and online models Flash-VStream, by\n4.19% and 23.7% on OVBench, respectively.\n", "link": "http://arxiv.org/abs/2501.00584v2", "date": "2025-04-17", "relevancy": 2.9699, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.618}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.618}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Video%20Understanding%3A%20OVBench%20and%20VideoChat-Online&body=Title%3A%20Online%20Video%20Understanding%3A%20OVBench%20and%20VideoChat-Online%0AAuthor%3A%20Zhenpeng%20Huang%20and%20Xinhao%20Li%20and%20Jiaqi%20Li%20and%20Jing%20Wang%20and%20Xiangyu%20Zeng%20and%20Cheng%20Liang%20and%20Tao%20Wu%20and%20Xi%20Chen%20and%20Liang%20Li%20and%20Limin%20Wang%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20significantly%20progressed%20in%0Aoffline%20video%20understanding.%20However%2C%20applying%20these%20models%20to%20real-world%0Ascenarios%2C%20such%20as%20autonomous%20driving%20and%20human-computer%20interaction%2C%20presents%0Aunique%20challenges%20due%20to%20the%20need%20for%20real-time%20processing%20of%20continuous%20online%0Avideo%20streams.%20To%20this%20end%2C%20this%20paper%20presents%20systematic%20efforts%20from%20three%0Aperspectives%3A%20evaluation%20benchmark%2C%20model%20architecture%2C%20and%20training%20strategy.%0AFirst%2C%20we%20introduce%20OVBench%2C%20a%20comprehensive%20question-answering%20benchmark%0Adesigned%20to%20evaluate%20models%27%20ability%20to%20perceive%2C%20memorize%2C%20and%20reason%20within%0Aonline%20video%20contexts.%20It%20features%206%20core%20task%20types%20across%20three%20temporal%0Acontexts-past%2C%20current%2C%20and%20future-forming%2016%20subtasks%20from%20diverse%20datasets.%0ASecond%2C%20we%20propose%20a%20new%20Pyramid%20Memory%20Bank%20%28PMB%29%20that%20effectively%20retains%20key%0Aspatiotemporal%20information%20in%20video%20streams.%20Third%2C%20we%20proposed%20an%0Aoffline-to-online%20learning%20paradigm%2C%20designing%20an%20interleaved%20dialogue%20format%0Afor%20online%20video%20data%20and%20constructing%20an%20instruction-tuning%20dataset%20tailored%0Afor%20online%20video%20training.%20This%20framework%20led%20to%20the%20development%20of%0AVideoChat-Online%2C%20a%20robust%20and%20efficient%20model%20for%20online%20video%20understanding.%0ADespite%20the%20lower%20computational%20cost%20and%20higher%20efficiency%2C%20VideoChat-Online%0Aoutperforms%20existing%20state-of-the-art%20offline%20and%20online%20models%20across%20popular%0Aoffline%20video%20benchmarks%20and%20OVBench%2C%20demonstrating%20the%20effectiveness%20of%20our%0Amodel%20architecture%20and%20training%20strategy.%20%25%20Our%20approach%20surpasses%20existing%0Astate-of-the-art%20offline%20models%20Qwen2-VL%207B%20and%20online%20models%20Flash-VStream%2C%20by%0A4.19%25%20and%2023.7%25%20on%20OVBench%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.00584v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Video%2520Understanding%253A%2520OVBench%2520and%2520VideoChat-Online%26entry.906535625%3DZhenpeng%2520Huang%2520and%2520Xinhao%2520Li%2520and%2520Jiaqi%2520Li%2520and%2520Jing%2520Wang%2520and%2520Xiangyu%2520Zeng%2520and%2520Cheng%2520Liang%2520and%2520Tao%2520Wu%2520and%2520Xi%2520Chen%2520and%2520Liang%2520Li%2520and%2520Limin%2520Wang%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520significantly%2520progressed%2520in%250Aoffline%2520video%2520understanding.%2520However%252C%2520applying%2520these%2520models%2520to%2520real-world%250Ascenarios%252C%2520such%2520as%2520autonomous%2520driving%2520and%2520human-computer%2520interaction%252C%2520presents%250Aunique%2520challenges%2520due%2520to%2520the%2520need%2520for%2520real-time%2520processing%2520of%2520continuous%2520online%250Avideo%2520streams.%2520To%2520this%2520end%252C%2520this%2520paper%2520presents%2520systematic%2520efforts%2520from%2520three%250Aperspectives%253A%2520evaluation%2520benchmark%252C%2520model%2520architecture%252C%2520and%2520training%2520strategy.%250AFirst%252C%2520we%2520introduce%2520OVBench%252C%2520a%2520comprehensive%2520question-answering%2520benchmark%250Adesigned%2520to%2520evaluate%2520models%2527%2520ability%2520to%2520perceive%252C%2520memorize%252C%2520and%2520reason%2520within%250Aonline%2520video%2520contexts.%2520It%2520features%25206%2520core%2520task%2520types%2520across%2520three%2520temporal%250Acontexts-past%252C%2520current%252C%2520and%2520future-forming%252016%2520subtasks%2520from%2520diverse%2520datasets.%250ASecond%252C%2520we%2520propose%2520a%2520new%2520Pyramid%2520Memory%2520Bank%2520%2528PMB%2529%2520that%2520effectively%2520retains%2520key%250Aspatiotemporal%2520information%2520in%2520video%2520streams.%2520Third%252C%2520we%2520proposed%2520an%250Aoffline-to-online%2520learning%2520paradigm%252C%2520designing%2520an%2520interleaved%2520dialogue%2520format%250Afor%2520online%2520video%2520data%2520and%2520constructing%2520an%2520instruction-tuning%2520dataset%2520tailored%250Afor%2520online%2520video%2520training.%2520This%2520framework%2520led%2520to%2520the%2520development%2520of%250AVideoChat-Online%252C%2520a%2520robust%2520and%2520efficient%2520model%2520for%2520online%2520video%2520understanding.%250ADespite%2520the%2520lower%2520computational%2520cost%2520and%2520higher%2520efficiency%252C%2520VideoChat-Online%250Aoutperforms%2520existing%2520state-of-the-art%2520offline%2520and%2520online%2520models%2520across%2520popular%250Aoffline%2520video%2520benchmarks%2520and%2520OVBench%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520our%250Amodel%2520architecture%2520and%2520training%2520strategy.%2520%2525%2520Our%2520approach%2520surpasses%2520existing%250Astate-of-the-art%2520offline%2520models%2520Qwen2-VL%25207B%2520and%2520online%2520models%2520Flash-VStream%252C%2520by%250A4.19%2525%2520and%252023.7%2525%2520on%2520OVBench%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.00584v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Video%20Understanding%3A%20OVBench%20and%20VideoChat-Online&entry.906535625=Zhenpeng%20Huang%20and%20Xinhao%20Li%20and%20Jiaqi%20Li%20and%20Jing%20Wang%20and%20Xiangyu%20Zeng%20and%20Cheng%20Liang%20and%20Tao%20Wu%20and%20Xi%20Chen%20and%20Liang%20Li%20and%20Limin%20Wang&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20significantly%20progressed%20in%0Aoffline%20video%20understanding.%20However%2C%20applying%20these%20models%20to%20real-world%0Ascenarios%2C%20such%20as%20autonomous%20driving%20and%20human-computer%20interaction%2C%20presents%0Aunique%20challenges%20due%20to%20the%20need%20for%20real-time%20processing%20of%20continuous%20online%0Avideo%20streams.%20To%20this%20end%2C%20this%20paper%20presents%20systematic%20efforts%20from%20three%0Aperspectives%3A%20evaluation%20benchmark%2C%20model%20architecture%2C%20and%20training%20strategy.%0AFirst%2C%20we%20introduce%20OVBench%2C%20a%20comprehensive%20question-answering%20benchmark%0Adesigned%20to%20evaluate%20models%27%20ability%20to%20perceive%2C%20memorize%2C%20and%20reason%20within%0Aonline%20video%20contexts.%20It%20features%206%20core%20task%20types%20across%20three%20temporal%0Acontexts-past%2C%20current%2C%20and%20future-forming%2016%20subtasks%20from%20diverse%20datasets.%0ASecond%2C%20we%20propose%20a%20new%20Pyramid%20Memory%20Bank%20%28PMB%29%20that%20effectively%20retains%20key%0Aspatiotemporal%20information%20in%20video%20streams.%20Third%2C%20we%20proposed%20an%0Aoffline-to-online%20learning%20paradigm%2C%20designing%20an%20interleaved%20dialogue%20format%0Afor%20online%20video%20data%20and%20constructing%20an%20instruction-tuning%20dataset%20tailored%0Afor%20online%20video%20training.%20This%20framework%20led%20to%20the%20development%20of%0AVideoChat-Online%2C%20a%20robust%20and%20efficient%20model%20for%20online%20video%20understanding.%0ADespite%20the%20lower%20computational%20cost%20and%20higher%20efficiency%2C%20VideoChat-Online%0Aoutperforms%20existing%20state-of-the-art%20offline%20and%20online%20models%20across%20popular%0Aoffline%20video%20benchmarks%20and%20OVBench%2C%20demonstrating%20the%20effectiveness%20of%20our%0Amodel%20architecture%20and%20training%20strategy.%20%25%20Our%20approach%20surpasses%20existing%0Astate-of-the-art%20offline%20models%20Qwen2-VL%207B%20and%20online%20models%20Flash-VStream%2C%20by%0A4.19%25%20and%2023.7%25%20on%20OVBench%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.00584v2&entry.124074799=Read"},
{"title": "ViTa-Zero: Zero-shot Visuotactile Object 6D Pose Estimation", "author": "Hongyu Li and James Akl and Srinath Sridhar and Tye Brady and Taskin Padir", "abstract": "  Object 6D pose estimation is a critical challenge in robotics, particularly\nfor manipulation tasks. While prior research combining visual and tactile\n(visuotactile) information has shown promise, these approaches often struggle\nwith generalization due to the limited availability of visuotactile data. In\nthis paper, we introduce ViTa-Zero, a zero-shot visuotactile pose estimation\nframework. Our key innovation lies in leveraging a visual model as its backbone\nand performing feasibility checking and test-time optimization based on\nphysical constraints derived from tactile and proprioceptive observations.\nSpecifically, we model the gripper-object interaction as a spring-mass system,\nwhere tactile sensors induce attractive forces, and proprioception generates\nrepulsive forces. We validate our framework through experiments on a real-world\nrobot setup, demonstrating its effectiveness across representative visual\nbackbones and manipulation scenarios, including grasping, object picking, and\nbimanual handover. Compared to the visual models, our approach overcomes some\ndrastic failure modes while tracking the in-hand object pose. In our\nexperiments, our approach shows an average increase of 55% in AUC of ADD-S and\n60% in ADD, along with an 80% lower position error compared to FoundationPose.\n", "link": "http://arxiv.org/abs/2504.13179v1", "date": "2025-04-17", "relevancy": 2.9687, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6147}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5969}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViTa-Zero%3A%20Zero-shot%20Visuotactile%20Object%206D%20Pose%20Estimation&body=Title%3A%20ViTa-Zero%3A%20Zero-shot%20Visuotactile%20Object%206D%20Pose%20Estimation%0AAuthor%3A%20Hongyu%20Li%20and%20James%20Akl%20and%20Srinath%20Sridhar%20and%20Tye%20Brady%20and%20Taskin%20Padir%0AAbstract%3A%20%20%20Object%206D%20pose%20estimation%20is%20a%20critical%20challenge%20in%20robotics%2C%20particularly%0Afor%20manipulation%20tasks.%20While%20prior%20research%20combining%20visual%20and%20tactile%0A%28visuotactile%29%20information%20has%20shown%20promise%2C%20these%20approaches%20often%20struggle%0Awith%20generalization%20due%20to%20the%20limited%20availability%20of%20visuotactile%20data.%20In%0Athis%20paper%2C%20we%20introduce%20ViTa-Zero%2C%20a%20zero-shot%20visuotactile%20pose%20estimation%0Aframework.%20Our%20key%20innovation%20lies%20in%20leveraging%20a%20visual%20model%20as%20its%20backbone%0Aand%20performing%20feasibility%20checking%20and%20test-time%20optimization%20based%20on%0Aphysical%20constraints%20derived%20from%20tactile%20and%20proprioceptive%20observations.%0ASpecifically%2C%20we%20model%20the%20gripper-object%20interaction%20as%20a%20spring-mass%20system%2C%0Awhere%20tactile%20sensors%20induce%20attractive%20forces%2C%20and%20proprioception%20generates%0Arepulsive%20forces.%20We%20validate%20our%20framework%20through%20experiments%20on%20a%20real-world%0Arobot%20setup%2C%20demonstrating%20its%20effectiveness%20across%20representative%20visual%0Abackbones%20and%20manipulation%20scenarios%2C%20including%20grasping%2C%20object%20picking%2C%20and%0Abimanual%20handover.%20Compared%20to%20the%20visual%20models%2C%20our%20approach%20overcomes%20some%0Adrastic%20failure%20modes%20while%20tracking%20the%20in-hand%20object%20pose.%20In%20our%0Aexperiments%2C%20our%20approach%20shows%20an%20average%20increase%20of%2055%25%20in%20AUC%20of%20ADD-S%20and%0A60%25%20in%20ADD%2C%20along%20with%20an%2080%25%20lower%20position%20error%20compared%20to%20FoundationPose.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13179v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViTa-Zero%253A%2520Zero-shot%2520Visuotactile%2520Object%25206D%2520Pose%2520Estimation%26entry.906535625%3DHongyu%2520Li%2520and%2520James%2520Akl%2520and%2520Srinath%2520Sridhar%2520and%2520Tye%2520Brady%2520and%2520Taskin%2520Padir%26entry.1292438233%3D%2520%2520Object%25206D%2520pose%2520estimation%2520is%2520a%2520critical%2520challenge%2520in%2520robotics%252C%2520particularly%250Afor%2520manipulation%2520tasks.%2520While%2520prior%2520research%2520combining%2520visual%2520and%2520tactile%250A%2528visuotactile%2529%2520information%2520has%2520shown%2520promise%252C%2520these%2520approaches%2520often%2520struggle%250Awith%2520generalization%2520due%2520to%2520the%2520limited%2520availability%2520of%2520visuotactile%2520data.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520ViTa-Zero%252C%2520a%2520zero-shot%2520visuotactile%2520pose%2520estimation%250Aframework.%2520Our%2520key%2520innovation%2520lies%2520in%2520leveraging%2520a%2520visual%2520model%2520as%2520its%2520backbone%250Aand%2520performing%2520feasibility%2520checking%2520and%2520test-time%2520optimization%2520based%2520on%250Aphysical%2520constraints%2520derived%2520from%2520tactile%2520and%2520proprioceptive%2520observations.%250ASpecifically%252C%2520we%2520model%2520the%2520gripper-object%2520interaction%2520as%2520a%2520spring-mass%2520system%252C%250Awhere%2520tactile%2520sensors%2520induce%2520attractive%2520forces%252C%2520and%2520proprioception%2520generates%250Arepulsive%2520forces.%2520We%2520validate%2520our%2520framework%2520through%2520experiments%2520on%2520a%2520real-world%250Arobot%2520setup%252C%2520demonstrating%2520its%2520effectiveness%2520across%2520representative%2520visual%250Abackbones%2520and%2520manipulation%2520scenarios%252C%2520including%2520grasping%252C%2520object%2520picking%252C%2520and%250Abimanual%2520handover.%2520Compared%2520to%2520the%2520visual%2520models%252C%2520our%2520approach%2520overcomes%2520some%250Adrastic%2520failure%2520modes%2520while%2520tracking%2520the%2520in-hand%2520object%2520pose.%2520In%2520our%250Aexperiments%252C%2520our%2520approach%2520shows%2520an%2520average%2520increase%2520of%252055%2525%2520in%2520AUC%2520of%2520ADD-S%2520and%250A60%2525%2520in%2520ADD%252C%2520along%2520with%2520an%252080%2525%2520lower%2520position%2520error%2520compared%2520to%2520FoundationPose.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13179v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViTa-Zero%3A%20Zero-shot%20Visuotactile%20Object%206D%20Pose%20Estimation&entry.906535625=Hongyu%20Li%20and%20James%20Akl%20and%20Srinath%20Sridhar%20and%20Tye%20Brady%20and%20Taskin%20Padir&entry.1292438233=%20%20Object%206D%20pose%20estimation%20is%20a%20critical%20challenge%20in%20robotics%2C%20particularly%0Afor%20manipulation%20tasks.%20While%20prior%20research%20combining%20visual%20and%20tactile%0A%28visuotactile%29%20information%20has%20shown%20promise%2C%20these%20approaches%20often%20struggle%0Awith%20generalization%20due%20to%20the%20limited%20availability%20of%20visuotactile%20data.%20In%0Athis%20paper%2C%20we%20introduce%20ViTa-Zero%2C%20a%20zero-shot%20visuotactile%20pose%20estimation%0Aframework.%20Our%20key%20innovation%20lies%20in%20leveraging%20a%20visual%20model%20as%20its%20backbone%0Aand%20performing%20feasibility%20checking%20and%20test-time%20optimization%20based%20on%0Aphysical%20constraints%20derived%20from%20tactile%20and%20proprioceptive%20observations.%0ASpecifically%2C%20we%20model%20the%20gripper-object%20interaction%20as%20a%20spring-mass%20system%2C%0Awhere%20tactile%20sensors%20induce%20attractive%20forces%2C%20and%20proprioception%20generates%0Arepulsive%20forces.%20We%20validate%20our%20framework%20through%20experiments%20on%20a%20real-world%0Arobot%20setup%2C%20demonstrating%20its%20effectiveness%20across%20representative%20visual%0Abackbones%20and%20manipulation%20scenarios%2C%20including%20grasping%2C%20object%20picking%2C%20and%0Abimanual%20handover.%20Compared%20to%20the%20visual%20models%2C%20our%20approach%20overcomes%20some%0Adrastic%20failure%20modes%20while%20tracking%20the%20in-hand%20object%20pose.%20In%20our%0Aexperiments%2C%20our%20approach%20shows%20an%20average%20increase%20of%2055%25%20in%20AUC%20of%20ADD-S%20and%0A60%25%20in%20ADD%2C%20along%20with%20an%2080%25%20lower%20position%20error%20compared%20to%20FoundationPose.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13179v1&entry.124074799=Read"},
{"title": "EarthGPT-X: Enabling MLLMs to Flexibly and Comprehensively Understand\n  Multi-Source Remote Sensing Imagery", "author": "Wei Zhang and Miaoxin Cai and Yaqian Ning and Tong Zhang and Yin Zhuang and He Chen and Jun Li and Xuerui Mao", "abstract": "  Recent advances in the visual-language area have developed natural\nmulti-modal large language models (MLLMs) for spatial reasoning through visual\nprompting. However, due to remote sensing (RS) imagery containing abundant\ngeospatial information that differs from natural images, it is challenging to\neffectively adapt natural spatial models to the RS domain. Moreover, current RS\nMLLMs are limited in overly narrow interpretation levels and interaction\nmanner, hindering their applicability in real-world scenarios. To address those\nchallenges, a spatial MLLM named EarthGPT-X is proposed, enabling a\ncomprehensive understanding of multi-source RS imagery, such as optical,\nsynthetic aperture radar (SAR), and infrared. EarthGPT-X offers zoom-in and\nzoom-out insight, and possesses flexible multi-grained interactive abilities.\nMoreover, EarthGPT-X unifies two types of critical spatial tasks (i.e.,\nreferring and grounding) into a visual prompting framework. To achieve these\nversatile capabilities, several key strategies are developed. The first is the\nmulti-modal content integration method, which enhances the interplay between\nimages, visual prompts, and text instructions. Subsequently, a cross-domain\none-stage fusion training strategy is proposed, utilizing the large language\nmodel (LLM) as a unified interface for multi-source multi-task learning.\nFurthermore, by incorporating a pixel perception module, the referring and\ngrounding tasks are seamlessly unified within a single framework. In addition,\nthe experiments conducted demonstrate the superiority of the proposed\nEarthGPT-X in multi-grained tasks and its impressive flexibility in multi-modal\ninteraction, revealing significant advancements of MLLM in the RS field.\n", "link": "http://arxiv.org/abs/2504.12795v1", "date": "2025-04-17", "relevancy": 2.9571, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5993}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5993}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EarthGPT-X%3A%20Enabling%20MLLMs%20to%20Flexibly%20and%20Comprehensively%20Understand%0A%20%20Multi-Source%20Remote%20Sensing%20Imagery&body=Title%3A%20EarthGPT-X%3A%20Enabling%20MLLMs%20to%20Flexibly%20and%20Comprehensively%20Understand%0A%20%20Multi-Source%20Remote%20Sensing%20Imagery%0AAuthor%3A%20Wei%20Zhang%20and%20Miaoxin%20Cai%20and%20Yaqian%20Ning%20and%20Tong%20Zhang%20and%20Yin%20Zhuang%20and%20He%20Chen%20and%20Jun%20Li%20and%20Xuerui%20Mao%0AAbstract%3A%20%20%20Recent%20advances%20in%20the%20visual-language%20area%20have%20developed%20natural%0Amulti-modal%20large%20language%20models%20%28MLLMs%29%20for%20spatial%20reasoning%20through%20visual%0Aprompting.%20However%2C%20due%20to%20remote%20sensing%20%28RS%29%20imagery%20containing%20abundant%0Ageospatial%20information%20that%20differs%20from%20natural%20images%2C%20it%20is%20challenging%20to%0Aeffectively%20adapt%20natural%20spatial%20models%20to%20the%20RS%20domain.%20Moreover%2C%20current%20RS%0AMLLMs%20are%20limited%20in%20overly%20narrow%20interpretation%20levels%20and%20interaction%0Amanner%2C%20hindering%20their%20applicability%20in%20real-world%20scenarios.%20To%20address%20those%0Achallenges%2C%20a%20spatial%20MLLM%20named%20EarthGPT-X%20is%20proposed%2C%20enabling%20a%0Acomprehensive%20understanding%20of%20multi-source%20RS%20imagery%2C%20such%20as%20optical%2C%0Asynthetic%20aperture%20radar%20%28SAR%29%2C%20and%20infrared.%20EarthGPT-X%20offers%20zoom-in%20and%0Azoom-out%20insight%2C%20and%20possesses%20flexible%20multi-grained%20interactive%20abilities.%0AMoreover%2C%20EarthGPT-X%20unifies%20two%20types%20of%20critical%20spatial%20tasks%20%28i.e.%2C%0Areferring%20and%20grounding%29%20into%20a%20visual%20prompting%20framework.%20To%20achieve%20these%0Aversatile%20capabilities%2C%20several%20key%20strategies%20are%20developed.%20The%20first%20is%20the%0Amulti-modal%20content%20integration%20method%2C%20which%20enhances%20the%20interplay%20between%0Aimages%2C%20visual%20prompts%2C%20and%20text%20instructions.%20Subsequently%2C%20a%20cross-domain%0Aone-stage%20fusion%20training%20strategy%20is%20proposed%2C%20utilizing%20the%20large%20language%0Amodel%20%28LLM%29%20as%20a%20unified%20interface%20for%20multi-source%20multi-task%20learning.%0AFurthermore%2C%20by%20incorporating%20a%20pixel%20perception%20module%2C%20the%20referring%20and%0Agrounding%20tasks%20are%20seamlessly%20unified%20within%20a%20single%20framework.%20In%20addition%2C%0Athe%20experiments%20conducted%20demonstrate%20the%20superiority%20of%20the%20proposed%0AEarthGPT-X%20in%20multi-grained%20tasks%20and%20its%20impressive%20flexibility%20in%20multi-modal%0Ainteraction%2C%20revealing%20significant%20advancements%20of%20MLLM%20in%20the%20RS%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12795v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEarthGPT-X%253A%2520Enabling%2520MLLMs%2520to%2520Flexibly%2520and%2520Comprehensively%2520Understand%250A%2520%2520Multi-Source%2520Remote%2520Sensing%2520Imagery%26entry.906535625%3DWei%2520Zhang%2520and%2520Miaoxin%2520Cai%2520and%2520Yaqian%2520Ning%2520and%2520Tong%2520Zhang%2520and%2520Yin%2520Zhuang%2520and%2520He%2520Chen%2520and%2520Jun%2520Li%2520and%2520Xuerui%2520Mao%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520the%2520visual-language%2520area%2520have%2520developed%2520natural%250Amulti-modal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520for%2520spatial%2520reasoning%2520through%2520visual%250Aprompting.%2520However%252C%2520due%2520to%2520remote%2520sensing%2520%2528RS%2529%2520imagery%2520containing%2520abundant%250Ageospatial%2520information%2520that%2520differs%2520from%2520natural%2520images%252C%2520it%2520is%2520challenging%2520to%250Aeffectively%2520adapt%2520natural%2520spatial%2520models%2520to%2520the%2520RS%2520domain.%2520Moreover%252C%2520current%2520RS%250AMLLMs%2520are%2520limited%2520in%2520overly%2520narrow%2520interpretation%2520levels%2520and%2520interaction%250Amanner%252C%2520hindering%2520their%2520applicability%2520in%2520real-world%2520scenarios.%2520To%2520address%2520those%250Achallenges%252C%2520a%2520spatial%2520MLLM%2520named%2520EarthGPT-X%2520is%2520proposed%252C%2520enabling%2520a%250Acomprehensive%2520understanding%2520of%2520multi-source%2520RS%2520imagery%252C%2520such%2520as%2520optical%252C%250Asynthetic%2520aperture%2520radar%2520%2528SAR%2529%252C%2520and%2520infrared.%2520EarthGPT-X%2520offers%2520zoom-in%2520and%250Azoom-out%2520insight%252C%2520and%2520possesses%2520flexible%2520multi-grained%2520interactive%2520abilities.%250AMoreover%252C%2520EarthGPT-X%2520unifies%2520two%2520types%2520of%2520critical%2520spatial%2520tasks%2520%2528i.e.%252C%250Areferring%2520and%2520grounding%2529%2520into%2520a%2520visual%2520prompting%2520framework.%2520To%2520achieve%2520these%250Aversatile%2520capabilities%252C%2520several%2520key%2520strategies%2520are%2520developed.%2520The%2520first%2520is%2520the%250Amulti-modal%2520content%2520integration%2520method%252C%2520which%2520enhances%2520the%2520interplay%2520between%250Aimages%252C%2520visual%2520prompts%252C%2520and%2520text%2520instructions.%2520Subsequently%252C%2520a%2520cross-domain%250Aone-stage%2520fusion%2520training%2520strategy%2520is%2520proposed%252C%2520utilizing%2520the%2520large%2520language%250Amodel%2520%2528LLM%2529%2520as%2520a%2520unified%2520interface%2520for%2520multi-source%2520multi-task%2520learning.%250AFurthermore%252C%2520by%2520incorporating%2520a%2520pixel%2520perception%2520module%252C%2520the%2520referring%2520and%250Agrounding%2520tasks%2520are%2520seamlessly%2520unified%2520within%2520a%2520single%2520framework.%2520In%2520addition%252C%250Athe%2520experiments%2520conducted%2520demonstrate%2520the%2520superiority%2520of%2520the%2520proposed%250AEarthGPT-X%2520in%2520multi-grained%2520tasks%2520and%2520its%2520impressive%2520flexibility%2520in%2520multi-modal%250Ainteraction%252C%2520revealing%2520significant%2520advancements%2520of%2520MLLM%2520in%2520the%2520RS%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12795v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EarthGPT-X%3A%20Enabling%20MLLMs%20to%20Flexibly%20and%20Comprehensively%20Understand%0A%20%20Multi-Source%20Remote%20Sensing%20Imagery&entry.906535625=Wei%20Zhang%20and%20Miaoxin%20Cai%20and%20Yaqian%20Ning%20and%20Tong%20Zhang%20and%20Yin%20Zhuang%20and%20He%20Chen%20and%20Jun%20Li%20and%20Xuerui%20Mao&entry.1292438233=%20%20Recent%20advances%20in%20the%20visual-language%20area%20have%20developed%20natural%0Amulti-modal%20large%20language%20models%20%28MLLMs%29%20for%20spatial%20reasoning%20through%20visual%0Aprompting.%20However%2C%20due%20to%20remote%20sensing%20%28RS%29%20imagery%20containing%20abundant%0Ageospatial%20information%20that%20differs%20from%20natural%20images%2C%20it%20is%20challenging%20to%0Aeffectively%20adapt%20natural%20spatial%20models%20to%20the%20RS%20domain.%20Moreover%2C%20current%20RS%0AMLLMs%20are%20limited%20in%20overly%20narrow%20interpretation%20levels%20and%20interaction%0Amanner%2C%20hindering%20their%20applicability%20in%20real-world%20scenarios.%20To%20address%20those%0Achallenges%2C%20a%20spatial%20MLLM%20named%20EarthGPT-X%20is%20proposed%2C%20enabling%20a%0Acomprehensive%20understanding%20of%20multi-source%20RS%20imagery%2C%20such%20as%20optical%2C%0Asynthetic%20aperture%20radar%20%28SAR%29%2C%20and%20infrared.%20EarthGPT-X%20offers%20zoom-in%20and%0Azoom-out%20insight%2C%20and%20possesses%20flexible%20multi-grained%20interactive%20abilities.%0AMoreover%2C%20EarthGPT-X%20unifies%20two%20types%20of%20critical%20spatial%20tasks%20%28i.e.%2C%0Areferring%20and%20grounding%29%20into%20a%20visual%20prompting%20framework.%20To%20achieve%20these%0Aversatile%20capabilities%2C%20several%20key%20strategies%20are%20developed.%20The%20first%20is%20the%0Amulti-modal%20content%20integration%20method%2C%20which%20enhances%20the%20interplay%20between%0Aimages%2C%20visual%20prompts%2C%20and%20text%20instructions.%20Subsequently%2C%20a%20cross-domain%0Aone-stage%20fusion%20training%20strategy%20is%20proposed%2C%20utilizing%20the%20large%20language%0Amodel%20%28LLM%29%20as%20a%20unified%20interface%20for%20multi-source%20multi-task%20learning.%0AFurthermore%2C%20by%20incorporating%20a%20pixel%20perception%20module%2C%20the%20referring%20and%0Agrounding%20tasks%20are%20seamlessly%20unified%20within%20a%20single%20framework.%20In%20addition%2C%0Athe%20experiments%20conducted%20demonstrate%20the%20superiority%20of%20the%20proposed%0AEarthGPT-X%20in%20multi-grained%20tasks%20and%20its%20impressive%20flexibility%20in%20multi-modal%0Ainteraction%2C%20revealing%20significant%20advancements%20of%20MLLM%20in%20the%20RS%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12795v1&entry.124074799=Read"},
{"title": "3D-PointZshotS: Geometry-Aware 3D Point Cloud Zero-Shot Semantic\n  Segmentation Narrowing the Visual-Semantic Gap", "author": "Minmin Yang and Huantao Ren and Senem Velipasalar", "abstract": "  Existing zero-shot 3D point cloud segmentation methods often struggle with\nlimited transferability from seen classes to unseen classes and from semantic\nto visual space. To alleviate this, we introduce 3D-PointZshotS, a\ngeometry-aware zero-shot segmentation framework that enhances both feature\ngeneration and alignment using latent geometric prototypes (LGPs).\nSpecifically, we integrate LGPs into a generator via a cross-attention\nmechanism, enriching semantic features with fine-grained geometric details. To\nfurther enhance stability and generalization, we introduce a self-consistency\nloss, which enforces feature robustness against point-wise perturbations.\nAdditionally, we re-represent visual and semantic features in a shared space,\nbridging the semantic-visual gap and facilitating knowledge transfer to unseen\nclasses. Experiments on three real-world datasets, namely ScanNet,\nSemanticKITTI, and S3DIS, demonstrate that our method achieves superior\nperformance over four baselines in terms of harmonic mIoU. The code is\navailable at \\href{https://github.com/LexieYang/3D-PointZshotS}{Github}.\n", "link": "http://arxiv.org/abs/2504.12442v1", "date": "2025-04-16", "relevancy": 2.955, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6003}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5883}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D-PointZshotS%3A%20Geometry-Aware%203D%20Point%20Cloud%20Zero-Shot%20Semantic%0A%20%20Segmentation%20Narrowing%20the%20Visual-Semantic%20Gap&body=Title%3A%203D-PointZshotS%3A%20Geometry-Aware%203D%20Point%20Cloud%20Zero-Shot%20Semantic%0A%20%20Segmentation%20Narrowing%20the%20Visual-Semantic%20Gap%0AAuthor%3A%20Minmin%20Yang%20and%20Huantao%20Ren%20and%20Senem%20Velipasalar%0AAbstract%3A%20%20%20Existing%20zero-shot%203D%20point%20cloud%20segmentation%20methods%20often%20struggle%20with%0Alimited%20transferability%20from%20seen%20classes%20to%20unseen%20classes%20and%20from%20semantic%0Ato%20visual%20space.%20To%20alleviate%20this%2C%20we%20introduce%203D-PointZshotS%2C%20a%0Ageometry-aware%20zero-shot%20segmentation%20framework%20that%20enhances%20both%20feature%0Ageneration%20and%20alignment%20using%20latent%20geometric%20prototypes%20%28LGPs%29.%0ASpecifically%2C%20we%20integrate%20LGPs%20into%20a%20generator%20via%20a%20cross-attention%0Amechanism%2C%20enriching%20semantic%20features%20with%20fine-grained%20geometric%20details.%20To%0Afurther%20enhance%20stability%20and%20generalization%2C%20we%20introduce%20a%20self-consistency%0Aloss%2C%20which%20enforces%20feature%20robustness%20against%20point-wise%20perturbations.%0AAdditionally%2C%20we%20re-represent%20visual%20and%20semantic%20features%20in%20a%20shared%20space%2C%0Abridging%20the%20semantic-visual%20gap%20and%20facilitating%20knowledge%20transfer%20to%20unseen%0Aclasses.%20Experiments%20on%20three%20real-world%20datasets%2C%20namely%20ScanNet%2C%0ASemanticKITTI%2C%20and%20S3DIS%2C%20demonstrate%20that%20our%20method%20achieves%20superior%0Aperformance%20over%20four%20baselines%20in%20terms%20of%20harmonic%20mIoU.%20The%20code%20is%0Aavailable%20at%20%5Chref%7Bhttps%3A//github.com/LexieYang/3D-PointZshotS%7D%7BGithub%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12442v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D-PointZshotS%253A%2520Geometry-Aware%25203D%2520Point%2520Cloud%2520Zero-Shot%2520Semantic%250A%2520%2520Segmentation%2520Narrowing%2520the%2520Visual-Semantic%2520Gap%26entry.906535625%3DMinmin%2520Yang%2520and%2520Huantao%2520Ren%2520and%2520Senem%2520Velipasalar%26entry.1292438233%3D%2520%2520Existing%2520zero-shot%25203D%2520point%2520cloud%2520segmentation%2520methods%2520often%2520struggle%2520with%250Alimited%2520transferability%2520from%2520seen%2520classes%2520to%2520unseen%2520classes%2520and%2520from%2520semantic%250Ato%2520visual%2520space.%2520To%2520alleviate%2520this%252C%2520we%2520introduce%25203D-PointZshotS%252C%2520a%250Ageometry-aware%2520zero-shot%2520segmentation%2520framework%2520that%2520enhances%2520both%2520feature%250Ageneration%2520and%2520alignment%2520using%2520latent%2520geometric%2520prototypes%2520%2528LGPs%2529.%250ASpecifically%252C%2520we%2520integrate%2520LGPs%2520into%2520a%2520generator%2520via%2520a%2520cross-attention%250Amechanism%252C%2520enriching%2520semantic%2520features%2520with%2520fine-grained%2520geometric%2520details.%2520To%250Afurther%2520enhance%2520stability%2520and%2520generalization%252C%2520we%2520introduce%2520a%2520self-consistency%250Aloss%252C%2520which%2520enforces%2520feature%2520robustness%2520against%2520point-wise%2520perturbations.%250AAdditionally%252C%2520we%2520re-represent%2520visual%2520and%2520semantic%2520features%2520in%2520a%2520shared%2520space%252C%250Abridging%2520the%2520semantic-visual%2520gap%2520and%2520facilitating%2520knowledge%2520transfer%2520to%2520unseen%250Aclasses.%2520Experiments%2520on%2520three%2520real-world%2520datasets%252C%2520namely%2520ScanNet%252C%250ASemanticKITTI%252C%2520and%2520S3DIS%252C%2520demonstrate%2520that%2520our%2520method%2520achieves%2520superior%250Aperformance%2520over%2520four%2520baselines%2520in%2520terms%2520of%2520harmonic%2520mIoU.%2520The%2520code%2520is%250Aavailable%2520at%2520%255Chref%257Bhttps%253A//github.com/LexieYang/3D-PointZshotS%257D%257BGithub%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12442v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-PointZshotS%3A%20Geometry-Aware%203D%20Point%20Cloud%20Zero-Shot%20Semantic%0A%20%20Segmentation%20Narrowing%20the%20Visual-Semantic%20Gap&entry.906535625=Minmin%20Yang%20and%20Huantao%20Ren%20and%20Senem%20Velipasalar&entry.1292438233=%20%20Existing%20zero-shot%203D%20point%20cloud%20segmentation%20methods%20often%20struggle%20with%0Alimited%20transferability%20from%20seen%20classes%20to%20unseen%20classes%20and%20from%20semantic%0Ato%20visual%20space.%20To%20alleviate%20this%2C%20we%20introduce%203D-PointZshotS%2C%20a%0Ageometry-aware%20zero-shot%20segmentation%20framework%20that%20enhances%20both%20feature%0Ageneration%20and%20alignment%20using%20latent%20geometric%20prototypes%20%28LGPs%29.%0ASpecifically%2C%20we%20integrate%20LGPs%20into%20a%20generator%20via%20a%20cross-attention%0Amechanism%2C%20enriching%20semantic%20features%20with%20fine-grained%20geometric%20details.%20To%0Afurther%20enhance%20stability%20and%20generalization%2C%20we%20introduce%20a%20self-consistency%0Aloss%2C%20which%20enforces%20feature%20robustness%20against%20point-wise%20perturbations.%0AAdditionally%2C%20we%20re-represent%20visual%20and%20semantic%20features%20in%20a%20shared%20space%2C%0Abridging%20the%20semantic-visual%20gap%20and%20facilitating%20knowledge%20transfer%20to%20unseen%0Aclasses.%20Experiments%20on%20three%20real-world%20datasets%2C%20namely%20ScanNet%2C%0ASemanticKITTI%2C%20and%20S3DIS%2C%20demonstrate%20that%20our%20method%20achieves%20superior%0Aperformance%20over%20four%20baselines%20in%20terms%20of%20harmonic%20mIoU.%20The%20code%20is%0Aavailable%20at%20%5Chref%7Bhttps%3A//github.com/LexieYang/3D-PointZshotS%7D%7BGithub%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12442v1&entry.124074799=Read"},
{"title": "CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography", "author": "I-Sheng Fang and Jun-Cheng Chen", "abstract": "  Large language models (LLMs) and multimodal large language models (MLLMs)\nhave significantly advanced artificial intelligence. However, visual reasoning,\nreasoning involving both visual and textual inputs, remains underexplored.\nRecent advancements, including the reasoning models like OpenAI o1 and Gemini\n2.0 Flash Thinking, which incorporate image inputs, have opened this\ncapability. In this ongoing work, we focus specifically on photography-related\ntasks because a photo is a visual snapshot of the physical world where the\nunderlying physics (i.e., illumination, blur extent, etc.) interplay with the\ncamera parameters. Successfully reasoning from the visual information of a\nphoto to identify these numerical camera settings requires the MLLMs to have a\ndeeper understanding of the underlying physics for precise visual\ncomprehension, representing a challenging and intelligent capability essential\nfor practical applications like photography assistant agents. We aim to\nevaluate MLLMs on their ability to distinguish visual differences related to\nnumerical camera settings, extending a methodology previously proposed for\nvision-language models (VLMs). Our preliminary results demonstrate the\nimportance of visual reasoning in photography-related tasks. Moreover, these\nresults show that no single MLLM consistently dominates across all evaluation\ntasks, demonstrating ongoing challenges and opportunities in developing MLLMs\nwith better visual reasoning.\n", "link": "http://arxiv.org/abs/2504.10090v2", "date": "2025-04-17", "relevancy": 2.943, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6063}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6063}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CameraBench%3A%20Benchmarking%20Visual%20Reasoning%20in%20MLLMs%20via%20Photography&body=Title%3A%20CameraBench%3A%20Benchmarking%20Visual%20Reasoning%20in%20MLLMs%20via%20Photography%0AAuthor%3A%20I-Sheng%20Fang%20and%20Jun-Cheng%20Chen%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20and%20multimodal%20large%20language%20models%20%28MLLMs%29%0Ahave%20significantly%20advanced%20artificial%20intelligence.%20However%2C%20visual%20reasoning%2C%0Areasoning%20involving%20both%20visual%20and%20textual%20inputs%2C%20remains%20underexplored.%0ARecent%20advancements%2C%20including%20the%20reasoning%20models%20like%20OpenAI%20o1%20and%20Gemini%0A2.0%20Flash%20Thinking%2C%20which%20incorporate%20image%20inputs%2C%20have%20opened%20this%0Acapability.%20In%20this%20ongoing%20work%2C%20we%20focus%20specifically%20on%20photography-related%0Atasks%20because%20a%20photo%20is%20a%20visual%20snapshot%20of%20the%20physical%20world%20where%20the%0Aunderlying%20physics%20%28i.e.%2C%20illumination%2C%20blur%20extent%2C%20etc.%29%20interplay%20with%20the%0Acamera%20parameters.%20Successfully%20reasoning%20from%20the%20visual%20information%20of%20a%0Aphoto%20to%20identify%20these%20numerical%20camera%20settings%20requires%20the%20MLLMs%20to%20have%20a%0Adeeper%20understanding%20of%20the%20underlying%20physics%20for%20precise%20visual%0Acomprehension%2C%20representing%20a%20challenging%20and%20intelligent%20capability%20essential%0Afor%20practical%20applications%20like%20photography%20assistant%20agents.%20We%20aim%20to%0Aevaluate%20MLLMs%20on%20their%20ability%20to%20distinguish%20visual%20differences%20related%20to%0Anumerical%20camera%20settings%2C%20extending%20a%20methodology%20previously%20proposed%20for%0Avision-language%20models%20%28VLMs%29.%20Our%20preliminary%20results%20demonstrate%20the%0Aimportance%20of%20visual%20reasoning%20in%20photography-related%20tasks.%20Moreover%2C%20these%0Aresults%20show%20that%20no%20single%20MLLM%20consistently%20dominates%20across%20all%20evaluation%0Atasks%2C%20demonstrating%20ongoing%20challenges%20and%20opportunities%20in%20developing%20MLLMs%0Awith%20better%20visual%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.10090v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCameraBench%253A%2520Benchmarking%2520Visual%2520Reasoning%2520in%2520MLLMs%2520via%2520Photography%26entry.906535625%3DI-Sheng%2520Fang%2520and%2520Jun-Cheng%2520Chen%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520and%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%250Ahave%2520significantly%2520advanced%2520artificial%2520intelligence.%2520However%252C%2520visual%2520reasoning%252C%250Areasoning%2520involving%2520both%2520visual%2520and%2520textual%2520inputs%252C%2520remains%2520underexplored.%250ARecent%2520advancements%252C%2520including%2520the%2520reasoning%2520models%2520like%2520OpenAI%2520o1%2520and%2520Gemini%250A2.0%2520Flash%2520Thinking%252C%2520which%2520incorporate%2520image%2520inputs%252C%2520have%2520opened%2520this%250Acapability.%2520In%2520this%2520ongoing%2520work%252C%2520we%2520focus%2520specifically%2520on%2520photography-related%250Atasks%2520because%2520a%2520photo%2520is%2520a%2520visual%2520snapshot%2520of%2520the%2520physical%2520world%2520where%2520the%250Aunderlying%2520physics%2520%2528i.e.%252C%2520illumination%252C%2520blur%2520extent%252C%2520etc.%2529%2520interplay%2520with%2520the%250Acamera%2520parameters.%2520Successfully%2520reasoning%2520from%2520the%2520visual%2520information%2520of%2520a%250Aphoto%2520to%2520identify%2520these%2520numerical%2520camera%2520settings%2520requires%2520the%2520MLLMs%2520to%2520have%2520a%250Adeeper%2520understanding%2520of%2520the%2520underlying%2520physics%2520for%2520precise%2520visual%250Acomprehension%252C%2520representing%2520a%2520challenging%2520and%2520intelligent%2520capability%2520essential%250Afor%2520practical%2520applications%2520like%2520photography%2520assistant%2520agents.%2520We%2520aim%2520to%250Aevaluate%2520MLLMs%2520on%2520their%2520ability%2520to%2520distinguish%2520visual%2520differences%2520related%2520to%250Anumerical%2520camera%2520settings%252C%2520extending%2520a%2520methodology%2520previously%2520proposed%2520for%250Avision-language%2520models%2520%2528VLMs%2529.%2520Our%2520preliminary%2520results%2520demonstrate%2520the%250Aimportance%2520of%2520visual%2520reasoning%2520in%2520photography-related%2520tasks.%2520Moreover%252C%2520these%250Aresults%2520show%2520that%2520no%2520single%2520MLLM%2520consistently%2520dominates%2520across%2520all%2520evaluation%250Atasks%252C%2520demonstrating%2520ongoing%2520challenges%2520and%2520opportunities%2520in%2520developing%2520MLLMs%250Awith%2520better%2520visual%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.10090v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CameraBench%3A%20Benchmarking%20Visual%20Reasoning%20in%20MLLMs%20via%20Photography&entry.906535625=I-Sheng%20Fang%20and%20Jun-Cheng%20Chen&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20and%20multimodal%20large%20language%20models%20%28MLLMs%29%0Ahave%20significantly%20advanced%20artificial%20intelligence.%20However%2C%20visual%20reasoning%2C%0Areasoning%20involving%20both%20visual%20and%20textual%20inputs%2C%20remains%20underexplored.%0ARecent%20advancements%2C%20including%20the%20reasoning%20models%20like%20OpenAI%20o1%20and%20Gemini%0A2.0%20Flash%20Thinking%2C%20which%20incorporate%20image%20inputs%2C%20have%20opened%20this%0Acapability.%20In%20this%20ongoing%20work%2C%20we%20focus%20specifically%20on%20photography-related%0Atasks%20because%20a%20photo%20is%20a%20visual%20snapshot%20of%20the%20physical%20world%20where%20the%0Aunderlying%20physics%20%28i.e.%2C%20illumination%2C%20blur%20extent%2C%20etc.%29%20interplay%20with%20the%0Acamera%20parameters.%20Successfully%20reasoning%20from%20the%20visual%20information%20of%20a%0Aphoto%20to%20identify%20these%20numerical%20camera%20settings%20requires%20the%20MLLMs%20to%20have%20a%0Adeeper%20understanding%20of%20the%20underlying%20physics%20for%20precise%20visual%0Acomprehension%2C%20representing%20a%20challenging%20and%20intelligent%20capability%20essential%0Afor%20practical%20applications%20like%20photography%20assistant%20agents.%20We%20aim%20to%0Aevaluate%20MLLMs%20on%20their%20ability%20to%20distinguish%20visual%20differences%20related%20to%0Anumerical%20camera%20settings%2C%20extending%20a%20methodology%20previously%20proposed%20for%0Avision-language%20models%20%28VLMs%29.%20Our%20preliminary%20results%20demonstrate%20the%0Aimportance%20of%20visual%20reasoning%20in%20photography-related%20tasks.%20Moreover%2C%20these%0Aresults%20show%20that%20no%20single%20MLLM%20consistently%20dominates%20across%20all%20evaluation%0Atasks%2C%20demonstrating%20ongoing%20challenges%20and%20opportunities%20in%20developing%20MLLMs%0Awith%20better%20visual%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.10090v2&entry.124074799=Read"},
{"title": "ViTOC: Vision Transformer and Object-aware Captioner", "author": "Feiyang Huang", "abstract": "  This paper presents ViTOC (Vision Transformer and Object-aware Captioner), a\nnovel vision-language model for image captioning that addresses the challenges\nof accuracy and diversity in generated descriptions. Unlike conventional\napproaches, ViTOC employs a dual-path architecture based on Vision Transformer\nand object detector, effectively fusing global visual features and local object\ninformation through learnable vectors. The model introduces an innovative\nobject-aware prompting strategy that significantly enhances its capability in\nhandling long-tail data. Experiments on the standard COCO dataset demonstrate\nthat ViTOC outperforms baseline models across all evaluation metrics.\nAdditionally, we propose a reference-free evaluation method based on CLIP to\nfurther validate the model's effectiveness. By utilizing pretrained visual\nmodel parameters, ViTOC achieves efficient end-to-end training.\n", "link": "http://arxiv.org/abs/2411.07265v4", "date": "2025-04-17", "relevancy": 2.8934, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5815}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5815}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViTOC%3A%20Vision%20Transformer%20and%20Object-aware%20Captioner&body=Title%3A%20ViTOC%3A%20Vision%20Transformer%20and%20Object-aware%20Captioner%0AAuthor%3A%20Feiyang%20Huang%0AAbstract%3A%20%20%20This%20paper%20presents%20ViTOC%20%28Vision%20Transformer%20and%20Object-aware%20Captioner%29%2C%20a%0Anovel%20vision-language%20model%20for%20image%20captioning%20that%20addresses%20the%20challenges%0Aof%20accuracy%20and%20diversity%20in%20generated%20descriptions.%20Unlike%20conventional%0Aapproaches%2C%20ViTOC%20employs%20a%20dual-path%20architecture%20based%20on%20Vision%20Transformer%0Aand%20object%20detector%2C%20effectively%20fusing%20global%20visual%20features%20and%20local%20object%0Ainformation%20through%20learnable%20vectors.%20The%20model%20introduces%20an%20innovative%0Aobject-aware%20prompting%20strategy%20that%20significantly%20enhances%20its%20capability%20in%0Ahandling%20long-tail%20data.%20Experiments%20on%20the%20standard%20COCO%20dataset%20demonstrate%0Athat%20ViTOC%20outperforms%20baseline%20models%20across%20all%20evaluation%20metrics.%0AAdditionally%2C%20we%20propose%20a%20reference-free%20evaluation%20method%20based%20on%20CLIP%20to%0Afurther%20validate%20the%20model%27s%20effectiveness.%20By%20utilizing%20pretrained%20visual%0Amodel%20parameters%2C%20ViTOC%20achieves%20efficient%20end-to-end%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07265v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViTOC%253A%2520Vision%2520Transformer%2520and%2520Object-aware%2520Captioner%26entry.906535625%3DFeiyang%2520Huang%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520ViTOC%2520%2528Vision%2520Transformer%2520and%2520Object-aware%2520Captioner%2529%252C%2520a%250Anovel%2520vision-language%2520model%2520for%2520image%2520captioning%2520that%2520addresses%2520the%2520challenges%250Aof%2520accuracy%2520and%2520diversity%2520in%2520generated%2520descriptions.%2520Unlike%2520conventional%250Aapproaches%252C%2520ViTOC%2520employs%2520a%2520dual-path%2520architecture%2520based%2520on%2520Vision%2520Transformer%250Aand%2520object%2520detector%252C%2520effectively%2520fusing%2520global%2520visual%2520features%2520and%2520local%2520object%250Ainformation%2520through%2520learnable%2520vectors.%2520The%2520model%2520introduces%2520an%2520innovative%250Aobject-aware%2520prompting%2520strategy%2520that%2520significantly%2520enhances%2520its%2520capability%2520in%250Ahandling%2520long-tail%2520data.%2520Experiments%2520on%2520the%2520standard%2520COCO%2520dataset%2520demonstrate%250Athat%2520ViTOC%2520outperforms%2520baseline%2520models%2520across%2520all%2520evaluation%2520metrics.%250AAdditionally%252C%2520we%2520propose%2520a%2520reference-free%2520evaluation%2520method%2520based%2520on%2520CLIP%2520to%250Afurther%2520validate%2520the%2520model%2527s%2520effectiveness.%2520By%2520utilizing%2520pretrained%2520visual%250Amodel%2520parameters%252C%2520ViTOC%2520achieves%2520efficient%2520end-to-end%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07265v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViTOC%3A%20Vision%20Transformer%20and%20Object-aware%20Captioner&entry.906535625=Feiyang%20Huang&entry.1292438233=%20%20This%20paper%20presents%20ViTOC%20%28Vision%20Transformer%20and%20Object-aware%20Captioner%29%2C%20a%0Anovel%20vision-language%20model%20for%20image%20captioning%20that%20addresses%20the%20challenges%0Aof%20accuracy%20and%20diversity%20in%20generated%20descriptions.%20Unlike%20conventional%0Aapproaches%2C%20ViTOC%20employs%20a%20dual-path%20architecture%20based%20on%20Vision%20Transformer%0Aand%20object%20detector%2C%20effectively%20fusing%20global%20visual%20features%20and%20local%20object%0Ainformation%20through%20learnable%20vectors.%20The%20model%20introduces%20an%20innovative%0Aobject-aware%20prompting%20strategy%20that%20significantly%20enhances%20its%20capability%20in%0Ahandling%20long-tail%20data.%20Experiments%20on%20the%20standard%20COCO%20dataset%20demonstrate%0Athat%20ViTOC%20outperforms%20baseline%20models%20across%20all%20evaluation%20metrics.%0AAdditionally%2C%20we%20propose%20a%20reference-free%20evaluation%20method%20based%20on%20CLIP%20to%0Afurther%20validate%20the%20model%27s%20effectiveness.%20By%20utilizing%20pretrained%20visual%0Amodel%20parameters%2C%20ViTOC%20achieves%20efficient%20end-to-end%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07265v4&entry.124074799=Read"},
{"title": "SEAL: Semantic Attention Learning for Long Video Representation", "author": "Lan Wang and Yujia Chen and Du Tran and Vishnu Naresh Boddeti and Wen-Sheng Chu", "abstract": "  Long video understanding presents challenges due to the inherent high\ncomputational complexity and redundant temporal information. An effective\nrepresentation for long videos must efficiently process such redundancy while\npreserving essential contents for downstream tasks. This paper introduces\nSEmantic Attention Learning (SEAL), a novel unified representation for long\nvideos. To reduce computational complexity, long videos are decomposed into\nthree distinct types of semantic entities: scenes, objects, and actions,\nallowing models to operate on a compact set of entities rather than a large\nnumber of frames or pixels. To further address redundancy, we propose an\nattention learning module that balances token relevance with diversity,\nformulated as a subset selection optimization problem. Our representation is\nversatile and applicable across various long video understanding tasks.\nExtensive experiments demonstrate that SEAL significantly outperforms\nstate-of-the-art methods in video question answering and temporal grounding\ntasks across diverse benchmarks, including LVBench, MovieChat-1K, and Ego4D.\n", "link": "http://arxiv.org/abs/2412.01798v3", "date": "2025-04-17", "relevancy": 2.8794, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5874}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5874}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEAL%3A%20Semantic%20Attention%20Learning%20for%20Long%20Video%20Representation&body=Title%3A%20SEAL%3A%20Semantic%20Attention%20Learning%20for%20Long%20Video%20Representation%0AAuthor%3A%20Lan%20Wang%20and%20Yujia%20Chen%20and%20Du%20Tran%20and%20Vishnu%20Naresh%20Boddeti%20and%20Wen-Sheng%20Chu%0AAbstract%3A%20%20%20Long%20video%20understanding%20presents%20challenges%20due%20to%20the%20inherent%20high%0Acomputational%20complexity%20and%20redundant%20temporal%20information.%20An%20effective%0Arepresentation%20for%20long%20videos%20must%20efficiently%20process%20such%20redundancy%20while%0Apreserving%20essential%20contents%20for%20downstream%20tasks.%20This%20paper%20introduces%0ASEmantic%20Attention%20Learning%20%28SEAL%29%2C%20a%20novel%20unified%20representation%20for%20long%0Avideos.%20To%20reduce%20computational%20complexity%2C%20long%20videos%20are%20decomposed%20into%0Athree%20distinct%20types%20of%20semantic%20entities%3A%20scenes%2C%20objects%2C%20and%20actions%2C%0Aallowing%20models%20to%20operate%20on%20a%20compact%20set%20of%20entities%20rather%20than%20a%20large%0Anumber%20of%20frames%20or%20pixels.%20To%20further%20address%20redundancy%2C%20we%20propose%20an%0Aattention%20learning%20module%20that%20balances%20token%20relevance%20with%20diversity%2C%0Aformulated%20as%20a%20subset%20selection%20optimization%20problem.%20Our%20representation%20is%0Aversatile%20and%20applicable%20across%20various%20long%20video%20understanding%20tasks.%0AExtensive%20experiments%20demonstrate%20that%20SEAL%20significantly%20outperforms%0Astate-of-the-art%20methods%20in%20video%20question%20answering%20and%20temporal%20grounding%0Atasks%20across%20diverse%20benchmarks%2C%20including%20LVBench%2C%20MovieChat-1K%2C%20and%20Ego4D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.01798v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEAL%253A%2520Semantic%2520Attention%2520Learning%2520for%2520Long%2520Video%2520Representation%26entry.906535625%3DLan%2520Wang%2520and%2520Yujia%2520Chen%2520and%2520Du%2520Tran%2520and%2520Vishnu%2520Naresh%2520Boddeti%2520and%2520Wen-Sheng%2520Chu%26entry.1292438233%3D%2520%2520Long%2520video%2520understanding%2520presents%2520challenges%2520due%2520to%2520the%2520inherent%2520high%250Acomputational%2520complexity%2520and%2520redundant%2520temporal%2520information.%2520An%2520effective%250Arepresentation%2520for%2520long%2520videos%2520must%2520efficiently%2520process%2520such%2520redundancy%2520while%250Apreserving%2520essential%2520contents%2520for%2520downstream%2520tasks.%2520This%2520paper%2520introduces%250ASEmantic%2520Attention%2520Learning%2520%2528SEAL%2529%252C%2520a%2520novel%2520unified%2520representation%2520for%2520long%250Avideos.%2520To%2520reduce%2520computational%2520complexity%252C%2520long%2520videos%2520are%2520decomposed%2520into%250Athree%2520distinct%2520types%2520of%2520semantic%2520entities%253A%2520scenes%252C%2520objects%252C%2520and%2520actions%252C%250Aallowing%2520models%2520to%2520operate%2520on%2520a%2520compact%2520set%2520of%2520entities%2520rather%2520than%2520a%2520large%250Anumber%2520of%2520frames%2520or%2520pixels.%2520To%2520further%2520address%2520redundancy%252C%2520we%2520propose%2520an%250Aattention%2520learning%2520module%2520that%2520balances%2520token%2520relevance%2520with%2520diversity%252C%250Aformulated%2520as%2520a%2520subset%2520selection%2520optimization%2520problem.%2520Our%2520representation%2520is%250Aversatile%2520and%2520applicable%2520across%2520various%2520long%2520video%2520understanding%2520tasks.%250AExtensive%2520experiments%2520demonstrate%2520that%2520SEAL%2520significantly%2520outperforms%250Astate-of-the-art%2520methods%2520in%2520video%2520question%2520answering%2520and%2520temporal%2520grounding%250Atasks%2520across%2520diverse%2520benchmarks%252C%2520including%2520LVBench%252C%2520MovieChat-1K%252C%2520and%2520Ego4D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01798v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEAL%3A%20Semantic%20Attention%20Learning%20for%20Long%20Video%20Representation&entry.906535625=Lan%20Wang%20and%20Yujia%20Chen%20and%20Du%20Tran%20and%20Vishnu%20Naresh%20Boddeti%20and%20Wen-Sheng%20Chu&entry.1292438233=%20%20Long%20video%20understanding%20presents%20challenges%20due%20to%20the%20inherent%20high%0Acomputational%20complexity%20and%20redundant%20temporal%20information.%20An%20effective%0Arepresentation%20for%20long%20videos%20must%20efficiently%20process%20such%20redundancy%20while%0Apreserving%20essential%20contents%20for%20downstream%20tasks.%20This%20paper%20introduces%0ASEmantic%20Attention%20Learning%20%28SEAL%29%2C%20a%20novel%20unified%20representation%20for%20long%0Avideos.%20To%20reduce%20computational%20complexity%2C%20long%20videos%20are%20decomposed%20into%0Athree%20distinct%20types%20of%20semantic%20entities%3A%20scenes%2C%20objects%2C%20and%20actions%2C%0Aallowing%20models%20to%20operate%20on%20a%20compact%20set%20of%20entities%20rather%20than%20a%20large%0Anumber%20of%20frames%20or%20pixels.%20To%20further%20address%20redundancy%2C%20we%20propose%20an%0Aattention%20learning%20module%20that%20balances%20token%20relevance%20with%20diversity%2C%0Aformulated%20as%20a%20subset%20selection%20optimization%20problem.%20Our%20representation%20is%0Aversatile%20and%20applicable%20across%20various%20long%20video%20understanding%20tasks.%0AExtensive%20experiments%20demonstrate%20that%20SEAL%20significantly%20outperforms%0Astate-of-the-art%20methods%20in%20video%20question%20answering%20and%20temporal%20grounding%0Atasks%20across%20diverse%20benchmarks%2C%20including%20LVBench%2C%20MovieChat-1K%2C%20and%20Ego4D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.01798v3&entry.124074799=Read"},
{"title": "AdaVid: Adaptive Video-Language Pretraining", "author": "Chaitanya Patel and Juan Carlos Niebles and Ehsan Adeli", "abstract": "  Contrastive video-language pretraining has demonstrated great success in\nlearning rich and robust video representations. However, deploying such video\nencoders on compute-constrained edge devices remains challenging due to their\nhigh computational demands. Additionally, existing models are typically trained\nto process only short video clips, often limited to 4 to 64 frames. In this\npaper, we introduce AdaVid, a flexible architectural framework designed to\nlearn efficient video encoders that can dynamically adapt their computational\nfootprint based on available resources. At the heart of AdaVid is an adaptive\ntransformer block, inspired by Matryoshka Representation Learning, which allows\nthe model to adjust its hidden embedding dimension at inference time. We show\nthat AdaVid-EgoVLP, trained on video-narration pairs from the large-scale Ego4D\ndataset, matches the performance of the standard EgoVLP on short video-language\nbenchmarks using only half the compute, and even outperforms EgoVLP when given\nequal computational resources. We further explore the trade-off between frame\ncount and compute on the challenging Diving48 classification benchmark, showing\nthat AdaVid enables the use of more frames without exceeding computational\nlimits. To handle longer videos, we also propose a lightweight hierarchical\nnetwork that aggregates short clip features, achieving a strong balance between\ncompute efficiency and accuracy across several long video benchmarks.\n", "link": "http://arxiv.org/abs/2504.12513v1", "date": "2025-04-16", "relevancy": 2.8756, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.581}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.581}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaVid%3A%20Adaptive%20Video-Language%20Pretraining&body=Title%3A%20AdaVid%3A%20Adaptive%20Video-Language%20Pretraining%0AAuthor%3A%20Chaitanya%20Patel%20and%20Juan%20Carlos%20Niebles%20and%20Ehsan%20Adeli%0AAbstract%3A%20%20%20Contrastive%20video-language%20pretraining%20has%20demonstrated%20great%20success%20in%0Alearning%20rich%20and%20robust%20video%20representations.%20However%2C%20deploying%20such%20video%0Aencoders%20on%20compute-constrained%20edge%20devices%20remains%20challenging%20due%20to%20their%0Ahigh%20computational%20demands.%20Additionally%2C%20existing%20models%20are%20typically%20trained%0Ato%20process%20only%20short%20video%20clips%2C%20often%20limited%20to%204%20to%2064%20frames.%20In%20this%0Apaper%2C%20we%20introduce%20AdaVid%2C%20a%20flexible%20architectural%20framework%20designed%20to%0Alearn%20efficient%20video%20encoders%20that%20can%20dynamically%20adapt%20their%20computational%0Afootprint%20based%20on%20available%20resources.%20At%20the%20heart%20of%20AdaVid%20is%20an%20adaptive%0Atransformer%20block%2C%20inspired%20by%20Matryoshka%20Representation%20Learning%2C%20which%20allows%0Athe%20model%20to%20adjust%20its%20hidden%20embedding%20dimension%20at%20inference%20time.%20We%20show%0Athat%20AdaVid-EgoVLP%2C%20trained%20on%20video-narration%20pairs%20from%20the%20large-scale%20Ego4D%0Adataset%2C%20matches%20the%20performance%20of%20the%20standard%20EgoVLP%20on%20short%20video-language%0Abenchmarks%20using%20only%20half%20the%20compute%2C%20and%20even%20outperforms%20EgoVLP%20when%20given%0Aequal%20computational%20resources.%20We%20further%20explore%20the%20trade-off%20between%20frame%0Acount%20and%20compute%20on%20the%20challenging%20Diving48%20classification%20benchmark%2C%20showing%0Athat%20AdaVid%20enables%20the%20use%20of%20more%20frames%20without%20exceeding%20computational%0Alimits.%20To%20handle%20longer%20videos%2C%20we%20also%20propose%20a%20lightweight%20hierarchical%0Anetwork%20that%20aggregates%20short%20clip%20features%2C%20achieving%20a%20strong%20balance%20between%0Acompute%20efficiency%20and%20accuracy%20across%20several%20long%20video%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12513v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaVid%253A%2520Adaptive%2520Video-Language%2520Pretraining%26entry.906535625%3DChaitanya%2520Patel%2520and%2520Juan%2520Carlos%2520Niebles%2520and%2520Ehsan%2520Adeli%26entry.1292438233%3D%2520%2520Contrastive%2520video-language%2520pretraining%2520has%2520demonstrated%2520great%2520success%2520in%250Alearning%2520rich%2520and%2520robust%2520video%2520representations.%2520However%252C%2520deploying%2520such%2520video%250Aencoders%2520on%2520compute-constrained%2520edge%2520devices%2520remains%2520challenging%2520due%2520to%2520their%250Ahigh%2520computational%2520demands.%2520Additionally%252C%2520existing%2520models%2520are%2520typically%2520trained%250Ato%2520process%2520only%2520short%2520video%2520clips%252C%2520often%2520limited%2520to%25204%2520to%252064%2520frames.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520AdaVid%252C%2520a%2520flexible%2520architectural%2520framework%2520designed%2520to%250Alearn%2520efficient%2520video%2520encoders%2520that%2520can%2520dynamically%2520adapt%2520their%2520computational%250Afootprint%2520based%2520on%2520available%2520resources.%2520At%2520the%2520heart%2520of%2520AdaVid%2520is%2520an%2520adaptive%250Atransformer%2520block%252C%2520inspired%2520by%2520Matryoshka%2520Representation%2520Learning%252C%2520which%2520allows%250Athe%2520model%2520to%2520adjust%2520its%2520hidden%2520embedding%2520dimension%2520at%2520inference%2520time.%2520We%2520show%250Athat%2520AdaVid-EgoVLP%252C%2520trained%2520on%2520video-narration%2520pairs%2520from%2520the%2520large-scale%2520Ego4D%250Adataset%252C%2520matches%2520the%2520performance%2520of%2520the%2520standard%2520EgoVLP%2520on%2520short%2520video-language%250Abenchmarks%2520using%2520only%2520half%2520the%2520compute%252C%2520and%2520even%2520outperforms%2520EgoVLP%2520when%2520given%250Aequal%2520computational%2520resources.%2520We%2520further%2520explore%2520the%2520trade-off%2520between%2520frame%250Acount%2520and%2520compute%2520on%2520the%2520challenging%2520Diving48%2520classification%2520benchmark%252C%2520showing%250Athat%2520AdaVid%2520enables%2520the%2520use%2520of%2520more%2520frames%2520without%2520exceeding%2520computational%250Alimits.%2520To%2520handle%2520longer%2520videos%252C%2520we%2520also%2520propose%2520a%2520lightweight%2520hierarchical%250Anetwork%2520that%2520aggregates%2520short%2520clip%2520features%252C%2520achieving%2520a%2520strong%2520balance%2520between%250Acompute%2520efficiency%2520and%2520accuracy%2520across%2520several%2520long%2520video%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12513v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaVid%3A%20Adaptive%20Video-Language%20Pretraining&entry.906535625=Chaitanya%20Patel%20and%20Juan%20Carlos%20Niebles%20and%20Ehsan%20Adeli&entry.1292438233=%20%20Contrastive%20video-language%20pretraining%20has%20demonstrated%20great%20success%20in%0Alearning%20rich%20and%20robust%20video%20representations.%20However%2C%20deploying%20such%20video%0Aencoders%20on%20compute-constrained%20edge%20devices%20remains%20challenging%20due%20to%20their%0Ahigh%20computational%20demands.%20Additionally%2C%20existing%20models%20are%20typically%20trained%0Ato%20process%20only%20short%20video%20clips%2C%20often%20limited%20to%204%20to%2064%20frames.%20In%20this%0Apaper%2C%20we%20introduce%20AdaVid%2C%20a%20flexible%20architectural%20framework%20designed%20to%0Alearn%20efficient%20video%20encoders%20that%20can%20dynamically%20adapt%20their%20computational%0Afootprint%20based%20on%20available%20resources.%20At%20the%20heart%20of%20AdaVid%20is%20an%20adaptive%0Atransformer%20block%2C%20inspired%20by%20Matryoshka%20Representation%20Learning%2C%20which%20allows%0Athe%20model%20to%20adjust%20its%20hidden%20embedding%20dimension%20at%20inference%20time.%20We%20show%0Athat%20AdaVid-EgoVLP%2C%20trained%20on%20video-narration%20pairs%20from%20the%20large-scale%20Ego4D%0Adataset%2C%20matches%20the%20performance%20of%20the%20standard%20EgoVLP%20on%20short%20video-language%0Abenchmarks%20using%20only%20half%20the%20compute%2C%20and%20even%20outperforms%20EgoVLP%20when%20given%0Aequal%20computational%20resources.%20We%20further%20explore%20the%20trade-off%20between%20frame%0Acount%20and%20compute%20on%20the%20challenging%20Diving48%20classification%20benchmark%2C%20showing%0Athat%20AdaVid%20enables%20the%20use%20of%20more%20frames%20without%20exceeding%20computational%0Alimits.%20To%20handle%20longer%20videos%2C%20we%20also%20propose%20a%20lightweight%20hierarchical%0Anetwork%20that%20aggregates%20short%20clip%20features%2C%20achieving%20a%20strong%20balance%20between%0Acompute%20efficiency%20and%20accuracy%20across%20several%20long%20video%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12513v1&entry.124074799=Read"},
{"title": "IMAGGarment-1: Fine-Grained Garment Generation for Controllable Fashion\n  Design", "author": "Fei Shen and Jian Yu and Cong Wang and Xin Jiang and Xiaoyu Du and Jinhui Tang", "abstract": "  This paper presents IMAGGarment-1, a fine-grained garment generation (FGG)\nframework that enables high-fidelity garment synthesis with precise control\nover silhouette, color, and logo placement. Unlike existing methods that are\nlimited to single-condition inputs, IMAGGarment-1 addresses the challenges of\nmulti-conditional controllability in personalized fashion design and digital\napparel applications. Specifically, IMAGGarment-1 employs a two-stage training\nstrategy to separately model global appearance and local details, while\nenabling unified and controllable generation through end-to-end inference. In\nthe first stage, we propose a global appearance model that jointly encodes\nsilhouette and color using a mixed attention module and a color adapter. In the\nsecond stage, we present a local enhancement model with an adaptive\nappearance-aware module to inject user-defined logos and spatial constraints,\nenabling accurate placement and visual consistency. To support this task, we\nrelease GarmentBench, a large-scale dataset comprising over 180K garment\nsamples paired with multi-level design conditions, including sketches, color\nreferences, logo placements, and textual prompts. Extensive experiments\ndemonstrate that our method outperforms existing baselines, achieving superior\nstructural stability, color fidelity, and local controllability performance.\nThe code and model are available at https://github.com/muzishen/IMAGGarment-1.\n", "link": "http://arxiv.org/abs/2504.13176v1", "date": "2025-04-17", "relevancy": 2.859, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.7498}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6989}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IMAGGarment-1%3A%20Fine-Grained%20Garment%20Generation%20for%20Controllable%20Fashion%0A%20%20Design&body=Title%3A%20IMAGGarment-1%3A%20Fine-Grained%20Garment%20Generation%20for%20Controllable%20Fashion%0A%20%20Design%0AAuthor%3A%20Fei%20Shen%20and%20Jian%20Yu%20and%20Cong%20Wang%20and%20Xin%20Jiang%20and%20Xiaoyu%20Du%20and%20Jinhui%20Tang%0AAbstract%3A%20%20%20This%20paper%20presents%20IMAGGarment-1%2C%20a%20fine-grained%20garment%20generation%20%28FGG%29%0Aframework%20that%20enables%20high-fidelity%20garment%20synthesis%20with%20precise%20control%0Aover%20silhouette%2C%20color%2C%20and%20logo%20placement.%20Unlike%20existing%20methods%20that%20are%0Alimited%20to%20single-condition%20inputs%2C%20IMAGGarment-1%20addresses%20the%20challenges%20of%0Amulti-conditional%20controllability%20in%20personalized%20fashion%20design%20and%20digital%0Aapparel%20applications.%20Specifically%2C%20IMAGGarment-1%20employs%20a%20two-stage%20training%0Astrategy%20to%20separately%20model%20global%20appearance%20and%20local%20details%2C%20while%0Aenabling%20unified%20and%20controllable%20generation%20through%20end-to-end%20inference.%20In%0Athe%20first%20stage%2C%20we%20propose%20a%20global%20appearance%20model%20that%20jointly%20encodes%0Asilhouette%20and%20color%20using%20a%20mixed%20attention%20module%20and%20a%20color%20adapter.%20In%20the%0Asecond%20stage%2C%20we%20present%20a%20local%20enhancement%20model%20with%20an%20adaptive%0Aappearance-aware%20module%20to%20inject%20user-defined%20logos%20and%20spatial%20constraints%2C%0Aenabling%20accurate%20placement%20and%20visual%20consistency.%20To%20support%20this%20task%2C%20we%0Arelease%20GarmentBench%2C%20a%20large-scale%20dataset%20comprising%20over%20180K%20garment%0Asamples%20paired%20with%20multi-level%20design%20conditions%2C%20including%20sketches%2C%20color%0Areferences%2C%20logo%20placements%2C%20and%20textual%20prompts.%20Extensive%20experiments%0Ademonstrate%20that%20our%20method%20outperforms%20existing%20baselines%2C%20achieving%20superior%0Astructural%20stability%2C%20color%20fidelity%2C%20and%20local%20controllability%20performance.%0AThe%20code%20and%20model%20are%20available%20at%20https%3A//github.com/muzishen/IMAGGarment-1.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13176v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIMAGGarment-1%253A%2520Fine-Grained%2520Garment%2520Generation%2520for%2520Controllable%2520Fashion%250A%2520%2520Design%26entry.906535625%3DFei%2520Shen%2520and%2520Jian%2520Yu%2520and%2520Cong%2520Wang%2520and%2520Xin%2520Jiang%2520and%2520Xiaoyu%2520Du%2520and%2520Jinhui%2520Tang%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520IMAGGarment-1%252C%2520a%2520fine-grained%2520garment%2520generation%2520%2528FGG%2529%250Aframework%2520that%2520enables%2520high-fidelity%2520garment%2520synthesis%2520with%2520precise%2520control%250Aover%2520silhouette%252C%2520color%252C%2520and%2520logo%2520placement.%2520Unlike%2520existing%2520methods%2520that%2520are%250Alimited%2520to%2520single-condition%2520inputs%252C%2520IMAGGarment-1%2520addresses%2520the%2520challenges%2520of%250Amulti-conditional%2520controllability%2520in%2520personalized%2520fashion%2520design%2520and%2520digital%250Aapparel%2520applications.%2520Specifically%252C%2520IMAGGarment-1%2520employs%2520a%2520two-stage%2520training%250Astrategy%2520to%2520separately%2520model%2520global%2520appearance%2520and%2520local%2520details%252C%2520while%250Aenabling%2520unified%2520and%2520controllable%2520generation%2520through%2520end-to-end%2520inference.%2520In%250Athe%2520first%2520stage%252C%2520we%2520propose%2520a%2520global%2520appearance%2520model%2520that%2520jointly%2520encodes%250Asilhouette%2520and%2520color%2520using%2520a%2520mixed%2520attention%2520module%2520and%2520a%2520color%2520adapter.%2520In%2520the%250Asecond%2520stage%252C%2520we%2520present%2520a%2520local%2520enhancement%2520model%2520with%2520an%2520adaptive%250Aappearance-aware%2520module%2520to%2520inject%2520user-defined%2520logos%2520and%2520spatial%2520constraints%252C%250Aenabling%2520accurate%2520placement%2520and%2520visual%2520consistency.%2520To%2520support%2520this%2520task%252C%2520we%250Arelease%2520GarmentBench%252C%2520a%2520large-scale%2520dataset%2520comprising%2520over%2520180K%2520garment%250Asamples%2520paired%2520with%2520multi-level%2520design%2520conditions%252C%2520including%2520sketches%252C%2520color%250Areferences%252C%2520logo%2520placements%252C%2520and%2520textual%2520prompts.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520our%2520method%2520outperforms%2520existing%2520baselines%252C%2520achieving%2520superior%250Astructural%2520stability%252C%2520color%2520fidelity%252C%2520and%2520local%2520controllability%2520performance.%250AThe%2520code%2520and%2520model%2520are%2520available%2520at%2520https%253A//github.com/muzishen/IMAGGarment-1.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13176v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IMAGGarment-1%3A%20Fine-Grained%20Garment%20Generation%20for%20Controllable%20Fashion%0A%20%20Design&entry.906535625=Fei%20Shen%20and%20Jian%20Yu%20and%20Cong%20Wang%20and%20Xin%20Jiang%20and%20Xiaoyu%20Du%20and%20Jinhui%20Tang&entry.1292438233=%20%20This%20paper%20presents%20IMAGGarment-1%2C%20a%20fine-grained%20garment%20generation%20%28FGG%29%0Aframework%20that%20enables%20high-fidelity%20garment%20synthesis%20with%20precise%20control%0Aover%20silhouette%2C%20color%2C%20and%20logo%20placement.%20Unlike%20existing%20methods%20that%20are%0Alimited%20to%20single-condition%20inputs%2C%20IMAGGarment-1%20addresses%20the%20challenges%20of%0Amulti-conditional%20controllability%20in%20personalized%20fashion%20design%20and%20digital%0Aapparel%20applications.%20Specifically%2C%20IMAGGarment-1%20employs%20a%20two-stage%20training%0Astrategy%20to%20separately%20model%20global%20appearance%20and%20local%20details%2C%20while%0Aenabling%20unified%20and%20controllable%20generation%20through%20end-to-end%20inference.%20In%0Athe%20first%20stage%2C%20we%20propose%20a%20global%20appearance%20model%20that%20jointly%20encodes%0Asilhouette%20and%20color%20using%20a%20mixed%20attention%20module%20and%20a%20color%20adapter.%20In%20the%0Asecond%20stage%2C%20we%20present%20a%20local%20enhancement%20model%20with%20an%20adaptive%0Aappearance-aware%20module%20to%20inject%20user-defined%20logos%20and%20spatial%20constraints%2C%0Aenabling%20accurate%20placement%20and%20visual%20consistency.%20To%20support%20this%20task%2C%20we%0Arelease%20GarmentBench%2C%20a%20large-scale%20dataset%20comprising%20over%20180K%20garment%0Asamples%20paired%20with%20multi-level%20design%20conditions%2C%20including%20sketches%2C%20color%0Areferences%2C%20logo%20placements%2C%20and%20textual%20prompts.%20Extensive%20experiments%0Ademonstrate%20that%20our%20method%20outperforms%20existing%20baselines%2C%20achieving%20superior%0Astructural%20stability%2C%20color%20fidelity%2C%20and%20local%20controllability%20performance.%0AThe%20code%20and%20model%20are%20available%20at%20https%3A//github.com/muzishen/IMAGGarment-1.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13176v1&entry.124074799=Read"},
{"title": "Vision and Language Integration for Domain Generalization", "author": "Yanmei Wang and Xiyao Liu and Fupeng Chu and Zhi Han", "abstract": "  Domain generalization aims at training on source domains to uncover a\ndomain-invariant feature space, allowing the model to perform robust\ngeneralization ability on unknown target domains. However, due to domain gaps,\nit is hard to find reliable common image feature space, and the reason for that\nis the lack of suitable basic units for images. Different from image in vision\nspace, language has comprehensive expression elements that can effectively\nconvey semantics. Inspired by the semantic completeness of language and\nintuitiveness of image, we propose VLCA, which combine language space and\nvision space, and connect the multiple image domains by using semantic space as\nthe bridge domain. Specifically, in language space, by taking advantage of the\ncompleteness of language basic units, we tend to capture the semantic\nrepresentation of the relations between categories through word vector\ndistance. Then, in vision space, by taking advantage of the intuitiveness of\nimage features, the common pattern of sample features with the same class is\nexplored through low-rank approximation. In the end, the language\nrepresentation is aligned with the vision representation through the multimodal\nspace of text and image. Experiments demonstrate the effectiveness of the\nproposed method.\n", "link": "http://arxiv.org/abs/2504.12966v1", "date": "2025-04-17", "relevancy": 2.8437, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5782}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5782}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20and%20Language%20Integration%20for%20Domain%20Generalization&body=Title%3A%20Vision%20and%20Language%20Integration%20for%20Domain%20Generalization%0AAuthor%3A%20Yanmei%20Wang%20and%20Xiyao%20Liu%20and%20Fupeng%20Chu%20and%20Zhi%20Han%0AAbstract%3A%20%20%20Domain%20generalization%20aims%20at%20training%20on%20source%20domains%20to%20uncover%20a%0Adomain-invariant%20feature%20space%2C%20allowing%20the%20model%20to%20perform%20robust%0Ageneralization%20ability%20on%20unknown%20target%20domains.%20However%2C%20due%20to%20domain%20gaps%2C%0Ait%20is%20hard%20to%20find%20reliable%20common%20image%20feature%20space%2C%20and%20the%20reason%20for%20that%0Ais%20the%20lack%20of%20suitable%20basic%20units%20for%20images.%20Different%20from%20image%20in%20vision%0Aspace%2C%20language%20has%20comprehensive%20expression%20elements%20that%20can%20effectively%0Aconvey%20semantics.%20Inspired%20by%20the%20semantic%20completeness%20of%20language%20and%0Aintuitiveness%20of%20image%2C%20we%20propose%20VLCA%2C%20which%20combine%20language%20space%20and%0Avision%20space%2C%20and%20connect%20the%20multiple%20image%20domains%20by%20using%20semantic%20space%20as%0Athe%20bridge%20domain.%20Specifically%2C%20in%20language%20space%2C%20by%20taking%20advantage%20of%20the%0Acompleteness%20of%20language%20basic%20units%2C%20we%20tend%20to%20capture%20the%20semantic%0Arepresentation%20of%20the%20relations%20between%20categories%20through%20word%20vector%0Adistance.%20Then%2C%20in%20vision%20space%2C%20by%20taking%20advantage%20of%20the%20intuitiveness%20of%0Aimage%20features%2C%20the%20common%20pattern%20of%20sample%20features%20with%20the%20same%20class%20is%0Aexplored%20through%20low-rank%20approximation.%20In%20the%20end%2C%20the%20language%0Arepresentation%20is%20aligned%20with%20the%20vision%20representation%20through%20the%20multimodal%0Aspace%20of%20text%20and%20image.%20Experiments%20demonstrate%20the%20effectiveness%20of%20the%0Aproposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12966v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520and%2520Language%2520Integration%2520for%2520Domain%2520Generalization%26entry.906535625%3DYanmei%2520Wang%2520and%2520Xiyao%2520Liu%2520and%2520Fupeng%2520Chu%2520and%2520Zhi%2520Han%26entry.1292438233%3D%2520%2520Domain%2520generalization%2520aims%2520at%2520training%2520on%2520source%2520domains%2520to%2520uncover%2520a%250Adomain-invariant%2520feature%2520space%252C%2520allowing%2520the%2520model%2520to%2520perform%2520robust%250Ageneralization%2520ability%2520on%2520unknown%2520target%2520domains.%2520However%252C%2520due%2520to%2520domain%2520gaps%252C%250Ait%2520is%2520hard%2520to%2520find%2520reliable%2520common%2520image%2520feature%2520space%252C%2520and%2520the%2520reason%2520for%2520that%250Ais%2520the%2520lack%2520of%2520suitable%2520basic%2520units%2520for%2520images.%2520Different%2520from%2520image%2520in%2520vision%250Aspace%252C%2520language%2520has%2520comprehensive%2520expression%2520elements%2520that%2520can%2520effectively%250Aconvey%2520semantics.%2520Inspired%2520by%2520the%2520semantic%2520completeness%2520of%2520language%2520and%250Aintuitiveness%2520of%2520image%252C%2520we%2520propose%2520VLCA%252C%2520which%2520combine%2520language%2520space%2520and%250Avision%2520space%252C%2520and%2520connect%2520the%2520multiple%2520image%2520domains%2520by%2520using%2520semantic%2520space%2520as%250Athe%2520bridge%2520domain.%2520Specifically%252C%2520in%2520language%2520space%252C%2520by%2520taking%2520advantage%2520of%2520the%250Acompleteness%2520of%2520language%2520basic%2520units%252C%2520we%2520tend%2520to%2520capture%2520the%2520semantic%250Arepresentation%2520of%2520the%2520relations%2520between%2520categories%2520through%2520word%2520vector%250Adistance.%2520Then%252C%2520in%2520vision%2520space%252C%2520by%2520taking%2520advantage%2520of%2520the%2520intuitiveness%2520of%250Aimage%2520features%252C%2520the%2520common%2520pattern%2520of%2520sample%2520features%2520with%2520the%2520same%2520class%2520is%250Aexplored%2520through%2520low-rank%2520approximation.%2520In%2520the%2520end%252C%2520the%2520language%250Arepresentation%2520is%2520aligned%2520with%2520the%2520vision%2520representation%2520through%2520the%2520multimodal%250Aspace%2520of%2520text%2520and%2520image.%2520Experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12966v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20and%20Language%20Integration%20for%20Domain%20Generalization&entry.906535625=Yanmei%20Wang%20and%20Xiyao%20Liu%20and%20Fupeng%20Chu%20and%20Zhi%20Han&entry.1292438233=%20%20Domain%20generalization%20aims%20at%20training%20on%20source%20domains%20to%20uncover%20a%0Adomain-invariant%20feature%20space%2C%20allowing%20the%20model%20to%20perform%20robust%0Ageneralization%20ability%20on%20unknown%20target%20domains.%20However%2C%20due%20to%20domain%20gaps%2C%0Ait%20is%20hard%20to%20find%20reliable%20common%20image%20feature%20space%2C%20and%20the%20reason%20for%20that%0Ais%20the%20lack%20of%20suitable%20basic%20units%20for%20images.%20Different%20from%20image%20in%20vision%0Aspace%2C%20language%20has%20comprehensive%20expression%20elements%20that%20can%20effectively%0Aconvey%20semantics.%20Inspired%20by%20the%20semantic%20completeness%20of%20language%20and%0Aintuitiveness%20of%20image%2C%20we%20propose%20VLCA%2C%20which%20combine%20language%20space%20and%0Avision%20space%2C%20and%20connect%20the%20multiple%20image%20domains%20by%20using%20semantic%20space%20as%0Athe%20bridge%20domain.%20Specifically%2C%20in%20language%20space%2C%20by%20taking%20advantage%20of%20the%0Acompleteness%20of%20language%20basic%20units%2C%20we%20tend%20to%20capture%20the%20semantic%0Arepresentation%20of%20the%20relations%20between%20categories%20through%20word%20vector%0Adistance.%20Then%2C%20in%20vision%20space%2C%20by%20taking%20advantage%20of%20the%20intuitiveness%20of%0Aimage%20features%2C%20the%20common%20pattern%20of%20sample%20features%20with%20the%20same%20class%20is%0Aexplored%20through%20low-rank%20approximation.%20In%20the%20end%2C%20the%20language%0Arepresentation%20is%20aligned%20with%20the%20vision%20representation%20through%20the%20multimodal%0Aspace%20of%20text%20and%20image.%20Experiments%20demonstrate%20the%20effectiveness%20of%20the%0Aproposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12966v1&entry.124074799=Read"},
{"title": "Towards Cardiac MRI Foundation Models: Comprehensive Visual-Tabular\n  Representations for Whole-Heart Assessment and Beyond", "author": "Yundi Zhang and Paul Hager and Che Liu and Suprosanna Shit and Chen Chen and Daniel Rueckert and Jiazhen Pan", "abstract": "  Cardiac magnetic resonance imaging is the gold standard for non-invasive\ncardiac assessment, offering rich spatio-temporal views of the cardiac anatomy\nand physiology. Patient-level health factors, such as demographics, metabolic,\nand lifestyle, are known to substantially influence cardiovascular health and\ndisease risk, yet remain uncaptured by CMR alone. To holistically understand\ncardiac health and to enable the best possible interpretation of an\nindividual's disease risk, CMR and patient-level factors must be jointly\nexploited within an integrated framework. Recent multi-modal approaches have\nbegun to bridge this gap, yet they often rely on limited spatio-temporal data\nand focus on isolated clinical tasks, thereby hindering the development of a\ncomprehensive representation for cardiac health evaluation. To overcome these\nlimitations, we introduce ViTa, a step toward foundation models that delivers a\ncomprehensive representation of the heart and a precise interpretation of\nindividual disease risk. Leveraging data from 42,000 UK Biobank participants,\nViTa integrates 3D+T cine stacks from short-axis and long-axis views, enabling\na complete capture of the cardiac cycle. These imaging data are then fused with\ndetailed tabular patient-level factors, enabling context-aware insights. This\nmulti-modal paradigm supports a wide spectrum of downstream tasks, including\ncardiac phenotype and physiological feature prediction, segmentation, and\nclassification of cardiac and metabolic diseases within a single unified\nframework. By learning a shared latent representation that bridges rich imaging\nfeatures and patient context, ViTa moves beyond traditional, task-specific\nmodels toward a universal, patient-specific understanding of cardiac health,\nhighlighting its potential to advance clinical utility and scalability in\ncardiac analysis.\n", "link": "http://arxiv.org/abs/2504.13037v1", "date": "2025-04-17", "relevancy": 2.8221, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5701}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5701}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Cardiac%20MRI%20Foundation%20Models%3A%20Comprehensive%20Visual-Tabular%0A%20%20Representations%20for%20Whole-Heart%20Assessment%20and%20Beyond&body=Title%3A%20Towards%20Cardiac%20MRI%20Foundation%20Models%3A%20Comprehensive%20Visual-Tabular%0A%20%20Representations%20for%20Whole-Heart%20Assessment%20and%20Beyond%0AAuthor%3A%20Yundi%20Zhang%20and%20Paul%20Hager%20and%20Che%20Liu%20and%20Suprosanna%20Shit%20and%20Chen%20Chen%20and%20Daniel%20Rueckert%20and%20Jiazhen%20Pan%0AAbstract%3A%20%20%20Cardiac%20magnetic%20resonance%20imaging%20is%20the%20gold%20standard%20for%20non-invasive%0Acardiac%20assessment%2C%20offering%20rich%20spatio-temporal%20views%20of%20the%20cardiac%20anatomy%0Aand%20physiology.%20Patient-level%20health%20factors%2C%20such%20as%20demographics%2C%20metabolic%2C%0Aand%20lifestyle%2C%20are%20known%20to%20substantially%20influence%20cardiovascular%20health%20and%0Adisease%20risk%2C%20yet%20remain%20uncaptured%20by%20CMR%20alone.%20To%20holistically%20understand%0Acardiac%20health%20and%20to%20enable%20the%20best%20possible%20interpretation%20of%20an%0Aindividual%27s%20disease%20risk%2C%20CMR%20and%20patient-level%20factors%20must%20be%20jointly%0Aexploited%20within%20an%20integrated%20framework.%20Recent%20multi-modal%20approaches%20have%0Abegun%20to%20bridge%20this%20gap%2C%20yet%20they%20often%20rely%20on%20limited%20spatio-temporal%20data%0Aand%20focus%20on%20isolated%20clinical%20tasks%2C%20thereby%20hindering%20the%20development%20of%20a%0Acomprehensive%20representation%20for%20cardiac%20health%20evaluation.%20To%20overcome%20these%0Alimitations%2C%20we%20introduce%20ViTa%2C%20a%20step%20toward%20foundation%20models%20that%20delivers%20a%0Acomprehensive%20representation%20of%20the%20heart%20and%20a%20precise%20interpretation%20of%0Aindividual%20disease%20risk.%20Leveraging%20data%20from%2042%2C000%20UK%20Biobank%20participants%2C%0AViTa%20integrates%203D%2BT%20cine%20stacks%20from%20short-axis%20and%20long-axis%20views%2C%20enabling%0Aa%20complete%20capture%20of%20the%20cardiac%20cycle.%20These%20imaging%20data%20are%20then%20fused%20with%0Adetailed%20tabular%20patient-level%20factors%2C%20enabling%20context-aware%20insights.%20This%0Amulti-modal%20paradigm%20supports%20a%20wide%20spectrum%20of%20downstream%20tasks%2C%20including%0Acardiac%20phenotype%20and%20physiological%20feature%20prediction%2C%20segmentation%2C%20and%0Aclassification%20of%20cardiac%20and%20metabolic%20diseases%20within%20a%20single%20unified%0Aframework.%20By%20learning%20a%20shared%20latent%20representation%20that%20bridges%20rich%20imaging%0Afeatures%20and%20patient%20context%2C%20ViTa%20moves%20beyond%20traditional%2C%20task-specific%0Amodels%20toward%20a%20universal%2C%20patient-specific%20understanding%20of%20cardiac%20health%2C%0Ahighlighting%20its%20potential%20to%20advance%20clinical%20utility%20and%20scalability%20in%0Acardiac%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13037v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Cardiac%2520MRI%2520Foundation%2520Models%253A%2520Comprehensive%2520Visual-Tabular%250A%2520%2520Representations%2520for%2520Whole-Heart%2520Assessment%2520and%2520Beyond%26entry.906535625%3DYundi%2520Zhang%2520and%2520Paul%2520Hager%2520and%2520Che%2520Liu%2520and%2520Suprosanna%2520Shit%2520and%2520Chen%2520Chen%2520and%2520Daniel%2520Rueckert%2520and%2520Jiazhen%2520Pan%26entry.1292438233%3D%2520%2520Cardiac%2520magnetic%2520resonance%2520imaging%2520is%2520the%2520gold%2520standard%2520for%2520non-invasive%250Acardiac%2520assessment%252C%2520offering%2520rich%2520spatio-temporal%2520views%2520of%2520the%2520cardiac%2520anatomy%250Aand%2520physiology.%2520Patient-level%2520health%2520factors%252C%2520such%2520as%2520demographics%252C%2520metabolic%252C%250Aand%2520lifestyle%252C%2520are%2520known%2520to%2520substantially%2520influence%2520cardiovascular%2520health%2520and%250Adisease%2520risk%252C%2520yet%2520remain%2520uncaptured%2520by%2520CMR%2520alone.%2520To%2520holistically%2520understand%250Acardiac%2520health%2520and%2520to%2520enable%2520the%2520best%2520possible%2520interpretation%2520of%2520an%250Aindividual%2527s%2520disease%2520risk%252C%2520CMR%2520and%2520patient-level%2520factors%2520must%2520be%2520jointly%250Aexploited%2520within%2520an%2520integrated%2520framework.%2520Recent%2520multi-modal%2520approaches%2520have%250Abegun%2520to%2520bridge%2520this%2520gap%252C%2520yet%2520they%2520often%2520rely%2520on%2520limited%2520spatio-temporal%2520data%250Aand%2520focus%2520on%2520isolated%2520clinical%2520tasks%252C%2520thereby%2520hindering%2520the%2520development%2520of%2520a%250Acomprehensive%2520representation%2520for%2520cardiac%2520health%2520evaluation.%2520To%2520overcome%2520these%250Alimitations%252C%2520we%2520introduce%2520ViTa%252C%2520a%2520step%2520toward%2520foundation%2520models%2520that%2520delivers%2520a%250Acomprehensive%2520representation%2520of%2520the%2520heart%2520and%2520a%2520precise%2520interpretation%2520of%250Aindividual%2520disease%2520risk.%2520Leveraging%2520data%2520from%252042%252C000%2520UK%2520Biobank%2520participants%252C%250AViTa%2520integrates%25203D%252BT%2520cine%2520stacks%2520from%2520short-axis%2520and%2520long-axis%2520views%252C%2520enabling%250Aa%2520complete%2520capture%2520of%2520the%2520cardiac%2520cycle.%2520These%2520imaging%2520data%2520are%2520then%2520fused%2520with%250Adetailed%2520tabular%2520patient-level%2520factors%252C%2520enabling%2520context-aware%2520insights.%2520This%250Amulti-modal%2520paradigm%2520supports%2520a%2520wide%2520spectrum%2520of%2520downstream%2520tasks%252C%2520including%250Acardiac%2520phenotype%2520and%2520physiological%2520feature%2520prediction%252C%2520segmentation%252C%2520and%250Aclassification%2520of%2520cardiac%2520and%2520metabolic%2520diseases%2520within%2520a%2520single%2520unified%250Aframework.%2520By%2520learning%2520a%2520shared%2520latent%2520representation%2520that%2520bridges%2520rich%2520imaging%250Afeatures%2520and%2520patient%2520context%252C%2520ViTa%2520moves%2520beyond%2520traditional%252C%2520task-specific%250Amodels%2520toward%2520a%2520universal%252C%2520patient-specific%2520understanding%2520of%2520cardiac%2520health%252C%250Ahighlighting%2520its%2520potential%2520to%2520advance%2520clinical%2520utility%2520and%2520scalability%2520in%250Acardiac%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13037v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Cardiac%20MRI%20Foundation%20Models%3A%20Comprehensive%20Visual-Tabular%0A%20%20Representations%20for%20Whole-Heart%20Assessment%20and%20Beyond&entry.906535625=Yundi%20Zhang%20and%20Paul%20Hager%20and%20Che%20Liu%20and%20Suprosanna%20Shit%20and%20Chen%20Chen%20and%20Daniel%20Rueckert%20and%20Jiazhen%20Pan&entry.1292438233=%20%20Cardiac%20magnetic%20resonance%20imaging%20is%20the%20gold%20standard%20for%20non-invasive%0Acardiac%20assessment%2C%20offering%20rich%20spatio-temporal%20views%20of%20the%20cardiac%20anatomy%0Aand%20physiology.%20Patient-level%20health%20factors%2C%20such%20as%20demographics%2C%20metabolic%2C%0Aand%20lifestyle%2C%20are%20known%20to%20substantially%20influence%20cardiovascular%20health%20and%0Adisease%20risk%2C%20yet%20remain%20uncaptured%20by%20CMR%20alone.%20To%20holistically%20understand%0Acardiac%20health%20and%20to%20enable%20the%20best%20possible%20interpretation%20of%20an%0Aindividual%27s%20disease%20risk%2C%20CMR%20and%20patient-level%20factors%20must%20be%20jointly%0Aexploited%20within%20an%20integrated%20framework.%20Recent%20multi-modal%20approaches%20have%0Abegun%20to%20bridge%20this%20gap%2C%20yet%20they%20often%20rely%20on%20limited%20spatio-temporal%20data%0Aand%20focus%20on%20isolated%20clinical%20tasks%2C%20thereby%20hindering%20the%20development%20of%20a%0Acomprehensive%20representation%20for%20cardiac%20health%20evaluation.%20To%20overcome%20these%0Alimitations%2C%20we%20introduce%20ViTa%2C%20a%20step%20toward%20foundation%20models%20that%20delivers%20a%0Acomprehensive%20representation%20of%20the%20heart%20and%20a%20precise%20interpretation%20of%0Aindividual%20disease%20risk.%20Leveraging%20data%20from%2042%2C000%20UK%20Biobank%20participants%2C%0AViTa%20integrates%203D%2BT%20cine%20stacks%20from%20short-axis%20and%20long-axis%20views%2C%20enabling%0Aa%20complete%20capture%20of%20the%20cardiac%20cycle.%20These%20imaging%20data%20are%20then%20fused%20with%0Adetailed%20tabular%20patient-level%20factors%2C%20enabling%20context-aware%20insights.%20This%0Amulti-modal%20paradigm%20supports%20a%20wide%20spectrum%20of%20downstream%20tasks%2C%20including%0Acardiac%20phenotype%20and%20physiological%20feature%20prediction%2C%20segmentation%2C%20and%0Aclassification%20of%20cardiac%20and%20metabolic%20diseases%20within%20a%20single%20unified%0Aframework.%20By%20learning%20a%20shared%20latent%20representation%20that%20bridges%20rich%20imaging%0Afeatures%20and%20patient%20context%2C%20ViTa%20moves%20beyond%20traditional%2C%20task-specific%0Amodels%20toward%20a%20universal%2C%20patient-specific%20understanding%20of%20cardiac%20health%2C%0Ahighlighting%20its%20potential%20to%20advance%20clinical%20utility%20and%20scalability%20in%0Acardiac%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13037v1&entry.124074799=Read"},
{"title": "Digital Twin Generation from Visual Data: A Survey", "author": "Andrew Melnik and Benjamin Alt and Giang Nguyen and Artur Wilkowski and Maciej Stefa\u0144czyk and Qirui Wu and Sinan Harms and Helge Rhodin and Manolis Savva and Michael Beetz", "abstract": "  This survey explores recent developments in generating digital twins from\nvideos. Such digital twins can be used for robotics application, media content\ncreation, or design and construction works. We analyze various approaches,\nincluding 3D Gaussian Splatting, generative in-painting, semantic segmentation,\nand foundation models highlighting their advantages and limitations.\nAdditionally, we discuss challenges such as occlusions, lighting variations,\nand scalability, as well as potential future research directions. This survey\naims to provide a comprehensive overview of state-of-the-art methodologies and\ntheir implications for real-world applications. Awesome list:\nhttps://github.com/ndrwmlnk/awesome-digital-twins\n", "link": "http://arxiv.org/abs/2504.13159v1", "date": "2025-04-17", "relevancy": 2.8115, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5698}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5698}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Digital%20Twin%20Generation%20from%20Visual%20Data%3A%20A%20Survey&body=Title%3A%20Digital%20Twin%20Generation%20from%20Visual%20Data%3A%20A%20Survey%0AAuthor%3A%20Andrew%20Melnik%20and%20Benjamin%20Alt%20and%20Giang%20Nguyen%20and%20Artur%20Wilkowski%20and%20Maciej%20Stefa%C5%84czyk%20and%20Qirui%20Wu%20and%20Sinan%20Harms%20and%20Helge%20Rhodin%20and%20Manolis%20Savva%20and%20Michael%20Beetz%0AAbstract%3A%20%20%20This%20survey%20explores%20recent%20developments%20in%20generating%20digital%20twins%20from%0Avideos.%20Such%20digital%20twins%20can%20be%20used%20for%20robotics%20application%2C%20media%20content%0Acreation%2C%20or%20design%20and%20construction%20works.%20We%20analyze%20various%20approaches%2C%0Aincluding%203D%20Gaussian%20Splatting%2C%20generative%20in-painting%2C%20semantic%20segmentation%2C%0Aand%20foundation%20models%20highlighting%20their%20advantages%20and%20limitations.%0AAdditionally%2C%20we%20discuss%20challenges%20such%20as%20occlusions%2C%20lighting%20variations%2C%0Aand%20scalability%2C%20as%20well%20as%20potential%20future%20research%20directions.%20This%20survey%0Aaims%20to%20provide%20a%20comprehensive%20overview%20of%20state-of-the-art%20methodologies%20and%0Atheir%20implications%20for%20real-world%20applications.%20Awesome%20list%3A%0Ahttps%3A//github.com/ndrwmlnk/awesome-digital-twins%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13159v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDigital%2520Twin%2520Generation%2520from%2520Visual%2520Data%253A%2520A%2520Survey%26entry.906535625%3DAndrew%2520Melnik%2520and%2520Benjamin%2520Alt%2520and%2520Giang%2520Nguyen%2520and%2520Artur%2520Wilkowski%2520and%2520Maciej%2520Stefa%25C5%2584czyk%2520and%2520Qirui%2520Wu%2520and%2520Sinan%2520Harms%2520and%2520Helge%2520Rhodin%2520and%2520Manolis%2520Savva%2520and%2520Michael%2520Beetz%26entry.1292438233%3D%2520%2520This%2520survey%2520explores%2520recent%2520developments%2520in%2520generating%2520digital%2520twins%2520from%250Avideos.%2520Such%2520digital%2520twins%2520can%2520be%2520used%2520for%2520robotics%2520application%252C%2520media%2520content%250Acreation%252C%2520or%2520design%2520and%2520construction%2520works.%2520We%2520analyze%2520various%2520approaches%252C%250Aincluding%25203D%2520Gaussian%2520Splatting%252C%2520generative%2520in-painting%252C%2520semantic%2520segmentation%252C%250Aand%2520foundation%2520models%2520highlighting%2520their%2520advantages%2520and%2520limitations.%250AAdditionally%252C%2520we%2520discuss%2520challenges%2520such%2520as%2520occlusions%252C%2520lighting%2520variations%252C%250Aand%2520scalability%252C%2520as%2520well%2520as%2520potential%2520future%2520research%2520directions.%2520This%2520survey%250Aaims%2520to%2520provide%2520a%2520comprehensive%2520overview%2520of%2520state-of-the-art%2520methodologies%2520and%250Atheir%2520implications%2520for%2520real-world%2520applications.%2520Awesome%2520list%253A%250Ahttps%253A//github.com/ndrwmlnk/awesome-digital-twins%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13159v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Digital%20Twin%20Generation%20from%20Visual%20Data%3A%20A%20Survey&entry.906535625=Andrew%20Melnik%20and%20Benjamin%20Alt%20and%20Giang%20Nguyen%20and%20Artur%20Wilkowski%20and%20Maciej%20Stefa%C5%84czyk%20and%20Qirui%20Wu%20and%20Sinan%20Harms%20and%20Helge%20Rhodin%20and%20Manolis%20Savva%20and%20Michael%20Beetz&entry.1292438233=%20%20This%20survey%20explores%20recent%20developments%20in%20generating%20digital%20twins%20from%0Avideos.%20Such%20digital%20twins%20can%20be%20used%20for%20robotics%20application%2C%20media%20content%0Acreation%2C%20or%20design%20and%20construction%20works.%20We%20analyze%20various%20approaches%2C%0Aincluding%203D%20Gaussian%20Splatting%2C%20generative%20in-painting%2C%20semantic%20segmentation%2C%0Aand%20foundation%20models%20highlighting%20their%20advantages%20and%20limitations.%0AAdditionally%2C%20we%20discuss%20challenges%20such%20as%20occlusions%2C%20lighting%20variations%2C%0Aand%20scalability%2C%20as%20well%20as%20potential%20future%20research%20directions.%20This%20survey%0Aaims%20to%20provide%20a%20comprehensive%20overview%20of%20state-of-the-art%20methodologies%20and%0Atheir%20implications%20for%20real-world%20applications.%20Awesome%20list%3A%0Ahttps%3A//github.com/ndrwmlnk/awesome-digital-twins%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13159v1&entry.124074799=Read"},
{"title": "Benchmarking the Spatial Robustness of DNNs via Natural and Adversarial\n  Localized Corruptions", "author": "Giulia Marchiori Pietrosanti and Giulio Rossolini and Alessandro Biondi and Giorgio Buttazzo", "abstract": "  The robustness of DNNs is a crucial factor in safety-critical applications,\nparticularly in complex and dynamic environments where localized corruptions\ncan arise. While previous studies have evaluated the robustness of semantic\nsegmentation (SS) models under whole-image natural or adversarial corruptions,\na comprehensive investigation into the spatial robustness of dense vision\nmodels under localized corruptions remained underexplored. This paper fills\nthis gap by introducing specialized metrics for benchmarking the spatial\nrobustness of segmentation models, alongside with an evaluation framework to\nassess the impact of localized corruptions. Furthermore, we uncover the\ninherent complexity of characterizing worst-case robustness using a single\nlocalized adversarial perturbation. To address this, we propose region-aware\nmulti-attack adversarial analysis, a method that enables a deeper understanding\nof model robustness against adversarial perturbations applied to specific\nregions. The proposed metrics and analysis were exploited to evaluate 14\nsegmentation models in driving scenarios, uncovering key insights into the\neffects of localized corruption in both natural and adversarial forms. The\nresults reveal that models respond to these two types of threats differently;\nfor instance, transformer-based segmentation models demonstrate notable\nrobustness to localized natural corruptions but are highly vulnerable to\nadversarial ones and vice-versa for CNN-based models. Consequently, we also\naddress the challenge of balancing robustness to both natural and adversarial\nlocalized corruptions by means of ensemble models, thereby achieving a broader\nthreat coverage and improved reliability for dense vision tasks.\n", "link": "http://arxiv.org/abs/2504.01632v2", "date": "2025-04-17", "relevancy": 2.8076, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.592}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5638}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20the%20Spatial%20Robustness%20of%20DNNs%20via%20Natural%20and%20Adversarial%0A%20%20Localized%20Corruptions&body=Title%3A%20Benchmarking%20the%20Spatial%20Robustness%20of%20DNNs%20via%20Natural%20and%20Adversarial%0A%20%20Localized%20Corruptions%0AAuthor%3A%20Giulia%20Marchiori%20Pietrosanti%20and%20Giulio%20Rossolini%20and%20Alessandro%20Biondi%20and%20Giorgio%20Buttazzo%0AAbstract%3A%20%20%20The%20robustness%20of%20DNNs%20is%20a%20crucial%20factor%20in%20safety-critical%20applications%2C%0Aparticularly%20in%20complex%20and%20dynamic%20environments%20where%20localized%20corruptions%0Acan%20arise.%20While%20previous%20studies%20have%20evaluated%20the%20robustness%20of%20semantic%0Asegmentation%20%28SS%29%20models%20under%20whole-image%20natural%20or%20adversarial%20corruptions%2C%0Aa%20comprehensive%20investigation%20into%20the%20spatial%20robustness%20of%20dense%20vision%0Amodels%20under%20localized%20corruptions%20remained%20underexplored.%20This%20paper%20fills%0Athis%20gap%20by%20introducing%20specialized%20metrics%20for%20benchmarking%20the%20spatial%0Arobustness%20of%20segmentation%20models%2C%20alongside%20with%20an%20evaluation%20framework%20to%0Aassess%20the%20impact%20of%20localized%20corruptions.%20Furthermore%2C%20we%20uncover%20the%0Ainherent%20complexity%20of%20characterizing%20worst-case%20robustness%20using%20a%20single%0Alocalized%20adversarial%20perturbation.%20To%20address%20this%2C%20we%20propose%20region-aware%0Amulti-attack%20adversarial%20analysis%2C%20a%20method%20that%20enables%20a%20deeper%20understanding%0Aof%20model%20robustness%20against%20adversarial%20perturbations%20applied%20to%20specific%0Aregions.%20The%20proposed%20metrics%20and%20analysis%20were%20exploited%20to%20evaluate%2014%0Asegmentation%20models%20in%20driving%20scenarios%2C%20uncovering%20key%20insights%20into%20the%0Aeffects%20of%20localized%20corruption%20in%20both%20natural%20and%20adversarial%20forms.%20The%0Aresults%20reveal%20that%20models%20respond%20to%20these%20two%20types%20of%20threats%20differently%3B%0Afor%20instance%2C%20transformer-based%20segmentation%20models%20demonstrate%20notable%0Arobustness%20to%20localized%20natural%20corruptions%20but%20are%20highly%20vulnerable%20to%0Aadversarial%20ones%20and%20vice-versa%20for%20CNN-based%20models.%20Consequently%2C%20we%20also%0Aaddress%20the%20challenge%20of%20balancing%20robustness%20to%20both%20natural%20and%20adversarial%0Alocalized%20corruptions%20by%20means%20of%20ensemble%20models%2C%20thereby%20achieving%20a%20broader%0Athreat%20coverage%20and%20improved%20reliability%20for%20dense%20vision%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.01632v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520the%2520Spatial%2520Robustness%2520of%2520DNNs%2520via%2520Natural%2520and%2520Adversarial%250A%2520%2520Localized%2520Corruptions%26entry.906535625%3DGiulia%2520Marchiori%2520Pietrosanti%2520and%2520Giulio%2520Rossolini%2520and%2520Alessandro%2520Biondi%2520and%2520Giorgio%2520Buttazzo%26entry.1292438233%3D%2520%2520The%2520robustness%2520of%2520DNNs%2520is%2520a%2520crucial%2520factor%2520in%2520safety-critical%2520applications%252C%250Aparticularly%2520in%2520complex%2520and%2520dynamic%2520environments%2520where%2520localized%2520corruptions%250Acan%2520arise.%2520While%2520previous%2520studies%2520have%2520evaluated%2520the%2520robustness%2520of%2520semantic%250Asegmentation%2520%2528SS%2529%2520models%2520under%2520whole-image%2520natural%2520or%2520adversarial%2520corruptions%252C%250Aa%2520comprehensive%2520investigation%2520into%2520the%2520spatial%2520robustness%2520of%2520dense%2520vision%250Amodels%2520under%2520localized%2520corruptions%2520remained%2520underexplored.%2520This%2520paper%2520fills%250Athis%2520gap%2520by%2520introducing%2520specialized%2520metrics%2520for%2520benchmarking%2520the%2520spatial%250Arobustness%2520of%2520segmentation%2520models%252C%2520alongside%2520with%2520an%2520evaluation%2520framework%2520to%250Aassess%2520the%2520impact%2520of%2520localized%2520corruptions.%2520Furthermore%252C%2520we%2520uncover%2520the%250Ainherent%2520complexity%2520of%2520characterizing%2520worst-case%2520robustness%2520using%2520a%2520single%250Alocalized%2520adversarial%2520perturbation.%2520To%2520address%2520this%252C%2520we%2520propose%2520region-aware%250Amulti-attack%2520adversarial%2520analysis%252C%2520a%2520method%2520that%2520enables%2520a%2520deeper%2520understanding%250Aof%2520model%2520robustness%2520against%2520adversarial%2520perturbations%2520applied%2520to%2520specific%250Aregions.%2520The%2520proposed%2520metrics%2520and%2520analysis%2520were%2520exploited%2520to%2520evaluate%252014%250Asegmentation%2520models%2520in%2520driving%2520scenarios%252C%2520uncovering%2520key%2520insights%2520into%2520the%250Aeffects%2520of%2520localized%2520corruption%2520in%2520both%2520natural%2520and%2520adversarial%2520forms.%2520The%250Aresults%2520reveal%2520that%2520models%2520respond%2520to%2520these%2520two%2520types%2520of%2520threats%2520differently%253B%250Afor%2520instance%252C%2520transformer-based%2520segmentation%2520models%2520demonstrate%2520notable%250Arobustness%2520to%2520localized%2520natural%2520corruptions%2520but%2520are%2520highly%2520vulnerable%2520to%250Aadversarial%2520ones%2520and%2520vice-versa%2520for%2520CNN-based%2520models.%2520Consequently%252C%2520we%2520also%250Aaddress%2520the%2520challenge%2520of%2520balancing%2520robustness%2520to%2520both%2520natural%2520and%2520adversarial%250Alocalized%2520corruptions%2520by%2520means%2520of%2520ensemble%2520models%252C%2520thereby%2520achieving%2520a%2520broader%250Athreat%2520coverage%2520and%2520improved%2520reliability%2520for%2520dense%2520vision%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.01632v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20the%20Spatial%20Robustness%20of%20DNNs%20via%20Natural%20and%20Adversarial%0A%20%20Localized%20Corruptions&entry.906535625=Giulia%20Marchiori%20Pietrosanti%20and%20Giulio%20Rossolini%20and%20Alessandro%20Biondi%20and%20Giorgio%20Buttazzo&entry.1292438233=%20%20The%20robustness%20of%20DNNs%20is%20a%20crucial%20factor%20in%20safety-critical%20applications%2C%0Aparticularly%20in%20complex%20and%20dynamic%20environments%20where%20localized%20corruptions%0Acan%20arise.%20While%20previous%20studies%20have%20evaluated%20the%20robustness%20of%20semantic%0Asegmentation%20%28SS%29%20models%20under%20whole-image%20natural%20or%20adversarial%20corruptions%2C%0Aa%20comprehensive%20investigation%20into%20the%20spatial%20robustness%20of%20dense%20vision%0Amodels%20under%20localized%20corruptions%20remained%20underexplored.%20This%20paper%20fills%0Athis%20gap%20by%20introducing%20specialized%20metrics%20for%20benchmarking%20the%20spatial%0Arobustness%20of%20segmentation%20models%2C%20alongside%20with%20an%20evaluation%20framework%20to%0Aassess%20the%20impact%20of%20localized%20corruptions.%20Furthermore%2C%20we%20uncover%20the%0Ainherent%20complexity%20of%20characterizing%20worst-case%20robustness%20using%20a%20single%0Alocalized%20adversarial%20perturbation.%20To%20address%20this%2C%20we%20propose%20region-aware%0Amulti-attack%20adversarial%20analysis%2C%20a%20method%20that%20enables%20a%20deeper%20understanding%0Aof%20model%20robustness%20against%20adversarial%20perturbations%20applied%20to%20specific%0Aregions.%20The%20proposed%20metrics%20and%20analysis%20were%20exploited%20to%20evaluate%2014%0Asegmentation%20models%20in%20driving%20scenarios%2C%20uncovering%20key%20insights%20into%20the%0Aeffects%20of%20localized%20corruption%20in%20both%20natural%20and%20adversarial%20forms.%20The%0Aresults%20reveal%20that%20models%20respond%20to%20these%20two%20types%20of%20threats%20differently%3B%0Afor%20instance%2C%20transformer-based%20segmentation%20models%20demonstrate%20notable%0Arobustness%20to%20localized%20natural%20corruptions%20but%20are%20highly%20vulnerable%20to%0Aadversarial%20ones%20and%20vice-versa%20for%20CNN-based%20models.%20Consequently%2C%20we%20also%0Aaddress%20the%20challenge%20of%20balancing%20robustness%20to%20both%20natural%20and%20adversarial%0Alocalized%20corruptions%20by%20means%20of%20ensemble%20models%2C%20thereby%20achieving%20a%20broader%0Athreat%20coverage%20and%20improved%20reliability%20for%20dense%20vision%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.01632v2&entry.124074799=Read"},
{"title": "DC-SAM: In-Context Segment Anything in Images and Videos via Dual\n  Consistency", "author": "Mengshi Qi and Pengfei Zhu and Xiangtai Li and Xiaoyang Bi and Lu Qi and Huadong Ma and Ming-Hsuan Yang", "abstract": "  Given a single labeled example, in-context segmentation aims to segment\ncorresponding objects. This setting, known as one-shot segmentation in few-shot\nlearning, explores the segmentation model's generalization ability and has been\napplied to various vision tasks, including scene understanding and image/video\nediting. While recent Segment Anything Models have achieved state-of-the-art\nresults in interactive segmentation, these approaches are not directly\napplicable to in-context segmentation. In this work, we propose the Dual\nConsistency SAM (DC-SAM) method based on prompt-tuning to adapt SAM and SAM2\nfor in-context segmentation of both images and videos. Our key insights are to\nenhance the features of the SAM's prompt encoder in segmentation by providing\nhigh-quality visual prompts. When generating a mask prior, we fuse the SAM\nfeatures to better align the prompt encoder. Then, we design a cycle-consistent\ncross-attention on fused features and initial visual prompts. Next, a\ndual-branch design is provided by using the discriminative positive and\nnegative prompts in the prompt encoder. Furthermore, we design a simple\nmask-tube training strategy to adopt our proposed dual consistency method into\nthe mask tube. Although the proposed DC-SAM is primarily designed for images,\nit can be seamlessly extended to the video domain with the support of SAM2.\nGiven the absence of in-context segmentation in the video domain, we manually\ncurate and construct the first benchmark from existing video segmentation\ndatasets, named In-Context Video Object Segmentation (IC-VOS), to better assess\nthe in-context capability of the model. Extensive experiments demonstrate that\nour method achieves 55.5 (+1.4) mIoU on COCO-20i, 73.0 (+1.1) mIoU on\nPASCAL-5i, and a J&F score of 71.52 on the proposed IC-VOS benchmark. Our\nsource code and benchmark are available at https://github.com/zaplm/DC-SAM.\n", "link": "http://arxiv.org/abs/2504.12080v2", "date": "2025-04-17", "relevancy": 2.8029, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5732}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5568}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DC-SAM%3A%20In-Context%20Segment%20Anything%20in%20Images%20and%20Videos%20via%20Dual%0A%20%20Consistency&body=Title%3A%20DC-SAM%3A%20In-Context%20Segment%20Anything%20in%20Images%20and%20Videos%20via%20Dual%0A%20%20Consistency%0AAuthor%3A%20Mengshi%20Qi%20and%20Pengfei%20Zhu%20and%20Xiangtai%20Li%20and%20Xiaoyang%20Bi%20and%20Lu%20Qi%20and%20Huadong%20Ma%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20%20%20Given%20a%20single%20labeled%20example%2C%20in-context%20segmentation%20aims%20to%20segment%0Acorresponding%20objects.%20This%20setting%2C%20known%20as%20one-shot%20segmentation%20in%20few-shot%0Alearning%2C%20explores%20the%20segmentation%20model%27s%20generalization%20ability%20and%20has%20been%0Aapplied%20to%20various%20vision%20tasks%2C%20including%20scene%20understanding%20and%20image/video%0Aediting.%20While%20recent%20Segment%20Anything%20Models%20have%20achieved%20state-of-the-art%0Aresults%20in%20interactive%20segmentation%2C%20these%20approaches%20are%20not%20directly%0Aapplicable%20to%20in-context%20segmentation.%20In%20this%20work%2C%20we%20propose%20the%20Dual%0AConsistency%20SAM%20%28DC-SAM%29%20method%20based%20on%20prompt-tuning%20to%20adapt%20SAM%20and%20SAM2%0Afor%20in-context%20segmentation%20of%20both%20images%20and%20videos.%20Our%20key%20insights%20are%20to%0Aenhance%20the%20features%20of%20the%20SAM%27s%20prompt%20encoder%20in%20segmentation%20by%20providing%0Ahigh-quality%20visual%20prompts.%20When%20generating%20a%20mask%20prior%2C%20we%20fuse%20the%20SAM%0Afeatures%20to%20better%20align%20the%20prompt%20encoder.%20Then%2C%20we%20design%20a%20cycle-consistent%0Across-attention%20on%20fused%20features%20and%20initial%20visual%20prompts.%20Next%2C%20a%0Adual-branch%20design%20is%20provided%20by%20using%20the%20discriminative%20positive%20and%0Anegative%20prompts%20in%20the%20prompt%20encoder.%20Furthermore%2C%20we%20design%20a%20simple%0Amask-tube%20training%20strategy%20to%20adopt%20our%20proposed%20dual%20consistency%20method%20into%0Athe%20mask%20tube.%20Although%20the%20proposed%20DC-SAM%20is%20primarily%20designed%20for%20images%2C%0Ait%20can%20be%20seamlessly%20extended%20to%20the%20video%20domain%20with%20the%20support%20of%20SAM2.%0AGiven%20the%20absence%20of%20in-context%20segmentation%20in%20the%20video%20domain%2C%20we%20manually%0Acurate%20and%20construct%20the%20first%20benchmark%20from%20existing%20video%20segmentation%0Adatasets%2C%20named%20In-Context%20Video%20Object%20Segmentation%20%28IC-VOS%29%2C%20to%20better%20assess%0Athe%20in-context%20capability%20of%20the%20model.%20Extensive%20experiments%20demonstrate%20that%0Aour%20method%20achieves%2055.5%20%28%2B1.4%29%20mIoU%20on%20COCO-20i%2C%2073.0%20%28%2B1.1%29%20mIoU%20on%0APASCAL-5i%2C%20and%20a%20J%26F%20score%20of%2071.52%20on%20the%20proposed%20IC-VOS%20benchmark.%20Our%0Asource%20code%20and%20benchmark%20are%20available%20at%20https%3A//github.com/zaplm/DC-SAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12080v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDC-SAM%253A%2520In-Context%2520Segment%2520Anything%2520in%2520Images%2520and%2520Videos%2520via%2520Dual%250A%2520%2520Consistency%26entry.906535625%3DMengshi%2520Qi%2520and%2520Pengfei%2520Zhu%2520and%2520Xiangtai%2520Li%2520and%2520Xiaoyang%2520Bi%2520and%2520Lu%2520Qi%2520and%2520Huadong%2520Ma%2520and%2520Ming-Hsuan%2520Yang%26entry.1292438233%3D%2520%2520Given%2520a%2520single%2520labeled%2520example%252C%2520in-context%2520segmentation%2520aims%2520to%2520segment%250Acorresponding%2520objects.%2520This%2520setting%252C%2520known%2520as%2520one-shot%2520segmentation%2520in%2520few-shot%250Alearning%252C%2520explores%2520the%2520segmentation%2520model%2527s%2520generalization%2520ability%2520and%2520has%2520been%250Aapplied%2520to%2520various%2520vision%2520tasks%252C%2520including%2520scene%2520understanding%2520and%2520image/video%250Aediting.%2520While%2520recent%2520Segment%2520Anything%2520Models%2520have%2520achieved%2520state-of-the-art%250Aresults%2520in%2520interactive%2520segmentation%252C%2520these%2520approaches%2520are%2520not%2520directly%250Aapplicable%2520to%2520in-context%2520segmentation.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520Dual%250AConsistency%2520SAM%2520%2528DC-SAM%2529%2520method%2520based%2520on%2520prompt-tuning%2520to%2520adapt%2520SAM%2520and%2520SAM2%250Afor%2520in-context%2520segmentation%2520of%2520both%2520images%2520and%2520videos.%2520Our%2520key%2520insights%2520are%2520to%250Aenhance%2520the%2520features%2520of%2520the%2520SAM%2527s%2520prompt%2520encoder%2520in%2520segmentation%2520by%2520providing%250Ahigh-quality%2520visual%2520prompts.%2520When%2520generating%2520a%2520mask%2520prior%252C%2520we%2520fuse%2520the%2520SAM%250Afeatures%2520to%2520better%2520align%2520the%2520prompt%2520encoder.%2520Then%252C%2520we%2520design%2520a%2520cycle-consistent%250Across-attention%2520on%2520fused%2520features%2520and%2520initial%2520visual%2520prompts.%2520Next%252C%2520a%250Adual-branch%2520design%2520is%2520provided%2520by%2520using%2520the%2520discriminative%2520positive%2520and%250Anegative%2520prompts%2520in%2520the%2520prompt%2520encoder.%2520Furthermore%252C%2520we%2520design%2520a%2520simple%250Amask-tube%2520training%2520strategy%2520to%2520adopt%2520our%2520proposed%2520dual%2520consistency%2520method%2520into%250Athe%2520mask%2520tube.%2520Although%2520the%2520proposed%2520DC-SAM%2520is%2520primarily%2520designed%2520for%2520images%252C%250Ait%2520can%2520be%2520seamlessly%2520extended%2520to%2520the%2520video%2520domain%2520with%2520the%2520support%2520of%2520SAM2.%250AGiven%2520the%2520absence%2520of%2520in-context%2520segmentation%2520in%2520the%2520video%2520domain%252C%2520we%2520manually%250Acurate%2520and%2520construct%2520the%2520first%2520benchmark%2520from%2520existing%2520video%2520segmentation%250Adatasets%252C%2520named%2520In-Context%2520Video%2520Object%2520Segmentation%2520%2528IC-VOS%2529%252C%2520to%2520better%2520assess%250Athe%2520in-context%2520capability%2520of%2520the%2520model.%2520Extensive%2520experiments%2520demonstrate%2520that%250Aour%2520method%2520achieves%252055.5%2520%2528%252B1.4%2529%2520mIoU%2520on%2520COCO-20i%252C%252073.0%2520%2528%252B1.1%2529%2520mIoU%2520on%250APASCAL-5i%252C%2520and%2520a%2520J%2526F%2520score%2520of%252071.52%2520on%2520the%2520proposed%2520IC-VOS%2520benchmark.%2520Our%250Asource%2520code%2520and%2520benchmark%2520are%2520available%2520at%2520https%253A//github.com/zaplm/DC-SAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12080v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DC-SAM%3A%20In-Context%20Segment%20Anything%20in%20Images%20and%20Videos%20via%20Dual%0A%20%20Consistency&entry.906535625=Mengshi%20Qi%20and%20Pengfei%20Zhu%20and%20Xiangtai%20Li%20and%20Xiaoyang%20Bi%20and%20Lu%20Qi%20and%20Huadong%20Ma%20and%20Ming-Hsuan%20Yang&entry.1292438233=%20%20Given%20a%20single%20labeled%20example%2C%20in-context%20segmentation%20aims%20to%20segment%0Acorresponding%20objects.%20This%20setting%2C%20known%20as%20one-shot%20segmentation%20in%20few-shot%0Alearning%2C%20explores%20the%20segmentation%20model%27s%20generalization%20ability%20and%20has%20been%0Aapplied%20to%20various%20vision%20tasks%2C%20including%20scene%20understanding%20and%20image/video%0Aediting.%20While%20recent%20Segment%20Anything%20Models%20have%20achieved%20state-of-the-art%0Aresults%20in%20interactive%20segmentation%2C%20these%20approaches%20are%20not%20directly%0Aapplicable%20to%20in-context%20segmentation.%20In%20this%20work%2C%20we%20propose%20the%20Dual%0AConsistency%20SAM%20%28DC-SAM%29%20method%20based%20on%20prompt-tuning%20to%20adapt%20SAM%20and%20SAM2%0Afor%20in-context%20segmentation%20of%20both%20images%20and%20videos.%20Our%20key%20insights%20are%20to%0Aenhance%20the%20features%20of%20the%20SAM%27s%20prompt%20encoder%20in%20segmentation%20by%20providing%0Ahigh-quality%20visual%20prompts.%20When%20generating%20a%20mask%20prior%2C%20we%20fuse%20the%20SAM%0Afeatures%20to%20better%20align%20the%20prompt%20encoder.%20Then%2C%20we%20design%20a%20cycle-consistent%0Across-attention%20on%20fused%20features%20and%20initial%20visual%20prompts.%20Next%2C%20a%0Adual-branch%20design%20is%20provided%20by%20using%20the%20discriminative%20positive%20and%0Anegative%20prompts%20in%20the%20prompt%20encoder.%20Furthermore%2C%20we%20design%20a%20simple%0Amask-tube%20training%20strategy%20to%20adopt%20our%20proposed%20dual%20consistency%20method%20into%0Athe%20mask%20tube.%20Although%20the%20proposed%20DC-SAM%20is%20primarily%20designed%20for%20images%2C%0Ait%20can%20be%20seamlessly%20extended%20to%20the%20video%20domain%20with%20the%20support%20of%20SAM2.%0AGiven%20the%20absence%20of%20in-context%20segmentation%20in%20the%20video%20domain%2C%20we%20manually%0Acurate%20and%20construct%20the%20first%20benchmark%20from%20existing%20video%20segmentation%0Adatasets%2C%20named%20In-Context%20Video%20Object%20Segmentation%20%28IC-VOS%29%2C%20to%20better%20assess%0Athe%20in-context%20capability%20of%20the%20model.%20Extensive%20experiments%20demonstrate%20that%0Aour%20method%20achieves%2055.5%20%28%2B1.4%29%20mIoU%20on%20COCO-20i%2C%2073.0%20%28%2B1.1%29%20mIoU%20on%0APASCAL-5i%2C%20and%20a%20J%26F%20score%20of%2071.52%20on%20the%20proposed%20IC-VOS%20benchmark.%20Our%0Asource%20code%20and%20benchmark%20are%20available%20at%20https%3A//github.com/zaplm/DC-SAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12080v2&entry.124074799=Read"},
{"title": "Pose and Facial Expression Transfer by using StyleGAN", "author": "Petr Jahoda and Jan Cech", "abstract": "  We propose a method to transfer pose and expression between face images.\nGiven a source and target face portrait, the model produces an output image in\nwhich the pose and expression of the source face image are transferred onto the\ntarget identity. The architecture consists of two encoders and a mapping\nnetwork that projects the two inputs into the latent space of StyleGAN2, which\nfinally generates the output. The training is self-supervised from video\nsequences of many individuals. Manual labeling is not required. Our model\nenables the synthesis of random identities with controllable pose and\nexpression. Close-to-real-time performance is achieved.\n", "link": "http://arxiv.org/abs/2504.13021v1", "date": "2025-04-17", "relevancy": 2.7985, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5785}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5604}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pose%20and%20Facial%20Expression%20Transfer%20by%20using%20StyleGAN&body=Title%3A%20Pose%20and%20Facial%20Expression%20Transfer%20by%20using%20StyleGAN%0AAuthor%3A%20Petr%20Jahoda%20and%20Jan%20Cech%0AAbstract%3A%20%20%20We%20propose%20a%20method%20to%20transfer%20pose%20and%20expression%20between%20face%20images.%0AGiven%20a%20source%20and%20target%20face%20portrait%2C%20the%20model%20produces%20an%20output%20image%20in%0Awhich%20the%20pose%20and%20expression%20of%20the%20source%20face%20image%20are%20transferred%20onto%20the%0Atarget%20identity.%20The%20architecture%20consists%20of%20two%20encoders%20and%20a%20mapping%0Anetwork%20that%20projects%20the%20two%20inputs%20into%20the%20latent%20space%20of%20StyleGAN2%2C%20which%0Afinally%20generates%20the%20output.%20The%20training%20is%20self-supervised%20from%20video%0Asequences%20of%20many%20individuals.%20Manual%20labeling%20is%20not%20required.%20Our%20model%0Aenables%20the%20synthesis%20of%20random%20identities%20with%20controllable%20pose%20and%0Aexpression.%20Close-to-real-time%20performance%20is%20achieved.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13021v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPose%2520and%2520Facial%2520Expression%2520Transfer%2520by%2520using%2520StyleGAN%26entry.906535625%3DPetr%2520Jahoda%2520and%2520Jan%2520Cech%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520method%2520to%2520transfer%2520pose%2520and%2520expression%2520between%2520face%2520images.%250AGiven%2520a%2520source%2520and%2520target%2520face%2520portrait%252C%2520the%2520model%2520produces%2520an%2520output%2520image%2520in%250Awhich%2520the%2520pose%2520and%2520expression%2520of%2520the%2520source%2520face%2520image%2520are%2520transferred%2520onto%2520the%250Atarget%2520identity.%2520The%2520architecture%2520consists%2520of%2520two%2520encoders%2520and%2520a%2520mapping%250Anetwork%2520that%2520projects%2520the%2520two%2520inputs%2520into%2520the%2520latent%2520space%2520of%2520StyleGAN2%252C%2520which%250Afinally%2520generates%2520the%2520output.%2520The%2520training%2520is%2520self-supervised%2520from%2520video%250Asequences%2520of%2520many%2520individuals.%2520Manual%2520labeling%2520is%2520not%2520required.%2520Our%2520model%250Aenables%2520the%2520synthesis%2520of%2520random%2520identities%2520with%2520controllable%2520pose%2520and%250Aexpression.%2520Close-to-real-time%2520performance%2520is%2520achieved.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13021v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pose%20and%20Facial%20Expression%20Transfer%20by%20using%20StyleGAN&entry.906535625=Petr%20Jahoda%20and%20Jan%20Cech&entry.1292438233=%20%20We%20propose%20a%20method%20to%20transfer%20pose%20and%20expression%20between%20face%20images.%0AGiven%20a%20source%20and%20target%20face%20portrait%2C%20the%20model%20produces%20an%20output%20image%20in%0Awhich%20the%20pose%20and%20expression%20of%20the%20source%20face%20image%20are%20transferred%20onto%20the%0Atarget%20identity.%20The%20architecture%20consists%20of%20two%20encoders%20and%20a%20mapping%0Anetwork%20that%20projects%20the%20two%20inputs%20into%20the%20latent%20space%20of%20StyleGAN2%2C%20which%0Afinally%20generates%20the%20output.%20The%20training%20is%20self-supervised%20from%20video%0Asequences%20of%20many%20individuals.%20Manual%20labeling%20is%20not%20required.%20Our%20model%0Aenables%20the%20synthesis%20of%20random%20identities%20with%20controllable%20pose%20and%0Aexpression.%20Close-to-real-time%20performance%20is%20achieved.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13021v1&entry.124074799=Read"},
{"title": "Transferable Foundation Models for Geometric Tasks on Point Cloud\n  Representations: Geometric Neural Operators", "author": "Blaine Quackenbush and Paul J. Atzberger", "abstract": "  We introduce methods for obtaining pretrained Geometric Neural Operators\n(GNPs) that can serve as basal foundation models for use in obtaining geometric\nfeatures. These can be used within data processing pipelines for machine\nlearning tasks and numerical methods. We show how our GNPs can be trained to\nlearn robust latent representations for the differential geometry of\npoint-clouds to provide estimates of metric, curvature, and other shape-related\nfeatures. We demonstrate how our pre-trained GNPs can be used (i) to estimate\nthe geometric properties of surfaces of arbitrary shape and topologies with\nrobustness in the presence of noise, (ii) to approximate solutions of geometric\npartial differential equations (PDEs) on manifolds, and (iii) to solve\nequations for shape deformations such as curvature driven flows. We release\ncodes and weights for using GNPs in the package geo_neural_op. This allows for\nincorporating our pre-trained GNPs as components for reuse within existing and\nnew data processing pipelines. The GNPs also can be used as part of numerical\nsolvers involving geometry or as part of methods for performing inference and\nother geometric tasks.\n", "link": "http://arxiv.org/abs/2503.04649v2", "date": "2025-04-17", "relevancy": 2.7841, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5708}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5589}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transferable%20Foundation%20Models%20for%20Geometric%20Tasks%20on%20Point%20Cloud%0A%20%20Representations%3A%20Geometric%20Neural%20Operators&body=Title%3A%20Transferable%20Foundation%20Models%20for%20Geometric%20Tasks%20on%20Point%20Cloud%0A%20%20Representations%3A%20Geometric%20Neural%20Operators%0AAuthor%3A%20Blaine%20Quackenbush%20and%20Paul%20J.%20Atzberger%0AAbstract%3A%20%20%20We%20introduce%20methods%20for%20obtaining%20pretrained%20Geometric%20Neural%20Operators%0A%28GNPs%29%20that%20can%20serve%20as%20basal%20foundation%20models%20for%20use%20in%20obtaining%20geometric%0Afeatures.%20These%20can%20be%20used%20within%20data%20processing%20pipelines%20for%20machine%0Alearning%20tasks%20and%20numerical%20methods.%20We%20show%20how%20our%20GNPs%20can%20be%20trained%20to%0Alearn%20robust%20latent%20representations%20for%20the%20differential%20geometry%20of%0Apoint-clouds%20to%20provide%20estimates%20of%20metric%2C%20curvature%2C%20and%20other%20shape-related%0Afeatures.%20We%20demonstrate%20how%20our%20pre-trained%20GNPs%20can%20be%20used%20%28i%29%20to%20estimate%0Athe%20geometric%20properties%20of%20surfaces%20of%20arbitrary%20shape%20and%20topologies%20with%0Arobustness%20in%20the%20presence%20of%20noise%2C%20%28ii%29%20to%20approximate%20solutions%20of%20geometric%0Apartial%20differential%20equations%20%28PDEs%29%20on%20manifolds%2C%20and%20%28iii%29%20to%20solve%0Aequations%20for%20shape%20deformations%20such%20as%20curvature%20driven%20flows.%20We%20release%0Acodes%20and%20weights%20for%20using%20GNPs%20in%20the%20package%20geo_neural_op.%20This%20allows%20for%0Aincorporating%20our%20pre-trained%20GNPs%20as%20components%20for%20reuse%20within%20existing%20and%0Anew%20data%20processing%20pipelines.%20The%20GNPs%20also%20can%20be%20used%20as%20part%20of%20numerical%0Asolvers%20involving%20geometry%20or%20as%20part%20of%20methods%20for%20performing%20inference%20and%0Aother%20geometric%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.04649v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransferable%2520Foundation%2520Models%2520for%2520Geometric%2520Tasks%2520on%2520Point%2520Cloud%250A%2520%2520Representations%253A%2520Geometric%2520Neural%2520Operators%26entry.906535625%3DBlaine%2520Quackenbush%2520and%2520Paul%2520J.%2520Atzberger%26entry.1292438233%3D%2520%2520We%2520introduce%2520methods%2520for%2520obtaining%2520pretrained%2520Geometric%2520Neural%2520Operators%250A%2528GNPs%2529%2520that%2520can%2520serve%2520as%2520basal%2520foundation%2520models%2520for%2520use%2520in%2520obtaining%2520geometric%250Afeatures.%2520These%2520can%2520be%2520used%2520within%2520data%2520processing%2520pipelines%2520for%2520machine%250Alearning%2520tasks%2520and%2520numerical%2520methods.%2520We%2520show%2520how%2520our%2520GNPs%2520can%2520be%2520trained%2520to%250Alearn%2520robust%2520latent%2520representations%2520for%2520the%2520differential%2520geometry%2520of%250Apoint-clouds%2520to%2520provide%2520estimates%2520of%2520metric%252C%2520curvature%252C%2520and%2520other%2520shape-related%250Afeatures.%2520We%2520demonstrate%2520how%2520our%2520pre-trained%2520GNPs%2520can%2520be%2520used%2520%2528i%2529%2520to%2520estimate%250Athe%2520geometric%2520properties%2520of%2520surfaces%2520of%2520arbitrary%2520shape%2520and%2520topologies%2520with%250Arobustness%2520in%2520the%2520presence%2520of%2520noise%252C%2520%2528ii%2529%2520to%2520approximate%2520solutions%2520of%2520geometric%250Apartial%2520differential%2520equations%2520%2528PDEs%2529%2520on%2520manifolds%252C%2520and%2520%2528iii%2529%2520to%2520solve%250Aequations%2520for%2520shape%2520deformations%2520such%2520as%2520curvature%2520driven%2520flows.%2520We%2520release%250Acodes%2520and%2520weights%2520for%2520using%2520GNPs%2520in%2520the%2520package%2520geo_neural_op.%2520This%2520allows%2520for%250Aincorporating%2520our%2520pre-trained%2520GNPs%2520as%2520components%2520for%2520reuse%2520within%2520existing%2520and%250Anew%2520data%2520processing%2520pipelines.%2520The%2520GNPs%2520also%2520can%2520be%2520used%2520as%2520part%2520of%2520numerical%250Asolvers%2520involving%2520geometry%2520or%2520as%2520part%2520of%2520methods%2520for%2520performing%2520inference%2520and%250Aother%2520geometric%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.04649v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transferable%20Foundation%20Models%20for%20Geometric%20Tasks%20on%20Point%20Cloud%0A%20%20Representations%3A%20Geometric%20Neural%20Operators&entry.906535625=Blaine%20Quackenbush%20and%20Paul%20J.%20Atzberger&entry.1292438233=%20%20We%20introduce%20methods%20for%20obtaining%20pretrained%20Geometric%20Neural%20Operators%0A%28GNPs%29%20that%20can%20serve%20as%20basal%20foundation%20models%20for%20use%20in%20obtaining%20geometric%0Afeatures.%20These%20can%20be%20used%20within%20data%20processing%20pipelines%20for%20machine%0Alearning%20tasks%20and%20numerical%20methods.%20We%20show%20how%20our%20GNPs%20can%20be%20trained%20to%0Alearn%20robust%20latent%20representations%20for%20the%20differential%20geometry%20of%0Apoint-clouds%20to%20provide%20estimates%20of%20metric%2C%20curvature%2C%20and%20other%20shape-related%0Afeatures.%20We%20demonstrate%20how%20our%20pre-trained%20GNPs%20can%20be%20used%20%28i%29%20to%20estimate%0Athe%20geometric%20properties%20of%20surfaces%20of%20arbitrary%20shape%20and%20topologies%20with%0Arobustness%20in%20the%20presence%20of%20noise%2C%20%28ii%29%20to%20approximate%20solutions%20of%20geometric%0Apartial%20differential%20equations%20%28PDEs%29%20on%20manifolds%2C%20and%20%28iii%29%20to%20solve%0Aequations%20for%20shape%20deformations%20such%20as%20curvature%20driven%20flows.%20We%20release%0Acodes%20and%20weights%20for%20using%20GNPs%20in%20the%20package%20geo_neural_op.%20This%20allows%20for%0Aincorporating%20our%20pre-trained%20GNPs%20as%20components%20for%20reuse%20within%20existing%20and%0Anew%20data%20processing%20pipelines.%20The%20GNPs%20also%20can%20be%20used%20as%20part%20of%20numerical%0Asolvers%20involving%20geometry%20or%20as%20part%20of%20methods%20for%20performing%20inference%20and%0Aother%20geometric%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.04649v2&entry.124074799=Read"},
{"title": "Analysis of Deep Learning-Based Colorization and Super-Resolution\n  Techniques for Lidar Imagery", "author": "Sier Ha and Honghao Du and Xianjia Yu and Jian Song and Tomi Westerlund", "abstract": "  Modern lidar systems can produce not only dense point clouds but also 360\ndegrees low-resolution images. This advancement facilitates the application of\ndeep learning (DL) techniques initially developed for conventional RGB cameras\nand simplifies fusion of point cloud data and images without complex processes\nlike lidar-camera calibration. Compared to RGB images from traditional cameras,\nlidar-generated images show greater robustness under low-light and harsh\nconditions, such as foggy weather. However, these images typically have lower\nresolution and often appear overly dark. While various studies have explored\nDL-based computer vision tasks such as object detection, segmentation, and\nkeypoint detection on lidar imagery, other potentially valuable techniques\nremain underexplored. This paper provides a comprehensive review and\nqualitative analysis of DL-based colorization and super-resolution methods\napplied to lidar imagery. Additionally, we assess the computational performance\nof these approaches, offering insights into their suitability for downstream\nrobotic and autonomous system applications like odometry and 3D reconstruction.\n", "link": "http://arxiv.org/abs/2409.11532v2", "date": "2025-04-16", "relevancy": 2.7622, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.556}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.556}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analysis%20of%20Deep%20Learning-Based%20Colorization%20and%20Super-Resolution%0A%20%20Techniques%20for%20Lidar%20Imagery&body=Title%3A%20Analysis%20of%20Deep%20Learning-Based%20Colorization%20and%20Super-Resolution%0A%20%20Techniques%20for%20Lidar%20Imagery%0AAuthor%3A%20Sier%20Ha%20and%20Honghao%20Du%20and%20Xianjia%20Yu%20and%20Jian%20Song%20and%20Tomi%20Westerlund%0AAbstract%3A%20%20%20Modern%20lidar%20systems%20can%20produce%20not%20only%20dense%20point%20clouds%20but%20also%20360%0Adegrees%20low-resolution%20images.%20This%20advancement%20facilitates%20the%20application%20of%0Adeep%20learning%20%28DL%29%20techniques%20initially%20developed%20for%20conventional%20RGB%20cameras%0Aand%20simplifies%20fusion%20of%20point%20cloud%20data%20and%20images%20without%20complex%20processes%0Alike%20lidar-camera%20calibration.%20Compared%20to%20RGB%20images%20from%20traditional%20cameras%2C%0Alidar-generated%20images%20show%20greater%20robustness%20under%20low-light%20and%20harsh%0Aconditions%2C%20such%20as%20foggy%20weather.%20However%2C%20these%20images%20typically%20have%20lower%0Aresolution%20and%20often%20appear%20overly%20dark.%20While%20various%20studies%20have%20explored%0ADL-based%20computer%20vision%20tasks%20such%20as%20object%20detection%2C%20segmentation%2C%20and%0Akeypoint%20detection%20on%20lidar%20imagery%2C%20other%20potentially%20valuable%20techniques%0Aremain%20underexplored.%20This%20paper%20provides%20a%20comprehensive%20review%20and%0Aqualitative%20analysis%20of%20DL-based%20colorization%20and%20super-resolution%20methods%0Aapplied%20to%20lidar%20imagery.%20Additionally%2C%20we%20assess%20the%20computational%20performance%0Aof%20these%20approaches%2C%20offering%20insights%20into%20their%20suitability%20for%20downstream%0Arobotic%20and%20autonomous%20system%20applications%20like%20odometry%20and%203D%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11532v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalysis%2520of%2520Deep%2520Learning-Based%2520Colorization%2520and%2520Super-Resolution%250A%2520%2520Techniques%2520for%2520Lidar%2520Imagery%26entry.906535625%3DSier%2520Ha%2520and%2520Honghao%2520Du%2520and%2520Xianjia%2520Yu%2520and%2520Jian%2520Song%2520and%2520Tomi%2520Westerlund%26entry.1292438233%3D%2520%2520Modern%2520lidar%2520systems%2520can%2520produce%2520not%2520only%2520dense%2520point%2520clouds%2520but%2520also%2520360%250Adegrees%2520low-resolution%2520images.%2520This%2520advancement%2520facilitates%2520the%2520application%2520of%250Adeep%2520learning%2520%2528DL%2529%2520techniques%2520initially%2520developed%2520for%2520conventional%2520RGB%2520cameras%250Aand%2520simplifies%2520fusion%2520of%2520point%2520cloud%2520data%2520and%2520images%2520without%2520complex%2520processes%250Alike%2520lidar-camera%2520calibration.%2520Compared%2520to%2520RGB%2520images%2520from%2520traditional%2520cameras%252C%250Alidar-generated%2520images%2520show%2520greater%2520robustness%2520under%2520low-light%2520and%2520harsh%250Aconditions%252C%2520such%2520as%2520foggy%2520weather.%2520However%252C%2520these%2520images%2520typically%2520have%2520lower%250Aresolution%2520and%2520often%2520appear%2520overly%2520dark.%2520While%2520various%2520studies%2520have%2520explored%250ADL-based%2520computer%2520vision%2520tasks%2520such%2520as%2520object%2520detection%252C%2520segmentation%252C%2520and%250Akeypoint%2520detection%2520on%2520lidar%2520imagery%252C%2520other%2520potentially%2520valuable%2520techniques%250Aremain%2520underexplored.%2520This%2520paper%2520provides%2520a%2520comprehensive%2520review%2520and%250Aqualitative%2520analysis%2520of%2520DL-based%2520colorization%2520and%2520super-resolution%2520methods%250Aapplied%2520to%2520lidar%2520imagery.%2520Additionally%252C%2520we%2520assess%2520the%2520computational%2520performance%250Aof%2520these%2520approaches%252C%2520offering%2520insights%2520into%2520their%2520suitability%2520for%2520downstream%250Arobotic%2520and%2520autonomous%2520system%2520applications%2520like%2520odometry%2520and%25203D%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11532v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analysis%20of%20Deep%20Learning-Based%20Colorization%20and%20Super-Resolution%0A%20%20Techniques%20for%20Lidar%20Imagery&entry.906535625=Sier%20Ha%20and%20Honghao%20Du%20and%20Xianjia%20Yu%20and%20Jian%20Song%20and%20Tomi%20Westerlund&entry.1292438233=%20%20Modern%20lidar%20systems%20can%20produce%20not%20only%20dense%20point%20clouds%20but%20also%20360%0Adegrees%20low-resolution%20images.%20This%20advancement%20facilitates%20the%20application%20of%0Adeep%20learning%20%28DL%29%20techniques%20initially%20developed%20for%20conventional%20RGB%20cameras%0Aand%20simplifies%20fusion%20of%20point%20cloud%20data%20and%20images%20without%20complex%20processes%0Alike%20lidar-camera%20calibration.%20Compared%20to%20RGB%20images%20from%20traditional%20cameras%2C%0Alidar-generated%20images%20show%20greater%20robustness%20under%20low-light%20and%20harsh%0Aconditions%2C%20such%20as%20foggy%20weather.%20However%2C%20these%20images%20typically%20have%20lower%0Aresolution%20and%20often%20appear%20overly%20dark.%20While%20various%20studies%20have%20explored%0ADL-based%20computer%20vision%20tasks%20such%20as%20object%20detection%2C%20segmentation%2C%20and%0Akeypoint%20detection%20on%20lidar%20imagery%2C%20other%20potentially%20valuable%20techniques%0Aremain%20underexplored.%20This%20paper%20provides%20a%20comprehensive%20review%20and%0Aqualitative%20analysis%20of%20DL-based%20colorization%20and%20super-resolution%20methods%0Aapplied%20to%20lidar%20imagery.%20Additionally%2C%20we%20assess%20the%20computational%20performance%0Aof%20these%20approaches%2C%20offering%20insights%20into%20their%20suitability%20for%20downstream%0Arobotic%20and%20autonomous%20system%20applications%20like%20odometry%20and%203D%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11532v2&entry.124074799=Read"},
{"title": "TongUI: Building Generalized GUI Agents by Learning from Multimodal Web\n  Tutorials", "author": "Bofei Zhang and Zirui Shang and Zhi Gao and Wang Zhang and Rui Xie and Xiaojian Ma and Tao Yuan and Xinxiao Wu and Song-Chun Zhu and Qing Li", "abstract": "  Building Graphical User Interface (GUI) agents is a promising research\ndirection, which simulates human interaction with computers or mobile phones to\nperform diverse GUI tasks. However, a major challenge in developing generalized\nGUI agents is the lack of sufficient trajectory data across various operating\nsystems and applications, mainly due to the high cost of manual annotations. In\nthis paper, we propose the TongUI framework that builds generalized GUI agents\nby learning from rich multimodal web tutorials. Concretely, we crawl and\nprocess online GUI tutorials (such as videos and articles) into GUI agent\ntrajectory data, through which we produce the GUI-Net dataset containing 143K\ntrajectory data across five operating systems and more than 200 applications.\nWe develop the TongUI agent by fine-tuning Qwen2.5-VL-3B/7B models on GUI-Net,\nwhich show remarkable performance improvements on commonly used grounding and\nnavigation benchmarks, outperforming baseline agents about 10\\% on multiple\nbenchmarks, showing the effectiveness of the GUI-Net dataset and underscoring\nthe significance of our TongUI framework. We will fully open-source the code,\nthe GUI-Net dataset, and the trained models soon.\n", "link": "http://arxiv.org/abs/2504.12679v1", "date": "2025-04-17", "relevancy": 2.7595, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5679}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5579}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5298}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TongUI%3A%20Building%20Generalized%20GUI%20Agents%20by%20Learning%20from%20Multimodal%20Web%0A%20%20Tutorials&body=Title%3A%20TongUI%3A%20Building%20Generalized%20GUI%20Agents%20by%20Learning%20from%20Multimodal%20Web%0A%20%20Tutorials%0AAuthor%3A%20Bofei%20Zhang%20and%20Zirui%20Shang%20and%20Zhi%20Gao%20and%20Wang%20Zhang%20and%20Rui%20Xie%20and%20Xiaojian%20Ma%20and%20Tao%20Yuan%20and%20Xinxiao%20Wu%20and%20Song-Chun%20Zhu%20and%20Qing%20Li%0AAbstract%3A%20%20%20Building%20Graphical%20User%20Interface%20%28GUI%29%20agents%20is%20a%20promising%20research%0Adirection%2C%20which%20simulates%20human%20interaction%20with%20computers%20or%20mobile%20phones%20to%0Aperform%20diverse%20GUI%20tasks.%20However%2C%20a%20major%20challenge%20in%20developing%20generalized%0AGUI%20agents%20is%20the%20lack%20of%20sufficient%20trajectory%20data%20across%20various%20operating%0Asystems%20and%20applications%2C%20mainly%20due%20to%20the%20high%20cost%20of%20manual%20annotations.%20In%0Athis%20paper%2C%20we%20propose%20the%20TongUI%20framework%20that%20builds%20generalized%20GUI%20agents%0Aby%20learning%20from%20rich%20multimodal%20web%20tutorials.%20Concretely%2C%20we%20crawl%20and%0Aprocess%20online%20GUI%20tutorials%20%28such%20as%20videos%20and%20articles%29%20into%20GUI%20agent%0Atrajectory%20data%2C%20through%20which%20we%20produce%20the%20GUI-Net%20dataset%20containing%20143K%0Atrajectory%20data%20across%20five%20operating%20systems%20and%20more%20than%20200%20applications.%0AWe%20develop%20the%20TongUI%20agent%20by%20fine-tuning%20Qwen2.5-VL-3B/7B%20models%20on%20GUI-Net%2C%0Awhich%20show%20remarkable%20performance%20improvements%20on%20commonly%20used%20grounding%20and%0Anavigation%20benchmarks%2C%20outperforming%20baseline%20agents%20about%2010%5C%25%20on%20multiple%0Abenchmarks%2C%20showing%20the%20effectiveness%20of%20the%20GUI-Net%20dataset%20and%20underscoring%0Athe%20significance%20of%20our%20TongUI%20framework.%20We%20will%20fully%20open-source%20the%20code%2C%0Athe%20GUI-Net%20dataset%2C%20and%20the%20trained%20models%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12679v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTongUI%253A%2520Building%2520Generalized%2520GUI%2520Agents%2520by%2520Learning%2520from%2520Multimodal%2520Web%250A%2520%2520Tutorials%26entry.906535625%3DBofei%2520Zhang%2520and%2520Zirui%2520Shang%2520and%2520Zhi%2520Gao%2520and%2520Wang%2520Zhang%2520and%2520Rui%2520Xie%2520and%2520Xiaojian%2520Ma%2520and%2520Tao%2520Yuan%2520and%2520Xinxiao%2520Wu%2520and%2520Song-Chun%2520Zhu%2520and%2520Qing%2520Li%26entry.1292438233%3D%2520%2520Building%2520Graphical%2520User%2520Interface%2520%2528GUI%2529%2520agents%2520is%2520a%2520promising%2520research%250Adirection%252C%2520which%2520simulates%2520human%2520interaction%2520with%2520computers%2520or%2520mobile%2520phones%2520to%250Aperform%2520diverse%2520GUI%2520tasks.%2520However%252C%2520a%2520major%2520challenge%2520in%2520developing%2520generalized%250AGUI%2520agents%2520is%2520the%2520lack%2520of%2520sufficient%2520trajectory%2520data%2520across%2520various%2520operating%250Asystems%2520and%2520applications%252C%2520mainly%2520due%2520to%2520the%2520high%2520cost%2520of%2520manual%2520annotations.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520the%2520TongUI%2520framework%2520that%2520builds%2520generalized%2520GUI%2520agents%250Aby%2520learning%2520from%2520rich%2520multimodal%2520web%2520tutorials.%2520Concretely%252C%2520we%2520crawl%2520and%250Aprocess%2520online%2520GUI%2520tutorials%2520%2528such%2520as%2520videos%2520and%2520articles%2529%2520into%2520GUI%2520agent%250Atrajectory%2520data%252C%2520through%2520which%2520we%2520produce%2520the%2520GUI-Net%2520dataset%2520containing%2520143K%250Atrajectory%2520data%2520across%2520five%2520operating%2520systems%2520and%2520more%2520than%2520200%2520applications.%250AWe%2520develop%2520the%2520TongUI%2520agent%2520by%2520fine-tuning%2520Qwen2.5-VL-3B/7B%2520models%2520on%2520GUI-Net%252C%250Awhich%2520show%2520remarkable%2520performance%2520improvements%2520on%2520commonly%2520used%2520grounding%2520and%250Anavigation%2520benchmarks%252C%2520outperforming%2520baseline%2520agents%2520about%252010%255C%2525%2520on%2520multiple%250Abenchmarks%252C%2520showing%2520the%2520effectiveness%2520of%2520the%2520GUI-Net%2520dataset%2520and%2520underscoring%250Athe%2520significance%2520of%2520our%2520TongUI%2520framework.%2520We%2520will%2520fully%2520open-source%2520the%2520code%252C%250Athe%2520GUI-Net%2520dataset%252C%2520and%2520the%2520trained%2520models%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12679v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TongUI%3A%20Building%20Generalized%20GUI%20Agents%20by%20Learning%20from%20Multimodal%20Web%0A%20%20Tutorials&entry.906535625=Bofei%20Zhang%20and%20Zirui%20Shang%20and%20Zhi%20Gao%20and%20Wang%20Zhang%20and%20Rui%20Xie%20and%20Xiaojian%20Ma%20and%20Tao%20Yuan%20and%20Xinxiao%20Wu%20and%20Song-Chun%20Zhu%20and%20Qing%20Li&entry.1292438233=%20%20Building%20Graphical%20User%20Interface%20%28GUI%29%20agents%20is%20a%20promising%20research%0Adirection%2C%20which%20simulates%20human%20interaction%20with%20computers%20or%20mobile%20phones%20to%0Aperform%20diverse%20GUI%20tasks.%20However%2C%20a%20major%20challenge%20in%20developing%20generalized%0AGUI%20agents%20is%20the%20lack%20of%20sufficient%20trajectory%20data%20across%20various%20operating%0Asystems%20and%20applications%2C%20mainly%20due%20to%20the%20high%20cost%20of%20manual%20annotations.%20In%0Athis%20paper%2C%20we%20propose%20the%20TongUI%20framework%20that%20builds%20generalized%20GUI%20agents%0Aby%20learning%20from%20rich%20multimodal%20web%20tutorials.%20Concretely%2C%20we%20crawl%20and%0Aprocess%20online%20GUI%20tutorials%20%28such%20as%20videos%20and%20articles%29%20into%20GUI%20agent%0Atrajectory%20data%2C%20through%20which%20we%20produce%20the%20GUI-Net%20dataset%20containing%20143K%0Atrajectory%20data%20across%20five%20operating%20systems%20and%20more%20than%20200%20applications.%0AWe%20develop%20the%20TongUI%20agent%20by%20fine-tuning%20Qwen2.5-VL-3B/7B%20models%20on%20GUI-Net%2C%0Awhich%20show%20remarkable%20performance%20improvements%20on%20commonly%20used%20grounding%20and%0Anavigation%20benchmarks%2C%20outperforming%20baseline%20agents%20about%2010%5C%25%20on%20multiple%0Abenchmarks%2C%20showing%20the%20effectiveness%20of%20the%20GUI-Net%20dataset%20and%20underscoring%0Athe%20significance%20of%20our%20TongUI%20framework.%20We%20will%20fully%20open-source%20the%20code%2C%0Athe%20GUI-Net%20dataset%2C%20and%20the%20trained%20models%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12679v1&entry.124074799=Read"},
{"title": "unPIC: A Geometric Multiview Prior for Image to 3D Synthesis", "author": "Rishabh Kabra and Drew A. Hudson and Sjoerd van Steenkiste and Joao Carreira and Niloy J. Mitra", "abstract": "  We introduce a hierarchical probabilistic approach to go from a 2D image to\nmultiview 3D: a diffusion \"prior\" predicts the unseen 3D geometry, which then\nconditions a diffusion \"decoder\" to generate novel views of the subject. We use\na pointmap-based geometric representation to coordinate the generation of\nmultiple target views simultaneously. We construct a predictable distribution\nof geometric features per target view to enable learnability across examples,\nand generalization to arbitrary inputs images. Our modular, geometry-driven\napproach to novel-view synthesis (called \"unPIC\") beats competing baselines\nsuch as CAT3D, EscherNet, Free3D, and One-2-3-45 on held-out objects from\nObjaverseXL, as well as unseen real-world objects from Google Scanned Objects,\nAmazon Berkeley Objects, and the Digital Twin Catalog.\n", "link": "http://arxiv.org/abs/2412.10273v2", "date": "2025-04-17", "relevancy": 2.7558, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7042}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7042}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6128}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20unPIC%3A%20A%20Geometric%20Multiview%20Prior%20for%20Image%20to%203D%20Synthesis&body=Title%3A%20unPIC%3A%20A%20Geometric%20Multiview%20Prior%20for%20Image%20to%203D%20Synthesis%0AAuthor%3A%20Rishabh%20Kabra%20and%20Drew%20A.%20Hudson%20and%20Sjoerd%20van%20Steenkiste%20and%20Joao%20Carreira%20and%20Niloy%20J.%20Mitra%0AAbstract%3A%20%20%20We%20introduce%20a%20hierarchical%20probabilistic%20approach%20to%20go%20from%20a%202D%20image%20to%0Amultiview%203D%3A%20a%20diffusion%20%22prior%22%20predicts%20the%20unseen%203D%20geometry%2C%20which%20then%0Aconditions%20a%20diffusion%20%22decoder%22%20to%20generate%20novel%20views%20of%20the%20subject.%20We%20use%0Aa%20pointmap-based%20geometric%20representation%20to%20coordinate%20the%20generation%20of%0Amultiple%20target%20views%20simultaneously.%20We%20construct%20a%20predictable%20distribution%0Aof%20geometric%20features%20per%20target%20view%20to%20enable%20learnability%20across%20examples%2C%0Aand%20generalization%20to%20arbitrary%20inputs%20images.%20Our%20modular%2C%20geometry-driven%0Aapproach%20to%20novel-view%20synthesis%20%28called%20%22unPIC%22%29%20beats%20competing%20baselines%0Asuch%20as%20CAT3D%2C%20EscherNet%2C%20Free3D%2C%20and%20One-2-3-45%20on%20held-out%20objects%20from%0AObjaverseXL%2C%20as%20well%20as%20unseen%20real-world%20objects%20from%20Google%20Scanned%20Objects%2C%0AAmazon%20Berkeley%20Objects%2C%20and%20the%20Digital%20Twin%20Catalog.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10273v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DunPIC%253A%2520A%2520Geometric%2520Multiview%2520Prior%2520for%2520Image%2520to%25203D%2520Synthesis%26entry.906535625%3DRishabh%2520Kabra%2520and%2520Drew%2520A.%2520Hudson%2520and%2520Sjoerd%2520van%2520Steenkiste%2520and%2520Joao%2520Carreira%2520and%2520Niloy%2520J.%2520Mitra%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520hierarchical%2520probabilistic%2520approach%2520to%2520go%2520from%2520a%25202D%2520image%2520to%250Amultiview%25203D%253A%2520a%2520diffusion%2520%2522prior%2522%2520predicts%2520the%2520unseen%25203D%2520geometry%252C%2520which%2520then%250Aconditions%2520a%2520diffusion%2520%2522decoder%2522%2520to%2520generate%2520novel%2520views%2520of%2520the%2520subject.%2520We%2520use%250Aa%2520pointmap-based%2520geometric%2520representation%2520to%2520coordinate%2520the%2520generation%2520of%250Amultiple%2520target%2520views%2520simultaneously.%2520We%2520construct%2520a%2520predictable%2520distribution%250Aof%2520geometric%2520features%2520per%2520target%2520view%2520to%2520enable%2520learnability%2520across%2520examples%252C%250Aand%2520generalization%2520to%2520arbitrary%2520inputs%2520images.%2520Our%2520modular%252C%2520geometry-driven%250Aapproach%2520to%2520novel-view%2520synthesis%2520%2528called%2520%2522unPIC%2522%2529%2520beats%2520competing%2520baselines%250Asuch%2520as%2520CAT3D%252C%2520EscherNet%252C%2520Free3D%252C%2520and%2520One-2-3-45%2520on%2520held-out%2520objects%2520from%250AObjaverseXL%252C%2520as%2520well%2520as%2520unseen%2520real-world%2520objects%2520from%2520Google%2520Scanned%2520Objects%252C%250AAmazon%2520Berkeley%2520Objects%252C%2520and%2520the%2520Digital%2520Twin%2520Catalog.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10273v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=unPIC%3A%20A%20Geometric%20Multiview%20Prior%20for%20Image%20to%203D%20Synthesis&entry.906535625=Rishabh%20Kabra%20and%20Drew%20A.%20Hudson%20and%20Sjoerd%20van%20Steenkiste%20and%20Joao%20Carreira%20and%20Niloy%20J.%20Mitra&entry.1292438233=%20%20We%20introduce%20a%20hierarchical%20probabilistic%20approach%20to%20go%20from%20a%202D%20image%20to%0Amultiview%203D%3A%20a%20diffusion%20%22prior%22%20predicts%20the%20unseen%203D%20geometry%2C%20which%20then%0Aconditions%20a%20diffusion%20%22decoder%22%20to%20generate%20novel%20views%20of%20the%20subject.%20We%20use%0Aa%20pointmap-based%20geometric%20representation%20to%20coordinate%20the%20generation%20of%0Amultiple%20target%20views%20simultaneously.%20We%20construct%20a%20predictable%20distribution%0Aof%20geometric%20features%20per%20target%20view%20to%20enable%20learnability%20across%20examples%2C%0Aand%20generalization%20to%20arbitrary%20inputs%20images.%20Our%20modular%2C%20geometry-driven%0Aapproach%20to%20novel-view%20synthesis%20%28called%20%22unPIC%22%29%20beats%20competing%20baselines%0Asuch%20as%20CAT3D%2C%20EscherNet%2C%20Free3D%2C%20and%20One-2-3-45%20on%20held-out%20objects%20from%0AObjaverseXL%2C%20as%20well%20as%20unseen%20real-world%20objects%20from%20Google%20Scanned%20Objects%2C%0AAmazon%20Berkeley%20Objects%2C%20and%20the%20Digital%20Twin%20Catalog.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10273v2&entry.124074799=Read"},
{"title": "EchoWorld: Learning Motion-Aware World Models for Echocardiography Probe\n  Guidance", "author": "Yang Yue and Yulin Wang and Haojun Jiang and Pan Liu and Shiji Song and Gao Huang", "abstract": "  Echocardiography is crucial for cardiovascular disease detection but relies\nheavily on experienced sonographers. Echocardiography probe guidance systems,\nwhich provide real-time movement instructions for acquiring standard plane\nimages, offer a promising solution for AI-assisted or fully autonomous\nscanning. However, developing effective machine learning models for this task\nremains challenging, as they must grasp heart anatomy and the intricate\ninterplay between probe motion and visual signals. To address this, we present\nEchoWorld, a motion-aware world modeling framework for probe guidance that\nencodes anatomical knowledge and motion-induced visual dynamics, while\neffectively leveraging past visual-motion sequences to enhance guidance\nprecision. EchoWorld employs a pre-training strategy inspired by world modeling\nprinciples, where the model predicts masked anatomical regions and simulates\nthe visual outcomes of probe adjustments. Built upon this pre-trained model, we\nintroduce a motion-aware attention mechanism in the fine-tuning stage that\neffectively integrates historical visual-motion data, enabling precise and\nadaptive probe guidance. Trained on more than one million ultrasound images\nfrom over 200 routine scans, EchoWorld effectively captures key\nechocardiographic knowledge, as validated by qualitative analysis. Moreover,\nour method significantly reduces guidance errors compared to existing visual\nbackbones and guidance frameworks, excelling in both single-frame and\nsequential evaluation protocols. Code is available at\nhttps://github.com/LeapLabTHU/EchoWorld.\n", "link": "http://arxiv.org/abs/2504.13065v1", "date": "2025-04-17", "relevancy": 2.7338, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5481}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5461}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EchoWorld%3A%20Learning%20Motion-Aware%20World%20Models%20for%20Echocardiography%20Probe%0A%20%20Guidance&body=Title%3A%20EchoWorld%3A%20Learning%20Motion-Aware%20World%20Models%20for%20Echocardiography%20Probe%0A%20%20Guidance%0AAuthor%3A%20Yang%20Yue%20and%20Yulin%20Wang%20and%20Haojun%20Jiang%20and%20Pan%20Liu%20and%20Shiji%20Song%20and%20Gao%20Huang%0AAbstract%3A%20%20%20Echocardiography%20is%20crucial%20for%20cardiovascular%20disease%20detection%20but%20relies%0Aheavily%20on%20experienced%20sonographers.%20Echocardiography%20probe%20guidance%20systems%2C%0Awhich%20provide%20real-time%20movement%20instructions%20for%20acquiring%20standard%20plane%0Aimages%2C%20offer%20a%20promising%20solution%20for%20AI-assisted%20or%20fully%20autonomous%0Ascanning.%20However%2C%20developing%20effective%20machine%20learning%20models%20for%20this%20task%0Aremains%20challenging%2C%20as%20they%20must%20grasp%20heart%20anatomy%20and%20the%20intricate%0Ainterplay%20between%20probe%20motion%20and%20visual%20signals.%20To%20address%20this%2C%20we%20present%0AEchoWorld%2C%20a%20motion-aware%20world%20modeling%20framework%20for%20probe%20guidance%20that%0Aencodes%20anatomical%20knowledge%20and%20motion-induced%20visual%20dynamics%2C%20while%0Aeffectively%20leveraging%20past%20visual-motion%20sequences%20to%20enhance%20guidance%0Aprecision.%20EchoWorld%20employs%20a%20pre-training%20strategy%20inspired%20by%20world%20modeling%0Aprinciples%2C%20where%20the%20model%20predicts%20masked%20anatomical%20regions%20and%20simulates%0Athe%20visual%20outcomes%20of%20probe%20adjustments.%20Built%20upon%20this%20pre-trained%20model%2C%20we%0Aintroduce%20a%20motion-aware%20attention%20mechanism%20in%20the%20fine-tuning%20stage%20that%0Aeffectively%20integrates%20historical%20visual-motion%20data%2C%20enabling%20precise%20and%0Aadaptive%20probe%20guidance.%20Trained%20on%20more%20than%20one%20million%20ultrasound%20images%0Afrom%20over%20200%20routine%20scans%2C%20EchoWorld%20effectively%20captures%20key%0Aechocardiographic%20knowledge%2C%20as%20validated%20by%20qualitative%20analysis.%20Moreover%2C%0Aour%20method%20significantly%20reduces%20guidance%20errors%20compared%20to%20existing%20visual%0Abackbones%20and%20guidance%20frameworks%2C%20excelling%20in%20both%20single-frame%20and%0Asequential%20evaluation%20protocols.%20Code%20is%20available%20at%0Ahttps%3A//github.com/LeapLabTHU/EchoWorld.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13065v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEchoWorld%253A%2520Learning%2520Motion-Aware%2520World%2520Models%2520for%2520Echocardiography%2520Probe%250A%2520%2520Guidance%26entry.906535625%3DYang%2520Yue%2520and%2520Yulin%2520Wang%2520and%2520Haojun%2520Jiang%2520and%2520Pan%2520Liu%2520and%2520Shiji%2520Song%2520and%2520Gao%2520Huang%26entry.1292438233%3D%2520%2520Echocardiography%2520is%2520crucial%2520for%2520cardiovascular%2520disease%2520detection%2520but%2520relies%250Aheavily%2520on%2520experienced%2520sonographers.%2520Echocardiography%2520probe%2520guidance%2520systems%252C%250Awhich%2520provide%2520real-time%2520movement%2520instructions%2520for%2520acquiring%2520standard%2520plane%250Aimages%252C%2520offer%2520a%2520promising%2520solution%2520for%2520AI-assisted%2520or%2520fully%2520autonomous%250Ascanning.%2520However%252C%2520developing%2520effective%2520machine%2520learning%2520models%2520for%2520this%2520task%250Aremains%2520challenging%252C%2520as%2520they%2520must%2520grasp%2520heart%2520anatomy%2520and%2520the%2520intricate%250Ainterplay%2520between%2520probe%2520motion%2520and%2520visual%2520signals.%2520To%2520address%2520this%252C%2520we%2520present%250AEchoWorld%252C%2520a%2520motion-aware%2520world%2520modeling%2520framework%2520for%2520probe%2520guidance%2520that%250Aencodes%2520anatomical%2520knowledge%2520and%2520motion-induced%2520visual%2520dynamics%252C%2520while%250Aeffectively%2520leveraging%2520past%2520visual-motion%2520sequences%2520to%2520enhance%2520guidance%250Aprecision.%2520EchoWorld%2520employs%2520a%2520pre-training%2520strategy%2520inspired%2520by%2520world%2520modeling%250Aprinciples%252C%2520where%2520the%2520model%2520predicts%2520masked%2520anatomical%2520regions%2520and%2520simulates%250Athe%2520visual%2520outcomes%2520of%2520probe%2520adjustments.%2520Built%2520upon%2520this%2520pre-trained%2520model%252C%2520we%250Aintroduce%2520a%2520motion-aware%2520attention%2520mechanism%2520in%2520the%2520fine-tuning%2520stage%2520that%250Aeffectively%2520integrates%2520historical%2520visual-motion%2520data%252C%2520enabling%2520precise%2520and%250Aadaptive%2520probe%2520guidance.%2520Trained%2520on%2520more%2520than%2520one%2520million%2520ultrasound%2520images%250Afrom%2520over%2520200%2520routine%2520scans%252C%2520EchoWorld%2520effectively%2520captures%2520key%250Aechocardiographic%2520knowledge%252C%2520as%2520validated%2520by%2520qualitative%2520analysis.%2520Moreover%252C%250Aour%2520method%2520significantly%2520reduces%2520guidance%2520errors%2520compared%2520to%2520existing%2520visual%250Abackbones%2520and%2520guidance%2520frameworks%252C%2520excelling%2520in%2520both%2520single-frame%2520and%250Asequential%2520evaluation%2520protocols.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/LeapLabTHU/EchoWorld.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13065v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EchoWorld%3A%20Learning%20Motion-Aware%20World%20Models%20for%20Echocardiography%20Probe%0A%20%20Guidance&entry.906535625=Yang%20Yue%20and%20Yulin%20Wang%20and%20Haojun%20Jiang%20and%20Pan%20Liu%20and%20Shiji%20Song%20and%20Gao%20Huang&entry.1292438233=%20%20Echocardiography%20is%20crucial%20for%20cardiovascular%20disease%20detection%20but%20relies%0Aheavily%20on%20experienced%20sonographers.%20Echocardiography%20probe%20guidance%20systems%2C%0Awhich%20provide%20real-time%20movement%20instructions%20for%20acquiring%20standard%20plane%0Aimages%2C%20offer%20a%20promising%20solution%20for%20AI-assisted%20or%20fully%20autonomous%0Ascanning.%20However%2C%20developing%20effective%20machine%20learning%20models%20for%20this%20task%0Aremains%20challenging%2C%20as%20they%20must%20grasp%20heart%20anatomy%20and%20the%20intricate%0Ainterplay%20between%20probe%20motion%20and%20visual%20signals.%20To%20address%20this%2C%20we%20present%0AEchoWorld%2C%20a%20motion-aware%20world%20modeling%20framework%20for%20probe%20guidance%20that%0Aencodes%20anatomical%20knowledge%20and%20motion-induced%20visual%20dynamics%2C%20while%0Aeffectively%20leveraging%20past%20visual-motion%20sequences%20to%20enhance%20guidance%0Aprecision.%20EchoWorld%20employs%20a%20pre-training%20strategy%20inspired%20by%20world%20modeling%0Aprinciples%2C%20where%20the%20model%20predicts%20masked%20anatomical%20regions%20and%20simulates%0Athe%20visual%20outcomes%20of%20probe%20adjustments.%20Built%20upon%20this%20pre-trained%20model%2C%20we%0Aintroduce%20a%20motion-aware%20attention%20mechanism%20in%20the%20fine-tuning%20stage%20that%0Aeffectively%20integrates%20historical%20visual-motion%20data%2C%20enabling%20precise%20and%0Aadaptive%20probe%20guidance.%20Trained%20on%20more%20than%20one%20million%20ultrasound%20images%0Afrom%20over%20200%20routine%20scans%2C%20EchoWorld%20effectively%20captures%20key%0Aechocardiographic%20knowledge%2C%20as%20validated%20by%20qualitative%20analysis.%20Moreover%2C%0Aour%20method%20significantly%20reduces%20guidance%20errors%20compared%20to%20existing%20visual%0Abackbones%20and%20guidance%20frameworks%2C%20excelling%20in%20both%20single-frame%20and%0Asequential%20evaluation%20protocols.%20Code%20is%20available%20at%0Ahttps%3A//github.com/LeapLabTHU/EchoWorld.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13065v1&entry.124074799=Read"},
{"title": "Geolocation with Real Human Gameplay Data: A Large-Scale Dataset and\n  Human-Like Reasoning Framework", "author": "Zirui Song and Jingpu Yang and Yuan Huang and Jonathan Tonglet and Zeyu Zhang and Tao Cheng and Meng Fang and Iryna Gurevych and Xiuying Chen", "abstract": "  Geolocation, the task of identifying an image's location, requires complex\nreasoning and is crucial for navigation, monitoring, and cultural preservation.\nHowever, current methods often produce coarse, imprecise, and non-interpretable\nlocalization. A major challenge lies in the quality and scale of existing\ngeolocation datasets. These datasets are typically small-scale and\nautomatically constructed, leading to noisy data and inconsistent task\ndifficulty, with images that either reveal answers too easily or lack\nsufficient clues for reliable inference. To address these challenges, we\nintroduce a comprehensive geolocation framework with three key components:\nGeoComp, a large-scale dataset; GeoCoT, a novel reasoning method; and GeoEval,\nan evaluation metric, collectively designed to address critical challenges and\ndrive advancements in geolocation research. At the core of this framework is\nGeoComp (Geolocation Competition Dataset), a large-scale dataset collected from\na geolocation game platform involving 740K users over two years. It comprises\n25 million entries of metadata and 3 million geo-tagged locations spanning much\nof the globe, with each location annotated thousands to tens of thousands of\ntimes by human users. The dataset offers diverse difficulty levels for detailed\nanalysis and highlights key gaps in current models. Building on this dataset,\nwe propose Geographical Chain-of-Thought (GeoCoT), a novel multi-step reasoning\nframework designed to enhance the reasoning capabilities of Large Vision Models\n(LVMs) in geolocation tasks. GeoCoT improves performance by integrating\ncontextual and spatial cues through a multi-step process that mimics human\ngeolocation reasoning. Finally, using the GeoEval metric, we demonstrate that\nGeoCoT significantly boosts geolocation accuracy by up to 25% while enhancing\ninterpretability.\n", "link": "http://arxiv.org/abs/2502.13759v2", "date": "2025-04-16", "relevancy": 2.717, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5611}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5346}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5346}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geolocation%20with%20Real%20Human%20Gameplay%20Data%3A%20A%20Large-Scale%20Dataset%20and%0A%20%20Human-Like%20Reasoning%20Framework&body=Title%3A%20Geolocation%20with%20Real%20Human%20Gameplay%20Data%3A%20A%20Large-Scale%20Dataset%20and%0A%20%20Human-Like%20Reasoning%20Framework%0AAuthor%3A%20Zirui%20Song%20and%20Jingpu%20Yang%20and%20Yuan%20Huang%20and%20Jonathan%20Tonglet%20and%20Zeyu%20Zhang%20and%20Tao%20Cheng%20and%20Meng%20Fang%20and%20Iryna%20Gurevych%20and%20Xiuying%20Chen%0AAbstract%3A%20%20%20Geolocation%2C%20the%20task%20of%20identifying%20an%20image%27s%20location%2C%20requires%20complex%0Areasoning%20and%20is%20crucial%20for%20navigation%2C%20monitoring%2C%20and%20cultural%20preservation.%0AHowever%2C%20current%20methods%20often%20produce%20coarse%2C%20imprecise%2C%20and%20non-interpretable%0Alocalization.%20A%20major%20challenge%20lies%20in%20the%20quality%20and%20scale%20of%20existing%0Ageolocation%20datasets.%20These%20datasets%20are%20typically%20small-scale%20and%0Aautomatically%20constructed%2C%20leading%20to%20noisy%20data%20and%20inconsistent%20task%0Adifficulty%2C%20with%20images%20that%20either%20reveal%20answers%20too%20easily%20or%20lack%0Asufficient%20clues%20for%20reliable%20inference.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20a%20comprehensive%20geolocation%20framework%20with%20three%20key%20components%3A%0AGeoComp%2C%20a%20large-scale%20dataset%3B%20GeoCoT%2C%20a%20novel%20reasoning%20method%3B%20and%20GeoEval%2C%0Aan%20evaluation%20metric%2C%20collectively%20designed%20to%20address%20critical%20challenges%20and%0Adrive%20advancements%20in%20geolocation%20research.%20At%20the%20core%20of%20this%20framework%20is%0AGeoComp%20%28Geolocation%20Competition%20Dataset%29%2C%20a%20large-scale%20dataset%20collected%20from%0Aa%20geolocation%20game%20platform%20involving%20740K%20users%20over%20two%20years.%20It%20comprises%0A25%20million%20entries%20of%20metadata%20and%203%20million%20geo-tagged%20locations%20spanning%20much%0Aof%20the%20globe%2C%20with%20each%20location%20annotated%20thousands%20to%20tens%20of%20thousands%20of%0Atimes%20by%20human%20users.%20The%20dataset%20offers%20diverse%20difficulty%20levels%20for%20detailed%0Aanalysis%20and%20highlights%20key%20gaps%20in%20current%20models.%20Building%20on%20this%20dataset%2C%0Awe%20propose%20Geographical%20Chain-of-Thought%20%28GeoCoT%29%2C%20a%20novel%20multi-step%20reasoning%0Aframework%20designed%20to%20enhance%20the%20reasoning%20capabilities%20of%20Large%20Vision%20Models%0A%28LVMs%29%20in%20geolocation%20tasks.%20GeoCoT%20improves%20performance%20by%20integrating%0Acontextual%20and%20spatial%20cues%20through%20a%20multi-step%20process%20that%20mimics%20human%0Ageolocation%20reasoning.%20Finally%2C%20using%20the%20GeoEval%20metric%2C%20we%20demonstrate%20that%0AGeoCoT%20significantly%20boosts%20geolocation%20accuracy%20by%20up%20to%2025%25%20while%20enhancing%0Ainterpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13759v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeolocation%2520with%2520Real%2520Human%2520Gameplay%2520Data%253A%2520A%2520Large-Scale%2520Dataset%2520and%250A%2520%2520Human-Like%2520Reasoning%2520Framework%26entry.906535625%3DZirui%2520Song%2520and%2520Jingpu%2520Yang%2520and%2520Yuan%2520Huang%2520and%2520Jonathan%2520Tonglet%2520and%2520Zeyu%2520Zhang%2520and%2520Tao%2520Cheng%2520and%2520Meng%2520Fang%2520and%2520Iryna%2520Gurevych%2520and%2520Xiuying%2520Chen%26entry.1292438233%3D%2520%2520Geolocation%252C%2520the%2520task%2520of%2520identifying%2520an%2520image%2527s%2520location%252C%2520requires%2520complex%250Areasoning%2520and%2520is%2520crucial%2520for%2520navigation%252C%2520monitoring%252C%2520and%2520cultural%2520preservation.%250AHowever%252C%2520current%2520methods%2520often%2520produce%2520coarse%252C%2520imprecise%252C%2520and%2520non-interpretable%250Alocalization.%2520A%2520major%2520challenge%2520lies%2520in%2520the%2520quality%2520and%2520scale%2520of%2520existing%250Ageolocation%2520datasets.%2520These%2520datasets%2520are%2520typically%2520small-scale%2520and%250Aautomatically%2520constructed%252C%2520leading%2520to%2520noisy%2520data%2520and%2520inconsistent%2520task%250Adifficulty%252C%2520with%2520images%2520that%2520either%2520reveal%2520answers%2520too%2520easily%2520or%2520lack%250Asufficient%2520clues%2520for%2520reliable%2520inference.%2520To%2520address%2520these%2520challenges%252C%2520we%250Aintroduce%2520a%2520comprehensive%2520geolocation%2520framework%2520with%2520three%2520key%2520components%253A%250AGeoComp%252C%2520a%2520large-scale%2520dataset%253B%2520GeoCoT%252C%2520a%2520novel%2520reasoning%2520method%253B%2520and%2520GeoEval%252C%250Aan%2520evaluation%2520metric%252C%2520collectively%2520designed%2520to%2520address%2520critical%2520challenges%2520and%250Adrive%2520advancements%2520in%2520geolocation%2520research.%2520At%2520the%2520core%2520of%2520this%2520framework%2520is%250AGeoComp%2520%2528Geolocation%2520Competition%2520Dataset%2529%252C%2520a%2520large-scale%2520dataset%2520collected%2520from%250Aa%2520geolocation%2520game%2520platform%2520involving%2520740K%2520users%2520over%2520two%2520years.%2520It%2520comprises%250A25%2520million%2520entries%2520of%2520metadata%2520and%25203%2520million%2520geo-tagged%2520locations%2520spanning%2520much%250Aof%2520the%2520globe%252C%2520with%2520each%2520location%2520annotated%2520thousands%2520to%2520tens%2520of%2520thousands%2520of%250Atimes%2520by%2520human%2520users.%2520The%2520dataset%2520offers%2520diverse%2520difficulty%2520levels%2520for%2520detailed%250Aanalysis%2520and%2520highlights%2520key%2520gaps%2520in%2520current%2520models.%2520Building%2520on%2520this%2520dataset%252C%250Awe%2520propose%2520Geographical%2520Chain-of-Thought%2520%2528GeoCoT%2529%252C%2520a%2520novel%2520multi-step%2520reasoning%250Aframework%2520designed%2520to%2520enhance%2520the%2520reasoning%2520capabilities%2520of%2520Large%2520Vision%2520Models%250A%2528LVMs%2529%2520in%2520geolocation%2520tasks.%2520GeoCoT%2520improves%2520performance%2520by%2520integrating%250Acontextual%2520and%2520spatial%2520cues%2520through%2520a%2520multi-step%2520process%2520that%2520mimics%2520human%250Ageolocation%2520reasoning.%2520Finally%252C%2520using%2520the%2520GeoEval%2520metric%252C%2520we%2520demonstrate%2520that%250AGeoCoT%2520significantly%2520boosts%2520geolocation%2520accuracy%2520by%2520up%2520to%252025%2525%2520while%2520enhancing%250Ainterpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13759v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geolocation%20with%20Real%20Human%20Gameplay%20Data%3A%20A%20Large-Scale%20Dataset%20and%0A%20%20Human-Like%20Reasoning%20Framework&entry.906535625=Zirui%20Song%20and%20Jingpu%20Yang%20and%20Yuan%20Huang%20and%20Jonathan%20Tonglet%20and%20Zeyu%20Zhang%20and%20Tao%20Cheng%20and%20Meng%20Fang%20and%20Iryna%20Gurevych%20and%20Xiuying%20Chen&entry.1292438233=%20%20Geolocation%2C%20the%20task%20of%20identifying%20an%20image%27s%20location%2C%20requires%20complex%0Areasoning%20and%20is%20crucial%20for%20navigation%2C%20monitoring%2C%20and%20cultural%20preservation.%0AHowever%2C%20current%20methods%20often%20produce%20coarse%2C%20imprecise%2C%20and%20non-interpretable%0Alocalization.%20A%20major%20challenge%20lies%20in%20the%20quality%20and%20scale%20of%20existing%0Ageolocation%20datasets.%20These%20datasets%20are%20typically%20small-scale%20and%0Aautomatically%20constructed%2C%20leading%20to%20noisy%20data%20and%20inconsistent%20task%0Adifficulty%2C%20with%20images%20that%20either%20reveal%20answers%20too%20easily%20or%20lack%0Asufficient%20clues%20for%20reliable%20inference.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20a%20comprehensive%20geolocation%20framework%20with%20three%20key%20components%3A%0AGeoComp%2C%20a%20large-scale%20dataset%3B%20GeoCoT%2C%20a%20novel%20reasoning%20method%3B%20and%20GeoEval%2C%0Aan%20evaluation%20metric%2C%20collectively%20designed%20to%20address%20critical%20challenges%20and%0Adrive%20advancements%20in%20geolocation%20research.%20At%20the%20core%20of%20this%20framework%20is%0AGeoComp%20%28Geolocation%20Competition%20Dataset%29%2C%20a%20large-scale%20dataset%20collected%20from%0Aa%20geolocation%20game%20platform%20involving%20740K%20users%20over%20two%20years.%20It%20comprises%0A25%20million%20entries%20of%20metadata%20and%203%20million%20geo-tagged%20locations%20spanning%20much%0Aof%20the%20globe%2C%20with%20each%20location%20annotated%20thousands%20to%20tens%20of%20thousands%20of%0Atimes%20by%20human%20users.%20The%20dataset%20offers%20diverse%20difficulty%20levels%20for%20detailed%0Aanalysis%20and%20highlights%20key%20gaps%20in%20current%20models.%20Building%20on%20this%20dataset%2C%0Awe%20propose%20Geographical%20Chain-of-Thought%20%28GeoCoT%29%2C%20a%20novel%20multi-step%20reasoning%0Aframework%20designed%20to%20enhance%20the%20reasoning%20capabilities%20of%20Large%20Vision%20Models%0A%28LVMs%29%20in%20geolocation%20tasks.%20GeoCoT%20improves%20performance%20by%20integrating%0Acontextual%20and%20spatial%20cues%20through%20a%20multi-step%20process%20that%20mimics%20human%0Ageolocation%20reasoning.%20Finally%2C%20using%20the%20GeoEval%20metric%2C%20we%20demonstrate%20that%0AGeoCoT%20significantly%20boosts%20geolocation%20accuracy%20by%20up%20to%2025%25%20while%20enhancing%0Ainterpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13759v2&entry.124074799=Read"},
{"title": "ChemVLM: Exploring the Power of Multimodal Large Language Models in\n  Chemistry Area", "author": "Junxian Li and Di Zhang and Xunzhi Wang and Zeying Hao and Jingdi Lei and Qian Tan and Cai Zhou and Wei Liu and Yaotian Yang and Xinrui Xiong and Weiyun Wang and Zhe Chen and Wenhai Wang and Wei Li and Shufei Zhang and Mao Su and Wanli Ouyang and Yuqiang Li and Dongzhan Zhou", "abstract": "  Large Language Models (LLMs) have achieved remarkable success and have been\napplied across various scientific fields, including chemistry. However, many\nchemical tasks require the processing of visual information, which cannot be\nsuccessfully handled by existing chemical LLMs. This brings a growing need for\nmodels capable of integrating multimodal information in the chemical domain. In\nthis paper, we introduce \\textbf{ChemVLM}, an open-source chemical multimodal\nlarge language model specifically designed for chemical applications. ChemVLM\nis trained on a carefully curated bilingual multimodal dataset that enhances\nits ability to understand both textual and visual chemical information,\nincluding molecular structures, reactions, and chemistry examination questions.\nWe develop three datasets for comprehensive evaluation, tailored to Chemical\nOptical Character Recognition (OCR), Multimodal Chemical Reasoning (MMCR), and\nMultimodal Molecule Understanding tasks. We benchmark ChemVLM against a range\nof open-source and proprietary multimodal large language models on various\ntasks. Experimental results demonstrate that ChemVLM achieves competitive\nperformance across all evaluated tasks. Our model can be found at\nhttps://huggingface.co/AI4Chem/ChemVLM-26B.\n", "link": "http://arxiv.org/abs/2408.07246v5", "date": "2025-04-17", "relevancy": 2.7164, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5514}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5392}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChemVLM%3A%20Exploring%20the%20Power%20of%20Multimodal%20Large%20Language%20Models%20in%0A%20%20Chemistry%20Area&body=Title%3A%20ChemVLM%3A%20Exploring%20the%20Power%20of%20Multimodal%20Large%20Language%20Models%20in%0A%20%20Chemistry%20Area%0AAuthor%3A%20Junxian%20Li%20and%20Di%20Zhang%20and%20Xunzhi%20Wang%20and%20Zeying%20Hao%20and%20Jingdi%20Lei%20and%20Qian%20Tan%20and%20Cai%20Zhou%20and%20Wei%20Liu%20and%20Yaotian%20Yang%20and%20Xinrui%20Xiong%20and%20Weiyun%20Wang%20and%20Zhe%20Chen%20and%20Wenhai%20Wang%20and%20Wei%20Li%20and%20Shufei%20Zhang%20and%20Mao%20Su%20and%20Wanli%20Ouyang%20and%20Yuqiang%20Li%20and%20Dongzhan%20Zhou%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20remarkable%20success%20and%20have%20been%0Aapplied%20across%20various%20scientific%20fields%2C%20including%20chemistry.%20However%2C%20many%0Achemical%20tasks%20require%20the%20processing%20of%20visual%20information%2C%20which%20cannot%20be%0Asuccessfully%20handled%20by%20existing%20chemical%20LLMs.%20This%20brings%20a%20growing%20need%20for%0Amodels%20capable%20of%20integrating%20multimodal%20information%20in%20the%20chemical%20domain.%20In%0Athis%20paper%2C%20we%20introduce%20%5Ctextbf%7BChemVLM%7D%2C%20an%20open-source%20chemical%20multimodal%0Alarge%20language%20model%20specifically%20designed%20for%20chemical%20applications.%20ChemVLM%0Ais%20trained%20on%20a%20carefully%20curated%20bilingual%20multimodal%20dataset%20that%20enhances%0Aits%20ability%20to%20understand%20both%20textual%20and%20visual%20chemical%20information%2C%0Aincluding%20molecular%20structures%2C%20reactions%2C%20and%20chemistry%20examination%20questions.%0AWe%20develop%20three%20datasets%20for%20comprehensive%20evaluation%2C%20tailored%20to%20Chemical%0AOptical%20Character%20Recognition%20%28OCR%29%2C%20Multimodal%20Chemical%20Reasoning%20%28MMCR%29%2C%20and%0AMultimodal%20Molecule%20Understanding%20tasks.%20We%20benchmark%20ChemVLM%20against%20a%20range%0Aof%20open-source%20and%20proprietary%20multimodal%20large%20language%20models%20on%20various%0Atasks.%20Experimental%20results%20demonstrate%20that%20ChemVLM%20achieves%20competitive%0Aperformance%20across%20all%20evaluated%20tasks.%20Our%20model%20can%20be%20found%20at%0Ahttps%3A//huggingface.co/AI4Chem/ChemVLM-26B.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07246v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChemVLM%253A%2520Exploring%2520the%2520Power%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520in%250A%2520%2520Chemistry%2520Area%26entry.906535625%3DJunxian%2520Li%2520and%2520Di%2520Zhang%2520and%2520Xunzhi%2520Wang%2520and%2520Zeying%2520Hao%2520and%2520Jingdi%2520Lei%2520and%2520Qian%2520Tan%2520and%2520Cai%2520Zhou%2520and%2520Wei%2520Liu%2520and%2520Yaotian%2520Yang%2520and%2520Xinrui%2520Xiong%2520and%2520Weiyun%2520Wang%2520and%2520Zhe%2520Chen%2520and%2520Wenhai%2520Wang%2520and%2520Wei%2520Li%2520and%2520Shufei%2520Zhang%2520and%2520Mao%2520Su%2520and%2520Wanli%2520Ouyang%2520and%2520Yuqiang%2520Li%2520and%2520Dongzhan%2520Zhou%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520achieved%2520remarkable%2520success%2520and%2520have%2520been%250Aapplied%2520across%2520various%2520scientific%2520fields%252C%2520including%2520chemistry.%2520However%252C%2520many%250Achemical%2520tasks%2520require%2520the%2520processing%2520of%2520visual%2520information%252C%2520which%2520cannot%2520be%250Asuccessfully%2520handled%2520by%2520existing%2520chemical%2520LLMs.%2520This%2520brings%2520a%2520growing%2520need%2520for%250Amodels%2520capable%2520of%2520integrating%2520multimodal%2520information%2520in%2520the%2520chemical%2520domain.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520%255Ctextbf%257BChemVLM%257D%252C%2520an%2520open-source%2520chemical%2520multimodal%250Alarge%2520language%2520model%2520specifically%2520designed%2520for%2520chemical%2520applications.%2520ChemVLM%250Ais%2520trained%2520on%2520a%2520carefully%2520curated%2520bilingual%2520multimodal%2520dataset%2520that%2520enhances%250Aits%2520ability%2520to%2520understand%2520both%2520textual%2520and%2520visual%2520chemical%2520information%252C%250Aincluding%2520molecular%2520structures%252C%2520reactions%252C%2520and%2520chemistry%2520examination%2520questions.%250AWe%2520develop%2520three%2520datasets%2520for%2520comprehensive%2520evaluation%252C%2520tailored%2520to%2520Chemical%250AOptical%2520Character%2520Recognition%2520%2528OCR%2529%252C%2520Multimodal%2520Chemical%2520Reasoning%2520%2528MMCR%2529%252C%2520and%250AMultimodal%2520Molecule%2520Understanding%2520tasks.%2520We%2520benchmark%2520ChemVLM%2520against%2520a%2520range%250Aof%2520open-source%2520and%2520proprietary%2520multimodal%2520large%2520language%2520models%2520on%2520various%250Atasks.%2520Experimental%2520results%2520demonstrate%2520that%2520ChemVLM%2520achieves%2520competitive%250Aperformance%2520across%2520all%2520evaluated%2520tasks.%2520Our%2520model%2520can%2520be%2520found%2520at%250Ahttps%253A//huggingface.co/AI4Chem/ChemVLM-26B.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07246v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChemVLM%3A%20Exploring%20the%20Power%20of%20Multimodal%20Large%20Language%20Models%20in%0A%20%20Chemistry%20Area&entry.906535625=Junxian%20Li%20and%20Di%20Zhang%20and%20Xunzhi%20Wang%20and%20Zeying%20Hao%20and%20Jingdi%20Lei%20and%20Qian%20Tan%20and%20Cai%20Zhou%20and%20Wei%20Liu%20and%20Yaotian%20Yang%20and%20Xinrui%20Xiong%20and%20Weiyun%20Wang%20and%20Zhe%20Chen%20and%20Wenhai%20Wang%20and%20Wei%20Li%20and%20Shufei%20Zhang%20and%20Mao%20Su%20and%20Wanli%20Ouyang%20and%20Yuqiang%20Li%20and%20Dongzhan%20Zhou&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20remarkable%20success%20and%20have%20been%0Aapplied%20across%20various%20scientific%20fields%2C%20including%20chemistry.%20However%2C%20many%0Achemical%20tasks%20require%20the%20processing%20of%20visual%20information%2C%20which%20cannot%20be%0Asuccessfully%20handled%20by%20existing%20chemical%20LLMs.%20This%20brings%20a%20growing%20need%20for%0Amodels%20capable%20of%20integrating%20multimodal%20information%20in%20the%20chemical%20domain.%20In%0Athis%20paper%2C%20we%20introduce%20%5Ctextbf%7BChemVLM%7D%2C%20an%20open-source%20chemical%20multimodal%0Alarge%20language%20model%20specifically%20designed%20for%20chemical%20applications.%20ChemVLM%0Ais%20trained%20on%20a%20carefully%20curated%20bilingual%20multimodal%20dataset%20that%20enhances%0Aits%20ability%20to%20understand%20both%20textual%20and%20visual%20chemical%20information%2C%0Aincluding%20molecular%20structures%2C%20reactions%2C%20and%20chemistry%20examination%20questions.%0AWe%20develop%20three%20datasets%20for%20comprehensive%20evaluation%2C%20tailored%20to%20Chemical%0AOptical%20Character%20Recognition%20%28OCR%29%2C%20Multimodal%20Chemical%20Reasoning%20%28MMCR%29%2C%20and%0AMultimodal%20Molecule%20Understanding%20tasks.%20We%20benchmark%20ChemVLM%20against%20a%20range%0Aof%20open-source%20and%20proprietary%20multimodal%20large%20language%20models%20on%20various%0Atasks.%20Experimental%20results%20demonstrate%20that%20ChemVLM%20achieves%20competitive%0Aperformance%20across%20all%20evaluated%20tasks.%20Our%20model%20can%20be%20found%20at%0Ahttps%3A//huggingface.co/AI4Chem/ChemVLM-26B.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07246v5&entry.124074799=Read"},
{"title": "Human-Activity AGV Quality Assessment: A Benchmark Dataset and an\n  Objective Evaluation Metric", "author": "Zhichao Zhang and Wei Sun and Xinyue Li and Yunhao Li and Qihang Ge and Jun Jia and Zicheng Zhang and Zhongpeng Ji and Fengyu Sun and Shangling Jui and Xiongkuo Min and Guangtao Zhai", "abstract": "  AI-driven video generation techniques have made significant progress in\nrecent years. However, AI-generated videos (AGVs) involving human activities\noften exhibit substantial visual and semantic distortions, hindering the\npractical application of video generation technologies in real-world scenarios.\nTo address this challenge, we conduct a pioneering study on human activity AGV\nquality assessment, focusing on visual quality evaluation and the\nidentification of semantic distortions. First, we construct the AI-Generated\nHuman activity Video Quality Assessment (Human-AGVQA) dataset, consisting of\n6,000 AGVs derived from 15 popular text-to-video (T2V) models using 400 text\nprompts that describe diverse human activities. We conduct a subjective study\nto evaluate the human appearance quality, action continuity quality, and\noverall video quality of AGVs, and identify semantic issues of human body\nparts. Based on Human-AGVQA, we benchmark the performance of T2V models and\nanalyze their strengths and weaknesses in generating different categories of\nhuman activities. Second, we develop an objective evaluation metric, named\nAI-Generated Human activity Video Quality metric (GHVQ), to automatically\nanalyze the quality of human activity AGVs. GHVQ systematically extracts\nhuman-focused quality features, AI-generated content-aware quality features,\nand temporal continuity features, making it a comprehensive and explainable\nquality metric for human activity AGVs. The extensive experimental results show\nthat GHVQ outperforms existing quality metrics on the Human-AGVQA dataset by a\nlarge margin, demonstrating its efficacy in assessing the quality of human\nactivity AGVs. The Human-AGVQA dataset and GHVQ metric will be released\npublicly.\n", "link": "http://arxiv.org/abs/2411.16619v2", "date": "2025-04-17", "relevancy": 2.7125, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.564}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5371}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-Activity%20AGV%20Quality%20Assessment%3A%20A%20Benchmark%20Dataset%20and%20an%0A%20%20Objective%20Evaluation%20Metric&body=Title%3A%20Human-Activity%20AGV%20Quality%20Assessment%3A%20A%20Benchmark%20Dataset%20and%20an%0A%20%20Objective%20Evaluation%20Metric%0AAuthor%3A%20Zhichao%20Zhang%20and%20Wei%20Sun%20and%20Xinyue%20Li%20and%20Yunhao%20Li%20and%20Qihang%20Ge%20and%20Jun%20Jia%20and%20Zicheng%20Zhang%20and%20Zhongpeng%20Ji%20and%20Fengyu%20Sun%20and%20Shangling%20Jui%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai%0AAbstract%3A%20%20%20AI-driven%20video%20generation%20techniques%20have%20made%20significant%20progress%20in%0Arecent%20years.%20However%2C%20AI-generated%20videos%20%28AGVs%29%20involving%20human%20activities%0Aoften%20exhibit%20substantial%20visual%20and%20semantic%20distortions%2C%20hindering%20the%0Apractical%20application%20of%20video%20generation%20technologies%20in%20real-world%20scenarios.%0ATo%20address%20this%20challenge%2C%20we%20conduct%20a%20pioneering%20study%20on%20human%20activity%20AGV%0Aquality%20assessment%2C%20focusing%20on%20visual%20quality%20evaluation%20and%20the%0Aidentification%20of%20semantic%20distortions.%20First%2C%20we%20construct%20the%20AI-Generated%0AHuman%20activity%20Video%20Quality%20Assessment%20%28Human-AGVQA%29%20dataset%2C%20consisting%20of%0A6%2C000%20AGVs%20derived%20from%2015%20popular%20text-to-video%20%28T2V%29%20models%20using%20400%20text%0Aprompts%20that%20describe%20diverse%20human%20activities.%20We%20conduct%20a%20subjective%20study%0Ato%20evaluate%20the%20human%20appearance%20quality%2C%20action%20continuity%20quality%2C%20and%0Aoverall%20video%20quality%20of%20AGVs%2C%20and%20identify%20semantic%20issues%20of%20human%20body%0Aparts.%20Based%20on%20Human-AGVQA%2C%20we%20benchmark%20the%20performance%20of%20T2V%20models%20and%0Aanalyze%20their%20strengths%20and%20weaknesses%20in%20generating%20different%20categories%20of%0Ahuman%20activities.%20Second%2C%20we%20develop%20an%20objective%20evaluation%20metric%2C%20named%0AAI-Generated%20Human%20activity%20Video%20Quality%20metric%20%28GHVQ%29%2C%20to%20automatically%0Aanalyze%20the%20quality%20of%20human%20activity%20AGVs.%20GHVQ%20systematically%20extracts%0Ahuman-focused%20quality%20features%2C%20AI-generated%20content-aware%20quality%20features%2C%0Aand%20temporal%20continuity%20features%2C%20making%20it%20a%20comprehensive%20and%20explainable%0Aquality%20metric%20for%20human%20activity%20AGVs.%20The%20extensive%20experimental%20results%20show%0Athat%20GHVQ%20outperforms%20existing%20quality%20metrics%20on%20the%20Human-AGVQA%20dataset%20by%20a%0Alarge%20margin%2C%20demonstrating%20its%20efficacy%20in%20assessing%20the%20quality%20of%20human%0Aactivity%20AGVs.%20The%20Human-AGVQA%20dataset%20and%20GHVQ%20metric%20will%20be%20released%0Apublicly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16619v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-Activity%2520AGV%2520Quality%2520Assessment%253A%2520A%2520Benchmark%2520Dataset%2520and%2520an%250A%2520%2520Objective%2520Evaluation%2520Metric%26entry.906535625%3DZhichao%2520Zhang%2520and%2520Wei%2520Sun%2520and%2520Xinyue%2520Li%2520and%2520Yunhao%2520Li%2520and%2520Qihang%2520Ge%2520and%2520Jun%2520Jia%2520and%2520Zicheng%2520Zhang%2520and%2520Zhongpeng%2520Ji%2520and%2520Fengyu%2520Sun%2520and%2520Shangling%2520Jui%2520and%2520Xiongkuo%2520Min%2520and%2520Guangtao%2520Zhai%26entry.1292438233%3D%2520%2520AI-driven%2520video%2520generation%2520techniques%2520have%2520made%2520significant%2520progress%2520in%250Arecent%2520years.%2520However%252C%2520AI-generated%2520videos%2520%2528AGVs%2529%2520involving%2520human%2520activities%250Aoften%2520exhibit%2520substantial%2520visual%2520and%2520semantic%2520distortions%252C%2520hindering%2520the%250Apractical%2520application%2520of%2520video%2520generation%2520technologies%2520in%2520real-world%2520scenarios.%250ATo%2520address%2520this%2520challenge%252C%2520we%2520conduct%2520a%2520pioneering%2520study%2520on%2520human%2520activity%2520AGV%250Aquality%2520assessment%252C%2520focusing%2520on%2520visual%2520quality%2520evaluation%2520and%2520the%250Aidentification%2520of%2520semantic%2520distortions.%2520First%252C%2520we%2520construct%2520the%2520AI-Generated%250AHuman%2520activity%2520Video%2520Quality%2520Assessment%2520%2528Human-AGVQA%2529%2520dataset%252C%2520consisting%2520of%250A6%252C000%2520AGVs%2520derived%2520from%252015%2520popular%2520text-to-video%2520%2528T2V%2529%2520models%2520using%2520400%2520text%250Aprompts%2520that%2520describe%2520diverse%2520human%2520activities.%2520We%2520conduct%2520a%2520subjective%2520study%250Ato%2520evaluate%2520the%2520human%2520appearance%2520quality%252C%2520action%2520continuity%2520quality%252C%2520and%250Aoverall%2520video%2520quality%2520of%2520AGVs%252C%2520and%2520identify%2520semantic%2520issues%2520of%2520human%2520body%250Aparts.%2520Based%2520on%2520Human-AGVQA%252C%2520we%2520benchmark%2520the%2520performance%2520of%2520T2V%2520models%2520and%250Aanalyze%2520their%2520strengths%2520and%2520weaknesses%2520in%2520generating%2520different%2520categories%2520of%250Ahuman%2520activities.%2520Second%252C%2520we%2520develop%2520an%2520objective%2520evaluation%2520metric%252C%2520named%250AAI-Generated%2520Human%2520activity%2520Video%2520Quality%2520metric%2520%2528GHVQ%2529%252C%2520to%2520automatically%250Aanalyze%2520the%2520quality%2520of%2520human%2520activity%2520AGVs.%2520GHVQ%2520systematically%2520extracts%250Ahuman-focused%2520quality%2520features%252C%2520AI-generated%2520content-aware%2520quality%2520features%252C%250Aand%2520temporal%2520continuity%2520features%252C%2520making%2520it%2520a%2520comprehensive%2520and%2520explainable%250Aquality%2520metric%2520for%2520human%2520activity%2520AGVs.%2520The%2520extensive%2520experimental%2520results%2520show%250Athat%2520GHVQ%2520outperforms%2520existing%2520quality%2520metrics%2520on%2520the%2520Human-AGVQA%2520dataset%2520by%2520a%250Alarge%2520margin%252C%2520demonstrating%2520its%2520efficacy%2520in%2520assessing%2520the%2520quality%2520of%2520human%250Aactivity%2520AGVs.%2520The%2520Human-AGVQA%2520dataset%2520and%2520GHVQ%2520metric%2520will%2520be%2520released%250Apublicly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16619v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-Activity%20AGV%20Quality%20Assessment%3A%20A%20Benchmark%20Dataset%20and%20an%0A%20%20Objective%20Evaluation%20Metric&entry.906535625=Zhichao%20Zhang%20and%20Wei%20Sun%20and%20Xinyue%20Li%20and%20Yunhao%20Li%20and%20Qihang%20Ge%20and%20Jun%20Jia%20and%20Zicheng%20Zhang%20and%20Zhongpeng%20Ji%20and%20Fengyu%20Sun%20and%20Shangling%20Jui%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai&entry.1292438233=%20%20AI-driven%20video%20generation%20techniques%20have%20made%20significant%20progress%20in%0Arecent%20years.%20However%2C%20AI-generated%20videos%20%28AGVs%29%20involving%20human%20activities%0Aoften%20exhibit%20substantial%20visual%20and%20semantic%20distortions%2C%20hindering%20the%0Apractical%20application%20of%20video%20generation%20technologies%20in%20real-world%20scenarios.%0ATo%20address%20this%20challenge%2C%20we%20conduct%20a%20pioneering%20study%20on%20human%20activity%20AGV%0Aquality%20assessment%2C%20focusing%20on%20visual%20quality%20evaluation%20and%20the%0Aidentification%20of%20semantic%20distortions.%20First%2C%20we%20construct%20the%20AI-Generated%0AHuman%20activity%20Video%20Quality%20Assessment%20%28Human-AGVQA%29%20dataset%2C%20consisting%20of%0A6%2C000%20AGVs%20derived%20from%2015%20popular%20text-to-video%20%28T2V%29%20models%20using%20400%20text%0Aprompts%20that%20describe%20diverse%20human%20activities.%20We%20conduct%20a%20subjective%20study%0Ato%20evaluate%20the%20human%20appearance%20quality%2C%20action%20continuity%20quality%2C%20and%0Aoverall%20video%20quality%20of%20AGVs%2C%20and%20identify%20semantic%20issues%20of%20human%20body%0Aparts.%20Based%20on%20Human-AGVQA%2C%20we%20benchmark%20the%20performance%20of%20T2V%20models%20and%0Aanalyze%20their%20strengths%20and%20weaknesses%20in%20generating%20different%20categories%20of%0Ahuman%20activities.%20Second%2C%20we%20develop%20an%20objective%20evaluation%20metric%2C%20named%0AAI-Generated%20Human%20activity%20Video%20Quality%20metric%20%28GHVQ%29%2C%20to%20automatically%0Aanalyze%20the%20quality%20of%20human%20activity%20AGVs.%20GHVQ%20systematically%20extracts%0Ahuman-focused%20quality%20features%2C%20AI-generated%20content-aware%20quality%20features%2C%0Aand%20temporal%20continuity%20features%2C%20making%20it%20a%20comprehensive%20and%20explainable%0Aquality%20metric%20for%20human%20activity%20AGVs.%20The%20extensive%20experimental%20results%20show%0Athat%20GHVQ%20outperforms%20existing%20quality%20metrics%20on%20the%20Human-AGVQA%20dataset%20by%20a%0Alarge%20margin%2C%20demonstrating%20its%20efficacy%20in%20assessing%20the%20quality%20of%20human%0Aactivity%20AGVs.%20The%20Human-AGVQA%20dataset%20and%20GHVQ%20metric%20will%20be%20released%0Apublicly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16619v2&entry.124074799=Read"},
{"title": "Collaborative Perception Datasets for Autonomous Driving: A Review", "author": "Naibang Wang and Deyong Shang and Yan Gong and Xiaoxi Hu and Ziying Song and Lei Yang and Yuhan Huang and Xiaoyu Wang and Jianli Lu", "abstract": "  Collaborative perception has attracted growing interest from academia and\nindustry due to its potential to enhance perception accuracy, safety, and\nrobustness in autonomous driving through multi-agent information fusion. With\nthe advancement of Vehicle-to-Everything (V2X) communication, numerous\ncollaborative perception datasets have emerged, varying in cooperation\nparadigms, sensor configurations, data sources, and application scenarios.\nHowever, the absence of systematic summarization and comparative analysis\nhinders effective resource utilization and standardization of model evaluation.\nAs the first comprehensive review focused on collaborative perception datasets,\nthis work reviews and compares existing resources from a multi-dimensional\nperspective. We categorize datasets based on cooperation paradigms, examine\ntheir data sources and scenarios, and analyze sensor modalities and supported\ntasks. A detailed comparative analysis is conducted across multiple dimensions.\nWe also outline key challenges and future directions, including dataset\nscalability, diversity, domain adaptation, standardization, privacy, and the\nintegration of large language models. To support ongoing research, we provide a\ncontinuously updated online repository of collaborative perception datasets and\nrelated literature:\nhttps://github.com/frankwnb/Collaborative-Perception-Datasets-for-Autonomous-Driving.\n", "link": "http://arxiv.org/abs/2504.12696v1", "date": "2025-04-17", "relevancy": 2.69, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5635}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5252}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collaborative%20Perception%20Datasets%20for%20Autonomous%20Driving%3A%20A%20Review&body=Title%3A%20Collaborative%20Perception%20Datasets%20for%20Autonomous%20Driving%3A%20A%20Review%0AAuthor%3A%20Naibang%20Wang%20and%20Deyong%20Shang%20and%20Yan%20Gong%20and%20Xiaoxi%20Hu%20and%20Ziying%20Song%20and%20Lei%20Yang%20and%20Yuhan%20Huang%20and%20Xiaoyu%20Wang%20and%20Jianli%20Lu%0AAbstract%3A%20%20%20Collaborative%20perception%20has%20attracted%20growing%20interest%20from%20academia%20and%0Aindustry%20due%20to%20its%20potential%20to%20enhance%20perception%20accuracy%2C%20safety%2C%20and%0Arobustness%20in%20autonomous%20driving%20through%20multi-agent%20information%20fusion.%20With%0Athe%20advancement%20of%20Vehicle-to-Everything%20%28V2X%29%20communication%2C%20numerous%0Acollaborative%20perception%20datasets%20have%20emerged%2C%20varying%20in%20cooperation%0Aparadigms%2C%20sensor%20configurations%2C%20data%20sources%2C%20and%20application%20scenarios.%0AHowever%2C%20the%20absence%20of%20systematic%20summarization%20and%20comparative%20analysis%0Ahinders%20effective%20resource%20utilization%20and%20standardization%20of%20model%20evaluation.%0AAs%20the%20first%20comprehensive%20review%20focused%20on%20collaborative%20perception%20datasets%2C%0Athis%20work%20reviews%20and%20compares%20existing%20resources%20from%20a%20multi-dimensional%0Aperspective.%20We%20categorize%20datasets%20based%20on%20cooperation%20paradigms%2C%20examine%0Atheir%20data%20sources%20and%20scenarios%2C%20and%20analyze%20sensor%20modalities%20and%20supported%0Atasks.%20A%20detailed%20comparative%20analysis%20is%20conducted%20across%20multiple%20dimensions.%0AWe%20also%20outline%20key%20challenges%20and%20future%20directions%2C%20including%20dataset%0Ascalability%2C%20diversity%2C%20domain%20adaptation%2C%20standardization%2C%20privacy%2C%20and%20the%0Aintegration%20of%20large%20language%20models.%20To%20support%20ongoing%20research%2C%20we%20provide%20a%0Acontinuously%20updated%20online%20repository%20of%20collaborative%20perception%20datasets%20and%0Arelated%20literature%3A%0Ahttps%3A//github.com/frankwnb/Collaborative-Perception-Datasets-for-Autonomous-Driving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12696v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollaborative%2520Perception%2520Datasets%2520for%2520Autonomous%2520Driving%253A%2520A%2520Review%26entry.906535625%3DNaibang%2520Wang%2520and%2520Deyong%2520Shang%2520and%2520Yan%2520Gong%2520and%2520Xiaoxi%2520Hu%2520and%2520Ziying%2520Song%2520and%2520Lei%2520Yang%2520and%2520Yuhan%2520Huang%2520and%2520Xiaoyu%2520Wang%2520and%2520Jianli%2520Lu%26entry.1292438233%3D%2520%2520Collaborative%2520perception%2520has%2520attracted%2520growing%2520interest%2520from%2520academia%2520and%250Aindustry%2520due%2520to%2520its%2520potential%2520to%2520enhance%2520perception%2520accuracy%252C%2520safety%252C%2520and%250Arobustness%2520in%2520autonomous%2520driving%2520through%2520multi-agent%2520information%2520fusion.%2520With%250Athe%2520advancement%2520of%2520Vehicle-to-Everything%2520%2528V2X%2529%2520communication%252C%2520numerous%250Acollaborative%2520perception%2520datasets%2520have%2520emerged%252C%2520varying%2520in%2520cooperation%250Aparadigms%252C%2520sensor%2520configurations%252C%2520data%2520sources%252C%2520and%2520application%2520scenarios.%250AHowever%252C%2520the%2520absence%2520of%2520systematic%2520summarization%2520and%2520comparative%2520analysis%250Ahinders%2520effective%2520resource%2520utilization%2520and%2520standardization%2520of%2520model%2520evaluation.%250AAs%2520the%2520first%2520comprehensive%2520review%2520focused%2520on%2520collaborative%2520perception%2520datasets%252C%250Athis%2520work%2520reviews%2520and%2520compares%2520existing%2520resources%2520from%2520a%2520multi-dimensional%250Aperspective.%2520We%2520categorize%2520datasets%2520based%2520on%2520cooperation%2520paradigms%252C%2520examine%250Atheir%2520data%2520sources%2520and%2520scenarios%252C%2520and%2520analyze%2520sensor%2520modalities%2520and%2520supported%250Atasks.%2520A%2520detailed%2520comparative%2520analysis%2520is%2520conducted%2520across%2520multiple%2520dimensions.%250AWe%2520also%2520outline%2520key%2520challenges%2520and%2520future%2520directions%252C%2520including%2520dataset%250Ascalability%252C%2520diversity%252C%2520domain%2520adaptation%252C%2520standardization%252C%2520privacy%252C%2520and%2520the%250Aintegration%2520of%2520large%2520language%2520models.%2520To%2520support%2520ongoing%2520research%252C%2520we%2520provide%2520a%250Acontinuously%2520updated%2520online%2520repository%2520of%2520collaborative%2520perception%2520datasets%2520and%250Arelated%2520literature%253A%250Ahttps%253A//github.com/frankwnb/Collaborative-Perception-Datasets-for-Autonomous-Driving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12696v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collaborative%20Perception%20Datasets%20for%20Autonomous%20Driving%3A%20A%20Review&entry.906535625=Naibang%20Wang%20and%20Deyong%20Shang%20and%20Yan%20Gong%20and%20Xiaoxi%20Hu%20and%20Ziying%20Song%20and%20Lei%20Yang%20and%20Yuhan%20Huang%20and%20Xiaoyu%20Wang%20and%20Jianli%20Lu&entry.1292438233=%20%20Collaborative%20perception%20has%20attracted%20growing%20interest%20from%20academia%20and%0Aindustry%20due%20to%20its%20potential%20to%20enhance%20perception%20accuracy%2C%20safety%2C%20and%0Arobustness%20in%20autonomous%20driving%20through%20multi-agent%20information%20fusion.%20With%0Athe%20advancement%20of%20Vehicle-to-Everything%20%28V2X%29%20communication%2C%20numerous%0Acollaborative%20perception%20datasets%20have%20emerged%2C%20varying%20in%20cooperation%0Aparadigms%2C%20sensor%20configurations%2C%20data%20sources%2C%20and%20application%20scenarios.%0AHowever%2C%20the%20absence%20of%20systematic%20summarization%20and%20comparative%20analysis%0Ahinders%20effective%20resource%20utilization%20and%20standardization%20of%20model%20evaluation.%0AAs%20the%20first%20comprehensive%20review%20focused%20on%20collaborative%20perception%20datasets%2C%0Athis%20work%20reviews%20and%20compares%20existing%20resources%20from%20a%20multi-dimensional%0Aperspective.%20We%20categorize%20datasets%20based%20on%20cooperation%20paradigms%2C%20examine%0Atheir%20data%20sources%20and%20scenarios%2C%20and%20analyze%20sensor%20modalities%20and%20supported%0Atasks.%20A%20detailed%20comparative%20analysis%20is%20conducted%20across%20multiple%20dimensions.%0AWe%20also%20outline%20key%20challenges%20and%20future%20directions%2C%20including%20dataset%0Ascalability%2C%20diversity%2C%20domain%20adaptation%2C%20standardization%2C%20privacy%2C%20and%20the%0Aintegration%20of%20large%20language%20models.%20To%20support%20ongoing%20research%2C%20we%20provide%20a%0Acontinuously%20updated%20online%20repository%20of%20collaborative%20perception%20datasets%20and%0Arelated%20literature%3A%0Ahttps%3A//github.com/frankwnb/Collaborative-Perception-Datasets-for-Autonomous-Driving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12696v1&entry.124074799=Read"},
{"title": "GraphOmni: A Comprehensive and Extendable Benchmark Framework for Large\n  Language Models on Graph-theoretic Tasks", "author": "Hao Xu and Xiangru Jian and Xinjian Zhao and Wei Pang and Chao Zhang and Suyuchen Wang and Qixin Zhang and Joao Monteiro and Qiuzhuang Sun and Tianshu Yu", "abstract": "  In this paper, we presented GraphOmni, a comprehensive benchmark framework\nfor systematically evaluating the graph reasoning capabilities of LLMs. By\nanalyzing critical dimensions, including graph types, serialization formats,\nand prompt schemes, we provided extensive insights into the strengths and\nlimitations of current LLMs. Our empirical findings emphasize that no single\nserialization or prompting strategy consistently outperforms others. Motivated\nby these insights, we propose a reinforcement learning-based approach that\ndynamically selects the best serialization-prompt pairings, resulting in\nsignificant accuracy improvements. GraphOmni's modular and extensible design\nestablishes a robust foundation for future research, facilitating advancements\ntoward general-purpose graph reasoning models.\n", "link": "http://arxiv.org/abs/2504.12764v1", "date": "2025-04-17", "relevancy": 2.6859, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5459}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5459}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5197}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphOmni%3A%20A%20Comprehensive%20and%20Extendable%20Benchmark%20Framework%20for%20Large%0A%20%20Language%20Models%20on%20Graph-theoretic%20Tasks&body=Title%3A%20GraphOmni%3A%20A%20Comprehensive%20and%20Extendable%20Benchmark%20Framework%20for%20Large%0A%20%20Language%20Models%20on%20Graph-theoretic%20Tasks%0AAuthor%3A%20Hao%20Xu%20and%20Xiangru%20Jian%20and%20Xinjian%20Zhao%20and%20Wei%20Pang%20and%20Chao%20Zhang%20and%20Suyuchen%20Wang%20and%20Qixin%20Zhang%20and%20Joao%20Monteiro%20and%20Qiuzhuang%20Sun%20and%20Tianshu%20Yu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20presented%20GraphOmni%2C%20a%20comprehensive%20benchmark%20framework%0Afor%20systematically%20evaluating%20the%20graph%20reasoning%20capabilities%20of%20LLMs.%20By%0Aanalyzing%20critical%20dimensions%2C%20including%20graph%20types%2C%20serialization%20formats%2C%0Aand%20prompt%20schemes%2C%20we%20provided%20extensive%20insights%20into%20the%20strengths%20and%0Alimitations%20of%20current%20LLMs.%20Our%20empirical%20findings%20emphasize%20that%20no%20single%0Aserialization%20or%20prompting%20strategy%20consistently%20outperforms%20others.%20Motivated%0Aby%20these%20insights%2C%20we%20propose%20a%20reinforcement%20learning-based%20approach%20that%0Adynamically%20selects%20the%20best%20serialization-prompt%20pairings%2C%20resulting%20in%0Asignificant%20accuracy%20improvements.%20GraphOmni%27s%20modular%20and%20extensible%20design%0Aestablishes%20a%20robust%20foundation%20for%20future%20research%2C%20facilitating%20advancements%0Atoward%20general-purpose%20graph%20reasoning%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12764v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphOmni%253A%2520A%2520Comprehensive%2520and%2520Extendable%2520Benchmark%2520Framework%2520for%2520Large%250A%2520%2520Language%2520Models%2520on%2520Graph-theoretic%2520Tasks%26entry.906535625%3DHao%2520Xu%2520and%2520Xiangru%2520Jian%2520and%2520Xinjian%2520Zhao%2520and%2520Wei%2520Pang%2520and%2520Chao%2520Zhang%2520and%2520Suyuchen%2520Wang%2520and%2520Qixin%2520Zhang%2520and%2520Joao%2520Monteiro%2520and%2520Qiuzhuang%2520Sun%2520and%2520Tianshu%2520Yu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520presented%2520GraphOmni%252C%2520a%2520comprehensive%2520benchmark%2520framework%250Afor%2520systematically%2520evaluating%2520the%2520graph%2520reasoning%2520capabilities%2520of%2520LLMs.%2520By%250Aanalyzing%2520critical%2520dimensions%252C%2520including%2520graph%2520types%252C%2520serialization%2520formats%252C%250Aand%2520prompt%2520schemes%252C%2520we%2520provided%2520extensive%2520insights%2520into%2520the%2520strengths%2520and%250Alimitations%2520of%2520current%2520LLMs.%2520Our%2520empirical%2520findings%2520emphasize%2520that%2520no%2520single%250Aserialization%2520or%2520prompting%2520strategy%2520consistently%2520outperforms%2520others.%2520Motivated%250Aby%2520these%2520insights%252C%2520we%2520propose%2520a%2520reinforcement%2520learning-based%2520approach%2520that%250Adynamically%2520selects%2520the%2520best%2520serialization-prompt%2520pairings%252C%2520resulting%2520in%250Asignificant%2520accuracy%2520improvements.%2520GraphOmni%2527s%2520modular%2520and%2520extensible%2520design%250Aestablishes%2520a%2520robust%2520foundation%2520for%2520future%2520research%252C%2520facilitating%2520advancements%250Atoward%2520general-purpose%2520graph%2520reasoning%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12764v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphOmni%3A%20A%20Comprehensive%20and%20Extendable%20Benchmark%20Framework%20for%20Large%0A%20%20Language%20Models%20on%20Graph-theoretic%20Tasks&entry.906535625=Hao%20Xu%20and%20Xiangru%20Jian%20and%20Xinjian%20Zhao%20and%20Wei%20Pang%20and%20Chao%20Zhang%20and%20Suyuchen%20Wang%20and%20Qixin%20Zhang%20and%20Joao%20Monteiro%20and%20Qiuzhuang%20Sun%20and%20Tianshu%20Yu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20presented%20GraphOmni%2C%20a%20comprehensive%20benchmark%20framework%0Afor%20systematically%20evaluating%20the%20graph%20reasoning%20capabilities%20of%20LLMs.%20By%0Aanalyzing%20critical%20dimensions%2C%20including%20graph%20types%2C%20serialization%20formats%2C%0Aand%20prompt%20schemes%2C%20we%20provided%20extensive%20insights%20into%20the%20strengths%20and%0Alimitations%20of%20current%20LLMs.%20Our%20empirical%20findings%20emphasize%20that%20no%20single%0Aserialization%20or%20prompting%20strategy%20consistently%20outperforms%20others.%20Motivated%0Aby%20these%20insights%2C%20we%20propose%20a%20reinforcement%20learning-based%20approach%20that%0Adynamically%20selects%20the%20best%20serialization-prompt%20pairings%2C%20resulting%20in%0Asignificant%20accuracy%20improvements.%20GraphOmni%27s%20modular%20and%20extensible%20design%0Aestablishes%20a%20robust%20foundation%20for%20future%20research%2C%20facilitating%20advancements%0Atoward%20general-purpose%20graph%20reasoning%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12764v1&entry.124074799=Read"},
{"title": "Computer-Aided Design of Personalized Occlusal Positioning Splints Using\n  Multimodal 3D Data", "author": "Agnieszka Anna Tomaka and Leszek Luchowski and Micha\u0142 Tarnawski and Dariusz Pojda", "abstract": "  Contemporary digital technology has a pivotal role in the design of\ncustomized medical appliances, including occlusal splints used in the treatment\nof stomatognathic system dysfunctions. We present an approach to computer-aided\ndesign and precision assessment of positioning occlusal splints, bridging\nclinical concepts with current digital dental practice. In our model, a 3D\nsplint is generated based on a transformation matrix that represents the\ntherapeutic change in mandibular position, defined by a specialist using a\nvirtual patient model reconstructed from intraoral scans, CBCT, 3D facial scans\nand plaster model digitisation. The paper introduces a novel method for\ngenerating splints that accurately reproduce occlusal conditions in the\ntherapeutic position, including a mechanism for resolving surface conflicts\nthrough virtual embossing. We demonstrate how transformation matrices can be\nacquired through clinical tools and intraoral devices, and evaluate the\naccuracy of the designed and printed splints using profile and surface\ndeviation analysis. The proposed method enables reproducible, patient-specific\nsplint fabrication and opens new possibilities in diagnostics, multimodal image\nregistration and quantification of occlusal discrepancies.\n", "link": "http://arxiv.org/abs/2504.12868v1", "date": "2025-04-17", "relevancy": 2.6849, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5377}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5377}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Computer-Aided%20Design%20of%20Personalized%20Occlusal%20Positioning%20Splints%20Using%0A%20%20Multimodal%203D%20Data&body=Title%3A%20Computer-Aided%20Design%20of%20Personalized%20Occlusal%20Positioning%20Splints%20Using%0A%20%20Multimodal%203D%20Data%0AAuthor%3A%20Agnieszka%20Anna%20Tomaka%20and%20Leszek%20Luchowski%20and%20Micha%C5%82%20Tarnawski%20and%20Dariusz%20Pojda%0AAbstract%3A%20%20%20Contemporary%20digital%20technology%20has%20a%20pivotal%20role%20in%20the%20design%20of%0Acustomized%20medical%20appliances%2C%20including%20occlusal%20splints%20used%20in%20the%20treatment%0Aof%20stomatognathic%20system%20dysfunctions.%20We%20present%20an%20approach%20to%20computer-aided%0Adesign%20and%20precision%20assessment%20of%20positioning%20occlusal%20splints%2C%20bridging%0Aclinical%20concepts%20with%20current%20digital%20dental%20practice.%20In%20our%20model%2C%20a%203D%0Asplint%20is%20generated%20based%20on%20a%20transformation%20matrix%20that%20represents%20the%0Atherapeutic%20change%20in%20mandibular%20position%2C%20defined%20by%20a%20specialist%20using%20a%0Avirtual%20patient%20model%20reconstructed%20from%20intraoral%20scans%2C%20CBCT%2C%203D%20facial%20scans%0Aand%20plaster%20model%20digitisation.%20The%20paper%20introduces%20a%20novel%20method%20for%0Agenerating%20splints%20that%20accurately%20reproduce%20occlusal%20conditions%20in%20the%0Atherapeutic%20position%2C%20including%20a%20mechanism%20for%20resolving%20surface%20conflicts%0Athrough%20virtual%20embossing.%20We%20demonstrate%20how%20transformation%20matrices%20can%20be%0Aacquired%20through%20clinical%20tools%20and%20intraoral%20devices%2C%20and%20evaluate%20the%0Aaccuracy%20of%20the%20designed%20and%20printed%20splints%20using%20profile%20and%20surface%0Adeviation%20analysis.%20The%20proposed%20method%20enables%20reproducible%2C%20patient-specific%0Asplint%20fabrication%20and%20opens%20new%20possibilities%20in%20diagnostics%2C%20multimodal%20image%0Aregistration%20and%20quantification%20of%20occlusal%20discrepancies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12868v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComputer-Aided%2520Design%2520of%2520Personalized%2520Occlusal%2520Positioning%2520Splints%2520Using%250A%2520%2520Multimodal%25203D%2520Data%26entry.906535625%3DAgnieszka%2520Anna%2520Tomaka%2520and%2520Leszek%2520Luchowski%2520and%2520Micha%25C5%2582%2520Tarnawski%2520and%2520Dariusz%2520Pojda%26entry.1292438233%3D%2520%2520Contemporary%2520digital%2520technology%2520has%2520a%2520pivotal%2520role%2520in%2520the%2520design%2520of%250Acustomized%2520medical%2520appliances%252C%2520including%2520occlusal%2520splints%2520used%2520in%2520the%2520treatment%250Aof%2520stomatognathic%2520system%2520dysfunctions.%2520We%2520present%2520an%2520approach%2520to%2520computer-aided%250Adesign%2520and%2520precision%2520assessment%2520of%2520positioning%2520occlusal%2520splints%252C%2520bridging%250Aclinical%2520concepts%2520with%2520current%2520digital%2520dental%2520practice.%2520In%2520our%2520model%252C%2520a%25203D%250Asplint%2520is%2520generated%2520based%2520on%2520a%2520transformation%2520matrix%2520that%2520represents%2520the%250Atherapeutic%2520change%2520in%2520mandibular%2520position%252C%2520defined%2520by%2520a%2520specialist%2520using%2520a%250Avirtual%2520patient%2520model%2520reconstructed%2520from%2520intraoral%2520scans%252C%2520CBCT%252C%25203D%2520facial%2520scans%250Aand%2520plaster%2520model%2520digitisation.%2520The%2520paper%2520introduces%2520a%2520novel%2520method%2520for%250Agenerating%2520splints%2520that%2520accurately%2520reproduce%2520occlusal%2520conditions%2520in%2520the%250Atherapeutic%2520position%252C%2520including%2520a%2520mechanism%2520for%2520resolving%2520surface%2520conflicts%250Athrough%2520virtual%2520embossing.%2520We%2520demonstrate%2520how%2520transformation%2520matrices%2520can%2520be%250Aacquired%2520through%2520clinical%2520tools%2520and%2520intraoral%2520devices%252C%2520and%2520evaluate%2520the%250Aaccuracy%2520of%2520the%2520designed%2520and%2520printed%2520splints%2520using%2520profile%2520and%2520surface%250Adeviation%2520analysis.%2520The%2520proposed%2520method%2520enables%2520reproducible%252C%2520patient-specific%250Asplint%2520fabrication%2520and%2520opens%2520new%2520possibilities%2520in%2520diagnostics%252C%2520multimodal%2520image%250Aregistration%2520and%2520quantification%2520of%2520occlusal%2520discrepancies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12868v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Computer-Aided%20Design%20of%20Personalized%20Occlusal%20Positioning%20Splints%20Using%0A%20%20Multimodal%203D%20Data&entry.906535625=Agnieszka%20Anna%20Tomaka%20and%20Leszek%20Luchowski%20and%20Micha%C5%82%20Tarnawski%20and%20Dariusz%20Pojda&entry.1292438233=%20%20Contemporary%20digital%20technology%20has%20a%20pivotal%20role%20in%20the%20design%20of%0Acustomized%20medical%20appliances%2C%20including%20occlusal%20splints%20used%20in%20the%20treatment%0Aof%20stomatognathic%20system%20dysfunctions.%20We%20present%20an%20approach%20to%20computer-aided%0Adesign%20and%20precision%20assessment%20of%20positioning%20occlusal%20splints%2C%20bridging%0Aclinical%20concepts%20with%20current%20digital%20dental%20practice.%20In%20our%20model%2C%20a%203D%0Asplint%20is%20generated%20based%20on%20a%20transformation%20matrix%20that%20represents%20the%0Atherapeutic%20change%20in%20mandibular%20position%2C%20defined%20by%20a%20specialist%20using%20a%0Avirtual%20patient%20model%20reconstructed%20from%20intraoral%20scans%2C%20CBCT%2C%203D%20facial%20scans%0Aand%20plaster%20model%20digitisation.%20The%20paper%20introduces%20a%20novel%20method%20for%0Agenerating%20splints%20that%20accurately%20reproduce%20occlusal%20conditions%20in%20the%0Atherapeutic%20position%2C%20including%20a%20mechanism%20for%20resolving%20surface%20conflicts%0Athrough%20virtual%20embossing.%20We%20demonstrate%20how%20transformation%20matrices%20can%20be%0Aacquired%20through%20clinical%20tools%20and%20intraoral%20devices%2C%20and%20evaluate%20the%0Aaccuracy%20of%20the%20designed%20and%20printed%20splints%20using%20profile%20and%20surface%0Adeviation%20analysis.%20The%20proposed%20method%20enables%20reproducible%2C%20patient-specific%0Asplint%20fabrication%20and%20opens%20new%20possibilities%20in%20diagnostics%2C%20multimodal%20image%0Aregistration%20and%20quantification%20of%20occlusal%20discrepancies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12868v1&entry.124074799=Read"},
{"title": "Understanding LLM Behaviors via Compression: Data Generation, Knowledge\n  Acquisition and Scaling Laws", "author": "Zhixuan Pan and Shaowen Wang and Jian Li", "abstract": "  Large Language Models (LLMs) have demonstrated remarkable capabilities across\nnumerous tasks, yet principled explanations for their underlying mechanisms and\nseveral phenomena, such as scaling laws, hallucinations, and related behaviors,\nremain elusive. In this work, we revisit the classical relationship between\ncompression and prediction, grounded in Kolmogorov complexity and Shannon\ninformation theory, to provide deeper insights into LLM behaviors. By\nleveraging the Kolmogorov Structure Function and interpreting LLM compression\nas a two-part coding process, we offer a detailed view of how LLMs acquire and\nstore information across increasing model and data scales -- from pervasive\nsyntactic patterns to progressively rarer knowledge elements. Motivated by this\ntheoretical perspective and natural assumptions inspired by Heap's and Zipf's\nlaws, we introduce a simplified yet representative hierarchical data-generation\nframework called the Syntax-Knowledge model. Under the Bayesian setting, we\nshow that prediction and compression within this model naturally lead to\ndiverse learning and scaling behaviors of LLMs. In particular, our theoretical\nanalysis offers intuitive and principled explanations for both data and model\nscaling laws, the dynamics of knowledge acquisition during training and\nfine-tuning, factual knowledge hallucinations in LLMs. The experimental results\nvalidate our theoretical predictions.\n", "link": "http://arxiv.org/abs/2504.09597v2", "date": "2025-04-17", "relevancy": 2.6836, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5592}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5592}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20LLM%20Behaviors%20via%20Compression%3A%20Data%20Generation%2C%20Knowledge%0A%20%20Acquisition%20and%20Scaling%20Laws&body=Title%3A%20Understanding%20LLM%20Behaviors%20via%20Compression%3A%20Data%20Generation%2C%20Knowledge%0A%20%20Acquisition%20and%20Scaling%20Laws%0AAuthor%3A%20Zhixuan%20Pan%20and%20Shaowen%20Wang%20and%20Jian%20Li%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20across%0Anumerous%20tasks%2C%20yet%20principled%20explanations%20for%20their%20underlying%20mechanisms%20and%0Aseveral%20phenomena%2C%20such%20as%20scaling%20laws%2C%20hallucinations%2C%20and%20related%20behaviors%2C%0Aremain%20elusive.%20In%20this%20work%2C%20we%20revisit%20the%20classical%20relationship%20between%0Acompression%20and%20prediction%2C%20grounded%20in%20Kolmogorov%20complexity%20and%20Shannon%0Ainformation%20theory%2C%20to%20provide%20deeper%20insights%20into%20LLM%20behaviors.%20By%0Aleveraging%20the%20Kolmogorov%20Structure%20Function%20and%20interpreting%20LLM%20compression%0Aas%20a%20two-part%20coding%20process%2C%20we%20offer%20a%20detailed%20view%20of%20how%20LLMs%20acquire%20and%0Astore%20information%20across%20increasing%20model%20and%20data%20scales%20--%20from%20pervasive%0Asyntactic%20patterns%20to%20progressively%20rarer%20knowledge%20elements.%20Motivated%20by%20this%0Atheoretical%20perspective%20and%20natural%20assumptions%20inspired%20by%20Heap%27s%20and%20Zipf%27s%0Alaws%2C%20we%20introduce%20a%20simplified%20yet%20representative%20hierarchical%20data-generation%0Aframework%20called%20the%20Syntax-Knowledge%20model.%20Under%20the%20Bayesian%20setting%2C%20we%0Ashow%20that%20prediction%20and%20compression%20within%20this%20model%20naturally%20lead%20to%0Adiverse%20learning%20and%20scaling%20behaviors%20of%20LLMs.%20In%20particular%2C%20our%20theoretical%0Aanalysis%20offers%20intuitive%20and%20principled%20explanations%20for%20both%20data%20and%20model%0Ascaling%20laws%2C%20the%20dynamics%20of%20knowledge%20acquisition%20during%20training%20and%0Afine-tuning%2C%20factual%20knowledge%20hallucinations%20in%20LLMs.%20The%20experimental%20results%0Avalidate%20our%20theoretical%20predictions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.09597v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520LLM%2520Behaviors%2520via%2520Compression%253A%2520Data%2520Generation%252C%2520Knowledge%250A%2520%2520Acquisition%2520and%2520Scaling%2520Laws%26entry.906535625%3DZhixuan%2520Pan%2520and%2520Shaowen%2520Wang%2520and%2520Jian%2520Li%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%2520across%250Anumerous%2520tasks%252C%2520yet%2520principled%2520explanations%2520for%2520their%2520underlying%2520mechanisms%2520and%250Aseveral%2520phenomena%252C%2520such%2520as%2520scaling%2520laws%252C%2520hallucinations%252C%2520and%2520related%2520behaviors%252C%250Aremain%2520elusive.%2520In%2520this%2520work%252C%2520we%2520revisit%2520the%2520classical%2520relationship%2520between%250Acompression%2520and%2520prediction%252C%2520grounded%2520in%2520Kolmogorov%2520complexity%2520and%2520Shannon%250Ainformation%2520theory%252C%2520to%2520provide%2520deeper%2520insights%2520into%2520LLM%2520behaviors.%2520By%250Aleveraging%2520the%2520Kolmogorov%2520Structure%2520Function%2520and%2520interpreting%2520LLM%2520compression%250Aas%2520a%2520two-part%2520coding%2520process%252C%2520we%2520offer%2520a%2520detailed%2520view%2520of%2520how%2520LLMs%2520acquire%2520and%250Astore%2520information%2520across%2520increasing%2520model%2520and%2520data%2520scales%2520--%2520from%2520pervasive%250Asyntactic%2520patterns%2520to%2520progressively%2520rarer%2520knowledge%2520elements.%2520Motivated%2520by%2520this%250Atheoretical%2520perspective%2520and%2520natural%2520assumptions%2520inspired%2520by%2520Heap%2527s%2520and%2520Zipf%2527s%250Alaws%252C%2520we%2520introduce%2520a%2520simplified%2520yet%2520representative%2520hierarchical%2520data-generation%250Aframework%2520called%2520the%2520Syntax-Knowledge%2520model.%2520Under%2520the%2520Bayesian%2520setting%252C%2520we%250Ashow%2520that%2520prediction%2520and%2520compression%2520within%2520this%2520model%2520naturally%2520lead%2520to%250Adiverse%2520learning%2520and%2520scaling%2520behaviors%2520of%2520LLMs.%2520In%2520particular%252C%2520our%2520theoretical%250Aanalysis%2520offers%2520intuitive%2520and%2520principled%2520explanations%2520for%2520both%2520data%2520and%2520model%250Ascaling%2520laws%252C%2520the%2520dynamics%2520of%2520knowledge%2520acquisition%2520during%2520training%2520and%250Afine-tuning%252C%2520factual%2520knowledge%2520hallucinations%2520in%2520LLMs.%2520The%2520experimental%2520results%250Avalidate%2520our%2520theoretical%2520predictions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.09597v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20LLM%20Behaviors%20via%20Compression%3A%20Data%20Generation%2C%20Knowledge%0A%20%20Acquisition%20and%20Scaling%20Laws&entry.906535625=Zhixuan%20Pan%20and%20Shaowen%20Wang%20and%20Jian%20Li&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20across%0Anumerous%20tasks%2C%20yet%20principled%20explanations%20for%20their%20underlying%20mechanisms%20and%0Aseveral%20phenomena%2C%20such%20as%20scaling%20laws%2C%20hallucinations%2C%20and%20related%20behaviors%2C%0Aremain%20elusive.%20In%20this%20work%2C%20we%20revisit%20the%20classical%20relationship%20between%0Acompression%20and%20prediction%2C%20grounded%20in%20Kolmogorov%20complexity%20and%20Shannon%0Ainformation%20theory%2C%20to%20provide%20deeper%20insights%20into%20LLM%20behaviors.%20By%0Aleveraging%20the%20Kolmogorov%20Structure%20Function%20and%20interpreting%20LLM%20compression%0Aas%20a%20two-part%20coding%20process%2C%20we%20offer%20a%20detailed%20view%20of%20how%20LLMs%20acquire%20and%0Astore%20information%20across%20increasing%20model%20and%20data%20scales%20--%20from%20pervasive%0Asyntactic%20patterns%20to%20progressively%20rarer%20knowledge%20elements.%20Motivated%20by%20this%0Atheoretical%20perspective%20and%20natural%20assumptions%20inspired%20by%20Heap%27s%20and%20Zipf%27s%0Alaws%2C%20we%20introduce%20a%20simplified%20yet%20representative%20hierarchical%20data-generation%0Aframework%20called%20the%20Syntax-Knowledge%20model.%20Under%20the%20Bayesian%20setting%2C%20we%0Ashow%20that%20prediction%20and%20compression%20within%20this%20model%20naturally%20lead%20to%0Adiverse%20learning%20and%20scaling%20behaviors%20of%20LLMs.%20In%20particular%2C%20our%20theoretical%0Aanalysis%20offers%20intuitive%20and%20principled%20explanations%20for%20both%20data%20and%20model%0Ascaling%20laws%2C%20the%20dynamics%20of%20knowledge%20acquisition%20during%20training%20and%0Afine-tuning%2C%20factual%20knowledge%20hallucinations%20in%20LLMs.%20The%20experimental%20results%0Avalidate%20our%20theoretical%20predictions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.09597v2&entry.124074799=Read"},
{"title": "It's All Connected: A Journey Through Test-Time Memorization,\n  Attentional Bias, Retention, and Online Optimization", "author": "Ali Behrouz and Meisam Razaviyayn and Peilin Zhong and Vahab Mirrokni", "abstract": "  Designing efficient and effective architectural backbones has been in the\ncore of research efforts to enhance the capability of foundation models.\nInspired by the human cognitive phenomenon of attentional bias-the natural\ntendency to prioritize certain events or stimuli-we reconceptualize neural\narchitectures, including Transformers, Titans, and modern linear recurrent\nneural networks as associative memory modules that learn a mapping of keys and\nvalues using an internal objective, referred to as attentional bias.\nSurprisingly, we observed that most existing sequence models leverage either\n(1) dot-product similarity, or (2) L2 regression objectives as their\nattentional bias. Going beyond these objectives, we present a set of\nalternative attentional bias configurations along with their effective\napproximations to stabilize their training procedure. We then reinterpret\nforgetting mechanisms in modern deep learning architectures as a form of\nretention regularization, providing a novel set of forget gates for sequence\nmodels. Building upon these insights, we present Miras, a general framework to\ndesign deep learning architectures based on four choices of: (i) associative\nmemory architecture, (ii) attentional bias objective, (iii) retention gate, and\n(iv) memory learning algorithm. We present three novel sequence models-Moneta,\nYaad, and Memora-that go beyond the power of existing linear RNNs while\nmaintaining a fast parallelizable training process. Our experiments show\ndifferent design choices in Miras yield models with varying strengths. For\nexample, certain instances of Miras achieve exceptional performance in special\ntasks such as language modeling, commonsense reasoning, and recall intensive\ntasks, even outperforming Transformers and other modern linear recurrent\nmodels.\n", "link": "http://arxiv.org/abs/2504.13173v1", "date": "2025-04-17", "relevancy": 2.6666, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.537}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5315}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20It%27s%20All%20Connected%3A%20A%20Journey%20Through%20Test-Time%20Memorization%2C%0A%20%20Attentional%20Bias%2C%20Retention%2C%20and%20Online%20Optimization&body=Title%3A%20It%27s%20All%20Connected%3A%20A%20Journey%20Through%20Test-Time%20Memorization%2C%0A%20%20Attentional%20Bias%2C%20Retention%2C%20and%20Online%20Optimization%0AAuthor%3A%20Ali%20Behrouz%20and%20Meisam%20Razaviyayn%20and%20Peilin%20Zhong%20and%20Vahab%20Mirrokni%0AAbstract%3A%20%20%20Designing%20efficient%20and%20effective%20architectural%20backbones%20has%20been%20in%20the%0Acore%20of%20research%20efforts%20to%20enhance%20the%20capability%20of%20foundation%20models.%0AInspired%20by%20the%20human%20cognitive%20phenomenon%20of%20attentional%20bias-the%20natural%0Atendency%20to%20prioritize%20certain%20events%20or%20stimuli-we%20reconceptualize%20neural%0Aarchitectures%2C%20including%20Transformers%2C%20Titans%2C%20and%20modern%20linear%20recurrent%0Aneural%20networks%20as%20associative%20memory%20modules%20that%20learn%20a%20mapping%20of%20keys%20and%0Avalues%20using%20an%20internal%20objective%2C%20referred%20to%20as%20attentional%20bias.%0ASurprisingly%2C%20we%20observed%20that%20most%20existing%20sequence%20models%20leverage%20either%0A%281%29%20dot-product%20similarity%2C%20or%20%282%29%20L2%20regression%20objectives%20as%20their%0Aattentional%20bias.%20Going%20beyond%20these%20objectives%2C%20we%20present%20a%20set%20of%0Aalternative%20attentional%20bias%20configurations%20along%20with%20their%20effective%0Aapproximations%20to%20stabilize%20their%20training%20procedure.%20We%20then%20reinterpret%0Aforgetting%20mechanisms%20in%20modern%20deep%20learning%20architectures%20as%20a%20form%20of%0Aretention%20regularization%2C%20providing%20a%20novel%20set%20of%20forget%20gates%20for%20sequence%0Amodels.%20Building%20upon%20these%20insights%2C%20we%20present%20Miras%2C%20a%20general%20framework%20to%0Adesign%20deep%20learning%20architectures%20based%20on%20four%20choices%20of%3A%20%28i%29%20associative%0Amemory%20architecture%2C%20%28ii%29%20attentional%20bias%20objective%2C%20%28iii%29%20retention%20gate%2C%20and%0A%28iv%29%20memory%20learning%20algorithm.%20We%20present%20three%20novel%20sequence%20models-Moneta%2C%0AYaad%2C%20and%20Memora-that%20go%20beyond%20the%20power%20of%20existing%20linear%20RNNs%20while%0Amaintaining%20a%20fast%20parallelizable%20training%20process.%20Our%20experiments%20show%0Adifferent%20design%20choices%20in%20Miras%20yield%20models%20with%20varying%20strengths.%20For%0Aexample%2C%20certain%20instances%20of%20Miras%20achieve%20exceptional%20performance%20in%20special%0Atasks%20such%20as%20language%20modeling%2C%20commonsense%20reasoning%2C%20and%20recall%20intensive%0Atasks%2C%20even%20outperforming%20Transformers%20and%20other%20modern%20linear%20recurrent%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13173v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIt%2527s%2520All%2520Connected%253A%2520A%2520Journey%2520Through%2520Test-Time%2520Memorization%252C%250A%2520%2520Attentional%2520Bias%252C%2520Retention%252C%2520and%2520Online%2520Optimization%26entry.906535625%3DAli%2520Behrouz%2520and%2520Meisam%2520Razaviyayn%2520and%2520Peilin%2520Zhong%2520and%2520Vahab%2520Mirrokni%26entry.1292438233%3D%2520%2520Designing%2520efficient%2520and%2520effective%2520architectural%2520backbones%2520has%2520been%2520in%2520the%250Acore%2520of%2520research%2520efforts%2520to%2520enhance%2520the%2520capability%2520of%2520foundation%2520models.%250AInspired%2520by%2520the%2520human%2520cognitive%2520phenomenon%2520of%2520attentional%2520bias-the%2520natural%250Atendency%2520to%2520prioritize%2520certain%2520events%2520or%2520stimuli-we%2520reconceptualize%2520neural%250Aarchitectures%252C%2520including%2520Transformers%252C%2520Titans%252C%2520and%2520modern%2520linear%2520recurrent%250Aneural%2520networks%2520as%2520associative%2520memory%2520modules%2520that%2520learn%2520a%2520mapping%2520of%2520keys%2520and%250Avalues%2520using%2520an%2520internal%2520objective%252C%2520referred%2520to%2520as%2520attentional%2520bias.%250ASurprisingly%252C%2520we%2520observed%2520that%2520most%2520existing%2520sequence%2520models%2520leverage%2520either%250A%25281%2529%2520dot-product%2520similarity%252C%2520or%2520%25282%2529%2520L2%2520regression%2520objectives%2520as%2520their%250Aattentional%2520bias.%2520Going%2520beyond%2520these%2520objectives%252C%2520we%2520present%2520a%2520set%2520of%250Aalternative%2520attentional%2520bias%2520configurations%2520along%2520with%2520their%2520effective%250Aapproximations%2520to%2520stabilize%2520their%2520training%2520procedure.%2520We%2520then%2520reinterpret%250Aforgetting%2520mechanisms%2520in%2520modern%2520deep%2520learning%2520architectures%2520as%2520a%2520form%2520of%250Aretention%2520regularization%252C%2520providing%2520a%2520novel%2520set%2520of%2520forget%2520gates%2520for%2520sequence%250Amodels.%2520Building%2520upon%2520these%2520insights%252C%2520we%2520present%2520Miras%252C%2520a%2520general%2520framework%2520to%250Adesign%2520deep%2520learning%2520architectures%2520based%2520on%2520four%2520choices%2520of%253A%2520%2528i%2529%2520associative%250Amemory%2520architecture%252C%2520%2528ii%2529%2520attentional%2520bias%2520objective%252C%2520%2528iii%2529%2520retention%2520gate%252C%2520and%250A%2528iv%2529%2520memory%2520learning%2520algorithm.%2520We%2520present%2520three%2520novel%2520sequence%2520models-Moneta%252C%250AYaad%252C%2520and%2520Memora-that%2520go%2520beyond%2520the%2520power%2520of%2520existing%2520linear%2520RNNs%2520while%250Amaintaining%2520a%2520fast%2520parallelizable%2520training%2520process.%2520Our%2520experiments%2520show%250Adifferent%2520design%2520choices%2520in%2520Miras%2520yield%2520models%2520with%2520varying%2520strengths.%2520For%250Aexample%252C%2520certain%2520instances%2520of%2520Miras%2520achieve%2520exceptional%2520performance%2520in%2520special%250Atasks%2520such%2520as%2520language%2520modeling%252C%2520commonsense%2520reasoning%252C%2520and%2520recall%2520intensive%250Atasks%252C%2520even%2520outperforming%2520Transformers%2520and%2520other%2520modern%2520linear%2520recurrent%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13173v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=It%27s%20All%20Connected%3A%20A%20Journey%20Through%20Test-Time%20Memorization%2C%0A%20%20Attentional%20Bias%2C%20Retention%2C%20and%20Online%20Optimization&entry.906535625=Ali%20Behrouz%20and%20Meisam%20Razaviyayn%20and%20Peilin%20Zhong%20and%20Vahab%20Mirrokni&entry.1292438233=%20%20Designing%20efficient%20and%20effective%20architectural%20backbones%20has%20been%20in%20the%0Acore%20of%20research%20efforts%20to%20enhance%20the%20capability%20of%20foundation%20models.%0AInspired%20by%20the%20human%20cognitive%20phenomenon%20of%20attentional%20bias-the%20natural%0Atendency%20to%20prioritize%20certain%20events%20or%20stimuli-we%20reconceptualize%20neural%0Aarchitectures%2C%20including%20Transformers%2C%20Titans%2C%20and%20modern%20linear%20recurrent%0Aneural%20networks%20as%20associative%20memory%20modules%20that%20learn%20a%20mapping%20of%20keys%20and%0Avalues%20using%20an%20internal%20objective%2C%20referred%20to%20as%20attentional%20bias.%0ASurprisingly%2C%20we%20observed%20that%20most%20existing%20sequence%20models%20leverage%20either%0A%281%29%20dot-product%20similarity%2C%20or%20%282%29%20L2%20regression%20objectives%20as%20their%0Aattentional%20bias.%20Going%20beyond%20these%20objectives%2C%20we%20present%20a%20set%20of%0Aalternative%20attentional%20bias%20configurations%20along%20with%20their%20effective%0Aapproximations%20to%20stabilize%20their%20training%20procedure.%20We%20then%20reinterpret%0Aforgetting%20mechanisms%20in%20modern%20deep%20learning%20architectures%20as%20a%20form%20of%0Aretention%20regularization%2C%20providing%20a%20novel%20set%20of%20forget%20gates%20for%20sequence%0Amodels.%20Building%20upon%20these%20insights%2C%20we%20present%20Miras%2C%20a%20general%20framework%20to%0Adesign%20deep%20learning%20architectures%20based%20on%20four%20choices%20of%3A%20%28i%29%20associative%0Amemory%20architecture%2C%20%28ii%29%20attentional%20bias%20objective%2C%20%28iii%29%20retention%20gate%2C%20and%0A%28iv%29%20memory%20learning%20algorithm.%20We%20present%20three%20novel%20sequence%20models-Moneta%2C%0AYaad%2C%20and%20Memora-that%20go%20beyond%20the%20power%20of%20existing%20linear%20RNNs%20while%0Amaintaining%20a%20fast%20parallelizable%20training%20process.%20Our%20experiments%20show%0Adifferent%20design%20choices%20in%20Miras%20yield%20models%20with%20varying%20strengths.%20For%0Aexample%2C%20certain%20instances%20of%20Miras%20achieve%20exceptional%20performance%20in%20special%0Atasks%20such%20as%20language%20modeling%2C%20commonsense%20reasoning%2C%20and%20recall%20intensive%0Atasks%2C%20even%20outperforming%20Transformers%20and%20other%20modern%20linear%20recurrent%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13173v1&entry.124074799=Read"},
{"title": "Hierarchical Feature Learning for Medical Point Clouds via State Space\n  Model", "author": "Guoqing Zhang and Jingyun Yang and Yang Li", "abstract": "  Deep learning-based point cloud modeling has been widely investigated as an\nindispensable component of general shape analysis. Recently, transformer and\nstate space model (SSM) have shown promising capacities in point cloud\nlearning. However, limited research has been conducted on medical point clouds,\nwhich have great potential in disease diagnosis and treatment. This paper\npresents an SSM-based hierarchical feature learning framework for medical point\ncloud understanding. Specifically, we down-sample input into multiple levels\nthrough the farthest point sampling. At each level, we perform a series of\nk-nearest neighbor (KNN) queries to aggregate multi-scale structural\ninformation. To assist SSM in processing point clouds, we introduce\ncoordinate-order and inside-out scanning strategies for efficient serialization\nof irregular points. Point features are calculated progressively from short\nneighbor sequences and long point sequences through vanilla and group Point SSM\nblocks, to capture both local patterns and long-range dependencies. To evaluate\nthe proposed method, we build a large-scale medical point cloud dataset named\nMedPointS for anatomy classification, completion, and segmentation. Extensive\nexperiments conducted on MedPointS demonstrate that our method achieves\nsuperior performance across all tasks. The dataset is available at\nhttps://flemme-docs.readthedocs.io/en/latest/medpoints.html. Code is merged to\na public medical imaging platform: https://github.com/wlsdzyzl/flemme.\n", "link": "http://arxiv.org/abs/2504.13015v1", "date": "2025-04-17", "relevancy": 2.6612, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5629}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5192}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Feature%20Learning%20for%20Medical%20Point%20Clouds%20via%20State%20Space%0A%20%20Model&body=Title%3A%20Hierarchical%20Feature%20Learning%20for%20Medical%20Point%20Clouds%20via%20State%20Space%0A%20%20Model%0AAuthor%3A%20Guoqing%20Zhang%20and%20Jingyun%20Yang%20and%20Yang%20Li%0AAbstract%3A%20%20%20Deep%20learning-based%20point%20cloud%20modeling%20has%20been%20widely%20investigated%20as%20an%0Aindispensable%20component%20of%20general%20shape%20analysis.%20Recently%2C%20transformer%20and%0Astate%20space%20model%20%28SSM%29%20have%20shown%20promising%20capacities%20in%20point%20cloud%0Alearning.%20However%2C%20limited%20research%20has%20been%20conducted%20on%20medical%20point%20clouds%2C%0Awhich%20have%20great%20potential%20in%20disease%20diagnosis%20and%20treatment.%20This%20paper%0Apresents%20an%20SSM-based%20hierarchical%20feature%20learning%20framework%20for%20medical%20point%0Acloud%20understanding.%20Specifically%2C%20we%20down-sample%20input%20into%20multiple%20levels%0Athrough%20the%20farthest%20point%20sampling.%20At%20each%20level%2C%20we%20perform%20a%20series%20of%0Ak-nearest%20neighbor%20%28KNN%29%20queries%20to%20aggregate%20multi-scale%20structural%0Ainformation.%20To%20assist%20SSM%20in%20processing%20point%20clouds%2C%20we%20introduce%0Acoordinate-order%20and%20inside-out%20scanning%20strategies%20for%20efficient%20serialization%0Aof%20irregular%20points.%20Point%20features%20are%20calculated%20progressively%20from%20short%0Aneighbor%20sequences%20and%20long%20point%20sequences%20through%20vanilla%20and%20group%20Point%20SSM%0Ablocks%2C%20to%20capture%20both%20local%20patterns%20and%20long-range%20dependencies.%20To%20evaluate%0Athe%20proposed%20method%2C%20we%20build%20a%20large-scale%20medical%20point%20cloud%20dataset%20named%0AMedPointS%20for%20anatomy%20classification%2C%20completion%2C%20and%20segmentation.%20Extensive%0Aexperiments%20conducted%20on%20MedPointS%20demonstrate%20that%20our%20method%20achieves%0Asuperior%20performance%20across%20all%20tasks.%20The%20dataset%20is%20available%20at%0Ahttps%3A//flemme-docs.readthedocs.io/en/latest/medpoints.html.%20Code%20is%20merged%20to%0Aa%20public%20medical%20imaging%20platform%3A%20https%3A//github.com/wlsdzyzl/flemme.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13015v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Feature%2520Learning%2520for%2520Medical%2520Point%2520Clouds%2520via%2520State%2520Space%250A%2520%2520Model%26entry.906535625%3DGuoqing%2520Zhang%2520and%2520Jingyun%2520Yang%2520and%2520Yang%2520Li%26entry.1292438233%3D%2520%2520Deep%2520learning-based%2520point%2520cloud%2520modeling%2520has%2520been%2520widely%2520investigated%2520as%2520an%250Aindispensable%2520component%2520of%2520general%2520shape%2520analysis.%2520Recently%252C%2520transformer%2520and%250Astate%2520space%2520model%2520%2528SSM%2529%2520have%2520shown%2520promising%2520capacities%2520in%2520point%2520cloud%250Alearning.%2520However%252C%2520limited%2520research%2520has%2520been%2520conducted%2520on%2520medical%2520point%2520clouds%252C%250Awhich%2520have%2520great%2520potential%2520in%2520disease%2520diagnosis%2520and%2520treatment.%2520This%2520paper%250Apresents%2520an%2520SSM-based%2520hierarchical%2520feature%2520learning%2520framework%2520for%2520medical%2520point%250Acloud%2520understanding.%2520Specifically%252C%2520we%2520down-sample%2520input%2520into%2520multiple%2520levels%250Athrough%2520the%2520farthest%2520point%2520sampling.%2520At%2520each%2520level%252C%2520we%2520perform%2520a%2520series%2520of%250Ak-nearest%2520neighbor%2520%2528KNN%2529%2520queries%2520to%2520aggregate%2520multi-scale%2520structural%250Ainformation.%2520To%2520assist%2520SSM%2520in%2520processing%2520point%2520clouds%252C%2520we%2520introduce%250Acoordinate-order%2520and%2520inside-out%2520scanning%2520strategies%2520for%2520efficient%2520serialization%250Aof%2520irregular%2520points.%2520Point%2520features%2520are%2520calculated%2520progressively%2520from%2520short%250Aneighbor%2520sequences%2520and%2520long%2520point%2520sequences%2520through%2520vanilla%2520and%2520group%2520Point%2520SSM%250Ablocks%252C%2520to%2520capture%2520both%2520local%2520patterns%2520and%2520long-range%2520dependencies.%2520To%2520evaluate%250Athe%2520proposed%2520method%252C%2520we%2520build%2520a%2520large-scale%2520medical%2520point%2520cloud%2520dataset%2520named%250AMedPointS%2520for%2520anatomy%2520classification%252C%2520completion%252C%2520and%2520segmentation.%2520Extensive%250Aexperiments%2520conducted%2520on%2520MedPointS%2520demonstrate%2520that%2520our%2520method%2520achieves%250Asuperior%2520performance%2520across%2520all%2520tasks.%2520The%2520dataset%2520is%2520available%2520at%250Ahttps%253A//flemme-docs.readthedocs.io/en/latest/medpoints.html.%2520Code%2520is%2520merged%2520to%250Aa%2520public%2520medical%2520imaging%2520platform%253A%2520https%253A//github.com/wlsdzyzl/flemme.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13015v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Feature%20Learning%20for%20Medical%20Point%20Clouds%20via%20State%20Space%0A%20%20Model&entry.906535625=Guoqing%20Zhang%20and%20Jingyun%20Yang%20and%20Yang%20Li&entry.1292438233=%20%20Deep%20learning-based%20point%20cloud%20modeling%20has%20been%20widely%20investigated%20as%20an%0Aindispensable%20component%20of%20general%20shape%20analysis.%20Recently%2C%20transformer%20and%0Astate%20space%20model%20%28SSM%29%20have%20shown%20promising%20capacities%20in%20point%20cloud%0Alearning.%20However%2C%20limited%20research%20has%20been%20conducted%20on%20medical%20point%20clouds%2C%0Awhich%20have%20great%20potential%20in%20disease%20diagnosis%20and%20treatment.%20This%20paper%0Apresents%20an%20SSM-based%20hierarchical%20feature%20learning%20framework%20for%20medical%20point%0Acloud%20understanding.%20Specifically%2C%20we%20down-sample%20input%20into%20multiple%20levels%0Athrough%20the%20farthest%20point%20sampling.%20At%20each%20level%2C%20we%20perform%20a%20series%20of%0Ak-nearest%20neighbor%20%28KNN%29%20queries%20to%20aggregate%20multi-scale%20structural%0Ainformation.%20To%20assist%20SSM%20in%20processing%20point%20clouds%2C%20we%20introduce%0Acoordinate-order%20and%20inside-out%20scanning%20strategies%20for%20efficient%20serialization%0Aof%20irregular%20points.%20Point%20features%20are%20calculated%20progressively%20from%20short%0Aneighbor%20sequences%20and%20long%20point%20sequences%20through%20vanilla%20and%20group%20Point%20SSM%0Ablocks%2C%20to%20capture%20both%20local%20patterns%20and%20long-range%20dependencies.%20To%20evaluate%0Athe%20proposed%20method%2C%20we%20build%20a%20large-scale%20medical%20point%20cloud%20dataset%20named%0AMedPointS%20for%20anatomy%20classification%2C%20completion%2C%20and%20segmentation.%20Extensive%0Aexperiments%20conducted%20on%20MedPointS%20demonstrate%20that%20our%20method%20achieves%0Asuperior%20performance%20across%20all%20tasks.%20The%20dataset%20is%20available%20at%0Ahttps%3A//flemme-docs.readthedocs.io/en/latest/medpoints.html.%20Code%20is%20merged%20to%0Aa%20public%20medical%20imaging%20platform%3A%20https%3A//github.com/wlsdzyzl/flemme.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13015v1&entry.124074799=Read"},
{"title": "Enhancing Person-to-Person Virtual Try-On with Multi-Garment Virtual\n  Try-Off", "author": "Riza Velioglu and Petra Bevandic and Robin Chan and Barbara Hammer", "abstract": "  Computer vision is transforming fashion through Virtual Try-On (VTON) and\nVirtual Try-Off (VTOFF). VTON generates images of a person in a specified\ngarment using a target photo and a standardized garment image, while a more\nchallenging variant, Person-to-Person Virtual Try-On (p2p-VTON), uses a photo\nof another person wearing the garment. VTOFF, on the other hand, extracts\nstandardized garment images from clothed individuals. We introduce TryOffDiff,\na diffusion-based VTOFF model. Built on a latent diffusion framework with\nSigLIP image conditioning, it effectively captures garment properties like\ntexture, shape, and patterns. TryOffDiff achieves state-of-the-art results on\nVITON-HD and strong performance on DressCode dataset, covering upper-body,\nlower-body, and dresses. Enhanced with class-specific embeddings, it pioneers\nmulti-garment VTOFF, the first of its kind. When paired with VTON models, it\nimproves p2p-VTON by minimizing unwanted attribute transfer, such as skin\ncolor. Code is available at: https://rizavelioglu.github.io/tryoffdiff/\n", "link": "http://arxiv.org/abs/2504.13078v1", "date": "2025-04-17", "relevancy": 2.654, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6653}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6645}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Person-to-Person%20Virtual%20Try-On%20with%20Multi-Garment%20Virtual%0A%20%20Try-Off&body=Title%3A%20Enhancing%20Person-to-Person%20Virtual%20Try-On%20with%20Multi-Garment%20Virtual%0A%20%20Try-Off%0AAuthor%3A%20Riza%20Velioglu%20and%20Petra%20Bevandic%20and%20Robin%20Chan%20and%20Barbara%20Hammer%0AAbstract%3A%20%20%20Computer%20vision%20is%20transforming%20fashion%20through%20Virtual%20Try-On%20%28VTON%29%20and%0AVirtual%20Try-Off%20%28VTOFF%29.%20VTON%20generates%20images%20of%20a%20person%20in%20a%20specified%0Agarment%20using%20a%20target%20photo%20and%20a%20standardized%20garment%20image%2C%20while%20a%20more%0Achallenging%20variant%2C%20Person-to-Person%20Virtual%20Try-On%20%28p2p-VTON%29%2C%20uses%20a%20photo%0Aof%20another%20person%20wearing%20the%20garment.%20VTOFF%2C%20on%20the%20other%20hand%2C%20extracts%0Astandardized%20garment%20images%20from%20clothed%20individuals.%20We%20introduce%20TryOffDiff%2C%0Aa%20diffusion-based%20VTOFF%20model.%20Built%20on%20a%20latent%20diffusion%20framework%20with%0ASigLIP%20image%20conditioning%2C%20it%20effectively%20captures%20garment%20properties%20like%0Atexture%2C%20shape%2C%20and%20patterns.%20TryOffDiff%20achieves%20state-of-the-art%20results%20on%0AVITON-HD%20and%20strong%20performance%20on%20DressCode%20dataset%2C%20covering%20upper-body%2C%0Alower-body%2C%20and%20dresses.%20Enhanced%20with%20class-specific%20embeddings%2C%20it%20pioneers%0Amulti-garment%20VTOFF%2C%20the%20first%20of%20its%20kind.%20When%20paired%20with%20VTON%20models%2C%20it%0Aimproves%20p2p-VTON%20by%20minimizing%20unwanted%20attribute%20transfer%2C%20such%20as%20skin%0Acolor.%20Code%20is%20available%20at%3A%20https%3A//rizavelioglu.github.io/tryoffdiff/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13078v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Person-to-Person%2520Virtual%2520Try-On%2520with%2520Multi-Garment%2520Virtual%250A%2520%2520Try-Off%26entry.906535625%3DRiza%2520Velioglu%2520and%2520Petra%2520Bevandic%2520and%2520Robin%2520Chan%2520and%2520Barbara%2520Hammer%26entry.1292438233%3D%2520%2520Computer%2520vision%2520is%2520transforming%2520fashion%2520through%2520Virtual%2520Try-On%2520%2528VTON%2529%2520and%250AVirtual%2520Try-Off%2520%2528VTOFF%2529.%2520VTON%2520generates%2520images%2520of%2520a%2520person%2520in%2520a%2520specified%250Agarment%2520using%2520a%2520target%2520photo%2520and%2520a%2520standardized%2520garment%2520image%252C%2520while%2520a%2520more%250Achallenging%2520variant%252C%2520Person-to-Person%2520Virtual%2520Try-On%2520%2528p2p-VTON%2529%252C%2520uses%2520a%2520photo%250Aof%2520another%2520person%2520wearing%2520the%2520garment.%2520VTOFF%252C%2520on%2520the%2520other%2520hand%252C%2520extracts%250Astandardized%2520garment%2520images%2520from%2520clothed%2520individuals.%2520We%2520introduce%2520TryOffDiff%252C%250Aa%2520diffusion-based%2520VTOFF%2520model.%2520Built%2520on%2520a%2520latent%2520diffusion%2520framework%2520with%250ASigLIP%2520image%2520conditioning%252C%2520it%2520effectively%2520captures%2520garment%2520properties%2520like%250Atexture%252C%2520shape%252C%2520and%2520patterns.%2520TryOffDiff%2520achieves%2520state-of-the-art%2520results%2520on%250AVITON-HD%2520and%2520strong%2520performance%2520on%2520DressCode%2520dataset%252C%2520covering%2520upper-body%252C%250Alower-body%252C%2520and%2520dresses.%2520Enhanced%2520with%2520class-specific%2520embeddings%252C%2520it%2520pioneers%250Amulti-garment%2520VTOFF%252C%2520the%2520first%2520of%2520its%2520kind.%2520When%2520paired%2520with%2520VTON%2520models%252C%2520it%250Aimproves%2520p2p-VTON%2520by%2520minimizing%2520unwanted%2520attribute%2520transfer%252C%2520such%2520as%2520skin%250Acolor.%2520Code%2520is%2520available%2520at%253A%2520https%253A//rizavelioglu.github.io/tryoffdiff/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13078v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Person-to-Person%20Virtual%20Try-On%20with%20Multi-Garment%20Virtual%0A%20%20Try-Off&entry.906535625=Riza%20Velioglu%20and%20Petra%20Bevandic%20and%20Robin%20Chan%20and%20Barbara%20Hammer&entry.1292438233=%20%20Computer%20vision%20is%20transforming%20fashion%20through%20Virtual%20Try-On%20%28VTON%29%20and%0AVirtual%20Try-Off%20%28VTOFF%29.%20VTON%20generates%20images%20of%20a%20person%20in%20a%20specified%0Agarment%20using%20a%20target%20photo%20and%20a%20standardized%20garment%20image%2C%20while%20a%20more%0Achallenging%20variant%2C%20Person-to-Person%20Virtual%20Try-On%20%28p2p-VTON%29%2C%20uses%20a%20photo%0Aof%20another%20person%20wearing%20the%20garment.%20VTOFF%2C%20on%20the%20other%20hand%2C%20extracts%0Astandardized%20garment%20images%20from%20clothed%20individuals.%20We%20introduce%20TryOffDiff%2C%0Aa%20diffusion-based%20VTOFF%20model.%20Built%20on%20a%20latent%20diffusion%20framework%20with%0ASigLIP%20image%20conditioning%2C%20it%20effectively%20captures%20garment%20properties%20like%0Atexture%2C%20shape%2C%20and%20patterns.%20TryOffDiff%20achieves%20state-of-the-art%20results%20on%0AVITON-HD%20and%20strong%20performance%20on%20DressCode%20dataset%2C%20covering%20upper-body%2C%0Alower-body%2C%20and%20dresses.%20Enhanced%20with%20class-specific%20embeddings%2C%20it%20pioneers%0Amulti-garment%20VTOFF%2C%20the%20first%20of%20its%20kind.%20When%20paired%20with%20VTON%20models%2C%20it%0Aimproves%20p2p-VTON%20by%20minimizing%20unwanted%20attribute%20transfer%2C%20such%20as%20skin%0Acolor.%20Code%20is%20available%20at%3A%20https%3A//rizavelioglu.github.io/tryoffdiff/%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13078v1&entry.124074799=Read"},
{"title": "Causal integration of chemical structures improves representations of\n  microscopy images for morphological profiling", "author": "Yemin Yu and Neil Tenenholtz and Lester Mackey and Ying Wei and David Alvarez-Melis and Ava P. Amini and Alex X. Lu", "abstract": "  Recent advances in self-supervised deep learning have improved our ability to\nquantify cellular morphological changes in high-throughput microscopy screens,\na process known as morphological profiling. However, most current methods only\nlearn from images, despite many screens being inherently multimodal, as they\ninvolve both a chemical or genetic perturbation as well as an image-based\nreadout. We hypothesized that incorporating chemical compound structure during\nself-supervised pre-training could improve learned representations of images in\nhigh-throughput microscopy screens. We introduce a representation learning\nframework, MICON (Molecular-Image Contrastive Learning), that models chemical\ncompounds as treatments that induce counterfactual transformations of cell\nphenotypes. MICON significantly outperforms classical hand-crafted features\nsuch as CellProfiler and existing deep-learning-based representation learning\nmethods in challenging evaluation settings where models must identify\nreproducible effects of drugs across independent replicates and data-generating\ncenters. We demonstrate that incorporating chemical compound information into\nthe learning process provides consistent improvements in our evaluation setting\nand that modeling compounds specifically as treatments in a causal framework\noutperforms approaches that directly align images and compounds in a single\nrepresentation space. Our findings point to a new direction for representation\nlearning in morphological profiling, suggesting that methods should explicitly\naccount for the multimodal nature of microscopy screening data.\n", "link": "http://arxiv.org/abs/2504.09544v2", "date": "2025-04-16", "relevancy": 2.6367, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5652}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5105}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal%20integration%20of%20chemical%20structures%20improves%20representations%20of%0A%20%20microscopy%20images%20for%20morphological%20profiling&body=Title%3A%20Causal%20integration%20of%20chemical%20structures%20improves%20representations%20of%0A%20%20microscopy%20images%20for%20morphological%20profiling%0AAuthor%3A%20Yemin%20Yu%20and%20Neil%20Tenenholtz%20and%20Lester%20Mackey%20and%20Ying%20Wei%20and%20David%20Alvarez-Melis%20and%20Ava%20P.%20Amini%20and%20Alex%20X.%20Lu%0AAbstract%3A%20%20%20Recent%20advances%20in%20self-supervised%20deep%20learning%20have%20improved%20our%20ability%20to%0Aquantify%20cellular%20morphological%20changes%20in%20high-throughput%20microscopy%20screens%2C%0Aa%20process%20known%20as%20morphological%20profiling.%20However%2C%20most%20current%20methods%20only%0Alearn%20from%20images%2C%20despite%20many%20screens%20being%20inherently%20multimodal%2C%20as%20they%0Ainvolve%20both%20a%20chemical%20or%20genetic%20perturbation%20as%20well%20as%20an%20image-based%0Areadout.%20We%20hypothesized%20that%20incorporating%20chemical%20compound%20structure%20during%0Aself-supervised%20pre-training%20could%20improve%20learned%20representations%20of%20images%20in%0Ahigh-throughput%20microscopy%20screens.%20We%20introduce%20a%20representation%20learning%0Aframework%2C%20MICON%20%28Molecular-Image%20Contrastive%20Learning%29%2C%20that%20models%20chemical%0Acompounds%20as%20treatments%20that%20induce%20counterfactual%20transformations%20of%20cell%0Aphenotypes.%20MICON%20significantly%20outperforms%20classical%20hand-crafted%20features%0Asuch%20as%20CellProfiler%20and%20existing%20deep-learning-based%20representation%20learning%0Amethods%20in%20challenging%20evaluation%20settings%20where%20models%20must%20identify%0Areproducible%20effects%20of%20drugs%20across%20independent%20replicates%20and%20data-generating%0Acenters.%20We%20demonstrate%20that%20incorporating%20chemical%20compound%20information%20into%0Athe%20learning%20process%20provides%20consistent%20improvements%20in%20our%20evaluation%20setting%0Aand%20that%20modeling%20compounds%20specifically%20as%20treatments%20in%20a%20causal%20framework%0Aoutperforms%20approaches%20that%20directly%20align%20images%20and%20compounds%20in%20a%20single%0Arepresentation%20space.%20Our%20findings%20point%20to%20a%20new%20direction%20for%20representation%0Alearning%20in%20morphological%20profiling%2C%20suggesting%20that%20methods%20should%20explicitly%0Aaccount%20for%20the%20multimodal%20nature%20of%20microscopy%20screening%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.09544v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal%2520integration%2520of%2520chemical%2520structures%2520improves%2520representations%2520of%250A%2520%2520microscopy%2520images%2520for%2520morphological%2520profiling%26entry.906535625%3DYemin%2520Yu%2520and%2520Neil%2520Tenenholtz%2520and%2520Lester%2520Mackey%2520and%2520Ying%2520Wei%2520and%2520David%2520Alvarez-Melis%2520and%2520Ava%2520P.%2520Amini%2520and%2520Alex%2520X.%2520Lu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520self-supervised%2520deep%2520learning%2520have%2520improved%2520our%2520ability%2520to%250Aquantify%2520cellular%2520morphological%2520changes%2520in%2520high-throughput%2520microscopy%2520screens%252C%250Aa%2520process%2520known%2520as%2520morphological%2520profiling.%2520However%252C%2520most%2520current%2520methods%2520only%250Alearn%2520from%2520images%252C%2520despite%2520many%2520screens%2520being%2520inherently%2520multimodal%252C%2520as%2520they%250Ainvolve%2520both%2520a%2520chemical%2520or%2520genetic%2520perturbation%2520as%2520well%2520as%2520an%2520image-based%250Areadout.%2520We%2520hypothesized%2520that%2520incorporating%2520chemical%2520compound%2520structure%2520during%250Aself-supervised%2520pre-training%2520could%2520improve%2520learned%2520representations%2520of%2520images%2520in%250Ahigh-throughput%2520microscopy%2520screens.%2520We%2520introduce%2520a%2520representation%2520learning%250Aframework%252C%2520MICON%2520%2528Molecular-Image%2520Contrastive%2520Learning%2529%252C%2520that%2520models%2520chemical%250Acompounds%2520as%2520treatments%2520that%2520induce%2520counterfactual%2520transformations%2520of%2520cell%250Aphenotypes.%2520MICON%2520significantly%2520outperforms%2520classical%2520hand-crafted%2520features%250Asuch%2520as%2520CellProfiler%2520and%2520existing%2520deep-learning-based%2520representation%2520learning%250Amethods%2520in%2520challenging%2520evaluation%2520settings%2520where%2520models%2520must%2520identify%250Areproducible%2520effects%2520of%2520drugs%2520across%2520independent%2520replicates%2520and%2520data-generating%250Acenters.%2520We%2520demonstrate%2520that%2520incorporating%2520chemical%2520compound%2520information%2520into%250Athe%2520learning%2520process%2520provides%2520consistent%2520improvements%2520in%2520our%2520evaluation%2520setting%250Aand%2520that%2520modeling%2520compounds%2520specifically%2520as%2520treatments%2520in%2520a%2520causal%2520framework%250Aoutperforms%2520approaches%2520that%2520directly%2520align%2520images%2520and%2520compounds%2520in%2520a%2520single%250Arepresentation%2520space.%2520Our%2520findings%2520point%2520to%2520a%2520new%2520direction%2520for%2520representation%250Alearning%2520in%2520morphological%2520profiling%252C%2520suggesting%2520that%2520methods%2520should%2520explicitly%250Aaccount%2520for%2520the%2520multimodal%2520nature%2520of%2520microscopy%2520screening%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.09544v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal%20integration%20of%20chemical%20structures%20improves%20representations%20of%0A%20%20microscopy%20images%20for%20morphological%20profiling&entry.906535625=Yemin%20Yu%20and%20Neil%20Tenenholtz%20and%20Lester%20Mackey%20and%20Ying%20Wei%20and%20David%20Alvarez-Melis%20and%20Ava%20P.%20Amini%20and%20Alex%20X.%20Lu&entry.1292438233=%20%20Recent%20advances%20in%20self-supervised%20deep%20learning%20have%20improved%20our%20ability%20to%0Aquantify%20cellular%20morphological%20changes%20in%20high-throughput%20microscopy%20screens%2C%0Aa%20process%20known%20as%20morphological%20profiling.%20However%2C%20most%20current%20methods%20only%0Alearn%20from%20images%2C%20despite%20many%20screens%20being%20inherently%20multimodal%2C%20as%20they%0Ainvolve%20both%20a%20chemical%20or%20genetic%20perturbation%20as%20well%20as%20an%20image-based%0Areadout.%20We%20hypothesized%20that%20incorporating%20chemical%20compound%20structure%20during%0Aself-supervised%20pre-training%20could%20improve%20learned%20representations%20of%20images%20in%0Ahigh-throughput%20microscopy%20screens.%20We%20introduce%20a%20representation%20learning%0Aframework%2C%20MICON%20%28Molecular-Image%20Contrastive%20Learning%29%2C%20that%20models%20chemical%0Acompounds%20as%20treatments%20that%20induce%20counterfactual%20transformations%20of%20cell%0Aphenotypes.%20MICON%20significantly%20outperforms%20classical%20hand-crafted%20features%0Asuch%20as%20CellProfiler%20and%20existing%20deep-learning-based%20representation%20learning%0Amethods%20in%20challenging%20evaluation%20settings%20where%20models%20must%20identify%0Areproducible%20effects%20of%20drugs%20across%20independent%20replicates%20and%20data-generating%0Acenters.%20We%20demonstrate%20that%20incorporating%20chemical%20compound%20information%20into%0Athe%20learning%20process%20provides%20consistent%20improvements%20in%20our%20evaluation%20setting%0Aand%20that%20modeling%20compounds%20specifically%20as%20treatments%20in%20a%20causal%20framework%0Aoutperforms%20approaches%20that%20directly%20align%20images%20and%20compounds%20in%20a%20single%0Arepresentation%20space.%20Our%20findings%20point%20to%20a%20new%20direction%20for%20representation%0Alearning%20in%20morphological%20profiling%2C%20suggesting%20that%20methods%20should%20explicitly%0Aaccount%20for%20the%20multimodal%20nature%20of%20microscopy%20screening%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.09544v2&entry.124074799=Read"},
{"title": "Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and\n  Model Size in Large Language Models From Edge to Giant", "author": "Jemin Lee and Sihyeong Park and Jinse Kwon and Jihun Oh and Yongin Kwon", "abstract": "  Quantization has gained attention as a promising solution for the\ncost-effective deployment of large and small language models. However, most\nprior work has been limited to perplexity or basic knowledge tasks and lacks a\ncomprehensive evaluation of recent models like Llama-3.3. In this paper, we\nconduct a comprehensive evaluation of instruction-tuned models spanning 1B to\n405B parameters, applying four quantization methods across 13 datasets. Our\nfindings reveal that (1) quantized models generally surpass smaller FP16\nbaselines, yet they often struggle with instruction-following and hallucination\ndetection; (2) FP8 consistently emerges as the most robust option across tasks,\nand AWQ tends to outperform GPTQ in weight-only quantization; (3) smaller\nmodels can suffer severe accuracy drops at 4-bit quantization, while 70B-scale\nmodels maintain stable performance; (4) notably, \\textit{hard} tasks do not\nalways experience the largest accuracy losses, indicating that quantization\nmagnifies a model's inherent weaknesses rather than simply correlating with\ntask difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant\nperformance declines in coding and STEM tasks, though reasoning may sometimes\nimprove.\n", "link": "http://arxiv.org/abs/2409.11055v2", "date": "2025-04-17", "relevancy": 2.6103, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.535}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.535}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Trade-Offs%3A%20Quantization%20Methods%2C%20Task%20Difficulty%2C%20and%0A%20%20Model%20Size%20in%20Large%20Language%20Models%20From%20Edge%20to%20Giant&body=Title%3A%20Exploring%20the%20Trade-Offs%3A%20Quantization%20Methods%2C%20Task%20Difficulty%2C%20and%0A%20%20Model%20Size%20in%20Large%20Language%20Models%20From%20Edge%20to%20Giant%0AAuthor%3A%20Jemin%20Lee%20and%20Sihyeong%20Park%20and%20Jinse%20Kwon%20and%20Jihun%20Oh%20and%20Yongin%20Kwon%0AAbstract%3A%20%20%20Quantization%20has%20gained%20attention%20as%20a%20promising%20solution%20for%20the%0Acost-effective%20deployment%20of%20large%20and%20small%20language%20models.%20However%2C%20most%0Aprior%20work%20has%20been%20limited%20to%20perplexity%20or%20basic%20knowledge%20tasks%20and%20lacks%20a%0Acomprehensive%20evaluation%20of%20recent%20models%20like%20Llama-3.3.%20In%20this%20paper%2C%20we%0Aconduct%20a%20comprehensive%20evaluation%20of%20instruction-tuned%20models%20spanning%201B%20to%0A405B%20parameters%2C%20applying%20four%20quantization%20methods%20across%2013%20datasets.%20Our%0Afindings%20reveal%20that%20%281%29%20quantized%20models%20generally%20surpass%20smaller%20FP16%0Abaselines%2C%20yet%20they%20often%20struggle%20with%20instruction-following%20and%20hallucination%0Adetection%3B%20%282%29%20FP8%20consistently%20emerges%20as%20the%20most%20robust%20option%20across%20tasks%2C%0Aand%20AWQ%20tends%20to%20outperform%20GPTQ%20in%20weight-only%20quantization%3B%20%283%29%20smaller%0Amodels%20can%20suffer%20severe%20accuracy%20drops%20at%204-bit%20quantization%2C%20while%2070B-scale%0Amodels%20maintain%20stable%20performance%3B%20%284%29%20notably%2C%20%5Ctextit%7Bhard%7D%20tasks%20do%20not%0Aalways%20experience%20the%20largest%20accuracy%20losses%2C%20indicating%20that%20quantization%0Amagnifies%20a%20model%27s%20inherent%20weaknesses%20rather%20than%20simply%20correlating%20with%0Atask%20difficulty%3B%20and%20%285%29%20an%20LLM-based%20judge%20%28MT-Bench%29%20highlights%20significant%0Aperformance%20declines%20in%20coding%20and%20STEM%20tasks%2C%20though%20reasoning%20may%20sometimes%0Aimprove.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11055v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Trade-Offs%253A%2520Quantization%2520Methods%252C%2520Task%2520Difficulty%252C%2520and%250A%2520%2520Model%2520Size%2520in%2520Large%2520Language%2520Models%2520From%2520Edge%2520to%2520Giant%26entry.906535625%3DJemin%2520Lee%2520and%2520Sihyeong%2520Park%2520and%2520Jinse%2520Kwon%2520and%2520Jihun%2520Oh%2520and%2520Yongin%2520Kwon%26entry.1292438233%3D%2520%2520Quantization%2520has%2520gained%2520attention%2520as%2520a%2520promising%2520solution%2520for%2520the%250Acost-effective%2520deployment%2520of%2520large%2520and%2520small%2520language%2520models.%2520However%252C%2520most%250Aprior%2520work%2520has%2520been%2520limited%2520to%2520perplexity%2520or%2520basic%2520knowledge%2520tasks%2520and%2520lacks%2520a%250Acomprehensive%2520evaluation%2520of%2520recent%2520models%2520like%2520Llama-3.3.%2520In%2520this%2520paper%252C%2520we%250Aconduct%2520a%2520comprehensive%2520evaluation%2520of%2520instruction-tuned%2520models%2520spanning%25201B%2520to%250A405B%2520parameters%252C%2520applying%2520four%2520quantization%2520methods%2520across%252013%2520datasets.%2520Our%250Afindings%2520reveal%2520that%2520%25281%2529%2520quantized%2520models%2520generally%2520surpass%2520smaller%2520FP16%250Abaselines%252C%2520yet%2520they%2520often%2520struggle%2520with%2520instruction-following%2520and%2520hallucination%250Adetection%253B%2520%25282%2529%2520FP8%2520consistently%2520emerges%2520as%2520the%2520most%2520robust%2520option%2520across%2520tasks%252C%250Aand%2520AWQ%2520tends%2520to%2520outperform%2520GPTQ%2520in%2520weight-only%2520quantization%253B%2520%25283%2529%2520smaller%250Amodels%2520can%2520suffer%2520severe%2520accuracy%2520drops%2520at%25204-bit%2520quantization%252C%2520while%252070B-scale%250Amodels%2520maintain%2520stable%2520performance%253B%2520%25284%2529%2520notably%252C%2520%255Ctextit%257Bhard%257D%2520tasks%2520do%2520not%250Aalways%2520experience%2520the%2520largest%2520accuracy%2520losses%252C%2520indicating%2520that%2520quantization%250Amagnifies%2520a%2520model%2527s%2520inherent%2520weaknesses%2520rather%2520than%2520simply%2520correlating%2520with%250Atask%2520difficulty%253B%2520and%2520%25285%2529%2520an%2520LLM-based%2520judge%2520%2528MT-Bench%2529%2520highlights%2520significant%250Aperformance%2520declines%2520in%2520coding%2520and%2520STEM%2520tasks%252C%2520though%2520reasoning%2520may%2520sometimes%250Aimprove.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11055v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Trade-Offs%3A%20Quantization%20Methods%2C%20Task%20Difficulty%2C%20and%0A%20%20Model%20Size%20in%20Large%20Language%20Models%20From%20Edge%20to%20Giant&entry.906535625=Jemin%20Lee%20and%20Sihyeong%20Park%20and%20Jinse%20Kwon%20and%20Jihun%20Oh%20and%20Yongin%20Kwon&entry.1292438233=%20%20Quantization%20has%20gained%20attention%20as%20a%20promising%20solution%20for%20the%0Acost-effective%20deployment%20of%20large%20and%20small%20language%20models.%20However%2C%20most%0Aprior%20work%20has%20been%20limited%20to%20perplexity%20or%20basic%20knowledge%20tasks%20and%20lacks%20a%0Acomprehensive%20evaluation%20of%20recent%20models%20like%20Llama-3.3.%20In%20this%20paper%2C%20we%0Aconduct%20a%20comprehensive%20evaluation%20of%20instruction-tuned%20models%20spanning%201B%20to%0A405B%20parameters%2C%20applying%20four%20quantization%20methods%20across%2013%20datasets.%20Our%0Afindings%20reveal%20that%20%281%29%20quantized%20models%20generally%20surpass%20smaller%20FP16%0Abaselines%2C%20yet%20they%20often%20struggle%20with%20instruction-following%20and%20hallucination%0Adetection%3B%20%282%29%20FP8%20consistently%20emerges%20as%20the%20most%20robust%20option%20across%20tasks%2C%0Aand%20AWQ%20tends%20to%20outperform%20GPTQ%20in%20weight-only%20quantization%3B%20%283%29%20smaller%0Amodels%20can%20suffer%20severe%20accuracy%20drops%20at%204-bit%20quantization%2C%20while%2070B-scale%0Amodels%20maintain%20stable%20performance%3B%20%284%29%20notably%2C%20%5Ctextit%7Bhard%7D%20tasks%20do%20not%0Aalways%20experience%20the%20largest%20accuracy%20losses%2C%20indicating%20that%20quantization%0Amagnifies%20a%20model%27s%20inherent%20weaknesses%20rather%20than%20simply%20correlating%20with%0Atask%20difficulty%3B%20and%20%285%29%20an%20LLM-based%20judge%20%28MT-Bench%29%20highlights%20significant%0Aperformance%20declines%20in%20coding%20and%20STEM%20tasks%2C%20though%20reasoning%20may%20sometimes%0Aimprove.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11055v2&entry.124074799=Read"},
{"title": "Large Language Models as Attribution Regularizers for Efficient Model\n  Training", "author": "Davor Vukadin and Marin \u0160ili\u0107 and Goran Dela\u010d", "abstract": "  Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse domains. However, effectively leveraging their vast knowledge for\ntraining smaller downstream models remains an open challenge, especially in\ndomains like tabular data learning, where simpler models are often preferred\ndue to interpretability and efficiency.\n  In this paper, we introduce a novel yet straightforward method for\nincorporating LLM-generated global task feature attributions into the training\nprocess of smaller networks. Specifically, we propose an attribution-matching\nregularization term that aligns the training dynamics of the smaller model with\nthe insights provided by the LLM. By doing so, our approach yields superior\nperformance in few-shot learning scenarios. Notably, our method requires only\nblack-box API access to the LLM, making it easy to integrate into existing\ntraining pipelines with minimal computational overhead.\n  Furthermore, we demonstrate how this method can be used to address common\nissues in real-world datasets, such as skewness and bias. By integrating\nhigh-level knowledge from LLMs, our approach improves generalization, even when\ntraining data is limited or imbalanced. We validate its effectiveness through\nextensive experiments across multiple tasks, demonstrating improved learning\nefficiency and model robustness.\n", "link": "http://arxiv.org/abs/2502.20268v2", "date": "2025-04-17", "relevancy": 2.6084, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5445}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5121}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20as%20Attribution%20Regularizers%20for%20Efficient%20Model%0A%20%20Training&body=Title%3A%20Large%20Language%20Models%20as%20Attribution%20Regularizers%20for%20Efficient%20Model%0A%20%20Training%0AAuthor%3A%20Davor%20Vukadin%20and%20Marin%20%C5%A0ili%C4%87%20and%20Goran%20Dela%C4%8D%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20performance%20across%0Adiverse%20domains.%20However%2C%20effectively%20leveraging%20their%20vast%20knowledge%20for%0Atraining%20smaller%20downstream%20models%20remains%20an%20open%20challenge%2C%20especially%20in%0Adomains%20like%20tabular%20data%20learning%2C%20where%20simpler%20models%20are%20often%20preferred%0Adue%20to%20interpretability%20and%20efficiency.%0A%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20yet%20straightforward%20method%20for%0Aincorporating%20LLM-generated%20global%20task%20feature%20attributions%20into%20the%20training%0Aprocess%20of%20smaller%20networks.%20Specifically%2C%20we%20propose%20an%20attribution-matching%0Aregularization%20term%20that%20aligns%20the%20training%20dynamics%20of%20the%20smaller%20model%20with%0Athe%20insights%20provided%20by%20the%20LLM.%20By%20doing%20so%2C%20our%20approach%20yields%20superior%0Aperformance%20in%20few-shot%20learning%20scenarios.%20Notably%2C%20our%20method%20requires%20only%0Ablack-box%20API%20access%20to%20the%20LLM%2C%20making%20it%20easy%20to%20integrate%20into%20existing%0Atraining%20pipelines%20with%20minimal%20computational%20overhead.%0A%20%20Furthermore%2C%20we%20demonstrate%20how%20this%20method%20can%20be%20used%20to%20address%20common%0Aissues%20in%20real-world%20datasets%2C%20such%20as%20skewness%20and%20bias.%20By%20integrating%0Ahigh-level%20knowledge%20from%20LLMs%2C%20our%20approach%20improves%20generalization%2C%20even%20when%0Atraining%20data%20is%20limited%20or%20imbalanced.%20We%20validate%20its%20effectiveness%20through%0Aextensive%20experiments%20across%20multiple%20tasks%2C%20demonstrating%20improved%20learning%0Aefficiency%20and%20model%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.20268v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520as%2520Attribution%2520Regularizers%2520for%2520Efficient%2520Model%250A%2520%2520Training%26entry.906535625%3DDavor%2520Vukadin%2520and%2520Marin%2520%25C5%25A0ili%25C4%2587%2520and%2520Goran%2520Dela%25C4%258D%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520performance%2520across%250Adiverse%2520domains.%2520However%252C%2520effectively%2520leveraging%2520their%2520vast%2520knowledge%2520for%250Atraining%2520smaller%2520downstream%2520models%2520remains%2520an%2520open%2520challenge%252C%2520especially%2520in%250Adomains%2520like%2520tabular%2520data%2520learning%252C%2520where%2520simpler%2520models%2520are%2520often%2520preferred%250Adue%2520to%2520interpretability%2520and%2520efficiency.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520yet%2520straightforward%2520method%2520for%250Aincorporating%2520LLM-generated%2520global%2520task%2520feature%2520attributions%2520into%2520the%2520training%250Aprocess%2520of%2520smaller%2520networks.%2520Specifically%252C%2520we%2520propose%2520an%2520attribution-matching%250Aregularization%2520term%2520that%2520aligns%2520the%2520training%2520dynamics%2520of%2520the%2520smaller%2520model%2520with%250Athe%2520insights%2520provided%2520by%2520the%2520LLM.%2520By%2520doing%2520so%252C%2520our%2520approach%2520yields%2520superior%250Aperformance%2520in%2520few-shot%2520learning%2520scenarios.%2520Notably%252C%2520our%2520method%2520requires%2520only%250Ablack-box%2520API%2520access%2520to%2520the%2520LLM%252C%2520making%2520it%2520easy%2520to%2520integrate%2520into%2520existing%250Atraining%2520pipelines%2520with%2520minimal%2520computational%2520overhead.%250A%2520%2520Furthermore%252C%2520we%2520demonstrate%2520how%2520this%2520method%2520can%2520be%2520used%2520to%2520address%2520common%250Aissues%2520in%2520real-world%2520datasets%252C%2520such%2520as%2520skewness%2520and%2520bias.%2520By%2520integrating%250Ahigh-level%2520knowledge%2520from%2520LLMs%252C%2520our%2520approach%2520improves%2520generalization%252C%2520even%2520when%250Atraining%2520data%2520is%2520limited%2520or%2520imbalanced.%2520We%2520validate%2520its%2520effectiveness%2520through%250Aextensive%2520experiments%2520across%2520multiple%2520tasks%252C%2520demonstrating%2520improved%2520learning%250Aefficiency%2520and%2520model%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.20268v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20as%20Attribution%20Regularizers%20for%20Efficient%20Model%0A%20%20Training&entry.906535625=Davor%20Vukadin%20and%20Marin%20%C5%A0ili%C4%87%20and%20Goran%20Dela%C4%8D&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20performance%20across%0Adiverse%20domains.%20However%2C%20effectively%20leveraging%20their%20vast%20knowledge%20for%0Atraining%20smaller%20downstream%20models%20remains%20an%20open%20challenge%2C%20especially%20in%0Adomains%20like%20tabular%20data%20learning%2C%20where%20simpler%20models%20are%20often%20preferred%0Adue%20to%20interpretability%20and%20efficiency.%0A%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20yet%20straightforward%20method%20for%0Aincorporating%20LLM-generated%20global%20task%20feature%20attributions%20into%20the%20training%0Aprocess%20of%20smaller%20networks.%20Specifically%2C%20we%20propose%20an%20attribution-matching%0Aregularization%20term%20that%20aligns%20the%20training%20dynamics%20of%20the%20smaller%20model%20with%0Athe%20insights%20provided%20by%20the%20LLM.%20By%20doing%20so%2C%20our%20approach%20yields%20superior%0Aperformance%20in%20few-shot%20learning%20scenarios.%20Notably%2C%20our%20method%20requires%20only%0Ablack-box%20API%20access%20to%20the%20LLM%2C%20making%20it%20easy%20to%20integrate%20into%20existing%0Atraining%20pipelines%20with%20minimal%20computational%20overhead.%0A%20%20Furthermore%2C%20we%20demonstrate%20how%20this%20method%20can%20be%20used%20to%20address%20common%0Aissues%20in%20real-world%20datasets%2C%20such%20as%20skewness%20and%20bias.%20By%20integrating%0Ahigh-level%20knowledge%20from%20LLMs%2C%20our%20approach%20improves%20generalization%2C%20even%20when%0Atraining%20data%20is%20limited%20or%20imbalanced.%20We%20validate%20its%20effectiveness%20through%0Aextensive%20experiments%20across%20multiple%20tasks%2C%20demonstrating%20improved%20learning%0Aefficiency%20and%20model%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.20268v2&entry.124074799=Read"},
{"title": "Visually Consistent Hierarchical Image Classification", "author": "Seulki Park and Youren Zhang and Stella X. Yu and Sara Beery and Jonathan Huang", "abstract": "  Hierarchical classification predicts labels across multiple levels of a\ntaxonomy, e.g., from coarse-level 'Bird' to mid-level 'Hummingbird' to\nfine-level 'Green hermit', allowing flexible recognition under varying visual\nconditions. It is commonly framed as multiple single-level tasks, but each\nlevel may rely on different visual cues: Distinguishing 'Bird' from 'Plant'\nrelies on global features like feathers or leaves, while separating 'Anna's\nhummingbird' from 'Green hermit' requires local details such as head\ncoloration. Prior methods improve accuracy using external semantic supervision,\nbut such statistical learning criteria fail to ensure consistent visual\ngrounding at test time, resulting in incorrect hierarchical classification. We\npropose, for the first time, to enforce internal visual consistency by aligning\nfine-to-coarse predictions through intra-image segmentation. Our method\noutperforms zero-shot CLIP and state-of-the-art baselines on hierarchical\nclassification benchmarks, achieving both higher accuracy and more consistent\npredictions. It also improves internal image segmentation without requiring\npixel-level annotations.\n", "link": "http://arxiv.org/abs/2406.11608v2", "date": "2025-04-16", "relevancy": 2.6021, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5245}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5242}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5125}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visually%20Consistent%20Hierarchical%20Image%20Classification&body=Title%3A%20Visually%20Consistent%20Hierarchical%20Image%20Classification%0AAuthor%3A%20Seulki%20Park%20and%20Youren%20Zhang%20and%20Stella%20X.%20Yu%20and%20Sara%20Beery%20and%20Jonathan%20Huang%0AAbstract%3A%20%20%20Hierarchical%20classification%20predicts%20labels%20across%20multiple%20levels%20of%20a%0Ataxonomy%2C%20e.g.%2C%20from%20coarse-level%20%27Bird%27%20to%20mid-level%20%27Hummingbird%27%20to%0Afine-level%20%27Green%20hermit%27%2C%20allowing%20flexible%20recognition%20under%20varying%20visual%0Aconditions.%20It%20is%20commonly%20framed%20as%20multiple%20single-level%20tasks%2C%20but%20each%0Alevel%20may%20rely%20on%20different%20visual%20cues%3A%20Distinguishing%20%27Bird%27%20from%20%27Plant%27%0Arelies%20on%20global%20features%20like%20feathers%20or%20leaves%2C%20while%20separating%20%27Anna%27s%0Ahummingbird%27%20from%20%27Green%20hermit%27%20requires%20local%20details%20such%20as%20head%0Acoloration.%20Prior%20methods%20improve%20accuracy%20using%20external%20semantic%20supervision%2C%0Abut%20such%20statistical%20learning%20criteria%20fail%20to%20ensure%20consistent%20visual%0Agrounding%20at%20test%20time%2C%20resulting%20in%20incorrect%20hierarchical%20classification.%20We%0Apropose%2C%20for%20the%20first%20time%2C%20to%20enforce%20internal%20visual%20consistency%20by%20aligning%0Afine-to-coarse%20predictions%20through%20intra-image%20segmentation.%20Our%20method%0Aoutperforms%20zero-shot%20CLIP%20and%20state-of-the-art%20baselines%20on%20hierarchical%0Aclassification%20benchmarks%2C%20achieving%20both%20higher%20accuracy%20and%20more%20consistent%0Apredictions.%20It%20also%20improves%20internal%20image%20segmentation%20without%20requiring%0Apixel-level%20annotations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11608v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisually%2520Consistent%2520Hierarchical%2520Image%2520Classification%26entry.906535625%3DSeulki%2520Park%2520and%2520Youren%2520Zhang%2520and%2520Stella%2520X.%2520Yu%2520and%2520Sara%2520Beery%2520and%2520Jonathan%2520Huang%26entry.1292438233%3D%2520%2520Hierarchical%2520classification%2520predicts%2520labels%2520across%2520multiple%2520levels%2520of%2520a%250Ataxonomy%252C%2520e.g.%252C%2520from%2520coarse-level%2520%2527Bird%2527%2520to%2520mid-level%2520%2527Hummingbird%2527%2520to%250Afine-level%2520%2527Green%2520hermit%2527%252C%2520allowing%2520flexible%2520recognition%2520under%2520varying%2520visual%250Aconditions.%2520It%2520is%2520commonly%2520framed%2520as%2520multiple%2520single-level%2520tasks%252C%2520but%2520each%250Alevel%2520may%2520rely%2520on%2520different%2520visual%2520cues%253A%2520Distinguishing%2520%2527Bird%2527%2520from%2520%2527Plant%2527%250Arelies%2520on%2520global%2520features%2520like%2520feathers%2520or%2520leaves%252C%2520while%2520separating%2520%2527Anna%2527s%250Ahummingbird%2527%2520from%2520%2527Green%2520hermit%2527%2520requires%2520local%2520details%2520such%2520as%2520head%250Acoloration.%2520Prior%2520methods%2520improve%2520accuracy%2520using%2520external%2520semantic%2520supervision%252C%250Abut%2520such%2520statistical%2520learning%2520criteria%2520fail%2520to%2520ensure%2520consistent%2520visual%250Agrounding%2520at%2520test%2520time%252C%2520resulting%2520in%2520incorrect%2520hierarchical%2520classification.%2520We%250Apropose%252C%2520for%2520the%2520first%2520time%252C%2520to%2520enforce%2520internal%2520visual%2520consistency%2520by%2520aligning%250Afine-to-coarse%2520predictions%2520through%2520intra-image%2520segmentation.%2520Our%2520method%250Aoutperforms%2520zero-shot%2520CLIP%2520and%2520state-of-the-art%2520baselines%2520on%2520hierarchical%250Aclassification%2520benchmarks%252C%2520achieving%2520both%2520higher%2520accuracy%2520and%2520more%2520consistent%250Apredictions.%2520It%2520also%2520improves%2520internal%2520image%2520segmentation%2520without%2520requiring%250Apixel-level%2520annotations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11608v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visually%20Consistent%20Hierarchical%20Image%20Classification&entry.906535625=Seulki%20Park%20and%20Youren%20Zhang%20and%20Stella%20X.%20Yu%20and%20Sara%20Beery%20and%20Jonathan%20Huang&entry.1292438233=%20%20Hierarchical%20classification%20predicts%20labels%20across%20multiple%20levels%20of%20a%0Ataxonomy%2C%20e.g.%2C%20from%20coarse-level%20%27Bird%27%20to%20mid-level%20%27Hummingbird%27%20to%0Afine-level%20%27Green%20hermit%27%2C%20allowing%20flexible%20recognition%20under%20varying%20visual%0Aconditions.%20It%20is%20commonly%20framed%20as%20multiple%20single-level%20tasks%2C%20but%20each%0Alevel%20may%20rely%20on%20different%20visual%20cues%3A%20Distinguishing%20%27Bird%27%20from%20%27Plant%27%0Arelies%20on%20global%20features%20like%20feathers%20or%20leaves%2C%20while%20separating%20%27Anna%27s%0Ahummingbird%27%20from%20%27Green%20hermit%27%20requires%20local%20details%20such%20as%20head%0Acoloration.%20Prior%20methods%20improve%20accuracy%20using%20external%20semantic%20supervision%2C%0Abut%20such%20statistical%20learning%20criteria%20fail%20to%20ensure%20consistent%20visual%0Agrounding%20at%20test%20time%2C%20resulting%20in%20incorrect%20hierarchical%20classification.%20We%0Apropose%2C%20for%20the%20first%20time%2C%20to%20enforce%20internal%20visual%20consistency%20by%20aligning%0Afine-to-coarse%20predictions%20through%20intra-image%20segmentation.%20Our%20method%0Aoutperforms%20zero-shot%20CLIP%20and%20state-of-the-art%20baselines%20on%20hierarchical%0Aclassification%20benchmarks%2C%20achieving%20both%20higher%20accuracy%20and%20more%20consistent%0Apredictions.%20It%20also%20improves%20internal%20image%20segmentation%20without%20requiring%0Apixel-level%20annotations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11608v2&entry.124074799=Read"},
{"title": "UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction\n  Synthesis", "author": "Xinyi Liu and Xiaoyi Zhang and Ziyun Zhang and Yan Lu", "abstract": "  Recent advancements in Large Vision-Language Models are accelerating the\ndevelopment of Graphical User Interface (GUI) agents that utilize human-like\nvision perception capabilities to enhance productivity on digital devices.\nCompared to approaches predicated on GUI metadata, which are platform-dependent\nand vulnerable to implementation variations, vision-based approaches offer\nbroader applicability. In this vision-based paradigm, the GUI instruction\ngrounding, which maps user instruction to the location of corresponding element\non the given screenshot, remains a critical challenge, particularly due to\nlimited public training dataset and resource-intensive manual instruction data\nannotation. In this paper, we delve into unexplored challenges in this task\nincluding element-to-screen ratio, unbalanced element type, and implicit\ninstruction. To address these challenges, we introduce a large-scale data\nsynthesis pipeline UI-E2I-Synth for generating varying complex instruction\ndatasets using GPT-4o instead of human annotators. Furthermore, we propose a\nnew GUI instruction grounding benchmark UI-I2E-Bench, which is designed to\naddress the limitations of existing benchmarks by incorporating diverse\nannotation aspects. Our model, trained on the synthesized data, achieves\nsuperior performance in GUI instruction grounding, demonstrating the\nadvancements of proposed data synthesis pipeline. The proposed benchmark,\naccompanied by extensive analyses, provides practical insights for future\nresearch in GUI grounding. We will release corresponding artifacts at\nhttps://colmon46.github.io/i2e-bench-leaderboard/ .\n", "link": "http://arxiv.org/abs/2504.11257v3", "date": "2025-04-17", "relevancy": 2.6012, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.53}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5229}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UI-E2I-Synth%3A%20Advancing%20GUI%20Grounding%20with%20Large-Scale%20Instruction%0A%20%20Synthesis&body=Title%3A%20UI-E2I-Synth%3A%20Advancing%20GUI%20Grounding%20with%20Large-Scale%20Instruction%0A%20%20Synthesis%0AAuthor%3A%20Xinyi%20Liu%20and%20Xiaoyi%20Zhang%20and%20Ziyun%20Zhang%20and%20Yan%20Lu%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Large%20Vision-Language%20Models%20are%20accelerating%20the%0Adevelopment%20of%20Graphical%20User%20Interface%20%28GUI%29%20agents%20that%20utilize%20human-like%0Avision%20perception%20capabilities%20to%20enhance%20productivity%20on%20digital%20devices.%0ACompared%20to%20approaches%20predicated%20on%20GUI%20metadata%2C%20which%20are%20platform-dependent%0Aand%20vulnerable%20to%20implementation%20variations%2C%20vision-based%20approaches%20offer%0Abroader%20applicability.%20In%20this%20vision-based%20paradigm%2C%20the%20GUI%20instruction%0Agrounding%2C%20which%20maps%20user%20instruction%20to%20the%20location%20of%20corresponding%20element%0Aon%20the%20given%20screenshot%2C%20remains%20a%20critical%20challenge%2C%20particularly%20due%20to%0Alimited%20public%20training%20dataset%20and%20resource-intensive%20manual%20instruction%20data%0Aannotation.%20In%20this%20paper%2C%20we%20delve%20into%20unexplored%20challenges%20in%20this%20task%0Aincluding%20element-to-screen%20ratio%2C%20unbalanced%20element%20type%2C%20and%20implicit%0Ainstruction.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20large-scale%20data%0Asynthesis%20pipeline%20UI-E2I-Synth%20for%20generating%20varying%20complex%20instruction%0Adatasets%20using%20GPT-4o%20instead%20of%20human%20annotators.%20Furthermore%2C%20we%20propose%20a%0Anew%20GUI%20instruction%20grounding%20benchmark%20UI-I2E-Bench%2C%20which%20is%20designed%20to%0Aaddress%20the%20limitations%20of%20existing%20benchmarks%20by%20incorporating%20diverse%0Aannotation%20aspects.%20Our%20model%2C%20trained%20on%20the%20synthesized%20data%2C%20achieves%0Asuperior%20performance%20in%20GUI%20instruction%20grounding%2C%20demonstrating%20the%0Aadvancements%20of%20proposed%20data%20synthesis%20pipeline.%20The%20proposed%20benchmark%2C%0Aaccompanied%20by%20extensive%20analyses%2C%20provides%20practical%20insights%20for%20future%0Aresearch%20in%20GUI%20grounding.%20We%20will%20release%20corresponding%20artifacts%20at%0Ahttps%3A//colmon46.github.io/i2e-bench-leaderboard/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.11257v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUI-E2I-Synth%253A%2520Advancing%2520GUI%2520Grounding%2520with%2520Large-Scale%2520Instruction%250A%2520%2520Synthesis%26entry.906535625%3DXinyi%2520Liu%2520and%2520Xiaoyi%2520Zhang%2520and%2520Ziyun%2520Zhang%2520and%2520Yan%2520Lu%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Large%2520Vision-Language%2520Models%2520are%2520accelerating%2520the%250Adevelopment%2520of%2520Graphical%2520User%2520Interface%2520%2528GUI%2529%2520agents%2520that%2520utilize%2520human-like%250Avision%2520perception%2520capabilities%2520to%2520enhance%2520productivity%2520on%2520digital%2520devices.%250ACompared%2520to%2520approaches%2520predicated%2520on%2520GUI%2520metadata%252C%2520which%2520are%2520platform-dependent%250Aand%2520vulnerable%2520to%2520implementation%2520variations%252C%2520vision-based%2520approaches%2520offer%250Abroader%2520applicability.%2520In%2520this%2520vision-based%2520paradigm%252C%2520the%2520GUI%2520instruction%250Agrounding%252C%2520which%2520maps%2520user%2520instruction%2520to%2520the%2520location%2520of%2520corresponding%2520element%250Aon%2520the%2520given%2520screenshot%252C%2520remains%2520a%2520critical%2520challenge%252C%2520particularly%2520due%2520to%250Alimited%2520public%2520training%2520dataset%2520and%2520resource-intensive%2520manual%2520instruction%2520data%250Aannotation.%2520In%2520this%2520paper%252C%2520we%2520delve%2520into%2520unexplored%2520challenges%2520in%2520this%2520task%250Aincluding%2520element-to-screen%2520ratio%252C%2520unbalanced%2520element%2520type%252C%2520and%2520implicit%250Ainstruction.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520large-scale%2520data%250Asynthesis%2520pipeline%2520UI-E2I-Synth%2520for%2520generating%2520varying%2520complex%2520instruction%250Adatasets%2520using%2520GPT-4o%2520instead%2520of%2520human%2520annotators.%2520Furthermore%252C%2520we%2520propose%2520a%250Anew%2520GUI%2520instruction%2520grounding%2520benchmark%2520UI-I2E-Bench%252C%2520which%2520is%2520designed%2520to%250Aaddress%2520the%2520limitations%2520of%2520existing%2520benchmarks%2520by%2520incorporating%2520diverse%250Aannotation%2520aspects.%2520Our%2520model%252C%2520trained%2520on%2520the%2520synthesized%2520data%252C%2520achieves%250Asuperior%2520performance%2520in%2520GUI%2520instruction%2520grounding%252C%2520demonstrating%2520the%250Aadvancements%2520of%2520proposed%2520data%2520synthesis%2520pipeline.%2520The%2520proposed%2520benchmark%252C%250Aaccompanied%2520by%2520extensive%2520analyses%252C%2520provides%2520practical%2520insights%2520for%2520future%250Aresearch%2520in%2520GUI%2520grounding.%2520We%2520will%2520release%2520corresponding%2520artifacts%2520at%250Ahttps%253A//colmon46.github.io/i2e-bench-leaderboard/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.11257v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UI-E2I-Synth%3A%20Advancing%20GUI%20Grounding%20with%20Large-Scale%20Instruction%0A%20%20Synthesis&entry.906535625=Xinyi%20Liu%20and%20Xiaoyi%20Zhang%20and%20Ziyun%20Zhang%20and%20Yan%20Lu&entry.1292438233=%20%20Recent%20advancements%20in%20Large%20Vision-Language%20Models%20are%20accelerating%20the%0Adevelopment%20of%20Graphical%20User%20Interface%20%28GUI%29%20agents%20that%20utilize%20human-like%0Avision%20perception%20capabilities%20to%20enhance%20productivity%20on%20digital%20devices.%0ACompared%20to%20approaches%20predicated%20on%20GUI%20metadata%2C%20which%20are%20platform-dependent%0Aand%20vulnerable%20to%20implementation%20variations%2C%20vision-based%20approaches%20offer%0Abroader%20applicability.%20In%20this%20vision-based%20paradigm%2C%20the%20GUI%20instruction%0Agrounding%2C%20which%20maps%20user%20instruction%20to%20the%20location%20of%20corresponding%20element%0Aon%20the%20given%20screenshot%2C%20remains%20a%20critical%20challenge%2C%20particularly%20due%20to%0Alimited%20public%20training%20dataset%20and%20resource-intensive%20manual%20instruction%20data%0Aannotation.%20In%20this%20paper%2C%20we%20delve%20into%20unexplored%20challenges%20in%20this%20task%0Aincluding%20element-to-screen%20ratio%2C%20unbalanced%20element%20type%2C%20and%20implicit%0Ainstruction.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20large-scale%20data%0Asynthesis%20pipeline%20UI-E2I-Synth%20for%20generating%20varying%20complex%20instruction%0Adatasets%20using%20GPT-4o%20instead%20of%20human%20annotators.%20Furthermore%2C%20we%20propose%20a%0Anew%20GUI%20instruction%20grounding%20benchmark%20UI-I2E-Bench%2C%20which%20is%20designed%20to%0Aaddress%20the%20limitations%20of%20existing%20benchmarks%20by%20incorporating%20diverse%0Aannotation%20aspects.%20Our%20model%2C%20trained%20on%20the%20synthesized%20data%2C%20achieves%0Asuperior%20performance%20in%20GUI%20instruction%20grounding%2C%20demonstrating%20the%0Aadvancements%20of%20proposed%20data%20synthesis%20pipeline.%20The%20proposed%20benchmark%2C%0Aaccompanied%20by%20extensive%20analyses%2C%20provides%20practical%20insights%20for%20future%0Aresearch%20in%20GUI%20grounding.%20We%20will%20release%20corresponding%20artifacts%20at%0Ahttps%3A//colmon46.github.io/i2e-bench-leaderboard/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.11257v3&entry.124074799=Read"},
{"title": "A Coding-Theoretic Analysis of Hyperspherical Prototypical Learning\n  Geometry", "author": "Martin Lindstr\u00f6m and Borja Rodr\u00edguez-G\u00e1lvez and Ragnar Thobaben and Mikael Skoglund", "abstract": "  Hyperspherical Prototypical Learning (HPL) is a supervised approach to\nrepresentation learning that designs class prototypes on the unit hypersphere.\nThe prototypes bias the representations to class separation in a scale\ninvariant and known geometry. Previous approaches to HPL have either of the\nfollowing shortcomings: (i) they follow an unprincipled optimisation procedure;\nor (ii) they are theoretically sound, but are constrained to only one possible\nlatent dimension. In this paper, we address both shortcomings. To address (i),\nwe present a principled optimisation procedure whose solution we show is\noptimal. To address (ii), we construct well-separated prototypes in a wide\nrange of dimensions using linear block codes. Additionally, we give a full\ncharacterisation of the optimal prototype placement in terms of achievable and\nconverse bounds, showing that our proposed methods are near-optimal.\n", "link": "http://arxiv.org/abs/2407.07664v2", "date": "2025-04-17", "relevancy": 2.5933, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5491}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Coding-Theoretic%20Analysis%20of%20Hyperspherical%20Prototypical%20Learning%0A%20%20Geometry&body=Title%3A%20A%20Coding-Theoretic%20Analysis%20of%20Hyperspherical%20Prototypical%20Learning%0A%20%20Geometry%0AAuthor%3A%20Martin%20Lindstr%C3%B6m%20and%20Borja%20Rodr%C3%ADguez-G%C3%A1lvez%20and%20Ragnar%20Thobaben%20and%20Mikael%20Skoglund%0AAbstract%3A%20%20%20Hyperspherical%20Prototypical%20Learning%20%28HPL%29%20is%20a%20supervised%20approach%20to%0Arepresentation%20learning%20that%20designs%20class%20prototypes%20on%20the%20unit%20hypersphere.%0AThe%20prototypes%20bias%20the%20representations%20to%20class%20separation%20in%20a%20scale%0Ainvariant%20and%20known%20geometry.%20Previous%20approaches%20to%20HPL%20have%20either%20of%20the%0Afollowing%20shortcomings%3A%20%28i%29%20they%20follow%20an%20unprincipled%20optimisation%20procedure%3B%0Aor%20%28ii%29%20they%20are%20theoretically%20sound%2C%20but%20are%20constrained%20to%20only%20one%20possible%0Alatent%20dimension.%20In%20this%20paper%2C%20we%20address%20both%20shortcomings.%20To%20address%20%28i%29%2C%0Awe%20present%20a%20principled%20optimisation%20procedure%20whose%20solution%20we%20show%20is%0Aoptimal.%20To%20address%20%28ii%29%2C%20we%20construct%20well-separated%20prototypes%20in%20a%20wide%0Arange%20of%20dimensions%20using%20linear%20block%20codes.%20Additionally%2C%20we%20give%20a%20full%0Acharacterisation%20of%20the%20optimal%20prototype%20placement%20in%20terms%20of%20achievable%20and%0Aconverse%20bounds%2C%20showing%20that%20our%20proposed%20methods%20are%20near-optimal.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07664v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Coding-Theoretic%2520Analysis%2520of%2520Hyperspherical%2520Prototypical%2520Learning%250A%2520%2520Geometry%26entry.906535625%3DMartin%2520Lindstr%25C3%25B6m%2520and%2520Borja%2520Rodr%25C3%25ADguez-G%25C3%25A1lvez%2520and%2520Ragnar%2520Thobaben%2520and%2520Mikael%2520Skoglund%26entry.1292438233%3D%2520%2520Hyperspherical%2520Prototypical%2520Learning%2520%2528HPL%2529%2520is%2520a%2520supervised%2520approach%2520to%250Arepresentation%2520learning%2520that%2520designs%2520class%2520prototypes%2520on%2520the%2520unit%2520hypersphere.%250AThe%2520prototypes%2520bias%2520the%2520representations%2520to%2520class%2520separation%2520in%2520a%2520scale%250Ainvariant%2520and%2520known%2520geometry.%2520Previous%2520approaches%2520to%2520HPL%2520have%2520either%2520of%2520the%250Afollowing%2520shortcomings%253A%2520%2528i%2529%2520they%2520follow%2520an%2520unprincipled%2520optimisation%2520procedure%253B%250Aor%2520%2528ii%2529%2520they%2520are%2520theoretically%2520sound%252C%2520but%2520are%2520constrained%2520to%2520only%2520one%2520possible%250Alatent%2520dimension.%2520In%2520this%2520paper%252C%2520we%2520address%2520both%2520shortcomings.%2520To%2520address%2520%2528i%2529%252C%250Awe%2520present%2520a%2520principled%2520optimisation%2520procedure%2520whose%2520solution%2520we%2520show%2520is%250Aoptimal.%2520To%2520address%2520%2528ii%2529%252C%2520we%2520construct%2520well-separated%2520prototypes%2520in%2520a%2520wide%250Arange%2520of%2520dimensions%2520using%2520linear%2520block%2520codes.%2520Additionally%252C%2520we%2520give%2520a%2520full%250Acharacterisation%2520of%2520the%2520optimal%2520prototype%2520placement%2520in%2520terms%2520of%2520achievable%2520and%250Aconverse%2520bounds%252C%2520showing%2520that%2520our%2520proposed%2520methods%2520are%2520near-optimal.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07664v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Coding-Theoretic%20Analysis%20of%20Hyperspherical%20Prototypical%20Learning%0A%20%20Geometry&entry.906535625=Martin%20Lindstr%C3%B6m%20and%20Borja%20Rodr%C3%ADguez-G%C3%A1lvez%20and%20Ragnar%20Thobaben%20and%20Mikael%20Skoglund&entry.1292438233=%20%20Hyperspherical%20Prototypical%20Learning%20%28HPL%29%20is%20a%20supervised%20approach%20to%0Arepresentation%20learning%20that%20designs%20class%20prototypes%20on%20the%20unit%20hypersphere.%0AThe%20prototypes%20bias%20the%20representations%20to%20class%20separation%20in%20a%20scale%0Ainvariant%20and%20known%20geometry.%20Previous%20approaches%20to%20HPL%20have%20either%20of%20the%0Afollowing%20shortcomings%3A%20%28i%29%20they%20follow%20an%20unprincipled%20optimisation%20procedure%3B%0Aor%20%28ii%29%20they%20are%20theoretically%20sound%2C%20but%20are%20constrained%20to%20only%20one%20possible%0Alatent%20dimension.%20In%20this%20paper%2C%20we%20address%20both%20shortcomings.%20To%20address%20%28i%29%2C%0Awe%20present%20a%20principled%20optimisation%20procedure%20whose%20solution%20we%20show%20is%0Aoptimal.%20To%20address%20%28ii%29%2C%20we%20construct%20well-separated%20prototypes%20in%20a%20wide%0Arange%20of%20dimensions%20using%20linear%20block%20codes.%20Additionally%2C%20we%20give%20a%20full%0Acharacterisation%20of%20the%20optimal%20prototype%20placement%20in%20terms%20of%20achievable%20and%0Aconverse%20bounds%2C%20showing%20that%20our%20proposed%20methods%20are%20near-optimal.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07664v2&entry.124074799=Read"},
{"title": "Generate, but Verify: Reducing Hallucination in Vision-Language Models\n  with Retrospective Resampling", "author": "Tsung-Han Wu and Heekyung Lee and Jiaxin Ge and Joseph E. Gonzalez and Trevor Darrell and David M. Chan", "abstract": "  Vision-Language Models (VLMs) excel at visual understanding but often suffer\nfrom visual hallucinations, where they generate descriptions of nonexistent\nobjects, actions, or concepts, posing significant risks in safety-critical\napplications. Existing hallucination mitigation methods typically follow one of\ntwo paradigms: generation adjustment, which modifies decoding behavior to align\ntext with visual inputs, and post-hoc verification, where external models\nassess and correct outputs. While effective, generation adjustment methods\noften rely on heuristics and lack correction mechanisms, while post-hoc\nverification is complicated, typically requiring multiple models and tending to\nreject outputs rather than refine them. In this work, we introduce REVERSE, a\nunified framework that integrates hallucination-aware training with on-the-fly\nself-verification. By leveraging a new hallucination-verification dataset\ncontaining over 1.3M semi-synthetic samples, along with a novel inference-time\nretrospective resampling technique, our approach enables VLMs to both detect\nhallucinations during generation and dynamically revise those hallucinations.\nOur evaluations show that REVERSE achieves state-of-the-art hallucination\nreduction, outperforming the best existing methods by up to 12% on CHAIR-MSCOCO\nand 28% on HaloQuest. Our dataset, model, and code are available at:\nhttps://reverse-vlm.github.io.\n", "link": "http://arxiv.org/abs/2504.13169v1", "date": "2025-04-17", "relevancy": 2.5871, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5205}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5159}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generate%2C%20but%20Verify%3A%20Reducing%20Hallucination%20in%20Vision-Language%20Models%0A%20%20with%20Retrospective%20Resampling&body=Title%3A%20Generate%2C%20but%20Verify%3A%20Reducing%20Hallucination%20in%20Vision-Language%20Models%0A%20%20with%20Retrospective%20Resampling%0AAuthor%3A%20Tsung-Han%20Wu%20and%20Heekyung%20Lee%20and%20Jiaxin%20Ge%20and%20Joseph%20E.%20Gonzalez%20and%20Trevor%20Darrell%20and%20David%20M.%20Chan%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20excel%20at%20visual%20understanding%20but%20often%20suffer%0Afrom%20visual%20hallucinations%2C%20where%20they%20generate%20descriptions%20of%20nonexistent%0Aobjects%2C%20actions%2C%20or%20concepts%2C%20posing%20significant%20risks%20in%20safety-critical%0Aapplications.%20Existing%20hallucination%20mitigation%20methods%20typically%20follow%20one%20of%0Atwo%20paradigms%3A%20generation%20adjustment%2C%20which%20modifies%20decoding%20behavior%20to%20align%0Atext%20with%20visual%20inputs%2C%20and%20post-hoc%20verification%2C%20where%20external%20models%0Aassess%20and%20correct%20outputs.%20While%20effective%2C%20generation%20adjustment%20methods%0Aoften%20rely%20on%20heuristics%20and%20lack%20correction%20mechanisms%2C%20while%20post-hoc%0Averification%20is%20complicated%2C%20typically%20requiring%20multiple%20models%20and%20tending%20to%0Areject%20outputs%20rather%20than%20refine%20them.%20In%20this%20work%2C%20we%20introduce%20REVERSE%2C%20a%0Aunified%20framework%20that%20integrates%20hallucination-aware%20training%20with%20on-the-fly%0Aself-verification.%20By%20leveraging%20a%20new%20hallucination-verification%20dataset%0Acontaining%20over%201.3M%20semi-synthetic%20samples%2C%20along%20with%20a%20novel%20inference-time%0Aretrospective%20resampling%20technique%2C%20our%20approach%20enables%20VLMs%20to%20both%20detect%0Ahallucinations%20during%20generation%20and%20dynamically%20revise%20those%20hallucinations.%0AOur%20evaluations%20show%20that%20REVERSE%20achieves%20state-of-the-art%20hallucination%0Areduction%2C%20outperforming%20the%20best%20existing%20methods%20by%20up%20to%2012%25%20on%20CHAIR-MSCOCO%0Aand%2028%25%20on%20HaloQuest.%20Our%20dataset%2C%20model%2C%20and%20code%20are%20available%20at%3A%0Ahttps%3A//reverse-vlm.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13169v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerate%252C%2520but%2520Verify%253A%2520Reducing%2520Hallucination%2520in%2520Vision-Language%2520Models%250A%2520%2520with%2520Retrospective%2520Resampling%26entry.906535625%3DTsung-Han%2520Wu%2520and%2520Heekyung%2520Lee%2520and%2520Jiaxin%2520Ge%2520and%2520Joseph%2520E.%2520Gonzalez%2520and%2520Trevor%2520Darrell%2520and%2520David%2520M.%2520Chan%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520excel%2520at%2520visual%2520understanding%2520but%2520often%2520suffer%250Afrom%2520visual%2520hallucinations%252C%2520where%2520they%2520generate%2520descriptions%2520of%2520nonexistent%250Aobjects%252C%2520actions%252C%2520or%2520concepts%252C%2520posing%2520significant%2520risks%2520in%2520safety-critical%250Aapplications.%2520Existing%2520hallucination%2520mitigation%2520methods%2520typically%2520follow%2520one%2520of%250Atwo%2520paradigms%253A%2520generation%2520adjustment%252C%2520which%2520modifies%2520decoding%2520behavior%2520to%2520align%250Atext%2520with%2520visual%2520inputs%252C%2520and%2520post-hoc%2520verification%252C%2520where%2520external%2520models%250Aassess%2520and%2520correct%2520outputs.%2520While%2520effective%252C%2520generation%2520adjustment%2520methods%250Aoften%2520rely%2520on%2520heuristics%2520and%2520lack%2520correction%2520mechanisms%252C%2520while%2520post-hoc%250Averification%2520is%2520complicated%252C%2520typically%2520requiring%2520multiple%2520models%2520and%2520tending%2520to%250Areject%2520outputs%2520rather%2520than%2520refine%2520them.%2520In%2520this%2520work%252C%2520we%2520introduce%2520REVERSE%252C%2520a%250Aunified%2520framework%2520that%2520integrates%2520hallucination-aware%2520training%2520with%2520on-the-fly%250Aself-verification.%2520By%2520leveraging%2520a%2520new%2520hallucination-verification%2520dataset%250Acontaining%2520over%25201.3M%2520semi-synthetic%2520samples%252C%2520along%2520with%2520a%2520novel%2520inference-time%250Aretrospective%2520resampling%2520technique%252C%2520our%2520approach%2520enables%2520VLMs%2520to%2520both%2520detect%250Ahallucinations%2520during%2520generation%2520and%2520dynamically%2520revise%2520those%2520hallucinations.%250AOur%2520evaluations%2520show%2520that%2520REVERSE%2520achieves%2520state-of-the-art%2520hallucination%250Areduction%252C%2520outperforming%2520the%2520best%2520existing%2520methods%2520by%2520up%2520to%252012%2525%2520on%2520CHAIR-MSCOCO%250Aand%252028%2525%2520on%2520HaloQuest.%2520Our%2520dataset%252C%2520model%252C%2520and%2520code%2520are%2520available%2520at%253A%250Ahttps%253A//reverse-vlm.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13169v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generate%2C%20but%20Verify%3A%20Reducing%20Hallucination%20in%20Vision-Language%20Models%0A%20%20with%20Retrospective%20Resampling&entry.906535625=Tsung-Han%20Wu%20and%20Heekyung%20Lee%20and%20Jiaxin%20Ge%20and%20Joseph%20E.%20Gonzalez%20and%20Trevor%20Darrell%20and%20David%20M.%20Chan&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20excel%20at%20visual%20understanding%20but%20often%20suffer%0Afrom%20visual%20hallucinations%2C%20where%20they%20generate%20descriptions%20of%20nonexistent%0Aobjects%2C%20actions%2C%20or%20concepts%2C%20posing%20significant%20risks%20in%20safety-critical%0Aapplications.%20Existing%20hallucination%20mitigation%20methods%20typically%20follow%20one%20of%0Atwo%20paradigms%3A%20generation%20adjustment%2C%20which%20modifies%20decoding%20behavior%20to%20align%0Atext%20with%20visual%20inputs%2C%20and%20post-hoc%20verification%2C%20where%20external%20models%0Aassess%20and%20correct%20outputs.%20While%20effective%2C%20generation%20adjustment%20methods%0Aoften%20rely%20on%20heuristics%20and%20lack%20correction%20mechanisms%2C%20while%20post-hoc%0Averification%20is%20complicated%2C%20typically%20requiring%20multiple%20models%20and%20tending%20to%0Areject%20outputs%20rather%20than%20refine%20them.%20In%20this%20work%2C%20we%20introduce%20REVERSE%2C%20a%0Aunified%20framework%20that%20integrates%20hallucination-aware%20training%20with%20on-the-fly%0Aself-verification.%20By%20leveraging%20a%20new%20hallucination-verification%20dataset%0Acontaining%20over%201.3M%20semi-synthetic%20samples%2C%20along%20with%20a%20novel%20inference-time%0Aretrospective%20resampling%20technique%2C%20our%20approach%20enables%20VLMs%20to%20both%20detect%0Ahallucinations%20during%20generation%20and%20dynamically%20revise%20those%20hallucinations.%0AOur%20evaluations%20show%20that%20REVERSE%20achieves%20state-of-the-art%20hallucination%0Areduction%2C%20outperforming%20the%20best%20existing%20methods%20by%20up%20to%2012%25%20on%20CHAIR-MSCOCO%0Aand%2028%25%20on%20HaloQuest.%20Our%20dataset%2C%20model%2C%20and%20code%20are%20available%20at%3A%0Ahttps%3A//reverse-vlm.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13169v1&entry.124074799=Read"},
{"title": "Hierarchical Vector Quantized Graph Autoencoder with Annealing-Based\n  Code Selection", "author": "Long Zeng and Jianxiang Yu and Jiapeng Zhu and Qingsong Zhong and Xiang Li", "abstract": "  Graph self-supervised learning has gained significant attention recently.\nHowever, many existing approaches heavily depend on perturbations, and\ninappropriate perturbations may corrupt the graph's inherent information. The\nVector Quantized Variational Autoencoder (VQ-VAE) is a powerful autoencoder\nextensively used in fields such as computer vision; however, its application to\ngraph data remains underexplored. In this paper, we provide an empirical\nanalysis of vector quantization in the context of graph autoencoders,\ndemonstrating its significant enhancement of the model's capacity to capture\ngraph topology. Furthermore, we identify two key challenges associated with\nvector quantization when applying in graph data: codebook underutilization and\ncodebook space sparsity. For the first challenge, we propose an annealing-based\nencoding strategy that promotes broad code utilization in the early stages of\ntraining, gradually shifting focus toward the most effective codes as training\nprogresses. For the second challenge, we introduce a hierarchical two-layer\ncodebook that captures relationships between embeddings through clustering. The\nsecond layer codebook links similar codes, encouraging the model to learn\ncloser embeddings for nodes with similar features and structural topology in\nthe graph. Our proposed model outperforms 16 representative baseline methods in\nself-supervised link prediction and node classification tasks across multiple\ndatasets.\n", "link": "http://arxiv.org/abs/2504.12715v1", "date": "2025-04-17", "relevancy": 2.5861, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.544}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5379}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Vector%20Quantized%20Graph%20Autoencoder%20with%20Annealing-Based%0A%20%20Code%20Selection&body=Title%3A%20Hierarchical%20Vector%20Quantized%20Graph%20Autoencoder%20with%20Annealing-Based%0A%20%20Code%20Selection%0AAuthor%3A%20Long%20Zeng%20and%20Jianxiang%20Yu%20and%20Jiapeng%20Zhu%20and%20Qingsong%20Zhong%20and%20Xiang%20Li%0AAbstract%3A%20%20%20Graph%20self-supervised%20learning%20has%20gained%20significant%20attention%20recently.%0AHowever%2C%20many%20existing%20approaches%20heavily%20depend%20on%20perturbations%2C%20and%0Ainappropriate%20perturbations%20may%20corrupt%20the%20graph%27s%20inherent%20information.%20The%0AVector%20Quantized%20Variational%20Autoencoder%20%28VQ-VAE%29%20is%20a%20powerful%20autoencoder%0Aextensively%20used%20in%20fields%20such%20as%20computer%20vision%3B%20however%2C%20its%20application%20to%0Agraph%20data%20remains%20underexplored.%20In%20this%20paper%2C%20we%20provide%20an%20empirical%0Aanalysis%20of%20vector%20quantization%20in%20the%20context%20of%20graph%20autoencoders%2C%0Ademonstrating%20its%20significant%20enhancement%20of%20the%20model%27s%20capacity%20to%20capture%0Agraph%20topology.%20Furthermore%2C%20we%20identify%20two%20key%20challenges%20associated%20with%0Avector%20quantization%20when%20applying%20in%20graph%20data%3A%20codebook%20underutilization%20and%0Acodebook%20space%20sparsity.%20For%20the%20first%20challenge%2C%20we%20propose%20an%20annealing-based%0Aencoding%20strategy%20that%20promotes%20broad%20code%20utilization%20in%20the%20early%20stages%20of%0Atraining%2C%20gradually%20shifting%20focus%20toward%20the%20most%20effective%20codes%20as%20training%0Aprogresses.%20For%20the%20second%20challenge%2C%20we%20introduce%20a%20hierarchical%20two-layer%0Acodebook%20that%20captures%20relationships%20between%20embeddings%20through%20clustering.%20The%0Asecond%20layer%20codebook%20links%20similar%20codes%2C%20encouraging%20the%20model%20to%20learn%0Acloser%20embeddings%20for%20nodes%20with%20similar%20features%20and%20structural%20topology%20in%0Athe%20graph.%20Our%20proposed%20model%20outperforms%2016%20representative%20baseline%20methods%20in%0Aself-supervised%20link%20prediction%20and%20node%20classification%20tasks%20across%20multiple%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12715v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Vector%2520Quantized%2520Graph%2520Autoencoder%2520with%2520Annealing-Based%250A%2520%2520Code%2520Selection%26entry.906535625%3DLong%2520Zeng%2520and%2520Jianxiang%2520Yu%2520and%2520Jiapeng%2520Zhu%2520and%2520Qingsong%2520Zhong%2520and%2520Xiang%2520Li%26entry.1292438233%3D%2520%2520Graph%2520self-supervised%2520learning%2520has%2520gained%2520significant%2520attention%2520recently.%250AHowever%252C%2520many%2520existing%2520approaches%2520heavily%2520depend%2520on%2520perturbations%252C%2520and%250Ainappropriate%2520perturbations%2520may%2520corrupt%2520the%2520graph%2527s%2520inherent%2520information.%2520The%250AVector%2520Quantized%2520Variational%2520Autoencoder%2520%2528VQ-VAE%2529%2520is%2520a%2520powerful%2520autoencoder%250Aextensively%2520used%2520in%2520fields%2520such%2520as%2520computer%2520vision%253B%2520however%252C%2520its%2520application%2520to%250Agraph%2520data%2520remains%2520underexplored.%2520In%2520this%2520paper%252C%2520we%2520provide%2520an%2520empirical%250Aanalysis%2520of%2520vector%2520quantization%2520in%2520the%2520context%2520of%2520graph%2520autoencoders%252C%250Ademonstrating%2520its%2520significant%2520enhancement%2520of%2520the%2520model%2527s%2520capacity%2520to%2520capture%250Agraph%2520topology.%2520Furthermore%252C%2520we%2520identify%2520two%2520key%2520challenges%2520associated%2520with%250Avector%2520quantization%2520when%2520applying%2520in%2520graph%2520data%253A%2520codebook%2520underutilization%2520and%250Acodebook%2520space%2520sparsity.%2520For%2520the%2520first%2520challenge%252C%2520we%2520propose%2520an%2520annealing-based%250Aencoding%2520strategy%2520that%2520promotes%2520broad%2520code%2520utilization%2520in%2520the%2520early%2520stages%2520of%250Atraining%252C%2520gradually%2520shifting%2520focus%2520toward%2520the%2520most%2520effective%2520codes%2520as%2520training%250Aprogresses.%2520For%2520the%2520second%2520challenge%252C%2520we%2520introduce%2520a%2520hierarchical%2520two-layer%250Acodebook%2520that%2520captures%2520relationships%2520between%2520embeddings%2520through%2520clustering.%2520The%250Asecond%2520layer%2520codebook%2520links%2520similar%2520codes%252C%2520encouraging%2520the%2520model%2520to%2520learn%250Acloser%2520embeddings%2520for%2520nodes%2520with%2520similar%2520features%2520and%2520structural%2520topology%2520in%250Athe%2520graph.%2520Our%2520proposed%2520model%2520outperforms%252016%2520representative%2520baseline%2520methods%2520in%250Aself-supervised%2520link%2520prediction%2520and%2520node%2520classification%2520tasks%2520across%2520multiple%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12715v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Vector%20Quantized%20Graph%20Autoencoder%20with%20Annealing-Based%0A%20%20Code%20Selection&entry.906535625=Long%20Zeng%20and%20Jianxiang%20Yu%20and%20Jiapeng%20Zhu%20and%20Qingsong%20Zhong%20and%20Xiang%20Li&entry.1292438233=%20%20Graph%20self-supervised%20learning%20has%20gained%20significant%20attention%20recently.%0AHowever%2C%20many%20existing%20approaches%20heavily%20depend%20on%20perturbations%2C%20and%0Ainappropriate%20perturbations%20may%20corrupt%20the%20graph%27s%20inherent%20information.%20The%0AVector%20Quantized%20Variational%20Autoencoder%20%28VQ-VAE%29%20is%20a%20powerful%20autoencoder%0Aextensively%20used%20in%20fields%20such%20as%20computer%20vision%3B%20however%2C%20its%20application%20to%0Agraph%20data%20remains%20underexplored.%20In%20this%20paper%2C%20we%20provide%20an%20empirical%0Aanalysis%20of%20vector%20quantization%20in%20the%20context%20of%20graph%20autoencoders%2C%0Ademonstrating%20its%20significant%20enhancement%20of%20the%20model%27s%20capacity%20to%20capture%0Agraph%20topology.%20Furthermore%2C%20we%20identify%20two%20key%20challenges%20associated%20with%0Avector%20quantization%20when%20applying%20in%20graph%20data%3A%20codebook%20underutilization%20and%0Acodebook%20space%20sparsity.%20For%20the%20first%20challenge%2C%20we%20propose%20an%20annealing-based%0Aencoding%20strategy%20that%20promotes%20broad%20code%20utilization%20in%20the%20early%20stages%20of%0Atraining%2C%20gradually%20shifting%20focus%20toward%20the%20most%20effective%20codes%20as%20training%0Aprogresses.%20For%20the%20second%20challenge%2C%20we%20introduce%20a%20hierarchical%20two-layer%0Acodebook%20that%20captures%20relationships%20between%20embeddings%20through%20clustering.%20The%0Asecond%20layer%20codebook%20links%20similar%20codes%2C%20encouraging%20the%20model%20to%20learn%0Acloser%20embeddings%20for%20nodes%20with%20similar%20features%20and%20structural%20topology%20in%0Athe%20graph.%20Our%20proposed%20model%20outperforms%2016%20representative%20baseline%20methods%20in%0Aself-supervised%20link%20prediction%20and%20node%20classification%20tasks%20across%20multiple%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12715v1&entry.124074799=Read"},
{"title": "Asynchronous Graph Generator", "author": "Christopher P. Ley and Felipe Tobar", "abstract": "  We introduce the asynchronous graph generator (AGG), a novel graph attention\nnetwork for imputation and prediction of multi-channel time series. Free from\nrecurrent components or assumptions about temporal/spatial regularity, AGG\nencodes measurements, timestamps and channel-specific features directly in the\nnodes via learnable embeddings. Through an attention mechanism, these\nembeddings allow for discovering expressive relationships among the variables\nof interest in the form of a homogeneous graph. Once trained, AGG performs\nimputation by \\emph{conditional attention generation}, i.e., by creating a new\nnode conditioned on given timestamps and channel specification. The proposed\nAGG is compared to related methods in the literature and its performance is\nanalysed from a data augmentation perspective. Our experiments reveal that AGG\nachieved state-of-the-art results in time series imputation, classification and\nprediction for the benchmark datasets \\emph{Beijing Air Quality},\n\\emph{PhysioNet ICU 2012} and \\emph{UCI localisation}, outperforming other\nrecent attention-based networks.\n", "link": "http://arxiv.org/abs/2309.17335v4", "date": "2025-04-16", "relevancy": 2.5853, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5297}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5152}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Asynchronous%20Graph%20Generator&body=Title%3A%20Asynchronous%20Graph%20Generator%0AAuthor%3A%20Christopher%20P.%20Ley%20and%20Felipe%20Tobar%0AAbstract%3A%20%20%20We%20introduce%20the%20asynchronous%20graph%20generator%20%28AGG%29%2C%20a%20novel%20graph%20attention%0Anetwork%20for%20imputation%20and%20prediction%20of%20multi-channel%20time%20series.%20Free%20from%0Arecurrent%20components%20or%20assumptions%20about%20temporal/spatial%20regularity%2C%20AGG%0Aencodes%20measurements%2C%20timestamps%20and%20channel-specific%20features%20directly%20in%20the%0Anodes%20via%20learnable%20embeddings.%20Through%20an%20attention%20mechanism%2C%20these%0Aembeddings%20allow%20for%20discovering%20expressive%20relationships%20among%20the%20variables%0Aof%20interest%20in%20the%20form%20of%20a%20homogeneous%20graph.%20Once%20trained%2C%20AGG%20performs%0Aimputation%20by%20%5Cemph%7Bconditional%20attention%20generation%7D%2C%20i.e.%2C%20by%20creating%20a%20new%0Anode%20conditioned%20on%20given%20timestamps%20and%20channel%20specification.%20The%20proposed%0AAGG%20is%20compared%20to%20related%20methods%20in%20the%20literature%20and%20its%20performance%20is%0Aanalysed%20from%20a%20data%20augmentation%20perspective.%20Our%20experiments%20reveal%20that%20AGG%0Aachieved%20state-of-the-art%20results%20in%20time%20series%20imputation%2C%20classification%20and%0Aprediction%20for%20the%20benchmark%20datasets%20%5Cemph%7BBeijing%20Air%20Quality%7D%2C%0A%5Cemph%7BPhysioNet%20ICU%202012%7D%20and%20%5Cemph%7BUCI%20localisation%7D%2C%20outperforming%20other%0Arecent%20attention-based%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.17335v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAsynchronous%2520Graph%2520Generator%26entry.906535625%3DChristopher%2520P.%2520Ley%2520and%2520Felipe%2520Tobar%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520asynchronous%2520graph%2520generator%2520%2528AGG%2529%252C%2520a%2520novel%2520graph%2520attention%250Anetwork%2520for%2520imputation%2520and%2520prediction%2520of%2520multi-channel%2520time%2520series.%2520Free%2520from%250Arecurrent%2520components%2520or%2520assumptions%2520about%2520temporal/spatial%2520regularity%252C%2520AGG%250Aencodes%2520measurements%252C%2520timestamps%2520and%2520channel-specific%2520features%2520directly%2520in%2520the%250Anodes%2520via%2520learnable%2520embeddings.%2520Through%2520an%2520attention%2520mechanism%252C%2520these%250Aembeddings%2520allow%2520for%2520discovering%2520expressive%2520relationships%2520among%2520the%2520variables%250Aof%2520interest%2520in%2520the%2520form%2520of%2520a%2520homogeneous%2520graph.%2520Once%2520trained%252C%2520AGG%2520performs%250Aimputation%2520by%2520%255Cemph%257Bconditional%2520attention%2520generation%257D%252C%2520i.e.%252C%2520by%2520creating%2520a%2520new%250Anode%2520conditioned%2520on%2520given%2520timestamps%2520and%2520channel%2520specification.%2520The%2520proposed%250AAGG%2520is%2520compared%2520to%2520related%2520methods%2520in%2520the%2520literature%2520and%2520its%2520performance%2520is%250Aanalysed%2520from%2520a%2520data%2520augmentation%2520perspective.%2520Our%2520experiments%2520reveal%2520that%2520AGG%250Aachieved%2520state-of-the-art%2520results%2520in%2520time%2520series%2520imputation%252C%2520classification%2520and%250Aprediction%2520for%2520the%2520benchmark%2520datasets%2520%255Cemph%257BBeijing%2520Air%2520Quality%257D%252C%250A%255Cemph%257BPhysioNet%2520ICU%25202012%257D%2520and%2520%255Cemph%257BUCI%2520localisation%257D%252C%2520outperforming%2520other%250Arecent%2520attention-based%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.17335v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Asynchronous%20Graph%20Generator&entry.906535625=Christopher%20P.%20Ley%20and%20Felipe%20Tobar&entry.1292438233=%20%20We%20introduce%20the%20asynchronous%20graph%20generator%20%28AGG%29%2C%20a%20novel%20graph%20attention%0Anetwork%20for%20imputation%20and%20prediction%20of%20multi-channel%20time%20series.%20Free%20from%0Arecurrent%20components%20or%20assumptions%20about%20temporal/spatial%20regularity%2C%20AGG%0Aencodes%20measurements%2C%20timestamps%20and%20channel-specific%20features%20directly%20in%20the%0Anodes%20via%20learnable%20embeddings.%20Through%20an%20attention%20mechanism%2C%20these%0Aembeddings%20allow%20for%20discovering%20expressive%20relationships%20among%20the%20variables%0Aof%20interest%20in%20the%20form%20of%20a%20homogeneous%20graph.%20Once%20trained%2C%20AGG%20performs%0Aimputation%20by%20%5Cemph%7Bconditional%20attention%20generation%7D%2C%20i.e.%2C%20by%20creating%20a%20new%0Anode%20conditioned%20on%20given%20timestamps%20and%20channel%20specification.%20The%20proposed%0AAGG%20is%20compared%20to%20related%20methods%20in%20the%20literature%20and%20its%20performance%20is%0Aanalysed%20from%20a%20data%20augmentation%20perspective.%20Our%20experiments%20reveal%20that%20AGG%0Aachieved%20state-of-the-art%20results%20in%20time%20series%20imputation%2C%20classification%20and%0Aprediction%20for%20the%20benchmark%20datasets%20%5Cemph%7BBeijing%20Air%20Quality%7D%2C%0A%5Cemph%7BPhysioNet%20ICU%202012%7D%20and%20%5Cemph%7BUCI%20localisation%7D%2C%20outperforming%20other%0Arecent%20attention-based%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.17335v4&entry.124074799=Read"},
{"title": "Post-Hurricane Debris Segmentation Using Fine-Tuned Foundational Vision\n  Models", "author": "Kooshan Amini and Yuhao Liu and Jamie Ellen Padgett and Guha Balakrishnan and Ashok Veeraraghavan", "abstract": "  Timely and accurate detection of hurricane debris is critical for effective\ndisaster response and community resilience. While post-disaster aerial imagery\nis readily available, robust debris segmentation solutions applicable across\nmultiple disaster regions remain limited. Developing a generalized solution is\nchallenging due to varying environmental and imaging conditions that alter\ndebris' visual signatures across different regions, further compounded by the\nscarcity of training data. This study addresses these challenges by fine-tuning\npre-trained foundational vision models, achieving robust performance with a\nrelatively small, high-quality dataset. Specifically, this work introduces an\nopen-source dataset comprising approximately 1,200 manually annotated aerial\nRGB images from Hurricanes Ian, Ida, and Ike. To mitigate human biases and\nenhance data quality, labels from multiple annotators are strategically\naggregated and visual prompt engineering is employed. The resulting fine-tuned\nmodel, named fCLIPSeg, achieves a Dice score of 0.70 on data from Hurricane Ida\n-- a disaster event entirely excluded during training -- with virtually no\nfalse positives in debris-free areas. This work presents the first\nevent-agnostic debris segmentation model requiring only standard RGB imagery\nduring deployment, making it well-suited for rapid, large-scale post-disaster\nimpact assessments and recovery planning.\n", "link": "http://arxiv.org/abs/2504.12542v1", "date": "2025-04-17", "relevancy": 2.5783, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.518}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.518}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Post-Hurricane%20Debris%20Segmentation%20Using%20Fine-Tuned%20Foundational%20Vision%0A%20%20Models&body=Title%3A%20Post-Hurricane%20Debris%20Segmentation%20Using%20Fine-Tuned%20Foundational%20Vision%0A%20%20Models%0AAuthor%3A%20Kooshan%20Amini%20and%20Yuhao%20Liu%20and%20Jamie%20Ellen%20Padgett%20and%20Guha%20Balakrishnan%20and%20Ashok%20Veeraraghavan%0AAbstract%3A%20%20%20Timely%20and%20accurate%20detection%20of%20hurricane%20debris%20is%20critical%20for%20effective%0Adisaster%20response%20and%20community%20resilience.%20While%20post-disaster%20aerial%20imagery%0Ais%20readily%20available%2C%20robust%20debris%20segmentation%20solutions%20applicable%20across%0Amultiple%20disaster%20regions%20remain%20limited.%20Developing%20a%20generalized%20solution%20is%0Achallenging%20due%20to%20varying%20environmental%20and%20imaging%20conditions%20that%20alter%0Adebris%27%20visual%20signatures%20across%20different%20regions%2C%20further%20compounded%20by%20the%0Ascarcity%20of%20training%20data.%20This%20study%20addresses%20these%20challenges%20by%20fine-tuning%0Apre-trained%20foundational%20vision%20models%2C%20achieving%20robust%20performance%20with%20a%0Arelatively%20small%2C%20high-quality%20dataset.%20Specifically%2C%20this%20work%20introduces%20an%0Aopen-source%20dataset%20comprising%20approximately%201%2C200%20manually%20annotated%20aerial%0ARGB%20images%20from%20Hurricanes%20Ian%2C%20Ida%2C%20and%20Ike.%20To%20mitigate%20human%20biases%20and%0Aenhance%20data%20quality%2C%20labels%20from%20multiple%20annotators%20are%20strategically%0Aaggregated%20and%20visual%20prompt%20engineering%20is%20employed.%20The%20resulting%20fine-tuned%0Amodel%2C%20named%20fCLIPSeg%2C%20achieves%20a%20Dice%20score%20of%200.70%20on%20data%20from%20Hurricane%20Ida%0A--%20a%20disaster%20event%20entirely%20excluded%20during%20training%20--%20with%20virtually%20no%0Afalse%20positives%20in%20debris-free%20areas.%20This%20work%20presents%20the%20first%0Aevent-agnostic%20debris%20segmentation%20model%20requiring%20only%20standard%20RGB%20imagery%0Aduring%20deployment%2C%20making%20it%20well-suited%20for%20rapid%2C%20large-scale%20post-disaster%0Aimpact%20assessments%20and%20recovery%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12542v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPost-Hurricane%2520Debris%2520Segmentation%2520Using%2520Fine-Tuned%2520Foundational%2520Vision%250A%2520%2520Models%26entry.906535625%3DKooshan%2520Amini%2520and%2520Yuhao%2520Liu%2520and%2520Jamie%2520Ellen%2520Padgett%2520and%2520Guha%2520Balakrishnan%2520and%2520Ashok%2520Veeraraghavan%26entry.1292438233%3D%2520%2520Timely%2520and%2520accurate%2520detection%2520of%2520hurricane%2520debris%2520is%2520critical%2520for%2520effective%250Adisaster%2520response%2520and%2520community%2520resilience.%2520While%2520post-disaster%2520aerial%2520imagery%250Ais%2520readily%2520available%252C%2520robust%2520debris%2520segmentation%2520solutions%2520applicable%2520across%250Amultiple%2520disaster%2520regions%2520remain%2520limited.%2520Developing%2520a%2520generalized%2520solution%2520is%250Achallenging%2520due%2520to%2520varying%2520environmental%2520and%2520imaging%2520conditions%2520that%2520alter%250Adebris%2527%2520visual%2520signatures%2520across%2520different%2520regions%252C%2520further%2520compounded%2520by%2520the%250Ascarcity%2520of%2520training%2520data.%2520This%2520study%2520addresses%2520these%2520challenges%2520by%2520fine-tuning%250Apre-trained%2520foundational%2520vision%2520models%252C%2520achieving%2520robust%2520performance%2520with%2520a%250Arelatively%2520small%252C%2520high-quality%2520dataset.%2520Specifically%252C%2520this%2520work%2520introduces%2520an%250Aopen-source%2520dataset%2520comprising%2520approximately%25201%252C200%2520manually%2520annotated%2520aerial%250ARGB%2520images%2520from%2520Hurricanes%2520Ian%252C%2520Ida%252C%2520and%2520Ike.%2520To%2520mitigate%2520human%2520biases%2520and%250Aenhance%2520data%2520quality%252C%2520labels%2520from%2520multiple%2520annotators%2520are%2520strategically%250Aaggregated%2520and%2520visual%2520prompt%2520engineering%2520is%2520employed.%2520The%2520resulting%2520fine-tuned%250Amodel%252C%2520named%2520fCLIPSeg%252C%2520achieves%2520a%2520Dice%2520score%2520of%25200.70%2520on%2520data%2520from%2520Hurricane%2520Ida%250A--%2520a%2520disaster%2520event%2520entirely%2520excluded%2520during%2520training%2520--%2520with%2520virtually%2520no%250Afalse%2520positives%2520in%2520debris-free%2520areas.%2520This%2520work%2520presents%2520the%2520first%250Aevent-agnostic%2520debris%2520segmentation%2520model%2520requiring%2520only%2520standard%2520RGB%2520imagery%250Aduring%2520deployment%252C%2520making%2520it%2520well-suited%2520for%2520rapid%252C%2520large-scale%2520post-disaster%250Aimpact%2520assessments%2520and%2520recovery%2520planning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12542v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Post-Hurricane%20Debris%20Segmentation%20Using%20Fine-Tuned%20Foundational%20Vision%0A%20%20Models&entry.906535625=Kooshan%20Amini%20and%20Yuhao%20Liu%20and%20Jamie%20Ellen%20Padgett%20and%20Guha%20Balakrishnan%20and%20Ashok%20Veeraraghavan&entry.1292438233=%20%20Timely%20and%20accurate%20detection%20of%20hurricane%20debris%20is%20critical%20for%20effective%0Adisaster%20response%20and%20community%20resilience.%20While%20post-disaster%20aerial%20imagery%0Ais%20readily%20available%2C%20robust%20debris%20segmentation%20solutions%20applicable%20across%0Amultiple%20disaster%20regions%20remain%20limited.%20Developing%20a%20generalized%20solution%20is%0Achallenging%20due%20to%20varying%20environmental%20and%20imaging%20conditions%20that%20alter%0Adebris%27%20visual%20signatures%20across%20different%20regions%2C%20further%20compounded%20by%20the%0Ascarcity%20of%20training%20data.%20This%20study%20addresses%20these%20challenges%20by%20fine-tuning%0Apre-trained%20foundational%20vision%20models%2C%20achieving%20robust%20performance%20with%20a%0Arelatively%20small%2C%20high-quality%20dataset.%20Specifically%2C%20this%20work%20introduces%20an%0Aopen-source%20dataset%20comprising%20approximately%201%2C200%20manually%20annotated%20aerial%0ARGB%20images%20from%20Hurricanes%20Ian%2C%20Ida%2C%20and%20Ike.%20To%20mitigate%20human%20biases%20and%0Aenhance%20data%20quality%2C%20labels%20from%20multiple%20annotators%20are%20strategically%0Aaggregated%20and%20visual%20prompt%20engineering%20is%20employed.%20The%20resulting%20fine-tuned%0Amodel%2C%20named%20fCLIPSeg%2C%20achieves%20a%20Dice%20score%20of%200.70%20on%20data%20from%20Hurricane%20Ida%0A--%20a%20disaster%20event%20entirely%20excluded%20during%20training%20--%20with%20virtually%20no%0Afalse%20positives%20in%20debris-free%20areas.%20This%20work%20presents%20the%20first%0Aevent-agnostic%20debris%20segmentation%20model%20requiring%20only%20standard%20RGB%20imagery%0Aduring%20deployment%2C%20making%20it%20well-suited%20for%20rapid%2C%20large-scale%20post-disaster%0Aimpact%20assessments%20and%20recovery%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12542v1&entry.124074799=Read"},
{"title": "Continual Learning Strategies for 3D Engineering Regression Problems: A\n  Benchmarking Study", "author": "Kaira M. Samuel and Faez Ahmed", "abstract": "  Engineering problems that apply machine learning often involve\ncomputationally intensive methods but rely on limited datasets. As engineering\ndata evolves with new designs and constraints, models must incorporate new\nknowledge over time. However, high computational costs make retraining models\nfrom scratch infeasible. Continual learning (CL) offers a promising solution by\nenabling models to learn from sequential data while mitigating catastrophic\nforgetting, where a model forgets previously learned mappings. This work\nintroduces CL to engineering design by benchmarking several CL methods on\nrepresentative regression tasks. We apply these strategies to five engineering\ndatasets and construct nine new engineering CL benchmarks to evaluate their\nability to address forgetting and improve generalization. Preliminary results\nshow that applying existing CL methods to these tasks improves performance over\nnaive baselines. In particular, the Replay strategy achieved performance\ncomparable to retraining in several benchmarks while reducing training time by\nnearly half, demonstrating its potential for real-world engineering workflows.\nThe code and datasets used in this work will be available at:\nhttps://github.com/kmsamuel/cl-for-engineering-release.\n", "link": "http://arxiv.org/abs/2504.12503v1", "date": "2025-04-16", "relevancy": 2.5685, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5195}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5108}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Learning%20Strategies%20for%203D%20Engineering%20Regression%20Problems%3A%20A%0A%20%20Benchmarking%20Study&body=Title%3A%20Continual%20Learning%20Strategies%20for%203D%20Engineering%20Regression%20Problems%3A%20A%0A%20%20Benchmarking%20Study%0AAuthor%3A%20Kaira%20M.%20Samuel%20and%20Faez%20Ahmed%0AAbstract%3A%20%20%20Engineering%20problems%20that%20apply%20machine%20learning%20often%20involve%0Acomputationally%20intensive%20methods%20but%20rely%20on%20limited%20datasets.%20As%20engineering%0Adata%20evolves%20with%20new%20designs%20and%20constraints%2C%20models%20must%20incorporate%20new%0Aknowledge%20over%20time.%20However%2C%20high%20computational%20costs%20make%20retraining%20models%0Afrom%20scratch%20infeasible.%20Continual%20learning%20%28CL%29%20offers%20a%20promising%20solution%20by%0Aenabling%20models%20to%20learn%20from%20sequential%20data%20while%20mitigating%20catastrophic%0Aforgetting%2C%20where%20a%20model%20forgets%20previously%20learned%20mappings.%20This%20work%0Aintroduces%20CL%20to%20engineering%20design%20by%20benchmarking%20several%20CL%20methods%20on%0Arepresentative%20regression%20tasks.%20We%20apply%20these%20strategies%20to%20five%20engineering%0Adatasets%20and%20construct%20nine%20new%20engineering%20CL%20benchmarks%20to%20evaluate%20their%0Aability%20to%20address%20forgetting%20and%20improve%20generalization.%20Preliminary%20results%0Ashow%20that%20applying%20existing%20CL%20methods%20to%20these%20tasks%20improves%20performance%20over%0Anaive%20baselines.%20In%20particular%2C%20the%20Replay%20strategy%20achieved%20performance%0Acomparable%20to%20retraining%20in%20several%20benchmarks%20while%20reducing%20training%20time%20by%0Anearly%20half%2C%20demonstrating%20its%20potential%20for%20real-world%20engineering%20workflows.%0AThe%20code%20and%20datasets%20used%20in%20this%20work%20will%20be%20available%20at%3A%0Ahttps%3A//github.com/kmsamuel/cl-for-engineering-release.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12503v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Learning%2520Strategies%2520for%25203D%2520Engineering%2520Regression%2520Problems%253A%2520A%250A%2520%2520Benchmarking%2520Study%26entry.906535625%3DKaira%2520M.%2520Samuel%2520and%2520Faez%2520Ahmed%26entry.1292438233%3D%2520%2520Engineering%2520problems%2520that%2520apply%2520machine%2520learning%2520often%2520involve%250Acomputationally%2520intensive%2520methods%2520but%2520rely%2520on%2520limited%2520datasets.%2520As%2520engineering%250Adata%2520evolves%2520with%2520new%2520designs%2520and%2520constraints%252C%2520models%2520must%2520incorporate%2520new%250Aknowledge%2520over%2520time.%2520However%252C%2520high%2520computational%2520costs%2520make%2520retraining%2520models%250Afrom%2520scratch%2520infeasible.%2520Continual%2520learning%2520%2528CL%2529%2520offers%2520a%2520promising%2520solution%2520by%250Aenabling%2520models%2520to%2520learn%2520from%2520sequential%2520data%2520while%2520mitigating%2520catastrophic%250Aforgetting%252C%2520where%2520a%2520model%2520forgets%2520previously%2520learned%2520mappings.%2520This%2520work%250Aintroduces%2520CL%2520to%2520engineering%2520design%2520by%2520benchmarking%2520several%2520CL%2520methods%2520on%250Arepresentative%2520regression%2520tasks.%2520We%2520apply%2520these%2520strategies%2520to%2520five%2520engineering%250Adatasets%2520and%2520construct%2520nine%2520new%2520engineering%2520CL%2520benchmarks%2520to%2520evaluate%2520their%250Aability%2520to%2520address%2520forgetting%2520and%2520improve%2520generalization.%2520Preliminary%2520results%250Ashow%2520that%2520applying%2520existing%2520CL%2520methods%2520to%2520these%2520tasks%2520improves%2520performance%2520over%250Anaive%2520baselines.%2520In%2520particular%252C%2520the%2520Replay%2520strategy%2520achieved%2520performance%250Acomparable%2520to%2520retraining%2520in%2520several%2520benchmarks%2520while%2520reducing%2520training%2520time%2520by%250Anearly%2520half%252C%2520demonstrating%2520its%2520potential%2520for%2520real-world%2520engineering%2520workflows.%250AThe%2520code%2520and%2520datasets%2520used%2520in%2520this%2520work%2520will%2520be%2520available%2520at%253A%250Ahttps%253A//github.com/kmsamuel/cl-for-engineering-release.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12503v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Learning%20Strategies%20for%203D%20Engineering%20Regression%20Problems%3A%20A%0A%20%20Benchmarking%20Study&entry.906535625=Kaira%20M.%20Samuel%20and%20Faez%20Ahmed&entry.1292438233=%20%20Engineering%20problems%20that%20apply%20machine%20learning%20often%20involve%0Acomputationally%20intensive%20methods%20but%20rely%20on%20limited%20datasets.%20As%20engineering%0Adata%20evolves%20with%20new%20designs%20and%20constraints%2C%20models%20must%20incorporate%20new%0Aknowledge%20over%20time.%20However%2C%20high%20computational%20costs%20make%20retraining%20models%0Afrom%20scratch%20infeasible.%20Continual%20learning%20%28CL%29%20offers%20a%20promising%20solution%20by%0Aenabling%20models%20to%20learn%20from%20sequential%20data%20while%20mitigating%20catastrophic%0Aforgetting%2C%20where%20a%20model%20forgets%20previously%20learned%20mappings.%20This%20work%0Aintroduces%20CL%20to%20engineering%20design%20by%20benchmarking%20several%20CL%20methods%20on%0Arepresentative%20regression%20tasks.%20We%20apply%20these%20strategies%20to%20five%20engineering%0Adatasets%20and%20construct%20nine%20new%20engineering%20CL%20benchmarks%20to%20evaluate%20their%0Aability%20to%20address%20forgetting%20and%20improve%20generalization.%20Preliminary%20results%0Ashow%20that%20applying%20existing%20CL%20methods%20to%20these%20tasks%20improves%20performance%20over%0Anaive%20baselines.%20In%20particular%2C%20the%20Replay%20strategy%20achieved%20performance%0Acomparable%20to%20retraining%20in%20several%20benchmarks%20while%20reducing%20training%20time%20by%0Anearly%20half%2C%20demonstrating%20its%20potential%20for%20real-world%20engineering%20workflows.%0AThe%20code%20and%20datasets%20used%20in%20this%20work%20will%20be%20available%20at%3A%0Ahttps%3A//github.com/kmsamuel/cl-for-engineering-release.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12503v1&entry.124074799=Read"},
{"title": "Understanding Attention Mechanism in Video Diffusion Models", "author": "Bingyan Liu and Chengyu Wang and Tongtong Su and Huan Ten and Jun Huang and Kailing Guo and Kui Jia", "abstract": "  Text-to-video (T2V) synthesis models, such as OpenAI's Sora, have garnered\nsignificant attention due to their ability to generate high-quality videos from\na text prompt. In diffusion-based T2V models, the attention mechanism is a\ncritical component. However, it remains unclear what intermediate features are\nlearned and how attention blocks in T2V models affect various aspects of video\nsynthesis, such as image quality and temporal consistency. In this paper, we\nconduct an in-depth perturbation analysis of the spatial and temporal attention\nblocks of T2V models using an information-theoretic approach. Our results\nindicate that temporal and spatial attention maps affect not only the timing\nand layout of the videos but also the complexity of spatiotemporal elements and\nthe aesthetic quality of the synthesized videos. Notably, high-entropy\nattention maps are often key elements linked to superior video quality, whereas\nlow-entropy attention maps are associated with the video's intra-frame\nstructure. Based on our findings, we propose two novel methods to enhance video\nquality and enable text-guided video editing. These methods rely entirely on\nlightweight manipulation of the attention matrices in T2V models. The efficacy\nand effectiveness of our methods are further validated through experimental\nevaluation across multiple datasets.\n", "link": "http://arxiv.org/abs/2504.12027v2", "date": "2025-04-17", "relevancy": 2.5527, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7086}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6322}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Attention%20Mechanism%20in%20Video%20Diffusion%20Models&body=Title%3A%20Understanding%20Attention%20Mechanism%20in%20Video%20Diffusion%20Models%0AAuthor%3A%20Bingyan%20Liu%20and%20Chengyu%20Wang%20and%20Tongtong%20Su%20and%20Huan%20Ten%20and%20Jun%20Huang%20and%20Kailing%20Guo%20and%20Kui%20Jia%0AAbstract%3A%20%20%20Text-to-video%20%28T2V%29%20synthesis%20models%2C%20such%20as%20OpenAI%27s%20Sora%2C%20have%20garnered%0Asignificant%20attention%20due%20to%20their%20ability%20to%20generate%20high-quality%20videos%20from%0Aa%20text%20prompt.%20In%20diffusion-based%20T2V%20models%2C%20the%20attention%20mechanism%20is%20a%0Acritical%20component.%20However%2C%20it%20remains%20unclear%20what%20intermediate%20features%20are%0Alearned%20and%20how%20attention%20blocks%20in%20T2V%20models%20affect%20various%20aspects%20of%20video%0Asynthesis%2C%20such%20as%20image%20quality%20and%20temporal%20consistency.%20In%20this%20paper%2C%20we%0Aconduct%20an%20in-depth%20perturbation%20analysis%20of%20the%20spatial%20and%20temporal%20attention%0Ablocks%20of%20T2V%20models%20using%20an%20information-theoretic%20approach.%20Our%20results%0Aindicate%20that%20temporal%20and%20spatial%20attention%20maps%20affect%20not%20only%20the%20timing%0Aand%20layout%20of%20the%20videos%20but%20also%20the%20complexity%20of%20spatiotemporal%20elements%20and%0Athe%20aesthetic%20quality%20of%20the%20synthesized%20videos.%20Notably%2C%20high-entropy%0Aattention%20maps%20are%20often%20key%20elements%20linked%20to%20superior%20video%20quality%2C%20whereas%0Alow-entropy%20attention%20maps%20are%20associated%20with%20the%20video%27s%20intra-frame%0Astructure.%20Based%20on%20our%20findings%2C%20we%20propose%20two%20novel%20methods%20to%20enhance%20video%0Aquality%20and%20enable%20text-guided%20video%20editing.%20These%20methods%20rely%20entirely%20on%0Alightweight%20manipulation%20of%20the%20attention%20matrices%20in%20T2V%20models.%20The%20efficacy%0Aand%20effectiveness%20of%20our%20methods%20are%20further%20validated%20through%20experimental%0Aevaluation%20across%20multiple%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12027v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Attention%2520Mechanism%2520in%2520Video%2520Diffusion%2520Models%26entry.906535625%3DBingyan%2520Liu%2520and%2520Chengyu%2520Wang%2520and%2520Tongtong%2520Su%2520and%2520Huan%2520Ten%2520and%2520Jun%2520Huang%2520and%2520Kailing%2520Guo%2520and%2520Kui%2520Jia%26entry.1292438233%3D%2520%2520Text-to-video%2520%2528T2V%2529%2520synthesis%2520models%252C%2520such%2520as%2520OpenAI%2527s%2520Sora%252C%2520have%2520garnered%250Asignificant%2520attention%2520due%2520to%2520their%2520ability%2520to%2520generate%2520high-quality%2520videos%2520from%250Aa%2520text%2520prompt.%2520In%2520diffusion-based%2520T2V%2520models%252C%2520the%2520attention%2520mechanism%2520is%2520a%250Acritical%2520component.%2520However%252C%2520it%2520remains%2520unclear%2520what%2520intermediate%2520features%2520are%250Alearned%2520and%2520how%2520attention%2520blocks%2520in%2520T2V%2520models%2520affect%2520various%2520aspects%2520of%2520video%250Asynthesis%252C%2520such%2520as%2520image%2520quality%2520and%2520temporal%2520consistency.%2520In%2520this%2520paper%252C%2520we%250Aconduct%2520an%2520in-depth%2520perturbation%2520analysis%2520of%2520the%2520spatial%2520and%2520temporal%2520attention%250Ablocks%2520of%2520T2V%2520models%2520using%2520an%2520information-theoretic%2520approach.%2520Our%2520results%250Aindicate%2520that%2520temporal%2520and%2520spatial%2520attention%2520maps%2520affect%2520not%2520only%2520the%2520timing%250Aand%2520layout%2520of%2520the%2520videos%2520but%2520also%2520the%2520complexity%2520of%2520spatiotemporal%2520elements%2520and%250Athe%2520aesthetic%2520quality%2520of%2520the%2520synthesized%2520videos.%2520Notably%252C%2520high-entropy%250Aattention%2520maps%2520are%2520often%2520key%2520elements%2520linked%2520to%2520superior%2520video%2520quality%252C%2520whereas%250Alow-entropy%2520attention%2520maps%2520are%2520associated%2520with%2520the%2520video%2527s%2520intra-frame%250Astructure.%2520Based%2520on%2520our%2520findings%252C%2520we%2520propose%2520two%2520novel%2520methods%2520to%2520enhance%2520video%250Aquality%2520and%2520enable%2520text-guided%2520video%2520editing.%2520These%2520methods%2520rely%2520entirely%2520on%250Alightweight%2520manipulation%2520of%2520the%2520attention%2520matrices%2520in%2520T2V%2520models.%2520The%2520efficacy%250Aand%2520effectiveness%2520of%2520our%2520methods%2520are%2520further%2520validated%2520through%2520experimental%250Aevaluation%2520across%2520multiple%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12027v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Attention%20Mechanism%20in%20Video%20Diffusion%20Models&entry.906535625=Bingyan%20Liu%20and%20Chengyu%20Wang%20and%20Tongtong%20Su%20and%20Huan%20Ten%20and%20Jun%20Huang%20and%20Kailing%20Guo%20and%20Kui%20Jia&entry.1292438233=%20%20Text-to-video%20%28T2V%29%20synthesis%20models%2C%20such%20as%20OpenAI%27s%20Sora%2C%20have%20garnered%0Asignificant%20attention%20due%20to%20their%20ability%20to%20generate%20high-quality%20videos%20from%0Aa%20text%20prompt.%20In%20diffusion-based%20T2V%20models%2C%20the%20attention%20mechanism%20is%20a%0Acritical%20component.%20However%2C%20it%20remains%20unclear%20what%20intermediate%20features%20are%0Alearned%20and%20how%20attention%20blocks%20in%20T2V%20models%20affect%20various%20aspects%20of%20video%0Asynthesis%2C%20such%20as%20image%20quality%20and%20temporal%20consistency.%20In%20this%20paper%2C%20we%0Aconduct%20an%20in-depth%20perturbation%20analysis%20of%20the%20spatial%20and%20temporal%20attention%0Ablocks%20of%20T2V%20models%20using%20an%20information-theoretic%20approach.%20Our%20results%0Aindicate%20that%20temporal%20and%20spatial%20attention%20maps%20affect%20not%20only%20the%20timing%0Aand%20layout%20of%20the%20videos%20but%20also%20the%20complexity%20of%20spatiotemporal%20elements%20and%0Athe%20aesthetic%20quality%20of%20the%20synthesized%20videos.%20Notably%2C%20high-entropy%0Aattention%20maps%20are%20often%20key%20elements%20linked%20to%20superior%20video%20quality%2C%20whereas%0Alow-entropy%20attention%20maps%20are%20associated%20with%20the%20video%27s%20intra-frame%0Astructure.%20Based%20on%20our%20findings%2C%20we%20propose%20two%20novel%20methods%20to%20enhance%20video%0Aquality%20and%20enable%20text-guided%20video%20editing.%20These%20methods%20rely%20entirely%20on%0Alightweight%20manipulation%20of%20the%20attention%20matrices%20in%20T2V%20models.%20The%20efficacy%0Aand%20effectiveness%20of%20our%20methods%20are%20further%20validated%20through%20experimental%0Aevaluation%20across%20multiple%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12027v2&entry.124074799=Read"},
{"title": "A Genetic Approach to Gradient-Free Kinodynamic Planning in Uneven\n  Terrains", "author": "Otobong Jerome and Alexandr Klimchik and Alexander Maloletov and Geesara Kulathunga", "abstract": "  This paper proposes a genetic algorithm-based kinodynamic planning algorithm\n(GAKD) for car-like vehicles navigating uneven terrains modeled as triangular\nmeshes. The algorithm's distinct feature is trajectory optimization over a\nfixed-length receding horizon using a genetic algorithm with heuristic-based\nmutation, ensuring the vehicle's controls remain within its valid operational\nrange. By addressing challenges posed by uneven terrain meshes, such as\nchanging face normals, GAKD offers a practical solution for path planning in\ncomplex environments. Comparative evaluations against Model Predictive Path\nIntegral (MPPI) and log-MPPI methods show that GAKD achieves up to 20 percent\nimprovement in traversability cost while maintaining comparable path length.\nThese results demonstrate GAKD's potential in improving vehicle navigation on\nchallenging terrains.\n", "link": "http://arxiv.org/abs/2504.12678v1", "date": "2025-04-17", "relevancy": 2.5509, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5268}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5087}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Genetic%20Approach%20to%20Gradient-Free%20Kinodynamic%20Planning%20in%20Uneven%0A%20%20Terrains&body=Title%3A%20A%20Genetic%20Approach%20to%20Gradient-Free%20Kinodynamic%20Planning%20in%20Uneven%0A%20%20Terrains%0AAuthor%3A%20Otobong%20Jerome%20and%20Alexandr%20Klimchik%20and%20Alexander%20Maloletov%20and%20Geesara%20Kulathunga%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20genetic%20algorithm-based%20kinodynamic%20planning%20algorithm%0A%28GAKD%29%20for%20car-like%20vehicles%20navigating%20uneven%20terrains%20modeled%20as%20triangular%0Ameshes.%20The%20algorithm%27s%20distinct%20feature%20is%20trajectory%20optimization%20over%20a%0Afixed-length%20receding%20horizon%20using%20a%20genetic%20algorithm%20with%20heuristic-based%0Amutation%2C%20ensuring%20the%20vehicle%27s%20controls%20remain%20within%20its%20valid%20operational%0Arange.%20By%20addressing%20challenges%20posed%20by%20uneven%20terrain%20meshes%2C%20such%20as%0Achanging%20face%20normals%2C%20GAKD%20offers%20a%20practical%20solution%20for%20path%20planning%20in%0Acomplex%20environments.%20Comparative%20evaluations%20against%20Model%20Predictive%20Path%0AIntegral%20%28MPPI%29%20and%20log-MPPI%20methods%20show%20that%20GAKD%20achieves%20up%20to%2020%20percent%0Aimprovement%20in%20traversability%20cost%20while%20maintaining%20comparable%20path%20length.%0AThese%20results%20demonstrate%20GAKD%27s%20potential%20in%20improving%20vehicle%20navigation%20on%0Achallenging%20terrains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12678v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Genetic%2520Approach%2520to%2520Gradient-Free%2520Kinodynamic%2520Planning%2520in%2520Uneven%250A%2520%2520Terrains%26entry.906535625%3DOtobong%2520Jerome%2520and%2520Alexandr%2520Klimchik%2520and%2520Alexander%2520Maloletov%2520and%2520Geesara%2520Kulathunga%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520genetic%2520algorithm-based%2520kinodynamic%2520planning%2520algorithm%250A%2528GAKD%2529%2520for%2520car-like%2520vehicles%2520navigating%2520uneven%2520terrains%2520modeled%2520as%2520triangular%250Ameshes.%2520The%2520algorithm%2527s%2520distinct%2520feature%2520is%2520trajectory%2520optimization%2520over%2520a%250Afixed-length%2520receding%2520horizon%2520using%2520a%2520genetic%2520algorithm%2520with%2520heuristic-based%250Amutation%252C%2520ensuring%2520the%2520vehicle%2527s%2520controls%2520remain%2520within%2520its%2520valid%2520operational%250Arange.%2520By%2520addressing%2520challenges%2520posed%2520by%2520uneven%2520terrain%2520meshes%252C%2520such%2520as%250Achanging%2520face%2520normals%252C%2520GAKD%2520offers%2520a%2520practical%2520solution%2520for%2520path%2520planning%2520in%250Acomplex%2520environments.%2520Comparative%2520evaluations%2520against%2520Model%2520Predictive%2520Path%250AIntegral%2520%2528MPPI%2529%2520and%2520log-MPPI%2520methods%2520show%2520that%2520GAKD%2520achieves%2520up%2520to%252020%2520percent%250Aimprovement%2520in%2520traversability%2520cost%2520while%2520maintaining%2520comparable%2520path%2520length.%250AThese%2520results%2520demonstrate%2520GAKD%2527s%2520potential%2520in%2520improving%2520vehicle%2520navigation%2520on%250Achallenging%2520terrains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12678v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Genetic%20Approach%20to%20Gradient-Free%20Kinodynamic%20Planning%20in%20Uneven%0A%20%20Terrains&entry.906535625=Otobong%20Jerome%20and%20Alexandr%20Klimchik%20and%20Alexander%20Maloletov%20and%20Geesara%20Kulathunga&entry.1292438233=%20%20This%20paper%20proposes%20a%20genetic%20algorithm-based%20kinodynamic%20planning%20algorithm%0A%28GAKD%29%20for%20car-like%20vehicles%20navigating%20uneven%20terrains%20modeled%20as%20triangular%0Ameshes.%20The%20algorithm%27s%20distinct%20feature%20is%20trajectory%20optimization%20over%20a%0Afixed-length%20receding%20horizon%20using%20a%20genetic%20algorithm%20with%20heuristic-based%0Amutation%2C%20ensuring%20the%20vehicle%27s%20controls%20remain%20within%20its%20valid%20operational%0Arange.%20By%20addressing%20challenges%20posed%20by%20uneven%20terrain%20meshes%2C%20such%20as%0Achanging%20face%20normals%2C%20GAKD%20offers%20a%20practical%20solution%20for%20path%20planning%20in%0Acomplex%20environments.%20Comparative%20evaluations%20against%20Model%20Predictive%20Path%0AIntegral%20%28MPPI%29%20and%20log-MPPI%20methods%20show%20that%20GAKD%20achieves%20up%20to%2020%20percent%0Aimprovement%20in%20traversability%20cost%20while%20maintaining%20comparable%20path%20length.%0AThese%20results%20demonstrate%20GAKD%27s%20potential%20in%20improving%20vehicle%20navigation%20on%0Achallenging%20terrains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12678v1&entry.124074799=Read"},
{"title": "Perceive With Confidence: Statistical Safety Assurances for Navigation\n  with Learning-Based Perception", "author": "Zhiting Mei and Anushri Dixit and Meghan Booker and Emily Zhou and Mariko Storey-Matsutani and Allen Z. Ren and Ola Shorinwa and Anirudha Majumdar", "abstract": "  Rapid advances in perception have enabled large pre-trained models to be used\nout of the box for transforming high-dimensional, noisy, and partial\nobservations of the world into rich occupancy representations. However, the\nreliability of these models and consequently their safe integration onto robots\nremains unknown when deployed in environments unseen during training. To\nprovide safety guarantees, we rigorously quantify the uncertainty of\npre-trained perception systems for object detection and scene completion via a\nnovel calibration technique based on conformal prediction. Crucially, this\nprocedure guarantees robustness to distribution shifts in states when\nperception outputs are used in conjunction with a planner. As a result, the\ncalibrated perception system can be used in combination with any safe planner\nto provide an end-to-end statistical assurance on safety in unseen\nenvironments. We evaluate the resulting approach, Perceive with Confidence\n(PwC), in simulation and on hardware where a quadruped robot navigates through\npreviously unseen indoor, static environments. These experiments validate the\nsafety assurances for obstacle avoidance provided by PwC. In simulation, our\nmethod reduces obstacle misdetection by $70\\%$ compared to uncalibrated\nperception models. While misdetections lead to collisions for baseline methods,\nour approach consistently achieves $100\\%$ safety. We further demonstrate\nreducing the conservatism of our method without sacrificing safety, achieving a\n$46\\%$ increase in success rates in challenging environments while maintaining\n$100\\%$ safety. In hardware experiments, our method improves empirical safety\nby $40\\%$ over baselines and reduces obstacle misdetection by $93.3\\%$. The\nsafety gap widens to $46.7\\%$ when navigation speed increases, highlighting our\napproach's robustness under more demanding conditions.\n", "link": "http://arxiv.org/abs/2403.08185v3", "date": "2025-04-17", "relevancy": 2.5327, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6705}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6478}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perceive%20With%20Confidence%3A%20Statistical%20Safety%20Assurances%20for%20Navigation%0A%20%20with%20Learning-Based%20Perception&body=Title%3A%20Perceive%20With%20Confidence%3A%20Statistical%20Safety%20Assurances%20for%20Navigation%0A%20%20with%20Learning-Based%20Perception%0AAuthor%3A%20Zhiting%20Mei%20and%20Anushri%20Dixit%20and%20Meghan%20Booker%20and%20Emily%20Zhou%20and%20Mariko%20Storey-Matsutani%20and%20Allen%20Z.%20Ren%20and%20Ola%20Shorinwa%20and%20Anirudha%20Majumdar%0AAbstract%3A%20%20%20Rapid%20advances%20in%20perception%20have%20enabled%20large%20pre-trained%20models%20to%20be%20used%0Aout%20of%20the%20box%20for%20transforming%20high-dimensional%2C%20noisy%2C%20and%20partial%0Aobservations%20of%20the%20world%20into%20rich%20occupancy%20representations.%20However%2C%20the%0Areliability%20of%20these%20models%20and%20consequently%20their%20safe%20integration%20onto%20robots%0Aremains%20unknown%20when%20deployed%20in%20environments%20unseen%20during%20training.%20To%0Aprovide%20safety%20guarantees%2C%20we%20rigorously%20quantify%20the%20uncertainty%20of%0Apre-trained%20perception%20systems%20for%20object%20detection%20and%20scene%20completion%20via%20a%0Anovel%20calibration%20technique%20based%20on%20conformal%20prediction.%20Crucially%2C%20this%0Aprocedure%20guarantees%20robustness%20to%20distribution%20shifts%20in%20states%20when%0Aperception%20outputs%20are%20used%20in%20conjunction%20with%20a%20planner.%20As%20a%20result%2C%20the%0Acalibrated%20perception%20system%20can%20be%20used%20in%20combination%20with%20any%20safe%20planner%0Ato%20provide%20an%20end-to-end%20statistical%20assurance%20on%20safety%20in%20unseen%0Aenvironments.%20We%20evaluate%20the%20resulting%20approach%2C%20Perceive%20with%20Confidence%0A%28PwC%29%2C%20in%20simulation%20and%20on%20hardware%20where%20a%20quadruped%20robot%20navigates%20through%0Apreviously%20unseen%20indoor%2C%20static%20environments.%20These%20experiments%20validate%20the%0Asafety%20assurances%20for%20obstacle%20avoidance%20provided%20by%20PwC.%20In%20simulation%2C%20our%0Amethod%20reduces%20obstacle%20misdetection%20by%20%2470%5C%25%24%20compared%20to%20uncalibrated%0Aperception%20models.%20While%20misdetections%20lead%20to%20collisions%20for%20baseline%20methods%2C%0Aour%20approach%20consistently%20achieves%20%24100%5C%25%24%20safety.%20We%20further%20demonstrate%0Areducing%20the%20conservatism%20of%20our%20method%20without%20sacrificing%20safety%2C%20achieving%20a%0A%2446%5C%25%24%20increase%20in%20success%20rates%20in%20challenging%20environments%20while%20maintaining%0A%24100%5C%25%24%20safety.%20In%20hardware%20experiments%2C%20our%20method%20improves%20empirical%20safety%0Aby%20%2440%5C%25%24%20over%20baselines%20and%20reduces%20obstacle%20misdetection%20by%20%2493.3%5C%25%24.%20The%0Asafety%20gap%20widens%20to%20%2446.7%5C%25%24%20when%20navigation%20speed%20increases%2C%20highlighting%20our%0Aapproach%27s%20robustness%20under%20more%20demanding%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08185v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerceive%2520With%2520Confidence%253A%2520Statistical%2520Safety%2520Assurances%2520for%2520Navigation%250A%2520%2520with%2520Learning-Based%2520Perception%26entry.906535625%3DZhiting%2520Mei%2520and%2520Anushri%2520Dixit%2520and%2520Meghan%2520Booker%2520and%2520Emily%2520Zhou%2520and%2520Mariko%2520Storey-Matsutani%2520and%2520Allen%2520Z.%2520Ren%2520and%2520Ola%2520Shorinwa%2520and%2520Anirudha%2520Majumdar%26entry.1292438233%3D%2520%2520Rapid%2520advances%2520in%2520perception%2520have%2520enabled%2520large%2520pre-trained%2520models%2520to%2520be%2520used%250Aout%2520of%2520the%2520box%2520for%2520transforming%2520high-dimensional%252C%2520noisy%252C%2520and%2520partial%250Aobservations%2520of%2520the%2520world%2520into%2520rich%2520occupancy%2520representations.%2520However%252C%2520the%250Areliability%2520of%2520these%2520models%2520and%2520consequently%2520their%2520safe%2520integration%2520onto%2520robots%250Aremains%2520unknown%2520when%2520deployed%2520in%2520environments%2520unseen%2520during%2520training.%2520To%250Aprovide%2520safety%2520guarantees%252C%2520we%2520rigorously%2520quantify%2520the%2520uncertainty%2520of%250Apre-trained%2520perception%2520systems%2520for%2520object%2520detection%2520and%2520scene%2520completion%2520via%2520a%250Anovel%2520calibration%2520technique%2520based%2520on%2520conformal%2520prediction.%2520Crucially%252C%2520this%250Aprocedure%2520guarantees%2520robustness%2520to%2520distribution%2520shifts%2520in%2520states%2520when%250Aperception%2520outputs%2520are%2520used%2520in%2520conjunction%2520with%2520a%2520planner.%2520As%2520a%2520result%252C%2520the%250Acalibrated%2520perception%2520system%2520can%2520be%2520used%2520in%2520combination%2520with%2520any%2520safe%2520planner%250Ato%2520provide%2520an%2520end-to-end%2520statistical%2520assurance%2520on%2520safety%2520in%2520unseen%250Aenvironments.%2520We%2520evaluate%2520the%2520resulting%2520approach%252C%2520Perceive%2520with%2520Confidence%250A%2528PwC%2529%252C%2520in%2520simulation%2520and%2520on%2520hardware%2520where%2520a%2520quadruped%2520robot%2520navigates%2520through%250Apreviously%2520unseen%2520indoor%252C%2520static%2520environments.%2520These%2520experiments%2520validate%2520the%250Asafety%2520assurances%2520for%2520obstacle%2520avoidance%2520provided%2520by%2520PwC.%2520In%2520simulation%252C%2520our%250Amethod%2520reduces%2520obstacle%2520misdetection%2520by%2520%252470%255C%2525%2524%2520compared%2520to%2520uncalibrated%250Aperception%2520models.%2520While%2520misdetections%2520lead%2520to%2520collisions%2520for%2520baseline%2520methods%252C%250Aour%2520approach%2520consistently%2520achieves%2520%2524100%255C%2525%2524%2520safety.%2520We%2520further%2520demonstrate%250Areducing%2520the%2520conservatism%2520of%2520our%2520method%2520without%2520sacrificing%2520safety%252C%2520achieving%2520a%250A%252446%255C%2525%2524%2520increase%2520in%2520success%2520rates%2520in%2520challenging%2520environments%2520while%2520maintaining%250A%2524100%255C%2525%2524%2520safety.%2520In%2520hardware%2520experiments%252C%2520our%2520method%2520improves%2520empirical%2520safety%250Aby%2520%252440%255C%2525%2524%2520over%2520baselines%2520and%2520reduces%2520obstacle%2520misdetection%2520by%2520%252493.3%255C%2525%2524.%2520The%250Asafety%2520gap%2520widens%2520to%2520%252446.7%255C%2525%2524%2520when%2520navigation%2520speed%2520increases%252C%2520highlighting%2520our%250Aapproach%2527s%2520robustness%2520under%2520more%2520demanding%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.08185v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perceive%20With%20Confidence%3A%20Statistical%20Safety%20Assurances%20for%20Navigation%0A%20%20with%20Learning-Based%20Perception&entry.906535625=Zhiting%20Mei%20and%20Anushri%20Dixit%20and%20Meghan%20Booker%20and%20Emily%20Zhou%20and%20Mariko%20Storey-Matsutani%20and%20Allen%20Z.%20Ren%20and%20Ola%20Shorinwa%20and%20Anirudha%20Majumdar&entry.1292438233=%20%20Rapid%20advances%20in%20perception%20have%20enabled%20large%20pre-trained%20models%20to%20be%20used%0Aout%20of%20the%20box%20for%20transforming%20high-dimensional%2C%20noisy%2C%20and%20partial%0Aobservations%20of%20the%20world%20into%20rich%20occupancy%20representations.%20However%2C%20the%0Areliability%20of%20these%20models%20and%20consequently%20their%20safe%20integration%20onto%20robots%0Aremains%20unknown%20when%20deployed%20in%20environments%20unseen%20during%20training.%20To%0Aprovide%20safety%20guarantees%2C%20we%20rigorously%20quantify%20the%20uncertainty%20of%0Apre-trained%20perception%20systems%20for%20object%20detection%20and%20scene%20completion%20via%20a%0Anovel%20calibration%20technique%20based%20on%20conformal%20prediction.%20Crucially%2C%20this%0Aprocedure%20guarantees%20robustness%20to%20distribution%20shifts%20in%20states%20when%0Aperception%20outputs%20are%20used%20in%20conjunction%20with%20a%20planner.%20As%20a%20result%2C%20the%0Acalibrated%20perception%20system%20can%20be%20used%20in%20combination%20with%20any%20safe%20planner%0Ato%20provide%20an%20end-to-end%20statistical%20assurance%20on%20safety%20in%20unseen%0Aenvironments.%20We%20evaluate%20the%20resulting%20approach%2C%20Perceive%20with%20Confidence%0A%28PwC%29%2C%20in%20simulation%20and%20on%20hardware%20where%20a%20quadruped%20robot%20navigates%20through%0Apreviously%20unseen%20indoor%2C%20static%20environments.%20These%20experiments%20validate%20the%0Asafety%20assurances%20for%20obstacle%20avoidance%20provided%20by%20PwC.%20In%20simulation%2C%20our%0Amethod%20reduces%20obstacle%20misdetection%20by%20%2470%5C%25%24%20compared%20to%20uncalibrated%0Aperception%20models.%20While%20misdetections%20lead%20to%20collisions%20for%20baseline%20methods%2C%0Aour%20approach%20consistently%20achieves%20%24100%5C%25%24%20safety.%20We%20further%20demonstrate%0Areducing%20the%20conservatism%20of%20our%20method%20without%20sacrificing%20safety%2C%20achieving%20a%0A%2446%5C%25%24%20increase%20in%20success%20rates%20in%20challenging%20environments%20while%20maintaining%0A%24100%5C%25%24%20safety.%20In%20hardware%20experiments%2C%20our%20method%20improves%20empirical%20safety%0Aby%20%2440%5C%25%24%20over%20baselines%20and%20reduces%20obstacle%20misdetection%20by%20%2493.3%5C%25%24.%20The%0Asafety%20gap%20widens%20to%20%2446.7%5C%25%24%20when%20navigation%20speed%20increases%2C%20highlighting%20our%0Aapproach%27s%20robustness%20under%20more%20demanding%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08185v3&entry.124074799=Read"},
{"title": "ChatEXAONEPath: An Expert-level Multimodal Large Language Model for\n  Histopathology Using Whole Slide Images", "author": "Sangwook Kim and Soonyoung Lee and Jongseong Jang", "abstract": "  Recent studies have made significant progress in developing large language\nmodels (LLMs) in the medical domain, which can answer expert-level questions\nand demonstrate the potential to assist clinicians in real-world clinical\nscenarios. Studies have also witnessed the importance of integrating various\nmodalities with the existing LLMs for a better understanding of complex\nclinical contexts, which are innately multi-faceted by nature. Although studies\nhave demonstrated the ability of multimodal LLMs in histopathology to answer\nquestions from given images, they lack in understanding of thorough clinical\ncontext due to the patch-level data with limited information from public\ndatasets. Thus, developing WSI-level MLLMs is significant in terms of the\nscalability and applicability of MLLMs in histopathology. In this study, we\nintroduce an expert-level MLLM for histopathology using WSIs, dubbed as\nChatEXAONEPath. We present a retrieval-based data generation pipeline using\n10,094 pairs of WSIs and histopathology reports from The Cancer Genome Atlas\n(TCGA). We also showcase an AI-based evaluation protocol for a comprehensive\nunderstanding of the medical context from given multimodal information and\nevaluate generated answers compared to the original histopathology reports. We\ndemonstrate the ability of diagnosing the given histopathology images using\nChatEXAONEPath with the acceptance rate of 62.9% from 1,134 pairs of WSIs and\nreports. Our proposed model can understand pan-cancer WSIs and clinical context\nfrom various cancer types. We argue that our proposed model has the potential\nto assist clinicians by comprehensively understanding complex morphology of\nWSIs for cancer diagnosis through the integration of multiple modalities.\n", "link": "http://arxiv.org/abs/2504.13023v1", "date": "2025-04-17", "relevancy": 2.5295, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5238}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4969}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChatEXAONEPath%3A%20An%20Expert-level%20Multimodal%20Large%20Language%20Model%20for%0A%20%20Histopathology%20Using%20Whole%20Slide%20Images&body=Title%3A%20ChatEXAONEPath%3A%20An%20Expert-level%20Multimodal%20Large%20Language%20Model%20for%0A%20%20Histopathology%20Using%20Whole%20Slide%20Images%0AAuthor%3A%20Sangwook%20Kim%20and%20Soonyoung%20Lee%20and%20Jongseong%20Jang%0AAbstract%3A%20%20%20Recent%20studies%20have%20made%20significant%20progress%20in%20developing%20large%20language%0Amodels%20%28LLMs%29%20in%20the%20medical%20domain%2C%20which%20can%20answer%20expert-level%20questions%0Aand%20demonstrate%20the%20potential%20to%20assist%20clinicians%20in%20real-world%20clinical%0Ascenarios.%20Studies%20have%20also%20witnessed%20the%20importance%20of%20integrating%20various%0Amodalities%20with%20the%20existing%20LLMs%20for%20a%20better%20understanding%20of%20complex%0Aclinical%20contexts%2C%20which%20are%20innately%20multi-faceted%20by%20nature.%20Although%20studies%0Ahave%20demonstrated%20the%20ability%20of%20multimodal%20LLMs%20in%20histopathology%20to%20answer%0Aquestions%20from%20given%20images%2C%20they%20lack%20in%20understanding%20of%20thorough%20clinical%0Acontext%20due%20to%20the%20patch-level%20data%20with%20limited%20information%20from%20public%0Adatasets.%20Thus%2C%20developing%20WSI-level%20MLLMs%20is%20significant%20in%20terms%20of%20the%0Ascalability%20and%20applicability%20of%20MLLMs%20in%20histopathology.%20In%20this%20study%2C%20we%0Aintroduce%20an%20expert-level%20MLLM%20for%20histopathology%20using%20WSIs%2C%20dubbed%20as%0AChatEXAONEPath.%20We%20present%20a%20retrieval-based%20data%20generation%20pipeline%20using%0A10%2C094%20pairs%20of%20WSIs%20and%20histopathology%20reports%20from%20The%20Cancer%20Genome%20Atlas%0A%28TCGA%29.%20We%20also%20showcase%20an%20AI-based%20evaluation%20protocol%20for%20a%20comprehensive%0Aunderstanding%20of%20the%20medical%20context%20from%20given%20multimodal%20information%20and%0Aevaluate%20generated%20answers%20compared%20to%20the%20original%20histopathology%20reports.%20We%0Ademonstrate%20the%20ability%20of%20diagnosing%20the%20given%20histopathology%20images%20using%0AChatEXAONEPath%20with%20the%20acceptance%20rate%20of%2062.9%25%20from%201%2C134%20pairs%20of%20WSIs%20and%0Areports.%20Our%20proposed%20model%20can%20understand%20pan-cancer%20WSIs%20and%20clinical%20context%0Afrom%20various%20cancer%20types.%20We%20argue%20that%20our%20proposed%20model%20has%20the%20potential%0Ato%20assist%20clinicians%20by%20comprehensively%20understanding%20complex%20morphology%20of%0AWSIs%20for%20cancer%20diagnosis%20through%20the%20integration%20of%20multiple%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13023v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChatEXAONEPath%253A%2520An%2520Expert-level%2520Multimodal%2520Large%2520Language%2520Model%2520for%250A%2520%2520Histopathology%2520Using%2520Whole%2520Slide%2520Images%26entry.906535625%3DSangwook%2520Kim%2520and%2520Soonyoung%2520Lee%2520and%2520Jongseong%2520Jang%26entry.1292438233%3D%2520%2520Recent%2520studies%2520have%2520made%2520significant%2520progress%2520in%2520developing%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520in%2520the%2520medical%2520domain%252C%2520which%2520can%2520answer%2520expert-level%2520questions%250Aand%2520demonstrate%2520the%2520potential%2520to%2520assist%2520clinicians%2520in%2520real-world%2520clinical%250Ascenarios.%2520Studies%2520have%2520also%2520witnessed%2520the%2520importance%2520of%2520integrating%2520various%250Amodalities%2520with%2520the%2520existing%2520LLMs%2520for%2520a%2520better%2520understanding%2520of%2520complex%250Aclinical%2520contexts%252C%2520which%2520are%2520innately%2520multi-faceted%2520by%2520nature.%2520Although%2520studies%250Ahave%2520demonstrated%2520the%2520ability%2520of%2520multimodal%2520LLMs%2520in%2520histopathology%2520to%2520answer%250Aquestions%2520from%2520given%2520images%252C%2520they%2520lack%2520in%2520understanding%2520of%2520thorough%2520clinical%250Acontext%2520due%2520to%2520the%2520patch-level%2520data%2520with%2520limited%2520information%2520from%2520public%250Adatasets.%2520Thus%252C%2520developing%2520WSI-level%2520MLLMs%2520is%2520significant%2520in%2520terms%2520of%2520the%250Ascalability%2520and%2520applicability%2520of%2520MLLMs%2520in%2520histopathology.%2520In%2520this%2520study%252C%2520we%250Aintroduce%2520an%2520expert-level%2520MLLM%2520for%2520histopathology%2520using%2520WSIs%252C%2520dubbed%2520as%250AChatEXAONEPath.%2520We%2520present%2520a%2520retrieval-based%2520data%2520generation%2520pipeline%2520using%250A10%252C094%2520pairs%2520of%2520WSIs%2520and%2520histopathology%2520reports%2520from%2520The%2520Cancer%2520Genome%2520Atlas%250A%2528TCGA%2529.%2520We%2520also%2520showcase%2520an%2520AI-based%2520evaluation%2520protocol%2520for%2520a%2520comprehensive%250Aunderstanding%2520of%2520the%2520medical%2520context%2520from%2520given%2520multimodal%2520information%2520and%250Aevaluate%2520generated%2520answers%2520compared%2520to%2520the%2520original%2520histopathology%2520reports.%2520We%250Ademonstrate%2520the%2520ability%2520of%2520diagnosing%2520the%2520given%2520histopathology%2520images%2520using%250AChatEXAONEPath%2520with%2520the%2520acceptance%2520rate%2520of%252062.9%2525%2520from%25201%252C134%2520pairs%2520of%2520WSIs%2520and%250Areports.%2520Our%2520proposed%2520model%2520can%2520understand%2520pan-cancer%2520WSIs%2520and%2520clinical%2520context%250Afrom%2520various%2520cancer%2520types.%2520We%2520argue%2520that%2520our%2520proposed%2520model%2520has%2520the%2520potential%250Ato%2520assist%2520clinicians%2520by%2520comprehensively%2520understanding%2520complex%2520morphology%2520of%250AWSIs%2520for%2520cancer%2520diagnosis%2520through%2520the%2520integration%2520of%2520multiple%2520modalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13023v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChatEXAONEPath%3A%20An%20Expert-level%20Multimodal%20Large%20Language%20Model%20for%0A%20%20Histopathology%20Using%20Whole%20Slide%20Images&entry.906535625=Sangwook%20Kim%20and%20Soonyoung%20Lee%20and%20Jongseong%20Jang&entry.1292438233=%20%20Recent%20studies%20have%20made%20significant%20progress%20in%20developing%20large%20language%0Amodels%20%28LLMs%29%20in%20the%20medical%20domain%2C%20which%20can%20answer%20expert-level%20questions%0Aand%20demonstrate%20the%20potential%20to%20assist%20clinicians%20in%20real-world%20clinical%0Ascenarios.%20Studies%20have%20also%20witnessed%20the%20importance%20of%20integrating%20various%0Amodalities%20with%20the%20existing%20LLMs%20for%20a%20better%20understanding%20of%20complex%0Aclinical%20contexts%2C%20which%20are%20innately%20multi-faceted%20by%20nature.%20Although%20studies%0Ahave%20demonstrated%20the%20ability%20of%20multimodal%20LLMs%20in%20histopathology%20to%20answer%0Aquestions%20from%20given%20images%2C%20they%20lack%20in%20understanding%20of%20thorough%20clinical%0Acontext%20due%20to%20the%20patch-level%20data%20with%20limited%20information%20from%20public%0Adatasets.%20Thus%2C%20developing%20WSI-level%20MLLMs%20is%20significant%20in%20terms%20of%20the%0Ascalability%20and%20applicability%20of%20MLLMs%20in%20histopathology.%20In%20this%20study%2C%20we%0Aintroduce%20an%20expert-level%20MLLM%20for%20histopathology%20using%20WSIs%2C%20dubbed%20as%0AChatEXAONEPath.%20We%20present%20a%20retrieval-based%20data%20generation%20pipeline%20using%0A10%2C094%20pairs%20of%20WSIs%20and%20histopathology%20reports%20from%20The%20Cancer%20Genome%20Atlas%0A%28TCGA%29.%20We%20also%20showcase%20an%20AI-based%20evaluation%20protocol%20for%20a%20comprehensive%0Aunderstanding%20of%20the%20medical%20context%20from%20given%20multimodal%20information%20and%0Aevaluate%20generated%20answers%20compared%20to%20the%20original%20histopathology%20reports.%20We%0Ademonstrate%20the%20ability%20of%20diagnosing%20the%20given%20histopathology%20images%20using%0AChatEXAONEPath%20with%20the%20acceptance%20rate%20of%2062.9%25%20from%201%2C134%20pairs%20of%20WSIs%20and%0Areports.%20Our%20proposed%20model%20can%20understand%20pan-cancer%20WSIs%20and%20clinical%20context%0Afrom%20various%20cancer%20types.%20We%20argue%20that%20our%20proposed%20model%20has%20the%20potential%0Ato%20assist%20clinicians%20by%20comprehensively%20understanding%20complex%20morphology%20of%0AWSIs%20for%20cancer%20diagnosis%20through%20the%20integration%20of%20multiple%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13023v1&entry.124074799=Read"},
{"title": "On Linear Representations and Pretraining Data Frequency in Language\n  Models", "author": "Jack Merullo and Noah A. Smith and Sarah Wiegreffe and Yanai Elazar", "abstract": "  Pretraining data has a direct impact on the behaviors and quality of language\nmodels (LMs), but we only understand the most basic principles of this\nrelationship. While most work focuses on pretraining data's effect on\ndownstream task behavior, we investigate its relationship to LM\nrepresentations. Previous work has discovered that, in language models, some\nconcepts are encoded `linearly' in the representations, but what factors cause\nthese representations to form? We study the connection between pretraining data\nfrequency and models' linear representations of factual relations. We find\nevidence that the formation of linear representations is strongly connected to\npretraining term frequencies; specifically for subject-relation-object fact\ntriplets, both subject-object co-occurrence frequency and in-context learning\naccuracy for the relation are highly correlated with linear representations.\nThis is the case across all phases of pretraining. In OLMo-7B and GPT-J, we\ndiscover that a linear representation consistently (but not exclusively) forms\nwhen the subjects and objects within a relation co-occur at least 1k and 2k\ntimes, respectively, regardless of when these occurrences happen during\npretraining. Finally, we train a regression model on measurements of linear\nrepresentation quality in fully-trained LMs that can predict how often a term\nwas seen in pretraining. Our model achieves low error even on inputs from a\ndifferent model with a different pretraining dataset, providing a new method\nfor estimating properties of the otherwise-unknown training data of closed-data\nmodels. We conclude that the strength of linear representations in LMs contains\nsignal about the models' pretraining corpora that may provide new avenues for\ncontrolling and improving model behavior: particularly, manipulating the\nmodels' training data to meet specific frequency thresholds.\n", "link": "http://arxiv.org/abs/2504.12459v1", "date": "2025-04-16", "relevancy": 2.5279, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5151}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5151}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Linear%20Representations%20and%20Pretraining%20Data%20Frequency%20in%20Language%0A%20%20Models&body=Title%3A%20On%20Linear%20Representations%20and%20Pretraining%20Data%20Frequency%20in%20Language%0A%20%20Models%0AAuthor%3A%20Jack%20Merullo%20and%20Noah%20A.%20Smith%20and%20Sarah%20Wiegreffe%20and%20Yanai%20Elazar%0AAbstract%3A%20%20%20Pretraining%20data%20has%20a%20direct%20impact%20on%20the%20behaviors%20and%20quality%20of%20language%0Amodels%20%28LMs%29%2C%20but%20we%20only%20understand%20the%20most%20basic%20principles%20of%20this%0Arelationship.%20While%20most%20work%20focuses%20on%20pretraining%20data%27s%20effect%20on%0Adownstream%20task%20behavior%2C%20we%20investigate%20its%20relationship%20to%20LM%0Arepresentations.%20Previous%20work%20has%20discovered%20that%2C%20in%20language%20models%2C%20some%0Aconcepts%20are%20encoded%20%60linearly%27%20in%20the%20representations%2C%20but%20what%20factors%20cause%0Athese%20representations%20to%20form%3F%20We%20study%20the%20connection%20between%20pretraining%20data%0Afrequency%20and%20models%27%20linear%20representations%20of%20factual%20relations.%20We%20find%0Aevidence%20that%20the%20formation%20of%20linear%20representations%20is%20strongly%20connected%20to%0Apretraining%20term%20frequencies%3B%20specifically%20for%20subject-relation-object%20fact%0Atriplets%2C%20both%20subject-object%20co-occurrence%20frequency%20and%20in-context%20learning%0Aaccuracy%20for%20the%20relation%20are%20highly%20correlated%20with%20linear%20representations.%0AThis%20is%20the%20case%20across%20all%20phases%20of%20pretraining.%20In%20OLMo-7B%20and%20GPT-J%2C%20we%0Adiscover%20that%20a%20linear%20representation%20consistently%20%28but%20not%20exclusively%29%20forms%0Awhen%20the%20subjects%20and%20objects%20within%20a%20relation%20co-occur%20at%20least%201k%20and%202k%0Atimes%2C%20respectively%2C%20regardless%20of%20when%20these%20occurrences%20happen%20during%0Apretraining.%20Finally%2C%20we%20train%20a%20regression%20model%20on%20measurements%20of%20linear%0Arepresentation%20quality%20in%20fully-trained%20LMs%20that%20can%20predict%20how%20often%20a%20term%0Awas%20seen%20in%20pretraining.%20Our%20model%20achieves%20low%20error%20even%20on%20inputs%20from%20a%0Adifferent%20model%20with%20a%20different%20pretraining%20dataset%2C%20providing%20a%20new%20method%0Afor%20estimating%20properties%20of%20the%20otherwise-unknown%20training%20data%20of%20closed-data%0Amodels.%20We%20conclude%20that%20the%20strength%20of%20linear%20representations%20in%20LMs%20contains%0Asignal%20about%20the%20models%27%20pretraining%20corpora%20that%20may%20provide%20new%20avenues%20for%0Acontrolling%20and%20improving%20model%20behavior%3A%20particularly%2C%20manipulating%20the%0Amodels%27%20training%20data%20to%20meet%20specific%20frequency%20thresholds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12459v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Linear%2520Representations%2520and%2520Pretraining%2520Data%2520Frequency%2520in%2520Language%250A%2520%2520Models%26entry.906535625%3DJack%2520Merullo%2520and%2520Noah%2520A.%2520Smith%2520and%2520Sarah%2520Wiegreffe%2520and%2520Yanai%2520Elazar%26entry.1292438233%3D%2520%2520Pretraining%2520data%2520has%2520a%2520direct%2520impact%2520on%2520the%2520behaviors%2520and%2520quality%2520of%2520language%250Amodels%2520%2528LMs%2529%252C%2520but%2520we%2520only%2520understand%2520the%2520most%2520basic%2520principles%2520of%2520this%250Arelationship.%2520While%2520most%2520work%2520focuses%2520on%2520pretraining%2520data%2527s%2520effect%2520on%250Adownstream%2520task%2520behavior%252C%2520we%2520investigate%2520its%2520relationship%2520to%2520LM%250Arepresentations.%2520Previous%2520work%2520has%2520discovered%2520that%252C%2520in%2520language%2520models%252C%2520some%250Aconcepts%2520are%2520encoded%2520%2560linearly%2527%2520in%2520the%2520representations%252C%2520but%2520what%2520factors%2520cause%250Athese%2520representations%2520to%2520form%253F%2520We%2520study%2520the%2520connection%2520between%2520pretraining%2520data%250Afrequency%2520and%2520models%2527%2520linear%2520representations%2520of%2520factual%2520relations.%2520We%2520find%250Aevidence%2520that%2520the%2520formation%2520of%2520linear%2520representations%2520is%2520strongly%2520connected%2520to%250Apretraining%2520term%2520frequencies%253B%2520specifically%2520for%2520subject-relation-object%2520fact%250Atriplets%252C%2520both%2520subject-object%2520co-occurrence%2520frequency%2520and%2520in-context%2520learning%250Aaccuracy%2520for%2520the%2520relation%2520are%2520highly%2520correlated%2520with%2520linear%2520representations.%250AThis%2520is%2520the%2520case%2520across%2520all%2520phases%2520of%2520pretraining.%2520In%2520OLMo-7B%2520and%2520GPT-J%252C%2520we%250Adiscover%2520that%2520a%2520linear%2520representation%2520consistently%2520%2528but%2520not%2520exclusively%2529%2520forms%250Awhen%2520the%2520subjects%2520and%2520objects%2520within%2520a%2520relation%2520co-occur%2520at%2520least%25201k%2520and%25202k%250Atimes%252C%2520respectively%252C%2520regardless%2520of%2520when%2520these%2520occurrences%2520happen%2520during%250Apretraining.%2520Finally%252C%2520we%2520train%2520a%2520regression%2520model%2520on%2520measurements%2520of%2520linear%250Arepresentation%2520quality%2520in%2520fully-trained%2520LMs%2520that%2520can%2520predict%2520how%2520often%2520a%2520term%250Awas%2520seen%2520in%2520pretraining.%2520Our%2520model%2520achieves%2520low%2520error%2520even%2520on%2520inputs%2520from%2520a%250Adifferent%2520model%2520with%2520a%2520different%2520pretraining%2520dataset%252C%2520providing%2520a%2520new%2520method%250Afor%2520estimating%2520properties%2520of%2520the%2520otherwise-unknown%2520training%2520data%2520of%2520closed-data%250Amodels.%2520We%2520conclude%2520that%2520the%2520strength%2520of%2520linear%2520representations%2520in%2520LMs%2520contains%250Asignal%2520about%2520the%2520models%2527%2520pretraining%2520corpora%2520that%2520may%2520provide%2520new%2520avenues%2520for%250Acontrolling%2520and%2520improving%2520model%2520behavior%253A%2520particularly%252C%2520manipulating%2520the%250Amodels%2527%2520training%2520data%2520to%2520meet%2520specific%2520frequency%2520thresholds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12459v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Linear%20Representations%20and%20Pretraining%20Data%20Frequency%20in%20Language%0A%20%20Models&entry.906535625=Jack%20Merullo%20and%20Noah%20A.%20Smith%20and%20Sarah%20Wiegreffe%20and%20Yanai%20Elazar&entry.1292438233=%20%20Pretraining%20data%20has%20a%20direct%20impact%20on%20the%20behaviors%20and%20quality%20of%20language%0Amodels%20%28LMs%29%2C%20but%20we%20only%20understand%20the%20most%20basic%20principles%20of%20this%0Arelationship.%20While%20most%20work%20focuses%20on%20pretraining%20data%27s%20effect%20on%0Adownstream%20task%20behavior%2C%20we%20investigate%20its%20relationship%20to%20LM%0Arepresentations.%20Previous%20work%20has%20discovered%20that%2C%20in%20language%20models%2C%20some%0Aconcepts%20are%20encoded%20%60linearly%27%20in%20the%20representations%2C%20but%20what%20factors%20cause%0Athese%20representations%20to%20form%3F%20We%20study%20the%20connection%20between%20pretraining%20data%0Afrequency%20and%20models%27%20linear%20representations%20of%20factual%20relations.%20We%20find%0Aevidence%20that%20the%20formation%20of%20linear%20representations%20is%20strongly%20connected%20to%0Apretraining%20term%20frequencies%3B%20specifically%20for%20subject-relation-object%20fact%0Atriplets%2C%20both%20subject-object%20co-occurrence%20frequency%20and%20in-context%20learning%0Aaccuracy%20for%20the%20relation%20are%20highly%20correlated%20with%20linear%20representations.%0AThis%20is%20the%20case%20across%20all%20phases%20of%20pretraining.%20In%20OLMo-7B%20and%20GPT-J%2C%20we%0Adiscover%20that%20a%20linear%20representation%20consistently%20%28but%20not%20exclusively%29%20forms%0Awhen%20the%20subjects%20and%20objects%20within%20a%20relation%20co-occur%20at%20least%201k%20and%202k%0Atimes%2C%20respectively%2C%20regardless%20of%20when%20these%20occurrences%20happen%20during%0Apretraining.%20Finally%2C%20we%20train%20a%20regression%20model%20on%20measurements%20of%20linear%0Arepresentation%20quality%20in%20fully-trained%20LMs%20that%20can%20predict%20how%20often%20a%20term%0Awas%20seen%20in%20pretraining.%20Our%20model%20achieves%20low%20error%20even%20on%20inputs%20from%20a%0Adifferent%20model%20with%20a%20different%20pretraining%20dataset%2C%20providing%20a%20new%20method%0Afor%20estimating%20properties%20of%20the%20otherwise-unknown%20training%20data%20of%20closed-data%0Amodels.%20We%20conclude%20that%20the%20strength%20of%20linear%20representations%20in%20LMs%20contains%0Asignal%20about%20the%20models%27%20pretraining%20corpora%20that%20may%20provide%20new%20avenues%20for%0Acontrolling%20and%20improving%20model%20behavior%3A%20particularly%2C%20manipulating%20the%0Amodels%27%20training%20data%20to%20meet%20specific%20frequency%20thresholds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12459v1&entry.124074799=Read"},
{"title": "Decentralized Nonconvex Composite Federated Learning with Gradient\n  Tracking and Momentum", "author": "Yuan Zhou and Xinli Shi and Xuelong Li and Jiachen Zhong and Guanghui Wen and Jinde Cao", "abstract": "  Decentralized Federated Learning (DFL) eliminates the reliance on the\nserver-client architecture inherent in traditional federated learning,\nattracting significant research interest in recent years. Simultaneously, the\nobjective functions in machine learning tasks are often nonconvex and\nfrequently incorporate additional, potentially nonsmooth regularization terms\nto satisfy practical requirements, thereby forming nonconvex composite\noptimization problems. Employing DFL methods to solve such general optimization\nproblems leads to the formulation of Decentralized Nonconvex Composite\nFederated Learning (DNCFL), a topic that remains largely underexplored. In this\npaper, we propose a novel DNCFL algorithm, termed \\bf{DEPOSITUM}. Built upon\nproximal stochastic gradient tracking, DEPOSITUM mitigates the impact of data\nheterogeneity by enabling clients to approximate the global gradient. The\nintroduction of momentums in the proximal gradient descent step, replacing\ntracking variables, reduces the variance introduced by stochastic gradients.\nAdditionally, DEPOSITUM supports local updates of client variables,\nsignificantly reducing communication costs. Theoretical analysis demonstrates\nthat DEPOSITUM achieves an expected $\\epsilon$-stationary point with an\niteration complexity of $\\mathcal{O}(1/\\epsilon^2)$. The proximal gradient,\nconsensus errors, and gradient estimation errors decrease at a sublinear rate\nof $\\mathcal{O}(1/T)$. With appropriate parameter selection, the algorithm\nachieves network-independent linear speedup without requiring mega-batch\nsampling. Finally, we apply DEPOSITUM to the training of neural networks on\nreal-world datasets, systematically examining the influence of various\nhyperparameters on its performance. Comparisons with other federated composite\noptimization algorithms validate the effectiveness of the proposed method.\n", "link": "http://arxiv.org/abs/2504.12742v1", "date": "2025-04-17", "relevancy": 2.5208, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5227}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5045}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decentralized%20Nonconvex%20Composite%20Federated%20Learning%20with%20Gradient%0A%20%20Tracking%20and%20Momentum&body=Title%3A%20Decentralized%20Nonconvex%20Composite%20Federated%20Learning%20with%20Gradient%0A%20%20Tracking%20and%20Momentum%0AAuthor%3A%20Yuan%20Zhou%20and%20Xinli%20Shi%20and%20Xuelong%20Li%20and%20Jiachen%20Zhong%20and%20Guanghui%20Wen%20and%20Jinde%20Cao%0AAbstract%3A%20%20%20Decentralized%20Federated%20Learning%20%28DFL%29%20eliminates%20the%20reliance%20on%20the%0Aserver-client%20architecture%20inherent%20in%20traditional%20federated%20learning%2C%0Aattracting%20significant%20research%20interest%20in%20recent%20years.%20Simultaneously%2C%20the%0Aobjective%20functions%20in%20machine%20learning%20tasks%20are%20often%20nonconvex%20and%0Afrequently%20incorporate%20additional%2C%20potentially%20nonsmooth%20regularization%20terms%0Ato%20satisfy%20practical%20requirements%2C%20thereby%20forming%20nonconvex%20composite%0Aoptimization%20problems.%20Employing%20DFL%20methods%20to%20solve%20such%20general%20optimization%0Aproblems%20leads%20to%20the%20formulation%20of%20Decentralized%20Nonconvex%20Composite%0AFederated%20Learning%20%28DNCFL%29%2C%20a%20topic%20that%20remains%20largely%20underexplored.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20DNCFL%20algorithm%2C%20termed%20%5Cbf%7BDEPOSITUM%7D.%20Built%20upon%0Aproximal%20stochastic%20gradient%20tracking%2C%20DEPOSITUM%20mitigates%20the%20impact%20of%20data%0Aheterogeneity%20by%20enabling%20clients%20to%20approximate%20the%20global%20gradient.%20The%0Aintroduction%20of%20momentums%20in%20the%20proximal%20gradient%20descent%20step%2C%20replacing%0Atracking%20variables%2C%20reduces%20the%20variance%20introduced%20by%20stochastic%20gradients.%0AAdditionally%2C%20DEPOSITUM%20supports%20local%20updates%20of%20client%20variables%2C%0Asignificantly%20reducing%20communication%20costs.%20Theoretical%20analysis%20demonstrates%0Athat%20DEPOSITUM%20achieves%20an%20expected%20%24%5Cepsilon%24-stationary%20point%20with%20an%0Aiteration%20complexity%20of%20%24%5Cmathcal%7BO%7D%281/%5Cepsilon%5E2%29%24.%20The%20proximal%20gradient%2C%0Aconsensus%20errors%2C%20and%20gradient%20estimation%20errors%20decrease%20at%20a%20sublinear%20rate%0Aof%20%24%5Cmathcal%7BO%7D%281/T%29%24.%20With%20appropriate%20parameter%20selection%2C%20the%20algorithm%0Aachieves%20network-independent%20linear%20speedup%20without%20requiring%20mega-batch%0Asampling.%20Finally%2C%20we%20apply%20DEPOSITUM%20to%20the%20training%20of%20neural%20networks%20on%0Areal-world%20datasets%2C%20systematically%20examining%20the%20influence%20of%20various%0Ahyperparameters%20on%20its%20performance.%20Comparisons%20with%20other%20federated%20composite%0Aoptimization%20algorithms%20validate%20the%20effectiveness%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12742v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecentralized%2520Nonconvex%2520Composite%2520Federated%2520Learning%2520with%2520Gradient%250A%2520%2520Tracking%2520and%2520Momentum%26entry.906535625%3DYuan%2520Zhou%2520and%2520Xinli%2520Shi%2520and%2520Xuelong%2520Li%2520and%2520Jiachen%2520Zhong%2520and%2520Guanghui%2520Wen%2520and%2520Jinde%2520Cao%26entry.1292438233%3D%2520%2520Decentralized%2520Federated%2520Learning%2520%2528DFL%2529%2520eliminates%2520the%2520reliance%2520on%2520the%250Aserver-client%2520architecture%2520inherent%2520in%2520traditional%2520federated%2520learning%252C%250Aattracting%2520significant%2520research%2520interest%2520in%2520recent%2520years.%2520Simultaneously%252C%2520the%250Aobjective%2520functions%2520in%2520machine%2520learning%2520tasks%2520are%2520often%2520nonconvex%2520and%250Afrequently%2520incorporate%2520additional%252C%2520potentially%2520nonsmooth%2520regularization%2520terms%250Ato%2520satisfy%2520practical%2520requirements%252C%2520thereby%2520forming%2520nonconvex%2520composite%250Aoptimization%2520problems.%2520Employing%2520DFL%2520methods%2520to%2520solve%2520such%2520general%2520optimization%250Aproblems%2520leads%2520to%2520the%2520formulation%2520of%2520Decentralized%2520Nonconvex%2520Composite%250AFederated%2520Learning%2520%2528DNCFL%2529%252C%2520a%2520topic%2520that%2520remains%2520largely%2520underexplored.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520DNCFL%2520algorithm%252C%2520termed%2520%255Cbf%257BDEPOSITUM%257D.%2520Built%2520upon%250Aproximal%2520stochastic%2520gradient%2520tracking%252C%2520DEPOSITUM%2520mitigates%2520the%2520impact%2520of%2520data%250Aheterogeneity%2520by%2520enabling%2520clients%2520to%2520approximate%2520the%2520global%2520gradient.%2520The%250Aintroduction%2520of%2520momentums%2520in%2520the%2520proximal%2520gradient%2520descent%2520step%252C%2520replacing%250Atracking%2520variables%252C%2520reduces%2520the%2520variance%2520introduced%2520by%2520stochastic%2520gradients.%250AAdditionally%252C%2520DEPOSITUM%2520supports%2520local%2520updates%2520of%2520client%2520variables%252C%250Asignificantly%2520reducing%2520communication%2520costs.%2520Theoretical%2520analysis%2520demonstrates%250Athat%2520DEPOSITUM%2520achieves%2520an%2520expected%2520%2524%255Cepsilon%2524-stationary%2520point%2520with%2520an%250Aiteration%2520complexity%2520of%2520%2524%255Cmathcal%257BO%257D%25281/%255Cepsilon%255E2%2529%2524.%2520The%2520proximal%2520gradient%252C%250Aconsensus%2520errors%252C%2520and%2520gradient%2520estimation%2520errors%2520decrease%2520at%2520a%2520sublinear%2520rate%250Aof%2520%2524%255Cmathcal%257BO%257D%25281/T%2529%2524.%2520With%2520appropriate%2520parameter%2520selection%252C%2520the%2520algorithm%250Aachieves%2520network-independent%2520linear%2520speedup%2520without%2520requiring%2520mega-batch%250Asampling.%2520Finally%252C%2520we%2520apply%2520DEPOSITUM%2520to%2520the%2520training%2520of%2520neural%2520networks%2520on%250Areal-world%2520datasets%252C%2520systematically%2520examining%2520the%2520influence%2520of%2520various%250Ahyperparameters%2520on%2520its%2520performance.%2520Comparisons%2520with%2520other%2520federated%2520composite%250Aoptimization%2520algorithms%2520validate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12742v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decentralized%20Nonconvex%20Composite%20Federated%20Learning%20with%20Gradient%0A%20%20Tracking%20and%20Momentum&entry.906535625=Yuan%20Zhou%20and%20Xinli%20Shi%20and%20Xuelong%20Li%20and%20Jiachen%20Zhong%20and%20Guanghui%20Wen%20and%20Jinde%20Cao&entry.1292438233=%20%20Decentralized%20Federated%20Learning%20%28DFL%29%20eliminates%20the%20reliance%20on%20the%0Aserver-client%20architecture%20inherent%20in%20traditional%20federated%20learning%2C%0Aattracting%20significant%20research%20interest%20in%20recent%20years.%20Simultaneously%2C%20the%0Aobjective%20functions%20in%20machine%20learning%20tasks%20are%20often%20nonconvex%20and%0Afrequently%20incorporate%20additional%2C%20potentially%20nonsmooth%20regularization%20terms%0Ato%20satisfy%20practical%20requirements%2C%20thereby%20forming%20nonconvex%20composite%0Aoptimization%20problems.%20Employing%20DFL%20methods%20to%20solve%20such%20general%20optimization%0Aproblems%20leads%20to%20the%20formulation%20of%20Decentralized%20Nonconvex%20Composite%0AFederated%20Learning%20%28DNCFL%29%2C%20a%20topic%20that%20remains%20largely%20underexplored.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20DNCFL%20algorithm%2C%20termed%20%5Cbf%7BDEPOSITUM%7D.%20Built%20upon%0Aproximal%20stochastic%20gradient%20tracking%2C%20DEPOSITUM%20mitigates%20the%20impact%20of%20data%0Aheterogeneity%20by%20enabling%20clients%20to%20approximate%20the%20global%20gradient.%20The%0Aintroduction%20of%20momentums%20in%20the%20proximal%20gradient%20descent%20step%2C%20replacing%0Atracking%20variables%2C%20reduces%20the%20variance%20introduced%20by%20stochastic%20gradients.%0AAdditionally%2C%20DEPOSITUM%20supports%20local%20updates%20of%20client%20variables%2C%0Asignificantly%20reducing%20communication%20costs.%20Theoretical%20analysis%20demonstrates%0Athat%20DEPOSITUM%20achieves%20an%20expected%20%24%5Cepsilon%24-stationary%20point%20with%20an%0Aiteration%20complexity%20of%20%24%5Cmathcal%7BO%7D%281/%5Cepsilon%5E2%29%24.%20The%20proximal%20gradient%2C%0Aconsensus%20errors%2C%20and%20gradient%20estimation%20errors%20decrease%20at%20a%20sublinear%20rate%0Aof%20%24%5Cmathcal%7BO%7D%281/T%29%24.%20With%20appropriate%20parameter%20selection%2C%20the%20algorithm%0Aachieves%20network-independent%20linear%20speedup%20without%20requiring%20mega-batch%0Asampling.%20Finally%2C%20we%20apply%20DEPOSITUM%20to%20the%20training%20of%20neural%20networks%20on%0Areal-world%20datasets%2C%20systematically%20examining%20the%20influence%20of%20various%0Ahyperparameters%20on%20its%20performance.%20Comparisons%20with%20other%20federated%20composite%0Aoptimization%20algorithms%20validate%20the%20effectiveness%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12742v1&entry.124074799=Read"},
{"title": "Understanding the Limits of Vision Language Models Through the Lens of\n  the Binding Problem", "author": "Declan Campbell and Sunayana Rane and Tyler Giallanza and Nicol\u00f2 De Sabbata and Kia Ghods and Amogh Joshi and Alexander Ku and Steven M. Frankland and Thomas L. Griffiths and Jonathan D. Cohen and Taylor W. Webb", "abstract": "  Recent work has documented striking heterogeneity in the performance of\nstate-of-the-art vision language models (VLMs), including both multimodal\nlanguage models and text-to-image models. These models are able to describe and\ngenerate a diverse array of complex, naturalistic images, yet they exhibit\nsurprising failures on basic multi-object reasoning tasks -- such as counting,\nlocalization, and simple forms of visual analogy -- that humans perform with\nnear perfect accuracy. To better understand this puzzling pattern of successes\nand failures, we turn to theoretical accounts of the binding problem in\ncognitive science and neuroscience, a fundamental problem that arises when a\nshared set of representational resources must be used to represent distinct\nentities (e.g., to represent multiple objects in an image), necessitating the\nuse of serial processing to avoid interference. We find that many of the\npuzzling failures of state-of-the-art VLMs can be explained as arising due to\nthe binding problem, and that these failure modes are strikingly similar to the\nlimitations exhibited by rapid, feedforward processing in the human brain.\n", "link": "http://arxiv.org/abs/2411.00238v2", "date": "2025-04-16", "relevancy": 2.5183, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6456}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6456}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20the%20Limits%20of%20Vision%20Language%20Models%20Through%20the%20Lens%20of%0A%20%20the%20Binding%20Problem&body=Title%3A%20Understanding%20the%20Limits%20of%20Vision%20Language%20Models%20Through%20the%20Lens%20of%0A%20%20the%20Binding%20Problem%0AAuthor%3A%20Declan%20Campbell%20and%20Sunayana%20Rane%20and%20Tyler%20Giallanza%20and%20Nicol%C3%B2%20De%20Sabbata%20and%20Kia%20Ghods%20and%20Amogh%20Joshi%20and%20Alexander%20Ku%20and%20Steven%20M.%20Frankland%20and%20Thomas%20L.%20Griffiths%20and%20Jonathan%20D.%20Cohen%20and%20Taylor%20W.%20Webb%0AAbstract%3A%20%20%20Recent%20work%20has%20documented%20striking%20heterogeneity%20in%20the%20performance%20of%0Astate-of-the-art%20vision%20language%20models%20%28VLMs%29%2C%20including%20both%20multimodal%0Alanguage%20models%20and%20text-to-image%20models.%20These%20models%20are%20able%20to%20describe%20and%0Agenerate%20a%20diverse%20array%20of%20complex%2C%20naturalistic%20images%2C%20yet%20they%20exhibit%0Asurprising%20failures%20on%20basic%20multi-object%20reasoning%20tasks%20--%20such%20as%20counting%2C%0Alocalization%2C%20and%20simple%20forms%20of%20visual%20analogy%20--%20that%20humans%20perform%20with%0Anear%20perfect%20accuracy.%20To%20better%20understand%20this%20puzzling%20pattern%20of%20successes%0Aand%20failures%2C%20we%20turn%20to%20theoretical%20accounts%20of%20the%20binding%20problem%20in%0Acognitive%20science%20and%20neuroscience%2C%20a%20fundamental%20problem%20that%20arises%20when%20a%0Ashared%20set%20of%20representational%20resources%20must%20be%20used%20to%20represent%20distinct%0Aentities%20%28e.g.%2C%20to%20represent%20multiple%20objects%20in%20an%20image%29%2C%20necessitating%20the%0Ause%20of%20serial%20processing%20to%20avoid%20interference.%20We%20find%20that%20many%20of%20the%0Apuzzling%20failures%20of%20state-of-the-art%20VLMs%20can%20be%20explained%20as%20arising%20due%20to%0Athe%20binding%20problem%2C%20and%20that%20these%20failure%20modes%20are%20strikingly%20similar%20to%20the%0Alimitations%20exhibited%20by%20rapid%2C%20feedforward%20processing%20in%20the%20human%20brain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.00238v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520the%2520Limits%2520of%2520Vision%2520Language%2520Models%2520Through%2520the%2520Lens%2520of%250A%2520%2520the%2520Binding%2520Problem%26entry.906535625%3DDeclan%2520Campbell%2520and%2520Sunayana%2520Rane%2520and%2520Tyler%2520Giallanza%2520and%2520Nicol%25C3%25B2%2520De%2520Sabbata%2520and%2520Kia%2520Ghods%2520and%2520Amogh%2520Joshi%2520and%2520Alexander%2520Ku%2520and%2520Steven%2520M.%2520Frankland%2520and%2520Thomas%2520L.%2520Griffiths%2520and%2520Jonathan%2520D.%2520Cohen%2520and%2520Taylor%2520W.%2520Webb%26entry.1292438233%3D%2520%2520Recent%2520work%2520has%2520documented%2520striking%2520heterogeneity%2520in%2520the%2520performance%2520of%250Astate-of-the-art%2520vision%2520language%2520models%2520%2528VLMs%2529%252C%2520including%2520both%2520multimodal%250Alanguage%2520models%2520and%2520text-to-image%2520models.%2520These%2520models%2520are%2520able%2520to%2520describe%2520and%250Agenerate%2520a%2520diverse%2520array%2520of%2520complex%252C%2520naturalistic%2520images%252C%2520yet%2520they%2520exhibit%250Asurprising%2520failures%2520on%2520basic%2520multi-object%2520reasoning%2520tasks%2520--%2520such%2520as%2520counting%252C%250Alocalization%252C%2520and%2520simple%2520forms%2520of%2520visual%2520analogy%2520--%2520that%2520humans%2520perform%2520with%250Anear%2520perfect%2520accuracy.%2520To%2520better%2520understand%2520this%2520puzzling%2520pattern%2520of%2520successes%250Aand%2520failures%252C%2520we%2520turn%2520to%2520theoretical%2520accounts%2520of%2520the%2520binding%2520problem%2520in%250Acognitive%2520science%2520and%2520neuroscience%252C%2520a%2520fundamental%2520problem%2520that%2520arises%2520when%2520a%250Ashared%2520set%2520of%2520representational%2520resources%2520must%2520be%2520used%2520to%2520represent%2520distinct%250Aentities%2520%2528e.g.%252C%2520to%2520represent%2520multiple%2520objects%2520in%2520an%2520image%2529%252C%2520necessitating%2520the%250Ause%2520of%2520serial%2520processing%2520to%2520avoid%2520interference.%2520We%2520find%2520that%2520many%2520of%2520the%250Apuzzling%2520failures%2520of%2520state-of-the-art%2520VLMs%2520can%2520be%2520explained%2520as%2520arising%2520due%2520to%250Athe%2520binding%2520problem%252C%2520and%2520that%2520these%2520failure%2520modes%2520are%2520strikingly%2520similar%2520to%2520the%250Alimitations%2520exhibited%2520by%2520rapid%252C%2520feedforward%2520processing%2520in%2520the%2520human%2520brain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.00238v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20the%20Limits%20of%20Vision%20Language%20Models%20Through%20the%20Lens%20of%0A%20%20the%20Binding%20Problem&entry.906535625=Declan%20Campbell%20and%20Sunayana%20Rane%20and%20Tyler%20Giallanza%20and%20Nicol%C3%B2%20De%20Sabbata%20and%20Kia%20Ghods%20and%20Amogh%20Joshi%20and%20Alexander%20Ku%20and%20Steven%20M.%20Frankland%20and%20Thomas%20L.%20Griffiths%20and%20Jonathan%20D.%20Cohen%20and%20Taylor%20W.%20Webb&entry.1292438233=%20%20Recent%20work%20has%20documented%20striking%20heterogeneity%20in%20the%20performance%20of%0Astate-of-the-art%20vision%20language%20models%20%28VLMs%29%2C%20including%20both%20multimodal%0Alanguage%20models%20and%20text-to-image%20models.%20These%20models%20are%20able%20to%20describe%20and%0Agenerate%20a%20diverse%20array%20of%20complex%2C%20naturalistic%20images%2C%20yet%20they%20exhibit%0Asurprising%20failures%20on%20basic%20multi-object%20reasoning%20tasks%20--%20such%20as%20counting%2C%0Alocalization%2C%20and%20simple%20forms%20of%20visual%20analogy%20--%20that%20humans%20perform%20with%0Anear%20perfect%20accuracy.%20To%20better%20understand%20this%20puzzling%20pattern%20of%20successes%0Aand%20failures%2C%20we%20turn%20to%20theoretical%20accounts%20of%20the%20binding%20problem%20in%0Acognitive%20science%20and%20neuroscience%2C%20a%20fundamental%20problem%20that%20arises%20when%20a%0Ashared%20set%20of%20representational%20resources%20must%20be%20used%20to%20represent%20distinct%0Aentities%20%28e.g.%2C%20to%20represent%20multiple%20objects%20in%20an%20image%29%2C%20necessitating%20the%0Ause%20of%20serial%20processing%20to%20avoid%20interference.%20We%20find%20that%20many%20of%20the%0Apuzzling%20failures%20of%20state-of-the-art%20VLMs%20can%20be%20explained%20as%20arising%20due%20to%0Athe%20binding%20problem%2C%20and%20that%20these%20failure%20modes%20are%20strikingly%20similar%20to%20the%0Alimitations%20exhibited%20by%20rapid%2C%20feedforward%20processing%20in%20the%20human%20brain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.00238v2&entry.124074799=Read"},
{"title": "Unsupervised Cross-Domain 3D Human Pose Estimation via\n  Pseudo-Label-Guided Global Transforms", "author": "Jingjing Liu and Zhiyong Wang and Xinyu Fan and Amirhossein Dadashzadeh and Honghai Liu and Majid Mirmehdi", "abstract": "  Existing 3D human pose estimation methods often suffer in performance, when\napplied to cross-scenario inference, due to domain shifts in characteristics\nsuch as camera viewpoint, position, posture, and body size. Among these\nfactors, camera viewpoints and locations {have been shown} to contribute\nsignificantly to the domain gap by influencing the global positions of human\nposes. To address this, we propose a novel framework that explicitly conducts\nglobal transformations between pose positions in the camera coordinate systems\nof source and target domains. We start with a Pseudo-Label Generation Module\nthat is applied to the 2D poses of the target dataset to generate pseudo-3D\nposes. Then, a Global Transformation Module leverages a human-centered\ncoordinate system as a novel bridging mechanism to seamlessly align the\npositional orientations of poses across disparate domains, ensuring consistent\nspatial referencing. To further enhance generalization, a Pose Augmentor is\nincorporated to address variations in human posture and body size. This process\nis iterative, allowing refined pseudo-labels to progressively improve guidance\nfor domain adaptation. Our method is evaluated on various cross-dataset\nbenchmarks, including Human3.6M, MPI-INF-3DHP, and 3DPW. The proposed method\noutperforms state-of-the-art approaches and even outperforms the target-trained\nmodel.\n", "link": "http://arxiv.org/abs/2504.12699v1", "date": "2025-04-17", "relevancy": 2.5148, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6388}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6294}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Cross-Domain%203D%20Human%20Pose%20Estimation%20via%0A%20%20Pseudo-Label-Guided%20Global%20Transforms&body=Title%3A%20Unsupervised%20Cross-Domain%203D%20Human%20Pose%20Estimation%20via%0A%20%20Pseudo-Label-Guided%20Global%20Transforms%0AAuthor%3A%20Jingjing%20Liu%20and%20Zhiyong%20Wang%20and%20Xinyu%20Fan%20and%20Amirhossein%20Dadashzadeh%20and%20Honghai%20Liu%20and%20Majid%20Mirmehdi%0AAbstract%3A%20%20%20Existing%203D%20human%20pose%20estimation%20methods%20often%20suffer%20in%20performance%2C%20when%0Aapplied%20to%20cross-scenario%20inference%2C%20due%20to%20domain%20shifts%20in%20characteristics%0Asuch%20as%20camera%20viewpoint%2C%20position%2C%20posture%2C%20and%20body%20size.%20Among%20these%0Afactors%2C%20camera%20viewpoints%20and%20locations%20%7Bhave%20been%20shown%7D%20to%20contribute%0Asignificantly%20to%20the%20domain%20gap%20by%20influencing%20the%20global%20positions%20of%20human%0Aposes.%20To%20address%20this%2C%20we%20propose%20a%20novel%20framework%20that%20explicitly%20conducts%0Aglobal%20transformations%20between%20pose%20positions%20in%20the%20camera%20coordinate%20systems%0Aof%20source%20and%20target%20domains.%20We%20start%20with%20a%20Pseudo-Label%20Generation%20Module%0Athat%20is%20applied%20to%20the%202D%20poses%20of%20the%20target%20dataset%20to%20generate%20pseudo-3D%0Aposes.%20Then%2C%20a%20Global%20Transformation%20Module%20leverages%20a%20human-centered%0Acoordinate%20system%20as%20a%20novel%20bridging%20mechanism%20to%20seamlessly%20align%20the%0Apositional%20orientations%20of%20poses%20across%20disparate%20domains%2C%20ensuring%20consistent%0Aspatial%20referencing.%20To%20further%20enhance%20generalization%2C%20a%20Pose%20Augmentor%20is%0Aincorporated%20to%20address%20variations%20in%20human%20posture%20and%20body%20size.%20This%20process%0Ais%20iterative%2C%20allowing%20refined%20pseudo-labels%20to%20progressively%20improve%20guidance%0Afor%20domain%20adaptation.%20Our%20method%20is%20evaluated%20on%20various%20cross-dataset%0Abenchmarks%2C%20including%20Human3.6M%2C%20MPI-INF-3DHP%2C%20and%203DPW.%20The%20proposed%20method%0Aoutperforms%20state-of-the-art%20approaches%20and%20even%20outperforms%20the%20target-trained%0Amodel.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12699v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Cross-Domain%25203D%2520Human%2520Pose%2520Estimation%2520via%250A%2520%2520Pseudo-Label-Guided%2520Global%2520Transforms%26entry.906535625%3DJingjing%2520Liu%2520and%2520Zhiyong%2520Wang%2520and%2520Xinyu%2520Fan%2520and%2520Amirhossein%2520Dadashzadeh%2520and%2520Honghai%2520Liu%2520and%2520Majid%2520Mirmehdi%26entry.1292438233%3D%2520%2520Existing%25203D%2520human%2520pose%2520estimation%2520methods%2520often%2520suffer%2520in%2520performance%252C%2520when%250Aapplied%2520to%2520cross-scenario%2520inference%252C%2520due%2520to%2520domain%2520shifts%2520in%2520characteristics%250Asuch%2520as%2520camera%2520viewpoint%252C%2520position%252C%2520posture%252C%2520and%2520body%2520size.%2520Among%2520these%250Afactors%252C%2520camera%2520viewpoints%2520and%2520locations%2520%257Bhave%2520been%2520shown%257D%2520to%2520contribute%250Asignificantly%2520to%2520the%2520domain%2520gap%2520by%2520influencing%2520the%2520global%2520positions%2520of%2520human%250Aposes.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520novel%2520framework%2520that%2520explicitly%2520conducts%250Aglobal%2520transformations%2520between%2520pose%2520positions%2520in%2520the%2520camera%2520coordinate%2520systems%250Aof%2520source%2520and%2520target%2520domains.%2520We%2520start%2520with%2520a%2520Pseudo-Label%2520Generation%2520Module%250Athat%2520is%2520applied%2520to%2520the%25202D%2520poses%2520of%2520the%2520target%2520dataset%2520to%2520generate%2520pseudo-3D%250Aposes.%2520Then%252C%2520a%2520Global%2520Transformation%2520Module%2520leverages%2520a%2520human-centered%250Acoordinate%2520system%2520as%2520a%2520novel%2520bridging%2520mechanism%2520to%2520seamlessly%2520align%2520the%250Apositional%2520orientations%2520of%2520poses%2520across%2520disparate%2520domains%252C%2520ensuring%2520consistent%250Aspatial%2520referencing.%2520To%2520further%2520enhance%2520generalization%252C%2520a%2520Pose%2520Augmentor%2520is%250Aincorporated%2520to%2520address%2520variations%2520in%2520human%2520posture%2520and%2520body%2520size.%2520This%2520process%250Ais%2520iterative%252C%2520allowing%2520refined%2520pseudo-labels%2520to%2520progressively%2520improve%2520guidance%250Afor%2520domain%2520adaptation.%2520Our%2520method%2520is%2520evaluated%2520on%2520various%2520cross-dataset%250Abenchmarks%252C%2520including%2520Human3.6M%252C%2520MPI-INF-3DHP%252C%2520and%25203DPW.%2520The%2520proposed%2520method%250Aoutperforms%2520state-of-the-art%2520approaches%2520and%2520even%2520outperforms%2520the%2520target-trained%250Amodel.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12699v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Cross-Domain%203D%20Human%20Pose%20Estimation%20via%0A%20%20Pseudo-Label-Guided%20Global%20Transforms&entry.906535625=Jingjing%20Liu%20and%20Zhiyong%20Wang%20and%20Xinyu%20Fan%20and%20Amirhossein%20Dadashzadeh%20and%20Honghai%20Liu%20and%20Majid%20Mirmehdi&entry.1292438233=%20%20Existing%203D%20human%20pose%20estimation%20methods%20often%20suffer%20in%20performance%2C%20when%0Aapplied%20to%20cross-scenario%20inference%2C%20due%20to%20domain%20shifts%20in%20characteristics%0Asuch%20as%20camera%20viewpoint%2C%20position%2C%20posture%2C%20and%20body%20size.%20Among%20these%0Afactors%2C%20camera%20viewpoints%20and%20locations%20%7Bhave%20been%20shown%7D%20to%20contribute%0Asignificantly%20to%20the%20domain%20gap%20by%20influencing%20the%20global%20positions%20of%20human%0Aposes.%20To%20address%20this%2C%20we%20propose%20a%20novel%20framework%20that%20explicitly%20conducts%0Aglobal%20transformations%20between%20pose%20positions%20in%20the%20camera%20coordinate%20systems%0Aof%20source%20and%20target%20domains.%20We%20start%20with%20a%20Pseudo-Label%20Generation%20Module%0Athat%20is%20applied%20to%20the%202D%20poses%20of%20the%20target%20dataset%20to%20generate%20pseudo-3D%0Aposes.%20Then%2C%20a%20Global%20Transformation%20Module%20leverages%20a%20human-centered%0Acoordinate%20system%20as%20a%20novel%20bridging%20mechanism%20to%20seamlessly%20align%20the%0Apositional%20orientations%20of%20poses%20across%20disparate%20domains%2C%20ensuring%20consistent%0Aspatial%20referencing.%20To%20further%20enhance%20generalization%2C%20a%20Pose%20Augmentor%20is%0Aincorporated%20to%20address%20variations%20in%20human%20posture%20and%20body%20size.%20This%20process%0Ais%20iterative%2C%20allowing%20refined%20pseudo-labels%20to%20progressively%20improve%20guidance%0Afor%20domain%20adaptation.%20Our%20method%20is%20evaluated%20on%20various%20cross-dataset%0Abenchmarks%2C%20including%20Human3.6M%2C%20MPI-INF-3DHP%2C%20and%203DPW.%20The%20proposed%20method%0Aoutperforms%20state-of-the-art%20approaches%20and%20even%20outperforms%20the%20target-trained%0Amodel.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12699v1&entry.124074799=Read"},
{"title": "A general language model for peptide identification", "author": "Jixiu Zhai and Tianchi Lu and Haitian Zhong and Ziyang Xu and Yuhuan Liu and Shengrui Xu and Jingwan Wang and Dan Huang", "abstract": "  Advances in peptide identification are revolutionizing our ability to\ndecipher protein functions and accelerate therapeutic discovery. We present\nPDeepPP, a deep learning framework that integrates pretrained protein language\nmodels with parallel transformer-CNN architectures, achieving state-of-the-art\nperformance in peptide characterization tasks. The model's hybrid architecture\ndemonstrates unique capabilities in capturing both local sequence motifs and\nglobal structural features, as evidenced by 29% improved cluster separation in\nUMAP visualizations compared to conventional approaches. Evaluated across 33\nbiological recognition tasks - including post-translational modification site\nprediction and bioactive peptide identification - PDeepPP outperformed existing\nmethods in 25 tasks with average AUC improvements of 4.2%. Notably, it achieved\n0.9726 accuracy with PR AUC 0.9977 in antimicrobial peptide detection while\nreducing false negatives by 37.5% in antimalarial recognition scenarios. This\nframework enables accurate large-scale peptide analysis, achieving 218*\nacceleration over sequence-alignment-based methods while maintaining 99.5%\nspecificity in critical glycosylation site detection.PDeepPP establishes a new\nparadigm for computational peptide analysis through its synergistic\narchitecture design, enabling rapid yet precise functional annotation that\nbridges molecular pattern recognition with translational biomedical\napplications.We have made our implementation, including code, data, and\npretrained models, publicly available via GitHub\n(https://github.com/fondress/PDeepPP) and Hugging Face\n(https://huggingface.co/fondress/PDeppPP).\n", "link": "http://arxiv.org/abs/2502.15610v2", "date": "2025-04-17", "relevancy": 2.5128, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5092}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5092}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4893}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20general%20language%20model%20for%20peptide%20identification&body=Title%3A%20A%20general%20language%20model%20for%20peptide%20identification%0AAuthor%3A%20Jixiu%20Zhai%20and%20Tianchi%20Lu%20and%20Haitian%20Zhong%20and%20Ziyang%20Xu%20and%20Yuhuan%20Liu%20and%20Shengrui%20Xu%20and%20Jingwan%20Wang%20and%20Dan%20Huang%0AAbstract%3A%20%20%20Advances%20in%20peptide%20identification%20are%20revolutionizing%20our%20ability%20to%0Adecipher%20protein%20functions%20and%20accelerate%20therapeutic%20discovery.%20We%20present%0APDeepPP%2C%20a%20deep%20learning%20framework%20that%20integrates%20pretrained%20protein%20language%0Amodels%20with%20parallel%20transformer-CNN%20architectures%2C%20achieving%20state-of-the-art%0Aperformance%20in%20peptide%20characterization%20tasks.%20The%20model%27s%20hybrid%20architecture%0Ademonstrates%20unique%20capabilities%20in%20capturing%20both%20local%20sequence%20motifs%20and%0Aglobal%20structural%20features%2C%20as%20evidenced%20by%2029%25%20improved%20cluster%20separation%20in%0AUMAP%20visualizations%20compared%20to%20conventional%20approaches.%20Evaluated%20across%2033%0Abiological%20recognition%20tasks%20-%20including%20post-translational%20modification%20site%0Aprediction%20and%20bioactive%20peptide%20identification%20-%20PDeepPP%20outperformed%20existing%0Amethods%20in%2025%20tasks%20with%20average%20AUC%20improvements%20of%204.2%25.%20Notably%2C%20it%20achieved%0A0.9726%20accuracy%20with%20PR%20AUC%200.9977%20in%20antimicrobial%20peptide%20detection%20while%0Areducing%20false%20negatives%20by%2037.5%25%20in%20antimalarial%20recognition%20scenarios.%20This%0Aframework%20enables%20accurate%20large-scale%20peptide%20analysis%2C%20achieving%20218%2A%0Aacceleration%20over%20sequence-alignment-based%20methods%20while%20maintaining%2099.5%25%0Aspecificity%20in%20critical%20glycosylation%20site%20detection.PDeepPP%20establishes%20a%20new%0Aparadigm%20for%20computational%20peptide%20analysis%20through%20its%20synergistic%0Aarchitecture%20design%2C%20enabling%20rapid%20yet%20precise%20functional%20annotation%20that%0Abridges%20molecular%20pattern%20recognition%20with%20translational%20biomedical%0Aapplications.We%20have%20made%20our%20implementation%2C%20including%20code%2C%20data%2C%20and%0Apretrained%20models%2C%20publicly%20available%20via%20GitHub%0A%28https%3A//github.com/fondress/PDeepPP%29%20and%20Hugging%20Face%0A%28https%3A//huggingface.co/fondress/PDeppPP%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15610v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520general%2520language%2520model%2520for%2520peptide%2520identification%26entry.906535625%3DJixiu%2520Zhai%2520and%2520Tianchi%2520Lu%2520and%2520Haitian%2520Zhong%2520and%2520Ziyang%2520Xu%2520and%2520Yuhuan%2520Liu%2520and%2520Shengrui%2520Xu%2520and%2520Jingwan%2520Wang%2520and%2520Dan%2520Huang%26entry.1292438233%3D%2520%2520Advances%2520in%2520peptide%2520identification%2520are%2520revolutionizing%2520our%2520ability%2520to%250Adecipher%2520protein%2520functions%2520and%2520accelerate%2520therapeutic%2520discovery.%2520We%2520present%250APDeepPP%252C%2520a%2520deep%2520learning%2520framework%2520that%2520integrates%2520pretrained%2520protein%2520language%250Amodels%2520with%2520parallel%2520transformer-CNN%2520architectures%252C%2520achieving%2520state-of-the-art%250Aperformance%2520in%2520peptide%2520characterization%2520tasks.%2520The%2520model%2527s%2520hybrid%2520architecture%250Ademonstrates%2520unique%2520capabilities%2520in%2520capturing%2520both%2520local%2520sequence%2520motifs%2520and%250Aglobal%2520structural%2520features%252C%2520as%2520evidenced%2520by%252029%2525%2520improved%2520cluster%2520separation%2520in%250AUMAP%2520visualizations%2520compared%2520to%2520conventional%2520approaches.%2520Evaluated%2520across%252033%250Abiological%2520recognition%2520tasks%2520-%2520including%2520post-translational%2520modification%2520site%250Aprediction%2520and%2520bioactive%2520peptide%2520identification%2520-%2520PDeepPP%2520outperformed%2520existing%250Amethods%2520in%252025%2520tasks%2520with%2520average%2520AUC%2520improvements%2520of%25204.2%2525.%2520Notably%252C%2520it%2520achieved%250A0.9726%2520accuracy%2520with%2520PR%2520AUC%25200.9977%2520in%2520antimicrobial%2520peptide%2520detection%2520while%250Areducing%2520false%2520negatives%2520by%252037.5%2525%2520in%2520antimalarial%2520recognition%2520scenarios.%2520This%250Aframework%2520enables%2520accurate%2520large-scale%2520peptide%2520analysis%252C%2520achieving%2520218%252A%250Aacceleration%2520over%2520sequence-alignment-based%2520methods%2520while%2520maintaining%252099.5%2525%250Aspecificity%2520in%2520critical%2520glycosylation%2520site%2520detection.PDeepPP%2520establishes%2520a%2520new%250Aparadigm%2520for%2520computational%2520peptide%2520analysis%2520through%2520its%2520synergistic%250Aarchitecture%2520design%252C%2520enabling%2520rapid%2520yet%2520precise%2520functional%2520annotation%2520that%250Abridges%2520molecular%2520pattern%2520recognition%2520with%2520translational%2520biomedical%250Aapplications.We%2520have%2520made%2520our%2520implementation%252C%2520including%2520code%252C%2520data%252C%2520and%250Apretrained%2520models%252C%2520publicly%2520available%2520via%2520GitHub%250A%2528https%253A//github.com/fondress/PDeepPP%2529%2520and%2520Hugging%2520Face%250A%2528https%253A//huggingface.co/fondress/PDeppPP%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15610v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20general%20language%20model%20for%20peptide%20identification&entry.906535625=Jixiu%20Zhai%20and%20Tianchi%20Lu%20and%20Haitian%20Zhong%20and%20Ziyang%20Xu%20and%20Yuhuan%20Liu%20and%20Shengrui%20Xu%20and%20Jingwan%20Wang%20and%20Dan%20Huang&entry.1292438233=%20%20Advances%20in%20peptide%20identification%20are%20revolutionizing%20our%20ability%20to%0Adecipher%20protein%20functions%20and%20accelerate%20therapeutic%20discovery.%20We%20present%0APDeepPP%2C%20a%20deep%20learning%20framework%20that%20integrates%20pretrained%20protein%20language%0Amodels%20with%20parallel%20transformer-CNN%20architectures%2C%20achieving%20state-of-the-art%0Aperformance%20in%20peptide%20characterization%20tasks.%20The%20model%27s%20hybrid%20architecture%0Ademonstrates%20unique%20capabilities%20in%20capturing%20both%20local%20sequence%20motifs%20and%0Aglobal%20structural%20features%2C%20as%20evidenced%20by%2029%25%20improved%20cluster%20separation%20in%0AUMAP%20visualizations%20compared%20to%20conventional%20approaches.%20Evaluated%20across%2033%0Abiological%20recognition%20tasks%20-%20including%20post-translational%20modification%20site%0Aprediction%20and%20bioactive%20peptide%20identification%20-%20PDeepPP%20outperformed%20existing%0Amethods%20in%2025%20tasks%20with%20average%20AUC%20improvements%20of%204.2%25.%20Notably%2C%20it%20achieved%0A0.9726%20accuracy%20with%20PR%20AUC%200.9977%20in%20antimicrobial%20peptide%20detection%20while%0Areducing%20false%20negatives%20by%2037.5%25%20in%20antimalarial%20recognition%20scenarios.%20This%0Aframework%20enables%20accurate%20large-scale%20peptide%20analysis%2C%20achieving%20218%2A%0Aacceleration%20over%20sequence-alignment-based%20methods%20while%20maintaining%2099.5%25%0Aspecificity%20in%20critical%20glycosylation%20site%20detection.PDeepPP%20establishes%20a%20new%0Aparadigm%20for%20computational%20peptide%20analysis%20through%20its%20synergistic%0Aarchitecture%20design%2C%20enabling%20rapid%20yet%20precise%20functional%20annotation%20that%0Abridges%20molecular%20pattern%20recognition%20with%20translational%20biomedical%0Aapplications.We%20have%20made%20our%20implementation%2C%20including%20code%2C%20data%2C%20and%0Apretrained%20models%2C%20publicly%20available%20via%20GitHub%0A%28https%3A//github.com/fondress/PDeepPP%29%20and%20Hugging%20Face%0A%28https%3A//huggingface.co/fondress/PDeppPP%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15610v2&entry.124074799=Read"},
{"title": "Transfer Learning for Temporal Link Prediction", "author": "Ayan Chatterjee and Barbara Ikica and Babak Ravandi and John Palowitch", "abstract": "  Link prediction on graphs has applications spanning from recommender systems\nto drug discovery. Temporal link prediction (TLP) refers to predicting future\nlinks in a temporally evolving graph and adds additional complexity related to\nthe dynamic nature of graphs. State-of-the-art TLP models incorporate memory\nmodules alongside graph neural networks to learn both the temporal mechanisms\nof incoming nodes and the evolving graph topology. However, memory modules only\nstore information about nodes seen at train time, and hence such models cannot\nbe directly transferred to entirely new graphs at test time and deployment. In\nthis work, we study a new transfer learning task for temporal link prediction,\nand develop transfer-effective methods for memory-laden models. Specifically,\nmotivated by work showing the informativeness of structural signals for the TLP\ntask, we augment a structural mapping module to the existing TLP model\narchitectures, which learns a mapping from graph structural (topological)\nfeatures to memory embeddings. Our work paves the way for a memory-free\nfoundation model for TLP.\n", "link": "http://arxiv.org/abs/2504.10925v2", "date": "2025-04-17", "relevancy": 2.5, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5144}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.506}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transfer%20Learning%20for%20Temporal%20Link%20Prediction&body=Title%3A%20Transfer%20Learning%20for%20Temporal%20Link%20Prediction%0AAuthor%3A%20Ayan%20Chatterjee%20and%20Barbara%20Ikica%20and%20Babak%20Ravandi%20and%20John%20Palowitch%0AAbstract%3A%20%20%20Link%20prediction%20on%20graphs%20has%20applications%20spanning%20from%20recommender%20systems%0Ato%20drug%20discovery.%20Temporal%20link%20prediction%20%28TLP%29%20refers%20to%20predicting%20future%0Alinks%20in%20a%20temporally%20evolving%20graph%20and%20adds%20additional%20complexity%20related%20to%0Athe%20dynamic%20nature%20of%20graphs.%20State-of-the-art%20TLP%20models%20incorporate%20memory%0Amodules%20alongside%20graph%20neural%20networks%20to%20learn%20both%20the%20temporal%20mechanisms%0Aof%20incoming%20nodes%20and%20the%20evolving%20graph%20topology.%20However%2C%20memory%20modules%20only%0Astore%20information%20about%20nodes%20seen%20at%20train%20time%2C%20and%20hence%20such%20models%20cannot%0Abe%20directly%20transferred%20to%20entirely%20new%20graphs%20at%20test%20time%20and%20deployment.%20In%0Athis%20work%2C%20we%20study%20a%20new%20transfer%20learning%20task%20for%20temporal%20link%20prediction%2C%0Aand%20develop%20transfer-effective%20methods%20for%20memory-laden%20models.%20Specifically%2C%0Amotivated%20by%20work%20showing%20the%20informativeness%20of%20structural%20signals%20for%20the%20TLP%0Atask%2C%20we%20augment%20a%20structural%20mapping%20module%20to%20the%20existing%20TLP%20model%0Aarchitectures%2C%20which%20learns%20a%20mapping%20from%20graph%20structural%20%28topological%29%0Afeatures%20to%20memory%20embeddings.%20Our%20work%20paves%20the%20way%20for%20a%20memory-free%0Afoundation%20model%20for%20TLP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.10925v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransfer%2520Learning%2520for%2520Temporal%2520Link%2520Prediction%26entry.906535625%3DAyan%2520Chatterjee%2520and%2520Barbara%2520Ikica%2520and%2520Babak%2520Ravandi%2520and%2520John%2520Palowitch%26entry.1292438233%3D%2520%2520Link%2520prediction%2520on%2520graphs%2520has%2520applications%2520spanning%2520from%2520recommender%2520systems%250Ato%2520drug%2520discovery.%2520Temporal%2520link%2520prediction%2520%2528TLP%2529%2520refers%2520to%2520predicting%2520future%250Alinks%2520in%2520a%2520temporally%2520evolving%2520graph%2520and%2520adds%2520additional%2520complexity%2520related%2520to%250Athe%2520dynamic%2520nature%2520of%2520graphs.%2520State-of-the-art%2520TLP%2520models%2520incorporate%2520memory%250Amodules%2520alongside%2520graph%2520neural%2520networks%2520to%2520learn%2520both%2520the%2520temporal%2520mechanisms%250Aof%2520incoming%2520nodes%2520and%2520the%2520evolving%2520graph%2520topology.%2520However%252C%2520memory%2520modules%2520only%250Astore%2520information%2520about%2520nodes%2520seen%2520at%2520train%2520time%252C%2520and%2520hence%2520such%2520models%2520cannot%250Abe%2520directly%2520transferred%2520to%2520entirely%2520new%2520graphs%2520at%2520test%2520time%2520and%2520deployment.%2520In%250Athis%2520work%252C%2520we%2520study%2520a%2520new%2520transfer%2520learning%2520task%2520for%2520temporal%2520link%2520prediction%252C%250Aand%2520develop%2520transfer-effective%2520methods%2520for%2520memory-laden%2520models.%2520Specifically%252C%250Amotivated%2520by%2520work%2520showing%2520the%2520informativeness%2520of%2520structural%2520signals%2520for%2520the%2520TLP%250Atask%252C%2520we%2520augment%2520a%2520structural%2520mapping%2520module%2520to%2520the%2520existing%2520TLP%2520model%250Aarchitectures%252C%2520which%2520learns%2520a%2520mapping%2520from%2520graph%2520structural%2520%2528topological%2529%250Afeatures%2520to%2520memory%2520embeddings.%2520Our%2520work%2520paves%2520the%2520way%2520for%2520a%2520memory-free%250Afoundation%2520model%2520for%2520TLP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.10925v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transfer%20Learning%20for%20Temporal%20Link%20Prediction&entry.906535625=Ayan%20Chatterjee%20and%20Barbara%20Ikica%20and%20Babak%20Ravandi%20and%20John%20Palowitch&entry.1292438233=%20%20Link%20prediction%20on%20graphs%20has%20applications%20spanning%20from%20recommender%20systems%0Ato%20drug%20discovery.%20Temporal%20link%20prediction%20%28TLP%29%20refers%20to%20predicting%20future%0Alinks%20in%20a%20temporally%20evolving%20graph%20and%20adds%20additional%20complexity%20related%20to%0Athe%20dynamic%20nature%20of%20graphs.%20State-of-the-art%20TLP%20models%20incorporate%20memory%0Amodules%20alongside%20graph%20neural%20networks%20to%20learn%20both%20the%20temporal%20mechanisms%0Aof%20incoming%20nodes%20and%20the%20evolving%20graph%20topology.%20However%2C%20memory%20modules%20only%0Astore%20information%20about%20nodes%20seen%20at%20train%20time%2C%20and%20hence%20such%20models%20cannot%0Abe%20directly%20transferred%20to%20entirely%20new%20graphs%20at%20test%20time%20and%20deployment.%20In%0Athis%20work%2C%20we%20study%20a%20new%20transfer%20learning%20task%20for%20temporal%20link%20prediction%2C%0Aand%20develop%20transfer-effective%20methods%20for%20memory-laden%20models.%20Specifically%2C%0Amotivated%20by%20work%20showing%20the%20informativeness%20of%20structural%20signals%20for%20the%20TLP%0Atask%2C%20we%20augment%20a%20structural%20mapping%20module%20to%20the%20existing%20TLP%20model%0Aarchitectures%2C%20which%20learns%20a%20mapping%20from%20graph%20structural%20%28topological%29%0Afeatures%20to%20memory%20embeddings.%20Our%20work%20paves%20the%20way%20for%20a%20memory-free%0Afoundation%20model%20for%20TLP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.10925v2&entry.124074799=Read"},
{"title": "Adaptive Decision Boundary for Few-Shot Class-Incremental Learning", "author": "Linhao Li and Yongzhang Tan and Siyuan Yang and Hao Cheng and Yongfeng Dong and Liang Yang", "abstract": "  Few-Shot Class-Incremental Learning (FSCIL) aims to continuously learn new\nclasses from a limited set of training samples without forgetting knowledge of\npreviously learned classes. Conventional FSCIL methods typically build a robust\nfeature extractor during the base training session with abundant training\nsamples and subsequently freeze this extractor, only fine-tuning the classifier\nin subsequent incremental phases. However, current strategies primarily focus\non preventing catastrophic forgetting, considering only the relationship\nbetween novel and base classes, without paying attention to the specific\ndecision spaces of each class. To address this challenge, we propose a\nplug-and-play Adaptive Decision Boundary Strategy (ADBS), which is compatible\nwith most FSCIL methods. Specifically, we assign a specific decision boundary\nto each class and adaptively adjust these boundaries during training to\noptimally refine the decision spaces for the classes in each session.\nFurthermore, to amplify the distinctiveness between classes, we employ a novel\ninter-class constraint loss that optimizes the decision boundaries and\nprototypes for each class. Extensive experiments on three benchmarks, namely\nCIFAR100, miniImageNet, and CUB200, demonstrate that incorporating our ADBS\nmethod with existing FSCIL techniques significantly improves performance,\nachieving overall state-of-the-art results.\n", "link": "http://arxiv.org/abs/2504.10976v2", "date": "2025-04-17", "relevancy": 2.4876, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5158}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4925}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Decision%20Boundary%20for%20Few-Shot%20Class-Incremental%20Learning&body=Title%3A%20Adaptive%20Decision%20Boundary%20for%20Few-Shot%20Class-Incremental%20Learning%0AAuthor%3A%20Linhao%20Li%20and%20Yongzhang%20Tan%20and%20Siyuan%20Yang%20and%20Hao%20Cheng%20and%20Yongfeng%20Dong%20and%20Liang%20Yang%0AAbstract%3A%20%20%20Few-Shot%20Class-Incremental%20Learning%20%28FSCIL%29%20aims%20to%20continuously%20learn%20new%0Aclasses%20from%20a%20limited%20set%20of%20training%20samples%20without%20forgetting%20knowledge%20of%0Apreviously%20learned%20classes.%20Conventional%20FSCIL%20methods%20typically%20build%20a%20robust%0Afeature%20extractor%20during%20the%20base%20training%20session%20with%20abundant%20training%0Asamples%20and%20subsequently%20freeze%20this%20extractor%2C%20only%20fine-tuning%20the%20classifier%0Ain%20subsequent%20incremental%20phases.%20However%2C%20current%20strategies%20primarily%20focus%0Aon%20preventing%20catastrophic%20forgetting%2C%20considering%20only%20the%20relationship%0Abetween%20novel%20and%20base%20classes%2C%20without%20paying%20attention%20to%20the%20specific%0Adecision%20spaces%20of%20each%20class.%20To%20address%20this%20challenge%2C%20we%20propose%20a%0Aplug-and-play%20Adaptive%20Decision%20Boundary%20Strategy%20%28ADBS%29%2C%20which%20is%20compatible%0Awith%20most%20FSCIL%20methods.%20Specifically%2C%20we%20assign%20a%20specific%20decision%20boundary%0Ato%20each%20class%20and%20adaptively%20adjust%20these%20boundaries%20during%20training%20to%0Aoptimally%20refine%20the%20decision%20spaces%20for%20the%20classes%20in%20each%20session.%0AFurthermore%2C%20to%20amplify%20the%20distinctiveness%20between%20classes%2C%20we%20employ%20a%20novel%0Ainter-class%20constraint%20loss%20that%20optimizes%20the%20decision%20boundaries%20and%0Aprototypes%20for%20each%20class.%20Extensive%20experiments%20on%20three%20benchmarks%2C%20namely%0ACIFAR100%2C%20miniImageNet%2C%20and%20CUB200%2C%20demonstrate%20that%20incorporating%20our%20ADBS%0Amethod%20with%20existing%20FSCIL%20techniques%20significantly%20improves%20performance%2C%0Aachieving%20overall%20state-of-the-art%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.10976v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Decision%2520Boundary%2520for%2520Few-Shot%2520Class-Incremental%2520Learning%26entry.906535625%3DLinhao%2520Li%2520and%2520Yongzhang%2520Tan%2520and%2520Siyuan%2520Yang%2520and%2520Hao%2520Cheng%2520and%2520Yongfeng%2520Dong%2520and%2520Liang%2520Yang%26entry.1292438233%3D%2520%2520Few-Shot%2520Class-Incremental%2520Learning%2520%2528FSCIL%2529%2520aims%2520to%2520continuously%2520learn%2520new%250Aclasses%2520from%2520a%2520limited%2520set%2520of%2520training%2520samples%2520without%2520forgetting%2520knowledge%2520of%250Apreviously%2520learned%2520classes.%2520Conventional%2520FSCIL%2520methods%2520typically%2520build%2520a%2520robust%250Afeature%2520extractor%2520during%2520the%2520base%2520training%2520session%2520with%2520abundant%2520training%250Asamples%2520and%2520subsequently%2520freeze%2520this%2520extractor%252C%2520only%2520fine-tuning%2520the%2520classifier%250Ain%2520subsequent%2520incremental%2520phases.%2520However%252C%2520current%2520strategies%2520primarily%2520focus%250Aon%2520preventing%2520catastrophic%2520forgetting%252C%2520considering%2520only%2520the%2520relationship%250Abetween%2520novel%2520and%2520base%2520classes%252C%2520without%2520paying%2520attention%2520to%2520the%2520specific%250Adecision%2520spaces%2520of%2520each%2520class.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%250Aplug-and-play%2520Adaptive%2520Decision%2520Boundary%2520Strategy%2520%2528ADBS%2529%252C%2520which%2520is%2520compatible%250Awith%2520most%2520FSCIL%2520methods.%2520Specifically%252C%2520we%2520assign%2520a%2520specific%2520decision%2520boundary%250Ato%2520each%2520class%2520and%2520adaptively%2520adjust%2520these%2520boundaries%2520during%2520training%2520to%250Aoptimally%2520refine%2520the%2520decision%2520spaces%2520for%2520the%2520classes%2520in%2520each%2520session.%250AFurthermore%252C%2520to%2520amplify%2520the%2520distinctiveness%2520between%2520classes%252C%2520we%2520employ%2520a%2520novel%250Ainter-class%2520constraint%2520loss%2520that%2520optimizes%2520the%2520decision%2520boundaries%2520and%250Aprototypes%2520for%2520each%2520class.%2520Extensive%2520experiments%2520on%2520three%2520benchmarks%252C%2520namely%250ACIFAR100%252C%2520miniImageNet%252C%2520and%2520CUB200%252C%2520demonstrate%2520that%2520incorporating%2520our%2520ADBS%250Amethod%2520with%2520existing%2520FSCIL%2520techniques%2520significantly%2520improves%2520performance%252C%250Aachieving%2520overall%2520state-of-the-art%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.10976v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Decision%20Boundary%20for%20Few-Shot%20Class-Incremental%20Learning&entry.906535625=Linhao%20Li%20and%20Yongzhang%20Tan%20and%20Siyuan%20Yang%20and%20Hao%20Cheng%20and%20Yongfeng%20Dong%20and%20Liang%20Yang&entry.1292438233=%20%20Few-Shot%20Class-Incremental%20Learning%20%28FSCIL%29%20aims%20to%20continuously%20learn%20new%0Aclasses%20from%20a%20limited%20set%20of%20training%20samples%20without%20forgetting%20knowledge%20of%0Apreviously%20learned%20classes.%20Conventional%20FSCIL%20methods%20typically%20build%20a%20robust%0Afeature%20extractor%20during%20the%20base%20training%20session%20with%20abundant%20training%0Asamples%20and%20subsequently%20freeze%20this%20extractor%2C%20only%20fine-tuning%20the%20classifier%0Ain%20subsequent%20incremental%20phases.%20However%2C%20current%20strategies%20primarily%20focus%0Aon%20preventing%20catastrophic%20forgetting%2C%20considering%20only%20the%20relationship%0Abetween%20novel%20and%20base%20classes%2C%20without%20paying%20attention%20to%20the%20specific%0Adecision%20spaces%20of%20each%20class.%20To%20address%20this%20challenge%2C%20we%20propose%20a%0Aplug-and-play%20Adaptive%20Decision%20Boundary%20Strategy%20%28ADBS%29%2C%20which%20is%20compatible%0Awith%20most%20FSCIL%20methods.%20Specifically%2C%20we%20assign%20a%20specific%20decision%20boundary%0Ato%20each%20class%20and%20adaptively%20adjust%20these%20boundaries%20during%20training%20to%0Aoptimally%20refine%20the%20decision%20spaces%20for%20the%20classes%20in%20each%20session.%0AFurthermore%2C%20to%20amplify%20the%20distinctiveness%20between%20classes%2C%20we%20employ%20a%20novel%0Ainter-class%20constraint%20loss%20that%20optimizes%20the%20decision%20boundaries%20and%0Aprototypes%20for%20each%20class.%20Extensive%20experiments%20on%20three%20benchmarks%2C%20namely%0ACIFAR100%2C%20miniImageNet%2C%20and%20CUB200%2C%20demonstrate%20that%20incorporating%20our%20ADBS%0Amethod%20with%20existing%20FSCIL%20techniques%20significantly%20improves%20performance%2C%0Aachieving%20overall%20state-of-the-art%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.10976v2&entry.124074799=Read"},
{"title": "Aspect-Based Summarization with Self-Aspect Retrieval Enhanced\n  Generation", "author": "Yichao Feng and Shuai Zhao and Yueqiu Li and Luwei Xiao and Xiaobao Wu and Anh Tuan Luu", "abstract": "  Aspect-based summarization aims to generate summaries tailored to specific\naspects, addressing the resource constraints and limited generalizability of\ntraditional summarization approaches. Recently, large language models have\nshown promise in this task without the need for training. However, they rely\nexcessively on prompt engineering and face token limits and hallucination\nchallenges, especially with in-context learning. To address these challenges,\nin this paper, we propose a novel framework for aspect-based summarization:\nSelf-Aspect Retrieval Enhanced Summary Generation. Rather than relying solely\non in-context learning, given an aspect, we employ an embedding-driven\nretrieval mechanism to identify its relevant text segments. This approach\nextracts the pertinent content while avoiding unnecessary details, thereby\nmitigating the challenge of token limits. Moreover, our framework optimizes\ntoken usage by deleting unrelated parts of the text and ensuring that the model\ngenerates output strictly based on the given aspect. With extensive experiments\non benchmark datasets, we demonstrate that our framework not only achieves\nsuperior performance but also effectively mitigates the token limitation\nproblem.\n", "link": "http://arxiv.org/abs/2504.13054v1", "date": "2025-04-17", "relevancy": 2.4714, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.496}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4934}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aspect-Based%20Summarization%20with%20Self-Aspect%20Retrieval%20Enhanced%0A%20%20Generation&body=Title%3A%20Aspect-Based%20Summarization%20with%20Self-Aspect%20Retrieval%20Enhanced%0A%20%20Generation%0AAuthor%3A%20Yichao%20Feng%20and%20Shuai%20Zhao%20and%20Yueqiu%20Li%20and%20Luwei%20Xiao%20and%20Xiaobao%20Wu%20and%20Anh%20Tuan%20Luu%0AAbstract%3A%20%20%20Aspect-based%20summarization%20aims%20to%20generate%20summaries%20tailored%20to%20specific%0Aaspects%2C%20addressing%20the%20resource%20constraints%20and%20limited%20generalizability%20of%0Atraditional%20summarization%20approaches.%20Recently%2C%20large%20language%20models%20have%0Ashown%20promise%20in%20this%20task%20without%20the%20need%20for%20training.%20However%2C%20they%20rely%0Aexcessively%20on%20prompt%20engineering%20and%20face%20token%20limits%20and%20hallucination%0Achallenges%2C%20especially%20with%20in-context%20learning.%20To%20address%20these%20challenges%2C%0Ain%20this%20paper%2C%20we%20propose%20a%20novel%20framework%20for%20aspect-based%20summarization%3A%0ASelf-Aspect%20Retrieval%20Enhanced%20Summary%20Generation.%20Rather%20than%20relying%20solely%0Aon%20in-context%20learning%2C%20given%20an%20aspect%2C%20we%20employ%20an%20embedding-driven%0Aretrieval%20mechanism%20to%20identify%20its%20relevant%20text%20segments.%20This%20approach%0Aextracts%20the%20pertinent%20content%20while%20avoiding%20unnecessary%20details%2C%20thereby%0Amitigating%20the%20challenge%20of%20token%20limits.%20Moreover%2C%20our%20framework%20optimizes%0Atoken%20usage%20by%20deleting%20unrelated%20parts%20of%20the%20text%20and%20ensuring%20that%20the%20model%0Agenerates%20output%20strictly%20based%20on%20the%20given%20aspect.%20With%20extensive%20experiments%0Aon%20benchmark%20datasets%2C%20we%20demonstrate%20that%20our%20framework%20not%20only%20achieves%0Asuperior%20performance%20but%20also%20effectively%20mitigates%20the%20token%20limitation%0Aproblem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13054v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAspect-Based%2520Summarization%2520with%2520Self-Aspect%2520Retrieval%2520Enhanced%250A%2520%2520Generation%26entry.906535625%3DYichao%2520Feng%2520and%2520Shuai%2520Zhao%2520and%2520Yueqiu%2520Li%2520and%2520Luwei%2520Xiao%2520and%2520Xiaobao%2520Wu%2520and%2520Anh%2520Tuan%2520Luu%26entry.1292438233%3D%2520%2520Aspect-based%2520summarization%2520aims%2520to%2520generate%2520summaries%2520tailored%2520to%2520specific%250Aaspects%252C%2520addressing%2520the%2520resource%2520constraints%2520and%2520limited%2520generalizability%2520of%250Atraditional%2520summarization%2520approaches.%2520Recently%252C%2520large%2520language%2520models%2520have%250Ashown%2520promise%2520in%2520this%2520task%2520without%2520the%2520need%2520for%2520training.%2520However%252C%2520they%2520rely%250Aexcessively%2520on%2520prompt%2520engineering%2520and%2520face%2520token%2520limits%2520and%2520hallucination%250Achallenges%252C%2520especially%2520with%2520in-context%2520learning.%2520To%2520address%2520these%2520challenges%252C%250Ain%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520framework%2520for%2520aspect-based%2520summarization%253A%250ASelf-Aspect%2520Retrieval%2520Enhanced%2520Summary%2520Generation.%2520Rather%2520than%2520relying%2520solely%250Aon%2520in-context%2520learning%252C%2520given%2520an%2520aspect%252C%2520we%2520employ%2520an%2520embedding-driven%250Aretrieval%2520mechanism%2520to%2520identify%2520its%2520relevant%2520text%2520segments.%2520This%2520approach%250Aextracts%2520the%2520pertinent%2520content%2520while%2520avoiding%2520unnecessary%2520details%252C%2520thereby%250Amitigating%2520the%2520challenge%2520of%2520token%2520limits.%2520Moreover%252C%2520our%2520framework%2520optimizes%250Atoken%2520usage%2520by%2520deleting%2520unrelated%2520parts%2520of%2520the%2520text%2520and%2520ensuring%2520that%2520the%2520model%250Agenerates%2520output%2520strictly%2520based%2520on%2520the%2520given%2520aspect.%2520With%2520extensive%2520experiments%250Aon%2520benchmark%2520datasets%252C%2520we%2520demonstrate%2520that%2520our%2520framework%2520not%2520only%2520achieves%250Asuperior%2520performance%2520but%2520also%2520effectively%2520mitigates%2520the%2520token%2520limitation%250Aproblem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13054v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aspect-Based%20Summarization%20with%20Self-Aspect%20Retrieval%20Enhanced%0A%20%20Generation&entry.906535625=Yichao%20Feng%20and%20Shuai%20Zhao%20and%20Yueqiu%20Li%20and%20Luwei%20Xiao%20and%20Xiaobao%20Wu%20and%20Anh%20Tuan%20Luu&entry.1292438233=%20%20Aspect-based%20summarization%20aims%20to%20generate%20summaries%20tailored%20to%20specific%0Aaspects%2C%20addressing%20the%20resource%20constraints%20and%20limited%20generalizability%20of%0Atraditional%20summarization%20approaches.%20Recently%2C%20large%20language%20models%20have%0Ashown%20promise%20in%20this%20task%20without%20the%20need%20for%20training.%20However%2C%20they%20rely%0Aexcessively%20on%20prompt%20engineering%20and%20face%20token%20limits%20and%20hallucination%0Achallenges%2C%20especially%20with%20in-context%20learning.%20To%20address%20these%20challenges%2C%0Ain%20this%20paper%2C%20we%20propose%20a%20novel%20framework%20for%20aspect-based%20summarization%3A%0ASelf-Aspect%20Retrieval%20Enhanced%20Summary%20Generation.%20Rather%20than%20relying%20solely%0Aon%20in-context%20learning%2C%20given%20an%20aspect%2C%20we%20employ%20an%20embedding-driven%0Aretrieval%20mechanism%20to%20identify%20its%20relevant%20text%20segments.%20This%20approach%0Aextracts%20the%20pertinent%20content%20while%20avoiding%20unnecessary%20details%2C%20thereby%0Amitigating%20the%20challenge%20of%20token%20limits.%20Moreover%2C%20our%20framework%20optimizes%0Atoken%20usage%20by%20deleting%20unrelated%20parts%20of%20the%20text%20and%20ensuring%20that%20the%20model%0Agenerates%20output%20strictly%20based%20on%20the%20given%20aspect.%20With%20extensive%20experiments%0Aon%20benchmark%20datasets%2C%20we%20demonstrate%20that%20our%20framework%20not%20only%20achieves%0Asuperior%20performance%20but%20also%20effectively%20mitigates%20the%20token%20limitation%0Aproblem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13054v1&entry.124074799=Read"},
{"title": "ValueCompass: A Framework for Measuring Contextual Value Alignment\n  Between Human and LLMs", "author": "Hua Shen and Tiffany Knearem and Reshmi Ghosh and Yu-Ju Yang and Nicholas Clark and Tanushree Mitra and Yun Huang", "abstract": "  As AI systems become more advanced, ensuring their alignment with a diverse\nrange of individuals and societal values becomes increasingly critical. But how\ncan we capture fundamental human values and assess the degree to which AI\nsystems align with them? We introduce ValueCompass, a framework of fundamental\nvalues, grounded in psychological theory and a systematic review, to identify\nand evaluate human-AI alignment. We apply ValueCompass to measure the value\nalignment of humans and large language models (LLMs) across four real-world\nscenarios: collaborative writing, education, public sectors, and healthcare.\nOur findings reveal concerning misalignments between humans and LLMs, such as\nhumans frequently endorse values like \"National Security\" which were largely\nrejected by LLMs. We also observe that values differ across scenarios,\nhighlighting the need for context-aware AI alignment strategies. This work\nprovides valuable insights into the design space of human-AI alignment, laying\nthe foundations for developing AI systems that responsibly reflect societal\nvalues and ethics.\n", "link": "http://arxiv.org/abs/2409.09586v2", "date": "2025-04-16", "relevancy": 2.4687, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5101}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5101}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ValueCompass%3A%20A%20Framework%20for%20Measuring%20Contextual%20Value%20Alignment%0A%20%20Between%20Human%20and%20LLMs&body=Title%3A%20ValueCompass%3A%20A%20Framework%20for%20Measuring%20Contextual%20Value%20Alignment%0A%20%20Between%20Human%20and%20LLMs%0AAuthor%3A%20Hua%20Shen%20and%20Tiffany%20Knearem%20and%20Reshmi%20Ghosh%20and%20Yu-Ju%20Yang%20and%20Nicholas%20Clark%20and%20Tanushree%20Mitra%20and%20Yun%20Huang%0AAbstract%3A%20%20%20As%20AI%20systems%20become%20more%20advanced%2C%20ensuring%20their%20alignment%20with%20a%20diverse%0Arange%20of%20individuals%20and%20societal%20values%20becomes%20increasingly%20critical.%20But%20how%0Acan%20we%20capture%20fundamental%20human%20values%20and%20assess%20the%20degree%20to%20which%20AI%0Asystems%20align%20with%20them%3F%20We%20introduce%20ValueCompass%2C%20a%20framework%20of%20fundamental%0Avalues%2C%20grounded%20in%20psychological%20theory%20and%20a%20systematic%20review%2C%20to%20identify%0Aand%20evaluate%20human-AI%20alignment.%20We%20apply%20ValueCompass%20to%20measure%20the%20value%0Aalignment%20of%20humans%20and%20large%20language%20models%20%28LLMs%29%20across%20four%20real-world%0Ascenarios%3A%20collaborative%20writing%2C%20education%2C%20public%20sectors%2C%20and%20healthcare.%0AOur%20findings%20reveal%20concerning%20misalignments%20between%20humans%20and%20LLMs%2C%20such%20as%0Ahumans%20frequently%20endorse%20values%20like%20%22National%20Security%22%20which%20were%20largely%0Arejected%20by%20LLMs.%20We%20also%20observe%20that%20values%20differ%20across%20scenarios%2C%0Ahighlighting%20the%20need%20for%20context-aware%20AI%20alignment%20strategies.%20This%20work%0Aprovides%20valuable%20insights%20into%20the%20design%20space%20of%20human-AI%20alignment%2C%20laying%0Athe%20foundations%20for%20developing%20AI%20systems%20that%20responsibly%20reflect%20societal%0Avalues%20and%20ethics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09586v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DValueCompass%253A%2520A%2520Framework%2520for%2520Measuring%2520Contextual%2520Value%2520Alignment%250A%2520%2520Between%2520Human%2520and%2520LLMs%26entry.906535625%3DHua%2520Shen%2520and%2520Tiffany%2520Knearem%2520and%2520Reshmi%2520Ghosh%2520and%2520Yu-Ju%2520Yang%2520and%2520Nicholas%2520Clark%2520and%2520Tanushree%2520Mitra%2520and%2520Yun%2520Huang%26entry.1292438233%3D%2520%2520As%2520AI%2520systems%2520become%2520more%2520advanced%252C%2520ensuring%2520their%2520alignment%2520with%2520a%2520diverse%250Arange%2520of%2520individuals%2520and%2520societal%2520values%2520becomes%2520increasingly%2520critical.%2520But%2520how%250Acan%2520we%2520capture%2520fundamental%2520human%2520values%2520and%2520assess%2520the%2520degree%2520to%2520which%2520AI%250Asystems%2520align%2520with%2520them%253F%2520We%2520introduce%2520ValueCompass%252C%2520a%2520framework%2520of%2520fundamental%250Avalues%252C%2520grounded%2520in%2520psychological%2520theory%2520and%2520a%2520systematic%2520review%252C%2520to%2520identify%250Aand%2520evaluate%2520human-AI%2520alignment.%2520We%2520apply%2520ValueCompass%2520to%2520measure%2520the%2520value%250Aalignment%2520of%2520humans%2520and%2520large%2520language%2520models%2520%2528LLMs%2529%2520across%2520four%2520real-world%250Ascenarios%253A%2520collaborative%2520writing%252C%2520education%252C%2520public%2520sectors%252C%2520and%2520healthcare.%250AOur%2520findings%2520reveal%2520concerning%2520misalignments%2520between%2520humans%2520and%2520LLMs%252C%2520such%2520as%250Ahumans%2520frequently%2520endorse%2520values%2520like%2520%2522National%2520Security%2522%2520which%2520were%2520largely%250Arejected%2520by%2520LLMs.%2520We%2520also%2520observe%2520that%2520values%2520differ%2520across%2520scenarios%252C%250Ahighlighting%2520the%2520need%2520for%2520context-aware%2520AI%2520alignment%2520strategies.%2520This%2520work%250Aprovides%2520valuable%2520insights%2520into%2520the%2520design%2520space%2520of%2520human-AI%2520alignment%252C%2520laying%250Athe%2520foundations%2520for%2520developing%2520AI%2520systems%2520that%2520responsibly%2520reflect%2520societal%250Avalues%2520and%2520ethics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09586v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ValueCompass%3A%20A%20Framework%20for%20Measuring%20Contextual%20Value%20Alignment%0A%20%20Between%20Human%20and%20LLMs&entry.906535625=Hua%20Shen%20and%20Tiffany%20Knearem%20and%20Reshmi%20Ghosh%20and%20Yu-Ju%20Yang%20and%20Nicholas%20Clark%20and%20Tanushree%20Mitra%20and%20Yun%20Huang&entry.1292438233=%20%20As%20AI%20systems%20become%20more%20advanced%2C%20ensuring%20their%20alignment%20with%20a%20diverse%0Arange%20of%20individuals%20and%20societal%20values%20becomes%20increasingly%20critical.%20But%20how%0Acan%20we%20capture%20fundamental%20human%20values%20and%20assess%20the%20degree%20to%20which%20AI%0Asystems%20align%20with%20them%3F%20We%20introduce%20ValueCompass%2C%20a%20framework%20of%20fundamental%0Avalues%2C%20grounded%20in%20psychological%20theory%20and%20a%20systematic%20review%2C%20to%20identify%0Aand%20evaluate%20human-AI%20alignment.%20We%20apply%20ValueCompass%20to%20measure%20the%20value%0Aalignment%20of%20humans%20and%20large%20language%20models%20%28LLMs%29%20across%20four%20real-world%0Ascenarios%3A%20collaborative%20writing%2C%20education%2C%20public%20sectors%2C%20and%20healthcare.%0AOur%20findings%20reveal%20concerning%20misalignments%20between%20humans%20and%20LLMs%2C%20such%20as%0Ahumans%20frequently%20endorse%20values%20like%20%22National%20Security%22%20which%20were%20largely%0Arejected%20by%20LLMs.%20We%20also%20observe%20that%20values%20differ%20across%20scenarios%2C%0Ahighlighting%20the%20need%20for%20context-aware%20AI%20alignment%20strategies.%20This%20work%0Aprovides%20valuable%20insights%20into%20the%20design%20space%20of%20human-AI%20alignment%2C%20laying%0Athe%20foundations%20for%20developing%20AI%20systems%20that%20responsibly%20reflect%20societal%0Avalues%20and%20ethics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09586v2&entry.124074799=Read"},
{"title": "MobilePoser: Real-Time Full-Body Pose Estimation and 3D Human\n  Translation from IMUs in Mobile Consumer Devices", "author": "Vasco Xu and Chenfeng Gao and Henry Hoffmann and Karan Ahuja", "abstract": "  There has been a continued trend towards minimizing instrumentation for\nfull-body motion capture, going from specialized rooms and equipment, to arrays\nof worn sensors and recently sparse inertial pose capture methods. However, as\nthese techniques migrate towards lower-fidelity IMUs on ubiquitous commodity\ndevices, like phones, watches, and earbuds, challenges arise including\ncompromised online performance, temporal consistency, and loss of global\ntranslation due to sensor noise and drift. Addressing these challenges, we\nintroduce MobilePoser, a real-time system for full-body pose and global\ntranslation estimation using any available subset of IMUs already present in\nthese consumer devices. MobilePoser employs a multi-stage deep neural network\nfor kinematic pose estimation followed by a physics-based motion optimizer,\nachieving state-of-the-art accuracy while remaining lightweight. We conclude\nwith a series of demonstrative applications to illustrate the unique potential\nof MobilePoser across a variety of fields, such as health and wellness, gaming,\nand indoor navigation to name a few.\n", "link": "http://arxiv.org/abs/2504.12492v1", "date": "2025-04-16", "relevancy": 2.4624, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6599}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5852}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MobilePoser%3A%20Real-Time%20Full-Body%20Pose%20Estimation%20and%203D%20Human%0A%20%20Translation%20from%20IMUs%20in%20Mobile%20Consumer%20Devices&body=Title%3A%20MobilePoser%3A%20Real-Time%20Full-Body%20Pose%20Estimation%20and%203D%20Human%0A%20%20Translation%20from%20IMUs%20in%20Mobile%20Consumer%20Devices%0AAuthor%3A%20Vasco%20Xu%20and%20Chenfeng%20Gao%20and%20Henry%20Hoffmann%20and%20Karan%20Ahuja%0AAbstract%3A%20%20%20There%20has%20been%20a%20continued%20trend%20towards%20minimizing%20instrumentation%20for%0Afull-body%20motion%20capture%2C%20going%20from%20specialized%20rooms%20and%20equipment%2C%20to%20arrays%0Aof%20worn%20sensors%20and%20recently%20sparse%20inertial%20pose%20capture%20methods.%20However%2C%20as%0Athese%20techniques%20migrate%20towards%20lower-fidelity%20IMUs%20on%20ubiquitous%20commodity%0Adevices%2C%20like%20phones%2C%20watches%2C%20and%20earbuds%2C%20challenges%20arise%20including%0Acompromised%20online%20performance%2C%20temporal%20consistency%2C%20and%20loss%20of%20global%0Atranslation%20due%20to%20sensor%20noise%20and%20drift.%20Addressing%20these%20challenges%2C%20we%0Aintroduce%20MobilePoser%2C%20a%20real-time%20system%20for%20full-body%20pose%20and%20global%0Atranslation%20estimation%20using%20any%20available%20subset%20of%20IMUs%20already%20present%20in%0Athese%20consumer%20devices.%20MobilePoser%20employs%20a%20multi-stage%20deep%20neural%20network%0Afor%20kinematic%20pose%20estimation%20followed%20by%20a%20physics-based%20motion%20optimizer%2C%0Aachieving%20state-of-the-art%20accuracy%20while%20remaining%20lightweight.%20We%20conclude%0Awith%20a%20series%20of%20demonstrative%20applications%20to%20illustrate%20the%20unique%20potential%0Aof%20MobilePoser%20across%20a%20variety%20of%20fields%2C%20such%20as%20health%20and%20wellness%2C%20gaming%2C%0Aand%20indoor%20navigation%20to%20name%20a%20few.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12492v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMobilePoser%253A%2520Real-Time%2520Full-Body%2520Pose%2520Estimation%2520and%25203D%2520Human%250A%2520%2520Translation%2520from%2520IMUs%2520in%2520Mobile%2520Consumer%2520Devices%26entry.906535625%3DVasco%2520Xu%2520and%2520Chenfeng%2520Gao%2520and%2520Henry%2520Hoffmann%2520and%2520Karan%2520Ahuja%26entry.1292438233%3D%2520%2520There%2520has%2520been%2520a%2520continued%2520trend%2520towards%2520minimizing%2520instrumentation%2520for%250Afull-body%2520motion%2520capture%252C%2520going%2520from%2520specialized%2520rooms%2520and%2520equipment%252C%2520to%2520arrays%250Aof%2520worn%2520sensors%2520and%2520recently%2520sparse%2520inertial%2520pose%2520capture%2520methods.%2520However%252C%2520as%250Athese%2520techniques%2520migrate%2520towards%2520lower-fidelity%2520IMUs%2520on%2520ubiquitous%2520commodity%250Adevices%252C%2520like%2520phones%252C%2520watches%252C%2520and%2520earbuds%252C%2520challenges%2520arise%2520including%250Acompromised%2520online%2520performance%252C%2520temporal%2520consistency%252C%2520and%2520loss%2520of%2520global%250Atranslation%2520due%2520to%2520sensor%2520noise%2520and%2520drift.%2520Addressing%2520these%2520challenges%252C%2520we%250Aintroduce%2520MobilePoser%252C%2520a%2520real-time%2520system%2520for%2520full-body%2520pose%2520and%2520global%250Atranslation%2520estimation%2520using%2520any%2520available%2520subset%2520of%2520IMUs%2520already%2520present%2520in%250Athese%2520consumer%2520devices.%2520MobilePoser%2520employs%2520a%2520multi-stage%2520deep%2520neural%2520network%250Afor%2520kinematic%2520pose%2520estimation%2520followed%2520by%2520a%2520physics-based%2520motion%2520optimizer%252C%250Aachieving%2520state-of-the-art%2520accuracy%2520while%2520remaining%2520lightweight.%2520We%2520conclude%250Awith%2520a%2520series%2520of%2520demonstrative%2520applications%2520to%2520illustrate%2520the%2520unique%2520potential%250Aof%2520MobilePoser%2520across%2520a%2520variety%2520of%2520fields%252C%2520such%2520as%2520health%2520and%2520wellness%252C%2520gaming%252C%250Aand%2520indoor%2520navigation%2520to%2520name%2520a%2520few.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12492v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MobilePoser%3A%20Real-Time%20Full-Body%20Pose%20Estimation%20and%203D%20Human%0A%20%20Translation%20from%20IMUs%20in%20Mobile%20Consumer%20Devices&entry.906535625=Vasco%20Xu%20and%20Chenfeng%20Gao%20and%20Henry%20Hoffmann%20and%20Karan%20Ahuja&entry.1292438233=%20%20There%20has%20been%20a%20continued%20trend%20towards%20minimizing%20instrumentation%20for%0Afull-body%20motion%20capture%2C%20going%20from%20specialized%20rooms%20and%20equipment%2C%20to%20arrays%0Aof%20worn%20sensors%20and%20recently%20sparse%20inertial%20pose%20capture%20methods.%20However%2C%20as%0Athese%20techniques%20migrate%20towards%20lower-fidelity%20IMUs%20on%20ubiquitous%20commodity%0Adevices%2C%20like%20phones%2C%20watches%2C%20and%20earbuds%2C%20challenges%20arise%20including%0Acompromised%20online%20performance%2C%20temporal%20consistency%2C%20and%20loss%20of%20global%0Atranslation%20due%20to%20sensor%20noise%20and%20drift.%20Addressing%20these%20challenges%2C%20we%0Aintroduce%20MobilePoser%2C%20a%20real-time%20system%20for%20full-body%20pose%20and%20global%0Atranslation%20estimation%20using%20any%20available%20subset%20of%20IMUs%20already%20present%20in%0Athese%20consumer%20devices.%20MobilePoser%20employs%20a%20multi-stage%20deep%20neural%20network%0Afor%20kinematic%20pose%20estimation%20followed%20by%20a%20physics-based%20motion%20optimizer%2C%0Aachieving%20state-of-the-art%20accuracy%20while%20remaining%20lightweight.%20We%20conclude%0Awith%20a%20series%20of%20demonstrative%20applications%20to%20illustrate%20the%20unique%20potential%0Aof%20MobilePoser%20across%20a%20variety%20of%20fields%2C%20such%20as%20health%20and%20wellness%2C%20gaming%2C%0Aand%20indoor%20navigation%20to%20name%20a%20few.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12492v1&entry.124074799=Read"},
{"title": "Control the GNN: Utilizing Neural Controller with Lyapunov Stability for\n  Test-Time Feature Reconstruction", "author": "Jielong Yang and Rui Ding and Feng Ji and Hongbin Wang and Linbo Xie", "abstract": "  The performance of graph neural networks (GNNs) is susceptible to\ndiscrepancies between training and testing sample distributions. Prior studies\nhave attempted to mitigating the impact of distribution shift by reconstructing\nnode features during the testing phase without modifying the model parameters.\nHowever, these approaches lack theoretical analysis of the proximity between\npredictions and ground truth at test time. In this paper, we propose a novel\nnode feature reconstruction method grounded in Lyapunov stability theory.\nSpecifically, we model the GNN as a control system during the testing phase,\nconsidering node features as control variables. A neural controller that\nadheres to the Lyapunov stability criterion is then employed to reconstruct\nthese node features, ensuring that the predictions progressively approach the\nground truth at test time. We validate the effectiveness of our approach\nthrough extensive experiments across multiple datasets, demonstrating\nsignificant performance improvements.\n", "link": "http://arxiv.org/abs/2410.09708v2", "date": "2025-04-17", "relevancy": 2.4588, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5042}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4887}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Control%20the%20GNN%3A%20Utilizing%20Neural%20Controller%20with%20Lyapunov%20Stability%20for%0A%20%20Test-Time%20Feature%20Reconstruction&body=Title%3A%20Control%20the%20GNN%3A%20Utilizing%20Neural%20Controller%20with%20Lyapunov%20Stability%20for%0A%20%20Test-Time%20Feature%20Reconstruction%0AAuthor%3A%20Jielong%20Yang%20and%20Rui%20Ding%20and%20Feng%20Ji%20and%20Hongbin%20Wang%20and%20Linbo%20Xie%0AAbstract%3A%20%20%20The%20performance%20of%20graph%20neural%20networks%20%28GNNs%29%20is%20susceptible%20to%0Adiscrepancies%20between%20training%20and%20testing%20sample%20distributions.%20Prior%20studies%0Ahave%20attempted%20to%20mitigating%20the%20impact%20of%20distribution%20shift%20by%20reconstructing%0Anode%20features%20during%20the%20testing%20phase%20without%20modifying%20the%20model%20parameters.%0AHowever%2C%20these%20approaches%20lack%20theoretical%20analysis%20of%20the%20proximity%20between%0Apredictions%20and%20ground%20truth%20at%20test%20time.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Anode%20feature%20reconstruction%20method%20grounded%20in%20Lyapunov%20stability%20theory.%0ASpecifically%2C%20we%20model%20the%20GNN%20as%20a%20control%20system%20during%20the%20testing%20phase%2C%0Aconsidering%20node%20features%20as%20control%20variables.%20A%20neural%20controller%20that%0Aadheres%20to%20the%20Lyapunov%20stability%20criterion%20is%20then%20employed%20to%20reconstruct%0Athese%20node%20features%2C%20ensuring%20that%20the%20predictions%20progressively%20approach%20the%0Aground%20truth%20at%20test%20time.%20We%20validate%20the%20effectiveness%20of%20our%20approach%0Athrough%20extensive%20experiments%20across%20multiple%20datasets%2C%20demonstrating%0Asignificant%20performance%20improvements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.09708v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControl%2520the%2520GNN%253A%2520Utilizing%2520Neural%2520Controller%2520with%2520Lyapunov%2520Stability%2520for%250A%2520%2520Test-Time%2520Feature%2520Reconstruction%26entry.906535625%3DJielong%2520Yang%2520and%2520Rui%2520Ding%2520and%2520Feng%2520Ji%2520and%2520Hongbin%2520Wang%2520and%2520Linbo%2520Xie%26entry.1292438233%3D%2520%2520The%2520performance%2520of%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520is%2520susceptible%2520to%250Adiscrepancies%2520between%2520training%2520and%2520testing%2520sample%2520distributions.%2520Prior%2520studies%250Ahave%2520attempted%2520to%2520mitigating%2520the%2520impact%2520of%2520distribution%2520shift%2520by%2520reconstructing%250Anode%2520features%2520during%2520the%2520testing%2520phase%2520without%2520modifying%2520the%2520model%2520parameters.%250AHowever%252C%2520these%2520approaches%2520lack%2520theoretical%2520analysis%2520of%2520the%2520proximity%2520between%250Apredictions%2520and%2520ground%2520truth%2520at%2520test%2520time.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250Anode%2520feature%2520reconstruction%2520method%2520grounded%2520in%2520Lyapunov%2520stability%2520theory.%250ASpecifically%252C%2520we%2520model%2520the%2520GNN%2520as%2520a%2520control%2520system%2520during%2520the%2520testing%2520phase%252C%250Aconsidering%2520node%2520features%2520as%2520control%2520variables.%2520A%2520neural%2520controller%2520that%250Aadheres%2520to%2520the%2520Lyapunov%2520stability%2520criterion%2520is%2520then%2520employed%2520to%2520reconstruct%250Athese%2520node%2520features%252C%2520ensuring%2520that%2520the%2520predictions%2520progressively%2520approach%2520the%250Aground%2520truth%2520at%2520test%2520time.%2520We%2520validate%2520the%2520effectiveness%2520of%2520our%2520approach%250Athrough%2520extensive%2520experiments%2520across%2520multiple%2520datasets%252C%2520demonstrating%250Asignificant%2520performance%2520improvements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.09708v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Control%20the%20GNN%3A%20Utilizing%20Neural%20Controller%20with%20Lyapunov%20Stability%20for%0A%20%20Test-Time%20Feature%20Reconstruction&entry.906535625=Jielong%20Yang%20and%20Rui%20Ding%20and%20Feng%20Ji%20and%20Hongbin%20Wang%20and%20Linbo%20Xie&entry.1292438233=%20%20The%20performance%20of%20graph%20neural%20networks%20%28GNNs%29%20is%20susceptible%20to%0Adiscrepancies%20between%20training%20and%20testing%20sample%20distributions.%20Prior%20studies%0Ahave%20attempted%20to%20mitigating%20the%20impact%20of%20distribution%20shift%20by%20reconstructing%0Anode%20features%20during%20the%20testing%20phase%20without%20modifying%20the%20model%20parameters.%0AHowever%2C%20these%20approaches%20lack%20theoretical%20analysis%20of%20the%20proximity%20between%0Apredictions%20and%20ground%20truth%20at%20test%20time.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Anode%20feature%20reconstruction%20method%20grounded%20in%20Lyapunov%20stability%20theory.%0ASpecifically%2C%20we%20model%20the%20GNN%20as%20a%20control%20system%20during%20the%20testing%20phase%2C%0Aconsidering%20node%20features%20as%20control%20variables.%20A%20neural%20controller%20that%0Aadheres%20to%20the%20Lyapunov%20stability%20criterion%20is%20then%20employed%20to%20reconstruct%0Athese%20node%20features%2C%20ensuring%20that%20the%20predictions%20progressively%20approach%20the%0Aground%20truth%20at%20test%20time.%20We%20validate%20the%20effectiveness%20of%20our%20approach%0Athrough%20extensive%20experiments%20across%20multiple%20datasets%2C%20demonstrating%0Asignificant%20performance%20improvements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.09708v2&entry.124074799=Read"},
{"title": "VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference\n  Optimization for Large Video Models", "author": "Haojian Huang and Haodong Chen and Shengqiong Wu and Meng Luo and Jinlan Fu and Xinya Du and Hanwang Zhang and Hao Fei", "abstract": "  Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown\npromise in video understanding but often suffer from misalignment with human\nintuition and video hallucination issues. To address these challenges, we\nintroduce VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal\nDirect Preference Optimization. VistaDPO enhances text-video preference\nalignment across three hierarchical levels: i) Instance Level, aligning overall\nvideo content with responses; ii) Temporal Level, aligning video temporal\nsemantics with event descriptions; and iii) Perceptive Level, aligning spatial\nobjects with language tokens. Given the lack of datasets for fine-grained\nvideo-language preference alignment, we construct VistaDPO-7k, a dataset of\n7.2K QA pairs annotated with chosen and rejected responses, along with\nspatial-temporal grounding information such as timestamps, keyframes, and\nbounding boxes. Extensive experiments on benchmarks such as Video\nHallucination, Video QA, and Captioning performance tasks demonstrate that\nVistaDPO significantly improves the performance of existing LVMs, effectively\nmitigating video-language misalignment and hallucination. The code and data are\navailable at https://github.com/HaroldChen19/VistaDPO.\n", "link": "http://arxiv.org/abs/2504.13122v1", "date": "2025-04-17", "relevancy": 2.4574, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6206}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6206}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VistaDPO%3A%20Video%20Hierarchical%20Spatial-Temporal%20Direct%20Preference%0A%20%20Optimization%20for%20Large%20Video%20Models&body=Title%3A%20VistaDPO%3A%20Video%20Hierarchical%20Spatial-Temporal%20Direct%20Preference%0A%20%20Optimization%20for%20Large%20Video%20Models%0AAuthor%3A%20Haojian%20Huang%20and%20Haodong%20Chen%20and%20Shengqiong%20Wu%20and%20Meng%20Luo%20and%20Jinlan%20Fu%20and%20Xinya%20Du%20and%20Hanwang%20Zhang%20and%20Hao%20Fei%0AAbstract%3A%20%20%20Large%20Video%20Models%20%28LVMs%29%20built%20upon%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%0Apromise%20in%20video%20understanding%20but%20often%20suffer%20from%20misalignment%20with%20human%0Aintuition%20and%20video%20hallucination%20issues.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20VistaDPO%2C%20a%20novel%20framework%20for%20Video%20Hierarchical%20Spatial-Temporal%0ADirect%20Preference%20Optimization.%20VistaDPO%20enhances%20text-video%20preference%0Aalignment%20across%20three%20hierarchical%20levels%3A%20i%29%20Instance%20Level%2C%20aligning%20overall%0Avideo%20content%20with%20responses%3B%20ii%29%20Temporal%20Level%2C%20aligning%20video%20temporal%0Asemantics%20with%20event%20descriptions%3B%20and%20iii%29%20Perceptive%20Level%2C%20aligning%20spatial%0Aobjects%20with%20language%20tokens.%20Given%20the%20lack%20of%20datasets%20for%20fine-grained%0Avideo-language%20preference%20alignment%2C%20we%20construct%20VistaDPO-7k%2C%20a%20dataset%20of%0A7.2K%20QA%20pairs%20annotated%20with%20chosen%20and%20rejected%20responses%2C%20along%20with%0Aspatial-temporal%20grounding%20information%20such%20as%20timestamps%2C%20keyframes%2C%20and%0Abounding%20boxes.%20Extensive%20experiments%20on%20benchmarks%20such%20as%20Video%0AHallucination%2C%20Video%20QA%2C%20and%20Captioning%20performance%20tasks%20demonstrate%20that%0AVistaDPO%20significantly%20improves%20the%20performance%20of%20existing%20LVMs%2C%20effectively%0Amitigating%20video-language%20misalignment%20and%20hallucination.%20The%20code%20and%20data%20are%0Aavailable%20at%20https%3A//github.com/HaroldChen19/VistaDPO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13122v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVistaDPO%253A%2520Video%2520Hierarchical%2520Spatial-Temporal%2520Direct%2520Preference%250A%2520%2520Optimization%2520for%2520Large%2520Video%2520Models%26entry.906535625%3DHaojian%2520Huang%2520and%2520Haodong%2520Chen%2520and%2520Shengqiong%2520Wu%2520and%2520Meng%2520Luo%2520and%2520Jinlan%2520Fu%2520and%2520Xinya%2520Du%2520and%2520Hanwang%2520Zhang%2520and%2520Hao%2520Fei%26entry.1292438233%3D%2520%2520Large%2520Video%2520Models%2520%2528LVMs%2529%2520built%2520upon%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%250Apromise%2520in%2520video%2520understanding%2520but%2520often%2520suffer%2520from%2520misalignment%2520with%2520human%250Aintuition%2520and%2520video%2520hallucination%2520issues.%2520To%2520address%2520these%2520challenges%252C%2520we%250Aintroduce%2520VistaDPO%252C%2520a%2520novel%2520framework%2520for%2520Video%2520Hierarchical%2520Spatial-Temporal%250ADirect%2520Preference%2520Optimization.%2520VistaDPO%2520enhances%2520text-video%2520preference%250Aalignment%2520across%2520three%2520hierarchical%2520levels%253A%2520i%2529%2520Instance%2520Level%252C%2520aligning%2520overall%250Avideo%2520content%2520with%2520responses%253B%2520ii%2529%2520Temporal%2520Level%252C%2520aligning%2520video%2520temporal%250Asemantics%2520with%2520event%2520descriptions%253B%2520and%2520iii%2529%2520Perceptive%2520Level%252C%2520aligning%2520spatial%250Aobjects%2520with%2520language%2520tokens.%2520Given%2520the%2520lack%2520of%2520datasets%2520for%2520fine-grained%250Avideo-language%2520preference%2520alignment%252C%2520we%2520construct%2520VistaDPO-7k%252C%2520a%2520dataset%2520of%250A7.2K%2520QA%2520pairs%2520annotated%2520with%2520chosen%2520and%2520rejected%2520responses%252C%2520along%2520with%250Aspatial-temporal%2520grounding%2520information%2520such%2520as%2520timestamps%252C%2520keyframes%252C%2520and%250Abounding%2520boxes.%2520Extensive%2520experiments%2520on%2520benchmarks%2520such%2520as%2520Video%250AHallucination%252C%2520Video%2520QA%252C%2520and%2520Captioning%2520performance%2520tasks%2520demonstrate%2520that%250AVistaDPO%2520significantly%2520improves%2520the%2520performance%2520of%2520existing%2520LVMs%252C%2520effectively%250Amitigating%2520video-language%2520misalignment%2520and%2520hallucination.%2520The%2520code%2520and%2520data%2520are%250Aavailable%2520at%2520https%253A//github.com/HaroldChen19/VistaDPO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13122v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VistaDPO%3A%20Video%20Hierarchical%20Spatial-Temporal%20Direct%20Preference%0A%20%20Optimization%20for%20Large%20Video%20Models&entry.906535625=Haojian%20Huang%20and%20Haodong%20Chen%20and%20Shengqiong%20Wu%20and%20Meng%20Luo%20and%20Jinlan%20Fu%20and%20Xinya%20Du%20and%20Hanwang%20Zhang%20and%20Hao%20Fei&entry.1292438233=%20%20Large%20Video%20Models%20%28LVMs%29%20built%20upon%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%0Apromise%20in%20video%20understanding%20but%20often%20suffer%20from%20misalignment%20with%20human%0Aintuition%20and%20video%20hallucination%20issues.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20VistaDPO%2C%20a%20novel%20framework%20for%20Video%20Hierarchical%20Spatial-Temporal%0ADirect%20Preference%20Optimization.%20VistaDPO%20enhances%20text-video%20preference%0Aalignment%20across%20three%20hierarchical%20levels%3A%20i%29%20Instance%20Level%2C%20aligning%20overall%0Avideo%20content%20with%20responses%3B%20ii%29%20Temporal%20Level%2C%20aligning%20video%20temporal%0Asemantics%20with%20event%20descriptions%3B%20and%20iii%29%20Perceptive%20Level%2C%20aligning%20spatial%0Aobjects%20with%20language%20tokens.%20Given%20the%20lack%20of%20datasets%20for%20fine-grained%0Avideo-language%20preference%20alignment%2C%20we%20construct%20VistaDPO-7k%2C%20a%20dataset%20of%0A7.2K%20QA%20pairs%20annotated%20with%20chosen%20and%20rejected%20responses%2C%20along%20with%0Aspatial-temporal%20grounding%20information%20such%20as%20timestamps%2C%20keyframes%2C%20and%0Abounding%20boxes.%20Extensive%20experiments%20on%20benchmarks%20such%20as%20Video%0AHallucination%2C%20Video%20QA%2C%20and%20Captioning%20performance%20tasks%20demonstrate%20that%0AVistaDPO%20significantly%20improves%20the%20performance%20of%20existing%20LVMs%2C%20effectively%0Amitigating%20video-language%20misalignment%20and%20hallucination.%20The%20code%20and%20data%20are%0Aavailable%20at%20https%3A//github.com/HaroldChen19/VistaDPO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13122v1&entry.124074799=Read"},
{"title": "UniPhys: Unified Planner and Controller with Diffusion for Flexible\n  Physics-Based Character Control", "author": "Yan Wu and Korrawe Karunratanakul and Zhengyi Luo and Siyu Tang", "abstract": "  Generating natural and physically plausible character motion remains\nchallenging, particularly for long-horizon control with diverse guidance\nsignals. While prior work combines high-level diffusion-based motion planners\nwith low-level physics controllers, these systems suffer from domain gaps that\ndegrade motion quality and require task-specific fine-tuning. To tackle this\nproblem, we introduce UniPhys, a diffusion-based behavior cloning framework\nthat unifies motion planning and control into a single model. UniPhys enables\nflexible, expressive character motion conditioned on multi-modal inputs such as\ntext, trajectories, and goals. To address accumulated prediction errors over\nlong sequences, UniPhys is trained with the Diffusion Forcing paradigm,\nlearning to denoise noisy motion histories and handle discrepancies introduced\nby the physics simulator. This design allows UniPhys to robustly generate\nphysically plausible, long-horizon motions. Through guided sampling, UniPhys\ngeneralizes to a wide range of control signals, including unseen ones, without\nrequiring task-specific fine-tuning. Experiments show that UniPhys outperforms\nprior methods in motion naturalness, generalization, and robustness across\ndiverse control tasks.\n", "link": "http://arxiv.org/abs/2504.12540v1", "date": "2025-04-17", "relevancy": 2.4561, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6571}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5895}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5676}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniPhys%3A%20Unified%20Planner%20and%20Controller%20with%20Diffusion%20for%20Flexible%0A%20%20Physics-Based%20Character%20Control&body=Title%3A%20UniPhys%3A%20Unified%20Planner%20and%20Controller%20with%20Diffusion%20for%20Flexible%0A%20%20Physics-Based%20Character%20Control%0AAuthor%3A%20Yan%20Wu%20and%20Korrawe%20Karunratanakul%20and%20Zhengyi%20Luo%20and%20Siyu%20Tang%0AAbstract%3A%20%20%20Generating%20natural%20and%20physically%20plausible%20character%20motion%20remains%0Achallenging%2C%20particularly%20for%20long-horizon%20control%20with%20diverse%20guidance%0Asignals.%20While%20prior%20work%20combines%20high-level%20diffusion-based%20motion%20planners%0Awith%20low-level%20physics%20controllers%2C%20these%20systems%20suffer%20from%20domain%20gaps%20that%0Adegrade%20motion%20quality%20and%20require%20task-specific%20fine-tuning.%20To%20tackle%20this%0Aproblem%2C%20we%20introduce%20UniPhys%2C%20a%20diffusion-based%20behavior%20cloning%20framework%0Athat%20unifies%20motion%20planning%20and%20control%20into%20a%20single%20model.%20UniPhys%20enables%0Aflexible%2C%20expressive%20character%20motion%20conditioned%20on%20multi-modal%20inputs%20such%20as%0Atext%2C%20trajectories%2C%20and%20goals.%20To%20address%20accumulated%20prediction%20errors%20over%0Along%20sequences%2C%20UniPhys%20is%20trained%20with%20the%20Diffusion%20Forcing%20paradigm%2C%0Alearning%20to%20denoise%20noisy%20motion%20histories%20and%20handle%20discrepancies%20introduced%0Aby%20the%20physics%20simulator.%20This%20design%20allows%20UniPhys%20to%20robustly%20generate%0Aphysically%20plausible%2C%20long-horizon%20motions.%20Through%20guided%20sampling%2C%20UniPhys%0Ageneralizes%20to%20a%20wide%20range%20of%20control%20signals%2C%20including%20unseen%20ones%2C%20without%0Arequiring%20task-specific%20fine-tuning.%20Experiments%20show%20that%20UniPhys%20outperforms%0Aprior%20methods%20in%20motion%20naturalness%2C%20generalization%2C%20and%20robustness%20across%0Adiverse%20control%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12540v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniPhys%253A%2520Unified%2520Planner%2520and%2520Controller%2520with%2520Diffusion%2520for%2520Flexible%250A%2520%2520Physics-Based%2520Character%2520Control%26entry.906535625%3DYan%2520Wu%2520and%2520Korrawe%2520Karunratanakul%2520and%2520Zhengyi%2520Luo%2520and%2520Siyu%2520Tang%26entry.1292438233%3D%2520%2520Generating%2520natural%2520and%2520physically%2520plausible%2520character%2520motion%2520remains%250Achallenging%252C%2520particularly%2520for%2520long-horizon%2520control%2520with%2520diverse%2520guidance%250Asignals.%2520While%2520prior%2520work%2520combines%2520high-level%2520diffusion-based%2520motion%2520planners%250Awith%2520low-level%2520physics%2520controllers%252C%2520these%2520systems%2520suffer%2520from%2520domain%2520gaps%2520that%250Adegrade%2520motion%2520quality%2520and%2520require%2520task-specific%2520fine-tuning.%2520To%2520tackle%2520this%250Aproblem%252C%2520we%2520introduce%2520UniPhys%252C%2520a%2520diffusion-based%2520behavior%2520cloning%2520framework%250Athat%2520unifies%2520motion%2520planning%2520and%2520control%2520into%2520a%2520single%2520model.%2520UniPhys%2520enables%250Aflexible%252C%2520expressive%2520character%2520motion%2520conditioned%2520on%2520multi-modal%2520inputs%2520such%2520as%250Atext%252C%2520trajectories%252C%2520and%2520goals.%2520To%2520address%2520accumulated%2520prediction%2520errors%2520over%250Along%2520sequences%252C%2520UniPhys%2520is%2520trained%2520with%2520the%2520Diffusion%2520Forcing%2520paradigm%252C%250Alearning%2520to%2520denoise%2520noisy%2520motion%2520histories%2520and%2520handle%2520discrepancies%2520introduced%250Aby%2520the%2520physics%2520simulator.%2520This%2520design%2520allows%2520UniPhys%2520to%2520robustly%2520generate%250Aphysically%2520plausible%252C%2520long-horizon%2520motions.%2520Through%2520guided%2520sampling%252C%2520UniPhys%250Ageneralizes%2520to%2520a%2520wide%2520range%2520of%2520control%2520signals%252C%2520including%2520unseen%2520ones%252C%2520without%250Arequiring%2520task-specific%2520fine-tuning.%2520Experiments%2520show%2520that%2520UniPhys%2520outperforms%250Aprior%2520methods%2520in%2520motion%2520naturalness%252C%2520generalization%252C%2520and%2520robustness%2520across%250Adiverse%2520control%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12540v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniPhys%3A%20Unified%20Planner%20and%20Controller%20with%20Diffusion%20for%20Flexible%0A%20%20Physics-Based%20Character%20Control&entry.906535625=Yan%20Wu%20and%20Korrawe%20Karunratanakul%20and%20Zhengyi%20Luo%20and%20Siyu%20Tang&entry.1292438233=%20%20Generating%20natural%20and%20physically%20plausible%20character%20motion%20remains%0Achallenging%2C%20particularly%20for%20long-horizon%20control%20with%20diverse%20guidance%0Asignals.%20While%20prior%20work%20combines%20high-level%20diffusion-based%20motion%20planners%0Awith%20low-level%20physics%20controllers%2C%20these%20systems%20suffer%20from%20domain%20gaps%20that%0Adegrade%20motion%20quality%20and%20require%20task-specific%20fine-tuning.%20To%20tackle%20this%0Aproblem%2C%20we%20introduce%20UniPhys%2C%20a%20diffusion-based%20behavior%20cloning%20framework%0Athat%20unifies%20motion%20planning%20and%20control%20into%20a%20single%20model.%20UniPhys%20enables%0Aflexible%2C%20expressive%20character%20motion%20conditioned%20on%20multi-modal%20inputs%20such%20as%0Atext%2C%20trajectories%2C%20and%20goals.%20To%20address%20accumulated%20prediction%20errors%20over%0Along%20sequences%2C%20UniPhys%20is%20trained%20with%20the%20Diffusion%20Forcing%20paradigm%2C%0Alearning%20to%20denoise%20noisy%20motion%20histories%20and%20handle%20discrepancies%20introduced%0Aby%20the%20physics%20simulator.%20This%20design%20allows%20UniPhys%20to%20robustly%20generate%0Aphysically%20plausible%2C%20long-horizon%20motions.%20Through%20guided%20sampling%2C%20UniPhys%0Ageneralizes%20to%20a%20wide%20range%20of%20control%20signals%2C%20including%20unseen%20ones%2C%20without%0Arequiring%20task-specific%20fine-tuning.%20Experiments%20show%20that%20UniPhys%20outperforms%0Aprior%20methods%20in%20motion%20naturalness%2C%20generalization%2C%20and%20robustness%20across%0Adiverse%20control%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12540v1&entry.124074799=Read"},
{"title": "SkyReels-V2: Infinite-length Film Generative Model", "author": "Guibin Chen and Dixuan Lin and Jiangping Yang and Chunze Lin and Juncheng Zhu and Mingyuan Fan and Hao Zhang and Sheng Chen and Zheng Chen and Chengchen Ma and Weiming Xiong and Wei Wang and Nuo Pang and Kang Kang and Zhiheng Xu and Yuzhe Jin and Yupeng Liang and Yubing Song and Peng Zhao and Boyuan Xu and Di Qiu and Debang Li and Zhengcong Fei and Yang Li and Yahui Zhou", "abstract": "  Recent advances in video generation have been driven by diffusion models and\nautoregressive frameworks, yet critical challenges persist in harmonizing\nprompt adherence, visual quality, motion dynamics, and duration: compromises in\nmotion dynamics to enhance temporal visual quality, constrained video duration\n(5-10 seconds) to prioritize resolution, and inadequate shot-aware generation\nstemming from general-purpose MLLMs' inability to interpret cinematic grammar,\nsuch as shot composition, actor expressions, and camera motions. These\nintertwined limitations hinder realistic long-form synthesis and professional\nfilm-style generation. To address these limitations, we propose SkyReels-V2, an\nInfinite-length Film Generative Model, that synergizes Multi-modal Large\nLanguage Model (MLLM), Multi-stage Pretraining, Reinforcement Learning, and\nDiffusion Forcing Framework. Firstly, we design a comprehensive structural\nrepresentation of video that combines the general descriptions by the\nMulti-modal LLM and the detailed shot language by sub-expert models. Aided with\nhuman annotation, we then train a unified Video Captioner, named\nSkyCaptioner-V1, to efficiently label the video data. Secondly, we establish\nprogressive-resolution pretraining for the fundamental video generation,\nfollowed by a four-stage post-training enhancement: Initial concept-balanced\nSupervised Fine-Tuning (SFT) improves baseline quality; Motion-specific\nReinforcement Learning (RL) training with human-annotated and synthetic\ndistortion data addresses dynamic artifacts; Our diffusion forcing framework\nwith non-decreasing noise schedules enables long-video synthesis in an\nefficient search space; Final high-quality SFT refines visual fidelity. All the\ncode and models are available at https://github.com/SkyworkAI/SkyReels-V2.\n", "link": "http://arxiv.org/abs/2504.13074v1", "date": "2025-04-17", "relevancy": 2.4485, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6298}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6179}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SkyReels-V2%3A%20Infinite-length%20Film%20Generative%20Model&body=Title%3A%20SkyReels-V2%3A%20Infinite-length%20Film%20Generative%20Model%0AAuthor%3A%20Guibin%20Chen%20and%20Dixuan%20Lin%20and%20Jiangping%20Yang%20and%20Chunze%20Lin%20and%20Juncheng%20Zhu%20and%20Mingyuan%20Fan%20and%20Hao%20Zhang%20and%20Sheng%20Chen%20and%20Zheng%20Chen%20and%20Chengchen%20Ma%20and%20Weiming%20Xiong%20and%20Wei%20Wang%20and%20Nuo%20Pang%20and%20Kang%20Kang%20and%20Zhiheng%20Xu%20and%20Yuzhe%20Jin%20and%20Yupeng%20Liang%20and%20Yubing%20Song%20and%20Peng%20Zhao%20and%20Boyuan%20Xu%20and%20Di%20Qiu%20and%20Debang%20Li%20and%20Zhengcong%20Fei%20and%20Yang%20Li%20and%20Yahui%20Zhou%0AAbstract%3A%20%20%20Recent%20advances%20in%20video%20generation%20have%20been%20driven%20by%20diffusion%20models%20and%0Aautoregressive%20frameworks%2C%20yet%20critical%20challenges%20persist%20in%20harmonizing%0Aprompt%20adherence%2C%20visual%20quality%2C%20motion%20dynamics%2C%20and%20duration%3A%20compromises%20in%0Amotion%20dynamics%20to%20enhance%20temporal%20visual%20quality%2C%20constrained%20video%20duration%0A%285-10%20seconds%29%20to%20prioritize%20resolution%2C%20and%20inadequate%20shot-aware%20generation%0Astemming%20from%20general-purpose%20MLLMs%27%20inability%20to%20interpret%20cinematic%20grammar%2C%0Asuch%20as%20shot%20composition%2C%20actor%20expressions%2C%20and%20camera%20motions.%20These%0Aintertwined%20limitations%20hinder%20realistic%20long-form%20synthesis%20and%20professional%0Afilm-style%20generation.%20To%20address%20these%20limitations%2C%20we%20propose%20SkyReels-V2%2C%20an%0AInfinite-length%20Film%20Generative%20Model%2C%20that%20synergizes%20Multi-modal%20Large%0ALanguage%20Model%20%28MLLM%29%2C%20Multi-stage%20Pretraining%2C%20Reinforcement%20Learning%2C%20and%0ADiffusion%20Forcing%20Framework.%20Firstly%2C%20we%20design%20a%20comprehensive%20structural%0Arepresentation%20of%20video%20that%20combines%20the%20general%20descriptions%20by%20the%0AMulti-modal%20LLM%20and%20the%20detailed%20shot%20language%20by%20sub-expert%20models.%20Aided%20with%0Ahuman%20annotation%2C%20we%20then%20train%20a%20unified%20Video%20Captioner%2C%20named%0ASkyCaptioner-V1%2C%20to%20efficiently%20label%20the%20video%20data.%20Secondly%2C%20we%20establish%0Aprogressive-resolution%20pretraining%20for%20the%20fundamental%20video%20generation%2C%0Afollowed%20by%20a%20four-stage%20post-training%20enhancement%3A%20Initial%20concept-balanced%0ASupervised%20Fine-Tuning%20%28SFT%29%20improves%20baseline%20quality%3B%20Motion-specific%0AReinforcement%20Learning%20%28RL%29%20training%20with%20human-annotated%20and%20synthetic%0Adistortion%20data%20addresses%20dynamic%20artifacts%3B%20Our%20diffusion%20forcing%20framework%0Awith%20non-decreasing%20noise%20schedules%20enables%20long-video%20synthesis%20in%20an%0Aefficient%20search%20space%3B%20Final%20high-quality%20SFT%20refines%20visual%20fidelity.%20All%20the%0Acode%20and%20models%20are%20available%20at%20https%3A//github.com/SkyworkAI/SkyReels-V2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13074v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkyReels-V2%253A%2520Infinite-length%2520Film%2520Generative%2520Model%26entry.906535625%3DGuibin%2520Chen%2520and%2520Dixuan%2520Lin%2520and%2520Jiangping%2520Yang%2520and%2520Chunze%2520Lin%2520and%2520Juncheng%2520Zhu%2520and%2520Mingyuan%2520Fan%2520and%2520Hao%2520Zhang%2520and%2520Sheng%2520Chen%2520and%2520Zheng%2520Chen%2520and%2520Chengchen%2520Ma%2520and%2520Weiming%2520Xiong%2520and%2520Wei%2520Wang%2520and%2520Nuo%2520Pang%2520and%2520Kang%2520Kang%2520and%2520Zhiheng%2520Xu%2520and%2520Yuzhe%2520Jin%2520and%2520Yupeng%2520Liang%2520and%2520Yubing%2520Song%2520and%2520Peng%2520Zhao%2520and%2520Boyuan%2520Xu%2520and%2520Di%2520Qiu%2520and%2520Debang%2520Li%2520and%2520Zhengcong%2520Fei%2520and%2520Yang%2520Li%2520and%2520Yahui%2520Zhou%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520video%2520generation%2520have%2520been%2520driven%2520by%2520diffusion%2520models%2520and%250Aautoregressive%2520frameworks%252C%2520yet%2520critical%2520challenges%2520persist%2520in%2520harmonizing%250Aprompt%2520adherence%252C%2520visual%2520quality%252C%2520motion%2520dynamics%252C%2520and%2520duration%253A%2520compromises%2520in%250Amotion%2520dynamics%2520to%2520enhance%2520temporal%2520visual%2520quality%252C%2520constrained%2520video%2520duration%250A%25285-10%2520seconds%2529%2520to%2520prioritize%2520resolution%252C%2520and%2520inadequate%2520shot-aware%2520generation%250Astemming%2520from%2520general-purpose%2520MLLMs%2527%2520inability%2520to%2520interpret%2520cinematic%2520grammar%252C%250Asuch%2520as%2520shot%2520composition%252C%2520actor%2520expressions%252C%2520and%2520camera%2520motions.%2520These%250Aintertwined%2520limitations%2520hinder%2520realistic%2520long-form%2520synthesis%2520and%2520professional%250Afilm-style%2520generation.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520SkyReels-V2%252C%2520an%250AInfinite-length%2520Film%2520Generative%2520Model%252C%2520that%2520synergizes%2520Multi-modal%2520Large%250ALanguage%2520Model%2520%2528MLLM%2529%252C%2520Multi-stage%2520Pretraining%252C%2520Reinforcement%2520Learning%252C%2520and%250ADiffusion%2520Forcing%2520Framework.%2520Firstly%252C%2520we%2520design%2520a%2520comprehensive%2520structural%250Arepresentation%2520of%2520video%2520that%2520combines%2520the%2520general%2520descriptions%2520by%2520the%250AMulti-modal%2520LLM%2520and%2520the%2520detailed%2520shot%2520language%2520by%2520sub-expert%2520models.%2520Aided%2520with%250Ahuman%2520annotation%252C%2520we%2520then%2520train%2520a%2520unified%2520Video%2520Captioner%252C%2520named%250ASkyCaptioner-V1%252C%2520to%2520efficiently%2520label%2520the%2520video%2520data.%2520Secondly%252C%2520we%2520establish%250Aprogressive-resolution%2520pretraining%2520for%2520the%2520fundamental%2520video%2520generation%252C%250Afollowed%2520by%2520a%2520four-stage%2520post-training%2520enhancement%253A%2520Initial%2520concept-balanced%250ASupervised%2520Fine-Tuning%2520%2528SFT%2529%2520improves%2520baseline%2520quality%253B%2520Motion-specific%250AReinforcement%2520Learning%2520%2528RL%2529%2520training%2520with%2520human-annotated%2520and%2520synthetic%250Adistortion%2520data%2520addresses%2520dynamic%2520artifacts%253B%2520Our%2520diffusion%2520forcing%2520framework%250Awith%2520non-decreasing%2520noise%2520schedules%2520enables%2520long-video%2520synthesis%2520in%2520an%250Aefficient%2520search%2520space%253B%2520Final%2520high-quality%2520SFT%2520refines%2520visual%2520fidelity.%2520All%2520the%250Acode%2520and%2520models%2520are%2520available%2520at%2520https%253A//github.com/SkyworkAI/SkyReels-V2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13074v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SkyReels-V2%3A%20Infinite-length%20Film%20Generative%20Model&entry.906535625=Guibin%20Chen%20and%20Dixuan%20Lin%20and%20Jiangping%20Yang%20and%20Chunze%20Lin%20and%20Juncheng%20Zhu%20and%20Mingyuan%20Fan%20and%20Hao%20Zhang%20and%20Sheng%20Chen%20and%20Zheng%20Chen%20and%20Chengchen%20Ma%20and%20Weiming%20Xiong%20and%20Wei%20Wang%20and%20Nuo%20Pang%20and%20Kang%20Kang%20and%20Zhiheng%20Xu%20and%20Yuzhe%20Jin%20and%20Yupeng%20Liang%20and%20Yubing%20Song%20and%20Peng%20Zhao%20and%20Boyuan%20Xu%20and%20Di%20Qiu%20and%20Debang%20Li%20and%20Zhengcong%20Fei%20and%20Yang%20Li%20and%20Yahui%20Zhou&entry.1292438233=%20%20Recent%20advances%20in%20video%20generation%20have%20been%20driven%20by%20diffusion%20models%20and%0Aautoregressive%20frameworks%2C%20yet%20critical%20challenges%20persist%20in%20harmonizing%0Aprompt%20adherence%2C%20visual%20quality%2C%20motion%20dynamics%2C%20and%20duration%3A%20compromises%20in%0Amotion%20dynamics%20to%20enhance%20temporal%20visual%20quality%2C%20constrained%20video%20duration%0A%285-10%20seconds%29%20to%20prioritize%20resolution%2C%20and%20inadequate%20shot-aware%20generation%0Astemming%20from%20general-purpose%20MLLMs%27%20inability%20to%20interpret%20cinematic%20grammar%2C%0Asuch%20as%20shot%20composition%2C%20actor%20expressions%2C%20and%20camera%20motions.%20These%0Aintertwined%20limitations%20hinder%20realistic%20long-form%20synthesis%20and%20professional%0Afilm-style%20generation.%20To%20address%20these%20limitations%2C%20we%20propose%20SkyReels-V2%2C%20an%0AInfinite-length%20Film%20Generative%20Model%2C%20that%20synergizes%20Multi-modal%20Large%0ALanguage%20Model%20%28MLLM%29%2C%20Multi-stage%20Pretraining%2C%20Reinforcement%20Learning%2C%20and%0ADiffusion%20Forcing%20Framework.%20Firstly%2C%20we%20design%20a%20comprehensive%20structural%0Arepresentation%20of%20video%20that%20combines%20the%20general%20descriptions%20by%20the%0AMulti-modal%20LLM%20and%20the%20detailed%20shot%20language%20by%20sub-expert%20models.%20Aided%20with%0Ahuman%20annotation%2C%20we%20then%20train%20a%20unified%20Video%20Captioner%2C%20named%0ASkyCaptioner-V1%2C%20to%20efficiently%20label%20the%20video%20data.%20Secondly%2C%20we%20establish%0Aprogressive-resolution%20pretraining%20for%20the%20fundamental%20video%20generation%2C%0Afollowed%20by%20a%20four-stage%20post-training%20enhancement%3A%20Initial%20concept-balanced%0ASupervised%20Fine-Tuning%20%28SFT%29%20improves%20baseline%20quality%3B%20Motion-specific%0AReinforcement%20Learning%20%28RL%29%20training%20with%20human-annotated%20and%20synthetic%0Adistortion%20data%20addresses%20dynamic%20artifacts%3B%20Our%20diffusion%20forcing%20framework%0Awith%20non-decreasing%20noise%20schedules%20enables%20long-video%20synthesis%20in%20an%0Aefficient%20search%20space%3B%20Final%20high-quality%20SFT%20refines%20visual%20fidelity.%20All%20the%0Acode%20and%20models%20are%20available%20at%20https%3A//github.com/SkyworkAI/SkyReels-V2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13074v1&entry.124074799=Read"},
{"title": "Imaging for All-Day Wearable Smart Glasses", "author": "Michael Goesele and Daniel Andersen and Yujia Chen and Simon Green and Eddy Ilg and Chao Li and Johnson Liu and Grace Kuo and Logan Wan and Richard Newcombe", "abstract": "  In recent years smart glasses technology has rapidly advanced, opening up\nentirely new areas for mobile computing. We expect future smart glasses will\nneed to be all-day wearable, adopting a small form factor to meet the\nrequirements of volume, weight, fashionability and social acceptability, which\nputs significant constraints on the space of possible solutions. Additional\nchallenges arise due to the fact that smart glasses are worn in arbitrary\nenvironments while their wearer moves and performs everyday activities. In this\npaper, we systematically analyze the space of imaging from smart glasses and\nderive several fundamental limits that govern this imaging domain. We discuss\nthe impact of these limits on achievable image quality and camera module size\n-- comparing in particular to related devices such as mobile phones. We then\npropose a novel distributed imaging approach that allows to minimize the size\nof the individual camera modules when compared to a standard monolithic camera\ndesign. Finally, we demonstrate the properties of this novel approach in a\nseries of experiments using synthetic data as well as images captured with two\ndifferent prototype implementations.\n", "link": "http://arxiv.org/abs/2504.13060v1", "date": "2025-04-17", "relevancy": 2.4478, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5041}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4823}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imaging%20for%20All-Day%20Wearable%20Smart%20Glasses&body=Title%3A%20Imaging%20for%20All-Day%20Wearable%20Smart%20Glasses%0AAuthor%3A%20Michael%20Goesele%20and%20Daniel%20Andersen%20and%20Yujia%20Chen%20and%20Simon%20Green%20and%20Eddy%20Ilg%20and%20Chao%20Li%20and%20Johnson%20Liu%20and%20Grace%20Kuo%20and%20Logan%20Wan%20and%20Richard%20Newcombe%0AAbstract%3A%20%20%20In%20recent%20years%20smart%20glasses%20technology%20has%20rapidly%20advanced%2C%20opening%20up%0Aentirely%20new%20areas%20for%20mobile%20computing.%20We%20expect%20future%20smart%20glasses%20will%0Aneed%20to%20be%20all-day%20wearable%2C%20adopting%20a%20small%20form%20factor%20to%20meet%20the%0Arequirements%20of%20volume%2C%20weight%2C%20fashionability%20and%20social%20acceptability%2C%20which%0Aputs%20significant%20constraints%20on%20the%20space%20of%20possible%20solutions.%20Additional%0Achallenges%20arise%20due%20to%20the%20fact%20that%20smart%20glasses%20are%20worn%20in%20arbitrary%0Aenvironments%20while%20their%20wearer%20moves%20and%20performs%20everyday%20activities.%20In%20this%0Apaper%2C%20we%20systematically%20analyze%20the%20space%20of%20imaging%20from%20smart%20glasses%20and%0Aderive%20several%20fundamental%20limits%20that%20govern%20this%20imaging%20domain.%20We%20discuss%0Athe%20impact%20of%20these%20limits%20on%20achievable%20image%20quality%20and%20camera%20module%20size%0A--%20comparing%20in%20particular%20to%20related%20devices%20such%20as%20mobile%20phones.%20We%20then%0Apropose%20a%20novel%20distributed%20imaging%20approach%20that%20allows%20to%20minimize%20the%20size%0Aof%20the%20individual%20camera%20modules%20when%20compared%20to%20a%20standard%20monolithic%20camera%0Adesign.%20Finally%2C%20we%20demonstrate%20the%20properties%20of%20this%20novel%20approach%20in%20a%0Aseries%20of%20experiments%20using%20synthetic%20data%20as%20well%20as%20images%20captured%20with%20two%0Adifferent%20prototype%20implementations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13060v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImaging%2520for%2520All-Day%2520Wearable%2520Smart%2520Glasses%26entry.906535625%3DMichael%2520Goesele%2520and%2520Daniel%2520Andersen%2520and%2520Yujia%2520Chen%2520and%2520Simon%2520Green%2520and%2520Eddy%2520Ilg%2520and%2520Chao%2520Li%2520and%2520Johnson%2520Liu%2520and%2520Grace%2520Kuo%2520and%2520Logan%2520Wan%2520and%2520Richard%2520Newcombe%26entry.1292438233%3D%2520%2520In%2520recent%2520years%2520smart%2520glasses%2520technology%2520has%2520rapidly%2520advanced%252C%2520opening%2520up%250Aentirely%2520new%2520areas%2520for%2520mobile%2520computing.%2520We%2520expect%2520future%2520smart%2520glasses%2520will%250Aneed%2520to%2520be%2520all-day%2520wearable%252C%2520adopting%2520a%2520small%2520form%2520factor%2520to%2520meet%2520the%250Arequirements%2520of%2520volume%252C%2520weight%252C%2520fashionability%2520and%2520social%2520acceptability%252C%2520which%250Aputs%2520significant%2520constraints%2520on%2520the%2520space%2520of%2520possible%2520solutions.%2520Additional%250Achallenges%2520arise%2520due%2520to%2520the%2520fact%2520that%2520smart%2520glasses%2520are%2520worn%2520in%2520arbitrary%250Aenvironments%2520while%2520their%2520wearer%2520moves%2520and%2520performs%2520everyday%2520activities.%2520In%2520this%250Apaper%252C%2520we%2520systematically%2520analyze%2520the%2520space%2520of%2520imaging%2520from%2520smart%2520glasses%2520and%250Aderive%2520several%2520fundamental%2520limits%2520that%2520govern%2520this%2520imaging%2520domain.%2520We%2520discuss%250Athe%2520impact%2520of%2520these%2520limits%2520on%2520achievable%2520image%2520quality%2520and%2520camera%2520module%2520size%250A--%2520comparing%2520in%2520particular%2520to%2520related%2520devices%2520such%2520as%2520mobile%2520phones.%2520We%2520then%250Apropose%2520a%2520novel%2520distributed%2520imaging%2520approach%2520that%2520allows%2520to%2520minimize%2520the%2520size%250Aof%2520the%2520individual%2520camera%2520modules%2520when%2520compared%2520to%2520a%2520standard%2520monolithic%2520camera%250Adesign.%2520Finally%252C%2520we%2520demonstrate%2520the%2520properties%2520of%2520this%2520novel%2520approach%2520in%2520a%250Aseries%2520of%2520experiments%2520using%2520synthetic%2520data%2520as%2520well%2520as%2520images%2520captured%2520with%2520two%250Adifferent%2520prototype%2520implementations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13060v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imaging%20for%20All-Day%20Wearable%20Smart%20Glasses&entry.906535625=Michael%20Goesele%20and%20Daniel%20Andersen%20and%20Yujia%20Chen%20and%20Simon%20Green%20and%20Eddy%20Ilg%20and%20Chao%20Li%20and%20Johnson%20Liu%20and%20Grace%20Kuo%20and%20Logan%20Wan%20and%20Richard%20Newcombe&entry.1292438233=%20%20In%20recent%20years%20smart%20glasses%20technology%20has%20rapidly%20advanced%2C%20opening%20up%0Aentirely%20new%20areas%20for%20mobile%20computing.%20We%20expect%20future%20smart%20glasses%20will%0Aneed%20to%20be%20all-day%20wearable%2C%20adopting%20a%20small%20form%20factor%20to%20meet%20the%0Arequirements%20of%20volume%2C%20weight%2C%20fashionability%20and%20social%20acceptability%2C%20which%0Aputs%20significant%20constraints%20on%20the%20space%20of%20possible%20solutions.%20Additional%0Achallenges%20arise%20due%20to%20the%20fact%20that%20smart%20glasses%20are%20worn%20in%20arbitrary%0Aenvironments%20while%20their%20wearer%20moves%20and%20performs%20everyday%20activities.%20In%20this%0Apaper%2C%20we%20systematically%20analyze%20the%20space%20of%20imaging%20from%20smart%20glasses%20and%0Aderive%20several%20fundamental%20limits%20that%20govern%20this%20imaging%20domain.%20We%20discuss%0Athe%20impact%20of%20these%20limits%20on%20achievable%20image%20quality%20and%20camera%20module%20size%0A--%20comparing%20in%20particular%20to%20related%20devices%20such%20as%20mobile%20phones.%20We%20then%0Apropose%20a%20novel%20distributed%20imaging%20approach%20that%20allows%20to%20minimize%20the%20size%0Aof%20the%20individual%20camera%20modules%20when%20compared%20to%20a%20standard%20monolithic%20camera%0Adesign.%20Finally%2C%20we%20demonstrate%20the%20properties%20of%20this%20novel%20approach%20in%20a%0Aseries%20of%20experiments%20using%20synthetic%20data%20as%20well%20as%20images%20captured%20with%20two%0Adifferent%20prototype%20implementations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13060v1&entry.124074799=Read"},
{"title": "Memorization: A Close Look at Books", "author": "Iris Ma and Ian Domingo and Alberto Krone-Martins and Pierre Baldi and Cristina V. Lopes", "abstract": "  To what extent can entire books be extracted from LLMs? Using the Llama 3 70B\nfamily of models, and the \"prefix-prompting\" extraction technique, we were able\nto auto-regressively reconstruct, with a very high level of similarity, one\nentire book (Alice's Adventures in Wonderland) from just the first 500 tokens.\nWe were also able to obtain high extraction rates on several other books,\npiece-wise. However, these successes do not extend uniformly to all books. We\nshow that extraction rates of books correlate with book popularity and thus,\nlikely duplication in the training data.\n  We also confirm the undoing of mitigations in the instruction-tuned Llama\n3.1, following recent work (Nasr et al., 2025). We further find that this\nundoing comes from changes to only a tiny fraction of weights concentrated\nprimarily in the lower transformer blocks. Our results provide evidence of the\nlimits of current regurgitation mitigation strategies and introduce a framework\nfor studying how fine-tuning affects the retrieval of verbatim memorization in\naligned LLMs.\n", "link": "http://arxiv.org/abs/2504.12549v1", "date": "2025-04-17", "relevancy": 2.4378, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4929}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4929}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memorization%3A%20A%20Close%20Look%20at%20Books&body=Title%3A%20Memorization%3A%20A%20Close%20Look%20at%20Books%0AAuthor%3A%20Iris%20Ma%20and%20Ian%20Domingo%20and%20Alberto%20Krone-Martins%20and%20Pierre%20Baldi%20and%20Cristina%20V.%20Lopes%0AAbstract%3A%20%20%20To%20what%20extent%20can%20entire%20books%20be%20extracted%20from%20LLMs%3F%20Using%20the%20Llama%203%2070B%0Afamily%20of%20models%2C%20and%20the%20%22prefix-prompting%22%20extraction%20technique%2C%20we%20were%20able%0Ato%20auto-regressively%20reconstruct%2C%20with%20a%20very%20high%20level%20of%20similarity%2C%20one%0Aentire%20book%20%28Alice%27s%20Adventures%20in%20Wonderland%29%20from%20just%20the%20first%20500%20tokens.%0AWe%20were%20also%20able%20to%20obtain%20high%20extraction%20rates%20on%20several%20other%20books%2C%0Apiece-wise.%20However%2C%20these%20successes%20do%20not%20extend%20uniformly%20to%20all%20books.%20We%0Ashow%20that%20extraction%20rates%20of%20books%20correlate%20with%20book%20popularity%20and%20thus%2C%0Alikely%20duplication%20in%20the%20training%20data.%0A%20%20We%20also%20confirm%20the%20undoing%20of%20mitigations%20in%20the%20instruction-tuned%20Llama%0A3.1%2C%20following%20recent%20work%20%28Nasr%20et%20al.%2C%202025%29.%20We%20further%20find%20that%20this%0Aundoing%20comes%20from%20changes%20to%20only%20a%20tiny%20fraction%20of%20weights%20concentrated%0Aprimarily%20in%20the%20lower%20transformer%20blocks.%20Our%20results%20provide%20evidence%20of%20the%0Alimits%20of%20current%20regurgitation%20mitigation%20strategies%20and%20introduce%20a%20framework%0Afor%20studying%20how%20fine-tuning%20affects%20the%20retrieval%20of%20verbatim%20memorization%20in%0Aaligned%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12549v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemorization%253A%2520A%2520Close%2520Look%2520at%2520Books%26entry.906535625%3DIris%2520Ma%2520and%2520Ian%2520Domingo%2520and%2520Alberto%2520Krone-Martins%2520and%2520Pierre%2520Baldi%2520and%2520Cristina%2520V.%2520Lopes%26entry.1292438233%3D%2520%2520To%2520what%2520extent%2520can%2520entire%2520books%2520be%2520extracted%2520from%2520LLMs%253F%2520Using%2520the%2520Llama%25203%252070B%250Afamily%2520of%2520models%252C%2520and%2520the%2520%2522prefix-prompting%2522%2520extraction%2520technique%252C%2520we%2520were%2520able%250Ato%2520auto-regressively%2520reconstruct%252C%2520with%2520a%2520very%2520high%2520level%2520of%2520similarity%252C%2520one%250Aentire%2520book%2520%2528Alice%2527s%2520Adventures%2520in%2520Wonderland%2529%2520from%2520just%2520the%2520first%2520500%2520tokens.%250AWe%2520were%2520also%2520able%2520to%2520obtain%2520high%2520extraction%2520rates%2520on%2520several%2520other%2520books%252C%250Apiece-wise.%2520However%252C%2520these%2520successes%2520do%2520not%2520extend%2520uniformly%2520to%2520all%2520books.%2520We%250Ashow%2520that%2520extraction%2520rates%2520of%2520books%2520correlate%2520with%2520book%2520popularity%2520and%2520thus%252C%250Alikely%2520duplication%2520in%2520the%2520training%2520data.%250A%2520%2520We%2520also%2520confirm%2520the%2520undoing%2520of%2520mitigations%2520in%2520the%2520instruction-tuned%2520Llama%250A3.1%252C%2520following%2520recent%2520work%2520%2528Nasr%2520et%2520al.%252C%25202025%2529.%2520We%2520further%2520find%2520that%2520this%250Aundoing%2520comes%2520from%2520changes%2520to%2520only%2520a%2520tiny%2520fraction%2520of%2520weights%2520concentrated%250Aprimarily%2520in%2520the%2520lower%2520transformer%2520blocks.%2520Our%2520results%2520provide%2520evidence%2520of%2520the%250Alimits%2520of%2520current%2520regurgitation%2520mitigation%2520strategies%2520and%2520introduce%2520a%2520framework%250Afor%2520studying%2520how%2520fine-tuning%2520affects%2520the%2520retrieval%2520of%2520verbatim%2520memorization%2520in%250Aaligned%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12549v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memorization%3A%20A%20Close%20Look%20at%20Books&entry.906535625=Iris%20Ma%20and%20Ian%20Domingo%20and%20Alberto%20Krone-Martins%20and%20Pierre%20Baldi%20and%20Cristina%20V.%20Lopes&entry.1292438233=%20%20To%20what%20extent%20can%20entire%20books%20be%20extracted%20from%20LLMs%3F%20Using%20the%20Llama%203%2070B%0Afamily%20of%20models%2C%20and%20the%20%22prefix-prompting%22%20extraction%20technique%2C%20we%20were%20able%0Ato%20auto-regressively%20reconstruct%2C%20with%20a%20very%20high%20level%20of%20similarity%2C%20one%0Aentire%20book%20%28Alice%27s%20Adventures%20in%20Wonderland%29%20from%20just%20the%20first%20500%20tokens.%0AWe%20were%20also%20able%20to%20obtain%20high%20extraction%20rates%20on%20several%20other%20books%2C%0Apiece-wise.%20However%2C%20these%20successes%20do%20not%20extend%20uniformly%20to%20all%20books.%20We%0Ashow%20that%20extraction%20rates%20of%20books%20correlate%20with%20book%20popularity%20and%20thus%2C%0Alikely%20duplication%20in%20the%20training%20data.%0A%20%20We%20also%20confirm%20the%20undoing%20of%20mitigations%20in%20the%20instruction-tuned%20Llama%0A3.1%2C%20following%20recent%20work%20%28Nasr%20et%20al.%2C%202025%29.%20We%20further%20find%20that%20this%0Aundoing%20comes%20from%20changes%20to%20only%20a%20tiny%20fraction%20of%20weights%20concentrated%0Aprimarily%20in%20the%20lower%20transformer%20blocks.%20Our%20results%20provide%20evidence%20of%20the%0Alimits%20of%20current%20regurgitation%20mitigation%20strategies%20and%20introduce%20a%20framework%0Afor%20studying%20how%20fine-tuning%20affects%20the%20retrieval%20of%20verbatim%20memorization%20in%0Aaligned%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12549v1&entry.124074799=Read"},
{"title": "Good Seed Makes a Good Crop: Discovering Secret Seeds in Text-to-Image\n  Diffusion Models", "author": "Katherine Xu and Lingzhi Zhang and Jianbo Shi", "abstract": "  Recent advances in text-to-image (T2I) diffusion models have facilitated\ncreative and photorealistic image synthesis. By varying the random seeds, we\ncan generate many images for a fixed text prompt. Technically, the seed\ncontrols the initial noise and, in multi-step diffusion inference, the noise\nused for reparameterization at intermediate timesteps in the reverse diffusion\nprocess. However, the specific impact of the random seed on the generated\nimages remains relatively unexplored. In this work, we conduct a large-scale\nscientific study into the impact of random seeds during diffusion inference.\nRemarkably, we reveal that the best 'golden' seed achieved an impressive FID of\n21.60, compared to the worst 'inferior' seed's FID of 31.97. Additionally, a\nclassifier can predict the seed number used to generate an image with over\n99.9% accuracy in just a few epochs, establishing that seeds are highly\ndistinguishable based on generated images. Encouraged by these findings, we\nexamined the influence of seeds on interpretable visual dimensions. We find\nthat certain seeds consistently produce grayscale images, prominent sky\nregions, or image borders. Seeds also affect image composition, including\nobject location, size, and depth. Moreover, by leveraging these 'golden' seeds,\nwe demonstrate improved image generation such as high-fidelity inference and\ndiversified sampling. Our investigation extends to inpainting tasks, where we\nuncover some seeds that tend to insert unwanted text artifacts. Overall, our\nextensive analyses highlight the importance of selecting good seeds and offer\npractical utility for image generation.\n", "link": "http://arxiv.org/abs/2405.14828v2", "date": "2025-04-16", "relevancy": 2.4344, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6159}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6155}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5988}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Good%20Seed%20Makes%20a%20Good%20Crop%3A%20Discovering%20Secret%20Seeds%20in%20Text-to-Image%0A%20%20Diffusion%20Models&body=Title%3A%20Good%20Seed%20Makes%20a%20Good%20Crop%3A%20Discovering%20Secret%20Seeds%20in%20Text-to-Image%0A%20%20Diffusion%20Models%0AAuthor%3A%20Katherine%20Xu%20and%20Lingzhi%20Zhang%20and%20Jianbo%20Shi%0AAbstract%3A%20%20%20Recent%20advances%20in%20text-to-image%20%28T2I%29%20diffusion%20models%20have%20facilitated%0Acreative%20and%20photorealistic%20image%20synthesis.%20By%20varying%20the%20random%20seeds%2C%20we%0Acan%20generate%20many%20images%20for%20a%20fixed%20text%20prompt.%20Technically%2C%20the%20seed%0Acontrols%20the%20initial%20noise%20and%2C%20in%20multi-step%20diffusion%20inference%2C%20the%20noise%0Aused%20for%20reparameterization%20at%20intermediate%20timesteps%20in%20the%20reverse%20diffusion%0Aprocess.%20However%2C%20the%20specific%20impact%20of%20the%20random%20seed%20on%20the%20generated%0Aimages%20remains%20relatively%20unexplored.%20In%20this%20work%2C%20we%20conduct%20a%20large-scale%0Ascientific%20study%20into%20the%20impact%20of%20random%20seeds%20during%20diffusion%20inference.%0ARemarkably%2C%20we%20reveal%20that%20the%20best%20%27golden%27%20seed%20achieved%20an%20impressive%20FID%20of%0A21.60%2C%20compared%20to%20the%20worst%20%27inferior%27%20seed%27s%20FID%20of%2031.97.%20Additionally%2C%20a%0Aclassifier%20can%20predict%20the%20seed%20number%20used%20to%20generate%20an%20image%20with%20over%0A99.9%25%20accuracy%20in%20just%20a%20few%20epochs%2C%20establishing%20that%20seeds%20are%20highly%0Adistinguishable%20based%20on%20generated%20images.%20Encouraged%20by%20these%20findings%2C%20we%0Aexamined%20the%20influence%20of%20seeds%20on%20interpretable%20visual%20dimensions.%20We%20find%0Athat%20certain%20seeds%20consistently%20produce%20grayscale%20images%2C%20prominent%20sky%0Aregions%2C%20or%20image%20borders.%20Seeds%20also%20affect%20image%20composition%2C%20including%0Aobject%20location%2C%20size%2C%20and%20depth.%20Moreover%2C%20by%20leveraging%20these%20%27golden%27%20seeds%2C%0Awe%20demonstrate%20improved%20image%20generation%20such%20as%20high-fidelity%20inference%20and%0Adiversified%20sampling.%20Our%20investigation%20extends%20to%20inpainting%20tasks%2C%20where%20we%0Auncover%20some%20seeds%20that%20tend%20to%20insert%20unwanted%20text%20artifacts.%20Overall%2C%20our%0Aextensive%20analyses%20highlight%20the%20importance%20of%20selecting%20good%20seeds%20and%20offer%0Apractical%20utility%20for%20image%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14828v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGood%2520Seed%2520Makes%2520a%2520Good%2520Crop%253A%2520Discovering%2520Secret%2520Seeds%2520in%2520Text-to-Image%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DKatherine%2520Xu%2520and%2520Lingzhi%2520Zhang%2520and%2520Jianbo%2520Shi%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520text-to-image%2520%2528T2I%2529%2520diffusion%2520models%2520have%2520facilitated%250Acreative%2520and%2520photorealistic%2520image%2520synthesis.%2520By%2520varying%2520the%2520random%2520seeds%252C%2520we%250Acan%2520generate%2520many%2520images%2520for%2520a%2520fixed%2520text%2520prompt.%2520Technically%252C%2520the%2520seed%250Acontrols%2520the%2520initial%2520noise%2520and%252C%2520in%2520multi-step%2520diffusion%2520inference%252C%2520the%2520noise%250Aused%2520for%2520reparameterization%2520at%2520intermediate%2520timesteps%2520in%2520the%2520reverse%2520diffusion%250Aprocess.%2520However%252C%2520the%2520specific%2520impact%2520of%2520the%2520random%2520seed%2520on%2520the%2520generated%250Aimages%2520remains%2520relatively%2520unexplored.%2520In%2520this%2520work%252C%2520we%2520conduct%2520a%2520large-scale%250Ascientific%2520study%2520into%2520the%2520impact%2520of%2520random%2520seeds%2520during%2520diffusion%2520inference.%250ARemarkably%252C%2520we%2520reveal%2520that%2520the%2520best%2520%2527golden%2527%2520seed%2520achieved%2520an%2520impressive%2520FID%2520of%250A21.60%252C%2520compared%2520to%2520the%2520worst%2520%2527inferior%2527%2520seed%2527s%2520FID%2520of%252031.97.%2520Additionally%252C%2520a%250Aclassifier%2520can%2520predict%2520the%2520seed%2520number%2520used%2520to%2520generate%2520an%2520image%2520with%2520over%250A99.9%2525%2520accuracy%2520in%2520just%2520a%2520few%2520epochs%252C%2520establishing%2520that%2520seeds%2520are%2520highly%250Adistinguishable%2520based%2520on%2520generated%2520images.%2520Encouraged%2520by%2520these%2520findings%252C%2520we%250Aexamined%2520the%2520influence%2520of%2520seeds%2520on%2520interpretable%2520visual%2520dimensions.%2520We%2520find%250Athat%2520certain%2520seeds%2520consistently%2520produce%2520grayscale%2520images%252C%2520prominent%2520sky%250Aregions%252C%2520or%2520image%2520borders.%2520Seeds%2520also%2520affect%2520image%2520composition%252C%2520including%250Aobject%2520location%252C%2520size%252C%2520and%2520depth.%2520Moreover%252C%2520by%2520leveraging%2520these%2520%2527golden%2527%2520seeds%252C%250Awe%2520demonstrate%2520improved%2520image%2520generation%2520such%2520as%2520high-fidelity%2520inference%2520and%250Adiversified%2520sampling.%2520Our%2520investigation%2520extends%2520to%2520inpainting%2520tasks%252C%2520where%2520we%250Auncover%2520some%2520seeds%2520that%2520tend%2520to%2520insert%2520unwanted%2520text%2520artifacts.%2520Overall%252C%2520our%250Aextensive%2520analyses%2520highlight%2520the%2520importance%2520of%2520selecting%2520good%2520seeds%2520and%2520offer%250Apractical%2520utility%2520for%2520image%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14828v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Good%20Seed%20Makes%20a%20Good%20Crop%3A%20Discovering%20Secret%20Seeds%20in%20Text-to-Image%0A%20%20Diffusion%20Models&entry.906535625=Katherine%20Xu%20and%20Lingzhi%20Zhang%20and%20Jianbo%20Shi&entry.1292438233=%20%20Recent%20advances%20in%20text-to-image%20%28T2I%29%20diffusion%20models%20have%20facilitated%0Acreative%20and%20photorealistic%20image%20synthesis.%20By%20varying%20the%20random%20seeds%2C%20we%0Acan%20generate%20many%20images%20for%20a%20fixed%20text%20prompt.%20Technically%2C%20the%20seed%0Acontrols%20the%20initial%20noise%20and%2C%20in%20multi-step%20diffusion%20inference%2C%20the%20noise%0Aused%20for%20reparameterization%20at%20intermediate%20timesteps%20in%20the%20reverse%20diffusion%0Aprocess.%20However%2C%20the%20specific%20impact%20of%20the%20random%20seed%20on%20the%20generated%0Aimages%20remains%20relatively%20unexplored.%20In%20this%20work%2C%20we%20conduct%20a%20large-scale%0Ascientific%20study%20into%20the%20impact%20of%20random%20seeds%20during%20diffusion%20inference.%0ARemarkably%2C%20we%20reveal%20that%20the%20best%20%27golden%27%20seed%20achieved%20an%20impressive%20FID%20of%0A21.60%2C%20compared%20to%20the%20worst%20%27inferior%27%20seed%27s%20FID%20of%2031.97.%20Additionally%2C%20a%0Aclassifier%20can%20predict%20the%20seed%20number%20used%20to%20generate%20an%20image%20with%20over%0A99.9%25%20accuracy%20in%20just%20a%20few%20epochs%2C%20establishing%20that%20seeds%20are%20highly%0Adistinguishable%20based%20on%20generated%20images.%20Encouraged%20by%20these%20findings%2C%20we%0Aexamined%20the%20influence%20of%20seeds%20on%20interpretable%20visual%20dimensions.%20We%20find%0Athat%20certain%20seeds%20consistently%20produce%20grayscale%20images%2C%20prominent%20sky%0Aregions%2C%20or%20image%20borders.%20Seeds%20also%20affect%20image%20composition%2C%20including%0Aobject%20location%2C%20size%2C%20and%20depth.%20Moreover%2C%20by%20leveraging%20these%20%27golden%27%20seeds%2C%0Awe%20demonstrate%20improved%20image%20generation%20such%20as%20high-fidelity%20inference%20and%0Adiversified%20sampling.%20Our%20investigation%20extends%20to%20inpainting%20tasks%2C%20where%20we%0Auncover%20some%20seeds%20that%20tend%20to%20insert%20unwanted%20text%20artifacts.%20Overall%2C%20our%0Aextensive%20analyses%20highlight%20the%20importance%20of%20selecting%20good%20seeds%20and%20offer%0Apractical%20utility%20for%20image%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14828v2&entry.124074799=Read"},
{"title": "DVLTA-VQA: Decoupled Vision-Language Modeling with Text-Guided\n  Adaptation for Blind Video Quality Assessment", "author": "Li Yu and Situo Wang and Wei Zhou and Moncef Gabbouj", "abstract": "  Inspired by the dual-stream theory of the human visual system (HVS) - where\nthe ventral stream is responsible for object recognition and detail analysis,\nwhile the dorsal stream focuses on spatial relationships and motion perception\n- an increasing number of video quality assessment (VQA) works built upon this\nframework are proposed. Recent advancements in large multi-modal models,\nnotably Contrastive Language-Image Pretraining (CLIP), have motivated\nresearchers to incorporate CLIP into dual-stream-based VQA methods. This\nintegration aims to harness the model's superior semantic understanding\ncapabilities to replicate the object recognition and detail analysis in ventral\nstream, as well as spatial relationship analysis in dorsal stream. However,\nCLIP is originally designed for images and lacks the ability to capture\ntemporal and motion information inherent in videos.To address the limitation,\nthis paper propose a Decoupled Vision-Language Modeling with Text-Guided\nAdaptation for Blind Video Quality Assessment (DVLTA-VQA), which decouples\nCLIP's visual and textual components, and integrates them into different stages\nof the NR-VQA pipeline. Specifically, a Video-Based Temporal CLIP module is\nproposed to explicitly model temporal dynamics and enhance motion perception,\naligning with the dorsal stream. Additionally, a Temporal Context Module is\ndeveloped to refine inter-frame dependencies, further improving motion\nmodeling. On the ventral stream side, a Basic Visual Feature Extraction Module\nis employed to strengthen detail analysis. Finally, a text-guided adaptive\nfusion strategy is proposed to enable dynamic weighting of features,\nfacilitating more effective integration of spatial and temporal information.\n", "link": "http://arxiv.org/abs/2504.11733v2", "date": "2025-04-17", "relevancy": 2.4211, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6134}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6052}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DVLTA-VQA%3A%20Decoupled%20Vision-Language%20Modeling%20with%20Text-Guided%0A%20%20Adaptation%20for%20Blind%20Video%20Quality%20Assessment&body=Title%3A%20DVLTA-VQA%3A%20Decoupled%20Vision-Language%20Modeling%20with%20Text-Guided%0A%20%20Adaptation%20for%20Blind%20Video%20Quality%20Assessment%0AAuthor%3A%20Li%20Yu%20and%20Situo%20Wang%20and%20Wei%20Zhou%20and%20Moncef%20Gabbouj%0AAbstract%3A%20%20%20Inspired%20by%20the%20dual-stream%20theory%20of%20the%20human%20visual%20system%20%28HVS%29%20-%20where%0Athe%20ventral%20stream%20is%20responsible%20for%20object%20recognition%20and%20detail%20analysis%2C%0Awhile%20the%20dorsal%20stream%20focuses%20on%20spatial%20relationships%20and%20motion%20perception%0A-%20an%20increasing%20number%20of%20video%20quality%20assessment%20%28VQA%29%20works%20built%20upon%20this%0Aframework%20are%20proposed.%20Recent%20advancements%20in%20large%20multi-modal%20models%2C%0Anotably%20Contrastive%20Language-Image%20Pretraining%20%28CLIP%29%2C%20have%20motivated%0Aresearchers%20to%20incorporate%20CLIP%20into%20dual-stream-based%20VQA%20methods.%20This%0Aintegration%20aims%20to%20harness%20the%20model%27s%20superior%20semantic%20understanding%0Acapabilities%20to%20replicate%20the%20object%20recognition%20and%20detail%20analysis%20in%20ventral%0Astream%2C%20as%20well%20as%20spatial%20relationship%20analysis%20in%20dorsal%20stream.%20However%2C%0ACLIP%20is%20originally%20designed%20for%20images%20and%20lacks%20the%20ability%20to%20capture%0Atemporal%20and%20motion%20information%20inherent%20in%20videos.To%20address%20the%20limitation%2C%0Athis%20paper%20propose%20a%20Decoupled%20Vision-Language%20Modeling%20with%20Text-Guided%0AAdaptation%20for%20Blind%20Video%20Quality%20Assessment%20%28DVLTA-VQA%29%2C%20which%20decouples%0ACLIP%27s%20visual%20and%20textual%20components%2C%20and%20integrates%20them%20into%20different%20stages%0Aof%20the%20NR-VQA%20pipeline.%20Specifically%2C%20a%20Video-Based%20Temporal%20CLIP%20module%20is%0Aproposed%20to%20explicitly%20model%20temporal%20dynamics%20and%20enhance%20motion%20perception%2C%0Aaligning%20with%20the%20dorsal%20stream.%20Additionally%2C%20a%20Temporal%20Context%20Module%20is%0Adeveloped%20to%20refine%20inter-frame%20dependencies%2C%20further%20improving%20motion%0Amodeling.%20On%20the%20ventral%20stream%20side%2C%20a%20Basic%20Visual%20Feature%20Extraction%20Module%0Ais%20employed%20to%20strengthen%20detail%20analysis.%20Finally%2C%20a%20text-guided%20adaptive%0Afusion%20strategy%20is%20proposed%20to%20enable%20dynamic%20weighting%20of%20features%2C%0Afacilitating%20more%20effective%20integration%20of%20spatial%20and%20temporal%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.11733v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDVLTA-VQA%253A%2520Decoupled%2520Vision-Language%2520Modeling%2520with%2520Text-Guided%250A%2520%2520Adaptation%2520for%2520Blind%2520Video%2520Quality%2520Assessment%26entry.906535625%3DLi%2520Yu%2520and%2520Situo%2520Wang%2520and%2520Wei%2520Zhou%2520and%2520Moncef%2520Gabbouj%26entry.1292438233%3D%2520%2520Inspired%2520by%2520the%2520dual-stream%2520theory%2520of%2520the%2520human%2520visual%2520system%2520%2528HVS%2529%2520-%2520where%250Athe%2520ventral%2520stream%2520is%2520responsible%2520for%2520object%2520recognition%2520and%2520detail%2520analysis%252C%250Awhile%2520the%2520dorsal%2520stream%2520focuses%2520on%2520spatial%2520relationships%2520and%2520motion%2520perception%250A-%2520an%2520increasing%2520number%2520of%2520video%2520quality%2520assessment%2520%2528VQA%2529%2520works%2520built%2520upon%2520this%250Aframework%2520are%2520proposed.%2520Recent%2520advancements%2520in%2520large%2520multi-modal%2520models%252C%250Anotably%2520Contrastive%2520Language-Image%2520Pretraining%2520%2528CLIP%2529%252C%2520have%2520motivated%250Aresearchers%2520to%2520incorporate%2520CLIP%2520into%2520dual-stream-based%2520VQA%2520methods.%2520This%250Aintegration%2520aims%2520to%2520harness%2520the%2520model%2527s%2520superior%2520semantic%2520understanding%250Acapabilities%2520to%2520replicate%2520the%2520object%2520recognition%2520and%2520detail%2520analysis%2520in%2520ventral%250Astream%252C%2520as%2520well%2520as%2520spatial%2520relationship%2520analysis%2520in%2520dorsal%2520stream.%2520However%252C%250ACLIP%2520is%2520originally%2520designed%2520for%2520images%2520and%2520lacks%2520the%2520ability%2520to%2520capture%250Atemporal%2520and%2520motion%2520information%2520inherent%2520in%2520videos.To%2520address%2520the%2520limitation%252C%250Athis%2520paper%2520propose%2520a%2520Decoupled%2520Vision-Language%2520Modeling%2520with%2520Text-Guided%250AAdaptation%2520for%2520Blind%2520Video%2520Quality%2520Assessment%2520%2528DVLTA-VQA%2529%252C%2520which%2520decouples%250ACLIP%2527s%2520visual%2520and%2520textual%2520components%252C%2520and%2520integrates%2520them%2520into%2520different%2520stages%250Aof%2520the%2520NR-VQA%2520pipeline.%2520Specifically%252C%2520a%2520Video-Based%2520Temporal%2520CLIP%2520module%2520is%250Aproposed%2520to%2520explicitly%2520model%2520temporal%2520dynamics%2520and%2520enhance%2520motion%2520perception%252C%250Aaligning%2520with%2520the%2520dorsal%2520stream.%2520Additionally%252C%2520a%2520Temporal%2520Context%2520Module%2520is%250Adeveloped%2520to%2520refine%2520inter-frame%2520dependencies%252C%2520further%2520improving%2520motion%250Amodeling.%2520On%2520the%2520ventral%2520stream%2520side%252C%2520a%2520Basic%2520Visual%2520Feature%2520Extraction%2520Module%250Ais%2520employed%2520to%2520strengthen%2520detail%2520analysis.%2520Finally%252C%2520a%2520text-guided%2520adaptive%250Afusion%2520strategy%2520is%2520proposed%2520to%2520enable%2520dynamic%2520weighting%2520of%2520features%252C%250Afacilitating%2520more%2520effective%2520integration%2520of%2520spatial%2520and%2520temporal%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.11733v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DVLTA-VQA%3A%20Decoupled%20Vision-Language%20Modeling%20with%20Text-Guided%0A%20%20Adaptation%20for%20Blind%20Video%20Quality%20Assessment&entry.906535625=Li%20Yu%20and%20Situo%20Wang%20and%20Wei%20Zhou%20and%20Moncef%20Gabbouj&entry.1292438233=%20%20Inspired%20by%20the%20dual-stream%20theory%20of%20the%20human%20visual%20system%20%28HVS%29%20-%20where%0Athe%20ventral%20stream%20is%20responsible%20for%20object%20recognition%20and%20detail%20analysis%2C%0Awhile%20the%20dorsal%20stream%20focuses%20on%20spatial%20relationships%20and%20motion%20perception%0A-%20an%20increasing%20number%20of%20video%20quality%20assessment%20%28VQA%29%20works%20built%20upon%20this%0Aframework%20are%20proposed.%20Recent%20advancements%20in%20large%20multi-modal%20models%2C%0Anotably%20Contrastive%20Language-Image%20Pretraining%20%28CLIP%29%2C%20have%20motivated%0Aresearchers%20to%20incorporate%20CLIP%20into%20dual-stream-based%20VQA%20methods.%20This%0Aintegration%20aims%20to%20harness%20the%20model%27s%20superior%20semantic%20understanding%0Acapabilities%20to%20replicate%20the%20object%20recognition%20and%20detail%20analysis%20in%20ventral%0Astream%2C%20as%20well%20as%20spatial%20relationship%20analysis%20in%20dorsal%20stream.%20However%2C%0ACLIP%20is%20originally%20designed%20for%20images%20and%20lacks%20the%20ability%20to%20capture%0Atemporal%20and%20motion%20information%20inherent%20in%20videos.To%20address%20the%20limitation%2C%0Athis%20paper%20propose%20a%20Decoupled%20Vision-Language%20Modeling%20with%20Text-Guided%0AAdaptation%20for%20Blind%20Video%20Quality%20Assessment%20%28DVLTA-VQA%29%2C%20which%20decouples%0ACLIP%27s%20visual%20and%20textual%20components%2C%20and%20integrates%20them%20into%20different%20stages%0Aof%20the%20NR-VQA%20pipeline.%20Specifically%2C%20a%20Video-Based%20Temporal%20CLIP%20module%20is%0Aproposed%20to%20explicitly%20model%20temporal%20dynamics%20and%20enhance%20motion%20perception%2C%0Aaligning%20with%20the%20dorsal%20stream.%20Additionally%2C%20a%20Temporal%20Context%20Module%20is%0Adeveloped%20to%20refine%20inter-frame%20dependencies%2C%20further%20improving%20motion%0Amodeling.%20On%20the%20ventral%20stream%20side%2C%20a%20Basic%20Visual%20Feature%20Extraction%20Module%0Ais%20employed%20to%20strengthen%20detail%20analysis.%20Finally%2C%20a%20text-guided%20adaptive%0Afusion%20strategy%20is%20proposed%20to%20enable%20dynamic%20weighting%20of%20features%2C%0Afacilitating%20more%20effective%20integration%20of%20spatial%20and%20temporal%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.11733v2&entry.124074799=Read"},
{"title": "An All-Atom Generative Model for Designing Protein Complexes", "author": "Ruizhe Chen and Dongyu Xue and Xiangxin Zhou and Zaixiang Zheng and Xiangxiang Zeng and Quanquan Gu", "abstract": "  Proteins typically exist in complexes, interacting with other proteins or\nbiomolecules to perform their specific biological roles. Research on\nsingle-chain protein modeling has been extensively and deeply explored, with\nadvancements seen in models like the series of ESM and AlphaFold. Despite these\ndevelopments, the study and modeling of multi-chain proteins remain largely\nuncharted, though they are vital for understanding biological functions.\nRecognizing the importance of these interactions, we introduce APM (All-Atom\nProtein Generative Model), a model specifically designed for modeling\nmulti-chain proteins. By integrating atom-level information and leveraging data\non multi-chain proteins, APM is capable of precisely modeling inter-chain\ninteractions and designing protein complexes with binding capabilities from\nscratch. It also performs folding and inverse-folding tasks for multi-chain\nproteins. Moreover, APM demonstrates versatility in downstream applications: it\nachieves enhanced performance through supervised fine-tuning (SFT) while also\nsupporting zero-shot sampling in certain tasks, achieving state-of-the-art\nresults. Code will be released at https://github.com/bytedance/apm.\n", "link": "http://arxiv.org/abs/2504.13075v1", "date": "2025-04-17", "relevancy": 2.4205, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4848}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4838}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20All-Atom%20Generative%20Model%20for%20Designing%20Protein%20Complexes&body=Title%3A%20An%20All-Atom%20Generative%20Model%20for%20Designing%20Protein%20Complexes%0AAuthor%3A%20Ruizhe%20Chen%20and%20Dongyu%20Xue%20and%20Xiangxin%20Zhou%20and%20Zaixiang%20Zheng%20and%20Xiangxiang%20Zeng%20and%20Quanquan%20Gu%0AAbstract%3A%20%20%20Proteins%20typically%20exist%20in%20complexes%2C%20interacting%20with%20other%20proteins%20or%0Abiomolecules%20to%20perform%20their%20specific%20biological%20roles.%20Research%20on%0Asingle-chain%20protein%20modeling%20has%20been%20extensively%20and%20deeply%20explored%2C%20with%0Aadvancements%20seen%20in%20models%20like%20the%20series%20of%20ESM%20and%20AlphaFold.%20Despite%20these%0Adevelopments%2C%20the%20study%20and%20modeling%20of%20multi-chain%20proteins%20remain%20largely%0Auncharted%2C%20though%20they%20are%20vital%20for%20understanding%20biological%20functions.%0ARecognizing%20the%20importance%20of%20these%20interactions%2C%20we%20introduce%20APM%20%28All-Atom%0AProtein%20Generative%20Model%29%2C%20a%20model%20specifically%20designed%20for%20modeling%0Amulti-chain%20proteins.%20By%20integrating%20atom-level%20information%20and%20leveraging%20data%0Aon%20multi-chain%20proteins%2C%20APM%20is%20capable%20of%20precisely%20modeling%20inter-chain%0Ainteractions%20and%20designing%20protein%20complexes%20with%20binding%20capabilities%20from%0Ascratch.%20It%20also%20performs%20folding%20and%20inverse-folding%20tasks%20for%20multi-chain%0Aproteins.%20Moreover%2C%20APM%20demonstrates%20versatility%20in%20downstream%20applications%3A%20it%0Aachieves%20enhanced%20performance%20through%20supervised%20fine-tuning%20%28SFT%29%20while%20also%0Asupporting%20zero-shot%20sampling%20in%20certain%20tasks%2C%20achieving%20state-of-the-art%0Aresults.%20Code%20will%20be%20released%20at%20https%3A//github.com/bytedance/apm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13075v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520All-Atom%2520Generative%2520Model%2520for%2520Designing%2520Protein%2520Complexes%26entry.906535625%3DRuizhe%2520Chen%2520and%2520Dongyu%2520Xue%2520and%2520Xiangxin%2520Zhou%2520and%2520Zaixiang%2520Zheng%2520and%2520Xiangxiang%2520Zeng%2520and%2520Quanquan%2520Gu%26entry.1292438233%3D%2520%2520Proteins%2520typically%2520exist%2520in%2520complexes%252C%2520interacting%2520with%2520other%2520proteins%2520or%250Abiomolecules%2520to%2520perform%2520their%2520specific%2520biological%2520roles.%2520Research%2520on%250Asingle-chain%2520protein%2520modeling%2520has%2520been%2520extensively%2520and%2520deeply%2520explored%252C%2520with%250Aadvancements%2520seen%2520in%2520models%2520like%2520the%2520series%2520of%2520ESM%2520and%2520AlphaFold.%2520Despite%2520these%250Adevelopments%252C%2520the%2520study%2520and%2520modeling%2520of%2520multi-chain%2520proteins%2520remain%2520largely%250Auncharted%252C%2520though%2520they%2520are%2520vital%2520for%2520understanding%2520biological%2520functions.%250ARecognizing%2520the%2520importance%2520of%2520these%2520interactions%252C%2520we%2520introduce%2520APM%2520%2528All-Atom%250AProtein%2520Generative%2520Model%2529%252C%2520a%2520model%2520specifically%2520designed%2520for%2520modeling%250Amulti-chain%2520proteins.%2520By%2520integrating%2520atom-level%2520information%2520and%2520leveraging%2520data%250Aon%2520multi-chain%2520proteins%252C%2520APM%2520is%2520capable%2520of%2520precisely%2520modeling%2520inter-chain%250Ainteractions%2520and%2520designing%2520protein%2520complexes%2520with%2520binding%2520capabilities%2520from%250Ascratch.%2520It%2520also%2520performs%2520folding%2520and%2520inverse-folding%2520tasks%2520for%2520multi-chain%250Aproteins.%2520Moreover%252C%2520APM%2520demonstrates%2520versatility%2520in%2520downstream%2520applications%253A%2520it%250Aachieves%2520enhanced%2520performance%2520through%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520while%2520also%250Asupporting%2520zero-shot%2520sampling%2520in%2520certain%2520tasks%252C%2520achieving%2520state-of-the-art%250Aresults.%2520Code%2520will%2520be%2520released%2520at%2520https%253A//github.com/bytedance/apm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13075v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20All-Atom%20Generative%20Model%20for%20Designing%20Protein%20Complexes&entry.906535625=Ruizhe%20Chen%20and%20Dongyu%20Xue%20and%20Xiangxin%20Zhou%20and%20Zaixiang%20Zheng%20and%20Xiangxiang%20Zeng%20and%20Quanquan%20Gu&entry.1292438233=%20%20Proteins%20typically%20exist%20in%20complexes%2C%20interacting%20with%20other%20proteins%20or%0Abiomolecules%20to%20perform%20their%20specific%20biological%20roles.%20Research%20on%0Asingle-chain%20protein%20modeling%20has%20been%20extensively%20and%20deeply%20explored%2C%20with%0Aadvancements%20seen%20in%20models%20like%20the%20series%20of%20ESM%20and%20AlphaFold.%20Despite%20these%0Adevelopments%2C%20the%20study%20and%20modeling%20of%20multi-chain%20proteins%20remain%20largely%0Auncharted%2C%20though%20they%20are%20vital%20for%20understanding%20biological%20functions.%0ARecognizing%20the%20importance%20of%20these%20interactions%2C%20we%20introduce%20APM%20%28All-Atom%0AProtein%20Generative%20Model%29%2C%20a%20model%20specifically%20designed%20for%20modeling%0Amulti-chain%20proteins.%20By%20integrating%20atom-level%20information%20and%20leveraging%20data%0Aon%20multi-chain%20proteins%2C%20APM%20is%20capable%20of%20precisely%20modeling%20inter-chain%0Ainteractions%20and%20designing%20protein%20complexes%20with%20binding%20capabilities%20from%0Ascratch.%20It%20also%20performs%20folding%20and%20inverse-folding%20tasks%20for%20multi-chain%0Aproteins.%20Moreover%2C%20APM%20demonstrates%20versatility%20in%20downstream%20applications%3A%20it%0Aachieves%20enhanced%20performance%20through%20supervised%20fine-tuning%20%28SFT%29%20while%20also%0Asupporting%20zero-shot%20sampling%20in%20certain%20tasks%2C%20achieving%20state-of-the-art%0Aresults.%20Code%20will%20be%20released%20at%20https%3A//github.com/bytedance/apm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13075v1&entry.124074799=Read"},
{"title": "Can Masked Autoencoders Also Listen to Birds?", "author": "Lukas Rauch and Ilyass Moummad and Ren\u00e9 Heinrich and Alexis Joly and Bernhard Sick and Christoph Scholz", "abstract": "  Masked Autoencoders (MAEs) pretrained on AudioSet fail to capture the\nfine-grained acoustic characteristics of specialized domains such as\nbioacoustic monitoring. Bird sound classification is critical for assessing\nenvironmental health, yet general-purpose models inadequately address its\nunique acoustic challenges. To address this, we introduce Bird-MAE, a\ndomain-specialized MAE pretrained on the large-scale BirdSet dataset. We\nexplore adjustments to pretraining, fine-tuning and utilizing frozen\nrepresentations. Bird-MAE achieves state-of-the-art results across all BirdSet\ndownstream tasks, substantially improving multi-label classification\nperformance compared to the general-purpose Audio-MAE baseline. Additionally,\nwe propose prototypical probing, a parameter-efficient method for leveraging\nMAEs' frozen representations. Bird-MAE's prototypical probes outperform linear\nprobing by up to 37\\% in MAP and narrow the gap to fine-tuning to approximately\n3\\% on average on BirdSet.\n", "link": "http://arxiv.org/abs/2504.12880v1", "date": "2025-04-17", "relevancy": 2.4146, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5044}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4723}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Masked%20Autoencoders%20Also%20Listen%20to%20Birds%3F&body=Title%3A%20Can%20Masked%20Autoencoders%20Also%20Listen%20to%20Birds%3F%0AAuthor%3A%20Lukas%20Rauch%20and%20Ilyass%20Moummad%20and%20Ren%C3%A9%20Heinrich%20and%20Alexis%20Joly%20and%20Bernhard%20Sick%20and%20Christoph%20Scholz%0AAbstract%3A%20%20%20Masked%20Autoencoders%20%28MAEs%29%20pretrained%20on%20AudioSet%20fail%20to%20capture%20the%0Afine-grained%20acoustic%20characteristics%20of%20specialized%20domains%20such%20as%0Abioacoustic%20monitoring.%20Bird%20sound%20classification%20is%20critical%20for%20assessing%0Aenvironmental%20health%2C%20yet%20general-purpose%20models%20inadequately%20address%20its%0Aunique%20acoustic%20challenges.%20To%20address%20this%2C%20we%20introduce%20Bird-MAE%2C%20a%0Adomain-specialized%20MAE%20pretrained%20on%20the%20large-scale%20BirdSet%20dataset.%20We%0Aexplore%20adjustments%20to%20pretraining%2C%20fine-tuning%20and%20utilizing%20frozen%0Arepresentations.%20Bird-MAE%20achieves%20state-of-the-art%20results%20across%20all%20BirdSet%0Adownstream%20tasks%2C%20substantially%20improving%20multi-label%20classification%0Aperformance%20compared%20to%20the%20general-purpose%20Audio-MAE%20baseline.%20Additionally%2C%0Awe%20propose%20prototypical%20probing%2C%20a%20parameter-efficient%20method%20for%20leveraging%0AMAEs%27%20frozen%20representations.%20Bird-MAE%27s%20prototypical%20probes%20outperform%20linear%0Aprobing%20by%20up%20to%2037%5C%25%20in%20MAP%20and%20narrow%20the%20gap%20to%20fine-tuning%20to%20approximately%0A3%5C%25%20on%20average%20on%20BirdSet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12880v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Masked%2520Autoencoders%2520Also%2520Listen%2520to%2520Birds%253F%26entry.906535625%3DLukas%2520Rauch%2520and%2520Ilyass%2520Moummad%2520and%2520Ren%25C3%25A9%2520Heinrich%2520and%2520Alexis%2520Joly%2520and%2520Bernhard%2520Sick%2520and%2520Christoph%2520Scholz%26entry.1292438233%3D%2520%2520Masked%2520Autoencoders%2520%2528MAEs%2529%2520pretrained%2520on%2520AudioSet%2520fail%2520to%2520capture%2520the%250Afine-grained%2520acoustic%2520characteristics%2520of%2520specialized%2520domains%2520such%2520as%250Abioacoustic%2520monitoring.%2520Bird%2520sound%2520classification%2520is%2520critical%2520for%2520assessing%250Aenvironmental%2520health%252C%2520yet%2520general-purpose%2520models%2520inadequately%2520address%2520its%250Aunique%2520acoustic%2520challenges.%2520To%2520address%2520this%252C%2520we%2520introduce%2520Bird-MAE%252C%2520a%250Adomain-specialized%2520MAE%2520pretrained%2520on%2520the%2520large-scale%2520BirdSet%2520dataset.%2520We%250Aexplore%2520adjustments%2520to%2520pretraining%252C%2520fine-tuning%2520and%2520utilizing%2520frozen%250Arepresentations.%2520Bird-MAE%2520achieves%2520state-of-the-art%2520results%2520across%2520all%2520BirdSet%250Adownstream%2520tasks%252C%2520substantially%2520improving%2520multi-label%2520classification%250Aperformance%2520compared%2520to%2520the%2520general-purpose%2520Audio-MAE%2520baseline.%2520Additionally%252C%250Awe%2520propose%2520prototypical%2520probing%252C%2520a%2520parameter-efficient%2520method%2520for%2520leveraging%250AMAEs%2527%2520frozen%2520representations.%2520Bird-MAE%2527s%2520prototypical%2520probes%2520outperform%2520linear%250Aprobing%2520by%2520up%2520to%252037%255C%2525%2520in%2520MAP%2520and%2520narrow%2520the%2520gap%2520to%2520fine-tuning%2520to%2520approximately%250A3%255C%2525%2520on%2520average%2520on%2520BirdSet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12880v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Masked%20Autoencoders%20Also%20Listen%20to%20Birds%3F&entry.906535625=Lukas%20Rauch%20and%20Ilyass%20Moummad%20and%20Ren%C3%A9%20Heinrich%20and%20Alexis%20Joly%20and%20Bernhard%20Sick%20and%20Christoph%20Scholz&entry.1292438233=%20%20Masked%20Autoencoders%20%28MAEs%29%20pretrained%20on%20AudioSet%20fail%20to%20capture%20the%0Afine-grained%20acoustic%20characteristics%20of%20specialized%20domains%20such%20as%0Abioacoustic%20monitoring.%20Bird%20sound%20classification%20is%20critical%20for%20assessing%0Aenvironmental%20health%2C%20yet%20general-purpose%20models%20inadequately%20address%20its%0Aunique%20acoustic%20challenges.%20To%20address%20this%2C%20we%20introduce%20Bird-MAE%2C%20a%0Adomain-specialized%20MAE%20pretrained%20on%20the%20large-scale%20BirdSet%20dataset.%20We%0Aexplore%20adjustments%20to%20pretraining%2C%20fine-tuning%20and%20utilizing%20frozen%0Arepresentations.%20Bird-MAE%20achieves%20state-of-the-art%20results%20across%20all%20BirdSet%0Adownstream%20tasks%2C%20substantially%20improving%20multi-label%20classification%0Aperformance%20compared%20to%20the%20general-purpose%20Audio-MAE%20baseline.%20Additionally%2C%0Awe%20propose%20prototypical%20probing%2C%20a%20parameter-efficient%20method%20for%20leveraging%0AMAEs%27%20frozen%20representations.%20Bird-MAE%27s%20prototypical%20probes%20outperform%20linear%0Aprobing%20by%20up%20to%2037%5C%25%20in%20MAP%20and%20narrow%20the%20gap%20to%20fine-tuning%20to%20approximately%0A3%5C%25%20on%20average%20on%20BirdSet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12880v1&entry.124074799=Read"},
{"title": "Self-Supervised Pre-training with Combined Datasets for 3D Perception in\n  Autonomous Driving", "author": "Shumin Wang and Zhuoran Yang and Lidian Wang and Zhipeng Tang and Heng Li and Lehan Pan and Sha Zhang and Jie Peng and Jianmin Ji and Yanyong Zhang", "abstract": "  The significant achievements of pre-trained models leveraging large volumes\nof data in the field of NLP and 2D vision inspire us to explore the potential\nof extensive data pre-training for 3D perception in autonomous driving. Toward\nthis goal, this paper proposes to utilize massive unlabeled data from\nheterogeneous datasets to pre-train 3D perception models. We introduce a\nself-supervised pre-training framework that learns effective 3D representations\nfrom scratch on unlabeled data, combined with a prompt adapter based domain\nadaptation strategy to reduce dataset bias. The approach significantly improves\nmodel performance on downstream tasks such as 3D object detection, BEV\nsegmentation, 3D object tracking, and occupancy prediction, and shows steady\nperformance increase as the training data volume scales up, demonstrating the\npotential of continually benefit 3D perception models for autonomous driving.\nWe will release the source code to inspire further investigations in the\ncommunity.\n", "link": "http://arxiv.org/abs/2504.12709v1", "date": "2025-04-17", "relevancy": 2.4114, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6092}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6023}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Pre-training%20with%20Combined%20Datasets%20for%203D%20Perception%20in%0A%20%20Autonomous%20Driving&body=Title%3A%20Self-Supervised%20Pre-training%20with%20Combined%20Datasets%20for%203D%20Perception%20in%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Shumin%20Wang%20and%20Zhuoran%20Yang%20and%20Lidian%20Wang%20and%20Zhipeng%20Tang%20and%20Heng%20Li%20and%20Lehan%20Pan%20and%20Sha%20Zhang%20and%20Jie%20Peng%20and%20Jianmin%20Ji%20and%20Yanyong%20Zhang%0AAbstract%3A%20%20%20The%20significant%20achievements%20of%20pre-trained%20models%20leveraging%20large%20volumes%0Aof%20data%20in%20the%20field%20of%20NLP%20and%202D%20vision%20inspire%20us%20to%20explore%20the%20potential%0Aof%20extensive%20data%20pre-training%20for%203D%20perception%20in%20autonomous%20driving.%20Toward%0Athis%20goal%2C%20this%20paper%20proposes%20to%20utilize%20massive%20unlabeled%20data%20from%0Aheterogeneous%20datasets%20to%20pre-train%203D%20perception%20models.%20We%20introduce%20a%0Aself-supervised%20pre-training%20framework%20that%20learns%20effective%203D%20representations%0Afrom%20scratch%20on%20unlabeled%20data%2C%20combined%20with%20a%20prompt%20adapter%20based%20domain%0Aadaptation%20strategy%20to%20reduce%20dataset%20bias.%20The%20approach%20significantly%20improves%0Amodel%20performance%20on%20downstream%20tasks%20such%20as%203D%20object%20detection%2C%20BEV%0Asegmentation%2C%203D%20object%20tracking%2C%20and%20occupancy%20prediction%2C%20and%20shows%20steady%0Aperformance%20increase%20as%20the%20training%20data%20volume%20scales%20up%2C%20demonstrating%20the%0Apotential%20of%20continually%20benefit%203D%20perception%20models%20for%20autonomous%20driving.%0AWe%20will%20release%20the%20source%20code%20to%20inspire%20further%20investigations%20in%20the%0Acommunity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12709v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Pre-training%2520with%2520Combined%2520Datasets%2520for%25203D%2520Perception%2520in%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DShumin%2520Wang%2520and%2520Zhuoran%2520Yang%2520and%2520Lidian%2520Wang%2520and%2520Zhipeng%2520Tang%2520and%2520Heng%2520Li%2520and%2520Lehan%2520Pan%2520and%2520Sha%2520Zhang%2520and%2520Jie%2520Peng%2520and%2520Jianmin%2520Ji%2520and%2520Yanyong%2520Zhang%26entry.1292438233%3D%2520%2520The%2520significant%2520achievements%2520of%2520pre-trained%2520models%2520leveraging%2520large%2520volumes%250Aof%2520data%2520in%2520the%2520field%2520of%2520NLP%2520and%25202D%2520vision%2520inspire%2520us%2520to%2520explore%2520the%2520potential%250Aof%2520extensive%2520data%2520pre-training%2520for%25203D%2520perception%2520in%2520autonomous%2520driving.%2520Toward%250Athis%2520goal%252C%2520this%2520paper%2520proposes%2520to%2520utilize%2520massive%2520unlabeled%2520data%2520from%250Aheterogeneous%2520datasets%2520to%2520pre-train%25203D%2520perception%2520models.%2520We%2520introduce%2520a%250Aself-supervised%2520pre-training%2520framework%2520that%2520learns%2520effective%25203D%2520representations%250Afrom%2520scratch%2520on%2520unlabeled%2520data%252C%2520combined%2520with%2520a%2520prompt%2520adapter%2520based%2520domain%250Aadaptation%2520strategy%2520to%2520reduce%2520dataset%2520bias.%2520The%2520approach%2520significantly%2520improves%250Amodel%2520performance%2520on%2520downstream%2520tasks%2520such%2520as%25203D%2520object%2520detection%252C%2520BEV%250Asegmentation%252C%25203D%2520object%2520tracking%252C%2520and%2520occupancy%2520prediction%252C%2520and%2520shows%2520steady%250Aperformance%2520increase%2520as%2520the%2520training%2520data%2520volume%2520scales%2520up%252C%2520demonstrating%2520the%250Apotential%2520of%2520continually%2520benefit%25203D%2520perception%2520models%2520for%2520autonomous%2520driving.%250AWe%2520will%2520release%2520the%2520source%2520code%2520to%2520inspire%2520further%2520investigations%2520in%2520the%250Acommunity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12709v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Pre-training%20with%20Combined%20Datasets%20for%203D%20Perception%20in%0A%20%20Autonomous%20Driving&entry.906535625=Shumin%20Wang%20and%20Zhuoran%20Yang%20and%20Lidian%20Wang%20and%20Zhipeng%20Tang%20and%20Heng%20Li%20and%20Lehan%20Pan%20and%20Sha%20Zhang%20and%20Jie%20Peng%20and%20Jianmin%20Ji%20and%20Yanyong%20Zhang&entry.1292438233=%20%20The%20significant%20achievements%20of%20pre-trained%20models%20leveraging%20large%20volumes%0Aof%20data%20in%20the%20field%20of%20NLP%20and%202D%20vision%20inspire%20us%20to%20explore%20the%20potential%0Aof%20extensive%20data%20pre-training%20for%203D%20perception%20in%20autonomous%20driving.%20Toward%0Athis%20goal%2C%20this%20paper%20proposes%20to%20utilize%20massive%20unlabeled%20data%20from%0Aheterogeneous%20datasets%20to%20pre-train%203D%20perception%20models.%20We%20introduce%20a%0Aself-supervised%20pre-training%20framework%20that%20learns%20effective%203D%20representations%0Afrom%20scratch%20on%20unlabeled%20data%2C%20combined%20with%20a%20prompt%20adapter%20based%20domain%0Aadaptation%20strategy%20to%20reduce%20dataset%20bias.%20The%20approach%20significantly%20improves%0Amodel%20performance%20on%20downstream%20tasks%20such%20as%203D%20object%20detection%2C%20BEV%0Asegmentation%2C%203D%20object%20tracking%2C%20and%20occupancy%20prediction%2C%20and%20shows%20steady%0Aperformance%20increase%20as%20the%20training%20data%20volume%20scales%20up%2C%20demonstrating%20the%0Apotential%20of%20continually%20benefit%203D%20perception%20models%20for%20autonomous%20driving.%0AWe%20will%20release%20the%20source%20code%20to%20inspire%20further%20investigations%20in%20the%0Acommunity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12709v1&entry.124074799=Read"},
{"title": "Featuremetric benchmarking: Quantum computer benchmarks based on circuit\n  features", "author": "Timothy Proctor and Anh Tran and Xingxin Liu and Aditya Dhumuntarao and Stefan Seritan and Alaina Green and Norbert M Linke", "abstract": "  Benchmarks that concisely summarize the performance of many-qubit quantum\ncomputers are essential for measuring progress towards the goal of useful\nquantum computation. In this work, we present a benchmarking framework that is\nbased on quantifying how a quantum computer's performance on quantum circuits\nvaries as a function of features of those circuits, such as circuit depth,\nwidth, two-qubit gate density, problem input size, or algorithmic depth. Our\nfeaturemetric benchmarking framework generalizes volumetric benchmarking -- a\nwidely-used methodology that quantifies performance versus circuit width and\ndepth -- and we show that it enables richer and more faithful models of quantum\ncomputer performance. We demonstrate featuremetric benchmarking with example\nbenchmarks run on IBM Q and IonQ systems of up to 27 qubits, and we show how to\nproduce performance summaries from the data using Gaussian process regression.\nOur data analysis methods are also of interest in the special case of\nvolumetric benchmarking, as they enable the creation of intuitive\ntwo-dimensional capability regions using data from few circuits.\n", "link": "http://arxiv.org/abs/2504.12575v1", "date": "2025-04-17", "relevancy": 1.5933, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4003}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4003}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Featuremetric%20benchmarking%3A%20Quantum%20computer%20benchmarks%20based%20on%20circuit%0A%20%20features&body=Title%3A%20Featuremetric%20benchmarking%3A%20Quantum%20computer%20benchmarks%20based%20on%20circuit%0A%20%20features%0AAuthor%3A%20Timothy%20Proctor%20and%20Anh%20Tran%20and%20Xingxin%20Liu%20and%20Aditya%20Dhumuntarao%20and%20Stefan%20Seritan%20and%20Alaina%20Green%20and%20Norbert%20M%20Linke%0AAbstract%3A%20%20%20Benchmarks%20that%20concisely%20summarize%20the%20performance%20of%20many-qubit%20quantum%0Acomputers%20are%20essential%20for%20measuring%20progress%20towards%20the%20goal%20of%20useful%0Aquantum%20computation.%20In%20this%20work%2C%20we%20present%20a%20benchmarking%20framework%20that%20is%0Abased%20on%20quantifying%20how%20a%20quantum%20computer%27s%20performance%20on%20quantum%20circuits%0Avaries%20as%20a%20function%20of%20features%20of%20those%20circuits%2C%20such%20as%20circuit%20depth%2C%0Awidth%2C%20two-qubit%20gate%20density%2C%20problem%20input%20size%2C%20or%20algorithmic%20depth.%20Our%0Afeaturemetric%20benchmarking%20framework%20generalizes%20volumetric%20benchmarking%20--%20a%0Awidely-used%20methodology%20that%20quantifies%20performance%20versus%20circuit%20width%20and%0Adepth%20--%20and%20we%20show%20that%20it%20enables%20richer%20and%20more%20faithful%20models%20of%20quantum%0Acomputer%20performance.%20We%20demonstrate%20featuremetric%20benchmarking%20with%20example%0Abenchmarks%20run%20on%20IBM%20Q%20and%20IonQ%20systems%20of%20up%20to%2027%20qubits%2C%20and%20we%20show%20how%20to%0Aproduce%20performance%20summaries%20from%20the%20data%20using%20Gaussian%20process%20regression.%0AOur%20data%20analysis%20methods%20are%20also%20of%20interest%20in%20the%20special%20case%20of%0Avolumetric%20benchmarking%2C%20as%20they%20enable%20the%20creation%20of%20intuitive%0Atwo-dimensional%20capability%20regions%20using%20data%20from%20few%20circuits.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12575v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeaturemetric%2520benchmarking%253A%2520Quantum%2520computer%2520benchmarks%2520based%2520on%2520circuit%250A%2520%2520features%26entry.906535625%3DTimothy%2520Proctor%2520and%2520Anh%2520Tran%2520and%2520Xingxin%2520Liu%2520and%2520Aditya%2520Dhumuntarao%2520and%2520Stefan%2520Seritan%2520and%2520Alaina%2520Green%2520and%2520Norbert%2520M%2520Linke%26entry.1292438233%3D%2520%2520Benchmarks%2520that%2520concisely%2520summarize%2520the%2520performance%2520of%2520many-qubit%2520quantum%250Acomputers%2520are%2520essential%2520for%2520measuring%2520progress%2520towards%2520the%2520goal%2520of%2520useful%250Aquantum%2520computation.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520benchmarking%2520framework%2520that%2520is%250Abased%2520on%2520quantifying%2520how%2520a%2520quantum%2520computer%2527s%2520performance%2520on%2520quantum%2520circuits%250Avaries%2520as%2520a%2520function%2520of%2520features%2520of%2520those%2520circuits%252C%2520such%2520as%2520circuit%2520depth%252C%250Awidth%252C%2520two-qubit%2520gate%2520density%252C%2520problem%2520input%2520size%252C%2520or%2520algorithmic%2520depth.%2520Our%250Afeaturemetric%2520benchmarking%2520framework%2520generalizes%2520volumetric%2520benchmarking%2520--%2520a%250Awidely-used%2520methodology%2520that%2520quantifies%2520performance%2520versus%2520circuit%2520width%2520and%250Adepth%2520--%2520and%2520we%2520show%2520that%2520it%2520enables%2520richer%2520and%2520more%2520faithful%2520models%2520of%2520quantum%250Acomputer%2520performance.%2520We%2520demonstrate%2520featuremetric%2520benchmarking%2520with%2520example%250Abenchmarks%2520run%2520on%2520IBM%2520Q%2520and%2520IonQ%2520systems%2520of%2520up%2520to%252027%2520qubits%252C%2520and%2520we%2520show%2520how%2520to%250Aproduce%2520performance%2520summaries%2520from%2520the%2520data%2520using%2520Gaussian%2520process%2520regression.%250AOur%2520data%2520analysis%2520methods%2520are%2520also%2520of%2520interest%2520in%2520the%2520special%2520case%2520of%250Avolumetric%2520benchmarking%252C%2520as%2520they%2520enable%2520the%2520creation%2520of%2520intuitive%250Atwo-dimensional%2520capability%2520regions%2520using%2520data%2520from%2520few%2520circuits.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12575v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Featuremetric%20benchmarking%3A%20Quantum%20computer%20benchmarks%20based%20on%20circuit%0A%20%20features&entry.906535625=Timothy%20Proctor%20and%20Anh%20Tran%20and%20Xingxin%20Liu%20and%20Aditya%20Dhumuntarao%20and%20Stefan%20Seritan%20and%20Alaina%20Green%20and%20Norbert%20M%20Linke&entry.1292438233=%20%20Benchmarks%20that%20concisely%20summarize%20the%20performance%20of%20many-qubit%20quantum%0Acomputers%20are%20essential%20for%20measuring%20progress%20towards%20the%20goal%20of%20useful%0Aquantum%20computation.%20In%20this%20work%2C%20we%20present%20a%20benchmarking%20framework%20that%20is%0Abased%20on%20quantifying%20how%20a%20quantum%20computer%27s%20performance%20on%20quantum%20circuits%0Avaries%20as%20a%20function%20of%20features%20of%20those%20circuits%2C%20such%20as%20circuit%20depth%2C%0Awidth%2C%20two-qubit%20gate%20density%2C%20problem%20input%20size%2C%20or%20algorithmic%20depth.%20Our%0Afeaturemetric%20benchmarking%20framework%20generalizes%20volumetric%20benchmarking%20--%20a%0Awidely-used%20methodology%20that%20quantifies%20performance%20versus%20circuit%20width%20and%0Adepth%20--%20and%20we%20show%20that%20it%20enables%20richer%20and%20more%20faithful%20models%20of%20quantum%0Acomputer%20performance.%20We%20demonstrate%20featuremetric%20benchmarking%20with%20example%0Abenchmarks%20run%20on%20IBM%20Q%20and%20IonQ%20systems%20of%20up%20to%2027%20qubits%2C%20and%20we%20show%20how%20to%0Aproduce%20performance%20summaries%20from%20the%20data%20using%20Gaussian%20process%20regression.%0AOur%20data%20analysis%20methods%20are%20also%20of%20interest%20in%20the%20special%20case%20of%0Avolumetric%20benchmarking%2C%20as%20they%20enable%20the%20creation%20of%20intuitive%0Atwo-dimensional%20capability%20regions%20using%20data%20from%20few%20circuits.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12575v1&entry.124074799=Read"},
{"title": "EventVAD: Training-Free Event-Aware Video Anomaly Detection", "author": "Yihua Shao and Haojin He and Sijie Li and Siyu Chen and Xinwei Long and Fanhu Zeng and Yuxuan Fan and Muyang Zhang and Ziyang Yan and Ao Ma and Xiaochen Wang and Hao Tang and Yan Wang and Shuyan Li", "abstract": "  Video Anomaly Detection~(VAD) focuses on identifying anomalies within videos.\nSupervised methods require an amount of in-domain training data and often\nstruggle to generalize to unseen anomalies. In contrast, training-free methods\nleverage the intrinsic world knowledge of large language models (LLMs) to\ndetect anomalies but face challenges in localizing fine-grained visual\ntransitions and diverse events. Therefore, we propose EventVAD, an event-aware\nvideo anomaly detection framework that combines tailored dynamic graph\narchitectures and multimodal LLMs through temporal-event reasoning.\nSpecifically, EventVAD first employs dynamic spatiotemporal graph modeling with\ntime-decay constraints to capture event-aware video features. Then, it performs\nadaptive noise filtering and uses signal ratio thresholding to detect event\nboundaries via unsupervised statistical features. The statistical boundary\ndetection module reduces the complexity of processing long videos for MLLMs and\nimproves their temporal reasoning through event consistency. Finally, it\nutilizes a hierarchical prompting strategy to guide MLLMs in performing\nreasoning before determining final decisions. We conducted extensive\nexperiments on the UCF-Crime and XD-Violence datasets. The results demonstrate\nthat EventVAD with a 7B MLLM achieves state-of-the-art (SOTA) in training-free\nsettings, outperforming strong baselines that use 7B or larger MLLMs.\n", "link": "http://arxiv.org/abs/2504.13092v1", "date": "2025-04-17", "relevancy": 1.6838, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5744}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5483}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EventVAD%3A%20Training-Free%20Event-Aware%20Video%20Anomaly%20Detection&body=Title%3A%20EventVAD%3A%20Training-Free%20Event-Aware%20Video%20Anomaly%20Detection%0AAuthor%3A%20Yihua%20Shao%20and%20Haojin%20He%20and%20Sijie%20Li%20and%20Siyu%20Chen%20and%20Xinwei%20Long%20and%20Fanhu%20Zeng%20and%20Yuxuan%20Fan%20and%20Muyang%20Zhang%20and%20Ziyang%20Yan%20and%20Ao%20Ma%20and%20Xiaochen%20Wang%20and%20Hao%20Tang%20and%20Yan%20Wang%20and%20Shuyan%20Li%0AAbstract%3A%20%20%20Video%20Anomaly%20Detection~%28VAD%29%20focuses%20on%20identifying%20anomalies%20within%20videos.%0ASupervised%20methods%20require%20an%20amount%20of%20in-domain%20training%20data%20and%20often%0Astruggle%20to%20generalize%20to%20unseen%20anomalies.%20In%20contrast%2C%20training-free%20methods%0Aleverage%20the%20intrinsic%20world%20knowledge%20of%20large%20language%20models%20%28LLMs%29%20to%0Adetect%20anomalies%20but%20face%20challenges%20in%20localizing%20fine-grained%20visual%0Atransitions%20and%20diverse%20events.%20Therefore%2C%20we%20propose%20EventVAD%2C%20an%20event-aware%0Avideo%20anomaly%20detection%20framework%20that%20combines%20tailored%20dynamic%20graph%0Aarchitectures%20and%20multimodal%20LLMs%20through%20temporal-event%20reasoning.%0ASpecifically%2C%20EventVAD%20first%20employs%20dynamic%20spatiotemporal%20graph%20modeling%20with%0Atime-decay%20constraints%20to%20capture%20event-aware%20video%20features.%20Then%2C%20it%20performs%0Aadaptive%20noise%20filtering%20and%20uses%20signal%20ratio%20thresholding%20to%20detect%20event%0Aboundaries%20via%20unsupervised%20statistical%20features.%20The%20statistical%20boundary%0Adetection%20module%20reduces%20the%20complexity%20of%20processing%20long%20videos%20for%20MLLMs%20and%0Aimproves%20their%20temporal%20reasoning%20through%20event%20consistency.%20Finally%2C%20it%0Autilizes%20a%20hierarchical%20prompting%20strategy%20to%20guide%20MLLMs%20in%20performing%0Areasoning%20before%20determining%20final%20decisions.%20We%20conducted%20extensive%0Aexperiments%20on%20the%20UCF-Crime%20and%20XD-Violence%20datasets.%20The%20results%20demonstrate%0Athat%20EventVAD%20with%20a%207B%20MLLM%20achieves%20state-of-the-art%20%28SOTA%29%20in%20training-free%0Asettings%2C%20outperforming%20strong%20baselines%20that%20use%207B%20or%20larger%20MLLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13092v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEventVAD%253A%2520Training-Free%2520Event-Aware%2520Video%2520Anomaly%2520Detection%26entry.906535625%3DYihua%2520Shao%2520and%2520Haojin%2520He%2520and%2520Sijie%2520Li%2520and%2520Siyu%2520Chen%2520and%2520Xinwei%2520Long%2520and%2520Fanhu%2520Zeng%2520and%2520Yuxuan%2520Fan%2520and%2520Muyang%2520Zhang%2520and%2520Ziyang%2520Yan%2520and%2520Ao%2520Ma%2520and%2520Xiaochen%2520Wang%2520and%2520Hao%2520Tang%2520and%2520Yan%2520Wang%2520and%2520Shuyan%2520Li%26entry.1292438233%3D%2520%2520Video%2520Anomaly%2520Detection~%2528VAD%2529%2520focuses%2520on%2520identifying%2520anomalies%2520within%2520videos.%250ASupervised%2520methods%2520require%2520an%2520amount%2520of%2520in-domain%2520training%2520data%2520and%2520often%250Astruggle%2520to%2520generalize%2520to%2520unseen%2520anomalies.%2520In%2520contrast%252C%2520training-free%2520methods%250Aleverage%2520the%2520intrinsic%2520world%2520knowledge%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%250Adetect%2520anomalies%2520but%2520face%2520challenges%2520in%2520localizing%2520fine-grained%2520visual%250Atransitions%2520and%2520diverse%2520events.%2520Therefore%252C%2520we%2520propose%2520EventVAD%252C%2520an%2520event-aware%250Avideo%2520anomaly%2520detection%2520framework%2520that%2520combines%2520tailored%2520dynamic%2520graph%250Aarchitectures%2520and%2520multimodal%2520LLMs%2520through%2520temporal-event%2520reasoning.%250ASpecifically%252C%2520EventVAD%2520first%2520employs%2520dynamic%2520spatiotemporal%2520graph%2520modeling%2520with%250Atime-decay%2520constraints%2520to%2520capture%2520event-aware%2520video%2520features.%2520Then%252C%2520it%2520performs%250Aadaptive%2520noise%2520filtering%2520and%2520uses%2520signal%2520ratio%2520thresholding%2520to%2520detect%2520event%250Aboundaries%2520via%2520unsupervised%2520statistical%2520features.%2520The%2520statistical%2520boundary%250Adetection%2520module%2520reduces%2520the%2520complexity%2520of%2520processing%2520long%2520videos%2520for%2520MLLMs%2520and%250Aimproves%2520their%2520temporal%2520reasoning%2520through%2520event%2520consistency.%2520Finally%252C%2520it%250Autilizes%2520a%2520hierarchical%2520prompting%2520strategy%2520to%2520guide%2520MLLMs%2520in%2520performing%250Areasoning%2520before%2520determining%2520final%2520decisions.%2520We%2520conducted%2520extensive%250Aexperiments%2520on%2520the%2520UCF-Crime%2520and%2520XD-Violence%2520datasets.%2520The%2520results%2520demonstrate%250Athat%2520EventVAD%2520with%2520a%25207B%2520MLLM%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%2520in%2520training-free%250Asettings%252C%2520outperforming%2520strong%2520baselines%2520that%2520use%25207B%2520or%2520larger%2520MLLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13092v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EventVAD%3A%20Training-Free%20Event-Aware%20Video%20Anomaly%20Detection&entry.906535625=Yihua%20Shao%20and%20Haojin%20He%20and%20Sijie%20Li%20and%20Siyu%20Chen%20and%20Xinwei%20Long%20and%20Fanhu%20Zeng%20and%20Yuxuan%20Fan%20and%20Muyang%20Zhang%20and%20Ziyang%20Yan%20and%20Ao%20Ma%20and%20Xiaochen%20Wang%20and%20Hao%20Tang%20and%20Yan%20Wang%20and%20Shuyan%20Li&entry.1292438233=%20%20Video%20Anomaly%20Detection~%28VAD%29%20focuses%20on%20identifying%20anomalies%20within%20videos.%0ASupervised%20methods%20require%20an%20amount%20of%20in-domain%20training%20data%20and%20often%0Astruggle%20to%20generalize%20to%20unseen%20anomalies.%20In%20contrast%2C%20training-free%20methods%0Aleverage%20the%20intrinsic%20world%20knowledge%20of%20large%20language%20models%20%28LLMs%29%20to%0Adetect%20anomalies%20but%20face%20challenges%20in%20localizing%20fine-grained%20visual%0Atransitions%20and%20diverse%20events.%20Therefore%2C%20we%20propose%20EventVAD%2C%20an%20event-aware%0Avideo%20anomaly%20detection%20framework%20that%20combines%20tailored%20dynamic%20graph%0Aarchitectures%20and%20multimodal%20LLMs%20through%20temporal-event%20reasoning.%0ASpecifically%2C%20EventVAD%20first%20employs%20dynamic%20spatiotemporal%20graph%20modeling%20with%0Atime-decay%20constraints%20to%20capture%20event-aware%20video%20features.%20Then%2C%20it%20performs%0Aadaptive%20noise%20filtering%20and%20uses%20signal%20ratio%20thresholding%20to%20detect%20event%0Aboundaries%20via%20unsupervised%20statistical%20features.%20The%20statistical%20boundary%0Adetection%20module%20reduces%20the%20complexity%20of%20processing%20long%20videos%20for%20MLLMs%20and%0Aimproves%20their%20temporal%20reasoning%20through%20event%20consistency.%20Finally%2C%20it%0Autilizes%20a%20hierarchical%20prompting%20strategy%20to%20guide%20MLLMs%20in%20performing%0Areasoning%20before%20determining%20final%20decisions.%20We%20conducted%20extensive%0Aexperiments%20on%20the%20UCF-Crime%20and%20XD-Violence%20datasets.%20The%20results%20demonstrate%0Athat%20EventVAD%20with%20a%207B%20MLLM%20achieves%20state-of-the-art%20%28SOTA%29%20in%20training-free%0Asettings%2C%20outperforming%20strong%20baselines%20that%20use%207B%20or%20larger%20MLLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13092v1&entry.124074799=Read"},
{"title": "Are Retrials All You Need? Enhancing Large Language Model Reasoning\n  Without Verbalized Feedback", "author": "Nearchos Potamitis and Akhil Arora", "abstract": "  Recent advancements in large language models (LLMs) have catalyzed the\ndevelopment of general-purpose autonomous agents, demonstrating remarkable\nperformance in complex reasoning tasks across various domains. This surge has\nspurred the evolution of a plethora of prompt-based reasoning frameworks. A\nrecent focus has been on iterative reasoning strategies that refine outputs\nthrough self-evaluation and verbalized feedback. However, these strategies\nrequire additional computational complexity to enable models to recognize and\ncorrect their mistakes, leading to a significant increase in their cost. In\nthis work, we introduce the concept of ``retrials without feedback'', an\nembarrassingly simple yet powerful mechanism for enhancing reasoning frameworks\nby allowing LLMs to retry problem-solving attempts upon identifying incorrect\nanswers. Unlike conventional iterative refinement methods, our method does not\nrequire explicit self-reflection or verbalized feedback, simplifying the\nrefinement process. Our findings indicate that simpler retrial-based approaches\noften outperform more sophisticated reasoning frameworks, suggesting that the\nbenefits of complex methods may not always justify their computational costs.\nBy challenging the prevailing assumption that more intricate reasoning\nstrategies inherently lead to better performance, our work offers new insights\ninto how simpler, more efficient approaches can achieve optimal results. So,\nare retrials all you need?\n", "link": "http://arxiv.org/abs/2504.12951v1", "date": "2025-04-17", "relevancy": 1.9989, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5063}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5063}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Retrials%20All%20You%20Need%3F%20Enhancing%20Large%20Language%20Model%20Reasoning%0A%20%20Without%20Verbalized%20Feedback&body=Title%3A%20Are%20Retrials%20All%20You%20Need%3F%20Enhancing%20Large%20Language%20Model%20Reasoning%0A%20%20Without%20Verbalized%20Feedback%0AAuthor%3A%20Nearchos%20Potamitis%20and%20Akhil%20Arora%0AAbstract%3A%20%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20catalyzed%20the%0Adevelopment%20of%20general-purpose%20autonomous%20agents%2C%20demonstrating%20remarkable%0Aperformance%20in%20complex%20reasoning%20tasks%20across%20various%20domains.%20This%20surge%20has%0Aspurred%20the%20evolution%20of%20a%20plethora%20of%20prompt-based%20reasoning%20frameworks.%20A%0Arecent%20focus%20has%20been%20on%20iterative%20reasoning%20strategies%20that%20refine%20outputs%0Athrough%20self-evaluation%20and%20verbalized%20feedback.%20However%2C%20these%20strategies%0Arequire%20additional%20computational%20complexity%20to%20enable%20models%20to%20recognize%20and%0Acorrect%20their%20mistakes%2C%20leading%20to%20a%20significant%20increase%20in%20their%20cost.%20In%0Athis%20work%2C%20we%20introduce%20the%20concept%20of%20%60%60retrials%20without%20feedback%27%27%2C%20an%0Aembarrassingly%20simple%20yet%20powerful%20mechanism%20for%20enhancing%20reasoning%20frameworks%0Aby%20allowing%20LLMs%20to%20retry%20problem-solving%20attempts%20upon%20identifying%20incorrect%0Aanswers.%20Unlike%20conventional%20iterative%20refinement%20methods%2C%20our%20method%20does%20not%0Arequire%20explicit%20self-reflection%20or%20verbalized%20feedback%2C%20simplifying%20the%0Arefinement%20process.%20Our%20findings%20indicate%20that%20simpler%20retrial-based%20approaches%0Aoften%20outperform%20more%20sophisticated%20reasoning%20frameworks%2C%20suggesting%20that%20the%0Abenefits%20of%20complex%20methods%20may%20not%20always%20justify%20their%20computational%20costs.%0ABy%20challenging%20the%20prevailing%20assumption%20that%20more%20intricate%20reasoning%0Astrategies%20inherently%20lead%20to%20better%20performance%2C%20our%20work%20offers%20new%20insights%0Ainto%20how%20simpler%2C%20more%20efficient%20approaches%20can%20achieve%20optimal%20results.%20So%2C%0Aare%20retrials%20all%20you%20need%3F%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12951v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Retrials%2520All%2520You%2520Need%253F%2520Enhancing%2520Large%2520Language%2520Model%2520Reasoning%250A%2520%2520Without%2520Verbalized%2520Feedback%26entry.906535625%3DNearchos%2520Potamitis%2520and%2520Akhil%2520Arora%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520catalyzed%2520the%250Adevelopment%2520of%2520general-purpose%2520autonomous%2520agents%252C%2520demonstrating%2520remarkable%250Aperformance%2520in%2520complex%2520reasoning%2520tasks%2520across%2520various%2520domains.%2520This%2520surge%2520has%250Aspurred%2520the%2520evolution%2520of%2520a%2520plethora%2520of%2520prompt-based%2520reasoning%2520frameworks.%2520A%250Arecent%2520focus%2520has%2520been%2520on%2520iterative%2520reasoning%2520strategies%2520that%2520refine%2520outputs%250Athrough%2520self-evaluation%2520and%2520verbalized%2520feedback.%2520However%252C%2520these%2520strategies%250Arequire%2520additional%2520computational%2520complexity%2520to%2520enable%2520models%2520to%2520recognize%2520and%250Acorrect%2520their%2520mistakes%252C%2520leading%2520to%2520a%2520significant%2520increase%2520in%2520their%2520cost.%2520In%250Athis%2520work%252C%2520we%2520introduce%2520the%2520concept%2520of%2520%2560%2560retrials%2520without%2520feedback%2527%2527%252C%2520an%250Aembarrassingly%2520simple%2520yet%2520powerful%2520mechanism%2520for%2520enhancing%2520reasoning%2520frameworks%250Aby%2520allowing%2520LLMs%2520to%2520retry%2520problem-solving%2520attempts%2520upon%2520identifying%2520incorrect%250Aanswers.%2520Unlike%2520conventional%2520iterative%2520refinement%2520methods%252C%2520our%2520method%2520does%2520not%250Arequire%2520explicit%2520self-reflection%2520or%2520verbalized%2520feedback%252C%2520simplifying%2520the%250Arefinement%2520process.%2520Our%2520findings%2520indicate%2520that%2520simpler%2520retrial-based%2520approaches%250Aoften%2520outperform%2520more%2520sophisticated%2520reasoning%2520frameworks%252C%2520suggesting%2520that%2520the%250Abenefits%2520of%2520complex%2520methods%2520may%2520not%2520always%2520justify%2520their%2520computational%2520costs.%250ABy%2520challenging%2520the%2520prevailing%2520assumption%2520that%2520more%2520intricate%2520reasoning%250Astrategies%2520inherently%2520lead%2520to%2520better%2520performance%252C%2520our%2520work%2520offers%2520new%2520insights%250Ainto%2520how%2520simpler%252C%2520more%2520efficient%2520approaches%2520can%2520achieve%2520optimal%2520results.%2520So%252C%250Aare%2520retrials%2520all%2520you%2520need%253F%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12951v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Retrials%20All%20You%20Need%3F%20Enhancing%20Large%20Language%20Model%20Reasoning%0A%20%20Without%20Verbalized%20Feedback&entry.906535625=Nearchos%20Potamitis%20and%20Akhil%20Arora&entry.1292438233=%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20catalyzed%20the%0Adevelopment%20of%20general-purpose%20autonomous%20agents%2C%20demonstrating%20remarkable%0Aperformance%20in%20complex%20reasoning%20tasks%20across%20various%20domains.%20This%20surge%20has%0Aspurred%20the%20evolution%20of%20a%20plethora%20of%20prompt-based%20reasoning%20frameworks.%20A%0Arecent%20focus%20has%20been%20on%20iterative%20reasoning%20strategies%20that%20refine%20outputs%0Athrough%20self-evaluation%20and%20verbalized%20feedback.%20However%2C%20these%20strategies%0Arequire%20additional%20computational%20complexity%20to%20enable%20models%20to%20recognize%20and%0Acorrect%20their%20mistakes%2C%20leading%20to%20a%20significant%20increase%20in%20their%20cost.%20In%0Athis%20work%2C%20we%20introduce%20the%20concept%20of%20%60%60retrials%20without%20feedback%27%27%2C%20an%0Aembarrassingly%20simple%20yet%20powerful%20mechanism%20for%20enhancing%20reasoning%20frameworks%0Aby%20allowing%20LLMs%20to%20retry%20problem-solving%20attempts%20upon%20identifying%20incorrect%0Aanswers.%20Unlike%20conventional%20iterative%20refinement%20methods%2C%20our%20method%20does%20not%0Arequire%20explicit%20self-reflection%20or%20verbalized%20feedback%2C%20simplifying%20the%0Arefinement%20process.%20Our%20findings%20indicate%20that%20simpler%20retrial-based%20approaches%0Aoften%20outperform%20more%20sophisticated%20reasoning%20frameworks%2C%20suggesting%20that%20the%0Abenefits%20of%20complex%20methods%20may%20not%20always%20justify%20their%20computational%20costs.%0ABy%20challenging%20the%20prevailing%20assumption%20that%20more%20intricate%20reasoning%0Astrategies%20inherently%20lead%20to%20better%20performance%2C%20our%20work%20offers%20new%20insights%0Ainto%20how%20simpler%2C%20more%20efficient%20approaches%20can%20achieve%20optimal%20results.%20So%2C%0Aare%20retrials%20all%20you%20need%3F%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12951v1&entry.124074799=Read"},
{"title": "Antidistillation Sampling", "author": "Yash Savani and Asher Trockman and Zhili Feng and Avi Schwarzschild and Alexander Robey and Marc Finzi and J. Zico Kolter", "abstract": "  Frontier models that generate extended reasoning traces inadvertently produce\nrich token sequences that can facilitate model distillation. Recognizing this\nvulnerability, model owners may seek sampling strategies that limit the\neffectiveness of distillation without compromising model performance.\n\\emph{Antidistillation sampling} provides exactly this capability. By\nstrategically modifying a model's next-token probability distribution,\nantidistillation sampling poisons reasoning traces, rendering them\nsignificantly less effective for distillation while preserving the model's\npractical utility. For further details, see https://antidistillation.com.\n", "link": "http://arxiv.org/abs/2504.13146v1", "date": "2025-04-17", "relevancy": 1.9108, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5028}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4704}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Antidistillation%20Sampling&body=Title%3A%20Antidistillation%20Sampling%0AAuthor%3A%20Yash%20Savani%20and%20Asher%20Trockman%20and%20Zhili%20Feng%20and%20Avi%20Schwarzschild%20and%20Alexander%20Robey%20and%20Marc%20Finzi%20and%20J.%20Zico%20Kolter%0AAbstract%3A%20%20%20Frontier%20models%20that%20generate%20extended%20reasoning%20traces%20inadvertently%20produce%0Arich%20token%20sequences%20that%20can%20facilitate%20model%20distillation.%20Recognizing%20this%0Avulnerability%2C%20model%20owners%20may%20seek%20sampling%20strategies%20that%20limit%20the%0Aeffectiveness%20of%20distillation%20without%20compromising%20model%20performance.%0A%5Cemph%7BAntidistillation%20sampling%7D%20provides%20exactly%20this%20capability.%20By%0Astrategically%20modifying%20a%20model%27s%20next-token%20probability%20distribution%2C%0Aantidistillation%20sampling%20poisons%20reasoning%20traces%2C%20rendering%20them%0Asignificantly%20less%20effective%20for%20distillation%20while%20preserving%20the%20model%27s%0Apractical%20utility.%20For%20further%20details%2C%20see%20https%3A//antidistillation.com.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13146v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAntidistillation%2520Sampling%26entry.906535625%3DYash%2520Savani%2520and%2520Asher%2520Trockman%2520and%2520Zhili%2520Feng%2520and%2520Avi%2520Schwarzschild%2520and%2520Alexander%2520Robey%2520and%2520Marc%2520Finzi%2520and%2520J.%2520Zico%2520Kolter%26entry.1292438233%3D%2520%2520Frontier%2520models%2520that%2520generate%2520extended%2520reasoning%2520traces%2520inadvertently%2520produce%250Arich%2520token%2520sequences%2520that%2520can%2520facilitate%2520model%2520distillation.%2520Recognizing%2520this%250Avulnerability%252C%2520model%2520owners%2520may%2520seek%2520sampling%2520strategies%2520that%2520limit%2520the%250Aeffectiveness%2520of%2520distillation%2520without%2520compromising%2520model%2520performance.%250A%255Cemph%257BAntidistillation%2520sampling%257D%2520provides%2520exactly%2520this%2520capability.%2520By%250Astrategically%2520modifying%2520a%2520model%2527s%2520next-token%2520probability%2520distribution%252C%250Aantidistillation%2520sampling%2520poisons%2520reasoning%2520traces%252C%2520rendering%2520them%250Asignificantly%2520less%2520effective%2520for%2520distillation%2520while%2520preserving%2520the%2520model%2527s%250Apractical%2520utility.%2520For%2520further%2520details%252C%2520see%2520https%253A//antidistillation.com.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13146v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Antidistillation%20Sampling&entry.906535625=Yash%20Savani%20and%20Asher%20Trockman%20and%20Zhili%20Feng%20and%20Avi%20Schwarzschild%20and%20Alexander%20Robey%20and%20Marc%20Finzi%20and%20J.%20Zico%20Kolter&entry.1292438233=%20%20Frontier%20models%20that%20generate%20extended%20reasoning%20traces%20inadvertently%20produce%0Arich%20token%20sequences%20that%20can%20facilitate%20model%20distillation.%20Recognizing%20this%0Avulnerability%2C%20model%20owners%20may%20seek%20sampling%20strategies%20that%20limit%20the%0Aeffectiveness%20of%20distillation%20without%20compromising%20model%20performance.%0A%5Cemph%7BAntidistillation%20sampling%7D%20provides%20exactly%20this%20capability.%20By%0Astrategically%20modifying%20a%20model%27s%20next-token%20probability%20distribution%2C%0Aantidistillation%20sampling%20poisons%20reasoning%20traces%2C%20rendering%20them%0Asignificantly%20less%20effective%20for%20distillation%20while%20preserving%20the%20model%27s%0Apractical%20utility.%20For%20further%20details%2C%20see%20https%3A//antidistillation.com.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13146v1&entry.124074799=Read"},
{"title": "Unveiling Molecular Moieties through Hierarchical Grad-CAM Graph\n  Explainability", "author": "Salvatore Contino and Paolo Sortino and Maria Rita Gulotta and Ugo Perricone and Roberto Pirrone", "abstract": "  Background: Virtual Screening (VS) has become an essential tool in drug\ndiscovery, enabling the rapid and cost-effective identification of potential\nbioactive molecules. Among recent advancements, Graph Neural Networks (GNNs)\nhave gained prominence for their ability to model complex molecular structures\nusing graph-based representations. However, the integration of explainable\nmethods to elucidate the specific contributions of molecular substructures to\nbiological activity remains a significant challenge. This limitation hampers\nboth the interpretability of predictive models and the rational design of novel\ntherapeutics.\\\\ Results: We trained 20 GNN models on a dataset of small\nmolecules with the goal of predicting their activity on 20 distinct protein\ntargets from the Kinase family. These classifiers achieved state-of-the-art\nperformance in virtual screening tasks, demonstrating high accuracy and\nrobustness on different targets. Building upon these models, we implemented the\nHierarchical Grad-CAM graph Explainer (HGE) framework, enabling an in-depth\nanalysis of the molecular moieties driving protein-ligand binding\nstabilization. HGE exploits Grad-CAM explanations at the atom, ring, and\nwhole-molecule levels, leveraging the message-passing mechanism to highlight\nthe most relevant chemical moieties. Validation against experimental data from\nthe literature confirmed the ability of the explainer to recognize a molecular\npattern of drugs and correctly annotate them to the known target. Conclusion:\nOur approach may represent a valid support to shorten both the screening and\nthe hit discovery process. Detailed knowledge of the molecular substructures\nthat play a role in the binding process can help the computational chemist to\ngain insights into the structure optimization, as well as in drug repurposing\ntasks.\n", "link": "http://arxiv.org/abs/2402.01744v4", "date": "2025-04-17", "relevancy": 1.9822, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5037}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4939}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20Molecular%20Moieties%20through%20Hierarchical%20Grad-CAM%20Graph%0A%20%20Explainability&body=Title%3A%20Unveiling%20Molecular%20Moieties%20through%20Hierarchical%20Grad-CAM%20Graph%0A%20%20Explainability%0AAuthor%3A%20Salvatore%20Contino%20and%20Paolo%20Sortino%20and%20Maria%20Rita%20Gulotta%20and%20Ugo%20Perricone%20and%20Roberto%20Pirrone%0AAbstract%3A%20%20%20Background%3A%20Virtual%20Screening%20%28VS%29%20has%20become%20an%20essential%20tool%20in%20drug%0Adiscovery%2C%20enabling%20the%20rapid%20and%20cost-effective%20identification%20of%20potential%0Abioactive%20molecules.%20Among%20recent%20advancements%2C%20Graph%20Neural%20Networks%20%28GNNs%29%0Ahave%20gained%20prominence%20for%20their%20ability%20to%20model%20complex%20molecular%20structures%0Ausing%20graph-based%20representations.%20However%2C%20the%20integration%20of%20explainable%0Amethods%20to%20elucidate%20the%20specific%20contributions%20of%20molecular%20substructures%20to%0Abiological%20activity%20remains%20a%20significant%20challenge.%20This%20limitation%20hampers%0Aboth%20the%20interpretability%20of%20predictive%20models%20and%20the%20rational%20design%20of%20novel%0Atherapeutics.%5C%5C%20Results%3A%20We%20trained%2020%20GNN%20models%20on%20a%20dataset%20of%20small%0Amolecules%20with%20the%20goal%20of%20predicting%20their%20activity%20on%2020%20distinct%20protein%0Atargets%20from%20the%20Kinase%20family.%20These%20classifiers%20achieved%20state-of-the-art%0Aperformance%20in%20virtual%20screening%20tasks%2C%20demonstrating%20high%20accuracy%20and%0Arobustness%20on%20different%20targets.%20Building%20upon%20these%20models%2C%20we%20implemented%20the%0AHierarchical%20Grad-CAM%20graph%20Explainer%20%28HGE%29%20framework%2C%20enabling%20an%20in-depth%0Aanalysis%20of%20the%20molecular%20moieties%20driving%20protein-ligand%20binding%0Astabilization.%20HGE%20exploits%20Grad-CAM%20explanations%20at%20the%20atom%2C%20ring%2C%20and%0Awhole-molecule%20levels%2C%20leveraging%20the%20message-passing%20mechanism%20to%20highlight%0Athe%20most%20relevant%20chemical%20moieties.%20Validation%20against%20experimental%20data%20from%0Athe%20literature%20confirmed%20the%20ability%20of%20the%20explainer%20to%20recognize%20a%20molecular%0Apattern%20of%20drugs%20and%20correctly%20annotate%20them%20to%20the%20known%20target.%20Conclusion%3A%0AOur%20approach%20may%20represent%20a%20valid%20support%20to%20shorten%20both%20the%20screening%20and%0Athe%20hit%20discovery%20process.%20Detailed%20knowledge%20of%20the%20molecular%20substructures%0Athat%20play%20a%20role%20in%20the%20binding%20process%20can%20help%20the%20computational%20chemist%20to%0Again%20insights%20into%20the%20structure%20optimization%2C%20as%20well%20as%20in%20drug%20repurposing%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01744v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520Molecular%2520Moieties%2520through%2520Hierarchical%2520Grad-CAM%2520Graph%250A%2520%2520Explainability%26entry.906535625%3DSalvatore%2520Contino%2520and%2520Paolo%2520Sortino%2520and%2520Maria%2520Rita%2520Gulotta%2520and%2520Ugo%2520Perricone%2520and%2520Roberto%2520Pirrone%26entry.1292438233%3D%2520%2520Background%253A%2520Virtual%2520Screening%2520%2528VS%2529%2520has%2520become%2520an%2520essential%2520tool%2520in%2520drug%250Adiscovery%252C%2520enabling%2520the%2520rapid%2520and%2520cost-effective%2520identification%2520of%2520potential%250Abioactive%2520molecules.%2520Among%2520recent%2520advancements%252C%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%250Ahave%2520gained%2520prominence%2520for%2520their%2520ability%2520to%2520model%2520complex%2520molecular%2520structures%250Ausing%2520graph-based%2520representations.%2520However%252C%2520the%2520integration%2520of%2520explainable%250Amethods%2520to%2520elucidate%2520the%2520specific%2520contributions%2520of%2520molecular%2520substructures%2520to%250Abiological%2520activity%2520remains%2520a%2520significant%2520challenge.%2520This%2520limitation%2520hampers%250Aboth%2520the%2520interpretability%2520of%2520predictive%2520models%2520and%2520the%2520rational%2520design%2520of%2520novel%250Atherapeutics.%255C%255C%2520Results%253A%2520We%2520trained%252020%2520GNN%2520models%2520on%2520a%2520dataset%2520of%2520small%250Amolecules%2520with%2520the%2520goal%2520of%2520predicting%2520their%2520activity%2520on%252020%2520distinct%2520protein%250Atargets%2520from%2520the%2520Kinase%2520family.%2520These%2520classifiers%2520achieved%2520state-of-the-art%250Aperformance%2520in%2520virtual%2520screening%2520tasks%252C%2520demonstrating%2520high%2520accuracy%2520and%250Arobustness%2520on%2520different%2520targets.%2520Building%2520upon%2520these%2520models%252C%2520we%2520implemented%2520the%250AHierarchical%2520Grad-CAM%2520graph%2520Explainer%2520%2528HGE%2529%2520framework%252C%2520enabling%2520an%2520in-depth%250Aanalysis%2520of%2520the%2520molecular%2520moieties%2520driving%2520protein-ligand%2520binding%250Astabilization.%2520HGE%2520exploits%2520Grad-CAM%2520explanations%2520at%2520the%2520atom%252C%2520ring%252C%2520and%250Awhole-molecule%2520levels%252C%2520leveraging%2520the%2520message-passing%2520mechanism%2520to%2520highlight%250Athe%2520most%2520relevant%2520chemical%2520moieties.%2520Validation%2520against%2520experimental%2520data%2520from%250Athe%2520literature%2520confirmed%2520the%2520ability%2520of%2520the%2520explainer%2520to%2520recognize%2520a%2520molecular%250Apattern%2520of%2520drugs%2520and%2520correctly%2520annotate%2520them%2520to%2520the%2520known%2520target.%2520Conclusion%253A%250AOur%2520approach%2520may%2520represent%2520a%2520valid%2520support%2520to%2520shorten%2520both%2520the%2520screening%2520and%250Athe%2520hit%2520discovery%2520process.%2520Detailed%2520knowledge%2520of%2520the%2520molecular%2520substructures%250Athat%2520play%2520a%2520role%2520in%2520the%2520binding%2520process%2520can%2520help%2520the%2520computational%2520chemist%2520to%250Again%2520insights%2520into%2520the%2520structure%2520optimization%252C%2520as%2520well%2520as%2520in%2520drug%2520repurposing%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01744v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20Molecular%20Moieties%20through%20Hierarchical%20Grad-CAM%20Graph%0A%20%20Explainability&entry.906535625=Salvatore%20Contino%20and%20Paolo%20Sortino%20and%20Maria%20Rita%20Gulotta%20and%20Ugo%20Perricone%20and%20Roberto%20Pirrone&entry.1292438233=%20%20Background%3A%20Virtual%20Screening%20%28VS%29%20has%20become%20an%20essential%20tool%20in%20drug%0Adiscovery%2C%20enabling%20the%20rapid%20and%20cost-effective%20identification%20of%20potential%0Abioactive%20molecules.%20Among%20recent%20advancements%2C%20Graph%20Neural%20Networks%20%28GNNs%29%0Ahave%20gained%20prominence%20for%20their%20ability%20to%20model%20complex%20molecular%20structures%0Ausing%20graph-based%20representations.%20However%2C%20the%20integration%20of%20explainable%0Amethods%20to%20elucidate%20the%20specific%20contributions%20of%20molecular%20substructures%20to%0Abiological%20activity%20remains%20a%20significant%20challenge.%20This%20limitation%20hampers%0Aboth%20the%20interpretability%20of%20predictive%20models%20and%20the%20rational%20design%20of%20novel%0Atherapeutics.%5C%5C%20Results%3A%20We%20trained%2020%20GNN%20models%20on%20a%20dataset%20of%20small%0Amolecules%20with%20the%20goal%20of%20predicting%20their%20activity%20on%2020%20distinct%20protein%0Atargets%20from%20the%20Kinase%20family.%20These%20classifiers%20achieved%20state-of-the-art%0Aperformance%20in%20virtual%20screening%20tasks%2C%20demonstrating%20high%20accuracy%20and%0Arobustness%20on%20different%20targets.%20Building%20upon%20these%20models%2C%20we%20implemented%20the%0AHierarchical%20Grad-CAM%20graph%20Explainer%20%28HGE%29%20framework%2C%20enabling%20an%20in-depth%0Aanalysis%20of%20the%20molecular%20moieties%20driving%20protein-ligand%20binding%0Astabilization.%20HGE%20exploits%20Grad-CAM%20explanations%20at%20the%20atom%2C%20ring%2C%20and%0Awhole-molecule%20levels%2C%20leveraging%20the%20message-passing%20mechanism%20to%20highlight%0Athe%20most%20relevant%20chemical%20moieties.%20Validation%20against%20experimental%20data%20from%0Athe%20literature%20confirmed%20the%20ability%20of%20the%20explainer%20to%20recognize%20a%20molecular%0Apattern%20of%20drugs%20and%20correctly%20annotate%20them%20to%20the%20known%20target.%20Conclusion%3A%0AOur%20approach%20may%20represent%20a%20valid%20support%20to%20shorten%20both%20the%20screening%20and%0Athe%20hit%20discovery%20process.%20Detailed%20knowledge%20of%20the%20molecular%20substructures%0Athat%20play%20a%20role%20in%20the%20binding%20process%20can%20help%20the%20computational%20chemist%20to%0Again%20insights%20into%20the%20structure%20optimization%2C%20as%20well%20as%20in%20drug%20repurposing%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01744v4&entry.124074799=Read"},
{"title": "Transfer Learning via Auxiliary Labels with Application to\n  Cold-Hardiness Prediction", "author": "Kristen Goebel and Paola Pesantez-Cabrera and Markus Keller and Alan Fern", "abstract": "  Cold temperatures can cause significant frost damage to fruit crops depending\non their resilience, or cold hardiness, which changes throughout the dormancy\nseason. This has led to the development of predictive cold-hardiness models,\nwhich help farmers decide when to deploy expensive frost-mitigation measures.\nUnfortunately, cold-hardiness data for model training is only available for\nsome fruit cultivars due to the need for specialized equipment and expertise.\nRather, farmers often do have years of phenological data (e.g. date of\nbudbreak) that they regularly collect for their crops. In this work, we\nintroduce a new transfer-learning framework, Transfer via Auxiliary Labels\n(TAL), that allows farmers to leverage the phenological data to produce more\naccurate cold-hardiness predictions, even when no cold-hardiness data is\navailable for their specific crop. The framework assumes a set of source tasks\n(cultivars) where each has associated primary labels (cold hardiness) and\nauxiliary labels (phenology). However, the target task (new cultivar) is\nassumed to only have the auxiliary labels. The goal of TAL is to predict\nprimary labels for the target task via transfer from the source tasks.\nSurprisingly, despite the vast literature on transfer learning, to our\nknowledge, the TAL formulation has not been previously addressed. Thus, we\npropose several new TAL approaches based on model selection and averaging that\ncan leverage recent deep multi-task models for cold-hardiness prediction. Our\nresults on real-world cold-hardiness and phenological data for multiple grape\ncultivars demonstrate that TAL can leverage the phenological data to improve\ncold-hardiness predictions in the absence of cold-hardiness data.\n", "link": "http://arxiv.org/abs/2504.13142v1", "date": "2025-04-17", "relevancy": 2.2612, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4679}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4451}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.4437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transfer%20Learning%20via%20Auxiliary%20Labels%20with%20Application%20to%0A%20%20Cold-Hardiness%20Prediction&body=Title%3A%20Transfer%20Learning%20via%20Auxiliary%20Labels%20with%20Application%20to%0A%20%20Cold-Hardiness%20Prediction%0AAuthor%3A%20Kristen%20Goebel%20and%20Paola%20Pesantez-Cabrera%20and%20Markus%20Keller%20and%20Alan%20Fern%0AAbstract%3A%20%20%20Cold%20temperatures%20can%20cause%20significant%20frost%20damage%20to%20fruit%20crops%20depending%0Aon%20their%20resilience%2C%20or%20cold%20hardiness%2C%20which%20changes%20throughout%20the%20dormancy%0Aseason.%20This%20has%20led%20to%20the%20development%20of%20predictive%20cold-hardiness%20models%2C%0Awhich%20help%20farmers%20decide%20when%20to%20deploy%20expensive%20frost-mitigation%20measures.%0AUnfortunately%2C%20cold-hardiness%20data%20for%20model%20training%20is%20only%20available%20for%0Asome%20fruit%20cultivars%20due%20to%20the%20need%20for%20specialized%20equipment%20and%20expertise.%0ARather%2C%20farmers%20often%20do%20have%20years%20of%20phenological%20data%20%28e.g.%20date%20of%0Abudbreak%29%20that%20they%20regularly%20collect%20for%20their%20crops.%20In%20this%20work%2C%20we%0Aintroduce%20a%20new%20transfer-learning%20framework%2C%20Transfer%20via%20Auxiliary%20Labels%0A%28TAL%29%2C%20that%20allows%20farmers%20to%20leverage%20the%20phenological%20data%20to%20produce%20more%0Aaccurate%20cold-hardiness%20predictions%2C%20even%20when%20no%20cold-hardiness%20data%20is%0Aavailable%20for%20their%20specific%20crop.%20The%20framework%20assumes%20a%20set%20of%20source%20tasks%0A%28cultivars%29%20where%20each%20has%20associated%20primary%20labels%20%28cold%20hardiness%29%20and%0Aauxiliary%20labels%20%28phenology%29.%20However%2C%20the%20target%20task%20%28new%20cultivar%29%20is%0Aassumed%20to%20only%20have%20the%20auxiliary%20labels.%20The%20goal%20of%20TAL%20is%20to%20predict%0Aprimary%20labels%20for%20the%20target%20task%20via%20transfer%20from%20the%20source%20tasks.%0ASurprisingly%2C%20despite%20the%20vast%20literature%20on%20transfer%20learning%2C%20to%20our%0Aknowledge%2C%20the%20TAL%20formulation%20has%20not%20been%20previously%20addressed.%20Thus%2C%20we%0Apropose%20several%20new%20TAL%20approaches%20based%20on%20model%20selection%20and%20averaging%20that%0Acan%20leverage%20recent%20deep%20multi-task%20models%20for%20cold-hardiness%20prediction.%20Our%0Aresults%20on%20real-world%20cold-hardiness%20and%20phenological%20data%20for%20multiple%20grape%0Acultivars%20demonstrate%20that%20TAL%20can%20leverage%20the%20phenological%20data%20to%20improve%0Acold-hardiness%20predictions%20in%20the%20absence%20of%20cold-hardiness%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13142v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransfer%2520Learning%2520via%2520Auxiliary%2520Labels%2520with%2520Application%2520to%250A%2520%2520Cold-Hardiness%2520Prediction%26entry.906535625%3DKristen%2520Goebel%2520and%2520Paola%2520Pesantez-Cabrera%2520and%2520Markus%2520Keller%2520and%2520Alan%2520Fern%26entry.1292438233%3D%2520%2520Cold%2520temperatures%2520can%2520cause%2520significant%2520frost%2520damage%2520to%2520fruit%2520crops%2520depending%250Aon%2520their%2520resilience%252C%2520or%2520cold%2520hardiness%252C%2520which%2520changes%2520throughout%2520the%2520dormancy%250Aseason.%2520This%2520has%2520led%2520to%2520the%2520development%2520of%2520predictive%2520cold-hardiness%2520models%252C%250Awhich%2520help%2520farmers%2520decide%2520when%2520to%2520deploy%2520expensive%2520frost-mitigation%2520measures.%250AUnfortunately%252C%2520cold-hardiness%2520data%2520for%2520model%2520training%2520is%2520only%2520available%2520for%250Asome%2520fruit%2520cultivars%2520due%2520to%2520the%2520need%2520for%2520specialized%2520equipment%2520and%2520expertise.%250ARather%252C%2520farmers%2520often%2520do%2520have%2520years%2520of%2520phenological%2520data%2520%2528e.g.%2520date%2520of%250Abudbreak%2529%2520that%2520they%2520regularly%2520collect%2520for%2520their%2520crops.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520a%2520new%2520transfer-learning%2520framework%252C%2520Transfer%2520via%2520Auxiliary%2520Labels%250A%2528TAL%2529%252C%2520that%2520allows%2520farmers%2520to%2520leverage%2520the%2520phenological%2520data%2520to%2520produce%2520more%250Aaccurate%2520cold-hardiness%2520predictions%252C%2520even%2520when%2520no%2520cold-hardiness%2520data%2520is%250Aavailable%2520for%2520their%2520specific%2520crop.%2520The%2520framework%2520assumes%2520a%2520set%2520of%2520source%2520tasks%250A%2528cultivars%2529%2520where%2520each%2520has%2520associated%2520primary%2520labels%2520%2528cold%2520hardiness%2529%2520and%250Aauxiliary%2520labels%2520%2528phenology%2529.%2520However%252C%2520the%2520target%2520task%2520%2528new%2520cultivar%2529%2520is%250Aassumed%2520to%2520only%2520have%2520the%2520auxiliary%2520labels.%2520The%2520goal%2520of%2520TAL%2520is%2520to%2520predict%250Aprimary%2520labels%2520for%2520the%2520target%2520task%2520via%2520transfer%2520from%2520the%2520source%2520tasks.%250ASurprisingly%252C%2520despite%2520the%2520vast%2520literature%2520on%2520transfer%2520learning%252C%2520to%2520our%250Aknowledge%252C%2520the%2520TAL%2520formulation%2520has%2520not%2520been%2520previously%2520addressed.%2520Thus%252C%2520we%250Apropose%2520several%2520new%2520TAL%2520approaches%2520based%2520on%2520model%2520selection%2520and%2520averaging%2520that%250Acan%2520leverage%2520recent%2520deep%2520multi-task%2520models%2520for%2520cold-hardiness%2520prediction.%2520Our%250Aresults%2520on%2520real-world%2520cold-hardiness%2520and%2520phenological%2520data%2520for%2520multiple%2520grape%250Acultivars%2520demonstrate%2520that%2520TAL%2520can%2520leverage%2520the%2520phenological%2520data%2520to%2520improve%250Acold-hardiness%2520predictions%2520in%2520the%2520absence%2520of%2520cold-hardiness%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13142v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transfer%20Learning%20via%20Auxiliary%20Labels%20with%20Application%20to%0A%20%20Cold-Hardiness%20Prediction&entry.906535625=Kristen%20Goebel%20and%20Paola%20Pesantez-Cabrera%20and%20Markus%20Keller%20and%20Alan%20Fern&entry.1292438233=%20%20Cold%20temperatures%20can%20cause%20significant%20frost%20damage%20to%20fruit%20crops%20depending%0Aon%20their%20resilience%2C%20or%20cold%20hardiness%2C%20which%20changes%20throughout%20the%20dormancy%0Aseason.%20This%20has%20led%20to%20the%20development%20of%20predictive%20cold-hardiness%20models%2C%0Awhich%20help%20farmers%20decide%20when%20to%20deploy%20expensive%20frost-mitigation%20measures.%0AUnfortunately%2C%20cold-hardiness%20data%20for%20model%20training%20is%20only%20available%20for%0Asome%20fruit%20cultivars%20due%20to%20the%20need%20for%20specialized%20equipment%20and%20expertise.%0ARather%2C%20farmers%20often%20do%20have%20years%20of%20phenological%20data%20%28e.g.%20date%20of%0Abudbreak%29%20that%20they%20regularly%20collect%20for%20their%20crops.%20In%20this%20work%2C%20we%0Aintroduce%20a%20new%20transfer-learning%20framework%2C%20Transfer%20via%20Auxiliary%20Labels%0A%28TAL%29%2C%20that%20allows%20farmers%20to%20leverage%20the%20phenological%20data%20to%20produce%20more%0Aaccurate%20cold-hardiness%20predictions%2C%20even%20when%20no%20cold-hardiness%20data%20is%0Aavailable%20for%20their%20specific%20crop.%20The%20framework%20assumes%20a%20set%20of%20source%20tasks%0A%28cultivars%29%20where%20each%20has%20associated%20primary%20labels%20%28cold%20hardiness%29%20and%0Aauxiliary%20labels%20%28phenology%29.%20However%2C%20the%20target%20task%20%28new%20cultivar%29%20is%0Aassumed%20to%20only%20have%20the%20auxiliary%20labels.%20The%20goal%20of%20TAL%20is%20to%20predict%0Aprimary%20labels%20for%20the%20target%20task%20via%20transfer%20from%20the%20source%20tasks.%0ASurprisingly%2C%20despite%20the%20vast%20literature%20on%20transfer%20learning%2C%20to%20our%0Aknowledge%2C%20the%20TAL%20formulation%20has%20not%20been%20previously%20addressed.%20Thus%2C%20we%0Apropose%20several%20new%20TAL%20approaches%20based%20on%20model%20selection%20and%20averaging%20that%0Acan%20leverage%20recent%20deep%20multi-task%20models%20for%20cold-hardiness%20prediction.%20Our%0Aresults%20on%20real-world%20cold-hardiness%20and%20phenological%20data%20for%20multiple%20grape%0Acultivars%20demonstrate%20that%20TAL%20can%20leverage%20the%20phenological%20data%20to%20improve%0Acold-hardiness%20predictions%20in%20the%20absence%20of%20cold-hardiness%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13142v1&entry.124074799=Read"},
{"title": "Safety with Agency: Human-Centered Safety Filter with Application to\n  AI-Assisted Motorsports", "author": "Donggeon David Oh and Justin Lidard and Haimin Hu and Himani Sinhmar and Elle Lazarski and Deepak Gopinath and Emily S. Sumner and Jonathan A. DeCastro and Guy Rosman and Naomi Ehrich Leonard and Jaime Fern\u00e1ndez Fisac", "abstract": "  We propose a human-centered safety filter (HCSF) for shared autonomy that\nsignificantly enhances system safety without compromising human agency. Our\nHCSF is built on a neural safety value function, which we first learn scalably\nthrough black-box interactions and then use at deployment to enforce a novel\nstate-action control barrier function (Q-CBF) safety constraint. Since this\nQ-CBF safety filter does not require any knowledge of the system dynamics for\nboth synthesis and runtime safety monitoring and intervention, our method\napplies readily to complex, black-box shared autonomy systems. Notably, our\nHCSF's CBF-based interventions modify the human's actions minimally and\nsmoothly, avoiding the abrupt, last-moment corrections delivered by many\nconventional safety filters. We validate our approach in a comprehensive\nin-person user study using Assetto Corsa-a high-fidelity car racing simulator\nwith black-box dynamics-to assess robustness in \"driving on the edge\"\nscenarios. We compare both trajectory data and drivers' perceptions of our HCSF\nassistance against unassisted driving and a conventional safety filter.\nExperimental results show that 1) compared to having no assistance, our HCSF\nimproves both safety and user satisfaction without compromising human agency or\ncomfort, and 2) relative to a conventional safety filter, our proposed HCSF\nboosts human agency, comfort, and satisfaction while maintaining robustness.\n", "link": "http://arxiv.org/abs/2504.11717v2", "date": "2025-04-17", "relevancy": 2.1252, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5509}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5276}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safety%20with%20Agency%3A%20Human-Centered%20Safety%20Filter%20with%20Application%20to%0A%20%20AI-Assisted%20Motorsports&body=Title%3A%20Safety%20with%20Agency%3A%20Human-Centered%20Safety%20Filter%20with%20Application%20to%0A%20%20AI-Assisted%20Motorsports%0AAuthor%3A%20Donggeon%20David%20Oh%20and%20Justin%20Lidard%20and%20Haimin%20Hu%20and%20Himani%20Sinhmar%20and%20Elle%20Lazarski%20and%20Deepak%20Gopinath%20and%20Emily%20S.%20Sumner%20and%20Jonathan%20A.%20DeCastro%20and%20Guy%20Rosman%20and%20Naomi%20Ehrich%20Leonard%20and%20Jaime%20Fern%C3%A1ndez%20Fisac%0AAbstract%3A%20%20%20We%20propose%20a%20human-centered%20safety%20filter%20%28HCSF%29%20for%20shared%20autonomy%20that%0Asignificantly%20enhances%20system%20safety%20without%20compromising%20human%20agency.%20Our%0AHCSF%20is%20built%20on%20a%20neural%20safety%20value%20function%2C%20which%20we%20first%20learn%20scalably%0Athrough%20black-box%20interactions%20and%20then%20use%20at%20deployment%20to%20enforce%20a%20novel%0Astate-action%20control%20barrier%20function%20%28Q-CBF%29%20safety%20constraint.%20Since%20this%0AQ-CBF%20safety%20filter%20does%20not%20require%20any%20knowledge%20of%20the%20system%20dynamics%20for%0Aboth%20synthesis%20and%20runtime%20safety%20monitoring%20and%20intervention%2C%20our%20method%0Aapplies%20readily%20to%20complex%2C%20black-box%20shared%20autonomy%20systems.%20Notably%2C%20our%0AHCSF%27s%20CBF-based%20interventions%20modify%20the%20human%27s%20actions%20minimally%20and%0Asmoothly%2C%20avoiding%20the%20abrupt%2C%20last-moment%20corrections%20delivered%20by%20many%0Aconventional%20safety%20filters.%20We%20validate%20our%20approach%20in%20a%20comprehensive%0Ain-person%20user%20study%20using%20Assetto%20Corsa-a%20high-fidelity%20car%20racing%20simulator%0Awith%20black-box%20dynamics-to%20assess%20robustness%20in%20%22driving%20on%20the%20edge%22%0Ascenarios.%20We%20compare%20both%20trajectory%20data%20and%20drivers%27%20perceptions%20of%20our%20HCSF%0Aassistance%20against%20unassisted%20driving%20and%20a%20conventional%20safety%20filter.%0AExperimental%20results%20show%20that%201%29%20compared%20to%20having%20no%20assistance%2C%20our%20HCSF%0Aimproves%20both%20safety%20and%20user%20satisfaction%20without%20compromising%20human%20agency%20or%0Acomfort%2C%20and%202%29%20relative%20to%20a%20conventional%20safety%20filter%2C%20our%20proposed%20HCSF%0Aboosts%20human%20agency%2C%20comfort%2C%20and%20satisfaction%20while%20maintaining%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.11717v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafety%2520with%2520Agency%253A%2520Human-Centered%2520Safety%2520Filter%2520with%2520Application%2520to%250A%2520%2520AI-Assisted%2520Motorsports%26entry.906535625%3DDonggeon%2520David%2520Oh%2520and%2520Justin%2520Lidard%2520and%2520Haimin%2520Hu%2520and%2520Himani%2520Sinhmar%2520and%2520Elle%2520Lazarski%2520and%2520Deepak%2520Gopinath%2520and%2520Emily%2520S.%2520Sumner%2520and%2520Jonathan%2520A.%2520DeCastro%2520and%2520Guy%2520Rosman%2520and%2520Naomi%2520Ehrich%2520Leonard%2520and%2520Jaime%2520Fern%25C3%25A1ndez%2520Fisac%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520human-centered%2520safety%2520filter%2520%2528HCSF%2529%2520for%2520shared%2520autonomy%2520that%250Asignificantly%2520enhances%2520system%2520safety%2520without%2520compromising%2520human%2520agency.%2520Our%250AHCSF%2520is%2520built%2520on%2520a%2520neural%2520safety%2520value%2520function%252C%2520which%2520we%2520first%2520learn%2520scalably%250Athrough%2520black-box%2520interactions%2520and%2520then%2520use%2520at%2520deployment%2520to%2520enforce%2520a%2520novel%250Astate-action%2520control%2520barrier%2520function%2520%2528Q-CBF%2529%2520safety%2520constraint.%2520Since%2520this%250AQ-CBF%2520safety%2520filter%2520does%2520not%2520require%2520any%2520knowledge%2520of%2520the%2520system%2520dynamics%2520for%250Aboth%2520synthesis%2520and%2520runtime%2520safety%2520monitoring%2520and%2520intervention%252C%2520our%2520method%250Aapplies%2520readily%2520to%2520complex%252C%2520black-box%2520shared%2520autonomy%2520systems.%2520Notably%252C%2520our%250AHCSF%2527s%2520CBF-based%2520interventions%2520modify%2520the%2520human%2527s%2520actions%2520minimally%2520and%250Asmoothly%252C%2520avoiding%2520the%2520abrupt%252C%2520last-moment%2520corrections%2520delivered%2520by%2520many%250Aconventional%2520safety%2520filters.%2520We%2520validate%2520our%2520approach%2520in%2520a%2520comprehensive%250Ain-person%2520user%2520study%2520using%2520Assetto%2520Corsa-a%2520high-fidelity%2520car%2520racing%2520simulator%250Awith%2520black-box%2520dynamics-to%2520assess%2520robustness%2520in%2520%2522driving%2520on%2520the%2520edge%2522%250Ascenarios.%2520We%2520compare%2520both%2520trajectory%2520data%2520and%2520drivers%2527%2520perceptions%2520of%2520our%2520HCSF%250Aassistance%2520against%2520unassisted%2520driving%2520and%2520a%2520conventional%2520safety%2520filter.%250AExperimental%2520results%2520show%2520that%25201%2529%2520compared%2520to%2520having%2520no%2520assistance%252C%2520our%2520HCSF%250Aimproves%2520both%2520safety%2520and%2520user%2520satisfaction%2520without%2520compromising%2520human%2520agency%2520or%250Acomfort%252C%2520and%25202%2529%2520relative%2520to%2520a%2520conventional%2520safety%2520filter%252C%2520our%2520proposed%2520HCSF%250Aboosts%2520human%2520agency%252C%2520comfort%252C%2520and%2520satisfaction%2520while%2520maintaining%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.11717v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safety%20with%20Agency%3A%20Human-Centered%20Safety%20Filter%20with%20Application%20to%0A%20%20AI-Assisted%20Motorsports&entry.906535625=Donggeon%20David%20Oh%20and%20Justin%20Lidard%20and%20Haimin%20Hu%20and%20Himani%20Sinhmar%20and%20Elle%20Lazarski%20and%20Deepak%20Gopinath%20and%20Emily%20S.%20Sumner%20and%20Jonathan%20A.%20DeCastro%20and%20Guy%20Rosman%20and%20Naomi%20Ehrich%20Leonard%20and%20Jaime%20Fern%C3%A1ndez%20Fisac&entry.1292438233=%20%20We%20propose%20a%20human-centered%20safety%20filter%20%28HCSF%29%20for%20shared%20autonomy%20that%0Asignificantly%20enhances%20system%20safety%20without%20compromising%20human%20agency.%20Our%0AHCSF%20is%20built%20on%20a%20neural%20safety%20value%20function%2C%20which%20we%20first%20learn%20scalably%0Athrough%20black-box%20interactions%20and%20then%20use%20at%20deployment%20to%20enforce%20a%20novel%0Astate-action%20control%20barrier%20function%20%28Q-CBF%29%20safety%20constraint.%20Since%20this%0AQ-CBF%20safety%20filter%20does%20not%20require%20any%20knowledge%20of%20the%20system%20dynamics%20for%0Aboth%20synthesis%20and%20runtime%20safety%20monitoring%20and%20intervention%2C%20our%20method%0Aapplies%20readily%20to%20complex%2C%20black-box%20shared%20autonomy%20systems.%20Notably%2C%20our%0AHCSF%27s%20CBF-based%20interventions%20modify%20the%20human%27s%20actions%20minimally%20and%0Asmoothly%2C%20avoiding%20the%20abrupt%2C%20last-moment%20corrections%20delivered%20by%20many%0Aconventional%20safety%20filters.%20We%20validate%20our%20approach%20in%20a%20comprehensive%0Ain-person%20user%20study%20using%20Assetto%20Corsa-a%20high-fidelity%20car%20racing%20simulator%0Awith%20black-box%20dynamics-to%20assess%20robustness%20in%20%22driving%20on%20the%20edge%22%0Ascenarios.%20We%20compare%20both%20trajectory%20data%20and%20drivers%27%20perceptions%20of%20our%20HCSF%0Aassistance%20against%20unassisted%20driving%20and%20a%20conventional%20safety%20filter.%0AExperimental%20results%20show%20that%201%29%20compared%20to%20having%20no%20assistance%2C%20our%20HCSF%0Aimproves%20both%20safety%20and%20user%20satisfaction%20without%20compromising%20human%20agency%20or%0Acomfort%2C%20and%202%29%20relative%20to%20a%20conventional%20safety%20filter%2C%20our%20proposed%20HCSF%0Aboosts%20human%20agency%2C%20comfort%2C%20and%20satisfaction%20while%20maintaining%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.11717v2&entry.124074799=Read"},
{"title": "iHHO-SMOTe: A Cleansed Approach for Handling Outliers and Reducing Noise\n  to Improve Imbalanced Data Classification", "author": "Khaled SH. Raslan and Almohammady S. Alsharkawy and K. R. Raslan", "abstract": "  Classifying imbalanced datasets remains a significant challenge in machine\nlearning, particularly with big data where instances are unevenly distributed\namong classes, leading to class imbalance issues that impact classifier\nperformance. While Synthetic Minority Over-sampling Technique (SMOTE) addresses\nthis challenge by generating new instances for the under-represented minority\nclass, it faces obstacles in the form of noise and outliers during the creation\nof new samples. In this paper, a proposed approach, iHHO-SMOTe, which addresses\nthe limitations of SMOTE by first cleansing the data from noise points. This\nprocess involves employing feature selection using a random forest to identify\nthe most valuable features, followed by applying the Density-Based Spatial\nClustering of Applications with Noise (DBSCAN) algorithm to detect outliers\nbased on the selected features. The identified outliers from the minority\nclasses are then removed, creating a refined dataset for subsequent\noversampling using the hybrid approach called iHHO-SMOTe. The comprehensive\nexperiments across diverse datasets demonstrate the exceptional performance of\nthe proposed model, with an AUC score exceeding 0.99, a high G-means score of\n0.99 highlighting its robustness, and an outstanding F1-score consistently\nexceeding 0.967. These findings collectively establish Cleansed iHHO-SMOTe as a\nformidable contender in addressing imbalanced datasets, focusing on noise\nreduction and outlier handling for improved classification models.\n", "link": "http://arxiv.org/abs/2504.12850v1", "date": "2025-04-17", "relevancy": 2.3718, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5125}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4588}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20iHHO-SMOTe%3A%20A%20Cleansed%20Approach%20for%20Handling%20Outliers%20and%20Reducing%20Noise%0A%20%20to%20Improve%20Imbalanced%20Data%20Classification&body=Title%3A%20iHHO-SMOTe%3A%20A%20Cleansed%20Approach%20for%20Handling%20Outliers%20and%20Reducing%20Noise%0A%20%20to%20Improve%20Imbalanced%20Data%20Classification%0AAuthor%3A%20Khaled%20SH.%20Raslan%20and%20Almohammady%20S.%20Alsharkawy%20and%20K.%20R.%20Raslan%0AAbstract%3A%20%20%20Classifying%20imbalanced%20datasets%20remains%20a%20significant%20challenge%20in%20machine%0Alearning%2C%20particularly%20with%20big%20data%20where%20instances%20are%20unevenly%20distributed%0Aamong%20classes%2C%20leading%20to%20class%20imbalance%20issues%20that%20impact%20classifier%0Aperformance.%20While%20Synthetic%20Minority%20Over-sampling%20Technique%20%28SMOTE%29%20addresses%0Athis%20challenge%20by%20generating%20new%20instances%20for%20the%20under-represented%20minority%0Aclass%2C%20it%20faces%20obstacles%20in%20the%20form%20of%20noise%20and%20outliers%20during%20the%20creation%0Aof%20new%20samples.%20In%20this%20paper%2C%20a%20proposed%20approach%2C%20iHHO-SMOTe%2C%20which%20addresses%0Athe%20limitations%20of%20SMOTE%20by%20first%20cleansing%20the%20data%20from%20noise%20points.%20This%0Aprocess%20involves%20employing%20feature%20selection%20using%20a%20random%20forest%20to%20identify%0Athe%20most%20valuable%20features%2C%20followed%20by%20applying%20the%20Density-Based%20Spatial%0AClustering%20of%20Applications%20with%20Noise%20%28DBSCAN%29%20algorithm%20to%20detect%20outliers%0Abased%20on%20the%20selected%20features.%20The%20identified%20outliers%20from%20the%20minority%0Aclasses%20are%20then%20removed%2C%20creating%20a%20refined%20dataset%20for%20subsequent%0Aoversampling%20using%20the%20hybrid%20approach%20called%20iHHO-SMOTe.%20The%20comprehensive%0Aexperiments%20across%20diverse%20datasets%20demonstrate%20the%20exceptional%20performance%20of%0Athe%20proposed%20model%2C%20with%20an%20AUC%20score%20exceeding%200.99%2C%20a%20high%20G-means%20score%20of%0A0.99%20highlighting%20its%20robustness%2C%20and%20an%20outstanding%20F1-score%20consistently%0Aexceeding%200.967.%20These%20findings%20collectively%20establish%20Cleansed%20iHHO-SMOTe%20as%20a%0Aformidable%20contender%20in%20addressing%20imbalanced%20datasets%2C%20focusing%20on%20noise%0Areduction%20and%20outlier%20handling%20for%20improved%20classification%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12850v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DiHHO-SMOTe%253A%2520A%2520Cleansed%2520Approach%2520for%2520Handling%2520Outliers%2520and%2520Reducing%2520Noise%250A%2520%2520to%2520Improve%2520Imbalanced%2520Data%2520Classification%26entry.906535625%3DKhaled%2520SH.%2520Raslan%2520and%2520Almohammady%2520S.%2520Alsharkawy%2520and%2520K.%2520R.%2520Raslan%26entry.1292438233%3D%2520%2520Classifying%2520imbalanced%2520datasets%2520remains%2520a%2520significant%2520challenge%2520in%2520machine%250Alearning%252C%2520particularly%2520with%2520big%2520data%2520where%2520instances%2520are%2520unevenly%2520distributed%250Aamong%2520classes%252C%2520leading%2520to%2520class%2520imbalance%2520issues%2520that%2520impact%2520classifier%250Aperformance.%2520While%2520Synthetic%2520Minority%2520Over-sampling%2520Technique%2520%2528SMOTE%2529%2520addresses%250Athis%2520challenge%2520by%2520generating%2520new%2520instances%2520for%2520the%2520under-represented%2520minority%250Aclass%252C%2520it%2520faces%2520obstacles%2520in%2520the%2520form%2520of%2520noise%2520and%2520outliers%2520during%2520the%2520creation%250Aof%2520new%2520samples.%2520In%2520this%2520paper%252C%2520a%2520proposed%2520approach%252C%2520iHHO-SMOTe%252C%2520which%2520addresses%250Athe%2520limitations%2520of%2520SMOTE%2520by%2520first%2520cleansing%2520the%2520data%2520from%2520noise%2520points.%2520This%250Aprocess%2520involves%2520employing%2520feature%2520selection%2520using%2520a%2520random%2520forest%2520to%2520identify%250Athe%2520most%2520valuable%2520features%252C%2520followed%2520by%2520applying%2520the%2520Density-Based%2520Spatial%250AClustering%2520of%2520Applications%2520with%2520Noise%2520%2528DBSCAN%2529%2520algorithm%2520to%2520detect%2520outliers%250Abased%2520on%2520the%2520selected%2520features.%2520The%2520identified%2520outliers%2520from%2520the%2520minority%250Aclasses%2520are%2520then%2520removed%252C%2520creating%2520a%2520refined%2520dataset%2520for%2520subsequent%250Aoversampling%2520using%2520the%2520hybrid%2520approach%2520called%2520iHHO-SMOTe.%2520The%2520comprehensive%250Aexperiments%2520across%2520diverse%2520datasets%2520demonstrate%2520the%2520exceptional%2520performance%2520of%250Athe%2520proposed%2520model%252C%2520with%2520an%2520AUC%2520score%2520exceeding%25200.99%252C%2520a%2520high%2520G-means%2520score%2520of%250A0.99%2520highlighting%2520its%2520robustness%252C%2520and%2520an%2520outstanding%2520F1-score%2520consistently%250Aexceeding%25200.967.%2520These%2520findings%2520collectively%2520establish%2520Cleansed%2520iHHO-SMOTe%2520as%2520a%250Aformidable%2520contender%2520in%2520addressing%2520imbalanced%2520datasets%252C%2520focusing%2520on%2520noise%250Areduction%2520and%2520outlier%2520handling%2520for%2520improved%2520classification%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12850v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=iHHO-SMOTe%3A%20A%20Cleansed%20Approach%20for%20Handling%20Outliers%20and%20Reducing%20Noise%0A%20%20to%20Improve%20Imbalanced%20Data%20Classification&entry.906535625=Khaled%20SH.%20Raslan%20and%20Almohammady%20S.%20Alsharkawy%20and%20K.%20R.%20Raslan&entry.1292438233=%20%20Classifying%20imbalanced%20datasets%20remains%20a%20significant%20challenge%20in%20machine%0Alearning%2C%20particularly%20with%20big%20data%20where%20instances%20are%20unevenly%20distributed%0Aamong%20classes%2C%20leading%20to%20class%20imbalance%20issues%20that%20impact%20classifier%0Aperformance.%20While%20Synthetic%20Minority%20Over-sampling%20Technique%20%28SMOTE%29%20addresses%0Athis%20challenge%20by%20generating%20new%20instances%20for%20the%20under-represented%20minority%0Aclass%2C%20it%20faces%20obstacles%20in%20the%20form%20of%20noise%20and%20outliers%20during%20the%20creation%0Aof%20new%20samples.%20In%20this%20paper%2C%20a%20proposed%20approach%2C%20iHHO-SMOTe%2C%20which%20addresses%0Athe%20limitations%20of%20SMOTE%20by%20first%20cleansing%20the%20data%20from%20noise%20points.%20This%0Aprocess%20involves%20employing%20feature%20selection%20using%20a%20random%20forest%20to%20identify%0Athe%20most%20valuable%20features%2C%20followed%20by%20applying%20the%20Density-Based%20Spatial%0AClustering%20of%20Applications%20with%20Noise%20%28DBSCAN%29%20algorithm%20to%20detect%20outliers%0Abased%20on%20the%20selected%20features.%20The%20identified%20outliers%20from%20the%20minority%0Aclasses%20are%20then%20removed%2C%20creating%20a%20refined%20dataset%20for%20subsequent%0Aoversampling%20using%20the%20hybrid%20approach%20called%20iHHO-SMOTe.%20The%20comprehensive%0Aexperiments%20across%20diverse%20datasets%20demonstrate%20the%20exceptional%20performance%20of%0Athe%20proposed%20model%2C%20with%20an%20AUC%20score%20exceeding%200.99%2C%20a%20high%20G-means%20score%20of%0A0.99%20highlighting%20its%20robustness%2C%20and%20an%20outstanding%20F1-score%20consistently%0Aexceeding%200.967.%20These%20findings%20collectively%20establish%20Cleansed%20iHHO-SMOTe%20as%20a%0Aformidable%20contender%20in%20addressing%20imbalanced%20datasets%2C%20focusing%20on%20noise%0Areduction%20and%20outlier%20handling%20for%20improved%20classification%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12850v1&entry.124074799=Read"},
{"title": "Unified Domain Adaptive Semantic Segmentation", "author": "Zhe Zhang and Gaochang Wu and Jing Zhang and Xiatian Zhu and Dacheng Tao and Tianyou Chai", "abstract": "  Unsupervised Domain Adaptive Semantic Segmentation (UDA-SS) aims to transfer\nthe supervision from a labeled source domain to an unlabeled target domain. The\nmajority of existing UDA-SS works typically consider images whilst recent\nattempts have extended further to tackle videos by modeling the temporal\ndimension. Although the two lines of research share the major challenges --\novercoming the underlying domain distribution shift, their studies are largely\nindependent, resulting in fragmented insights, a lack of holistic\nunderstanding, and missed opportunities for cross-pollination of ideas. This\nfragmentation prevents the unification of methods, leading to redundant efforts\nand suboptimal knowledge transfer across image and video domains. Under this\nobservation, we advocate unifying the study of UDA-SS across video and image\nscenarios, enabling a more comprehensive understanding, synergistic\nadvancements, and efficient knowledge sharing. To that end, we explore the\nunified UDA-SS from a general data augmentation perspective, serving as a\nunifying conceptual framework, enabling improved generalization, and potential\nfor cross-pollination of ideas, ultimately contributing to the overall progress\nand practical impact of this field of research. Specifically, we propose a\nQuad-directional Mixup (QuadMix) method, characterized by tackling distinct\npoint attributes and feature inconsistencies through four-directional paths for\nintra- and inter-domain mixing in a feature space. To deal with temporal shifts\nwith videos, we incorporate optical flow-guided feature aggregation across\nspatial and temporal dimensions for fine-grained domain alignment. Extensive\nexperiments show that our method outperforms the state-of-the-art works by\nlarge margins on four challenging UDA-SS benchmarks. Our source code and models\nwill be released at https://github.com/ZHE-SAPI/UDASS.\n", "link": "http://arxiv.org/abs/2311.13254v4", "date": "2025-04-17", "relevancy": 2.3067, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5855}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5786}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5713}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Domain%20Adaptive%20Semantic%20Segmentation&body=Title%3A%20Unified%20Domain%20Adaptive%20Semantic%20Segmentation%0AAuthor%3A%20Zhe%20Zhang%20and%20Gaochang%20Wu%20and%20Jing%20Zhang%20and%20Xiatian%20Zhu%20and%20Dacheng%20Tao%20and%20Tianyou%20Chai%0AAbstract%3A%20%20%20Unsupervised%20Domain%20Adaptive%20Semantic%20Segmentation%20%28UDA-SS%29%20aims%20to%20transfer%0Athe%20supervision%20from%20a%20labeled%20source%20domain%20to%20an%20unlabeled%20target%20domain.%20The%0Amajority%20of%20existing%20UDA-SS%20works%20typically%20consider%20images%20whilst%20recent%0Aattempts%20have%20extended%20further%20to%20tackle%20videos%20by%20modeling%20the%20temporal%0Adimension.%20Although%20the%20two%20lines%20of%20research%20share%20the%20major%20challenges%20--%0Aovercoming%20the%20underlying%20domain%20distribution%20shift%2C%20their%20studies%20are%20largely%0Aindependent%2C%20resulting%20in%20fragmented%20insights%2C%20a%20lack%20of%20holistic%0Aunderstanding%2C%20and%20missed%20opportunities%20for%20cross-pollination%20of%20ideas.%20This%0Afragmentation%20prevents%20the%20unification%20of%20methods%2C%20leading%20to%20redundant%20efforts%0Aand%20suboptimal%20knowledge%20transfer%20across%20image%20and%20video%20domains.%20Under%20this%0Aobservation%2C%20we%20advocate%20unifying%20the%20study%20of%20UDA-SS%20across%20video%20and%20image%0Ascenarios%2C%20enabling%20a%20more%20comprehensive%20understanding%2C%20synergistic%0Aadvancements%2C%20and%20efficient%20knowledge%20sharing.%20To%20that%20end%2C%20we%20explore%20the%0Aunified%20UDA-SS%20from%20a%20general%20data%20augmentation%20perspective%2C%20serving%20as%20a%0Aunifying%20conceptual%20framework%2C%20enabling%20improved%20generalization%2C%20and%20potential%0Afor%20cross-pollination%20of%20ideas%2C%20ultimately%20contributing%20to%20the%20overall%20progress%0Aand%20practical%20impact%20of%20this%20field%20of%20research.%20Specifically%2C%20we%20propose%20a%0AQuad-directional%20Mixup%20%28QuadMix%29%20method%2C%20characterized%20by%20tackling%20distinct%0Apoint%20attributes%20and%20feature%20inconsistencies%20through%20four-directional%20paths%20for%0Aintra-%20and%20inter-domain%20mixing%20in%20a%20feature%20space.%20To%20deal%20with%20temporal%20shifts%0Awith%20videos%2C%20we%20incorporate%20optical%20flow-guided%20feature%20aggregation%20across%0Aspatial%20and%20temporal%20dimensions%20for%20fine-grained%20domain%20alignment.%20Extensive%0Aexperiments%20show%20that%20our%20method%20outperforms%20the%20state-of-the-art%20works%20by%0Alarge%20margins%20on%20four%20challenging%20UDA-SS%20benchmarks.%20Our%20source%20code%20and%20models%0Awill%20be%20released%20at%20https%3A//github.com/ZHE-SAPI/UDASS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.13254v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Domain%2520Adaptive%2520Semantic%2520Segmentation%26entry.906535625%3DZhe%2520Zhang%2520and%2520Gaochang%2520Wu%2520and%2520Jing%2520Zhang%2520and%2520Xiatian%2520Zhu%2520and%2520Dacheng%2520Tao%2520and%2520Tianyou%2520Chai%26entry.1292438233%3D%2520%2520Unsupervised%2520Domain%2520Adaptive%2520Semantic%2520Segmentation%2520%2528UDA-SS%2529%2520aims%2520to%2520transfer%250Athe%2520supervision%2520from%2520a%2520labeled%2520source%2520domain%2520to%2520an%2520unlabeled%2520target%2520domain.%2520The%250Amajority%2520of%2520existing%2520UDA-SS%2520works%2520typically%2520consider%2520images%2520whilst%2520recent%250Aattempts%2520have%2520extended%2520further%2520to%2520tackle%2520videos%2520by%2520modeling%2520the%2520temporal%250Adimension.%2520Although%2520the%2520two%2520lines%2520of%2520research%2520share%2520the%2520major%2520challenges%2520--%250Aovercoming%2520the%2520underlying%2520domain%2520distribution%2520shift%252C%2520their%2520studies%2520are%2520largely%250Aindependent%252C%2520resulting%2520in%2520fragmented%2520insights%252C%2520a%2520lack%2520of%2520holistic%250Aunderstanding%252C%2520and%2520missed%2520opportunities%2520for%2520cross-pollination%2520of%2520ideas.%2520This%250Afragmentation%2520prevents%2520the%2520unification%2520of%2520methods%252C%2520leading%2520to%2520redundant%2520efforts%250Aand%2520suboptimal%2520knowledge%2520transfer%2520across%2520image%2520and%2520video%2520domains.%2520Under%2520this%250Aobservation%252C%2520we%2520advocate%2520unifying%2520the%2520study%2520of%2520UDA-SS%2520across%2520video%2520and%2520image%250Ascenarios%252C%2520enabling%2520a%2520more%2520comprehensive%2520understanding%252C%2520synergistic%250Aadvancements%252C%2520and%2520efficient%2520knowledge%2520sharing.%2520To%2520that%2520end%252C%2520we%2520explore%2520the%250Aunified%2520UDA-SS%2520from%2520a%2520general%2520data%2520augmentation%2520perspective%252C%2520serving%2520as%2520a%250Aunifying%2520conceptual%2520framework%252C%2520enabling%2520improved%2520generalization%252C%2520and%2520potential%250Afor%2520cross-pollination%2520of%2520ideas%252C%2520ultimately%2520contributing%2520to%2520the%2520overall%2520progress%250Aand%2520practical%2520impact%2520of%2520this%2520field%2520of%2520research.%2520Specifically%252C%2520we%2520propose%2520a%250AQuad-directional%2520Mixup%2520%2528QuadMix%2529%2520method%252C%2520characterized%2520by%2520tackling%2520distinct%250Apoint%2520attributes%2520and%2520feature%2520inconsistencies%2520through%2520four-directional%2520paths%2520for%250Aintra-%2520and%2520inter-domain%2520mixing%2520in%2520a%2520feature%2520space.%2520To%2520deal%2520with%2520temporal%2520shifts%250Awith%2520videos%252C%2520we%2520incorporate%2520optical%2520flow-guided%2520feature%2520aggregation%2520across%250Aspatial%2520and%2520temporal%2520dimensions%2520for%2520fine-grained%2520domain%2520alignment.%2520Extensive%250Aexperiments%2520show%2520that%2520our%2520method%2520outperforms%2520the%2520state-of-the-art%2520works%2520by%250Alarge%2520margins%2520on%2520four%2520challenging%2520UDA-SS%2520benchmarks.%2520Our%2520source%2520code%2520and%2520models%250Awill%2520be%2520released%2520at%2520https%253A//github.com/ZHE-SAPI/UDASS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.13254v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Domain%20Adaptive%20Semantic%20Segmentation&entry.906535625=Zhe%20Zhang%20and%20Gaochang%20Wu%20and%20Jing%20Zhang%20and%20Xiatian%20Zhu%20and%20Dacheng%20Tao%20and%20Tianyou%20Chai&entry.1292438233=%20%20Unsupervised%20Domain%20Adaptive%20Semantic%20Segmentation%20%28UDA-SS%29%20aims%20to%20transfer%0Athe%20supervision%20from%20a%20labeled%20source%20domain%20to%20an%20unlabeled%20target%20domain.%20The%0Amajority%20of%20existing%20UDA-SS%20works%20typically%20consider%20images%20whilst%20recent%0Aattempts%20have%20extended%20further%20to%20tackle%20videos%20by%20modeling%20the%20temporal%0Adimension.%20Although%20the%20two%20lines%20of%20research%20share%20the%20major%20challenges%20--%0Aovercoming%20the%20underlying%20domain%20distribution%20shift%2C%20their%20studies%20are%20largely%0Aindependent%2C%20resulting%20in%20fragmented%20insights%2C%20a%20lack%20of%20holistic%0Aunderstanding%2C%20and%20missed%20opportunities%20for%20cross-pollination%20of%20ideas.%20This%0Afragmentation%20prevents%20the%20unification%20of%20methods%2C%20leading%20to%20redundant%20efforts%0Aand%20suboptimal%20knowledge%20transfer%20across%20image%20and%20video%20domains.%20Under%20this%0Aobservation%2C%20we%20advocate%20unifying%20the%20study%20of%20UDA-SS%20across%20video%20and%20image%0Ascenarios%2C%20enabling%20a%20more%20comprehensive%20understanding%2C%20synergistic%0Aadvancements%2C%20and%20efficient%20knowledge%20sharing.%20To%20that%20end%2C%20we%20explore%20the%0Aunified%20UDA-SS%20from%20a%20general%20data%20augmentation%20perspective%2C%20serving%20as%20a%0Aunifying%20conceptual%20framework%2C%20enabling%20improved%20generalization%2C%20and%20potential%0Afor%20cross-pollination%20of%20ideas%2C%20ultimately%20contributing%20to%20the%20overall%20progress%0Aand%20practical%20impact%20of%20this%20field%20of%20research.%20Specifically%2C%20we%20propose%20a%0AQuad-directional%20Mixup%20%28QuadMix%29%20method%2C%20characterized%20by%20tackling%20distinct%0Apoint%20attributes%20and%20feature%20inconsistencies%20through%20four-directional%20paths%20for%0Aintra-%20and%20inter-domain%20mixing%20in%20a%20feature%20space.%20To%20deal%20with%20temporal%20shifts%0Awith%20videos%2C%20we%20incorporate%20optical%20flow-guided%20feature%20aggregation%20across%0Aspatial%20and%20temporal%20dimensions%20for%20fine-grained%20domain%20alignment.%20Extensive%0Aexperiments%20show%20that%20our%20method%20outperforms%20the%20state-of-the-art%20works%20by%0Alarge%20margins%20on%20four%20challenging%20UDA-SS%20benchmarks.%20Our%20source%20code%20and%20models%0Awill%20be%20released%20at%20https%3A//github.com/ZHE-SAPI/UDASS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.13254v4&entry.124074799=Read"},
{"title": "IdentiARAT: Toward Automated Identification of Individual ARAT Items\n  from Wearable Sensors", "author": "Daniel Homm and Patrick Carqueville and Christian Eichhorn and Thomas Weikert and Thomas Menard and David A. Plecher and Chris Awai Easthope", "abstract": "  This study explores the potential of using wrist-worn inertial sensors to\nautomate the labeling of ARAT (Action Research Arm Test) items. While the ARAT\nis commonly used to assess upper limb motor function, its limitations include\nsubjectivity and time consumption of clinical staff. By using IMU (Inertial\nMeasurement Unit) sensors and MiniROCKET as a time series classification\ntechnique, this investigation aims to classify ARAT items based on sensor\nrecordings. We test common preprocessing strategies to efficiently leverage\nincluded information in the data. Afterward, we use the best preprocessing to\nimprove the classification. The dataset includes recordings of 45 participants\nperforming various ARAT items. Results show that MiniROCKET offers a fast and\nreliable approach for classifying ARAT domains, although challenges remain in\ndistinguishing between individual resembling items. Future work may involve\nimproving classification through more advanced machine-learning models and data\nenhancements.\n", "link": "http://arxiv.org/abs/2504.12921v1", "date": "2025-04-17", "relevancy": 1.8205, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4845}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4533}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IdentiARAT%3A%20Toward%20Automated%20Identification%20of%20Individual%20ARAT%20Items%0A%20%20from%20Wearable%20Sensors&body=Title%3A%20IdentiARAT%3A%20Toward%20Automated%20Identification%20of%20Individual%20ARAT%20Items%0A%20%20from%20Wearable%20Sensors%0AAuthor%3A%20Daniel%20Homm%20and%20Patrick%20Carqueville%20and%20Christian%20Eichhorn%20and%20Thomas%20Weikert%20and%20Thomas%20Menard%20and%20David%20A.%20Plecher%20and%20Chris%20Awai%20Easthope%0AAbstract%3A%20%20%20This%20study%20explores%20the%20potential%20of%20using%20wrist-worn%20inertial%20sensors%20to%0Aautomate%20the%20labeling%20of%20ARAT%20%28Action%20Research%20Arm%20Test%29%20items.%20While%20the%20ARAT%0Ais%20commonly%20used%20to%20assess%20upper%20limb%20motor%20function%2C%20its%20limitations%20include%0Asubjectivity%20and%20time%20consumption%20of%20clinical%20staff.%20By%20using%20IMU%20%28Inertial%0AMeasurement%20Unit%29%20sensors%20and%20MiniROCKET%20as%20a%20time%20series%20classification%0Atechnique%2C%20this%20investigation%20aims%20to%20classify%20ARAT%20items%20based%20on%20sensor%0Arecordings.%20We%20test%20common%20preprocessing%20strategies%20to%20efficiently%20leverage%0Aincluded%20information%20in%20the%20data.%20Afterward%2C%20we%20use%20the%20best%20preprocessing%20to%0Aimprove%20the%20classification.%20The%20dataset%20includes%20recordings%20of%2045%20participants%0Aperforming%20various%20ARAT%20items.%20Results%20show%20that%20MiniROCKET%20offers%20a%20fast%20and%0Areliable%20approach%20for%20classifying%20ARAT%20domains%2C%20although%20challenges%20remain%20in%0Adistinguishing%20between%20individual%20resembling%20items.%20Future%20work%20may%20involve%0Aimproving%20classification%20through%20more%20advanced%20machine-learning%20models%20and%20data%0Aenhancements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12921v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentiARAT%253A%2520Toward%2520Automated%2520Identification%2520of%2520Individual%2520ARAT%2520Items%250A%2520%2520from%2520Wearable%2520Sensors%26entry.906535625%3DDaniel%2520Homm%2520and%2520Patrick%2520Carqueville%2520and%2520Christian%2520Eichhorn%2520and%2520Thomas%2520Weikert%2520and%2520Thomas%2520Menard%2520and%2520David%2520A.%2520Plecher%2520and%2520Chris%2520Awai%2520Easthope%26entry.1292438233%3D%2520%2520This%2520study%2520explores%2520the%2520potential%2520of%2520using%2520wrist-worn%2520inertial%2520sensors%2520to%250Aautomate%2520the%2520labeling%2520of%2520ARAT%2520%2528Action%2520Research%2520Arm%2520Test%2529%2520items.%2520While%2520the%2520ARAT%250Ais%2520commonly%2520used%2520to%2520assess%2520upper%2520limb%2520motor%2520function%252C%2520its%2520limitations%2520include%250Asubjectivity%2520and%2520time%2520consumption%2520of%2520clinical%2520staff.%2520By%2520using%2520IMU%2520%2528Inertial%250AMeasurement%2520Unit%2529%2520sensors%2520and%2520MiniROCKET%2520as%2520a%2520time%2520series%2520classification%250Atechnique%252C%2520this%2520investigation%2520aims%2520to%2520classify%2520ARAT%2520items%2520based%2520on%2520sensor%250Arecordings.%2520We%2520test%2520common%2520preprocessing%2520strategies%2520to%2520efficiently%2520leverage%250Aincluded%2520information%2520in%2520the%2520data.%2520Afterward%252C%2520we%2520use%2520the%2520best%2520preprocessing%2520to%250Aimprove%2520the%2520classification.%2520The%2520dataset%2520includes%2520recordings%2520of%252045%2520participants%250Aperforming%2520various%2520ARAT%2520items.%2520Results%2520show%2520that%2520MiniROCKET%2520offers%2520a%2520fast%2520and%250Areliable%2520approach%2520for%2520classifying%2520ARAT%2520domains%252C%2520although%2520challenges%2520remain%2520in%250Adistinguishing%2520between%2520individual%2520resembling%2520items.%2520Future%2520work%2520may%2520involve%250Aimproving%2520classification%2520through%2520more%2520advanced%2520machine-learning%2520models%2520and%2520data%250Aenhancements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12921v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IdentiARAT%3A%20Toward%20Automated%20Identification%20of%20Individual%20ARAT%20Items%0A%20%20from%20Wearable%20Sensors&entry.906535625=Daniel%20Homm%20and%20Patrick%20Carqueville%20and%20Christian%20Eichhorn%20and%20Thomas%20Weikert%20and%20Thomas%20Menard%20and%20David%20A.%20Plecher%20and%20Chris%20Awai%20Easthope&entry.1292438233=%20%20This%20study%20explores%20the%20potential%20of%20using%20wrist-worn%20inertial%20sensors%20to%0Aautomate%20the%20labeling%20of%20ARAT%20%28Action%20Research%20Arm%20Test%29%20items.%20While%20the%20ARAT%0Ais%20commonly%20used%20to%20assess%20upper%20limb%20motor%20function%2C%20its%20limitations%20include%0Asubjectivity%20and%20time%20consumption%20of%20clinical%20staff.%20By%20using%20IMU%20%28Inertial%0AMeasurement%20Unit%29%20sensors%20and%20MiniROCKET%20as%20a%20time%20series%20classification%0Atechnique%2C%20this%20investigation%20aims%20to%20classify%20ARAT%20items%20based%20on%20sensor%0Arecordings.%20We%20test%20common%20preprocessing%20strategies%20to%20efficiently%20leverage%0Aincluded%20information%20in%20the%20data.%20Afterward%2C%20we%20use%20the%20best%20preprocessing%20to%0Aimprove%20the%20classification.%20The%20dataset%20includes%20recordings%20of%2045%20participants%0Aperforming%20various%20ARAT%20items.%20Results%20show%20that%20MiniROCKET%20offers%20a%20fast%20and%0Areliable%20approach%20for%20classifying%20ARAT%20domains%2C%20although%20challenges%20remain%20in%0Adistinguishing%20between%20individual%20resembling%20items.%20Future%20work%20may%20involve%0Aimproving%20classification%20through%20more%20advanced%20machine-learning%20models%20and%20data%0Aenhancements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12921v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


