<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="#"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GSAC: Leveraging Gaussian Splatting for Photorealistic Avatar Creation\n  with Unity Integration", "author": "Rendong Zhang and Alexandra Watkins and Nilanjan Sarkar", "abstract": "  Photorealistic avatars have become essential for immersive applications in\nvirtual reality (VR) and augmented reality (AR), enabling lifelike interactions\nin areas such as training simulations, telemedicine, and virtual collaboration.\nThese avatars bridge the gap between the physical and digital worlds, improving\nthe user experience through realistic human representation. However, existing\navatar creation techniques face significant challenges, including high costs,\nlong creation times, and limited utility in virtual applications. Manual\nmethods, such as MetaHuman, require extensive time and expertise, while\nautomatic approaches, such as NeRF-based pipelines often lack efficiency,\ndetailed facial expression fidelity, and are unable to be rendered at a speed\nsufficent for real-time applications. By involving several cutting-edge modern\ntechniques, we introduce an end-to-end 3D Gaussian Splatting (3DGS) avatar\ncreation pipeline that leverages monocular video input to create a scalable and\nefficient photorealistic avatar directly compatible with the Unity game engine.\nOur pipeline incorporates a novel Gaussian splatting technique with customized\npreprocessing that enables the user of \"in the wild\" monocular video capture,\ndetailed facial expression reconstruction and embedding within a fully rigged\navatar model. Additionally, we present a Unity-integrated Gaussian Splatting\nAvatar Editor, offering a user-friendly environment for VR/AR application\ndevelopment. Experimental results validate the effectiveness of our\npreprocessing pipeline in standardizing custom data for 3DGS training and\ndemonstrate the versatility of Gaussian avatars in Unity, highlighting the\nscalability and practicality of our approach.\n", "link": "http://arxiv.org/abs/2504.12999v1", "date": "2025-04-17", "relevancy": 3.5381, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7139}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7139}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GSAC%3A%20Leveraging%20Gaussian%20Splatting%20for%20Photorealistic%20Avatar%20Creation%0A%20%20with%20Unity%20Integration&body=Title%3A%20GSAC%3A%20Leveraging%20Gaussian%20Splatting%20for%20Photorealistic%20Avatar%20Creation%0A%20%20with%20Unity%20Integration%0AAuthor%3A%20Rendong%20Zhang%20and%20Alexandra%20Watkins%20and%20Nilanjan%20Sarkar%0AAbstract%3A%20%20%20Photorealistic%20avatars%20have%20become%20essential%20for%20immersive%20applications%20in%0Avirtual%20reality%20%28VR%29%20and%20augmented%20reality%20%28AR%29%2C%20enabling%20lifelike%20interactions%0Ain%20areas%20such%20as%20training%20simulations%2C%20telemedicine%2C%20and%20virtual%20collaboration.%0AThese%20avatars%20bridge%20the%20gap%20between%20the%20physical%20and%20digital%20worlds%2C%20improving%0Athe%20user%20experience%20through%20realistic%20human%20representation.%20However%2C%20existing%0Aavatar%20creation%20techniques%20face%20significant%20challenges%2C%20including%20high%20costs%2C%0Along%20creation%20times%2C%20and%20limited%20utility%20in%20virtual%20applications.%20Manual%0Amethods%2C%20such%20as%20MetaHuman%2C%20require%20extensive%20time%20and%20expertise%2C%20while%0Aautomatic%20approaches%2C%20such%20as%20NeRF-based%20pipelines%20often%20lack%20efficiency%2C%0Adetailed%20facial%20expression%20fidelity%2C%20and%20are%20unable%20to%20be%20rendered%20at%20a%20speed%0Asufficent%20for%20real-time%20applications.%20By%20involving%20several%20cutting-edge%20modern%0Atechniques%2C%20we%20introduce%20an%20end-to-end%203D%20Gaussian%20Splatting%20%283DGS%29%20avatar%0Acreation%20pipeline%20that%20leverages%20monocular%20video%20input%20to%20create%20a%20scalable%20and%0Aefficient%20photorealistic%20avatar%20directly%20compatible%20with%20the%20Unity%20game%20engine.%0AOur%20pipeline%20incorporates%20a%20novel%20Gaussian%20splatting%20technique%20with%20customized%0Apreprocessing%20that%20enables%20the%20user%20of%20%22in%20the%20wild%22%20monocular%20video%20capture%2C%0Adetailed%20facial%20expression%20reconstruction%20and%20embedding%20within%20a%20fully%20rigged%0Aavatar%20model.%20Additionally%2C%20we%20present%20a%20Unity-integrated%20Gaussian%20Splatting%0AAvatar%20Editor%2C%20offering%20a%20user-friendly%20environment%20for%20VR/AR%20application%0Adevelopment.%20Experimental%20results%20validate%20the%20effectiveness%20of%20our%0Apreprocessing%20pipeline%20in%20standardizing%20custom%20data%20for%203DGS%20training%20and%0Ademonstrate%20the%20versatility%20of%20Gaussian%20avatars%20in%20Unity%2C%20highlighting%20the%0Ascalability%20and%20practicality%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12999v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGSAC%253A%2520Leveraging%2520Gaussian%2520Splatting%2520for%2520Photorealistic%2520Avatar%2520Creation%250A%2520%2520with%2520Unity%2520Integration%26entry.906535625%3DRendong%2520Zhang%2520and%2520Alexandra%2520Watkins%2520and%2520Nilanjan%2520Sarkar%26entry.1292438233%3D%2520%2520Photorealistic%2520avatars%2520have%2520become%2520essential%2520for%2520immersive%2520applications%2520in%250Avirtual%2520reality%2520%2528VR%2529%2520and%2520augmented%2520reality%2520%2528AR%2529%252C%2520enabling%2520lifelike%2520interactions%250Ain%2520areas%2520such%2520as%2520training%2520simulations%252C%2520telemedicine%252C%2520and%2520virtual%2520collaboration.%250AThese%2520avatars%2520bridge%2520the%2520gap%2520between%2520the%2520physical%2520and%2520digital%2520worlds%252C%2520improving%250Athe%2520user%2520experience%2520through%2520realistic%2520human%2520representation.%2520However%252C%2520existing%250Aavatar%2520creation%2520techniques%2520face%2520significant%2520challenges%252C%2520including%2520high%2520costs%252C%250Along%2520creation%2520times%252C%2520and%2520limited%2520utility%2520in%2520virtual%2520applications.%2520Manual%250Amethods%252C%2520such%2520as%2520MetaHuman%252C%2520require%2520extensive%2520time%2520and%2520expertise%252C%2520while%250Aautomatic%2520approaches%252C%2520such%2520as%2520NeRF-based%2520pipelines%2520often%2520lack%2520efficiency%252C%250Adetailed%2520facial%2520expression%2520fidelity%252C%2520and%2520are%2520unable%2520to%2520be%2520rendered%2520at%2520a%2520speed%250Asufficent%2520for%2520real-time%2520applications.%2520By%2520involving%2520several%2520cutting-edge%2520modern%250Atechniques%252C%2520we%2520introduce%2520an%2520end-to-end%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520avatar%250Acreation%2520pipeline%2520that%2520leverages%2520monocular%2520video%2520input%2520to%2520create%2520a%2520scalable%2520and%250Aefficient%2520photorealistic%2520avatar%2520directly%2520compatible%2520with%2520the%2520Unity%2520game%2520engine.%250AOur%2520pipeline%2520incorporates%2520a%2520novel%2520Gaussian%2520splatting%2520technique%2520with%2520customized%250Apreprocessing%2520that%2520enables%2520the%2520user%2520of%2520%2522in%2520the%2520wild%2522%2520monocular%2520video%2520capture%252C%250Adetailed%2520facial%2520expression%2520reconstruction%2520and%2520embedding%2520within%2520a%2520fully%2520rigged%250Aavatar%2520model.%2520Additionally%252C%2520we%2520present%2520a%2520Unity-integrated%2520Gaussian%2520Splatting%250AAvatar%2520Editor%252C%2520offering%2520a%2520user-friendly%2520environment%2520for%2520VR/AR%2520application%250Adevelopment.%2520Experimental%2520results%2520validate%2520the%2520effectiveness%2520of%2520our%250Apreprocessing%2520pipeline%2520in%2520standardizing%2520custom%2520data%2520for%25203DGS%2520training%2520and%250Ademonstrate%2520the%2520versatility%2520of%2520Gaussian%2520avatars%2520in%2520Unity%252C%2520highlighting%2520the%250Ascalability%2520and%2520practicality%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12999v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GSAC%3A%20Leveraging%20Gaussian%20Splatting%20for%20Photorealistic%20Avatar%20Creation%0A%20%20with%20Unity%20Integration&entry.906535625=Rendong%20Zhang%20and%20Alexandra%20Watkins%20and%20Nilanjan%20Sarkar&entry.1292438233=%20%20Photorealistic%20avatars%20have%20become%20essential%20for%20immersive%20applications%20in%0Avirtual%20reality%20%28VR%29%20and%20augmented%20reality%20%28AR%29%2C%20enabling%20lifelike%20interactions%0Ain%20areas%20such%20as%20training%20simulations%2C%20telemedicine%2C%20and%20virtual%20collaboration.%0AThese%20avatars%20bridge%20the%20gap%20between%20the%20physical%20and%20digital%20worlds%2C%20improving%0Athe%20user%20experience%20through%20realistic%20human%20representation.%20However%2C%20existing%0Aavatar%20creation%20techniques%20face%20significant%20challenges%2C%20including%20high%20costs%2C%0Along%20creation%20times%2C%20and%20limited%20utility%20in%20virtual%20applications.%20Manual%0Amethods%2C%20such%20as%20MetaHuman%2C%20require%20extensive%20time%20and%20expertise%2C%20while%0Aautomatic%20approaches%2C%20such%20as%20NeRF-based%20pipelines%20often%20lack%20efficiency%2C%0Adetailed%20facial%20expression%20fidelity%2C%20and%20are%20unable%20to%20be%20rendered%20at%20a%20speed%0Asufficent%20for%20real-time%20applications.%20By%20involving%20several%20cutting-edge%20modern%0Atechniques%2C%20we%20introduce%20an%20end-to-end%203D%20Gaussian%20Splatting%20%283DGS%29%20avatar%0Acreation%20pipeline%20that%20leverages%20monocular%20video%20input%20to%20create%20a%20scalable%20and%0Aefficient%20photorealistic%20avatar%20directly%20compatible%20with%20the%20Unity%20game%20engine.%0AOur%20pipeline%20incorporates%20a%20novel%20Gaussian%20splatting%20technique%20with%20customized%0Apreprocessing%20that%20enables%20the%20user%20of%20%22in%20the%20wild%22%20monocular%20video%20capture%2C%0Adetailed%20facial%20expression%20reconstruction%20and%20embedding%20within%20a%20fully%20rigged%0Aavatar%20model.%20Additionally%2C%20we%20present%20a%20Unity-integrated%20Gaussian%20Splatting%0AAvatar%20Editor%2C%20offering%20a%20user-friendly%20environment%20for%20VR/AR%20application%0Adevelopment.%20Experimental%20results%20validate%20the%20effectiveness%20of%20our%0Apreprocessing%20pipeline%20in%20standardizing%20custom%20data%20for%203DGS%20training%20and%0Ademonstrate%20the%20versatility%20of%20Gaussian%20avatars%20in%20Unity%2C%20highlighting%20the%0Ascalability%20and%20practicality%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12999v1&entry.124074799=Read"},
{"title": "Real-time High-fidelity Gaussian Human Avatars with Position-based\n  Interpolation of Spatially Distributed MLPs", "author": "Youyi Zhan and Tianjia Shao and Yin Yang and Kun Zhou", "abstract": "  Many works have succeeded in reconstructing Gaussian human avatars from\nmulti-view videos. However, they either struggle to capture pose-dependent\nappearance details with a single MLP, or rely on a computationally intensive\nneural network to reconstruct high-fidelity appearance but with rendering\nperformance degraded to non-real-time. We propose a novel Gaussian human avatar\nrepresentation that can reconstruct high-fidelity pose-dependence appearance\nwith details and meanwhile can be rendered in real time. Our Gaussian avatar is\nempowered by spatially distributed MLPs which are explicitly located on\ndifferent positions on human body. The parameters stored in each Gaussian are\nobtained by interpolating from the outputs of its nearby MLPs based on their\ndistances. To avoid undesired smooth Gaussian property changing during\ninterpolation, for each Gaussian we define a set of Gaussian offset basis, and\na linear combination of basis represents the Gaussian property offsets relative\nto the neutral properties. Then we propose to let the MLPs output a set of\ncoefficients corresponding to the basis. In this way, although Gaussian\ncoefficients are derived from interpolation and change smoothly, the Gaussian\noffset basis is learned freely without constraints. The smoothly varying\ncoefficients combined with freely learned basis can still produce distinctly\ndifferent Gaussian property offsets, allowing the ability to learn\nhigh-frequency spatial signals. We further use control points to constrain the\nGaussians distributed on a surface layer rather than allowing them to be\nirregularly distributed inside the body, to help the human avatar generalize\nbetter when animated under novel poses. Compared to the state-of-the-art\nmethod, our method achieves better appearance quality with finer details while\nthe rendering speed is significantly faster under novel views and novel poses.\n", "link": "http://arxiv.org/abs/2504.12909v1", "date": "2025-04-17", "relevancy": 3.4695, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7181}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7181}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-time%20High-fidelity%20Gaussian%20Human%20Avatars%20with%20Position-based%0A%20%20Interpolation%20of%20Spatially%20Distributed%20MLPs&body=Title%3A%20Real-time%20High-fidelity%20Gaussian%20Human%20Avatars%20with%20Position-based%0A%20%20Interpolation%20of%20Spatially%20Distributed%20MLPs%0AAuthor%3A%20Youyi%20Zhan%20and%20Tianjia%20Shao%20and%20Yin%20Yang%20and%20Kun%20Zhou%0AAbstract%3A%20%20%20Many%20works%20have%20succeeded%20in%20reconstructing%20Gaussian%20human%20avatars%20from%0Amulti-view%20videos.%20However%2C%20they%20either%20struggle%20to%20capture%20pose-dependent%0Aappearance%20details%20with%20a%20single%20MLP%2C%20or%20rely%20on%20a%20computationally%20intensive%0Aneural%20network%20to%20reconstruct%20high-fidelity%20appearance%20but%20with%20rendering%0Aperformance%20degraded%20to%20non-real-time.%20We%20propose%20a%20novel%20Gaussian%20human%20avatar%0Arepresentation%20that%20can%20reconstruct%20high-fidelity%20pose-dependence%20appearance%0Awith%20details%20and%20meanwhile%20can%20be%20rendered%20in%20real%20time.%20Our%20Gaussian%20avatar%20is%0Aempowered%20by%20spatially%20distributed%20MLPs%20which%20are%20explicitly%20located%20on%0Adifferent%20positions%20on%20human%20body.%20The%20parameters%20stored%20in%20each%20Gaussian%20are%0Aobtained%20by%20interpolating%20from%20the%20outputs%20of%20its%20nearby%20MLPs%20based%20on%20their%0Adistances.%20To%20avoid%20undesired%20smooth%20Gaussian%20property%20changing%20during%0Ainterpolation%2C%20for%20each%20Gaussian%20we%20define%20a%20set%20of%20Gaussian%20offset%20basis%2C%20and%0Aa%20linear%20combination%20of%20basis%20represents%20the%20Gaussian%20property%20offsets%20relative%0Ato%20the%20neutral%20properties.%20Then%20we%20propose%20to%20let%20the%20MLPs%20output%20a%20set%20of%0Acoefficients%20corresponding%20to%20the%20basis.%20In%20this%20way%2C%20although%20Gaussian%0Acoefficients%20are%20derived%20from%20interpolation%20and%20change%20smoothly%2C%20the%20Gaussian%0Aoffset%20basis%20is%20learned%20freely%20without%20constraints.%20The%20smoothly%20varying%0Acoefficients%20combined%20with%20freely%20learned%20basis%20can%20still%20produce%20distinctly%0Adifferent%20Gaussian%20property%20offsets%2C%20allowing%20the%20ability%20to%20learn%0Ahigh-frequency%20spatial%20signals.%20We%20further%20use%20control%20points%20to%20constrain%20the%0AGaussians%20distributed%20on%20a%20surface%20layer%20rather%20than%20allowing%20them%20to%20be%0Airregularly%20distributed%20inside%20the%20body%2C%20to%20help%20the%20human%20avatar%20generalize%0Abetter%20when%20animated%20under%20novel%20poses.%20Compared%20to%20the%20state-of-the-art%0Amethod%2C%20our%20method%20achieves%20better%20appearance%20quality%20with%20finer%20details%20while%0Athe%20rendering%20speed%20is%20significantly%20faster%20under%20novel%20views%20and%20novel%20poses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12909v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-time%2520High-fidelity%2520Gaussian%2520Human%2520Avatars%2520with%2520Position-based%250A%2520%2520Interpolation%2520of%2520Spatially%2520Distributed%2520MLPs%26entry.906535625%3DYouyi%2520Zhan%2520and%2520Tianjia%2520Shao%2520and%2520Yin%2520Yang%2520and%2520Kun%2520Zhou%26entry.1292438233%3D%2520%2520Many%2520works%2520have%2520succeeded%2520in%2520reconstructing%2520Gaussian%2520human%2520avatars%2520from%250Amulti-view%2520videos.%2520However%252C%2520they%2520either%2520struggle%2520to%2520capture%2520pose-dependent%250Aappearance%2520details%2520with%2520a%2520single%2520MLP%252C%2520or%2520rely%2520on%2520a%2520computationally%2520intensive%250Aneural%2520network%2520to%2520reconstruct%2520high-fidelity%2520appearance%2520but%2520with%2520rendering%250Aperformance%2520degraded%2520to%2520non-real-time.%2520We%2520propose%2520a%2520novel%2520Gaussian%2520human%2520avatar%250Arepresentation%2520that%2520can%2520reconstruct%2520high-fidelity%2520pose-dependence%2520appearance%250Awith%2520details%2520and%2520meanwhile%2520can%2520be%2520rendered%2520in%2520real%2520time.%2520Our%2520Gaussian%2520avatar%2520is%250Aempowered%2520by%2520spatially%2520distributed%2520MLPs%2520which%2520are%2520explicitly%2520located%2520on%250Adifferent%2520positions%2520on%2520human%2520body.%2520The%2520parameters%2520stored%2520in%2520each%2520Gaussian%2520are%250Aobtained%2520by%2520interpolating%2520from%2520the%2520outputs%2520of%2520its%2520nearby%2520MLPs%2520based%2520on%2520their%250Adistances.%2520To%2520avoid%2520undesired%2520smooth%2520Gaussian%2520property%2520changing%2520during%250Ainterpolation%252C%2520for%2520each%2520Gaussian%2520we%2520define%2520a%2520set%2520of%2520Gaussian%2520offset%2520basis%252C%2520and%250Aa%2520linear%2520combination%2520of%2520basis%2520represents%2520the%2520Gaussian%2520property%2520offsets%2520relative%250Ato%2520the%2520neutral%2520properties.%2520Then%2520we%2520propose%2520to%2520let%2520the%2520MLPs%2520output%2520a%2520set%2520of%250Acoefficients%2520corresponding%2520to%2520the%2520basis.%2520In%2520this%2520way%252C%2520although%2520Gaussian%250Acoefficients%2520are%2520derived%2520from%2520interpolation%2520and%2520change%2520smoothly%252C%2520the%2520Gaussian%250Aoffset%2520basis%2520is%2520learned%2520freely%2520without%2520constraints.%2520The%2520smoothly%2520varying%250Acoefficients%2520combined%2520with%2520freely%2520learned%2520basis%2520can%2520still%2520produce%2520distinctly%250Adifferent%2520Gaussian%2520property%2520offsets%252C%2520allowing%2520the%2520ability%2520to%2520learn%250Ahigh-frequency%2520spatial%2520signals.%2520We%2520further%2520use%2520control%2520points%2520to%2520constrain%2520the%250AGaussians%2520distributed%2520on%2520a%2520surface%2520layer%2520rather%2520than%2520allowing%2520them%2520to%2520be%250Airregularly%2520distributed%2520inside%2520the%2520body%252C%2520to%2520help%2520the%2520human%2520avatar%2520generalize%250Abetter%2520when%2520animated%2520under%2520novel%2520poses.%2520Compared%2520to%2520the%2520state-of-the-art%250Amethod%252C%2520our%2520method%2520achieves%2520better%2520appearance%2520quality%2520with%2520finer%2520details%2520while%250Athe%2520rendering%2520speed%2520is%2520significantly%2520faster%2520under%2520novel%2520views%2520and%2520novel%2520poses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12909v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-time%20High-fidelity%20Gaussian%20Human%20Avatars%20with%20Position-based%0A%20%20Interpolation%20of%20Spatially%20Distributed%20MLPs&entry.906535625=Youyi%20Zhan%20and%20Tianjia%20Shao%20and%20Yin%20Yang%20and%20Kun%20Zhou&entry.1292438233=%20%20Many%20works%20have%20succeeded%20in%20reconstructing%20Gaussian%20human%20avatars%20from%0Amulti-view%20videos.%20However%2C%20they%20either%20struggle%20to%20capture%20pose-dependent%0Aappearance%20details%20with%20a%20single%20MLP%2C%20or%20rely%20on%20a%20computationally%20intensive%0Aneural%20network%20to%20reconstruct%20high-fidelity%20appearance%20but%20with%20rendering%0Aperformance%20degraded%20to%20non-real-time.%20We%20propose%20a%20novel%20Gaussian%20human%20avatar%0Arepresentation%20that%20can%20reconstruct%20high-fidelity%20pose-dependence%20appearance%0Awith%20details%20and%20meanwhile%20can%20be%20rendered%20in%20real%20time.%20Our%20Gaussian%20avatar%20is%0Aempowered%20by%20spatially%20distributed%20MLPs%20which%20are%20explicitly%20located%20on%0Adifferent%20positions%20on%20human%20body.%20The%20parameters%20stored%20in%20each%20Gaussian%20are%0Aobtained%20by%20interpolating%20from%20the%20outputs%20of%20its%20nearby%20MLPs%20based%20on%20their%0Adistances.%20To%20avoid%20undesired%20smooth%20Gaussian%20property%20changing%20during%0Ainterpolation%2C%20for%20each%20Gaussian%20we%20define%20a%20set%20of%20Gaussian%20offset%20basis%2C%20and%0Aa%20linear%20combination%20of%20basis%20represents%20the%20Gaussian%20property%20offsets%20relative%0Ato%20the%20neutral%20properties.%20Then%20we%20propose%20to%20let%20the%20MLPs%20output%20a%20set%20of%0Acoefficients%20corresponding%20to%20the%20basis.%20In%20this%20way%2C%20although%20Gaussian%0Acoefficients%20are%20derived%20from%20interpolation%20and%20change%20smoothly%2C%20the%20Gaussian%0Aoffset%20basis%20is%20learned%20freely%20without%20constraints.%20The%20smoothly%20varying%0Acoefficients%20combined%20with%20freely%20learned%20basis%20can%20still%20produce%20distinctly%0Adifferent%20Gaussian%20property%20offsets%2C%20allowing%20the%20ability%20to%20learn%0Ahigh-frequency%20spatial%20signals.%20We%20further%20use%20control%20points%20to%20constrain%20the%0AGaussians%20distributed%20on%20a%20surface%20layer%20rather%20than%20allowing%20them%20to%20be%0Airregularly%20distributed%20inside%20the%20body%2C%20to%20help%20the%20human%20avatar%20generalize%0Abetter%20when%20animated%20under%20novel%20poses.%20Compared%20to%20the%20state-of-the-art%0Amethod%2C%20our%20method%20achieves%20better%20appearance%20quality%20with%20finer%20details%20while%0Athe%20rendering%20speed%20is%20significantly%20faster%20under%20novel%20views%20and%20novel%20poses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12909v1&entry.124074799=Read"},
{"title": "TSGS: Improving Gaussian Splatting for Transparent Surface\n  Reconstruction via Normal and De-lighting Priors", "author": "Mingwei Li and Pu Pang and Hehe Fan and Hua Huang and Yi Yang", "abstract": "  Reconstructing transparent surfaces is essential for tasks such as robotic\nmanipulation in labs, yet it poses a significant challenge for 3D\nreconstruction techniques like 3D Gaussian Splatting (3DGS). These methods\noften encounter a transparency-depth dilemma, where the pursuit of\nphotorealistic rendering through standard $\\alpha$-blending undermines\ngeometric precision, resulting in considerable depth estimation errors for\ntransparent materials. To address this issue, we introduce Transparent Surface\nGaussian Splatting (TSGS), a new framework that separates geometry learning\nfrom appearance refinement. In the geometry learning stage, TSGS focuses on\ngeometry by using specular-suppressed inputs to accurately represent surfaces.\nIn the second stage, TSGS improves visual fidelity through anisotropic specular\nmodeling, crucially maintaining the established opacity to ensure geometric\naccuracy. To enhance depth inference, TSGS employs a first-surface depth\nextraction method. This technique uses a sliding window over $\\alpha$-blending\nweights to pinpoint the most likely surface location and calculates a robust\nweighted average depth. To evaluate the transparent surface reconstruction task\nunder realistic conditions, we collect a TransLab dataset that includes complex\ntransparent laboratory glassware. Extensive experiments on TransLab show that\nTSGS achieves accurate geometric reconstruction and realistic rendering of\ntransparent objects simultaneously within the efficient 3DGS framework.\nSpecifically, TSGS significantly surpasses current leading methods, achieving a\n37.3% reduction in chamfer distance and an 8.0% improvement in F1 score\ncompared to the top baseline. The code and dataset will be released at\nhttps://longxiang-ai.github.io/TSGS/.\n", "link": "http://arxiv.org/abs/2504.12799v1", "date": "2025-04-17", "relevancy": 3.4681, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7418}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7058}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TSGS%3A%20Improving%20Gaussian%20Splatting%20for%20Transparent%20Surface%0A%20%20Reconstruction%20via%20Normal%20and%20De-lighting%20Priors&body=Title%3A%20TSGS%3A%20Improving%20Gaussian%20Splatting%20for%20Transparent%20Surface%0A%20%20Reconstruction%20via%20Normal%20and%20De-lighting%20Priors%0AAuthor%3A%20Mingwei%20Li%20and%20Pu%20Pang%20and%20Hehe%20Fan%20and%20Hua%20Huang%20and%20Yi%20Yang%0AAbstract%3A%20%20%20Reconstructing%20transparent%20surfaces%20is%20essential%20for%20tasks%20such%20as%20robotic%0Amanipulation%20in%20labs%2C%20yet%20it%20poses%20a%20significant%20challenge%20for%203D%0Areconstruction%20techniques%20like%203D%20Gaussian%20Splatting%20%283DGS%29.%20These%20methods%0Aoften%20encounter%20a%20transparency-depth%20dilemma%2C%20where%20the%20pursuit%20of%0Aphotorealistic%20rendering%20through%20standard%20%24%5Calpha%24-blending%20undermines%0Ageometric%20precision%2C%20resulting%20in%20considerable%20depth%20estimation%20errors%20for%0Atransparent%20materials.%20To%20address%20this%20issue%2C%20we%20introduce%20Transparent%20Surface%0AGaussian%20Splatting%20%28TSGS%29%2C%20a%20new%20framework%20that%20separates%20geometry%20learning%0Afrom%20appearance%20refinement.%20In%20the%20geometry%20learning%20stage%2C%20TSGS%20focuses%20on%0Ageometry%20by%20using%20specular-suppressed%20inputs%20to%20accurately%20represent%20surfaces.%0AIn%20the%20second%20stage%2C%20TSGS%20improves%20visual%20fidelity%20through%20anisotropic%20specular%0Amodeling%2C%20crucially%20maintaining%20the%20established%20opacity%20to%20ensure%20geometric%0Aaccuracy.%20To%20enhance%20depth%20inference%2C%20TSGS%20employs%20a%20first-surface%20depth%0Aextraction%20method.%20This%20technique%20uses%20a%20sliding%20window%20over%20%24%5Calpha%24-blending%0Aweights%20to%20pinpoint%20the%20most%20likely%20surface%20location%20and%20calculates%20a%20robust%0Aweighted%20average%20depth.%20To%20evaluate%20the%20transparent%20surface%20reconstruction%20task%0Aunder%20realistic%20conditions%2C%20we%20collect%20a%20TransLab%20dataset%20that%20includes%20complex%0Atransparent%20laboratory%20glassware.%20Extensive%20experiments%20on%20TransLab%20show%20that%0ATSGS%20achieves%20accurate%20geometric%20reconstruction%20and%20realistic%20rendering%20of%0Atransparent%20objects%20simultaneously%20within%20the%20efficient%203DGS%20framework.%0ASpecifically%2C%20TSGS%20significantly%20surpasses%20current%20leading%20methods%2C%20achieving%20a%0A37.3%25%20reduction%20in%20chamfer%20distance%20and%20an%208.0%25%20improvement%20in%20F1%20score%0Acompared%20to%20the%20top%20baseline.%20The%20code%20and%20dataset%20will%20be%20released%20at%0Ahttps%3A//longxiang-ai.github.io/TSGS/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12799v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTSGS%253A%2520Improving%2520Gaussian%2520Splatting%2520for%2520Transparent%2520Surface%250A%2520%2520Reconstruction%2520via%2520Normal%2520and%2520De-lighting%2520Priors%26entry.906535625%3DMingwei%2520Li%2520and%2520Pu%2520Pang%2520and%2520Hehe%2520Fan%2520and%2520Hua%2520Huang%2520and%2520Yi%2520Yang%26entry.1292438233%3D%2520%2520Reconstructing%2520transparent%2520surfaces%2520is%2520essential%2520for%2520tasks%2520such%2520as%2520robotic%250Amanipulation%2520in%2520labs%252C%2520yet%2520it%2520poses%2520a%2520significant%2520challenge%2520for%25203D%250Areconstruction%2520techniques%2520like%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529.%2520These%2520methods%250Aoften%2520encounter%2520a%2520transparency-depth%2520dilemma%252C%2520where%2520the%2520pursuit%2520of%250Aphotorealistic%2520rendering%2520through%2520standard%2520%2524%255Calpha%2524-blending%2520undermines%250Ageometric%2520precision%252C%2520resulting%2520in%2520considerable%2520depth%2520estimation%2520errors%2520for%250Atransparent%2520materials.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520Transparent%2520Surface%250AGaussian%2520Splatting%2520%2528TSGS%2529%252C%2520a%2520new%2520framework%2520that%2520separates%2520geometry%2520learning%250Afrom%2520appearance%2520refinement.%2520In%2520the%2520geometry%2520learning%2520stage%252C%2520TSGS%2520focuses%2520on%250Ageometry%2520by%2520using%2520specular-suppressed%2520inputs%2520to%2520accurately%2520represent%2520surfaces.%250AIn%2520the%2520second%2520stage%252C%2520TSGS%2520improves%2520visual%2520fidelity%2520through%2520anisotropic%2520specular%250Amodeling%252C%2520crucially%2520maintaining%2520the%2520established%2520opacity%2520to%2520ensure%2520geometric%250Aaccuracy.%2520To%2520enhance%2520depth%2520inference%252C%2520TSGS%2520employs%2520a%2520first-surface%2520depth%250Aextraction%2520method.%2520This%2520technique%2520uses%2520a%2520sliding%2520window%2520over%2520%2524%255Calpha%2524-blending%250Aweights%2520to%2520pinpoint%2520the%2520most%2520likely%2520surface%2520location%2520and%2520calculates%2520a%2520robust%250Aweighted%2520average%2520depth.%2520To%2520evaluate%2520the%2520transparent%2520surface%2520reconstruction%2520task%250Aunder%2520realistic%2520conditions%252C%2520we%2520collect%2520a%2520TransLab%2520dataset%2520that%2520includes%2520complex%250Atransparent%2520laboratory%2520glassware.%2520Extensive%2520experiments%2520on%2520TransLab%2520show%2520that%250ATSGS%2520achieves%2520accurate%2520geometric%2520reconstruction%2520and%2520realistic%2520rendering%2520of%250Atransparent%2520objects%2520simultaneously%2520within%2520the%2520efficient%25203DGS%2520framework.%250ASpecifically%252C%2520TSGS%2520significantly%2520surpasses%2520current%2520leading%2520methods%252C%2520achieving%2520a%250A37.3%2525%2520reduction%2520in%2520chamfer%2520distance%2520and%2520an%25208.0%2525%2520improvement%2520in%2520F1%2520score%250Acompared%2520to%2520the%2520top%2520baseline.%2520The%2520code%2520and%2520dataset%2520will%2520be%2520released%2520at%250Ahttps%253A//longxiang-ai.github.io/TSGS/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12799v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TSGS%3A%20Improving%20Gaussian%20Splatting%20for%20Transparent%20Surface%0A%20%20Reconstruction%20via%20Normal%20and%20De-lighting%20Priors&entry.906535625=Mingwei%20Li%20and%20Pu%20Pang%20and%20Hehe%20Fan%20and%20Hua%20Huang%20and%20Yi%20Yang&entry.1292438233=%20%20Reconstructing%20transparent%20surfaces%20is%20essential%20for%20tasks%20such%20as%20robotic%0Amanipulation%20in%20labs%2C%20yet%20it%20poses%20a%20significant%20challenge%20for%203D%0Areconstruction%20techniques%20like%203D%20Gaussian%20Splatting%20%283DGS%29.%20These%20methods%0Aoften%20encounter%20a%20transparency-depth%20dilemma%2C%20where%20the%20pursuit%20of%0Aphotorealistic%20rendering%20through%20standard%20%24%5Calpha%24-blending%20undermines%0Ageometric%20precision%2C%20resulting%20in%20considerable%20depth%20estimation%20errors%20for%0Atransparent%20materials.%20To%20address%20this%20issue%2C%20we%20introduce%20Transparent%20Surface%0AGaussian%20Splatting%20%28TSGS%29%2C%20a%20new%20framework%20that%20separates%20geometry%20learning%0Afrom%20appearance%20refinement.%20In%20the%20geometry%20learning%20stage%2C%20TSGS%20focuses%20on%0Ageometry%20by%20using%20specular-suppressed%20inputs%20to%20accurately%20represent%20surfaces.%0AIn%20the%20second%20stage%2C%20TSGS%20improves%20visual%20fidelity%20through%20anisotropic%20specular%0Amodeling%2C%20crucially%20maintaining%20the%20established%20opacity%20to%20ensure%20geometric%0Aaccuracy.%20To%20enhance%20depth%20inference%2C%20TSGS%20employs%20a%20first-surface%20depth%0Aextraction%20method.%20This%20technique%20uses%20a%20sliding%20window%20over%20%24%5Calpha%24-blending%0Aweights%20to%20pinpoint%20the%20most%20likely%20surface%20location%20and%20calculates%20a%20robust%0Aweighted%20average%20depth.%20To%20evaluate%20the%20transparent%20surface%20reconstruction%20task%0Aunder%20realistic%20conditions%2C%20we%20collect%20a%20TransLab%20dataset%20that%20includes%20complex%0Atransparent%20laboratory%20glassware.%20Extensive%20experiments%20on%20TransLab%20show%20that%0ATSGS%20achieves%20accurate%20geometric%20reconstruction%20and%20realistic%20rendering%20of%0Atransparent%20objects%20simultaneously%20within%20the%20efficient%203DGS%20framework.%0ASpecifically%2C%20TSGS%20significantly%20surpasses%20current%20leading%20methods%2C%20achieving%20a%0A37.3%25%20reduction%20in%20chamfer%20distance%20and%20an%208.0%25%20improvement%20in%20F1%20score%0Acompared%20to%20the%20top%20baseline.%20The%20code%20and%20dataset%20will%20be%20released%20at%0Ahttps%3A//longxiang-ai.github.io/TSGS/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12799v1&entry.124074799=Read"},
{"title": "AAA-Gaussians: Anti-Aliased and Artifact-Free 3D Gaussian Rendering", "author": "Michael Steiner and Thomas K\u00f6hler and Lukas Radl and Felix Windisch and Dieter Schmalstieg and Markus Steinberger", "abstract": "  Although 3D Gaussian Splatting (3DGS) has revolutionized 3D reconstruction,\nit still faces challenges such as aliasing, projection artifacts, and view\ninconsistencies, primarily due to the simplification of treating splats as 2D\nentities. We argue that incorporating full 3D evaluation of Gaussians\nthroughout the 3DGS pipeline can effectively address these issues while\npreserving rasterization efficiency. Specifically, we introduce an adaptive 3D\nsmoothing filter to mitigate aliasing and present a stable view-space bounding\nmethod that eliminates popping artifacts when Gaussians extend beyond the view\nfrustum. Furthermore, we promote tile-based culling to 3D with screen-space\nplanes, accelerating rendering and reducing sorting costs for hierarchical\nrasterization. Our method achieves state-of-the-art quality on in-distribution\nevaluation sets and significantly outperforms other approaches for\nout-of-distribution views. Our qualitative evaluations further demonstrate the\neffective removal of aliasing, distortions, and popping artifacts, ensuring\nreal-time, artifact-free rendering.\n", "link": "http://arxiv.org/abs/2504.12811v1", "date": "2025-04-17", "relevancy": 3.4388, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7195}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6857}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AAA-Gaussians%3A%20Anti-Aliased%20and%20Artifact-Free%203D%20Gaussian%20Rendering&body=Title%3A%20AAA-Gaussians%3A%20Anti-Aliased%20and%20Artifact-Free%203D%20Gaussian%20Rendering%0AAuthor%3A%20Michael%20Steiner%20and%20Thomas%20K%C3%B6hler%20and%20Lukas%20Radl%20and%20Felix%20Windisch%20and%20Dieter%20Schmalstieg%20and%20Markus%20Steinberger%0AAbstract%3A%20%20%20Although%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20revolutionized%203D%20reconstruction%2C%0Ait%20still%20faces%20challenges%20such%20as%20aliasing%2C%20projection%20artifacts%2C%20and%20view%0Ainconsistencies%2C%20primarily%20due%20to%20the%20simplification%20of%20treating%20splats%20as%202D%0Aentities.%20We%20argue%20that%20incorporating%20full%203D%20evaluation%20of%20Gaussians%0Athroughout%20the%203DGS%20pipeline%20can%20effectively%20address%20these%20issues%20while%0Apreserving%20rasterization%20efficiency.%20Specifically%2C%20we%20introduce%20an%20adaptive%203D%0Asmoothing%20filter%20to%20mitigate%20aliasing%20and%20present%20a%20stable%20view-space%20bounding%0Amethod%20that%20eliminates%20popping%20artifacts%20when%20Gaussians%20extend%20beyond%20the%20view%0Afrustum.%20Furthermore%2C%20we%20promote%20tile-based%20culling%20to%203D%20with%20screen-space%0Aplanes%2C%20accelerating%20rendering%20and%20reducing%20sorting%20costs%20for%20hierarchical%0Arasterization.%20Our%20method%20achieves%20state-of-the-art%20quality%20on%20in-distribution%0Aevaluation%20sets%20and%20significantly%20outperforms%20other%20approaches%20for%0Aout-of-distribution%20views.%20Our%20qualitative%20evaluations%20further%20demonstrate%20the%0Aeffective%20removal%20of%20aliasing%2C%20distortions%2C%20and%20popping%20artifacts%2C%20ensuring%0Areal-time%2C%20artifact-free%20rendering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12811v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAAA-Gaussians%253A%2520Anti-Aliased%2520and%2520Artifact-Free%25203D%2520Gaussian%2520Rendering%26entry.906535625%3DMichael%2520Steiner%2520and%2520Thomas%2520K%25C3%25B6hler%2520and%2520Lukas%2520Radl%2520and%2520Felix%2520Windisch%2520and%2520Dieter%2520Schmalstieg%2520and%2520Markus%2520Steinberger%26entry.1292438233%3D%2520%2520Although%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520revolutionized%25203D%2520reconstruction%252C%250Ait%2520still%2520faces%2520challenges%2520such%2520as%2520aliasing%252C%2520projection%2520artifacts%252C%2520and%2520view%250Ainconsistencies%252C%2520primarily%2520due%2520to%2520the%2520simplification%2520of%2520treating%2520splats%2520as%25202D%250Aentities.%2520We%2520argue%2520that%2520incorporating%2520full%25203D%2520evaluation%2520of%2520Gaussians%250Athroughout%2520the%25203DGS%2520pipeline%2520can%2520effectively%2520address%2520these%2520issues%2520while%250Apreserving%2520rasterization%2520efficiency.%2520Specifically%252C%2520we%2520introduce%2520an%2520adaptive%25203D%250Asmoothing%2520filter%2520to%2520mitigate%2520aliasing%2520and%2520present%2520a%2520stable%2520view-space%2520bounding%250Amethod%2520that%2520eliminates%2520popping%2520artifacts%2520when%2520Gaussians%2520extend%2520beyond%2520the%2520view%250Afrustum.%2520Furthermore%252C%2520we%2520promote%2520tile-based%2520culling%2520to%25203D%2520with%2520screen-space%250Aplanes%252C%2520accelerating%2520rendering%2520and%2520reducing%2520sorting%2520costs%2520for%2520hierarchical%250Arasterization.%2520Our%2520method%2520achieves%2520state-of-the-art%2520quality%2520on%2520in-distribution%250Aevaluation%2520sets%2520and%2520significantly%2520outperforms%2520other%2520approaches%2520for%250Aout-of-distribution%2520views.%2520Our%2520qualitative%2520evaluations%2520further%2520demonstrate%2520the%250Aeffective%2520removal%2520of%2520aliasing%252C%2520distortions%252C%2520and%2520popping%2520artifacts%252C%2520ensuring%250Areal-time%252C%2520artifact-free%2520rendering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12811v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AAA-Gaussians%3A%20Anti-Aliased%20and%20Artifact-Free%203D%20Gaussian%20Rendering&entry.906535625=Michael%20Steiner%20and%20Thomas%20K%C3%B6hler%20and%20Lukas%20Radl%20and%20Felix%20Windisch%20and%20Dieter%20Schmalstieg%20and%20Markus%20Steinberger&entry.1292438233=%20%20Although%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20revolutionized%203D%20reconstruction%2C%0Ait%20still%20faces%20challenges%20such%20as%20aliasing%2C%20projection%20artifacts%2C%20and%20view%0Ainconsistencies%2C%20primarily%20due%20to%20the%20simplification%20of%20treating%20splats%20as%202D%0Aentities.%20We%20argue%20that%20incorporating%20full%203D%20evaluation%20of%20Gaussians%0Athroughout%20the%203DGS%20pipeline%20can%20effectively%20address%20these%20issues%20while%0Apreserving%20rasterization%20efficiency.%20Specifically%2C%20we%20introduce%20an%20adaptive%203D%0Asmoothing%20filter%20to%20mitigate%20aliasing%20and%20present%20a%20stable%20view-space%20bounding%0Amethod%20that%20eliminates%20popping%20artifacts%20when%20Gaussians%20extend%20beyond%20the%20view%0Afrustum.%20Furthermore%2C%20we%20promote%20tile-based%20culling%20to%203D%20with%20screen-space%0Aplanes%2C%20accelerating%20rendering%20and%20reducing%20sorting%20costs%20for%20hierarchical%0Arasterization.%20Our%20method%20achieves%20state-of-the-art%20quality%20on%20in-distribution%0Aevaluation%20sets%20and%20significantly%20outperforms%20other%20approaches%20for%0Aout-of-distribution%20views.%20Our%20qualitative%20evaluations%20further%20demonstrate%20the%0Aeffective%20removal%20of%20aliasing%2C%20distortions%2C%20and%20popping%20artifacts%2C%20ensuring%0Areal-time%2C%20artifact-free%20rendering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12811v1&entry.124074799=Read"},
{"title": "ODHSR: Online Dense 3D Reconstruction of Humans and Scenes from\n  Monocular Videos", "author": "Zetong Zhang and Manuel kaufmann and Lixin Xue and Jie Song and Martin R. Oswald", "abstract": "  Creating a photorealistic scene and human reconstruction from a single\nmonocular in-the-wild video figures prominently in the perception of a\nhuman-centric 3D world. Recent neural rendering advances have enabled holistic\nhuman-scene reconstruction but require pre-calibrated camera and human poses,\nand days of training time. In this work, we introduce a novel unified framework\nthat simultaneously performs camera tracking, human pose estimation and\nhuman-scene reconstruction in an online fashion. 3D Gaussian Splatting is\nutilized to learn Gaussian primitives for humans and scenes efficiently, and\nreconstruction-based camera tracking and human pose estimation modules are\ndesigned to enable holistic understanding and effective disentanglement of pose\nand appearance. Specifically, we design a human deformation module to\nreconstruct the details and enhance generalizability to out-of-distribution\nposes faithfully. Aiming to learn the spatial correlation between human and\nscene accurately, we introduce occlusion-aware human silhouette rendering and\nmonocular geometric priors, which further improve reconstruction quality.\nExperiments on the EMDB and NeuMan datasets demonstrate superior or on-par\nperformance with existing methods in camera tracking, human pose estimation,\nnovel view synthesis and runtime. Our project page is at\nhttps://eth-ait.github.io/ODHSR.\n", "link": "http://arxiv.org/abs/2504.13167v1", "date": "2025-04-17", "relevancy": 3.3802, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6832}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6752}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ODHSR%3A%20Online%20Dense%203D%20Reconstruction%20of%20Humans%20and%20Scenes%20from%0A%20%20Monocular%20Videos&body=Title%3A%20ODHSR%3A%20Online%20Dense%203D%20Reconstruction%20of%20Humans%20and%20Scenes%20from%0A%20%20Monocular%20Videos%0AAuthor%3A%20Zetong%20Zhang%20and%20Manuel%20kaufmann%20and%20Lixin%20Xue%20and%20Jie%20Song%20and%20Martin%20R.%20Oswald%0AAbstract%3A%20%20%20Creating%20a%20photorealistic%20scene%20and%20human%20reconstruction%20from%20a%20single%0Amonocular%20in-the-wild%20video%20figures%20prominently%20in%20the%20perception%20of%20a%0Ahuman-centric%203D%20world.%20Recent%20neural%20rendering%20advances%20have%20enabled%20holistic%0Ahuman-scene%20reconstruction%20but%20require%20pre-calibrated%20camera%20and%20human%20poses%2C%0Aand%20days%20of%20training%20time.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20unified%20framework%0Athat%20simultaneously%20performs%20camera%20tracking%2C%20human%20pose%20estimation%20and%0Ahuman-scene%20reconstruction%20in%20an%20online%20fashion.%203D%20Gaussian%20Splatting%20is%0Autilized%20to%20learn%20Gaussian%20primitives%20for%20humans%20and%20scenes%20efficiently%2C%20and%0Areconstruction-based%20camera%20tracking%20and%20human%20pose%20estimation%20modules%20are%0Adesigned%20to%20enable%20holistic%20understanding%20and%20effective%20disentanglement%20of%20pose%0Aand%20appearance.%20Specifically%2C%20we%20design%20a%20human%20deformation%20module%20to%0Areconstruct%20the%20details%20and%20enhance%20generalizability%20to%20out-of-distribution%0Aposes%20faithfully.%20Aiming%20to%20learn%20the%20spatial%20correlation%20between%20human%20and%0Ascene%20accurately%2C%20we%20introduce%20occlusion-aware%20human%20silhouette%20rendering%20and%0Amonocular%20geometric%20priors%2C%20which%20further%20improve%20reconstruction%20quality.%0AExperiments%20on%20the%20EMDB%20and%20NeuMan%20datasets%20demonstrate%20superior%20or%20on-par%0Aperformance%20with%20existing%20methods%20in%20camera%20tracking%2C%20human%20pose%20estimation%2C%0Anovel%20view%20synthesis%20and%20runtime.%20Our%20project%20page%20is%20at%0Ahttps%3A//eth-ait.github.io/ODHSR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13167v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DODHSR%253A%2520Online%2520Dense%25203D%2520Reconstruction%2520of%2520Humans%2520and%2520Scenes%2520from%250A%2520%2520Monocular%2520Videos%26entry.906535625%3DZetong%2520Zhang%2520and%2520Manuel%2520kaufmann%2520and%2520Lixin%2520Xue%2520and%2520Jie%2520Song%2520and%2520Martin%2520R.%2520Oswald%26entry.1292438233%3D%2520%2520Creating%2520a%2520photorealistic%2520scene%2520and%2520human%2520reconstruction%2520from%2520a%2520single%250Amonocular%2520in-the-wild%2520video%2520figures%2520prominently%2520in%2520the%2520perception%2520of%2520a%250Ahuman-centric%25203D%2520world.%2520Recent%2520neural%2520rendering%2520advances%2520have%2520enabled%2520holistic%250Ahuman-scene%2520reconstruction%2520but%2520require%2520pre-calibrated%2520camera%2520and%2520human%2520poses%252C%250Aand%2520days%2520of%2520training%2520time.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520unified%2520framework%250Athat%2520simultaneously%2520performs%2520camera%2520tracking%252C%2520human%2520pose%2520estimation%2520and%250Ahuman-scene%2520reconstruction%2520in%2520an%2520online%2520fashion.%25203D%2520Gaussian%2520Splatting%2520is%250Autilized%2520to%2520learn%2520Gaussian%2520primitives%2520for%2520humans%2520and%2520scenes%2520efficiently%252C%2520and%250Areconstruction-based%2520camera%2520tracking%2520and%2520human%2520pose%2520estimation%2520modules%2520are%250Adesigned%2520to%2520enable%2520holistic%2520understanding%2520and%2520effective%2520disentanglement%2520of%2520pose%250Aand%2520appearance.%2520Specifically%252C%2520we%2520design%2520a%2520human%2520deformation%2520module%2520to%250Areconstruct%2520the%2520details%2520and%2520enhance%2520generalizability%2520to%2520out-of-distribution%250Aposes%2520faithfully.%2520Aiming%2520to%2520learn%2520the%2520spatial%2520correlation%2520between%2520human%2520and%250Ascene%2520accurately%252C%2520we%2520introduce%2520occlusion-aware%2520human%2520silhouette%2520rendering%2520and%250Amonocular%2520geometric%2520priors%252C%2520which%2520further%2520improve%2520reconstruction%2520quality.%250AExperiments%2520on%2520the%2520EMDB%2520and%2520NeuMan%2520datasets%2520demonstrate%2520superior%2520or%2520on-par%250Aperformance%2520with%2520existing%2520methods%2520in%2520camera%2520tracking%252C%2520human%2520pose%2520estimation%252C%250Anovel%2520view%2520synthesis%2520and%2520runtime.%2520Our%2520project%2520page%2520is%2520at%250Ahttps%253A//eth-ait.github.io/ODHSR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13167v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ODHSR%3A%20Online%20Dense%203D%20Reconstruction%20of%20Humans%20and%20Scenes%20from%0A%20%20Monocular%20Videos&entry.906535625=Zetong%20Zhang%20and%20Manuel%20kaufmann%20and%20Lixin%20Xue%20and%20Jie%20Song%20and%20Martin%20R.%20Oswald&entry.1292438233=%20%20Creating%20a%20photorealistic%20scene%20and%20human%20reconstruction%20from%20a%20single%0Amonocular%20in-the-wild%20video%20figures%20prominently%20in%20the%20perception%20of%20a%0Ahuman-centric%203D%20world.%20Recent%20neural%20rendering%20advances%20have%20enabled%20holistic%0Ahuman-scene%20reconstruction%20but%20require%20pre-calibrated%20camera%20and%20human%20poses%2C%0Aand%20days%20of%20training%20time.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20unified%20framework%0Athat%20simultaneously%20performs%20camera%20tracking%2C%20human%20pose%20estimation%20and%0Ahuman-scene%20reconstruction%20in%20an%20online%20fashion.%203D%20Gaussian%20Splatting%20is%0Autilized%20to%20learn%20Gaussian%20primitives%20for%20humans%20and%20scenes%20efficiently%2C%20and%0Areconstruction-based%20camera%20tracking%20and%20human%20pose%20estimation%20modules%20are%0Adesigned%20to%20enable%20holistic%20understanding%20and%20effective%20disentanglement%20of%20pose%0Aand%20appearance.%20Specifically%2C%20we%20design%20a%20human%20deformation%20module%20to%0Areconstruct%20the%20details%20and%20enhance%20generalizability%20to%20out-of-distribution%0Aposes%20faithfully.%20Aiming%20to%20learn%20the%20spatial%20correlation%20between%20human%20and%0Ascene%20accurately%2C%20we%20introduce%20occlusion-aware%20human%20silhouette%20rendering%20and%0Amonocular%20geometric%20priors%2C%20which%20further%20improve%20reconstruction%20quality.%0AExperiments%20on%20the%20EMDB%20and%20NeuMan%20datasets%20demonstrate%20superior%20or%20on-par%0Aperformance%20with%20existing%20methods%20in%20camera%20tracking%2C%20human%20pose%20estimation%2C%0Anovel%20view%20synthesis%20and%20runtime.%20Our%20project%20page%20is%20at%0Ahttps%3A//eth-ait.github.io/ODHSR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13167v1&entry.124074799=Read"},
{"title": "Training-Free Hierarchical Scene Understanding for Gaussian Splatting\n  with Superpoint Graphs", "author": "Shaohui Dai and Yansong Qu and Zheyan Li and Xinyang Li and Shengchuan Zhang and Liujuan Cao", "abstract": "  Bridging natural language and 3D geometry is a crucial step toward flexible,\nlanguage-driven scene understanding. While recent advances in 3D Gaussian\nSplatting (3DGS) have enabled fast and high-quality scene reconstruction,\nresearch has also explored incorporating open-vocabulary understanding into\n3DGS. However, most existing methods require iterative optimization over\nper-view 2D semantic feature maps, which not only results in inefficiencies but\nalso leads to inconsistent 3D semantics across views. To address these\nlimitations, we introduce a training-free framework that constructs a\nsuperpoint graph directly from Gaussian primitives. The superpoint graph\npartitions the scene into spatially compact and semantically coherent regions,\nforming view-consistent 3D entities and providing a structured foundation for\nopen-vocabulary understanding. Based on the graph structure, we design an\nefficient reprojection strategy that lifts 2D semantic features onto the\nsuperpoints, avoiding costly multi-view iterative training. The resulting\nrepresentation ensures strong 3D semantic coherence and naturally supports\nhierarchical understanding, enabling both coarse- and fine-grained\nopen-vocabulary perception within a unified semantic field. Extensive\nexperiments demonstrate that our method achieves state-of-the-art\nopen-vocabulary segmentation performance, with semantic field reconstruction\ncompleted over $30\\times$ faster. Our code will be available at\nhttps://github.com/Atrovast/THGS.\n", "link": "http://arxiv.org/abs/2504.13153v1", "date": "2025-04-17", "relevancy": 3.3722, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6983}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6865}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-Free%20Hierarchical%20Scene%20Understanding%20for%20Gaussian%20Splatting%0A%20%20with%20Superpoint%20Graphs&body=Title%3A%20Training-Free%20Hierarchical%20Scene%20Understanding%20for%20Gaussian%20Splatting%0A%20%20with%20Superpoint%20Graphs%0AAuthor%3A%20Shaohui%20Dai%20and%20Yansong%20Qu%20and%20Zheyan%20Li%20and%20Xinyang%20Li%20and%20Shengchuan%20Zhang%20and%20Liujuan%20Cao%0AAbstract%3A%20%20%20Bridging%20natural%20language%20and%203D%20geometry%20is%20a%20crucial%20step%20toward%20flexible%2C%0Alanguage-driven%20scene%20understanding.%20While%20recent%20advances%20in%203D%20Gaussian%0ASplatting%20%283DGS%29%20have%20enabled%20fast%20and%20high-quality%20scene%20reconstruction%2C%0Aresearch%20has%20also%20explored%20incorporating%20open-vocabulary%20understanding%20into%0A3DGS.%20However%2C%20most%20existing%20methods%20require%20iterative%20optimization%20over%0Aper-view%202D%20semantic%20feature%20maps%2C%20which%20not%20only%20results%20in%20inefficiencies%20but%0Aalso%20leads%20to%20inconsistent%203D%20semantics%20across%20views.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20a%20training-free%20framework%20that%20constructs%20a%0Asuperpoint%20graph%20directly%20from%20Gaussian%20primitives.%20The%20superpoint%20graph%0Apartitions%20the%20scene%20into%20spatially%20compact%20and%20semantically%20coherent%20regions%2C%0Aforming%20view-consistent%203D%20entities%20and%20providing%20a%20structured%20foundation%20for%0Aopen-vocabulary%20understanding.%20Based%20on%20the%20graph%20structure%2C%20we%20design%20an%0Aefficient%20reprojection%20strategy%20that%20lifts%202D%20semantic%20features%20onto%20the%0Asuperpoints%2C%20avoiding%20costly%20multi-view%20iterative%20training.%20The%20resulting%0Arepresentation%20ensures%20strong%203D%20semantic%20coherence%20and%20naturally%20supports%0Ahierarchical%20understanding%2C%20enabling%20both%20coarse-%20and%20fine-grained%0Aopen-vocabulary%20perception%20within%20a%20unified%20semantic%20field.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%0Aopen-vocabulary%20segmentation%20performance%2C%20with%20semantic%20field%20reconstruction%0Acompleted%20over%20%2430%5Ctimes%24%20faster.%20Our%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/Atrovast/THGS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13153v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-Free%2520Hierarchical%2520Scene%2520Understanding%2520for%2520Gaussian%2520Splatting%250A%2520%2520with%2520Superpoint%2520Graphs%26entry.906535625%3DShaohui%2520Dai%2520and%2520Yansong%2520Qu%2520and%2520Zheyan%2520Li%2520and%2520Xinyang%2520Li%2520and%2520Shengchuan%2520Zhang%2520and%2520Liujuan%2520Cao%26entry.1292438233%3D%2520%2520Bridging%2520natural%2520language%2520and%25203D%2520geometry%2520is%2520a%2520crucial%2520step%2520toward%2520flexible%252C%250Alanguage-driven%2520scene%2520understanding.%2520While%2520recent%2520advances%2520in%25203D%2520Gaussian%250ASplatting%2520%25283DGS%2529%2520have%2520enabled%2520fast%2520and%2520high-quality%2520scene%2520reconstruction%252C%250Aresearch%2520has%2520also%2520explored%2520incorporating%2520open-vocabulary%2520understanding%2520into%250A3DGS.%2520However%252C%2520most%2520existing%2520methods%2520require%2520iterative%2520optimization%2520over%250Aper-view%25202D%2520semantic%2520feature%2520maps%252C%2520which%2520not%2520only%2520results%2520in%2520inefficiencies%2520but%250Aalso%2520leads%2520to%2520inconsistent%25203D%2520semantics%2520across%2520views.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520introduce%2520a%2520training-free%2520framework%2520that%2520constructs%2520a%250Asuperpoint%2520graph%2520directly%2520from%2520Gaussian%2520primitives.%2520The%2520superpoint%2520graph%250Apartitions%2520the%2520scene%2520into%2520spatially%2520compact%2520and%2520semantically%2520coherent%2520regions%252C%250Aforming%2520view-consistent%25203D%2520entities%2520and%2520providing%2520a%2520structured%2520foundation%2520for%250Aopen-vocabulary%2520understanding.%2520Based%2520on%2520the%2520graph%2520structure%252C%2520we%2520design%2520an%250Aefficient%2520reprojection%2520strategy%2520that%2520lifts%25202D%2520semantic%2520features%2520onto%2520the%250Asuperpoints%252C%2520avoiding%2520costly%2520multi-view%2520iterative%2520training.%2520The%2520resulting%250Arepresentation%2520ensures%2520strong%25203D%2520semantic%2520coherence%2520and%2520naturally%2520supports%250Ahierarchical%2520understanding%252C%2520enabling%2520both%2520coarse-%2520and%2520fine-grained%250Aopen-vocabulary%2520perception%2520within%2520a%2520unified%2520semantic%2520field.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%250Aopen-vocabulary%2520segmentation%2520performance%252C%2520with%2520semantic%2520field%2520reconstruction%250Acompleted%2520over%2520%252430%255Ctimes%2524%2520faster.%2520Our%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/Atrovast/THGS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13153v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-Free%20Hierarchical%20Scene%20Understanding%20for%20Gaussian%20Splatting%0A%20%20with%20Superpoint%20Graphs&entry.906535625=Shaohui%20Dai%20and%20Yansong%20Qu%20and%20Zheyan%20Li%20and%20Xinyang%20Li%20and%20Shengchuan%20Zhang%20and%20Liujuan%20Cao&entry.1292438233=%20%20Bridging%20natural%20language%20and%203D%20geometry%20is%20a%20crucial%20step%20toward%20flexible%2C%0Alanguage-driven%20scene%20understanding.%20While%20recent%20advances%20in%203D%20Gaussian%0ASplatting%20%283DGS%29%20have%20enabled%20fast%20and%20high-quality%20scene%20reconstruction%2C%0Aresearch%20has%20also%20explored%20incorporating%20open-vocabulary%20understanding%20into%0A3DGS.%20However%2C%20most%20existing%20methods%20require%20iterative%20optimization%20over%0Aper-view%202D%20semantic%20feature%20maps%2C%20which%20not%20only%20results%20in%20inefficiencies%20but%0Aalso%20leads%20to%20inconsistent%203D%20semantics%20across%20views.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20a%20training-free%20framework%20that%20constructs%20a%0Asuperpoint%20graph%20directly%20from%20Gaussian%20primitives.%20The%20superpoint%20graph%0Apartitions%20the%20scene%20into%20spatially%20compact%20and%20semantically%20coherent%20regions%2C%0Aforming%20view-consistent%203D%20entities%20and%20providing%20a%20structured%20foundation%20for%0Aopen-vocabulary%20understanding.%20Based%20on%20the%20graph%20structure%2C%20we%20design%20an%0Aefficient%20reprojection%20strategy%20that%20lifts%202D%20semantic%20features%20onto%20the%0Asuperpoints%2C%20avoiding%20costly%20multi-view%20iterative%20training.%20The%20resulting%0Arepresentation%20ensures%20strong%203D%20semantic%20coherence%20and%20naturally%20supports%0Ahierarchical%20understanding%2C%20enabling%20both%20coarse-%20and%20fine-grained%0Aopen-vocabulary%20perception%20within%20a%20unified%20semantic%20field.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%0Aopen-vocabulary%20segmentation%20performance%2C%20with%20semantic%20field%20reconstruction%0Acompleted%20over%20%2430%5Ctimes%24%20faster.%20Our%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/Atrovast/THGS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13153v1&entry.124074799=Read"},
{"title": "Second-order Optimization of Gaussian Splats with Importance Sampling", "author": "Hamza Pehlivan and Andrea Boscolo Camiletto and Lin Geng Foo and Marc Habermann and Christian Theobalt", "abstract": "  3D Gaussian Splatting (3DGS) is widely used for novel view synthesis due to\nits high rendering quality and fast inference time. However, 3DGS predominantly\nrelies on first-order optimizers such as Adam, which leads to long training\ntimes. To address this limitation, we propose a novel second-order optimization\nstrategy based on Levenberg-Marquardt (LM) and Conjugate Gradient (CG), which\nwe specifically tailor towards Gaussian Splatting. Our key insight is that the\nJacobian in 3DGS exhibits significant sparsity since each Gaussian affects only\na limited number of pixels. We exploit this sparsity by proposing a matrix-free\nand GPU-parallelized LM optimization. To further improve its efficiency, we\npropose sampling strategies for both the camera views and loss function and,\nconsequently, the normal equation, significantly reducing the computational\ncomplexity. In addition, we increase the convergence rate of the second-order\napproximation by introducing an effective heuristic to determine the learning\nrate that avoids the expensive computation cost of line search methods. As a\nresult, our method achieves a $3\\times$ speedup over standard LM and\noutperforms Adam by $~6\\times$ when the Gaussian count is low while remaining\ncompetitive for moderate counts. Project Page:\nhttps://vcai.mpi-inf.mpg.de/projects/LM-IS\n", "link": "http://arxiv.org/abs/2504.12905v1", "date": "2025-04-17", "relevancy": 3.3131, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6899}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6574}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Second-order%20Optimization%20of%20Gaussian%20Splats%20with%20Importance%20Sampling&body=Title%3A%20Second-order%20Optimization%20of%20Gaussian%20Splats%20with%20Importance%20Sampling%0AAuthor%3A%20Hamza%20Pehlivan%20and%20Andrea%20Boscolo%20Camiletto%20and%20Lin%20Geng%20Foo%20and%20Marc%20Habermann%20and%20Christian%20Theobalt%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20is%20widely%20used%20for%20novel%20view%20synthesis%20due%20to%0Aits%20high%20rendering%20quality%20and%20fast%20inference%20time.%20However%2C%203DGS%20predominantly%0Arelies%20on%20first-order%20optimizers%20such%20as%20Adam%2C%20which%20leads%20to%20long%20training%0Atimes.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20novel%20second-order%20optimization%0Astrategy%20based%20on%20Levenberg-Marquardt%20%28LM%29%20and%20Conjugate%20Gradient%20%28CG%29%2C%20which%0Awe%20specifically%20tailor%20towards%20Gaussian%20Splatting.%20Our%20key%20insight%20is%20that%20the%0AJacobian%20in%203DGS%20exhibits%20significant%20sparsity%20since%20each%20Gaussian%20affects%20only%0Aa%20limited%20number%20of%20pixels.%20We%20exploit%20this%20sparsity%20by%20proposing%20a%20matrix-free%0Aand%20GPU-parallelized%20LM%20optimization.%20To%20further%20improve%20its%20efficiency%2C%20we%0Apropose%20sampling%20strategies%20for%20both%20the%20camera%20views%20and%20loss%20function%20and%2C%0Aconsequently%2C%20the%20normal%20equation%2C%20significantly%20reducing%20the%20computational%0Acomplexity.%20In%20addition%2C%20we%20increase%20the%20convergence%20rate%20of%20the%20second-order%0Aapproximation%20by%20introducing%20an%20effective%20heuristic%20to%20determine%20the%20learning%0Arate%20that%20avoids%20the%20expensive%20computation%20cost%20of%20line%20search%20methods.%20As%20a%0Aresult%2C%20our%20method%20achieves%20a%20%243%5Ctimes%24%20speedup%20over%20standard%20LM%20and%0Aoutperforms%20Adam%20by%20%24~6%5Ctimes%24%20when%20the%20Gaussian%20count%20is%20low%20while%20remaining%0Acompetitive%20for%20moderate%20counts.%20Project%20Page%3A%0Ahttps%3A//vcai.mpi-inf.mpg.de/projects/LM-IS%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12905v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSecond-order%2520Optimization%2520of%2520Gaussian%2520Splats%2520with%2520Importance%2520Sampling%26entry.906535625%3DHamza%2520Pehlivan%2520and%2520Andrea%2520Boscolo%2520Camiletto%2520and%2520Lin%2520Geng%2520Foo%2520and%2520Marc%2520Habermann%2520and%2520Christian%2520Theobalt%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520is%2520widely%2520used%2520for%2520novel%2520view%2520synthesis%2520due%2520to%250Aits%2520high%2520rendering%2520quality%2520and%2520fast%2520inference%2520time.%2520However%252C%25203DGS%2520predominantly%250Arelies%2520on%2520first-order%2520optimizers%2520such%2520as%2520Adam%252C%2520which%2520leads%2520to%2520long%2520training%250Atimes.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%2520novel%2520second-order%2520optimization%250Astrategy%2520based%2520on%2520Levenberg-Marquardt%2520%2528LM%2529%2520and%2520Conjugate%2520Gradient%2520%2528CG%2529%252C%2520which%250Awe%2520specifically%2520tailor%2520towards%2520Gaussian%2520Splatting.%2520Our%2520key%2520insight%2520is%2520that%2520the%250AJacobian%2520in%25203DGS%2520exhibits%2520significant%2520sparsity%2520since%2520each%2520Gaussian%2520affects%2520only%250Aa%2520limited%2520number%2520of%2520pixels.%2520We%2520exploit%2520this%2520sparsity%2520by%2520proposing%2520a%2520matrix-free%250Aand%2520GPU-parallelized%2520LM%2520optimization.%2520To%2520further%2520improve%2520its%2520efficiency%252C%2520we%250Apropose%2520sampling%2520strategies%2520for%2520both%2520the%2520camera%2520views%2520and%2520loss%2520function%2520and%252C%250Aconsequently%252C%2520the%2520normal%2520equation%252C%2520significantly%2520reducing%2520the%2520computational%250Acomplexity.%2520In%2520addition%252C%2520we%2520increase%2520the%2520convergence%2520rate%2520of%2520the%2520second-order%250Aapproximation%2520by%2520introducing%2520an%2520effective%2520heuristic%2520to%2520determine%2520the%2520learning%250Arate%2520that%2520avoids%2520the%2520expensive%2520computation%2520cost%2520of%2520line%2520search%2520methods.%2520As%2520a%250Aresult%252C%2520our%2520method%2520achieves%2520a%2520%25243%255Ctimes%2524%2520speedup%2520over%2520standard%2520LM%2520and%250Aoutperforms%2520Adam%2520by%2520%2524~6%255Ctimes%2524%2520when%2520the%2520Gaussian%2520count%2520is%2520low%2520while%2520remaining%250Acompetitive%2520for%2520moderate%2520counts.%2520Project%2520Page%253A%250Ahttps%253A//vcai.mpi-inf.mpg.de/projects/LM-IS%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12905v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Second-order%20Optimization%20of%20Gaussian%20Splats%20with%20Importance%20Sampling&entry.906535625=Hamza%20Pehlivan%20and%20Andrea%20Boscolo%20Camiletto%20and%20Lin%20Geng%20Foo%20and%20Marc%20Habermann%20and%20Christian%20Theobalt&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20is%20widely%20used%20for%20novel%20view%20synthesis%20due%20to%0Aits%20high%20rendering%20quality%20and%20fast%20inference%20time.%20However%2C%203DGS%20predominantly%0Arelies%20on%20first-order%20optimizers%20such%20as%20Adam%2C%20which%20leads%20to%20long%20training%0Atimes.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20novel%20second-order%20optimization%0Astrategy%20based%20on%20Levenberg-Marquardt%20%28LM%29%20and%20Conjugate%20Gradient%20%28CG%29%2C%20which%0Awe%20specifically%20tailor%20towards%20Gaussian%20Splatting.%20Our%20key%20insight%20is%20that%20the%0AJacobian%20in%203DGS%20exhibits%20significant%20sparsity%20since%20each%20Gaussian%20affects%20only%0Aa%20limited%20number%20of%20pixels.%20We%20exploit%20this%20sparsity%20by%20proposing%20a%20matrix-free%0Aand%20GPU-parallelized%20LM%20optimization.%20To%20further%20improve%20its%20efficiency%2C%20we%0Apropose%20sampling%20strategies%20for%20both%20the%20camera%20views%20and%20loss%20function%20and%2C%0Aconsequently%2C%20the%20normal%20equation%2C%20significantly%20reducing%20the%20computational%0Acomplexity.%20In%20addition%2C%20we%20increase%20the%20convergence%20rate%20of%20the%20second-order%0Aapproximation%20by%20introducing%20an%20effective%20heuristic%20to%20determine%20the%20learning%0Arate%20that%20avoids%20the%20expensive%20computation%20cost%20of%20line%20search%20methods.%20As%20a%0Aresult%2C%20our%20method%20achieves%20a%20%243%5Ctimes%24%20speedup%20over%20standard%20LM%20and%0Aoutperforms%20Adam%20by%20%24~6%5Ctimes%24%20when%20the%20Gaussian%20count%20is%20low%20while%20remaining%0Acompetitive%20for%20moderate%20counts.%20Project%20Page%3A%0Ahttps%3A//vcai.mpi-inf.mpg.de/projects/LM-IS%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12905v1&entry.124074799=Read"},
{"title": "CompGS++: Compressed Gaussian Splatting for Static and Dynamic Scene\n  Representation", "author": "Xiangrui Liu and Xinju Wu and Shiqi Wang and Zhu Li and Sam Kwong", "abstract": "  Gaussian splatting demonstrates proficiency for 3D scene modeling but suffers\nfrom substantial data volume due to inherent primitive redundancy. To enable\nfuture photorealistic 3D immersive visual communication applications,\nsignificant compression is essential for transmission over the existing\nInternet infrastructure. Hence, we propose Compressed Gaussian Splatting\n(CompGS++), a novel framework that leverages compact Gaussian primitives to\nachieve accurate 3D modeling with substantial size reduction for both static\nand dynamic scenes. Our design is based on the principle of eliminating\nredundancy both between and within primitives. Specifically, we develop a\ncomprehensive prediction paradigm to address inter-primitive redundancy through\nspatial and temporal primitive prediction modules. The spatial primitive\nprediction module establishes predictive relationships for scene primitives and\nenables most primitives to be encoded as compact residuals, substantially\nreducing the spatial redundancy. We further devise a temporal primitive\nprediction module to handle dynamic scenes, which exploits primitive\ncorrelations across timestamps to effectively reduce temporal redundancy.\nMoreover, we devise a rate-constrained optimization module that jointly\nminimizes reconstruction error and rate consumption. This module effectively\neliminates parameter redundancy within primitives and enhances the overall\ncompactness of scene representations. Comprehensive evaluations across multiple\nbenchmark datasets demonstrate that CompGS++ significantly outperforms existing\nmethods, achieving superior compression performance while preserving accurate\nscene modeling. Our implementation will be made publicly available on GitHub to\nfacilitate further research.\n", "link": "http://arxiv.org/abs/2504.13022v1", "date": "2025-04-17", "relevancy": 3.3038, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6943}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6613}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CompGS%2B%2B%3A%20Compressed%20Gaussian%20Splatting%20for%20Static%20and%20Dynamic%20Scene%0A%20%20Representation&body=Title%3A%20CompGS%2B%2B%3A%20Compressed%20Gaussian%20Splatting%20for%20Static%20and%20Dynamic%20Scene%0A%20%20Representation%0AAuthor%3A%20Xiangrui%20Liu%20and%20Xinju%20Wu%20and%20Shiqi%20Wang%20and%20Zhu%20Li%20and%20Sam%20Kwong%0AAbstract%3A%20%20%20Gaussian%20splatting%20demonstrates%20proficiency%20for%203D%20scene%20modeling%20but%20suffers%0Afrom%20substantial%20data%20volume%20due%20to%20inherent%20primitive%20redundancy.%20To%20enable%0Afuture%20photorealistic%203D%20immersive%20visual%20communication%20applications%2C%0Asignificant%20compression%20is%20essential%20for%20transmission%20over%20the%20existing%0AInternet%20infrastructure.%20Hence%2C%20we%20propose%20Compressed%20Gaussian%20Splatting%0A%28CompGS%2B%2B%29%2C%20a%20novel%20framework%20that%20leverages%20compact%20Gaussian%20primitives%20to%0Aachieve%20accurate%203D%20modeling%20with%20substantial%20size%20reduction%20for%20both%20static%0Aand%20dynamic%20scenes.%20Our%20design%20is%20based%20on%20the%20principle%20of%20eliminating%0Aredundancy%20both%20between%20and%20within%20primitives.%20Specifically%2C%20we%20develop%20a%0Acomprehensive%20prediction%20paradigm%20to%20address%20inter-primitive%20redundancy%20through%0Aspatial%20and%20temporal%20primitive%20prediction%20modules.%20The%20spatial%20primitive%0Aprediction%20module%20establishes%20predictive%20relationships%20for%20scene%20primitives%20and%0Aenables%20most%20primitives%20to%20be%20encoded%20as%20compact%20residuals%2C%20substantially%0Areducing%20the%20spatial%20redundancy.%20We%20further%20devise%20a%20temporal%20primitive%0Aprediction%20module%20to%20handle%20dynamic%20scenes%2C%20which%20exploits%20primitive%0Acorrelations%20across%20timestamps%20to%20effectively%20reduce%20temporal%20redundancy.%0AMoreover%2C%20we%20devise%20a%20rate-constrained%20optimization%20module%20that%20jointly%0Aminimizes%20reconstruction%20error%20and%20rate%20consumption.%20This%20module%20effectively%0Aeliminates%20parameter%20redundancy%20within%20primitives%20and%20enhances%20the%20overall%0Acompactness%20of%20scene%20representations.%20Comprehensive%20evaluations%20across%20multiple%0Abenchmark%20datasets%20demonstrate%20that%20CompGS%2B%2B%20significantly%20outperforms%20existing%0Amethods%2C%20achieving%20superior%20compression%20performance%20while%20preserving%20accurate%0Ascene%20modeling.%20Our%20implementation%20will%20be%20made%20publicly%20available%20on%20GitHub%20to%0Afacilitate%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13022v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompGS%252B%252B%253A%2520Compressed%2520Gaussian%2520Splatting%2520for%2520Static%2520and%2520Dynamic%2520Scene%250A%2520%2520Representation%26entry.906535625%3DXiangrui%2520Liu%2520and%2520Xinju%2520Wu%2520and%2520Shiqi%2520Wang%2520and%2520Zhu%2520Li%2520and%2520Sam%2520Kwong%26entry.1292438233%3D%2520%2520Gaussian%2520splatting%2520demonstrates%2520proficiency%2520for%25203D%2520scene%2520modeling%2520but%2520suffers%250Afrom%2520substantial%2520data%2520volume%2520due%2520to%2520inherent%2520primitive%2520redundancy.%2520To%2520enable%250Afuture%2520photorealistic%25203D%2520immersive%2520visual%2520communication%2520applications%252C%250Asignificant%2520compression%2520is%2520essential%2520for%2520transmission%2520over%2520the%2520existing%250AInternet%2520infrastructure.%2520Hence%252C%2520we%2520propose%2520Compressed%2520Gaussian%2520Splatting%250A%2528CompGS%252B%252B%2529%252C%2520a%2520novel%2520framework%2520that%2520leverages%2520compact%2520Gaussian%2520primitives%2520to%250Aachieve%2520accurate%25203D%2520modeling%2520with%2520substantial%2520size%2520reduction%2520for%2520both%2520static%250Aand%2520dynamic%2520scenes.%2520Our%2520design%2520is%2520based%2520on%2520the%2520principle%2520of%2520eliminating%250Aredundancy%2520both%2520between%2520and%2520within%2520primitives.%2520Specifically%252C%2520we%2520develop%2520a%250Acomprehensive%2520prediction%2520paradigm%2520to%2520address%2520inter-primitive%2520redundancy%2520through%250Aspatial%2520and%2520temporal%2520primitive%2520prediction%2520modules.%2520The%2520spatial%2520primitive%250Aprediction%2520module%2520establishes%2520predictive%2520relationships%2520for%2520scene%2520primitives%2520and%250Aenables%2520most%2520primitives%2520to%2520be%2520encoded%2520as%2520compact%2520residuals%252C%2520substantially%250Areducing%2520the%2520spatial%2520redundancy.%2520We%2520further%2520devise%2520a%2520temporal%2520primitive%250Aprediction%2520module%2520to%2520handle%2520dynamic%2520scenes%252C%2520which%2520exploits%2520primitive%250Acorrelations%2520across%2520timestamps%2520to%2520effectively%2520reduce%2520temporal%2520redundancy.%250AMoreover%252C%2520we%2520devise%2520a%2520rate-constrained%2520optimization%2520module%2520that%2520jointly%250Aminimizes%2520reconstruction%2520error%2520and%2520rate%2520consumption.%2520This%2520module%2520effectively%250Aeliminates%2520parameter%2520redundancy%2520within%2520primitives%2520and%2520enhances%2520the%2520overall%250Acompactness%2520of%2520scene%2520representations.%2520Comprehensive%2520evaluations%2520across%2520multiple%250Abenchmark%2520datasets%2520demonstrate%2520that%2520CompGS%252B%252B%2520significantly%2520outperforms%2520existing%250Amethods%252C%2520achieving%2520superior%2520compression%2520performance%2520while%2520preserving%2520accurate%250Ascene%2520modeling.%2520Our%2520implementation%2520will%2520be%2520made%2520publicly%2520available%2520on%2520GitHub%2520to%250Afacilitate%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13022v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CompGS%2B%2B%3A%20Compressed%20Gaussian%20Splatting%20for%20Static%20and%20Dynamic%20Scene%0A%20%20Representation&entry.906535625=Xiangrui%20Liu%20and%20Xinju%20Wu%20and%20Shiqi%20Wang%20and%20Zhu%20Li%20and%20Sam%20Kwong&entry.1292438233=%20%20Gaussian%20splatting%20demonstrates%20proficiency%20for%203D%20scene%20modeling%20but%20suffers%0Afrom%20substantial%20data%20volume%20due%20to%20inherent%20primitive%20redundancy.%20To%20enable%0Afuture%20photorealistic%203D%20immersive%20visual%20communication%20applications%2C%0Asignificant%20compression%20is%20essential%20for%20transmission%20over%20the%20existing%0AInternet%20infrastructure.%20Hence%2C%20we%20propose%20Compressed%20Gaussian%20Splatting%0A%28CompGS%2B%2B%29%2C%20a%20novel%20framework%20that%20leverages%20compact%20Gaussian%20primitives%20to%0Aachieve%20accurate%203D%20modeling%20with%20substantial%20size%20reduction%20for%20both%20static%0Aand%20dynamic%20scenes.%20Our%20design%20is%20based%20on%20the%20principle%20of%20eliminating%0Aredundancy%20both%20between%20and%20within%20primitives.%20Specifically%2C%20we%20develop%20a%0Acomprehensive%20prediction%20paradigm%20to%20address%20inter-primitive%20redundancy%20through%0Aspatial%20and%20temporal%20primitive%20prediction%20modules.%20The%20spatial%20primitive%0Aprediction%20module%20establishes%20predictive%20relationships%20for%20scene%20primitives%20and%0Aenables%20most%20primitives%20to%20be%20encoded%20as%20compact%20residuals%2C%20substantially%0Areducing%20the%20spatial%20redundancy.%20We%20further%20devise%20a%20temporal%20primitive%0Aprediction%20module%20to%20handle%20dynamic%20scenes%2C%20which%20exploits%20primitive%0Acorrelations%20across%20timestamps%20to%20effectively%20reduce%20temporal%20redundancy.%0AMoreover%2C%20we%20devise%20a%20rate-constrained%20optimization%20module%20that%20jointly%0Aminimizes%20reconstruction%20error%20and%20rate%20consumption.%20This%20module%20effectively%0Aeliminates%20parameter%20redundancy%20within%20primitives%20and%20enhances%20the%20overall%0Acompactness%20of%20scene%20representations.%20Comprehensive%20evaluations%20across%20multiple%0Abenchmark%20datasets%20demonstrate%20that%20CompGS%2B%2B%20significantly%20outperforms%20existing%0Amethods%2C%20achieving%20superior%20compression%20performance%20while%20preserving%20accurate%0Ascene%20modeling.%20Our%20implementation%20will%20be%20made%20publicly%20available%20on%20GitHub%20to%0Afacilitate%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13022v1&entry.124074799=Read"},
{"title": "CAGE-GS: High-fidelity Cage Based 3D Gaussian Splatting Deformation", "author": "Yifei Tong and Runze Tian and Xiao Han and Dingyao Liu and Fenggen Yu and Yan Zhang", "abstract": "  As 3D Gaussian Splatting (3DGS) gains popularity as a 3D representation of\nreal scenes, enabling user-friendly deformation to create novel scenes while\npreserving fine details from the original 3DGS has attracted significant\nresearch attention. We introduce CAGE-GS, a cage-based 3DGS deformation method\nthat seamlessly aligns a source 3DGS scene with a user-defined target shape.\nOur approach learns a deformation cage from the target, which guides the\ngeometric transformation of the source scene. While the cages effectively\ncontrol structural alignment, preserving the textural appearance of 3DGS\nremains challenging due to the complexity of covariance parameters. To address\nthis, we employ a Jacobian matrix-based strategy to update the covariance\nparameters of each Gaussian, ensuring texture fidelity post-deformation. Our\nmethod is highly flexible, accommodating various target shape representations,\nincluding texts, images, point clouds, meshes and 3DGS models. Extensive\nexperiments and ablation studies on both public datasets and newly proposed\nscenes demonstrate that our method significantly outperforms existing\ntechniques in both efficiency and deformation quality.\n", "link": "http://arxiv.org/abs/2504.12800v1", "date": "2025-04-17", "relevancy": 3.2798, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6809}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6497}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAGE-GS%3A%20High-fidelity%20Cage%20Based%203D%20Gaussian%20Splatting%20Deformation&body=Title%3A%20CAGE-GS%3A%20High-fidelity%20Cage%20Based%203D%20Gaussian%20Splatting%20Deformation%0AAuthor%3A%20Yifei%20Tong%20and%20Runze%20Tian%20and%20Xiao%20Han%20and%20Dingyao%20Liu%20and%20Fenggen%20Yu%20and%20Yan%20Zhang%0AAbstract%3A%20%20%20As%203D%20Gaussian%20Splatting%20%283DGS%29%20gains%20popularity%20as%20a%203D%20representation%20of%0Areal%20scenes%2C%20enabling%20user-friendly%20deformation%20to%20create%20novel%20scenes%20while%0Apreserving%20fine%20details%20from%20the%20original%203DGS%20has%20attracted%20significant%0Aresearch%20attention.%20We%20introduce%20CAGE-GS%2C%20a%20cage-based%203DGS%20deformation%20method%0Athat%20seamlessly%20aligns%20a%20source%203DGS%20scene%20with%20a%20user-defined%20target%20shape.%0AOur%20approach%20learns%20a%20deformation%20cage%20from%20the%20target%2C%20which%20guides%20the%0Ageometric%20transformation%20of%20the%20source%20scene.%20While%20the%20cages%20effectively%0Acontrol%20structural%20alignment%2C%20preserving%20the%20textural%20appearance%20of%203DGS%0Aremains%20challenging%20due%20to%20the%20complexity%20of%20covariance%20parameters.%20To%20address%0Athis%2C%20we%20employ%20a%20Jacobian%20matrix-based%20strategy%20to%20update%20the%20covariance%0Aparameters%20of%20each%20Gaussian%2C%20ensuring%20texture%20fidelity%20post-deformation.%20Our%0Amethod%20is%20highly%20flexible%2C%20accommodating%20various%20target%20shape%20representations%2C%0Aincluding%20texts%2C%20images%2C%20point%20clouds%2C%20meshes%20and%203DGS%20models.%20Extensive%0Aexperiments%20and%20ablation%20studies%20on%20both%20public%20datasets%20and%20newly%20proposed%0Ascenes%20demonstrate%20that%20our%20method%20significantly%20outperforms%20existing%0Atechniques%20in%20both%20efficiency%20and%20deformation%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12800v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAGE-GS%253A%2520High-fidelity%2520Cage%2520Based%25203D%2520Gaussian%2520Splatting%2520Deformation%26entry.906535625%3DYifei%2520Tong%2520and%2520Runze%2520Tian%2520and%2520Xiao%2520Han%2520and%2520Dingyao%2520Liu%2520and%2520Fenggen%2520Yu%2520and%2520Yan%2520Zhang%26entry.1292438233%3D%2520%2520As%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520gains%2520popularity%2520as%2520a%25203D%2520representation%2520of%250Areal%2520scenes%252C%2520enabling%2520user-friendly%2520deformation%2520to%2520create%2520novel%2520scenes%2520while%250Apreserving%2520fine%2520details%2520from%2520the%2520original%25203DGS%2520has%2520attracted%2520significant%250Aresearch%2520attention.%2520We%2520introduce%2520CAGE-GS%252C%2520a%2520cage-based%25203DGS%2520deformation%2520method%250Athat%2520seamlessly%2520aligns%2520a%2520source%25203DGS%2520scene%2520with%2520a%2520user-defined%2520target%2520shape.%250AOur%2520approach%2520learns%2520a%2520deformation%2520cage%2520from%2520the%2520target%252C%2520which%2520guides%2520the%250Ageometric%2520transformation%2520of%2520the%2520source%2520scene.%2520While%2520the%2520cages%2520effectively%250Acontrol%2520structural%2520alignment%252C%2520preserving%2520the%2520textural%2520appearance%2520of%25203DGS%250Aremains%2520challenging%2520due%2520to%2520the%2520complexity%2520of%2520covariance%2520parameters.%2520To%2520address%250Athis%252C%2520we%2520employ%2520a%2520Jacobian%2520matrix-based%2520strategy%2520to%2520update%2520the%2520covariance%250Aparameters%2520of%2520each%2520Gaussian%252C%2520ensuring%2520texture%2520fidelity%2520post-deformation.%2520Our%250Amethod%2520is%2520highly%2520flexible%252C%2520accommodating%2520various%2520target%2520shape%2520representations%252C%250Aincluding%2520texts%252C%2520images%252C%2520point%2520clouds%252C%2520meshes%2520and%25203DGS%2520models.%2520Extensive%250Aexperiments%2520and%2520ablation%2520studies%2520on%2520both%2520public%2520datasets%2520and%2520newly%2520proposed%250Ascenes%2520demonstrate%2520that%2520our%2520method%2520significantly%2520outperforms%2520existing%250Atechniques%2520in%2520both%2520efficiency%2520and%2520deformation%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12800v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAGE-GS%3A%20High-fidelity%20Cage%20Based%203D%20Gaussian%20Splatting%20Deformation&entry.906535625=Yifei%20Tong%20and%20Runze%20Tian%20and%20Xiao%20Han%20and%20Dingyao%20Liu%20and%20Fenggen%20Yu%20and%20Yan%20Zhang&entry.1292438233=%20%20As%203D%20Gaussian%20Splatting%20%283DGS%29%20gains%20popularity%20as%20a%203D%20representation%20of%0Areal%20scenes%2C%20enabling%20user-friendly%20deformation%20to%20create%20novel%20scenes%20while%0Apreserving%20fine%20details%20from%20the%20original%203DGS%20has%20attracted%20significant%0Aresearch%20attention.%20We%20introduce%20CAGE-GS%2C%20a%20cage-based%203DGS%20deformation%20method%0Athat%20seamlessly%20aligns%20a%20source%203DGS%20scene%20with%20a%20user-defined%20target%20shape.%0AOur%20approach%20learns%20a%20deformation%20cage%20from%20the%20target%2C%20which%20guides%20the%0Ageometric%20transformation%20of%20the%20source%20scene.%20While%20the%20cages%20effectively%0Acontrol%20structural%20alignment%2C%20preserving%20the%20textural%20appearance%20of%203DGS%0Aremains%20challenging%20due%20to%20the%20complexity%20of%20covariance%20parameters.%20To%20address%0Athis%2C%20we%20employ%20a%20Jacobian%20matrix-based%20strategy%20to%20update%20the%20covariance%0Aparameters%20of%20each%20Gaussian%2C%20ensuring%20texture%20fidelity%20post-deformation.%20Our%0Amethod%20is%20highly%20flexible%2C%20accommodating%20various%20target%20shape%20representations%2C%0Aincluding%20texts%2C%20images%2C%20point%20clouds%2C%20meshes%20and%203DGS%20models.%20Extensive%0Aexperiments%20and%20ablation%20studies%20on%20both%20public%20datasets%20and%20newly%20proposed%0Ascenes%20demonstrate%20that%20our%20method%20significantly%20outperforms%20existing%0Atechniques%20in%20both%20efficiency%20and%20deformation%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12800v1&entry.124074799=Read"},
{"title": "Novel Demonstration Generation with Gaussian Splatting Enables Robust\n  One-Shot Manipulation", "author": "Sizhe Yang and Wenye Yu and Jia Zeng and Jun Lv and Kerui Ren and Cewu Lu and Dahua Lin and Jiangmiao Pang", "abstract": "  Visuomotor policies learned from teleoperated demonstrations face challenges\nsuch as lengthy data collection, high costs, and limited data diversity.\nExisting approaches address these issues by augmenting image observations in\nRGB space or employing Real-to-Sim-to-Real pipelines based on physical\nsimulators. However, the former is constrained to 2D data augmentation, while\nthe latter suffers from imprecise physical simulation caused by inaccurate\ngeometric reconstruction. This paper introduces RoboSplat, a novel method that\ngenerates diverse, visually realistic demonstrations by directly manipulating\n3D Gaussians. Specifically, we reconstruct the scene through 3D Gaussian\nSplatting (3DGS), directly edit the reconstructed scene, and augment data\nacross six types of generalization with five techniques: 3D Gaussian\nreplacement for varying object types, scene appearance, and robot embodiments;\nequivariant transformations for different object poses; visual attribute\nediting for various lighting conditions; novel view synthesis for new camera\nperspectives; and 3D content generation for diverse object types. Comprehensive\nreal-world experiments demonstrate that RoboSplat significantly enhances the\ngeneralization of visuomotor policies under diverse disturbances. Notably,\nwhile policies trained on hundreds of real-world demonstrations with additional\n2D data augmentation achieve an average success rate of 57.2%, RoboSplat\nattains 87.8% in one-shot settings across six types of generalization in the\nreal world.\n", "link": "http://arxiv.org/abs/2504.13175v1", "date": "2025-04-17", "relevancy": 3.2468, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.673}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6625}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Novel%20Demonstration%20Generation%20with%20Gaussian%20Splatting%20Enables%20Robust%0A%20%20One-Shot%20Manipulation&body=Title%3A%20Novel%20Demonstration%20Generation%20with%20Gaussian%20Splatting%20Enables%20Robust%0A%20%20One-Shot%20Manipulation%0AAuthor%3A%20Sizhe%20Yang%20and%20Wenye%20Yu%20and%20Jia%20Zeng%20and%20Jun%20Lv%20and%20Kerui%20Ren%20and%20Cewu%20Lu%20and%20Dahua%20Lin%20and%20Jiangmiao%20Pang%0AAbstract%3A%20%20%20Visuomotor%20policies%20learned%20from%20teleoperated%20demonstrations%20face%20challenges%0Asuch%20as%20lengthy%20data%20collection%2C%20high%20costs%2C%20and%20limited%20data%20diversity.%0AExisting%20approaches%20address%20these%20issues%20by%20augmenting%20image%20observations%20in%0ARGB%20space%20or%20employing%20Real-to-Sim-to-Real%20pipelines%20based%20on%20physical%0Asimulators.%20However%2C%20the%20former%20is%20constrained%20to%202D%20data%20augmentation%2C%20while%0Athe%20latter%20suffers%20from%20imprecise%20physical%20simulation%20caused%20by%20inaccurate%0Ageometric%20reconstruction.%20This%20paper%20introduces%20RoboSplat%2C%20a%20novel%20method%20that%0Agenerates%20diverse%2C%20visually%20realistic%20demonstrations%20by%20directly%20manipulating%0A3D%20Gaussians.%20Specifically%2C%20we%20reconstruct%20the%20scene%20through%203D%20Gaussian%0ASplatting%20%283DGS%29%2C%20directly%20edit%20the%20reconstructed%20scene%2C%20and%20augment%20data%0Aacross%20six%20types%20of%20generalization%20with%20five%20techniques%3A%203D%20Gaussian%0Areplacement%20for%20varying%20object%20types%2C%20scene%20appearance%2C%20and%20robot%20embodiments%3B%0Aequivariant%20transformations%20for%20different%20object%20poses%3B%20visual%20attribute%0Aediting%20for%20various%20lighting%20conditions%3B%20novel%20view%20synthesis%20for%20new%20camera%0Aperspectives%3B%20and%203D%20content%20generation%20for%20diverse%20object%20types.%20Comprehensive%0Areal-world%20experiments%20demonstrate%20that%20RoboSplat%20significantly%20enhances%20the%0Ageneralization%20of%20visuomotor%20policies%20under%20diverse%20disturbances.%20Notably%2C%0Awhile%20policies%20trained%20on%20hundreds%20of%20real-world%20demonstrations%20with%20additional%0A2D%20data%20augmentation%20achieve%20an%20average%20success%20rate%20of%2057.2%25%2C%20RoboSplat%0Aattains%2087.8%25%20in%20one-shot%20settings%20across%20six%20types%20of%20generalization%20in%20the%0Areal%20world.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13175v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNovel%2520Demonstration%2520Generation%2520with%2520Gaussian%2520Splatting%2520Enables%2520Robust%250A%2520%2520One-Shot%2520Manipulation%26entry.906535625%3DSizhe%2520Yang%2520and%2520Wenye%2520Yu%2520and%2520Jia%2520Zeng%2520and%2520Jun%2520Lv%2520and%2520Kerui%2520Ren%2520and%2520Cewu%2520Lu%2520and%2520Dahua%2520Lin%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3D%2520%2520Visuomotor%2520policies%2520learned%2520from%2520teleoperated%2520demonstrations%2520face%2520challenges%250Asuch%2520as%2520lengthy%2520data%2520collection%252C%2520high%2520costs%252C%2520and%2520limited%2520data%2520diversity.%250AExisting%2520approaches%2520address%2520these%2520issues%2520by%2520augmenting%2520image%2520observations%2520in%250ARGB%2520space%2520or%2520employing%2520Real-to-Sim-to-Real%2520pipelines%2520based%2520on%2520physical%250Asimulators.%2520However%252C%2520the%2520former%2520is%2520constrained%2520to%25202D%2520data%2520augmentation%252C%2520while%250Athe%2520latter%2520suffers%2520from%2520imprecise%2520physical%2520simulation%2520caused%2520by%2520inaccurate%250Ageometric%2520reconstruction.%2520This%2520paper%2520introduces%2520RoboSplat%252C%2520a%2520novel%2520method%2520that%250Agenerates%2520diverse%252C%2520visually%2520realistic%2520demonstrations%2520by%2520directly%2520manipulating%250A3D%2520Gaussians.%2520Specifically%252C%2520we%2520reconstruct%2520the%2520scene%2520through%25203D%2520Gaussian%250ASplatting%2520%25283DGS%2529%252C%2520directly%2520edit%2520the%2520reconstructed%2520scene%252C%2520and%2520augment%2520data%250Aacross%2520six%2520types%2520of%2520generalization%2520with%2520five%2520techniques%253A%25203D%2520Gaussian%250Areplacement%2520for%2520varying%2520object%2520types%252C%2520scene%2520appearance%252C%2520and%2520robot%2520embodiments%253B%250Aequivariant%2520transformations%2520for%2520different%2520object%2520poses%253B%2520visual%2520attribute%250Aediting%2520for%2520various%2520lighting%2520conditions%253B%2520novel%2520view%2520synthesis%2520for%2520new%2520camera%250Aperspectives%253B%2520and%25203D%2520content%2520generation%2520for%2520diverse%2520object%2520types.%2520Comprehensive%250Areal-world%2520experiments%2520demonstrate%2520that%2520RoboSplat%2520significantly%2520enhances%2520the%250Ageneralization%2520of%2520visuomotor%2520policies%2520under%2520diverse%2520disturbances.%2520Notably%252C%250Awhile%2520policies%2520trained%2520on%2520hundreds%2520of%2520real-world%2520demonstrations%2520with%2520additional%250A2D%2520data%2520augmentation%2520achieve%2520an%2520average%2520success%2520rate%2520of%252057.2%2525%252C%2520RoboSplat%250Aattains%252087.8%2525%2520in%2520one-shot%2520settings%2520across%2520six%2520types%2520of%2520generalization%2520in%2520the%250Areal%2520world.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13175v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Novel%20Demonstration%20Generation%20with%20Gaussian%20Splatting%20Enables%20Robust%0A%20%20One-Shot%20Manipulation&entry.906535625=Sizhe%20Yang%20and%20Wenye%20Yu%20and%20Jia%20Zeng%20and%20Jun%20Lv%20and%20Kerui%20Ren%20and%20Cewu%20Lu%20and%20Dahua%20Lin%20and%20Jiangmiao%20Pang&entry.1292438233=%20%20Visuomotor%20policies%20learned%20from%20teleoperated%20demonstrations%20face%20challenges%0Asuch%20as%20lengthy%20data%20collection%2C%20high%20costs%2C%20and%20limited%20data%20diversity.%0AExisting%20approaches%20address%20these%20issues%20by%20augmenting%20image%20observations%20in%0ARGB%20space%20or%20employing%20Real-to-Sim-to-Real%20pipelines%20based%20on%20physical%0Asimulators.%20However%2C%20the%20former%20is%20constrained%20to%202D%20data%20augmentation%2C%20while%0Athe%20latter%20suffers%20from%20imprecise%20physical%20simulation%20caused%20by%20inaccurate%0Ageometric%20reconstruction.%20This%20paper%20introduces%20RoboSplat%2C%20a%20novel%20method%20that%0Agenerates%20diverse%2C%20visually%20realistic%20demonstrations%20by%20directly%20manipulating%0A3D%20Gaussians.%20Specifically%2C%20we%20reconstruct%20the%20scene%20through%203D%20Gaussian%0ASplatting%20%283DGS%29%2C%20directly%20edit%20the%20reconstructed%20scene%2C%20and%20augment%20data%0Aacross%20six%20types%20of%20generalization%20with%20five%20techniques%3A%203D%20Gaussian%0Areplacement%20for%20varying%20object%20types%2C%20scene%20appearance%2C%20and%20robot%20embodiments%3B%0Aequivariant%20transformations%20for%20different%20object%20poses%3B%20visual%20attribute%0Aediting%20for%20various%20lighting%20conditions%3B%20novel%20view%20synthesis%20for%20new%20camera%0Aperspectives%3B%20and%203D%20content%20generation%20for%20diverse%20object%20types.%20Comprehensive%0Areal-world%20experiments%20demonstrate%20that%20RoboSplat%20significantly%20enhances%20the%0Ageneralization%20of%20visuomotor%20policies%20under%20diverse%20disturbances.%20Notably%2C%0Awhile%20policies%20trained%20on%20hundreds%20of%20real-world%20demonstrations%20with%20additional%0A2D%20data%20augmentation%20achieve%20an%20average%20success%20rate%20of%2057.2%25%2C%20RoboSplat%0Aattains%2087.8%25%20in%20one-shot%20settings%20across%20six%20types%20of%20generalization%20in%20the%0Areal%20world.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13175v1&entry.124074799=Read"},
{"title": "PerceptionLM: Open-Access Data and Models for Detailed Visual\n  Understanding", "author": "Jang Hyun Cho and Andrea Madotto and Effrosyni Mavroudi and Triantafyllos Afouras and Tushar Nagarajan and Muhammad Maaz and Yale Song and Tengyu Ma and Shuming Hu and Suyog Jain and Miguel Martin and Huiyu Wang and Hanoona Rasheed and Peize Sun and Po-Yao Huang and Daniel Bolya and Nikhila Ravi and Shashank Jain and Tammy Stark and Shane Moon and Babak Damavandi and Vivian Lee and Andrew Westbury and Salman Khan and Philipp Kr\u00e4henb\u00fchl and Piotr Doll\u00e1r and Lorenzo Torresani and Kristen Grauman and Christoph Feichtenhofer", "abstract": "  Vision-language models are integral to computer vision research, yet many\nhigh-performing models remain closed-source, obscuring their data, design and\ntraining recipe. The research community has responded by using distillation\nfrom black-box models to label training data, achieving strong benchmark\nresults, at the cost of measurable scientific progress. However, without\nknowing the details of the teacher model and its data sources, scientific\nprogress remains difficult to measure. In this paper, we study building a\nPerception Language Model (PLM) in a fully open and reproducible framework for\ntransparent research in image and video understanding. We analyze standard\ntraining pipelines without distillation from proprietary models and explore\nlarge-scale synthetic data to identify critical data gaps, particularly in\ndetailed video understanding. To bridge these gaps, we release 2.8M\nhuman-labeled instances of fine-grained video question-answer pairs and\nspatio-temporally grounded video captions. Additionally, we introduce\nPLM-VideoBench, a suite for evaluating challenging video understanding tasks\nfocusing on the ability to reason about \"what\", \"where\", \"when\", and \"how\" of a\nvideo. We make our work fully reproducible by providing data, training recipes,\ncode & models.\n", "link": "http://arxiv.org/abs/2504.13180v1", "date": "2025-04-17", "relevancy": 3.113, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.639}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.639}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PerceptionLM%3A%20Open-Access%20Data%20and%20Models%20for%20Detailed%20Visual%0A%20%20Understanding&body=Title%3A%20PerceptionLM%3A%20Open-Access%20Data%20and%20Models%20for%20Detailed%20Visual%0A%20%20Understanding%0AAuthor%3A%20Jang%20Hyun%20Cho%20and%20Andrea%20Madotto%20and%20Effrosyni%20Mavroudi%20and%20Triantafyllos%20Afouras%20and%20Tushar%20Nagarajan%20and%20Muhammad%20Maaz%20and%20Yale%20Song%20and%20Tengyu%20Ma%20and%20Shuming%20Hu%20and%20Suyog%20Jain%20and%20Miguel%20Martin%20and%20Huiyu%20Wang%20and%20Hanoona%20Rasheed%20and%20Peize%20Sun%20and%20Po-Yao%20Huang%20and%20Daniel%20Bolya%20and%20Nikhila%20Ravi%20and%20Shashank%20Jain%20and%20Tammy%20Stark%20and%20Shane%20Moon%20and%20Babak%20Damavandi%20and%20Vivian%20Lee%20and%20Andrew%20Westbury%20and%20Salman%20Khan%20and%20Philipp%20Kr%C3%A4henb%C3%BChl%20and%20Piotr%20Doll%C3%A1r%20and%20Lorenzo%20Torresani%20and%20Kristen%20Grauman%20and%20Christoph%20Feichtenhofer%0AAbstract%3A%20%20%20Vision-language%20models%20are%20integral%20to%20computer%20vision%20research%2C%20yet%20many%0Ahigh-performing%20models%20remain%20closed-source%2C%20obscuring%20their%20data%2C%20design%20and%0Atraining%20recipe.%20The%20research%20community%20has%20responded%20by%20using%20distillation%0Afrom%20black-box%20models%20to%20label%20training%20data%2C%20achieving%20strong%20benchmark%0Aresults%2C%20at%20the%20cost%20of%20measurable%20scientific%20progress.%20However%2C%20without%0Aknowing%20the%20details%20of%20the%20teacher%20model%20and%20its%20data%20sources%2C%20scientific%0Aprogress%20remains%20difficult%20to%20measure.%20In%20this%20paper%2C%20we%20study%20building%20a%0APerception%20Language%20Model%20%28PLM%29%20in%20a%20fully%20open%20and%20reproducible%20framework%20for%0Atransparent%20research%20in%20image%20and%20video%20understanding.%20We%20analyze%20standard%0Atraining%20pipelines%20without%20distillation%20from%20proprietary%20models%20and%20explore%0Alarge-scale%20synthetic%20data%20to%20identify%20critical%20data%20gaps%2C%20particularly%20in%0Adetailed%20video%20understanding.%20To%20bridge%20these%20gaps%2C%20we%20release%202.8M%0Ahuman-labeled%20instances%20of%20fine-grained%20video%20question-answer%20pairs%20and%0Aspatio-temporally%20grounded%20video%20captions.%20Additionally%2C%20we%20introduce%0APLM-VideoBench%2C%20a%20suite%20for%20evaluating%20challenging%20video%20understanding%20tasks%0Afocusing%20on%20the%20ability%20to%20reason%20about%20%22what%22%2C%20%22where%22%2C%20%22when%22%2C%20and%20%22how%22%20of%20a%0Avideo.%20We%20make%20our%20work%20fully%20reproducible%20by%20providing%20data%2C%20training%20recipes%2C%0Acode%20%26%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13180v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerceptionLM%253A%2520Open-Access%2520Data%2520and%2520Models%2520for%2520Detailed%2520Visual%250A%2520%2520Understanding%26entry.906535625%3DJang%2520Hyun%2520Cho%2520and%2520Andrea%2520Madotto%2520and%2520Effrosyni%2520Mavroudi%2520and%2520Triantafyllos%2520Afouras%2520and%2520Tushar%2520Nagarajan%2520and%2520Muhammad%2520Maaz%2520and%2520Yale%2520Song%2520and%2520Tengyu%2520Ma%2520and%2520Shuming%2520Hu%2520and%2520Suyog%2520Jain%2520and%2520Miguel%2520Martin%2520and%2520Huiyu%2520Wang%2520and%2520Hanoona%2520Rasheed%2520and%2520Peize%2520Sun%2520and%2520Po-Yao%2520Huang%2520and%2520Daniel%2520Bolya%2520and%2520Nikhila%2520Ravi%2520and%2520Shashank%2520Jain%2520and%2520Tammy%2520Stark%2520and%2520Shane%2520Moon%2520and%2520Babak%2520Damavandi%2520and%2520Vivian%2520Lee%2520and%2520Andrew%2520Westbury%2520and%2520Salman%2520Khan%2520and%2520Philipp%2520Kr%25C3%25A4henb%25C3%25BChl%2520and%2520Piotr%2520Doll%25C3%25A1r%2520and%2520Lorenzo%2520Torresani%2520and%2520Kristen%2520Grauman%2520and%2520Christoph%2520Feichtenhofer%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520are%2520integral%2520to%2520computer%2520vision%2520research%252C%2520yet%2520many%250Ahigh-performing%2520models%2520remain%2520closed-source%252C%2520obscuring%2520their%2520data%252C%2520design%2520and%250Atraining%2520recipe.%2520The%2520research%2520community%2520has%2520responded%2520by%2520using%2520distillation%250Afrom%2520black-box%2520models%2520to%2520label%2520training%2520data%252C%2520achieving%2520strong%2520benchmark%250Aresults%252C%2520at%2520the%2520cost%2520of%2520measurable%2520scientific%2520progress.%2520However%252C%2520without%250Aknowing%2520the%2520details%2520of%2520the%2520teacher%2520model%2520and%2520its%2520data%2520sources%252C%2520scientific%250Aprogress%2520remains%2520difficult%2520to%2520measure.%2520In%2520this%2520paper%252C%2520we%2520study%2520building%2520a%250APerception%2520Language%2520Model%2520%2528PLM%2529%2520in%2520a%2520fully%2520open%2520and%2520reproducible%2520framework%2520for%250Atransparent%2520research%2520in%2520image%2520and%2520video%2520understanding.%2520We%2520analyze%2520standard%250Atraining%2520pipelines%2520without%2520distillation%2520from%2520proprietary%2520models%2520and%2520explore%250Alarge-scale%2520synthetic%2520data%2520to%2520identify%2520critical%2520data%2520gaps%252C%2520particularly%2520in%250Adetailed%2520video%2520understanding.%2520To%2520bridge%2520these%2520gaps%252C%2520we%2520release%25202.8M%250Ahuman-labeled%2520instances%2520of%2520fine-grained%2520video%2520question-answer%2520pairs%2520and%250Aspatio-temporally%2520grounded%2520video%2520captions.%2520Additionally%252C%2520we%2520introduce%250APLM-VideoBench%252C%2520a%2520suite%2520for%2520evaluating%2520challenging%2520video%2520understanding%2520tasks%250Afocusing%2520on%2520the%2520ability%2520to%2520reason%2520about%2520%2522what%2522%252C%2520%2522where%2522%252C%2520%2522when%2522%252C%2520and%2520%2522how%2522%2520of%2520a%250Avideo.%2520We%2520make%2520our%2520work%2520fully%2520reproducible%2520by%2520providing%2520data%252C%2520training%2520recipes%252C%250Acode%2520%2526%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13180v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PerceptionLM%3A%20Open-Access%20Data%20and%20Models%20for%20Detailed%20Visual%0A%20%20Understanding&entry.906535625=Jang%20Hyun%20Cho%20and%20Andrea%20Madotto%20and%20Effrosyni%20Mavroudi%20and%20Triantafyllos%20Afouras%20and%20Tushar%20Nagarajan%20and%20Muhammad%20Maaz%20and%20Yale%20Song%20and%20Tengyu%20Ma%20and%20Shuming%20Hu%20and%20Suyog%20Jain%20and%20Miguel%20Martin%20and%20Huiyu%20Wang%20and%20Hanoona%20Rasheed%20and%20Peize%20Sun%20and%20Po-Yao%20Huang%20and%20Daniel%20Bolya%20and%20Nikhila%20Ravi%20and%20Shashank%20Jain%20and%20Tammy%20Stark%20and%20Shane%20Moon%20and%20Babak%20Damavandi%20and%20Vivian%20Lee%20and%20Andrew%20Westbury%20and%20Salman%20Khan%20and%20Philipp%20Kr%C3%A4henb%C3%BChl%20and%20Piotr%20Doll%C3%A1r%20and%20Lorenzo%20Torresani%20and%20Kristen%20Grauman%20and%20Christoph%20Feichtenhofer&entry.1292438233=%20%20Vision-language%20models%20are%20integral%20to%20computer%20vision%20research%2C%20yet%20many%0Ahigh-performing%20models%20remain%20closed-source%2C%20obscuring%20their%20data%2C%20design%20and%0Atraining%20recipe.%20The%20research%20community%20has%20responded%20by%20using%20distillation%0Afrom%20black-box%20models%20to%20label%20training%20data%2C%20achieving%20strong%20benchmark%0Aresults%2C%20at%20the%20cost%20of%20measurable%20scientific%20progress.%20However%2C%20without%0Aknowing%20the%20details%20of%20the%20teacher%20model%20and%20its%20data%20sources%2C%20scientific%0Aprogress%20remains%20difficult%20to%20measure.%20In%20this%20paper%2C%20we%20study%20building%20a%0APerception%20Language%20Model%20%28PLM%29%20in%20a%20fully%20open%20and%20reproducible%20framework%20for%0Atransparent%20research%20in%20image%20and%20video%20understanding.%20We%20analyze%20standard%0Atraining%20pipelines%20without%20distillation%20from%20proprietary%20models%20and%20explore%0Alarge-scale%20synthetic%20data%20to%20identify%20critical%20data%20gaps%2C%20particularly%20in%0Adetailed%20video%20understanding.%20To%20bridge%20these%20gaps%2C%20we%20release%202.8M%0Ahuman-labeled%20instances%20of%20fine-grained%20video%20question-answer%20pairs%20and%0Aspatio-temporally%20grounded%20video%20captions.%20Additionally%2C%20we%20introduce%0APLM-VideoBench%2C%20a%20suite%20for%20evaluating%20challenging%20video%20understanding%20tasks%0Afocusing%20on%20the%20ability%20to%20reason%20about%20%22what%22%2C%20%22where%22%2C%20%22when%22%2C%20and%20%22how%22%20of%20a%0Avideo.%20We%20make%20our%20work%20fully%20reproducible%20by%20providing%20data%2C%20training%20recipes%2C%0Acode%20%26%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13180v1&entry.124074799=Read"},
{"title": "HiScene: Creating Hierarchical 3D Scenes with Isometric View Generation", "author": "Wenqi Dong and Bangbang Yang and Zesong Yang and Yuan Li and Tao Hu and Hujun Bao and Yuewen Ma and Zhaopeng Cui", "abstract": "  Scene-level 3D generation represents a critical frontier in multimedia and\ncomputer graphics, yet existing approaches either suffer from limited object\ncategories or lack editing flexibility for interactive applications. In this\npaper, we present HiScene, a novel hierarchical framework that bridges the gap\nbetween 2D image generation and 3D object generation and delivers high-fidelity\nscenes with compositional identities and aesthetic scene content. Our key\ninsight is treating scenes as hierarchical \"objects\" under isometric views,\nwhere a room functions as a complex object that can be further decomposed into\nmanipulatable items. This hierarchical approach enables us to generate 3D\ncontent that aligns with 2D representations while maintaining compositional\nstructure. To ensure completeness and spatial alignment of each decomposed\ninstance, we develop a video-diffusion-based amodal completion technique that\neffectively handles occlusions and shadows between objects, and introduce shape\nprior injection to ensure spatial coherence within the scene. Experimental\nresults demonstrate that our method produces more natural object arrangements\nand complete object instances suitable for interactive applications, while\nmaintaining physical plausibility and alignment with user inputs.\n", "link": "http://arxiv.org/abs/2504.13072v1", "date": "2025-04-17", "relevancy": 3.0888, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6369}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6369}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HiScene%3A%20Creating%20Hierarchical%203D%20Scenes%20with%20Isometric%20View%20Generation&body=Title%3A%20HiScene%3A%20Creating%20Hierarchical%203D%20Scenes%20with%20Isometric%20View%20Generation%0AAuthor%3A%20Wenqi%20Dong%20and%20Bangbang%20Yang%20and%20Zesong%20Yang%20and%20Yuan%20Li%20and%20Tao%20Hu%20and%20Hujun%20Bao%20and%20Yuewen%20Ma%20and%20Zhaopeng%20Cui%0AAbstract%3A%20%20%20Scene-level%203D%20generation%20represents%20a%20critical%20frontier%20in%20multimedia%20and%0Acomputer%20graphics%2C%20yet%20existing%20approaches%20either%20suffer%20from%20limited%20object%0Acategories%20or%20lack%20editing%20flexibility%20for%20interactive%20applications.%20In%20this%0Apaper%2C%20we%20present%20HiScene%2C%20a%20novel%20hierarchical%20framework%20that%20bridges%20the%20gap%0Abetween%202D%20image%20generation%20and%203D%20object%20generation%20and%20delivers%20high-fidelity%0Ascenes%20with%20compositional%20identities%20and%20aesthetic%20scene%20content.%20Our%20key%0Ainsight%20is%20treating%20scenes%20as%20hierarchical%20%22objects%22%20under%20isometric%20views%2C%0Awhere%20a%20room%20functions%20as%20a%20complex%20object%20that%20can%20be%20further%20decomposed%20into%0Amanipulatable%20items.%20This%20hierarchical%20approach%20enables%20us%20to%20generate%203D%0Acontent%20that%20aligns%20with%202D%20representations%20while%20maintaining%20compositional%0Astructure.%20To%20ensure%20completeness%20and%20spatial%20alignment%20of%20each%20decomposed%0Ainstance%2C%20we%20develop%20a%20video-diffusion-based%20amodal%20completion%20technique%20that%0Aeffectively%20handles%20occlusions%20and%20shadows%20between%20objects%2C%20and%20introduce%20shape%0Aprior%20injection%20to%20ensure%20spatial%20coherence%20within%20the%20scene.%20Experimental%0Aresults%20demonstrate%20that%20our%20method%20produces%20more%20natural%20object%20arrangements%0Aand%20complete%20object%20instances%20suitable%20for%20interactive%20applications%2C%20while%0Amaintaining%20physical%20plausibility%20and%20alignment%20with%20user%20inputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13072v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHiScene%253A%2520Creating%2520Hierarchical%25203D%2520Scenes%2520with%2520Isometric%2520View%2520Generation%26entry.906535625%3DWenqi%2520Dong%2520and%2520Bangbang%2520Yang%2520and%2520Zesong%2520Yang%2520and%2520Yuan%2520Li%2520and%2520Tao%2520Hu%2520and%2520Hujun%2520Bao%2520and%2520Yuewen%2520Ma%2520and%2520Zhaopeng%2520Cui%26entry.1292438233%3D%2520%2520Scene-level%25203D%2520generation%2520represents%2520a%2520critical%2520frontier%2520in%2520multimedia%2520and%250Acomputer%2520graphics%252C%2520yet%2520existing%2520approaches%2520either%2520suffer%2520from%2520limited%2520object%250Acategories%2520or%2520lack%2520editing%2520flexibility%2520for%2520interactive%2520applications.%2520In%2520this%250Apaper%252C%2520we%2520present%2520HiScene%252C%2520a%2520novel%2520hierarchical%2520framework%2520that%2520bridges%2520the%2520gap%250Abetween%25202D%2520image%2520generation%2520and%25203D%2520object%2520generation%2520and%2520delivers%2520high-fidelity%250Ascenes%2520with%2520compositional%2520identities%2520and%2520aesthetic%2520scene%2520content.%2520Our%2520key%250Ainsight%2520is%2520treating%2520scenes%2520as%2520hierarchical%2520%2522objects%2522%2520under%2520isometric%2520views%252C%250Awhere%2520a%2520room%2520functions%2520as%2520a%2520complex%2520object%2520that%2520can%2520be%2520further%2520decomposed%2520into%250Amanipulatable%2520items.%2520This%2520hierarchical%2520approach%2520enables%2520us%2520to%2520generate%25203D%250Acontent%2520that%2520aligns%2520with%25202D%2520representations%2520while%2520maintaining%2520compositional%250Astructure.%2520To%2520ensure%2520completeness%2520and%2520spatial%2520alignment%2520of%2520each%2520decomposed%250Ainstance%252C%2520we%2520develop%2520a%2520video-diffusion-based%2520amodal%2520completion%2520technique%2520that%250Aeffectively%2520handles%2520occlusions%2520and%2520shadows%2520between%2520objects%252C%2520and%2520introduce%2520shape%250Aprior%2520injection%2520to%2520ensure%2520spatial%2520coherence%2520within%2520the%2520scene.%2520Experimental%250Aresults%2520demonstrate%2520that%2520our%2520method%2520produces%2520more%2520natural%2520object%2520arrangements%250Aand%2520complete%2520object%2520instances%2520suitable%2520for%2520interactive%2520applications%252C%2520while%250Amaintaining%2520physical%2520plausibility%2520and%2520alignment%2520with%2520user%2520inputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13072v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiScene%3A%20Creating%20Hierarchical%203D%20Scenes%20with%20Isometric%20View%20Generation&entry.906535625=Wenqi%20Dong%20and%20Bangbang%20Yang%20and%20Zesong%20Yang%20and%20Yuan%20Li%20and%20Tao%20Hu%20and%20Hujun%20Bao%20and%20Yuewen%20Ma%20and%20Zhaopeng%20Cui&entry.1292438233=%20%20Scene-level%203D%20generation%20represents%20a%20critical%20frontier%20in%20multimedia%20and%0Acomputer%20graphics%2C%20yet%20existing%20approaches%20either%20suffer%20from%20limited%20object%0Acategories%20or%20lack%20editing%20flexibility%20for%20interactive%20applications.%20In%20this%0Apaper%2C%20we%20present%20HiScene%2C%20a%20novel%20hierarchical%20framework%20that%20bridges%20the%20gap%0Abetween%202D%20image%20generation%20and%203D%20object%20generation%20and%20delivers%20high-fidelity%0Ascenes%20with%20compositional%20identities%20and%20aesthetic%20scene%20content.%20Our%20key%0Ainsight%20is%20treating%20scenes%20as%20hierarchical%20%22objects%22%20under%20isometric%20views%2C%0Awhere%20a%20room%20functions%20as%20a%20complex%20object%20that%20can%20be%20further%20decomposed%20into%0Amanipulatable%20items.%20This%20hierarchical%20approach%20enables%20us%20to%20generate%203D%0Acontent%20that%20aligns%20with%202D%20representations%20while%20maintaining%20compositional%0Astructure.%20To%20ensure%20completeness%20and%20spatial%20alignment%20of%20each%20decomposed%0Ainstance%2C%20we%20develop%20a%20video-diffusion-based%20amodal%20completion%20technique%20that%0Aeffectively%20handles%20occlusions%20and%20shadows%20between%20objects%2C%20and%20introduce%20shape%0Aprior%20injection%20to%20ensure%20spatial%20coherence%20within%20the%20scene.%20Experimental%0Aresults%20demonstrate%20that%20our%20method%20produces%20more%20natural%20object%20arrangements%0Aand%20complete%20object%20instances%20suitable%20for%20interactive%20applications%2C%20while%0Amaintaining%20physical%20plausibility%20and%20alignment%20with%20user%20inputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13072v1&entry.124074799=Read"},
{"title": "Probing and Inducing Combinational Creativity in Vision-Language Models", "author": "Yongqian Peng and Yuxi Ma and Mengmeng Wang and Yuxuan Wang and Yizhou Wang and Chi Zhang and Yixin Zhu and Zilong Zheng", "abstract": "  The ability to combine existing concepts into novel ideas stands as a\nfundamental hallmark of human intelligence. Recent advances in Vision-Language\nModels (VLMs) like GPT-4V and DALLE-3 have sparked debate about whether their\noutputs reflect combinational creativity--defined by M. A. Boden (1998) as\nsynthesizing novel ideas through combining existing concepts--or sophisticated\npattern matching of training data. Drawing inspiration from cognitive science,\nwe investigate the combinational creativity of VLMs from the lens of concept\nblending. We propose the Identification-Explanation-Implication (IEI)\nframework, which decomposes creative processes into three levels: identifying\ninput spaces, extracting shared attributes, and deriving novel semantic\nimplications. To validate this framework, we curate CreativeMashup, a\nhigh-quality dataset of 666 artist-generated visual mashups annotated according\nto the IEI framework. Through extensive experiments, we demonstrate that in\ncomprehension tasks, best VLMs have surpassed average human performance while\nfalling short of expert-level understanding; in generation tasks, incorporating\nour IEI framework into the generation pipeline significantly enhances the\ncreative quality of VLMs outputs. Our findings establish both a theoretical\nfoundation for evaluating artificial creativity and practical guidelines for\nimproving creative generation in VLMs.\n", "link": "http://arxiv.org/abs/2504.13120v1", "date": "2025-04-17", "relevancy": 3.0404, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6258}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6258}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probing%20and%20Inducing%20Combinational%20Creativity%20in%20Vision-Language%20Models&body=Title%3A%20Probing%20and%20Inducing%20Combinational%20Creativity%20in%20Vision-Language%20Models%0AAuthor%3A%20Yongqian%20Peng%20and%20Yuxi%20Ma%20and%20Mengmeng%20Wang%20and%20Yuxuan%20Wang%20and%20Yizhou%20Wang%20and%20Chi%20Zhang%20and%20Yixin%20Zhu%20and%20Zilong%20Zheng%0AAbstract%3A%20%20%20The%20ability%20to%20combine%20existing%20concepts%20into%20novel%20ideas%20stands%20as%20a%0Afundamental%20hallmark%20of%20human%20intelligence.%20Recent%20advances%20in%20Vision-Language%0AModels%20%28VLMs%29%20like%20GPT-4V%20and%20DALLE-3%20have%20sparked%20debate%20about%20whether%20their%0Aoutputs%20reflect%20combinational%20creativity--defined%20by%20M.%20A.%20Boden%20%281998%29%20as%0Asynthesizing%20novel%20ideas%20through%20combining%20existing%20concepts--or%20sophisticated%0Apattern%20matching%20of%20training%20data.%20Drawing%20inspiration%20from%20cognitive%20science%2C%0Awe%20investigate%20the%20combinational%20creativity%20of%20VLMs%20from%20the%20lens%20of%20concept%0Ablending.%20We%20propose%20the%20Identification-Explanation-Implication%20%28IEI%29%0Aframework%2C%20which%20decomposes%20creative%20processes%20into%20three%20levels%3A%20identifying%0Ainput%20spaces%2C%20extracting%20shared%20attributes%2C%20and%20deriving%20novel%20semantic%0Aimplications.%20To%20validate%20this%20framework%2C%20we%20curate%20CreativeMashup%2C%20a%0Ahigh-quality%20dataset%20of%20666%20artist-generated%20visual%20mashups%20annotated%20according%0Ato%20the%20IEI%20framework.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20that%20in%0Acomprehension%20tasks%2C%20best%20VLMs%20have%20surpassed%20average%20human%20performance%20while%0Afalling%20short%20of%20expert-level%20understanding%3B%20in%20generation%20tasks%2C%20incorporating%0Aour%20IEI%20framework%20into%20the%20generation%20pipeline%20significantly%20enhances%20the%0Acreative%20quality%20of%20VLMs%20outputs.%20Our%20findings%20establish%20both%20a%20theoretical%0Afoundation%20for%20evaluating%20artificial%20creativity%20and%20practical%20guidelines%20for%0Aimproving%20creative%20generation%20in%20VLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13120v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbing%2520and%2520Inducing%2520Combinational%2520Creativity%2520in%2520Vision-Language%2520Models%26entry.906535625%3DYongqian%2520Peng%2520and%2520Yuxi%2520Ma%2520and%2520Mengmeng%2520Wang%2520and%2520Yuxuan%2520Wang%2520and%2520Yizhou%2520Wang%2520and%2520Chi%2520Zhang%2520and%2520Yixin%2520Zhu%2520and%2520Zilong%2520Zheng%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520combine%2520existing%2520concepts%2520into%2520novel%2520ideas%2520stands%2520as%2520a%250Afundamental%2520hallmark%2520of%2520human%2520intelligence.%2520Recent%2520advances%2520in%2520Vision-Language%250AModels%2520%2528VLMs%2529%2520like%2520GPT-4V%2520and%2520DALLE-3%2520have%2520sparked%2520debate%2520about%2520whether%2520their%250Aoutputs%2520reflect%2520combinational%2520creativity--defined%2520by%2520M.%2520A.%2520Boden%2520%25281998%2529%2520as%250Asynthesizing%2520novel%2520ideas%2520through%2520combining%2520existing%2520concepts--or%2520sophisticated%250Apattern%2520matching%2520of%2520training%2520data.%2520Drawing%2520inspiration%2520from%2520cognitive%2520science%252C%250Awe%2520investigate%2520the%2520combinational%2520creativity%2520of%2520VLMs%2520from%2520the%2520lens%2520of%2520concept%250Ablending.%2520We%2520propose%2520the%2520Identification-Explanation-Implication%2520%2528IEI%2529%250Aframework%252C%2520which%2520decomposes%2520creative%2520processes%2520into%2520three%2520levels%253A%2520identifying%250Ainput%2520spaces%252C%2520extracting%2520shared%2520attributes%252C%2520and%2520deriving%2520novel%2520semantic%250Aimplications.%2520To%2520validate%2520this%2520framework%252C%2520we%2520curate%2520CreativeMashup%252C%2520a%250Ahigh-quality%2520dataset%2520of%2520666%2520artist-generated%2520visual%2520mashups%2520annotated%2520according%250Ato%2520the%2520IEI%2520framework.%2520Through%2520extensive%2520experiments%252C%2520we%2520demonstrate%2520that%2520in%250Acomprehension%2520tasks%252C%2520best%2520VLMs%2520have%2520surpassed%2520average%2520human%2520performance%2520while%250Afalling%2520short%2520of%2520expert-level%2520understanding%253B%2520in%2520generation%2520tasks%252C%2520incorporating%250Aour%2520IEI%2520framework%2520into%2520the%2520generation%2520pipeline%2520significantly%2520enhances%2520the%250Acreative%2520quality%2520of%2520VLMs%2520outputs.%2520Our%2520findings%2520establish%2520both%2520a%2520theoretical%250Afoundation%2520for%2520evaluating%2520artificial%2520creativity%2520and%2520practical%2520guidelines%2520for%250Aimproving%2520creative%2520generation%2520in%2520VLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13120v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probing%20and%20Inducing%20Combinational%20Creativity%20in%20Vision-Language%20Models&entry.906535625=Yongqian%20Peng%20and%20Yuxi%20Ma%20and%20Mengmeng%20Wang%20and%20Yuxuan%20Wang%20and%20Yizhou%20Wang%20and%20Chi%20Zhang%20and%20Yixin%20Zhu%20and%20Zilong%20Zheng&entry.1292438233=%20%20The%20ability%20to%20combine%20existing%20concepts%20into%20novel%20ideas%20stands%20as%20a%0Afundamental%20hallmark%20of%20human%20intelligence.%20Recent%20advances%20in%20Vision-Language%0AModels%20%28VLMs%29%20like%20GPT-4V%20and%20DALLE-3%20have%20sparked%20debate%20about%20whether%20their%0Aoutputs%20reflect%20combinational%20creativity--defined%20by%20M.%20A.%20Boden%20%281998%29%20as%0Asynthesizing%20novel%20ideas%20through%20combining%20existing%20concepts--or%20sophisticated%0Apattern%20matching%20of%20training%20data.%20Drawing%20inspiration%20from%20cognitive%20science%2C%0Awe%20investigate%20the%20combinational%20creativity%20of%20VLMs%20from%20the%20lens%20of%20concept%0Ablending.%20We%20propose%20the%20Identification-Explanation-Implication%20%28IEI%29%0Aframework%2C%20which%20decomposes%20creative%20processes%20into%20three%20levels%3A%20identifying%0Ainput%20spaces%2C%20extracting%20shared%20attributes%2C%20and%20deriving%20novel%20semantic%0Aimplications.%20To%20validate%20this%20framework%2C%20we%20curate%20CreativeMashup%2C%20a%0Ahigh-quality%20dataset%20of%20666%20artist-generated%20visual%20mashups%20annotated%20according%0Ato%20the%20IEI%20framework.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20that%20in%0Acomprehension%20tasks%2C%20best%20VLMs%20have%20surpassed%20average%20human%20performance%20while%0Afalling%20short%20of%20expert-level%20understanding%3B%20in%20generation%20tasks%2C%20incorporating%0Aour%20IEI%20framework%20into%20the%20generation%20pipeline%20significantly%20enhances%20the%0Acreative%20quality%20of%20VLMs%20outputs.%20Our%20findings%20establish%20both%20a%20theoretical%0Afoundation%20for%20evaluating%20artificial%20creativity%20and%20practical%20guidelines%20for%0Aimproving%20creative%20generation%20in%20VLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13120v1&entry.124074799=Read"},
{"title": "RGB-Phase Speckle: Cross-Scene Stereo 3D Reconstruction via Wrapped\n  Pre-Normalization", "author": "Kai Yang and Zijian Bai and Yang Xiao and Xinyu Li and Xiaohan Shi", "abstract": "  3D reconstruction garners increasing attention alongside the advancement of\nhigh-level image applications, where dense stereo matching (DSM) serves as a\npivotal technique. Previous studies often rely on publicly available datasets\nfor training, focusing on modifying network architectures or incorporating\nspecialized modules to extract domain-invariant features and thus improve model\nrobustness. In contrast, inspired by single-frame structured-light\nphase-shifting encoding, this study introduces RGB-Speckle, a cross-scene 3D\nreconstruction framework based on an active stereo camera system, designed to\nenhance robustness. Specifically, we propose a novel phase pre-normalization\nencoding-decoding method: first, we randomly perturb phase-shift maps and embed\nthem into the three RGB channels to generate color speckle patterns;\nsubsequently, the camera captures phase-encoded images modulated by objects as\ninput to a stereo matching network. This technique effectively mitigates\nexternal interference and ensures consistent input data for RGB-Speckle,\nthereby bolstering cross-domain 3D reconstruction stability. To validate the\nproposed method, we conduct complex experiments: (1) construct a color speckle\ndataset for complex scenarios based on the proposed encoding scheme; (2)\nevaluate the impact of the phase pre-normalization encoding-decoding technique\non 3D reconstruction accuracy; and (3) further investigate its robustness\nacross diverse conditions. Experimental results demonstrate that the proposed\nRGB-Speckle model offers significant advantages in cross-domain and cross-scene\n3D reconstruction tasks, enhancing model generalization and reinforcing\nrobustness in challenging environments, thus providing a novel solution for\nrobust 3D reconstruction research.\n", "link": "http://arxiv.org/abs/2503.06125v2", "date": "2025-04-17", "relevancy": 3.0119, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6171}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.595}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RGB-Phase%20Speckle%3A%20Cross-Scene%20Stereo%203D%20Reconstruction%20via%20Wrapped%0A%20%20Pre-Normalization&body=Title%3A%20RGB-Phase%20Speckle%3A%20Cross-Scene%20Stereo%203D%20Reconstruction%20via%20Wrapped%0A%20%20Pre-Normalization%0AAuthor%3A%20Kai%20Yang%20and%20Zijian%20Bai%20and%20Yang%20Xiao%20and%20Xinyu%20Li%20and%20Xiaohan%20Shi%0AAbstract%3A%20%20%203D%20reconstruction%20garners%20increasing%20attention%20alongside%20the%20advancement%20of%0Ahigh-level%20image%20applications%2C%20where%20dense%20stereo%20matching%20%28DSM%29%20serves%20as%20a%0Apivotal%20technique.%20Previous%20studies%20often%20rely%20on%20publicly%20available%20datasets%0Afor%20training%2C%20focusing%20on%20modifying%20network%20architectures%20or%20incorporating%0Aspecialized%20modules%20to%20extract%20domain-invariant%20features%20and%20thus%20improve%20model%0Arobustness.%20In%20contrast%2C%20inspired%20by%20single-frame%20structured-light%0Aphase-shifting%20encoding%2C%20this%20study%20introduces%20RGB-Speckle%2C%20a%20cross-scene%203D%0Areconstruction%20framework%20based%20on%20an%20active%20stereo%20camera%20system%2C%20designed%20to%0Aenhance%20robustness.%20Specifically%2C%20we%20propose%20a%20novel%20phase%20pre-normalization%0Aencoding-decoding%20method%3A%20first%2C%20we%20randomly%20perturb%20phase-shift%20maps%20and%20embed%0Athem%20into%20the%20three%20RGB%20channels%20to%20generate%20color%20speckle%20patterns%3B%0Asubsequently%2C%20the%20camera%20captures%20phase-encoded%20images%20modulated%20by%20objects%20as%0Ainput%20to%20a%20stereo%20matching%20network.%20This%20technique%20effectively%20mitigates%0Aexternal%20interference%20and%20ensures%20consistent%20input%20data%20for%20RGB-Speckle%2C%0Athereby%20bolstering%20cross-domain%203D%20reconstruction%20stability.%20To%20validate%20the%0Aproposed%20method%2C%20we%20conduct%20complex%20experiments%3A%20%281%29%20construct%20a%20color%20speckle%0Adataset%20for%20complex%20scenarios%20based%20on%20the%20proposed%20encoding%20scheme%3B%20%282%29%0Aevaluate%20the%20impact%20of%20the%20phase%20pre-normalization%20encoding-decoding%20technique%0Aon%203D%20reconstruction%20accuracy%3B%20and%20%283%29%20further%20investigate%20its%20robustness%0Aacross%20diverse%20conditions.%20Experimental%20results%20demonstrate%20that%20the%20proposed%0ARGB-Speckle%20model%20offers%20significant%20advantages%20in%20cross-domain%20and%20cross-scene%0A3D%20reconstruction%20tasks%2C%20enhancing%20model%20generalization%20and%20reinforcing%0Arobustness%20in%20challenging%20environments%2C%20thus%20providing%20a%20novel%20solution%20for%0Arobust%203D%20reconstruction%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.06125v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRGB-Phase%2520Speckle%253A%2520Cross-Scene%2520Stereo%25203D%2520Reconstruction%2520via%2520Wrapped%250A%2520%2520Pre-Normalization%26entry.906535625%3DKai%2520Yang%2520and%2520Zijian%2520Bai%2520and%2520Yang%2520Xiao%2520and%2520Xinyu%2520Li%2520and%2520Xiaohan%2520Shi%26entry.1292438233%3D%2520%25203D%2520reconstruction%2520garners%2520increasing%2520attention%2520alongside%2520the%2520advancement%2520of%250Ahigh-level%2520image%2520applications%252C%2520where%2520dense%2520stereo%2520matching%2520%2528DSM%2529%2520serves%2520as%2520a%250Apivotal%2520technique.%2520Previous%2520studies%2520often%2520rely%2520on%2520publicly%2520available%2520datasets%250Afor%2520training%252C%2520focusing%2520on%2520modifying%2520network%2520architectures%2520or%2520incorporating%250Aspecialized%2520modules%2520to%2520extract%2520domain-invariant%2520features%2520and%2520thus%2520improve%2520model%250Arobustness.%2520In%2520contrast%252C%2520inspired%2520by%2520single-frame%2520structured-light%250Aphase-shifting%2520encoding%252C%2520this%2520study%2520introduces%2520RGB-Speckle%252C%2520a%2520cross-scene%25203D%250Areconstruction%2520framework%2520based%2520on%2520an%2520active%2520stereo%2520camera%2520system%252C%2520designed%2520to%250Aenhance%2520robustness.%2520Specifically%252C%2520we%2520propose%2520a%2520novel%2520phase%2520pre-normalization%250Aencoding-decoding%2520method%253A%2520first%252C%2520we%2520randomly%2520perturb%2520phase-shift%2520maps%2520and%2520embed%250Athem%2520into%2520the%2520three%2520RGB%2520channels%2520to%2520generate%2520color%2520speckle%2520patterns%253B%250Asubsequently%252C%2520the%2520camera%2520captures%2520phase-encoded%2520images%2520modulated%2520by%2520objects%2520as%250Ainput%2520to%2520a%2520stereo%2520matching%2520network.%2520This%2520technique%2520effectively%2520mitigates%250Aexternal%2520interference%2520and%2520ensures%2520consistent%2520input%2520data%2520for%2520RGB-Speckle%252C%250Athereby%2520bolstering%2520cross-domain%25203D%2520reconstruction%2520stability.%2520To%2520validate%2520the%250Aproposed%2520method%252C%2520we%2520conduct%2520complex%2520experiments%253A%2520%25281%2529%2520construct%2520a%2520color%2520speckle%250Adataset%2520for%2520complex%2520scenarios%2520based%2520on%2520the%2520proposed%2520encoding%2520scheme%253B%2520%25282%2529%250Aevaluate%2520the%2520impact%2520of%2520the%2520phase%2520pre-normalization%2520encoding-decoding%2520technique%250Aon%25203D%2520reconstruction%2520accuracy%253B%2520and%2520%25283%2529%2520further%2520investigate%2520its%2520robustness%250Aacross%2520diverse%2520conditions.%2520Experimental%2520results%2520demonstrate%2520that%2520the%2520proposed%250ARGB-Speckle%2520model%2520offers%2520significant%2520advantages%2520in%2520cross-domain%2520and%2520cross-scene%250A3D%2520reconstruction%2520tasks%252C%2520enhancing%2520model%2520generalization%2520and%2520reinforcing%250Arobustness%2520in%2520challenging%2520environments%252C%2520thus%2520providing%2520a%2520novel%2520solution%2520for%250Arobust%25203D%2520reconstruction%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.06125v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RGB-Phase%20Speckle%3A%20Cross-Scene%20Stereo%203D%20Reconstruction%20via%20Wrapped%0A%20%20Pre-Normalization&entry.906535625=Kai%20Yang%20and%20Zijian%20Bai%20and%20Yang%20Xiao%20and%20Xinyu%20Li%20and%20Xiaohan%20Shi&entry.1292438233=%20%203D%20reconstruction%20garners%20increasing%20attention%20alongside%20the%20advancement%20of%0Ahigh-level%20image%20applications%2C%20where%20dense%20stereo%20matching%20%28DSM%29%20serves%20as%20a%0Apivotal%20technique.%20Previous%20studies%20often%20rely%20on%20publicly%20available%20datasets%0Afor%20training%2C%20focusing%20on%20modifying%20network%20architectures%20or%20incorporating%0Aspecialized%20modules%20to%20extract%20domain-invariant%20features%20and%20thus%20improve%20model%0Arobustness.%20In%20contrast%2C%20inspired%20by%20single-frame%20structured-light%0Aphase-shifting%20encoding%2C%20this%20study%20introduces%20RGB-Speckle%2C%20a%20cross-scene%203D%0Areconstruction%20framework%20based%20on%20an%20active%20stereo%20camera%20system%2C%20designed%20to%0Aenhance%20robustness.%20Specifically%2C%20we%20propose%20a%20novel%20phase%20pre-normalization%0Aencoding-decoding%20method%3A%20first%2C%20we%20randomly%20perturb%20phase-shift%20maps%20and%20embed%0Athem%20into%20the%20three%20RGB%20channels%20to%20generate%20color%20speckle%20patterns%3B%0Asubsequently%2C%20the%20camera%20captures%20phase-encoded%20images%20modulated%20by%20objects%20as%0Ainput%20to%20a%20stereo%20matching%20network.%20This%20technique%20effectively%20mitigates%0Aexternal%20interference%20and%20ensures%20consistent%20input%20data%20for%20RGB-Speckle%2C%0Athereby%20bolstering%20cross-domain%203D%20reconstruction%20stability.%20To%20validate%20the%0Aproposed%20method%2C%20we%20conduct%20complex%20experiments%3A%20%281%29%20construct%20a%20color%20speckle%0Adataset%20for%20complex%20scenarios%20based%20on%20the%20proposed%20encoding%20scheme%3B%20%282%29%0Aevaluate%20the%20impact%20of%20the%20phase%20pre-normalization%20encoding-decoding%20technique%0Aon%203D%20reconstruction%20accuracy%3B%20and%20%283%29%20further%20investigate%20its%20robustness%0Aacross%20diverse%20conditions.%20Experimental%20results%20demonstrate%20that%20the%20proposed%0ARGB-Speckle%20model%20offers%20significant%20advantages%20in%20cross-domain%20and%20cross-scene%0A3D%20reconstruction%20tasks%2C%20enhancing%20model%20generalization%20and%20reinforcing%0Arobustness%20in%20challenging%20environments%2C%20thus%20providing%20a%20novel%20solution%20for%0Arobust%203D%20reconstruction%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.06125v2&entry.124074799=Read"},
{"title": "All-in-One Transferring Image Compression from Human Perception to\n  Multi-Machine Perception", "author": "Jiancheng Zhao and Xiang Ji and Zhuoxiao Li and Zunian Wan and Weihang Ran and Mingze Ma and Muyao Niu and Yifan Zhan and Cheng-Ching Tseng and Yinqiang Zheng", "abstract": "  Efficiently transferring Learned Image Compression (LIC) model from human\nperception to machine perception is an emerging challenge in vision-centric\nrepresentation learning. Existing approaches typically adapt LIC to downstream\ntasks in a single-task manner, which is inefficient, lacks task interaction,\nand results in multiple task-specific bitstreams. To address these limitations,\nwe propose an asymmetric adaptor framework that supports multi-task adaptation\nwithin a single model. Our method introduces a shared adaptor to learn general\nsemantic features and task-specific adaptors to preserve task-level\ndistinctions. With only lightweight plug-in modules and a frozen base codec,\nour method achieves strong performance across multiple tasks while maintaining\ncompression efficiency. Experiments on the PASCAL-Context benchmark demonstrate\nthat our method outperforms both Fully Fine-Tuned and other Parameter Efficient\nFine-Tuned (PEFT) baselines, and validating the effectiveness of multi-vision\ntransferring.\n", "link": "http://arxiv.org/abs/2504.12997v1", "date": "2025-04-17", "relevancy": 3.0039, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6354}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5878}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20All-in-One%20Transferring%20Image%20Compression%20from%20Human%20Perception%20to%0A%20%20Multi-Machine%20Perception&body=Title%3A%20All-in-One%20Transferring%20Image%20Compression%20from%20Human%20Perception%20to%0A%20%20Multi-Machine%20Perception%0AAuthor%3A%20Jiancheng%20Zhao%20and%20Xiang%20Ji%20and%20Zhuoxiao%20Li%20and%20Zunian%20Wan%20and%20Weihang%20Ran%20and%20Mingze%20Ma%20and%20Muyao%20Niu%20and%20Yifan%20Zhan%20and%20Cheng-Ching%20Tseng%20and%20Yinqiang%20Zheng%0AAbstract%3A%20%20%20Efficiently%20transferring%20Learned%20Image%20Compression%20%28LIC%29%20model%20from%20human%0Aperception%20to%20machine%20perception%20is%20an%20emerging%20challenge%20in%20vision-centric%0Arepresentation%20learning.%20Existing%20approaches%20typically%20adapt%20LIC%20to%20downstream%0Atasks%20in%20a%20single-task%20manner%2C%20which%20is%20inefficient%2C%20lacks%20task%20interaction%2C%0Aand%20results%20in%20multiple%20task-specific%20bitstreams.%20To%20address%20these%20limitations%2C%0Awe%20propose%20an%20asymmetric%20adaptor%20framework%20that%20supports%20multi-task%20adaptation%0Awithin%20a%20single%20model.%20Our%20method%20introduces%20a%20shared%20adaptor%20to%20learn%20general%0Asemantic%20features%20and%20task-specific%20adaptors%20to%20preserve%20task-level%0Adistinctions.%20With%20only%20lightweight%20plug-in%20modules%20and%20a%20frozen%20base%20codec%2C%0Aour%20method%20achieves%20strong%20performance%20across%20multiple%20tasks%20while%20maintaining%0Acompression%20efficiency.%20Experiments%20on%20the%20PASCAL-Context%20benchmark%20demonstrate%0Athat%20our%20method%20outperforms%20both%20Fully%20Fine-Tuned%20and%20other%20Parameter%20Efficient%0AFine-Tuned%20%28PEFT%29%20baselines%2C%20and%20validating%20the%20effectiveness%20of%20multi-vision%0Atransferring.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12997v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAll-in-One%2520Transferring%2520Image%2520Compression%2520from%2520Human%2520Perception%2520to%250A%2520%2520Multi-Machine%2520Perception%26entry.906535625%3DJiancheng%2520Zhao%2520and%2520Xiang%2520Ji%2520and%2520Zhuoxiao%2520Li%2520and%2520Zunian%2520Wan%2520and%2520Weihang%2520Ran%2520and%2520Mingze%2520Ma%2520and%2520Muyao%2520Niu%2520and%2520Yifan%2520Zhan%2520and%2520Cheng-Ching%2520Tseng%2520and%2520Yinqiang%2520Zheng%26entry.1292438233%3D%2520%2520Efficiently%2520transferring%2520Learned%2520Image%2520Compression%2520%2528LIC%2529%2520model%2520from%2520human%250Aperception%2520to%2520machine%2520perception%2520is%2520an%2520emerging%2520challenge%2520in%2520vision-centric%250Arepresentation%2520learning.%2520Existing%2520approaches%2520typically%2520adapt%2520LIC%2520to%2520downstream%250Atasks%2520in%2520a%2520single-task%2520manner%252C%2520which%2520is%2520inefficient%252C%2520lacks%2520task%2520interaction%252C%250Aand%2520results%2520in%2520multiple%2520task-specific%2520bitstreams.%2520To%2520address%2520these%2520limitations%252C%250Awe%2520propose%2520an%2520asymmetric%2520adaptor%2520framework%2520that%2520supports%2520multi-task%2520adaptation%250Awithin%2520a%2520single%2520model.%2520Our%2520method%2520introduces%2520a%2520shared%2520adaptor%2520to%2520learn%2520general%250Asemantic%2520features%2520and%2520task-specific%2520adaptors%2520to%2520preserve%2520task-level%250Adistinctions.%2520With%2520only%2520lightweight%2520plug-in%2520modules%2520and%2520a%2520frozen%2520base%2520codec%252C%250Aour%2520method%2520achieves%2520strong%2520performance%2520across%2520multiple%2520tasks%2520while%2520maintaining%250Acompression%2520efficiency.%2520Experiments%2520on%2520the%2520PASCAL-Context%2520benchmark%2520demonstrate%250Athat%2520our%2520method%2520outperforms%2520both%2520Fully%2520Fine-Tuned%2520and%2520other%2520Parameter%2520Efficient%250AFine-Tuned%2520%2528PEFT%2529%2520baselines%252C%2520and%2520validating%2520the%2520effectiveness%2520of%2520multi-vision%250Atransferring.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12997v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=All-in-One%20Transferring%20Image%20Compression%20from%20Human%20Perception%20to%0A%20%20Multi-Machine%20Perception&entry.906535625=Jiancheng%20Zhao%20and%20Xiang%20Ji%20and%20Zhuoxiao%20Li%20and%20Zunian%20Wan%20and%20Weihang%20Ran%20and%20Mingze%20Ma%20and%20Muyao%20Niu%20and%20Yifan%20Zhan%20and%20Cheng-Ching%20Tseng%20and%20Yinqiang%20Zheng&entry.1292438233=%20%20Efficiently%20transferring%20Learned%20Image%20Compression%20%28LIC%29%20model%20from%20human%0Aperception%20to%20machine%20perception%20is%20an%20emerging%20challenge%20in%20vision-centric%0Arepresentation%20learning.%20Existing%20approaches%20typically%20adapt%20LIC%20to%20downstream%0Atasks%20in%20a%20single-task%20manner%2C%20which%20is%20inefficient%2C%20lacks%20task%20interaction%2C%0Aand%20results%20in%20multiple%20task-specific%20bitstreams.%20To%20address%20these%20limitations%2C%0Awe%20propose%20an%20asymmetric%20adaptor%20framework%20that%20supports%20multi-task%20adaptation%0Awithin%20a%20single%20model.%20Our%20method%20introduces%20a%20shared%20adaptor%20to%20learn%20general%0Asemantic%20features%20and%20task-specific%20adaptors%20to%20preserve%20task-level%0Adistinctions.%20With%20only%20lightweight%20plug-in%20modules%20and%20a%20frozen%20base%20codec%2C%0Aour%20method%20achieves%20strong%20performance%20across%20multiple%20tasks%20while%20maintaining%0Acompression%20efficiency.%20Experiments%20on%20the%20PASCAL-Context%20benchmark%20demonstrate%0Athat%20our%20method%20outperforms%20both%20Fully%20Fine-Tuned%20and%20other%20Parameter%20Efficient%0AFine-Tuned%20%28PEFT%29%20baselines%2C%20and%20validating%20the%20effectiveness%20of%20multi-vision%0Atransferring.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12997v1&entry.124074799=Read"},
{"title": "Perception Encoder: The best visual embeddings are not at the output of\n  the network", "author": "Daniel Bolya and Po-Yao Huang and Peize Sun and Jang Hyun Cho and Andrea Madotto and Chen Wei and Tengyu Ma and Jiale Zhi and Jathushan Rajasegaran and Hanoona Rasheed and Junke Wang and Marco Monteiro and Hu Xu and Shiyu Dong and Nikhila Ravi and Daniel Li and Piotr Doll\u00e1r and Christoph Feichtenhofer", "abstract": "  We introduce Perception Encoder (PE), a state-of-the-art encoder for image\nand video understanding trained via simple vision-language learning.\nTraditionally, vision encoders have relied on a variety of pretraining\nobjectives, each tailored to specific downstream tasks such as classification,\ncaptioning, or localization. Surprisingly, after scaling our carefully tuned\nimage pretraining recipe and refining with our robust video data engine, we\nfind that contrastive vision-language training alone can produce strong,\ngeneral embeddings for all of these downstream tasks. There is only one caveat:\nthese embeddings are hidden within the intermediate layers of the network. To\ndraw them out, we introduce two alignment methods, language alignment for\nmultimodal language modeling, and spatial alignment for dense prediction.\nTogether with the core contrastive checkpoint, our PE family of models achieves\nstate-of-the-art performance on a wide variety of tasks, including zero-shot\nimage and video classification and retrieval; document, image, and video Q&A;\nand spatial tasks such as detection, depth estimation, and tracking. To foster\nfurther research, we are releasing our models, code, and a novel dataset of\nsynthetically and human-annotated videos.\n", "link": "http://arxiv.org/abs/2504.13181v1", "date": "2025-04-17", "relevancy": 3.0012, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6251}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6251}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perception%20Encoder%3A%20The%20best%20visual%20embeddings%20are%20not%20at%20the%20output%20of%0A%20%20the%20network&body=Title%3A%20Perception%20Encoder%3A%20The%20best%20visual%20embeddings%20are%20not%20at%20the%20output%20of%0A%20%20the%20network%0AAuthor%3A%20Daniel%20Bolya%20and%20Po-Yao%20Huang%20and%20Peize%20Sun%20and%20Jang%20Hyun%20Cho%20and%20Andrea%20Madotto%20and%20Chen%20Wei%20and%20Tengyu%20Ma%20and%20Jiale%20Zhi%20and%20Jathushan%20Rajasegaran%20and%20Hanoona%20Rasheed%20and%20Junke%20Wang%20and%20Marco%20Monteiro%20and%20Hu%20Xu%20and%20Shiyu%20Dong%20and%20Nikhila%20Ravi%20and%20Daniel%20Li%20and%20Piotr%20Doll%C3%A1r%20and%20Christoph%20Feichtenhofer%0AAbstract%3A%20%20%20We%20introduce%20Perception%20Encoder%20%28PE%29%2C%20a%20state-of-the-art%20encoder%20for%20image%0Aand%20video%20understanding%20trained%20via%20simple%20vision-language%20learning.%0ATraditionally%2C%20vision%20encoders%20have%20relied%20on%20a%20variety%20of%20pretraining%0Aobjectives%2C%20each%20tailored%20to%20specific%20downstream%20tasks%20such%20as%20classification%2C%0Acaptioning%2C%20or%20localization.%20Surprisingly%2C%20after%20scaling%20our%20carefully%20tuned%0Aimage%20pretraining%20recipe%20and%20refining%20with%20our%20robust%20video%20data%20engine%2C%20we%0Afind%20that%20contrastive%20vision-language%20training%20alone%20can%20produce%20strong%2C%0Ageneral%20embeddings%20for%20all%20of%20these%20downstream%20tasks.%20There%20is%20only%20one%20caveat%3A%0Athese%20embeddings%20are%20hidden%20within%20the%20intermediate%20layers%20of%20the%20network.%20To%0Adraw%20them%20out%2C%20we%20introduce%20two%20alignment%20methods%2C%20language%20alignment%20for%0Amultimodal%20language%20modeling%2C%20and%20spatial%20alignment%20for%20dense%20prediction.%0ATogether%20with%20the%20core%20contrastive%20checkpoint%2C%20our%20PE%20family%20of%20models%20achieves%0Astate-of-the-art%20performance%20on%20a%20wide%20variety%20of%20tasks%2C%20including%20zero-shot%0Aimage%20and%20video%20classification%20and%20retrieval%3B%20document%2C%20image%2C%20and%20video%20Q%26A%3B%0Aand%20spatial%20tasks%20such%20as%20detection%2C%20depth%20estimation%2C%20and%20tracking.%20To%20foster%0Afurther%20research%2C%20we%20are%20releasing%20our%20models%2C%20code%2C%20and%20a%20novel%20dataset%20of%0Asynthetically%20and%20human-annotated%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13181v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerception%2520Encoder%253A%2520The%2520best%2520visual%2520embeddings%2520are%2520not%2520at%2520the%2520output%2520of%250A%2520%2520the%2520network%26entry.906535625%3DDaniel%2520Bolya%2520and%2520Po-Yao%2520Huang%2520and%2520Peize%2520Sun%2520and%2520Jang%2520Hyun%2520Cho%2520and%2520Andrea%2520Madotto%2520and%2520Chen%2520Wei%2520and%2520Tengyu%2520Ma%2520and%2520Jiale%2520Zhi%2520and%2520Jathushan%2520Rajasegaran%2520and%2520Hanoona%2520Rasheed%2520and%2520Junke%2520Wang%2520and%2520Marco%2520Monteiro%2520and%2520Hu%2520Xu%2520and%2520Shiyu%2520Dong%2520and%2520Nikhila%2520Ravi%2520and%2520Daniel%2520Li%2520and%2520Piotr%2520Doll%25C3%25A1r%2520and%2520Christoph%2520Feichtenhofer%26entry.1292438233%3D%2520%2520We%2520introduce%2520Perception%2520Encoder%2520%2528PE%2529%252C%2520a%2520state-of-the-art%2520encoder%2520for%2520image%250Aand%2520video%2520understanding%2520trained%2520via%2520simple%2520vision-language%2520learning.%250ATraditionally%252C%2520vision%2520encoders%2520have%2520relied%2520on%2520a%2520variety%2520of%2520pretraining%250Aobjectives%252C%2520each%2520tailored%2520to%2520specific%2520downstream%2520tasks%2520such%2520as%2520classification%252C%250Acaptioning%252C%2520or%2520localization.%2520Surprisingly%252C%2520after%2520scaling%2520our%2520carefully%2520tuned%250Aimage%2520pretraining%2520recipe%2520and%2520refining%2520with%2520our%2520robust%2520video%2520data%2520engine%252C%2520we%250Afind%2520that%2520contrastive%2520vision-language%2520training%2520alone%2520can%2520produce%2520strong%252C%250Ageneral%2520embeddings%2520for%2520all%2520of%2520these%2520downstream%2520tasks.%2520There%2520is%2520only%2520one%2520caveat%253A%250Athese%2520embeddings%2520are%2520hidden%2520within%2520the%2520intermediate%2520layers%2520of%2520the%2520network.%2520To%250Adraw%2520them%2520out%252C%2520we%2520introduce%2520two%2520alignment%2520methods%252C%2520language%2520alignment%2520for%250Amultimodal%2520language%2520modeling%252C%2520and%2520spatial%2520alignment%2520for%2520dense%2520prediction.%250ATogether%2520with%2520the%2520core%2520contrastive%2520checkpoint%252C%2520our%2520PE%2520family%2520of%2520models%2520achieves%250Astate-of-the-art%2520performance%2520on%2520a%2520wide%2520variety%2520of%2520tasks%252C%2520including%2520zero-shot%250Aimage%2520and%2520video%2520classification%2520and%2520retrieval%253B%2520document%252C%2520image%252C%2520and%2520video%2520Q%2526A%253B%250Aand%2520spatial%2520tasks%2520such%2520as%2520detection%252C%2520depth%2520estimation%252C%2520and%2520tracking.%2520To%2520foster%250Afurther%2520research%252C%2520we%2520are%2520releasing%2520our%2520models%252C%2520code%252C%2520and%2520a%2520novel%2520dataset%2520of%250Asynthetically%2520and%2520human-annotated%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13181v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perception%20Encoder%3A%20The%20best%20visual%20embeddings%20are%20not%20at%20the%20output%20of%0A%20%20the%20network&entry.906535625=Daniel%20Bolya%20and%20Po-Yao%20Huang%20and%20Peize%20Sun%20and%20Jang%20Hyun%20Cho%20and%20Andrea%20Madotto%20and%20Chen%20Wei%20and%20Tengyu%20Ma%20and%20Jiale%20Zhi%20and%20Jathushan%20Rajasegaran%20and%20Hanoona%20Rasheed%20and%20Junke%20Wang%20and%20Marco%20Monteiro%20and%20Hu%20Xu%20and%20Shiyu%20Dong%20and%20Nikhila%20Ravi%20and%20Daniel%20Li%20and%20Piotr%20Doll%C3%A1r%20and%20Christoph%20Feichtenhofer&entry.1292438233=%20%20We%20introduce%20Perception%20Encoder%20%28PE%29%2C%20a%20state-of-the-art%20encoder%20for%20image%0Aand%20video%20understanding%20trained%20via%20simple%20vision-language%20learning.%0ATraditionally%2C%20vision%20encoders%20have%20relied%20on%20a%20variety%20of%20pretraining%0Aobjectives%2C%20each%20tailored%20to%20specific%20downstream%20tasks%20such%20as%20classification%2C%0Acaptioning%2C%20or%20localization.%20Surprisingly%2C%20after%20scaling%20our%20carefully%20tuned%0Aimage%20pretraining%20recipe%20and%20refining%20with%20our%20robust%20video%20data%20engine%2C%20we%0Afind%20that%20contrastive%20vision-language%20training%20alone%20can%20produce%20strong%2C%0Ageneral%20embeddings%20for%20all%20of%20these%20downstream%20tasks.%20There%20is%20only%20one%20caveat%3A%0Athese%20embeddings%20are%20hidden%20within%20the%20intermediate%20layers%20of%20the%20network.%20To%0Adraw%20them%20out%2C%20we%20introduce%20two%20alignment%20methods%2C%20language%20alignment%20for%0Amultimodal%20language%20modeling%2C%20and%20spatial%20alignment%20for%20dense%20prediction.%0ATogether%20with%20the%20core%20contrastive%20checkpoint%2C%20our%20PE%20family%20of%20models%20achieves%0Astate-of-the-art%20performance%20on%20a%20wide%20variety%20of%20tasks%2C%20including%20zero-shot%0Aimage%20and%20video%20classification%20and%20retrieval%3B%20document%2C%20image%2C%20and%20video%20Q%26A%3B%0Aand%20spatial%20tasks%20such%20as%20detection%2C%20depth%20estimation%2C%20and%20tracking.%20To%20foster%0Afurther%20research%2C%20we%20are%20releasing%20our%20models%2C%20code%2C%20and%20a%20novel%20dataset%20of%0Asynthetically%20and%20human-annotated%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13181v1&entry.124074799=Read"},
{"title": "Do Vision-Language Models Represent Space and How? Evaluating Spatial\n  Frame of Reference Under Ambiguities", "author": "Zheyuan Zhang and Fengyuan Hu and Jayjun Lee and Freda Shi and Parisa Kordjamshidi and Joyce Chai and Ziqiao Ma", "abstract": "  Spatial expressions in situated communication can be ambiguous, as their\nmeanings vary depending on the frames of reference (FoR) adopted by speakers\nand listeners. While spatial language understanding and reasoning by\nvision-language models (VLMs) have gained increasing attention, potential\nambiguities in these models are still under-explored. To address this issue, we\npresent the COnsistent Multilingual Frame Of Reference Test (COMFORT), an\nevaluation protocol to systematically assess the spatial reasoning capabilities\nof VLMs. We evaluate nine state-of-the-art VLMs using COMFORT. Despite showing\nsome alignment with English conventions in resolving ambiguities, our\nexperiments reveal significant shortcomings of VLMs: notably, the models (1)\nexhibit poor robustness and consistency, (2) lack the flexibility to\naccommodate multiple FoRs, and (3) fail to adhere to language-specific or\nculture-specific conventions in cross-lingual tests, as English tends to\ndominate other languages. With a growing effort to align vision-language models\nwith human cognitive intuitions, we call for more attention to the ambiguous\nnature and cross-cultural diversity of spatial reasoning.\n", "link": "http://arxiv.org/abs/2410.17385v2", "date": "2025-04-17", "relevancy": 2.9978, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6354}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6354}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Vision-Language%20Models%20Represent%20Space%20and%20How%3F%20Evaluating%20Spatial%0A%20%20Frame%20of%20Reference%20Under%20Ambiguities&body=Title%3A%20Do%20Vision-Language%20Models%20Represent%20Space%20and%20How%3F%20Evaluating%20Spatial%0A%20%20Frame%20of%20Reference%20Under%20Ambiguities%0AAuthor%3A%20Zheyuan%20Zhang%20and%20Fengyuan%20Hu%20and%20Jayjun%20Lee%20and%20Freda%20Shi%20and%20Parisa%20Kordjamshidi%20and%20Joyce%20Chai%20and%20Ziqiao%20Ma%0AAbstract%3A%20%20%20Spatial%20expressions%20in%20situated%20communication%20can%20be%20ambiguous%2C%20as%20their%0Ameanings%20vary%20depending%20on%20the%20frames%20of%20reference%20%28FoR%29%20adopted%20by%20speakers%0Aand%20listeners.%20While%20spatial%20language%20understanding%20and%20reasoning%20by%0Avision-language%20models%20%28VLMs%29%20have%20gained%20increasing%20attention%2C%20potential%0Aambiguities%20in%20these%20models%20are%20still%20under-explored.%20To%20address%20this%20issue%2C%20we%0Apresent%20the%20COnsistent%20Multilingual%20Frame%20Of%20Reference%20Test%20%28COMFORT%29%2C%20an%0Aevaluation%20protocol%20to%20systematically%20assess%20the%20spatial%20reasoning%20capabilities%0Aof%20VLMs.%20We%20evaluate%20nine%20state-of-the-art%20VLMs%20using%20COMFORT.%20Despite%20showing%0Asome%20alignment%20with%20English%20conventions%20in%20resolving%20ambiguities%2C%20our%0Aexperiments%20reveal%20significant%20shortcomings%20of%20VLMs%3A%20notably%2C%20the%20models%20%281%29%0Aexhibit%20poor%20robustness%20and%20consistency%2C%20%282%29%20lack%20the%20flexibility%20to%0Aaccommodate%20multiple%20FoRs%2C%20and%20%283%29%20fail%20to%20adhere%20to%20language-specific%20or%0Aculture-specific%20conventions%20in%20cross-lingual%20tests%2C%20as%20English%20tends%20to%0Adominate%20other%20languages.%20With%20a%20growing%20effort%20to%20align%20vision-language%20models%0Awith%20human%20cognitive%20intuitions%2C%20we%20call%20for%20more%20attention%20to%20the%20ambiguous%0Anature%20and%20cross-cultural%20diversity%20of%20spatial%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17385v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Vision-Language%2520Models%2520Represent%2520Space%2520and%2520How%253F%2520Evaluating%2520Spatial%250A%2520%2520Frame%2520of%2520Reference%2520Under%2520Ambiguities%26entry.906535625%3DZheyuan%2520Zhang%2520and%2520Fengyuan%2520Hu%2520and%2520Jayjun%2520Lee%2520and%2520Freda%2520Shi%2520and%2520Parisa%2520Kordjamshidi%2520and%2520Joyce%2520Chai%2520and%2520Ziqiao%2520Ma%26entry.1292438233%3D%2520%2520Spatial%2520expressions%2520in%2520situated%2520communication%2520can%2520be%2520ambiguous%252C%2520as%2520their%250Ameanings%2520vary%2520depending%2520on%2520the%2520frames%2520of%2520reference%2520%2528FoR%2529%2520adopted%2520by%2520speakers%250Aand%2520listeners.%2520While%2520spatial%2520language%2520understanding%2520and%2520reasoning%2520by%250Avision-language%2520models%2520%2528VLMs%2529%2520have%2520gained%2520increasing%2520attention%252C%2520potential%250Aambiguities%2520in%2520these%2520models%2520are%2520still%2520under-explored.%2520To%2520address%2520this%2520issue%252C%2520we%250Apresent%2520the%2520COnsistent%2520Multilingual%2520Frame%2520Of%2520Reference%2520Test%2520%2528COMFORT%2529%252C%2520an%250Aevaluation%2520protocol%2520to%2520systematically%2520assess%2520the%2520spatial%2520reasoning%2520capabilities%250Aof%2520VLMs.%2520We%2520evaluate%2520nine%2520state-of-the-art%2520VLMs%2520using%2520COMFORT.%2520Despite%2520showing%250Asome%2520alignment%2520with%2520English%2520conventions%2520in%2520resolving%2520ambiguities%252C%2520our%250Aexperiments%2520reveal%2520significant%2520shortcomings%2520of%2520VLMs%253A%2520notably%252C%2520the%2520models%2520%25281%2529%250Aexhibit%2520poor%2520robustness%2520and%2520consistency%252C%2520%25282%2529%2520lack%2520the%2520flexibility%2520to%250Aaccommodate%2520multiple%2520FoRs%252C%2520and%2520%25283%2529%2520fail%2520to%2520adhere%2520to%2520language-specific%2520or%250Aculture-specific%2520conventions%2520in%2520cross-lingual%2520tests%252C%2520as%2520English%2520tends%2520to%250Adominate%2520other%2520languages.%2520With%2520a%2520growing%2520effort%2520to%2520align%2520vision-language%2520models%250Awith%2520human%2520cognitive%2520intuitions%252C%2520we%2520call%2520for%2520more%2520attention%2520to%2520the%2520ambiguous%250Anature%2520and%2520cross-cultural%2520diversity%2520of%2520spatial%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17385v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Vision-Language%20Models%20Represent%20Space%20and%20How%3F%20Evaluating%20Spatial%0A%20%20Frame%20of%20Reference%20Under%20Ambiguities&entry.906535625=Zheyuan%20Zhang%20and%20Fengyuan%20Hu%20and%20Jayjun%20Lee%20and%20Freda%20Shi%20and%20Parisa%20Kordjamshidi%20and%20Joyce%20Chai%20and%20Ziqiao%20Ma&entry.1292438233=%20%20Spatial%20expressions%20in%20situated%20communication%20can%20be%20ambiguous%2C%20as%20their%0Ameanings%20vary%20depending%20on%20the%20frames%20of%20reference%20%28FoR%29%20adopted%20by%20speakers%0Aand%20listeners.%20While%20spatial%20language%20understanding%20and%20reasoning%20by%0Avision-language%20models%20%28VLMs%29%20have%20gained%20increasing%20attention%2C%20potential%0Aambiguities%20in%20these%20models%20are%20still%20under-explored.%20To%20address%20this%20issue%2C%20we%0Apresent%20the%20COnsistent%20Multilingual%20Frame%20Of%20Reference%20Test%20%28COMFORT%29%2C%20an%0Aevaluation%20protocol%20to%20systematically%20assess%20the%20spatial%20reasoning%20capabilities%0Aof%20VLMs.%20We%20evaluate%20nine%20state-of-the-art%20VLMs%20using%20COMFORT.%20Despite%20showing%0Asome%20alignment%20with%20English%20conventions%20in%20resolving%20ambiguities%2C%20our%0Aexperiments%20reveal%20significant%20shortcomings%20of%20VLMs%3A%20notably%2C%20the%20models%20%281%29%0Aexhibit%20poor%20robustness%20and%20consistency%2C%20%282%29%20lack%20the%20flexibility%20to%0Aaccommodate%20multiple%20FoRs%2C%20and%20%283%29%20fail%20to%20adhere%20to%20language-specific%20or%0Aculture-specific%20conventions%20in%20cross-lingual%20tests%2C%20as%20English%20tends%20to%0Adominate%20other%20languages.%20With%20a%20growing%20effort%20to%20align%20vision-language%20models%0Awith%20human%20cognitive%20intuitions%2C%20we%20call%20for%20more%20attention%20to%20the%20ambiguous%0Anature%20and%20cross-cultural%20diversity%20of%20spatial%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17385v2&entry.124074799=Read"},
{"title": "AerialMegaDepth: Learning Aerial-Ground Reconstruction and View\n  Synthesis", "author": "Khiem Vuong and Anurag Ghosh and Deva Ramanan and Srinivasa Narasimhan and Shubham Tulsiani", "abstract": "  We explore the task of geometric reconstruction of images captured from a\nmixture of ground and aerial views. Current state-of-the-art learning-based\napproaches fail to handle the extreme viewpoint variation between aerial-ground\nimage pairs. Our hypothesis is that the lack of high-quality, co-registered\naerial-ground datasets for training is a key reason for this failure. Such data\nis difficult to assemble precisely because it is difficult to reconstruct in a\nscalable way. To overcome this challenge, we propose a scalable framework\ncombining pseudo-synthetic renderings from 3D city-wide meshes (e.g., Google\nEarth) with real, ground-level crowd-sourced images (e.g., MegaDepth). The\npseudo-synthetic data simulates a wide range of aerial viewpoints, while the\nreal, crowd-sourced images help improve visual fidelity for ground-level images\nwhere mesh-based renderings lack sufficient detail, effectively bridging the\ndomain gap between real images and pseudo-synthetic renderings. Using this\nhybrid dataset, we fine-tune several state-of-the-art algorithms and achieve\nsignificant improvements on real-world, zero-shot aerial-ground tasks. For\nexample, we observe that baseline DUSt3R localizes fewer than 5% of\naerial-ground pairs within 5 degrees of camera rotation error, while\nfine-tuning with our data raises accuracy to nearly 56%, addressing a major\nfailure point in handling large viewpoint changes. Beyond camera estimation and\nscene reconstruction, our dataset also improves performance on downstream tasks\nlike novel-view synthesis in challenging aerial-ground scenarios, demonstrating\nthe practical value of our approach in real-world applications.\n", "link": "http://arxiv.org/abs/2504.13157v1", "date": "2025-04-17", "relevancy": 2.9814, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6008}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5957}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AerialMegaDepth%3A%20Learning%20Aerial-Ground%20Reconstruction%20and%20View%0A%20%20Synthesis&body=Title%3A%20AerialMegaDepth%3A%20Learning%20Aerial-Ground%20Reconstruction%20and%20View%0A%20%20Synthesis%0AAuthor%3A%20Khiem%20Vuong%20and%20Anurag%20Ghosh%20and%20Deva%20Ramanan%20and%20Srinivasa%20Narasimhan%20and%20Shubham%20Tulsiani%0AAbstract%3A%20%20%20We%20explore%20the%20task%20of%20geometric%20reconstruction%20of%20images%20captured%20from%20a%0Amixture%20of%20ground%20and%20aerial%20views.%20Current%20state-of-the-art%20learning-based%0Aapproaches%20fail%20to%20handle%20the%20extreme%20viewpoint%20variation%20between%20aerial-ground%0Aimage%20pairs.%20Our%20hypothesis%20is%20that%20the%20lack%20of%20high-quality%2C%20co-registered%0Aaerial-ground%20datasets%20for%20training%20is%20a%20key%20reason%20for%20this%20failure.%20Such%20data%0Ais%20difficult%20to%20assemble%20precisely%20because%20it%20is%20difficult%20to%20reconstruct%20in%20a%0Ascalable%20way.%20To%20overcome%20this%20challenge%2C%20we%20propose%20a%20scalable%20framework%0Acombining%20pseudo-synthetic%20renderings%20from%203D%20city-wide%20meshes%20%28e.g.%2C%20Google%0AEarth%29%20with%20real%2C%20ground-level%20crowd-sourced%20images%20%28e.g.%2C%20MegaDepth%29.%20The%0Apseudo-synthetic%20data%20simulates%20a%20wide%20range%20of%20aerial%20viewpoints%2C%20while%20the%0Areal%2C%20crowd-sourced%20images%20help%20improve%20visual%20fidelity%20for%20ground-level%20images%0Awhere%20mesh-based%20renderings%20lack%20sufficient%20detail%2C%20effectively%20bridging%20the%0Adomain%20gap%20between%20real%20images%20and%20pseudo-synthetic%20renderings.%20Using%20this%0Ahybrid%20dataset%2C%20we%20fine-tune%20several%20state-of-the-art%20algorithms%20and%20achieve%0Asignificant%20improvements%20on%20real-world%2C%20zero-shot%20aerial-ground%20tasks.%20For%0Aexample%2C%20we%20observe%20that%20baseline%20DUSt3R%20localizes%20fewer%20than%205%25%20of%0Aaerial-ground%20pairs%20within%205%20degrees%20of%20camera%20rotation%20error%2C%20while%0Afine-tuning%20with%20our%20data%20raises%20accuracy%20to%20nearly%2056%25%2C%20addressing%20a%20major%0Afailure%20point%20in%20handling%20large%20viewpoint%20changes.%20Beyond%20camera%20estimation%20and%0Ascene%20reconstruction%2C%20our%20dataset%20also%20improves%20performance%20on%20downstream%20tasks%0Alike%20novel-view%20synthesis%20in%20challenging%20aerial-ground%20scenarios%2C%20demonstrating%0Athe%20practical%20value%20of%20our%20approach%20in%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13157v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAerialMegaDepth%253A%2520Learning%2520Aerial-Ground%2520Reconstruction%2520and%2520View%250A%2520%2520Synthesis%26entry.906535625%3DKhiem%2520Vuong%2520and%2520Anurag%2520Ghosh%2520and%2520Deva%2520Ramanan%2520and%2520Srinivasa%2520Narasimhan%2520and%2520Shubham%2520Tulsiani%26entry.1292438233%3D%2520%2520We%2520explore%2520the%2520task%2520of%2520geometric%2520reconstruction%2520of%2520images%2520captured%2520from%2520a%250Amixture%2520of%2520ground%2520and%2520aerial%2520views.%2520Current%2520state-of-the-art%2520learning-based%250Aapproaches%2520fail%2520to%2520handle%2520the%2520extreme%2520viewpoint%2520variation%2520between%2520aerial-ground%250Aimage%2520pairs.%2520Our%2520hypothesis%2520is%2520that%2520the%2520lack%2520of%2520high-quality%252C%2520co-registered%250Aaerial-ground%2520datasets%2520for%2520training%2520is%2520a%2520key%2520reason%2520for%2520this%2520failure.%2520Such%2520data%250Ais%2520difficult%2520to%2520assemble%2520precisely%2520because%2520it%2520is%2520difficult%2520to%2520reconstruct%2520in%2520a%250Ascalable%2520way.%2520To%2520overcome%2520this%2520challenge%252C%2520we%2520propose%2520a%2520scalable%2520framework%250Acombining%2520pseudo-synthetic%2520renderings%2520from%25203D%2520city-wide%2520meshes%2520%2528e.g.%252C%2520Google%250AEarth%2529%2520with%2520real%252C%2520ground-level%2520crowd-sourced%2520images%2520%2528e.g.%252C%2520MegaDepth%2529.%2520The%250Apseudo-synthetic%2520data%2520simulates%2520a%2520wide%2520range%2520of%2520aerial%2520viewpoints%252C%2520while%2520the%250Areal%252C%2520crowd-sourced%2520images%2520help%2520improve%2520visual%2520fidelity%2520for%2520ground-level%2520images%250Awhere%2520mesh-based%2520renderings%2520lack%2520sufficient%2520detail%252C%2520effectively%2520bridging%2520the%250Adomain%2520gap%2520between%2520real%2520images%2520and%2520pseudo-synthetic%2520renderings.%2520Using%2520this%250Ahybrid%2520dataset%252C%2520we%2520fine-tune%2520several%2520state-of-the-art%2520algorithms%2520and%2520achieve%250Asignificant%2520improvements%2520on%2520real-world%252C%2520zero-shot%2520aerial-ground%2520tasks.%2520For%250Aexample%252C%2520we%2520observe%2520that%2520baseline%2520DUSt3R%2520localizes%2520fewer%2520than%25205%2525%2520of%250Aaerial-ground%2520pairs%2520within%25205%2520degrees%2520of%2520camera%2520rotation%2520error%252C%2520while%250Afine-tuning%2520with%2520our%2520data%2520raises%2520accuracy%2520to%2520nearly%252056%2525%252C%2520addressing%2520a%2520major%250Afailure%2520point%2520in%2520handling%2520large%2520viewpoint%2520changes.%2520Beyond%2520camera%2520estimation%2520and%250Ascene%2520reconstruction%252C%2520our%2520dataset%2520also%2520improves%2520performance%2520on%2520downstream%2520tasks%250Alike%2520novel-view%2520synthesis%2520in%2520challenging%2520aerial-ground%2520scenarios%252C%2520demonstrating%250Athe%2520practical%2520value%2520of%2520our%2520approach%2520in%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13157v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AerialMegaDepth%3A%20Learning%20Aerial-Ground%20Reconstruction%20and%20View%0A%20%20Synthesis&entry.906535625=Khiem%20Vuong%20and%20Anurag%20Ghosh%20and%20Deva%20Ramanan%20and%20Srinivasa%20Narasimhan%20and%20Shubham%20Tulsiani&entry.1292438233=%20%20We%20explore%20the%20task%20of%20geometric%20reconstruction%20of%20images%20captured%20from%20a%0Amixture%20of%20ground%20and%20aerial%20views.%20Current%20state-of-the-art%20learning-based%0Aapproaches%20fail%20to%20handle%20the%20extreme%20viewpoint%20variation%20between%20aerial-ground%0Aimage%20pairs.%20Our%20hypothesis%20is%20that%20the%20lack%20of%20high-quality%2C%20co-registered%0Aaerial-ground%20datasets%20for%20training%20is%20a%20key%20reason%20for%20this%20failure.%20Such%20data%0Ais%20difficult%20to%20assemble%20precisely%20because%20it%20is%20difficult%20to%20reconstruct%20in%20a%0Ascalable%20way.%20To%20overcome%20this%20challenge%2C%20we%20propose%20a%20scalable%20framework%0Acombining%20pseudo-synthetic%20renderings%20from%203D%20city-wide%20meshes%20%28e.g.%2C%20Google%0AEarth%29%20with%20real%2C%20ground-level%20crowd-sourced%20images%20%28e.g.%2C%20MegaDepth%29.%20The%0Apseudo-synthetic%20data%20simulates%20a%20wide%20range%20of%20aerial%20viewpoints%2C%20while%20the%0Areal%2C%20crowd-sourced%20images%20help%20improve%20visual%20fidelity%20for%20ground-level%20images%0Awhere%20mesh-based%20renderings%20lack%20sufficient%20detail%2C%20effectively%20bridging%20the%0Adomain%20gap%20between%20real%20images%20and%20pseudo-synthetic%20renderings.%20Using%20this%0Ahybrid%20dataset%2C%20we%20fine-tune%20several%20state-of-the-art%20algorithms%20and%20achieve%0Asignificant%20improvements%20on%20real-world%2C%20zero-shot%20aerial-ground%20tasks.%20For%0Aexample%2C%20we%20observe%20that%20baseline%20DUSt3R%20localizes%20fewer%20than%205%25%20of%0Aaerial-ground%20pairs%20within%205%20degrees%20of%20camera%20rotation%20error%2C%20while%0Afine-tuning%20with%20our%20data%20raises%20accuracy%20to%20nearly%2056%25%2C%20addressing%20a%20major%0Afailure%20point%20in%20handling%20large%20viewpoint%20changes.%20Beyond%20camera%20estimation%20and%0Ascene%20reconstruction%2C%20our%20dataset%20also%20improves%20performance%20on%20downstream%20tasks%0Alike%20novel-view%20synthesis%20in%20challenging%20aerial-ground%20scenarios%2C%20demonstrating%0Athe%20practical%20value%20of%20our%20approach%20in%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13157v1&entry.124074799=Read"},
{"title": "Online Video Understanding: OVBench and VideoChat-Online", "author": "Zhenpeng Huang and Xinhao Li and Jiaqi Li and Jing Wang and Xiangyu Zeng and Cheng Liang and Tao Wu and Xi Chen and Liang Li and Limin Wang", "abstract": "  Multimodal Large Language Models (MLLMs) have significantly progressed in\noffline video understanding. However, applying these models to real-world\nscenarios, such as autonomous driving and human-computer interaction, presents\nunique challenges due to the need for real-time processing of continuous online\nvideo streams. To this end, this paper presents systematic efforts from three\nperspectives: evaluation benchmark, model architecture, and training strategy.\nFirst, we introduce OVBench, a comprehensive question-answering benchmark\ndesigned to evaluate models' ability to perceive, memorize, and reason within\nonline video contexts. It features 6 core task types across three temporal\ncontexts-past, current, and future-forming 16 subtasks from diverse datasets.\nSecond, we propose a new Pyramid Memory Bank (PMB) that effectively retains key\nspatiotemporal information in video streams. Third, we proposed an\noffline-to-online learning paradigm, designing an interleaved dialogue format\nfor online video data and constructing an instruction-tuning dataset tailored\nfor online video training. This framework led to the development of\nVideoChat-Online, a robust and efficient model for online video understanding.\nDespite the lower computational cost and higher efficiency, VideoChat-Online\noutperforms existing state-of-the-art offline and online models across popular\noffline video benchmarks and OVBench, demonstrating the effectiveness of our\nmodel architecture and training strategy. % Our approach surpasses existing\nstate-of-the-art offline models Qwen2-VL 7B and online models Flash-VStream, by\n4.19% and 23.7% on OVBench, respectively.\n", "link": "http://arxiv.org/abs/2501.00584v2", "date": "2025-04-17", "relevancy": 2.9699, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.618}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.618}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Video%20Understanding%3A%20OVBench%20and%20VideoChat-Online&body=Title%3A%20Online%20Video%20Understanding%3A%20OVBench%20and%20VideoChat-Online%0AAuthor%3A%20Zhenpeng%20Huang%20and%20Xinhao%20Li%20and%20Jiaqi%20Li%20and%20Jing%20Wang%20and%20Xiangyu%20Zeng%20and%20Cheng%20Liang%20and%20Tao%20Wu%20and%20Xi%20Chen%20and%20Liang%20Li%20and%20Limin%20Wang%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20significantly%20progressed%20in%0Aoffline%20video%20understanding.%20However%2C%20applying%20these%20models%20to%20real-world%0Ascenarios%2C%20such%20as%20autonomous%20driving%20and%20human-computer%20interaction%2C%20presents%0Aunique%20challenges%20due%20to%20the%20need%20for%20real-time%20processing%20of%20continuous%20online%0Avideo%20streams.%20To%20this%20end%2C%20this%20paper%20presents%20systematic%20efforts%20from%20three%0Aperspectives%3A%20evaluation%20benchmark%2C%20model%20architecture%2C%20and%20training%20strategy.%0AFirst%2C%20we%20introduce%20OVBench%2C%20a%20comprehensive%20question-answering%20benchmark%0Adesigned%20to%20evaluate%20models%27%20ability%20to%20perceive%2C%20memorize%2C%20and%20reason%20within%0Aonline%20video%20contexts.%20It%20features%206%20core%20task%20types%20across%20three%20temporal%0Acontexts-past%2C%20current%2C%20and%20future-forming%2016%20subtasks%20from%20diverse%20datasets.%0ASecond%2C%20we%20propose%20a%20new%20Pyramid%20Memory%20Bank%20%28PMB%29%20that%20effectively%20retains%20key%0Aspatiotemporal%20information%20in%20video%20streams.%20Third%2C%20we%20proposed%20an%0Aoffline-to-online%20learning%20paradigm%2C%20designing%20an%20interleaved%20dialogue%20format%0Afor%20online%20video%20data%20and%20constructing%20an%20instruction-tuning%20dataset%20tailored%0Afor%20online%20video%20training.%20This%20framework%20led%20to%20the%20development%20of%0AVideoChat-Online%2C%20a%20robust%20and%20efficient%20model%20for%20online%20video%20understanding.%0ADespite%20the%20lower%20computational%20cost%20and%20higher%20efficiency%2C%20VideoChat-Online%0Aoutperforms%20existing%20state-of-the-art%20offline%20and%20online%20models%20across%20popular%0Aoffline%20video%20benchmarks%20and%20OVBench%2C%20demonstrating%20the%20effectiveness%20of%20our%0Amodel%20architecture%20and%20training%20strategy.%20%25%20Our%20approach%20surpasses%20existing%0Astate-of-the-art%20offline%20models%20Qwen2-VL%207B%20and%20online%20models%20Flash-VStream%2C%20by%0A4.19%25%20and%2023.7%25%20on%20OVBench%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.00584v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Video%2520Understanding%253A%2520OVBench%2520and%2520VideoChat-Online%26entry.906535625%3DZhenpeng%2520Huang%2520and%2520Xinhao%2520Li%2520and%2520Jiaqi%2520Li%2520and%2520Jing%2520Wang%2520and%2520Xiangyu%2520Zeng%2520and%2520Cheng%2520Liang%2520and%2520Tao%2520Wu%2520and%2520Xi%2520Chen%2520and%2520Liang%2520Li%2520and%2520Limin%2520Wang%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520significantly%2520progressed%2520in%250Aoffline%2520video%2520understanding.%2520However%252C%2520applying%2520these%2520models%2520to%2520real-world%250Ascenarios%252C%2520such%2520as%2520autonomous%2520driving%2520and%2520human-computer%2520interaction%252C%2520presents%250Aunique%2520challenges%2520due%2520to%2520the%2520need%2520for%2520real-time%2520processing%2520of%2520continuous%2520online%250Avideo%2520streams.%2520To%2520this%2520end%252C%2520this%2520paper%2520presents%2520systematic%2520efforts%2520from%2520three%250Aperspectives%253A%2520evaluation%2520benchmark%252C%2520model%2520architecture%252C%2520and%2520training%2520strategy.%250AFirst%252C%2520we%2520introduce%2520OVBench%252C%2520a%2520comprehensive%2520question-answering%2520benchmark%250Adesigned%2520to%2520evaluate%2520models%2527%2520ability%2520to%2520perceive%252C%2520memorize%252C%2520and%2520reason%2520within%250Aonline%2520video%2520contexts.%2520It%2520features%25206%2520core%2520task%2520types%2520across%2520three%2520temporal%250Acontexts-past%252C%2520current%252C%2520and%2520future-forming%252016%2520subtasks%2520from%2520diverse%2520datasets.%250ASecond%252C%2520we%2520propose%2520a%2520new%2520Pyramid%2520Memory%2520Bank%2520%2528PMB%2529%2520that%2520effectively%2520retains%2520key%250Aspatiotemporal%2520information%2520in%2520video%2520streams.%2520Third%252C%2520we%2520proposed%2520an%250Aoffline-to-online%2520learning%2520paradigm%252C%2520designing%2520an%2520interleaved%2520dialogue%2520format%250Afor%2520online%2520video%2520data%2520and%2520constructing%2520an%2520instruction-tuning%2520dataset%2520tailored%250Afor%2520online%2520video%2520training.%2520This%2520framework%2520led%2520to%2520the%2520development%2520of%250AVideoChat-Online%252C%2520a%2520robust%2520and%2520efficient%2520model%2520for%2520online%2520video%2520understanding.%250ADespite%2520the%2520lower%2520computational%2520cost%2520and%2520higher%2520efficiency%252C%2520VideoChat-Online%250Aoutperforms%2520existing%2520state-of-the-art%2520offline%2520and%2520online%2520models%2520across%2520popular%250Aoffline%2520video%2520benchmarks%2520and%2520OVBench%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520our%250Amodel%2520architecture%2520and%2520training%2520strategy.%2520%2525%2520Our%2520approach%2520surpasses%2520existing%250Astate-of-the-art%2520offline%2520models%2520Qwen2-VL%25207B%2520and%2520online%2520models%2520Flash-VStream%252C%2520by%250A4.19%2525%2520and%252023.7%2525%2520on%2520OVBench%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.00584v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Video%20Understanding%3A%20OVBench%20and%20VideoChat-Online&entry.906535625=Zhenpeng%20Huang%20and%20Xinhao%20Li%20and%20Jiaqi%20Li%20and%20Jing%20Wang%20and%20Xiangyu%20Zeng%20and%20Cheng%20Liang%20and%20Tao%20Wu%20and%20Xi%20Chen%20and%20Liang%20Li%20and%20Limin%20Wang&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20significantly%20progressed%20in%0Aoffline%20video%20understanding.%20However%2C%20applying%20these%20models%20to%20real-world%0Ascenarios%2C%20such%20as%20autonomous%20driving%20and%20human-computer%20interaction%2C%20presents%0Aunique%20challenges%20due%20to%20the%20need%20for%20real-time%20processing%20of%20continuous%20online%0Avideo%20streams.%20To%20this%20end%2C%20this%20paper%20presents%20systematic%20efforts%20from%20three%0Aperspectives%3A%20evaluation%20benchmark%2C%20model%20architecture%2C%20and%20training%20strategy.%0AFirst%2C%20we%20introduce%20OVBench%2C%20a%20comprehensive%20question-answering%20benchmark%0Adesigned%20to%20evaluate%20models%27%20ability%20to%20perceive%2C%20memorize%2C%20and%20reason%20within%0Aonline%20video%20contexts.%20It%20features%206%20core%20task%20types%20across%20three%20temporal%0Acontexts-past%2C%20current%2C%20and%20future-forming%2016%20subtasks%20from%20diverse%20datasets.%0ASecond%2C%20we%20propose%20a%20new%20Pyramid%20Memory%20Bank%20%28PMB%29%20that%20effectively%20retains%20key%0Aspatiotemporal%20information%20in%20video%20streams.%20Third%2C%20we%20proposed%20an%0Aoffline-to-online%20learning%20paradigm%2C%20designing%20an%20interleaved%20dialogue%20format%0Afor%20online%20video%20data%20and%20constructing%20an%20instruction-tuning%20dataset%20tailored%0Afor%20online%20video%20training.%20This%20framework%20led%20to%20the%20development%20of%0AVideoChat-Online%2C%20a%20robust%20and%20efficient%20model%20for%20online%20video%20understanding.%0ADespite%20the%20lower%20computational%20cost%20and%20higher%20efficiency%2C%20VideoChat-Online%0Aoutperforms%20existing%20state-of-the-art%20offline%20and%20online%20models%20across%20popular%0Aoffline%20video%20benchmarks%20and%20OVBench%2C%20demonstrating%20the%20effectiveness%20of%20our%0Amodel%20architecture%20and%20training%20strategy.%20%25%20Our%20approach%20surpasses%20existing%0Astate-of-the-art%20offline%20models%20Qwen2-VL%207B%20and%20online%20models%20Flash-VStream%2C%20by%0A4.19%25%20and%2023.7%25%20on%20OVBench%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.00584v2&entry.124074799=Read"},
{"title": "ViTa-Zero: Zero-shot Visuotactile Object 6D Pose Estimation", "author": "Hongyu Li and James Akl and Srinath Sridhar and Tye Brady and Taskin Padir", "abstract": "  Object 6D pose estimation is a critical challenge in robotics, particularly\nfor manipulation tasks. While prior research combining visual and tactile\n(visuotactile) information has shown promise, these approaches often struggle\nwith generalization due to the limited availability of visuotactile data. In\nthis paper, we introduce ViTa-Zero, a zero-shot visuotactile pose estimation\nframework. Our key innovation lies in leveraging a visual model as its backbone\nand performing feasibility checking and test-time optimization based on\nphysical constraints derived from tactile and proprioceptive observations.\nSpecifically, we model the gripper-object interaction as a spring-mass system,\nwhere tactile sensors induce attractive forces, and proprioception generates\nrepulsive forces. We validate our framework through experiments on a real-world\nrobot setup, demonstrating its effectiveness across representative visual\nbackbones and manipulation scenarios, including grasping, object picking, and\nbimanual handover. Compared to the visual models, our approach overcomes some\ndrastic failure modes while tracking the in-hand object pose. In our\nexperiments, our approach shows an average increase of 55% in AUC of ADD-S and\n60% in ADD, along with an 80% lower position error compared to FoundationPose.\n", "link": "http://arxiv.org/abs/2504.13179v1", "date": "2025-04-17", "relevancy": 2.9687, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6147}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5969}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViTa-Zero%3A%20Zero-shot%20Visuotactile%20Object%206D%20Pose%20Estimation&body=Title%3A%20ViTa-Zero%3A%20Zero-shot%20Visuotactile%20Object%206D%20Pose%20Estimation%0AAuthor%3A%20Hongyu%20Li%20and%20James%20Akl%20and%20Srinath%20Sridhar%20and%20Tye%20Brady%20and%20Taskin%20Padir%0AAbstract%3A%20%20%20Object%206D%20pose%20estimation%20is%20a%20critical%20challenge%20in%20robotics%2C%20particularly%0Afor%20manipulation%20tasks.%20While%20prior%20research%20combining%20visual%20and%20tactile%0A%28visuotactile%29%20information%20has%20shown%20promise%2C%20these%20approaches%20often%20struggle%0Awith%20generalization%20due%20to%20the%20limited%20availability%20of%20visuotactile%20data.%20In%0Athis%20paper%2C%20we%20introduce%20ViTa-Zero%2C%20a%20zero-shot%20visuotactile%20pose%20estimation%0Aframework.%20Our%20key%20innovation%20lies%20in%20leveraging%20a%20visual%20model%20as%20its%20backbone%0Aand%20performing%20feasibility%20checking%20and%20test-time%20optimization%20based%20on%0Aphysical%20constraints%20derived%20from%20tactile%20and%20proprioceptive%20observations.%0ASpecifically%2C%20we%20model%20the%20gripper-object%20interaction%20as%20a%20spring-mass%20system%2C%0Awhere%20tactile%20sensors%20induce%20attractive%20forces%2C%20and%20proprioception%20generates%0Arepulsive%20forces.%20We%20validate%20our%20framework%20through%20experiments%20on%20a%20real-world%0Arobot%20setup%2C%20demonstrating%20its%20effectiveness%20across%20representative%20visual%0Abackbones%20and%20manipulation%20scenarios%2C%20including%20grasping%2C%20object%20picking%2C%20and%0Abimanual%20handover.%20Compared%20to%20the%20visual%20models%2C%20our%20approach%20overcomes%20some%0Adrastic%20failure%20modes%20while%20tracking%20the%20in-hand%20object%20pose.%20In%20our%0Aexperiments%2C%20our%20approach%20shows%20an%20average%20increase%20of%2055%25%20in%20AUC%20of%20ADD-S%20and%0A60%25%20in%20ADD%2C%20along%20with%20an%2080%25%20lower%20position%20error%20compared%20to%20FoundationPose.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13179v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViTa-Zero%253A%2520Zero-shot%2520Visuotactile%2520Object%25206D%2520Pose%2520Estimation%26entry.906535625%3DHongyu%2520Li%2520and%2520James%2520Akl%2520and%2520Srinath%2520Sridhar%2520and%2520Tye%2520Brady%2520and%2520Taskin%2520Padir%26entry.1292438233%3D%2520%2520Object%25206D%2520pose%2520estimation%2520is%2520a%2520critical%2520challenge%2520in%2520robotics%252C%2520particularly%250Afor%2520manipulation%2520tasks.%2520While%2520prior%2520research%2520combining%2520visual%2520and%2520tactile%250A%2528visuotactile%2529%2520information%2520has%2520shown%2520promise%252C%2520these%2520approaches%2520often%2520struggle%250Awith%2520generalization%2520due%2520to%2520the%2520limited%2520availability%2520of%2520visuotactile%2520data.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520ViTa-Zero%252C%2520a%2520zero-shot%2520visuotactile%2520pose%2520estimation%250Aframework.%2520Our%2520key%2520innovation%2520lies%2520in%2520leveraging%2520a%2520visual%2520model%2520as%2520its%2520backbone%250Aand%2520performing%2520feasibility%2520checking%2520and%2520test-time%2520optimization%2520based%2520on%250Aphysical%2520constraints%2520derived%2520from%2520tactile%2520and%2520proprioceptive%2520observations.%250ASpecifically%252C%2520we%2520model%2520the%2520gripper-object%2520interaction%2520as%2520a%2520spring-mass%2520system%252C%250Awhere%2520tactile%2520sensors%2520induce%2520attractive%2520forces%252C%2520and%2520proprioception%2520generates%250Arepulsive%2520forces.%2520We%2520validate%2520our%2520framework%2520through%2520experiments%2520on%2520a%2520real-world%250Arobot%2520setup%252C%2520demonstrating%2520its%2520effectiveness%2520across%2520representative%2520visual%250Abackbones%2520and%2520manipulation%2520scenarios%252C%2520including%2520grasping%252C%2520object%2520picking%252C%2520and%250Abimanual%2520handover.%2520Compared%2520to%2520the%2520visual%2520models%252C%2520our%2520approach%2520overcomes%2520some%250Adrastic%2520failure%2520modes%2520while%2520tracking%2520the%2520in-hand%2520object%2520pose.%2520In%2520our%250Aexperiments%252C%2520our%2520approach%2520shows%2520an%2520average%2520increase%2520of%252055%2525%2520in%2520AUC%2520of%2520ADD-S%2520and%250A60%2525%2520in%2520ADD%252C%2520along%2520with%2520an%252080%2525%2520lower%2520position%2520error%2520compared%2520to%2520FoundationPose.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13179v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViTa-Zero%3A%20Zero-shot%20Visuotactile%20Object%206D%20Pose%20Estimation&entry.906535625=Hongyu%20Li%20and%20James%20Akl%20and%20Srinath%20Sridhar%20and%20Tye%20Brady%20and%20Taskin%20Padir&entry.1292438233=%20%20Object%206D%20pose%20estimation%20is%20a%20critical%20challenge%20in%20robotics%2C%20particularly%0Afor%20manipulation%20tasks.%20While%20prior%20research%20combining%20visual%20and%20tactile%0A%28visuotactile%29%20information%20has%20shown%20promise%2C%20these%20approaches%20often%20struggle%0Awith%20generalization%20due%20to%20the%20limited%20availability%20of%20visuotactile%20data.%20In%0Athis%20paper%2C%20we%20introduce%20ViTa-Zero%2C%20a%20zero-shot%20visuotactile%20pose%20estimation%0Aframework.%20Our%20key%20innovation%20lies%20in%20leveraging%20a%20visual%20model%20as%20its%20backbone%0Aand%20performing%20feasibility%20checking%20and%20test-time%20optimization%20based%20on%0Aphysical%20constraints%20derived%20from%20tactile%20and%20proprioceptive%20observations.%0ASpecifically%2C%20we%20model%20the%20gripper-object%20interaction%20as%20a%20spring-mass%20system%2C%0Awhere%20tactile%20sensors%20induce%20attractive%20forces%2C%20and%20proprioception%20generates%0Arepulsive%20forces.%20We%20validate%20our%20framework%20through%20experiments%20on%20a%20real-world%0Arobot%20setup%2C%20demonstrating%20its%20effectiveness%20across%20representative%20visual%0Abackbones%20and%20manipulation%20scenarios%2C%20including%20grasping%2C%20object%20picking%2C%20and%0Abimanual%20handover.%20Compared%20to%20the%20visual%20models%2C%20our%20approach%20overcomes%20some%0Adrastic%20failure%20modes%20while%20tracking%20the%20in-hand%20object%20pose.%20In%20our%0Aexperiments%2C%20our%20approach%20shows%20an%20average%20increase%20of%2055%25%20in%20AUC%20of%20ADD-S%20and%0A60%25%20in%20ADD%2C%20along%20with%20an%2080%25%20lower%20position%20error%20compared%20to%20FoundationPose.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13179v1&entry.124074799=Read"},
{"title": "EarthGPT-X: Enabling MLLMs to Flexibly and Comprehensively Understand\n  Multi-Source Remote Sensing Imagery", "author": "Wei Zhang and Miaoxin Cai and Yaqian Ning and Tong Zhang and Yin Zhuang and He Chen and Jun Li and Xuerui Mao", "abstract": "  Recent advances in the visual-language area have developed natural\nmulti-modal large language models (MLLMs) for spatial reasoning through visual\nprompting. However, due to remote sensing (RS) imagery containing abundant\ngeospatial information that differs from natural images, it is challenging to\neffectively adapt natural spatial models to the RS domain. Moreover, current RS\nMLLMs are limited in overly narrow interpretation levels and interaction\nmanner, hindering their applicability in real-world scenarios. To address those\nchallenges, a spatial MLLM named EarthGPT-X is proposed, enabling a\ncomprehensive understanding of multi-source RS imagery, such as optical,\nsynthetic aperture radar (SAR), and infrared. EarthGPT-X offers zoom-in and\nzoom-out insight, and possesses flexible multi-grained interactive abilities.\nMoreover, EarthGPT-X unifies two types of critical spatial tasks (i.e.,\nreferring and grounding) into a visual prompting framework. To achieve these\nversatile capabilities, several key strategies are developed. The first is the\nmulti-modal content integration method, which enhances the interplay between\nimages, visual prompts, and text instructions. Subsequently, a cross-domain\none-stage fusion training strategy is proposed, utilizing the large language\nmodel (LLM) as a unified interface for multi-source multi-task learning.\nFurthermore, by incorporating a pixel perception module, the referring and\ngrounding tasks are seamlessly unified within a single framework. In addition,\nthe experiments conducted demonstrate the superiority of the proposed\nEarthGPT-X in multi-grained tasks and its impressive flexibility in multi-modal\ninteraction, revealing significant advancements of MLLM in the RS field.\n", "link": "http://arxiv.org/abs/2504.12795v1", "date": "2025-04-17", "relevancy": 2.9571, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5993}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5993}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EarthGPT-X%3A%20Enabling%20MLLMs%20to%20Flexibly%20and%20Comprehensively%20Understand%0A%20%20Multi-Source%20Remote%20Sensing%20Imagery&body=Title%3A%20EarthGPT-X%3A%20Enabling%20MLLMs%20to%20Flexibly%20and%20Comprehensively%20Understand%0A%20%20Multi-Source%20Remote%20Sensing%20Imagery%0AAuthor%3A%20Wei%20Zhang%20and%20Miaoxin%20Cai%20and%20Yaqian%20Ning%20and%20Tong%20Zhang%20and%20Yin%20Zhuang%20and%20He%20Chen%20and%20Jun%20Li%20and%20Xuerui%20Mao%0AAbstract%3A%20%20%20Recent%20advances%20in%20the%20visual-language%20area%20have%20developed%20natural%0Amulti-modal%20large%20language%20models%20%28MLLMs%29%20for%20spatial%20reasoning%20through%20visual%0Aprompting.%20However%2C%20due%20to%20remote%20sensing%20%28RS%29%20imagery%20containing%20abundant%0Ageospatial%20information%20that%20differs%20from%20natural%20images%2C%20it%20is%20challenging%20to%0Aeffectively%20adapt%20natural%20spatial%20models%20to%20the%20RS%20domain.%20Moreover%2C%20current%20RS%0AMLLMs%20are%20limited%20in%20overly%20narrow%20interpretation%20levels%20and%20interaction%0Amanner%2C%20hindering%20their%20applicability%20in%20real-world%20scenarios.%20To%20address%20those%0Achallenges%2C%20a%20spatial%20MLLM%20named%20EarthGPT-X%20is%20proposed%2C%20enabling%20a%0Acomprehensive%20understanding%20of%20multi-source%20RS%20imagery%2C%20such%20as%20optical%2C%0Asynthetic%20aperture%20radar%20%28SAR%29%2C%20and%20infrared.%20EarthGPT-X%20offers%20zoom-in%20and%0Azoom-out%20insight%2C%20and%20possesses%20flexible%20multi-grained%20interactive%20abilities.%0AMoreover%2C%20EarthGPT-X%20unifies%20two%20types%20of%20critical%20spatial%20tasks%20%28i.e.%2C%0Areferring%20and%20grounding%29%20into%20a%20visual%20prompting%20framework.%20To%20achieve%20these%0Aversatile%20capabilities%2C%20several%20key%20strategies%20are%20developed.%20The%20first%20is%20the%0Amulti-modal%20content%20integration%20method%2C%20which%20enhances%20the%20interplay%20between%0Aimages%2C%20visual%20prompts%2C%20and%20text%20instructions.%20Subsequently%2C%20a%20cross-domain%0Aone-stage%20fusion%20training%20strategy%20is%20proposed%2C%20utilizing%20the%20large%20language%0Amodel%20%28LLM%29%20as%20a%20unified%20interface%20for%20multi-source%20multi-task%20learning.%0AFurthermore%2C%20by%20incorporating%20a%20pixel%20perception%20module%2C%20the%20referring%20and%0Agrounding%20tasks%20are%20seamlessly%20unified%20within%20a%20single%20framework.%20In%20addition%2C%0Athe%20experiments%20conducted%20demonstrate%20the%20superiority%20of%20the%20proposed%0AEarthGPT-X%20in%20multi-grained%20tasks%20and%20its%20impressive%20flexibility%20in%20multi-modal%0Ainteraction%2C%20revealing%20significant%20advancements%20of%20MLLM%20in%20the%20RS%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12795v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEarthGPT-X%253A%2520Enabling%2520MLLMs%2520to%2520Flexibly%2520and%2520Comprehensively%2520Understand%250A%2520%2520Multi-Source%2520Remote%2520Sensing%2520Imagery%26entry.906535625%3DWei%2520Zhang%2520and%2520Miaoxin%2520Cai%2520and%2520Yaqian%2520Ning%2520and%2520Tong%2520Zhang%2520and%2520Yin%2520Zhuang%2520and%2520He%2520Chen%2520and%2520Jun%2520Li%2520and%2520Xuerui%2520Mao%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520the%2520visual-language%2520area%2520have%2520developed%2520natural%250Amulti-modal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520for%2520spatial%2520reasoning%2520through%2520visual%250Aprompting.%2520However%252C%2520due%2520to%2520remote%2520sensing%2520%2528RS%2529%2520imagery%2520containing%2520abundant%250Ageospatial%2520information%2520that%2520differs%2520from%2520natural%2520images%252C%2520it%2520is%2520challenging%2520to%250Aeffectively%2520adapt%2520natural%2520spatial%2520models%2520to%2520the%2520RS%2520domain.%2520Moreover%252C%2520current%2520RS%250AMLLMs%2520are%2520limited%2520in%2520overly%2520narrow%2520interpretation%2520levels%2520and%2520interaction%250Amanner%252C%2520hindering%2520their%2520applicability%2520in%2520real-world%2520scenarios.%2520To%2520address%2520those%250Achallenges%252C%2520a%2520spatial%2520MLLM%2520named%2520EarthGPT-X%2520is%2520proposed%252C%2520enabling%2520a%250Acomprehensive%2520understanding%2520of%2520multi-source%2520RS%2520imagery%252C%2520such%2520as%2520optical%252C%250Asynthetic%2520aperture%2520radar%2520%2528SAR%2529%252C%2520and%2520infrared.%2520EarthGPT-X%2520offers%2520zoom-in%2520and%250Azoom-out%2520insight%252C%2520and%2520possesses%2520flexible%2520multi-grained%2520interactive%2520abilities.%250AMoreover%252C%2520EarthGPT-X%2520unifies%2520two%2520types%2520of%2520critical%2520spatial%2520tasks%2520%2528i.e.%252C%250Areferring%2520and%2520grounding%2529%2520into%2520a%2520visual%2520prompting%2520framework.%2520To%2520achieve%2520these%250Aversatile%2520capabilities%252C%2520several%2520key%2520strategies%2520are%2520developed.%2520The%2520first%2520is%2520the%250Amulti-modal%2520content%2520integration%2520method%252C%2520which%2520enhances%2520the%2520interplay%2520between%250Aimages%252C%2520visual%2520prompts%252C%2520and%2520text%2520instructions.%2520Subsequently%252C%2520a%2520cross-domain%250Aone-stage%2520fusion%2520training%2520strategy%2520is%2520proposed%252C%2520utilizing%2520the%2520large%2520language%250Amodel%2520%2528LLM%2529%2520as%2520a%2520unified%2520interface%2520for%2520multi-source%2520multi-task%2520learning.%250AFurthermore%252C%2520by%2520incorporating%2520a%2520pixel%2520perception%2520module%252C%2520the%2520referring%2520and%250Agrounding%2520tasks%2520are%2520seamlessly%2520unified%2520within%2520a%2520single%2520framework.%2520In%2520addition%252C%250Athe%2520experiments%2520conducted%2520demonstrate%2520the%2520superiority%2520of%2520the%2520proposed%250AEarthGPT-X%2520in%2520multi-grained%2520tasks%2520and%2520its%2520impressive%2520flexibility%2520in%2520multi-modal%250Ainteraction%252C%2520revealing%2520significant%2520advancements%2520of%2520MLLM%2520in%2520the%2520RS%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12795v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EarthGPT-X%3A%20Enabling%20MLLMs%20to%20Flexibly%20and%20Comprehensively%20Understand%0A%20%20Multi-Source%20Remote%20Sensing%20Imagery&entry.906535625=Wei%20Zhang%20and%20Miaoxin%20Cai%20and%20Yaqian%20Ning%20and%20Tong%20Zhang%20and%20Yin%20Zhuang%20and%20He%20Chen%20and%20Jun%20Li%20and%20Xuerui%20Mao&entry.1292438233=%20%20Recent%20advances%20in%20the%20visual-language%20area%20have%20developed%20natural%0Amulti-modal%20large%20language%20models%20%28MLLMs%29%20for%20spatial%20reasoning%20through%20visual%0Aprompting.%20However%2C%20due%20to%20remote%20sensing%20%28RS%29%20imagery%20containing%20abundant%0Ageospatial%20information%20that%20differs%20from%20natural%20images%2C%20it%20is%20challenging%20to%0Aeffectively%20adapt%20natural%20spatial%20models%20to%20the%20RS%20domain.%20Moreover%2C%20current%20RS%0AMLLMs%20are%20limited%20in%20overly%20narrow%20interpretation%20levels%20and%20interaction%0Amanner%2C%20hindering%20their%20applicability%20in%20real-world%20scenarios.%20To%20address%20those%0Achallenges%2C%20a%20spatial%20MLLM%20named%20EarthGPT-X%20is%20proposed%2C%20enabling%20a%0Acomprehensive%20understanding%20of%20multi-source%20RS%20imagery%2C%20such%20as%20optical%2C%0Asynthetic%20aperture%20radar%20%28SAR%29%2C%20and%20infrared.%20EarthGPT-X%20offers%20zoom-in%20and%0Azoom-out%20insight%2C%20and%20possesses%20flexible%20multi-grained%20interactive%20abilities.%0AMoreover%2C%20EarthGPT-X%20unifies%20two%20types%20of%20critical%20spatial%20tasks%20%28i.e.%2C%0Areferring%20and%20grounding%29%20into%20a%20visual%20prompting%20framework.%20To%20achieve%20these%0Aversatile%20capabilities%2C%20several%20key%20strategies%20are%20developed.%20The%20first%20is%20the%0Amulti-modal%20content%20integration%20method%2C%20which%20enhances%20the%20interplay%20between%0Aimages%2C%20visual%20prompts%2C%20and%20text%20instructions.%20Subsequently%2C%20a%20cross-domain%0Aone-stage%20fusion%20training%20strategy%20is%20proposed%2C%20utilizing%20the%20large%20language%0Amodel%20%28LLM%29%20as%20a%20unified%20interface%20for%20multi-source%20multi-task%20learning.%0AFurthermore%2C%20by%20incorporating%20a%20pixel%20perception%20module%2C%20the%20referring%20and%0Agrounding%20tasks%20are%20seamlessly%20unified%20within%20a%20single%20framework.%20In%20addition%2C%0Athe%20experiments%20conducted%20demonstrate%20the%20superiority%20of%20the%20proposed%0AEarthGPT-X%20in%20multi-grained%20tasks%20and%20its%20impressive%20flexibility%20in%20multi-modal%0Ainteraction%2C%20revealing%20significant%20advancements%20of%20MLLM%20in%20the%20RS%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12795v1&entry.124074799=Read"},
{"title": "CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography", "author": "I-Sheng Fang and Jun-Cheng Chen", "abstract": "  Large language models (LLMs) and multimodal large language models (MLLMs)\nhave significantly advanced artificial intelligence. However, visual reasoning,\nreasoning involving both visual and textual inputs, remains underexplored.\nRecent advancements, including the reasoning models like OpenAI o1 and Gemini\n2.0 Flash Thinking, which incorporate image inputs, have opened this\ncapability. In this ongoing work, we focus specifically on photography-related\ntasks because a photo is a visual snapshot of the physical world where the\nunderlying physics (i.e., illumination, blur extent, etc.) interplay with the\ncamera parameters. Successfully reasoning from the visual information of a\nphoto to identify these numerical camera settings requires the MLLMs to have a\ndeeper understanding of the underlying physics for precise visual\ncomprehension, representing a challenging and intelligent capability essential\nfor practical applications like photography assistant agents. We aim to\nevaluate MLLMs on their ability to distinguish visual differences related to\nnumerical camera settings, extending a methodology previously proposed for\nvision-language models (VLMs). Our preliminary results demonstrate the\nimportance of visual reasoning in photography-related tasks. Moreover, these\nresults show that no single MLLM consistently dominates across all evaluation\ntasks, demonstrating ongoing challenges and opportunities in developing MLLMs\nwith better visual reasoning.\n", "link": "http://arxiv.org/abs/2504.10090v2", "date": "2025-04-17", "relevancy": 2.943, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6063}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6063}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CameraBench%3A%20Benchmarking%20Visual%20Reasoning%20in%20MLLMs%20via%20Photography&body=Title%3A%20CameraBench%3A%20Benchmarking%20Visual%20Reasoning%20in%20MLLMs%20via%20Photography%0AAuthor%3A%20I-Sheng%20Fang%20and%20Jun-Cheng%20Chen%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20and%20multimodal%20large%20language%20models%20%28MLLMs%29%0Ahave%20significantly%20advanced%20artificial%20intelligence.%20However%2C%20visual%20reasoning%2C%0Areasoning%20involving%20both%20visual%20and%20textual%20inputs%2C%20remains%20underexplored.%0ARecent%20advancements%2C%20including%20the%20reasoning%20models%20like%20OpenAI%20o1%20and%20Gemini%0A2.0%20Flash%20Thinking%2C%20which%20incorporate%20image%20inputs%2C%20have%20opened%20this%0Acapability.%20In%20this%20ongoing%20work%2C%20we%20focus%20specifically%20on%20photography-related%0Atasks%20because%20a%20photo%20is%20a%20visual%20snapshot%20of%20the%20physical%20world%20where%20the%0Aunderlying%20physics%20%28i.e.%2C%20illumination%2C%20blur%20extent%2C%20etc.%29%20interplay%20with%20the%0Acamera%20parameters.%20Successfully%20reasoning%20from%20the%20visual%20information%20of%20a%0Aphoto%20to%20identify%20these%20numerical%20camera%20settings%20requires%20the%20MLLMs%20to%20have%20a%0Adeeper%20understanding%20of%20the%20underlying%20physics%20for%20precise%20visual%0Acomprehension%2C%20representing%20a%20challenging%20and%20intelligent%20capability%20essential%0Afor%20practical%20applications%20like%20photography%20assistant%20agents.%20We%20aim%20to%0Aevaluate%20MLLMs%20on%20their%20ability%20to%20distinguish%20visual%20differences%20related%20to%0Anumerical%20camera%20settings%2C%20extending%20a%20methodology%20previously%20proposed%20for%0Avision-language%20models%20%28VLMs%29.%20Our%20preliminary%20results%20demonstrate%20the%0Aimportance%20of%20visual%20reasoning%20in%20photography-related%20tasks.%20Moreover%2C%20these%0Aresults%20show%20that%20no%20single%20MLLM%20consistently%20dominates%20across%20all%20evaluation%0Atasks%2C%20demonstrating%20ongoing%20challenges%20and%20opportunities%20in%20developing%20MLLMs%0Awith%20better%20visual%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.10090v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCameraBench%253A%2520Benchmarking%2520Visual%2520Reasoning%2520in%2520MLLMs%2520via%2520Photography%26entry.906535625%3DI-Sheng%2520Fang%2520and%2520Jun-Cheng%2520Chen%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520and%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%250Ahave%2520significantly%2520advanced%2520artificial%2520intelligence.%2520However%252C%2520visual%2520reasoning%252C%250Areasoning%2520involving%2520both%2520visual%2520and%2520textual%2520inputs%252C%2520remains%2520underexplored.%250ARecent%2520advancements%252C%2520including%2520the%2520reasoning%2520models%2520like%2520OpenAI%2520o1%2520and%2520Gemini%250A2.0%2520Flash%2520Thinking%252C%2520which%2520incorporate%2520image%2520inputs%252C%2520have%2520opened%2520this%250Acapability.%2520In%2520this%2520ongoing%2520work%252C%2520we%2520focus%2520specifically%2520on%2520photography-related%250Atasks%2520because%2520a%2520photo%2520is%2520a%2520visual%2520snapshot%2520of%2520the%2520physical%2520world%2520where%2520the%250Aunderlying%2520physics%2520%2528i.e.%252C%2520illumination%252C%2520blur%2520extent%252C%2520etc.%2529%2520interplay%2520with%2520the%250Acamera%2520parameters.%2520Successfully%2520reasoning%2520from%2520the%2520visual%2520information%2520of%2520a%250Aphoto%2520to%2520identify%2520these%2520numerical%2520camera%2520settings%2520requires%2520the%2520MLLMs%2520to%2520have%2520a%250Adeeper%2520understanding%2520of%2520the%2520underlying%2520physics%2520for%2520precise%2520visual%250Acomprehension%252C%2520representing%2520a%2520challenging%2520and%2520intelligent%2520capability%2520essential%250Afor%2520practical%2520applications%2520like%2520photography%2520assistant%2520agents.%2520We%2520aim%2520to%250Aevaluate%2520MLLMs%2520on%2520their%2520ability%2520to%2520distinguish%2520visual%2520differences%2520related%2520to%250Anumerical%2520camera%2520settings%252C%2520extending%2520a%2520methodology%2520previously%2520proposed%2520for%250Avision-language%2520models%2520%2528VLMs%2529.%2520Our%2520preliminary%2520results%2520demonstrate%2520the%250Aimportance%2520of%2520visual%2520reasoning%2520in%2520photography-related%2520tasks.%2520Moreover%252C%2520these%250Aresults%2520show%2520that%2520no%2520single%2520MLLM%2520consistently%2520dominates%2520across%2520all%2520evaluation%250Atasks%252C%2520demonstrating%2520ongoing%2520challenges%2520and%2520opportunities%2520in%2520developing%2520MLLMs%250Awith%2520better%2520visual%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.10090v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CameraBench%3A%20Benchmarking%20Visual%20Reasoning%20in%20MLLMs%20via%20Photography&entry.906535625=I-Sheng%20Fang%20and%20Jun-Cheng%20Chen&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20and%20multimodal%20large%20language%20models%20%28MLLMs%29%0Ahave%20significantly%20advanced%20artificial%20intelligence.%20However%2C%20visual%20reasoning%2C%0Areasoning%20involving%20both%20visual%20and%20textual%20inputs%2C%20remains%20underexplored.%0ARecent%20advancements%2C%20including%20the%20reasoning%20models%20like%20OpenAI%20o1%20and%20Gemini%0A2.0%20Flash%20Thinking%2C%20which%20incorporate%20image%20inputs%2C%20have%20opened%20this%0Acapability.%20In%20this%20ongoing%20work%2C%20we%20focus%20specifically%20on%20photography-related%0Atasks%20because%20a%20photo%20is%20a%20visual%20snapshot%20of%20the%20physical%20world%20where%20the%0Aunderlying%20physics%20%28i.e.%2C%20illumination%2C%20blur%20extent%2C%20etc.%29%20interplay%20with%20the%0Acamera%20parameters.%20Successfully%20reasoning%20from%20the%20visual%20information%20of%20a%0Aphoto%20to%20identify%20these%20numerical%20camera%20settings%20requires%20the%20MLLMs%20to%20have%20a%0Adeeper%20understanding%20of%20the%20underlying%20physics%20for%20precise%20visual%0Acomprehension%2C%20representing%20a%20challenging%20and%20intelligent%20capability%20essential%0Afor%20practical%20applications%20like%20photography%20assistant%20agents.%20We%20aim%20to%0Aevaluate%20MLLMs%20on%20their%20ability%20to%20distinguish%20visual%20differences%20related%20to%0Anumerical%20camera%20settings%2C%20extending%20a%20methodology%20previously%20proposed%20for%0Avision-language%20models%20%28VLMs%29.%20Our%20preliminary%20results%20demonstrate%20the%0Aimportance%20of%20visual%20reasoning%20in%20photography-related%20tasks.%20Moreover%2C%20these%0Aresults%20show%20that%20no%20single%20MLLM%20consistently%20dominates%20across%20all%20evaluation%0Atasks%2C%20demonstrating%20ongoing%20challenges%20and%20opportunities%20in%20developing%20MLLMs%0Awith%20better%20visual%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.10090v2&entry.124074799=Read"},
{"title": "IMAGGarment-1: Fine-Grained Garment Generation for Controllable Fashion\n  Design", "author": "Fei Shen and Jian Yu and Cong Wang and Xin Jiang and Xiaoyu Du and Jinhui Tang", "abstract": "  This paper presents IMAGGarment-1, a fine-grained garment generation (FGG)\nframework that enables high-fidelity garment synthesis with precise control\nover silhouette, color, and logo placement. Unlike existing methods that are\nlimited to single-condition inputs, IMAGGarment-1 addresses the challenges of\nmulti-conditional controllability in personalized fashion design and digital\napparel applications. Specifically, IMAGGarment-1 employs a two-stage training\nstrategy to separately model global appearance and local details, while\nenabling unified and controllable generation through end-to-end inference. In\nthe first stage, we propose a global appearance model that jointly encodes\nsilhouette and color using a mixed attention module and a color adapter. In the\nsecond stage, we present a local enhancement model with an adaptive\nappearance-aware module to inject user-defined logos and spatial constraints,\nenabling accurate placement and visual consistency. To support this task, we\nrelease GarmentBench, a large-scale dataset comprising over 180K garment\nsamples paired with multi-level design conditions, including sketches, color\nreferences, logo placements, and textual prompts. Extensive experiments\ndemonstrate that our method outperforms existing baselines, achieving superior\nstructural stability, color fidelity, and local controllability performance.\nThe code and model are available at https://github.com/muzishen/IMAGGarment-1.\n", "link": "http://arxiv.org/abs/2504.13176v1", "date": "2025-04-17", "relevancy": 2.859, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.7498}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6989}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IMAGGarment-1%3A%20Fine-Grained%20Garment%20Generation%20for%20Controllable%20Fashion%0A%20%20Design&body=Title%3A%20IMAGGarment-1%3A%20Fine-Grained%20Garment%20Generation%20for%20Controllable%20Fashion%0A%20%20Design%0AAuthor%3A%20Fei%20Shen%20and%20Jian%20Yu%20and%20Cong%20Wang%20and%20Xin%20Jiang%20and%20Xiaoyu%20Du%20and%20Jinhui%20Tang%0AAbstract%3A%20%20%20This%20paper%20presents%20IMAGGarment-1%2C%20a%20fine-grained%20garment%20generation%20%28FGG%29%0Aframework%20that%20enables%20high-fidelity%20garment%20synthesis%20with%20precise%20control%0Aover%20silhouette%2C%20color%2C%20and%20logo%20placement.%20Unlike%20existing%20methods%20that%20are%0Alimited%20to%20single-condition%20inputs%2C%20IMAGGarment-1%20addresses%20the%20challenges%20of%0Amulti-conditional%20controllability%20in%20personalized%20fashion%20design%20and%20digital%0Aapparel%20applications.%20Specifically%2C%20IMAGGarment-1%20employs%20a%20two-stage%20training%0Astrategy%20to%20separately%20model%20global%20appearance%20and%20local%20details%2C%20while%0Aenabling%20unified%20and%20controllable%20generation%20through%20end-to-end%20inference.%20In%0Athe%20first%20stage%2C%20we%20propose%20a%20global%20appearance%20model%20that%20jointly%20encodes%0Asilhouette%20and%20color%20using%20a%20mixed%20attention%20module%20and%20a%20color%20adapter.%20In%20the%0Asecond%20stage%2C%20we%20present%20a%20local%20enhancement%20model%20with%20an%20adaptive%0Aappearance-aware%20module%20to%20inject%20user-defined%20logos%20and%20spatial%20constraints%2C%0Aenabling%20accurate%20placement%20and%20visual%20consistency.%20To%20support%20this%20task%2C%20we%0Arelease%20GarmentBench%2C%20a%20large-scale%20dataset%20comprising%20over%20180K%20garment%0Asamples%20paired%20with%20multi-level%20design%20conditions%2C%20including%20sketches%2C%20color%0Areferences%2C%20logo%20placements%2C%20and%20textual%20prompts.%20Extensive%20experiments%0Ademonstrate%20that%20our%20method%20outperforms%20existing%20baselines%2C%20achieving%20superior%0Astructural%20stability%2C%20color%20fidelity%2C%20and%20local%20controllability%20performance.%0AThe%20code%20and%20model%20are%20available%20at%20https%3A//github.com/muzishen/IMAGGarment-1.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13176v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIMAGGarment-1%253A%2520Fine-Grained%2520Garment%2520Generation%2520for%2520Controllable%2520Fashion%250A%2520%2520Design%26entry.906535625%3DFei%2520Shen%2520and%2520Jian%2520Yu%2520and%2520Cong%2520Wang%2520and%2520Xin%2520Jiang%2520and%2520Xiaoyu%2520Du%2520and%2520Jinhui%2520Tang%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520IMAGGarment-1%252C%2520a%2520fine-grained%2520garment%2520generation%2520%2528FGG%2529%250Aframework%2520that%2520enables%2520high-fidelity%2520garment%2520synthesis%2520with%2520precise%2520control%250Aover%2520silhouette%252C%2520color%252C%2520and%2520logo%2520placement.%2520Unlike%2520existing%2520methods%2520that%2520are%250Alimited%2520to%2520single-condition%2520inputs%252C%2520IMAGGarment-1%2520addresses%2520the%2520challenges%2520of%250Amulti-conditional%2520controllability%2520in%2520personalized%2520fashion%2520design%2520and%2520digital%250Aapparel%2520applications.%2520Specifically%252C%2520IMAGGarment-1%2520employs%2520a%2520two-stage%2520training%250Astrategy%2520to%2520separately%2520model%2520global%2520appearance%2520and%2520local%2520details%252C%2520while%250Aenabling%2520unified%2520and%2520controllable%2520generation%2520through%2520end-to-end%2520inference.%2520In%250Athe%2520first%2520stage%252C%2520we%2520propose%2520a%2520global%2520appearance%2520model%2520that%2520jointly%2520encodes%250Asilhouette%2520and%2520color%2520using%2520a%2520mixed%2520attention%2520module%2520and%2520a%2520color%2520adapter.%2520In%2520the%250Asecond%2520stage%252C%2520we%2520present%2520a%2520local%2520enhancement%2520model%2520with%2520an%2520adaptive%250Aappearance-aware%2520module%2520to%2520inject%2520user-defined%2520logos%2520and%2520spatial%2520constraints%252C%250Aenabling%2520accurate%2520placement%2520and%2520visual%2520consistency.%2520To%2520support%2520this%2520task%252C%2520we%250Arelease%2520GarmentBench%252C%2520a%2520large-scale%2520dataset%2520comprising%2520over%2520180K%2520garment%250Asamples%2520paired%2520with%2520multi-level%2520design%2520conditions%252C%2520including%2520sketches%252C%2520color%250Areferences%252C%2520logo%2520placements%252C%2520and%2520textual%2520prompts.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520our%2520method%2520outperforms%2520existing%2520baselines%252C%2520achieving%2520superior%250Astructural%2520stability%252C%2520color%2520fidelity%252C%2520and%2520local%2520controllability%2520performance.%250AThe%2520code%2520and%2520model%2520are%2520available%2520at%2520https%253A//github.com/muzishen/IMAGGarment-1.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13176v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IMAGGarment-1%3A%20Fine-Grained%20Garment%20Generation%20for%20Controllable%20Fashion%0A%20%20Design&entry.906535625=Fei%20Shen%20and%20Jian%20Yu%20and%20Cong%20Wang%20and%20Xin%20Jiang%20and%20Xiaoyu%20Du%20and%20Jinhui%20Tang&entry.1292438233=%20%20This%20paper%20presents%20IMAGGarment-1%2C%20a%20fine-grained%20garment%20generation%20%28FGG%29%0Aframework%20that%20enables%20high-fidelity%20garment%20synthesis%20with%20precise%20control%0Aover%20silhouette%2C%20color%2C%20and%20logo%20placement.%20Unlike%20existing%20methods%20that%20are%0Alimited%20to%20single-condition%20inputs%2C%20IMAGGarment-1%20addresses%20the%20challenges%20of%0Amulti-conditional%20controllability%20in%20personalized%20fashion%20design%20and%20digital%0Aapparel%20applications.%20Specifically%2C%20IMAGGarment-1%20employs%20a%20two-stage%20training%0Astrategy%20to%20separately%20model%20global%20appearance%20and%20local%20details%2C%20while%0Aenabling%20unified%20and%20controllable%20generation%20through%20end-to-end%20inference.%20In%0Athe%20first%20stage%2C%20we%20propose%20a%20global%20appearance%20model%20that%20jointly%20encodes%0Asilhouette%20and%20color%20using%20a%20mixed%20attention%20module%20and%20a%20color%20adapter.%20In%20the%0Asecond%20stage%2C%20we%20present%20a%20local%20enhancement%20model%20with%20an%20adaptive%0Aappearance-aware%20module%20to%20inject%20user-defined%20logos%20and%20spatial%20constraints%2C%0Aenabling%20accurate%20placement%20and%20visual%20consistency.%20To%20support%20this%20task%2C%20we%0Arelease%20GarmentBench%2C%20a%20large-scale%20dataset%20comprising%20over%20180K%20garment%0Asamples%20paired%20with%20multi-level%20design%20conditions%2C%20including%20sketches%2C%20color%0Areferences%2C%20logo%20placements%2C%20and%20textual%20prompts.%20Extensive%20experiments%0Ademonstrate%20that%20our%20method%20outperforms%20existing%20baselines%2C%20achieving%20superior%0Astructural%20stability%2C%20color%20fidelity%2C%20and%20local%20controllability%20performance.%0AThe%20code%20and%20model%20are%20available%20at%20https%3A//github.com/muzishen/IMAGGarment-1.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13176v1&entry.124074799=Read"},
{"title": "Vision and Language Integration for Domain Generalization", "author": "Yanmei Wang and Xiyao Liu and Fupeng Chu and Zhi Han", "abstract": "  Domain generalization aims at training on source domains to uncover a\ndomain-invariant feature space, allowing the model to perform robust\ngeneralization ability on unknown target domains. However, due to domain gaps,\nit is hard to find reliable common image feature space, and the reason for that\nis the lack of suitable basic units for images. Different from image in vision\nspace, language has comprehensive expression elements that can effectively\nconvey semantics. Inspired by the semantic completeness of language and\nintuitiveness of image, we propose VLCA, which combine language space and\nvision space, and connect the multiple image domains by using semantic space as\nthe bridge domain. Specifically, in language space, by taking advantage of the\ncompleteness of language basic units, we tend to capture the semantic\nrepresentation of the relations between categories through word vector\ndistance. Then, in vision space, by taking advantage of the intuitiveness of\nimage features, the common pattern of sample features with the same class is\nexplored through low-rank approximation. In the end, the language\nrepresentation is aligned with the vision representation through the multimodal\nspace of text and image. Experiments demonstrate the effectiveness of the\nproposed method.\n", "link": "http://arxiv.org/abs/2504.12966v1", "date": "2025-04-17", "relevancy": 2.8437, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5782}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5782}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20and%20Language%20Integration%20for%20Domain%20Generalization&body=Title%3A%20Vision%20and%20Language%20Integration%20for%20Domain%20Generalization%0AAuthor%3A%20Yanmei%20Wang%20and%20Xiyao%20Liu%20and%20Fupeng%20Chu%20and%20Zhi%20Han%0AAbstract%3A%20%20%20Domain%20generalization%20aims%20at%20training%20on%20source%20domains%20to%20uncover%20a%0Adomain-invariant%20feature%20space%2C%20allowing%20the%20model%20to%20perform%20robust%0Ageneralization%20ability%20on%20unknown%20target%20domains.%20However%2C%20due%20to%20domain%20gaps%2C%0Ait%20is%20hard%20to%20find%20reliable%20common%20image%20feature%20space%2C%20and%20the%20reason%20for%20that%0Ais%20the%20lack%20of%20suitable%20basic%20units%20for%20images.%20Different%20from%20image%20in%20vision%0Aspace%2C%20language%20has%20comprehensive%20expression%20elements%20that%20can%20effectively%0Aconvey%20semantics.%20Inspired%20by%20the%20semantic%20completeness%20of%20language%20and%0Aintuitiveness%20of%20image%2C%20we%20propose%20VLCA%2C%20which%20combine%20language%20space%20and%0Avision%20space%2C%20and%20connect%20the%20multiple%20image%20domains%20by%20using%20semantic%20space%20as%0Athe%20bridge%20domain.%20Specifically%2C%20in%20language%20space%2C%20by%20taking%20advantage%20of%20the%0Acompleteness%20of%20language%20basic%20units%2C%20we%20tend%20to%20capture%20the%20semantic%0Arepresentation%20of%20the%20relations%20between%20categories%20through%20word%20vector%0Adistance.%20Then%2C%20in%20vision%20space%2C%20by%20taking%20advantage%20of%20the%20intuitiveness%20of%0Aimage%20features%2C%20the%20common%20pattern%20of%20sample%20features%20with%20the%20same%20class%20is%0Aexplored%20through%20low-rank%20approximation.%20In%20the%20end%2C%20the%20language%0Arepresentation%20is%20aligned%20with%20the%20vision%20representation%20through%20the%20multimodal%0Aspace%20of%20text%20and%20image.%20Experiments%20demonstrate%20the%20effectiveness%20of%20the%0Aproposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12966v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520and%2520Language%2520Integration%2520for%2520Domain%2520Generalization%26entry.906535625%3DYanmei%2520Wang%2520and%2520Xiyao%2520Liu%2520and%2520Fupeng%2520Chu%2520and%2520Zhi%2520Han%26entry.1292438233%3D%2520%2520Domain%2520generalization%2520aims%2520at%2520training%2520on%2520source%2520domains%2520to%2520uncover%2520a%250Adomain-invariant%2520feature%2520space%252C%2520allowing%2520the%2520model%2520to%2520perform%2520robust%250Ageneralization%2520ability%2520on%2520unknown%2520target%2520domains.%2520However%252C%2520due%2520to%2520domain%2520gaps%252C%250Ait%2520is%2520hard%2520to%2520find%2520reliable%2520common%2520image%2520feature%2520space%252C%2520and%2520the%2520reason%2520for%2520that%250Ais%2520the%2520lack%2520of%2520suitable%2520basic%2520units%2520for%2520images.%2520Different%2520from%2520image%2520in%2520vision%250Aspace%252C%2520language%2520has%2520comprehensive%2520expression%2520elements%2520that%2520can%2520effectively%250Aconvey%2520semantics.%2520Inspired%2520by%2520the%2520semantic%2520completeness%2520of%2520language%2520and%250Aintuitiveness%2520of%2520image%252C%2520we%2520propose%2520VLCA%252C%2520which%2520combine%2520language%2520space%2520and%250Avision%2520space%252C%2520and%2520connect%2520the%2520multiple%2520image%2520domains%2520by%2520using%2520semantic%2520space%2520as%250Athe%2520bridge%2520domain.%2520Specifically%252C%2520in%2520language%2520space%252C%2520by%2520taking%2520advantage%2520of%2520the%250Acompleteness%2520of%2520language%2520basic%2520units%252C%2520we%2520tend%2520to%2520capture%2520the%2520semantic%250Arepresentation%2520of%2520the%2520relations%2520between%2520categories%2520through%2520word%2520vector%250Adistance.%2520Then%252C%2520in%2520vision%2520space%252C%2520by%2520taking%2520advantage%2520of%2520the%2520intuitiveness%2520of%250Aimage%2520features%252C%2520the%2520common%2520pattern%2520of%2520sample%2520features%2520with%2520the%2520same%2520class%2520is%250Aexplored%2520through%2520low-rank%2520approximation.%2520In%2520the%2520end%252C%2520the%2520language%250Arepresentation%2520is%2520aligned%2520with%2520the%2520vision%2520representation%2520through%2520the%2520multimodal%250Aspace%2520of%2520text%2520and%2520image.%2520Experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12966v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20and%20Language%20Integration%20for%20Domain%20Generalization&entry.906535625=Yanmei%20Wang%20and%20Xiyao%20Liu%20and%20Fupeng%20Chu%20and%20Zhi%20Han&entry.1292438233=%20%20Domain%20generalization%20aims%20at%20training%20on%20source%20domains%20to%20uncover%20a%0Adomain-invariant%20feature%20space%2C%20allowing%20the%20model%20to%20perform%20robust%0Ageneralization%20ability%20on%20unknown%20target%20domains.%20However%2C%20due%20to%20domain%20gaps%2C%0Ait%20is%20hard%20to%20find%20reliable%20common%20image%20feature%20space%2C%20and%20the%20reason%20for%20that%0Ais%20the%20lack%20of%20suitable%20basic%20units%20for%20images.%20Different%20from%20image%20in%20vision%0Aspace%2C%20language%20has%20comprehensive%20expression%20elements%20that%20can%20effectively%0Aconvey%20semantics.%20Inspired%20by%20the%20semantic%20completeness%20of%20language%20and%0Aintuitiveness%20of%20image%2C%20we%20propose%20VLCA%2C%20which%20combine%20language%20space%20and%0Avision%20space%2C%20and%20connect%20the%20multiple%20image%20domains%20by%20using%20semantic%20space%20as%0Athe%20bridge%20domain.%20Specifically%2C%20in%20language%20space%2C%20by%20taking%20advantage%20of%20the%0Acompleteness%20of%20language%20basic%20units%2C%20we%20tend%20to%20capture%20the%20semantic%0Arepresentation%20of%20the%20relations%20between%20categories%20through%20word%20vector%0Adistance.%20Then%2C%20in%20vision%20space%2C%20by%20taking%20advantage%20of%20the%20intuitiveness%20of%0Aimage%20features%2C%20the%20common%20pattern%20of%20sample%20features%20with%20the%20same%20class%20is%0Aexplored%20through%20low-rank%20approximation.%20In%20the%20end%2C%20the%20language%0Arepresentation%20is%20aligned%20with%20the%20vision%20representation%20through%20the%20multimodal%0Aspace%20of%20text%20and%20image.%20Experiments%20demonstrate%20the%20effectiveness%20of%20the%0Aproposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12966v1&entry.124074799=Read"},
{"title": "Towards Cardiac MRI Foundation Models: Comprehensive Visual-Tabular\n  Representations for Whole-Heart Assessment and Beyond", "author": "Yundi Zhang and Paul Hager and Che Liu and Suprosanna Shit and Chen Chen and Daniel Rueckert and Jiazhen Pan", "abstract": "  Cardiac magnetic resonance imaging is the gold standard for non-invasive\ncardiac assessment, offering rich spatio-temporal views of the cardiac anatomy\nand physiology. Patient-level health factors, such as demographics, metabolic,\nand lifestyle, are known to substantially influence cardiovascular health and\ndisease risk, yet remain uncaptured by CMR alone. To holistically understand\ncardiac health and to enable the best possible interpretation of an\nindividual's disease risk, CMR and patient-level factors must be jointly\nexploited within an integrated framework. Recent multi-modal approaches have\nbegun to bridge this gap, yet they often rely on limited spatio-temporal data\nand focus on isolated clinical tasks, thereby hindering the development of a\ncomprehensive representation for cardiac health evaluation. To overcome these\nlimitations, we introduce ViTa, a step toward foundation models that delivers a\ncomprehensive representation of the heart and a precise interpretation of\nindividual disease risk. Leveraging data from 42,000 UK Biobank participants,\nViTa integrates 3D+T cine stacks from short-axis and long-axis views, enabling\na complete capture of the cardiac cycle. These imaging data are then fused with\ndetailed tabular patient-level factors, enabling context-aware insights. This\nmulti-modal paradigm supports a wide spectrum of downstream tasks, including\ncardiac phenotype and physiological feature prediction, segmentation, and\nclassification of cardiac and metabolic diseases within a single unified\nframework. By learning a shared latent representation that bridges rich imaging\nfeatures and patient context, ViTa moves beyond traditional, task-specific\nmodels toward a universal, patient-specific understanding of cardiac health,\nhighlighting its potential to advance clinical utility and scalability in\ncardiac analysis.\n", "link": "http://arxiv.org/abs/2504.13037v1", "date": "2025-04-17", "relevancy": 2.8223, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5701}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5701}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Cardiac%20MRI%20Foundation%20Models%3A%20Comprehensive%20Visual-Tabular%0A%20%20Representations%20for%20Whole-Heart%20Assessment%20and%20Beyond&body=Title%3A%20Towards%20Cardiac%20MRI%20Foundation%20Models%3A%20Comprehensive%20Visual-Tabular%0A%20%20Representations%20for%20Whole-Heart%20Assessment%20and%20Beyond%0AAuthor%3A%20Yundi%20Zhang%20and%20Paul%20Hager%20and%20Che%20Liu%20and%20Suprosanna%20Shit%20and%20Chen%20Chen%20and%20Daniel%20Rueckert%20and%20Jiazhen%20Pan%0AAbstract%3A%20%20%20Cardiac%20magnetic%20resonance%20imaging%20is%20the%20gold%20standard%20for%20non-invasive%0Acardiac%20assessment%2C%20offering%20rich%20spatio-temporal%20views%20of%20the%20cardiac%20anatomy%0Aand%20physiology.%20Patient-level%20health%20factors%2C%20such%20as%20demographics%2C%20metabolic%2C%0Aand%20lifestyle%2C%20are%20known%20to%20substantially%20influence%20cardiovascular%20health%20and%0Adisease%20risk%2C%20yet%20remain%20uncaptured%20by%20CMR%20alone.%20To%20holistically%20understand%0Acardiac%20health%20and%20to%20enable%20the%20best%20possible%20interpretation%20of%20an%0Aindividual%27s%20disease%20risk%2C%20CMR%20and%20patient-level%20factors%20must%20be%20jointly%0Aexploited%20within%20an%20integrated%20framework.%20Recent%20multi-modal%20approaches%20have%0Abegun%20to%20bridge%20this%20gap%2C%20yet%20they%20often%20rely%20on%20limited%20spatio-temporal%20data%0Aand%20focus%20on%20isolated%20clinical%20tasks%2C%20thereby%20hindering%20the%20development%20of%20a%0Acomprehensive%20representation%20for%20cardiac%20health%20evaluation.%20To%20overcome%20these%0Alimitations%2C%20we%20introduce%20ViTa%2C%20a%20step%20toward%20foundation%20models%20that%20delivers%20a%0Acomprehensive%20representation%20of%20the%20heart%20and%20a%20precise%20interpretation%20of%0Aindividual%20disease%20risk.%20Leveraging%20data%20from%2042%2C000%20UK%20Biobank%20participants%2C%0AViTa%20integrates%203D%2BT%20cine%20stacks%20from%20short-axis%20and%20long-axis%20views%2C%20enabling%0Aa%20complete%20capture%20of%20the%20cardiac%20cycle.%20These%20imaging%20data%20are%20then%20fused%20with%0Adetailed%20tabular%20patient-level%20factors%2C%20enabling%20context-aware%20insights.%20This%0Amulti-modal%20paradigm%20supports%20a%20wide%20spectrum%20of%20downstream%20tasks%2C%20including%0Acardiac%20phenotype%20and%20physiological%20feature%20prediction%2C%20segmentation%2C%20and%0Aclassification%20of%20cardiac%20and%20metabolic%20diseases%20within%20a%20single%20unified%0Aframework.%20By%20learning%20a%20shared%20latent%20representation%20that%20bridges%20rich%20imaging%0Afeatures%20and%20patient%20context%2C%20ViTa%20moves%20beyond%20traditional%2C%20task-specific%0Amodels%20toward%20a%20universal%2C%20patient-specific%20understanding%20of%20cardiac%20health%2C%0Ahighlighting%20its%20potential%20to%20advance%20clinical%20utility%20and%20scalability%20in%0Acardiac%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13037v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Cardiac%2520MRI%2520Foundation%2520Models%253A%2520Comprehensive%2520Visual-Tabular%250A%2520%2520Representations%2520for%2520Whole-Heart%2520Assessment%2520and%2520Beyond%26entry.906535625%3DYundi%2520Zhang%2520and%2520Paul%2520Hager%2520and%2520Che%2520Liu%2520and%2520Suprosanna%2520Shit%2520and%2520Chen%2520Chen%2520and%2520Daniel%2520Rueckert%2520and%2520Jiazhen%2520Pan%26entry.1292438233%3D%2520%2520Cardiac%2520magnetic%2520resonance%2520imaging%2520is%2520the%2520gold%2520standard%2520for%2520non-invasive%250Acardiac%2520assessment%252C%2520offering%2520rich%2520spatio-temporal%2520views%2520of%2520the%2520cardiac%2520anatomy%250Aand%2520physiology.%2520Patient-level%2520health%2520factors%252C%2520such%2520as%2520demographics%252C%2520metabolic%252C%250Aand%2520lifestyle%252C%2520are%2520known%2520to%2520substantially%2520influence%2520cardiovascular%2520health%2520and%250Adisease%2520risk%252C%2520yet%2520remain%2520uncaptured%2520by%2520CMR%2520alone.%2520To%2520holistically%2520understand%250Acardiac%2520health%2520and%2520to%2520enable%2520the%2520best%2520possible%2520interpretation%2520of%2520an%250Aindividual%2527s%2520disease%2520risk%252C%2520CMR%2520and%2520patient-level%2520factors%2520must%2520be%2520jointly%250Aexploited%2520within%2520an%2520integrated%2520framework.%2520Recent%2520multi-modal%2520approaches%2520have%250Abegun%2520to%2520bridge%2520this%2520gap%252C%2520yet%2520they%2520often%2520rely%2520on%2520limited%2520spatio-temporal%2520data%250Aand%2520focus%2520on%2520isolated%2520clinical%2520tasks%252C%2520thereby%2520hindering%2520the%2520development%2520of%2520a%250Acomprehensive%2520representation%2520for%2520cardiac%2520health%2520evaluation.%2520To%2520overcome%2520these%250Alimitations%252C%2520we%2520introduce%2520ViTa%252C%2520a%2520step%2520toward%2520foundation%2520models%2520that%2520delivers%2520a%250Acomprehensive%2520representation%2520of%2520the%2520heart%2520and%2520a%2520precise%2520interpretation%2520of%250Aindividual%2520disease%2520risk.%2520Leveraging%2520data%2520from%252042%252C000%2520UK%2520Biobank%2520participants%252C%250AViTa%2520integrates%25203D%252BT%2520cine%2520stacks%2520from%2520short-axis%2520and%2520long-axis%2520views%252C%2520enabling%250Aa%2520complete%2520capture%2520of%2520the%2520cardiac%2520cycle.%2520These%2520imaging%2520data%2520are%2520then%2520fused%2520with%250Adetailed%2520tabular%2520patient-level%2520factors%252C%2520enabling%2520context-aware%2520insights.%2520This%250Amulti-modal%2520paradigm%2520supports%2520a%2520wide%2520spectrum%2520of%2520downstream%2520tasks%252C%2520including%250Acardiac%2520phenotype%2520and%2520physiological%2520feature%2520prediction%252C%2520segmentation%252C%2520and%250Aclassification%2520of%2520cardiac%2520and%2520metabolic%2520diseases%2520within%2520a%2520single%2520unified%250Aframework.%2520By%2520learning%2520a%2520shared%2520latent%2520representation%2520that%2520bridges%2520rich%2520imaging%250Afeatures%2520and%2520patient%2520context%252C%2520ViTa%2520moves%2520beyond%2520traditional%252C%2520task-specific%250Amodels%2520toward%2520a%2520universal%252C%2520patient-specific%2520understanding%2520of%2520cardiac%2520health%252C%250Ahighlighting%2520its%2520potential%2520to%2520advance%2520clinical%2520utility%2520and%2520scalability%2520in%250Acardiac%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13037v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Cardiac%20MRI%20Foundation%20Models%3A%20Comprehensive%20Visual-Tabular%0A%20%20Representations%20for%20Whole-Heart%20Assessment%20and%20Beyond&entry.906535625=Yundi%20Zhang%20and%20Paul%20Hager%20and%20Che%20Liu%20and%20Suprosanna%20Shit%20and%20Chen%20Chen%20and%20Daniel%20Rueckert%20and%20Jiazhen%20Pan&entry.1292438233=%20%20Cardiac%20magnetic%20resonance%20imaging%20is%20the%20gold%20standard%20for%20non-invasive%0Acardiac%20assessment%2C%20offering%20rich%20spatio-temporal%20views%20of%20the%20cardiac%20anatomy%0Aand%20physiology.%20Patient-level%20health%20factors%2C%20such%20as%20demographics%2C%20metabolic%2C%0Aand%20lifestyle%2C%20are%20known%20to%20substantially%20influence%20cardiovascular%20health%20and%0Adisease%20risk%2C%20yet%20remain%20uncaptured%20by%20CMR%20alone.%20To%20holistically%20understand%0Acardiac%20health%20and%20to%20enable%20the%20best%20possible%20interpretation%20of%20an%0Aindividual%27s%20disease%20risk%2C%20CMR%20and%20patient-level%20factors%20must%20be%20jointly%0Aexploited%20within%20an%20integrated%20framework.%20Recent%20multi-modal%20approaches%20have%0Abegun%20to%20bridge%20this%20gap%2C%20yet%20they%20often%20rely%20on%20limited%20spatio-temporal%20data%0Aand%20focus%20on%20isolated%20clinical%20tasks%2C%20thereby%20hindering%20the%20development%20of%20a%0Acomprehensive%20representation%20for%20cardiac%20health%20evaluation.%20To%20overcome%20these%0Alimitations%2C%20we%20introduce%20ViTa%2C%20a%20step%20toward%20foundation%20models%20that%20delivers%20a%0Acomprehensive%20representation%20of%20the%20heart%20and%20a%20precise%20interpretation%20of%0Aindividual%20disease%20risk.%20Leveraging%20data%20from%2042%2C000%20UK%20Biobank%20participants%2C%0AViTa%20integrates%203D%2BT%20cine%20stacks%20from%20short-axis%20and%20long-axis%20views%2C%20enabling%0Aa%20complete%20capture%20of%20the%20cardiac%20cycle.%20These%20imaging%20data%20are%20then%20fused%20with%0Adetailed%20tabular%20patient-level%20factors%2C%20enabling%20context-aware%20insights.%20This%0Amulti-modal%20paradigm%20supports%20a%20wide%20spectrum%20of%20downstream%20tasks%2C%20including%0Acardiac%20phenotype%20and%20physiological%20feature%20prediction%2C%20segmentation%2C%20and%0Aclassification%20of%20cardiac%20and%20metabolic%20diseases%20within%20a%20single%20unified%0Aframework.%20By%20learning%20a%20shared%20latent%20representation%20that%20bridges%20rich%20imaging%0Afeatures%20and%20patient%20context%2C%20ViTa%20moves%20beyond%20traditional%2C%20task-specific%0Amodels%20toward%20a%20universal%2C%20patient-specific%20understanding%20of%20cardiac%20health%2C%0Ahighlighting%20its%20potential%20to%20advance%20clinical%20utility%20and%20scalability%20in%0Acardiac%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13037v1&entry.124074799=Read"},
{"title": "Digital Twin Generation from Visual Data: A Survey", "author": "Andrew Melnik and Benjamin Alt and Giang Nguyen and Artur Wilkowski and Maciej Stefa\u0144czyk and Qirui Wu and Sinan Harms and Helge Rhodin and Manolis Savva and Michael Beetz", "abstract": "  This survey explores recent developments in generating digital twins from\nvideos. Such digital twins can be used for robotics application, media content\ncreation, or design and construction works. We analyze various approaches,\nincluding 3D Gaussian Splatting, generative in-painting, semantic segmentation,\nand foundation models highlighting their advantages and limitations.\nAdditionally, we discuss challenges such as occlusions, lighting variations,\nand scalability, as well as potential future research directions. This survey\naims to provide a comprehensive overview of state-of-the-art methodologies and\ntheir implications for real-world applications. Awesome list:\nhttps://github.com/ndrwmlnk/awesome-digital-twins\n", "link": "http://arxiv.org/abs/2504.13159v1", "date": "2025-04-17", "relevancy": 2.8113, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5699}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5699}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Digital%20Twin%20Generation%20from%20Visual%20Data%3A%20A%20Survey&body=Title%3A%20Digital%20Twin%20Generation%20from%20Visual%20Data%3A%20A%20Survey%0AAuthor%3A%20Andrew%20Melnik%20and%20Benjamin%20Alt%20and%20Giang%20Nguyen%20and%20Artur%20Wilkowski%20and%20Maciej%20Stefa%C5%84czyk%20and%20Qirui%20Wu%20and%20Sinan%20Harms%20and%20Helge%20Rhodin%20and%20Manolis%20Savva%20and%20Michael%20Beetz%0AAbstract%3A%20%20%20This%20survey%20explores%20recent%20developments%20in%20generating%20digital%20twins%20from%0Avideos.%20Such%20digital%20twins%20can%20be%20used%20for%20robotics%20application%2C%20media%20content%0Acreation%2C%20or%20design%20and%20construction%20works.%20We%20analyze%20various%20approaches%2C%0Aincluding%203D%20Gaussian%20Splatting%2C%20generative%20in-painting%2C%20semantic%20segmentation%2C%0Aand%20foundation%20models%20highlighting%20their%20advantages%20and%20limitations.%0AAdditionally%2C%20we%20discuss%20challenges%20such%20as%20occlusions%2C%20lighting%20variations%2C%0Aand%20scalability%2C%20as%20well%20as%20potential%20future%20research%20directions.%20This%20survey%0Aaims%20to%20provide%20a%20comprehensive%20overview%20of%20state-of-the-art%20methodologies%20and%0Atheir%20implications%20for%20real-world%20applications.%20Awesome%20list%3A%0Ahttps%3A//github.com/ndrwmlnk/awesome-digital-twins%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13159v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDigital%2520Twin%2520Generation%2520from%2520Visual%2520Data%253A%2520A%2520Survey%26entry.906535625%3DAndrew%2520Melnik%2520and%2520Benjamin%2520Alt%2520and%2520Giang%2520Nguyen%2520and%2520Artur%2520Wilkowski%2520and%2520Maciej%2520Stefa%25C5%2584czyk%2520and%2520Qirui%2520Wu%2520and%2520Sinan%2520Harms%2520and%2520Helge%2520Rhodin%2520and%2520Manolis%2520Savva%2520and%2520Michael%2520Beetz%26entry.1292438233%3D%2520%2520This%2520survey%2520explores%2520recent%2520developments%2520in%2520generating%2520digital%2520twins%2520from%250Avideos.%2520Such%2520digital%2520twins%2520can%2520be%2520used%2520for%2520robotics%2520application%252C%2520media%2520content%250Acreation%252C%2520or%2520design%2520and%2520construction%2520works.%2520We%2520analyze%2520various%2520approaches%252C%250Aincluding%25203D%2520Gaussian%2520Splatting%252C%2520generative%2520in-painting%252C%2520semantic%2520segmentation%252C%250Aand%2520foundation%2520models%2520highlighting%2520their%2520advantages%2520and%2520limitations.%250AAdditionally%252C%2520we%2520discuss%2520challenges%2520such%2520as%2520occlusions%252C%2520lighting%2520variations%252C%250Aand%2520scalability%252C%2520as%2520well%2520as%2520potential%2520future%2520research%2520directions.%2520This%2520survey%250Aaims%2520to%2520provide%2520a%2520comprehensive%2520overview%2520of%2520state-of-the-art%2520methodologies%2520and%250Atheir%2520implications%2520for%2520real-world%2520applications.%2520Awesome%2520list%253A%250Ahttps%253A//github.com/ndrwmlnk/awesome-digital-twins%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13159v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Digital%20Twin%20Generation%20from%20Visual%20Data%3A%20A%20Survey&entry.906535625=Andrew%20Melnik%20and%20Benjamin%20Alt%20and%20Giang%20Nguyen%20and%20Artur%20Wilkowski%20and%20Maciej%20Stefa%C5%84czyk%20and%20Qirui%20Wu%20and%20Sinan%20Harms%20and%20Helge%20Rhodin%20and%20Manolis%20Savva%20and%20Michael%20Beetz&entry.1292438233=%20%20This%20survey%20explores%20recent%20developments%20in%20generating%20digital%20twins%20from%0Avideos.%20Such%20digital%20twins%20can%20be%20used%20for%20robotics%20application%2C%20media%20content%0Acreation%2C%20or%20design%20and%20construction%20works.%20We%20analyze%20various%20approaches%2C%0Aincluding%203D%20Gaussian%20Splatting%2C%20generative%20in-painting%2C%20semantic%20segmentation%2C%0Aand%20foundation%20models%20highlighting%20their%20advantages%20and%20limitations.%0AAdditionally%2C%20we%20discuss%20challenges%20such%20as%20occlusions%2C%20lighting%20variations%2C%0Aand%20scalability%2C%20as%20well%20as%20potential%20future%20research%20directions.%20This%20survey%0Aaims%20to%20provide%20a%20comprehensive%20overview%20of%20state-of-the-art%20methodologies%20and%0Atheir%20implications%20for%20real-world%20applications.%20Awesome%20list%3A%0Ahttps%3A//github.com/ndrwmlnk/awesome-digital-twins%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13159v1&entry.124074799=Read"},
{"title": "Benchmarking the Spatial Robustness of DNNs via Natural and Adversarial\n  Localized Corruptions", "author": "Giulia Marchiori Pietrosanti and Giulio Rossolini and Alessandro Biondi and Giorgio Buttazzo", "abstract": "  The robustness of DNNs is a crucial factor in safety-critical applications,\nparticularly in complex and dynamic environments where localized corruptions\ncan arise. While previous studies have evaluated the robustness of semantic\nsegmentation (SS) models under whole-image natural or adversarial corruptions,\na comprehensive investigation into the spatial robustness of dense vision\nmodels under localized corruptions remained underexplored. This paper fills\nthis gap by introducing specialized metrics for benchmarking the spatial\nrobustness of segmentation models, alongside with an evaluation framework to\nassess the impact of localized corruptions. Furthermore, we uncover the\ninherent complexity of characterizing worst-case robustness using a single\nlocalized adversarial perturbation. To address this, we propose region-aware\nmulti-attack adversarial analysis, a method that enables a deeper understanding\nof model robustness against adversarial perturbations applied to specific\nregions. The proposed metrics and analysis were exploited to evaluate 14\nsegmentation models in driving scenarios, uncovering key insights into the\neffects of localized corruption in both natural and adversarial forms. The\nresults reveal that models respond to these two types of threats differently;\nfor instance, transformer-based segmentation models demonstrate notable\nrobustness to localized natural corruptions but are highly vulnerable to\nadversarial ones and vice-versa for CNN-based models. Consequently, we also\naddress the challenge of balancing robustness to both natural and adversarial\nlocalized corruptions by means of ensemble models, thereby achieving a broader\nthreat coverage and improved reliability for dense vision tasks.\n", "link": "http://arxiv.org/abs/2504.01632v2", "date": "2025-04-17", "relevancy": 2.8082, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5922}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.564}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20the%20Spatial%20Robustness%20of%20DNNs%20via%20Natural%20and%20Adversarial%0A%20%20Localized%20Corruptions&body=Title%3A%20Benchmarking%20the%20Spatial%20Robustness%20of%20DNNs%20via%20Natural%20and%20Adversarial%0A%20%20Localized%20Corruptions%0AAuthor%3A%20Giulia%20Marchiori%20Pietrosanti%20and%20Giulio%20Rossolini%20and%20Alessandro%20Biondi%20and%20Giorgio%20Buttazzo%0AAbstract%3A%20%20%20The%20robustness%20of%20DNNs%20is%20a%20crucial%20factor%20in%20safety-critical%20applications%2C%0Aparticularly%20in%20complex%20and%20dynamic%20environments%20where%20localized%20corruptions%0Acan%20arise.%20While%20previous%20studies%20have%20evaluated%20the%20robustness%20of%20semantic%0Asegmentation%20%28SS%29%20models%20under%20whole-image%20natural%20or%20adversarial%20corruptions%2C%0Aa%20comprehensive%20investigation%20into%20the%20spatial%20robustness%20of%20dense%20vision%0Amodels%20under%20localized%20corruptions%20remained%20underexplored.%20This%20paper%20fills%0Athis%20gap%20by%20introducing%20specialized%20metrics%20for%20benchmarking%20the%20spatial%0Arobustness%20of%20segmentation%20models%2C%20alongside%20with%20an%20evaluation%20framework%20to%0Aassess%20the%20impact%20of%20localized%20corruptions.%20Furthermore%2C%20we%20uncover%20the%0Ainherent%20complexity%20of%20characterizing%20worst-case%20robustness%20using%20a%20single%0Alocalized%20adversarial%20perturbation.%20To%20address%20this%2C%20we%20propose%20region-aware%0Amulti-attack%20adversarial%20analysis%2C%20a%20method%20that%20enables%20a%20deeper%20understanding%0Aof%20model%20robustness%20against%20adversarial%20perturbations%20applied%20to%20specific%0Aregions.%20The%20proposed%20metrics%20and%20analysis%20were%20exploited%20to%20evaluate%2014%0Asegmentation%20models%20in%20driving%20scenarios%2C%20uncovering%20key%20insights%20into%20the%0Aeffects%20of%20localized%20corruption%20in%20both%20natural%20and%20adversarial%20forms.%20The%0Aresults%20reveal%20that%20models%20respond%20to%20these%20two%20types%20of%20threats%20differently%3B%0Afor%20instance%2C%20transformer-based%20segmentation%20models%20demonstrate%20notable%0Arobustness%20to%20localized%20natural%20corruptions%20but%20are%20highly%20vulnerable%20to%0Aadversarial%20ones%20and%20vice-versa%20for%20CNN-based%20models.%20Consequently%2C%20we%20also%0Aaddress%20the%20challenge%20of%20balancing%20robustness%20to%20both%20natural%20and%20adversarial%0Alocalized%20corruptions%20by%20means%20of%20ensemble%20models%2C%20thereby%20achieving%20a%20broader%0Athreat%20coverage%20and%20improved%20reliability%20for%20dense%20vision%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.01632v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520the%2520Spatial%2520Robustness%2520of%2520DNNs%2520via%2520Natural%2520and%2520Adversarial%250A%2520%2520Localized%2520Corruptions%26entry.906535625%3DGiulia%2520Marchiori%2520Pietrosanti%2520and%2520Giulio%2520Rossolini%2520and%2520Alessandro%2520Biondi%2520and%2520Giorgio%2520Buttazzo%26entry.1292438233%3D%2520%2520The%2520robustness%2520of%2520DNNs%2520is%2520a%2520crucial%2520factor%2520in%2520safety-critical%2520applications%252C%250Aparticularly%2520in%2520complex%2520and%2520dynamic%2520environments%2520where%2520localized%2520corruptions%250Acan%2520arise.%2520While%2520previous%2520studies%2520have%2520evaluated%2520the%2520robustness%2520of%2520semantic%250Asegmentation%2520%2528SS%2529%2520models%2520under%2520whole-image%2520natural%2520or%2520adversarial%2520corruptions%252C%250Aa%2520comprehensive%2520investigation%2520into%2520the%2520spatial%2520robustness%2520of%2520dense%2520vision%250Amodels%2520under%2520localized%2520corruptions%2520remained%2520underexplored.%2520This%2520paper%2520fills%250Athis%2520gap%2520by%2520introducing%2520specialized%2520metrics%2520for%2520benchmarking%2520the%2520spatial%250Arobustness%2520of%2520segmentation%2520models%252C%2520alongside%2520with%2520an%2520evaluation%2520framework%2520to%250Aassess%2520the%2520impact%2520of%2520localized%2520corruptions.%2520Furthermore%252C%2520we%2520uncover%2520the%250Ainherent%2520complexity%2520of%2520characterizing%2520worst-case%2520robustness%2520using%2520a%2520single%250Alocalized%2520adversarial%2520perturbation.%2520To%2520address%2520this%252C%2520we%2520propose%2520region-aware%250Amulti-attack%2520adversarial%2520analysis%252C%2520a%2520method%2520that%2520enables%2520a%2520deeper%2520understanding%250Aof%2520model%2520robustness%2520against%2520adversarial%2520perturbations%2520applied%2520to%2520specific%250Aregions.%2520The%2520proposed%2520metrics%2520and%2520analysis%2520were%2520exploited%2520to%2520evaluate%252014%250Asegmentation%2520models%2520in%2520driving%2520scenarios%252C%2520uncovering%2520key%2520insights%2520into%2520the%250Aeffects%2520of%2520localized%2520corruption%2520in%2520both%2520natural%2520and%2520adversarial%2520forms.%2520The%250Aresults%2520reveal%2520that%2520models%2520respond%2520to%2520these%2520two%2520types%2520of%2520threats%2520differently%253B%250Afor%2520instance%252C%2520transformer-based%2520segmentation%2520models%2520demonstrate%2520notable%250Arobustness%2520to%2520localized%2520natural%2520corruptions%2520but%2520are%2520highly%2520vulnerable%2520to%250Aadversarial%2520ones%2520and%2520vice-versa%2520for%2520CNN-based%2520models.%2520Consequently%252C%2520we%2520also%250Aaddress%2520the%2520challenge%2520of%2520balancing%2520robustness%2520to%2520both%2520natural%2520and%2520adversarial%250Alocalized%2520corruptions%2520by%2520means%2520of%2520ensemble%2520models%252C%2520thereby%2520achieving%2520a%2520broader%250Athreat%2520coverage%2520and%2520improved%2520reliability%2520for%2520dense%2520vision%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.01632v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20the%20Spatial%20Robustness%20of%20DNNs%20via%20Natural%20and%20Adversarial%0A%20%20Localized%20Corruptions&entry.906535625=Giulia%20Marchiori%20Pietrosanti%20and%20Giulio%20Rossolini%20and%20Alessandro%20Biondi%20and%20Giorgio%20Buttazzo&entry.1292438233=%20%20The%20robustness%20of%20DNNs%20is%20a%20crucial%20factor%20in%20safety-critical%20applications%2C%0Aparticularly%20in%20complex%20and%20dynamic%20environments%20where%20localized%20corruptions%0Acan%20arise.%20While%20previous%20studies%20have%20evaluated%20the%20robustness%20of%20semantic%0Asegmentation%20%28SS%29%20models%20under%20whole-image%20natural%20or%20adversarial%20corruptions%2C%0Aa%20comprehensive%20investigation%20into%20the%20spatial%20robustness%20of%20dense%20vision%0Amodels%20under%20localized%20corruptions%20remained%20underexplored.%20This%20paper%20fills%0Athis%20gap%20by%20introducing%20specialized%20metrics%20for%20benchmarking%20the%20spatial%0Arobustness%20of%20segmentation%20models%2C%20alongside%20with%20an%20evaluation%20framework%20to%0Aassess%20the%20impact%20of%20localized%20corruptions.%20Furthermore%2C%20we%20uncover%20the%0Ainherent%20complexity%20of%20characterizing%20worst-case%20robustness%20using%20a%20single%0Alocalized%20adversarial%20perturbation.%20To%20address%20this%2C%20we%20propose%20region-aware%0Amulti-attack%20adversarial%20analysis%2C%20a%20method%20that%20enables%20a%20deeper%20understanding%0Aof%20model%20robustness%20against%20adversarial%20perturbations%20applied%20to%20specific%0Aregions.%20The%20proposed%20metrics%20and%20analysis%20were%20exploited%20to%20evaluate%2014%0Asegmentation%20models%20in%20driving%20scenarios%2C%20uncovering%20key%20insights%20into%20the%0Aeffects%20of%20localized%20corruption%20in%20both%20natural%20and%20adversarial%20forms.%20The%0Aresults%20reveal%20that%20models%20respond%20to%20these%20two%20types%20of%20threats%20differently%3B%0Afor%20instance%2C%20transformer-based%20segmentation%20models%20demonstrate%20notable%0Arobustness%20to%20localized%20natural%20corruptions%20but%20are%20highly%20vulnerable%20to%0Aadversarial%20ones%20and%20vice-versa%20for%20CNN-based%20models.%20Consequently%2C%20we%20also%0Aaddress%20the%20challenge%20of%20balancing%20robustness%20to%20both%20natural%20and%20adversarial%0Alocalized%20corruptions%20by%20means%20of%20ensemble%20models%2C%20thereby%20achieving%20a%20broader%0Athreat%20coverage%20and%20improved%20reliability%20for%20dense%20vision%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.01632v2&entry.124074799=Read"},
{"title": "DC-SAM: In-Context Segment Anything in Images and Videos via Dual\n  Consistency", "author": "Mengshi Qi and Pengfei Zhu and Xiangtai Li and Xiaoyang Bi and Lu Qi and Huadong Ma and Ming-Hsuan Yang", "abstract": "  Given a single labeled example, in-context segmentation aims to segment\ncorresponding objects. This setting, known as one-shot segmentation in few-shot\nlearning, explores the segmentation model's generalization ability and has been\napplied to various vision tasks, including scene understanding and image/video\nediting. While recent Segment Anything Models have achieved state-of-the-art\nresults in interactive segmentation, these approaches are not directly\napplicable to in-context segmentation. In this work, we propose the Dual\nConsistency SAM (DC-SAM) method based on prompt-tuning to adapt SAM and SAM2\nfor in-context segmentation of both images and videos. Our key insights are to\nenhance the features of the SAM's prompt encoder in segmentation by providing\nhigh-quality visual prompts. When generating a mask prior, we fuse the SAM\nfeatures to better align the prompt encoder. Then, we design a cycle-consistent\ncross-attention on fused features and initial visual prompts. Next, a\ndual-branch design is provided by using the discriminative positive and\nnegative prompts in the prompt encoder. Furthermore, we design a simple\nmask-tube training strategy to adopt our proposed dual consistency method into\nthe mask tube. Although the proposed DC-SAM is primarily designed for images,\nit can be seamlessly extended to the video domain with the support of SAM2.\nGiven the absence of in-context segmentation in the video domain, we manually\ncurate and construct the first benchmark from existing video segmentation\ndatasets, named In-Context Video Object Segmentation (IC-VOS), to better assess\nthe in-context capability of the model. Extensive experiments demonstrate that\nour method achieves 55.5 (+1.4) mIoU on COCO-20i, 73.0 (+1.1) mIoU on\nPASCAL-5i, and a J&F score of 71.52 on the proposed IC-VOS benchmark. Our\nsource code and benchmark are available at https://github.com/zaplm/DC-SAM.\n", "link": "http://arxiv.org/abs/2504.12080v2", "date": "2025-04-17", "relevancy": 2.8029, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5732}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5568}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DC-SAM%3A%20In-Context%20Segment%20Anything%20in%20Images%20and%20Videos%20via%20Dual%0A%20%20Consistency&body=Title%3A%20DC-SAM%3A%20In-Context%20Segment%20Anything%20in%20Images%20and%20Videos%20via%20Dual%0A%20%20Consistency%0AAuthor%3A%20Mengshi%20Qi%20and%20Pengfei%20Zhu%20and%20Xiangtai%20Li%20and%20Xiaoyang%20Bi%20and%20Lu%20Qi%20and%20Huadong%20Ma%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20%20%20Given%20a%20single%20labeled%20example%2C%20in-context%20segmentation%20aims%20to%20segment%0Acorresponding%20objects.%20This%20setting%2C%20known%20as%20one-shot%20segmentation%20in%20few-shot%0Alearning%2C%20explores%20the%20segmentation%20model%27s%20generalization%20ability%20and%20has%20been%0Aapplied%20to%20various%20vision%20tasks%2C%20including%20scene%20understanding%20and%20image/video%0Aediting.%20While%20recent%20Segment%20Anything%20Models%20have%20achieved%20state-of-the-art%0Aresults%20in%20interactive%20segmentation%2C%20these%20approaches%20are%20not%20directly%0Aapplicable%20to%20in-context%20segmentation.%20In%20this%20work%2C%20we%20propose%20the%20Dual%0AConsistency%20SAM%20%28DC-SAM%29%20method%20based%20on%20prompt-tuning%20to%20adapt%20SAM%20and%20SAM2%0Afor%20in-context%20segmentation%20of%20both%20images%20and%20videos.%20Our%20key%20insights%20are%20to%0Aenhance%20the%20features%20of%20the%20SAM%27s%20prompt%20encoder%20in%20segmentation%20by%20providing%0Ahigh-quality%20visual%20prompts.%20When%20generating%20a%20mask%20prior%2C%20we%20fuse%20the%20SAM%0Afeatures%20to%20better%20align%20the%20prompt%20encoder.%20Then%2C%20we%20design%20a%20cycle-consistent%0Across-attention%20on%20fused%20features%20and%20initial%20visual%20prompts.%20Next%2C%20a%0Adual-branch%20design%20is%20provided%20by%20using%20the%20discriminative%20positive%20and%0Anegative%20prompts%20in%20the%20prompt%20encoder.%20Furthermore%2C%20we%20design%20a%20simple%0Amask-tube%20training%20strategy%20to%20adopt%20our%20proposed%20dual%20consistency%20method%20into%0Athe%20mask%20tube.%20Although%20the%20proposed%20DC-SAM%20is%20primarily%20designed%20for%20images%2C%0Ait%20can%20be%20seamlessly%20extended%20to%20the%20video%20domain%20with%20the%20support%20of%20SAM2.%0AGiven%20the%20absence%20of%20in-context%20segmentation%20in%20the%20video%20domain%2C%20we%20manually%0Acurate%20and%20construct%20the%20first%20benchmark%20from%20existing%20video%20segmentation%0Adatasets%2C%20named%20In-Context%20Video%20Object%20Segmentation%20%28IC-VOS%29%2C%20to%20better%20assess%0Athe%20in-context%20capability%20of%20the%20model.%20Extensive%20experiments%20demonstrate%20that%0Aour%20method%20achieves%2055.5%20%28%2B1.4%29%20mIoU%20on%20COCO-20i%2C%2073.0%20%28%2B1.1%29%20mIoU%20on%0APASCAL-5i%2C%20and%20a%20J%26F%20score%20of%2071.52%20on%20the%20proposed%20IC-VOS%20benchmark.%20Our%0Asource%20code%20and%20benchmark%20are%20available%20at%20https%3A//github.com/zaplm/DC-SAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12080v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDC-SAM%253A%2520In-Context%2520Segment%2520Anything%2520in%2520Images%2520and%2520Videos%2520via%2520Dual%250A%2520%2520Consistency%26entry.906535625%3DMengshi%2520Qi%2520and%2520Pengfei%2520Zhu%2520and%2520Xiangtai%2520Li%2520and%2520Xiaoyang%2520Bi%2520and%2520Lu%2520Qi%2520and%2520Huadong%2520Ma%2520and%2520Ming-Hsuan%2520Yang%26entry.1292438233%3D%2520%2520Given%2520a%2520single%2520labeled%2520example%252C%2520in-context%2520segmentation%2520aims%2520to%2520segment%250Acorresponding%2520objects.%2520This%2520setting%252C%2520known%2520as%2520one-shot%2520segmentation%2520in%2520few-shot%250Alearning%252C%2520explores%2520the%2520segmentation%2520model%2527s%2520generalization%2520ability%2520and%2520has%2520been%250Aapplied%2520to%2520various%2520vision%2520tasks%252C%2520including%2520scene%2520understanding%2520and%2520image/video%250Aediting.%2520While%2520recent%2520Segment%2520Anything%2520Models%2520have%2520achieved%2520state-of-the-art%250Aresults%2520in%2520interactive%2520segmentation%252C%2520these%2520approaches%2520are%2520not%2520directly%250Aapplicable%2520to%2520in-context%2520segmentation.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520Dual%250AConsistency%2520SAM%2520%2528DC-SAM%2529%2520method%2520based%2520on%2520prompt-tuning%2520to%2520adapt%2520SAM%2520and%2520SAM2%250Afor%2520in-context%2520segmentation%2520of%2520both%2520images%2520and%2520videos.%2520Our%2520key%2520insights%2520are%2520to%250Aenhance%2520the%2520features%2520of%2520the%2520SAM%2527s%2520prompt%2520encoder%2520in%2520segmentation%2520by%2520providing%250Ahigh-quality%2520visual%2520prompts.%2520When%2520generating%2520a%2520mask%2520prior%252C%2520we%2520fuse%2520the%2520SAM%250Afeatures%2520to%2520better%2520align%2520the%2520prompt%2520encoder.%2520Then%252C%2520we%2520design%2520a%2520cycle-consistent%250Across-attention%2520on%2520fused%2520features%2520and%2520initial%2520visual%2520prompts.%2520Next%252C%2520a%250Adual-branch%2520design%2520is%2520provided%2520by%2520using%2520the%2520discriminative%2520positive%2520and%250Anegative%2520prompts%2520in%2520the%2520prompt%2520encoder.%2520Furthermore%252C%2520we%2520design%2520a%2520simple%250Amask-tube%2520training%2520strategy%2520to%2520adopt%2520our%2520proposed%2520dual%2520consistency%2520method%2520into%250Athe%2520mask%2520tube.%2520Although%2520the%2520proposed%2520DC-SAM%2520is%2520primarily%2520designed%2520for%2520images%252C%250Ait%2520can%2520be%2520seamlessly%2520extended%2520to%2520the%2520video%2520domain%2520with%2520the%2520support%2520of%2520SAM2.%250AGiven%2520the%2520absence%2520of%2520in-context%2520segmentation%2520in%2520the%2520video%2520domain%252C%2520we%2520manually%250Acurate%2520and%2520construct%2520the%2520first%2520benchmark%2520from%2520existing%2520video%2520segmentation%250Adatasets%252C%2520named%2520In-Context%2520Video%2520Object%2520Segmentation%2520%2528IC-VOS%2529%252C%2520to%2520better%2520assess%250Athe%2520in-context%2520capability%2520of%2520the%2520model.%2520Extensive%2520experiments%2520demonstrate%2520that%250Aour%2520method%2520achieves%252055.5%2520%2528%252B1.4%2529%2520mIoU%2520on%2520COCO-20i%252C%252073.0%2520%2528%252B1.1%2529%2520mIoU%2520on%250APASCAL-5i%252C%2520and%2520a%2520J%2526F%2520score%2520of%252071.52%2520on%2520the%2520proposed%2520IC-VOS%2520benchmark.%2520Our%250Asource%2520code%2520and%2520benchmark%2520are%2520available%2520at%2520https%253A//github.com/zaplm/DC-SAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12080v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DC-SAM%3A%20In-Context%20Segment%20Anything%20in%20Images%20and%20Videos%20via%20Dual%0A%20%20Consistency&entry.906535625=Mengshi%20Qi%20and%20Pengfei%20Zhu%20and%20Xiangtai%20Li%20and%20Xiaoyang%20Bi%20and%20Lu%20Qi%20and%20Huadong%20Ma%20and%20Ming-Hsuan%20Yang&entry.1292438233=%20%20Given%20a%20single%20labeled%20example%2C%20in-context%20segmentation%20aims%20to%20segment%0Acorresponding%20objects.%20This%20setting%2C%20known%20as%20one-shot%20segmentation%20in%20few-shot%0Alearning%2C%20explores%20the%20segmentation%20model%27s%20generalization%20ability%20and%20has%20been%0Aapplied%20to%20various%20vision%20tasks%2C%20including%20scene%20understanding%20and%20image/video%0Aediting.%20While%20recent%20Segment%20Anything%20Models%20have%20achieved%20state-of-the-art%0Aresults%20in%20interactive%20segmentation%2C%20these%20approaches%20are%20not%20directly%0Aapplicable%20to%20in-context%20segmentation.%20In%20this%20work%2C%20we%20propose%20the%20Dual%0AConsistency%20SAM%20%28DC-SAM%29%20method%20based%20on%20prompt-tuning%20to%20adapt%20SAM%20and%20SAM2%0Afor%20in-context%20segmentation%20of%20both%20images%20and%20videos.%20Our%20key%20insights%20are%20to%0Aenhance%20the%20features%20of%20the%20SAM%27s%20prompt%20encoder%20in%20segmentation%20by%20providing%0Ahigh-quality%20visual%20prompts.%20When%20generating%20a%20mask%20prior%2C%20we%20fuse%20the%20SAM%0Afeatures%20to%20better%20align%20the%20prompt%20encoder.%20Then%2C%20we%20design%20a%20cycle-consistent%0Across-attention%20on%20fused%20features%20and%20initial%20visual%20prompts.%20Next%2C%20a%0Adual-branch%20design%20is%20provided%20by%20using%20the%20discriminative%20positive%20and%0Anegative%20prompts%20in%20the%20prompt%20encoder.%20Furthermore%2C%20we%20design%20a%20simple%0Amask-tube%20training%20strategy%20to%20adopt%20our%20proposed%20dual%20consistency%20method%20into%0Athe%20mask%20tube.%20Although%20the%20proposed%20DC-SAM%20is%20primarily%20designed%20for%20images%2C%0Ait%20can%20be%20seamlessly%20extended%20to%20the%20video%20domain%20with%20the%20support%20of%20SAM2.%0AGiven%20the%20absence%20of%20in-context%20segmentation%20in%20the%20video%20domain%2C%20we%20manually%0Acurate%20and%20construct%20the%20first%20benchmark%20from%20existing%20video%20segmentation%0Adatasets%2C%20named%20In-Context%20Video%20Object%20Segmentation%20%28IC-VOS%29%2C%20to%20better%20assess%0Athe%20in-context%20capability%20of%20the%20model.%20Extensive%20experiments%20demonstrate%20that%0Aour%20method%20achieves%2055.5%20%28%2B1.4%29%20mIoU%20on%20COCO-20i%2C%2073.0%20%28%2B1.1%29%20mIoU%20on%0APASCAL-5i%2C%20and%20a%20J%26F%20score%20of%2071.52%20on%20the%20proposed%20IC-VOS%20benchmark.%20Our%0Asource%20code%20and%20benchmark%20are%20available%20at%20https%3A//github.com/zaplm/DC-SAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12080v2&entry.124074799=Read"},
{"title": "Pose and Facial Expression Transfer by using StyleGAN", "author": "Petr Jahoda and Jan Cech", "abstract": "  We propose a method to transfer pose and expression between face images.\nGiven a source and target face portrait, the model produces an output image in\nwhich the pose and expression of the source face image are transferred onto the\ntarget identity. The architecture consists of two encoders and a mapping\nnetwork that projects the two inputs into the latent space of StyleGAN2, which\nfinally generates the output. The training is self-supervised from video\nsequences of many individuals. Manual labeling is not required. Our model\nenables the synthesis of random identities with controllable pose and\nexpression. Close-to-real-time performance is achieved.\n", "link": "http://arxiv.org/abs/2504.13021v1", "date": "2025-04-17", "relevancy": 2.7985, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5785}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5604}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pose%20and%20Facial%20Expression%20Transfer%20by%20using%20StyleGAN&body=Title%3A%20Pose%20and%20Facial%20Expression%20Transfer%20by%20using%20StyleGAN%0AAuthor%3A%20Petr%20Jahoda%20and%20Jan%20Cech%0AAbstract%3A%20%20%20We%20propose%20a%20method%20to%20transfer%20pose%20and%20expression%20between%20face%20images.%0AGiven%20a%20source%20and%20target%20face%20portrait%2C%20the%20model%20produces%20an%20output%20image%20in%0Awhich%20the%20pose%20and%20expression%20of%20the%20source%20face%20image%20are%20transferred%20onto%20the%0Atarget%20identity.%20The%20architecture%20consists%20of%20two%20encoders%20and%20a%20mapping%0Anetwork%20that%20projects%20the%20two%20inputs%20into%20the%20latent%20space%20of%20StyleGAN2%2C%20which%0Afinally%20generates%20the%20output.%20The%20training%20is%20self-supervised%20from%20video%0Asequences%20of%20many%20individuals.%20Manual%20labeling%20is%20not%20required.%20Our%20model%0Aenables%20the%20synthesis%20of%20random%20identities%20with%20controllable%20pose%20and%0Aexpression.%20Close-to-real-time%20performance%20is%20achieved.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13021v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPose%2520and%2520Facial%2520Expression%2520Transfer%2520by%2520using%2520StyleGAN%26entry.906535625%3DPetr%2520Jahoda%2520and%2520Jan%2520Cech%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520method%2520to%2520transfer%2520pose%2520and%2520expression%2520between%2520face%2520images.%250AGiven%2520a%2520source%2520and%2520target%2520face%2520portrait%252C%2520the%2520model%2520produces%2520an%2520output%2520image%2520in%250Awhich%2520the%2520pose%2520and%2520expression%2520of%2520the%2520source%2520face%2520image%2520are%2520transferred%2520onto%2520the%250Atarget%2520identity.%2520The%2520architecture%2520consists%2520of%2520two%2520encoders%2520and%2520a%2520mapping%250Anetwork%2520that%2520projects%2520the%2520two%2520inputs%2520into%2520the%2520latent%2520space%2520of%2520StyleGAN2%252C%2520which%250Afinally%2520generates%2520the%2520output.%2520The%2520training%2520is%2520self-supervised%2520from%2520video%250Asequences%2520of%2520many%2520individuals.%2520Manual%2520labeling%2520is%2520not%2520required.%2520Our%2520model%250Aenables%2520the%2520synthesis%2520of%2520random%2520identities%2520with%2520controllable%2520pose%2520and%250Aexpression.%2520Close-to-real-time%2520performance%2520is%2520achieved.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13021v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pose%20and%20Facial%20Expression%20Transfer%20by%20using%20StyleGAN&entry.906535625=Petr%20Jahoda%20and%20Jan%20Cech&entry.1292438233=%20%20We%20propose%20a%20method%20to%20transfer%20pose%20and%20expression%20between%20face%20images.%0AGiven%20a%20source%20and%20target%20face%20portrait%2C%20the%20model%20produces%20an%20output%20image%20in%0Awhich%20the%20pose%20and%20expression%20of%20the%20source%20face%20image%20are%20transferred%20onto%20the%0Atarget%20identity.%20The%20architecture%20consists%20of%20two%20encoders%20and%20a%20mapping%0Anetwork%20that%20projects%20the%20two%20inputs%20into%20the%20latent%20space%20of%20StyleGAN2%2C%20which%0Afinally%20generates%20the%20output.%20The%20training%20is%20self-supervised%20from%20video%0Asequences%20of%20many%20individuals.%20Manual%20labeling%20is%20not%20required.%20Our%20model%0Aenables%20the%20synthesis%20of%20random%20identities%20with%20controllable%20pose%20and%0Aexpression.%20Close-to-real-time%20performance%20is%20achieved.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13021v1&entry.124074799=Read"},
{"title": "EchoWorld: Learning Motion-Aware World Models for Echocardiography Probe\n  Guidance", "author": "Yang Yue and Yulin Wang and Haojun Jiang and Pan Liu and Shiji Song and Gao Huang", "abstract": "  Echocardiography is crucial for cardiovascular disease detection but relies\nheavily on experienced sonographers. Echocardiography probe guidance systems,\nwhich provide real-time movement instructions for acquiring standard plane\nimages, offer a promising solution for AI-assisted or fully autonomous\nscanning. However, developing effective machine learning models for this task\nremains challenging, as they must grasp heart anatomy and the intricate\ninterplay between probe motion and visual signals. To address this, we present\nEchoWorld, a motion-aware world modeling framework for probe guidance that\nencodes anatomical knowledge and motion-induced visual dynamics, while\neffectively leveraging past visual-motion sequences to enhance guidance\nprecision. EchoWorld employs a pre-training strategy inspired by world modeling\nprinciples, where the model predicts masked anatomical regions and simulates\nthe visual outcomes of probe adjustments. Built upon this pre-trained model, we\nintroduce a motion-aware attention mechanism in the fine-tuning stage that\neffectively integrates historical visual-motion data, enabling precise and\nadaptive probe guidance. Trained on more than one million ultrasound images\nfrom over 200 routine scans, EchoWorld effectively captures key\nechocardiographic knowledge, as validated by qualitative analysis. Moreover,\nour method significantly reduces guidance errors compared to existing visual\nbackbones and guidance frameworks, excelling in both single-frame and\nsequential evaluation protocols. Code is available at\nhttps://github.com/LeapLabTHU/EchoWorld.\n", "link": "http://arxiv.org/abs/2504.13065v1", "date": "2025-04-17", "relevancy": 2.7338, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5481}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5461}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EchoWorld%3A%20Learning%20Motion-Aware%20World%20Models%20for%20Echocardiography%20Probe%0A%20%20Guidance&body=Title%3A%20EchoWorld%3A%20Learning%20Motion-Aware%20World%20Models%20for%20Echocardiography%20Probe%0A%20%20Guidance%0AAuthor%3A%20Yang%20Yue%20and%20Yulin%20Wang%20and%20Haojun%20Jiang%20and%20Pan%20Liu%20and%20Shiji%20Song%20and%20Gao%20Huang%0AAbstract%3A%20%20%20Echocardiography%20is%20crucial%20for%20cardiovascular%20disease%20detection%20but%20relies%0Aheavily%20on%20experienced%20sonographers.%20Echocardiography%20probe%20guidance%20systems%2C%0Awhich%20provide%20real-time%20movement%20instructions%20for%20acquiring%20standard%20plane%0Aimages%2C%20offer%20a%20promising%20solution%20for%20AI-assisted%20or%20fully%20autonomous%0Ascanning.%20However%2C%20developing%20effective%20machine%20learning%20models%20for%20this%20task%0Aremains%20challenging%2C%20as%20they%20must%20grasp%20heart%20anatomy%20and%20the%20intricate%0Ainterplay%20between%20probe%20motion%20and%20visual%20signals.%20To%20address%20this%2C%20we%20present%0AEchoWorld%2C%20a%20motion-aware%20world%20modeling%20framework%20for%20probe%20guidance%20that%0Aencodes%20anatomical%20knowledge%20and%20motion-induced%20visual%20dynamics%2C%20while%0Aeffectively%20leveraging%20past%20visual-motion%20sequences%20to%20enhance%20guidance%0Aprecision.%20EchoWorld%20employs%20a%20pre-training%20strategy%20inspired%20by%20world%20modeling%0Aprinciples%2C%20where%20the%20model%20predicts%20masked%20anatomical%20regions%20and%20simulates%0Athe%20visual%20outcomes%20of%20probe%20adjustments.%20Built%20upon%20this%20pre-trained%20model%2C%20we%0Aintroduce%20a%20motion-aware%20attention%20mechanism%20in%20the%20fine-tuning%20stage%20that%0Aeffectively%20integrates%20historical%20visual-motion%20data%2C%20enabling%20precise%20and%0Aadaptive%20probe%20guidance.%20Trained%20on%20more%20than%20one%20million%20ultrasound%20images%0Afrom%20over%20200%20routine%20scans%2C%20EchoWorld%20effectively%20captures%20key%0Aechocardiographic%20knowledge%2C%20as%20validated%20by%20qualitative%20analysis.%20Moreover%2C%0Aour%20method%20significantly%20reduces%20guidance%20errors%20compared%20to%20existing%20visual%0Abackbones%20and%20guidance%20frameworks%2C%20excelling%20in%20both%20single-frame%20and%0Asequential%20evaluation%20protocols.%20Code%20is%20available%20at%0Ahttps%3A//github.com/LeapLabTHU/EchoWorld.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13065v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEchoWorld%253A%2520Learning%2520Motion-Aware%2520World%2520Models%2520for%2520Echocardiography%2520Probe%250A%2520%2520Guidance%26entry.906535625%3DYang%2520Yue%2520and%2520Yulin%2520Wang%2520and%2520Haojun%2520Jiang%2520and%2520Pan%2520Liu%2520and%2520Shiji%2520Song%2520and%2520Gao%2520Huang%26entry.1292438233%3D%2520%2520Echocardiography%2520is%2520crucial%2520for%2520cardiovascular%2520disease%2520detection%2520but%2520relies%250Aheavily%2520on%2520experienced%2520sonographers.%2520Echocardiography%2520probe%2520guidance%2520systems%252C%250Awhich%2520provide%2520real-time%2520movement%2520instructions%2520for%2520acquiring%2520standard%2520plane%250Aimages%252C%2520offer%2520a%2520promising%2520solution%2520for%2520AI-assisted%2520or%2520fully%2520autonomous%250Ascanning.%2520However%252C%2520developing%2520effective%2520machine%2520learning%2520models%2520for%2520this%2520task%250Aremains%2520challenging%252C%2520as%2520they%2520must%2520grasp%2520heart%2520anatomy%2520and%2520the%2520intricate%250Ainterplay%2520between%2520probe%2520motion%2520and%2520visual%2520signals.%2520To%2520address%2520this%252C%2520we%2520present%250AEchoWorld%252C%2520a%2520motion-aware%2520world%2520modeling%2520framework%2520for%2520probe%2520guidance%2520that%250Aencodes%2520anatomical%2520knowledge%2520and%2520motion-induced%2520visual%2520dynamics%252C%2520while%250Aeffectively%2520leveraging%2520past%2520visual-motion%2520sequences%2520to%2520enhance%2520guidance%250Aprecision.%2520EchoWorld%2520employs%2520a%2520pre-training%2520strategy%2520inspired%2520by%2520world%2520modeling%250Aprinciples%252C%2520where%2520the%2520model%2520predicts%2520masked%2520anatomical%2520regions%2520and%2520simulates%250Athe%2520visual%2520outcomes%2520of%2520probe%2520adjustments.%2520Built%2520upon%2520this%2520pre-trained%2520model%252C%2520we%250Aintroduce%2520a%2520motion-aware%2520attention%2520mechanism%2520in%2520the%2520fine-tuning%2520stage%2520that%250Aeffectively%2520integrates%2520historical%2520visual-motion%2520data%252C%2520enabling%2520precise%2520and%250Aadaptive%2520probe%2520guidance.%2520Trained%2520on%2520more%2520than%2520one%2520million%2520ultrasound%2520images%250Afrom%2520over%2520200%2520routine%2520scans%252C%2520EchoWorld%2520effectively%2520captures%2520key%250Aechocardiographic%2520knowledge%252C%2520as%2520validated%2520by%2520qualitative%2520analysis.%2520Moreover%252C%250Aour%2520method%2520significantly%2520reduces%2520guidance%2520errors%2520compared%2520to%2520existing%2520visual%250Abackbones%2520and%2520guidance%2520frameworks%252C%2520excelling%2520in%2520both%2520single-frame%2520and%250Asequential%2520evaluation%2520protocols.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/LeapLabTHU/EchoWorld.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13065v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EchoWorld%3A%20Learning%20Motion-Aware%20World%20Models%20for%20Echocardiography%20Probe%0A%20%20Guidance&entry.906535625=Yang%20Yue%20and%20Yulin%20Wang%20and%20Haojun%20Jiang%20and%20Pan%20Liu%20and%20Shiji%20Song%20and%20Gao%20Huang&entry.1292438233=%20%20Echocardiography%20is%20crucial%20for%20cardiovascular%20disease%20detection%20but%20relies%0Aheavily%20on%20experienced%20sonographers.%20Echocardiography%20probe%20guidance%20systems%2C%0Awhich%20provide%20real-time%20movement%20instructions%20for%20acquiring%20standard%20plane%0Aimages%2C%20offer%20a%20promising%20solution%20for%20AI-assisted%20or%20fully%20autonomous%0Ascanning.%20However%2C%20developing%20effective%20machine%20learning%20models%20for%20this%20task%0Aremains%20challenging%2C%20as%20they%20must%20grasp%20heart%20anatomy%20and%20the%20intricate%0Ainterplay%20between%20probe%20motion%20and%20visual%20signals.%20To%20address%20this%2C%20we%20present%0AEchoWorld%2C%20a%20motion-aware%20world%20modeling%20framework%20for%20probe%20guidance%20that%0Aencodes%20anatomical%20knowledge%20and%20motion-induced%20visual%20dynamics%2C%20while%0Aeffectively%20leveraging%20past%20visual-motion%20sequences%20to%20enhance%20guidance%0Aprecision.%20EchoWorld%20employs%20a%20pre-training%20strategy%20inspired%20by%20world%20modeling%0Aprinciples%2C%20where%20the%20model%20predicts%20masked%20anatomical%20regions%20and%20simulates%0Athe%20visual%20outcomes%20of%20probe%20adjustments.%20Built%20upon%20this%20pre-trained%20model%2C%20we%0Aintroduce%20a%20motion-aware%20attention%20mechanism%20in%20the%20fine-tuning%20stage%20that%0Aeffectively%20integrates%20historical%20visual-motion%20data%2C%20enabling%20precise%20and%0Aadaptive%20probe%20guidance.%20Trained%20on%20more%20than%20one%20million%20ultrasound%20images%0Afrom%20over%20200%20routine%20scans%2C%20EchoWorld%20effectively%20captures%20key%0Aechocardiographic%20knowledge%2C%20as%20validated%20by%20qualitative%20analysis.%20Moreover%2C%0Aour%20method%20significantly%20reduces%20guidance%20errors%20compared%20to%20existing%20visual%0Abackbones%20and%20guidance%20frameworks%2C%20excelling%20in%20both%20single-frame%20and%0Asequential%20evaluation%20protocols.%20Code%20is%20available%20at%0Ahttps%3A//github.com/LeapLabTHU/EchoWorld.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13065v1&entry.124074799=Read"},
{"title": "Computer-Aided Design of Personalized Occlusal Positioning Splints Using\n  Multimodal 3D Data", "author": "Agnieszka Anna Tomaka and Leszek Luchowski and Micha\u0142 Tarnawski and Dariusz Pojda", "abstract": "  Contemporary digital technology has a pivotal role in the design of\ncustomized medical appliances, including occlusal splints used in the treatment\nof stomatognathic system dysfunctions. We present an approach to computer-aided\ndesign and precision assessment of positioning occlusal splints, bridging\nclinical concepts with current digital dental practice. In our model, a 3D\nsplint is generated based on a transformation matrix that represents the\ntherapeutic change in mandibular position, defined by a specialist using a\nvirtual patient model reconstructed from intraoral scans, CBCT, 3D facial scans\nand plaster model digitisation. The paper introduces a novel method for\ngenerating splints that accurately reproduce occlusal conditions in the\ntherapeutic position, including a mechanism for resolving surface conflicts\nthrough virtual embossing. We demonstrate how transformation matrices can be\nacquired through clinical tools and intraoral devices, and evaluate the\naccuracy of the designed and printed splints using profile and surface\ndeviation analysis. The proposed method enables reproducible, patient-specific\nsplint fabrication and opens new possibilities in diagnostics, multimodal image\nregistration and quantification of occlusal discrepancies.\n", "link": "http://arxiv.org/abs/2504.12868v1", "date": "2025-04-17", "relevancy": 2.6849, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5377}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5377}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Computer-Aided%20Design%20of%20Personalized%20Occlusal%20Positioning%20Splints%20Using%0A%20%20Multimodal%203D%20Data&body=Title%3A%20Computer-Aided%20Design%20of%20Personalized%20Occlusal%20Positioning%20Splints%20Using%0A%20%20Multimodal%203D%20Data%0AAuthor%3A%20Agnieszka%20Anna%20Tomaka%20and%20Leszek%20Luchowski%20and%20Micha%C5%82%20Tarnawski%20and%20Dariusz%20Pojda%0AAbstract%3A%20%20%20Contemporary%20digital%20technology%20has%20a%20pivotal%20role%20in%20the%20design%20of%0Acustomized%20medical%20appliances%2C%20including%20occlusal%20splints%20used%20in%20the%20treatment%0Aof%20stomatognathic%20system%20dysfunctions.%20We%20present%20an%20approach%20to%20computer-aided%0Adesign%20and%20precision%20assessment%20of%20positioning%20occlusal%20splints%2C%20bridging%0Aclinical%20concepts%20with%20current%20digital%20dental%20practice.%20In%20our%20model%2C%20a%203D%0Asplint%20is%20generated%20based%20on%20a%20transformation%20matrix%20that%20represents%20the%0Atherapeutic%20change%20in%20mandibular%20position%2C%20defined%20by%20a%20specialist%20using%20a%0Avirtual%20patient%20model%20reconstructed%20from%20intraoral%20scans%2C%20CBCT%2C%203D%20facial%20scans%0Aand%20plaster%20model%20digitisation.%20The%20paper%20introduces%20a%20novel%20method%20for%0Agenerating%20splints%20that%20accurately%20reproduce%20occlusal%20conditions%20in%20the%0Atherapeutic%20position%2C%20including%20a%20mechanism%20for%20resolving%20surface%20conflicts%0Athrough%20virtual%20embossing.%20We%20demonstrate%20how%20transformation%20matrices%20can%20be%0Aacquired%20through%20clinical%20tools%20and%20intraoral%20devices%2C%20and%20evaluate%20the%0Aaccuracy%20of%20the%20designed%20and%20printed%20splints%20using%20profile%20and%20surface%0Adeviation%20analysis.%20The%20proposed%20method%20enables%20reproducible%2C%20patient-specific%0Asplint%20fabrication%20and%20opens%20new%20possibilities%20in%20diagnostics%2C%20multimodal%20image%0Aregistration%20and%20quantification%20of%20occlusal%20discrepancies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12868v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComputer-Aided%2520Design%2520of%2520Personalized%2520Occlusal%2520Positioning%2520Splints%2520Using%250A%2520%2520Multimodal%25203D%2520Data%26entry.906535625%3DAgnieszka%2520Anna%2520Tomaka%2520and%2520Leszek%2520Luchowski%2520and%2520Micha%25C5%2582%2520Tarnawski%2520and%2520Dariusz%2520Pojda%26entry.1292438233%3D%2520%2520Contemporary%2520digital%2520technology%2520has%2520a%2520pivotal%2520role%2520in%2520the%2520design%2520of%250Acustomized%2520medical%2520appliances%252C%2520including%2520occlusal%2520splints%2520used%2520in%2520the%2520treatment%250Aof%2520stomatognathic%2520system%2520dysfunctions.%2520We%2520present%2520an%2520approach%2520to%2520computer-aided%250Adesign%2520and%2520precision%2520assessment%2520of%2520positioning%2520occlusal%2520splints%252C%2520bridging%250Aclinical%2520concepts%2520with%2520current%2520digital%2520dental%2520practice.%2520In%2520our%2520model%252C%2520a%25203D%250Asplint%2520is%2520generated%2520based%2520on%2520a%2520transformation%2520matrix%2520that%2520represents%2520the%250Atherapeutic%2520change%2520in%2520mandibular%2520position%252C%2520defined%2520by%2520a%2520specialist%2520using%2520a%250Avirtual%2520patient%2520model%2520reconstructed%2520from%2520intraoral%2520scans%252C%2520CBCT%252C%25203D%2520facial%2520scans%250Aand%2520plaster%2520model%2520digitisation.%2520The%2520paper%2520introduces%2520a%2520novel%2520method%2520for%250Agenerating%2520splints%2520that%2520accurately%2520reproduce%2520occlusal%2520conditions%2520in%2520the%250Atherapeutic%2520position%252C%2520including%2520a%2520mechanism%2520for%2520resolving%2520surface%2520conflicts%250Athrough%2520virtual%2520embossing.%2520We%2520demonstrate%2520how%2520transformation%2520matrices%2520can%2520be%250Aacquired%2520through%2520clinical%2520tools%2520and%2520intraoral%2520devices%252C%2520and%2520evaluate%2520the%250Aaccuracy%2520of%2520the%2520designed%2520and%2520printed%2520splints%2520using%2520profile%2520and%2520surface%250Adeviation%2520analysis.%2520The%2520proposed%2520method%2520enables%2520reproducible%252C%2520patient-specific%250Asplint%2520fabrication%2520and%2520opens%2520new%2520possibilities%2520in%2520diagnostics%252C%2520multimodal%2520image%250Aregistration%2520and%2520quantification%2520of%2520occlusal%2520discrepancies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12868v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Computer-Aided%20Design%20of%20Personalized%20Occlusal%20Positioning%20Splints%20Using%0A%20%20Multimodal%203D%20Data&entry.906535625=Agnieszka%20Anna%20Tomaka%20and%20Leszek%20Luchowski%20and%20Micha%C5%82%20Tarnawski%20and%20Dariusz%20Pojda&entry.1292438233=%20%20Contemporary%20digital%20technology%20has%20a%20pivotal%20role%20in%20the%20design%20of%0Acustomized%20medical%20appliances%2C%20including%20occlusal%20splints%20used%20in%20the%20treatment%0Aof%20stomatognathic%20system%20dysfunctions.%20We%20present%20an%20approach%20to%20computer-aided%0Adesign%20and%20precision%20assessment%20of%20positioning%20occlusal%20splints%2C%20bridging%0Aclinical%20concepts%20with%20current%20digital%20dental%20practice.%20In%20our%20model%2C%20a%203D%0Asplint%20is%20generated%20based%20on%20a%20transformation%20matrix%20that%20represents%20the%0Atherapeutic%20change%20in%20mandibular%20position%2C%20defined%20by%20a%20specialist%20using%20a%0Avirtual%20patient%20model%20reconstructed%20from%20intraoral%20scans%2C%20CBCT%2C%203D%20facial%20scans%0Aand%20plaster%20model%20digitisation.%20The%20paper%20introduces%20a%20novel%20method%20for%0Agenerating%20splints%20that%20accurately%20reproduce%20occlusal%20conditions%20in%20the%0Atherapeutic%20position%2C%20including%20a%20mechanism%20for%20resolving%20surface%20conflicts%0Athrough%20virtual%20embossing.%20We%20demonstrate%20how%20transformation%20matrices%20can%20be%0Aacquired%20through%20clinical%20tools%20and%20intraoral%20devices%2C%20and%20evaluate%20the%0Aaccuracy%20of%20the%20designed%20and%20printed%20splints%20using%20profile%20and%20surface%0Adeviation%20analysis.%20The%20proposed%20method%20enables%20reproducible%2C%20patient-specific%0Asplint%20fabrication%20and%20opens%20new%20possibilities%20in%20diagnostics%2C%20multimodal%20image%0Aregistration%20and%20quantification%20of%20occlusal%20discrepancies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12868v1&entry.124074799=Read"},
{"title": "Understanding LLM Behaviors via Compression: Data Generation, Knowledge\n  Acquisition and Scaling Laws", "author": "Zhixuan Pan and Shaowen Wang and Jian Li", "abstract": "  Large Language Models (LLMs) have demonstrated remarkable capabilities across\nnumerous tasks, yet principled explanations for their underlying mechanisms and\nseveral phenomena, such as scaling laws, hallucinations, and related behaviors,\nremain elusive. In this work, we revisit the classical relationship between\ncompression and prediction, grounded in Kolmogorov complexity and Shannon\ninformation theory, to provide deeper insights into LLM behaviors. By\nleveraging the Kolmogorov Structure Function and interpreting LLM compression\nas a two-part coding process, we offer a detailed view of how LLMs acquire and\nstore information across increasing model and data scales -- from pervasive\nsyntactic patterns to progressively rarer knowledge elements. Motivated by this\ntheoretical perspective and natural assumptions inspired by Heap's and Zipf's\nlaws, we introduce a simplified yet representative hierarchical data-generation\nframework called the Syntax-Knowledge model. Under the Bayesian setting, we\nshow that prediction and compression within this model naturally lead to\ndiverse learning and scaling behaviors of LLMs. In particular, our theoretical\nanalysis offers intuitive and principled explanations for both data and model\nscaling laws, the dynamics of knowledge acquisition during training and\nfine-tuning, factual knowledge hallucinations in LLMs. The experimental results\nvalidate our theoretical predictions.\n", "link": "http://arxiv.org/abs/2504.09597v2", "date": "2025-04-17", "relevancy": 2.6836, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5592}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5592}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20LLM%20Behaviors%20via%20Compression%3A%20Data%20Generation%2C%20Knowledge%0A%20%20Acquisition%20and%20Scaling%20Laws&body=Title%3A%20Understanding%20LLM%20Behaviors%20via%20Compression%3A%20Data%20Generation%2C%20Knowledge%0A%20%20Acquisition%20and%20Scaling%20Laws%0AAuthor%3A%20Zhixuan%20Pan%20and%20Shaowen%20Wang%20and%20Jian%20Li%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20across%0Anumerous%20tasks%2C%20yet%20principled%20explanations%20for%20their%20underlying%20mechanisms%20and%0Aseveral%20phenomena%2C%20such%20as%20scaling%20laws%2C%20hallucinations%2C%20and%20related%20behaviors%2C%0Aremain%20elusive.%20In%20this%20work%2C%20we%20revisit%20the%20classical%20relationship%20between%0Acompression%20and%20prediction%2C%20grounded%20in%20Kolmogorov%20complexity%20and%20Shannon%0Ainformation%20theory%2C%20to%20provide%20deeper%20insights%20into%20LLM%20behaviors.%20By%0Aleveraging%20the%20Kolmogorov%20Structure%20Function%20and%20interpreting%20LLM%20compression%0Aas%20a%20two-part%20coding%20process%2C%20we%20offer%20a%20detailed%20view%20of%20how%20LLMs%20acquire%20and%0Astore%20information%20across%20increasing%20model%20and%20data%20scales%20--%20from%20pervasive%0Asyntactic%20patterns%20to%20progressively%20rarer%20knowledge%20elements.%20Motivated%20by%20this%0Atheoretical%20perspective%20and%20natural%20assumptions%20inspired%20by%20Heap%27s%20and%20Zipf%27s%0Alaws%2C%20we%20introduce%20a%20simplified%20yet%20representative%20hierarchical%20data-generation%0Aframework%20called%20the%20Syntax-Knowledge%20model.%20Under%20the%20Bayesian%20setting%2C%20we%0Ashow%20that%20prediction%20and%20compression%20within%20this%20model%20naturally%20lead%20to%0Adiverse%20learning%20and%20scaling%20behaviors%20of%20LLMs.%20In%20particular%2C%20our%20theoretical%0Aanalysis%20offers%20intuitive%20and%20principled%20explanations%20for%20both%20data%20and%20model%0Ascaling%20laws%2C%20the%20dynamics%20of%20knowledge%20acquisition%20during%20training%20and%0Afine-tuning%2C%20factual%20knowledge%20hallucinations%20in%20LLMs.%20The%20experimental%20results%0Avalidate%20our%20theoretical%20predictions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.09597v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520LLM%2520Behaviors%2520via%2520Compression%253A%2520Data%2520Generation%252C%2520Knowledge%250A%2520%2520Acquisition%2520and%2520Scaling%2520Laws%26entry.906535625%3DZhixuan%2520Pan%2520and%2520Shaowen%2520Wang%2520and%2520Jian%2520Li%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%2520across%250Anumerous%2520tasks%252C%2520yet%2520principled%2520explanations%2520for%2520their%2520underlying%2520mechanisms%2520and%250Aseveral%2520phenomena%252C%2520such%2520as%2520scaling%2520laws%252C%2520hallucinations%252C%2520and%2520related%2520behaviors%252C%250Aremain%2520elusive.%2520In%2520this%2520work%252C%2520we%2520revisit%2520the%2520classical%2520relationship%2520between%250Acompression%2520and%2520prediction%252C%2520grounded%2520in%2520Kolmogorov%2520complexity%2520and%2520Shannon%250Ainformation%2520theory%252C%2520to%2520provide%2520deeper%2520insights%2520into%2520LLM%2520behaviors.%2520By%250Aleveraging%2520the%2520Kolmogorov%2520Structure%2520Function%2520and%2520interpreting%2520LLM%2520compression%250Aas%2520a%2520two-part%2520coding%2520process%252C%2520we%2520offer%2520a%2520detailed%2520view%2520of%2520how%2520LLMs%2520acquire%2520and%250Astore%2520information%2520across%2520increasing%2520model%2520and%2520data%2520scales%2520--%2520from%2520pervasive%250Asyntactic%2520patterns%2520to%2520progressively%2520rarer%2520knowledge%2520elements.%2520Motivated%2520by%2520this%250Atheoretical%2520perspective%2520and%2520natural%2520assumptions%2520inspired%2520by%2520Heap%2527s%2520and%2520Zipf%2527s%250Alaws%252C%2520we%2520introduce%2520a%2520simplified%2520yet%2520representative%2520hierarchical%2520data-generation%250Aframework%2520called%2520the%2520Syntax-Knowledge%2520model.%2520Under%2520the%2520Bayesian%2520setting%252C%2520we%250Ashow%2520that%2520prediction%2520and%2520compression%2520within%2520this%2520model%2520naturally%2520lead%2520to%250Adiverse%2520learning%2520and%2520scaling%2520behaviors%2520of%2520LLMs.%2520In%2520particular%252C%2520our%2520theoretical%250Aanalysis%2520offers%2520intuitive%2520and%2520principled%2520explanations%2520for%2520both%2520data%2520and%2520model%250Ascaling%2520laws%252C%2520the%2520dynamics%2520of%2520knowledge%2520acquisition%2520during%2520training%2520and%250Afine-tuning%252C%2520factual%2520knowledge%2520hallucinations%2520in%2520LLMs.%2520The%2520experimental%2520results%250Avalidate%2520our%2520theoretical%2520predictions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.09597v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20LLM%20Behaviors%20via%20Compression%3A%20Data%20Generation%2C%20Knowledge%0A%20%20Acquisition%20and%20Scaling%20Laws&entry.906535625=Zhixuan%20Pan%20and%20Shaowen%20Wang%20and%20Jian%20Li&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20across%0Anumerous%20tasks%2C%20yet%20principled%20explanations%20for%20their%20underlying%20mechanisms%20and%0Aseveral%20phenomena%2C%20such%20as%20scaling%20laws%2C%20hallucinations%2C%20and%20related%20behaviors%2C%0Aremain%20elusive.%20In%20this%20work%2C%20we%20revisit%20the%20classical%20relationship%20between%0Acompression%20and%20prediction%2C%20grounded%20in%20Kolmogorov%20complexity%20and%20Shannon%0Ainformation%20theory%2C%20to%20provide%20deeper%20insights%20into%20LLM%20behaviors.%20By%0Aleveraging%20the%20Kolmogorov%20Structure%20Function%20and%20interpreting%20LLM%20compression%0Aas%20a%20two-part%20coding%20process%2C%20we%20offer%20a%20detailed%20view%20of%20how%20LLMs%20acquire%20and%0Astore%20information%20across%20increasing%20model%20and%20data%20scales%20--%20from%20pervasive%0Asyntactic%20patterns%20to%20progressively%20rarer%20knowledge%20elements.%20Motivated%20by%20this%0Atheoretical%20perspective%20and%20natural%20assumptions%20inspired%20by%20Heap%27s%20and%20Zipf%27s%0Alaws%2C%20we%20introduce%20a%20simplified%20yet%20representative%20hierarchical%20data-generation%0Aframework%20called%20the%20Syntax-Knowledge%20model.%20Under%20the%20Bayesian%20setting%2C%20we%0Ashow%20that%20prediction%20and%20compression%20within%20this%20model%20naturally%20lead%20to%0Adiverse%20learning%20and%20scaling%20behaviors%20of%20LLMs.%20In%20particular%2C%20our%20theoretical%0Aanalysis%20offers%20intuitive%20and%20principled%20explanations%20for%20both%20data%20and%20model%0Ascaling%20laws%2C%20the%20dynamics%20of%20knowledge%20acquisition%20during%20training%20and%0Afine-tuning%2C%20factual%20knowledge%20hallucinations%20in%20LLMs.%20The%20experimental%20results%0Avalidate%20our%20theoretical%20predictions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.09597v2&entry.124074799=Read"},
{"title": "It's All Connected: A Journey Through Test-Time Memorization,\n  Attentional Bias, Retention, and Online Optimization", "author": "Ali Behrouz and Meisam Razaviyayn and Peilin Zhong and Vahab Mirrokni", "abstract": "  Designing efficient and effective architectural backbones has been in the\ncore of research efforts to enhance the capability of foundation models.\nInspired by the human cognitive phenomenon of attentional bias-the natural\ntendency to prioritize certain events or stimuli-we reconceptualize neural\narchitectures, including Transformers, Titans, and modern linear recurrent\nneural networks as associative memory modules that learn a mapping of keys and\nvalues using an internal objective, referred to as attentional bias.\nSurprisingly, we observed that most existing sequence models leverage either\n(1) dot-product similarity, or (2) L2 regression objectives as their\nattentional bias. Going beyond these objectives, we present a set of\nalternative attentional bias configurations along with their effective\napproximations to stabilize their training procedure. We then reinterpret\nforgetting mechanisms in modern deep learning architectures as a form of\nretention regularization, providing a novel set of forget gates for sequence\nmodels. Building upon these insights, we present Miras, a general framework to\ndesign deep learning architectures based on four choices of: (i) associative\nmemory architecture, (ii) attentional bias objective, (iii) retention gate, and\n(iv) memory learning algorithm. We present three novel sequence models-Moneta,\nYaad, and Memora-that go beyond the power of existing linear RNNs while\nmaintaining a fast parallelizable training process. Our experiments show\ndifferent design choices in Miras yield models with varying strengths. For\nexample, certain instances of Miras achieve exceptional performance in special\ntasks such as language modeling, commonsense reasoning, and recall intensive\ntasks, even outperforming Transformers and other modern linear recurrent\nmodels.\n", "link": "http://arxiv.org/abs/2504.13173v1", "date": "2025-04-17", "relevancy": 2.6666, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.537}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5315}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20It%27s%20All%20Connected%3A%20A%20Journey%20Through%20Test-Time%20Memorization%2C%0A%20%20Attentional%20Bias%2C%20Retention%2C%20and%20Online%20Optimization&body=Title%3A%20It%27s%20All%20Connected%3A%20A%20Journey%20Through%20Test-Time%20Memorization%2C%0A%20%20Attentional%20Bias%2C%20Retention%2C%20and%20Online%20Optimization%0AAuthor%3A%20Ali%20Behrouz%20and%20Meisam%20Razaviyayn%20and%20Peilin%20Zhong%20and%20Vahab%20Mirrokni%0AAbstract%3A%20%20%20Designing%20efficient%20and%20effective%20architectural%20backbones%20has%20been%20in%20the%0Acore%20of%20research%20efforts%20to%20enhance%20the%20capability%20of%20foundation%20models.%0AInspired%20by%20the%20human%20cognitive%20phenomenon%20of%20attentional%20bias-the%20natural%0Atendency%20to%20prioritize%20certain%20events%20or%20stimuli-we%20reconceptualize%20neural%0Aarchitectures%2C%20including%20Transformers%2C%20Titans%2C%20and%20modern%20linear%20recurrent%0Aneural%20networks%20as%20associative%20memory%20modules%20that%20learn%20a%20mapping%20of%20keys%20and%0Avalues%20using%20an%20internal%20objective%2C%20referred%20to%20as%20attentional%20bias.%0ASurprisingly%2C%20we%20observed%20that%20most%20existing%20sequence%20models%20leverage%20either%0A%281%29%20dot-product%20similarity%2C%20or%20%282%29%20L2%20regression%20objectives%20as%20their%0Aattentional%20bias.%20Going%20beyond%20these%20objectives%2C%20we%20present%20a%20set%20of%0Aalternative%20attentional%20bias%20configurations%20along%20with%20their%20effective%0Aapproximations%20to%20stabilize%20their%20training%20procedure.%20We%20then%20reinterpret%0Aforgetting%20mechanisms%20in%20modern%20deep%20learning%20architectures%20as%20a%20form%20of%0Aretention%20regularization%2C%20providing%20a%20novel%20set%20of%20forget%20gates%20for%20sequence%0Amodels.%20Building%20upon%20these%20insights%2C%20we%20present%20Miras%2C%20a%20general%20framework%20to%0Adesign%20deep%20learning%20architectures%20based%20on%20four%20choices%20of%3A%20%28i%29%20associative%0Amemory%20architecture%2C%20%28ii%29%20attentional%20bias%20objective%2C%20%28iii%29%20retention%20gate%2C%20and%0A%28iv%29%20memory%20learning%20algorithm.%20We%20present%20three%20novel%20sequence%20models-Moneta%2C%0AYaad%2C%20and%20Memora-that%20go%20beyond%20the%20power%20of%20existing%20linear%20RNNs%20while%0Amaintaining%20a%20fast%20parallelizable%20training%20process.%20Our%20experiments%20show%0Adifferent%20design%20choices%20in%20Miras%20yield%20models%20with%20varying%20strengths.%20For%0Aexample%2C%20certain%20instances%20of%20Miras%20achieve%20exceptional%20performance%20in%20special%0Atasks%20such%20as%20language%20modeling%2C%20commonsense%20reasoning%2C%20and%20recall%20intensive%0Atasks%2C%20even%20outperforming%20Transformers%20and%20other%20modern%20linear%20recurrent%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13173v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIt%2527s%2520All%2520Connected%253A%2520A%2520Journey%2520Through%2520Test-Time%2520Memorization%252C%250A%2520%2520Attentional%2520Bias%252C%2520Retention%252C%2520and%2520Online%2520Optimization%26entry.906535625%3DAli%2520Behrouz%2520and%2520Meisam%2520Razaviyayn%2520and%2520Peilin%2520Zhong%2520and%2520Vahab%2520Mirrokni%26entry.1292438233%3D%2520%2520Designing%2520efficient%2520and%2520effective%2520architectural%2520backbones%2520has%2520been%2520in%2520the%250Acore%2520of%2520research%2520efforts%2520to%2520enhance%2520the%2520capability%2520of%2520foundation%2520models.%250AInspired%2520by%2520the%2520human%2520cognitive%2520phenomenon%2520of%2520attentional%2520bias-the%2520natural%250Atendency%2520to%2520prioritize%2520certain%2520events%2520or%2520stimuli-we%2520reconceptualize%2520neural%250Aarchitectures%252C%2520including%2520Transformers%252C%2520Titans%252C%2520and%2520modern%2520linear%2520recurrent%250Aneural%2520networks%2520as%2520associative%2520memory%2520modules%2520that%2520learn%2520a%2520mapping%2520of%2520keys%2520and%250Avalues%2520using%2520an%2520internal%2520objective%252C%2520referred%2520to%2520as%2520attentional%2520bias.%250ASurprisingly%252C%2520we%2520observed%2520that%2520most%2520existing%2520sequence%2520models%2520leverage%2520either%250A%25281%2529%2520dot-product%2520similarity%252C%2520or%2520%25282%2529%2520L2%2520regression%2520objectives%2520as%2520their%250Aattentional%2520bias.%2520Going%2520beyond%2520these%2520objectives%252C%2520we%2520present%2520a%2520set%2520of%250Aalternative%2520attentional%2520bias%2520configurations%2520along%2520with%2520their%2520effective%250Aapproximations%2520to%2520stabilize%2520their%2520training%2520procedure.%2520We%2520then%2520reinterpret%250Aforgetting%2520mechanisms%2520in%2520modern%2520deep%2520learning%2520architectures%2520as%2520a%2520form%2520of%250Aretention%2520regularization%252C%2520providing%2520a%2520novel%2520set%2520of%2520forget%2520gates%2520for%2520sequence%250Amodels.%2520Building%2520upon%2520these%2520insights%252C%2520we%2520present%2520Miras%252C%2520a%2520general%2520framework%2520to%250Adesign%2520deep%2520learning%2520architectures%2520based%2520on%2520four%2520choices%2520of%253A%2520%2528i%2529%2520associative%250Amemory%2520architecture%252C%2520%2528ii%2529%2520attentional%2520bias%2520objective%252C%2520%2528iii%2529%2520retention%2520gate%252C%2520and%250A%2528iv%2529%2520memory%2520learning%2520algorithm.%2520We%2520present%2520three%2520novel%2520sequence%2520models-Moneta%252C%250AYaad%252C%2520and%2520Memora-that%2520go%2520beyond%2520the%2520power%2520of%2520existing%2520linear%2520RNNs%2520while%250Amaintaining%2520a%2520fast%2520parallelizable%2520training%2520process.%2520Our%2520experiments%2520show%250Adifferent%2520design%2520choices%2520in%2520Miras%2520yield%2520models%2520with%2520varying%2520strengths.%2520For%250Aexample%252C%2520certain%2520instances%2520of%2520Miras%2520achieve%2520exceptional%2520performance%2520in%2520special%250Atasks%2520such%2520as%2520language%2520modeling%252C%2520commonsense%2520reasoning%252C%2520and%2520recall%2520intensive%250Atasks%252C%2520even%2520outperforming%2520Transformers%2520and%2520other%2520modern%2520linear%2520recurrent%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13173v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=It%27s%20All%20Connected%3A%20A%20Journey%20Through%20Test-Time%20Memorization%2C%0A%20%20Attentional%20Bias%2C%20Retention%2C%20and%20Online%20Optimization&entry.906535625=Ali%20Behrouz%20and%20Meisam%20Razaviyayn%20and%20Peilin%20Zhong%20and%20Vahab%20Mirrokni&entry.1292438233=%20%20Designing%20efficient%20and%20effective%20architectural%20backbones%20has%20been%20in%20the%0Acore%20of%20research%20efforts%20to%20enhance%20the%20capability%20of%20foundation%20models.%0AInspired%20by%20the%20human%20cognitive%20phenomenon%20of%20attentional%20bias-the%20natural%0Atendency%20to%20prioritize%20certain%20events%20or%20stimuli-we%20reconceptualize%20neural%0Aarchitectures%2C%20including%20Transformers%2C%20Titans%2C%20and%20modern%20linear%20recurrent%0Aneural%20networks%20as%20associative%20memory%20modules%20that%20learn%20a%20mapping%20of%20keys%20and%0Avalues%20using%20an%20internal%20objective%2C%20referred%20to%20as%20attentional%20bias.%0ASurprisingly%2C%20we%20observed%20that%20most%20existing%20sequence%20models%20leverage%20either%0A%281%29%20dot-product%20similarity%2C%20or%20%282%29%20L2%20regression%20objectives%20as%20their%0Aattentional%20bias.%20Going%20beyond%20these%20objectives%2C%20we%20present%20a%20set%20of%0Aalternative%20attentional%20bias%20configurations%20along%20with%20their%20effective%0Aapproximations%20to%20stabilize%20their%20training%20procedure.%20We%20then%20reinterpret%0Aforgetting%20mechanisms%20in%20modern%20deep%20learning%20architectures%20as%20a%20form%20of%0Aretention%20regularization%2C%20providing%20a%20novel%20set%20of%20forget%20gates%20for%20sequence%0Amodels.%20Building%20upon%20these%20insights%2C%20we%20present%20Miras%2C%20a%20general%20framework%20to%0Adesign%20deep%20learning%20architectures%20based%20on%20four%20choices%20of%3A%20%28i%29%20associative%0Amemory%20architecture%2C%20%28ii%29%20attentional%20bias%20objective%2C%20%28iii%29%20retention%20gate%2C%20and%0A%28iv%29%20memory%20learning%20algorithm.%20We%20present%20three%20novel%20sequence%20models-Moneta%2C%0AYaad%2C%20and%20Memora-that%20go%20beyond%20the%20power%20of%20existing%20linear%20RNNs%20while%0Amaintaining%20a%20fast%20parallelizable%20training%20process.%20Our%20experiments%20show%0Adifferent%20design%20choices%20in%20Miras%20yield%20models%20with%20varying%20strengths.%20For%0Aexample%2C%20certain%20instances%20of%20Miras%20achieve%20exceptional%20performance%20in%20special%0Atasks%20such%20as%20language%20modeling%2C%20commonsense%20reasoning%2C%20and%20recall%20intensive%0Atasks%2C%20even%20outperforming%20Transformers%20and%20other%20modern%20linear%20recurrent%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13173v1&entry.124074799=Read"},
{"title": "Hierarchical Feature Learning for Medical Point Clouds via State Space\n  Model", "author": "Guoqing Zhang and Jingyun Yang and Yang Li", "abstract": "  Deep learning-based point cloud modeling has been widely investigated as an\nindispensable component of general shape analysis. Recently, transformer and\nstate space model (SSM) have shown promising capacities in point cloud\nlearning. However, limited research has been conducted on medical point clouds,\nwhich have great potential in disease diagnosis and treatment. This paper\npresents an SSM-based hierarchical feature learning framework for medical point\ncloud understanding. Specifically, we down-sample input into multiple levels\nthrough the farthest point sampling. At each level, we perform a series of\nk-nearest neighbor (KNN) queries to aggregate multi-scale structural\ninformation. To assist SSM in processing point clouds, we introduce\ncoordinate-order and inside-out scanning strategies for efficient serialization\nof irregular points. Point features are calculated progressively from short\nneighbor sequences and long point sequences through vanilla and group Point SSM\nblocks, to capture both local patterns and long-range dependencies. To evaluate\nthe proposed method, we build a large-scale medical point cloud dataset named\nMedPointS for anatomy classification, completion, and segmentation. Extensive\nexperiments conducted on MedPointS demonstrate that our method achieves\nsuperior performance across all tasks. The dataset is available at\nhttps://flemme-docs.readthedocs.io/en/latest/medpoints.html. Code is merged to\na public medical imaging platform: https://github.com/wlsdzyzl/flemme.\n", "link": "http://arxiv.org/abs/2504.13015v1", "date": "2025-04-17", "relevancy": 2.6612, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5629}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5192}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Feature%20Learning%20for%20Medical%20Point%20Clouds%20via%20State%20Space%0A%20%20Model&body=Title%3A%20Hierarchical%20Feature%20Learning%20for%20Medical%20Point%20Clouds%20via%20State%20Space%0A%20%20Model%0AAuthor%3A%20Guoqing%20Zhang%20and%20Jingyun%20Yang%20and%20Yang%20Li%0AAbstract%3A%20%20%20Deep%20learning-based%20point%20cloud%20modeling%20has%20been%20widely%20investigated%20as%20an%0Aindispensable%20component%20of%20general%20shape%20analysis.%20Recently%2C%20transformer%20and%0Astate%20space%20model%20%28SSM%29%20have%20shown%20promising%20capacities%20in%20point%20cloud%0Alearning.%20However%2C%20limited%20research%20has%20been%20conducted%20on%20medical%20point%20clouds%2C%0Awhich%20have%20great%20potential%20in%20disease%20diagnosis%20and%20treatment.%20This%20paper%0Apresents%20an%20SSM-based%20hierarchical%20feature%20learning%20framework%20for%20medical%20point%0Acloud%20understanding.%20Specifically%2C%20we%20down-sample%20input%20into%20multiple%20levels%0Athrough%20the%20farthest%20point%20sampling.%20At%20each%20level%2C%20we%20perform%20a%20series%20of%0Ak-nearest%20neighbor%20%28KNN%29%20queries%20to%20aggregate%20multi-scale%20structural%0Ainformation.%20To%20assist%20SSM%20in%20processing%20point%20clouds%2C%20we%20introduce%0Acoordinate-order%20and%20inside-out%20scanning%20strategies%20for%20efficient%20serialization%0Aof%20irregular%20points.%20Point%20features%20are%20calculated%20progressively%20from%20short%0Aneighbor%20sequences%20and%20long%20point%20sequences%20through%20vanilla%20and%20group%20Point%20SSM%0Ablocks%2C%20to%20capture%20both%20local%20patterns%20and%20long-range%20dependencies.%20To%20evaluate%0Athe%20proposed%20method%2C%20we%20build%20a%20large-scale%20medical%20point%20cloud%20dataset%20named%0AMedPointS%20for%20anatomy%20classification%2C%20completion%2C%20and%20segmentation.%20Extensive%0Aexperiments%20conducted%20on%20MedPointS%20demonstrate%20that%20our%20method%20achieves%0Asuperior%20performance%20across%20all%20tasks.%20The%20dataset%20is%20available%20at%0Ahttps%3A//flemme-docs.readthedocs.io/en/latest/medpoints.html.%20Code%20is%20merged%20to%0Aa%20public%20medical%20imaging%20platform%3A%20https%3A//github.com/wlsdzyzl/flemme.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13015v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Feature%2520Learning%2520for%2520Medical%2520Point%2520Clouds%2520via%2520State%2520Space%250A%2520%2520Model%26entry.906535625%3DGuoqing%2520Zhang%2520and%2520Jingyun%2520Yang%2520and%2520Yang%2520Li%26entry.1292438233%3D%2520%2520Deep%2520learning-based%2520point%2520cloud%2520modeling%2520has%2520been%2520widely%2520investigated%2520as%2520an%250Aindispensable%2520component%2520of%2520general%2520shape%2520analysis.%2520Recently%252C%2520transformer%2520and%250Astate%2520space%2520model%2520%2528SSM%2529%2520have%2520shown%2520promising%2520capacities%2520in%2520point%2520cloud%250Alearning.%2520However%252C%2520limited%2520research%2520has%2520been%2520conducted%2520on%2520medical%2520point%2520clouds%252C%250Awhich%2520have%2520great%2520potential%2520in%2520disease%2520diagnosis%2520and%2520treatment.%2520This%2520paper%250Apresents%2520an%2520SSM-based%2520hierarchical%2520feature%2520learning%2520framework%2520for%2520medical%2520point%250Acloud%2520understanding.%2520Specifically%252C%2520we%2520down-sample%2520input%2520into%2520multiple%2520levels%250Athrough%2520the%2520farthest%2520point%2520sampling.%2520At%2520each%2520level%252C%2520we%2520perform%2520a%2520series%2520of%250Ak-nearest%2520neighbor%2520%2528KNN%2529%2520queries%2520to%2520aggregate%2520multi-scale%2520structural%250Ainformation.%2520To%2520assist%2520SSM%2520in%2520processing%2520point%2520clouds%252C%2520we%2520introduce%250Acoordinate-order%2520and%2520inside-out%2520scanning%2520strategies%2520for%2520efficient%2520serialization%250Aof%2520irregular%2520points.%2520Point%2520features%2520are%2520calculated%2520progressively%2520from%2520short%250Aneighbor%2520sequences%2520and%2520long%2520point%2520sequences%2520through%2520vanilla%2520and%2520group%2520Point%2520SSM%250Ablocks%252C%2520to%2520capture%2520both%2520local%2520patterns%2520and%2520long-range%2520dependencies.%2520To%2520evaluate%250Athe%2520proposed%2520method%252C%2520we%2520build%2520a%2520large-scale%2520medical%2520point%2520cloud%2520dataset%2520named%250AMedPointS%2520for%2520anatomy%2520classification%252C%2520completion%252C%2520and%2520segmentation.%2520Extensive%250Aexperiments%2520conducted%2520on%2520MedPointS%2520demonstrate%2520that%2520our%2520method%2520achieves%250Asuperior%2520performance%2520across%2520all%2520tasks.%2520The%2520dataset%2520is%2520available%2520at%250Ahttps%253A//flemme-docs.readthedocs.io/en/latest/medpoints.html.%2520Code%2520is%2520merged%2520to%250Aa%2520public%2520medical%2520imaging%2520platform%253A%2520https%253A//github.com/wlsdzyzl/flemme.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13015v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Feature%20Learning%20for%20Medical%20Point%20Clouds%20via%20State%20Space%0A%20%20Model&entry.906535625=Guoqing%20Zhang%20and%20Jingyun%20Yang%20and%20Yang%20Li&entry.1292438233=%20%20Deep%20learning-based%20point%20cloud%20modeling%20has%20been%20widely%20investigated%20as%20an%0Aindispensable%20component%20of%20general%20shape%20analysis.%20Recently%2C%20transformer%20and%0Astate%20space%20model%20%28SSM%29%20have%20shown%20promising%20capacities%20in%20point%20cloud%0Alearning.%20However%2C%20limited%20research%20has%20been%20conducted%20on%20medical%20point%20clouds%2C%0Awhich%20have%20great%20potential%20in%20disease%20diagnosis%20and%20treatment.%20This%20paper%0Apresents%20an%20SSM-based%20hierarchical%20feature%20learning%20framework%20for%20medical%20point%0Acloud%20understanding.%20Specifically%2C%20we%20down-sample%20input%20into%20multiple%20levels%0Athrough%20the%20farthest%20point%20sampling.%20At%20each%20level%2C%20we%20perform%20a%20series%20of%0Ak-nearest%20neighbor%20%28KNN%29%20queries%20to%20aggregate%20multi-scale%20structural%0Ainformation.%20To%20assist%20SSM%20in%20processing%20point%20clouds%2C%20we%20introduce%0Acoordinate-order%20and%20inside-out%20scanning%20strategies%20for%20efficient%20serialization%0Aof%20irregular%20points.%20Point%20features%20are%20calculated%20progressively%20from%20short%0Aneighbor%20sequences%20and%20long%20point%20sequences%20through%20vanilla%20and%20group%20Point%20SSM%0Ablocks%2C%20to%20capture%20both%20local%20patterns%20and%20long-range%20dependencies.%20To%20evaluate%0Athe%20proposed%20method%2C%20we%20build%20a%20large-scale%20medical%20point%20cloud%20dataset%20named%0AMedPointS%20for%20anatomy%20classification%2C%20completion%2C%20and%20segmentation.%20Extensive%0Aexperiments%20conducted%20on%20MedPointS%20demonstrate%20that%20our%20method%20achieves%0Asuperior%20performance%20across%20all%20tasks.%20The%20dataset%20is%20available%20at%0Ahttps%3A//flemme-docs.readthedocs.io/en/latest/medpoints.html.%20Code%20is%20merged%20to%0Aa%20public%20medical%20imaging%20platform%3A%20https%3A//github.com/wlsdzyzl/flemme.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13015v1&entry.124074799=Read"},
{"title": "Enhancing Person-to-Person Virtual Try-On with Multi-Garment Virtual\n  Try-Off", "author": "Riza Velioglu and Petra Bevandic and Robin Chan and Barbara Hammer", "abstract": "  Computer vision is transforming fashion through Virtual Try-On (VTON) and\nVirtual Try-Off (VTOFF). VTON generates images of a person in a specified\ngarment using a target photo and a standardized garment image, while a more\nchallenging variant, Person-to-Person Virtual Try-On (p2p-VTON), uses a photo\nof another person wearing the garment. VTOFF, on the other hand, extracts\nstandardized garment images from clothed individuals. We introduce TryOffDiff,\na diffusion-based VTOFF model. Built on a latent diffusion framework with\nSigLIP image conditioning, it effectively captures garment properties like\ntexture, shape, and patterns. TryOffDiff achieves state-of-the-art results on\nVITON-HD and strong performance on DressCode dataset, covering upper-body,\nlower-body, and dresses. Enhanced with class-specific embeddings, it pioneers\nmulti-garment VTOFF, the first of its kind. When paired with VTON models, it\nimproves p2p-VTON by minimizing unwanted attribute transfer, such as skin\ncolor. Code is available at: https://rizavelioglu.github.io/tryoffdiff/\n", "link": "http://arxiv.org/abs/2504.13078v1", "date": "2025-04-17", "relevancy": 2.654, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6653}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6645}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Person-to-Person%20Virtual%20Try-On%20with%20Multi-Garment%20Virtual%0A%20%20Try-Off&body=Title%3A%20Enhancing%20Person-to-Person%20Virtual%20Try-On%20with%20Multi-Garment%20Virtual%0A%20%20Try-Off%0AAuthor%3A%20Riza%20Velioglu%20and%20Petra%20Bevandic%20and%20Robin%20Chan%20and%20Barbara%20Hammer%0AAbstract%3A%20%20%20Computer%20vision%20is%20transforming%20fashion%20through%20Virtual%20Try-On%20%28VTON%29%20and%0AVirtual%20Try-Off%20%28VTOFF%29.%20VTON%20generates%20images%20of%20a%20person%20in%20a%20specified%0Agarment%20using%20a%20target%20photo%20and%20a%20standardized%20garment%20image%2C%20while%20a%20more%0Achallenging%20variant%2C%20Person-to-Person%20Virtual%20Try-On%20%28p2p-VTON%29%2C%20uses%20a%20photo%0Aof%20another%20person%20wearing%20the%20garment.%20VTOFF%2C%20on%20the%20other%20hand%2C%20extracts%0Astandardized%20garment%20images%20from%20clothed%20individuals.%20We%20introduce%20TryOffDiff%2C%0Aa%20diffusion-based%20VTOFF%20model.%20Built%20on%20a%20latent%20diffusion%20framework%20with%0ASigLIP%20image%20conditioning%2C%20it%20effectively%20captures%20garment%20properties%20like%0Atexture%2C%20shape%2C%20and%20patterns.%20TryOffDiff%20achieves%20state-of-the-art%20results%20on%0AVITON-HD%20and%20strong%20performance%20on%20DressCode%20dataset%2C%20covering%20upper-body%2C%0Alower-body%2C%20and%20dresses.%20Enhanced%20with%20class-specific%20embeddings%2C%20it%20pioneers%0Amulti-garment%20VTOFF%2C%20the%20first%20of%20its%20kind.%20When%20paired%20with%20VTON%20models%2C%20it%0Aimproves%20p2p-VTON%20by%20minimizing%20unwanted%20attribute%20transfer%2C%20such%20as%20skin%0Acolor.%20Code%20is%20available%20at%3A%20https%3A//rizavelioglu.github.io/tryoffdiff/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13078v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Person-to-Person%2520Virtual%2520Try-On%2520with%2520Multi-Garment%2520Virtual%250A%2520%2520Try-Off%26entry.906535625%3DRiza%2520Velioglu%2520and%2520Petra%2520Bevandic%2520and%2520Robin%2520Chan%2520and%2520Barbara%2520Hammer%26entry.1292438233%3D%2520%2520Computer%2520vision%2520is%2520transforming%2520fashion%2520through%2520Virtual%2520Try-On%2520%2528VTON%2529%2520and%250AVirtual%2520Try-Off%2520%2528VTOFF%2529.%2520VTON%2520generates%2520images%2520of%2520a%2520person%2520in%2520a%2520specified%250Agarment%2520using%2520a%2520target%2520photo%2520and%2520a%2520standardized%2520garment%2520image%252C%2520while%2520a%2520more%250Achallenging%2520variant%252C%2520Person-to-Person%2520Virtual%2520Try-On%2520%2528p2p-VTON%2529%252C%2520uses%2520a%2520photo%250Aof%2520another%2520person%2520wearing%2520the%2520garment.%2520VTOFF%252C%2520on%2520the%2520other%2520hand%252C%2520extracts%250Astandardized%2520garment%2520images%2520from%2520clothed%2520individuals.%2520We%2520introduce%2520TryOffDiff%252C%250Aa%2520diffusion-based%2520VTOFF%2520model.%2520Built%2520on%2520a%2520latent%2520diffusion%2520framework%2520with%250ASigLIP%2520image%2520conditioning%252C%2520it%2520effectively%2520captures%2520garment%2520properties%2520like%250Atexture%252C%2520shape%252C%2520and%2520patterns.%2520TryOffDiff%2520achieves%2520state-of-the-art%2520results%2520on%250AVITON-HD%2520and%2520strong%2520performance%2520on%2520DressCode%2520dataset%252C%2520covering%2520upper-body%252C%250Alower-body%252C%2520and%2520dresses.%2520Enhanced%2520with%2520class-specific%2520embeddings%252C%2520it%2520pioneers%250Amulti-garment%2520VTOFF%252C%2520the%2520first%2520of%2520its%2520kind.%2520When%2520paired%2520with%2520VTON%2520models%252C%2520it%250Aimproves%2520p2p-VTON%2520by%2520minimizing%2520unwanted%2520attribute%2520transfer%252C%2520such%2520as%2520skin%250Acolor.%2520Code%2520is%2520available%2520at%253A%2520https%253A//rizavelioglu.github.io/tryoffdiff/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13078v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Person-to-Person%20Virtual%20Try-On%20with%20Multi-Garment%20Virtual%0A%20%20Try-Off&entry.906535625=Riza%20Velioglu%20and%20Petra%20Bevandic%20and%20Robin%20Chan%20and%20Barbara%20Hammer&entry.1292438233=%20%20Computer%20vision%20is%20transforming%20fashion%20through%20Virtual%20Try-On%20%28VTON%29%20and%0AVirtual%20Try-Off%20%28VTOFF%29.%20VTON%20generates%20images%20of%20a%20person%20in%20a%20specified%0Agarment%20using%20a%20target%20photo%20and%20a%20standardized%20garment%20image%2C%20while%20a%20more%0Achallenging%20variant%2C%20Person-to-Person%20Virtual%20Try-On%20%28p2p-VTON%29%2C%20uses%20a%20photo%0Aof%20another%20person%20wearing%20the%20garment.%20VTOFF%2C%20on%20the%20other%20hand%2C%20extracts%0Astandardized%20garment%20images%20from%20clothed%20individuals.%20We%20introduce%20TryOffDiff%2C%0Aa%20diffusion-based%20VTOFF%20model.%20Built%20on%20a%20latent%20diffusion%20framework%20with%0ASigLIP%20image%20conditioning%2C%20it%20effectively%20captures%20garment%20properties%20like%0Atexture%2C%20shape%2C%20and%20patterns.%20TryOffDiff%20achieves%20state-of-the-art%20results%20on%0AVITON-HD%20and%20strong%20performance%20on%20DressCode%20dataset%2C%20covering%20upper-body%2C%0Alower-body%2C%20and%20dresses.%20Enhanced%20with%20class-specific%20embeddings%2C%20it%20pioneers%0Amulti-garment%20VTOFF%2C%20the%20first%20of%20its%20kind.%20When%20paired%20with%20VTON%20models%2C%20it%0Aimproves%20p2p-VTON%20by%20minimizing%20unwanted%20attribute%20transfer%2C%20such%20as%20skin%0Acolor.%20Code%20is%20available%20at%3A%20https%3A//rizavelioglu.github.io/tryoffdiff/%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13078v1&entry.124074799=Read"},
{"title": "Large Language Models as Attribution Regularizers for Efficient Model\n  Training", "author": "Davor Vukadin and Marin \u0160ili\u0107 and Goran Dela\u010d", "abstract": "  Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse domains. However, effectively leveraging their vast knowledge for\ntraining smaller downstream models remains an open challenge, especially in\ndomains like tabular data learning, where simpler models are often preferred\ndue to interpretability and efficiency.\n  In this paper, we introduce a novel yet straightforward method for\nincorporating LLM-generated global task feature attributions into the training\nprocess of smaller networks. Specifically, we propose an attribution-matching\nregularization term that aligns the training dynamics of the smaller model with\nthe insights provided by the LLM. By doing so, our approach yields superior\nperformance in few-shot learning scenarios. Notably, our method requires only\nblack-box API access to the LLM, making it easy to integrate into existing\ntraining pipelines with minimal computational overhead.\n  Furthermore, we demonstrate how this method can be used to address common\nissues in real-world datasets, such as skewness and bias. By integrating\nhigh-level knowledge from LLMs, our approach improves generalization, even when\ntraining data is limited or imbalanced. We validate its effectiveness through\nextensive experiments across multiple tasks, demonstrating improved learning\nefficiency and model robustness.\n", "link": "http://arxiv.org/abs/2502.20268v2", "date": "2025-04-17", "relevancy": 2.6084, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5445}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5121}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20as%20Attribution%20Regularizers%20for%20Efficient%20Model%0A%20%20Training&body=Title%3A%20Large%20Language%20Models%20as%20Attribution%20Regularizers%20for%20Efficient%20Model%0A%20%20Training%0AAuthor%3A%20Davor%20Vukadin%20and%20Marin%20%C5%A0ili%C4%87%20and%20Goran%20Dela%C4%8D%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20performance%20across%0Adiverse%20domains.%20However%2C%20effectively%20leveraging%20their%20vast%20knowledge%20for%0Atraining%20smaller%20downstream%20models%20remains%20an%20open%20challenge%2C%20especially%20in%0Adomains%20like%20tabular%20data%20learning%2C%20where%20simpler%20models%20are%20often%20preferred%0Adue%20to%20interpretability%20and%20efficiency.%0A%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20yet%20straightforward%20method%20for%0Aincorporating%20LLM-generated%20global%20task%20feature%20attributions%20into%20the%20training%0Aprocess%20of%20smaller%20networks.%20Specifically%2C%20we%20propose%20an%20attribution-matching%0Aregularization%20term%20that%20aligns%20the%20training%20dynamics%20of%20the%20smaller%20model%20with%0Athe%20insights%20provided%20by%20the%20LLM.%20By%20doing%20so%2C%20our%20approach%20yields%20superior%0Aperformance%20in%20few-shot%20learning%20scenarios.%20Notably%2C%20our%20method%20requires%20only%0Ablack-box%20API%20access%20to%20the%20LLM%2C%20making%20it%20easy%20to%20integrate%20into%20existing%0Atraining%20pipelines%20with%20minimal%20computational%20overhead.%0A%20%20Furthermore%2C%20we%20demonstrate%20how%20this%20method%20can%20be%20used%20to%20address%20common%0Aissues%20in%20real-world%20datasets%2C%20such%20as%20skewness%20and%20bias.%20By%20integrating%0Ahigh-level%20knowledge%20from%20LLMs%2C%20our%20approach%20improves%20generalization%2C%20even%20when%0Atraining%20data%20is%20limited%20or%20imbalanced.%20We%20validate%20its%20effectiveness%20through%0Aextensive%20experiments%20across%20multiple%20tasks%2C%20demonstrating%20improved%20learning%0Aefficiency%20and%20model%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.20268v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520as%2520Attribution%2520Regularizers%2520for%2520Efficient%2520Model%250A%2520%2520Training%26entry.906535625%3DDavor%2520Vukadin%2520and%2520Marin%2520%25C5%25A0ili%25C4%2587%2520and%2520Goran%2520Dela%25C4%258D%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520performance%2520across%250Adiverse%2520domains.%2520However%252C%2520effectively%2520leveraging%2520their%2520vast%2520knowledge%2520for%250Atraining%2520smaller%2520downstream%2520models%2520remains%2520an%2520open%2520challenge%252C%2520especially%2520in%250Adomains%2520like%2520tabular%2520data%2520learning%252C%2520where%2520simpler%2520models%2520are%2520often%2520preferred%250Adue%2520to%2520interpretability%2520and%2520efficiency.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520yet%2520straightforward%2520method%2520for%250Aincorporating%2520LLM-generated%2520global%2520task%2520feature%2520attributions%2520into%2520the%2520training%250Aprocess%2520of%2520smaller%2520networks.%2520Specifically%252C%2520we%2520propose%2520an%2520attribution-matching%250Aregularization%2520term%2520that%2520aligns%2520the%2520training%2520dynamics%2520of%2520the%2520smaller%2520model%2520with%250Athe%2520insights%2520provided%2520by%2520the%2520LLM.%2520By%2520doing%2520so%252C%2520our%2520approach%2520yields%2520superior%250Aperformance%2520in%2520few-shot%2520learning%2520scenarios.%2520Notably%252C%2520our%2520method%2520requires%2520only%250Ablack-box%2520API%2520access%2520to%2520the%2520LLM%252C%2520making%2520it%2520easy%2520to%2520integrate%2520into%2520existing%250Atraining%2520pipelines%2520with%2520minimal%2520computational%2520overhead.%250A%2520%2520Furthermore%252C%2520we%2520demonstrate%2520how%2520this%2520method%2520can%2520be%2520used%2520to%2520address%2520common%250Aissues%2520in%2520real-world%2520datasets%252C%2520such%2520as%2520skewness%2520and%2520bias.%2520By%2520integrating%250Ahigh-level%2520knowledge%2520from%2520LLMs%252C%2520our%2520approach%2520improves%2520generalization%252C%2520even%2520when%250Atraining%2520data%2520is%2520limited%2520or%2520imbalanced.%2520We%2520validate%2520its%2520effectiveness%2520through%250Aextensive%2520experiments%2520across%2520multiple%2520tasks%252C%2520demonstrating%2520improved%2520learning%250Aefficiency%2520and%2520model%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.20268v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20as%20Attribution%20Regularizers%20for%20Efficient%20Model%0A%20%20Training&entry.906535625=Davor%20Vukadin%20and%20Marin%20%C5%A0ili%C4%87%20and%20Goran%20Dela%C4%8D&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20performance%20across%0Adiverse%20domains.%20However%2C%20effectively%20leveraging%20their%20vast%20knowledge%20for%0Atraining%20smaller%20downstream%20models%20remains%20an%20open%20challenge%2C%20especially%20in%0Adomains%20like%20tabular%20data%20learning%2C%20where%20simpler%20models%20are%20often%20preferred%0Adue%20to%20interpretability%20and%20efficiency.%0A%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20yet%20straightforward%20method%20for%0Aincorporating%20LLM-generated%20global%20task%20feature%20attributions%20into%20the%20training%0Aprocess%20of%20smaller%20networks.%20Specifically%2C%20we%20propose%20an%20attribution-matching%0Aregularization%20term%20that%20aligns%20the%20training%20dynamics%20of%20the%20smaller%20model%20with%0Athe%20insights%20provided%20by%20the%20LLM.%20By%20doing%20so%2C%20our%20approach%20yields%20superior%0Aperformance%20in%20few-shot%20learning%20scenarios.%20Notably%2C%20our%20method%20requires%20only%0Ablack-box%20API%20access%20to%20the%20LLM%2C%20making%20it%20easy%20to%20integrate%20into%20existing%0Atraining%20pipelines%20with%20minimal%20computational%20overhead.%0A%20%20Furthermore%2C%20we%20demonstrate%20how%20this%20method%20can%20be%20used%20to%20address%20common%0Aissues%20in%20real-world%20datasets%2C%20such%20as%20skewness%20and%20bias.%20By%20integrating%0Ahigh-level%20knowledge%20from%20LLMs%2C%20our%20approach%20improves%20generalization%2C%20even%20when%0Atraining%20data%20is%20limited%20or%20imbalanced.%20We%20validate%20its%20effectiveness%20through%0Aextensive%20experiments%20across%20multiple%20tasks%2C%20demonstrating%20improved%20learning%0Aefficiency%20and%20model%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.20268v2&entry.124074799=Read"},
{"title": "A Coding-Theoretic Analysis of Hyperspherical Prototypical Learning\n  Geometry", "author": "Martin Lindstr\u00f6m and Borja Rodr\u00edguez-G\u00e1lvez and Ragnar Thobaben and Mikael Skoglund", "abstract": "  Hyperspherical Prototypical Learning (HPL) is a supervised approach to\nrepresentation learning that designs class prototypes on the unit hypersphere.\nThe prototypes bias the representations to class separation in a scale\ninvariant and known geometry. Previous approaches to HPL have either of the\nfollowing shortcomings: (i) they follow an unprincipled optimisation procedure;\nor (ii) they are theoretically sound, but are constrained to only one possible\nlatent dimension. In this paper, we address both shortcomings. To address (i),\nwe present a principled optimisation procedure whose solution we show is\noptimal. To address (ii), we construct well-separated prototypes in a wide\nrange of dimensions using linear block codes. Additionally, we give a full\ncharacterisation of the optimal prototype placement in terms of achievable and\nconverse bounds, showing that our proposed methods are near-optimal.\n", "link": "http://arxiv.org/abs/2407.07664v2", "date": "2025-04-17", "relevancy": 2.5933, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5491}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Coding-Theoretic%20Analysis%20of%20Hyperspherical%20Prototypical%20Learning%0A%20%20Geometry&body=Title%3A%20A%20Coding-Theoretic%20Analysis%20of%20Hyperspherical%20Prototypical%20Learning%0A%20%20Geometry%0AAuthor%3A%20Martin%20Lindstr%C3%B6m%20and%20Borja%20Rodr%C3%ADguez-G%C3%A1lvez%20and%20Ragnar%20Thobaben%20and%20Mikael%20Skoglund%0AAbstract%3A%20%20%20Hyperspherical%20Prototypical%20Learning%20%28HPL%29%20is%20a%20supervised%20approach%20to%0Arepresentation%20learning%20that%20designs%20class%20prototypes%20on%20the%20unit%20hypersphere.%0AThe%20prototypes%20bias%20the%20representations%20to%20class%20separation%20in%20a%20scale%0Ainvariant%20and%20known%20geometry.%20Previous%20approaches%20to%20HPL%20have%20either%20of%20the%0Afollowing%20shortcomings%3A%20%28i%29%20they%20follow%20an%20unprincipled%20optimisation%20procedure%3B%0Aor%20%28ii%29%20they%20are%20theoretically%20sound%2C%20but%20are%20constrained%20to%20only%20one%20possible%0Alatent%20dimension.%20In%20this%20paper%2C%20we%20address%20both%20shortcomings.%20To%20address%20%28i%29%2C%0Awe%20present%20a%20principled%20optimisation%20procedure%20whose%20solution%20we%20show%20is%0Aoptimal.%20To%20address%20%28ii%29%2C%20we%20construct%20well-separated%20prototypes%20in%20a%20wide%0Arange%20of%20dimensions%20using%20linear%20block%20codes.%20Additionally%2C%20we%20give%20a%20full%0Acharacterisation%20of%20the%20optimal%20prototype%20placement%20in%20terms%20of%20achievable%20and%0Aconverse%20bounds%2C%20showing%20that%20our%20proposed%20methods%20are%20near-optimal.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07664v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Coding-Theoretic%2520Analysis%2520of%2520Hyperspherical%2520Prototypical%2520Learning%250A%2520%2520Geometry%26entry.906535625%3DMartin%2520Lindstr%25C3%25B6m%2520and%2520Borja%2520Rodr%25C3%25ADguez-G%25C3%25A1lvez%2520and%2520Ragnar%2520Thobaben%2520and%2520Mikael%2520Skoglund%26entry.1292438233%3D%2520%2520Hyperspherical%2520Prototypical%2520Learning%2520%2528HPL%2529%2520is%2520a%2520supervised%2520approach%2520to%250Arepresentation%2520learning%2520that%2520designs%2520class%2520prototypes%2520on%2520the%2520unit%2520hypersphere.%250AThe%2520prototypes%2520bias%2520the%2520representations%2520to%2520class%2520separation%2520in%2520a%2520scale%250Ainvariant%2520and%2520known%2520geometry.%2520Previous%2520approaches%2520to%2520HPL%2520have%2520either%2520of%2520the%250Afollowing%2520shortcomings%253A%2520%2528i%2529%2520they%2520follow%2520an%2520unprincipled%2520optimisation%2520procedure%253B%250Aor%2520%2528ii%2529%2520they%2520are%2520theoretically%2520sound%252C%2520but%2520are%2520constrained%2520to%2520only%2520one%2520possible%250Alatent%2520dimension.%2520In%2520this%2520paper%252C%2520we%2520address%2520both%2520shortcomings.%2520To%2520address%2520%2528i%2529%252C%250Awe%2520present%2520a%2520principled%2520optimisation%2520procedure%2520whose%2520solution%2520we%2520show%2520is%250Aoptimal.%2520To%2520address%2520%2528ii%2529%252C%2520we%2520construct%2520well-separated%2520prototypes%2520in%2520a%2520wide%250Arange%2520of%2520dimensions%2520using%2520linear%2520block%2520codes.%2520Additionally%252C%2520we%2520give%2520a%2520full%250Acharacterisation%2520of%2520the%2520optimal%2520prototype%2520placement%2520in%2520terms%2520of%2520achievable%2520and%250Aconverse%2520bounds%252C%2520showing%2520that%2520our%2520proposed%2520methods%2520are%2520near-optimal.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07664v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Coding-Theoretic%20Analysis%20of%20Hyperspherical%20Prototypical%20Learning%0A%20%20Geometry&entry.906535625=Martin%20Lindstr%C3%B6m%20and%20Borja%20Rodr%C3%ADguez-G%C3%A1lvez%20and%20Ragnar%20Thobaben%20and%20Mikael%20Skoglund&entry.1292438233=%20%20Hyperspherical%20Prototypical%20Learning%20%28HPL%29%20is%20a%20supervised%20approach%20to%0Arepresentation%20learning%20that%20designs%20class%20prototypes%20on%20the%20unit%20hypersphere.%0AThe%20prototypes%20bias%20the%20representations%20to%20class%20separation%20in%20a%20scale%0Ainvariant%20and%20known%20geometry.%20Previous%20approaches%20to%20HPL%20have%20either%20of%20the%0Afollowing%20shortcomings%3A%20%28i%29%20they%20follow%20an%20unprincipled%20optimisation%20procedure%3B%0Aor%20%28ii%29%20they%20are%20theoretically%20sound%2C%20but%20are%20constrained%20to%20only%20one%20possible%0Alatent%20dimension.%20In%20this%20paper%2C%20we%20address%20both%20shortcomings.%20To%20address%20%28i%29%2C%0Awe%20present%20a%20principled%20optimisation%20procedure%20whose%20solution%20we%20show%20is%0Aoptimal.%20To%20address%20%28ii%29%2C%20we%20construct%20well-separated%20prototypes%20in%20a%20wide%0Arange%20of%20dimensions%20using%20linear%20block%20codes.%20Additionally%2C%20we%20give%20a%20full%0Acharacterisation%20of%20the%20optimal%20prototype%20placement%20in%20terms%20of%20achievable%20and%0Aconverse%20bounds%2C%20showing%20that%20our%20proposed%20methods%20are%20near-optimal.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07664v2&entry.124074799=Read"},
{"title": "Generate, but Verify: Reducing Hallucination in Vision-Language Models\n  with Retrospective Resampling", "author": "Tsung-Han Wu and Heekyung Lee and Jiaxin Ge and Joseph E. Gonzalez and Trevor Darrell and David M. Chan", "abstract": "  Vision-Language Models (VLMs) excel at visual understanding but often suffer\nfrom visual hallucinations, where they generate descriptions of nonexistent\nobjects, actions, or concepts, posing significant risks in safety-critical\napplications. Existing hallucination mitigation methods typically follow one of\ntwo paradigms: generation adjustment, which modifies decoding behavior to align\ntext with visual inputs, and post-hoc verification, where external models\nassess and correct outputs. While effective, generation adjustment methods\noften rely on heuristics and lack correction mechanisms, while post-hoc\nverification is complicated, typically requiring multiple models and tending to\nreject outputs rather than refine them. In this work, we introduce REVERSE, a\nunified framework that integrates hallucination-aware training with on-the-fly\nself-verification. By leveraging a new hallucination-verification dataset\ncontaining over 1.3M semi-synthetic samples, along with a novel inference-time\nretrospective resampling technique, our approach enables VLMs to both detect\nhallucinations during generation and dynamically revise those hallucinations.\nOur evaluations show that REVERSE achieves state-of-the-art hallucination\nreduction, outperforming the best existing methods by up to 12% on CHAIR-MSCOCO\nand 28% on HaloQuest. Our dataset, model, and code are available at:\nhttps://reverse-vlm.github.io.\n", "link": "http://arxiv.org/abs/2504.13169v1", "date": "2025-04-17", "relevancy": 2.5871, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5205}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5159}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generate%2C%20but%20Verify%3A%20Reducing%20Hallucination%20in%20Vision-Language%20Models%0A%20%20with%20Retrospective%20Resampling&body=Title%3A%20Generate%2C%20but%20Verify%3A%20Reducing%20Hallucination%20in%20Vision-Language%20Models%0A%20%20with%20Retrospective%20Resampling%0AAuthor%3A%20Tsung-Han%20Wu%20and%20Heekyung%20Lee%20and%20Jiaxin%20Ge%20and%20Joseph%20E.%20Gonzalez%20and%20Trevor%20Darrell%20and%20David%20M.%20Chan%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20excel%20at%20visual%20understanding%20but%20often%20suffer%0Afrom%20visual%20hallucinations%2C%20where%20they%20generate%20descriptions%20of%20nonexistent%0Aobjects%2C%20actions%2C%20or%20concepts%2C%20posing%20significant%20risks%20in%20safety-critical%0Aapplications.%20Existing%20hallucination%20mitigation%20methods%20typically%20follow%20one%20of%0Atwo%20paradigms%3A%20generation%20adjustment%2C%20which%20modifies%20decoding%20behavior%20to%20align%0Atext%20with%20visual%20inputs%2C%20and%20post-hoc%20verification%2C%20where%20external%20models%0Aassess%20and%20correct%20outputs.%20While%20effective%2C%20generation%20adjustment%20methods%0Aoften%20rely%20on%20heuristics%20and%20lack%20correction%20mechanisms%2C%20while%20post-hoc%0Averification%20is%20complicated%2C%20typically%20requiring%20multiple%20models%20and%20tending%20to%0Areject%20outputs%20rather%20than%20refine%20them.%20In%20this%20work%2C%20we%20introduce%20REVERSE%2C%20a%0Aunified%20framework%20that%20integrates%20hallucination-aware%20training%20with%20on-the-fly%0Aself-verification.%20By%20leveraging%20a%20new%20hallucination-verification%20dataset%0Acontaining%20over%201.3M%20semi-synthetic%20samples%2C%20along%20with%20a%20novel%20inference-time%0Aretrospective%20resampling%20technique%2C%20our%20approach%20enables%20VLMs%20to%20both%20detect%0Ahallucinations%20during%20generation%20and%20dynamically%20revise%20those%20hallucinations.%0AOur%20evaluations%20show%20that%20REVERSE%20achieves%20state-of-the-art%20hallucination%0Areduction%2C%20outperforming%20the%20best%20existing%20methods%20by%20up%20to%2012%25%20on%20CHAIR-MSCOCO%0Aand%2028%25%20on%20HaloQuest.%20Our%20dataset%2C%20model%2C%20and%20code%20are%20available%20at%3A%0Ahttps%3A//reverse-vlm.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13169v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerate%252C%2520but%2520Verify%253A%2520Reducing%2520Hallucination%2520in%2520Vision-Language%2520Models%250A%2520%2520with%2520Retrospective%2520Resampling%26entry.906535625%3DTsung-Han%2520Wu%2520and%2520Heekyung%2520Lee%2520and%2520Jiaxin%2520Ge%2520and%2520Joseph%2520E.%2520Gonzalez%2520and%2520Trevor%2520Darrell%2520and%2520David%2520M.%2520Chan%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520excel%2520at%2520visual%2520understanding%2520but%2520often%2520suffer%250Afrom%2520visual%2520hallucinations%252C%2520where%2520they%2520generate%2520descriptions%2520of%2520nonexistent%250Aobjects%252C%2520actions%252C%2520or%2520concepts%252C%2520posing%2520significant%2520risks%2520in%2520safety-critical%250Aapplications.%2520Existing%2520hallucination%2520mitigation%2520methods%2520typically%2520follow%2520one%2520of%250Atwo%2520paradigms%253A%2520generation%2520adjustment%252C%2520which%2520modifies%2520decoding%2520behavior%2520to%2520align%250Atext%2520with%2520visual%2520inputs%252C%2520and%2520post-hoc%2520verification%252C%2520where%2520external%2520models%250Aassess%2520and%2520correct%2520outputs.%2520While%2520effective%252C%2520generation%2520adjustment%2520methods%250Aoften%2520rely%2520on%2520heuristics%2520and%2520lack%2520correction%2520mechanisms%252C%2520while%2520post-hoc%250Averification%2520is%2520complicated%252C%2520typically%2520requiring%2520multiple%2520models%2520and%2520tending%2520to%250Areject%2520outputs%2520rather%2520than%2520refine%2520them.%2520In%2520this%2520work%252C%2520we%2520introduce%2520REVERSE%252C%2520a%250Aunified%2520framework%2520that%2520integrates%2520hallucination-aware%2520training%2520with%2520on-the-fly%250Aself-verification.%2520By%2520leveraging%2520a%2520new%2520hallucination-verification%2520dataset%250Acontaining%2520over%25201.3M%2520semi-synthetic%2520samples%252C%2520along%2520with%2520a%2520novel%2520inference-time%250Aretrospective%2520resampling%2520technique%252C%2520our%2520approach%2520enables%2520VLMs%2520to%2520both%2520detect%250Ahallucinations%2520during%2520generation%2520and%2520dynamically%2520revise%2520those%2520hallucinations.%250AOur%2520evaluations%2520show%2520that%2520REVERSE%2520achieves%2520state-of-the-art%2520hallucination%250Areduction%252C%2520outperforming%2520the%2520best%2520existing%2520methods%2520by%2520up%2520to%252012%2525%2520on%2520CHAIR-MSCOCO%250Aand%252028%2525%2520on%2520HaloQuest.%2520Our%2520dataset%252C%2520model%252C%2520and%2520code%2520are%2520available%2520at%253A%250Ahttps%253A//reverse-vlm.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13169v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generate%2C%20but%20Verify%3A%20Reducing%20Hallucination%20in%20Vision-Language%20Models%0A%20%20with%20Retrospective%20Resampling&entry.906535625=Tsung-Han%20Wu%20and%20Heekyung%20Lee%20and%20Jiaxin%20Ge%20and%20Joseph%20E.%20Gonzalez%20and%20Trevor%20Darrell%20and%20David%20M.%20Chan&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20excel%20at%20visual%20understanding%20but%20often%20suffer%0Afrom%20visual%20hallucinations%2C%20where%20they%20generate%20descriptions%20of%20nonexistent%0Aobjects%2C%20actions%2C%20or%20concepts%2C%20posing%20significant%20risks%20in%20safety-critical%0Aapplications.%20Existing%20hallucination%20mitigation%20methods%20typically%20follow%20one%20of%0Atwo%20paradigms%3A%20generation%20adjustment%2C%20which%20modifies%20decoding%20behavior%20to%20align%0Atext%20with%20visual%20inputs%2C%20and%20post-hoc%20verification%2C%20where%20external%20models%0Aassess%20and%20correct%20outputs.%20While%20effective%2C%20generation%20adjustment%20methods%0Aoften%20rely%20on%20heuristics%20and%20lack%20correction%20mechanisms%2C%20while%20post-hoc%0Averification%20is%20complicated%2C%20typically%20requiring%20multiple%20models%20and%20tending%20to%0Areject%20outputs%20rather%20than%20refine%20them.%20In%20this%20work%2C%20we%20introduce%20REVERSE%2C%20a%0Aunified%20framework%20that%20integrates%20hallucination-aware%20training%20with%20on-the-fly%0Aself-verification.%20By%20leveraging%20a%20new%20hallucination-verification%20dataset%0Acontaining%20over%201.3M%20semi-synthetic%20samples%2C%20along%20with%20a%20novel%20inference-time%0Aretrospective%20resampling%20technique%2C%20our%20approach%20enables%20VLMs%20to%20both%20detect%0Ahallucinations%20during%20generation%20and%20dynamically%20revise%20those%20hallucinations.%0AOur%20evaluations%20show%20that%20REVERSE%20achieves%20state-of-the-art%20hallucination%0Areduction%2C%20outperforming%20the%20best%20existing%20methods%20by%20up%20to%2012%25%20on%20CHAIR-MSCOCO%0Aand%2028%25%20on%20HaloQuest.%20Our%20dataset%2C%20model%2C%20and%20code%20are%20available%20at%3A%0Ahttps%3A//reverse-vlm.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13169v1&entry.124074799=Read"},
{"title": "Perceive With Confidence: Statistical Safety Assurances for Navigation\n  with Learning-Based Perception", "author": "Zhiting Mei and Anushri Dixit and Meghan Booker and Emily Zhou and Mariko Storey-Matsutani and Allen Z. Ren and Ola Shorinwa and Anirudha Majumdar", "abstract": "  Rapid advances in perception have enabled large pre-trained models to be used\nout of the box for transforming high-dimensional, noisy, and partial\nobservations of the world into rich occupancy representations. However, the\nreliability of these models and consequently their safe integration onto robots\nremains unknown when deployed in environments unseen during training. To\nprovide safety guarantees, we rigorously quantify the uncertainty of\npre-trained perception systems for object detection and scene completion via a\nnovel calibration technique based on conformal prediction. Crucially, this\nprocedure guarantees robustness to distribution shifts in states when\nperception outputs are used in conjunction with a planner. As a result, the\ncalibrated perception system can be used in combination with any safe planner\nto provide an end-to-end statistical assurance on safety in unseen\nenvironments. We evaluate the resulting approach, Perceive with Confidence\n(PwC), in simulation and on hardware where a quadruped robot navigates through\npreviously unseen indoor, static environments. These experiments validate the\nsafety assurances for obstacle avoidance provided by PwC. In simulation, our\nmethod reduces obstacle misdetection by $70\\%$ compared to uncalibrated\nperception models. While misdetections lead to collisions for baseline methods,\nour approach consistently achieves $100\\%$ safety. We further demonstrate\nreducing the conservatism of our method without sacrificing safety, achieving a\n$46\\%$ increase in success rates in challenging environments while maintaining\n$100\\%$ safety. In hardware experiments, our method improves empirical safety\nby $40\\%$ over baselines and reduces obstacle misdetection by $93.3\\%$. The\nsafety gap widens to $46.7\\%$ when navigation speed increases, highlighting our\napproach's robustness under more demanding conditions.\n", "link": "http://arxiv.org/abs/2403.08185v3", "date": "2025-04-17", "relevancy": 2.5327, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6705}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6478}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perceive%20With%20Confidence%3A%20Statistical%20Safety%20Assurances%20for%20Navigation%0A%20%20with%20Learning-Based%20Perception&body=Title%3A%20Perceive%20With%20Confidence%3A%20Statistical%20Safety%20Assurances%20for%20Navigation%0A%20%20with%20Learning-Based%20Perception%0AAuthor%3A%20Zhiting%20Mei%20and%20Anushri%20Dixit%20and%20Meghan%20Booker%20and%20Emily%20Zhou%20and%20Mariko%20Storey-Matsutani%20and%20Allen%20Z.%20Ren%20and%20Ola%20Shorinwa%20and%20Anirudha%20Majumdar%0AAbstract%3A%20%20%20Rapid%20advances%20in%20perception%20have%20enabled%20large%20pre-trained%20models%20to%20be%20used%0Aout%20of%20the%20box%20for%20transforming%20high-dimensional%2C%20noisy%2C%20and%20partial%0Aobservations%20of%20the%20world%20into%20rich%20occupancy%20representations.%20However%2C%20the%0Areliability%20of%20these%20models%20and%20consequently%20their%20safe%20integration%20onto%20robots%0Aremains%20unknown%20when%20deployed%20in%20environments%20unseen%20during%20training.%20To%0Aprovide%20safety%20guarantees%2C%20we%20rigorously%20quantify%20the%20uncertainty%20of%0Apre-trained%20perception%20systems%20for%20object%20detection%20and%20scene%20completion%20via%20a%0Anovel%20calibration%20technique%20based%20on%20conformal%20prediction.%20Crucially%2C%20this%0Aprocedure%20guarantees%20robustness%20to%20distribution%20shifts%20in%20states%20when%0Aperception%20outputs%20are%20used%20in%20conjunction%20with%20a%20planner.%20As%20a%20result%2C%20the%0Acalibrated%20perception%20system%20can%20be%20used%20in%20combination%20with%20any%20safe%20planner%0Ato%20provide%20an%20end-to-end%20statistical%20assurance%20on%20safety%20in%20unseen%0Aenvironments.%20We%20evaluate%20the%20resulting%20approach%2C%20Perceive%20with%20Confidence%0A%28PwC%29%2C%20in%20simulation%20and%20on%20hardware%20where%20a%20quadruped%20robot%20navigates%20through%0Apreviously%20unseen%20indoor%2C%20static%20environments.%20These%20experiments%20validate%20the%0Asafety%20assurances%20for%20obstacle%20avoidance%20provided%20by%20PwC.%20In%20simulation%2C%20our%0Amethod%20reduces%20obstacle%20misdetection%20by%20%2470%5C%25%24%20compared%20to%20uncalibrated%0Aperception%20models.%20While%20misdetections%20lead%20to%20collisions%20for%20baseline%20methods%2C%0Aour%20approach%20consistently%20achieves%20%24100%5C%25%24%20safety.%20We%20further%20demonstrate%0Areducing%20the%20conservatism%20of%20our%20method%20without%20sacrificing%20safety%2C%20achieving%20a%0A%2446%5C%25%24%20increase%20in%20success%20rates%20in%20challenging%20environments%20while%20maintaining%0A%24100%5C%25%24%20safety.%20In%20hardware%20experiments%2C%20our%20method%20improves%20empirical%20safety%0Aby%20%2440%5C%25%24%20over%20baselines%20and%20reduces%20obstacle%20misdetection%20by%20%2493.3%5C%25%24.%20The%0Asafety%20gap%20widens%20to%20%2446.7%5C%25%24%20when%20navigation%20speed%20increases%2C%20highlighting%20our%0Aapproach%27s%20robustness%20under%20more%20demanding%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08185v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerceive%2520With%2520Confidence%253A%2520Statistical%2520Safety%2520Assurances%2520for%2520Navigation%250A%2520%2520with%2520Learning-Based%2520Perception%26entry.906535625%3DZhiting%2520Mei%2520and%2520Anushri%2520Dixit%2520and%2520Meghan%2520Booker%2520and%2520Emily%2520Zhou%2520and%2520Mariko%2520Storey-Matsutani%2520and%2520Allen%2520Z.%2520Ren%2520and%2520Ola%2520Shorinwa%2520and%2520Anirudha%2520Majumdar%26entry.1292438233%3D%2520%2520Rapid%2520advances%2520in%2520perception%2520have%2520enabled%2520large%2520pre-trained%2520models%2520to%2520be%2520used%250Aout%2520of%2520the%2520box%2520for%2520transforming%2520high-dimensional%252C%2520noisy%252C%2520and%2520partial%250Aobservations%2520of%2520the%2520world%2520into%2520rich%2520occupancy%2520representations.%2520However%252C%2520the%250Areliability%2520of%2520these%2520models%2520and%2520consequently%2520their%2520safe%2520integration%2520onto%2520robots%250Aremains%2520unknown%2520when%2520deployed%2520in%2520environments%2520unseen%2520during%2520training.%2520To%250Aprovide%2520safety%2520guarantees%252C%2520we%2520rigorously%2520quantify%2520the%2520uncertainty%2520of%250Apre-trained%2520perception%2520systems%2520for%2520object%2520detection%2520and%2520scene%2520completion%2520via%2520a%250Anovel%2520calibration%2520technique%2520based%2520on%2520conformal%2520prediction.%2520Crucially%252C%2520this%250Aprocedure%2520guarantees%2520robustness%2520to%2520distribution%2520shifts%2520in%2520states%2520when%250Aperception%2520outputs%2520are%2520used%2520in%2520conjunction%2520with%2520a%2520planner.%2520As%2520a%2520result%252C%2520the%250Acalibrated%2520perception%2520system%2520can%2520be%2520used%2520in%2520combination%2520with%2520any%2520safe%2520planner%250Ato%2520provide%2520an%2520end-to-end%2520statistical%2520assurance%2520on%2520safety%2520in%2520unseen%250Aenvironments.%2520We%2520evaluate%2520the%2520resulting%2520approach%252C%2520Perceive%2520with%2520Confidence%250A%2528PwC%2529%252C%2520in%2520simulation%2520and%2520on%2520hardware%2520where%2520a%2520quadruped%2520robot%2520navigates%2520through%250Apreviously%2520unseen%2520indoor%252C%2520static%2520environments.%2520These%2520experiments%2520validate%2520the%250Asafety%2520assurances%2520for%2520obstacle%2520avoidance%2520provided%2520by%2520PwC.%2520In%2520simulation%252C%2520our%250Amethod%2520reduces%2520obstacle%2520misdetection%2520by%2520%252470%255C%2525%2524%2520compared%2520to%2520uncalibrated%250Aperception%2520models.%2520While%2520misdetections%2520lead%2520to%2520collisions%2520for%2520baseline%2520methods%252C%250Aour%2520approach%2520consistently%2520achieves%2520%2524100%255C%2525%2524%2520safety.%2520We%2520further%2520demonstrate%250Areducing%2520the%2520conservatism%2520of%2520our%2520method%2520without%2520sacrificing%2520safety%252C%2520achieving%2520a%250A%252446%255C%2525%2524%2520increase%2520in%2520success%2520rates%2520in%2520challenging%2520environments%2520while%2520maintaining%250A%2524100%255C%2525%2524%2520safety.%2520In%2520hardware%2520experiments%252C%2520our%2520method%2520improves%2520empirical%2520safety%250Aby%2520%252440%255C%2525%2524%2520over%2520baselines%2520and%2520reduces%2520obstacle%2520misdetection%2520by%2520%252493.3%255C%2525%2524.%2520The%250Asafety%2520gap%2520widens%2520to%2520%252446.7%255C%2525%2524%2520when%2520navigation%2520speed%2520increases%252C%2520highlighting%2520our%250Aapproach%2527s%2520robustness%2520under%2520more%2520demanding%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.08185v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perceive%20With%20Confidence%3A%20Statistical%20Safety%20Assurances%20for%20Navigation%0A%20%20with%20Learning-Based%20Perception&entry.906535625=Zhiting%20Mei%20and%20Anushri%20Dixit%20and%20Meghan%20Booker%20and%20Emily%20Zhou%20and%20Mariko%20Storey-Matsutani%20and%20Allen%20Z.%20Ren%20and%20Ola%20Shorinwa%20and%20Anirudha%20Majumdar&entry.1292438233=%20%20Rapid%20advances%20in%20perception%20have%20enabled%20large%20pre-trained%20models%20to%20be%20used%0Aout%20of%20the%20box%20for%20transforming%20high-dimensional%2C%20noisy%2C%20and%20partial%0Aobservations%20of%20the%20world%20into%20rich%20occupancy%20representations.%20However%2C%20the%0Areliability%20of%20these%20models%20and%20consequently%20their%20safe%20integration%20onto%20robots%0Aremains%20unknown%20when%20deployed%20in%20environments%20unseen%20during%20training.%20To%0Aprovide%20safety%20guarantees%2C%20we%20rigorously%20quantify%20the%20uncertainty%20of%0Apre-trained%20perception%20systems%20for%20object%20detection%20and%20scene%20completion%20via%20a%0Anovel%20calibration%20technique%20based%20on%20conformal%20prediction.%20Crucially%2C%20this%0Aprocedure%20guarantees%20robustness%20to%20distribution%20shifts%20in%20states%20when%0Aperception%20outputs%20are%20used%20in%20conjunction%20with%20a%20planner.%20As%20a%20result%2C%20the%0Acalibrated%20perception%20system%20can%20be%20used%20in%20combination%20with%20any%20safe%20planner%0Ato%20provide%20an%20end-to-end%20statistical%20assurance%20on%20safety%20in%20unseen%0Aenvironments.%20We%20evaluate%20the%20resulting%20approach%2C%20Perceive%20with%20Confidence%0A%28PwC%29%2C%20in%20simulation%20and%20on%20hardware%20where%20a%20quadruped%20robot%20navigates%20through%0Apreviously%20unseen%20indoor%2C%20static%20environments.%20These%20experiments%20validate%20the%0Asafety%20assurances%20for%20obstacle%20avoidance%20provided%20by%20PwC.%20In%20simulation%2C%20our%0Amethod%20reduces%20obstacle%20misdetection%20by%20%2470%5C%25%24%20compared%20to%20uncalibrated%0Aperception%20models.%20While%20misdetections%20lead%20to%20collisions%20for%20baseline%20methods%2C%0Aour%20approach%20consistently%20achieves%20%24100%5C%25%24%20safety.%20We%20further%20demonstrate%0Areducing%20the%20conservatism%20of%20our%20method%20without%20sacrificing%20safety%2C%20achieving%20a%0A%2446%5C%25%24%20increase%20in%20success%20rates%20in%20challenging%20environments%20while%20maintaining%0A%24100%5C%25%24%20safety.%20In%20hardware%20experiments%2C%20our%20method%20improves%20empirical%20safety%0Aby%20%2440%5C%25%24%20over%20baselines%20and%20reduces%20obstacle%20misdetection%20by%20%2493.3%5C%25%24.%20The%0Asafety%20gap%20widens%20to%20%2446.7%5C%25%24%20when%20navigation%20speed%20increases%2C%20highlighting%20our%0Aapproach%27s%20robustness%20under%20more%20demanding%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08185v3&entry.124074799=Read"},
{"title": "ChatEXAONEPath: An Expert-level Multimodal Large Language Model for\n  Histopathology Using Whole Slide Images", "author": "Sangwook Kim and Soonyoung Lee and Jongseong Jang", "abstract": "  Recent studies have made significant progress in developing large language\nmodels (LLMs) in the medical domain, which can answer expert-level questions\nand demonstrate the potential to assist clinicians in real-world clinical\nscenarios. Studies have also witnessed the importance of integrating various\nmodalities with the existing LLMs for a better understanding of complex\nclinical contexts, which are innately multi-faceted by nature. Although studies\nhave demonstrated the ability of multimodal LLMs in histopathology to answer\nquestions from given images, they lack in understanding of thorough clinical\ncontext due to the patch-level data with limited information from public\ndatasets. Thus, developing WSI-level MLLMs is significant in terms of the\nscalability and applicability of MLLMs in histopathology. In this study, we\nintroduce an expert-level MLLM for histopathology using WSIs, dubbed as\nChatEXAONEPath. We present a retrieval-based data generation pipeline using\n10,094 pairs of WSIs and histopathology reports from The Cancer Genome Atlas\n(TCGA). We also showcase an AI-based evaluation protocol for a comprehensive\nunderstanding of the medical context from given multimodal information and\nevaluate generated answers compared to the original histopathology reports. We\ndemonstrate the ability of diagnosing the given histopathology images using\nChatEXAONEPath with the acceptance rate of 62.9% from 1,134 pairs of WSIs and\nreports. Our proposed model can understand pan-cancer WSIs and clinical context\nfrom various cancer types. We argue that our proposed model has the potential\nto assist clinicians by comprehensively understanding complex morphology of\nWSIs for cancer diagnosis through the integration of multiple modalities.\n", "link": "http://arxiv.org/abs/2504.13023v1", "date": "2025-04-17", "relevancy": 2.5295, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5238}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4969}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChatEXAONEPath%3A%20An%20Expert-level%20Multimodal%20Large%20Language%20Model%20for%0A%20%20Histopathology%20Using%20Whole%20Slide%20Images&body=Title%3A%20ChatEXAONEPath%3A%20An%20Expert-level%20Multimodal%20Large%20Language%20Model%20for%0A%20%20Histopathology%20Using%20Whole%20Slide%20Images%0AAuthor%3A%20Sangwook%20Kim%20and%20Soonyoung%20Lee%20and%20Jongseong%20Jang%0AAbstract%3A%20%20%20Recent%20studies%20have%20made%20significant%20progress%20in%20developing%20large%20language%0Amodels%20%28LLMs%29%20in%20the%20medical%20domain%2C%20which%20can%20answer%20expert-level%20questions%0Aand%20demonstrate%20the%20potential%20to%20assist%20clinicians%20in%20real-world%20clinical%0Ascenarios.%20Studies%20have%20also%20witnessed%20the%20importance%20of%20integrating%20various%0Amodalities%20with%20the%20existing%20LLMs%20for%20a%20better%20understanding%20of%20complex%0Aclinical%20contexts%2C%20which%20are%20innately%20multi-faceted%20by%20nature.%20Although%20studies%0Ahave%20demonstrated%20the%20ability%20of%20multimodal%20LLMs%20in%20histopathology%20to%20answer%0Aquestions%20from%20given%20images%2C%20they%20lack%20in%20understanding%20of%20thorough%20clinical%0Acontext%20due%20to%20the%20patch-level%20data%20with%20limited%20information%20from%20public%0Adatasets.%20Thus%2C%20developing%20WSI-level%20MLLMs%20is%20significant%20in%20terms%20of%20the%0Ascalability%20and%20applicability%20of%20MLLMs%20in%20histopathology.%20In%20this%20study%2C%20we%0Aintroduce%20an%20expert-level%20MLLM%20for%20histopathology%20using%20WSIs%2C%20dubbed%20as%0AChatEXAONEPath.%20We%20present%20a%20retrieval-based%20data%20generation%20pipeline%20using%0A10%2C094%20pairs%20of%20WSIs%20and%20histopathology%20reports%20from%20The%20Cancer%20Genome%20Atlas%0A%28TCGA%29.%20We%20also%20showcase%20an%20AI-based%20evaluation%20protocol%20for%20a%20comprehensive%0Aunderstanding%20of%20the%20medical%20context%20from%20given%20multimodal%20information%20and%0Aevaluate%20generated%20answers%20compared%20to%20the%20original%20histopathology%20reports.%20We%0Ademonstrate%20the%20ability%20of%20diagnosing%20the%20given%20histopathology%20images%20using%0AChatEXAONEPath%20with%20the%20acceptance%20rate%20of%2062.9%25%20from%201%2C134%20pairs%20of%20WSIs%20and%0Areports.%20Our%20proposed%20model%20can%20understand%20pan-cancer%20WSIs%20and%20clinical%20context%0Afrom%20various%20cancer%20types.%20We%20argue%20that%20our%20proposed%20model%20has%20the%20potential%0Ato%20assist%20clinicians%20by%20comprehensively%20understanding%20complex%20morphology%20of%0AWSIs%20for%20cancer%20diagnosis%20through%20the%20integration%20of%20multiple%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13023v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChatEXAONEPath%253A%2520An%2520Expert-level%2520Multimodal%2520Large%2520Language%2520Model%2520for%250A%2520%2520Histopathology%2520Using%2520Whole%2520Slide%2520Images%26entry.906535625%3DSangwook%2520Kim%2520and%2520Soonyoung%2520Lee%2520and%2520Jongseong%2520Jang%26entry.1292438233%3D%2520%2520Recent%2520studies%2520have%2520made%2520significant%2520progress%2520in%2520developing%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520in%2520the%2520medical%2520domain%252C%2520which%2520can%2520answer%2520expert-level%2520questions%250Aand%2520demonstrate%2520the%2520potential%2520to%2520assist%2520clinicians%2520in%2520real-world%2520clinical%250Ascenarios.%2520Studies%2520have%2520also%2520witnessed%2520the%2520importance%2520of%2520integrating%2520various%250Amodalities%2520with%2520the%2520existing%2520LLMs%2520for%2520a%2520better%2520understanding%2520of%2520complex%250Aclinical%2520contexts%252C%2520which%2520are%2520innately%2520multi-faceted%2520by%2520nature.%2520Although%2520studies%250Ahave%2520demonstrated%2520the%2520ability%2520of%2520multimodal%2520LLMs%2520in%2520histopathology%2520to%2520answer%250Aquestions%2520from%2520given%2520images%252C%2520they%2520lack%2520in%2520understanding%2520of%2520thorough%2520clinical%250Acontext%2520due%2520to%2520the%2520patch-level%2520data%2520with%2520limited%2520information%2520from%2520public%250Adatasets.%2520Thus%252C%2520developing%2520WSI-level%2520MLLMs%2520is%2520significant%2520in%2520terms%2520of%2520the%250Ascalability%2520and%2520applicability%2520of%2520MLLMs%2520in%2520histopathology.%2520In%2520this%2520study%252C%2520we%250Aintroduce%2520an%2520expert-level%2520MLLM%2520for%2520histopathology%2520using%2520WSIs%252C%2520dubbed%2520as%250AChatEXAONEPath.%2520We%2520present%2520a%2520retrieval-based%2520data%2520generation%2520pipeline%2520using%250A10%252C094%2520pairs%2520of%2520WSIs%2520and%2520histopathology%2520reports%2520from%2520The%2520Cancer%2520Genome%2520Atlas%250A%2528TCGA%2529.%2520We%2520also%2520showcase%2520an%2520AI-based%2520evaluation%2520protocol%2520for%2520a%2520comprehensive%250Aunderstanding%2520of%2520the%2520medical%2520context%2520from%2520given%2520multimodal%2520information%2520and%250Aevaluate%2520generated%2520answers%2520compared%2520to%2520the%2520original%2520histopathology%2520reports.%2520We%250Ademonstrate%2520the%2520ability%2520of%2520diagnosing%2520the%2520given%2520histopathology%2520images%2520using%250AChatEXAONEPath%2520with%2520the%2520acceptance%2520rate%2520of%252062.9%2525%2520from%25201%252C134%2520pairs%2520of%2520WSIs%2520and%250Areports.%2520Our%2520proposed%2520model%2520can%2520understand%2520pan-cancer%2520WSIs%2520and%2520clinical%2520context%250Afrom%2520various%2520cancer%2520types.%2520We%2520argue%2520that%2520our%2520proposed%2520model%2520has%2520the%2520potential%250Ato%2520assist%2520clinicians%2520by%2520comprehensively%2520understanding%2520complex%2520morphology%2520of%250AWSIs%2520for%2520cancer%2520diagnosis%2520through%2520the%2520integration%2520of%2520multiple%2520modalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13023v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChatEXAONEPath%3A%20An%20Expert-level%20Multimodal%20Large%20Language%20Model%20for%0A%20%20Histopathology%20Using%20Whole%20Slide%20Images&entry.906535625=Sangwook%20Kim%20and%20Soonyoung%20Lee%20and%20Jongseong%20Jang&entry.1292438233=%20%20Recent%20studies%20have%20made%20significant%20progress%20in%20developing%20large%20language%0Amodels%20%28LLMs%29%20in%20the%20medical%20domain%2C%20which%20can%20answer%20expert-level%20questions%0Aand%20demonstrate%20the%20potential%20to%20assist%20clinicians%20in%20real-world%20clinical%0Ascenarios.%20Studies%20have%20also%20witnessed%20the%20importance%20of%20integrating%20various%0Amodalities%20with%20the%20existing%20LLMs%20for%20a%20better%20understanding%20of%20complex%0Aclinical%20contexts%2C%20which%20are%20innately%20multi-faceted%20by%20nature.%20Although%20studies%0Ahave%20demonstrated%20the%20ability%20of%20multimodal%20LLMs%20in%20histopathology%20to%20answer%0Aquestions%20from%20given%20images%2C%20they%20lack%20in%20understanding%20of%20thorough%20clinical%0Acontext%20due%20to%20the%20patch-level%20data%20with%20limited%20information%20from%20public%0Adatasets.%20Thus%2C%20developing%20WSI-level%20MLLMs%20is%20significant%20in%20terms%20of%20the%0Ascalability%20and%20applicability%20of%20MLLMs%20in%20histopathology.%20In%20this%20study%2C%20we%0Aintroduce%20an%20expert-level%20MLLM%20for%20histopathology%20using%20WSIs%2C%20dubbed%20as%0AChatEXAONEPath.%20We%20present%20a%20retrieval-based%20data%20generation%20pipeline%20using%0A10%2C094%20pairs%20of%20WSIs%20and%20histopathology%20reports%20from%20The%20Cancer%20Genome%20Atlas%0A%28TCGA%29.%20We%20also%20showcase%20an%20AI-based%20evaluation%20protocol%20for%20a%20comprehensive%0Aunderstanding%20of%20the%20medical%20context%20from%20given%20multimodal%20information%20and%0Aevaluate%20generated%20answers%20compared%20to%20the%20original%20histopathology%20reports.%20We%0Ademonstrate%20the%20ability%20of%20diagnosing%20the%20given%20histopathology%20images%20using%0AChatEXAONEPath%20with%20the%20acceptance%20rate%20of%2062.9%25%20from%201%2C134%20pairs%20of%20WSIs%20and%0Areports.%20Our%20proposed%20model%20can%20understand%20pan-cancer%20WSIs%20and%20clinical%20context%0Afrom%20various%20cancer%20types.%20We%20argue%20that%20our%20proposed%20model%20has%20the%20potential%0Ato%20assist%20clinicians%20by%20comprehensively%20understanding%20complex%20morphology%20of%0AWSIs%20for%20cancer%20diagnosis%20through%20the%20integration%20of%20multiple%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13023v1&entry.124074799=Read"},
{"title": "A general language model for peptide identification", "author": "Jixiu Zhai and Tianchi Lu and Haitian Zhong and Ziyang Xu and Yuhuan Liu and Shengrui Xu and Jingwan Wang and Dan Huang", "abstract": "  Advances in peptide identification are revolutionizing our ability to\ndecipher protein functions and accelerate therapeutic discovery. We present\nPDeepPP, a deep learning framework that integrates pretrained protein language\nmodels with parallel transformer-CNN architectures, achieving state-of-the-art\nperformance in peptide characterization tasks. The model's hybrid architecture\ndemonstrates unique capabilities in capturing both local sequence motifs and\nglobal structural features, as evidenced by 29% improved cluster separation in\nUMAP visualizations compared to conventional approaches. Evaluated across 33\nbiological recognition tasks - including post-translational modification site\nprediction and bioactive peptide identification - PDeepPP outperformed existing\nmethods in 25 tasks with average AUC improvements of 4.2%. Notably, it achieved\n0.9726 accuracy with PR AUC 0.9977 in antimicrobial peptide detection while\nreducing false negatives by 37.5% in antimalarial recognition scenarios. This\nframework enables accurate large-scale peptide analysis, achieving 218*\nacceleration over sequence-alignment-based methods while maintaining 99.5%\nspecificity in critical glycosylation site detection.PDeepPP establishes a new\nparadigm for computational peptide analysis through its synergistic\narchitecture design, enabling rapid yet precise functional annotation that\nbridges molecular pattern recognition with translational biomedical\napplications.We have made our implementation, including code, data, and\npretrained models, publicly available via GitHub\n(https://github.com/fondress/PDeepPP) and Hugging Face\n(https://huggingface.co/fondress/PDeppPP).\n", "link": "http://arxiv.org/abs/2502.15610v2", "date": "2025-04-17", "relevancy": 2.5128, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5092}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5092}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4893}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20general%20language%20model%20for%20peptide%20identification&body=Title%3A%20A%20general%20language%20model%20for%20peptide%20identification%0AAuthor%3A%20Jixiu%20Zhai%20and%20Tianchi%20Lu%20and%20Haitian%20Zhong%20and%20Ziyang%20Xu%20and%20Yuhuan%20Liu%20and%20Shengrui%20Xu%20and%20Jingwan%20Wang%20and%20Dan%20Huang%0AAbstract%3A%20%20%20Advances%20in%20peptide%20identification%20are%20revolutionizing%20our%20ability%20to%0Adecipher%20protein%20functions%20and%20accelerate%20therapeutic%20discovery.%20We%20present%0APDeepPP%2C%20a%20deep%20learning%20framework%20that%20integrates%20pretrained%20protein%20language%0Amodels%20with%20parallel%20transformer-CNN%20architectures%2C%20achieving%20state-of-the-art%0Aperformance%20in%20peptide%20characterization%20tasks.%20The%20model%27s%20hybrid%20architecture%0Ademonstrates%20unique%20capabilities%20in%20capturing%20both%20local%20sequence%20motifs%20and%0Aglobal%20structural%20features%2C%20as%20evidenced%20by%2029%25%20improved%20cluster%20separation%20in%0AUMAP%20visualizations%20compared%20to%20conventional%20approaches.%20Evaluated%20across%2033%0Abiological%20recognition%20tasks%20-%20including%20post-translational%20modification%20site%0Aprediction%20and%20bioactive%20peptide%20identification%20-%20PDeepPP%20outperformed%20existing%0Amethods%20in%2025%20tasks%20with%20average%20AUC%20improvements%20of%204.2%25.%20Notably%2C%20it%20achieved%0A0.9726%20accuracy%20with%20PR%20AUC%200.9977%20in%20antimicrobial%20peptide%20detection%20while%0Areducing%20false%20negatives%20by%2037.5%25%20in%20antimalarial%20recognition%20scenarios.%20This%0Aframework%20enables%20accurate%20large-scale%20peptide%20analysis%2C%20achieving%20218%2A%0Aacceleration%20over%20sequence-alignment-based%20methods%20while%20maintaining%2099.5%25%0Aspecificity%20in%20critical%20glycosylation%20site%20detection.PDeepPP%20establishes%20a%20new%0Aparadigm%20for%20computational%20peptide%20analysis%20through%20its%20synergistic%0Aarchitecture%20design%2C%20enabling%20rapid%20yet%20precise%20functional%20annotation%20that%0Abridges%20molecular%20pattern%20recognition%20with%20translational%20biomedical%0Aapplications.We%20have%20made%20our%20implementation%2C%20including%20code%2C%20data%2C%20and%0Apretrained%20models%2C%20publicly%20available%20via%20GitHub%0A%28https%3A//github.com/fondress/PDeepPP%29%20and%20Hugging%20Face%0A%28https%3A//huggingface.co/fondress/PDeppPP%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15610v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520general%2520language%2520model%2520for%2520peptide%2520identification%26entry.906535625%3DJixiu%2520Zhai%2520and%2520Tianchi%2520Lu%2520and%2520Haitian%2520Zhong%2520and%2520Ziyang%2520Xu%2520and%2520Yuhuan%2520Liu%2520and%2520Shengrui%2520Xu%2520and%2520Jingwan%2520Wang%2520and%2520Dan%2520Huang%26entry.1292438233%3D%2520%2520Advances%2520in%2520peptide%2520identification%2520are%2520revolutionizing%2520our%2520ability%2520to%250Adecipher%2520protein%2520functions%2520and%2520accelerate%2520therapeutic%2520discovery.%2520We%2520present%250APDeepPP%252C%2520a%2520deep%2520learning%2520framework%2520that%2520integrates%2520pretrained%2520protein%2520language%250Amodels%2520with%2520parallel%2520transformer-CNN%2520architectures%252C%2520achieving%2520state-of-the-art%250Aperformance%2520in%2520peptide%2520characterization%2520tasks.%2520The%2520model%2527s%2520hybrid%2520architecture%250Ademonstrates%2520unique%2520capabilities%2520in%2520capturing%2520both%2520local%2520sequence%2520motifs%2520and%250Aglobal%2520structural%2520features%252C%2520as%2520evidenced%2520by%252029%2525%2520improved%2520cluster%2520separation%2520in%250AUMAP%2520visualizations%2520compared%2520to%2520conventional%2520approaches.%2520Evaluated%2520across%252033%250Abiological%2520recognition%2520tasks%2520-%2520including%2520post-translational%2520modification%2520site%250Aprediction%2520and%2520bioactive%2520peptide%2520identification%2520-%2520PDeepPP%2520outperformed%2520existing%250Amethods%2520in%252025%2520tasks%2520with%2520average%2520AUC%2520improvements%2520of%25204.2%2525.%2520Notably%252C%2520it%2520achieved%250A0.9726%2520accuracy%2520with%2520PR%2520AUC%25200.9977%2520in%2520antimicrobial%2520peptide%2520detection%2520while%250Areducing%2520false%2520negatives%2520by%252037.5%2525%2520in%2520antimalarial%2520recognition%2520scenarios.%2520This%250Aframework%2520enables%2520accurate%2520large-scale%2520peptide%2520analysis%252C%2520achieving%2520218%252A%250Aacceleration%2520over%2520sequence-alignment-based%2520methods%2520while%2520maintaining%252099.5%2525%250Aspecificity%2520in%2520critical%2520glycosylation%2520site%2520detection.PDeepPP%2520establishes%2520a%2520new%250Aparadigm%2520for%2520computational%2520peptide%2520analysis%2520through%2520its%2520synergistic%250Aarchitecture%2520design%252C%2520enabling%2520rapid%2520yet%2520precise%2520functional%2520annotation%2520that%250Abridges%2520molecular%2520pattern%2520recognition%2520with%2520translational%2520biomedical%250Aapplications.We%2520have%2520made%2520our%2520implementation%252C%2520including%2520code%252C%2520data%252C%2520and%250Apretrained%2520models%252C%2520publicly%2520available%2520via%2520GitHub%250A%2528https%253A//github.com/fondress/PDeepPP%2529%2520and%2520Hugging%2520Face%250A%2528https%253A//huggingface.co/fondress/PDeppPP%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15610v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20general%20language%20model%20for%20peptide%20identification&entry.906535625=Jixiu%20Zhai%20and%20Tianchi%20Lu%20and%20Haitian%20Zhong%20and%20Ziyang%20Xu%20and%20Yuhuan%20Liu%20and%20Shengrui%20Xu%20and%20Jingwan%20Wang%20and%20Dan%20Huang&entry.1292438233=%20%20Advances%20in%20peptide%20identification%20are%20revolutionizing%20our%20ability%20to%0Adecipher%20protein%20functions%20and%20accelerate%20therapeutic%20discovery.%20We%20present%0APDeepPP%2C%20a%20deep%20learning%20framework%20that%20integrates%20pretrained%20protein%20language%0Amodels%20with%20parallel%20transformer-CNN%20architectures%2C%20achieving%20state-of-the-art%0Aperformance%20in%20peptide%20characterization%20tasks.%20The%20model%27s%20hybrid%20architecture%0Ademonstrates%20unique%20capabilities%20in%20capturing%20both%20local%20sequence%20motifs%20and%0Aglobal%20structural%20features%2C%20as%20evidenced%20by%2029%25%20improved%20cluster%20separation%20in%0AUMAP%20visualizations%20compared%20to%20conventional%20approaches.%20Evaluated%20across%2033%0Abiological%20recognition%20tasks%20-%20including%20post-translational%20modification%20site%0Aprediction%20and%20bioactive%20peptide%20identification%20-%20PDeepPP%20outperformed%20existing%0Amethods%20in%2025%20tasks%20with%20average%20AUC%20improvements%20of%204.2%25.%20Notably%2C%20it%20achieved%0A0.9726%20accuracy%20with%20PR%20AUC%200.9977%20in%20antimicrobial%20peptide%20detection%20while%0Areducing%20false%20negatives%20by%2037.5%25%20in%20antimalarial%20recognition%20scenarios.%20This%0Aframework%20enables%20accurate%20large-scale%20peptide%20analysis%2C%20achieving%20218%2A%0Aacceleration%20over%20sequence-alignment-based%20methods%20while%20maintaining%2099.5%25%0Aspecificity%20in%20critical%20glycosylation%20site%20detection.PDeepPP%20establishes%20a%20new%0Aparadigm%20for%20computational%20peptide%20analysis%20through%20its%20synergistic%0Aarchitecture%20design%2C%20enabling%20rapid%20yet%20precise%20functional%20annotation%20that%0Abridges%20molecular%20pattern%20recognition%20with%20translational%20biomedical%0Aapplications.We%20have%20made%20our%20implementation%2C%20including%20code%2C%20data%2C%20and%0Apretrained%20models%2C%20publicly%20available%20via%20GitHub%0A%28https%3A//github.com/fondress/PDeepPP%29%20and%20Hugging%20Face%0A%28https%3A//huggingface.co/fondress/PDeppPP%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15610v2&entry.124074799=Read"},
{"title": "Aspect-Based Summarization with Self-Aspect Retrieval Enhanced\n  Generation", "author": "Yichao Feng and Shuai Zhao and Yueqiu Li and Luwei Xiao and Xiaobao Wu and Anh Tuan Luu", "abstract": "  Aspect-based summarization aims to generate summaries tailored to specific\naspects, addressing the resource constraints and limited generalizability of\ntraditional summarization approaches. Recently, large language models have\nshown promise in this task without the need for training. However, they rely\nexcessively on prompt engineering and face token limits and hallucination\nchallenges, especially with in-context learning. To address these challenges,\nin this paper, we propose a novel framework for aspect-based summarization:\nSelf-Aspect Retrieval Enhanced Summary Generation. Rather than relying solely\non in-context learning, given an aspect, we employ an embedding-driven\nretrieval mechanism to identify its relevant text segments. This approach\nextracts the pertinent content while avoiding unnecessary details, thereby\nmitigating the challenge of token limits. Moreover, our framework optimizes\ntoken usage by deleting unrelated parts of the text and ensuring that the model\ngenerates output strictly based on the given aspect. With extensive experiments\non benchmark datasets, we demonstrate that our framework not only achieves\nsuperior performance but also effectively mitigates the token limitation\nproblem.\n", "link": "http://arxiv.org/abs/2504.13054v1", "date": "2025-04-17", "relevancy": 2.4714, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.496}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4934}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aspect-Based%20Summarization%20with%20Self-Aspect%20Retrieval%20Enhanced%0A%20%20Generation&body=Title%3A%20Aspect-Based%20Summarization%20with%20Self-Aspect%20Retrieval%20Enhanced%0A%20%20Generation%0AAuthor%3A%20Yichao%20Feng%20and%20Shuai%20Zhao%20and%20Yueqiu%20Li%20and%20Luwei%20Xiao%20and%20Xiaobao%20Wu%20and%20Anh%20Tuan%20Luu%0AAbstract%3A%20%20%20Aspect-based%20summarization%20aims%20to%20generate%20summaries%20tailored%20to%20specific%0Aaspects%2C%20addressing%20the%20resource%20constraints%20and%20limited%20generalizability%20of%0Atraditional%20summarization%20approaches.%20Recently%2C%20large%20language%20models%20have%0Ashown%20promise%20in%20this%20task%20without%20the%20need%20for%20training.%20However%2C%20they%20rely%0Aexcessively%20on%20prompt%20engineering%20and%20face%20token%20limits%20and%20hallucination%0Achallenges%2C%20especially%20with%20in-context%20learning.%20To%20address%20these%20challenges%2C%0Ain%20this%20paper%2C%20we%20propose%20a%20novel%20framework%20for%20aspect-based%20summarization%3A%0ASelf-Aspect%20Retrieval%20Enhanced%20Summary%20Generation.%20Rather%20than%20relying%20solely%0Aon%20in-context%20learning%2C%20given%20an%20aspect%2C%20we%20employ%20an%20embedding-driven%0Aretrieval%20mechanism%20to%20identify%20its%20relevant%20text%20segments.%20This%20approach%0Aextracts%20the%20pertinent%20content%20while%20avoiding%20unnecessary%20details%2C%20thereby%0Amitigating%20the%20challenge%20of%20token%20limits.%20Moreover%2C%20our%20framework%20optimizes%0Atoken%20usage%20by%20deleting%20unrelated%20parts%20of%20the%20text%20and%20ensuring%20that%20the%20model%0Agenerates%20output%20strictly%20based%20on%20the%20given%20aspect.%20With%20extensive%20experiments%0Aon%20benchmark%20datasets%2C%20we%20demonstrate%20that%20our%20framework%20not%20only%20achieves%0Asuperior%20performance%20but%20also%20effectively%20mitigates%20the%20token%20limitation%0Aproblem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13054v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAspect-Based%2520Summarization%2520with%2520Self-Aspect%2520Retrieval%2520Enhanced%250A%2520%2520Generation%26entry.906535625%3DYichao%2520Feng%2520and%2520Shuai%2520Zhao%2520and%2520Yueqiu%2520Li%2520and%2520Luwei%2520Xiao%2520and%2520Xiaobao%2520Wu%2520and%2520Anh%2520Tuan%2520Luu%26entry.1292438233%3D%2520%2520Aspect-based%2520summarization%2520aims%2520to%2520generate%2520summaries%2520tailored%2520to%2520specific%250Aaspects%252C%2520addressing%2520the%2520resource%2520constraints%2520and%2520limited%2520generalizability%2520of%250Atraditional%2520summarization%2520approaches.%2520Recently%252C%2520large%2520language%2520models%2520have%250Ashown%2520promise%2520in%2520this%2520task%2520without%2520the%2520need%2520for%2520training.%2520However%252C%2520they%2520rely%250Aexcessively%2520on%2520prompt%2520engineering%2520and%2520face%2520token%2520limits%2520and%2520hallucination%250Achallenges%252C%2520especially%2520with%2520in-context%2520learning.%2520To%2520address%2520these%2520challenges%252C%250Ain%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520framework%2520for%2520aspect-based%2520summarization%253A%250ASelf-Aspect%2520Retrieval%2520Enhanced%2520Summary%2520Generation.%2520Rather%2520than%2520relying%2520solely%250Aon%2520in-context%2520learning%252C%2520given%2520an%2520aspect%252C%2520we%2520employ%2520an%2520embedding-driven%250Aretrieval%2520mechanism%2520to%2520identify%2520its%2520relevant%2520text%2520segments.%2520This%2520approach%250Aextracts%2520the%2520pertinent%2520content%2520while%2520avoiding%2520unnecessary%2520details%252C%2520thereby%250Amitigating%2520the%2520challenge%2520of%2520token%2520limits.%2520Moreover%252C%2520our%2520framework%2520optimizes%250Atoken%2520usage%2520by%2520deleting%2520unrelated%2520parts%2520of%2520the%2520text%2520and%2520ensuring%2520that%2520the%2520model%250Agenerates%2520output%2520strictly%2520based%2520on%2520the%2520given%2520aspect.%2520With%2520extensive%2520experiments%250Aon%2520benchmark%2520datasets%252C%2520we%2520demonstrate%2520that%2520our%2520framework%2520not%2520only%2520achieves%250Asuperior%2520performance%2520but%2520also%2520effectively%2520mitigates%2520the%2520token%2520limitation%250Aproblem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13054v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aspect-Based%20Summarization%20with%20Self-Aspect%20Retrieval%20Enhanced%0A%20%20Generation&entry.906535625=Yichao%20Feng%20and%20Shuai%20Zhao%20and%20Yueqiu%20Li%20and%20Luwei%20Xiao%20and%20Xiaobao%20Wu%20and%20Anh%20Tuan%20Luu&entry.1292438233=%20%20Aspect-based%20summarization%20aims%20to%20generate%20summaries%20tailored%20to%20specific%0Aaspects%2C%20addressing%20the%20resource%20constraints%20and%20limited%20generalizability%20of%0Atraditional%20summarization%20approaches.%20Recently%2C%20large%20language%20models%20have%0Ashown%20promise%20in%20this%20task%20without%20the%20need%20for%20training.%20However%2C%20they%20rely%0Aexcessively%20on%20prompt%20engineering%20and%20face%20token%20limits%20and%20hallucination%0Achallenges%2C%20especially%20with%20in-context%20learning.%20To%20address%20these%20challenges%2C%0Ain%20this%20paper%2C%20we%20propose%20a%20novel%20framework%20for%20aspect-based%20summarization%3A%0ASelf-Aspect%20Retrieval%20Enhanced%20Summary%20Generation.%20Rather%20than%20relying%20solely%0Aon%20in-context%20learning%2C%20given%20an%20aspect%2C%20we%20employ%20an%20embedding-driven%0Aretrieval%20mechanism%20to%20identify%20its%20relevant%20text%20segments.%20This%20approach%0Aextracts%20the%20pertinent%20content%20while%20avoiding%20unnecessary%20details%2C%20thereby%0Amitigating%20the%20challenge%20of%20token%20limits.%20Moreover%2C%20our%20framework%20optimizes%0Atoken%20usage%20by%20deleting%20unrelated%20parts%20of%20the%20text%20and%20ensuring%20that%20the%20model%0Agenerates%20output%20strictly%20based%20on%20the%20given%20aspect.%20With%20extensive%20experiments%0Aon%20benchmark%20datasets%2C%20we%20demonstrate%20that%20our%20framework%20not%20only%20achieves%0Asuperior%20performance%20but%20also%20effectively%20mitigates%20the%20token%20limitation%0Aproblem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13054v1&entry.124074799=Read"},
{"title": "VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference\n  Optimization for Large Video Models", "author": "Haojian Huang and Haodong Chen and Shengqiong Wu and Meng Luo and Jinlan Fu and Xinya Du and Hanwang Zhang and Hao Fei", "abstract": "  Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown\npromise in video understanding but often suffer from misalignment with human\nintuition and video hallucination issues. To address these challenges, we\nintroduce VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal\nDirect Preference Optimization. VistaDPO enhances text-video preference\nalignment across three hierarchical levels: i) Instance Level, aligning overall\nvideo content with responses; ii) Temporal Level, aligning video temporal\nsemantics with event descriptions; and iii) Perceptive Level, aligning spatial\nobjects with language tokens. Given the lack of datasets for fine-grained\nvideo-language preference alignment, we construct VistaDPO-7k, a dataset of\n7.2K QA pairs annotated with chosen and rejected responses, along with\nspatial-temporal grounding information such as timestamps, keyframes, and\nbounding boxes. Extensive experiments on benchmarks such as Video\nHallucination, Video QA, and Captioning performance tasks demonstrate that\nVistaDPO significantly improves the performance of existing LVMs, effectively\nmitigating video-language misalignment and hallucination. The code and data are\navailable at https://github.com/HaroldChen19/VistaDPO.\n", "link": "http://arxiv.org/abs/2504.13122v1", "date": "2025-04-17", "relevancy": 2.4574, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6206}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6206}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VistaDPO%3A%20Video%20Hierarchical%20Spatial-Temporal%20Direct%20Preference%0A%20%20Optimization%20for%20Large%20Video%20Models&body=Title%3A%20VistaDPO%3A%20Video%20Hierarchical%20Spatial-Temporal%20Direct%20Preference%0A%20%20Optimization%20for%20Large%20Video%20Models%0AAuthor%3A%20Haojian%20Huang%20and%20Haodong%20Chen%20and%20Shengqiong%20Wu%20and%20Meng%20Luo%20and%20Jinlan%20Fu%20and%20Xinya%20Du%20and%20Hanwang%20Zhang%20and%20Hao%20Fei%0AAbstract%3A%20%20%20Large%20Video%20Models%20%28LVMs%29%20built%20upon%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%0Apromise%20in%20video%20understanding%20but%20often%20suffer%20from%20misalignment%20with%20human%0Aintuition%20and%20video%20hallucination%20issues.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20VistaDPO%2C%20a%20novel%20framework%20for%20Video%20Hierarchical%20Spatial-Temporal%0ADirect%20Preference%20Optimization.%20VistaDPO%20enhances%20text-video%20preference%0Aalignment%20across%20three%20hierarchical%20levels%3A%20i%29%20Instance%20Level%2C%20aligning%20overall%0Avideo%20content%20with%20responses%3B%20ii%29%20Temporal%20Level%2C%20aligning%20video%20temporal%0Asemantics%20with%20event%20descriptions%3B%20and%20iii%29%20Perceptive%20Level%2C%20aligning%20spatial%0Aobjects%20with%20language%20tokens.%20Given%20the%20lack%20of%20datasets%20for%20fine-grained%0Avideo-language%20preference%20alignment%2C%20we%20construct%20VistaDPO-7k%2C%20a%20dataset%20of%0A7.2K%20QA%20pairs%20annotated%20with%20chosen%20and%20rejected%20responses%2C%20along%20with%0Aspatial-temporal%20grounding%20information%20such%20as%20timestamps%2C%20keyframes%2C%20and%0Abounding%20boxes.%20Extensive%20experiments%20on%20benchmarks%20such%20as%20Video%0AHallucination%2C%20Video%20QA%2C%20and%20Captioning%20performance%20tasks%20demonstrate%20that%0AVistaDPO%20significantly%20improves%20the%20performance%20of%20existing%20LVMs%2C%20effectively%0Amitigating%20video-language%20misalignment%20and%20hallucination.%20The%20code%20and%20data%20are%0Aavailable%20at%20https%3A//github.com/HaroldChen19/VistaDPO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13122v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVistaDPO%253A%2520Video%2520Hierarchical%2520Spatial-Temporal%2520Direct%2520Preference%250A%2520%2520Optimization%2520for%2520Large%2520Video%2520Models%26entry.906535625%3DHaojian%2520Huang%2520and%2520Haodong%2520Chen%2520and%2520Shengqiong%2520Wu%2520and%2520Meng%2520Luo%2520and%2520Jinlan%2520Fu%2520and%2520Xinya%2520Du%2520and%2520Hanwang%2520Zhang%2520and%2520Hao%2520Fei%26entry.1292438233%3D%2520%2520Large%2520Video%2520Models%2520%2528LVMs%2529%2520built%2520upon%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%250Apromise%2520in%2520video%2520understanding%2520but%2520often%2520suffer%2520from%2520misalignment%2520with%2520human%250Aintuition%2520and%2520video%2520hallucination%2520issues.%2520To%2520address%2520these%2520challenges%252C%2520we%250Aintroduce%2520VistaDPO%252C%2520a%2520novel%2520framework%2520for%2520Video%2520Hierarchical%2520Spatial-Temporal%250ADirect%2520Preference%2520Optimization.%2520VistaDPO%2520enhances%2520text-video%2520preference%250Aalignment%2520across%2520three%2520hierarchical%2520levels%253A%2520i%2529%2520Instance%2520Level%252C%2520aligning%2520overall%250Avideo%2520content%2520with%2520responses%253B%2520ii%2529%2520Temporal%2520Level%252C%2520aligning%2520video%2520temporal%250Asemantics%2520with%2520event%2520descriptions%253B%2520and%2520iii%2529%2520Perceptive%2520Level%252C%2520aligning%2520spatial%250Aobjects%2520with%2520language%2520tokens.%2520Given%2520the%2520lack%2520of%2520datasets%2520for%2520fine-grained%250Avideo-language%2520preference%2520alignment%252C%2520we%2520construct%2520VistaDPO-7k%252C%2520a%2520dataset%2520of%250A7.2K%2520QA%2520pairs%2520annotated%2520with%2520chosen%2520and%2520rejected%2520responses%252C%2520along%2520with%250Aspatial-temporal%2520grounding%2520information%2520such%2520as%2520timestamps%252C%2520keyframes%252C%2520and%250Abounding%2520boxes.%2520Extensive%2520experiments%2520on%2520benchmarks%2520such%2520as%2520Video%250AHallucination%252C%2520Video%2520QA%252C%2520and%2520Captioning%2520performance%2520tasks%2520demonstrate%2520that%250AVistaDPO%2520significantly%2520improves%2520the%2520performance%2520of%2520existing%2520LVMs%252C%2520effectively%250Amitigating%2520video-language%2520misalignment%2520and%2520hallucination.%2520The%2520code%2520and%2520data%2520are%250Aavailable%2520at%2520https%253A//github.com/HaroldChen19/VistaDPO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13122v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VistaDPO%3A%20Video%20Hierarchical%20Spatial-Temporal%20Direct%20Preference%0A%20%20Optimization%20for%20Large%20Video%20Models&entry.906535625=Haojian%20Huang%20and%20Haodong%20Chen%20and%20Shengqiong%20Wu%20and%20Meng%20Luo%20and%20Jinlan%20Fu%20and%20Xinya%20Du%20and%20Hanwang%20Zhang%20and%20Hao%20Fei&entry.1292438233=%20%20Large%20Video%20Models%20%28LVMs%29%20built%20upon%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%0Apromise%20in%20video%20understanding%20but%20often%20suffer%20from%20misalignment%20with%20human%0Aintuition%20and%20video%20hallucination%20issues.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20VistaDPO%2C%20a%20novel%20framework%20for%20Video%20Hierarchical%20Spatial-Temporal%0ADirect%20Preference%20Optimization.%20VistaDPO%20enhances%20text-video%20preference%0Aalignment%20across%20three%20hierarchical%20levels%3A%20i%29%20Instance%20Level%2C%20aligning%20overall%0Avideo%20content%20with%20responses%3B%20ii%29%20Temporal%20Level%2C%20aligning%20video%20temporal%0Asemantics%20with%20event%20descriptions%3B%20and%20iii%29%20Perceptive%20Level%2C%20aligning%20spatial%0Aobjects%20with%20language%20tokens.%20Given%20the%20lack%20of%20datasets%20for%20fine-grained%0Avideo-language%20preference%20alignment%2C%20we%20construct%20VistaDPO-7k%2C%20a%20dataset%20of%0A7.2K%20QA%20pairs%20annotated%20with%20chosen%20and%20rejected%20responses%2C%20along%20with%0Aspatial-temporal%20grounding%20information%20such%20as%20timestamps%2C%20keyframes%2C%20and%0Abounding%20boxes.%20Extensive%20experiments%20on%20benchmarks%20such%20as%20Video%0AHallucination%2C%20Video%20QA%2C%20and%20Captioning%20performance%20tasks%20demonstrate%20that%0AVistaDPO%20significantly%20improves%20the%20performance%20of%20existing%20LVMs%2C%20effectively%0Amitigating%20video-language%20misalignment%20and%20hallucination.%20The%20code%20and%20data%20are%0Aavailable%20at%20https%3A//github.com/HaroldChen19/VistaDPO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13122v1&entry.124074799=Read"},
{"title": "SkyReels-V2: Infinite-length Film Generative Model", "author": "Guibin Chen and Dixuan Lin and Jiangping Yang and Chunze Lin and Juncheng Zhu and Mingyuan Fan and Hao Zhang and Sheng Chen and Zheng Chen and Chengchen Ma and Weiming Xiong and Wei Wang and Nuo Pang and Kang Kang and Zhiheng Xu and Yuzhe Jin and Yupeng Liang and Yubing Song and Peng Zhao and Boyuan Xu and Di Qiu and Debang Li and Zhengcong Fei and Yang Li and Yahui Zhou", "abstract": "  Recent advances in video generation have been driven by diffusion models and\nautoregressive frameworks, yet critical challenges persist in harmonizing\nprompt adherence, visual quality, motion dynamics, and duration: compromises in\nmotion dynamics to enhance temporal visual quality, constrained video duration\n(5-10 seconds) to prioritize resolution, and inadequate shot-aware generation\nstemming from general-purpose MLLMs' inability to interpret cinematic grammar,\nsuch as shot composition, actor expressions, and camera motions. These\nintertwined limitations hinder realistic long-form synthesis and professional\nfilm-style generation. To address these limitations, we propose SkyReels-V2, an\nInfinite-length Film Generative Model, that synergizes Multi-modal Large\nLanguage Model (MLLM), Multi-stage Pretraining, Reinforcement Learning, and\nDiffusion Forcing Framework. Firstly, we design a comprehensive structural\nrepresentation of video that combines the general descriptions by the\nMulti-modal LLM and the detailed shot language by sub-expert models. Aided with\nhuman annotation, we then train a unified Video Captioner, named\nSkyCaptioner-V1, to efficiently label the video data. Secondly, we establish\nprogressive-resolution pretraining for the fundamental video generation,\nfollowed by a four-stage post-training enhancement: Initial concept-balanced\nSupervised Fine-Tuning (SFT) improves baseline quality; Motion-specific\nReinforcement Learning (RL) training with human-annotated and synthetic\ndistortion data addresses dynamic artifacts; Our diffusion forcing framework\nwith non-decreasing noise schedules enables long-video synthesis in an\nefficient search space; Final high-quality SFT refines visual fidelity. All the\ncode and models are available at https://github.com/SkyworkAI/SkyReels-V2.\n", "link": "http://arxiv.org/abs/2504.13074v1", "date": "2025-04-17", "relevancy": 2.4485, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6298}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6179}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SkyReels-V2%3A%20Infinite-length%20Film%20Generative%20Model&body=Title%3A%20SkyReels-V2%3A%20Infinite-length%20Film%20Generative%20Model%0AAuthor%3A%20Guibin%20Chen%20and%20Dixuan%20Lin%20and%20Jiangping%20Yang%20and%20Chunze%20Lin%20and%20Juncheng%20Zhu%20and%20Mingyuan%20Fan%20and%20Hao%20Zhang%20and%20Sheng%20Chen%20and%20Zheng%20Chen%20and%20Chengchen%20Ma%20and%20Weiming%20Xiong%20and%20Wei%20Wang%20and%20Nuo%20Pang%20and%20Kang%20Kang%20and%20Zhiheng%20Xu%20and%20Yuzhe%20Jin%20and%20Yupeng%20Liang%20and%20Yubing%20Song%20and%20Peng%20Zhao%20and%20Boyuan%20Xu%20and%20Di%20Qiu%20and%20Debang%20Li%20and%20Zhengcong%20Fei%20and%20Yang%20Li%20and%20Yahui%20Zhou%0AAbstract%3A%20%20%20Recent%20advances%20in%20video%20generation%20have%20been%20driven%20by%20diffusion%20models%20and%0Aautoregressive%20frameworks%2C%20yet%20critical%20challenges%20persist%20in%20harmonizing%0Aprompt%20adherence%2C%20visual%20quality%2C%20motion%20dynamics%2C%20and%20duration%3A%20compromises%20in%0Amotion%20dynamics%20to%20enhance%20temporal%20visual%20quality%2C%20constrained%20video%20duration%0A%285-10%20seconds%29%20to%20prioritize%20resolution%2C%20and%20inadequate%20shot-aware%20generation%0Astemming%20from%20general-purpose%20MLLMs%27%20inability%20to%20interpret%20cinematic%20grammar%2C%0Asuch%20as%20shot%20composition%2C%20actor%20expressions%2C%20and%20camera%20motions.%20These%0Aintertwined%20limitations%20hinder%20realistic%20long-form%20synthesis%20and%20professional%0Afilm-style%20generation.%20To%20address%20these%20limitations%2C%20we%20propose%20SkyReels-V2%2C%20an%0AInfinite-length%20Film%20Generative%20Model%2C%20that%20synergizes%20Multi-modal%20Large%0ALanguage%20Model%20%28MLLM%29%2C%20Multi-stage%20Pretraining%2C%20Reinforcement%20Learning%2C%20and%0ADiffusion%20Forcing%20Framework.%20Firstly%2C%20we%20design%20a%20comprehensive%20structural%0Arepresentation%20of%20video%20that%20combines%20the%20general%20descriptions%20by%20the%0AMulti-modal%20LLM%20and%20the%20detailed%20shot%20language%20by%20sub-expert%20models.%20Aided%20with%0Ahuman%20annotation%2C%20we%20then%20train%20a%20unified%20Video%20Captioner%2C%20named%0ASkyCaptioner-V1%2C%20to%20efficiently%20label%20the%20video%20data.%20Secondly%2C%20we%20establish%0Aprogressive-resolution%20pretraining%20for%20the%20fundamental%20video%20generation%2C%0Afollowed%20by%20a%20four-stage%20post-training%20enhancement%3A%20Initial%20concept-balanced%0ASupervised%20Fine-Tuning%20%28SFT%29%20improves%20baseline%20quality%3B%20Motion-specific%0AReinforcement%20Learning%20%28RL%29%20training%20with%20human-annotated%20and%20synthetic%0Adistortion%20data%20addresses%20dynamic%20artifacts%3B%20Our%20diffusion%20forcing%20framework%0Awith%20non-decreasing%20noise%20schedules%20enables%20long-video%20synthesis%20in%20an%0Aefficient%20search%20space%3B%20Final%20high-quality%20SFT%20refines%20visual%20fidelity.%20All%20the%0Acode%20and%20models%20are%20available%20at%20https%3A//github.com/SkyworkAI/SkyReels-V2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13074v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkyReels-V2%253A%2520Infinite-length%2520Film%2520Generative%2520Model%26entry.906535625%3DGuibin%2520Chen%2520and%2520Dixuan%2520Lin%2520and%2520Jiangping%2520Yang%2520and%2520Chunze%2520Lin%2520and%2520Juncheng%2520Zhu%2520and%2520Mingyuan%2520Fan%2520and%2520Hao%2520Zhang%2520and%2520Sheng%2520Chen%2520and%2520Zheng%2520Chen%2520and%2520Chengchen%2520Ma%2520and%2520Weiming%2520Xiong%2520and%2520Wei%2520Wang%2520and%2520Nuo%2520Pang%2520and%2520Kang%2520Kang%2520and%2520Zhiheng%2520Xu%2520and%2520Yuzhe%2520Jin%2520and%2520Yupeng%2520Liang%2520and%2520Yubing%2520Song%2520and%2520Peng%2520Zhao%2520and%2520Boyuan%2520Xu%2520and%2520Di%2520Qiu%2520and%2520Debang%2520Li%2520and%2520Zhengcong%2520Fei%2520and%2520Yang%2520Li%2520and%2520Yahui%2520Zhou%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520video%2520generation%2520have%2520been%2520driven%2520by%2520diffusion%2520models%2520and%250Aautoregressive%2520frameworks%252C%2520yet%2520critical%2520challenges%2520persist%2520in%2520harmonizing%250Aprompt%2520adherence%252C%2520visual%2520quality%252C%2520motion%2520dynamics%252C%2520and%2520duration%253A%2520compromises%2520in%250Amotion%2520dynamics%2520to%2520enhance%2520temporal%2520visual%2520quality%252C%2520constrained%2520video%2520duration%250A%25285-10%2520seconds%2529%2520to%2520prioritize%2520resolution%252C%2520and%2520inadequate%2520shot-aware%2520generation%250Astemming%2520from%2520general-purpose%2520MLLMs%2527%2520inability%2520to%2520interpret%2520cinematic%2520grammar%252C%250Asuch%2520as%2520shot%2520composition%252C%2520actor%2520expressions%252C%2520and%2520camera%2520motions.%2520These%250Aintertwined%2520limitations%2520hinder%2520realistic%2520long-form%2520synthesis%2520and%2520professional%250Afilm-style%2520generation.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520SkyReels-V2%252C%2520an%250AInfinite-length%2520Film%2520Generative%2520Model%252C%2520that%2520synergizes%2520Multi-modal%2520Large%250ALanguage%2520Model%2520%2528MLLM%2529%252C%2520Multi-stage%2520Pretraining%252C%2520Reinforcement%2520Learning%252C%2520and%250ADiffusion%2520Forcing%2520Framework.%2520Firstly%252C%2520we%2520design%2520a%2520comprehensive%2520structural%250Arepresentation%2520of%2520video%2520that%2520combines%2520the%2520general%2520descriptions%2520by%2520the%250AMulti-modal%2520LLM%2520and%2520the%2520detailed%2520shot%2520language%2520by%2520sub-expert%2520models.%2520Aided%2520with%250Ahuman%2520annotation%252C%2520we%2520then%2520train%2520a%2520unified%2520Video%2520Captioner%252C%2520named%250ASkyCaptioner-V1%252C%2520to%2520efficiently%2520label%2520the%2520video%2520data.%2520Secondly%252C%2520we%2520establish%250Aprogressive-resolution%2520pretraining%2520for%2520the%2520fundamental%2520video%2520generation%252C%250Afollowed%2520by%2520a%2520four-stage%2520post-training%2520enhancement%253A%2520Initial%2520concept-balanced%250ASupervised%2520Fine-Tuning%2520%2528SFT%2529%2520improves%2520baseline%2520quality%253B%2520Motion-specific%250AReinforcement%2520Learning%2520%2528RL%2529%2520training%2520with%2520human-annotated%2520and%2520synthetic%250Adistortion%2520data%2520addresses%2520dynamic%2520artifacts%253B%2520Our%2520diffusion%2520forcing%2520framework%250Awith%2520non-decreasing%2520noise%2520schedules%2520enables%2520long-video%2520synthesis%2520in%2520an%250Aefficient%2520search%2520space%253B%2520Final%2520high-quality%2520SFT%2520refines%2520visual%2520fidelity.%2520All%2520the%250Acode%2520and%2520models%2520are%2520available%2520at%2520https%253A//github.com/SkyworkAI/SkyReels-V2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13074v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SkyReels-V2%3A%20Infinite-length%20Film%20Generative%20Model&entry.906535625=Guibin%20Chen%20and%20Dixuan%20Lin%20and%20Jiangping%20Yang%20and%20Chunze%20Lin%20and%20Juncheng%20Zhu%20and%20Mingyuan%20Fan%20and%20Hao%20Zhang%20and%20Sheng%20Chen%20and%20Zheng%20Chen%20and%20Chengchen%20Ma%20and%20Weiming%20Xiong%20and%20Wei%20Wang%20and%20Nuo%20Pang%20and%20Kang%20Kang%20and%20Zhiheng%20Xu%20and%20Yuzhe%20Jin%20and%20Yupeng%20Liang%20and%20Yubing%20Song%20and%20Peng%20Zhao%20and%20Boyuan%20Xu%20and%20Di%20Qiu%20and%20Debang%20Li%20and%20Zhengcong%20Fei%20and%20Yang%20Li%20and%20Yahui%20Zhou&entry.1292438233=%20%20Recent%20advances%20in%20video%20generation%20have%20been%20driven%20by%20diffusion%20models%20and%0Aautoregressive%20frameworks%2C%20yet%20critical%20challenges%20persist%20in%20harmonizing%0Aprompt%20adherence%2C%20visual%20quality%2C%20motion%20dynamics%2C%20and%20duration%3A%20compromises%20in%0Amotion%20dynamics%20to%20enhance%20temporal%20visual%20quality%2C%20constrained%20video%20duration%0A%285-10%20seconds%29%20to%20prioritize%20resolution%2C%20and%20inadequate%20shot-aware%20generation%0Astemming%20from%20general-purpose%20MLLMs%27%20inability%20to%20interpret%20cinematic%20grammar%2C%0Asuch%20as%20shot%20composition%2C%20actor%20expressions%2C%20and%20camera%20motions.%20These%0Aintertwined%20limitations%20hinder%20realistic%20long-form%20synthesis%20and%20professional%0Afilm-style%20generation.%20To%20address%20these%20limitations%2C%20we%20propose%20SkyReels-V2%2C%20an%0AInfinite-length%20Film%20Generative%20Model%2C%20that%20synergizes%20Multi-modal%20Large%0ALanguage%20Model%20%28MLLM%29%2C%20Multi-stage%20Pretraining%2C%20Reinforcement%20Learning%2C%20and%0ADiffusion%20Forcing%20Framework.%20Firstly%2C%20we%20design%20a%20comprehensive%20structural%0Arepresentation%20of%20video%20that%20combines%20the%20general%20descriptions%20by%20the%0AMulti-modal%20LLM%20and%20the%20detailed%20shot%20language%20by%20sub-expert%20models.%20Aided%20with%0Ahuman%20annotation%2C%20we%20then%20train%20a%20unified%20Video%20Captioner%2C%20named%0ASkyCaptioner-V1%2C%20to%20efficiently%20label%20the%20video%20data.%20Secondly%2C%20we%20establish%0Aprogressive-resolution%20pretraining%20for%20the%20fundamental%20video%20generation%2C%0Afollowed%20by%20a%20four-stage%20post-training%20enhancement%3A%20Initial%20concept-balanced%0ASupervised%20Fine-Tuning%20%28SFT%29%20improves%20baseline%20quality%3B%20Motion-specific%0AReinforcement%20Learning%20%28RL%29%20training%20with%20human-annotated%20and%20synthetic%0Adistortion%20data%20addresses%20dynamic%20artifacts%3B%20Our%20diffusion%20forcing%20framework%0Awith%20non-decreasing%20noise%20schedules%20enables%20long-video%20synthesis%20in%20an%0Aefficient%20search%20space%3B%20Final%20high-quality%20SFT%20refines%20visual%20fidelity.%20All%20the%0Acode%20and%20models%20are%20available%20at%20https%3A//github.com/SkyworkAI/SkyReels-V2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13074v1&entry.124074799=Read"},
{"title": "Imaging for All-Day Wearable Smart Glasses", "author": "Michael Goesele and Daniel Andersen and Yujia Chen and Simon Green and Eddy Ilg and Chao Li and Johnson Liu and Grace Kuo and Logan Wan and Richard Newcombe", "abstract": "  In recent years smart glasses technology has rapidly advanced, opening up\nentirely new areas for mobile computing. We expect future smart glasses will\nneed to be all-day wearable, adopting a small form factor to meet the\nrequirements of volume, weight, fashionability and social acceptability, which\nputs significant constraints on the space of possible solutions. Additional\nchallenges arise due to the fact that smart glasses are worn in arbitrary\nenvironments while their wearer moves and performs everyday activities. In this\npaper, we systematically analyze the space of imaging from smart glasses and\nderive several fundamental limits that govern this imaging domain. We discuss\nthe impact of these limits on achievable image quality and camera module size\n-- comparing in particular to related devices such as mobile phones. We then\npropose a novel distributed imaging approach that allows to minimize the size\nof the individual camera modules when compared to a standard monolithic camera\ndesign. Finally, we demonstrate the properties of this novel approach in a\nseries of experiments using synthetic data as well as images captured with two\ndifferent prototype implementations.\n", "link": "http://arxiv.org/abs/2504.13060v1", "date": "2025-04-17", "relevancy": 2.4478, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5041}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4823}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imaging%20for%20All-Day%20Wearable%20Smart%20Glasses&body=Title%3A%20Imaging%20for%20All-Day%20Wearable%20Smart%20Glasses%0AAuthor%3A%20Michael%20Goesele%20and%20Daniel%20Andersen%20and%20Yujia%20Chen%20and%20Simon%20Green%20and%20Eddy%20Ilg%20and%20Chao%20Li%20and%20Johnson%20Liu%20and%20Grace%20Kuo%20and%20Logan%20Wan%20and%20Richard%20Newcombe%0AAbstract%3A%20%20%20In%20recent%20years%20smart%20glasses%20technology%20has%20rapidly%20advanced%2C%20opening%20up%0Aentirely%20new%20areas%20for%20mobile%20computing.%20We%20expect%20future%20smart%20glasses%20will%0Aneed%20to%20be%20all-day%20wearable%2C%20adopting%20a%20small%20form%20factor%20to%20meet%20the%0Arequirements%20of%20volume%2C%20weight%2C%20fashionability%20and%20social%20acceptability%2C%20which%0Aputs%20significant%20constraints%20on%20the%20space%20of%20possible%20solutions.%20Additional%0Achallenges%20arise%20due%20to%20the%20fact%20that%20smart%20glasses%20are%20worn%20in%20arbitrary%0Aenvironments%20while%20their%20wearer%20moves%20and%20performs%20everyday%20activities.%20In%20this%0Apaper%2C%20we%20systematically%20analyze%20the%20space%20of%20imaging%20from%20smart%20glasses%20and%0Aderive%20several%20fundamental%20limits%20that%20govern%20this%20imaging%20domain.%20We%20discuss%0Athe%20impact%20of%20these%20limits%20on%20achievable%20image%20quality%20and%20camera%20module%20size%0A--%20comparing%20in%20particular%20to%20related%20devices%20such%20as%20mobile%20phones.%20We%20then%0Apropose%20a%20novel%20distributed%20imaging%20approach%20that%20allows%20to%20minimize%20the%20size%0Aof%20the%20individual%20camera%20modules%20when%20compared%20to%20a%20standard%20monolithic%20camera%0Adesign.%20Finally%2C%20we%20demonstrate%20the%20properties%20of%20this%20novel%20approach%20in%20a%0Aseries%20of%20experiments%20using%20synthetic%20data%20as%20well%20as%20images%20captured%20with%20two%0Adifferent%20prototype%20implementations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13060v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImaging%2520for%2520All-Day%2520Wearable%2520Smart%2520Glasses%26entry.906535625%3DMichael%2520Goesele%2520and%2520Daniel%2520Andersen%2520and%2520Yujia%2520Chen%2520and%2520Simon%2520Green%2520and%2520Eddy%2520Ilg%2520and%2520Chao%2520Li%2520and%2520Johnson%2520Liu%2520and%2520Grace%2520Kuo%2520and%2520Logan%2520Wan%2520and%2520Richard%2520Newcombe%26entry.1292438233%3D%2520%2520In%2520recent%2520years%2520smart%2520glasses%2520technology%2520has%2520rapidly%2520advanced%252C%2520opening%2520up%250Aentirely%2520new%2520areas%2520for%2520mobile%2520computing.%2520We%2520expect%2520future%2520smart%2520glasses%2520will%250Aneed%2520to%2520be%2520all-day%2520wearable%252C%2520adopting%2520a%2520small%2520form%2520factor%2520to%2520meet%2520the%250Arequirements%2520of%2520volume%252C%2520weight%252C%2520fashionability%2520and%2520social%2520acceptability%252C%2520which%250Aputs%2520significant%2520constraints%2520on%2520the%2520space%2520of%2520possible%2520solutions.%2520Additional%250Achallenges%2520arise%2520due%2520to%2520the%2520fact%2520that%2520smart%2520glasses%2520are%2520worn%2520in%2520arbitrary%250Aenvironments%2520while%2520their%2520wearer%2520moves%2520and%2520performs%2520everyday%2520activities.%2520In%2520this%250Apaper%252C%2520we%2520systematically%2520analyze%2520the%2520space%2520of%2520imaging%2520from%2520smart%2520glasses%2520and%250Aderive%2520several%2520fundamental%2520limits%2520that%2520govern%2520this%2520imaging%2520domain.%2520We%2520discuss%250Athe%2520impact%2520of%2520these%2520limits%2520on%2520achievable%2520image%2520quality%2520and%2520camera%2520module%2520size%250A--%2520comparing%2520in%2520particular%2520to%2520related%2520devices%2520such%2520as%2520mobile%2520phones.%2520We%2520then%250Apropose%2520a%2520novel%2520distributed%2520imaging%2520approach%2520that%2520allows%2520to%2520minimize%2520the%2520size%250Aof%2520the%2520individual%2520camera%2520modules%2520when%2520compared%2520to%2520a%2520standard%2520monolithic%2520camera%250Adesign.%2520Finally%252C%2520we%2520demonstrate%2520the%2520properties%2520of%2520this%2520novel%2520approach%2520in%2520a%250Aseries%2520of%2520experiments%2520using%2520synthetic%2520data%2520as%2520well%2520as%2520images%2520captured%2520with%2520two%250Adifferent%2520prototype%2520implementations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13060v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imaging%20for%20All-Day%20Wearable%20Smart%20Glasses&entry.906535625=Michael%20Goesele%20and%20Daniel%20Andersen%20and%20Yujia%20Chen%20and%20Simon%20Green%20and%20Eddy%20Ilg%20and%20Chao%20Li%20and%20Johnson%20Liu%20and%20Grace%20Kuo%20and%20Logan%20Wan%20and%20Richard%20Newcombe&entry.1292438233=%20%20In%20recent%20years%20smart%20glasses%20technology%20has%20rapidly%20advanced%2C%20opening%20up%0Aentirely%20new%20areas%20for%20mobile%20computing.%20We%20expect%20future%20smart%20glasses%20will%0Aneed%20to%20be%20all-day%20wearable%2C%20adopting%20a%20small%20form%20factor%20to%20meet%20the%0Arequirements%20of%20volume%2C%20weight%2C%20fashionability%20and%20social%20acceptability%2C%20which%0Aputs%20significant%20constraints%20on%20the%20space%20of%20possible%20solutions.%20Additional%0Achallenges%20arise%20due%20to%20the%20fact%20that%20smart%20glasses%20are%20worn%20in%20arbitrary%0Aenvironments%20while%20their%20wearer%20moves%20and%20performs%20everyday%20activities.%20In%20this%0Apaper%2C%20we%20systematically%20analyze%20the%20space%20of%20imaging%20from%20smart%20glasses%20and%0Aderive%20several%20fundamental%20limits%20that%20govern%20this%20imaging%20domain.%20We%20discuss%0Athe%20impact%20of%20these%20limits%20on%20achievable%20image%20quality%20and%20camera%20module%20size%0A--%20comparing%20in%20particular%20to%20related%20devices%20such%20as%20mobile%20phones.%20We%20then%0Apropose%20a%20novel%20distributed%20imaging%20approach%20that%20allows%20to%20minimize%20the%20size%0Aof%20the%20individual%20camera%20modules%20when%20compared%20to%20a%20standard%20monolithic%20camera%0Adesign.%20Finally%2C%20we%20demonstrate%20the%20properties%20of%20this%20novel%20approach%20in%20a%0Aseries%20of%20experiments%20using%20synthetic%20data%20as%20well%20as%20images%20captured%20with%20two%0Adifferent%20prototype%20implementations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13060v1&entry.124074799=Read"},
{"title": "An All-Atom Generative Model for Designing Protein Complexes", "author": "Ruizhe Chen and Dongyu Xue and Xiangxin Zhou and Zaixiang Zheng and Xiangxiang Zeng and Quanquan Gu", "abstract": "  Proteins typically exist in complexes, interacting with other proteins or\nbiomolecules to perform their specific biological roles. Research on\nsingle-chain protein modeling has been extensively and deeply explored, with\nadvancements seen in models like the series of ESM and AlphaFold. Despite these\ndevelopments, the study and modeling of multi-chain proteins remain largely\nuncharted, though they are vital for understanding biological functions.\nRecognizing the importance of these interactions, we introduce APM (All-Atom\nProtein Generative Model), a model specifically designed for modeling\nmulti-chain proteins. By integrating atom-level information and leveraging data\non multi-chain proteins, APM is capable of precisely modeling inter-chain\ninteractions and designing protein complexes with binding capabilities from\nscratch. It also performs folding and inverse-folding tasks for multi-chain\nproteins. Moreover, APM demonstrates versatility in downstream applications: it\nachieves enhanced performance through supervised fine-tuning (SFT) while also\nsupporting zero-shot sampling in certain tasks, achieving state-of-the-art\nresults. Code will be released at https://github.com/bytedance/apm.\n", "link": "http://arxiv.org/abs/2504.13075v1", "date": "2025-04-17", "relevancy": 2.4205, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4848}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4838}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20All-Atom%20Generative%20Model%20for%20Designing%20Protein%20Complexes&body=Title%3A%20An%20All-Atom%20Generative%20Model%20for%20Designing%20Protein%20Complexes%0AAuthor%3A%20Ruizhe%20Chen%20and%20Dongyu%20Xue%20and%20Xiangxin%20Zhou%20and%20Zaixiang%20Zheng%20and%20Xiangxiang%20Zeng%20and%20Quanquan%20Gu%0AAbstract%3A%20%20%20Proteins%20typically%20exist%20in%20complexes%2C%20interacting%20with%20other%20proteins%20or%0Abiomolecules%20to%20perform%20their%20specific%20biological%20roles.%20Research%20on%0Asingle-chain%20protein%20modeling%20has%20been%20extensively%20and%20deeply%20explored%2C%20with%0Aadvancements%20seen%20in%20models%20like%20the%20series%20of%20ESM%20and%20AlphaFold.%20Despite%20these%0Adevelopments%2C%20the%20study%20and%20modeling%20of%20multi-chain%20proteins%20remain%20largely%0Auncharted%2C%20though%20they%20are%20vital%20for%20understanding%20biological%20functions.%0ARecognizing%20the%20importance%20of%20these%20interactions%2C%20we%20introduce%20APM%20%28All-Atom%0AProtein%20Generative%20Model%29%2C%20a%20model%20specifically%20designed%20for%20modeling%0Amulti-chain%20proteins.%20By%20integrating%20atom-level%20information%20and%20leveraging%20data%0Aon%20multi-chain%20proteins%2C%20APM%20is%20capable%20of%20precisely%20modeling%20inter-chain%0Ainteractions%20and%20designing%20protein%20complexes%20with%20binding%20capabilities%20from%0Ascratch.%20It%20also%20performs%20folding%20and%20inverse-folding%20tasks%20for%20multi-chain%0Aproteins.%20Moreover%2C%20APM%20demonstrates%20versatility%20in%20downstream%20applications%3A%20it%0Aachieves%20enhanced%20performance%20through%20supervised%20fine-tuning%20%28SFT%29%20while%20also%0Asupporting%20zero-shot%20sampling%20in%20certain%20tasks%2C%20achieving%20state-of-the-art%0Aresults.%20Code%20will%20be%20released%20at%20https%3A//github.com/bytedance/apm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13075v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520All-Atom%2520Generative%2520Model%2520for%2520Designing%2520Protein%2520Complexes%26entry.906535625%3DRuizhe%2520Chen%2520and%2520Dongyu%2520Xue%2520and%2520Xiangxin%2520Zhou%2520and%2520Zaixiang%2520Zheng%2520and%2520Xiangxiang%2520Zeng%2520and%2520Quanquan%2520Gu%26entry.1292438233%3D%2520%2520Proteins%2520typically%2520exist%2520in%2520complexes%252C%2520interacting%2520with%2520other%2520proteins%2520or%250Abiomolecules%2520to%2520perform%2520their%2520specific%2520biological%2520roles.%2520Research%2520on%250Asingle-chain%2520protein%2520modeling%2520has%2520been%2520extensively%2520and%2520deeply%2520explored%252C%2520with%250Aadvancements%2520seen%2520in%2520models%2520like%2520the%2520series%2520of%2520ESM%2520and%2520AlphaFold.%2520Despite%2520these%250Adevelopments%252C%2520the%2520study%2520and%2520modeling%2520of%2520multi-chain%2520proteins%2520remain%2520largely%250Auncharted%252C%2520though%2520they%2520are%2520vital%2520for%2520understanding%2520biological%2520functions.%250ARecognizing%2520the%2520importance%2520of%2520these%2520interactions%252C%2520we%2520introduce%2520APM%2520%2528All-Atom%250AProtein%2520Generative%2520Model%2529%252C%2520a%2520model%2520specifically%2520designed%2520for%2520modeling%250Amulti-chain%2520proteins.%2520By%2520integrating%2520atom-level%2520information%2520and%2520leveraging%2520data%250Aon%2520multi-chain%2520proteins%252C%2520APM%2520is%2520capable%2520of%2520precisely%2520modeling%2520inter-chain%250Ainteractions%2520and%2520designing%2520protein%2520complexes%2520with%2520binding%2520capabilities%2520from%250Ascratch.%2520It%2520also%2520performs%2520folding%2520and%2520inverse-folding%2520tasks%2520for%2520multi-chain%250Aproteins.%2520Moreover%252C%2520APM%2520demonstrates%2520versatility%2520in%2520downstream%2520applications%253A%2520it%250Aachieves%2520enhanced%2520performance%2520through%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520while%2520also%250Asupporting%2520zero-shot%2520sampling%2520in%2520certain%2520tasks%252C%2520achieving%2520state-of-the-art%250Aresults.%2520Code%2520will%2520be%2520released%2520at%2520https%253A//github.com/bytedance/apm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13075v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20All-Atom%20Generative%20Model%20for%20Designing%20Protein%20Complexes&entry.906535625=Ruizhe%20Chen%20and%20Dongyu%20Xue%20and%20Xiangxin%20Zhou%20and%20Zaixiang%20Zheng%20and%20Xiangxiang%20Zeng%20and%20Quanquan%20Gu&entry.1292438233=%20%20Proteins%20typically%20exist%20in%20complexes%2C%20interacting%20with%20other%20proteins%20or%0Abiomolecules%20to%20perform%20their%20specific%20biological%20roles.%20Research%20on%0Asingle-chain%20protein%20modeling%20has%20been%20extensively%20and%20deeply%20explored%2C%20with%0Aadvancements%20seen%20in%20models%20like%20the%20series%20of%20ESM%20and%20AlphaFold.%20Despite%20these%0Adevelopments%2C%20the%20study%20and%20modeling%20of%20multi-chain%20proteins%20remain%20largely%0Auncharted%2C%20though%20they%20are%20vital%20for%20understanding%20biological%20functions.%0ARecognizing%20the%20importance%20of%20these%20interactions%2C%20we%20introduce%20APM%20%28All-Atom%0AProtein%20Generative%20Model%29%2C%20a%20model%20specifically%20designed%20for%20modeling%0Amulti-chain%20proteins.%20By%20integrating%20atom-level%20information%20and%20leveraging%20data%0Aon%20multi-chain%20proteins%2C%20APM%20is%20capable%20of%20precisely%20modeling%20inter-chain%0Ainteractions%20and%20designing%20protein%20complexes%20with%20binding%20capabilities%20from%0Ascratch.%20It%20also%20performs%20folding%20and%20inverse-folding%20tasks%20for%20multi-chain%0Aproteins.%20Moreover%2C%20APM%20demonstrates%20versatility%20in%20downstream%20applications%3A%20it%0Aachieves%20enhanced%20performance%20through%20supervised%20fine-tuning%20%28SFT%29%20while%20also%0Asupporting%20zero-shot%20sampling%20in%20certain%20tasks%2C%20achieving%20state-of-the-art%0Aresults.%20Code%20will%20be%20released%20at%20https%3A//github.com/bytedance/apm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13075v1&entry.124074799=Read"},
{"title": "Can Masked Autoencoders Also Listen to Birds?", "author": "Lukas Rauch and Ilyass Moummad and Ren\u00e9 Heinrich and Alexis Joly and Bernhard Sick and Christoph Scholz", "abstract": "  Masked Autoencoders (MAEs) pretrained on AudioSet fail to capture the\nfine-grained acoustic characteristics of specialized domains such as\nbioacoustic monitoring. Bird sound classification is critical for assessing\nenvironmental health, yet general-purpose models inadequately address its\nunique acoustic challenges. To address this, we introduce Bird-MAE, a\ndomain-specialized MAE pretrained on the large-scale BirdSet dataset. We\nexplore adjustments to pretraining, fine-tuning and utilizing frozen\nrepresentations. Bird-MAE achieves state-of-the-art results across all BirdSet\ndownstream tasks, substantially improving multi-label classification\nperformance compared to the general-purpose Audio-MAE baseline. Additionally,\nwe propose prototypical probing, a parameter-efficient method for leveraging\nMAEs' frozen representations. Bird-MAE's prototypical probes outperform linear\nprobing by up to 37\\% in MAP and narrow the gap to fine-tuning to approximately\n3\\% on average on BirdSet.\n", "link": "http://arxiv.org/abs/2504.12880v1", "date": "2025-04-17", "relevancy": 2.4144, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5043}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4723}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Masked%20Autoencoders%20Also%20Listen%20to%20Birds%3F&body=Title%3A%20Can%20Masked%20Autoencoders%20Also%20Listen%20to%20Birds%3F%0AAuthor%3A%20Lukas%20Rauch%20and%20Ilyass%20Moummad%20and%20Ren%C3%A9%20Heinrich%20and%20Alexis%20Joly%20and%20Bernhard%20Sick%20and%20Christoph%20Scholz%0AAbstract%3A%20%20%20Masked%20Autoencoders%20%28MAEs%29%20pretrained%20on%20AudioSet%20fail%20to%20capture%20the%0Afine-grained%20acoustic%20characteristics%20of%20specialized%20domains%20such%20as%0Abioacoustic%20monitoring.%20Bird%20sound%20classification%20is%20critical%20for%20assessing%0Aenvironmental%20health%2C%20yet%20general-purpose%20models%20inadequately%20address%20its%0Aunique%20acoustic%20challenges.%20To%20address%20this%2C%20we%20introduce%20Bird-MAE%2C%20a%0Adomain-specialized%20MAE%20pretrained%20on%20the%20large-scale%20BirdSet%20dataset.%20We%0Aexplore%20adjustments%20to%20pretraining%2C%20fine-tuning%20and%20utilizing%20frozen%0Arepresentations.%20Bird-MAE%20achieves%20state-of-the-art%20results%20across%20all%20BirdSet%0Adownstream%20tasks%2C%20substantially%20improving%20multi-label%20classification%0Aperformance%20compared%20to%20the%20general-purpose%20Audio-MAE%20baseline.%20Additionally%2C%0Awe%20propose%20prototypical%20probing%2C%20a%20parameter-efficient%20method%20for%20leveraging%0AMAEs%27%20frozen%20representations.%20Bird-MAE%27s%20prototypical%20probes%20outperform%20linear%0Aprobing%20by%20up%20to%2037%5C%25%20in%20MAP%20and%20narrow%20the%20gap%20to%20fine-tuning%20to%20approximately%0A3%5C%25%20on%20average%20on%20BirdSet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12880v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Masked%2520Autoencoders%2520Also%2520Listen%2520to%2520Birds%253F%26entry.906535625%3DLukas%2520Rauch%2520and%2520Ilyass%2520Moummad%2520and%2520Ren%25C3%25A9%2520Heinrich%2520and%2520Alexis%2520Joly%2520and%2520Bernhard%2520Sick%2520and%2520Christoph%2520Scholz%26entry.1292438233%3D%2520%2520Masked%2520Autoencoders%2520%2528MAEs%2529%2520pretrained%2520on%2520AudioSet%2520fail%2520to%2520capture%2520the%250Afine-grained%2520acoustic%2520characteristics%2520of%2520specialized%2520domains%2520such%2520as%250Abioacoustic%2520monitoring.%2520Bird%2520sound%2520classification%2520is%2520critical%2520for%2520assessing%250Aenvironmental%2520health%252C%2520yet%2520general-purpose%2520models%2520inadequately%2520address%2520its%250Aunique%2520acoustic%2520challenges.%2520To%2520address%2520this%252C%2520we%2520introduce%2520Bird-MAE%252C%2520a%250Adomain-specialized%2520MAE%2520pretrained%2520on%2520the%2520large-scale%2520BirdSet%2520dataset.%2520We%250Aexplore%2520adjustments%2520to%2520pretraining%252C%2520fine-tuning%2520and%2520utilizing%2520frozen%250Arepresentations.%2520Bird-MAE%2520achieves%2520state-of-the-art%2520results%2520across%2520all%2520BirdSet%250Adownstream%2520tasks%252C%2520substantially%2520improving%2520multi-label%2520classification%250Aperformance%2520compared%2520to%2520the%2520general-purpose%2520Audio-MAE%2520baseline.%2520Additionally%252C%250Awe%2520propose%2520prototypical%2520probing%252C%2520a%2520parameter-efficient%2520method%2520for%2520leveraging%250AMAEs%2527%2520frozen%2520representations.%2520Bird-MAE%2527s%2520prototypical%2520probes%2520outperform%2520linear%250Aprobing%2520by%2520up%2520to%252037%255C%2525%2520in%2520MAP%2520and%2520narrow%2520the%2520gap%2520to%2520fine-tuning%2520to%2520approximately%250A3%255C%2525%2520on%2520average%2520on%2520BirdSet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12880v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Masked%20Autoencoders%20Also%20Listen%20to%20Birds%3F&entry.906535625=Lukas%20Rauch%20and%20Ilyass%20Moummad%20and%20Ren%C3%A9%20Heinrich%20and%20Alexis%20Joly%20and%20Bernhard%20Sick%20and%20Christoph%20Scholz&entry.1292438233=%20%20Masked%20Autoencoders%20%28MAEs%29%20pretrained%20on%20AudioSet%20fail%20to%20capture%20the%0Afine-grained%20acoustic%20characteristics%20of%20specialized%20domains%20such%20as%0Abioacoustic%20monitoring.%20Bird%20sound%20classification%20is%20critical%20for%20assessing%0Aenvironmental%20health%2C%20yet%20general-purpose%20models%20inadequately%20address%20its%0Aunique%20acoustic%20challenges.%20To%20address%20this%2C%20we%20introduce%20Bird-MAE%2C%20a%0Adomain-specialized%20MAE%20pretrained%20on%20the%20large-scale%20BirdSet%20dataset.%20We%0Aexplore%20adjustments%20to%20pretraining%2C%20fine-tuning%20and%20utilizing%20frozen%0Arepresentations.%20Bird-MAE%20achieves%20state-of-the-art%20results%20across%20all%20BirdSet%0Adownstream%20tasks%2C%20substantially%20improving%20multi-label%20classification%0Aperformance%20compared%20to%20the%20general-purpose%20Audio-MAE%20baseline.%20Additionally%2C%0Awe%20propose%20prototypical%20probing%2C%20a%20parameter-efficient%20method%20for%20leveraging%0AMAEs%27%20frozen%20representations.%20Bird-MAE%27s%20prototypical%20probes%20outperform%20linear%0Aprobing%20by%20up%20to%2037%5C%25%20in%20MAP%20and%20narrow%20the%20gap%20to%20fine-tuning%20to%20approximately%0A3%5C%25%20on%20average%20on%20BirdSet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12880v1&entry.124074799=Read"},
{"title": "St4RTrack: Simultaneous 4D Reconstruction and Tracking in the World", "author": "Haiwen Feng and Junyi Zhang and Qianqian Wang and Yufei Ye and Pengcheng Yu and Michael J. Black and Trevor Darrell and Angjoo Kanazawa", "abstract": "  Dynamic 3D reconstruction and point tracking in videos are typically treated\nas separate tasks, despite their deep connection. We propose St4RTrack, a\nfeed-forward framework that simultaneously reconstructs and tracks dynamic\nvideo content in a world coordinate frame from RGB inputs. This is achieved by\npredicting two appropriately defined pointmaps for a pair of frames captured at\ndifferent moments. Specifically, we predict both pointmaps at the same moment,\nin the same world, capturing both static and dynamic scene geometry while\nmaintaining 3D correspondences. Chaining these predictions through the video\nsequence with respect to a reference frame naturally computes long-range\ncorrespondences, effectively combining 3D reconstruction with 3D tracking.\nUnlike prior methods that rely heavily on 4D ground truth supervision, we\nemploy a novel adaptation scheme based on a reprojection loss. We establish a\nnew extensive benchmark for world-frame reconstruction and tracking,\ndemonstrating the effectiveness and efficiency of our unified, data-driven\nframework. Our code, model, and benchmark will be released.\n", "link": "http://arxiv.org/abs/2504.13152v1", "date": "2025-04-17", "relevancy": 2.4068, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.616}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6115}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20St4RTrack%3A%20Simultaneous%204D%20Reconstruction%20and%20Tracking%20in%20the%20World&body=Title%3A%20St4RTrack%3A%20Simultaneous%204D%20Reconstruction%20and%20Tracking%20in%20the%20World%0AAuthor%3A%20Haiwen%20Feng%20and%20Junyi%20Zhang%20and%20Qianqian%20Wang%20and%20Yufei%20Ye%20and%20Pengcheng%20Yu%20and%20Michael%20J.%20Black%20and%20Trevor%20Darrell%20and%20Angjoo%20Kanazawa%0AAbstract%3A%20%20%20Dynamic%203D%20reconstruction%20and%20point%20tracking%20in%20videos%20are%20typically%20treated%0Aas%20separate%20tasks%2C%20despite%20their%20deep%20connection.%20We%20propose%20St4RTrack%2C%20a%0Afeed-forward%20framework%20that%20simultaneously%20reconstructs%20and%20tracks%20dynamic%0Avideo%20content%20in%20a%20world%20coordinate%20frame%20from%20RGB%20inputs.%20This%20is%20achieved%20by%0Apredicting%20two%20appropriately%20defined%20pointmaps%20for%20a%20pair%20of%20frames%20captured%20at%0Adifferent%20moments.%20Specifically%2C%20we%20predict%20both%20pointmaps%20at%20the%20same%20moment%2C%0Ain%20the%20same%20world%2C%20capturing%20both%20static%20and%20dynamic%20scene%20geometry%20while%0Amaintaining%203D%20correspondences.%20Chaining%20these%20predictions%20through%20the%20video%0Asequence%20with%20respect%20to%20a%20reference%20frame%20naturally%20computes%20long-range%0Acorrespondences%2C%20effectively%20combining%203D%20reconstruction%20with%203D%20tracking.%0AUnlike%20prior%20methods%20that%20rely%20heavily%20on%204D%20ground%20truth%20supervision%2C%20we%0Aemploy%20a%20novel%20adaptation%20scheme%20based%20on%20a%20reprojection%20loss.%20We%20establish%20a%0Anew%20extensive%20benchmark%20for%20world-frame%20reconstruction%20and%20tracking%2C%0Ademonstrating%20the%20effectiveness%20and%20efficiency%20of%20our%20unified%2C%20data-driven%0Aframework.%20Our%20code%2C%20model%2C%20and%20benchmark%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13152v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSt4RTrack%253A%2520Simultaneous%25204D%2520Reconstruction%2520and%2520Tracking%2520in%2520the%2520World%26entry.906535625%3DHaiwen%2520Feng%2520and%2520Junyi%2520Zhang%2520and%2520Qianqian%2520Wang%2520and%2520Yufei%2520Ye%2520and%2520Pengcheng%2520Yu%2520and%2520Michael%2520J.%2520Black%2520and%2520Trevor%2520Darrell%2520and%2520Angjoo%2520Kanazawa%26entry.1292438233%3D%2520%2520Dynamic%25203D%2520reconstruction%2520and%2520point%2520tracking%2520in%2520videos%2520are%2520typically%2520treated%250Aas%2520separate%2520tasks%252C%2520despite%2520their%2520deep%2520connection.%2520We%2520propose%2520St4RTrack%252C%2520a%250Afeed-forward%2520framework%2520that%2520simultaneously%2520reconstructs%2520and%2520tracks%2520dynamic%250Avideo%2520content%2520in%2520a%2520world%2520coordinate%2520frame%2520from%2520RGB%2520inputs.%2520This%2520is%2520achieved%2520by%250Apredicting%2520two%2520appropriately%2520defined%2520pointmaps%2520for%2520a%2520pair%2520of%2520frames%2520captured%2520at%250Adifferent%2520moments.%2520Specifically%252C%2520we%2520predict%2520both%2520pointmaps%2520at%2520the%2520same%2520moment%252C%250Ain%2520the%2520same%2520world%252C%2520capturing%2520both%2520static%2520and%2520dynamic%2520scene%2520geometry%2520while%250Amaintaining%25203D%2520correspondences.%2520Chaining%2520these%2520predictions%2520through%2520the%2520video%250Asequence%2520with%2520respect%2520to%2520a%2520reference%2520frame%2520naturally%2520computes%2520long-range%250Acorrespondences%252C%2520effectively%2520combining%25203D%2520reconstruction%2520with%25203D%2520tracking.%250AUnlike%2520prior%2520methods%2520that%2520rely%2520heavily%2520on%25204D%2520ground%2520truth%2520supervision%252C%2520we%250Aemploy%2520a%2520novel%2520adaptation%2520scheme%2520based%2520on%2520a%2520reprojection%2520loss.%2520We%2520establish%2520a%250Anew%2520extensive%2520benchmark%2520for%2520world-frame%2520reconstruction%2520and%2520tracking%252C%250Ademonstrating%2520the%2520effectiveness%2520and%2520efficiency%2520of%2520our%2520unified%252C%2520data-driven%250Aframework.%2520Our%2520code%252C%2520model%252C%2520and%2520benchmark%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13152v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=St4RTrack%3A%20Simultaneous%204D%20Reconstruction%20and%20Tracking%20in%20the%20World&entry.906535625=Haiwen%20Feng%20and%20Junyi%20Zhang%20and%20Qianqian%20Wang%20and%20Yufei%20Ye%20and%20Pengcheng%20Yu%20and%20Michael%20J.%20Black%20and%20Trevor%20Darrell%20and%20Angjoo%20Kanazawa&entry.1292438233=%20%20Dynamic%203D%20reconstruction%20and%20point%20tracking%20in%20videos%20are%20typically%20treated%0Aas%20separate%20tasks%2C%20despite%20their%20deep%20connection.%20We%20propose%20St4RTrack%2C%20a%0Afeed-forward%20framework%20that%20simultaneously%20reconstructs%20and%20tracks%20dynamic%0Avideo%20content%20in%20a%20world%20coordinate%20frame%20from%20RGB%20inputs.%20This%20is%20achieved%20by%0Apredicting%20two%20appropriately%20defined%20pointmaps%20for%20a%20pair%20of%20frames%20captured%20at%0Adifferent%20moments.%20Specifically%2C%20we%20predict%20both%20pointmaps%20at%20the%20same%20moment%2C%0Ain%20the%20same%20world%2C%20capturing%20both%20static%20and%20dynamic%20scene%20geometry%20while%0Amaintaining%203D%20correspondences.%20Chaining%20these%20predictions%20through%20the%20video%0Asequence%20with%20respect%20to%20a%20reference%20frame%20naturally%20computes%20long-range%0Acorrespondences%2C%20effectively%20combining%203D%20reconstruction%20with%203D%20tracking.%0AUnlike%20prior%20methods%20that%20rely%20heavily%20on%204D%20ground%20truth%20supervision%2C%20we%0Aemploy%20a%20novel%20adaptation%20scheme%20based%20on%20a%20reprojection%20loss.%20We%20establish%20a%0Anew%20extensive%20benchmark%20for%20world-frame%20reconstruction%20and%20tracking%2C%0Ademonstrating%20the%20effectiveness%20and%20efficiency%20of%20our%20unified%2C%20data-driven%0Aframework.%20Our%20code%2C%20model%2C%20and%20benchmark%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13152v1&entry.124074799=Read"},
{"title": "Uncertainty Quantification via H\u00f6lder Divergence for Multi-View\n  Representation Learning", "author": "Yan Zhang and Ming Li and Chun Li and Zhaoxia Liu and Ye Zhang and Fei Richard Yu", "abstract": "  Evidence-based deep learning represents a burgeoning paradigm for uncertainty\nestimation, offering reliable predictions with negligible extra computational\noverheads. Existing methods usually adopt Kullback-Leibler divergence to\nestimate the uncertainty of network predictions, ignoring domain gaps among\nvarious modalities. To tackle this issue, this paper introduces a novel\nalgorithm based on H\\\"older Divergence (HD) to enhance the reliability of\nmulti-view learning by addressing inherent uncertainty challenges from\nincomplete or noisy data. Generally, our method extracts the representations of\nmultiple modalities through parallel network branches, and then employs HD to\nestimate the prediction uncertainties. Through the Dempster-Shafer theory,\nintegration of uncertainty from different modalities, thereby generating a\ncomprehensive result that considers all available representations.\nMathematically, HD proves to better measure the ``distance'' between real data\ndistribution and predictive distribution of the model and improve the\nperformances of multi-class recognition tasks.\n  Specifically, our method surpass the existing state-of-the-art counterparts\non all evaluating benchmarks.\n  We further conduct extensive experiments on different backbones to verify our\nsuperior robustness. It is demonstrated that our method successfully pushes the\ncorresponding performance boundaries. Finally, we perform experiments on more\nchallenging scenarios, \\textit{i.e.}, learning with incomplete or noisy data,\nrevealing that our method exhibits a high tolerance to such corrupted data.\n", "link": "http://arxiv.org/abs/2411.00826v2", "date": "2025-04-17", "relevancy": 2.4043, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6464}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.6005}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty%20Quantification%20via%20H%C3%B6lder%20Divergence%20for%20Multi-View%0A%20%20Representation%20Learning&body=Title%3A%20Uncertainty%20Quantification%20via%20H%C3%B6lder%20Divergence%20for%20Multi-View%0A%20%20Representation%20Learning%0AAuthor%3A%20Yan%20Zhang%20and%20Ming%20Li%20and%20Chun%20Li%20and%20Zhaoxia%20Liu%20and%20Ye%20Zhang%20and%20Fei%20Richard%20Yu%0AAbstract%3A%20%20%20Evidence-based%20deep%20learning%20represents%20a%20burgeoning%20paradigm%20for%20uncertainty%0Aestimation%2C%20offering%20reliable%20predictions%20with%20negligible%20extra%20computational%0Aoverheads.%20Existing%20methods%20usually%20adopt%20Kullback-Leibler%20divergence%20to%0Aestimate%20the%20uncertainty%20of%20network%20predictions%2C%20ignoring%20domain%20gaps%20among%0Avarious%20modalities.%20To%20tackle%20this%20issue%2C%20this%20paper%20introduces%20a%20novel%0Aalgorithm%20based%20on%20H%5C%22older%20Divergence%20%28HD%29%20to%20enhance%20the%20reliability%20of%0Amulti-view%20learning%20by%20addressing%20inherent%20uncertainty%20challenges%20from%0Aincomplete%20or%20noisy%20data.%20Generally%2C%20our%20method%20extracts%20the%20representations%20of%0Amultiple%20modalities%20through%20parallel%20network%20branches%2C%20and%20then%20employs%20HD%20to%0Aestimate%20the%20prediction%20uncertainties.%20Through%20the%20Dempster-Shafer%20theory%2C%0Aintegration%20of%20uncertainty%20from%20different%20modalities%2C%20thereby%20generating%20a%0Acomprehensive%20result%20that%20considers%20all%20available%20representations.%0AMathematically%2C%20HD%20proves%20to%20better%20measure%20the%20%60%60distance%27%27%20between%20real%20data%0Adistribution%20and%20predictive%20distribution%20of%20the%20model%20and%20improve%20the%0Aperformances%20of%20multi-class%20recognition%20tasks.%0A%20%20Specifically%2C%20our%20method%20surpass%20the%20existing%20state-of-the-art%20counterparts%0Aon%20all%20evaluating%20benchmarks.%0A%20%20We%20further%20conduct%20extensive%20experiments%20on%20different%20backbones%20to%20verify%20our%0Asuperior%20robustness.%20It%20is%20demonstrated%20that%20our%20method%20successfully%20pushes%20the%0Acorresponding%20performance%20boundaries.%20Finally%2C%20we%20perform%20experiments%20on%20more%0Achallenging%20scenarios%2C%20%5Ctextit%7Bi.e.%7D%2C%20learning%20with%20incomplete%20or%20noisy%20data%2C%0Arevealing%20that%20our%20method%20exhibits%20a%20high%20tolerance%20to%20such%20corrupted%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.00826v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty%2520Quantification%2520via%2520H%25C3%25B6lder%2520Divergence%2520for%2520Multi-View%250A%2520%2520Representation%2520Learning%26entry.906535625%3DYan%2520Zhang%2520and%2520Ming%2520Li%2520and%2520Chun%2520Li%2520and%2520Zhaoxia%2520Liu%2520and%2520Ye%2520Zhang%2520and%2520Fei%2520Richard%2520Yu%26entry.1292438233%3D%2520%2520Evidence-based%2520deep%2520learning%2520represents%2520a%2520burgeoning%2520paradigm%2520for%2520uncertainty%250Aestimation%252C%2520offering%2520reliable%2520predictions%2520with%2520negligible%2520extra%2520computational%250Aoverheads.%2520Existing%2520methods%2520usually%2520adopt%2520Kullback-Leibler%2520divergence%2520to%250Aestimate%2520the%2520uncertainty%2520of%2520network%2520predictions%252C%2520ignoring%2520domain%2520gaps%2520among%250Avarious%2520modalities.%2520To%2520tackle%2520this%2520issue%252C%2520this%2520paper%2520introduces%2520a%2520novel%250Aalgorithm%2520based%2520on%2520H%255C%2522older%2520Divergence%2520%2528HD%2529%2520to%2520enhance%2520the%2520reliability%2520of%250Amulti-view%2520learning%2520by%2520addressing%2520inherent%2520uncertainty%2520challenges%2520from%250Aincomplete%2520or%2520noisy%2520data.%2520Generally%252C%2520our%2520method%2520extracts%2520the%2520representations%2520of%250Amultiple%2520modalities%2520through%2520parallel%2520network%2520branches%252C%2520and%2520then%2520employs%2520HD%2520to%250Aestimate%2520the%2520prediction%2520uncertainties.%2520Through%2520the%2520Dempster-Shafer%2520theory%252C%250Aintegration%2520of%2520uncertainty%2520from%2520different%2520modalities%252C%2520thereby%2520generating%2520a%250Acomprehensive%2520result%2520that%2520considers%2520all%2520available%2520representations.%250AMathematically%252C%2520HD%2520proves%2520to%2520better%2520measure%2520the%2520%2560%2560distance%2527%2527%2520between%2520real%2520data%250Adistribution%2520and%2520predictive%2520distribution%2520of%2520the%2520model%2520and%2520improve%2520the%250Aperformances%2520of%2520multi-class%2520recognition%2520tasks.%250A%2520%2520Specifically%252C%2520our%2520method%2520surpass%2520the%2520existing%2520state-of-the-art%2520counterparts%250Aon%2520all%2520evaluating%2520benchmarks.%250A%2520%2520We%2520further%2520conduct%2520extensive%2520experiments%2520on%2520different%2520backbones%2520to%2520verify%2520our%250Asuperior%2520robustness.%2520It%2520is%2520demonstrated%2520that%2520our%2520method%2520successfully%2520pushes%2520the%250Acorresponding%2520performance%2520boundaries.%2520Finally%252C%2520we%2520perform%2520experiments%2520on%2520more%250Achallenging%2520scenarios%252C%2520%255Ctextit%257Bi.e.%257D%252C%2520learning%2520with%2520incomplete%2520or%2520noisy%2520data%252C%250Arevealing%2520that%2520our%2520method%2520exhibits%2520a%2520high%2520tolerance%2520to%2520such%2520corrupted%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.00826v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty%20Quantification%20via%20H%C3%B6lder%20Divergence%20for%20Multi-View%0A%20%20Representation%20Learning&entry.906535625=Yan%20Zhang%20and%20Ming%20Li%20and%20Chun%20Li%20and%20Zhaoxia%20Liu%20and%20Ye%20Zhang%20and%20Fei%20Richard%20Yu&entry.1292438233=%20%20Evidence-based%20deep%20learning%20represents%20a%20burgeoning%20paradigm%20for%20uncertainty%0Aestimation%2C%20offering%20reliable%20predictions%20with%20negligible%20extra%20computational%0Aoverheads.%20Existing%20methods%20usually%20adopt%20Kullback-Leibler%20divergence%20to%0Aestimate%20the%20uncertainty%20of%20network%20predictions%2C%20ignoring%20domain%20gaps%20among%0Avarious%20modalities.%20To%20tackle%20this%20issue%2C%20this%20paper%20introduces%20a%20novel%0Aalgorithm%20based%20on%20H%5C%22older%20Divergence%20%28HD%29%20to%20enhance%20the%20reliability%20of%0Amulti-view%20learning%20by%20addressing%20inherent%20uncertainty%20challenges%20from%0Aincomplete%20or%20noisy%20data.%20Generally%2C%20our%20method%20extracts%20the%20representations%20of%0Amultiple%20modalities%20through%20parallel%20network%20branches%2C%20and%20then%20employs%20HD%20to%0Aestimate%20the%20prediction%20uncertainties.%20Through%20the%20Dempster-Shafer%20theory%2C%0Aintegration%20of%20uncertainty%20from%20different%20modalities%2C%20thereby%20generating%20a%0Acomprehensive%20result%20that%20considers%20all%20available%20representations.%0AMathematically%2C%20HD%20proves%20to%20better%20measure%20the%20%60%60distance%27%27%20between%20real%20data%0Adistribution%20and%20predictive%20distribution%20of%20the%20model%20and%20improve%20the%0Aperformances%20of%20multi-class%20recognition%20tasks.%0A%20%20Specifically%2C%20our%20method%20surpass%20the%20existing%20state-of-the-art%20counterparts%0Aon%20all%20evaluating%20benchmarks.%0A%20%20We%20further%20conduct%20extensive%20experiments%20on%20different%20backbones%20to%20verify%20our%0Asuperior%20robustness.%20It%20is%20demonstrated%20that%20our%20method%20successfully%20pushes%20the%0Acorresponding%20performance%20boundaries.%20Finally%2C%20we%20perform%20experiments%20on%20more%0Achallenging%20scenarios%2C%20%5Ctextit%7Bi.e.%7D%2C%20learning%20with%20incomplete%20or%20noisy%20data%2C%0Arevealing%20that%20our%20method%20exhibits%20a%20high%20tolerance%20to%20such%20corrupted%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.00826v2&entry.124074799=Read"},
{"title": "TwoSquared: 4D Generation from 2D Image Pairs", "author": "Lu Sang and Zehranaz Canfes and Dongliang Cao and Riccardo Marin and Florian Bernard and Daniel Cremers", "abstract": "  Despite the astonishing progress in generative AI, 4D dynamic object\ngeneration remains an open challenge. With limited high-quality training data\nand heavy computing requirements, the combination of hallucinating unseen\ngeometry together with unseen movement poses great challenges to generative\nmodels. In this work, we propose TwoSquared as a method to obtain a 4D\nphysically plausible sequence starting from only two 2D RGB images\ncorresponding to the beginning and end of the action. Instead of directly\nsolving the 4D generation problem, TwoSquared decomposes the problem into two\nsteps: 1) an image-to-3D module generation based on the existing generative\nmodel trained on high-quality 3D assets, and 2) a physically inspired\ndeformation module to predict intermediate movements. To this end, our method\ndoes not require templates or object-class-specific prior knowledge and can\ntake in-the-wild images as input. In our experiments, we demonstrate that\nTwoSquared is capable of producing texture-consistent and geometry-consistent\n4D sequences only given 2D images.\n", "link": "http://arxiv.org/abs/2504.12825v1", "date": "2025-04-17", "relevancy": 2.3819, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6319}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5967}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5796}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TwoSquared%3A%204D%20Generation%20from%202D%20Image%20Pairs&body=Title%3A%20TwoSquared%3A%204D%20Generation%20from%202D%20Image%20Pairs%0AAuthor%3A%20Lu%20Sang%20and%20Zehranaz%20Canfes%20and%20Dongliang%20Cao%20and%20Riccardo%20Marin%20and%20Florian%20Bernard%20and%20Daniel%20Cremers%0AAbstract%3A%20%20%20Despite%20the%20astonishing%20progress%20in%20generative%20AI%2C%204D%20dynamic%20object%0Ageneration%20remains%20an%20open%20challenge.%20With%20limited%20high-quality%20training%20data%0Aand%20heavy%20computing%20requirements%2C%20the%20combination%20of%20hallucinating%20unseen%0Ageometry%20together%20with%20unseen%20movement%20poses%20great%20challenges%20to%20generative%0Amodels.%20In%20this%20work%2C%20we%20propose%20TwoSquared%20as%20a%20method%20to%20obtain%20a%204D%0Aphysically%20plausible%20sequence%20starting%20from%20only%20two%202D%20RGB%20images%0Acorresponding%20to%20the%20beginning%20and%20end%20of%20the%20action.%20Instead%20of%20directly%0Asolving%20the%204D%20generation%20problem%2C%20TwoSquared%20decomposes%20the%20problem%20into%20two%0Asteps%3A%201%29%20an%20image-to-3D%20module%20generation%20based%20on%20the%20existing%20generative%0Amodel%20trained%20on%20high-quality%203D%20assets%2C%20and%202%29%20a%20physically%20inspired%0Adeformation%20module%20to%20predict%20intermediate%20movements.%20To%20this%20end%2C%20our%20method%0Adoes%20not%20require%20templates%20or%20object-class-specific%20prior%20knowledge%20and%20can%0Atake%20in-the-wild%20images%20as%20input.%20In%20our%20experiments%2C%20we%20demonstrate%20that%0ATwoSquared%20is%20capable%20of%20producing%20texture-consistent%20and%20geometry-consistent%0A4D%20sequences%20only%20given%202D%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12825v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTwoSquared%253A%25204D%2520Generation%2520from%25202D%2520Image%2520Pairs%26entry.906535625%3DLu%2520Sang%2520and%2520Zehranaz%2520Canfes%2520and%2520Dongliang%2520Cao%2520and%2520Riccardo%2520Marin%2520and%2520Florian%2520Bernard%2520and%2520Daniel%2520Cremers%26entry.1292438233%3D%2520%2520Despite%2520the%2520astonishing%2520progress%2520in%2520generative%2520AI%252C%25204D%2520dynamic%2520object%250Ageneration%2520remains%2520an%2520open%2520challenge.%2520With%2520limited%2520high-quality%2520training%2520data%250Aand%2520heavy%2520computing%2520requirements%252C%2520the%2520combination%2520of%2520hallucinating%2520unseen%250Ageometry%2520together%2520with%2520unseen%2520movement%2520poses%2520great%2520challenges%2520to%2520generative%250Amodels.%2520In%2520this%2520work%252C%2520we%2520propose%2520TwoSquared%2520as%2520a%2520method%2520to%2520obtain%2520a%25204D%250Aphysically%2520plausible%2520sequence%2520starting%2520from%2520only%2520two%25202D%2520RGB%2520images%250Acorresponding%2520to%2520the%2520beginning%2520and%2520end%2520of%2520the%2520action.%2520Instead%2520of%2520directly%250Asolving%2520the%25204D%2520generation%2520problem%252C%2520TwoSquared%2520decomposes%2520the%2520problem%2520into%2520two%250Asteps%253A%25201%2529%2520an%2520image-to-3D%2520module%2520generation%2520based%2520on%2520the%2520existing%2520generative%250Amodel%2520trained%2520on%2520high-quality%25203D%2520assets%252C%2520and%25202%2529%2520a%2520physically%2520inspired%250Adeformation%2520module%2520to%2520predict%2520intermediate%2520movements.%2520To%2520this%2520end%252C%2520our%2520method%250Adoes%2520not%2520require%2520templates%2520or%2520object-class-specific%2520prior%2520knowledge%2520and%2520can%250Atake%2520in-the-wild%2520images%2520as%2520input.%2520In%2520our%2520experiments%252C%2520we%2520demonstrate%2520that%250ATwoSquared%2520is%2520capable%2520of%2520producing%2520texture-consistent%2520and%2520geometry-consistent%250A4D%2520sequences%2520only%2520given%25202D%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12825v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TwoSquared%3A%204D%20Generation%20from%202D%20Image%20Pairs&entry.906535625=Lu%20Sang%20and%20Zehranaz%20Canfes%20and%20Dongliang%20Cao%20and%20Riccardo%20Marin%20and%20Florian%20Bernard%20and%20Daniel%20Cremers&entry.1292438233=%20%20Despite%20the%20astonishing%20progress%20in%20generative%20AI%2C%204D%20dynamic%20object%0Ageneration%20remains%20an%20open%20challenge.%20With%20limited%20high-quality%20training%20data%0Aand%20heavy%20computing%20requirements%2C%20the%20combination%20of%20hallucinating%20unseen%0Ageometry%20together%20with%20unseen%20movement%20poses%20great%20challenges%20to%20generative%0Amodels.%20In%20this%20work%2C%20we%20propose%20TwoSquared%20as%20a%20method%20to%20obtain%20a%204D%0Aphysically%20plausible%20sequence%20starting%20from%20only%20two%202D%20RGB%20images%0Acorresponding%20to%20the%20beginning%20and%20end%20of%20the%20action.%20Instead%20of%20directly%0Asolving%20the%204D%20generation%20problem%2C%20TwoSquared%20decomposes%20the%20problem%20into%20two%0Asteps%3A%201%29%20an%20image-to-3D%20module%20generation%20based%20on%20the%20existing%20generative%0Amodel%20trained%20on%20high-quality%203D%20assets%2C%20and%202%29%20a%20physically%20inspired%0Adeformation%20module%20to%20predict%20intermediate%20movements.%20To%20this%20end%2C%20our%20method%0Adoes%20not%20require%20templates%20or%20object-class-specific%20prior%20knowledge%20and%20can%0Atake%20in-the-wild%20images%20as%20input.%20In%20our%20experiments%2C%20we%20demonstrate%20that%0ATwoSquared%20is%20capable%20of%20producing%20texture-consistent%20and%20geometry-consistent%0A4D%20sequences%20only%20given%202D%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12825v1&entry.124074799=Read"},
{"title": "Beyond the Frame: Generating 360\u00b0 Panoramic Videos from Perspective\n  Videos", "author": "Rundong Luo and Matthew Wallingford and Ali Farhadi and Noah Snavely and Wei-Chiu Ma", "abstract": "  360{\\deg} videos have emerged as a promising medium to represent our dynamic\nvisual world. Compared to the \"tunnel vision\" of standard cameras, their\nborderless field of view offers a more complete perspective of our\nsurroundings. While existing video models excel at producing standard videos,\ntheir ability to generate full panoramic videos remains elusive. In this paper,\nwe investigate the task of video-to-360{\\deg} generation: given a perspective\nvideo as input, our goal is to generate a full panoramic video that is\nconsistent with the original video. Unlike conventional video generation tasks,\nthe output's field of view is significantly larger, and the model is required\nto have a deep understanding of both the spatial layout of the scene and the\ndynamics of objects to maintain spatio-temporal consistency. To address these\nchallenges, we first leverage the abundant 360{\\deg} videos available online\nand develop a high-quality data filtering pipeline to curate pairwise training\ndata. We then carefully design a series of geometry- and motion-aware\noperations to facilitate the learning process and improve the quality of\n360{\\deg} video generation. Experimental results demonstrate that our model can\ngenerate realistic and coherent 360{\\deg} videos from in-the-wild perspective\nvideo. In addition, we showcase its potential applications, including video\nstabilization, camera viewpoint control, and interactive visual question\nanswering.\n", "link": "http://arxiv.org/abs/2504.07940v2", "date": "2025-04-17", "relevancy": 2.3757, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.622}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5893}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20the%20Frame%3A%20Generating%20360%C2%B0%20Panoramic%20Videos%20from%20Perspective%0A%20%20Videos&body=Title%3A%20Beyond%20the%20Frame%3A%20Generating%20360%C2%B0%20Panoramic%20Videos%20from%20Perspective%0A%20%20Videos%0AAuthor%3A%20Rundong%20Luo%20and%20Matthew%20Wallingford%20and%20Ali%20Farhadi%20and%20Noah%20Snavely%20and%20Wei-Chiu%20Ma%0AAbstract%3A%20%20%20360%7B%5Cdeg%7D%20videos%20have%20emerged%20as%20a%20promising%20medium%20to%20represent%20our%20dynamic%0Avisual%20world.%20Compared%20to%20the%20%22tunnel%20vision%22%20of%20standard%20cameras%2C%20their%0Aborderless%20field%20of%20view%20offers%20a%20more%20complete%20perspective%20of%20our%0Asurroundings.%20While%20existing%20video%20models%20excel%20at%20producing%20standard%20videos%2C%0Atheir%20ability%20to%20generate%20full%20panoramic%20videos%20remains%20elusive.%20In%20this%20paper%2C%0Awe%20investigate%20the%20task%20of%20video-to-360%7B%5Cdeg%7D%20generation%3A%20given%20a%20perspective%0Avideo%20as%20input%2C%20our%20goal%20is%20to%20generate%20a%20full%20panoramic%20video%20that%20is%0Aconsistent%20with%20the%20original%20video.%20Unlike%20conventional%20video%20generation%20tasks%2C%0Athe%20output%27s%20field%20of%20view%20is%20significantly%20larger%2C%20and%20the%20model%20is%20required%0Ato%20have%20a%20deep%20understanding%20of%20both%20the%20spatial%20layout%20of%20the%20scene%20and%20the%0Adynamics%20of%20objects%20to%20maintain%20spatio-temporal%20consistency.%20To%20address%20these%0Achallenges%2C%20we%20first%20leverage%20the%20abundant%20360%7B%5Cdeg%7D%20videos%20available%20online%0Aand%20develop%20a%20high-quality%20data%20filtering%20pipeline%20to%20curate%20pairwise%20training%0Adata.%20We%20then%20carefully%20design%20a%20series%20of%20geometry-%20and%20motion-aware%0Aoperations%20to%20facilitate%20the%20learning%20process%20and%20improve%20the%20quality%20of%0A360%7B%5Cdeg%7D%20video%20generation.%20Experimental%20results%20demonstrate%20that%20our%20model%20can%0Agenerate%20realistic%20and%20coherent%20360%7B%5Cdeg%7D%20videos%20from%20in-the-wild%20perspective%0Avideo.%20In%20addition%2C%20we%20showcase%20its%20potential%20applications%2C%20including%20video%0Astabilization%2C%20camera%20viewpoint%20control%2C%20and%20interactive%20visual%20question%0Aanswering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.07940v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520the%2520Frame%253A%2520Generating%2520360%25C2%25B0%2520Panoramic%2520Videos%2520from%2520Perspective%250A%2520%2520Videos%26entry.906535625%3DRundong%2520Luo%2520and%2520Matthew%2520Wallingford%2520and%2520Ali%2520Farhadi%2520and%2520Noah%2520Snavely%2520and%2520Wei-Chiu%2520Ma%26entry.1292438233%3D%2520%2520360%257B%255Cdeg%257D%2520videos%2520have%2520emerged%2520as%2520a%2520promising%2520medium%2520to%2520represent%2520our%2520dynamic%250Avisual%2520world.%2520Compared%2520to%2520the%2520%2522tunnel%2520vision%2522%2520of%2520standard%2520cameras%252C%2520their%250Aborderless%2520field%2520of%2520view%2520offers%2520a%2520more%2520complete%2520perspective%2520of%2520our%250Asurroundings.%2520While%2520existing%2520video%2520models%2520excel%2520at%2520producing%2520standard%2520videos%252C%250Atheir%2520ability%2520to%2520generate%2520full%2520panoramic%2520videos%2520remains%2520elusive.%2520In%2520this%2520paper%252C%250Awe%2520investigate%2520the%2520task%2520of%2520video-to-360%257B%255Cdeg%257D%2520generation%253A%2520given%2520a%2520perspective%250Avideo%2520as%2520input%252C%2520our%2520goal%2520is%2520to%2520generate%2520a%2520full%2520panoramic%2520video%2520that%2520is%250Aconsistent%2520with%2520the%2520original%2520video.%2520Unlike%2520conventional%2520video%2520generation%2520tasks%252C%250Athe%2520output%2527s%2520field%2520of%2520view%2520is%2520significantly%2520larger%252C%2520and%2520the%2520model%2520is%2520required%250Ato%2520have%2520a%2520deep%2520understanding%2520of%2520both%2520the%2520spatial%2520layout%2520of%2520the%2520scene%2520and%2520the%250Adynamics%2520of%2520objects%2520to%2520maintain%2520spatio-temporal%2520consistency.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520first%2520leverage%2520the%2520abundant%2520360%257B%255Cdeg%257D%2520videos%2520available%2520online%250Aand%2520develop%2520a%2520high-quality%2520data%2520filtering%2520pipeline%2520to%2520curate%2520pairwise%2520training%250Adata.%2520We%2520then%2520carefully%2520design%2520a%2520series%2520of%2520geometry-%2520and%2520motion-aware%250Aoperations%2520to%2520facilitate%2520the%2520learning%2520process%2520and%2520improve%2520the%2520quality%2520of%250A360%257B%255Cdeg%257D%2520video%2520generation.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520model%2520can%250Agenerate%2520realistic%2520and%2520coherent%2520360%257B%255Cdeg%257D%2520videos%2520from%2520in-the-wild%2520perspective%250Avideo.%2520In%2520addition%252C%2520we%2520showcase%2520its%2520potential%2520applications%252C%2520including%2520video%250Astabilization%252C%2520camera%2520viewpoint%2520control%252C%2520and%2520interactive%2520visual%2520question%250Aanswering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.07940v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20the%20Frame%3A%20Generating%20360%C2%B0%20Panoramic%20Videos%20from%20Perspective%0A%20%20Videos&entry.906535625=Rundong%20Luo%20and%20Matthew%20Wallingford%20and%20Ali%20Farhadi%20and%20Noah%20Snavely%20and%20Wei-Chiu%20Ma&entry.1292438233=%20%20360%7B%5Cdeg%7D%20videos%20have%20emerged%20as%20a%20promising%20medium%20to%20represent%20our%20dynamic%0Avisual%20world.%20Compared%20to%20the%20%22tunnel%20vision%22%20of%20standard%20cameras%2C%20their%0Aborderless%20field%20of%20view%20offers%20a%20more%20complete%20perspective%20of%20our%0Asurroundings.%20While%20existing%20video%20models%20excel%20at%20producing%20standard%20videos%2C%0Atheir%20ability%20to%20generate%20full%20panoramic%20videos%20remains%20elusive.%20In%20this%20paper%2C%0Awe%20investigate%20the%20task%20of%20video-to-360%7B%5Cdeg%7D%20generation%3A%20given%20a%20perspective%0Avideo%20as%20input%2C%20our%20goal%20is%20to%20generate%20a%20full%20panoramic%20video%20that%20is%0Aconsistent%20with%20the%20original%20video.%20Unlike%20conventional%20video%20generation%20tasks%2C%0Athe%20output%27s%20field%20of%20view%20is%20significantly%20larger%2C%20and%20the%20model%20is%20required%0Ato%20have%20a%20deep%20understanding%20of%20both%20the%20spatial%20layout%20of%20the%20scene%20and%20the%0Adynamics%20of%20objects%20to%20maintain%20spatio-temporal%20consistency.%20To%20address%20these%0Achallenges%2C%20we%20first%20leverage%20the%20abundant%20360%7B%5Cdeg%7D%20videos%20available%20online%0Aand%20develop%20a%20high-quality%20data%20filtering%20pipeline%20to%20curate%20pairwise%20training%0Adata.%20We%20then%20carefully%20design%20a%20series%20of%20geometry-%20and%20motion-aware%0Aoperations%20to%20facilitate%20the%20learning%20process%20and%20improve%20the%20quality%20of%0A360%7B%5Cdeg%7D%20video%20generation.%20Experimental%20results%20demonstrate%20that%20our%20model%20can%0Agenerate%20realistic%20and%20coherent%20360%7B%5Cdeg%7D%20videos%20from%20in-the-wild%20perspective%0Avideo.%20In%20addition%2C%20we%20showcase%20its%20potential%20applications%2C%20including%20video%0Astabilization%2C%20camera%20viewpoint%20control%2C%20and%20interactive%20visual%20question%0Aanswering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.07940v2&entry.124074799=Read"},
{"title": "iHHO-SMOTe: A Cleansed Approach for Handling Outliers and Reducing Noise\n  to Improve Imbalanced Data Classification", "author": "Khaled SH. Raslan and Almohammady S. Alsharkawy and K. R. Raslan", "abstract": "  Classifying imbalanced datasets remains a significant challenge in machine\nlearning, particularly with big data where instances are unevenly distributed\namong classes, leading to class imbalance issues that impact classifier\nperformance. While Synthetic Minority Over-sampling Technique (SMOTE) addresses\nthis challenge by generating new instances for the under-represented minority\nclass, it faces obstacles in the form of noise and outliers during the creation\nof new samples. In this paper, a proposed approach, iHHO-SMOTe, which addresses\nthe limitations of SMOTE by first cleansing the data from noise points. This\nprocess involves employing feature selection using a random forest to identify\nthe most valuable features, followed by applying the Density-Based Spatial\nClustering of Applications with Noise (DBSCAN) algorithm to detect outliers\nbased on the selected features. The identified outliers from the minority\nclasses are then removed, creating a refined dataset for subsequent\noversampling using the hybrid approach called iHHO-SMOTe. The comprehensive\nexperiments across diverse datasets demonstrate the exceptional performance of\nthe proposed model, with an AUC score exceeding 0.99, a high G-means score of\n0.99 highlighting its robustness, and an outstanding F1-score consistently\nexceeding 0.967. These findings collectively establish Cleansed iHHO-SMOTe as a\nformidable contender in addressing imbalanced datasets, focusing on noise\nreduction and outlier handling for improved classification models.\n", "link": "http://arxiv.org/abs/2504.12850v1", "date": "2025-04-17", "relevancy": 2.3718, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5125}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4588}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20iHHO-SMOTe%3A%20A%20Cleansed%20Approach%20for%20Handling%20Outliers%20and%20Reducing%20Noise%0A%20%20to%20Improve%20Imbalanced%20Data%20Classification&body=Title%3A%20iHHO-SMOTe%3A%20A%20Cleansed%20Approach%20for%20Handling%20Outliers%20and%20Reducing%20Noise%0A%20%20to%20Improve%20Imbalanced%20Data%20Classification%0AAuthor%3A%20Khaled%20SH.%20Raslan%20and%20Almohammady%20S.%20Alsharkawy%20and%20K.%20R.%20Raslan%0AAbstract%3A%20%20%20Classifying%20imbalanced%20datasets%20remains%20a%20significant%20challenge%20in%20machine%0Alearning%2C%20particularly%20with%20big%20data%20where%20instances%20are%20unevenly%20distributed%0Aamong%20classes%2C%20leading%20to%20class%20imbalance%20issues%20that%20impact%20classifier%0Aperformance.%20While%20Synthetic%20Minority%20Over-sampling%20Technique%20%28SMOTE%29%20addresses%0Athis%20challenge%20by%20generating%20new%20instances%20for%20the%20under-represented%20minority%0Aclass%2C%20it%20faces%20obstacles%20in%20the%20form%20of%20noise%20and%20outliers%20during%20the%20creation%0Aof%20new%20samples.%20In%20this%20paper%2C%20a%20proposed%20approach%2C%20iHHO-SMOTe%2C%20which%20addresses%0Athe%20limitations%20of%20SMOTE%20by%20first%20cleansing%20the%20data%20from%20noise%20points.%20This%0Aprocess%20involves%20employing%20feature%20selection%20using%20a%20random%20forest%20to%20identify%0Athe%20most%20valuable%20features%2C%20followed%20by%20applying%20the%20Density-Based%20Spatial%0AClustering%20of%20Applications%20with%20Noise%20%28DBSCAN%29%20algorithm%20to%20detect%20outliers%0Abased%20on%20the%20selected%20features.%20The%20identified%20outliers%20from%20the%20minority%0Aclasses%20are%20then%20removed%2C%20creating%20a%20refined%20dataset%20for%20subsequent%0Aoversampling%20using%20the%20hybrid%20approach%20called%20iHHO-SMOTe.%20The%20comprehensive%0Aexperiments%20across%20diverse%20datasets%20demonstrate%20the%20exceptional%20performance%20of%0Athe%20proposed%20model%2C%20with%20an%20AUC%20score%20exceeding%200.99%2C%20a%20high%20G-means%20score%20of%0A0.99%20highlighting%20its%20robustness%2C%20and%20an%20outstanding%20F1-score%20consistently%0Aexceeding%200.967.%20These%20findings%20collectively%20establish%20Cleansed%20iHHO-SMOTe%20as%20a%0Aformidable%20contender%20in%20addressing%20imbalanced%20datasets%2C%20focusing%20on%20noise%0Areduction%20and%20outlier%20handling%20for%20improved%20classification%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12850v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DiHHO-SMOTe%253A%2520A%2520Cleansed%2520Approach%2520for%2520Handling%2520Outliers%2520and%2520Reducing%2520Noise%250A%2520%2520to%2520Improve%2520Imbalanced%2520Data%2520Classification%26entry.906535625%3DKhaled%2520SH.%2520Raslan%2520and%2520Almohammady%2520S.%2520Alsharkawy%2520and%2520K.%2520R.%2520Raslan%26entry.1292438233%3D%2520%2520Classifying%2520imbalanced%2520datasets%2520remains%2520a%2520significant%2520challenge%2520in%2520machine%250Alearning%252C%2520particularly%2520with%2520big%2520data%2520where%2520instances%2520are%2520unevenly%2520distributed%250Aamong%2520classes%252C%2520leading%2520to%2520class%2520imbalance%2520issues%2520that%2520impact%2520classifier%250Aperformance.%2520While%2520Synthetic%2520Minority%2520Over-sampling%2520Technique%2520%2528SMOTE%2529%2520addresses%250Athis%2520challenge%2520by%2520generating%2520new%2520instances%2520for%2520the%2520under-represented%2520minority%250Aclass%252C%2520it%2520faces%2520obstacles%2520in%2520the%2520form%2520of%2520noise%2520and%2520outliers%2520during%2520the%2520creation%250Aof%2520new%2520samples.%2520In%2520this%2520paper%252C%2520a%2520proposed%2520approach%252C%2520iHHO-SMOTe%252C%2520which%2520addresses%250Athe%2520limitations%2520of%2520SMOTE%2520by%2520first%2520cleansing%2520the%2520data%2520from%2520noise%2520points.%2520This%250Aprocess%2520involves%2520employing%2520feature%2520selection%2520using%2520a%2520random%2520forest%2520to%2520identify%250Athe%2520most%2520valuable%2520features%252C%2520followed%2520by%2520applying%2520the%2520Density-Based%2520Spatial%250AClustering%2520of%2520Applications%2520with%2520Noise%2520%2528DBSCAN%2529%2520algorithm%2520to%2520detect%2520outliers%250Abased%2520on%2520the%2520selected%2520features.%2520The%2520identified%2520outliers%2520from%2520the%2520minority%250Aclasses%2520are%2520then%2520removed%252C%2520creating%2520a%2520refined%2520dataset%2520for%2520subsequent%250Aoversampling%2520using%2520the%2520hybrid%2520approach%2520called%2520iHHO-SMOTe.%2520The%2520comprehensive%250Aexperiments%2520across%2520diverse%2520datasets%2520demonstrate%2520the%2520exceptional%2520performance%2520of%250Athe%2520proposed%2520model%252C%2520with%2520an%2520AUC%2520score%2520exceeding%25200.99%252C%2520a%2520high%2520G-means%2520score%2520of%250A0.99%2520highlighting%2520its%2520robustness%252C%2520and%2520an%2520outstanding%2520F1-score%2520consistently%250Aexceeding%25200.967.%2520These%2520findings%2520collectively%2520establish%2520Cleansed%2520iHHO-SMOTe%2520as%2520a%250Aformidable%2520contender%2520in%2520addressing%2520imbalanced%2520datasets%252C%2520focusing%2520on%2520noise%250Areduction%2520and%2520outlier%2520handling%2520for%2520improved%2520classification%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12850v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=iHHO-SMOTe%3A%20A%20Cleansed%20Approach%20for%20Handling%20Outliers%20and%20Reducing%20Noise%0A%20%20to%20Improve%20Imbalanced%20Data%20Classification&entry.906535625=Khaled%20SH.%20Raslan%20and%20Almohammady%20S.%20Alsharkawy%20and%20K.%20R.%20Raslan&entry.1292438233=%20%20Classifying%20imbalanced%20datasets%20remains%20a%20significant%20challenge%20in%20machine%0Alearning%2C%20particularly%20with%20big%20data%20where%20instances%20are%20unevenly%20distributed%0Aamong%20classes%2C%20leading%20to%20class%20imbalance%20issues%20that%20impact%20classifier%0Aperformance.%20While%20Synthetic%20Minority%20Over-sampling%20Technique%20%28SMOTE%29%20addresses%0Athis%20challenge%20by%20generating%20new%20instances%20for%20the%20under-represented%20minority%0Aclass%2C%20it%20faces%20obstacles%20in%20the%20form%20of%20noise%20and%20outliers%20during%20the%20creation%0Aof%20new%20samples.%20In%20this%20paper%2C%20a%20proposed%20approach%2C%20iHHO-SMOTe%2C%20which%20addresses%0Athe%20limitations%20of%20SMOTE%20by%20first%20cleansing%20the%20data%20from%20noise%20points.%20This%0Aprocess%20involves%20employing%20feature%20selection%20using%20a%20random%20forest%20to%20identify%0Athe%20most%20valuable%20features%2C%20followed%20by%20applying%20the%20Density-Based%20Spatial%0AClustering%20of%20Applications%20with%20Noise%20%28DBSCAN%29%20algorithm%20to%20detect%20outliers%0Abased%20on%20the%20selected%20features.%20The%20identified%20outliers%20from%20the%20minority%0Aclasses%20are%20then%20removed%2C%20creating%20a%20refined%20dataset%20for%20subsequent%0Aoversampling%20using%20the%20hybrid%20approach%20called%20iHHO-SMOTe.%20The%20comprehensive%0Aexperiments%20across%20diverse%20datasets%20demonstrate%20the%20exceptional%20performance%20of%0Athe%20proposed%20model%2C%20with%20an%20AUC%20score%20exceeding%200.99%2C%20a%20high%20G-means%20score%20of%0A0.99%20highlighting%20its%20robustness%2C%20and%20an%20outstanding%20F1-score%20consistently%0Aexceeding%200.967.%20These%20findings%20collectively%20establish%20Cleansed%20iHHO-SMOTe%20as%20a%0Aformidable%20contender%20in%20addressing%20imbalanced%20datasets%2C%20focusing%20on%20noise%0Areduction%20and%20outlier%20handling%20for%20improved%20classification%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12850v1&entry.124074799=Read"},
{"title": "Long Range Navigator (LRN): Extending robot planning horizons beyond\n  metric maps", "author": "Matt Schmittle and Rohan Baijal and Nathan Hatch and Rosario Scalise and Mateo Guaman Castro and Sidharth Talia and Khimya Khetarpal and Byron Boots and Siddhartha Srinivasa", "abstract": "  A robot navigating an outdoor environment with no prior knowledge of the\nspace must rely on its local sensing to perceive its surroundings and plan.\nThis can come in the form of a local metric map or local policy with some fixed\nhorizon. Beyond that, there is a fog of unknown space marked with some fixed\ncost. A limited planning horizon can often result in myopic decisions leading\nthe robot off course or worse, into very difficult terrain. Ideally, we would\nlike the robot to have full knowledge that can be orders of magnitude larger\nthan a local cost map. In practice, this is intractable due to sparse sensing\ninformation and often computationally expensive. In this work, we make a key\nobservation that long-range navigation only necessitates identifying good\nfrontier directions for planning instead of full map knowledge. To this end, we\npropose Long Range Navigator (LRN), that learns an intermediate affordance\nrepresentation mapping high-dimensional camera images to `affordable' frontiers\nfor planning, and then optimizing for maximum alignment with the desired goal.\nLRN notably is trained entirely on unlabeled ego-centric videos making it easy\nto scale and adapt to new platforms. Through extensive off-road experiments on\nSpot and a Big Vehicle, we find that augmenting existing navigation stacks with\nLRN reduces human interventions at test-time and leads to faster decision\nmaking indicating the relevance of LRN. https://personalrobotics.github.io/lrn\n", "link": "http://arxiv.org/abs/2504.13149v1", "date": "2025-04-17", "relevancy": 2.367, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6047}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5937}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long%20Range%20Navigator%20%28LRN%29%3A%20Extending%20robot%20planning%20horizons%20beyond%0A%20%20metric%20maps&body=Title%3A%20Long%20Range%20Navigator%20%28LRN%29%3A%20Extending%20robot%20planning%20horizons%20beyond%0A%20%20metric%20maps%0AAuthor%3A%20Matt%20Schmittle%20and%20Rohan%20Baijal%20and%20Nathan%20Hatch%20and%20Rosario%20Scalise%20and%20Mateo%20Guaman%20Castro%20and%20Sidharth%20Talia%20and%20Khimya%20Khetarpal%20and%20Byron%20Boots%20and%20Siddhartha%20Srinivasa%0AAbstract%3A%20%20%20A%20robot%20navigating%20an%20outdoor%20environment%20with%20no%20prior%20knowledge%20of%20the%0Aspace%20must%20rely%20on%20its%20local%20sensing%20to%20perceive%20its%20surroundings%20and%20plan.%0AThis%20can%20come%20in%20the%20form%20of%20a%20local%20metric%20map%20or%20local%20policy%20with%20some%20fixed%0Ahorizon.%20Beyond%20that%2C%20there%20is%20a%20fog%20of%20unknown%20space%20marked%20with%20some%20fixed%0Acost.%20A%20limited%20planning%20horizon%20can%20often%20result%20in%20myopic%20decisions%20leading%0Athe%20robot%20off%20course%20or%20worse%2C%20into%20very%20difficult%20terrain.%20Ideally%2C%20we%20would%0Alike%20the%20robot%20to%20have%20full%20knowledge%20that%20can%20be%20orders%20of%20magnitude%20larger%0Athan%20a%20local%20cost%20map.%20In%20practice%2C%20this%20is%20intractable%20due%20to%20sparse%20sensing%0Ainformation%20and%20often%20computationally%20expensive.%20In%20this%20work%2C%20we%20make%20a%20key%0Aobservation%20that%20long-range%20navigation%20only%20necessitates%20identifying%20good%0Afrontier%20directions%20for%20planning%20instead%20of%20full%20map%20knowledge.%20To%20this%20end%2C%20we%0Apropose%20Long%20Range%20Navigator%20%28LRN%29%2C%20that%20learns%20an%20intermediate%20affordance%0Arepresentation%20mapping%20high-dimensional%20camera%20images%20to%20%60affordable%27%20frontiers%0Afor%20planning%2C%20and%20then%20optimizing%20for%20maximum%20alignment%20with%20the%20desired%20goal.%0ALRN%20notably%20is%20trained%20entirely%20on%20unlabeled%20ego-centric%20videos%20making%20it%20easy%0Ato%20scale%20and%20adapt%20to%20new%20platforms.%20Through%20extensive%20off-road%20experiments%20on%0ASpot%20and%20a%20Big%20Vehicle%2C%20we%20find%20that%20augmenting%20existing%20navigation%20stacks%20with%0ALRN%20reduces%20human%20interventions%20at%20test-time%20and%20leads%20to%20faster%20decision%0Amaking%20indicating%20the%20relevance%20of%20LRN.%20https%3A//personalrobotics.github.io/lrn%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13149v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong%2520Range%2520Navigator%2520%2528LRN%2529%253A%2520Extending%2520robot%2520planning%2520horizons%2520beyond%250A%2520%2520metric%2520maps%26entry.906535625%3DMatt%2520Schmittle%2520and%2520Rohan%2520Baijal%2520and%2520Nathan%2520Hatch%2520and%2520Rosario%2520Scalise%2520and%2520Mateo%2520Guaman%2520Castro%2520and%2520Sidharth%2520Talia%2520and%2520Khimya%2520Khetarpal%2520and%2520Byron%2520Boots%2520and%2520Siddhartha%2520Srinivasa%26entry.1292438233%3D%2520%2520A%2520robot%2520navigating%2520an%2520outdoor%2520environment%2520with%2520no%2520prior%2520knowledge%2520of%2520the%250Aspace%2520must%2520rely%2520on%2520its%2520local%2520sensing%2520to%2520perceive%2520its%2520surroundings%2520and%2520plan.%250AThis%2520can%2520come%2520in%2520the%2520form%2520of%2520a%2520local%2520metric%2520map%2520or%2520local%2520policy%2520with%2520some%2520fixed%250Ahorizon.%2520Beyond%2520that%252C%2520there%2520is%2520a%2520fog%2520of%2520unknown%2520space%2520marked%2520with%2520some%2520fixed%250Acost.%2520A%2520limited%2520planning%2520horizon%2520can%2520often%2520result%2520in%2520myopic%2520decisions%2520leading%250Athe%2520robot%2520off%2520course%2520or%2520worse%252C%2520into%2520very%2520difficult%2520terrain.%2520Ideally%252C%2520we%2520would%250Alike%2520the%2520robot%2520to%2520have%2520full%2520knowledge%2520that%2520can%2520be%2520orders%2520of%2520magnitude%2520larger%250Athan%2520a%2520local%2520cost%2520map.%2520In%2520practice%252C%2520this%2520is%2520intractable%2520due%2520to%2520sparse%2520sensing%250Ainformation%2520and%2520often%2520computationally%2520expensive.%2520In%2520this%2520work%252C%2520we%2520make%2520a%2520key%250Aobservation%2520that%2520long-range%2520navigation%2520only%2520necessitates%2520identifying%2520good%250Afrontier%2520directions%2520for%2520planning%2520instead%2520of%2520full%2520map%2520knowledge.%2520To%2520this%2520end%252C%2520we%250Apropose%2520Long%2520Range%2520Navigator%2520%2528LRN%2529%252C%2520that%2520learns%2520an%2520intermediate%2520affordance%250Arepresentation%2520mapping%2520high-dimensional%2520camera%2520images%2520to%2520%2560affordable%2527%2520frontiers%250Afor%2520planning%252C%2520and%2520then%2520optimizing%2520for%2520maximum%2520alignment%2520with%2520the%2520desired%2520goal.%250ALRN%2520notably%2520is%2520trained%2520entirely%2520on%2520unlabeled%2520ego-centric%2520videos%2520making%2520it%2520easy%250Ato%2520scale%2520and%2520adapt%2520to%2520new%2520platforms.%2520Through%2520extensive%2520off-road%2520experiments%2520on%250ASpot%2520and%2520a%2520Big%2520Vehicle%252C%2520we%2520find%2520that%2520augmenting%2520existing%2520navigation%2520stacks%2520with%250ALRN%2520reduces%2520human%2520interventions%2520at%2520test-time%2520and%2520leads%2520to%2520faster%2520decision%250Amaking%2520indicating%2520the%2520relevance%2520of%2520LRN.%2520https%253A//personalrobotics.github.io/lrn%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13149v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long%20Range%20Navigator%20%28LRN%29%3A%20Extending%20robot%20planning%20horizons%20beyond%0A%20%20metric%20maps&entry.906535625=Matt%20Schmittle%20and%20Rohan%20Baijal%20and%20Nathan%20Hatch%20and%20Rosario%20Scalise%20and%20Mateo%20Guaman%20Castro%20and%20Sidharth%20Talia%20and%20Khimya%20Khetarpal%20and%20Byron%20Boots%20and%20Siddhartha%20Srinivasa&entry.1292438233=%20%20A%20robot%20navigating%20an%20outdoor%20environment%20with%20no%20prior%20knowledge%20of%20the%0Aspace%20must%20rely%20on%20its%20local%20sensing%20to%20perceive%20its%20surroundings%20and%20plan.%0AThis%20can%20come%20in%20the%20form%20of%20a%20local%20metric%20map%20or%20local%20policy%20with%20some%20fixed%0Ahorizon.%20Beyond%20that%2C%20there%20is%20a%20fog%20of%20unknown%20space%20marked%20with%20some%20fixed%0Acost.%20A%20limited%20planning%20horizon%20can%20often%20result%20in%20myopic%20decisions%20leading%0Athe%20robot%20off%20course%20or%20worse%2C%20into%20very%20difficult%20terrain.%20Ideally%2C%20we%20would%0Alike%20the%20robot%20to%20have%20full%20knowledge%20that%20can%20be%20orders%20of%20magnitude%20larger%0Athan%20a%20local%20cost%20map.%20In%20practice%2C%20this%20is%20intractable%20due%20to%20sparse%20sensing%0Ainformation%20and%20often%20computationally%20expensive.%20In%20this%20work%2C%20we%20make%20a%20key%0Aobservation%20that%20long-range%20navigation%20only%20necessitates%20identifying%20good%0Afrontier%20directions%20for%20planning%20instead%20of%20full%20map%20knowledge.%20To%20this%20end%2C%20we%0Apropose%20Long%20Range%20Navigator%20%28LRN%29%2C%20that%20learns%20an%20intermediate%20affordance%0Arepresentation%20mapping%20high-dimensional%20camera%20images%20to%20%60affordable%27%20frontiers%0Afor%20planning%2C%20and%20then%20optimizing%20for%20maximum%20alignment%20with%20the%20desired%20goal.%0ALRN%20notably%20is%20trained%20entirely%20on%20unlabeled%20ego-centric%20videos%20making%20it%20easy%0Ato%20scale%20and%20adapt%20to%20new%20platforms.%20Through%20extensive%20off-road%20experiments%20on%0ASpot%20and%20a%20Big%20Vehicle%2C%20we%20find%20that%20augmenting%20existing%20navigation%20stacks%20with%0ALRN%20reduces%20human%20interventions%20at%20test-time%20and%20leads%20to%20faster%20decision%0Amaking%20indicating%20the%20relevance%20of%20LRN.%20https%3A//personalrobotics.github.io/lrn%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13149v1&entry.124074799=Read"},
{"title": "TTRD3: Texture Transfer Residual Denoising Dual Diffusion Model for\n  Remote Sensing Image Super-Resolution", "author": "Yide Liu and Haijiang Sun and Xiaowen Zhang and Qiaoyuan Liu and Zhouchang Chen and Chongzhuo Xiao", "abstract": "  Remote Sensing Image Super-Resolution (RSISR) reconstructs high-resolution\n(HR) remote sensing images from low-resolution inputs to support fine-grained\nground object interpretation. Existing methods face three key challenges: (1)\nDifficulty in extracting multi-scale features from spatially heterogeneous RS\nscenes, (2) Limited prior information causing semantic inconsistency in\nreconstructions, and (3) Trade-off imbalance between geometric accuracy and\nvisual quality. To address these issues, we propose the Texture Transfer\nResidual Denoising Dual Diffusion Model (TTRD3) with three innovations: First,\na Multi-scale Feature Aggregation Block (MFAB) employing parallel heterogeneous\nconvolutional kernels for multi-scale feature extraction. Second, a Sparse\nTexture Transfer Guidance (STTG) module that transfers HR texture priors from\nreference images of similar scenes. Third, a Residual Denoising Dual Diffusion\nModel (RDDM) framework combining residual diffusion for deterministic\nreconstruction and noise diffusion for diverse generation. Experiments on\nmulti-source RS datasets demonstrate TTRD3's superiority over state-of-the-art\nmethods, achieving 1.43% LPIPS improvement and 3.67% FID enhancement compared\nto best-performing baselines. Code/model: https://github.com/LED-666/TTRD3.\n", "link": "http://arxiv.org/abs/2504.13026v1", "date": "2025-04-17", "relevancy": 2.3539, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6058}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5997}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TTRD3%3A%20Texture%20Transfer%20Residual%20Denoising%20Dual%20Diffusion%20Model%20for%0A%20%20Remote%20Sensing%20Image%20Super-Resolution&body=Title%3A%20TTRD3%3A%20Texture%20Transfer%20Residual%20Denoising%20Dual%20Diffusion%20Model%20for%0A%20%20Remote%20Sensing%20Image%20Super-Resolution%0AAuthor%3A%20Yide%20Liu%20and%20Haijiang%20Sun%20and%20Xiaowen%20Zhang%20and%20Qiaoyuan%20Liu%20and%20Zhouchang%20Chen%20and%20Chongzhuo%20Xiao%0AAbstract%3A%20%20%20Remote%20Sensing%20Image%20Super-Resolution%20%28RSISR%29%20reconstructs%20high-resolution%0A%28HR%29%20remote%20sensing%20images%20from%20low-resolution%20inputs%20to%20support%20fine-grained%0Aground%20object%20interpretation.%20Existing%20methods%20face%20three%20key%20challenges%3A%20%281%29%0ADifficulty%20in%20extracting%20multi-scale%20features%20from%20spatially%20heterogeneous%20RS%0Ascenes%2C%20%282%29%20Limited%20prior%20information%20causing%20semantic%20inconsistency%20in%0Areconstructions%2C%20and%20%283%29%20Trade-off%20imbalance%20between%20geometric%20accuracy%20and%0Avisual%20quality.%20To%20address%20these%20issues%2C%20we%20propose%20the%20Texture%20Transfer%0AResidual%20Denoising%20Dual%20Diffusion%20Model%20%28TTRD3%29%20with%20three%20innovations%3A%20First%2C%0Aa%20Multi-scale%20Feature%20Aggregation%20Block%20%28MFAB%29%20employing%20parallel%20heterogeneous%0Aconvolutional%20kernels%20for%20multi-scale%20feature%20extraction.%20Second%2C%20a%20Sparse%0ATexture%20Transfer%20Guidance%20%28STTG%29%20module%20that%20transfers%20HR%20texture%20priors%20from%0Areference%20images%20of%20similar%20scenes.%20Third%2C%20a%20Residual%20Denoising%20Dual%20Diffusion%0AModel%20%28RDDM%29%20framework%20combining%20residual%20diffusion%20for%20deterministic%0Areconstruction%20and%20noise%20diffusion%20for%20diverse%20generation.%20Experiments%20on%0Amulti-source%20RS%20datasets%20demonstrate%20TTRD3%27s%20superiority%20over%20state-of-the-art%0Amethods%2C%20achieving%201.43%25%20LPIPS%20improvement%20and%203.67%25%20FID%20enhancement%20compared%0Ato%20best-performing%20baselines.%20Code/model%3A%20https%3A//github.com/LED-666/TTRD3.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13026v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTTRD3%253A%2520Texture%2520Transfer%2520Residual%2520Denoising%2520Dual%2520Diffusion%2520Model%2520for%250A%2520%2520Remote%2520Sensing%2520Image%2520Super-Resolution%26entry.906535625%3DYide%2520Liu%2520and%2520Haijiang%2520Sun%2520and%2520Xiaowen%2520Zhang%2520and%2520Qiaoyuan%2520Liu%2520and%2520Zhouchang%2520Chen%2520and%2520Chongzhuo%2520Xiao%26entry.1292438233%3D%2520%2520Remote%2520Sensing%2520Image%2520Super-Resolution%2520%2528RSISR%2529%2520reconstructs%2520high-resolution%250A%2528HR%2529%2520remote%2520sensing%2520images%2520from%2520low-resolution%2520inputs%2520to%2520support%2520fine-grained%250Aground%2520object%2520interpretation.%2520Existing%2520methods%2520face%2520three%2520key%2520challenges%253A%2520%25281%2529%250ADifficulty%2520in%2520extracting%2520multi-scale%2520features%2520from%2520spatially%2520heterogeneous%2520RS%250Ascenes%252C%2520%25282%2529%2520Limited%2520prior%2520information%2520causing%2520semantic%2520inconsistency%2520in%250Areconstructions%252C%2520and%2520%25283%2529%2520Trade-off%2520imbalance%2520between%2520geometric%2520accuracy%2520and%250Avisual%2520quality.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520the%2520Texture%2520Transfer%250AResidual%2520Denoising%2520Dual%2520Diffusion%2520Model%2520%2528TTRD3%2529%2520with%2520three%2520innovations%253A%2520First%252C%250Aa%2520Multi-scale%2520Feature%2520Aggregation%2520Block%2520%2528MFAB%2529%2520employing%2520parallel%2520heterogeneous%250Aconvolutional%2520kernels%2520for%2520multi-scale%2520feature%2520extraction.%2520Second%252C%2520a%2520Sparse%250ATexture%2520Transfer%2520Guidance%2520%2528STTG%2529%2520module%2520that%2520transfers%2520HR%2520texture%2520priors%2520from%250Areference%2520images%2520of%2520similar%2520scenes.%2520Third%252C%2520a%2520Residual%2520Denoising%2520Dual%2520Diffusion%250AModel%2520%2528RDDM%2529%2520framework%2520combining%2520residual%2520diffusion%2520for%2520deterministic%250Areconstruction%2520and%2520noise%2520diffusion%2520for%2520diverse%2520generation.%2520Experiments%2520on%250Amulti-source%2520RS%2520datasets%2520demonstrate%2520TTRD3%2527s%2520superiority%2520over%2520state-of-the-art%250Amethods%252C%2520achieving%25201.43%2525%2520LPIPS%2520improvement%2520and%25203.67%2525%2520FID%2520enhancement%2520compared%250Ato%2520best-performing%2520baselines.%2520Code/model%253A%2520https%253A//github.com/LED-666/TTRD3.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13026v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TTRD3%3A%20Texture%20Transfer%20Residual%20Denoising%20Dual%20Diffusion%20Model%20for%0A%20%20Remote%20Sensing%20Image%20Super-Resolution&entry.906535625=Yide%20Liu%20and%20Haijiang%20Sun%20and%20Xiaowen%20Zhang%20and%20Qiaoyuan%20Liu%20and%20Zhouchang%20Chen%20and%20Chongzhuo%20Xiao&entry.1292438233=%20%20Remote%20Sensing%20Image%20Super-Resolution%20%28RSISR%29%20reconstructs%20high-resolution%0A%28HR%29%20remote%20sensing%20images%20from%20low-resolution%20inputs%20to%20support%20fine-grained%0Aground%20object%20interpretation.%20Existing%20methods%20face%20three%20key%20challenges%3A%20%281%29%0ADifficulty%20in%20extracting%20multi-scale%20features%20from%20spatially%20heterogeneous%20RS%0Ascenes%2C%20%282%29%20Limited%20prior%20information%20causing%20semantic%20inconsistency%20in%0Areconstructions%2C%20and%20%283%29%20Trade-off%20imbalance%20between%20geometric%20accuracy%20and%0Avisual%20quality.%20To%20address%20these%20issues%2C%20we%20propose%20the%20Texture%20Transfer%0AResidual%20Denoising%20Dual%20Diffusion%20Model%20%28TTRD3%29%20with%20three%20innovations%3A%20First%2C%0Aa%20Multi-scale%20Feature%20Aggregation%20Block%20%28MFAB%29%20employing%20parallel%20heterogeneous%0Aconvolutional%20kernels%20for%20multi-scale%20feature%20extraction.%20Second%2C%20a%20Sparse%0ATexture%20Transfer%20Guidance%20%28STTG%29%20module%20that%20transfers%20HR%20texture%20priors%20from%0Areference%20images%20of%20similar%20scenes.%20Third%2C%20a%20Residual%20Denoising%20Dual%20Diffusion%0AModel%20%28RDDM%29%20framework%20combining%20residual%20diffusion%20for%20deterministic%0Areconstruction%20and%20noise%20diffusion%20for%20diverse%20generation.%20Experiments%20on%0Amulti-source%20RS%20datasets%20demonstrate%20TTRD3%27s%20superiority%20over%20state-of-the-art%0Amethods%2C%20achieving%201.43%25%20LPIPS%20improvement%20and%203.67%25%20FID%20enhancement%20compared%0Ato%20best-performing%20baselines.%20Code/model%3A%20https%3A//github.com/LED-666/TTRD3.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13026v1&entry.124074799=Read"},
{"title": "SC3EF: A Joint Self-Correlation and Cross-Correspondence Estimation\n  Framework for Visible and Thermal Image Registration", "author": "Xi Tong and Xing Luo and Jiangxin Yang and Yanpeng Cao", "abstract": "  Multispectral imaging plays a critical role in a range of intelligent\ntransportation applications, including advanced driver assistance systems\n(ADAS), traffic monitoring, and night vision. However, accurate visible and\nthermal (RGB-T) image registration poses a significant challenge due to the\nconsiderable modality differences. In this paper, we present a novel joint\nSelf-Correlation and Cross-Correspondence Estimation Framework (SC3EF),\nleveraging both local representative features and global contextual cues to\neffectively generate RGB-T correspondences. For this purpose, we design a\nconvolution-transformer-based pipeline to extract local representative features\nand encode global correlations of intra-modality for inter-modality\ncorrespondence estimation between unaligned visible and thermal images. After\nmerging the local and global correspondence estimation results, we further\nemploy a hierarchical optical flow estimation decoder to progressively refine\nthe estimated dense correspondence maps. Extensive experiments demonstrate the\neffectiveness of our proposed method, outperforming the current\nstate-of-the-art (SOTA) methods on representative RGB-T datasets. Furthermore,\nit also shows competitive generalization capabilities across challenging\nscenarios, including large parallax, severe occlusions, adverse weather, and\nother cross-modal datasets (e.g., RGB-N and RGB-D).\n", "link": "http://arxiv.org/abs/2504.12869v1", "date": "2025-04-17", "relevancy": 2.3447, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5958}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5942}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5734}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SC3EF%3A%20A%20Joint%20Self-Correlation%20and%20Cross-Correspondence%20Estimation%0A%20%20Framework%20for%20Visible%20and%20Thermal%20Image%20Registration&body=Title%3A%20SC3EF%3A%20A%20Joint%20Self-Correlation%20and%20Cross-Correspondence%20Estimation%0A%20%20Framework%20for%20Visible%20and%20Thermal%20Image%20Registration%0AAuthor%3A%20Xi%20Tong%20and%20Xing%20Luo%20and%20Jiangxin%20Yang%20and%20Yanpeng%20Cao%0AAbstract%3A%20%20%20Multispectral%20imaging%20plays%20a%20critical%20role%20in%20a%20range%20of%20intelligent%0Atransportation%20applications%2C%20including%20advanced%20driver%20assistance%20systems%0A%28ADAS%29%2C%20traffic%20monitoring%2C%20and%20night%20vision.%20However%2C%20accurate%20visible%20and%0Athermal%20%28RGB-T%29%20image%20registration%20poses%20a%20significant%20challenge%20due%20to%20the%0Aconsiderable%20modality%20differences.%20In%20this%20paper%2C%20we%20present%20a%20novel%20joint%0ASelf-Correlation%20and%20Cross-Correspondence%20Estimation%20Framework%20%28SC3EF%29%2C%0Aleveraging%20both%20local%20representative%20features%20and%20global%20contextual%20cues%20to%0Aeffectively%20generate%20RGB-T%20correspondences.%20For%20this%20purpose%2C%20we%20design%20a%0Aconvolution-transformer-based%20pipeline%20to%20extract%20local%20representative%20features%0Aand%20encode%20global%20correlations%20of%20intra-modality%20for%20inter-modality%0Acorrespondence%20estimation%20between%20unaligned%20visible%20and%20thermal%20images.%20After%0Amerging%20the%20local%20and%20global%20correspondence%20estimation%20results%2C%20we%20further%0Aemploy%20a%20hierarchical%20optical%20flow%20estimation%20decoder%20to%20progressively%20refine%0Athe%20estimated%20dense%20correspondence%20maps.%20Extensive%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20our%20proposed%20method%2C%20outperforming%20the%20current%0Astate-of-the-art%20%28SOTA%29%20methods%20on%20representative%20RGB-T%20datasets.%20Furthermore%2C%0Ait%20also%20shows%20competitive%20generalization%20capabilities%20across%20challenging%0Ascenarios%2C%20including%20large%20parallax%2C%20severe%20occlusions%2C%20adverse%20weather%2C%20and%0Aother%20cross-modal%20datasets%20%28e.g.%2C%20RGB-N%20and%20RGB-D%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12869v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSC3EF%253A%2520A%2520Joint%2520Self-Correlation%2520and%2520Cross-Correspondence%2520Estimation%250A%2520%2520Framework%2520for%2520Visible%2520and%2520Thermal%2520Image%2520Registration%26entry.906535625%3DXi%2520Tong%2520and%2520Xing%2520Luo%2520and%2520Jiangxin%2520Yang%2520and%2520Yanpeng%2520Cao%26entry.1292438233%3D%2520%2520Multispectral%2520imaging%2520plays%2520a%2520critical%2520role%2520in%2520a%2520range%2520of%2520intelligent%250Atransportation%2520applications%252C%2520including%2520advanced%2520driver%2520assistance%2520systems%250A%2528ADAS%2529%252C%2520traffic%2520monitoring%252C%2520and%2520night%2520vision.%2520However%252C%2520accurate%2520visible%2520and%250Athermal%2520%2528RGB-T%2529%2520image%2520registration%2520poses%2520a%2520significant%2520challenge%2520due%2520to%2520the%250Aconsiderable%2520modality%2520differences.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520joint%250ASelf-Correlation%2520and%2520Cross-Correspondence%2520Estimation%2520Framework%2520%2528SC3EF%2529%252C%250Aleveraging%2520both%2520local%2520representative%2520features%2520and%2520global%2520contextual%2520cues%2520to%250Aeffectively%2520generate%2520RGB-T%2520correspondences.%2520For%2520this%2520purpose%252C%2520we%2520design%2520a%250Aconvolution-transformer-based%2520pipeline%2520to%2520extract%2520local%2520representative%2520features%250Aand%2520encode%2520global%2520correlations%2520of%2520intra-modality%2520for%2520inter-modality%250Acorrespondence%2520estimation%2520between%2520unaligned%2520visible%2520and%2520thermal%2520images.%2520After%250Amerging%2520the%2520local%2520and%2520global%2520correspondence%2520estimation%2520results%252C%2520we%2520further%250Aemploy%2520a%2520hierarchical%2520optical%2520flow%2520estimation%2520decoder%2520to%2520progressively%2520refine%250Athe%2520estimated%2520dense%2520correspondence%2520maps.%2520Extensive%2520experiments%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520proposed%2520method%252C%2520outperforming%2520the%2520current%250Astate-of-the-art%2520%2528SOTA%2529%2520methods%2520on%2520representative%2520RGB-T%2520datasets.%2520Furthermore%252C%250Ait%2520also%2520shows%2520competitive%2520generalization%2520capabilities%2520across%2520challenging%250Ascenarios%252C%2520including%2520large%2520parallax%252C%2520severe%2520occlusions%252C%2520adverse%2520weather%252C%2520and%250Aother%2520cross-modal%2520datasets%2520%2528e.g.%252C%2520RGB-N%2520and%2520RGB-D%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12869v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SC3EF%3A%20A%20Joint%20Self-Correlation%20and%20Cross-Correspondence%20Estimation%0A%20%20Framework%20for%20Visible%20and%20Thermal%20Image%20Registration&entry.906535625=Xi%20Tong%20and%20Xing%20Luo%20and%20Jiangxin%20Yang%20and%20Yanpeng%20Cao&entry.1292438233=%20%20Multispectral%20imaging%20plays%20a%20critical%20role%20in%20a%20range%20of%20intelligent%0Atransportation%20applications%2C%20including%20advanced%20driver%20assistance%20systems%0A%28ADAS%29%2C%20traffic%20monitoring%2C%20and%20night%20vision.%20However%2C%20accurate%20visible%20and%0Athermal%20%28RGB-T%29%20image%20registration%20poses%20a%20significant%20challenge%20due%20to%20the%0Aconsiderable%20modality%20differences.%20In%20this%20paper%2C%20we%20present%20a%20novel%20joint%0ASelf-Correlation%20and%20Cross-Correspondence%20Estimation%20Framework%20%28SC3EF%29%2C%0Aleveraging%20both%20local%20representative%20features%20and%20global%20contextual%20cues%20to%0Aeffectively%20generate%20RGB-T%20correspondences.%20For%20this%20purpose%2C%20we%20design%20a%0Aconvolution-transformer-based%20pipeline%20to%20extract%20local%20representative%20features%0Aand%20encode%20global%20correlations%20of%20intra-modality%20for%20inter-modality%0Acorrespondence%20estimation%20between%20unaligned%20visible%20and%20thermal%20images.%20After%0Amerging%20the%20local%20and%20global%20correspondence%20estimation%20results%2C%20we%20further%0Aemploy%20a%20hierarchical%20optical%20flow%20estimation%20decoder%20to%20progressively%20refine%0Athe%20estimated%20dense%20correspondence%20maps.%20Extensive%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20our%20proposed%20method%2C%20outperforming%20the%20current%0Astate-of-the-art%20%28SOTA%29%20methods%20on%20representative%20RGB-T%20datasets.%20Furthermore%2C%0Ait%20also%20shows%20competitive%20generalization%20capabilities%20across%20challenging%0Ascenarios%2C%20including%20large%20parallax%2C%20severe%20occlusions%2C%20adverse%20weather%2C%20and%0Aother%20cross-modal%20datasets%20%28e.g.%2C%20RGB-N%20and%20RGB-D%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12869v1&entry.124074799=Read"},
{"title": "Low-hallucination Synthetic Captions for Large-Scale Vision-Language\n  Model Pre-training", "author": "Xinsong Zhang and Yarong Zeng and Xinting Huang and Hu Hu and Runquan Xie and Han Hu and Zhanhui Kang", "abstract": "  In recent years, the field of vision-language model pre-training has\nexperienced rapid advancements, driven primarily by the continuous enhancement\nof textual capabilities in large language models. However, existing training\nparadigms for multimodal large language models heavily rely on high-quality\nimage-text pairs. As models and data scales grow exponentially, the\navailability of such meticulously curated data has become increasingly scarce\nand saturated, thereby severely limiting further advancements in this domain.\nThis study investigates scalable caption generation techniques for\nvision-language model pre-training and demonstrates that large-scale\nlow-hallucination synthetic captions can serve dual purposes: 1) acting as a\nviable alternative to real-world data for pre-training paradigms and 2)\nachieving superior performance enhancement when integrated into vision-language\nmodels through empirical validation. This paper presents three key\ncontributions: 1) a novel pipeline for generating high-quality,\nlow-hallucination, and knowledge-rich synthetic captions. Our continuous DPO\nmethodology yields remarkable results in reducing hallucinations. Specifically,\nthe non-hallucination caption rate on a held-out test set increases from 48.2%\nto 77.9% for a 7B-size model. 2) Comprehensive empirical validation reveals\nthat our synthetic captions confer superior pre-training advantages over their\ncounterparts. Across 35 vision language tasks, the model trained with our data\nachieves a significant performance gain of at least 6.2% compared to alt-text\npairs and other previous work. Meanwhile, it also offers considerable support\nin the text-to-image domain. With our dataset, the FID score is reduced by 17.1\non a real-world validation benchmark and 13.3 on the MSCOCO validation\nbenchmark. 3) We will release Hunyuan-Recap100M, a low-hallucination and\nknowledge-intensive synthetic caption dataset.\n", "link": "http://arxiv.org/abs/2504.13123v1", "date": "2025-04-17", "relevancy": 2.342, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5914}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5914}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-hallucination%20Synthetic%20Captions%20for%20Large-Scale%20Vision-Language%0A%20%20Model%20Pre-training&body=Title%3A%20Low-hallucination%20Synthetic%20Captions%20for%20Large-Scale%20Vision-Language%0A%20%20Model%20Pre-training%0AAuthor%3A%20Xinsong%20Zhang%20and%20Yarong%20Zeng%20and%20Xinting%20Huang%20and%20Hu%20Hu%20and%20Runquan%20Xie%20and%20Han%20Hu%20and%20Zhanhui%20Kang%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20field%20of%20vision-language%20model%20pre-training%20has%0Aexperienced%20rapid%20advancements%2C%20driven%20primarily%20by%20the%20continuous%20enhancement%0Aof%20textual%20capabilities%20in%20large%20language%20models.%20However%2C%20existing%20training%0Aparadigms%20for%20multimodal%20large%20language%20models%20heavily%20rely%20on%20high-quality%0Aimage-text%20pairs.%20As%20models%20and%20data%20scales%20grow%20exponentially%2C%20the%0Aavailability%20of%20such%20meticulously%20curated%20data%20has%20become%20increasingly%20scarce%0Aand%20saturated%2C%20thereby%20severely%20limiting%20further%20advancements%20in%20this%20domain.%0AThis%20study%20investigates%20scalable%20caption%20generation%20techniques%20for%0Avision-language%20model%20pre-training%20and%20demonstrates%20that%20large-scale%0Alow-hallucination%20synthetic%20captions%20can%20serve%20dual%20purposes%3A%201%29%20acting%20as%20a%0Aviable%20alternative%20to%20real-world%20data%20for%20pre-training%20paradigms%20and%202%29%0Aachieving%20superior%20performance%20enhancement%20when%20integrated%20into%20vision-language%0Amodels%20through%20empirical%20validation.%20This%20paper%20presents%20three%20key%0Acontributions%3A%201%29%20a%20novel%20pipeline%20for%20generating%20high-quality%2C%0Alow-hallucination%2C%20and%20knowledge-rich%20synthetic%20captions.%20Our%20continuous%20DPO%0Amethodology%20yields%20remarkable%20results%20in%20reducing%20hallucinations.%20Specifically%2C%0Athe%20non-hallucination%20caption%20rate%20on%20a%20held-out%20test%20set%20increases%20from%2048.2%25%0Ato%2077.9%25%20for%20a%207B-size%20model.%202%29%20Comprehensive%20empirical%20validation%20reveals%0Athat%20our%20synthetic%20captions%20confer%20superior%20pre-training%20advantages%20over%20their%0Acounterparts.%20Across%2035%20vision%20language%20tasks%2C%20the%20model%20trained%20with%20our%20data%0Aachieves%20a%20significant%20performance%20gain%20of%20at%20least%206.2%25%20compared%20to%20alt-text%0Apairs%20and%20other%20previous%20work.%20Meanwhile%2C%20it%20also%20offers%20considerable%20support%0Ain%20the%20text-to-image%20domain.%20With%20our%20dataset%2C%20the%20FID%20score%20is%20reduced%20by%2017.1%0Aon%20a%20real-world%20validation%20benchmark%20and%2013.3%20on%20the%20MSCOCO%20validation%0Abenchmark.%203%29%20We%20will%20release%20Hunyuan-Recap100M%2C%20a%20low-hallucination%20and%0Aknowledge-intensive%20synthetic%20caption%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13123v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-hallucination%2520Synthetic%2520Captions%2520for%2520Large-Scale%2520Vision-Language%250A%2520%2520Model%2520Pre-training%26entry.906535625%3DXinsong%2520Zhang%2520and%2520Yarong%2520Zeng%2520and%2520Xinting%2520Huang%2520and%2520Hu%2520Hu%2520and%2520Runquan%2520Xie%2520and%2520Han%2520Hu%2520and%2520Zhanhui%2520Kang%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520field%2520of%2520vision-language%2520model%2520pre-training%2520has%250Aexperienced%2520rapid%2520advancements%252C%2520driven%2520primarily%2520by%2520the%2520continuous%2520enhancement%250Aof%2520textual%2520capabilities%2520in%2520large%2520language%2520models.%2520However%252C%2520existing%2520training%250Aparadigms%2520for%2520multimodal%2520large%2520language%2520models%2520heavily%2520rely%2520on%2520high-quality%250Aimage-text%2520pairs.%2520As%2520models%2520and%2520data%2520scales%2520grow%2520exponentially%252C%2520the%250Aavailability%2520of%2520such%2520meticulously%2520curated%2520data%2520has%2520become%2520increasingly%2520scarce%250Aand%2520saturated%252C%2520thereby%2520severely%2520limiting%2520further%2520advancements%2520in%2520this%2520domain.%250AThis%2520study%2520investigates%2520scalable%2520caption%2520generation%2520techniques%2520for%250Avision-language%2520model%2520pre-training%2520and%2520demonstrates%2520that%2520large-scale%250Alow-hallucination%2520synthetic%2520captions%2520can%2520serve%2520dual%2520purposes%253A%25201%2529%2520acting%2520as%2520a%250Aviable%2520alternative%2520to%2520real-world%2520data%2520for%2520pre-training%2520paradigms%2520and%25202%2529%250Aachieving%2520superior%2520performance%2520enhancement%2520when%2520integrated%2520into%2520vision-language%250Amodels%2520through%2520empirical%2520validation.%2520This%2520paper%2520presents%2520three%2520key%250Acontributions%253A%25201%2529%2520a%2520novel%2520pipeline%2520for%2520generating%2520high-quality%252C%250Alow-hallucination%252C%2520and%2520knowledge-rich%2520synthetic%2520captions.%2520Our%2520continuous%2520DPO%250Amethodology%2520yields%2520remarkable%2520results%2520in%2520reducing%2520hallucinations.%2520Specifically%252C%250Athe%2520non-hallucination%2520caption%2520rate%2520on%2520a%2520held-out%2520test%2520set%2520increases%2520from%252048.2%2525%250Ato%252077.9%2525%2520for%2520a%25207B-size%2520model.%25202%2529%2520Comprehensive%2520empirical%2520validation%2520reveals%250Athat%2520our%2520synthetic%2520captions%2520confer%2520superior%2520pre-training%2520advantages%2520over%2520their%250Acounterparts.%2520Across%252035%2520vision%2520language%2520tasks%252C%2520the%2520model%2520trained%2520with%2520our%2520data%250Aachieves%2520a%2520significant%2520performance%2520gain%2520of%2520at%2520least%25206.2%2525%2520compared%2520to%2520alt-text%250Apairs%2520and%2520other%2520previous%2520work.%2520Meanwhile%252C%2520it%2520also%2520offers%2520considerable%2520support%250Ain%2520the%2520text-to-image%2520domain.%2520With%2520our%2520dataset%252C%2520the%2520FID%2520score%2520is%2520reduced%2520by%252017.1%250Aon%2520a%2520real-world%2520validation%2520benchmark%2520and%252013.3%2520on%2520the%2520MSCOCO%2520validation%250Abenchmark.%25203%2529%2520We%2520will%2520release%2520Hunyuan-Recap100M%252C%2520a%2520low-hallucination%2520and%250Aknowledge-intensive%2520synthetic%2520caption%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13123v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-hallucination%20Synthetic%20Captions%20for%20Large-Scale%20Vision-Language%0A%20%20Model%20Pre-training&entry.906535625=Xinsong%20Zhang%20and%20Yarong%20Zeng%20and%20Xinting%20Huang%20and%20Hu%20Hu%20and%20Runquan%20Xie%20and%20Han%20Hu%20and%20Zhanhui%20Kang&entry.1292438233=%20%20In%20recent%20years%2C%20the%20field%20of%20vision-language%20model%20pre-training%20has%0Aexperienced%20rapid%20advancements%2C%20driven%20primarily%20by%20the%20continuous%20enhancement%0Aof%20textual%20capabilities%20in%20large%20language%20models.%20However%2C%20existing%20training%0Aparadigms%20for%20multimodal%20large%20language%20models%20heavily%20rely%20on%20high-quality%0Aimage-text%20pairs.%20As%20models%20and%20data%20scales%20grow%20exponentially%2C%20the%0Aavailability%20of%20such%20meticulously%20curated%20data%20has%20become%20increasingly%20scarce%0Aand%20saturated%2C%20thereby%20severely%20limiting%20further%20advancements%20in%20this%20domain.%0AThis%20study%20investigates%20scalable%20caption%20generation%20techniques%20for%0Avision-language%20model%20pre-training%20and%20demonstrates%20that%20large-scale%0Alow-hallucination%20synthetic%20captions%20can%20serve%20dual%20purposes%3A%201%29%20acting%20as%20a%0Aviable%20alternative%20to%20real-world%20data%20for%20pre-training%20paradigms%20and%202%29%0Aachieving%20superior%20performance%20enhancement%20when%20integrated%20into%20vision-language%0Amodels%20through%20empirical%20validation.%20This%20paper%20presents%20three%20key%0Acontributions%3A%201%29%20a%20novel%20pipeline%20for%20generating%20high-quality%2C%0Alow-hallucination%2C%20and%20knowledge-rich%20synthetic%20captions.%20Our%20continuous%20DPO%0Amethodology%20yields%20remarkable%20results%20in%20reducing%20hallucinations.%20Specifically%2C%0Athe%20non-hallucination%20caption%20rate%20on%20a%20held-out%20test%20set%20increases%20from%2048.2%25%0Ato%2077.9%25%20for%20a%207B-size%20model.%202%29%20Comprehensive%20empirical%20validation%20reveals%0Athat%20our%20synthetic%20captions%20confer%20superior%20pre-training%20advantages%20over%20their%0Acounterparts.%20Across%2035%20vision%20language%20tasks%2C%20the%20model%20trained%20with%20our%20data%0Aachieves%20a%20significant%20performance%20gain%20of%20at%20least%206.2%25%20compared%20to%20alt-text%0Apairs%20and%20other%20previous%20work.%20Meanwhile%2C%20it%20also%20offers%20considerable%20support%0Ain%20the%20text-to-image%20domain.%20With%20our%20dataset%2C%20the%20FID%20score%20is%20reduced%20by%2017.1%0Aon%20a%20real-world%20validation%20benchmark%20and%2013.3%20on%20the%20MSCOCO%20validation%0Abenchmark.%203%29%20We%20will%20release%20Hunyuan-Recap100M%2C%20a%20low-hallucination%20and%0Aknowledge-intensive%20synthetic%20caption%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13123v1&entry.124074799=Read"},
{"title": "RUKA: Rethinking the Design of Humanoid Hands with Learning", "author": "Anya Zorin and Irmak Guzey and Billy Yan and Aadhithya Iyer and Lisa Kondrich and Nikhil X. Bhattasali and Lerrel Pinto", "abstract": "  Dexterous manipulation is a fundamental capability for robotic systems, yet\nprogress has been limited by hardware trade-offs between precision,\ncompactness, strength, and affordability. Existing control methods impose\ncompromises on hand designs and applications. However, learning-based\napproaches present opportunities to rethink these trade-offs, particularly to\naddress challenges with tendon-driven actuation and low-cost materials. This\nwork presents RUKA, a tendon-driven humanoid hand that is compact, affordable,\nand capable. Made from 3D-printed parts and off-the-shelf components, RUKA has\n5 fingers with 15 underactuated degrees of freedom enabling diverse human-like\ngrasps. Its tendon-driven actuation allows powerful grasping in a compact,\nhuman-sized form factor. To address control challenges, we learn\njoint-to-actuator and fingertip-to-actuator models from motion-capture data\ncollected by the MANUS glove, leveraging the hand's morphological accuracy.\nExtensive evaluations demonstrate RUKA's superior reachability, durability, and\nstrength compared to other robotic hands. Teleoperation tasks further showcase\nRUKA's dexterous movements. The open-source design and assembly instructions of\nRUKA, code, and data are available at https://ruka-hand.github.io/.\n", "link": "http://arxiv.org/abs/2504.13165v1", "date": "2025-04-17", "relevancy": 2.3151, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6283}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.548}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RUKA%3A%20Rethinking%20the%20Design%20of%20Humanoid%20Hands%20with%20Learning&body=Title%3A%20RUKA%3A%20Rethinking%20the%20Design%20of%20Humanoid%20Hands%20with%20Learning%0AAuthor%3A%20Anya%20Zorin%20and%20Irmak%20Guzey%20and%20Billy%20Yan%20and%20Aadhithya%20Iyer%20and%20Lisa%20Kondrich%20and%20Nikhil%20X.%20Bhattasali%20and%20Lerrel%20Pinto%0AAbstract%3A%20%20%20Dexterous%20manipulation%20is%20a%20fundamental%20capability%20for%20robotic%20systems%2C%20yet%0Aprogress%20has%20been%20limited%20by%20hardware%20trade-offs%20between%20precision%2C%0Acompactness%2C%20strength%2C%20and%20affordability.%20Existing%20control%20methods%20impose%0Acompromises%20on%20hand%20designs%20and%20applications.%20However%2C%20learning-based%0Aapproaches%20present%20opportunities%20to%20rethink%20these%20trade-offs%2C%20particularly%20to%0Aaddress%20challenges%20with%20tendon-driven%20actuation%20and%20low-cost%20materials.%20This%0Awork%20presents%20RUKA%2C%20a%20tendon-driven%20humanoid%20hand%20that%20is%20compact%2C%20affordable%2C%0Aand%20capable.%20Made%20from%203D-printed%20parts%20and%20off-the-shelf%20components%2C%20RUKA%20has%0A5%20fingers%20with%2015%20underactuated%20degrees%20of%20freedom%20enabling%20diverse%20human-like%0Agrasps.%20Its%20tendon-driven%20actuation%20allows%20powerful%20grasping%20in%20a%20compact%2C%0Ahuman-sized%20form%20factor.%20To%20address%20control%20challenges%2C%20we%20learn%0Ajoint-to-actuator%20and%20fingertip-to-actuator%20models%20from%20motion-capture%20data%0Acollected%20by%20the%20MANUS%20glove%2C%20leveraging%20the%20hand%27s%20morphological%20accuracy.%0AExtensive%20evaluations%20demonstrate%20RUKA%27s%20superior%20reachability%2C%20durability%2C%20and%0Astrength%20compared%20to%20other%20robotic%20hands.%20Teleoperation%20tasks%20further%20showcase%0ARUKA%27s%20dexterous%20movements.%20The%20open-source%20design%20and%20assembly%20instructions%20of%0ARUKA%2C%20code%2C%20and%20data%20are%20available%20at%20https%3A//ruka-hand.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13165v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRUKA%253A%2520Rethinking%2520the%2520Design%2520of%2520Humanoid%2520Hands%2520with%2520Learning%26entry.906535625%3DAnya%2520Zorin%2520and%2520Irmak%2520Guzey%2520and%2520Billy%2520Yan%2520and%2520Aadhithya%2520Iyer%2520and%2520Lisa%2520Kondrich%2520and%2520Nikhil%2520X.%2520Bhattasali%2520and%2520Lerrel%2520Pinto%26entry.1292438233%3D%2520%2520Dexterous%2520manipulation%2520is%2520a%2520fundamental%2520capability%2520for%2520robotic%2520systems%252C%2520yet%250Aprogress%2520has%2520been%2520limited%2520by%2520hardware%2520trade-offs%2520between%2520precision%252C%250Acompactness%252C%2520strength%252C%2520and%2520affordability.%2520Existing%2520control%2520methods%2520impose%250Acompromises%2520on%2520hand%2520designs%2520and%2520applications.%2520However%252C%2520learning-based%250Aapproaches%2520present%2520opportunities%2520to%2520rethink%2520these%2520trade-offs%252C%2520particularly%2520to%250Aaddress%2520challenges%2520with%2520tendon-driven%2520actuation%2520and%2520low-cost%2520materials.%2520This%250Awork%2520presents%2520RUKA%252C%2520a%2520tendon-driven%2520humanoid%2520hand%2520that%2520is%2520compact%252C%2520affordable%252C%250Aand%2520capable.%2520Made%2520from%25203D-printed%2520parts%2520and%2520off-the-shelf%2520components%252C%2520RUKA%2520has%250A5%2520fingers%2520with%252015%2520underactuated%2520degrees%2520of%2520freedom%2520enabling%2520diverse%2520human-like%250Agrasps.%2520Its%2520tendon-driven%2520actuation%2520allows%2520powerful%2520grasping%2520in%2520a%2520compact%252C%250Ahuman-sized%2520form%2520factor.%2520To%2520address%2520control%2520challenges%252C%2520we%2520learn%250Ajoint-to-actuator%2520and%2520fingertip-to-actuator%2520models%2520from%2520motion-capture%2520data%250Acollected%2520by%2520the%2520MANUS%2520glove%252C%2520leveraging%2520the%2520hand%2527s%2520morphological%2520accuracy.%250AExtensive%2520evaluations%2520demonstrate%2520RUKA%2527s%2520superior%2520reachability%252C%2520durability%252C%2520and%250Astrength%2520compared%2520to%2520other%2520robotic%2520hands.%2520Teleoperation%2520tasks%2520further%2520showcase%250ARUKA%2527s%2520dexterous%2520movements.%2520The%2520open-source%2520design%2520and%2520assembly%2520instructions%2520of%250ARUKA%252C%2520code%252C%2520and%2520data%2520are%2520available%2520at%2520https%253A//ruka-hand.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13165v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RUKA%3A%20Rethinking%20the%20Design%20of%20Humanoid%20Hands%20with%20Learning&entry.906535625=Anya%20Zorin%20and%20Irmak%20Guzey%20and%20Billy%20Yan%20and%20Aadhithya%20Iyer%20and%20Lisa%20Kondrich%20and%20Nikhil%20X.%20Bhattasali%20and%20Lerrel%20Pinto&entry.1292438233=%20%20Dexterous%20manipulation%20is%20a%20fundamental%20capability%20for%20robotic%20systems%2C%20yet%0Aprogress%20has%20been%20limited%20by%20hardware%20trade-offs%20between%20precision%2C%0Acompactness%2C%20strength%2C%20and%20affordability.%20Existing%20control%20methods%20impose%0Acompromises%20on%20hand%20designs%20and%20applications.%20However%2C%20learning-based%0Aapproaches%20present%20opportunities%20to%20rethink%20these%20trade-offs%2C%20particularly%20to%0Aaddress%20challenges%20with%20tendon-driven%20actuation%20and%20low-cost%20materials.%20This%0Awork%20presents%20RUKA%2C%20a%20tendon-driven%20humanoid%20hand%20that%20is%20compact%2C%20affordable%2C%0Aand%20capable.%20Made%20from%203D-printed%20parts%20and%20off-the-shelf%20components%2C%20RUKA%20has%0A5%20fingers%20with%2015%20underactuated%20degrees%20of%20freedom%20enabling%20diverse%20human-like%0Agrasps.%20Its%20tendon-driven%20actuation%20allows%20powerful%20grasping%20in%20a%20compact%2C%0Ahuman-sized%20form%20factor.%20To%20address%20control%20challenges%2C%20we%20learn%0Ajoint-to-actuator%20and%20fingertip-to-actuator%20models%20from%20motion-capture%20data%0Acollected%20by%20the%20MANUS%20glove%2C%20leveraging%20the%20hand%27s%20morphological%20accuracy.%0AExtensive%20evaluations%20demonstrate%20RUKA%27s%20superior%20reachability%2C%20durability%2C%20and%0Astrength%20compared%20to%20other%20robotic%20hands.%20Teleoperation%20tasks%20further%20showcase%0ARUKA%27s%20dexterous%20movements.%20The%20open-source%20design%20and%20assembly%20instructions%20of%0ARUKA%2C%20code%2C%20and%20data%20are%20available%20at%20https%3A//ruka-hand.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13165v1&entry.124074799=Read"},
{"title": "An Empirically Grounded Identifiability Theory Will Accelerate\n  Self-Supervised Learning Research", "author": "Patrik Reizinger and Randall Balestriero and David Klindt and Wieland Brendel", "abstract": "  Self-Supervised Learning (SSL) powers many current AI systems. As research\ninterest and investment grow, the SSL design space continues to expand. The\nPlatonic view of SSL, following the Platonic Representation Hypothesis (PRH),\nsuggests that despite different methods and engineering approaches, all\nrepresentations converge to the same Platonic ideal. However, this phenomenon\nlacks precise theoretical explanation. By synthesizing evidence from\nIdentifiability Theory (IT), we show that the PRH can emerge in SSL. However,\ncurrent IT cannot explain SSL's empirical success. To bridge the gap between\ntheory and practice, we propose expanding IT into what we term Singular\nIdentifiability Theory (SITh), a broader theoretical framework encompassing the\nentire SSL pipeline. SITh would allow deeper insights into the implicit data\nassumptions in SSL and advance the field towards learning more interpretable\nand generalizable representations. We highlight three critical directions for\nfuture research: 1) training dynamics and convergence properties of SSL; 2) the\nimpact of finite samples, batch size, and data diversity; and 3) the role of\ninductive biases in architecture, augmentations, initialization schemes, and\noptimizers.\n", "link": "http://arxiv.org/abs/2504.13101v1", "date": "2025-04-17", "relevancy": 2.3068, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4755}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4585}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Empirically%20Grounded%20Identifiability%20Theory%20Will%20Accelerate%0A%20%20Self-Supervised%20Learning%20Research&body=Title%3A%20An%20Empirically%20Grounded%20Identifiability%20Theory%20Will%20Accelerate%0A%20%20Self-Supervised%20Learning%20Research%0AAuthor%3A%20Patrik%20Reizinger%20and%20Randall%20Balestriero%20and%20David%20Klindt%20and%20Wieland%20Brendel%0AAbstract%3A%20%20%20Self-Supervised%20Learning%20%28SSL%29%20powers%20many%20current%20AI%20systems.%20As%20research%0Ainterest%20and%20investment%20grow%2C%20the%20SSL%20design%20space%20continues%20to%20expand.%20The%0APlatonic%20view%20of%20SSL%2C%20following%20the%20Platonic%20Representation%20Hypothesis%20%28PRH%29%2C%0Asuggests%20that%20despite%20different%20methods%20and%20engineering%20approaches%2C%20all%0Arepresentations%20converge%20to%20the%20same%20Platonic%20ideal.%20However%2C%20this%20phenomenon%0Alacks%20precise%20theoretical%20explanation.%20By%20synthesizing%20evidence%20from%0AIdentifiability%20Theory%20%28IT%29%2C%20we%20show%20that%20the%20PRH%20can%20emerge%20in%20SSL.%20However%2C%0Acurrent%20IT%20cannot%20explain%20SSL%27s%20empirical%20success.%20To%20bridge%20the%20gap%20between%0Atheory%20and%20practice%2C%20we%20propose%20expanding%20IT%20into%20what%20we%20term%20Singular%0AIdentifiability%20Theory%20%28SITh%29%2C%20a%20broader%20theoretical%20framework%20encompassing%20the%0Aentire%20SSL%20pipeline.%20SITh%20would%20allow%20deeper%20insights%20into%20the%20implicit%20data%0Aassumptions%20in%20SSL%20and%20advance%20the%20field%20towards%20learning%20more%20interpretable%0Aand%20generalizable%20representations.%20We%20highlight%20three%20critical%20directions%20for%0Afuture%20research%3A%201%29%20training%20dynamics%20and%20convergence%20properties%20of%20SSL%3B%202%29%20the%0Aimpact%20of%20finite%20samples%2C%20batch%20size%2C%20and%20data%20diversity%3B%20and%203%29%20the%20role%20of%0Ainductive%20biases%20in%20architecture%2C%20augmentations%2C%20initialization%20schemes%2C%20and%0Aoptimizers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13101v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Empirically%2520Grounded%2520Identifiability%2520Theory%2520Will%2520Accelerate%250A%2520%2520Self-Supervised%2520Learning%2520Research%26entry.906535625%3DPatrik%2520Reizinger%2520and%2520Randall%2520Balestriero%2520and%2520David%2520Klindt%2520and%2520Wieland%2520Brendel%26entry.1292438233%3D%2520%2520Self-Supervised%2520Learning%2520%2528SSL%2529%2520powers%2520many%2520current%2520AI%2520systems.%2520As%2520research%250Ainterest%2520and%2520investment%2520grow%252C%2520the%2520SSL%2520design%2520space%2520continues%2520to%2520expand.%2520The%250APlatonic%2520view%2520of%2520SSL%252C%2520following%2520the%2520Platonic%2520Representation%2520Hypothesis%2520%2528PRH%2529%252C%250Asuggests%2520that%2520despite%2520different%2520methods%2520and%2520engineering%2520approaches%252C%2520all%250Arepresentations%2520converge%2520to%2520the%2520same%2520Platonic%2520ideal.%2520However%252C%2520this%2520phenomenon%250Alacks%2520precise%2520theoretical%2520explanation.%2520By%2520synthesizing%2520evidence%2520from%250AIdentifiability%2520Theory%2520%2528IT%2529%252C%2520we%2520show%2520that%2520the%2520PRH%2520can%2520emerge%2520in%2520SSL.%2520However%252C%250Acurrent%2520IT%2520cannot%2520explain%2520SSL%2527s%2520empirical%2520success.%2520To%2520bridge%2520the%2520gap%2520between%250Atheory%2520and%2520practice%252C%2520we%2520propose%2520expanding%2520IT%2520into%2520what%2520we%2520term%2520Singular%250AIdentifiability%2520Theory%2520%2528SITh%2529%252C%2520a%2520broader%2520theoretical%2520framework%2520encompassing%2520the%250Aentire%2520SSL%2520pipeline.%2520SITh%2520would%2520allow%2520deeper%2520insights%2520into%2520the%2520implicit%2520data%250Aassumptions%2520in%2520SSL%2520and%2520advance%2520the%2520field%2520towards%2520learning%2520more%2520interpretable%250Aand%2520generalizable%2520representations.%2520We%2520highlight%2520three%2520critical%2520directions%2520for%250Afuture%2520research%253A%25201%2529%2520training%2520dynamics%2520and%2520convergence%2520properties%2520of%2520SSL%253B%25202%2529%2520the%250Aimpact%2520of%2520finite%2520samples%252C%2520batch%2520size%252C%2520and%2520data%2520diversity%253B%2520and%25203%2529%2520the%2520role%2520of%250Ainductive%2520biases%2520in%2520architecture%252C%2520augmentations%252C%2520initialization%2520schemes%252C%2520and%250Aoptimizers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13101v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Empirically%20Grounded%20Identifiability%20Theory%20Will%20Accelerate%0A%20%20Self-Supervised%20Learning%20Research&entry.906535625=Patrik%20Reizinger%20and%20Randall%20Balestriero%20and%20David%20Klindt%20and%20Wieland%20Brendel&entry.1292438233=%20%20Self-Supervised%20Learning%20%28SSL%29%20powers%20many%20current%20AI%20systems.%20As%20research%0Ainterest%20and%20investment%20grow%2C%20the%20SSL%20design%20space%20continues%20to%20expand.%20The%0APlatonic%20view%20of%20SSL%2C%20following%20the%20Platonic%20Representation%20Hypothesis%20%28PRH%29%2C%0Asuggests%20that%20despite%20different%20methods%20and%20engineering%20approaches%2C%20all%0Arepresentations%20converge%20to%20the%20same%20Platonic%20ideal.%20However%2C%20this%20phenomenon%0Alacks%20precise%20theoretical%20explanation.%20By%20synthesizing%20evidence%20from%0AIdentifiability%20Theory%20%28IT%29%2C%20we%20show%20that%20the%20PRH%20can%20emerge%20in%20SSL.%20However%2C%0Acurrent%20IT%20cannot%20explain%20SSL%27s%20empirical%20success.%20To%20bridge%20the%20gap%20between%0Atheory%20and%20practice%2C%20we%20propose%20expanding%20IT%20into%20what%20we%20term%20Singular%0AIdentifiability%20Theory%20%28SITh%29%2C%20a%20broader%20theoretical%20framework%20encompassing%20the%0Aentire%20SSL%20pipeline.%20SITh%20would%20allow%20deeper%20insights%20into%20the%20implicit%20data%0Aassumptions%20in%20SSL%20and%20advance%20the%20field%20towards%20learning%20more%20interpretable%0Aand%20generalizable%20representations.%20We%20highlight%20three%20critical%20directions%20for%0Afuture%20research%3A%201%29%20training%20dynamics%20and%20convergence%20properties%20of%20SSL%3B%202%29%20the%0Aimpact%20of%20finite%20samples%2C%20batch%20size%2C%20and%20data%20diversity%3B%20and%203%29%20the%20role%20of%0Ainductive%20biases%20in%20architecture%2C%20augmentations%2C%20initialization%20schemes%2C%20and%0Aoptimizers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13101v1&entry.124074799=Read"},
{"title": "Unified Domain Adaptive Semantic Segmentation", "author": "Zhe Zhang and Gaochang Wu and Jing Zhang and Xiatian Zhu and Dacheng Tao and Tianyou Chai", "abstract": "  Unsupervised Domain Adaptive Semantic Segmentation (UDA-SS) aims to transfer\nthe supervision from a labeled source domain to an unlabeled target domain. The\nmajority of existing UDA-SS works typically consider images whilst recent\nattempts have extended further to tackle videos by modeling the temporal\ndimension. Although the two lines of research share the major challenges --\novercoming the underlying domain distribution shift, their studies are largely\nindependent, resulting in fragmented insights, a lack of holistic\nunderstanding, and missed opportunities for cross-pollination of ideas. This\nfragmentation prevents the unification of methods, leading to redundant efforts\nand suboptimal knowledge transfer across image and video domains. Under this\nobservation, we advocate unifying the study of UDA-SS across video and image\nscenarios, enabling a more comprehensive understanding, synergistic\nadvancements, and efficient knowledge sharing. To that end, we explore the\nunified UDA-SS from a general data augmentation perspective, serving as a\nunifying conceptual framework, enabling improved generalization, and potential\nfor cross-pollination of ideas, ultimately contributing to the overall progress\nand practical impact of this field of research. Specifically, we propose a\nQuad-directional Mixup (QuadMix) method, characterized by tackling distinct\npoint attributes and feature inconsistencies through four-directional paths for\nintra- and inter-domain mixing in a feature space. To deal with temporal shifts\nwith videos, we incorporate optical flow-guided feature aggregation across\nspatial and temporal dimensions for fine-grained domain alignment. Extensive\nexperiments show that our method outperforms the state-of-the-art works by\nlarge margins on four challenging UDA-SS benchmarks. Our source code and models\nwill be released at https://github.com/ZHE-SAPI/UDASS.\n", "link": "http://arxiv.org/abs/2311.13254v4", "date": "2025-04-17", "relevancy": 2.3067, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5855}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5786}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5713}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Domain%20Adaptive%20Semantic%20Segmentation&body=Title%3A%20Unified%20Domain%20Adaptive%20Semantic%20Segmentation%0AAuthor%3A%20Zhe%20Zhang%20and%20Gaochang%20Wu%20and%20Jing%20Zhang%20and%20Xiatian%20Zhu%20and%20Dacheng%20Tao%20and%20Tianyou%20Chai%0AAbstract%3A%20%20%20Unsupervised%20Domain%20Adaptive%20Semantic%20Segmentation%20%28UDA-SS%29%20aims%20to%20transfer%0Athe%20supervision%20from%20a%20labeled%20source%20domain%20to%20an%20unlabeled%20target%20domain.%20The%0Amajority%20of%20existing%20UDA-SS%20works%20typically%20consider%20images%20whilst%20recent%0Aattempts%20have%20extended%20further%20to%20tackle%20videos%20by%20modeling%20the%20temporal%0Adimension.%20Although%20the%20two%20lines%20of%20research%20share%20the%20major%20challenges%20--%0Aovercoming%20the%20underlying%20domain%20distribution%20shift%2C%20their%20studies%20are%20largely%0Aindependent%2C%20resulting%20in%20fragmented%20insights%2C%20a%20lack%20of%20holistic%0Aunderstanding%2C%20and%20missed%20opportunities%20for%20cross-pollination%20of%20ideas.%20This%0Afragmentation%20prevents%20the%20unification%20of%20methods%2C%20leading%20to%20redundant%20efforts%0Aand%20suboptimal%20knowledge%20transfer%20across%20image%20and%20video%20domains.%20Under%20this%0Aobservation%2C%20we%20advocate%20unifying%20the%20study%20of%20UDA-SS%20across%20video%20and%20image%0Ascenarios%2C%20enabling%20a%20more%20comprehensive%20understanding%2C%20synergistic%0Aadvancements%2C%20and%20efficient%20knowledge%20sharing.%20To%20that%20end%2C%20we%20explore%20the%0Aunified%20UDA-SS%20from%20a%20general%20data%20augmentation%20perspective%2C%20serving%20as%20a%0Aunifying%20conceptual%20framework%2C%20enabling%20improved%20generalization%2C%20and%20potential%0Afor%20cross-pollination%20of%20ideas%2C%20ultimately%20contributing%20to%20the%20overall%20progress%0Aand%20practical%20impact%20of%20this%20field%20of%20research.%20Specifically%2C%20we%20propose%20a%0AQuad-directional%20Mixup%20%28QuadMix%29%20method%2C%20characterized%20by%20tackling%20distinct%0Apoint%20attributes%20and%20feature%20inconsistencies%20through%20four-directional%20paths%20for%0Aintra-%20and%20inter-domain%20mixing%20in%20a%20feature%20space.%20To%20deal%20with%20temporal%20shifts%0Awith%20videos%2C%20we%20incorporate%20optical%20flow-guided%20feature%20aggregation%20across%0Aspatial%20and%20temporal%20dimensions%20for%20fine-grained%20domain%20alignment.%20Extensive%0Aexperiments%20show%20that%20our%20method%20outperforms%20the%20state-of-the-art%20works%20by%0Alarge%20margins%20on%20four%20challenging%20UDA-SS%20benchmarks.%20Our%20source%20code%20and%20models%0Awill%20be%20released%20at%20https%3A//github.com/ZHE-SAPI/UDASS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.13254v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Domain%2520Adaptive%2520Semantic%2520Segmentation%26entry.906535625%3DZhe%2520Zhang%2520and%2520Gaochang%2520Wu%2520and%2520Jing%2520Zhang%2520and%2520Xiatian%2520Zhu%2520and%2520Dacheng%2520Tao%2520and%2520Tianyou%2520Chai%26entry.1292438233%3D%2520%2520Unsupervised%2520Domain%2520Adaptive%2520Semantic%2520Segmentation%2520%2528UDA-SS%2529%2520aims%2520to%2520transfer%250Athe%2520supervision%2520from%2520a%2520labeled%2520source%2520domain%2520to%2520an%2520unlabeled%2520target%2520domain.%2520The%250Amajority%2520of%2520existing%2520UDA-SS%2520works%2520typically%2520consider%2520images%2520whilst%2520recent%250Aattempts%2520have%2520extended%2520further%2520to%2520tackle%2520videos%2520by%2520modeling%2520the%2520temporal%250Adimension.%2520Although%2520the%2520two%2520lines%2520of%2520research%2520share%2520the%2520major%2520challenges%2520--%250Aovercoming%2520the%2520underlying%2520domain%2520distribution%2520shift%252C%2520their%2520studies%2520are%2520largely%250Aindependent%252C%2520resulting%2520in%2520fragmented%2520insights%252C%2520a%2520lack%2520of%2520holistic%250Aunderstanding%252C%2520and%2520missed%2520opportunities%2520for%2520cross-pollination%2520of%2520ideas.%2520This%250Afragmentation%2520prevents%2520the%2520unification%2520of%2520methods%252C%2520leading%2520to%2520redundant%2520efforts%250Aand%2520suboptimal%2520knowledge%2520transfer%2520across%2520image%2520and%2520video%2520domains.%2520Under%2520this%250Aobservation%252C%2520we%2520advocate%2520unifying%2520the%2520study%2520of%2520UDA-SS%2520across%2520video%2520and%2520image%250Ascenarios%252C%2520enabling%2520a%2520more%2520comprehensive%2520understanding%252C%2520synergistic%250Aadvancements%252C%2520and%2520efficient%2520knowledge%2520sharing.%2520To%2520that%2520end%252C%2520we%2520explore%2520the%250Aunified%2520UDA-SS%2520from%2520a%2520general%2520data%2520augmentation%2520perspective%252C%2520serving%2520as%2520a%250Aunifying%2520conceptual%2520framework%252C%2520enabling%2520improved%2520generalization%252C%2520and%2520potential%250Afor%2520cross-pollination%2520of%2520ideas%252C%2520ultimately%2520contributing%2520to%2520the%2520overall%2520progress%250Aand%2520practical%2520impact%2520of%2520this%2520field%2520of%2520research.%2520Specifically%252C%2520we%2520propose%2520a%250AQuad-directional%2520Mixup%2520%2528QuadMix%2529%2520method%252C%2520characterized%2520by%2520tackling%2520distinct%250Apoint%2520attributes%2520and%2520feature%2520inconsistencies%2520through%2520four-directional%2520paths%2520for%250Aintra-%2520and%2520inter-domain%2520mixing%2520in%2520a%2520feature%2520space.%2520To%2520deal%2520with%2520temporal%2520shifts%250Awith%2520videos%252C%2520we%2520incorporate%2520optical%2520flow-guided%2520feature%2520aggregation%2520across%250Aspatial%2520and%2520temporal%2520dimensions%2520for%2520fine-grained%2520domain%2520alignment.%2520Extensive%250Aexperiments%2520show%2520that%2520our%2520method%2520outperforms%2520the%2520state-of-the-art%2520works%2520by%250Alarge%2520margins%2520on%2520four%2520challenging%2520UDA-SS%2520benchmarks.%2520Our%2520source%2520code%2520and%2520models%250Awill%2520be%2520released%2520at%2520https%253A//github.com/ZHE-SAPI/UDASS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.13254v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Domain%20Adaptive%20Semantic%20Segmentation&entry.906535625=Zhe%20Zhang%20and%20Gaochang%20Wu%20and%20Jing%20Zhang%20and%20Xiatian%20Zhu%20and%20Dacheng%20Tao%20and%20Tianyou%20Chai&entry.1292438233=%20%20Unsupervised%20Domain%20Adaptive%20Semantic%20Segmentation%20%28UDA-SS%29%20aims%20to%20transfer%0Athe%20supervision%20from%20a%20labeled%20source%20domain%20to%20an%20unlabeled%20target%20domain.%20The%0Amajority%20of%20existing%20UDA-SS%20works%20typically%20consider%20images%20whilst%20recent%0Aattempts%20have%20extended%20further%20to%20tackle%20videos%20by%20modeling%20the%20temporal%0Adimension.%20Although%20the%20two%20lines%20of%20research%20share%20the%20major%20challenges%20--%0Aovercoming%20the%20underlying%20domain%20distribution%20shift%2C%20their%20studies%20are%20largely%0Aindependent%2C%20resulting%20in%20fragmented%20insights%2C%20a%20lack%20of%20holistic%0Aunderstanding%2C%20and%20missed%20opportunities%20for%20cross-pollination%20of%20ideas.%20This%0Afragmentation%20prevents%20the%20unification%20of%20methods%2C%20leading%20to%20redundant%20efforts%0Aand%20suboptimal%20knowledge%20transfer%20across%20image%20and%20video%20domains.%20Under%20this%0Aobservation%2C%20we%20advocate%20unifying%20the%20study%20of%20UDA-SS%20across%20video%20and%20image%0Ascenarios%2C%20enabling%20a%20more%20comprehensive%20understanding%2C%20synergistic%0Aadvancements%2C%20and%20efficient%20knowledge%20sharing.%20To%20that%20end%2C%20we%20explore%20the%0Aunified%20UDA-SS%20from%20a%20general%20data%20augmentation%20perspective%2C%20serving%20as%20a%0Aunifying%20conceptual%20framework%2C%20enabling%20improved%20generalization%2C%20and%20potential%0Afor%20cross-pollination%20of%20ideas%2C%20ultimately%20contributing%20to%20the%20overall%20progress%0Aand%20practical%20impact%20of%20this%20field%20of%20research.%20Specifically%2C%20we%20propose%20a%0AQuad-directional%20Mixup%20%28QuadMix%29%20method%2C%20characterized%20by%20tackling%20distinct%0Apoint%20attributes%20and%20feature%20inconsistencies%20through%20four-directional%20paths%20for%0Aintra-%20and%20inter-domain%20mixing%20in%20a%20feature%20space.%20To%20deal%20with%20temporal%20shifts%0Awith%20videos%2C%20we%20incorporate%20optical%20flow-guided%20feature%20aggregation%20across%0Aspatial%20and%20temporal%20dimensions%20for%20fine-grained%20domain%20alignment.%20Extensive%0Aexperiments%20show%20that%20our%20method%20outperforms%20the%20state-of-the-art%20works%20by%0Alarge%20margins%20on%20four%20challenging%20UDA-SS%20benchmarks.%20Our%20source%20code%20and%20models%0Awill%20be%20released%20at%20https%3A//github.com/ZHE-SAPI/UDASS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.13254v4&entry.124074799=Read"},
{"title": "Rethinking Temporal Fusion with a Unified Gradient Descent View for 3D\n  Semantic Occupancy Prediction", "author": "Dubing Chen and Huan Zheng and Jin Fang and Xingping Dong and Xianfei Li and Wenlong Liao and Tao He and Pai Peng and Jianbing Shen", "abstract": "  We present GDFusion, a temporal fusion method for vision-based 3D semantic\noccupancy prediction (VisionOcc). GDFusion opens up the underexplored aspects\nof temporal fusion within the VisionOcc framework, focusing on both temporal\ncues and fusion strategies. It systematically examines the entire VisionOcc\npipeline, identifying three fundamental yet previously overlooked temporal\ncues: scene-level consistency, motion calibration, and geometric\ncomplementation. These cues capture diverse facets of temporal evolution and\nmake distinct contributions across various modules in the VisionOcc framework.\nTo effectively fuse temporal signals across heterogeneous representations, we\npropose a novel fusion strategy by reinterpreting the formulation of vanilla\nRNNs. This reinterpretation leverages gradient descent on features to unify the\nintegration of diverse temporal information, seamlessly embedding the proposed\ntemporal cues into the network. Extensive experiments on nuScenes demonstrate\nthat GDFusion significantly outperforms established baselines. Notably, on\nOcc3D benchmark, it achieves 1.4\\%-4.8\\% mIoU improvements and reduces memory\nconsumption by 27\\%-72\\%.\n", "link": "http://arxiv.org/abs/2504.12959v1", "date": "2025-04-17", "relevancy": 2.3025, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.579}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5774}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Temporal%20Fusion%20with%20a%20Unified%20Gradient%20Descent%20View%20for%203D%0A%20%20Semantic%20Occupancy%20Prediction&body=Title%3A%20Rethinking%20Temporal%20Fusion%20with%20a%20Unified%20Gradient%20Descent%20View%20for%203D%0A%20%20Semantic%20Occupancy%20Prediction%0AAuthor%3A%20Dubing%20Chen%20and%20Huan%20Zheng%20and%20Jin%20Fang%20and%20Xingping%20Dong%20and%20Xianfei%20Li%20and%20Wenlong%20Liao%20and%20Tao%20He%20and%20Pai%20Peng%20and%20Jianbing%20Shen%0AAbstract%3A%20%20%20We%20present%20GDFusion%2C%20a%20temporal%20fusion%20method%20for%20vision-based%203D%20semantic%0Aoccupancy%20prediction%20%28VisionOcc%29.%20GDFusion%20opens%20up%20the%20underexplored%20aspects%0Aof%20temporal%20fusion%20within%20the%20VisionOcc%20framework%2C%20focusing%20on%20both%20temporal%0Acues%20and%20fusion%20strategies.%20It%20systematically%20examines%20the%20entire%20VisionOcc%0Apipeline%2C%20identifying%20three%20fundamental%20yet%20previously%20overlooked%20temporal%0Acues%3A%20scene-level%20consistency%2C%20motion%20calibration%2C%20and%20geometric%0Acomplementation.%20These%20cues%20capture%20diverse%20facets%20of%20temporal%20evolution%20and%0Amake%20distinct%20contributions%20across%20various%20modules%20in%20the%20VisionOcc%20framework.%0ATo%20effectively%20fuse%20temporal%20signals%20across%20heterogeneous%20representations%2C%20we%0Apropose%20a%20novel%20fusion%20strategy%20by%20reinterpreting%20the%20formulation%20of%20vanilla%0ARNNs.%20This%20reinterpretation%20leverages%20gradient%20descent%20on%20features%20to%20unify%20the%0Aintegration%20of%20diverse%20temporal%20information%2C%20seamlessly%20embedding%20the%20proposed%0Atemporal%20cues%20into%20the%20network.%20Extensive%20experiments%20on%20nuScenes%20demonstrate%0Athat%20GDFusion%20significantly%20outperforms%20established%20baselines.%20Notably%2C%20on%0AOcc3D%20benchmark%2C%20it%20achieves%201.4%5C%25-4.8%5C%25%20mIoU%20improvements%20and%20reduces%20memory%0Aconsumption%20by%2027%5C%25-72%5C%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12959v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Temporal%2520Fusion%2520with%2520a%2520Unified%2520Gradient%2520Descent%2520View%2520for%25203D%250A%2520%2520Semantic%2520Occupancy%2520Prediction%26entry.906535625%3DDubing%2520Chen%2520and%2520Huan%2520Zheng%2520and%2520Jin%2520Fang%2520and%2520Xingping%2520Dong%2520and%2520Xianfei%2520Li%2520and%2520Wenlong%2520Liao%2520and%2520Tao%2520He%2520and%2520Pai%2520Peng%2520and%2520Jianbing%2520Shen%26entry.1292438233%3D%2520%2520We%2520present%2520GDFusion%252C%2520a%2520temporal%2520fusion%2520method%2520for%2520vision-based%25203D%2520semantic%250Aoccupancy%2520prediction%2520%2528VisionOcc%2529.%2520GDFusion%2520opens%2520up%2520the%2520underexplored%2520aspects%250Aof%2520temporal%2520fusion%2520within%2520the%2520VisionOcc%2520framework%252C%2520focusing%2520on%2520both%2520temporal%250Acues%2520and%2520fusion%2520strategies.%2520It%2520systematically%2520examines%2520the%2520entire%2520VisionOcc%250Apipeline%252C%2520identifying%2520three%2520fundamental%2520yet%2520previously%2520overlooked%2520temporal%250Acues%253A%2520scene-level%2520consistency%252C%2520motion%2520calibration%252C%2520and%2520geometric%250Acomplementation.%2520These%2520cues%2520capture%2520diverse%2520facets%2520of%2520temporal%2520evolution%2520and%250Amake%2520distinct%2520contributions%2520across%2520various%2520modules%2520in%2520the%2520VisionOcc%2520framework.%250ATo%2520effectively%2520fuse%2520temporal%2520signals%2520across%2520heterogeneous%2520representations%252C%2520we%250Apropose%2520a%2520novel%2520fusion%2520strategy%2520by%2520reinterpreting%2520the%2520formulation%2520of%2520vanilla%250ARNNs.%2520This%2520reinterpretation%2520leverages%2520gradient%2520descent%2520on%2520features%2520to%2520unify%2520the%250Aintegration%2520of%2520diverse%2520temporal%2520information%252C%2520seamlessly%2520embedding%2520the%2520proposed%250Atemporal%2520cues%2520into%2520the%2520network.%2520Extensive%2520experiments%2520on%2520nuScenes%2520demonstrate%250Athat%2520GDFusion%2520significantly%2520outperforms%2520established%2520baselines.%2520Notably%252C%2520on%250AOcc3D%2520benchmark%252C%2520it%2520achieves%25201.4%255C%2525-4.8%255C%2525%2520mIoU%2520improvements%2520and%2520reduces%2520memory%250Aconsumption%2520by%252027%255C%2525-72%255C%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12959v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Temporal%20Fusion%20with%20a%20Unified%20Gradient%20Descent%20View%20for%203D%0A%20%20Semantic%20Occupancy%20Prediction&entry.906535625=Dubing%20Chen%20and%20Huan%20Zheng%20and%20Jin%20Fang%20and%20Xingping%20Dong%20and%20Xianfei%20Li%20and%20Wenlong%20Liao%20and%20Tao%20He%20and%20Pai%20Peng%20and%20Jianbing%20Shen&entry.1292438233=%20%20We%20present%20GDFusion%2C%20a%20temporal%20fusion%20method%20for%20vision-based%203D%20semantic%0Aoccupancy%20prediction%20%28VisionOcc%29.%20GDFusion%20opens%20up%20the%20underexplored%20aspects%0Aof%20temporal%20fusion%20within%20the%20VisionOcc%20framework%2C%20focusing%20on%20both%20temporal%0Acues%20and%20fusion%20strategies.%20It%20systematically%20examines%20the%20entire%20VisionOcc%0Apipeline%2C%20identifying%20three%20fundamental%20yet%20previously%20overlooked%20temporal%0Acues%3A%20scene-level%20consistency%2C%20motion%20calibration%2C%20and%20geometric%0Acomplementation.%20These%20cues%20capture%20diverse%20facets%20of%20temporal%20evolution%20and%0Amake%20distinct%20contributions%20across%20various%20modules%20in%20the%20VisionOcc%20framework.%0ATo%20effectively%20fuse%20temporal%20signals%20across%20heterogeneous%20representations%2C%20we%0Apropose%20a%20novel%20fusion%20strategy%20by%20reinterpreting%20the%20formulation%20of%20vanilla%0ARNNs.%20This%20reinterpretation%20leverages%20gradient%20descent%20on%20features%20to%20unify%20the%0Aintegration%20of%20diverse%20temporal%20information%2C%20seamlessly%20embedding%20the%20proposed%0Atemporal%20cues%20into%20the%20network.%20Extensive%20experiments%20on%20nuScenes%20demonstrate%0Athat%20GDFusion%20significantly%20outperforms%20established%20baselines.%20Notably%2C%20on%0AOcc3D%20benchmark%2C%20it%20achieves%201.4%5C%25-4.8%5C%25%20mIoU%20improvements%20and%20reduces%20memory%0Aconsumption%20by%2027%5C%25-72%5C%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12959v1&entry.124074799=Read"},
{"title": "Prototypes are Balanced Units for Efficient and Effective Partially\n  Relevant Video Retrieval", "author": "WonJun Moon and Cheol-Ho Cho and Woojin Jun and Minho Shim and Taeoh Kim and Inwoong Lee and Dongyoon Wee and Jae-Pil Heo", "abstract": "  In a retrieval system, simultaneously achieving search accuracy and\nefficiency is inherently challenging. This challenge is particularly pronounced\nin partially relevant video retrieval (PRVR), where incorporating more diverse\ncontext representations at varying temporal scales for each video enhances\naccuracy but increases computational and memory costs. To address this\ndichotomy, we propose a prototypical PRVR framework that encodes diverse\ncontexts within a video into a fixed number of prototypes. We then introduce\nseveral strategies to enhance text association and video understanding within\nthe prototypes, along with an orthogonal objective to ensure that the\nprototypes capture a diverse range of content. To keep the prototypes\nsearchable via text queries while accurately encoding video contexts, we\nimplement cross- and uni-modal reconstruction tasks. The cross-modal\nreconstruction task aligns the prototypes with textual features within a shared\nspace, while the uni-modal reconstruction task preserves all video contexts\nduring encoding. Additionally, we employ a video mixing technique to provide\nweak guidance to further align prototypes and associated textual\nrepresentations. Extensive evaluations on TVR, ActivityNet-Captions, and\nQVHighlights validate the effectiveness of our approach without sacrificing\nefficiency.\n", "link": "http://arxiv.org/abs/2504.13035v1", "date": "2025-04-17", "relevancy": 2.2644, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5695}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5695}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prototypes%20are%20Balanced%20Units%20for%20Efficient%20and%20Effective%20Partially%0A%20%20Relevant%20Video%20Retrieval&body=Title%3A%20Prototypes%20are%20Balanced%20Units%20for%20Efficient%20and%20Effective%20Partially%0A%20%20Relevant%20Video%20Retrieval%0AAuthor%3A%20WonJun%20Moon%20and%20Cheol-Ho%20Cho%20and%20Woojin%20Jun%20and%20Minho%20Shim%20and%20Taeoh%20Kim%20and%20Inwoong%20Lee%20and%20Dongyoon%20Wee%20and%20Jae-Pil%20Heo%0AAbstract%3A%20%20%20In%20a%20retrieval%20system%2C%20simultaneously%20achieving%20search%20accuracy%20and%0Aefficiency%20is%20inherently%20challenging.%20This%20challenge%20is%20particularly%20pronounced%0Ain%20partially%20relevant%20video%20retrieval%20%28PRVR%29%2C%20where%20incorporating%20more%20diverse%0Acontext%20representations%20at%20varying%20temporal%20scales%20for%20each%20video%20enhances%0Aaccuracy%20but%20increases%20computational%20and%20memory%20costs.%20To%20address%20this%0Adichotomy%2C%20we%20propose%20a%20prototypical%20PRVR%20framework%20that%20encodes%20diverse%0Acontexts%20within%20a%20video%20into%20a%20fixed%20number%20of%20prototypes.%20We%20then%20introduce%0Aseveral%20strategies%20to%20enhance%20text%20association%20and%20video%20understanding%20within%0Athe%20prototypes%2C%20along%20with%20an%20orthogonal%20objective%20to%20ensure%20that%20the%0Aprototypes%20capture%20a%20diverse%20range%20of%20content.%20To%20keep%20the%20prototypes%0Asearchable%20via%20text%20queries%20while%20accurately%20encoding%20video%20contexts%2C%20we%0Aimplement%20cross-%20and%20uni-modal%20reconstruction%20tasks.%20The%20cross-modal%0Areconstruction%20task%20aligns%20the%20prototypes%20with%20textual%20features%20within%20a%20shared%0Aspace%2C%20while%20the%20uni-modal%20reconstruction%20task%20preserves%20all%20video%20contexts%0Aduring%20encoding.%20Additionally%2C%20we%20employ%20a%20video%20mixing%20technique%20to%20provide%0Aweak%20guidance%20to%20further%20align%20prototypes%20and%20associated%20textual%0Arepresentations.%20Extensive%20evaluations%20on%20TVR%2C%20ActivityNet-Captions%2C%20and%0AQVHighlights%20validate%20the%20effectiveness%20of%20our%20approach%20without%20sacrificing%0Aefficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13035v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrototypes%2520are%2520Balanced%2520Units%2520for%2520Efficient%2520and%2520Effective%2520Partially%250A%2520%2520Relevant%2520Video%2520Retrieval%26entry.906535625%3DWonJun%2520Moon%2520and%2520Cheol-Ho%2520Cho%2520and%2520Woojin%2520Jun%2520and%2520Minho%2520Shim%2520and%2520Taeoh%2520Kim%2520and%2520Inwoong%2520Lee%2520and%2520Dongyoon%2520Wee%2520and%2520Jae-Pil%2520Heo%26entry.1292438233%3D%2520%2520In%2520a%2520retrieval%2520system%252C%2520simultaneously%2520achieving%2520search%2520accuracy%2520and%250Aefficiency%2520is%2520inherently%2520challenging.%2520This%2520challenge%2520is%2520particularly%2520pronounced%250Ain%2520partially%2520relevant%2520video%2520retrieval%2520%2528PRVR%2529%252C%2520where%2520incorporating%2520more%2520diverse%250Acontext%2520representations%2520at%2520varying%2520temporal%2520scales%2520for%2520each%2520video%2520enhances%250Aaccuracy%2520but%2520increases%2520computational%2520and%2520memory%2520costs.%2520To%2520address%2520this%250Adichotomy%252C%2520we%2520propose%2520a%2520prototypical%2520PRVR%2520framework%2520that%2520encodes%2520diverse%250Acontexts%2520within%2520a%2520video%2520into%2520a%2520fixed%2520number%2520of%2520prototypes.%2520We%2520then%2520introduce%250Aseveral%2520strategies%2520to%2520enhance%2520text%2520association%2520and%2520video%2520understanding%2520within%250Athe%2520prototypes%252C%2520along%2520with%2520an%2520orthogonal%2520objective%2520to%2520ensure%2520that%2520the%250Aprototypes%2520capture%2520a%2520diverse%2520range%2520of%2520content.%2520To%2520keep%2520the%2520prototypes%250Asearchable%2520via%2520text%2520queries%2520while%2520accurately%2520encoding%2520video%2520contexts%252C%2520we%250Aimplement%2520cross-%2520and%2520uni-modal%2520reconstruction%2520tasks.%2520The%2520cross-modal%250Areconstruction%2520task%2520aligns%2520the%2520prototypes%2520with%2520textual%2520features%2520within%2520a%2520shared%250Aspace%252C%2520while%2520the%2520uni-modal%2520reconstruction%2520task%2520preserves%2520all%2520video%2520contexts%250Aduring%2520encoding.%2520Additionally%252C%2520we%2520employ%2520a%2520video%2520mixing%2520technique%2520to%2520provide%250Aweak%2520guidance%2520to%2520further%2520align%2520prototypes%2520and%2520associated%2520textual%250Arepresentations.%2520Extensive%2520evaluations%2520on%2520TVR%252C%2520ActivityNet-Captions%252C%2520and%250AQVHighlights%2520validate%2520the%2520effectiveness%2520of%2520our%2520approach%2520without%2520sacrificing%250Aefficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13035v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prototypes%20are%20Balanced%20Units%20for%20Efficient%20and%20Effective%20Partially%0A%20%20Relevant%20Video%20Retrieval&entry.906535625=WonJun%20Moon%20and%20Cheol-Ho%20Cho%20and%20Woojin%20Jun%20and%20Minho%20Shim%20and%20Taeoh%20Kim%20and%20Inwoong%20Lee%20and%20Dongyoon%20Wee%20and%20Jae-Pil%20Heo&entry.1292438233=%20%20In%20a%20retrieval%20system%2C%20simultaneously%20achieving%20search%20accuracy%20and%0Aefficiency%20is%20inherently%20challenging.%20This%20challenge%20is%20particularly%20pronounced%0Ain%20partially%20relevant%20video%20retrieval%20%28PRVR%29%2C%20where%20incorporating%20more%20diverse%0Acontext%20representations%20at%20varying%20temporal%20scales%20for%20each%20video%20enhances%0Aaccuracy%20but%20increases%20computational%20and%20memory%20costs.%20To%20address%20this%0Adichotomy%2C%20we%20propose%20a%20prototypical%20PRVR%20framework%20that%20encodes%20diverse%0Acontexts%20within%20a%20video%20into%20a%20fixed%20number%20of%20prototypes.%20We%20then%20introduce%0Aseveral%20strategies%20to%20enhance%20text%20association%20and%20video%20understanding%20within%0Athe%20prototypes%2C%20along%20with%20an%20orthogonal%20objective%20to%20ensure%20that%20the%0Aprototypes%20capture%20a%20diverse%20range%20of%20content.%20To%20keep%20the%20prototypes%0Asearchable%20via%20text%20queries%20while%20accurately%20encoding%20video%20contexts%2C%20we%0Aimplement%20cross-%20and%20uni-modal%20reconstruction%20tasks.%20The%20cross-modal%0Areconstruction%20task%20aligns%20the%20prototypes%20with%20textual%20features%20within%20a%20shared%0Aspace%2C%20while%20the%20uni-modal%20reconstruction%20task%20preserves%20all%20video%20contexts%0Aduring%20encoding.%20Additionally%2C%20we%20employ%20a%20video%20mixing%20technique%20to%20provide%0Aweak%20guidance%20to%20further%20align%20prototypes%20and%20associated%20textual%0Arepresentations.%20Extensive%20evaluations%20on%20TVR%2C%20ActivityNet-Captions%2C%20and%0AQVHighlights%20validate%20the%20effectiveness%20of%20our%20approach%20without%20sacrificing%0Aefficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13035v1&entry.124074799=Read"},
{"title": "Transfer Learning via Auxiliary Labels with Application to\n  Cold-Hardiness Prediction", "author": "Kristen Goebel and Paola Pesantez-Cabrera and Markus Keller and Alan Fern", "abstract": "  Cold temperatures can cause significant frost damage to fruit crops depending\non their resilience, or cold hardiness, which changes throughout the dormancy\nseason. This has led to the development of predictive cold-hardiness models,\nwhich help farmers decide when to deploy expensive frost-mitigation measures.\nUnfortunately, cold-hardiness data for model training is only available for\nsome fruit cultivars due to the need for specialized equipment and expertise.\nRather, farmers often do have years of phenological data (e.g. date of\nbudbreak) that they regularly collect for their crops. In this work, we\nintroduce a new transfer-learning framework, Transfer via Auxiliary Labels\n(TAL), that allows farmers to leverage the phenological data to produce more\naccurate cold-hardiness predictions, even when no cold-hardiness data is\navailable for their specific crop. The framework assumes a set of source tasks\n(cultivars) where each has associated primary labels (cold hardiness) and\nauxiliary labels (phenology). However, the target task (new cultivar) is\nassumed to only have the auxiliary labels. The goal of TAL is to predict\nprimary labels for the target task via transfer from the source tasks.\nSurprisingly, despite the vast literature on transfer learning, to our\nknowledge, the TAL formulation has not been previously addressed. Thus, we\npropose several new TAL approaches based on model selection and averaging that\ncan leverage recent deep multi-task models for cold-hardiness prediction. Our\nresults on real-world cold-hardiness and phenological data for multiple grape\ncultivars demonstrate that TAL can leverage the phenological data to improve\ncold-hardiness predictions in the absence of cold-hardiness data.\n", "link": "http://arxiv.org/abs/2504.13142v1", "date": "2025-04-17", "relevancy": 2.2612, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4679}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4451}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.4437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transfer%20Learning%20via%20Auxiliary%20Labels%20with%20Application%20to%0A%20%20Cold-Hardiness%20Prediction&body=Title%3A%20Transfer%20Learning%20via%20Auxiliary%20Labels%20with%20Application%20to%0A%20%20Cold-Hardiness%20Prediction%0AAuthor%3A%20Kristen%20Goebel%20and%20Paola%20Pesantez-Cabrera%20and%20Markus%20Keller%20and%20Alan%20Fern%0AAbstract%3A%20%20%20Cold%20temperatures%20can%20cause%20significant%20frost%20damage%20to%20fruit%20crops%20depending%0Aon%20their%20resilience%2C%20or%20cold%20hardiness%2C%20which%20changes%20throughout%20the%20dormancy%0Aseason.%20This%20has%20led%20to%20the%20development%20of%20predictive%20cold-hardiness%20models%2C%0Awhich%20help%20farmers%20decide%20when%20to%20deploy%20expensive%20frost-mitigation%20measures.%0AUnfortunately%2C%20cold-hardiness%20data%20for%20model%20training%20is%20only%20available%20for%0Asome%20fruit%20cultivars%20due%20to%20the%20need%20for%20specialized%20equipment%20and%20expertise.%0ARather%2C%20farmers%20often%20do%20have%20years%20of%20phenological%20data%20%28e.g.%20date%20of%0Abudbreak%29%20that%20they%20regularly%20collect%20for%20their%20crops.%20In%20this%20work%2C%20we%0Aintroduce%20a%20new%20transfer-learning%20framework%2C%20Transfer%20via%20Auxiliary%20Labels%0A%28TAL%29%2C%20that%20allows%20farmers%20to%20leverage%20the%20phenological%20data%20to%20produce%20more%0Aaccurate%20cold-hardiness%20predictions%2C%20even%20when%20no%20cold-hardiness%20data%20is%0Aavailable%20for%20their%20specific%20crop.%20The%20framework%20assumes%20a%20set%20of%20source%20tasks%0A%28cultivars%29%20where%20each%20has%20associated%20primary%20labels%20%28cold%20hardiness%29%20and%0Aauxiliary%20labels%20%28phenology%29.%20However%2C%20the%20target%20task%20%28new%20cultivar%29%20is%0Aassumed%20to%20only%20have%20the%20auxiliary%20labels.%20The%20goal%20of%20TAL%20is%20to%20predict%0Aprimary%20labels%20for%20the%20target%20task%20via%20transfer%20from%20the%20source%20tasks.%0ASurprisingly%2C%20despite%20the%20vast%20literature%20on%20transfer%20learning%2C%20to%20our%0Aknowledge%2C%20the%20TAL%20formulation%20has%20not%20been%20previously%20addressed.%20Thus%2C%20we%0Apropose%20several%20new%20TAL%20approaches%20based%20on%20model%20selection%20and%20averaging%20that%0Acan%20leverage%20recent%20deep%20multi-task%20models%20for%20cold-hardiness%20prediction.%20Our%0Aresults%20on%20real-world%20cold-hardiness%20and%20phenological%20data%20for%20multiple%20grape%0Acultivars%20demonstrate%20that%20TAL%20can%20leverage%20the%20phenological%20data%20to%20improve%0Acold-hardiness%20predictions%20in%20the%20absence%20of%20cold-hardiness%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13142v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransfer%2520Learning%2520via%2520Auxiliary%2520Labels%2520with%2520Application%2520to%250A%2520%2520Cold-Hardiness%2520Prediction%26entry.906535625%3DKristen%2520Goebel%2520and%2520Paola%2520Pesantez-Cabrera%2520and%2520Markus%2520Keller%2520and%2520Alan%2520Fern%26entry.1292438233%3D%2520%2520Cold%2520temperatures%2520can%2520cause%2520significant%2520frost%2520damage%2520to%2520fruit%2520crops%2520depending%250Aon%2520their%2520resilience%252C%2520or%2520cold%2520hardiness%252C%2520which%2520changes%2520throughout%2520the%2520dormancy%250Aseason.%2520This%2520has%2520led%2520to%2520the%2520development%2520of%2520predictive%2520cold-hardiness%2520models%252C%250Awhich%2520help%2520farmers%2520decide%2520when%2520to%2520deploy%2520expensive%2520frost-mitigation%2520measures.%250AUnfortunately%252C%2520cold-hardiness%2520data%2520for%2520model%2520training%2520is%2520only%2520available%2520for%250Asome%2520fruit%2520cultivars%2520due%2520to%2520the%2520need%2520for%2520specialized%2520equipment%2520and%2520expertise.%250ARather%252C%2520farmers%2520often%2520do%2520have%2520years%2520of%2520phenological%2520data%2520%2528e.g.%2520date%2520of%250Abudbreak%2529%2520that%2520they%2520regularly%2520collect%2520for%2520their%2520crops.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520a%2520new%2520transfer-learning%2520framework%252C%2520Transfer%2520via%2520Auxiliary%2520Labels%250A%2528TAL%2529%252C%2520that%2520allows%2520farmers%2520to%2520leverage%2520the%2520phenological%2520data%2520to%2520produce%2520more%250Aaccurate%2520cold-hardiness%2520predictions%252C%2520even%2520when%2520no%2520cold-hardiness%2520data%2520is%250Aavailable%2520for%2520their%2520specific%2520crop.%2520The%2520framework%2520assumes%2520a%2520set%2520of%2520source%2520tasks%250A%2528cultivars%2529%2520where%2520each%2520has%2520associated%2520primary%2520labels%2520%2528cold%2520hardiness%2529%2520and%250Aauxiliary%2520labels%2520%2528phenology%2529.%2520However%252C%2520the%2520target%2520task%2520%2528new%2520cultivar%2529%2520is%250Aassumed%2520to%2520only%2520have%2520the%2520auxiliary%2520labels.%2520The%2520goal%2520of%2520TAL%2520is%2520to%2520predict%250Aprimary%2520labels%2520for%2520the%2520target%2520task%2520via%2520transfer%2520from%2520the%2520source%2520tasks.%250ASurprisingly%252C%2520despite%2520the%2520vast%2520literature%2520on%2520transfer%2520learning%252C%2520to%2520our%250Aknowledge%252C%2520the%2520TAL%2520formulation%2520has%2520not%2520been%2520previously%2520addressed.%2520Thus%252C%2520we%250Apropose%2520several%2520new%2520TAL%2520approaches%2520based%2520on%2520model%2520selection%2520and%2520averaging%2520that%250Acan%2520leverage%2520recent%2520deep%2520multi-task%2520models%2520for%2520cold-hardiness%2520prediction.%2520Our%250Aresults%2520on%2520real-world%2520cold-hardiness%2520and%2520phenological%2520data%2520for%2520multiple%2520grape%250Acultivars%2520demonstrate%2520that%2520TAL%2520can%2520leverage%2520the%2520phenological%2520data%2520to%2520improve%250Acold-hardiness%2520predictions%2520in%2520the%2520absence%2520of%2520cold-hardiness%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13142v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transfer%20Learning%20via%20Auxiliary%20Labels%20with%20Application%20to%0A%20%20Cold-Hardiness%20Prediction&entry.906535625=Kristen%20Goebel%20and%20Paola%20Pesantez-Cabrera%20and%20Markus%20Keller%20and%20Alan%20Fern&entry.1292438233=%20%20Cold%20temperatures%20can%20cause%20significant%20frost%20damage%20to%20fruit%20crops%20depending%0Aon%20their%20resilience%2C%20or%20cold%20hardiness%2C%20which%20changes%20throughout%20the%20dormancy%0Aseason.%20This%20has%20led%20to%20the%20development%20of%20predictive%20cold-hardiness%20models%2C%0Awhich%20help%20farmers%20decide%20when%20to%20deploy%20expensive%20frost-mitigation%20measures.%0AUnfortunately%2C%20cold-hardiness%20data%20for%20model%20training%20is%20only%20available%20for%0Asome%20fruit%20cultivars%20due%20to%20the%20need%20for%20specialized%20equipment%20and%20expertise.%0ARather%2C%20farmers%20often%20do%20have%20years%20of%20phenological%20data%20%28e.g.%20date%20of%0Abudbreak%29%20that%20they%20regularly%20collect%20for%20their%20crops.%20In%20this%20work%2C%20we%0Aintroduce%20a%20new%20transfer-learning%20framework%2C%20Transfer%20via%20Auxiliary%20Labels%0A%28TAL%29%2C%20that%20allows%20farmers%20to%20leverage%20the%20phenological%20data%20to%20produce%20more%0Aaccurate%20cold-hardiness%20predictions%2C%20even%20when%20no%20cold-hardiness%20data%20is%0Aavailable%20for%20their%20specific%20crop.%20The%20framework%20assumes%20a%20set%20of%20source%20tasks%0A%28cultivars%29%20where%20each%20has%20associated%20primary%20labels%20%28cold%20hardiness%29%20and%0Aauxiliary%20labels%20%28phenology%29.%20However%2C%20the%20target%20task%20%28new%20cultivar%29%20is%0Aassumed%20to%20only%20have%20the%20auxiliary%20labels.%20The%20goal%20of%20TAL%20is%20to%20predict%0Aprimary%20labels%20for%20the%20target%20task%20via%20transfer%20from%20the%20source%20tasks.%0ASurprisingly%2C%20despite%20the%20vast%20literature%20on%20transfer%20learning%2C%20to%20our%0Aknowledge%2C%20the%20TAL%20formulation%20has%20not%20been%20previously%20addressed.%20Thus%2C%20we%0Apropose%20several%20new%20TAL%20approaches%20based%20on%20model%20selection%20and%20averaging%20that%0Acan%20leverage%20recent%20deep%20multi-task%20models%20for%20cold-hardiness%20prediction.%20Our%0Aresults%20on%20real-world%20cold-hardiness%20and%20phenological%20data%20for%20multiple%20grape%0Acultivars%20demonstrate%20that%20TAL%20can%20leverage%20the%20phenological%20data%20to%20improve%0Acold-hardiness%20predictions%20in%20the%20absence%20of%20cold-hardiness%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13142v1&entry.124074799=Read"},
{"title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction", "author": "Yuchao Gu and Weijia Mao and Mike Zheng Shou", "abstract": "  Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context video modeling\nfaces challenges due to visual redundancy. Training on long videos is\ncomputationally expensive, as vision tokens grow much faster than language\ntokens. To tackle this issue, we propose balancing locality and long-range\ndependency through long short-term context modeling. A high-resolution\nshort-term context window ensures fine-grained temporal consistency, while an\nunlimited long-term context window encodes long-range information using fewer\ntokens. With this approach, we can train on long video sequences with a\nmanageable token context length, thereby significantly reducing training time\nand memory usage. Furthermore, we propose a multi-level KV cache designed to\nsupport the long short-term context modeling, which accelerating inference on\nlong video sequences. We demonstrate that FAR achieves state-of-the-art\nperformance in both short- and long-video generation, providing a simple yet\neffective baseline for video autoregressive modeling. The code is released at\nhttps://github.com/showlab/FAR.\n", "link": "http://arxiv.org/abs/2503.19325v2", "date": "2025-04-17", "relevancy": 2.2594, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5934}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5707}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long-Context%20Autoregressive%20Video%20Modeling%20with%20Next-Frame%20Prediction&body=Title%3A%20Long-Context%20Autoregressive%20Video%20Modeling%20with%20Next-Frame%20Prediction%0AAuthor%3A%20Yuchao%20Gu%20and%20Weijia%20Mao%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20%20%20Long-context%20autoregressive%20modeling%20has%20significantly%20advanced%20language%0Ageneration%2C%20but%20video%20generation%20still%20struggles%20to%20fully%20utilize%20extended%0Atemporal%20contexts.%20To%20investigate%20long-context%20video%20modeling%2C%20we%20introduce%0AFrame%20AutoRegressive%20%28FAR%29%2C%20a%20strong%20baseline%20for%20video%20autoregressive%0Amodeling.%20Just%20as%20language%20models%20learn%20causal%20dependencies%20between%20tokens%0A%28i.e.%2C%20Token%20AR%29%2C%20FAR%20models%20temporal%20causal%20dependencies%20between%20continuous%0Aframes%2C%20achieving%20better%20convergence%20than%20Token%20AR%20and%20video%20diffusion%0Atransformers.%20Building%20on%20FAR%2C%20we%20observe%20that%20long-context%20video%20modeling%0Afaces%20challenges%20due%20to%20visual%20redundancy.%20Training%20on%20long%20videos%20is%0Acomputationally%20expensive%2C%20as%20vision%20tokens%20grow%20much%20faster%20than%20language%0Atokens.%20To%20tackle%20this%20issue%2C%20we%20propose%20balancing%20locality%20and%20long-range%0Adependency%20through%20long%20short-term%20context%20modeling.%20A%20high-resolution%0Ashort-term%20context%20window%20ensures%20fine-grained%20temporal%20consistency%2C%20while%20an%0Aunlimited%20long-term%20context%20window%20encodes%20long-range%20information%20using%20fewer%0Atokens.%20With%20this%20approach%2C%20we%20can%20train%20on%20long%20video%20sequences%20with%20a%0Amanageable%20token%20context%20length%2C%20thereby%20significantly%20reducing%20training%20time%0Aand%20memory%20usage.%20Furthermore%2C%20we%20propose%20a%20multi-level%20KV%20cache%20designed%20to%0Asupport%20the%20long%20short-term%20context%20modeling%2C%20which%20accelerating%20inference%20on%0Along%20video%20sequences.%20We%20demonstrate%20that%20FAR%20achieves%20state-of-the-art%0Aperformance%20in%20both%20short-%20and%20long-video%20generation%2C%20providing%20a%20simple%20yet%0Aeffective%20baseline%20for%20video%20autoregressive%20modeling.%20The%20code%20is%20released%20at%0Ahttps%3A//github.com/showlab/FAR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.19325v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong-Context%2520Autoregressive%2520Video%2520Modeling%2520with%2520Next-Frame%2520Prediction%26entry.906535625%3DYuchao%2520Gu%2520and%2520Weijia%2520Mao%2520and%2520Mike%2520Zheng%2520Shou%26entry.1292438233%3D%2520%2520Long-context%2520autoregressive%2520modeling%2520has%2520significantly%2520advanced%2520language%250Ageneration%252C%2520but%2520video%2520generation%2520still%2520struggles%2520to%2520fully%2520utilize%2520extended%250Atemporal%2520contexts.%2520To%2520investigate%2520long-context%2520video%2520modeling%252C%2520we%2520introduce%250AFrame%2520AutoRegressive%2520%2528FAR%2529%252C%2520a%2520strong%2520baseline%2520for%2520video%2520autoregressive%250Amodeling.%2520Just%2520as%2520language%2520models%2520learn%2520causal%2520dependencies%2520between%2520tokens%250A%2528i.e.%252C%2520Token%2520AR%2529%252C%2520FAR%2520models%2520temporal%2520causal%2520dependencies%2520between%2520continuous%250Aframes%252C%2520achieving%2520better%2520convergence%2520than%2520Token%2520AR%2520and%2520video%2520diffusion%250Atransformers.%2520Building%2520on%2520FAR%252C%2520we%2520observe%2520that%2520long-context%2520video%2520modeling%250Afaces%2520challenges%2520due%2520to%2520visual%2520redundancy.%2520Training%2520on%2520long%2520videos%2520is%250Acomputationally%2520expensive%252C%2520as%2520vision%2520tokens%2520grow%2520much%2520faster%2520than%2520language%250Atokens.%2520To%2520tackle%2520this%2520issue%252C%2520we%2520propose%2520balancing%2520locality%2520and%2520long-range%250Adependency%2520through%2520long%2520short-term%2520context%2520modeling.%2520A%2520high-resolution%250Ashort-term%2520context%2520window%2520ensures%2520fine-grained%2520temporal%2520consistency%252C%2520while%2520an%250Aunlimited%2520long-term%2520context%2520window%2520encodes%2520long-range%2520information%2520using%2520fewer%250Atokens.%2520With%2520this%2520approach%252C%2520we%2520can%2520train%2520on%2520long%2520video%2520sequences%2520with%2520a%250Amanageable%2520token%2520context%2520length%252C%2520thereby%2520significantly%2520reducing%2520training%2520time%250Aand%2520memory%2520usage.%2520Furthermore%252C%2520we%2520propose%2520a%2520multi-level%2520KV%2520cache%2520designed%2520to%250Asupport%2520the%2520long%2520short-term%2520context%2520modeling%252C%2520which%2520accelerating%2520inference%2520on%250Along%2520video%2520sequences.%2520We%2520demonstrate%2520that%2520FAR%2520achieves%2520state-of-the-art%250Aperformance%2520in%2520both%2520short-%2520and%2520long-video%2520generation%252C%2520providing%2520a%2520simple%2520yet%250Aeffective%2520baseline%2520for%2520video%2520autoregressive%2520modeling.%2520The%2520code%2520is%2520released%2520at%250Ahttps%253A//github.com/showlab/FAR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.19325v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-Context%20Autoregressive%20Video%20Modeling%20with%20Next-Frame%20Prediction&entry.906535625=Yuchao%20Gu%20and%20Weijia%20Mao%20and%20Mike%20Zheng%20Shou&entry.1292438233=%20%20Long-context%20autoregressive%20modeling%20has%20significantly%20advanced%20language%0Ageneration%2C%20but%20video%20generation%20still%20struggles%20to%20fully%20utilize%20extended%0Atemporal%20contexts.%20To%20investigate%20long-context%20video%20modeling%2C%20we%20introduce%0AFrame%20AutoRegressive%20%28FAR%29%2C%20a%20strong%20baseline%20for%20video%20autoregressive%0Amodeling.%20Just%20as%20language%20models%20learn%20causal%20dependencies%20between%20tokens%0A%28i.e.%2C%20Token%20AR%29%2C%20FAR%20models%20temporal%20causal%20dependencies%20between%20continuous%0Aframes%2C%20achieving%20better%20convergence%20than%20Token%20AR%20and%20video%20diffusion%0Atransformers.%20Building%20on%20FAR%2C%20we%20observe%20that%20long-context%20video%20modeling%0Afaces%20challenges%20due%20to%20visual%20redundancy.%20Training%20on%20long%20videos%20is%0Acomputationally%20expensive%2C%20as%20vision%20tokens%20grow%20much%20faster%20than%20language%0Atokens.%20To%20tackle%20this%20issue%2C%20we%20propose%20balancing%20locality%20and%20long-range%0Adependency%20through%20long%20short-term%20context%20modeling.%20A%20high-resolution%0Ashort-term%20context%20window%20ensures%20fine-grained%20temporal%20consistency%2C%20while%20an%0Aunlimited%20long-term%20context%20window%20encodes%20long-range%20information%20using%20fewer%0Atokens.%20With%20this%20approach%2C%20we%20can%20train%20on%20long%20video%20sequences%20with%20a%0Amanageable%20token%20context%20length%2C%20thereby%20significantly%20reducing%20training%20time%0Aand%20memory%20usage.%20Furthermore%2C%20we%20propose%20a%20multi-level%20KV%20cache%20designed%20to%0Asupport%20the%20long%20short-term%20context%20modeling%2C%20which%20accelerating%20inference%20on%0Along%20video%20sequences.%20We%20demonstrate%20that%20FAR%20achieves%20state-of-the-art%0Aperformance%20in%20both%20short-%20and%20long-video%20generation%2C%20providing%20a%20simple%20yet%0Aeffective%20baseline%20for%20video%20autoregressive%20modeling.%20The%20code%20is%20released%20at%0Ahttps%3A//github.com/showlab/FAR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.19325v2&entry.124074799=Read"},
{"title": "CAP-Net: A Unified Network for 6D Pose and Size Estimation of\n  Categorical Articulated Parts from a Single RGB-D Image", "author": "Jingshun Huang and Haitao Lin and Tianyu Wang and Yanwei Fu and Xiangyang Xue and Yi Zhu", "abstract": "  This paper tackles category-level pose estimation of articulated objects in\nrobotic manipulation tasks and introduces a new benchmark dataset. While recent\nmethods estimate part poses and sizes at the category level, they often rely on\ngeometric cues and complex multi-stage pipelines that first segment parts from\nthe point cloud, followed by Normalized Part Coordinate Space (NPCS) estimation\nfor 6D poses. These approaches overlook dense semantic cues from RGB images,\nleading to suboptimal accuracy, particularly for objects with small parts. To\naddress these limitations, we propose a single-stage Network, CAP-Net, for\nestimating the 6D poses and sizes of Categorical Articulated Parts. This method\ncombines RGB-D features to generate instance segmentation and NPCS\nrepresentations for each part in an end-to-end manner. CAP-Net uses a unified\nnetwork to simultaneously predict point-wise class labels, centroid offsets,\nand NPCS maps. A clustering algorithm then groups points of the same predicted\nclass based on their estimated centroid distances to isolate each part.\nFinally, the NPCS region of each part is aligned with the point cloud to\nrecover its final pose and size. To bridge the sim-to-real domain gap, we\nintroduce the RGBD-Art dataset, the largest RGB-D articulated dataset to date,\nfeaturing photorealistic RGB images and depth noise simulated from real\nsensors. Experimental evaluations on the RGBD-Art dataset demonstrate that our\nmethod significantly outperforms the state-of-the-art approach. Real-world\ndeployments of our model in robotic tasks underscore its robustness and\nexceptional sim-to-real transfer capabilities, confirming its substantial\npractical utility. Our dataset, code and pre-trained models are available on\nthe project page.\n", "link": "http://arxiv.org/abs/2504.11230v2", "date": "2025-04-17", "relevancy": 2.2451, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5721}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5654}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAP-Net%3A%20A%20Unified%20Network%20for%206D%20Pose%20and%20Size%20Estimation%20of%0A%20%20Categorical%20Articulated%20Parts%20from%20a%20Single%20RGB-D%20Image&body=Title%3A%20CAP-Net%3A%20A%20Unified%20Network%20for%206D%20Pose%20and%20Size%20Estimation%20of%0A%20%20Categorical%20Articulated%20Parts%20from%20a%20Single%20RGB-D%20Image%0AAuthor%3A%20Jingshun%20Huang%20and%20Haitao%20Lin%20and%20Tianyu%20Wang%20and%20Yanwei%20Fu%20and%20Xiangyang%20Xue%20and%20Yi%20Zhu%0AAbstract%3A%20%20%20This%20paper%20tackles%20category-level%20pose%20estimation%20of%20articulated%20objects%20in%0Arobotic%20manipulation%20tasks%20and%20introduces%20a%20new%20benchmark%20dataset.%20While%20recent%0Amethods%20estimate%20part%20poses%20and%20sizes%20at%20the%20category%20level%2C%20they%20often%20rely%20on%0Ageometric%20cues%20and%20complex%20multi-stage%20pipelines%20that%20first%20segment%20parts%20from%0Athe%20point%20cloud%2C%20followed%20by%20Normalized%20Part%20Coordinate%20Space%20%28NPCS%29%20estimation%0Afor%206D%20poses.%20These%20approaches%20overlook%20dense%20semantic%20cues%20from%20RGB%20images%2C%0Aleading%20to%20suboptimal%20accuracy%2C%20particularly%20for%20objects%20with%20small%20parts.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20a%20single-stage%20Network%2C%20CAP-Net%2C%20for%0Aestimating%20the%206D%20poses%20and%20sizes%20of%20Categorical%20Articulated%20Parts.%20This%20method%0Acombines%20RGB-D%20features%20to%20generate%20instance%20segmentation%20and%20NPCS%0Arepresentations%20for%20each%20part%20in%20an%20end-to-end%20manner.%20CAP-Net%20uses%20a%20unified%0Anetwork%20to%20simultaneously%20predict%20point-wise%20class%20labels%2C%20centroid%20offsets%2C%0Aand%20NPCS%20maps.%20A%20clustering%20algorithm%20then%20groups%20points%20of%20the%20same%20predicted%0Aclass%20based%20on%20their%20estimated%20centroid%20distances%20to%20isolate%20each%20part.%0AFinally%2C%20the%20NPCS%20region%20of%20each%20part%20is%20aligned%20with%20the%20point%20cloud%20to%0Arecover%20its%20final%20pose%20and%20size.%20To%20bridge%20the%20sim-to-real%20domain%20gap%2C%20we%0Aintroduce%20the%20RGBD-Art%20dataset%2C%20the%20largest%20RGB-D%20articulated%20dataset%20to%20date%2C%0Afeaturing%20photorealistic%20RGB%20images%20and%20depth%20noise%20simulated%20from%20real%0Asensors.%20Experimental%20evaluations%20on%20the%20RGBD-Art%20dataset%20demonstrate%20that%20our%0Amethod%20significantly%20outperforms%20the%20state-of-the-art%20approach.%20Real-world%0Adeployments%20of%20our%20model%20in%20robotic%20tasks%20underscore%20its%20robustness%20and%0Aexceptional%20sim-to-real%20transfer%20capabilities%2C%20confirming%20its%20substantial%0Apractical%20utility.%20Our%20dataset%2C%20code%20and%20pre-trained%20models%20are%20available%20on%0Athe%20project%20page.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.11230v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAP-Net%253A%2520A%2520Unified%2520Network%2520for%25206D%2520Pose%2520and%2520Size%2520Estimation%2520of%250A%2520%2520Categorical%2520Articulated%2520Parts%2520from%2520a%2520Single%2520RGB-D%2520Image%26entry.906535625%3DJingshun%2520Huang%2520and%2520Haitao%2520Lin%2520and%2520Tianyu%2520Wang%2520and%2520Yanwei%2520Fu%2520and%2520Xiangyang%2520Xue%2520and%2520Yi%2520Zhu%26entry.1292438233%3D%2520%2520This%2520paper%2520tackles%2520category-level%2520pose%2520estimation%2520of%2520articulated%2520objects%2520in%250Arobotic%2520manipulation%2520tasks%2520and%2520introduces%2520a%2520new%2520benchmark%2520dataset.%2520While%2520recent%250Amethods%2520estimate%2520part%2520poses%2520and%2520sizes%2520at%2520the%2520category%2520level%252C%2520they%2520often%2520rely%2520on%250Ageometric%2520cues%2520and%2520complex%2520multi-stage%2520pipelines%2520that%2520first%2520segment%2520parts%2520from%250Athe%2520point%2520cloud%252C%2520followed%2520by%2520Normalized%2520Part%2520Coordinate%2520Space%2520%2528NPCS%2529%2520estimation%250Afor%25206D%2520poses.%2520These%2520approaches%2520overlook%2520dense%2520semantic%2520cues%2520from%2520RGB%2520images%252C%250Aleading%2520to%2520suboptimal%2520accuracy%252C%2520particularly%2520for%2520objects%2520with%2520small%2520parts.%2520To%250Aaddress%2520these%2520limitations%252C%2520we%2520propose%2520a%2520single-stage%2520Network%252C%2520CAP-Net%252C%2520for%250Aestimating%2520the%25206D%2520poses%2520and%2520sizes%2520of%2520Categorical%2520Articulated%2520Parts.%2520This%2520method%250Acombines%2520RGB-D%2520features%2520to%2520generate%2520instance%2520segmentation%2520and%2520NPCS%250Arepresentations%2520for%2520each%2520part%2520in%2520an%2520end-to-end%2520manner.%2520CAP-Net%2520uses%2520a%2520unified%250Anetwork%2520to%2520simultaneously%2520predict%2520point-wise%2520class%2520labels%252C%2520centroid%2520offsets%252C%250Aand%2520NPCS%2520maps.%2520A%2520clustering%2520algorithm%2520then%2520groups%2520points%2520of%2520the%2520same%2520predicted%250Aclass%2520based%2520on%2520their%2520estimated%2520centroid%2520distances%2520to%2520isolate%2520each%2520part.%250AFinally%252C%2520the%2520NPCS%2520region%2520of%2520each%2520part%2520is%2520aligned%2520with%2520the%2520point%2520cloud%2520to%250Arecover%2520its%2520final%2520pose%2520and%2520size.%2520To%2520bridge%2520the%2520sim-to-real%2520domain%2520gap%252C%2520we%250Aintroduce%2520the%2520RGBD-Art%2520dataset%252C%2520the%2520largest%2520RGB-D%2520articulated%2520dataset%2520to%2520date%252C%250Afeaturing%2520photorealistic%2520RGB%2520images%2520and%2520depth%2520noise%2520simulated%2520from%2520real%250Asensors.%2520Experimental%2520evaluations%2520on%2520the%2520RGBD-Art%2520dataset%2520demonstrate%2520that%2520our%250Amethod%2520significantly%2520outperforms%2520the%2520state-of-the-art%2520approach.%2520Real-world%250Adeployments%2520of%2520our%2520model%2520in%2520robotic%2520tasks%2520underscore%2520its%2520robustness%2520and%250Aexceptional%2520sim-to-real%2520transfer%2520capabilities%252C%2520confirming%2520its%2520substantial%250Apractical%2520utility.%2520Our%2520dataset%252C%2520code%2520and%2520pre-trained%2520models%2520are%2520available%2520on%250Athe%2520project%2520page.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.11230v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAP-Net%3A%20A%20Unified%20Network%20for%206D%20Pose%20and%20Size%20Estimation%20of%0A%20%20Categorical%20Articulated%20Parts%20from%20a%20Single%20RGB-D%20Image&entry.906535625=Jingshun%20Huang%20and%20Haitao%20Lin%20and%20Tianyu%20Wang%20and%20Yanwei%20Fu%20and%20Xiangyang%20Xue%20and%20Yi%20Zhu&entry.1292438233=%20%20This%20paper%20tackles%20category-level%20pose%20estimation%20of%20articulated%20objects%20in%0Arobotic%20manipulation%20tasks%20and%20introduces%20a%20new%20benchmark%20dataset.%20While%20recent%0Amethods%20estimate%20part%20poses%20and%20sizes%20at%20the%20category%20level%2C%20they%20often%20rely%20on%0Ageometric%20cues%20and%20complex%20multi-stage%20pipelines%20that%20first%20segment%20parts%20from%0Athe%20point%20cloud%2C%20followed%20by%20Normalized%20Part%20Coordinate%20Space%20%28NPCS%29%20estimation%0Afor%206D%20poses.%20These%20approaches%20overlook%20dense%20semantic%20cues%20from%20RGB%20images%2C%0Aleading%20to%20suboptimal%20accuracy%2C%20particularly%20for%20objects%20with%20small%20parts.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20a%20single-stage%20Network%2C%20CAP-Net%2C%20for%0Aestimating%20the%206D%20poses%20and%20sizes%20of%20Categorical%20Articulated%20Parts.%20This%20method%0Acombines%20RGB-D%20features%20to%20generate%20instance%20segmentation%20and%20NPCS%0Arepresentations%20for%20each%20part%20in%20an%20end-to-end%20manner.%20CAP-Net%20uses%20a%20unified%0Anetwork%20to%20simultaneously%20predict%20point-wise%20class%20labels%2C%20centroid%20offsets%2C%0Aand%20NPCS%20maps.%20A%20clustering%20algorithm%20then%20groups%20points%20of%20the%20same%20predicted%0Aclass%20based%20on%20their%20estimated%20centroid%20distances%20to%20isolate%20each%20part.%0AFinally%2C%20the%20NPCS%20region%20of%20each%20part%20is%20aligned%20with%20the%20point%20cloud%20to%0Arecover%20its%20final%20pose%20and%20size.%20To%20bridge%20the%20sim-to-real%20domain%20gap%2C%20we%0Aintroduce%20the%20RGBD-Art%20dataset%2C%20the%20largest%20RGB-D%20articulated%20dataset%20to%20date%2C%0Afeaturing%20photorealistic%20RGB%20images%20and%20depth%20noise%20simulated%20from%20real%0Asensors.%20Experimental%20evaluations%20on%20the%20RGBD-Art%20dataset%20demonstrate%20that%20our%0Amethod%20significantly%20outperforms%20the%20state-of-the-art%20approach.%20Real-world%0Adeployments%20of%20our%20model%20in%20robotic%20tasks%20underscore%20its%20robustness%20and%0Aexceptional%20sim-to-real%20transfer%20capabilities%2C%20confirming%20its%20substantial%0Apractical%20utility.%20Our%20dataset%2C%20code%20and%20pre-trained%20models%20are%20available%20on%0Athe%20project%20page.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.11230v2&entry.124074799=Read"},
{"title": "Efficient Masked Image Compression with Position-Indexed Self-Attention", "author": "Chengjie Dai and Tiantian Song and Hui Tang and Fangdong Chen and Bowei Yang and Guanghua Song", "abstract": "  In recent years, image compression for high-level vision tasks has attracted\nconsiderable attention from researchers. Given that object information in\nimages plays a far more crucial role in downstream tasks than background\ninformation, some studies have proposed semantically structuring the bitstream\nto selectively transmit and reconstruct only the information required by these\ntasks. However, such methods structure the bitstream after encoding, meaning\nthat the coding process still relies on the entire image, even though much of\nthe encoded information will not be transmitted. This leads to redundant\ncomputations. Traditional image compression methods require a two-dimensional\nimage as input, and even if the unimportant regions of the image are set to\nzero by applying a semantic mask, these regions still participate in subsequent\ncomputations as part of the image. To address such limitations, we propose an\nimage compression method based on a position-indexed self-attention mechanism\nthat encodes and decodes only the visible parts of the masked image. Compared\nto existing semantic-structured compression methods, our approach can\nsignificantly reduce computational costs.\n", "link": "http://arxiv.org/abs/2504.12923v1", "date": "2025-04-17", "relevancy": 2.2364, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5723}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5559}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Masked%20Image%20Compression%20with%20Position-Indexed%20Self-Attention&body=Title%3A%20Efficient%20Masked%20Image%20Compression%20with%20Position-Indexed%20Self-Attention%0AAuthor%3A%20Chengjie%20Dai%20and%20Tiantian%20Song%20and%20Hui%20Tang%20and%20Fangdong%20Chen%20and%20Bowei%20Yang%20and%20Guanghua%20Song%0AAbstract%3A%20%20%20In%20recent%20years%2C%20image%20compression%20for%20high-level%20vision%20tasks%20has%20attracted%0Aconsiderable%20attention%20from%20researchers.%20Given%20that%20object%20information%20in%0Aimages%20plays%20a%20far%20more%20crucial%20role%20in%20downstream%20tasks%20than%20background%0Ainformation%2C%20some%20studies%20have%20proposed%20semantically%20structuring%20the%20bitstream%0Ato%20selectively%20transmit%20and%20reconstruct%20only%20the%20information%20required%20by%20these%0Atasks.%20However%2C%20such%20methods%20structure%20the%20bitstream%20after%20encoding%2C%20meaning%0Athat%20the%20coding%20process%20still%20relies%20on%20the%20entire%20image%2C%20even%20though%20much%20of%0Athe%20encoded%20information%20will%20not%20be%20transmitted.%20This%20leads%20to%20redundant%0Acomputations.%20Traditional%20image%20compression%20methods%20require%20a%20two-dimensional%0Aimage%20as%20input%2C%20and%20even%20if%20the%20unimportant%20regions%20of%20the%20image%20are%20set%20to%0Azero%20by%20applying%20a%20semantic%20mask%2C%20these%20regions%20still%20participate%20in%20subsequent%0Acomputations%20as%20part%20of%20the%20image.%20To%20address%20such%20limitations%2C%20we%20propose%20an%0Aimage%20compression%20method%20based%20on%20a%20position-indexed%20self-attention%20mechanism%0Athat%20encodes%20and%20decodes%20only%20the%20visible%20parts%20of%20the%20masked%20image.%20Compared%0Ato%20existing%20semantic-structured%20compression%20methods%2C%20our%20approach%20can%0Asignificantly%20reduce%20computational%20costs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12923v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Masked%2520Image%2520Compression%2520with%2520Position-Indexed%2520Self-Attention%26entry.906535625%3DChengjie%2520Dai%2520and%2520Tiantian%2520Song%2520and%2520Hui%2520Tang%2520and%2520Fangdong%2520Chen%2520and%2520Bowei%2520Yang%2520and%2520Guanghua%2520Song%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520image%2520compression%2520for%2520high-level%2520vision%2520tasks%2520has%2520attracted%250Aconsiderable%2520attention%2520from%2520researchers.%2520Given%2520that%2520object%2520information%2520in%250Aimages%2520plays%2520a%2520far%2520more%2520crucial%2520role%2520in%2520downstream%2520tasks%2520than%2520background%250Ainformation%252C%2520some%2520studies%2520have%2520proposed%2520semantically%2520structuring%2520the%2520bitstream%250Ato%2520selectively%2520transmit%2520and%2520reconstruct%2520only%2520the%2520information%2520required%2520by%2520these%250Atasks.%2520However%252C%2520such%2520methods%2520structure%2520the%2520bitstream%2520after%2520encoding%252C%2520meaning%250Athat%2520the%2520coding%2520process%2520still%2520relies%2520on%2520the%2520entire%2520image%252C%2520even%2520though%2520much%2520of%250Athe%2520encoded%2520information%2520will%2520not%2520be%2520transmitted.%2520This%2520leads%2520to%2520redundant%250Acomputations.%2520Traditional%2520image%2520compression%2520methods%2520require%2520a%2520two-dimensional%250Aimage%2520as%2520input%252C%2520and%2520even%2520if%2520the%2520unimportant%2520regions%2520of%2520the%2520image%2520are%2520set%2520to%250Azero%2520by%2520applying%2520a%2520semantic%2520mask%252C%2520these%2520regions%2520still%2520participate%2520in%2520subsequent%250Acomputations%2520as%2520part%2520of%2520the%2520image.%2520To%2520address%2520such%2520limitations%252C%2520we%2520propose%2520an%250Aimage%2520compression%2520method%2520based%2520on%2520a%2520position-indexed%2520self-attention%2520mechanism%250Athat%2520encodes%2520and%2520decodes%2520only%2520the%2520visible%2520parts%2520of%2520the%2520masked%2520image.%2520Compared%250Ato%2520existing%2520semantic-structured%2520compression%2520methods%252C%2520our%2520approach%2520can%250Asignificantly%2520reduce%2520computational%2520costs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12923v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Masked%20Image%20Compression%20with%20Position-Indexed%20Self-Attention&entry.906535625=Chengjie%20Dai%20and%20Tiantian%20Song%20and%20Hui%20Tang%20and%20Fangdong%20Chen%20and%20Bowei%20Yang%20and%20Guanghua%20Song&entry.1292438233=%20%20In%20recent%20years%2C%20image%20compression%20for%20high-level%20vision%20tasks%20has%20attracted%0Aconsiderable%20attention%20from%20researchers.%20Given%20that%20object%20information%20in%0Aimages%20plays%20a%20far%20more%20crucial%20role%20in%20downstream%20tasks%20than%20background%0Ainformation%2C%20some%20studies%20have%20proposed%20semantically%20structuring%20the%20bitstream%0Ato%20selectively%20transmit%20and%20reconstruct%20only%20the%20information%20required%20by%20these%0Atasks.%20However%2C%20such%20methods%20structure%20the%20bitstream%20after%20encoding%2C%20meaning%0Athat%20the%20coding%20process%20still%20relies%20on%20the%20entire%20image%2C%20even%20though%20much%20of%0Athe%20encoded%20information%20will%20not%20be%20transmitted.%20This%20leads%20to%20redundant%0Acomputations.%20Traditional%20image%20compression%20methods%20require%20a%20two-dimensional%0Aimage%20as%20input%2C%20and%20even%20if%20the%20unimportant%20regions%20of%20the%20image%20are%20set%20to%0Azero%20by%20applying%20a%20semantic%20mask%2C%20these%20regions%20still%20participate%20in%20subsequent%0Acomputations%20as%20part%20of%20the%20image.%20To%20address%20such%20limitations%2C%20we%20propose%20an%0Aimage%20compression%20method%20based%20on%20a%20position-indexed%20self-attention%20mechanism%0Athat%20encodes%20and%20decodes%20only%20the%20visible%20parts%20of%20the%20masked%20image.%20Compared%0Ato%20existing%20semantic-structured%20compression%20methods%2C%20our%20approach%20can%0Asignificantly%20reduce%20computational%20costs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12923v1&entry.124074799=Read"},
{"title": "PCBEAR: Pose Concept Bottleneck for Explainable Action Recognition", "author": "Jongseo Lee and Wooil Lee and Gyeong-Moon Park and Seong Tae Kim and Jinwoo Choi", "abstract": "  Human action recognition (HAR) has achieved impressive results with deep\nlearning models, but their decision-making process remains opaque due to their\nblack-box nature. Ensuring interpretability is crucial, especially for\nreal-world applications requiring transparency and accountability. Existing\nvideo XAI methods primarily rely on feature attribution or static textual\nconcepts, both of which struggle to capture motion dynamics and temporal\ndependencies essential for action understanding. To address these challenges,\nwe propose Pose Concept Bottleneck for Explainable Action Recognition (PCBEAR),\na novel concept bottleneck framework that introduces human pose sequences as\nmotion-aware, structured concepts for video action recognition. Unlike methods\nbased on pixel-level features or static textual descriptions, PCBEAR leverages\nhuman skeleton poses, which focus solely on body movements, providing robust\nand interpretable explanations of motion dynamics. We define two types of\npose-based concepts: static pose concepts for spatial configurations at\nindividual frames, and dynamic pose concepts for motion patterns across\nmultiple frames. To construct these concepts, PCBEAR applies clustering to\nvideo pose sequences, allowing for automatic discovery of meaningful concepts\nwithout manual annotation. We validate PCBEAR on KTH, Penn-Action, and HAA500,\nshowing that it achieves high classification performance while offering\ninterpretable, motion-driven explanations. Our method provides both strong\npredictive performance and human-understandable insights into the model's\nreasoning process, enabling test-time interventions for debugging and improving\nmodel behavior.\n", "link": "http://arxiv.org/abs/2504.13140v1", "date": "2025-04-17", "relevancy": 2.2354, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6067}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5276}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5235}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PCBEAR%3A%20Pose%20Concept%20Bottleneck%20for%20Explainable%20Action%20Recognition&body=Title%3A%20PCBEAR%3A%20Pose%20Concept%20Bottleneck%20for%20Explainable%20Action%20Recognition%0AAuthor%3A%20Jongseo%20Lee%20and%20Wooil%20Lee%20and%20Gyeong-Moon%20Park%20and%20Seong%20Tae%20Kim%20and%20Jinwoo%20Choi%0AAbstract%3A%20%20%20Human%20action%20recognition%20%28HAR%29%20has%20achieved%20impressive%20results%20with%20deep%0Alearning%20models%2C%20but%20their%20decision-making%20process%20remains%20opaque%20due%20to%20their%0Ablack-box%20nature.%20Ensuring%20interpretability%20is%20crucial%2C%20especially%20for%0Areal-world%20applications%20requiring%20transparency%20and%20accountability.%20Existing%0Avideo%20XAI%20methods%20primarily%20rely%20on%20feature%20attribution%20or%20static%20textual%0Aconcepts%2C%20both%20of%20which%20struggle%20to%20capture%20motion%20dynamics%20and%20temporal%0Adependencies%20essential%20for%20action%20understanding.%20To%20address%20these%20challenges%2C%0Awe%20propose%20Pose%20Concept%20Bottleneck%20for%20Explainable%20Action%20Recognition%20%28PCBEAR%29%2C%0Aa%20novel%20concept%20bottleneck%20framework%20that%20introduces%20human%20pose%20sequences%20as%0Amotion-aware%2C%20structured%20concepts%20for%20video%20action%20recognition.%20Unlike%20methods%0Abased%20on%20pixel-level%20features%20or%20static%20textual%20descriptions%2C%20PCBEAR%20leverages%0Ahuman%20skeleton%20poses%2C%20which%20focus%20solely%20on%20body%20movements%2C%20providing%20robust%0Aand%20interpretable%20explanations%20of%20motion%20dynamics.%20We%20define%20two%20types%20of%0Apose-based%20concepts%3A%20static%20pose%20concepts%20for%20spatial%20configurations%20at%0Aindividual%20frames%2C%20and%20dynamic%20pose%20concepts%20for%20motion%20patterns%20across%0Amultiple%20frames.%20To%20construct%20these%20concepts%2C%20PCBEAR%20applies%20clustering%20to%0Avideo%20pose%20sequences%2C%20allowing%20for%20automatic%20discovery%20of%20meaningful%20concepts%0Awithout%20manual%20annotation.%20We%20validate%20PCBEAR%20on%20KTH%2C%20Penn-Action%2C%20and%20HAA500%2C%0Ashowing%20that%20it%20achieves%20high%20classification%20performance%20while%20offering%0Ainterpretable%2C%20motion-driven%20explanations.%20Our%20method%20provides%20both%20strong%0Apredictive%20performance%20and%20human-understandable%20insights%20into%20the%20model%27s%0Areasoning%20process%2C%20enabling%20test-time%20interventions%20for%20debugging%20and%20improving%0Amodel%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13140v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPCBEAR%253A%2520Pose%2520Concept%2520Bottleneck%2520for%2520Explainable%2520Action%2520Recognition%26entry.906535625%3DJongseo%2520Lee%2520and%2520Wooil%2520Lee%2520and%2520Gyeong-Moon%2520Park%2520and%2520Seong%2520Tae%2520Kim%2520and%2520Jinwoo%2520Choi%26entry.1292438233%3D%2520%2520Human%2520action%2520recognition%2520%2528HAR%2529%2520has%2520achieved%2520impressive%2520results%2520with%2520deep%250Alearning%2520models%252C%2520but%2520their%2520decision-making%2520process%2520remains%2520opaque%2520due%2520to%2520their%250Ablack-box%2520nature.%2520Ensuring%2520interpretability%2520is%2520crucial%252C%2520especially%2520for%250Areal-world%2520applications%2520requiring%2520transparency%2520and%2520accountability.%2520Existing%250Avideo%2520XAI%2520methods%2520primarily%2520rely%2520on%2520feature%2520attribution%2520or%2520static%2520textual%250Aconcepts%252C%2520both%2520of%2520which%2520struggle%2520to%2520capture%2520motion%2520dynamics%2520and%2520temporal%250Adependencies%2520essential%2520for%2520action%2520understanding.%2520To%2520address%2520these%2520challenges%252C%250Awe%2520propose%2520Pose%2520Concept%2520Bottleneck%2520for%2520Explainable%2520Action%2520Recognition%2520%2528PCBEAR%2529%252C%250Aa%2520novel%2520concept%2520bottleneck%2520framework%2520that%2520introduces%2520human%2520pose%2520sequences%2520as%250Amotion-aware%252C%2520structured%2520concepts%2520for%2520video%2520action%2520recognition.%2520Unlike%2520methods%250Abased%2520on%2520pixel-level%2520features%2520or%2520static%2520textual%2520descriptions%252C%2520PCBEAR%2520leverages%250Ahuman%2520skeleton%2520poses%252C%2520which%2520focus%2520solely%2520on%2520body%2520movements%252C%2520providing%2520robust%250Aand%2520interpretable%2520explanations%2520of%2520motion%2520dynamics.%2520We%2520define%2520two%2520types%2520of%250Apose-based%2520concepts%253A%2520static%2520pose%2520concepts%2520for%2520spatial%2520configurations%2520at%250Aindividual%2520frames%252C%2520and%2520dynamic%2520pose%2520concepts%2520for%2520motion%2520patterns%2520across%250Amultiple%2520frames.%2520To%2520construct%2520these%2520concepts%252C%2520PCBEAR%2520applies%2520clustering%2520to%250Avideo%2520pose%2520sequences%252C%2520allowing%2520for%2520automatic%2520discovery%2520of%2520meaningful%2520concepts%250Awithout%2520manual%2520annotation.%2520We%2520validate%2520PCBEAR%2520on%2520KTH%252C%2520Penn-Action%252C%2520and%2520HAA500%252C%250Ashowing%2520that%2520it%2520achieves%2520high%2520classification%2520performance%2520while%2520offering%250Ainterpretable%252C%2520motion-driven%2520explanations.%2520Our%2520method%2520provides%2520both%2520strong%250Apredictive%2520performance%2520and%2520human-understandable%2520insights%2520into%2520the%2520model%2527s%250Areasoning%2520process%252C%2520enabling%2520test-time%2520interventions%2520for%2520debugging%2520and%2520improving%250Amodel%2520behavior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13140v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PCBEAR%3A%20Pose%20Concept%20Bottleneck%20for%20Explainable%20Action%20Recognition&entry.906535625=Jongseo%20Lee%20and%20Wooil%20Lee%20and%20Gyeong-Moon%20Park%20and%20Seong%20Tae%20Kim%20and%20Jinwoo%20Choi&entry.1292438233=%20%20Human%20action%20recognition%20%28HAR%29%20has%20achieved%20impressive%20results%20with%20deep%0Alearning%20models%2C%20but%20their%20decision-making%20process%20remains%20opaque%20due%20to%20their%0Ablack-box%20nature.%20Ensuring%20interpretability%20is%20crucial%2C%20especially%20for%0Areal-world%20applications%20requiring%20transparency%20and%20accountability.%20Existing%0Avideo%20XAI%20methods%20primarily%20rely%20on%20feature%20attribution%20or%20static%20textual%0Aconcepts%2C%20both%20of%20which%20struggle%20to%20capture%20motion%20dynamics%20and%20temporal%0Adependencies%20essential%20for%20action%20understanding.%20To%20address%20these%20challenges%2C%0Awe%20propose%20Pose%20Concept%20Bottleneck%20for%20Explainable%20Action%20Recognition%20%28PCBEAR%29%2C%0Aa%20novel%20concept%20bottleneck%20framework%20that%20introduces%20human%20pose%20sequences%20as%0Amotion-aware%2C%20structured%20concepts%20for%20video%20action%20recognition.%20Unlike%20methods%0Abased%20on%20pixel-level%20features%20or%20static%20textual%20descriptions%2C%20PCBEAR%20leverages%0Ahuman%20skeleton%20poses%2C%20which%20focus%20solely%20on%20body%20movements%2C%20providing%20robust%0Aand%20interpretable%20explanations%20of%20motion%20dynamics.%20We%20define%20two%20types%20of%0Apose-based%20concepts%3A%20static%20pose%20concepts%20for%20spatial%20configurations%20at%0Aindividual%20frames%2C%20and%20dynamic%20pose%20concepts%20for%20motion%20patterns%20across%0Amultiple%20frames.%20To%20construct%20these%20concepts%2C%20PCBEAR%20applies%20clustering%20to%0Avideo%20pose%20sequences%2C%20allowing%20for%20automatic%20discovery%20of%20meaningful%20concepts%0Awithout%20manual%20annotation.%20We%20validate%20PCBEAR%20on%20KTH%2C%20Penn-Action%2C%20and%20HAA500%2C%0Ashowing%20that%20it%20achieves%20high%20classification%20performance%20while%20offering%0Ainterpretable%2C%20motion-driven%20explanations.%20Our%20method%20provides%20both%20strong%0Apredictive%20performance%20and%20human-understandable%20insights%20into%20the%20model%27s%0Areasoning%20process%2C%20enabling%20test-time%20interventions%20for%20debugging%20and%20improving%0Amodel%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13140v1&entry.124074799=Read"},
{"title": "CDXLSTM: Boosting Remote Sensing Change Detection with Extended Long\n  Short-Term Memory", "author": "Zhenkai Wu and Xiaowen Ma and Rongrong Lian and Kai Zheng and Wei Zhang", "abstract": "  In complex scenes and varied conditions, effectively integrating\nspatial-temporal context is crucial for accurately identifying changes.\nHowever, current RS-CD methods lack a balanced consideration of performance and\nefficiency. CNNs lack global context, Transformers are computationally\nexpensive, and Mambas face CUDA dependence and local correlation loss. In this\npaper, we propose CDXLSTM, with a core component that is a powerful XLSTM-based\nfeature enhancement layer, integrating the advantages of linear computational\ncomplexity, global context perception, and strong interpret-ability.\nSpecifically, we introduce a scale-specific Feature Enhancer layer,\nincorporating a Cross-Temporal Global Perceptron customized for\nsemantic-accurate deep features, and a Cross-Temporal Spatial Refiner\ncustomized for detail-rich shallow features. Additionally, we propose a\nCross-Scale Interactive Fusion module to progressively interact global change\nrepresentations with spatial responses. Extensive experimental results\ndemonstrate that CDXLSTM achieves state-of-the-art performance across three\nbenchmark datasets, offering a compelling balance between efficiency and\naccuracy. Code is available at https://github.com/xwmaxwma/rschange.\n", "link": "http://arxiv.org/abs/2411.07863v3", "date": "2025-04-17", "relevancy": 2.2036, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5784}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5492}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CDXLSTM%3A%20Boosting%20Remote%20Sensing%20Change%20Detection%20with%20Extended%20Long%0A%20%20Short-Term%20Memory&body=Title%3A%20CDXLSTM%3A%20Boosting%20Remote%20Sensing%20Change%20Detection%20with%20Extended%20Long%0A%20%20Short-Term%20Memory%0AAuthor%3A%20Zhenkai%20Wu%20and%20Xiaowen%20Ma%20and%20Rongrong%20Lian%20and%20Kai%20Zheng%20and%20Wei%20Zhang%0AAbstract%3A%20%20%20In%20complex%20scenes%20and%20varied%20conditions%2C%20effectively%20integrating%0Aspatial-temporal%20context%20is%20crucial%20for%20accurately%20identifying%20changes.%0AHowever%2C%20current%20RS-CD%20methods%20lack%20a%20balanced%20consideration%20of%20performance%20and%0Aefficiency.%20CNNs%20lack%20global%20context%2C%20Transformers%20are%20computationally%0Aexpensive%2C%20and%20Mambas%20face%20CUDA%20dependence%20and%20local%20correlation%20loss.%20In%20this%0Apaper%2C%20we%20propose%20CDXLSTM%2C%20with%20a%20core%20component%20that%20is%20a%20powerful%20XLSTM-based%0Afeature%20enhancement%20layer%2C%20integrating%20the%20advantages%20of%20linear%20computational%0Acomplexity%2C%20global%20context%20perception%2C%20and%20strong%20interpret-ability.%0ASpecifically%2C%20we%20introduce%20a%20scale-specific%20Feature%20Enhancer%20layer%2C%0Aincorporating%20a%20Cross-Temporal%20Global%20Perceptron%20customized%20for%0Asemantic-accurate%20deep%20features%2C%20and%20a%20Cross-Temporal%20Spatial%20Refiner%0Acustomized%20for%20detail-rich%20shallow%20features.%20Additionally%2C%20we%20propose%20a%0ACross-Scale%20Interactive%20Fusion%20module%20to%20progressively%20interact%20global%20change%0Arepresentations%20with%20spatial%20responses.%20Extensive%20experimental%20results%0Ademonstrate%20that%20CDXLSTM%20achieves%20state-of-the-art%20performance%20across%20three%0Abenchmark%20datasets%2C%20offering%20a%20compelling%20balance%20between%20efficiency%20and%0Aaccuracy.%20Code%20is%20available%20at%20https%3A//github.com/xwmaxwma/rschange.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07863v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCDXLSTM%253A%2520Boosting%2520Remote%2520Sensing%2520Change%2520Detection%2520with%2520Extended%2520Long%250A%2520%2520Short-Term%2520Memory%26entry.906535625%3DZhenkai%2520Wu%2520and%2520Xiaowen%2520Ma%2520and%2520Rongrong%2520Lian%2520and%2520Kai%2520Zheng%2520and%2520Wei%2520Zhang%26entry.1292438233%3D%2520%2520In%2520complex%2520scenes%2520and%2520varied%2520conditions%252C%2520effectively%2520integrating%250Aspatial-temporal%2520context%2520is%2520crucial%2520for%2520accurately%2520identifying%2520changes.%250AHowever%252C%2520current%2520RS-CD%2520methods%2520lack%2520a%2520balanced%2520consideration%2520of%2520performance%2520and%250Aefficiency.%2520CNNs%2520lack%2520global%2520context%252C%2520Transformers%2520are%2520computationally%250Aexpensive%252C%2520and%2520Mambas%2520face%2520CUDA%2520dependence%2520and%2520local%2520correlation%2520loss.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520CDXLSTM%252C%2520with%2520a%2520core%2520component%2520that%2520is%2520a%2520powerful%2520XLSTM-based%250Afeature%2520enhancement%2520layer%252C%2520integrating%2520the%2520advantages%2520of%2520linear%2520computational%250Acomplexity%252C%2520global%2520context%2520perception%252C%2520and%2520strong%2520interpret-ability.%250ASpecifically%252C%2520we%2520introduce%2520a%2520scale-specific%2520Feature%2520Enhancer%2520layer%252C%250Aincorporating%2520a%2520Cross-Temporal%2520Global%2520Perceptron%2520customized%2520for%250Asemantic-accurate%2520deep%2520features%252C%2520and%2520a%2520Cross-Temporal%2520Spatial%2520Refiner%250Acustomized%2520for%2520detail-rich%2520shallow%2520features.%2520Additionally%252C%2520we%2520propose%2520a%250ACross-Scale%2520Interactive%2520Fusion%2520module%2520to%2520progressively%2520interact%2520global%2520change%250Arepresentations%2520with%2520spatial%2520responses.%2520Extensive%2520experimental%2520results%250Ademonstrate%2520that%2520CDXLSTM%2520achieves%2520state-of-the-art%2520performance%2520across%2520three%250Abenchmark%2520datasets%252C%2520offering%2520a%2520compelling%2520balance%2520between%2520efficiency%2520and%250Aaccuracy.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/xwmaxwma/rschange.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07863v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CDXLSTM%3A%20Boosting%20Remote%20Sensing%20Change%20Detection%20with%20Extended%20Long%0A%20%20Short-Term%20Memory&entry.906535625=Zhenkai%20Wu%20and%20Xiaowen%20Ma%20and%20Rongrong%20Lian%20and%20Kai%20Zheng%20and%20Wei%20Zhang&entry.1292438233=%20%20In%20complex%20scenes%20and%20varied%20conditions%2C%20effectively%20integrating%0Aspatial-temporal%20context%20is%20crucial%20for%20accurately%20identifying%20changes.%0AHowever%2C%20current%20RS-CD%20methods%20lack%20a%20balanced%20consideration%20of%20performance%20and%0Aefficiency.%20CNNs%20lack%20global%20context%2C%20Transformers%20are%20computationally%0Aexpensive%2C%20and%20Mambas%20face%20CUDA%20dependence%20and%20local%20correlation%20loss.%20In%20this%0Apaper%2C%20we%20propose%20CDXLSTM%2C%20with%20a%20core%20component%20that%20is%20a%20powerful%20XLSTM-based%0Afeature%20enhancement%20layer%2C%20integrating%20the%20advantages%20of%20linear%20computational%0Acomplexity%2C%20global%20context%20perception%2C%20and%20strong%20interpret-ability.%0ASpecifically%2C%20we%20introduce%20a%20scale-specific%20Feature%20Enhancer%20layer%2C%0Aincorporating%20a%20Cross-Temporal%20Global%20Perceptron%20customized%20for%0Asemantic-accurate%20deep%20features%2C%20and%20a%20Cross-Temporal%20Spatial%20Refiner%0Acustomized%20for%20detail-rich%20shallow%20features.%20Additionally%2C%20we%20propose%20a%0ACross-Scale%20Interactive%20Fusion%20module%20to%20progressively%20interact%20global%20change%0Arepresentations%20with%20spatial%20responses.%20Extensive%20experimental%20results%0Ademonstrate%20that%20CDXLSTM%20achieves%20state-of-the-art%20performance%20across%20three%0Abenchmark%20datasets%2C%20offering%20a%20compelling%20balance%20between%20efficiency%20and%0Aaccuracy.%20Code%20is%20available%20at%20https%3A//github.com/xwmaxwma/rschange.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07863v3&entry.124074799=Read"},
{"title": "Sign-In to the Lottery: Reparameterizing Sparse Training From Scratch", "author": "Advait Gadhikar and Tom Jacobs and Chao Zhou and Rebekka Burkholz", "abstract": "  The performance gap between training sparse neural networks from scratch\n(PaI) and dense-to-sparse training presents a major roadblock for efficient\ndeep learning. According to the Lottery Ticket Hypothesis, PaI hinges on\nfinding a problem specific parameter initialization. As we show, to this end,\ndetermining correct parameter signs is sufficient. Yet, they remain elusive to\nPaI. To address this issue, we propose Sign-In, which employs a dynamic\nreparameterization that provably induces sign flips. Such sign flips are\ncomplementary to the ones that dense-to-sparse training can accomplish,\nrendering Sign-In as an orthogonal method. While our experiments and theory\nsuggest performance improvements of PaI, they also carve out the main open\nchallenge to close the gap between PaI and dense-to-sparse training.\n", "link": "http://arxiv.org/abs/2504.12801v1", "date": "2025-04-17", "relevancy": 2.2035, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4552}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4373}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4296}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sign-In%20to%20the%20Lottery%3A%20Reparameterizing%20Sparse%20Training%20From%20Scratch&body=Title%3A%20Sign-In%20to%20the%20Lottery%3A%20Reparameterizing%20Sparse%20Training%20From%20Scratch%0AAuthor%3A%20Advait%20Gadhikar%20and%20Tom%20Jacobs%20and%20Chao%20Zhou%20and%20Rebekka%20Burkholz%0AAbstract%3A%20%20%20The%20performance%20gap%20between%20training%20sparse%20neural%20networks%20from%20scratch%0A%28PaI%29%20and%20dense-to-sparse%20training%20presents%20a%20major%20roadblock%20for%20efficient%0Adeep%20learning.%20According%20to%20the%20Lottery%20Ticket%20Hypothesis%2C%20PaI%20hinges%20on%0Afinding%20a%20problem%20specific%20parameter%20initialization.%20As%20we%20show%2C%20to%20this%20end%2C%0Adetermining%20correct%20parameter%20signs%20is%20sufficient.%20Yet%2C%20they%20remain%20elusive%20to%0APaI.%20To%20address%20this%20issue%2C%20we%20propose%20Sign-In%2C%20which%20employs%20a%20dynamic%0Areparameterization%20that%20provably%20induces%20sign%20flips.%20Such%20sign%20flips%20are%0Acomplementary%20to%20the%20ones%20that%20dense-to-sparse%20training%20can%20accomplish%2C%0Arendering%20Sign-In%20as%20an%20orthogonal%20method.%20While%20our%20experiments%20and%20theory%0Asuggest%20performance%20improvements%20of%20PaI%2C%20they%20also%20carve%20out%20the%20main%20open%0Achallenge%20to%20close%20the%20gap%20between%20PaI%20and%20dense-to-sparse%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12801v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSign-In%2520to%2520the%2520Lottery%253A%2520Reparameterizing%2520Sparse%2520Training%2520From%2520Scratch%26entry.906535625%3DAdvait%2520Gadhikar%2520and%2520Tom%2520Jacobs%2520and%2520Chao%2520Zhou%2520and%2520Rebekka%2520Burkholz%26entry.1292438233%3D%2520%2520The%2520performance%2520gap%2520between%2520training%2520sparse%2520neural%2520networks%2520from%2520scratch%250A%2528PaI%2529%2520and%2520dense-to-sparse%2520training%2520presents%2520a%2520major%2520roadblock%2520for%2520efficient%250Adeep%2520learning.%2520According%2520to%2520the%2520Lottery%2520Ticket%2520Hypothesis%252C%2520PaI%2520hinges%2520on%250Afinding%2520a%2520problem%2520specific%2520parameter%2520initialization.%2520As%2520we%2520show%252C%2520to%2520this%2520end%252C%250Adetermining%2520correct%2520parameter%2520signs%2520is%2520sufficient.%2520Yet%252C%2520they%2520remain%2520elusive%2520to%250APaI.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520Sign-In%252C%2520which%2520employs%2520a%2520dynamic%250Areparameterization%2520that%2520provably%2520induces%2520sign%2520flips.%2520Such%2520sign%2520flips%2520are%250Acomplementary%2520to%2520the%2520ones%2520that%2520dense-to-sparse%2520training%2520can%2520accomplish%252C%250Arendering%2520Sign-In%2520as%2520an%2520orthogonal%2520method.%2520While%2520our%2520experiments%2520and%2520theory%250Asuggest%2520performance%2520improvements%2520of%2520PaI%252C%2520they%2520also%2520carve%2520out%2520the%2520main%2520open%250Achallenge%2520to%2520close%2520the%2520gap%2520between%2520PaI%2520and%2520dense-to-sparse%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12801v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sign-In%20to%20the%20Lottery%3A%20Reparameterizing%20Sparse%20Training%20From%20Scratch&entry.906535625=Advait%20Gadhikar%20and%20Tom%20Jacobs%20and%20Chao%20Zhou%20and%20Rebekka%20Burkholz&entry.1292438233=%20%20The%20performance%20gap%20between%20training%20sparse%20neural%20networks%20from%20scratch%0A%28PaI%29%20and%20dense-to-sparse%20training%20presents%20a%20major%20roadblock%20for%20efficient%0Adeep%20learning.%20According%20to%20the%20Lottery%20Ticket%20Hypothesis%2C%20PaI%20hinges%20on%0Afinding%20a%20problem%20specific%20parameter%20initialization.%20As%20we%20show%2C%20to%20this%20end%2C%0Adetermining%20correct%20parameter%20signs%20is%20sufficient.%20Yet%2C%20they%20remain%20elusive%20to%0APaI.%20To%20address%20this%20issue%2C%20we%20propose%20Sign-In%2C%20which%20employs%20a%20dynamic%0Areparameterization%20that%20provably%20induces%20sign%20flips.%20Such%20sign%20flips%20are%0Acomplementary%20to%20the%20ones%20that%20dense-to-sparse%20training%20can%20accomplish%2C%0Arendering%20Sign-In%20as%20an%20orthogonal%20method.%20While%20our%20experiments%20and%20theory%0Asuggest%20performance%20improvements%20of%20PaI%2C%20they%20also%20carve%20out%20the%20main%20open%0Achallenge%20to%20close%20the%20gap%20between%20PaI%20and%20dense-to-sparse%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12801v1&entry.124074799=Read"},
{"title": "3D-PNAS: 3D Industrial Surface Anomaly Synthesis with Perlin Noise", "author": "Yifeng Cheng and Juan Du", "abstract": "  Large pretrained vision foundation models have shown significant potential in\nvarious vision tasks. However, for industrial anomaly detection, the scarcity\nof real defect samples poses a critical challenge in leveraging these models.\nWhile 2D anomaly generation has significantly advanced with established\ngenerative models, the adoption of 3D sensors in industrial manufacturing has\nmade leveraging 3D data for surface quality inspection an emerging trend. In\ncontrast to 2D techniques, 3D anomaly generation remains largely unexplored,\nlimiting the potential of 3D data in industrial quality inspection. To address\nthis gap, we propose a novel yet simple 3D anomaly generation method, 3D-PNAS,\nbased on Perlin noise and surface parameterization. Our method generates\nrealistic 3D surface anomalies by projecting the point cloud onto a 2D plane,\nsampling multi-scale noise values from a Perlin noise field, and perturbing the\npoint cloud along its normal direction. Through comprehensive visualization\nexperiments, we demonstrate how key parameters - including noise scale,\nperturbation strength, and octaves, provide fine-grained control over the\ngenerated anomalies, enabling the creation of diverse defect patterns from\npronounced deformations to subtle surface variations. Additionally, our\ncross-category experiments show that the method produces consistent yet\ngeometrically plausible anomalies across different object types, adapting to\ntheir specific surface characteristics. We also provide a comprehensive\ncodebase and visualization toolkit to facilitate future research.\n", "link": "http://arxiv.org/abs/2504.12856v1", "date": "2025-04-17", "relevancy": 2.1946, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.55}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.55}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D-PNAS%3A%203D%20Industrial%20Surface%20Anomaly%20Synthesis%20with%20Perlin%20Noise&body=Title%3A%203D-PNAS%3A%203D%20Industrial%20Surface%20Anomaly%20Synthesis%20with%20Perlin%20Noise%0AAuthor%3A%20Yifeng%20Cheng%20and%20Juan%20Du%0AAbstract%3A%20%20%20Large%20pretrained%20vision%20foundation%20models%20have%20shown%20significant%20potential%20in%0Avarious%20vision%20tasks.%20However%2C%20for%20industrial%20anomaly%20detection%2C%20the%20scarcity%0Aof%20real%20defect%20samples%20poses%20a%20critical%20challenge%20in%20leveraging%20these%20models.%0AWhile%202D%20anomaly%20generation%20has%20significantly%20advanced%20with%20established%0Agenerative%20models%2C%20the%20adoption%20of%203D%20sensors%20in%20industrial%20manufacturing%20has%0Amade%20leveraging%203D%20data%20for%20surface%20quality%20inspection%20an%20emerging%20trend.%20In%0Acontrast%20to%202D%20techniques%2C%203D%20anomaly%20generation%20remains%20largely%20unexplored%2C%0Alimiting%20the%20potential%20of%203D%20data%20in%20industrial%20quality%20inspection.%20To%20address%0Athis%20gap%2C%20we%20propose%20a%20novel%20yet%20simple%203D%20anomaly%20generation%20method%2C%203D-PNAS%2C%0Abased%20on%20Perlin%20noise%20and%20surface%20parameterization.%20Our%20method%20generates%0Arealistic%203D%20surface%20anomalies%20by%20projecting%20the%20point%20cloud%20onto%20a%202D%20plane%2C%0Asampling%20multi-scale%20noise%20values%20from%20a%20Perlin%20noise%20field%2C%20and%20perturbing%20the%0Apoint%20cloud%20along%20its%20normal%20direction.%20Through%20comprehensive%20visualization%0Aexperiments%2C%20we%20demonstrate%20how%20key%20parameters%20-%20including%20noise%20scale%2C%0Aperturbation%20strength%2C%20and%20octaves%2C%20provide%20fine-grained%20control%20over%20the%0Agenerated%20anomalies%2C%20enabling%20the%20creation%20of%20diverse%20defect%20patterns%20from%0Apronounced%20deformations%20to%20subtle%20surface%20variations.%20Additionally%2C%20our%0Across-category%20experiments%20show%20that%20the%20method%20produces%20consistent%20yet%0Ageometrically%20plausible%20anomalies%20across%20different%20object%20types%2C%20adapting%20to%0Atheir%20specific%20surface%20characteristics.%20We%20also%20provide%20a%20comprehensive%0Acodebase%20and%20visualization%20toolkit%20to%20facilitate%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12856v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D-PNAS%253A%25203D%2520Industrial%2520Surface%2520Anomaly%2520Synthesis%2520with%2520Perlin%2520Noise%26entry.906535625%3DYifeng%2520Cheng%2520and%2520Juan%2520Du%26entry.1292438233%3D%2520%2520Large%2520pretrained%2520vision%2520foundation%2520models%2520have%2520shown%2520significant%2520potential%2520in%250Avarious%2520vision%2520tasks.%2520However%252C%2520for%2520industrial%2520anomaly%2520detection%252C%2520the%2520scarcity%250Aof%2520real%2520defect%2520samples%2520poses%2520a%2520critical%2520challenge%2520in%2520leveraging%2520these%2520models.%250AWhile%25202D%2520anomaly%2520generation%2520has%2520significantly%2520advanced%2520with%2520established%250Agenerative%2520models%252C%2520the%2520adoption%2520of%25203D%2520sensors%2520in%2520industrial%2520manufacturing%2520has%250Amade%2520leveraging%25203D%2520data%2520for%2520surface%2520quality%2520inspection%2520an%2520emerging%2520trend.%2520In%250Acontrast%2520to%25202D%2520techniques%252C%25203D%2520anomaly%2520generation%2520remains%2520largely%2520unexplored%252C%250Alimiting%2520the%2520potential%2520of%25203D%2520data%2520in%2520industrial%2520quality%2520inspection.%2520To%2520address%250Athis%2520gap%252C%2520we%2520propose%2520a%2520novel%2520yet%2520simple%25203D%2520anomaly%2520generation%2520method%252C%25203D-PNAS%252C%250Abased%2520on%2520Perlin%2520noise%2520and%2520surface%2520parameterization.%2520Our%2520method%2520generates%250Arealistic%25203D%2520surface%2520anomalies%2520by%2520projecting%2520the%2520point%2520cloud%2520onto%2520a%25202D%2520plane%252C%250Asampling%2520multi-scale%2520noise%2520values%2520from%2520a%2520Perlin%2520noise%2520field%252C%2520and%2520perturbing%2520the%250Apoint%2520cloud%2520along%2520its%2520normal%2520direction.%2520Through%2520comprehensive%2520visualization%250Aexperiments%252C%2520we%2520demonstrate%2520how%2520key%2520parameters%2520-%2520including%2520noise%2520scale%252C%250Aperturbation%2520strength%252C%2520and%2520octaves%252C%2520provide%2520fine-grained%2520control%2520over%2520the%250Agenerated%2520anomalies%252C%2520enabling%2520the%2520creation%2520of%2520diverse%2520defect%2520patterns%2520from%250Apronounced%2520deformations%2520to%2520subtle%2520surface%2520variations.%2520Additionally%252C%2520our%250Across-category%2520experiments%2520show%2520that%2520the%2520method%2520produces%2520consistent%2520yet%250Ageometrically%2520plausible%2520anomalies%2520across%2520different%2520object%2520types%252C%2520adapting%2520to%250Atheir%2520specific%2520surface%2520characteristics.%2520We%2520also%2520provide%2520a%2520comprehensive%250Acodebase%2520and%2520visualization%2520toolkit%2520to%2520facilitate%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12856v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-PNAS%3A%203D%20Industrial%20Surface%20Anomaly%20Synthesis%20with%20Perlin%20Noise&entry.906535625=Yifeng%20Cheng%20and%20Juan%20Du&entry.1292438233=%20%20Large%20pretrained%20vision%20foundation%20models%20have%20shown%20significant%20potential%20in%0Avarious%20vision%20tasks.%20However%2C%20for%20industrial%20anomaly%20detection%2C%20the%20scarcity%0Aof%20real%20defect%20samples%20poses%20a%20critical%20challenge%20in%20leveraging%20these%20models.%0AWhile%202D%20anomaly%20generation%20has%20significantly%20advanced%20with%20established%0Agenerative%20models%2C%20the%20adoption%20of%203D%20sensors%20in%20industrial%20manufacturing%20has%0Amade%20leveraging%203D%20data%20for%20surface%20quality%20inspection%20an%20emerging%20trend.%20In%0Acontrast%20to%202D%20techniques%2C%203D%20anomaly%20generation%20remains%20largely%20unexplored%2C%0Alimiting%20the%20potential%20of%203D%20data%20in%20industrial%20quality%20inspection.%20To%20address%0Athis%20gap%2C%20we%20propose%20a%20novel%20yet%20simple%203D%20anomaly%20generation%20method%2C%203D-PNAS%2C%0Abased%20on%20Perlin%20noise%20and%20surface%20parameterization.%20Our%20method%20generates%0Arealistic%203D%20surface%20anomalies%20by%20projecting%20the%20point%20cloud%20onto%20a%202D%20plane%2C%0Asampling%20multi-scale%20noise%20values%20from%20a%20Perlin%20noise%20field%2C%20and%20perturbing%20the%0Apoint%20cloud%20along%20its%20normal%20direction.%20Through%20comprehensive%20visualization%0Aexperiments%2C%20we%20demonstrate%20how%20key%20parameters%20-%20including%20noise%20scale%2C%0Aperturbation%20strength%2C%20and%20octaves%2C%20provide%20fine-grained%20control%20over%20the%0Agenerated%20anomalies%2C%20enabling%20the%20creation%20of%20diverse%20defect%20patterns%20from%0Apronounced%20deformations%20to%20subtle%20surface%20variations.%20Additionally%2C%20our%0Across-category%20experiments%20show%20that%20the%20method%20produces%20consistent%20yet%0Ageometrically%20plausible%20anomalies%20across%20different%20object%20types%2C%20adapting%20to%0Atheir%20specific%20surface%20characteristics.%20We%20also%20provide%20a%20comprehensive%0Acodebase%20and%20visualization%20toolkit%20to%20facilitate%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12856v1&entry.124074799=Read"},
{"title": "Inference-friendly Graph Compression for Graph Neural Networks", "author": "Yangxin Fan and Haolai Che and Yinghui Wu", "abstract": "  Graph Neural Networks (GNNs) have demonstrated promising performance in graph\nanalysis. Nevertheless, the inference process of GNNs remains costly, hindering\ntheir applications for large graphs. This paper proposes inference-friendly\ngraph compression (IFGC), a graph compression scheme to accelerate GNNs\ninference. Given a graph $G$ and a GNN $M$, an IFGC computes a small compressed\ngraph $G_c$, to best preserve the inference results of $M$ over $G$, such that\nthe result can be directly inferred by accessing $G_c$ with no or little\ndecompression cost. (1) We characterize IFGC with a class of inference\nequivalence relation. The relation captures the node pairs in $G$ that are not\ndistinguishable for GNN inference. (2) We introduce three practical\nspecifications of IFGC for representative GNNs: structural preserving\ncompression (SPGC), which computes $G_c$ that can be directly processed by GNN\ninference without decompression; ($\\alpha$, $r$)-compression, that allows for a\nconfigurable trade-off between compression ratio and inference quality, and\nanchored compression that preserves inference results for specific nodes of\ninterest. For each scheme, we introduce compression and inference algorithms\nwith guarantees of efficiency and quality of the inferred results. We conduct\nextensive experiments on diverse sets of large-scale graphs, which verifies the\neffectiveness and efficiency of our graph compression approaches.\n", "link": "http://arxiv.org/abs/2504.13034v1", "date": "2025-04-17", "relevancy": 2.1924, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4651}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4252}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4251}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inference-friendly%20Graph%20Compression%20for%20Graph%20Neural%20Networks&body=Title%3A%20Inference-friendly%20Graph%20Compression%20for%20Graph%20Neural%20Networks%0AAuthor%3A%20Yangxin%20Fan%20and%20Haolai%20Che%20and%20Yinghui%20Wu%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20demonstrated%20promising%20performance%20in%20graph%0Aanalysis.%20Nevertheless%2C%20the%20inference%20process%20of%20GNNs%20remains%20costly%2C%20hindering%0Atheir%20applications%20for%20large%20graphs.%20This%20paper%20proposes%20inference-friendly%0Agraph%20compression%20%28IFGC%29%2C%20a%20graph%20compression%20scheme%20to%20accelerate%20GNNs%0Ainference.%20Given%20a%20graph%20%24G%24%20and%20a%20GNN%20%24M%24%2C%20an%20IFGC%20computes%20a%20small%20compressed%0Agraph%20%24G_c%24%2C%20to%20best%20preserve%20the%20inference%20results%20of%20%24M%24%20over%20%24G%24%2C%20such%20that%0Athe%20result%20can%20be%20directly%20inferred%20by%20accessing%20%24G_c%24%20with%20no%20or%20little%0Adecompression%20cost.%20%281%29%20We%20characterize%20IFGC%20with%20a%20class%20of%20inference%0Aequivalence%20relation.%20The%20relation%20captures%20the%20node%20pairs%20in%20%24G%24%20that%20are%20not%0Adistinguishable%20for%20GNN%20inference.%20%282%29%20We%20introduce%20three%20practical%0Aspecifications%20of%20IFGC%20for%20representative%20GNNs%3A%20structural%20preserving%0Acompression%20%28SPGC%29%2C%20which%20computes%20%24G_c%24%20that%20can%20be%20directly%20processed%20by%20GNN%0Ainference%20without%20decompression%3B%20%28%24%5Calpha%24%2C%20%24r%24%29-compression%2C%20that%20allows%20for%20a%0Aconfigurable%20trade-off%20between%20compression%20ratio%20and%20inference%20quality%2C%20and%0Aanchored%20compression%20that%20preserves%20inference%20results%20for%20specific%20nodes%20of%0Ainterest.%20For%20each%20scheme%2C%20we%20introduce%20compression%20and%20inference%20algorithms%0Awith%20guarantees%20of%20efficiency%20and%20quality%20of%20the%20inferred%20results.%20We%20conduct%0Aextensive%20experiments%20on%20diverse%20sets%20of%20large-scale%20graphs%2C%20which%20verifies%20the%0Aeffectiveness%20and%20efficiency%20of%20our%20graph%20compression%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13034v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInference-friendly%2520Graph%2520Compression%2520for%2520Graph%2520Neural%2520Networks%26entry.906535625%3DYangxin%2520Fan%2520and%2520Haolai%2520Che%2520and%2520Yinghui%2520Wu%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520demonstrated%2520promising%2520performance%2520in%2520graph%250Aanalysis.%2520Nevertheless%252C%2520the%2520inference%2520process%2520of%2520GNNs%2520remains%2520costly%252C%2520hindering%250Atheir%2520applications%2520for%2520large%2520graphs.%2520This%2520paper%2520proposes%2520inference-friendly%250Agraph%2520compression%2520%2528IFGC%2529%252C%2520a%2520graph%2520compression%2520scheme%2520to%2520accelerate%2520GNNs%250Ainference.%2520Given%2520a%2520graph%2520%2524G%2524%2520and%2520a%2520GNN%2520%2524M%2524%252C%2520an%2520IFGC%2520computes%2520a%2520small%2520compressed%250Agraph%2520%2524G_c%2524%252C%2520to%2520best%2520preserve%2520the%2520inference%2520results%2520of%2520%2524M%2524%2520over%2520%2524G%2524%252C%2520such%2520that%250Athe%2520result%2520can%2520be%2520directly%2520inferred%2520by%2520accessing%2520%2524G_c%2524%2520with%2520no%2520or%2520little%250Adecompression%2520cost.%2520%25281%2529%2520We%2520characterize%2520IFGC%2520with%2520a%2520class%2520of%2520inference%250Aequivalence%2520relation.%2520The%2520relation%2520captures%2520the%2520node%2520pairs%2520in%2520%2524G%2524%2520that%2520are%2520not%250Adistinguishable%2520for%2520GNN%2520inference.%2520%25282%2529%2520We%2520introduce%2520three%2520practical%250Aspecifications%2520of%2520IFGC%2520for%2520representative%2520GNNs%253A%2520structural%2520preserving%250Acompression%2520%2528SPGC%2529%252C%2520which%2520computes%2520%2524G_c%2524%2520that%2520can%2520be%2520directly%2520processed%2520by%2520GNN%250Ainference%2520without%2520decompression%253B%2520%2528%2524%255Calpha%2524%252C%2520%2524r%2524%2529-compression%252C%2520that%2520allows%2520for%2520a%250Aconfigurable%2520trade-off%2520between%2520compression%2520ratio%2520and%2520inference%2520quality%252C%2520and%250Aanchored%2520compression%2520that%2520preserves%2520inference%2520results%2520for%2520specific%2520nodes%2520of%250Ainterest.%2520For%2520each%2520scheme%252C%2520we%2520introduce%2520compression%2520and%2520inference%2520algorithms%250Awith%2520guarantees%2520of%2520efficiency%2520and%2520quality%2520of%2520the%2520inferred%2520results.%2520We%2520conduct%250Aextensive%2520experiments%2520on%2520diverse%2520sets%2520of%2520large-scale%2520graphs%252C%2520which%2520verifies%2520the%250Aeffectiveness%2520and%2520efficiency%2520of%2520our%2520graph%2520compression%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13034v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inference-friendly%20Graph%20Compression%20for%20Graph%20Neural%20Networks&entry.906535625=Yangxin%20Fan%20and%20Haolai%20Che%20and%20Yinghui%20Wu&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20demonstrated%20promising%20performance%20in%20graph%0Aanalysis.%20Nevertheless%2C%20the%20inference%20process%20of%20GNNs%20remains%20costly%2C%20hindering%0Atheir%20applications%20for%20large%20graphs.%20This%20paper%20proposes%20inference-friendly%0Agraph%20compression%20%28IFGC%29%2C%20a%20graph%20compression%20scheme%20to%20accelerate%20GNNs%0Ainference.%20Given%20a%20graph%20%24G%24%20and%20a%20GNN%20%24M%24%2C%20an%20IFGC%20computes%20a%20small%20compressed%0Agraph%20%24G_c%24%2C%20to%20best%20preserve%20the%20inference%20results%20of%20%24M%24%20over%20%24G%24%2C%20such%20that%0Athe%20result%20can%20be%20directly%20inferred%20by%20accessing%20%24G_c%24%20with%20no%20or%20little%0Adecompression%20cost.%20%281%29%20We%20characterize%20IFGC%20with%20a%20class%20of%20inference%0Aequivalence%20relation.%20The%20relation%20captures%20the%20node%20pairs%20in%20%24G%24%20that%20are%20not%0Adistinguishable%20for%20GNN%20inference.%20%282%29%20We%20introduce%20three%20practical%0Aspecifications%20of%20IFGC%20for%20representative%20GNNs%3A%20structural%20preserving%0Acompression%20%28SPGC%29%2C%20which%20computes%20%24G_c%24%20that%20can%20be%20directly%20processed%20by%20GNN%0Ainference%20without%20decompression%3B%20%28%24%5Calpha%24%2C%20%24r%24%29-compression%2C%20that%20allows%20for%20a%0Aconfigurable%20trade-off%20between%20compression%20ratio%20and%20inference%20quality%2C%20and%0Aanchored%20compression%20that%20preserves%20inference%20results%20for%20specific%20nodes%20of%0Ainterest.%20For%20each%20scheme%2C%20we%20introduce%20compression%20and%20inference%20algorithms%0Awith%20guarantees%20of%20efficiency%20and%20quality%20of%20the%20inferred%20results.%20We%20conduct%0Aextensive%20experiments%20on%20diverse%20sets%20of%20large-scale%20graphs%2C%20which%20verifies%20the%0Aeffectiveness%20and%20efficiency%20of%20our%20graph%20compression%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13034v1&entry.124074799=Read"},
{"title": "Integrating Physics and Topology in Neural Networks for Learning Rigid\n  Body Dynamics", "author": "Amaury Wei and Olga Fink", "abstract": "  Rigid body interactions are fundamental to numerous scientific disciplines,\nbut remain challenging to simulate due to their abrupt nonlinear nature and\nsensitivity to complex, often unknown environmental factors. These challenges\ncall for adaptable learning-based methods capable of capturing complex\ninteractions beyond explicit physical models and simulations. While graph\nneural networks can handle simple scenarios, they struggle with complex scenes\nand long-term predictions. We introduce a novel framework for modeling rigid\nbody dynamics and learning collision interactions, addressing key limitations\nof existing graph-based methods. Our approach extends the traditional\nrepresentation of meshes by incorporating higher-order topology complexes,\noffering a physically consistent representation. Additionally, we propose a\nphysics-informed message-passing neural architecture, embedding physical laws\ndirectly in the model. Our method demonstrates superior accuracy, even during\nlong rollouts, and exhibits strong generalization to unseen scenarios.\nImportantly, this work addresses the challenge of multi-entity dynamic\ninteractions, with applications spanning diverse scientific and engineering\ndomains.\n", "link": "http://arxiv.org/abs/2411.11467v2", "date": "2025-04-17", "relevancy": 2.1866, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.555}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5492}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20Physics%20and%20Topology%20in%20Neural%20Networks%20for%20Learning%20Rigid%0A%20%20Body%20Dynamics&body=Title%3A%20Integrating%20Physics%20and%20Topology%20in%20Neural%20Networks%20for%20Learning%20Rigid%0A%20%20Body%20Dynamics%0AAuthor%3A%20Amaury%20Wei%20and%20Olga%20Fink%0AAbstract%3A%20%20%20Rigid%20body%20interactions%20are%20fundamental%20to%20numerous%20scientific%20disciplines%2C%0Abut%20remain%20challenging%20to%20simulate%20due%20to%20their%20abrupt%20nonlinear%20nature%20and%0Asensitivity%20to%20complex%2C%20often%20unknown%20environmental%20factors.%20These%20challenges%0Acall%20for%20adaptable%20learning-based%20methods%20capable%20of%20capturing%20complex%0Ainteractions%20beyond%20explicit%20physical%20models%20and%20simulations.%20While%20graph%0Aneural%20networks%20can%20handle%20simple%20scenarios%2C%20they%20struggle%20with%20complex%20scenes%0Aand%20long-term%20predictions.%20We%20introduce%20a%20novel%20framework%20for%20modeling%20rigid%0Abody%20dynamics%20and%20learning%20collision%20interactions%2C%20addressing%20key%20limitations%0Aof%20existing%20graph-based%20methods.%20Our%20approach%20extends%20the%20traditional%0Arepresentation%20of%20meshes%20by%20incorporating%20higher-order%20topology%20complexes%2C%0Aoffering%20a%20physically%20consistent%20representation.%20Additionally%2C%20we%20propose%20a%0Aphysics-informed%20message-passing%20neural%20architecture%2C%20embedding%20physical%20laws%0Adirectly%20in%20the%20model.%20Our%20method%20demonstrates%20superior%20accuracy%2C%20even%20during%0Along%20rollouts%2C%20and%20exhibits%20strong%20generalization%20to%20unseen%20scenarios.%0AImportantly%2C%20this%20work%20addresses%20the%20challenge%20of%20multi-entity%20dynamic%0Ainteractions%2C%20with%20applications%20spanning%20diverse%20scientific%20and%20engineering%0Adomains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11467v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520Physics%2520and%2520Topology%2520in%2520Neural%2520Networks%2520for%2520Learning%2520Rigid%250A%2520%2520Body%2520Dynamics%26entry.906535625%3DAmaury%2520Wei%2520and%2520Olga%2520Fink%26entry.1292438233%3D%2520%2520Rigid%2520body%2520interactions%2520are%2520fundamental%2520to%2520numerous%2520scientific%2520disciplines%252C%250Abut%2520remain%2520challenging%2520to%2520simulate%2520due%2520to%2520their%2520abrupt%2520nonlinear%2520nature%2520and%250Asensitivity%2520to%2520complex%252C%2520often%2520unknown%2520environmental%2520factors.%2520These%2520challenges%250Acall%2520for%2520adaptable%2520learning-based%2520methods%2520capable%2520of%2520capturing%2520complex%250Ainteractions%2520beyond%2520explicit%2520physical%2520models%2520and%2520simulations.%2520While%2520graph%250Aneural%2520networks%2520can%2520handle%2520simple%2520scenarios%252C%2520they%2520struggle%2520with%2520complex%2520scenes%250Aand%2520long-term%2520predictions.%2520We%2520introduce%2520a%2520novel%2520framework%2520for%2520modeling%2520rigid%250Abody%2520dynamics%2520and%2520learning%2520collision%2520interactions%252C%2520addressing%2520key%2520limitations%250Aof%2520existing%2520graph-based%2520methods.%2520Our%2520approach%2520extends%2520the%2520traditional%250Arepresentation%2520of%2520meshes%2520by%2520incorporating%2520higher-order%2520topology%2520complexes%252C%250Aoffering%2520a%2520physically%2520consistent%2520representation.%2520Additionally%252C%2520we%2520propose%2520a%250Aphysics-informed%2520message-passing%2520neural%2520architecture%252C%2520embedding%2520physical%2520laws%250Adirectly%2520in%2520the%2520model.%2520Our%2520method%2520demonstrates%2520superior%2520accuracy%252C%2520even%2520during%250Along%2520rollouts%252C%2520and%2520exhibits%2520strong%2520generalization%2520to%2520unseen%2520scenarios.%250AImportantly%252C%2520this%2520work%2520addresses%2520the%2520challenge%2520of%2520multi-entity%2520dynamic%250Ainteractions%252C%2520with%2520applications%2520spanning%2520diverse%2520scientific%2520and%2520engineering%250Adomains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11467v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Physics%20and%20Topology%20in%20Neural%20Networks%20for%20Learning%20Rigid%0A%20%20Body%20Dynamics&entry.906535625=Amaury%20Wei%20and%20Olga%20Fink&entry.1292438233=%20%20Rigid%20body%20interactions%20are%20fundamental%20to%20numerous%20scientific%20disciplines%2C%0Abut%20remain%20challenging%20to%20simulate%20due%20to%20their%20abrupt%20nonlinear%20nature%20and%0Asensitivity%20to%20complex%2C%20often%20unknown%20environmental%20factors.%20These%20challenges%0Acall%20for%20adaptable%20learning-based%20methods%20capable%20of%20capturing%20complex%0Ainteractions%20beyond%20explicit%20physical%20models%20and%20simulations.%20While%20graph%0Aneural%20networks%20can%20handle%20simple%20scenarios%2C%20they%20struggle%20with%20complex%20scenes%0Aand%20long-term%20predictions.%20We%20introduce%20a%20novel%20framework%20for%20modeling%20rigid%0Abody%20dynamics%20and%20learning%20collision%20interactions%2C%20addressing%20key%20limitations%0Aof%20existing%20graph-based%20methods.%20Our%20approach%20extends%20the%20traditional%0Arepresentation%20of%20meshes%20by%20incorporating%20higher-order%20topology%20complexes%2C%0Aoffering%20a%20physically%20consistent%20representation.%20Additionally%2C%20we%20propose%20a%0Aphysics-informed%20message-passing%20neural%20architecture%2C%20embedding%20physical%20laws%0Adirectly%20in%20the%20model.%20Our%20method%20demonstrates%20superior%20accuracy%2C%20even%20during%0Along%20rollouts%2C%20and%20exhibits%20strong%20generalization%20to%20unseen%20scenarios.%0AImportantly%2C%20this%20work%20addresses%20the%20challenge%20of%20multi-entity%20dynamic%0Ainteractions%2C%20with%20applications%20spanning%20diverse%20scientific%20and%20engineering%0Adomains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11467v2&entry.124074799=Read"},
{"title": "Taccel: Scaling Up Vision-based Tactile Robotics via High-performance\n  GPU Simulation", "author": "Yuyang Li and Wenxin Du and Chang Yu and Puhao Li and Zihang Zhao and Tengyu Liu and Chenfanfu Jiang and Yixin Zhu and Siyuan Huang", "abstract": "  Tactile sensing is crucial for achieving human-level robotic capabilities in\nmanipulation tasks. VBTSs have emerged as a promising solution, offering high\nspatial resolution and cost-effectiveness by sensing contact through\ncamera-captured deformation patterns of elastic gel pads. However, these\nsensors' complex physical characteristics and visual signal processing\nrequirements present unique challenges for robotic applications. The lack of\nefficient and accurate simulation tools for VBTS has significantly limited the\nscale and scope of tactile robotics research. Here we present Taccel, a\nhigh-performance simulation platform that integrates IPC and ABD to model\nrobots, tactile sensors, and objects with both accuracy and unprecedented\nspeed, achieving an 18-fold acceleration over real-time across thousands of\nparallel environments. Unlike previous simulators that operate at sub-real-time\nspeeds with limited parallelization, Taccel provides precise physics simulation\nand realistic tactile signals while supporting flexible robot-sensor\nconfigurations through user-friendly APIs. Through extensive validation in\nobject recognition, robotic grasping, and articulated object manipulation, we\ndemonstrate precise simulation and successful sim-to-real transfer. These\ncapabilities position Taccel as a powerful tool for scaling up tactile robotics\nresearch and development. By enabling large-scale simulation and\nexperimentation with tactile sensing, Taccel accelerates the development of\nmore capable robotic systems, potentially transforming how robots interact with\nand understand their physical environment.\n", "link": "http://arxiv.org/abs/2504.12908v1", "date": "2025-04-17", "relevancy": 2.1742, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5549}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5426}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.54}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Taccel%3A%20Scaling%20Up%20Vision-based%20Tactile%20Robotics%20via%20High-performance%0A%20%20GPU%20Simulation&body=Title%3A%20Taccel%3A%20Scaling%20Up%20Vision-based%20Tactile%20Robotics%20via%20High-performance%0A%20%20GPU%20Simulation%0AAuthor%3A%20Yuyang%20Li%20and%20Wenxin%20Du%20and%20Chang%20Yu%20and%20Puhao%20Li%20and%20Zihang%20Zhao%20and%20Tengyu%20Liu%20and%20Chenfanfu%20Jiang%20and%20Yixin%20Zhu%20and%20Siyuan%20Huang%0AAbstract%3A%20%20%20Tactile%20sensing%20is%20crucial%20for%20achieving%20human-level%20robotic%20capabilities%20in%0Amanipulation%20tasks.%20VBTSs%20have%20emerged%20as%20a%20promising%20solution%2C%20offering%20high%0Aspatial%20resolution%20and%20cost-effectiveness%20by%20sensing%20contact%20through%0Acamera-captured%20deformation%20patterns%20of%20elastic%20gel%20pads.%20However%2C%20these%0Asensors%27%20complex%20physical%20characteristics%20and%20visual%20signal%20processing%0Arequirements%20present%20unique%20challenges%20for%20robotic%20applications.%20The%20lack%20of%0Aefficient%20and%20accurate%20simulation%20tools%20for%20VBTS%20has%20significantly%20limited%20the%0Ascale%20and%20scope%20of%20tactile%20robotics%20research.%20Here%20we%20present%20Taccel%2C%20a%0Ahigh-performance%20simulation%20platform%20that%20integrates%20IPC%20and%20ABD%20to%20model%0Arobots%2C%20tactile%20sensors%2C%20and%20objects%20with%20both%20accuracy%20and%20unprecedented%0Aspeed%2C%20achieving%20an%2018-fold%20acceleration%20over%20real-time%20across%20thousands%20of%0Aparallel%20environments.%20Unlike%20previous%20simulators%20that%20operate%20at%20sub-real-time%0Aspeeds%20with%20limited%20parallelization%2C%20Taccel%20provides%20precise%20physics%20simulation%0Aand%20realistic%20tactile%20signals%20while%20supporting%20flexible%20robot-sensor%0Aconfigurations%20through%20user-friendly%20APIs.%20Through%20extensive%20validation%20in%0Aobject%20recognition%2C%20robotic%20grasping%2C%20and%20articulated%20object%20manipulation%2C%20we%0Ademonstrate%20precise%20simulation%20and%20successful%20sim-to-real%20transfer.%20These%0Acapabilities%20position%20Taccel%20as%20a%20powerful%20tool%20for%20scaling%20up%20tactile%20robotics%0Aresearch%20and%20development.%20By%20enabling%20large-scale%20simulation%20and%0Aexperimentation%20with%20tactile%20sensing%2C%20Taccel%20accelerates%20the%20development%20of%0Amore%20capable%20robotic%20systems%2C%20potentially%20transforming%20how%20robots%20interact%20with%0Aand%20understand%20their%20physical%20environment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12908v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaccel%253A%2520Scaling%2520Up%2520Vision-based%2520Tactile%2520Robotics%2520via%2520High-performance%250A%2520%2520GPU%2520Simulation%26entry.906535625%3DYuyang%2520Li%2520and%2520Wenxin%2520Du%2520and%2520Chang%2520Yu%2520and%2520Puhao%2520Li%2520and%2520Zihang%2520Zhao%2520and%2520Tengyu%2520Liu%2520and%2520Chenfanfu%2520Jiang%2520and%2520Yixin%2520Zhu%2520and%2520Siyuan%2520Huang%26entry.1292438233%3D%2520%2520Tactile%2520sensing%2520is%2520crucial%2520for%2520achieving%2520human-level%2520robotic%2520capabilities%2520in%250Amanipulation%2520tasks.%2520VBTSs%2520have%2520emerged%2520as%2520a%2520promising%2520solution%252C%2520offering%2520high%250Aspatial%2520resolution%2520and%2520cost-effectiveness%2520by%2520sensing%2520contact%2520through%250Acamera-captured%2520deformation%2520patterns%2520of%2520elastic%2520gel%2520pads.%2520However%252C%2520these%250Asensors%2527%2520complex%2520physical%2520characteristics%2520and%2520visual%2520signal%2520processing%250Arequirements%2520present%2520unique%2520challenges%2520for%2520robotic%2520applications.%2520The%2520lack%2520of%250Aefficient%2520and%2520accurate%2520simulation%2520tools%2520for%2520VBTS%2520has%2520significantly%2520limited%2520the%250Ascale%2520and%2520scope%2520of%2520tactile%2520robotics%2520research.%2520Here%2520we%2520present%2520Taccel%252C%2520a%250Ahigh-performance%2520simulation%2520platform%2520that%2520integrates%2520IPC%2520and%2520ABD%2520to%2520model%250Arobots%252C%2520tactile%2520sensors%252C%2520and%2520objects%2520with%2520both%2520accuracy%2520and%2520unprecedented%250Aspeed%252C%2520achieving%2520an%252018-fold%2520acceleration%2520over%2520real-time%2520across%2520thousands%2520of%250Aparallel%2520environments.%2520Unlike%2520previous%2520simulators%2520that%2520operate%2520at%2520sub-real-time%250Aspeeds%2520with%2520limited%2520parallelization%252C%2520Taccel%2520provides%2520precise%2520physics%2520simulation%250Aand%2520realistic%2520tactile%2520signals%2520while%2520supporting%2520flexible%2520robot-sensor%250Aconfigurations%2520through%2520user-friendly%2520APIs.%2520Through%2520extensive%2520validation%2520in%250Aobject%2520recognition%252C%2520robotic%2520grasping%252C%2520and%2520articulated%2520object%2520manipulation%252C%2520we%250Ademonstrate%2520precise%2520simulation%2520and%2520successful%2520sim-to-real%2520transfer.%2520These%250Acapabilities%2520position%2520Taccel%2520as%2520a%2520powerful%2520tool%2520for%2520scaling%2520up%2520tactile%2520robotics%250Aresearch%2520and%2520development.%2520By%2520enabling%2520large-scale%2520simulation%2520and%250Aexperimentation%2520with%2520tactile%2520sensing%252C%2520Taccel%2520accelerates%2520the%2520development%2520of%250Amore%2520capable%2520robotic%2520systems%252C%2520potentially%2520transforming%2520how%2520robots%2520interact%2520with%250Aand%2520understand%2520their%2520physical%2520environment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12908v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Taccel%3A%20Scaling%20Up%20Vision-based%20Tactile%20Robotics%20via%20High-performance%0A%20%20GPU%20Simulation&entry.906535625=Yuyang%20Li%20and%20Wenxin%20Du%20and%20Chang%20Yu%20and%20Puhao%20Li%20and%20Zihang%20Zhao%20and%20Tengyu%20Liu%20and%20Chenfanfu%20Jiang%20and%20Yixin%20Zhu%20and%20Siyuan%20Huang&entry.1292438233=%20%20Tactile%20sensing%20is%20crucial%20for%20achieving%20human-level%20robotic%20capabilities%20in%0Amanipulation%20tasks.%20VBTSs%20have%20emerged%20as%20a%20promising%20solution%2C%20offering%20high%0Aspatial%20resolution%20and%20cost-effectiveness%20by%20sensing%20contact%20through%0Acamera-captured%20deformation%20patterns%20of%20elastic%20gel%20pads.%20However%2C%20these%0Asensors%27%20complex%20physical%20characteristics%20and%20visual%20signal%20processing%0Arequirements%20present%20unique%20challenges%20for%20robotic%20applications.%20The%20lack%20of%0Aefficient%20and%20accurate%20simulation%20tools%20for%20VBTS%20has%20significantly%20limited%20the%0Ascale%20and%20scope%20of%20tactile%20robotics%20research.%20Here%20we%20present%20Taccel%2C%20a%0Ahigh-performance%20simulation%20platform%20that%20integrates%20IPC%20and%20ABD%20to%20model%0Arobots%2C%20tactile%20sensors%2C%20and%20objects%20with%20both%20accuracy%20and%20unprecedented%0Aspeed%2C%20achieving%20an%2018-fold%20acceleration%20over%20real-time%20across%20thousands%20of%0Aparallel%20environments.%20Unlike%20previous%20simulators%20that%20operate%20at%20sub-real-time%0Aspeeds%20with%20limited%20parallelization%2C%20Taccel%20provides%20precise%20physics%20simulation%0Aand%20realistic%20tactile%20signals%20while%20supporting%20flexible%20robot-sensor%0Aconfigurations%20through%20user-friendly%20APIs.%20Through%20extensive%20validation%20in%0Aobject%20recognition%2C%20robotic%20grasping%2C%20and%20articulated%20object%20manipulation%2C%20we%0Ademonstrate%20precise%20simulation%20and%20successful%20sim-to-real%20transfer.%20These%0Acapabilities%20position%20Taccel%20as%20a%20powerful%20tool%20for%20scaling%20up%20tactile%20robotics%0Aresearch%20and%20development.%20By%20enabling%20large-scale%20simulation%20and%0Aexperimentation%20with%20tactile%20sensing%2C%20Taccel%20accelerates%20the%20development%20of%0Amore%20capable%20robotic%20systems%2C%20potentially%20transforming%20how%20robots%20interact%20with%0Aand%20understand%20their%20physical%20environment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12908v1&entry.124074799=Read"},
{"title": "Expert Kernel Generation Network Driven by Contextual Mapping for\n  Hyperspectral Image Classification", "author": "Guandong Li and Mengxia Ye", "abstract": "  Deep neural networks face several challenges in hyperspectral image\nclassification, including high-dimensional data, sparse distribution of ground\nobjects, and spectral redundancy, which often lead to classification\noverfitting and limited generalization capability. To more efficiently adapt to\nground object distributions while extracting image features without introducing\nexcessive parameters and skipping redundant information, this paper proposes\nEKGNet based on an improved 3D-DenseNet model, consisting of a context-aware\nmapping network and a dynamic kernel generation module. The context-aware\nmapping module translates global contextual information of hyperspectral inputs\ninto instructions for combining base convolutional kernels, while the dynamic\nkernels are composed of K groups of base convolutions, analogous to K different\ntypes of experts specializing in fundamental patterns across various\ndimensions. The mapping module and dynamic kernel generation mechanism form a\ntightly coupled system - the former generates meaningful combination weights\nbased on inputs, while the latter constructs an adaptive expert convolution\nsystem using these weights. This dynamic approach enables the model to focus\nmore flexibly on key spatial structures when processing different regions,\nrather than relying on the fixed receptive field of a single static\nconvolutional kernel. EKGNet enhances model representation capability through a\n3D dynamic expert convolution system without increasing network depth or width.\nThe proposed method demonstrates superior performance on IN, UP, and KSC\ndatasets, outperforming mainstream hyperspectral image classification\napproaches.\n", "link": "http://arxiv.org/abs/2504.13045v1", "date": "2025-04-17", "relevancy": 2.1727, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5607}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.542}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Expert%20Kernel%20Generation%20Network%20Driven%20by%20Contextual%20Mapping%20for%0A%20%20Hyperspectral%20Image%20Classification&body=Title%3A%20Expert%20Kernel%20Generation%20Network%20Driven%20by%20Contextual%20Mapping%20for%0A%20%20Hyperspectral%20Image%20Classification%0AAuthor%3A%20Guandong%20Li%20and%20Mengxia%20Ye%0AAbstract%3A%20%20%20Deep%20neural%20networks%20face%20several%20challenges%20in%20hyperspectral%20image%0Aclassification%2C%20including%20high-dimensional%20data%2C%20sparse%20distribution%20of%20ground%0Aobjects%2C%20and%20spectral%20redundancy%2C%20which%20often%20lead%20to%20classification%0Aoverfitting%20and%20limited%20generalization%20capability.%20To%20more%20efficiently%20adapt%20to%0Aground%20object%20distributions%20while%20extracting%20image%20features%20without%20introducing%0Aexcessive%20parameters%20and%20skipping%20redundant%20information%2C%20this%20paper%20proposes%0AEKGNet%20based%20on%20an%20improved%203D-DenseNet%20model%2C%20consisting%20of%20a%20context-aware%0Amapping%20network%20and%20a%20dynamic%20kernel%20generation%20module.%20The%20context-aware%0Amapping%20module%20translates%20global%20contextual%20information%20of%20hyperspectral%20inputs%0Ainto%20instructions%20for%20combining%20base%20convolutional%20kernels%2C%20while%20the%20dynamic%0Akernels%20are%20composed%20of%20K%20groups%20of%20base%20convolutions%2C%20analogous%20to%20K%20different%0Atypes%20of%20experts%20specializing%20in%20fundamental%20patterns%20across%20various%0Adimensions.%20The%20mapping%20module%20and%20dynamic%20kernel%20generation%20mechanism%20form%20a%0Atightly%20coupled%20system%20-%20the%20former%20generates%20meaningful%20combination%20weights%0Abased%20on%20inputs%2C%20while%20the%20latter%20constructs%20an%20adaptive%20expert%20convolution%0Asystem%20using%20these%20weights.%20This%20dynamic%20approach%20enables%20the%20model%20to%20focus%0Amore%20flexibly%20on%20key%20spatial%20structures%20when%20processing%20different%20regions%2C%0Arather%20than%20relying%20on%20the%20fixed%20receptive%20field%20of%20a%20single%20static%0Aconvolutional%20kernel.%20EKGNet%20enhances%20model%20representation%20capability%20through%20a%0A3D%20dynamic%20expert%20convolution%20system%20without%20increasing%20network%20depth%20or%20width.%0AThe%20proposed%20method%20demonstrates%20superior%20performance%20on%20IN%2C%20UP%2C%20and%20KSC%0Adatasets%2C%20outperforming%20mainstream%20hyperspectral%20image%20classification%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13045v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExpert%2520Kernel%2520Generation%2520Network%2520Driven%2520by%2520Contextual%2520Mapping%2520for%250A%2520%2520Hyperspectral%2520Image%2520Classification%26entry.906535625%3DGuandong%2520Li%2520and%2520Mengxia%2520Ye%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520face%2520several%2520challenges%2520in%2520hyperspectral%2520image%250Aclassification%252C%2520including%2520high-dimensional%2520data%252C%2520sparse%2520distribution%2520of%2520ground%250Aobjects%252C%2520and%2520spectral%2520redundancy%252C%2520which%2520often%2520lead%2520to%2520classification%250Aoverfitting%2520and%2520limited%2520generalization%2520capability.%2520To%2520more%2520efficiently%2520adapt%2520to%250Aground%2520object%2520distributions%2520while%2520extracting%2520image%2520features%2520without%2520introducing%250Aexcessive%2520parameters%2520and%2520skipping%2520redundant%2520information%252C%2520this%2520paper%2520proposes%250AEKGNet%2520based%2520on%2520an%2520improved%25203D-DenseNet%2520model%252C%2520consisting%2520of%2520a%2520context-aware%250Amapping%2520network%2520and%2520a%2520dynamic%2520kernel%2520generation%2520module.%2520The%2520context-aware%250Amapping%2520module%2520translates%2520global%2520contextual%2520information%2520of%2520hyperspectral%2520inputs%250Ainto%2520instructions%2520for%2520combining%2520base%2520convolutional%2520kernels%252C%2520while%2520the%2520dynamic%250Akernels%2520are%2520composed%2520of%2520K%2520groups%2520of%2520base%2520convolutions%252C%2520analogous%2520to%2520K%2520different%250Atypes%2520of%2520experts%2520specializing%2520in%2520fundamental%2520patterns%2520across%2520various%250Adimensions.%2520The%2520mapping%2520module%2520and%2520dynamic%2520kernel%2520generation%2520mechanism%2520form%2520a%250Atightly%2520coupled%2520system%2520-%2520the%2520former%2520generates%2520meaningful%2520combination%2520weights%250Abased%2520on%2520inputs%252C%2520while%2520the%2520latter%2520constructs%2520an%2520adaptive%2520expert%2520convolution%250Asystem%2520using%2520these%2520weights.%2520This%2520dynamic%2520approach%2520enables%2520the%2520model%2520to%2520focus%250Amore%2520flexibly%2520on%2520key%2520spatial%2520structures%2520when%2520processing%2520different%2520regions%252C%250Arather%2520than%2520relying%2520on%2520the%2520fixed%2520receptive%2520field%2520of%2520a%2520single%2520static%250Aconvolutional%2520kernel.%2520EKGNet%2520enhances%2520model%2520representation%2520capability%2520through%2520a%250A3D%2520dynamic%2520expert%2520convolution%2520system%2520without%2520increasing%2520network%2520depth%2520or%2520width.%250AThe%2520proposed%2520method%2520demonstrates%2520superior%2520performance%2520on%2520IN%252C%2520UP%252C%2520and%2520KSC%250Adatasets%252C%2520outperforming%2520mainstream%2520hyperspectral%2520image%2520classification%250Aapproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13045v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Expert%20Kernel%20Generation%20Network%20Driven%20by%20Contextual%20Mapping%20for%0A%20%20Hyperspectral%20Image%20Classification&entry.906535625=Guandong%20Li%20and%20Mengxia%20Ye&entry.1292438233=%20%20Deep%20neural%20networks%20face%20several%20challenges%20in%20hyperspectral%20image%0Aclassification%2C%20including%20high-dimensional%20data%2C%20sparse%20distribution%20of%20ground%0Aobjects%2C%20and%20spectral%20redundancy%2C%20which%20often%20lead%20to%20classification%0Aoverfitting%20and%20limited%20generalization%20capability.%20To%20more%20efficiently%20adapt%20to%0Aground%20object%20distributions%20while%20extracting%20image%20features%20without%20introducing%0Aexcessive%20parameters%20and%20skipping%20redundant%20information%2C%20this%20paper%20proposes%0AEKGNet%20based%20on%20an%20improved%203D-DenseNet%20model%2C%20consisting%20of%20a%20context-aware%0Amapping%20network%20and%20a%20dynamic%20kernel%20generation%20module.%20The%20context-aware%0Amapping%20module%20translates%20global%20contextual%20information%20of%20hyperspectral%20inputs%0Ainto%20instructions%20for%20combining%20base%20convolutional%20kernels%2C%20while%20the%20dynamic%0Akernels%20are%20composed%20of%20K%20groups%20of%20base%20convolutions%2C%20analogous%20to%20K%20different%0Atypes%20of%20experts%20specializing%20in%20fundamental%20patterns%20across%20various%0Adimensions.%20The%20mapping%20module%20and%20dynamic%20kernel%20generation%20mechanism%20form%20a%0Atightly%20coupled%20system%20-%20the%20former%20generates%20meaningful%20combination%20weights%0Abased%20on%20inputs%2C%20while%20the%20latter%20constructs%20an%20adaptive%20expert%20convolution%0Asystem%20using%20these%20weights.%20This%20dynamic%20approach%20enables%20the%20model%20to%20focus%0Amore%20flexibly%20on%20key%20spatial%20structures%20when%20processing%20different%20regions%2C%0Arather%20than%20relying%20on%20the%20fixed%20receptive%20field%20of%20a%20single%20static%0Aconvolutional%20kernel.%20EKGNet%20enhances%20model%20representation%20capability%20through%20a%0A3D%20dynamic%20expert%20convolution%20system%20without%20increasing%20network%20depth%20or%20width.%0AThe%20proposed%20method%20demonstrates%20superior%20performance%20on%20IN%2C%20UP%2C%20and%20KSC%0Adatasets%2C%20outperforming%20mainstream%20hyperspectral%20image%20classification%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13045v1&entry.124074799=Read"},
{"title": "NTIRE 2025 Challenge on Short-form UGC Video Quality Assessment and\n  Enhancement: Methods and Results", "author": "Xin Li and Kun Yuan and Bingchen Li and Fengbin Guan and Yizhen Shao and Zihao Yu and Xijun Wang and Yiting Lu and Wei Luo and Suhang Yao and Ming Sun and Chao Zhou and Zhibo Chen and Radu Timofte and Yabin Zhang and Ao-Xiang Zhang and Tianwu Zhi and Jianzhao Liu and Yang Li and Jingwen Xu and Yiting Liao and Yushen Zuo and Mingyang Wu and Renjie Li and Shengyun Zhong and Zhengzhong Tu and Yufan Liu and Xiangguang Chen and Zuowei Cao and Minhao Tang and Shan Liu and Kexin Zhang and Jingfen Xie and Yan Wang and Kai Chen and Shijie Zhao and Yunchen Zhang and Xiangkai Xu and Hong Gao and Ji Shi and Yiming Bao and Xiugang Dong and Xiangsheng Zhou and Yaofeng Tu and Ying Liang and Yiwen Wang and Xinning Chai and Yuxuan Zhang and Zhengxue Cheng and Yingsheng Qin and Yucai Yang and Rong Xie and Li Song and Wei Sun and Kang Fu and Linhan Cao and Dandan Zhu and Kaiwei Zhang and Yucheng Zhu and Zicheng Zhang and Menghan Hu and Xiongkuo Min and Guangtao Zhai and Zhi Jin and Jiawei Wu and Wei Wang and Wenjian Zhang and Yuhai Lan and Gaoxiong Yi and Hengyuan Na and Wang Luo and Di Wu and MingYin Bai and Jiawang Du and Zilong Lu and Zhenyu Jiang and Hui Zeng and Ziguan Cui and Zongliang Gan and Guijin Tang and Xinglin Xie and Kehuan Song and Xiaoqiang Lu and Licheng Jiao and Fang Liu and Xu Liu and Puhua Chen and Ha Thu Nguyen and Katrien De Moor and Seyed Ali Amirshahi and Mohamed-Chaker Larabi and Qi Tang and Linfeng He and Zhiyong Gao and Zixuan Gao and Guohua Zhang and Zhiye Huang and Yi Deng and Qingmiao Jiang and Lu Chen and Yi Yang and Xi Liao and Nourine Mohammed Nadir and Yuxuan Jiang and Qiang Zhu and Siyue Teng and Fan Zhang and Shuyuan Zhu and Bing Zeng and David Bull and Meiqin Liu and Chao Yao and Yao Zhao", "abstract": "  This paper presents a review for the NTIRE 2025 Challenge on Short-form UGC\nVideo Quality Assessment and Enhancement. The challenge comprises two tracks:\n(i) Efficient Video Quality Assessment (KVQ), and (ii) Diffusion-based Image\nSuper-Resolution (KwaiSR). Track 1 aims to advance the development of\nlightweight and efficient video quality assessment (VQA) models, with an\nemphasis on eliminating reliance on model ensembles, redundant weights, and\nother computationally expensive components in the previous IQA/VQA\ncompetitions. Track 2 introduces a new short-form UGC dataset tailored for\nsingle image super-resolution, i.e., the KwaiSR dataset. It consists of 1,800\nsynthetically generated S-UGC image pairs and 1,900 real-world S-UGC images,\nwhich are split into training, validation, and test sets using a ratio of\n8:1:1. The primary objective of the challenge is to drive research that\nbenefits the user experience of short-form UGC platforms such as Kwai and\nTikTok. This challenge attracted 266 participants and received 18 valid final\nsubmissions with corresponding fact sheets, significantly contributing to the\nprogress of short-form UGC VQA and image superresolution. The project is\npublicly available at https://github.com/lixinustc/KVQE-\nChallengeCVPR-NTIRE2025.\n", "link": "http://arxiv.org/abs/2504.13131v1", "date": "2025-04-17", "relevancy": 2.1579, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5584}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5298}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NTIRE%202025%20Challenge%20on%20Short-form%20UGC%20Video%20Quality%20Assessment%20and%0A%20%20Enhancement%3A%20Methods%20and%20Results&body=Title%3A%20NTIRE%202025%20Challenge%20on%20Short-form%20UGC%20Video%20Quality%20Assessment%20and%0A%20%20Enhancement%3A%20Methods%20and%20Results%0AAuthor%3A%20Xin%20Li%20and%20Kun%20Yuan%20and%20Bingchen%20Li%20and%20Fengbin%20Guan%20and%20Yizhen%20Shao%20and%20Zihao%20Yu%20and%20Xijun%20Wang%20and%20Yiting%20Lu%20and%20Wei%20Luo%20and%20Suhang%20Yao%20and%20Ming%20Sun%20and%20Chao%20Zhou%20and%20Zhibo%20Chen%20and%20Radu%20Timofte%20and%20Yabin%20Zhang%20and%20Ao-Xiang%20Zhang%20and%20Tianwu%20Zhi%20and%20Jianzhao%20Liu%20and%20Yang%20Li%20and%20Jingwen%20Xu%20and%20Yiting%20Liao%20and%20Yushen%20Zuo%20and%20Mingyang%20Wu%20and%20Renjie%20Li%20and%20Shengyun%20Zhong%20and%20Zhengzhong%20Tu%20and%20Yufan%20Liu%20and%20Xiangguang%20Chen%20and%20Zuowei%20Cao%20and%20Minhao%20Tang%20and%20Shan%20Liu%20and%20Kexin%20Zhang%20and%20Jingfen%20Xie%20and%20Yan%20Wang%20and%20Kai%20Chen%20and%20Shijie%20Zhao%20and%20Yunchen%20Zhang%20and%20Xiangkai%20Xu%20and%20Hong%20Gao%20and%20Ji%20Shi%20and%20Yiming%20Bao%20and%20Xiugang%20Dong%20and%20Xiangsheng%20Zhou%20and%20Yaofeng%20Tu%20and%20Ying%20Liang%20and%20Yiwen%20Wang%20and%20Xinning%20Chai%20and%20Yuxuan%20Zhang%20and%20Zhengxue%20Cheng%20and%20Yingsheng%20Qin%20and%20Yucai%20Yang%20and%20Rong%20Xie%20and%20Li%20Song%20and%20Wei%20Sun%20and%20Kang%20Fu%20and%20Linhan%20Cao%20and%20Dandan%20Zhu%20and%20Kaiwei%20Zhang%20and%20Yucheng%20Zhu%20and%20Zicheng%20Zhang%20and%20Menghan%20Hu%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai%20and%20Zhi%20Jin%20and%20Jiawei%20Wu%20and%20Wei%20Wang%20and%20Wenjian%20Zhang%20and%20Yuhai%20Lan%20and%20Gaoxiong%20Yi%20and%20Hengyuan%20Na%20and%20Wang%20Luo%20and%20Di%20Wu%20and%20MingYin%20Bai%20and%20Jiawang%20Du%20and%20Zilong%20Lu%20and%20Zhenyu%20Jiang%20and%20Hui%20Zeng%20and%20Ziguan%20Cui%20and%20Zongliang%20Gan%20and%20Guijin%20Tang%20and%20Xinglin%20Xie%20and%20Kehuan%20Song%20and%20Xiaoqiang%20Lu%20and%20Licheng%20Jiao%20and%20Fang%20Liu%20and%20Xu%20Liu%20and%20Puhua%20Chen%20and%20Ha%20Thu%20Nguyen%20and%20Katrien%20De%20Moor%20and%20Seyed%20Ali%20Amirshahi%20and%20Mohamed-Chaker%20Larabi%20and%20Qi%20Tang%20and%20Linfeng%20He%20and%20Zhiyong%20Gao%20and%20Zixuan%20Gao%20and%20Guohua%20Zhang%20and%20Zhiye%20Huang%20and%20Yi%20Deng%20and%20Qingmiao%20Jiang%20and%20Lu%20Chen%20and%20Yi%20Yang%20and%20Xi%20Liao%20and%20Nourine%20Mohammed%20Nadir%20and%20Yuxuan%20Jiang%20and%20Qiang%20Zhu%20and%20Siyue%20Teng%20and%20Fan%20Zhang%20and%20Shuyuan%20Zhu%20and%20Bing%20Zeng%20and%20David%20Bull%20and%20Meiqin%20Liu%20and%20Chao%20Yao%20and%20Yao%20Zhao%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20review%20for%20the%20NTIRE%202025%20Challenge%20on%20Short-form%20UGC%0AVideo%20Quality%20Assessment%20and%20Enhancement.%20The%20challenge%20comprises%20two%20tracks%3A%0A%28i%29%20Efficient%20Video%20Quality%20Assessment%20%28KVQ%29%2C%20and%20%28ii%29%20Diffusion-based%20Image%0ASuper-Resolution%20%28KwaiSR%29.%20Track%201%20aims%20to%20advance%20the%20development%20of%0Alightweight%20and%20efficient%20video%20quality%20assessment%20%28VQA%29%20models%2C%20with%20an%0Aemphasis%20on%20eliminating%20reliance%20on%20model%20ensembles%2C%20redundant%20weights%2C%20and%0Aother%20computationally%20expensive%20components%20in%20the%20previous%20IQA/VQA%0Acompetitions.%20Track%202%20introduces%20a%20new%20short-form%20UGC%20dataset%20tailored%20for%0Asingle%20image%20super-resolution%2C%20i.e.%2C%20the%20KwaiSR%20dataset.%20It%20consists%20of%201%2C800%0Asynthetically%20generated%20S-UGC%20image%20pairs%20and%201%2C900%20real-world%20S-UGC%20images%2C%0Awhich%20are%20split%20into%20training%2C%20validation%2C%20and%20test%20sets%20using%20a%20ratio%20of%0A8%3A1%3A1.%20The%20primary%20objective%20of%20the%20challenge%20is%20to%20drive%20research%20that%0Abenefits%20the%20user%20experience%20of%20short-form%20UGC%20platforms%20such%20as%20Kwai%20and%0ATikTok.%20This%20challenge%20attracted%20266%20participants%20and%20received%2018%20valid%20final%0Asubmissions%20with%20corresponding%20fact%20sheets%2C%20significantly%20contributing%20to%20the%0Aprogress%20of%20short-form%20UGC%20VQA%20and%20image%20superresolution.%20The%20project%20is%0Apublicly%20available%20at%20https%3A//github.com/lixinustc/KVQE-%0AChallengeCVPR-NTIRE2025.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13131v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNTIRE%25202025%2520Challenge%2520on%2520Short-form%2520UGC%2520Video%2520Quality%2520Assessment%2520and%250A%2520%2520Enhancement%253A%2520Methods%2520and%2520Results%26entry.906535625%3DXin%2520Li%2520and%2520Kun%2520Yuan%2520and%2520Bingchen%2520Li%2520and%2520Fengbin%2520Guan%2520and%2520Yizhen%2520Shao%2520and%2520Zihao%2520Yu%2520and%2520Xijun%2520Wang%2520and%2520Yiting%2520Lu%2520and%2520Wei%2520Luo%2520and%2520Suhang%2520Yao%2520and%2520Ming%2520Sun%2520and%2520Chao%2520Zhou%2520and%2520Zhibo%2520Chen%2520and%2520Radu%2520Timofte%2520and%2520Yabin%2520Zhang%2520and%2520Ao-Xiang%2520Zhang%2520and%2520Tianwu%2520Zhi%2520and%2520Jianzhao%2520Liu%2520and%2520Yang%2520Li%2520and%2520Jingwen%2520Xu%2520and%2520Yiting%2520Liao%2520and%2520Yushen%2520Zuo%2520and%2520Mingyang%2520Wu%2520and%2520Renjie%2520Li%2520and%2520Shengyun%2520Zhong%2520and%2520Zhengzhong%2520Tu%2520and%2520Yufan%2520Liu%2520and%2520Xiangguang%2520Chen%2520and%2520Zuowei%2520Cao%2520and%2520Minhao%2520Tang%2520and%2520Shan%2520Liu%2520and%2520Kexin%2520Zhang%2520and%2520Jingfen%2520Xie%2520and%2520Yan%2520Wang%2520and%2520Kai%2520Chen%2520and%2520Shijie%2520Zhao%2520and%2520Yunchen%2520Zhang%2520and%2520Xiangkai%2520Xu%2520and%2520Hong%2520Gao%2520and%2520Ji%2520Shi%2520and%2520Yiming%2520Bao%2520and%2520Xiugang%2520Dong%2520and%2520Xiangsheng%2520Zhou%2520and%2520Yaofeng%2520Tu%2520and%2520Ying%2520Liang%2520and%2520Yiwen%2520Wang%2520and%2520Xinning%2520Chai%2520and%2520Yuxuan%2520Zhang%2520and%2520Zhengxue%2520Cheng%2520and%2520Yingsheng%2520Qin%2520and%2520Yucai%2520Yang%2520and%2520Rong%2520Xie%2520and%2520Li%2520Song%2520and%2520Wei%2520Sun%2520and%2520Kang%2520Fu%2520and%2520Linhan%2520Cao%2520and%2520Dandan%2520Zhu%2520and%2520Kaiwei%2520Zhang%2520and%2520Yucheng%2520Zhu%2520and%2520Zicheng%2520Zhang%2520and%2520Menghan%2520Hu%2520and%2520Xiongkuo%2520Min%2520and%2520Guangtao%2520Zhai%2520and%2520Zhi%2520Jin%2520and%2520Jiawei%2520Wu%2520and%2520Wei%2520Wang%2520and%2520Wenjian%2520Zhang%2520and%2520Yuhai%2520Lan%2520and%2520Gaoxiong%2520Yi%2520and%2520Hengyuan%2520Na%2520and%2520Wang%2520Luo%2520and%2520Di%2520Wu%2520and%2520MingYin%2520Bai%2520and%2520Jiawang%2520Du%2520and%2520Zilong%2520Lu%2520and%2520Zhenyu%2520Jiang%2520and%2520Hui%2520Zeng%2520and%2520Ziguan%2520Cui%2520and%2520Zongliang%2520Gan%2520and%2520Guijin%2520Tang%2520and%2520Xinglin%2520Xie%2520and%2520Kehuan%2520Song%2520and%2520Xiaoqiang%2520Lu%2520and%2520Licheng%2520Jiao%2520and%2520Fang%2520Liu%2520and%2520Xu%2520Liu%2520and%2520Puhua%2520Chen%2520and%2520Ha%2520Thu%2520Nguyen%2520and%2520Katrien%2520De%2520Moor%2520and%2520Seyed%2520Ali%2520Amirshahi%2520and%2520Mohamed-Chaker%2520Larabi%2520and%2520Qi%2520Tang%2520and%2520Linfeng%2520He%2520and%2520Zhiyong%2520Gao%2520and%2520Zixuan%2520Gao%2520and%2520Guohua%2520Zhang%2520and%2520Zhiye%2520Huang%2520and%2520Yi%2520Deng%2520and%2520Qingmiao%2520Jiang%2520and%2520Lu%2520Chen%2520and%2520Yi%2520Yang%2520and%2520Xi%2520Liao%2520and%2520Nourine%2520Mohammed%2520Nadir%2520and%2520Yuxuan%2520Jiang%2520and%2520Qiang%2520Zhu%2520and%2520Siyue%2520Teng%2520and%2520Fan%2520Zhang%2520and%2520Shuyuan%2520Zhu%2520and%2520Bing%2520Zeng%2520and%2520David%2520Bull%2520and%2520Meiqin%2520Liu%2520and%2520Chao%2520Yao%2520and%2520Yao%2520Zhao%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520review%2520for%2520the%2520NTIRE%25202025%2520Challenge%2520on%2520Short-form%2520UGC%250AVideo%2520Quality%2520Assessment%2520and%2520Enhancement.%2520The%2520challenge%2520comprises%2520two%2520tracks%253A%250A%2528i%2529%2520Efficient%2520Video%2520Quality%2520Assessment%2520%2528KVQ%2529%252C%2520and%2520%2528ii%2529%2520Diffusion-based%2520Image%250ASuper-Resolution%2520%2528KwaiSR%2529.%2520Track%25201%2520aims%2520to%2520advance%2520the%2520development%2520of%250Alightweight%2520and%2520efficient%2520video%2520quality%2520assessment%2520%2528VQA%2529%2520models%252C%2520with%2520an%250Aemphasis%2520on%2520eliminating%2520reliance%2520on%2520model%2520ensembles%252C%2520redundant%2520weights%252C%2520and%250Aother%2520computationally%2520expensive%2520components%2520in%2520the%2520previous%2520IQA/VQA%250Acompetitions.%2520Track%25202%2520introduces%2520a%2520new%2520short-form%2520UGC%2520dataset%2520tailored%2520for%250Asingle%2520image%2520super-resolution%252C%2520i.e.%252C%2520the%2520KwaiSR%2520dataset.%2520It%2520consists%2520of%25201%252C800%250Asynthetically%2520generated%2520S-UGC%2520image%2520pairs%2520and%25201%252C900%2520real-world%2520S-UGC%2520images%252C%250Awhich%2520are%2520split%2520into%2520training%252C%2520validation%252C%2520and%2520test%2520sets%2520using%2520a%2520ratio%2520of%250A8%253A1%253A1.%2520The%2520primary%2520objective%2520of%2520the%2520challenge%2520is%2520to%2520drive%2520research%2520that%250Abenefits%2520the%2520user%2520experience%2520of%2520short-form%2520UGC%2520platforms%2520such%2520as%2520Kwai%2520and%250ATikTok.%2520This%2520challenge%2520attracted%2520266%2520participants%2520and%2520received%252018%2520valid%2520final%250Asubmissions%2520with%2520corresponding%2520fact%2520sheets%252C%2520significantly%2520contributing%2520to%2520the%250Aprogress%2520of%2520short-form%2520UGC%2520VQA%2520and%2520image%2520superresolution.%2520The%2520project%2520is%250Apublicly%2520available%2520at%2520https%253A//github.com/lixinustc/KVQE-%250AChallengeCVPR-NTIRE2025.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13131v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NTIRE%202025%20Challenge%20on%20Short-form%20UGC%20Video%20Quality%20Assessment%20and%0A%20%20Enhancement%3A%20Methods%20and%20Results&entry.906535625=Xin%20Li%20and%20Kun%20Yuan%20and%20Bingchen%20Li%20and%20Fengbin%20Guan%20and%20Yizhen%20Shao%20and%20Zihao%20Yu%20and%20Xijun%20Wang%20and%20Yiting%20Lu%20and%20Wei%20Luo%20and%20Suhang%20Yao%20and%20Ming%20Sun%20and%20Chao%20Zhou%20and%20Zhibo%20Chen%20and%20Radu%20Timofte%20and%20Yabin%20Zhang%20and%20Ao-Xiang%20Zhang%20and%20Tianwu%20Zhi%20and%20Jianzhao%20Liu%20and%20Yang%20Li%20and%20Jingwen%20Xu%20and%20Yiting%20Liao%20and%20Yushen%20Zuo%20and%20Mingyang%20Wu%20and%20Renjie%20Li%20and%20Shengyun%20Zhong%20and%20Zhengzhong%20Tu%20and%20Yufan%20Liu%20and%20Xiangguang%20Chen%20and%20Zuowei%20Cao%20and%20Minhao%20Tang%20and%20Shan%20Liu%20and%20Kexin%20Zhang%20and%20Jingfen%20Xie%20and%20Yan%20Wang%20and%20Kai%20Chen%20and%20Shijie%20Zhao%20and%20Yunchen%20Zhang%20and%20Xiangkai%20Xu%20and%20Hong%20Gao%20and%20Ji%20Shi%20and%20Yiming%20Bao%20and%20Xiugang%20Dong%20and%20Xiangsheng%20Zhou%20and%20Yaofeng%20Tu%20and%20Ying%20Liang%20and%20Yiwen%20Wang%20and%20Xinning%20Chai%20and%20Yuxuan%20Zhang%20and%20Zhengxue%20Cheng%20and%20Yingsheng%20Qin%20and%20Yucai%20Yang%20and%20Rong%20Xie%20and%20Li%20Song%20and%20Wei%20Sun%20and%20Kang%20Fu%20and%20Linhan%20Cao%20and%20Dandan%20Zhu%20and%20Kaiwei%20Zhang%20and%20Yucheng%20Zhu%20and%20Zicheng%20Zhang%20and%20Menghan%20Hu%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai%20and%20Zhi%20Jin%20and%20Jiawei%20Wu%20and%20Wei%20Wang%20and%20Wenjian%20Zhang%20and%20Yuhai%20Lan%20and%20Gaoxiong%20Yi%20and%20Hengyuan%20Na%20and%20Wang%20Luo%20and%20Di%20Wu%20and%20MingYin%20Bai%20and%20Jiawang%20Du%20and%20Zilong%20Lu%20and%20Zhenyu%20Jiang%20and%20Hui%20Zeng%20and%20Ziguan%20Cui%20and%20Zongliang%20Gan%20and%20Guijin%20Tang%20and%20Xinglin%20Xie%20and%20Kehuan%20Song%20and%20Xiaoqiang%20Lu%20and%20Licheng%20Jiao%20and%20Fang%20Liu%20and%20Xu%20Liu%20and%20Puhua%20Chen%20and%20Ha%20Thu%20Nguyen%20and%20Katrien%20De%20Moor%20and%20Seyed%20Ali%20Amirshahi%20and%20Mohamed-Chaker%20Larabi%20and%20Qi%20Tang%20and%20Linfeng%20He%20and%20Zhiyong%20Gao%20and%20Zixuan%20Gao%20and%20Guohua%20Zhang%20and%20Zhiye%20Huang%20and%20Yi%20Deng%20and%20Qingmiao%20Jiang%20and%20Lu%20Chen%20and%20Yi%20Yang%20and%20Xi%20Liao%20and%20Nourine%20Mohammed%20Nadir%20and%20Yuxuan%20Jiang%20and%20Qiang%20Zhu%20and%20Siyue%20Teng%20and%20Fan%20Zhang%20and%20Shuyuan%20Zhu%20and%20Bing%20Zeng%20and%20David%20Bull%20and%20Meiqin%20Liu%20and%20Chao%20Yao%20and%20Yao%20Zhao&entry.1292438233=%20%20This%20paper%20presents%20a%20review%20for%20the%20NTIRE%202025%20Challenge%20on%20Short-form%20UGC%0AVideo%20Quality%20Assessment%20and%20Enhancement.%20The%20challenge%20comprises%20two%20tracks%3A%0A%28i%29%20Efficient%20Video%20Quality%20Assessment%20%28KVQ%29%2C%20and%20%28ii%29%20Diffusion-based%20Image%0ASuper-Resolution%20%28KwaiSR%29.%20Track%201%20aims%20to%20advance%20the%20development%20of%0Alightweight%20and%20efficient%20video%20quality%20assessment%20%28VQA%29%20models%2C%20with%20an%0Aemphasis%20on%20eliminating%20reliance%20on%20model%20ensembles%2C%20redundant%20weights%2C%20and%0Aother%20computationally%20expensive%20components%20in%20the%20previous%20IQA/VQA%0Acompetitions.%20Track%202%20introduces%20a%20new%20short-form%20UGC%20dataset%20tailored%20for%0Asingle%20image%20super-resolution%2C%20i.e.%2C%20the%20KwaiSR%20dataset.%20It%20consists%20of%201%2C800%0Asynthetically%20generated%20S-UGC%20image%20pairs%20and%201%2C900%20real-world%20S-UGC%20images%2C%0Awhich%20are%20split%20into%20training%2C%20validation%2C%20and%20test%20sets%20using%20a%20ratio%20of%0A8%3A1%3A1.%20The%20primary%20objective%20of%20the%20challenge%20is%20to%20drive%20research%20that%0Abenefits%20the%20user%20experience%20of%20short-form%20UGC%20platforms%20such%20as%20Kwai%20and%0ATikTok.%20This%20challenge%20attracted%20266%20participants%20and%20received%2018%20valid%20final%0Asubmissions%20with%20corresponding%20fact%20sheets%2C%20significantly%20contributing%20to%20the%0Aprogress%20of%20short-form%20UGC%20VQA%20and%20image%20superresolution.%20The%20project%20is%0Apublicly%20available%20at%20https%3A//github.com/lixinustc/KVQE-%0AChallengeCVPR-NTIRE2025.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13131v1&entry.124074799=Read"},
{"title": "Tree-NeRV: A Tree-Structured Neural Representation for Efficient\n  Non-Uniform Video Encoding", "author": "Jiancheng Zhao and Yifan Zhan and Qingtian Zhu and Mingze Ma and Muyao Niu and Zunian Wan and Xiang Ji and Yinqiang Zheng", "abstract": "  Implicit Neural Representations for Videos (NeRV) have emerged as a powerful\nparadigm for video representation, enabling direct mappings from frame indices\nto video frames. However, existing NeRV-based methods do not fully exploit\ntemporal redundancy, as they rely on uniform sampling along the temporal axis,\nleading to suboptimal rate-distortion (RD) performance. To address this\nlimitation, we propose Tree-NeRV, a novel tree-structured feature\nrepresentation for efficient and adaptive video encoding. Unlike conventional\napproaches, Tree-NeRV organizes feature representations within a Binary Search\nTree (BST), enabling non-uniform sampling along the temporal axis.\nAdditionally, we introduce an optimization-driven sampling strategy,\ndynamically allocating higher sampling density to regions with greater temporal\nvariation. Extensive experiments demonstrate that Tree-NeRV achieves superior\ncompression efficiency and reconstruction quality, outperforming prior uniform\nsampling-based methods. Code will be released.\n", "link": "http://arxiv.org/abs/2504.12899v1", "date": "2025-04-17", "relevancy": 2.1524, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.578}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5267}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tree-NeRV%3A%20A%20Tree-Structured%20Neural%20Representation%20for%20Efficient%0A%20%20Non-Uniform%20Video%20Encoding&body=Title%3A%20Tree-NeRV%3A%20A%20Tree-Structured%20Neural%20Representation%20for%20Efficient%0A%20%20Non-Uniform%20Video%20Encoding%0AAuthor%3A%20Jiancheng%20Zhao%20and%20Yifan%20Zhan%20and%20Qingtian%20Zhu%20and%20Mingze%20Ma%20and%20Muyao%20Niu%20and%20Zunian%20Wan%20and%20Xiang%20Ji%20and%20Yinqiang%20Zheng%0AAbstract%3A%20%20%20Implicit%20Neural%20Representations%20for%20Videos%20%28NeRV%29%20have%20emerged%20as%20a%20powerful%0Aparadigm%20for%20video%20representation%2C%20enabling%20direct%20mappings%20from%20frame%20indices%0Ato%20video%20frames.%20However%2C%20existing%20NeRV-based%20methods%20do%20not%20fully%20exploit%0Atemporal%20redundancy%2C%20as%20they%20rely%20on%20uniform%20sampling%20along%20the%20temporal%20axis%2C%0Aleading%20to%20suboptimal%20rate-distortion%20%28RD%29%20performance.%20To%20address%20this%0Alimitation%2C%20we%20propose%20Tree-NeRV%2C%20a%20novel%20tree-structured%20feature%0Arepresentation%20for%20efficient%20and%20adaptive%20video%20encoding.%20Unlike%20conventional%0Aapproaches%2C%20Tree-NeRV%20organizes%20feature%20representations%20within%20a%20Binary%20Search%0ATree%20%28BST%29%2C%20enabling%20non-uniform%20sampling%20along%20the%20temporal%20axis.%0AAdditionally%2C%20we%20introduce%20an%20optimization-driven%20sampling%20strategy%2C%0Adynamically%20allocating%20higher%20sampling%20density%20to%20regions%20with%20greater%20temporal%0Avariation.%20Extensive%20experiments%20demonstrate%20that%20Tree-NeRV%20achieves%20superior%0Acompression%20efficiency%20and%20reconstruction%20quality%2C%20outperforming%20prior%20uniform%0Asampling-based%20methods.%20Code%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12899v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTree-NeRV%253A%2520A%2520Tree-Structured%2520Neural%2520Representation%2520for%2520Efficient%250A%2520%2520Non-Uniform%2520Video%2520Encoding%26entry.906535625%3DJiancheng%2520Zhao%2520and%2520Yifan%2520Zhan%2520and%2520Qingtian%2520Zhu%2520and%2520Mingze%2520Ma%2520and%2520Muyao%2520Niu%2520and%2520Zunian%2520Wan%2520and%2520Xiang%2520Ji%2520and%2520Yinqiang%2520Zheng%26entry.1292438233%3D%2520%2520Implicit%2520Neural%2520Representations%2520for%2520Videos%2520%2528NeRV%2529%2520have%2520emerged%2520as%2520a%2520powerful%250Aparadigm%2520for%2520video%2520representation%252C%2520enabling%2520direct%2520mappings%2520from%2520frame%2520indices%250Ato%2520video%2520frames.%2520However%252C%2520existing%2520NeRV-based%2520methods%2520do%2520not%2520fully%2520exploit%250Atemporal%2520redundancy%252C%2520as%2520they%2520rely%2520on%2520uniform%2520sampling%2520along%2520the%2520temporal%2520axis%252C%250Aleading%2520to%2520suboptimal%2520rate-distortion%2520%2528RD%2529%2520performance.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520propose%2520Tree-NeRV%252C%2520a%2520novel%2520tree-structured%2520feature%250Arepresentation%2520for%2520efficient%2520and%2520adaptive%2520video%2520encoding.%2520Unlike%2520conventional%250Aapproaches%252C%2520Tree-NeRV%2520organizes%2520feature%2520representations%2520within%2520a%2520Binary%2520Search%250ATree%2520%2528BST%2529%252C%2520enabling%2520non-uniform%2520sampling%2520along%2520the%2520temporal%2520axis.%250AAdditionally%252C%2520we%2520introduce%2520an%2520optimization-driven%2520sampling%2520strategy%252C%250Adynamically%2520allocating%2520higher%2520sampling%2520density%2520to%2520regions%2520with%2520greater%2520temporal%250Avariation.%2520Extensive%2520experiments%2520demonstrate%2520that%2520Tree-NeRV%2520achieves%2520superior%250Acompression%2520efficiency%2520and%2520reconstruction%2520quality%252C%2520outperforming%2520prior%2520uniform%250Asampling-based%2520methods.%2520Code%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12899v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tree-NeRV%3A%20A%20Tree-Structured%20Neural%20Representation%20for%20Efficient%0A%20%20Non-Uniform%20Video%20Encoding&entry.906535625=Jiancheng%20Zhao%20and%20Yifan%20Zhan%20and%20Qingtian%20Zhu%20and%20Mingze%20Ma%20and%20Muyao%20Niu%20and%20Zunian%20Wan%20and%20Xiang%20Ji%20and%20Yinqiang%20Zheng&entry.1292438233=%20%20Implicit%20Neural%20Representations%20for%20Videos%20%28NeRV%29%20have%20emerged%20as%20a%20powerful%0Aparadigm%20for%20video%20representation%2C%20enabling%20direct%20mappings%20from%20frame%20indices%0Ato%20video%20frames.%20However%2C%20existing%20NeRV-based%20methods%20do%20not%20fully%20exploit%0Atemporal%20redundancy%2C%20as%20they%20rely%20on%20uniform%20sampling%20along%20the%20temporal%20axis%2C%0Aleading%20to%20suboptimal%20rate-distortion%20%28RD%29%20performance.%20To%20address%20this%0Alimitation%2C%20we%20propose%20Tree-NeRV%2C%20a%20novel%20tree-structured%20feature%0Arepresentation%20for%20efficient%20and%20adaptive%20video%20encoding.%20Unlike%20conventional%0Aapproaches%2C%20Tree-NeRV%20organizes%20feature%20representations%20within%20a%20Binary%20Search%0ATree%20%28BST%29%2C%20enabling%20non-uniform%20sampling%20along%20the%20temporal%20axis.%0AAdditionally%2C%20we%20introduce%20an%20optimization-driven%20sampling%20strategy%2C%0Adynamically%20allocating%20higher%20sampling%20density%20to%20regions%20with%20greater%20temporal%0Avariation.%20Extensive%20experiments%20demonstrate%20that%20Tree-NeRV%20achieves%20superior%0Acompression%20efficiency%20and%20reconstruction%20quality%2C%20outperforming%20prior%20uniform%0Asampling-based%20methods.%20Code%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12899v1&entry.124074799=Read"},
{"title": "Imperative MPC: An End-to-End Self-Supervised Learning with\n  Differentiable MPC for UAV Attitude Control", "author": "Haonan He and Yuheng Qiu and Junyi Geng", "abstract": "  Modeling and control of nonlinear dynamics are critical in robotics,\nespecially in scenarios with unpredictable external influences and complex\ndynamics. Traditional cascaded modular control pipelines often yield suboptimal\nperformance due to conservative assumptions and tedious parameter tuning. Pure\ndata-driven approaches promise robust performance but suffer from low sample\nefficiency, sim-to-real gaps, and reliance on extensive datasets. Hybrid\nmethods combining learning-based and traditional model-based control in an\nend-to-end manner offer a promising alternative. This work presents a\nself-supervised learning framework combining learning-based inertial odometry\n(IO) module and differentiable model predictive control (d-MPC) for Unmanned\nAerial Vehicle (UAV) attitude control. The IO denoises raw IMU measurements and\npredicts UAV attitudes, which are then optimized by MPC for control actions in\na bi-level optimization (BLO) setup, where the inner MPC optimizes control\nactions and the upper level minimizes discrepancy between real-world and\npredicted performance. The framework is thus end-to-end and can be trained in a\nself-supervised manner. This approach combines the strength of learning-based\nperception with the interpretable model-based control. Results show the\neffectiveness even under strong wind. It can simultaneously enhance both the\nMPC parameter learning and IMU prediction performance.\n", "link": "http://arxiv.org/abs/2504.13088v1", "date": "2025-04-17", "relevancy": 2.1511, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5654}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5418}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imperative%20MPC%3A%20An%20End-to-End%20Self-Supervised%20Learning%20with%0A%20%20Differentiable%20MPC%20for%20UAV%20Attitude%20Control&body=Title%3A%20Imperative%20MPC%3A%20An%20End-to-End%20Self-Supervised%20Learning%20with%0A%20%20Differentiable%20MPC%20for%20UAV%20Attitude%20Control%0AAuthor%3A%20Haonan%20He%20and%20Yuheng%20Qiu%20and%20Junyi%20Geng%0AAbstract%3A%20%20%20Modeling%20and%20control%20of%20nonlinear%20dynamics%20are%20critical%20in%20robotics%2C%0Aespecially%20in%20scenarios%20with%20unpredictable%20external%20influences%20and%20complex%0Adynamics.%20Traditional%20cascaded%20modular%20control%20pipelines%20often%20yield%20suboptimal%0Aperformance%20due%20to%20conservative%20assumptions%20and%20tedious%20parameter%20tuning.%20Pure%0Adata-driven%20approaches%20promise%20robust%20performance%20but%20suffer%20from%20low%20sample%0Aefficiency%2C%20sim-to-real%20gaps%2C%20and%20reliance%20on%20extensive%20datasets.%20Hybrid%0Amethods%20combining%20learning-based%20and%20traditional%20model-based%20control%20in%20an%0Aend-to-end%20manner%20offer%20a%20promising%20alternative.%20This%20work%20presents%20a%0Aself-supervised%20learning%20framework%20combining%20learning-based%20inertial%20odometry%0A%28IO%29%20module%20and%20differentiable%20model%20predictive%20control%20%28d-MPC%29%20for%20Unmanned%0AAerial%20Vehicle%20%28UAV%29%20attitude%20control.%20The%20IO%20denoises%20raw%20IMU%20measurements%20and%0Apredicts%20UAV%20attitudes%2C%20which%20are%20then%20optimized%20by%20MPC%20for%20control%20actions%20in%0Aa%20bi-level%20optimization%20%28BLO%29%20setup%2C%20where%20the%20inner%20MPC%20optimizes%20control%0Aactions%20and%20the%20upper%20level%20minimizes%20discrepancy%20between%20real-world%20and%0Apredicted%20performance.%20The%20framework%20is%20thus%20end-to-end%20and%20can%20be%20trained%20in%20a%0Aself-supervised%20manner.%20This%20approach%20combines%20the%20strength%20of%20learning-based%0Aperception%20with%20the%20interpretable%20model-based%20control.%20Results%20show%20the%0Aeffectiveness%20even%20under%20strong%20wind.%20It%20can%20simultaneously%20enhance%20both%20the%0AMPC%20parameter%20learning%20and%20IMU%20prediction%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13088v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImperative%2520MPC%253A%2520An%2520End-to-End%2520Self-Supervised%2520Learning%2520with%250A%2520%2520Differentiable%2520MPC%2520for%2520UAV%2520Attitude%2520Control%26entry.906535625%3DHaonan%2520He%2520and%2520Yuheng%2520Qiu%2520and%2520Junyi%2520Geng%26entry.1292438233%3D%2520%2520Modeling%2520and%2520control%2520of%2520nonlinear%2520dynamics%2520are%2520critical%2520in%2520robotics%252C%250Aespecially%2520in%2520scenarios%2520with%2520unpredictable%2520external%2520influences%2520and%2520complex%250Adynamics.%2520Traditional%2520cascaded%2520modular%2520control%2520pipelines%2520often%2520yield%2520suboptimal%250Aperformance%2520due%2520to%2520conservative%2520assumptions%2520and%2520tedious%2520parameter%2520tuning.%2520Pure%250Adata-driven%2520approaches%2520promise%2520robust%2520performance%2520but%2520suffer%2520from%2520low%2520sample%250Aefficiency%252C%2520sim-to-real%2520gaps%252C%2520and%2520reliance%2520on%2520extensive%2520datasets.%2520Hybrid%250Amethods%2520combining%2520learning-based%2520and%2520traditional%2520model-based%2520control%2520in%2520an%250Aend-to-end%2520manner%2520offer%2520a%2520promising%2520alternative.%2520This%2520work%2520presents%2520a%250Aself-supervised%2520learning%2520framework%2520combining%2520learning-based%2520inertial%2520odometry%250A%2528IO%2529%2520module%2520and%2520differentiable%2520model%2520predictive%2520control%2520%2528d-MPC%2529%2520for%2520Unmanned%250AAerial%2520Vehicle%2520%2528UAV%2529%2520attitude%2520control.%2520The%2520IO%2520denoises%2520raw%2520IMU%2520measurements%2520and%250Apredicts%2520UAV%2520attitudes%252C%2520which%2520are%2520then%2520optimized%2520by%2520MPC%2520for%2520control%2520actions%2520in%250Aa%2520bi-level%2520optimization%2520%2528BLO%2529%2520setup%252C%2520where%2520the%2520inner%2520MPC%2520optimizes%2520control%250Aactions%2520and%2520the%2520upper%2520level%2520minimizes%2520discrepancy%2520between%2520real-world%2520and%250Apredicted%2520performance.%2520The%2520framework%2520is%2520thus%2520end-to-end%2520and%2520can%2520be%2520trained%2520in%2520a%250Aself-supervised%2520manner.%2520This%2520approach%2520combines%2520the%2520strength%2520of%2520learning-based%250Aperception%2520with%2520the%2520interpretable%2520model-based%2520control.%2520Results%2520show%2520the%250Aeffectiveness%2520even%2520under%2520strong%2520wind.%2520It%2520can%2520simultaneously%2520enhance%2520both%2520the%250AMPC%2520parameter%2520learning%2520and%2520IMU%2520prediction%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13088v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imperative%20MPC%3A%20An%20End-to-End%20Self-Supervised%20Learning%20with%0A%20%20Differentiable%20MPC%20for%20UAV%20Attitude%20Control&entry.906535625=Haonan%20He%20and%20Yuheng%20Qiu%20and%20Junyi%20Geng&entry.1292438233=%20%20Modeling%20and%20control%20of%20nonlinear%20dynamics%20are%20critical%20in%20robotics%2C%0Aespecially%20in%20scenarios%20with%20unpredictable%20external%20influences%20and%20complex%0Adynamics.%20Traditional%20cascaded%20modular%20control%20pipelines%20often%20yield%20suboptimal%0Aperformance%20due%20to%20conservative%20assumptions%20and%20tedious%20parameter%20tuning.%20Pure%0Adata-driven%20approaches%20promise%20robust%20performance%20but%20suffer%20from%20low%20sample%0Aefficiency%2C%20sim-to-real%20gaps%2C%20and%20reliance%20on%20extensive%20datasets.%20Hybrid%0Amethods%20combining%20learning-based%20and%20traditional%20model-based%20control%20in%20an%0Aend-to-end%20manner%20offer%20a%20promising%20alternative.%20This%20work%20presents%20a%0Aself-supervised%20learning%20framework%20combining%20learning-based%20inertial%20odometry%0A%28IO%29%20module%20and%20differentiable%20model%20predictive%20control%20%28d-MPC%29%20for%20Unmanned%0AAerial%20Vehicle%20%28UAV%29%20attitude%20control.%20The%20IO%20denoises%20raw%20IMU%20measurements%20and%0Apredicts%20UAV%20attitudes%2C%20which%20are%20then%20optimized%20by%20MPC%20for%20control%20actions%20in%0Aa%20bi-level%20optimization%20%28BLO%29%20setup%2C%20where%20the%20inner%20MPC%20optimizes%20control%0Aactions%20and%20the%20upper%20level%20minimizes%20discrepancy%20between%20real-world%20and%0Apredicted%20performance.%20The%20framework%20is%20thus%20end-to-end%20and%20can%20be%20trained%20in%20a%0Aself-supervised%20manner.%20This%20approach%20combines%20the%20strength%20of%20learning-based%0Aperception%20with%20the%20interpretable%20model-based%20control.%20Results%20show%20the%0Aeffectiveness%20even%20under%20strong%20wind.%20It%20can%20simultaneously%20enhance%20both%20the%0AMPC%20parameter%20learning%20and%20IMU%20prediction%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13088v1&entry.124074799=Read"},
{"title": "Effective Dual-Region Augmentation for Reduced Reliance on Large Amounts\n  of Labeled Data", "author": "Prasanna Reddy Pulakurthi and Majid Rabbani and Celso M. de Melo and Sohail A. Dianat and Raghuveer M. Rao", "abstract": "  This paper introduces a novel dual-region augmentation approach designed to\nreduce reliance on large-scale labeled datasets while improving model\nrobustness and adaptability across diverse computer vision tasks, including\nsource-free domain adaptation (SFDA) and person re-identification (ReID). Our\nmethod performs targeted data transformations by applying random noise\nperturbations to foreground objects and spatially shuffling background patches.\nThis effectively increases the diversity of the training data, improving model\nrobustness and generalization. Evaluations on the PACS dataset for SFDA\ndemonstrate that our augmentation strategy consistently outperforms existing\nmethods, achieving significant accuracy improvements in both single-target and\nmulti-target adaptation settings. By augmenting training data through\nstructured transformations, our method enables model generalization across\ndomains, providing a scalable solution for reducing reliance on manually\nannotated datasets. Furthermore, experiments on Market-1501 and DukeMTMC-reID\ndatasets validate the effectiveness of our approach for person ReID, surpassing\ntraditional augmentation techniques.\n", "link": "http://arxiv.org/abs/2504.13077v1", "date": "2025-04-17", "relevancy": 2.1509, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5448}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5369}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Effective%20Dual-Region%20Augmentation%20for%20Reduced%20Reliance%20on%20Large%20Amounts%0A%20%20of%20Labeled%20Data&body=Title%3A%20Effective%20Dual-Region%20Augmentation%20for%20Reduced%20Reliance%20on%20Large%20Amounts%0A%20%20of%20Labeled%20Data%0AAuthor%3A%20Prasanna%20Reddy%20Pulakurthi%20and%20Majid%20Rabbani%20and%20Celso%20M.%20de%20Melo%20and%20Sohail%20A.%20Dianat%20and%20Raghuveer%20M.%20Rao%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20dual-region%20augmentation%20approach%20designed%20to%0Areduce%20reliance%20on%20large-scale%20labeled%20datasets%20while%20improving%20model%0Arobustness%20and%20adaptability%20across%20diverse%20computer%20vision%20tasks%2C%20including%0Asource-free%20domain%20adaptation%20%28SFDA%29%20and%20person%20re-identification%20%28ReID%29.%20Our%0Amethod%20performs%20targeted%20data%20transformations%20by%20applying%20random%20noise%0Aperturbations%20to%20foreground%20objects%20and%20spatially%20shuffling%20background%20patches.%0AThis%20effectively%20increases%20the%20diversity%20of%20the%20training%20data%2C%20improving%20model%0Arobustness%20and%20generalization.%20Evaluations%20on%20the%20PACS%20dataset%20for%20SFDA%0Ademonstrate%20that%20our%20augmentation%20strategy%20consistently%20outperforms%20existing%0Amethods%2C%20achieving%20significant%20accuracy%20improvements%20in%20both%20single-target%20and%0Amulti-target%20adaptation%20settings.%20By%20augmenting%20training%20data%20through%0Astructured%20transformations%2C%20our%20method%20enables%20model%20generalization%20across%0Adomains%2C%20providing%20a%20scalable%20solution%20for%20reducing%20reliance%20on%20manually%0Aannotated%20datasets.%20Furthermore%2C%20experiments%20on%20Market-1501%20and%20DukeMTMC-reID%0Adatasets%20validate%20the%20effectiveness%20of%20our%20approach%20for%20person%20ReID%2C%20surpassing%0Atraditional%20augmentation%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13077v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEffective%2520Dual-Region%2520Augmentation%2520for%2520Reduced%2520Reliance%2520on%2520Large%2520Amounts%250A%2520%2520of%2520Labeled%2520Data%26entry.906535625%3DPrasanna%2520Reddy%2520Pulakurthi%2520and%2520Majid%2520Rabbani%2520and%2520Celso%2520M.%2520de%2520Melo%2520and%2520Sohail%2520A.%2520Dianat%2520and%2520Raghuveer%2520M.%2520Rao%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520dual-region%2520augmentation%2520approach%2520designed%2520to%250Areduce%2520reliance%2520on%2520large-scale%2520labeled%2520datasets%2520while%2520improving%2520model%250Arobustness%2520and%2520adaptability%2520across%2520diverse%2520computer%2520vision%2520tasks%252C%2520including%250Asource-free%2520domain%2520adaptation%2520%2528SFDA%2529%2520and%2520person%2520re-identification%2520%2528ReID%2529.%2520Our%250Amethod%2520performs%2520targeted%2520data%2520transformations%2520by%2520applying%2520random%2520noise%250Aperturbations%2520to%2520foreground%2520objects%2520and%2520spatially%2520shuffling%2520background%2520patches.%250AThis%2520effectively%2520increases%2520the%2520diversity%2520of%2520the%2520training%2520data%252C%2520improving%2520model%250Arobustness%2520and%2520generalization.%2520Evaluations%2520on%2520the%2520PACS%2520dataset%2520for%2520SFDA%250Ademonstrate%2520that%2520our%2520augmentation%2520strategy%2520consistently%2520outperforms%2520existing%250Amethods%252C%2520achieving%2520significant%2520accuracy%2520improvements%2520in%2520both%2520single-target%2520and%250Amulti-target%2520adaptation%2520settings.%2520By%2520augmenting%2520training%2520data%2520through%250Astructured%2520transformations%252C%2520our%2520method%2520enables%2520model%2520generalization%2520across%250Adomains%252C%2520providing%2520a%2520scalable%2520solution%2520for%2520reducing%2520reliance%2520on%2520manually%250Aannotated%2520datasets.%2520Furthermore%252C%2520experiments%2520on%2520Market-1501%2520and%2520DukeMTMC-reID%250Adatasets%2520validate%2520the%2520effectiveness%2520of%2520our%2520approach%2520for%2520person%2520ReID%252C%2520surpassing%250Atraditional%2520augmentation%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13077v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Effective%20Dual-Region%20Augmentation%20for%20Reduced%20Reliance%20on%20Large%20Amounts%0A%20%20of%20Labeled%20Data&entry.906535625=Prasanna%20Reddy%20Pulakurthi%20and%20Majid%20Rabbani%20and%20Celso%20M.%20de%20Melo%20and%20Sohail%20A.%20Dianat%20and%20Raghuveer%20M.%20Rao&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20dual-region%20augmentation%20approach%20designed%20to%0Areduce%20reliance%20on%20large-scale%20labeled%20datasets%20while%20improving%20model%0Arobustness%20and%20adaptability%20across%20diverse%20computer%20vision%20tasks%2C%20including%0Asource-free%20domain%20adaptation%20%28SFDA%29%20and%20person%20re-identification%20%28ReID%29.%20Our%0Amethod%20performs%20targeted%20data%20transformations%20by%20applying%20random%20noise%0Aperturbations%20to%20foreground%20objects%20and%20spatially%20shuffling%20background%20patches.%0AThis%20effectively%20increases%20the%20diversity%20of%20the%20training%20data%2C%20improving%20model%0Arobustness%20and%20generalization.%20Evaluations%20on%20the%20PACS%20dataset%20for%20SFDA%0Ademonstrate%20that%20our%20augmentation%20strategy%20consistently%20outperforms%20existing%0Amethods%2C%20achieving%20significant%20accuracy%20improvements%20in%20both%20single-target%20and%0Amulti-target%20adaptation%20settings.%20By%20augmenting%20training%20data%20through%0Astructured%20transformations%2C%20our%20method%20enables%20model%20generalization%20across%0Adomains%2C%20providing%20a%20scalable%20solution%20for%20reducing%20reliance%20on%20manually%0Aannotated%20datasets.%20Furthermore%2C%20experiments%20on%20Market-1501%20and%20DukeMTMC-reID%0Adatasets%20validate%20the%20effectiveness%20of%20our%20approach%20for%20person%20ReID%2C%20surpassing%0Atraditional%20augmentation%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13077v1&entry.124074799=Read"},
{"title": "ALT: A Python Package for Lightweight Feature Representation in Time\n  Series Classification", "author": "Bal\u00e1zs P. Halmos and Bal\u00e1zs Haj\u00f3s and Vince \u00c1. Moln\u00e1r and Marcell T. Kurbucz and Antal Jakov\u00e1c", "abstract": "  We introduce ALT, an open-source Python package created for efficient and\naccurate time series classification (TSC). The package implements the adaptive\nlaw-based transformation (ALT) algorithm, which transforms raw time series data\ninto a linearly separable feature space using variable-length shifted time\nwindows. This adaptive approach enhances its predecessor, the linear law-based\ntransformation (LLT), by effectively capturing patterns of varying temporal\nscales. The software is implemented for scalability, interpretability, and ease\nof use, achieving state-of-the-art performance with minimal computational\noverhead. Extensive benchmarking on real-world datasets demonstrates the\nutility of ALT for diverse TSC tasks in physics and related domains.\n", "link": "http://arxiv.org/abs/2504.12841v1", "date": "2025-04-17", "relevancy": 2.1422, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4604}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4152}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ALT%3A%20A%20Python%20Package%20for%20Lightweight%20Feature%20Representation%20in%20Time%0A%20%20Series%20Classification&body=Title%3A%20ALT%3A%20A%20Python%20Package%20for%20Lightweight%20Feature%20Representation%20in%20Time%0A%20%20Series%20Classification%0AAuthor%3A%20Bal%C3%A1zs%20P.%20Halmos%20and%20Bal%C3%A1zs%20Haj%C3%B3s%20and%20Vince%20%C3%81.%20Moln%C3%A1r%20and%20Marcell%20T.%20Kurbucz%20and%20Antal%20Jakov%C3%A1c%0AAbstract%3A%20%20%20We%20introduce%20ALT%2C%20an%20open-source%20Python%20package%20created%20for%20efficient%20and%0Aaccurate%20time%20series%20classification%20%28TSC%29.%20The%20package%20implements%20the%20adaptive%0Alaw-based%20transformation%20%28ALT%29%20algorithm%2C%20which%20transforms%20raw%20time%20series%20data%0Ainto%20a%20linearly%20separable%20feature%20space%20using%20variable-length%20shifted%20time%0Awindows.%20This%20adaptive%20approach%20enhances%20its%20predecessor%2C%20the%20linear%20law-based%0Atransformation%20%28LLT%29%2C%20by%20effectively%20capturing%20patterns%20of%20varying%20temporal%0Ascales.%20The%20software%20is%20implemented%20for%20scalability%2C%20interpretability%2C%20and%20ease%0Aof%20use%2C%20achieving%20state-of-the-art%20performance%20with%20minimal%20computational%0Aoverhead.%20Extensive%20benchmarking%20on%20real-world%20datasets%20demonstrates%20the%0Autility%20of%20ALT%20for%20diverse%20TSC%20tasks%20in%20physics%20and%20related%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12841v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DALT%253A%2520A%2520Python%2520Package%2520for%2520Lightweight%2520Feature%2520Representation%2520in%2520Time%250A%2520%2520Series%2520Classification%26entry.906535625%3DBal%25C3%25A1zs%2520P.%2520Halmos%2520and%2520Bal%25C3%25A1zs%2520Haj%25C3%25B3s%2520and%2520Vince%2520%25C3%2581.%2520Moln%25C3%25A1r%2520and%2520Marcell%2520T.%2520Kurbucz%2520and%2520Antal%2520Jakov%25C3%25A1c%26entry.1292438233%3D%2520%2520We%2520introduce%2520ALT%252C%2520an%2520open-source%2520Python%2520package%2520created%2520for%2520efficient%2520and%250Aaccurate%2520time%2520series%2520classification%2520%2528TSC%2529.%2520The%2520package%2520implements%2520the%2520adaptive%250Alaw-based%2520transformation%2520%2528ALT%2529%2520algorithm%252C%2520which%2520transforms%2520raw%2520time%2520series%2520data%250Ainto%2520a%2520linearly%2520separable%2520feature%2520space%2520using%2520variable-length%2520shifted%2520time%250Awindows.%2520This%2520adaptive%2520approach%2520enhances%2520its%2520predecessor%252C%2520the%2520linear%2520law-based%250Atransformation%2520%2528LLT%2529%252C%2520by%2520effectively%2520capturing%2520patterns%2520of%2520varying%2520temporal%250Ascales.%2520The%2520software%2520is%2520implemented%2520for%2520scalability%252C%2520interpretability%252C%2520and%2520ease%250Aof%2520use%252C%2520achieving%2520state-of-the-art%2520performance%2520with%2520minimal%2520computational%250Aoverhead.%2520Extensive%2520benchmarking%2520on%2520real-world%2520datasets%2520demonstrates%2520the%250Autility%2520of%2520ALT%2520for%2520diverse%2520TSC%2520tasks%2520in%2520physics%2520and%2520related%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12841v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ALT%3A%20A%20Python%20Package%20for%20Lightweight%20Feature%20Representation%20in%20Time%0A%20%20Series%20Classification&entry.906535625=Bal%C3%A1zs%20P.%20Halmos%20and%20Bal%C3%A1zs%20Haj%C3%B3s%20and%20Vince%20%C3%81.%20Moln%C3%A1r%20and%20Marcell%20T.%20Kurbucz%20and%20Antal%20Jakov%C3%A1c&entry.1292438233=%20%20We%20introduce%20ALT%2C%20an%20open-source%20Python%20package%20created%20for%20efficient%20and%0Aaccurate%20time%20series%20classification%20%28TSC%29.%20The%20package%20implements%20the%20adaptive%0Alaw-based%20transformation%20%28ALT%29%20algorithm%2C%20which%20transforms%20raw%20time%20series%20data%0Ainto%20a%20linearly%20separable%20feature%20space%20using%20variable-length%20shifted%20time%0Awindows.%20This%20adaptive%20approach%20enhances%20its%20predecessor%2C%20the%20linear%20law-based%0Atransformation%20%28LLT%29%2C%20by%20effectively%20capturing%20patterns%20of%20varying%20temporal%0Ascales.%20The%20software%20is%20implemented%20for%20scalability%2C%20interpretability%2C%20and%20ease%0Aof%20use%2C%20achieving%20state-of-the-art%20performance%20with%20minimal%20computational%0Aoverhead.%20Extensive%20benchmarking%20on%20real-world%20datasets%20demonstrates%20the%0Autility%20of%20ALT%20for%20diverse%20TSC%20tasks%20in%20physics%20and%20related%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12841v1&entry.124074799=Read"},
{"title": "Learn2Decompose: Learning Problem Decomposition for Efficient Sequential\n  Multi-object Manipulation Planning", "author": "Yan Zhang and Teng Xue and Amirreza Razmjoo and Sylvain Calinon", "abstract": "  We present a Reactive Task and Motion Planning (TAMP) approach for efficient\nsequential multi-object manipulation in dynamic environments. Conventional TAMP\nsolvers experience an exponential increase in planning time as the planning\nhorizon and number of objects grow, limiting their applicability in real-world\nscenarios. To address this, we propose learning problem decomposition from\ndemonstrations to accelerate TAMP solvers. Our approach consists of three key\ncomponents: goal decomposition learning, temporal distance learning, and object\nreduction. Goal decomposition identifies the necessary sequences of states that\nthe system must pass through before reaching the final goal, treating them as\nsubgoal sequences. Temporal distance learning predicts the temporal distance\nbetween two states, enabling the system to identify the closest subgoal from a\ndisturbed state. Object reduction minimizes the set of active objects\nconsidered during replanning, further improving efficiency. We evaluate our\napproach on three benchmarks, demonstrating its effectiveness in improving\nreplanning efficiency for sequential multi-object manipulation tasks in dynamic\nenvironments.\n", "link": "http://arxiv.org/abs/2408.06843v2", "date": "2025-04-17", "relevancy": 2.136, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5851}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5396}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learn2Decompose%3A%20Learning%20Problem%20Decomposition%20for%20Efficient%20Sequential%0A%20%20Multi-object%20Manipulation%20Planning&body=Title%3A%20Learn2Decompose%3A%20Learning%20Problem%20Decomposition%20for%20Efficient%20Sequential%0A%20%20Multi-object%20Manipulation%20Planning%0AAuthor%3A%20Yan%20Zhang%20and%20Teng%20Xue%20and%20Amirreza%20Razmjoo%20and%20Sylvain%20Calinon%0AAbstract%3A%20%20%20We%20present%20a%20Reactive%20Task%20and%20Motion%20Planning%20%28TAMP%29%20approach%20for%20efficient%0Asequential%20multi-object%20manipulation%20in%20dynamic%20environments.%20Conventional%20TAMP%0Asolvers%20experience%20an%20exponential%20increase%20in%20planning%20time%20as%20the%20planning%0Ahorizon%20and%20number%20of%20objects%20grow%2C%20limiting%20their%20applicability%20in%20real-world%0Ascenarios.%20To%20address%20this%2C%20we%20propose%20learning%20problem%20decomposition%20from%0Ademonstrations%20to%20accelerate%20TAMP%20solvers.%20Our%20approach%20consists%20of%20three%20key%0Acomponents%3A%20goal%20decomposition%20learning%2C%20temporal%20distance%20learning%2C%20and%20object%0Areduction.%20Goal%20decomposition%20identifies%20the%20necessary%20sequences%20of%20states%20that%0Athe%20system%20must%20pass%20through%20before%20reaching%20the%20final%20goal%2C%20treating%20them%20as%0Asubgoal%20sequences.%20Temporal%20distance%20learning%20predicts%20the%20temporal%20distance%0Abetween%20two%20states%2C%20enabling%20the%20system%20to%20identify%20the%20closest%20subgoal%20from%20a%0Adisturbed%20state.%20Object%20reduction%20minimizes%20the%20set%20of%20active%20objects%0Aconsidered%20during%20replanning%2C%20further%20improving%20efficiency.%20We%20evaluate%20our%0Aapproach%20on%20three%20benchmarks%2C%20demonstrating%20its%20effectiveness%20in%20improving%0Areplanning%20efficiency%20for%20sequential%20multi-object%20manipulation%20tasks%20in%20dynamic%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06843v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearn2Decompose%253A%2520Learning%2520Problem%2520Decomposition%2520for%2520Efficient%2520Sequential%250A%2520%2520Multi-object%2520Manipulation%2520Planning%26entry.906535625%3DYan%2520Zhang%2520and%2520Teng%2520Xue%2520and%2520Amirreza%2520Razmjoo%2520and%2520Sylvain%2520Calinon%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520Reactive%2520Task%2520and%2520Motion%2520Planning%2520%2528TAMP%2529%2520approach%2520for%2520efficient%250Asequential%2520multi-object%2520manipulation%2520in%2520dynamic%2520environments.%2520Conventional%2520TAMP%250Asolvers%2520experience%2520an%2520exponential%2520increase%2520in%2520planning%2520time%2520as%2520the%2520planning%250Ahorizon%2520and%2520number%2520of%2520objects%2520grow%252C%2520limiting%2520their%2520applicability%2520in%2520real-world%250Ascenarios.%2520To%2520address%2520this%252C%2520we%2520propose%2520learning%2520problem%2520decomposition%2520from%250Ademonstrations%2520to%2520accelerate%2520TAMP%2520solvers.%2520Our%2520approach%2520consists%2520of%2520three%2520key%250Acomponents%253A%2520goal%2520decomposition%2520learning%252C%2520temporal%2520distance%2520learning%252C%2520and%2520object%250Areduction.%2520Goal%2520decomposition%2520identifies%2520the%2520necessary%2520sequences%2520of%2520states%2520that%250Athe%2520system%2520must%2520pass%2520through%2520before%2520reaching%2520the%2520final%2520goal%252C%2520treating%2520them%2520as%250Asubgoal%2520sequences.%2520Temporal%2520distance%2520learning%2520predicts%2520the%2520temporal%2520distance%250Abetween%2520two%2520states%252C%2520enabling%2520the%2520system%2520to%2520identify%2520the%2520closest%2520subgoal%2520from%2520a%250Adisturbed%2520state.%2520Object%2520reduction%2520minimizes%2520the%2520set%2520of%2520active%2520objects%250Aconsidered%2520during%2520replanning%252C%2520further%2520improving%2520efficiency.%2520We%2520evaluate%2520our%250Aapproach%2520on%2520three%2520benchmarks%252C%2520demonstrating%2520its%2520effectiveness%2520in%2520improving%250Areplanning%2520efficiency%2520for%2520sequential%2520multi-object%2520manipulation%2520tasks%2520in%2520dynamic%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06843v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learn2Decompose%3A%20Learning%20Problem%20Decomposition%20for%20Efficient%20Sequential%0A%20%20Multi-object%20Manipulation%20Planning&entry.906535625=Yan%20Zhang%20and%20Teng%20Xue%20and%20Amirreza%20Razmjoo%20and%20Sylvain%20Calinon&entry.1292438233=%20%20We%20present%20a%20Reactive%20Task%20and%20Motion%20Planning%20%28TAMP%29%20approach%20for%20efficient%0Asequential%20multi-object%20manipulation%20in%20dynamic%20environments.%20Conventional%20TAMP%0Asolvers%20experience%20an%20exponential%20increase%20in%20planning%20time%20as%20the%20planning%0Ahorizon%20and%20number%20of%20objects%20grow%2C%20limiting%20their%20applicability%20in%20real-world%0Ascenarios.%20To%20address%20this%2C%20we%20propose%20learning%20problem%20decomposition%20from%0Ademonstrations%20to%20accelerate%20TAMP%20solvers.%20Our%20approach%20consists%20of%20three%20key%0Acomponents%3A%20goal%20decomposition%20learning%2C%20temporal%20distance%20learning%2C%20and%20object%0Areduction.%20Goal%20decomposition%20identifies%20the%20necessary%20sequences%20of%20states%20that%0Athe%20system%20must%20pass%20through%20before%20reaching%20the%20final%20goal%2C%20treating%20them%20as%0Asubgoal%20sequences.%20Temporal%20distance%20learning%20predicts%20the%20temporal%20distance%0Abetween%20two%20states%2C%20enabling%20the%20system%20to%20identify%20the%20closest%20subgoal%20from%20a%0Adisturbed%20state.%20Object%20reduction%20minimizes%20the%20set%20of%20active%20objects%0Aconsidered%20during%20replanning%2C%20further%20improving%20efficiency.%20We%20evaluate%20our%0Aapproach%20on%20three%20benchmarks%2C%20demonstrating%20its%20effectiveness%20in%20improving%0Areplanning%20efficiency%20for%20sequential%20multi-object%20manipulation%20tasks%20in%20dynamic%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06843v2&entry.124074799=Read"},
{"title": "Saliency-Aware Diffusion Reconstruction for Effective Invisible\n  Watermark Removal", "author": "Inzamamul Alam and Md Tanvir Islam and Simon S. Woo", "abstract": "  As digital content becomes increasingly ubiquitous, the need for robust\nwatermark removal techniques has grown due to the inadequacy of existing\nembedding techniques, which lack robustness. This paper introduces a novel\nSaliency-Aware Diffusion Reconstruction (SADRE) framework for watermark\nelimination on the web, combining adaptive noise injection, region-specific\nperturbations, and advanced diffusion-based reconstruction. SADRE disrupts\nembedded watermarks by injecting targeted noise into latent representations\nguided by saliency masks although preserving essential image features. A\nreverse diffusion process ensures high-fidelity image restoration, leveraging\nadaptive noise levels determined by watermark strength. Our framework is\ntheoretically grounded with stability guarantees and achieves robust watermark\nremoval across diverse scenarios. Empirical evaluations on state-of-the-art\n(SOTA) watermarking techniques demonstrate SADRE's superiority in balancing\nwatermark disruption and image quality. SADRE sets a new benchmark for\nwatermark elimination, offering a flexible and reliable solution for real-world\nweb content. Code is available\non~\\href{https://github.com/inzamamulDU/SADRE}{\\textbf{https://github.com/inzamamulDU/SADRE}}.\n", "link": "http://arxiv.org/abs/2504.12809v1", "date": "2025-04-17", "relevancy": 2.1296, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5575}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5377}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Saliency-Aware%20Diffusion%20Reconstruction%20for%20Effective%20Invisible%0A%20%20Watermark%20Removal&body=Title%3A%20Saliency-Aware%20Diffusion%20Reconstruction%20for%20Effective%20Invisible%0A%20%20Watermark%20Removal%0AAuthor%3A%20Inzamamul%20Alam%20and%20Md%20Tanvir%20Islam%20and%20Simon%20S.%20Woo%0AAbstract%3A%20%20%20As%20digital%20content%20becomes%20increasingly%20ubiquitous%2C%20the%20need%20for%20robust%0Awatermark%20removal%20techniques%20has%20grown%20due%20to%20the%20inadequacy%20of%20existing%0Aembedding%20techniques%2C%20which%20lack%20robustness.%20This%20paper%20introduces%20a%20novel%0ASaliency-Aware%20Diffusion%20Reconstruction%20%28SADRE%29%20framework%20for%20watermark%0Aelimination%20on%20the%20web%2C%20combining%20adaptive%20noise%20injection%2C%20region-specific%0Aperturbations%2C%20and%20advanced%20diffusion-based%20reconstruction.%20SADRE%20disrupts%0Aembedded%20watermarks%20by%20injecting%20targeted%20noise%20into%20latent%20representations%0Aguided%20by%20saliency%20masks%20although%20preserving%20essential%20image%20features.%20A%0Areverse%20diffusion%20process%20ensures%20high-fidelity%20image%20restoration%2C%20leveraging%0Aadaptive%20noise%20levels%20determined%20by%20watermark%20strength.%20Our%20framework%20is%0Atheoretically%20grounded%20with%20stability%20guarantees%20and%20achieves%20robust%20watermark%0Aremoval%20across%20diverse%20scenarios.%20Empirical%20evaluations%20on%20state-of-the-art%0A%28SOTA%29%20watermarking%20techniques%20demonstrate%20SADRE%27s%20superiority%20in%20balancing%0Awatermark%20disruption%20and%20image%20quality.%20SADRE%20sets%20a%20new%20benchmark%20for%0Awatermark%20elimination%2C%20offering%20a%20flexible%20and%20reliable%20solution%20for%20real-world%0Aweb%20content.%20Code%20is%20available%0Aon~%5Chref%7Bhttps%3A//github.com/inzamamulDU/SADRE%7D%7B%5Ctextbf%7Bhttps%3A//github.com/inzamamulDU/SADRE%7D%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12809v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSaliency-Aware%2520Diffusion%2520Reconstruction%2520for%2520Effective%2520Invisible%250A%2520%2520Watermark%2520Removal%26entry.906535625%3DInzamamul%2520Alam%2520and%2520Md%2520Tanvir%2520Islam%2520and%2520Simon%2520S.%2520Woo%26entry.1292438233%3D%2520%2520As%2520digital%2520content%2520becomes%2520increasingly%2520ubiquitous%252C%2520the%2520need%2520for%2520robust%250Awatermark%2520removal%2520techniques%2520has%2520grown%2520due%2520to%2520the%2520inadequacy%2520of%2520existing%250Aembedding%2520techniques%252C%2520which%2520lack%2520robustness.%2520This%2520paper%2520introduces%2520a%2520novel%250ASaliency-Aware%2520Diffusion%2520Reconstruction%2520%2528SADRE%2529%2520framework%2520for%2520watermark%250Aelimination%2520on%2520the%2520web%252C%2520combining%2520adaptive%2520noise%2520injection%252C%2520region-specific%250Aperturbations%252C%2520and%2520advanced%2520diffusion-based%2520reconstruction.%2520SADRE%2520disrupts%250Aembedded%2520watermarks%2520by%2520injecting%2520targeted%2520noise%2520into%2520latent%2520representations%250Aguided%2520by%2520saliency%2520masks%2520although%2520preserving%2520essential%2520image%2520features.%2520A%250Areverse%2520diffusion%2520process%2520ensures%2520high-fidelity%2520image%2520restoration%252C%2520leveraging%250Aadaptive%2520noise%2520levels%2520determined%2520by%2520watermark%2520strength.%2520Our%2520framework%2520is%250Atheoretically%2520grounded%2520with%2520stability%2520guarantees%2520and%2520achieves%2520robust%2520watermark%250Aremoval%2520across%2520diverse%2520scenarios.%2520Empirical%2520evaluations%2520on%2520state-of-the-art%250A%2528SOTA%2529%2520watermarking%2520techniques%2520demonstrate%2520SADRE%2527s%2520superiority%2520in%2520balancing%250Awatermark%2520disruption%2520and%2520image%2520quality.%2520SADRE%2520sets%2520a%2520new%2520benchmark%2520for%250Awatermark%2520elimination%252C%2520offering%2520a%2520flexible%2520and%2520reliable%2520solution%2520for%2520real-world%250Aweb%2520content.%2520Code%2520is%2520available%250Aon~%255Chref%257Bhttps%253A//github.com/inzamamulDU/SADRE%257D%257B%255Ctextbf%257Bhttps%253A//github.com/inzamamulDU/SADRE%257D%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12809v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Saliency-Aware%20Diffusion%20Reconstruction%20for%20Effective%20Invisible%0A%20%20Watermark%20Removal&entry.906535625=Inzamamul%20Alam%20and%20Md%20Tanvir%20Islam%20and%20Simon%20S.%20Woo&entry.1292438233=%20%20As%20digital%20content%20becomes%20increasingly%20ubiquitous%2C%20the%20need%20for%20robust%0Awatermark%20removal%20techniques%20has%20grown%20due%20to%20the%20inadequacy%20of%20existing%0Aembedding%20techniques%2C%20which%20lack%20robustness.%20This%20paper%20introduces%20a%20novel%0ASaliency-Aware%20Diffusion%20Reconstruction%20%28SADRE%29%20framework%20for%20watermark%0Aelimination%20on%20the%20web%2C%20combining%20adaptive%20noise%20injection%2C%20region-specific%0Aperturbations%2C%20and%20advanced%20diffusion-based%20reconstruction.%20SADRE%20disrupts%0Aembedded%20watermarks%20by%20injecting%20targeted%20noise%20into%20latent%20representations%0Aguided%20by%20saliency%20masks%20although%20preserving%20essential%20image%20features.%20A%0Areverse%20diffusion%20process%20ensures%20high-fidelity%20image%20restoration%2C%20leveraging%0Aadaptive%20noise%20levels%20determined%20by%20watermark%20strength.%20Our%20framework%20is%0Atheoretically%20grounded%20with%20stability%20guarantees%20and%20achieves%20robust%20watermark%0Aremoval%20across%20diverse%20scenarios.%20Empirical%20evaluations%20on%20state-of-the-art%0A%28SOTA%29%20watermarking%20techniques%20demonstrate%20SADRE%27s%20superiority%20in%20balancing%0Awatermark%20disruption%20and%20image%20quality.%20SADRE%20sets%20a%20new%20benchmark%20for%0Awatermark%20elimination%2C%20offering%20a%20flexible%20and%20reliable%20solution%20for%20real-world%0Aweb%20content.%20Code%20is%20available%0Aon~%5Chref%7Bhttps%3A//github.com/inzamamulDU/SADRE%7D%7B%5Ctextbf%7Bhttps%3A//github.com/inzamamulDU/SADRE%7D%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12809v1&entry.124074799=Read"},
{"title": "MathPhys-Guided Coarse-to-Fine Anomaly Synthesis with SQE-Driven\n  Bi-Level Optimization for Anomaly Detection", "author": "Long Qian and Bingke Zhu and Yingying Chen and Ming Tang and Jinqiao Wang", "abstract": "  Anomaly detection is a crucial task in computer vision, yet collecting\nreal-world defect images is inherently difficult due to the rarity and\nunpredictability of anomalies. Consequently, researchers have turned to\nsynthetic methods for training data augmentation. However, existing synthetic\nstrategies (e.g., naive cut-and-paste or inpainting) overlook the underlying\nphysical causes of defects, leading to inconsistent, low-fidelity anomalies\nthat hamper model generalization to real-world complexities. In this thesis, we\nintroduced a novel pipeline that generates synthetic anomalies through\nMath-Physics model guidance, refines them via a Coarse-to-Fine approach and\nemploys a bi-level optimization strategy with a Synthesis Quality\nEstimator(SQE). By incorporating physical modeling of cracks, corrosion, and\ndeformation, our method produces realistic defect masks, which are subsequently\nenhanced in two phases. The first stage (npcF) enforces a PDE-based consistency\nto achieve a globally coherent anomaly structure, while the second stage\n(npcF++) further improves local fidelity using wavelet transforms and boundary\nsynergy blocks. Additionally, we leverage SQE-driven weighting, ensuring that\nhigh-quality synthetic samples receive greater emphasis during training. To\nvalidate our approach, we conducted comprehensive experiments on three widely\nadopted industrial anomaly detection benchmarks: MVTec AD, VisA, and BTAD.\nAcross these datasets, the proposed pipeline achieves state-of-the-art (SOTA)\nresults in both image-AUROC and pixel-AUROC, confirming the effectiveness of\nour MaPhC2F and BiSQAD.\n", "link": "http://arxiv.org/abs/2504.12970v1", "date": "2025-04-17", "relevancy": 2.1236, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.539}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5292}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5235}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MathPhys-Guided%20Coarse-to-Fine%20Anomaly%20Synthesis%20with%20SQE-Driven%0A%20%20Bi-Level%20Optimization%20for%20Anomaly%20Detection&body=Title%3A%20MathPhys-Guided%20Coarse-to-Fine%20Anomaly%20Synthesis%20with%20SQE-Driven%0A%20%20Bi-Level%20Optimization%20for%20Anomaly%20Detection%0AAuthor%3A%20Long%20Qian%20and%20Bingke%20Zhu%20and%20Yingying%20Chen%20and%20Ming%20Tang%20and%20Jinqiao%20Wang%0AAbstract%3A%20%20%20Anomaly%20detection%20is%20a%20crucial%20task%20in%20computer%20vision%2C%20yet%20collecting%0Areal-world%20defect%20images%20is%20inherently%20difficult%20due%20to%20the%20rarity%20and%0Aunpredictability%20of%20anomalies.%20Consequently%2C%20researchers%20have%20turned%20to%0Asynthetic%20methods%20for%20training%20data%20augmentation.%20However%2C%20existing%20synthetic%0Astrategies%20%28e.g.%2C%20naive%20cut-and-paste%20or%20inpainting%29%20overlook%20the%20underlying%0Aphysical%20causes%20of%20defects%2C%20leading%20to%20inconsistent%2C%20low-fidelity%20anomalies%0Athat%20hamper%20model%20generalization%20to%20real-world%20complexities.%20In%20this%20thesis%2C%20we%0Aintroduced%20a%20novel%20pipeline%20that%20generates%20synthetic%20anomalies%20through%0AMath-Physics%20model%20guidance%2C%20refines%20them%20via%20a%20Coarse-to-Fine%20approach%20and%0Aemploys%20a%20bi-level%20optimization%20strategy%20with%20a%20Synthesis%20Quality%0AEstimator%28SQE%29.%20By%20incorporating%20physical%20modeling%20of%20cracks%2C%20corrosion%2C%20and%0Adeformation%2C%20our%20method%20produces%20realistic%20defect%20masks%2C%20which%20are%20subsequently%0Aenhanced%20in%20two%20phases.%20The%20first%20stage%20%28npcF%29%20enforces%20a%20PDE-based%20consistency%0Ato%20achieve%20a%20globally%20coherent%20anomaly%20structure%2C%20while%20the%20second%20stage%0A%28npcF%2B%2B%29%20further%20improves%20local%20fidelity%20using%20wavelet%20transforms%20and%20boundary%0Asynergy%20blocks.%20Additionally%2C%20we%20leverage%20SQE-driven%20weighting%2C%20ensuring%20that%0Ahigh-quality%20synthetic%20samples%20receive%20greater%20emphasis%20during%20training.%20To%0Avalidate%20our%20approach%2C%20we%20conducted%20comprehensive%20experiments%20on%20three%20widely%0Aadopted%20industrial%20anomaly%20detection%20benchmarks%3A%20MVTec%20AD%2C%20VisA%2C%20and%20BTAD.%0AAcross%20these%20datasets%2C%20the%20proposed%20pipeline%20achieves%20state-of-the-art%20%28SOTA%29%0Aresults%20in%20both%20image-AUROC%20and%20pixel-AUROC%2C%20confirming%20the%20effectiveness%20of%0Aour%20MaPhC2F%20and%20BiSQAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12970v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMathPhys-Guided%2520Coarse-to-Fine%2520Anomaly%2520Synthesis%2520with%2520SQE-Driven%250A%2520%2520Bi-Level%2520Optimization%2520for%2520Anomaly%2520Detection%26entry.906535625%3DLong%2520Qian%2520and%2520Bingke%2520Zhu%2520and%2520Yingying%2520Chen%2520and%2520Ming%2520Tang%2520and%2520Jinqiao%2520Wang%26entry.1292438233%3D%2520%2520Anomaly%2520detection%2520is%2520a%2520crucial%2520task%2520in%2520computer%2520vision%252C%2520yet%2520collecting%250Areal-world%2520defect%2520images%2520is%2520inherently%2520difficult%2520due%2520to%2520the%2520rarity%2520and%250Aunpredictability%2520of%2520anomalies.%2520Consequently%252C%2520researchers%2520have%2520turned%2520to%250Asynthetic%2520methods%2520for%2520training%2520data%2520augmentation.%2520However%252C%2520existing%2520synthetic%250Astrategies%2520%2528e.g.%252C%2520naive%2520cut-and-paste%2520or%2520inpainting%2529%2520overlook%2520the%2520underlying%250Aphysical%2520causes%2520of%2520defects%252C%2520leading%2520to%2520inconsistent%252C%2520low-fidelity%2520anomalies%250Athat%2520hamper%2520model%2520generalization%2520to%2520real-world%2520complexities.%2520In%2520this%2520thesis%252C%2520we%250Aintroduced%2520a%2520novel%2520pipeline%2520that%2520generates%2520synthetic%2520anomalies%2520through%250AMath-Physics%2520model%2520guidance%252C%2520refines%2520them%2520via%2520a%2520Coarse-to-Fine%2520approach%2520and%250Aemploys%2520a%2520bi-level%2520optimization%2520strategy%2520with%2520a%2520Synthesis%2520Quality%250AEstimator%2528SQE%2529.%2520By%2520incorporating%2520physical%2520modeling%2520of%2520cracks%252C%2520corrosion%252C%2520and%250Adeformation%252C%2520our%2520method%2520produces%2520realistic%2520defect%2520masks%252C%2520which%2520are%2520subsequently%250Aenhanced%2520in%2520two%2520phases.%2520The%2520first%2520stage%2520%2528npcF%2529%2520enforces%2520a%2520PDE-based%2520consistency%250Ato%2520achieve%2520a%2520globally%2520coherent%2520anomaly%2520structure%252C%2520while%2520the%2520second%2520stage%250A%2528npcF%252B%252B%2529%2520further%2520improves%2520local%2520fidelity%2520using%2520wavelet%2520transforms%2520and%2520boundary%250Asynergy%2520blocks.%2520Additionally%252C%2520we%2520leverage%2520SQE-driven%2520weighting%252C%2520ensuring%2520that%250Ahigh-quality%2520synthetic%2520samples%2520receive%2520greater%2520emphasis%2520during%2520training.%2520To%250Avalidate%2520our%2520approach%252C%2520we%2520conducted%2520comprehensive%2520experiments%2520on%2520three%2520widely%250Aadopted%2520industrial%2520anomaly%2520detection%2520benchmarks%253A%2520MVTec%2520AD%252C%2520VisA%252C%2520and%2520BTAD.%250AAcross%2520these%2520datasets%252C%2520the%2520proposed%2520pipeline%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%250Aresults%2520in%2520both%2520image-AUROC%2520and%2520pixel-AUROC%252C%2520confirming%2520the%2520effectiveness%2520of%250Aour%2520MaPhC2F%2520and%2520BiSQAD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12970v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MathPhys-Guided%20Coarse-to-Fine%20Anomaly%20Synthesis%20with%20SQE-Driven%0A%20%20Bi-Level%20Optimization%20for%20Anomaly%20Detection&entry.906535625=Long%20Qian%20and%20Bingke%20Zhu%20and%20Yingying%20Chen%20and%20Ming%20Tang%20and%20Jinqiao%20Wang&entry.1292438233=%20%20Anomaly%20detection%20is%20a%20crucial%20task%20in%20computer%20vision%2C%20yet%20collecting%0Areal-world%20defect%20images%20is%20inherently%20difficult%20due%20to%20the%20rarity%20and%0Aunpredictability%20of%20anomalies.%20Consequently%2C%20researchers%20have%20turned%20to%0Asynthetic%20methods%20for%20training%20data%20augmentation.%20However%2C%20existing%20synthetic%0Astrategies%20%28e.g.%2C%20naive%20cut-and-paste%20or%20inpainting%29%20overlook%20the%20underlying%0Aphysical%20causes%20of%20defects%2C%20leading%20to%20inconsistent%2C%20low-fidelity%20anomalies%0Athat%20hamper%20model%20generalization%20to%20real-world%20complexities.%20In%20this%20thesis%2C%20we%0Aintroduced%20a%20novel%20pipeline%20that%20generates%20synthetic%20anomalies%20through%0AMath-Physics%20model%20guidance%2C%20refines%20them%20via%20a%20Coarse-to-Fine%20approach%20and%0Aemploys%20a%20bi-level%20optimization%20strategy%20with%20a%20Synthesis%20Quality%0AEstimator%28SQE%29.%20By%20incorporating%20physical%20modeling%20of%20cracks%2C%20corrosion%2C%20and%0Adeformation%2C%20our%20method%20produces%20realistic%20defect%20masks%2C%20which%20are%20subsequently%0Aenhanced%20in%20two%20phases.%20The%20first%20stage%20%28npcF%29%20enforces%20a%20PDE-based%20consistency%0Ato%20achieve%20a%20globally%20coherent%20anomaly%20structure%2C%20while%20the%20second%20stage%0A%28npcF%2B%2B%29%20further%20improves%20local%20fidelity%20using%20wavelet%20transforms%20and%20boundary%0Asynergy%20blocks.%20Additionally%2C%20we%20leverage%20SQE-driven%20weighting%2C%20ensuring%20that%0Ahigh-quality%20synthetic%20samples%20receive%20greater%20emphasis%20during%20training.%20To%0Avalidate%20our%20approach%2C%20we%20conducted%20comprehensive%20experiments%20on%20three%20widely%0Aadopted%20industrial%20anomaly%20detection%20benchmarks%3A%20MVTec%20AD%2C%20VisA%2C%20and%20BTAD.%0AAcross%20these%20datasets%2C%20the%20proposed%20pipeline%20achieves%20state-of-the-art%20%28SOTA%29%0Aresults%20in%20both%20image-AUROC%20and%20pixel-AUROC%2C%20confirming%20the%20effectiveness%20of%0Aour%20MaPhC2F%20and%20BiSQAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12970v1&entry.124074799=Read"},
{"title": "VariFace: Fair and Diverse Synthetic Dataset Generation for Face\n  Recognition", "author": "Michael Yeung and Toya Teramoto and Songtao Wu and Tatsuo Fujiwara and Kenji Suzuki and Tamaki Kojima", "abstract": "  The use of large-scale, web-scraped datasets to train face recognition models\nhas raised significant privacy and bias concerns. Synthetic methods mitigate\nthese concerns and provide scalable and controllable face generation to enable\nfair and accurate face recognition. However, existing synthetic datasets\ndisplay limited intraclass and interclass diversity and do not match the face\nrecognition performance obtained using real datasets. Here, we propose\nVariFace, a two-stage diffusion-based pipeline to create fair and diverse\nsynthetic face datasets to train face recognition models. Specifically, we\nintroduce three methods: Face Recognition Consistency to refine demographic\nlabels, Face Vendi Score Guidance to improve interclass diversity, and\nDivergence Score Conditioning to balance the identity preservation-intraclass\ndiversity trade-off. When constrained to the same dataset size, VariFace\nconsiderably outperforms previous synthetic datasets (0.9200 $\\rightarrow$\n0.9405) and achieves comparable performance to face recognition models trained\nwith real data (Real Gap = -0.0065). In an unconstrained setting, VariFace not\nonly consistently achieves better performance compared to previous synthetic\nmethods across dataset sizes but also, for the first time, outperforms the real\ndataset (CASIA-WebFace) across six evaluation datasets. This sets a new\nstate-of-the-art performance with an average face verification accuracy of\n0.9567 (Real Gap = +0.0097) across LFW, CFP-FP, CPLFW, AgeDB, and CALFW\ndatasets and 0.9366 (Real Gap = +0.0380) on the RFW dataset.\n", "link": "http://arxiv.org/abs/2412.06235v2", "date": "2025-04-17", "relevancy": 2.1218, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5417}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5234}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5201}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VariFace%3A%20Fair%20and%20Diverse%20Synthetic%20Dataset%20Generation%20for%20Face%0A%20%20Recognition&body=Title%3A%20VariFace%3A%20Fair%20and%20Diverse%20Synthetic%20Dataset%20Generation%20for%20Face%0A%20%20Recognition%0AAuthor%3A%20Michael%20Yeung%20and%20Toya%20Teramoto%20and%20Songtao%20Wu%20and%20Tatsuo%20Fujiwara%20and%20Kenji%20Suzuki%20and%20Tamaki%20Kojima%0AAbstract%3A%20%20%20The%20use%20of%20large-scale%2C%20web-scraped%20datasets%20to%20train%20face%20recognition%20models%0Ahas%20raised%20significant%20privacy%20and%20bias%20concerns.%20Synthetic%20methods%20mitigate%0Athese%20concerns%20and%20provide%20scalable%20and%20controllable%20face%20generation%20to%20enable%0Afair%20and%20accurate%20face%20recognition.%20However%2C%20existing%20synthetic%20datasets%0Adisplay%20limited%20intraclass%20and%20interclass%20diversity%20and%20do%20not%20match%20the%20face%0Arecognition%20performance%20obtained%20using%20real%20datasets.%20Here%2C%20we%20propose%0AVariFace%2C%20a%20two-stage%20diffusion-based%20pipeline%20to%20create%20fair%20and%20diverse%0Asynthetic%20face%20datasets%20to%20train%20face%20recognition%20models.%20Specifically%2C%20we%0Aintroduce%20three%20methods%3A%20Face%20Recognition%20Consistency%20to%20refine%20demographic%0Alabels%2C%20Face%20Vendi%20Score%20Guidance%20to%20improve%20interclass%20diversity%2C%20and%0ADivergence%20Score%20Conditioning%20to%20balance%20the%20identity%20preservation-intraclass%0Adiversity%20trade-off.%20When%20constrained%20to%20the%20same%20dataset%20size%2C%20VariFace%0Aconsiderably%20outperforms%20previous%20synthetic%20datasets%20%280.9200%20%24%5Crightarrow%24%0A0.9405%29%20and%20achieves%20comparable%20performance%20to%20face%20recognition%20models%20trained%0Awith%20real%20data%20%28Real%20Gap%20%3D%20-0.0065%29.%20In%20an%20unconstrained%20setting%2C%20VariFace%20not%0Aonly%20consistently%20achieves%20better%20performance%20compared%20to%20previous%20synthetic%0Amethods%20across%20dataset%20sizes%20but%20also%2C%20for%20the%20first%20time%2C%20outperforms%20the%20real%0Adataset%20%28CASIA-WebFace%29%20across%20six%20evaluation%20datasets.%20This%20sets%20a%20new%0Astate-of-the-art%20performance%20with%20an%20average%20face%20verification%20accuracy%20of%0A0.9567%20%28Real%20Gap%20%3D%20%2B0.0097%29%20across%20LFW%2C%20CFP-FP%2C%20CPLFW%2C%20AgeDB%2C%20and%20CALFW%0Adatasets%20and%200.9366%20%28Real%20Gap%20%3D%20%2B0.0380%29%20on%20the%20RFW%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.06235v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVariFace%253A%2520Fair%2520and%2520Diverse%2520Synthetic%2520Dataset%2520Generation%2520for%2520Face%250A%2520%2520Recognition%26entry.906535625%3DMichael%2520Yeung%2520and%2520Toya%2520Teramoto%2520and%2520Songtao%2520Wu%2520and%2520Tatsuo%2520Fujiwara%2520and%2520Kenji%2520Suzuki%2520and%2520Tamaki%2520Kojima%26entry.1292438233%3D%2520%2520The%2520use%2520of%2520large-scale%252C%2520web-scraped%2520datasets%2520to%2520train%2520face%2520recognition%2520models%250Ahas%2520raised%2520significant%2520privacy%2520and%2520bias%2520concerns.%2520Synthetic%2520methods%2520mitigate%250Athese%2520concerns%2520and%2520provide%2520scalable%2520and%2520controllable%2520face%2520generation%2520to%2520enable%250Afair%2520and%2520accurate%2520face%2520recognition.%2520However%252C%2520existing%2520synthetic%2520datasets%250Adisplay%2520limited%2520intraclass%2520and%2520interclass%2520diversity%2520and%2520do%2520not%2520match%2520the%2520face%250Arecognition%2520performance%2520obtained%2520using%2520real%2520datasets.%2520Here%252C%2520we%2520propose%250AVariFace%252C%2520a%2520two-stage%2520diffusion-based%2520pipeline%2520to%2520create%2520fair%2520and%2520diverse%250Asynthetic%2520face%2520datasets%2520to%2520train%2520face%2520recognition%2520models.%2520Specifically%252C%2520we%250Aintroduce%2520three%2520methods%253A%2520Face%2520Recognition%2520Consistency%2520to%2520refine%2520demographic%250Alabels%252C%2520Face%2520Vendi%2520Score%2520Guidance%2520to%2520improve%2520interclass%2520diversity%252C%2520and%250ADivergence%2520Score%2520Conditioning%2520to%2520balance%2520the%2520identity%2520preservation-intraclass%250Adiversity%2520trade-off.%2520When%2520constrained%2520to%2520the%2520same%2520dataset%2520size%252C%2520VariFace%250Aconsiderably%2520outperforms%2520previous%2520synthetic%2520datasets%2520%25280.9200%2520%2524%255Crightarrow%2524%250A0.9405%2529%2520and%2520achieves%2520comparable%2520performance%2520to%2520face%2520recognition%2520models%2520trained%250Awith%2520real%2520data%2520%2528Real%2520Gap%2520%253D%2520-0.0065%2529.%2520In%2520an%2520unconstrained%2520setting%252C%2520VariFace%2520not%250Aonly%2520consistently%2520achieves%2520better%2520performance%2520compared%2520to%2520previous%2520synthetic%250Amethods%2520across%2520dataset%2520sizes%2520but%2520also%252C%2520for%2520the%2520first%2520time%252C%2520outperforms%2520the%2520real%250Adataset%2520%2528CASIA-WebFace%2529%2520across%2520six%2520evaluation%2520datasets.%2520This%2520sets%2520a%2520new%250Astate-of-the-art%2520performance%2520with%2520an%2520average%2520face%2520verification%2520accuracy%2520of%250A0.9567%2520%2528Real%2520Gap%2520%253D%2520%252B0.0097%2529%2520across%2520LFW%252C%2520CFP-FP%252C%2520CPLFW%252C%2520AgeDB%252C%2520and%2520CALFW%250Adatasets%2520and%25200.9366%2520%2528Real%2520Gap%2520%253D%2520%252B0.0380%2529%2520on%2520the%2520RFW%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.06235v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VariFace%3A%20Fair%20and%20Diverse%20Synthetic%20Dataset%20Generation%20for%20Face%0A%20%20Recognition&entry.906535625=Michael%20Yeung%20and%20Toya%20Teramoto%20and%20Songtao%20Wu%20and%20Tatsuo%20Fujiwara%20and%20Kenji%20Suzuki%20and%20Tamaki%20Kojima&entry.1292438233=%20%20The%20use%20of%20large-scale%2C%20web-scraped%20datasets%20to%20train%20face%20recognition%20models%0Ahas%20raised%20significant%20privacy%20and%20bias%20concerns.%20Synthetic%20methods%20mitigate%0Athese%20concerns%20and%20provide%20scalable%20and%20controllable%20face%20generation%20to%20enable%0Afair%20and%20accurate%20face%20recognition.%20However%2C%20existing%20synthetic%20datasets%0Adisplay%20limited%20intraclass%20and%20interclass%20diversity%20and%20do%20not%20match%20the%20face%0Arecognition%20performance%20obtained%20using%20real%20datasets.%20Here%2C%20we%20propose%0AVariFace%2C%20a%20two-stage%20diffusion-based%20pipeline%20to%20create%20fair%20and%20diverse%0Asynthetic%20face%20datasets%20to%20train%20face%20recognition%20models.%20Specifically%2C%20we%0Aintroduce%20three%20methods%3A%20Face%20Recognition%20Consistency%20to%20refine%20demographic%0Alabels%2C%20Face%20Vendi%20Score%20Guidance%20to%20improve%20interclass%20diversity%2C%20and%0ADivergence%20Score%20Conditioning%20to%20balance%20the%20identity%20preservation-intraclass%0Adiversity%20trade-off.%20When%20constrained%20to%20the%20same%20dataset%20size%2C%20VariFace%0Aconsiderably%20outperforms%20previous%20synthetic%20datasets%20%280.9200%20%24%5Crightarrow%24%0A0.9405%29%20and%20achieves%20comparable%20performance%20to%20face%20recognition%20models%20trained%0Awith%20real%20data%20%28Real%20Gap%20%3D%20-0.0065%29.%20In%20an%20unconstrained%20setting%2C%20VariFace%20not%0Aonly%20consistently%20achieves%20better%20performance%20compared%20to%20previous%20synthetic%0Amethods%20across%20dataset%20sizes%20but%20also%2C%20for%20the%20first%20time%2C%20outperforms%20the%20real%0Adataset%20%28CASIA-WebFace%29%20across%20six%20evaluation%20datasets.%20This%20sets%20a%20new%0Astate-of-the-art%20performance%20with%20an%20average%20face%20verification%20accuracy%20of%0A0.9567%20%28Real%20Gap%20%3D%20%2B0.0097%29%20across%20LFW%2C%20CFP-FP%2C%20CPLFW%2C%20AgeDB%2C%20and%20CALFW%0Adatasets%20and%200.9366%20%28Real%20Gap%20%3D%20%2B0.0380%29%20on%20the%20RFW%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.06235v2&entry.124074799=Read"},
{"title": "Rethinking Few-Shot Image Fusion: Granular Ball Priors Enable\n  General-Purpose Deep Fusion", "author": "Minjie Deng and Yan Wei and Hao Zhai and An Wu and Yuncan Ouyang and Qianyao Peng", "abstract": "  In image fusion tasks, the absence of real fused images as priors presents a\nfundamental challenge. Most deep learning-based fusion methods rely on\nlarge-scale paired datasets to extract global weighting features from raw\nimages, thereby generating fused outputs that approximate real fused images. In\ncontrast to previous studies, this paper explores few-shot training of neural\nnetworks under the condition of having prior knowledge. We propose a novel\nfusion framework named GBFF, and a Granular Ball Significant Extraction\nalgorithm specifically designed for the few-shot prior setting. All pixel pairs\ninvolved in the fusion process are initially modeled as a Coarse-Grained\nGranular Ball. At the local level, Fine-Grained Granular Balls are used to\nslide through the brightness space to extract Non-Salient Pixel Pairs, and\nperform splitting operations to obtain Salient Pixel Pairs. Pixel-wise weights\nare then computed to generate a pseudo-supervised image. At the global level,\npixel pairs with significant contributions to the fusion process are\ncategorized into the Positive Region, while those whose contributions cannot be\naccurately determined are assigned to the Boundary Region. The Granular Ball\nperforms modality-aware adaptation based on the proportion of the positive\nregion, thereby adjusting the neural network's loss function and enabling it to\ncomplement the information of the boundary region. Extensive experiments\ndemonstrate the effectiveness of both the proposed algorithm and the underlying\ntheory. Compared with state-of-the-art (SOTA) methods, our approach shows\nstrong competitiveness in terms of both fusion time and image expressiveness.\nOur code is publicly available at:\n", "link": "http://arxiv.org/abs/2504.08937v2", "date": "2025-04-17", "relevancy": 2.1128, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5492}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5297}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Few-Shot%20Image%20Fusion%3A%20Granular%20Ball%20Priors%20Enable%0A%20%20General-Purpose%20Deep%20Fusion&body=Title%3A%20Rethinking%20Few-Shot%20Image%20Fusion%3A%20Granular%20Ball%20Priors%20Enable%0A%20%20General-Purpose%20Deep%20Fusion%0AAuthor%3A%20Minjie%20Deng%20and%20Yan%20Wei%20and%20Hao%20Zhai%20and%20An%20Wu%20and%20Yuncan%20Ouyang%20and%20Qianyao%20Peng%0AAbstract%3A%20%20%20In%20image%20fusion%20tasks%2C%20the%20absence%20of%20real%20fused%20images%20as%20priors%20presents%20a%0Afundamental%20challenge.%20Most%20deep%20learning-based%20fusion%20methods%20rely%20on%0Alarge-scale%20paired%20datasets%20to%20extract%20global%20weighting%20features%20from%20raw%0Aimages%2C%20thereby%20generating%20fused%20outputs%20that%20approximate%20real%20fused%20images.%20In%0Acontrast%20to%20previous%20studies%2C%20this%20paper%20explores%20few-shot%20training%20of%20neural%0Anetworks%20under%20the%20condition%20of%20having%20prior%20knowledge.%20We%20propose%20a%20novel%0Afusion%20framework%20named%20GBFF%2C%20and%20a%20Granular%20Ball%20Significant%20Extraction%0Aalgorithm%20specifically%20designed%20for%20the%20few-shot%20prior%20setting.%20All%20pixel%20pairs%0Ainvolved%20in%20the%20fusion%20process%20are%20initially%20modeled%20as%20a%20Coarse-Grained%0AGranular%20Ball.%20At%20the%20local%20level%2C%20Fine-Grained%20Granular%20Balls%20are%20used%20to%0Aslide%20through%20the%20brightness%20space%20to%20extract%20Non-Salient%20Pixel%20Pairs%2C%20and%0Aperform%20splitting%20operations%20to%20obtain%20Salient%20Pixel%20Pairs.%20Pixel-wise%20weights%0Aare%20then%20computed%20to%20generate%20a%20pseudo-supervised%20image.%20At%20the%20global%20level%2C%0Apixel%20pairs%20with%20significant%20contributions%20to%20the%20fusion%20process%20are%0Acategorized%20into%20the%20Positive%20Region%2C%20while%20those%20whose%20contributions%20cannot%20be%0Aaccurately%20determined%20are%20assigned%20to%20the%20Boundary%20Region.%20The%20Granular%20Ball%0Aperforms%20modality-aware%20adaptation%20based%20on%20the%20proportion%20of%20the%20positive%0Aregion%2C%20thereby%20adjusting%20the%20neural%20network%27s%20loss%20function%20and%20enabling%20it%20to%0Acomplement%20the%20information%20of%20the%20boundary%20region.%20Extensive%20experiments%0Ademonstrate%20the%20effectiveness%20of%20both%20the%20proposed%20algorithm%20and%20the%20underlying%0Atheory.%20Compared%20with%20state-of-the-art%20%28SOTA%29%20methods%2C%20our%20approach%20shows%0Astrong%20competitiveness%20in%20terms%20of%20both%20fusion%20time%20and%20image%20expressiveness.%0AOur%20code%20is%20publicly%20available%20at%3A%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.08937v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Few-Shot%2520Image%2520Fusion%253A%2520Granular%2520Ball%2520Priors%2520Enable%250A%2520%2520General-Purpose%2520Deep%2520Fusion%26entry.906535625%3DMinjie%2520Deng%2520and%2520Yan%2520Wei%2520and%2520Hao%2520Zhai%2520and%2520An%2520Wu%2520and%2520Yuncan%2520Ouyang%2520and%2520Qianyao%2520Peng%26entry.1292438233%3D%2520%2520In%2520image%2520fusion%2520tasks%252C%2520the%2520absence%2520of%2520real%2520fused%2520images%2520as%2520priors%2520presents%2520a%250Afundamental%2520challenge.%2520Most%2520deep%2520learning-based%2520fusion%2520methods%2520rely%2520on%250Alarge-scale%2520paired%2520datasets%2520to%2520extract%2520global%2520weighting%2520features%2520from%2520raw%250Aimages%252C%2520thereby%2520generating%2520fused%2520outputs%2520that%2520approximate%2520real%2520fused%2520images.%2520In%250Acontrast%2520to%2520previous%2520studies%252C%2520this%2520paper%2520explores%2520few-shot%2520training%2520of%2520neural%250Anetworks%2520under%2520the%2520condition%2520of%2520having%2520prior%2520knowledge.%2520We%2520propose%2520a%2520novel%250Afusion%2520framework%2520named%2520GBFF%252C%2520and%2520a%2520Granular%2520Ball%2520Significant%2520Extraction%250Aalgorithm%2520specifically%2520designed%2520for%2520the%2520few-shot%2520prior%2520setting.%2520All%2520pixel%2520pairs%250Ainvolved%2520in%2520the%2520fusion%2520process%2520are%2520initially%2520modeled%2520as%2520a%2520Coarse-Grained%250AGranular%2520Ball.%2520At%2520the%2520local%2520level%252C%2520Fine-Grained%2520Granular%2520Balls%2520are%2520used%2520to%250Aslide%2520through%2520the%2520brightness%2520space%2520to%2520extract%2520Non-Salient%2520Pixel%2520Pairs%252C%2520and%250Aperform%2520splitting%2520operations%2520to%2520obtain%2520Salient%2520Pixel%2520Pairs.%2520Pixel-wise%2520weights%250Aare%2520then%2520computed%2520to%2520generate%2520a%2520pseudo-supervised%2520image.%2520At%2520the%2520global%2520level%252C%250Apixel%2520pairs%2520with%2520significant%2520contributions%2520to%2520the%2520fusion%2520process%2520are%250Acategorized%2520into%2520the%2520Positive%2520Region%252C%2520while%2520those%2520whose%2520contributions%2520cannot%2520be%250Aaccurately%2520determined%2520are%2520assigned%2520to%2520the%2520Boundary%2520Region.%2520The%2520Granular%2520Ball%250Aperforms%2520modality-aware%2520adaptation%2520based%2520on%2520the%2520proportion%2520of%2520the%2520positive%250Aregion%252C%2520thereby%2520adjusting%2520the%2520neural%2520network%2527s%2520loss%2520function%2520and%2520enabling%2520it%2520to%250Acomplement%2520the%2520information%2520of%2520the%2520boundary%2520region.%2520Extensive%2520experiments%250Ademonstrate%2520the%2520effectiveness%2520of%2520both%2520the%2520proposed%2520algorithm%2520and%2520the%2520underlying%250Atheory.%2520Compared%2520with%2520state-of-the-art%2520%2528SOTA%2529%2520methods%252C%2520our%2520approach%2520shows%250Astrong%2520competitiveness%2520in%2520terms%2520of%2520both%2520fusion%2520time%2520and%2520image%2520expressiveness.%250AOur%2520code%2520is%2520publicly%2520available%2520at%253A%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.08937v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Few-Shot%20Image%20Fusion%3A%20Granular%20Ball%20Priors%20Enable%0A%20%20General-Purpose%20Deep%20Fusion&entry.906535625=Minjie%20Deng%20and%20Yan%20Wei%20and%20Hao%20Zhai%20and%20An%20Wu%20and%20Yuncan%20Ouyang%20and%20Qianyao%20Peng&entry.1292438233=%20%20In%20image%20fusion%20tasks%2C%20the%20absence%20of%20real%20fused%20images%20as%20priors%20presents%20a%0Afundamental%20challenge.%20Most%20deep%20learning-based%20fusion%20methods%20rely%20on%0Alarge-scale%20paired%20datasets%20to%20extract%20global%20weighting%20features%20from%20raw%0Aimages%2C%20thereby%20generating%20fused%20outputs%20that%20approximate%20real%20fused%20images.%20In%0Acontrast%20to%20previous%20studies%2C%20this%20paper%20explores%20few-shot%20training%20of%20neural%0Anetworks%20under%20the%20condition%20of%20having%20prior%20knowledge.%20We%20propose%20a%20novel%0Afusion%20framework%20named%20GBFF%2C%20and%20a%20Granular%20Ball%20Significant%20Extraction%0Aalgorithm%20specifically%20designed%20for%20the%20few-shot%20prior%20setting.%20All%20pixel%20pairs%0Ainvolved%20in%20the%20fusion%20process%20are%20initially%20modeled%20as%20a%20Coarse-Grained%0AGranular%20Ball.%20At%20the%20local%20level%2C%20Fine-Grained%20Granular%20Balls%20are%20used%20to%0Aslide%20through%20the%20brightness%20space%20to%20extract%20Non-Salient%20Pixel%20Pairs%2C%20and%0Aperform%20splitting%20operations%20to%20obtain%20Salient%20Pixel%20Pairs.%20Pixel-wise%20weights%0Aare%20then%20computed%20to%20generate%20a%20pseudo-supervised%20image.%20At%20the%20global%20level%2C%0Apixel%20pairs%20with%20significant%20contributions%20to%20the%20fusion%20process%20are%0Acategorized%20into%20the%20Positive%20Region%2C%20while%20those%20whose%20contributions%20cannot%20be%0Aaccurately%20determined%20are%20assigned%20to%20the%20Boundary%20Region.%20The%20Granular%20Ball%0Aperforms%20modality-aware%20adaptation%20based%20on%20the%20proportion%20of%20the%20positive%0Aregion%2C%20thereby%20adjusting%20the%20neural%20network%27s%20loss%20function%20and%20enabling%20it%20to%0Acomplement%20the%20information%20of%20the%20boundary%20region.%20Extensive%20experiments%0Ademonstrate%20the%20effectiveness%20of%20both%20the%20proposed%20algorithm%20and%20the%20underlying%0Atheory.%20Compared%20with%20state-of-the-art%20%28SOTA%29%20methods%2C%20our%20approach%20shows%0Astrong%20competitiveness%20in%20terms%20of%20both%20fusion%20time%20and%20image%20expressiveness.%0AOur%20code%20is%20publicly%20available%20at%3A%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.08937v2&entry.124074799=Read"},
{"title": "Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study\n  on Out-of-Distribution Generalisation", "author": "Qiming Bao and Alex Yuxuan Peng and Tim Hartill and Neset Tan and Zhenyun Deng and Michael Witbrock and Jiamou Liu", "abstract": "  Combining deep learning with symbolic logic reasoning aims to capitalize on\nthe success of both fields and is drawing increasing attention. Inspired by\nDeepLogic, an end-to-end model trained to perform inference on logic programs,\nwe introduce IMA-GloVe-GA, an iterative neural inference network for multi-step\nreasoning expressed in natural language. In our model, reasoning is performed\nusing an iterative memory neural network based on RNN with a gated attention\nmechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES\nV1 and CONCEPTRULES V2. Experimental results show DeepLogic with gated\nattention can achieve higher test accuracy than DeepLogic and other RNN\nbaseline models. Our model achieves better out-of-distribution generalisation\nthan RoBERTa-Large when the rules have been shuffled. Furthermore, to address\nthe issue of unbalanced distribution of reasoning depths in the current\nmulti-step reasoning datasets, we develop PARARULE-Plus, a large dataset with\nmore examples that require deeper reasoning steps. Experimental results show\nthat the addition of PARARULE-Plus can increase the model's performance on\nexamples requiring deeper reasoning depths. The source code and data are\navailable at\nhttps://github.com/Strong-AI-Lab/Multi-Step-Deductive-Reasoning-Over-Natural-Language.\n", "link": "http://arxiv.org/abs/2207.14000v4", "date": "2025-04-17", "relevancy": 2.1083, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5413}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5242}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5242}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Step%20Deductive%20Reasoning%20Over%20Natural%20Language%3A%20An%20Empirical%20Study%0A%20%20on%20Out-of-Distribution%20Generalisation&body=Title%3A%20Multi-Step%20Deductive%20Reasoning%20Over%20Natural%20Language%3A%20An%20Empirical%20Study%0A%20%20on%20Out-of-Distribution%20Generalisation%0AAuthor%3A%20Qiming%20Bao%20and%20Alex%20Yuxuan%20Peng%20and%20Tim%20Hartill%20and%20Neset%20Tan%20and%20Zhenyun%20Deng%20and%20Michael%20Witbrock%20and%20Jiamou%20Liu%0AAbstract%3A%20%20%20Combining%20deep%20learning%20with%20symbolic%20logic%20reasoning%20aims%20to%20capitalize%20on%0Athe%20success%20of%20both%20fields%20and%20is%20drawing%20increasing%20attention.%20Inspired%20by%0ADeepLogic%2C%20an%20end-to-end%20model%20trained%20to%20perform%20inference%20on%20logic%20programs%2C%0Awe%20introduce%20IMA-GloVe-GA%2C%20an%20iterative%20neural%20inference%20network%20for%20multi-step%0Areasoning%20expressed%20in%20natural%20language.%20In%20our%20model%2C%20reasoning%20is%20performed%0Ausing%20an%20iterative%20memory%20neural%20network%20based%20on%20RNN%20with%20a%20gated%20attention%0Amechanism.%20We%20evaluate%20IMA-GloVe-GA%20on%20three%20datasets%3A%20PARARULES%2C%20CONCEPTRULES%0AV1%20and%20CONCEPTRULES%20V2.%20Experimental%20results%20show%20DeepLogic%20with%20gated%0Aattention%20can%20achieve%20higher%20test%20accuracy%20than%20DeepLogic%20and%20other%20RNN%0Abaseline%20models.%20Our%20model%20achieves%20better%20out-of-distribution%20generalisation%0Athan%20RoBERTa-Large%20when%20the%20rules%20have%20been%20shuffled.%20Furthermore%2C%20to%20address%0Athe%20issue%20of%20unbalanced%20distribution%20of%20reasoning%20depths%20in%20the%20current%0Amulti-step%20reasoning%20datasets%2C%20we%20develop%20PARARULE-Plus%2C%20a%20large%20dataset%20with%0Amore%20examples%20that%20require%20deeper%20reasoning%20steps.%20Experimental%20results%20show%0Athat%20the%20addition%20of%20PARARULE-Plus%20can%20increase%20the%20model%27s%20performance%20on%0Aexamples%20requiring%20deeper%20reasoning%20depths.%20The%20source%20code%20and%20data%20are%0Aavailable%20at%0Ahttps%3A//github.com/Strong-AI-Lab/Multi-Step-Deductive-Reasoning-Over-Natural-Language.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2207.14000v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Step%2520Deductive%2520Reasoning%2520Over%2520Natural%2520Language%253A%2520An%2520Empirical%2520Study%250A%2520%2520on%2520Out-of-Distribution%2520Generalisation%26entry.906535625%3DQiming%2520Bao%2520and%2520Alex%2520Yuxuan%2520Peng%2520and%2520Tim%2520Hartill%2520and%2520Neset%2520Tan%2520and%2520Zhenyun%2520Deng%2520and%2520Michael%2520Witbrock%2520and%2520Jiamou%2520Liu%26entry.1292438233%3D%2520%2520Combining%2520deep%2520learning%2520with%2520symbolic%2520logic%2520reasoning%2520aims%2520to%2520capitalize%2520on%250Athe%2520success%2520of%2520both%2520fields%2520and%2520is%2520drawing%2520increasing%2520attention.%2520Inspired%2520by%250ADeepLogic%252C%2520an%2520end-to-end%2520model%2520trained%2520to%2520perform%2520inference%2520on%2520logic%2520programs%252C%250Awe%2520introduce%2520IMA-GloVe-GA%252C%2520an%2520iterative%2520neural%2520inference%2520network%2520for%2520multi-step%250Areasoning%2520expressed%2520in%2520natural%2520language.%2520In%2520our%2520model%252C%2520reasoning%2520is%2520performed%250Ausing%2520an%2520iterative%2520memory%2520neural%2520network%2520based%2520on%2520RNN%2520with%2520a%2520gated%2520attention%250Amechanism.%2520We%2520evaluate%2520IMA-GloVe-GA%2520on%2520three%2520datasets%253A%2520PARARULES%252C%2520CONCEPTRULES%250AV1%2520and%2520CONCEPTRULES%2520V2.%2520Experimental%2520results%2520show%2520DeepLogic%2520with%2520gated%250Aattention%2520can%2520achieve%2520higher%2520test%2520accuracy%2520than%2520DeepLogic%2520and%2520other%2520RNN%250Abaseline%2520models.%2520Our%2520model%2520achieves%2520better%2520out-of-distribution%2520generalisation%250Athan%2520RoBERTa-Large%2520when%2520the%2520rules%2520have%2520been%2520shuffled.%2520Furthermore%252C%2520to%2520address%250Athe%2520issue%2520of%2520unbalanced%2520distribution%2520of%2520reasoning%2520depths%2520in%2520the%2520current%250Amulti-step%2520reasoning%2520datasets%252C%2520we%2520develop%2520PARARULE-Plus%252C%2520a%2520large%2520dataset%2520with%250Amore%2520examples%2520that%2520require%2520deeper%2520reasoning%2520steps.%2520Experimental%2520results%2520show%250Athat%2520the%2520addition%2520of%2520PARARULE-Plus%2520can%2520increase%2520the%2520model%2527s%2520performance%2520on%250Aexamples%2520requiring%2520deeper%2520reasoning%2520depths.%2520The%2520source%2520code%2520and%2520data%2520are%250Aavailable%2520at%250Ahttps%253A//github.com/Strong-AI-Lab/Multi-Step-Deductive-Reasoning-Over-Natural-Language.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2207.14000v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Step%20Deductive%20Reasoning%20Over%20Natural%20Language%3A%20An%20Empirical%20Study%0A%20%20on%20Out-of-Distribution%20Generalisation&entry.906535625=Qiming%20Bao%20and%20Alex%20Yuxuan%20Peng%20and%20Tim%20Hartill%20and%20Neset%20Tan%20and%20Zhenyun%20Deng%20and%20Michael%20Witbrock%20and%20Jiamou%20Liu&entry.1292438233=%20%20Combining%20deep%20learning%20with%20symbolic%20logic%20reasoning%20aims%20to%20capitalize%20on%0Athe%20success%20of%20both%20fields%20and%20is%20drawing%20increasing%20attention.%20Inspired%20by%0ADeepLogic%2C%20an%20end-to-end%20model%20trained%20to%20perform%20inference%20on%20logic%20programs%2C%0Awe%20introduce%20IMA-GloVe-GA%2C%20an%20iterative%20neural%20inference%20network%20for%20multi-step%0Areasoning%20expressed%20in%20natural%20language.%20In%20our%20model%2C%20reasoning%20is%20performed%0Ausing%20an%20iterative%20memory%20neural%20network%20based%20on%20RNN%20with%20a%20gated%20attention%0Amechanism.%20We%20evaluate%20IMA-GloVe-GA%20on%20three%20datasets%3A%20PARARULES%2C%20CONCEPTRULES%0AV1%20and%20CONCEPTRULES%20V2.%20Experimental%20results%20show%20DeepLogic%20with%20gated%0Aattention%20can%20achieve%20higher%20test%20accuracy%20than%20DeepLogic%20and%20other%20RNN%0Abaseline%20models.%20Our%20model%20achieves%20better%20out-of-distribution%20generalisation%0Athan%20RoBERTa-Large%20when%20the%20rules%20have%20been%20shuffled.%20Furthermore%2C%20to%20address%0Athe%20issue%20of%20unbalanced%20distribution%20of%20reasoning%20depths%20in%20the%20current%0Amulti-step%20reasoning%20datasets%2C%20we%20develop%20PARARULE-Plus%2C%20a%20large%20dataset%20with%0Amore%20examples%20that%20require%20deeper%20reasoning%20steps.%20Experimental%20results%20show%0Athat%20the%20addition%20of%20PARARULE-Plus%20can%20increase%20the%20model%27s%20performance%20on%0Aexamples%20requiring%20deeper%20reasoning%20depths.%20The%20source%20code%20and%20data%20are%0Aavailable%20at%0Ahttps%3A//github.com/Strong-AI-Lab/Multi-Step-Deductive-Reasoning-Over-Natural-Language.%0A&entry.1838667208=http%3A//arxiv.org/abs/2207.14000v4&entry.124074799=Read"},
{"title": "InstructRAG: Leveraging Retrieval-Augmented Generation on Instruction\n  Graphs for LLM-Based Task Planning", "author": "Zheng Wang and Shu Xian Teo and Jun Jie Chew and Wei Shi", "abstract": "  Recent advancements in large language models (LLMs) have enabled their use as\nagents for planning complex tasks. Existing methods typically rely on a\nthought-action-observation (TAO) process to enhance LLM performance, but these\napproaches are often constrained by the LLMs' limited knowledge of complex\ntasks. Retrieval-augmented generation (RAG) offers new opportunities by\nleveraging external databases to ground generation in retrieved information. In\nthis paper, we identify two key challenges (enlargability and transferability)\nin applying RAG to task planning. We propose InstructRAG, a novel solution\nwithin a multi-agent meta-reinforcement learning framework, to address these\nchallenges. InstructRAG includes a graph to organize past instruction paths\n(sequences of correct actions), an RL-Agent with Reinforcement Learning to\nexpand graph coverage for enlargability, and an ML-Agent with Meta-Learning to\nimprove task generalization for transferability. The two agents are trained\nend-to-end to optimize overall planning performance. Our experiments on four\nwidely used task planning datasets demonstrate that InstructRAG significantly\nenhances performance and adapts efficiently to new tasks, achieving up to a\n19.2% improvement over the best existing approach.\n", "link": "http://arxiv.org/abs/2504.13032v1", "date": "2025-04-17", "relevancy": 2.1066, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5961}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5154}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InstructRAG%3A%20Leveraging%20Retrieval-Augmented%20Generation%20on%20Instruction%0A%20%20Graphs%20for%20LLM-Based%20Task%20Planning&body=Title%3A%20InstructRAG%3A%20Leveraging%20Retrieval-Augmented%20Generation%20on%20Instruction%0A%20%20Graphs%20for%20LLM-Based%20Task%20Planning%0AAuthor%3A%20Zheng%20Wang%20and%20Shu%20Xian%20Teo%20and%20Jun%20Jie%20Chew%20and%20Wei%20Shi%0AAbstract%3A%20%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20enabled%20their%20use%20as%0Aagents%20for%20planning%20complex%20tasks.%20Existing%20methods%20typically%20rely%20on%20a%0Athought-action-observation%20%28TAO%29%20process%20to%20enhance%20LLM%20performance%2C%20but%20these%0Aapproaches%20are%20often%20constrained%20by%20the%20LLMs%27%20limited%20knowledge%20of%20complex%0Atasks.%20Retrieval-augmented%20generation%20%28RAG%29%20offers%20new%20opportunities%20by%0Aleveraging%20external%20databases%20to%20ground%20generation%20in%20retrieved%20information.%20In%0Athis%20paper%2C%20we%20identify%20two%20key%20challenges%20%28enlargability%20and%20transferability%29%0Ain%20applying%20RAG%20to%20task%20planning.%20We%20propose%20InstructRAG%2C%20a%20novel%20solution%0Awithin%20a%20multi-agent%20meta-reinforcement%20learning%20framework%2C%20to%20address%20these%0Achallenges.%20InstructRAG%20includes%20a%20graph%20to%20organize%20past%20instruction%20paths%0A%28sequences%20of%20correct%20actions%29%2C%20an%20RL-Agent%20with%20Reinforcement%20Learning%20to%0Aexpand%20graph%20coverage%20for%20enlargability%2C%20and%20an%20ML-Agent%20with%20Meta-Learning%20to%0Aimprove%20task%20generalization%20for%20transferability.%20The%20two%20agents%20are%20trained%0Aend-to-end%20to%20optimize%20overall%20planning%20performance.%20Our%20experiments%20on%20four%0Awidely%20used%20task%20planning%20datasets%20demonstrate%20that%20InstructRAG%20significantly%0Aenhances%20performance%20and%20adapts%20efficiently%20to%20new%20tasks%2C%20achieving%20up%20to%20a%0A19.2%25%20improvement%20over%20the%20best%20existing%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13032v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstructRAG%253A%2520Leveraging%2520Retrieval-Augmented%2520Generation%2520on%2520Instruction%250A%2520%2520Graphs%2520for%2520LLM-Based%2520Task%2520Planning%26entry.906535625%3DZheng%2520Wang%2520and%2520Shu%2520Xian%2520Teo%2520and%2520Jun%2520Jie%2520Chew%2520and%2520Wei%2520Shi%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520enabled%2520their%2520use%2520as%250Aagents%2520for%2520planning%2520complex%2520tasks.%2520Existing%2520methods%2520typically%2520rely%2520on%2520a%250Athought-action-observation%2520%2528TAO%2529%2520process%2520to%2520enhance%2520LLM%2520performance%252C%2520but%2520these%250Aapproaches%2520are%2520often%2520constrained%2520by%2520the%2520LLMs%2527%2520limited%2520knowledge%2520of%2520complex%250Atasks.%2520Retrieval-augmented%2520generation%2520%2528RAG%2529%2520offers%2520new%2520opportunities%2520by%250Aleveraging%2520external%2520databases%2520to%2520ground%2520generation%2520in%2520retrieved%2520information.%2520In%250Athis%2520paper%252C%2520we%2520identify%2520two%2520key%2520challenges%2520%2528enlargability%2520and%2520transferability%2529%250Ain%2520applying%2520RAG%2520to%2520task%2520planning.%2520We%2520propose%2520InstructRAG%252C%2520a%2520novel%2520solution%250Awithin%2520a%2520multi-agent%2520meta-reinforcement%2520learning%2520framework%252C%2520to%2520address%2520these%250Achallenges.%2520InstructRAG%2520includes%2520a%2520graph%2520to%2520organize%2520past%2520instruction%2520paths%250A%2528sequences%2520of%2520correct%2520actions%2529%252C%2520an%2520RL-Agent%2520with%2520Reinforcement%2520Learning%2520to%250Aexpand%2520graph%2520coverage%2520for%2520enlargability%252C%2520and%2520an%2520ML-Agent%2520with%2520Meta-Learning%2520to%250Aimprove%2520task%2520generalization%2520for%2520transferability.%2520The%2520two%2520agents%2520are%2520trained%250Aend-to-end%2520to%2520optimize%2520overall%2520planning%2520performance.%2520Our%2520experiments%2520on%2520four%250Awidely%2520used%2520task%2520planning%2520datasets%2520demonstrate%2520that%2520InstructRAG%2520significantly%250Aenhances%2520performance%2520and%2520adapts%2520efficiently%2520to%2520new%2520tasks%252C%2520achieving%2520up%2520to%2520a%250A19.2%2525%2520improvement%2520over%2520the%2520best%2520existing%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13032v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InstructRAG%3A%20Leveraging%20Retrieval-Augmented%20Generation%20on%20Instruction%0A%20%20Graphs%20for%20LLM-Based%20Task%20Planning&entry.906535625=Zheng%20Wang%20and%20Shu%20Xian%20Teo%20and%20Jun%20Jie%20Chew%20and%20Wei%20Shi&entry.1292438233=%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20enabled%20their%20use%20as%0Aagents%20for%20planning%20complex%20tasks.%20Existing%20methods%20typically%20rely%20on%20a%0Athought-action-observation%20%28TAO%29%20process%20to%20enhance%20LLM%20performance%2C%20but%20these%0Aapproaches%20are%20often%20constrained%20by%20the%20LLMs%27%20limited%20knowledge%20of%20complex%0Atasks.%20Retrieval-augmented%20generation%20%28RAG%29%20offers%20new%20opportunities%20by%0Aleveraging%20external%20databases%20to%20ground%20generation%20in%20retrieved%20information.%20In%0Athis%20paper%2C%20we%20identify%20two%20key%20challenges%20%28enlargability%20and%20transferability%29%0Ain%20applying%20RAG%20to%20task%20planning.%20We%20propose%20InstructRAG%2C%20a%20novel%20solution%0Awithin%20a%20multi-agent%20meta-reinforcement%20learning%20framework%2C%20to%20address%20these%0Achallenges.%20InstructRAG%20includes%20a%20graph%20to%20organize%20past%20instruction%20paths%0A%28sequences%20of%20correct%20actions%29%2C%20an%20RL-Agent%20with%20Reinforcement%20Learning%20to%0Aexpand%20graph%20coverage%20for%20enlargability%2C%20and%20an%20ML-Agent%20with%20Meta-Learning%20to%0Aimprove%20task%20generalization%20for%20transferability.%20The%20two%20agents%20are%20trained%0Aend-to-end%20to%20optimize%20overall%20planning%20performance.%20Our%20experiments%20on%20four%0Awidely%20used%20task%20planning%20datasets%20demonstrate%20that%20InstructRAG%20significantly%0Aenhances%20performance%20and%20adapts%20efficiently%20to%20new%20tasks%2C%20achieving%20up%20to%20a%0A19.2%25%20improvement%20over%20the%20best%20existing%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13032v1&entry.124074799=Read"},
{"title": "A Multi-task Learning Balanced Attention Convolutional Neural Network\n  Model for Few-shot Underwater Acoustic Target Recognition", "author": "Wei Huang and Shumeng Sun and Junpeng Lu and Zhenpeng Xu and Zhengyang Xiu and Hao Zhang", "abstract": "  Underwater acoustic target recognition (UATR) is of great significance for\nthe protection of marine diversity and national defense security. The\ndevelopment of deep learning provides new opportunities for UATR, but faces\nchallenges brought by the scarcity of reference samples and complex\nenvironmental interference. To address these issues, we proposes a multi-task\nbalanced channel attention convolutional neural network (MT-BCA-CNN). The\nmethod integrates a channel attention mechanism with a multi-task learning\nstrategy, constructing a shared feature extractor and multi-task classifiers to\njointly optimize target classification and feature reconstruction tasks. The\nchannel attention mechanism dynamically enhances discriminative acoustic\nfeatures such as harmonic structures while suppressing noise. Experiments on\nthe Watkins Marine Life Dataset demonstrate that MT-BCA-CNN achieves 97\\%\nclassification accuracy and 95\\% $F1$-score in 27-class few-shot scenarios,\nsignificantly outperforming traditional CNN and ACNN models, as well as popular\nstate-of-the-art UATR methods. Ablation studies confirm the synergistic\nbenefits of multi-task learning and attention mechanisms, while a dynamic\nweighting adjustment strategy effectively balances task contributions. This\nwork provides an efficient solution for few-shot underwater acoustic\nrecognition, advancing research in marine bioacoustics and sonar signal\nprocessing.\n", "link": "http://arxiv.org/abs/2504.13102v1", "date": "2025-04-17", "relevancy": 2.0942, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5355}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5256}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4884}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multi-task%20Learning%20Balanced%20Attention%20Convolutional%20Neural%20Network%0A%20%20Model%20for%20Few-shot%20Underwater%20Acoustic%20Target%20Recognition&body=Title%3A%20A%20Multi-task%20Learning%20Balanced%20Attention%20Convolutional%20Neural%20Network%0A%20%20Model%20for%20Few-shot%20Underwater%20Acoustic%20Target%20Recognition%0AAuthor%3A%20Wei%20Huang%20and%20Shumeng%20Sun%20and%20Junpeng%20Lu%20and%20Zhenpeng%20Xu%20and%20Zhengyang%20Xiu%20and%20Hao%20Zhang%0AAbstract%3A%20%20%20Underwater%20acoustic%20target%20recognition%20%28UATR%29%20is%20of%20great%20significance%20for%0Athe%20protection%20of%20marine%20diversity%20and%20national%20defense%20security.%20The%0Adevelopment%20of%20deep%20learning%20provides%20new%20opportunities%20for%20UATR%2C%20but%20faces%0Achallenges%20brought%20by%20the%20scarcity%20of%20reference%20samples%20and%20complex%0Aenvironmental%20interference.%20To%20address%20these%20issues%2C%20we%20proposes%20a%20multi-task%0Abalanced%20channel%20attention%20convolutional%20neural%20network%20%28MT-BCA-CNN%29.%20The%0Amethod%20integrates%20a%20channel%20attention%20mechanism%20with%20a%20multi-task%20learning%0Astrategy%2C%20constructing%20a%20shared%20feature%20extractor%20and%20multi-task%20classifiers%20to%0Ajointly%20optimize%20target%20classification%20and%20feature%20reconstruction%20tasks.%20The%0Achannel%20attention%20mechanism%20dynamically%20enhances%20discriminative%20acoustic%0Afeatures%20such%20as%20harmonic%20structures%20while%20suppressing%20noise.%20Experiments%20on%0Athe%20Watkins%20Marine%20Life%20Dataset%20demonstrate%20that%20MT-BCA-CNN%20achieves%2097%5C%25%0Aclassification%20accuracy%20and%2095%5C%25%20%24F1%24-score%20in%2027-class%20few-shot%20scenarios%2C%0Asignificantly%20outperforming%20traditional%20CNN%20and%20ACNN%20models%2C%20as%20well%20as%20popular%0Astate-of-the-art%20UATR%20methods.%20Ablation%20studies%20confirm%20the%20synergistic%0Abenefits%20of%20multi-task%20learning%20and%20attention%20mechanisms%2C%20while%20a%20dynamic%0Aweighting%20adjustment%20strategy%20effectively%20balances%20task%20contributions.%20This%0Awork%20provides%20an%20efficient%20solution%20for%20few-shot%20underwater%20acoustic%0Arecognition%2C%20advancing%20research%20in%20marine%20bioacoustics%20and%20sonar%20signal%0Aprocessing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13102v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multi-task%2520Learning%2520Balanced%2520Attention%2520Convolutional%2520Neural%2520Network%250A%2520%2520Model%2520for%2520Few-shot%2520Underwater%2520Acoustic%2520Target%2520Recognition%26entry.906535625%3DWei%2520Huang%2520and%2520Shumeng%2520Sun%2520and%2520Junpeng%2520Lu%2520and%2520Zhenpeng%2520Xu%2520and%2520Zhengyang%2520Xiu%2520and%2520Hao%2520Zhang%26entry.1292438233%3D%2520%2520Underwater%2520acoustic%2520target%2520recognition%2520%2528UATR%2529%2520is%2520of%2520great%2520significance%2520for%250Athe%2520protection%2520of%2520marine%2520diversity%2520and%2520national%2520defense%2520security.%2520The%250Adevelopment%2520of%2520deep%2520learning%2520provides%2520new%2520opportunities%2520for%2520UATR%252C%2520but%2520faces%250Achallenges%2520brought%2520by%2520the%2520scarcity%2520of%2520reference%2520samples%2520and%2520complex%250Aenvironmental%2520interference.%2520To%2520address%2520these%2520issues%252C%2520we%2520proposes%2520a%2520multi-task%250Abalanced%2520channel%2520attention%2520convolutional%2520neural%2520network%2520%2528MT-BCA-CNN%2529.%2520The%250Amethod%2520integrates%2520a%2520channel%2520attention%2520mechanism%2520with%2520a%2520multi-task%2520learning%250Astrategy%252C%2520constructing%2520a%2520shared%2520feature%2520extractor%2520and%2520multi-task%2520classifiers%2520to%250Ajointly%2520optimize%2520target%2520classification%2520and%2520feature%2520reconstruction%2520tasks.%2520The%250Achannel%2520attention%2520mechanism%2520dynamically%2520enhances%2520discriminative%2520acoustic%250Afeatures%2520such%2520as%2520harmonic%2520structures%2520while%2520suppressing%2520noise.%2520Experiments%2520on%250Athe%2520Watkins%2520Marine%2520Life%2520Dataset%2520demonstrate%2520that%2520MT-BCA-CNN%2520achieves%252097%255C%2525%250Aclassification%2520accuracy%2520and%252095%255C%2525%2520%2524F1%2524-score%2520in%252027-class%2520few-shot%2520scenarios%252C%250Asignificantly%2520outperforming%2520traditional%2520CNN%2520and%2520ACNN%2520models%252C%2520as%2520well%2520as%2520popular%250Astate-of-the-art%2520UATR%2520methods.%2520Ablation%2520studies%2520confirm%2520the%2520synergistic%250Abenefits%2520of%2520multi-task%2520learning%2520and%2520attention%2520mechanisms%252C%2520while%2520a%2520dynamic%250Aweighting%2520adjustment%2520strategy%2520effectively%2520balances%2520task%2520contributions.%2520This%250Awork%2520provides%2520an%2520efficient%2520solution%2520for%2520few-shot%2520underwater%2520acoustic%250Arecognition%252C%2520advancing%2520research%2520in%2520marine%2520bioacoustics%2520and%2520sonar%2520signal%250Aprocessing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13102v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multi-task%20Learning%20Balanced%20Attention%20Convolutional%20Neural%20Network%0A%20%20Model%20for%20Few-shot%20Underwater%20Acoustic%20Target%20Recognition&entry.906535625=Wei%20Huang%20and%20Shumeng%20Sun%20and%20Junpeng%20Lu%20and%20Zhenpeng%20Xu%20and%20Zhengyang%20Xiu%20and%20Hao%20Zhang&entry.1292438233=%20%20Underwater%20acoustic%20target%20recognition%20%28UATR%29%20is%20of%20great%20significance%20for%0Athe%20protection%20of%20marine%20diversity%20and%20national%20defense%20security.%20The%0Adevelopment%20of%20deep%20learning%20provides%20new%20opportunities%20for%20UATR%2C%20but%20faces%0Achallenges%20brought%20by%20the%20scarcity%20of%20reference%20samples%20and%20complex%0Aenvironmental%20interference.%20To%20address%20these%20issues%2C%20we%20proposes%20a%20multi-task%0Abalanced%20channel%20attention%20convolutional%20neural%20network%20%28MT-BCA-CNN%29.%20The%0Amethod%20integrates%20a%20channel%20attention%20mechanism%20with%20a%20multi-task%20learning%0Astrategy%2C%20constructing%20a%20shared%20feature%20extractor%20and%20multi-task%20classifiers%20to%0Ajointly%20optimize%20target%20classification%20and%20feature%20reconstruction%20tasks.%20The%0Achannel%20attention%20mechanism%20dynamically%20enhances%20discriminative%20acoustic%0Afeatures%20such%20as%20harmonic%20structures%20while%20suppressing%20noise.%20Experiments%20on%0Athe%20Watkins%20Marine%20Life%20Dataset%20demonstrate%20that%20MT-BCA-CNN%20achieves%2097%5C%25%0Aclassification%20accuracy%20and%2095%5C%25%20%24F1%24-score%20in%2027-class%20few-shot%20scenarios%2C%0Asignificantly%20outperforming%20traditional%20CNN%20and%20ACNN%20models%2C%20as%20well%20as%20popular%0Astate-of-the-art%20UATR%20methods.%20Ablation%20studies%20confirm%20the%20synergistic%0Abenefits%20of%20multi-task%20learning%20and%20attention%20mechanisms%2C%20while%20a%20dynamic%0Aweighting%20adjustment%20strategy%20effectively%20balances%20task%20contributions.%20This%0Awork%20provides%20an%20efficient%20solution%20for%20few-shot%20underwater%20acoustic%0Arecognition%2C%20advancing%20research%20in%20marine%20bioacoustics%20and%20sonar%20signal%0Aprocessing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13102v1&entry.124074799=Read"},
{"title": "Curriculum-based Sample Efficient Reinforcement Learning for Robust\n  Stabilization of a Quadrotor", "author": "Fausto Mauricio Lagos Suarez and Akshit Saradagi and Vidya Sumathy and Shruti Kotpaliwar and George Nikolakopoulos", "abstract": "  This article introduces a curriculum learning approach to develop a\nreinforcement learning-based robust stabilizing controller for a Quadrotor that\nmeets predefined performance criteria. The learning objective is to achieve\ndesired positions from random initial conditions while adhering to both\ntransient and steady-state performance specifications. This objective is\nchallenging for conventional one-stage end-to-end reinforcement learning, due\nto the strong coupling between position and orientation dynamics, the\ncomplexity in designing and tuning the reward function, and poor sample\nefficiency, which necessitates substantial computational resources and leads to\nextended convergence times. To address these challenges, this work decomposes\nthe learning objective into a three-stage curriculum that incrementally\nincreases task complexity. The curriculum begins with learning to achieve\nstable hovering from a fixed initial condition, followed by progressively\nintroducing randomization in initial positions, orientations and velocities. A\nnovel additive reward function is proposed, to incorporate transient and\nsteady-state performance specifications. The results demonstrate that the\nProximal Policy Optimization (PPO)-based curriculum learning approach, coupled\nwith the proposed reward structure, achieves superior performance compared to a\nsingle-stage PPO-trained policy with the same reward function, while\nsignificantly reducing computational resource requirements and convergence\ntime. The curriculum-trained policy's performance and robustness are thoroughly\nvalidated under random initial conditions and in the presence of disturbances.\n", "link": "http://arxiv.org/abs/2501.18490v2", "date": "2025-04-17", "relevancy": 2.0858, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5443}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.529}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Curriculum-based%20Sample%20Efficient%20Reinforcement%20Learning%20for%20Robust%0A%20%20Stabilization%20of%20a%20Quadrotor&body=Title%3A%20Curriculum-based%20Sample%20Efficient%20Reinforcement%20Learning%20for%20Robust%0A%20%20Stabilization%20of%20a%20Quadrotor%0AAuthor%3A%20Fausto%20Mauricio%20Lagos%20Suarez%20and%20Akshit%20Saradagi%20and%20Vidya%20Sumathy%20and%20Shruti%20Kotpaliwar%20and%20George%20Nikolakopoulos%0AAbstract%3A%20%20%20This%20article%20introduces%20a%20curriculum%20learning%20approach%20to%20develop%20a%0Areinforcement%20learning-based%20robust%20stabilizing%20controller%20for%20a%20Quadrotor%20that%0Ameets%20predefined%20performance%20criteria.%20The%20learning%20objective%20is%20to%20achieve%0Adesired%20positions%20from%20random%20initial%20conditions%20while%20adhering%20to%20both%0Atransient%20and%20steady-state%20performance%20specifications.%20This%20objective%20is%0Achallenging%20for%20conventional%20one-stage%20end-to-end%20reinforcement%20learning%2C%20due%0Ato%20the%20strong%20coupling%20between%20position%20and%20orientation%20dynamics%2C%20the%0Acomplexity%20in%20designing%20and%20tuning%20the%20reward%20function%2C%20and%20poor%20sample%0Aefficiency%2C%20which%20necessitates%20substantial%20computational%20resources%20and%20leads%20to%0Aextended%20convergence%20times.%20To%20address%20these%20challenges%2C%20this%20work%20decomposes%0Athe%20learning%20objective%20into%20a%20three-stage%20curriculum%20that%20incrementally%0Aincreases%20task%20complexity.%20The%20curriculum%20begins%20with%20learning%20to%20achieve%0Astable%20hovering%20from%20a%20fixed%20initial%20condition%2C%20followed%20by%20progressively%0Aintroducing%20randomization%20in%20initial%20positions%2C%20orientations%20and%20velocities.%20A%0Anovel%20additive%20reward%20function%20is%20proposed%2C%20to%20incorporate%20transient%20and%0Asteady-state%20performance%20specifications.%20The%20results%20demonstrate%20that%20the%0AProximal%20Policy%20Optimization%20%28PPO%29-based%20curriculum%20learning%20approach%2C%20coupled%0Awith%20the%20proposed%20reward%20structure%2C%20achieves%20superior%20performance%20compared%20to%20a%0Asingle-stage%20PPO-trained%20policy%20with%20the%20same%20reward%20function%2C%20while%0Asignificantly%20reducing%20computational%20resource%20requirements%20and%20convergence%0Atime.%20The%20curriculum-trained%20policy%27s%20performance%20and%20robustness%20are%20thoroughly%0Avalidated%20under%20random%20initial%20conditions%20and%20in%20the%20presence%20of%20disturbances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.18490v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCurriculum-based%2520Sample%2520Efficient%2520Reinforcement%2520Learning%2520for%2520Robust%250A%2520%2520Stabilization%2520of%2520a%2520Quadrotor%26entry.906535625%3DFausto%2520Mauricio%2520Lagos%2520Suarez%2520and%2520Akshit%2520Saradagi%2520and%2520Vidya%2520Sumathy%2520and%2520Shruti%2520Kotpaliwar%2520and%2520George%2520Nikolakopoulos%26entry.1292438233%3D%2520%2520This%2520article%2520introduces%2520a%2520curriculum%2520learning%2520approach%2520to%2520develop%2520a%250Areinforcement%2520learning-based%2520robust%2520stabilizing%2520controller%2520for%2520a%2520Quadrotor%2520that%250Ameets%2520predefined%2520performance%2520criteria.%2520The%2520learning%2520objective%2520is%2520to%2520achieve%250Adesired%2520positions%2520from%2520random%2520initial%2520conditions%2520while%2520adhering%2520to%2520both%250Atransient%2520and%2520steady-state%2520performance%2520specifications.%2520This%2520objective%2520is%250Achallenging%2520for%2520conventional%2520one-stage%2520end-to-end%2520reinforcement%2520learning%252C%2520due%250Ato%2520the%2520strong%2520coupling%2520between%2520position%2520and%2520orientation%2520dynamics%252C%2520the%250Acomplexity%2520in%2520designing%2520and%2520tuning%2520the%2520reward%2520function%252C%2520and%2520poor%2520sample%250Aefficiency%252C%2520which%2520necessitates%2520substantial%2520computational%2520resources%2520and%2520leads%2520to%250Aextended%2520convergence%2520times.%2520To%2520address%2520these%2520challenges%252C%2520this%2520work%2520decomposes%250Athe%2520learning%2520objective%2520into%2520a%2520three-stage%2520curriculum%2520that%2520incrementally%250Aincreases%2520task%2520complexity.%2520The%2520curriculum%2520begins%2520with%2520learning%2520to%2520achieve%250Astable%2520hovering%2520from%2520a%2520fixed%2520initial%2520condition%252C%2520followed%2520by%2520progressively%250Aintroducing%2520randomization%2520in%2520initial%2520positions%252C%2520orientations%2520and%2520velocities.%2520A%250Anovel%2520additive%2520reward%2520function%2520is%2520proposed%252C%2520to%2520incorporate%2520transient%2520and%250Asteady-state%2520performance%2520specifications.%2520The%2520results%2520demonstrate%2520that%2520the%250AProximal%2520Policy%2520Optimization%2520%2528PPO%2529-based%2520curriculum%2520learning%2520approach%252C%2520coupled%250Awith%2520the%2520proposed%2520reward%2520structure%252C%2520achieves%2520superior%2520performance%2520compared%2520to%2520a%250Asingle-stage%2520PPO-trained%2520policy%2520with%2520the%2520same%2520reward%2520function%252C%2520while%250Asignificantly%2520reducing%2520computational%2520resource%2520requirements%2520and%2520convergence%250Atime.%2520The%2520curriculum-trained%2520policy%2527s%2520performance%2520and%2520robustness%2520are%2520thoroughly%250Avalidated%2520under%2520random%2520initial%2520conditions%2520and%2520in%2520the%2520presence%2520of%2520disturbances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18490v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Curriculum-based%20Sample%20Efficient%20Reinforcement%20Learning%20for%20Robust%0A%20%20Stabilization%20of%20a%20Quadrotor&entry.906535625=Fausto%20Mauricio%20Lagos%20Suarez%20and%20Akshit%20Saradagi%20and%20Vidya%20Sumathy%20and%20Shruti%20Kotpaliwar%20and%20George%20Nikolakopoulos&entry.1292438233=%20%20This%20article%20introduces%20a%20curriculum%20learning%20approach%20to%20develop%20a%0Areinforcement%20learning-based%20robust%20stabilizing%20controller%20for%20a%20Quadrotor%20that%0Ameets%20predefined%20performance%20criteria.%20The%20learning%20objective%20is%20to%20achieve%0Adesired%20positions%20from%20random%20initial%20conditions%20while%20adhering%20to%20both%0Atransient%20and%20steady-state%20performance%20specifications.%20This%20objective%20is%0Achallenging%20for%20conventional%20one-stage%20end-to-end%20reinforcement%20learning%2C%20due%0Ato%20the%20strong%20coupling%20between%20position%20and%20orientation%20dynamics%2C%20the%0Acomplexity%20in%20designing%20and%20tuning%20the%20reward%20function%2C%20and%20poor%20sample%0Aefficiency%2C%20which%20necessitates%20substantial%20computational%20resources%20and%20leads%20to%0Aextended%20convergence%20times.%20To%20address%20these%20challenges%2C%20this%20work%20decomposes%0Athe%20learning%20objective%20into%20a%20three-stage%20curriculum%20that%20incrementally%0Aincreases%20task%20complexity.%20The%20curriculum%20begins%20with%20learning%20to%20achieve%0Astable%20hovering%20from%20a%20fixed%20initial%20condition%2C%20followed%20by%20progressively%0Aintroducing%20randomization%20in%20initial%20positions%2C%20orientations%20and%20velocities.%20A%0Anovel%20additive%20reward%20function%20is%20proposed%2C%20to%20incorporate%20transient%20and%0Asteady-state%20performance%20specifications.%20The%20results%20demonstrate%20that%20the%0AProximal%20Policy%20Optimization%20%28PPO%29-based%20curriculum%20learning%20approach%2C%20coupled%0Awith%20the%20proposed%20reward%20structure%2C%20achieves%20superior%20performance%20compared%20to%20a%0Asingle-stage%20PPO-trained%20policy%20with%20the%20same%20reward%20function%2C%20while%0Asignificantly%20reducing%20computational%20resource%20requirements%20and%20convergence%0Atime.%20The%20curriculum-trained%20policy%27s%20performance%20and%20robustness%20are%20thoroughly%0Avalidated%20under%20random%20initial%20conditions%20and%20in%20the%20presence%20of%20disturbances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.18490v2&entry.124074799=Read"},
{"title": "A Survey and Evaluation of Adversarial Attacks for Object Detection", "author": "Khoi Nguyen Tiet Nguyen and Wenyu Zhang and Kangkang Lu and Yuhuan Wu and Xingjian Zheng and Hui Li Tan and Liangli Zhen", "abstract": "  Deep learning models achieve remarkable accuracy in computer vision tasks,\nyet remain vulnerable to adversarial examples--carefully crafted perturbations\nto input images that can deceive these models into making confident but\nincorrect predictions. This vulnerability pose significant risks in high-stakes\napplications such as autonomous vehicles, security surveillance, and\nsafety-critical inspection systems. While the existing literature extensively\ncovers adversarial attacks in image classification, comprehensive analyses of\nsuch attacks on object detection systems remain limited. This paper presents a\nnovel taxonomic framework for categorizing adversarial attacks specific to\nobject detection architectures, synthesizes existing robustness metrics, and\nprovides a comprehensive empirical evaluation of state-of-the-art attack\nmethodologies on popular object detection models, including both traditional\ndetectors and modern detectors with vision-language pretraining. Through\nrigorous analysis of open-source attack implementations and their effectiveness\nacross diverse detection architectures, we derive key insights into attack\ncharacteristics. Furthermore, we delineate critical research gaps and emerging\nchallenges to guide future investigations in securing object detection systems\nagainst adversarial threats. Our findings establish a foundation for developing\nmore robust detection models while highlighting the urgent need for\nstandardized evaluation protocols in this rapidly evolving domain.\n", "link": "http://arxiv.org/abs/2408.01934v5", "date": "2025-04-17", "relevancy": 2.0839, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5444}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5209}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20and%20Evaluation%20of%20Adversarial%20Attacks%20for%20Object%20Detection&body=Title%3A%20A%20Survey%20and%20Evaluation%20of%20Adversarial%20Attacks%20for%20Object%20Detection%0AAuthor%3A%20Khoi%20Nguyen%20Tiet%20Nguyen%20and%20Wenyu%20Zhang%20and%20Kangkang%20Lu%20and%20Yuhuan%20Wu%20and%20Xingjian%20Zheng%20and%20Hui%20Li%20Tan%20and%20Liangli%20Zhen%0AAbstract%3A%20%20%20Deep%20learning%20models%20achieve%20remarkable%20accuracy%20in%20computer%20vision%20tasks%2C%0Ayet%20remain%20vulnerable%20to%20adversarial%20examples--carefully%20crafted%20perturbations%0Ato%20input%20images%20that%20can%20deceive%20these%20models%20into%20making%20confident%20but%0Aincorrect%20predictions.%20This%20vulnerability%20pose%20significant%20risks%20in%20high-stakes%0Aapplications%20such%20as%20autonomous%20vehicles%2C%20security%20surveillance%2C%20and%0Asafety-critical%20inspection%20systems.%20While%20the%20existing%20literature%20extensively%0Acovers%20adversarial%20attacks%20in%20image%20classification%2C%20comprehensive%20analyses%20of%0Asuch%20attacks%20on%20object%20detection%20systems%20remain%20limited.%20This%20paper%20presents%20a%0Anovel%20taxonomic%20framework%20for%20categorizing%20adversarial%20attacks%20specific%20to%0Aobject%20detection%20architectures%2C%20synthesizes%20existing%20robustness%20metrics%2C%20and%0Aprovides%20a%20comprehensive%20empirical%20evaluation%20of%20state-of-the-art%20attack%0Amethodologies%20on%20popular%20object%20detection%20models%2C%20including%20both%20traditional%0Adetectors%20and%20modern%20detectors%20with%20vision-language%20pretraining.%20Through%0Arigorous%20analysis%20of%20open-source%20attack%20implementations%20and%20their%20effectiveness%0Aacross%20diverse%20detection%20architectures%2C%20we%20derive%20key%20insights%20into%20attack%0Acharacteristics.%20Furthermore%2C%20we%20delineate%20critical%20research%20gaps%20and%20emerging%0Achallenges%20to%20guide%20future%20investigations%20in%20securing%20object%20detection%20systems%0Aagainst%20adversarial%20threats.%20Our%20findings%20establish%20a%20foundation%20for%20developing%0Amore%20robust%20detection%20models%20while%20highlighting%20the%20urgent%20need%20for%0Astandardized%20evaluation%20protocols%20in%20this%20rapidly%20evolving%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01934v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520and%2520Evaluation%2520of%2520Adversarial%2520Attacks%2520for%2520Object%2520Detection%26entry.906535625%3DKhoi%2520Nguyen%2520Tiet%2520Nguyen%2520and%2520Wenyu%2520Zhang%2520and%2520Kangkang%2520Lu%2520and%2520Yuhuan%2520Wu%2520and%2520Xingjian%2520Zheng%2520and%2520Hui%2520Li%2520Tan%2520and%2520Liangli%2520Zhen%26entry.1292438233%3D%2520%2520Deep%2520learning%2520models%2520achieve%2520remarkable%2520accuracy%2520in%2520computer%2520vision%2520tasks%252C%250Ayet%2520remain%2520vulnerable%2520to%2520adversarial%2520examples--carefully%2520crafted%2520perturbations%250Ato%2520input%2520images%2520that%2520can%2520deceive%2520these%2520models%2520into%2520making%2520confident%2520but%250Aincorrect%2520predictions.%2520This%2520vulnerability%2520pose%2520significant%2520risks%2520in%2520high-stakes%250Aapplications%2520such%2520as%2520autonomous%2520vehicles%252C%2520security%2520surveillance%252C%2520and%250Asafety-critical%2520inspection%2520systems.%2520While%2520the%2520existing%2520literature%2520extensively%250Acovers%2520adversarial%2520attacks%2520in%2520image%2520classification%252C%2520comprehensive%2520analyses%2520of%250Asuch%2520attacks%2520on%2520object%2520detection%2520systems%2520remain%2520limited.%2520This%2520paper%2520presents%2520a%250Anovel%2520taxonomic%2520framework%2520for%2520categorizing%2520adversarial%2520attacks%2520specific%2520to%250Aobject%2520detection%2520architectures%252C%2520synthesizes%2520existing%2520robustness%2520metrics%252C%2520and%250Aprovides%2520a%2520comprehensive%2520empirical%2520evaluation%2520of%2520state-of-the-art%2520attack%250Amethodologies%2520on%2520popular%2520object%2520detection%2520models%252C%2520including%2520both%2520traditional%250Adetectors%2520and%2520modern%2520detectors%2520with%2520vision-language%2520pretraining.%2520Through%250Arigorous%2520analysis%2520of%2520open-source%2520attack%2520implementations%2520and%2520their%2520effectiveness%250Aacross%2520diverse%2520detection%2520architectures%252C%2520we%2520derive%2520key%2520insights%2520into%2520attack%250Acharacteristics.%2520Furthermore%252C%2520we%2520delineate%2520critical%2520research%2520gaps%2520and%2520emerging%250Achallenges%2520to%2520guide%2520future%2520investigations%2520in%2520securing%2520object%2520detection%2520systems%250Aagainst%2520adversarial%2520threats.%2520Our%2520findings%2520establish%2520a%2520foundation%2520for%2520developing%250Amore%2520robust%2520detection%2520models%2520while%2520highlighting%2520the%2520urgent%2520need%2520for%250Astandardized%2520evaluation%2520protocols%2520in%2520this%2520rapidly%2520evolving%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01934v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20and%20Evaluation%20of%20Adversarial%20Attacks%20for%20Object%20Detection&entry.906535625=Khoi%20Nguyen%20Tiet%20Nguyen%20and%20Wenyu%20Zhang%20and%20Kangkang%20Lu%20and%20Yuhuan%20Wu%20and%20Xingjian%20Zheng%20and%20Hui%20Li%20Tan%20and%20Liangli%20Zhen&entry.1292438233=%20%20Deep%20learning%20models%20achieve%20remarkable%20accuracy%20in%20computer%20vision%20tasks%2C%0Ayet%20remain%20vulnerable%20to%20adversarial%20examples--carefully%20crafted%20perturbations%0Ato%20input%20images%20that%20can%20deceive%20these%20models%20into%20making%20confident%20but%0Aincorrect%20predictions.%20This%20vulnerability%20pose%20significant%20risks%20in%20high-stakes%0Aapplications%20such%20as%20autonomous%20vehicles%2C%20security%20surveillance%2C%20and%0Asafety-critical%20inspection%20systems.%20While%20the%20existing%20literature%20extensively%0Acovers%20adversarial%20attacks%20in%20image%20classification%2C%20comprehensive%20analyses%20of%0Asuch%20attacks%20on%20object%20detection%20systems%20remain%20limited.%20This%20paper%20presents%20a%0Anovel%20taxonomic%20framework%20for%20categorizing%20adversarial%20attacks%20specific%20to%0Aobject%20detection%20architectures%2C%20synthesizes%20existing%20robustness%20metrics%2C%20and%0Aprovides%20a%20comprehensive%20empirical%20evaluation%20of%20state-of-the-art%20attack%0Amethodologies%20on%20popular%20object%20detection%20models%2C%20including%20both%20traditional%0Adetectors%20and%20modern%20detectors%20with%20vision-language%20pretraining.%20Through%0Arigorous%20analysis%20of%20open-source%20attack%20implementations%20and%20their%20effectiveness%0Aacross%20diverse%20detection%20architectures%2C%20we%20derive%20key%20insights%20into%20attack%0Acharacteristics.%20Furthermore%2C%20we%20delineate%20critical%20research%20gaps%20and%20emerging%0Achallenges%20to%20guide%20future%20investigations%20in%20securing%20object%20detection%20systems%0Aagainst%20adversarial%20threats.%20Our%20findings%20establish%20a%20foundation%20for%20developing%0Amore%20robust%20detection%20models%20while%20highlighting%20the%20urgent%20need%20for%0Astandardized%20evaluation%20protocols%20in%20this%20rapidly%20evolving%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01934v5&entry.124074799=Read"},
{"title": "Krysalis Hand: A Lightweight, High-Payload, 18-DoF Anthropomorphic\n  End-Effector for Robotic Learning and Dexterous Manipulation", "author": "Al Arsh Basheer and Justin Chang and Yuyang Chen and David Kim and Iman Soltani", "abstract": "  This paper presents the Krysalis Hand, a five-finger robotic end-effector\nthat combines a lightweight design, high payload capacity, and a high number of\ndegrees of freedom (DoF) to enable dexterous manipulation in both industrial\nand research settings. This design integrates the actuators within the hand\nwhile maintaining an anthropomorphic form. Each finger joint features a\nself-locking mechanism that allows the hand to sustain large external forces\nwithout active motor engagement. This approach shifts the payload limitation\nfrom the motor strength to the mechanical strength of the hand, allowing the\nuse of smaller, more cost-effective motors. With 18 DoF and weighing only 790\ngrams, the Krysalis Hand delivers an active squeezing force of 10 N per finger\nand supports a passive payload capacity exceeding 10 lbs. These characteristics\nmake Krysalis Hand one of the lightest, strongest, and most dexterous robotic\nend-effectors of its kind. Experimental evaluations validate its ability to\nperform intricate manipulation tasks and handle heavy payloads, underscoring\nits potential for industrial applications as well as academic research. All\ncode related to the Krysalis Hand, including control and teleoperation, is\navailable on the project GitHub repository:\nhttps://github.com/Soltanilara/Krysalis_Hand\n", "link": "http://arxiv.org/abs/2504.12967v1", "date": "2025-04-17", "relevancy": 2.0801, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5455}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5214}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Krysalis%20Hand%3A%20A%20Lightweight%2C%20High-Payload%2C%2018-DoF%20Anthropomorphic%0A%20%20End-Effector%20for%20Robotic%20Learning%20and%20Dexterous%20Manipulation&body=Title%3A%20Krysalis%20Hand%3A%20A%20Lightweight%2C%20High-Payload%2C%2018-DoF%20Anthropomorphic%0A%20%20End-Effector%20for%20Robotic%20Learning%20and%20Dexterous%20Manipulation%0AAuthor%3A%20Al%20Arsh%20Basheer%20and%20Justin%20Chang%20and%20Yuyang%20Chen%20and%20David%20Kim%20and%20Iman%20Soltani%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20Krysalis%20Hand%2C%20a%20five-finger%20robotic%20end-effector%0Athat%20combines%20a%20lightweight%20design%2C%20high%20payload%20capacity%2C%20and%20a%20high%20number%20of%0Adegrees%20of%20freedom%20%28DoF%29%20to%20enable%20dexterous%20manipulation%20in%20both%20industrial%0Aand%20research%20settings.%20This%20design%20integrates%20the%20actuators%20within%20the%20hand%0Awhile%20maintaining%20an%20anthropomorphic%20form.%20Each%20finger%20joint%20features%20a%0Aself-locking%20mechanism%20that%20allows%20the%20hand%20to%20sustain%20large%20external%20forces%0Awithout%20active%20motor%20engagement.%20This%20approach%20shifts%20the%20payload%20limitation%0Afrom%20the%20motor%20strength%20to%20the%20mechanical%20strength%20of%20the%20hand%2C%20allowing%20the%0Ause%20of%20smaller%2C%20more%20cost-effective%20motors.%20With%2018%20DoF%20and%20weighing%20only%20790%0Agrams%2C%20the%20Krysalis%20Hand%20delivers%20an%20active%20squeezing%20force%20of%2010%20N%20per%20finger%0Aand%20supports%20a%20passive%20payload%20capacity%20exceeding%2010%20lbs.%20These%20characteristics%0Amake%20Krysalis%20Hand%20one%20of%20the%20lightest%2C%20strongest%2C%20and%20most%20dexterous%20robotic%0Aend-effectors%20of%20its%20kind.%20Experimental%20evaluations%20validate%20its%20ability%20to%0Aperform%20intricate%20manipulation%20tasks%20and%20handle%20heavy%20payloads%2C%20underscoring%0Aits%20potential%20for%20industrial%20applications%20as%20well%20as%20academic%20research.%20All%0Acode%20related%20to%20the%20Krysalis%20Hand%2C%20including%20control%20and%20teleoperation%2C%20is%0Aavailable%20on%20the%20project%20GitHub%20repository%3A%0Ahttps%3A//github.com/Soltanilara/Krysalis_Hand%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKrysalis%2520Hand%253A%2520A%2520Lightweight%252C%2520High-Payload%252C%252018-DoF%2520Anthropomorphic%250A%2520%2520End-Effector%2520for%2520Robotic%2520Learning%2520and%2520Dexterous%2520Manipulation%26entry.906535625%3DAl%2520Arsh%2520Basheer%2520and%2520Justin%2520Chang%2520and%2520Yuyang%2520Chen%2520and%2520David%2520Kim%2520and%2520Iman%2520Soltani%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520Krysalis%2520Hand%252C%2520a%2520five-finger%2520robotic%2520end-effector%250Athat%2520combines%2520a%2520lightweight%2520design%252C%2520high%2520payload%2520capacity%252C%2520and%2520a%2520high%2520number%2520of%250Adegrees%2520of%2520freedom%2520%2528DoF%2529%2520to%2520enable%2520dexterous%2520manipulation%2520in%2520both%2520industrial%250Aand%2520research%2520settings.%2520This%2520design%2520integrates%2520the%2520actuators%2520within%2520the%2520hand%250Awhile%2520maintaining%2520an%2520anthropomorphic%2520form.%2520Each%2520finger%2520joint%2520features%2520a%250Aself-locking%2520mechanism%2520that%2520allows%2520the%2520hand%2520to%2520sustain%2520large%2520external%2520forces%250Awithout%2520active%2520motor%2520engagement.%2520This%2520approach%2520shifts%2520the%2520payload%2520limitation%250Afrom%2520the%2520motor%2520strength%2520to%2520the%2520mechanical%2520strength%2520of%2520the%2520hand%252C%2520allowing%2520the%250Ause%2520of%2520smaller%252C%2520more%2520cost-effective%2520motors.%2520With%252018%2520DoF%2520and%2520weighing%2520only%2520790%250Agrams%252C%2520the%2520Krysalis%2520Hand%2520delivers%2520an%2520active%2520squeezing%2520force%2520of%252010%2520N%2520per%2520finger%250Aand%2520supports%2520a%2520passive%2520payload%2520capacity%2520exceeding%252010%2520lbs.%2520These%2520characteristics%250Amake%2520Krysalis%2520Hand%2520one%2520of%2520the%2520lightest%252C%2520strongest%252C%2520and%2520most%2520dexterous%2520robotic%250Aend-effectors%2520of%2520its%2520kind.%2520Experimental%2520evaluations%2520validate%2520its%2520ability%2520to%250Aperform%2520intricate%2520manipulation%2520tasks%2520and%2520handle%2520heavy%2520payloads%252C%2520underscoring%250Aits%2520potential%2520for%2520industrial%2520applications%2520as%2520well%2520as%2520academic%2520research.%2520All%250Acode%2520related%2520to%2520the%2520Krysalis%2520Hand%252C%2520including%2520control%2520and%2520teleoperation%252C%2520is%250Aavailable%2520on%2520the%2520project%2520GitHub%2520repository%253A%250Ahttps%253A//github.com/Soltanilara/Krysalis_Hand%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Krysalis%20Hand%3A%20A%20Lightweight%2C%20High-Payload%2C%2018-DoF%20Anthropomorphic%0A%20%20End-Effector%20for%20Robotic%20Learning%20and%20Dexterous%20Manipulation&entry.906535625=Al%20Arsh%20Basheer%20and%20Justin%20Chang%20and%20Yuyang%20Chen%20and%20David%20Kim%20and%20Iman%20Soltani&entry.1292438233=%20%20This%20paper%20presents%20the%20Krysalis%20Hand%2C%20a%20five-finger%20robotic%20end-effector%0Athat%20combines%20a%20lightweight%20design%2C%20high%20payload%20capacity%2C%20and%20a%20high%20number%20of%0Adegrees%20of%20freedom%20%28DoF%29%20to%20enable%20dexterous%20manipulation%20in%20both%20industrial%0Aand%20research%20settings.%20This%20design%20integrates%20the%20actuators%20within%20the%20hand%0Awhile%20maintaining%20an%20anthropomorphic%20form.%20Each%20finger%20joint%20features%20a%0Aself-locking%20mechanism%20that%20allows%20the%20hand%20to%20sustain%20large%20external%20forces%0Awithout%20active%20motor%20engagement.%20This%20approach%20shifts%20the%20payload%20limitation%0Afrom%20the%20motor%20strength%20to%20the%20mechanical%20strength%20of%20the%20hand%2C%20allowing%20the%0Ause%20of%20smaller%2C%20more%20cost-effective%20motors.%20With%2018%20DoF%20and%20weighing%20only%20790%0Agrams%2C%20the%20Krysalis%20Hand%20delivers%20an%20active%20squeezing%20force%20of%2010%20N%20per%20finger%0Aand%20supports%20a%20passive%20payload%20capacity%20exceeding%2010%20lbs.%20These%20characteristics%0Amake%20Krysalis%20Hand%20one%20of%20the%20lightest%2C%20strongest%2C%20and%20most%20dexterous%20robotic%0Aend-effectors%20of%20its%20kind.%20Experimental%20evaluations%20validate%20its%20ability%20to%0Aperform%20intricate%20manipulation%20tasks%20and%20handle%20heavy%20payloads%2C%20underscoring%0Aits%20potential%20for%20industrial%20applications%20as%20well%20as%20academic%20research.%20All%0Acode%20related%20to%20the%20Krysalis%20Hand%2C%20including%20control%20and%20teleoperation%2C%20is%0Aavailable%20on%20the%20project%20GitHub%20repository%3A%0Ahttps%3A//github.com/Soltanilara/Krysalis_Hand%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12967v1&entry.124074799=Read"},
{"title": "Abstract Meaning Representation-Based Logic-Driven Data Augmentation for\n  Logical Reasoning", "author": "Qiming Bao and Alex Yuxuan Peng and Zhenyun Deng and Wanjun Zhong and Gael Gendron and Timothy Pistotti and Neset Tan and Nathan Young and Yang Chen and Yonghua Zhu and Paul Denny and Michael Witbrock and Jiamou Liu", "abstract": "  Combining large language models with logical reasoning enhances their\ncapacity to address problems in a robust and reliable manner. Nevertheless, the\nintricate nature of logical reasoning poses challenges when gathering reliable\ndata from the web to build comprehensive training datasets, subsequently\naffecting performance on downstream tasks. To address this, we introduce a\nnovel logic-driven data augmentation approach, AMR-LDA. AMR-LDA converts the\noriginal text into an Abstract Meaning Representation (AMR) graph, a structured\nsemantic representation that encapsulates the logical structure of the\nsentence, upon which operations are performed to generate logically modified\nAMR graphs. The modified AMR graphs are subsequently converted back into text\nto create augmented data. Notably, our methodology is architecture-agnostic and\nenhances both generative large language models, such as GPT-3.5 and GPT-4,\nthrough prompt augmentation, and discriminative large language models through\ncontrastive learning with logic-driven data augmentation. Empirical evidence\nunderscores the efficacy of our proposed method with improvement in performance\nacross seven downstream tasks, such as reading comprehension requiring logical\nreasoning, textual entailment, and natural language inference. Furthermore, our\nmethod leads on the ReClor leaderboard at\nhttps://eval.ai/web/challenges/challenge-page/503/leaderboard/1347. The source\ncode and data are publicly available at\nhttps://github.com/Strong-AI-Lab/Logical-Equivalence-driven-AMR-Data-Augmentation-for-Representation-Learning.\n", "link": "http://arxiv.org/abs/2305.12599v7", "date": "2025-04-17", "relevancy": 2.0754, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5245}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5177}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Abstract%20Meaning%20Representation-Based%20Logic-Driven%20Data%20Augmentation%20for%0A%20%20Logical%20Reasoning&body=Title%3A%20Abstract%20Meaning%20Representation-Based%20Logic-Driven%20Data%20Augmentation%20for%0A%20%20Logical%20Reasoning%0AAuthor%3A%20Qiming%20Bao%20and%20Alex%20Yuxuan%20Peng%20and%20Zhenyun%20Deng%20and%20Wanjun%20Zhong%20and%20Gael%20Gendron%20and%20Timothy%20Pistotti%20and%20Neset%20Tan%20and%20Nathan%20Young%20and%20Yang%20Chen%20and%20Yonghua%20Zhu%20and%20Paul%20Denny%20and%20Michael%20Witbrock%20and%20Jiamou%20Liu%0AAbstract%3A%20%20%20Combining%20large%20language%20models%20with%20logical%20reasoning%20enhances%20their%0Acapacity%20to%20address%20problems%20in%20a%20robust%20and%20reliable%20manner.%20Nevertheless%2C%20the%0Aintricate%20nature%20of%20logical%20reasoning%20poses%20challenges%20when%20gathering%20reliable%0Adata%20from%20the%20web%20to%20build%20comprehensive%20training%20datasets%2C%20subsequently%0Aaffecting%20performance%20on%20downstream%20tasks.%20To%20address%20this%2C%20we%20introduce%20a%0Anovel%20logic-driven%20data%20augmentation%20approach%2C%20AMR-LDA.%20AMR-LDA%20converts%20the%0Aoriginal%20text%20into%20an%20Abstract%20Meaning%20Representation%20%28AMR%29%20graph%2C%20a%20structured%0Asemantic%20representation%20that%20encapsulates%20the%20logical%20structure%20of%20the%0Asentence%2C%20upon%20which%20operations%20are%20performed%20to%20generate%20logically%20modified%0AAMR%20graphs.%20The%20modified%20AMR%20graphs%20are%20subsequently%20converted%20back%20into%20text%0Ato%20create%20augmented%20data.%20Notably%2C%20our%20methodology%20is%20architecture-agnostic%20and%0Aenhances%20both%20generative%20large%20language%20models%2C%20such%20as%20GPT-3.5%20and%20GPT-4%2C%0Athrough%20prompt%20augmentation%2C%20and%20discriminative%20large%20language%20models%20through%0Acontrastive%20learning%20with%20logic-driven%20data%20augmentation.%20Empirical%20evidence%0Aunderscores%20the%20efficacy%20of%20our%20proposed%20method%20with%20improvement%20in%20performance%0Aacross%20seven%20downstream%20tasks%2C%20such%20as%20reading%20comprehension%20requiring%20logical%0Areasoning%2C%20textual%20entailment%2C%20and%20natural%20language%20inference.%20Furthermore%2C%20our%0Amethod%20leads%20on%20the%20ReClor%20leaderboard%20at%0Ahttps%3A//eval.ai/web/challenges/challenge-page/503/leaderboard/1347.%20The%20source%0Acode%20and%20data%20are%20publicly%20available%20at%0Ahttps%3A//github.com/Strong-AI-Lab/Logical-Equivalence-driven-AMR-Data-Augmentation-for-Representation-Learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.12599v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAbstract%2520Meaning%2520Representation-Based%2520Logic-Driven%2520Data%2520Augmentation%2520for%250A%2520%2520Logical%2520Reasoning%26entry.906535625%3DQiming%2520Bao%2520and%2520Alex%2520Yuxuan%2520Peng%2520and%2520Zhenyun%2520Deng%2520and%2520Wanjun%2520Zhong%2520and%2520Gael%2520Gendron%2520and%2520Timothy%2520Pistotti%2520and%2520Neset%2520Tan%2520and%2520Nathan%2520Young%2520and%2520Yang%2520Chen%2520and%2520Yonghua%2520Zhu%2520and%2520Paul%2520Denny%2520and%2520Michael%2520Witbrock%2520and%2520Jiamou%2520Liu%26entry.1292438233%3D%2520%2520Combining%2520large%2520language%2520models%2520with%2520logical%2520reasoning%2520enhances%2520their%250Acapacity%2520to%2520address%2520problems%2520in%2520a%2520robust%2520and%2520reliable%2520manner.%2520Nevertheless%252C%2520the%250Aintricate%2520nature%2520of%2520logical%2520reasoning%2520poses%2520challenges%2520when%2520gathering%2520reliable%250Adata%2520from%2520the%2520web%2520to%2520build%2520comprehensive%2520training%2520datasets%252C%2520subsequently%250Aaffecting%2520performance%2520on%2520downstream%2520tasks.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%250Anovel%2520logic-driven%2520data%2520augmentation%2520approach%252C%2520AMR-LDA.%2520AMR-LDA%2520converts%2520the%250Aoriginal%2520text%2520into%2520an%2520Abstract%2520Meaning%2520Representation%2520%2528AMR%2529%2520graph%252C%2520a%2520structured%250Asemantic%2520representation%2520that%2520encapsulates%2520the%2520logical%2520structure%2520of%2520the%250Asentence%252C%2520upon%2520which%2520operations%2520are%2520performed%2520to%2520generate%2520logically%2520modified%250AAMR%2520graphs.%2520The%2520modified%2520AMR%2520graphs%2520are%2520subsequently%2520converted%2520back%2520into%2520text%250Ato%2520create%2520augmented%2520data.%2520Notably%252C%2520our%2520methodology%2520is%2520architecture-agnostic%2520and%250Aenhances%2520both%2520generative%2520large%2520language%2520models%252C%2520such%2520as%2520GPT-3.5%2520and%2520GPT-4%252C%250Athrough%2520prompt%2520augmentation%252C%2520and%2520discriminative%2520large%2520language%2520models%2520through%250Acontrastive%2520learning%2520with%2520logic-driven%2520data%2520augmentation.%2520Empirical%2520evidence%250Aunderscores%2520the%2520efficacy%2520of%2520our%2520proposed%2520method%2520with%2520improvement%2520in%2520performance%250Aacross%2520seven%2520downstream%2520tasks%252C%2520such%2520as%2520reading%2520comprehension%2520requiring%2520logical%250Areasoning%252C%2520textual%2520entailment%252C%2520and%2520natural%2520language%2520inference.%2520Furthermore%252C%2520our%250Amethod%2520leads%2520on%2520the%2520ReClor%2520leaderboard%2520at%250Ahttps%253A//eval.ai/web/challenges/challenge-page/503/leaderboard/1347.%2520The%2520source%250Acode%2520and%2520data%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/Strong-AI-Lab/Logical-Equivalence-driven-AMR-Data-Augmentation-for-Representation-Learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.12599v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Abstract%20Meaning%20Representation-Based%20Logic-Driven%20Data%20Augmentation%20for%0A%20%20Logical%20Reasoning&entry.906535625=Qiming%20Bao%20and%20Alex%20Yuxuan%20Peng%20and%20Zhenyun%20Deng%20and%20Wanjun%20Zhong%20and%20Gael%20Gendron%20and%20Timothy%20Pistotti%20and%20Neset%20Tan%20and%20Nathan%20Young%20and%20Yang%20Chen%20and%20Yonghua%20Zhu%20and%20Paul%20Denny%20and%20Michael%20Witbrock%20and%20Jiamou%20Liu&entry.1292438233=%20%20Combining%20large%20language%20models%20with%20logical%20reasoning%20enhances%20their%0Acapacity%20to%20address%20problems%20in%20a%20robust%20and%20reliable%20manner.%20Nevertheless%2C%20the%0Aintricate%20nature%20of%20logical%20reasoning%20poses%20challenges%20when%20gathering%20reliable%0Adata%20from%20the%20web%20to%20build%20comprehensive%20training%20datasets%2C%20subsequently%0Aaffecting%20performance%20on%20downstream%20tasks.%20To%20address%20this%2C%20we%20introduce%20a%0Anovel%20logic-driven%20data%20augmentation%20approach%2C%20AMR-LDA.%20AMR-LDA%20converts%20the%0Aoriginal%20text%20into%20an%20Abstract%20Meaning%20Representation%20%28AMR%29%20graph%2C%20a%20structured%0Asemantic%20representation%20that%20encapsulates%20the%20logical%20structure%20of%20the%0Asentence%2C%20upon%20which%20operations%20are%20performed%20to%20generate%20logically%20modified%0AAMR%20graphs.%20The%20modified%20AMR%20graphs%20are%20subsequently%20converted%20back%20into%20text%0Ato%20create%20augmented%20data.%20Notably%2C%20our%20methodology%20is%20architecture-agnostic%20and%0Aenhances%20both%20generative%20large%20language%20models%2C%20such%20as%20GPT-3.5%20and%20GPT-4%2C%0Athrough%20prompt%20augmentation%2C%20and%20discriminative%20large%20language%20models%20through%0Acontrastive%20learning%20with%20logic-driven%20data%20augmentation.%20Empirical%20evidence%0Aunderscores%20the%20efficacy%20of%20our%20proposed%20method%20with%20improvement%20in%20performance%0Aacross%20seven%20downstream%20tasks%2C%20such%20as%20reading%20comprehension%20requiring%20logical%0Areasoning%2C%20textual%20entailment%2C%20and%20natural%20language%20inference.%20Furthermore%2C%20our%0Amethod%20leads%20on%20the%20ReClor%20leaderboard%20at%0Ahttps%3A//eval.ai/web/challenges/challenge-page/503/leaderboard/1347.%20The%20source%0Acode%20and%20data%20are%20publicly%20available%20at%0Ahttps%3A//github.com/Strong-AI-Lab/Logical-Equivalence-driven-AMR-Data-Augmentation-for-Representation-Learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.12599v7&entry.124074799=Read"},
{"title": "$\\texttt{Complex-Edit}$: CoT-Like Instruction Generation for\n  Complexity-Controllable Image Editing Benchmark", "author": "Siwei Yang and Mude Hui and Bingchen Zhao and Yuyin Zhou and Nataniel Ruiz and Cihang Xie", "abstract": "  We introduce $\\texttt{Complex-Edit}$, a comprehensive benchmark designed to\nsystematically evaluate instruction-based image editing models across\ninstructions of varying complexity. To develop this benchmark, we harness\nGPT-4o to automatically collect a diverse set of editing instructions at scale.\nOur approach follows a well-structured ``Chain-of-Edit'' pipeline: we first\ngenerate individual atomic editing tasks independently and then integrate them\nto form cohesive, complex instructions. Additionally, we introduce a suite of\nmetrics to assess various aspects of editing performance, along with a\nVLM-based auto-evaluation pipeline that supports large-scale assessments. Our\nbenchmark yields several notable insights: 1) Open-source models significantly\nunderperform relative to proprietary, closed-source models, with the\nperformance gap widening as instruction complexity increases; 2) Increased\ninstructional complexity primarily impairs the models' ability to retain key\nelements from the input images and to preserve the overall aesthetic quality;\n3) Decomposing a complex instruction into a sequence of atomic steps, executed\nin a step-by-step manner, substantially degrades performance across multiple\nmetrics; 4) A straightforward Best-of-N selection strategy improves results for\nboth direct editing and the step-by-step sequential approach; and 5) We observe\na ``curse of synthetic data'': when synthetic data is involved in model\ntraining, the edited images from such models tend to appear increasingly\nsynthetic as the complexity of the editing instructions rises -- a phenomenon\nthat intriguingly also manifests in the latest GPT-4o outputs.\n", "link": "http://arxiv.org/abs/2504.13143v1", "date": "2025-04-17", "relevancy": 1.4878, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5158}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5104}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24%5Ctexttt%7BComplex-Edit%7D%24%3A%20CoT-Like%20Instruction%20Generation%20for%0A%20%20Complexity-Controllable%20Image%20Editing%20Benchmark&body=Title%3A%20%24%5Ctexttt%7BComplex-Edit%7D%24%3A%20CoT-Like%20Instruction%20Generation%20for%0A%20%20Complexity-Controllable%20Image%20Editing%20Benchmark%0AAuthor%3A%20Siwei%20Yang%20and%20Mude%20Hui%20and%20Bingchen%20Zhao%20and%20Yuyin%20Zhou%20and%20Nataniel%20Ruiz%20and%20Cihang%20Xie%0AAbstract%3A%20%20%20We%20introduce%20%24%5Ctexttt%7BComplex-Edit%7D%24%2C%20a%20comprehensive%20benchmark%20designed%20to%0Asystematically%20evaluate%20instruction-based%20image%20editing%20models%20across%0Ainstructions%20of%20varying%20complexity.%20To%20develop%20this%20benchmark%2C%20we%20harness%0AGPT-4o%20to%20automatically%20collect%20a%20diverse%20set%20of%20editing%20instructions%20at%20scale.%0AOur%20approach%20follows%20a%20well-structured%20%60%60Chain-of-Edit%27%27%20pipeline%3A%20we%20first%0Agenerate%20individual%20atomic%20editing%20tasks%20independently%20and%20then%20integrate%20them%0Ato%20form%20cohesive%2C%20complex%20instructions.%20Additionally%2C%20we%20introduce%20a%20suite%20of%0Ametrics%20to%20assess%20various%20aspects%20of%20editing%20performance%2C%20along%20with%20a%0AVLM-based%20auto-evaluation%20pipeline%20that%20supports%20large-scale%20assessments.%20Our%0Abenchmark%20yields%20several%20notable%20insights%3A%201%29%20Open-source%20models%20significantly%0Aunderperform%20relative%20to%20proprietary%2C%20closed-source%20models%2C%20with%20the%0Aperformance%20gap%20widening%20as%20instruction%20complexity%20increases%3B%202%29%20Increased%0Ainstructional%20complexity%20primarily%20impairs%20the%20models%27%20ability%20to%20retain%20key%0Aelements%20from%20the%20input%20images%20and%20to%20preserve%20the%20overall%20aesthetic%20quality%3B%0A3%29%20Decomposing%20a%20complex%20instruction%20into%20a%20sequence%20of%20atomic%20steps%2C%20executed%0Ain%20a%20step-by-step%20manner%2C%20substantially%20degrades%20performance%20across%20multiple%0Ametrics%3B%204%29%20A%20straightforward%20Best-of-N%20selection%20strategy%20improves%20results%20for%0Aboth%20direct%20editing%20and%20the%20step-by-step%20sequential%20approach%3B%20and%205%29%20We%20observe%0Aa%20%60%60curse%20of%20synthetic%20data%27%27%3A%20when%20synthetic%20data%20is%20involved%20in%20model%0Atraining%2C%20the%20edited%20images%20from%20such%20models%20tend%20to%20appear%20increasingly%0Asynthetic%20as%20the%20complexity%20of%20the%20editing%20instructions%20rises%20--%20a%20phenomenon%0Athat%20intriguingly%20also%20manifests%20in%20the%20latest%20GPT-4o%20outputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13143v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524%255Ctexttt%257BComplex-Edit%257D%2524%253A%2520CoT-Like%2520Instruction%2520Generation%2520for%250A%2520%2520Complexity-Controllable%2520Image%2520Editing%2520Benchmark%26entry.906535625%3DSiwei%2520Yang%2520and%2520Mude%2520Hui%2520and%2520Bingchen%2520Zhao%2520and%2520Yuyin%2520Zhou%2520and%2520Nataniel%2520Ruiz%2520and%2520Cihang%2520Xie%26entry.1292438233%3D%2520%2520We%2520introduce%2520%2524%255Ctexttt%257BComplex-Edit%257D%2524%252C%2520a%2520comprehensive%2520benchmark%2520designed%2520to%250Asystematically%2520evaluate%2520instruction-based%2520image%2520editing%2520models%2520across%250Ainstructions%2520of%2520varying%2520complexity.%2520To%2520develop%2520this%2520benchmark%252C%2520we%2520harness%250AGPT-4o%2520to%2520automatically%2520collect%2520a%2520diverse%2520set%2520of%2520editing%2520instructions%2520at%2520scale.%250AOur%2520approach%2520follows%2520a%2520well-structured%2520%2560%2560Chain-of-Edit%2527%2527%2520pipeline%253A%2520we%2520first%250Agenerate%2520individual%2520atomic%2520editing%2520tasks%2520independently%2520and%2520then%2520integrate%2520them%250Ato%2520form%2520cohesive%252C%2520complex%2520instructions.%2520Additionally%252C%2520we%2520introduce%2520a%2520suite%2520of%250Ametrics%2520to%2520assess%2520various%2520aspects%2520of%2520editing%2520performance%252C%2520along%2520with%2520a%250AVLM-based%2520auto-evaluation%2520pipeline%2520that%2520supports%2520large-scale%2520assessments.%2520Our%250Abenchmark%2520yields%2520several%2520notable%2520insights%253A%25201%2529%2520Open-source%2520models%2520significantly%250Aunderperform%2520relative%2520to%2520proprietary%252C%2520closed-source%2520models%252C%2520with%2520the%250Aperformance%2520gap%2520widening%2520as%2520instruction%2520complexity%2520increases%253B%25202%2529%2520Increased%250Ainstructional%2520complexity%2520primarily%2520impairs%2520the%2520models%2527%2520ability%2520to%2520retain%2520key%250Aelements%2520from%2520the%2520input%2520images%2520and%2520to%2520preserve%2520the%2520overall%2520aesthetic%2520quality%253B%250A3%2529%2520Decomposing%2520a%2520complex%2520instruction%2520into%2520a%2520sequence%2520of%2520atomic%2520steps%252C%2520executed%250Ain%2520a%2520step-by-step%2520manner%252C%2520substantially%2520degrades%2520performance%2520across%2520multiple%250Ametrics%253B%25204%2529%2520A%2520straightforward%2520Best-of-N%2520selection%2520strategy%2520improves%2520results%2520for%250Aboth%2520direct%2520editing%2520and%2520the%2520step-by-step%2520sequential%2520approach%253B%2520and%25205%2529%2520We%2520observe%250Aa%2520%2560%2560curse%2520of%2520synthetic%2520data%2527%2527%253A%2520when%2520synthetic%2520data%2520is%2520involved%2520in%2520model%250Atraining%252C%2520the%2520edited%2520images%2520from%2520such%2520models%2520tend%2520to%2520appear%2520increasingly%250Asynthetic%2520as%2520the%2520complexity%2520of%2520the%2520editing%2520instructions%2520rises%2520--%2520a%2520phenomenon%250Athat%2520intriguingly%2520also%2520manifests%2520in%2520the%2520latest%2520GPT-4o%2520outputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13143v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24%5Ctexttt%7BComplex-Edit%7D%24%3A%20CoT-Like%20Instruction%20Generation%20for%0A%20%20Complexity-Controllable%20Image%20Editing%20Benchmark&entry.906535625=Siwei%20Yang%20and%20Mude%20Hui%20and%20Bingchen%20Zhao%20and%20Yuyin%20Zhou%20and%20Nataniel%20Ruiz%20and%20Cihang%20Xie&entry.1292438233=%20%20We%20introduce%20%24%5Ctexttt%7BComplex-Edit%7D%24%2C%20a%20comprehensive%20benchmark%20designed%20to%0Asystematically%20evaluate%20instruction-based%20image%20editing%20models%20across%0Ainstructions%20of%20varying%20complexity.%20To%20develop%20this%20benchmark%2C%20we%20harness%0AGPT-4o%20to%20automatically%20collect%20a%20diverse%20set%20of%20editing%20instructions%20at%20scale.%0AOur%20approach%20follows%20a%20well-structured%20%60%60Chain-of-Edit%27%27%20pipeline%3A%20we%20first%0Agenerate%20individual%20atomic%20editing%20tasks%20independently%20and%20then%20integrate%20them%0Ato%20form%20cohesive%2C%20complex%20instructions.%20Additionally%2C%20we%20introduce%20a%20suite%20of%0Ametrics%20to%20assess%20various%20aspects%20of%20editing%20performance%2C%20along%20with%20a%0AVLM-based%20auto-evaluation%20pipeline%20that%20supports%20large-scale%20assessments.%20Our%0Abenchmark%20yields%20several%20notable%20insights%3A%201%29%20Open-source%20models%20significantly%0Aunderperform%20relative%20to%20proprietary%2C%20closed-source%20models%2C%20with%20the%0Aperformance%20gap%20widening%20as%20instruction%20complexity%20increases%3B%202%29%20Increased%0Ainstructional%20complexity%20primarily%20impairs%20the%20models%27%20ability%20to%20retain%20key%0Aelements%20from%20the%20input%20images%20and%20to%20preserve%20the%20overall%20aesthetic%20quality%3B%0A3%29%20Decomposing%20a%20complex%20instruction%20into%20a%20sequence%20of%20atomic%20steps%2C%20executed%0Ain%20a%20step-by-step%20manner%2C%20substantially%20degrades%20performance%20across%20multiple%0Ametrics%3B%204%29%20A%20straightforward%20Best-of-N%20selection%20strategy%20improves%20results%20for%0Aboth%20direct%20editing%20and%20the%20step-by-step%20sequential%20approach%3B%20and%205%29%20We%20observe%0Aa%20%60%60curse%20of%20synthetic%20data%27%27%3A%20when%20synthetic%20data%20is%20involved%20in%20model%0Atraining%2C%20the%20edited%20images%20from%20such%20models%20tend%20to%20appear%20increasingly%0Asynthetic%20as%20the%20complexity%20of%20the%20editing%20instructions%20rises%20--%20a%20phenomenon%0Athat%20intriguingly%20also%20manifests%20in%20the%20latest%20GPT-4o%20outputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13143v1&entry.124074799=Read"},
{"title": "Multimodal Fake News Video Explanation: Dataset, Analysis and Evaluation", "author": "Lizhi Chen and Zhong Qian and Peifeng Li and Qiaoming Zhu", "abstract": "  Multimodal fake news videos are difficult to interpret because they require\ncomprehensive consideration of the correlation and consistency between multiple\nmodes. Existing methods deal with fake news videos as a classification problem,\nbut it's not clear why news videos are identified as fake. Without proper\nexplanation, the end user may not understand the underlying meaning of the\nfalsehood. Therefore, we propose a new problem - Fake news video Explanation\n(FNVE) - given a multimodal news post containing a video and title, our goal is\nto generate natural language explanations to reveal the falsity of the news\nvideo. To that end, we developed FakeVE, a new dataset of 2,672 fake news video\nposts that can definitively explain four real-life fake news video aspects. In\norder to understand the characteristics of fake news video explanation, we\nconducted an exploratory analysis of FakeVE from different perspectives. In\naddition, we propose a Multimodal Relation Graph Transformer (MRGT) based on\nthe architecture of multimodal Transformer to benchmark FakeVE. The empirical\nresults show that the results of the various benchmarks (adopted by FakeVE) are\nconvincing and provide a detailed analysis of the differences in explanation\ngeneration of the benchmark models.\n", "link": "http://arxiv.org/abs/2501.08514v3", "date": "2025-04-17", "relevancy": 1.983, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5118}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4958}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4893}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Fake%20News%20Video%20Explanation%3A%20Dataset%2C%20Analysis%20and%20Evaluation&body=Title%3A%20Multimodal%20Fake%20News%20Video%20Explanation%3A%20Dataset%2C%20Analysis%20and%20Evaluation%0AAuthor%3A%20Lizhi%20Chen%20and%20Zhong%20Qian%20and%20Peifeng%20Li%20and%20Qiaoming%20Zhu%0AAbstract%3A%20%20%20Multimodal%20fake%20news%20videos%20are%20difficult%20to%20interpret%20because%20they%20require%0Acomprehensive%20consideration%20of%20the%20correlation%20and%20consistency%20between%20multiple%0Amodes.%20Existing%20methods%20deal%20with%20fake%20news%20videos%20as%20a%20classification%20problem%2C%0Abut%20it%27s%20not%20clear%20why%20news%20videos%20are%20identified%20as%20fake.%20Without%20proper%0Aexplanation%2C%20the%20end%20user%20may%20not%20understand%20the%20underlying%20meaning%20of%20the%0Afalsehood.%20Therefore%2C%20we%20propose%20a%20new%20problem%20-%20Fake%20news%20video%20Explanation%0A%28FNVE%29%20-%20given%20a%20multimodal%20news%20post%20containing%20a%20video%20and%20title%2C%20our%20goal%20is%0Ato%20generate%20natural%20language%20explanations%20to%20reveal%20the%20falsity%20of%20the%20news%0Avideo.%20To%20that%20end%2C%20we%20developed%20FakeVE%2C%20a%20new%20dataset%20of%202%2C672%20fake%20news%20video%0Aposts%20that%20can%20definitively%20explain%20four%20real-life%20fake%20news%20video%20aspects.%20In%0Aorder%20to%20understand%20the%20characteristics%20of%20fake%20news%20video%20explanation%2C%20we%0Aconducted%20an%20exploratory%20analysis%20of%20FakeVE%20from%20different%20perspectives.%20In%0Aaddition%2C%20we%20propose%20a%20Multimodal%20Relation%20Graph%20Transformer%20%28MRGT%29%20based%20on%0Athe%20architecture%20of%20multimodal%20Transformer%20to%20benchmark%20FakeVE.%20The%20empirical%0Aresults%20show%20that%20the%20results%20of%20the%20various%20benchmarks%20%28adopted%20by%20FakeVE%29%20are%0Aconvincing%20and%20provide%20a%20detailed%20analysis%20of%20the%20differences%20in%20explanation%0Ageneration%20of%20the%20benchmark%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08514v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Fake%2520News%2520Video%2520Explanation%253A%2520Dataset%252C%2520Analysis%2520and%2520Evaluation%26entry.906535625%3DLizhi%2520Chen%2520and%2520Zhong%2520Qian%2520and%2520Peifeng%2520Li%2520and%2520Qiaoming%2520Zhu%26entry.1292438233%3D%2520%2520Multimodal%2520fake%2520news%2520videos%2520are%2520difficult%2520to%2520interpret%2520because%2520they%2520require%250Acomprehensive%2520consideration%2520of%2520the%2520correlation%2520and%2520consistency%2520between%2520multiple%250Amodes.%2520Existing%2520methods%2520deal%2520with%2520fake%2520news%2520videos%2520as%2520a%2520classification%2520problem%252C%250Abut%2520it%2527s%2520not%2520clear%2520why%2520news%2520videos%2520are%2520identified%2520as%2520fake.%2520Without%2520proper%250Aexplanation%252C%2520the%2520end%2520user%2520may%2520not%2520understand%2520the%2520underlying%2520meaning%2520of%2520the%250Afalsehood.%2520Therefore%252C%2520we%2520propose%2520a%2520new%2520problem%2520-%2520Fake%2520news%2520video%2520Explanation%250A%2528FNVE%2529%2520-%2520given%2520a%2520multimodal%2520news%2520post%2520containing%2520a%2520video%2520and%2520title%252C%2520our%2520goal%2520is%250Ato%2520generate%2520natural%2520language%2520explanations%2520to%2520reveal%2520the%2520falsity%2520of%2520the%2520news%250Avideo.%2520To%2520that%2520end%252C%2520we%2520developed%2520FakeVE%252C%2520a%2520new%2520dataset%2520of%25202%252C672%2520fake%2520news%2520video%250Aposts%2520that%2520can%2520definitively%2520explain%2520four%2520real-life%2520fake%2520news%2520video%2520aspects.%2520In%250Aorder%2520to%2520understand%2520the%2520characteristics%2520of%2520fake%2520news%2520video%2520explanation%252C%2520we%250Aconducted%2520an%2520exploratory%2520analysis%2520of%2520FakeVE%2520from%2520different%2520perspectives.%2520In%250Aaddition%252C%2520we%2520propose%2520a%2520Multimodal%2520Relation%2520Graph%2520Transformer%2520%2528MRGT%2529%2520based%2520on%250Athe%2520architecture%2520of%2520multimodal%2520Transformer%2520to%2520benchmark%2520FakeVE.%2520The%2520empirical%250Aresults%2520show%2520that%2520the%2520results%2520of%2520the%2520various%2520benchmarks%2520%2528adopted%2520by%2520FakeVE%2529%2520are%250Aconvincing%2520and%2520provide%2520a%2520detailed%2520analysis%2520of%2520the%2520differences%2520in%2520explanation%250Ageneration%2520of%2520the%2520benchmark%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08514v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Fake%20News%20Video%20Explanation%3A%20Dataset%2C%20Analysis%20and%20Evaluation&entry.906535625=Lizhi%20Chen%20and%20Zhong%20Qian%20and%20Peifeng%20Li%20and%20Qiaoming%20Zhu&entry.1292438233=%20%20Multimodal%20fake%20news%20videos%20are%20difficult%20to%20interpret%20because%20they%20require%0Acomprehensive%20consideration%20of%20the%20correlation%20and%20consistency%20between%20multiple%0Amodes.%20Existing%20methods%20deal%20with%20fake%20news%20videos%20as%20a%20classification%20problem%2C%0Abut%20it%27s%20not%20clear%20why%20news%20videos%20are%20identified%20as%20fake.%20Without%20proper%0Aexplanation%2C%20the%20end%20user%20may%20not%20understand%20the%20underlying%20meaning%20of%20the%0Afalsehood.%20Therefore%2C%20we%20propose%20a%20new%20problem%20-%20Fake%20news%20video%20Explanation%0A%28FNVE%29%20-%20given%20a%20multimodal%20news%20post%20containing%20a%20video%20and%20title%2C%20our%20goal%20is%0Ato%20generate%20natural%20language%20explanations%20to%20reveal%20the%20falsity%20of%20the%20news%0Avideo.%20To%20that%20end%2C%20we%20developed%20FakeVE%2C%20a%20new%20dataset%20of%202%2C672%20fake%20news%20video%0Aposts%20that%20can%20definitively%20explain%20four%20real-life%20fake%20news%20video%20aspects.%20In%0Aorder%20to%20understand%20the%20characteristics%20of%20fake%20news%20video%20explanation%2C%20we%0Aconducted%20an%20exploratory%20analysis%20of%20FakeVE%20from%20different%20perspectives.%20In%0Aaddition%2C%20we%20propose%20a%20Multimodal%20Relation%20Graph%20Transformer%20%28MRGT%29%20based%20on%0Athe%20architecture%20of%20multimodal%20Transformer%20to%20benchmark%20FakeVE.%20The%20empirical%0Aresults%20show%20that%20the%20results%20of%20the%20various%20benchmarks%20%28adopted%20by%20FakeVE%29%20are%0Aconvincing%20and%20provide%20a%20detailed%20analysis%20of%20the%20differences%20in%20explanation%0Ageneration%20of%20the%20benchmark%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08514v3&entry.124074799=Read"},
{"title": "Mesh-Informed Reduced Order Models for Aneurysm Rupture Risk Prediction", "author": "Giuseppe Alessio D'Inverno and Saeid Moradizadeh and Sajad Salavatidezfouli and Pasquale Claudio Africa and Gianluigi Rozza", "abstract": "  The complexity of the cardiovascular system needs to be accurately reproduced\nin order to promptly acknowledge health conditions; to this aim, advanced\nmultifidelity and multiphysics numerical models are crucial. On one side, Full\nOrder Models (FOMs) deliver accurate hemodynamic assessments, but their high\ncomputational demands hinder their real-time clinical application. In contrast,\nReduced Order Models (ROMs) provide more efficient yet accurate solutions,\nessential for personalized healthcare and timely clinical decision-making. In\nthis work, we explore the application of computational fluid dynamics (CFD) in\ncardiovascular medicine by integrating FOMs with ROMs for predicting the risk\nof aortic aneurysm growth and rupture. Wall Shear Stress (WSS) and the\nOscillatory Shear Index (OSI), sampled at different growth stages of the\nthoracic aortic aneurysm, are predicted by means of Graph Neural Networks\n(GNNs). GNNs exploit the natural graph structure of the mesh obtained by the\nFinite Volume (FV) discretization, taking into account the spatial local\ninformation, regardless of the dimension of the input graph. Our experimental\nvalidation framework yields promising results, confirming our method as a valid\nalternative that overcomes the curse of dimensionality.\n", "link": "http://arxiv.org/abs/2410.03802v3", "date": "2025-04-17", "relevancy": 1.9202, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5217}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4855}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mesh-Informed%20Reduced%20Order%20Models%20for%20Aneurysm%20Rupture%20Risk%20Prediction&body=Title%3A%20Mesh-Informed%20Reduced%20Order%20Models%20for%20Aneurysm%20Rupture%20Risk%20Prediction%0AAuthor%3A%20Giuseppe%20Alessio%20D%27Inverno%20and%20Saeid%20Moradizadeh%20and%20Sajad%20Salavatidezfouli%20and%20Pasquale%20Claudio%20Africa%20and%20Gianluigi%20Rozza%0AAbstract%3A%20%20%20The%20complexity%20of%20the%20cardiovascular%20system%20needs%20to%20be%20accurately%20reproduced%0Ain%20order%20to%20promptly%20acknowledge%20health%20conditions%3B%20to%20this%20aim%2C%20advanced%0Amultifidelity%20and%20multiphysics%20numerical%20models%20are%20crucial.%20On%20one%20side%2C%20Full%0AOrder%20Models%20%28FOMs%29%20deliver%20accurate%20hemodynamic%20assessments%2C%20but%20their%20high%0Acomputational%20demands%20hinder%20their%20real-time%20clinical%20application.%20In%20contrast%2C%0AReduced%20Order%20Models%20%28ROMs%29%20provide%20more%20efficient%20yet%20accurate%20solutions%2C%0Aessential%20for%20personalized%20healthcare%20and%20timely%20clinical%20decision-making.%20In%0Athis%20work%2C%20we%20explore%20the%20application%20of%20computational%20fluid%20dynamics%20%28CFD%29%20in%0Acardiovascular%20medicine%20by%20integrating%20FOMs%20with%20ROMs%20for%20predicting%20the%20risk%0Aof%20aortic%20aneurysm%20growth%20and%20rupture.%20Wall%20Shear%20Stress%20%28WSS%29%20and%20the%0AOscillatory%20Shear%20Index%20%28OSI%29%2C%20sampled%20at%20different%20growth%20stages%20of%20the%0Athoracic%20aortic%20aneurysm%2C%20are%20predicted%20by%20means%20of%20Graph%20Neural%20Networks%0A%28GNNs%29.%20GNNs%20exploit%20the%20natural%20graph%20structure%20of%20the%20mesh%20obtained%20by%20the%0AFinite%20Volume%20%28FV%29%20discretization%2C%20taking%20into%20account%20the%20spatial%20local%0Ainformation%2C%20regardless%20of%20the%20dimension%20of%20the%20input%20graph.%20Our%20experimental%0Avalidation%20framework%20yields%20promising%20results%2C%20confirming%20our%20method%20as%20a%20valid%0Aalternative%20that%20overcomes%20the%20curse%20of%20dimensionality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03802v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMesh-Informed%2520Reduced%2520Order%2520Models%2520for%2520Aneurysm%2520Rupture%2520Risk%2520Prediction%26entry.906535625%3DGiuseppe%2520Alessio%2520D%2527Inverno%2520and%2520Saeid%2520Moradizadeh%2520and%2520Sajad%2520Salavatidezfouli%2520and%2520Pasquale%2520Claudio%2520Africa%2520and%2520Gianluigi%2520Rozza%26entry.1292438233%3D%2520%2520The%2520complexity%2520of%2520the%2520cardiovascular%2520system%2520needs%2520to%2520be%2520accurately%2520reproduced%250Ain%2520order%2520to%2520promptly%2520acknowledge%2520health%2520conditions%253B%2520to%2520this%2520aim%252C%2520advanced%250Amultifidelity%2520and%2520multiphysics%2520numerical%2520models%2520are%2520crucial.%2520On%2520one%2520side%252C%2520Full%250AOrder%2520Models%2520%2528FOMs%2529%2520deliver%2520accurate%2520hemodynamic%2520assessments%252C%2520but%2520their%2520high%250Acomputational%2520demands%2520hinder%2520their%2520real-time%2520clinical%2520application.%2520In%2520contrast%252C%250AReduced%2520Order%2520Models%2520%2528ROMs%2529%2520provide%2520more%2520efficient%2520yet%2520accurate%2520solutions%252C%250Aessential%2520for%2520personalized%2520healthcare%2520and%2520timely%2520clinical%2520decision-making.%2520In%250Athis%2520work%252C%2520we%2520explore%2520the%2520application%2520of%2520computational%2520fluid%2520dynamics%2520%2528CFD%2529%2520in%250Acardiovascular%2520medicine%2520by%2520integrating%2520FOMs%2520with%2520ROMs%2520for%2520predicting%2520the%2520risk%250Aof%2520aortic%2520aneurysm%2520growth%2520and%2520rupture.%2520Wall%2520Shear%2520Stress%2520%2528WSS%2529%2520and%2520the%250AOscillatory%2520Shear%2520Index%2520%2528OSI%2529%252C%2520sampled%2520at%2520different%2520growth%2520stages%2520of%2520the%250Athoracic%2520aortic%2520aneurysm%252C%2520are%2520predicted%2520by%2520means%2520of%2520Graph%2520Neural%2520Networks%250A%2528GNNs%2529.%2520GNNs%2520exploit%2520the%2520natural%2520graph%2520structure%2520of%2520the%2520mesh%2520obtained%2520by%2520the%250AFinite%2520Volume%2520%2528FV%2529%2520discretization%252C%2520taking%2520into%2520account%2520the%2520spatial%2520local%250Ainformation%252C%2520regardless%2520of%2520the%2520dimension%2520of%2520the%2520input%2520graph.%2520Our%2520experimental%250Avalidation%2520framework%2520yields%2520promising%2520results%252C%2520confirming%2520our%2520method%2520as%2520a%2520valid%250Aalternative%2520that%2520overcomes%2520the%2520curse%2520of%2520dimensionality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03802v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mesh-Informed%20Reduced%20Order%20Models%20for%20Aneurysm%20Rupture%20Risk%20Prediction&entry.906535625=Giuseppe%20Alessio%20D%27Inverno%20and%20Saeid%20Moradizadeh%20and%20Sajad%20Salavatidezfouli%20and%20Pasquale%20Claudio%20Africa%20and%20Gianluigi%20Rozza&entry.1292438233=%20%20The%20complexity%20of%20the%20cardiovascular%20system%20needs%20to%20be%20accurately%20reproduced%0Ain%20order%20to%20promptly%20acknowledge%20health%20conditions%3B%20to%20this%20aim%2C%20advanced%0Amultifidelity%20and%20multiphysics%20numerical%20models%20are%20crucial.%20On%20one%20side%2C%20Full%0AOrder%20Models%20%28FOMs%29%20deliver%20accurate%20hemodynamic%20assessments%2C%20but%20their%20high%0Acomputational%20demands%20hinder%20their%20real-time%20clinical%20application.%20In%20contrast%2C%0AReduced%20Order%20Models%20%28ROMs%29%20provide%20more%20efficient%20yet%20accurate%20solutions%2C%0Aessential%20for%20personalized%20healthcare%20and%20timely%20clinical%20decision-making.%20In%0Athis%20work%2C%20we%20explore%20the%20application%20of%20computational%20fluid%20dynamics%20%28CFD%29%20in%0Acardiovascular%20medicine%20by%20integrating%20FOMs%20with%20ROMs%20for%20predicting%20the%20risk%0Aof%20aortic%20aneurysm%20growth%20and%20rupture.%20Wall%20Shear%20Stress%20%28WSS%29%20and%20the%0AOscillatory%20Shear%20Index%20%28OSI%29%2C%20sampled%20at%20different%20growth%20stages%20of%20the%0Athoracic%20aortic%20aneurysm%2C%20are%20predicted%20by%20means%20of%20Graph%20Neural%20Networks%0A%28GNNs%29.%20GNNs%20exploit%20the%20natural%20graph%20structure%20of%20the%20mesh%20obtained%20by%20the%0AFinite%20Volume%20%28FV%29%20discretization%2C%20taking%20into%20account%20the%20spatial%20local%0Ainformation%2C%20regardless%20of%20the%20dimension%20of%20the%20input%20graph.%20Our%20experimental%0Avalidation%20framework%20yields%20promising%20results%2C%20confirming%20our%20method%20as%20a%20valid%0Aalternative%20that%20overcomes%20the%20curse%20of%20dimensionality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03802v3&entry.124074799=Read"},
{"title": "A Robust Prototype-Based Network with Interpretable RBF Classifier\n  Foundations", "author": "Sascha Saralajew and Ashish Rana and Thomas Villmann and Ammar Shaker", "abstract": "  Prototype-based classification learning methods are known to be inherently\ninterpretable. However, this paradigm suffers from major limitations compared\nto deep models, such as lower performance. This led to the development of the\nso-called deep Prototype-Based Networks (PBNs), also known as prototypical\nparts models. In this work, we analyze these models with respect to different\nproperties, including interpretability. In particular, we focus on the\nClassification-by-Components (CBC) approach, which uses a probabilistic model\nto ensure interpretability and can be used as a shallow or deep architecture.\nWe show that this model has several shortcomings, like creating contradicting\nexplanations. Based on these findings, we propose an extension of CBC that\nsolves these issues. Moreover, we prove that this extension has robustness\nguarantees and derive a loss that optimizes robustness. Additionally, our\nanalysis shows that most (deep) PBNs are related to (deep) RBF classifiers,\nwhich implies that our robustness guarantees generalize to shallow RBF\nclassifiers. The empirical evaluation demonstrates that our deep PBN yields\nstate-of-the-art classification accuracy on different benchmarks while\nresolving the interpretability shortcomings of other approaches. Further, our\nshallow PBN variant outperforms other shallow PBNs while being inherently\ninterpretable and exhibiting provable robustness guarantees.\n", "link": "http://arxiv.org/abs/2412.15499v3", "date": "2025-04-17", "relevancy": 1.9758, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5236}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4965}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4795}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Robust%20Prototype-Based%20Network%20with%20Interpretable%20RBF%20Classifier%0A%20%20Foundations&body=Title%3A%20A%20Robust%20Prototype-Based%20Network%20with%20Interpretable%20RBF%20Classifier%0A%20%20Foundations%0AAuthor%3A%20Sascha%20Saralajew%20and%20Ashish%20Rana%20and%20Thomas%20Villmann%20and%20Ammar%20Shaker%0AAbstract%3A%20%20%20Prototype-based%20classification%20learning%20methods%20are%20known%20to%20be%20inherently%0Ainterpretable.%20However%2C%20this%20paradigm%20suffers%20from%20major%20limitations%20compared%0Ato%20deep%20models%2C%20such%20as%20lower%20performance.%20This%20led%20to%20the%20development%20of%20the%0Aso-called%20deep%20Prototype-Based%20Networks%20%28PBNs%29%2C%20also%20known%20as%20prototypical%0Aparts%20models.%20In%20this%20work%2C%20we%20analyze%20these%20models%20with%20respect%20to%20different%0Aproperties%2C%20including%20interpretability.%20In%20particular%2C%20we%20focus%20on%20the%0AClassification-by-Components%20%28CBC%29%20approach%2C%20which%20uses%20a%20probabilistic%20model%0Ato%20ensure%20interpretability%20and%20can%20be%20used%20as%20a%20shallow%20or%20deep%20architecture.%0AWe%20show%20that%20this%20model%20has%20several%20shortcomings%2C%20like%20creating%20contradicting%0Aexplanations.%20Based%20on%20these%20findings%2C%20we%20propose%20an%20extension%20of%20CBC%20that%0Asolves%20these%20issues.%20Moreover%2C%20we%20prove%20that%20this%20extension%20has%20robustness%0Aguarantees%20and%20derive%20a%20loss%20that%20optimizes%20robustness.%20Additionally%2C%20our%0Aanalysis%20shows%20that%20most%20%28deep%29%20PBNs%20are%20related%20to%20%28deep%29%20RBF%20classifiers%2C%0Awhich%20implies%20that%20our%20robustness%20guarantees%20generalize%20to%20shallow%20RBF%0Aclassifiers.%20The%20empirical%20evaluation%20demonstrates%20that%20our%20deep%20PBN%20yields%0Astate-of-the-art%20classification%20accuracy%20on%20different%20benchmarks%20while%0Aresolving%20the%20interpretability%20shortcomings%20of%20other%20approaches.%20Further%2C%20our%0Ashallow%20PBN%20variant%20outperforms%20other%20shallow%20PBNs%20while%20being%20inherently%0Ainterpretable%20and%20exhibiting%20provable%20robustness%20guarantees.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15499v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Robust%2520Prototype-Based%2520Network%2520with%2520Interpretable%2520RBF%2520Classifier%250A%2520%2520Foundations%26entry.906535625%3DSascha%2520Saralajew%2520and%2520Ashish%2520Rana%2520and%2520Thomas%2520Villmann%2520and%2520Ammar%2520Shaker%26entry.1292438233%3D%2520%2520Prototype-based%2520classification%2520learning%2520methods%2520are%2520known%2520to%2520be%2520inherently%250Ainterpretable.%2520However%252C%2520this%2520paradigm%2520suffers%2520from%2520major%2520limitations%2520compared%250Ato%2520deep%2520models%252C%2520such%2520as%2520lower%2520performance.%2520This%2520led%2520to%2520the%2520development%2520of%2520the%250Aso-called%2520deep%2520Prototype-Based%2520Networks%2520%2528PBNs%2529%252C%2520also%2520known%2520as%2520prototypical%250Aparts%2520models.%2520In%2520this%2520work%252C%2520we%2520analyze%2520these%2520models%2520with%2520respect%2520to%2520different%250Aproperties%252C%2520including%2520interpretability.%2520In%2520particular%252C%2520we%2520focus%2520on%2520the%250AClassification-by-Components%2520%2528CBC%2529%2520approach%252C%2520which%2520uses%2520a%2520probabilistic%2520model%250Ato%2520ensure%2520interpretability%2520and%2520can%2520be%2520used%2520as%2520a%2520shallow%2520or%2520deep%2520architecture.%250AWe%2520show%2520that%2520this%2520model%2520has%2520several%2520shortcomings%252C%2520like%2520creating%2520contradicting%250Aexplanations.%2520Based%2520on%2520these%2520findings%252C%2520we%2520propose%2520an%2520extension%2520of%2520CBC%2520that%250Asolves%2520these%2520issues.%2520Moreover%252C%2520we%2520prove%2520that%2520this%2520extension%2520has%2520robustness%250Aguarantees%2520and%2520derive%2520a%2520loss%2520that%2520optimizes%2520robustness.%2520Additionally%252C%2520our%250Aanalysis%2520shows%2520that%2520most%2520%2528deep%2529%2520PBNs%2520are%2520related%2520to%2520%2528deep%2529%2520RBF%2520classifiers%252C%250Awhich%2520implies%2520that%2520our%2520robustness%2520guarantees%2520generalize%2520to%2520shallow%2520RBF%250Aclassifiers.%2520The%2520empirical%2520evaluation%2520demonstrates%2520that%2520our%2520deep%2520PBN%2520yields%250Astate-of-the-art%2520classification%2520accuracy%2520on%2520different%2520benchmarks%2520while%250Aresolving%2520the%2520interpretability%2520shortcomings%2520of%2520other%2520approaches.%2520Further%252C%2520our%250Ashallow%2520PBN%2520variant%2520outperforms%2520other%2520shallow%2520PBNs%2520while%2520being%2520inherently%250Ainterpretable%2520and%2520exhibiting%2520provable%2520robustness%2520guarantees.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15499v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Robust%20Prototype-Based%20Network%20with%20Interpretable%20RBF%20Classifier%0A%20%20Foundations&entry.906535625=Sascha%20Saralajew%20and%20Ashish%20Rana%20and%20Thomas%20Villmann%20and%20Ammar%20Shaker&entry.1292438233=%20%20Prototype-based%20classification%20learning%20methods%20are%20known%20to%20be%20inherently%0Ainterpretable.%20However%2C%20this%20paradigm%20suffers%20from%20major%20limitations%20compared%0Ato%20deep%20models%2C%20such%20as%20lower%20performance.%20This%20led%20to%20the%20development%20of%20the%0Aso-called%20deep%20Prototype-Based%20Networks%20%28PBNs%29%2C%20also%20known%20as%20prototypical%0Aparts%20models.%20In%20this%20work%2C%20we%20analyze%20these%20models%20with%20respect%20to%20different%0Aproperties%2C%20including%20interpretability.%20In%20particular%2C%20we%20focus%20on%20the%0AClassification-by-Components%20%28CBC%29%20approach%2C%20which%20uses%20a%20probabilistic%20model%0Ato%20ensure%20interpretability%20and%20can%20be%20used%20as%20a%20shallow%20or%20deep%20architecture.%0AWe%20show%20that%20this%20model%20has%20several%20shortcomings%2C%20like%20creating%20contradicting%0Aexplanations.%20Based%20on%20these%20findings%2C%20we%20propose%20an%20extension%20of%20CBC%20that%0Asolves%20these%20issues.%20Moreover%2C%20we%20prove%20that%20this%20extension%20has%20robustness%0Aguarantees%20and%20derive%20a%20loss%20that%20optimizes%20robustness.%20Additionally%2C%20our%0Aanalysis%20shows%20that%20most%20%28deep%29%20PBNs%20are%20related%20to%20%28deep%29%20RBF%20classifiers%2C%0Awhich%20implies%20that%20our%20robustness%20guarantees%20generalize%20to%20shallow%20RBF%0Aclassifiers.%20The%20empirical%20evaluation%20demonstrates%20that%20our%20deep%20PBN%20yields%0Astate-of-the-art%20classification%20accuracy%20on%20different%20benchmarks%20while%0Aresolving%20the%20interpretability%20shortcomings%20of%20other%20approaches.%20Further%2C%20our%0Ashallow%20PBN%20variant%20outperforms%20other%20shallow%20PBNs%20while%20being%20inherently%0Ainterpretable%20and%20exhibiting%20provable%20robustness%20guarantees.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15499v3&entry.124074799=Read"},
{"title": "EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text\n  Prompting", "author": "Guanrou Yang and Chen Yang and Qian Chen and Ziyang Ma and Wenxi Chen and Wen Wang and Tianrui Wang and Yifan Yang and Zhikang Niu and Wenrui Liu and Fan Yu and Zhihao Du and Zhifu Gao and ShiLiang Zhang and Xie Chen", "abstract": "  Human speech goes beyond the mere transfer of information; it is a profound\nexchange of emotions and a connection between individuals. While Text-to-Speech\n(TTS) models have made huge progress, they still face challenges in controlling\nthe emotional expression in the generated speech. In this work, we propose\nEmoVoice, a novel emotion-controllable TTS model that exploits large language\nmodels (LLMs) to enable fine-grained freestyle natural language emotion\ncontrol, and a phoneme boost variant design that makes the model output phoneme\ntokens and audio tokens in parallel to enhance content consistency, inspired by\nchain-of-thought (CoT) and modality-of-thought (CoM) techniques. Besides, we\nintroduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring\nexpressive speech and fine-grained emotion labels with natural language\ndescriptions. EmoVoice achieves state-of-the-art performance on the English\nEmoVoice-DB test set using only synthetic training data, and on the Chinese\nSecap test set using our in-house data. We further investigate the reliability\nof existing emotion evaluation metrics and their alignment with human\nperceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and\nGemini to assess emotional speech. Demo samples are available at\nhttps://anonymous.4open.science/r/EmoVoice-DF55. Dataset, code, and checkpoints\nwill be released.\n", "link": "http://arxiv.org/abs/2504.12867v1", "date": "2025-04-17", "relevancy": 1.9165, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4804}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4804}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EmoVoice%3A%20LLM-based%20Emotional%20Text-To-Speech%20Model%20with%20Freestyle%20Text%0A%20%20Prompting&body=Title%3A%20EmoVoice%3A%20LLM-based%20Emotional%20Text-To-Speech%20Model%20with%20Freestyle%20Text%0A%20%20Prompting%0AAuthor%3A%20Guanrou%20Yang%20and%20Chen%20Yang%20and%20Qian%20Chen%20and%20Ziyang%20Ma%20and%20Wenxi%20Chen%20and%20Wen%20Wang%20and%20Tianrui%20Wang%20and%20Yifan%20Yang%20and%20Zhikang%20Niu%20and%20Wenrui%20Liu%20and%20Fan%20Yu%20and%20Zhihao%20Du%20and%20Zhifu%20Gao%20and%20ShiLiang%20Zhang%20and%20Xie%20Chen%0AAbstract%3A%20%20%20Human%20speech%20goes%20beyond%20the%20mere%20transfer%20of%20information%3B%20it%20is%20a%20profound%0Aexchange%20of%20emotions%20and%20a%20connection%20between%20individuals.%20While%20Text-to-Speech%0A%28TTS%29%20models%20have%20made%20huge%20progress%2C%20they%20still%20face%20challenges%20in%20controlling%0Athe%20emotional%20expression%20in%20the%20generated%20speech.%20In%20this%20work%2C%20we%20propose%0AEmoVoice%2C%20a%20novel%20emotion-controllable%20TTS%20model%20that%20exploits%20large%20language%0Amodels%20%28LLMs%29%20to%20enable%20fine-grained%20freestyle%20natural%20language%20emotion%0Acontrol%2C%20and%20a%20phoneme%20boost%20variant%20design%20that%20makes%20the%20model%20output%20phoneme%0Atokens%20and%20audio%20tokens%20in%20parallel%20to%20enhance%20content%20consistency%2C%20inspired%20by%0Achain-of-thought%20%28CoT%29%20and%20modality-of-thought%20%28CoM%29%20techniques.%20Besides%2C%20we%0Aintroduce%20EmoVoice-DB%2C%20a%20high-quality%2040-hour%20English%20emotion%20dataset%20featuring%0Aexpressive%20speech%20and%20fine-grained%20emotion%20labels%20with%20natural%20language%0Adescriptions.%20EmoVoice%20achieves%20state-of-the-art%20performance%20on%20the%20English%0AEmoVoice-DB%20test%20set%20using%20only%20synthetic%20training%20data%2C%20and%20on%20the%20Chinese%0ASecap%20test%20set%20using%20our%20in-house%20data.%20We%20further%20investigate%20the%20reliability%0Aof%20existing%20emotion%20evaluation%20metrics%20and%20their%20alignment%20with%20human%0Aperceptual%20preferences%2C%20and%20explore%20using%20SOTA%20multimodal%20LLMs%20GPT-4o-audio%20and%0AGemini%20to%20assess%20emotional%20speech.%20Demo%20samples%20are%20available%20at%0Ahttps%3A//anonymous.4open.science/r/EmoVoice-DF55.%20Dataset%2C%20code%2C%20and%20checkpoints%0Awill%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12867v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmoVoice%253A%2520LLM-based%2520Emotional%2520Text-To-Speech%2520Model%2520with%2520Freestyle%2520Text%250A%2520%2520Prompting%26entry.906535625%3DGuanrou%2520Yang%2520and%2520Chen%2520Yang%2520and%2520Qian%2520Chen%2520and%2520Ziyang%2520Ma%2520and%2520Wenxi%2520Chen%2520and%2520Wen%2520Wang%2520and%2520Tianrui%2520Wang%2520and%2520Yifan%2520Yang%2520and%2520Zhikang%2520Niu%2520and%2520Wenrui%2520Liu%2520and%2520Fan%2520Yu%2520and%2520Zhihao%2520Du%2520and%2520Zhifu%2520Gao%2520and%2520ShiLiang%2520Zhang%2520and%2520Xie%2520Chen%26entry.1292438233%3D%2520%2520Human%2520speech%2520goes%2520beyond%2520the%2520mere%2520transfer%2520of%2520information%253B%2520it%2520is%2520a%2520profound%250Aexchange%2520of%2520emotions%2520and%2520a%2520connection%2520between%2520individuals.%2520While%2520Text-to-Speech%250A%2528TTS%2529%2520models%2520have%2520made%2520huge%2520progress%252C%2520they%2520still%2520face%2520challenges%2520in%2520controlling%250Athe%2520emotional%2520expression%2520in%2520the%2520generated%2520speech.%2520In%2520this%2520work%252C%2520we%2520propose%250AEmoVoice%252C%2520a%2520novel%2520emotion-controllable%2520TTS%2520model%2520that%2520exploits%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520to%2520enable%2520fine-grained%2520freestyle%2520natural%2520language%2520emotion%250Acontrol%252C%2520and%2520a%2520phoneme%2520boost%2520variant%2520design%2520that%2520makes%2520the%2520model%2520output%2520phoneme%250Atokens%2520and%2520audio%2520tokens%2520in%2520parallel%2520to%2520enhance%2520content%2520consistency%252C%2520inspired%2520by%250Achain-of-thought%2520%2528CoT%2529%2520and%2520modality-of-thought%2520%2528CoM%2529%2520techniques.%2520Besides%252C%2520we%250Aintroduce%2520EmoVoice-DB%252C%2520a%2520high-quality%252040-hour%2520English%2520emotion%2520dataset%2520featuring%250Aexpressive%2520speech%2520and%2520fine-grained%2520emotion%2520labels%2520with%2520natural%2520language%250Adescriptions.%2520EmoVoice%2520achieves%2520state-of-the-art%2520performance%2520on%2520the%2520English%250AEmoVoice-DB%2520test%2520set%2520using%2520only%2520synthetic%2520training%2520data%252C%2520and%2520on%2520the%2520Chinese%250ASecap%2520test%2520set%2520using%2520our%2520in-house%2520data.%2520We%2520further%2520investigate%2520the%2520reliability%250Aof%2520existing%2520emotion%2520evaluation%2520metrics%2520and%2520their%2520alignment%2520with%2520human%250Aperceptual%2520preferences%252C%2520and%2520explore%2520using%2520SOTA%2520multimodal%2520LLMs%2520GPT-4o-audio%2520and%250AGemini%2520to%2520assess%2520emotional%2520speech.%2520Demo%2520samples%2520are%2520available%2520at%250Ahttps%253A//anonymous.4open.science/r/EmoVoice-DF55.%2520Dataset%252C%2520code%252C%2520and%2520checkpoints%250Awill%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12867v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EmoVoice%3A%20LLM-based%20Emotional%20Text-To-Speech%20Model%20with%20Freestyle%20Text%0A%20%20Prompting&entry.906535625=Guanrou%20Yang%20and%20Chen%20Yang%20and%20Qian%20Chen%20and%20Ziyang%20Ma%20and%20Wenxi%20Chen%20and%20Wen%20Wang%20and%20Tianrui%20Wang%20and%20Yifan%20Yang%20and%20Zhikang%20Niu%20and%20Wenrui%20Liu%20and%20Fan%20Yu%20and%20Zhihao%20Du%20and%20Zhifu%20Gao%20and%20ShiLiang%20Zhang%20and%20Xie%20Chen&entry.1292438233=%20%20Human%20speech%20goes%20beyond%20the%20mere%20transfer%20of%20information%3B%20it%20is%20a%20profound%0Aexchange%20of%20emotions%20and%20a%20connection%20between%20individuals.%20While%20Text-to-Speech%0A%28TTS%29%20models%20have%20made%20huge%20progress%2C%20they%20still%20face%20challenges%20in%20controlling%0Athe%20emotional%20expression%20in%20the%20generated%20speech.%20In%20this%20work%2C%20we%20propose%0AEmoVoice%2C%20a%20novel%20emotion-controllable%20TTS%20model%20that%20exploits%20large%20language%0Amodels%20%28LLMs%29%20to%20enable%20fine-grained%20freestyle%20natural%20language%20emotion%0Acontrol%2C%20and%20a%20phoneme%20boost%20variant%20design%20that%20makes%20the%20model%20output%20phoneme%0Atokens%20and%20audio%20tokens%20in%20parallel%20to%20enhance%20content%20consistency%2C%20inspired%20by%0Achain-of-thought%20%28CoT%29%20and%20modality-of-thought%20%28CoM%29%20techniques.%20Besides%2C%20we%0Aintroduce%20EmoVoice-DB%2C%20a%20high-quality%2040-hour%20English%20emotion%20dataset%20featuring%0Aexpressive%20speech%20and%20fine-grained%20emotion%20labels%20with%20natural%20language%0Adescriptions.%20EmoVoice%20achieves%20state-of-the-art%20performance%20on%20the%20English%0AEmoVoice-DB%20test%20set%20using%20only%20synthetic%20training%20data%2C%20and%20on%20the%20Chinese%0ASecap%20test%20set%20using%20our%20in-house%20data.%20We%20further%20investigate%20the%20reliability%0Aof%20existing%20emotion%20evaluation%20metrics%20and%20their%20alignment%20with%20human%0Aperceptual%20preferences%2C%20and%20explore%20using%20SOTA%20multimodal%20LLMs%20GPT-4o-audio%20and%0AGemini%20to%20assess%20emotional%20speech.%20Demo%20samples%20are%20available%20at%0Ahttps%3A//anonymous.4open.science/r/EmoVoice-DF55.%20Dataset%2C%20code%2C%20and%20checkpoints%0Awill%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12867v1&entry.124074799=Read"},
{"title": "UncAD: Towards Safe End-to-end Autonomous Driving via Online Map\n  Uncertainty", "author": "Pengxuan Yang and Yupeng Zheng and Qichao Zhang and Kefei Zhu and Zebin Xing and Qiao Lin and Yun-Fu Liu and Zhiguo Su and Dongbin Zhao", "abstract": "  End-to-end autonomous driving aims to produce planning trajectories from raw\nsensors directly. Currently, most approaches integrate perception, prediction,\nand planning modules into a fully differentiable network, promising great\nscalability. However, these methods typically rely on deterministic modeling of\nonline maps in the perception module for guiding or constraining vehicle\nplanning, which may incorporate erroneous perception information and further\ncompromise planning safety. To address this issue, we delve into the importance\nof online map uncertainty for enhancing autonomous driving safety and propose a\nnovel paradigm named UncAD. Specifically, UncAD first estimates the uncertainty\nof the online map in the perception module. It then leverages the uncertainty\nto guide motion prediction and planning modules to produce multi-modal\ntrajectories. Finally, to achieve safer autonomous driving, UncAD proposes an\nuncertainty-collision-aware planning selection strategy according to the online\nmap uncertainty to evaluate and select the best trajectory. In this study, we\nincorporate UncAD into various state-of-the-art (SOTA) end-to-end methods.\nExperiments on the nuScenes dataset show that integrating UncAD, with only a\n1.9% increase in parameters, can reduce collision rates by up to 26% and\ndrivable area conflict rate by up to 42%. Codes, pre-trained models, and demo\nvideos can be accessed at https://github.com/pengxuanyang/UncAD.\n", "link": "http://arxiv.org/abs/2504.12826v1", "date": "2025-04-17", "relevancy": 1.905, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.681}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5855}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UncAD%3A%20Towards%20Safe%20End-to-end%20Autonomous%20Driving%20via%20Online%20Map%0A%20%20Uncertainty&body=Title%3A%20UncAD%3A%20Towards%20Safe%20End-to-end%20Autonomous%20Driving%20via%20Online%20Map%0A%20%20Uncertainty%0AAuthor%3A%20Pengxuan%20Yang%20and%20Yupeng%20Zheng%20and%20Qichao%20Zhang%20and%20Kefei%20Zhu%20and%20Zebin%20Xing%20and%20Qiao%20Lin%20and%20Yun-Fu%20Liu%20and%20Zhiguo%20Su%20and%20Dongbin%20Zhao%0AAbstract%3A%20%20%20End-to-end%20autonomous%20driving%20aims%20to%20produce%20planning%20trajectories%20from%20raw%0Asensors%20directly.%20Currently%2C%20most%20approaches%20integrate%20perception%2C%20prediction%2C%0Aand%20planning%20modules%20into%20a%20fully%20differentiable%20network%2C%20promising%20great%0Ascalability.%20However%2C%20these%20methods%20typically%20rely%20on%20deterministic%20modeling%20of%0Aonline%20maps%20in%20the%20perception%20module%20for%20guiding%20or%20constraining%20vehicle%0Aplanning%2C%20which%20may%20incorporate%20erroneous%20perception%20information%20and%20further%0Acompromise%20planning%20safety.%20To%20address%20this%20issue%2C%20we%20delve%20into%20the%20importance%0Aof%20online%20map%20uncertainty%20for%20enhancing%20autonomous%20driving%20safety%20and%20propose%20a%0Anovel%20paradigm%20named%20UncAD.%20Specifically%2C%20UncAD%20first%20estimates%20the%20uncertainty%0Aof%20the%20online%20map%20in%20the%20perception%20module.%20It%20then%20leverages%20the%20uncertainty%0Ato%20guide%20motion%20prediction%20and%20planning%20modules%20to%20produce%20multi-modal%0Atrajectories.%20Finally%2C%20to%20achieve%20safer%20autonomous%20driving%2C%20UncAD%20proposes%20an%0Auncertainty-collision-aware%20planning%20selection%20strategy%20according%20to%20the%20online%0Amap%20uncertainty%20to%20evaluate%20and%20select%20the%20best%20trajectory.%20In%20this%20study%2C%20we%0Aincorporate%20UncAD%20into%20various%20state-of-the-art%20%28SOTA%29%20end-to-end%20methods.%0AExperiments%20on%20the%20nuScenes%20dataset%20show%20that%20integrating%20UncAD%2C%20with%20only%20a%0A1.9%25%20increase%20in%20parameters%2C%20can%20reduce%20collision%20rates%20by%20up%20to%2026%25%20and%0Adrivable%20area%20conflict%20rate%20by%20up%20to%2042%25.%20Codes%2C%20pre-trained%20models%2C%20and%20demo%0Avideos%20can%20be%20accessed%20at%20https%3A//github.com/pengxuanyang/UncAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12826v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncAD%253A%2520Towards%2520Safe%2520End-to-end%2520Autonomous%2520Driving%2520via%2520Online%2520Map%250A%2520%2520Uncertainty%26entry.906535625%3DPengxuan%2520Yang%2520and%2520Yupeng%2520Zheng%2520and%2520Qichao%2520Zhang%2520and%2520Kefei%2520Zhu%2520and%2520Zebin%2520Xing%2520and%2520Qiao%2520Lin%2520and%2520Yun-Fu%2520Liu%2520and%2520Zhiguo%2520Su%2520and%2520Dongbin%2520Zhao%26entry.1292438233%3D%2520%2520End-to-end%2520autonomous%2520driving%2520aims%2520to%2520produce%2520planning%2520trajectories%2520from%2520raw%250Asensors%2520directly.%2520Currently%252C%2520most%2520approaches%2520integrate%2520perception%252C%2520prediction%252C%250Aand%2520planning%2520modules%2520into%2520a%2520fully%2520differentiable%2520network%252C%2520promising%2520great%250Ascalability.%2520However%252C%2520these%2520methods%2520typically%2520rely%2520on%2520deterministic%2520modeling%2520of%250Aonline%2520maps%2520in%2520the%2520perception%2520module%2520for%2520guiding%2520or%2520constraining%2520vehicle%250Aplanning%252C%2520which%2520may%2520incorporate%2520erroneous%2520perception%2520information%2520and%2520further%250Acompromise%2520planning%2520safety.%2520To%2520address%2520this%2520issue%252C%2520we%2520delve%2520into%2520the%2520importance%250Aof%2520online%2520map%2520uncertainty%2520for%2520enhancing%2520autonomous%2520driving%2520safety%2520and%2520propose%2520a%250Anovel%2520paradigm%2520named%2520UncAD.%2520Specifically%252C%2520UncAD%2520first%2520estimates%2520the%2520uncertainty%250Aof%2520the%2520online%2520map%2520in%2520the%2520perception%2520module.%2520It%2520then%2520leverages%2520the%2520uncertainty%250Ato%2520guide%2520motion%2520prediction%2520and%2520planning%2520modules%2520to%2520produce%2520multi-modal%250Atrajectories.%2520Finally%252C%2520to%2520achieve%2520safer%2520autonomous%2520driving%252C%2520UncAD%2520proposes%2520an%250Auncertainty-collision-aware%2520planning%2520selection%2520strategy%2520according%2520to%2520the%2520online%250Amap%2520uncertainty%2520to%2520evaluate%2520and%2520select%2520the%2520best%2520trajectory.%2520In%2520this%2520study%252C%2520we%250Aincorporate%2520UncAD%2520into%2520various%2520state-of-the-art%2520%2528SOTA%2529%2520end-to-end%2520methods.%250AExperiments%2520on%2520the%2520nuScenes%2520dataset%2520show%2520that%2520integrating%2520UncAD%252C%2520with%2520only%2520a%250A1.9%2525%2520increase%2520in%2520parameters%252C%2520can%2520reduce%2520collision%2520rates%2520by%2520up%2520to%252026%2525%2520and%250Adrivable%2520area%2520conflict%2520rate%2520by%2520up%2520to%252042%2525.%2520Codes%252C%2520pre-trained%2520models%252C%2520and%2520demo%250Avideos%2520can%2520be%2520accessed%2520at%2520https%253A//github.com/pengxuanyang/UncAD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12826v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UncAD%3A%20Towards%20Safe%20End-to-end%20Autonomous%20Driving%20via%20Online%20Map%0A%20%20Uncertainty&entry.906535625=Pengxuan%20Yang%20and%20Yupeng%20Zheng%20and%20Qichao%20Zhang%20and%20Kefei%20Zhu%20and%20Zebin%20Xing%20and%20Qiao%20Lin%20and%20Yun-Fu%20Liu%20and%20Zhiguo%20Su%20and%20Dongbin%20Zhao&entry.1292438233=%20%20End-to-end%20autonomous%20driving%20aims%20to%20produce%20planning%20trajectories%20from%20raw%0Asensors%20directly.%20Currently%2C%20most%20approaches%20integrate%20perception%2C%20prediction%2C%0Aand%20planning%20modules%20into%20a%20fully%20differentiable%20network%2C%20promising%20great%0Ascalability.%20However%2C%20these%20methods%20typically%20rely%20on%20deterministic%20modeling%20of%0Aonline%20maps%20in%20the%20perception%20module%20for%20guiding%20or%20constraining%20vehicle%0Aplanning%2C%20which%20may%20incorporate%20erroneous%20perception%20information%20and%20further%0Acompromise%20planning%20safety.%20To%20address%20this%20issue%2C%20we%20delve%20into%20the%20importance%0Aof%20online%20map%20uncertainty%20for%20enhancing%20autonomous%20driving%20safety%20and%20propose%20a%0Anovel%20paradigm%20named%20UncAD.%20Specifically%2C%20UncAD%20first%20estimates%20the%20uncertainty%0Aof%20the%20online%20map%20in%20the%20perception%20module.%20It%20then%20leverages%20the%20uncertainty%0Ato%20guide%20motion%20prediction%20and%20planning%20modules%20to%20produce%20multi-modal%0Atrajectories.%20Finally%2C%20to%20achieve%20safer%20autonomous%20driving%2C%20UncAD%20proposes%20an%0Auncertainty-collision-aware%20planning%20selection%20strategy%20according%20to%20the%20online%0Amap%20uncertainty%20to%20evaluate%20and%20select%20the%20best%20trajectory.%20In%20this%20study%2C%20we%0Aincorporate%20UncAD%20into%20various%20state-of-the-art%20%28SOTA%29%20end-to-end%20methods.%0AExperiments%20on%20the%20nuScenes%20dataset%20show%20that%20integrating%20UncAD%2C%20with%20only%20a%0A1.9%25%20increase%20in%20parameters%2C%20can%20reduce%20collision%20rates%20by%20up%20to%2026%25%20and%0Adrivable%20area%20conflict%20rate%20by%20up%20to%2042%25.%20Codes%2C%20pre-trained%20models%2C%20and%20demo%0Avideos%20can%20be%20accessed%20at%20https%3A//github.com/pengxuanyang/UncAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12826v1&entry.124074799=Read"},
{"title": "Enhancing Explainability and Reliable Decision-Making in Particle Swarm\n  Optimization through Communication Topologies", "author": "Nitin Gupta and Indu Bala and Bapi Dutta and Luis Mart\u00ednez and Anupam Yadav", "abstract": "  Swarm intelligence effectively optimizes complex systems across fields like\nengineering and healthcare, yet algorithm solutions often suffer from low\nreliability due to unclear configurations and hyperparameters. This study\nanalyzes Particle Swarm Optimization (PSO), focusing on how different\ncommunication topologies Ring, Star, and Von Neumann affect convergence and\nsearch behaviors. Using an adapted IOHxplainer , an explainable benchmarking\ntool, we investigate how these topologies influence information flow,\ndiversity, and convergence speed, clarifying the balance between exploration\nand exploitation. Through visualization and statistical analysis, the research\nenhances interpretability of PSO's decisions and provides practical guidelines\nfor choosing suitable topologies for specific optimization tasks. Ultimately,\nthis contributes to making swarm based optimization more transparent, robust,\nand trustworthy.\n", "link": "http://arxiv.org/abs/2504.12803v1", "date": "2025-04-17", "relevancy": 1.3449, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4524}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4501}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Explainability%20and%20Reliable%20Decision-Making%20in%20Particle%20Swarm%0A%20%20Optimization%20through%20Communication%20Topologies&body=Title%3A%20Enhancing%20Explainability%20and%20Reliable%20Decision-Making%20in%20Particle%20Swarm%0A%20%20Optimization%20through%20Communication%20Topologies%0AAuthor%3A%20Nitin%20Gupta%20and%20Indu%20Bala%20and%20Bapi%20Dutta%20and%20Luis%20Mart%C3%ADnez%20and%20Anupam%20Yadav%0AAbstract%3A%20%20%20Swarm%20intelligence%20effectively%20optimizes%20complex%20systems%20across%20fields%20like%0Aengineering%20and%20healthcare%2C%20yet%20algorithm%20solutions%20often%20suffer%20from%20low%0Areliability%20due%20to%20unclear%20configurations%20and%20hyperparameters.%20This%20study%0Aanalyzes%20Particle%20Swarm%20Optimization%20%28PSO%29%2C%20focusing%20on%20how%20different%0Acommunication%20topologies%20Ring%2C%20Star%2C%20and%20Von%20Neumann%20affect%20convergence%20and%0Asearch%20behaviors.%20Using%20an%20adapted%20IOHxplainer%20%2C%20an%20explainable%20benchmarking%0Atool%2C%20we%20investigate%20how%20these%20topologies%20influence%20information%20flow%2C%0Adiversity%2C%20and%20convergence%20speed%2C%20clarifying%20the%20balance%20between%20exploration%0Aand%20exploitation.%20Through%20visualization%20and%20statistical%20analysis%2C%20the%20research%0Aenhances%20interpretability%20of%20PSO%27s%20decisions%20and%20provides%20practical%20guidelines%0Afor%20choosing%20suitable%20topologies%20for%20specific%20optimization%20tasks.%20Ultimately%2C%0Athis%20contributes%20to%20making%20swarm%20based%20optimization%20more%20transparent%2C%20robust%2C%0Aand%20trustworthy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12803v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Explainability%2520and%2520Reliable%2520Decision-Making%2520in%2520Particle%2520Swarm%250A%2520%2520Optimization%2520through%2520Communication%2520Topologies%26entry.906535625%3DNitin%2520Gupta%2520and%2520Indu%2520Bala%2520and%2520Bapi%2520Dutta%2520and%2520Luis%2520Mart%25C3%25ADnez%2520and%2520Anupam%2520Yadav%26entry.1292438233%3D%2520%2520Swarm%2520intelligence%2520effectively%2520optimizes%2520complex%2520systems%2520across%2520fields%2520like%250Aengineering%2520and%2520healthcare%252C%2520yet%2520algorithm%2520solutions%2520often%2520suffer%2520from%2520low%250Areliability%2520due%2520to%2520unclear%2520configurations%2520and%2520hyperparameters.%2520This%2520study%250Aanalyzes%2520Particle%2520Swarm%2520Optimization%2520%2528PSO%2529%252C%2520focusing%2520on%2520how%2520different%250Acommunication%2520topologies%2520Ring%252C%2520Star%252C%2520and%2520Von%2520Neumann%2520affect%2520convergence%2520and%250Asearch%2520behaviors.%2520Using%2520an%2520adapted%2520IOHxplainer%2520%252C%2520an%2520explainable%2520benchmarking%250Atool%252C%2520we%2520investigate%2520how%2520these%2520topologies%2520influence%2520information%2520flow%252C%250Adiversity%252C%2520and%2520convergence%2520speed%252C%2520clarifying%2520the%2520balance%2520between%2520exploration%250Aand%2520exploitation.%2520Through%2520visualization%2520and%2520statistical%2520analysis%252C%2520the%2520research%250Aenhances%2520interpretability%2520of%2520PSO%2527s%2520decisions%2520and%2520provides%2520practical%2520guidelines%250Afor%2520choosing%2520suitable%2520topologies%2520for%2520specific%2520optimization%2520tasks.%2520Ultimately%252C%250Athis%2520contributes%2520to%2520making%2520swarm%2520based%2520optimization%2520more%2520transparent%252C%2520robust%252C%250Aand%2520trustworthy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12803v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Explainability%20and%20Reliable%20Decision-Making%20in%20Particle%20Swarm%0A%20%20Optimization%20through%20Communication%20Topologies&entry.906535625=Nitin%20Gupta%20and%20Indu%20Bala%20and%20Bapi%20Dutta%20and%20Luis%20Mart%C3%ADnez%20and%20Anupam%20Yadav&entry.1292438233=%20%20Swarm%20intelligence%20effectively%20optimizes%20complex%20systems%20across%20fields%20like%0Aengineering%20and%20healthcare%2C%20yet%20algorithm%20solutions%20often%20suffer%20from%20low%0Areliability%20due%20to%20unclear%20configurations%20and%20hyperparameters.%20This%20study%0Aanalyzes%20Particle%20Swarm%20Optimization%20%28PSO%29%2C%20focusing%20on%20how%20different%0Acommunication%20topologies%20Ring%2C%20Star%2C%20and%20Von%20Neumann%20affect%20convergence%20and%0Asearch%20behaviors.%20Using%20an%20adapted%20IOHxplainer%20%2C%20an%20explainable%20benchmarking%0Atool%2C%20we%20investigate%20how%20these%20topologies%20influence%20information%20flow%2C%0Adiversity%2C%20and%20convergence%20speed%2C%20clarifying%20the%20balance%20between%20exploration%0Aand%20exploitation.%20Through%20visualization%20and%20statistical%20analysis%2C%20the%20research%0Aenhances%20interpretability%20of%20PSO%27s%20decisions%20and%20provides%20practical%20guidelines%0Afor%20choosing%20suitable%20topologies%20for%20specific%20optimization%20tasks.%20Ultimately%2C%0Athis%20contributes%20to%20making%20swarm%20based%20optimization%20more%20transparent%2C%20robust%2C%0Aand%20trustworthy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12803v1&entry.124074799=Read"},
{"title": "SHA256 at SemEval-2025 Task 4: Selective Amnesia -- Constrained\n  Unlearning for Large Language Models via Knowledge Isolation", "author": "Saransh Agrawal and Kuan-Hao Huang", "abstract": "  Large language models (LLMs) frequently memorize sensitive information during\ntraining, posing risks when deploying publicly accessible models. Current\nmachine unlearning methods struggle to selectively remove specific data\nassociations without degrading overall model capabilities. This paper presents\nour solution to SemEval-2025 Task 4 on targeted unlearning, which introduces a\ntwo-stage methodology that combines causal mediation analysis with\nlayer-specific optimization. Through systematic causal tracing experiments on\nOLMo architectures (1B and 7B parameters), we identify the critical role of the\nfirst few transformer layers (layers 0-5) in storing subject-attribute\nassociations within MLP modules. Building on this insight, we develop a\nconstrained optimization approach that freezes upper layers while applying a\nnovel joint loss function to lower layers-simultaneously maximizing forget set\nloss via output token cross-entropy penalties and minimizing retain set\ndeviation through adaptive regularization. Our method achieves 2nd place in the\n1B model track, demonstrating strong task performance while maintaining 88% of\nbaseline MMLU accuracy. These results establish causal-informed layer\noptimization as a promising paradigm for efficient, precise unlearning in LLMs,\noffering a significant step forward in addressing data privacy concerns in AI\nsystems.\n", "link": "http://arxiv.org/abs/2504.12996v1", "date": "2025-04-17", "relevancy": 1.9587, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5226}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4888}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SHA256%20at%20SemEval-2025%20Task%204%3A%20Selective%20Amnesia%20--%20Constrained%0A%20%20Unlearning%20for%20Large%20Language%20Models%20via%20Knowledge%20Isolation&body=Title%3A%20SHA256%20at%20SemEval-2025%20Task%204%3A%20Selective%20Amnesia%20--%20Constrained%0A%20%20Unlearning%20for%20Large%20Language%20Models%20via%20Knowledge%20Isolation%0AAuthor%3A%20Saransh%20Agrawal%20and%20Kuan-Hao%20Huang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20frequently%20memorize%20sensitive%20information%20during%0Atraining%2C%20posing%20risks%20when%20deploying%20publicly%20accessible%20models.%20Current%0Amachine%20unlearning%20methods%20struggle%20to%20selectively%20remove%20specific%20data%0Aassociations%20without%20degrading%20overall%20model%20capabilities.%20This%20paper%20presents%0Aour%20solution%20to%20SemEval-2025%20Task%204%20on%20targeted%20unlearning%2C%20which%20introduces%20a%0Atwo-stage%20methodology%20that%20combines%20causal%20mediation%20analysis%20with%0Alayer-specific%20optimization.%20Through%20systematic%20causal%20tracing%20experiments%20on%0AOLMo%20architectures%20%281B%20and%207B%20parameters%29%2C%20we%20identify%20the%20critical%20role%20of%20the%0Afirst%20few%20transformer%20layers%20%28layers%200-5%29%20in%20storing%20subject-attribute%0Aassociations%20within%20MLP%20modules.%20Building%20on%20this%20insight%2C%20we%20develop%20a%0Aconstrained%20optimization%20approach%20that%20freezes%20upper%20layers%20while%20applying%20a%0Anovel%20joint%20loss%20function%20to%20lower%20layers-simultaneously%20maximizing%20forget%20set%0Aloss%20via%20output%20token%20cross-entropy%20penalties%20and%20minimizing%20retain%20set%0Adeviation%20through%20adaptive%20regularization.%20Our%20method%20achieves%202nd%20place%20in%20the%0A1B%20model%20track%2C%20demonstrating%20strong%20task%20performance%20while%20maintaining%2088%25%20of%0Abaseline%20MMLU%20accuracy.%20These%20results%20establish%20causal-informed%20layer%0Aoptimization%20as%20a%20promising%20paradigm%20for%20efficient%2C%20precise%20unlearning%20in%20LLMs%2C%0Aoffering%20a%20significant%20step%20forward%20in%20addressing%20data%20privacy%20concerns%20in%20AI%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12996v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSHA256%2520at%2520SemEval-2025%2520Task%25204%253A%2520Selective%2520Amnesia%2520--%2520Constrained%250A%2520%2520Unlearning%2520for%2520Large%2520Language%2520Models%2520via%2520Knowledge%2520Isolation%26entry.906535625%3DSaransh%2520Agrawal%2520and%2520Kuan-Hao%2520Huang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520frequently%2520memorize%2520sensitive%2520information%2520during%250Atraining%252C%2520posing%2520risks%2520when%2520deploying%2520publicly%2520accessible%2520models.%2520Current%250Amachine%2520unlearning%2520methods%2520struggle%2520to%2520selectively%2520remove%2520specific%2520data%250Aassociations%2520without%2520degrading%2520overall%2520model%2520capabilities.%2520This%2520paper%2520presents%250Aour%2520solution%2520to%2520SemEval-2025%2520Task%25204%2520on%2520targeted%2520unlearning%252C%2520which%2520introduces%2520a%250Atwo-stage%2520methodology%2520that%2520combines%2520causal%2520mediation%2520analysis%2520with%250Alayer-specific%2520optimization.%2520Through%2520systematic%2520causal%2520tracing%2520experiments%2520on%250AOLMo%2520architectures%2520%25281B%2520and%25207B%2520parameters%2529%252C%2520we%2520identify%2520the%2520critical%2520role%2520of%2520the%250Afirst%2520few%2520transformer%2520layers%2520%2528layers%25200-5%2529%2520in%2520storing%2520subject-attribute%250Aassociations%2520within%2520MLP%2520modules.%2520Building%2520on%2520this%2520insight%252C%2520we%2520develop%2520a%250Aconstrained%2520optimization%2520approach%2520that%2520freezes%2520upper%2520layers%2520while%2520applying%2520a%250Anovel%2520joint%2520loss%2520function%2520to%2520lower%2520layers-simultaneously%2520maximizing%2520forget%2520set%250Aloss%2520via%2520output%2520token%2520cross-entropy%2520penalties%2520and%2520minimizing%2520retain%2520set%250Adeviation%2520through%2520adaptive%2520regularization.%2520Our%2520method%2520achieves%25202nd%2520place%2520in%2520the%250A1B%2520model%2520track%252C%2520demonstrating%2520strong%2520task%2520performance%2520while%2520maintaining%252088%2525%2520of%250Abaseline%2520MMLU%2520accuracy.%2520These%2520results%2520establish%2520causal-informed%2520layer%250Aoptimization%2520as%2520a%2520promising%2520paradigm%2520for%2520efficient%252C%2520precise%2520unlearning%2520in%2520LLMs%252C%250Aoffering%2520a%2520significant%2520step%2520forward%2520in%2520addressing%2520data%2520privacy%2520concerns%2520in%2520AI%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12996v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SHA256%20at%20SemEval-2025%20Task%204%3A%20Selective%20Amnesia%20--%20Constrained%0A%20%20Unlearning%20for%20Large%20Language%20Models%20via%20Knowledge%20Isolation&entry.906535625=Saransh%20Agrawal%20and%20Kuan-Hao%20Huang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20frequently%20memorize%20sensitive%20information%20during%0Atraining%2C%20posing%20risks%20when%20deploying%20publicly%20accessible%20models.%20Current%0Amachine%20unlearning%20methods%20struggle%20to%20selectively%20remove%20specific%20data%0Aassociations%20without%20degrading%20overall%20model%20capabilities.%20This%20paper%20presents%0Aour%20solution%20to%20SemEval-2025%20Task%204%20on%20targeted%20unlearning%2C%20which%20introduces%20a%0Atwo-stage%20methodology%20that%20combines%20causal%20mediation%20analysis%20with%0Alayer-specific%20optimization.%20Through%20systematic%20causal%20tracing%20experiments%20on%0AOLMo%20architectures%20%281B%20and%207B%20parameters%29%2C%20we%20identify%20the%20critical%20role%20of%20the%0Afirst%20few%20transformer%20layers%20%28layers%200-5%29%20in%20storing%20subject-attribute%0Aassociations%20within%20MLP%20modules.%20Building%20on%20this%20insight%2C%20we%20develop%20a%0Aconstrained%20optimization%20approach%20that%20freezes%20upper%20layers%20while%20applying%20a%0Anovel%20joint%20loss%20function%20to%20lower%20layers-simultaneously%20maximizing%20forget%20set%0Aloss%20via%20output%20token%20cross-entropy%20penalties%20and%20minimizing%20retain%20set%0Adeviation%20through%20adaptive%20regularization.%20Our%20method%20achieves%202nd%20place%20in%20the%0A1B%20model%20track%2C%20demonstrating%20strong%20task%20performance%20while%20maintaining%2088%25%20of%0Abaseline%20MMLU%20accuracy.%20These%20results%20establish%20causal-informed%20layer%0Aoptimization%20as%20a%20promising%20paradigm%20for%20efficient%2C%20precise%20unlearning%20in%20LLMs%2C%0Aoffering%20a%20significant%20step%20forward%20in%20addressing%20data%20privacy%20concerns%20in%20AI%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12996v1&entry.124074799=Read"},
{"title": "Learning from Similar Linear Representations: Adaptivity, Minimaxity,\n  and Robustness", "author": "Ye Tian and Yuqi Gu and Yang Feng", "abstract": "  Representation multi-task learning (MTL) has achieved tremendous success in\npractice. However, the theoretical understanding of these methods is still\nlacking. Most existing theoretical works focus on cases where all tasks share\nthe same representation, and claim that MTL almost always improves performance.\nNevertheless, as the number of tasks grows, assuming all tasks share the same\nrepresentation is unrealistic. Furthermore, empirical findings often indicate\nthat a shared representation does not necessarily improve single-task learning\nperformance. In this paper, we aim to understand how to learn from tasks with\n\\textit{similar but not exactly the same} linear representations, while dealing\nwith outlier tasks. Assuming a known intrinsic dimension, we propose a\npenalized empirical risk minimization method and a spectral method that are\n\\textit{adaptive} to the similarity structure and \\textit{robust} to outlier\ntasks. Both algorithms outperform single-task learning when representations\nacross tasks are sufficiently similar and the proportion of outlier tasks is\nsmall. Moreover, they always perform at least as well as single-task learning,\neven when the representations are dissimilar. We provide information-theoretic\nlower bounds to demonstrate that both methods are nearly \\textit{minimax}\noptimal in a large regime, with the spectral method being optimal in the\nabsence of outlier tasks. Additionally, we introduce a thresholding algorithm\nto adapt to an unknown intrinsic dimension. We conduct extensive numerical\nexperiments to validate our theoretical findings.\n", "link": "http://arxiv.org/abs/2303.17765v4", "date": "2025-04-17", "relevancy": 2.0589, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5375}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5063}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Similar%20Linear%20Representations%3A%20Adaptivity%2C%20Minimaxity%2C%0A%20%20and%20Robustness&body=Title%3A%20Learning%20from%20Similar%20Linear%20Representations%3A%20Adaptivity%2C%20Minimaxity%2C%0A%20%20and%20Robustness%0AAuthor%3A%20Ye%20Tian%20and%20Yuqi%20Gu%20and%20Yang%20Feng%0AAbstract%3A%20%20%20Representation%20multi-task%20learning%20%28MTL%29%20has%20achieved%20tremendous%20success%20in%0Apractice.%20However%2C%20the%20theoretical%20understanding%20of%20these%20methods%20is%20still%0Alacking.%20Most%20existing%20theoretical%20works%20focus%20on%20cases%20where%20all%20tasks%20share%0Athe%20same%20representation%2C%20and%20claim%20that%20MTL%20almost%20always%20improves%20performance.%0ANevertheless%2C%20as%20the%20number%20of%20tasks%20grows%2C%20assuming%20all%20tasks%20share%20the%20same%0Arepresentation%20is%20unrealistic.%20Furthermore%2C%20empirical%20findings%20often%20indicate%0Athat%20a%20shared%20representation%20does%20not%20necessarily%20improve%20single-task%20learning%0Aperformance.%20In%20this%20paper%2C%20we%20aim%20to%20understand%20how%20to%20learn%20from%20tasks%20with%0A%5Ctextit%7Bsimilar%20but%20not%20exactly%20the%20same%7D%20linear%20representations%2C%20while%20dealing%0Awith%20outlier%20tasks.%20Assuming%20a%20known%20intrinsic%20dimension%2C%20we%20propose%20a%0Apenalized%20empirical%20risk%20minimization%20method%20and%20a%20spectral%20method%20that%20are%0A%5Ctextit%7Badaptive%7D%20to%20the%20similarity%20structure%20and%20%5Ctextit%7Brobust%7D%20to%20outlier%0Atasks.%20Both%20algorithms%20outperform%20single-task%20learning%20when%20representations%0Aacross%20tasks%20are%20sufficiently%20similar%20and%20the%20proportion%20of%20outlier%20tasks%20is%0Asmall.%20Moreover%2C%20they%20always%20perform%20at%20least%20as%20well%20as%20single-task%20learning%2C%0Aeven%20when%20the%20representations%20are%20dissimilar.%20We%20provide%20information-theoretic%0Alower%20bounds%20to%20demonstrate%20that%20both%20methods%20are%20nearly%20%5Ctextit%7Bminimax%7D%0Aoptimal%20in%20a%20large%20regime%2C%20with%20the%20spectral%20method%20being%20optimal%20in%20the%0Aabsence%20of%20outlier%20tasks.%20Additionally%2C%20we%20introduce%20a%20thresholding%20algorithm%0Ato%20adapt%20to%20an%20unknown%20intrinsic%20dimension.%20We%20conduct%20extensive%20numerical%0Aexperiments%20to%20validate%20our%20theoretical%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.17765v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520Similar%2520Linear%2520Representations%253A%2520Adaptivity%252C%2520Minimaxity%252C%250A%2520%2520and%2520Robustness%26entry.906535625%3DYe%2520Tian%2520and%2520Yuqi%2520Gu%2520and%2520Yang%2520Feng%26entry.1292438233%3D%2520%2520Representation%2520multi-task%2520learning%2520%2528MTL%2529%2520has%2520achieved%2520tremendous%2520success%2520in%250Apractice.%2520However%252C%2520the%2520theoretical%2520understanding%2520of%2520these%2520methods%2520is%2520still%250Alacking.%2520Most%2520existing%2520theoretical%2520works%2520focus%2520on%2520cases%2520where%2520all%2520tasks%2520share%250Athe%2520same%2520representation%252C%2520and%2520claim%2520that%2520MTL%2520almost%2520always%2520improves%2520performance.%250ANevertheless%252C%2520as%2520the%2520number%2520of%2520tasks%2520grows%252C%2520assuming%2520all%2520tasks%2520share%2520the%2520same%250Arepresentation%2520is%2520unrealistic.%2520Furthermore%252C%2520empirical%2520findings%2520often%2520indicate%250Athat%2520a%2520shared%2520representation%2520does%2520not%2520necessarily%2520improve%2520single-task%2520learning%250Aperformance.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520understand%2520how%2520to%2520learn%2520from%2520tasks%2520with%250A%255Ctextit%257Bsimilar%2520but%2520not%2520exactly%2520the%2520same%257D%2520linear%2520representations%252C%2520while%2520dealing%250Awith%2520outlier%2520tasks.%2520Assuming%2520a%2520known%2520intrinsic%2520dimension%252C%2520we%2520propose%2520a%250Apenalized%2520empirical%2520risk%2520minimization%2520method%2520and%2520a%2520spectral%2520method%2520that%2520are%250A%255Ctextit%257Badaptive%257D%2520to%2520the%2520similarity%2520structure%2520and%2520%255Ctextit%257Brobust%257D%2520to%2520outlier%250Atasks.%2520Both%2520algorithms%2520outperform%2520single-task%2520learning%2520when%2520representations%250Aacross%2520tasks%2520are%2520sufficiently%2520similar%2520and%2520the%2520proportion%2520of%2520outlier%2520tasks%2520is%250Asmall.%2520Moreover%252C%2520they%2520always%2520perform%2520at%2520least%2520as%2520well%2520as%2520single-task%2520learning%252C%250Aeven%2520when%2520the%2520representations%2520are%2520dissimilar.%2520We%2520provide%2520information-theoretic%250Alower%2520bounds%2520to%2520demonstrate%2520that%2520both%2520methods%2520are%2520nearly%2520%255Ctextit%257Bminimax%257D%250Aoptimal%2520in%2520a%2520large%2520regime%252C%2520with%2520the%2520spectral%2520method%2520being%2520optimal%2520in%2520the%250Aabsence%2520of%2520outlier%2520tasks.%2520Additionally%252C%2520we%2520introduce%2520a%2520thresholding%2520algorithm%250Ato%2520adapt%2520to%2520an%2520unknown%2520intrinsic%2520dimension.%2520We%2520conduct%2520extensive%2520numerical%250Aexperiments%2520to%2520validate%2520our%2520theoretical%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.17765v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Similar%20Linear%20Representations%3A%20Adaptivity%2C%20Minimaxity%2C%0A%20%20and%20Robustness&entry.906535625=Ye%20Tian%20and%20Yuqi%20Gu%20and%20Yang%20Feng&entry.1292438233=%20%20Representation%20multi-task%20learning%20%28MTL%29%20has%20achieved%20tremendous%20success%20in%0Apractice.%20However%2C%20the%20theoretical%20understanding%20of%20these%20methods%20is%20still%0Alacking.%20Most%20existing%20theoretical%20works%20focus%20on%20cases%20where%20all%20tasks%20share%0Athe%20same%20representation%2C%20and%20claim%20that%20MTL%20almost%20always%20improves%20performance.%0ANevertheless%2C%20as%20the%20number%20of%20tasks%20grows%2C%20assuming%20all%20tasks%20share%20the%20same%0Arepresentation%20is%20unrealistic.%20Furthermore%2C%20empirical%20findings%20often%20indicate%0Athat%20a%20shared%20representation%20does%20not%20necessarily%20improve%20single-task%20learning%0Aperformance.%20In%20this%20paper%2C%20we%20aim%20to%20understand%20how%20to%20learn%20from%20tasks%20with%0A%5Ctextit%7Bsimilar%20but%20not%20exactly%20the%20same%7D%20linear%20representations%2C%20while%20dealing%0Awith%20outlier%20tasks.%20Assuming%20a%20known%20intrinsic%20dimension%2C%20we%20propose%20a%0Apenalized%20empirical%20risk%20minimization%20method%20and%20a%20spectral%20method%20that%20are%0A%5Ctextit%7Badaptive%7D%20to%20the%20similarity%20structure%20and%20%5Ctextit%7Brobust%7D%20to%20outlier%0Atasks.%20Both%20algorithms%20outperform%20single-task%20learning%20when%20representations%0Aacross%20tasks%20are%20sufficiently%20similar%20and%20the%20proportion%20of%20outlier%20tasks%20is%0Asmall.%20Moreover%2C%20they%20always%20perform%20at%20least%20as%20well%20as%20single-task%20learning%2C%0Aeven%20when%20the%20representations%20are%20dissimilar.%20We%20provide%20information-theoretic%0Alower%20bounds%20to%20demonstrate%20that%20both%20methods%20are%20nearly%20%5Ctextit%7Bminimax%7D%0Aoptimal%20in%20a%20large%20regime%2C%20with%20the%20spectral%20method%20being%20optimal%20in%20the%0Aabsence%20of%20outlier%20tasks.%20Additionally%2C%20we%20introduce%20a%20thresholding%20algorithm%0Ato%20adapt%20to%20an%20unknown%20intrinsic%20dimension.%20We%20conduct%20extensive%20numerical%0Aexperiments%20to%20validate%20our%20theoretical%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.17765v4&entry.124074799=Read"},
{"title": "Protecting Confidentiality, Privacy and Integrity in Collaborative\n  Learning", "author": "Dong Chen and Alice Dethise and Istemi Ekin Akkus and Ivica Rimac and Klaus Satzke and Antti Koskela and Marco Canini and Wei Wang and Ruichuan Chen", "abstract": "  A collaboration between dataset owners and model owners is needed to\nfacilitate effective machine learning (ML) training. During this collaboration,\nhowever, dataset owners and model owners want to protect the confidentiality of\ntheir respective assets (i.e., datasets, models and training code), with the\ndataset owners also caring about the privacy of individual users whose data is\nin their datasets. Existing solutions either provide limited confidentiality\nfor models and training code, or suffer from privacy issues due to collusion.\n  We present Citadel++, a collaborative ML training system designed to\nsimultaneously protect the confidentiality of datasets, models and training\ncode as well as the privacy of individual users. Citadel++ enhances\ndifferential privacy mechanisms to safeguard the privacy of individual user\ndata while maintaining model utility. By employing Virtual Machine-level\nTrusted Execution Environments (TEEs) as well as the improved sandboxing and\nintegrity mechanisms through OS-level techniques, Citadel++ effectively\npreserves the confidentiality of datasets, models and training code, and\nenforces our privacy mechanisms even when the models and training code have\nbeen maliciously designed. Our experiments show that Citadel++ provides model\nutility and performance while adhering to the confidentiality and privacy\nrequirements of dataset owners and model owners, outperforming the\nstate-of-the-art privacy-preserving training systems by up to 543x on CPU and\n113x on GPU TEEs.\n", "link": "http://arxiv.org/abs/2412.08534v2", "date": "2025-04-17", "relevancy": 1.9262, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5122}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4822}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Protecting%20Confidentiality%2C%20Privacy%20and%20Integrity%20in%20Collaborative%0A%20%20Learning&body=Title%3A%20Protecting%20Confidentiality%2C%20Privacy%20and%20Integrity%20in%20Collaborative%0A%20%20Learning%0AAuthor%3A%20Dong%20Chen%20and%20Alice%20Dethise%20and%20Istemi%20Ekin%20Akkus%20and%20Ivica%20Rimac%20and%20Klaus%20Satzke%20and%20Antti%20Koskela%20and%20Marco%20Canini%20and%20Wei%20Wang%20and%20Ruichuan%20Chen%0AAbstract%3A%20%20%20A%20collaboration%20between%20dataset%20owners%20and%20model%20owners%20is%20needed%20to%0Afacilitate%20effective%20machine%20learning%20%28ML%29%20training.%20During%20this%20collaboration%2C%0Ahowever%2C%20dataset%20owners%20and%20model%20owners%20want%20to%20protect%20the%20confidentiality%20of%0Atheir%20respective%20assets%20%28i.e.%2C%20datasets%2C%20models%20and%20training%20code%29%2C%20with%20the%0Adataset%20owners%20also%20caring%20about%20the%20privacy%20of%20individual%20users%20whose%20data%20is%0Ain%20their%20datasets.%20Existing%20solutions%20either%20provide%20limited%20confidentiality%0Afor%20models%20and%20training%20code%2C%20or%20suffer%20from%20privacy%20issues%20due%20to%20collusion.%0A%20%20We%20present%20Citadel%2B%2B%2C%20a%20collaborative%20ML%20training%20system%20designed%20to%0Asimultaneously%20protect%20the%20confidentiality%20of%20datasets%2C%20models%20and%20training%0Acode%20as%20well%20as%20the%20privacy%20of%20individual%20users.%20Citadel%2B%2B%20enhances%0Adifferential%20privacy%20mechanisms%20to%20safeguard%20the%20privacy%20of%20individual%20user%0Adata%20while%20maintaining%20model%20utility.%20By%20employing%20Virtual%20Machine-level%0ATrusted%20Execution%20Environments%20%28TEEs%29%20as%20well%20as%20the%20improved%20sandboxing%20and%0Aintegrity%20mechanisms%20through%20OS-level%20techniques%2C%20Citadel%2B%2B%20effectively%0Apreserves%20the%20confidentiality%20of%20datasets%2C%20models%20and%20training%20code%2C%20and%0Aenforces%20our%20privacy%20mechanisms%20even%20when%20the%20models%20and%20training%20code%20have%0Abeen%20maliciously%20designed.%20Our%20experiments%20show%20that%20Citadel%2B%2B%20provides%20model%0Autility%20and%20performance%20while%20adhering%20to%20the%20confidentiality%20and%20privacy%0Arequirements%20of%20dataset%20owners%20and%20model%20owners%2C%20outperforming%20the%0Astate-of-the-art%20privacy-preserving%20training%20systems%20by%20up%20to%20543x%20on%20CPU%20and%0A113x%20on%20GPU%20TEEs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08534v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProtecting%2520Confidentiality%252C%2520Privacy%2520and%2520Integrity%2520in%2520Collaborative%250A%2520%2520Learning%26entry.906535625%3DDong%2520Chen%2520and%2520Alice%2520Dethise%2520and%2520Istemi%2520Ekin%2520Akkus%2520and%2520Ivica%2520Rimac%2520and%2520Klaus%2520Satzke%2520and%2520Antti%2520Koskela%2520and%2520Marco%2520Canini%2520and%2520Wei%2520Wang%2520and%2520Ruichuan%2520Chen%26entry.1292438233%3D%2520%2520A%2520collaboration%2520between%2520dataset%2520owners%2520and%2520model%2520owners%2520is%2520needed%2520to%250Afacilitate%2520effective%2520machine%2520learning%2520%2528ML%2529%2520training.%2520During%2520this%2520collaboration%252C%250Ahowever%252C%2520dataset%2520owners%2520and%2520model%2520owners%2520want%2520to%2520protect%2520the%2520confidentiality%2520of%250Atheir%2520respective%2520assets%2520%2528i.e.%252C%2520datasets%252C%2520models%2520and%2520training%2520code%2529%252C%2520with%2520the%250Adataset%2520owners%2520also%2520caring%2520about%2520the%2520privacy%2520of%2520individual%2520users%2520whose%2520data%2520is%250Ain%2520their%2520datasets.%2520Existing%2520solutions%2520either%2520provide%2520limited%2520confidentiality%250Afor%2520models%2520and%2520training%2520code%252C%2520or%2520suffer%2520from%2520privacy%2520issues%2520due%2520to%2520collusion.%250A%2520%2520We%2520present%2520Citadel%252B%252B%252C%2520a%2520collaborative%2520ML%2520training%2520system%2520designed%2520to%250Asimultaneously%2520protect%2520the%2520confidentiality%2520of%2520datasets%252C%2520models%2520and%2520training%250Acode%2520as%2520well%2520as%2520the%2520privacy%2520of%2520individual%2520users.%2520Citadel%252B%252B%2520enhances%250Adifferential%2520privacy%2520mechanisms%2520to%2520safeguard%2520the%2520privacy%2520of%2520individual%2520user%250Adata%2520while%2520maintaining%2520model%2520utility.%2520By%2520employing%2520Virtual%2520Machine-level%250ATrusted%2520Execution%2520Environments%2520%2528TEEs%2529%2520as%2520well%2520as%2520the%2520improved%2520sandboxing%2520and%250Aintegrity%2520mechanisms%2520through%2520OS-level%2520techniques%252C%2520Citadel%252B%252B%2520effectively%250Apreserves%2520the%2520confidentiality%2520of%2520datasets%252C%2520models%2520and%2520training%2520code%252C%2520and%250Aenforces%2520our%2520privacy%2520mechanisms%2520even%2520when%2520the%2520models%2520and%2520training%2520code%2520have%250Abeen%2520maliciously%2520designed.%2520Our%2520experiments%2520show%2520that%2520Citadel%252B%252B%2520provides%2520model%250Autility%2520and%2520performance%2520while%2520adhering%2520to%2520the%2520confidentiality%2520and%2520privacy%250Arequirements%2520of%2520dataset%2520owners%2520and%2520model%2520owners%252C%2520outperforming%2520the%250Astate-of-the-art%2520privacy-preserving%2520training%2520systems%2520by%2520up%2520to%2520543x%2520on%2520CPU%2520and%250A113x%2520on%2520GPU%2520TEEs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08534v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Protecting%20Confidentiality%2C%20Privacy%20and%20Integrity%20in%20Collaborative%0A%20%20Learning&entry.906535625=Dong%20Chen%20and%20Alice%20Dethise%20and%20Istemi%20Ekin%20Akkus%20and%20Ivica%20Rimac%20and%20Klaus%20Satzke%20and%20Antti%20Koskela%20and%20Marco%20Canini%20and%20Wei%20Wang%20and%20Ruichuan%20Chen&entry.1292438233=%20%20A%20collaboration%20between%20dataset%20owners%20and%20model%20owners%20is%20needed%20to%0Afacilitate%20effective%20machine%20learning%20%28ML%29%20training.%20During%20this%20collaboration%2C%0Ahowever%2C%20dataset%20owners%20and%20model%20owners%20want%20to%20protect%20the%20confidentiality%20of%0Atheir%20respective%20assets%20%28i.e.%2C%20datasets%2C%20models%20and%20training%20code%29%2C%20with%20the%0Adataset%20owners%20also%20caring%20about%20the%20privacy%20of%20individual%20users%20whose%20data%20is%0Ain%20their%20datasets.%20Existing%20solutions%20either%20provide%20limited%20confidentiality%0Afor%20models%20and%20training%20code%2C%20or%20suffer%20from%20privacy%20issues%20due%20to%20collusion.%0A%20%20We%20present%20Citadel%2B%2B%2C%20a%20collaborative%20ML%20training%20system%20designed%20to%0Asimultaneously%20protect%20the%20confidentiality%20of%20datasets%2C%20models%20and%20training%0Acode%20as%20well%20as%20the%20privacy%20of%20individual%20users.%20Citadel%2B%2B%20enhances%0Adifferential%20privacy%20mechanisms%20to%20safeguard%20the%20privacy%20of%20individual%20user%0Adata%20while%20maintaining%20model%20utility.%20By%20employing%20Virtual%20Machine-level%0ATrusted%20Execution%20Environments%20%28TEEs%29%20as%20well%20as%20the%20improved%20sandboxing%20and%0Aintegrity%20mechanisms%20through%20OS-level%20techniques%2C%20Citadel%2B%2B%20effectively%0Apreserves%20the%20confidentiality%20of%20datasets%2C%20models%20and%20training%20code%2C%20and%0Aenforces%20our%20privacy%20mechanisms%20even%20when%20the%20models%20and%20training%20code%20have%0Abeen%20maliciously%20designed.%20Our%20experiments%20show%20that%20Citadel%2B%2B%20provides%20model%0Autility%20and%20performance%20while%20adhering%20to%20the%20confidentiality%20and%20privacy%0Arequirements%20of%20dataset%20owners%20and%20model%20owners%2C%20outperforming%20the%0Astate-of-the-art%20privacy-preserving%20training%20systems%20by%20up%20to%20543x%20on%20CPU%20and%0A113x%20on%20GPU%20TEEs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08534v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


