<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250605.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Dy3DGS-SLAM: Monocular 3D Gaussian Splatting SLAM for Dynamic\n  Environments", "author": "Mingrui Li and Yiming Zhou and Hongxing Zhou and Xinggang Hu and Florian Roemer and Hongyu Wang and Ahmad Osman", "abstract": "  Current Simultaneous Localization and Mapping (SLAM) methods based on Neural\nRadiance Fields (NeRF) or 3D Gaussian Splatting excel in reconstructing static\n3D scenes but struggle with tracking and reconstruction in dynamic\nenvironments, such as real-world scenes with moving elements. Existing\nNeRF-based SLAM approaches addressing dynamic challenges typically rely on\nRGB-D inputs, with few methods accommodating pure RGB input. To overcome these\nlimitations, we propose Dy3DGS-SLAM, the first 3D Gaussian Splatting (3DGS)\nSLAM method for dynamic scenes using monocular RGB input. To address dynamic\ninterference, we fuse optical flow masks and depth masks through a\nprobabilistic model to obtain a fused dynamic mask. With only a single network\niteration, this can constrain tracking scales and refine rendered geometry.\nBased on the fused dynamic mask, we designed a novel motion loss to constrain\nthe pose estimation network for tracking. In mapping, we use the rendering loss\nof dynamic pixels, color, and depth to eliminate transient interference and\nocclusion caused by dynamic objects. Experimental results demonstrate that\nDy3DGS-SLAM achieves state-of-the-art tracking and rendering in dynamic\nenvironments, outperforming or matching existing RGB-D methods.\n", "link": "http://arxiv.org/abs/2506.05965v1", "date": "2025-06-06", "relevancy": 3.5798, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7942}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7022}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dy3DGS-SLAM%3A%20Monocular%203D%20Gaussian%20Splatting%20SLAM%20for%20Dynamic%0A%20%20Environments&body=Title%3A%20Dy3DGS-SLAM%3A%20Monocular%203D%20Gaussian%20Splatting%20SLAM%20for%20Dynamic%0A%20%20Environments%0AAuthor%3A%20Mingrui%20Li%20and%20Yiming%20Zhou%20and%20Hongxing%20Zhou%20and%20Xinggang%20Hu%20and%20Florian%20Roemer%20and%20Hongyu%20Wang%20and%20Ahmad%20Osman%0AAbstract%3A%20%20%20Current%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20methods%20based%20on%20Neural%0ARadiance%20Fields%20%28NeRF%29%20or%203D%20Gaussian%20Splatting%20excel%20in%20reconstructing%20static%0A3D%20scenes%20but%20struggle%20with%20tracking%20and%20reconstruction%20in%20dynamic%0Aenvironments%2C%20such%20as%20real-world%20scenes%20with%20moving%20elements.%20Existing%0ANeRF-based%20SLAM%20approaches%20addressing%20dynamic%20challenges%20typically%20rely%20on%0ARGB-D%20inputs%2C%20with%20few%20methods%20accommodating%20pure%20RGB%20input.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20Dy3DGS-SLAM%2C%20the%20first%203D%20Gaussian%20Splatting%20%283DGS%29%0ASLAM%20method%20for%20dynamic%20scenes%20using%20monocular%20RGB%20input.%20To%20address%20dynamic%0Ainterference%2C%20we%20fuse%20optical%20flow%20masks%20and%20depth%20masks%20through%20a%0Aprobabilistic%20model%20to%20obtain%20a%20fused%20dynamic%20mask.%20With%20only%20a%20single%20network%0Aiteration%2C%20this%20can%20constrain%20tracking%20scales%20and%20refine%20rendered%20geometry.%0ABased%20on%20the%20fused%20dynamic%20mask%2C%20we%20designed%20a%20novel%20motion%20loss%20to%20constrain%0Athe%20pose%20estimation%20network%20for%20tracking.%20In%20mapping%2C%20we%20use%20the%20rendering%20loss%0Aof%20dynamic%20pixels%2C%20color%2C%20and%20depth%20to%20eliminate%20transient%20interference%20and%0Aocclusion%20caused%20by%20dynamic%20objects.%20Experimental%20results%20demonstrate%20that%0ADy3DGS-SLAM%20achieves%20state-of-the-art%20tracking%20and%20rendering%20in%20dynamic%0Aenvironments%2C%20outperforming%20or%20matching%20existing%20RGB-D%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05965v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDy3DGS-SLAM%253A%2520Monocular%25203D%2520Gaussian%2520Splatting%2520SLAM%2520for%2520Dynamic%250A%2520%2520Environments%26entry.906535625%3DMingrui%2520Li%2520and%2520Yiming%2520Zhou%2520and%2520Hongxing%2520Zhou%2520and%2520Xinggang%2520Hu%2520and%2520Florian%2520Roemer%2520and%2520Hongyu%2520Wang%2520and%2520Ahmad%2520Osman%26entry.1292438233%3D%2520%2520Current%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529%2520methods%2520based%2520on%2520Neural%250ARadiance%2520Fields%2520%2528NeRF%2529%2520or%25203D%2520Gaussian%2520Splatting%2520excel%2520in%2520reconstructing%2520static%250A3D%2520scenes%2520but%2520struggle%2520with%2520tracking%2520and%2520reconstruction%2520in%2520dynamic%250Aenvironments%252C%2520such%2520as%2520real-world%2520scenes%2520with%2520moving%2520elements.%2520Existing%250ANeRF-based%2520SLAM%2520approaches%2520addressing%2520dynamic%2520challenges%2520typically%2520rely%2520on%250ARGB-D%2520inputs%252C%2520with%2520few%2520methods%2520accommodating%2520pure%2520RGB%2520input.%2520To%2520overcome%2520these%250Alimitations%252C%2520we%2520propose%2520Dy3DGS-SLAM%252C%2520the%2520first%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%250ASLAM%2520method%2520for%2520dynamic%2520scenes%2520using%2520monocular%2520RGB%2520input.%2520To%2520address%2520dynamic%250Ainterference%252C%2520we%2520fuse%2520optical%2520flow%2520masks%2520and%2520depth%2520masks%2520through%2520a%250Aprobabilistic%2520model%2520to%2520obtain%2520a%2520fused%2520dynamic%2520mask.%2520With%2520only%2520a%2520single%2520network%250Aiteration%252C%2520this%2520can%2520constrain%2520tracking%2520scales%2520and%2520refine%2520rendered%2520geometry.%250ABased%2520on%2520the%2520fused%2520dynamic%2520mask%252C%2520we%2520designed%2520a%2520novel%2520motion%2520loss%2520to%2520constrain%250Athe%2520pose%2520estimation%2520network%2520for%2520tracking.%2520In%2520mapping%252C%2520we%2520use%2520the%2520rendering%2520loss%250Aof%2520dynamic%2520pixels%252C%2520color%252C%2520and%2520depth%2520to%2520eliminate%2520transient%2520interference%2520and%250Aocclusion%2520caused%2520by%2520dynamic%2520objects.%2520Experimental%2520results%2520demonstrate%2520that%250ADy3DGS-SLAM%2520achieves%2520state-of-the-art%2520tracking%2520and%2520rendering%2520in%2520dynamic%250Aenvironments%252C%2520outperforming%2520or%2520matching%2520existing%2520RGB-D%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05965v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dy3DGS-SLAM%3A%20Monocular%203D%20Gaussian%20Splatting%20SLAM%20for%20Dynamic%0A%20%20Environments&entry.906535625=Mingrui%20Li%20and%20Yiming%20Zhou%20and%20Hongxing%20Zhou%20and%20Xinggang%20Hu%20and%20Florian%20Roemer%20and%20Hongyu%20Wang%20and%20Ahmad%20Osman&entry.1292438233=%20%20Current%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20methods%20based%20on%20Neural%0ARadiance%20Fields%20%28NeRF%29%20or%203D%20Gaussian%20Splatting%20excel%20in%20reconstructing%20static%0A3D%20scenes%20but%20struggle%20with%20tracking%20and%20reconstruction%20in%20dynamic%0Aenvironments%2C%20such%20as%20real-world%20scenes%20with%20moving%20elements.%20Existing%0ANeRF-based%20SLAM%20approaches%20addressing%20dynamic%20challenges%20typically%20rely%20on%0ARGB-D%20inputs%2C%20with%20few%20methods%20accommodating%20pure%20RGB%20input.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20Dy3DGS-SLAM%2C%20the%20first%203D%20Gaussian%20Splatting%20%283DGS%29%0ASLAM%20method%20for%20dynamic%20scenes%20using%20monocular%20RGB%20input.%20To%20address%20dynamic%0Ainterference%2C%20we%20fuse%20optical%20flow%20masks%20and%20depth%20masks%20through%20a%0Aprobabilistic%20model%20to%20obtain%20a%20fused%20dynamic%20mask.%20With%20only%20a%20single%20network%0Aiteration%2C%20this%20can%20constrain%20tracking%20scales%20and%20refine%20rendered%20geometry.%0ABased%20on%20the%20fused%20dynamic%20mask%2C%20we%20designed%20a%20novel%20motion%20loss%20to%20constrain%0Athe%20pose%20estimation%20network%20for%20tracking.%20In%20mapping%2C%20we%20use%20the%20rendering%20loss%0Aof%20dynamic%20pixels%2C%20color%2C%20and%20depth%20to%20eliminate%20transient%20interference%20and%0Aocclusion%20caused%20by%20dynamic%20objects.%20Experimental%20results%20demonstrate%20that%0ADy3DGS-SLAM%20achieves%20state-of-the-art%20tracking%20and%20rendering%20in%20dynamic%0Aenvironments%2C%20outperforming%20or%20matching%20existing%20RGB-D%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05965v1&entry.124074799=Read"},
{"title": "SurGSplat: Progressive Geometry-Constrained Gaussian Splatting for\n  Surgical Scene Reconstruction", "author": "Yuchao Zheng and Jianing Zhang and Guochen Ning and Hongen Liao", "abstract": "  Intraoperative navigation relies heavily on precise 3D reconstruction to\nensure accuracy and safety during surgical procedures. However, endoscopic\nscenarios present unique challenges, including sparse features and inconsistent\nlighting, which render many existing Structure-from-Motion (SfM)-based methods\ninadequate and prone to reconstruction failure. To mitigate these constraints,\nwe propose SurGSplat, a novel paradigm designed to progressively refine 3D\nGaussian Splatting (3DGS) through the integration of geometric constraints. By\nenabling the detailed reconstruction of vascular structures and other critical\nfeatures, SurGSplat provides surgeons with enhanced visual clarity,\nfacilitating precise intraoperative decision-making. Experimental evaluations\ndemonstrate that SurGSplat achieves superior performance in both novel view\nsynthesis (NVS) and pose estimation accuracy, establishing it as a\nhigh-fidelity and efficient solution for surgical scene reconstruction. More\ninformation and results can be found on the page https://surgsplat.github.io/.\n", "link": "http://arxiv.org/abs/2506.05935v1", "date": "2025-06-06", "relevancy": 3.3887, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7152}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6999}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6181}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SurGSplat%3A%20Progressive%20Geometry-Constrained%20Gaussian%20Splatting%20for%0A%20%20Surgical%20Scene%20Reconstruction&body=Title%3A%20SurGSplat%3A%20Progressive%20Geometry-Constrained%20Gaussian%20Splatting%20for%0A%20%20Surgical%20Scene%20Reconstruction%0AAuthor%3A%20Yuchao%20Zheng%20and%20Jianing%20Zhang%20and%20Guochen%20Ning%20and%20Hongen%20Liao%0AAbstract%3A%20%20%20Intraoperative%20navigation%20relies%20heavily%20on%20precise%203D%20reconstruction%20to%0Aensure%20accuracy%20and%20safety%20during%20surgical%20procedures.%20However%2C%20endoscopic%0Ascenarios%20present%20unique%20challenges%2C%20including%20sparse%20features%20and%20inconsistent%0Alighting%2C%20which%20render%20many%20existing%20Structure-from-Motion%20%28SfM%29-based%20methods%0Ainadequate%20and%20prone%20to%20reconstruction%20failure.%20To%20mitigate%20these%20constraints%2C%0Awe%20propose%20SurGSplat%2C%20a%20novel%20paradigm%20designed%20to%20progressively%20refine%203D%0AGaussian%20Splatting%20%283DGS%29%20through%20the%20integration%20of%20geometric%20constraints.%20By%0Aenabling%20the%20detailed%20reconstruction%20of%20vascular%20structures%20and%20other%20critical%0Afeatures%2C%20SurGSplat%20provides%20surgeons%20with%20enhanced%20visual%20clarity%2C%0Afacilitating%20precise%20intraoperative%20decision-making.%20Experimental%20evaluations%0Ademonstrate%20that%20SurGSplat%20achieves%20superior%20performance%20in%20both%20novel%20view%0Asynthesis%20%28NVS%29%20and%20pose%20estimation%20accuracy%2C%20establishing%20it%20as%20a%0Ahigh-fidelity%20and%20efficient%20solution%20for%20surgical%20scene%20reconstruction.%20More%0Ainformation%20and%20results%20can%20be%20found%20on%20the%20page%20https%3A//surgsplat.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05935v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurGSplat%253A%2520Progressive%2520Geometry-Constrained%2520Gaussian%2520Splatting%2520for%250A%2520%2520Surgical%2520Scene%2520Reconstruction%26entry.906535625%3DYuchao%2520Zheng%2520and%2520Jianing%2520Zhang%2520and%2520Guochen%2520Ning%2520and%2520Hongen%2520Liao%26entry.1292438233%3D%2520%2520Intraoperative%2520navigation%2520relies%2520heavily%2520on%2520precise%25203D%2520reconstruction%2520to%250Aensure%2520accuracy%2520and%2520safety%2520during%2520surgical%2520procedures.%2520However%252C%2520endoscopic%250Ascenarios%2520present%2520unique%2520challenges%252C%2520including%2520sparse%2520features%2520and%2520inconsistent%250Alighting%252C%2520which%2520render%2520many%2520existing%2520Structure-from-Motion%2520%2528SfM%2529-based%2520methods%250Ainadequate%2520and%2520prone%2520to%2520reconstruction%2520failure.%2520To%2520mitigate%2520these%2520constraints%252C%250Awe%2520propose%2520SurGSplat%252C%2520a%2520novel%2520paradigm%2520designed%2520to%2520progressively%2520refine%25203D%250AGaussian%2520Splatting%2520%25283DGS%2529%2520through%2520the%2520integration%2520of%2520geometric%2520constraints.%2520By%250Aenabling%2520the%2520detailed%2520reconstruction%2520of%2520vascular%2520structures%2520and%2520other%2520critical%250Afeatures%252C%2520SurGSplat%2520provides%2520surgeons%2520with%2520enhanced%2520visual%2520clarity%252C%250Afacilitating%2520precise%2520intraoperative%2520decision-making.%2520Experimental%2520evaluations%250Ademonstrate%2520that%2520SurGSplat%2520achieves%2520superior%2520performance%2520in%2520both%2520novel%2520view%250Asynthesis%2520%2528NVS%2529%2520and%2520pose%2520estimation%2520accuracy%252C%2520establishing%2520it%2520as%2520a%250Ahigh-fidelity%2520and%2520efficient%2520solution%2520for%2520surgical%2520scene%2520reconstruction.%2520More%250Ainformation%2520and%2520results%2520can%2520be%2520found%2520on%2520the%2520page%2520https%253A//surgsplat.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05935v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SurGSplat%3A%20Progressive%20Geometry-Constrained%20Gaussian%20Splatting%20for%0A%20%20Surgical%20Scene%20Reconstruction&entry.906535625=Yuchao%20Zheng%20and%20Jianing%20Zhang%20and%20Guochen%20Ning%20and%20Hongen%20Liao&entry.1292438233=%20%20Intraoperative%20navigation%20relies%20heavily%20on%20precise%203D%20reconstruction%20to%0Aensure%20accuracy%20and%20safety%20during%20surgical%20procedures.%20However%2C%20endoscopic%0Ascenarios%20present%20unique%20challenges%2C%20including%20sparse%20features%20and%20inconsistent%0Alighting%2C%20which%20render%20many%20existing%20Structure-from-Motion%20%28SfM%29-based%20methods%0Ainadequate%20and%20prone%20to%20reconstruction%20failure.%20To%20mitigate%20these%20constraints%2C%0Awe%20propose%20SurGSplat%2C%20a%20novel%20paradigm%20designed%20to%20progressively%20refine%203D%0AGaussian%20Splatting%20%283DGS%29%20through%20the%20integration%20of%20geometric%20constraints.%20By%0Aenabling%20the%20detailed%20reconstruction%20of%20vascular%20structures%20and%20other%20critical%0Afeatures%2C%20SurGSplat%20provides%20surgeons%20with%20enhanced%20visual%20clarity%2C%0Afacilitating%20precise%20intraoperative%20decision-making.%20Experimental%20evaluations%0Ademonstrate%20that%20SurGSplat%20achieves%20superior%20performance%20in%20both%20novel%20view%0Asynthesis%20%28NVS%29%20and%20pose%20estimation%20accuracy%2C%20establishing%20it%20as%20a%0Ahigh-fidelity%20and%20efficient%20solution%20for%20surgical%20scene%20reconstruction.%20More%0Ainformation%20and%20results%20can%20be%20found%20on%20the%20page%20https%3A//surgsplat.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05935v1&entry.124074799=Read"},
{"title": "BecomingLit: Relightable Gaussian Avatars with Hybrid Neural Shading", "author": "Jonathan Schmidt and Simon Giebenhain and Matthias Niessner", "abstract": "  We introduce BecomingLit, a novel method for reconstructing relightable,\nhigh-resolution head avatars that can be rendered from novel viewpoints at\ninteractive rates. Therefore, we propose a new low-cost light stage capture\nsetup, tailored specifically towards capturing faces. Using this setup, we\ncollect a novel dataset consisting of diverse multi-view sequences of numerous\nsubjects under varying illumination conditions and facial expressions. By\nleveraging our new dataset, we introduce a new relightable avatar\nrepresentation based on 3D Gaussian primitives that we animate with a\nparametric head model and an expression-dependent dynamics module. We propose a\nnew hybrid neural shading approach, combining a neural diffuse BRDF with an\nanalytical specular term. Our method reconstructs disentangled materials from\nour dynamic light stage recordings and enables all-frequency relighting of our\navatars with both point lights and environment maps. In addition, our avatars\ncan easily be animated and controlled from monocular videos. We validate our\napproach in extensive experiments on our dataset, where we consistently\noutperform existing state-of-the-art methods in relighting and reenactment by a\nsignificant margin.\n", "link": "http://arxiv.org/abs/2506.06271v1", "date": "2025-06-06", "relevancy": 3.3037, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6913}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6913}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BecomingLit%3A%20Relightable%20Gaussian%20Avatars%20with%20Hybrid%20Neural%20Shading&body=Title%3A%20BecomingLit%3A%20Relightable%20Gaussian%20Avatars%20with%20Hybrid%20Neural%20Shading%0AAuthor%3A%20Jonathan%20Schmidt%20and%20Simon%20Giebenhain%20and%20Matthias%20Niessner%0AAbstract%3A%20%20%20We%20introduce%20BecomingLit%2C%20a%20novel%20method%20for%20reconstructing%20relightable%2C%0Ahigh-resolution%20head%20avatars%20that%20can%20be%20rendered%20from%20novel%20viewpoints%20at%0Ainteractive%20rates.%20Therefore%2C%20we%20propose%20a%20new%20low-cost%20light%20stage%20capture%0Asetup%2C%20tailored%20specifically%20towards%20capturing%20faces.%20Using%20this%20setup%2C%20we%0Acollect%20a%20novel%20dataset%20consisting%20of%20diverse%20multi-view%20sequences%20of%20numerous%0Asubjects%20under%20varying%20illumination%20conditions%20and%20facial%20expressions.%20By%0Aleveraging%20our%20new%20dataset%2C%20we%20introduce%20a%20new%20relightable%20avatar%0Arepresentation%20based%20on%203D%20Gaussian%20primitives%20that%20we%20animate%20with%20a%0Aparametric%20head%20model%20and%20an%20expression-dependent%20dynamics%20module.%20We%20propose%20a%0Anew%20hybrid%20neural%20shading%20approach%2C%20combining%20a%20neural%20diffuse%20BRDF%20with%20an%0Aanalytical%20specular%20term.%20Our%20method%20reconstructs%20disentangled%20materials%20from%0Aour%20dynamic%20light%20stage%20recordings%20and%20enables%20all-frequency%20relighting%20of%20our%0Aavatars%20with%20both%20point%20lights%20and%20environment%20maps.%20In%20addition%2C%20our%20avatars%0Acan%20easily%20be%20animated%20and%20controlled%20from%20monocular%20videos.%20We%20validate%20our%0Aapproach%20in%20extensive%20experiments%20on%20our%20dataset%2C%20where%20we%20consistently%0Aoutperform%20existing%20state-of-the-art%20methods%20in%20relighting%20and%20reenactment%20by%20a%0Asignificant%20margin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06271v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBecomingLit%253A%2520Relightable%2520Gaussian%2520Avatars%2520with%2520Hybrid%2520Neural%2520Shading%26entry.906535625%3DJonathan%2520Schmidt%2520and%2520Simon%2520Giebenhain%2520and%2520Matthias%2520Niessner%26entry.1292438233%3D%2520%2520We%2520introduce%2520BecomingLit%252C%2520a%2520novel%2520method%2520for%2520reconstructing%2520relightable%252C%250Ahigh-resolution%2520head%2520avatars%2520that%2520can%2520be%2520rendered%2520from%2520novel%2520viewpoints%2520at%250Ainteractive%2520rates.%2520Therefore%252C%2520we%2520propose%2520a%2520new%2520low-cost%2520light%2520stage%2520capture%250Asetup%252C%2520tailored%2520specifically%2520towards%2520capturing%2520faces.%2520Using%2520this%2520setup%252C%2520we%250Acollect%2520a%2520novel%2520dataset%2520consisting%2520of%2520diverse%2520multi-view%2520sequences%2520of%2520numerous%250Asubjects%2520under%2520varying%2520illumination%2520conditions%2520and%2520facial%2520expressions.%2520By%250Aleveraging%2520our%2520new%2520dataset%252C%2520we%2520introduce%2520a%2520new%2520relightable%2520avatar%250Arepresentation%2520based%2520on%25203D%2520Gaussian%2520primitives%2520that%2520we%2520animate%2520with%2520a%250Aparametric%2520head%2520model%2520and%2520an%2520expression-dependent%2520dynamics%2520module.%2520We%2520propose%2520a%250Anew%2520hybrid%2520neural%2520shading%2520approach%252C%2520combining%2520a%2520neural%2520diffuse%2520BRDF%2520with%2520an%250Aanalytical%2520specular%2520term.%2520Our%2520method%2520reconstructs%2520disentangled%2520materials%2520from%250Aour%2520dynamic%2520light%2520stage%2520recordings%2520and%2520enables%2520all-frequency%2520relighting%2520of%2520our%250Aavatars%2520with%2520both%2520point%2520lights%2520and%2520environment%2520maps.%2520In%2520addition%252C%2520our%2520avatars%250Acan%2520easily%2520be%2520animated%2520and%2520controlled%2520from%2520monocular%2520videos.%2520We%2520validate%2520our%250Aapproach%2520in%2520extensive%2520experiments%2520on%2520our%2520dataset%252C%2520where%2520we%2520consistently%250Aoutperform%2520existing%2520state-of-the-art%2520methods%2520in%2520relighting%2520and%2520reenactment%2520by%2520a%250Asignificant%2520margin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06271v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BecomingLit%3A%20Relightable%20Gaussian%20Avatars%20with%20Hybrid%20Neural%20Shading&entry.906535625=Jonathan%20Schmidt%20and%20Simon%20Giebenhain%20and%20Matthias%20Niessner&entry.1292438233=%20%20We%20introduce%20BecomingLit%2C%20a%20novel%20method%20for%20reconstructing%20relightable%2C%0Ahigh-resolution%20head%20avatars%20that%20can%20be%20rendered%20from%20novel%20viewpoints%20at%0Ainteractive%20rates.%20Therefore%2C%20we%20propose%20a%20new%20low-cost%20light%20stage%20capture%0Asetup%2C%20tailored%20specifically%20towards%20capturing%20faces.%20Using%20this%20setup%2C%20we%0Acollect%20a%20novel%20dataset%20consisting%20of%20diverse%20multi-view%20sequences%20of%20numerous%0Asubjects%20under%20varying%20illumination%20conditions%20and%20facial%20expressions.%20By%0Aleveraging%20our%20new%20dataset%2C%20we%20introduce%20a%20new%20relightable%20avatar%0Arepresentation%20based%20on%203D%20Gaussian%20primitives%20that%20we%20animate%20with%20a%0Aparametric%20head%20model%20and%20an%20expression-dependent%20dynamics%20module.%20We%20propose%20a%0Anew%20hybrid%20neural%20shading%20approach%2C%20combining%20a%20neural%20diffuse%20BRDF%20with%20an%0Aanalytical%20specular%20term.%20Our%20method%20reconstructs%20disentangled%20materials%20from%0Aour%20dynamic%20light%20stage%20recordings%20and%20enables%20all-frequency%20relighting%20of%20our%0Aavatars%20with%20both%20point%20lights%20and%20environment%20maps.%20In%20addition%2C%20our%20avatars%0Acan%20easily%20be%20animated%20and%20controlled%20from%20monocular%20videos.%20We%20validate%20our%0Aapproach%20in%20extensive%20experiments%20on%20our%20dataset%2C%20where%20we%20consistently%0Aoutperform%20existing%20state-of-the-art%20methods%20in%20relighting%20and%20reenactment%20by%20a%0Asignificant%20margin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06271v1&entry.124074799=Read"},
{"title": "HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided\n  Versatile Diffusion", "author": "Shiyi Zhang and Dong Liang and Hairong Zheng and Yihang Zhou", "abstract": "  Reconstructing visual information from brain activity bridges the gap between\nneuroscience and computer vision. Even though progress has been made in\ndecoding images from fMRI using generative models, a challenge remains in\naccurately recovering highly complex visual stimuli. This difficulty stems from\ntheir elemental density and diversity, sophisticated spatial structures, and\nmultifaceted semantic information.\n  To address these challenges, we propose HAVIR that contains two adapters: (1)\nThe AutoKL Adapter transforms fMRI voxels into a latent diffusion prior,\ncapturing topological structures; (2) The CLIP Adapter converts the voxels to\nCLIP text and image embeddings, containing semantic information. These\ncomplementary representations are fused by Versatile Diffusion to generate the\nfinal reconstructed image. To extract the most essential semantic information\nfrom complex scenarios, the CLIP Adapter is trained with text captions\ndescribing the visual stimuli and their corresponding semantic images\nsynthesized from these captions. The experimental results demonstrate that\nHAVIR effectively reconstructs both structural features and semantic\ninformation of visual stimuli even in complex scenarios, outperforming existing\nmodels.\n", "link": "http://arxiv.org/abs/2506.06035v1", "date": "2025-06-06", "relevancy": 3.0852, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6196}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6196}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HAVIR%3A%20HierArchical%20Vision%20to%20Image%20Reconstruction%20using%20CLIP-Guided%0A%20%20Versatile%20Diffusion&body=Title%3A%20HAVIR%3A%20HierArchical%20Vision%20to%20Image%20Reconstruction%20using%20CLIP-Guided%0A%20%20Versatile%20Diffusion%0AAuthor%3A%20Shiyi%20Zhang%20and%20Dong%20Liang%20and%20Hairong%20Zheng%20and%20Yihang%20Zhou%0AAbstract%3A%20%20%20Reconstructing%20visual%20information%20from%20brain%20activity%20bridges%20the%20gap%20between%0Aneuroscience%20and%20computer%20vision.%20Even%20though%20progress%20has%20been%20made%20in%0Adecoding%20images%20from%20fMRI%20using%20generative%20models%2C%20a%20challenge%20remains%20in%0Aaccurately%20recovering%20highly%20complex%20visual%20stimuli.%20This%20difficulty%20stems%20from%0Atheir%20elemental%20density%20and%20diversity%2C%20sophisticated%20spatial%20structures%2C%20and%0Amultifaceted%20semantic%20information.%0A%20%20To%20address%20these%20challenges%2C%20we%20propose%20HAVIR%20that%20contains%20two%20adapters%3A%20%281%29%0AThe%20AutoKL%20Adapter%20transforms%20fMRI%20voxels%20into%20a%20latent%20diffusion%20prior%2C%0Acapturing%20topological%20structures%3B%20%282%29%20The%20CLIP%20Adapter%20converts%20the%20voxels%20to%0ACLIP%20text%20and%20image%20embeddings%2C%20containing%20semantic%20information.%20These%0Acomplementary%20representations%20are%20fused%20by%20Versatile%20Diffusion%20to%20generate%20the%0Afinal%20reconstructed%20image.%20To%20extract%20the%20most%20essential%20semantic%20information%0Afrom%20complex%20scenarios%2C%20the%20CLIP%20Adapter%20is%20trained%20with%20text%20captions%0Adescribing%20the%20visual%20stimuli%20and%20their%20corresponding%20semantic%20images%0Asynthesized%20from%20these%20captions.%20The%20experimental%20results%20demonstrate%20that%0AHAVIR%20effectively%20reconstructs%20both%20structural%20features%20and%20semantic%0Ainformation%20of%20visual%20stimuli%20even%20in%20complex%20scenarios%2C%20outperforming%20existing%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06035v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHAVIR%253A%2520HierArchical%2520Vision%2520to%2520Image%2520Reconstruction%2520using%2520CLIP-Guided%250A%2520%2520Versatile%2520Diffusion%26entry.906535625%3DShiyi%2520Zhang%2520and%2520Dong%2520Liang%2520and%2520Hairong%2520Zheng%2520and%2520Yihang%2520Zhou%26entry.1292438233%3D%2520%2520Reconstructing%2520visual%2520information%2520from%2520brain%2520activity%2520bridges%2520the%2520gap%2520between%250Aneuroscience%2520and%2520computer%2520vision.%2520Even%2520though%2520progress%2520has%2520been%2520made%2520in%250Adecoding%2520images%2520from%2520fMRI%2520using%2520generative%2520models%252C%2520a%2520challenge%2520remains%2520in%250Aaccurately%2520recovering%2520highly%2520complex%2520visual%2520stimuli.%2520This%2520difficulty%2520stems%2520from%250Atheir%2520elemental%2520density%2520and%2520diversity%252C%2520sophisticated%2520spatial%2520structures%252C%2520and%250Amultifaceted%2520semantic%2520information.%250A%2520%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520HAVIR%2520that%2520contains%2520two%2520adapters%253A%2520%25281%2529%250AThe%2520AutoKL%2520Adapter%2520transforms%2520fMRI%2520voxels%2520into%2520a%2520latent%2520diffusion%2520prior%252C%250Acapturing%2520topological%2520structures%253B%2520%25282%2529%2520The%2520CLIP%2520Adapter%2520converts%2520the%2520voxels%2520to%250ACLIP%2520text%2520and%2520image%2520embeddings%252C%2520containing%2520semantic%2520information.%2520These%250Acomplementary%2520representations%2520are%2520fused%2520by%2520Versatile%2520Diffusion%2520to%2520generate%2520the%250Afinal%2520reconstructed%2520image.%2520To%2520extract%2520the%2520most%2520essential%2520semantic%2520information%250Afrom%2520complex%2520scenarios%252C%2520the%2520CLIP%2520Adapter%2520is%2520trained%2520with%2520text%2520captions%250Adescribing%2520the%2520visual%2520stimuli%2520and%2520their%2520corresponding%2520semantic%2520images%250Asynthesized%2520from%2520these%2520captions.%2520The%2520experimental%2520results%2520demonstrate%2520that%250AHAVIR%2520effectively%2520reconstructs%2520both%2520structural%2520features%2520and%2520semantic%250Ainformation%2520of%2520visual%2520stimuli%2520even%2520in%2520complex%2520scenarios%252C%2520outperforming%2520existing%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06035v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HAVIR%3A%20HierArchical%20Vision%20to%20Image%20Reconstruction%20using%20CLIP-Guided%0A%20%20Versatile%20Diffusion&entry.906535625=Shiyi%20Zhang%20and%20Dong%20Liang%20and%20Hairong%20Zheng%20and%20Yihang%20Zhou&entry.1292438233=%20%20Reconstructing%20visual%20information%20from%20brain%20activity%20bridges%20the%20gap%20between%0Aneuroscience%20and%20computer%20vision.%20Even%20though%20progress%20has%20been%20made%20in%0Adecoding%20images%20from%20fMRI%20using%20generative%20models%2C%20a%20challenge%20remains%20in%0Aaccurately%20recovering%20highly%20complex%20visual%20stimuli.%20This%20difficulty%20stems%20from%0Atheir%20elemental%20density%20and%20diversity%2C%20sophisticated%20spatial%20structures%2C%20and%0Amultifaceted%20semantic%20information.%0A%20%20To%20address%20these%20challenges%2C%20we%20propose%20HAVIR%20that%20contains%20two%20adapters%3A%20%281%29%0AThe%20AutoKL%20Adapter%20transforms%20fMRI%20voxels%20into%20a%20latent%20diffusion%20prior%2C%0Acapturing%20topological%20structures%3B%20%282%29%20The%20CLIP%20Adapter%20converts%20the%20voxels%20to%0ACLIP%20text%20and%20image%20embeddings%2C%20containing%20semantic%20information.%20These%0Acomplementary%20representations%20are%20fused%20by%20Versatile%20Diffusion%20to%20generate%20the%0Afinal%20reconstructed%20image.%20To%20extract%20the%20most%20essential%20semantic%20information%0Afrom%20complex%20scenarios%2C%20the%20CLIP%20Adapter%20is%20trained%20with%20text%20captions%0Adescribing%20the%20visual%20stimuli%20and%20their%20corresponding%20semantic%20images%0Asynthesized%20from%20these%20captions.%20The%20experimental%20results%20demonstrate%20that%0AHAVIR%20effectively%20reconstructs%20both%20structural%20features%20and%20semantic%0Ainformation%20of%20visual%20stimuli%20even%20in%20complex%20scenarios%2C%20outperforming%20existing%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06035v1&entry.124074799=Read"},
{"title": "GenIR: Generative Visual Feedback for Mental Image Retrieval", "author": "Diji Yang and Minghao Liu and Chung-Hsiang Lo and Yi Zhang and James Davis", "abstract": "  Vision-language models (VLMs) have shown strong performance on text-to-image\nretrieval benchmarks. However, bridging this success to real-world applications\nremains a challenge. In practice, human search behavior is rarely a one-shot\naction. Instead, it is often a multi-round process guided by clues in mind,\nthat is, a mental image ranging from vague recollections to vivid mental\nrepresentations of the target image. Motivated by this gap, we study the task\nof Mental Image Retrieval (MIR), which targets the realistic yet underexplored\nsetting where users refine their search for a mentally envisioned image through\nmulti-round interactions with an image search engine. Central to successful\ninteractive retrieval is the capability of machines to provide users with\nclear, actionable feedback; however, existing methods rely on indirect or\nabstract verbal feedback, which can be ambiguous, misleading, or ineffective\nfor users to refine the query. To overcome this, we propose GenIR, a generative\nmulti-round retrieval paradigm leveraging diffusion-based image generation to\nexplicitly reify the AI system's understanding at each round. These synthetic\nvisual representations provide clear, interpretable feedback, enabling users to\nrefine their queries intuitively and effectively. We further introduce a fully\nautomated pipeline to generate a high-quality multi-round MIR dataset.\nExperimental results demonstrate that GenIR significantly outperforms existing\ninteractive methods in the MIR scenario. This work establishes a new task with\na dataset and an effective generative retrieval method, providing a foundation\nfor future research in this direction.\n", "link": "http://arxiv.org/abs/2506.06220v1", "date": "2025-06-06", "relevancy": 3.0678, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6357}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6131}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenIR%3A%20Generative%20Visual%20Feedback%20for%20Mental%20Image%20Retrieval&body=Title%3A%20GenIR%3A%20Generative%20Visual%20Feedback%20for%20Mental%20Image%20Retrieval%0AAuthor%3A%20Diji%20Yang%20and%20Minghao%20Liu%20and%20Chung-Hsiang%20Lo%20and%20Yi%20Zhang%20and%20James%20Davis%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20have%20shown%20strong%20performance%20on%20text-to-image%0Aretrieval%20benchmarks.%20However%2C%20bridging%20this%20success%20to%20real-world%20applications%0Aremains%20a%20challenge.%20In%20practice%2C%20human%20search%20behavior%20is%20rarely%20a%20one-shot%0Aaction.%20Instead%2C%20it%20is%20often%20a%20multi-round%20process%20guided%20by%20clues%20in%20mind%2C%0Athat%20is%2C%20a%20mental%20image%20ranging%20from%20vague%20recollections%20to%20vivid%20mental%0Arepresentations%20of%20the%20target%20image.%20Motivated%20by%20this%20gap%2C%20we%20study%20the%20task%0Aof%20Mental%20Image%20Retrieval%20%28MIR%29%2C%20which%20targets%20the%20realistic%20yet%20underexplored%0Asetting%20where%20users%20refine%20their%20search%20for%20a%20mentally%20envisioned%20image%20through%0Amulti-round%20interactions%20with%20an%20image%20search%20engine.%20Central%20to%20successful%0Ainteractive%20retrieval%20is%20the%20capability%20of%20machines%20to%20provide%20users%20with%0Aclear%2C%20actionable%20feedback%3B%20however%2C%20existing%20methods%20rely%20on%20indirect%20or%0Aabstract%20verbal%20feedback%2C%20which%20can%20be%20ambiguous%2C%20misleading%2C%20or%20ineffective%0Afor%20users%20to%20refine%20the%20query.%20To%20overcome%20this%2C%20we%20propose%20GenIR%2C%20a%20generative%0Amulti-round%20retrieval%20paradigm%20leveraging%20diffusion-based%20image%20generation%20to%0Aexplicitly%20reify%20the%20AI%20system%27s%20understanding%20at%20each%20round.%20These%20synthetic%0Avisual%20representations%20provide%20clear%2C%20interpretable%20feedback%2C%20enabling%20users%20to%0Arefine%20their%20queries%20intuitively%20and%20effectively.%20We%20further%20introduce%20a%20fully%0Aautomated%20pipeline%20to%20generate%20a%20high-quality%20multi-round%20MIR%20dataset.%0AExperimental%20results%20demonstrate%20that%20GenIR%20significantly%20outperforms%20existing%0Ainteractive%20methods%20in%20the%20MIR%20scenario.%20This%20work%20establishes%20a%20new%20task%20with%0Aa%20dataset%20and%20an%20effective%20generative%20retrieval%20method%2C%20providing%20a%20foundation%0Afor%20future%20research%20in%20this%20direction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06220v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenIR%253A%2520Generative%2520Visual%2520Feedback%2520for%2520Mental%2520Image%2520Retrieval%26entry.906535625%3DDiji%2520Yang%2520and%2520Minghao%2520Liu%2520and%2520Chung-Hsiang%2520Lo%2520and%2520Yi%2520Zhang%2520and%2520James%2520Davis%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520have%2520shown%2520strong%2520performance%2520on%2520text-to-image%250Aretrieval%2520benchmarks.%2520However%252C%2520bridging%2520this%2520success%2520to%2520real-world%2520applications%250Aremains%2520a%2520challenge.%2520In%2520practice%252C%2520human%2520search%2520behavior%2520is%2520rarely%2520a%2520one-shot%250Aaction.%2520Instead%252C%2520it%2520is%2520often%2520a%2520multi-round%2520process%2520guided%2520by%2520clues%2520in%2520mind%252C%250Athat%2520is%252C%2520a%2520mental%2520image%2520ranging%2520from%2520vague%2520recollections%2520to%2520vivid%2520mental%250Arepresentations%2520of%2520the%2520target%2520image.%2520Motivated%2520by%2520this%2520gap%252C%2520we%2520study%2520the%2520task%250Aof%2520Mental%2520Image%2520Retrieval%2520%2528MIR%2529%252C%2520which%2520targets%2520the%2520realistic%2520yet%2520underexplored%250Asetting%2520where%2520users%2520refine%2520their%2520search%2520for%2520a%2520mentally%2520envisioned%2520image%2520through%250Amulti-round%2520interactions%2520with%2520an%2520image%2520search%2520engine.%2520Central%2520to%2520successful%250Ainteractive%2520retrieval%2520is%2520the%2520capability%2520of%2520machines%2520to%2520provide%2520users%2520with%250Aclear%252C%2520actionable%2520feedback%253B%2520however%252C%2520existing%2520methods%2520rely%2520on%2520indirect%2520or%250Aabstract%2520verbal%2520feedback%252C%2520which%2520can%2520be%2520ambiguous%252C%2520misleading%252C%2520or%2520ineffective%250Afor%2520users%2520to%2520refine%2520the%2520query.%2520To%2520overcome%2520this%252C%2520we%2520propose%2520GenIR%252C%2520a%2520generative%250Amulti-round%2520retrieval%2520paradigm%2520leveraging%2520diffusion-based%2520image%2520generation%2520to%250Aexplicitly%2520reify%2520the%2520AI%2520system%2527s%2520understanding%2520at%2520each%2520round.%2520These%2520synthetic%250Avisual%2520representations%2520provide%2520clear%252C%2520interpretable%2520feedback%252C%2520enabling%2520users%2520to%250Arefine%2520their%2520queries%2520intuitively%2520and%2520effectively.%2520We%2520further%2520introduce%2520a%2520fully%250Aautomated%2520pipeline%2520to%2520generate%2520a%2520high-quality%2520multi-round%2520MIR%2520dataset.%250AExperimental%2520results%2520demonstrate%2520that%2520GenIR%2520significantly%2520outperforms%2520existing%250Ainteractive%2520methods%2520in%2520the%2520MIR%2520scenario.%2520This%2520work%2520establishes%2520a%2520new%2520task%2520with%250Aa%2520dataset%2520and%2520an%2520effective%2520generative%2520retrieval%2520method%252C%2520providing%2520a%2520foundation%250Afor%2520future%2520research%2520in%2520this%2520direction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06220v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenIR%3A%20Generative%20Visual%20Feedback%20for%20Mental%20Image%20Retrieval&entry.906535625=Diji%20Yang%20and%20Minghao%20Liu%20and%20Chung-Hsiang%20Lo%20and%20Yi%20Zhang%20and%20James%20Davis&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20have%20shown%20strong%20performance%20on%20text-to-image%0Aretrieval%20benchmarks.%20However%2C%20bridging%20this%20success%20to%20real-world%20applications%0Aremains%20a%20challenge.%20In%20practice%2C%20human%20search%20behavior%20is%20rarely%20a%20one-shot%0Aaction.%20Instead%2C%20it%20is%20often%20a%20multi-round%20process%20guided%20by%20clues%20in%20mind%2C%0Athat%20is%2C%20a%20mental%20image%20ranging%20from%20vague%20recollections%20to%20vivid%20mental%0Arepresentations%20of%20the%20target%20image.%20Motivated%20by%20this%20gap%2C%20we%20study%20the%20task%0Aof%20Mental%20Image%20Retrieval%20%28MIR%29%2C%20which%20targets%20the%20realistic%20yet%20underexplored%0Asetting%20where%20users%20refine%20their%20search%20for%20a%20mentally%20envisioned%20image%20through%0Amulti-round%20interactions%20with%20an%20image%20search%20engine.%20Central%20to%20successful%0Ainteractive%20retrieval%20is%20the%20capability%20of%20machines%20to%20provide%20users%20with%0Aclear%2C%20actionable%20feedback%3B%20however%2C%20existing%20methods%20rely%20on%20indirect%20or%0Aabstract%20verbal%20feedback%2C%20which%20can%20be%20ambiguous%2C%20misleading%2C%20or%20ineffective%0Afor%20users%20to%20refine%20the%20query.%20To%20overcome%20this%2C%20we%20propose%20GenIR%2C%20a%20generative%0Amulti-round%20retrieval%20paradigm%20leveraging%20diffusion-based%20image%20generation%20to%0Aexplicitly%20reify%20the%20AI%20system%27s%20understanding%20at%20each%20round.%20These%20synthetic%0Avisual%20representations%20provide%20clear%2C%20interpretable%20feedback%2C%20enabling%20users%20to%0Arefine%20their%20queries%20intuitively%20and%20effectively.%20We%20further%20introduce%20a%20fully%0Aautomated%20pipeline%20to%20generate%20a%20high-quality%20multi-round%20MIR%20dataset.%0AExperimental%20results%20demonstrate%20that%20GenIR%20significantly%20outperforms%20existing%0Ainteractive%20methods%20in%20the%20MIR%20scenario.%20This%20work%20establishes%20a%20new%20task%20with%0Aa%20dataset%20and%20an%20effective%20generative%20retrieval%20method%2C%20providing%20a%20foundation%0Afor%20future%20research%20in%20this%20direction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06220v1&entry.124074799=Read"},
{"title": "Visual Graph Arena: Evaluating Visual Conceptualization of Vision and\n  Multimodal Large Language Models", "author": "Zahra Babaiee and Peyman M. Kiasari and Daniela Rus and Radu Grosu", "abstract": "  Recent advancements in multimodal large language models have driven\nbreakthroughs in visual question answering. Yet, a critical gap persists,\n`conceptualization'-the ability to recognize and reason about the same concept\ndespite variations in visual form, a basic ability of human reasoning. To\naddress this challenge, we introduce the Visual Graph Arena (VGA), a dataset\nfeaturing six graph-based tasks designed to evaluate and improve AI systems'\ncapacity for visual abstraction. VGA uses diverse graph layouts (e.g.,\nKamada-Kawai vs. planar) to test reasoning independent of visual form.\nExperiments with state-of-the-art vision models and multimodal LLMs reveal a\nstriking divide: humans achieved near-perfect accuracy across tasks, while\nmodels totally failed on isomorphism detection and showed limited success in\npath/cycle tasks. We further identify behavioral anomalies suggesting\npseudo-intelligent pattern matching rather than genuine understanding. These\nfindings underscore fundamental limitations in current AI models for visual\nunderstanding. By isolating the challenge of representation-invariant\nreasoning, the VGA provides a framework to drive progress toward human-like\nconceptualization in AI visual models. The Visual Graph Arena is available at:\n\\href{https://vga.csail.mit.edu/}{vga.csail.mit.edu}\n", "link": "http://arxiv.org/abs/2506.06242v1", "date": "2025-06-06", "relevancy": 3.0588, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.634}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.634}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Graph%20Arena%3A%20Evaluating%20Visual%20Conceptualization%20of%20Vision%20and%0A%20%20Multimodal%20Large%20Language%20Models&body=Title%3A%20Visual%20Graph%20Arena%3A%20Evaluating%20Visual%20Conceptualization%20of%20Vision%20and%0A%20%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Zahra%20Babaiee%20and%20Peyman%20M.%20Kiasari%20and%20Daniela%20Rus%20and%20Radu%20Grosu%0AAbstract%3A%20%20%20Recent%20advancements%20in%20multimodal%20large%20language%20models%20have%20driven%0Abreakthroughs%20in%20visual%20question%20answering.%20Yet%2C%20a%20critical%20gap%20persists%2C%0A%60conceptualization%27-the%20ability%20to%20recognize%20and%20reason%20about%20the%20same%20concept%0Adespite%20variations%20in%20visual%20form%2C%20a%20basic%20ability%20of%20human%20reasoning.%20To%0Aaddress%20this%20challenge%2C%20we%20introduce%20the%20Visual%20Graph%20Arena%20%28VGA%29%2C%20a%20dataset%0Afeaturing%20six%20graph-based%20tasks%20designed%20to%20evaluate%20and%20improve%20AI%20systems%27%0Acapacity%20for%20visual%20abstraction.%20VGA%20uses%20diverse%20graph%20layouts%20%28e.g.%2C%0AKamada-Kawai%20vs.%20planar%29%20to%20test%20reasoning%20independent%20of%20visual%20form.%0AExperiments%20with%20state-of-the-art%20vision%20models%20and%20multimodal%20LLMs%20reveal%20a%0Astriking%20divide%3A%20humans%20achieved%20near-perfect%20accuracy%20across%20tasks%2C%20while%0Amodels%20totally%20failed%20on%20isomorphism%20detection%20and%20showed%20limited%20success%20in%0Apath/cycle%20tasks.%20We%20further%20identify%20behavioral%20anomalies%20suggesting%0Apseudo-intelligent%20pattern%20matching%20rather%20than%20genuine%20understanding.%20These%0Afindings%20underscore%20fundamental%20limitations%20in%20current%20AI%20models%20for%20visual%0Aunderstanding.%20By%20isolating%20the%20challenge%20of%20representation-invariant%0Areasoning%2C%20the%20VGA%20provides%20a%20framework%20to%20drive%20progress%20toward%20human-like%0Aconceptualization%20in%20AI%20visual%20models.%20The%20Visual%20Graph%20Arena%20is%20available%20at%3A%0A%5Chref%7Bhttps%3A//vga.csail.mit.edu/%7D%7Bvga.csail.mit.edu%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06242v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Graph%2520Arena%253A%2520Evaluating%2520Visual%2520Conceptualization%2520of%2520Vision%2520and%250A%2520%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DZahra%2520Babaiee%2520and%2520Peyman%2520M.%2520Kiasari%2520and%2520Daniela%2520Rus%2520and%2520Radu%2520Grosu%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520multimodal%2520large%2520language%2520models%2520have%2520driven%250Abreakthroughs%2520in%2520visual%2520question%2520answering.%2520Yet%252C%2520a%2520critical%2520gap%2520persists%252C%250A%2560conceptualization%2527-the%2520ability%2520to%2520recognize%2520and%2520reason%2520about%2520the%2520same%2520concept%250Adespite%2520variations%2520in%2520visual%2520form%252C%2520a%2520basic%2520ability%2520of%2520human%2520reasoning.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520introduce%2520the%2520Visual%2520Graph%2520Arena%2520%2528VGA%2529%252C%2520a%2520dataset%250Afeaturing%2520six%2520graph-based%2520tasks%2520designed%2520to%2520evaluate%2520and%2520improve%2520AI%2520systems%2527%250Acapacity%2520for%2520visual%2520abstraction.%2520VGA%2520uses%2520diverse%2520graph%2520layouts%2520%2528e.g.%252C%250AKamada-Kawai%2520vs.%2520planar%2529%2520to%2520test%2520reasoning%2520independent%2520of%2520visual%2520form.%250AExperiments%2520with%2520state-of-the-art%2520vision%2520models%2520and%2520multimodal%2520LLMs%2520reveal%2520a%250Astriking%2520divide%253A%2520humans%2520achieved%2520near-perfect%2520accuracy%2520across%2520tasks%252C%2520while%250Amodels%2520totally%2520failed%2520on%2520isomorphism%2520detection%2520and%2520showed%2520limited%2520success%2520in%250Apath/cycle%2520tasks.%2520We%2520further%2520identify%2520behavioral%2520anomalies%2520suggesting%250Apseudo-intelligent%2520pattern%2520matching%2520rather%2520than%2520genuine%2520understanding.%2520These%250Afindings%2520underscore%2520fundamental%2520limitations%2520in%2520current%2520AI%2520models%2520for%2520visual%250Aunderstanding.%2520By%2520isolating%2520the%2520challenge%2520of%2520representation-invariant%250Areasoning%252C%2520the%2520VGA%2520provides%2520a%2520framework%2520to%2520drive%2520progress%2520toward%2520human-like%250Aconceptualization%2520in%2520AI%2520visual%2520models.%2520The%2520Visual%2520Graph%2520Arena%2520is%2520available%2520at%253A%250A%255Chref%257Bhttps%253A//vga.csail.mit.edu/%257D%257Bvga.csail.mit.edu%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06242v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Graph%20Arena%3A%20Evaluating%20Visual%20Conceptualization%20of%20Vision%20and%0A%20%20Multimodal%20Large%20Language%20Models&entry.906535625=Zahra%20Babaiee%20and%20Peyman%20M.%20Kiasari%20and%20Daniela%20Rus%20and%20Radu%20Grosu&entry.1292438233=%20%20Recent%20advancements%20in%20multimodal%20large%20language%20models%20have%20driven%0Abreakthroughs%20in%20visual%20question%20answering.%20Yet%2C%20a%20critical%20gap%20persists%2C%0A%60conceptualization%27-the%20ability%20to%20recognize%20and%20reason%20about%20the%20same%20concept%0Adespite%20variations%20in%20visual%20form%2C%20a%20basic%20ability%20of%20human%20reasoning.%20To%0Aaddress%20this%20challenge%2C%20we%20introduce%20the%20Visual%20Graph%20Arena%20%28VGA%29%2C%20a%20dataset%0Afeaturing%20six%20graph-based%20tasks%20designed%20to%20evaluate%20and%20improve%20AI%20systems%27%0Acapacity%20for%20visual%20abstraction.%20VGA%20uses%20diverse%20graph%20layouts%20%28e.g.%2C%0AKamada-Kawai%20vs.%20planar%29%20to%20test%20reasoning%20independent%20of%20visual%20form.%0AExperiments%20with%20state-of-the-art%20vision%20models%20and%20multimodal%20LLMs%20reveal%20a%0Astriking%20divide%3A%20humans%20achieved%20near-perfect%20accuracy%20across%20tasks%2C%20while%0Amodels%20totally%20failed%20on%20isomorphism%20detection%20and%20showed%20limited%20success%20in%0Apath/cycle%20tasks.%20We%20further%20identify%20behavioral%20anomalies%20suggesting%0Apseudo-intelligent%20pattern%20matching%20rather%20than%20genuine%20understanding.%20These%0Afindings%20underscore%20fundamental%20limitations%20in%20current%20AI%20models%20for%20visual%0Aunderstanding.%20By%20isolating%20the%20challenge%20of%20representation-invariant%0Areasoning%2C%20the%20VGA%20provides%20a%20framework%20to%20drive%20progress%20toward%20human-like%0Aconceptualization%20in%20AI%20visual%20models.%20The%20Visual%20Graph%20Arena%20is%20available%20at%3A%0A%5Chref%7Bhttps%3A//vga.csail.mit.edu/%7D%7Bvga.csail.mit.edu%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06242v1&entry.124074799=Read"},
{"title": "Self-Supervised Generative-Contrastive Learning of Multi-Modal Euclidean\n  Input for 3D Shape Latent Representations: A Dynamic Switching Approach", "author": "Chengzhi Wu and Julius Pfrommer and Mingyuan Zhou and J\u00fcrgen Beyerer", "abstract": "  We propose a combined generative and contrastive neural architecture for\nlearning latent representations of 3D volumetric shapes. The architecture uses\ntwo encoder branches for voxel grids and multi-view images from the same\nunderlying shape. The main idea is to combine a contrastive loss between the\nresulting latent representations with an additional reconstruction loss. That\nhelps to avoid collapsing the latent representations as a trivial solution for\nminimizing the contrastive loss. A novel dynamic switching approach is used to\ncross-train two encoders with a shared decoder. The switching approach also\nenables the stop gradient operation on a random branch. Further classification\nexperiments show that the latent representations learned with our\nself-supervised method integrate more useful information from the additional\ninput data implicitly, thus leading to better reconstruction and classification\nperformance.\n", "link": "http://arxiv.org/abs/2301.04612v2", "date": "2025-06-06", "relevancy": 3.0464, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6415}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6033}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Generative-Contrastive%20Learning%20of%20Multi-Modal%20Euclidean%0A%20%20Input%20for%203D%20Shape%20Latent%20Representations%3A%20A%20Dynamic%20Switching%20Approach&body=Title%3A%20Self-Supervised%20Generative-Contrastive%20Learning%20of%20Multi-Modal%20Euclidean%0A%20%20Input%20for%203D%20Shape%20Latent%20Representations%3A%20A%20Dynamic%20Switching%20Approach%0AAuthor%3A%20Chengzhi%20Wu%20and%20Julius%20Pfrommer%20and%20Mingyuan%20Zhou%20and%20J%C3%BCrgen%20Beyerer%0AAbstract%3A%20%20%20We%20propose%20a%20combined%20generative%20and%20contrastive%20neural%20architecture%20for%0Alearning%20latent%20representations%20of%203D%20volumetric%20shapes.%20The%20architecture%20uses%0Atwo%20encoder%20branches%20for%20voxel%20grids%20and%20multi-view%20images%20from%20the%20same%0Aunderlying%20shape.%20The%20main%20idea%20is%20to%20combine%20a%20contrastive%20loss%20between%20the%0Aresulting%20latent%20representations%20with%20an%20additional%20reconstruction%20loss.%20That%0Ahelps%20to%20avoid%20collapsing%20the%20latent%20representations%20as%20a%20trivial%20solution%20for%0Aminimizing%20the%20contrastive%20loss.%20A%20novel%20dynamic%20switching%20approach%20is%20used%20to%0Across-train%20two%20encoders%20with%20a%20shared%20decoder.%20The%20switching%20approach%20also%0Aenables%20the%20stop%20gradient%20operation%20on%20a%20random%20branch.%20Further%20classification%0Aexperiments%20show%20that%20the%20latent%20representations%20learned%20with%20our%0Aself-supervised%20method%20integrate%20more%20useful%20information%20from%20the%20additional%0Ainput%20data%20implicitly%2C%20thus%20leading%20to%20better%20reconstruction%20and%20classification%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.04612v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Generative-Contrastive%2520Learning%2520of%2520Multi-Modal%2520Euclidean%250A%2520%2520Input%2520for%25203D%2520Shape%2520Latent%2520Representations%253A%2520A%2520Dynamic%2520Switching%2520Approach%26entry.906535625%3DChengzhi%2520Wu%2520and%2520Julius%2520Pfrommer%2520and%2520Mingyuan%2520Zhou%2520and%2520J%25C3%25BCrgen%2520Beyerer%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520combined%2520generative%2520and%2520contrastive%2520neural%2520architecture%2520for%250Alearning%2520latent%2520representations%2520of%25203D%2520volumetric%2520shapes.%2520The%2520architecture%2520uses%250Atwo%2520encoder%2520branches%2520for%2520voxel%2520grids%2520and%2520multi-view%2520images%2520from%2520the%2520same%250Aunderlying%2520shape.%2520The%2520main%2520idea%2520is%2520to%2520combine%2520a%2520contrastive%2520loss%2520between%2520the%250Aresulting%2520latent%2520representations%2520with%2520an%2520additional%2520reconstruction%2520loss.%2520That%250Ahelps%2520to%2520avoid%2520collapsing%2520the%2520latent%2520representations%2520as%2520a%2520trivial%2520solution%2520for%250Aminimizing%2520the%2520contrastive%2520loss.%2520A%2520novel%2520dynamic%2520switching%2520approach%2520is%2520used%2520to%250Across-train%2520two%2520encoders%2520with%2520a%2520shared%2520decoder.%2520The%2520switching%2520approach%2520also%250Aenables%2520the%2520stop%2520gradient%2520operation%2520on%2520a%2520random%2520branch.%2520Further%2520classification%250Aexperiments%2520show%2520that%2520the%2520latent%2520representations%2520learned%2520with%2520our%250Aself-supervised%2520method%2520integrate%2520more%2520useful%2520information%2520from%2520the%2520additional%250Ainput%2520data%2520implicitly%252C%2520thus%2520leading%2520to%2520better%2520reconstruction%2520and%2520classification%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.04612v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Generative-Contrastive%20Learning%20of%20Multi-Modal%20Euclidean%0A%20%20Input%20for%203D%20Shape%20Latent%20Representations%3A%20A%20Dynamic%20Switching%20Approach&entry.906535625=Chengzhi%20Wu%20and%20Julius%20Pfrommer%20and%20Mingyuan%20Zhou%20and%20J%C3%BCrgen%20Beyerer&entry.1292438233=%20%20We%20propose%20a%20combined%20generative%20and%20contrastive%20neural%20architecture%20for%0Alearning%20latent%20representations%20of%203D%20volumetric%20shapes.%20The%20architecture%20uses%0Atwo%20encoder%20branches%20for%20voxel%20grids%20and%20multi-view%20images%20from%20the%20same%0Aunderlying%20shape.%20The%20main%20idea%20is%20to%20combine%20a%20contrastive%20loss%20between%20the%0Aresulting%20latent%20representations%20with%20an%20additional%20reconstruction%20loss.%20That%0Ahelps%20to%20avoid%20collapsing%20the%20latent%20representations%20as%20a%20trivial%20solution%20for%0Aminimizing%20the%20contrastive%20loss.%20A%20novel%20dynamic%20switching%20approach%20is%20used%20to%0Across-train%20two%20encoders%20with%20a%20shared%20decoder.%20The%20switching%20approach%20also%0Aenables%20the%20stop%20gradient%20operation%20on%20a%20random%20branch.%20Further%20classification%0Aexperiments%20show%20that%20the%20latent%20representations%20learned%20with%20our%0Aself-supervised%20method%20integrate%20more%20useful%20information%20from%20the%20additional%0Ainput%20data%20implicitly%2C%20thus%20leading%20to%20better%20reconstruction%20and%20classification%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.04612v2&entry.124074799=Read"},
{"title": "Leopard: A Vision Language Model For Text-Rich Multi-Image Tasks", "author": "Mengzhao Jia and Wenhao Yu and Kaixin Ma and Tianqing Fang and Zhihan Zhang and Siru Ouyang and Hongming Zhang and Dong Yu and Meng Jiang", "abstract": "  Text-rich images, where text serves as the central visual element guiding the\noverall understanding, are prevalent in real-world applications, such as\npresentation slides, scanned documents, and webpage snapshots. Tasks involving\nmultiple text-rich images are especially challenging, as they require not only\nunderstanding the content of individual images but reasoning about\ninter-relationships and logical flows across multiple visual inputs. Despite\nthe importance of these scenarios, current multimodal large language models\n(MLLMs) struggle to handle such tasks due to two key challenges: (1) the\nscarcity of high-quality instruction tuning datasets for text-rich multi-image\nscenarios, and (2) the difficulty in balancing image resolution with visual\nfeature sequence length. To address these challenges, we propose Leopard, an\nMLLM tailored for handling vision-language tasks involving multiple text-rich\nimages. First, we curated about one million high-quality multimodal\ninstruction-tuning data, tailored to text-rich, multi-image scenarios. Second,\nwe proposed an adaptive high-resolution multi-image encoding module to\ndynamically optimize the allocation of visual sequence length based on the\noriginal aspect ratios and resolutions of images. Experiments on a diverse set\nof benchmarks reveal that our model consistently outperforms state-of-the-art\nsystems, such as Llama-3.2 and Qwen2-VL, in challenging text-rich, multi-image\nevaluations. Remarkably, our approach achieves outstanding performance using\nonly 1.2M training instances, all of which are fully open-sourced,\ndemonstrating both high efficiency and effectiveness compared to models trained\non large-scale in-house data. Our code and data are available at\nhttps://github.com/tencent-ailab/Leopard.\n", "link": "http://arxiv.org/abs/2410.01744v3", "date": "2025-06-06", "relevancy": 3.0084, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6104}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5973}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leopard%3A%20A%20Vision%20Language%20Model%20For%20Text-Rich%20Multi-Image%20Tasks&body=Title%3A%20Leopard%3A%20A%20Vision%20Language%20Model%20For%20Text-Rich%20Multi-Image%20Tasks%0AAuthor%3A%20Mengzhao%20Jia%20and%20Wenhao%20Yu%20and%20Kaixin%20Ma%20and%20Tianqing%20Fang%20and%20Zhihan%20Zhang%20and%20Siru%20Ouyang%20and%20Hongming%20Zhang%20and%20Dong%20Yu%20and%20Meng%20Jiang%0AAbstract%3A%20%20%20Text-rich%20images%2C%20where%20text%20serves%20as%20the%20central%20visual%20element%20guiding%20the%0Aoverall%20understanding%2C%20are%20prevalent%20in%20real-world%20applications%2C%20such%20as%0Apresentation%20slides%2C%20scanned%20documents%2C%20and%20webpage%20snapshots.%20Tasks%20involving%0Amultiple%20text-rich%20images%20are%20especially%20challenging%2C%20as%20they%20require%20not%20only%0Aunderstanding%20the%20content%20of%20individual%20images%20but%20reasoning%20about%0Ainter-relationships%20and%20logical%20flows%20across%20multiple%20visual%20inputs.%20Despite%0Athe%20importance%20of%20these%20scenarios%2C%20current%20multimodal%20large%20language%20models%0A%28MLLMs%29%20struggle%20to%20handle%20such%20tasks%20due%20to%20two%20key%20challenges%3A%20%281%29%20the%0Ascarcity%20of%20high-quality%20instruction%20tuning%20datasets%20for%20text-rich%20multi-image%0Ascenarios%2C%20and%20%282%29%20the%20difficulty%20in%20balancing%20image%20resolution%20with%20visual%0Afeature%20sequence%20length.%20To%20address%20these%20challenges%2C%20we%20propose%20Leopard%2C%20an%0AMLLM%20tailored%20for%20handling%20vision-language%20tasks%20involving%20multiple%20text-rich%0Aimages.%20First%2C%20we%20curated%20about%20one%20million%20high-quality%20multimodal%0Ainstruction-tuning%20data%2C%20tailored%20to%20text-rich%2C%20multi-image%20scenarios.%20Second%2C%0Awe%20proposed%20an%20adaptive%20high-resolution%20multi-image%20encoding%20module%20to%0Adynamically%20optimize%20the%20allocation%20of%20visual%20sequence%20length%20based%20on%20the%0Aoriginal%20aspect%20ratios%20and%20resolutions%20of%20images.%20Experiments%20on%20a%20diverse%20set%0Aof%20benchmarks%20reveal%20that%20our%20model%20consistently%20outperforms%20state-of-the-art%0Asystems%2C%20such%20as%20Llama-3.2%20and%20Qwen2-VL%2C%20in%20challenging%20text-rich%2C%20multi-image%0Aevaluations.%20Remarkably%2C%20our%20approach%20achieves%20outstanding%20performance%20using%0Aonly%201.2M%20training%20instances%2C%20all%20of%20which%20are%20fully%20open-sourced%2C%0Ademonstrating%20both%20high%20efficiency%20and%20effectiveness%20compared%20to%20models%20trained%0Aon%20large-scale%20in-house%20data.%20Our%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/tencent-ailab/Leopard.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01744v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeopard%253A%2520A%2520Vision%2520Language%2520Model%2520For%2520Text-Rich%2520Multi-Image%2520Tasks%26entry.906535625%3DMengzhao%2520Jia%2520and%2520Wenhao%2520Yu%2520and%2520Kaixin%2520Ma%2520and%2520Tianqing%2520Fang%2520and%2520Zhihan%2520Zhang%2520and%2520Siru%2520Ouyang%2520and%2520Hongming%2520Zhang%2520and%2520Dong%2520Yu%2520and%2520Meng%2520Jiang%26entry.1292438233%3D%2520%2520Text-rich%2520images%252C%2520where%2520text%2520serves%2520as%2520the%2520central%2520visual%2520element%2520guiding%2520the%250Aoverall%2520understanding%252C%2520are%2520prevalent%2520in%2520real-world%2520applications%252C%2520such%2520as%250Apresentation%2520slides%252C%2520scanned%2520documents%252C%2520and%2520webpage%2520snapshots.%2520Tasks%2520involving%250Amultiple%2520text-rich%2520images%2520are%2520especially%2520challenging%252C%2520as%2520they%2520require%2520not%2520only%250Aunderstanding%2520the%2520content%2520of%2520individual%2520images%2520but%2520reasoning%2520about%250Ainter-relationships%2520and%2520logical%2520flows%2520across%2520multiple%2520visual%2520inputs.%2520Despite%250Athe%2520importance%2520of%2520these%2520scenarios%252C%2520current%2520multimodal%2520large%2520language%2520models%250A%2528MLLMs%2529%2520struggle%2520to%2520handle%2520such%2520tasks%2520due%2520to%2520two%2520key%2520challenges%253A%2520%25281%2529%2520the%250Ascarcity%2520of%2520high-quality%2520instruction%2520tuning%2520datasets%2520for%2520text-rich%2520multi-image%250Ascenarios%252C%2520and%2520%25282%2529%2520the%2520difficulty%2520in%2520balancing%2520image%2520resolution%2520with%2520visual%250Afeature%2520sequence%2520length.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520Leopard%252C%2520an%250AMLLM%2520tailored%2520for%2520handling%2520vision-language%2520tasks%2520involving%2520multiple%2520text-rich%250Aimages.%2520First%252C%2520we%2520curated%2520about%2520one%2520million%2520high-quality%2520multimodal%250Ainstruction-tuning%2520data%252C%2520tailored%2520to%2520text-rich%252C%2520multi-image%2520scenarios.%2520Second%252C%250Awe%2520proposed%2520an%2520adaptive%2520high-resolution%2520multi-image%2520encoding%2520module%2520to%250Adynamically%2520optimize%2520the%2520allocation%2520of%2520visual%2520sequence%2520length%2520based%2520on%2520the%250Aoriginal%2520aspect%2520ratios%2520and%2520resolutions%2520of%2520images.%2520Experiments%2520on%2520a%2520diverse%2520set%250Aof%2520benchmarks%2520reveal%2520that%2520our%2520model%2520consistently%2520outperforms%2520state-of-the-art%250Asystems%252C%2520such%2520as%2520Llama-3.2%2520and%2520Qwen2-VL%252C%2520in%2520challenging%2520text-rich%252C%2520multi-image%250Aevaluations.%2520Remarkably%252C%2520our%2520approach%2520achieves%2520outstanding%2520performance%2520using%250Aonly%25201.2M%2520training%2520instances%252C%2520all%2520of%2520which%2520are%2520fully%2520open-sourced%252C%250Ademonstrating%2520both%2520high%2520efficiency%2520and%2520effectiveness%2520compared%2520to%2520models%2520trained%250Aon%2520large-scale%2520in-house%2520data.%2520Our%2520code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/tencent-ailab/Leopard.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01744v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leopard%3A%20A%20Vision%20Language%20Model%20For%20Text-Rich%20Multi-Image%20Tasks&entry.906535625=Mengzhao%20Jia%20and%20Wenhao%20Yu%20and%20Kaixin%20Ma%20and%20Tianqing%20Fang%20and%20Zhihan%20Zhang%20and%20Siru%20Ouyang%20and%20Hongming%20Zhang%20and%20Dong%20Yu%20and%20Meng%20Jiang&entry.1292438233=%20%20Text-rich%20images%2C%20where%20text%20serves%20as%20the%20central%20visual%20element%20guiding%20the%0Aoverall%20understanding%2C%20are%20prevalent%20in%20real-world%20applications%2C%20such%20as%0Apresentation%20slides%2C%20scanned%20documents%2C%20and%20webpage%20snapshots.%20Tasks%20involving%0Amultiple%20text-rich%20images%20are%20especially%20challenging%2C%20as%20they%20require%20not%20only%0Aunderstanding%20the%20content%20of%20individual%20images%20but%20reasoning%20about%0Ainter-relationships%20and%20logical%20flows%20across%20multiple%20visual%20inputs.%20Despite%0Athe%20importance%20of%20these%20scenarios%2C%20current%20multimodal%20large%20language%20models%0A%28MLLMs%29%20struggle%20to%20handle%20such%20tasks%20due%20to%20two%20key%20challenges%3A%20%281%29%20the%0Ascarcity%20of%20high-quality%20instruction%20tuning%20datasets%20for%20text-rich%20multi-image%0Ascenarios%2C%20and%20%282%29%20the%20difficulty%20in%20balancing%20image%20resolution%20with%20visual%0Afeature%20sequence%20length.%20To%20address%20these%20challenges%2C%20we%20propose%20Leopard%2C%20an%0AMLLM%20tailored%20for%20handling%20vision-language%20tasks%20involving%20multiple%20text-rich%0Aimages.%20First%2C%20we%20curated%20about%20one%20million%20high-quality%20multimodal%0Ainstruction-tuning%20data%2C%20tailored%20to%20text-rich%2C%20multi-image%20scenarios.%20Second%2C%0Awe%20proposed%20an%20adaptive%20high-resolution%20multi-image%20encoding%20module%20to%0Adynamically%20optimize%20the%20allocation%20of%20visual%20sequence%20length%20based%20on%20the%0Aoriginal%20aspect%20ratios%20and%20resolutions%20of%20images.%20Experiments%20on%20a%20diverse%20set%0Aof%20benchmarks%20reveal%20that%20our%20model%20consistently%20outperforms%20state-of-the-art%0Asystems%2C%20such%20as%20Llama-3.2%20and%20Qwen2-VL%2C%20in%20challenging%20text-rich%2C%20multi-image%0Aevaluations.%20Remarkably%2C%20our%20approach%20achieves%20outstanding%20performance%20using%0Aonly%201.2M%20training%20instances%2C%20all%20of%20which%20are%20fully%20open-sourced%2C%0Ademonstrating%20both%20high%20efficiency%20and%20effectiveness%20compared%20to%20models%20trained%0Aon%20large-scale%20in-house%20data.%20Our%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/tencent-ailab/Leopard.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01744v3&entry.124074799=Read"},
{"title": "Restereo: Diffusion stereo video generation and restoration", "author": "Xingchang Huang and Ashish Kumar Singh and Florian Dubost and Cristina Nader Vasconcelos and Sakar Khattar and Liang Shi and Christian Theobalt and Cengiz Oztireli and Gurprit Singh", "abstract": "  Stereo video generation has been gaining increasing attention with recent\nadvancements in video diffusion models. However, most existing methods focus on\ngenerating 3D stereoscopic videos from monocular 2D videos. These approaches\ntypically assume that the input monocular video is of high quality, making the\ntask primarily about inpainting occluded regions in the warped video while\npreserving disoccluded areas. In this paper, we introduce a new pipeline that\nnot only generates stereo videos but also enhances both left-view and\nright-view videos consistently with a single model. Our approach achieves this\nby fine-tuning the model on degraded data for restoration, as well as\nconditioning the model on warped masks for consistent stereo generation. As a\nresult, our method can be fine-tuned on a relatively small synthetic stereo\nvideo datasets and applied to low-quality real-world videos, performing both\nstereo video generation and restoration. Experiments demonstrate that our\nmethod outperforms existing approaches both qualitatively and quantitatively in\nstereo video generation from low-resolution inputs.\n", "link": "http://arxiv.org/abs/2506.06023v1", "date": "2025-06-06", "relevancy": 2.9621, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6387}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5693}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Restereo%3A%20Diffusion%20stereo%20video%20generation%20and%20restoration&body=Title%3A%20Restereo%3A%20Diffusion%20stereo%20video%20generation%20and%20restoration%0AAuthor%3A%20Xingchang%20Huang%20and%20Ashish%20Kumar%20Singh%20and%20Florian%20Dubost%20and%20Cristina%20Nader%20Vasconcelos%20and%20Sakar%20Khattar%20and%20Liang%20Shi%20and%20Christian%20Theobalt%20and%20Cengiz%20Oztireli%20and%20Gurprit%20Singh%0AAbstract%3A%20%20%20Stereo%20video%20generation%20has%20been%20gaining%20increasing%20attention%20with%20recent%0Aadvancements%20in%20video%20diffusion%20models.%20However%2C%20most%20existing%20methods%20focus%20on%0Agenerating%203D%20stereoscopic%20videos%20from%20monocular%202D%20videos.%20These%20approaches%0Atypically%20assume%20that%20the%20input%20monocular%20video%20is%20of%20high%20quality%2C%20making%20the%0Atask%20primarily%20about%20inpainting%20occluded%20regions%20in%20the%20warped%20video%20while%0Apreserving%20disoccluded%20areas.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20pipeline%20that%0Anot%20only%20generates%20stereo%20videos%20but%20also%20enhances%20both%20left-view%20and%0Aright-view%20videos%20consistently%20with%20a%20single%20model.%20Our%20approach%20achieves%20this%0Aby%20fine-tuning%20the%20model%20on%20degraded%20data%20for%20restoration%2C%20as%20well%20as%0Aconditioning%20the%20model%20on%20warped%20masks%20for%20consistent%20stereo%20generation.%20As%20a%0Aresult%2C%20our%20method%20can%20be%20fine-tuned%20on%20a%20relatively%20small%20synthetic%20stereo%0Avideo%20datasets%20and%20applied%20to%20low-quality%20real-world%20videos%2C%20performing%20both%0Astereo%20video%20generation%20and%20restoration.%20Experiments%20demonstrate%20that%20our%0Amethod%20outperforms%20existing%20approaches%20both%20qualitatively%20and%20quantitatively%20in%0Astereo%20video%20generation%20from%20low-resolution%20inputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06023v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRestereo%253A%2520Diffusion%2520stereo%2520video%2520generation%2520and%2520restoration%26entry.906535625%3DXingchang%2520Huang%2520and%2520Ashish%2520Kumar%2520Singh%2520and%2520Florian%2520Dubost%2520and%2520Cristina%2520Nader%2520Vasconcelos%2520and%2520Sakar%2520Khattar%2520and%2520Liang%2520Shi%2520and%2520Christian%2520Theobalt%2520and%2520Cengiz%2520Oztireli%2520and%2520Gurprit%2520Singh%26entry.1292438233%3D%2520%2520Stereo%2520video%2520generation%2520has%2520been%2520gaining%2520increasing%2520attention%2520with%2520recent%250Aadvancements%2520in%2520video%2520diffusion%2520models.%2520However%252C%2520most%2520existing%2520methods%2520focus%2520on%250Agenerating%25203D%2520stereoscopic%2520videos%2520from%2520monocular%25202D%2520videos.%2520These%2520approaches%250Atypically%2520assume%2520that%2520the%2520input%2520monocular%2520video%2520is%2520of%2520high%2520quality%252C%2520making%2520the%250Atask%2520primarily%2520about%2520inpainting%2520occluded%2520regions%2520in%2520the%2520warped%2520video%2520while%250Apreserving%2520disoccluded%2520areas.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520new%2520pipeline%2520that%250Anot%2520only%2520generates%2520stereo%2520videos%2520but%2520also%2520enhances%2520both%2520left-view%2520and%250Aright-view%2520videos%2520consistently%2520with%2520a%2520single%2520model.%2520Our%2520approach%2520achieves%2520this%250Aby%2520fine-tuning%2520the%2520model%2520on%2520degraded%2520data%2520for%2520restoration%252C%2520as%2520well%2520as%250Aconditioning%2520the%2520model%2520on%2520warped%2520masks%2520for%2520consistent%2520stereo%2520generation.%2520As%2520a%250Aresult%252C%2520our%2520method%2520can%2520be%2520fine-tuned%2520on%2520a%2520relatively%2520small%2520synthetic%2520stereo%250Avideo%2520datasets%2520and%2520applied%2520to%2520low-quality%2520real-world%2520videos%252C%2520performing%2520both%250Astereo%2520video%2520generation%2520and%2520restoration.%2520Experiments%2520demonstrate%2520that%2520our%250Amethod%2520outperforms%2520existing%2520approaches%2520both%2520qualitatively%2520and%2520quantitatively%2520in%250Astereo%2520video%2520generation%2520from%2520low-resolution%2520inputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06023v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Restereo%3A%20Diffusion%20stereo%20video%20generation%20and%20restoration&entry.906535625=Xingchang%20Huang%20and%20Ashish%20Kumar%20Singh%20and%20Florian%20Dubost%20and%20Cristina%20Nader%20Vasconcelos%20and%20Sakar%20Khattar%20and%20Liang%20Shi%20and%20Christian%20Theobalt%20and%20Cengiz%20Oztireli%20and%20Gurprit%20Singh&entry.1292438233=%20%20Stereo%20video%20generation%20has%20been%20gaining%20increasing%20attention%20with%20recent%0Aadvancements%20in%20video%20diffusion%20models.%20However%2C%20most%20existing%20methods%20focus%20on%0Agenerating%203D%20stereoscopic%20videos%20from%20monocular%202D%20videos.%20These%20approaches%0Atypically%20assume%20that%20the%20input%20monocular%20video%20is%20of%20high%20quality%2C%20making%20the%0Atask%20primarily%20about%20inpainting%20occluded%20regions%20in%20the%20warped%20video%20while%0Apreserving%20disoccluded%20areas.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20pipeline%20that%0Anot%20only%20generates%20stereo%20videos%20but%20also%20enhances%20both%20left-view%20and%0Aright-view%20videos%20consistently%20with%20a%20single%20model.%20Our%20approach%20achieves%20this%0Aby%20fine-tuning%20the%20model%20on%20degraded%20data%20for%20restoration%2C%20as%20well%20as%0Aconditioning%20the%20model%20on%20warped%20masks%20for%20consistent%20stereo%20generation.%20As%20a%0Aresult%2C%20our%20method%20can%20be%20fine-tuned%20on%20a%20relatively%20small%20synthetic%20stereo%0Avideo%20datasets%20and%20applied%20to%20low-quality%20real-world%20videos%2C%20performing%20both%0Astereo%20video%20generation%20and%20restoration.%20Experiments%20demonstrate%20that%20our%0Amethod%20outperforms%20existing%20approaches%20both%20qualitatively%20and%20quantitatively%20in%0Astereo%20video%20generation%20from%20low-resolution%20inputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06023v1&entry.124074799=Read"},
{"title": "CoMemo: LVLMs Need Image Context with Image Memory", "author": "Shi Liu and Weijie Su and Xizhou Zhu and Wenhai Wang and Jifeng Dai", "abstract": "  Recent advancements in Large Vision-Language Models built upon Large Language\nModels have established aligning visual features with LLM representations as\nthe dominant paradigm. However, inherited LLM architectural designs introduce\nsuboptimal characteristics for multimodal processing. First, LVLMs exhibit a\nbimodal distribution in attention allocation, leading to the progressive\nneglect of middle visual content as context expands. Second, conventional\npositional encoding schemes fail to preserve vital 2D structural relationships\nwhen processing dynamic high-resolution images. To address these limitations,\nwe propose CoMemo - a dual-path architecture that combines a Context image path\nwith an image Memory path for visual processing, effectively alleviating visual\ninformation neglect. Additionally, we introduce RoPE-DHR, a novel positional\nencoding mechanism that employs thumbnail-based positional aggregation to\nmaintain 2D spatial awareness while mitigating remote decay in extended\nsequences. Evaluations across seven benchmarks,including long-context\ncomprehension, multi-image reasoning, and visual question answering,\ndemonstrate CoMemo's superior performance compared to conventional LVLM\narchitectures. Project page is available at\nhttps://lalbj.github.io/projects/CoMemo/.\n", "link": "http://arxiv.org/abs/2506.06279v1", "date": "2025-06-06", "relevancy": 2.9578, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6004}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6004}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoMemo%3A%20LVLMs%20Need%20Image%20Context%20with%20Image%20Memory&body=Title%3A%20CoMemo%3A%20LVLMs%20Need%20Image%20Context%20with%20Image%20Memory%0AAuthor%3A%20Shi%20Liu%20and%20Weijie%20Su%20and%20Xizhou%20Zhu%20and%20Wenhai%20Wang%20and%20Jifeng%20Dai%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Large%20Vision-Language%20Models%20built%20upon%20Large%20Language%0AModels%20have%20established%20aligning%20visual%20features%20with%20LLM%20representations%20as%0Athe%20dominant%20paradigm.%20However%2C%20inherited%20LLM%20architectural%20designs%20introduce%0Asuboptimal%20characteristics%20for%20multimodal%20processing.%20First%2C%20LVLMs%20exhibit%20a%0Abimodal%20distribution%20in%20attention%20allocation%2C%20leading%20to%20the%20progressive%0Aneglect%20of%20middle%20visual%20content%20as%20context%20expands.%20Second%2C%20conventional%0Apositional%20encoding%20schemes%20fail%20to%20preserve%20vital%202D%20structural%20relationships%0Awhen%20processing%20dynamic%20high-resolution%20images.%20To%20address%20these%20limitations%2C%0Awe%20propose%20CoMemo%20-%20a%20dual-path%20architecture%20that%20combines%20a%20Context%20image%20path%0Awith%20an%20image%20Memory%20path%20for%20visual%20processing%2C%20effectively%20alleviating%20visual%0Ainformation%20neglect.%20Additionally%2C%20we%20introduce%20RoPE-DHR%2C%20a%20novel%20positional%0Aencoding%20mechanism%20that%20employs%20thumbnail-based%20positional%20aggregation%20to%0Amaintain%202D%20spatial%20awareness%20while%20mitigating%20remote%20decay%20in%20extended%0Asequences.%20Evaluations%20across%20seven%20benchmarks%2Cincluding%20long-context%0Acomprehension%2C%20multi-image%20reasoning%2C%20and%20visual%20question%20answering%2C%0Ademonstrate%20CoMemo%27s%20superior%20performance%20compared%20to%20conventional%20LVLM%0Aarchitectures.%20Project%20page%20is%20available%20at%0Ahttps%3A//lalbj.github.io/projects/CoMemo/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06279v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoMemo%253A%2520LVLMs%2520Need%2520Image%2520Context%2520with%2520Image%2520Memory%26entry.906535625%3DShi%2520Liu%2520and%2520Weijie%2520Su%2520and%2520Xizhou%2520Zhu%2520and%2520Wenhai%2520Wang%2520and%2520Jifeng%2520Dai%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Large%2520Vision-Language%2520Models%2520built%2520upon%2520Large%2520Language%250AModels%2520have%2520established%2520aligning%2520visual%2520features%2520with%2520LLM%2520representations%2520as%250Athe%2520dominant%2520paradigm.%2520However%252C%2520inherited%2520LLM%2520architectural%2520designs%2520introduce%250Asuboptimal%2520characteristics%2520for%2520multimodal%2520processing.%2520First%252C%2520LVLMs%2520exhibit%2520a%250Abimodal%2520distribution%2520in%2520attention%2520allocation%252C%2520leading%2520to%2520the%2520progressive%250Aneglect%2520of%2520middle%2520visual%2520content%2520as%2520context%2520expands.%2520Second%252C%2520conventional%250Apositional%2520encoding%2520schemes%2520fail%2520to%2520preserve%2520vital%25202D%2520structural%2520relationships%250Awhen%2520processing%2520dynamic%2520high-resolution%2520images.%2520To%2520address%2520these%2520limitations%252C%250Awe%2520propose%2520CoMemo%2520-%2520a%2520dual-path%2520architecture%2520that%2520combines%2520a%2520Context%2520image%2520path%250Awith%2520an%2520image%2520Memory%2520path%2520for%2520visual%2520processing%252C%2520effectively%2520alleviating%2520visual%250Ainformation%2520neglect.%2520Additionally%252C%2520we%2520introduce%2520RoPE-DHR%252C%2520a%2520novel%2520positional%250Aencoding%2520mechanism%2520that%2520employs%2520thumbnail-based%2520positional%2520aggregation%2520to%250Amaintain%25202D%2520spatial%2520awareness%2520while%2520mitigating%2520remote%2520decay%2520in%2520extended%250Asequences.%2520Evaluations%2520across%2520seven%2520benchmarks%252Cincluding%2520long-context%250Acomprehension%252C%2520multi-image%2520reasoning%252C%2520and%2520visual%2520question%2520answering%252C%250Ademonstrate%2520CoMemo%2527s%2520superior%2520performance%2520compared%2520to%2520conventional%2520LVLM%250Aarchitectures.%2520Project%2520page%2520is%2520available%2520at%250Ahttps%253A//lalbj.github.io/projects/CoMemo/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06279v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoMemo%3A%20LVLMs%20Need%20Image%20Context%20with%20Image%20Memory&entry.906535625=Shi%20Liu%20and%20Weijie%20Su%20and%20Xizhou%20Zhu%20and%20Wenhai%20Wang%20and%20Jifeng%20Dai&entry.1292438233=%20%20Recent%20advancements%20in%20Large%20Vision-Language%20Models%20built%20upon%20Large%20Language%0AModels%20have%20established%20aligning%20visual%20features%20with%20LLM%20representations%20as%0Athe%20dominant%20paradigm.%20However%2C%20inherited%20LLM%20architectural%20designs%20introduce%0Asuboptimal%20characteristics%20for%20multimodal%20processing.%20First%2C%20LVLMs%20exhibit%20a%0Abimodal%20distribution%20in%20attention%20allocation%2C%20leading%20to%20the%20progressive%0Aneglect%20of%20middle%20visual%20content%20as%20context%20expands.%20Second%2C%20conventional%0Apositional%20encoding%20schemes%20fail%20to%20preserve%20vital%202D%20structural%20relationships%0Awhen%20processing%20dynamic%20high-resolution%20images.%20To%20address%20these%20limitations%2C%0Awe%20propose%20CoMemo%20-%20a%20dual-path%20architecture%20that%20combines%20a%20Context%20image%20path%0Awith%20an%20image%20Memory%20path%20for%20visual%20processing%2C%20effectively%20alleviating%20visual%0Ainformation%20neglect.%20Additionally%2C%20we%20introduce%20RoPE-DHR%2C%20a%20novel%20positional%0Aencoding%20mechanism%20that%20employs%20thumbnail-based%20positional%20aggregation%20to%0Amaintain%202D%20spatial%20awareness%20while%20mitigating%20remote%20decay%20in%20extended%0Asequences.%20Evaluations%20across%20seven%20benchmarks%2Cincluding%20long-context%0Acomprehension%2C%20multi-image%20reasoning%2C%20and%20visual%20question%20answering%2C%0Ademonstrate%20CoMemo%27s%20superior%20performance%20compared%20to%20conventional%20LVLM%0Aarchitectures.%20Project%20page%20is%20available%20at%0Ahttps%3A//lalbj.github.io/projects/CoMemo/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06279v1&entry.124074799=Read"},
{"title": "Full Conformal Adaptation of Medical Vision-Language Models", "author": "Julio Silva-Rodr\u00edguez and Leo Fillioux and Paul-Henry Courn\u00e8de and Maria Vakalopoulou and Stergios Christodoulidis and Ismail Ben Ayed and Jose Dolz", "abstract": "  Vision-language models (VLMs) pre-trained at large scale have shown\nunprecedented transferability capabilities and are being progressively\nintegrated into medical image analysis. Although its discriminative potential\nhas been widely explored, its reliability aspect remains overlooked. This work\ninvestigates their behavior under the increasingly popular split conformal\nprediction (SCP) framework, which theoretically guarantees a given error level\non output sets by leveraging a labeled calibration set. However, the zero-shot\nperformance of VLMs is inherently limited, and common practice involves\nfew-shot transfer learning pipelines, which cannot absorb the rigid\nexchangeability assumptions of SCP. To alleviate this issue, we propose full\nconformal adaptation, a novel setting for jointly adapting and conformalizing\npre-trained foundation models, which operates transductively over each test\ndata point using a few-shot adaptation set. Moreover, we complement this\nframework with SS-Text, a novel training-free linear probe solver for VLMs that\nalleviates the computational cost of such a transductive approach. We provide\ncomprehensive experiments using 3 different modality-specialized medical VLMs\nand 9 adaptation tasks. Our framework requires exactly the same data as SCP,\nand provides consistent relative improvements of up to 27% on set efficiency\nwhile maintaining the same coverage guarantees.\n", "link": "http://arxiv.org/abs/2506.06076v1", "date": "2025-06-06", "relevancy": 2.9556, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6123}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5805}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Full%20Conformal%20Adaptation%20of%20Medical%20Vision-Language%20Models&body=Title%3A%20Full%20Conformal%20Adaptation%20of%20Medical%20Vision-Language%20Models%0AAuthor%3A%20Julio%20Silva-Rodr%C3%ADguez%20and%20Leo%20Fillioux%20and%20Paul-Henry%20Courn%C3%A8de%20and%20Maria%20Vakalopoulou%20and%20Stergios%20Christodoulidis%20and%20Ismail%20Ben%20Ayed%20and%20Jose%20Dolz%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20pre-trained%20at%20large%20scale%20have%20shown%0Aunprecedented%20transferability%20capabilities%20and%20are%20being%20progressively%0Aintegrated%20into%20medical%20image%20analysis.%20Although%20its%20discriminative%20potential%0Ahas%20been%20widely%20explored%2C%20its%20reliability%20aspect%20remains%20overlooked.%20This%20work%0Ainvestigates%20their%20behavior%20under%20the%20increasingly%20popular%20split%20conformal%0Aprediction%20%28SCP%29%20framework%2C%20which%20theoretically%20guarantees%20a%20given%20error%20level%0Aon%20output%20sets%20by%20leveraging%20a%20labeled%20calibration%20set.%20However%2C%20the%20zero-shot%0Aperformance%20of%20VLMs%20is%20inherently%20limited%2C%20and%20common%20practice%20involves%0Afew-shot%20transfer%20learning%20pipelines%2C%20which%20cannot%20absorb%20the%20rigid%0Aexchangeability%20assumptions%20of%20SCP.%20To%20alleviate%20this%20issue%2C%20we%20propose%20full%0Aconformal%20adaptation%2C%20a%20novel%20setting%20for%20jointly%20adapting%20and%20conformalizing%0Apre-trained%20foundation%20models%2C%20which%20operates%20transductively%20over%20each%20test%0Adata%20point%20using%20a%20few-shot%20adaptation%20set.%20Moreover%2C%20we%20complement%20this%0Aframework%20with%20SS-Text%2C%20a%20novel%20training-free%20linear%20probe%20solver%20for%20VLMs%20that%0Aalleviates%20the%20computational%20cost%20of%20such%20a%20transductive%20approach.%20We%20provide%0Acomprehensive%20experiments%20using%203%20different%20modality-specialized%20medical%20VLMs%0Aand%209%20adaptation%20tasks.%20Our%20framework%20requires%20exactly%20the%20same%20data%20as%20SCP%2C%0Aand%20provides%20consistent%20relative%20improvements%20of%20up%20to%2027%25%20on%20set%20efficiency%0Awhile%20maintaining%20the%20same%20coverage%20guarantees.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06076v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFull%2520Conformal%2520Adaptation%2520of%2520Medical%2520Vision-Language%2520Models%26entry.906535625%3DJulio%2520Silva-Rodr%25C3%25ADguez%2520and%2520Leo%2520Fillioux%2520and%2520Paul-Henry%2520Courn%25C3%25A8de%2520and%2520Maria%2520Vakalopoulou%2520and%2520Stergios%2520Christodoulidis%2520and%2520Ismail%2520Ben%2520Ayed%2520and%2520Jose%2520Dolz%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520pre-trained%2520at%2520large%2520scale%2520have%2520shown%250Aunprecedented%2520transferability%2520capabilities%2520and%2520are%2520being%2520progressively%250Aintegrated%2520into%2520medical%2520image%2520analysis.%2520Although%2520its%2520discriminative%2520potential%250Ahas%2520been%2520widely%2520explored%252C%2520its%2520reliability%2520aspect%2520remains%2520overlooked.%2520This%2520work%250Ainvestigates%2520their%2520behavior%2520under%2520the%2520increasingly%2520popular%2520split%2520conformal%250Aprediction%2520%2528SCP%2529%2520framework%252C%2520which%2520theoretically%2520guarantees%2520a%2520given%2520error%2520level%250Aon%2520output%2520sets%2520by%2520leveraging%2520a%2520labeled%2520calibration%2520set.%2520However%252C%2520the%2520zero-shot%250Aperformance%2520of%2520VLMs%2520is%2520inherently%2520limited%252C%2520and%2520common%2520practice%2520involves%250Afew-shot%2520transfer%2520learning%2520pipelines%252C%2520which%2520cannot%2520absorb%2520the%2520rigid%250Aexchangeability%2520assumptions%2520of%2520SCP.%2520To%2520alleviate%2520this%2520issue%252C%2520we%2520propose%2520full%250Aconformal%2520adaptation%252C%2520a%2520novel%2520setting%2520for%2520jointly%2520adapting%2520and%2520conformalizing%250Apre-trained%2520foundation%2520models%252C%2520which%2520operates%2520transductively%2520over%2520each%2520test%250Adata%2520point%2520using%2520a%2520few-shot%2520adaptation%2520set.%2520Moreover%252C%2520we%2520complement%2520this%250Aframework%2520with%2520SS-Text%252C%2520a%2520novel%2520training-free%2520linear%2520probe%2520solver%2520for%2520VLMs%2520that%250Aalleviates%2520the%2520computational%2520cost%2520of%2520such%2520a%2520transductive%2520approach.%2520We%2520provide%250Acomprehensive%2520experiments%2520using%25203%2520different%2520modality-specialized%2520medical%2520VLMs%250Aand%25209%2520adaptation%2520tasks.%2520Our%2520framework%2520requires%2520exactly%2520the%2520same%2520data%2520as%2520SCP%252C%250Aand%2520provides%2520consistent%2520relative%2520improvements%2520of%2520up%2520to%252027%2525%2520on%2520set%2520efficiency%250Awhile%2520maintaining%2520the%2520same%2520coverage%2520guarantees.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06076v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Full%20Conformal%20Adaptation%20of%20Medical%20Vision-Language%20Models&entry.906535625=Julio%20Silva-Rodr%C3%ADguez%20and%20Leo%20Fillioux%20and%20Paul-Henry%20Courn%C3%A8de%20and%20Maria%20Vakalopoulou%20and%20Stergios%20Christodoulidis%20and%20Ismail%20Ben%20Ayed%20and%20Jose%20Dolz&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20pre-trained%20at%20large%20scale%20have%20shown%0Aunprecedented%20transferability%20capabilities%20and%20are%20being%20progressively%0Aintegrated%20into%20medical%20image%20analysis.%20Although%20its%20discriminative%20potential%0Ahas%20been%20widely%20explored%2C%20its%20reliability%20aspect%20remains%20overlooked.%20This%20work%0Ainvestigates%20their%20behavior%20under%20the%20increasingly%20popular%20split%20conformal%0Aprediction%20%28SCP%29%20framework%2C%20which%20theoretically%20guarantees%20a%20given%20error%20level%0Aon%20output%20sets%20by%20leveraging%20a%20labeled%20calibration%20set.%20However%2C%20the%20zero-shot%0Aperformance%20of%20VLMs%20is%20inherently%20limited%2C%20and%20common%20practice%20involves%0Afew-shot%20transfer%20learning%20pipelines%2C%20which%20cannot%20absorb%20the%20rigid%0Aexchangeability%20assumptions%20of%20SCP.%20To%20alleviate%20this%20issue%2C%20we%20propose%20full%0Aconformal%20adaptation%2C%20a%20novel%20setting%20for%20jointly%20adapting%20and%20conformalizing%0Apre-trained%20foundation%20models%2C%20which%20operates%20transductively%20over%20each%20test%0Adata%20point%20using%20a%20few-shot%20adaptation%20set.%20Moreover%2C%20we%20complement%20this%0Aframework%20with%20SS-Text%2C%20a%20novel%20training-free%20linear%20probe%20solver%20for%20VLMs%20that%0Aalleviates%20the%20computational%20cost%20of%20such%20a%20transductive%20approach.%20We%20provide%0Acomprehensive%20experiments%20using%203%20different%20modality-specialized%20medical%20VLMs%0Aand%209%20adaptation%20tasks.%20Our%20framework%20requires%20exactly%20the%20same%20data%20as%20SCP%2C%0Aand%20provides%20consistent%20relative%20improvements%20of%20up%20to%2027%25%20on%20set%20efficiency%0Awhile%20maintaining%20the%20same%20coverage%20guarantees.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06076v1&entry.124074799=Read"},
{"title": "Sparse Autoencoders Learn Monosemantic Features in Vision-Language\n  Models", "author": "Mateusz Pach and Shyamgopal Karthik and Quentin Bouniot and Serge Belongie and Zeynep Akata", "abstract": "  Given that interpretability and steerability are crucial to AI safety, Sparse\nAutoencoders (SAEs) have emerged as a tool to enhance them in Large Language\nModels (LLMs). In this work, we extend the application of SAEs to\nVision-Language Models (VLMs), such as CLIP, and introduce a comprehensive\nframework for evaluating monosemanticity at the neuron-level in vision\nrepresentations. To ensure that our evaluation aligns with human perception, we\npropose a benchmark derived from a large-scale user study. Our experimental\nresults reveal that SAEs trained on VLMs significantly enhance the\nmonosemanticity of individual neurons, with sparsity and wide latents being the\nmost influential factors. Notably, we demonstrate that applying SAE\ninterventions on CLIP's vision encoder directly steers multimodal LLM outputs\n(e.g., LLaVA), without any modifications to the underlying model. These\nfindings emphasize the practicality and efficacy of SAEs as an unsupervised\ntool for enhancing both interpretability and control of VLMs. Code is available\nat https://github.com/ExplainableML/sae-for-vlm.\n", "link": "http://arxiv.org/abs/2504.02821v2", "date": "2025-06-06", "relevancy": 2.91, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6054}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6054}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Autoencoders%20Learn%20Monosemantic%20Features%20in%20Vision-Language%0A%20%20Models&body=Title%3A%20Sparse%20Autoencoders%20Learn%20Monosemantic%20Features%20in%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Mateusz%20Pach%20and%20Shyamgopal%20Karthik%20and%20Quentin%20Bouniot%20and%20Serge%20Belongie%20and%20Zeynep%20Akata%0AAbstract%3A%20%20%20Given%20that%20interpretability%20and%20steerability%20are%20crucial%20to%20AI%20safety%2C%20Sparse%0AAutoencoders%20%28SAEs%29%20have%20emerged%20as%20a%20tool%20to%20enhance%20them%20in%20Large%20Language%0AModels%20%28LLMs%29.%20In%20this%20work%2C%20we%20extend%20the%20application%20of%20SAEs%20to%0AVision-Language%20Models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20and%20introduce%20a%20comprehensive%0Aframework%20for%20evaluating%20monosemanticity%20at%20the%20neuron-level%20in%20vision%0Arepresentations.%20To%20ensure%20that%20our%20evaluation%20aligns%20with%20human%20perception%2C%20we%0Apropose%20a%20benchmark%20derived%20from%20a%20large-scale%20user%20study.%20Our%20experimental%0Aresults%20reveal%20that%20SAEs%20trained%20on%20VLMs%20significantly%20enhance%20the%0Amonosemanticity%20of%20individual%20neurons%2C%20with%20sparsity%20and%20wide%20latents%20being%20the%0Amost%20influential%20factors.%20Notably%2C%20we%20demonstrate%20that%20applying%20SAE%0Ainterventions%20on%20CLIP%27s%20vision%20encoder%20directly%20steers%20multimodal%20LLM%20outputs%0A%28e.g.%2C%20LLaVA%29%2C%20without%20any%20modifications%20to%20the%20underlying%20model.%20These%0Afindings%20emphasize%20the%20practicality%20and%20efficacy%20of%20SAEs%20as%20an%20unsupervised%0Atool%20for%20enhancing%20both%20interpretability%20and%20control%20of%20VLMs.%20Code%20is%20available%0Aat%20https%3A//github.com/ExplainableML/sae-for-vlm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.02821v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Autoencoders%2520Learn%2520Monosemantic%2520Features%2520in%2520Vision-Language%250A%2520%2520Models%26entry.906535625%3DMateusz%2520Pach%2520and%2520Shyamgopal%2520Karthik%2520and%2520Quentin%2520Bouniot%2520and%2520Serge%2520Belongie%2520and%2520Zeynep%2520Akata%26entry.1292438233%3D%2520%2520Given%2520that%2520interpretability%2520and%2520steerability%2520are%2520crucial%2520to%2520AI%2520safety%252C%2520Sparse%250AAutoencoders%2520%2528SAEs%2529%2520have%2520emerged%2520as%2520a%2520tool%2520to%2520enhance%2520them%2520in%2520Large%2520Language%250AModels%2520%2528LLMs%2529.%2520In%2520this%2520work%252C%2520we%2520extend%2520the%2520application%2520of%2520SAEs%2520to%250AVision-Language%2520Models%2520%2528VLMs%2529%252C%2520such%2520as%2520CLIP%252C%2520and%2520introduce%2520a%2520comprehensive%250Aframework%2520for%2520evaluating%2520monosemanticity%2520at%2520the%2520neuron-level%2520in%2520vision%250Arepresentations.%2520To%2520ensure%2520that%2520our%2520evaluation%2520aligns%2520with%2520human%2520perception%252C%2520we%250Apropose%2520a%2520benchmark%2520derived%2520from%2520a%2520large-scale%2520user%2520study.%2520Our%2520experimental%250Aresults%2520reveal%2520that%2520SAEs%2520trained%2520on%2520VLMs%2520significantly%2520enhance%2520the%250Amonosemanticity%2520of%2520individual%2520neurons%252C%2520with%2520sparsity%2520and%2520wide%2520latents%2520being%2520the%250Amost%2520influential%2520factors.%2520Notably%252C%2520we%2520demonstrate%2520that%2520applying%2520SAE%250Ainterventions%2520on%2520CLIP%2527s%2520vision%2520encoder%2520directly%2520steers%2520multimodal%2520LLM%2520outputs%250A%2528e.g.%252C%2520LLaVA%2529%252C%2520without%2520any%2520modifications%2520to%2520the%2520underlying%2520model.%2520These%250Afindings%2520emphasize%2520the%2520practicality%2520and%2520efficacy%2520of%2520SAEs%2520as%2520an%2520unsupervised%250Atool%2520for%2520enhancing%2520both%2520interpretability%2520and%2520control%2520of%2520VLMs.%2520Code%2520is%2520available%250Aat%2520https%253A//github.com/ExplainableML/sae-for-vlm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.02821v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Autoencoders%20Learn%20Monosemantic%20Features%20in%20Vision-Language%0A%20%20Models&entry.906535625=Mateusz%20Pach%20and%20Shyamgopal%20Karthik%20and%20Quentin%20Bouniot%20and%20Serge%20Belongie%20and%20Zeynep%20Akata&entry.1292438233=%20%20Given%20that%20interpretability%20and%20steerability%20are%20crucial%20to%20AI%20safety%2C%20Sparse%0AAutoencoders%20%28SAEs%29%20have%20emerged%20as%20a%20tool%20to%20enhance%20them%20in%20Large%20Language%0AModels%20%28LLMs%29.%20In%20this%20work%2C%20we%20extend%20the%20application%20of%20SAEs%20to%0AVision-Language%20Models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20and%20introduce%20a%20comprehensive%0Aframework%20for%20evaluating%20monosemanticity%20at%20the%20neuron-level%20in%20vision%0Arepresentations.%20To%20ensure%20that%20our%20evaluation%20aligns%20with%20human%20perception%2C%20we%0Apropose%20a%20benchmark%20derived%20from%20a%20large-scale%20user%20study.%20Our%20experimental%0Aresults%20reveal%20that%20SAEs%20trained%20on%20VLMs%20significantly%20enhance%20the%0Amonosemanticity%20of%20individual%20neurons%2C%20with%20sparsity%20and%20wide%20latents%20being%20the%0Amost%20influential%20factors.%20Notably%2C%20we%20demonstrate%20that%20applying%20SAE%0Ainterventions%20on%20CLIP%27s%20vision%20encoder%20directly%20steers%20multimodal%20LLM%20outputs%0A%28e.g.%2C%20LLaVA%29%2C%20without%20any%20modifications%20to%20the%20underlying%20model.%20These%0Afindings%20emphasize%20the%20practicality%20and%20efficacy%20of%20SAEs%20as%20an%20unsupervised%0Atool%20for%20enhancing%20both%20interpretability%20and%20control%20of%20VLMs.%20Code%20is%20available%0Aat%20https%3A//github.com/ExplainableML/sae-for-vlm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.02821v2&entry.124074799=Read"},
{"title": "Challenging Vision-Language Models with Surgical Data: A New Dataset and\n  Broad Benchmarking Study", "author": "Leon Mayer and Tim R\u00e4dsch and Dominik Michael and Lucas Luttner and Amine Yamlahi and Evangelia Christodoulou and Patrick Godau and Marcel Knopp and Annika Reinke and Fiona Kolbinger and Lena Maier-Hein", "abstract": "  While traditional computer vision models have historically struggled to\ngeneralize to endoscopic domains, the emergence of foundation models has shown\npromising cross-domain performance. In this work, we present the first\nlarge-scale study assessing the capabilities of Vision Language Models (VLMs)\nfor endoscopic tasks with a specific focus on laparoscopic surgery. Using a\ndiverse set of state-of-the-art models, multiple surgical datasets, and\nextensive human reference annotations, we address three key research questions:\n(1) Can current VLMs solve basic perception tasks on surgical images? (2) Can\nthey handle advanced frame-based endoscopic scene understanding tasks? and (3)\nHow do specialized medical VLMs compare to generalist models in this context?\nOur results reveal that VLMs can effectively perform basic surgical perception\ntasks, such as object counting and localization, with performance levels\ncomparable to general domain tasks. However, their performance deteriorates\nsignificantly when the tasks require medical knowledge. Notably, we find that\nspecialized medical VLMs currently underperform compared to generalist models\nacross both basic and advanced surgical tasks, suggesting that they are not yet\noptimized for the complexity of surgical environments. These findings highlight\nthe need for further advancements to enable VLMs to handle the unique\nchallenges posed by surgery. Overall, our work provides important insights for\nthe development of next-generation endoscopic AI systems and identifies key\nareas for improvement in medical visual language models.\n", "link": "http://arxiv.org/abs/2506.06232v1", "date": "2025-06-06", "relevancy": 2.9024, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6085}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6085}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5244}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Challenging%20Vision-Language%20Models%20with%20Surgical%20Data%3A%20A%20New%20Dataset%20and%0A%20%20Broad%20Benchmarking%20Study&body=Title%3A%20Challenging%20Vision-Language%20Models%20with%20Surgical%20Data%3A%20A%20New%20Dataset%20and%0A%20%20Broad%20Benchmarking%20Study%0AAuthor%3A%20Leon%20Mayer%20and%20Tim%20R%C3%A4dsch%20and%20Dominik%20Michael%20and%20Lucas%20Luttner%20and%20Amine%20Yamlahi%20and%20Evangelia%20Christodoulou%20and%20Patrick%20Godau%20and%20Marcel%20Knopp%20and%20Annika%20Reinke%20and%20Fiona%20Kolbinger%20and%20Lena%20Maier-Hein%0AAbstract%3A%20%20%20While%20traditional%20computer%20vision%20models%20have%20historically%20struggled%20to%0Ageneralize%20to%20endoscopic%20domains%2C%20the%20emergence%20of%20foundation%20models%20has%20shown%0Apromising%20cross-domain%20performance.%20In%20this%20work%2C%20we%20present%20the%20first%0Alarge-scale%20study%20assessing%20the%20capabilities%20of%20Vision%20Language%20Models%20%28VLMs%29%0Afor%20endoscopic%20tasks%20with%20a%20specific%20focus%20on%20laparoscopic%20surgery.%20Using%20a%0Adiverse%20set%20of%20state-of-the-art%20models%2C%20multiple%20surgical%20datasets%2C%20and%0Aextensive%20human%20reference%20annotations%2C%20we%20address%20three%20key%20research%20questions%3A%0A%281%29%20Can%20current%20VLMs%20solve%20basic%20perception%20tasks%20on%20surgical%20images%3F%20%282%29%20Can%0Athey%20handle%20advanced%20frame-based%20endoscopic%20scene%20understanding%20tasks%3F%20and%20%283%29%0AHow%20do%20specialized%20medical%20VLMs%20compare%20to%20generalist%20models%20in%20this%20context%3F%0AOur%20results%20reveal%20that%20VLMs%20can%20effectively%20perform%20basic%20surgical%20perception%0Atasks%2C%20such%20as%20object%20counting%20and%20localization%2C%20with%20performance%20levels%0Acomparable%20to%20general%20domain%20tasks.%20However%2C%20their%20performance%20deteriorates%0Asignificantly%20when%20the%20tasks%20require%20medical%20knowledge.%20Notably%2C%20we%20find%20that%0Aspecialized%20medical%20VLMs%20currently%20underperform%20compared%20to%20generalist%20models%0Aacross%20both%20basic%20and%20advanced%20surgical%20tasks%2C%20suggesting%20that%20they%20are%20not%20yet%0Aoptimized%20for%20the%20complexity%20of%20surgical%20environments.%20These%20findings%20highlight%0Athe%20need%20for%20further%20advancements%20to%20enable%20VLMs%20to%20handle%20the%20unique%0Achallenges%20posed%20by%20surgery.%20Overall%2C%20our%20work%20provides%20important%20insights%20for%0Athe%20development%20of%20next-generation%20endoscopic%20AI%20systems%20and%20identifies%20key%0Aareas%20for%20improvement%20in%20medical%20visual%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06232v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChallenging%2520Vision-Language%2520Models%2520with%2520Surgical%2520Data%253A%2520A%2520New%2520Dataset%2520and%250A%2520%2520Broad%2520Benchmarking%2520Study%26entry.906535625%3DLeon%2520Mayer%2520and%2520Tim%2520R%25C3%25A4dsch%2520and%2520Dominik%2520Michael%2520and%2520Lucas%2520Luttner%2520and%2520Amine%2520Yamlahi%2520and%2520Evangelia%2520Christodoulou%2520and%2520Patrick%2520Godau%2520and%2520Marcel%2520Knopp%2520and%2520Annika%2520Reinke%2520and%2520Fiona%2520Kolbinger%2520and%2520Lena%2520Maier-Hein%26entry.1292438233%3D%2520%2520While%2520traditional%2520computer%2520vision%2520models%2520have%2520historically%2520struggled%2520to%250Ageneralize%2520to%2520endoscopic%2520domains%252C%2520the%2520emergence%2520of%2520foundation%2520models%2520has%2520shown%250Apromising%2520cross-domain%2520performance.%2520In%2520this%2520work%252C%2520we%2520present%2520the%2520first%250Alarge-scale%2520study%2520assessing%2520the%2520capabilities%2520of%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%250Afor%2520endoscopic%2520tasks%2520with%2520a%2520specific%2520focus%2520on%2520laparoscopic%2520surgery.%2520Using%2520a%250Adiverse%2520set%2520of%2520state-of-the-art%2520models%252C%2520multiple%2520surgical%2520datasets%252C%2520and%250Aextensive%2520human%2520reference%2520annotations%252C%2520we%2520address%2520three%2520key%2520research%2520questions%253A%250A%25281%2529%2520Can%2520current%2520VLMs%2520solve%2520basic%2520perception%2520tasks%2520on%2520surgical%2520images%253F%2520%25282%2529%2520Can%250Athey%2520handle%2520advanced%2520frame-based%2520endoscopic%2520scene%2520understanding%2520tasks%253F%2520and%2520%25283%2529%250AHow%2520do%2520specialized%2520medical%2520VLMs%2520compare%2520to%2520generalist%2520models%2520in%2520this%2520context%253F%250AOur%2520results%2520reveal%2520that%2520VLMs%2520can%2520effectively%2520perform%2520basic%2520surgical%2520perception%250Atasks%252C%2520such%2520as%2520object%2520counting%2520and%2520localization%252C%2520with%2520performance%2520levels%250Acomparable%2520to%2520general%2520domain%2520tasks.%2520However%252C%2520their%2520performance%2520deteriorates%250Asignificantly%2520when%2520the%2520tasks%2520require%2520medical%2520knowledge.%2520Notably%252C%2520we%2520find%2520that%250Aspecialized%2520medical%2520VLMs%2520currently%2520underperform%2520compared%2520to%2520generalist%2520models%250Aacross%2520both%2520basic%2520and%2520advanced%2520surgical%2520tasks%252C%2520suggesting%2520that%2520they%2520are%2520not%2520yet%250Aoptimized%2520for%2520the%2520complexity%2520of%2520surgical%2520environments.%2520These%2520findings%2520highlight%250Athe%2520need%2520for%2520further%2520advancements%2520to%2520enable%2520VLMs%2520to%2520handle%2520the%2520unique%250Achallenges%2520posed%2520by%2520surgery.%2520Overall%252C%2520our%2520work%2520provides%2520important%2520insights%2520for%250Athe%2520development%2520of%2520next-generation%2520endoscopic%2520AI%2520systems%2520and%2520identifies%2520key%250Aareas%2520for%2520improvement%2520in%2520medical%2520visual%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06232v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Challenging%20Vision-Language%20Models%20with%20Surgical%20Data%3A%20A%20New%20Dataset%20and%0A%20%20Broad%20Benchmarking%20Study&entry.906535625=Leon%20Mayer%20and%20Tim%20R%C3%A4dsch%20and%20Dominik%20Michael%20and%20Lucas%20Luttner%20and%20Amine%20Yamlahi%20and%20Evangelia%20Christodoulou%20and%20Patrick%20Godau%20and%20Marcel%20Knopp%20and%20Annika%20Reinke%20and%20Fiona%20Kolbinger%20and%20Lena%20Maier-Hein&entry.1292438233=%20%20While%20traditional%20computer%20vision%20models%20have%20historically%20struggled%20to%0Ageneralize%20to%20endoscopic%20domains%2C%20the%20emergence%20of%20foundation%20models%20has%20shown%0Apromising%20cross-domain%20performance.%20In%20this%20work%2C%20we%20present%20the%20first%0Alarge-scale%20study%20assessing%20the%20capabilities%20of%20Vision%20Language%20Models%20%28VLMs%29%0Afor%20endoscopic%20tasks%20with%20a%20specific%20focus%20on%20laparoscopic%20surgery.%20Using%20a%0Adiverse%20set%20of%20state-of-the-art%20models%2C%20multiple%20surgical%20datasets%2C%20and%0Aextensive%20human%20reference%20annotations%2C%20we%20address%20three%20key%20research%20questions%3A%0A%281%29%20Can%20current%20VLMs%20solve%20basic%20perception%20tasks%20on%20surgical%20images%3F%20%282%29%20Can%0Athey%20handle%20advanced%20frame-based%20endoscopic%20scene%20understanding%20tasks%3F%20and%20%283%29%0AHow%20do%20specialized%20medical%20VLMs%20compare%20to%20generalist%20models%20in%20this%20context%3F%0AOur%20results%20reveal%20that%20VLMs%20can%20effectively%20perform%20basic%20surgical%20perception%0Atasks%2C%20such%20as%20object%20counting%20and%20localization%2C%20with%20performance%20levels%0Acomparable%20to%20general%20domain%20tasks.%20However%2C%20their%20performance%20deteriorates%0Asignificantly%20when%20the%20tasks%20require%20medical%20knowledge.%20Notably%2C%20we%20find%20that%0Aspecialized%20medical%20VLMs%20currently%20underperform%20compared%20to%20generalist%20models%0Aacross%20both%20basic%20and%20advanced%20surgical%20tasks%2C%20suggesting%20that%20they%20are%20not%20yet%0Aoptimized%20for%20the%20complexity%20of%20surgical%20environments.%20These%20findings%20highlight%0Athe%20need%20for%20further%20advancements%20to%20enable%20VLMs%20to%20handle%20the%20unique%0Achallenges%20posed%20by%20surgery.%20Overall%2C%20our%20work%20provides%20important%20insights%20for%0Athe%20development%20of%20next-generation%20endoscopic%20AI%20systems%20and%20identifies%20key%0Aareas%20for%20improvement%20in%20medical%20visual%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06232v1&entry.124074799=Read"},
{"title": "Bridging Perspectives: A Survey on Cross-view Collaborative Intelligence\n  with Egocentric-Exocentric Vision", "author": "Yuping He and Yifei Huang and Guo Chen and Lidong Lu and Baoqi Pei and Jilan Xu and Tong Lu and Yoichi Sato", "abstract": "  Perceiving the world from both egocentric (first-person) and exocentric\n(third-person) perspectives is fundamental to human cognition, enabling rich\nand complementary understanding of dynamic environments. In recent years,\nallowing the machines to leverage the synergistic potential of these dual\nperspectives has emerged as a compelling research direction in video\nunderstanding. In this survey, we provide a comprehensive review of video\nunderstanding from both exocentric and egocentric viewpoints. We begin by\nhighlighting the practical applications of integrating egocentric and\nexocentric techniques, envisioning their potential collaboration across\ndomains. We then identify key research tasks to realize these applications.\nNext, we systematically organize and review recent advancements into three main\nresearch directions: (1) leveraging egocentric data to enhance exocentric\nunderstanding, (2) utilizing exocentric data to improve egocentric analysis,\nand (3) joint learning frameworks that unify both perspectives. For each\ndirection, we analyze a diverse set of tasks and relevant works. Additionally,\nwe discuss benchmark datasets that support research in both perspectives,\nevaluating their scope, diversity, and applicability. Finally, we discuss\nlimitations in current works and propose promising future research directions.\nBy synthesizing insights from both perspectives, our goal is to inspire\nadvancements in video understanding and artificial intelligence, bringing\nmachines closer to perceiving the world in a human-like manner. A GitHub repo\nof related works can be found at\nhttps://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision.\n", "link": "http://arxiv.org/abs/2506.06253v1", "date": "2025-06-06", "relevancy": 2.8266, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5757}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5757}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Perspectives%3A%20A%20Survey%20on%20Cross-view%20Collaborative%20Intelligence%0A%20%20with%20Egocentric-Exocentric%20Vision&body=Title%3A%20Bridging%20Perspectives%3A%20A%20Survey%20on%20Cross-view%20Collaborative%20Intelligence%0A%20%20with%20Egocentric-Exocentric%20Vision%0AAuthor%3A%20Yuping%20He%20and%20Yifei%20Huang%20and%20Guo%20Chen%20and%20Lidong%20Lu%20and%20Baoqi%20Pei%20and%20Jilan%20Xu%20and%20Tong%20Lu%20and%20Yoichi%20Sato%0AAbstract%3A%20%20%20Perceiving%20the%20world%20from%20both%20egocentric%20%28first-person%29%20and%20exocentric%0A%28third-person%29%20perspectives%20is%20fundamental%20to%20human%20cognition%2C%20enabling%20rich%0Aand%20complementary%20understanding%20of%20dynamic%20environments.%20In%20recent%20years%2C%0Aallowing%20the%20machines%20to%20leverage%20the%20synergistic%20potential%20of%20these%20dual%0Aperspectives%20has%20emerged%20as%20a%20compelling%20research%20direction%20in%20video%0Aunderstanding.%20In%20this%20survey%2C%20we%20provide%20a%20comprehensive%20review%20of%20video%0Aunderstanding%20from%20both%20exocentric%20and%20egocentric%20viewpoints.%20We%20begin%20by%0Ahighlighting%20the%20practical%20applications%20of%20integrating%20egocentric%20and%0Aexocentric%20techniques%2C%20envisioning%20their%20potential%20collaboration%20across%0Adomains.%20We%20then%20identify%20key%20research%20tasks%20to%20realize%20these%20applications.%0ANext%2C%20we%20systematically%20organize%20and%20review%20recent%20advancements%20into%20three%20main%0Aresearch%20directions%3A%20%281%29%20leveraging%20egocentric%20data%20to%20enhance%20exocentric%0Aunderstanding%2C%20%282%29%20utilizing%20exocentric%20data%20to%20improve%20egocentric%20analysis%2C%0Aand%20%283%29%20joint%20learning%20frameworks%20that%20unify%20both%20perspectives.%20For%20each%0Adirection%2C%20we%20analyze%20a%20diverse%20set%20of%20tasks%20and%20relevant%20works.%20Additionally%2C%0Awe%20discuss%20benchmark%20datasets%20that%20support%20research%20in%20both%20perspectives%2C%0Aevaluating%20their%20scope%2C%20diversity%2C%20and%20applicability.%20Finally%2C%20we%20discuss%0Alimitations%20in%20current%20works%20and%20propose%20promising%20future%20research%20directions.%0ABy%20synthesizing%20insights%20from%20both%20perspectives%2C%20our%20goal%20is%20to%20inspire%0Aadvancements%20in%20video%20understanding%20and%20artificial%20intelligence%2C%20bringing%0Amachines%20closer%20to%20perceiving%20the%20world%20in%20a%20human-like%20manner.%20A%20GitHub%20repo%0Aof%20related%20works%20can%20be%20found%20at%0Ahttps%3A//github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06253v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Perspectives%253A%2520A%2520Survey%2520on%2520Cross-view%2520Collaborative%2520Intelligence%250A%2520%2520with%2520Egocentric-Exocentric%2520Vision%26entry.906535625%3DYuping%2520He%2520and%2520Yifei%2520Huang%2520and%2520Guo%2520Chen%2520and%2520Lidong%2520Lu%2520and%2520Baoqi%2520Pei%2520and%2520Jilan%2520Xu%2520and%2520Tong%2520Lu%2520and%2520Yoichi%2520Sato%26entry.1292438233%3D%2520%2520Perceiving%2520the%2520world%2520from%2520both%2520egocentric%2520%2528first-person%2529%2520and%2520exocentric%250A%2528third-person%2529%2520perspectives%2520is%2520fundamental%2520to%2520human%2520cognition%252C%2520enabling%2520rich%250Aand%2520complementary%2520understanding%2520of%2520dynamic%2520environments.%2520In%2520recent%2520years%252C%250Aallowing%2520the%2520machines%2520to%2520leverage%2520the%2520synergistic%2520potential%2520of%2520these%2520dual%250Aperspectives%2520has%2520emerged%2520as%2520a%2520compelling%2520research%2520direction%2520in%2520video%250Aunderstanding.%2520In%2520this%2520survey%252C%2520we%2520provide%2520a%2520comprehensive%2520review%2520of%2520video%250Aunderstanding%2520from%2520both%2520exocentric%2520and%2520egocentric%2520viewpoints.%2520We%2520begin%2520by%250Ahighlighting%2520the%2520practical%2520applications%2520of%2520integrating%2520egocentric%2520and%250Aexocentric%2520techniques%252C%2520envisioning%2520their%2520potential%2520collaboration%2520across%250Adomains.%2520We%2520then%2520identify%2520key%2520research%2520tasks%2520to%2520realize%2520these%2520applications.%250ANext%252C%2520we%2520systematically%2520organize%2520and%2520review%2520recent%2520advancements%2520into%2520three%2520main%250Aresearch%2520directions%253A%2520%25281%2529%2520leveraging%2520egocentric%2520data%2520to%2520enhance%2520exocentric%250Aunderstanding%252C%2520%25282%2529%2520utilizing%2520exocentric%2520data%2520to%2520improve%2520egocentric%2520analysis%252C%250Aand%2520%25283%2529%2520joint%2520learning%2520frameworks%2520that%2520unify%2520both%2520perspectives.%2520For%2520each%250Adirection%252C%2520we%2520analyze%2520a%2520diverse%2520set%2520of%2520tasks%2520and%2520relevant%2520works.%2520Additionally%252C%250Awe%2520discuss%2520benchmark%2520datasets%2520that%2520support%2520research%2520in%2520both%2520perspectives%252C%250Aevaluating%2520their%2520scope%252C%2520diversity%252C%2520and%2520applicability.%2520Finally%252C%2520we%2520discuss%250Alimitations%2520in%2520current%2520works%2520and%2520propose%2520promising%2520future%2520research%2520directions.%250ABy%2520synthesizing%2520insights%2520from%2520both%2520perspectives%252C%2520our%2520goal%2520is%2520to%2520inspire%250Aadvancements%2520in%2520video%2520understanding%2520and%2520artificial%2520intelligence%252C%2520bringing%250Amachines%2520closer%2520to%2520perceiving%2520the%2520world%2520in%2520a%2520human-like%2520manner.%2520A%2520GitHub%2520repo%250Aof%2520related%2520works%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06253v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Perspectives%3A%20A%20Survey%20on%20Cross-view%20Collaborative%20Intelligence%0A%20%20with%20Egocentric-Exocentric%20Vision&entry.906535625=Yuping%20He%20and%20Yifei%20Huang%20and%20Guo%20Chen%20and%20Lidong%20Lu%20and%20Baoqi%20Pei%20and%20Jilan%20Xu%20and%20Tong%20Lu%20and%20Yoichi%20Sato&entry.1292438233=%20%20Perceiving%20the%20world%20from%20both%20egocentric%20%28first-person%29%20and%20exocentric%0A%28third-person%29%20perspectives%20is%20fundamental%20to%20human%20cognition%2C%20enabling%20rich%0Aand%20complementary%20understanding%20of%20dynamic%20environments.%20In%20recent%20years%2C%0Aallowing%20the%20machines%20to%20leverage%20the%20synergistic%20potential%20of%20these%20dual%0Aperspectives%20has%20emerged%20as%20a%20compelling%20research%20direction%20in%20video%0Aunderstanding.%20In%20this%20survey%2C%20we%20provide%20a%20comprehensive%20review%20of%20video%0Aunderstanding%20from%20both%20exocentric%20and%20egocentric%20viewpoints.%20We%20begin%20by%0Ahighlighting%20the%20practical%20applications%20of%20integrating%20egocentric%20and%0Aexocentric%20techniques%2C%20envisioning%20their%20potential%20collaboration%20across%0Adomains.%20We%20then%20identify%20key%20research%20tasks%20to%20realize%20these%20applications.%0ANext%2C%20we%20systematically%20organize%20and%20review%20recent%20advancements%20into%20three%20main%0Aresearch%20directions%3A%20%281%29%20leveraging%20egocentric%20data%20to%20enhance%20exocentric%0Aunderstanding%2C%20%282%29%20utilizing%20exocentric%20data%20to%20improve%20egocentric%20analysis%2C%0Aand%20%283%29%20joint%20learning%20frameworks%20that%20unify%20both%20perspectives.%20For%20each%0Adirection%2C%20we%20analyze%20a%20diverse%20set%20of%20tasks%20and%20relevant%20works.%20Additionally%2C%0Awe%20discuss%20benchmark%20datasets%20that%20support%20research%20in%20both%20perspectives%2C%0Aevaluating%20their%20scope%2C%20diversity%2C%20and%20applicability.%20Finally%2C%20we%20discuss%0Alimitations%20in%20current%20works%20and%20propose%20promising%20future%20research%20directions.%0ABy%20synthesizing%20insights%20from%20both%20perspectives%2C%20our%20goal%20is%20to%20inspire%0Aadvancements%20in%20video%20understanding%20and%20artificial%20intelligence%2C%20bringing%0Amachines%20closer%20to%20perceiving%20the%20world%20in%20a%20human-like%20manner.%20A%20GitHub%20repo%0Aof%20related%20works%20can%20be%20found%20at%0Ahttps%3A//github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06253v1&entry.124074799=Read"},
{"title": "SemiOccam: A Robust Semi-Supervised Image Recognition Network Using\n  Sparse Labels", "author": "Rui Yann and Xianglei Xing", "abstract": "  We present SemiOccam, an image recognition network that leverages\nsemi-supervised learning in a highly efficient manner. Existing works often\nrely on complex training techniques and architectures, requiring hundreds of\nGPU hours for training, while their generalization ability when dealing with\nextremely limited labeled data remains to be improved. To address these\nlimitations, we construct a hierarchical mixture density classification\ndecision mechanism by optimizing mutual information between feature\nrepresentations and target classes, compressing redundant information while\nretaining crucial discriminative components. Experimental results demonstrate\nthat our method achieves state-of-the-art performance on various datasets when\nusing negligible labeled samples, and its simple architecture keeps training\ntime to minute-level. Notably, this paper reveals a long-overlooked data\nleakage issue in the STL-10 dataset for semi-supervised learning tasks and\nremoves duplicates to ensure the reliability of experimental results. We also\nrelease the deduplicated CleanSTL-10 dataset to facilitate fair and reliable\nresearch in future semi-supervised learning. Code available at\nhttps://github.com/Shu1L0n9/SemiOccam.\n", "link": "http://arxiv.org/abs/2506.03582v2", "date": "2025-06-06", "relevancy": 2.8107, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5928}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5569}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SemiOccam%3A%20A%20Robust%20Semi-Supervised%20Image%20Recognition%20Network%20Using%0A%20%20Sparse%20Labels&body=Title%3A%20SemiOccam%3A%20A%20Robust%20Semi-Supervised%20Image%20Recognition%20Network%20Using%0A%20%20Sparse%20Labels%0AAuthor%3A%20Rui%20Yann%20and%20Xianglei%20Xing%0AAbstract%3A%20%20%20We%20present%20SemiOccam%2C%20an%20image%20recognition%20network%20that%20leverages%0Asemi-supervised%20learning%20in%20a%20highly%20efficient%20manner.%20Existing%20works%20often%0Arely%20on%20complex%20training%20techniques%20and%20architectures%2C%20requiring%20hundreds%20of%0AGPU%20hours%20for%20training%2C%20while%20their%20generalization%20ability%20when%20dealing%20with%0Aextremely%20limited%20labeled%20data%20remains%20to%20be%20improved.%20To%20address%20these%0Alimitations%2C%20we%20construct%20a%20hierarchical%20mixture%20density%20classification%0Adecision%20mechanism%20by%20optimizing%20mutual%20information%20between%20feature%0Arepresentations%20and%20target%20classes%2C%20compressing%20redundant%20information%20while%0Aretaining%20crucial%20discriminative%20components.%20Experimental%20results%20demonstrate%0Athat%20our%20method%20achieves%20state-of-the-art%20performance%20on%20various%20datasets%20when%0Ausing%20negligible%20labeled%20samples%2C%20and%20its%20simple%20architecture%20keeps%20training%0Atime%20to%20minute-level.%20Notably%2C%20this%20paper%20reveals%20a%20long-overlooked%20data%0Aleakage%20issue%20in%20the%20STL-10%20dataset%20for%20semi-supervised%20learning%20tasks%20and%0Aremoves%20duplicates%20to%20ensure%20the%20reliability%20of%20experimental%20results.%20We%20also%0Arelease%20the%20deduplicated%20CleanSTL-10%20dataset%20to%20facilitate%20fair%20and%20reliable%0Aresearch%20in%20future%20semi-supervised%20learning.%20Code%20available%20at%0Ahttps%3A//github.com/Shu1L0n9/SemiOccam.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03582v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemiOccam%253A%2520A%2520Robust%2520Semi-Supervised%2520Image%2520Recognition%2520Network%2520Using%250A%2520%2520Sparse%2520Labels%26entry.906535625%3DRui%2520Yann%2520and%2520Xianglei%2520Xing%26entry.1292438233%3D%2520%2520We%2520present%2520SemiOccam%252C%2520an%2520image%2520recognition%2520network%2520that%2520leverages%250Asemi-supervised%2520learning%2520in%2520a%2520highly%2520efficient%2520manner.%2520Existing%2520works%2520often%250Arely%2520on%2520complex%2520training%2520techniques%2520and%2520architectures%252C%2520requiring%2520hundreds%2520of%250AGPU%2520hours%2520for%2520training%252C%2520while%2520their%2520generalization%2520ability%2520when%2520dealing%2520with%250Aextremely%2520limited%2520labeled%2520data%2520remains%2520to%2520be%2520improved.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520construct%2520a%2520hierarchical%2520mixture%2520density%2520classification%250Adecision%2520mechanism%2520by%2520optimizing%2520mutual%2520information%2520between%2520feature%250Arepresentations%2520and%2520target%2520classes%252C%2520compressing%2520redundant%2520information%2520while%250Aretaining%2520crucial%2520discriminative%2520components.%2520Experimental%2520results%2520demonstrate%250Athat%2520our%2520method%2520achieves%2520state-of-the-art%2520performance%2520on%2520various%2520datasets%2520when%250Ausing%2520negligible%2520labeled%2520samples%252C%2520and%2520its%2520simple%2520architecture%2520keeps%2520training%250Atime%2520to%2520minute-level.%2520Notably%252C%2520this%2520paper%2520reveals%2520a%2520long-overlooked%2520data%250Aleakage%2520issue%2520in%2520the%2520STL-10%2520dataset%2520for%2520semi-supervised%2520learning%2520tasks%2520and%250Aremoves%2520duplicates%2520to%2520ensure%2520the%2520reliability%2520of%2520experimental%2520results.%2520We%2520also%250Arelease%2520the%2520deduplicated%2520CleanSTL-10%2520dataset%2520to%2520facilitate%2520fair%2520and%2520reliable%250Aresearch%2520in%2520future%2520semi-supervised%2520learning.%2520Code%2520available%2520at%250Ahttps%253A//github.com/Shu1L0n9/SemiOccam.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03582v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SemiOccam%3A%20A%20Robust%20Semi-Supervised%20Image%20Recognition%20Network%20Using%0A%20%20Sparse%20Labels&entry.906535625=Rui%20Yann%20and%20Xianglei%20Xing&entry.1292438233=%20%20We%20present%20SemiOccam%2C%20an%20image%20recognition%20network%20that%20leverages%0Asemi-supervised%20learning%20in%20a%20highly%20efficient%20manner.%20Existing%20works%20often%0Arely%20on%20complex%20training%20techniques%20and%20architectures%2C%20requiring%20hundreds%20of%0AGPU%20hours%20for%20training%2C%20while%20their%20generalization%20ability%20when%20dealing%20with%0Aextremely%20limited%20labeled%20data%20remains%20to%20be%20improved.%20To%20address%20these%0Alimitations%2C%20we%20construct%20a%20hierarchical%20mixture%20density%20classification%0Adecision%20mechanism%20by%20optimizing%20mutual%20information%20between%20feature%0Arepresentations%20and%20target%20classes%2C%20compressing%20redundant%20information%20while%0Aretaining%20crucial%20discriminative%20components.%20Experimental%20results%20demonstrate%0Athat%20our%20method%20achieves%20state-of-the-art%20performance%20on%20various%20datasets%20when%0Ausing%20negligible%20labeled%20samples%2C%20and%20its%20simple%20architecture%20keeps%20training%0Atime%20to%20minute-level.%20Notably%2C%20this%20paper%20reveals%20a%20long-overlooked%20data%0Aleakage%20issue%20in%20the%20STL-10%20dataset%20for%20semi-supervised%20learning%20tasks%20and%0Aremoves%20duplicates%20to%20ensure%20the%20reliability%20of%20experimental%20results.%20We%20also%0Arelease%20the%20deduplicated%20CleanSTL-10%20dataset%20to%20facilitate%20fair%20and%20reliable%0Aresearch%20in%20future%20semi-supervised%20learning.%20Code%20available%20at%0Ahttps%3A//github.com/Shu1L0n9/SemiOccam.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03582v2&entry.124074799=Read"},
{"title": "CLaMR: Contextualized Late-Interaction for Multimodal Content Retrieval", "author": "David Wan and Han Wang and Elias Stengel-Eskin and Jaemin Cho and Mohit Bansal", "abstract": "  Online video web content is richly multimodal: a single video blends vision,\nspeech, ambient audio, and on-screen text. Retrieval systems typically treat\nthese modalities as independent retrieval sources, which can lead to noisy and\nsubpar retrieval. We explore multimodal video content retrieval, where\nrelevance can be scored from one particular modality or jointly across multiple\nmodalities simultaneously. Consequently, an effective retriever must\ndynamically choose which modality (or set of modalities) best addresses the\nquery. We introduce CLaMR, a multimodal, late-interaction retriever that\njointly indexes 4 modalities: video frames, transcribed speech, on-screen text,\nand metadata. CLaMR jointly encodes all modalities with a unified multimodal\nbackbone for improved contextualization and is trained to enhance dynamic\nmodality selection via two key innovations. First, given the lack of training\ndata for multimodal retrieval, we introduce MultiVENT 2.0++, a large-scale\nsynthetic training dataset built on MultiVENT 2.0 (event-centric videos in\nvarious languages paired with queries) with modality-targeted queries. Next, we\npropose a modality-aware loss that jointly trains according to a standard\ncontrastive objective alongside an objective for learning correct modality\nusage. On the test sets of MultiVENT 2.0++ and MSRVTT, conventional aggregation\nstrategies, such as averaging similarities for baseline retrievers, degrade\nperformance by introducing noise from irrelevant modalities. In contrast, CLaMR\nconsistently outperforms existing retrievers: on MultiVENT 2.0++, CLaMR\nimproves nDCG@10 by 25.6 over the best single-modality retriever and by 35.4\nover the best multi-modality retriever. We illustrate CLaMR's downstream\nutility on long-video QA, retrieving relevant frames and obtaining a 3.50%\nboost over LanguageBind on Video-MME and 1.42% over dense sampling on\nLongVideoBench.\n", "link": "http://arxiv.org/abs/2506.06144v1", "date": "2025-06-06", "relevancy": 2.7503, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5911}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5296}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5296}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLaMR%3A%20Contextualized%20Late-Interaction%20for%20Multimodal%20Content%20Retrieval&body=Title%3A%20CLaMR%3A%20Contextualized%20Late-Interaction%20for%20Multimodal%20Content%20Retrieval%0AAuthor%3A%20David%20Wan%20and%20Han%20Wang%20and%20Elias%20Stengel-Eskin%20and%20Jaemin%20Cho%20and%20Mohit%20Bansal%0AAbstract%3A%20%20%20Online%20video%20web%20content%20is%20richly%20multimodal%3A%20a%20single%20video%20blends%20vision%2C%0Aspeech%2C%20ambient%20audio%2C%20and%20on-screen%20text.%20Retrieval%20systems%20typically%20treat%0Athese%20modalities%20as%20independent%20retrieval%20sources%2C%20which%20can%20lead%20to%20noisy%20and%0Asubpar%20retrieval.%20We%20explore%20multimodal%20video%20content%20retrieval%2C%20where%0Arelevance%20can%20be%20scored%20from%20one%20particular%20modality%20or%20jointly%20across%20multiple%0Amodalities%20simultaneously.%20Consequently%2C%20an%20effective%20retriever%20must%0Adynamically%20choose%20which%20modality%20%28or%20set%20of%20modalities%29%20best%20addresses%20the%0Aquery.%20We%20introduce%20CLaMR%2C%20a%20multimodal%2C%20late-interaction%20retriever%20that%0Ajointly%20indexes%204%20modalities%3A%20video%20frames%2C%20transcribed%20speech%2C%20on-screen%20text%2C%0Aand%20metadata.%20CLaMR%20jointly%20encodes%20all%20modalities%20with%20a%20unified%20multimodal%0Abackbone%20for%20improved%20contextualization%20and%20is%20trained%20to%20enhance%20dynamic%0Amodality%20selection%20via%20two%20key%20innovations.%20First%2C%20given%20the%20lack%20of%20training%0Adata%20for%20multimodal%20retrieval%2C%20we%20introduce%20MultiVENT%202.0%2B%2B%2C%20a%20large-scale%0Asynthetic%20training%20dataset%20built%20on%20MultiVENT%202.0%20%28event-centric%20videos%20in%0Avarious%20languages%20paired%20with%20queries%29%20with%20modality-targeted%20queries.%20Next%2C%20we%0Apropose%20a%20modality-aware%20loss%20that%20jointly%20trains%20according%20to%20a%20standard%0Acontrastive%20objective%20alongside%20an%20objective%20for%20learning%20correct%20modality%0Ausage.%20On%20the%20test%20sets%20of%20MultiVENT%202.0%2B%2B%20and%20MSRVTT%2C%20conventional%20aggregation%0Astrategies%2C%20such%20as%20averaging%20similarities%20for%20baseline%20retrievers%2C%20degrade%0Aperformance%20by%20introducing%20noise%20from%20irrelevant%20modalities.%20In%20contrast%2C%20CLaMR%0Aconsistently%20outperforms%20existing%20retrievers%3A%20on%20MultiVENT%202.0%2B%2B%2C%20CLaMR%0Aimproves%20nDCG%4010%20by%2025.6%20over%20the%20best%20single-modality%20retriever%20and%20by%2035.4%0Aover%20the%20best%20multi-modality%20retriever.%20We%20illustrate%20CLaMR%27s%20downstream%0Autility%20on%20long-video%20QA%2C%20retrieving%20relevant%20frames%20and%20obtaining%20a%203.50%25%0Aboost%20over%20LanguageBind%20on%20Video-MME%20and%201.42%25%20over%20dense%20sampling%20on%0ALongVideoBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06144v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLaMR%253A%2520Contextualized%2520Late-Interaction%2520for%2520Multimodal%2520Content%2520Retrieval%26entry.906535625%3DDavid%2520Wan%2520and%2520Han%2520Wang%2520and%2520Elias%2520Stengel-Eskin%2520and%2520Jaemin%2520Cho%2520and%2520Mohit%2520Bansal%26entry.1292438233%3D%2520%2520Online%2520video%2520web%2520content%2520is%2520richly%2520multimodal%253A%2520a%2520single%2520video%2520blends%2520vision%252C%250Aspeech%252C%2520ambient%2520audio%252C%2520and%2520on-screen%2520text.%2520Retrieval%2520systems%2520typically%2520treat%250Athese%2520modalities%2520as%2520independent%2520retrieval%2520sources%252C%2520which%2520can%2520lead%2520to%2520noisy%2520and%250Asubpar%2520retrieval.%2520We%2520explore%2520multimodal%2520video%2520content%2520retrieval%252C%2520where%250Arelevance%2520can%2520be%2520scored%2520from%2520one%2520particular%2520modality%2520or%2520jointly%2520across%2520multiple%250Amodalities%2520simultaneously.%2520Consequently%252C%2520an%2520effective%2520retriever%2520must%250Adynamically%2520choose%2520which%2520modality%2520%2528or%2520set%2520of%2520modalities%2529%2520best%2520addresses%2520the%250Aquery.%2520We%2520introduce%2520CLaMR%252C%2520a%2520multimodal%252C%2520late-interaction%2520retriever%2520that%250Ajointly%2520indexes%25204%2520modalities%253A%2520video%2520frames%252C%2520transcribed%2520speech%252C%2520on-screen%2520text%252C%250Aand%2520metadata.%2520CLaMR%2520jointly%2520encodes%2520all%2520modalities%2520with%2520a%2520unified%2520multimodal%250Abackbone%2520for%2520improved%2520contextualization%2520and%2520is%2520trained%2520to%2520enhance%2520dynamic%250Amodality%2520selection%2520via%2520two%2520key%2520innovations.%2520First%252C%2520given%2520the%2520lack%2520of%2520training%250Adata%2520for%2520multimodal%2520retrieval%252C%2520we%2520introduce%2520MultiVENT%25202.0%252B%252B%252C%2520a%2520large-scale%250Asynthetic%2520training%2520dataset%2520built%2520on%2520MultiVENT%25202.0%2520%2528event-centric%2520videos%2520in%250Avarious%2520languages%2520paired%2520with%2520queries%2529%2520with%2520modality-targeted%2520queries.%2520Next%252C%2520we%250Apropose%2520a%2520modality-aware%2520loss%2520that%2520jointly%2520trains%2520according%2520to%2520a%2520standard%250Acontrastive%2520objective%2520alongside%2520an%2520objective%2520for%2520learning%2520correct%2520modality%250Ausage.%2520On%2520the%2520test%2520sets%2520of%2520MultiVENT%25202.0%252B%252B%2520and%2520MSRVTT%252C%2520conventional%2520aggregation%250Astrategies%252C%2520such%2520as%2520averaging%2520similarities%2520for%2520baseline%2520retrievers%252C%2520degrade%250Aperformance%2520by%2520introducing%2520noise%2520from%2520irrelevant%2520modalities.%2520In%2520contrast%252C%2520CLaMR%250Aconsistently%2520outperforms%2520existing%2520retrievers%253A%2520on%2520MultiVENT%25202.0%252B%252B%252C%2520CLaMR%250Aimproves%2520nDCG%254010%2520by%252025.6%2520over%2520the%2520best%2520single-modality%2520retriever%2520and%2520by%252035.4%250Aover%2520the%2520best%2520multi-modality%2520retriever.%2520We%2520illustrate%2520CLaMR%2527s%2520downstream%250Autility%2520on%2520long-video%2520QA%252C%2520retrieving%2520relevant%2520frames%2520and%2520obtaining%2520a%25203.50%2525%250Aboost%2520over%2520LanguageBind%2520on%2520Video-MME%2520and%25201.42%2525%2520over%2520dense%2520sampling%2520on%250ALongVideoBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06144v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLaMR%3A%20Contextualized%20Late-Interaction%20for%20Multimodal%20Content%20Retrieval&entry.906535625=David%20Wan%20and%20Han%20Wang%20and%20Elias%20Stengel-Eskin%20and%20Jaemin%20Cho%20and%20Mohit%20Bansal&entry.1292438233=%20%20Online%20video%20web%20content%20is%20richly%20multimodal%3A%20a%20single%20video%20blends%20vision%2C%0Aspeech%2C%20ambient%20audio%2C%20and%20on-screen%20text.%20Retrieval%20systems%20typically%20treat%0Athese%20modalities%20as%20independent%20retrieval%20sources%2C%20which%20can%20lead%20to%20noisy%20and%0Asubpar%20retrieval.%20We%20explore%20multimodal%20video%20content%20retrieval%2C%20where%0Arelevance%20can%20be%20scored%20from%20one%20particular%20modality%20or%20jointly%20across%20multiple%0Amodalities%20simultaneously.%20Consequently%2C%20an%20effective%20retriever%20must%0Adynamically%20choose%20which%20modality%20%28or%20set%20of%20modalities%29%20best%20addresses%20the%0Aquery.%20We%20introduce%20CLaMR%2C%20a%20multimodal%2C%20late-interaction%20retriever%20that%0Ajointly%20indexes%204%20modalities%3A%20video%20frames%2C%20transcribed%20speech%2C%20on-screen%20text%2C%0Aand%20metadata.%20CLaMR%20jointly%20encodes%20all%20modalities%20with%20a%20unified%20multimodal%0Abackbone%20for%20improved%20contextualization%20and%20is%20trained%20to%20enhance%20dynamic%0Amodality%20selection%20via%20two%20key%20innovations.%20First%2C%20given%20the%20lack%20of%20training%0Adata%20for%20multimodal%20retrieval%2C%20we%20introduce%20MultiVENT%202.0%2B%2B%2C%20a%20large-scale%0Asynthetic%20training%20dataset%20built%20on%20MultiVENT%202.0%20%28event-centric%20videos%20in%0Avarious%20languages%20paired%20with%20queries%29%20with%20modality-targeted%20queries.%20Next%2C%20we%0Apropose%20a%20modality-aware%20loss%20that%20jointly%20trains%20according%20to%20a%20standard%0Acontrastive%20objective%20alongside%20an%20objective%20for%20learning%20correct%20modality%0Ausage.%20On%20the%20test%20sets%20of%20MultiVENT%202.0%2B%2B%20and%20MSRVTT%2C%20conventional%20aggregation%0Astrategies%2C%20such%20as%20averaging%20similarities%20for%20baseline%20retrievers%2C%20degrade%0Aperformance%20by%20introducing%20noise%20from%20irrelevant%20modalities.%20In%20contrast%2C%20CLaMR%0Aconsistently%20outperforms%20existing%20retrievers%3A%20on%20MultiVENT%202.0%2B%2B%2C%20CLaMR%0Aimproves%20nDCG%4010%20by%2025.6%20over%20the%20best%20single-modality%20retriever%20and%20by%2035.4%0Aover%20the%20best%20multi-modality%20retriever.%20We%20illustrate%20CLaMR%27s%20downstream%0Autility%20on%20long-video%20QA%2C%20retrieving%20relevant%20frames%20and%20obtaining%20a%203.50%25%0Aboost%20over%20LanguageBind%20on%20Video-MME%20and%201.42%25%20over%20dense%20sampling%20on%0ALongVideoBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06144v1&entry.124074799=Read"},
{"title": "MimeQA: Towards Socially-Intelligent Nonverbal Foundation Models", "author": "Hengzhi Li and Megan Tjandrasuwita and Yi R. Fung and Armando Solar-Lezama and Paul Pu Liang", "abstract": "  As AI becomes more closely integrated with peoples' daily activities,\nsocially intelligent AI that can understand and interact seamlessly with humans\nin daily lives is increasingly important. However, current works in AI social\nreasoning all rely on language-only or language-dominant approaches to\nbenchmark and training models, resulting in systems that are improving in\nverbal communication but struggle with nonverbal social understanding. To\naddress this limitation, we tap into a novel data source rich in nonverbal\nsocial interactions -- mime videos. Mimes refer to the art of expression\nthrough gesture and movement without spoken words, which presents unique\nchallenges and opportunities in interpreting nonverbal social communication. We\ncontribute a new dataset called MimeQA, obtained by sourcing 8 hours of videos\nclips from YouTube and developing a comprehensive video question-answering\nbenchmark comprising 806 carefully annotated and verified question-answer\npairs, designed to probe nonverbal social reasoning capabilities. Using MimeQA,\nwe evaluate state-of-the-art video large language models (vLLMs) and find that\nthey achieve low overall accuracy, ranging from 20-30%, while humans score 86%.\nOur analysis reveals that vLLMs often fail to ground imagined objects and\nover-rely on the text prompt while ignoring subtle nonverbal interactions. We\nhope to inspire future work in AI models that embody true social intelligence\ncapable of interpreting non-verbal human interactions.\n", "link": "http://arxiv.org/abs/2502.16671v2", "date": "2025-06-06", "relevancy": 2.7064, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5532}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5532}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MimeQA%3A%20Towards%20Socially-Intelligent%20Nonverbal%20Foundation%20Models&body=Title%3A%20MimeQA%3A%20Towards%20Socially-Intelligent%20Nonverbal%20Foundation%20Models%0AAuthor%3A%20Hengzhi%20Li%20and%20Megan%20Tjandrasuwita%20and%20Yi%20R.%20Fung%20and%20Armando%20Solar-Lezama%20and%20Paul%20Pu%20Liang%0AAbstract%3A%20%20%20As%20AI%20becomes%20more%20closely%20integrated%20with%20peoples%27%20daily%20activities%2C%0Asocially%20intelligent%20AI%20that%20can%20understand%20and%20interact%20seamlessly%20with%20humans%0Ain%20daily%20lives%20is%20increasingly%20important.%20However%2C%20current%20works%20in%20AI%20social%0Areasoning%20all%20rely%20on%20language-only%20or%20language-dominant%20approaches%20to%0Abenchmark%20and%20training%20models%2C%20resulting%20in%20systems%20that%20are%20improving%20in%0Averbal%20communication%20but%20struggle%20with%20nonverbal%20social%20understanding.%20To%0Aaddress%20this%20limitation%2C%20we%20tap%20into%20a%20novel%20data%20source%20rich%20in%20nonverbal%0Asocial%20interactions%20--%20mime%20videos.%20Mimes%20refer%20to%20the%20art%20of%20expression%0Athrough%20gesture%20and%20movement%20without%20spoken%20words%2C%20which%20presents%20unique%0Achallenges%20and%20opportunities%20in%20interpreting%20nonverbal%20social%20communication.%20We%0Acontribute%20a%20new%20dataset%20called%20MimeQA%2C%20obtained%20by%20sourcing%208%20hours%20of%20videos%0Aclips%20from%20YouTube%20and%20developing%20a%20comprehensive%20video%20question-answering%0Abenchmark%20comprising%20806%20carefully%20annotated%20and%20verified%20question-answer%0Apairs%2C%20designed%20to%20probe%20nonverbal%20social%20reasoning%20capabilities.%20Using%20MimeQA%2C%0Awe%20evaluate%20state-of-the-art%20video%20large%20language%20models%20%28vLLMs%29%20and%20find%20that%0Athey%20achieve%20low%20overall%20accuracy%2C%20ranging%20from%2020-30%25%2C%20while%20humans%20score%2086%25.%0AOur%20analysis%20reveals%20that%20vLLMs%20often%20fail%20to%20ground%20imagined%20objects%20and%0Aover-rely%20on%20the%20text%20prompt%20while%20ignoring%20subtle%20nonverbal%20interactions.%20We%0Ahope%20to%20inspire%20future%20work%20in%20AI%20models%20that%20embody%20true%20social%20intelligence%0Acapable%20of%20interpreting%20non-verbal%20human%20interactions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.16671v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMimeQA%253A%2520Towards%2520Socially-Intelligent%2520Nonverbal%2520Foundation%2520Models%26entry.906535625%3DHengzhi%2520Li%2520and%2520Megan%2520Tjandrasuwita%2520and%2520Yi%2520R.%2520Fung%2520and%2520Armando%2520Solar-Lezama%2520and%2520Paul%2520Pu%2520Liang%26entry.1292438233%3D%2520%2520As%2520AI%2520becomes%2520more%2520closely%2520integrated%2520with%2520peoples%2527%2520daily%2520activities%252C%250Asocially%2520intelligent%2520AI%2520that%2520can%2520understand%2520and%2520interact%2520seamlessly%2520with%2520humans%250Ain%2520daily%2520lives%2520is%2520increasingly%2520important.%2520However%252C%2520current%2520works%2520in%2520AI%2520social%250Areasoning%2520all%2520rely%2520on%2520language-only%2520or%2520language-dominant%2520approaches%2520to%250Abenchmark%2520and%2520training%2520models%252C%2520resulting%2520in%2520systems%2520that%2520are%2520improving%2520in%250Averbal%2520communication%2520but%2520struggle%2520with%2520nonverbal%2520social%2520understanding.%2520To%250Aaddress%2520this%2520limitation%252C%2520we%2520tap%2520into%2520a%2520novel%2520data%2520source%2520rich%2520in%2520nonverbal%250Asocial%2520interactions%2520--%2520mime%2520videos.%2520Mimes%2520refer%2520to%2520the%2520art%2520of%2520expression%250Athrough%2520gesture%2520and%2520movement%2520without%2520spoken%2520words%252C%2520which%2520presents%2520unique%250Achallenges%2520and%2520opportunities%2520in%2520interpreting%2520nonverbal%2520social%2520communication.%2520We%250Acontribute%2520a%2520new%2520dataset%2520called%2520MimeQA%252C%2520obtained%2520by%2520sourcing%25208%2520hours%2520of%2520videos%250Aclips%2520from%2520YouTube%2520and%2520developing%2520a%2520comprehensive%2520video%2520question-answering%250Abenchmark%2520comprising%2520806%2520carefully%2520annotated%2520and%2520verified%2520question-answer%250Apairs%252C%2520designed%2520to%2520probe%2520nonverbal%2520social%2520reasoning%2520capabilities.%2520Using%2520MimeQA%252C%250Awe%2520evaluate%2520state-of-the-art%2520video%2520large%2520language%2520models%2520%2528vLLMs%2529%2520and%2520find%2520that%250Athey%2520achieve%2520low%2520overall%2520accuracy%252C%2520ranging%2520from%252020-30%2525%252C%2520while%2520humans%2520score%252086%2525.%250AOur%2520analysis%2520reveals%2520that%2520vLLMs%2520often%2520fail%2520to%2520ground%2520imagined%2520objects%2520and%250Aover-rely%2520on%2520the%2520text%2520prompt%2520while%2520ignoring%2520subtle%2520nonverbal%2520interactions.%2520We%250Ahope%2520to%2520inspire%2520future%2520work%2520in%2520AI%2520models%2520that%2520embody%2520true%2520social%2520intelligence%250Acapable%2520of%2520interpreting%2520non-verbal%2520human%2520interactions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.16671v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MimeQA%3A%20Towards%20Socially-Intelligent%20Nonverbal%20Foundation%20Models&entry.906535625=Hengzhi%20Li%20and%20Megan%20Tjandrasuwita%20and%20Yi%20R.%20Fung%20and%20Armando%20Solar-Lezama%20and%20Paul%20Pu%20Liang&entry.1292438233=%20%20As%20AI%20becomes%20more%20closely%20integrated%20with%20peoples%27%20daily%20activities%2C%0Asocially%20intelligent%20AI%20that%20can%20understand%20and%20interact%20seamlessly%20with%20humans%0Ain%20daily%20lives%20is%20increasingly%20important.%20However%2C%20current%20works%20in%20AI%20social%0Areasoning%20all%20rely%20on%20language-only%20or%20language-dominant%20approaches%20to%0Abenchmark%20and%20training%20models%2C%20resulting%20in%20systems%20that%20are%20improving%20in%0Averbal%20communication%20but%20struggle%20with%20nonverbal%20social%20understanding.%20To%0Aaddress%20this%20limitation%2C%20we%20tap%20into%20a%20novel%20data%20source%20rich%20in%20nonverbal%0Asocial%20interactions%20--%20mime%20videos.%20Mimes%20refer%20to%20the%20art%20of%20expression%0Athrough%20gesture%20and%20movement%20without%20spoken%20words%2C%20which%20presents%20unique%0Achallenges%20and%20opportunities%20in%20interpreting%20nonverbal%20social%20communication.%20We%0Acontribute%20a%20new%20dataset%20called%20MimeQA%2C%20obtained%20by%20sourcing%208%20hours%20of%20videos%0Aclips%20from%20YouTube%20and%20developing%20a%20comprehensive%20video%20question-answering%0Abenchmark%20comprising%20806%20carefully%20annotated%20and%20verified%20question-answer%0Apairs%2C%20designed%20to%20probe%20nonverbal%20social%20reasoning%20capabilities.%20Using%20MimeQA%2C%0Awe%20evaluate%20state-of-the-art%20video%20large%20language%20models%20%28vLLMs%29%20and%20find%20that%0Athey%20achieve%20low%20overall%20accuracy%2C%20ranging%20from%2020-30%25%2C%20while%20humans%20score%2086%25.%0AOur%20analysis%20reveals%20that%20vLLMs%20often%20fail%20to%20ground%20imagined%20objects%20and%0Aover-rely%20on%20the%20text%20prompt%20while%20ignoring%20subtle%20nonverbal%20interactions.%20We%0Ahope%20to%20inspire%20future%20work%20in%20AI%20models%20that%20embody%20true%20social%20intelligence%0Acapable%20of%20interpreting%20non-verbal%20human%20interactions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.16671v2&entry.124074799=Read"},
{"title": "TerraFM: A Scalable Foundation Model for Unified Multisensor Earth\n  Observation", "author": "Muhammad Sohail Danish and Muhammad Akhtar Munir and Syed Roshaan Ali Shah and Muhammad Haris Khan and Rao Muhammad Anwer and Jorma Laaksonen and Fahad Shahbaz Khan and Salman Khan", "abstract": "  Modern Earth observation (EO) increasingly leverages deep learning to harness\nthe scale and diversity of satellite imagery across sensors and regions. While\nrecent foundation models have demonstrated promising generalization across EO\ntasks, many remain limited by the scale, geographical coverage, and spectral\ndiversity of their training data, factors critical for learning globally\ntransferable representations. In this work, we introduce TerraFM, a scalable\nself-supervised learning model that leverages globally distributed Sentinel-1\nand Sentinel-2 imagery, combined with large spatial tiles and land-cover aware\nsampling to enrich spatial and semantic coverage. By treating sensing\nmodalities as natural augmentations in our self-supervised approach, we unify\nradar and optical inputs via modality-specific patch embeddings and adaptive\ncross-attention fusion. Our training strategy integrates local-global\ncontrastive learning and introduces a dual-centering mechanism that\nincorporates class-frequency-aware regularization to address long-tailed\ndistributions in land cover.TerraFM achieves strong generalization on both\nclassification and segmentation tasks, outperforming prior models on GEO-Bench\nand Copernicus-Bench. Our code and pretrained models are publicly available at:\nhttps://github.com/mbzuai-oryx/TerraFM .\n", "link": "http://arxiv.org/abs/2506.06281v1", "date": "2025-06-06", "relevancy": 2.6939, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5411}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5376}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5376}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TerraFM%3A%20A%20Scalable%20Foundation%20Model%20for%20Unified%20Multisensor%20Earth%0A%20%20Observation&body=Title%3A%20TerraFM%3A%20A%20Scalable%20Foundation%20Model%20for%20Unified%20Multisensor%20Earth%0A%20%20Observation%0AAuthor%3A%20Muhammad%20Sohail%20Danish%20and%20Muhammad%20Akhtar%20Munir%20and%20Syed%20Roshaan%20Ali%20Shah%20and%20Muhammad%20Haris%20Khan%20and%20Rao%20Muhammad%20Anwer%20and%20Jorma%20Laaksonen%20and%20Fahad%20Shahbaz%20Khan%20and%20Salman%20Khan%0AAbstract%3A%20%20%20Modern%20Earth%20observation%20%28EO%29%20increasingly%20leverages%20deep%20learning%20to%20harness%0Athe%20scale%20and%20diversity%20of%20satellite%20imagery%20across%20sensors%20and%20regions.%20While%0Arecent%20foundation%20models%20have%20demonstrated%20promising%20generalization%20across%20EO%0Atasks%2C%20many%20remain%20limited%20by%20the%20scale%2C%20geographical%20coverage%2C%20and%20spectral%0Adiversity%20of%20their%20training%20data%2C%20factors%20critical%20for%20learning%20globally%0Atransferable%20representations.%20In%20this%20work%2C%20we%20introduce%20TerraFM%2C%20a%20scalable%0Aself-supervised%20learning%20model%20that%20leverages%20globally%20distributed%20Sentinel-1%0Aand%20Sentinel-2%20imagery%2C%20combined%20with%20large%20spatial%20tiles%20and%20land-cover%20aware%0Asampling%20to%20enrich%20spatial%20and%20semantic%20coverage.%20By%20treating%20sensing%0Amodalities%20as%20natural%20augmentations%20in%20our%20self-supervised%20approach%2C%20we%20unify%0Aradar%20and%20optical%20inputs%20via%20modality-specific%20patch%20embeddings%20and%20adaptive%0Across-attention%20fusion.%20Our%20training%20strategy%20integrates%20local-global%0Acontrastive%20learning%20and%20introduces%20a%20dual-centering%20mechanism%20that%0Aincorporates%20class-frequency-aware%20regularization%20to%20address%20long-tailed%0Adistributions%20in%20land%20cover.TerraFM%20achieves%20strong%20generalization%20on%20both%0Aclassification%20and%20segmentation%20tasks%2C%20outperforming%20prior%20models%20on%20GEO-Bench%0Aand%20Copernicus-Bench.%20Our%20code%20and%20pretrained%20models%20are%20publicly%20available%20at%3A%0Ahttps%3A//github.com/mbzuai-oryx/TerraFM%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06281v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTerraFM%253A%2520A%2520Scalable%2520Foundation%2520Model%2520for%2520Unified%2520Multisensor%2520Earth%250A%2520%2520Observation%26entry.906535625%3DMuhammad%2520Sohail%2520Danish%2520and%2520Muhammad%2520Akhtar%2520Munir%2520and%2520Syed%2520Roshaan%2520Ali%2520Shah%2520and%2520Muhammad%2520Haris%2520Khan%2520and%2520Rao%2520Muhammad%2520Anwer%2520and%2520Jorma%2520Laaksonen%2520and%2520Fahad%2520Shahbaz%2520Khan%2520and%2520Salman%2520Khan%26entry.1292438233%3D%2520%2520Modern%2520Earth%2520observation%2520%2528EO%2529%2520increasingly%2520leverages%2520deep%2520learning%2520to%2520harness%250Athe%2520scale%2520and%2520diversity%2520of%2520satellite%2520imagery%2520across%2520sensors%2520and%2520regions.%2520While%250Arecent%2520foundation%2520models%2520have%2520demonstrated%2520promising%2520generalization%2520across%2520EO%250Atasks%252C%2520many%2520remain%2520limited%2520by%2520the%2520scale%252C%2520geographical%2520coverage%252C%2520and%2520spectral%250Adiversity%2520of%2520their%2520training%2520data%252C%2520factors%2520critical%2520for%2520learning%2520globally%250Atransferable%2520representations.%2520In%2520this%2520work%252C%2520we%2520introduce%2520TerraFM%252C%2520a%2520scalable%250Aself-supervised%2520learning%2520model%2520that%2520leverages%2520globally%2520distributed%2520Sentinel-1%250Aand%2520Sentinel-2%2520imagery%252C%2520combined%2520with%2520large%2520spatial%2520tiles%2520and%2520land-cover%2520aware%250Asampling%2520to%2520enrich%2520spatial%2520and%2520semantic%2520coverage.%2520By%2520treating%2520sensing%250Amodalities%2520as%2520natural%2520augmentations%2520in%2520our%2520self-supervised%2520approach%252C%2520we%2520unify%250Aradar%2520and%2520optical%2520inputs%2520via%2520modality-specific%2520patch%2520embeddings%2520and%2520adaptive%250Across-attention%2520fusion.%2520Our%2520training%2520strategy%2520integrates%2520local-global%250Acontrastive%2520learning%2520and%2520introduces%2520a%2520dual-centering%2520mechanism%2520that%250Aincorporates%2520class-frequency-aware%2520regularization%2520to%2520address%2520long-tailed%250Adistributions%2520in%2520land%2520cover.TerraFM%2520achieves%2520strong%2520generalization%2520on%2520both%250Aclassification%2520and%2520segmentation%2520tasks%252C%2520outperforming%2520prior%2520models%2520on%2520GEO-Bench%250Aand%2520Copernicus-Bench.%2520Our%2520code%2520and%2520pretrained%2520models%2520are%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/mbzuai-oryx/TerraFM%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06281v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TerraFM%3A%20A%20Scalable%20Foundation%20Model%20for%20Unified%20Multisensor%20Earth%0A%20%20Observation&entry.906535625=Muhammad%20Sohail%20Danish%20and%20Muhammad%20Akhtar%20Munir%20and%20Syed%20Roshaan%20Ali%20Shah%20and%20Muhammad%20Haris%20Khan%20and%20Rao%20Muhammad%20Anwer%20and%20Jorma%20Laaksonen%20and%20Fahad%20Shahbaz%20Khan%20and%20Salman%20Khan&entry.1292438233=%20%20Modern%20Earth%20observation%20%28EO%29%20increasingly%20leverages%20deep%20learning%20to%20harness%0Athe%20scale%20and%20diversity%20of%20satellite%20imagery%20across%20sensors%20and%20regions.%20While%0Arecent%20foundation%20models%20have%20demonstrated%20promising%20generalization%20across%20EO%0Atasks%2C%20many%20remain%20limited%20by%20the%20scale%2C%20geographical%20coverage%2C%20and%20spectral%0Adiversity%20of%20their%20training%20data%2C%20factors%20critical%20for%20learning%20globally%0Atransferable%20representations.%20In%20this%20work%2C%20we%20introduce%20TerraFM%2C%20a%20scalable%0Aself-supervised%20learning%20model%20that%20leverages%20globally%20distributed%20Sentinel-1%0Aand%20Sentinel-2%20imagery%2C%20combined%20with%20large%20spatial%20tiles%20and%20land-cover%20aware%0Asampling%20to%20enrich%20spatial%20and%20semantic%20coverage.%20By%20treating%20sensing%0Amodalities%20as%20natural%20augmentations%20in%20our%20self-supervised%20approach%2C%20we%20unify%0Aradar%20and%20optical%20inputs%20via%20modality-specific%20patch%20embeddings%20and%20adaptive%0Across-attention%20fusion.%20Our%20training%20strategy%20integrates%20local-global%0Acontrastive%20learning%20and%20introduces%20a%20dual-centering%20mechanism%20that%0Aincorporates%20class-frequency-aware%20regularization%20to%20address%20long-tailed%0Adistributions%20in%20land%20cover.TerraFM%20achieves%20strong%20generalization%20on%20both%0Aclassification%20and%20segmentation%20tasks%2C%20outperforming%20prior%20models%20on%20GEO-Bench%0Aand%20Copernicus-Bench.%20Our%20code%20and%20pretrained%20models%20are%20publicly%20available%20at%3A%0Ahttps%3A//github.com/mbzuai-oryx/TerraFM%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06281v1&entry.124074799=Read"},
{"title": "DynamicMind: A Tri-Mode Thinking System for Large Language Models", "author": "Wei Li and Yanbin Wei and Qiushi Huang and Jiangyue Yan and Yang Chen and James T. Kwok and Yu Zhang", "abstract": "  Modern large language models (LLMs) often struggle to dynamically adapt their\nreasoning depth to varying task complexities, leading to suboptimal performance\nor inefficient resource utilization. To address this, we introduce DynamicMind,\na novel tri-mode thinking system. DynamicMind empowers LLMs to autonomously\nselect between Fast, Normal, and Slow thinking modes for zero-shot question\nanswering (ZSQA) tasks through cognitive-inspired prompt engineering. Our\nframework's core innovations include: (1) expanding the established\ndual-process framework of fast and slow thinking into a tri-mode thinking\nsystem involving a normal thinking mode to preserve the intrinsic capabilities\nof LLM; (2) proposing the Thinking Density metric, which aligns computational\nresource allocation with problem complexity; and (3) developing the Thinking\nMode Capacity (TMC) dataset and a lightweight Mind Router to predict the\noptimal thinking mode. Extensive experiments across diverse mathematical,\ncommonsense, and scientific QA benchmarks demonstrate that DynamicMind achieves\nsuperior ZSQA capabilities while establishing an effective trade-off between\nperformance and computational efficiency.\n", "link": "http://arxiv.org/abs/2506.05936v1", "date": "2025-06-06", "relevancy": 2.6808, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5525}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.528}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DynamicMind%3A%20A%20Tri-Mode%20Thinking%20System%20for%20Large%20Language%20Models&body=Title%3A%20DynamicMind%3A%20A%20Tri-Mode%20Thinking%20System%20for%20Large%20Language%20Models%0AAuthor%3A%20Wei%20Li%20and%20Yanbin%20Wei%20and%20Qiushi%20Huang%20and%20Jiangyue%20Yan%20and%20Yang%20Chen%20and%20James%20T.%20Kwok%20and%20Yu%20Zhang%0AAbstract%3A%20%20%20Modern%20large%20language%20models%20%28LLMs%29%20often%20struggle%20to%20dynamically%20adapt%20their%0Areasoning%20depth%20to%20varying%20task%20complexities%2C%20leading%20to%20suboptimal%20performance%0Aor%20inefficient%20resource%20utilization.%20To%20address%20this%2C%20we%20introduce%20DynamicMind%2C%0Aa%20novel%20tri-mode%20thinking%20system.%20DynamicMind%20empowers%20LLMs%20to%20autonomously%0Aselect%20between%20Fast%2C%20Normal%2C%20and%20Slow%20thinking%20modes%20for%20zero-shot%20question%0Aanswering%20%28ZSQA%29%20tasks%20through%20cognitive-inspired%20prompt%20engineering.%20Our%0Aframework%27s%20core%20innovations%20include%3A%20%281%29%20expanding%20the%20established%0Adual-process%20framework%20of%20fast%20and%20slow%20thinking%20into%20a%20tri-mode%20thinking%0Asystem%20involving%20a%20normal%20thinking%20mode%20to%20preserve%20the%20intrinsic%20capabilities%0Aof%20LLM%3B%20%282%29%20proposing%20the%20Thinking%20Density%20metric%2C%20which%20aligns%20computational%0Aresource%20allocation%20with%20problem%20complexity%3B%20and%20%283%29%20developing%20the%20Thinking%0AMode%20Capacity%20%28TMC%29%20dataset%20and%20a%20lightweight%20Mind%20Router%20to%20predict%20the%0Aoptimal%20thinking%20mode.%20Extensive%20experiments%20across%20diverse%20mathematical%2C%0Acommonsense%2C%20and%20scientific%20QA%20benchmarks%20demonstrate%20that%20DynamicMind%20achieves%0Asuperior%20ZSQA%20capabilities%20while%20establishing%20an%20effective%20trade-off%20between%0Aperformance%20and%20computational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05936v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamicMind%253A%2520A%2520Tri-Mode%2520Thinking%2520System%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DWei%2520Li%2520and%2520Yanbin%2520Wei%2520and%2520Qiushi%2520Huang%2520and%2520Jiangyue%2520Yan%2520and%2520Yang%2520Chen%2520and%2520James%2520T.%2520Kwok%2520and%2520Yu%2520Zhang%26entry.1292438233%3D%2520%2520Modern%2520large%2520language%2520models%2520%2528LLMs%2529%2520often%2520struggle%2520to%2520dynamically%2520adapt%2520their%250Areasoning%2520depth%2520to%2520varying%2520task%2520complexities%252C%2520leading%2520to%2520suboptimal%2520performance%250Aor%2520inefficient%2520resource%2520utilization.%2520To%2520address%2520this%252C%2520we%2520introduce%2520DynamicMind%252C%250Aa%2520novel%2520tri-mode%2520thinking%2520system.%2520DynamicMind%2520empowers%2520LLMs%2520to%2520autonomously%250Aselect%2520between%2520Fast%252C%2520Normal%252C%2520and%2520Slow%2520thinking%2520modes%2520for%2520zero-shot%2520question%250Aanswering%2520%2528ZSQA%2529%2520tasks%2520through%2520cognitive-inspired%2520prompt%2520engineering.%2520Our%250Aframework%2527s%2520core%2520innovations%2520include%253A%2520%25281%2529%2520expanding%2520the%2520established%250Adual-process%2520framework%2520of%2520fast%2520and%2520slow%2520thinking%2520into%2520a%2520tri-mode%2520thinking%250Asystem%2520involving%2520a%2520normal%2520thinking%2520mode%2520to%2520preserve%2520the%2520intrinsic%2520capabilities%250Aof%2520LLM%253B%2520%25282%2529%2520proposing%2520the%2520Thinking%2520Density%2520metric%252C%2520which%2520aligns%2520computational%250Aresource%2520allocation%2520with%2520problem%2520complexity%253B%2520and%2520%25283%2529%2520developing%2520the%2520Thinking%250AMode%2520Capacity%2520%2528TMC%2529%2520dataset%2520and%2520a%2520lightweight%2520Mind%2520Router%2520to%2520predict%2520the%250Aoptimal%2520thinking%2520mode.%2520Extensive%2520experiments%2520across%2520diverse%2520mathematical%252C%250Acommonsense%252C%2520and%2520scientific%2520QA%2520benchmarks%2520demonstrate%2520that%2520DynamicMind%2520achieves%250Asuperior%2520ZSQA%2520capabilities%2520while%2520establishing%2520an%2520effective%2520trade-off%2520between%250Aperformance%2520and%2520computational%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05936v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynamicMind%3A%20A%20Tri-Mode%20Thinking%20System%20for%20Large%20Language%20Models&entry.906535625=Wei%20Li%20and%20Yanbin%20Wei%20and%20Qiushi%20Huang%20and%20Jiangyue%20Yan%20and%20Yang%20Chen%20and%20James%20T.%20Kwok%20and%20Yu%20Zhang&entry.1292438233=%20%20Modern%20large%20language%20models%20%28LLMs%29%20often%20struggle%20to%20dynamically%20adapt%20their%0Areasoning%20depth%20to%20varying%20task%20complexities%2C%20leading%20to%20suboptimal%20performance%0Aor%20inefficient%20resource%20utilization.%20To%20address%20this%2C%20we%20introduce%20DynamicMind%2C%0Aa%20novel%20tri-mode%20thinking%20system.%20DynamicMind%20empowers%20LLMs%20to%20autonomously%0Aselect%20between%20Fast%2C%20Normal%2C%20and%20Slow%20thinking%20modes%20for%20zero-shot%20question%0Aanswering%20%28ZSQA%29%20tasks%20through%20cognitive-inspired%20prompt%20engineering.%20Our%0Aframework%27s%20core%20innovations%20include%3A%20%281%29%20expanding%20the%20established%0Adual-process%20framework%20of%20fast%20and%20slow%20thinking%20into%20a%20tri-mode%20thinking%0Asystem%20involving%20a%20normal%20thinking%20mode%20to%20preserve%20the%20intrinsic%20capabilities%0Aof%20LLM%3B%20%282%29%20proposing%20the%20Thinking%20Density%20metric%2C%20which%20aligns%20computational%0Aresource%20allocation%20with%20problem%20complexity%3B%20and%20%283%29%20developing%20the%20Thinking%0AMode%20Capacity%20%28TMC%29%20dataset%20and%20a%20lightweight%20Mind%20Router%20to%20predict%20the%0Aoptimal%20thinking%20mode.%20Extensive%20experiments%20across%20diverse%20mathematical%2C%0Acommonsense%2C%20and%20scientific%20QA%20benchmarks%20demonstrate%20that%20DynamicMind%20achieves%0Asuperior%20ZSQA%20capabilities%20while%20establishing%20an%20effective%20trade-off%20between%0Aperformance%20and%20computational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05936v1&entry.124074799=Read"},
{"title": "Graph Neural Network Generalization with Gaussian Mixture Model Based\n  Augmentation", "author": "Yassine Abbahaddou and Fragkiskos D. Malliaros and Johannes F. Lutzeyer and Amine Mohamed Aboussalah and Michalis Vazirgiannis", "abstract": "  Graph Neural Networks (GNNs) have shown great promise in tasks like node and\ngraph classification, but they often struggle to generalize, particularly to\nunseen or out-of-distribution (OOD) data. These challenges are exacerbated when\ntraining data is limited in size or diversity. To address these issues, we\nintroduce a theoretical framework using Rademacher complexity to compute a\nregret bound on the generalization error and then characterize the effect of\ndata augmentation. This framework informs the design of GRATIN, an efficient\ngraph data augmentation algorithm leveraging the capability of Gaussian Mixture\nModels (GMMs) to approximate any distribution. Our approach not only\noutperforms existing augmentation techniques in terms of generalization but\nalso offers improved time complexity, making it highly suitable for real-world\napplications.\n", "link": "http://arxiv.org/abs/2411.08638v3", "date": "2025-06-06", "relevancy": 2.6683, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5397}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5322}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5292}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Neural%20Network%20Generalization%20with%20Gaussian%20Mixture%20Model%20Based%0A%20%20Augmentation&body=Title%3A%20Graph%20Neural%20Network%20Generalization%20with%20Gaussian%20Mixture%20Model%20Based%0A%20%20Augmentation%0AAuthor%3A%20Yassine%20Abbahaddou%20and%20Fragkiskos%20D.%20Malliaros%20and%20Johannes%20F.%20Lutzeyer%20and%20Amine%20Mohamed%20Aboussalah%20and%20Michalis%20Vazirgiannis%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20shown%20great%20promise%20in%20tasks%20like%20node%20and%0Agraph%20classification%2C%20but%20they%20often%20struggle%20to%20generalize%2C%20particularly%20to%0Aunseen%20or%20out-of-distribution%20%28OOD%29%20data.%20These%20challenges%20are%20exacerbated%20when%0Atraining%20data%20is%20limited%20in%20size%20or%20diversity.%20To%20address%20these%20issues%2C%20we%0Aintroduce%20a%20theoretical%20framework%20using%20Rademacher%20complexity%20to%20compute%20a%0Aregret%20bound%20on%20the%20generalization%20error%20and%20then%20characterize%20the%20effect%20of%0Adata%20augmentation.%20This%20framework%20informs%20the%20design%20of%20GRATIN%2C%20an%20efficient%0Agraph%20data%20augmentation%20algorithm%20leveraging%20the%20capability%20of%20Gaussian%20Mixture%0AModels%20%28GMMs%29%20to%20approximate%20any%20distribution.%20Our%20approach%20not%20only%0Aoutperforms%20existing%20augmentation%20techniques%20in%20terms%20of%20generalization%20but%0Aalso%20offers%20improved%20time%20complexity%2C%20making%20it%20highly%20suitable%20for%20real-world%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08638v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Neural%2520Network%2520Generalization%2520with%2520Gaussian%2520Mixture%2520Model%2520Based%250A%2520%2520Augmentation%26entry.906535625%3DYassine%2520Abbahaddou%2520and%2520Fragkiskos%2520D.%2520Malliaros%2520and%2520Johannes%2520F.%2520Lutzeyer%2520and%2520Amine%2520Mohamed%2520Aboussalah%2520and%2520Michalis%2520Vazirgiannis%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520shown%2520great%2520promise%2520in%2520tasks%2520like%2520node%2520and%250Agraph%2520classification%252C%2520but%2520they%2520often%2520struggle%2520to%2520generalize%252C%2520particularly%2520to%250Aunseen%2520or%2520out-of-distribution%2520%2528OOD%2529%2520data.%2520These%2520challenges%2520are%2520exacerbated%2520when%250Atraining%2520data%2520is%2520limited%2520in%2520size%2520or%2520diversity.%2520To%2520address%2520these%2520issues%252C%2520we%250Aintroduce%2520a%2520theoretical%2520framework%2520using%2520Rademacher%2520complexity%2520to%2520compute%2520a%250Aregret%2520bound%2520on%2520the%2520generalization%2520error%2520and%2520then%2520characterize%2520the%2520effect%2520of%250Adata%2520augmentation.%2520This%2520framework%2520informs%2520the%2520design%2520of%2520GRATIN%252C%2520an%2520efficient%250Agraph%2520data%2520augmentation%2520algorithm%2520leveraging%2520the%2520capability%2520of%2520Gaussian%2520Mixture%250AModels%2520%2528GMMs%2529%2520to%2520approximate%2520any%2520distribution.%2520Our%2520approach%2520not%2520only%250Aoutperforms%2520existing%2520augmentation%2520techniques%2520in%2520terms%2520of%2520generalization%2520but%250Aalso%2520offers%2520improved%2520time%2520complexity%252C%2520making%2520it%2520highly%2520suitable%2520for%2520real-world%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08638v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Neural%20Network%20Generalization%20with%20Gaussian%20Mixture%20Model%20Based%0A%20%20Augmentation&entry.906535625=Yassine%20Abbahaddou%20and%20Fragkiskos%20D.%20Malliaros%20and%20Johannes%20F.%20Lutzeyer%20and%20Amine%20Mohamed%20Aboussalah%20and%20Michalis%20Vazirgiannis&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20shown%20great%20promise%20in%20tasks%20like%20node%20and%0Agraph%20classification%2C%20but%20they%20often%20struggle%20to%20generalize%2C%20particularly%20to%0Aunseen%20or%20out-of-distribution%20%28OOD%29%20data.%20These%20challenges%20are%20exacerbated%20when%0Atraining%20data%20is%20limited%20in%20size%20or%20diversity.%20To%20address%20these%20issues%2C%20we%0Aintroduce%20a%20theoretical%20framework%20using%20Rademacher%20complexity%20to%20compute%20a%0Aregret%20bound%20on%20the%20generalization%20error%20and%20then%20characterize%20the%20effect%20of%0Adata%20augmentation.%20This%20framework%20informs%20the%20design%20of%20GRATIN%2C%20an%20efficient%0Agraph%20data%20augmentation%20algorithm%20leveraging%20the%20capability%20of%20Gaussian%20Mixture%0AModels%20%28GMMs%29%20to%20approximate%20any%20distribution.%20Our%20approach%20not%20only%0Aoutperforms%20existing%20augmentation%20techniques%20in%20terms%20of%20generalization%20but%0Aalso%20offers%20improved%20time%20complexity%2C%20making%20it%20highly%20suitable%20for%20real-world%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08638v3&entry.124074799=Read"},
{"title": "RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration\n  with Content-Addressable Memory", "author": "Yi-Chun Liao and Chieh-Lin Tsai and Yuan-Hao Chang and Cam\u00e9lia Slimani and Jalil Boukhobza and Tei-Wei Kuo", "abstract": "  Although deep learning has demonstrated remarkable capabilities in learning\nfrom unstructured data, modern tree-based ensemble models remain superior in\nextracting relevant information and learning from structured datasets. While\nseveral efforts have been made to accelerate tree-based models, the inherent\ncharacteristics of the models pose significant challenges for conventional\naccelerators. Recent research leveraging content-addressable memory (CAM)\noffers a promising solution for accelerating tree-based models, yet existing\ndesigns suffer from excessive memory consumption and low utilization. This work\naddresses these challenges by introducing RETENTION, an end-to-end framework\nthat significantly reduces CAM capacity requirement for tree-based model\ninference. We propose an iterative pruning algorithm with a novel pruning\ncriterion tailored for bagging-based models (e.g., Random Forest), which\nminimizes model complexity while ensuring controlled accuracy degradation.\nAdditionally, we present a tree mapping scheme that incorporates two innovative\ndata placement strategies to alleviate the memory redundancy caused by the\nwidespread use of don't care states in CAM. Experimental results show that\nimplementing the tree mapping scheme alone achieves $1.46\\times$ to $21.30\n\\times$ better space efficiency, while the full RETENTION framework yields\n$4.35\\times$ to $207.12\\times$ improvement with less than 3% accuracy loss.\nThese results demonstrate that RETENTION is highly effective in reducing CAM\ncapacity requirement, providing a resource-efficient direction for tree-based\nmodel acceleration.\n", "link": "http://arxiv.org/abs/2506.05994v1", "date": "2025-06-06", "relevancy": 2.6274, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5365}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5287}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RETENTION%3A%20Resource-Efficient%20Tree-Based%20Ensemble%20Model%20Acceleration%0A%20%20with%20Content-Addressable%20Memory&body=Title%3A%20RETENTION%3A%20Resource-Efficient%20Tree-Based%20Ensemble%20Model%20Acceleration%0A%20%20with%20Content-Addressable%20Memory%0AAuthor%3A%20Yi-Chun%20Liao%20and%20Chieh-Lin%20Tsai%20and%20Yuan-Hao%20Chang%20and%20Cam%C3%A9lia%20Slimani%20and%20Jalil%20Boukhobza%20and%20Tei-Wei%20Kuo%0AAbstract%3A%20%20%20Although%20deep%20learning%20has%20demonstrated%20remarkable%20capabilities%20in%20learning%0Afrom%20unstructured%20data%2C%20modern%20tree-based%20ensemble%20models%20remain%20superior%20in%0Aextracting%20relevant%20information%20and%20learning%20from%20structured%20datasets.%20While%0Aseveral%20efforts%20have%20been%20made%20to%20accelerate%20tree-based%20models%2C%20the%20inherent%0Acharacteristics%20of%20the%20models%20pose%20significant%20challenges%20for%20conventional%0Aaccelerators.%20Recent%20research%20leveraging%20content-addressable%20memory%20%28CAM%29%0Aoffers%20a%20promising%20solution%20for%20accelerating%20tree-based%20models%2C%20yet%20existing%0Adesigns%20suffer%20from%20excessive%20memory%20consumption%20and%20low%20utilization.%20This%20work%0Aaddresses%20these%20challenges%20by%20introducing%20RETENTION%2C%20an%20end-to-end%20framework%0Athat%20significantly%20reduces%20CAM%20capacity%20requirement%20for%20tree-based%20model%0Ainference.%20We%20propose%20an%20iterative%20pruning%20algorithm%20with%20a%20novel%20pruning%0Acriterion%20tailored%20for%20bagging-based%20models%20%28e.g.%2C%20Random%20Forest%29%2C%20which%0Aminimizes%20model%20complexity%20while%20ensuring%20controlled%20accuracy%20degradation.%0AAdditionally%2C%20we%20present%20a%20tree%20mapping%20scheme%20that%20incorporates%20two%20innovative%0Adata%20placement%20strategies%20to%20alleviate%20the%20memory%20redundancy%20caused%20by%20the%0Awidespread%20use%20of%20don%27t%20care%20states%20in%20CAM.%20Experimental%20results%20show%20that%0Aimplementing%20the%20tree%20mapping%20scheme%20alone%20achieves%20%241.46%5Ctimes%24%20to%20%2421.30%0A%5Ctimes%24%20better%20space%20efficiency%2C%20while%20the%20full%20RETENTION%20framework%20yields%0A%244.35%5Ctimes%24%20to%20%24207.12%5Ctimes%24%20improvement%20with%20less%20than%203%25%20accuracy%20loss.%0AThese%20results%20demonstrate%20that%20RETENTION%20is%20highly%20effective%20in%20reducing%20CAM%0Acapacity%20requirement%2C%20providing%20a%20resource-efficient%20direction%20for%20tree-based%0Amodel%20acceleration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05994v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRETENTION%253A%2520Resource-Efficient%2520Tree-Based%2520Ensemble%2520Model%2520Acceleration%250A%2520%2520with%2520Content-Addressable%2520Memory%26entry.906535625%3DYi-Chun%2520Liao%2520and%2520Chieh-Lin%2520Tsai%2520and%2520Yuan-Hao%2520Chang%2520and%2520Cam%25C3%25A9lia%2520Slimani%2520and%2520Jalil%2520Boukhobza%2520and%2520Tei-Wei%2520Kuo%26entry.1292438233%3D%2520%2520Although%2520deep%2520learning%2520has%2520demonstrated%2520remarkable%2520capabilities%2520in%2520learning%250Afrom%2520unstructured%2520data%252C%2520modern%2520tree-based%2520ensemble%2520models%2520remain%2520superior%2520in%250Aextracting%2520relevant%2520information%2520and%2520learning%2520from%2520structured%2520datasets.%2520While%250Aseveral%2520efforts%2520have%2520been%2520made%2520to%2520accelerate%2520tree-based%2520models%252C%2520the%2520inherent%250Acharacteristics%2520of%2520the%2520models%2520pose%2520significant%2520challenges%2520for%2520conventional%250Aaccelerators.%2520Recent%2520research%2520leveraging%2520content-addressable%2520memory%2520%2528CAM%2529%250Aoffers%2520a%2520promising%2520solution%2520for%2520accelerating%2520tree-based%2520models%252C%2520yet%2520existing%250Adesigns%2520suffer%2520from%2520excessive%2520memory%2520consumption%2520and%2520low%2520utilization.%2520This%2520work%250Aaddresses%2520these%2520challenges%2520by%2520introducing%2520RETENTION%252C%2520an%2520end-to-end%2520framework%250Athat%2520significantly%2520reduces%2520CAM%2520capacity%2520requirement%2520for%2520tree-based%2520model%250Ainference.%2520We%2520propose%2520an%2520iterative%2520pruning%2520algorithm%2520with%2520a%2520novel%2520pruning%250Acriterion%2520tailored%2520for%2520bagging-based%2520models%2520%2528e.g.%252C%2520Random%2520Forest%2529%252C%2520which%250Aminimizes%2520model%2520complexity%2520while%2520ensuring%2520controlled%2520accuracy%2520degradation.%250AAdditionally%252C%2520we%2520present%2520a%2520tree%2520mapping%2520scheme%2520that%2520incorporates%2520two%2520innovative%250Adata%2520placement%2520strategies%2520to%2520alleviate%2520the%2520memory%2520redundancy%2520caused%2520by%2520the%250Awidespread%2520use%2520of%2520don%2527t%2520care%2520states%2520in%2520CAM.%2520Experimental%2520results%2520show%2520that%250Aimplementing%2520the%2520tree%2520mapping%2520scheme%2520alone%2520achieves%2520%25241.46%255Ctimes%2524%2520to%2520%252421.30%250A%255Ctimes%2524%2520better%2520space%2520efficiency%252C%2520while%2520the%2520full%2520RETENTION%2520framework%2520yields%250A%25244.35%255Ctimes%2524%2520to%2520%2524207.12%255Ctimes%2524%2520improvement%2520with%2520less%2520than%25203%2525%2520accuracy%2520loss.%250AThese%2520results%2520demonstrate%2520that%2520RETENTION%2520is%2520highly%2520effective%2520in%2520reducing%2520CAM%250Acapacity%2520requirement%252C%2520providing%2520a%2520resource-efficient%2520direction%2520for%2520tree-based%250Amodel%2520acceleration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05994v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RETENTION%3A%20Resource-Efficient%20Tree-Based%20Ensemble%20Model%20Acceleration%0A%20%20with%20Content-Addressable%20Memory&entry.906535625=Yi-Chun%20Liao%20and%20Chieh-Lin%20Tsai%20and%20Yuan-Hao%20Chang%20and%20Cam%C3%A9lia%20Slimani%20and%20Jalil%20Boukhobza%20and%20Tei-Wei%20Kuo&entry.1292438233=%20%20Although%20deep%20learning%20has%20demonstrated%20remarkable%20capabilities%20in%20learning%0Afrom%20unstructured%20data%2C%20modern%20tree-based%20ensemble%20models%20remain%20superior%20in%0Aextracting%20relevant%20information%20and%20learning%20from%20structured%20datasets.%20While%0Aseveral%20efforts%20have%20been%20made%20to%20accelerate%20tree-based%20models%2C%20the%20inherent%0Acharacteristics%20of%20the%20models%20pose%20significant%20challenges%20for%20conventional%0Aaccelerators.%20Recent%20research%20leveraging%20content-addressable%20memory%20%28CAM%29%0Aoffers%20a%20promising%20solution%20for%20accelerating%20tree-based%20models%2C%20yet%20existing%0Adesigns%20suffer%20from%20excessive%20memory%20consumption%20and%20low%20utilization.%20This%20work%0Aaddresses%20these%20challenges%20by%20introducing%20RETENTION%2C%20an%20end-to-end%20framework%0Athat%20significantly%20reduces%20CAM%20capacity%20requirement%20for%20tree-based%20model%0Ainference.%20We%20propose%20an%20iterative%20pruning%20algorithm%20with%20a%20novel%20pruning%0Acriterion%20tailored%20for%20bagging-based%20models%20%28e.g.%2C%20Random%20Forest%29%2C%20which%0Aminimizes%20model%20complexity%20while%20ensuring%20controlled%20accuracy%20degradation.%0AAdditionally%2C%20we%20present%20a%20tree%20mapping%20scheme%20that%20incorporates%20two%20innovative%0Adata%20placement%20strategies%20to%20alleviate%20the%20memory%20redundancy%20caused%20by%20the%0Awidespread%20use%20of%20don%27t%20care%20states%20in%20CAM.%20Experimental%20results%20show%20that%0Aimplementing%20the%20tree%20mapping%20scheme%20alone%20achieves%20%241.46%5Ctimes%24%20to%20%2421.30%0A%5Ctimes%24%20better%20space%20efficiency%2C%20while%20the%20full%20RETENTION%20framework%20yields%0A%244.35%5Ctimes%24%20to%20%24207.12%5Ctimes%24%20improvement%20with%20less%20than%203%25%20accuracy%20loss.%0AThese%20results%20demonstrate%20that%20RETENTION%20is%20highly%20effective%20in%20reducing%20CAM%0Acapacity%20requirement%2C%20providing%20a%20resource-efficient%20direction%20for%20tree-based%0Amodel%20acceleration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05994v1&entry.124074799=Read"},
{"title": "Assessing Intersectional Bias in Representations of Pre-Trained Image\n  Recognition Models", "author": "Valerie Krug and Sebastian Stober", "abstract": "  Deep Learning models have achieved remarkable success. Training them is often\naccelerated by building on top of pre-trained models which poses the risk of\nperpetuating encoded biases. Here, we investigate biases in the representations\nof commonly used ImageNet classifiers for facial images while considering\nintersections of sensitive variables age, race and gender. To assess the\nbiases, we use linear classifier probes and visualize activations as\ntopographic maps. We find that representations in ImageNet classifiers\nparticularly allow differentiation between ages. Less strongly pronounced, the\nmodels appear to associate certain ethnicities and distinguish genders in\nmiddle-aged groups.\n", "link": "http://arxiv.org/abs/2506.03664v2", "date": "2025-06-06", "relevancy": 2.6087, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5276}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5276}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.51}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assessing%20Intersectional%20Bias%20in%20Representations%20of%20Pre-Trained%20Image%0A%20%20Recognition%20Models&body=Title%3A%20Assessing%20Intersectional%20Bias%20in%20Representations%20of%20Pre-Trained%20Image%0A%20%20Recognition%20Models%0AAuthor%3A%20Valerie%20Krug%20and%20Sebastian%20Stober%0AAbstract%3A%20%20%20Deep%20Learning%20models%20have%20achieved%20remarkable%20success.%20Training%20them%20is%20often%0Aaccelerated%20by%20building%20on%20top%20of%20pre-trained%20models%20which%20poses%20the%20risk%20of%0Aperpetuating%20encoded%20biases.%20Here%2C%20we%20investigate%20biases%20in%20the%20representations%0Aof%20commonly%20used%20ImageNet%20classifiers%20for%20facial%20images%20while%20considering%0Aintersections%20of%20sensitive%20variables%20age%2C%20race%20and%20gender.%20To%20assess%20the%0Abiases%2C%20we%20use%20linear%20classifier%20probes%20and%20visualize%20activations%20as%0Atopographic%20maps.%20We%20find%20that%20representations%20in%20ImageNet%20classifiers%0Aparticularly%20allow%20differentiation%20between%20ages.%20Less%20strongly%20pronounced%2C%20the%0Amodels%20appear%20to%20associate%20certain%20ethnicities%20and%20distinguish%20genders%20in%0Amiddle-aged%20groups.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03664v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssessing%2520Intersectional%2520Bias%2520in%2520Representations%2520of%2520Pre-Trained%2520Image%250A%2520%2520Recognition%2520Models%26entry.906535625%3DValerie%2520Krug%2520and%2520Sebastian%2520Stober%26entry.1292438233%3D%2520%2520Deep%2520Learning%2520models%2520have%2520achieved%2520remarkable%2520success.%2520Training%2520them%2520is%2520often%250Aaccelerated%2520by%2520building%2520on%2520top%2520of%2520pre-trained%2520models%2520which%2520poses%2520the%2520risk%2520of%250Aperpetuating%2520encoded%2520biases.%2520Here%252C%2520we%2520investigate%2520biases%2520in%2520the%2520representations%250Aof%2520commonly%2520used%2520ImageNet%2520classifiers%2520for%2520facial%2520images%2520while%2520considering%250Aintersections%2520of%2520sensitive%2520variables%2520age%252C%2520race%2520and%2520gender.%2520To%2520assess%2520the%250Abiases%252C%2520we%2520use%2520linear%2520classifier%2520probes%2520and%2520visualize%2520activations%2520as%250Atopographic%2520maps.%2520We%2520find%2520that%2520representations%2520in%2520ImageNet%2520classifiers%250Aparticularly%2520allow%2520differentiation%2520between%2520ages.%2520Less%2520strongly%2520pronounced%252C%2520the%250Amodels%2520appear%2520to%2520associate%2520certain%2520ethnicities%2520and%2520distinguish%2520genders%2520in%250Amiddle-aged%2520groups.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03664v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20Intersectional%20Bias%20in%20Representations%20of%20Pre-Trained%20Image%0A%20%20Recognition%20Models&entry.906535625=Valerie%20Krug%20and%20Sebastian%20Stober&entry.1292438233=%20%20Deep%20Learning%20models%20have%20achieved%20remarkable%20success.%20Training%20them%20is%20often%0Aaccelerated%20by%20building%20on%20top%20of%20pre-trained%20models%20which%20poses%20the%20risk%20of%0Aperpetuating%20encoded%20biases.%20Here%2C%20we%20investigate%20biases%20in%20the%20representations%0Aof%20commonly%20used%20ImageNet%20classifiers%20for%20facial%20images%20while%20considering%0Aintersections%20of%20sensitive%20variables%20age%2C%20race%20and%20gender.%20To%20assess%20the%0Abiases%2C%20we%20use%20linear%20classifier%20probes%20and%20visualize%20activations%20as%0Atopographic%20maps.%20We%20find%20that%20representations%20in%20ImageNet%20classifiers%0Aparticularly%20allow%20differentiation%20between%20ages.%20Less%20strongly%20pronounced%2C%20the%0Amodels%20appear%20to%20associate%20certain%20ethnicities%20and%20distinguish%20genders%20in%0Amiddle-aged%20groups.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03664v2&entry.124074799=Read"},
{"title": "Token Signature: Predicting Chain-of-Thought Gains with Token Decoding\n  Feature in Large Language Models", "author": "Peijie Liu and Fengli Xu and Yong Li", "abstract": "  Chain-of-Thought (CoT) technique has proven effective in improving the\nperformance of large language models (LLMs) on complex reasoning tasks.\nHowever, the performance gains are inconsistent across different tasks, and the\nunderlying mechanism remains a long-standing research question. In this work,\nwe make a preliminary observation that the monotonicity of token probability\ndistributions may be correlated with the gains achieved through CoT reasoning.\nLeveraging this insight, we propose two indicators based on the token\nprobability distribution to assess CoT effectiveness across different tasks. By\ncombining instance-level indicators with logistic regression model, we\nintroduce Dynamic CoT, a method that dynamically select between CoT and direct\nanswer. Furthermore, we extend Dynamic CoT to closed-source models by\ntransferring decision strategies learned from open-source models. Our\nindicators for assessing CoT effectiveness achieve an accuracy of 89.2\\%, and\nDynamic CoT reduces token consumption by more than 35\\% while maintaining high\naccuracy. Overall, our work offers a novel perspective on the underlying\nmechanisms of CoT reasoning and provides a framework for its more efficient\ndeployment.\n", "link": "http://arxiv.org/abs/2506.06008v1", "date": "2025-06-06", "relevancy": 2.6082, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.565}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Token%20Signature%3A%20Predicting%20Chain-of-Thought%20Gains%20with%20Token%20Decoding%0A%20%20Feature%20in%20Large%20Language%20Models&body=Title%3A%20Token%20Signature%3A%20Predicting%20Chain-of-Thought%20Gains%20with%20Token%20Decoding%0A%20%20Feature%20in%20Large%20Language%20Models%0AAuthor%3A%20Peijie%20Liu%20and%20Fengli%20Xu%20and%20Yong%20Li%0AAbstract%3A%20%20%20Chain-of-Thought%20%28CoT%29%20technique%20has%20proven%20effective%20in%20improving%20the%0Aperformance%20of%20large%20language%20models%20%28LLMs%29%20on%20complex%20reasoning%20tasks.%0AHowever%2C%20the%20performance%20gains%20are%20inconsistent%20across%20different%20tasks%2C%20and%20the%0Aunderlying%20mechanism%20remains%20a%20long-standing%20research%20question.%20In%20this%20work%2C%0Awe%20make%20a%20preliminary%20observation%20that%20the%20monotonicity%20of%20token%20probability%0Adistributions%20may%20be%20correlated%20with%20the%20gains%20achieved%20through%20CoT%20reasoning.%0ALeveraging%20this%20insight%2C%20we%20propose%20two%20indicators%20based%20on%20the%20token%0Aprobability%20distribution%20to%20assess%20CoT%20effectiveness%20across%20different%20tasks.%20By%0Acombining%20instance-level%20indicators%20with%20logistic%20regression%20model%2C%20we%0Aintroduce%20Dynamic%20CoT%2C%20a%20method%20that%20dynamically%20select%20between%20CoT%20and%20direct%0Aanswer.%20Furthermore%2C%20we%20extend%20Dynamic%20CoT%20to%20closed-source%20models%20by%0Atransferring%20decision%20strategies%20learned%20from%20open-source%20models.%20Our%0Aindicators%20for%20assessing%20CoT%20effectiveness%20achieve%20an%20accuracy%20of%2089.2%5C%25%2C%20and%0ADynamic%20CoT%20reduces%20token%20consumption%20by%20more%20than%2035%5C%25%20while%20maintaining%20high%0Aaccuracy.%20Overall%2C%20our%20work%20offers%20a%20novel%20perspective%20on%20the%20underlying%0Amechanisms%20of%20CoT%20reasoning%20and%20provides%20a%20framework%20for%20its%20more%20efficient%0Adeployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06008v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToken%2520Signature%253A%2520Predicting%2520Chain-of-Thought%2520Gains%2520with%2520Token%2520Decoding%250A%2520%2520Feature%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DPeijie%2520Liu%2520and%2520Fengli%2520Xu%2520and%2520Yong%2520Li%26entry.1292438233%3D%2520%2520Chain-of-Thought%2520%2528CoT%2529%2520technique%2520has%2520proven%2520effective%2520in%2520improving%2520the%250Aperformance%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520on%2520complex%2520reasoning%2520tasks.%250AHowever%252C%2520the%2520performance%2520gains%2520are%2520inconsistent%2520across%2520different%2520tasks%252C%2520and%2520the%250Aunderlying%2520mechanism%2520remains%2520a%2520long-standing%2520research%2520question.%2520In%2520this%2520work%252C%250Awe%2520make%2520a%2520preliminary%2520observation%2520that%2520the%2520monotonicity%2520of%2520token%2520probability%250Adistributions%2520may%2520be%2520correlated%2520with%2520the%2520gains%2520achieved%2520through%2520CoT%2520reasoning.%250ALeveraging%2520this%2520insight%252C%2520we%2520propose%2520two%2520indicators%2520based%2520on%2520the%2520token%250Aprobability%2520distribution%2520to%2520assess%2520CoT%2520effectiveness%2520across%2520different%2520tasks.%2520By%250Acombining%2520instance-level%2520indicators%2520with%2520logistic%2520regression%2520model%252C%2520we%250Aintroduce%2520Dynamic%2520CoT%252C%2520a%2520method%2520that%2520dynamically%2520select%2520between%2520CoT%2520and%2520direct%250Aanswer.%2520Furthermore%252C%2520we%2520extend%2520Dynamic%2520CoT%2520to%2520closed-source%2520models%2520by%250Atransferring%2520decision%2520strategies%2520learned%2520from%2520open-source%2520models.%2520Our%250Aindicators%2520for%2520assessing%2520CoT%2520effectiveness%2520achieve%2520an%2520accuracy%2520of%252089.2%255C%2525%252C%2520and%250ADynamic%2520CoT%2520reduces%2520token%2520consumption%2520by%2520more%2520than%252035%255C%2525%2520while%2520maintaining%2520high%250Aaccuracy.%2520Overall%252C%2520our%2520work%2520offers%2520a%2520novel%2520perspective%2520on%2520the%2520underlying%250Amechanisms%2520of%2520CoT%2520reasoning%2520and%2520provides%2520a%2520framework%2520for%2520its%2520more%2520efficient%250Adeployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06008v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Token%20Signature%3A%20Predicting%20Chain-of-Thought%20Gains%20with%20Token%20Decoding%0A%20%20Feature%20in%20Large%20Language%20Models&entry.906535625=Peijie%20Liu%20and%20Fengli%20Xu%20and%20Yong%20Li&entry.1292438233=%20%20Chain-of-Thought%20%28CoT%29%20technique%20has%20proven%20effective%20in%20improving%20the%0Aperformance%20of%20large%20language%20models%20%28LLMs%29%20on%20complex%20reasoning%20tasks.%0AHowever%2C%20the%20performance%20gains%20are%20inconsistent%20across%20different%20tasks%2C%20and%20the%0Aunderlying%20mechanism%20remains%20a%20long-standing%20research%20question.%20In%20this%20work%2C%0Awe%20make%20a%20preliminary%20observation%20that%20the%20monotonicity%20of%20token%20probability%0Adistributions%20may%20be%20correlated%20with%20the%20gains%20achieved%20through%20CoT%20reasoning.%0ALeveraging%20this%20insight%2C%20we%20propose%20two%20indicators%20based%20on%20the%20token%0Aprobability%20distribution%20to%20assess%20CoT%20effectiveness%20across%20different%20tasks.%20By%0Acombining%20instance-level%20indicators%20with%20logistic%20regression%20model%2C%20we%0Aintroduce%20Dynamic%20CoT%2C%20a%20method%20that%20dynamically%20select%20between%20CoT%20and%20direct%0Aanswer.%20Furthermore%2C%20we%20extend%20Dynamic%20CoT%20to%20closed-source%20models%20by%0Atransferring%20decision%20strategies%20learned%20from%20open-source%20models.%20Our%0Aindicators%20for%20assessing%20CoT%20effectiveness%20achieve%20an%20accuracy%20of%2089.2%5C%25%2C%20and%0ADynamic%20CoT%20reduces%20token%20consumption%20by%20more%20than%2035%5C%25%20while%20maintaining%20high%0Aaccuracy.%20Overall%2C%20our%20work%20offers%20a%20novel%20perspective%20on%20the%20underlying%0Amechanisms%20of%20CoT%20reasoning%20and%20provides%20a%20framework%20for%20its%20more%20efficient%0Adeployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06008v1&entry.124074799=Read"},
{"title": "Unlocking Recursive Thinking of LLMs: Alignment via Refinement", "author": "Haoke Zhang and Xiaobo Liang and Cunxiang Wang and Juntao Li and Min Zhang", "abstract": "  The OpenAI o1-series models have demonstrated that leveraging long-form Chain\nof Thought (CoT) can substantially enhance performance. However, the recursive\nthinking capabilities of Large Language Models (LLMs) remain limited,\nparticularly in the absence of expert-curated data for distillation. In this\npaper, we propose \\textbf{AvR}: \\textbf{Alignment via Refinement}, a novel\nmethod aimed at unlocking the potential of LLMs for recursive reasoning through\nlong-form CoT. AvR introduces a refinement process that integrates criticism\nand improvement actions, guided by differentiable learning techniques to\noptimize \\textbf{refinement-aware rewards}. As a result, the synthesized\nmulti-round data can be organized as a long refinement thought, further\nenabling test-time scaling. Experimental results show that AvR significantly\noutperforms conventional preference optimization methods. Notably, with only 3k\nsynthetic samples, our method boosts the performance of the LLaMA-3-8B-Instruct\nmodel by over 20\\% in win rate on AlpacaEval 2.0. Our code is available at\nGithub (https://github.com/Banner-Z/AvR.git).\n", "link": "http://arxiv.org/abs/2506.06009v1", "date": "2025-06-06", "relevancy": 2.5661, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.518}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5108}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unlocking%20Recursive%20Thinking%20of%20LLMs%3A%20Alignment%20via%20Refinement&body=Title%3A%20Unlocking%20Recursive%20Thinking%20of%20LLMs%3A%20Alignment%20via%20Refinement%0AAuthor%3A%20Haoke%20Zhang%20and%20Xiaobo%20Liang%20and%20Cunxiang%20Wang%20and%20Juntao%20Li%20and%20Min%20Zhang%0AAbstract%3A%20%20%20The%20OpenAI%20o1-series%20models%20have%20demonstrated%20that%20leveraging%20long-form%20Chain%0Aof%20Thought%20%28CoT%29%20can%20substantially%20enhance%20performance.%20However%2C%20the%20recursive%0Athinking%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%20remain%20limited%2C%0Aparticularly%20in%20the%20absence%20of%20expert-curated%20data%20for%20distillation.%20In%20this%0Apaper%2C%20we%20propose%20%5Ctextbf%7BAvR%7D%3A%20%5Ctextbf%7BAlignment%20via%20Refinement%7D%2C%20a%20novel%0Amethod%20aimed%20at%20unlocking%20the%20potential%20of%20LLMs%20for%20recursive%20reasoning%20through%0Along-form%20CoT.%20AvR%20introduces%20a%20refinement%20process%20that%20integrates%20criticism%0Aand%20improvement%20actions%2C%20guided%20by%20differentiable%20learning%20techniques%20to%0Aoptimize%20%5Ctextbf%7Brefinement-aware%20rewards%7D.%20As%20a%20result%2C%20the%20synthesized%0Amulti-round%20data%20can%20be%20organized%20as%20a%20long%20refinement%20thought%2C%20further%0Aenabling%20test-time%20scaling.%20Experimental%20results%20show%20that%20AvR%20significantly%0Aoutperforms%20conventional%20preference%20optimization%20methods.%20Notably%2C%20with%20only%203k%0Asynthetic%20samples%2C%20our%20method%20boosts%20the%20performance%20of%20the%20LLaMA-3-8B-Instruct%0Amodel%20by%20over%2020%5C%25%20in%20win%20rate%20on%20AlpacaEval%202.0.%20Our%20code%20is%20available%20at%0AGithub%20%28https%3A//github.com/Banner-Z/AvR.git%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06009v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnlocking%2520Recursive%2520Thinking%2520of%2520LLMs%253A%2520Alignment%2520via%2520Refinement%26entry.906535625%3DHaoke%2520Zhang%2520and%2520Xiaobo%2520Liang%2520and%2520Cunxiang%2520Wang%2520and%2520Juntao%2520Li%2520and%2520Min%2520Zhang%26entry.1292438233%3D%2520%2520The%2520OpenAI%2520o1-series%2520models%2520have%2520demonstrated%2520that%2520leveraging%2520long-form%2520Chain%250Aof%2520Thought%2520%2528CoT%2529%2520can%2520substantially%2520enhance%2520performance.%2520However%252C%2520the%2520recursive%250Athinking%2520capabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520remain%2520limited%252C%250Aparticularly%2520in%2520the%2520absence%2520of%2520expert-curated%2520data%2520for%2520distillation.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520%255Ctextbf%257BAvR%257D%253A%2520%255Ctextbf%257BAlignment%2520via%2520Refinement%257D%252C%2520a%2520novel%250Amethod%2520aimed%2520at%2520unlocking%2520the%2520potential%2520of%2520LLMs%2520for%2520recursive%2520reasoning%2520through%250Along-form%2520CoT.%2520AvR%2520introduces%2520a%2520refinement%2520process%2520that%2520integrates%2520criticism%250Aand%2520improvement%2520actions%252C%2520guided%2520by%2520differentiable%2520learning%2520techniques%2520to%250Aoptimize%2520%255Ctextbf%257Brefinement-aware%2520rewards%257D.%2520As%2520a%2520result%252C%2520the%2520synthesized%250Amulti-round%2520data%2520can%2520be%2520organized%2520as%2520a%2520long%2520refinement%2520thought%252C%2520further%250Aenabling%2520test-time%2520scaling.%2520Experimental%2520results%2520show%2520that%2520AvR%2520significantly%250Aoutperforms%2520conventional%2520preference%2520optimization%2520methods.%2520Notably%252C%2520with%2520only%25203k%250Asynthetic%2520samples%252C%2520our%2520method%2520boosts%2520the%2520performance%2520of%2520the%2520LLaMA-3-8B-Instruct%250Amodel%2520by%2520over%252020%255C%2525%2520in%2520win%2520rate%2520on%2520AlpacaEval%25202.0.%2520Our%2520code%2520is%2520available%2520at%250AGithub%2520%2528https%253A//github.com/Banner-Z/AvR.git%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06009v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unlocking%20Recursive%20Thinking%20of%20LLMs%3A%20Alignment%20via%20Refinement&entry.906535625=Haoke%20Zhang%20and%20Xiaobo%20Liang%20and%20Cunxiang%20Wang%20and%20Juntao%20Li%20and%20Min%20Zhang&entry.1292438233=%20%20The%20OpenAI%20o1-series%20models%20have%20demonstrated%20that%20leveraging%20long-form%20Chain%0Aof%20Thought%20%28CoT%29%20can%20substantially%20enhance%20performance.%20However%2C%20the%20recursive%0Athinking%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%20remain%20limited%2C%0Aparticularly%20in%20the%20absence%20of%20expert-curated%20data%20for%20distillation.%20In%20this%0Apaper%2C%20we%20propose%20%5Ctextbf%7BAvR%7D%3A%20%5Ctextbf%7BAlignment%20via%20Refinement%7D%2C%20a%20novel%0Amethod%20aimed%20at%20unlocking%20the%20potential%20of%20LLMs%20for%20recursive%20reasoning%20through%0Along-form%20CoT.%20AvR%20introduces%20a%20refinement%20process%20that%20integrates%20criticism%0Aand%20improvement%20actions%2C%20guided%20by%20differentiable%20learning%20techniques%20to%0Aoptimize%20%5Ctextbf%7Brefinement-aware%20rewards%7D.%20As%20a%20result%2C%20the%20synthesized%0Amulti-round%20data%20can%20be%20organized%20as%20a%20long%20refinement%20thought%2C%20further%0Aenabling%20test-time%20scaling.%20Experimental%20results%20show%20that%20AvR%20significantly%0Aoutperforms%20conventional%20preference%20optimization%20methods.%20Notably%2C%20with%20only%203k%0Asynthetic%20samples%2C%20our%20method%20boosts%20the%20performance%20of%20the%20LLaMA-3-8B-Instruct%0Amodel%20by%20over%2020%5C%25%20in%20win%20rate%20on%20AlpacaEval%202.0.%20Our%20code%20is%20available%20at%0AGithub%20%28https%3A//github.com/Banner-Z/AvR.git%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06009v1&entry.124074799=Read"},
{"title": "Domain Adaptation in Agricultural Image Analysis: A Comprehensive Review\n  from Shallow Models to Deep Learning", "author": "Xing Hu and Siyuan Chen and Dawei Zhang", "abstract": "  With the increasing use of computer vision in agriculture, image analysis has\nbecome crucial for tasks like crop health monitoring and pest detection.\nHowever, significant domain shifts between source and target domains-due to\nenvironmental differences, crop types, and data acquisition methods-pose\nchallenges. These domain gaps limit the ability of models to generalize across\nregions, seasons, and complex agricultural environments. This paper explores\nhow Domain Adaptation (DA) techniques can address these challenges, focusing on\ntheir role in enhancing the cross-domain transferability of agricultural image\nanalysis. DA has gained attention in agricultural vision tasks due to its\npotential to mitigate domain heterogeneity. The paper systematically reviews\nrecent advances in DA for agricultural imagery, particularly its practical\napplications in complex agricultural environments. We examine the key drivers\nfor adopting DA in agriculture, such as limited labeled data, weak model\ntransferability, and dynamic environmental conditions. We also discuss its use\nin crop health monitoring, pest detection, and fruit recognition, highlighting\nimprovements in performance across regions and seasons. The paper categorizes\nDA methods into shallow and deep learning models, with further divisions into\nsupervised, semi-supervised, and unsupervised approaches. A special focus is\ngiven to adversarial learning-based DA methods, which have shown great promise\nin challenging agricultural scenarios. Finally, we review key public datasets\nin agricultural imagery, analyzing their value and limitations in DA research.\nThis review provides a comprehensive framework for researchers, offering\ninsights into current research gaps and supporting the advancement of DA\nmethods in agricultural image analysis.\n", "link": "http://arxiv.org/abs/2506.05972v1", "date": "2025-06-06", "relevancy": 2.562, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5125}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5125}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain%20Adaptation%20in%20Agricultural%20Image%20Analysis%3A%20A%20Comprehensive%20Review%0A%20%20from%20Shallow%20Models%20to%20Deep%20Learning&body=Title%3A%20Domain%20Adaptation%20in%20Agricultural%20Image%20Analysis%3A%20A%20Comprehensive%20Review%0A%20%20from%20Shallow%20Models%20to%20Deep%20Learning%0AAuthor%3A%20Xing%20Hu%20and%20Siyuan%20Chen%20and%20Dawei%20Zhang%0AAbstract%3A%20%20%20With%20the%20increasing%20use%20of%20computer%20vision%20in%20agriculture%2C%20image%20analysis%20has%0Abecome%20crucial%20for%20tasks%20like%20crop%20health%20monitoring%20and%20pest%20detection.%0AHowever%2C%20significant%20domain%20shifts%20between%20source%20and%20target%20domains-due%20to%0Aenvironmental%20differences%2C%20crop%20types%2C%20and%20data%20acquisition%20methods-pose%0Achallenges.%20These%20domain%20gaps%20limit%20the%20ability%20of%20models%20to%20generalize%20across%0Aregions%2C%20seasons%2C%20and%20complex%20agricultural%20environments.%20This%20paper%20explores%0Ahow%20Domain%20Adaptation%20%28DA%29%20techniques%20can%20address%20these%20challenges%2C%20focusing%20on%0Atheir%20role%20in%20enhancing%20the%20cross-domain%20transferability%20of%20agricultural%20image%0Aanalysis.%20DA%20has%20gained%20attention%20in%20agricultural%20vision%20tasks%20due%20to%20its%0Apotential%20to%20mitigate%20domain%20heterogeneity.%20The%20paper%20systematically%20reviews%0Arecent%20advances%20in%20DA%20for%20agricultural%20imagery%2C%20particularly%20its%20practical%0Aapplications%20in%20complex%20agricultural%20environments.%20We%20examine%20the%20key%20drivers%0Afor%20adopting%20DA%20in%20agriculture%2C%20such%20as%20limited%20labeled%20data%2C%20weak%20model%0Atransferability%2C%20and%20dynamic%20environmental%20conditions.%20We%20also%20discuss%20its%20use%0Ain%20crop%20health%20monitoring%2C%20pest%20detection%2C%20and%20fruit%20recognition%2C%20highlighting%0Aimprovements%20in%20performance%20across%20regions%20and%20seasons.%20The%20paper%20categorizes%0ADA%20methods%20into%20shallow%20and%20deep%20learning%20models%2C%20with%20further%20divisions%20into%0Asupervised%2C%20semi-supervised%2C%20and%20unsupervised%20approaches.%20A%20special%20focus%20is%0Agiven%20to%20adversarial%20learning-based%20DA%20methods%2C%20which%20have%20shown%20great%20promise%0Ain%20challenging%20agricultural%20scenarios.%20Finally%2C%20we%20review%20key%20public%20datasets%0Ain%20agricultural%20imagery%2C%20analyzing%20their%20value%20and%20limitations%20in%20DA%20research.%0AThis%20review%20provides%20a%20comprehensive%20framework%20for%20researchers%2C%20offering%0Ainsights%20into%20current%20research%20gaps%20and%20supporting%20the%20advancement%20of%20DA%0Amethods%20in%20agricultural%20image%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05972v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain%2520Adaptation%2520in%2520Agricultural%2520Image%2520Analysis%253A%2520A%2520Comprehensive%2520Review%250A%2520%2520from%2520Shallow%2520Models%2520to%2520Deep%2520Learning%26entry.906535625%3DXing%2520Hu%2520and%2520Siyuan%2520Chen%2520and%2520Dawei%2520Zhang%26entry.1292438233%3D%2520%2520With%2520the%2520increasing%2520use%2520of%2520computer%2520vision%2520in%2520agriculture%252C%2520image%2520analysis%2520has%250Abecome%2520crucial%2520for%2520tasks%2520like%2520crop%2520health%2520monitoring%2520and%2520pest%2520detection.%250AHowever%252C%2520significant%2520domain%2520shifts%2520between%2520source%2520and%2520target%2520domains-due%2520to%250Aenvironmental%2520differences%252C%2520crop%2520types%252C%2520and%2520data%2520acquisition%2520methods-pose%250Achallenges.%2520These%2520domain%2520gaps%2520limit%2520the%2520ability%2520of%2520models%2520to%2520generalize%2520across%250Aregions%252C%2520seasons%252C%2520and%2520complex%2520agricultural%2520environments.%2520This%2520paper%2520explores%250Ahow%2520Domain%2520Adaptation%2520%2528DA%2529%2520techniques%2520can%2520address%2520these%2520challenges%252C%2520focusing%2520on%250Atheir%2520role%2520in%2520enhancing%2520the%2520cross-domain%2520transferability%2520of%2520agricultural%2520image%250Aanalysis.%2520DA%2520has%2520gained%2520attention%2520in%2520agricultural%2520vision%2520tasks%2520due%2520to%2520its%250Apotential%2520to%2520mitigate%2520domain%2520heterogeneity.%2520The%2520paper%2520systematically%2520reviews%250Arecent%2520advances%2520in%2520DA%2520for%2520agricultural%2520imagery%252C%2520particularly%2520its%2520practical%250Aapplications%2520in%2520complex%2520agricultural%2520environments.%2520We%2520examine%2520the%2520key%2520drivers%250Afor%2520adopting%2520DA%2520in%2520agriculture%252C%2520such%2520as%2520limited%2520labeled%2520data%252C%2520weak%2520model%250Atransferability%252C%2520and%2520dynamic%2520environmental%2520conditions.%2520We%2520also%2520discuss%2520its%2520use%250Ain%2520crop%2520health%2520monitoring%252C%2520pest%2520detection%252C%2520and%2520fruit%2520recognition%252C%2520highlighting%250Aimprovements%2520in%2520performance%2520across%2520regions%2520and%2520seasons.%2520The%2520paper%2520categorizes%250ADA%2520methods%2520into%2520shallow%2520and%2520deep%2520learning%2520models%252C%2520with%2520further%2520divisions%2520into%250Asupervised%252C%2520semi-supervised%252C%2520and%2520unsupervised%2520approaches.%2520A%2520special%2520focus%2520is%250Agiven%2520to%2520adversarial%2520learning-based%2520DA%2520methods%252C%2520which%2520have%2520shown%2520great%2520promise%250Ain%2520challenging%2520agricultural%2520scenarios.%2520Finally%252C%2520we%2520review%2520key%2520public%2520datasets%250Ain%2520agricultural%2520imagery%252C%2520analyzing%2520their%2520value%2520and%2520limitations%2520in%2520DA%2520research.%250AThis%2520review%2520provides%2520a%2520comprehensive%2520framework%2520for%2520researchers%252C%2520offering%250Ainsights%2520into%2520current%2520research%2520gaps%2520and%2520supporting%2520the%2520advancement%2520of%2520DA%250Amethods%2520in%2520agricultural%2520image%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05972v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain%20Adaptation%20in%20Agricultural%20Image%20Analysis%3A%20A%20Comprehensive%20Review%0A%20%20from%20Shallow%20Models%20to%20Deep%20Learning&entry.906535625=Xing%20Hu%20and%20Siyuan%20Chen%20and%20Dawei%20Zhang&entry.1292438233=%20%20With%20the%20increasing%20use%20of%20computer%20vision%20in%20agriculture%2C%20image%20analysis%20has%0Abecome%20crucial%20for%20tasks%20like%20crop%20health%20monitoring%20and%20pest%20detection.%0AHowever%2C%20significant%20domain%20shifts%20between%20source%20and%20target%20domains-due%20to%0Aenvironmental%20differences%2C%20crop%20types%2C%20and%20data%20acquisition%20methods-pose%0Achallenges.%20These%20domain%20gaps%20limit%20the%20ability%20of%20models%20to%20generalize%20across%0Aregions%2C%20seasons%2C%20and%20complex%20agricultural%20environments.%20This%20paper%20explores%0Ahow%20Domain%20Adaptation%20%28DA%29%20techniques%20can%20address%20these%20challenges%2C%20focusing%20on%0Atheir%20role%20in%20enhancing%20the%20cross-domain%20transferability%20of%20agricultural%20image%0Aanalysis.%20DA%20has%20gained%20attention%20in%20agricultural%20vision%20tasks%20due%20to%20its%0Apotential%20to%20mitigate%20domain%20heterogeneity.%20The%20paper%20systematically%20reviews%0Arecent%20advances%20in%20DA%20for%20agricultural%20imagery%2C%20particularly%20its%20practical%0Aapplications%20in%20complex%20agricultural%20environments.%20We%20examine%20the%20key%20drivers%0Afor%20adopting%20DA%20in%20agriculture%2C%20such%20as%20limited%20labeled%20data%2C%20weak%20model%0Atransferability%2C%20and%20dynamic%20environmental%20conditions.%20We%20also%20discuss%20its%20use%0Ain%20crop%20health%20monitoring%2C%20pest%20detection%2C%20and%20fruit%20recognition%2C%20highlighting%0Aimprovements%20in%20performance%20across%20regions%20and%20seasons.%20The%20paper%20categorizes%0ADA%20methods%20into%20shallow%20and%20deep%20learning%20models%2C%20with%20further%20divisions%20into%0Asupervised%2C%20semi-supervised%2C%20and%20unsupervised%20approaches.%20A%20special%20focus%20is%0Agiven%20to%20adversarial%20learning-based%20DA%20methods%2C%20which%20have%20shown%20great%20promise%0Ain%20challenging%20agricultural%20scenarios.%20Finally%2C%20we%20review%20key%20public%20datasets%0Ain%20agricultural%20imagery%2C%20analyzing%20their%20value%20and%20limitations%20in%20DA%20research.%0AThis%20review%20provides%20a%20comprehensive%20framework%20for%20researchers%2C%20offering%0Ainsights%20into%20current%20research%20gaps%20and%20supporting%20the%20advancement%20of%20DA%0Amethods%20in%20agricultural%20image%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05972v1&entry.124074799=Read"},
{"title": "WisWheat: A Three-Tiered Vision-Language Dataset for Wheat Management", "author": "Bowen Yuan and Selena Song and Javier Fernandez and Yadan Luo and Mahsa Baktashmotlagh and Zijian Wang", "abstract": "  Wheat management strategies play a critical role in determining yield.\nTraditional management decisions often rely on labour-intensive expert\ninspections, which are expensive, subjective and difficult to scale. Recently,\nVision-Language Models (VLMs) have emerged as a promising solution to enable\nscalable, data-driven management support. However, due to a lack of\ndomain-specific knowledge, directly applying VLMs to wheat management tasks\nresults in poor quantification and reasoning capabilities, ultimately producing\nvague or even misleading management recommendations. In response, we propose\nWisWheat, a wheat-specific dataset with a three-layered design to enhance VLM\nperformance on wheat management tasks: (1) a foundational pretraining dataset\nof 47,871 image-caption pairs for coarsely adapting VLMs to wheat morphology;\n(2) a quantitative dataset comprising 7,263 VQA-style image-question-answer\ntriplets for quantitative trait measuring tasks; and (3) an Instruction\nFine-tuning dataset with 4,888 samples targeting biotic and abiotic stress\ndiagnosis and management plan for different phenological stages. Extensive\nexperimental results demonstrate that fine-tuning open-source VLMs (e.g.,\nQwen2.5 7B) on our dataset leads to significant performance improvements.\nSpecifically, the Qwen2.5 VL 7B fine-tuned on our wheat instruction dataset\nachieves accuracy scores of 79.2% and 84.6% on wheat stress and growth stage\nconversation tasks respectively, surpassing even general-purpose commercial\nmodels such as GPT-4o by a margin of 11.9% and 34.6%.\n", "link": "http://arxiv.org/abs/2506.06084v1", "date": "2025-06-06", "relevancy": 2.5339, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5264}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5264}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WisWheat%3A%20A%20Three-Tiered%20Vision-Language%20Dataset%20for%20Wheat%20Management&body=Title%3A%20WisWheat%3A%20A%20Three-Tiered%20Vision-Language%20Dataset%20for%20Wheat%20Management%0AAuthor%3A%20Bowen%20Yuan%20and%20Selena%20Song%20and%20Javier%20Fernandez%20and%20Yadan%20Luo%20and%20Mahsa%20Baktashmotlagh%20and%20Zijian%20Wang%0AAbstract%3A%20%20%20Wheat%20management%20strategies%20play%20a%20critical%20role%20in%20determining%20yield.%0ATraditional%20management%20decisions%20often%20rely%20on%20labour-intensive%20expert%0Ainspections%2C%20which%20are%20expensive%2C%20subjective%20and%20difficult%20to%20scale.%20Recently%2C%0AVision-Language%20Models%20%28VLMs%29%20have%20emerged%20as%20a%20promising%20solution%20to%20enable%0Ascalable%2C%20data-driven%20management%20support.%20However%2C%20due%20to%20a%20lack%20of%0Adomain-specific%20knowledge%2C%20directly%20applying%20VLMs%20to%20wheat%20management%20tasks%0Aresults%20in%20poor%20quantification%20and%20reasoning%20capabilities%2C%20ultimately%20producing%0Avague%20or%20even%20misleading%20management%20recommendations.%20In%20response%2C%20we%20propose%0AWisWheat%2C%20a%20wheat-specific%20dataset%20with%20a%20three-layered%20design%20to%20enhance%20VLM%0Aperformance%20on%20wheat%20management%20tasks%3A%20%281%29%20a%20foundational%20pretraining%20dataset%0Aof%2047%2C871%20image-caption%20pairs%20for%20coarsely%20adapting%20VLMs%20to%20wheat%20morphology%3B%0A%282%29%20a%20quantitative%20dataset%20comprising%207%2C263%20VQA-style%20image-question-answer%0Atriplets%20for%20quantitative%20trait%20measuring%20tasks%3B%20and%20%283%29%20an%20Instruction%0AFine-tuning%20dataset%20with%204%2C888%20samples%20targeting%20biotic%20and%20abiotic%20stress%0Adiagnosis%20and%20management%20plan%20for%20different%20phenological%20stages.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20fine-tuning%20open-source%20VLMs%20%28e.g.%2C%0AQwen2.5%207B%29%20on%20our%20dataset%20leads%20to%20significant%20performance%20improvements.%0ASpecifically%2C%20the%20Qwen2.5%20VL%207B%20fine-tuned%20on%20our%20wheat%20instruction%20dataset%0Aachieves%20accuracy%20scores%20of%2079.2%25%20and%2084.6%25%20on%20wheat%20stress%20and%20growth%20stage%0Aconversation%20tasks%20respectively%2C%20surpassing%20even%20general-purpose%20commercial%0Amodels%20such%20as%20GPT-4o%20by%20a%20margin%20of%2011.9%25%20and%2034.6%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06084v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWisWheat%253A%2520A%2520Three-Tiered%2520Vision-Language%2520Dataset%2520for%2520Wheat%2520Management%26entry.906535625%3DBowen%2520Yuan%2520and%2520Selena%2520Song%2520and%2520Javier%2520Fernandez%2520and%2520Yadan%2520Luo%2520and%2520Mahsa%2520Baktashmotlagh%2520and%2520Zijian%2520Wang%26entry.1292438233%3D%2520%2520Wheat%2520management%2520strategies%2520play%2520a%2520critical%2520role%2520in%2520determining%2520yield.%250ATraditional%2520management%2520decisions%2520often%2520rely%2520on%2520labour-intensive%2520expert%250Ainspections%252C%2520which%2520are%2520expensive%252C%2520subjective%2520and%2520difficult%2520to%2520scale.%2520Recently%252C%250AVision-Language%2520Models%2520%2528VLMs%2529%2520have%2520emerged%2520as%2520a%2520promising%2520solution%2520to%2520enable%250Ascalable%252C%2520data-driven%2520management%2520support.%2520However%252C%2520due%2520to%2520a%2520lack%2520of%250Adomain-specific%2520knowledge%252C%2520directly%2520applying%2520VLMs%2520to%2520wheat%2520management%2520tasks%250Aresults%2520in%2520poor%2520quantification%2520and%2520reasoning%2520capabilities%252C%2520ultimately%2520producing%250Avague%2520or%2520even%2520misleading%2520management%2520recommendations.%2520In%2520response%252C%2520we%2520propose%250AWisWheat%252C%2520a%2520wheat-specific%2520dataset%2520with%2520a%2520three-layered%2520design%2520to%2520enhance%2520VLM%250Aperformance%2520on%2520wheat%2520management%2520tasks%253A%2520%25281%2529%2520a%2520foundational%2520pretraining%2520dataset%250Aof%252047%252C871%2520image-caption%2520pairs%2520for%2520coarsely%2520adapting%2520VLMs%2520to%2520wheat%2520morphology%253B%250A%25282%2529%2520a%2520quantitative%2520dataset%2520comprising%25207%252C263%2520VQA-style%2520image-question-answer%250Atriplets%2520for%2520quantitative%2520trait%2520measuring%2520tasks%253B%2520and%2520%25283%2529%2520an%2520Instruction%250AFine-tuning%2520dataset%2520with%25204%252C888%2520samples%2520targeting%2520biotic%2520and%2520abiotic%2520stress%250Adiagnosis%2520and%2520management%2520plan%2520for%2520different%2520phenological%2520stages.%2520Extensive%250Aexperimental%2520results%2520demonstrate%2520that%2520fine-tuning%2520open-source%2520VLMs%2520%2528e.g.%252C%250AQwen2.5%25207B%2529%2520on%2520our%2520dataset%2520leads%2520to%2520significant%2520performance%2520improvements.%250ASpecifically%252C%2520the%2520Qwen2.5%2520VL%25207B%2520fine-tuned%2520on%2520our%2520wheat%2520instruction%2520dataset%250Aachieves%2520accuracy%2520scores%2520of%252079.2%2525%2520and%252084.6%2525%2520on%2520wheat%2520stress%2520and%2520growth%2520stage%250Aconversation%2520tasks%2520respectively%252C%2520surpassing%2520even%2520general-purpose%2520commercial%250Amodels%2520such%2520as%2520GPT-4o%2520by%2520a%2520margin%2520of%252011.9%2525%2520and%252034.6%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06084v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WisWheat%3A%20A%20Three-Tiered%20Vision-Language%20Dataset%20for%20Wheat%20Management&entry.906535625=Bowen%20Yuan%20and%20Selena%20Song%20and%20Javier%20Fernandez%20and%20Yadan%20Luo%20and%20Mahsa%20Baktashmotlagh%20and%20Zijian%20Wang&entry.1292438233=%20%20Wheat%20management%20strategies%20play%20a%20critical%20role%20in%20determining%20yield.%0ATraditional%20management%20decisions%20often%20rely%20on%20labour-intensive%20expert%0Ainspections%2C%20which%20are%20expensive%2C%20subjective%20and%20difficult%20to%20scale.%20Recently%2C%0AVision-Language%20Models%20%28VLMs%29%20have%20emerged%20as%20a%20promising%20solution%20to%20enable%0Ascalable%2C%20data-driven%20management%20support.%20However%2C%20due%20to%20a%20lack%20of%0Adomain-specific%20knowledge%2C%20directly%20applying%20VLMs%20to%20wheat%20management%20tasks%0Aresults%20in%20poor%20quantification%20and%20reasoning%20capabilities%2C%20ultimately%20producing%0Avague%20or%20even%20misleading%20management%20recommendations.%20In%20response%2C%20we%20propose%0AWisWheat%2C%20a%20wheat-specific%20dataset%20with%20a%20three-layered%20design%20to%20enhance%20VLM%0Aperformance%20on%20wheat%20management%20tasks%3A%20%281%29%20a%20foundational%20pretraining%20dataset%0Aof%2047%2C871%20image-caption%20pairs%20for%20coarsely%20adapting%20VLMs%20to%20wheat%20morphology%3B%0A%282%29%20a%20quantitative%20dataset%20comprising%207%2C263%20VQA-style%20image-question-answer%0Atriplets%20for%20quantitative%20trait%20measuring%20tasks%3B%20and%20%283%29%20an%20Instruction%0AFine-tuning%20dataset%20with%204%2C888%20samples%20targeting%20biotic%20and%20abiotic%20stress%0Adiagnosis%20and%20management%20plan%20for%20different%20phenological%20stages.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20fine-tuning%20open-source%20VLMs%20%28e.g.%2C%0AQwen2.5%207B%29%20on%20our%20dataset%20leads%20to%20significant%20performance%20improvements.%0ASpecifically%2C%20the%20Qwen2.5%20VL%207B%20fine-tuned%20on%20our%20wheat%20instruction%20dataset%0Aachieves%20accuracy%20scores%20of%2079.2%25%20and%2084.6%25%20on%20wheat%20stress%20and%20growth%20stage%0Aconversation%20tasks%20respectively%2C%20surpassing%20even%20general-purpose%20commercial%0Amodels%20such%20as%20GPT-4o%20by%20a%20margin%20of%2011.9%25%20and%2034.6%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06084v1&entry.124074799=Read"},
{"title": "Let's Put Ourselves in Sally's Shoes: Shoes-of-Others Prefixing Improves\n  Theory of Mind in Large Language Models", "author": "Kazutoshi Shinoda and Nobukatsu Hojo and Kyosuke Nishida and Yoshihiro Yamazaki and Keita Suzuki and Hiroaki Sugiyama and Kuniko Saito", "abstract": "  Recent studies have shown that Theory of Mind (ToM) in large language models\n(LLMs) has not reached human-level performance yet. Since fine-tuning LLMs on\nToM datasets often degrades their generalization, several inference-time\nmethods have been proposed to enhance ToM in LLMs. However, existing\ninference-time methods for ToM are specialized for inferring beliefs from\ncontexts involving changes in the world state. In this study, we present a new\ninference-time method for ToM, Shoes-of-Others (SoO) prefixing, which makes\nfewer assumptions about contexts and is applicable to broader scenarios. SoO\nprefixing simply specifies the beginning of LLM outputs with ``Let's put\nourselves in A's shoes.'', where A denotes the target character's name. We\nevaluate SoO prefixing on two benchmarks that assess ToM in conversational and\nnarrative contexts without changes in the world state and find that it\nconsistently improves ToM across five categories of mental states. Our analysis\nsuggests that SoO prefixing elicits faithful thoughts, thereby improving the\nToM performance.\n", "link": "http://arxiv.org/abs/2506.05970v1", "date": "2025-06-06", "relevancy": 2.5211, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.513}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.513}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Let%27s%20Put%20Ourselves%20in%20Sally%27s%20Shoes%3A%20Shoes-of-Others%20Prefixing%20Improves%0A%20%20Theory%20of%20Mind%20in%20Large%20Language%20Models&body=Title%3A%20Let%27s%20Put%20Ourselves%20in%20Sally%27s%20Shoes%3A%20Shoes-of-Others%20Prefixing%20Improves%0A%20%20Theory%20of%20Mind%20in%20Large%20Language%20Models%0AAuthor%3A%20Kazutoshi%20Shinoda%20and%20Nobukatsu%20Hojo%20and%20Kyosuke%20Nishida%20and%20Yoshihiro%20Yamazaki%20and%20Keita%20Suzuki%20and%20Hiroaki%20Sugiyama%20and%20Kuniko%20Saito%0AAbstract%3A%20%20%20Recent%20studies%20have%20shown%20that%20Theory%20of%20Mind%20%28ToM%29%20in%20large%20language%20models%0A%28LLMs%29%20has%20not%20reached%20human-level%20performance%20yet.%20Since%20fine-tuning%20LLMs%20on%0AToM%20datasets%20often%20degrades%20their%20generalization%2C%20several%20inference-time%0Amethods%20have%20been%20proposed%20to%20enhance%20ToM%20in%20LLMs.%20However%2C%20existing%0Ainference-time%20methods%20for%20ToM%20are%20specialized%20for%20inferring%20beliefs%20from%0Acontexts%20involving%20changes%20in%20the%20world%20state.%20In%20this%20study%2C%20we%20present%20a%20new%0Ainference-time%20method%20for%20ToM%2C%20Shoes-of-Others%20%28SoO%29%20prefixing%2C%20which%20makes%0Afewer%20assumptions%20about%20contexts%20and%20is%20applicable%20to%20broader%20scenarios.%20SoO%0Aprefixing%20simply%20specifies%20the%20beginning%20of%20LLM%20outputs%20with%20%60%60Let%27s%20put%0Aourselves%20in%20A%27s%20shoes.%27%27%2C%20where%20A%20denotes%20the%20target%20character%27s%20name.%20We%0Aevaluate%20SoO%20prefixing%20on%20two%20benchmarks%20that%20assess%20ToM%20in%20conversational%20and%0Anarrative%20contexts%20without%20changes%20in%20the%20world%20state%20and%20find%20that%20it%0Aconsistently%20improves%20ToM%20across%20five%20categories%20of%20mental%20states.%20Our%20analysis%0Asuggests%20that%20SoO%20prefixing%20elicits%20faithful%20thoughts%2C%20thereby%20improving%20the%0AToM%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05970v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLet%2527s%2520Put%2520Ourselves%2520in%2520Sally%2527s%2520Shoes%253A%2520Shoes-of-Others%2520Prefixing%2520Improves%250A%2520%2520Theory%2520of%2520Mind%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DKazutoshi%2520Shinoda%2520and%2520Nobukatsu%2520Hojo%2520and%2520Kyosuke%2520Nishida%2520and%2520Yoshihiro%2520Yamazaki%2520and%2520Keita%2520Suzuki%2520and%2520Hiroaki%2520Sugiyama%2520and%2520Kuniko%2520Saito%26entry.1292438233%3D%2520%2520Recent%2520studies%2520have%2520shown%2520that%2520Theory%2520of%2520Mind%2520%2528ToM%2529%2520in%2520large%2520language%2520models%250A%2528LLMs%2529%2520has%2520not%2520reached%2520human-level%2520performance%2520yet.%2520Since%2520fine-tuning%2520LLMs%2520on%250AToM%2520datasets%2520often%2520degrades%2520their%2520generalization%252C%2520several%2520inference-time%250Amethods%2520have%2520been%2520proposed%2520to%2520enhance%2520ToM%2520in%2520LLMs.%2520However%252C%2520existing%250Ainference-time%2520methods%2520for%2520ToM%2520are%2520specialized%2520for%2520inferring%2520beliefs%2520from%250Acontexts%2520involving%2520changes%2520in%2520the%2520world%2520state.%2520In%2520this%2520study%252C%2520we%2520present%2520a%2520new%250Ainference-time%2520method%2520for%2520ToM%252C%2520Shoes-of-Others%2520%2528SoO%2529%2520prefixing%252C%2520which%2520makes%250Afewer%2520assumptions%2520about%2520contexts%2520and%2520is%2520applicable%2520to%2520broader%2520scenarios.%2520SoO%250Aprefixing%2520simply%2520specifies%2520the%2520beginning%2520of%2520LLM%2520outputs%2520with%2520%2560%2560Let%2527s%2520put%250Aourselves%2520in%2520A%2527s%2520shoes.%2527%2527%252C%2520where%2520A%2520denotes%2520the%2520target%2520character%2527s%2520name.%2520We%250Aevaluate%2520SoO%2520prefixing%2520on%2520two%2520benchmarks%2520that%2520assess%2520ToM%2520in%2520conversational%2520and%250Anarrative%2520contexts%2520without%2520changes%2520in%2520the%2520world%2520state%2520and%2520find%2520that%2520it%250Aconsistently%2520improves%2520ToM%2520across%2520five%2520categories%2520of%2520mental%2520states.%2520Our%2520analysis%250Asuggests%2520that%2520SoO%2520prefixing%2520elicits%2520faithful%2520thoughts%252C%2520thereby%2520improving%2520the%250AToM%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05970v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Let%27s%20Put%20Ourselves%20in%20Sally%27s%20Shoes%3A%20Shoes-of-Others%20Prefixing%20Improves%0A%20%20Theory%20of%20Mind%20in%20Large%20Language%20Models&entry.906535625=Kazutoshi%20Shinoda%20and%20Nobukatsu%20Hojo%20and%20Kyosuke%20Nishida%20and%20Yoshihiro%20Yamazaki%20and%20Keita%20Suzuki%20and%20Hiroaki%20Sugiyama%20and%20Kuniko%20Saito&entry.1292438233=%20%20Recent%20studies%20have%20shown%20that%20Theory%20of%20Mind%20%28ToM%29%20in%20large%20language%20models%0A%28LLMs%29%20has%20not%20reached%20human-level%20performance%20yet.%20Since%20fine-tuning%20LLMs%20on%0AToM%20datasets%20often%20degrades%20their%20generalization%2C%20several%20inference-time%0Amethods%20have%20been%20proposed%20to%20enhance%20ToM%20in%20LLMs.%20However%2C%20existing%0Ainference-time%20methods%20for%20ToM%20are%20specialized%20for%20inferring%20beliefs%20from%0Acontexts%20involving%20changes%20in%20the%20world%20state.%20In%20this%20study%2C%20we%20present%20a%20new%0Ainference-time%20method%20for%20ToM%2C%20Shoes-of-Others%20%28SoO%29%20prefixing%2C%20which%20makes%0Afewer%20assumptions%20about%20contexts%20and%20is%20applicable%20to%20broader%20scenarios.%20SoO%0Aprefixing%20simply%20specifies%20the%20beginning%20of%20LLM%20outputs%20with%20%60%60Let%27s%20put%0Aourselves%20in%20A%27s%20shoes.%27%27%2C%20where%20A%20denotes%20the%20target%20character%27s%20name.%20We%0Aevaluate%20SoO%20prefixing%20on%20two%20benchmarks%20that%20assess%20ToM%20in%20conversational%20and%0Anarrative%20contexts%20without%20changes%20in%20the%20world%20state%20and%20find%20that%20it%0Aconsistently%20improves%20ToM%20across%20five%20categories%20of%20mental%20states.%20Our%20analysis%0Asuggests%20that%20SoO%20prefixing%20elicits%20faithful%20thoughts%2C%20thereby%20improving%20the%0AToM%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05970v1&entry.124074799=Read"},
{"title": "DPCore: Dynamic Prompt Coreset for Continual Test-Time Adaptation", "author": "Yunbei Zhang and Akshay Mehra and Shuaicheng Niu and Jihun Hamm", "abstract": "  Continual Test-Time Adaptation (CTTA) seeks to adapt source pre-trained\nmodels to continually changing, unseen target domains. While existing CTTA\nmethods assume structured domain changes with uniform durations, real-world\nenvironments often exhibit dynamic patterns where domains recur with varying\nfrequencies and durations. Current approaches, which adapt the same parameters\nacross different domains, struggle in such dynamic conditions-they face\nconvergence issues with brief domain exposures, risk forgetting previously\nlearned knowledge, or misapplying it to irrelevant domains. To remedy this, we\npropose DPCore, a method designed for robust performance across diverse domain\nchange patterns while ensuring computational efficiency. DPCore integrates\nthree key components: Visual Prompt Adaptation for efficient domain alignment,\na Prompt Coreset for knowledge preservation, and a Dynamic Update mechanism\nthat intelligently adjusts existing prompts for similar domains while creating\nnew ones for substantially different domains. Extensive experiments on four\nbenchmarks demonstrate that DPCore consistently outperforms various CTTA\nmethods, achieving state-of-the-art performance in both structured and dynamic\nsettings while reducing trainable parameters by 99% and computation time by 64%\ncompared to previous approaches.\n", "link": "http://arxiv.org/abs/2406.10737v4", "date": "2025-06-06", "relevancy": 2.5092, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5174}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5124}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DPCore%3A%20Dynamic%20Prompt%20Coreset%20for%20Continual%20Test-Time%20Adaptation&body=Title%3A%20DPCore%3A%20Dynamic%20Prompt%20Coreset%20for%20Continual%20Test-Time%20Adaptation%0AAuthor%3A%20Yunbei%20Zhang%20and%20Akshay%20Mehra%20and%20Shuaicheng%20Niu%20and%20Jihun%20Hamm%0AAbstract%3A%20%20%20Continual%20Test-Time%20Adaptation%20%28CTTA%29%20seeks%20to%20adapt%20source%20pre-trained%0Amodels%20to%20continually%20changing%2C%20unseen%20target%20domains.%20While%20existing%20CTTA%0Amethods%20assume%20structured%20domain%20changes%20with%20uniform%20durations%2C%20real-world%0Aenvironments%20often%20exhibit%20dynamic%20patterns%20where%20domains%20recur%20with%20varying%0Afrequencies%20and%20durations.%20Current%20approaches%2C%20which%20adapt%20the%20same%20parameters%0Aacross%20different%20domains%2C%20struggle%20in%20such%20dynamic%20conditions-they%20face%0Aconvergence%20issues%20with%20brief%20domain%20exposures%2C%20risk%20forgetting%20previously%0Alearned%20knowledge%2C%20or%20misapplying%20it%20to%20irrelevant%20domains.%20To%20remedy%20this%2C%20we%0Apropose%20DPCore%2C%20a%20method%20designed%20for%20robust%20performance%20across%20diverse%20domain%0Achange%20patterns%20while%20ensuring%20computational%20efficiency.%20DPCore%20integrates%0Athree%20key%20components%3A%20Visual%20Prompt%20Adaptation%20for%20efficient%20domain%20alignment%2C%0Aa%20Prompt%20Coreset%20for%20knowledge%20preservation%2C%20and%20a%20Dynamic%20Update%20mechanism%0Athat%20intelligently%20adjusts%20existing%20prompts%20for%20similar%20domains%20while%20creating%0Anew%20ones%20for%20substantially%20different%20domains.%20Extensive%20experiments%20on%20four%0Abenchmarks%20demonstrate%20that%20DPCore%20consistently%20outperforms%20various%20CTTA%0Amethods%2C%20achieving%20state-of-the-art%20performance%20in%20both%20structured%20and%20dynamic%0Asettings%20while%20reducing%20trainable%20parameters%20by%2099%25%20and%20computation%20time%20by%2064%25%0Acompared%20to%20previous%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10737v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDPCore%253A%2520Dynamic%2520Prompt%2520Coreset%2520for%2520Continual%2520Test-Time%2520Adaptation%26entry.906535625%3DYunbei%2520Zhang%2520and%2520Akshay%2520Mehra%2520and%2520Shuaicheng%2520Niu%2520and%2520Jihun%2520Hamm%26entry.1292438233%3D%2520%2520Continual%2520Test-Time%2520Adaptation%2520%2528CTTA%2529%2520seeks%2520to%2520adapt%2520source%2520pre-trained%250Amodels%2520to%2520continually%2520changing%252C%2520unseen%2520target%2520domains.%2520While%2520existing%2520CTTA%250Amethods%2520assume%2520structured%2520domain%2520changes%2520with%2520uniform%2520durations%252C%2520real-world%250Aenvironments%2520often%2520exhibit%2520dynamic%2520patterns%2520where%2520domains%2520recur%2520with%2520varying%250Afrequencies%2520and%2520durations.%2520Current%2520approaches%252C%2520which%2520adapt%2520the%2520same%2520parameters%250Aacross%2520different%2520domains%252C%2520struggle%2520in%2520such%2520dynamic%2520conditions-they%2520face%250Aconvergence%2520issues%2520with%2520brief%2520domain%2520exposures%252C%2520risk%2520forgetting%2520previously%250Alearned%2520knowledge%252C%2520or%2520misapplying%2520it%2520to%2520irrelevant%2520domains.%2520To%2520remedy%2520this%252C%2520we%250Apropose%2520DPCore%252C%2520a%2520method%2520designed%2520for%2520robust%2520performance%2520across%2520diverse%2520domain%250Achange%2520patterns%2520while%2520ensuring%2520computational%2520efficiency.%2520DPCore%2520integrates%250Athree%2520key%2520components%253A%2520Visual%2520Prompt%2520Adaptation%2520for%2520efficient%2520domain%2520alignment%252C%250Aa%2520Prompt%2520Coreset%2520for%2520knowledge%2520preservation%252C%2520and%2520a%2520Dynamic%2520Update%2520mechanism%250Athat%2520intelligently%2520adjusts%2520existing%2520prompts%2520for%2520similar%2520domains%2520while%2520creating%250Anew%2520ones%2520for%2520substantially%2520different%2520domains.%2520Extensive%2520experiments%2520on%2520four%250Abenchmarks%2520demonstrate%2520that%2520DPCore%2520consistently%2520outperforms%2520various%2520CTTA%250Amethods%252C%2520achieving%2520state-of-the-art%2520performance%2520in%2520both%2520structured%2520and%2520dynamic%250Asettings%2520while%2520reducing%2520trainable%2520parameters%2520by%252099%2525%2520and%2520computation%2520time%2520by%252064%2525%250Acompared%2520to%2520previous%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10737v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DPCore%3A%20Dynamic%20Prompt%20Coreset%20for%20Continual%20Test-Time%20Adaptation&entry.906535625=Yunbei%20Zhang%20and%20Akshay%20Mehra%20and%20Shuaicheng%20Niu%20and%20Jihun%20Hamm&entry.1292438233=%20%20Continual%20Test-Time%20Adaptation%20%28CTTA%29%20seeks%20to%20adapt%20source%20pre-trained%0Amodels%20to%20continually%20changing%2C%20unseen%20target%20domains.%20While%20existing%20CTTA%0Amethods%20assume%20structured%20domain%20changes%20with%20uniform%20durations%2C%20real-world%0Aenvironments%20often%20exhibit%20dynamic%20patterns%20where%20domains%20recur%20with%20varying%0Afrequencies%20and%20durations.%20Current%20approaches%2C%20which%20adapt%20the%20same%20parameters%0Aacross%20different%20domains%2C%20struggle%20in%20such%20dynamic%20conditions-they%20face%0Aconvergence%20issues%20with%20brief%20domain%20exposures%2C%20risk%20forgetting%20previously%0Alearned%20knowledge%2C%20or%20misapplying%20it%20to%20irrelevant%20domains.%20To%20remedy%20this%2C%20we%0Apropose%20DPCore%2C%20a%20method%20designed%20for%20robust%20performance%20across%20diverse%20domain%0Achange%20patterns%20while%20ensuring%20computational%20efficiency.%20DPCore%20integrates%0Athree%20key%20components%3A%20Visual%20Prompt%20Adaptation%20for%20efficient%20domain%20alignment%2C%0Aa%20Prompt%20Coreset%20for%20knowledge%20preservation%2C%20and%20a%20Dynamic%20Update%20mechanism%0Athat%20intelligently%20adjusts%20existing%20prompts%20for%20similar%20domains%20while%20creating%0Anew%20ones%20for%20substantially%20different%20domains.%20Extensive%20experiments%20on%20four%0Abenchmarks%20demonstrate%20that%20DPCore%20consistently%20outperforms%20various%20CTTA%0Amethods%2C%20achieving%20state-of-the-art%20performance%20in%20both%20structured%20and%20dynamic%0Asettings%20while%20reducing%20trainable%20parameters%20by%2099%25%20and%20computation%20time%20by%2064%25%0Acompared%20to%20previous%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10737v4&entry.124074799=Read"},
{"title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "author": "Zehan Wang and Jiayang Xu and Ziang Zhang and Tianyu Pang and Chao Du and Hengshuang Zhao and Zhou Zhao", "abstract": "  Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.\n", "link": "http://arxiv.org/abs/2505.24870v2", "date": "2025-06-06", "relevancy": 2.4828, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6229}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6229}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenSpace%3A%20Benchmarking%20Spatially-Aware%20Image%20Generation&body=Title%3A%20GenSpace%3A%20Benchmarking%20Spatially-Aware%20Image%20Generation%0AAuthor%3A%20Zehan%20Wang%20and%20Jiayang%20Xu%20and%20Ziang%20Zhang%20and%20Tianyu%20Pang%20and%20Chao%20Du%20and%20Hengshuang%20Zhao%20and%20Zhou%20Zhao%0AAbstract%3A%20%20%20Humans%20can%20intuitively%20compose%20and%20arrange%20scenes%20in%20the%203D%20space%20for%0Aphotography.%20However%2C%20can%20advanced%20AI%20image%20generators%20plan%20scenes%20with%20similar%0A3D%20spatial%20awareness%20when%20creating%20images%20from%20text%20or%20image%20prompts%3F%20We%0Apresent%20GenSpace%2C%20a%20novel%20benchmark%20and%20evaluation%20pipeline%20to%20comprehensively%0Aassess%20the%20spatial%20awareness%20of%20current%20image%20generation%20models.%20Furthermore%2C%0Astandard%20evaluations%20using%20general%20Vision-Language%20Models%20%28VLMs%29%20frequently%0Afail%20to%20capture%20the%20detailed%20spatial%20errors.%20To%20handle%20this%20challenge%2C%20we%0Apropose%20a%20specialized%20evaluation%20pipeline%20and%20metric%2C%20which%20reconstructs%203D%0Ascene%20geometry%20using%20multiple%20visual%20foundation%20models%20and%20provides%20a%20more%0Aaccurate%20and%20human-aligned%20metric%20of%20spatial%20faithfulness.%20Our%20findings%20show%0Athat%20while%20AI%20models%20create%20visually%20appealing%20images%20and%20can%20follow%20general%0Ainstructions%2C%20they%20struggle%20with%20specific%203D%20details%20like%20object%20placement%2C%0Arelationships%2C%20and%20measurements.%20We%20summarize%20three%20core%20limitations%20in%20the%0Aspatial%20perception%20of%20current%20state-of-the-art%20image%20generation%20models%3A%201%29%0AObject%20Perspective%20Understanding%2C%202%29%20Egocentric-Allocentric%20Transformation%20and%0A3%29%20Metric%20Measurement%20Adherence%2C%20highlighting%20possible%20directions%20for%20improving%0Aspatial%20intelligence%20in%20image%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24870v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenSpace%253A%2520Benchmarking%2520Spatially-Aware%2520Image%2520Generation%26entry.906535625%3DZehan%2520Wang%2520and%2520Jiayang%2520Xu%2520and%2520Ziang%2520Zhang%2520and%2520Tianyu%2520Pang%2520and%2520Chao%2520Du%2520and%2520Hengshuang%2520Zhao%2520and%2520Zhou%2520Zhao%26entry.1292438233%3D%2520%2520Humans%2520can%2520intuitively%2520compose%2520and%2520arrange%2520scenes%2520in%2520the%25203D%2520space%2520for%250Aphotography.%2520However%252C%2520can%2520advanced%2520AI%2520image%2520generators%2520plan%2520scenes%2520with%2520similar%250A3D%2520spatial%2520awareness%2520when%2520creating%2520images%2520from%2520text%2520or%2520image%2520prompts%253F%2520We%250Apresent%2520GenSpace%252C%2520a%2520novel%2520benchmark%2520and%2520evaluation%2520pipeline%2520to%2520comprehensively%250Aassess%2520the%2520spatial%2520awareness%2520of%2520current%2520image%2520generation%2520models.%2520Furthermore%252C%250Astandard%2520evaluations%2520using%2520general%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520frequently%250Afail%2520to%2520capture%2520the%2520detailed%2520spatial%2520errors.%2520To%2520handle%2520this%2520challenge%252C%2520we%250Apropose%2520a%2520specialized%2520evaluation%2520pipeline%2520and%2520metric%252C%2520which%2520reconstructs%25203D%250Ascene%2520geometry%2520using%2520multiple%2520visual%2520foundation%2520models%2520and%2520provides%2520a%2520more%250Aaccurate%2520and%2520human-aligned%2520metric%2520of%2520spatial%2520faithfulness.%2520Our%2520findings%2520show%250Athat%2520while%2520AI%2520models%2520create%2520visually%2520appealing%2520images%2520and%2520can%2520follow%2520general%250Ainstructions%252C%2520they%2520struggle%2520with%2520specific%25203D%2520details%2520like%2520object%2520placement%252C%250Arelationships%252C%2520and%2520measurements.%2520We%2520summarize%2520three%2520core%2520limitations%2520in%2520the%250Aspatial%2520perception%2520of%2520current%2520state-of-the-art%2520image%2520generation%2520models%253A%25201%2529%250AObject%2520Perspective%2520Understanding%252C%25202%2529%2520Egocentric-Allocentric%2520Transformation%2520and%250A3%2529%2520Metric%2520Measurement%2520Adherence%252C%2520highlighting%2520possible%2520directions%2520for%2520improving%250Aspatial%2520intelligence%2520in%2520image%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24870v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenSpace%3A%20Benchmarking%20Spatially-Aware%20Image%20Generation&entry.906535625=Zehan%20Wang%20and%20Jiayang%20Xu%20and%20Ziang%20Zhang%20and%20Tianyu%20Pang%20and%20Chao%20Du%20and%20Hengshuang%20Zhao%20and%20Zhou%20Zhao&entry.1292438233=%20%20Humans%20can%20intuitively%20compose%20and%20arrange%20scenes%20in%20the%203D%20space%20for%0Aphotography.%20However%2C%20can%20advanced%20AI%20image%20generators%20plan%20scenes%20with%20similar%0A3D%20spatial%20awareness%20when%20creating%20images%20from%20text%20or%20image%20prompts%3F%20We%0Apresent%20GenSpace%2C%20a%20novel%20benchmark%20and%20evaluation%20pipeline%20to%20comprehensively%0Aassess%20the%20spatial%20awareness%20of%20current%20image%20generation%20models.%20Furthermore%2C%0Astandard%20evaluations%20using%20general%20Vision-Language%20Models%20%28VLMs%29%20frequently%0Afail%20to%20capture%20the%20detailed%20spatial%20errors.%20To%20handle%20this%20challenge%2C%20we%0Apropose%20a%20specialized%20evaluation%20pipeline%20and%20metric%2C%20which%20reconstructs%203D%0Ascene%20geometry%20using%20multiple%20visual%20foundation%20models%20and%20provides%20a%20more%0Aaccurate%20and%20human-aligned%20metric%20of%20spatial%20faithfulness.%20Our%20findings%20show%0Athat%20while%20AI%20models%20create%20visually%20appealing%20images%20and%20can%20follow%20general%0Ainstructions%2C%20they%20struggle%20with%20specific%203D%20details%20like%20object%20placement%2C%0Arelationships%2C%20and%20measurements.%20We%20summarize%20three%20core%20limitations%20in%20the%0Aspatial%20perception%20of%20current%20state-of-the-art%20image%20generation%20models%3A%201%29%0AObject%20Perspective%20Understanding%2C%202%29%20Egocentric-Allocentric%20Transformation%20and%0A3%29%20Metric%20Measurement%20Adherence%2C%20highlighting%20possible%20directions%20for%20improving%0Aspatial%20intelligence%20in%20image%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24870v2&entry.124074799=Read"},
{"title": "Tug-of-war between idiom's figurative and literal meanings in LLMs", "author": "Soyoung Oh and Xinting Huang and Mathis Pink and Michael Hahn and Vera Demberg", "abstract": "  Idioms present a unique challenge for language models due to their\nnon-compositional figurative meanings, which often strongly diverge from the\nidiom's literal interpretation. This duality requires a model to learn\nrepresenting and deciding between the two meanings to interpret an idiom in a\nfigurative sense, or literally. In this paper, we employ tools from mechanistic\ninterpretability to trace how a large pretrained causal transformer\n(LLama3.2-1B-base) deals with this ambiguity. We localize three steps of idiom\nprocessing: First, the idiom's figurative meaning is retrieved in early\nattention and MLP sublayers. We identify specific attention heads which boost\nthe figurative meaning of the idiom while suppressing the idiom's literal\ninterpretation. The model subsequently represents the figurative representation\nthrough an intermediate path. Meanwhile, a parallel bypass route forwards\nliteral interpretation, ensuring that a both reading remain available. Overall,\nour findings provide a mechanistic evidence for idiom comprehension in an\nautoregressive transformer.\n", "link": "http://arxiv.org/abs/2506.01723v3", "date": "2025-06-06", "relevancy": 2.4355, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4949}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4949}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tug-of-war%20between%20idiom%27s%20figurative%20and%20literal%20meanings%20in%20LLMs&body=Title%3A%20Tug-of-war%20between%20idiom%27s%20figurative%20and%20literal%20meanings%20in%20LLMs%0AAuthor%3A%20Soyoung%20Oh%20and%20Xinting%20Huang%20and%20Mathis%20Pink%20and%20Michael%20Hahn%20and%20Vera%20Demberg%0AAbstract%3A%20%20%20Idioms%20present%20a%20unique%20challenge%20for%20language%20models%20due%20to%20their%0Anon-compositional%20figurative%20meanings%2C%20which%20often%20strongly%20diverge%20from%20the%0Aidiom%27s%20literal%20interpretation.%20This%20duality%20requires%20a%20model%20to%20learn%0Arepresenting%20and%20deciding%20between%20the%20two%20meanings%20to%20interpret%20an%20idiom%20in%20a%0Afigurative%20sense%2C%20or%20literally.%20In%20this%20paper%2C%20we%20employ%20tools%20from%20mechanistic%0Ainterpretability%20to%20trace%20how%20a%20large%20pretrained%20causal%20transformer%0A%28LLama3.2-1B-base%29%20deals%20with%20this%20ambiguity.%20We%20localize%20three%20steps%20of%20idiom%0Aprocessing%3A%20First%2C%20the%20idiom%27s%20figurative%20meaning%20is%20retrieved%20in%20early%0Aattention%20and%20MLP%20sublayers.%20We%20identify%20specific%20attention%20heads%20which%20boost%0Athe%20figurative%20meaning%20of%20the%20idiom%20while%20suppressing%20the%20idiom%27s%20literal%0Ainterpretation.%20The%20model%20subsequently%20represents%20the%20figurative%20representation%0Athrough%20an%20intermediate%20path.%20Meanwhile%2C%20a%20parallel%20bypass%20route%20forwards%0Aliteral%20interpretation%2C%20ensuring%20that%20a%20both%20reading%20remain%20available.%20Overall%2C%0Aour%20findings%20provide%20a%20mechanistic%20evidence%20for%20idiom%20comprehension%20in%20an%0Aautoregressive%20transformer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.01723v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTug-of-war%2520between%2520idiom%2527s%2520figurative%2520and%2520literal%2520meanings%2520in%2520LLMs%26entry.906535625%3DSoyoung%2520Oh%2520and%2520Xinting%2520Huang%2520and%2520Mathis%2520Pink%2520and%2520Michael%2520Hahn%2520and%2520Vera%2520Demberg%26entry.1292438233%3D%2520%2520Idioms%2520present%2520a%2520unique%2520challenge%2520for%2520language%2520models%2520due%2520to%2520their%250Anon-compositional%2520figurative%2520meanings%252C%2520which%2520often%2520strongly%2520diverge%2520from%2520the%250Aidiom%2527s%2520literal%2520interpretation.%2520This%2520duality%2520requires%2520a%2520model%2520to%2520learn%250Arepresenting%2520and%2520deciding%2520between%2520the%2520two%2520meanings%2520to%2520interpret%2520an%2520idiom%2520in%2520a%250Afigurative%2520sense%252C%2520or%2520literally.%2520In%2520this%2520paper%252C%2520we%2520employ%2520tools%2520from%2520mechanistic%250Ainterpretability%2520to%2520trace%2520how%2520a%2520large%2520pretrained%2520causal%2520transformer%250A%2528LLama3.2-1B-base%2529%2520deals%2520with%2520this%2520ambiguity.%2520We%2520localize%2520three%2520steps%2520of%2520idiom%250Aprocessing%253A%2520First%252C%2520the%2520idiom%2527s%2520figurative%2520meaning%2520is%2520retrieved%2520in%2520early%250Aattention%2520and%2520MLP%2520sublayers.%2520We%2520identify%2520specific%2520attention%2520heads%2520which%2520boost%250Athe%2520figurative%2520meaning%2520of%2520the%2520idiom%2520while%2520suppressing%2520the%2520idiom%2527s%2520literal%250Ainterpretation.%2520The%2520model%2520subsequently%2520represents%2520the%2520figurative%2520representation%250Athrough%2520an%2520intermediate%2520path.%2520Meanwhile%252C%2520a%2520parallel%2520bypass%2520route%2520forwards%250Aliteral%2520interpretation%252C%2520ensuring%2520that%2520a%2520both%2520reading%2520remain%2520available.%2520Overall%252C%250Aour%2520findings%2520provide%2520a%2520mechanistic%2520evidence%2520for%2520idiom%2520comprehension%2520in%2520an%250Aautoregressive%2520transformer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.01723v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tug-of-war%20between%20idiom%27s%20figurative%20and%20literal%20meanings%20in%20LLMs&entry.906535625=Soyoung%20Oh%20and%20Xinting%20Huang%20and%20Mathis%20Pink%20and%20Michael%20Hahn%20and%20Vera%20Demberg&entry.1292438233=%20%20Idioms%20present%20a%20unique%20challenge%20for%20language%20models%20due%20to%20their%0Anon-compositional%20figurative%20meanings%2C%20which%20often%20strongly%20diverge%20from%20the%0Aidiom%27s%20literal%20interpretation.%20This%20duality%20requires%20a%20model%20to%20learn%0Arepresenting%20and%20deciding%20between%20the%20two%20meanings%20to%20interpret%20an%20idiom%20in%20a%0Afigurative%20sense%2C%20or%20literally.%20In%20this%20paper%2C%20we%20employ%20tools%20from%20mechanistic%0Ainterpretability%20to%20trace%20how%20a%20large%20pretrained%20causal%20transformer%0A%28LLama3.2-1B-base%29%20deals%20with%20this%20ambiguity.%20We%20localize%20three%20steps%20of%20idiom%0Aprocessing%3A%20First%2C%20the%20idiom%27s%20figurative%20meaning%20is%20retrieved%20in%20early%0Aattention%20and%20MLP%20sublayers.%20We%20identify%20specific%20attention%20heads%20which%20boost%0Athe%20figurative%20meaning%20of%20the%20idiom%20while%20suppressing%20the%20idiom%27s%20literal%0Ainterpretation.%20The%20model%20subsequently%20represents%20the%20figurative%20representation%0Athrough%20an%20intermediate%20path.%20Meanwhile%2C%20a%20parallel%20bypass%20route%20forwards%0Aliteral%20interpretation%2C%20ensuring%20that%20a%20both%20reading%20remain%20available.%20Overall%2C%0Aour%20findings%20provide%20a%20mechanistic%20evidence%20for%20idiom%20comprehension%20in%20an%0Aautoregressive%20transformer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.01723v3&entry.124074799=Read"},
{"title": "MOGO: Residual Quantized Hierarchical Causal Transformer for\n  High-Quality and Real-Time 3D Human Motion Generation", "author": "Dongjie Fu and Tengjiao Sun and Pengcheng Fang and Xiaohao Cai and Hansung Kim", "abstract": "  Recent advances in transformer-based text-to-motion generation have led to\nimpressive progress in synthesizing high-quality human motion. Nevertheless,\njointly achieving high fidelity, streaming capability, real-time\nresponsiveness, and scalability remains a fundamental challenge. In this paper,\nwe propose MOGO (Motion Generation with One-pass), a novel autoregressive\nframework tailored for efficient and real-time 3D motion generation. MOGO\ncomprises two key components: (1) MoSA-VQ, a motion scale-adaptive residual\nvector quantization module that hierarchically discretizes motion sequences\nwith learnable scaling to produce compact yet expressive representations; and\n(2) RQHC-Transformer, a residual quantized hierarchical causal transformer that\ngenerates multi-layer motion tokens in a single forward pass, significantly\nreducing inference latency. To enhance semantic fidelity, we further introduce\na text condition alignment mechanism that improves motion decoding under\ntextual control. Extensive experiments on benchmark datasets including\nHumanML3D, KIT-ML, and CMP demonstrate that MOGO achieves competitive or\nsuperior generation quality compared to state-of-the-art transformer-based\nmethods, while offering substantial improvements in real-time performance,\nstreaming generation, and generalization under zero-shot settings.\n", "link": "http://arxiv.org/abs/2506.05952v1", "date": "2025-06-06", "relevancy": 2.4024, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6356}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5763}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOGO%3A%20Residual%20Quantized%20Hierarchical%20Causal%20Transformer%20for%0A%20%20High-Quality%20and%20Real-Time%203D%20Human%20Motion%20Generation&body=Title%3A%20MOGO%3A%20Residual%20Quantized%20Hierarchical%20Causal%20Transformer%20for%0A%20%20High-Quality%20and%20Real-Time%203D%20Human%20Motion%20Generation%0AAuthor%3A%20Dongjie%20Fu%20and%20Tengjiao%20Sun%20and%20Pengcheng%20Fang%20and%20Xiaohao%20Cai%20and%20Hansung%20Kim%0AAbstract%3A%20%20%20Recent%20advances%20in%20transformer-based%20text-to-motion%20generation%20have%20led%20to%0Aimpressive%20progress%20in%20synthesizing%20high-quality%20human%20motion.%20Nevertheless%2C%0Ajointly%20achieving%20high%20fidelity%2C%20streaming%20capability%2C%20real-time%0Aresponsiveness%2C%20and%20scalability%20remains%20a%20fundamental%20challenge.%20In%20this%20paper%2C%0Awe%20propose%20MOGO%20%28Motion%20Generation%20with%20One-pass%29%2C%20a%20novel%20autoregressive%0Aframework%20tailored%20for%20efficient%20and%20real-time%203D%20motion%20generation.%20MOGO%0Acomprises%20two%20key%20components%3A%20%281%29%20MoSA-VQ%2C%20a%20motion%20scale-adaptive%20residual%0Avector%20quantization%20module%20that%20hierarchically%20discretizes%20motion%20sequences%0Awith%20learnable%20scaling%20to%20produce%20compact%20yet%20expressive%20representations%3B%20and%0A%282%29%20RQHC-Transformer%2C%20a%20residual%20quantized%20hierarchical%20causal%20transformer%20that%0Agenerates%20multi-layer%20motion%20tokens%20in%20a%20single%20forward%20pass%2C%20significantly%0Areducing%20inference%20latency.%20To%20enhance%20semantic%20fidelity%2C%20we%20further%20introduce%0Aa%20text%20condition%20alignment%20mechanism%20that%20improves%20motion%20decoding%20under%0Atextual%20control.%20Extensive%20experiments%20on%20benchmark%20datasets%20including%0AHumanML3D%2C%20KIT-ML%2C%20and%20CMP%20demonstrate%20that%20MOGO%20achieves%20competitive%20or%0Asuperior%20generation%20quality%20compared%20to%20state-of-the-art%20transformer-based%0Amethods%2C%20while%20offering%20substantial%20improvements%20in%20real-time%20performance%2C%0Astreaming%20generation%2C%20and%20generalization%20under%20zero-shot%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05952v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOGO%253A%2520Residual%2520Quantized%2520Hierarchical%2520Causal%2520Transformer%2520for%250A%2520%2520High-Quality%2520and%2520Real-Time%25203D%2520Human%2520Motion%2520Generation%26entry.906535625%3DDongjie%2520Fu%2520and%2520Tengjiao%2520Sun%2520and%2520Pengcheng%2520Fang%2520and%2520Xiaohao%2520Cai%2520and%2520Hansung%2520Kim%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520transformer-based%2520text-to-motion%2520generation%2520have%2520led%2520to%250Aimpressive%2520progress%2520in%2520synthesizing%2520high-quality%2520human%2520motion.%2520Nevertheless%252C%250Ajointly%2520achieving%2520high%2520fidelity%252C%2520streaming%2520capability%252C%2520real-time%250Aresponsiveness%252C%2520and%2520scalability%2520remains%2520a%2520fundamental%2520challenge.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520MOGO%2520%2528Motion%2520Generation%2520with%2520One-pass%2529%252C%2520a%2520novel%2520autoregressive%250Aframework%2520tailored%2520for%2520efficient%2520and%2520real-time%25203D%2520motion%2520generation.%2520MOGO%250Acomprises%2520two%2520key%2520components%253A%2520%25281%2529%2520MoSA-VQ%252C%2520a%2520motion%2520scale-adaptive%2520residual%250Avector%2520quantization%2520module%2520that%2520hierarchically%2520discretizes%2520motion%2520sequences%250Awith%2520learnable%2520scaling%2520to%2520produce%2520compact%2520yet%2520expressive%2520representations%253B%2520and%250A%25282%2529%2520RQHC-Transformer%252C%2520a%2520residual%2520quantized%2520hierarchical%2520causal%2520transformer%2520that%250Agenerates%2520multi-layer%2520motion%2520tokens%2520in%2520a%2520single%2520forward%2520pass%252C%2520significantly%250Areducing%2520inference%2520latency.%2520To%2520enhance%2520semantic%2520fidelity%252C%2520we%2520further%2520introduce%250Aa%2520text%2520condition%2520alignment%2520mechanism%2520that%2520improves%2520motion%2520decoding%2520under%250Atextual%2520control.%2520Extensive%2520experiments%2520on%2520benchmark%2520datasets%2520including%250AHumanML3D%252C%2520KIT-ML%252C%2520and%2520CMP%2520demonstrate%2520that%2520MOGO%2520achieves%2520competitive%2520or%250Asuperior%2520generation%2520quality%2520compared%2520to%2520state-of-the-art%2520transformer-based%250Amethods%252C%2520while%2520offering%2520substantial%2520improvements%2520in%2520real-time%2520performance%252C%250Astreaming%2520generation%252C%2520and%2520generalization%2520under%2520zero-shot%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05952v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOGO%3A%20Residual%20Quantized%20Hierarchical%20Causal%20Transformer%20for%0A%20%20High-Quality%20and%20Real-Time%203D%20Human%20Motion%20Generation&entry.906535625=Dongjie%20Fu%20and%20Tengjiao%20Sun%20and%20Pengcheng%20Fang%20and%20Xiaohao%20Cai%20and%20Hansung%20Kim&entry.1292438233=%20%20Recent%20advances%20in%20transformer-based%20text-to-motion%20generation%20have%20led%20to%0Aimpressive%20progress%20in%20synthesizing%20high-quality%20human%20motion.%20Nevertheless%2C%0Ajointly%20achieving%20high%20fidelity%2C%20streaming%20capability%2C%20real-time%0Aresponsiveness%2C%20and%20scalability%20remains%20a%20fundamental%20challenge.%20In%20this%20paper%2C%0Awe%20propose%20MOGO%20%28Motion%20Generation%20with%20One-pass%29%2C%20a%20novel%20autoregressive%0Aframework%20tailored%20for%20efficient%20and%20real-time%203D%20motion%20generation.%20MOGO%0Acomprises%20two%20key%20components%3A%20%281%29%20MoSA-VQ%2C%20a%20motion%20scale-adaptive%20residual%0Avector%20quantization%20module%20that%20hierarchically%20discretizes%20motion%20sequences%0Awith%20learnable%20scaling%20to%20produce%20compact%20yet%20expressive%20representations%3B%20and%0A%282%29%20RQHC-Transformer%2C%20a%20residual%20quantized%20hierarchical%20causal%20transformer%20that%0Agenerates%20multi-layer%20motion%20tokens%20in%20a%20single%20forward%20pass%2C%20significantly%0Areducing%20inference%20latency.%20To%20enhance%20semantic%20fidelity%2C%20we%20further%20introduce%0Aa%20text%20condition%20alignment%20mechanism%20that%20improves%20motion%20decoding%20under%0Atextual%20control.%20Extensive%20experiments%20on%20benchmark%20datasets%20including%0AHumanML3D%2C%20KIT-ML%2C%20and%20CMP%20demonstrate%20that%20MOGO%20achieves%20competitive%20or%0Asuperior%20generation%20quality%20compared%20to%20state-of-the-art%20transformer-based%0Amethods%2C%20while%20offering%20substantial%20improvements%20in%20real-time%20performance%2C%0Astreaming%20generation%2C%20and%20generalization%20under%20zero-shot%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05952v1&entry.124074799=Read"},
{"title": "A Novel Large-scale Crop Dataset and Dual-stream Transformer Method for\n  Fine-grained Hierarchical Crop Classification from Integrated Hyperspectral\n  EnMAP Data and Multispectral Sentinel-2 Time Series", "author": "Wenyuan Li and Shunlin Liang and Yuxiang Zhang and Liqin Liu and Keyan Chen and Yongzhe Chen and Han Ma and Jianglei Xu and Yichuan Ma and Shikang Guan and Zhenwei Shi", "abstract": "  Fine-grained crop classification is crucial for precision agriculture and\nfood security monitoring. It requires simultaneous capture of both phenological\ndynamics (obtained from multi-temporal satellite data like Sentinel-2) and\nsubtle spectral variations (demanding nanometer-scale spectral resolution from\nhyperspectral imagery). Research combining these two modalities remains scarce\ncurrently due to challenges in hyperspectral data acquisition and crop types\nannotation costs. To address these issues, we construct a hierarchical\nhyperspectral crop dataset (H2Crop) by integrating 30m-resolution EnMAP\nhyperspectral data with Sentinel-2 time series. With over one million annotated\nfield parcels organized in a four-tier crop taxonomy, H2Crop establishes a\nvital benchmark for fine-grained agricultural crop classification and\nhyperspectral image processing. We propose a dual-stream Transformer\narchitecture that synergistically processes these modalities. It coordinates\ntwo specialized pathways: a spectral-spatial Transformer extracts fine-grained\nsignatures from hyperspectral EnMAP data, while a temporal Swin Transformer\nextracts crop growth patterns from Sentinel-2 time series. The designed\nhierarchy classification heads with hierarchical fusion then simultaneously\ndelivers multi-level classification across all taxonomic tiers. Experiments\ndemonstrate that adding hyperspectral EnMAP data to Sentinel-2 time series\nyields a 4.2% average F1-scores improvement (peaking at 6.3%). Extensive\ncomparisons also confirming our method's higher accuracy over existing deep\nlearning approaches for crop type classification and the consistent benefits of\nhyperspectral data across varying temporal windows and crop change scenarios.\nCodes and dataset will be available at https://github.com/flyakon/H2Crop and\nwww.glass.hku.hk\n  Keywords: Crop type classification, precision agriculture, remote sensing,\ndeep learning, hyperspectral data, Sentinel-2 time series, fine-grained crops\n", "link": "http://arxiv.org/abs/2506.06155v1", "date": "2025-06-06", "relevancy": 2.4022, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4834}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.481}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Large-scale%20Crop%20Dataset%20and%20Dual-stream%20Transformer%20Method%20for%0A%20%20Fine-grained%20Hierarchical%20Crop%20Classification%20from%20Integrated%20Hyperspectral%0A%20%20EnMAP%20Data%20and%20Multispectral%20Sentinel-2%20Time%20Series&body=Title%3A%20A%20Novel%20Large-scale%20Crop%20Dataset%20and%20Dual-stream%20Transformer%20Method%20for%0A%20%20Fine-grained%20Hierarchical%20Crop%20Classification%20from%20Integrated%20Hyperspectral%0A%20%20EnMAP%20Data%20and%20Multispectral%20Sentinel-2%20Time%20Series%0AAuthor%3A%20Wenyuan%20Li%20and%20Shunlin%20Liang%20and%20Yuxiang%20Zhang%20and%20Liqin%20Liu%20and%20Keyan%20Chen%20and%20Yongzhe%20Chen%20and%20Han%20Ma%20and%20Jianglei%20Xu%20and%20Yichuan%20Ma%20and%20Shikang%20Guan%20and%20Zhenwei%20Shi%0AAbstract%3A%20%20%20Fine-grained%20crop%20classification%20is%20crucial%20for%20precision%20agriculture%20and%0Afood%20security%20monitoring.%20It%20requires%20simultaneous%20capture%20of%20both%20phenological%0Adynamics%20%28obtained%20from%20multi-temporal%20satellite%20data%20like%20Sentinel-2%29%20and%0Asubtle%20spectral%20variations%20%28demanding%20nanometer-scale%20spectral%20resolution%20from%0Ahyperspectral%20imagery%29.%20Research%20combining%20these%20two%20modalities%20remains%20scarce%0Acurrently%20due%20to%20challenges%20in%20hyperspectral%20data%20acquisition%20and%20crop%20types%0Aannotation%20costs.%20To%20address%20these%20issues%2C%20we%20construct%20a%20hierarchical%0Ahyperspectral%20crop%20dataset%20%28H2Crop%29%20by%20integrating%2030m-resolution%20EnMAP%0Ahyperspectral%20data%20with%20Sentinel-2%20time%20series.%20With%20over%20one%20million%20annotated%0Afield%20parcels%20organized%20in%20a%20four-tier%20crop%20taxonomy%2C%20H2Crop%20establishes%20a%0Avital%20benchmark%20for%20fine-grained%20agricultural%20crop%20classification%20and%0Ahyperspectral%20image%20processing.%20We%20propose%20a%20dual-stream%20Transformer%0Aarchitecture%20that%20synergistically%20processes%20these%20modalities.%20It%20coordinates%0Atwo%20specialized%20pathways%3A%20a%20spectral-spatial%20Transformer%20extracts%20fine-grained%0Asignatures%20from%20hyperspectral%20EnMAP%20data%2C%20while%20a%20temporal%20Swin%20Transformer%0Aextracts%20crop%20growth%20patterns%20from%20Sentinel-2%20time%20series.%20The%20designed%0Ahierarchy%20classification%20heads%20with%20hierarchical%20fusion%20then%20simultaneously%0Adelivers%20multi-level%20classification%20across%20all%20taxonomic%20tiers.%20Experiments%0Ademonstrate%20that%20adding%20hyperspectral%20EnMAP%20data%20to%20Sentinel-2%20time%20series%0Ayields%20a%204.2%25%20average%20F1-scores%20improvement%20%28peaking%20at%206.3%25%29.%20Extensive%0Acomparisons%20also%20confirming%20our%20method%27s%20higher%20accuracy%20over%20existing%20deep%0Alearning%20approaches%20for%20crop%20type%20classification%20and%20the%20consistent%20benefits%20of%0Ahyperspectral%20data%20across%20varying%20temporal%20windows%20and%20crop%20change%20scenarios.%0ACodes%20and%20dataset%20will%20be%20available%20at%20https%3A//github.com/flyakon/H2Crop%20and%0Awww.glass.hku.hk%0A%20%20Keywords%3A%20Crop%20type%20classification%2C%20precision%20agriculture%2C%20remote%20sensing%2C%0Adeep%20learning%2C%20hyperspectral%20data%2C%20Sentinel-2%20time%20series%2C%20fine-grained%20crops%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06155v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Novel%2520Large-scale%2520Crop%2520Dataset%2520and%2520Dual-stream%2520Transformer%2520Method%2520for%250A%2520%2520Fine-grained%2520Hierarchical%2520Crop%2520Classification%2520from%2520Integrated%2520Hyperspectral%250A%2520%2520EnMAP%2520Data%2520and%2520Multispectral%2520Sentinel-2%2520Time%2520Series%26entry.906535625%3DWenyuan%2520Li%2520and%2520Shunlin%2520Liang%2520and%2520Yuxiang%2520Zhang%2520and%2520Liqin%2520Liu%2520and%2520Keyan%2520Chen%2520and%2520Yongzhe%2520Chen%2520and%2520Han%2520Ma%2520and%2520Jianglei%2520Xu%2520and%2520Yichuan%2520Ma%2520and%2520Shikang%2520Guan%2520and%2520Zhenwei%2520Shi%26entry.1292438233%3D%2520%2520Fine-grained%2520crop%2520classification%2520is%2520crucial%2520for%2520precision%2520agriculture%2520and%250Afood%2520security%2520monitoring.%2520It%2520requires%2520simultaneous%2520capture%2520of%2520both%2520phenological%250Adynamics%2520%2528obtained%2520from%2520multi-temporal%2520satellite%2520data%2520like%2520Sentinel-2%2529%2520and%250Asubtle%2520spectral%2520variations%2520%2528demanding%2520nanometer-scale%2520spectral%2520resolution%2520from%250Ahyperspectral%2520imagery%2529.%2520Research%2520combining%2520these%2520two%2520modalities%2520remains%2520scarce%250Acurrently%2520due%2520to%2520challenges%2520in%2520hyperspectral%2520data%2520acquisition%2520and%2520crop%2520types%250Aannotation%2520costs.%2520To%2520address%2520these%2520issues%252C%2520we%2520construct%2520a%2520hierarchical%250Ahyperspectral%2520crop%2520dataset%2520%2528H2Crop%2529%2520by%2520integrating%252030m-resolution%2520EnMAP%250Ahyperspectral%2520data%2520with%2520Sentinel-2%2520time%2520series.%2520With%2520over%2520one%2520million%2520annotated%250Afield%2520parcels%2520organized%2520in%2520a%2520four-tier%2520crop%2520taxonomy%252C%2520H2Crop%2520establishes%2520a%250Avital%2520benchmark%2520for%2520fine-grained%2520agricultural%2520crop%2520classification%2520and%250Ahyperspectral%2520image%2520processing.%2520We%2520propose%2520a%2520dual-stream%2520Transformer%250Aarchitecture%2520that%2520synergistically%2520processes%2520these%2520modalities.%2520It%2520coordinates%250Atwo%2520specialized%2520pathways%253A%2520a%2520spectral-spatial%2520Transformer%2520extracts%2520fine-grained%250Asignatures%2520from%2520hyperspectral%2520EnMAP%2520data%252C%2520while%2520a%2520temporal%2520Swin%2520Transformer%250Aextracts%2520crop%2520growth%2520patterns%2520from%2520Sentinel-2%2520time%2520series.%2520The%2520designed%250Ahierarchy%2520classification%2520heads%2520with%2520hierarchical%2520fusion%2520then%2520simultaneously%250Adelivers%2520multi-level%2520classification%2520across%2520all%2520taxonomic%2520tiers.%2520Experiments%250Ademonstrate%2520that%2520adding%2520hyperspectral%2520EnMAP%2520data%2520to%2520Sentinel-2%2520time%2520series%250Ayields%2520a%25204.2%2525%2520average%2520F1-scores%2520improvement%2520%2528peaking%2520at%25206.3%2525%2529.%2520Extensive%250Acomparisons%2520also%2520confirming%2520our%2520method%2527s%2520higher%2520accuracy%2520over%2520existing%2520deep%250Alearning%2520approaches%2520for%2520crop%2520type%2520classification%2520and%2520the%2520consistent%2520benefits%2520of%250Ahyperspectral%2520data%2520across%2520varying%2520temporal%2520windows%2520and%2520crop%2520change%2520scenarios.%250ACodes%2520and%2520dataset%2520will%2520be%2520available%2520at%2520https%253A//github.com/flyakon/H2Crop%2520and%250Awww.glass.hku.hk%250A%2520%2520Keywords%253A%2520Crop%2520type%2520classification%252C%2520precision%2520agriculture%252C%2520remote%2520sensing%252C%250Adeep%2520learning%252C%2520hyperspectral%2520data%252C%2520Sentinel-2%2520time%2520series%252C%2520fine-grained%2520crops%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06155v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Large-scale%20Crop%20Dataset%20and%20Dual-stream%20Transformer%20Method%20for%0A%20%20Fine-grained%20Hierarchical%20Crop%20Classification%20from%20Integrated%20Hyperspectral%0A%20%20EnMAP%20Data%20and%20Multispectral%20Sentinel-2%20Time%20Series&entry.906535625=Wenyuan%20Li%20and%20Shunlin%20Liang%20and%20Yuxiang%20Zhang%20and%20Liqin%20Liu%20and%20Keyan%20Chen%20and%20Yongzhe%20Chen%20and%20Han%20Ma%20and%20Jianglei%20Xu%20and%20Yichuan%20Ma%20and%20Shikang%20Guan%20and%20Zhenwei%20Shi&entry.1292438233=%20%20Fine-grained%20crop%20classification%20is%20crucial%20for%20precision%20agriculture%20and%0Afood%20security%20monitoring.%20It%20requires%20simultaneous%20capture%20of%20both%20phenological%0Adynamics%20%28obtained%20from%20multi-temporal%20satellite%20data%20like%20Sentinel-2%29%20and%0Asubtle%20spectral%20variations%20%28demanding%20nanometer-scale%20spectral%20resolution%20from%0Ahyperspectral%20imagery%29.%20Research%20combining%20these%20two%20modalities%20remains%20scarce%0Acurrently%20due%20to%20challenges%20in%20hyperspectral%20data%20acquisition%20and%20crop%20types%0Aannotation%20costs.%20To%20address%20these%20issues%2C%20we%20construct%20a%20hierarchical%0Ahyperspectral%20crop%20dataset%20%28H2Crop%29%20by%20integrating%2030m-resolution%20EnMAP%0Ahyperspectral%20data%20with%20Sentinel-2%20time%20series.%20With%20over%20one%20million%20annotated%0Afield%20parcels%20organized%20in%20a%20four-tier%20crop%20taxonomy%2C%20H2Crop%20establishes%20a%0Avital%20benchmark%20for%20fine-grained%20agricultural%20crop%20classification%20and%0Ahyperspectral%20image%20processing.%20We%20propose%20a%20dual-stream%20Transformer%0Aarchitecture%20that%20synergistically%20processes%20these%20modalities.%20It%20coordinates%0Atwo%20specialized%20pathways%3A%20a%20spectral-spatial%20Transformer%20extracts%20fine-grained%0Asignatures%20from%20hyperspectral%20EnMAP%20data%2C%20while%20a%20temporal%20Swin%20Transformer%0Aextracts%20crop%20growth%20patterns%20from%20Sentinel-2%20time%20series.%20The%20designed%0Ahierarchy%20classification%20heads%20with%20hierarchical%20fusion%20then%20simultaneously%0Adelivers%20multi-level%20classification%20across%20all%20taxonomic%20tiers.%20Experiments%0Ademonstrate%20that%20adding%20hyperspectral%20EnMAP%20data%20to%20Sentinel-2%20time%20series%0Ayields%20a%204.2%25%20average%20F1-scores%20improvement%20%28peaking%20at%206.3%25%29.%20Extensive%0Acomparisons%20also%20confirming%20our%20method%27s%20higher%20accuracy%20over%20existing%20deep%0Alearning%20approaches%20for%20crop%20type%20classification%20and%20the%20consistent%20benefits%20of%0Ahyperspectral%20data%20across%20varying%20temporal%20windows%20and%20crop%20change%20scenarios.%0ACodes%20and%20dataset%20will%20be%20available%20at%20https%3A//github.com/flyakon/H2Crop%20and%0Awww.glass.hku.hk%0A%20%20Keywords%3A%20Crop%20type%20classification%2C%20precision%20agriculture%2C%20remote%20sensing%2C%0Adeep%20learning%2C%20hyperspectral%20data%2C%20Sentinel-2%20time%20series%2C%20fine-grained%20crops%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06155v1&entry.124074799=Read"},
{"title": "Bootstrapping World Models from Dynamics Models in Multimodal Foundation\n  Models", "author": "Yifu Qiu and Yftah Ziser and Anna Korhonen and Shay B. Cohen and Edoardo M. Ponti", "abstract": "  To what extent do vision-and-language foundation models possess a realistic\nworld model (observation $\\times$ action $\\rightarrow$ observation) and a\ndynamics model (observation $\\times$ observation $\\rightarrow$ action), when\nactions are expressed through language? While open-source foundation models\nstruggle with both, we find that fine-tuning them to acquire a dynamics model\nthrough supervision is significantly easier than acquiring a world model. In\nturn, dynamics models can be used to bootstrap world models through two main\nstrategies: 1) weakly supervised learning from synthetic data and 2) inference\ntime verification. Firstly, the dynamics model can annotate actions for\nunlabelled pairs of video frame observations to expand the training data. We\nfurther propose a new objective, where image tokens in observation pairs are\nweighted by their importance, as predicted by a recognition model. Secondly,\nthe dynamics models can assign rewards to multiple samples of the world model\nto score them, effectively guiding search at inference time. We evaluate the\nworld models resulting from both strategies through the task of action-centric\nimage editing on Aurora-Bench. Our best model achieves a performance\ncompetitive with state-of-the-art image editing models, improving on them by a\nmargin of $15\\%$ on real-world subsets according to GPT4o-as-judge, and\nachieving the best average human evaluation across all subsets of Aurora-Bench.\n", "link": "http://arxiv.org/abs/2506.06006v1", "date": "2025-06-06", "relevancy": 2.3932, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6045}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6045}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bootstrapping%20World%20Models%20from%20Dynamics%20Models%20in%20Multimodal%20Foundation%0A%20%20Models&body=Title%3A%20Bootstrapping%20World%20Models%20from%20Dynamics%20Models%20in%20Multimodal%20Foundation%0A%20%20Models%0AAuthor%3A%20Yifu%20Qiu%20and%20Yftah%20Ziser%20and%20Anna%20Korhonen%20and%20Shay%20B.%20Cohen%20and%20Edoardo%20M.%20Ponti%0AAbstract%3A%20%20%20To%20what%20extent%20do%20vision-and-language%20foundation%20models%20possess%20a%20realistic%0Aworld%20model%20%28observation%20%24%5Ctimes%24%20action%20%24%5Crightarrow%24%20observation%29%20and%20a%0Adynamics%20model%20%28observation%20%24%5Ctimes%24%20observation%20%24%5Crightarrow%24%20action%29%2C%20when%0Aactions%20are%20expressed%20through%20language%3F%20While%20open-source%20foundation%20models%0Astruggle%20with%20both%2C%20we%20find%20that%20fine-tuning%20them%20to%20acquire%20a%20dynamics%20model%0Athrough%20supervision%20is%20significantly%20easier%20than%20acquiring%20a%20world%20model.%20In%0Aturn%2C%20dynamics%20models%20can%20be%20used%20to%20bootstrap%20world%20models%20through%20two%20main%0Astrategies%3A%201%29%20weakly%20supervised%20learning%20from%20synthetic%20data%20and%202%29%20inference%0Atime%20verification.%20Firstly%2C%20the%20dynamics%20model%20can%20annotate%20actions%20for%0Aunlabelled%20pairs%20of%20video%20frame%20observations%20to%20expand%20the%20training%20data.%20We%0Afurther%20propose%20a%20new%20objective%2C%20where%20image%20tokens%20in%20observation%20pairs%20are%0Aweighted%20by%20their%20importance%2C%20as%20predicted%20by%20a%20recognition%20model.%20Secondly%2C%0Athe%20dynamics%20models%20can%20assign%20rewards%20to%20multiple%20samples%20of%20the%20world%20model%0Ato%20score%20them%2C%20effectively%20guiding%20search%20at%20inference%20time.%20We%20evaluate%20the%0Aworld%20models%20resulting%20from%20both%20strategies%20through%20the%20task%20of%20action-centric%0Aimage%20editing%20on%20Aurora-Bench.%20Our%20best%20model%20achieves%20a%20performance%0Acompetitive%20with%20state-of-the-art%20image%20editing%20models%2C%20improving%20on%20them%20by%20a%0Amargin%20of%20%2415%5C%25%24%20on%20real-world%20subsets%20according%20to%20GPT4o-as-judge%2C%20and%0Aachieving%20the%20best%20average%20human%20evaluation%20across%20all%20subsets%20of%20Aurora-Bench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06006v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBootstrapping%2520World%2520Models%2520from%2520Dynamics%2520Models%2520in%2520Multimodal%2520Foundation%250A%2520%2520Models%26entry.906535625%3DYifu%2520Qiu%2520and%2520Yftah%2520Ziser%2520and%2520Anna%2520Korhonen%2520and%2520Shay%2520B.%2520Cohen%2520and%2520Edoardo%2520M.%2520Ponti%26entry.1292438233%3D%2520%2520To%2520what%2520extent%2520do%2520vision-and-language%2520foundation%2520models%2520possess%2520a%2520realistic%250Aworld%2520model%2520%2528observation%2520%2524%255Ctimes%2524%2520action%2520%2524%255Crightarrow%2524%2520observation%2529%2520and%2520a%250Adynamics%2520model%2520%2528observation%2520%2524%255Ctimes%2524%2520observation%2520%2524%255Crightarrow%2524%2520action%2529%252C%2520when%250Aactions%2520are%2520expressed%2520through%2520language%253F%2520While%2520open-source%2520foundation%2520models%250Astruggle%2520with%2520both%252C%2520we%2520find%2520that%2520fine-tuning%2520them%2520to%2520acquire%2520a%2520dynamics%2520model%250Athrough%2520supervision%2520is%2520significantly%2520easier%2520than%2520acquiring%2520a%2520world%2520model.%2520In%250Aturn%252C%2520dynamics%2520models%2520can%2520be%2520used%2520to%2520bootstrap%2520world%2520models%2520through%2520two%2520main%250Astrategies%253A%25201%2529%2520weakly%2520supervised%2520learning%2520from%2520synthetic%2520data%2520and%25202%2529%2520inference%250Atime%2520verification.%2520Firstly%252C%2520the%2520dynamics%2520model%2520can%2520annotate%2520actions%2520for%250Aunlabelled%2520pairs%2520of%2520video%2520frame%2520observations%2520to%2520expand%2520the%2520training%2520data.%2520We%250Afurther%2520propose%2520a%2520new%2520objective%252C%2520where%2520image%2520tokens%2520in%2520observation%2520pairs%2520are%250Aweighted%2520by%2520their%2520importance%252C%2520as%2520predicted%2520by%2520a%2520recognition%2520model.%2520Secondly%252C%250Athe%2520dynamics%2520models%2520can%2520assign%2520rewards%2520to%2520multiple%2520samples%2520of%2520the%2520world%2520model%250Ato%2520score%2520them%252C%2520effectively%2520guiding%2520search%2520at%2520inference%2520time.%2520We%2520evaluate%2520the%250Aworld%2520models%2520resulting%2520from%2520both%2520strategies%2520through%2520the%2520task%2520of%2520action-centric%250Aimage%2520editing%2520on%2520Aurora-Bench.%2520Our%2520best%2520model%2520achieves%2520a%2520performance%250Acompetitive%2520with%2520state-of-the-art%2520image%2520editing%2520models%252C%2520improving%2520on%2520them%2520by%2520a%250Amargin%2520of%2520%252415%255C%2525%2524%2520on%2520real-world%2520subsets%2520according%2520to%2520GPT4o-as-judge%252C%2520and%250Aachieving%2520the%2520best%2520average%2520human%2520evaluation%2520across%2520all%2520subsets%2520of%2520Aurora-Bench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06006v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bootstrapping%20World%20Models%20from%20Dynamics%20Models%20in%20Multimodal%20Foundation%0A%20%20Models&entry.906535625=Yifu%20Qiu%20and%20Yftah%20Ziser%20and%20Anna%20Korhonen%20and%20Shay%20B.%20Cohen%20and%20Edoardo%20M.%20Ponti&entry.1292438233=%20%20To%20what%20extent%20do%20vision-and-language%20foundation%20models%20possess%20a%20realistic%0Aworld%20model%20%28observation%20%24%5Ctimes%24%20action%20%24%5Crightarrow%24%20observation%29%20and%20a%0Adynamics%20model%20%28observation%20%24%5Ctimes%24%20observation%20%24%5Crightarrow%24%20action%29%2C%20when%0Aactions%20are%20expressed%20through%20language%3F%20While%20open-source%20foundation%20models%0Astruggle%20with%20both%2C%20we%20find%20that%20fine-tuning%20them%20to%20acquire%20a%20dynamics%20model%0Athrough%20supervision%20is%20significantly%20easier%20than%20acquiring%20a%20world%20model.%20In%0Aturn%2C%20dynamics%20models%20can%20be%20used%20to%20bootstrap%20world%20models%20through%20two%20main%0Astrategies%3A%201%29%20weakly%20supervised%20learning%20from%20synthetic%20data%20and%202%29%20inference%0Atime%20verification.%20Firstly%2C%20the%20dynamics%20model%20can%20annotate%20actions%20for%0Aunlabelled%20pairs%20of%20video%20frame%20observations%20to%20expand%20the%20training%20data.%20We%0Afurther%20propose%20a%20new%20objective%2C%20where%20image%20tokens%20in%20observation%20pairs%20are%0Aweighted%20by%20their%20importance%2C%20as%20predicted%20by%20a%20recognition%20model.%20Secondly%2C%0Athe%20dynamics%20models%20can%20assign%20rewards%20to%20multiple%20samples%20of%20the%20world%20model%0Ato%20score%20them%2C%20effectively%20guiding%20search%20at%20inference%20time.%20We%20evaluate%20the%0Aworld%20models%20resulting%20from%20both%20strategies%20through%20the%20task%20of%20action-centric%0Aimage%20editing%20on%20Aurora-Bench.%20Our%20best%20model%20achieves%20a%20performance%0Acompetitive%20with%20state-of-the-art%20image%20editing%20models%2C%20improving%20on%20them%20by%20a%0Amargin%20of%20%2415%5C%25%24%20on%20real-world%20subsets%20according%20to%20GPT4o-as-judge%2C%20and%0Aachieving%20the%20best%20average%20human%20evaluation%20across%20all%20subsets%20of%20Aurora-Bench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06006v1&entry.124074799=Read"},
{"title": "Subspecialty-Specific Foundation Model for Intelligent Gastrointestinal\n  Pathology", "author": "Lianghui Zhu and Xitong Ling and Minxi Ouyang and Xiaoping Liu and Tian Guan and Mingxi Fu and Zhiqiang Cheng and Fanglei Fu and Maomao Zeng and Liming Liu and Song Duan and Qiang Huang and Ying Xiao and Jianming Li and Shanming Lu and Zhenghua Piao and Mingxi Zhu and Yibo Jin and Shan Xu and Qiming He and Yizhi Wang and Junru Cheng and Xuanyu Wang and Luxi Xie and Houqiang Li and Sufang Tian and Yonghong He", "abstract": "  Gastrointestinal (GI) diseases represent a clinically significant burden,\nnecessitating precise diagnostic approaches to optimize patient outcomes.\nConventional histopathological diagnosis suffers from limited reproducibility\nand diagnostic variability. To overcome these limitations, we develop Digepath,\na specialized foundation model for GI pathology. Our framework introduces a\ndual-phase iterative optimization strategy combining pretraining with\nfine-screening, specifically designed to address the detection of sparsely\ndistributed lesion areas in whole-slide images. Digepath is pretrained on over\n353 million multi-scale images from 210,043 H&E-stained slides of GI diseases.\nIt attains state-of-the-art performance on 33 out of 34 tasks related to GI\npathology, including pathological diagnosis, protein expression status\nprediction, gene mutation prediction, and prognosis evaluation. We further\ntranslate the intelligent screening module for early GI cancer and achieve\nnear-perfect 99.70% sensitivity across nine independent medical institutions.\nThis work not only advances AI-driven precision pathology for GI diseases but\nalso bridge critical gaps in histopathological practice.\n", "link": "http://arxiv.org/abs/2505.21928v2", "date": "2025-06-06", "relevancy": 2.391, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4799}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4799}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Subspecialty-Specific%20Foundation%20Model%20for%20Intelligent%20Gastrointestinal%0A%20%20Pathology&body=Title%3A%20Subspecialty-Specific%20Foundation%20Model%20for%20Intelligent%20Gastrointestinal%0A%20%20Pathology%0AAuthor%3A%20Lianghui%20Zhu%20and%20Xitong%20Ling%20and%20Minxi%20Ouyang%20and%20Xiaoping%20Liu%20and%20Tian%20Guan%20and%20Mingxi%20Fu%20and%20Zhiqiang%20Cheng%20and%20Fanglei%20Fu%20and%20Maomao%20Zeng%20and%20Liming%20Liu%20and%20Song%20Duan%20and%20Qiang%20Huang%20and%20Ying%20Xiao%20and%20Jianming%20Li%20and%20Shanming%20Lu%20and%20Zhenghua%20Piao%20and%20Mingxi%20Zhu%20and%20Yibo%20Jin%20and%20Shan%20Xu%20and%20Qiming%20He%20and%20Yizhi%20Wang%20and%20Junru%20Cheng%20and%20Xuanyu%20Wang%20and%20Luxi%20Xie%20and%20Houqiang%20Li%20and%20Sufang%20Tian%20and%20Yonghong%20He%0AAbstract%3A%20%20%20Gastrointestinal%20%28GI%29%20diseases%20represent%20a%20clinically%20significant%20burden%2C%0Anecessitating%20precise%20diagnostic%20approaches%20to%20optimize%20patient%20outcomes.%0AConventional%20histopathological%20diagnosis%20suffers%20from%20limited%20reproducibility%0Aand%20diagnostic%20variability.%20To%20overcome%20these%20limitations%2C%20we%20develop%20Digepath%2C%0Aa%20specialized%20foundation%20model%20for%20GI%20pathology.%20Our%20framework%20introduces%20a%0Adual-phase%20iterative%20optimization%20strategy%20combining%20pretraining%20with%0Afine-screening%2C%20specifically%20designed%20to%20address%20the%20detection%20of%20sparsely%0Adistributed%20lesion%20areas%20in%20whole-slide%20images.%20Digepath%20is%20pretrained%20on%20over%0A353%20million%20multi-scale%20images%20from%20210%2C043%20H%26E-stained%20slides%20of%20GI%20diseases.%0AIt%20attains%20state-of-the-art%20performance%20on%2033%20out%20of%2034%20tasks%20related%20to%20GI%0Apathology%2C%20including%20pathological%20diagnosis%2C%20protein%20expression%20status%0Aprediction%2C%20gene%20mutation%20prediction%2C%20and%20prognosis%20evaluation.%20We%20further%0Atranslate%20the%20intelligent%20screening%20module%20for%20early%20GI%20cancer%20and%20achieve%0Anear-perfect%2099.70%25%20sensitivity%20across%20nine%20independent%20medical%20institutions.%0AThis%20work%20not%20only%20advances%20AI-driven%20precision%20pathology%20for%20GI%20diseases%20but%0Aalso%20bridge%20critical%20gaps%20in%20histopathological%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21928v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSubspecialty-Specific%2520Foundation%2520Model%2520for%2520Intelligent%2520Gastrointestinal%250A%2520%2520Pathology%26entry.906535625%3DLianghui%2520Zhu%2520and%2520Xitong%2520Ling%2520and%2520Minxi%2520Ouyang%2520and%2520Xiaoping%2520Liu%2520and%2520Tian%2520Guan%2520and%2520Mingxi%2520Fu%2520and%2520Zhiqiang%2520Cheng%2520and%2520Fanglei%2520Fu%2520and%2520Maomao%2520Zeng%2520and%2520Liming%2520Liu%2520and%2520Song%2520Duan%2520and%2520Qiang%2520Huang%2520and%2520Ying%2520Xiao%2520and%2520Jianming%2520Li%2520and%2520Shanming%2520Lu%2520and%2520Zhenghua%2520Piao%2520and%2520Mingxi%2520Zhu%2520and%2520Yibo%2520Jin%2520and%2520Shan%2520Xu%2520and%2520Qiming%2520He%2520and%2520Yizhi%2520Wang%2520and%2520Junru%2520Cheng%2520and%2520Xuanyu%2520Wang%2520and%2520Luxi%2520Xie%2520and%2520Houqiang%2520Li%2520and%2520Sufang%2520Tian%2520and%2520Yonghong%2520He%26entry.1292438233%3D%2520%2520Gastrointestinal%2520%2528GI%2529%2520diseases%2520represent%2520a%2520clinically%2520significant%2520burden%252C%250Anecessitating%2520precise%2520diagnostic%2520approaches%2520to%2520optimize%2520patient%2520outcomes.%250AConventional%2520histopathological%2520diagnosis%2520suffers%2520from%2520limited%2520reproducibility%250Aand%2520diagnostic%2520variability.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520develop%2520Digepath%252C%250Aa%2520specialized%2520foundation%2520model%2520for%2520GI%2520pathology.%2520Our%2520framework%2520introduces%2520a%250Adual-phase%2520iterative%2520optimization%2520strategy%2520combining%2520pretraining%2520with%250Afine-screening%252C%2520specifically%2520designed%2520to%2520address%2520the%2520detection%2520of%2520sparsely%250Adistributed%2520lesion%2520areas%2520in%2520whole-slide%2520images.%2520Digepath%2520is%2520pretrained%2520on%2520over%250A353%2520million%2520multi-scale%2520images%2520from%2520210%252C043%2520H%2526E-stained%2520slides%2520of%2520GI%2520diseases.%250AIt%2520attains%2520state-of-the-art%2520performance%2520on%252033%2520out%2520of%252034%2520tasks%2520related%2520to%2520GI%250Apathology%252C%2520including%2520pathological%2520diagnosis%252C%2520protein%2520expression%2520status%250Aprediction%252C%2520gene%2520mutation%2520prediction%252C%2520and%2520prognosis%2520evaluation.%2520We%2520further%250Atranslate%2520the%2520intelligent%2520screening%2520module%2520for%2520early%2520GI%2520cancer%2520and%2520achieve%250Anear-perfect%252099.70%2525%2520sensitivity%2520across%2520nine%2520independent%2520medical%2520institutions.%250AThis%2520work%2520not%2520only%2520advances%2520AI-driven%2520precision%2520pathology%2520for%2520GI%2520diseases%2520but%250Aalso%2520bridge%2520critical%2520gaps%2520in%2520histopathological%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21928v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Subspecialty-Specific%20Foundation%20Model%20for%20Intelligent%20Gastrointestinal%0A%20%20Pathology&entry.906535625=Lianghui%20Zhu%20and%20Xitong%20Ling%20and%20Minxi%20Ouyang%20and%20Xiaoping%20Liu%20and%20Tian%20Guan%20and%20Mingxi%20Fu%20and%20Zhiqiang%20Cheng%20and%20Fanglei%20Fu%20and%20Maomao%20Zeng%20and%20Liming%20Liu%20and%20Song%20Duan%20and%20Qiang%20Huang%20and%20Ying%20Xiao%20and%20Jianming%20Li%20and%20Shanming%20Lu%20and%20Zhenghua%20Piao%20and%20Mingxi%20Zhu%20and%20Yibo%20Jin%20and%20Shan%20Xu%20and%20Qiming%20He%20and%20Yizhi%20Wang%20and%20Junru%20Cheng%20and%20Xuanyu%20Wang%20and%20Luxi%20Xie%20and%20Houqiang%20Li%20and%20Sufang%20Tian%20and%20Yonghong%20He&entry.1292438233=%20%20Gastrointestinal%20%28GI%29%20diseases%20represent%20a%20clinically%20significant%20burden%2C%0Anecessitating%20precise%20diagnostic%20approaches%20to%20optimize%20patient%20outcomes.%0AConventional%20histopathological%20diagnosis%20suffers%20from%20limited%20reproducibility%0Aand%20diagnostic%20variability.%20To%20overcome%20these%20limitations%2C%20we%20develop%20Digepath%2C%0Aa%20specialized%20foundation%20model%20for%20GI%20pathology.%20Our%20framework%20introduces%20a%0Adual-phase%20iterative%20optimization%20strategy%20combining%20pretraining%20with%0Afine-screening%2C%20specifically%20designed%20to%20address%20the%20detection%20of%20sparsely%0Adistributed%20lesion%20areas%20in%20whole-slide%20images.%20Digepath%20is%20pretrained%20on%20over%0A353%20million%20multi-scale%20images%20from%20210%2C043%20H%26E-stained%20slides%20of%20GI%20diseases.%0AIt%20attains%20state-of-the-art%20performance%20on%2033%20out%20of%2034%20tasks%20related%20to%20GI%0Apathology%2C%20including%20pathological%20diagnosis%2C%20protein%20expression%20status%0Aprediction%2C%20gene%20mutation%20prediction%2C%20and%20prognosis%20evaluation.%20We%20further%0Atranslate%20the%20intelligent%20screening%20module%20for%20early%20GI%20cancer%20and%20achieve%0Anear-perfect%2099.70%25%20sensitivity%20across%20nine%20independent%20medical%20institutions.%0AThis%20work%20not%20only%20advances%20AI-driven%20precision%20pathology%20for%20GI%20diseases%20but%0Aalso%20bridge%20critical%20gaps%20in%20histopathological%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21928v2&entry.124074799=Read"},
{"title": "MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness\n  Against VLM-based Attacks", "author": "Zonglin Wu and Yule Xue and Xin Wei and Yiren Song", "abstract": "  As automated attack techniques rapidly advance, CAPTCHAs remain a critical\ndefense mechanism against malicious bots. However, existing CAPTCHA schemes\nencompass a diverse range of modalities -- from static distorted text and\nobfuscated images to interactive clicks, sliding puzzles, and logic-based\nquestions -- yet the community still lacks a unified, large-scale, multimodal\nbenchmark to rigorously evaluate their security robustness. To address this\ngap, we introduce MCA-Bench, a comprehensive and reproducible benchmarking\nsuite that integrates heterogeneous CAPTCHA types into a single evaluation\nprotocol. Leveraging a shared vision-language model backbone, we fine-tune\nspecialized cracking agents for each CAPTCHA category, enabling consistent,\ncross-modal assessments. Extensive experiments reveal that MCA-Bench\neffectively maps the vulnerability spectrum of modern CAPTCHA designs under\nvaried attack settings, and crucially offers the first quantitative analysis of\nhow challenge complexity, interaction depth, and model solvability interrelate.\nBased on these findings, we propose three actionable design principles and\nidentify key open challenges, laying the groundwork for systematic CAPTCHA\nhardening, fair benchmarking, and broader community collaboration. Datasets and\ncode are available online.\n", "link": "http://arxiv.org/abs/2506.05982v1", "date": "2025-06-06", "relevancy": 2.3875, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.478}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.478}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4764}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MCA-Bench%3A%20A%20Multimodal%20Benchmark%20for%20Evaluating%20CAPTCHA%20Robustness%0A%20%20Against%20VLM-based%20Attacks&body=Title%3A%20MCA-Bench%3A%20A%20Multimodal%20Benchmark%20for%20Evaluating%20CAPTCHA%20Robustness%0A%20%20Against%20VLM-based%20Attacks%0AAuthor%3A%20Zonglin%20Wu%20and%20Yule%20Xue%20and%20Xin%20Wei%20and%20Yiren%20Song%0AAbstract%3A%20%20%20As%20automated%20attack%20techniques%20rapidly%20advance%2C%20CAPTCHAs%20remain%20a%20critical%0Adefense%20mechanism%20against%20malicious%20bots.%20However%2C%20existing%20CAPTCHA%20schemes%0Aencompass%20a%20diverse%20range%20of%20modalities%20--%20from%20static%20distorted%20text%20and%0Aobfuscated%20images%20to%20interactive%20clicks%2C%20sliding%20puzzles%2C%20and%20logic-based%0Aquestions%20--%20yet%20the%20community%20still%20lacks%20a%20unified%2C%20large-scale%2C%20multimodal%0Abenchmark%20to%20rigorously%20evaluate%20their%20security%20robustness.%20To%20address%20this%0Agap%2C%20we%20introduce%20MCA-Bench%2C%20a%20comprehensive%20and%20reproducible%20benchmarking%0Asuite%20that%20integrates%20heterogeneous%20CAPTCHA%20types%20into%20a%20single%20evaluation%0Aprotocol.%20Leveraging%20a%20shared%20vision-language%20model%20backbone%2C%20we%20fine-tune%0Aspecialized%20cracking%20agents%20for%20each%20CAPTCHA%20category%2C%20enabling%20consistent%2C%0Across-modal%20assessments.%20Extensive%20experiments%20reveal%20that%20MCA-Bench%0Aeffectively%20maps%20the%20vulnerability%20spectrum%20of%20modern%20CAPTCHA%20designs%20under%0Avaried%20attack%20settings%2C%20and%20crucially%20offers%20the%20first%20quantitative%20analysis%20of%0Ahow%20challenge%20complexity%2C%20interaction%20depth%2C%20and%20model%20solvability%20interrelate.%0ABased%20on%20these%20findings%2C%20we%20propose%20three%20actionable%20design%20principles%20and%0Aidentify%20key%20open%20challenges%2C%20laying%20the%20groundwork%20for%20systematic%20CAPTCHA%0Ahardening%2C%20fair%20benchmarking%2C%20and%20broader%20community%20collaboration.%20Datasets%20and%0Acode%20are%20available%20online.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05982v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMCA-Bench%253A%2520A%2520Multimodal%2520Benchmark%2520for%2520Evaluating%2520CAPTCHA%2520Robustness%250A%2520%2520Against%2520VLM-based%2520Attacks%26entry.906535625%3DZonglin%2520Wu%2520and%2520Yule%2520Xue%2520and%2520Xin%2520Wei%2520and%2520Yiren%2520Song%26entry.1292438233%3D%2520%2520As%2520automated%2520attack%2520techniques%2520rapidly%2520advance%252C%2520CAPTCHAs%2520remain%2520a%2520critical%250Adefense%2520mechanism%2520against%2520malicious%2520bots.%2520However%252C%2520existing%2520CAPTCHA%2520schemes%250Aencompass%2520a%2520diverse%2520range%2520of%2520modalities%2520--%2520from%2520static%2520distorted%2520text%2520and%250Aobfuscated%2520images%2520to%2520interactive%2520clicks%252C%2520sliding%2520puzzles%252C%2520and%2520logic-based%250Aquestions%2520--%2520yet%2520the%2520community%2520still%2520lacks%2520a%2520unified%252C%2520large-scale%252C%2520multimodal%250Abenchmark%2520to%2520rigorously%2520evaluate%2520their%2520security%2520robustness.%2520To%2520address%2520this%250Agap%252C%2520we%2520introduce%2520MCA-Bench%252C%2520a%2520comprehensive%2520and%2520reproducible%2520benchmarking%250Asuite%2520that%2520integrates%2520heterogeneous%2520CAPTCHA%2520types%2520into%2520a%2520single%2520evaluation%250Aprotocol.%2520Leveraging%2520a%2520shared%2520vision-language%2520model%2520backbone%252C%2520we%2520fine-tune%250Aspecialized%2520cracking%2520agents%2520for%2520each%2520CAPTCHA%2520category%252C%2520enabling%2520consistent%252C%250Across-modal%2520assessments.%2520Extensive%2520experiments%2520reveal%2520that%2520MCA-Bench%250Aeffectively%2520maps%2520the%2520vulnerability%2520spectrum%2520of%2520modern%2520CAPTCHA%2520designs%2520under%250Avaried%2520attack%2520settings%252C%2520and%2520crucially%2520offers%2520the%2520first%2520quantitative%2520analysis%2520of%250Ahow%2520challenge%2520complexity%252C%2520interaction%2520depth%252C%2520and%2520model%2520solvability%2520interrelate.%250ABased%2520on%2520these%2520findings%252C%2520we%2520propose%2520three%2520actionable%2520design%2520principles%2520and%250Aidentify%2520key%2520open%2520challenges%252C%2520laying%2520the%2520groundwork%2520for%2520systematic%2520CAPTCHA%250Ahardening%252C%2520fair%2520benchmarking%252C%2520and%2520broader%2520community%2520collaboration.%2520Datasets%2520and%250Acode%2520are%2520available%2520online.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05982v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCA-Bench%3A%20A%20Multimodal%20Benchmark%20for%20Evaluating%20CAPTCHA%20Robustness%0A%20%20Against%20VLM-based%20Attacks&entry.906535625=Zonglin%20Wu%20and%20Yule%20Xue%20and%20Xin%20Wei%20and%20Yiren%20Song&entry.1292438233=%20%20As%20automated%20attack%20techniques%20rapidly%20advance%2C%20CAPTCHAs%20remain%20a%20critical%0Adefense%20mechanism%20against%20malicious%20bots.%20However%2C%20existing%20CAPTCHA%20schemes%0Aencompass%20a%20diverse%20range%20of%20modalities%20--%20from%20static%20distorted%20text%20and%0Aobfuscated%20images%20to%20interactive%20clicks%2C%20sliding%20puzzles%2C%20and%20logic-based%0Aquestions%20--%20yet%20the%20community%20still%20lacks%20a%20unified%2C%20large-scale%2C%20multimodal%0Abenchmark%20to%20rigorously%20evaluate%20their%20security%20robustness.%20To%20address%20this%0Agap%2C%20we%20introduce%20MCA-Bench%2C%20a%20comprehensive%20and%20reproducible%20benchmarking%0Asuite%20that%20integrates%20heterogeneous%20CAPTCHA%20types%20into%20a%20single%20evaluation%0Aprotocol.%20Leveraging%20a%20shared%20vision-language%20model%20backbone%2C%20we%20fine-tune%0Aspecialized%20cracking%20agents%20for%20each%20CAPTCHA%20category%2C%20enabling%20consistent%2C%0Across-modal%20assessments.%20Extensive%20experiments%20reveal%20that%20MCA-Bench%0Aeffectively%20maps%20the%20vulnerability%20spectrum%20of%20modern%20CAPTCHA%20designs%20under%0Avaried%20attack%20settings%2C%20and%20crucially%20offers%20the%20first%20quantitative%20analysis%20of%0Ahow%20challenge%20complexity%2C%20interaction%20depth%2C%20and%20model%20solvability%20interrelate.%0ABased%20on%20these%20findings%2C%20we%20propose%20three%20actionable%20design%20principles%20and%0Aidentify%20key%20open%20challenges%2C%20laying%20the%20groundwork%20for%20systematic%20CAPTCHA%0Ahardening%2C%20fair%20benchmarking%2C%20and%20broader%20community%20collaboration.%20Datasets%20and%0Acode%20are%20available%20online.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05982v1&entry.124074799=Read"},
{"title": "UAV-UGV Cooperative Trajectory Optimization and Task Allocation for\n  Medical Rescue Tasks in Post-Disaster Environments", "author": "Kaiyuan Chen and Wanpeng Zhao and Yongxi Liu and Yuanqing Xia and Wannian Liang and Shuo Wang", "abstract": "  In post-disaster scenarios, rapid and efficient delivery of medical resources\nis critical and challenging due to severe damage to infrastructure. To provide\nan optimized solution, we propose a cooperative trajectory optimization and\ntask allocation framework leveraging unmanned aerial vehicles (UAVs) and\nunmanned ground vehicles (UGVs). This study integrates a Genetic Algorithm (GA)\nfor efficient task allocation among multiple UAVs and UGVs, and employs an\ninformed-RRT* (Rapidly-exploring Random Tree Star) algorithm for collision-free\ntrajectory generation. Further optimization of task sequencing and path\nefficiency is conducted using Covariance Matrix Adaptation Evolution Strategy\n(CMA-ES). Simulation experiments conducted in a realistic post-disaster\nenvironment demonstrate that our proposed approach significantly improves the\noverall efficiency of medical rescue operations compared to traditional\nstrategies, showing substantial reductions in total mission completion time and\ntraveled distance. Additionally, the cooperative utilization of UAVs and UGVs\neffectively balances their complementary advantages, highlighting the system' s\nscalability and practicality for real-world deployment.\n", "link": "http://arxiv.org/abs/2506.06136v1", "date": "2025-06-06", "relevancy": 2.3791, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4844}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4726}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UAV-UGV%20Cooperative%20Trajectory%20Optimization%20and%20Task%20Allocation%20for%0A%20%20Medical%20Rescue%20Tasks%20in%20Post-Disaster%20Environments&body=Title%3A%20UAV-UGV%20Cooperative%20Trajectory%20Optimization%20and%20Task%20Allocation%20for%0A%20%20Medical%20Rescue%20Tasks%20in%20Post-Disaster%20Environments%0AAuthor%3A%20Kaiyuan%20Chen%20and%20Wanpeng%20Zhao%20and%20Yongxi%20Liu%20and%20Yuanqing%20Xia%20and%20Wannian%20Liang%20and%20Shuo%20Wang%0AAbstract%3A%20%20%20In%20post-disaster%20scenarios%2C%20rapid%20and%20efficient%20delivery%20of%20medical%20resources%0Ais%20critical%20and%20challenging%20due%20to%20severe%20damage%20to%20infrastructure.%20To%20provide%0Aan%20optimized%20solution%2C%20we%20propose%20a%20cooperative%20trajectory%20optimization%20and%0Atask%20allocation%20framework%20leveraging%20unmanned%20aerial%20vehicles%20%28UAVs%29%20and%0Aunmanned%20ground%20vehicles%20%28UGVs%29.%20This%20study%20integrates%20a%20Genetic%20Algorithm%20%28GA%29%0Afor%20efficient%20task%20allocation%20among%20multiple%20UAVs%20and%20UGVs%2C%20and%20employs%20an%0Ainformed-RRT%2A%20%28Rapidly-exploring%20Random%20Tree%20Star%29%20algorithm%20for%20collision-free%0Atrajectory%20generation.%20Further%20optimization%20of%20task%20sequencing%20and%20path%0Aefficiency%20is%20conducted%20using%20Covariance%20Matrix%20Adaptation%20Evolution%20Strategy%0A%28CMA-ES%29.%20Simulation%20experiments%20conducted%20in%20a%20realistic%20post-disaster%0Aenvironment%20demonstrate%20that%20our%20proposed%20approach%20significantly%20improves%20the%0Aoverall%20efficiency%20of%20medical%20rescue%20operations%20compared%20to%20traditional%0Astrategies%2C%20showing%20substantial%20reductions%20in%20total%20mission%20completion%20time%20and%0Atraveled%20distance.%20Additionally%2C%20the%20cooperative%20utilization%20of%20UAVs%20and%20UGVs%0Aeffectively%20balances%20their%20complementary%20advantages%2C%20highlighting%20the%20system%27%20s%0Ascalability%20and%20practicality%20for%20real-world%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06136v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUAV-UGV%2520Cooperative%2520Trajectory%2520Optimization%2520and%2520Task%2520Allocation%2520for%250A%2520%2520Medical%2520Rescue%2520Tasks%2520in%2520Post-Disaster%2520Environments%26entry.906535625%3DKaiyuan%2520Chen%2520and%2520Wanpeng%2520Zhao%2520and%2520Yongxi%2520Liu%2520and%2520Yuanqing%2520Xia%2520and%2520Wannian%2520Liang%2520and%2520Shuo%2520Wang%26entry.1292438233%3D%2520%2520In%2520post-disaster%2520scenarios%252C%2520rapid%2520and%2520efficient%2520delivery%2520of%2520medical%2520resources%250Ais%2520critical%2520and%2520challenging%2520due%2520to%2520severe%2520damage%2520to%2520infrastructure.%2520To%2520provide%250Aan%2520optimized%2520solution%252C%2520we%2520propose%2520a%2520cooperative%2520trajectory%2520optimization%2520and%250Atask%2520allocation%2520framework%2520leveraging%2520unmanned%2520aerial%2520vehicles%2520%2528UAVs%2529%2520and%250Aunmanned%2520ground%2520vehicles%2520%2528UGVs%2529.%2520This%2520study%2520integrates%2520a%2520Genetic%2520Algorithm%2520%2528GA%2529%250Afor%2520efficient%2520task%2520allocation%2520among%2520multiple%2520UAVs%2520and%2520UGVs%252C%2520and%2520employs%2520an%250Ainformed-RRT%252A%2520%2528Rapidly-exploring%2520Random%2520Tree%2520Star%2529%2520algorithm%2520for%2520collision-free%250Atrajectory%2520generation.%2520Further%2520optimization%2520of%2520task%2520sequencing%2520and%2520path%250Aefficiency%2520is%2520conducted%2520using%2520Covariance%2520Matrix%2520Adaptation%2520Evolution%2520Strategy%250A%2528CMA-ES%2529.%2520Simulation%2520experiments%2520conducted%2520in%2520a%2520realistic%2520post-disaster%250Aenvironment%2520demonstrate%2520that%2520our%2520proposed%2520approach%2520significantly%2520improves%2520the%250Aoverall%2520efficiency%2520of%2520medical%2520rescue%2520operations%2520compared%2520to%2520traditional%250Astrategies%252C%2520showing%2520substantial%2520reductions%2520in%2520total%2520mission%2520completion%2520time%2520and%250Atraveled%2520distance.%2520Additionally%252C%2520the%2520cooperative%2520utilization%2520of%2520UAVs%2520and%2520UGVs%250Aeffectively%2520balances%2520their%2520complementary%2520advantages%252C%2520highlighting%2520the%2520system%2527%2520s%250Ascalability%2520and%2520practicality%2520for%2520real-world%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06136v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UAV-UGV%20Cooperative%20Trajectory%20Optimization%20and%20Task%20Allocation%20for%0A%20%20Medical%20Rescue%20Tasks%20in%20Post-Disaster%20Environments&entry.906535625=Kaiyuan%20Chen%20and%20Wanpeng%20Zhao%20and%20Yongxi%20Liu%20and%20Yuanqing%20Xia%20and%20Wannian%20Liang%20and%20Shuo%20Wang&entry.1292438233=%20%20In%20post-disaster%20scenarios%2C%20rapid%20and%20efficient%20delivery%20of%20medical%20resources%0Ais%20critical%20and%20challenging%20due%20to%20severe%20damage%20to%20infrastructure.%20To%20provide%0Aan%20optimized%20solution%2C%20we%20propose%20a%20cooperative%20trajectory%20optimization%20and%0Atask%20allocation%20framework%20leveraging%20unmanned%20aerial%20vehicles%20%28UAVs%29%20and%0Aunmanned%20ground%20vehicles%20%28UGVs%29.%20This%20study%20integrates%20a%20Genetic%20Algorithm%20%28GA%29%0Afor%20efficient%20task%20allocation%20among%20multiple%20UAVs%20and%20UGVs%2C%20and%20employs%20an%0Ainformed-RRT%2A%20%28Rapidly-exploring%20Random%20Tree%20Star%29%20algorithm%20for%20collision-free%0Atrajectory%20generation.%20Further%20optimization%20of%20task%20sequencing%20and%20path%0Aefficiency%20is%20conducted%20using%20Covariance%20Matrix%20Adaptation%20Evolution%20Strategy%0A%28CMA-ES%29.%20Simulation%20experiments%20conducted%20in%20a%20realistic%20post-disaster%0Aenvironment%20demonstrate%20that%20our%20proposed%20approach%20significantly%20improves%20the%0Aoverall%20efficiency%20of%20medical%20rescue%20operations%20compared%20to%20traditional%0Astrategies%2C%20showing%20substantial%20reductions%20in%20total%20mission%20completion%20time%20and%0Atraveled%20distance.%20Additionally%2C%20the%20cooperative%20utilization%20of%20UAVs%20and%20UGVs%0Aeffectively%20balances%20their%20complementary%20advantages%2C%20highlighting%20the%20system%27%20s%0Ascalability%20and%20practicality%20for%20real-world%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06136v1&entry.124074799=Read"},
{"title": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study", "author": "Sabri Eyuboglu and Ryan Ehrlich and Simran Arora and Neel Guha and Dylan Zinsley and Emily Liu and Will Tennien and Atri Rudra and James Zou and Azalia Mirhoseini and Christopher Re", "abstract": "  Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining.\n", "link": "http://arxiv.org/abs/2506.06266v1", "date": "2025-06-06", "relevancy": 2.376, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4811}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4811}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4635}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cartridges%3A%20Lightweight%20and%20general-purpose%20long%20context%20representations%0A%20%20via%20self-study&body=Title%3A%20Cartridges%3A%20Lightweight%20and%20general-purpose%20long%20context%20representations%0A%20%20via%20self-study%0AAuthor%3A%20Sabri%20Eyuboglu%20and%20Ryan%20Ehrlich%20and%20Simran%20Arora%20and%20Neel%20Guha%20and%20Dylan%20Zinsley%20and%20Emily%20Liu%20and%20Will%20Tennien%20and%20Atri%20Rudra%20and%20James%20Zou%20and%20Azalia%20Mirhoseini%20and%20Christopher%20Re%0AAbstract%3A%20%20%20Large%20language%20models%20are%20often%20used%20to%20answer%20queries%20grounded%20in%20large%20text%0Acorpora%20%28e.g.%20codebases%2C%20legal%20documents%2C%20or%20chat%20histories%29%20by%20placing%20the%0Aentire%20corpus%20in%20the%20context%20window%20and%20leveraging%20in-context%20learning%20%28ICL%29.%0AAlthough%20current%20models%20support%20contexts%20of%20100K-1M%20tokens%2C%20this%20setup%20is%0Acostly%20to%20serve%20because%20the%20memory%20consumption%20of%20the%20KV%20cache%20scales%20with%0Ainput%20length.%20We%20explore%20an%20alternative%3A%20training%20a%20smaller%20KV%20cache%20offline%20on%0Aeach%20corpus.%20At%20inference%20time%2C%20we%20load%20this%20trained%20KV%20cache%2C%20which%20we%20call%20a%0ACartridge%2C%20and%20decode%20a%20response.%20Critically%2C%20the%20cost%20of%20training%20a%20Cartridge%0Acan%20be%20amortized%20across%20all%20the%20queries%20referencing%20the%20same%20corpus.%20However%2C%0Awe%20find%20that%20the%20naive%20approach%20of%20training%20the%20Cartridge%20with%20next-token%0Aprediction%20on%20the%20corpus%20is%20not%20competitive%20with%20ICL.%20Instead%2C%20we%20propose%0Aself-study%2C%20a%20training%20recipe%20in%20which%20we%20generate%20synthetic%20conversations%0Aabout%20the%20corpus%20and%20train%20the%20Cartridge%20with%20a%20context-distillation%20objective.%0AWe%20find%20that%20Cartridges%20trained%20with%20self-study%20replicate%20the%20functionality%20of%0AICL%2C%20while%20being%20significantly%20cheaper%20to%20serve.%20On%20challenging%20long-context%0Abenchmarks%2C%20Cartridges%20trained%20with%20self-study%20match%20ICL%20performance%20while%0Ausing%2038.6x%20less%20memory%20and%20enabling%2026.4x%20higher%20throughput.%20Self-study%20also%0Aextends%20the%20model%27s%20effective%20context%20length%20%28e.g.%20from%20128k%20to%20484k%20tokens%20on%0AMTOB%29%20and%20surprisingly%2C%20leads%20to%20Cartridges%20that%20can%20be%20composed%20at%20inference%0Atime%20without%20retraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06266v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCartridges%253A%2520Lightweight%2520and%2520general-purpose%2520long%2520context%2520representations%250A%2520%2520via%2520self-study%26entry.906535625%3DSabri%2520Eyuboglu%2520and%2520Ryan%2520Ehrlich%2520and%2520Simran%2520Arora%2520and%2520Neel%2520Guha%2520and%2520Dylan%2520Zinsley%2520and%2520Emily%2520Liu%2520and%2520Will%2520Tennien%2520and%2520Atri%2520Rudra%2520and%2520James%2520Zou%2520and%2520Azalia%2520Mirhoseini%2520and%2520Christopher%2520Re%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520are%2520often%2520used%2520to%2520answer%2520queries%2520grounded%2520in%2520large%2520text%250Acorpora%2520%2528e.g.%2520codebases%252C%2520legal%2520documents%252C%2520or%2520chat%2520histories%2529%2520by%2520placing%2520the%250Aentire%2520corpus%2520in%2520the%2520context%2520window%2520and%2520leveraging%2520in-context%2520learning%2520%2528ICL%2529.%250AAlthough%2520current%2520models%2520support%2520contexts%2520of%2520100K-1M%2520tokens%252C%2520this%2520setup%2520is%250Acostly%2520to%2520serve%2520because%2520the%2520memory%2520consumption%2520of%2520the%2520KV%2520cache%2520scales%2520with%250Ainput%2520length.%2520We%2520explore%2520an%2520alternative%253A%2520training%2520a%2520smaller%2520KV%2520cache%2520offline%2520on%250Aeach%2520corpus.%2520At%2520inference%2520time%252C%2520we%2520load%2520this%2520trained%2520KV%2520cache%252C%2520which%2520we%2520call%2520a%250ACartridge%252C%2520and%2520decode%2520a%2520response.%2520Critically%252C%2520the%2520cost%2520of%2520training%2520a%2520Cartridge%250Acan%2520be%2520amortized%2520across%2520all%2520the%2520queries%2520referencing%2520the%2520same%2520corpus.%2520However%252C%250Awe%2520find%2520that%2520the%2520naive%2520approach%2520of%2520training%2520the%2520Cartridge%2520with%2520next-token%250Aprediction%2520on%2520the%2520corpus%2520is%2520not%2520competitive%2520with%2520ICL.%2520Instead%252C%2520we%2520propose%250Aself-study%252C%2520a%2520training%2520recipe%2520in%2520which%2520we%2520generate%2520synthetic%2520conversations%250Aabout%2520the%2520corpus%2520and%2520train%2520the%2520Cartridge%2520with%2520a%2520context-distillation%2520objective.%250AWe%2520find%2520that%2520Cartridges%2520trained%2520with%2520self-study%2520replicate%2520the%2520functionality%2520of%250AICL%252C%2520while%2520being%2520significantly%2520cheaper%2520to%2520serve.%2520On%2520challenging%2520long-context%250Abenchmarks%252C%2520Cartridges%2520trained%2520with%2520self-study%2520match%2520ICL%2520performance%2520while%250Ausing%252038.6x%2520less%2520memory%2520and%2520enabling%252026.4x%2520higher%2520throughput.%2520Self-study%2520also%250Aextends%2520the%2520model%2527s%2520effective%2520context%2520length%2520%2528e.g.%2520from%2520128k%2520to%2520484k%2520tokens%2520on%250AMTOB%2529%2520and%2520surprisingly%252C%2520leads%2520to%2520Cartridges%2520that%2520can%2520be%2520composed%2520at%2520inference%250Atime%2520without%2520retraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06266v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cartridges%3A%20Lightweight%20and%20general-purpose%20long%20context%20representations%0A%20%20via%20self-study&entry.906535625=Sabri%20Eyuboglu%20and%20Ryan%20Ehrlich%20and%20Simran%20Arora%20and%20Neel%20Guha%20and%20Dylan%20Zinsley%20and%20Emily%20Liu%20and%20Will%20Tennien%20and%20Atri%20Rudra%20and%20James%20Zou%20and%20Azalia%20Mirhoseini%20and%20Christopher%20Re&entry.1292438233=%20%20Large%20language%20models%20are%20often%20used%20to%20answer%20queries%20grounded%20in%20large%20text%0Acorpora%20%28e.g.%20codebases%2C%20legal%20documents%2C%20or%20chat%20histories%29%20by%20placing%20the%0Aentire%20corpus%20in%20the%20context%20window%20and%20leveraging%20in-context%20learning%20%28ICL%29.%0AAlthough%20current%20models%20support%20contexts%20of%20100K-1M%20tokens%2C%20this%20setup%20is%0Acostly%20to%20serve%20because%20the%20memory%20consumption%20of%20the%20KV%20cache%20scales%20with%0Ainput%20length.%20We%20explore%20an%20alternative%3A%20training%20a%20smaller%20KV%20cache%20offline%20on%0Aeach%20corpus.%20At%20inference%20time%2C%20we%20load%20this%20trained%20KV%20cache%2C%20which%20we%20call%20a%0ACartridge%2C%20and%20decode%20a%20response.%20Critically%2C%20the%20cost%20of%20training%20a%20Cartridge%0Acan%20be%20amortized%20across%20all%20the%20queries%20referencing%20the%20same%20corpus.%20However%2C%0Awe%20find%20that%20the%20naive%20approach%20of%20training%20the%20Cartridge%20with%20next-token%0Aprediction%20on%20the%20corpus%20is%20not%20competitive%20with%20ICL.%20Instead%2C%20we%20propose%0Aself-study%2C%20a%20training%20recipe%20in%20which%20we%20generate%20synthetic%20conversations%0Aabout%20the%20corpus%20and%20train%20the%20Cartridge%20with%20a%20context-distillation%20objective.%0AWe%20find%20that%20Cartridges%20trained%20with%20self-study%20replicate%20the%20functionality%20of%0AICL%2C%20while%20being%20significantly%20cheaper%20to%20serve.%20On%20challenging%20long-context%0Abenchmarks%2C%20Cartridges%20trained%20with%20self-study%20match%20ICL%20performance%20while%0Ausing%2038.6x%20less%20memory%20and%20enabling%2026.4x%20higher%20throughput.%20Self-study%20also%0Aextends%20the%20model%27s%20effective%20context%20length%20%28e.g.%20from%20128k%20to%20484k%20tokens%20on%0AMTOB%29%20and%20surprisingly%2C%20leads%20to%20Cartridges%20that%20can%20be%20composed%20at%20inference%0Atime%20without%20retraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06266v1&entry.124074799=Read"},
{"title": "Eigenspectrum Analysis of Neural Networks without Aspect Ratio Bias", "author": "Yuanzhe Hu and Kinshuk Goel and Vlad Killiakov and Yaoqing Yang", "abstract": "  Diagnosing deep neural networks (DNNs) through the eigenspectrum of weight\nmatrices has been an active area of research in recent years. At a high level,\neigenspectrum analysis of DNNs involves measuring the heavytailness of the\nempirical spectral densities (ESD) of weight matrices. It provides insight into\nhow well a model is trained and can guide decisions on assigning better\nlayer-wise training hyperparameters. In this paper, we address a challenge\nassociated with such eigenspectrum methods: the impact of the aspect ratio of\nweight matrices on estimated heavytailness metrics. We demonstrate that\nmatrices of varying sizes (and aspect ratios) introduce a non-negligible bias\nin estimating heavytailness metrics, leading to inaccurate model diagnosis and\nlayer-wise hyperparameter assignment. To overcome this challenge, we propose\nFARMS (Fixed-Aspect-Ratio Matrix Subsampling), a method that normalizes the\nweight matrices by subsampling submatrices with a fixed aspect ratio. Instead\nof measuring the heavytailness of the original ESD, we measure the average ESD\nof these subsampled submatrices. We show that measuring the heavytailness of\nthese submatrices with the fixed aspect ratio can effectively mitigate the\naspect ratio bias. We validate our approach across various optimization\ntechniques and application domains that involve eigenspectrum analysis of\nweights, including image classification in computer vision (CV) models,\nscientific machine learning (SciML) model training, and large language model\n(LLM) pruning. Our results show that despite its simplicity, FARMS uniformly\nimproves the accuracy of eigenspectrum analysis while enabling more effective\nlayer-wise hyperparameter assignment in these application domains. In one of\nthe LLM pruning experiments, FARMS reduces the perplexity of the LLaMA-7B model\nby 17.3% when compared with the state-of-the-art method.\n", "link": "http://arxiv.org/abs/2506.06280v1", "date": "2025-06-06", "relevancy": 2.3506, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4952}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4716}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Eigenspectrum%20Analysis%20of%20Neural%20Networks%20without%20Aspect%20Ratio%20Bias&body=Title%3A%20Eigenspectrum%20Analysis%20of%20Neural%20Networks%20without%20Aspect%20Ratio%20Bias%0AAuthor%3A%20Yuanzhe%20Hu%20and%20Kinshuk%20Goel%20and%20Vlad%20Killiakov%20and%20Yaoqing%20Yang%0AAbstract%3A%20%20%20Diagnosing%20deep%20neural%20networks%20%28DNNs%29%20through%20the%20eigenspectrum%20of%20weight%0Amatrices%20has%20been%20an%20active%20area%20of%20research%20in%20recent%20years.%20At%20a%20high%20level%2C%0Aeigenspectrum%20analysis%20of%20DNNs%20involves%20measuring%20the%20heavytailness%20of%20the%0Aempirical%20spectral%20densities%20%28ESD%29%20of%20weight%20matrices.%20It%20provides%20insight%20into%0Ahow%20well%20a%20model%20is%20trained%20and%20can%20guide%20decisions%20on%20assigning%20better%0Alayer-wise%20training%20hyperparameters.%20In%20this%20paper%2C%20we%20address%20a%20challenge%0Aassociated%20with%20such%20eigenspectrum%20methods%3A%20the%20impact%20of%20the%20aspect%20ratio%20of%0Aweight%20matrices%20on%20estimated%20heavytailness%20metrics.%20We%20demonstrate%20that%0Amatrices%20of%20varying%20sizes%20%28and%20aspect%20ratios%29%20introduce%20a%20non-negligible%20bias%0Ain%20estimating%20heavytailness%20metrics%2C%20leading%20to%20inaccurate%20model%20diagnosis%20and%0Alayer-wise%20hyperparameter%20assignment.%20To%20overcome%20this%20challenge%2C%20we%20propose%0AFARMS%20%28Fixed-Aspect-Ratio%20Matrix%20Subsampling%29%2C%20a%20method%20that%20normalizes%20the%0Aweight%20matrices%20by%20subsampling%20submatrices%20with%20a%20fixed%20aspect%20ratio.%20Instead%0Aof%20measuring%20the%20heavytailness%20of%20the%20original%20ESD%2C%20we%20measure%20the%20average%20ESD%0Aof%20these%20subsampled%20submatrices.%20We%20show%20that%20measuring%20the%20heavytailness%20of%0Athese%20submatrices%20with%20the%20fixed%20aspect%20ratio%20can%20effectively%20mitigate%20the%0Aaspect%20ratio%20bias.%20We%20validate%20our%20approach%20across%20various%20optimization%0Atechniques%20and%20application%20domains%20that%20involve%20eigenspectrum%20analysis%20of%0Aweights%2C%20including%20image%20classification%20in%20computer%20vision%20%28CV%29%20models%2C%0Ascientific%20machine%20learning%20%28SciML%29%20model%20training%2C%20and%20large%20language%20model%0A%28LLM%29%20pruning.%20Our%20results%20show%20that%20despite%20its%20simplicity%2C%20FARMS%20uniformly%0Aimproves%20the%20accuracy%20of%20eigenspectrum%20analysis%20while%20enabling%20more%20effective%0Alayer-wise%20hyperparameter%20assignment%20in%20these%20application%20domains.%20In%20one%20of%0Athe%20LLM%20pruning%20experiments%2C%20FARMS%20reduces%20the%20perplexity%20of%20the%20LLaMA-7B%20model%0Aby%2017.3%25%20when%20compared%20with%20the%20state-of-the-art%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06280v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEigenspectrum%2520Analysis%2520of%2520Neural%2520Networks%2520without%2520Aspect%2520Ratio%2520Bias%26entry.906535625%3DYuanzhe%2520Hu%2520and%2520Kinshuk%2520Goel%2520and%2520Vlad%2520Killiakov%2520and%2520Yaoqing%2520Yang%26entry.1292438233%3D%2520%2520Diagnosing%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520through%2520the%2520eigenspectrum%2520of%2520weight%250Amatrices%2520has%2520been%2520an%2520active%2520area%2520of%2520research%2520in%2520recent%2520years.%2520At%2520a%2520high%2520level%252C%250Aeigenspectrum%2520analysis%2520of%2520DNNs%2520involves%2520measuring%2520the%2520heavytailness%2520of%2520the%250Aempirical%2520spectral%2520densities%2520%2528ESD%2529%2520of%2520weight%2520matrices.%2520It%2520provides%2520insight%2520into%250Ahow%2520well%2520a%2520model%2520is%2520trained%2520and%2520can%2520guide%2520decisions%2520on%2520assigning%2520better%250Alayer-wise%2520training%2520hyperparameters.%2520In%2520this%2520paper%252C%2520we%2520address%2520a%2520challenge%250Aassociated%2520with%2520such%2520eigenspectrum%2520methods%253A%2520the%2520impact%2520of%2520the%2520aspect%2520ratio%2520of%250Aweight%2520matrices%2520on%2520estimated%2520heavytailness%2520metrics.%2520We%2520demonstrate%2520that%250Amatrices%2520of%2520varying%2520sizes%2520%2528and%2520aspect%2520ratios%2529%2520introduce%2520a%2520non-negligible%2520bias%250Ain%2520estimating%2520heavytailness%2520metrics%252C%2520leading%2520to%2520inaccurate%2520model%2520diagnosis%2520and%250Alayer-wise%2520hyperparameter%2520assignment.%2520To%2520overcome%2520this%2520challenge%252C%2520we%2520propose%250AFARMS%2520%2528Fixed-Aspect-Ratio%2520Matrix%2520Subsampling%2529%252C%2520a%2520method%2520that%2520normalizes%2520the%250Aweight%2520matrices%2520by%2520subsampling%2520submatrices%2520with%2520a%2520fixed%2520aspect%2520ratio.%2520Instead%250Aof%2520measuring%2520the%2520heavytailness%2520of%2520the%2520original%2520ESD%252C%2520we%2520measure%2520the%2520average%2520ESD%250Aof%2520these%2520subsampled%2520submatrices.%2520We%2520show%2520that%2520measuring%2520the%2520heavytailness%2520of%250Athese%2520submatrices%2520with%2520the%2520fixed%2520aspect%2520ratio%2520can%2520effectively%2520mitigate%2520the%250Aaspect%2520ratio%2520bias.%2520We%2520validate%2520our%2520approach%2520across%2520various%2520optimization%250Atechniques%2520and%2520application%2520domains%2520that%2520involve%2520eigenspectrum%2520analysis%2520of%250Aweights%252C%2520including%2520image%2520classification%2520in%2520computer%2520vision%2520%2528CV%2529%2520models%252C%250Ascientific%2520machine%2520learning%2520%2528SciML%2529%2520model%2520training%252C%2520and%2520large%2520language%2520model%250A%2528LLM%2529%2520pruning.%2520Our%2520results%2520show%2520that%2520despite%2520its%2520simplicity%252C%2520FARMS%2520uniformly%250Aimproves%2520the%2520accuracy%2520of%2520eigenspectrum%2520analysis%2520while%2520enabling%2520more%2520effective%250Alayer-wise%2520hyperparameter%2520assignment%2520in%2520these%2520application%2520domains.%2520In%2520one%2520of%250Athe%2520LLM%2520pruning%2520experiments%252C%2520FARMS%2520reduces%2520the%2520perplexity%2520of%2520the%2520LLaMA-7B%2520model%250Aby%252017.3%2525%2520when%2520compared%2520with%2520the%2520state-of-the-art%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06280v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Eigenspectrum%20Analysis%20of%20Neural%20Networks%20without%20Aspect%20Ratio%20Bias&entry.906535625=Yuanzhe%20Hu%20and%20Kinshuk%20Goel%20and%20Vlad%20Killiakov%20and%20Yaoqing%20Yang&entry.1292438233=%20%20Diagnosing%20deep%20neural%20networks%20%28DNNs%29%20through%20the%20eigenspectrum%20of%20weight%0Amatrices%20has%20been%20an%20active%20area%20of%20research%20in%20recent%20years.%20At%20a%20high%20level%2C%0Aeigenspectrum%20analysis%20of%20DNNs%20involves%20measuring%20the%20heavytailness%20of%20the%0Aempirical%20spectral%20densities%20%28ESD%29%20of%20weight%20matrices.%20It%20provides%20insight%20into%0Ahow%20well%20a%20model%20is%20trained%20and%20can%20guide%20decisions%20on%20assigning%20better%0Alayer-wise%20training%20hyperparameters.%20In%20this%20paper%2C%20we%20address%20a%20challenge%0Aassociated%20with%20such%20eigenspectrum%20methods%3A%20the%20impact%20of%20the%20aspect%20ratio%20of%0Aweight%20matrices%20on%20estimated%20heavytailness%20metrics.%20We%20demonstrate%20that%0Amatrices%20of%20varying%20sizes%20%28and%20aspect%20ratios%29%20introduce%20a%20non-negligible%20bias%0Ain%20estimating%20heavytailness%20metrics%2C%20leading%20to%20inaccurate%20model%20diagnosis%20and%0Alayer-wise%20hyperparameter%20assignment.%20To%20overcome%20this%20challenge%2C%20we%20propose%0AFARMS%20%28Fixed-Aspect-Ratio%20Matrix%20Subsampling%29%2C%20a%20method%20that%20normalizes%20the%0Aweight%20matrices%20by%20subsampling%20submatrices%20with%20a%20fixed%20aspect%20ratio.%20Instead%0Aof%20measuring%20the%20heavytailness%20of%20the%20original%20ESD%2C%20we%20measure%20the%20average%20ESD%0Aof%20these%20subsampled%20submatrices.%20We%20show%20that%20measuring%20the%20heavytailness%20of%0Athese%20submatrices%20with%20the%20fixed%20aspect%20ratio%20can%20effectively%20mitigate%20the%0Aaspect%20ratio%20bias.%20We%20validate%20our%20approach%20across%20various%20optimization%0Atechniques%20and%20application%20domains%20that%20involve%20eigenspectrum%20analysis%20of%0Aweights%2C%20including%20image%20classification%20in%20computer%20vision%20%28CV%29%20models%2C%0Ascientific%20machine%20learning%20%28SciML%29%20model%20training%2C%20and%20large%20language%20model%0A%28LLM%29%20pruning.%20Our%20results%20show%20that%20despite%20its%20simplicity%2C%20FARMS%20uniformly%0Aimproves%20the%20accuracy%20of%20eigenspectrum%20analysis%20while%20enabling%20more%20effective%0Alayer-wise%20hyperparameter%20assignment%20in%20these%20application%20domains.%20In%20one%20of%0Athe%20LLM%20pruning%20experiments%2C%20FARMS%20reduces%20the%20perplexity%20of%20the%20LLaMA-7B%20model%0Aby%2017.3%25%20when%20compared%20with%20the%20state-of-the-art%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06280v1&entry.124074799=Read"},
{"title": "Gradient Similarity Surgery in Multi-Task Deep Learning", "author": "Thomas Borsani and Andrea Rosani and Giuseppe Nicosia and Giuseppe Di Fatta", "abstract": "  The multi-task learning ($MTL$) paradigm aims to simultaneously learn\nmultiple tasks within a single model capturing higher-level, more general\nhidden patterns that are shared by the tasks. In deep learning, a significant\nchallenge in the backpropagation training process is the design of advanced\noptimisers to improve the convergence speed and stability of the gradient\ndescent learning rule. In particular, in multi-task deep learning ($MTDL$) the\nmultitude of tasks may generate potentially conflicting gradients that would\nhinder the concurrent convergence of the diverse loss functions. This challenge\narises when the gradients of the task objectives have either different\nmagnitudes or opposite directions, causing one or a few to dominate or to\ninterfere with each other, thus degrading the training process. Gradient\nsurgery methods address the problem explicitly dealing with conflicting\ngradients by adjusting the overall gradient trajectory. This work introduces a\nnovel gradient surgery method, the Similarity-Aware Momentum Gradient Surgery\n(SAM-GS), which provides an effective and scalable approach based on a gradient\nmagnitude similarity measure to guide the optimisation process. The SAM-GS\nsurgery adopts gradient equalisation and modulation of the first-order\nmomentum. A series of experimental tests have shown the effectiveness of SAM-GS\non synthetic problems and $MTL$ benchmarks. Gradient magnitude similarity plays\na crucial role in regularising gradient aggregation in $MTDL$ for the\noptimisation of the learning process.\n", "link": "http://arxiv.org/abs/2506.06130v1", "date": "2025-06-06", "relevancy": 2.3479, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4879}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4644}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient%20Similarity%20Surgery%20in%20Multi-Task%20Deep%20Learning&body=Title%3A%20Gradient%20Similarity%20Surgery%20in%20Multi-Task%20Deep%20Learning%0AAuthor%3A%20Thomas%20Borsani%20and%20Andrea%20Rosani%20and%20Giuseppe%20Nicosia%20and%20Giuseppe%20Di%20Fatta%0AAbstract%3A%20%20%20The%20multi-task%20learning%20%28%24MTL%24%29%20paradigm%20aims%20to%20simultaneously%20learn%0Amultiple%20tasks%20within%20a%20single%20model%20capturing%20higher-level%2C%20more%20general%0Ahidden%20patterns%20that%20are%20shared%20by%20the%20tasks.%20In%20deep%20learning%2C%20a%20significant%0Achallenge%20in%20the%20backpropagation%20training%20process%20is%20the%20design%20of%20advanced%0Aoptimisers%20to%20improve%20the%20convergence%20speed%20and%20stability%20of%20the%20gradient%0Adescent%20learning%20rule.%20In%20particular%2C%20in%20multi-task%20deep%20learning%20%28%24MTDL%24%29%20the%0Amultitude%20of%20tasks%20may%20generate%20potentially%20conflicting%20gradients%20that%20would%0Ahinder%20the%20concurrent%20convergence%20of%20the%20diverse%20loss%20functions.%20This%20challenge%0Aarises%20when%20the%20gradients%20of%20the%20task%20objectives%20have%20either%20different%0Amagnitudes%20or%20opposite%20directions%2C%20causing%20one%20or%20a%20few%20to%20dominate%20or%20to%0Ainterfere%20with%20each%20other%2C%20thus%20degrading%20the%20training%20process.%20Gradient%0Asurgery%20methods%20address%20the%20problem%20explicitly%20dealing%20with%20conflicting%0Agradients%20by%20adjusting%20the%20overall%20gradient%20trajectory.%20This%20work%20introduces%20a%0Anovel%20gradient%20surgery%20method%2C%20the%20Similarity-Aware%20Momentum%20Gradient%20Surgery%0A%28SAM-GS%29%2C%20which%20provides%20an%20effective%20and%20scalable%20approach%20based%20on%20a%20gradient%0Amagnitude%20similarity%20measure%20to%20guide%20the%20optimisation%20process.%20The%20SAM-GS%0Asurgery%20adopts%20gradient%20equalisation%20and%20modulation%20of%20the%20first-order%0Amomentum.%20A%20series%20of%20experimental%20tests%20have%20shown%20the%20effectiveness%20of%20SAM-GS%0Aon%20synthetic%20problems%20and%20%24MTL%24%20benchmarks.%20Gradient%20magnitude%20similarity%20plays%0Aa%20crucial%20role%20in%20regularising%20gradient%20aggregation%20in%20%24MTDL%24%20for%20the%0Aoptimisation%20of%20the%20learning%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06130v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient%2520Similarity%2520Surgery%2520in%2520Multi-Task%2520Deep%2520Learning%26entry.906535625%3DThomas%2520Borsani%2520and%2520Andrea%2520Rosani%2520and%2520Giuseppe%2520Nicosia%2520and%2520Giuseppe%2520Di%2520Fatta%26entry.1292438233%3D%2520%2520The%2520multi-task%2520learning%2520%2528%2524MTL%2524%2529%2520paradigm%2520aims%2520to%2520simultaneously%2520learn%250Amultiple%2520tasks%2520within%2520a%2520single%2520model%2520capturing%2520higher-level%252C%2520more%2520general%250Ahidden%2520patterns%2520that%2520are%2520shared%2520by%2520the%2520tasks.%2520In%2520deep%2520learning%252C%2520a%2520significant%250Achallenge%2520in%2520the%2520backpropagation%2520training%2520process%2520is%2520the%2520design%2520of%2520advanced%250Aoptimisers%2520to%2520improve%2520the%2520convergence%2520speed%2520and%2520stability%2520of%2520the%2520gradient%250Adescent%2520learning%2520rule.%2520In%2520particular%252C%2520in%2520multi-task%2520deep%2520learning%2520%2528%2524MTDL%2524%2529%2520the%250Amultitude%2520of%2520tasks%2520may%2520generate%2520potentially%2520conflicting%2520gradients%2520that%2520would%250Ahinder%2520the%2520concurrent%2520convergence%2520of%2520the%2520diverse%2520loss%2520functions.%2520This%2520challenge%250Aarises%2520when%2520the%2520gradients%2520of%2520the%2520task%2520objectives%2520have%2520either%2520different%250Amagnitudes%2520or%2520opposite%2520directions%252C%2520causing%2520one%2520or%2520a%2520few%2520to%2520dominate%2520or%2520to%250Ainterfere%2520with%2520each%2520other%252C%2520thus%2520degrading%2520the%2520training%2520process.%2520Gradient%250Asurgery%2520methods%2520address%2520the%2520problem%2520explicitly%2520dealing%2520with%2520conflicting%250Agradients%2520by%2520adjusting%2520the%2520overall%2520gradient%2520trajectory.%2520This%2520work%2520introduces%2520a%250Anovel%2520gradient%2520surgery%2520method%252C%2520the%2520Similarity-Aware%2520Momentum%2520Gradient%2520Surgery%250A%2528SAM-GS%2529%252C%2520which%2520provides%2520an%2520effective%2520and%2520scalable%2520approach%2520based%2520on%2520a%2520gradient%250Amagnitude%2520similarity%2520measure%2520to%2520guide%2520the%2520optimisation%2520process.%2520The%2520SAM-GS%250Asurgery%2520adopts%2520gradient%2520equalisation%2520and%2520modulation%2520of%2520the%2520first-order%250Amomentum.%2520A%2520series%2520of%2520experimental%2520tests%2520have%2520shown%2520the%2520effectiveness%2520of%2520SAM-GS%250Aon%2520synthetic%2520problems%2520and%2520%2524MTL%2524%2520benchmarks.%2520Gradient%2520magnitude%2520similarity%2520plays%250Aa%2520crucial%2520role%2520in%2520regularising%2520gradient%2520aggregation%2520in%2520%2524MTDL%2524%2520for%2520the%250Aoptimisation%2520of%2520the%2520learning%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06130v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient%20Similarity%20Surgery%20in%20Multi-Task%20Deep%20Learning&entry.906535625=Thomas%20Borsani%20and%20Andrea%20Rosani%20and%20Giuseppe%20Nicosia%20and%20Giuseppe%20Di%20Fatta&entry.1292438233=%20%20The%20multi-task%20learning%20%28%24MTL%24%29%20paradigm%20aims%20to%20simultaneously%20learn%0Amultiple%20tasks%20within%20a%20single%20model%20capturing%20higher-level%2C%20more%20general%0Ahidden%20patterns%20that%20are%20shared%20by%20the%20tasks.%20In%20deep%20learning%2C%20a%20significant%0Achallenge%20in%20the%20backpropagation%20training%20process%20is%20the%20design%20of%20advanced%0Aoptimisers%20to%20improve%20the%20convergence%20speed%20and%20stability%20of%20the%20gradient%0Adescent%20learning%20rule.%20In%20particular%2C%20in%20multi-task%20deep%20learning%20%28%24MTDL%24%29%20the%0Amultitude%20of%20tasks%20may%20generate%20potentially%20conflicting%20gradients%20that%20would%0Ahinder%20the%20concurrent%20convergence%20of%20the%20diverse%20loss%20functions.%20This%20challenge%0Aarises%20when%20the%20gradients%20of%20the%20task%20objectives%20have%20either%20different%0Amagnitudes%20or%20opposite%20directions%2C%20causing%20one%20or%20a%20few%20to%20dominate%20or%20to%0Ainterfere%20with%20each%20other%2C%20thus%20degrading%20the%20training%20process.%20Gradient%0Asurgery%20methods%20address%20the%20problem%20explicitly%20dealing%20with%20conflicting%0Agradients%20by%20adjusting%20the%20overall%20gradient%20trajectory.%20This%20work%20introduces%20a%0Anovel%20gradient%20surgery%20method%2C%20the%20Similarity-Aware%20Momentum%20Gradient%20Surgery%0A%28SAM-GS%29%2C%20which%20provides%20an%20effective%20and%20scalable%20approach%20based%20on%20a%20gradient%0Amagnitude%20similarity%20measure%20to%20guide%20the%20optimisation%20process.%20The%20SAM-GS%0Asurgery%20adopts%20gradient%20equalisation%20and%20modulation%20of%20the%20first-order%0Amomentum.%20A%20series%20of%20experimental%20tests%20have%20shown%20the%20effectiveness%20of%20SAM-GS%0Aon%20synthetic%20problems%20and%20%24MTL%24%20benchmarks.%20Gradient%20magnitude%20similarity%20plays%0Aa%20crucial%20role%20in%20regularising%20gradient%20aggregation%20in%20%24MTDL%24%20for%20the%0Aoptimisation%20of%20the%20learning%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06130v1&entry.124074799=Read"},
{"title": "From Prototypes to General Distributions: An Efficient Curriculum for\n  Masked Image Modeling", "author": "Jinhong Lin and Cheng-En Wu and Huanran Li and Jifan Zhang and Yu Hen Hu and Pedro Morgado", "abstract": "  Masked Image Modeling (MIM) has emerged as a powerful self-supervised\nlearning paradigm for visual representation learning, enabling models to\nacquire rich visual representations by predicting masked portions of images\nfrom their visible regions. While this approach has shown promising results, we\nhypothesize that its effectiveness may be limited by optimization challenges\nduring early training stages, where models are expected to learn complex image\ndistributions from partial observations before developing basic visual\nprocessing capabilities. To address this limitation, we propose a\nprototype-driven curriculum leagrning framework that structures the learning\nprocess to progress from prototypical examples to more complex variations in\nthe dataset. Our approach introduces a temperature-based annealing scheme that\ngradually expands the training distribution, enabling more stable and efficient\nlearning trajectories. Through extensive experiments on ImageNet-1K, we\ndemonstrate that our curriculum learning strategy significantly improves both\ntraining efficiency and representation quality while requiring substantially\nfewer training epochs compared to standard Masked Auto-Encoding. Our findings\nsuggest that carefully controlling the order of training examples plays a\ncrucial role in self-supervised visual learning, providing a practical solution\nto the early-stage optimization challenges in MIM.\n", "link": "http://arxiv.org/abs/2411.10685v2", "date": "2025-06-06", "relevancy": 2.3439, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5903}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5874}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5828}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Prototypes%20to%20General%20Distributions%3A%20An%20Efficient%20Curriculum%20for%0A%20%20Masked%20Image%20Modeling&body=Title%3A%20From%20Prototypes%20to%20General%20Distributions%3A%20An%20Efficient%20Curriculum%20for%0A%20%20Masked%20Image%20Modeling%0AAuthor%3A%20Jinhong%20Lin%20and%20Cheng-En%20Wu%20and%20Huanran%20Li%20and%20Jifan%20Zhang%20and%20Yu%20Hen%20Hu%20and%20Pedro%20Morgado%0AAbstract%3A%20%20%20Masked%20Image%20Modeling%20%28MIM%29%20has%20emerged%20as%20a%20powerful%20self-supervised%0Alearning%20paradigm%20for%20visual%20representation%20learning%2C%20enabling%20models%20to%0Aacquire%20rich%20visual%20representations%20by%20predicting%20masked%20portions%20of%20images%0Afrom%20their%20visible%20regions.%20While%20this%20approach%20has%20shown%20promising%20results%2C%20we%0Ahypothesize%20that%20its%20effectiveness%20may%20be%20limited%20by%20optimization%20challenges%0Aduring%20early%20training%20stages%2C%20where%20models%20are%20expected%20to%20learn%20complex%20image%0Adistributions%20from%20partial%20observations%20before%20developing%20basic%20visual%0Aprocessing%20capabilities.%20To%20address%20this%20limitation%2C%20we%20propose%20a%0Aprototype-driven%20curriculum%20leagrning%20framework%20that%20structures%20the%20learning%0Aprocess%20to%20progress%20from%20prototypical%20examples%20to%20more%20complex%20variations%20in%0Athe%20dataset.%20Our%20approach%20introduces%20a%20temperature-based%20annealing%20scheme%20that%0Agradually%20expands%20the%20training%20distribution%2C%20enabling%20more%20stable%20and%20efficient%0Alearning%20trajectories.%20Through%20extensive%20experiments%20on%20ImageNet-1K%2C%20we%0Ademonstrate%20that%20our%20curriculum%20learning%20strategy%20significantly%20improves%20both%0Atraining%20efficiency%20and%20representation%20quality%20while%20requiring%20substantially%0Afewer%20training%20epochs%20compared%20to%20standard%20Masked%20Auto-Encoding.%20Our%20findings%0Asuggest%20that%20carefully%20controlling%20the%20order%20of%20training%20examples%20plays%20a%0Acrucial%20role%20in%20self-supervised%20visual%20learning%2C%20providing%20a%20practical%20solution%0Ato%20the%20early-stage%20optimization%20challenges%20in%20MIM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10685v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Prototypes%2520to%2520General%2520Distributions%253A%2520An%2520Efficient%2520Curriculum%2520for%250A%2520%2520Masked%2520Image%2520Modeling%26entry.906535625%3DJinhong%2520Lin%2520and%2520Cheng-En%2520Wu%2520and%2520Huanran%2520Li%2520and%2520Jifan%2520Zhang%2520and%2520Yu%2520Hen%2520Hu%2520and%2520Pedro%2520Morgado%26entry.1292438233%3D%2520%2520Masked%2520Image%2520Modeling%2520%2528MIM%2529%2520has%2520emerged%2520as%2520a%2520powerful%2520self-supervised%250Alearning%2520paradigm%2520for%2520visual%2520representation%2520learning%252C%2520enabling%2520models%2520to%250Aacquire%2520rich%2520visual%2520representations%2520by%2520predicting%2520masked%2520portions%2520of%2520images%250Afrom%2520their%2520visible%2520regions.%2520While%2520this%2520approach%2520has%2520shown%2520promising%2520results%252C%2520we%250Ahypothesize%2520that%2520its%2520effectiveness%2520may%2520be%2520limited%2520by%2520optimization%2520challenges%250Aduring%2520early%2520training%2520stages%252C%2520where%2520models%2520are%2520expected%2520to%2520learn%2520complex%2520image%250Adistributions%2520from%2520partial%2520observations%2520before%2520developing%2520basic%2520visual%250Aprocessing%2520capabilities.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%250Aprototype-driven%2520curriculum%2520leagrning%2520framework%2520that%2520structures%2520the%2520learning%250Aprocess%2520to%2520progress%2520from%2520prototypical%2520examples%2520to%2520more%2520complex%2520variations%2520in%250Athe%2520dataset.%2520Our%2520approach%2520introduces%2520a%2520temperature-based%2520annealing%2520scheme%2520that%250Agradually%2520expands%2520the%2520training%2520distribution%252C%2520enabling%2520more%2520stable%2520and%2520efficient%250Alearning%2520trajectories.%2520Through%2520extensive%2520experiments%2520on%2520ImageNet-1K%252C%2520we%250Ademonstrate%2520that%2520our%2520curriculum%2520learning%2520strategy%2520significantly%2520improves%2520both%250Atraining%2520efficiency%2520and%2520representation%2520quality%2520while%2520requiring%2520substantially%250Afewer%2520training%2520epochs%2520compared%2520to%2520standard%2520Masked%2520Auto-Encoding.%2520Our%2520findings%250Asuggest%2520that%2520carefully%2520controlling%2520the%2520order%2520of%2520training%2520examples%2520plays%2520a%250Acrucial%2520role%2520in%2520self-supervised%2520visual%2520learning%252C%2520providing%2520a%2520practical%2520solution%250Ato%2520the%2520early-stage%2520optimization%2520challenges%2520in%2520MIM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10685v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Prototypes%20to%20General%20Distributions%3A%20An%20Efficient%20Curriculum%20for%0A%20%20Masked%20Image%20Modeling&entry.906535625=Jinhong%20Lin%20and%20Cheng-En%20Wu%20and%20Huanran%20Li%20and%20Jifan%20Zhang%20and%20Yu%20Hen%20Hu%20and%20Pedro%20Morgado&entry.1292438233=%20%20Masked%20Image%20Modeling%20%28MIM%29%20has%20emerged%20as%20a%20powerful%20self-supervised%0Alearning%20paradigm%20for%20visual%20representation%20learning%2C%20enabling%20models%20to%0Aacquire%20rich%20visual%20representations%20by%20predicting%20masked%20portions%20of%20images%0Afrom%20their%20visible%20regions.%20While%20this%20approach%20has%20shown%20promising%20results%2C%20we%0Ahypothesize%20that%20its%20effectiveness%20may%20be%20limited%20by%20optimization%20challenges%0Aduring%20early%20training%20stages%2C%20where%20models%20are%20expected%20to%20learn%20complex%20image%0Adistributions%20from%20partial%20observations%20before%20developing%20basic%20visual%0Aprocessing%20capabilities.%20To%20address%20this%20limitation%2C%20we%20propose%20a%0Aprototype-driven%20curriculum%20leagrning%20framework%20that%20structures%20the%20learning%0Aprocess%20to%20progress%20from%20prototypical%20examples%20to%20more%20complex%20variations%20in%0Athe%20dataset.%20Our%20approach%20introduces%20a%20temperature-based%20annealing%20scheme%20that%0Agradually%20expands%20the%20training%20distribution%2C%20enabling%20more%20stable%20and%20efficient%0Alearning%20trajectories.%20Through%20extensive%20experiments%20on%20ImageNet-1K%2C%20we%0Ademonstrate%20that%20our%20curriculum%20learning%20strategy%20significantly%20improves%20both%0Atraining%20efficiency%20and%20representation%20quality%20while%20requiring%20substantially%0Afewer%20training%20epochs%20compared%20to%20standard%20Masked%20Auto-Encoding.%20Our%20findings%0Asuggest%20that%20carefully%20controlling%20the%20order%20of%20training%20examples%20plays%20a%0Acrucial%20role%20in%20self-supervised%20visual%20learning%2C%20providing%20a%20practical%20solution%0Ato%20the%20early-stage%20optimization%20challenges%20in%20MIM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10685v2&entry.124074799=Read"},
{"title": "HIGHT: Hierarchical Graph Tokenization for Molecule-Language Alignment", "author": "Yongqiang Chen and Quanming Yao and Juzheng Zhang and James Cheng and Yatao Bian", "abstract": "  Recently, there has been a surge of interest in extending the success of\nlarge language models (LLMs) from texts to molecules. Most existing approaches\nadopt a graph neural network to represent a molecule as a series of node tokens\nfor molecule-language alignment, which, however, have overlooked the inherent\nhierarchical structures in molecules. Notably, higher-order molecular\nstructures contain rich semantics of functional groups, which encode crucial\nbiochemical functionalities of the molecules. We show that neglecting the\nhierarchical information in tokenization will lead to subpar molecule-language\nalignment and severe hallucination. To address this limitation, we propose\nHIerarchical GrapH Tokenization (HIGHT). HIGHT employs a hierarchical graph\ntokenizer that encodes the hierarchy of atom, motif, and molecular levels of\ninformative tokens to improve the molecular perception of LLMs. HIGHT also\nadopts an augmented instruction tuning dataset, enriched with the hierarchical\ngraph information, to further enhance the molecule-language alignment.\nExtensive experiments on 14 real-world benchmarks verify the effectiveness of\nHIGHT in reducing hallucination by 40%, and significant improvements in various\nmolecule-language downstream tasks. The project is available at https:\n//higraphllm.github.io/.\n", "link": "http://arxiv.org/abs/2406.14021v2", "date": "2025-06-06", "relevancy": 2.3429, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4941}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4665}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HIGHT%3A%20Hierarchical%20Graph%20Tokenization%20for%20Molecule-Language%20Alignment&body=Title%3A%20HIGHT%3A%20Hierarchical%20Graph%20Tokenization%20for%20Molecule-Language%20Alignment%0AAuthor%3A%20Yongqiang%20Chen%20and%20Quanming%20Yao%20and%20Juzheng%20Zhang%20and%20James%20Cheng%20and%20Yatao%20Bian%0AAbstract%3A%20%20%20Recently%2C%20there%20has%20been%20a%20surge%20of%20interest%20in%20extending%20the%20success%20of%0Alarge%20language%20models%20%28LLMs%29%20from%20texts%20to%20molecules.%20Most%20existing%20approaches%0Aadopt%20a%20graph%20neural%20network%20to%20represent%20a%20molecule%20as%20a%20series%20of%20node%20tokens%0Afor%20molecule-language%20alignment%2C%20which%2C%20however%2C%20have%20overlooked%20the%20inherent%0Ahierarchical%20structures%20in%20molecules.%20Notably%2C%20higher-order%20molecular%0Astructures%20contain%20rich%20semantics%20of%20functional%20groups%2C%20which%20encode%20crucial%0Abiochemical%20functionalities%20of%20the%20molecules.%20We%20show%20that%20neglecting%20the%0Ahierarchical%20information%20in%20tokenization%20will%20lead%20to%20subpar%20molecule-language%0Aalignment%20and%20severe%20hallucination.%20To%20address%20this%20limitation%2C%20we%20propose%0AHIerarchical%20GrapH%20Tokenization%20%28HIGHT%29.%20HIGHT%20employs%20a%20hierarchical%20graph%0Atokenizer%20that%20encodes%20the%20hierarchy%20of%20atom%2C%20motif%2C%20and%20molecular%20levels%20of%0Ainformative%20tokens%20to%20improve%20the%20molecular%20perception%20of%20LLMs.%20HIGHT%20also%0Aadopts%20an%20augmented%20instruction%20tuning%20dataset%2C%20enriched%20with%20the%20hierarchical%0Agraph%20information%2C%20to%20further%20enhance%20the%20molecule-language%20alignment.%0AExtensive%20experiments%20on%2014%20real-world%20benchmarks%20verify%20the%20effectiveness%20of%0AHIGHT%20in%20reducing%20hallucination%20by%2040%25%2C%20and%20significant%20improvements%20in%20various%0Amolecule-language%20downstream%20tasks.%20The%20project%20is%20available%20at%20https%3A%0A//higraphllm.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14021v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHIGHT%253A%2520Hierarchical%2520Graph%2520Tokenization%2520for%2520Molecule-Language%2520Alignment%26entry.906535625%3DYongqiang%2520Chen%2520and%2520Quanming%2520Yao%2520and%2520Juzheng%2520Zhang%2520and%2520James%2520Cheng%2520and%2520Yatao%2520Bian%26entry.1292438233%3D%2520%2520Recently%252C%2520there%2520has%2520been%2520a%2520surge%2520of%2520interest%2520in%2520extending%2520the%2520success%2520of%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520from%2520texts%2520to%2520molecules.%2520Most%2520existing%2520approaches%250Aadopt%2520a%2520graph%2520neural%2520network%2520to%2520represent%2520a%2520molecule%2520as%2520a%2520series%2520of%2520node%2520tokens%250Afor%2520molecule-language%2520alignment%252C%2520which%252C%2520however%252C%2520have%2520overlooked%2520the%2520inherent%250Ahierarchical%2520structures%2520in%2520molecules.%2520Notably%252C%2520higher-order%2520molecular%250Astructures%2520contain%2520rich%2520semantics%2520of%2520functional%2520groups%252C%2520which%2520encode%2520crucial%250Abiochemical%2520functionalities%2520of%2520the%2520molecules.%2520We%2520show%2520that%2520neglecting%2520the%250Ahierarchical%2520information%2520in%2520tokenization%2520will%2520lead%2520to%2520subpar%2520molecule-language%250Aalignment%2520and%2520severe%2520hallucination.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%250AHIerarchical%2520GrapH%2520Tokenization%2520%2528HIGHT%2529.%2520HIGHT%2520employs%2520a%2520hierarchical%2520graph%250Atokenizer%2520that%2520encodes%2520the%2520hierarchy%2520of%2520atom%252C%2520motif%252C%2520and%2520molecular%2520levels%2520of%250Ainformative%2520tokens%2520to%2520improve%2520the%2520molecular%2520perception%2520of%2520LLMs.%2520HIGHT%2520also%250Aadopts%2520an%2520augmented%2520instruction%2520tuning%2520dataset%252C%2520enriched%2520with%2520the%2520hierarchical%250Agraph%2520information%252C%2520to%2520further%2520enhance%2520the%2520molecule-language%2520alignment.%250AExtensive%2520experiments%2520on%252014%2520real-world%2520benchmarks%2520verify%2520the%2520effectiveness%2520of%250AHIGHT%2520in%2520reducing%2520hallucination%2520by%252040%2525%252C%2520and%2520significant%2520improvements%2520in%2520various%250Amolecule-language%2520downstream%2520tasks.%2520The%2520project%2520is%2520available%2520at%2520https%253A%250A//higraphllm.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14021v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HIGHT%3A%20Hierarchical%20Graph%20Tokenization%20for%20Molecule-Language%20Alignment&entry.906535625=Yongqiang%20Chen%20and%20Quanming%20Yao%20and%20Juzheng%20Zhang%20and%20James%20Cheng%20and%20Yatao%20Bian&entry.1292438233=%20%20Recently%2C%20there%20has%20been%20a%20surge%20of%20interest%20in%20extending%20the%20success%20of%0Alarge%20language%20models%20%28LLMs%29%20from%20texts%20to%20molecules.%20Most%20existing%20approaches%0Aadopt%20a%20graph%20neural%20network%20to%20represent%20a%20molecule%20as%20a%20series%20of%20node%20tokens%0Afor%20molecule-language%20alignment%2C%20which%2C%20however%2C%20have%20overlooked%20the%20inherent%0Ahierarchical%20structures%20in%20molecules.%20Notably%2C%20higher-order%20molecular%0Astructures%20contain%20rich%20semantics%20of%20functional%20groups%2C%20which%20encode%20crucial%0Abiochemical%20functionalities%20of%20the%20molecules.%20We%20show%20that%20neglecting%20the%0Ahierarchical%20information%20in%20tokenization%20will%20lead%20to%20subpar%20molecule-language%0Aalignment%20and%20severe%20hallucination.%20To%20address%20this%20limitation%2C%20we%20propose%0AHIerarchical%20GrapH%20Tokenization%20%28HIGHT%29.%20HIGHT%20employs%20a%20hierarchical%20graph%0Atokenizer%20that%20encodes%20the%20hierarchy%20of%20atom%2C%20motif%2C%20and%20molecular%20levels%20of%0Ainformative%20tokens%20to%20improve%20the%20molecular%20perception%20of%20LLMs.%20HIGHT%20also%0Aadopts%20an%20augmented%20instruction%20tuning%20dataset%2C%20enriched%20with%20the%20hierarchical%0Agraph%20information%2C%20to%20further%20enhance%20the%20molecule-language%20alignment.%0AExtensive%20experiments%20on%2014%20real-world%20benchmarks%20verify%20the%20effectiveness%20of%0AHIGHT%20in%20reducing%20hallucination%20by%2040%25%2C%20and%20significant%20improvements%20in%20various%0Amolecule-language%20downstream%20tasks.%20The%20project%20is%20available%20at%20https%3A%0A//higraphllm.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14021v2&entry.124074799=Read"},
{"title": "Jacobian Sparse Autoencoders: Sparsify Computations, Not Just\n  Activations", "author": "Lucy Farnik and Tim Lawson and Conor Houghton and Laurence Aitchison", "abstract": "  Sparse autoencoders (SAEs) have been successfully used to discover sparse and\nhuman-interpretable representations of the latent activations of LLMs. However,\nwe would ultimately like to understand the computations performed by LLMs and\nnot just their representations. The extent to which SAEs can help us understand\ncomputations is unclear because they are not designed to \"sparsify\"\ncomputations in any sense, only latent activations. To solve this, we propose\nJacobian SAEs (JSAEs), which yield not only sparsity in the input and output\nactivations of a given model component but also sparsity in the computation\n(formally, the Jacobian) connecting them. With a na\\\"ive implementation, the\nJacobians in LLMs would be computationally intractable due to their size. One\nkey technical contribution is thus finding an efficient way of computing\nJacobians in this setup. We find that JSAEs extract a relatively large degree\nof computational sparsity while preserving downstream LLM performance\napproximately as well as traditional SAEs. We also show that Jacobians are a\nreasonable proxy for computational sparsity because MLPs are approximately\nlinear when rewritten in the JSAE basis. Lastly, we show that JSAEs achieve a\ngreater degree of computational sparsity on pre-trained LLMs than on the\nequivalent randomized LLM. This shows that the sparsity of the computational\ngraph appears to be a property that LLMs learn through training, and suggests\nthat JSAEs might be more suitable for understanding learned transformer\ncomputations than standard SAEs.\n", "link": "http://arxiv.org/abs/2502.18147v2", "date": "2025-06-06", "relevancy": 2.3388, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4864}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4675}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Jacobian%20Sparse%20Autoencoders%3A%20Sparsify%20Computations%2C%20Not%20Just%0A%20%20Activations&body=Title%3A%20Jacobian%20Sparse%20Autoencoders%3A%20Sparsify%20Computations%2C%20Not%20Just%0A%20%20Activations%0AAuthor%3A%20Lucy%20Farnik%20and%20Tim%20Lawson%20and%20Conor%20Houghton%20and%20Laurence%20Aitchison%0AAbstract%3A%20%20%20Sparse%20autoencoders%20%28SAEs%29%20have%20been%20successfully%20used%20to%20discover%20sparse%20and%0Ahuman-interpretable%20representations%20of%20the%20latent%20activations%20of%20LLMs.%20However%2C%0Awe%20would%20ultimately%20like%20to%20understand%20the%20computations%20performed%20by%20LLMs%20and%0Anot%20just%20their%20representations.%20The%20extent%20to%20which%20SAEs%20can%20help%20us%20understand%0Acomputations%20is%20unclear%20because%20they%20are%20not%20designed%20to%20%22sparsify%22%0Acomputations%20in%20any%20sense%2C%20only%20latent%20activations.%20To%20solve%20this%2C%20we%20propose%0AJacobian%20SAEs%20%28JSAEs%29%2C%20which%20yield%20not%20only%20sparsity%20in%20the%20input%20and%20output%0Aactivations%20of%20a%20given%20model%20component%20but%20also%20sparsity%20in%20the%20computation%0A%28formally%2C%20the%20Jacobian%29%20connecting%20them.%20With%20a%20na%5C%22ive%20implementation%2C%20the%0AJacobians%20in%20LLMs%20would%20be%20computationally%20intractable%20due%20to%20their%20size.%20One%0Akey%20technical%20contribution%20is%20thus%20finding%20an%20efficient%20way%20of%20computing%0AJacobians%20in%20this%20setup.%20We%20find%20that%20JSAEs%20extract%20a%20relatively%20large%20degree%0Aof%20computational%20sparsity%20while%20preserving%20downstream%20LLM%20performance%0Aapproximately%20as%20well%20as%20traditional%20SAEs.%20We%20also%20show%20that%20Jacobians%20are%20a%0Areasonable%20proxy%20for%20computational%20sparsity%20because%20MLPs%20are%20approximately%0Alinear%20when%20rewritten%20in%20the%20JSAE%20basis.%20Lastly%2C%20we%20show%20that%20JSAEs%20achieve%20a%0Agreater%20degree%20of%20computational%20sparsity%20on%20pre-trained%20LLMs%20than%20on%20the%0Aequivalent%20randomized%20LLM.%20This%20shows%20that%20the%20sparsity%20of%20the%20computational%0Agraph%20appears%20to%20be%20a%20property%20that%20LLMs%20learn%20through%20training%2C%20and%20suggests%0Athat%20JSAEs%20might%20be%20more%20suitable%20for%20understanding%20learned%20transformer%0Acomputations%20than%20standard%20SAEs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18147v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJacobian%2520Sparse%2520Autoencoders%253A%2520Sparsify%2520Computations%252C%2520Not%2520Just%250A%2520%2520Activations%26entry.906535625%3DLucy%2520Farnik%2520and%2520Tim%2520Lawson%2520and%2520Conor%2520Houghton%2520and%2520Laurence%2520Aitchison%26entry.1292438233%3D%2520%2520Sparse%2520autoencoders%2520%2528SAEs%2529%2520have%2520been%2520successfully%2520used%2520to%2520discover%2520sparse%2520and%250Ahuman-interpretable%2520representations%2520of%2520the%2520latent%2520activations%2520of%2520LLMs.%2520However%252C%250Awe%2520would%2520ultimately%2520like%2520to%2520understand%2520the%2520computations%2520performed%2520by%2520LLMs%2520and%250Anot%2520just%2520their%2520representations.%2520The%2520extent%2520to%2520which%2520SAEs%2520can%2520help%2520us%2520understand%250Acomputations%2520is%2520unclear%2520because%2520they%2520are%2520not%2520designed%2520to%2520%2522sparsify%2522%250Acomputations%2520in%2520any%2520sense%252C%2520only%2520latent%2520activations.%2520To%2520solve%2520this%252C%2520we%2520propose%250AJacobian%2520SAEs%2520%2528JSAEs%2529%252C%2520which%2520yield%2520not%2520only%2520sparsity%2520in%2520the%2520input%2520and%2520output%250Aactivations%2520of%2520a%2520given%2520model%2520component%2520but%2520also%2520sparsity%2520in%2520the%2520computation%250A%2528formally%252C%2520the%2520Jacobian%2529%2520connecting%2520them.%2520With%2520a%2520na%255C%2522ive%2520implementation%252C%2520the%250AJacobians%2520in%2520LLMs%2520would%2520be%2520computationally%2520intractable%2520due%2520to%2520their%2520size.%2520One%250Akey%2520technical%2520contribution%2520is%2520thus%2520finding%2520an%2520efficient%2520way%2520of%2520computing%250AJacobians%2520in%2520this%2520setup.%2520We%2520find%2520that%2520JSAEs%2520extract%2520a%2520relatively%2520large%2520degree%250Aof%2520computational%2520sparsity%2520while%2520preserving%2520downstream%2520LLM%2520performance%250Aapproximately%2520as%2520well%2520as%2520traditional%2520SAEs.%2520We%2520also%2520show%2520that%2520Jacobians%2520are%2520a%250Areasonable%2520proxy%2520for%2520computational%2520sparsity%2520because%2520MLPs%2520are%2520approximately%250Alinear%2520when%2520rewritten%2520in%2520the%2520JSAE%2520basis.%2520Lastly%252C%2520we%2520show%2520that%2520JSAEs%2520achieve%2520a%250Agreater%2520degree%2520of%2520computational%2520sparsity%2520on%2520pre-trained%2520LLMs%2520than%2520on%2520the%250Aequivalent%2520randomized%2520LLM.%2520This%2520shows%2520that%2520the%2520sparsity%2520of%2520the%2520computational%250Agraph%2520appears%2520to%2520be%2520a%2520property%2520that%2520LLMs%2520learn%2520through%2520training%252C%2520and%2520suggests%250Athat%2520JSAEs%2520might%2520be%2520more%2520suitable%2520for%2520understanding%2520learned%2520transformer%250Acomputations%2520than%2520standard%2520SAEs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18147v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Jacobian%20Sparse%20Autoencoders%3A%20Sparsify%20Computations%2C%20Not%20Just%0A%20%20Activations&entry.906535625=Lucy%20Farnik%20and%20Tim%20Lawson%20and%20Conor%20Houghton%20and%20Laurence%20Aitchison&entry.1292438233=%20%20Sparse%20autoencoders%20%28SAEs%29%20have%20been%20successfully%20used%20to%20discover%20sparse%20and%0Ahuman-interpretable%20representations%20of%20the%20latent%20activations%20of%20LLMs.%20However%2C%0Awe%20would%20ultimately%20like%20to%20understand%20the%20computations%20performed%20by%20LLMs%20and%0Anot%20just%20their%20representations.%20The%20extent%20to%20which%20SAEs%20can%20help%20us%20understand%0Acomputations%20is%20unclear%20because%20they%20are%20not%20designed%20to%20%22sparsify%22%0Acomputations%20in%20any%20sense%2C%20only%20latent%20activations.%20To%20solve%20this%2C%20we%20propose%0AJacobian%20SAEs%20%28JSAEs%29%2C%20which%20yield%20not%20only%20sparsity%20in%20the%20input%20and%20output%0Aactivations%20of%20a%20given%20model%20component%20but%20also%20sparsity%20in%20the%20computation%0A%28formally%2C%20the%20Jacobian%29%20connecting%20them.%20With%20a%20na%5C%22ive%20implementation%2C%20the%0AJacobians%20in%20LLMs%20would%20be%20computationally%20intractable%20due%20to%20their%20size.%20One%0Akey%20technical%20contribution%20is%20thus%20finding%20an%20efficient%20way%20of%20computing%0AJacobians%20in%20this%20setup.%20We%20find%20that%20JSAEs%20extract%20a%20relatively%20large%20degree%0Aof%20computational%20sparsity%20while%20preserving%20downstream%20LLM%20performance%0Aapproximately%20as%20well%20as%20traditional%20SAEs.%20We%20also%20show%20that%20Jacobians%20are%20a%0Areasonable%20proxy%20for%20computational%20sparsity%20because%20MLPs%20are%20approximately%0Alinear%20when%20rewritten%20in%20the%20JSAE%20basis.%20Lastly%2C%20we%20show%20that%20JSAEs%20achieve%20a%0Agreater%20degree%20of%20computational%20sparsity%20on%20pre-trained%20LLMs%20than%20on%20the%0Aequivalent%20randomized%20LLM.%20This%20shows%20that%20the%20sparsity%20of%20the%20computational%0Agraph%20appears%20to%20be%20a%20property%20that%20LLMs%20learn%20through%20training%2C%20and%20suggests%0Athat%20JSAEs%20might%20be%20more%20suitable%20for%20understanding%20learned%20transformer%0Acomputations%20than%20standard%20SAEs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18147v2&entry.124074799=Read"},
{"title": "STSBench: A Spatio-temporal Scenario Benchmark for Multi-modal Large\n  Language Models in Autonomous Driving", "author": "Christian Fruhwirth-Reisinger and Du\u0161an Mali\u0107 and Wei Lin and David Schinagl and Samuel Schulter and Horst Possegger", "abstract": "  We introduce STSBench, a scenario-based framework to benchmark the holistic\nunderstanding of vision-language models (VLMs) for autonomous driving. The\nframework automatically mines pre-defined traffic scenarios from any dataset\nusing ground-truth annotations, provides an intuitive user interface for\nefficient human verification, and generates multiple-choice questions for model\nevaluation. Applied to the NuScenes dataset, we present STSnu, the first\nbenchmark that evaluates the spatio-temporal reasoning capabilities of VLMs\nbased on comprehensive 3D perception. Existing benchmarks typically target\noff-the-shelf or fine-tuned VLMs for images or videos from a single viewpoint\nand focus on semantic tasks such as object recognition, dense captioning, risk\nassessment, or scene understanding. In contrast, STSnu evaluates driving expert\nVLMs for end-to-end driving, operating on videos from multi-view cameras or\nLiDAR. It specifically assesses their ability to reason about both ego-vehicle\nactions and complex interactions among traffic participants, a crucial\ncapability for autonomous vehicles. The benchmark features 43 diverse scenarios\nspanning multiple views and frames, resulting in 971 human-verified\nmultiple-choice questions. A thorough evaluation uncovers critical shortcomings\nin existing models' ability to reason about fundamental traffic dynamics in\ncomplex environments. These findings highlight the urgent need for\narchitectural advances that explicitly model spatio-temporal reasoning. By\naddressing a core gap in spatio-temporal evaluation, STSBench enables the\ndevelopment of more robust and explainable VLMs for autonomous driving.\n", "link": "http://arxiv.org/abs/2506.06218v1", "date": "2025-06-06", "relevancy": 2.3374, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5914}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5914}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STSBench%3A%20A%20Spatio-temporal%20Scenario%20Benchmark%20for%20Multi-modal%20Large%0A%20%20Language%20Models%20in%20Autonomous%20Driving&body=Title%3A%20STSBench%3A%20A%20Spatio-temporal%20Scenario%20Benchmark%20for%20Multi-modal%20Large%0A%20%20Language%20Models%20in%20Autonomous%20Driving%0AAuthor%3A%20Christian%20Fruhwirth-Reisinger%20and%20Du%C5%A1an%20Mali%C4%87%20and%20Wei%20Lin%20and%20David%20Schinagl%20and%20Samuel%20Schulter%20and%20Horst%20Possegger%0AAbstract%3A%20%20%20We%20introduce%20STSBench%2C%20a%20scenario-based%20framework%20to%20benchmark%20the%20holistic%0Aunderstanding%20of%20vision-language%20models%20%28VLMs%29%20for%20autonomous%20driving.%20The%0Aframework%20automatically%20mines%20pre-defined%20traffic%20scenarios%20from%20any%20dataset%0Ausing%20ground-truth%20annotations%2C%20provides%20an%20intuitive%20user%20interface%20for%0Aefficient%20human%20verification%2C%20and%20generates%20multiple-choice%20questions%20for%20model%0Aevaluation.%20Applied%20to%20the%20NuScenes%20dataset%2C%20we%20present%20STSnu%2C%20the%20first%0Abenchmark%20that%20evaluates%20the%20spatio-temporal%20reasoning%20capabilities%20of%20VLMs%0Abased%20on%20comprehensive%203D%20perception.%20Existing%20benchmarks%20typically%20target%0Aoff-the-shelf%20or%20fine-tuned%20VLMs%20for%20images%20or%20videos%20from%20a%20single%20viewpoint%0Aand%20focus%20on%20semantic%20tasks%20such%20as%20object%20recognition%2C%20dense%20captioning%2C%20risk%0Aassessment%2C%20or%20scene%20understanding.%20In%20contrast%2C%20STSnu%20evaluates%20driving%20expert%0AVLMs%20for%20end-to-end%20driving%2C%20operating%20on%20videos%20from%20multi-view%20cameras%20or%0ALiDAR.%20It%20specifically%20assesses%20their%20ability%20to%20reason%20about%20both%20ego-vehicle%0Aactions%20and%20complex%20interactions%20among%20traffic%20participants%2C%20a%20crucial%0Acapability%20for%20autonomous%20vehicles.%20The%20benchmark%20features%2043%20diverse%20scenarios%0Aspanning%20multiple%20views%20and%20frames%2C%20resulting%20in%20971%20human-verified%0Amultiple-choice%20questions.%20A%20thorough%20evaluation%20uncovers%20critical%20shortcomings%0Ain%20existing%20models%27%20ability%20to%20reason%20about%20fundamental%20traffic%20dynamics%20in%0Acomplex%20environments.%20These%20findings%20highlight%20the%20urgent%20need%20for%0Aarchitectural%20advances%20that%20explicitly%20model%20spatio-temporal%20reasoning.%20By%0Aaddressing%20a%20core%20gap%20in%20spatio-temporal%20evaluation%2C%20STSBench%20enables%20the%0Adevelopment%20of%20more%20robust%20and%20explainable%20VLMs%20for%20autonomous%20driving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06218v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTSBench%253A%2520A%2520Spatio-temporal%2520Scenario%2520Benchmark%2520for%2520Multi-modal%2520Large%250A%2520%2520Language%2520Models%2520in%2520Autonomous%2520Driving%26entry.906535625%3DChristian%2520Fruhwirth-Reisinger%2520and%2520Du%25C5%25A1an%2520Mali%25C4%2587%2520and%2520Wei%2520Lin%2520and%2520David%2520Schinagl%2520and%2520Samuel%2520Schulter%2520and%2520Horst%2520Possegger%26entry.1292438233%3D%2520%2520We%2520introduce%2520STSBench%252C%2520a%2520scenario-based%2520framework%2520to%2520benchmark%2520the%2520holistic%250Aunderstanding%2520of%2520vision-language%2520models%2520%2528VLMs%2529%2520for%2520autonomous%2520driving.%2520The%250Aframework%2520automatically%2520mines%2520pre-defined%2520traffic%2520scenarios%2520from%2520any%2520dataset%250Ausing%2520ground-truth%2520annotations%252C%2520provides%2520an%2520intuitive%2520user%2520interface%2520for%250Aefficient%2520human%2520verification%252C%2520and%2520generates%2520multiple-choice%2520questions%2520for%2520model%250Aevaluation.%2520Applied%2520to%2520the%2520NuScenes%2520dataset%252C%2520we%2520present%2520STSnu%252C%2520the%2520first%250Abenchmark%2520that%2520evaluates%2520the%2520spatio-temporal%2520reasoning%2520capabilities%2520of%2520VLMs%250Abased%2520on%2520comprehensive%25203D%2520perception.%2520Existing%2520benchmarks%2520typically%2520target%250Aoff-the-shelf%2520or%2520fine-tuned%2520VLMs%2520for%2520images%2520or%2520videos%2520from%2520a%2520single%2520viewpoint%250Aand%2520focus%2520on%2520semantic%2520tasks%2520such%2520as%2520object%2520recognition%252C%2520dense%2520captioning%252C%2520risk%250Aassessment%252C%2520or%2520scene%2520understanding.%2520In%2520contrast%252C%2520STSnu%2520evaluates%2520driving%2520expert%250AVLMs%2520for%2520end-to-end%2520driving%252C%2520operating%2520on%2520videos%2520from%2520multi-view%2520cameras%2520or%250ALiDAR.%2520It%2520specifically%2520assesses%2520their%2520ability%2520to%2520reason%2520about%2520both%2520ego-vehicle%250Aactions%2520and%2520complex%2520interactions%2520among%2520traffic%2520participants%252C%2520a%2520crucial%250Acapability%2520for%2520autonomous%2520vehicles.%2520The%2520benchmark%2520features%252043%2520diverse%2520scenarios%250Aspanning%2520multiple%2520views%2520and%2520frames%252C%2520resulting%2520in%2520971%2520human-verified%250Amultiple-choice%2520questions.%2520A%2520thorough%2520evaluation%2520uncovers%2520critical%2520shortcomings%250Ain%2520existing%2520models%2527%2520ability%2520to%2520reason%2520about%2520fundamental%2520traffic%2520dynamics%2520in%250Acomplex%2520environments.%2520These%2520findings%2520highlight%2520the%2520urgent%2520need%2520for%250Aarchitectural%2520advances%2520that%2520explicitly%2520model%2520spatio-temporal%2520reasoning.%2520By%250Aaddressing%2520a%2520core%2520gap%2520in%2520spatio-temporal%2520evaluation%252C%2520STSBench%2520enables%2520the%250Adevelopment%2520of%2520more%2520robust%2520and%2520explainable%2520VLMs%2520for%2520autonomous%2520driving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06218v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STSBench%3A%20A%20Spatio-temporal%20Scenario%20Benchmark%20for%20Multi-modal%20Large%0A%20%20Language%20Models%20in%20Autonomous%20Driving&entry.906535625=Christian%20Fruhwirth-Reisinger%20and%20Du%C5%A1an%20Mali%C4%87%20and%20Wei%20Lin%20and%20David%20Schinagl%20and%20Samuel%20Schulter%20and%20Horst%20Possegger&entry.1292438233=%20%20We%20introduce%20STSBench%2C%20a%20scenario-based%20framework%20to%20benchmark%20the%20holistic%0Aunderstanding%20of%20vision-language%20models%20%28VLMs%29%20for%20autonomous%20driving.%20The%0Aframework%20automatically%20mines%20pre-defined%20traffic%20scenarios%20from%20any%20dataset%0Ausing%20ground-truth%20annotations%2C%20provides%20an%20intuitive%20user%20interface%20for%0Aefficient%20human%20verification%2C%20and%20generates%20multiple-choice%20questions%20for%20model%0Aevaluation.%20Applied%20to%20the%20NuScenes%20dataset%2C%20we%20present%20STSnu%2C%20the%20first%0Abenchmark%20that%20evaluates%20the%20spatio-temporal%20reasoning%20capabilities%20of%20VLMs%0Abased%20on%20comprehensive%203D%20perception.%20Existing%20benchmarks%20typically%20target%0Aoff-the-shelf%20or%20fine-tuned%20VLMs%20for%20images%20or%20videos%20from%20a%20single%20viewpoint%0Aand%20focus%20on%20semantic%20tasks%20such%20as%20object%20recognition%2C%20dense%20captioning%2C%20risk%0Aassessment%2C%20or%20scene%20understanding.%20In%20contrast%2C%20STSnu%20evaluates%20driving%20expert%0AVLMs%20for%20end-to-end%20driving%2C%20operating%20on%20videos%20from%20multi-view%20cameras%20or%0ALiDAR.%20It%20specifically%20assesses%20their%20ability%20to%20reason%20about%20both%20ego-vehicle%0Aactions%20and%20complex%20interactions%20among%20traffic%20participants%2C%20a%20crucial%0Acapability%20for%20autonomous%20vehicles.%20The%20benchmark%20features%2043%20diverse%20scenarios%0Aspanning%20multiple%20views%20and%20frames%2C%20resulting%20in%20971%20human-verified%0Amultiple-choice%20questions.%20A%20thorough%20evaluation%20uncovers%20critical%20shortcomings%0Ain%20existing%20models%27%20ability%20to%20reason%20about%20fundamental%20traffic%20dynamics%20in%0Acomplex%20environments.%20These%20findings%20highlight%20the%20urgent%20need%20for%0Aarchitectural%20advances%20that%20explicitly%20model%20spatio-temporal%20reasoning.%20By%0Aaddressing%20a%20core%20gap%20in%20spatio-temporal%20evaluation%2C%20STSBench%20enables%20the%0Adevelopment%20of%20more%20robust%20and%20explainable%20VLMs%20for%20autonomous%20driving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06218v1&entry.124074799=Read"},
{"title": "ICU-TSB: A Benchmark for Temporal Patient Representation Learning for\n  Unsupervised Stratification into Patient Cohorts", "author": "Dimitrios Proios and Alban Bornet and Anthony Yazdani and Jose F Rodrigues Jr and Douglas Teodoro", "abstract": "  Patient stratification identifying clinically meaningful subgroups is\nessential for advancing personalized medicine through improved diagnostics and\ntreatment strategies. Electronic health records (EHRs), particularly those from\nintensive care units (ICUs), contain rich temporal clinical data that can be\nleveraged for this purpose. In this work, we introduce ICU-TSB (Temporal\nStratification Benchmark), the first comprehensive benchmark for evaluating\npatient stratification based on temporal patient representation learning using\nthree publicly available ICU EHR datasets. A key contribution of our benchmark\nis a novel hierarchical evaluation framework utilizing disease taxonomies to\nmeasure the alignment of discovered clusters with clinically validated disease\ngroupings. In our experiments with ICU-TSB, we compared statistical methods and\nseveral recurrent neural networks, including LSTM and GRU, for their ability to\ngenerate effective patient representations for subsequent clustering of patient\ntrajectories. Our results demonstrate that temporal representation learning can\nrediscover clinically meaningful patient cohorts; nevertheless, it remains a\nchallenging task, with v-measuring varying from up to 0.46 at the top level of\nthe taxonomy to up to 0.40 at the lowest level. To further enhance the\npractical utility of our findings, we also evaluate multiple strategies for\nassigning interpretable labels to the identified clusters. The experiments and\nbenchmark are fully reproducible and available at\nhttps://github.com/ds4dh/CBMS2025stratification.\n", "link": "http://arxiv.org/abs/2506.06192v1", "date": "2025-06-06", "relevancy": 2.3251, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4795}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4629}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ICU-TSB%3A%20A%20Benchmark%20for%20Temporal%20Patient%20Representation%20Learning%20for%0A%20%20Unsupervised%20Stratification%20into%20Patient%20Cohorts&body=Title%3A%20ICU-TSB%3A%20A%20Benchmark%20for%20Temporal%20Patient%20Representation%20Learning%20for%0A%20%20Unsupervised%20Stratification%20into%20Patient%20Cohorts%0AAuthor%3A%20Dimitrios%20Proios%20and%20Alban%20Bornet%20and%20Anthony%20Yazdani%20and%20Jose%20F%20Rodrigues%20Jr%20and%20Douglas%20Teodoro%0AAbstract%3A%20%20%20Patient%20stratification%20identifying%20clinically%20meaningful%20subgroups%20is%0Aessential%20for%20advancing%20personalized%20medicine%20through%20improved%20diagnostics%20and%0Atreatment%20strategies.%20Electronic%20health%20records%20%28EHRs%29%2C%20particularly%20those%20from%0Aintensive%20care%20units%20%28ICUs%29%2C%20contain%20rich%20temporal%20clinical%20data%20that%20can%20be%0Aleveraged%20for%20this%20purpose.%20In%20this%20work%2C%20we%20introduce%20ICU-TSB%20%28Temporal%0AStratification%20Benchmark%29%2C%20the%20first%20comprehensive%20benchmark%20for%20evaluating%0Apatient%20stratification%20based%20on%20temporal%20patient%20representation%20learning%20using%0Athree%20publicly%20available%20ICU%20EHR%20datasets.%20A%20key%20contribution%20of%20our%20benchmark%0Ais%20a%20novel%20hierarchical%20evaluation%20framework%20utilizing%20disease%20taxonomies%20to%0Ameasure%20the%20alignment%20of%20discovered%20clusters%20with%20clinically%20validated%20disease%0Agroupings.%20In%20our%20experiments%20with%20ICU-TSB%2C%20we%20compared%20statistical%20methods%20and%0Aseveral%20recurrent%20neural%20networks%2C%20including%20LSTM%20and%20GRU%2C%20for%20their%20ability%20to%0Agenerate%20effective%20patient%20representations%20for%20subsequent%20clustering%20of%20patient%0Atrajectories.%20Our%20results%20demonstrate%20that%20temporal%20representation%20learning%20can%0Arediscover%20clinically%20meaningful%20patient%20cohorts%3B%20nevertheless%2C%20it%20remains%20a%0Achallenging%20task%2C%20with%20v-measuring%20varying%20from%20up%20to%200.46%20at%20the%20top%20level%20of%0Athe%20taxonomy%20to%20up%20to%200.40%20at%20the%20lowest%20level.%20To%20further%20enhance%20the%0Apractical%20utility%20of%20our%20findings%2C%20we%20also%20evaluate%20multiple%20strategies%20for%0Aassigning%20interpretable%20labels%20to%20the%20identified%20clusters.%20The%20experiments%20and%0Abenchmark%20are%20fully%20reproducible%20and%20available%20at%0Ahttps%3A//github.com/ds4dh/CBMS2025stratification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06192v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DICU-TSB%253A%2520A%2520Benchmark%2520for%2520Temporal%2520Patient%2520Representation%2520Learning%2520for%250A%2520%2520Unsupervised%2520Stratification%2520into%2520Patient%2520Cohorts%26entry.906535625%3DDimitrios%2520Proios%2520and%2520Alban%2520Bornet%2520and%2520Anthony%2520Yazdani%2520and%2520Jose%2520F%2520Rodrigues%2520Jr%2520and%2520Douglas%2520Teodoro%26entry.1292438233%3D%2520%2520Patient%2520stratification%2520identifying%2520clinically%2520meaningful%2520subgroups%2520is%250Aessential%2520for%2520advancing%2520personalized%2520medicine%2520through%2520improved%2520diagnostics%2520and%250Atreatment%2520strategies.%2520Electronic%2520health%2520records%2520%2528EHRs%2529%252C%2520particularly%2520those%2520from%250Aintensive%2520care%2520units%2520%2528ICUs%2529%252C%2520contain%2520rich%2520temporal%2520clinical%2520data%2520that%2520can%2520be%250Aleveraged%2520for%2520this%2520purpose.%2520In%2520this%2520work%252C%2520we%2520introduce%2520ICU-TSB%2520%2528Temporal%250AStratification%2520Benchmark%2529%252C%2520the%2520first%2520comprehensive%2520benchmark%2520for%2520evaluating%250Apatient%2520stratification%2520based%2520on%2520temporal%2520patient%2520representation%2520learning%2520using%250Athree%2520publicly%2520available%2520ICU%2520EHR%2520datasets.%2520A%2520key%2520contribution%2520of%2520our%2520benchmark%250Ais%2520a%2520novel%2520hierarchical%2520evaluation%2520framework%2520utilizing%2520disease%2520taxonomies%2520to%250Ameasure%2520the%2520alignment%2520of%2520discovered%2520clusters%2520with%2520clinically%2520validated%2520disease%250Agroupings.%2520In%2520our%2520experiments%2520with%2520ICU-TSB%252C%2520we%2520compared%2520statistical%2520methods%2520and%250Aseveral%2520recurrent%2520neural%2520networks%252C%2520including%2520LSTM%2520and%2520GRU%252C%2520for%2520their%2520ability%2520to%250Agenerate%2520effective%2520patient%2520representations%2520for%2520subsequent%2520clustering%2520of%2520patient%250Atrajectories.%2520Our%2520results%2520demonstrate%2520that%2520temporal%2520representation%2520learning%2520can%250Arediscover%2520clinically%2520meaningful%2520patient%2520cohorts%253B%2520nevertheless%252C%2520it%2520remains%2520a%250Achallenging%2520task%252C%2520with%2520v-measuring%2520varying%2520from%2520up%2520to%25200.46%2520at%2520the%2520top%2520level%2520of%250Athe%2520taxonomy%2520to%2520up%2520to%25200.40%2520at%2520the%2520lowest%2520level.%2520To%2520further%2520enhance%2520the%250Apractical%2520utility%2520of%2520our%2520findings%252C%2520we%2520also%2520evaluate%2520multiple%2520strategies%2520for%250Aassigning%2520interpretable%2520labels%2520to%2520the%2520identified%2520clusters.%2520The%2520experiments%2520and%250Abenchmark%2520are%2520fully%2520reproducible%2520and%2520available%2520at%250Ahttps%253A//github.com/ds4dh/CBMS2025stratification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06192v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ICU-TSB%3A%20A%20Benchmark%20for%20Temporal%20Patient%20Representation%20Learning%20for%0A%20%20Unsupervised%20Stratification%20into%20Patient%20Cohorts&entry.906535625=Dimitrios%20Proios%20and%20Alban%20Bornet%20and%20Anthony%20Yazdani%20and%20Jose%20F%20Rodrigues%20Jr%20and%20Douglas%20Teodoro&entry.1292438233=%20%20Patient%20stratification%20identifying%20clinically%20meaningful%20subgroups%20is%0Aessential%20for%20advancing%20personalized%20medicine%20through%20improved%20diagnostics%20and%0Atreatment%20strategies.%20Electronic%20health%20records%20%28EHRs%29%2C%20particularly%20those%20from%0Aintensive%20care%20units%20%28ICUs%29%2C%20contain%20rich%20temporal%20clinical%20data%20that%20can%20be%0Aleveraged%20for%20this%20purpose.%20In%20this%20work%2C%20we%20introduce%20ICU-TSB%20%28Temporal%0AStratification%20Benchmark%29%2C%20the%20first%20comprehensive%20benchmark%20for%20evaluating%0Apatient%20stratification%20based%20on%20temporal%20patient%20representation%20learning%20using%0Athree%20publicly%20available%20ICU%20EHR%20datasets.%20A%20key%20contribution%20of%20our%20benchmark%0Ais%20a%20novel%20hierarchical%20evaluation%20framework%20utilizing%20disease%20taxonomies%20to%0Ameasure%20the%20alignment%20of%20discovered%20clusters%20with%20clinically%20validated%20disease%0Agroupings.%20In%20our%20experiments%20with%20ICU-TSB%2C%20we%20compared%20statistical%20methods%20and%0Aseveral%20recurrent%20neural%20networks%2C%20including%20LSTM%20and%20GRU%2C%20for%20their%20ability%20to%0Agenerate%20effective%20patient%20representations%20for%20subsequent%20clustering%20of%20patient%0Atrajectories.%20Our%20results%20demonstrate%20that%20temporal%20representation%20learning%20can%0Arediscover%20clinically%20meaningful%20patient%20cohorts%3B%20nevertheless%2C%20it%20remains%20a%0Achallenging%20task%2C%20with%20v-measuring%20varying%20from%20up%20to%200.46%20at%20the%20top%20level%20of%0Athe%20taxonomy%20to%20up%20to%200.40%20at%20the%20lowest%20level.%20To%20further%20enhance%20the%0Apractical%20utility%20of%20our%20findings%2C%20we%20also%20evaluate%20multiple%20strategies%20for%0Aassigning%20interpretable%20labels%20to%20the%20identified%20clusters.%20The%20experiments%20and%0Abenchmark%20are%20fully%20reproducible%20and%20available%20at%0Ahttps%3A//github.com/ds4dh/CBMS2025stratification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06192v1&entry.124074799=Read"},
{"title": "Towards an Explainable Comparison and Alignment of Feature Embeddings", "author": "Mohammad Jalali and Bahar Dibaei Nia and Farzan Farnia", "abstract": "  While several feature embedding models have been developed in the literature,\ncomparisons of these embeddings have largely focused on their numerical\nperformance in classification-related downstream applications. However, an\ninterpretable comparison of different embeddings requires identifying and\nanalyzing mismatches between sample groups clustered within the embedding\nspaces. In this work, we propose the \\emph{Spectral Pairwise Embedding\nComparison (SPEC)} framework to compare embeddings and identify their\ndifferences in clustering a reference dataset. Our approach examines the kernel\nmatrices derived from two embeddings and leverages the eigendecomposition of\nthe difference kernel matrix to detect sample clusters that are captured\ndifferently by the two embeddings. We present a scalable implementation of this\nkernel-based approach, with computational complexity that grows linearly with\nthe sample size. Furthermore, we introduce an optimization problem using this\nframework to align two embeddings, ensuring that clusters identified in one\nembedding are also captured in the other model. We provide numerical results\ndemonstrating the SPEC's application to compare and align embeddings on\nlarge-scale datasets such as ImageNet and MS-COCO. The code is available at\n[https://github.com/mjalali/embedding-comparison](github.com/mjalali/embedding-comparison).\n", "link": "http://arxiv.org/abs/2506.06231v1", "date": "2025-06-06", "relevancy": 2.3133, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4628}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4626}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20an%20Explainable%20Comparison%20and%20Alignment%20of%20Feature%20Embeddings&body=Title%3A%20Towards%20an%20Explainable%20Comparison%20and%20Alignment%20of%20Feature%20Embeddings%0AAuthor%3A%20Mohammad%20Jalali%20and%20Bahar%20Dibaei%20Nia%20and%20Farzan%20Farnia%0AAbstract%3A%20%20%20While%20several%20feature%20embedding%20models%20have%20been%20developed%20in%20the%20literature%2C%0Acomparisons%20of%20these%20embeddings%20have%20largely%20focused%20on%20their%20numerical%0Aperformance%20in%20classification-related%20downstream%20applications.%20However%2C%20an%0Ainterpretable%20comparison%20of%20different%20embeddings%20requires%20identifying%20and%0Aanalyzing%20mismatches%20between%20sample%20groups%20clustered%20within%20the%20embedding%0Aspaces.%20In%20this%20work%2C%20we%20propose%20the%20%5Cemph%7BSpectral%20Pairwise%20Embedding%0AComparison%20%28SPEC%29%7D%20framework%20to%20compare%20embeddings%20and%20identify%20their%0Adifferences%20in%20clustering%20a%20reference%20dataset.%20Our%20approach%20examines%20the%20kernel%0Amatrices%20derived%20from%20two%20embeddings%20and%20leverages%20the%20eigendecomposition%20of%0Athe%20difference%20kernel%20matrix%20to%20detect%20sample%20clusters%20that%20are%20captured%0Adifferently%20by%20the%20two%20embeddings.%20We%20present%20a%20scalable%20implementation%20of%20this%0Akernel-based%20approach%2C%20with%20computational%20complexity%20that%20grows%20linearly%20with%0Athe%20sample%20size.%20Furthermore%2C%20we%20introduce%20an%20optimization%20problem%20using%20this%0Aframework%20to%20align%20two%20embeddings%2C%20ensuring%20that%20clusters%20identified%20in%20one%0Aembedding%20are%20also%20captured%20in%20the%20other%20model.%20We%20provide%20numerical%20results%0Ademonstrating%20the%20SPEC%27s%20application%20to%20compare%20and%20align%20embeddings%20on%0Alarge-scale%20datasets%20such%20as%20ImageNet%20and%20MS-COCO.%20The%20code%20is%20available%20at%0A%5Bhttps%3A//github.com/mjalali/embedding-comparison%5D%28github.com/mjalali/embedding-comparison%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06231v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520an%2520Explainable%2520Comparison%2520and%2520Alignment%2520of%2520Feature%2520Embeddings%26entry.906535625%3DMohammad%2520Jalali%2520and%2520Bahar%2520Dibaei%2520Nia%2520and%2520Farzan%2520Farnia%26entry.1292438233%3D%2520%2520While%2520several%2520feature%2520embedding%2520models%2520have%2520been%2520developed%2520in%2520the%2520literature%252C%250Acomparisons%2520of%2520these%2520embeddings%2520have%2520largely%2520focused%2520on%2520their%2520numerical%250Aperformance%2520in%2520classification-related%2520downstream%2520applications.%2520However%252C%2520an%250Ainterpretable%2520comparison%2520of%2520different%2520embeddings%2520requires%2520identifying%2520and%250Aanalyzing%2520mismatches%2520between%2520sample%2520groups%2520clustered%2520within%2520the%2520embedding%250Aspaces.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520%255Cemph%257BSpectral%2520Pairwise%2520Embedding%250AComparison%2520%2528SPEC%2529%257D%2520framework%2520to%2520compare%2520embeddings%2520and%2520identify%2520their%250Adifferences%2520in%2520clustering%2520a%2520reference%2520dataset.%2520Our%2520approach%2520examines%2520the%2520kernel%250Amatrices%2520derived%2520from%2520two%2520embeddings%2520and%2520leverages%2520the%2520eigendecomposition%2520of%250Athe%2520difference%2520kernel%2520matrix%2520to%2520detect%2520sample%2520clusters%2520that%2520are%2520captured%250Adifferently%2520by%2520the%2520two%2520embeddings.%2520We%2520present%2520a%2520scalable%2520implementation%2520of%2520this%250Akernel-based%2520approach%252C%2520with%2520computational%2520complexity%2520that%2520grows%2520linearly%2520with%250Athe%2520sample%2520size.%2520Furthermore%252C%2520we%2520introduce%2520an%2520optimization%2520problem%2520using%2520this%250Aframework%2520to%2520align%2520two%2520embeddings%252C%2520ensuring%2520that%2520clusters%2520identified%2520in%2520one%250Aembedding%2520are%2520also%2520captured%2520in%2520the%2520other%2520model.%2520We%2520provide%2520numerical%2520results%250Ademonstrating%2520the%2520SPEC%2527s%2520application%2520to%2520compare%2520and%2520align%2520embeddings%2520on%250Alarge-scale%2520datasets%2520such%2520as%2520ImageNet%2520and%2520MS-COCO.%2520The%2520code%2520is%2520available%2520at%250A%255Bhttps%253A//github.com/mjalali/embedding-comparison%255D%2528github.com/mjalali/embedding-comparison%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06231v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20an%20Explainable%20Comparison%20and%20Alignment%20of%20Feature%20Embeddings&entry.906535625=Mohammad%20Jalali%20and%20Bahar%20Dibaei%20Nia%20and%20Farzan%20Farnia&entry.1292438233=%20%20While%20several%20feature%20embedding%20models%20have%20been%20developed%20in%20the%20literature%2C%0Acomparisons%20of%20these%20embeddings%20have%20largely%20focused%20on%20their%20numerical%0Aperformance%20in%20classification-related%20downstream%20applications.%20However%2C%20an%0Ainterpretable%20comparison%20of%20different%20embeddings%20requires%20identifying%20and%0Aanalyzing%20mismatches%20between%20sample%20groups%20clustered%20within%20the%20embedding%0Aspaces.%20In%20this%20work%2C%20we%20propose%20the%20%5Cemph%7BSpectral%20Pairwise%20Embedding%0AComparison%20%28SPEC%29%7D%20framework%20to%20compare%20embeddings%20and%20identify%20their%0Adifferences%20in%20clustering%20a%20reference%20dataset.%20Our%20approach%20examines%20the%20kernel%0Amatrices%20derived%20from%20two%20embeddings%20and%20leverages%20the%20eigendecomposition%20of%0Athe%20difference%20kernel%20matrix%20to%20detect%20sample%20clusters%20that%20are%20captured%0Adifferently%20by%20the%20two%20embeddings.%20We%20present%20a%20scalable%20implementation%20of%20this%0Akernel-based%20approach%2C%20with%20computational%20complexity%20that%20grows%20linearly%20with%0Athe%20sample%20size.%20Furthermore%2C%20we%20introduce%20an%20optimization%20problem%20using%20this%0Aframework%20to%20align%20two%20embeddings%2C%20ensuring%20that%20clusters%20identified%20in%20one%0Aembedding%20are%20also%20captured%20in%20the%20other%20model.%20We%20provide%20numerical%20results%0Ademonstrating%20the%20SPEC%27s%20application%20to%20compare%20and%20align%20embeddings%20on%0Alarge-scale%20datasets%20such%20as%20ImageNet%20and%20MS-COCO.%20The%20code%20is%20available%20at%0A%5Bhttps%3A//github.com/mjalali/embedding-comparison%5D%28github.com/mjalali/embedding-comparison%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06231v1&entry.124074799=Read"},
{"title": "Improving Long-Range Navigation with Spatially-Enhanced Recurrent Memory\n  via End-to-End Reinforcement Learning", "author": "Fan Yang and Per Frivik and David Hoeller and Chen Wang and Cesar Cadena and Marco Hutter", "abstract": "  Recent advancements in robot navigation, especially with end-to-end learning\napproaches like reinforcement learning (RL), have shown remarkable efficiency\nand effectiveness. Yet, successful navigation still relies on two key\ncapabilities: mapping and planning, whether explicit or implicit. Classical\napproaches use explicit mapping pipelines to register ego-centric observations\ninto a coherent map frame for the planner. In contrast, end-to-end learning\nachieves this implicitly, often through recurrent neural networks (RNNs) that\nfuse current and past observations into a latent space for planning. While\narchitectures such as LSTM and GRU capture temporal dependencies, our findings\nreveal a key limitation: their inability to perform effective spatial\nmemorization. This skill is essential for transforming and integrating\nsequential observations from varying perspectives to build spatial\nrepresentations that support downstream planning. To address this, we propose\nSpatially-Enhanced Recurrent Units (SRUs), a simple yet effective modification\nto existing RNNs, designed to enhance spatial memorization capabilities. We\nintroduce an attention-based architecture with SRUs, enabling long-range\nnavigation using a single forward-facing stereo camera. Regularization\ntechniques are employed to ensure robust end-to-end recurrent training via RL.\nExperimental results show our approach improves long-range navigation by 23.5%\ncompared to existing RNNs. Furthermore, with SRU memory, our method outperforms\nthe RL baseline with explicit mapping and memory modules, achieving a 29.6%\nimprovement in diverse environments requiring long-horizon mapping and\nmemorization. Finally, we address the sim-to-real gap by leveraging large-scale\npretraining on synthetic depth data, enabling zero-shot transfer to diverse and\ncomplex real-world environments.\n", "link": "http://arxiv.org/abs/2506.05997v1", "date": "2025-06-06", "relevancy": 2.282, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5835}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5661}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Long-Range%20Navigation%20with%20Spatially-Enhanced%20Recurrent%20Memory%0A%20%20via%20End-to-End%20Reinforcement%20Learning&body=Title%3A%20Improving%20Long-Range%20Navigation%20with%20Spatially-Enhanced%20Recurrent%20Memory%0A%20%20via%20End-to-End%20Reinforcement%20Learning%0AAuthor%3A%20Fan%20Yang%20and%20Per%20Frivik%20and%20David%20Hoeller%20and%20Chen%20Wang%20and%20Cesar%20Cadena%20and%20Marco%20Hutter%0AAbstract%3A%20%20%20Recent%20advancements%20in%20robot%20navigation%2C%20especially%20with%20end-to-end%20learning%0Aapproaches%20like%20reinforcement%20learning%20%28RL%29%2C%20have%20shown%20remarkable%20efficiency%0Aand%20effectiveness.%20Yet%2C%20successful%20navigation%20still%20relies%20on%20two%20key%0Acapabilities%3A%20mapping%20and%20planning%2C%20whether%20explicit%20or%20implicit.%20Classical%0Aapproaches%20use%20explicit%20mapping%20pipelines%20to%20register%20ego-centric%20observations%0Ainto%20a%20coherent%20map%20frame%20for%20the%20planner.%20In%20contrast%2C%20end-to-end%20learning%0Aachieves%20this%20implicitly%2C%20often%20through%20recurrent%20neural%20networks%20%28RNNs%29%20that%0Afuse%20current%20and%20past%20observations%20into%20a%20latent%20space%20for%20planning.%20While%0Aarchitectures%20such%20as%20LSTM%20and%20GRU%20capture%20temporal%20dependencies%2C%20our%20findings%0Areveal%20a%20key%20limitation%3A%20their%20inability%20to%20perform%20effective%20spatial%0Amemorization.%20This%20skill%20is%20essential%20for%20transforming%20and%20integrating%0Asequential%20observations%20from%20varying%20perspectives%20to%20build%20spatial%0Arepresentations%20that%20support%20downstream%20planning.%20To%20address%20this%2C%20we%20propose%0ASpatially-Enhanced%20Recurrent%20Units%20%28SRUs%29%2C%20a%20simple%20yet%20effective%20modification%0Ato%20existing%20RNNs%2C%20designed%20to%20enhance%20spatial%20memorization%20capabilities.%20We%0Aintroduce%20an%20attention-based%20architecture%20with%20SRUs%2C%20enabling%20long-range%0Anavigation%20using%20a%20single%20forward-facing%20stereo%20camera.%20Regularization%0Atechniques%20are%20employed%20to%20ensure%20robust%20end-to-end%20recurrent%20training%20via%20RL.%0AExperimental%20results%20show%20our%20approach%20improves%20long-range%20navigation%20by%2023.5%25%0Acompared%20to%20existing%20RNNs.%20Furthermore%2C%20with%20SRU%20memory%2C%20our%20method%20outperforms%0Athe%20RL%20baseline%20with%20explicit%20mapping%20and%20memory%20modules%2C%20achieving%20a%2029.6%25%0Aimprovement%20in%20diverse%20environments%20requiring%20long-horizon%20mapping%20and%0Amemorization.%20Finally%2C%20we%20address%20the%20sim-to-real%20gap%20by%20leveraging%20large-scale%0Apretraining%20on%20synthetic%20depth%20data%2C%20enabling%20zero-shot%20transfer%20to%20diverse%20and%0Acomplex%20real-world%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05997v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Long-Range%2520Navigation%2520with%2520Spatially-Enhanced%2520Recurrent%2520Memory%250A%2520%2520via%2520End-to-End%2520Reinforcement%2520Learning%26entry.906535625%3DFan%2520Yang%2520and%2520Per%2520Frivik%2520and%2520David%2520Hoeller%2520and%2520Chen%2520Wang%2520and%2520Cesar%2520Cadena%2520and%2520Marco%2520Hutter%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520robot%2520navigation%252C%2520especially%2520with%2520end-to-end%2520learning%250Aapproaches%2520like%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520have%2520shown%2520remarkable%2520efficiency%250Aand%2520effectiveness.%2520Yet%252C%2520successful%2520navigation%2520still%2520relies%2520on%2520two%2520key%250Acapabilities%253A%2520mapping%2520and%2520planning%252C%2520whether%2520explicit%2520or%2520implicit.%2520Classical%250Aapproaches%2520use%2520explicit%2520mapping%2520pipelines%2520to%2520register%2520ego-centric%2520observations%250Ainto%2520a%2520coherent%2520map%2520frame%2520for%2520the%2520planner.%2520In%2520contrast%252C%2520end-to-end%2520learning%250Aachieves%2520this%2520implicitly%252C%2520often%2520through%2520recurrent%2520neural%2520networks%2520%2528RNNs%2529%2520that%250Afuse%2520current%2520and%2520past%2520observations%2520into%2520a%2520latent%2520space%2520for%2520planning.%2520While%250Aarchitectures%2520such%2520as%2520LSTM%2520and%2520GRU%2520capture%2520temporal%2520dependencies%252C%2520our%2520findings%250Areveal%2520a%2520key%2520limitation%253A%2520their%2520inability%2520to%2520perform%2520effective%2520spatial%250Amemorization.%2520This%2520skill%2520is%2520essential%2520for%2520transforming%2520and%2520integrating%250Asequential%2520observations%2520from%2520varying%2520perspectives%2520to%2520build%2520spatial%250Arepresentations%2520that%2520support%2520downstream%2520planning.%2520To%2520address%2520this%252C%2520we%2520propose%250ASpatially-Enhanced%2520Recurrent%2520Units%2520%2528SRUs%2529%252C%2520a%2520simple%2520yet%2520effective%2520modification%250Ato%2520existing%2520RNNs%252C%2520designed%2520to%2520enhance%2520spatial%2520memorization%2520capabilities.%2520We%250Aintroduce%2520an%2520attention-based%2520architecture%2520with%2520SRUs%252C%2520enabling%2520long-range%250Anavigation%2520using%2520a%2520single%2520forward-facing%2520stereo%2520camera.%2520Regularization%250Atechniques%2520are%2520employed%2520to%2520ensure%2520robust%2520end-to-end%2520recurrent%2520training%2520via%2520RL.%250AExperimental%2520results%2520show%2520our%2520approach%2520improves%2520long-range%2520navigation%2520by%252023.5%2525%250Acompared%2520to%2520existing%2520RNNs.%2520Furthermore%252C%2520with%2520SRU%2520memory%252C%2520our%2520method%2520outperforms%250Athe%2520RL%2520baseline%2520with%2520explicit%2520mapping%2520and%2520memory%2520modules%252C%2520achieving%2520a%252029.6%2525%250Aimprovement%2520in%2520diverse%2520environments%2520requiring%2520long-horizon%2520mapping%2520and%250Amemorization.%2520Finally%252C%2520we%2520address%2520the%2520sim-to-real%2520gap%2520by%2520leveraging%2520large-scale%250Apretraining%2520on%2520synthetic%2520depth%2520data%252C%2520enabling%2520zero-shot%2520transfer%2520to%2520diverse%2520and%250Acomplex%2520real-world%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05997v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Long-Range%20Navigation%20with%20Spatially-Enhanced%20Recurrent%20Memory%0A%20%20via%20End-to-End%20Reinforcement%20Learning&entry.906535625=Fan%20Yang%20and%20Per%20Frivik%20and%20David%20Hoeller%20and%20Chen%20Wang%20and%20Cesar%20Cadena%20and%20Marco%20Hutter&entry.1292438233=%20%20Recent%20advancements%20in%20robot%20navigation%2C%20especially%20with%20end-to-end%20learning%0Aapproaches%20like%20reinforcement%20learning%20%28RL%29%2C%20have%20shown%20remarkable%20efficiency%0Aand%20effectiveness.%20Yet%2C%20successful%20navigation%20still%20relies%20on%20two%20key%0Acapabilities%3A%20mapping%20and%20planning%2C%20whether%20explicit%20or%20implicit.%20Classical%0Aapproaches%20use%20explicit%20mapping%20pipelines%20to%20register%20ego-centric%20observations%0Ainto%20a%20coherent%20map%20frame%20for%20the%20planner.%20In%20contrast%2C%20end-to-end%20learning%0Aachieves%20this%20implicitly%2C%20often%20through%20recurrent%20neural%20networks%20%28RNNs%29%20that%0Afuse%20current%20and%20past%20observations%20into%20a%20latent%20space%20for%20planning.%20While%0Aarchitectures%20such%20as%20LSTM%20and%20GRU%20capture%20temporal%20dependencies%2C%20our%20findings%0Areveal%20a%20key%20limitation%3A%20their%20inability%20to%20perform%20effective%20spatial%0Amemorization.%20This%20skill%20is%20essential%20for%20transforming%20and%20integrating%0Asequential%20observations%20from%20varying%20perspectives%20to%20build%20spatial%0Arepresentations%20that%20support%20downstream%20planning.%20To%20address%20this%2C%20we%20propose%0ASpatially-Enhanced%20Recurrent%20Units%20%28SRUs%29%2C%20a%20simple%20yet%20effective%20modification%0Ato%20existing%20RNNs%2C%20designed%20to%20enhance%20spatial%20memorization%20capabilities.%20We%0Aintroduce%20an%20attention-based%20architecture%20with%20SRUs%2C%20enabling%20long-range%0Anavigation%20using%20a%20single%20forward-facing%20stereo%20camera.%20Regularization%0Atechniques%20are%20employed%20to%20ensure%20robust%20end-to-end%20recurrent%20training%20via%20RL.%0AExperimental%20results%20show%20our%20approach%20improves%20long-range%20navigation%20by%2023.5%25%0Acompared%20to%20existing%20RNNs.%20Furthermore%2C%20with%20SRU%20memory%2C%20our%20method%20outperforms%0Athe%20RL%20baseline%20with%20explicit%20mapping%20and%20memory%20modules%2C%20achieving%20a%2029.6%25%0Aimprovement%20in%20diverse%20environments%20requiring%20long-horizon%20mapping%20and%0Amemorization.%20Finally%2C%20we%20address%20the%20sim-to-real%20gap%20by%20leveraging%20large-scale%0Apretraining%20on%20synthetic%20depth%20data%2C%20enabling%20zero-shot%20transfer%20to%20diverse%20and%0Acomplex%20real-world%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05997v1&entry.124074799=Read"},
{"title": "A Physics-informed End-to-End Occupancy Framework for Motion Planning of\n  Autonomous Vehicles", "author": "Shuqi Shen and Junjie Yang and Hongliang Lu and Hui Zhong and Qiming Zhang and Xinhu Zheng", "abstract": "  Accurate and interpretable motion planning is essential for autonomous\nvehicles (AVs) navigating complex and uncertain environments. While recent\nend-to-end occupancy prediction methods have improved environmental\nunderstanding, they typically lack explicit physical constraints, limiting\nsafety and generalization. In this paper, we propose a unified end-to-end\nframework that integrates verifiable physical rules into the occupancy learning\nprocess. Specifically, we embed artificial potential fields (APF) as\nphysics-informed guidance during network training to ensure that predicted\noccupancy maps are both data-efficient and physically plausible. Our\narchitecture combines convolutional and recurrent neural networks to capture\nspatial and temporal dependencies while preserving model flexibility.\nExperimental results demonstrate that our method improves task completion rate,\nsafety margins, and planning efficiency across diverse driving scenarios,\nconfirming its potential for reliable deployment in real-world AV systems.\n", "link": "http://arxiv.org/abs/2505.07855v2", "date": "2025-06-06", "relevancy": 2.2799, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6042}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5723}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Physics-informed%20End-to-End%20Occupancy%20Framework%20for%20Motion%20Planning%20of%0A%20%20Autonomous%20Vehicles&body=Title%3A%20A%20Physics-informed%20End-to-End%20Occupancy%20Framework%20for%20Motion%20Planning%20of%0A%20%20Autonomous%20Vehicles%0AAuthor%3A%20Shuqi%20Shen%20and%20Junjie%20Yang%20and%20Hongliang%20Lu%20and%20Hui%20Zhong%20and%20Qiming%20Zhang%20and%20Xinhu%20Zheng%0AAbstract%3A%20%20%20Accurate%20and%20interpretable%20motion%20planning%20is%20essential%20for%20autonomous%0Avehicles%20%28AVs%29%20navigating%20complex%20and%20uncertain%20environments.%20While%20recent%0Aend-to-end%20occupancy%20prediction%20methods%20have%20improved%20environmental%0Aunderstanding%2C%20they%20typically%20lack%20explicit%20physical%20constraints%2C%20limiting%0Asafety%20and%20generalization.%20In%20this%20paper%2C%20we%20propose%20a%20unified%20end-to-end%0Aframework%20that%20integrates%20verifiable%20physical%20rules%20into%20the%20occupancy%20learning%0Aprocess.%20Specifically%2C%20we%20embed%20artificial%20potential%20fields%20%28APF%29%20as%0Aphysics-informed%20guidance%20during%20network%20training%20to%20ensure%20that%20predicted%0Aoccupancy%20maps%20are%20both%20data-efficient%20and%20physically%20plausible.%20Our%0Aarchitecture%20combines%20convolutional%20and%20recurrent%20neural%20networks%20to%20capture%0Aspatial%20and%20temporal%20dependencies%20while%20preserving%20model%20flexibility.%0AExperimental%20results%20demonstrate%20that%20our%20method%20improves%20task%20completion%20rate%2C%0Asafety%20margins%2C%20and%20planning%20efficiency%20across%20diverse%20driving%20scenarios%2C%0Aconfirming%20its%20potential%20for%20reliable%20deployment%20in%20real-world%20AV%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07855v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Physics-informed%2520End-to-End%2520Occupancy%2520Framework%2520for%2520Motion%2520Planning%2520of%250A%2520%2520Autonomous%2520Vehicles%26entry.906535625%3DShuqi%2520Shen%2520and%2520Junjie%2520Yang%2520and%2520Hongliang%2520Lu%2520and%2520Hui%2520Zhong%2520and%2520Qiming%2520Zhang%2520and%2520Xinhu%2520Zheng%26entry.1292438233%3D%2520%2520Accurate%2520and%2520interpretable%2520motion%2520planning%2520is%2520essential%2520for%2520autonomous%250Avehicles%2520%2528AVs%2529%2520navigating%2520complex%2520and%2520uncertain%2520environments.%2520While%2520recent%250Aend-to-end%2520occupancy%2520prediction%2520methods%2520have%2520improved%2520environmental%250Aunderstanding%252C%2520they%2520typically%2520lack%2520explicit%2520physical%2520constraints%252C%2520limiting%250Asafety%2520and%2520generalization.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520unified%2520end-to-end%250Aframework%2520that%2520integrates%2520verifiable%2520physical%2520rules%2520into%2520the%2520occupancy%2520learning%250Aprocess.%2520Specifically%252C%2520we%2520embed%2520artificial%2520potential%2520fields%2520%2528APF%2529%2520as%250Aphysics-informed%2520guidance%2520during%2520network%2520training%2520to%2520ensure%2520that%2520predicted%250Aoccupancy%2520maps%2520are%2520both%2520data-efficient%2520and%2520physically%2520plausible.%2520Our%250Aarchitecture%2520combines%2520convolutional%2520and%2520recurrent%2520neural%2520networks%2520to%2520capture%250Aspatial%2520and%2520temporal%2520dependencies%2520while%2520preserving%2520model%2520flexibility.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520method%2520improves%2520task%2520completion%2520rate%252C%250Asafety%2520margins%252C%2520and%2520planning%2520efficiency%2520across%2520diverse%2520driving%2520scenarios%252C%250Aconfirming%2520its%2520potential%2520for%2520reliable%2520deployment%2520in%2520real-world%2520AV%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07855v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Physics-informed%20End-to-End%20Occupancy%20Framework%20for%20Motion%20Planning%20of%0A%20%20Autonomous%20Vehicles&entry.906535625=Shuqi%20Shen%20and%20Junjie%20Yang%20and%20Hongliang%20Lu%20and%20Hui%20Zhong%20and%20Qiming%20Zhang%20and%20Xinhu%20Zheng&entry.1292438233=%20%20Accurate%20and%20interpretable%20motion%20planning%20is%20essential%20for%20autonomous%0Avehicles%20%28AVs%29%20navigating%20complex%20and%20uncertain%20environments.%20While%20recent%0Aend-to-end%20occupancy%20prediction%20methods%20have%20improved%20environmental%0Aunderstanding%2C%20they%20typically%20lack%20explicit%20physical%20constraints%2C%20limiting%0Asafety%20and%20generalization.%20In%20this%20paper%2C%20we%20propose%20a%20unified%20end-to-end%0Aframework%20that%20integrates%20verifiable%20physical%20rules%20into%20the%20occupancy%20learning%0Aprocess.%20Specifically%2C%20we%20embed%20artificial%20potential%20fields%20%28APF%29%20as%0Aphysics-informed%20guidance%20during%20network%20training%20to%20ensure%20that%20predicted%0Aoccupancy%20maps%20are%20both%20data-efficient%20and%20physically%20plausible.%20Our%0Aarchitecture%20combines%20convolutional%20and%20recurrent%20neural%20networks%20to%20capture%0Aspatial%20and%20temporal%20dependencies%20while%20preserving%20model%20flexibility.%0AExperimental%20results%20demonstrate%20that%20our%20method%20improves%20task%20completion%20rate%2C%0Asafety%20margins%2C%20and%20planning%20efficiency%20across%20diverse%20driving%20scenarios%2C%0Aconfirming%20its%20potential%20for%20reliable%20deployment%20in%20real-world%20AV%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07855v2&entry.124074799=Read"},
{"title": "Audio-Aware Large Language Models as Judges for Speaking Styles", "author": "Cheng-Han Chiang and Xiaofei Wang and Chung-Ching Lin and Kevin Lin and Linjie Li and Radu Kopetz and Yao Qian and Zhendong Wang and Zhengyuan Yang and Hung-yi Lee and Lijuan Wang", "abstract": "  Audio-aware large language models (ALLMs) can understand the textual and\nnon-textual information in the audio input. In this paper, we explore using\nALLMs as an automatic judge to assess the speaking styles of speeches. We use\nALLM judges to evaluate the speeches generated by SLMs on two tasks: voice\nstyle instruction following and role-playing. The speaking style we consider\nincludes emotion, volume, speaking pace, word emphasis, pitch control, and\nnon-verbal elements. We use four spoken language models (SLMs) to complete the\ntwo tasks and use humans and ALLMs to judge the SLMs' responses. We compare two\nALLM judges, GPT-4o-audio and Gemini-2.5-pro, with human evaluation results and\nshow that the agreement between Gemini and human judges is comparable to the\nagreement between human evaluators. These promising results show that ALLMs can\nbe used as a judge to evaluate SLMs. Our results also reveal that current SLMs,\neven GPT-4o-audio, still have room for improvement in controlling the speaking\nstyle and generating natural dialogues.\n", "link": "http://arxiv.org/abs/2506.05984v1", "date": "2025-06-06", "relevancy": 2.2546, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4546}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4491}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Audio-Aware%20Large%20Language%20Models%20as%20Judges%20for%20Speaking%20Styles&body=Title%3A%20Audio-Aware%20Large%20Language%20Models%20as%20Judges%20for%20Speaking%20Styles%0AAuthor%3A%20Cheng-Han%20Chiang%20and%20Xiaofei%20Wang%20and%20Chung-Ching%20Lin%20and%20Kevin%20Lin%20and%20Linjie%20Li%20and%20Radu%20Kopetz%20and%20Yao%20Qian%20and%20Zhendong%20Wang%20and%20Zhengyuan%20Yang%20and%20Hung-yi%20Lee%20and%20Lijuan%20Wang%0AAbstract%3A%20%20%20Audio-aware%20large%20language%20models%20%28ALLMs%29%20can%20understand%20the%20textual%20and%0Anon-textual%20information%20in%20the%20audio%20input.%20In%20this%20paper%2C%20we%20explore%20using%0AALLMs%20as%20an%20automatic%20judge%20to%20assess%20the%20speaking%20styles%20of%20speeches.%20We%20use%0AALLM%20judges%20to%20evaluate%20the%20speeches%20generated%20by%20SLMs%20on%20two%20tasks%3A%20voice%0Astyle%20instruction%20following%20and%20role-playing.%20The%20speaking%20style%20we%20consider%0Aincludes%20emotion%2C%20volume%2C%20speaking%20pace%2C%20word%20emphasis%2C%20pitch%20control%2C%20and%0Anon-verbal%20elements.%20We%20use%20four%20spoken%20language%20models%20%28SLMs%29%20to%20complete%20the%0Atwo%20tasks%20and%20use%20humans%20and%20ALLMs%20to%20judge%20the%20SLMs%27%20responses.%20We%20compare%20two%0AALLM%20judges%2C%20GPT-4o-audio%20and%20Gemini-2.5-pro%2C%20with%20human%20evaluation%20results%20and%0Ashow%20that%20the%20agreement%20between%20Gemini%20and%20human%20judges%20is%20comparable%20to%20the%0Aagreement%20between%20human%20evaluators.%20These%20promising%20results%20show%20that%20ALLMs%20can%0Abe%20used%20as%20a%20judge%20to%20evaluate%20SLMs.%20Our%20results%20also%20reveal%20that%20current%20SLMs%2C%0Aeven%20GPT-4o-audio%2C%20still%20have%20room%20for%20improvement%20in%20controlling%20the%20speaking%0Astyle%20and%20generating%20natural%20dialogues.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05984v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudio-Aware%2520Large%2520Language%2520Models%2520as%2520Judges%2520for%2520Speaking%2520Styles%26entry.906535625%3DCheng-Han%2520Chiang%2520and%2520Xiaofei%2520Wang%2520and%2520Chung-Ching%2520Lin%2520and%2520Kevin%2520Lin%2520and%2520Linjie%2520Li%2520and%2520Radu%2520Kopetz%2520and%2520Yao%2520Qian%2520and%2520Zhendong%2520Wang%2520and%2520Zhengyuan%2520Yang%2520and%2520Hung-yi%2520Lee%2520and%2520Lijuan%2520Wang%26entry.1292438233%3D%2520%2520Audio-aware%2520large%2520language%2520models%2520%2528ALLMs%2529%2520can%2520understand%2520the%2520textual%2520and%250Anon-textual%2520information%2520in%2520the%2520audio%2520input.%2520In%2520this%2520paper%252C%2520we%2520explore%2520using%250AALLMs%2520as%2520an%2520automatic%2520judge%2520to%2520assess%2520the%2520speaking%2520styles%2520of%2520speeches.%2520We%2520use%250AALLM%2520judges%2520to%2520evaluate%2520the%2520speeches%2520generated%2520by%2520SLMs%2520on%2520two%2520tasks%253A%2520voice%250Astyle%2520instruction%2520following%2520and%2520role-playing.%2520The%2520speaking%2520style%2520we%2520consider%250Aincludes%2520emotion%252C%2520volume%252C%2520speaking%2520pace%252C%2520word%2520emphasis%252C%2520pitch%2520control%252C%2520and%250Anon-verbal%2520elements.%2520We%2520use%2520four%2520spoken%2520language%2520models%2520%2528SLMs%2529%2520to%2520complete%2520the%250Atwo%2520tasks%2520and%2520use%2520humans%2520and%2520ALLMs%2520to%2520judge%2520the%2520SLMs%2527%2520responses.%2520We%2520compare%2520two%250AALLM%2520judges%252C%2520GPT-4o-audio%2520and%2520Gemini-2.5-pro%252C%2520with%2520human%2520evaluation%2520results%2520and%250Ashow%2520that%2520the%2520agreement%2520between%2520Gemini%2520and%2520human%2520judges%2520is%2520comparable%2520to%2520the%250Aagreement%2520between%2520human%2520evaluators.%2520These%2520promising%2520results%2520show%2520that%2520ALLMs%2520can%250Abe%2520used%2520as%2520a%2520judge%2520to%2520evaluate%2520SLMs.%2520Our%2520results%2520also%2520reveal%2520that%2520current%2520SLMs%252C%250Aeven%2520GPT-4o-audio%252C%2520still%2520have%2520room%2520for%2520improvement%2520in%2520controlling%2520the%2520speaking%250Astyle%2520and%2520generating%2520natural%2520dialogues.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05984v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Audio-Aware%20Large%20Language%20Models%20as%20Judges%20for%20Speaking%20Styles&entry.906535625=Cheng-Han%20Chiang%20and%20Xiaofei%20Wang%20and%20Chung-Ching%20Lin%20and%20Kevin%20Lin%20and%20Linjie%20Li%20and%20Radu%20Kopetz%20and%20Yao%20Qian%20and%20Zhendong%20Wang%20and%20Zhengyuan%20Yang%20and%20Hung-yi%20Lee%20and%20Lijuan%20Wang&entry.1292438233=%20%20Audio-aware%20large%20language%20models%20%28ALLMs%29%20can%20understand%20the%20textual%20and%0Anon-textual%20information%20in%20the%20audio%20input.%20In%20this%20paper%2C%20we%20explore%20using%0AALLMs%20as%20an%20automatic%20judge%20to%20assess%20the%20speaking%20styles%20of%20speeches.%20We%20use%0AALLM%20judges%20to%20evaluate%20the%20speeches%20generated%20by%20SLMs%20on%20two%20tasks%3A%20voice%0Astyle%20instruction%20following%20and%20role-playing.%20The%20speaking%20style%20we%20consider%0Aincludes%20emotion%2C%20volume%2C%20speaking%20pace%2C%20word%20emphasis%2C%20pitch%20control%2C%20and%0Anon-verbal%20elements.%20We%20use%20four%20spoken%20language%20models%20%28SLMs%29%20to%20complete%20the%0Atwo%20tasks%20and%20use%20humans%20and%20ALLMs%20to%20judge%20the%20SLMs%27%20responses.%20We%20compare%20two%0AALLM%20judges%2C%20GPT-4o-audio%20and%20Gemini-2.5-pro%2C%20with%20human%20evaluation%20results%20and%0Ashow%20that%20the%20agreement%20between%20Gemini%20and%20human%20judges%20is%20comparable%20to%20the%0Aagreement%20between%20human%20evaluators.%20These%20promising%20results%20show%20that%20ALLMs%20can%0Abe%20used%20as%20a%20judge%20to%20evaluate%20SLMs.%20Our%20results%20also%20reveal%20that%20current%20SLMs%2C%0Aeven%20GPT-4o-audio%2C%20still%20have%20room%20for%20improvement%20in%20controlling%20the%20speaking%0Astyle%20and%20generating%20natural%20dialogues.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05984v1&entry.124074799=Read"},
{"title": "Quantifying Adversarial Uncertainty in Evidential Deep Learning using\n  Conflict Resolution", "author": "Charmaine Barker and Daniel Bethell and Simos Gerasimou", "abstract": "  Reliability of deep learning models is critical for deployment in high-stakes\napplications, where out-of-distribution or adversarial inputs may lead to\ndetrimental outcomes. Evidential Deep Learning, an efficient paradigm for\nuncertainty quantification, models predictions as Dirichlet distributions of a\nsingle forward pass. However, EDL is particularly vulnerable to adversarially\nperturbed inputs, making overconfident errors. Conflict-aware Evidential Deep\nLearning (C-EDL) is a lightweight post-hoc uncertainty quantification approach\nthat mitigates these issues, enhancing adversarial and OOD robustness without\nretraining. C-EDL generates diverse, task-preserving transformations per input\nand quantifies representational disagreement to calibrate uncertainty estimates\nwhen needed. C-EDL's conflict-aware prediction adjustment improves detection of\nOOD and adversarial inputs, maintaining high in-distribution accuracy and low\ncomputational overhead. Our experimental evaluation shows that C-EDL\nsignificantly outperforms state-of-the-art EDL variants and competitive\nbaselines, achieving substantial reductions in coverage for OOD data (up to\n55%) and adversarial data (up to 90%), across a range of datasets, attack\ntypes, and uncertainty metrics.\n", "link": "http://arxiv.org/abs/2506.05937v1", "date": "2025-06-06", "relevancy": 2.2535, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.71}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5696}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantifying%20Adversarial%20Uncertainty%20in%20Evidential%20Deep%20Learning%20using%0A%20%20Conflict%20Resolution&body=Title%3A%20Quantifying%20Adversarial%20Uncertainty%20in%20Evidential%20Deep%20Learning%20using%0A%20%20Conflict%20Resolution%0AAuthor%3A%20Charmaine%20Barker%20and%20Daniel%20Bethell%20and%20Simos%20Gerasimou%0AAbstract%3A%20%20%20Reliability%20of%20deep%20learning%20models%20is%20critical%20for%20deployment%20in%20high-stakes%0Aapplications%2C%20where%20out-of-distribution%20or%20adversarial%20inputs%20may%20lead%20to%0Adetrimental%20outcomes.%20Evidential%20Deep%20Learning%2C%20an%20efficient%20paradigm%20for%0Auncertainty%20quantification%2C%20models%20predictions%20as%20Dirichlet%20distributions%20of%20a%0Asingle%20forward%20pass.%20However%2C%20EDL%20is%20particularly%20vulnerable%20to%20adversarially%0Aperturbed%20inputs%2C%20making%20overconfident%20errors.%20Conflict-aware%20Evidential%20Deep%0ALearning%20%28C-EDL%29%20is%20a%20lightweight%20post-hoc%20uncertainty%20quantification%20approach%0Athat%20mitigates%20these%20issues%2C%20enhancing%20adversarial%20and%20OOD%20robustness%20without%0Aretraining.%20C-EDL%20generates%20diverse%2C%20task-preserving%20transformations%20per%20input%0Aand%20quantifies%20representational%20disagreement%20to%20calibrate%20uncertainty%20estimates%0Awhen%20needed.%20C-EDL%27s%20conflict-aware%20prediction%20adjustment%20improves%20detection%20of%0AOOD%20and%20adversarial%20inputs%2C%20maintaining%20high%20in-distribution%20accuracy%20and%20low%0Acomputational%20overhead.%20Our%20experimental%20evaluation%20shows%20that%20C-EDL%0Asignificantly%20outperforms%20state-of-the-art%20EDL%20variants%20and%20competitive%0Abaselines%2C%20achieving%20substantial%20reductions%20in%20coverage%20for%20OOD%20data%20%28up%20to%0A55%25%29%20and%20adversarial%20data%20%28up%20to%2090%25%29%2C%20across%20a%20range%20of%20datasets%2C%20attack%0Atypes%2C%20and%20uncertainty%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05937v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantifying%2520Adversarial%2520Uncertainty%2520in%2520Evidential%2520Deep%2520Learning%2520using%250A%2520%2520Conflict%2520Resolution%26entry.906535625%3DCharmaine%2520Barker%2520and%2520Daniel%2520Bethell%2520and%2520Simos%2520Gerasimou%26entry.1292438233%3D%2520%2520Reliability%2520of%2520deep%2520learning%2520models%2520is%2520critical%2520for%2520deployment%2520in%2520high-stakes%250Aapplications%252C%2520where%2520out-of-distribution%2520or%2520adversarial%2520inputs%2520may%2520lead%2520to%250Adetrimental%2520outcomes.%2520Evidential%2520Deep%2520Learning%252C%2520an%2520efficient%2520paradigm%2520for%250Auncertainty%2520quantification%252C%2520models%2520predictions%2520as%2520Dirichlet%2520distributions%2520of%2520a%250Asingle%2520forward%2520pass.%2520However%252C%2520EDL%2520is%2520particularly%2520vulnerable%2520to%2520adversarially%250Aperturbed%2520inputs%252C%2520making%2520overconfident%2520errors.%2520Conflict-aware%2520Evidential%2520Deep%250ALearning%2520%2528C-EDL%2529%2520is%2520a%2520lightweight%2520post-hoc%2520uncertainty%2520quantification%2520approach%250Athat%2520mitigates%2520these%2520issues%252C%2520enhancing%2520adversarial%2520and%2520OOD%2520robustness%2520without%250Aretraining.%2520C-EDL%2520generates%2520diverse%252C%2520task-preserving%2520transformations%2520per%2520input%250Aand%2520quantifies%2520representational%2520disagreement%2520to%2520calibrate%2520uncertainty%2520estimates%250Awhen%2520needed.%2520C-EDL%2527s%2520conflict-aware%2520prediction%2520adjustment%2520improves%2520detection%2520of%250AOOD%2520and%2520adversarial%2520inputs%252C%2520maintaining%2520high%2520in-distribution%2520accuracy%2520and%2520low%250Acomputational%2520overhead.%2520Our%2520experimental%2520evaluation%2520shows%2520that%2520C-EDL%250Asignificantly%2520outperforms%2520state-of-the-art%2520EDL%2520variants%2520and%2520competitive%250Abaselines%252C%2520achieving%2520substantial%2520reductions%2520in%2520coverage%2520for%2520OOD%2520data%2520%2528up%2520to%250A55%2525%2529%2520and%2520adversarial%2520data%2520%2528up%2520to%252090%2525%2529%252C%2520across%2520a%2520range%2520of%2520datasets%252C%2520attack%250Atypes%252C%2520and%2520uncertainty%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05937v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantifying%20Adversarial%20Uncertainty%20in%20Evidential%20Deep%20Learning%20using%0A%20%20Conflict%20Resolution&entry.906535625=Charmaine%20Barker%20and%20Daniel%20Bethell%20and%20Simos%20Gerasimou&entry.1292438233=%20%20Reliability%20of%20deep%20learning%20models%20is%20critical%20for%20deployment%20in%20high-stakes%0Aapplications%2C%20where%20out-of-distribution%20or%20adversarial%20inputs%20may%20lead%20to%0Adetrimental%20outcomes.%20Evidential%20Deep%20Learning%2C%20an%20efficient%20paradigm%20for%0Auncertainty%20quantification%2C%20models%20predictions%20as%20Dirichlet%20distributions%20of%20a%0Asingle%20forward%20pass.%20However%2C%20EDL%20is%20particularly%20vulnerable%20to%20adversarially%0Aperturbed%20inputs%2C%20making%20overconfident%20errors.%20Conflict-aware%20Evidential%20Deep%0ALearning%20%28C-EDL%29%20is%20a%20lightweight%20post-hoc%20uncertainty%20quantification%20approach%0Athat%20mitigates%20these%20issues%2C%20enhancing%20adversarial%20and%20OOD%20robustness%20without%0Aretraining.%20C-EDL%20generates%20diverse%2C%20task-preserving%20transformations%20per%20input%0Aand%20quantifies%20representational%20disagreement%20to%20calibrate%20uncertainty%20estimates%0Awhen%20needed.%20C-EDL%27s%20conflict-aware%20prediction%20adjustment%20improves%20detection%20of%0AOOD%20and%20adversarial%20inputs%2C%20maintaining%20high%20in-distribution%20accuracy%20and%20low%0Acomputational%20overhead.%20Our%20experimental%20evaluation%20shows%20that%20C-EDL%0Asignificantly%20outperforms%20state-of-the-art%20EDL%20variants%20and%20competitive%0Abaselines%2C%20achieving%20substantial%20reductions%20in%20coverage%20for%20OOD%20data%20%28up%20to%0A55%25%29%20and%20adversarial%20data%20%28up%20to%2090%25%29%2C%20across%20a%20range%20of%20datasets%2C%20attack%0Atypes%2C%20and%20uncertainty%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05937v1&entry.124074799=Read"},
{"title": "Defurnishing with X-Ray Vision: Joint Removal of Furniture from\n  Panoramas and Mesh", "author": "Alan Dolhasz and Chen Ma and Dave Gausebeck and Kevin Chen and Gregor Miller and Lucas Hayne and Gunnar Hovden and Azwad Sabik and Olaf Brandt and Mira Slavcheva", "abstract": "  We present a pipeline for generating defurnished replicas of indoor spaces\nrepresented as textured meshes and corresponding multi-view panoramic images.\nTo achieve this, we first segment and remove furniture from the mesh\nrepresentation, extend planes, and fill holes, obtaining a simplified\ndefurnished mesh (SDM). This SDM acts as an ``X-ray'' of the scene's underlying\nstructure, guiding the defurnishing process. We extract Canny edges from depth\nand normal images rendered from the SDM. We then use these as a guide to remove\nthe furniture from panorama images via ControlNet inpainting. This control\nsignal ensures the availability of global geometric information that may be\nhidden from a particular panoramic view by the furniture being removed. The\ninpainted panoramas are used to texture the mesh. We show that our approach\nproduces higher quality assets than methods that rely on neural radiance\nfields, which tend to produce blurry low-resolution images, or RGB-D\ninpainting, which is highly susceptible to hallucinations.\n", "link": "http://arxiv.org/abs/2506.05338v2", "date": "2025-06-06", "relevancy": 2.2529, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5658}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5658}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Defurnishing%20with%20X-Ray%20Vision%3A%20Joint%20Removal%20of%20Furniture%20from%0A%20%20Panoramas%20and%20Mesh&body=Title%3A%20Defurnishing%20with%20X-Ray%20Vision%3A%20Joint%20Removal%20of%20Furniture%20from%0A%20%20Panoramas%20and%20Mesh%0AAuthor%3A%20Alan%20Dolhasz%20and%20Chen%20Ma%20and%20Dave%20Gausebeck%20and%20Kevin%20Chen%20and%20Gregor%20Miller%20and%20Lucas%20Hayne%20and%20Gunnar%20Hovden%20and%20Azwad%20Sabik%20and%20Olaf%20Brandt%20and%20Mira%20Slavcheva%0AAbstract%3A%20%20%20We%20present%20a%20pipeline%20for%20generating%20defurnished%20replicas%20of%20indoor%20spaces%0Arepresented%20as%20textured%20meshes%20and%20corresponding%20multi-view%20panoramic%20images.%0ATo%20achieve%20this%2C%20we%20first%20segment%20and%20remove%20furniture%20from%20the%20mesh%0Arepresentation%2C%20extend%20planes%2C%20and%20fill%20holes%2C%20obtaining%20a%20simplified%0Adefurnished%20mesh%20%28SDM%29.%20This%20SDM%20acts%20as%20an%20%60%60X-ray%27%27%20of%20the%20scene%27s%20underlying%0Astructure%2C%20guiding%20the%20defurnishing%20process.%20We%20extract%20Canny%20edges%20from%20depth%0Aand%20normal%20images%20rendered%20from%20the%20SDM.%20We%20then%20use%20these%20as%20a%20guide%20to%20remove%0Athe%20furniture%20from%20panorama%20images%20via%20ControlNet%20inpainting.%20This%20control%0Asignal%20ensures%20the%20availability%20of%20global%20geometric%20information%20that%20may%20be%0Ahidden%20from%20a%20particular%20panoramic%20view%20by%20the%20furniture%20being%20removed.%20The%0Ainpainted%20panoramas%20are%20used%20to%20texture%20the%20mesh.%20We%20show%20that%20our%20approach%0Aproduces%20higher%20quality%20assets%20than%20methods%20that%20rely%20on%20neural%20radiance%0Afields%2C%20which%20tend%20to%20produce%20blurry%20low-resolution%20images%2C%20or%20RGB-D%0Ainpainting%2C%20which%20is%20highly%20susceptible%20to%20hallucinations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05338v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDefurnishing%2520with%2520X-Ray%2520Vision%253A%2520Joint%2520Removal%2520of%2520Furniture%2520from%250A%2520%2520Panoramas%2520and%2520Mesh%26entry.906535625%3DAlan%2520Dolhasz%2520and%2520Chen%2520Ma%2520and%2520Dave%2520Gausebeck%2520and%2520Kevin%2520Chen%2520and%2520Gregor%2520Miller%2520and%2520Lucas%2520Hayne%2520and%2520Gunnar%2520Hovden%2520and%2520Azwad%2520Sabik%2520and%2520Olaf%2520Brandt%2520and%2520Mira%2520Slavcheva%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520pipeline%2520for%2520generating%2520defurnished%2520replicas%2520of%2520indoor%2520spaces%250Arepresented%2520as%2520textured%2520meshes%2520and%2520corresponding%2520multi-view%2520panoramic%2520images.%250ATo%2520achieve%2520this%252C%2520we%2520first%2520segment%2520and%2520remove%2520furniture%2520from%2520the%2520mesh%250Arepresentation%252C%2520extend%2520planes%252C%2520and%2520fill%2520holes%252C%2520obtaining%2520a%2520simplified%250Adefurnished%2520mesh%2520%2528SDM%2529.%2520This%2520SDM%2520acts%2520as%2520an%2520%2560%2560X-ray%2527%2527%2520of%2520the%2520scene%2527s%2520underlying%250Astructure%252C%2520guiding%2520the%2520defurnishing%2520process.%2520We%2520extract%2520Canny%2520edges%2520from%2520depth%250Aand%2520normal%2520images%2520rendered%2520from%2520the%2520SDM.%2520We%2520then%2520use%2520these%2520as%2520a%2520guide%2520to%2520remove%250Athe%2520furniture%2520from%2520panorama%2520images%2520via%2520ControlNet%2520inpainting.%2520This%2520control%250Asignal%2520ensures%2520the%2520availability%2520of%2520global%2520geometric%2520information%2520that%2520may%2520be%250Ahidden%2520from%2520a%2520particular%2520panoramic%2520view%2520by%2520the%2520furniture%2520being%2520removed.%2520The%250Ainpainted%2520panoramas%2520are%2520used%2520to%2520texture%2520the%2520mesh.%2520We%2520show%2520that%2520our%2520approach%250Aproduces%2520higher%2520quality%2520assets%2520than%2520methods%2520that%2520rely%2520on%2520neural%2520radiance%250Afields%252C%2520which%2520tend%2520to%2520produce%2520blurry%2520low-resolution%2520images%252C%2520or%2520RGB-D%250Ainpainting%252C%2520which%2520is%2520highly%2520susceptible%2520to%2520hallucinations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05338v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Defurnishing%20with%20X-Ray%20Vision%3A%20Joint%20Removal%20of%20Furniture%20from%0A%20%20Panoramas%20and%20Mesh&entry.906535625=Alan%20Dolhasz%20and%20Chen%20Ma%20and%20Dave%20Gausebeck%20and%20Kevin%20Chen%20and%20Gregor%20Miller%20and%20Lucas%20Hayne%20and%20Gunnar%20Hovden%20and%20Azwad%20Sabik%20and%20Olaf%20Brandt%20and%20Mira%20Slavcheva&entry.1292438233=%20%20We%20present%20a%20pipeline%20for%20generating%20defurnished%20replicas%20of%20indoor%20spaces%0Arepresented%20as%20textured%20meshes%20and%20corresponding%20multi-view%20panoramic%20images.%0ATo%20achieve%20this%2C%20we%20first%20segment%20and%20remove%20furniture%20from%20the%20mesh%0Arepresentation%2C%20extend%20planes%2C%20and%20fill%20holes%2C%20obtaining%20a%20simplified%0Adefurnished%20mesh%20%28SDM%29.%20This%20SDM%20acts%20as%20an%20%60%60X-ray%27%27%20of%20the%20scene%27s%20underlying%0Astructure%2C%20guiding%20the%20defurnishing%20process.%20We%20extract%20Canny%20edges%20from%20depth%0Aand%20normal%20images%20rendered%20from%20the%20SDM.%20We%20then%20use%20these%20as%20a%20guide%20to%20remove%0Athe%20furniture%20from%20panorama%20images%20via%20ControlNet%20inpainting.%20This%20control%0Asignal%20ensures%20the%20availability%20of%20global%20geometric%20information%20that%20may%20be%0Ahidden%20from%20a%20particular%20panoramic%20view%20by%20the%20furniture%20being%20removed.%20The%0Ainpainted%20panoramas%20are%20used%20to%20texture%20the%20mesh.%20We%20show%20that%20our%20approach%0Aproduces%20higher%20quality%20assets%20than%20methods%20that%20rely%20on%20neural%20radiance%0Afields%2C%20which%20tend%20to%20produce%20blurry%20low-resolution%20images%2C%20or%20RGB-D%0Ainpainting%2C%20which%20is%20highly%20susceptible%20to%20hallucinations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05338v2&entry.124074799=Read"},
{"title": "O-MaMa @ EgoExo4D Correspondence Challenge: Learning Object Mask\n  Matching between Egocentric and Exocentric Views", "author": "Lorenzo Mur-Labadia and Maria Santos-Villafranca and Alejandro Perez-Yus and Jesus Bermudez-Cameo and Ruben Martinez-Cantin and Jose J. Guerrero", "abstract": "  The goal of the correspondence task is to segment specific objects across\ndifferent views. This technical report re-defines cross-image segmentation by\ntreating it as a mask matching task. Our method consists of: (1) A Mask-Context\nEncoder that pools dense DINOv2 semantic features to obtain discriminative\nobject-level representations from FastSAM mask candidates, (2) an\nEgo$\\leftrightarrow$Exo Cross-Attention that fuses multi-perspective\nobservations, (3) a Mask Matching contrastive loss that aligns cross-view\nfeatures in a shared latent space, and (4) a Hard Negative Adjacent Mining\nstrategy to encourage the model to better differentiate between nearby objects.\n", "link": "http://arxiv.org/abs/2506.06026v1", "date": "2025-06-06", "relevancy": 2.2409, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5607}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5607}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20O-MaMa%20%40%20EgoExo4D%20Correspondence%20Challenge%3A%20Learning%20Object%20Mask%0A%20%20Matching%20between%20Egocentric%20and%20Exocentric%20Views&body=Title%3A%20O-MaMa%20%40%20EgoExo4D%20Correspondence%20Challenge%3A%20Learning%20Object%20Mask%0A%20%20Matching%20between%20Egocentric%20and%20Exocentric%20Views%0AAuthor%3A%20Lorenzo%20Mur-Labadia%20and%20Maria%20Santos-Villafranca%20and%20Alejandro%20Perez-Yus%20and%20Jesus%20Bermudez-Cameo%20and%20Ruben%20Martinez-Cantin%20and%20Jose%20J.%20Guerrero%0AAbstract%3A%20%20%20The%20goal%20of%20the%20correspondence%20task%20is%20to%20segment%20specific%20objects%20across%0Adifferent%20views.%20This%20technical%20report%20re-defines%20cross-image%20segmentation%20by%0Atreating%20it%20as%20a%20mask%20matching%20task.%20Our%20method%20consists%20of%3A%20%281%29%20A%20Mask-Context%0AEncoder%20that%20pools%20dense%20DINOv2%20semantic%20features%20to%20obtain%20discriminative%0Aobject-level%20representations%20from%20FastSAM%20mask%20candidates%2C%20%282%29%20an%0AEgo%24%5Cleftrightarrow%24Exo%20Cross-Attention%20that%20fuses%20multi-perspective%0Aobservations%2C%20%283%29%20a%20Mask%20Matching%20contrastive%20loss%20that%20aligns%20cross-view%0Afeatures%20in%20a%20shared%20latent%20space%2C%20and%20%284%29%20a%20Hard%20Negative%20Adjacent%20Mining%0Astrategy%20to%20encourage%20the%20model%20to%20better%20differentiate%20between%20nearby%20objects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06026v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DO-MaMa%2520%2540%2520EgoExo4D%2520Correspondence%2520Challenge%253A%2520Learning%2520Object%2520Mask%250A%2520%2520Matching%2520between%2520Egocentric%2520and%2520Exocentric%2520Views%26entry.906535625%3DLorenzo%2520Mur-Labadia%2520and%2520Maria%2520Santos-Villafranca%2520and%2520Alejandro%2520Perez-Yus%2520and%2520Jesus%2520Bermudez-Cameo%2520and%2520Ruben%2520Martinez-Cantin%2520and%2520Jose%2520J.%2520Guerrero%26entry.1292438233%3D%2520%2520The%2520goal%2520of%2520the%2520correspondence%2520task%2520is%2520to%2520segment%2520specific%2520objects%2520across%250Adifferent%2520views.%2520This%2520technical%2520report%2520re-defines%2520cross-image%2520segmentation%2520by%250Atreating%2520it%2520as%2520a%2520mask%2520matching%2520task.%2520Our%2520method%2520consists%2520of%253A%2520%25281%2529%2520A%2520Mask-Context%250AEncoder%2520that%2520pools%2520dense%2520DINOv2%2520semantic%2520features%2520to%2520obtain%2520discriminative%250Aobject-level%2520representations%2520from%2520FastSAM%2520mask%2520candidates%252C%2520%25282%2529%2520an%250AEgo%2524%255Cleftrightarrow%2524Exo%2520Cross-Attention%2520that%2520fuses%2520multi-perspective%250Aobservations%252C%2520%25283%2529%2520a%2520Mask%2520Matching%2520contrastive%2520loss%2520that%2520aligns%2520cross-view%250Afeatures%2520in%2520a%2520shared%2520latent%2520space%252C%2520and%2520%25284%2529%2520a%2520Hard%2520Negative%2520Adjacent%2520Mining%250Astrategy%2520to%2520encourage%2520the%2520model%2520to%2520better%2520differentiate%2520between%2520nearby%2520objects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06026v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=O-MaMa%20%40%20EgoExo4D%20Correspondence%20Challenge%3A%20Learning%20Object%20Mask%0A%20%20Matching%20between%20Egocentric%20and%20Exocentric%20Views&entry.906535625=Lorenzo%20Mur-Labadia%20and%20Maria%20Santos-Villafranca%20and%20Alejandro%20Perez-Yus%20and%20Jesus%20Bermudez-Cameo%20and%20Ruben%20Martinez-Cantin%20and%20Jose%20J.%20Guerrero&entry.1292438233=%20%20The%20goal%20of%20the%20correspondence%20task%20is%20to%20segment%20specific%20objects%20across%0Adifferent%20views.%20This%20technical%20report%20re-defines%20cross-image%20segmentation%20by%0Atreating%20it%20as%20a%20mask%20matching%20task.%20Our%20method%20consists%20of%3A%20%281%29%20A%20Mask-Context%0AEncoder%20that%20pools%20dense%20DINOv2%20semantic%20features%20to%20obtain%20discriminative%0Aobject-level%20representations%20from%20FastSAM%20mask%20candidates%2C%20%282%29%20an%0AEgo%24%5Cleftrightarrow%24Exo%20Cross-Attention%20that%20fuses%20multi-perspective%0Aobservations%2C%20%283%29%20a%20Mask%20Matching%20contrastive%20loss%20that%20aligns%20cross-view%0Afeatures%20in%20a%20shared%20latent%20space%2C%20and%20%284%29%20a%20Hard%20Negative%20Adjacent%20Mining%0Astrategy%20to%20encourage%20the%20model%20to%20better%20differentiate%20between%20nearby%20objects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06026v1&entry.124074799=Read"},
{"title": "VideoChat-A1: Thinking with Long Videos by Chain-of-Shot Reasoning", "author": "Zikang Wang and Boyu Chen and Zhengrong Yue and Yi Wang and Yu Qiao and Limin Wang and Yali Wang", "abstract": "  The recent advance in video understanding has been driven by multimodal large\nlanguage models (MLLMs). But these MLLMs are good at analyzing short videos,\nwhile suffering from difficulties in understanding videos with a longer\ncontext. To address this difficulty, several agent paradigms have recently been\nproposed, using MLLMs as agents for retrieving extra contextual knowledge in a\nlong video. However, most existing agents ignore the key fact that a long video\nis composed with multiple shots, i.e., to answer the user question from a long\nvideo, it is critical to deeply understand its relevant shots like human.\nWithout such insight, these agents often mistakenly find redundant even noisy\ntemporal context, restricting their capacity for long video understanding. To\nfill this gap, we propose VideoChat-A1, a novel long video agent paradigm.\nDifferent from the previous works, our VideoChat-A1 can deeply think with long\nvideos, via a distinct chain-of-shot reasoning paradigm. More specifically, it\ncan progressively select the relevant shots of user question, and look into\nthese shots in a coarse-to-fine partition. By multi-modal reasoning along the\nshot chain, VideoChat-A1 can effectively mimic step-by-step human thinking\nprocess, allowing to interactively discover preferable temporal context for\nthoughtful understanding in long videos. Extensive experiments show that, our\nVideoChat-A1 achieves the state-of-the-art performance on the mainstream long\nvideo QA benchmarks, e.g., it achieves 77.0 on VideoMME and 70.1 on EgoSchema,\noutperforming its strong baselines (e.g., Intern2.5VL-8B and\nInternVideo2.5-8B), by up to 10.8\\% and 6.2\\%. Compared to leading close-source\nGPT-4o and Gemini 1.5 Pro, VideoChat-A1 offers competitive accuracy, but with\n7\\% input frames and 12\\% inference time on average.\n", "link": "http://arxiv.org/abs/2506.06097v1", "date": "2025-06-06", "relevancy": 2.2404, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.564}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.559}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoChat-A1%3A%20Thinking%20with%20Long%20Videos%20by%20Chain-of-Shot%20Reasoning&body=Title%3A%20VideoChat-A1%3A%20Thinking%20with%20Long%20Videos%20by%20Chain-of-Shot%20Reasoning%0AAuthor%3A%20Zikang%20Wang%20and%20Boyu%20Chen%20and%20Zhengrong%20Yue%20and%20Yi%20Wang%20and%20Yu%20Qiao%20and%20Limin%20Wang%20and%20Yali%20Wang%0AAbstract%3A%20%20%20The%20recent%20advance%20in%20video%20understanding%20has%20been%20driven%20by%20multimodal%20large%0Alanguage%20models%20%28MLLMs%29.%20But%20these%20MLLMs%20are%20good%20at%20analyzing%20short%20videos%2C%0Awhile%20suffering%20from%20difficulties%20in%20understanding%20videos%20with%20a%20longer%0Acontext.%20To%20address%20this%20difficulty%2C%20several%20agent%20paradigms%20have%20recently%20been%0Aproposed%2C%20using%20MLLMs%20as%20agents%20for%20retrieving%20extra%20contextual%20knowledge%20in%20a%0Along%20video.%20However%2C%20most%20existing%20agents%20ignore%20the%20key%20fact%20that%20a%20long%20video%0Ais%20composed%20with%20multiple%20shots%2C%20i.e.%2C%20to%20answer%20the%20user%20question%20from%20a%20long%0Avideo%2C%20it%20is%20critical%20to%20deeply%20understand%20its%20relevant%20shots%20like%20human.%0AWithout%20such%20insight%2C%20these%20agents%20often%20mistakenly%20find%20redundant%20even%20noisy%0Atemporal%20context%2C%20restricting%20their%20capacity%20for%20long%20video%20understanding.%20To%0Afill%20this%20gap%2C%20we%20propose%20VideoChat-A1%2C%20a%20novel%20long%20video%20agent%20paradigm.%0ADifferent%20from%20the%20previous%20works%2C%20our%20VideoChat-A1%20can%20deeply%20think%20with%20long%0Avideos%2C%20via%20a%20distinct%20chain-of-shot%20reasoning%20paradigm.%20More%20specifically%2C%20it%0Acan%20progressively%20select%20the%20relevant%20shots%20of%20user%20question%2C%20and%20look%20into%0Athese%20shots%20in%20a%20coarse-to-fine%20partition.%20By%20multi-modal%20reasoning%20along%20the%0Ashot%20chain%2C%20VideoChat-A1%20can%20effectively%20mimic%20step-by-step%20human%20thinking%0Aprocess%2C%20allowing%20to%20interactively%20discover%20preferable%20temporal%20context%20for%0Athoughtful%20understanding%20in%20long%20videos.%20Extensive%20experiments%20show%20that%2C%20our%0AVideoChat-A1%20achieves%20the%20state-of-the-art%20performance%20on%20the%20mainstream%20long%0Avideo%20QA%20benchmarks%2C%20e.g.%2C%20it%20achieves%2077.0%20on%20VideoMME%20and%2070.1%20on%20EgoSchema%2C%0Aoutperforming%20its%20strong%20baselines%20%28e.g.%2C%20Intern2.5VL-8B%20and%0AInternVideo2.5-8B%29%2C%20by%20up%20to%2010.8%5C%25%20and%206.2%5C%25.%20Compared%20to%20leading%20close-source%0AGPT-4o%20and%20Gemini%201.5%20Pro%2C%20VideoChat-A1%20offers%20competitive%20accuracy%2C%20but%20with%0A7%5C%25%20input%20frames%20and%2012%5C%25%20inference%20time%20on%20average.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06097v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoChat-A1%253A%2520Thinking%2520with%2520Long%2520Videos%2520by%2520Chain-of-Shot%2520Reasoning%26entry.906535625%3DZikang%2520Wang%2520and%2520Boyu%2520Chen%2520and%2520Zhengrong%2520Yue%2520and%2520Yi%2520Wang%2520and%2520Yu%2520Qiao%2520and%2520Limin%2520Wang%2520and%2520Yali%2520Wang%26entry.1292438233%3D%2520%2520The%2520recent%2520advance%2520in%2520video%2520understanding%2520has%2520been%2520driven%2520by%2520multimodal%2520large%250Alanguage%2520models%2520%2528MLLMs%2529.%2520But%2520these%2520MLLMs%2520are%2520good%2520at%2520analyzing%2520short%2520videos%252C%250Awhile%2520suffering%2520from%2520difficulties%2520in%2520understanding%2520videos%2520with%2520a%2520longer%250Acontext.%2520To%2520address%2520this%2520difficulty%252C%2520several%2520agent%2520paradigms%2520have%2520recently%2520been%250Aproposed%252C%2520using%2520MLLMs%2520as%2520agents%2520for%2520retrieving%2520extra%2520contextual%2520knowledge%2520in%2520a%250Along%2520video.%2520However%252C%2520most%2520existing%2520agents%2520ignore%2520the%2520key%2520fact%2520that%2520a%2520long%2520video%250Ais%2520composed%2520with%2520multiple%2520shots%252C%2520i.e.%252C%2520to%2520answer%2520the%2520user%2520question%2520from%2520a%2520long%250Avideo%252C%2520it%2520is%2520critical%2520to%2520deeply%2520understand%2520its%2520relevant%2520shots%2520like%2520human.%250AWithout%2520such%2520insight%252C%2520these%2520agents%2520often%2520mistakenly%2520find%2520redundant%2520even%2520noisy%250Atemporal%2520context%252C%2520restricting%2520their%2520capacity%2520for%2520long%2520video%2520understanding.%2520To%250Afill%2520this%2520gap%252C%2520we%2520propose%2520VideoChat-A1%252C%2520a%2520novel%2520long%2520video%2520agent%2520paradigm.%250ADifferent%2520from%2520the%2520previous%2520works%252C%2520our%2520VideoChat-A1%2520can%2520deeply%2520think%2520with%2520long%250Avideos%252C%2520via%2520a%2520distinct%2520chain-of-shot%2520reasoning%2520paradigm.%2520More%2520specifically%252C%2520it%250Acan%2520progressively%2520select%2520the%2520relevant%2520shots%2520of%2520user%2520question%252C%2520and%2520look%2520into%250Athese%2520shots%2520in%2520a%2520coarse-to-fine%2520partition.%2520By%2520multi-modal%2520reasoning%2520along%2520the%250Ashot%2520chain%252C%2520VideoChat-A1%2520can%2520effectively%2520mimic%2520step-by-step%2520human%2520thinking%250Aprocess%252C%2520allowing%2520to%2520interactively%2520discover%2520preferable%2520temporal%2520context%2520for%250Athoughtful%2520understanding%2520in%2520long%2520videos.%2520Extensive%2520experiments%2520show%2520that%252C%2520our%250AVideoChat-A1%2520achieves%2520the%2520state-of-the-art%2520performance%2520on%2520the%2520mainstream%2520long%250Avideo%2520QA%2520benchmarks%252C%2520e.g.%252C%2520it%2520achieves%252077.0%2520on%2520VideoMME%2520and%252070.1%2520on%2520EgoSchema%252C%250Aoutperforming%2520its%2520strong%2520baselines%2520%2528e.g.%252C%2520Intern2.5VL-8B%2520and%250AInternVideo2.5-8B%2529%252C%2520by%2520up%2520to%252010.8%255C%2525%2520and%25206.2%255C%2525.%2520Compared%2520to%2520leading%2520close-source%250AGPT-4o%2520and%2520Gemini%25201.5%2520Pro%252C%2520VideoChat-A1%2520offers%2520competitive%2520accuracy%252C%2520but%2520with%250A7%255C%2525%2520input%2520frames%2520and%252012%255C%2525%2520inference%2520time%2520on%2520average.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06097v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoChat-A1%3A%20Thinking%20with%20Long%20Videos%20by%20Chain-of-Shot%20Reasoning&entry.906535625=Zikang%20Wang%20and%20Boyu%20Chen%20and%20Zhengrong%20Yue%20and%20Yi%20Wang%20and%20Yu%20Qiao%20and%20Limin%20Wang%20and%20Yali%20Wang&entry.1292438233=%20%20The%20recent%20advance%20in%20video%20understanding%20has%20been%20driven%20by%20multimodal%20large%0Alanguage%20models%20%28MLLMs%29.%20But%20these%20MLLMs%20are%20good%20at%20analyzing%20short%20videos%2C%0Awhile%20suffering%20from%20difficulties%20in%20understanding%20videos%20with%20a%20longer%0Acontext.%20To%20address%20this%20difficulty%2C%20several%20agent%20paradigms%20have%20recently%20been%0Aproposed%2C%20using%20MLLMs%20as%20agents%20for%20retrieving%20extra%20contextual%20knowledge%20in%20a%0Along%20video.%20However%2C%20most%20existing%20agents%20ignore%20the%20key%20fact%20that%20a%20long%20video%0Ais%20composed%20with%20multiple%20shots%2C%20i.e.%2C%20to%20answer%20the%20user%20question%20from%20a%20long%0Avideo%2C%20it%20is%20critical%20to%20deeply%20understand%20its%20relevant%20shots%20like%20human.%0AWithout%20such%20insight%2C%20these%20agents%20often%20mistakenly%20find%20redundant%20even%20noisy%0Atemporal%20context%2C%20restricting%20their%20capacity%20for%20long%20video%20understanding.%20To%0Afill%20this%20gap%2C%20we%20propose%20VideoChat-A1%2C%20a%20novel%20long%20video%20agent%20paradigm.%0ADifferent%20from%20the%20previous%20works%2C%20our%20VideoChat-A1%20can%20deeply%20think%20with%20long%0Avideos%2C%20via%20a%20distinct%20chain-of-shot%20reasoning%20paradigm.%20More%20specifically%2C%20it%0Acan%20progressively%20select%20the%20relevant%20shots%20of%20user%20question%2C%20and%20look%20into%0Athese%20shots%20in%20a%20coarse-to-fine%20partition.%20By%20multi-modal%20reasoning%20along%20the%0Ashot%20chain%2C%20VideoChat-A1%20can%20effectively%20mimic%20step-by-step%20human%20thinking%0Aprocess%2C%20allowing%20to%20interactively%20discover%20preferable%20temporal%20context%20for%0Athoughtful%20understanding%20in%20long%20videos.%20Extensive%20experiments%20show%20that%2C%20our%0AVideoChat-A1%20achieves%20the%20state-of-the-art%20performance%20on%20the%20mainstream%20long%0Avideo%20QA%20benchmarks%2C%20e.g.%2C%20it%20achieves%2077.0%20on%20VideoMME%20and%2070.1%20on%20EgoSchema%2C%0Aoutperforming%20its%20strong%20baselines%20%28e.g.%2C%20Intern2.5VL-8B%20and%0AInternVideo2.5-8B%29%2C%20by%20up%20to%2010.8%5C%25%20and%206.2%5C%25.%20Compared%20to%20leading%20close-source%0AGPT-4o%20and%20Gemini%201.5%20Pro%2C%20VideoChat-A1%20offers%20competitive%20accuracy%2C%20but%20with%0A7%5C%25%20input%20frames%20and%2012%5C%25%20inference%20time%20on%20average.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06097v1&entry.124074799=Read"},
{"title": "Does It Make Sense to Speak of Introspection in Large Language Models?", "author": "Iulia M. Comsa and Murray Shanahan", "abstract": "  Large language models (LLMs) exhibit compelling linguistic behaviour, and\nsometimes offer self-reports, that is to say statements about their own nature,\ninner workings, or behaviour. In humans, such reports are often attributed to a\nfaculty of introspection and are typically linked to consciousness. This raises\nthe question of how to interpret self-reports produced by LLMs, given their\nincreasing linguistic fluency and cognitive capabilities. To what extent (if\nany) can the concept of introspection be meaningfully applied to LLMs? Here, we\npresent and critique two examples of apparent introspective self-report from\nLLMs. In the first example, an LLM attempts to describe the process behind its\nown \"creative\" writing, and we argue this is not a valid example of\nintrospection. In the second example, an LLM correctly infers the value of its\nown temperature parameter, and we argue that this can be legitimately\nconsidered a minimal example of introspection, albeit one that is (presumably)\nnot accompanied by conscious experience.\n", "link": "http://arxiv.org/abs/2506.05068v2", "date": "2025-06-06", "relevancy": 2.2379, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4652}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4652}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Does%20It%20Make%20Sense%20to%20Speak%20of%20Introspection%20in%20Large%20Language%20Models%3F&body=Title%3A%20Does%20It%20Make%20Sense%20to%20Speak%20of%20Introspection%20in%20Large%20Language%20Models%3F%0AAuthor%3A%20Iulia%20M.%20Comsa%20and%20Murray%20Shanahan%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20exhibit%20compelling%20linguistic%20behaviour%2C%20and%0Asometimes%20offer%20self-reports%2C%20that%20is%20to%20say%20statements%20about%20their%20own%20nature%2C%0Ainner%20workings%2C%20or%20behaviour.%20In%20humans%2C%20such%20reports%20are%20often%20attributed%20to%20a%0Afaculty%20of%20introspection%20and%20are%20typically%20linked%20to%20consciousness.%20This%20raises%0Athe%20question%20of%20how%20to%20interpret%20self-reports%20produced%20by%20LLMs%2C%20given%20their%0Aincreasing%20linguistic%20fluency%20and%20cognitive%20capabilities.%20To%20what%20extent%20%28if%0Aany%29%20can%20the%20concept%20of%20introspection%20be%20meaningfully%20applied%20to%20LLMs%3F%20Here%2C%20we%0Apresent%20and%20critique%20two%20examples%20of%20apparent%20introspective%20self-report%20from%0ALLMs.%20In%20the%20first%20example%2C%20an%20LLM%20attempts%20to%20describe%20the%20process%20behind%20its%0Aown%20%22creative%22%20writing%2C%20and%20we%20argue%20this%20is%20not%20a%20valid%20example%20of%0Aintrospection.%20In%20the%20second%20example%2C%20an%20LLM%20correctly%20infers%20the%20value%20of%20its%0Aown%20temperature%20parameter%2C%20and%20we%20argue%20that%20this%20can%20be%20legitimately%0Aconsidered%20a%20minimal%20example%20of%20introspection%2C%20albeit%20one%20that%20is%20%28presumably%29%0Anot%20accompanied%20by%20conscious%20experience.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05068v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoes%2520It%2520Make%2520Sense%2520to%2520Speak%2520of%2520Introspection%2520in%2520Large%2520Language%2520Models%253F%26entry.906535625%3DIulia%2520M.%2520Comsa%2520and%2520Murray%2520Shanahan%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520exhibit%2520compelling%2520linguistic%2520behaviour%252C%2520and%250Asometimes%2520offer%2520self-reports%252C%2520that%2520is%2520to%2520say%2520statements%2520about%2520their%2520own%2520nature%252C%250Ainner%2520workings%252C%2520or%2520behaviour.%2520In%2520humans%252C%2520such%2520reports%2520are%2520often%2520attributed%2520to%2520a%250Afaculty%2520of%2520introspection%2520and%2520are%2520typically%2520linked%2520to%2520consciousness.%2520This%2520raises%250Athe%2520question%2520of%2520how%2520to%2520interpret%2520self-reports%2520produced%2520by%2520LLMs%252C%2520given%2520their%250Aincreasing%2520linguistic%2520fluency%2520and%2520cognitive%2520capabilities.%2520To%2520what%2520extent%2520%2528if%250Aany%2529%2520can%2520the%2520concept%2520of%2520introspection%2520be%2520meaningfully%2520applied%2520to%2520LLMs%253F%2520Here%252C%2520we%250Apresent%2520and%2520critique%2520two%2520examples%2520of%2520apparent%2520introspective%2520self-report%2520from%250ALLMs.%2520In%2520the%2520first%2520example%252C%2520an%2520LLM%2520attempts%2520to%2520describe%2520the%2520process%2520behind%2520its%250Aown%2520%2522creative%2522%2520writing%252C%2520and%2520we%2520argue%2520this%2520is%2520not%2520a%2520valid%2520example%2520of%250Aintrospection.%2520In%2520the%2520second%2520example%252C%2520an%2520LLM%2520correctly%2520infers%2520the%2520value%2520of%2520its%250Aown%2520temperature%2520parameter%252C%2520and%2520we%2520argue%2520that%2520this%2520can%2520be%2520legitimately%250Aconsidered%2520a%2520minimal%2520example%2520of%2520introspection%252C%2520albeit%2520one%2520that%2520is%2520%2528presumably%2529%250Anot%2520accompanied%2520by%2520conscious%2520experience.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05068v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Does%20It%20Make%20Sense%20to%20Speak%20of%20Introspection%20in%20Large%20Language%20Models%3F&entry.906535625=Iulia%20M.%20Comsa%20and%20Murray%20Shanahan&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20exhibit%20compelling%20linguistic%20behaviour%2C%20and%0Asometimes%20offer%20self-reports%2C%20that%20is%20to%20say%20statements%20about%20their%20own%20nature%2C%0Ainner%20workings%2C%20or%20behaviour.%20In%20humans%2C%20such%20reports%20are%20often%20attributed%20to%20a%0Afaculty%20of%20introspection%20and%20are%20typically%20linked%20to%20consciousness.%20This%20raises%0Athe%20question%20of%20how%20to%20interpret%20self-reports%20produced%20by%20LLMs%2C%20given%20their%0Aincreasing%20linguistic%20fluency%20and%20cognitive%20capabilities.%20To%20what%20extent%20%28if%0Aany%29%20can%20the%20concept%20of%20introspection%20be%20meaningfully%20applied%20to%20LLMs%3F%20Here%2C%20we%0Apresent%20and%20critique%20two%20examples%20of%20apparent%20introspective%20self-report%20from%0ALLMs.%20In%20the%20first%20example%2C%20an%20LLM%20attempts%20to%20describe%20the%20process%20behind%20its%0Aown%20%22creative%22%20writing%2C%20and%20we%20argue%20this%20is%20not%20a%20valid%20example%20of%0Aintrospection.%20In%20the%20second%20example%2C%20an%20LLM%20correctly%20infers%20the%20value%20of%20its%0Aown%20temperature%20parameter%2C%20and%20we%20argue%20that%20this%20can%20be%20legitimately%0Aconsidered%20a%20minimal%20example%20of%20introspection%2C%20albeit%20one%20that%20is%20%28presumably%29%0Anot%20accompanied%20by%20conscious%20experience.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05068v2&entry.124074799=Read"},
{"title": "HO-FMN: Hyperparameter Optimization for Fast Minimum-Norm Attacks", "author": "Raffaele Mura and Giuseppe Floris and Luca Scionis and Giorgio Piras and Maura Pintor and Ambra Demontis and Giorgio Giacinto and Battista Biggio and Fabio Roli", "abstract": "  Gradient-based attacks are a primary tool to evaluate robustness of\nmachine-learning models. However, many attacks tend to provide\noverly-optimistic evaluations as they use fixed loss functions, optimizers,\nstep-size schedulers, and default hyperparameters. In this work, we tackle\nthese limitations by proposing a parametric variation of the well-known fast\nminimum-norm attack algorithm, whose loss, optimizer, step-size scheduler, and\nhyperparameters can be dynamically adjusted. We re-evaluate 12 robust models,\nshowing that our attack finds smaller adversarial perturbations without\nrequiring any additional tuning. This also enables reporting adversarial\nrobustness as a function of the perturbation budget, providing a more complete\nevaluation than that offered by fixed-budget attacks, while remaining\nefficient. We release our open-source code at https://github.com/pralab/HO-FMN.\n", "link": "http://arxiv.org/abs/2407.08806v2", "date": "2025-06-06", "relevancy": 2.2252, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4692}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4412}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HO-FMN%3A%20Hyperparameter%20Optimization%20for%20Fast%20Minimum-Norm%20Attacks&body=Title%3A%20HO-FMN%3A%20Hyperparameter%20Optimization%20for%20Fast%20Minimum-Norm%20Attacks%0AAuthor%3A%20Raffaele%20Mura%20and%20Giuseppe%20Floris%20and%20Luca%20Scionis%20and%20Giorgio%20Piras%20and%20Maura%20Pintor%20and%20Ambra%20Demontis%20and%20Giorgio%20Giacinto%20and%20Battista%20Biggio%20and%20Fabio%20Roli%0AAbstract%3A%20%20%20Gradient-based%20attacks%20are%20a%20primary%20tool%20to%20evaluate%20robustness%20of%0Amachine-learning%20models.%20However%2C%20many%20attacks%20tend%20to%20provide%0Aoverly-optimistic%20evaluations%20as%20they%20use%20fixed%20loss%20functions%2C%20optimizers%2C%0Astep-size%20schedulers%2C%20and%20default%20hyperparameters.%20In%20this%20work%2C%20we%20tackle%0Athese%20limitations%20by%20proposing%20a%20parametric%20variation%20of%20the%20well-known%20fast%0Aminimum-norm%20attack%20algorithm%2C%20whose%20loss%2C%20optimizer%2C%20step-size%20scheduler%2C%20and%0Ahyperparameters%20can%20be%20dynamically%20adjusted.%20We%20re-evaluate%2012%20robust%20models%2C%0Ashowing%20that%20our%20attack%20finds%20smaller%20adversarial%20perturbations%20without%0Arequiring%20any%20additional%20tuning.%20This%20also%20enables%20reporting%20adversarial%0Arobustness%20as%20a%20function%20of%20the%20perturbation%20budget%2C%20providing%20a%20more%20complete%0Aevaluation%20than%20that%20offered%20by%20fixed-budget%20attacks%2C%20while%20remaining%0Aefficient.%20We%20release%20our%20open-source%20code%20at%20https%3A//github.com/pralab/HO-FMN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08806v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHO-FMN%253A%2520Hyperparameter%2520Optimization%2520for%2520Fast%2520Minimum-Norm%2520Attacks%26entry.906535625%3DRaffaele%2520Mura%2520and%2520Giuseppe%2520Floris%2520and%2520Luca%2520Scionis%2520and%2520Giorgio%2520Piras%2520and%2520Maura%2520Pintor%2520and%2520Ambra%2520Demontis%2520and%2520Giorgio%2520Giacinto%2520and%2520Battista%2520Biggio%2520and%2520Fabio%2520Roli%26entry.1292438233%3D%2520%2520Gradient-based%2520attacks%2520are%2520a%2520primary%2520tool%2520to%2520evaluate%2520robustness%2520of%250Amachine-learning%2520models.%2520However%252C%2520many%2520attacks%2520tend%2520to%2520provide%250Aoverly-optimistic%2520evaluations%2520as%2520they%2520use%2520fixed%2520loss%2520functions%252C%2520optimizers%252C%250Astep-size%2520schedulers%252C%2520and%2520default%2520hyperparameters.%2520In%2520this%2520work%252C%2520we%2520tackle%250Athese%2520limitations%2520by%2520proposing%2520a%2520parametric%2520variation%2520of%2520the%2520well-known%2520fast%250Aminimum-norm%2520attack%2520algorithm%252C%2520whose%2520loss%252C%2520optimizer%252C%2520step-size%2520scheduler%252C%2520and%250Ahyperparameters%2520can%2520be%2520dynamically%2520adjusted.%2520We%2520re-evaluate%252012%2520robust%2520models%252C%250Ashowing%2520that%2520our%2520attack%2520finds%2520smaller%2520adversarial%2520perturbations%2520without%250Arequiring%2520any%2520additional%2520tuning.%2520This%2520also%2520enables%2520reporting%2520adversarial%250Arobustness%2520as%2520a%2520function%2520of%2520the%2520perturbation%2520budget%252C%2520providing%2520a%2520more%2520complete%250Aevaluation%2520than%2520that%2520offered%2520by%2520fixed-budget%2520attacks%252C%2520while%2520remaining%250Aefficient.%2520We%2520release%2520our%2520open-source%2520code%2520at%2520https%253A//github.com/pralab/HO-FMN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08806v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HO-FMN%3A%20Hyperparameter%20Optimization%20for%20Fast%20Minimum-Norm%20Attacks&entry.906535625=Raffaele%20Mura%20and%20Giuseppe%20Floris%20and%20Luca%20Scionis%20and%20Giorgio%20Piras%20and%20Maura%20Pintor%20and%20Ambra%20Demontis%20and%20Giorgio%20Giacinto%20and%20Battista%20Biggio%20and%20Fabio%20Roli&entry.1292438233=%20%20Gradient-based%20attacks%20are%20a%20primary%20tool%20to%20evaluate%20robustness%20of%0Amachine-learning%20models.%20However%2C%20many%20attacks%20tend%20to%20provide%0Aoverly-optimistic%20evaluations%20as%20they%20use%20fixed%20loss%20functions%2C%20optimizers%2C%0Astep-size%20schedulers%2C%20and%20default%20hyperparameters.%20In%20this%20work%2C%20we%20tackle%0Athese%20limitations%20by%20proposing%20a%20parametric%20variation%20of%20the%20well-known%20fast%0Aminimum-norm%20attack%20algorithm%2C%20whose%20loss%2C%20optimizer%2C%20step-size%20scheduler%2C%20and%0Ahyperparameters%20can%20be%20dynamically%20adjusted.%20We%20re-evaluate%2012%20robust%20models%2C%0Ashowing%20that%20our%20attack%20finds%20smaller%20adversarial%20perturbations%20without%0Arequiring%20any%20additional%20tuning.%20This%20also%20enables%20reporting%20adversarial%0Arobustness%20as%20a%20function%20of%20the%20perturbation%20budget%2C%20providing%20a%20more%20complete%0Aevaluation%20than%20that%20offered%20by%20fixed-budget%20attacks%2C%20while%20remaining%0Aefficient.%20We%20release%20our%20open-source%20code%20at%20https%3A//github.com/pralab/HO-FMN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08806v2&entry.124074799=Read"},
{"title": "In Search of Forgotten Domain Generalization", "author": "Prasanna Mayilvahanan and Roland S. Zimmermann and Thadd\u00e4us Wiedemer and Evgenia Rusak and Attila Juhos and Matthias Bethge and Wieland Brendel", "abstract": "  Out-of-Domain (OOD) generalization is the ability of a model trained on one\nor more domains to generalize to unseen domains. In the ImageNet era of\ncomputer vision, evaluation sets for measuring a model's OOD performance were\ndesigned to be strictly OOD with respect to style. However, the emergence of\nfoundation models and expansive web-scale datasets has obfuscated this\nevaluation process, as datasets cover a broad range of domains and risk test\ndomain contamination. In search of the forgotten domain generalization, we\ncreate large-scale datasets subsampled from LAION -- LAION-Natural and\nLAION-Rendition -- that are strictly OOD to corresponding ImageNet and\nDomainNet test sets in terms of style. Training CLIP models on these datasets\nreveals that a significant portion of their performance is explained by\nin-domain examples. This indicates that the OOD generalization challenges from\nthe ImageNet era still prevail and that training on web-scale data merely\ncreates the illusion of OOD generalization. Furthermore, through a systematic\nexploration of combining natural and rendition datasets in varying proportions,\nwe identify optimal mixing ratios for model generalization across these\ndomains. Our datasets and results re-enable meaningful assessment of OOD\nrobustness at scale -- a crucial prerequisite for improving model robustness.\n", "link": "http://arxiv.org/abs/2410.08258v2", "date": "2025-06-06", "relevancy": 2.212, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5682}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5499}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In%20Search%20of%20Forgotten%20Domain%20Generalization&body=Title%3A%20In%20Search%20of%20Forgotten%20Domain%20Generalization%0AAuthor%3A%20Prasanna%20Mayilvahanan%20and%20Roland%20S.%20Zimmermann%20and%20Thadd%C3%A4us%20Wiedemer%20and%20Evgenia%20Rusak%20and%20Attila%20Juhos%20and%20Matthias%20Bethge%20and%20Wieland%20Brendel%0AAbstract%3A%20%20%20Out-of-Domain%20%28OOD%29%20generalization%20is%20the%20ability%20of%20a%20model%20trained%20on%20one%0Aor%20more%20domains%20to%20generalize%20to%20unseen%20domains.%20In%20the%20ImageNet%20era%20of%0Acomputer%20vision%2C%20evaluation%20sets%20for%20measuring%20a%20model%27s%20OOD%20performance%20were%0Adesigned%20to%20be%20strictly%20OOD%20with%20respect%20to%20style.%20However%2C%20the%20emergence%20of%0Afoundation%20models%20and%20expansive%20web-scale%20datasets%20has%20obfuscated%20this%0Aevaluation%20process%2C%20as%20datasets%20cover%20a%20broad%20range%20of%20domains%20and%20risk%20test%0Adomain%20contamination.%20In%20search%20of%20the%20forgotten%20domain%20generalization%2C%20we%0Acreate%20large-scale%20datasets%20subsampled%20from%20LAION%20--%20LAION-Natural%20and%0ALAION-Rendition%20--%20that%20are%20strictly%20OOD%20to%20corresponding%20ImageNet%20and%0ADomainNet%20test%20sets%20in%20terms%20of%20style.%20Training%20CLIP%20models%20on%20these%20datasets%0Areveals%20that%20a%20significant%20portion%20of%20their%20performance%20is%20explained%20by%0Ain-domain%20examples.%20This%20indicates%20that%20the%20OOD%20generalization%20challenges%20from%0Athe%20ImageNet%20era%20still%20prevail%20and%20that%20training%20on%20web-scale%20data%20merely%0Acreates%20the%20illusion%20of%20OOD%20generalization.%20Furthermore%2C%20through%20a%20systematic%0Aexploration%20of%20combining%20natural%20and%20rendition%20datasets%20in%20varying%20proportions%2C%0Awe%20identify%20optimal%20mixing%20ratios%20for%20model%20generalization%20across%20these%0Adomains.%20Our%20datasets%20and%20results%20re-enable%20meaningful%20assessment%20of%20OOD%0Arobustness%20at%20scale%20--%20a%20crucial%20prerequisite%20for%20improving%20model%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08258v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn%2520Search%2520of%2520Forgotten%2520Domain%2520Generalization%26entry.906535625%3DPrasanna%2520Mayilvahanan%2520and%2520Roland%2520S.%2520Zimmermann%2520and%2520Thadd%25C3%25A4us%2520Wiedemer%2520and%2520Evgenia%2520Rusak%2520and%2520Attila%2520Juhos%2520and%2520Matthias%2520Bethge%2520and%2520Wieland%2520Brendel%26entry.1292438233%3D%2520%2520Out-of-Domain%2520%2528OOD%2529%2520generalization%2520is%2520the%2520ability%2520of%2520a%2520model%2520trained%2520on%2520one%250Aor%2520more%2520domains%2520to%2520generalize%2520to%2520unseen%2520domains.%2520In%2520the%2520ImageNet%2520era%2520of%250Acomputer%2520vision%252C%2520evaluation%2520sets%2520for%2520measuring%2520a%2520model%2527s%2520OOD%2520performance%2520were%250Adesigned%2520to%2520be%2520strictly%2520OOD%2520with%2520respect%2520to%2520style.%2520However%252C%2520the%2520emergence%2520of%250Afoundation%2520models%2520and%2520expansive%2520web-scale%2520datasets%2520has%2520obfuscated%2520this%250Aevaluation%2520process%252C%2520as%2520datasets%2520cover%2520a%2520broad%2520range%2520of%2520domains%2520and%2520risk%2520test%250Adomain%2520contamination.%2520In%2520search%2520of%2520the%2520forgotten%2520domain%2520generalization%252C%2520we%250Acreate%2520large-scale%2520datasets%2520subsampled%2520from%2520LAION%2520--%2520LAION-Natural%2520and%250ALAION-Rendition%2520--%2520that%2520are%2520strictly%2520OOD%2520to%2520corresponding%2520ImageNet%2520and%250ADomainNet%2520test%2520sets%2520in%2520terms%2520of%2520style.%2520Training%2520CLIP%2520models%2520on%2520these%2520datasets%250Areveals%2520that%2520a%2520significant%2520portion%2520of%2520their%2520performance%2520is%2520explained%2520by%250Ain-domain%2520examples.%2520This%2520indicates%2520that%2520the%2520OOD%2520generalization%2520challenges%2520from%250Athe%2520ImageNet%2520era%2520still%2520prevail%2520and%2520that%2520training%2520on%2520web-scale%2520data%2520merely%250Acreates%2520the%2520illusion%2520of%2520OOD%2520generalization.%2520Furthermore%252C%2520through%2520a%2520systematic%250Aexploration%2520of%2520combining%2520natural%2520and%2520rendition%2520datasets%2520in%2520varying%2520proportions%252C%250Awe%2520identify%2520optimal%2520mixing%2520ratios%2520for%2520model%2520generalization%2520across%2520these%250Adomains.%2520Our%2520datasets%2520and%2520results%2520re-enable%2520meaningful%2520assessment%2520of%2520OOD%250Arobustness%2520at%2520scale%2520--%2520a%2520crucial%2520prerequisite%2520for%2520improving%2520model%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08258v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In%20Search%20of%20Forgotten%20Domain%20Generalization&entry.906535625=Prasanna%20Mayilvahanan%20and%20Roland%20S.%20Zimmermann%20and%20Thadd%C3%A4us%20Wiedemer%20and%20Evgenia%20Rusak%20and%20Attila%20Juhos%20and%20Matthias%20Bethge%20and%20Wieland%20Brendel&entry.1292438233=%20%20Out-of-Domain%20%28OOD%29%20generalization%20is%20the%20ability%20of%20a%20model%20trained%20on%20one%0Aor%20more%20domains%20to%20generalize%20to%20unseen%20domains.%20In%20the%20ImageNet%20era%20of%0Acomputer%20vision%2C%20evaluation%20sets%20for%20measuring%20a%20model%27s%20OOD%20performance%20were%0Adesigned%20to%20be%20strictly%20OOD%20with%20respect%20to%20style.%20However%2C%20the%20emergence%20of%0Afoundation%20models%20and%20expansive%20web-scale%20datasets%20has%20obfuscated%20this%0Aevaluation%20process%2C%20as%20datasets%20cover%20a%20broad%20range%20of%20domains%20and%20risk%20test%0Adomain%20contamination.%20In%20search%20of%20the%20forgotten%20domain%20generalization%2C%20we%0Acreate%20large-scale%20datasets%20subsampled%20from%20LAION%20--%20LAION-Natural%20and%0ALAION-Rendition%20--%20that%20are%20strictly%20OOD%20to%20corresponding%20ImageNet%20and%0ADomainNet%20test%20sets%20in%20terms%20of%20style.%20Training%20CLIP%20models%20on%20these%20datasets%0Areveals%20that%20a%20significant%20portion%20of%20their%20performance%20is%20explained%20by%0Ain-domain%20examples.%20This%20indicates%20that%20the%20OOD%20generalization%20challenges%20from%0Athe%20ImageNet%20era%20still%20prevail%20and%20that%20training%20on%20web-scale%20data%20merely%0Acreates%20the%20illusion%20of%20OOD%20generalization.%20Furthermore%2C%20through%20a%20systematic%0Aexploration%20of%20combining%20natural%20and%20rendition%20datasets%20in%20varying%20proportions%2C%0Awe%20identify%20optimal%20mixing%20ratios%20for%20model%20generalization%20across%20these%0Adomains.%20Our%20datasets%20and%20results%20re-enable%20meaningful%20assessment%20of%20OOD%0Arobustness%20at%20scale%20--%20a%20crucial%20prerequisite%20for%20improving%20model%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08258v2&entry.124074799=Read"},
{"title": "A novel non-convex minimax $p$-th order concave penalty function\n  approach to low-rank tensor completion", "author": "Hongbing Zhang and Bing Zheng", "abstract": "  The low-rank tensor completion (LRTC) problem aims to reconstruct a tensor\nfrom partial sample information, which has attracted significant interest in a\nwide range of practical applications such as image processing and computer\nvision. Among the various techniques employed for the LRTC problem, non-convex\nrelaxation methods have been widely studied for their effectiveness in handling\ntensor singular values, which are crucial for accurate tensor recovery. While\nthe minimax concave penalty (MCP) non-convex relaxation method has achieved\npromising results in tackling the LRTC problem and gained widely adopted, it\nexhibits a notable limitation: insufficient penalty on small singular values\nduring the singular value handling process, resulting in inefficient tensor\nrecovery. To address this issue and enhance recovery performance, a novel\nminimax $p$-th order concave penalty (MPCP) function is proposed. Based on this\nnovel function, a tensor $p$-th order $\\tau$ norm is proposed as a non-convex\nrelaxation for tensor rank approximation, thereby establishing an MPCP-based\nLRTC model. Furthermore, theoretical convergence guarantees are rigorously\nestablished for the proposed method. Extensive numerical experiments conducted\non multiple real datasets demonstrate that the proposed method outperforms the\nstate-of-the-art methods in both visual quality and quantitative metrics.\n", "link": "http://arxiv.org/abs/2502.19979v2", "date": "2025-06-06", "relevancy": 2.2069, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4559}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4379}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20novel%20non-convex%20minimax%20%24p%24-th%20order%20concave%20penalty%20function%0A%20%20approach%20to%20low-rank%20tensor%20completion&body=Title%3A%20A%20novel%20non-convex%20minimax%20%24p%24-th%20order%20concave%20penalty%20function%0A%20%20approach%20to%20low-rank%20tensor%20completion%0AAuthor%3A%20Hongbing%20Zhang%20and%20Bing%20Zheng%0AAbstract%3A%20%20%20The%20low-rank%20tensor%20completion%20%28LRTC%29%20problem%20aims%20to%20reconstruct%20a%20tensor%0Afrom%20partial%20sample%20information%2C%20which%20has%20attracted%20significant%20interest%20in%20a%0Awide%20range%20of%20practical%20applications%20such%20as%20image%20processing%20and%20computer%0Avision.%20Among%20the%20various%20techniques%20employed%20for%20the%20LRTC%20problem%2C%20non-convex%0Arelaxation%20methods%20have%20been%20widely%20studied%20for%20their%20effectiveness%20in%20handling%0Atensor%20singular%20values%2C%20which%20are%20crucial%20for%20accurate%20tensor%20recovery.%20While%0Athe%20minimax%20concave%20penalty%20%28MCP%29%20non-convex%20relaxation%20method%20has%20achieved%0Apromising%20results%20in%20tackling%20the%20LRTC%20problem%20and%20gained%20widely%20adopted%2C%20it%0Aexhibits%20a%20notable%20limitation%3A%20insufficient%20penalty%20on%20small%20singular%20values%0Aduring%20the%20singular%20value%20handling%20process%2C%20resulting%20in%20inefficient%20tensor%0Arecovery.%20To%20address%20this%20issue%20and%20enhance%20recovery%20performance%2C%20a%20novel%0Aminimax%20%24p%24-th%20order%20concave%20penalty%20%28MPCP%29%20function%20is%20proposed.%20Based%20on%20this%0Anovel%20function%2C%20a%20tensor%20%24p%24-th%20order%20%24%5Ctau%24%20norm%20is%20proposed%20as%20a%20non-convex%0Arelaxation%20for%20tensor%20rank%20approximation%2C%20thereby%20establishing%20an%20MPCP-based%0ALRTC%20model.%20Furthermore%2C%20theoretical%20convergence%20guarantees%20are%20rigorously%0Aestablished%20for%20the%20proposed%20method.%20Extensive%20numerical%20experiments%20conducted%0Aon%20multiple%20real%20datasets%20demonstrate%20that%20the%20proposed%20method%20outperforms%20the%0Astate-of-the-art%20methods%20in%20both%20visual%20quality%20and%20quantitative%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19979v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520novel%2520non-convex%2520minimax%2520%2524p%2524-th%2520order%2520concave%2520penalty%2520function%250A%2520%2520approach%2520to%2520low-rank%2520tensor%2520completion%26entry.906535625%3DHongbing%2520Zhang%2520and%2520Bing%2520Zheng%26entry.1292438233%3D%2520%2520The%2520low-rank%2520tensor%2520completion%2520%2528LRTC%2529%2520problem%2520aims%2520to%2520reconstruct%2520a%2520tensor%250Afrom%2520partial%2520sample%2520information%252C%2520which%2520has%2520attracted%2520significant%2520interest%2520in%2520a%250Awide%2520range%2520of%2520practical%2520applications%2520such%2520as%2520image%2520processing%2520and%2520computer%250Avision.%2520Among%2520the%2520various%2520techniques%2520employed%2520for%2520the%2520LRTC%2520problem%252C%2520non-convex%250Arelaxation%2520methods%2520have%2520been%2520widely%2520studied%2520for%2520their%2520effectiveness%2520in%2520handling%250Atensor%2520singular%2520values%252C%2520which%2520are%2520crucial%2520for%2520accurate%2520tensor%2520recovery.%2520While%250Athe%2520minimax%2520concave%2520penalty%2520%2528MCP%2529%2520non-convex%2520relaxation%2520method%2520has%2520achieved%250Apromising%2520results%2520in%2520tackling%2520the%2520LRTC%2520problem%2520and%2520gained%2520widely%2520adopted%252C%2520it%250Aexhibits%2520a%2520notable%2520limitation%253A%2520insufficient%2520penalty%2520on%2520small%2520singular%2520values%250Aduring%2520the%2520singular%2520value%2520handling%2520process%252C%2520resulting%2520in%2520inefficient%2520tensor%250Arecovery.%2520To%2520address%2520this%2520issue%2520and%2520enhance%2520recovery%2520performance%252C%2520a%2520novel%250Aminimax%2520%2524p%2524-th%2520order%2520concave%2520penalty%2520%2528MPCP%2529%2520function%2520is%2520proposed.%2520Based%2520on%2520this%250Anovel%2520function%252C%2520a%2520tensor%2520%2524p%2524-th%2520order%2520%2524%255Ctau%2524%2520norm%2520is%2520proposed%2520as%2520a%2520non-convex%250Arelaxation%2520for%2520tensor%2520rank%2520approximation%252C%2520thereby%2520establishing%2520an%2520MPCP-based%250ALRTC%2520model.%2520Furthermore%252C%2520theoretical%2520convergence%2520guarantees%2520are%2520rigorously%250Aestablished%2520for%2520the%2520proposed%2520method.%2520Extensive%2520numerical%2520experiments%2520conducted%250Aon%2520multiple%2520real%2520datasets%2520demonstrate%2520that%2520the%2520proposed%2520method%2520outperforms%2520the%250Astate-of-the-art%2520methods%2520in%2520both%2520visual%2520quality%2520and%2520quantitative%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19979v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20novel%20non-convex%20minimax%20%24p%24-th%20order%20concave%20penalty%20function%0A%20%20approach%20to%20low-rank%20tensor%20completion&entry.906535625=Hongbing%20Zhang%20and%20Bing%20Zheng&entry.1292438233=%20%20The%20low-rank%20tensor%20completion%20%28LRTC%29%20problem%20aims%20to%20reconstruct%20a%20tensor%0Afrom%20partial%20sample%20information%2C%20which%20has%20attracted%20significant%20interest%20in%20a%0Awide%20range%20of%20practical%20applications%20such%20as%20image%20processing%20and%20computer%0Avision.%20Among%20the%20various%20techniques%20employed%20for%20the%20LRTC%20problem%2C%20non-convex%0Arelaxation%20methods%20have%20been%20widely%20studied%20for%20their%20effectiveness%20in%20handling%0Atensor%20singular%20values%2C%20which%20are%20crucial%20for%20accurate%20tensor%20recovery.%20While%0Athe%20minimax%20concave%20penalty%20%28MCP%29%20non-convex%20relaxation%20method%20has%20achieved%0Apromising%20results%20in%20tackling%20the%20LRTC%20problem%20and%20gained%20widely%20adopted%2C%20it%0Aexhibits%20a%20notable%20limitation%3A%20insufficient%20penalty%20on%20small%20singular%20values%0Aduring%20the%20singular%20value%20handling%20process%2C%20resulting%20in%20inefficient%20tensor%0Arecovery.%20To%20address%20this%20issue%20and%20enhance%20recovery%20performance%2C%20a%20novel%0Aminimax%20%24p%24-th%20order%20concave%20penalty%20%28MPCP%29%20function%20is%20proposed.%20Based%20on%20this%0Anovel%20function%2C%20a%20tensor%20%24p%24-th%20order%20%24%5Ctau%24%20norm%20is%20proposed%20as%20a%20non-convex%0Arelaxation%20for%20tensor%20rank%20approximation%2C%20thereby%20establishing%20an%20MPCP-based%0ALRTC%20model.%20Furthermore%2C%20theoretical%20convergence%20guarantees%20are%20rigorously%0Aestablished%20for%20the%20proposed%20method.%20Extensive%20numerical%20experiments%20conducted%0Aon%20multiple%20real%20datasets%20demonstrate%20that%20the%20proposed%20method%20outperforms%20the%0Astate-of-the-art%20methods%20in%20both%20visual%20quality%20and%20quantitative%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19979v2&entry.124074799=Read"},
{"title": "Diving into Self-Evolving Training for Multimodal Reasoning", "author": "Wei Liu and Junlong Li and Xiwen Zhang and Fan Zhou and Yu Cheng and Junxian He", "abstract": "  Self-evolving trainin--where models iteratively learn from their own\noutputs--has emerged as a key approach for complex reasoning tasks, addressing\nthe scarcity of high-quality chain-of-thought data. However, its effectiveness\nin multimodal reasoning, a domain more intricate than text-only reasoning,\nremains underexplored, and the understanding of critical factors in this\ntraining paradigm remains limited. Furthermore, a central challenge for this\ntraining method is performance saturation, which impedes further improvements\nand scalability. Inspired by reinforcement learning (RL), in this paper, we\nreframe self-evolving training for multimodal reasoning through the lens of RL,\nidentifying three pivotal factors: Training Method, Reward Model, and Prompt\nVariation. Through systematic analysis, we establish relatively optimal design\nprinciples that significantly enhance multimodal reasoning capabilities.\nMoreover, delving deeper into training dynamics, we uncover the roots of\nsaturation and propose a new automatic balancing mechanism to mitigate this\nlimitation. Building on these insights, we propose M-STAR (Multimodal\nSelf-evolving Training for Reasoning), a framework that achieves consistent\nperformance gains across models of varying sizes and diverse benchmarks. All\nresources are made publicly available at https://mstar-lmm.github.io.\n", "link": "http://arxiv.org/abs/2412.17451v3", "date": "2025-06-06", "relevancy": 2.1992, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5595}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5432}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diving%20into%20Self-Evolving%20Training%20for%20Multimodal%20Reasoning&body=Title%3A%20Diving%20into%20Self-Evolving%20Training%20for%20Multimodal%20Reasoning%0AAuthor%3A%20Wei%20Liu%20and%20Junlong%20Li%20and%20Xiwen%20Zhang%20and%20Fan%20Zhou%20and%20Yu%20Cheng%20and%20Junxian%20He%0AAbstract%3A%20%20%20Self-evolving%20trainin--where%20models%20iteratively%20learn%20from%20their%20own%0Aoutputs--has%20emerged%20as%20a%20key%20approach%20for%20complex%20reasoning%20tasks%2C%20addressing%0Athe%20scarcity%20of%20high-quality%20chain-of-thought%20data.%20However%2C%20its%20effectiveness%0Ain%20multimodal%20reasoning%2C%20a%20domain%20more%20intricate%20than%20text-only%20reasoning%2C%0Aremains%20underexplored%2C%20and%20the%20understanding%20of%20critical%20factors%20in%20this%0Atraining%20paradigm%20remains%20limited.%20Furthermore%2C%20a%20central%20challenge%20for%20this%0Atraining%20method%20is%20performance%20saturation%2C%20which%20impedes%20further%20improvements%0Aand%20scalability.%20Inspired%20by%20reinforcement%20learning%20%28RL%29%2C%20in%20this%20paper%2C%20we%0Areframe%20self-evolving%20training%20for%20multimodal%20reasoning%20through%20the%20lens%20of%20RL%2C%0Aidentifying%20three%20pivotal%20factors%3A%20Training%20Method%2C%20Reward%20Model%2C%20and%20Prompt%0AVariation.%20Through%20systematic%20analysis%2C%20we%20establish%20relatively%20optimal%20design%0Aprinciples%20that%20significantly%20enhance%20multimodal%20reasoning%20capabilities.%0AMoreover%2C%20delving%20deeper%20into%20training%20dynamics%2C%20we%20uncover%20the%20roots%20of%0Asaturation%20and%20propose%20a%20new%20automatic%20balancing%20mechanism%20to%20mitigate%20this%0Alimitation.%20Building%20on%20these%20insights%2C%20we%20propose%20M-STAR%20%28Multimodal%0ASelf-evolving%20Training%20for%20Reasoning%29%2C%20a%20framework%20that%20achieves%20consistent%0Aperformance%20gains%20across%20models%20of%20varying%20sizes%20and%20diverse%20benchmarks.%20All%0Aresources%20are%20made%20publicly%20available%20at%20https%3A//mstar-lmm.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17451v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiving%2520into%2520Self-Evolving%2520Training%2520for%2520Multimodal%2520Reasoning%26entry.906535625%3DWei%2520Liu%2520and%2520Junlong%2520Li%2520and%2520Xiwen%2520Zhang%2520and%2520Fan%2520Zhou%2520and%2520Yu%2520Cheng%2520and%2520Junxian%2520He%26entry.1292438233%3D%2520%2520Self-evolving%2520trainin--where%2520models%2520iteratively%2520learn%2520from%2520their%2520own%250Aoutputs--has%2520emerged%2520as%2520a%2520key%2520approach%2520for%2520complex%2520reasoning%2520tasks%252C%2520addressing%250Athe%2520scarcity%2520of%2520high-quality%2520chain-of-thought%2520data.%2520However%252C%2520its%2520effectiveness%250Ain%2520multimodal%2520reasoning%252C%2520a%2520domain%2520more%2520intricate%2520than%2520text-only%2520reasoning%252C%250Aremains%2520underexplored%252C%2520and%2520the%2520understanding%2520of%2520critical%2520factors%2520in%2520this%250Atraining%2520paradigm%2520remains%2520limited.%2520Furthermore%252C%2520a%2520central%2520challenge%2520for%2520this%250Atraining%2520method%2520is%2520performance%2520saturation%252C%2520which%2520impedes%2520further%2520improvements%250Aand%2520scalability.%2520Inspired%2520by%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520in%2520this%2520paper%252C%2520we%250Areframe%2520self-evolving%2520training%2520for%2520multimodal%2520reasoning%2520through%2520the%2520lens%2520of%2520RL%252C%250Aidentifying%2520three%2520pivotal%2520factors%253A%2520Training%2520Method%252C%2520Reward%2520Model%252C%2520and%2520Prompt%250AVariation.%2520Through%2520systematic%2520analysis%252C%2520we%2520establish%2520relatively%2520optimal%2520design%250Aprinciples%2520that%2520significantly%2520enhance%2520multimodal%2520reasoning%2520capabilities.%250AMoreover%252C%2520delving%2520deeper%2520into%2520training%2520dynamics%252C%2520we%2520uncover%2520the%2520roots%2520of%250Asaturation%2520and%2520propose%2520a%2520new%2520automatic%2520balancing%2520mechanism%2520to%2520mitigate%2520this%250Alimitation.%2520Building%2520on%2520these%2520insights%252C%2520we%2520propose%2520M-STAR%2520%2528Multimodal%250ASelf-evolving%2520Training%2520for%2520Reasoning%2529%252C%2520a%2520framework%2520that%2520achieves%2520consistent%250Aperformance%2520gains%2520across%2520models%2520of%2520varying%2520sizes%2520and%2520diverse%2520benchmarks.%2520All%250Aresources%2520are%2520made%2520publicly%2520available%2520at%2520https%253A//mstar-lmm.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17451v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diving%20into%20Self-Evolving%20Training%20for%20Multimodal%20Reasoning&entry.906535625=Wei%20Liu%20and%20Junlong%20Li%20and%20Xiwen%20Zhang%20and%20Fan%20Zhou%20and%20Yu%20Cheng%20and%20Junxian%20He&entry.1292438233=%20%20Self-evolving%20trainin--where%20models%20iteratively%20learn%20from%20their%20own%0Aoutputs--has%20emerged%20as%20a%20key%20approach%20for%20complex%20reasoning%20tasks%2C%20addressing%0Athe%20scarcity%20of%20high-quality%20chain-of-thought%20data.%20However%2C%20its%20effectiveness%0Ain%20multimodal%20reasoning%2C%20a%20domain%20more%20intricate%20than%20text-only%20reasoning%2C%0Aremains%20underexplored%2C%20and%20the%20understanding%20of%20critical%20factors%20in%20this%0Atraining%20paradigm%20remains%20limited.%20Furthermore%2C%20a%20central%20challenge%20for%20this%0Atraining%20method%20is%20performance%20saturation%2C%20which%20impedes%20further%20improvements%0Aand%20scalability.%20Inspired%20by%20reinforcement%20learning%20%28RL%29%2C%20in%20this%20paper%2C%20we%0Areframe%20self-evolving%20training%20for%20multimodal%20reasoning%20through%20the%20lens%20of%20RL%2C%0Aidentifying%20three%20pivotal%20factors%3A%20Training%20Method%2C%20Reward%20Model%2C%20and%20Prompt%0AVariation.%20Through%20systematic%20analysis%2C%20we%20establish%20relatively%20optimal%20design%0Aprinciples%20that%20significantly%20enhance%20multimodal%20reasoning%20capabilities.%0AMoreover%2C%20delving%20deeper%20into%20training%20dynamics%2C%20we%20uncover%20the%20roots%20of%0Asaturation%20and%20propose%20a%20new%20automatic%20balancing%20mechanism%20to%20mitigate%20this%0Alimitation.%20Building%20on%20these%20insights%2C%20we%20propose%20M-STAR%20%28Multimodal%0ASelf-evolving%20Training%20for%20Reasoning%29%2C%20a%20framework%20that%20achieves%20consistent%0Aperformance%20gains%20across%20models%20of%20varying%20sizes%20and%20diverse%20benchmarks.%20All%0Aresources%20are%20made%20publicly%20available%20at%20https%3A//mstar-lmm.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17451v3&entry.124074799=Read"},
{"title": "Rethinking Machine Unlearning in Image Generation Models", "author": "Renyang Liu and Wenjie Feng and Tianwei Zhang and Wei Zhou and Xueqi Cheng and See-Kiong Ng", "abstract": "  With the surge and widespread application of image generation models, data\nprivacy and content safety have become major concerns and attracted great\nattention from users, service providers, and policymakers. Machine unlearning\n(MU) is recognized as a cost-effective and promising means to address these\nchallenges. Despite some advancements, image generation model unlearning (IGMU)\nstill faces remarkable gaps in practice, e.g., unclear task discrimination and\nunlearning guidelines, lack of an effective evaluation framework, and\nunreliable evaluation metrics. These can hinder the understanding of unlearning\nmechanisms and the design of practical unlearning algorithms. We perform\nexhaustive assessments over existing state-of-the-art unlearning algorithms and\nevaluation standards, and discover several critical flaws and challenges in\nIGMU tasks. Driven by these limitations, we make several core contributions, to\nfacilitate the comprehensive understanding, standardized categorization, and\nreliable evaluation of IGMU. Specifically, (1) We design CatIGMU, a novel\nhierarchical task categorization framework. It provides detailed implementation\nguidance for IGMU, assisting in the design of unlearning algorithms and the\nconstruction of testbeds. (2) We introduce EvalIGMU, a comprehensive evaluation\nframework. It includes reliable quantitative metrics across five critical\naspects. (3) We construct DataIGM, a high-quality unlearning dataset, which can\nbe used for extensive evaluations of IGMU, training content detectors for\njudgment, and benchmarking the state-of-the-art unlearning algorithms. With\nEvalIGMU and DataIGM, we discover that most existing IGMU algorithms cannot\nhandle the unlearning well across different evaluation dimensions, especially\nfor preservation and robustness. Code and models are available at\nhttps://github.com/ryliu68/IGMU.\n", "link": "http://arxiv.org/abs/2506.02761v2", "date": "2025-06-06", "relevancy": 2.1973, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5769}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.573}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Machine%20Unlearning%20in%20Image%20Generation%20Models&body=Title%3A%20Rethinking%20Machine%20Unlearning%20in%20Image%20Generation%20Models%0AAuthor%3A%20Renyang%20Liu%20and%20Wenjie%20Feng%20and%20Tianwei%20Zhang%20and%20Wei%20Zhou%20and%20Xueqi%20Cheng%20and%20See-Kiong%20Ng%0AAbstract%3A%20%20%20With%20the%20surge%20and%20widespread%20application%20of%20image%20generation%20models%2C%20data%0Aprivacy%20and%20content%20safety%20have%20become%20major%20concerns%20and%20attracted%20great%0Aattention%20from%20users%2C%20service%20providers%2C%20and%20policymakers.%20Machine%20unlearning%0A%28MU%29%20is%20recognized%20as%20a%20cost-effective%20and%20promising%20means%20to%20address%20these%0Achallenges.%20Despite%20some%20advancements%2C%20image%20generation%20model%20unlearning%20%28IGMU%29%0Astill%20faces%20remarkable%20gaps%20in%20practice%2C%20e.g.%2C%20unclear%20task%20discrimination%20and%0Aunlearning%20guidelines%2C%20lack%20of%20an%20effective%20evaluation%20framework%2C%20and%0Aunreliable%20evaluation%20metrics.%20These%20can%20hinder%20the%20understanding%20of%20unlearning%0Amechanisms%20and%20the%20design%20of%20practical%20unlearning%20algorithms.%20We%20perform%0Aexhaustive%20assessments%20over%20existing%20state-of-the-art%20unlearning%20algorithms%20and%0Aevaluation%20standards%2C%20and%20discover%20several%20critical%20flaws%20and%20challenges%20in%0AIGMU%20tasks.%20Driven%20by%20these%20limitations%2C%20we%20make%20several%20core%20contributions%2C%20to%0Afacilitate%20the%20comprehensive%20understanding%2C%20standardized%20categorization%2C%20and%0Areliable%20evaluation%20of%20IGMU.%20Specifically%2C%20%281%29%20We%20design%20CatIGMU%2C%20a%20novel%0Ahierarchical%20task%20categorization%20framework.%20It%20provides%20detailed%20implementation%0Aguidance%20for%20IGMU%2C%20assisting%20in%20the%20design%20of%20unlearning%20algorithms%20and%20the%0Aconstruction%20of%20testbeds.%20%282%29%20We%20introduce%20EvalIGMU%2C%20a%20comprehensive%20evaluation%0Aframework.%20It%20includes%20reliable%20quantitative%20metrics%20across%20five%20critical%0Aaspects.%20%283%29%20We%20construct%20DataIGM%2C%20a%20high-quality%20unlearning%20dataset%2C%20which%20can%0Abe%20used%20for%20extensive%20evaluations%20of%20IGMU%2C%20training%20content%20detectors%20for%0Ajudgment%2C%20and%20benchmarking%20the%20state-of-the-art%20unlearning%20algorithms.%20With%0AEvalIGMU%20and%20DataIGM%2C%20we%20discover%20that%20most%20existing%20IGMU%20algorithms%20cannot%0Ahandle%20the%20unlearning%20well%20across%20different%20evaluation%20dimensions%2C%20especially%0Afor%20preservation%20and%20robustness.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/ryliu68/IGMU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02761v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Machine%2520Unlearning%2520in%2520Image%2520Generation%2520Models%26entry.906535625%3DRenyang%2520Liu%2520and%2520Wenjie%2520Feng%2520and%2520Tianwei%2520Zhang%2520and%2520Wei%2520Zhou%2520and%2520Xueqi%2520Cheng%2520and%2520See-Kiong%2520Ng%26entry.1292438233%3D%2520%2520With%2520the%2520surge%2520and%2520widespread%2520application%2520of%2520image%2520generation%2520models%252C%2520data%250Aprivacy%2520and%2520content%2520safety%2520have%2520become%2520major%2520concerns%2520and%2520attracted%2520great%250Aattention%2520from%2520users%252C%2520service%2520providers%252C%2520and%2520policymakers.%2520Machine%2520unlearning%250A%2528MU%2529%2520is%2520recognized%2520as%2520a%2520cost-effective%2520and%2520promising%2520means%2520to%2520address%2520these%250Achallenges.%2520Despite%2520some%2520advancements%252C%2520image%2520generation%2520model%2520unlearning%2520%2528IGMU%2529%250Astill%2520faces%2520remarkable%2520gaps%2520in%2520practice%252C%2520e.g.%252C%2520unclear%2520task%2520discrimination%2520and%250Aunlearning%2520guidelines%252C%2520lack%2520of%2520an%2520effective%2520evaluation%2520framework%252C%2520and%250Aunreliable%2520evaluation%2520metrics.%2520These%2520can%2520hinder%2520the%2520understanding%2520of%2520unlearning%250Amechanisms%2520and%2520the%2520design%2520of%2520practical%2520unlearning%2520algorithms.%2520We%2520perform%250Aexhaustive%2520assessments%2520over%2520existing%2520state-of-the-art%2520unlearning%2520algorithms%2520and%250Aevaluation%2520standards%252C%2520and%2520discover%2520several%2520critical%2520flaws%2520and%2520challenges%2520in%250AIGMU%2520tasks.%2520Driven%2520by%2520these%2520limitations%252C%2520we%2520make%2520several%2520core%2520contributions%252C%2520to%250Afacilitate%2520the%2520comprehensive%2520understanding%252C%2520standardized%2520categorization%252C%2520and%250Areliable%2520evaluation%2520of%2520IGMU.%2520Specifically%252C%2520%25281%2529%2520We%2520design%2520CatIGMU%252C%2520a%2520novel%250Ahierarchical%2520task%2520categorization%2520framework.%2520It%2520provides%2520detailed%2520implementation%250Aguidance%2520for%2520IGMU%252C%2520assisting%2520in%2520the%2520design%2520of%2520unlearning%2520algorithms%2520and%2520the%250Aconstruction%2520of%2520testbeds.%2520%25282%2529%2520We%2520introduce%2520EvalIGMU%252C%2520a%2520comprehensive%2520evaluation%250Aframework.%2520It%2520includes%2520reliable%2520quantitative%2520metrics%2520across%2520five%2520critical%250Aaspects.%2520%25283%2529%2520We%2520construct%2520DataIGM%252C%2520a%2520high-quality%2520unlearning%2520dataset%252C%2520which%2520can%250Abe%2520used%2520for%2520extensive%2520evaluations%2520of%2520IGMU%252C%2520training%2520content%2520detectors%2520for%250Ajudgment%252C%2520and%2520benchmarking%2520the%2520state-of-the-art%2520unlearning%2520algorithms.%2520With%250AEvalIGMU%2520and%2520DataIGM%252C%2520we%2520discover%2520that%2520most%2520existing%2520IGMU%2520algorithms%2520cannot%250Ahandle%2520the%2520unlearning%2520well%2520across%2520different%2520evaluation%2520dimensions%252C%2520especially%250Afor%2520preservation%2520and%2520robustness.%2520Code%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/ryliu68/IGMU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02761v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Machine%20Unlearning%20in%20Image%20Generation%20Models&entry.906535625=Renyang%20Liu%20and%20Wenjie%20Feng%20and%20Tianwei%20Zhang%20and%20Wei%20Zhou%20and%20Xueqi%20Cheng%20and%20See-Kiong%20Ng&entry.1292438233=%20%20With%20the%20surge%20and%20widespread%20application%20of%20image%20generation%20models%2C%20data%0Aprivacy%20and%20content%20safety%20have%20become%20major%20concerns%20and%20attracted%20great%0Aattention%20from%20users%2C%20service%20providers%2C%20and%20policymakers.%20Machine%20unlearning%0A%28MU%29%20is%20recognized%20as%20a%20cost-effective%20and%20promising%20means%20to%20address%20these%0Achallenges.%20Despite%20some%20advancements%2C%20image%20generation%20model%20unlearning%20%28IGMU%29%0Astill%20faces%20remarkable%20gaps%20in%20practice%2C%20e.g.%2C%20unclear%20task%20discrimination%20and%0Aunlearning%20guidelines%2C%20lack%20of%20an%20effective%20evaluation%20framework%2C%20and%0Aunreliable%20evaluation%20metrics.%20These%20can%20hinder%20the%20understanding%20of%20unlearning%0Amechanisms%20and%20the%20design%20of%20practical%20unlearning%20algorithms.%20We%20perform%0Aexhaustive%20assessments%20over%20existing%20state-of-the-art%20unlearning%20algorithms%20and%0Aevaluation%20standards%2C%20and%20discover%20several%20critical%20flaws%20and%20challenges%20in%0AIGMU%20tasks.%20Driven%20by%20these%20limitations%2C%20we%20make%20several%20core%20contributions%2C%20to%0Afacilitate%20the%20comprehensive%20understanding%2C%20standardized%20categorization%2C%20and%0Areliable%20evaluation%20of%20IGMU.%20Specifically%2C%20%281%29%20We%20design%20CatIGMU%2C%20a%20novel%0Ahierarchical%20task%20categorization%20framework.%20It%20provides%20detailed%20implementation%0Aguidance%20for%20IGMU%2C%20assisting%20in%20the%20design%20of%20unlearning%20algorithms%20and%20the%0Aconstruction%20of%20testbeds.%20%282%29%20We%20introduce%20EvalIGMU%2C%20a%20comprehensive%20evaluation%0Aframework.%20It%20includes%20reliable%20quantitative%20metrics%20across%20five%20critical%0Aaspects.%20%283%29%20We%20construct%20DataIGM%2C%20a%20high-quality%20unlearning%20dataset%2C%20which%20can%0Abe%20used%20for%20extensive%20evaluations%20of%20IGMU%2C%20training%20content%20detectors%20for%0Ajudgment%2C%20and%20benchmarking%20the%20state-of-the-art%20unlearning%20algorithms.%20With%0AEvalIGMU%20and%20DataIGM%2C%20we%20discover%20that%20most%20existing%20IGMU%20algorithms%20cannot%0Ahandle%20the%20unlearning%20well%20across%20different%20evaluation%20dimensions%2C%20especially%0Afor%20preservation%20and%20robustness.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/ryliu68/IGMU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02761v2&entry.124074799=Read"},
{"title": "SeedEdit 3.0: Fast and High-Quality Generative Image Editing", "author": "Peng Wang and Yichun Shi and Xiaochen Lian and Zhonghua Zhai and Xin Xia and Xuefeng Xiao and Weilin Huang and Jianchao Yang", "abstract": "  We introduce SeedEdit 3.0, in companion with our T2I model Seedream 3.0,\nwhich significantly improves over our previous SeedEdit versions in both\naspects of edit instruction following and image content (e.g., ID/IP)\npreservation on real image inputs. Additional to model upgrading with T2I, in\nthis report, we present several key improvements. First, we develop an enhanced\ndata curation pipeline with a meta-info paradigm and meta-info embedding\nstrategy that help mix images from multiple data sources. This allows us to\nscale editing data effectively, and meta information is helpfult to connect VLM\nwith diffusion model more closely. Second, we introduce a joint learning\npipeline for computing a diffusion loss and reward losses. Finally, we evaluate\nSeedEdit 3.0 on our testing benchmarks, for real/synthetic image editing, where\nit achieves a best trade-off between multiple aspects, yielding a high\nusability rate of 56.1%, compared to SeedEdit 1.6 (38.4%), GPT4o (37.1%) and\nGemini 2.0 (30.3%).\n", "link": "http://arxiv.org/abs/2506.05083v2", "date": "2025-06-06", "relevancy": 2.196, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5556}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5476}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SeedEdit%203.0%3A%20Fast%20and%20High-Quality%20Generative%20Image%20Editing&body=Title%3A%20SeedEdit%203.0%3A%20Fast%20and%20High-Quality%20Generative%20Image%20Editing%0AAuthor%3A%20Peng%20Wang%20and%20Yichun%20Shi%20and%20Xiaochen%20Lian%20and%20Zhonghua%20Zhai%20and%20Xin%20Xia%20and%20Xuefeng%20Xiao%20and%20Weilin%20Huang%20and%20Jianchao%20Yang%0AAbstract%3A%20%20%20We%20introduce%20SeedEdit%203.0%2C%20in%20companion%20with%20our%20T2I%20model%20Seedream%203.0%2C%0Awhich%20significantly%20improves%20over%20our%20previous%20SeedEdit%20versions%20in%20both%0Aaspects%20of%20edit%20instruction%20following%20and%20image%20content%20%28e.g.%2C%20ID/IP%29%0Apreservation%20on%20real%20image%20inputs.%20Additional%20to%20model%20upgrading%20with%20T2I%2C%20in%0Athis%20report%2C%20we%20present%20several%20key%20improvements.%20First%2C%20we%20develop%20an%20enhanced%0Adata%20curation%20pipeline%20with%20a%20meta-info%20paradigm%20and%20meta-info%20embedding%0Astrategy%20that%20help%20mix%20images%20from%20multiple%20data%20sources.%20This%20allows%20us%20to%0Ascale%20editing%20data%20effectively%2C%20and%20meta%20information%20is%20helpfult%20to%20connect%20VLM%0Awith%20diffusion%20model%20more%20closely.%20Second%2C%20we%20introduce%20a%20joint%20learning%0Apipeline%20for%20computing%20a%20diffusion%20loss%20and%20reward%20losses.%20Finally%2C%20we%20evaluate%0ASeedEdit%203.0%20on%20our%20testing%20benchmarks%2C%20for%20real/synthetic%20image%20editing%2C%20where%0Ait%20achieves%20a%20best%20trade-off%20between%20multiple%20aspects%2C%20yielding%20a%20high%0Ausability%20rate%20of%2056.1%25%2C%20compared%20to%20SeedEdit%201.6%20%2838.4%25%29%2C%20GPT4o%20%2837.1%25%29%20and%0AGemini%202.0%20%2830.3%25%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05083v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeedEdit%25203.0%253A%2520Fast%2520and%2520High-Quality%2520Generative%2520Image%2520Editing%26entry.906535625%3DPeng%2520Wang%2520and%2520Yichun%2520Shi%2520and%2520Xiaochen%2520Lian%2520and%2520Zhonghua%2520Zhai%2520and%2520Xin%2520Xia%2520and%2520Xuefeng%2520Xiao%2520and%2520Weilin%2520Huang%2520and%2520Jianchao%2520Yang%26entry.1292438233%3D%2520%2520We%2520introduce%2520SeedEdit%25203.0%252C%2520in%2520companion%2520with%2520our%2520T2I%2520model%2520Seedream%25203.0%252C%250Awhich%2520significantly%2520improves%2520over%2520our%2520previous%2520SeedEdit%2520versions%2520in%2520both%250Aaspects%2520of%2520edit%2520instruction%2520following%2520and%2520image%2520content%2520%2528e.g.%252C%2520ID/IP%2529%250Apreservation%2520on%2520real%2520image%2520inputs.%2520Additional%2520to%2520model%2520upgrading%2520with%2520T2I%252C%2520in%250Athis%2520report%252C%2520we%2520present%2520several%2520key%2520improvements.%2520First%252C%2520we%2520develop%2520an%2520enhanced%250Adata%2520curation%2520pipeline%2520with%2520a%2520meta-info%2520paradigm%2520and%2520meta-info%2520embedding%250Astrategy%2520that%2520help%2520mix%2520images%2520from%2520multiple%2520data%2520sources.%2520This%2520allows%2520us%2520to%250Ascale%2520editing%2520data%2520effectively%252C%2520and%2520meta%2520information%2520is%2520helpfult%2520to%2520connect%2520VLM%250Awith%2520diffusion%2520model%2520more%2520closely.%2520Second%252C%2520we%2520introduce%2520a%2520joint%2520learning%250Apipeline%2520for%2520computing%2520a%2520diffusion%2520loss%2520and%2520reward%2520losses.%2520Finally%252C%2520we%2520evaluate%250ASeedEdit%25203.0%2520on%2520our%2520testing%2520benchmarks%252C%2520for%2520real/synthetic%2520image%2520editing%252C%2520where%250Ait%2520achieves%2520a%2520best%2520trade-off%2520between%2520multiple%2520aspects%252C%2520yielding%2520a%2520high%250Ausability%2520rate%2520of%252056.1%2525%252C%2520compared%2520to%2520SeedEdit%25201.6%2520%252838.4%2525%2529%252C%2520GPT4o%2520%252837.1%2525%2529%2520and%250AGemini%25202.0%2520%252830.3%2525%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05083v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SeedEdit%203.0%3A%20Fast%20and%20High-Quality%20Generative%20Image%20Editing&entry.906535625=Peng%20Wang%20and%20Yichun%20Shi%20and%20Xiaochen%20Lian%20and%20Zhonghua%20Zhai%20and%20Xin%20Xia%20and%20Xuefeng%20Xiao%20and%20Weilin%20Huang%20and%20Jianchao%20Yang&entry.1292438233=%20%20We%20introduce%20SeedEdit%203.0%2C%20in%20companion%20with%20our%20T2I%20model%20Seedream%203.0%2C%0Awhich%20significantly%20improves%20over%20our%20previous%20SeedEdit%20versions%20in%20both%0Aaspects%20of%20edit%20instruction%20following%20and%20image%20content%20%28e.g.%2C%20ID/IP%29%0Apreservation%20on%20real%20image%20inputs.%20Additional%20to%20model%20upgrading%20with%20T2I%2C%20in%0Athis%20report%2C%20we%20present%20several%20key%20improvements.%20First%2C%20we%20develop%20an%20enhanced%0Adata%20curation%20pipeline%20with%20a%20meta-info%20paradigm%20and%20meta-info%20embedding%0Astrategy%20that%20help%20mix%20images%20from%20multiple%20data%20sources.%20This%20allows%20us%20to%0Ascale%20editing%20data%20effectively%2C%20and%20meta%20information%20is%20helpfult%20to%20connect%20VLM%0Awith%20diffusion%20model%20more%20closely.%20Second%2C%20we%20introduce%20a%20joint%20learning%0Apipeline%20for%20computing%20a%20diffusion%20loss%20and%20reward%20losses.%20Finally%2C%20we%20evaluate%0ASeedEdit%203.0%20on%20our%20testing%20benchmarks%2C%20for%20real/synthetic%20image%20editing%2C%20where%0Ait%20achieves%20a%20best%20trade-off%20between%20multiple%20aspects%2C%20yielding%20a%20high%0Ausability%20rate%20of%2056.1%25%2C%20compared%20to%20SeedEdit%201.6%20%2838.4%25%29%2C%20GPT4o%20%2837.1%25%29%20and%0AGemini%202.0%20%2830.3%25%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05083v2&entry.124074799=Read"},
{"title": "VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time\n  Series Forecasters", "author": "Mouxiang Chen and Lefei Shen and Zhuo Li and Xiaoyun Joy Wang and Jianling Sun and Chenghao Liu", "abstract": "  Foundation models have emerged as a promising approach in time series\nforecasting (TSF). Existing approaches either repurpose large language models\n(LLMs) or build large-scale time series datasets to develop TSF foundation\nmodels for universal forecasting. However, these methods face challenges due to\nthe severe cross-domain gap or in-domain heterogeneity. This paper explores a\nnew road to building a TSF foundation model from rich, high-quality natural\nimages. Our key insight is that a visual masked autoencoder, pre-trained on the\nImageNet dataset, can naturally be a numeric series forecaster. By\nreformulating TSF as an image reconstruction task, we bridge the gap between\nimage pre-training and TSF downstream tasks. Surprisingly, without further\nadaptation in the time series domain, the proposed VisionTS could achieve\nbetter zero-shot forecast performance than existing TSF foundation models. With\nfine-tuning for one epoch, VisionTS could further improve the forecasting and\nachieve state-of-the-art performance in most cases. Extensive experiments\nreveal intrinsic similarities between images and real-world time series,\nsuggesting that visual models may offer a \"free lunch\" for TSF and highlight\nthe potential for future cross-modality research. Our code is publicly\navailable at https://github.com/Keytoyze/VisionTS.\n", "link": "http://arxiv.org/abs/2408.17253v4", "date": "2025-06-06", "relevancy": 2.1952, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.556}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5474}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisionTS%3A%20Visual%20Masked%20Autoencoders%20Are%20Free-Lunch%20Zero-Shot%20Time%0A%20%20Series%20Forecasters&body=Title%3A%20VisionTS%3A%20Visual%20Masked%20Autoencoders%20Are%20Free-Lunch%20Zero-Shot%20Time%0A%20%20Series%20Forecasters%0AAuthor%3A%20Mouxiang%20Chen%20and%20Lefei%20Shen%20and%20Zhuo%20Li%20and%20Xiaoyun%20Joy%20Wang%20and%20Jianling%20Sun%20and%20Chenghao%20Liu%0AAbstract%3A%20%20%20Foundation%20models%20have%20emerged%20as%20a%20promising%20approach%20in%20time%20series%0Aforecasting%20%28TSF%29.%20Existing%20approaches%20either%20repurpose%20large%20language%20models%0A%28LLMs%29%20or%20build%20large-scale%20time%20series%20datasets%20to%20develop%20TSF%20foundation%0Amodels%20for%20universal%20forecasting.%20However%2C%20these%20methods%20face%20challenges%20due%20to%0Athe%20severe%20cross-domain%20gap%20or%20in-domain%20heterogeneity.%20This%20paper%20explores%20a%0Anew%20road%20to%20building%20a%20TSF%20foundation%20model%20from%20rich%2C%20high-quality%20natural%0Aimages.%20Our%20key%20insight%20is%20that%20a%20visual%20masked%20autoencoder%2C%20pre-trained%20on%20the%0AImageNet%20dataset%2C%20can%20naturally%20be%20a%20numeric%20series%20forecaster.%20By%0Areformulating%20TSF%20as%20an%20image%20reconstruction%20task%2C%20we%20bridge%20the%20gap%20between%0Aimage%20pre-training%20and%20TSF%20downstream%20tasks.%20Surprisingly%2C%20without%20further%0Aadaptation%20in%20the%20time%20series%20domain%2C%20the%20proposed%20VisionTS%20could%20achieve%0Abetter%20zero-shot%20forecast%20performance%20than%20existing%20TSF%20foundation%20models.%20With%0Afine-tuning%20for%20one%20epoch%2C%20VisionTS%20could%20further%20improve%20the%20forecasting%20and%0Aachieve%20state-of-the-art%20performance%20in%20most%20cases.%20Extensive%20experiments%0Areveal%20intrinsic%20similarities%20between%20images%20and%20real-world%20time%20series%2C%0Asuggesting%20that%20visual%20models%20may%20offer%20a%20%22free%20lunch%22%20for%20TSF%20and%20highlight%0Athe%20potential%20for%20future%20cross-modality%20research.%20Our%20code%20is%20publicly%0Aavailable%20at%20https%3A//github.com/Keytoyze/VisionTS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17253v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisionTS%253A%2520Visual%2520Masked%2520Autoencoders%2520Are%2520Free-Lunch%2520Zero-Shot%2520Time%250A%2520%2520Series%2520Forecasters%26entry.906535625%3DMouxiang%2520Chen%2520and%2520Lefei%2520Shen%2520and%2520Zhuo%2520Li%2520and%2520Xiaoyun%2520Joy%2520Wang%2520and%2520Jianling%2520Sun%2520and%2520Chenghao%2520Liu%26entry.1292438233%3D%2520%2520Foundation%2520models%2520have%2520emerged%2520as%2520a%2520promising%2520approach%2520in%2520time%2520series%250Aforecasting%2520%2528TSF%2529.%2520Existing%2520approaches%2520either%2520repurpose%2520large%2520language%2520models%250A%2528LLMs%2529%2520or%2520build%2520large-scale%2520time%2520series%2520datasets%2520to%2520develop%2520TSF%2520foundation%250Amodels%2520for%2520universal%2520forecasting.%2520However%252C%2520these%2520methods%2520face%2520challenges%2520due%2520to%250Athe%2520severe%2520cross-domain%2520gap%2520or%2520in-domain%2520heterogeneity.%2520This%2520paper%2520explores%2520a%250Anew%2520road%2520to%2520building%2520a%2520TSF%2520foundation%2520model%2520from%2520rich%252C%2520high-quality%2520natural%250Aimages.%2520Our%2520key%2520insight%2520is%2520that%2520a%2520visual%2520masked%2520autoencoder%252C%2520pre-trained%2520on%2520the%250AImageNet%2520dataset%252C%2520can%2520naturally%2520be%2520a%2520numeric%2520series%2520forecaster.%2520By%250Areformulating%2520TSF%2520as%2520an%2520image%2520reconstruction%2520task%252C%2520we%2520bridge%2520the%2520gap%2520between%250Aimage%2520pre-training%2520and%2520TSF%2520downstream%2520tasks.%2520Surprisingly%252C%2520without%2520further%250Aadaptation%2520in%2520the%2520time%2520series%2520domain%252C%2520the%2520proposed%2520VisionTS%2520could%2520achieve%250Abetter%2520zero-shot%2520forecast%2520performance%2520than%2520existing%2520TSF%2520foundation%2520models.%2520With%250Afine-tuning%2520for%2520one%2520epoch%252C%2520VisionTS%2520could%2520further%2520improve%2520the%2520forecasting%2520and%250Aachieve%2520state-of-the-art%2520performance%2520in%2520most%2520cases.%2520Extensive%2520experiments%250Areveal%2520intrinsic%2520similarities%2520between%2520images%2520and%2520real-world%2520time%2520series%252C%250Asuggesting%2520that%2520visual%2520models%2520may%2520offer%2520a%2520%2522free%2520lunch%2522%2520for%2520TSF%2520and%2520highlight%250Athe%2520potential%2520for%2520future%2520cross-modality%2520research.%2520Our%2520code%2520is%2520publicly%250Aavailable%2520at%2520https%253A//github.com/Keytoyze/VisionTS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17253v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisionTS%3A%20Visual%20Masked%20Autoencoders%20Are%20Free-Lunch%20Zero-Shot%20Time%0A%20%20Series%20Forecasters&entry.906535625=Mouxiang%20Chen%20and%20Lefei%20Shen%20and%20Zhuo%20Li%20and%20Xiaoyun%20Joy%20Wang%20and%20Jianling%20Sun%20and%20Chenghao%20Liu&entry.1292438233=%20%20Foundation%20models%20have%20emerged%20as%20a%20promising%20approach%20in%20time%20series%0Aforecasting%20%28TSF%29.%20Existing%20approaches%20either%20repurpose%20large%20language%20models%0A%28LLMs%29%20or%20build%20large-scale%20time%20series%20datasets%20to%20develop%20TSF%20foundation%0Amodels%20for%20universal%20forecasting.%20However%2C%20these%20methods%20face%20challenges%20due%20to%0Athe%20severe%20cross-domain%20gap%20or%20in-domain%20heterogeneity.%20This%20paper%20explores%20a%0Anew%20road%20to%20building%20a%20TSF%20foundation%20model%20from%20rich%2C%20high-quality%20natural%0Aimages.%20Our%20key%20insight%20is%20that%20a%20visual%20masked%20autoencoder%2C%20pre-trained%20on%20the%0AImageNet%20dataset%2C%20can%20naturally%20be%20a%20numeric%20series%20forecaster.%20By%0Areformulating%20TSF%20as%20an%20image%20reconstruction%20task%2C%20we%20bridge%20the%20gap%20between%0Aimage%20pre-training%20and%20TSF%20downstream%20tasks.%20Surprisingly%2C%20without%20further%0Aadaptation%20in%20the%20time%20series%20domain%2C%20the%20proposed%20VisionTS%20could%20achieve%0Abetter%20zero-shot%20forecast%20performance%20than%20existing%20TSF%20foundation%20models.%20With%0Afine-tuning%20for%20one%20epoch%2C%20VisionTS%20could%20further%20improve%20the%20forecasting%20and%0Aachieve%20state-of-the-art%20performance%20in%20most%20cases.%20Extensive%20experiments%0Areveal%20intrinsic%20similarities%20between%20images%20and%20real-world%20time%20series%2C%0Asuggesting%20that%20visual%20models%20may%20offer%20a%20%22free%20lunch%22%20for%20TSF%20and%20highlight%0Athe%20potential%20for%20future%20cross-modality%20research.%20Our%20code%20is%20publicly%0Aavailable%20at%20https%3A//github.com/Keytoyze/VisionTS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17253v4&entry.124074799=Read"},
{"title": "Marginalizing and Conditioning Gaussians onto Linear Approximations of\n  Smooth Manifolds with Applications in Robotics", "author": "Zi Cong Guo and James R. Forbes and Timothy D. Barfoot", "abstract": "  We present closed-form expressions for marginalizing and conditioning\nGaussians onto linear manifolds, and demonstrate how to apply these expressions\nto smooth nonlinear manifolds through linearization. Although marginalization\nand conditioning onto axis-aligned manifolds are well-established procedures,\ndoing so onto non-axis-aligned manifolds is not as well understood. We\ndemonstrate the utility of our expressions through three applications: 1)\napproximation of the projected normal distribution, where the quality of our\nlinearized approximation increases as problem nonlinearity decreases; 2)\ncovariance extraction in Koopman SLAM, where our covariances are shown to be\nconsistent on a real-world dataset; and 3) covariance extraction in constrained\nGTSAM, where our covariances are shown to be consistent in simulation.\n", "link": "http://arxiv.org/abs/2409.09871v3", "date": "2025-06-06", "relevancy": 2.1921, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5713}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5319}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Marginalizing%20and%20Conditioning%20Gaussians%20onto%20Linear%20Approximations%20of%0A%20%20Smooth%20Manifolds%20with%20Applications%20in%20Robotics&body=Title%3A%20Marginalizing%20and%20Conditioning%20Gaussians%20onto%20Linear%20Approximations%20of%0A%20%20Smooth%20Manifolds%20with%20Applications%20in%20Robotics%0AAuthor%3A%20Zi%20Cong%20Guo%20and%20James%20R.%20Forbes%20and%20Timothy%20D.%20Barfoot%0AAbstract%3A%20%20%20We%20present%20closed-form%20expressions%20for%20marginalizing%20and%20conditioning%0AGaussians%20onto%20linear%20manifolds%2C%20and%20demonstrate%20how%20to%20apply%20these%20expressions%0Ato%20smooth%20nonlinear%20manifolds%20through%20linearization.%20Although%20marginalization%0Aand%20conditioning%20onto%20axis-aligned%20manifolds%20are%20well-established%20procedures%2C%0Adoing%20so%20onto%20non-axis-aligned%20manifolds%20is%20not%20as%20well%20understood.%20We%0Ademonstrate%20the%20utility%20of%20our%20expressions%20through%20three%20applications%3A%201%29%0Aapproximation%20of%20the%20projected%20normal%20distribution%2C%20where%20the%20quality%20of%20our%0Alinearized%20approximation%20increases%20as%20problem%20nonlinearity%20decreases%3B%202%29%0Acovariance%20extraction%20in%20Koopman%20SLAM%2C%20where%20our%20covariances%20are%20shown%20to%20be%0Aconsistent%20on%20a%20real-world%20dataset%3B%20and%203%29%20covariance%20extraction%20in%20constrained%0AGTSAM%2C%20where%20our%20covariances%20are%20shown%20to%20be%20consistent%20in%20simulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09871v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMarginalizing%2520and%2520Conditioning%2520Gaussians%2520onto%2520Linear%2520Approximations%2520of%250A%2520%2520Smooth%2520Manifolds%2520with%2520Applications%2520in%2520Robotics%26entry.906535625%3DZi%2520Cong%2520Guo%2520and%2520James%2520R.%2520Forbes%2520and%2520Timothy%2520D.%2520Barfoot%26entry.1292438233%3D%2520%2520We%2520present%2520closed-form%2520expressions%2520for%2520marginalizing%2520and%2520conditioning%250AGaussians%2520onto%2520linear%2520manifolds%252C%2520and%2520demonstrate%2520how%2520to%2520apply%2520these%2520expressions%250Ato%2520smooth%2520nonlinear%2520manifolds%2520through%2520linearization.%2520Although%2520marginalization%250Aand%2520conditioning%2520onto%2520axis-aligned%2520manifolds%2520are%2520well-established%2520procedures%252C%250Adoing%2520so%2520onto%2520non-axis-aligned%2520manifolds%2520is%2520not%2520as%2520well%2520understood.%2520We%250Ademonstrate%2520the%2520utility%2520of%2520our%2520expressions%2520through%2520three%2520applications%253A%25201%2529%250Aapproximation%2520of%2520the%2520projected%2520normal%2520distribution%252C%2520where%2520the%2520quality%2520of%2520our%250Alinearized%2520approximation%2520increases%2520as%2520problem%2520nonlinearity%2520decreases%253B%25202%2529%250Acovariance%2520extraction%2520in%2520Koopman%2520SLAM%252C%2520where%2520our%2520covariances%2520are%2520shown%2520to%2520be%250Aconsistent%2520on%2520a%2520real-world%2520dataset%253B%2520and%25203%2529%2520covariance%2520extraction%2520in%2520constrained%250AGTSAM%252C%2520where%2520our%2520covariances%2520are%2520shown%2520to%2520be%2520consistent%2520in%2520simulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09871v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Marginalizing%20and%20Conditioning%20Gaussians%20onto%20Linear%20Approximations%20of%0A%20%20Smooth%20Manifolds%20with%20Applications%20in%20Robotics&entry.906535625=Zi%20Cong%20Guo%20and%20James%20R.%20Forbes%20and%20Timothy%20D.%20Barfoot&entry.1292438233=%20%20We%20present%20closed-form%20expressions%20for%20marginalizing%20and%20conditioning%0AGaussians%20onto%20linear%20manifolds%2C%20and%20demonstrate%20how%20to%20apply%20these%20expressions%0Ato%20smooth%20nonlinear%20manifolds%20through%20linearization.%20Although%20marginalization%0Aand%20conditioning%20onto%20axis-aligned%20manifolds%20are%20well-established%20procedures%2C%0Adoing%20so%20onto%20non-axis-aligned%20manifolds%20is%20not%20as%20well%20understood.%20We%0Ademonstrate%20the%20utility%20of%20our%20expressions%20through%20three%20applications%3A%201%29%0Aapproximation%20of%20the%20projected%20normal%20distribution%2C%20where%20the%20quality%20of%20our%0Alinearized%20approximation%20increases%20as%20problem%20nonlinearity%20decreases%3B%202%29%0Acovariance%20extraction%20in%20Koopman%20SLAM%2C%20where%20our%20covariances%20are%20shown%20to%20be%0Aconsistent%20on%20a%20real-world%20dataset%3B%20and%203%29%20covariance%20extraction%20in%20constrained%0AGTSAM%2C%20where%20our%20covariances%20are%20shown%20to%20be%20consistent%20in%20simulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09871v3&entry.124074799=Read"},
{"title": "semantic-features: A User-Friendly Tool for Studying Contextual Word\n  Embeddings in Interpretable Semantic Spaces", "author": "Jwalanthi Ranganathan and Rohan Jha and Kanishka Misra and Kyle Mahowald", "abstract": "  We introduce semantic-features, an extensible, easy-to-use library based on\nChronis et al. (2023) for studying contextualized word embeddings of LMs by\nprojecting them into interpretable spaces. We apply this tool in an experiment\nwhere we measure the contextual effect of the choice of dative construction\n(prepositional or double object) on the semantic interpretation of utterances\n(Bresnan, 2007). Specifically, we test whether \"London\" in \"I sent London the\nletter.\" is more likely to be interpreted as an animate referent (e.g., as the\nname of a person) than in \"I sent the letter to London.\" To this end, we devise\na dataset of 450 sentence pairs, one in each dative construction, with\nrecipients being ambiguous with respect to person-hood vs. place-hood. By\napplying semantic-features, we show that the contextualized word embeddings of\nthree masked language models show the expected sensitivities. This leaves us\noptimistic about the usefulness of our tool.\n", "link": "http://arxiv.org/abs/2506.06169v1", "date": "2025-06-06", "relevancy": 2.1903, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5545}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5545}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20semantic-features%3A%20A%20User-Friendly%20Tool%20for%20Studying%20Contextual%20Word%0A%20%20Embeddings%20in%20Interpretable%20Semantic%20Spaces&body=Title%3A%20semantic-features%3A%20A%20User-Friendly%20Tool%20for%20Studying%20Contextual%20Word%0A%20%20Embeddings%20in%20Interpretable%20Semantic%20Spaces%0AAuthor%3A%20Jwalanthi%20Ranganathan%20and%20Rohan%20Jha%20and%20Kanishka%20Misra%20and%20Kyle%20Mahowald%0AAbstract%3A%20%20%20We%20introduce%20semantic-features%2C%20an%20extensible%2C%20easy-to-use%20library%20based%20on%0AChronis%20et%20al.%20%282023%29%20for%20studying%20contextualized%20word%20embeddings%20of%20LMs%20by%0Aprojecting%20them%20into%20interpretable%20spaces.%20We%20apply%20this%20tool%20in%20an%20experiment%0Awhere%20we%20measure%20the%20contextual%20effect%20of%20the%20choice%20of%20dative%20construction%0A%28prepositional%20or%20double%20object%29%20on%20the%20semantic%20interpretation%20of%20utterances%0A%28Bresnan%2C%202007%29.%20Specifically%2C%20we%20test%20whether%20%22London%22%20in%20%22I%20sent%20London%20the%0Aletter.%22%20is%20more%20likely%20to%20be%20interpreted%20as%20an%20animate%20referent%20%28e.g.%2C%20as%20the%0Aname%20of%20a%20person%29%20than%20in%20%22I%20sent%20the%20letter%20to%20London.%22%20To%20this%20end%2C%20we%20devise%0Aa%20dataset%20of%20450%20sentence%20pairs%2C%20one%20in%20each%20dative%20construction%2C%20with%0Arecipients%20being%20ambiguous%20with%20respect%20to%20person-hood%20vs.%20place-hood.%20By%0Aapplying%20semantic-features%2C%20we%20show%20that%20the%20contextualized%20word%20embeddings%20of%0Athree%20masked%20language%20models%20show%20the%20expected%20sensitivities.%20This%20leaves%20us%0Aoptimistic%20about%20the%20usefulness%20of%20our%20tool.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06169v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dsemantic-features%253A%2520A%2520User-Friendly%2520Tool%2520for%2520Studying%2520Contextual%2520Word%250A%2520%2520Embeddings%2520in%2520Interpretable%2520Semantic%2520Spaces%26entry.906535625%3DJwalanthi%2520Ranganathan%2520and%2520Rohan%2520Jha%2520and%2520Kanishka%2520Misra%2520and%2520Kyle%2520Mahowald%26entry.1292438233%3D%2520%2520We%2520introduce%2520semantic-features%252C%2520an%2520extensible%252C%2520easy-to-use%2520library%2520based%2520on%250AChronis%2520et%2520al.%2520%25282023%2529%2520for%2520studying%2520contextualized%2520word%2520embeddings%2520of%2520LMs%2520by%250Aprojecting%2520them%2520into%2520interpretable%2520spaces.%2520We%2520apply%2520this%2520tool%2520in%2520an%2520experiment%250Awhere%2520we%2520measure%2520the%2520contextual%2520effect%2520of%2520the%2520choice%2520of%2520dative%2520construction%250A%2528prepositional%2520or%2520double%2520object%2529%2520on%2520the%2520semantic%2520interpretation%2520of%2520utterances%250A%2528Bresnan%252C%25202007%2529.%2520Specifically%252C%2520we%2520test%2520whether%2520%2522London%2522%2520in%2520%2522I%2520sent%2520London%2520the%250Aletter.%2522%2520is%2520more%2520likely%2520to%2520be%2520interpreted%2520as%2520an%2520animate%2520referent%2520%2528e.g.%252C%2520as%2520the%250Aname%2520of%2520a%2520person%2529%2520than%2520in%2520%2522I%2520sent%2520the%2520letter%2520to%2520London.%2522%2520To%2520this%2520end%252C%2520we%2520devise%250Aa%2520dataset%2520of%2520450%2520sentence%2520pairs%252C%2520one%2520in%2520each%2520dative%2520construction%252C%2520with%250Arecipients%2520being%2520ambiguous%2520with%2520respect%2520to%2520person-hood%2520vs.%2520place-hood.%2520By%250Aapplying%2520semantic-features%252C%2520we%2520show%2520that%2520the%2520contextualized%2520word%2520embeddings%2520of%250Athree%2520masked%2520language%2520models%2520show%2520the%2520expected%2520sensitivities.%2520This%2520leaves%2520us%250Aoptimistic%2520about%2520the%2520usefulness%2520of%2520our%2520tool.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06169v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=semantic-features%3A%20A%20User-Friendly%20Tool%20for%20Studying%20Contextual%20Word%0A%20%20Embeddings%20in%20Interpretable%20Semantic%20Spaces&entry.906535625=Jwalanthi%20Ranganathan%20and%20Rohan%20Jha%20and%20Kanishka%20Misra%20and%20Kyle%20Mahowald&entry.1292438233=%20%20We%20introduce%20semantic-features%2C%20an%20extensible%2C%20easy-to-use%20library%20based%20on%0AChronis%20et%20al.%20%282023%29%20for%20studying%20contextualized%20word%20embeddings%20of%20LMs%20by%0Aprojecting%20them%20into%20interpretable%20spaces.%20We%20apply%20this%20tool%20in%20an%20experiment%0Awhere%20we%20measure%20the%20contextual%20effect%20of%20the%20choice%20of%20dative%20construction%0A%28prepositional%20or%20double%20object%29%20on%20the%20semantic%20interpretation%20of%20utterances%0A%28Bresnan%2C%202007%29.%20Specifically%2C%20we%20test%20whether%20%22London%22%20in%20%22I%20sent%20London%20the%0Aletter.%22%20is%20more%20likely%20to%20be%20interpreted%20as%20an%20animate%20referent%20%28e.g.%2C%20as%20the%0Aname%20of%20a%20person%29%20than%20in%20%22I%20sent%20the%20letter%20to%20London.%22%20To%20this%20end%2C%20we%20devise%0Aa%20dataset%20of%20450%20sentence%20pairs%2C%20one%20in%20each%20dative%20construction%2C%20with%0Arecipients%20being%20ambiguous%20with%20respect%20to%20person-hood%20vs.%20place-hood.%20By%0Aapplying%20semantic-features%2C%20we%20show%20that%20the%20contextualized%20word%20embeddings%20of%0Athree%20masked%20language%20models%20show%20the%20expected%20sensitivities.%20This%20leaves%20us%0Aoptimistic%20about%20the%20usefulness%20of%20our%20tool.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06169v1&entry.124074799=Read"},
{"title": "Joint-GCG: Unified Gradient-Based Poisoning Attacks on\n  Retrieval-Augmented Generation Systems", "author": "Haowei Wang and Rupeng Zhang and Junjie Wang and Mingyang Li and Yuekai Huang and Dandan Wang and Qing Wang", "abstract": "  Retrieval-Augmented Generation (RAG) systems enhance Large Language Models\n(LLMs) by retrieving relevant documents from external corpora before generating\nresponses. This approach significantly expands LLM capabilities by leveraging\nvast, up-to-date external knowledge. However, this reliance on external\nknowledge makes RAG systems vulnerable to corpus poisoning attacks that\nmanipulate generated outputs via poisoned document injection. Existing\npoisoning attack strategies typically treat the retrieval and generation stages\nas disjointed, limiting their effectiveness. We propose Joint-GCG, the first\nframework to unify gradient-based attacks across both retriever and generator\nmodels through three innovations: (1) Cross-Vocabulary Projection for aligning\nembedding spaces, (2) Gradient Tokenization Alignment for synchronizing\ntoken-level gradient signals, and (3) Adaptive Weighted Fusion for dynamically\nbalancing attacking objectives. Evaluations demonstrate that Joint-GCG achieves\nat most 25% and an average of 5% higher attack success rate than previous\nmethods across multiple retrievers and generators. While optimized under a\nwhite-box assumption, the generated poisons show unprecedented transferability\nto unseen models. Joint-GCG's innovative unification of gradient-based attacks\nacross retrieval and generation stages fundamentally reshapes our understanding\nof vulnerabilities within RAG systems. Our code is available at\nhttps://github.com/NicerWang/Joint-GCG.\n", "link": "http://arxiv.org/abs/2506.06151v1", "date": "2025-06-06", "relevancy": 2.1726, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5944}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5382}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint-GCG%3A%20Unified%20Gradient-Based%20Poisoning%20Attacks%20on%0A%20%20Retrieval-Augmented%20Generation%20Systems&body=Title%3A%20Joint-GCG%3A%20Unified%20Gradient-Based%20Poisoning%20Attacks%20on%0A%20%20Retrieval-Augmented%20Generation%20Systems%0AAuthor%3A%20Haowei%20Wang%20and%20Rupeng%20Zhang%20and%20Junjie%20Wang%20and%20Mingyang%20Li%20and%20Yuekai%20Huang%20and%20Dandan%20Wang%20and%20Qing%20Wang%0AAbstract%3A%20%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20systems%20enhance%20Large%20Language%20Models%0A%28LLMs%29%20by%20retrieving%20relevant%20documents%20from%20external%20corpora%20before%20generating%0Aresponses.%20This%20approach%20significantly%20expands%20LLM%20capabilities%20by%20leveraging%0Avast%2C%20up-to-date%20external%20knowledge.%20However%2C%20this%20reliance%20on%20external%0Aknowledge%20makes%20RAG%20systems%20vulnerable%20to%20corpus%20poisoning%20attacks%20that%0Amanipulate%20generated%20outputs%20via%20poisoned%20document%20injection.%20Existing%0Apoisoning%20attack%20strategies%20typically%20treat%20the%20retrieval%20and%20generation%20stages%0Aas%20disjointed%2C%20limiting%20their%20effectiveness.%20We%20propose%20Joint-GCG%2C%20the%20first%0Aframework%20to%20unify%20gradient-based%20attacks%20across%20both%20retriever%20and%20generator%0Amodels%20through%20three%20innovations%3A%20%281%29%20Cross-Vocabulary%20Projection%20for%20aligning%0Aembedding%20spaces%2C%20%282%29%20Gradient%20Tokenization%20Alignment%20for%20synchronizing%0Atoken-level%20gradient%20signals%2C%20and%20%283%29%20Adaptive%20Weighted%20Fusion%20for%20dynamically%0Abalancing%20attacking%20objectives.%20Evaluations%20demonstrate%20that%20Joint-GCG%20achieves%0Aat%20most%2025%25%20and%20an%20average%20of%205%25%20higher%20attack%20success%20rate%20than%20previous%0Amethods%20across%20multiple%20retrievers%20and%20generators.%20While%20optimized%20under%20a%0Awhite-box%20assumption%2C%20the%20generated%20poisons%20show%20unprecedented%20transferability%0Ato%20unseen%20models.%20Joint-GCG%27s%20innovative%20unification%20of%20gradient-based%20attacks%0Aacross%20retrieval%20and%20generation%20stages%20fundamentally%20reshapes%20our%20understanding%0Aof%20vulnerabilities%20within%20RAG%20systems.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/NicerWang/Joint-GCG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06151v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint-GCG%253A%2520Unified%2520Gradient-Based%2520Poisoning%2520Attacks%2520on%250A%2520%2520Retrieval-Augmented%2520Generation%2520Systems%26entry.906535625%3DHaowei%2520Wang%2520and%2520Rupeng%2520Zhang%2520and%2520Junjie%2520Wang%2520and%2520Mingyang%2520Li%2520and%2520Yuekai%2520Huang%2520and%2520Dandan%2520Wang%2520and%2520Qing%2520Wang%26entry.1292438233%3D%2520%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520systems%2520enhance%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520by%2520retrieving%2520relevant%2520documents%2520from%2520external%2520corpora%2520before%2520generating%250Aresponses.%2520This%2520approach%2520significantly%2520expands%2520LLM%2520capabilities%2520by%2520leveraging%250Avast%252C%2520up-to-date%2520external%2520knowledge.%2520However%252C%2520this%2520reliance%2520on%2520external%250Aknowledge%2520makes%2520RAG%2520systems%2520vulnerable%2520to%2520corpus%2520poisoning%2520attacks%2520that%250Amanipulate%2520generated%2520outputs%2520via%2520poisoned%2520document%2520injection.%2520Existing%250Apoisoning%2520attack%2520strategies%2520typically%2520treat%2520the%2520retrieval%2520and%2520generation%2520stages%250Aas%2520disjointed%252C%2520limiting%2520their%2520effectiveness.%2520We%2520propose%2520Joint-GCG%252C%2520the%2520first%250Aframework%2520to%2520unify%2520gradient-based%2520attacks%2520across%2520both%2520retriever%2520and%2520generator%250Amodels%2520through%2520three%2520innovations%253A%2520%25281%2529%2520Cross-Vocabulary%2520Projection%2520for%2520aligning%250Aembedding%2520spaces%252C%2520%25282%2529%2520Gradient%2520Tokenization%2520Alignment%2520for%2520synchronizing%250Atoken-level%2520gradient%2520signals%252C%2520and%2520%25283%2529%2520Adaptive%2520Weighted%2520Fusion%2520for%2520dynamically%250Abalancing%2520attacking%2520objectives.%2520Evaluations%2520demonstrate%2520that%2520Joint-GCG%2520achieves%250Aat%2520most%252025%2525%2520and%2520an%2520average%2520of%25205%2525%2520higher%2520attack%2520success%2520rate%2520than%2520previous%250Amethods%2520across%2520multiple%2520retrievers%2520and%2520generators.%2520While%2520optimized%2520under%2520a%250Awhite-box%2520assumption%252C%2520the%2520generated%2520poisons%2520show%2520unprecedented%2520transferability%250Ato%2520unseen%2520models.%2520Joint-GCG%2527s%2520innovative%2520unification%2520of%2520gradient-based%2520attacks%250Aacross%2520retrieval%2520and%2520generation%2520stages%2520fundamentally%2520reshapes%2520our%2520understanding%250Aof%2520vulnerabilities%2520within%2520RAG%2520systems.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/NicerWang/Joint-GCG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06151v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint-GCG%3A%20Unified%20Gradient-Based%20Poisoning%20Attacks%20on%0A%20%20Retrieval-Augmented%20Generation%20Systems&entry.906535625=Haowei%20Wang%20and%20Rupeng%20Zhang%20and%20Junjie%20Wang%20and%20Mingyang%20Li%20and%20Yuekai%20Huang%20and%20Dandan%20Wang%20and%20Qing%20Wang&entry.1292438233=%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20systems%20enhance%20Large%20Language%20Models%0A%28LLMs%29%20by%20retrieving%20relevant%20documents%20from%20external%20corpora%20before%20generating%0Aresponses.%20This%20approach%20significantly%20expands%20LLM%20capabilities%20by%20leveraging%0Avast%2C%20up-to-date%20external%20knowledge.%20However%2C%20this%20reliance%20on%20external%0Aknowledge%20makes%20RAG%20systems%20vulnerable%20to%20corpus%20poisoning%20attacks%20that%0Amanipulate%20generated%20outputs%20via%20poisoned%20document%20injection.%20Existing%0Apoisoning%20attack%20strategies%20typically%20treat%20the%20retrieval%20and%20generation%20stages%0Aas%20disjointed%2C%20limiting%20their%20effectiveness.%20We%20propose%20Joint-GCG%2C%20the%20first%0Aframework%20to%20unify%20gradient-based%20attacks%20across%20both%20retriever%20and%20generator%0Amodels%20through%20three%20innovations%3A%20%281%29%20Cross-Vocabulary%20Projection%20for%20aligning%0Aembedding%20spaces%2C%20%282%29%20Gradient%20Tokenization%20Alignment%20for%20synchronizing%0Atoken-level%20gradient%20signals%2C%20and%20%283%29%20Adaptive%20Weighted%20Fusion%20for%20dynamically%0Abalancing%20attacking%20objectives.%20Evaluations%20demonstrate%20that%20Joint-GCG%20achieves%0Aat%20most%2025%25%20and%20an%20average%20of%205%25%20higher%20attack%20success%20rate%20than%20previous%0Amethods%20across%20multiple%20retrievers%20and%20generators.%20While%20optimized%20under%20a%0Awhite-box%20assumption%2C%20the%20generated%20poisons%20show%20unprecedented%20transferability%0Ato%20unseen%20models.%20Joint-GCG%27s%20innovative%20unification%20of%20gradient-based%20attacks%0Aacross%20retrieval%20and%20generation%20stages%20fundamentally%20reshapes%20our%20understanding%0Aof%20vulnerabilities%20within%20RAG%20systems.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/NicerWang/Joint-GCG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06151v1&entry.124074799=Read"},
{"title": "Movie Facts and Fibs (MF$^2$): A Benchmark for Long Movie Understanding", "author": "Emmanouil Zaranis and Ant\u00f3nio Farinhas and Saul Santos and Beatriz Canaverde and Miguel Moura Ramos and Aditya K Surikuchi and Andr\u00e9 Viveiros and Baohao Liao and Elena Bueno-Benito and Nithin Sivakumaran and Pavlo Vasylenko and Shoubin Yu and Sonal Sannigrahi and Wafaa Mohammed and Ben Peters and Danae S\u00e1nchez Villegas and Elias Stengel-Eskin and Giuseppe Attanasio and Jaehong Yoon and Stella Frank and Alessandro Suglia and Chrysoula Zerva and Desmond Elliott and Mariella Dimiccoli and Mohit Bansal and Oswald Lanz and Raffaella Bernardi and Raquel Fern\u00e1ndez and Sandro Pezzelle and Vlad Niculae and Andr\u00e9 F. T. Martins", "abstract": "  Despite recent progress in vision-language models (VLMs), holistic\nunderstanding of long-form video content remains a significant challenge,\npartly due to limitations in current benchmarks. Many focus on peripheral,\n``needle-in-a-haystack'' details, encouraging context-insensitive retrieval\nover deep comprehension. Others rely on large-scale, semi-automatically\ngenerated questions (often produced by language models themselves) that are\neasier for models to answer but fail to reflect genuine understanding. In this\npaper, we introduce MF$^2$, a new benchmark for evaluating whether models can\ncomprehend, consolidate, and recall key narrative information from full-length\nmovies (50-170 minutes long). MF$^2$ includes over 50 full-length,\nopen-licensed movies, each paired with manually constructed sets of claim pairs\n-- one true (fact) and one plausible but false (fib), totalling over 850 pairs.\nThese claims target core narrative elements such as character motivations and\nemotions, causal chains, and event order, and refer to memorable moments that\nhumans can recall without rewatching the movie. Instead of multiple-choice\nformats, we adopt a binary claim evaluation protocol: for each pair, models\nmust correctly identify both the true and false claims. This reduces biases\nlike answer ordering and enables a more precise assessment of reasoning. Our\nexperiments demonstrate that both open-weight and closed state-of-the-art\nmodels fall well short of human performance, underscoring the relative ease of\nthe task for humans and their superior ability to retain and reason over\ncritical narrative information -- an ability current VLMs lack.\n", "link": "http://arxiv.org/abs/2506.06275v1", "date": "2025-06-06", "relevancy": 2.1699, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5585}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5585}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Movie%20Facts%20and%20Fibs%20%28MF%24%5E2%24%29%3A%20A%20Benchmark%20for%20Long%20Movie%20Understanding&body=Title%3A%20Movie%20Facts%20and%20Fibs%20%28MF%24%5E2%24%29%3A%20A%20Benchmark%20for%20Long%20Movie%20Understanding%0AAuthor%3A%20Emmanouil%20Zaranis%20and%20Ant%C3%B3nio%20Farinhas%20and%20Saul%20Santos%20and%20Beatriz%20Canaverde%20and%20Miguel%20Moura%20Ramos%20and%20Aditya%20K%20Surikuchi%20and%20Andr%C3%A9%20Viveiros%20and%20Baohao%20Liao%20and%20Elena%20Bueno-Benito%20and%20Nithin%20Sivakumaran%20and%20Pavlo%20Vasylenko%20and%20Shoubin%20Yu%20and%20Sonal%20Sannigrahi%20and%20Wafaa%20Mohammed%20and%20Ben%20Peters%20and%20Danae%20S%C3%A1nchez%20Villegas%20and%20Elias%20Stengel-Eskin%20and%20Giuseppe%20Attanasio%20and%20Jaehong%20Yoon%20and%20Stella%20Frank%20and%20Alessandro%20Suglia%20and%20Chrysoula%20Zerva%20and%20Desmond%20Elliott%20and%20Mariella%20Dimiccoli%20and%20Mohit%20Bansal%20and%20Oswald%20Lanz%20and%20Raffaella%20Bernardi%20and%20Raquel%20Fern%C3%A1ndez%20and%20Sandro%20Pezzelle%20and%20Vlad%20Niculae%20and%20Andr%C3%A9%20F.%20T.%20Martins%0AAbstract%3A%20%20%20Despite%20recent%20progress%20in%20vision-language%20models%20%28VLMs%29%2C%20holistic%0Aunderstanding%20of%20long-form%20video%20content%20remains%20a%20significant%20challenge%2C%0Apartly%20due%20to%20limitations%20in%20current%20benchmarks.%20Many%20focus%20on%20peripheral%2C%0A%60%60needle-in-a-haystack%27%27%20details%2C%20encouraging%20context-insensitive%20retrieval%0Aover%20deep%20comprehension.%20Others%20rely%20on%20large-scale%2C%20semi-automatically%0Agenerated%20questions%20%28often%20produced%20by%20language%20models%20themselves%29%20that%20are%0Aeasier%20for%20models%20to%20answer%20but%20fail%20to%20reflect%20genuine%20understanding.%20In%20this%0Apaper%2C%20we%20introduce%20MF%24%5E2%24%2C%20a%20new%20benchmark%20for%20evaluating%20whether%20models%20can%0Acomprehend%2C%20consolidate%2C%20and%20recall%20key%20narrative%20information%20from%20full-length%0Amovies%20%2850-170%20minutes%20long%29.%20MF%24%5E2%24%20includes%20over%2050%20full-length%2C%0Aopen-licensed%20movies%2C%20each%20paired%20with%20manually%20constructed%20sets%20of%20claim%20pairs%0A--%20one%20true%20%28fact%29%20and%20one%20plausible%20but%20false%20%28fib%29%2C%20totalling%20over%20850%20pairs.%0AThese%20claims%20target%20core%20narrative%20elements%20such%20as%20character%20motivations%20and%0Aemotions%2C%20causal%20chains%2C%20and%20event%20order%2C%20and%20refer%20to%20memorable%20moments%20that%0Ahumans%20can%20recall%20without%20rewatching%20the%20movie.%20Instead%20of%20multiple-choice%0Aformats%2C%20we%20adopt%20a%20binary%20claim%20evaluation%20protocol%3A%20for%20each%20pair%2C%20models%0Amust%20correctly%20identify%20both%20the%20true%20and%20false%20claims.%20This%20reduces%20biases%0Alike%20answer%20ordering%20and%20enables%20a%20more%20precise%20assessment%20of%20reasoning.%20Our%0Aexperiments%20demonstrate%20that%20both%20open-weight%20and%20closed%20state-of-the-art%0Amodels%20fall%20well%20short%20of%20human%20performance%2C%20underscoring%20the%20relative%20ease%20of%0Athe%20task%20for%20humans%20and%20their%20superior%20ability%20to%20retain%20and%20reason%20over%0Acritical%20narrative%20information%20--%20an%20ability%20current%20VLMs%20lack.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06275v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMovie%2520Facts%2520and%2520Fibs%2520%2528MF%2524%255E2%2524%2529%253A%2520A%2520Benchmark%2520for%2520Long%2520Movie%2520Understanding%26entry.906535625%3DEmmanouil%2520Zaranis%2520and%2520Ant%25C3%25B3nio%2520Farinhas%2520and%2520Saul%2520Santos%2520and%2520Beatriz%2520Canaverde%2520and%2520Miguel%2520Moura%2520Ramos%2520and%2520Aditya%2520K%2520Surikuchi%2520and%2520Andr%25C3%25A9%2520Viveiros%2520and%2520Baohao%2520Liao%2520and%2520Elena%2520Bueno-Benito%2520and%2520Nithin%2520Sivakumaran%2520and%2520Pavlo%2520Vasylenko%2520and%2520Shoubin%2520Yu%2520and%2520Sonal%2520Sannigrahi%2520and%2520Wafaa%2520Mohammed%2520and%2520Ben%2520Peters%2520and%2520Danae%2520S%25C3%25A1nchez%2520Villegas%2520and%2520Elias%2520Stengel-Eskin%2520and%2520Giuseppe%2520Attanasio%2520and%2520Jaehong%2520Yoon%2520and%2520Stella%2520Frank%2520and%2520Alessandro%2520Suglia%2520and%2520Chrysoula%2520Zerva%2520and%2520Desmond%2520Elliott%2520and%2520Mariella%2520Dimiccoli%2520and%2520Mohit%2520Bansal%2520and%2520Oswald%2520Lanz%2520and%2520Raffaella%2520Bernardi%2520and%2520Raquel%2520Fern%25C3%25A1ndez%2520and%2520Sandro%2520Pezzelle%2520and%2520Vlad%2520Niculae%2520and%2520Andr%25C3%25A9%2520F.%2520T.%2520Martins%26entry.1292438233%3D%2520%2520Despite%2520recent%2520progress%2520in%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520holistic%250Aunderstanding%2520of%2520long-form%2520video%2520content%2520remains%2520a%2520significant%2520challenge%252C%250Apartly%2520due%2520to%2520limitations%2520in%2520current%2520benchmarks.%2520Many%2520focus%2520on%2520peripheral%252C%250A%2560%2560needle-in-a-haystack%2527%2527%2520details%252C%2520encouraging%2520context-insensitive%2520retrieval%250Aover%2520deep%2520comprehension.%2520Others%2520rely%2520on%2520large-scale%252C%2520semi-automatically%250Agenerated%2520questions%2520%2528often%2520produced%2520by%2520language%2520models%2520themselves%2529%2520that%2520are%250Aeasier%2520for%2520models%2520to%2520answer%2520but%2520fail%2520to%2520reflect%2520genuine%2520understanding.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520MF%2524%255E2%2524%252C%2520a%2520new%2520benchmark%2520for%2520evaluating%2520whether%2520models%2520can%250Acomprehend%252C%2520consolidate%252C%2520and%2520recall%2520key%2520narrative%2520information%2520from%2520full-length%250Amovies%2520%252850-170%2520minutes%2520long%2529.%2520MF%2524%255E2%2524%2520includes%2520over%252050%2520full-length%252C%250Aopen-licensed%2520movies%252C%2520each%2520paired%2520with%2520manually%2520constructed%2520sets%2520of%2520claim%2520pairs%250A--%2520one%2520true%2520%2528fact%2529%2520and%2520one%2520plausible%2520but%2520false%2520%2528fib%2529%252C%2520totalling%2520over%2520850%2520pairs.%250AThese%2520claims%2520target%2520core%2520narrative%2520elements%2520such%2520as%2520character%2520motivations%2520and%250Aemotions%252C%2520causal%2520chains%252C%2520and%2520event%2520order%252C%2520and%2520refer%2520to%2520memorable%2520moments%2520that%250Ahumans%2520can%2520recall%2520without%2520rewatching%2520the%2520movie.%2520Instead%2520of%2520multiple-choice%250Aformats%252C%2520we%2520adopt%2520a%2520binary%2520claim%2520evaluation%2520protocol%253A%2520for%2520each%2520pair%252C%2520models%250Amust%2520correctly%2520identify%2520both%2520the%2520true%2520and%2520false%2520claims.%2520This%2520reduces%2520biases%250Alike%2520answer%2520ordering%2520and%2520enables%2520a%2520more%2520precise%2520assessment%2520of%2520reasoning.%2520Our%250Aexperiments%2520demonstrate%2520that%2520both%2520open-weight%2520and%2520closed%2520state-of-the-art%250Amodels%2520fall%2520well%2520short%2520of%2520human%2520performance%252C%2520underscoring%2520the%2520relative%2520ease%2520of%250Athe%2520task%2520for%2520humans%2520and%2520their%2520superior%2520ability%2520to%2520retain%2520and%2520reason%2520over%250Acritical%2520narrative%2520information%2520--%2520an%2520ability%2520current%2520VLMs%2520lack.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06275v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Movie%20Facts%20and%20Fibs%20%28MF%24%5E2%24%29%3A%20A%20Benchmark%20for%20Long%20Movie%20Understanding&entry.906535625=Emmanouil%20Zaranis%20and%20Ant%C3%B3nio%20Farinhas%20and%20Saul%20Santos%20and%20Beatriz%20Canaverde%20and%20Miguel%20Moura%20Ramos%20and%20Aditya%20K%20Surikuchi%20and%20Andr%C3%A9%20Viveiros%20and%20Baohao%20Liao%20and%20Elena%20Bueno-Benito%20and%20Nithin%20Sivakumaran%20and%20Pavlo%20Vasylenko%20and%20Shoubin%20Yu%20and%20Sonal%20Sannigrahi%20and%20Wafaa%20Mohammed%20and%20Ben%20Peters%20and%20Danae%20S%C3%A1nchez%20Villegas%20and%20Elias%20Stengel-Eskin%20and%20Giuseppe%20Attanasio%20and%20Jaehong%20Yoon%20and%20Stella%20Frank%20and%20Alessandro%20Suglia%20and%20Chrysoula%20Zerva%20and%20Desmond%20Elliott%20and%20Mariella%20Dimiccoli%20and%20Mohit%20Bansal%20and%20Oswald%20Lanz%20and%20Raffaella%20Bernardi%20and%20Raquel%20Fern%C3%A1ndez%20and%20Sandro%20Pezzelle%20and%20Vlad%20Niculae%20and%20Andr%C3%A9%20F.%20T.%20Martins&entry.1292438233=%20%20Despite%20recent%20progress%20in%20vision-language%20models%20%28VLMs%29%2C%20holistic%0Aunderstanding%20of%20long-form%20video%20content%20remains%20a%20significant%20challenge%2C%0Apartly%20due%20to%20limitations%20in%20current%20benchmarks.%20Many%20focus%20on%20peripheral%2C%0A%60%60needle-in-a-haystack%27%27%20details%2C%20encouraging%20context-insensitive%20retrieval%0Aover%20deep%20comprehension.%20Others%20rely%20on%20large-scale%2C%20semi-automatically%0Agenerated%20questions%20%28often%20produced%20by%20language%20models%20themselves%29%20that%20are%0Aeasier%20for%20models%20to%20answer%20but%20fail%20to%20reflect%20genuine%20understanding.%20In%20this%0Apaper%2C%20we%20introduce%20MF%24%5E2%24%2C%20a%20new%20benchmark%20for%20evaluating%20whether%20models%20can%0Acomprehend%2C%20consolidate%2C%20and%20recall%20key%20narrative%20information%20from%20full-length%0Amovies%20%2850-170%20minutes%20long%29.%20MF%24%5E2%24%20includes%20over%2050%20full-length%2C%0Aopen-licensed%20movies%2C%20each%20paired%20with%20manually%20constructed%20sets%20of%20claim%20pairs%0A--%20one%20true%20%28fact%29%20and%20one%20plausible%20but%20false%20%28fib%29%2C%20totalling%20over%20850%20pairs.%0AThese%20claims%20target%20core%20narrative%20elements%20such%20as%20character%20motivations%20and%0Aemotions%2C%20causal%20chains%2C%20and%20event%20order%2C%20and%20refer%20to%20memorable%20moments%20that%0Ahumans%20can%20recall%20without%20rewatching%20the%20movie.%20Instead%20of%20multiple-choice%0Aformats%2C%20we%20adopt%20a%20binary%20claim%20evaluation%20protocol%3A%20for%20each%20pair%2C%20models%0Amust%20correctly%20identify%20both%20the%20true%20and%20false%20claims.%20This%20reduces%20biases%0Alike%20answer%20ordering%20and%20enables%20a%20more%20precise%20assessment%20of%20reasoning.%20Our%0Aexperiments%20demonstrate%20that%20both%20open-weight%20and%20closed%20state-of-the-art%0Amodels%20fall%20well%20short%20of%20human%20performance%2C%20underscoring%20the%20relative%20ease%20of%0Athe%20task%20for%20humans%20and%20their%20superior%20ability%20to%20retain%20and%20reason%20over%0Acritical%20narrative%20information%20--%20an%20ability%20current%20VLMs%20lack.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06275v1&entry.124074799=Read"},
{"title": "Progressive Data Dropout: An Embarrassingly Simple Approach to Faster\n  Training", "author": "Shriram M S and Xinyue Hao and Shihao Hou and Yang Lu and Laura Sevilla-Lara and Anurag Arnab and Shreyank N Gowda", "abstract": "  The success of the machine learning field has reliably depended on training\non large datasets. While effective, this trend comes at an extraordinary cost.\nThis is due to two deeply intertwined factors: the size of models and the size\nof datasets. While promising research efforts focus on reducing the size of\nmodels, the other half of the equation remains fairly mysterious. Indeed, it is\nsurprising that the standard approach to training remains to iterate over and\nover, uniformly sampling the training dataset. In this paper we explore a\nseries of alternative training paradigms that leverage insights from\nhard-data-mining and dropout, simple enough to implement and use that can\nbecome the new training standard. The proposed Progressive Data Dropout reduces\nthe number of effective epochs to as little as 12.4% of the baseline. This\nsavings actually do not come at any cost for accuracy. Surprisingly, the\nproposed method improves accuracy by up to 4.82%. Our approach requires no\nchanges to model architecture or optimizer, and can be applied across standard\ntraining pipelines, thus posing an excellent opportunity for wide adoption.\nCode can be found here: https://github.com/bazyagami/LearningWithRevision\n", "link": "http://arxiv.org/abs/2505.22342v2", "date": "2025-06-06", "relevancy": 2.1659, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.566}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5314}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Progressive%20Data%20Dropout%3A%20An%20Embarrassingly%20Simple%20Approach%20to%20Faster%0A%20%20Training&body=Title%3A%20Progressive%20Data%20Dropout%3A%20An%20Embarrassingly%20Simple%20Approach%20to%20Faster%0A%20%20Training%0AAuthor%3A%20Shriram%20M%20S%20and%20Xinyue%20Hao%20and%20Shihao%20Hou%20and%20Yang%20Lu%20and%20Laura%20Sevilla-Lara%20and%20Anurag%20Arnab%20and%20Shreyank%20N%20Gowda%0AAbstract%3A%20%20%20The%20success%20of%20the%20machine%20learning%20field%20has%20reliably%20depended%20on%20training%0Aon%20large%20datasets.%20While%20effective%2C%20this%20trend%20comes%20at%20an%20extraordinary%20cost.%0AThis%20is%20due%20to%20two%20deeply%20intertwined%20factors%3A%20the%20size%20of%20models%20and%20the%20size%0Aof%20datasets.%20While%20promising%20research%20efforts%20focus%20on%20reducing%20the%20size%20of%0Amodels%2C%20the%20other%20half%20of%20the%20equation%20remains%20fairly%20mysterious.%20Indeed%2C%20it%20is%0Asurprising%20that%20the%20standard%20approach%20to%20training%20remains%20to%20iterate%20over%20and%0Aover%2C%20uniformly%20sampling%20the%20training%20dataset.%20In%20this%20paper%20we%20explore%20a%0Aseries%20of%20alternative%20training%20paradigms%20that%20leverage%20insights%20from%0Ahard-data-mining%20and%20dropout%2C%20simple%20enough%20to%20implement%20and%20use%20that%20can%0Abecome%20the%20new%20training%20standard.%20The%20proposed%20Progressive%20Data%20Dropout%20reduces%0Athe%20number%20of%20effective%20epochs%20to%20as%20little%20as%2012.4%25%20of%20the%20baseline.%20This%0Asavings%20actually%20do%20not%20come%20at%20any%20cost%20for%20accuracy.%20Surprisingly%2C%20the%0Aproposed%20method%20improves%20accuracy%20by%20up%20to%204.82%25.%20Our%20approach%20requires%20no%0Achanges%20to%20model%20architecture%20or%20optimizer%2C%20and%20can%20be%20applied%20across%20standard%0Atraining%20pipelines%2C%20thus%20posing%20an%20excellent%20opportunity%20for%20wide%20adoption.%0ACode%20can%20be%20found%20here%3A%20https%3A//github.com/bazyagami/LearningWithRevision%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22342v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgressive%2520Data%2520Dropout%253A%2520An%2520Embarrassingly%2520Simple%2520Approach%2520to%2520Faster%250A%2520%2520Training%26entry.906535625%3DShriram%2520M%2520S%2520and%2520Xinyue%2520Hao%2520and%2520Shihao%2520Hou%2520and%2520Yang%2520Lu%2520and%2520Laura%2520Sevilla-Lara%2520and%2520Anurag%2520Arnab%2520and%2520Shreyank%2520N%2520Gowda%26entry.1292438233%3D%2520%2520The%2520success%2520of%2520the%2520machine%2520learning%2520field%2520has%2520reliably%2520depended%2520on%2520training%250Aon%2520large%2520datasets.%2520While%2520effective%252C%2520this%2520trend%2520comes%2520at%2520an%2520extraordinary%2520cost.%250AThis%2520is%2520due%2520to%2520two%2520deeply%2520intertwined%2520factors%253A%2520the%2520size%2520of%2520models%2520and%2520the%2520size%250Aof%2520datasets.%2520While%2520promising%2520research%2520efforts%2520focus%2520on%2520reducing%2520the%2520size%2520of%250Amodels%252C%2520the%2520other%2520half%2520of%2520the%2520equation%2520remains%2520fairly%2520mysterious.%2520Indeed%252C%2520it%2520is%250Asurprising%2520that%2520the%2520standard%2520approach%2520to%2520training%2520remains%2520to%2520iterate%2520over%2520and%250Aover%252C%2520uniformly%2520sampling%2520the%2520training%2520dataset.%2520In%2520this%2520paper%2520we%2520explore%2520a%250Aseries%2520of%2520alternative%2520training%2520paradigms%2520that%2520leverage%2520insights%2520from%250Ahard-data-mining%2520and%2520dropout%252C%2520simple%2520enough%2520to%2520implement%2520and%2520use%2520that%2520can%250Abecome%2520the%2520new%2520training%2520standard.%2520The%2520proposed%2520Progressive%2520Data%2520Dropout%2520reduces%250Athe%2520number%2520of%2520effective%2520epochs%2520to%2520as%2520little%2520as%252012.4%2525%2520of%2520the%2520baseline.%2520This%250Asavings%2520actually%2520do%2520not%2520come%2520at%2520any%2520cost%2520for%2520accuracy.%2520Surprisingly%252C%2520the%250Aproposed%2520method%2520improves%2520accuracy%2520by%2520up%2520to%25204.82%2525.%2520Our%2520approach%2520requires%2520no%250Achanges%2520to%2520model%2520architecture%2520or%2520optimizer%252C%2520and%2520can%2520be%2520applied%2520across%2520standard%250Atraining%2520pipelines%252C%2520thus%2520posing%2520an%2520excellent%2520opportunity%2520for%2520wide%2520adoption.%250ACode%2520can%2520be%2520found%2520here%253A%2520https%253A//github.com/bazyagami/LearningWithRevision%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22342v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Progressive%20Data%20Dropout%3A%20An%20Embarrassingly%20Simple%20Approach%20to%20Faster%0A%20%20Training&entry.906535625=Shriram%20M%20S%20and%20Xinyue%20Hao%20and%20Shihao%20Hou%20and%20Yang%20Lu%20and%20Laura%20Sevilla-Lara%20and%20Anurag%20Arnab%20and%20Shreyank%20N%20Gowda&entry.1292438233=%20%20The%20success%20of%20the%20machine%20learning%20field%20has%20reliably%20depended%20on%20training%0Aon%20large%20datasets.%20While%20effective%2C%20this%20trend%20comes%20at%20an%20extraordinary%20cost.%0AThis%20is%20due%20to%20two%20deeply%20intertwined%20factors%3A%20the%20size%20of%20models%20and%20the%20size%0Aof%20datasets.%20While%20promising%20research%20efforts%20focus%20on%20reducing%20the%20size%20of%0Amodels%2C%20the%20other%20half%20of%20the%20equation%20remains%20fairly%20mysterious.%20Indeed%2C%20it%20is%0Asurprising%20that%20the%20standard%20approach%20to%20training%20remains%20to%20iterate%20over%20and%0Aover%2C%20uniformly%20sampling%20the%20training%20dataset.%20In%20this%20paper%20we%20explore%20a%0Aseries%20of%20alternative%20training%20paradigms%20that%20leverage%20insights%20from%0Ahard-data-mining%20and%20dropout%2C%20simple%20enough%20to%20implement%20and%20use%20that%20can%0Abecome%20the%20new%20training%20standard.%20The%20proposed%20Progressive%20Data%20Dropout%20reduces%0Athe%20number%20of%20effective%20epochs%20to%20as%20little%20as%2012.4%25%20of%20the%20baseline.%20This%0Asavings%20actually%20do%20not%20come%20at%20any%20cost%20for%20accuracy.%20Surprisingly%2C%20the%0Aproposed%20method%20improves%20accuracy%20by%20up%20to%204.82%25.%20Our%20approach%20requires%20no%0Achanges%20to%20model%20architecture%20or%20optimizer%2C%20and%20can%20be%20applied%20across%20standard%0Atraining%20pipelines%2C%20thus%20posing%20an%20excellent%20opportunity%20for%20wide%20adoption.%0ACode%20can%20be%20found%20here%3A%20https%3A//github.com/bazyagami/LearningWithRevision%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22342v2&entry.124074799=Read"},
{"title": "SDS-Net: Shallow-Deep Synergism-detection Network for infrared small\n  target detection", "author": "Taoran Yue and Xiaojin Lu and Jiaxi Cai and Yuanping Chen and Shibing Chu", "abstract": "  Current CNN-based infrared small target detection(IRSTD) methods generally\noverlook the heterogeneity between shallow and deep features, leading to\ninefficient collaboration between shallow fine grained structural information\nand deep high-level semantic representations. Additionally, the dependency\nrelationships and fusion mechanisms across different feature hierarchies lack\nsystematic modeling, which fails to fully exploit the complementarity of\nmultilevel features. These limitations hinder IRSTD performance while incurring\nsubstantial computational costs. To address these challenges, this paper\nproposes a shallow-deep synergistic detection network (SDS-Net) that\nefficiently models multilevel feature representations to increase both the\ndetection accuracy and computational efficiency in IRSTD tasks. SDS-Net\nintroduces a dual-branch architecture that separately models the structural\ncharacteristics and semantic properties of features, effectively preserving\nshallow spatial details while capturing deep semantic representations, thereby\nachieving high-precision detection with significantly improved inference speed.\nFurthermore, the network incorporates an adaptive feature fusion module to\ndynamically model cross-layer feature correlations, enhancing overall feature\ncollaboration and representation capability. Comprehensive experiments on three\npublic datasets (NUAA-SIRST, NUDT-SIRST, and IRSTD-1K) demonstrate that SDS-Net\noutperforms state-of-the-art IRSTD methods while maintaining low computational\ncomplexity and high inference efficiency, showing superior detection\nperformance and broad application prospects. Our code will be made public at\nhttps://github.com/PhysiLearn/SDS-Net.\n", "link": "http://arxiv.org/abs/2506.06042v1", "date": "2025-06-06", "relevancy": 2.1656, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5783}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5408}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SDS-Net%3A%20Shallow-Deep%20Synergism-detection%20Network%20for%20infrared%20small%0A%20%20target%20detection&body=Title%3A%20SDS-Net%3A%20Shallow-Deep%20Synergism-detection%20Network%20for%20infrared%20small%0A%20%20target%20detection%0AAuthor%3A%20Taoran%20Yue%20and%20Xiaojin%20Lu%20and%20Jiaxi%20Cai%20and%20Yuanping%20Chen%20and%20Shibing%20Chu%0AAbstract%3A%20%20%20Current%20CNN-based%20infrared%20small%20target%20detection%28IRSTD%29%20methods%20generally%0Aoverlook%20the%20heterogeneity%20between%20shallow%20and%20deep%20features%2C%20leading%20to%0Ainefficient%20collaboration%20between%20shallow%20fine%20grained%20structural%20information%0Aand%20deep%20high-level%20semantic%20representations.%20Additionally%2C%20the%20dependency%0Arelationships%20and%20fusion%20mechanisms%20across%20different%20feature%20hierarchies%20lack%0Asystematic%20modeling%2C%20which%20fails%20to%20fully%20exploit%20the%20complementarity%20of%0Amultilevel%20features.%20These%20limitations%20hinder%20IRSTD%20performance%20while%20incurring%0Asubstantial%20computational%20costs.%20To%20address%20these%20challenges%2C%20this%20paper%0Aproposes%20a%20shallow-deep%20synergistic%20detection%20network%20%28SDS-Net%29%20that%0Aefficiently%20models%20multilevel%20feature%20representations%20to%20increase%20both%20the%0Adetection%20accuracy%20and%20computational%20efficiency%20in%20IRSTD%20tasks.%20SDS-Net%0Aintroduces%20a%20dual-branch%20architecture%20that%20separately%20models%20the%20structural%0Acharacteristics%20and%20semantic%20properties%20of%20features%2C%20effectively%20preserving%0Ashallow%20spatial%20details%20while%20capturing%20deep%20semantic%20representations%2C%20thereby%0Aachieving%20high-precision%20detection%20with%20significantly%20improved%20inference%20speed.%0AFurthermore%2C%20the%20network%20incorporates%20an%20adaptive%20feature%20fusion%20module%20to%0Adynamically%20model%20cross-layer%20feature%20correlations%2C%20enhancing%20overall%20feature%0Acollaboration%20and%20representation%20capability.%20Comprehensive%20experiments%20on%20three%0Apublic%20datasets%20%28NUAA-SIRST%2C%20NUDT-SIRST%2C%20and%20IRSTD-1K%29%20demonstrate%20that%20SDS-Net%0Aoutperforms%20state-of-the-art%20IRSTD%20methods%20while%20maintaining%20low%20computational%0Acomplexity%20and%20high%20inference%20efficiency%2C%20showing%20superior%20detection%0Aperformance%20and%20broad%20application%20prospects.%20Our%20code%20will%20be%20made%20public%20at%0Ahttps%3A//github.com/PhysiLearn/SDS-Net.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06042v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSDS-Net%253A%2520Shallow-Deep%2520Synergism-detection%2520Network%2520for%2520infrared%2520small%250A%2520%2520target%2520detection%26entry.906535625%3DTaoran%2520Yue%2520and%2520Xiaojin%2520Lu%2520and%2520Jiaxi%2520Cai%2520and%2520Yuanping%2520Chen%2520and%2520Shibing%2520Chu%26entry.1292438233%3D%2520%2520Current%2520CNN-based%2520infrared%2520small%2520target%2520detection%2528IRSTD%2529%2520methods%2520generally%250Aoverlook%2520the%2520heterogeneity%2520between%2520shallow%2520and%2520deep%2520features%252C%2520leading%2520to%250Ainefficient%2520collaboration%2520between%2520shallow%2520fine%2520grained%2520structural%2520information%250Aand%2520deep%2520high-level%2520semantic%2520representations.%2520Additionally%252C%2520the%2520dependency%250Arelationships%2520and%2520fusion%2520mechanisms%2520across%2520different%2520feature%2520hierarchies%2520lack%250Asystematic%2520modeling%252C%2520which%2520fails%2520to%2520fully%2520exploit%2520the%2520complementarity%2520of%250Amultilevel%2520features.%2520These%2520limitations%2520hinder%2520IRSTD%2520performance%2520while%2520incurring%250Asubstantial%2520computational%2520costs.%2520To%2520address%2520these%2520challenges%252C%2520this%2520paper%250Aproposes%2520a%2520shallow-deep%2520synergistic%2520detection%2520network%2520%2528SDS-Net%2529%2520that%250Aefficiently%2520models%2520multilevel%2520feature%2520representations%2520to%2520increase%2520both%2520the%250Adetection%2520accuracy%2520and%2520computational%2520efficiency%2520in%2520IRSTD%2520tasks.%2520SDS-Net%250Aintroduces%2520a%2520dual-branch%2520architecture%2520that%2520separately%2520models%2520the%2520structural%250Acharacteristics%2520and%2520semantic%2520properties%2520of%2520features%252C%2520effectively%2520preserving%250Ashallow%2520spatial%2520details%2520while%2520capturing%2520deep%2520semantic%2520representations%252C%2520thereby%250Aachieving%2520high-precision%2520detection%2520with%2520significantly%2520improved%2520inference%2520speed.%250AFurthermore%252C%2520the%2520network%2520incorporates%2520an%2520adaptive%2520feature%2520fusion%2520module%2520to%250Adynamically%2520model%2520cross-layer%2520feature%2520correlations%252C%2520enhancing%2520overall%2520feature%250Acollaboration%2520and%2520representation%2520capability.%2520Comprehensive%2520experiments%2520on%2520three%250Apublic%2520datasets%2520%2528NUAA-SIRST%252C%2520NUDT-SIRST%252C%2520and%2520IRSTD-1K%2529%2520demonstrate%2520that%2520SDS-Net%250Aoutperforms%2520state-of-the-art%2520IRSTD%2520methods%2520while%2520maintaining%2520low%2520computational%250Acomplexity%2520and%2520high%2520inference%2520efficiency%252C%2520showing%2520superior%2520detection%250Aperformance%2520and%2520broad%2520application%2520prospects.%2520Our%2520code%2520will%2520be%2520made%2520public%2520at%250Ahttps%253A//github.com/PhysiLearn/SDS-Net.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06042v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SDS-Net%3A%20Shallow-Deep%20Synergism-detection%20Network%20for%20infrared%20small%0A%20%20target%20detection&entry.906535625=Taoran%20Yue%20and%20Xiaojin%20Lu%20and%20Jiaxi%20Cai%20and%20Yuanping%20Chen%20and%20Shibing%20Chu&entry.1292438233=%20%20Current%20CNN-based%20infrared%20small%20target%20detection%28IRSTD%29%20methods%20generally%0Aoverlook%20the%20heterogeneity%20between%20shallow%20and%20deep%20features%2C%20leading%20to%0Ainefficient%20collaboration%20between%20shallow%20fine%20grained%20structural%20information%0Aand%20deep%20high-level%20semantic%20representations.%20Additionally%2C%20the%20dependency%0Arelationships%20and%20fusion%20mechanisms%20across%20different%20feature%20hierarchies%20lack%0Asystematic%20modeling%2C%20which%20fails%20to%20fully%20exploit%20the%20complementarity%20of%0Amultilevel%20features.%20These%20limitations%20hinder%20IRSTD%20performance%20while%20incurring%0Asubstantial%20computational%20costs.%20To%20address%20these%20challenges%2C%20this%20paper%0Aproposes%20a%20shallow-deep%20synergistic%20detection%20network%20%28SDS-Net%29%20that%0Aefficiently%20models%20multilevel%20feature%20representations%20to%20increase%20both%20the%0Adetection%20accuracy%20and%20computational%20efficiency%20in%20IRSTD%20tasks.%20SDS-Net%0Aintroduces%20a%20dual-branch%20architecture%20that%20separately%20models%20the%20structural%0Acharacteristics%20and%20semantic%20properties%20of%20features%2C%20effectively%20preserving%0Ashallow%20spatial%20details%20while%20capturing%20deep%20semantic%20representations%2C%20thereby%0Aachieving%20high-precision%20detection%20with%20significantly%20improved%20inference%20speed.%0AFurthermore%2C%20the%20network%20incorporates%20an%20adaptive%20feature%20fusion%20module%20to%0Adynamically%20model%20cross-layer%20feature%20correlations%2C%20enhancing%20overall%20feature%0Acollaboration%20and%20representation%20capability.%20Comprehensive%20experiments%20on%20three%0Apublic%20datasets%20%28NUAA-SIRST%2C%20NUDT-SIRST%2C%20and%20IRSTD-1K%29%20demonstrate%20that%20SDS-Net%0Aoutperforms%20state-of-the-art%20IRSTD%20methods%20while%20maintaining%20low%20computational%0Acomplexity%20and%20high%20inference%20efficiency%2C%20showing%20superior%20detection%0Aperformance%20and%20broad%20application%20prospects.%20Our%20code%20will%20be%20made%20public%20at%0Ahttps%3A//github.com/PhysiLearn/SDS-Net.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06042v1&entry.124074799=Read"},
{"title": "Unisoma: A Unified Transformer-based Solver for Multi-Solid Systems", "author": "Shilong Tao and Zhe Feng and Haonan Sun and Zhanxing Zhu and Yunhuai Liu", "abstract": "  Multi-solid systems are foundational to a wide range of real-world\napplications, yet modeling their complex interactions remains challenging.\nExisting deep learning methods predominantly rely on implicit modeling, where\nthe factors influencing solid deformation are not explicitly represented but\nare instead indirectly learned. However, as the number of solids increases,\nthese methods struggle to accurately capture intricate physical interactions.\nIn this paper, we introduce a novel explicit modeling paradigm that\nincorporates factors influencing solid deformation through structured modules.\nSpecifically, we present Unisoma, a unified and flexible Transformer-based\nmodel capable of handling variable numbers of solids. Unisoma directly captures\nphysical interactions using contact modules and adaptive interaction allocation\nmechanism, and learns the deformation through a triplet relationship. Compared\nto implicit modeling techniques, explicit modeling is more well-suited for\nmulti-solid systems with diverse coupling patterns, as it enables detailed\ntreatment of each solid while preventing information blending and confusion.\nExperimentally, Unisoma achieves consistent state-of-the-art performance across\nseven well-established datasets and two complex multi-solid tasks. Code is\navaiable at \\href{this link}{https://github.com/therontau0054/Unisoma}.\n", "link": "http://arxiv.org/abs/2506.06021v1", "date": "2025-06-06", "relevancy": 2.1626, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5455}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5378}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unisoma%3A%20A%20Unified%20Transformer-based%20Solver%20for%20Multi-Solid%20Systems&body=Title%3A%20Unisoma%3A%20A%20Unified%20Transformer-based%20Solver%20for%20Multi-Solid%20Systems%0AAuthor%3A%20Shilong%20Tao%20and%20Zhe%20Feng%20and%20Haonan%20Sun%20and%20Zhanxing%20Zhu%20and%20Yunhuai%20Liu%0AAbstract%3A%20%20%20Multi-solid%20systems%20are%20foundational%20to%20a%20wide%20range%20of%20real-world%0Aapplications%2C%20yet%20modeling%20their%20complex%20interactions%20remains%20challenging.%0AExisting%20deep%20learning%20methods%20predominantly%20rely%20on%20implicit%20modeling%2C%20where%0Athe%20factors%20influencing%20solid%20deformation%20are%20not%20explicitly%20represented%20but%0Aare%20instead%20indirectly%20learned.%20However%2C%20as%20the%20number%20of%20solids%20increases%2C%0Athese%20methods%20struggle%20to%20accurately%20capture%20intricate%20physical%20interactions.%0AIn%20this%20paper%2C%20we%20introduce%20a%20novel%20explicit%20modeling%20paradigm%20that%0Aincorporates%20factors%20influencing%20solid%20deformation%20through%20structured%20modules.%0ASpecifically%2C%20we%20present%20Unisoma%2C%20a%20unified%20and%20flexible%20Transformer-based%0Amodel%20capable%20of%20handling%20variable%20numbers%20of%20solids.%20Unisoma%20directly%20captures%0Aphysical%20interactions%20using%20contact%20modules%20and%20adaptive%20interaction%20allocation%0Amechanism%2C%20and%20learns%20the%20deformation%20through%20a%20triplet%20relationship.%20Compared%0Ato%20implicit%20modeling%20techniques%2C%20explicit%20modeling%20is%20more%20well-suited%20for%0Amulti-solid%20systems%20with%20diverse%20coupling%20patterns%2C%20as%20it%20enables%20detailed%0Atreatment%20of%20each%20solid%20while%20preventing%20information%20blending%20and%20confusion.%0AExperimentally%2C%20Unisoma%20achieves%20consistent%20state-of-the-art%20performance%20across%0Aseven%20well-established%20datasets%20and%20two%20complex%20multi-solid%20tasks.%20Code%20is%0Aavaiable%20at%20%5Chref%7Bthis%20link%7D%7Bhttps%3A//github.com/therontau0054/Unisoma%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06021v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnisoma%253A%2520A%2520Unified%2520Transformer-based%2520Solver%2520for%2520Multi-Solid%2520Systems%26entry.906535625%3DShilong%2520Tao%2520and%2520Zhe%2520Feng%2520and%2520Haonan%2520Sun%2520and%2520Zhanxing%2520Zhu%2520and%2520Yunhuai%2520Liu%26entry.1292438233%3D%2520%2520Multi-solid%2520systems%2520are%2520foundational%2520to%2520a%2520wide%2520range%2520of%2520real-world%250Aapplications%252C%2520yet%2520modeling%2520their%2520complex%2520interactions%2520remains%2520challenging.%250AExisting%2520deep%2520learning%2520methods%2520predominantly%2520rely%2520on%2520implicit%2520modeling%252C%2520where%250Athe%2520factors%2520influencing%2520solid%2520deformation%2520are%2520not%2520explicitly%2520represented%2520but%250Aare%2520instead%2520indirectly%2520learned.%2520However%252C%2520as%2520the%2520number%2520of%2520solids%2520increases%252C%250Athese%2520methods%2520struggle%2520to%2520accurately%2520capture%2520intricate%2520physical%2520interactions.%250AIn%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520explicit%2520modeling%2520paradigm%2520that%250Aincorporates%2520factors%2520influencing%2520solid%2520deformation%2520through%2520structured%2520modules.%250ASpecifically%252C%2520we%2520present%2520Unisoma%252C%2520a%2520unified%2520and%2520flexible%2520Transformer-based%250Amodel%2520capable%2520of%2520handling%2520variable%2520numbers%2520of%2520solids.%2520Unisoma%2520directly%2520captures%250Aphysical%2520interactions%2520using%2520contact%2520modules%2520and%2520adaptive%2520interaction%2520allocation%250Amechanism%252C%2520and%2520learns%2520the%2520deformation%2520through%2520a%2520triplet%2520relationship.%2520Compared%250Ato%2520implicit%2520modeling%2520techniques%252C%2520explicit%2520modeling%2520is%2520more%2520well-suited%2520for%250Amulti-solid%2520systems%2520with%2520diverse%2520coupling%2520patterns%252C%2520as%2520it%2520enables%2520detailed%250Atreatment%2520of%2520each%2520solid%2520while%2520preventing%2520information%2520blending%2520and%2520confusion.%250AExperimentally%252C%2520Unisoma%2520achieves%2520consistent%2520state-of-the-art%2520performance%2520across%250Aseven%2520well-established%2520datasets%2520and%2520two%2520complex%2520multi-solid%2520tasks.%2520Code%2520is%250Aavaiable%2520at%2520%255Chref%257Bthis%2520link%257D%257Bhttps%253A//github.com/therontau0054/Unisoma%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06021v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unisoma%3A%20A%20Unified%20Transformer-based%20Solver%20for%20Multi-Solid%20Systems&entry.906535625=Shilong%20Tao%20and%20Zhe%20Feng%20and%20Haonan%20Sun%20and%20Zhanxing%20Zhu%20and%20Yunhuai%20Liu&entry.1292438233=%20%20Multi-solid%20systems%20are%20foundational%20to%20a%20wide%20range%20of%20real-world%0Aapplications%2C%20yet%20modeling%20their%20complex%20interactions%20remains%20challenging.%0AExisting%20deep%20learning%20methods%20predominantly%20rely%20on%20implicit%20modeling%2C%20where%0Athe%20factors%20influencing%20solid%20deformation%20are%20not%20explicitly%20represented%20but%0Aare%20instead%20indirectly%20learned.%20However%2C%20as%20the%20number%20of%20solids%20increases%2C%0Athese%20methods%20struggle%20to%20accurately%20capture%20intricate%20physical%20interactions.%0AIn%20this%20paper%2C%20we%20introduce%20a%20novel%20explicit%20modeling%20paradigm%20that%0Aincorporates%20factors%20influencing%20solid%20deformation%20through%20structured%20modules.%0ASpecifically%2C%20we%20present%20Unisoma%2C%20a%20unified%20and%20flexible%20Transformer-based%0Amodel%20capable%20of%20handling%20variable%20numbers%20of%20solids.%20Unisoma%20directly%20captures%0Aphysical%20interactions%20using%20contact%20modules%20and%20adaptive%20interaction%20allocation%0Amechanism%2C%20and%20learns%20the%20deformation%20through%20a%20triplet%20relationship.%20Compared%0Ato%20implicit%20modeling%20techniques%2C%20explicit%20modeling%20is%20more%20well-suited%20for%0Amulti-solid%20systems%20with%20diverse%20coupling%20patterns%2C%20as%20it%20enables%20detailed%0Atreatment%20of%20each%20solid%20while%20preventing%20information%20blending%20and%20confusion.%0AExperimentally%2C%20Unisoma%20achieves%20consistent%20state-of-the-art%20performance%20across%0Aseven%20well-established%20datasets%20and%20two%20complex%20multi-solid%20tasks.%20Code%20is%0Aavaiable%20at%20%5Chref%7Bthis%20link%7D%7Bhttps%3A//github.com/therontau0054/Unisoma%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06021v1&entry.124074799=Read"},
{"title": "Diffusion-Based Hierarchical Graph Neural Networks for Simulating\n  Nonlinear Solid Mechanics", "author": "Tobias W\u00fcrth and Niklas Freymuth and Gerhard Neumann and Luise K\u00e4rger", "abstract": "  Graph-based learned simulators have emerged as a promising approach for\nsimulating physical systems on unstructured meshes, offering speed and\ngeneralization across diverse geometries. However, they often struggle with\ncapturing global phenomena, such as bending or long-range correlations, and\nsuffer from error accumulation over long rollouts due to their reliance on\nlocal message passing and direct next-step prediction. We address these\nlimitations by introducing the Rolling Diffusion-Batched Inference Network\n(ROBIN), a novel learned simulator that integrates two key innovations: (i)\nRolling Diffusion, a parallelized inference scheme that amortizes the cost of\ndiffusion-based refinement across physical time steps by overlapping denoising\nsteps across a temporal window. (ii) A Hierarchical Graph Neural Network built\non algebraic multigrid coarsening, enabling multiscale message passing across\ndifferent mesh resolutions. This architecture, implemented via\nAlgebraic-hierarchical Message Passing Networks, captures both fine-scale local\ndynamics and global structural effects critical for phenomena like beam bending\nor multi-body contact. We validate ROBIN on challenging 2D and 3D solid\nmechanics benchmarks involving geometric, material, and contact nonlinearities.\nROBIN achieves state-of-the-art accuracy on all tasks, substantially\noutperforming existing next-step learned simulators while reducing inference\ntime by up to an order of magnitude compared to standard diffusion simulators.\n", "link": "http://arxiv.org/abs/2506.06045v1", "date": "2025-06-06", "relevancy": 2.1619, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5676}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.543}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5271}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion-Based%20Hierarchical%20Graph%20Neural%20Networks%20for%20Simulating%0A%20%20Nonlinear%20Solid%20Mechanics&body=Title%3A%20Diffusion-Based%20Hierarchical%20Graph%20Neural%20Networks%20for%20Simulating%0A%20%20Nonlinear%20Solid%20Mechanics%0AAuthor%3A%20Tobias%20W%C3%BCrth%20and%20Niklas%20Freymuth%20and%20Gerhard%20Neumann%20and%20Luise%20K%C3%A4rger%0AAbstract%3A%20%20%20Graph-based%20learned%20simulators%20have%20emerged%20as%20a%20promising%20approach%20for%0Asimulating%20physical%20systems%20on%20unstructured%20meshes%2C%20offering%20speed%20and%0Ageneralization%20across%20diverse%20geometries.%20However%2C%20they%20often%20struggle%20with%0Acapturing%20global%20phenomena%2C%20such%20as%20bending%20or%20long-range%20correlations%2C%20and%0Asuffer%20from%20error%20accumulation%20over%20long%20rollouts%20due%20to%20their%20reliance%20on%0Alocal%20message%20passing%20and%20direct%20next-step%20prediction.%20We%20address%20these%0Alimitations%20by%20introducing%20the%20Rolling%20Diffusion-Batched%20Inference%20Network%0A%28ROBIN%29%2C%20a%20novel%20learned%20simulator%20that%20integrates%20two%20key%20innovations%3A%20%28i%29%0ARolling%20Diffusion%2C%20a%20parallelized%20inference%20scheme%20that%20amortizes%20the%20cost%20of%0Adiffusion-based%20refinement%20across%20physical%20time%20steps%20by%20overlapping%20denoising%0Asteps%20across%20a%20temporal%20window.%20%28ii%29%20A%20Hierarchical%20Graph%20Neural%20Network%20built%0Aon%20algebraic%20multigrid%20coarsening%2C%20enabling%20multiscale%20message%20passing%20across%0Adifferent%20mesh%20resolutions.%20This%20architecture%2C%20implemented%20via%0AAlgebraic-hierarchical%20Message%20Passing%20Networks%2C%20captures%20both%20fine-scale%20local%0Adynamics%20and%20global%20structural%20effects%20critical%20for%20phenomena%20like%20beam%20bending%0Aor%20multi-body%20contact.%20We%20validate%20ROBIN%20on%20challenging%202D%20and%203D%20solid%0Amechanics%20benchmarks%20involving%20geometric%2C%20material%2C%20and%20contact%20nonlinearities.%0AROBIN%20achieves%20state-of-the-art%20accuracy%20on%20all%20tasks%2C%20substantially%0Aoutperforming%20existing%20next-step%20learned%20simulators%20while%20reducing%20inference%0Atime%20by%20up%20to%20an%20order%20of%20magnitude%20compared%20to%20standard%20diffusion%20simulators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06045v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion-Based%2520Hierarchical%2520Graph%2520Neural%2520Networks%2520for%2520Simulating%250A%2520%2520Nonlinear%2520Solid%2520Mechanics%26entry.906535625%3DTobias%2520W%25C3%25BCrth%2520and%2520Niklas%2520Freymuth%2520and%2520Gerhard%2520Neumann%2520and%2520Luise%2520K%25C3%25A4rger%26entry.1292438233%3D%2520%2520Graph-based%2520learned%2520simulators%2520have%2520emerged%2520as%2520a%2520promising%2520approach%2520for%250Asimulating%2520physical%2520systems%2520on%2520unstructured%2520meshes%252C%2520offering%2520speed%2520and%250Ageneralization%2520across%2520diverse%2520geometries.%2520However%252C%2520they%2520often%2520struggle%2520with%250Acapturing%2520global%2520phenomena%252C%2520such%2520as%2520bending%2520or%2520long-range%2520correlations%252C%2520and%250Asuffer%2520from%2520error%2520accumulation%2520over%2520long%2520rollouts%2520due%2520to%2520their%2520reliance%2520on%250Alocal%2520message%2520passing%2520and%2520direct%2520next-step%2520prediction.%2520We%2520address%2520these%250Alimitations%2520by%2520introducing%2520the%2520Rolling%2520Diffusion-Batched%2520Inference%2520Network%250A%2528ROBIN%2529%252C%2520a%2520novel%2520learned%2520simulator%2520that%2520integrates%2520two%2520key%2520innovations%253A%2520%2528i%2529%250ARolling%2520Diffusion%252C%2520a%2520parallelized%2520inference%2520scheme%2520that%2520amortizes%2520the%2520cost%2520of%250Adiffusion-based%2520refinement%2520across%2520physical%2520time%2520steps%2520by%2520overlapping%2520denoising%250Asteps%2520across%2520a%2520temporal%2520window.%2520%2528ii%2529%2520A%2520Hierarchical%2520Graph%2520Neural%2520Network%2520built%250Aon%2520algebraic%2520multigrid%2520coarsening%252C%2520enabling%2520multiscale%2520message%2520passing%2520across%250Adifferent%2520mesh%2520resolutions.%2520This%2520architecture%252C%2520implemented%2520via%250AAlgebraic-hierarchical%2520Message%2520Passing%2520Networks%252C%2520captures%2520both%2520fine-scale%2520local%250Adynamics%2520and%2520global%2520structural%2520effects%2520critical%2520for%2520phenomena%2520like%2520beam%2520bending%250Aor%2520multi-body%2520contact.%2520We%2520validate%2520ROBIN%2520on%2520challenging%25202D%2520and%25203D%2520solid%250Amechanics%2520benchmarks%2520involving%2520geometric%252C%2520material%252C%2520and%2520contact%2520nonlinearities.%250AROBIN%2520achieves%2520state-of-the-art%2520accuracy%2520on%2520all%2520tasks%252C%2520substantially%250Aoutperforming%2520existing%2520next-step%2520learned%2520simulators%2520while%2520reducing%2520inference%250Atime%2520by%2520up%2520to%2520an%2520order%2520of%2520magnitude%2520compared%2520to%2520standard%2520diffusion%2520simulators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06045v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion-Based%20Hierarchical%20Graph%20Neural%20Networks%20for%20Simulating%0A%20%20Nonlinear%20Solid%20Mechanics&entry.906535625=Tobias%20W%C3%BCrth%20and%20Niklas%20Freymuth%20and%20Gerhard%20Neumann%20and%20Luise%20K%C3%A4rger&entry.1292438233=%20%20Graph-based%20learned%20simulators%20have%20emerged%20as%20a%20promising%20approach%20for%0Asimulating%20physical%20systems%20on%20unstructured%20meshes%2C%20offering%20speed%20and%0Ageneralization%20across%20diverse%20geometries.%20However%2C%20they%20often%20struggle%20with%0Acapturing%20global%20phenomena%2C%20such%20as%20bending%20or%20long-range%20correlations%2C%20and%0Asuffer%20from%20error%20accumulation%20over%20long%20rollouts%20due%20to%20their%20reliance%20on%0Alocal%20message%20passing%20and%20direct%20next-step%20prediction.%20We%20address%20these%0Alimitations%20by%20introducing%20the%20Rolling%20Diffusion-Batched%20Inference%20Network%0A%28ROBIN%29%2C%20a%20novel%20learned%20simulator%20that%20integrates%20two%20key%20innovations%3A%20%28i%29%0ARolling%20Diffusion%2C%20a%20parallelized%20inference%20scheme%20that%20amortizes%20the%20cost%20of%0Adiffusion-based%20refinement%20across%20physical%20time%20steps%20by%20overlapping%20denoising%0Asteps%20across%20a%20temporal%20window.%20%28ii%29%20A%20Hierarchical%20Graph%20Neural%20Network%20built%0Aon%20algebraic%20multigrid%20coarsening%2C%20enabling%20multiscale%20message%20passing%20across%0Adifferent%20mesh%20resolutions.%20This%20architecture%2C%20implemented%20via%0AAlgebraic-hierarchical%20Message%20Passing%20Networks%2C%20captures%20both%20fine-scale%20local%0Adynamics%20and%20global%20structural%20effects%20critical%20for%20phenomena%20like%20beam%20bending%0Aor%20multi-body%20contact.%20We%20validate%20ROBIN%20on%20challenging%202D%20and%203D%20solid%0Amechanics%20benchmarks%20involving%20geometric%2C%20material%2C%20and%20contact%20nonlinearities.%0AROBIN%20achieves%20state-of-the-art%20accuracy%20on%20all%20tasks%2C%20substantially%0Aoutperforming%20existing%20next-step%20learned%20simulators%20while%20reducing%20inference%0Atime%20by%20up%20to%20an%20order%20of%20magnitude%20compared%20to%20standard%20diffusion%20simulators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06045v1&entry.124074799=Read"},
{"title": "Normalizing Flows are Capable Generative Models", "author": "Shuangfei Zhai and Ruixiang Zhang and Preetum Nakkiran and David Berthelot and Jiatao Gu and Huangjie Zheng and Tianrong Chen and Miguel Angel Bautista and Navdeep Jaitly and Josh Susskind", "abstract": "  Normalizing Flows (NFs) are likelihood-based models for continuous inputs.\nThey have demonstrated promising results on both density estimation and\ngenerative modeling tasks, but have received relatively little attention in\nrecent years. In this work, we demonstrate that NFs are more powerful than\npreviously believed. We present TarFlow: a simple and scalable architecture\nthat enables highly performant NF models. TarFlow can be thought of as a\nTransformer-based variant of Masked Autoregressive Flows (MAFs): it consists of\na stack of autoregressive Transformer blocks on image patches, alternating the\nautoregression direction between layers. TarFlow is straightforward to train\nend-to-end, and capable of directly modeling and generating pixels. We also\npropose three key techniques to improve sample quality: Gaussian noise\naugmentation during training, a post training denoising procedure, and an\neffective guidance method for both class-conditional and unconditional\nsettings. Putting these together, TarFlow sets new state-of-the-art results on\nlikelihood estimation for images, beating the previous best methods by a large\nmargin, and generates samples with quality and diversity comparable to\ndiffusion models, for the first time with a stand-alone NF model. We make our\ncode available at https://github.com/apple/ml-tarflow.\n", "link": "http://arxiv.org/abs/2412.06329v3", "date": "2025-06-06", "relevancy": 2.1524, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.667}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5251}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Normalizing%20Flows%20are%20Capable%20Generative%20Models&body=Title%3A%20Normalizing%20Flows%20are%20Capable%20Generative%20Models%0AAuthor%3A%20Shuangfei%20Zhai%20and%20Ruixiang%20Zhang%20and%20Preetum%20Nakkiran%20and%20David%20Berthelot%20and%20Jiatao%20Gu%20and%20Huangjie%20Zheng%20and%20Tianrong%20Chen%20and%20Miguel%20Angel%20Bautista%20and%20Navdeep%20Jaitly%20and%20Josh%20Susskind%0AAbstract%3A%20%20%20Normalizing%20Flows%20%28NFs%29%20are%20likelihood-based%20models%20for%20continuous%20inputs.%0AThey%20have%20demonstrated%20promising%20results%20on%20both%20density%20estimation%20and%0Agenerative%20modeling%20tasks%2C%20but%20have%20received%20relatively%20little%20attention%20in%0Arecent%20years.%20In%20this%20work%2C%20we%20demonstrate%20that%20NFs%20are%20more%20powerful%20than%0Apreviously%20believed.%20We%20present%20TarFlow%3A%20a%20simple%20and%20scalable%20architecture%0Athat%20enables%20highly%20performant%20NF%20models.%20TarFlow%20can%20be%20thought%20of%20as%20a%0ATransformer-based%20variant%20of%20Masked%20Autoregressive%20Flows%20%28MAFs%29%3A%20it%20consists%20of%0Aa%20stack%20of%20autoregressive%20Transformer%20blocks%20on%20image%20patches%2C%20alternating%20the%0Aautoregression%20direction%20between%20layers.%20TarFlow%20is%20straightforward%20to%20train%0Aend-to-end%2C%20and%20capable%20of%20directly%20modeling%20and%20generating%20pixels.%20We%20also%0Apropose%20three%20key%20techniques%20to%20improve%20sample%20quality%3A%20Gaussian%20noise%0Aaugmentation%20during%20training%2C%20a%20post%20training%20denoising%20procedure%2C%20and%20an%0Aeffective%20guidance%20method%20for%20both%20class-conditional%20and%20unconditional%0Asettings.%20Putting%20these%20together%2C%20TarFlow%20sets%20new%20state-of-the-art%20results%20on%0Alikelihood%20estimation%20for%20images%2C%20beating%20the%20previous%20best%20methods%20by%20a%20large%0Amargin%2C%20and%20generates%20samples%20with%20quality%20and%20diversity%20comparable%20to%0Adiffusion%20models%2C%20for%20the%20first%20time%20with%20a%20stand-alone%20NF%20model.%20We%20make%20our%0Acode%20available%20at%20https%3A//github.com/apple/ml-tarflow.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.06329v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNormalizing%2520Flows%2520are%2520Capable%2520Generative%2520Models%26entry.906535625%3DShuangfei%2520Zhai%2520and%2520Ruixiang%2520Zhang%2520and%2520Preetum%2520Nakkiran%2520and%2520David%2520Berthelot%2520and%2520Jiatao%2520Gu%2520and%2520Huangjie%2520Zheng%2520and%2520Tianrong%2520Chen%2520and%2520Miguel%2520Angel%2520Bautista%2520and%2520Navdeep%2520Jaitly%2520and%2520Josh%2520Susskind%26entry.1292438233%3D%2520%2520Normalizing%2520Flows%2520%2528NFs%2529%2520are%2520likelihood-based%2520models%2520for%2520continuous%2520inputs.%250AThey%2520have%2520demonstrated%2520promising%2520results%2520on%2520both%2520density%2520estimation%2520and%250Agenerative%2520modeling%2520tasks%252C%2520but%2520have%2520received%2520relatively%2520little%2520attention%2520in%250Arecent%2520years.%2520In%2520this%2520work%252C%2520we%2520demonstrate%2520that%2520NFs%2520are%2520more%2520powerful%2520than%250Apreviously%2520believed.%2520We%2520present%2520TarFlow%253A%2520a%2520simple%2520and%2520scalable%2520architecture%250Athat%2520enables%2520highly%2520performant%2520NF%2520models.%2520TarFlow%2520can%2520be%2520thought%2520of%2520as%2520a%250ATransformer-based%2520variant%2520of%2520Masked%2520Autoregressive%2520Flows%2520%2528MAFs%2529%253A%2520it%2520consists%2520of%250Aa%2520stack%2520of%2520autoregressive%2520Transformer%2520blocks%2520on%2520image%2520patches%252C%2520alternating%2520the%250Aautoregression%2520direction%2520between%2520layers.%2520TarFlow%2520is%2520straightforward%2520to%2520train%250Aend-to-end%252C%2520and%2520capable%2520of%2520directly%2520modeling%2520and%2520generating%2520pixels.%2520We%2520also%250Apropose%2520three%2520key%2520techniques%2520to%2520improve%2520sample%2520quality%253A%2520Gaussian%2520noise%250Aaugmentation%2520during%2520training%252C%2520a%2520post%2520training%2520denoising%2520procedure%252C%2520and%2520an%250Aeffective%2520guidance%2520method%2520for%2520both%2520class-conditional%2520and%2520unconditional%250Asettings.%2520Putting%2520these%2520together%252C%2520TarFlow%2520sets%2520new%2520state-of-the-art%2520results%2520on%250Alikelihood%2520estimation%2520for%2520images%252C%2520beating%2520the%2520previous%2520best%2520methods%2520by%2520a%2520large%250Amargin%252C%2520and%2520generates%2520samples%2520with%2520quality%2520and%2520diversity%2520comparable%2520to%250Adiffusion%2520models%252C%2520for%2520the%2520first%2520time%2520with%2520a%2520stand-alone%2520NF%2520model.%2520We%2520make%2520our%250Acode%2520available%2520at%2520https%253A//github.com/apple/ml-tarflow.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.06329v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Normalizing%20Flows%20are%20Capable%20Generative%20Models&entry.906535625=Shuangfei%20Zhai%20and%20Ruixiang%20Zhang%20and%20Preetum%20Nakkiran%20and%20David%20Berthelot%20and%20Jiatao%20Gu%20and%20Huangjie%20Zheng%20and%20Tianrong%20Chen%20and%20Miguel%20Angel%20Bautista%20and%20Navdeep%20Jaitly%20and%20Josh%20Susskind&entry.1292438233=%20%20Normalizing%20Flows%20%28NFs%29%20are%20likelihood-based%20models%20for%20continuous%20inputs.%0AThey%20have%20demonstrated%20promising%20results%20on%20both%20density%20estimation%20and%0Agenerative%20modeling%20tasks%2C%20but%20have%20received%20relatively%20little%20attention%20in%0Arecent%20years.%20In%20this%20work%2C%20we%20demonstrate%20that%20NFs%20are%20more%20powerful%20than%0Apreviously%20believed.%20We%20present%20TarFlow%3A%20a%20simple%20and%20scalable%20architecture%0Athat%20enables%20highly%20performant%20NF%20models.%20TarFlow%20can%20be%20thought%20of%20as%20a%0ATransformer-based%20variant%20of%20Masked%20Autoregressive%20Flows%20%28MAFs%29%3A%20it%20consists%20of%0Aa%20stack%20of%20autoregressive%20Transformer%20blocks%20on%20image%20patches%2C%20alternating%20the%0Aautoregression%20direction%20between%20layers.%20TarFlow%20is%20straightforward%20to%20train%0Aend-to-end%2C%20and%20capable%20of%20directly%20modeling%20and%20generating%20pixels.%20We%20also%0Apropose%20three%20key%20techniques%20to%20improve%20sample%20quality%3A%20Gaussian%20noise%0Aaugmentation%20during%20training%2C%20a%20post%20training%20denoising%20procedure%2C%20and%20an%0Aeffective%20guidance%20method%20for%20both%20class-conditional%20and%20unconditional%0Asettings.%20Putting%20these%20together%2C%20TarFlow%20sets%20new%20state-of-the-art%20results%20on%0Alikelihood%20estimation%20for%20images%2C%20beating%20the%20previous%20best%20methods%20by%20a%20large%0Amargin%2C%20and%20generates%20samples%20with%20quality%20and%20diversity%20comparable%20to%0Adiffusion%20models%2C%20for%20the%20first%20time%20with%20a%20stand-alone%20NF%20model.%20We%20make%20our%0Acode%20available%20at%20https%3A//github.com/apple/ml-tarflow.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.06329v3&entry.124074799=Read"},
{"title": "Sketched Equivariant Imaging Regularization and Deep Internal Learning\n  for Inverse Problems", "author": "Guixian Xu and Jinglai Li and Junqi Tang", "abstract": "  Equivariant Imaging (EI) regularization has become the de-facto technique for\nunsupervised training of deep imaging networks, without any need of\nground-truth data. Observing that the EI-based unsupervised training paradigm\ncurrently has significant computational redundancy leading to inefficiency in\nhigh-dimensional applications, we propose a sketched EI regularization which\nleverages the randomized sketching techniques for acceleration. We apply our\nsketched EI regularization to develop an accelerated deep internal learning\nframework, which can be efficiently applied for test-time network adaptation.\nAdditionally, for network adaptation tasks, we propose a parameter-efficient\napproach to accelerate both EI and Sketched-EI via optimizing only the\nnormalization layers. Our numerical study on X-ray CT and multicoil magnetic\nresonance image reconstruction tasks demonstrate that our approach can achieve\nsignificant computational acceleration over standard EI counterpart in\nsingle-input setting and network adaptation at test time.\n", "link": "http://arxiv.org/abs/2411.05771v4", "date": "2025-06-06", "relevancy": 2.1494, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5429}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.539}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sketched%20Equivariant%20Imaging%20Regularization%20and%20Deep%20Internal%20Learning%0A%20%20for%20Inverse%20Problems&body=Title%3A%20Sketched%20Equivariant%20Imaging%20Regularization%20and%20Deep%20Internal%20Learning%0A%20%20for%20Inverse%20Problems%0AAuthor%3A%20Guixian%20Xu%20and%20Jinglai%20Li%20and%20Junqi%20Tang%0AAbstract%3A%20%20%20Equivariant%20Imaging%20%28EI%29%20regularization%20has%20become%20the%20de-facto%20technique%20for%0Aunsupervised%20training%20of%20deep%20imaging%20networks%2C%20without%20any%20need%20of%0Aground-truth%20data.%20Observing%20that%20the%20EI-based%20unsupervised%20training%20paradigm%0Acurrently%20has%20significant%20computational%20redundancy%20leading%20to%20inefficiency%20in%0Ahigh-dimensional%20applications%2C%20we%20propose%20a%20sketched%20EI%20regularization%20which%0Aleverages%20the%20randomized%20sketching%20techniques%20for%20acceleration.%20We%20apply%20our%0Asketched%20EI%20regularization%20to%20develop%20an%20accelerated%20deep%20internal%20learning%0Aframework%2C%20which%20can%20be%20efficiently%20applied%20for%20test-time%20network%20adaptation.%0AAdditionally%2C%20for%20network%20adaptation%20tasks%2C%20we%20propose%20a%20parameter-efficient%0Aapproach%20to%20accelerate%20both%20EI%20and%20Sketched-EI%20via%20optimizing%20only%20the%0Anormalization%20layers.%20Our%20numerical%20study%20on%20X-ray%20CT%20and%20multicoil%20magnetic%0Aresonance%20image%20reconstruction%20tasks%20demonstrate%20that%20our%20approach%20can%20achieve%0Asignificant%20computational%20acceleration%20over%20standard%20EI%20counterpart%20in%0Asingle-input%20setting%20and%20network%20adaptation%20at%20test%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05771v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSketched%2520Equivariant%2520Imaging%2520Regularization%2520and%2520Deep%2520Internal%2520Learning%250A%2520%2520for%2520Inverse%2520Problems%26entry.906535625%3DGuixian%2520Xu%2520and%2520Jinglai%2520Li%2520and%2520Junqi%2520Tang%26entry.1292438233%3D%2520%2520Equivariant%2520Imaging%2520%2528EI%2529%2520regularization%2520has%2520become%2520the%2520de-facto%2520technique%2520for%250Aunsupervised%2520training%2520of%2520deep%2520imaging%2520networks%252C%2520without%2520any%2520need%2520of%250Aground-truth%2520data.%2520Observing%2520that%2520the%2520EI-based%2520unsupervised%2520training%2520paradigm%250Acurrently%2520has%2520significant%2520computational%2520redundancy%2520leading%2520to%2520inefficiency%2520in%250Ahigh-dimensional%2520applications%252C%2520we%2520propose%2520a%2520sketched%2520EI%2520regularization%2520which%250Aleverages%2520the%2520randomized%2520sketching%2520techniques%2520for%2520acceleration.%2520We%2520apply%2520our%250Asketched%2520EI%2520regularization%2520to%2520develop%2520an%2520accelerated%2520deep%2520internal%2520learning%250Aframework%252C%2520which%2520can%2520be%2520efficiently%2520applied%2520for%2520test-time%2520network%2520adaptation.%250AAdditionally%252C%2520for%2520network%2520adaptation%2520tasks%252C%2520we%2520propose%2520a%2520parameter-efficient%250Aapproach%2520to%2520accelerate%2520both%2520EI%2520and%2520Sketched-EI%2520via%2520optimizing%2520only%2520the%250Anormalization%2520layers.%2520Our%2520numerical%2520study%2520on%2520X-ray%2520CT%2520and%2520multicoil%2520magnetic%250Aresonance%2520image%2520reconstruction%2520tasks%2520demonstrate%2520that%2520our%2520approach%2520can%2520achieve%250Asignificant%2520computational%2520acceleration%2520over%2520standard%2520EI%2520counterpart%2520in%250Asingle-input%2520setting%2520and%2520network%2520adaptation%2520at%2520test%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05771v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sketched%20Equivariant%20Imaging%20Regularization%20and%20Deep%20Internal%20Learning%0A%20%20for%20Inverse%20Problems&entry.906535625=Guixian%20Xu%20and%20Jinglai%20Li%20and%20Junqi%20Tang&entry.1292438233=%20%20Equivariant%20Imaging%20%28EI%29%20regularization%20has%20become%20the%20de-facto%20technique%20for%0Aunsupervised%20training%20of%20deep%20imaging%20networks%2C%20without%20any%20need%20of%0Aground-truth%20data.%20Observing%20that%20the%20EI-based%20unsupervised%20training%20paradigm%0Acurrently%20has%20significant%20computational%20redundancy%20leading%20to%20inefficiency%20in%0Ahigh-dimensional%20applications%2C%20we%20propose%20a%20sketched%20EI%20regularization%20which%0Aleverages%20the%20randomized%20sketching%20techniques%20for%20acceleration.%20We%20apply%20our%0Asketched%20EI%20regularization%20to%20develop%20an%20accelerated%20deep%20internal%20learning%0Aframework%2C%20which%20can%20be%20efficiently%20applied%20for%20test-time%20network%20adaptation.%0AAdditionally%2C%20for%20network%20adaptation%20tasks%2C%20we%20propose%20a%20parameter-efficient%0Aapproach%20to%20accelerate%20both%20EI%20and%20Sketched-EI%20via%20optimizing%20only%20the%0Anormalization%20layers.%20Our%20numerical%20study%20on%20X-ray%20CT%20and%20multicoil%20magnetic%0Aresonance%20image%20reconstruction%20tasks%20demonstrate%20that%20our%20approach%20can%20achieve%0Asignificant%20computational%20acceleration%20over%20standard%20EI%20counterpart%20in%0Asingle-input%20setting%20and%20network%20adaptation%20at%20test%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05771v4&entry.124074799=Read"},
{"title": "diffDemorph: Extending Reference-Free Demorphing to Unseen Faces", "author": "Nitish Shukla and Arun Ross", "abstract": "  A face morph is created by combining two face images corresponding to two\nidentities to produce a composite that successfully matches both the\nconstituent identities. Reference-free (RF) demorphing reverses this process\nusing only the morph image, without the need for additional reference images.\nPrevious RF demorphing methods are overly constrained, as they rely on\nassumptions about the distributions of training and testing morphs such as the\nmorphing technique used (e.g., landmark-based) and face image style (e.g.,\npassport photos). In this paper, we introduce a novel diffusion-based approach,\nreferred to as diffDeMorph, that effectively disentangles component images from\na composite morph image with high visual fidelity. Our method is the first to\ngeneralize across morph techniques and face styles, beating the current state\nof the art by $\\geq 59.46\\%$ under a common training protocol across all\ndatasets tested. We train our method on morphs created using synthetically\ngenerated face images and test on real morphs, thereby enhancing the\npracticality of the technique. Experiments on six datasets and two face\nmatchers establish the utility and efficacy of our method.\n", "link": "http://arxiv.org/abs/2505.14527v3", "date": "2025-06-06", "relevancy": 2.1434, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5399}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5353}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5348}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20diffDemorph%3A%20Extending%20Reference-Free%20Demorphing%20to%20Unseen%20Faces&body=Title%3A%20diffDemorph%3A%20Extending%20Reference-Free%20Demorphing%20to%20Unseen%20Faces%0AAuthor%3A%20Nitish%20Shukla%20and%20Arun%20Ross%0AAbstract%3A%20%20%20A%20face%20morph%20is%20created%20by%20combining%20two%20face%20images%20corresponding%20to%20two%0Aidentities%20to%20produce%20a%20composite%20that%20successfully%20matches%20both%20the%0Aconstituent%20identities.%20Reference-free%20%28RF%29%20demorphing%20reverses%20this%20process%0Ausing%20only%20the%20morph%20image%2C%20without%20the%20need%20for%20additional%20reference%20images.%0APrevious%20RF%20demorphing%20methods%20are%20overly%20constrained%2C%20as%20they%20rely%20on%0Aassumptions%20about%20the%20distributions%20of%20training%20and%20testing%20morphs%20such%20as%20the%0Amorphing%20technique%20used%20%28e.g.%2C%20landmark-based%29%20and%20face%20image%20style%20%28e.g.%2C%0Apassport%20photos%29.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20diffusion-based%20approach%2C%0Areferred%20to%20as%20diffDeMorph%2C%20that%20effectively%20disentangles%20component%20images%20from%0Aa%20composite%20morph%20image%20with%20high%20visual%20fidelity.%20Our%20method%20is%20the%20first%20to%0Ageneralize%20across%20morph%20techniques%20and%20face%20styles%2C%20beating%20the%20current%20state%0Aof%20the%20art%20by%20%24%5Cgeq%2059.46%5C%25%24%20under%20a%20common%20training%20protocol%20across%20all%0Adatasets%20tested.%20We%20train%20our%20method%20on%20morphs%20created%20using%20synthetically%0Agenerated%20face%20images%20and%20test%20on%20real%20morphs%2C%20thereby%20enhancing%20the%0Apracticality%20of%20the%20technique.%20Experiments%20on%20six%20datasets%20and%20two%20face%0Amatchers%20establish%20the%20utility%20and%20efficacy%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14527v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DdiffDemorph%253A%2520Extending%2520Reference-Free%2520Demorphing%2520to%2520Unseen%2520Faces%26entry.906535625%3DNitish%2520Shukla%2520and%2520Arun%2520Ross%26entry.1292438233%3D%2520%2520A%2520face%2520morph%2520is%2520created%2520by%2520combining%2520two%2520face%2520images%2520corresponding%2520to%2520two%250Aidentities%2520to%2520produce%2520a%2520composite%2520that%2520successfully%2520matches%2520both%2520the%250Aconstituent%2520identities.%2520Reference-free%2520%2528RF%2529%2520demorphing%2520reverses%2520this%2520process%250Ausing%2520only%2520the%2520morph%2520image%252C%2520without%2520the%2520need%2520for%2520additional%2520reference%2520images.%250APrevious%2520RF%2520demorphing%2520methods%2520are%2520overly%2520constrained%252C%2520as%2520they%2520rely%2520on%250Aassumptions%2520about%2520the%2520distributions%2520of%2520training%2520and%2520testing%2520morphs%2520such%2520as%2520the%250Amorphing%2520technique%2520used%2520%2528e.g.%252C%2520landmark-based%2529%2520and%2520face%2520image%2520style%2520%2528e.g.%252C%250Apassport%2520photos%2529.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520diffusion-based%2520approach%252C%250Areferred%2520to%2520as%2520diffDeMorph%252C%2520that%2520effectively%2520disentangles%2520component%2520images%2520from%250Aa%2520composite%2520morph%2520image%2520with%2520high%2520visual%2520fidelity.%2520Our%2520method%2520is%2520the%2520first%2520to%250Ageneralize%2520across%2520morph%2520techniques%2520and%2520face%2520styles%252C%2520beating%2520the%2520current%2520state%250Aof%2520the%2520art%2520by%2520%2524%255Cgeq%252059.46%255C%2525%2524%2520under%2520a%2520common%2520training%2520protocol%2520across%2520all%250Adatasets%2520tested.%2520We%2520train%2520our%2520method%2520on%2520morphs%2520created%2520using%2520synthetically%250Agenerated%2520face%2520images%2520and%2520test%2520on%2520real%2520morphs%252C%2520thereby%2520enhancing%2520the%250Apracticality%2520of%2520the%2520technique.%2520Experiments%2520on%2520six%2520datasets%2520and%2520two%2520face%250Amatchers%2520establish%2520the%2520utility%2520and%2520efficacy%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14527v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=diffDemorph%3A%20Extending%20Reference-Free%20Demorphing%20to%20Unseen%20Faces&entry.906535625=Nitish%20Shukla%20and%20Arun%20Ross&entry.1292438233=%20%20A%20face%20morph%20is%20created%20by%20combining%20two%20face%20images%20corresponding%20to%20two%0Aidentities%20to%20produce%20a%20composite%20that%20successfully%20matches%20both%20the%0Aconstituent%20identities.%20Reference-free%20%28RF%29%20demorphing%20reverses%20this%20process%0Ausing%20only%20the%20morph%20image%2C%20without%20the%20need%20for%20additional%20reference%20images.%0APrevious%20RF%20demorphing%20methods%20are%20overly%20constrained%2C%20as%20they%20rely%20on%0Aassumptions%20about%20the%20distributions%20of%20training%20and%20testing%20morphs%20such%20as%20the%0Amorphing%20technique%20used%20%28e.g.%2C%20landmark-based%29%20and%20face%20image%20style%20%28e.g.%2C%0Apassport%20photos%29.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20diffusion-based%20approach%2C%0Areferred%20to%20as%20diffDeMorph%2C%20that%20effectively%20disentangles%20component%20images%20from%0Aa%20composite%20morph%20image%20with%20high%20visual%20fidelity.%20Our%20method%20is%20the%20first%20to%0Ageneralize%20across%20morph%20techniques%20and%20face%20styles%2C%20beating%20the%20current%20state%0Aof%20the%20art%20by%20%24%5Cgeq%2059.46%5C%25%24%20under%20a%20common%20training%20protocol%20across%20all%0Adatasets%20tested.%20We%20train%20our%20method%20on%20morphs%20created%20using%20synthetically%0Agenerated%20face%20images%20and%20test%20on%20real%20morphs%2C%20thereby%20enhancing%20the%0Apracticality%20of%20the%20technique.%20Experiments%20on%20six%20datasets%20and%20two%20face%0Amatchers%20establish%20the%20utility%20and%20efficacy%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14527v3&entry.124074799=Read"},
{"title": "Model-Driven Graph Contrastive Learning", "author": "Ali Azizpour and Nicolas Zilberstein and Santiago Segarra", "abstract": "  We propose $\\textbf{MGCL}$, a model-driven graph contrastive learning (GCL)\nframework that leverages graphons (probabilistic generative models for graphs)\nto guide contrastive learning by accounting for the data's underlying\ngenerative process. GCL has emerged as a powerful self-supervised framework for\nlearning expressive node or graph representations without relying on annotated\nlabels, which are often scarce in real-world data. By contrasting augmented\nviews of graph data, GCL has demonstrated strong performance across various\ndownstream tasks, such as node and graph classification. However, existing\nmethods typically rely on manually designed or heuristic augmentation\nstrategies that are not tailored to the underlying data distribution and\noperate at the individual graph level, ignoring similarities among graphs\ngenerated from the same model. Conversely, in our proposed approach, MGCL first\nestimates the graphon associated with the observed data and then defines a\ngraphon-informed augmentation process, enabling data-adaptive and principled\naugmentations. Additionally, for graph-level tasks, MGCL clusters the dataset\nand estimates a graphon per group, enabling contrastive pairs to reflect shared\nsemantics and structure. Extensive experiments on benchmark datasets\ndemonstrate that MGCL achieves state-of-the-art performance, highlighting the\nadvantages of incorporating generative models into GCL.\n", "link": "http://arxiv.org/abs/2506.06212v1", "date": "2025-06-06", "relevancy": 2.1416, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5692}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5358}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5015}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model-Driven%20Graph%20Contrastive%20Learning&body=Title%3A%20Model-Driven%20Graph%20Contrastive%20Learning%0AAuthor%3A%20Ali%20Azizpour%20and%20Nicolas%20Zilberstein%20and%20Santiago%20Segarra%0AAbstract%3A%20%20%20We%20propose%20%24%5Ctextbf%7BMGCL%7D%24%2C%20a%20model-driven%20graph%20contrastive%20learning%20%28GCL%29%0Aframework%20that%20leverages%20graphons%20%28probabilistic%20generative%20models%20for%20graphs%29%0Ato%20guide%20contrastive%20learning%20by%20accounting%20for%20the%20data%27s%20underlying%0Agenerative%20process.%20GCL%20has%20emerged%20as%20a%20powerful%20self-supervised%20framework%20for%0Alearning%20expressive%20node%20or%20graph%20representations%20without%20relying%20on%20annotated%0Alabels%2C%20which%20are%20often%20scarce%20in%20real-world%20data.%20By%20contrasting%20augmented%0Aviews%20of%20graph%20data%2C%20GCL%20has%20demonstrated%20strong%20performance%20across%20various%0Adownstream%20tasks%2C%20such%20as%20node%20and%20graph%20classification.%20However%2C%20existing%0Amethods%20typically%20rely%20on%20manually%20designed%20or%20heuristic%20augmentation%0Astrategies%20that%20are%20not%20tailored%20to%20the%20underlying%20data%20distribution%20and%0Aoperate%20at%20the%20individual%20graph%20level%2C%20ignoring%20similarities%20among%20graphs%0Agenerated%20from%20the%20same%20model.%20Conversely%2C%20in%20our%20proposed%20approach%2C%20MGCL%20first%0Aestimates%20the%20graphon%20associated%20with%20the%20observed%20data%20and%20then%20defines%20a%0Agraphon-informed%20augmentation%20process%2C%20enabling%20data-adaptive%20and%20principled%0Aaugmentations.%20Additionally%2C%20for%20graph-level%20tasks%2C%20MGCL%20clusters%20the%20dataset%0Aand%20estimates%20a%20graphon%20per%20group%2C%20enabling%20contrastive%20pairs%20to%20reflect%20shared%0Asemantics%20and%20structure.%20Extensive%20experiments%20on%20benchmark%20datasets%0Ademonstrate%20that%20MGCL%20achieves%20state-of-the-art%20performance%2C%20highlighting%20the%0Aadvantages%20of%20incorporating%20generative%20models%20into%20GCL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06212v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel-Driven%2520Graph%2520Contrastive%2520Learning%26entry.906535625%3DAli%2520Azizpour%2520and%2520Nicolas%2520Zilberstein%2520and%2520Santiago%2520Segarra%26entry.1292438233%3D%2520%2520We%2520propose%2520%2524%255Ctextbf%257BMGCL%257D%2524%252C%2520a%2520model-driven%2520graph%2520contrastive%2520learning%2520%2528GCL%2529%250Aframework%2520that%2520leverages%2520graphons%2520%2528probabilistic%2520generative%2520models%2520for%2520graphs%2529%250Ato%2520guide%2520contrastive%2520learning%2520by%2520accounting%2520for%2520the%2520data%2527s%2520underlying%250Agenerative%2520process.%2520GCL%2520has%2520emerged%2520as%2520a%2520powerful%2520self-supervised%2520framework%2520for%250Alearning%2520expressive%2520node%2520or%2520graph%2520representations%2520without%2520relying%2520on%2520annotated%250Alabels%252C%2520which%2520are%2520often%2520scarce%2520in%2520real-world%2520data.%2520By%2520contrasting%2520augmented%250Aviews%2520of%2520graph%2520data%252C%2520GCL%2520has%2520demonstrated%2520strong%2520performance%2520across%2520various%250Adownstream%2520tasks%252C%2520such%2520as%2520node%2520and%2520graph%2520classification.%2520However%252C%2520existing%250Amethods%2520typically%2520rely%2520on%2520manually%2520designed%2520or%2520heuristic%2520augmentation%250Astrategies%2520that%2520are%2520not%2520tailored%2520to%2520the%2520underlying%2520data%2520distribution%2520and%250Aoperate%2520at%2520the%2520individual%2520graph%2520level%252C%2520ignoring%2520similarities%2520among%2520graphs%250Agenerated%2520from%2520the%2520same%2520model.%2520Conversely%252C%2520in%2520our%2520proposed%2520approach%252C%2520MGCL%2520first%250Aestimates%2520the%2520graphon%2520associated%2520with%2520the%2520observed%2520data%2520and%2520then%2520defines%2520a%250Agraphon-informed%2520augmentation%2520process%252C%2520enabling%2520data-adaptive%2520and%2520principled%250Aaugmentations.%2520Additionally%252C%2520for%2520graph-level%2520tasks%252C%2520MGCL%2520clusters%2520the%2520dataset%250Aand%2520estimates%2520a%2520graphon%2520per%2520group%252C%2520enabling%2520contrastive%2520pairs%2520to%2520reflect%2520shared%250Asemantics%2520and%2520structure.%2520Extensive%2520experiments%2520on%2520benchmark%2520datasets%250Ademonstrate%2520that%2520MGCL%2520achieves%2520state-of-the-art%2520performance%252C%2520highlighting%2520the%250Aadvantages%2520of%2520incorporating%2520generative%2520models%2520into%2520GCL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06212v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model-Driven%20Graph%20Contrastive%20Learning&entry.906535625=Ali%20Azizpour%20and%20Nicolas%20Zilberstein%20and%20Santiago%20Segarra&entry.1292438233=%20%20We%20propose%20%24%5Ctextbf%7BMGCL%7D%24%2C%20a%20model-driven%20graph%20contrastive%20learning%20%28GCL%29%0Aframework%20that%20leverages%20graphons%20%28probabilistic%20generative%20models%20for%20graphs%29%0Ato%20guide%20contrastive%20learning%20by%20accounting%20for%20the%20data%27s%20underlying%0Agenerative%20process.%20GCL%20has%20emerged%20as%20a%20powerful%20self-supervised%20framework%20for%0Alearning%20expressive%20node%20or%20graph%20representations%20without%20relying%20on%20annotated%0Alabels%2C%20which%20are%20often%20scarce%20in%20real-world%20data.%20By%20contrasting%20augmented%0Aviews%20of%20graph%20data%2C%20GCL%20has%20demonstrated%20strong%20performance%20across%20various%0Adownstream%20tasks%2C%20such%20as%20node%20and%20graph%20classification.%20However%2C%20existing%0Amethods%20typically%20rely%20on%20manually%20designed%20or%20heuristic%20augmentation%0Astrategies%20that%20are%20not%20tailored%20to%20the%20underlying%20data%20distribution%20and%0Aoperate%20at%20the%20individual%20graph%20level%2C%20ignoring%20similarities%20among%20graphs%0Agenerated%20from%20the%20same%20model.%20Conversely%2C%20in%20our%20proposed%20approach%2C%20MGCL%20first%0Aestimates%20the%20graphon%20associated%20with%20the%20observed%20data%20and%20then%20defines%20a%0Agraphon-informed%20augmentation%20process%2C%20enabling%20data-adaptive%20and%20principled%0Aaugmentations.%20Additionally%2C%20for%20graph-level%20tasks%2C%20MGCL%20clusters%20the%20dataset%0Aand%20estimates%20a%20graphon%20per%20group%2C%20enabling%20contrastive%20pairs%20to%20reflect%20shared%0Asemantics%20and%20structure.%20Extensive%20experiments%20on%20benchmark%20datasets%0Ademonstrate%20that%20MGCL%20achieves%20state-of-the-art%20performance%2C%20highlighting%20the%0Aadvantages%20of%20incorporating%20generative%20models%20into%20GCL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06212v1&entry.124074799=Read"},
{"title": "ExAct: A Video-Language Benchmark for Expert Action Analysis", "author": "Han Yi and Yulu Pan and Feihong He and Xinyu Liu and Benjamin Zhang and Oluwatumininu Oguntola and Gedas Bertasius", "abstract": "  We present ExAct, a new video-language benchmark for expert-level\nunderstanding of skilled physical human activities. Our new benchmark contains\n3521 expert-curated video question-answer pairs spanning 11 physical activities\nin 6 domains: Sports, Bike Repair, Cooking, Health, Music, and Dance. ExAct\nrequires the correct answer to be selected from five carefully designed\ncandidate options, thus necessitating a nuanced, fine-grained, expert-level\nunderstanding of physical human skills. Evaluating the recent state-of-the-art\nVLMs on ExAct reveals a substantial performance gap relative to human expert\nperformance. Specifically, the best-performing GPT-4o model achieves only\n44.70% accuracy, well below the 82.02% attained by trained human\nspecialists/experts. We believe that ExAct will be beneficial for developing\nand evaluating VLMs capable of precise understanding of human skills in various\nphysical and procedural domains. Dataset and code are available at\nhttps://texaser.github.io/exact_project_page/\n", "link": "http://arxiv.org/abs/2506.06277v1", "date": "2025-06-06", "relevancy": 2.1374, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5351}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5351}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ExAct%3A%20A%20Video-Language%20Benchmark%20for%20Expert%20Action%20Analysis&body=Title%3A%20ExAct%3A%20A%20Video-Language%20Benchmark%20for%20Expert%20Action%20Analysis%0AAuthor%3A%20Han%20Yi%20and%20Yulu%20Pan%20and%20Feihong%20He%20and%20Xinyu%20Liu%20and%20Benjamin%20Zhang%20and%20Oluwatumininu%20Oguntola%20and%20Gedas%20Bertasius%0AAbstract%3A%20%20%20We%20present%20ExAct%2C%20a%20new%20video-language%20benchmark%20for%20expert-level%0Aunderstanding%20of%20skilled%20physical%20human%20activities.%20Our%20new%20benchmark%20contains%0A3521%20expert-curated%20video%20question-answer%20pairs%20spanning%2011%20physical%20activities%0Ain%206%20domains%3A%20Sports%2C%20Bike%20Repair%2C%20Cooking%2C%20Health%2C%20Music%2C%20and%20Dance.%20ExAct%0Arequires%20the%20correct%20answer%20to%20be%20selected%20from%20five%20carefully%20designed%0Acandidate%20options%2C%20thus%20necessitating%20a%20nuanced%2C%20fine-grained%2C%20expert-level%0Aunderstanding%20of%20physical%20human%20skills.%20Evaluating%20the%20recent%20state-of-the-art%0AVLMs%20on%20ExAct%20reveals%20a%20substantial%20performance%20gap%20relative%20to%20human%20expert%0Aperformance.%20Specifically%2C%20the%20best-performing%20GPT-4o%20model%20achieves%20only%0A44.70%25%20accuracy%2C%20well%20below%20the%2082.02%25%20attained%20by%20trained%20human%0Aspecialists/experts.%20We%20believe%20that%20ExAct%20will%20be%20beneficial%20for%20developing%0Aand%20evaluating%20VLMs%20capable%20of%20precise%20understanding%20of%20human%20skills%20in%20various%0Aphysical%20and%20procedural%20domains.%20Dataset%20and%20code%20are%20available%20at%0Ahttps%3A//texaser.github.io/exact_project_page/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06277v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExAct%253A%2520A%2520Video-Language%2520Benchmark%2520for%2520Expert%2520Action%2520Analysis%26entry.906535625%3DHan%2520Yi%2520and%2520Yulu%2520Pan%2520and%2520Feihong%2520He%2520and%2520Xinyu%2520Liu%2520and%2520Benjamin%2520Zhang%2520and%2520Oluwatumininu%2520Oguntola%2520and%2520Gedas%2520Bertasius%26entry.1292438233%3D%2520%2520We%2520present%2520ExAct%252C%2520a%2520new%2520video-language%2520benchmark%2520for%2520expert-level%250Aunderstanding%2520of%2520skilled%2520physical%2520human%2520activities.%2520Our%2520new%2520benchmark%2520contains%250A3521%2520expert-curated%2520video%2520question-answer%2520pairs%2520spanning%252011%2520physical%2520activities%250Ain%25206%2520domains%253A%2520Sports%252C%2520Bike%2520Repair%252C%2520Cooking%252C%2520Health%252C%2520Music%252C%2520and%2520Dance.%2520ExAct%250Arequires%2520the%2520correct%2520answer%2520to%2520be%2520selected%2520from%2520five%2520carefully%2520designed%250Acandidate%2520options%252C%2520thus%2520necessitating%2520a%2520nuanced%252C%2520fine-grained%252C%2520expert-level%250Aunderstanding%2520of%2520physical%2520human%2520skills.%2520Evaluating%2520the%2520recent%2520state-of-the-art%250AVLMs%2520on%2520ExAct%2520reveals%2520a%2520substantial%2520performance%2520gap%2520relative%2520to%2520human%2520expert%250Aperformance.%2520Specifically%252C%2520the%2520best-performing%2520GPT-4o%2520model%2520achieves%2520only%250A44.70%2525%2520accuracy%252C%2520well%2520below%2520the%252082.02%2525%2520attained%2520by%2520trained%2520human%250Aspecialists/experts.%2520We%2520believe%2520that%2520ExAct%2520will%2520be%2520beneficial%2520for%2520developing%250Aand%2520evaluating%2520VLMs%2520capable%2520of%2520precise%2520understanding%2520of%2520human%2520skills%2520in%2520various%250Aphysical%2520and%2520procedural%2520domains.%2520Dataset%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//texaser.github.io/exact_project_page/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06277v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ExAct%3A%20A%20Video-Language%20Benchmark%20for%20Expert%20Action%20Analysis&entry.906535625=Han%20Yi%20and%20Yulu%20Pan%20and%20Feihong%20He%20and%20Xinyu%20Liu%20and%20Benjamin%20Zhang%20and%20Oluwatumininu%20Oguntola%20and%20Gedas%20Bertasius&entry.1292438233=%20%20We%20present%20ExAct%2C%20a%20new%20video-language%20benchmark%20for%20expert-level%0Aunderstanding%20of%20skilled%20physical%20human%20activities.%20Our%20new%20benchmark%20contains%0A3521%20expert-curated%20video%20question-answer%20pairs%20spanning%2011%20physical%20activities%0Ain%206%20domains%3A%20Sports%2C%20Bike%20Repair%2C%20Cooking%2C%20Health%2C%20Music%2C%20and%20Dance.%20ExAct%0Arequires%20the%20correct%20answer%20to%20be%20selected%20from%20five%20carefully%20designed%0Acandidate%20options%2C%20thus%20necessitating%20a%20nuanced%2C%20fine-grained%2C%20expert-level%0Aunderstanding%20of%20physical%20human%20skills.%20Evaluating%20the%20recent%20state-of-the-art%0AVLMs%20on%20ExAct%20reveals%20a%20substantial%20performance%20gap%20relative%20to%20human%20expert%0Aperformance.%20Specifically%2C%20the%20best-performing%20GPT-4o%20model%20achieves%20only%0A44.70%25%20accuracy%2C%20well%20below%20the%2082.02%25%20attained%20by%20trained%20human%0Aspecialists/experts.%20We%20believe%20that%20ExAct%20will%20be%20beneficial%20for%20developing%0Aand%20evaluating%20VLMs%20capable%20of%20precise%20understanding%20of%20human%20skills%20in%20various%0Aphysical%20and%20procedural%20domains.%20Dataset%20and%20code%20are%20available%20at%0Ahttps%3A//texaser.github.io/exact_project_page/%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06277v1&entry.124074799=Read"},
{"title": "Phonetically-Augmented Discriminative Rescoring for Voice Search Error\n  Correction", "author": "Christophe Van Gysel and Maggie Wu and Lyan Verwimp and Caglar Tirkaz and Marco Bertola and Zhihong Lei and Youssef Oualil", "abstract": "  End-to-end (E2E) Automatic Speech Recognition (ASR) models are trained using\npaired audio-text samples that are expensive to obtain, since high-quality\nground-truth data requires human annotators. Voice search applications, such as\ndigital media players, leverage ASR to allow users to search by voice as\nopposed to an on-screen keyboard. However, recent or infrequent movie titles\nmay not be sufficiently represented in the E2E ASR system's training data, and\nhence, may suffer poor recognition.\n  In this paper, we propose a phonetic correction system that consists of (a) a\nphonetic search based on the ASR model's output that generates phonetic\nalternatives that may not be considered by the E2E system, and (b) a rescorer\ncomponent that combines the ASR model recognition and the phonetic\nalternatives, and select a final system output.\n  We find that our approach improves word error rate between 4.4 and 7.6%\nrelative on benchmarks of popular movie titles over a series of competitive\nbaselines.\n", "link": "http://arxiv.org/abs/2506.06117v1", "date": "2025-06-06", "relevancy": 2.1368, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4396}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4213}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Phonetically-Augmented%20Discriminative%20Rescoring%20for%20Voice%20Search%20Error%0A%20%20Correction&body=Title%3A%20Phonetically-Augmented%20Discriminative%20Rescoring%20for%20Voice%20Search%20Error%0A%20%20Correction%0AAuthor%3A%20Christophe%20Van%20Gysel%20and%20Maggie%20Wu%20and%20Lyan%20Verwimp%20and%20Caglar%20Tirkaz%20and%20Marco%20Bertola%20and%20Zhihong%20Lei%20and%20Youssef%20Oualil%0AAbstract%3A%20%20%20End-to-end%20%28E2E%29%20Automatic%20Speech%20Recognition%20%28ASR%29%20models%20are%20trained%20using%0Apaired%20audio-text%20samples%20that%20are%20expensive%20to%20obtain%2C%20since%20high-quality%0Aground-truth%20data%20requires%20human%20annotators.%20Voice%20search%20applications%2C%20such%20as%0Adigital%20media%20players%2C%20leverage%20ASR%20to%20allow%20users%20to%20search%20by%20voice%20as%0Aopposed%20to%20an%20on-screen%20keyboard.%20However%2C%20recent%20or%20infrequent%20movie%20titles%0Amay%20not%20be%20sufficiently%20represented%20in%20the%20E2E%20ASR%20system%27s%20training%20data%2C%20and%0Ahence%2C%20may%20suffer%20poor%20recognition.%0A%20%20In%20this%20paper%2C%20we%20propose%20a%20phonetic%20correction%20system%20that%20consists%20of%20%28a%29%20a%0Aphonetic%20search%20based%20on%20the%20ASR%20model%27s%20output%20that%20generates%20phonetic%0Aalternatives%20that%20may%20not%20be%20considered%20by%20the%20E2E%20system%2C%20and%20%28b%29%20a%20rescorer%0Acomponent%20that%20combines%20the%20ASR%20model%20recognition%20and%20the%20phonetic%0Aalternatives%2C%20and%20select%20a%20final%20system%20output.%0A%20%20We%20find%20that%20our%20approach%20improves%20word%20error%20rate%20between%204.4%20and%207.6%25%0Arelative%20on%20benchmarks%20of%20popular%20movie%20titles%20over%20a%20series%20of%20competitive%0Abaselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06117v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhonetically-Augmented%2520Discriminative%2520Rescoring%2520for%2520Voice%2520Search%2520Error%250A%2520%2520Correction%26entry.906535625%3DChristophe%2520Van%2520Gysel%2520and%2520Maggie%2520Wu%2520and%2520Lyan%2520Verwimp%2520and%2520Caglar%2520Tirkaz%2520and%2520Marco%2520Bertola%2520and%2520Zhihong%2520Lei%2520and%2520Youssef%2520Oualil%26entry.1292438233%3D%2520%2520End-to-end%2520%2528E2E%2529%2520Automatic%2520Speech%2520Recognition%2520%2528ASR%2529%2520models%2520are%2520trained%2520using%250Apaired%2520audio-text%2520samples%2520that%2520are%2520expensive%2520to%2520obtain%252C%2520since%2520high-quality%250Aground-truth%2520data%2520requires%2520human%2520annotators.%2520Voice%2520search%2520applications%252C%2520such%2520as%250Adigital%2520media%2520players%252C%2520leverage%2520ASR%2520to%2520allow%2520users%2520to%2520search%2520by%2520voice%2520as%250Aopposed%2520to%2520an%2520on-screen%2520keyboard.%2520However%252C%2520recent%2520or%2520infrequent%2520movie%2520titles%250Amay%2520not%2520be%2520sufficiently%2520represented%2520in%2520the%2520E2E%2520ASR%2520system%2527s%2520training%2520data%252C%2520and%250Ahence%252C%2520may%2520suffer%2520poor%2520recognition.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520phonetic%2520correction%2520system%2520that%2520consists%2520of%2520%2528a%2529%2520a%250Aphonetic%2520search%2520based%2520on%2520the%2520ASR%2520model%2527s%2520output%2520that%2520generates%2520phonetic%250Aalternatives%2520that%2520may%2520not%2520be%2520considered%2520by%2520the%2520E2E%2520system%252C%2520and%2520%2528b%2529%2520a%2520rescorer%250Acomponent%2520that%2520combines%2520the%2520ASR%2520model%2520recognition%2520and%2520the%2520phonetic%250Aalternatives%252C%2520and%2520select%2520a%2520final%2520system%2520output.%250A%2520%2520We%2520find%2520that%2520our%2520approach%2520improves%2520word%2520error%2520rate%2520between%25204.4%2520and%25207.6%2525%250Arelative%2520on%2520benchmarks%2520of%2520popular%2520movie%2520titles%2520over%2520a%2520series%2520of%2520competitive%250Abaselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06117v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Phonetically-Augmented%20Discriminative%20Rescoring%20for%20Voice%20Search%20Error%0A%20%20Correction&entry.906535625=Christophe%20Van%20Gysel%20and%20Maggie%20Wu%20and%20Lyan%20Verwimp%20and%20Caglar%20Tirkaz%20and%20Marco%20Bertola%20and%20Zhihong%20Lei%20and%20Youssef%20Oualil&entry.1292438233=%20%20End-to-end%20%28E2E%29%20Automatic%20Speech%20Recognition%20%28ASR%29%20models%20are%20trained%20using%0Apaired%20audio-text%20samples%20that%20are%20expensive%20to%20obtain%2C%20since%20high-quality%0Aground-truth%20data%20requires%20human%20annotators.%20Voice%20search%20applications%2C%20such%20as%0Adigital%20media%20players%2C%20leverage%20ASR%20to%20allow%20users%20to%20search%20by%20voice%20as%0Aopposed%20to%20an%20on-screen%20keyboard.%20However%2C%20recent%20or%20infrequent%20movie%20titles%0Amay%20not%20be%20sufficiently%20represented%20in%20the%20E2E%20ASR%20system%27s%20training%20data%2C%20and%0Ahence%2C%20may%20suffer%20poor%20recognition.%0A%20%20In%20this%20paper%2C%20we%20propose%20a%20phonetic%20correction%20system%20that%20consists%20of%20%28a%29%20a%0Aphonetic%20search%20based%20on%20the%20ASR%20model%27s%20output%20that%20generates%20phonetic%0Aalternatives%20that%20may%20not%20be%20considered%20by%20the%20E2E%20system%2C%20and%20%28b%29%20a%20rescorer%0Acomponent%20that%20combines%20the%20ASR%20model%20recognition%20and%20the%20phonetic%0Aalternatives%2C%20and%20select%20a%20final%20system%20output.%0A%20%20We%20find%20that%20our%20approach%20improves%20word%20error%20rate%20between%204.4%20and%207.6%25%0Arelative%20on%20benchmarks%20of%20popular%20movie%20titles%20over%20a%20series%20of%20competitive%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06117v1&entry.124074799=Read"},
{"title": "ENMA: Tokenwise Autoregression for Generative Neural PDE Operators", "author": "Armand Kassa\u00ef Koupa\u00ef and Lise Le Boudec and Louis Serrano and Patrick Gallinari", "abstract": "  Solving time-dependent parametric partial differential equations (PDEs)\nremains a fundamental challenge for neural solvers, particularly when\ngeneralizing across a wide range of physical parameters and dynamics. When data\nis uncertain or incomplete-as is often the case-a natural approach is to turn\nto generative models. We introduce ENMA, a generative neural operator designed\nto model spatio-temporal dynamics arising from physical phenomena. ENMA\npredicts future dynamics in a compressed latent space using a generative masked\nautoregressive transformer trained with flow matching loss, enabling tokenwise\ngeneration. Irregularly sampled spatial observations are encoded into uniform\nlatent representations via attention mechanisms and further compressed through\na spatio-temporal convolutional encoder. This allows ENMA to perform in-context\nlearning at inference time by conditioning on either past states of the target\ntrajectory or auxiliary context trajectories with similar dynamics. The result\nis a robust and adaptable framework that generalizes to new PDE regimes and\nsupports one-shot surrogate modeling of time-dependent parametric PDEs.\n", "link": "http://arxiv.org/abs/2506.06158v1", "date": "2025-06-06", "relevancy": 2.1354, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5581}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5296}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ENMA%3A%20Tokenwise%20Autoregression%20for%20Generative%20Neural%20PDE%20Operators&body=Title%3A%20ENMA%3A%20Tokenwise%20Autoregression%20for%20Generative%20Neural%20PDE%20Operators%0AAuthor%3A%20Armand%20Kassa%C3%AF%20Koupa%C3%AF%20and%20Lise%20Le%20Boudec%20and%20Louis%20Serrano%20and%20Patrick%20Gallinari%0AAbstract%3A%20%20%20Solving%20time-dependent%20parametric%20partial%20differential%20equations%20%28PDEs%29%0Aremains%20a%20fundamental%20challenge%20for%20neural%20solvers%2C%20particularly%20when%0Ageneralizing%20across%20a%20wide%20range%20of%20physical%20parameters%20and%20dynamics.%20When%20data%0Ais%20uncertain%20or%20incomplete-as%20is%20often%20the%20case-a%20natural%20approach%20is%20to%20turn%0Ato%20generative%20models.%20We%20introduce%20ENMA%2C%20a%20generative%20neural%20operator%20designed%0Ato%20model%20spatio-temporal%20dynamics%20arising%20from%20physical%20phenomena.%20ENMA%0Apredicts%20future%20dynamics%20in%20a%20compressed%20latent%20space%20using%20a%20generative%20masked%0Aautoregressive%20transformer%20trained%20with%20flow%20matching%20loss%2C%20enabling%20tokenwise%0Ageneration.%20Irregularly%20sampled%20spatial%20observations%20are%20encoded%20into%20uniform%0Alatent%20representations%20via%20attention%20mechanisms%20and%20further%20compressed%20through%0Aa%20spatio-temporal%20convolutional%20encoder.%20This%20allows%20ENMA%20to%20perform%20in-context%0Alearning%20at%20inference%20time%20by%20conditioning%20on%20either%20past%20states%20of%20the%20target%0Atrajectory%20or%20auxiliary%20context%20trajectories%20with%20similar%20dynamics.%20The%20result%0Ais%20a%20robust%20and%20adaptable%20framework%20that%20generalizes%20to%20new%20PDE%20regimes%20and%0Asupports%20one-shot%20surrogate%20modeling%20of%20time-dependent%20parametric%20PDEs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06158v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DENMA%253A%2520Tokenwise%2520Autoregression%2520for%2520Generative%2520Neural%2520PDE%2520Operators%26entry.906535625%3DArmand%2520Kassa%25C3%25AF%2520Koupa%25C3%25AF%2520and%2520Lise%2520Le%2520Boudec%2520and%2520Louis%2520Serrano%2520and%2520Patrick%2520Gallinari%26entry.1292438233%3D%2520%2520Solving%2520time-dependent%2520parametric%2520partial%2520differential%2520equations%2520%2528PDEs%2529%250Aremains%2520a%2520fundamental%2520challenge%2520for%2520neural%2520solvers%252C%2520particularly%2520when%250Ageneralizing%2520across%2520a%2520wide%2520range%2520of%2520physical%2520parameters%2520and%2520dynamics.%2520When%2520data%250Ais%2520uncertain%2520or%2520incomplete-as%2520is%2520often%2520the%2520case-a%2520natural%2520approach%2520is%2520to%2520turn%250Ato%2520generative%2520models.%2520We%2520introduce%2520ENMA%252C%2520a%2520generative%2520neural%2520operator%2520designed%250Ato%2520model%2520spatio-temporal%2520dynamics%2520arising%2520from%2520physical%2520phenomena.%2520ENMA%250Apredicts%2520future%2520dynamics%2520in%2520a%2520compressed%2520latent%2520space%2520using%2520a%2520generative%2520masked%250Aautoregressive%2520transformer%2520trained%2520with%2520flow%2520matching%2520loss%252C%2520enabling%2520tokenwise%250Ageneration.%2520Irregularly%2520sampled%2520spatial%2520observations%2520are%2520encoded%2520into%2520uniform%250Alatent%2520representations%2520via%2520attention%2520mechanisms%2520and%2520further%2520compressed%2520through%250Aa%2520spatio-temporal%2520convolutional%2520encoder.%2520This%2520allows%2520ENMA%2520to%2520perform%2520in-context%250Alearning%2520at%2520inference%2520time%2520by%2520conditioning%2520on%2520either%2520past%2520states%2520of%2520the%2520target%250Atrajectory%2520or%2520auxiliary%2520context%2520trajectories%2520with%2520similar%2520dynamics.%2520The%2520result%250Ais%2520a%2520robust%2520and%2520adaptable%2520framework%2520that%2520generalizes%2520to%2520new%2520PDE%2520regimes%2520and%250Asupports%2520one-shot%2520surrogate%2520modeling%2520of%2520time-dependent%2520parametric%2520PDEs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06158v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ENMA%3A%20Tokenwise%20Autoregression%20for%20Generative%20Neural%20PDE%20Operators&entry.906535625=Armand%20Kassa%C3%AF%20Koupa%C3%AF%20and%20Lise%20Le%20Boudec%20and%20Louis%20Serrano%20and%20Patrick%20Gallinari&entry.1292438233=%20%20Solving%20time-dependent%20parametric%20partial%20differential%20equations%20%28PDEs%29%0Aremains%20a%20fundamental%20challenge%20for%20neural%20solvers%2C%20particularly%20when%0Ageneralizing%20across%20a%20wide%20range%20of%20physical%20parameters%20and%20dynamics.%20When%20data%0Ais%20uncertain%20or%20incomplete-as%20is%20often%20the%20case-a%20natural%20approach%20is%20to%20turn%0Ato%20generative%20models.%20We%20introduce%20ENMA%2C%20a%20generative%20neural%20operator%20designed%0Ato%20model%20spatio-temporal%20dynamics%20arising%20from%20physical%20phenomena.%20ENMA%0Apredicts%20future%20dynamics%20in%20a%20compressed%20latent%20space%20using%20a%20generative%20masked%0Aautoregressive%20transformer%20trained%20with%20flow%20matching%20loss%2C%20enabling%20tokenwise%0Ageneration.%20Irregularly%20sampled%20spatial%20observations%20are%20encoded%20into%20uniform%0Alatent%20representations%20via%20attention%20mechanisms%20and%20further%20compressed%20through%0Aa%20spatio-temporal%20convolutional%20encoder.%20This%20allows%20ENMA%20to%20perform%20in-context%0Alearning%20at%20inference%20time%20by%20conditioning%20on%20either%20past%20states%20of%20the%20target%0Atrajectory%20or%20auxiliary%20context%20trajectories%20with%20similar%20dynamics.%20The%20result%0Ais%20a%20robust%20and%20adaptable%20framework%20that%20generalizes%20to%20new%20PDE%20regimes%20and%0Asupports%20one-shot%20surrogate%20modeling%20of%20time-dependent%20parametric%20PDEs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06158v1&entry.124074799=Read"},
{"title": "ARMOR: Empowering Multimodal Understanding Model with Interleaved\n  Multimodal Generation Capability", "author": "Jianwen Sun and Yukang Feng and Chuanhao Li and Fanrui Zhang and Zizhen Li and Jiaxin Ai and Sizhuo Zhou and Yu Dai and Shenglin Zhang and Kaipeng Zhang", "abstract": "  Unified multimodal understanding and generation have recently received much\nattention in the area of vision and language. Existing UniMs are designed to\nsimultaneously learn both multimodal understanding and generation capabilities,\ndemanding substantial computational resources, and often struggle to generate\ninterleaved text-image. We present ARMOR, a resource-efficient and pure\nautoregressive framework that achieves both understanding and generation by\nfine-tuning existing multimodal large language models (MLLMs). Specifically,\nARMOR extends existing MLLMs from three perspectives: (1) For model\narchitecture, an asymmetric encoder-decoder architecture with a\nforward-switching mechanism is introduced to unify embedding space integrating\ntextual and visual modalities for enabling natural text-image interleaved\ngeneration with minimal computational overhead. (2) For training data, a\nmeticulously curated, high-quality interleaved dataset is collected for\nfine-tuning MLLMs. (3) For the training algorithm, we propose a ``what or how\nto generate'' algorithm to empower existing MLLMs with multimodal generation\ncapabilities while preserving their multimodal understanding capabilities,\nthrough three progressive training stages based on the collected dataset.\nExperimental results demonstrate that ARMOR upgrades existing MLLMs to UniMs\nwith promising image generation capabilities, using limited training resources.\nOur code will be released soon at https://github.com/finyorko/armor.\n", "link": "http://arxiv.org/abs/2503.06542v2", "date": "2025-06-06", "relevancy": 2.1283, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5673}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5111}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ARMOR%3A%20Empowering%20Multimodal%20Understanding%20Model%20with%20Interleaved%0A%20%20Multimodal%20Generation%20Capability&body=Title%3A%20ARMOR%3A%20Empowering%20Multimodal%20Understanding%20Model%20with%20Interleaved%0A%20%20Multimodal%20Generation%20Capability%0AAuthor%3A%20Jianwen%20Sun%20and%20Yukang%20Feng%20and%20Chuanhao%20Li%20and%20Fanrui%20Zhang%20and%20Zizhen%20Li%20and%20Jiaxin%20Ai%20and%20Sizhuo%20Zhou%20and%20Yu%20Dai%20and%20Shenglin%20Zhang%20and%20Kaipeng%20Zhang%0AAbstract%3A%20%20%20Unified%20multimodal%20understanding%20and%20generation%20have%20recently%20received%20much%0Aattention%20in%20the%20area%20of%20vision%20and%20language.%20Existing%20UniMs%20are%20designed%20to%0Asimultaneously%20learn%20both%20multimodal%20understanding%20and%20generation%20capabilities%2C%0Ademanding%20substantial%20computational%20resources%2C%20and%20often%20struggle%20to%20generate%0Ainterleaved%20text-image.%20We%20present%20ARMOR%2C%20a%20resource-efficient%20and%20pure%0Aautoregressive%20framework%20that%20achieves%20both%20understanding%20and%20generation%20by%0Afine-tuning%20existing%20multimodal%20large%20language%20models%20%28MLLMs%29.%20Specifically%2C%0AARMOR%20extends%20existing%20MLLMs%20from%20three%20perspectives%3A%20%281%29%20For%20model%0Aarchitecture%2C%20an%20asymmetric%20encoder-decoder%20architecture%20with%20a%0Aforward-switching%20mechanism%20is%20introduced%20to%20unify%20embedding%20space%20integrating%0Atextual%20and%20visual%20modalities%20for%20enabling%20natural%20text-image%20interleaved%0Ageneration%20with%20minimal%20computational%20overhead.%20%282%29%20For%20training%20data%2C%20a%0Ameticulously%20curated%2C%20high-quality%20interleaved%20dataset%20is%20collected%20for%0Afine-tuning%20MLLMs.%20%283%29%20For%20the%20training%20algorithm%2C%20we%20propose%20a%20%60%60what%20or%20how%0Ato%20generate%27%27%20algorithm%20to%20empower%20existing%20MLLMs%20with%20multimodal%20generation%0Acapabilities%20while%20preserving%20their%20multimodal%20understanding%20capabilities%2C%0Athrough%20three%20progressive%20training%20stages%20based%20on%20the%20collected%20dataset.%0AExperimental%20results%20demonstrate%20that%20ARMOR%20upgrades%20existing%20MLLMs%20to%20UniMs%0Awith%20promising%20image%20generation%20capabilities%2C%20using%20limited%20training%20resources.%0AOur%20code%20will%20be%20released%20soon%20at%20https%3A//github.com/finyorko/armor.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.06542v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DARMOR%253A%2520Empowering%2520Multimodal%2520Understanding%2520Model%2520with%2520Interleaved%250A%2520%2520Multimodal%2520Generation%2520Capability%26entry.906535625%3DJianwen%2520Sun%2520and%2520Yukang%2520Feng%2520and%2520Chuanhao%2520Li%2520and%2520Fanrui%2520Zhang%2520and%2520Zizhen%2520Li%2520and%2520Jiaxin%2520Ai%2520and%2520Sizhuo%2520Zhou%2520and%2520Yu%2520Dai%2520and%2520Shenglin%2520Zhang%2520and%2520Kaipeng%2520Zhang%26entry.1292438233%3D%2520%2520Unified%2520multimodal%2520understanding%2520and%2520generation%2520have%2520recently%2520received%2520much%250Aattention%2520in%2520the%2520area%2520of%2520vision%2520and%2520language.%2520Existing%2520UniMs%2520are%2520designed%2520to%250Asimultaneously%2520learn%2520both%2520multimodal%2520understanding%2520and%2520generation%2520capabilities%252C%250Ademanding%2520substantial%2520computational%2520resources%252C%2520and%2520often%2520struggle%2520to%2520generate%250Ainterleaved%2520text-image.%2520We%2520present%2520ARMOR%252C%2520a%2520resource-efficient%2520and%2520pure%250Aautoregressive%2520framework%2520that%2520achieves%2520both%2520understanding%2520and%2520generation%2520by%250Afine-tuning%2520existing%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529.%2520Specifically%252C%250AARMOR%2520extends%2520existing%2520MLLMs%2520from%2520three%2520perspectives%253A%2520%25281%2529%2520For%2520model%250Aarchitecture%252C%2520an%2520asymmetric%2520encoder-decoder%2520architecture%2520with%2520a%250Aforward-switching%2520mechanism%2520is%2520introduced%2520to%2520unify%2520embedding%2520space%2520integrating%250Atextual%2520and%2520visual%2520modalities%2520for%2520enabling%2520natural%2520text-image%2520interleaved%250Ageneration%2520with%2520minimal%2520computational%2520overhead.%2520%25282%2529%2520For%2520training%2520data%252C%2520a%250Ameticulously%2520curated%252C%2520high-quality%2520interleaved%2520dataset%2520is%2520collected%2520for%250Afine-tuning%2520MLLMs.%2520%25283%2529%2520For%2520the%2520training%2520algorithm%252C%2520we%2520propose%2520a%2520%2560%2560what%2520or%2520how%250Ato%2520generate%2527%2527%2520algorithm%2520to%2520empower%2520existing%2520MLLMs%2520with%2520multimodal%2520generation%250Acapabilities%2520while%2520preserving%2520their%2520multimodal%2520understanding%2520capabilities%252C%250Athrough%2520three%2520progressive%2520training%2520stages%2520based%2520on%2520the%2520collected%2520dataset.%250AExperimental%2520results%2520demonstrate%2520that%2520ARMOR%2520upgrades%2520existing%2520MLLMs%2520to%2520UniMs%250Awith%2520promising%2520image%2520generation%2520capabilities%252C%2520using%2520limited%2520training%2520resources.%250AOur%2520code%2520will%2520be%2520released%2520soon%2520at%2520https%253A//github.com/finyorko/armor.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.06542v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ARMOR%3A%20Empowering%20Multimodal%20Understanding%20Model%20with%20Interleaved%0A%20%20Multimodal%20Generation%20Capability&entry.906535625=Jianwen%20Sun%20and%20Yukang%20Feng%20and%20Chuanhao%20Li%20and%20Fanrui%20Zhang%20and%20Zizhen%20Li%20and%20Jiaxin%20Ai%20and%20Sizhuo%20Zhou%20and%20Yu%20Dai%20and%20Shenglin%20Zhang%20and%20Kaipeng%20Zhang&entry.1292438233=%20%20Unified%20multimodal%20understanding%20and%20generation%20have%20recently%20received%20much%0Aattention%20in%20the%20area%20of%20vision%20and%20language.%20Existing%20UniMs%20are%20designed%20to%0Asimultaneously%20learn%20both%20multimodal%20understanding%20and%20generation%20capabilities%2C%0Ademanding%20substantial%20computational%20resources%2C%20and%20often%20struggle%20to%20generate%0Ainterleaved%20text-image.%20We%20present%20ARMOR%2C%20a%20resource-efficient%20and%20pure%0Aautoregressive%20framework%20that%20achieves%20both%20understanding%20and%20generation%20by%0Afine-tuning%20existing%20multimodal%20large%20language%20models%20%28MLLMs%29.%20Specifically%2C%0AARMOR%20extends%20existing%20MLLMs%20from%20three%20perspectives%3A%20%281%29%20For%20model%0Aarchitecture%2C%20an%20asymmetric%20encoder-decoder%20architecture%20with%20a%0Aforward-switching%20mechanism%20is%20introduced%20to%20unify%20embedding%20space%20integrating%0Atextual%20and%20visual%20modalities%20for%20enabling%20natural%20text-image%20interleaved%0Ageneration%20with%20minimal%20computational%20overhead.%20%282%29%20For%20training%20data%2C%20a%0Ameticulously%20curated%2C%20high-quality%20interleaved%20dataset%20is%20collected%20for%0Afine-tuning%20MLLMs.%20%283%29%20For%20the%20training%20algorithm%2C%20we%20propose%20a%20%60%60what%20or%20how%0Ato%20generate%27%27%20algorithm%20to%20empower%20existing%20MLLMs%20with%20multimodal%20generation%0Acapabilities%20while%20preserving%20their%20multimodal%20understanding%20capabilities%2C%0Athrough%20three%20progressive%20training%20stages%20based%20on%20the%20collected%20dataset.%0AExperimental%20results%20demonstrate%20that%20ARMOR%20upgrades%20existing%20MLLMs%20to%20UniMs%0Awith%20promising%20image%20generation%20capabilities%2C%20using%20limited%20training%20resources.%0AOur%20code%20will%20be%20released%20soon%20at%20https%3A//github.com/finyorko/armor.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.06542v2&entry.124074799=Read"},
{"title": "Scalable unsupervised feature selection via weight stability", "author": "Xudong Zhang and Renato Cordeiro de Amorim", "abstract": "  Unsupervised feature selection is critical for improving clustering\nperformance in high-dimensional data, where irrelevant features can obscure\nmeaningful structure. In this work, we introduce the Minkowski weighted\n$k$-means++, a novel initialisation strategy for the Minkowski Weighted\n$k$-means. Our initialisation selects centroids probabilistically using feature\nrelevance estimates derived from the data itself. Building on this, we propose\ntwo new feature selection algorithms, FS-MWK++, which aggregates feature\nweights across a range of Minkowski exponents to identify stable and\ninformative features, and SFS-MWK++, a scalable variant based on subsampling.\nWe support our approach with a theoretical guarantee under mild assumptions and\nextensive experiments showing that our methods consistently outperform existing\nalternatives.\n", "link": "http://arxiv.org/abs/2506.06114v1", "date": "2025-06-06", "relevancy": 2.1216, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4465}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4304}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.3961}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20unsupervised%20feature%20selection%20via%20weight%20stability&body=Title%3A%20Scalable%20unsupervised%20feature%20selection%20via%20weight%20stability%0AAuthor%3A%20Xudong%20Zhang%20and%20Renato%20Cordeiro%20de%20Amorim%0AAbstract%3A%20%20%20Unsupervised%20feature%20selection%20is%20critical%20for%20improving%20clustering%0Aperformance%20in%20high-dimensional%20data%2C%20where%20irrelevant%20features%20can%20obscure%0Ameaningful%20structure.%20In%20this%20work%2C%20we%20introduce%20the%20Minkowski%20weighted%0A%24k%24-means%2B%2B%2C%20a%20novel%20initialisation%20strategy%20for%20the%20Minkowski%20Weighted%0A%24k%24-means.%20Our%20initialisation%20selects%20centroids%20probabilistically%20using%20feature%0Arelevance%20estimates%20derived%20from%20the%20data%20itself.%20Building%20on%20this%2C%20we%20propose%0Atwo%20new%20feature%20selection%20algorithms%2C%20FS-MWK%2B%2B%2C%20which%20aggregates%20feature%0Aweights%20across%20a%20range%20of%20Minkowski%20exponents%20to%20identify%20stable%20and%0Ainformative%20features%2C%20and%20SFS-MWK%2B%2B%2C%20a%20scalable%20variant%20based%20on%20subsampling.%0AWe%20support%20our%20approach%20with%20a%20theoretical%20guarantee%20under%20mild%20assumptions%20and%0Aextensive%20experiments%20showing%20that%20our%20methods%20consistently%20outperform%20existing%0Aalternatives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06114v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520unsupervised%2520feature%2520selection%2520via%2520weight%2520stability%26entry.906535625%3DXudong%2520Zhang%2520and%2520Renato%2520Cordeiro%2520de%2520Amorim%26entry.1292438233%3D%2520%2520Unsupervised%2520feature%2520selection%2520is%2520critical%2520for%2520improving%2520clustering%250Aperformance%2520in%2520high-dimensional%2520data%252C%2520where%2520irrelevant%2520features%2520can%2520obscure%250Ameaningful%2520structure.%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%2520Minkowski%2520weighted%250A%2524k%2524-means%252B%252B%252C%2520a%2520novel%2520initialisation%2520strategy%2520for%2520the%2520Minkowski%2520Weighted%250A%2524k%2524-means.%2520Our%2520initialisation%2520selects%2520centroids%2520probabilistically%2520using%2520feature%250Arelevance%2520estimates%2520derived%2520from%2520the%2520data%2520itself.%2520Building%2520on%2520this%252C%2520we%2520propose%250Atwo%2520new%2520feature%2520selection%2520algorithms%252C%2520FS-MWK%252B%252B%252C%2520which%2520aggregates%2520feature%250Aweights%2520across%2520a%2520range%2520of%2520Minkowski%2520exponents%2520to%2520identify%2520stable%2520and%250Ainformative%2520features%252C%2520and%2520SFS-MWK%252B%252B%252C%2520a%2520scalable%2520variant%2520based%2520on%2520subsampling.%250AWe%2520support%2520our%2520approach%2520with%2520a%2520theoretical%2520guarantee%2520under%2520mild%2520assumptions%2520and%250Aextensive%2520experiments%2520showing%2520that%2520our%2520methods%2520consistently%2520outperform%2520existing%250Aalternatives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06114v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20unsupervised%20feature%20selection%20via%20weight%20stability&entry.906535625=Xudong%20Zhang%20and%20Renato%20Cordeiro%20de%20Amorim&entry.1292438233=%20%20Unsupervised%20feature%20selection%20is%20critical%20for%20improving%20clustering%0Aperformance%20in%20high-dimensional%20data%2C%20where%20irrelevant%20features%20can%20obscure%0Ameaningful%20structure.%20In%20this%20work%2C%20we%20introduce%20the%20Minkowski%20weighted%0A%24k%24-means%2B%2B%2C%20a%20novel%20initialisation%20strategy%20for%20the%20Minkowski%20Weighted%0A%24k%24-means.%20Our%20initialisation%20selects%20centroids%20probabilistically%20using%20feature%0Arelevance%20estimates%20derived%20from%20the%20data%20itself.%20Building%20on%20this%2C%20we%20propose%0Atwo%20new%20feature%20selection%20algorithms%2C%20FS-MWK%2B%2B%2C%20which%20aggregates%20feature%0Aweights%20across%20a%20range%20of%20Minkowski%20exponents%20to%20identify%20stable%20and%0Ainformative%20features%2C%20and%20SFS-MWK%2B%2B%2C%20a%20scalable%20variant%20based%20on%20subsampling.%0AWe%20support%20our%20approach%20with%20a%20theoretical%20guarantee%20under%20mild%20assumptions%20and%0Aextensive%20experiments%20showing%20that%20our%20methods%20consistently%20outperform%20existing%0Aalternatives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06114v1&entry.124074799=Read"},
{"title": "Distributed Expectation Propagation for Multi-Object Tracking over\n  Sensor Networks", "author": "Qing Li and Runze Gan and James R. Hopgood and Michael E. Davies and Simon J. Godsill", "abstract": "  In this paper, we present a novel distributed expectation propagation\nalgorithm for multiple sensors, multiple objects tracking in cluttered\nenvironments. The proposed framework enables each sensor to operate locally\nwhile collaboratively exchanging moment estimates with other sensors, thus\neliminating the need to transmit all data to a central processing node.\nSpecifically, we introduce a fast and parallelisable Rao-Blackwellised Gibbs\nsampling scheme to approximate the tilted distributions, which enhances the\naccuracy and efficiency of expectation propagation updates. Results demonstrate\nthat the proposed algorithm improves both communication and inference\nefficiency for multi-object tracking tasks with dynamic sensor connectivity and\nvarying clutter levels.\n", "link": "http://arxiv.org/abs/2505.18795v2", "date": "2025-06-06", "relevancy": 2.1207, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5904}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.525}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5112}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distributed%20Expectation%20Propagation%20for%20Multi-Object%20Tracking%20over%0A%20%20Sensor%20Networks&body=Title%3A%20Distributed%20Expectation%20Propagation%20for%20Multi-Object%20Tracking%20over%0A%20%20Sensor%20Networks%0AAuthor%3A%20Qing%20Li%20and%20Runze%20Gan%20and%20James%20R.%20Hopgood%20and%20Michael%20E.%20Davies%20and%20Simon%20J.%20Godsill%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20distributed%20expectation%20propagation%0Aalgorithm%20for%20multiple%20sensors%2C%20multiple%20objects%20tracking%20in%20cluttered%0Aenvironments.%20The%20proposed%20framework%20enables%20each%20sensor%20to%20operate%20locally%0Awhile%20collaboratively%20exchanging%20moment%20estimates%20with%20other%20sensors%2C%20thus%0Aeliminating%20the%20need%20to%20transmit%20all%20data%20to%20a%20central%20processing%20node.%0ASpecifically%2C%20we%20introduce%20a%20fast%20and%20parallelisable%20Rao-Blackwellised%20Gibbs%0Asampling%20scheme%20to%20approximate%20the%20tilted%20distributions%2C%20which%20enhances%20the%0Aaccuracy%20and%20efficiency%20of%20expectation%20propagation%20updates.%20Results%20demonstrate%0Athat%20the%20proposed%20algorithm%20improves%20both%20communication%20and%20inference%0Aefficiency%20for%20multi-object%20tracking%20tasks%20with%20dynamic%20sensor%20connectivity%20and%0Avarying%20clutter%20levels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18795v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistributed%2520Expectation%2520Propagation%2520for%2520Multi-Object%2520Tracking%2520over%250A%2520%2520Sensor%2520Networks%26entry.906535625%3DQing%2520Li%2520and%2520Runze%2520Gan%2520and%2520James%2520R.%2520Hopgood%2520and%2520Michael%2520E.%2520Davies%2520and%2520Simon%2520J.%2520Godsill%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520distributed%2520expectation%2520propagation%250Aalgorithm%2520for%2520multiple%2520sensors%252C%2520multiple%2520objects%2520tracking%2520in%2520cluttered%250Aenvironments.%2520The%2520proposed%2520framework%2520enables%2520each%2520sensor%2520to%2520operate%2520locally%250Awhile%2520collaboratively%2520exchanging%2520moment%2520estimates%2520with%2520other%2520sensors%252C%2520thus%250Aeliminating%2520the%2520need%2520to%2520transmit%2520all%2520data%2520to%2520a%2520central%2520processing%2520node.%250ASpecifically%252C%2520we%2520introduce%2520a%2520fast%2520and%2520parallelisable%2520Rao-Blackwellised%2520Gibbs%250Asampling%2520scheme%2520to%2520approximate%2520the%2520tilted%2520distributions%252C%2520which%2520enhances%2520the%250Aaccuracy%2520and%2520efficiency%2520of%2520expectation%2520propagation%2520updates.%2520Results%2520demonstrate%250Athat%2520the%2520proposed%2520algorithm%2520improves%2520both%2520communication%2520and%2520inference%250Aefficiency%2520for%2520multi-object%2520tracking%2520tasks%2520with%2520dynamic%2520sensor%2520connectivity%2520and%250Avarying%2520clutter%2520levels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18795v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributed%20Expectation%20Propagation%20for%20Multi-Object%20Tracking%20over%0A%20%20Sensor%20Networks&entry.906535625=Qing%20Li%20and%20Runze%20Gan%20and%20James%20R.%20Hopgood%20and%20Michael%20E.%20Davies%20and%20Simon%20J.%20Godsill&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20distributed%20expectation%20propagation%0Aalgorithm%20for%20multiple%20sensors%2C%20multiple%20objects%20tracking%20in%20cluttered%0Aenvironments.%20The%20proposed%20framework%20enables%20each%20sensor%20to%20operate%20locally%0Awhile%20collaboratively%20exchanging%20moment%20estimates%20with%20other%20sensors%2C%20thus%0Aeliminating%20the%20need%20to%20transmit%20all%20data%20to%20a%20central%20processing%20node.%0ASpecifically%2C%20we%20introduce%20a%20fast%20and%20parallelisable%20Rao-Blackwellised%20Gibbs%0Asampling%20scheme%20to%20approximate%20the%20tilted%20distributions%2C%20which%20enhances%20the%0Aaccuracy%20and%20efficiency%20of%20expectation%20propagation%20updates.%20Results%20demonstrate%0Athat%20the%20proposed%20algorithm%20improves%20both%20communication%20and%20inference%0Aefficiency%20for%20multi-object%20tracking%20tasks%20with%20dynamic%20sensor%20connectivity%20and%0Avarying%20clutter%20levels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18795v2&entry.124074799=Read"},
{"title": "A Riemannian Optimization Perspective of the Gauss-Newton Method for\n  Feedforward Neural Networks", "author": "Semih Cayci", "abstract": "  We analyze the convergence of Gauss-Newton dynamics for training neural\nnetworks with smooth activation functions. In the underparameterized regime,\nthe Gauss-Newton gradient flow induces a Riemannian gradient flow on a\nlow-dimensional, smooth, embedded submanifold of the Euclidean output space.\nUsing tools from Riemannian optimization, we prove \\emph{last-iterate}\nconvergence of the Riemannian gradient flow to the optimal in-class predictor\nat an \\emph{exponential rate} that is independent of the conditioning of the\nGram matrix, \\emph{without} requiring explicit regularization. We further\ncharacterize the critical impacts of the neural network scaling factor and the\ninitialization on the convergence behavior. In the overparameterized regime, we\nshow that the Levenberg-Marquardt dynamics with an appropriately chosen damping\nschedule yields fast convergence rate despite potentially ill-conditioned\nneural tangent kernel matrices, analogous to the underparameterized regime.\nThese findings demonstrate the potential of Gauss-Newton methods for\nefficiently optimizing neural networks in the near-initialization regime,\nparticularly in ill-conditioned problems where kernel and Gram matrices have\nsmall singular values.\n", "link": "http://arxiv.org/abs/2412.14031v4", "date": "2025-06-06", "relevancy": 2.1187, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5449}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5209}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5135}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Riemannian%20Optimization%20Perspective%20of%20the%20Gauss-Newton%20Method%20for%0A%20%20Feedforward%20Neural%20Networks&body=Title%3A%20A%20Riemannian%20Optimization%20Perspective%20of%20the%20Gauss-Newton%20Method%20for%0A%20%20Feedforward%20Neural%20Networks%0AAuthor%3A%20Semih%20Cayci%0AAbstract%3A%20%20%20We%20analyze%20the%20convergence%20of%20Gauss-Newton%20dynamics%20for%20training%20neural%0Anetworks%20with%20smooth%20activation%20functions.%20In%20the%20underparameterized%20regime%2C%0Athe%20Gauss-Newton%20gradient%20flow%20induces%20a%20Riemannian%20gradient%20flow%20on%20a%0Alow-dimensional%2C%20smooth%2C%20embedded%20submanifold%20of%20the%20Euclidean%20output%20space.%0AUsing%20tools%20from%20Riemannian%20optimization%2C%20we%20prove%20%5Cemph%7Blast-iterate%7D%0Aconvergence%20of%20the%20Riemannian%20gradient%20flow%20to%20the%20optimal%20in-class%20predictor%0Aat%20an%20%5Cemph%7Bexponential%20rate%7D%20that%20is%20independent%20of%20the%20conditioning%20of%20the%0AGram%20matrix%2C%20%5Cemph%7Bwithout%7D%20requiring%20explicit%20regularization.%20We%20further%0Acharacterize%20the%20critical%20impacts%20of%20the%20neural%20network%20scaling%20factor%20and%20the%0Ainitialization%20on%20the%20convergence%20behavior.%20In%20the%20overparameterized%20regime%2C%20we%0Ashow%20that%20the%20Levenberg-Marquardt%20dynamics%20with%20an%20appropriately%20chosen%20damping%0Aschedule%20yields%20fast%20convergence%20rate%20despite%20potentially%20ill-conditioned%0Aneural%20tangent%20kernel%20matrices%2C%20analogous%20to%20the%20underparameterized%20regime.%0AThese%20findings%20demonstrate%20the%20potential%20of%20Gauss-Newton%20methods%20for%0Aefficiently%20optimizing%20neural%20networks%20in%20the%20near-initialization%20regime%2C%0Aparticularly%20in%20ill-conditioned%20problems%20where%20kernel%20and%20Gram%20matrices%20have%0Asmall%20singular%20values.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14031v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Riemannian%2520Optimization%2520Perspective%2520of%2520the%2520Gauss-Newton%2520Method%2520for%250A%2520%2520Feedforward%2520Neural%2520Networks%26entry.906535625%3DSemih%2520Cayci%26entry.1292438233%3D%2520%2520We%2520analyze%2520the%2520convergence%2520of%2520Gauss-Newton%2520dynamics%2520for%2520training%2520neural%250Anetworks%2520with%2520smooth%2520activation%2520functions.%2520In%2520the%2520underparameterized%2520regime%252C%250Athe%2520Gauss-Newton%2520gradient%2520flow%2520induces%2520a%2520Riemannian%2520gradient%2520flow%2520on%2520a%250Alow-dimensional%252C%2520smooth%252C%2520embedded%2520submanifold%2520of%2520the%2520Euclidean%2520output%2520space.%250AUsing%2520tools%2520from%2520Riemannian%2520optimization%252C%2520we%2520prove%2520%255Cemph%257Blast-iterate%257D%250Aconvergence%2520of%2520the%2520Riemannian%2520gradient%2520flow%2520to%2520the%2520optimal%2520in-class%2520predictor%250Aat%2520an%2520%255Cemph%257Bexponential%2520rate%257D%2520that%2520is%2520independent%2520of%2520the%2520conditioning%2520of%2520the%250AGram%2520matrix%252C%2520%255Cemph%257Bwithout%257D%2520requiring%2520explicit%2520regularization.%2520We%2520further%250Acharacterize%2520the%2520critical%2520impacts%2520of%2520the%2520neural%2520network%2520scaling%2520factor%2520and%2520the%250Ainitialization%2520on%2520the%2520convergence%2520behavior.%2520In%2520the%2520overparameterized%2520regime%252C%2520we%250Ashow%2520that%2520the%2520Levenberg-Marquardt%2520dynamics%2520with%2520an%2520appropriately%2520chosen%2520damping%250Aschedule%2520yields%2520fast%2520convergence%2520rate%2520despite%2520potentially%2520ill-conditioned%250Aneural%2520tangent%2520kernel%2520matrices%252C%2520analogous%2520to%2520the%2520underparameterized%2520regime.%250AThese%2520findings%2520demonstrate%2520the%2520potential%2520of%2520Gauss-Newton%2520methods%2520for%250Aefficiently%2520optimizing%2520neural%2520networks%2520in%2520the%2520near-initialization%2520regime%252C%250Aparticularly%2520in%2520ill-conditioned%2520problems%2520where%2520kernel%2520and%2520Gram%2520matrices%2520have%250Asmall%2520singular%2520values.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14031v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Riemannian%20Optimization%20Perspective%20of%20the%20Gauss-Newton%20Method%20for%0A%20%20Feedforward%20Neural%20Networks&entry.906535625=Semih%20Cayci&entry.1292438233=%20%20We%20analyze%20the%20convergence%20of%20Gauss-Newton%20dynamics%20for%20training%20neural%0Anetworks%20with%20smooth%20activation%20functions.%20In%20the%20underparameterized%20regime%2C%0Athe%20Gauss-Newton%20gradient%20flow%20induces%20a%20Riemannian%20gradient%20flow%20on%20a%0Alow-dimensional%2C%20smooth%2C%20embedded%20submanifold%20of%20the%20Euclidean%20output%20space.%0AUsing%20tools%20from%20Riemannian%20optimization%2C%20we%20prove%20%5Cemph%7Blast-iterate%7D%0Aconvergence%20of%20the%20Riemannian%20gradient%20flow%20to%20the%20optimal%20in-class%20predictor%0Aat%20an%20%5Cemph%7Bexponential%20rate%7D%20that%20is%20independent%20of%20the%20conditioning%20of%20the%0AGram%20matrix%2C%20%5Cemph%7Bwithout%7D%20requiring%20explicit%20regularization.%20We%20further%0Acharacterize%20the%20critical%20impacts%20of%20the%20neural%20network%20scaling%20factor%20and%20the%0Ainitialization%20on%20the%20convergence%20behavior.%20In%20the%20overparameterized%20regime%2C%20we%0Ashow%20that%20the%20Levenberg-Marquardt%20dynamics%20with%20an%20appropriately%20chosen%20damping%0Aschedule%20yields%20fast%20convergence%20rate%20despite%20potentially%20ill-conditioned%0Aneural%20tangent%20kernel%20matrices%2C%20analogous%20to%20the%20underparameterized%20regime.%0AThese%20findings%20demonstrate%20the%20potential%20of%20Gauss-Newton%20methods%20for%0Aefficiently%20optimizing%20neural%20networks%20in%20the%20near-initialization%20regime%2C%0Aparticularly%20in%20ill-conditioned%20problems%20where%20kernel%20and%20Gram%20matrices%20have%0Asmall%20singular%20values.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14031v4&entry.124074799=Read"},
{"title": "Similarity Matching Networks: Hebbian Learning and Convergence Over\n  Multiple Time Scales", "author": "Veronica Centorrino and Francesco Bullo and Giovanni Russo", "abstract": "  A recent breakthrough in biologically-plausible normative frameworks for\ndimensionality reduction is based upon the similarity matching cost function\nand the low-rank matrix approximation problem. Despite clear biological\ninterpretation, successful application in several domains, and experimental\nvalidation, a formal complete convergence analysis remains elusive. Building on\nthis framework, we consider and analyze a continuous-time neural network, the\n\\emph{similarity matching network}, for principal subspace projection. Derived\nfrom a min-max-min objective, this biologically-plausible network consists of\nthree coupled dynamics evolving at different time scales: neural dynamics,\nlateral synaptic dynamics, and feedforward synaptic dynamics at the fast,\nintermediate, and slow time scales, respectively. The feedforward and lateral\nsynaptic dynamics consist of Hebbian and anti-Hebbian learning rules,\nrespectively. By leveraging a multilevel optimization framework, we prove\nconvergence of the dynamics in the offline setting. Specifically, at the first\nlevel (fast time scale), we show strong convexity of the cost function and\nglobal exponential convergence of the corresponding gradient-flow dynamics. At\nthe second level (intermediate time scale), we prove strong concavity of the\ncost function and exponential convergence of the corresponding gradient-flow\ndynamics within the space of positive definite matrices. At the third and final\nlevel (slow time scale), we study a non-convex and non-smooth cost function,\nprovide explicit expressions for its global minima, and prove almost sure\nconvergence of the corresponding gradient-flow dynamics to the global minima.\nThese results rely on two empirically motivated conjectures that are supported\nby thorough numerical experiments. Finally, we validate the effectiveness of\nour approach via a numerical example.\n", "link": "http://arxiv.org/abs/2506.06134v1", "date": "2025-06-06", "relevancy": 2.1098, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5298}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5286}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5253}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Similarity%20Matching%20Networks%3A%20Hebbian%20Learning%20and%20Convergence%20Over%0A%20%20Multiple%20Time%20Scales&body=Title%3A%20Similarity%20Matching%20Networks%3A%20Hebbian%20Learning%20and%20Convergence%20Over%0A%20%20Multiple%20Time%20Scales%0AAuthor%3A%20Veronica%20Centorrino%20and%20Francesco%20Bullo%20and%20Giovanni%20Russo%0AAbstract%3A%20%20%20A%20recent%20breakthrough%20in%20biologically-plausible%20normative%20frameworks%20for%0Adimensionality%20reduction%20is%20based%20upon%20the%20similarity%20matching%20cost%20function%0Aand%20the%20low-rank%20matrix%20approximation%20problem.%20Despite%20clear%20biological%0Ainterpretation%2C%20successful%20application%20in%20several%20domains%2C%20and%20experimental%0Avalidation%2C%20a%20formal%20complete%20convergence%20analysis%20remains%20elusive.%20Building%20on%0Athis%20framework%2C%20we%20consider%20and%20analyze%20a%20continuous-time%20neural%20network%2C%20the%0A%5Cemph%7Bsimilarity%20matching%20network%7D%2C%20for%20principal%20subspace%20projection.%20Derived%0Afrom%20a%20min-max-min%20objective%2C%20this%20biologically-plausible%20network%20consists%20of%0Athree%20coupled%20dynamics%20evolving%20at%20different%20time%20scales%3A%20neural%20dynamics%2C%0Alateral%20synaptic%20dynamics%2C%20and%20feedforward%20synaptic%20dynamics%20at%20the%20fast%2C%0Aintermediate%2C%20and%20slow%20time%20scales%2C%20respectively.%20The%20feedforward%20and%20lateral%0Asynaptic%20dynamics%20consist%20of%20Hebbian%20and%20anti-Hebbian%20learning%20rules%2C%0Arespectively.%20By%20leveraging%20a%20multilevel%20optimization%20framework%2C%20we%20prove%0Aconvergence%20of%20the%20dynamics%20in%20the%20offline%20setting.%20Specifically%2C%20at%20the%20first%0Alevel%20%28fast%20time%20scale%29%2C%20we%20show%20strong%20convexity%20of%20the%20cost%20function%20and%0Aglobal%20exponential%20convergence%20of%20the%20corresponding%20gradient-flow%20dynamics.%20At%0Athe%20second%20level%20%28intermediate%20time%20scale%29%2C%20we%20prove%20strong%20concavity%20of%20the%0Acost%20function%20and%20exponential%20convergence%20of%20the%20corresponding%20gradient-flow%0Adynamics%20within%20the%20space%20of%20positive%20definite%20matrices.%20At%20the%20third%20and%20final%0Alevel%20%28slow%20time%20scale%29%2C%20we%20study%20a%20non-convex%20and%20non-smooth%20cost%20function%2C%0Aprovide%20explicit%20expressions%20for%20its%20global%20minima%2C%20and%20prove%20almost%20sure%0Aconvergence%20of%20the%20corresponding%20gradient-flow%20dynamics%20to%20the%20global%20minima.%0AThese%20results%20rely%20on%20two%20empirically%20motivated%20conjectures%20that%20are%20supported%0Aby%20thorough%20numerical%20experiments.%20Finally%2C%20we%20validate%20the%20effectiveness%20of%0Aour%20approach%20via%20a%20numerical%20example.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06134v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimilarity%2520Matching%2520Networks%253A%2520Hebbian%2520Learning%2520and%2520Convergence%2520Over%250A%2520%2520Multiple%2520Time%2520Scales%26entry.906535625%3DVeronica%2520Centorrino%2520and%2520Francesco%2520Bullo%2520and%2520Giovanni%2520Russo%26entry.1292438233%3D%2520%2520A%2520recent%2520breakthrough%2520in%2520biologically-plausible%2520normative%2520frameworks%2520for%250Adimensionality%2520reduction%2520is%2520based%2520upon%2520the%2520similarity%2520matching%2520cost%2520function%250Aand%2520the%2520low-rank%2520matrix%2520approximation%2520problem.%2520Despite%2520clear%2520biological%250Ainterpretation%252C%2520successful%2520application%2520in%2520several%2520domains%252C%2520and%2520experimental%250Avalidation%252C%2520a%2520formal%2520complete%2520convergence%2520analysis%2520remains%2520elusive.%2520Building%2520on%250Athis%2520framework%252C%2520we%2520consider%2520and%2520analyze%2520a%2520continuous-time%2520neural%2520network%252C%2520the%250A%255Cemph%257Bsimilarity%2520matching%2520network%257D%252C%2520for%2520principal%2520subspace%2520projection.%2520Derived%250Afrom%2520a%2520min-max-min%2520objective%252C%2520this%2520biologically-plausible%2520network%2520consists%2520of%250Athree%2520coupled%2520dynamics%2520evolving%2520at%2520different%2520time%2520scales%253A%2520neural%2520dynamics%252C%250Alateral%2520synaptic%2520dynamics%252C%2520and%2520feedforward%2520synaptic%2520dynamics%2520at%2520the%2520fast%252C%250Aintermediate%252C%2520and%2520slow%2520time%2520scales%252C%2520respectively.%2520The%2520feedforward%2520and%2520lateral%250Asynaptic%2520dynamics%2520consist%2520of%2520Hebbian%2520and%2520anti-Hebbian%2520learning%2520rules%252C%250Arespectively.%2520By%2520leveraging%2520a%2520multilevel%2520optimization%2520framework%252C%2520we%2520prove%250Aconvergence%2520of%2520the%2520dynamics%2520in%2520the%2520offline%2520setting.%2520Specifically%252C%2520at%2520the%2520first%250Alevel%2520%2528fast%2520time%2520scale%2529%252C%2520we%2520show%2520strong%2520convexity%2520of%2520the%2520cost%2520function%2520and%250Aglobal%2520exponential%2520convergence%2520of%2520the%2520corresponding%2520gradient-flow%2520dynamics.%2520At%250Athe%2520second%2520level%2520%2528intermediate%2520time%2520scale%2529%252C%2520we%2520prove%2520strong%2520concavity%2520of%2520the%250Acost%2520function%2520and%2520exponential%2520convergence%2520of%2520the%2520corresponding%2520gradient-flow%250Adynamics%2520within%2520the%2520space%2520of%2520positive%2520definite%2520matrices.%2520At%2520the%2520third%2520and%2520final%250Alevel%2520%2528slow%2520time%2520scale%2529%252C%2520we%2520study%2520a%2520non-convex%2520and%2520non-smooth%2520cost%2520function%252C%250Aprovide%2520explicit%2520expressions%2520for%2520its%2520global%2520minima%252C%2520and%2520prove%2520almost%2520sure%250Aconvergence%2520of%2520the%2520corresponding%2520gradient-flow%2520dynamics%2520to%2520the%2520global%2520minima.%250AThese%2520results%2520rely%2520on%2520two%2520empirically%2520motivated%2520conjectures%2520that%2520are%2520supported%250Aby%2520thorough%2520numerical%2520experiments.%2520Finally%252C%2520we%2520validate%2520the%2520effectiveness%2520of%250Aour%2520approach%2520via%2520a%2520numerical%2520example.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06134v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Similarity%20Matching%20Networks%3A%20Hebbian%20Learning%20and%20Convergence%20Over%0A%20%20Multiple%20Time%20Scales&entry.906535625=Veronica%20Centorrino%20and%20Francesco%20Bullo%20and%20Giovanni%20Russo&entry.1292438233=%20%20A%20recent%20breakthrough%20in%20biologically-plausible%20normative%20frameworks%20for%0Adimensionality%20reduction%20is%20based%20upon%20the%20similarity%20matching%20cost%20function%0Aand%20the%20low-rank%20matrix%20approximation%20problem.%20Despite%20clear%20biological%0Ainterpretation%2C%20successful%20application%20in%20several%20domains%2C%20and%20experimental%0Avalidation%2C%20a%20formal%20complete%20convergence%20analysis%20remains%20elusive.%20Building%20on%0Athis%20framework%2C%20we%20consider%20and%20analyze%20a%20continuous-time%20neural%20network%2C%20the%0A%5Cemph%7Bsimilarity%20matching%20network%7D%2C%20for%20principal%20subspace%20projection.%20Derived%0Afrom%20a%20min-max-min%20objective%2C%20this%20biologically-plausible%20network%20consists%20of%0Athree%20coupled%20dynamics%20evolving%20at%20different%20time%20scales%3A%20neural%20dynamics%2C%0Alateral%20synaptic%20dynamics%2C%20and%20feedforward%20synaptic%20dynamics%20at%20the%20fast%2C%0Aintermediate%2C%20and%20slow%20time%20scales%2C%20respectively.%20The%20feedforward%20and%20lateral%0Asynaptic%20dynamics%20consist%20of%20Hebbian%20and%20anti-Hebbian%20learning%20rules%2C%0Arespectively.%20By%20leveraging%20a%20multilevel%20optimization%20framework%2C%20we%20prove%0Aconvergence%20of%20the%20dynamics%20in%20the%20offline%20setting.%20Specifically%2C%20at%20the%20first%0Alevel%20%28fast%20time%20scale%29%2C%20we%20show%20strong%20convexity%20of%20the%20cost%20function%20and%0Aglobal%20exponential%20convergence%20of%20the%20corresponding%20gradient-flow%20dynamics.%20At%0Athe%20second%20level%20%28intermediate%20time%20scale%29%2C%20we%20prove%20strong%20concavity%20of%20the%0Acost%20function%20and%20exponential%20convergence%20of%20the%20corresponding%20gradient-flow%0Adynamics%20within%20the%20space%20of%20positive%20definite%20matrices.%20At%20the%20third%20and%20final%0Alevel%20%28slow%20time%20scale%29%2C%20we%20study%20a%20non-convex%20and%20non-smooth%20cost%20function%2C%0Aprovide%20explicit%20expressions%20for%20its%20global%20minima%2C%20and%20prove%20almost%20sure%0Aconvergence%20of%20the%20corresponding%20gradient-flow%20dynamics%20to%20the%20global%20minima.%0AThese%20results%20rely%20on%20two%20empirically%20motivated%20conjectures%20that%20are%20supported%0Aby%20thorough%20numerical%20experiments.%20Finally%2C%20we%20validate%20the%20effectiveness%20of%0Aour%20approach%20via%20a%20numerical%20example.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06134v1&entry.124074799=Read"},
{"title": "Machine learning for in-situ composition mapping in a self-driving\n  magnetron sputtering system", "author": "Sanna Jarl and Jens Sj\u00f6lund and Robert J. W. Frost and Anders Holst and Jonathan J. S. Scragg", "abstract": "  Self-driving labs (SDLs), employing automation and machine learning (ML) to\naccelerate experimental procedures, have enormous potential in the discovery of\nnew materials. However, in thin film science, SDLs are mainly restricted to\nsolution-based synthetic methods which are easier to automate but cannot access\nthe broad chemical space of inorganic materials. This work presents an SDL\nbased on magnetron co-sputtering. We are using combinatorial frameworks,\nobtaining accurate composition maps on multi-element, compositionally graded\nthin films. This normally requires time-consuming ex-situ analysis prone to\nsystematic errors. We present a rapid and calibration-free in-situ, ML driven\napproach to produce composition maps for arbitrary source combinations and\nsputtering conditions. We develop a method to predict the composition\ndistribution in a multi-element combinatorial thin film, using in-situ\nmeasurements from quartz-crystal microbalance sensors placed in a sputter\nchamber. For a given source, the sensor readings are learned as a function of\nthe sputtering pressure and magnetron power, through active learning using\nGaussian processes (GPs). The final GPs are combined with a geometric model of\nthe deposition flux distribution in the chamber, which allows interpolation of\nthe deposition rates from each source, at any position across the sample. We\ninvestigate several acquisition functions for the ML procedure. A fully\nBayesian GP - BALM (Bayesian active learning MacKay) - achieved the best\nperformance, learning the deposition rates for a single source in 10\nexperiments. Prediction accuracy for co-sputtering composition distributions\nwas verified experimentally. Our framework dramatically increases throughput by\navoiding the need for extensive characterisation or calibration, thus\ndemonstrating the potential of ML-guided SDLs to accelerate materials\nexploration.\n", "link": "http://arxiv.org/abs/2506.05999v1", "date": "2025-06-06", "relevancy": 2.1045, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5486}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5278}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Machine%20learning%20for%20in-situ%20composition%20mapping%20in%20a%20self-driving%0A%20%20magnetron%20sputtering%20system&body=Title%3A%20Machine%20learning%20for%20in-situ%20composition%20mapping%20in%20a%20self-driving%0A%20%20magnetron%20sputtering%20system%0AAuthor%3A%20Sanna%20Jarl%20and%20Jens%20Sj%C3%B6lund%20and%20Robert%20J.%20W.%20Frost%20and%20Anders%20Holst%20and%20Jonathan%20J.%20S.%20Scragg%0AAbstract%3A%20%20%20Self-driving%20labs%20%28SDLs%29%2C%20employing%20automation%20and%20machine%20learning%20%28ML%29%20to%0Aaccelerate%20experimental%20procedures%2C%20have%20enormous%20potential%20in%20the%20discovery%20of%0Anew%20materials.%20However%2C%20in%20thin%20film%20science%2C%20SDLs%20are%20mainly%20restricted%20to%0Asolution-based%20synthetic%20methods%20which%20are%20easier%20to%20automate%20but%20cannot%20access%0Athe%20broad%20chemical%20space%20of%20inorganic%20materials.%20This%20work%20presents%20an%20SDL%0Abased%20on%20magnetron%20co-sputtering.%20We%20are%20using%20combinatorial%20frameworks%2C%0Aobtaining%20accurate%20composition%20maps%20on%20multi-element%2C%20compositionally%20graded%0Athin%20films.%20This%20normally%20requires%20time-consuming%20ex-situ%20analysis%20prone%20to%0Asystematic%20errors.%20We%20present%20a%20rapid%20and%20calibration-free%20in-situ%2C%20ML%20driven%0Aapproach%20to%20produce%20composition%20maps%20for%20arbitrary%20source%20combinations%20and%0Asputtering%20conditions.%20We%20develop%20a%20method%20to%20predict%20the%20composition%0Adistribution%20in%20a%20multi-element%20combinatorial%20thin%20film%2C%20using%20in-situ%0Ameasurements%20from%20quartz-crystal%20microbalance%20sensors%20placed%20in%20a%20sputter%0Achamber.%20For%20a%20given%20source%2C%20the%20sensor%20readings%20are%20learned%20as%20a%20function%20of%0Athe%20sputtering%20pressure%20and%20magnetron%20power%2C%20through%20active%20learning%20using%0AGaussian%20processes%20%28GPs%29.%20The%20final%20GPs%20are%20combined%20with%20a%20geometric%20model%20of%0Athe%20deposition%20flux%20distribution%20in%20the%20chamber%2C%20which%20allows%20interpolation%20of%0Athe%20deposition%20rates%20from%20each%20source%2C%20at%20any%20position%20across%20the%20sample.%20We%0Ainvestigate%20several%20acquisition%20functions%20for%20the%20ML%20procedure.%20A%20fully%0ABayesian%20GP%20-%20BALM%20%28Bayesian%20active%20learning%20MacKay%29%20-%20achieved%20the%20best%0Aperformance%2C%20learning%20the%20deposition%20rates%20for%20a%20single%20source%20in%2010%0Aexperiments.%20Prediction%20accuracy%20for%20co-sputtering%20composition%20distributions%0Awas%20verified%20experimentally.%20Our%20framework%20dramatically%20increases%20throughput%20by%0Aavoiding%20the%20need%20for%20extensive%20characterisation%20or%20calibration%2C%20thus%0Ademonstrating%20the%20potential%20of%20ML-guided%20SDLs%20to%20accelerate%20materials%0Aexploration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05999v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMachine%2520learning%2520for%2520in-situ%2520composition%2520mapping%2520in%2520a%2520self-driving%250A%2520%2520magnetron%2520sputtering%2520system%26entry.906535625%3DSanna%2520Jarl%2520and%2520Jens%2520Sj%25C3%25B6lund%2520and%2520Robert%2520J.%2520W.%2520Frost%2520and%2520Anders%2520Holst%2520and%2520Jonathan%2520J.%2520S.%2520Scragg%26entry.1292438233%3D%2520%2520Self-driving%2520labs%2520%2528SDLs%2529%252C%2520employing%2520automation%2520and%2520machine%2520learning%2520%2528ML%2529%2520to%250Aaccelerate%2520experimental%2520procedures%252C%2520have%2520enormous%2520potential%2520in%2520the%2520discovery%2520of%250Anew%2520materials.%2520However%252C%2520in%2520thin%2520film%2520science%252C%2520SDLs%2520are%2520mainly%2520restricted%2520to%250Asolution-based%2520synthetic%2520methods%2520which%2520are%2520easier%2520to%2520automate%2520but%2520cannot%2520access%250Athe%2520broad%2520chemical%2520space%2520of%2520inorganic%2520materials.%2520This%2520work%2520presents%2520an%2520SDL%250Abased%2520on%2520magnetron%2520co-sputtering.%2520We%2520are%2520using%2520combinatorial%2520frameworks%252C%250Aobtaining%2520accurate%2520composition%2520maps%2520on%2520multi-element%252C%2520compositionally%2520graded%250Athin%2520films.%2520This%2520normally%2520requires%2520time-consuming%2520ex-situ%2520analysis%2520prone%2520to%250Asystematic%2520errors.%2520We%2520present%2520a%2520rapid%2520and%2520calibration-free%2520in-situ%252C%2520ML%2520driven%250Aapproach%2520to%2520produce%2520composition%2520maps%2520for%2520arbitrary%2520source%2520combinations%2520and%250Asputtering%2520conditions.%2520We%2520develop%2520a%2520method%2520to%2520predict%2520the%2520composition%250Adistribution%2520in%2520a%2520multi-element%2520combinatorial%2520thin%2520film%252C%2520using%2520in-situ%250Ameasurements%2520from%2520quartz-crystal%2520microbalance%2520sensors%2520placed%2520in%2520a%2520sputter%250Achamber.%2520For%2520a%2520given%2520source%252C%2520the%2520sensor%2520readings%2520are%2520learned%2520as%2520a%2520function%2520of%250Athe%2520sputtering%2520pressure%2520and%2520magnetron%2520power%252C%2520through%2520active%2520learning%2520using%250AGaussian%2520processes%2520%2528GPs%2529.%2520The%2520final%2520GPs%2520are%2520combined%2520with%2520a%2520geometric%2520model%2520of%250Athe%2520deposition%2520flux%2520distribution%2520in%2520the%2520chamber%252C%2520which%2520allows%2520interpolation%2520of%250Athe%2520deposition%2520rates%2520from%2520each%2520source%252C%2520at%2520any%2520position%2520across%2520the%2520sample.%2520We%250Ainvestigate%2520several%2520acquisition%2520functions%2520for%2520the%2520ML%2520procedure.%2520A%2520fully%250ABayesian%2520GP%2520-%2520BALM%2520%2528Bayesian%2520active%2520learning%2520MacKay%2529%2520-%2520achieved%2520the%2520best%250Aperformance%252C%2520learning%2520the%2520deposition%2520rates%2520for%2520a%2520single%2520source%2520in%252010%250Aexperiments.%2520Prediction%2520accuracy%2520for%2520co-sputtering%2520composition%2520distributions%250Awas%2520verified%2520experimentally.%2520Our%2520framework%2520dramatically%2520increases%2520throughput%2520by%250Aavoiding%2520the%2520need%2520for%2520extensive%2520characterisation%2520or%2520calibration%252C%2520thus%250Ademonstrating%2520the%2520potential%2520of%2520ML-guided%2520SDLs%2520to%2520accelerate%2520materials%250Aexploration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05999v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20learning%20for%20in-situ%20composition%20mapping%20in%20a%20self-driving%0A%20%20magnetron%20sputtering%20system&entry.906535625=Sanna%20Jarl%20and%20Jens%20Sj%C3%B6lund%20and%20Robert%20J.%20W.%20Frost%20and%20Anders%20Holst%20and%20Jonathan%20J.%20S.%20Scragg&entry.1292438233=%20%20Self-driving%20labs%20%28SDLs%29%2C%20employing%20automation%20and%20machine%20learning%20%28ML%29%20to%0Aaccelerate%20experimental%20procedures%2C%20have%20enormous%20potential%20in%20the%20discovery%20of%0Anew%20materials.%20However%2C%20in%20thin%20film%20science%2C%20SDLs%20are%20mainly%20restricted%20to%0Asolution-based%20synthetic%20methods%20which%20are%20easier%20to%20automate%20but%20cannot%20access%0Athe%20broad%20chemical%20space%20of%20inorganic%20materials.%20This%20work%20presents%20an%20SDL%0Abased%20on%20magnetron%20co-sputtering.%20We%20are%20using%20combinatorial%20frameworks%2C%0Aobtaining%20accurate%20composition%20maps%20on%20multi-element%2C%20compositionally%20graded%0Athin%20films.%20This%20normally%20requires%20time-consuming%20ex-situ%20analysis%20prone%20to%0Asystematic%20errors.%20We%20present%20a%20rapid%20and%20calibration-free%20in-situ%2C%20ML%20driven%0Aapproach%20to%20produce%20composition%20maps%20for%20arbitrary%20source%20combinations%20and%0Asputtering%20conditions.%20We%20develop%20a%20method%20to%20predict%20the%20composition%0Adistribution%20in%20a%20multi-element%20combinatorial%20thin%20film%2C%20using%20in-situ%0Ameasurements%20from%20quartz-crystal%20microbalance%20sensors%20placed%20in%20a%20sputter%0Achamber.%20For%20a%20given%20source%2C%20the%20sensor%20readings%20are%20learned%20as%20a%20function%20of%0Athe%20sputtering%20pressure%20and%20magnetron%20power%2C%20through%20active%20learning%20using%0AGaussian%20processes%20%28GPs%29.%20The%20final%20GPs%20are%20combined%20with%20a%20geometric%20model%20of%0Athe%20deposition%20flux%20distribution%20in%20the%20chamber%2C%20which%20allows%20interpolation%20of%0Athe%20deposition%20rates%20from%20each%20source%2C%20at%20any%20position%20across%20the%20sample.%20We%0Ainvestigate%20several%20acquisition%20functions%20for%20the%20ML%20procedure.%20A%20fully%0ABayesian%20GP%20-%20BALM%20%28Bayesian%20active%20learning%20MacKay%29%20-%20achieved%20the%20best%0Aperformance%2C%20learning%20the%20deposition%20rates%20for%20a%20single%20source%20in%2010%0Aexperiments.%20Prediction%20accuracy%20for%20co-sputtering%20composition%20distributions%0Awas%20verified%20experimentally.%20Our%20framework%20dramatically%20increases%20throughput%20by%0Aavoiding%20the%20need%20for%20extensive%20characterisation%20or%20calibration%2C%20thus%0Ademonstrating%20the%20potential%20of%20ML-guided%20SDLs%20to%20accelerate%20materials%0Aexploration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05999v1&entry.124074799=Read"},
{"title": "Balancing Beyond Discrete Categories: Continuous Demographic Labels for\n  Fair Face Recognition", "author": "Pedro C. Neto and Naser Damer and Jaime S. Cardoso and Ana F. Sequeira", "abstract": "  Bias has been a constant in face recognition models. Over the years,\nresearchers have looked at it from both the model and the data point of view.\nHowever, their approach to mitigation of data bias was limited and lacked\ninsight on the real nature of the problem. Here, in this document, we propose\nto revise our use of ethnicity labels as a continuous variable instead of a\ndiscrete value per identity. We validate our formulation both experimentally\nand theoretically, showcasing that not all identities from one ethnicity\ncontribute equally to the balance of the dataset; thus, having the same number\nof identities per ethnicity does not represent a balanced dataset. We further\nshow that models trained on datasets balanced in the continuous space\nconsistently outperform models trained on data balanced in the discrete space.\nWe trained more than 65 different models, and created more than 20 subsets of\nthe original datasets.\n", "link": "http://arxiv.org/abs/2506.01532v4", "date": "2025-06-06", "relevancy": 2.1014, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5351}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5239}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Balancing%20Beyond%20Discrete%20Categories%3A%20Continuous%20Demographic%20Labels%20for%0A%20%20Fair%20Face%20Recognition&body=Title%3A%20Balancing%20Beyond%20Discrete%20Categories%3A%20Continuous%20Demographic%20Labels%20for%0A%20%20Fair%20Face%20Recognition%0AAuthor%3A%20Pedro%20C.%20Neto%20and%20Naser%20Damer%20and%20Jaime%20S.%20Cardoso%20and%20Ana%20F.%20Sequeira%0AAbstract%3A%20%20%20Bias%20has%20been%20a%20constant%20in%20face%20recognition%20models.%20Over%20the%20years%2C%0Aresearchers%20have%20looked%20at%20it%20from%20both%20the%20model%20and%20the%20data%20point%20of%20view.%0AHowever%2C%20their%20approach%20to%20mitigation%20of%20data%20bias%20was%20limited%20and%20lacked%0Ainsight%20on%20the%20real%20nature%20of%20the%20problem.%20Here%2C%20in%20this%20document%2C%20we%20propose%0Ato%20revise%20our%20use%20of%20ethnicity%20labels%20as%20a%20continuous%20variable%20instead%20of%20a%0Adiscrete%20value%20per%20identity.%20We%20validate%20our%20formulation%20both%20experimentally%0Aand%20theoretically%2C%20showcasing%20that%20not%20all%20identities%20from%20one%20ethnicity%0Acontribute%20equally%20to%20the%20balance%20of%20the%20dataset%3B%20thus%2C%20having%20the%20same%20number%0Aof%20identities%20per%20ethnicity%20does%20not%20represent%20a%20balanced%20dataset.%20We%20further%0Ashow%20that%20models%20trained%20on%20datasets%20balanced%20in%20the%20continuous%20space%0Aconsistently%20outperform%20models%20trained%20on%20data%20balanced%20in%20the%20discrete%20space.%0AWe%20trained%20more%20than%2065%20different%20models%2C%20and%20created%20more%20than%2020%20subsets%20of%0Athe%20original%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.01532v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBalancing%2520Beyond%2520Discrete%2520Categories%253A%2520Continuous%2520Demographic%2520Labels%2520for%250A%2520%2520Fair%2520Face%2520Recognition%26entry.906535625%3DPedro%2520C.%2520Neto%2520and%2520Naser%2520Damer%2520and%2520Jaime%2520S.%2520Cardoso%2520and%2520Ana%2520F.%2520Sequeira%26entry.1292438233%3D%2520%2520Bias%2520has%2520been%2520a%2520constant%2520in%2520face%2520recognition%2520models.%2520Over%2520the%2520years%252C%250Aresearchers%2520have%2520looked%2520at%2520it%2520from%2520both%2520the%2520model%2520and%2520the%2520data%2520point%2520of%2520view.%250AHowever%252C%2520their%2520approach%2520to%2520mitigation%2520of%2520data%2520bias%2520was%2520limited%2520and%2520lacked%250Ainsight%2520on%2520the%2520real%2520nature%2520of%2520the%2520problem.%2520Here%252C%2520in%2520this%2520document%252C%2520we%2520propose%250Ato%2520revise%2520our%2520use%2520of%2520ethnicity%2520labels%2520as%2520a%2520continuous%2520variable%2520instead%2520of%2520a%250Adiscrete%2520value%2520per%2520identity.%2520We%2520validate%2520our%2520formulation%2520both%2520experimentally%250Aand%2520theoretically%252C%2520showcasing%2520that%2520not%2520all%2520identities%2520from%2520one%2520ethnicity%250Acontribute%2520equally%2520to%2520the%2520balance%2520of%2520the%2520dataset%253B%2520thus%252C%2520having%2520the%2520same%2520number%250Aof%2520identities%2520per%2520ethnicity%2520does%2520not%2520represent%2520a%2520balanced%2520dataset.%2520We%2520further%250Ashow%2520that%2520models%2520trained%2520on%2520datasets%2520balanced%2520in%2520the%2520continuous%2520space%250Aconsistently%2520outperform%2520models%2520trained%2520on%2520data%2520balanced%2520in%2520the%2520discrete%2520space.%250AWe%2520trained%2520more%2520than%252065%2520different%2520models%252C%2520and%2520created%2520more%2520than%252020%2520subsets%2520of%250Athe%2520original%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.01532v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Balancing%20Beyond%20Discrete%20Categories%3A%20Continuous%20Demographic%20Labels%20for%0A%20%20Fair%20Face%20Recognition&entry.906535625=Pedro%20C.%20Neto%20and%20Naser%20Damer%20and%20Jaime%20S.%20Cardoso%20and%20Ana%20F.%20Sequeira&entry.1292438233=%20%20Bias%20has%20been%20a%20constant%20in%20face%20recognition%20models.%20Over%20the%20years%2C%0Aresearchers%20have%20looked%20at%20it%20from%20both%20the%20model%20and%20the%20data%20point%20of%20view.%0AHowever%2C%20their%20approach%20to%20mitigation%20of%20data%20bias%20was%20limited%20and%20lacked%0Ainsight%20on%20the%20real%20nature%20of%20the%20problem.%20Here%2C%20in%20this%20document%2C%20we%20propose%0Ato%20revise%20our%20use%20of%20ethnicity%20labels%20as%20a%20continuous%20variable%20instead%20of%20a%0Adiscrete%20value%20per%20identity.%20We%20validate%20our%20formulation%20both%20experimentally%0Aand%20theoretically%2C%20showcasing%20that%20not%20all%20identities%20from%20one%20ethnicity%0Acontribute%20equally%20to%20the%20balance%20of%20the%20dataset%3B%20thus%2C%20having%20the%20same%20number%0Aof%20identities%20per%20ethnicity%20does%20not%20represent%20a%20balanced%20dataset.%20We%20further%0Ashow%20that%20models%20trained%20on%20datasets%20balanced%20in%20the%20continuous%20space%0Aconsistently%20outperform%20models%20trained%20on%20data%20balanced%20in%20the%20discrete%20space.%0AWe%20trained%20more%20than%2065%20different%20models%2C%20and%20created%20more%20than%2020%20subsets%20of%0Athe%20original%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.01532v4&entry.124074799=Read"},
{"title": "Neural Responses to Affective Sentences Reveal Signatures of Depression", "author": "Aditya Kommineni and Woojae Jeong and Kleanthis Avramidis and Colin McDaniel and Myzelle Hughes and Thomas McGee and Elsi Kaiser and Kristina Lerman and Idan A. Blank and Dani Byrd and Assal Habibi and B. Rael Cahn and Sudarsana Kadiri and Takfarinas Medani and Richard M. Leahy and Shrikanth Narayanan", "abstract": "  Major Depressive Disorder (MDD) is a highly prevalent mental health\ncondition, and a deeper understanding of its neurocognitive foundations is\nessential for identifying how core functions such as emotional and\nself-referential processing are affected. We investigate how depression alters\nthe temporal dynamics of emotional processing by measuring neural responses to\nself-referential affective sentences using surface electroencephalography (EEG)\nin healthy and depressed individuals. Our results reveal significant\ngroup-level differences in neural activity during sentence viewing, suggesting\ndisrupted integration of emotional and self-referential information in\ndepression. Deep learning model trained on these responses achieves an area\nunder the receiver operating curve (AUC) of 0.707 in distinguishing healthy\nfrom depressed participants, and 0.624 in differentiating depressed subgroups\nwith and without suicidal ideation. Spatial ablations highlight anterior\nelectrodes associated with semantic and affective processing as key\ncontributors. These findings suggest stable, stimulus-driven neural signatures\nof depression that may inform future diagnostic tools.\n", "link": "http://arxiv.org/abs/2506.06244v1", "date": "2025-06-06", "relevancy": 2.0947, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4228}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.417}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Responses%20to%20Affective%20Sentences%20Reveal%20Signatures%20of%20Depression&body=Title%3A%20Neural%20Responses%20to%20Affective%20Sentences%20Reveal%20Signatures%20of%20Depression%0AAuthor%3A%20Aditya%20Kommineni%20and%20Woojae%20Jeong%20and%20Kleanthis%20Avramidis%20and%20Colin%20McDaniel%20and%20Myzelle%20Hughes%20and%20Thomas%20McGee%20and%20Elsi%20Kaiser%20and%20Kristina%20Lerman%20and%20Idan%20A.%20Blank%20and%20Dani%20Byrd%20and%20Assal%20Habibi%20and%20B.%20Rael%20Cahn%20and%20Sudarsana%20Kadiri%20and%20Takfarinas%20Medani%20and%20Richard%20M.%20Leahy%20and%20Shrikanth%20Narayanan%0AAbstract%3A%20%20%20Major%20Depressive%20Disorder%20%28MDD%29%20is%20a%20highly%20prevalent%20mental%20health%0Acondition%2C%20and%20a%20deeper%20understanding%20of%20its%20neurocognitive%20foundations%20is%0Aessential%20for%20identifying%20how%20core%20functions%20such%20as%20emotional%20and%0Aself-referential%20processing%20are%20affected.%20We%20investigate%20how%20depression%20alters%0Athe%20temporal%20dynamics%20of%20emotional%20processing%20by%20measuring%20neural%20responses%20to%0Aself-referential%20affective%20sentences%20using%20surface%20electroencephalography%20%28EEG%29%0Ain%20healthy%20and%20depressed%20individuals.%20Our%20results%20reveal%20significant%0Agroup-level%20differences%20in%20neural%20activity%20during%20sentence%20viewing%2C%20suggesting%0Adisrupted%20integration%20of%20emotional%20and%20self-referential%20information%20in%0Adepression.%20Deep%20learning%20model%20trained%20on%20these%20responses%20achieves%20an%20area%0Aunder%20the%20receiver%20operating%20curve%20%28AUC%29%20of%200.707%20in%20distinguishing%20healthy%0Afrom%20depressed%20participants%2C%20and%200.624%20in%20differentiating%20depressed%20subgroups%0Awith%20and%20without%20suicidal%20ideation.%20Spatial%20ablations%20highlight%20anterior%0Aelectrodes%20associated%20with%20semantic%20and%20affective%20processing%20as%20key%0Acontributors.%20These%20findings%20suggest%20stable%2C%20stimulus-driven%20neural%20signatures%0Aof%20depression%20that%20may%20inform%20future%20diagnostic%20tools.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06244v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Responses%2520to%2520Affective%2520Sentences%2520Reveal%2520Signatures%2520of%2520Depression%26entry.906535625%3DAditya%2520Kommineni%2520and%2520Woojae%2520Jeong%2520and%2520Kleanthis%2520Avramidis%2520and%2520Colin%2520McDaniel%2520and%2520Myzelle%2520Hughes%2520and%2520Thomas%2520McGee%2520and%2520Elsi%2520Kaiser%2520and%2520Kristina%2520Lerman%2520and%2520Idan%2520A.%2520Blank%2520and%2520Dani%2520Byrd%2520and%2520Assal%2520Habibi%2520and%2520B.%2520Rael%2520Cahn%2520and%2520Sudarsana%2520Kadiri%2520and%2520Takfarinas%2520Medani%2520and%2520Richard%2520M.%2520Leahy%2520and%2520Shrikanth%2520Narayanan%26entry.1292438233%3D%2520%2520Major%2520Depressive%2520Disorder%2520%2528MDD%2529%2520is%2520a%2520highly%2520prevalent%2520mental%2520health%250Acondition%252C%2520and%2520a%2520deeper%2520understanding%2520of%2520its%2520neurocognitive%2520foundations%2520is%250Aessential%2520for%2520identifying%2520how%2520core%2520functions%2520such%2520as%2520emotional%2520and%250Aself-referential%2520processing%2520are%2520affected.%2520We%2520investigate%2520how%2520depression%2520alters%250Athe%2520temporal%2520dynamics%2520of%2520emotional%2520processing%2520by%2520measuring%2520neural%2520responses%2520to%250Aself-referential%2520affective%2520sentences%2520using%2520surface%2520electroencephalography%2520%2528EEG%2529%250Ain%2520healthy%2520and%2520depressed%2520individuals.%2520Our%2520results%2520reveal%2520significant%250Agroup-level%2520differences%2520in%2520neural%2520activity%2520during%2520sentence%2520viewing%252C%2520suggesting%250Adisrupted%2520integration%2520of%2520emotional%2520and%2520self-referential%2520information%2520in%250Adepression.%2520Deep%2520learning%2520model%2520trained%2520on%2520these%2520responses%2520achieves%2520an%2520area%250Aunder%2520the%2520receiver%2520operating%2520curve%2520%2528AUC%2529%2520of%25200.707%2520in%2520distinguishing%2520healthy%250Afrom%2520depressed%2520participants%252C%2520and%25200.624%2520in%2520differentiating%2520depressed%2520subgroups%250Awith%2520and%2520without%2520suicidal%2520ideation.%2520Spatial%2520ablations%2520highlight%2520anterior%250Aelectrodes%2520associated%2520with%2520semantic%2520and%2520affective%2520processing%2520as%2520key%250Acontributors.%2520These%2520findings%2520suggest%2520stable%252C%2520stimulus-driven%2520neural%2520signatures%250Aof%2520depression%2520that%2520may%2520inform%2520future%2520diagnostic%2520tools.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06244v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Responses%20to%20Affective%20Sentences%20Reveal%20Signatures%20of%20Depression&entry.906535625=Aditya%20Kommineni%20and%20Woojae%20Jeong%20and%20Kleanthis%20Avramidis%20and%20Colin%20McDaniel%20and%20Myzelle%20Hughes%20and%20Thomas%20McGee%20and%20Elsi%20Kaiser%20and%20Kristina%20Lerman%20and%20Idan%20A.%20Blank%20and%20Dani%20Byrd%20and%20Assal%20Habibi%20and%20B.%20Rael%20Cahn%20and%20Sudarsana%20Kadiri%20and%20Takfarinas%20Medani%20and%20Richard%20M.%20Leahy%20and%20Shrikanth%20Narayanan&entry.1292438233=%20%20Major%20Depressive%20Disorder%20%28MDD%29%20is%20a%20highly%20prevalent%20mental%20health%0Acondition%2C%20and%20a%20deeper%20understanding%20of%20its%20neurocognitive%20foundations%20is%0Aessential%20for%20identifying%20how%20core%20functions%20such%20as%20emotional%20and%0Aself-referential%20processing%20are%20affected.%20We%20investigate%20how%20depression%20alters%0Athe%20temporal%20dynamics%20of%20emotional%20processing%20by%20measuring%20neural%20responses%20to%0Aself-referential%20affective%20sentences%20using%20surface%20electroencephalography%20%28EEG%29%0Ain%20healthy%20and%20depressed%20individuals.%20Our%20results%20reveal%20significant%0Agroup-level%20differences%20in%20neural%20activity%20during%20sentence%20viewing%2C%20suggesting%0Adisrupted%20integration%20of%20emotional%20and%20self-referential%20information%20in%0Adepression.%20Deep%20learning%20model%20trained%20on%20these%20responses%20achieves%20an%20area%0Aunder%20the%20receiver%20operating%20curve%20%28AUC%29%20of%200.707%20in%20distinguishing%20healthy%0Afrom%20depressed%20participants%2C%20and%200.624%20in%20differentiating%20depressed%20subgroups%0Awith%20and%20without%20suicidal%20ideation.%20Spatial%20ablations%20highlight%20anterior%0Aelectrodes%20associated%20with%20semantic%20and%20affective%20processing%20as%20key%0Acontributors.%20These%20findings%20suggest%20stable%2C%20stimulus-driven%20neural%20signatures%0Aof%20depression%20that%20may%20inform%20future%20diagnostic%20tools.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06244v1&entry.124074799=Read"},
{"title": "Self driving algorithm for an active four wheel drive racecar", "author": "Gergely Bari and Laszlo Palkovics", "abstract": "  Controlling autonomous vehicles at their handling limits is a significant\nchallenge, particularly for electric vehicles with active four wheel drive\n(A4WD) systems offering independent wheel torque control. While traditional\nVehicle Dynamics Control (VDC) methods use complex physics-based models, this\nstudy explores Deep Reinforcement Learning (DRL) to develop a unified,\nhigh-performance controller. We employ the Proximal Policy Optimization (PPO)\nalgorithm to train an agent for optimal lap times in a simulated racecar\n(TORCS) at the tire grip limit. Critically, the agent learns an end-to-end\npolicy that directly maps vehicle states, like velocities, accelerations, and\nyaw rate, to a steering angle command and independent torque commands for each\nof the four wheels. This formulation bypasses conventional pedal inputs and\nexplicit torque vectoring algorithms, allowing the agent to implicitly learn\nthe A4WD control logic needed for maximizing performance and stability.\nSimulation results demonstrate the RL agent learns sophisticated strategies,\ndynamically optimizing wheel torque distribution corner-by-corner to enhance\nhandling and mitigate the vehicle's inherent understeer. The learned behaviors\nmimic and, in aspects of grip utilization, potentially surpass traditional\nphysics-based A4WD controllers while achieving competitive lap times. This\nresearch underscores DRL's potential to create adaptive control systems for\ncomplex vehicle dynamics, suggesting RL is a potent alternative for advancing\nautonomous driving in demanding, grip-limited scenarios for racing and road\nsafety.\n", "link": "http://arxiv.org/abs/2506.06077v1", "date": "2025-06-06", "relevancy": 2.0927, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5327}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5224}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self%20driving%20algorithm%20for%20an%20active%20four%20wheel%20drive%20racecar&body=Title%3A%20Self%20driving%20algorithm%20for%20an%20active%20four%20wheel%20drive%20racecar%0AAuthor%3A%20Gergely%20Bari%20and%20Laszlo%20Palkovics%0AAbstract%3A%20%20%20Controlling%20autonomous%20vehicles%20at%20their%20handling%20limits%20is%20a%20significant%0Achallenge%2C%20particularly%20for%20electric%20vehicles%20with%20active%20four%20wheel%20drive%0A%28A4WD%29%20systems%20offering%20independent%20wheel%20torque%20control.%20While%20traditional%0AVehicle%20Dynamics%20Control%20%28VDC%29%20methods%20use%20complex%20physics-based%20models%2C%20this%0Astudy%20explores%20Deep%20Reinforcement%20Learning%20%28DRL%29%20to%20develop%20a%20unified%2C%0Ahigh-performance%20controller.%20We%20employ%20the%20Proximal%20Policy%20Optimization%20%28PPO%29%0Aalgorithm%20to%20train%20an%20agent%20for%20optimal%20lap%20times%20in%20a%20simulated%20racecar%0A%28TORCS%29%20at%20the%20tire%20grip%20limit.%20Critically%2C%20the%20agent%20learns%20an%20end-to-end%0Apolicy%20that%20directly%20maps%20vehicle%20states%2C%20like%20velocities%2C%20accelerations%2C%20and%0Ayaw%20rate%2C%20to%20a%20steering%20angle%20command%20and%20independent%20torque%20commands%20for%20each%0Aof%20the%20four%20wheels.%20This%20formulation%20bypasses%20conventional%20pedal%20inputs%20and%0Aexplicit%20torque%20vectoring%20algorithms%2C%20allowing%20the%20agent%20to%20implicitly%20learn%0Athe%20A4WD%20control%20logic%20needed%20for%20maximizing%20performance%20and%20stability.%0ASimulation%20results%20demonstrate%20the%20RL%20agent%20learns%20sophisticated%20strategies%2C%0Adynamically%20optimizing%20wheel%20torque%20distribution%20corner-by-corner%20to%20enhance%0Ahandling%20and%20mitigate%20the%20vehicle%27s%20inherent%20understeer.%20The%20learned%20behaviors%0Amimic%20and%2C%20in%20aspects%20of%20grip%20utilization%2C%20potentially%20surpass%20traditional%0Aphysics-based%20A4WD%20controllers%20while%20achieving%20competitive%20lap%20times.%20This%0Aresearch%20underscores%20DRL%27s%20potential%20to%20create%20adaptive%20control%20systems%20for%0Acomplex%20vehicle%20dynamics%2C%20suggesting%20RL%20is%20a%20potent%20alternative%20for%20advancing%0Aautonomous%20driving%20in%20demanding%2C%20grip-limited%20scenarios%20for%20racing%20and%20road%0Asafety.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06077v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf%2520driving%2520algorithm%2520for%2520an%2520active%2520four%2520wheel%2520drive%2520racecar%26entry.906535625%3DGergely%2520Bari%2520and%2520Laszlo%2520Palkovics%26entry.1292438233%3D%2520%2520Controlling%2520autonomous%2520vehicles%2520at%2520their%2520handling%2520limits%2520is%2520a%2520significant%250Achallenge%252C%2520particularly%2520for%2520electric%2520vehicles%2520with%2520active%2520four%2520wheel%2520drive%250A%2528A4WD%2529%2520systems%2520offering%2520independent%2520wheel%2520torque%2520control.%2520While%2520traditional%250AVehicle%2520Dynamics%2520Control%2520%2528VDC%2529%2520methods%2520use%2520complex%2520physics-based%2520models%252C%2520this%250Astudy%2520explores%2520Deep%2520Reinforcement%2520Learning%2520%2528DRL%2529%2520to%2520develop%2520a%2520unified%252C%250Ahigh-performance%2520controller.%2520We%2520employ%2520the%2520Proximal%2520Policy%2520Optimization%2520%2528PPO%2529%250Aalgorithm%2520to%2520train%2520an%2520agent%2520for%2520optimal%2520lap%2520times%2520in%2520a%2520simulated%2520racecar%250A%2528TORCS%2529%2520at%2520the%2520tire%2520grip%2520limit.%2520Critically%252C%2520the%2520agent%2520learns%2520an%2520end-to-end%250Apolicy%2520that%2520directly%2520maps%2520vehicle%2520states%252C%2520like%2520velocities%252C%2520accelerations%252C%2520and%250Ayaw%2520rate%252C%2520to%2520a%2520steering%2520angle%2520command%2520and%2520independent%2520torque%2520commands%2520for%2520each%250Aof%2520the%2520four%2520wheels.%2520This%2520formulation%2520bypasses%2520conventional%2520pedal%2520inputs%2520and%250Aexplicit%2520torque%2520vectoring%2520algorithms%252C%2520allowing%2520the%2520agent%2520to%2520implicitly%2520learn%250Athe%2520A4WD%2520control%2520logic%2520needed%2520for%2520maximizing%2520performance%2520and%2520stability.%250ASimulation%2520results%2520demonstrate%2520the%2520RL%2520agent%2520learns%2520sophisticated%2520strategies%252C%250Adynamically%2520optimizing%2520wheel%2520torque%2520distribution%2520corner-by-corner%2520to%2520enhance%250Ahandling%2520and%2520mitigate%2520the%2520vehicle%2527s%2520inherent%2520understeer.%2520The%2520learned%2520behaviors%250Amimic%2520and%252C%2520in%2520aspects%2520of%2520grip%2520utilization%252C%2520potentially%2520surpass%2520traditional%250Aphysics-based%2520A4WD%2520controllers%2520while%2520achieving%2520competitive%2520lap%2520times.%2520This%250Aresearch%2520underscores%2520DRL%2527s%2520potential%2520to%2520create%2520adaptive%2520control%2520systems%2520for%250Acomplex%2520vehicle%2520dynamics%252C%2520suggesting%2520RL%2520is%2520a%2520potent%2520alternative%2520for%2520advancing%250Aautonomous%2520driving%2520in%2520demanding%252C%2520grip-limited%2520scenarios%2520for%2520racing%2520and%2520road%250Asafety.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06077v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self%20driving%20algorithm%20for%20an%20active%20four%20wheel%20drive%20racecar&entry.906535625=Gergely%20Bari%20and%20Laszlo%20Palkovics&entry.1292438233=%20%20Controlling%20autonomous%20vehicles%20at%20their%20handling%20limits%20is%20a%20significant%0Achallenge%2C%20particularly%20for%20electric%20vehicles%20with%20active%20four%20wheel%20drive%0A%28A4WD%29%20systems%20offering%20independent%20wheel%20torque%20control.%20While%20traditional%0AVehicle%20Dynamics%20Control%20%28VDC%29%20methods%20use%20complex%20physics-based%20models%2C%20this%0Astudy%20explores%20Deep%20Reinforcement%20Learning%20%28DRL%29%20to%20develop%20a%20unified%2C%0Ahigh-performance%20controller.%20We%20employ%20the%20Proximal%20Policy%20Optimization%20%28PPO%29%0Aalgorithm%20to%20train%20an%20agent%20for%20optimal%20lap%20times%20in%20a%20simulated%20racecar%0A%28TORCS%29%20at%20the%20tire%20grip%20limit.%20Critically%2C%20the%20agent%20learns%20an%20end-to-end%0Apolicy%20that%20directly%20maps%20vehicle%20states%2C%20like%20velocities%2C%20accelerations%2C%20and%0Ayaw%20rate%2C%20to%20a%20steering%20angle%20command%20and%20independent%20torque%20commands%20for%20each%0Aof%20the%20four%20wheels.%20This%20formulation%20bypasses%20conventional%20pedal%20inputs%20and%0Aexplicit%20torque%20vectoring%20algorithms%2C%20allowing%20the%20agent%20to%20implicitly%20learn%0Athe%20A4WD%20control%20logic%20needed%20for%20maximizing%20performance%20and%20stability.%0ASimulation%20results%20demonstrate%20the%20RL%20agent%20learns%20sophisticated%20strategies%2C%0Adynamically%20optimizing%20wheel%20torque%20distribution%20corner-by-corner%20to%20enhance%0Ahandling%20and%20mitigate%20the%20vehicle%27s%20inherent%20understeer.%20The%20learned%20behaviors%0Amimic%20and%2C%20in%20aspects%20of%20grip%20utilization%2C%20potentially%20surpass%20traditional%0Aphysics-based%20A4WD%20controllers%20while%20achieving%20competitive%20lap%20times.%20This%0Aresearch%20underscores%20DRL%27s%20potential%20to%20create%20adaptive%20control%20systems%20for%0Acomplex%20vehicle%20dynamics%2C%20suggesting%20RL%20is%20a%20potent%20alternative%20for%20advancing%0Aautonomous%20driving%20in%20demanding%2C%20grip-limited%20scenarios%20for%20racing%20and%20road%0Asafety.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06077v1&entry.124074799=Read"},
{"title": "CP-Bench: Evaluating Large Language Models for Constraint Modelling", "author": "Kostis Michailidis and Dimos Tsouros and Tias Guns", "abstract": "  Combinatorial problems are present in a wide range of industries. Constraint\nProgramming (CP) is a well-suited problem-solving paradigm, but its core\nprocess, namely constraint modelling, is a bottleneck for wider adoption.\nAiming to alleviate this bottleneck, recent studies have explored using Large\nLanguage Models (LLMs) as modelling assistants, transforming combinatorial\nproblem descriptions to executable constraint models, similar to coding\nassistants. However, the existing evaluation datasets for constraint modelling\nare often limited to small, homogeneous, or domain-specific instances, which do\nnot capture the diversity of real-world scenarios. This work addresses this gap\nby introducing CP-Bench, a novel benchmark dataset that includes a diverse set\nof well-known combinatorial problem classes sourced from the CP community,\nstructured explicitly for evaluating LLM-driven CP modelling. With this\ndataset, and given the variety of constraint modelling frameworks, we compare\nand evaluate the modelling capabilities of LLMs for three distinct constraint\nmodelling systems, which vary in abstraction level and underlying syntax: the\nhigh-level MiniZinc language and Python-based CPMpy library, and the\nlower-level Python interface of the OR-Tools CP-SAT solver. In order to enhance\nthe ability of LLMs to produce valid constraint models, we systematically\nevaluate the use of prompt-based and inference-time compute methods adapted\nfrom existing LLM-based code generation research. Our results underscore the\nmodelling convenience provided by Python-based frameworks, as well as the\neffectiveness of documentation-rich system prompts, which, augmented with\nrepeated sampling and self-verification, achieve further improvements, reaching\nup to 70\\% accuracy on this new, highly challenging benchmark.\n", "link": "http://arxiv.org/abs/2506.06052v1", "date": "2025-06-06", "relevancy": 2.0878, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5321}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5321}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CP-Bench%3A%20Evaluating%20Large%20Language%20Models%20for%20Constraint%20Modelling&body=Title%3A%20CP-Bench%3A%20Evaluating%20Large%20Language%20Models%20for%20Constraint%20Modelling%0AAuthor%3A%20Kostis%20Michailidis%20and%20Dimos%20Tsouros%20and%20Tias%20Guns%0AAbstract%3A%20%20%20Combinatorial%20problems%20are%20present%20in%20a%20wide%20range%20of%20industries.%20Constraint%0AProgramming%20%28CP%29%20is%20a%20well-suited%20problem-solving%20paradigm%2C%20but%20its%20core%0Aprocess%2C%20namely%20constraint%20modelling%2C%20is%20a%20bottleneck%20for%20wider%20adoption.%0AAiming%20to%20alleviate%20this%20bottleneck%2C%20recent%20studies%20have%20explored%20using%20Large%0ALanguage%20Models%20%28LLMs%29%20as%20modelling%20assistants%2C%20transforming%20combinatorial%0Aproblem%20descriptions%20to%20executable%20constraint%20models%2C%20similar%20to%20coding%0Aassistants.%20However%2C%20the%20existing%20evaluation%20datasets%20for%20constraint%20modelling%0Aare%20often%20limited%20to%20small%2C%20homogeneous%2C%20or%20domain-specific%20instances%2C%20which%20do%0Anot%20capture%20the%20diversity%20of%20real-world%20scenarios.%20This%20work%20addresses%20this%20gap%0Aby%20introducing%20CP-Bench%2C%20a%20novel%20benchmark%20dataset%20that%20includes%20a%20diverse%20set%0Aof%20well-known%20combinatorial%20problem%20classes%20sourced%20from%20the%20CP%20community%2C%0Astructured%20explicitly%20for%20evaluating%20LLM-driven%20CP%20modelling.%20With%20this%0Adataset%2C%20and%20given%20the%20variety%20of%20constraint%20modelling%20frameworks%2C%20we%20compare%0Aand%20evaluate%20the%20modelling%20capabilities%20of%20LLMs%20for%20three%20distinct%20constraint%0Amodelling%20systems%2C%20which%20vary%20in%20abstraction%20level%20and%20underlying%20syntax%3A%20the%0Ahigh-level%20MiniZinc%20language%20and%20Python-based%20CPMpy%20library%2C%20and%20the%0Alower-level%20Python%20interface%20of%20the%20OR-Tools%20CP-SAT%20solver.%20In%20order%20to%20enhance%0Athe%20ability%20of%20LLMs%20to%20produce%20valid%20constraint%20models%2C%20we%20systematically%0Aevaluate%20the%20use%20of%20prompt-based%20and%20inference-time%20compute%20methods%20adapted%0Afrom%20existing%20LLM-based%20code%20generation%20research.%20Our%20results%20underscore%20the%0Amodelling%20convenience%20provided%20by%20Python-based%20frameworks%2C%20as%20well%20as%20the%0Aeffectiveness%20of%20documentation-rich%20system%20prompts%2C%20which%2C%20augmented%20with%0Arepeated%20sampling%20and%20self-verification%2C%20achieve%20further%20improvements%2C%20reaching%0Aup%20to%2070%5C%25%20accuracy%20on%20this%20new%2C%20highly%20challenging%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06052v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCP-Bench%253A%2520Evaluating%2520Large%2520Language%2520Models%2520for%2520Constraint%2520Modelling%26entry.906535625%3DKostis%2520Michailidis%2520and%2520Dimos%2520Tsouros%2520and%2520Tias%2520Guns%26entry.1292438233%3D%2520%2520Combinatorial%2520problems%2520are%2520present%2520in%2520a%2520wide%2520range%2520of%2520industries.%2520Constraint%250AProgramming%2520%2528CP%2529%2520is%2520a%2520well-suited%2520problem-solving%2520paradigm%252C%2520but%2520its%2520core%250Aprocess%252C%2520namely%2520constraint%2520modelling%252C%2520is%2520a%2520bottleneck%2520for%2520wider%2520adoption.%250AAiming%2520to%2520alleviate%2520this%2520bottleneck%252C%2520recent%2520studies%2520have%2520explored%2520using%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520as%2520modelling%2520assistants%252C%2520transforming%2520combinatorial%250Aproblem%2520descriptions%2520to%2520executable%2520constraint%2520models%252C%2520similar%2520to%2520coding%250Aassistants.%2520However%252C%2520the%2520existing%2520evaluation%2520datasets%2520for%2520constraint%2520modelling%250Aare%2520often%2520limited%2520to%2520small%252C%2520homogeneous%252C%2520or%2520domain-specific%2520instances%252C%2520which%2520do%250Anot%2520capture%2520the%2520diversity%2520of%2520real-world%2520scenarios.%2520This%2520work%2520addresses%2520this%2520gap%250Aby%2520introducing%2520CP-Bench%252C%2520a%2520novel%2520benchmark%2520dataset%2520that%2520includes%2520a%2520diverse%2520set%250Aof%2520well-known%2520combinatorial%2520problem%2520classes%2520sourced%2520from%2520the%2520CP%2520community%252C%250Astructured%2520explicitly%2520for%2520evaluating%2520LLM-driven%2520CP%2520modelling.%2520With%2520this%250Adataset%252C%2520and%2520given%2520the%2520variety%2520of%2520constraint%2520modelling%2520frameworks%252C%2520we%2520compare%250Aand%2520evaluate%2520the%2520modelling%2520capabilities%2520of%2520LLMs%2520for%2520three%2520distinct%2520constraint%250Amodelling%2520systems%252C%2520which%2520vary%2520in%2520abstraction%2520level%2520and%2520underlying%2520syntax%253A%2520the%250Ahigh-level%2520MiniZinc%2520language%2520and%2520Python-based%2520CPMpy%2520library%252C%2520and%2520the%250Alower-level%2520Python%2520interface%2520of%2520the%2520OR-Tools%2520CP-SAT%2520solver.%2520In%2520order%2520to%2520enhance%250Athe%2520ability%2520of%2520LLMs%2520to%2520produce%2520valid%2520constraint%2520models%252C%2520we%2520systematically%250Aevaluate%2520the%2520use%2520of%2520prompt-based%2520and%2520inference-time%2520compute%2520methods%2520adapted%250Afrom%2520existing%2520LLM-based%2520code%2520generation%2520research.%2520Our%2520results%2520underscore%2520the%250Amodelling%2520convenience%2520provided%2520by%2520Python-based%2520frameworks%252C%2520as%2520well%2520as%2520the%250Aeffectiveness%2520of%2520documentation-rich%2520system%2520prompts%252C%2520which%252C%2520augmented%2520with%250Arepeated%2520sampling%2520and%2520self-verification%252C%2520achieve%2520further%2520improvements%252C%2520reaching%250Aup%2520to%252070%255C%2525%2520accuracy%2520on%2520this%2520new%252C%2520highly%2520challenging%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06052v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CP-Bench%3A%20Evaluating%20Large%20Language%20Models%20for%20Constraint%20Modelling&entry.906535625=Kostis%20Michailidis%20and%20Dimos%20Tsouros%20and%20Tias%20Guns&entry.1292438233=%20%20Combinatorial%20problems%20are%20present%20in%20a%20wide%20range%20of%20industries.%20Constraint%0AProgramming%20%28CP%29%20is%20a%20well-suited%20problem-solving%20paradigm%2C%20but%20its%20core%0Aprocess%2C%20namely%20constraint%20modelling%2C%20is%20a%20bottleneck%20for%20wider%20adoption.%0AAiming%20to%20alleviate%20this%20bottleneck%2C%20recent%20studies%20have%20explored%20using%20Large%0ALanguage%20Models%20%28LLMs%29%20as%20modelling%20assistants%2C%20transforming%20combinatorial%0Aproblem%20descriptions%20to%20executable%20constraint%20models%2C%20similar%20to%20coding%0Aassistants.%20However%2C%20the%20existing%20evaluation%20datasets%20for%20constraint%20modelling%0Aare%20often%20limited%20to%20small%2C%20homogeneous%2C%20or%20domain-specific%20instances%2C%20which%20do%0Anot%20capture%20the%20diversity%20of%20real-world%20scenarios.%20This%20work%20addresses%20this%20gap%0Aby%20introducing%20CP-Bench%2C%20a%20novel%20benchmark%20dataset%20that%20includes%20a%20diverse%20set%0Aof%20well-known%20combinatorial%20problem%20classes%20sourced%20from%20the%20CP%20community%2C%0Astructured%20explicitly%20for%20evaluating%20LLM-driven%20CP%20modelling.%20With%20this%0Adataset%2C%20and%20given%20the%20variety%20of%20constraint%20modelling%20frameworks%2C%20we%20compare%0Aand%20evaluate%20the%20modelling%20capabilities%20of%20LLMs%20for%20three%20distinct%20constraint%0Amodelling%20systems%2C%20which%20vary%20in%20abstraction%20level%20and%20underlying%20syntax%3A%20the%0Ahigh-level%20MiniZinc%20language%20and%20Python-based%20CPMpy%20library%2C%20and%20the%0Alower-level%20Python%20interface%20of%20the%20OR-Tools%20CP-SAT%20solver.%20In%20order%20to%20enhance%0Athe%20ability%20of%20LLMs%20to%20produce%20valid%20constraint%20models%2C%20we%20systematically%0Aevaluate%20the%20use%20of%20prompt-based%20and%20inference-time%20compute%20methods%20adapted%0Afrom%20existing%20LLM-based%20code%20generation%20research.%20Our%20results%20underscore%20the%0Amodelling%20convenience%20provided%20by%20Python-based%20frameworks%2C%20as%20well%20as%20the%0Aeffectiveness%20of%20documentation-rich%20system%20prompts%2C%20which%2C%20augmented%20with%0Arepeated%20sampling%20and%20self-verification%2C%20achieve%20further%20improvements%2C%20reaching%0Aup%20to%2070%5C%25%20accuracy%20on%20this%20new%2C%20highly%20challenging%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06052v1&entry.124074799=Read"},
{"title": "LlavaGuard: An Open VLM-based Framework for Safeguarding Vision Datasets\n  and Models", "author": "Lukas Helff and Felix Friedrich and Manuel Brack and Kristian Kersting and Patrick Schramowski", "abstract": "  This paper introduces LlavaGuard, a suite of VLM-based vision safeguards that\naddress the critical need for reliable guardrails in the era of large-scale\ndata and models. To this end, we establish a novel open framework, describing a\ncustomizable safety taxonomy, data preprocessing, augmentation, and training\nsetup. For teaching a VLM safeguard on safety, we further create a multimodal\nsafety dataset with high-quality human expert annotations, where each image is\nlabeled with a safety rating, category, and rationale. We also employ advanced\naugmentations to support context-specific assessments. The resulting LlavaGuard\nmodels, ranging from 0.5B to 7B, serve as a versatile tool for evaluating the\nsafety compliance of visual content against flexible policies. In comprehensive\nexperiments, LlavaGuard outperforms both state-of-the-art safeguards and VLMs\nin accuracy and in flexibly handling different policies. Additionally, we\ndemonstrate LlavaGuard's performance in two real-world applications:\nlarge-scale dataset annotation and moderation of text-to-image models. We make\nour entire framework, including the dataset, model weights, and training code.\n", "link": "http://arxiv.org/abs/2406.05113v3", "date": "2025-06-06", "relevancy": 2.0828, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5491}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5202}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LlavaGuard%3A%20An%20Open%20VLM-based%20Framework%20for%20Safeguarding%20Vision%20Datasets%0A%20%20and%20Models&body=Title%3A%20LlavaGuard%3A%20An%20Open%20VLM-based%20Framework%20for%20Safeguarding%20Vision%20Datasets%0A%20%20and%20Models%0AAuthor%3A%20Lukas%20Helff%20and%20Felix%20Friedrich%20and%20Manuel%20Brack%20and%20Kristian%20Kersting%20and%20Patrick%20Schramowski%0AAbstract%3A%20%20%20This%20paper%20introduces%20LlavaGuard%2C%20a%20suite%20of%20VLM-based%20vision%20safeguards%20that%0Aaddress%20the%20critical%20need%20for%20reliable%20guardrails%20in%20the%20era%20of%20large-scale%0Adata%20and%20models.%20To%20this%20end%2C%20we%20establish%20a%20novel%20open%20framework%2C%20describing%20a%0Acustomizable%20safety%20taxonomy%2C%20data%20preprocessing%2C%20augmentation%2C%20and%20training%0Asetup.%20For%20teaching%20a%20VLM%20safeguard%20on%20safety%2C%20we%20further%20create%20a%20multimodal%0Asafety%20dataset%20with%20high-quality%20human%20expert%20annotations%2C%20where%20each%20image%20is%0Alabeled%20with%20a%20safety%20rating%2C%20category%2C%20and%20rationale.%20We%20also%20employ%20advanced%0Aaugmentations%20to%20support%20context-specific%20assessments.%20The%20resulting%20LlavaGuard%0Amodels%2C%20ranging%20from%200.5B%20to%207B%2C%20serve%20as%20a%20versatile%20tool%20for%20evaluating%20the%0Asafety%20compliance%20of%20visual%20content%20against%20flexible%20policies.%20In%20comprehensive%0Aexperiments%2C%20LlavaGuard%20outperforms%20both%20state-of-the-art%20safeguards%20and%20VLMs%0Ain%20accuracy%20and%20in%20flexibly%20handling%20different%20policies.%20Additionally%2C%20we%0Ademonstrate%20LlavaGuard%27s%20performance%20in%20two%20real-world%20applications%3A%0Alarge-scale%20dataset%20annotation%20and%20moderation%20of%20text-to-image%20models.%20We%20make%0Aour%20entire%20framework%2C%20including%20the%20dataset%2C%20model%20weights%2C%20and%20training%20code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05113v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLlavaGuard%253A%2520An%2520Open%2520VLM-based%2520Framework%2520for%2520Safeguarding%2520Vision%2520Datasets%250A%2520%2520and%2520Models%26entry.906535625%3DLukas%2520Helff%2520and%2520Felix%2520Friedrich%2520and%2520Manuel%2520Brack%2520and%2520Kristian%2520Kersting%2520and%2520Patrick%2520Schramowski%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520LlavaGuard%252C%2520a%2520suite%2520of%2520VLM-based%2520vision%2520safeguards%2520that%250Aaddress%2520the%2520critical%2520need%2520for%2520reliable%2520guardrails%2520in%2520the%2520era%2520of%2520large-scale%250Adata%2520and%2520models.%2520To%2520this%2520end%252C%2520we%2520establish%2520a%2520novel%2520open%2520framework%252C%2520describing%2520a%250Acustomizable%2520safety%2520taxonomy%252C%2520data%2520preprocessing%252C%2520augmentation%252C%2520and%2520training%250Asetup.%2520For%2520teaching%2520a%2520VLM%2520safeguard%2520on%2520safety%252C%2520we%2520further%2520create%2520a%2520multimodal%250Asafety%2520dataset%2520with%2520high-quality%2520human%2520expert%2520annotations%252C%2520where%2520each%2520image%2520is%250Alabeled%2520with%2520a%2520safety%2520rating%252C%2520category%252C%2520and%2520rationale.%2520We%2520also%2520employ%2520advanced%250Aaugmentations%2520to%2520support%2520context-specific%2520assessments.%2520The%2520resulting%2520LlavaGuard%250Amodels%252C%2520ranging%2520from%25200.5B%2520to%25207B%252C%2520serve%2520as%2520a%2520versatile%2520tool%2520for%2520evaluating%2520the%250Asafety%2520compliance%2520of%2520visual%2520content%2520against%2520flexible%2520policies.%2520In%2520comprehensive%250Aexperiments%252C%2520LlavaGuard%2520outperforms%2520both%2520state-of-the-art%2520safeguards%2520and%2520VLMs%250Ain%2520accuracy%2520and%2520in%2520flexibly%2520handling%2520different%2520policies.%2520Additionally%252C%2520we%250Ademonstrate%2520LlavaGuard%2527s%2520performance%2520in%2520two%2520real-world%2520applications%253A%250Alarge-scale%2520dataset%2520annotation%2520and%2520moderation%2520of%2520text-to-image%2520models.%2520We%2520make%250Aour%2520entire%2520framework%252C%2520including%2520the%2520dataset%252C%2520model%2520weights%252C%2520and%2520training%2520code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05113v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LlavaGuard%3A%20An%20Open%20VLM-based%20Framework%20for%20Safeguarding%20Vision%20Datasets%0A%20%20and%20Models&entry.906535625=Lukas%20Helff%20and%20Felix%20Friedrich%20and%20Manuel%20Brack%20and%20Kristian%20Kersting%20and%20Patrick%20Schramowski&entry.1292438233=%20%20This%20paper%20introduces%20LlavaGuard%2C%20a%20suite%20of%20VLM-based%20vision%20safeguards%20that%0Aaddress%20the%20critical%20need%20for%20reliable%20guardrails%20in%20the%20era%20of%20large-scale%0Adata%20and%20models.%20To%20this%20end%2C%20we%20establish%20a%20novel%20open%20framework%2C%20describing%20a%0Acustomizable%20safety%20taxonomy%2C%20data%20preprocessing%2C%20augmentation%2C%20and%20training%0Asetup.%20For%20teaching%20a%20VLM%20safeguard%20on%20safety%2C%20we%20further%20create%20a%20multimodal%0Asafety%20dataset%20with%20high-quality%20human%20expert%20annotations%2C%20where%20each%20image%20is%0Alabeled%20with%20a%20safety%20rating%2C%20category%2C%20and%20rationale.%20We%20also%20employ%20advanced%0Aaugmentations%20to%20support%20context-specific%20assessments.%20The%20resulting%20LlavaGuard%0Amodels%2C%20ranging%20from%200.5B%20to%207B%2C%20serve%20as%20a%20versatile%20tool%20for%20evaluating%20the%0Asafety%20compliance%20of%20visual%20content%20against%20flexible%20policies.%20In%20comprehensive%0Aexperiments%2C%20LlavaGuard%20outperforms%20both%20state-of-the-art%20safeguards%20and%20VLMs%0Ain%20accuracy%20and%20in%20flexibly%20handling%20different%20policies.%20Additionally%2C%20we%0Ademonstrate%20LlavaGuard%27s%20performance%20in%20two%20real-world%20applications%3A%0Alarge-scale%20dataset%20annotation%20and%20moderation%20of%20text-to-image%20models.%20We%20make%0Aour%20entire%20framework%2C%20including%20the%20dataset%2C%20model%20weights%2C%20and%20training%20code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05113v3&entry.124074799=Read"},
{"title": "DyGMamba: Efficiently Modeling Long-Term Temporal Dependency on\n  Continuous-Time Dynamic Graphs with State Space Models", "author": "Zifeng Ding and Yifeng Li and Yuan He and Antonio Norelli and Jingcheng Wu and Volker Tresp and Michael Bronstein and Yunpu Ma", "abstract": "  Learning useful representations for continuous-time dynamic graphs (CTDGs) is\nchallenging, due to the concurrent need to span long node interaction histories\nand grasp nuanced temporal details. In particular, two problems emerge: (1)\nEncoding longer histories requires more computational resources, making it\ncrucial for CTDG models to maintain low computational complexity to ensure\nefficiency; (2) Meanwhile, more powerful models are needed to identify and\nselect the most critical temporal information within the extended context\nprovided by longer histories. To address these problems, we propose a CTDG\nrepresentation learning model named DyGMamba, originating from the popular\nMamba state space model (SSM). DyGMamba first leverages a node-level SSM to\nencode the sequence of historical node interactions. Another time-level SSM is\nthen employed to exploit the temporal patterns hidden in the historical graph,\nwhere its output is used to dynamically select the critical information from\nthe interaction history. We validate DyGMamba experimentally on the dynamic\nlink prediction task. The results show that our model achieves state-of-the-art\nin most cases. DyGMamba also maintains high efficiency in terms of\ncomputational resources, making it possible to capture long temporal\ndependencies with a limited computation budget.\n", "link": "http://arxiv.org/abs/2408.04713v4", "date": "2025-06-06", "relevancy": 2.0815, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5387}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5174}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DyGMamba%3A%20Efficiently%20Modeling%20Long-Term%20Temporal%20Dependency%20on%0A%20%20Continuous-Time%20Dynamic%20Graphs%20with%20State%20Space%20Models&body=Title%3A%20DyGMamba%3A%20Efficiently%20Modeling%20Long-Term%20Temporal%20Dependency%20on%0A%20%20Continuous-Time%20Dynamic%20Graphs%20with%20State%20Space%20Models%0AAuthor%3A%20Zifeng%20Ding%20and%20Yifeng%20Li%20and%20Yuan%20He%20and%20Antonio%20Norelli%20and%20Jingcheng%20Wu%20and%20Volker%20Tresp%20and%20Michael%20Bronstein%20and%20Yunpu%20Ma%0AAbstract%3A%20%20%20Learning%20useful%20representations%20for%20continuous-time%20dynamic%20graphs%20%28CTDGs%29%20is%0Achallenging%2C%20due%20to%20the%20concurrent%20need%20to%20span%20long%20node%20interaction%20histories%0Aand%20grasp%20nuanced%20temporal%20details.%20In%20particular%2C%20two%20problems%20emerge%3A%20%281%29%0AEncoding%20longer%20histories%20requires%20more%20computational%20resources%2C%20making%20it%0Acrucial%20for%20CTDG%20models%20to%20maintain%20low%20computational%20complexity%20to%20ensure%0Aefficiency%3B%20%282%29%20Meanwhile%2C%20more%20powerful%20models%20are%20needed%20to%20identify%20and%0Aselect%20the%20most%20critical%20temporal%20information%20within%20the%20extended%20context%0Aprovided%20by%20longer%20histories.%20To%20address%20these%20problems%2C%20we%20propose%20a%20CTDG%0Arepresentation%20learning%20model%20named%20DyGMamba%2C%20originating%20from%20the%20popular%0AMamba%20state%20space%20model%20%28SSM%29.%20DyGMamba%20first%20leverages%20a%20node-level%20SSM%20to%0Aencode%20the%20sequence%20of%20historical%20node%20interactions.%20Another%20time-level%20SSM%20is%0Athen%20employed%20to%20exploit%20the%20temporal%20patterns%20hidden%20in%20the%20historical%20graph%2C%0Awhere%20its%20output%20is%20used%20to%20dynamically%20select%20the%20critical%20information%20from%0Athe%20interaction%20history.%20We%20validate%20DyGMamba%20experimentally%20on%20the%20dynamic%0Alink%20prediction%20task.%20The%20results%20show%20that%20our%20model%20achieves%20state-of-the-art%0Ain%20most%20cases.%20DyGMamba%20also%20maintains%20high%20efficiency%20in%20terms%20of%0Acomputational%20resources%2C%20making%20it%20possible%20to%20capture%20long%20temporal%0Adependencies%20with%20a%20limited%20computation%20budget.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04713v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDyGMamba%253A%2520Efficiently%2520Modeling%2520Long-Term%2520Temporal%2520Dependency%2520on%250A%2520%2520Continuous-Time%2520Dynamic%2520Graphs%2520with%2520State%2520Space%2520Models%26entry.906535625%3DZifeng%2520Ding%2520and%2520Yifeng%2520Li%2520and%2520Yuan%2520He%2520and%2520Antonio%2520Norelli%2520and%2520Jingcheng%2520Wu%2520and%2520Volker%2520Tresp%2520and%2520Michael%2520Bronstein%2520and%2520Yunpu%2520Ma%26entry.1292438233%3D%2520%2520Learning%2520useful%2520representations%2520for%2520continuous-time%2520dynamic%2520graphs%2520%2528CTDGs%2529%2520is%250Achallenging%252C%2520due%2520to%2520the%2520concurrent%2520need%2520to%2520span%2520long%2520node%2520interaction%2520histories%250Aand%2520grasp%2520nuanced%2520temporal%2520details.%2520In%2520particular%252C%2520two%2520problems%2520emerge%253A%2520%25281%2529%250AEncoding%2520longer%2520histories%2520requires%2520more%2520computational%2520resources%252C%2520making%2520it%250Acrucial%2520for%2520CTDG%2520models%2520to%2520maintain%2520low%2520computational%2520complexity%2520to%2520ensure%250Aefficiency%253B%2520%25282%2529%2520Meanwhile%252C%2520more%2520powerful%2520models%2520are%2520needed%2520to%2520identify%2520and%250Aselect%2520the%2520most%2520critical%2520temporal%2520information%2520within%2520the%2520extended%2520context%250Aprovided%2520by%2520longer%2520histories.%2520To%2520address%2520these%2520problems%252C%2520we%2520propose%2520a%2520CTDG%250Arepresentation%2520learning%2520model%2520named%2520DyGMamba%252C%2520originating%2520from%2520the%2520popular%250AMamba%2520state%2520space%2520model%2520%2528SSM%2529.%2520DyGMamba%2520first%2520leverages%2520a%2520node-level%2520SSM%2520to%250Aencode%2520the%2520sequence%2520of%2520historical%2520node%2520interactions.%2520Another%2520time-level%2520SSM%2520is%250Athen%2520employed%2520to%2520exploit%2520the%2520temporal%2520patterns%2520hidden%2520in%2520the%2520historical%2520graph%252C%250Awhere%2520its%2520output%2520is%2520used%2520to%2520dynamically%2520select%2520the%2520critical%2520information%2520from%250Athe%2520interaction%2520history.%2520We%2520validate%2520DyGMamba%2520experimentally%2520on%2520the%2520dynamic%250Alink%2520prediction%2520task.%2520The%2520results%2520show%2520that%2520our%2520model%2520achieves%2520state-of-the-art%250Ain%2520most%2520cases.%2520DyGMamba%2520also%2520maintains%2520high%2520efficiency%2520in%2520terms%2520of%250Acomputational%2520resources%252C%2520making%2520it%2520possible%2520to%2520capture%2520long%2520temporal%250Adependencies%2520with%2520a%2520limited%2520computation%2520budget.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04713v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DyGMamba%3A%20Efficiently%20Modeling%20Long-Term%20Temporal%20Dependency%20on%0A%20%20Continuous-Time%20Dynamic%20Graphs%20with%20State%20Space%20Models&entry.906535625=Zifeng%20Ding%20and%20Yifeng%20Li%20and%20Yuan%20He%20and%20Antonio%20Norelli%20and%20Jingcheng%20Wu%20and%20Volker%20Tresp%20and%20Michael%20Bronstein%20and%20Yunpu%20Ma&entry.1292438233=%20%20Learning%20useful%20representations%20for%20continuous-time%20dynamic%20graphs%20%28CTDGs%29%20is%0Achallenging%2C%20due%20to%20the%20concurrent%20need%20to%20span%20long%20node%20interaction%20histories%0Aand%20grasp%20nuanced%20temporal%20details.%20In%20particular%2C%20two%20problems%20emerge%3A%20%281%29%0AEncoding%20longer%20histories%20requires%20more%20computational%20resources%2C%20making%20it%0Acrucial%20for%20CTDG%20models%20to%20maintain%20low%20computational%20complexity%20to%20ensure%0Aefficiency%3B%20%282%29%20Meanwhile%2C%20more%20powerful%20models%20are%20needed%20to%20identify%20and%0Aselect%20the%20most%20critical%20temporal%20information%20within%20the%20extended%20context%0Aprovided%20by%20longer%20histories.%20To%20address%20these%20problems%2C%20we%20propose%20a%20CTDG%0Arepresentation%20learning%20model%20named%20DyGMamba%2C%20originating%20from%20the%20popular%0AMamba%20state%20space%20model%20%28SSM%29.%20DyGMamba%20first%20leverages%20a%20node-level%20SSM%20to%0Aencode%20the%20sequence%20of%20historical%20node%20interactions.%20Another%20time-level%20SSM%20is%0Athen%20employed%20to%20exploit%20the%20temporal%20patterns%20hidden%20in%20the%20historical%20graph%2C%0Awhere%20its%20output%20is%20used%20to%20dynamically%20select%20the%20critical%20information%20from%0Athe%20interaction%20history.%20We%20validate%20DyGMamba%20experimentally%20on%20the%20dynamic%0Alink%20prediction%20task.%20The%20results%20show%20that%20our%20model%20achieves%20state-of-the-art%0Ain%20most%20cases.%20DyGMamba%20also%20maintains%20high%20efficiency%20in%20terms%20of%0Acomputational%20resources%2C%20making%20it%20possible%20to%20capture%20long%20temporal%0Adependencies%20with%20a%20limited%20computation%20budget.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04713v4&entry.124074799=Read"},
{"title": "MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient\n  Fine-Tuning of Large Language Models", "author": "Jie Cao and Tianwei Lin and Hongyang He and Rolan Yan and Wenqiao Zhang and Juncheng Li and Dongping Zhang and Siliang Tang and Yueting Zhuang", "abstract": "  Recent studies integrate Low-Rank Adaptation (LoRA) and Mixture-of-Experts\n(MoE) to further enhance the performance of parameter-efficient fine-tuning\n(PEFT) methods in Large Language Model (LLM) applications. Existing methods\nemploy \\emph{homogeneous} MoE-LoRA architectures composed of LoRA experts with\neither similar or identical structures and capacities. However, these\napproaches often suffer from representation collapse and expert load imbalance,\nwhich negatively impact the potential of LLMs. To address these challenges, we\npropose a \\emph{heterogeneous} \\textbf{Mixture-of-Adapters (MoA)} approach.\nThis method dynamically integrates PEFT adapter experts with diverse\nstructures, leveraging their complementary representational capabilities to\nfoster expert specialization, thereby enhancing the effective transfer of\npre-trained knowledge to downstream tasks. MoA supports two variants:\n\\textbf{(i)} \\textit{Soft MoA} achieves fine-grained integration by performing\na weighted fusion of all expert outputs; \\textbf{(ii)} \\textit{Sparse MoA}\nactivates adapter experts sparsely based on their contribution, achieving this\nwith negligible performance degradation. Experimental results demonstrate that\nheterogeneous MoA outperforms homogeneous MoE-LoRA methods in both performance\nand parameter efficiency. Our project is available at\nhttps://github.com/DCDmllm/MoA.\n", "link": "http://arxiv.org/abs/2506.05928v1", "date": "2025-06-06", "relevancy": 2.078, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5581}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4955}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoA%3A%20Heterogeneous%20Mixture%20of%20Adapters%20for%20Parameter-Efficient%0A%20%20Fine-Tuning%20of%20Large%20Language%20Models&body=Title%3A%20MoA%3A%20Heterogeneous%20Mixture%20of%20Adapters%20for%20Parameter-Efficient%0A%20%20Fine-Tuning%20of%20Large%20Language%20Models%0AAuthor%3A%20Jie%20Cao%20and%20Tianwei%20Lin%20and%20Hongyang%20He%20and%20Rolan%20Yan%20and%20Wenqiao%20Zhang%20and%20Juncheng%20Li%20and%20Dongping%20Zhang%20and%20Siliang%20Tang%20and%20Yueting%20Zhuang%0AAbstract%3A%20%20%20Recent%20studies%20integrate%20Low-Rank%20Adaptation%20%28LoRA%29%20and%20Mixture-of-Experts%0A%28MoE%29%20to%20further%20enhance%20the%20performance%20of%20parameter-efficient%20fine-tuning%0A%28PEFT%29%20methods%20in%20Large%20Language%20Model%20%28LLM%29%20applications.%20Existing%20methods%0Aemploy%20%5Cemph%7Bhomogeneous%7D%20MoE-LoRA%20architectures%20composed%20of%20LoRA%20experts%20with%0Aeither%20similar%20or%20identical%20structures%20and%20capacities.%20However%2C%20these%0Aapproaches%20often%20suffer%20from%20representation%20collapse%20and%20expert%20load%20imbalance%2C%0Awhich%20negatively%20impact%20the%20potential%20of%20LLMs.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20%5Cemph%7Bheterogeneous%7D%20%5Ctextbf%7BMixture-of-Adapters%20%28MoA%29%7D%20approach.%0AThis%20method%20dynamically%20integrates%20PEFT%20adapter%20experts%20with%20diverse%0Astructures%2C%20leveraging%20their%20complementary%20representational%20capabilities%20to%0Afoster%20expert%20specialization%2C%20thereby%20enhancing%20the%20effective%20transfer%20of%0Apre-trained%20knowledge%20to%20downstream%20tasks.%20MoA%20supports%20two%20variants%3A%0A%5Ctextbf%7B%28i%29%7D%20%5Ctextit%7BSoft%20MoA%7D%20achieves%20fine-grained%20integration%20by%20performing%0Aa%20weighted%20fusion%20of%20all%20expert%20outputs%3B%20%5Ctextbf%7B%28ii%29%7D%20%5Ctextit%7BSparse%20MoA%7D%0Aactivates%20adapter%20experts%20sparsely%20based%20on%20their%20contribution%2C%20achieving%20this%0Awith%20negligible%20performance%20degradation.%20Experimental%20results%20demonstrate%20that%0Aheterogeneous%20MoA%20outperforms%20homogeneous%20MoE-LoRA%20methods%20in%20both%20performance%0Aand%20parameter%20efficiency.%20Our%20project%20is%20available%20at%0Ahttps%3A//github.com/DCDmllm/MoA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05928v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoA%253A%2520Heterogeneous%2520Mixture%2520of%2520Adapters%2520for%2520Parameter-Efficient%250A%2520%2520Fine-Tuning%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DJie%2520Cao%2520and%2520Tianwei%2520Lin%2520and%2520Hongyang%2520He%2520and%2520Rolan%2520Yan%2520and%2520Wenqiao%2520Zhang%2520and%2520Juncheng%2520Li%2520and%2520Dongping%2520Zhang%2520and%2520Siliang%2520Tang%2520and%2520Yueting%2520Zhuang%26entry.1292438233%3D%2520%2520Recent%2520studies%2520integrate%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520and%2520Mixture-of-Experts%250A%2528MoE%2529%2520to%2520further%2520enhance%2520the%2520performance%2520of%2520parameter-efficient%2520fine-tuning%250A%2528PEFT%2529%2520methods%2520in%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520applications.%2520Existing%2520methods%250Aemploy%2520%255Cemph%257Bhomogeneous%257D%2520MoE-LoRA%2520architectures%2520composed%2520of%2520LoRA%2520experts%2520with%250Aeither%2520similar%2520or%2520identical%2520structures%2520and%2520capacities.%2520However%252C%2520these%250Aapproaches%2520often%2520suffer%2520from%2520representation%2520collapse%2520and%2520expert%2520load%2520imbalance%252C%250Awhich%2520negatively%2520impact%2520the%2520potential%2520of%2520LLMs.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520a%2520%255Cemph%257Bheterogeneous%257D%2520%255Ctextbf%257BMixture-of-Adapters%2520%2528MoA%2529%257D%2520approach.%250AThis%2520method%2520dynamically%2520integrates%2520PEFT%2520adapter%2520experts%2520with%2520diverse%250Astructures%252C%2520leveraging%2520their%2520complementary%2520representational%2520capabilities%2520to%250Afoster%2520expert%2520specialization%252C%2520thereby%2520enhancing%2520the%2520effective%2520transfer%2520of%250Apre-trained%2520knowledge%2520to%2520downstream%2520tasks.%2520MoA%2520supports%2520two%2520variants%253A%250A%255Ctextbf%257B%2528i%2529%257D%2520%255Ctextit%257BSoft%2520MoA%257D%2520achieves%2520fine-grained%2520integration%2520by%2520performing%250Aa%2520weighted%2520fusion%2520of%2520all%2520expert%2520outputs%253B%2520%255Ctextbf%257B%2528ii%2529%257D%2520%255Ctextit%257BSparse%2520MoA%257D%250Aactivates%2520adapter%2520experts%2520sparsely%2520based%2520on%2520their%2520contribution%252C%2520achieving%2520this%250Awith%2520negligible%2520performance%2520degradation.%2520Experimental%2520results%2520demonstrate%2520that%250Aheterogeneous%2520MoA%2520outperforms%2520homogeneous%2520MoE-LoRA%2520methods%2520in%2520both%2520performance%250Aand%2520parameter%2520efficiency.%2520Our%2520project%2520is%2520available%2520at%250Ahttps%253A//github.com/DCDmllm/MoA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05928v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoA%3A%20Heterogeneous%20Mixture%20of%20Adapters%20for%20Parameter-Efficient%0A%20%20Fine-Tuning%20of%20Large%20Language%20Models&entry.906535625=Jie%20Cao%20and%20Tianwei%20Lin%20and%20Hongyang%20He%20and%20Rolan%20Yan%20and%20Wenqiao%20Zhang%20and%20Juncheng%20Li%20and%20Dongping%20Zhang%20and%20Siliang%20Tang%20and%20Yueting%20Zhuang&entry.1292438233=%20%20Recent%20studies%20integrate%20Low-Rank%20Adaptation%20%28LoRA%29%20and%20Mixture-of-Experts%0A%28MoE%29%20to%20further%20enhance%20the%20performance%20of%20parameter-efficient%20fine-tuning%0A%28PEFT%29%20methods%20in%20Large%20Language%20Model%20%28LLM%29%20applications.%20Existing%20methods%0Aemploy%20%5Cemph%7Bhomogeneous%7D%20MoE-LoRA%20architectures%20composed%20of%20LoRA%20experts%20with%0Aeither%20similar%20or%20identical%20structures%20and%20capacities.%20However%2C%20these%0Aapproaches%20often%20suffer%20from%20representation%20collapse%20and%20expert%20load%20imbalance%2C%0Awhich%20negatively%20impact%20the%20potential%20of%20LLMs.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20%5Cemph%7Bheterogeneous%7D%20%5Ctextbf%7BMixture-of-Adapters%20%28MoA%29%7D%20approach.%0AThis%20method%20dynamically%20integrates%20PEFT%20adapter%20experts%20with%20diverse%0Astructures%2C%20leveraging%20their%20complementary%20representational%20capabilities%20to%0Afoster%20expert%20specialization%2C%20thereby%20enhancing%20the%20effective%20transfer%20of%0Apre-trained%20knowledge%20to%20downstream%20tasks.%20MoA%20supports%20two%20variants%3A%0A%5Ctextbf%7B%28i%29%7D%20%5Ctextit%7BSoft%20MoA%7D%20achieves%20fine-grained%20integration%20by%20performing%0Aa%20weighted%20fusion%20of%20all%20expert%20outputs%3B%20%5Ctextbf%7B%28ii%29%7D%20%5Ctextit%7BSparse%20MoA%7D%0Aactivates%20adapter%20experts%20sparsely%20based%20on%20their%20contribution%2C%20achieving%20this%0Awith%20negligible%20performance%20degradation.%20Experimental%20results%20demonstrate%20that%0Aheterogeneous%20MoA%20outperforms%20homogeneous%20MoE-LoRA%20methods%20in%20both%20performance%0Aand%20parameter%20efficiency.%20Our%20project%20is%20available%20at%0Ahttps%3A//github.com/DCDmllm/MoA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05928v1&entry.124074799=Read"},
{"title": "End-to-End Framework for Robot Lawnmower Coverage Path Planning using\n  Cellular Decomposition", "author": "Nikunj Shah and Utsav Dey and Kenji Nishimiya", "abstract": "  Efficient Coverage Path Planning (CPP) is necessary for autonomous robotic\nlawnmowers to effectively navigate and maintain lawns with diverse and\nirregular shapes. This paper introduces a comprehensive end-to-end pipeline for\nCPP, designed to convert user-defined boundaries on an aerial map into\noptimized coverage paths seamlessly. The pipeline includes user input\nextraction, coordinate transformation, area decomposition and path generation\nusing our novel AdaptiveDecompositionCPP algorithm, preview and customization\nthrough an interactive coverage path visualizer, and conversion to actionable\nGPS waypoints. The AdaptiveDecompositionCPP algorithm combines cellular\ndecomposition with an adaptive merging strategy to reduce non-mowing travel\nthereby enhancing operational efficiency. Experimental evaluations,\nencompassing both simulations and real-world lawnmower tests, demonstrate the\neffectiveness of the framework in coverage completeness and mowing efficiency.\n", "link": "http://arxiv.org/abs/2506.06028v1", "date": "2025-06-06", "relevancy": 2.0648, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5218}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5187}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20End-to-End%20Framework%20for%20Robot%20Lawnmower%20Coverage%20Path%20Planning%20using%0A%20%20Cellular%20Decomposition&body=Title%3A%20End-to-End%20Framework%20for%20Robot%20Lawnmower%20Coverage%20Path%20Planning%20using%0A%20%20Cellular%20Decomposition%0AAuthor%3A%20Nikunj%20Shah%20and%20Utsav%20Dey%20and%20Kenji%20Nishimiya%0AAbstract%3A%20%20%20Efficient%20Coverage%20Path%20Planning%20%28CPP%29%20is%20necessary%20for%20autonomous%20robotic%0Alawnmowers%20to%20effectively%20navigate%20and%20maintain%20lawns%20with%20diverse%20and%0Airregular%20shapes.%20This%20paper%20introduces%20a%20comprehensive%20end-to-end%20pipeline%20for%0ACPP%2C%20designed%20to%20convert%20user-defined%20boundaries%20on%20an%20aerial%20map%20into%0Aoptimized%20coverage%20paths%20seamlessly.%20The%20pipeline%20includes%20user%20input%0Aextraction%2C%20coordinate%20transformation%2C%20area%20decomposition%20and%20path%20generation%0Ausing%20our%20novel%20AdaptiveDecompositionCPP%20algorithm%2C%20preview%20and%20customization%0Athrough%20an%20interactive%20coverage%20path%20visualizer%2C%20and%20conversion%20to%20actionable%0AGPS%20waypoints.%20The%20AdaptiveDecompositionCPP%20algorithm%20combines%20cellular%0Adecomposition%20with%20an%20adaptive%20merging%20strategy%20to%20reduce%20non-mowing%20travel%0Athereby%20enhancing%20operational%20efficiency.%20Experimental%20evaluations%2C%0Aencompassing%20both%20simulations%20and%20real-world%20lawnmower%20tests%2C%20demonstrate%20the%0Aeffectiveness%20of%20the%20framework%20in%20coverage%20completeness%20and%20mowing%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06028v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnd-to-End%2520Framework%2520for%2520Robot%2520Lawnmower%2520Coverage%2520Path%2520Planning%2520using%250A%2520%2520Cellular%2520Decomposition%26entry.906535625%3DNikunj%2520Shah%2520and%2520Utsav%2520Dey%2520and%2520Kenji%2520Nishimiya%26entry.1292438233%3D%2520%2520Efficient%2520Coverage%2520Path%2520Planning%2520%2528CPP%2529%2520is%2520necessary%2520for%2520autonomous%2520robotic%250Alawnmowers%2520to%2520effectively%2520navigate%2520and%2520maintain%2520lawns%2520with%2520diverse%2520and%250Airregular%2520shapes.%2520This%2520paper%2520introduces%2520a%2520comprehensive%2520end-to-end%2520pipeline%2520for%250ACPP%252C%2520designed%2520to%2520convert%2520user-defined%2520boundaries%2520on%2520an%2520aerial%2520map%2520into%250Aoptimized%2520coverage%2520paths%2520seamlessly.%2520The%2520pipeline%2520includes%2520user%2520input%250Aextraction%252C%2520coordinate%2520transformation%252C%2520area%2520decomposition%2520and%2520path%2520generation%250Ausing%2520our%2520novel%2520AdaptiveDecompositionCPP%2520algorithm%252C%2520preview%2520and%2520customization%250Athrough%2520an%2520interactive%2520coverage%2520path%2520visualizer%252C%2520and%2520conversion%2520to%2520actionable%250AGPS%2520waypoints.%2520The%2520AdaptiveDecompositionCPP%2520algorithm%2520combines%2520cellular%250Adecomposition%2520with%2520an%2520adaptive%2520merging%2520strategy%2520to%2520reduce%2520non-mowing%2520travel%250Athereby%2520enhancing%2520operational%2520efficiency.%2520Experimental%2520evaluations%252C%250Aencompassing%2520both%2520simulations%2520and%2520real-world%2520lawnmower%2520tests%252C%2520demonstrate%2520the%250Aeffectiveness%2520of%2520the%2520framework%2520in%2520coverage%2520completeness%2520and%2520mowing%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06028v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=End-to-End%20Framework%20for%20Robot%20Lawnmower%20Coverage%20Path%20Planning%20using%0A%20%20Cellular%20Decomposition&entry.906535625=Nikunj%20Shah%20and%20Utsav%20Dey%20and%20Kenji%20Nishimiya&entry.1292438233=%20%20Efficient%20Coverage%20Path%20Planning%20%28CPP%29%20is%20necessary%20for%20autonomous%20robotic%0Alawnmowers%20to%20effectively%20navigate%20and%20maintain%20lawns%20with%20diverse%20and%0Airregular%20shapes.%20This%20paper%20introduces%20a%20comprehensive%20end-to-end%20pipeline%20for%0ACPP%2C%20designed%20to%20convert%20user-defined%20boundaries%20on%20an%20aerial%20map%20into%0Aoptimized%20coverage%20paths%20seamlessly.%20The%20pipeline%20includes%20user%20input%0Aextraction%2C%20coordinate%20transformation%2C%20area%20decomposition%20and%20path%20generation%0Ausing%20our%20novel%20AdaptiveDecompositionCPP%20algorithm%2C%20preview%20and%20customization%0Athrough%20an%20interactive%20coverage%20path%20visualizer%2C%20and%20conversion%20to%20actionable%0AGPS%20waypoints.%20The%20AdaptiveDecompositionCPP%20algorithm%20combines%20cellular%0Adecomposition%20with%20an%20adaptive%20merging%20strategy%20to%20reduce%20non-mowing%20travel%0Athereby%20enhancing%20operational%20efficiency.%20Experimental%20evaluations%2C%0Aencompassing%20both%20simulations%20and%20real-world%20lawnmower%20tests%2C%20demonstrate%20the%0Aeffectiveness%20of%20the%20framework%20in%20coverage%20completeness%20and%20mowing%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06028v1&entry.124074799=Read"},
{"title": "Pseudo-labelling meets Label Smoothing for Noisy Partial Label Learning", "author": "Darshana Saravanan and Naresh Manwani and Vineet Gandhi", "abstract": "  We motivate weakly supervised learning as an effective learning paradigm for\nproblems where curating perfectly annotated datasets is expensive and may\nrequire domain expertise such as fine-grained classification. We focus on\nPartial Label Learning (PLL), a weakly-supervised learning paradigm where each\ntraining instance is paired with a set of candidate labels (partial label), one\nof which is the true label. Noisy PLL (NPLL) relaxes this constraint by\nallowing some partial labels to not contain the true label, enhancing the\npracticality of the problem. Our work centres on NPLL and presents a framework\nthat initially assigns pseudo-labels to images by exploiting the noisy partial\nlabels through a weighted nearest neighbour algorithm. These pseudo-label and\nimage pairs are then used to train a deep neural network classifier with label\nsmoothing. The classifier's features and predictions are subsequently employed\nto refine and enhance the accuracy of pseudo-labels. We perform thorough\nexperiments on seven datasets and compare against nine NPLL and PLL methods. We\nachieve state-of-the-art results in all studied settings from the prior\nliterature, obtaining substantial gains in the simulated fine-grained\nbenchmarks. Further, we show the promising generalisation capability of our\nframework in realistic, fine-grained, crowd-sourced datasets.\n", "link": "http://arxiv.org/abs/2402.04835v3", "date": "2025-06-06", "relevancy": 2.0627, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.538}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5352}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pseudo-labelling%20meets%20Label%20Smoothing%20for%20Noisy%20Partial%20Label%20Learning&body=Title%3A%20Pseudo-labelling%20meets%20Label%20Smoothing%20for%20Noisy%20Partial%20Label%20Learning%0AAuthor%3A%20Darshana%20Saravanan%20and%20Naresh%20Manwani%20and%20Vineet%20Gandhi%0AAbstract%3A%20%20%20We%20motivate%20weakly%20supervised%20learning%20as%20an%20effective%20learning%20paradigm%20for%0Aproblems%20where%20curating%20perfectly%20annotated%20datasets%20is%20expensive%20and%20may%0Arequire%20domain%20expertise%20such%20as%20fine-grained%20classification.%20We%20focus%20on%0APartial%20Label%20Learning%20%28PLL%29%2C%20a%20weakly-supervised%20learning%20paradigm%20where%20each%0Atraining%20instance%20is%20paired%20with%20a%20set%20of%20candidate%20labels%20%28partial%20label%29%2C%20one%0Aof%20which%20is%20the%20true%20label.%20Noisy%20PLL%20%28NPLL%29%20relaxes%20this%20constraint%20by%0Aallowing%20some%20partial%20labels%20to%20not%20contain%20the%20true%20label%2C%20enhancing%20the%0Apracticality%20of%20the%20problem.%20Our%20work%20centres%20on%20NPLL%20and%20presents%20a%20framework%0Athat%20initially%20assigns%20pseudo-labels%20to%20images%20by%20exploiting%20the%20noisy%20partial%0Alabels%20through%20a%20weighted%20nearest%20neighbour%20algorithm.%20These%20pseudo-label%20and%0Aimage%20pairs%20are%20then%20used%20to%20train%20a%20deep%20neural%20network%20classifier%20with%20label%0Asmoothing.%20The%20classifier%27s%20features%20and%20predictions%20are%20subsequently%20employed%0Ato%20refine%20and%20enhance%20the%20accuracy%20of%20pseudo-labels.%20We%20perform%20thorough%0Aexperiments%20on%20seven%20datasets%20and%20compare%20against%20nine%20NPLL%20and%20PLL%20methods.%20We%0Aachieve%20state-of-the-art%20results%20in%20all%20studied%20settings%20from%20the%20prior%0Aliterature%2C%20obtaining%20substantial%20gains%20in%20the%20simulated%20fine-grained%0Abenchmarks.%20Further%2C%20we%20show%20the%20promising%20generalisation%20capability%20of%20our%0Aframework%20in%20realistic%2C%20fine-grained%2C%20crowd-sourced%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.04835v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPseudo-labelling%2520meets%2520Label%2520Smoothing%2520for%2520Noisy%2520Partial%2520Label%2520Learning%26entry.906535625%3DDarshana%2520Saravanan%2520and%2520Naresh%2520Manwani%2520and%2520Vineet%2520Gandhi%26entry.1292438233%3D%2520%2520We%2520motivate%2520weakly%2520supervised%2520learning%2520as%2520an%2520effective%2520learning%2520paradigm%2520for%250Aproblems%2520where%2520curating%2520perfectly%2520annotated%2520datasets%2520is%2520expensive%2520and%2520may%250Arequire%2520domain%2520expertise%2520such%2520as%2520fine-grained%2520classification.%2520We%2520focus%2520on%250APartial%2520Label%2520Learning%2520%2528PLL%2529%252C%2520a%2520weakly-supervised%2520learning%2520paradigm%2520where%2520each%250Atraining%2520instance%2520is%2520paired%2520with%2520a%2520set%2520of%2520candidate%2520labels%2520%2528partial%2520label%2529%252C%2520one%250Aof%2520which%2520is%2520the%2520true%2520label.%2520Noisy%2520PLL%2520%2528NPLL%2529%2520relaxes%2520this%2520constraint%2520by%250Aallowing%2520some%2520partial%2520labels%2520to%2520not%2520contain%2520the%2520true%2520label%252C%2520enhancing%2520the%250Apracticality%2520of%2520the%2520problem.%2520Our%2520work%2520centres%2520on%2520NPLL%2520and%2520presents%2520a%2520framework%250Athat%2520initially%2520assigns%2520pseudo-labels%2520to%2520images%2520by%2520exploiting%2520the%2520noisy%2520partial%250Alabels%2520through%2520a%2520weighted%2520nearest%2520neighbour%2520algorithm.%2520These%2520pseudo-label%2520and%250Aimage%2520pairs%2520are%2520then%2520used%2520to%2520train%2520a%2520deep%2520neural%2520network%2520classifier%2520with%2520label%250Asmoothing.%2520The%2520classifier%2527s%2520features%2520and%2520predictions%2520are%2520subsequently%2520employed%250Ato%2520refine%2520and%2520enhance%2520the%2520accuracy%2520of%2520pseudo-labels.%2520We%2520perform%2520thorough%250Aexperiments%2520on%2520seven%2520datasets%2520and%2520compare%2520against%2520nine%2520NPLL%2520and%2520PLL%2520methods.%2520We%250Aachieve%2520state-of-the-art%2520results%2520in%2520all%2520studied%2520settings%2520from%2520the%2520prior%250Aliterature%252C%2520obtaining%2520substantial%2520gains%2520in%2520the%2520simulated%2520fine-grained%250Abenchmarks.%2520Further%252C%2520we%2520show%2520the%2520promising%2520generalisation%2520capability%2520of%2520our%250Aframework%2520in%2520realistic%252C%2520fine-grained%252C%2520crowd-sourced%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.04835v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pseudo-labelling%20meets%20Label%20Smoothing%20for%20Noisy%20Partial%20Label%20Learning&entry.906535625=Darshana%20Saravanan%20and%20Naresh%20Manwani%20and%20Vineet%20Gandhi&entry.1292438233=%20%20We%20motivate%20weakly%20supervised%20learning%20as%20an%20effective%20learning%20paradigm%20for%0Aproblems%20where%20curating%20perfectly%20annotated%20datasets%20is%20expensive%20and%20may%0Arequire%20domain%20expertise%20such%20as%20fine-grained%20classification.%20We%20focus%20on%0APartial%20Label%20Learning%20%28PLL%29%2C%20a%20weakly-supervised%20learning%20paradigm%20where%20each%0Atraining%20instance%20is%20paired%20with%20a%20set%20of%20candidate%20labels%20%28partial%20label%29%2C%20one%0Aof%20which%20is%20the%20true%20label.%20Noisy%20PLL%20%28NPLL%29%20relaxes%20this%20constraint%20by%0Aallowing%20some%20partial%20labels%20to%20not%20contain%20the%20true%20label%2C%20enhancing%20the%0Apracticality%20of%20the%20problem.%20Our%20work%20centres%20on%20NPLL%20and%20presents%20a%20framework%0Athat%20initially%20assigns%20pseudo-labels%20to%20images%20by%20exploiting%20the%20noisy%20partial%0Alabels%20through%20a%20weighted%20nearest%20neighbour%20algorithm.%20These%20pseudo-label%20and%0Aimage%20pairs%20are%20then%20used%20to%20train%20a%20deep%20neural%20network%20classifier%20with%20label%0Asmoothing.%20The%20classifier%27s%20features%20and%20predictions%20are%20subsequently%20employed%0Ato%20refine%20and%20enhance%20the%20accuracy%20of%20pseudo-labels.%20We%20perform%20thorough%0Aexperiments%20on%20seven%20datasets%20and%20compare%20against%20nine%20NPLL%20and%20PLL%20methods.%20We%0Aachieve%20state-of-the-art%20results%20in%20all%20studied%20settings%20from%20the%20prior%0Aliterature%2C%20obtaining%20substantial%20gains%20in%20the%20simulated%20fine-grained%0Abenchmarks.%20Further%2C%20we%20show%20the%20promising%20generalisation%20capability%20of%20our%0Aframework%20in%20realistic%2C%20fine-grained%2C%20crowd-sourced%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.04835v3&entry.124074799=Read"},
{"title": "LinGuinE: Longitudinal Guidance Estimation for Volumetric Lung Tumour\n  Segmentation", "author": "Nadine Garibli and Mayank Patwari and Bence Csiba and Yi Wei and Kostas Sidiropoulos", "abstract": "  Segmentation of lung gross tumour volumes is an important first step in\nradiotherapy and surgical intervention, and is starting to play a role in\nassessing chemotherapy response. Response to a drug is measured by tracking the\ntumour volumes over a series of CT scans over a time period i.e. a longitudinal\nstudy. However, there currently exist few solutions for automated or\nsemi-automated longitudinal tumour segmentation. This paper introduces\nLinGuinE, an automated method to segment a longitudinal series of lung tumours.\nA radiologist must provide an initial input, indicating the location of the\ntumour in a CT scan at an arbitrary time point. LinGuinE samples points inside\nthis tumour and propagates them to another time point using rigid registration.\nA click validity classifier selects points which still fall within the tumour;\nthese are used to automatically create a segmentation in the new time point. We\ntest LinGuinE on a dataset acquired from a phase 3 clinical trial for lung\ntumours and the publicly available 4-D lung CBCT dataset. We find that LinGuinE\nimproves the Dice on both test sets by over 20% (p< 0.05) across 63\nlongitudinal studies. We show that any time point can be used as a starting\npoint, conduct ablation experiments, and find that our LinGuinE setup yields\nthe best results on both test datasets.\n", "link": "http://arxiv.org/abs/2506.06092v1", "date": "2025-06-06", "relevancy": 1.8315, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4815}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4542}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LinGuinE%3A%20Longitudinal%20Guidance%20Estimation%20for%20Volumetric%20Lung%20Tumour%0A%20%20Segmentation&body=Title%3A%20LinGuinE%3A%20Longitudinal%20Guidance%20Estimation%20for%20Volumetric%20Lung%20Tumour%0A%20%20Segmentation%0AAuthor%3A%20Nadine%20Garibli%20and%20Mayank%20Patwari%20and%20Bence%20Csiba%20and%20Yi%20Wei%20and%20Kostas%20Sidiropoulos%0AAbstract%3A%20%20%20Segmentation%20of%20lung%20gross%20tumour%20volumes%20is%20an%20important%20first%20step%20in%0Aradiotherapy%20and%20surgical%20intervention%2C%20and%20is%20starting%20to%20play%20a%20role%20in%0Aassessing%20chemotherapy%20response.%20Response%20to%20a%20drug%20is%20measured%20by%20tracking%20the%0Atumour%20volumes%20over%20a%20series%20of%20CT%20scans%20over%20a%20time%20period%20i.e.%20a%20longitudinal%0Astudy.%20However%2C%20there%20currently%20exist%20few%20solutions%20for%20automated%20or%0Asemi-automated%20longitudinal%20tumour%20segmentation.%20This%20paper%20introduces%0ALinGuinE%2C%20an%20automated%20method%20to%20segment%20a%20longitudinal%20series%20of%20lung%20tumours.%0AA%20radiologist%20must%20provide%20an%20initial%20input%2C%20indicating%20the%20location%20of%20the%0Atumour%20in%20a%20CT%20scan%20at%20an%20arbitrary%20time%20point.%20LinGuinE%20samples%20points%20inside%0Athis%20tumour%20and%20propagates%20them%20to%20another%20time%20point%20using%20rigid%20registration.%0AA%20click%20validity%20classifier%20selects%20points%20which%20still%20fall%20within%20the%20tumour%3B%0Athese%20are%20used%20to%20automatically%20create%20a%20segmentation%20in%20the%20new%20time%20point.%20We%0Atest%20LinGuinE%20on%20a%20dataset%20acquired%20from%20a%20phase%203%20clinical%20trial%20for%20lung%0Atumours%20and%20the%20publicly%20available%204-D%20lung%20CBCT%20dataset.%20We%20find%20that%20LinGuinE%0Aimproves%20the%20Dice%20on%20both%20test%20sets%20by%20over%2020%25%20%28p%3C%200.05%29%20across%2063%0Alongitudinal%20studies.%20We%20show%20that%20any%20time%20point%20can%20be%20used%20as%20a%20starting%0Apoint%2C%20conduct%20ablation%20experiments%2C%20and%20find%20that%20our%20LinGuinE%20setup%20yields%0Athe%20best%20results%20on%20both%20test%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06092v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLinGuinE%253A%2520Longitudinal%2520Guidance%2520Estimation%2520for%2520Volumetric%2520Lung%2520Tumour%250A%2520%2520Segmentation%26entry.906535625%3DNadine%2520Garibli%2520and%2520Mayank%2520Patwari%2520and%2520Bence%2520Csiba%2520and%2520Yi%2520Wei%2520and%2520Kostas%2520Sidiropoulos%26entry.1292438233%3D%2520%2520Segmentation%2520of%2520lung%2520gross%2520tumour%2520volumes%2520is%2520an%2520important%2520first%2520step%2520in%250Aradiotherapy%2520and%2520surgical%2520intervention%252C%2520and%2520is%2520starting%2520to%2520play%2520a%2520role%2520in%250Aassessing%2520chemotherapy%2520response.%2520Response%2520to%2520a%2520drug%2520is%2520measured%2520by%2520tracking%2520the%250Atumour%2520volumes%2520over%2520a%2520series%2520of%2520CT%2520scans%2520over%2520a%2520time%2520period%2520i.e.%2520a%2520longitudinal%250Astudy.%2520However%252C%2520there%2520currently%2520exist%2520few%2520solutions%2520for%2520automated%2520or%250Asemi-automated%2520longitudinal%2520tumour%2520segmentation.%2520This%2520paper%2520introduces%250ALinGuinE%252C%2520an%2520automated%2520method%2520to%2520segment%2520a%2520longitudinal%2520series%2520of%2520lung%2520tumours.%250AA%2520radiologist%2520must%2520provide%2520an%2520initial%2520input%252C%2520indicating%2520the%2520location%2520of%2520the%250Atumour%2520in%2520a%2520CT%2520scan%2520at%2520an%2520arbitrary%2520time%2520point.%2520LinGuinE%2520samples%2520points%2520inside%250Athis%2520tumour%2520and%2520propagates%2520them%2520to%2520another%2520time%2520point%2520using%2520rigid%2520registration.%250AA%2520click%2520validity%2520classifier%2520selects%2520points%2520which%2520still%2520fall%2520within%2520the%2520tumour%253B%250Athese%2520are%2520used%2520to%2520automatically%2520create%2520a%2520segmentation%2520in%2520the%2520new%2520time%2520point.%2520We%250Atest%2520LinGuinE%2520on%2520a%2520dataset%2520acquired%2520from%2520a%2520phase%25203%2520clinical%2520trial%2520for%2520lung%250Atumours%2520and%2520the%2520publicly%2520available%25204-D%2520lung%2520CBCT%2520dataset.%2520We%2520find%2520that%2520LinGuinE%250Aimproves%2520the%2520Dice%2520on%2520both%2520test%2520sets%2520by%2520over%252020%2525%2520%2528p%253C%25200.05%2529%2520across%252063%250Alongitudinal%2520studies.%2520We%2520show%2520that%2520any%2520time%2520point%2520can%2520be%2520used%2520as%2520a%2520starting%250Apoint%252C%2520conduct%2520ablation%2520experiments%252C%2520and%2520find%2520that%2520our%2520LinGuinE%2520setup%2520yields%250Athe%2520best%2520results%2520on%2520both%2520test%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06092v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LinGuinE%3A%20Longitudinal%20Guidance%20Estimation%20for%20Volumetric%20Lung%20Tumour%0A%20%20Segmentation&entry.906535625=Nadine%20Garibli%20and%20Mayank%20Patwari%20and%20Bence%20Csiba%20and%20Yi%20Wei%20and%20Kostas%20Sidiropoulos&entry.1292438233=%20%20Segmentation%20of%20lung%20gross%20tumour%20volumes%20is%20an%20important%20first%20step%20in%0Aradiotherapy%20and%20surgical%20intervention%2C%20and%20is%20starting%20to%20play%20a%20role%20in%0Aassessing%20chemotherapy%20response.%20Response%20to%20a%20drug%20is%20measured%20by%20tracking%20the%0Atumour%20volumes%20over%20a%20series%20of%20CT%20scans%20over%20a%20time%20period%20i.e.%20a%20longitudinal%0Astudy.%20However%2C%20there%20currently%20exist%20few%20solutions%20for%20automated%20or%0Asemi-automated%20longitudinal%20tumour%20segmentation.%20This%20paper%20introduces%0ALinGuinE%2C%20an%20automated%20method%20to%20segment%20a%20longitudinal%20series%20of%20lung%20tumours.%0AA%20radiologist%20must%20provide%20an%20initial%20input%2C%20indicating%20the%20location%20of%20the%0Atumour%20in%20a%20CT%20scan%20at%20an%20arbitrary%20time%20point.%20LinGuinE%20samples%20points%20inside%0Athis%20tumour%20and%20propagates%20them%20to%20another%20time%20point%20using%20rigid%20registration.%0AA%20click%20validity%20classifier%20selects%20points%20which%20still%20fall%20within%20the%20tumour%3B%0Athese%20are%20used%20to%20automatically%20create%20a%20segmentation%20in%20the%20new%20time%20point.%20We%0Atest%20LinGuinE%20on%20a%20dataset%20acquired%20from%20a%20phase%203%20clinical%20trial%20for%20lung%0Atumours%20and%20the%20publicly%20available%204-D%20lung%20CBCT%20dataset.%20We%20find%20that%20LinGuinE%0Aimproves%20the%20Dice%20on%20both%20test%20sets%20by%20over%2020%25%20%28p%3C%200.05%29%20across%2063%0Alongitudinal%20studies.%20We%20show%20that%20any%20time%20point%20can%20be%20used%20as%20a%20starting%0Apoint%2C%20conduct%20ablation%20experiments%2C%20and%20find%20that%20our%20LinGuinE%20setup%20yields%0Athe%20best%20results%20on%20both%20test%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06092v1&entry.124074799=Read"},
{"title": "Teaming in the AI Era: AI-Augmented Frameworks for Forming, Simulating,\n  and Optimizing Human Teams", "author": "Mohammed Almutairi", "abstract": "  Effective teamwork is essential across diverse domains. During the team\nformation stage, a key challenge is forming teams that effectively balance user\npreferences with task objectives to enhance overall team satisfaction. In the\nteam performing stage, maintaining cohesion and engagement is critical for\nsustaining high team performance. However, existing computational tools and\nalgorithms for team optimization often rely on static data inputs, narrow\nalgorithmic objectives, or solutions tailored for specific contexts, failing to\naccount for the dynamic interplay of team members personalities, evolving\ngoals, and changing individual preferences. Therefore, teams may encounter\nmember dissatisfaction, as purely algorithmic assignments can reduce members\ncommitment to team goals or experience suboptimal engagement due to the absence\nof timely, personalized guidance to help members adjust their behaviors and\ninteractions as team dynamics evolve. Ultimately, these challenges can lead to\nreduced overall team performance. My Ph.D. dissertation aims to develop\nAI-augmented team optimization frameworks and practical systems that enhance\nteam satisfaction, engagement, and performance. First, I propose a team\nformation framework that leverages a multi-armed bandit algorithm to\niteratively refine team composition based on user preferences, ensuring\nalignment between individual needs and collective team goals to enhance team\nsatisfaction. Second, I introduce tAIfa (Team AI Feedback Assistant), an\nAI-powered system that utilizes large language models (LLMs) to deliver\nimmediate, personalized feedback to both teams and individual members,\nenhancing cohesion and engagement. Finally, I present PuppeteerLLM, an\nLLM-based simulation framework that simulates multi-agent teams to model\ncomplex team dynamics within realistic environments, incorporating task-driven\ncollaboration and long-term coordination.\n", "link": "http://arxiv.org/abs/2506.05265v2", "date": "2025-06-06", "relevancy": 1.4117, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5054}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.461}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Teaming%20in%20the%20AI%20Era%3A%20AI-Augmented%20Frameworks%20for%20Forming%2C%20Simulating%2C%0A%20%20and%20Optimizing%20Human%20Teams&body=Title%3A%20Teaming%20in%20the%20AI%20Era%3A%20AI-Augmented%20Frameworks%20for%20Forming%2C%20Simulating%2C%0A%20%20and%20Optimizing%20Human%20Teams%0AAuthor%3A%20Mohammed%20Almutairi%0AAbstract%3A%20%20%20Effective%20teamwork%20is%20essential%20across%20diverse%20domains.%20During%20the%20team%0Aformation%20stage%2C%20a%20key%20challenge%20is%20forming%20teams%20that%20effectively%20balance%20user%0Apreferences%20with%20task%20objectives%20to%20enhance%20overall%20team%20satisfaction.%20In%20the%0Ateam%20performing%20stage%2C%20maintaining%20cohesion%20and%20engagement%20is%20critical%20for%0Asustaining%20high%20team%20performance.%20However%2C%20existing%20computational%20tools%20and%0Aalgorithms%20for%20team%20optimization%20often%20rely%20on%20static%20data%20inputs%2C%20narrow%0Aalgorithmic%20objectives%2C%20or%20solutions%20tailored%20for%20specific%20contexts%2C%20failing%20to%0Aaccount%20for%20the%20dynamic%20interplay%20of%20team%20members%20personalities%2C%20evolving%0Agoals%2C%20and%20changing%20individual%20preferences.%20Therefore%2C%20teams%20may%20encounter%0Amember%20dissatisfaction%2C%20as%20purely%20algorithmic%20assignments%20can%20reduce%20members%0Acommitment%20to%20team%20goals%20or%20experience%20suboptimal%20engagement%20due%20to%20the%20absence%0Aof%20timely%2C%20personalized%20guidance%20to%20help%20members%20adjust%20their%20behaviors%20and%0Ainteractions%20as%20team%20dynamics%20evolve.%20Ultimately%2C%20these%20challenges%20can%20lead%20to%0Areduced%20overall%20team%20performance.%20My%20Ph.D.%20dissertation%20aims%20to%20develop%0AAI-augmented%20team%20optimization%20frameworks%20and%20practical%20systems%20that%20enhance%0Ateam%20satisfaction%2C%20engagement%2C%20and%20performance.%20First%2C%20I%20propose%20a%20team%0Aformation%20framework%20that%20leverages%20a%20multi-armed%20bandit%20algorithm%20to%0Aiteratively%20refine%20team%20composition%20based%20on%20user%20preferences%2C%20ensuring%0Aalignment%20between%20individual%20needs%20and%20collective%20team%20goals%20to%20enhance%20team%0Asatisfaction.%20Second%2C%20I%20introduce%20tAIfa%20%28Team%20AI%20Feedback%20Assistant%29%2C%20an%0AAI-powered%20system%20that%20utilizes%20large%20language%20models%20%28LLMs%29%20to%20deliver%0Aimmediate%2C%20personalized%20feedback%20to%20both%20teams%20and%20individual%20members%2C%0Aenhancing%20cohesion%20and%20engagement.%20Finally%2C%20I%20present%20PuppeteerLLM%2C%20an%0ALLM-based%20simulation%20framework%20that%20simulates%20multi-agent%20teams%20to%20model%0Acomplex%20team%20dynamics%20within%20realistic%20environments%2C%20incorporating%20task-driven%0Acollaboration%20and%20long-term%20coordination.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05265v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeaming%2520in%2520the%2520AI%2520Era%253A%2520AI-Augmented%2520Frameworks%2520for%2520Forming%252C%2520Simulating%252C%250A%2520%2520and%2520Optimizing%2520Human%2520Teams%26entry.906535625%3DMohammed%2520Almutairi%26entry.1292438233%3D%2520%2520Effective%2520teamwork%2520is%2520essential%2520across%2520diverse%2520domains.%2520During%2520the%2520team%250Aformation%2520stage%252C%2520a%2520key%2520challenge%2520is%2520forming%2520teams%2520that%2520effectively%2520balance%2520user%250Apreferences%2520with%2520task%2520objectives%2520to%2520enhance%2520overall%2520team%2520satisfaction.%2520In%2520the%250Ateam%2520performing%2520stage%252C%2520maintaining%2520cohesion%2520and%2520engagement%2520is%2520critical%2520for%250Asustaining%2520high%2520team%2520performance.%2520However%252C%2520existing%2520computational%2520tools%2520and%250Aalgorithms%2520for%2520team%2520optimization%2520often%2520rely%2520on%2520static%2520data%2520inputs%252C%2520narrow%250Aalgorithmic%2520objectives%252C%2520or%2520solutions%2520tailored%2520for%2520specific%2520contexts%252C%2520failing%2520to%250Aaccount%2520for%2520the%2520dynamic%2520interplay%2520of%2520team%2520members%2520personalities%252C%2520evolving%250Agoals%252C%2520and%2520changing%2520individual%2520preferences.%2520Therefore%252C%2520teams%2520may%2520encounter%250Amember%2520dissatisfaction%252C%2520as%2520purely%2520algorithmic%2520assignments%2520can%2520reduce%2520members%250Acommitment%2520to%2520team%2520goals%2520or%2520experience%2520suboptimal%2520engagement%2520due%2520to%2520the%2520absence%250Aof%2520timely%252C%2520personalized%2520guidance%2520to%2520help%2520members%2520adjust%2520their%2520behaviors%2520and%250Ainteractions%2520as%2520team%2520dynamics%2520evolve.%2520Ultimately%252C%2520these%2520challenges%2520can%2520lead%2520to%250Areduced%2520overall%2520team%2520performance.%2520My%2520Ph.D.%2520dissertation%2520aims%2520to%2520develop%250AAI-augmented%2520team%2520optimization%2520frameworks%2520and%2520practical%2520systems%2520that%2520enhance%250Ateam%2520satisfaction%252C%2520engagement%252C%2520and%2520performance.%2520First%252C%2520I%2520propose%2520a%2520team%250Aformation%2520framework%2520that%2520leverages%2520a%2520multi-armed%2520bandit%2520algorithm%2520to%250Aiteratively%2520refine%2520team%2520composition%2520based%2520on%2520user%2520preferences%252C%2520ensuring%250Aalignment%2520between%2520individual%2520needs%2520and%2520collective%2520team%2520goals%2520to%2520enhance%2520team%250Asatisfaction.%2520Second%252C%2520I%2520introduce%2520tAIfa%2520%2528Team%2520AI%2520Feedback%2520Assistant%2529%252C%2520an%250AAI-powered%2520system%2520that%2520utilizes%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520deliver%250Aimmediate%252C%2520personalized%2520feedback%2520to%2520both%2520teams%2520and%2520individual%2520members%252C%250Aenhancing%2520cohesion%2520and%2520engagement.%2520Finally%252C%2520I%2520present%2520PuppeteerLLM%252C%2520an%250ALLM-based%2520simulation%2520framework%2520that%2520simulates%2520multi-agent%2520teams%2520to%2520model%250Acomplex%2520team%2520dynamics%2520within%2520realistic%2520environments%252C%2520incorporating%2520task-driven%250Acollaboration%2520and%2520long-term%2520coordination.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05265v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Teaming%20in%20the%20AI%20Era%3A%20AI-Augmented%20Frameworks%20for%20Forming%2C%20Simulating%2C%0A%20%20and%20Optimizing%20Human%20Teams&entry.906535625=Mohammed%20Almutairi&entry.1292438233=%20%20Effective%20teamwork%20is%20essential%20across%20diverse%20domains.%20During%20the%20team%0Aformation%20stage%2C%20a%20key%20challenge%20is%20forming%20teams%20that%20effectively%20balance%20user%0Apreferences%20with%20task%20objectives%20to%20enhance%20overall%20team%20satisfaction.%20In%20the%0Ateam%20performing%20stage%2C%20maintaining%20cohesion%20and%20engagement%20is%20critical%20for%0Asustaining%20high%20team%20performance.%20However%2C%20existing%20computational%20tools%20and%0Aalgorithms%20for%20team%20optimization%20often%20rely%20on%20static%20data%20inputs%2C%20narrow%0Aalgorithmic%20objectives%2C%20or%20solutions%20tailored%20for%20specific%20contexts%2C%20failing%20to%0Aaccount%20for%20the%20dynamic%20interplay%20of%20team%20members%20personalities%2C%20evolving%0Agoals%2C%20and%20changing%20individual%20preferences.%20Therefore%2C%20teams%20may%20encounter%0Amember%20dissatisfaction%2C%20as%20purely%20algorithmic%20assignments%20can%20reduce%20members%0Acommitment%20to%20team%20goals%20or%20experience%20suboptimal%20engagement%20due%20to%20the%20absence%0Aof%20timely%2C%20personalized%20guidance%20to%20help%20members%20adjust%20their%20behaviors%20and%0Ainteractions%20as%20team%20dynamics%20evolve.%20Ultimately%2C%20these%20challenges%20can%20lead%20to%0Areduced%20overall%20team%20performance.%20My%20Ph.D.%20dissertation%20aims%20to%20develop%0AAI-augmented%20team%20optimization%20frameworks%20and%20practical%20systems%20that%20enhance%0Ateam%20satisfaction%2C%20engagement%2C%20and%20performance.%20First%2C%20I%20propose%20a%20team%0Aformation%20framework%20that%20leverages%20a%20multi-armed%20bandit%20algorithm%20to%0Aiteratively%20refine%20team%20composition%20based%20on%20user%20preferences%2C%20ensuring%0Aalignment%20between%20individual%20needs%20and%20collective%20team%20goals%20to%20enhance%20team%0Asatisfaction.%20Second%2C%20I%20introduce%20tAIfa%20%28Team%20AI%20Feedback%20Assistant%29%2C%20an%0AAI-powered%20system%20that%20utilizes%20large%20language%20models%20%28LLMs%29%20to%20deliver%0Aimmediate%2C%20personalized%20feedback%20to%20both%20teams%20and%20individual%20members%2C%0Aenhancing%20cohesion%20and%20engagement.%20Finally%2C%20I%20present%20PuppeteerLLM%2C%20an%0ALLM-based%20simulation%20framework%20that%20simulates%20multi-agent%20teams%20to%20model%0Acomplex%20team%20dynamics%20within%20realistic%20environments%2C%20incorporating%20task-driven%0Acollaboration%20and%20long-term%20coordination.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05265v2&entry.124074799=Read"},
{"title": "Text-to-LoRA: Instant Transformer Adaption", "author": "Rujikorn Charakorn and Edoardo Cetin and Yujin Tang and Robert Tjarko Lange", "abstract": "  While Foundation Models provide a general tool for rapid content creation,\nthey regularly require task-specific adaptation. Traditionally, this exercise\ninvolves careful curation of datasets and repeated fine-tuning of the\nunderlying model. Fine-tuning techniques enable practitioners to adapt\nfoundation models for many new applications but require expensive and lengthy\ntraining while being notably sensitive to hyper-parameter choices. To overcome\nthese limitations, we introduce Text-to-LoRA (T2L), a model capable of adapting\nLarge Language Models on the fly solely based on a natural language description\nof the target task. T2L is a hypernetwork trained to construct LoRAs in a\nsingle inexpensive forward pass. After training T2L on a suite of 9 pre-trained\nLoRA adapters (GSM8K, Arc, etc.), we show that the ad-hoc reconstructed LoRA\ninstances match the performance of task-specific adapters across the\ncorresponding test sets. Furthermore, T2L can compress hundreds of LoRA\ninstances and zero-shot generalize to entirely unseen tasks. This approach\nprovides a significant step towards democratizing the specialization of\nfoundation models and enables language-based adaptation with minimal compute\nrequirements. Our code is available at https://github.com/SakanaAI/text-to-lora\n", "link": "http://arxiv.org/abs/2506.06105v1", "date": "2025-06-06", "relevancy": 1.6081, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5429}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5347}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5203}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-to-LoRA%3A%20Instant%20Transformer%20Adaption&body=Title%3A%20Text-to-LoRA%3A%20Instant%20Transformer%20Adaption%0AAuthor%3A%20Rujikorn%20Charakorn%20and%20Edoardo%20Cetin%20and%20Yujin%20Tang%20and%20Robert%20Tjarko%20Lange%0AAbstract%3A%20%20%20While%20Foundation%20Models%20provide%20a%20general%20tool%20for%20rapid%20content%20creation%2C%0Athey%20regularly%20require%20task-specific%20adaptation.%20Traditionally%2C%20this%20exercise%0Ainvolves%20careful%20curation%20of%20datasets%20and%20repeated%20fine-tuning%20of%20the%0Aunderlying%20model.%20Fine-tuning%20techniques%20enable%20practitioners%20to%20adapt%0Afoundation%20models%20for%20many%20new%20applications%20but%20require%20expensive%20and%20lengthy%0Atraining%20while%20being%20notably%20sensitive%20to%20hyper-parameter%20choices.%20To%20overcome%0Athese%20limitations%2C%20we%20introduce%20Text-to-LoRA%20%28T2L%29%2C%20a%20model%20capable%20of%20adapting%0ALarge%20Language%20Models%20on%20the%20fly%20solely%20based%20on%20a%20natural%20language%20description%0Aof%20the%20target%20task.%20T2L%20is%20a%20hypernetwork%20trained%20to%20construct%20LoRAs%20in%20a%0Asingle%20inexpensive%20forward%20pass.%20After%20training%20T2L%20on%20a%20suite%20of%209%20pre-trained%0ALoRA%20adapters%20%28GSM8K%2C%20Arc%2C%20etc.%29%2C%20we%20show%20that%20the%20ad-hoc%20reconstructed%20LoRA%0Ainstances%20match%20the%20performance%20of%20task-specific%20adapters%20across%20the%0Acorresponding%20test%20sets.%20Furthermore%2C%20T2L%20can%20compress%20hundreds%20of%20LoRA%0Ainstances%20and%20zero-shot%20generalize%20to%20entirely%20unseen%20tasks.%20This%20approach%0Aprovides%20a%20significant%20step%20towards%20democratizing%20the%20specialization%20of%0Afoundation%20models%20and%20enables%20language-based%20adaptation%20with%20minimal%20compute%0Arequirements.%20Our%20code%20is%20available%20at%20https%3A//github.com/SakanaAI/text-to-lora%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06105v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-to-LoRA%253A%2520Instant%2520Transformer%2520Adaption%26entry.906535625%3DRujikorn%2520Charakorn%2520and%2520Edoardo%2520Cetin%2520and%2520Yujin%2520Tang%2520and%2520Robert%2520Tjarko%2520Lange%26entry.1292438233%3D%2520%2520While%2520Foundation%2520Models%2520provide%2520a%2520general%2520tool%2520for%2520rapid%2520content%2520creation%252C%250Athey%2520regularly%2520require%2520task-specific%2520adaptation.%2520Traditionally%252C%2520this%2520exercise%250Ainvolves%2520careful%2520curation%2520of%2520datasets%2520and%2520repeated%2520fine-tuning%2520of%2520the%250Aunderlying%2520model.%2520Fine-tuning%2520techniques%2520enable%2520practitioners%2520to%2520adapt%250Afoundation%2520models%2520for%2520many%2520new%2520applications%2520but%2520require%2520expensive%2520and%2520lengthy%250Atraining%2520while%2520being%2520notably%2520sensitive%2520to%2520hyper-parameter%2520choices.%2520To%2520overcome%250Athese%2520limitations%252C%2520we%2520introduce%2520Text-to-LoRA%2520%2528T2L%2529%252C%2520a%2520model%2520capable%2520of%2520adapting%250ALarge%2520Language%2520Models%2520on%2520the%2520fly%2520solely%2520based%2520on%2520a%2520natural%2520language%2520description%250Aof%2520the%2520target%2520task.%2520T2L%2520is%2520a%2520hypernetwork%2520trained%2520to%2520construct%2520LoRAs%2520in%2520a%250Asingle%2520inexpensive%2520forward%2520pass.%2520After%2520training%2520T2L%2520on%2520a%2520suite%2520of%25209%2520pre-trained%250ALoRA%2520adapters%2520%2528GSM8K%252C%2520Arc%252C%2520etc.%2529%252C%2520we%2520show%2520that%2520the%2520ad-hoc%2520reconstructed%2520LoRA%250Ainstances%2520match%2520the%2520performance%2520of%2520task-specific%2520adapters%2520across%2520the%250Acorresponding%2520test%2520sets.%2520Furthermore%252C%2520T2L%2520can%2520compress%2520hundreds%2520of%2520LoRA%250Ainstances%2520and%2520zero-shot%2520generalize%2520to%2520entirely%2520unseen%2520tasks.%2520This%2520approach%250Aprovides%2520a%2520significant%2520step%2520towards%2520democratizing%2520the%2520specialization%2520of%250Afoundation%2520models%2520and%2520enables%2520language-based%2520adaptation%2520with%2520minimal%2520compute%250Arequirements.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/SakanaAI/text-to-lora%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06105v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-to-LoRA%3A%20Instant%20Transformer%20Adaption&entry.906535625=Rujikorn%20Charakorn%20and%20Edoardo%20Cetin%20and%20Yujin%20Tang%20and%20Robert%20Tjarko%20Lange&entry.1292438233=%20%20While%20Foundation%20Models%20provide%20a%20general%20tool%20for%20rapid%20content%20creation%2C%0Athey%20regularly%20require%20task-specific%20adaptation.%20Traditionally%2C%20this%20exercise%0Ainvolves%20careful%20curation%20of%20datasets%20and%20repeated%20fine-tuning%20of%20the%0Aunderlying%20model.%20Fine-tuning%20techniques%20enable%20practitioners%20to%20adapt%0Afoundation%20models%20for%20many%20new%20applications%20but%20require%20expensive%20and%20lengthy%0Atraining%20while%20being%20notably%20sensitive%20to%20hyper-parameter%20choices.%20To%20overcome%0Athese%20limitations%2C%20we%20introduce%20Text-to-LoRA%20%28T2L%29%2C%20a%20model%20capable%20of%20adapting%0ALarge%20Language%20Models%20on%20the%20fly%20solely%20based%20on%20a%20natural%20language%20description%0Aof%20the%20target%20task.%20T2L%20is%20a%20hypernetwork%20trained%20to%20construct%20LoRAs%20in%20a%0Asingle%20inexpensive%20forward%20pass.%20After%20training%20T2L%20on%20a%20suite%20of%209%20pre-trained%0ALoRA%20adapters%20%28GSM8K%2C%20Arc%2C%20etc.%29%2C%20we%20show%20that%20the%20ad-hoc%20reconstructed%20LoRA%0Ainstances%20match%20the%20performance%20of%20task-specific%20adapters%20across%20the%0Acorresponding%20test%20sets.%20Furthermore%2C%20T2L%20can%20compress%20hundreds%20of%20LoRA%0Ainstances%20and%20zero-shot%20generalize%20to%20entirely%20unseen%20tasks.%20This%20approach%0Aprovides%20a%20significant%20step%20towards%20democratizing%20the%20specialization%20of%0Afoundation%20models%20and%20enables%20language-based%20adaptation%20with%20minimal%20compute%0Arequirements.%20Our%20code%20is%20available%20at%20https%3A//github.com/SakanaAI/text-to-lora%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06105v1&entry.124074799=Read"},
{"title": "Scaffolding Creativity: Integrating Generative AI Tools and Real-world\n  Experiences in Business Education", "author": "Nicole C. Wang", "abstract": "  This exploratory study investigates the intersection of Generative AI tools\nand experiential learning in business education. Through a case study of an\ninnovative undergraduate course, we examine how students interact with and\nadapt to various AI modalities-from text-based tools to image\ngeneration-alongside real-world experiences. Our findings reveal how this\nintegrated approach enables novice users to overcome creative barriers,\naccelerates skill acquisition, and creates a dynamic interplay between\nAI-generated insights and real-world validation. We identify critical\ninteraction challenges, including prompt engineering patterns and the need for\nmore intuitive AI interfaces in educational contexts. These insights inform the\ndesign of future AI tools for creative learning and contribute to broader HCI\ndiscussions about human-AI collaboration in educational settings.\n", "link": "http://arxiv.org/abs/2501.06527v2", "date": "2025-06-06", "relevancy": 1.6079, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5493}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5332}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaffolding%20Creativity%3A%20Integrating%20Generative%20AI%20Tools%20and%20Real-world%0A%20%20Experiences%20in%20Business%20Education&body=Title%3A%20Scaffolding%20Creativity%3A%20Integrating%20Generative%20AI%20Tools%20and%20Real-world%0A%20%20Experiences%20in%20Business%20Education%0AAuthor%3A%20Nicole%20C.%20Wang%0AAbstract%3A%20%20%20This%20exploratory%20study%20investigates%20the%20intersection%20of%20Generative%20AI%20tools%0Aand%20experiential%20learning%20in%20business%20education.%20Through%20a%20case%20study%20of%20an%0Ainnovative%20undergraduate%20course%2C%20we%20examine%20how%20students%20interact%20with%20and%0Aadapt%20to%20various%20AI%20modalities-from%20text-based%20tools%20to%20image%0Ageneration-alongside%20real-world%20experiences.%20Our%20findings%20reveal%20how%20this%0Aintegrated%20approach%20enables%20novice%20users%20to%20overcome%20creative%20barriers%2C%0Aaccelerates%20skill%20acquisition%2C%20and%20creates%20a%20dynamic%20interplay%20between%0AAI-generated%20insights%20and%20real-world%20validation.%20We%20identify%20critical%0Ainteraction%20challenges%2C%20including%20prompt%20engineering%20patterns%20and%20the%20need%20for%0Amore%20intuitive%20AI%20interfaces%20in%20educational%20contexts.%20These%20insights%20inform%20the%0Adesign%20of%20future%20AI%20tools%20for%20creative%20learning%20and%20contribute%20to%20broader%20HCI%0Adiscussions%20about%20human-AI%20collaboration%20in%20educational%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06527v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaffolding%2520Creativity%253A%2520Integrating%2520Generative%2520AI%2520Tools%2520and%2520Real-world%250A%2520%2520Experiences%2520in%2520Business%2520Education%26entry.906535625%3DNicole%2520C.%2520Wang%26entry.1292438233%3D%2520%2520This%2520exploratory%2520study%2520investigates%2520the%2520intersection%2520of%2520Generative%2520AI%2520tools%250Aand%2520experiential%2520learning%2520in%2520business%2520education.%2520Through%2520a%2520case%2520study%2520of%2520an%250Ainnovative%2520undergraduate%2520course%252C%2520we%2520examine%2520how%2520students%2520interact%2520with%2520and%250Aadapt%2520to%2520various%2520AI%2520modalities-from%2520text-based%2520tools%2520to%2520image%250Ageneration-alongside%2520real-world%2520experiences.%2520Our%2520findings%2520reveal%2520how%2520this%250Aintegrated%2520approach%2520enables%2520novice%2520users%2520to%2520overcome%2520creative%2520barriers%252C%250Aaccelerates%2520skill%2520acquisition%252C%2520and%2520creates%2520a%2520dynamic%2520interplay%2520between%250AAI-generated%2520insights%2520and%2520real-world%2520validation.%2520We%2520identify%2520critical%250Ainteraction%2520challenges%252C%2520including%2520prompt%2520engineering%2520patterns%2520and%2520the%2520need%2520for%250Amore%2520intuitive%2520AI%2520interfaces%2520in%2520educational%2520contexts.%2520These%2520insights%2520inform%2520the%250Adesign%2520of%2520future%2520AI%2520tools%2520for%2520creative%2520learning%2520and%2520contribute%2520to%2520broader%2520HCI%250Adiscussions%2520about%2520human-AI%2520collaboration%2520in%2520educational%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06527v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaffolding%20Creativity%3A%20Integrating%20Generative%20AI%20Tools%20and%20Real-world%0A%20%20Experiences%20in%20Business%20Education&entry.906535625=Nicole%20C.%20Wang&entry.1292438233=%20%20This%20exploratory%20study%20investigates%20the%20intersection%20of%20Generative%20AI%20tools%0Aand%20experiential%20learning%20in%20business%20education.%20Through%20a%20case%20study%20of%20an%0Ainnovative%20undergraduate%20course%2C%20we%20examine%20how%20students%20interact%20with%20and%0Aadapt%20to%20various%20AI%20modalities-from%20text-based%20tools%20to%20image%0Ageneration-alongside%20real-world%20experiences.%20Our%20findings%20reveal%20how%20this%0Aintegrated%20approach%20enables%20novice%20users%20to%20overcome%20creative%20barriers%2C%0Aaccelerates%20skill%20acquisition%2C%20and%20creates%20a%20dynamic%20interplay%20between%0AAI-generated%20insights%20and%20real-world%20validation.%20We%20identify%20critical%0Ainteraction%20challenges%2C%20including%20prompt%20engineering%20patterns%20and%20the%20need%20for%0Amore%20intuitive%20AI%20interfaces%20in%20educational%20contexts.%20These%20insights%20inform%20the%0Adesign%20of%20future%20AI%20tools%20for%20creative%20learning%20and%20contribute%20to%20broader%20HCI%0Adiscussions%20about%20human-AI%20collaboration%20in%20educational%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06527v2&entry.124074799=Read"},
{"title": "ByzSecAgg: A Byzantine-Resistant Secure Aggregation Scheme for Federated\n  Learning Based on Coded Computing and Vector Commitment", "author": "Tayyebeh Jahani-Nezhad and Mohammad Ali Maddah-Ali and Giuseppe Caire", "abstract": "  In this paper, we propose ByzSecAgg, an efficient secure aggregation scheme\nfor federated learning that is resistant to Byzantine attacks and privacy\nleakages. Processing individual updates to manage adversarial behavior, while\npreserving the privacy of the data against colluding nodes, requires some sort\nof secure secret sharing. However, the communication load for secret sharing of\nlong vectors of updates can be very high. In federated settings, where users\nare often edge devices with potential bandwidth constraints, excessive\ncommunication overhead is undesirable. ByzSecAgg solves this problem by\npartitioning local updates into smaller sub-vectors and sharing them using ramp\nsecret sharing. However, this sharing method does not admit bilinear\ncomputations, such as pairwise distances calculations, which are needed for\ndistance-based outlier-detection algorithms, and effective methods for\nmitigating Byzantine attacks. To overcome this issue, each user runs another\nround of ramp sharing, with a different embedding of the data in the sharing\npolynomial. This technique, motivated by ideas from coded computing, enables\nsecure computation of pairwise distance. In addition, to maintain the integrity\nand privacy of the local update, ByzSecAgg also uses a vector commitment\nmethod, in which the commitment size remains constant (i.e., does not increase\nwith the length of the local update), while simultaneously allowing\nverification of the secret sharing process. In terms of communication load,\nByzSecAgg significantly outperforms the related baseline scheme, known as BREA.\n", "link": "http://arxiv.org/abs/2302.09913v4", "date": "2025-06-06", "relevancy": 1.7587, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4446}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4367}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4346}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ByzSecAgg%3A%20A%20Byzantine-Resistant%20Secure%20Aggregation%20Scheme%20for%20Federated%0A%20%20Learning%20Based%20on%20Coded%20Computing%20and%20Vector%20Commitment&body=Title%3A%20ByzSecAgg%3A%20A%20Byzantine-Resistant%20Secure%20Aggregation%20Scheme%20for%20Federated%0A%20%20Learning%20Based%20on%20Coded%20Computing%20and%20Vector%20Commitment%0AAuthor%3A%20Tayyebeh%20Jahani-Nezhad%20and%20Mohammad%20Ali%20Maddah-Ali%20and%20Giuseppe%20Caire%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20ByzSecAgg%2C%20an%20efficient%20secure%20aggregation%20scheme%0Afor%20federated%20learning%20that%20is%20resistant%20to%20Byzantine%20attacks%20and%20privacy%0Aleakages.%20Processing%20individual%20updates%20to%20manage%20adversarial%20behavior%2C%20while%0Apreserving%20the%20privacy%20of%20the%20data%20against%20colluding%20nodes%2C%20requires%20some%20sort%0Aof%20secure%20secret%20sharing.%20However%2C%20the%20communication%20load%20for%20secret%20sharing%20of%0Along%20vectors%20of%20updates%20can%20be%20very%20high.%20In%20federated%20settings%2C%20where%20users%0Aare%20often%20edge%20devices%20with%20potential%20bandwidth%20constraints%2C%20excessive%0Acommunication%20overhead%20is%20undesirable.%20ByzSecAgg%20solves%20this%20problem%20by%0Apartitioning%20local%20updates%20into%20smaller%20sub-vectors%20and%20sharing%20them%20using%20ramp%0Asecret%20sharing.%20However%2C%20this%20sharing%20method%20does%20not%20admit%20bilinear%0Acomputations%2C%20such%20as%20pairwise%20distances%20calculations%2C%20which%20are%20needed%20for%0Adistance-based%20outlier-detection%20algorithms%2C%20and%20effective%20methods%20for%0Amitigating%20Byzantine%20attacks.%20To%20overcome%20this%20issue%2C%20each%20user%20runs%20another%0Around%20of%20ramp%20sharing%2C%20with%20a%20different%20embedding%20of%20the%20data%20in%20the%20sharing%0Apolynomial.%20This%20technique%2C%20motivated%20by%20ideas%20from%20coded%20computing%2C%20enables%0Asecure%20computation%20of%20pairwise%20distance.%20In%20addition%2C%20to%20maintain%20the%20integrity%0Aand%20privacy%20of%20the%20local%20update%2C%20ByzSecAgg%20also%20uses%20a%20vector%20commitment%0Amethod%2C%20in%20which%20the%20commitment%20size%20remains%20constant%20%28i.e.%2C%20does%20not%20increase%0Awith%20the%20length%20of%20the%20local%20update%29%2C%20while%20simultaneously%20allowing%0Averification%20of%20the%20secret%20sharing%20process.%20In%20terms%20of%20communication%20load%2C%0AByzSecAgg%20significantly%20outperforms%20the%20related%20baseline%20scheme%2C%20known%20as%20BREA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.09913v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DByzSecAgg%253A%2520A%2520Byzantine-Resistant%2520Secure%2520Aggregation%2520Scheme%2520for%2520Federated%250A%2520%2520Learning%2520Based%2520on%2520Coded%2520Computing%2520and%2520Vector%2520Commitment%26entry.906535625%3DTayyebeh%2520Jahani-Nezhad%2520and%2520Mohammad%2520Ali%2520Maddah-Ali%2520and%2520Giuseppe%2520Caire%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520ByzSecAgg%252C%2520an%2520efficient%2520secure%2520aggregation%2520scheme%250Afor%2520federated%2520learning%2520that%2520is%2520resistant%2520to%2520Byzantine%2520attacks%2520and%2520privacy%250Aleakages.%2520Processing%2520individual%2520updates%2520to%2520manage%2520adversarial%2520behavior%252C%2520while%250Apreserving%2520the%2520privacy%2520of%2520the%2520data%2520against%2520colluding%2520nodes%252C%2520requires%2520some%2520sort%250Aof%2520secure%2520secret%2520sharing.%2520However%252C%2520the%2520communication%2520load%2520for%2520secret%2520sharing%2520of%250Along%2520vectors%2520of%2520updates%2520can%2520be%2520very%2520high.%2520In%2520federated%2520settings%252C%2520where%2520users%250Aare%2520often%2520edge%2520devices%2520with%2520potential%2520bandwidth%2520constraints%252C%2520excessive%250Acommunication%2520overhead%2520is%2520undesirable.%2520ByzSecAgg%2520solves%2520this%2520problem%2520by%250Apartitioning%2520local%2520updates%2520into%2520smaller%2520sub-vectors%2520and%2520sharing%2520them%2520using%2520ramp%250Asecret%2520sharing.%2520However%252C%2520this%2520sharing%2520method%2520does%2520not%2520admit%2520bilinear%250Acomputations%252C%2520such%2520as%2520pairwise%2520distances%2520calculations%252C%2520which%2520are%2520needed%2520for%250Adistance-based%2520outlier-detection%2520algorithms%252C%2520and%2520effective%2520methods%2520for%250Amitigating%2520Byzantine%2520attacks.%2520To%2520overcome%2520this%2520issue%252C%2520each%2520user%2520runs%2520another%250Around%2520of%2520ramp%2520sharing%252C%2520with%2520a%2520different%2520embedding%2520of%2520the%2520data%2520in%2520the%2520sharing%250Apolynomial.%2520This%2520technique%252C%2520motivated%2520by%2520ideas%2520from%2520coded%2520computing%252C%2520enables%250Asecure%2520computation%2520of%2520pairwise%2520distance.%2520In%2520addition%252C%2520to%2520maintain%2520the%2520integrity%250Aand%2520privacy%2520of%2520the%2520local%2520update%252C%2520ByzSecAgg%2520also%2520uses%2520a%2520vector%2520commitment%250Amethod%252C%2520in%2520which%2520the%2520commitment%2520size%2520remains%2520constant%2520%2528i.e.%252C%2520does%2520not%2520increase%250Awith%2520the%2520length%2520of%2520the%2520local%2520update%2529%252C%2520while%2520simultaneously%2520allowing%250Averification%2520of%2520the%2520secret%2520sharing%2520process.%2520In%2520terms%2520of%2520communication%2520load%252C%250AByzSecAgg%2520significantly%2520outperforms%2520the%2520related%2520baseline%2520scheme%252C%2520known%2520as%2520BREA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.09913v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ByzSecAgg%3A%20A%20Byzantine-Resistant%20Secure%20Aggregation%20Scheme%20for%20Federated%0A%20%20Learning%20Based%20on%20Coded%20Computing%20and%20Vector%20Commitment&entry.906535625=Tayyebeh%20Jahani-Nezhad%20and%20Mohammad%20Ali%20Maddah-Ali%20and%20Giuseppe%20Caire&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20ByzSecAgg%2C%20an%20efficient%20secure%20aggregation%20scheme%0Afor%20federated%20learning%20that%20is%20resistant%20to%20Byzantine%20attacks%20and%20privacy%0Aleakages.%20Processing%20individual%20updates%20to%20manage%20adversarial%20behavior%2C%20while%0Apreserving%20the%20privacy%20of%20the%20data%20against%20colluding%20nodes%2C%20requires%20some%20sort%0Aof%20secure%20secret%20sharing.%20However%2C%20the%20communication%20load%20for%20secret%20sharing%20of%0Along%20vectors%20of%20updates%20can%20be%20very%20high.%20In%20federated%20settings%2C%20where%20users%0Aare%20often%20edge%20devices%20with%20potential%20bandwidth%20constraints%2C%20excessive%0Acommunication%20overhead%20is%20undesirable.%20ByzSecAgg%20solves%20this%20problem%20by%0Apartitioning%20local%20updates%20into%20smaller%20sub-vectors%20and%20sharing%20them%20using%20ramp%0Asecret%20sharing.%20However%2C%20this%20sharing%20method%20does%20not%20admit%20bilinear%0Acomputations%2C%20such%20as%20pairwise%20distances%20calculations%2C%20which%20are%20needed%20for%0Adistance-based%20outlier-detection%20algorithms%2C%20and%20effective%20methods%20for%0Amitigating%20Byzantine%20attacks.%20To%20overcome%20this%20issue%2C%20each%20user%20runs%20another%0Around%20of%20ramp%20sharing%2C%20with%20a%20different%20embedding%20of%20the%20data%20in%20the%20sharing%0Apolynomial.%20This%20technique%2C%20motivated%20by%20ideas%20from%20coded%20computing%2C%20enables%0Asecure%20computation%20of%20pairwise%20distance.%20In%20addition%2C%20to%20maintain%20the%20integrity%0Aand%20privacy%20of%20the%20local%20update%2C%20ByzSecAgg%20also%20uses%20a%20vector%20commitment%0Amethod%2C%20in%20which%20the%20commitment%20size%20remains%20constant%20%28i.e.%2C%20does%20not%20increase%0Awith%20the%20length%20of%20the%20local%20update%29%2C%20while%20simultaneously%20allowing%0Averification%20of%20the%20secret%20sharing%20process.%20In%20terms%20of%20communication%20load%2C%0AByzSecAgg%20significantly%20outperforms%20the%20related%20baseline%20scheme%2C%20known%20as%20BREA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.09913v4&entry.124074799=Read"},
{"title": "A Theoretical Justification for Asymmetric Actor-Critic Algorithms", "author": "Gaspard Lambrechts and Damien Ernst and Aditya Mahajan", "abstract": "  In reinforcement learning for partially observable environments, many\nsuccessful algorithms have been developed within the asymmetric learning\nparadigm. This paradigm leverages additional state information available at\ntraining time for faster learning. Although the proposed learning objectives\nare usually theoretically sound, these methods still lack a precise theoretical\njustification for their potential benefits. We propose such a justification for\nasymmetric actor-critic algorithms with linear function approximators by\nadapting a finite-time convergence analysis to this setting. The resulting\nfinite-time bound reveals that the asymmetric critic eliminates error terms\narising from aliasing in the agent state.\n", "link": "http://arxiv.org/abs/2501.19116v2", "date": "2025-06-06", "relevancy": 1.3159, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4452}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4409}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Theoretical%20Justification%20for%20Asymmetric%20Actor-Critic%20Algorithms&body=Title%3A%20A%20Theoretical%20Justification%20for%20Asymmetric%20Actor-Critic%20Algorithms%0AAuthor%3A%20Gaspard%20Lambrechts%20and%20Damien%20Ernst%20and%20Aditya%20Mahajan%0AAbstract%3A%20%20%20In%20reinforcement%20learning%20for%20partially%20observable%20environments%2C%20many%0Asuccessful%20algorithms%20have%20been%20developed%20within%20the%20asymmetric%20learning%0Aparadigm.%20This%20paradigm%20leverages%20additional%20state%20information%20available%20at%0Atraining%20time%20for%20faster%20learning.%20Although%20the%20proposed%20learning%20objectives%0Aare%20usually%20theoretically%20sound%2C%20these%20methods%20still%20lack%20a%20precise%20theoretical%0Ajustification%20for%20their%20potential%20benefits.%20We%20propose%20such%20a%20justification%20for%0Aasymmetric%20actor-critic%20algorithms%20with%20linear%20function%20approximators%20by%0Aadapting%20a%20finite-time%20convergence%20analysis%20to%20this%20setting.%20The%20resulting%0Afinite-time%20bound%20reveals%20that%20the%20asymmetric%20critic%20eliminates%20error%20terms%0Aarising%20from%20aliasing%20in%20the%20agent%20state.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19116v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Theoretical%2520Justification%2520for%2520Asymmetric%2520Actor-Critic%2520Algorithms%26entry.906535625%3DGaspard%2520Lambrechts%2520and%2520Damien%2520Ernst%2520and%2520Aditya%2520Mahajan%26entry.1292438233%3D%2520%2520In%2520reinforcement%2520learning%2520for%2520partially%2520observable%2520environments%252C%2520many%250Asuccessful%2520algorithms%2520have%2520been%2520developed%2520within%2520the%2520asymmetric%2520learning%250Aparadigm.%2520This%2520paradigm%2520leverages%2520additional%2520state%2520information%2520available%2520at%250Atraining%2520time%2520for%2520faster%2520learning.%2520Although%2520the%2520proposed%2520learning%2520objectives%250Aare%2520usually%2520theoretically%2520sound%252C%2520these%2520methods%2520still%2520lack%2520a%2520precise%2520theoretical%250Ajustification%2520for%2520their%2520potential%2520benefits.%2520We%2520propose%2520such%2520a%2520justification%2520for%250Aasymmetric%2520actor-critic%2520algorithms%2520with%2520linear%2520function%2520approximators%2520by%250Aadapting%2520a%2520finite-time%2520convergence%2520analysis%2520to%2520this%2520setting.%2520The%2520resulting%250Afinite-time%2520bound%2520reveals%2520that%2520the%2520asymmetric%2520critic%2520eliminates%2520error%2520terms%250Aarising%2520from%2520aliasing%2520in%2520the%2520agent%2520state.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19116v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Theoretical%20Justification%20for%20Asymmetric%20Actor-Critic%20Algorithms&entry.906535625=Gaspard%20Lambrechts%20and%20Damien%20Ernst%20and%20Aditya%20Mahajan&entry.1292438233=%20%20In%20reinforcement%20learning%20for%20partially%20observable%20environments%2C%20many%0Asuccessful%20algorithms%20have%20been%20developed%20within%20the%20asymmetric%20learning%0Aparadigm.%20This%20paradigm%20leverages%20additional%20state%20information%20available%20at%0Atraining%20time%20for%20faster%20learning.%20Although%20the%20proposed%20learning%20objectives%0Aare%20usually%20theoretically%20sound%2C%20these%20methods%20still%20lack%20a%20precise%20theoretical%0Ajustification%20for%20their%20potential%20benefits.%20We%20propose%20such%20a%20justification%20for%0Aasymmetric%20actor-critic%20algorithms%20with%20linear%20function%20approximators%20by%0Aadapting%20a%20finite-time%20convergence%20analysis%20to%20this%20setting.%20The%20resulting%0Afinite-time%20bound%20reveals%20that%20the%20asymmetric%20critic%20eliminates%20error%20terms%0Aarising%20from%20aliasing%20in%20the%20agent%20state.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19116v2&entry.124074799=Read"},
{"title": "One Stone, Two Birds: Enhancing Adversarial Defense Through the Lens of\n  Distributional Discrepancy", "author": "Jiacheng Zhang and Benjamin I. P. Rubinstein and Jingfeng Zhang and Feng Liu", "abstract": "  Statistical adversarial data detection (SADD) detects whether an upcoming\nbatch contains adversarial examples (AEs) by measuring the distributional\ndiscrepancies between clean examples (CEs) and AEs. In this paper, we explore\nthe strength of SADD-based methods by theoretically showing that minimizing\ndistributional discrepancy can help reduce the expected loss on AEs. Despite\nthese advantages, SADD-based methods have a potential limitation: they discard\ninputs that are detected as AEs, leading to the loss of useful information\nwithin those inputs. To address this limitation, we propose a two-pronged\nadversarial defense method, named Distributional-discrepancy-based Adversarial\nDefense (DAD). In the training phase, DAD first optimizes the test power of the\nmaximum mean discrepancy (MMD) to derive MMD-OPT, which is a stone that kills\ntwo birds. MMD-OPT first serves as a guiding signal to minimize the\ndistributional discrepancy between CEs and AEs to train a denoiser. Then, it\nserves as a discriminator to differentiate CEs and AEs during inference.\nOverall, in the inference stage, DAD consists of a two-pronged process: (1)\ndirectly feeding the detected CEs into the classifier, and (2) removing noise\nfrom the detected AEs by the distributional-discrepancy-based denoiser.\nExtensive experiments show that DAD outperforms current state-of-the-art (SOTA)\ndefense methods by simultaneously improving clean and robust accuracy on\nCIFAR-10 and ImageNet-1K against adaptive white-box attacks. Codes are publicly\navailable at: https://github.com/tmlr-group/DAD.\n", "link": "http://arxiv.org/abs/2503.02169v2", "date": "2025-06-06", "relevancy": 1.5896, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5428}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5147}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20Stone%2C%20Two%20Birds%3A%20Enhancing%20Adversarial%20Defense%20Through%20the%20Lens%20of%0A%20%20Distributional%20Discrepancy&body=Title%3A%20One%20Stone%2C%20Two%20Birds%3A%20Enhancing%20Adversarial%20Defense%20Through%20the%20Lens%20of%0A%20%20Distributional%20Discrepancy%0AAuthor%3A%20Jiacheng%20Zhang%20and%20Benjamin%20I.%20P.%20Rubinstein%20and%20Jingfeng%20Zhang%20and%20Feng%20Liu%0AAbstract%3A%20%20%20Statistical%20adversarial%20data%20detection%20%28SADD%29%20detects%20whether%20an%20upcoming%0Abatch%20contains%20adversarial%20examples%20%28AEs%29%20by%20measuring%20the%20distributional%0Adiscrepancies%20between%20clean%20examples%20%28CEs%29%20and%20AEs.%20In%20this%20paper%2C%20we%20explore%0Athe%20strength%20of%20SADD-based%20methods%20by%20theoretically%20showing%20that%20minimizing%0Adistributional%20discrepancy%20can%20help%20reduce%20the%20expected%20loss%20on%20AEs.%20Despite%0Athese%20advantages%2C%20SADD-based%20methods%20have%20a%20potential%20limitation%3A%20they%20discard%0Ainputs%20that%20are%20detected%20as%20AEs%2C%20leading%20to%20the%20loss%20of%20useful%20information%0Awithin%20those%20inputs.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20two-pronged%0Aadversarial%20defense%20method%2C%20named%20Distributional-discrepancy-based%20Adversarial%0ADefense%20%28DAD%29.%20In%20the%20training%20phase%2C%20DAD%20first%20optimizes%20the%20test%20power%20of%20the%0Amaximum%20mean%20discrepancy%20%28MMD%29%20to%20derive%20MMD-OPT%2C%20which%20is%20a%20stone%20that%20kills%0Atwo%20birds.%20MMD-OPT%20first%20serves%20as%20a%20guiding%20signal%20to%20minimize%20the%0Adistributional%20discrepancy%20between%20CEs%20and%20AEs%20to%20train%20a%20denoiser.%20Then%2C%20it%0Aserves%20as%20a%20discriminator%20to%20differentiate%20CEs%20and%20AEs%20during%20inference.%0AOverall%2C%20in%20the%20inference%20stage%2C%20DAD%20consists%20of%20a%20two-pronged%20process%3A%20%281%29%0Adirectly%20feeding%20the%20detected%20CEs%20into%20the%20classifier%2C%20and%20%282%29%20removing%20noise%0Afrom%20the%20detected%20AEs%20by%20the%20distributional-discrepancy-based%20denoiser.%0AExtensive%20experiments%20show%20that%20DAD%20outperforms%20current%20state-of-the-art%20%28SOTA%29%0Adefense%20methods%20by%20simultaneously%20improving%20clean%20and%20robust%20accuracy%20on%0ACIFAR-10%20and%20ImageNet-1K%20against%20adaptive%20white-box%20attacks.%20Codes%20are%20publicly%0Aavailable%20at%3A%20https%3A//github.com/tmlr-group/DAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.02169v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520Stone%252C%2520Two%2520Birds%253A%2520Enhancing%2520Adversarial%2520Defense%2520Through%2520the%2520Lens%2520of%250A%2520%2520Distributional%2520Discrepancy%26entry.906535625%3DJiacheng%2520Zhang%2520and%2520Benjamin%2520I.%2520P.%2520Rubinstein%2520and%2520Jingfeng%2520Zhang%2520and%2520Feng%2520Liu%26entry.1292438233%3D%2520%2520Statistical%2520adversarial%2520data%2520detection%2520%2528SADD%2529%2520detects%2520whether%2520an%2520upcoming%250Abatch%2520contains%2520adversarial%2520examples%2520%2528AEs%2529%2520by%2520measuring%2520the%2520distributional%250Adiscrepancies%2520between%2520clean%2520examples%2520%2528CEs%2529%2520and%2520AEs.%2520In%2520this%2520paper%252C%2520we%2520explore%250Athe%2520strength%2520of%2520SADD-based%2520methods%2520by%2520theoretically%2520showing%2520that%2520minimizing%250Adistributional%2520discrepancy%2520can%2520help%2520reduce%2520the%2520expected%2520loss%2520on%2520AEs.%2520Despite%250Athese%2520advantages%252C%2520SADD-based%2520methods%2520have%2520a%2520potential%2520limitation%253A%2520they%2520discard%250Ainputs%2520that%2520are%2520detected%2520as%2520AEs%252C%2520leading%2520to%2520the%2520loss%2520of%2520useful%2520information%250Awithin%2520those%2520inputs.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%2520two-pronged%250Aadversarial%2520defense%2520method%252C%2520named%2520Distributional-discrepancy-based%2520Adversarial%250ADefense%2520%2528DAD%2529.%2520In%2520the%2520training%2520phase%252C%2520DAD%2520first%2520optimizes%2520the%2520test%2520power%2520of%2520the%250Amaximum%2520mean%2520discrepancy%2520%2528MMD%2529%2520to%2520derive%2520MMD-OPT%252C%2520which%2520is%2520a%2520stone%2520that%2520kills%250Atwo%2520birds.%2520MMD-OPT%2520first%2520serves%2520as%2520a%2520guiding%2520signal%2520to%2520minimize%2520the%250Adistributional%2520discrepancy%2520between%2520CEs%2520and%2520AEs%2520to%2520train%2520a%2520denoiser.%2520Then%252C%2520it%250Aserves%2520as%2520a%2520discriminator%2520to%2520differentiate%2520CEs%2520and%2520AEs%2520during%2520inference.%250AOverall%252C%2520in%2520the%2520inference%2520stage%252C%2520DAD%2520consists%2520of%2520a%2520two-pronged%2520process%253A%2520%25281%2529%250Adirectly%2520feeding%2520the%2520detected%2520CEs%2520into%2520the%2520classifier%252C%2520and%2520%25282%2529%2520removing%2520noise%250Afrom%2520the%2520detected%2520AEs%2520by%2520the%2520distributional-discrepancy-based%2520denoiser.%250AExtensive%2520experiments%2520show%2520that%2520DAD%2520outperforms%2520current%2520state-of-the-art%2520%2528SOTA%2529%250Adefense%2520methods%2520by%2520simultaneously%2520improving%2520clean%2520and%2520robust%2520accuracy%2520on%250ACIFAR-10%2520and%2520ImageNet-1K%2520against%2520adaptive%2520white-box%2520attacks.%2520Codes%2520are%2520publicly%250Aavailable%2520at%253A%2520https%253A//github.com/tmlr-group/DAD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.02169v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20Stone%2C%20Two%20Birds%3A%20Enhancing%20Adversarial%20Defense%20Through%20the%20Lens%20of%0A%20%20Distributional%20Discrepancy&entry.906535625=Jiacheng%20Zhang%20and%20Benjamin%20I.%20P.%20Rubinstein%20and%20Jingfeng%20Zhang%20and%20Feng%20Liu&entry.1292438233=%20%20Statistical%20adversarial%20data%20detection%20%28SADD%29%20detects%20whether%20an%20upcoming%0Abatch%20contains%20adversarial%20examples%20%28AEs%29%20by%20measuring%20the%20distributional%0Adiscrepancies%20between%20clean%20examples%20%28CEs%29%20and%20AEs.%20In%20this%20paper%2C%20we%20explore%0Athe%20strength%20of%20SADD-based%20methods%20by%20theoretically%20showing%20that%20minimizing%0Adistributional%20discrepancy%20can%20help%20reduce%20the%20expected%20loss%20on%20AEs.%20Despite%0Athese%20advantages%2C%20SADD-based%20methods%20have%20a%20potential%20limitation%3A%20they%20discard%0Ainputs%20that%20are%20detected%20as%20AEs%2C%20leading%20to%20the%20loss%20of%20useful%20information%0Awithin%20those%20inputs.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20two-pronged%0Aadversarial%20defense%20method%2C%20named%20Distributional-discrepancy-based%20Adversarial%0ADefense%20%28DAD%29.%20In%20the%20training%20phase%2C%20DAD%20first%20optimizes%20the%20test%20power%20of%20the%0Amaximum%20mean%20discrepancy%20%28MMD%29%20to%20derive%20MMD-OPT%2C%20which%20is%20a%20stone%20that%20kills%0Atwo%20birds.%20MMD-OPT%20first%20serves%20as%20a%20guiding%20signal%20to%20minimize%20the%0Adistributional%20discrepancy%20between%20CEs%20and%20AEs%20to%20train%20a%20denoiser.%20Then%2C%20it%0Aserves%20as%20a%20discriminator%20to%20differentiate%20CEs%20and%20AEs%20during%20inference.%0AOverall%2C%20in%20the%20inference%20stage%2C%20DAD%20consists%20of%20a%20two-pronged%20process%3A%20%281%29%0Adirectly%20feeding%20the%20detected%20CEs%20into%20the%20classifier%2C%20and%20%282%29%20removing%20noise%0Afrom%20the%20detected%20AEs%20by%20the%20distributional-discrepancy-based%20denoiser.%0AExtensive%20experiments%20show%20that%20DAD%20outperforms%20current%20state-of-the-art%20%28SOTA%29%0Adefense%20methods%20by%20simultaneously%20improving%20clean%20and%20robust%20accuracy%20on%0ACIFAR-10%20and%20ImageNet-1K%20against%20adaptive%20white-box%20attacks.%20Codes%20are%20publicly%0Aavailable%20at%3A%20https%3A//github.com/tmlr-group/DAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.02169v2&entry.124074799=Read"},
{"title": "Laplace Transform Based Low-Complexity Learning of Continuous Markov\n  Semigroups", "author": "Vladimir R. Kostic and Karim Lounici and H\u00e9l\u00e8ne Halconruy and Timoth\u00e9e Devergne and Pietro Novelli and Massimiliano Pontil", "abstract": "  Markov processes serve as a universal model for many real-world random\nprocesses. This paper presents a data-driven approach for learning these models\nthrough the spectral decomposition of the infinitesimal generator (IG) of the\nMarkov semigroup. The unbounded nature of IGs complicates traditional methods\nsuch as vector-valued regression and Hilbert-Schmidt operator analysis.\nExisting techniques, including physics-informed kernel regression, are\ncomputationally expensive and limited in scope, with no recovery guarantees for\ntransfer operator methods when the time-lag is small. We propose a novel method\nthat leverages the IG's resolvent, characterized by the Laplace transform of\ntransfer operators. This approach is robust to time-lag variations, ensuring\naccurate eigenvalue learning even for small time-lags. Our statistical analysis\napplies to a broader class of Markov processes than current methods while\nreducing computational complexity from quadratic to linear in the state\ndimension. Finally, we illustrate the behaviour of our method in two\nexperiments.\n", "link": "http://arxiv.org/abs/2410.14477v2", "date": "2025-06-06", "relevancy": 1.3927, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.495}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4601}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Laplace%20Transform%20Based%20Low-Complexity%20Learning%20of%20Continuous%20Markov%0A%20%20Semigroups&body=Title%3A%20Laplace%20Transform%20Based%20Low-Complexity%20Learning%20of%20Continuous%20Markov%0A%20%20Semigroups%0AAuthor%3A%20Vladimir%20R.%20Kostic%20and%20Karim%20Lounici%20and%20H%C3%A9l%C3%A8ne%20Halconruy%20and%20Timoth%C3%A9e%20Devergne%20and%20Pietro%20Novelli%20and%20Massimiliano%20Pontil%0AAbstract%3A%20%20%20Markov%20processes%20serve%20as%20a%20universal%20model%20for%20many%20real-world%20random%0Aprocesses.%20This%20paper%20presents%20a%20data-driven%20approach%20for%20learning%20these%20models%0Athrough%20the%20spectral%20decomposition%20of%20the%20infinitesimal%20generator%20%28IG%29%20of%20the%0AMarkov%20semigroup.%20The%20unbounded%20nature%20of%20IGs%20complicates%20traditional%20methods%0Asuch%20as%20vector-valued%20regression%20and%20Hilbert-Schmidt%20operator%20analysis.%0AExisting%20techniques%2C%20including%20physics-informed%20kernel%20regression%2C%20are%0Acomputationally%20expensive%20and%20limited%20in%20scope%2C%20with%20no%20recovery%20guarantees%20for%0Atransfer%20operator%20methods%20when%20the%20time-lag%20is%20small.%20We%20propose%20a%20novel%20method%0Athat%20leverages%20the%20IG%27s%20resolvent%2C%20characterized%20by%20the%20Laplace%20transform%20of%0Atransfer%20operators.%20This%20approach%20is%20robust%20to%20time-lag%20variations%2C%20ensuring%0Aaccurate%20eigenvalue%20learning%20even%20for%20small%20time-lags.%20Our%20statistical%20analysis%0Aapplies%20to%20a%20broader%20class%20of%20Markov%20processes%20than%20current%20methods%20while%0Areducing%20computational%20complexity%20from%20quadratic%20to%20linear%20in%20the%20state%0Adimension.%20Finally%2C%20we%20illustrate%20the%20behaviour%20of%20our%20method%20in%20two%0Aexperiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14477v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaplace%2520Transform%2520Based%2520Low-Complexity%2520Learning%2520of%2520Continuous%2520Markov%250A%2520%2520Semigroups%26entry.906535625%3DVladimir%2520R.%2520Kostic%2520and%2520Karim%2520Lounici%2520and%2520H%25C3%25A9l%25C3%25A8ne%2520Halconruy%2520and%2520Timoth%25C3%25A9e%2520Devergne%2520and%2520Pietro%2520Novelli%2520and%2520Massimiliano%2520Pontil%26entry.1292438233%3D%2520%2520Markov%2520processes%2520serve%2520as%2520a%2520universal%2520model%2520for%2520many%2520real-world%2520random%250Aprocesses.%2520This%2520paper%2520presents%2520a%2520data-driven%2520approach%2520for%2520learning%2520these%2520models%250Athrough%2520the%2520spectral%2520decomposition%2520of%2520the%2520infinitesimal%2520generator%2520%2528IG%2529%2520of%2520the%250AMarkov%2520semigroup.%2520The%2520unbounded%2520nature%2520of%2520IGs%2520complicates%2520traditional%2520methods%250Asuch%2520as%2520vector-valued%2520regression%2520and%2520Hilbert-Schmidt%2520operator%2520analysis.%250AExisting%2520techniques%252C%2520including%2520physics-informed%2520kernel%2520regression%252C%2520are%250Acomputationally%2520expensive%2520and%2520limited%2520in%2520scope%252C%2520with%2520no%2520recovery%2520guarantees%2520for%250Atransfer%2520operator%2520methods%2520when%2520the%2520time-lag%2520is%2520small.%2520We%2520propose%2520a%2520novel%2520method%250Athat%2520leverages%2520the%2520IG%2527s%2520resolvent%252C%2520characterized%2520by%2520the%2520Laplace%2520transform%2520of%250Atransfer%2520operators.%2520This%2520approach%2520is%2520robust%2520to%2520time-lag%2520variations%252C%2520ensuring%250Aaccurate%2520eigenvalue%2520learning%2520even%2520for%2520small%2520time-lags.%2520Our%2520statistical%2520analysis%250Aapplies%2520to%2520a%2520broader%2520class%2520of%2520Markov%2520processes%2520than%2520current%2520methods%2520while%250Areducing%2520computational%2520complexity%2520from%2520quadratic%2520to%2520linear%2520in%2520the%2520state%250Adimension.%2520Finally%252C%2520we%2520illustrate%2520the%2520behaviour%2520of%2520our%2520method%2520in%2520two%250Aexperiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14477v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Laplace%20Transform%20Based%20Low-Complexity%20Learning%20of%20Continuous%20Markov%0A%20%20Semigroups&entry.906535625=Vladimir%20R.%20Kostic%20and%20Karim%20Lounici%20and%20H%C3%A9l%C3%A8ne%20Halconruy%20and%20Timoth%C3%A9e%20Devergne%20and%20Pietro%20Novelli%20and%20Massimiliano%20Pontil&entry.1292438233=%20%20Markov%20processes%20serve%20as%20a%20universal%20model%20for%20many%20real-world%20random%0Aprocesses.%20This%20paper%20presents%20a%20data-driven%20approach%20for%20learning%20these%20models%0Athrough%20the%20spectral%20decomposition%20of%20the%20infinitesimal%20generator%20%28IG%29%20of%20the%0AMarkov%20semigroup.%20The%20unbounded%20nature%20of%20IGs%20complicates%20traditional%20methods%0Asuch%20as%20vector-valued%20regression%20and%20Hilbert-Schmidt%20operator%20analysis.%0AExisting%20techniques%2C%20including%20physics-informed%20kernel%20regression%2C%20are%0Acomputationally%20expensive%20and%20limited%20in%20scope%2C%20with%20no%20recovery%20guarantees%20for%0Atransfer%20operator%20methods%20when%20the%20time-lag%20is%20small.%20We%20propose%20a%20novel%20method%0Athat%20leverages%20the%20IG%27s%20resolvent%2C%20characterized%20by%20the%20Laplace%20transform%20of%0Atransfer%20operators.%20This%20approach%20is%20robust%20to%20time-lag%20variations%2C%20ensuring%0Aaccurate%20eigenvalue%20learning%20even%20for%20small%20time-lags.%20Our%20statistical%20analysis%0Aapplies%20to%20a%20broader%20class%20of%20Markov%20processes%20than%20current%20methods%20while%0Areducing%20computational%20complexity%20from%20quadratic%20to%20linear%20in%20the%20state%0Adimension.%20Finally%2C%20we%20illustrate%20the%20behaviour%20of%20our%20method%20in%20two%0Aexperiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14477v2&entry.124074799=Read"},
{"title": "On-board Mission Replanning for Adaptive Cooperative Multi-Robot Systems", "author": "Elim Kwan and Rehman Qureshi and Liam Fletcher and Colin Laganier and Victoria Nockles and Richard Walters", "abstract": "  Cooperative autonomous robotic systems have significant potential for\nexecuting complex multi-task missions across space, air, ground, and maritime\ndomains. But they commonly operate in remote, dynamic and hazardous\nenvironments, requiring rapid in-mission adaptation without reliance on fragile\nor slow communication links to centralised compute. Fast, on-board replanning\nalgorithms are therefore needed to enhance resilience. Reinforcement Learning\nshows strong promise for efficiently solving mission planning tasks when\nformulated as Travelling Salesperson Problems (TSPs), but existing methods: 1)\nare unsuitable for replanning, where agents do not start at a single location;\n2) do not allow cooperation between agents; 3) are unable to model tasks with\nvariable durations; or 4) lack practical considerations for on-board\ndeployment. Here we define the Cooperative Mission Replanning Problem as a\nnovel variant of multiple TSP with adaptations to overcome these issues, and\ndevelop a new encoder/decoder-based model using Graph Attention Networks and\nAttention Models to solve it effectively and efficiently. Using a simple\nexample of cooperative drones, we show our replanner consistently (90% of the\ntime) maintains performance within 10% of the state-of-the-art LKH3 heuristic\nsolver, whilst running 85-370 times faster on a Raspberry Pi. This work paves\nthe way for increased resilience in autonomous multi-agent systems.\n", "link": "http://arxiv.org/abs/2506.06094v1", "date": "2025-06-06", "relevancy": 1.6288, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5704}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5665}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On-board%20Mission%20Replanning%20for%20Adaptive%20Cooperative%20Multi-Robot%20Systems&body=Title%3A%20On-board%20Mission%20Replanning%20for%20Adaptive%20Cooperative%20Multi-Robot%20Systems%0AAuthor%3A%20Elim%20Kwan%20and%20Rehman%20Qureshi%20and%20Liam%20Fletcher%20and%20Colin%20Laganier%20and%20Victoria%20Nockles%20and%20Richard%20Walters%0AAbstract%3A%20%20%20Cooperative%20autonomous%20robotic%20systems%20have%20significant%20potential%20for%0Aexecuting%20complex%20multi-task%20missions%20across%20space%2C%20air%2C%20ground%2C%20and%20maritime%0Adomains.%20But%20they%20commonly%20operate%20in%20remote%2C%20dynamic%20and%20hazardous%0Aenvironments%2C%20requiring%20rapid%20in-mission%20adaptation%20without%20reliance%20on%20fragile%0Aor%20slow%20communication%20links%20to%20centralised%20compute.%20Fast%2C%20on-board%20replanning%0Aalgorithms%20are%20therefore%20needed%20to%20enhance%20resilience.%20Reinforcement%20Learning%0Ashows%20strong%20promise%20for%20efficiently%20solving%20mission%20planning%20tasks%20when%0Aformulated%20as%20Travelling%20Salesperson%20Problems%20%28TSPs%29%2C%20but%20existing%20methods%3A%201%29%0Aare%20unsuitable%20for%20replanning%2C%20where%20agents%20do%20not%20start%20at%20a%20single%20location%3B%0A2%29%20do%20not%20allow%20cooperation%20between%20agents%3B%203%29%20are%20unable%20to%20model%20tasks%20with%0Avariable%20durations%3B%20or%204%29%20lack%20practical%20considerations%20for%20on-board%0Adeployment.%20Here%20we%20define%20the%20Cooperative%20Mission%20Replanning%20Problem%20as%20a%0Anovel%20variant%20of%20multiple%20TSP%20with%20adaptations%20to%20overcome%20these%20issues%2C%20and%0Adevelop%20a%20new%20encoder/decoder-based%20model%20using%20Graph%20Attention%20Networks%20and%0AAttention%20Models%20to%20solve%20it%20effectively%20and%20efficiently.%20Using%20a%20simple%0Aexample%20of%20cooperative%20drones%2C%20we%20show%20our%20replanner%20consistently%20%2890%25%20of%20the%0Atime%29%20maintains%20performance%20within%2010%25%20of%20the%20state-of-the-art%20LKH3%20heuristic%0Asolver%2C%20whilst%20running%2085-370%20times%20faster%20on%20a%20Raspberry%20Pi.%20This%20work%20paves%0Athe%20way%20for%20increased%20resilience%20in%20autonomous%20multi-agent%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06094v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn-board%2520Mission%2520Replanning%2520for%2520Adaptive%2520Cooperative%2520Multi-Robot%2520Systems%26entry.906535625%3DElim%2520Kwan%2520and%2520Rehman%2520Qureshi%2520and%2520Liam%2520Fletcher%2520and%2520Colin%2520Laganier%2520and%2520Victoria%2520Nockles%2520and%2520Richard%2520Walters%26entry.1292438233%3D%2520%2520Cooperative%2520autonomous%2520robotic%2520systems%2520have%2520significant%2520potential%2520for%250Aexecuting%2520complex%2520multi-task%2520missions%2520across%2520space%252C%2520air%252C%2520ground%252C%2520and%2520maritime%250Adomains.%2520But%2520they%2520commonly%2520operate%2520in%2520remote%252C%2520dynamic%2520and%2520hazardous%250Aenvironments%252C%2520requiring%2520rapid%2520in-mission%2520adaptation%2520without%2520reliance%2520on%2520fragile%250Aor%2520slow%2520communication%2520links%2520to%2520centralised%2520compute.%2520Fast%252C%2520on-board%2520replanning%250Aalgorithms%2520are%2520therefore%2520needed%2520to%2520enhance%2520resilience.%2520Reinforcement%2520Learning%250Ashows%2520strong%2520promise%2520for%2520efficiently%2520solving%2520mission%2520planning%2520tasks%2520when%250Aformulated%2520as%2520Travelling%2520Salesperson%2520Problems%2520%2528TSPs%2529%252C%2520but%2520existing%2520methods%253A%25201%2529%250Aare%2520unsuitable%2520for%2520replanning%252C%2520where%2520agents%2520do%2520not%2520start%2520at%2520a%2520single%2520location%253B%250A2%2529%2520do%2520not%2520allow%2520cooperation%2520between%2520agents%253B%25203%2529%2520are%2520unable%2520to%2520model%2520tasks%2520with%250Avariable%2520durations%253B%2520or%25204%2529%2520lack%2520practical%2520considerations%2520for%2520on-board%250Adeployment.%2520Here%2520we%2520define%2520the%2520Cooperative%2520Mission%2520Replanning%2520Problem%2520as%2520a%250Anovel%2520variant%2520of%2520multiple%2520TSP%2520with%2520adaptations%2520to%2520overcome%2520these%2520issues%252C%2520and%250Adevelop%2520a%2520new%2520encoder/decoder-based%2520model%2520using%2520Graph%2520Attention%2520Networks%2520and%250AAttention%2520Models%2520to%2520solve%2520it%2520effectively%2520and%2520efficiently.%2520Using%2520a%2520simple%250Aexample%2520of%2520cooperative%2520drones%252C%2520we%2520show%2520our%2520replanner%2520consistently%2520%252890%2525%2520of%2520the%250Atime%2529%2520maintains%2520performance%2520within%252010%2525%2520of%2520the%2520state-of-the-art%2520LKH3%2520heuristic%250Asolver%252C%2520whilst%2520running%252085-370%2520times%2520faster%2520on%2520a%2520Raspberry%2520Pi.%2520This%2520work%2520paves%250Athe%2520way%2520for%2520increased%2520resilience%2520in%2520autonomous%2520multi-agent%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06094v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On-board%20Mission%20Replanning%20for%20Adaptive%20Cooperative%20Multi-Robot%20Systems&entry.906535625=Elim%20Kwan%20and%20Rehman%20Qureshi%20and%20Liam%20Fletcher%20and%20Colin%20Laganier%20and%20Victoria%20Nockles%20and%20Richard%20Walters&entry.1292438233=%20%20Cooperative%20autonomous%20robotic%20systems%20have%20significant%20potential%20for%0Aexecuting%20complex%20multi-task%20missions%20across%20space%2C%20air%2C%20ground%2C%20and%20maritime%0Adomains.%20But%20they%20commonly%20operate%20in%20remote%2C%20dynamic%20and%20hazardous%0Aenvironments%2C%20requiring%20rapid%20in-mission%20adaptation%20without%20reliance%20on%20fragile%0Aor%20slow%20communication%20links%20to%20centralised%20compute.%20Fast%2C%20on-board%20replanning%0Aalgorithms%20are%20therefore%20needed%20to%20enhance%20resilience.%20Reinforcement%20Learning%0Ashows%20strong%20promise%20for%20efficiently%20solving%20mission%20planning%20tasks%20when%0Aformulated%20as%20Travelling%20Salesperson%20Problems%20%28TSPs%29%2C%20but%20existing%20methods%3A%201%29%0Aare%20unsuitable%20for%20replanning%2C%20where%20agents%20do%20not%20start%20at%20a%20single%20location%3B%0A2%29%20do%20not%20allow%20cooperation%20between%20agents%3B%203%29%20are%20unable%20to%20model%20tasks%20with%0Avariable%20durations%3B%20or%204%29%20lack%20practical%20considerations%20for%20on-board%0Adeployment.%20Here%20we%20define%20the%20Cooperative%20Mission%20Replanning%20Problem%20as%20a%0Anovel%20variant%20of%20multiple%20TSP%20with%20adaptations%20to%20overcome%20these%20issues%2C%20and%0Adevelop%20a%20new%20encoder/decoder-based%20model%20using%20Graph%20Attention%20Networks%20and%0AAttention%20Models%20to%20solve%20it%20effectively%20and%20efficiently.%20Using%20a%20simple%0Aexample%20of%20cooperative%20drones%2C%20we%20show%20our%20replanner%20consistently%20%2890%25%20of%20the%0Atime%29%20maintains%20performance%20within%2010%25%20of%20the%20state-of-the-art%20LKH3%20heuristic%0Asolver%2C%20whilst%20running%2085-370%20times%20faster%20on%20a%20Raspberry%20Pi.%20This%20work%20paves%0Athe%20way%20for%20increased%20resilience%20in%20autonomous%20multi-agent%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06094v1&entry.124074799=Read"},
{"title": "Certification for Differentially Private Prediction in Gradient-Based\n  Training", "author": "Matthew Wicker and Philip Sosnin and Igor Shilov and Adrianna Janik and Mark N. M\u00fcller and Yves-Alexandre de Montjoye and Adrian Weller and Calvin Tsay", "abstract": "  We study private prediction where differential privacy is achieved by adding\nnoise to the outputs of a non-private model. Existing methods rely on noise\nproportional to the global sensitivity of the model, often resulting in\nsub-optimal privacy-utility trade-offs compared to private training. We\nintroduce a novel approach for computing dataset-specific upper bounds on\nprediction sensitivity by leveraging convex relaxation and bound propagation\ntechniques. By combining these bounds with the smooth sensitivity mechanism, we\nsignificantly improve the privacy analysis of private prediction compared to\nglobal sensitivity-based approaches. Experimental results across real-world\ndatasets in medical image classification and natural language processing\ndemonstrate that our sensitivity bounds are can be orders of magnitude tighter\nthan global sensitivity. Our approach provides a strong basis for the\ndevelopment of novel privacy preserving technologies.\n", "link": "http://arxiv.org/abs/2406.13433v3", "date": "2025-06-06", "relevancy": 1.434, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4892}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4708}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4573}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Certification%20for%20Differentially%20Private%20Prediction%20in%20Gradient-Based%0A%20%20Training&body=Title%3A%20Certification%20for%20Differentially%20Private%20Prediction%20in%20Gradient-Based%0A%20%20Training%0AAuthor%3A%20Matthew%20Wicker%20and%20Philip%20Sosnin%20and%20Igor%20Shilov%20and%20Adrianna%20Janik%20and%20Mark%20N.%20M%C3%BCller%20and%20Yves-Alexandre%20de%20Montjoye%20and%20Adrian%20Weller%20and%20Calvin%20Tsay%0AAbstract%3A%20%20%20We%20study%20private%20prediction%20where%20differential%20privacy%20is%20achieved%20by%20adding%0Anoise%20to%20the%20outputs%20of%20a%20non-private%20model.%20Existing%20methods%20rely%20on%20noise%0Aproportional%20to%20the%20global%20sensitivity%20of%20the%20model%2C%20often%20resulting%20in%0Asub-optimal%20privacy-utility%20trade-offs%20compared%20to%20private%20training.%20We%0Aintroduce%20a%20novel%20approach%20for%20computing%20dataset-specific%20upper%20bounds%20on%0Aprediction%20sensitivity%20by%20leveraging%20convex%20relaxation%20and%20bound%20propagation%0Atechniques.%20By%20combining%20these%20bounds%20with%20the%20smooth%20sensitivity%20mechanism%2C%20we%0Asignificantly%20improve%20the%20privacy%20analysis%20of%20private%20prediction%20compared%20to%0Aglobal%20sensitivity-based%20approaches.%20Experimental%20results%20across%20real-world%0Adatasets%20in%20medical%20image%20classification%20and%20natural%20language%20processing%0Ademonstrate%20that%20our%20sensitivity%20bounds%20are%20can%20be%20orders%20of%20magnitude%20tighter%0Athan%20global%20sensitivity.%20Our%20approach%20provides%20a%20strong%20basis%20for%20the%0Adevelopment%20of%20novel%20privacy%20preserving%20technologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13433v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCertification%2520for%2520Differentially%2520Private%2520Prediction%2520in%2520Gradient-Based%250A%2520%2520Training%26entry.906535625%3DMatthew%2520Wicker%2520and%2520Philip%2520Sosnin%2520and%2520Igor%2520Shilov%2520and%2520Adrianna%2520Janik%2520and%2520Mark%2520N.%2520M%25C3%25BCller%2520and%2520Yves-Alexandre%2520de%2520Montjoye%2520and%2520Adrian%2520Weller%2520and%2520Calvin%2520Tsay%26entry.1292438233%3D%2520%2520We%2520study%2520private%2520prediction%2520where%2520differential%2520privacy%2520is%2520achieved%2520by%2520adding%250Anoise%2520to%2520the%2520outputs%2520of%2520a%2520non-private%2520model.%2520Existing%2520methods%2520rely%2520on%2520noise%250Aproportional%2520to%2520the%2520global%2520sensitivity%2520of%2520the%2520model%252C%2520often%2520resulting%2520in%250Asub-optimal%2520privacy-utility%2520trade-offs%2520compared%2520to%2520private%2520training.%2520We%250Aintroduce%2520a%2520novel%2520approach%2520for%2520computing%2520dataset-specific%2520upper%2520bounds%2520on%250Aprediction%2520sensitivity%2520by%2520leveraging%2520convex%2520relaxation%2520and%2520bound%2520propagation%250Atechniques.%2520By%2520combining%2520these%2520bounds%2520with%2520the%2520smooth%2520sensitivity%2520mechanism%252C%2520we%250Asignificantly%2520improve%2520the%2520privacy%2520analysis%2520of%2520private%2520prediction%2520compared%2520to%250Aglobal%2520sensitivity-based%2520approaches.%2520Experimental%2520results%2520across%2520real-world%250Adatasets%2520in%2520medical%2520image%2520classification%2520and%2520natural%2520language%2520processing%250Ademonstrate%2520that%2520our%2520sensitivity%2520bounds%2520are%2520can%2520be%2520orders%2520of%2520magnitude%2520tighter%250Athan%2520global%2520sensitivity.%2520Our%2520approach%2520provides%2520a%2520strong%2520basis%2520for%2520the%250Adevelopment%2520of%2520novel%2520privacy%2520preserving%2520technologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13433v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Certification%20for%20Differentially%20Private%20Prediction%20in%20Gradient-Based%0A%20%20Training&entry.906535625=Matthew%20Wicker%20and%20Philip%20Sosnin%20and%20Igor%20Shilov%20and%20Adrianna%20Janik%20and%20Mark%20N.%20M%C3%BCller%20and%20Yves-Alexandre%20de%20Montjoye%20and%20Adrian%20Weller%20and%20Calvin%20Tsay&entry.1292438233=%20%20We%20study%20private%20prediction%20where%20differential%20privacy%20is%20achieved%20by%20adding%0Anoise%20to%20the%20outputs%20of%20a%20non-private%20model.%20Existing%20methods%20rely%20on%20noise%0Aproportional%20to%20the%20global%20sensitivity%20of%20the%20model%2C%20often%20resulting%20in%0Asub-optimal%20privacy-utility%20trade-offs%20compared%20to%20private%20training.%20We%0Aintroduce%20a%20novel%20approach%20for%20computing%20dataset-specific%20upper%20bounds%20on%0Aprediction%20sensitivity%20by%20leveraging%20convex%20relaxation%20and%20bound%20propagation%0Atechniques.%20By%20combining%20these%20bounds%20with%20the%20smooth%20sensitivity%20mechanism%2C%20we%0Asignificantly%20improve%20the%20privacy%20analysis%20of%20private%20prediction%20compared%20to%0Aglobal%20sensitivity-based%20approaches.%20Experimental%20results%20across%20real-world%0Adatasets%20in%20medical%20image%20classification%20and%20natural%20language%20processing%0Ademonstrate%20that%20our%20sensitivity%20bounds%20are%20can%20be%20orders%20of%20magnitude%20tighter%0Athan%20global%20sensitivity.%20Our%20approach%20provides%20a%20strong%20basis%20for%20the%0Adevelopment%20of%20novel%20privacy%20preserving%20technologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13433v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


