<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241117.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Towards High-Fidelity 3D Portrait Generation with Rich Details by\n  Cross-View Prior-Aware Diffusion", "author": "Haoran Wei and Wencheng Han and Xingping Dong and Jianbing Shen", "abstract": "  Recent diffusion-based Single-image 3D portrait generation methods typically\nemploy 2D diffusion models to provide multi-view knowledge, which is then\ndistilled into 3D representations. However, these methods usually struggle to\nproduce high-fidelity 3D models, frequently yielding excessively blurred\ntextures. We attribute this issue to the insufficient consideration of\ncross-view consistency during the diffusion process, resulting in significant\ndisparities between different views and ultimately leading to blurred 3D\nrepresentations. In this paper, we address this issue by comprehensively\nexploiting multi-view priors in both the conditioning and diffusion procedures\nto produce consistent, detail-rich portraits. From the conditioning standpoint,\nwe propose a Hybrid Priors Diffsion model, which explicitly and implicitly\nincorporates multi-view priors as conditions to enhance the status consistency\nof the generated multi-view portraits. From the diffusion perspective,\nconsidering the significant impact of the diffusion noise distribution on\ndetailed texture generation, we propose a Multi-View Noise Resamplig Strategy\nintegrated within the optimization process leveraging cross-view priors to\nenhance representation consistency. Extensive experiments demonstrate that our\nmethod can produce 3D portraits with accurate geometry and rich details from a\nsingle image. The project page is at\n\\url{https://haoran-wei.github.io/Portrait-Diffusion}.\n", "link": "http://arxiv.org/abs/2411.10369v1", "date": "2024-11-15", "relevancy": 3.3554, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6879}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6879}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20High-Fidelity%203D%20Portrait%20Generation%20with%20Rich%20Details%20by%0A%20%20Cross-View%20Prior-Aware%20Diffusion&body=Title%3A%20Towards%20High-Fidelity%203D%20Portrait%20Generation%20with%20Rich%20Details%20by%0A%20%20Cross-View%20Prior-Aware%20Diffusion%0AAuthor%3A%20Haoran%20Wei%20and%20Wencheng%20Han%20and%20Xingping%20Dong%20and%20Jianbing%20Shen%0AAbstract%3A%20%20%20Recent%20diffusion-based%20Single-image%203D%20portrait%20generation%20methods%20typically%0Aemploy%202D%20diffusion%20models%20to%20provide%20multi-view%20knowledge%2C%20which%20is%20then%0Adistilled%20into%203D%20representations.%20However%2C%20these%20methods%20usually%20struggle%20to%0Aproduce%20high-fidelity%203D%20models%2C%20frequently%20yielding%20excessively%20blurred%0Atextures.%20We%20attribute%20this%20issue%20to%20the%20insufficient%20consideration%20of%0Across-view%20consistency%20during%20the%20diffusion%20process%2C%20resulting%20in%20significant%0Adisparities%20between%20different%20views%20and%20ultimately%20leading%20to%20blurred%203D%0Arepresentations.%20In%20this%20paper%2C%20we%20address%20this%20issue%20by%20comprehensively%0Aexploiting%20multi-view%20priors%20in%20both%20the%20conditioning%20and%20diffusion%20procedures%0Ato%20produce%20consistent%2C%20detail-rich%20portraits.%20From%20the%20conditioning%20standpoint%2C%0Awe%20propose%20a%20Hybrid%20Priors%20Diffsion%20model%2C%20which%20explicitly%20and%20implicitly%0Aincorporates%20multi-view%20priors%20as%20conditions%20to%20enhance%20the%20status%20consistency%0Aof%20the%20generated%20multi-view%20portraits.%20From%20the%20diffusion%20perspective%2C%0Aconsidering%20the%20significant%20impact%20of%20the%20diffusion%20noise%20distribution%20on%0Adetailed%20texture%20generation%2C%20we%20propose%20a%20Multi-View%20Noise%20Resamplig%20Strategy%0Aintegrated%20within%20the%20optimization%20process%20leveraging%20cross-view%20priors%20to%0Aenhance%20representation%20consistency.%20Extensive%20experiments%20demonstrate%20that%20our%0Amethod%20can%20produce%203D%20portraits%20with%20accurate%20geometry%20and%20rich%20details%20from%20a%0Asingle%20image.%20The%20project%20page%20is%20at%0A%5Curl%7Bhttps%3A//haoran-wei.github.io/Portrait-Diffusion%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10369v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520High-Fidelity%25203D%2520Portrait%2520Generation%2520with%2520Rich%2520Details%2520by%250A%2520%2520Cross-View%2520Prior-Aware%2520Diffusion%26entry.906535625%3DHaoran%2520Wei%2520and%2520Wencheng%2520Han%2520and%2520Xingping%2520Dong%2520and%2520Jianbing%2520Shen%26entry.1292438233%3D%2520%2520Recent%2520diffusion-based%2520Single-image%25203D%2520portrait%2520generation%2520methods%2520typically%250Aemploy%25202D%2520diffusion%2520models%2520to%2520provide%2520multi-view%2520knowledge%252C%2520which%2520is%2520then%250Adistilled%2520into%25203D%2520representations.%2520However%252C%2520these%2520methods%2520usually%2520struggle%2520to%250Aproduce%2520high-fidelity%25203D%2520models%252C%2520frequently%2520yielding%2520excessively%2520blurred%250Atextures.%2520We%2520attribute%2520this%2520issue%2520to%2520the%2520insufficient%2520consideration%2520of%250Across-view%2520consistency%2520during%2520the%2520diffusion%2520process%252C%2520resulting%2520in%2520significant%250Adisparities%2520between%2520different%2520views%2520and%2520ultimately%2520leading%2520to%2520blurred%25203D%250Arepresentations.%2520In%2520this%2520paper%252C%2520we%2520address%2520this%2520issue%2520by%2520comprehensively%250Aexploiting%2520multi-view%2520priors%2520in%2520both%2520the%2520conditioning%2520and%2520diffusion%2520procedures%250Ato%2520produce%2520consistent%252C%2520detail-rich%2520portraits.%2520From%2520the%2520conditioning%2520standpoint%252C%250Awe%2520propose%2520a%2520Hybrid%2520Priors%2520Diffsion%2520model%252C%2520which%2520explicitly%2520and%2520implicitly%250Aincorporates%2520multi-view%2520priors%2520as%2520conditions%2520to%2520enhance%2520the%2520status%2520consistency%250Aof%2520the%2520generated%2520multi-view%2520portraits.%2520From%2520the%2520diffusion%2520perspective%252C%250Aconsidering%2520the%2520significant%2520impact%2520of%2520the%2520diffusion%2520noise%2520distribution%2520on%250Adetailed%2520texture%2520generation%252C%2520we%2520propose%2520a%2520Multi-View%2520Noise%2520Resamplig%2520Strategy%250Aintegrated%2520within%2520the%2520optimization%2520process%2520leveraging%2520cross-view%2520priors%2520to%250Aenhance%2520representation%2520consistency.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250Amethod%2520can%2520produce%25203D%2520portraits%2520with%2520accurate%2520geometry%2520and%2520rich%2520details%2520from%2520a%250Asingle%2520image.%2520The%2520project%2520page%2520is%2520at%250A%255Curl%257Bhttps%253A//haoran-wei.github.io/Portrait-Diffusion%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10369v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20High-Fidelity%203D%20Portrait%20Generation%20with%20Rich%20Details%20by%0A%20%20Cross-View%20Prior-Aware%20Diffusion&entry.906535625=Haoran%20Wei%20and%20Wencheng%20Han%20and%20Xingping%20Dong%20and%20Jianbing%20Shen&entry.1292438233=%20%20Recent%20diffusion-based%20Single-image%203D%20portrait%20generation%20methods%20typically%0Aemploy%202D%20diffusion%20models%20to%20provide%20multi-view%20knowledge%2C%20which%20is%20then%0Adistilled%20into%203D%20representations.%20However%2C%20these%20methods%20usually%20struggle%20to%0Aproduce%20high-fidelity%203D%20models%2C%20frequently%20yielding%20excessively%20blurred%0Atextures.%20We%20attribute%20this%20issue%20to%20the%20insufficient%20consideration%20of%0Across-view%20consistency%20during%20the%20diffusion%20process%2C%20resulting%20in%20significant%0Adisparities%20between%20different%20views%20and%20ultimately%20leading%20to%20blurred%203D%0Arepresentations.%20In%20this%20paper%2C%20we%20address%20this%20issue%20by%20comprehensively%0Aexploiting%20multi-view%20priors%20in%20both%20the%20conditioning%20and%20diffusion%20procedures%0Ato%20produce%20consistent%2C%20detail-rich%20portraits.%20From%20the%20conditioning%20standpoint%2C%0Awe%20propose%20a%20Hybrid%20Priors%20Diffsion%20model%2C%20which%20explicitly%20and%20implicitly%0Aincorporates%20multi-view%20priors%20as%20conditions%20to%20enhance%20the%20status%20consistency%0Aof%20the%20generated%20multi-view%20portraits.%20From%20the%20diffusion%20perspective%2C%0Aconsidering%20the%20significant%20impact%20of%20the%20diffusion%20noise%20distribution%20on%0Adetailed%20texture%20generation%2C%20we%20propose%20a%20Multi-View%20Noise%20Resamplig%20Strategy%0Aintegrated%20within%20the%20optimization%20process%20leveraging%20cross-view%20priors%20to%0Aenhance%20representation%20consistency.%20Extensive%20experiments%20demonstrate%20that%20our%0Amethod%20can%20produce%203D%20portraits%20with%20accurate%20geometry%20and%20rich%20details%20from%20a%0Asingle%20image.%20The%20project%20page%20is%20at%0A%5Curl%7Bhttps%3A//haoran-wei.github.io/Portrait-Diffusion%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10369v1&entry.124074799=Read"},
{"title": "4DPV: 4D Pet from Videos by Coarse-to-Fine Non-Rigid Radiance Fields", "author": "Sergio M. de Paco and Antonio Agudo", "abstract": "  We present a coarse-to-fine neural deformation model to simultaneously\nrecover the camera pose and the 4D reconstruction of an unknown object from\nmultiple RGB sequences in the wild. To that end, our approach does not consider\nany pre-built 3D template nor 3D training data as well as controlled\nillumination conditions, and can sort out the problem in a self-supervised\nmanner. Our model exploits canonical and image-variant spaces where both coarse\nand fine components are considered. We introduce a neural local quadratic model\nwith spatio-temporal consistency to encode fine details that is combined with\ncanonical embeddings in order to establish correspondences across sequences. We\nthoroughly validate the method on challenging scenarios with complex and\nreal-world deformations, providing both quantitative and qualitative\nevaluations, an ablation study and a comparison with respect to competing\napproaches. Our project is available at https://github.com/smontode24/4DPV.\n", "link": "http://arxiv.org/abs/2411.10275v1", "date": "2024-11-15", "relevancy": 3.043, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6124}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6124}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.601}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204DPV%3A%204D%20Pet%20from%20Videos%20by%20Coarse-to-Fine%20Non-Rigid%20Radiance%20Fields&body=Title%3A%204DPV%3A%204D%20Pet%20from%20Videos%20by%20Coarse-to-Fine%20Non-Rigid%20Radiance%20Fields%0AAuthor%3A%20Sergio%20M.%20de%20Paco%20and%20Antonio%20Agudo%0AAbstract%3A%20%20%20We%20present%20a%20coarse-to-fine%20neural%20deformation%20model%20to%20simultaneously%0Arecover%20the%20camera%20pose%20and%20the%204D%20reconstruction%20of%20an%20unknown%20object%20from%0Amultiple%20RGB%20sequences%20in%20the%20wild.%20To%20that%20end%2C%20our%20approach%20does%20not%20consider%0Aany%20pre-built%203D%20template%20nor%203D%20training%20data%20as%20well%20as%20controlled%0Aillumination%20conditions%2C%20and%20can%20sort%20out%20the%20problem%20in%20a%20self-supervised%0Amanner.%20Our%20model%20exploits%20canonical%20and%20image-variant%20spaces%20where%20both%20coarse%0Aand%20fine%20components%20are%20considered.%20We%20introduce%20a%20neural%20local%20quadratic%20model%0Awith%20spatio-temporal%20consistency%20to%20encode%20fine%20details%20that%20is%20combined%20with%0Acanonical%20embeddings%20in%20order%20to%20establish%20correspondences%20across%20sequences.%20We%0Athoroughly%20validate%20the%20method%20on%20challenging%20scenarios%20with%20complex%20and%0Areal-world%20deformations%2C%20providing%20both%20quantitative%20and%20qualitative%0Aevaluations%2C%20an%20ablation%20study%20and%20a%20comparison%20with%20respect%20to%20competing%0Aapproaches.%20Our%20project%20is%20available%20at%20https%3A//github.com/smontode24/4DPV.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10275v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4DPV%253A%25204D%2520Pet%2520from%2520Videos%2520by%2520Coarse-to-Fine%2520Non-Rigid%2520Radiance%2520Fields%26entry.906535625%3DSergio%2520M.%2520de%2520Paco%2520and%2520Antonio%2520Agudo%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520coarse-to-fine%2520neural%2520deformation%2520model%2520to%2520simultaneously%250Arecover%2520the%2520camera%2520pose%2520and%2520the%25204D%2520reconstruction%2520of%2520an%2520unknown%2520object%2520from%250Amultiple%2520RGB%2520sequences%2520in%2520the%2520wild.%2520To%2520that%2520end%252C%2520our%2520approach%2520does%2520not%2520consider%250Aany%2520pre-built%25203D%2520template%2520nor%25203D%2520training%2520data%2520as%2520well%2520as%2520controlled%250Aillumination%2520conditions%252C%2520and%2520can%2520sort%2520out%2520the%2520problem%2520in%2520a%2520self-supervised%250Amanner.%2520Our%2520model%2520exploits%2520canonical%2520and%2520image-variant%2520spaces%2520where%2520both%2520coarse%250Aand%2520fine%2520components%2520are%2520considered.%2520We%2520introduce%2520a%2520neural%2520local%2520quadratic%2520model%250Awith%2520spatio-temporal%2520consistency%2520to%2520encode%2520fine%2520details%2520that%2520is%2520combined%2520with%250Acanonical%2520embeddings%2520in%2520order%2520to%2520establish%2520correspondences%2520across%2520sequences.%2520We%250Athoroughly%2520validate%2520the%2520method%2520on%2520challenging%2520scenarios%2520with%2520complex%2520and%250Areal-world%2520deformations%252C%2520providing%2520both%2520quantitative%2520and%2520qualitative%250Aevaluations%252C%2520an%2520ablation%2520study%2520and%2520a%2520comparison%2520with%2520respect%2520to%2520competing%250Aapproaches.%2520Our%2520project%2520is%2520available%2520at%2520https%253A//github.com/smontode24/4DPV.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10275v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4DPV%3A%204D%20Pet%20from%20Videos%20by%20Coarse-to-Fine%20Non-Rigid%20Radiance%20Fields&entry.906535625=Sergio%20M.%20de%20Paco%20and%20Antonio%20Agudo&entry.1292438233=%20%20We%20present%20a%20coarse-to-fine%20neural%20deformation%20model%20to%20simultaneously%0Arecover%20the%20camera%20pose%20and%20the%204D%20reconstruction%20of%20an%20unknown%20object%20from%0Amultiple%20RGB%20sequences%20in%20the%20wild.%20To%20that%20end%2C%20our%20approach%20does%20not%20consider%0Aany%20pre-built%203D%20template%20nor%203D%20training%20data%20as%20well%20as%20controlled%0Aillumination%20conditions%2C%20and%20can%20sort%20out%20the%20problem%20in%20a%20self-supervised%0Amanner.%20Our%20model%20exploits%20canonical%20and%20image-variant%20spaces%20where%20both%20coarse%0Aand%20fine%20components%20are%20considered.%20We%20introduce%20a%20neural%20local%20quadratic%20model%0Awith%20spatio-temporal%20consistency%20to%20encode%20fine%20details%20that%20is%20combined%20with%0Acanonical%20embeddings%20in%20order%20to%20establish%20correspondences%20across%20sequences.%20We%0Athoroughly%20validate%20the%20method%20on%20challenging%20scenarios%20with%20complex%20and%0Areal-world%20deformations%2C%20providing%20both%20quantitative%20and%20qualitative%0Aevaluations%2C%20an%20ablation%20study%20and%20a%20comparison%20with%20respect%20to%20competing%0Aapproaches.%20Our%20project%20is%20available%20at%20https%3A//github.com/smontode24/4DPV.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10275v1&entry.124074799=Read"},
{"title": "Visual-Linguistic Agent: Towards Collaborative Contextual Object\n  Reasoning", "author": "Jingru Yang and Huan Yu and Yang Jingxin and Chentianye Xu and Yin Biao and Yu Sun and Shengfeng He", "abstract": "  Multimodal Large Language Models (MLLMs) excel at descriptive tasks within\nimages but often struggle with precise object localization, a critical element\nfor reliable visual interpretation. In contrast, traditional object detection\nmodels provide high localization accuracy but frequently generate detections\nlacking contextual coherence due to limited modeling of inter-object\nrelationships. To address this fundamental limitation, we introduce the\n\\textbf{Visual-Linguistic Agent (VLA), a collaborative framework that combines\nthe relational reasoning strengths of MLLMs with the precise localization\ncapabilities of traditional object detectors. In the VLA paradigm, the MLLM\nserves as a central Linguistic Agent, working collaboratively with specialized\nVision Agents for object detection and classification. The Linguistic Agent\nevaluates and refines detections by reasoning over spatial and contextual\nrelationships among objects, while the classification Vision Agent offers\ncorrective feedback to improve classification accuracy. This collaborative\napproach enables VLA to significantly enhance both spatial reasoning and object\nlocalization, addressing key challenges in multimodal understanding. Extensive\nevaluations on the COCO dataset demonstrate substantial performance\nimprovements across multiple detection models, highlighting VLA's potential to\nset a new benchmark in accurate and contextually coherent object detection.\n", "link": "http://arxiv.org/abs/2411.10252v1", "date": "2024-11-15", "relevancy": 2.9206, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6019}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6019}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual-Linguistic%20Agent%3A%20Towards%20Collaborative%20Contextual%20Object%0A%20%20Reasoning&body=Title%3A%20Visual-Linguistic%20Agent%3A%20Towards%20Collaborative%20Contextual%20Object%0A%20%20Reasoning%0AAuthor%3A%20Jingru%20Yang%20and%20Huan%20Yu%20and%20Yang%20Jingxin%20and%20Chentianye%20Xu%20and%20Yin%20Biao%20and%20Yu%20Sun%20and%20Shengfeng%20He%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20excel%20at%20descriptive%20tasks%20within%0Aimages%20but%20often%20struggle%20with%20precise%20object%20localization%2C%20a%20critical%20element%0Afor%20reliable%20visual%20interpretation.%20In%20contrast%2C%20traditional%20object%20detection%0Amodels%20provide%20high%20localization%20accuracy%20but%20frequently%20generate%20detections%0Alacking%20contextual%20coherence%20due%20to%20limited%20modeling%20of%20inter-object%0Arelationships.%20To%20address%20this%20fundamental%20limitation%2C%20we%20introduce%20the%0A%5Ctextbf%7BVisual-Linguistic%20Agent%20%28VLA%29%2C%20a%20collaborative%20framework%20that%20combines%0Athe%20relational%20reasoning%20strengths%20of%20MLLMs%20with%20the%20precise%20localization%0Acapabilities%20of%20traditional%20object%20detectors.%20In%20the%20VLA%20paradigm%2C%20the%20MLLM%0Aserves%20as%20a%20central%20Linguistic%20Agent%2C%20working%20collaboratively%20with%20specialized%0AVision%20Agents%20for%20object%20detection%20and%20classification.%20The%20Linguistic%20Agent%0Aevaluates%20and%20refines%20detections%20by%20reasoning%20over%20spatial%20and%20contextual%0Arelationships%20among%20objects%2C%20while%20the%20classification%20Vision%20Agent%20offers%0Acorrective%20feedback%20to%20improve%20classification%20accuracy.%20This%20collaborative%0Aapproach%20enables%20VLA%20to%20significantly%20enhance%20both%20spatial%20reasoning%20and%20object%0Alocalization%2C%20addressing%20key%20challenges%20in%20multimodal%20understanding.%20Extensive%0Aevaluations%20on%20the%20COCO%20dataset%20demonstrate%20substantial%20performance%0Aimprovements%20across%20multiple%20detection%20models%2C%20highlighting%20VLA%27s%20potential%20to%0Aset%20a%20new%20benchmark%20in%20accurate%20and%20contextually%20coherent%20object%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10252v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual-Linguistic%2520Agent%253A%2520Towards%2520Collaborative%2520Contextual%2520Object%250A%2520%2520Reasoning%26entry.906535625%3DJingru%2520Yang%2520and%2520Huan%2520Yu%2520and%2520Yang%2520Jingxin%2520and%2520Chentianye%2520Xu%2520and%2520Yin%2520Biao%2520and%2520Yu%2520Sun%2520and%2520Shengfeng%2520He%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520excel%2520at%2520descriptive%2520tasks%2520within%250Aimages%2520but%2520often%2520struggle%2520with%2520precise%2520object%2520localization%252C%2520a%2520critical%2520element%250Afor%2520reliable%2520visual%2520interpretation.%2520In%2520contrast%252C%2520traditional%2520object%2520detection%250Amodels%2520provide%2520high%2520localization%2520accuracy%2520but%2520frequently%2520generate%2520detections%250Alacking%2520contextual%2520coherence%2520due%2520to%2520limited%2520modeling%2520of%2520inter-object%250Arelationships.%2520To%2520address%2520this%2520fundamental%2520limitation%252C%2520we%2520introduce%2520the%250A%255Ctextbf%257BVisual-Linguistic%2520Agent%2520%2528VLA%2529%252C%2520a%2520collaborative%2520framework%2520that%2520combines%250Athe%2520relational%2520reasoning%2520strengths%2520of%2520MLLMs%2520with%2520the%2520precise%2520localization%250Acapabilities%2520of%2520traditional%2520object%2520detectors.%2520In%2520the%2520VLA%2520paradigm%252C%2520the%2520MLLM%250Aserves%2520as%2520a%2520central%2520Linguistic%2520Agent%252C%2520working%2520collaboratively%2520with%2520specialized%250AVision%2520Agents%2520for%2520object%2520detection%2520and%2520classification.%2520The%2520Linguistic%2520Agent%250Aevaluates%2520and%2520refines%2520detections%2520by%2520reasoning%2520over%2520spatial%2520and%2520contextual%250Arelationships%2520among%2520objects%252C%2520while%2520the%2520classification%2520Vision%2520Agent%2520offers%250Acorrective%2520feedback%2520to%2520improve%2520classification%2520accuracy.%2520This%2520collaborative%250Aapproach%2520enables%2520VLA%2520to%2520significantly%2520enhance%2520both%2520spatial%2520reasoning%2520and%2520object%250Alocalization%252C%2520addressing%2520key%2520challenges%2520in%2520multimodal%2520understanding.%2520Extensive%250Aevaluations%2520on%2520the%2520COCO%2520dataset%2520demonstrate%2520substantial%2520performance%250Aimprovements%2520across%2520multiple%2520detection%2520models%252C%2520highlighting%2520VLA%2527s%2520potential%2520to%250Aset%2520a%2520new%2520benchmark%2520in%2520accurate%2520and%2520contextually%2520coherent%2520object%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10252v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual-Linguistic%20Agent%3A%20Towards%20Collaborative%20Contextual%20Object%0A%20%20Reasoning&entry.906535625=Jingru%20Yang%20and%20Huan%20Yu%20and%20Yang%20Jingxin%20and%20Chentianye%20Xu%20and%20Yin%20Biao%20and%20Yu%20Sun%20and%20Shengfeng%20He&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20excel%20at%20descriptive%20tasks%20within%0Aimages%20but%20often%20struggle%20with%20precise%20object%20localization%2C%20a%20critical%20element%0Afor%20reliable%20visual%20interpretation.%20In%20contrast%2C%20traditional%20object%20detection%0Amodels%20provide%20high%20localization%20accuracy%20but%20frequently%20generate%20detections%0Alacking%20contextual%20coherence%20due%20to%20limited%20modeling%20of%20inter-object%0Arelationships.%20To%20address%20this%20fundamental%20limitation%2C%20we%20introduce%20the%0A%5Ctextbf%7BVisual-Linguistic%20Agent%20%28VLA%29%2C%20a%20collaborative%20framework%20that%20combines%0Athe%20relational%20reasoning%20strengths%20of%20MLLMs%20with%20the%20precise%20localization%0Acapabilities%20of%20traditional%20object%20detectors.%20In%20the%20VLA%20paradigm%2C%20the%20MLLM%0Aserves%20as%20a%20central%20Linguistic%20Agent%2C%20working%20collaboratively%20with%20specialized%0AVision%20Agents%20for%20object%20detection%20and%20classification.%20The%20Linguistic%20Agent%0Aevaluates%20and%20refines%20detections%20by%20reasoning%20over%20spatial%20and%20contextual%0Arelationships%20among%20objects%2C%20while%20the%20classification%20Vision%20Agent%20offers%0Acorrective%20feedback%20to%20improve%20classification%20accuracy.%20This%20collaborative%0Aapproach%20enables%20VLA%20to%20significantly%20enhance%20both%20spatial%20reasoning%20and%20object%0Alocalization%2C%20addressing%20key%20challenges%20in%20multimodal%20understanding.%20Extensive%0Aevaluations%20on%20the%20COCO%20dataset%20demonstrate%20substantial%20performance%0Aimprovements%20across%20multiple%20detection%20models%2C%20highlighting%20VLA%27s%20potential%20to%0Aset%20a%20new%20benchmark%20in%20accurate%20and%20contextually%20coherent%20object%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10252v1&entry.124074799=Read"},
{"title": "Image Matching Filtering and Refinement by Planes and Beyond", "author": "Fabio Bellavia and Zhenjun Zhao and Luca Morelli and Fabio Remondino", "abstract": "  This paper introduces a modular, non-deep learning method for filtering and\nrefining sparse correspondences in image matching. Assuming that motion flow\nwithin the scene can be approximated by local homography transformations,\nmatches are aggregated into overlapping clusters corresponding to virtual\nplanes using an iterative RANSAC-based approach, with non-conforming\ncorrespondences discarded. Moreover, the underlying planar structural design\nprovides an explicit map between local patches associated with the matches,\nenabling optional refinement of keypoint positions through cross-correlation\ntemplate matching after patch reprojection. Finally, to enhance robustness and\nfault-tolerance against violations of the piece-wise planar approximation\nassumption, a further strategy is designed for minimizing relative patch\ndistortion in the plane reprojection by introducing an intermediate homography\nthat projects both patches into a common plane. The proposed method is\nextensively evaluated on standard datasets and image matching pipelines, and\ncompared with state-of-the-art approaches. Unlike other current comparisons,\nthe proposed benchmark also takes into account the more general, real, and\npractical cases where camera intrinsics are unavailable. Experimental results\ndemonstrate that our proposed non-deep learning, geometry-based approach\nachieves performances that are either superior to or on par with recent\nstate-of-the-art deep learning methods. Finally, this study suggests that there\nare still development potential in actual image matching solutions in the\nconsidered research direction, which could be in the future incorporated in\nnovel deep image matching architectures.\n", "link": "http://arxiv.org/abs/2411.09484v2", "date": "2024-11-15", "relevancy": 2.886, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6134}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5709}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20Matching%20Filtering%20and%20Refinement%20by%20Planes%20and%20Beyond&body=Title%3A%20Image%20Matching%20Filtering%20and%20Refinement%20by%20Planes%20and%20Beyond%0AAuthor%3A%20Fabio%20Bellavia%20and%20Zhenjun%20Zhao%20and%20Luca%20Morelli%20and%20Fabio%20Remondino%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20modular%2C%20non-deep%20learning%20method%20for%20filtering%20and%0Arefining%20sparse%20correspondences%20in%20image%20matching.%20Assuming%20that%20motion%20flow%0Awithin%20the%20scene%20can%20be%20approximated%20by%20local%20homography%20transformations%2C%0Amatches%20are%20aggregated%20into%20overlapping%20clusters%20corresponding%20to%20virtual%0Aplanes%20using%20an%20iterative%20RANSAC-based%20approach%2C%20with%20non-conforming%0Acorrespondences%20discarded.%20Moreover%2C%20the%20underlying%20planar%20structural%20design%0Aprovides%20an%20explicit%20map%20between%20local%20patches%20associated%20with%20the%20matches%2C%0Aenabling%20optional%20refinement%20of%20keypoint%20positions%20through%20cross-correlation%0Atemplate%20matching%20after%20patch%20reprojection.%20Finally%2C%20to%20enhance%20robustness%20and%0Afault-tolerance%20against%20violations%20of%20the%20piece-wise%20planar%20approximation%0Aassumption%2C%20a%20further%20strategy%20is%20designed%20for%20minimizing%20relative%20patch%0Adistortion%20in%20the%20plane%20reprojection%20by%20introducing%20an%20intermediate%20homography%0Athat%20projects%20both%20patches%20into%20a%20common%20plane.%20The%20proposed%20method%20is%0Aextensively%20evaluated%20on%20standard%20datasets%20and%20image%20matching%20pipelines%2C%20and%0Acompared%20with%20state-of-the-art%20approaches.%20Unlike%20other%20current%20comparisons%2C%0Athe%20proposed%20benchmark%20also%20takes%20into%20account%20the%20more%20general%2C%20real%2C%20and%0Apractical%20cases%20where%20camera%20intrinsics%20are%20unavailable.%20Experimental%20results%0Ademonstrate%20that%20our%20proposed%20non-deep%20learning%2C%20geometry-based%20approach%0Aachieves%20performances%20that%20are%20either%20superior%20to%20or%20on%20par%20with%20recent%0Astate-of-the-art%20deep%20learning%20methods.%20Finally%2C%20this%20study%20suggests%20that%20there%0Aare%20still%20development%20potential%20in%20actual%20image%20matching%20solutions%20in%20the%0Aconsidered%20research%20direction%2C%20which%20could%20be%20in%20the%20future%20incorporated%20in%0Anovel%20deep%20image%20matching%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09484v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520Matching%2520Filtering%2520and%2520Refinement%2520by%2520Planes%2520and%2520Beyond%26entry.906535625%3DFabio%2520Bellavia%2520and%2520Zhenjun%2520Zhao%2520and%2520Luca%2520Morelli%2520and%2520Fabio%2520Remondino%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520modular%252C%2520non-deep%2520learning%2520method%2520for%2520filtering%2520and%250Arefining%2520sparse%2520correspondences%2520in%2520image%2520matching.%2520Assuming%2520that%2520motion%2520flow%250Awithin%2520the%2520scene%2520can%2520be%2520approximated%2520by%2520local%2520homography%2520transformations%252C%250Amatches%2520are%2520aggregated%2520into%2520overlapping%2520clusters%2520corresponding%2520to%2520virtual%250Aplanes%2520using%2520an%2520iterative%2520RANSAC-based%2520approach%252C%2520with%2520non-conforming%250Acorrespondences%2520discarded.%2520Moreover%252C%2520the%2520underlying%2520planar%2520structural%2520design%250Aprovides%2520an%2520explicit%2520map%2520between%2520local%2520patches%2520associated%2520with%2520the%2520matches%252C%250Aenabling%2520optional%2520refinement%2520of%2520keypoint%2520positions%2520through%2520cross-correlation%250Atemplate%2520matching%2520after%2520patch%2520reprojection.%2520Finally%252C%2520to%2520enhance%2520robustness%2520and%250Afault-tolerance%2520against%2520violations%2520of%2520the%2520piece-wise%2520planar%2520approximation%250Aassumption%252C%2520a%2520further%2520strategy%2520is%2520designed%2520for%2520minimizing%2520relative%2520patch%250Adistortion%2520in%2520the%2520plane%2520reprojection%2520by%2520introducing%2520an%2520intermediate%2520homography%250Athat%2520projects%2520both%2520patches%2520into%2520a%2520common%2520plane.%2520The%2520proposed%2520method%2520is%250Aextensively%2520evaluated%2520on%2520standard%2520datasets%2520and%2520image%2520matching%2520pipelines%252C%2520and%250Acompared%2520with%2520state-of-the-art%2520approaches.%2520Unlike%2520other%2520current%2520comparisons%252C%250Athe%2520proposed%2520benchmark%2520also%2520takes%2520into%2520account%2520the%2520more%2520general%252C%2520real%252C%2520and%250Apractical%2520cases%2520where%2520camera%2520intrinsics%2520are%2520unavailable.%2520Experimental%2520results%250Ademonstrate%2520that%2520our%2520proposed%2520non-deep%2520learning%252C%2520geometry-based%2520approach%250Aachieves%2520performances%2520that%2520are%2520either%2520superior%2520to%2520or%2520on%2520par%2520with%2520recent%250Astate-of-the-art%2520deep%2520learning%2520methods.%2520Finally%252C%2520this%2520study%2520suggests%2520that%2520there%250Aare%2520still%2520development%2520potential%2520in%2520actual%2520image%2520matching%2520solutions%2520in%2520the%250Aconsidered%2520research%2520direction%252C%2520which%2520could%2520be%2520in%2520the%2520future%2520incorporated%2520in%250Anovel%2520deep%2520image%2520matching%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09484v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20Matching%20Filtering%20and%20Refinement%20by%20Planes%20and%20Beyond&entry.906535625=Fabio%20Bellavia%20and%20Zhenjun%20Zhao%20and%20Luca%20Morelli%20and%20Fabio%20Remondino&entry.1292438233=%20%20This%20paper%20introduces%20a%20modular%2C%20non-deep%20learning%20method%20for%20filtering%20and%0Arefining%20sparse%20correspondences%20in%20image%20matching.%20Assuming%20that%20motion%20flow%0Awithin%20the%20scene%20can%20be%20approximated%20by%20local%20homography%20transformations%2C%0Amatches%20are%20aggregated%20into%20overlapping%20clusters%20corresponding%20to%20virtual%0Aplanes%20using%20an%20iterative%20RANSAC-based%20approach%2C%20with%20non-conforming%0Acorrespondences%20discarded.%20Moreover%2C%20the%20underlying%20planar%20structural%20design%0Aprovides%20an%20explicit%20map%20between%20local%20patches%20associated%20with%20the%20matches%2C%0Aenabling%20optional%20refinement%20of%20keypoint%20positions%20through%20cross-correlation%0Atemplate%20matching%20after%20patch%20reprojection.%20Finally%2C%20to%20enhance%20robustness%20and%0Afault-tolerance%20against%20violations%20of%20the%20piece-wise%20planar%20approximation%0Aassumption%2C%20a%20further%20strategy%20is%20designed%20for%20minimizing%20relative%20patch%0Adistortion%20in%20the%20plane%20reprojection%20by%20introducing%20an%20intermediate%20homography%0Athat%20projects%20both%20patches%20into%20a%20common%20plane.%20The%20proposed%20method%20is%0Aextensively%20evaluated%20on%20standard%20datasets%20and%20image%20matching%20pipelines%2C%20and%0Acompared%20with%20state-of-the-art%20approaches.%20Unlike%20other%20current%20comparisons%2C%0Athe%20proposed%20benchmark%20also%20takes%20into%20account%20the%20more%20general%2C%20real%2C%20and%0Apractical%20cases%20where%20camera%20intrinsics%20are%20unavailable.%20Experimental%20results%0Ademonstrate%20that%20our%20proposed%20non-deep%20learning%2C%20geometry-based%20approach%0Aachieves%20performances%20that%20are%20either%20superior%20to%20or%20on%20par%20with%20recent%0Astate-of-the-art%20deep%20learning%20methods.%20Finally%2C%20this%20study%20suggests%20that%20there%0Aare%20still%20development%20potential%20in%20actual%20image%20matching%20solutions%20in%20the%0Aconsidered%20research%20direction%2C%20which%20could%20be%20in%20the%20future%20incorporated%20in%0Anovel%20deep%20image%20matching%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09484v2&entry.124074799=Read"},
{"title": "Morpho-Aware Global Attention for Image Matting", "author": "Jingru Yang and Chengzhi Cao and Chentianye Xu and Zhongwei Xie and Kaixiang Huang and Yang Zhou and Shengfeng He", "abstract": "  Vision Transformers (ViTs) and Convolutional Neural Networks (CNNs) face\ninherent challenges in image matting, particularly in preserving fine\nstructural details. ViTs, with their global receptive field enabled by the\nself-attention mechanism, often lose local details such as hair strands.\nConversely, CNNs, constrained by their local receptive field, rely on deeper\nlayers to approximate global context but struggle to retain fine structures at\ngreater depths.\n  To overcome these limitations, we propose a novel Morpho-Aware Global\nAttention (MAGA) mechanism, designed to effectively capture the morphology of\nfine structures. MAGA employs Tetris-like convolutional patterns to align the\nlocal shapes of fine structures, ensuring optimal local correspondence while\nmaintaining sensitivity to morphological details. The extracted local\nmorphology information is used as query embeddings, which are projected onto\nglobal key embeddings to emphasize local details in a broader context.\nSubsequently, by projecting onto value embeddings, MAGA seamlessly integrates\nthese emphasized morphological details into a unified global structure.\n  This approach enables MAGA to simultaneously focus on local morphology and\nunify these details into a coherent whole, effectively preserving fine\nstructures. Extensive experiments show that our MAGA-based ViT achieves\nsignificant performance gains, outperforming state-of-the-art methods across\ntwo benchmarks with average improvements of 4.3% in SAD and 39.5% in MSE.\n", "link": "http://arxiv.org/abs/2411.10251v1", "date": "2024-11-15", "relevancy": 2.7954, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5898}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.55}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Morpho-Aware%20Global%20Attention%20for%20Image%20Matting&body=Title%3A%20Morpho-Aware%20Global%20Attention%20for%20Image%20Matting%0AAuthor%3A%20Jingru%20Yang%20and%20Chengzhi%20Cao%20and%20Chentianye%20Xu%20and%20Zhongwei%20Xie%20and%20Kaixiang%20Huang%20and%20Yang%20Zhou%20and%20Shengfeng%20He%0AAbstract%3A%20%20%20Vision%20Transformers%20%28ViTs%29%20and%20Convolutional%20Neural%20Networks%20%28CNNs%29%20face%0Ainherent%20challenges%20in%20image%20matting%2C%20particularly%20in%20preserving%20fine%0Astructural%20details.%20ViTs%2C%20with%20their%20global%20receptive%20field%20enabled%20by%20the%0Aself-attention%20mechanism%2C%20often%20lose%20local%20details%20such%20as%20hair%20strands.%0AConversely%2C%20CNNs%2C%20constrained%20by%20their%20local%20receptive%20field%2C%20rely%20on%20deeper%0Alayers%20to%20approximate%20global%20context%20but%20struggle%20to%20retain%20fine%20structures%20at%0Agreater%20depths.%0A%20%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%20novel%20Morpho-Aware%20Global%0AAttention%20%28MAGA%29%20mechanism%2C%20designed%20to%20effectively%20capture%20the%20morphology%20of%0Afine%20structures.%20MAGA%20employs%20Tetris-like%20convolutional%20patterns%20to%20align%20the%0Alocal%20shapes%20of%20fine%20structures%2C%20ensuring%20optimal%20local%20correspondence%20while%0Amaintaining%20sensitivity%20to%20morphological%20details.%20The%20extracted%20local%0Amorphology%20information%20is%20used%20as%20query%20embeddings%2C%20which%20are%20projected%20onto%0Aglobal%20key%20embeddings%20to%20emphasize%20local%20details%20in%20a%20broader%20context.%0ASubsequently%2C%20by%20projecting%20onto%20value%20embeddings%2C%20MAGA%20seamlessly%20integrates%0Athese%20emphasized%20morphological%20details%20into%20a%20unified%20global%20structure.%0A%20%20This%20approach%20enables%20MAGA%20to%20simultaneously%20focus%20on%20local%20morphology%20and%0Aunify%20these%20details%20into%20a%20coherent%20whole%2C%20effectively%20preserving%20fine%0Astructures.%20Extensive%20experiments%20show%20that%20our%20MAGA-based%20ViT%20achieves%0Asignificant%20performance%20gains%2C%20outperforming%20state-of-the-art%20methods%20across%0Atwo%20benchmarks%20with%20average%20improvements%20of%204.3%25%20in%20SAD%20and%2039.5%25%20in%20MSE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10251v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMorpho-Aware%2520Global%2520Attention%2520for%2520Image%2520Matting%26entry.906535625%3DJingru%2520Yang%2520and%2520Chengzhi%2520Cao%2520and%2520Chentianye%2520Xu%2520and%2520Zhongwei%2520Xie%2520and%2520Kaixiang%2520Huang%2520and%2520Yang%2520Zhou%2520and%2520Shengfeng%2520He%26entry.1292438233%3D%2520%2520Vision%2520Transformers%2520%2528ViTs%2529%2520and%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520face%250Ainherent%2520challenges%2520in%2520image%2520matting%252C%2520particularly%2520in%2520preserving%2520fine%250Astructural%2520details.%2520ViTs%252C%2520with%2520their%2520global%2520receptive%2520field%2520enabled%2520by%2520the%250Aself-attention%2520mechanism%252C%2520often%2520lose%2520local%2520details%2520such%2520as%2520hair%2520strands.%250AConversely%252C%2520CNNs%252C%2520constrained%2520by%2520their%2520local%2520receptive%2520field%252C%2520rely%2520on%2520deeper%250Alayers%2520to%2520approximate%2520global%2520context%2520but%2520struggle%2520to%2520retain%2520fine%2520structures%2520at%250Agreater%2520depths.%250A%2520%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%2520Morpho-Aware%2520Global%250AAttention%2520%2528MAGA%2529%2520mechanism%252C%2520designed%2520to%2520effectively%2520capture%2520the%2520morphology%2520of%250Afine%2520structures.%2520MAGA%2520employs%2520Tetris-like%2520convolutional%2520patterns%2520to%2520align%2520the%250Alocal%2520shapes%2520of%2520fine%2520structures%252C%2520ensuring%2520optimal%2520local%2520correspondence%2520while%250Amaintaining%2520sensitivity%2520to%2520morphological%2520details.%2520The%2520extracted%2520local%250Amorphology%2520information%2520is%2520used%2520as%2520query%2520embeddings%252C%2520which%2520are%2520projected%2520onto%250Aglobal%2520key%2520embeddings%2520to%2520emphasize%2520local%2520details%2520in%2520a%2520broader%2520context.%250ASubsequently%252C%2520by%2520projecting%2520onto%2520value%2520embeddings%252C%2520MAGA%2520seamlessly%2520integrates%250Athese%2520emphasized%2520morphological%2520details%2520into%2520a%2520unified%2520global%2520structure.%250A%2520%2520This%2520approach%2520enables%2520MAGA%2520to%2520simultaneously%2520focus%2520on%2520local%2520morphology%2520and%250Aunify%2520these%2520details%2520into%2520a%2520coherent%2520whole%252C%2520effectively%2520preserving%2520fine%250Astructures.%2520Extensive%2520experiments%2520show%2520that%2520our%2520MAGA-based%2520ViT%2520achieves%250Asignificant%2520performance%2520gains%252C%2520outperforming%2520state-of-the-art%2520methods%2520across%250Atwo%2520benchmarks%2520with%2520average%2520improvements%2520of%25204.3%2525%2520in%2520SAD%2520and%252039.5%2525%2520in%2520MSE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10251v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Morpho-Aware%20Global%20Attention%20for%20Image%20Matting&entry.906535625=Jingru%20Yang%20and%20Chengzhi%20Cao%20and%20Chentianye%20Xu%20and%20Zhongwei%20Xie%20and%20Kaixiang%20Huang%20and%20Yang%20Zhou%20and%20Shengfeng%20He&entry.1292438233=%20%20Vision%20Transformers%20%28ViTs%29%20and%20Convolutional%20Neural%20Networks%20%28CNNs%29%20face%0Ainherent%20challenges%20in%20image%20matting%2C%20particularly%20in%20preserving%20fine%0Astructural%20details.%20ViTs%2C%20with%20their%20global%20receptive%20field%20enabled%20by%20the%0Aself-attention%20mechanism%2C%20often%20lose%20local%20details%20such%20as%20hair%20strands.%0AConversely%2C%20CNNs%2C%20constrained%20by%20their%20local%20receptive%20field%2C%20rely%20on%20deeper%0Alayers%20to%20approximate%20global%20context%20but%20struggle%20to%20retain%20fine%20structures%20at%0Agreater%20depths.%0A%20%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%20novel%20Morpho-Aware%20Global%0AAttention%20%28MAGA%29%20mechanism%2C%20designed%20to%20effectively%20capture%20the%20morphology%20of%0Afine%20structures.%20MAGA%20employs%20Tetris-like%20convolutional%20patterns%20to%20align%20the%0Alocal%20shapes%20of%20fine%20structures%2C%20ensuring%20optimal%20local%20correspondence%20while%0Amaintaining%20sensitivity%20to%20morphological%20details.%20The%20extracted%20local%0Amorphology%20information%20is%20used%20as%20query%20embeddings%2C%20which%20are%20projected%20onto%0Aglobal%20key%20embeddings%20to%20emphasize%20local%20details%20in%20a%20broader%20context.%0ASubsequently%2C%20by%20projecting%20onto%20value%20embeddings%2C%20MAGA%20seamlessly%20integrates%0Athese%20emphasized%20morphological%20details%20into%20a%20unified%20global%20structure.%0A%20%20This%20approach%20enables%20MAGA%20to%20simultaneously%20focus%20on%20local%20morphology%20and%0Aunify%20these%20details%20into%20a%20coherent%20whole%2C%20effectively%20preserving%20fine%0Astructures.%20Extensive%20experiments%20show%20that%20our%20MAGA-based%20ViT%20achieves%0Asignificant%20performance%20gains%2C%20outperforming%20state-of-the-art%20methods%20across%0Atwo%20benchmarks%20with%20average%20improvements%20of%204.3%25%20in%20SAD%20and%2039.5%25%20in%20MSE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10251v1&entry.124074799=Read"},
{"title": "LLaVA-o1: Let Vision Language Models Reason Step-by-Step", "author": "Guowei Xu and Peng Jin and Li Hao and Yibing Song and Lichao Sun and Li Yuan", "abstract": "  Large language models have demonstrated substantial advancements in reasoning\ncapabilities, particularly through inference-time scaling, as illustrated by\nmodels such as OpenAI's o1. However, current Vision-Language Models (VLMs)\noften struggle to perform systematic and structured reasoning, especially when\nhandling complex visual question-answering tasks. In this work, we introduce\nLLaVA-o1, a novel VLM designed to conduct autonomous multistage reasoning.\nUnlike chain-of-thought prompting, LLaVA-o1 independently engages in sequential\nstages of summarization, visual interpretation, logical reasoning, and\nconclusion generation. This structured approach enables LLaVA-o1 to achieve\nmarked improvements in precision on reasoning-intensive tasks. To accomplish\nthis, we compile the LLaVA-o1-100k dataset, integrating samples from various\nvisual question answering sources and providing structured reasoning\nannotations. Besides, we propose an inference-time stage-level beam search\nmethod, which enables effective inference-time scaling. Remarkably, with only\n100k training samples and a simple yet effective inference time scaling method,\nLLaVA-o1 not only outperforms its base model by 8.9% on a wide range of\nmultimodal reasoning benchmarks, but also surpasses the performance of larger\nand even closed-source models, such as Gemini-1.5-pro, GPT-4o-mini, and\nLlama-3.2-90B-Vision-Instruct.\n", "link": "http://arxiv.org/abs/2411.10440v1", "date": "2024-11-15", "relevancy": 2.7699, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5854}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5854}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLaVA-o1%3A%20Let%20Vision%20Language%20Models%20Reason%20Step-by-Step&body=Title%3A%20LLaVA-o1%3A%20Let%20Vision%20Language%20Models%20Reason%20Step-by-Step%0AAuthor%3A%20Guowei%20Xu%20and%20Peng%20Jin%20and%20Li%20Hao%20and%20Yibing%20Song%20and%20Lichao%20Sun%20and%20Li%20Yuan%0AAbstract%3A%20%20%20Large%20language%20models%20have%20demonstrated%20substantial%20advancements%20in%20reasoning%0Acapabilities%2C%20particularly%20through%20inference-time%20scaling%2C%20as%20illustrated%20by%0Amodels%20such%20as%20OpenAI%27s%20o1.%20However%2C%20current%20Vision-Language%20Models%20%28VLMs%29%0Aoften%20struggle%20to%20perform%20systematic%20and%20structured%20reasoning%2C%20especially%20when%0Ahandling%20complex%20visual%20question-answering%20tasks.%20In%20this%20work%2C%20we%20introduce%0ALLaVA-o1%2C%20a%20novel%20VLM%20designed%20to%20conduct%20autonomous%20multistage%20reasoning.%0AUnlike%20chain-of-thought%20prompting%2C%20LLaVA-o1%20independently%20engages%20in%20sequential%0Astages%20of%20summarization%2C%20visual%20interpretation%2C%20logical%20reasoning%2C%20and%0Aconclusion%20generation.%20This%20structured%20approach%20enables%20LLaVA-o1%20to%20achieve%0Amarked%20improvements%20in%20precision%20on%20reasoning-intensive%20tasks.%20To%20accomplish%0Athis%2C%20we%20compile%20the%20LLaVA-o1-100k%20dataset%2C%20integrating%20samples%20from%20various%0Avisual%20question%20answering%20sources%20and%20providing%20structured%20reasoning%0Aannotations.%20Besides%2C%20we%20propose%20an%20inference-time%20stage-level%20beam%20search%0Amethod%2C%20which%20enables%20effective%20inference-time%20scaling.%20Remarkably%2C%20with%20only%0A100k%20training%20samples%20and%20a%20simple%20yet%20effective%20inference%20time%20scaling%20method%2C%0ALLaVA-o1%20not%20only%20outperforms%20its%20base%20model%20by%208.9%25%20on%20a%20wide%20range%20of%0Amultimodal%20reasoning%20benchmarks%2C%20but%20also%20surpasses%20the%20performance%20of%20larger%0Aand%20even%20closed-source%20models%2C%20such%20as%20Gemini-1.5-pro%2C%20GPT-4o-mini%2C%20and%0ALlama-3.2-90B-Vision-Instruct.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10440v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLaVA-o1%253A%2520Let%2520Vision%2520Language%2520Models%2520Reason%2520Step-by-Step%26entry.906535625%3DGuowei%2520Xu%2520and%2520Peng%2520Jin%2520and%2520Li%2520Hao%2520and%2520Yibing%2520Song%2520and%2520Lichao%2520Sun%2520and%2520Li%2520Yuan%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520have%2520demonstrated%2520substantial%2520advancements%2520in%2520reasoning%250Acapabilities%252C%2520particularly%2520through%2520inference-time%2520scaling%252C%2520as%2520illustrated%2520by%250Amodels%2520such%2520as%2520OpenAI%2527s%2520o1.%2520However%252C%2520current%2520Vision-Language%2520Models%2520%2528VLMs%2529%250Aoften%2520struggle%2520to%2520perform%2520systematic%2520and%2520structured%2520reasoning%252C%2520especially%2520when%250Ahandling%2520complex%2520visual%2520question-answering%2520tasks.%2520In%2520this%2520work%252C%2520we%2520introduce%250ALLaVA-o1%252C%2520a%2520novel%2520VLM%2520designed%2520to%2520conduct%2520autonomous%2520multistage%2520reasoning.%250AUnlike%2520chain-of-thought%2520prompting%252C%2520LLaVA-o1%2520independently%2520engages%2520in%2520sequential%250Astages%2520of%2520summarization%252C%2520visual%2520interpretation%252C%2520logical%2520reasoning%252C%2520and%250Aconclusion%2520generation.%2520This%2520structured%2520approach%2520enables%2520LLaVA-o1%2520to%2520achieve%250Amarked%2520improvements%2520in%2520precision%2520on%2520reasoning-intensive%2520tasks.%2520To%2520accomplish%250Athis%252C%2520we%2520compile%2520the%2520LLaVA-o1-100k%2520dataset%252C%2520integrating%2520samples%2520from%2520various%250Avisual%2520question%2520answering%2520sources%2520and%2520providing%2520structured%2520reasoning%250Aannotations.%2520Besides%252C%2520we%2520propose%2520an%2520inference-time%2520stage-level%2520beam%2520search%250Amethod%252C%2520which%2520enables%2520effective%2520inference-time%2520scaling.%2520Remarkably%252C%2520with%2520only%250A100k%2520training%2520samples%2520and%2520a%2520simple%2520yet%2520effective%2520inference%2520time%2520scaling%2520method%252C%250ALLaVA-o1%2520not%2520only%2520outperforms%2520its%2520base%2520model%2520by%25208.9%2525%2520on%2520a%2520wide%2520range%2520of%250Amultimodal%2520reasoning%2520benchmarks%252C%2520but%2520also%2520surpasses%2520the%2520performance%2520of%2520larger%250Aand%2520even%2520closed-source%2520models%252C%2520such%2520as%2520Gemini-1.5-pro%252C%2520GPT-4o-mini%252C%2520and%250ALlama-3.2-90B-Vision-Instruct.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10440v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLaVA-o1%3A%20Let%20Vision%20Language%20Models%20Reason%20Step-by-Step&entry.906535625=Guowei%20Xu%20and%20Peng%20Jin%20and%20Li%20Hao%20and%20Yibing%20Song%20and%20Lichao%20Sun%20and%20Li%20Yuan&entry.1292438233=%20%20Large%20language%20models%20have%20demonstrated%20substantial%20advancements%20in%20reasoning%0Acapabilities%2C%20particularly%20through%20inference-time%20scaling%2C%20as%20illustrated%20by%0Amodels%20such%20as%20OpenAI%27s%20o1.%20However%2C%20current%20Vision-Language%20Models%20%28VLMs%29%0Aoften%20struggle%20to%20perform%20systematic%20and%20structured%20reasoning%2C%20especially%20when%0Ahandling%20complex%20visual%20question-answering%20tasks.%20In%20this%20work%2C%20we%20introduce%0ALLaVA-o1%2C%20a%20novel%20VLM%20designed%20to%20conduct%20autonomous%20multistage%20reasoning.%0AUnlike%20chain-of-thought%20prompting%2C%20LLaVA-o1%20independently%20engages%20in%20sequential%0Astages%20of%20summarization%2C%20visual%20interpretation%2C%20logical%20reasoning%2C%20and%0Aconclusion%20generation.%20This%20structured%20approach%20enables%20LLaVA-o1%20to%20achieve%0Amarked%20improvements%20in%20precision%20on%20reasoning-intensive%20tasks.%20To%20accomplish%0Athis%2C%20we%20compile%20the%20LLaVA-o1-100k%20dataset%2C%20integrating%20samples%20from%20various%0Avisual%20question%20answering%20sources%20and%20providing%20structured%20reasoning%0Aannotations.%20Besides%2C%20we%20propose%20an%20inference-time%20stage-level%20beam%20search%0Amethod%2C%20which%20enables%20effective%20inference-time%20scaling.%20Remarkably%2C%20with%20only%0A100k%20training%20samples%20and%20a%20simple%20yet%20effective%20inference%20time%20scaling%20method%2C%0ALLaVA-o1%20not%20only%20outperforms%20its%20base%20model%20by%208.9%25%20on%20a%20wide%20range%20of%0Amultimodal%20reasoning%20benchmarks%2C%20but%20also%20surpasses%20the%20performance%20of%20larger%0Aand%20even%20closed-source%20models%2C%20such%20as%20Gemini-1.5-pro%2C%20GPT-4o-mini%2C%20and%0ALlama-3.2-90B-Vision-Instruct.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10440v1&entry.124074799=Read"},
{"title": "Treat Visual Tokens as Text? But Your MLLM Only Needs Fewer Efforts to\n  See", "author": "Zeliang Zhang and Phu Pham and Wentian Zhao and Kun Wan and Yu-Jhe Li and Jianing Zhou and Daniel Miranda and Ajinkya Kale and Chenliang Xu", "abstract": "  By treating visual tokens from visual encoders as text tokens, Multimodal\nLarge Language Models (MLLMs) have achieved remarkable progress across diverse\nvisual understanding tasks, leveraging the robust architectures of Large\nLanguage Models (LLMs). However, as token counts grow, the quadratic scaling of\ncomputation in LLMs introduces a significant efficiency bottleneck, impeding\nfurther scalability. Although recent approaches have explored pruning visual\ntokens or employing lighter LLM architectures, the computational overhead from\nan increasing number of visual tokens remains a substantial challenge.\n  In this study, we investigate the redundancy in visual computation at both\nthe parameter and computational pattern levels within LLaVA, a representative\nMLLM, and introduce a suite of streamlined strategies to enhance efficiency.\nThese include neighbor-aware visual token attention, pruning of inactive visual\nattention heads, and selective layer dropping for visual computations. By\nimplementing these strategies in LLaVA, we achieve a reduction in computational\ndemands of 88% while maintaining model performance across key benchmarks.\nAdditionally, we validate the existence of visual computational redundancy in\nother MLLMs, such as Qwen2-VL-7B and InternVL-2.0-4B/8B/26B. These results\npresent a novel pathway for MLLMs to handle dense visual tokens with minimal\ncomputational costs. Code and model checkpoints will be released to support\nfurther research.\n", "link": "http://arxiv.org/abs/2410.06169v2", "date": "2024-11-15", "relevancy": 2.7651, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5682}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5454}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Treat%20Visual%20Tokens%20as%20Text%3F%20But%20Your%20MLLM%20Only%20Needs%20Fewer%20Efforts%20to%0A%20%20See&body=Title%3A%20Treat%20Visual%20Tokens%20as%20Text%3F%20But%20Your%20MLLM%20Only%20Needs%20Fewer%20Efforts%20to%0A%20%20See%0AAuthor%3A%20Zeliang%20Zhang%20and%20Phu%20Pham%20and%20Wentian%20Zhao%20and%20Kun%20Wan%20and%20Yu-Jhe%20Li%20and%20Jianing%20Zhou%20and%20Daniel%20Miranda%20and%20Ajinkya%20Kale%20and%20Chenliang%20Xu%0AAbstract%3A%20%20%20By%20treating%20visual%20tokens%20from%20visual%20encoders%20as%20text%20tokens%2C%20Multimodal%0ALarge%20Language%20Models%20%28MLLMs%29%20have%20achieved%20remarkable%20progress%20across%20diverse%0Avisual%20understanding%20tasks%2C%20leveraging%20the%20robust%20architectures%20of%20Large%0ALanguage%20Models%20%28LLMs%29.%20However%2C%20as%20token%20counts%20grow%2C%20the%20quadratic%20scaling%20of%0Acomputation%20in%20LLMs%20introduces%20a%20significant%20efficiency%20bottleneck%2C%20impeding%0Afurther%20scalability.%20Although%20recent%20approaches%20have%20explored%20pruning%20visual%0Atokens%20or%20employing%20lighter%20LLM%20architectures%2C%20the%20computational%20overhead%20from%0Aan%20increasing%20number%20of%20visual%20tokens%20remains%20a%20substantial%20challenge.%0A%20%20In%20this%20study%2C%20we%20investigate%20the%20redundancy%20in%20visual%20computation%20at%20both%0Athe%20parameter%20and%20computational%20pattern%20levels%20within%20LLaVA%2C%20a%20representative%0AMLLM%2C%20and%20introduce%20a%20suite%20of%20streamlined%20strategies%20to%20enhance%20efficiency.%0AThese%20include%20neighbor-aware%20visual%20token%20attention%2C%20pruning%20of%20inactive%20visual%0Aattention%20heads%2C%20and%20selective%20layer%20dropping%20for%20visual%20computations.%20By%0Aimplementing%20these%20strategies%20in%20LLaVA%2C%20we%20achieve%20a%20reduction%20in%20computational%0Ademands%20of%2088%25%20while%20maintaining%20model%20performance%20across%20key%20benchmarks.%0AAdditionally%2C%20we%20validate%20the%20existence%20of%20visual%20computational%20redundancy%20in%0Aother%20MLLMs%2C%20such%20as%20Qwen2-VL-7B%20and%20InternVL-2.0-4B/8B/26B.%20These%20results%0Apresent%20a%20novel%20pathway%20for%20MLLMs%20to%20handle%20dense%20visual%20tokens%20with%20minimal%0Acomputational%20costs.%20Code%20and%20model%20checkpoints%20will%20be%20released%20to%20support%0Afurther%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06169v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTreat%2520Visual%2520Tokens%2520as%2520Text%253F%2520But%2520Your%2520MLLM%2520Only%2520Needs%2520Fewer%2520Efforts%2520to%250A%2520%2520See%26entry.906535625%3DZeliang%2520Zhang%2520and%2520Phu%2520Pham%2520and%2520Wentian%2520Zhao%2520and%2520Kun%2520Wan%2520and%2520Yu-Jhe%2520Li%2520and%2520Jianing%2520Zhou%2520and%2520Daniel%2520Miranda%2520and%2520Ajinkya%2520Kale%2520and%2520Chenliang%2520Xu%26entry.1292438233%3D%2520%2520By%2520treating%2520visual%2520tokens%2520from%2520visual%2520encoders%2520as%2520text%2520tokens%252C%2520Multimodal%250ALarge%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520achieved%2520remarkable%2520progress%2520across%2520diverse%250Avisual%2520understanding%2520tasks%252C%2520leveraging%2520the%2520robust%2520architectures%2520of%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529.%2520However%252C%2520as%2520token%2520counts%2520grow%252C%2520the%2520quadratic%2520scaling%2520of%250Acomputation%2520in%2520LLMs%2520introduces%2520a%2520significant%2520efficiency%2520bottleneck%252C%2520impeding%250Afurther%2520scalability.%2520Although%2520recent%2520approaches%2520have%2520explored%2520pruning%2520visual%250Atokens%2520or%2520employing%2520lighter%2520LLM%2520architectures%252C%2520the%2520computational%2520overhead%2520from%250Aan%2520increasing%2520number%2520of%2520visual%2520tokens%2520remains%2520a%2520substantial%2520challenge.%250A%2520%2520In%2520this%2520study%252C%2520we%2520investigate%2520the%2520redundancy%2520in%2520visual%2520computation%2520at%2520both%250Athe%2520parameter%2520and%2520computational%2520pattern%2520levels%2520within%2520LLaVA%252C%2520a%2520representative%250AMLLM%252C%2520and%2520introduce%2520a%2520suite%2520of%2520streamlined%2520strategies%2520to%2520enhance%2520efficiency.%250AThese%2520include%2520neighbor-aware%2520visual%2520token%2520attention%252C%2520pruning%2520of%2520inactive%2520visual%250Aattention%2520heads%252C%2520and%2520selective%2520layer%2520dropping%2520for%2520visual%2520computations.%2520By%250Aimplementing%2520these%2520strategies%2520in%2520LLaVA%252C%2520we%2520achieve%2520a%2520reduction%2520in%2520computational%250Ademands%2520of%252088%2525%2520while%2520maintaining%2520model%2520performance%2520across%2520key%2520benchmarks.%250AAdditionally%252C%2520we%2520validate%2520the%2520existence%2520of%2520visual%2520computational%2520redundancy%2520in%250Aother%2520MLLMs%252C%2520such%2520as%2520Qwen2-VL-7B%2520and%2520InternVL-2.0-4B/8B/26B.%2520These%2520results%250Apresent%2520a%2520novel%2520pathway%2520for%2520MLLMs%2520to%2520handle%2520dense%2520visual%2520tokens%2520with%2520minimal%250Acomputational%2520costs.%2520Code%2520and%2520model%2520checkpoints%2520will%2520be%2520released%2520to%2520support%250Afurther%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06169v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Treat%20Visual%20Tokens%20as%20Text%3F%20But%20Your%20MLLM%20Only%20Needs%20Fewer%20Efforts%20to%0A%20%20See&entry.906535625=Zeliang%20Zhang%20and%20Phu%20Pham%20and%20Wentian%20Zhao%20and%20Kun%20Wan%20and%20Yu-Jhe%20Li%20and%20Jianing%20Zhou%20and%20Daniel%20Miranda%20and%20Ajinkya%20Kale%20and%20Chenliang%20Xu&entry.1292438233=%20%20By%20treating%20visual%20tokens%20from%20visual%20encoders%20as%20text%20tokens%2C%20Multimodal%0ALarge%20Language%20Models%20%28MLLMs%29%20have%20achieved%20remarkable%20progress%20across%20diverse%0Avisual%20understanding%20tasks%2C%20leveraging%20the%20robust%20architectures%20of%20Large%0ALanguage%20Models%20%28LLMs%29.%20However%2C%20as%20token%20counts%20grow%2C%20the%20quadratic%20scaling%20of%0Acomputation%20in%20LLMs%20introduces%20a%20significant%20efficiency%20bottleneck%2C%20impeding%0Afurther%20scalability.%20Although%20recent%20approaches%20have%20explored%20pruning%20visual%0Atokens%20or%20employing%20lighter%20LLM%20architectures%2C%20the%20computational%20overhead%20from%0Aan%20increasing%20number%20of%20visual%20tokens%20remains%20a%20substantial%20challenge.%0A%20%20In%20this%20study%2C%20we%20investigate%20the%20redundancy%20in%20visual%20computation%20at%20both%0Athe%20parameter%20and%20computational%20pattern%20levels%20within%20LLaVA%2C%20a%20representative%0AMLLM%2C%20and%20introduce%20a%20suite%20of%20streamlined%20strategies%20to%20enhance%20efficiency.%0AThese%20include%20neighbor-aware%20visual%20token%20attention%2C%20pruning%20of%20inactive%20visual%0Aattention%20heads%2C%20and%20selective%20layer%20dropping%20for%20visual%20computations.%20By%0Aimplementing%20these%20strategies%20in%20LLaVA%2C%20we%20achieve%20a%20reduction%20in%20computational%0Ademands%20of%2088%25%20while%20maintaining%20model%20performance%20across%20key%20benchmarks.%0AAdditionally%2C%20we%20validate%20the%20existence%20of%20visual%20computational%20redundancy%20in%0Aother%20MLLMs%2C%20such%20as%20Qwen2-VL-7B%20and%20InternVL-2.0-4B/8B/26B.%20These%20results%0Apresent%20a%20novel%20pathway%20for%20MLLMs%20to%20handle%20dense%20visual%20tokens%20with%20minimal%0Acomputational%20costs.%20Code%20and%20model%20checkpoints%20will%20be%20released%20to%20support%0Afurther%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06169v2&entry.124074799=Read"},
{"title": "Fill in the blanks: Rethinking Interpretability in vision", "author": "Pathirage N. Deelaka and Tharindu Wickremasinghe and Devin Y. De Silva and Lisara N. Gajaweera", "abstract": "  Model interpretability is a key challenge that has yet to align with the\nadvancements observed in contemporary state-of-the-art deep learning models. In\nparticular, deep learning aided vision tasks require interpretability, in order\nfor their adoption in more specialized domains such as medical imaging.\nAlthough the field of explainable AI (XAI) developed methods for interpreting\nvision models along with early convolutional neural networks, recent XAI\nresearch has mainly focused on assigning attributes via saliency maps. As such,\nthese methods are restricted to providing explanations at a sample level, and\nmany explainability methods suffer from low adaptability across a wide range of\nvision models. In our work, we re-think vision-model explainability from a\nnovel perspective, to probe the general input structure that a model has learnt\nduring its training. To this end, we ask the question: \"How would a vision\nmodel fill-in a masked-image\". Experiments on standard vision datasets and\npre-trained models reveal consistent patterns, and could be intergrated as an\nadditional model-agnostic explainability tool in modern machine-learning\nplatforms. The code will be available at\n\\url{https://github.com/BoTZ-TND/FillingTheBlanks.git}\n", "link": "http://arxiv.org/abs/2411.10273v1", "date": "2024-11-15", "relevancy": 2.7582, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5643}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5643}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fill%20in%20the%20blanks%3A%20Rethinking%20Interpretability%20in%20vision&body=Title%3A%20Fill%20in%20the%20blanks%3A%20Rethinking%20Interpretability%20in%20vision%0AAuthor%3A%20Pathirage%20N.%20Deelaka%20and%20Tharindu%20Wickremasinghe%20and%20Devin%20Y.%20De%20Silva%20and%20Lisara%20N.%20Gajaweera%0AAbstract%3A%20%20%20Model%20interpretability%20is%20a%20key%20challenge%20that%20has%20yet%20to%20align%20with%20the%0Aadvancements%20observed%20in%20contemporary%20state-of-the-art%20deep%20learning%20models.%20In%0Aparticular%2C%20deep%20learning%20aided%20vision%20tasks%20require%20interpretability%2C%20in%20order%0Afor%20their%20adoption%20in%20more%20specialized%20domains%20such%20as%20medical%20imaging.%0AAlthough%20the%20field%20of%20explainable%20AI%20%28XAI%29%20developed%20methods%20for%20interpreting%0Avision%20models%20along%20with%20early%20convolutional%20neural%20networks%2C%20recent%20XAI%0Aresearch%20has%20mainly%20focused%20on%20assigning%20attributes%20via%20saliency%20maps.%20As%20such%2C%0Athese%20methods%20are%20restricted%20to%20providing%20explanations%20at%20a%20sample%20level%2C%20and%0Amany%20explainability%20methods%20suffer%20from%20low%20adaptability%20across%20a%20wide%20range%20of%0Avision%20models.%20In%20our%20work%2C%20we%20re-think%20vision-model%20explainability%20from%20a%0Anovel%20perspective%2C%20to%20probe%20the%20general%20input%20structure%20that%20a%20model%20has%20learnt%0Aduring%20its%20training.%20To%20this%20end%2C%20we%20ask%20the%20question%3A%20%22How%20would%20a%20vision%0Amodel%20fill-in%20a%20masked-image%22.%20Experiments%20on%20standard%20vision%20datasets%20and%0Apre-trained%20models%20reveal%20consistent%20patterns%2C%20and%20could%20be%20intergrated%20as%20an%0Aadditional%20model-agnostic%20explainability%20tool%20in%20modern%20machine-learning%0Aplatforms.%20The%20code%20will%20be%20available%20at%0A%5Curl%7Bhttps%3A//github.com/BoTZ-TND/FillingTheBlanks.git%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10273v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFill%2520in%2520the%2520blanks%253A%2520Rethinking%2520Interpretability%2520in%2520vision%26entry.906535625%3DPathirage%2520N.%2520Deelaka%2520and%2520Tharindu%2520Wickremasinghe%2520and%2520Devin%2520Y.%2520De%2520Silva%2520and%2520Lisara%2520N.%2520Gajaweera%26entry.1292438233%3D%2520%2520Model%2520interpretability%2520is%2520a%2520key%2520challenge%2520that%2520has%2520yet%2520to%2520align%2520with%2520the%250Aadvancements%2520observed%2520in%2520contemporary%2520state-of-the-art%2520deep%2520learning%2520models.%2520In%250Aparticular%252C%2520deep%2520learning%2520aided%2520vision%2520tasks%2520require%2520interpretability%252C%2520in%2520order%250Afor%2520their%2520adoption%2520in%2520more%2520specialized%2520domains%2520such%2520as%2520medical%2520imaging.%250AAlthough%2520the%2520field%2520of%2520explainable%2520AI%2520%2528XAI%2529%2520developed%2520methods%2520for%2520interpreting%250Avision%2520models%2520along%2520with%2520early%2520convolutional%2520neural%2520networks%252C%2520recent%2520XAI%250Aresearch%2520has%2520mainly%2520focused%2520on%2520assigning%2520attributes%2520via%2520saliency%2520maps.%2520As%2520such%252C%250Athese%2520methods%2520are%2520restricted%2520to%2520providing%2520explanations%2520at%2520a%2520sample%2520level%252C%2520and%250Amany%2520explainability%2520methods%2520suffer%2520from%2520low%2520adaptability%2520across%2520a%2520wide%2520range%2520of%250Avision%2520models.%2520In%2520our%2520work%252C%2520we%2520re-think%2520vision-model%2520explainability%2520from%2520a%250Anovel%2520perspective%252C%2520to%2520probe%2520the%2520general%2520input%2520structure%2520that%2520a%2520model%2520has%2520learnt%250Aduring%2520its%2520training.%2520To%2520this%2520end%252C%2520we%2520ask%2520the%2520question%253A%2520%2522How%2520would%2520a%2520vision%250Amodel%2520fill-in%2520a%2520masked-image%2522.%2520Experiments%2520on%2520standard%2520vision%2520datasets%2520and%250Apre-trained%2520models%2520reveal%2520consistent%2520patterns%252C%2520and%2520could%2520be%2520intergrated%2520as%2520an%250Aadditional%2520model-agnostic%2520explainability%2520tool%2520in%2520modern%2520machine-learning%250Aplatforms.%2520The%2520code%2520will%2520be%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/BoTZ-TND/FillingTheBlanks.git%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10273v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fill%20in%20the%20blanks%3A%20Rethinking%20Interpretability%20in%20vision&entry.906535625=Pathirage%20N.%20Deelaka%20and%20Tharindu%20Wickremasinghe%20and%20Devin%20Y.%20De%20Silva%20and%20Lisara%20N.%20Gajaweera&entry.1292438233=%20%20Model%20interpretability%20is%20a%20key%20challenge%20that%20has%20yet%20to%20align%20with%20the%0Aadvancements%20observed%20in%20contemporary%20state-of-the-art%20deep%20learning%20models.%20In%0Aparticular%2C%20deep%20learning%20aided%20vision%20tasks%20require%20interpretability%2C%20in%20order%0Afor%20their%20adoption%20in%20more%20specialized%20domains%20such%20as%20medical%20imaging.%0AAlthough%20the%20field%20of%20explainable%20AI%20%28XAI%29%20developed%20methods%20for%20interpreting%0Avision%20models%20along%20with%20early%20convolutional%20neural%20networks%2C%20recent%20XAI%0Aresearch%20has%20mainly%20focused%20on%20assigning%20attributes%20via%20saliency%20maps.%20As%20such%2C%0Athese%20methods%20are%20restricted%20to%20providing%20explanations%20at%20a%20sample%20level%2C%20and%0Amany%20explainability%20methods%20suffer%20from%20low%20adaptability%20across%20a%20wide%20range%20of%0Avision%20models.%20In%20our%20work%2C%20we%20re-think%20vision-model%20explainability%20from%20a%0Anovel%20perspective%2C%20to%20probe%20the%20general%20input%20structure%20that%20a%20model%20has%20learnt%0Aduring%20its%20training.%20To%20this%20end%2C%20we%20ask%20the%20question%3A%20%22How%20would%20a%20vision%0Amodel%20fill-in%20a%20masked-image%22.%20Experiments%20on%20standard%20vision%20datasets%20and%0Apre-trained%20models%20reveal%20consistent%20patterns%2C%20and%20could%20be%20intergrated%20as%20an%0Aadditional%20model-agnostic%20explainability%20tool%20in%20modern%20machine-learning%0Aplatforms.%20The%20code%20will%20be%20available%20at%0A%5Curl%7Bhttps%3A//github.com/BoTZ-TND/FillingTheBlanks.git%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10273v1&entry.124074799=Read"},
{"title": "RETR: Multi-View Radar Detection Transformer for Indoor Perception", "author": "Ryoma Yataka and Adriano Cardace and Pu Perry Wang and Petros Boufounos and Ryuhei Takahashi", "abstract": "  Indoor radar perception has seen rising interest due to affordable costs\ndriven by emerging automotive imaging radar developments and the benefits of\nreduced privacy concerns and reliability under hazardous conditions (e.g., fire\nand smoke). However, existing radar perception pipelines fail to account for\ndistinctive characteristics of the multi-view radar setting. In this paper, we\npropose Radar dEtection TRansformer (RETR), an extension of the popular DETR\narchitecture, tailored for multi-view radar perception. RETR inherits the\nadvantages of DETR, eliminating the need for hand-crafted components for object\ndetection and segmentation in the image plane. More importantly, RETR\nincorporates carefully designed modifications such as 1) depth-prioritized\nfeature similarity via a tunable positional encoding (TPE); 2) a tri-plane loss\nfrom both radar and camera coordinates; and 3) a learnable radar-to-camera\ntransformation via reparameterization, to account for the unique multi-view\nradar setting. Evaluated on two indoor radar perception datasets, our approach\noutperforms existing state-of-the-art methods by a margin of 15.38+ AP for\nobject detection and 11.77+ IoU for instance segmentation, respectively.\n", "link": "http://arxiv.org/abs/2411.10293v1", "date": "2024-11-15", "relevancy": 2.6881, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5561}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5344}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RETR%3A%20Multi-View%20Radar%20Detection%20Transformer%20for%20Indoor%20Perception&body=Title%3A%20RETR%3A%20Multi-View%20Radar%20Detection%20Transformer%20for%20Indoor%20Perception%0AAuthor%3A%20Ryoma%20Yataka%20and%20Adriano%20Cardace%20and%20Pu%20Perry%20Wang%20and%20Petros%20Boufounos%20and%20Ryuhei%20Takahashi%0AAbstract%3A%20%20%20Indoor%20radar%20perception%20has%20seen%20rising%20interest%20due%20to%20affordable%20costs%0Adriven%20by%20emerging%20automotive%20imaging%20radar%20developments%20and%20the%20benefits%20of%0Areduced%20privacy%20concerns%20and%20reliability%20under%20hazardous%20conditions%20%28e.g.%2C%20fire%0Aand%20smoke%29.%20However%2C%20existing%20radar%20perception%20pipelines%20fail%20to%20account%20for%0Adistinctive%20characteristics%20of%20the%20multi-view%20radar%20setting.%20In%20this%20paper%2C%20we%0Apropose%20Radar%20dEtection%20TRansformer%20%28RETR%29%2C%20an%20extension%20of%20the%20popular%20DETR%0Aarchitecture%2C%20tailored%20for%20multi-view%20radar%20perception.%20RETR%20inherits%20the%0Aadvantages%20of%20DETR%2C%20eliminating%20the%20need%20for%20hand-crafted%20components%20for%20object%0Adetection%20and%20segmentation%20in%20the%20image%20plane.%20More%20importantly%2C%20RETR%0Aincorporates%20carefully%20designed%20modifications%20such%20as%201%29%20depth-prioritized%0Afeature%20similarity%20via%20a%20tunable%20positional%20encoding%20%28TPE%29%3B%202%29%20a%20tri-plane%20loss%0Afrom%20both%20radar%20and%20camera%20coordinates%3B%20and%203%29%20a%20learnable%20radar-to-camera%0Atransformation%20via%20reparameterization%2C%20to%20account%20for%20the%20unique%20multi-view%0Aradar%20setting.%20Evaluated%20on%20two%20indoor%20radar%20perception%20datasets%2C%20our%20approach%0Aoutperforms%20existing%20state-of-the-art%20methods%20by%20a%20margin%20of%2015.38%2B%20AP%20for%0Aobject%20detection%20and%2011.77%2B%20IoU%20for%20instance%20segmentation%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10293v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRETR%253A%2520Multi-View%2520Radar%2520Detection%2520Transformer%2520for%2520Indoor%2520Perception%26entry.906535625%3DRyoma%2520Yataka%2520and%2520Adriano%2520Cardace%2520and%2520Pu%2520Perry%2520Wang%2520and%2520Petros%2520Boufounos%2520and%2520Ryuhei%2520Takahashi%26entry.1292438233%3D%2520%2520Indoor%2520radar%2520perception%2520has%2520seen%2520rising%2520interest%2520due%2520to%2520affordable%2520costs%250Adriven%2520by%2520emerging%2520automotive%2520imaging%2520radar%2520developments%2520and%2520the%2520benefits%2520of%250Areduced%2520privacy%2520concerns%2520and%2520reliability%2520under%2520hazardous%2520conditions%2520%2528e.g.%252C%2520fire%250Aand%2520smoke%2529.%2520However%252C%2520existing%2520radar%2520perception%2520pipelines%2520fail%2520to%2520account%2520for%250Adistinctive%2520characteristics%2520of%2520the%2520multi-view%2520radar%2520setting.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520Radar%2520dEtection%2520TRansformer%2520%2528RETR%2529%252C%2520an%2520extension%2520of%2520the%2520popular%2520DETR%250Aarchitecture%252C%2520tailored%2520for%2520multi-view%2520radar%2520perception.%2520RETR%2520inherits%2520the%250Aadvantages%2520of%2520DETR%252C%2520eliminating%2520the%2520need%2520for%2520hand-crafted%2520components%2520for%2520object%250Adetection%2520and%2520segmentation%2520in%2520the%2520image%2520plane.%2520More%2520importantly%252C%2520RETR%250Aincorporates%2520carefully%2520designed%2520modifications%2520such%2520as%25201%2529%2520depth-prioritized%250Afeature%2520similarity%2520via%2520a%2520tunable%2520positional%2520encoding%2520%2528TPE%2529%253B%25202%2529%2520a%2520tri-plane%2520loss%250Afrom%2520both%2520radar%2520and%2520camera%2520coordinates%253B%2520and%25203%2529%2520a%2520learnable%2520radar-to-camera%250Atransformation%2520via%2520reparameterization%252C%2520to%2520account%2520for%2520the%2520unique%2520multi-view%250Aradar%2520setting.%2520Evaluated%2520on%2520two%2520indoor%2520radar%2520perception%2520datasets%252C%2520our%2520approach%250Aoutperforms%2520existing%2520state-of-the-art%2520methods%2520by%2520a%2520margin%2520of%252015.38%252B%2520AP%2520for%250Aobject%2520detection%2520and%252011.77%252B%2520IoU%2520for%2520instance%2520segmentation%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10293v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RETR%3A%20Multi-View%20Radar%20Detection%20Transformer%20for%20Indoor%20Perception&entry.906535625=Ryoma%20Yataka%20and%20Adriano%20Cardace%20and%20Pu%20Perry%20Wang%20and%20Petros%20Boufounos%20and%20Ryuhei%20Takahashi&entry.1292438233=%20%20Indoor%20radar%20perception%20has%20seen%20rising%20interest%20due%20to%20affordable%20costs%0Adriven%20by%20emerging%20automotive%20imaging%20radar%20developments%20and%20the%20benefits%20of%0Areduced%20privacy%20concerns%20and%20reliability%20under%20hazardous%20conditions%20%28e.g.%2C%20fire%0Aand%20smoke%29.%20However%2C%20existing%20radar%20perception%20pipelines%20fail%20to%20account%20for%0Adistinctive%20characteristics%20of%20the%20multi-view%20radar%20setting.%20In%20this%20paper%2C%20we%0Apropose%20Radar%20dEtection%20TRansformer%20%28RETR%29%2C%20an%20extension%20of%20the%20popular%20DETR%0Aarchitecture%2C%20tailored%20for%20multi-view%20radar%20perception.%20RETR%20inherits%20the%0Aadvantages%20of%20DETR%2C%20eliminating%20the%20need%20for%20hand-crafted%20components%20for%20object%0Adetection%20and%20segmentation%20in%20the%20image%20plane.%20More%20importantly%2C%20RETR%0Aincorporates%20carefully%20designed%20modifications%20such%20as%201%29%20depth-prioritized%0Afeature%20similarity%20via%20a%20tunable%20positional%20encoding%20%28TPE%29%3B%202%29%20a%20tri-plane%20loss%0Afrom%20both%20radar%20and%20camera%20coordinates%3B%20and%203%29%20a%20learnable%20radar-to-camera%0Atransformation%20via%20reparameterization%2C%20to%20account%20for%20the%20unique%20multi-view%0Aradar%20setting.%20Evaluated%20on%20two%20indoor%20radar%20perception%20datasets%2C%20our%20approach%0Aoutperforms%20existing%20state-of-the-art%20methods%20by%20a%20margin%20of%2015.38%2B%20AP%20for%0Aobject%20detection%20and%2011.77%2B%20IoU%20for%20instance%20segmentation%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10293v1&entry.124074799=Read"},
{"title": "Features that Make a Difference: Leveraging Gradients for Improved\n  Dictionary Learning", "author": "Jeffrey Olmo and Jared Wilson and Max Forsey and Bryce Hepner and Thomas Vin Howe and David Wingate", "abstract": "  Sparse Autoencoders (SAEs) are a promising approach for extracting neural\nnetwork representations by learning a sparse and overcomplete decomposition of\nthe network's internal activations. However, SAEs are traditionally trained\nconsidering only activation values and not the effect those activations have on\ndownstream computations. This limits the information available to learn\nfeatures, and biases the autoencoder towards neglecting features which are\nrepresented with small activation values but strongly influence model outputs.\nTo address this, we introduce Gradient SAEs (g-SAEs), which modify the\n$k$-sparse autoencoder architecture by augmenting the TopK activation function\nto rely on the gradients of the input activation when selecting the $k$\nelements. For a given sparsity level, g-SAEs produce reconstructions that are\nmore faithful to original network performance when propagated through the\nnetwork. Additionally, we find evidence that g-SAEs learn latents that are on\naverage more effective at steering models in arbitrary contexts. By considering\nthe downstream effects of activations, our approach leverages the dual nature\nof neural network features as both $\\textit{representations}$, retrospectively,\nand $\\textit{actions}$, prospectively. While previous methods have approached\nthe problem of feature discovery primarily focused on the former aspect, g-SAEs\nrepresent a step towards accounting for the latter as well.\n", "link": "http://arxiv.org/abs/2411.10397v1", "date": "2024-11-15", "relevancy": 2.6218, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5499}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5206}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Features%20that%20Make%20a%20Difference%3A%20Leveraging%20Gradients%20for%20Improved%0A%20%20Dictionary%20Learning&body=Title%3A%20Features%20that%20Make%20a%20Difference%3A%20Leveraging%20Gradients%20for%20Improved%0A%20%20Dictionary%20Learning%0AAuthor%3A%20Jeffrey%20Olmo%20and%20Jared%20Wilson%20and%20Max%20Forsey%20and%20Bryce%20Hepner%20and%20Thomas%20Vin%20Howe%20and%20David%20Wingate%0AAbstract%3A%20%20%20Sparse%20Autoencoders%20%28SAEs%29%20are%20a%20promising%20approach%20for%20extracting%20neural%0Anetwork%20representations%20by%20learning%20a%20sparse%20and%20overcomplete%20decomposition%20of%0Athe%20network%27s%20internal%20activations.%20However%2C%20SAEs%20are%20traditionally%20trained%0Aconsidering%20only%20activation%20values%20and%20not%20the%20effect%20those%20activations%20have%20on%0Adownstream%20computations.%20This%20limits%20the%20information%20available%20to%20learn%0Afeatures%2C%20and%20biases%20the%20autoencoder%20towards%20neglecting%20features%20which%20are%0Arepresented%20with%20small%20activation%20values%20but%20strongly%20influence%20model%20outputs.%0ATo%20address%20this%2C%20we%20introduce%20Gradient%20SAEs%20%28g-SAEs%29%2C%20which%20modify%20the%0A%24k%24-sparse%20autoencoder%20architecture%20by%20augmenting%20the%20TopK%20activation%20function%0Ato%20rely%20on%20the%20gradients%20of%20the%20input%20activation%20when%20selecting%20the%20%24k%24%0Aelements.%20For%20a%20given%20sparsity%20level%2C%20g-SAEs%20produce%20reconstructions%20that%20are%0Amore%20faithful%20to%20original%20network%20performance%20when%20propagated%20through%20the%0Anetwork.%20Additionally%2C%20we%20find%20evidence%20that%20g-SAEs%20learn%20latents%20that%20are%20on%0Aaverage%20more%20effective%20at%20steering%20models%20in%20arbitrary%20contexts.%20By%20considering%0Athe%20downstream%20effects%20of%20activations%2C%20our%20approach%20leverages%20the%20dual%20nature%0Aof%20neural%20network%20features%20as%20both%20%24%5Ctextit%7Brepresentations%7D%24%2C%20retrospectively%2C%0Aand%20%24%5Ctextit%7Bactions%7D%24%2C%20prospectively.%20While%20previous%20methods%20have%20approached%0Athe%20problem%20of%20feature%20discovery%20primarily%20focused%20on%20the%20former%20aspect%2C%20g-SAEs%0Arepresent%20a%20step%20towards%20accounting%20for%20the%20latter%20as%20well.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10397v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeatures%2520that%2520Make%2520a%2520Difference%253A%2520Leveraging%2520Gradients%2520for%2520Improved%250A%2520%2520Dictionary%2520Learning%26entry.906535625%3DJeffrey%2520Olmo%2520and%2520Jared%2520Wilson%2520and%2520Max%2520Forsey%2520and%2520Bryce%2520Hepner%2520and%2520Thomas%2520Vin%2520Howe%2520and%2520David%2520Wingate%26entry.1292438233%3D%2520%2520Sparse%2520Autoencoders%2520%2528SAEs%2529%2520are%2520a%2520promising%2520approach%2520for%2520extracting%2520neural%250Anetwork%2520representations%2520by%2520learning%2520a%2520sparse%2520and%2520overcomplete%2520decomposition%2520of%250Athe%2520network%2527s%2520internal%2520activations.%2520However%252C%2520SAEs%2520are%2520traditionally%2520trained%250Aconsidering%2520only%2520activation%2520values%2520and%2520not%2520the%2520effect%2520those%2520activations%2520have%2520on%250Adownstream%2520computations.%2520This%2520limits%2520the%2520information%2520available%2520to%2520learn%250Afeatures%252C%2520and%2520biases%2520the%2520autoencoder%2520towards%2520neglecting%2520features%2520which%2520are%250Arepresented%2520with%2520small%2520activation%2520values%2520but%2520strongly%2520influence%2520model%2520outputs.%250ATo%2520address%2520this%252C%2520we%2520introduce%2520Gradient%2520SAEs%2520%2528g-SAEs%2529%252C%2520which%2520modify%2520the%250A%2524k%2524-sparse%2520autoencoder%2520architecture%2520by%2520augmenting%2520the%2520TopK%2520activation%2520function%250Ato%2520rely%2520on%2520the%2520gradients%2520of%2520the%2520input%2520activation%2520when%2520selecting%2520the%2520%2524k%2524%250Aelements.%2520For%2520a%2520given%2520sparsity%2520level%252C%2520g-SAEs%2520produce%2520reconstructions%2520that%2520are%250Amore%2520faithful%2520to%2520original%2520network%2520performance%2520when%2520propagated%2520through%2520the%250Anetwork.%2520Additionally%252C%2520we%2520find%2520evidence%2520that%2520g-SAEs%2520learn%2520latents%2520that%2520are%2520on%250Aaverage%2520more%2520effective%2520at%2520steering%2520models%2520in%2520arbitrary%2520contexts.%2520By%2520considering%250Athe%2520downstream%2520effects%2520of%2520activations%252C%2520our%2520approach%2520leverages%2520the%2520dual%2520nature%250Aof%2520neural%2520network%2520features%2520as%2520both%2520%2524%255Ctextit%257Brepresentations%257D%2524%252C%2520retrospectively%252C%250Aand%2520%2524%255Ctextit%257Bactions%257D%2524%252C%2520prospectively.%2520While%2520previous%2520methods%2520have%2520approached%250Athe%2520problem%2520of%2520feature%2520discovery%2520primarily%2520focused%2520on%2520the%2520former%2520aspect%252C%2520g-SAEs%250Arepresent%2520a%2520step%2520towards%2520accounting%2520for%2520the%2520latter%2520as%2520well.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10397v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Features%20that%20Make%20a%20Difference%3A%20Leveraging%20Gradients%20for%20Improved%0A%20%20Dictionary%20Learning&entry.906535625=Jeffrey%20Olmo%20and%20Jared%20Wilson%20and%20Max%20Forsey%20and%20Bryce%20Hepner%20and%20Thomas%20Vin%20Howe%20and%20David%20Wingate&entry.1292438233=%20%20Sparse%20Autoencoders%20%28SAEs%29%20are%20a%20promising%20approach%20for%20extracting%20neural%0Anetwork%20representations%20by%20learning%20a%20sparse%20and%20overcomplete%20decomposition%20of%0Athe%20network%27s%20internal%20activations.%20However%2C%20SAEs%20are%20traditionally%20trained%0Aconsidering%20only%20activation%20values%20and%20not%20the%20effect%20those%20activations%20have%20on%0Adownstream%20computations.%20This%20limits%20the%20information%20available%20to%20learn%0Afeatures%2C%20and%20biases%20the%20autoencoder%20towards%20neglecting%20features%20which%20are%0Arepresented%20with%20small%20activation%20values%20but%20strongly%20influence%20model%20outputs.%0ATo%20address%20this%2C%20we%20introduce%20Gradient%20SAEs%20%28g-SAEs%29%2C%20which%20modify%20the%0A%24k%24-sparse%20autoencoder%20architecture%20by%20augmenting%20the%20TopK%20activation%20function%0Ato%20rely%20on%20the%20gradients%20of%20the%20input%20activation%20when%20selecting%20the%20%24k%24%0Aelements.%20For%20a%20given%20sparsity%20level%2C%20g-SAEs%20produce%20reconstructions%20that%20are%0Amore%20faithful%20to%20original%20network%20performance%20when%20propagated%20through%20the%0Anetwork.%20Additionally%2C%20we%20find%20evidence%20that%20g-SAEs%20learn%20latents%20that%20are%20on%0Aaverage%20more%20effective%20at%20steering%20models%20in%20arbitrary%20contexts.%20By%20considering%0Athe%20downstream%20effects%20of%20activations%2C%20our%20approach%20leverages%20the%20dual%20nature%0Aof%20neural%20network%20features%20as%20both%20%24%5Ctextit%7Brepresentations%7D%24%2C%20retrospectively%2C%0Aand%20%24%5Ctextit%7Bactions%7D%24%2C%20prospectively.%20While%20previous%20methods%20have%20approached%0Athe%20problem%20of%20feature%20discovery%20primarily%20focused%20on%20the%20former%20aspect%2C%20g-SAEs%0Arepresent%20a%20step%20towards%20accounting%20for%20the%20latter%20as%20well.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10397v1&entry.124074799=Read"},
{"title": "Partial Scene Text Retrieval", "author": "Hao Wang and Minghui Liao and Zhouyi Xie and Wenyu Liu and Xiang Bai", "abstract": "  The task of partial scene text retrieval involves localizing and searching\nfor text instances that are the same or similar to a given query text from an\nimage gallery. However, existing methods can only handle text-line instances,\nleaving the problem of searching for partial patches within these text-line\ninstances unsolved due to a lack of patch annotations in the training data. To\naddress this issue, we propose a network that can simultaneously retrieve both\ntext-line instances and their partial patches. Our method embeds the two types\nof data (query text and scene text instances) into a shared feature space and\nmeasures their cross-modal similarities. To handle partial patches, our\nproposed approach adopts a Multiple Instance Learning (MIL) approach to learn\ntheir similarities with query text, without requiring extra annotations.\nHowever, constructing bags, which is a standard step of conventional MIL\napproaches, can introduce numerous noisy samples for training, and lower\ninference speed. To address this issue, we propose a Ranking MIL (RankMIL)\napproach to adaptively filter those noisy samples. Additionally, we present a\nDynamic Partial Match Algorithm (DPMA) that can directly search for the target\npartial patch from a text-line instance during the inference stage, without\nrequiring bags. This greatly improves the search efficiency and the performance\nof retrieving partial patches. The source code and dataset are available at\nhttps://github.com/lanfeng4659/PSTR.\n", "link": "http://arxiv.org/abs/2411.10261v1", "date": "2024-11-15", "relevancy": 2.6059, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5232}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5232}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Partial%20Scene%20Text%20Retrieval&body=Title%3A%20Partial%20Scene%20Text%20Retrieval%0AAuthor%3A%20Hao%20Wang%20and%20Minghui%20Liao%20and%20Zhouyi%20Xie%20and%20Wenyu%20Liu%20and%20Xiang%20Bai%0AAbstract%3A%20%20%20The%20task%20of%20partial%20scene%20text%20retrieval%20involves%20localizing%20and%20searching%0Afor%20text%20instances%20that%20are%20the%20same%20or%20similar%20to%20a%20given%20query%20text%20from%20an%0Aimage%20gallery.%20However%2C%20existing%20methods%20can%20only%20handle%20text-line%20instances%2C%0Aleaving%20the%20problem%20of%20searching%20for%20partial%20patches%20within%20these%20text-line%0Ainstances%20unsolved%20due%20to%20a%20lack%20of%20patch%20annotations%20in%20the%20training%20data.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20a%20network%20that%20can%20simultaneously%20retrieve%20both%0Atext-line%20instances%20and%20their%20partial%20patches.%20Our%20method%20embeds%20the%20two%20types%0Aof%20data%20%28query%20text%20and%20scene%20text%20instances%29%20into%20a%20shared%20feature%20space%20and%0Ameasures%20their%20cross-modal%20similarities.%20To%20handle%20partial%20patches%2C%20our%0Aproposed%20approach%20adopts%20a%20Multiple%20Instance%20Learning%20%28MIL%29%20approach%20to%20learn%0Atheir%20similarities%20with%20query%20text%2C%20without%20requiring%20extra%20annotations.%0AHowever%2C%20constructing%20bags%2C%20which%20is%20a%20standard%20step%20of%20conventional%20MIL%0Aapproaches%2C%20can%20introduce%20numerous%20noisy%20samples%20for%20training%2C%20and%20lower%0Ainference%20speed.%20To%20address%20this%20issue%2C%20we%20propose%20a%20Ranking%20MIL%20%28RankMIL%29%0Aapproach%20to%20adaptively%20filter%20those%20noisy%20samples.%20Additionally%2C%20we%20present%20a%0ADynamic%20Partial%20Match%20Algorithm%20%28DPMA%29%20that%20can%20directly%20search%20for%20the%20target%0Apartial%20patch%20from%20a%20text-line%20instance%20during%20the%20inference%20stage%2C%20without%0Arequiring%20bags.%20This%20greatly%20improves%20the%20search%20efficiency%20and%20the%20performance%0Aof%20retrieving%20partial%20patches.%20The%20source%20code%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/lanfeng4659/PSTR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10261v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPartial%2520Scene%2520Text%2520Retrieval%26entry.906535625%3DHao%2520Wang%2520and%2520Minghui%2520Liao%2520and%2520Zhouyi%2520Xie%2520and%2520Wenyu%2520Liu%2520and%2520Xiang%2520Bai%26entry.1292438233%3D%2520%2520The%2520task%2520of%2520partial%2520scene%2520text%2520retrieval%2520involves%2520localizing%2520and%2520searching%250Afor%2520text%2520instances%2520that%2520are%2520the%2520same%2520or%2520similar%2520to%2520a%2520given%2520query%2520text%2520from%2520an%250Aimage%2520gallery.%2520However%252C%2520existing%2520methods%2520can%2520only%2520handle%2520text-line%2520instances%252C%250Aleaving%2520the%2520problem%2520of%2520searching%2520for%2520partial%2520patches%2520within%2520these%2520text-line%250Ainstances%2520unsolved%2520due%2520to%2520a%2520lack%2520of%2520patch%2520annotations%2520in%2520the%2520training%2520data.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520propose%2520a%2520network%2520that%2520can%2520simultaneously%2520retrieve%2520both%250Atext-line%2520instances%2520and%2520their%2520partial%2520patches.%2520Our%2520method%2520embeds%2520the%2520two%2520types%250Aof%2520data%2520%2528query%2520text%2520and%2520scene%2520text%2520instances%2529%2520into%2520a%2520shared%2520feature%2520space%2520and%250Ameasures%2520their%2520cross-modal%2520similarities.%2520To%2520handle%2520partial%2520patches%252C%2520our%250Aproposed%2520approach%2520adopts%2520a%2520Multiple%2520Instance%2520Learning%2520%2528MIL%2529%2520approach%2520to%2520learn%250Atheir%2520similarities%2520with%2520query%2520text%252C%2520without%2520requiring%2520extra%2520annotations.%250AHowever%252C%2520constructing%2520bags%252C%2520which%2520is%2520a%2520standard%2520step%2520of%2520conventional%2520MIL%250Aapproaches%252C%2520can%2520introduce%2520numerous%2520noisy%2520samples%2520for%2520training%252C%2520and%2520lower%250Ainference%2520speed.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520Ranking%2520MIL%2520%2528RankMIL%2529%250Aapproach%2520to%2520adaptively%2520filter%2520those%2520noisy%2520samples.%2520Additionally%252C%2520we%2520present%2520a%250ADynamic%2520Partial%2520Match%2520Algorithm%2520%2528DPMA%2529%2520that%2520can%2520directly%2520search%2520for%2520the%2520target%250Apartial%2520patch%2520from%2520a%2520text-line%2520instance%2520during%2520the%2520inference%2520stage%252C%2520without%250Arequiring%2520bags.%2520This%2520greatly%2520improves%2520the%2520search%2520efficiency%2520and%2520the%2520performance%250Aof%2520retrieving%2520partial%2520patches.%2520The%2520source%2520code%2520and%2520dataset%2520are%2520available%2520at%250Ahttps%253A//github.com/lanfeng4659/PSTR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10261v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Partial%20Scene%20Text%20Retrieval&entry.906535625=Hao%20Wang%20and%20Minghui%20Liao%20and%20Zhouyi%20Xie%20and%20Wenyu%20Liu%20and%20Xiang%20Bai&entry.1292438233=%20%20The%20task%20of%20partial%20scene%20text%20retrieval%20involves%20localizing%20and%20searching%0Afor%20text%20instances%20that%20are%20the%20same%20or%20similar%20to%20a%20given%20query%20text%20from%20an%0Aimage%20gallery.%20However%2C%20existing%20methods%20can%20only%20handle%20text-line%20instances%2C%0Aleaving%20the%20problem%20of%20searching%20for%20partial%20patches%20within%20these%20text-line%0Ainstances%20unsolved%20due%20to%20a%20lack%20of%20patch%20annotations%20in%20the%20training%20data.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20a%20network%20that%20can%20simultaneously%20retrieve%20both%0Atext-line%20instances%20and%20their%20partial%20patches.%20Our%20method%20embeds%20the%20two%20types%0Aof%20data%20%28query%20text%20and%20scene%20text%20instances%29%20into%20a%20shared%20feature%20space%20and%0Ameasures%20their%20cross-modal%20similarities.%20To%20handle%20partial%20patches%2C%20our%0Aproposed%20approach%20adopts%20a%20Multiple%20Instance%20Learning%20%28MIL%29%20approach%20to%20learn%0Atheir%20similarities%20with%20query%20text%2C%20without%20requiring%20extra%20annotations.%0AHowever%2C%20constructing%20bags%2C%20which%20is%20a%20standard%20step%20of%20conventional%20MIL%0Aapproaches%2C%20can%20introduce%20numerous%20noisy%20samples%20for%20training%2C%20and%20lower%0Ainference%20speed.%20To%20address%20this%20issue%2C%20we%20propose%20a%20Ranking%20MIL%20%28RankMIL%29%0Aapproach%20to%20adaptively%20filter%20those%20noisy%20samples.%20Additionally%2C%20we%20present%20a%0ADynamic%20Partial%20Match%20Algorithm%20%28DPMA%29%20that%20can%20directly%20search%20for%20the%20target%0Apartial%20patch%20from%20a%20text-line%20instance%20during%20the%20inference%20stage%2C%20without%0Arequiring%20bags.%20This%20greatly%20improves%20the%20search%20efficiency%20and%20the%20performance%0Aof%20retrieving%20partial%20patches.%20The%20source%20code%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/lanfeng4659/PSTR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10261v1&entry.124074799=Read"},
{"title": "Mitigating the Linguistic Gap with Phonemic Representations for Robust\n  Cross-lingual Transfer", "author": "Haeji Jung and Changdae Oh and Jooeon Kang and Jimin Sohn and Kyungwoo Song and Jinkyu Kim and David R. Mortensen", "abstract": "  Approaches to improving multilingual language understanding often struggle\nwith significant performance gaps between high-resource and low-resource\nlanguages. While there are efforts to align the languages in a single latent\nspace to mitigate such gaps, how different input-level representations\ninfluence such gaps has not been investigated, particularly with phonemic\ninputs. We hypothesize that the performance gaps are affected by representation\ndiscrepancies between these languages, and revisit the use of phonemic\nrepresentations as a means to mitigate these discrepancies. To demonstrate the\neffectiveness of phonemic representations, we present experiments on three\nrepresentative cross-lingual tasks on 12 languages in total. The results show\nthat phonemic representations exhibit higher similarities between languages\ncompared to orthographic representations, and it consistently outperforms\ngrapheme-based baseline model on languages that are relatively low-resourced.\nWe present quantitative evidence from three cross-lingual tasks that\ndemonstrate the effectiveness of phonemic representations, and it is further\njustified by a theoretical analysis of the cross-lingual performance gap.\n", "link": "http://arxiv.org/abs/2402.14279v3", "date": "2024-11-15", "relevancy": 2.5832, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5311}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5094}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20the%20Linguistic%20Gap%20with%20Phonemic%20Representations%20for%20Robust%0A%20%20Cross-lingual%20Transfer&body=Title%3A%20Mitigating%20the%20Linguistic%20Gap%20with%20Phonemic%20Representations%20for%20Robust%0A%20%20Cross-lingual%20Transfer%0AAuthor%3A%20Haeji%20Jung%20and%20Changdae%20Oh%20and%20Jooeon%20Kang%20and%20Jimin%20Sohn%20and%20Kyungwoo%20Song%20and%20Jinkyu%20Kim%20and%20David%20R.%20Mortensen%0AAbstract%3A%20%20%20Approaches%20to%20improving%20multilingual%20language%20understanding%20often%20struggle%0Awith%20significant%20performance%20gaps%20between%20high-resource%20and%20low-resource%0Alanguages.%20While%20there%20are%20efforts%20to%20align%20the%20languages%20in%20a%20single%20latent%0Aspace%20to%20mitigate%20such%20gaps%2C%20how%20different%20input-level%20representations%0Ainfluence%20such%20gaps%20has%20not%20been%20investigated%2C%20particularly%20with%20phonemic%0Ainputs.%20We%20hypothesize%20that%20the%20performance%20gaps%20are%20affected%20by%20representation%0Adiscrepancies%20between%20these%20languages%2C%20and%20revisit%20the%20use%20of%20phonemic%0Arepresentations%20as%20a%20means%20to%20mitigate%20these%20discrepancies.%20To%20demonstrate%20the%0Aeffectiveness%20of%20phonemic%20representations%2C%20we%20present%20experiments%20on%20three%0Arepresentative%20cross-lingual%20tasks%20on%2012%20languages%20in%20total.%20The%20results%20show%0Athat%20phonemic%20representations%20exhibit%20higher%20similarities%20between%20languages%0Acompared%20to%20orthographic%20representations%2C%20and%20it%20consistently%20outperforms%0Agrapheme-based%20baseline%20model%20on%20languages%20that%20are%20relatively%20low-resourced.%0AWe%20present%20quantitative%20evidence%20from%20three%20cross-lingual%20tasks%20that%0Ademonstrate%20the%20effectiveness%20of%20phonemic%20representations%2C%20and%20it%20is%20further%0Ajustified%20by%20a%20theoretical%20analysis%20of%20the%20cross-lingual%20performance%20gap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14279v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520the%2520Linguistic%2520Gap%2520with%2520Phonemic%2520Representations%2520for%2520Robust%250A%2520%2520Cross-lingual%2520Transfer%26entry.906535625%3DHaeji%2520Jung%2520and%2520Changdae%2520Oh%2520and%2520Jooeon%2520Kang%2520and%2520Jimin%2520Sohn%2520and%2520Kyungwoo%2520Song%2520and%2520Jinkyu%2520Kim%2520and%2520David%2520R.%2520Mortensen%26entry.1292438233%3D%2520%2520Approaches%2520to%2520improving%2520multilingual%2520language%2520understanding%2520often%2520struggle%250Awith%2520significant%2520performance%2520gaps%2520between%2520high-resource%2520and%2520low-resource%250Alanguages.%2520While%2520there%2520are%2520efforts%2520to%2520align%2520the%2520languages%2520in%2520a%2520single%2520latent%250Aspace%2520to%2520mitigate%2520such%2520gaps%252C%2520how%2520different%2520input-level%2520representations%250Ainfluence%2520such%2520gaps%2520has%2520not%2520been%2520investigated%252C%2520particularly%2520with%2520phonemic%250Ainputs.%2520We%2520hypothesize%2520that%2520the%2520performance%2520gaps%2520are%2520affected%2520by%2520representation%250Adiscrepancies%2520between%2520these%2520languages%252C%2520and%2520revisit%2520the%2520use%2520of%2520phonemic%250Arepresentations%2520as%2520a%2520means%2520to%2520mitigate%2520these%2520discrepancies.%2520To%2520demonstrate%2520the%250Aeffectiveness%2520of%2520phonemic%2520representations%252C%2520we%2520present%2520experiments%2520on%2520three%250Arepresentative%2520cross-lingual%2520tasks%2520on%252012%2520languages%2520in%2520total.%2520The%2520results%2520show%250Athat%2520phonemic%2520representations%2520exhibit%2520higher%2520similarities%2520between%2520languages%250Acompared%2520to%2520orthographic%2520representations%252C%2520and%2520it%2520consistently%2520outperforms%250Agrapheme-based%2520baseline%2520model%2520on%2520languages%2520that%2520are%2520relatively%2520low-resourced.%250AWe%2520present%2520quantitative%2520evidence%2520from%2520three%2520cross-lingual%2520tasks%2520that%250Ademonstrate%2520the%2520effectiveness%2520of%2520phonemic%2520representations%252C%2520and%2520it%2520is%2520further%250Ajustified%2520by%2520a%2520theoretical%2520analysis%2520of%2520the%2520cross-lingual%2520performance%2520gap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14279v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20the%20Linguistic%20Gap%20with%20Phonemic%20Representations%20for%20Robust%0A%20%20Cross-lingual%20Transfer&entry.906535625=Haeji%20Jung%20and%20Changdae%20Oh%20and%20Jooeon%20Kang%20and%20Jimin%20Sohn%20and%20Kyungwoo%20Song%20and%20Jinkyu%20Kim%20and%20David%20R.%20Mortensen&entry.1292438233=%20%20Approaches%20to%20improving%20multilingual%20language%20understanding%20often%20struggle%0Awith%20significant%20performance%20gaps%20between%20high-resource%20and%20low-resource%0Alanguages.%20While%20there%20are%20efforts%20to%20align%20the%20languages%20in%20a%20single%20latent%0Aspace%20to%20mitigate%20such%20gaps%2C%20how%20different%20input-level%20representations%0Ainfluence%20such%20gaps%20has%20not%20been%20investigated%2C%20particularly%20with%20phonemic%0Ainputs.%20We%20hypothesize%20that%20the%20performance%20gaps%20are%20affected%20by%20representation%0Adiscrepancies%20between%20these%20languages%2C%20and%20revisit%20the%20use%20of%20phonemic%0Arepresentations%20as%20a%20means%20to%20mitigate%20these%20discrepancies.%20To%20demonstrate%20the%0Aeffectiveness%20of%20phonemic%20representations%2C%20we%20present%20experiments%20on%20three%0Arepresentative%20cross-lingual%20tasks%20on%2012%20languages%20in%20total.%20The%20results%20show%0Athat%20phonemic%20representations%20exhibit%20higher%20similarities%20between%20languages%0Acompared%20to%20orthographic%20representations%2C%20and%20it%20consistently%20outperforms%0Agrapheme-based%20baseline%20model%20on%20languages%20that%20are%20relatively%20low-resourced.%0AWe%20present%20quantitative%20evidence%20from%20three%20cross-lingual%20tasks%20that%0Ademonstrate%20the%20effectiveness%20of%20phonemic%20representations%2C%20and%20it%20is%20further%0Ajustified%20by%20a%20theoretical%20analysis%20of%20the%20cross-lingual%20performance%20gap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14279v3&entry.124074799=Read"},
{"title": "Back to Supervision: Boosting Word Boundary Detection through Frame\n  Classification", "author": "Simone Carnemolla and Salvatore Calcagno and Simone Palazzo and Daniela Giordano", "abstract": "  Speech segmentation at both word and phoneme levels is crucial for various\nspeech processing tasks. It significantly aids in extracting meaningful units\nfrom an utterance, thus enabling the generation of discrete elements. In this\nwork we propose a model-agnostic framework to perform word boundary detection\nin a supervised manner also employing a labels augmentation technique and an\noutput-frame selection strategy. We trained and tested on the Buckeye dataset\nand only tested on TIMIT one, using state-of-the-art encoder models, including\npre-trained solutions (Wav2Vec 2.0 and HuBERT), as well as convolutional and\nconvolutional recurrent networks. Our method, with the HuBERT encoder,\nsurpasses the performance of other state-of-the-art architectures, whether\ntrained in supervised or self-supervised settings on the same datasets.\nSpecifically, we achieved F-values of 0.8427 on the Buckeye dataset and 0.7436\non the TIMIT dataset, along with R-values of 0.8489 and 0.7807, respectively.\nThese results establish a new state-of-the-art for both datasets. Beyond the\nimmediate task, our approach offers a robust and efficient preprocessing method\nfor future research in audio tokenization.\n", "link": "http://arxiv.org/abs/2411.10423v1", "date": "2024-11-15", "relevancy": 2.5725, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.533}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5237}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Back%20to%20Supervision%3A%20Boosting%20Word%20Boundary%20Detection%20through%20Frame%0A%20%20Classification&body=Title%3A%20Back%20to%20Supervision%3A%20Boosting%20Word%20Boundary%20Detection%20through%20Frame%0A%20%20Classification%0AAuthor%3A%20Simone%20Carnemolla%20and%20Salvatore%20Calcagno%20and%20Simone%20Palazzo%20and%20Daniela%20Giordano%0AAbstract%3A%20%20%20Speech%20segmentation%20at%20both%20word%20and%20phoneme%20levels%20is%20crucial%20for%20various%0Aspeech%20processing%20tasks.%20It%20significantly%20aids%20in%20extracting%20meaningful%20units%0Afrom%20an%20utterance%2C%20thus%20enabling%20the%20generation%20of%20discrete%20elements.%20In%20this%0Awork%20we%20propose%20a%20model-agnostic%20framework%20to%20perform%20word%20boundary%20detection%0Ain%20a%20supervised%20manner%20also%20employing%20a%20labels%20augmentation%20technique%20and%20an%0Aoutput-frame%20selection%20strategy.%20We%20trained%20and%20tested%20on%20the%20Buckeye%20dataset%0Aand%20only%20tested%20on%20TIMIT%20one%2C%20using%20state-of-the-art%20encoder%20models%2C%20including%0Apre-trained%20solutions%20%28Wav2Vec%202.0%20and%20HuBERT%29%2C%20as%20well%20as%20convolutional%20and%0Aconvolutional%20recurrent%20networks.%20Our%20method%2C%20with%20the%20HuBERT%20encoder%2C%0Asurpasses%20the%20performance%20of%20other%20state-of-the-art%20architectures%2C%20whether%0Atrained%20in%20supervised%20or%20self-supervised%20settings%20on%20the%20same%20datasets.%0ASpecifically%2C%20we%20achieved%20F-values%20of%200.8427%20on%20the%20Buckeye%20dataset%20and%200.7436%0Aon%20the%20TIMIT%20dataset%2C%20along%20with%20R-values%20of%200.8489%20and%200.7807%2C%20respectively.%0AThese%20results%20establish%20a%20new%20state-of-the-art%20for%20both%20datasets.%20Beyond%20the%0Aimmediate%20task%2C%20our%20approach%20offers%20a%20robust%20and%20efficient%20preprocessing%20method%0Afor%20future%20research%20in%20audio%20tokenization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10423v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBack%2520to%2520Supervision%253A%2520Boosting%2520Word%2520Boundary%2520Detection%2520through%2520Frame%250A%2520%2520Classification%26entry.906535625%3DSimone%2520Carnemolla%2520and%2520Salvatore%2520Calcagno%2520and%2520Simone%2520Palazzo%2520and%2520Daniela%2520Giordano%26entry.1292438233%3D%2520%2520Speech%2520segmentation%2520at%2520both%2520word%2520and%2520phoneme%2520levels%2520is%2520crucial%2520for%2520various%250Aspeech%2520processing%2520tasks.%2520It%2520significantly%2520aids%2520in%2520extracting%2520meaningful%2520units%250Afrom%2520an%2520utterance%252C%2520thus%2520enabling%2520the%2520generation%2520of%2520discrete%2520elements.%2520In%2520this%250Awork%2520we%2520propose%2520a%2520model-agnostic%2520framework%2520to%2520perform%2520word%2520boundary%2520detection%250Ain%2520a%2520supervised%2520manner%2520also%2520employing%2520a%2520labels%2520augmentation%2520technique%2520and%2520an%250Aoutput-frame%2520selection%2520strategy.%2520We%2520trained%2520and%2520tested%2520on%2520the%2520Buckeye%2520dataset%250Aand%2520only%2520tested%2520on%2520TIMIT%2520one%252C%2520using%2520state-of-the-art%2520encoder%2520models%252C%2520including%250Apre-trained%2520solutions%2520%2528Wav2Vec%25202.0%2520and%2520HuBERT%2529%252C%2520as%2520well%2520as%2520convolutional%2520and%250Aconvolutional%2520recurrent%2520networks.%2520Our%2520method%252C%2520with%2520the%2520HuBERT%2520encoder%252C%250Asurpasses%2520the%2520performance%2520of%2520other%2520state-of-the-art%2520architectures%252C%2520whether%250Atrained%2520in%2520supervised%2520or%2520self-supervised%2520settings%2520on%2520the%2520same%2520datasets.%250ASpecifically%252C%2520we%2520achieved%2520F-values%2520of%25200.8427%2520on%2520the%2520Buckeye%2520dataset%2520and%25200.7436%250Aon%2520the%2520TIMIT%2520dataset%252C%2520along%2520with%2520R-values%2520of%25200.8489%2520and%25200.7807%252C%2520respectively.%250AThese%2520results%2520establish%2520a%2520new%2520state-of-the-art%2520for%2520both%2520datasets.%2520Beyond%2520the%250Aimmediate%2520task%252C%2520our%2520approach%2520offers%2520a%2520robust%2520and%2520efficient%2520preprocessing%2520method%250Afor%2520future%2520research%2520in%2520audio%2520tokenization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10423v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Back%20to%20Supervision%3A%20Boosting%20Word%20Boundary%20Detection%20through%20Frame%0A%20%20Classification&entry.906535625=Simone%20Carnemolla%20and%20Salvatore%20Calcagno%20and%20Simone%20Palazzo%20and%20Daniela%20Giordano&entry.1292438233=%20%20Speech%20segmentation%20at%20both%20word%20and%20phoneme%20levels%20is%20crucial%20for%20various%0Aspeech%20processing%20tasks.%20It%20significantly%20aids%20in%20extracting%20meaningful%20units%0Afrom%20an%20utterance%2C%20thus%20enabling%20the%20generation%20of%20discrete%20elements.%20In%20this%0Awork%20we%20propose%20a%20model-agnostic%20framework%20to%20perform%20word%20boundary%20detection%0Ain%20a%20supervised%20manner%20also%20employing%20a%20labels%20augmentation%20technique%20and%20an%0Aoutput-frame%20selection%20strategy.%20We%20trained%20and%20tested%20on%20the%20Buckeye%20dataset%0Aand%20only%20tested%20on%20TIMIT%20one%2C%20using%20state-of-the-art%20encoder%20models%2C%20including%0Apre-trained%20solutions%20%28Wav2Vec%202.0%20and%20HuBERT%29%2C%20as%20well%20as%20convolutional%20and%0Aconvolutional%20recurrent%20networks.%20Our%20method%2C%20with%20the%20HuBERT%20encoder%2C%0Asurpasses%20the%20performance%20of%20other%20state-of-the-art%20architectures%2C%20whether%0Atrained%20in%20supervised%20or%20self-supervised%20settings%20on%20the%20same%20datasets.%0ASpecifically%2C%20we%20achieved%20F-values%20of%200.8427%20on%20the%20Buckeye%20dataset%20and%200.7436%0Aon%20the%20TIMIT%20dataset%2C%20along%20with%20R-values%20of%200.8489%20and%200.7807%2C%20respectively.%0AThese%20results%20establish%20a%20new%20state-of-the-art%20for%20both%20datasets.%20Beyond%20the%0Aimmediate%20task%2C%20our%20approach%20offers%20a%20robust%20and%20efficient%20preprocessing%20method%0Afor%20future%20research%20in%20audio%20tokenization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10423v1&entry.124074799=Read"},
{"title": "Llama Guard 3 Vision: Safeguarding Human-AI Image Understanding\n  Conversations", "author": "Jianfeng Chi and Ujjwal Karn and Hongyuan Zhan and Eric Smith and Javier Rando and Yiming Zhang and Kate Plawiak and Zacharie Delpierre Coudert and Kartikeya Upasani and Mahesh Pasupuleti", "abstract": "  We introduce Llama Guard 3 Vision, a multimodal LLM-based safeguard for\nhuman-AI conversations that involves image understanding: it can be used to\nsafeguard content for both multimodal LLM inputs (prompt classification) and\noutputs (response classification). Unlike the previous text-only Llama Guard\nversions (Inan et al., 2023; Llama Team, 2024b,a), it is specifically designed\nto support image reasoning use cases and is optimized to detect harmful\nmultimodal (text and image) prompts and text responses to these prompts. Llama\nGuard 3 Vision is fine-tuned on Llama 3.2-Vision and demonstrates strong\nperformance on the internal benchmarks using the MLCommons taxonomy. We also\ntest its robustness against adversarial attacks. We believe that Llama Guard 3\nVision serves as a good starting point to build more capable and robust content\nmoderation tools for human-AI conversation with multimodal capabilities.\n", "link": "http://arxiv.org/abs/2411.10414v1", "date": "2024-11-15", "relevancy": 2.5572, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5186}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5079}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5079}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Llama%20Guard%203%20Vision%3A%20Safeguarding%20Human-AI%20Image%20Understanding%0A%20%20Conversations&body=Title%3A%20Llama%20Guard%203%20Vision%3A%20Safeguarding%20Human-AI%20Image%20Understanding%0A%20%20Conversations%0AAuthor%3A%20Jianfeng%20Chi%20and%20Ujjwal%20Karn%20and%20Hongyuan%20Zhan%20and%20Eric%20Smith%20and%20Javier%20Rando%20and%20Yiming%20Zhang%20and%20Kate%20Plawiak%20and%20Zacharie%20Delpierre%20Coudert%20and%20Kartikeya%20Upasani%20and%20Mahesh%20Pasupuleti%0AAbstract%3A%20%20%20We%20introduce%20Llama%20Guard%203%20Vision%2C%20a%20multimodal%20LLM-based%20safeguard%20for%0Ahuman-AI%20conversations%20that%20involves%20image%20understanding%3A%20it%20can%20be%20used%20to%0Asafeguard%20content%20for%20both%20multimodal%20LLM%20inputs%20%28prompt%20classification%29%20and%0Aoutputs%20%28response%20classification%29.%20Unlike%20the%20previous%20text-only%20Llama%20Guard%0Aversions%20%28Inan%20et%20al.%2C%202023%3B%20Llama%20Team%2C%202024b%2Ca%29%2C%20it%20is%20specifically%20designed%0Ato%20support%20image%20reasoning%20use%20cases%20and%20is%20optimized%20to%20detect%20harmful%0Amultimodal%20%28text%20and%20image%29%20prompts%20and%20text%20responses%20to%20these%20prompts.%20Llama%0AGuard%203%20Vision%20is%20fine-tuned%20on%20Llama%203.2-Vision%20and%20demonstrates%20strong%0Aperformance%20on%20the%20internal%20benchmarks%20using%20the%20MLCommons%20taxonomy.%20We%20also%0Atest%20its%20robustness%20against%20adversarial%20attacks.%20We%20believe%20that%20Llama%20Guard%203%0AVision%20serves%20as%20a%20good%20starting%20point%20to%20build%20more%20capable%20and%20robust%20content%0Amoderation%20tools%20for%20human-AI%20conversation%20with%20multimodal%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10414v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLlama%2520Guard%25203%2520Vision%253A%2520Safeguarding%2520Human-AI%2520Image%2520Understanding%250A%2520%2520Conversations%26entry.906535625%3DJianfeng%2520Chi%2520and%2520Ujjwal%2520Karn%2520and%2520Hongyuan%2520Zhan%2520and%2520Eric%2520Smith%2520and%2520Javier%2520Rando%2520and%2520Yiming%2520Zhang%2520and%2520Kate%2520Plawiak%2520and%2520Zacharie%2520Delpierre%2520Coudert%2520and%2520Kartikeya%2520Upasani%2520and%2520Mahesh%2520Pasupuleti%26entry.1292438233%3D%2520%2520We%2520introduce%2520Llama%2520Guard%25203%2520Vision%252C%2520a%2520multimodal%2520LLM-based%2520safeguard%2520for%250Ahuman-AI%2520conversations%2520that%2520involves%2520image%2520understanding%253A%2520it%2520can%2520be%2520used%2520to%250Asafeguard%2520content%2520for%2520both%2520multimodal%2520LLM%2520inputs%2520%2528prompt%2520classification%2529%2520and%250Aoutputs%2520%2528response%2520classification%2529.%2520Unlike%2520the%2520previous%2520text-only%2520Llama%2520Guard%250Aversions%2520%2528Inan%2520et%2520al.%252C%25202023%253B%2520Llama%2520Team%252C%25202024b%252Ca%2529%252C%2520it%2520is%2520specifically%2520designed%250Ato%2520support%2520image%2520reasoning%2520use%2520cases%2520and%2520is%2520optimized%2520to%2520detect%2520harmful%250Amultimodal%2520%2528text%2520and%2520image%2529%2520prompts%2520and%2520text%2520responses%2520to%2520these%2520prompts.%2520Llama%250AGuard%25203%2520Vision%2520is%2520fine-tuned%2520on%2520Llama%25203.2-Vision%2520and%2520demonstrates%2520strong%250Aperformance%2520on%2520the%2520internal%2520benchmarks%2520using%2520the%2520MLCommons%2520taxonomy.%2520We%2520also%250Atest%2520its%2520robustness%2520against%2520adversarial%2520attacks.%2520We%2520believe%2520that%2520Llama%2520Guard%25203%250AVision%2520serves%2520as%2520a%2520good%2520starting%2520point%2520to%2520build%2520more%2520capable%2520and%2520robust%2520content%250Amoderation%2520tools%2520for%2520human-AI%2520conversation%2520with%2520multimodal%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10414v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Llama%20Guard%203%20Vision%3A%20Safeguarding%20Human-AI%20Image%20Understanding%0A%20%20Conversations&entry.906535625=Jianfeng%20Chi%20and%20Ujjwal%20Karn%20and%20Hongyuan%20Zhan%20and%20Eric%20Smith%20and%20Javier%20Rando%20and%20Yiming%20Zhang%20and%20Kate%20Plawiak%20and%20Zacharie%20Delpierre%20Coudert%20and%20Kartikeya%20Upasani%20and%20Mahesh%20Pasupuleti&entry.1292438233=%20%20We%20introduce%20Llama%20Guard%203%20Vision%2C%20a%20multimodal%20LLM-based%20safeguard%20for%0Ahuman-AI%20conversations%20that%20involves%20image%20understanding%3A%20it%20can%20be%20used%20to%0Asafeguard%20content%20for%20both%20multimodal%20LLM%20inputs%20%28prompt%20classification%29%20and%0Aoutputs%20%28response%20classification%29.%20Unlike%20the%20previous%20text-only%20Llama%20Guard%0Aversions%20%28Inan%20et%20al.%2C%202023%3B%20Llama%20Team%2C%202024b%2Ca%29%2C%20it%20is%20specifically%20designed%0Ato%20support%20image%20reasoning%20use%20cases%20and%20is%20optimized%20to%20detect%20harmful%0Amultimodal%20%28text%20and%20image%29%20prompts%20and%20text%20responses%20to%20these%20prompts.%20Llama%0AGuard%203%20Vision%20is%20fine-tuned%20on%20Llama%203.2-Vision%20and%20demonstrates%20strong%0Aperformance%20on%20the%20internal%20benchmarks%20using%20the%20MLCommons%20taxonomy.%20We%20also%0Atest%20its%20robustness%20against%20adversarial%20attacks.%20We%20believe%20that%20Llama%20Guard%203%0AVision%20serves%20as%20a%20good%20starting%20point%20to%20build%20more%20capable%20and%20robust%20content%0Amoderation%20tools%20for%20human-AI%20conversation%20with%20multimodal%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10414v1&entry.124074799=Read"},
{"title": "ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric\n  Thermal Images", "author": "Fangqiang Ding and Yunzhou Zhu and Xiangyu Wen and Gaowen Liu and Chris Xiaoxuan Lu", "abstract": "  Designing egocentric 3D hand pose estimation systems that can perform\nreliably in complex, real-world scenarios is crucial for downstream\napplications. Previous approaches using RGB or NIR imagery struggle in\nchallenging conditions: RGB methods are susceptible to lighting variations and\nobstructions like handwear, while NIR techniques can be disrupted by sunlight\nor interference from other NIR-equipped devices. To address these limitations,\nwe present ThermoHands, the first benchmark focused on thermal image-based\negocentric 3D hand pose estimation, demonstrating the potential of thermal\nimaging to achieve robust performance under these conditions. The benchmark\nincludes a multi-view and multi-spectral dataset collected from 28 subjects\nperforming hand-object and hand-virtual interactions under diverse scenarios,\naccurately annotated with 3D hand poses through an automated process. We\nintroduce a new baseline method, TherFormer, utilizing dual transformer modules\nfor effective egocentric 3D hand pose estimation in thermal imagery. Our\nexperimental results highlight TherFormer's leading performance and affirm\nthermal imaging's effectiveness in enabling robust 3D hand pose estimation in\nadverse conditions.\n", "link": "http://arxiv.org/abs/2403.09871v4", "date": "2024-11-15", "relevancy": 2.5298, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.522}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5007}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ThermoHands%3A%20A%20Benchmark%20for%203D%20Hand%20Pose%20Estimation%20from%20Egocentric%0A%20%20Thermal%20Images&body=Title%3A%20ThermoHands%3A%20A%20Benchmark%20for%203D%20Hand%20Pose%20Estimation%20from%20Egocentric%0A%20%20Thermal%20Images%0AAuthor%3A%20Fangqiang%20Ding%20and%20Yunzhou%20Zhu%20and%20Xiangyu%20Wen%20and%20Gaowen%20Liu%20and%20Chris%20Xiaoxuan%20Lu%0AAbstract%3A%20%20%20Designing%20egocentric%203D%20hand%20pose%20estimation%20systems%20that%20can%20perform%0Areliably%20in%20complex%2C%20real-world%20scenarios%20is%20crucial%20for%20downstream%0Aapplications.%20Previous%20approaches%20using%20RGB%20or%20NIR%20imagery%20struggle%20in%0Achallenging%20conditions%3A%20RGB%20methods%20are%20susceptible%20to%20lighting%20variations%20and%0Aobstructions%20like%20handwear%2C%20while%20NIR%20techniques%20can%20be%20disrupted%20by%20sunlight%0Aor%20interference%20from%20other%20NIR-equipped%20devices.%20To%20address%20these%20limitations%2C%0Awe%20present%20ThermoHands%2C%20the%20first%20benchmark%20focused%20on%20thermal%20image-based%0Aegocentric%203D%20hand%20pose%20estimation%2C%20demonstrating%20the%20potential%20of%20thermal%0Aimaging%20to%20achieve%20robust%20performance%20under%20these%20conditions.%20The%20benchmark%0Aincludes%20a%20multi-view%20and%20multi-spectral%20dataset%20collected%20from%2028%20subjects%0Aperforming%20hand-object%20and%20hand-virtual%20interactions%20under%20diverse%20scenarios%2C%0Aaccurately%20annotated%20with%203D%20hand%20poses%20through%20an%20automated%20process.%20We%0Aintroduce%20a%20new%20baseline%20method%2C%20TherFormer%2C%20utilizing%20dual%20transformer%20modules%0Afor%20effective%20egocentric%203D%20hand%20pose%20estimation%20in%20thermal%20imagery.%20Our%0Aexperimental%20results%20highlight%20TherFormer%27s%20leading%20performance%20and%20affirm%0Athermal%20imaging%27s%20effectiveness%20in%20enabling%20robust%203D%20hand%20pose%20estimation%20in%0Aadverse%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09871v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThermoHands%253A%2520A%2520Benchmark%2520for%25203D%2520Hand%2520Pose%2520Estimation%2520from%2520Egocentric%250A%2520%2520Thermal%2520Images%26entry.906535625%3DFangqiang%2520Ding%2520and%2520Yunzhou%2520Zhu%2520and%2520Xiangyu%2520Wen%2520and%2520Gaowen%2520Liu%2520and%2520Chris%2520Xiaoxuan%2520Lu%26entry.1292438233%3D%2520%2520Designing%2520egocentric%25203D%2520hand%2520pose%2520estimation%2520systems%2520that%2520can%2520perform%250Areliably%2520in%2520complex%252C%2520real-world%2520scenarios%2520is%2520crucial%2520for%2520downstream%250Aapplications.%2520Previous%2520approaches%2520using%2520RGB%2520or%2520NIR%2520imagery%2520struggle%2520in%250Achallenging%2520conditions%253A%2520RGB%2520methods%2520are%2520susceptible%2520to%2520lighting%2520variations%2520and%250Aobstructions%2520like%2520handwear%252C%2520while%2520NIR%2520techniques%2520can%2520be%2520disrupted%2520by%2520sunlight%250Aor%2520interference%2520from%2520other%2520NIR-equipped%2520devices.%2520To%2520address%2520these%2520limitations%252C%250Awe%2520present%2520ThermoHands%252C%2520the%2520first%2520benchmark%2520focused%2520on%2520thermal%2520image-based%250Aegocentric%25203D%2520hand%2520pose%2520estimation%252C%2520demonstrating%2520the%2520potential%2520of%2520thermal%250Aimaging%2520to%2520achieve%2520robust%2520performance%2520under%2520these%2520conditions.%2520The%2520benchmark%250Aincludes%2520a%2520multi-view%2520and%2520multi-spectral%2520dataset%2520collected%2520from%252028%2520subjects%250Aperforming%2520hand-object%2520and%2520hand-virtual%2520interactions%2520under%2520diverse%2520scenarios%252C%250Aaccurately%2520annotated%2520with%25203D%2520hand%2520poses%2520through%2520an%2520automated%2520process.%2520We%250Aintroduce%2520a%2520new%2520baseline%2520method%252C%2520TherFormer%252C%2520utilizing%2520dual%2520transformer%2520modules%250Afor%2520effective%2520egocentric%25203D%2520hand%2520pose%2520estimation%2520in%2520thermal%2520imagery.%2520Our%250Aexperimental%2520results%2520highlight%2520TherFormer%2527s%2520leading%2520performance%2520and%2520affirm%250Athermal%2520imaging%2527s%2520effectiveness%2520in%2520enabling%2520robust%25203D%2520hand%2520pose%2520estimation%2520in%250Aadverse%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.09871v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ThermoHands%3A%20A%20Benchmark%20for%203D%20Hand%20Pose%20Estimation%20from%20Egocentric%0A%20%20Thermal%20Images&entry.906535625=Fangqiang%20Ding%20and%20Yunzhou%20Zhu%20and%20Xiangyu%20Wen%20and%20Gaowen%20Liu%20and%20Chris%20Xiaoxuan%20Lu&entry.1292438233=%20%20Designing%20egocentric%203D%20hand%20pose%20estimation%20systems%20that%20can%20perform%0Areliably%20in%20complex%2C%20real-world%20scenarios%20is%20crucial%20for%20downstream%0Aapplications.%20Previous%20approaches%20using%20RGB%20or%20NIR%20imagery%20struggle%20in%0Achallenging%20conditions%3A%20RGB%20methods%20are%20susceptible%20to%20lighting%20variations%20and%0Aobstructions%20like%20handwear%2C%20while%20NIR%20techniques%20can%20be%20disrupted%20by%20sunlight%0Aor%20interference%20from%20other%20NIR-equipped%20devices.%20To%20address%20these%20limitations%2C%0Awe%20present%20ThermoHands%2C%20the%20first%20benchmark%20focused%20on%20thermal%20image-based%0Aegocentric%203D%20hand%20pose%20estimation%2C%20demonstrating%20the%20potential%20of%20thermal%0Aimaging%20to%20achieve%20robust%20performance%20under%20these%20conditions.%20The%20benchmark%0Aincludes%20a%20multi-view%20and%20multi-spectral%20dataset%20collected%20from%2028%20subjects%0Aperforming%20hand-object%20and%20hand-virtual%20interactions%20under%20diverse%20scenarios%2C%0Aaccurately%20annotated%20with%203D%20hand%20poses%20through%20an%20automated%20process.%20We%0Aintroduce%20a%20new%20baseline%20method%2C%20TherFormer%2C%20utilizing%20dual%20transformer%20modules%0Afor%20effective%20egocentric%203D%20hand%20pose%20estimation%20in%20thermal%20imagery.%20Our%0Aexperimental%20results%20highlight%20TherFormer%27s%20leading%20performance%20and%20affirm%0Athermal%20imaging%27s%20effectiveness%20in%20enabling%20robust%203D%20hand%20pose%20estimation%20in%0Aadverse%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09871v4&entry.124074799=Read"},
{"title": "Lateral Movement Detection via Time-aware Subgraph Classification on\n  Authentication Logs", "author": "Jiajun Zhou and Jiacheng Yao and Xuanze Chen and Shanqing Yu and Qi Xuan and Xiaoniu Yang", "abstract": "  Lateral movement is a crucial component of advanced persistent threat (APT)\nattacks in networks. Attackers exploit security vulnerabilities in internal\nnetworks or IoT devices, expanding their control after initial infiltration to\nsteal sensitive data or carry out other malicious activities, posing a serious\nthreat to system security. Existing research suggests that attackers generally\nemploy seemingly unrelated operations to mask their malicious intentions,\nthereby evading existing lateral movement detection methods and hiding their\nintrusion traces. In this regard, we analyze host authentication log data from\na graph perspective and propose a multi-scale lateral movement detection\nframework called LMDetect. The main workflow of this framework proceeds as\nfollows: 1) Construct a heterogeneous multigraph from host authentication log\ndata to strengthen the correlations among internal system entities; 2) Design a\ntime-aware subgraph generator to extract subgraphs centered on authentication\nevents from the heterogeneous authentication multigraph; 3) Design a\nmulti-scale attention encoder that leverages both local and global attention to\ncapture hidden anomalous behavior patterns in the authentication subgraphs,\nthereby achieving lateral movement detection. Extensive experiments on two\nreal-world authentication log datasets demonstrate the effectiveness and\nsuperiority of our framework in detecting lateral movement behaviors.\n", "link": "http://arxiv.org/abs/2411.10279v1", "date": "2024-11-15", "relevancy": 2.4083, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4977}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4746}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4726}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lateral%20Movement%20Detection%20via%20Time-aware%20Subgraph%20Classification%20on%0A%20%20Authentication%20Logs&body=Title%3A%20Lateral%20Movement%20Detection%20via%20Time-aware%20Subgraph%20Classification%20on%0A%20%20Authentication%20Logs%0AAuthor%3A%20Jiajun%20Zhou%20and%20Jiacheng%20Yao%20and%20Xuanze%20Chen%20and%20Shanqing%20Yu%20and%20Qi%20Xuan%20and%20Xiaoniu%20Yang%0AAbstract%3A%20%20%20Lateral%20movement%20is%20a%20crucial%20component%20of%20advanced%20persistent%20threat%20%28APT%29%0Aattacks%20in%20networks.%20Attackers%20exploit%20security%20vulnerabilities%20in%20internal%0Anetworks%20or%20IoT%20devices%2C%20expanding%20their%20control%20after%20initial%20infiltration%20to%0Asteal%20sensitive%20data%20or%20carry%20out%20other%20malicious%20activities%2C%20posing%20a%20serious%0Athreat%20to%20system%20security.%20Existing%20research%20suggests%20that%20attackers%20generally%0Aemploy%20seemingly%20unrelated%20operations%20to%20mask%20their%20malicious%20intentions%2C%0Athereby%20evading%20existing%20lateral%20movement%20detection%20methods%20and%20hiding%20their%0Aintrusion%20traces.%20In%20this%20regard%2C%20we%20analyze%20host%20authentication%20log%20data%20from%0Aa%20graph%20perspective%20and%20propose%20a%20multi-scale%20lateral%20movement%20detection%0Aframework%20called%20LMDetect.%20The%20main%20workflow%20of%20this%20framework%20proceeds%20as%0Afollows%3A%201%29%20Construct%20a%20heterogeneous%20multigraph%20from%20host%20authentication%20log%0Adata%20to%20strengthen%20the%20correlations%20among%20internal%20system%20entities%3B%202%29%20Design%20a%0Atime-aware%20subgraph%20generator%20to%20extract%20subgraphs%20centered%20on%20authentication%0Aevents%20from%20the%20heterogeneous%20authentication%20multigraph%3B%203%29%20Design%20a%0Amulti-scale%20attention%20encoder%20that%20leverages%20both%20local%20and%20global%20attention%20to%0Acapture%20hidden%20anomalous%20behavior%20patterns%20in%20the%20authentication%20subgraphs%2C%0Athereby%20achieving%20lateral%20movement%20detection.%20Extensive%20experiments%20on%20two%0Areal-world%20authentication%20log%20datasets%20demonstrate%20the%20effectiveness%20and%0Asuperiority%20of%20our%20framework%20in%20detecting%20lateral%20movement%20behaviors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10279v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLateral%2520Movement%2520Detection%2520via%2520Time-aware%2520Subgraph%2520Classification%2520on%250A%2520%2520Authentication%2520Logs%26entry.906535625%3DJiajun%2520Zhou%2520and%2520Jiacheng%2520Yao%2520and%2520Xuanze%2520Chen%2520and%2520Shanqing%2520Yu%2520and%2520Qi%2520Xuan%2520and%2520Xiaoniu%2520Yang%26entry.1292438233%3D%2520%2520Lateral%2520movement%2520is%2520a%2520crucial%2520component%2520of%2520advanced%2520persistent%2520threat%2520%2528APT%2529%250Aattacks%2520in%2520networks.%2520Attackers%2520exploit%2520security%2520vulnerabilities%2520in%2520internal%250Anetworks%2520or%2520IoT%2520devices%252C%2520expanding%2520their%2520control%2520after%2520initial%2520infiltration%2520to%250Asteal%2520sensitive%2520data%2520or%2520carry%2520out%2520other%2520malicious%2520activities%252C%2520posing%2520a%2520serious%250Athreat%2520to%2520system%2520security.%2520Existing%2520research%2520suggests%2520that%2520attackers%2520generally%250Aemploy%2520seemingly%2520unrelated%2520operations%2520to%2520mask%2520their%2520malicious%2520intentions%252C%250Athereby%2520evading%2520existing%2520lateral%2520movement%2520detection%2520methods%2520and%2520hiding%2520their%250Aintrusion%2520traces.%2520In%2520this%2520regard%252C%2520we%2520analyze%2520host%2520authentication%2520log%2520data%2520from%250Aa%2520graph%2520perspective%2520and%2520propose%2520a%2520multi-scale%2520lateral%2520movement%2520detection%250Aframework%2520called%2520LMDetect.%2520The%2520main%2520workflow%2520of%2520this%2520framework%2520proceeds%2520as%250Afollows%253A%25201%2529%2520Construct%2520a%2520heterogeneous%2520multigraph%2520from%2520host%2520authentication%2520log%250Adata%2520to%2520strengthen%2520the%2520correlations%2520among%2520internal%2520system%2520entities%253B%25202%2529%2520Design%2520a%250Atime-aware%2520subgraph%2520generator%2520to%2520extract%2520subgraphs%2520centered%2520on%2520authentication%250Aevents%2520from%2520the%2520heterogeneous%2520authentication%2520multigraph%253B%25203%2529%2520Design%2520a%250Amulti-scale%2520attention%2520encoder%2520that%2520leverages%2520both%2520local%2520and%2520global%2520attention%2520to%250Acapture%2520hidden%2520anomalous%2520behavior%2520patterns%2520in%2520the%2520authentication%2520subgraphs%252C%250Athereby%2520achieving%2520lateral%2520movement%2520detection.%2520Extensive%2520experiments%2520on%2520two%250Areal-world%2520authentication%2520log%2520datasets%2520demonstrate%2520the%2520effectiveness%2520and%250Asuperiority%2520of%2520our%2520framework%2520in%2520detecting%2520lateral%2520movement%2520behaviors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10279v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lateral%20Movement%20Detection%20via%20Time-aware%20Subgraph%20Classification%20on%0A%20%20Authentication%20Logs&entry.906535625=Jiajun%20Zhou%20and%20Jiacheng%20Yao%20and%20Xuanze%20Chen%20and%20Shanqing%20Yu%20and%20Qi%20Xuan%20and%20Xiaoniu%20Yang&entry.1292438233=%20%20Lateral%20movement%20is%20a%20crucial%20component%20of%20advanced%20persistent%20threat%20%28APT%29%0Aattacks%20in%20networks.%20Attackers%20exploit%20security%20vulnerabilities%20in%20internal%0Anetworks%20or%20IoT%20devices%2C%20expanding%20their%20control%20after%20initial%20infiltration%20to%0Asteal%20sensitive%20data%20or%20carry%20out%20other%20malicious%20activities%2C%20posing%20a%20serious%0Athreat%20to%20system%20security.%20Existing%20research%20suggests%20that%20attackers%20generally%0Aemploy%20seemingly%20unrelated%20operations%20to%20mask%20their%20malicious%20intentions%2C%0Athereby%20evading%20existing%20lateral%20movement%20detection%20methods%20and%20hiding%20their%0Aintrusion%20traces.%20In%20this%20regard%2C%20we%20analyze%20host%20authentication%20log%20data%20from%0Aa%20graph%20perspective%20and%20propose%20a%20multi-scale%20lateral%20movement%20detection%0Aframework%20called%20LMDetect.%20The%20main%20workflow%20of%20this%20framework%20proceeds%20as%0Afollows%3A%201%29%20Construct%20a%20heterogeneous%20multigraph%20from%20host%20authentication%20log%0Adata%20to%20strengthen%20the%20correlations%20among%20internal%20system%20entities%3B%202%29%20Design%20a%0Atime-aware%20subgraph%20generator%20to%20extract%20subgraphs%20centered%20on%20authentication%0Aevents%20from%20the%20heterogeneous%20authentication%20multigraph%3B%203%29%20Design%20a%0Amulti-scale%20attention%20encoder%20that%20leverages%20both%20local%20and%20global%20attention%20to%0Acapture%20hidden%20anomalous%20behavior%20patterns%20in%20the%20authentication%20subgraphs%2C%0Athereby%20achieving%20lateral%20movement%20detection.%20Extensive%20experiments%20on%20two%0Areal-world%20authentication%20log%20datasets%20demonstrate%20the%20effectiveness%20and%0Asuperiority%20of%20our%20framework%20in%20detecting%20lateral%20movement%20behaviors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10279v1&entry.124074799=Read"},
{"title": "M-VAR: Decoupled Scale-wise Autoregressive Modeling for High-Quality\n  Image Generation", "author": "Sucheng Ren and Yaodong Yu and Nataniel Ruiz and Feng Wang and Alan Yuille and Cihang Xie", "abstract": "  There exists recent work in computer vision, named VAR, that proposes a new\nautoregressive paradigm for image generation. Diverging from the vanilla\nnext-token prediction, VAR structurally reformulates the image generation into\na coarse to fine next-scale prediction. In this paper, we show that this\nscale-wise autoregressive framework can be effectively decoupled into\n\\textit{intra-scale modeling}, which captures local spatial dependencies within\neach scale, and \\textit{inter-scale modeling}, which models cross-scale\nrelationships progressively from coarse-to-fine scales. This decoupling\nstructure allows to rebuild VAR in a more computationally efficient manner.\nSpecifically, for intra-scale modeling -- crucial for generating high-fidelity\nimages -- we retain the original bidirectional self-attention design to ensure\ncomprehensive modeling; for inter-scale modeling, which semantically connects\ndifferent scales but is computationally intensive, we apply linear-complexity\nmechanisms like Mamba to substantially reduce computational overhead. We term\nthis new framework M-VAR. Extensive experiments demonstrate that our method\noutperforms existing models in both image quality and generation speed. For\nexample, our 1.5B model, with fewer parameters and faster inference speed,\noutperforms the largest VAR-d30-2B. Moreover, our largest model M-VAR-d32\nimpressively registers 1.78 FID on ImageNet 256$\\times$256 and outperforms the\nprior-art autoregressive models LlamaGen/VAR by 0.4/0.19 and popular diffusion\nmodels LDM/DiT by 1.82/0.49, respectively. Code is avaiable at\n\\url{https://github.com/OliverRensu/MVAR}.\n", "link": "http://arxiv.org/abs/2411.10433v1", "date": "2024-11-15", "relevancy": 2.3818, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6318}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5915}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M-VAR%3A%20Decoupled%20Scale-wise%20Autoregressive%20Modeling%20for%20High-Quality%0A%20%20Image%20Generation&body=Title%3A%20M-VAR%3A%20Decoupled%20Scale-wise%20Autoregressive%20Modeling%20for%20High-Quality%0A%20%20Image%20Generation%0AAuthor%3A%20Sucheng%20Ren%20and%20Yaodong%20Yu%20and%20Nataniel%20Ruiz%20and%20Feng%20Wang%20and%20Alan%20Yuille%20and%20Cihang%20Xie%0AAbstract%3A%20%20%20There%20exists%20recent%20work%20in%20computer%20vision%2C%20named%20VAR%2C%20that%20proposes%20a%20new%0Aautoregressive%20paradigm%20for%20image%20generation.%20Diverging%20from%20the%20vanilla%0Anext-token%20prediction%2C%20VAR%20structurally%20reformulates%20the%20image%20generation%20into%0Aa%20coarse%20to%20fine%20next-scale%20prediction.%20In%20this%20paper%2C%20we%20show%20that%20this%0Ascale-wise%20autoregressive%20framework%20can%20be%20effectively%20decoupled%20into%0A%5Ctextit%7Bintra-scale%20modeling%7D%2C%20which%20captures%20local%20spatial%20dependencies%20within%0Aeach%20scale%2C%20and%20%5Ctextit%7Binter-scale%20modeling%7D%2C%20which%20models%20cross-scale%0Arelationships%20progressively%20from%20coarse-to-fine%20scales.%20This%20decoupling%0Astructure%20allows%20to%20rebuild%20VAR%20in%20a%20more%20computationally%20efficient%20manner.%0ASpecifically%2C%20for%20intra-scale%20modeling%20--%20crucial%20for%20generating%20high-fidelity%0Aimages%20--%20we%20retain%20the%20original%20bidirectional%20self-attention%20design%20to%20ensure%0Acomprehensive%20modeling%3B%20for%20inter-scale%20modeling%2C%20which%20semantically%20connects%0Adifferent%20scales%20but%20is%20computationally%20intensive%2C%20we%20apply%20linear-complexity%0Amechanisms%20like%20Mamba%20to%20substantially%20reduce%20computational%20overhead.%20We%20term%0Athis%20new%20framework%20M-VAR.%20Extensive%20experiments%20demonstrate%20that%20our%20method%0Aoutperforms%20existing%20models%20in%20both%20image%20quality%20and%20generation%20speed.%20For%0Aexample%2C%20our%201.5B%20model%2C%20with%20fewer%20parameters%20and%20faster%20inference%20speed%2C%0Aoutperforms%20the%20largest%20VAR-d30-2B.%20Moreover%2C%20our%20largest%20model%20M-VAR-d32%0Aimpressively%20registers%201.78%20FID%20on%20ImageNet%20256%24%5Ctimes%24256%20and%20outperforms%20the%0Aprior-art%20autoregressive%20models%20LlamaGen/VAR%20by%200.4/0.19%20and%20popular%20diffusion%0Amodels%20LDM/DiT%20by%201.82/0.49%2C%20respectively.%20Code%20is%20avaiable%20at%0A%5Curl%7Bhttps%3A//github.com/OliverRensu/MVAR%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10433v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM-VAR%253A%2520Decoupled%2520Scale-wise%2520Autoregressive%2520Modeling%2520for%2520High-Quality%250A%2520%2520Image%2520Generation%26entry.906535625%3DSucheng%2520Ren%2520and%2520Yaodong%2520Yu%2520and%2520Nataniel%2520Ruiz%2520and%2520Feng%2520Wang%2520and%2520Alan%2520Yuille%2520and%2520Cihang%2520Xie%26entry.1292438233%3D%2520%2520There%2520exists%2520recent%2520work%2520in%2520computer%2520vision%252C%2520named%2520VAR%252C%2520that%2520proposes%2520a%2520new%250Aautoregressive%2520paradigm%2520for%2520image%2520generation.%2520Diverging%2520from%2520the%2520vanilla%250Anext-token%2520prediction%252C%2520VAR%2520structurally%2520reformulates%2520the%2520image%2520generation%2520into%250Aa%2520coarse%2520to%2520fine%2520next-scale%2520prediction.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520this%250Ascale-wise%2520autoregressive%2520framework%2520can%2520be%2520effectively%2520decoupled%2520into%250A%255Ctextit%257Bintra-scale%2520modeling%257D%252C%2520which%2520captures%2520local%2520spatial%2520dependencies%2520within%250Aeach%2520scale%252C%2520and%2520%255Ctextit%257Binter-scale%2520modeling%257D%252C%2520which%2520models%2520cross-scale%250Arelationships%2520progressively%2520from%2520coarse-to-fine%2520scales.%2520This%2520decoupling%250Astructure%2520allows%2520to%2520rebuild%2520VAR%2520in%2520a%2520more%2520computationally%2520efficient%2520manner.%250ASpecifically%252C%2520for%2520intra-scale%2520modeling%2520--%2520crucial%2520for%2520generating%2520high-fidelity%250Aimages%2520--%2520we%2520retain%2520the%2520original%2520bidirectional%2520self-attention%2520design%2520to%2520ensure%250Acomprehensive%2520modeling%253B%2520for%2520inter-scale%2520modeling%252C%2520which%2520semantically%2520connects%250Adifferent%2520scales%2520but%2520is%2520computationally%2520intensive%252C%2520we%2520apply%2520linear-complexity%250Amechanisms%2520like%2520Mamba%2520to%2520substantially%2520reduce%2520computational%2520overhead.%2520We%2520term%250Athis%2520new%2520framework%2520M-VAR.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%250Aoutperforms%2520existing%2520models%2520in%2520both%2520image%2520quality%2520and%2520generation%2520speed.%2520For%250Aexample%252C%2520our%25201.5B%2520model%252C%2520with%2520fewer%2520parameters%2520and%2520faster%2520inference%2520speed%252C%250Aoutperforms%2520the%2520largest%2520VAR-d30-2B.%2520Moreover%252C%2520our%2520largest%2520model%2520M-VAR-d32%250Aimpressively%2520registers%25201.78%2520FID%2520on%2520ImageNet%2520256%2524%255Ctimes%2524256%2520and%2520outperforms%2520the%250Aprior-art%2520autoregressive%2520models%2520LlamaGen/VAR%2520by%25200.4/0.19%2520and%2520popular%2520diffusion%250Amodels%2520LDM/DiT%2520by%25201.82/0.49%252C%2520respectively.%2520Code%2520is%2520avaiable%2520at%250A%255Curl%257Bhttps%253A//github.com/OliverRensu/MVAR%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10433v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M-VAR%3A%20Decoupled%20Scale-wise%20Autoregressive%20Modeling%20for%20High-Quality%0A%20%20Image%20Generation&entry.906535625=Sucheng%20Ren%20and%20Yaodong%20Yu%20and%20Nataniel%20Ruiz%20and%20Feng%20Wang%20and%20Alan%20Yuille%20and%20Cihang%20Xie&entry.1292438233=%20%20There%20exists%20recent%20work%20in%20computer%20vision%2C%20named%20VAR%2C%20that%20proposes%20a%20new%0Aautoregressive%20paradigm%20for%20image%20generation.%20Diverging%20from%20the%20vanilla%0Anext-token%20prediction%2C%20VAR%20structurally%20reformulates%20the%20image%20generation%20into%0Aa%20coarse%20to%20fine%20next-scale%20prediction.%20In%20this%20paper%2C%20we%20show%20that%20this%0Ascale-wise%20autoregressive%20framework%20can%20be%20effectively%20decoupled%20into%0A%5Ctextit%7Bintra-scale%20modeling%7D%2C%20which%20captures%20local%20spatial%20dependencies%20within%0Aeach%20scale%2C%20and%20%5Ctextit%7Binter-scale%20modeling%7D%2C%20which%20models%20cross-scale%0Arelationships%20progressively%20from%20coarse-to-fine%20scales.%20This%20decoupling%0Astructure%20allows%20to%20rebuild%20VAR%20in%20a%20more%20computationally%20efficient%20manner.%0ASpecifically%2C%20for%20intra-scale%20modeling%20--%20crucial%20for%20generating%20high-fidelity%0Aimages%20--%20we%20retain%20the%20original%20bidirectional%20self-attention%20design%20to%20ensure%0Acomprehensive%20modeling%3B%20for%20inter-scale%20modeling%2C%20which%20semantically%20connects%0Adifferent%20scales%20but%20is%20computationally%20intensive%2C%20we%20apply%20linear-complexity%0Amechanisms%20like%20Mamba%20to%20substantially%20reduce%20computational%20overhead.%20We%20term%0Athis%20new%20framework%20M-VAR.%20Extensive%20experiments%20demonstrate%20that%20our%20method%0Aoutperforms%20existing%20models%20in%20both%20image%20quality%20and%20generation%20speed.%20For%0Aexample%2C%20our%201.5B%20model%2C%20with%20fewer%20parameters%20and%20faster%20inference%20speed%2C%0Aoutperforms%20the%20largest%20VAR-d30-2B.%20Moreover%2C%20our%20largest%20model%20M-VAR-d32%0Aimpressively%20registers%201.78%20FID%20on%20ImageNet%20256%24%5Ctimes%24256%20and%20outperforms%20the%0Aprior-art%20autoregressive%20models%20LlamaGen/VAR%20by%200.4/0.19%20and%20popular%20diffusion%0Amodels%20LDM/DiT%20by%201.82/0.49%2C%20respectively.%20Code%20is%20avaiable%20at%0A%5Curl%7Bhttps%3A//github.com/OliverRensu/MVAR%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10433v1&entry.124074799=Read"},
{"title": "VeriGraph: Scene Graphs for Execution Verifiable Robot Planning", "author": "Daniel Ekpo and Mara Levy and Saksham Suri and Chuong Huynh and Abhinav Shrivastava", "abstract": "  Recent advancements in vision-language models (VLMs) offer potential for\nrobot task planning, but challenges remain due to VLMs' tendency to generate\nincorrect action sequences. To address these limitations, we propose VeriGraph,\na novel framework that integrates VLMs for robotic planning while verifying\naction feasibility. VeriGraph employs scene graphs as an intermediate\nrepresentation, capturing key objects and spatial relationships to improve plan\nverification and refinement. The system generates a scene graph from input\nimages and uses it to iteratively check and correct action sequences generated\nby an LLM-based task planner, ensuring constraints are respected and actions\nare executable. Our approach significantly enhances task completion rates\nacross diverse manipulation scenarios, outperforming baseline methods by 58%\nfor language-based tasks and 30% for image-based tasks.\n", "link": "http://arxiv.org/abs/2411.10446v1", "date": "2024-11-15", "relevancy": 2.3792, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.664}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.581}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VeriGraph%3A%20Scene%20Graphs%20for%20Execution%20Verifiable%20Robot%20Planning&body=Title%3A%20VeriGraph%3A%20Scene%20Graphs%20for%20Execution%20Verifiable%20Robot%20Planning%0AAuthor%3A%20Daniel%20Ekpo%20and%20Mara%20Levy%20and%20Saksham%20Suri%20and%20Chuong%20Huynh%20and%20Abhinav%20Shrivastava%0AAbstract%3A%20%20%20Recent%20advancements%20in%20vision-language%20models%20%28VLMs%29%20offer%20potential%20for%0Arobot%20task%20planning%2C%20but%20challenges%20remain%20due%20to%20VLMs%27%20tendency%20to%20generate%0Aincorrect%20action%20sequences.%20To%20address%20these%20limitations%2C%20we%20propose%20VeriGraph%2C%0Aa%20novel%20framework%20that%20integrates%20VLMs%20for%20robotic%20planning%20while%20verifying%0Aaction%20feasibility.%20VeriGraph%20employs%20scene%20graphs%20as%20an%20intermediate%0Arepresentation%2C%20capturing%20key%20objects%20and%20spatial%20relationships%20to%20improve%20plan%0Averification%20and%20refinement.%20The%20system%20generates%20a%20scene%20graph%20from%20input%0Aimages%20and%20uses%20it%20to%20iteratively%20check%20and%20correct%20action%20sequences%20generated%0Aby%20an%20LLM-based%20task%20planner%2C%20ensuring%20constraints%20are%20respected%20and%20actions%0Aare%20executable.%20Our%20approach%20significantly%20enhances%20task%20completion%20rates%0Aacross%20diverse%20manipulation%20scenarios%2C%20outperforming%20baseline%20methods%20by%2058%25%0Afor%20language-based%20tasks%20and%2030%25%20for%20image-based%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10446v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVeriGraph%253A%2520Scene%2520Graphs%2520for%2520Execution%2520Verifiable%2520Robot%2520Planning%26entry.906535625%3DDaniel%2520Ekpo%2520and%2520Mara%2520Levy%2520and%2520Saksham%2520Suri%2520and%2520Chuong%2520Huynh%2520and%2520Abhinav%2520Shrivastava%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520vision-language%2520models%2520%2528VLMs%2529%2520offer%2520potential%2520for%250Arobot%2520task%2520planning%252C%2520but%2520challenges%2520remain%2520due%2520to%2520VLMs%2527%2520tendency%2520to%2520generate%250Aincorrect%2520action%2520sequences.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520VeriGraph%252C%250Aa%2520novel%2520framework%2520that%2520integrates%2520VLMs%2520for%2520robotic%2520planning%2520while%2520verifying%250Aaction%2520feasibility.%2520VeriGraph%2520employs%2520scene%2520graphs%2520as%2520an%2520intermediate%250Arepresentation%252C%2520capturing%2520key%2520objects%2520and%2520spatial%2520relationships%2520to%2520improve%2520plan%250Averification%2520and%2520refinement.%2520The%2520system%2520generates%2520a%2520scene%2520graph%2520from%2520input%250Aimages%2520and%2520uses%2520it%2520to%2520iteratively%2520check%2520and%2520correct%2520action%2520sequences%2520generated%250Aby%2520an%2520LLM-based%2520task%2520planner%252C%2520ensuring%2520constraints%2520are%2520respected%2520and%2520actions%250Aare%2520executable.%2520Our%2520approach%2520significantly%2520enhances%2520task%2520completion%2520rates%250Aacross%2520diverse%2520manipulation%2520scenarios%252C%2520outperforming%2520baseline%2520methods%2520by%252058%2525%250Afor%2520language-based%2520tasks%2520and%252030%2525%2520for%2520image-based%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10446v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VeriGraph%3A%20Scene%20Graphs%20for%20Execution%20Verifiable%20Robot%20Planning&entry.906535625=Daniel%20Ekpo%20and%20Mara%20Levy%20and%20Saksham%20Suri%20and%20Chuong%20Huynh%20and%20Abhinav%20Shrivastava&entry.1292438233=%20%20Recent%20advancements%20in%20vision-language%20models%20%28VLMs%29%20offer%20potential%20for%0Arobot%20task%20planning%2C%20but%20challenges%20remain%20due%20to%20VLMs%27%20tendency%20to%20generate%0Aincorrect%20action%20sequences.%20To%20address%20these%20limitations%2C%20we%20propose%20VeriGraph%2C%0Aa%20novel%20framework%20that%20integrates%20VLMs%20for%20robotic%20planning%20while%20verifying%0Aaction%20feasibility.%20VeriGraph%20employs%20scene%20graphs%20as%20an%20intermediate%0Arepresentation%2C%20capturing%20key%20objects%20and%20spatial%20relationships%20to%20improve%20plan%0Averification%20and%20refinement.%20The%20system%20generates%20a%20scene%20graph%20from%20input%0Aimages%20and%20uses%20it%20to%20iteratively%20check%20and%20correct%20action%20sequences%20generated%0Aby%20an%20LLM-based%20task%20planner%2C%20ensuring%20constraints%20are%20respected%20and%20actions%0Aare%20executable.%20Our%20approach%20significantly%20enhances%20task%20completion%20rates%0Aacross%20diverse%20manipulation%20scenarios%2C%20outperforming%20baseline%20methods%20by%2058%25%0Afor%20language-based%20tasks%20and%2030%25%20for%20image-based%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10446v1&entry.124074799=Read"},
{"title": "Comparative Analysis of Machine Learning Approaches for Bone Age\n  Assessment: A Comprehensive Study on Three Distinct Models", "author": "Nandavardhan R. and Somanathan R. and Vikram Suresh and Savaridassan P", "abstract": "  Radiologists and doctors make use of X-ray images of the non-dominant hands\nof children and infants to assess the possibility of genetic conditions and\ngrowth abnormalities. This is done by assessing the difference between the\nactual extent of growth found using the X-rays and the chronological age of the\nsubject. The assessment was done conventionally using The Greulich Pyle (GP) or\nTanner Whitehouse (TW) approach. These approaches require a high level of\nexpertise and may often lead to observer bias. Hence, to automate the process\nof assessing the X-rays, and to increase its accuracy and efficiency, several\nmachine learning models have been developed. These machine-learning models have\nseveral differences in their accuracy and efficiencies, leading to an unclear\nchoice for the suitable model depending on their needs and available resources.\nMethods: In this study, we have analyzed the 3 most widely used models for the\nautomation of bone age prediction, which are the Xception model, VGG model and\nCNN model. These models were trained on the preprocessed dataset and the\naccuracy was measured using the MAE in terms of months for each model. Using\nthis, the comparison between the models was done. Results: The 3 models,\nXception, VGG, and CNN models have been tested for accuracy and other relevant\nfactors.\n", "link": "http://arxiv.org/abs/2411.10345v1", "date": "2024-11-15", "relevancy": 2.3607, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4734}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4734}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparative%20Analysis%20of%20Machine%20Learning%20Approaches%20for%20Bone%20Age%0A%20%20Assessment%3A%20A%20Comprehensive%20Study%20on%20Three%20Distinct%20Models&body=Title%3A%20Comparative%20Analysis%20of%20Machine%20Learning%20Approaches%20for%20Bone%20Age%0A%20%20Assessment%3A%20A%20Comprehensive%20Study%20on%20Three%20Distinct%20Models%0AAuthor%3A%20Nandavardhan%20R.%20and%20Somanathan%20R.%20and%20Vikram%20Suresh%20and%20Savaridassan%20P%0AAbstract%3A%20%20%20Radiologists%20and%20doctors%20make%20use%20of%20X-ray%20images%20of%20the%20non-dominant%20hands%0Aof%20children%20and%20infants%20to%20assess%20the%20possibility%20of%20genetic%20conditions%20and%0Agrowth%20abnormalities.%20This%20is%20done%20by%20assessing%20the%20difference%20between%20the%0Aactual%20extent%20of%20growth%20found%20using%20the%20X-rays%20and%20the%20chronological%20age%20of%20the%0Asubject.%20The%20assessment%20was%20done%20conventionally%20using%20The%20Greulich%20Pyle%20%28GP%29%20or%0ATanner%20Whitehouse%20%28TW%29%20approach.%20These%20approaches%20require%20a%20high%20level%20of%0Aexpertise%20and%20may%20often%20lead%20to%20observer%20bias.%20Hence%2C%20to%20automate%20the%20process%0Aof%20assessing%20the%20X-rays%2C%20and%20to%20increase%20its%20accuracy%20and%20efficiency%2C%20several%0Amachine%20learning%20models%20have%20been%20developed.%20These%20machine-learning%20models%20have%0Aseveral%20differences%20in%20their%20accuracy%20and%20efficiencies%2C%20leading%20to%20an%20unclear%0Achoice%20for%20the%20suitable%20model%20depending%20on%20their%20needs%20and%20available%20resources.%0AMethods%3A%20In%20this%20study%2C%20we%20have%20analyzed%20the%203%20most%20widely%20used%20models%20for%20the%0Aautomation%20of%20bone%20age%20prediction%2C%20which%20are%20the%20Xception%20model%2C%20VGG%20model%20and%0ACNN%20model.%20These%20models%20were%20trained%20on%20the%20preprocessed%20dataset%20and%20the%0Aaccuracy%20was%20measured%20using%20the%20MAE%20in%20terms%20of%20months%20for%20each%20model.%20Using%0Athis%2C%20the%20comparison%20between%20the%20models%20was%20done.%20Results%3A%20The%203%20models%2C%0AXception%2C%20VGG%2C%20and%20CNN%20models%20have%20been%20tested%20for%20accuracy%20and%20other%20relevant%0Afactors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10345v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparative%2520Analysis%2520of%2520Machine%2520Learning%2520Approaches%2520for%2520Bone%2520Age%250A%2520%2520Assessment%253A%2520A%2520Comprehensive%2520Study%2520on%2520Three%2520Distinct%2520Models%26entry.906535625%3DNandavardhan%2520R.%2520and%2520Somanathan%2520R.%2520and%2520Vikram%2520Suresh%2520and%2520Savaridassan%2520P%26entry.1292438233%3D%2520%2520Radiologists%2520and%2520doctors%2520make%2520use%2520of%2520X-ray%2520images%2520of%2520the%2520non-dominant%2520hands%250Aof%2520children%2520and%2520infants%2520to%2520assess%2520the%2520possibility%2520of%2520genetic%2520conditions%2520and%250Agrowth%2520abnormalities.%2520This%2520is%2520done%2520by%2520assessing%2520the%2520difference%2520between%2520the%250Aactual%2520extent%2520of%2520growth%2520found%2520using%2520the%2520X-rays%2520and%2520the%2520chronological%2520age%2520of%2520the%250Asubject.%2520The%2520assessment%2520was%2520done%2520conventionally%2520using%2520The%2520Greulich%2520Pyle%2520%2528GP%2529%2520or%250ATanner%2520Whitehouse%2520%2528TW%2529%2520approach.%2520These%2520approaches%2520require%2520a%2520high%2520level%2520of%250Aexpertise%2520and%2520may%2520often%2520lead%2520to%2520observer%2520bias.%2520Hence%252C%2520to%2520automate%2520the%2520process%250Aof%2520assessing%2520the%2520X-rays%252C%2520and%2520to%2520increase%2520its%2520accuracy%2520and%2520efficiency%252C%2520several%250Amachine%2520learning%2520models%2520have%2520been%2520developed.%2520These%2520machine-learning%2520models%2520have%250Aseveral%2520differences%2520in%2520their%2520accuracy%2520and%2520efficiencies%252C%2520leading%2520to%2520an%2520unclear%250Achoice%2520for%2520the%2520suitable%2520model%2520depending%2520on%2520their%2520needs%2520and%2520available%2520resources.%250AMethods%253A%2520In%2520this%2520study%252C%2520we%2520have%2520analyzed%2520the%25203%2520most%2520widely%2520used%2520models%2520for%2520the%250Aautomation%2520of%2520bone%2520age%2520prediction%252C%2520which%2520are%2520the%2520Xception%2520model%252C%2520VGG%2520model%2520and%250ACNN%2520model.%2520These%2520models%2520were%2520trained%2520on%2520the%2520preprocessed%2520dataset%2520and%2520the%250Aaccuracy%2520was%2520measured%2520using%2520the%2520MAE%2520in%2520terms%2520of%2520months%2520for%2520each%2520model.%2520Using%250Athis%252C%2520the%2520comparison%2520between%2520the%2520models%2520was%2520done.%2520Results%253A%2520The%25203%2520models%252C%250AXception%252C%2520VGG%252C%2520and%2520CNN%2520models%2520have%2520been%2520tested%2520for%2520accuracy%2520and%2520other%2520relevant%250Afactors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10345v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparative%20Analysis%20of%20Machine%20Learning%20Approaches%20for%20Bone%20Age%0A%20%20Assessment%3A%20A%20Comprehensive%20Study%20on%20Three%20Distinct%20Models&entry.906535625=Nandavardhan%20R.%20and%20Somanathan%20R.%20and%20Vikram%20Suresh%20and%20Savaridassan%20P&entry.1292438233=%20%20Radiologists%20and%20doctors%20make%20use%20of%20X-ray%20images%20of%20the%20non-dominant%20hands%0Aof%20children%20and%20infants%20to%20assess%20the%20possibility%20of%20genetic%20conditions%20and%0Agrowth%20abnormalities.%20This%20is%20done%20by%20assessing%20the%20difference%20between%20the%0Aactual%20extent%20of%20growth%20found%20using%20the%20X-rays%20and%20the%20chronological%20age%20of%20the%0Asubject.%20The%20assessment%20was%20done%20conventionally%20using%20The%20Greulich%20Pyle%20%28GP%29%20or%0ATanner%20Whitehouse%20%28TW%29%20approach.%20These%20approaches%20require%20a%20high%20level%20of%0Aexpertise%20and%20may%20often%20lead%20to%20observer%20bias.%20Hence%2C%20to%20automate%20the%20process%0Aof%20assessing%20the%20X-rays%2C%20and%20to%20increase%20its%20accuracy%20and%20efficiency%2C%20several%0Amachine%20learning%20models%20have%20been%20developed.%20These%20machine-learning%20models%20have%0Aseveral%20differences%20in%20their%20accuracy%20and%20efficiencies%2C%20leading%20to%20an%20unclear%0Achoice%20for%20the%20suitable%20model%20depending%20on%20their%20needs%20and%20available%20resources.%0AMethods%3A%20In%20this%20study%2C%20we%20have%20analyzed%20the%203%20most%20widely%20used%20models%20for%20the%0Aautomation%20of%20bone%20age%20prediction%2C%20which%20are%20the%20Xception%20model%2C%20VGG%20model%20and%0ACNN%20model.%20These%20models%20were%20trained%20on%20the%20preprocessed%20dataset%20and%20the%0Aaccuracy%20was%20measured%20using%20the%20MAE%20in%20terms%20of%20months%20for%20each%20model.%20Using%0Athis%2C%20the%20comparison%20between%20the%20models%20was%20done.%20Results%3A%20The%203%20models%2C%0AXception%2C%20VGG%2C%20and%20CNN%20models%20have%20been%20tested%20for%20accuracy%20and%20other%20relevant%0Afactors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10345v1&entry.124074799=Read"},
{"title": "Safe Navigation in Unmapped Environments for Robotic Systems with Input\n  Constraints", "author": "Amirsaeid Safari and Jesse B. Hoagg", "abstract": "  This paper presents an approach for navigation and control in unmapped\nenvironments under input and state constraints using a composite control\nbarrier function (CBF). We consider the scenario where real-time perception\nfeedback (e.g., LiDAR) is used online to construct a local CBF that models\nlocal state constraints (e.g., local safety constraints such as obstacles) in\nthe a priori unmapped environment. The approach employs a soft-maximum function\nto synthesize a single time-varying CBF from the N most recently obtained local\nCBFs. Next, the input constraints are transformed into controller-state\nconstraints through the use of control dynamics. Then, we use a soft-minimum\nfunction to compose the input constraints with the time-varying CBF that models\nthe a priori unmapped environment. This composition yields a single relaxed\nCBF, which is used in a constrained optimization to obtain an optimal control\nthat satisfies the state and input constraints. The approach is validated\nthrough simulations of a nonholonomic ground robot that is equipped with LiDAR\nand navigates an unmapped environment. The robot successfully navigates the\nenvironment while avoiding the a priori unmapped obstacles and satisfying both\nspeed and input constraints.\n", "link": "http://arxiv.org/abs/2410.02106v2", "date": "2024-11-15", "relevancy": 2.339, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5987}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.584}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safe%20Navigation%20in%20Unmapped%20Environments%20for%20Robotic%20Systems%20with%20Input%0A%20%20Constraints&body=Title%3A%20Safe%20Navigation%20in%20Unmapped%20Environments%20for%20Robotic%20Systems%20with%20Input%0A%20%20Constraints%0AAuthor%3A%20Amirsaeid%20Safari%20and%20Jesse%20B.%20Hoagg%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20approach%20for%20navigation%20and%20control%20in%20unmapped%0Aenvironments%20under%20input%20and%20state%20constraints%20using%20a%20composite%20control%0Abarrier%20function%20%28CBF%29.%20We%20consider%20the%20scenario%20where%20real-time%20perception%0Afeedback%20%28e.g.%2C%20LiDAR%29%20is%20used%20online%20to%20construct%20a%20local%20CBF%20that%20models%0Alocal%20state%20constraints%20%28e.g.%2C%20local%20safety%20constraints%20such%20as%20obstacles%29%20in%0Athe%20a%20priori%20unmapped%20environment.%20The%20approach%20employs%20a%20soft-maximum%20function%0Ato%20synthesize%20a%20single%20time-varying%20CBF%20from%20the%20N%20most%20recently%20obtained%20local%0ACBFs.%20Next%2C%20the%20input%20constraints%20are%20transformed%20into%20controller-state%0Aconstraints%20through%20the%20use%20of%20control%20dynamics.%20Then%2C%20we%20use%20a%20soft-minimum%0Afunction%20to%20compose%20the%20input%20constraints%20with%20the%20time-varying%20CBF%20that%20models%0Athe%20a%20priori%20unmapped%20environment.%20This%20composition%20yields%20a%20single%20relaxed%0ACBF%2C%20which%20is%20used%20in%20a%20constrained%20optimization%20to%20obtain%20an%20optimal%20control%0Athat%20satisfies%20the%20state%20and%20input%20constraints.%20The%20approach%20is%20validated%0Athrough%20simulations%20of%20a%20nonholonomic%20ground%20robot%20that%20is%20equipped%20with%20LiDAR%0Aand%20navigates%20an%20unmapped%20environment.%20The%20robot%20successfully%20navigates%20the%0Aenvironment%20while%20avoiding%20the%20a%20priori%20unmapped%20obstacles%20and%20satisfying%20both%0Aspeed%20and%20input%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02106v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafe%2520Navigation%2520in%2520Unmapped%2520Environments%2520for%2520Robotic%2520Systems%2520with%2520Input%250A%2520%2520Constraints%26entry.906535625%3DAmirsaeid%2520Safari%2520and%2520Jesse%2520B.%2520Hoagg%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520approach%2520for%2520navigation%2520and%2520control%2520in%2520unmapped%250Aenvironments%2520under%2520input%2520and%2520state%2520constraints%2520using%2520a%2520composite%2520control%250Abarrier%2520function%2520%2528CBF%2529.%2520We%2520consider%2520the%2520scenario%2520where%2520real-time%2520perception%250Afeedback%2520%2528e.g.%252C%2520LiDAR%2529%2520is%2520used%2520online%2520to%2520construct%2520a%2520local%2520CBF%2520that%2520models%250Alocal%2520state%2520constraints%2520%2528e.g.%252C%2520local%2520safety%2520constraints%2520such%2520as%2520obstacles%2529%2520in%250Athe%2520a%2520priori%2520unmapped%2520environment.%2520The%2520approach%2520employs%2520a%2520soft-maximum%2520function%250Ato%2520synthesize%2520a%2520single%2520time-varying%2520CBF%2520from%2520the%2520N%2520most%2520recently%2520obtained%2520local%250ACBFs.%2520Next%252C%2520the%2520input%2520constraints%2520are%2520transformed%2520into%2520controller-state%250Aconstraints%2520through%2520the%2520use%2520of%2520control%2520dynamics.%2520Then%252C%2520we%2520use%2520a%2520soft-minimum%250Afunction%2520to%2520compose%2520the%2520input%2520constraints%2520with%2520the%2520time-varying%2520CBF%2520that%2520models%250Athe%2520a%2520priori%2520unmapped%2520environment.%2520This%2520composition%2520yields%2520a%2520single%2520relaxed%250ACBF%252C%2520which%2520is%2520used%2520in%2520a%2520constrained%2520optimization%2520to%2520obtain%2520an%2520optimal%2520control%250Athat%2520satisfies%2520the%2520state%2520and%2520input%2520constraints.%2520The%2520approach%2520is%2520validated%250Athrough%2520simulations%2520of%2520a%2520nonholonomic%2520ground%2520robot%2520that%2520is%2520equipped%2520with%2520LiDAR%250Aand%2520navigates%2520an%2520unmapped%2520environment.%2520The%2520robot%2520successfully%2520navigates%2520the%250Aenvironment%2520while%2520avoiding%2520the%2520a%2520priori%2520unmapped%2520obstacles%2520and%2520satisfying%2520both%250Aspeed%2520and%2520input%2520constraints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02106v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20Navigation%20in%20Unmapped%20Environments%20for%20Robotic%20Systems%20with%20Input%0A%20%20Constraints&entry.906535625=Amirsaeid%20Safari%20and%20Jesse%20B.%20Hoagg&entry.1292438233=%20%20This%20paper%20presents%20an%20approach%20for%20navigation%20and%20control%20in%20unmapped%0Aenvironments%20under%20input%20and%20state%20constraints%20using%20a%20composite%20control%0Abarrier%20function%20%28CBF%29.%20We%20consider%20the%20scenario%20where%20real-time%20perception%0Afeedback%20%28e.g.%2C%20LiDAR%29%20is%20used%20online%20to%20construct%20a%20local%20CBF%20that%20models%0Alocal%20state%20constraints%20%28e.g.%2C%20local%20safety%20constraints%20such%20as%20obstacles%29%20in%0Athe%20a%20priori%20unmapped%20environment.%20The%20approach%20employs%20a%20soft-maximum%20function%0Ato%20synthesize%20a%20single%20time-varying%20CBF%20from%20the%20N%20most%20recently%20obtained%20local%0ACBFs.%20Next%2C%20the%20input%20constraints%20are%20transformed%20into%20controller-state%0Aconstraints%20through%20the%20use%20of%20control%20dynamics.%20Then%2C%20we%20use%20a%20soft-minimum%0Afunction%20to%20compose%20the%20input%20constraints%20with%20the%20time-varying%20CBF%20that%20models%0Athe%20a%20priori%20unmapped%20environment.%20This%20composition%20yields%20a%20single%20relaxed%0ACBF%2C%20which%20is%20used%20in%20a%20constrained%20optimization%20to%20obtain%20an%20optimal%20control%0Athat%20satisfies%20the%20state%20and%20input%20constraints.%20The%20approach%20is%20validated%0Athrough%20simulations%20of%20a%20nonholonomic%20ground%20robot%20that%20is%20equipped%20with%20LiDAR%0Aand%20navigates%20an%20unmapped%20environment.%20The%20robot%20successfully%20navigates%20the%0Aenvironment%20while%20avoiding%20the%20a%20priori%20unmapped%20obstacles%20and%20satisfying%20both%0Aspeed%20and%20input%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02106v2&entry.124074799=Read"},
{"title": "Probabilistic Prior Driven Attention Mechanism Based on Diffusion Model\n  for Imaging Through Atmospheric Turbulence", "author": "Guodong Sun and Qixiang Ma and Liqiang Zhang and Hongwei Wang and Zixuan Gao and Haotian Zhang", "abstract": "  Atmospheric turbulence introduces severe spatial and geometric distortions,\nchallenging traditional image restoration methods. We propose the Probabilistic\nPrior Turbulence Removal Network (PPTRN), which combines probabilistic\ndiffusion-based prior modeling with Transformer-driven feature extraction to\naddress this issue. PPTRN employs a two-stage approach: first, a latent encoder\nand Transformer are jointly trained on clear images to establish robust feature\nrepresentations. Then, a Denoising Diffusion Probabilistic Model (DDPM) models\nprior distributions over latent vectors, guiding the Transformer in capturing\ndiverse feature variations essential for restoration. A key innovation in PPTRN\nis the Probabilistic Prior Driven Cross Attention mechanism, which integrates\nthe DDPM-generated prior with feature embeddings to reduce artifacts and\nenhance spatial coherence. Extensive experiments validate that PPTRN\nsignificantly improves restoration quality on turbulence-degraded images,\nsetting a new benchmark in clarity and structural fidelity.\n", "link": "http://arxiv.org/abs/2411.10321v1", "date": "2024-11-15", "relevancy": 2.3332, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6169}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.592}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20Prior%20Driven%20Attention%20Mechanism%20Based%20on%20Diffusion%20Model%0A%20%20for%20Imaging%20Through%20Atmospheric%20Turbulence&body=Title%3A%20Probabilistic%20Prior%20Driven%20Attention%20Mechanism%20Based%20on%20Diffusion%20Model%0A%20%20for%20Imaging%20Through%20Atmospheric%20Turbulence%0AAuthor%3A%20Guodong%20Sun%20and%20Qixiang%20Ma%20and%20Liqiang%20Zhang%20and%20Hongwei%20Wang%20and%20Zixuan%20Gao%20and%20Haotian%20Zhang%0AAbstract%3A%20%20%20Atmospheric%20turbulence%20introduces%20severe%20spatial%20and%20geometric%20distortions%2C%0Achallenging%20traditional%20image%20restoration%20methods.%20We%20propose%20the%20Probabilistic%0APrior%20Turbulence%20Removal%20Network%20%28PPTRN%29%2C%20which%20combines%20probabilistic%0Adiffusion-based%20prior%20modeling%20with%20Transformer-driven%20feature%20extraction%20to%0Aaddress%20this%20issue.%20PPTRN%20employs%20a%20two-stage%20approach%3A%20first%2C%20a%20latent%20encoder%0Aand%20Transformer%20are%20jointly%20trained%20on%20clear%20images%20to%20establish%20robust%20feature%0Arepresentations.%20Then%2C%20a%20Denoising%20Diffusion%20Probabilistic%20Model%20%28DDPM%29%20models%0Aprior%20distributions%20over%20latent%20vectors%2C%20guiding%20the%20Transformer%20in%20capturing%0Adiverse%20feature%20variations%20essential%20for%20restoration.%20A%20key%20innovation%20in%20PPTRN%0Ais%20the%20Probabilistic%20Prior%20Driven%20Cross%20Attention%20mechanism%2C%20which%20integrates%0Athe%20DDPM-generated%20prior%20with%20feature%20embeddings%20to%20reduce%20artifacts%20and%0Aenhance%20spatial%20coherence.%20Extensive%20experiments%20validate%20that%20PPTRN%0Asignificantly%20improves%20restoration%20quality%20on%20turbulence-degraded%20images%2C%0Asetting%20a%20new%20benchmark%20in%20clarity%20and%20structural%20fidelity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10321v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbabilistic%2520Prior%2520Driven%2520Attention%2520Mechanism%2520Based%2520on%2520Diffusion%2520Model%250A%2520%2520for%2520Imaging%2520Through%2520Atmospheric%2520Turbulence%26entry.906535625%3DGuodong%2520Sun%2520and%2520Qixiang%2520Ma%2520and%2520Liqiang%2520Zhang%2520and%2520Hongwei%2520Wang%2520and%2520Zixuan%2520Gao%2520and%2520Haotian%2520Zhang%26entry.1292438233%3D%2520%2520Atmospheric%2520turbulence%2520introduces%2520severe%2520spatial%2520and%2520geometric%2520distortions%252C%250Achallenging%2520traditional%2520image%2520restoration%2520methods.%2520We%2520propose%2520the%2520Probabilistic%250APrior%2520Turbulence%2520Removal%2520Network%2520%2528PPTRN%2529%252C%2520which%2520combines%2520probabilistic%250Adiffusion-based%2520prior%2520modeling%2520with%2520Transformer-driven%2520feature%2520extraction%2520to%250Aaddress%2520this%2520issue.%2520PPTRN%2520employs%2520a%2520two-stage%2520approach%253A%2520first%252C%2520a%2520latent%2520encoder%250Aand%2520Transformer%2520are%2520jointly%2520trained%2520on%2520clear%2520images%2520to%2520establish%2520robust%2520feature%250Arepresentations.%2520Then%252C%2520a%2520Denoising%2520Diffusion%2520Probabilistic%2520Model%2520%2528DDPM%2529%2520models%250Aprior%2520distributions%2520over%2520latent%2520vectors%252C%2520guiding%2520the%2520Transformer%2520in%2520capturing%250Adiverse%2520feature%2520variations%2520essential%2520for%2520restoration.%2520A%2520key%2520innovation%2520in%2520PPTRN%250Ais%2520the%2520Probabilistic%2520Prior%2520Driven%2520Cross%2520Attention%2520mechanism%252C%2520which%2520integrates%250Athe%2520DDPM-generated%2520prior%2520with%2520feature%2520embeddings%2520to%2520reduce%2520artifacts%2520and%250Aenhance%2520spatial%2520coherence.%2520Extensive%2520experiments%2520validate%2520that%2520PPTRN%250Asignificantly%2520improves%2520restoration%2520quality%2520on%2520turbulence-degraded%2520images%252C%250Asetting%2520a%2520new%2520benchmark%2520in%2520clarity%2520and%2520structural%2520fidelity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10321v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Prior%20Driven%20Attention%20Mechanism%20Based%20on%20Diffusion%20Model%0A%20%20for%20Imaging%20Through%20Atmospheric%20Turbulence&entry.906535625=Guodong%20Sun%20and%20Qixiang%20Ma%20and%20Liqiang%20Zhang%20and%20Hongwei%20Wang%20and%20Zixuan%20Gao%20and%20Haotian%20Zhang&entry.1292438233=%20%20Atmospheric%20turbulence%20introduces%20severe%20spatial%20and%20geometric%20distortions%2C%0Achallenging%20traditional%20image%20restoration%20methods.%20We%20propose%20the%20Probabilistic%0APrior%20Turbulence%20Removal%20Network%20%28PPTRN%29%2C%20which%20combines%20probabilistic%0Adiffusion-based%20prior%20modeling%20with%20Transformer-driven%20feature%20extraction%20to%0Aaddress%20this%20issue.%20PPTRN%20employs%20a%20two-stage%20approach%3A%20first%2C%20a%20latent%20encoder%0Aand%20Transformer%20are%20jointly%20trained%20on%20clear%20images%20to%20establish%20robust%20feature%0Arepresentations.%20Then%2C%20a%20Denoising%20Diffusion%20Probabilistic%20Model%20%28DDPM%29%20models%0Aprior%20distributions%20over%20latent%20vectors%2C%20guiding%20the%20Transformer%20in%20capturing%0Adiverse%20feature%20variations%20essential%20for%20restoration.%20A%20key%20innovation%20in%20PPTRN%0Ais%20the%20Probabilistic%20Prior%20Driven%20Cross%20Attention%20mechanism%2C%20which%20integrates%0Athe%20DDPM-generated%20prior%20with%20feature%20embeddings%20to%20reduce%20artifacts%20and%0Aenhance%20spatial%20coherence.%20Extensive%20experiments%20validate%20that%20PPTRN%0Asignificantly%20improves%20restoration%20quality%20on%20turbulence-degraded%20images%2C%0Asetting%20a%20new%20benchmark%20in%20clarity%20and%20structural%20fidelity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10321v1&entry.124074799=Read"},
{"title": "GenoCraft: A Comprehensive, User-Friendly Web-Based Platform for\n  High-Throughput Omics Data Analysis and Visualization", "author": "Yingzhou Lu and Minjie Shen and Ling Yue and Chenhao Li and Lulu Chen and Fan Meng and Xiao Wang and David Herrington and Yue Wang and Yue Zhao and Tianfan Fu and Capucine Van Rechem", "abstract": "  The surge in high-throughput omics data has reshaped the landscape of\nbiological research, underlining the need for powerful, user-friendly data\nanalysis and interpretation tools. This paper presents GenoCraft, a web-based\ncomprehensive software solution designed to handle the entire pipeline of omics\ndata processing. GenoCraft offers a unified platform featuring advanced\nbioinformatics tools, covering all aspects of omics data analysis. It\nencompasses a range of functionalities, such as normalization, quality control,\ndifferential analysis, network analysis, pathway analysis, and diverse\nvisualization techniques. This software makes state-of-the-art omics data\nanalysis more accessible to a wider range of users. With GenoCraft, researchers\nand data scientists have access to an array of cutting-edge bioinformatics\ntools under a user-friendly interface, making it a valuable resource for\nmanaging and analyzing large-scale omics data. The API with an interactive web\ninterface is publicly available at https://genocraft.stanford. edu/. We also\nrelease all the codes in https://github.com/futianfan/GenoCraft.\n", "link": "http://arxiv.org/abs/2312.14249v3", "date": "2024-11-15", "relevancy": 2.3119, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4826}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4523}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenoCraft%3A%20A%20Comprehensive%2C%20User-Friendly%20Web-Based%20Platform%20for%0A%20%20High-Throughput%20Omics%20Data%20Analysis%20and%20Visualization&body=Title%3A%20GenoCraft%3A%20A%20Comprehensive%2C%20User-Friendly%20Web-Based%20Platform%20for%0A%20%20High-Throughput%20Omics%20Data%20Analysis%20and%20Visualization%0AAuthor%3A%20Yingzhou%20Lu%20and%20Minjie%20Shen%20and%20Ling%20Yue%20and%20Chenhao%20Li%20and%20Lulu%20Chen%20and%20Fan%20Meng%20and%20Xiao%20Wang%20and%20David%20Herrington%20and%20Yue%20Wang%20and%20Yue%20Zhao%20and%20Tianfan%20Fu%20and%20Capucine%20Van%20Rechem%0AAbstract%3A%20%20%20The%20surge%20in%20high-throughput%20omics%20data%20has%20reshaped%20the%20landscape%20of%0Abiological%20research%2C%20underlining%20the%20need%20for%20powerful%2C%20user-friendly%20data%0Aanalysis%20and%20interpretation%20tools.%20This%20paper%20presents%20GenoCraft%2C%20a%20web-based%0Acomprehensive%20software%20solution%20designed%20to%20handle%20the%20entire%20pipeline%20of%20omics%0Adata%20processing.%20GenoCraft%20offers%20a%20unified%20platform%20featuring%20advanced%0Abioinformatics%20tools%2C%20covering%20all%20aspects%20of%20omics%20data%20analysis.%20It%0Aencompasses%20a%20range%20of%20functionalities%2C%20such%20as%20normalization%2C%20quality%20control%2C%0Adifferential%20analysis%2C%20network%20analysis%2C%20pathway%20analysis%2C%20and%20diverse%0Avisualization%20techniques.%20This%20software%20makes%20state-of-the-art%20omics%20data%0Aanalysis%20more%20accessible%20to%20a%20wider%20range%20of%20users.%20With%20GenoCraft%2C%20researchers%0Aand%20data%20scientists%20have%20access%20to%20an%20array%20of%20cutting-edge%20bioinformatics%0Atools%20under%20a%20user-friendly%20interface%2C%20making%20it%20a%20valuable%20resource%20for%0Amanaging%20and%20analyzing%20large-scale%20omics%20data.%20The%20API%20with%20an%20interactive%20web%0Ainterface%20is%20publicly%20available%20at%20https%3A//genocraft.stanford.%20edu/.%20We%20also%0Arelease%20all%20the%20codes%20in%20https%3A//github.com/futianfan/GenoCraft.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.14249v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenoCraft%253A%2520A%2520Comprehensive%252C%2520User-Friendly%2520Web-Based%2520Platform%2520for%250A%2520%2520High-Throughput%2520Omics%2520Data%2520Analysis%2520and%2520Visualization%26entry.906535625%3DYingzhou%2520Lu%2520and%2520Minjie%2520Shen%2520and%2520Ling%2520Yue%2520and%2520Chenhao%2520Li%2520and%2520Lulu%2520Chen%2520and%2520Fan%2520Meng%2520and%2520Xiao%2520Wang%2520and%2520David%2520Herrington%2520and%2520Yue%2520Wang%2520and%2520Yue%2520Zhao%2520and%2520Tianfan%2520Fu%2520and%2520Capucine%2520Van%2520Rechem%26entry.1292438233%3D%2520%2520The%2520surge%2520in%2520high-throughput%2520omics%2520data%2520has%2520reshaped%2520the%2520landscape%2520of%250Abiological%2520research%252C%2520underlining%2520the%2520need%2520for%2520powerful%252C%2520user-friendly%2520data%250Aanalysis%2520and%2520interpretation%2520tools.%2520This%2520paper%2520presents%2520GenoCraft%252C%2520a%2520web-based%250Acomprehensive%2520software%2520solution%2520designed%2520to%2520handle%2520the%2520entire%2520pipeline%2520of%2520omics%250Adata%2520processing.%2520GenoCraft%2520offers%2520a%2520unified%2520platform%2520featuring%2520advanced%250Abioinformatics%2520tools%252C%2520covering%2520all%2520aspects%2520of%2520omics%2520data%2520analysis.%2520It%250Aencompasses%2520a%2520range%2520of%2520functionalities%252C%2520such%2520as%2520normalization%252C%2520quality%2520control%252C%250Adifferential%2520analysis%252C%2520network%2520analysis%252C%2520pathway%2520analysis%252C%2520and%2520diverse%250Avisualization%2520techniques.%2520This%2520software%2520makes%2520state-of-the-art%2520omics%2520data%250Aanalysis%2520more%2520accessible%2520to%2520a%2520wider%2520range%2520of%2520users.%2520With%2520GenoCraft%252C%2520researchers%250Aand%2520data%2520scientists%2520have%2520access%2520to%2520an%2520array%2520of%2520cutting-edge%2520bioinformatics%250Atools%2520under%2520a%2520user-friendly%2520interface%252C%2520making%2520it%2520a%2520valuable%2520resource%2520for%250Amanaging%2520and%2520analyzing%2520large-scale%2520omics%2520data.%2520The%2520API%2520with%2520an%2520interactive%2520web%250Ainterface%2520is%2520publicly%2520available%2520at%2520https%253A//genocraft.stanford.%2520edu/.%2520We%2520also%250Arelease%2520all%2520the%2520codes%2520in%2520https%253A//github.com/futianfan/GenoCraft.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.14249v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenoCraft%3A%20A%20Comprehensive%2C%20User-Friendly%20Web-Based%20Platform%20for%0A%20%20High-Throughput%20Omics%20Data%20Analysis%20and%20Visualization&entry.906535625=Yingzhou%20Lu%20and%20Minjie%20Shen%20and%20Ling%20Yue%20and%20Chenhao%20Li%20and%20Lulu%20Chen%20and%20Fan%20Meng%20and%20Xiao%20Wang%20and%20David%20Herrington%20and%20Yue%20Wang%20and%20Yue%20Zhao%20and%20Tianfan%20Fu%20and%20Capucine%20Van%20Rechem&entry.1292438233=%20%20The%20surge%20in%20high-throughput%20omics%20data%20has%20reshaped%20the%20landscape%20of%0Abiological%20research%2C%20underlining%20the%20need%20for%20powerful%2C%20user-friendly%20data%0Aanalysis%20and%20interpretation%20tools.%20This%20paper%20presents%20GenoCraft%2C%20a%20web-based%0Acomprehensive%20software%20solution%20designed%20to%20handle%20the%20entire%20pipeline%20of%20omics%0Adata%20processing.%20GenoCraft%20offers%20a%20unified%20platform%20featuring%20advanced%0Abioinformatics%20tools%2C%20covering%20all%20aspects%20of%20omics%20data%20analysis.%20It%0Aencompasses%20a%20range%20of%20functionalities%2C%20such%20as%20normalization%2C%20quality%20control%2C%0Adifferential%20analysis%2C%20network%20analysis%2C%20pathway%20analysis%2C%20and%20diverse%0Avisualization%20techniques.%20This%20software%20makes%20state-of-the-art%20omics%20data%0Aanalysis%20more%20accessible%20to%20a%20wider%20range%20of%20users.%20With%20GenoCraft%2C%20researchers%0Aand%20data%20scientists%20have%20access%20to%20an%20array%20of%20cutting-edge%20bioinformatics%0Atools%20under%20a%20user-friendly%20interface%2C%20making%20it%20a%20valuable%20resource%20for%0Amanaging%20and%20analyzing%20large-scale%20omics%20data.%20The%20API%20with%20an%20interactive%20web%0Ainterface%20is%20publicly%20available%20at%20https%3A//genocraft.stanford.%20edu/.%20We%20also%0Arelease%20all%20the%20codes%20in%20https%3A//github.com/futianfan/GenoCraft.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.14249v3&entry.124074799=Read"},
{"title": "Y-MAP-Net: Real-time depth, normals, segmentation, multi-label\n  captioning and 2D human pose in RGB images", "author": "Ammar Qammaz and Nikolaos Vasilikopoulos and Iason Oikonomidis and Antonis A. Argyros", "abstract": "  We present Y-MAP-Net, a Y-shaped neural network architecture designed for\nreal-time multi-task learning on RGB images. Y-MAP-Net, simultaneously predicts\ndepth, surface normals, human pose, semantic segmentation and generates\nmulti-label captions, all from a single network evaluation. To achieve this, we\nadopt a multi-teacher, single-student training paradigm, where task-specific\nfoundation models supervise the network's learning, enabling it to distill\ntheir capabilities into a lightweight architecture suitable for real-time\napplications. Y-MAP-Net, exhibits strong generalization, simplicity and\ncomputational efficiency, making it ideal for robotics and other practical\nscenarios. To support future research, we will release our code publicly.\n", "link": "http://arxiv.org/abs/2411.10334v1", "date": "2024-11-15", "relevancy": 2.2592, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5783}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5558}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Y-MAP-Net%3A%20Real-time%20depth%2C%20normals%2C%20segmentation%2C%20multi-label%0A%20%20captioning%20and%202D%20human%20pose%20in%20RGB%20images&body=Title%3A%20Y-MAP-Net%3A%20Real-time%20depth%2C%20normals%2C%20segmentation%2C%20multi-label%0A%20%20captioning%20and%202D%20human%20pose%20in%20RGB%20images%0AAuthor%3A%20Ammar%20Qammaz%20and%20Nikolaos%20Vasilikopoulos%20and%20Iason%20Oikonomidis%20and%20Antonis%20A.%20Argyros%0AAbstract%3A%20%20%20We%20present%20Y-MAP-Net%2C%20a%20Y-shaped%20neural%20network%20architecture%20designed%20for%0Areal-time%20multi-task%20learning%20on%20RGB%20images.%20Y-MAP-Net%2C%20simultaneously%20predicts%0Adepth%2C%20surface%20normals%2C%20human%20pose%2C%20semantic%20segmentation%20and%20generates%0Amulti-label%20captions%2C%20all%20from%20a%20single%20network%20evaluation.%20To%20achieve%20this%2C%20we%0Aadopt%20a%20multi-teacher%2C%20single-student%20training%20paradigm%2C%20where%20task-specific%0Afoundation%20models%20supervise%20the%20network%27s%20learning%2C%20enabling%20it%20to%20distill%0Atheir%20capabilities%20into%20a%20lightweight%20architecture%20suitable%20for%20real-time%0Aapplications.%20Y-MAP-Net%2C%20exhibits%20strong%20generalization%2C%20simplicity%20and%0Acomputational%20efficiency%2C%20making%20it%20ideal%20for%20robotics%20and%20other%20practical%0Ascenarios.%20To%20support%20future%20research%2C%20we%20will%20release%20our%20code%20publicly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10334v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DY-MAP-Net%253A%2520Real-time%2520depth%252C%2520normals%252C%2520segmentation%252C%2520multi-label%250A%2520%2520captioning%2520and%25202D%2520human%2520pose%2520in%2520RGB%2520images%26entry.906535625%3DAmmar%2520Qammaz%2520and%2520Nikolaos%2520Vasilikopoulos%2520and%2520Iason%2520Oikonomidis%2520and%2520Antonis%2520A.%2520Argyros%26entry.1292438233%3D%2520%2520We%2520present%2520Y-MAP-Net%252C%2520a%2520Y-shaped%2520neural%2520network%2520architecture%2520designed%2520for%250Areal-time%2520multi-task%2520learning%2520on%2520RGB%2520images.%2520Y-MAP-Net%252C%2520simultaneously%2520predicts%250Adepth%252C%2520surface%2520normals%252C%2520human%2520pose%252C%2520semantic%2520segmentation%2520and%2520generates%250Amulti-label%2520captions%252C%2520all%2520from%2520a%2520single%2520network%2520evaluation.%2520To%2520achieve%2520this%252C%2520we%250Aadopt%2520a%2520multi-teacher%252C%2520single-student%2520training%2520paradigm%252C%2520where%2520task-specific%250Afoundation%2520models%2520supervise%2520the%2520network%2527s%2520learning%252C%2520enabling%2520it%2520to%2520distill%250Atheir%2520capabilities%2520into%2520a%2520lightweight%2520architecture%2520suitable%2520for%2520real-time%250Aapplications.%2520Y-MAP-Net%252C%2520exhibits%2520strong%2520generalization%252C%2520simplicity%2520and%250Acomputational%2520efficiency%252C%2520making%2520it%2520ideal%2520for%2520robotics%2520and%2520other%2520practical%250Ascenarios.%2520To%2520support%2520future%2520research%252C%2520we%2520will%2520release%2520our%2520code%2520publicly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10334v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Y-MAP-Net%3A%20Real-time%20depth%2C%20normals%2C%20segmentation%2C%20multi-label%0A%20%20captioning%20and%202D%20human%20pose%20in%20RGB%20images&entry.906535625=Ammar%20Qammaz%20and%20Nikolaos%20Vasilikopoulos%20and%20Iason%20Oikonomidis%20and%20Antonis%20A.%20Argyros&entry.1292438233=%20%20We%20present%20Y-MAP-Net%2C%20a%20Y-shaped%20neural%20network%20architecture%20designed%20for%0Areal-time%20multi-task%20learning%20on%20RGB%20images.%20Y-MAP-Net%2C%20simultaneously%20predicts%0Adepth%2C%20surface%20normals%2C%20human%20pose%2C%20semantic%20segmentation%20and%20generates%0Amulti-label%20captions%2C%20all%20from%20a%20single%20network%20evaluation.%20To%20achieve%20this%2C%20we%0Aadopt%20a%20multi-teacher%2C%20single-student%20training%20paradigm%2C%20where%20task-specific%0Afoundation%20models%20supervise%20the%20network%27s%20learning%2C%20enabling%20it%20to%20distill%0Atheir%20capabilities%20into%20a%20lightweight%20architecture%20suitable%20for%20real-time%0Aapplications.%20Y-MAP-Net%2C%20exhibits%20strong%20generalization%2C%20simplicity%20and%0Acomputational%20efficiency%2C%20making%20it%20ideal%20for%20robotics%20and%20other%20practical%0Ascenarios.%20To%20support%20future%20research%2C%20we%20will%20release%20our%20code%20publicly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10334v1&entry.124074799=Read"},
{"title": "Number it: Temporal Grounding Videos like Flipping Manga", "author": "Yongliang Wu and Xinting Hu and Yuyang Sun and Yizhou Zhou and Wenbo Zhu and Fengyun Rao and Bernt Schiele and Xu Yang", "abstract": "  Video Large Language Models (Vid-LLMs) have made remarkable advancements in\ncomprehending video content for QA dialogue. However, they struggle to extend\nthis visual understanding to tasks requiring precise temporal localization,\nknown as Video Temporal Grounding (VTG). To address this gap, we introduce\nNumber-Prompt (NumPro), a novel method that empowers Vid-LLMs to bridge visual\ncomprehension with temporal grounding by adding unique numerical identifiers to\neach video frame. Treating a video as a sequence of numbered frame images,\nNumPro transforms VTG into an intuitive process: flipping through manga panels\nin sequence. This allows Vid-LLMs to \"read\" event timelines, accurately linking\nvisual content with corresponding temporal information. Our experiments\ndemonstrate that NumPro significantly boosts VTG performance of top-tier\nVid-LLMs without additional computational cost. Furthermore, fine-tuning on a\nNumPro-enhanced dataset defines a new state-of-the-art for VTG, surpassing\nprevious top-performing methods by up to 6.9\\% in mIoU for moment retrieval and\n8.5\\% in mAP for highlight detection. The code will be available at\nhttps://github.com/yongliang-wu/NumPro.\n", "link": "http://arxiv.org/abs/2411.10332v1", "date": "2024-11-15", "relevancy": 2.2553, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5788}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5552}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Number%20it%3A%20Temporal%20Grounding%20Videos%20like%20Flipping%20Manga&body=Title%3A%20Number%20it%3A%20Temporal%20Grounding%20Videos%20like%20Flipping%20Manga%0AAuthor%3A%20Yongliang%20Wu%20and%20Xinting%20Hu%20and%20Yuyang%20Sun%20and%20Yizhou%20Zhou%20and%20Wenbo%20Zhu%20and%20Fengyun%20Rao%20and%20Bernt%20Schiele%20and%20Xu%20Yang%0AAbstract%3A%20%20%20Video%20Large%20Language%20Models%20%28Vid-LLMs%29%20have%20made%20remarkable%20advancements%20in%0Acomprehending%20video%20content%20for%20QA%20dialogue.%20However%2C%20they%20struggle%20to%20extend%0Athis%20visual%20understanding%20to%20tasks%20requiring%20precise%20temporal%20localization%2C%0Aknown%20as%20Video%20Temporal%20Grounding%20%28VTG%29.%20To%20address%20this%20gap%2C%20we%20introduce%0ANumber-Prompt%20%28NumPro%29%2C%20a%20novel%20method%20that%20empowers%20Vid-LLMs%20to%20bridge%20visual%0Acomprehension%20with%20temporal%20grounding%20by%20adding%20unique%20numerical%20identifiers%20to%0Aeach%20video%20frame.%20Treating%20a%20video%20as%20a%20sequence%20of%20numbered%20frame%20images%2C%0ANumPro%20transforms%20VTG%20into%20an%20intuitive%20process%3A%20flipping%20through%20manga%20panels%0Ain%20sequence.%20This%20allows%20Vid-LLMs%20to%20%22read%22%20event%20timelines%2C%20accurately%20linking%0Avisual%20content%20with%20corresponding%20temporal%20information.%20Our%20experiments%0Ademonstrate%20that%20NumPro%20significantly%20boosts%20VTG%20performance%20of%20top-tier%0AVid-LLMs%20without%20additional%20computational%20cost.%20Furthermore%2C%20fine-tuning%20on%20a%0ANumPro-enhanced%20dataset%20defines%20a%20new%20state-of-the-art%20for%20VTG%2C%20surpassing%0Aprevious%20top-performing%20methods%20by%20up%20to%206.9%5C%25%20in%20mIoU%20for%20moment%20retrieval%20and%0A8.5%5C%25%20in%20mAP%20for%20highlight%20detection.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/yongliang-wu/NumPro.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10332v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNumber%2520it%253A%2520Temporal%2520Grounding%2520Videos%2520like%2520Flipping%2520Manga%26entry.906535625%3DYongliang%2520Wu%2520and%2520Xinting%2520Hu%2520and%2520Yuyang%2520Sun%2520and%2520Yizhou%2520Zhou%2520and%2520Wenbo%2520Zhu%2520and%2520Fengyun%2520Rao%2520and%2520Bernt%2520Schiele%2520and%2520Xu%2520Yang%26entry.1292438233%3D%2520%2520Video%2520Large%2520Language%2520Models%2520%2528Vid-LLMs%2529%2520have%2520made%2520remarkable%2520advancements%2520in%250Acomprehending%2520video%2520content%2520for%2520QA%2520dialogue.%2520However%252C%2520they%2520struggle%2520to%2520extend%250Athis%2520visual%2520understanding%2520to%2520tasks%2520requiring%2520precise%2520temporal%2520localization%252C%250Aknown%2520as%2520Video%2520Temporal%2520Grounding%2520%2528VTG%2529.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%250ANumber-Prompt%2520%2528NumPro%2529%252C%2520a%2520novel%2520method%2520that%2520empowers%2520Vid-LLMs%2520to%2520bridge%2520visual%250Acomprehension%2520with%2520temporal%2520grounding%2520by%2520adding%2520unique%2520numerical%2520identifiers%2520to%250Aeach%2520video%2520frame.%2520Treating%2520a%2520video%2520as%2520a%2520sequence%2520of%2520numbered%2520frame%2520images%252C%250ANumPro%2520transforms%2520VTG%2520into%2520an%2520intuitive%2520process%253A%2520flipping%2520through%2520manga%2520panels%250Ain%2520sequence.%2520This%2520allows%2520Vid-LLMs%2520to%2520%2522read%2522%2520event%2520timelines%252C%2520accurately%2520linking%250Avisual%2520content%2520with%2520corresponding%2520temporal%2520information.%2520Our%2520experiments%250Ademonstrate%2520that%2520NumPro%2520significantly%2520boosts%2520VTG%2520performance%2520of%2520top-tier%250AVid-LLMs%2520without%2520additional%2520computational%2520cost.%2520Furthermore%252C%2520fine-tuning%2520on%2520a%250ANumPro-enhanced%2520dataset%2520defines%2520a%2520new%2520state-of-the-art%2520for%2520VTG%252C%2520surpassing%250Aprevious%2520top-performing%2520methods%2520by%2520up%2520to%25206.9%255C%2525%2520in%2520mIoU%2520for%2520moment%2520retrieval%2520and%250A8.5%255C%2525%2520in%2520mAP%2520for%2520highlight%2520detection.%2520The%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/yongliang-wu/NumPro.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10332v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Number%20it%3A%20Temporal%20Grounding%20Videos%20like%20Flipping%20Manga&entry.906535625=Yongliang%20Wu%20and%20Xinting%20Hu%20and%20Yuyang%20Sun%20and%20Yizhou%20Zhou%20and%20Wenbo%20Zhu%20and%20Fengyun%20Rao%20and%20Bernt%20Schiele%20and%20Xu%20Yang&entry.1292438233=%20%20Video%20Large%20Language%20Models%20%28Vid-LLMs%29%20have%20made%20remarkable%20advancements%20in%0Acomprehending%20video%20content%20for%20QA%20dialogue.%20However%2C%20they%20struggle%20to%20extend%0Athis%20visual%20understanding%20to%20tasks%20requiring%20precise%20temporal%20localization%2C%0Aknown%20as%20Video%20Temporal%20Grounding%20%28VTG%29.%20To%20address%20this%20gap%2C%20we%20introduce%0ANumber-Prompt%20%28NumPro%29%2C%20a%20novel%20method%20that%20empowers%20Vid-LLMs%20to%20bridge%20visual%0Acomprehension%20with%20temporal%20grounding%20by%20adding%20unique%20numerical%20identifiers%20to%0Aeach%20video%20frame.%20Treating%20a%20video%20as%20a%20sequence%20of%20numbered%20frame%20images%2C%0ANumPro%20transforms%20VTG%20into%20an%20intuitive%20process%3A%20flipping%20through%20manga%20panels%0Ain%20sequence.%20This%20allows%20Vid-LLMs%20to%20%22read%22%20event%20timelines%2C%20accurately%20linking%0Avisual%20content%20with%20corresponding%20temporal%20information.%20Our%20experiments%0Ademonstrate%20that%20NumPro%20significantly%20boosts%20VTG%20performance%20of%20top-tier%0AVid-LLMs%20without%20additional%20computational%20cost.%20Furthermore%2C%20fine-tuning%20on%20a%0ANumPro-enhanced%20dataset%20defines%20a%20new%20state-of-the-art%20for%20VTG%2C%20surpassing%0Aprevious%20top-performing%20methods%20by%20up%20to%206.9%5C%25%20in%20mIoU%20for%20moment%20retrieval%20and%0A8.5%5C%25%20in%20mAP%20for%20highlight%20detection.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/yongliang-wu/NumPro.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10332v1&entry.124074799=Read"},
{"title": "Interactive Image-Based Aphid Counting in Yellow Water Traps under\n  Stirring Actions", "author": "Xumin Gao and Mark Stevens and Grzegorz Cielniak", "abstract": "  The current vision-based aphid counting methods in water traps suffer from\nundercounts caused by occlusions and low visibility arising from dense\naggregation of insects and other objects. To address this problem, we propose a\nnovel aphid counting method through interactive stirring actions. We use\ninteractive stirring to alter the distribution of aphids in the yellow water\ntrap and capture a sequence of images which are then used for aphid detection\nand counting through an optimized small object detection network based on\nYolov5. We also propose a counting confidence evaluation system to evaluate the\nconfidence of count-ing results. The final counting result is a weighted sum of\nthe counting results from all sequence images based on the counting confidence.\nExperimental results show that our proposed aphid detection network\nsignificantly outperforms the original Yolov5, with improvements of 33.9% in\nAP@0.5 and 26.9% in AP@[0.5:0.95] on the aphid test set. In addition, the aphid\ncounting test results using our proposed counting confidence evaluation system\nshow significant improvements over the static counting method, closely aligning\nwith manual counting results.\n", "link": "http://arxiv.org/abs/2411.10357v1", "date": "2024-11-15", "relevancy": 2.2391, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4518}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4492}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interactive%20Image-Based%20Aphid%20Counting%20in%20Yellow%20Water%20Traps%20under%0A%20%20Stirring%20Actions&body=Title%3A%20Interactive%20Image-Based%20Aphid%20Counting%20in%20Yellow%20Water%20Traps%20under%0A%20%20Stirring%20Actions%0AAuthor%3A%20Xumin%20Gao%20and%20Mark%20Stevens%20and%20Grzegorz%20Cielniak%0AAbstract%3A%20%20%20The%20current%20vision-based%20aphid%20counting%20methods%20in%20water%20traps%20suffer%20from%0Aundercounts%20caused%20by%20occlusions%20and%20low%20visibility%20arising%20from%20dense%0Aaggregation%20of%20insects%20and%20other%20objects.%20To%20address%20this%20problem%2C%20we%20propose%20a%0Anovel%20aphid%20counting%20method%20through%20interactive%20stirring%20actions.%20We%20use%0Ainteractive%20stirring%20to%20alter%20the%20distribution%20of%20aphids%20in%20the%20yellow%20water%0Atrap%20and%20capture%20a%20sequence%20of%20images%20which%20are%20then%20used%20for%20aphid%20detection%0Aand%20counting%20through%20an%20optimized%20small%20object%20detection%20network%20based%20on%0AYolov5.%20We%20also%20propose%20a%20counting%20confidence%20evaluation%20system%20to%20evaluate%20the%0Aconfidence%20of%20count-ing%20results.%20The%20final%20counting%20result%20is%20a%20weighted%20sum%20of%0Athe%20counting%20results%20from%20all%20sequence%20images%20based%20on%20the%20counting%20confidence.%0AExperimental%20results%20show%20that%20our%20proposed%20aphid%20detection%20network%0Asignificantly%20outperforms%20the%20original%20Yolov5%2C%20with%20improvements%20of%2033.9%25%20in%0AAP%400.5%20and%2026.9%25%20in%20AP%40%5B0.5%3A0.95%5D%20on%20the%20aphid%20test%20set.%20In%20addition%2C%20the%20aphid%0Acounting%20test%20results%20using%20our%20proposed%20counting%20confidence%20evaluation%20system%0Ashow%20significant%20improvements%20over%20the%20static%20counting%20method%2C%20closely%20aligning%0Awith%20manual%20counting%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10357v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInteractive%2520Image-Based%2520Aphid%2520Counting%2520in%2520Yellow%2520Water%2520Traps%2520under%250A%2520%2520Stirring%2520Actions%26entry.906535625%3DXumin%2520Gao%2520and%2520Mark%2520Stevens%2520and%2520Grzegorz%2520Cielniak%26entry.1292438233%3D%2520%2520The%2520current%2520vision-based%2520aphid%2520counting%2520methods%2520in%2520water%2520traps%2520suffer%2520from%250Aundercounts%2520caused%2520by%2520occlusions%2520and%2520low%2520visibility%2520arising%2520from%2520dense%250Aaggregation%2520of%2520insects%2520and%2520other%2520objects.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520a%250Anovel%2520aphid%2520counting%2520method%2520through%2520interactive%2520stirring%2520actions.%2520We%2520use%250Ainteractive%2520stirring%2520to%2520alter%2520the%2520distribution%2520of%2520aphids%2520in%2520the%2520yellow%2520water%250Atrap%2520and%2520capture%2520a%2520sequence%2520of%2520images%2520which%2520are%2520then%2520used%2520for%2520aphid%2520detection%250Aand%2520counting%2520through%2520an%2520optimized%2520small%2520object%2520detection%2520network%2520based%2520on%250AYolov5.%2520We%2520also%2520propose%2520a%2520counting%2520confidence%2520evaluation%2520system%2520to%2520evaluate%2520the%250Aconfidence%2520of%2520count-ing%2520results.%2520The%2520final%2520counting%2520result%2520is%2520a%2520weighted%2520sum%2520of%250Athe%2520counting%2520results%2520from%2520all%2520sequence%2520images%2520based%2520on%2520the%2520counting%2520confidence.%250AExperimental%2520results%2520show%2520that%2520our%2520proposed%2520aphid%2520detection%2520network%250Asignificantly%2520outperforms%2520the%2520original%2520Yolov5%252C%2520with%2520improvements%2520of%252033.9%2525%2520in%250AAP%25400.5%2520and%252026.9%2525%2520in%2520AP%2540%255B0.5%253A0.95%255D%2520on%2520the%2520aphid%2520test%2520set.%2520In%2520addition%252C%2520the%2520aphid%250Acounting%2520test%2520results%2520using%2520our%2520proposed%2520counting%2520confidence%2520evaluation%2520system%250Ashow%2520significant%2520improvements%2520over%2520the%2520static%2520counting%2520method%252C%2520closely%2520aligning%250Awith%2520manual%2520counting%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10357v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interactive%20Image-Based%20Aphid%20Counting%20in%20Yellow%20Water%20Traps%20under%0A%20%20Stirring%20Actions&entry.906535625=Xumin%20Gao%20and%20Mark%20Stevens%20and%20Grzegorz%20Cielniak&entry.1292438233=%20%20The%20current%20vision-based%20aphid%20counting%20methods%20in%20water%20traps%20suffer%20from%0Aundercounts%20caused%20by%20occlusions%20and%20low%20visibility%20arising%20from%20dense%0Aaggregation%20of%20insects%20and%20other%20objects.%20To%20address%20this%20problem%2C%20we%20propose%20a%0Anovel%20aphid%20counting%20method%20through%20interactive%20stirring%20actions.%20We%20use%0Ainteractive%20stirring%20to%20alter%20the%20distribution%20of%20aphids%20in%20the%20yellow%20water%0Atrap%20and%20capture%20a%20sequence%20of%20images%20which%20are%20then%20used%20for%20aphid%20detection%0Aand%20counting%20through%20an%20optimized%20small%20object%20detection%20network%20based%20on%0AYolov5.%20We%20also%20propose%20a%20counting%20confidence%20evaluation%20system%20to%20evaluate%20the%0Aconfidence%20of%20count-ing%20results.%20The%20final%20counting%20result%20is%20a%20weighted%20sum%20of%0Athe%20counting%20results%20from%20all%20sequence%20images%20based%20on%20the%20counting%20confidence.%0AExperimental%20results%20show%20that%20our%20proposed%20aphid%20detection%20network%0Asignificantly%20outperforms%20the%20original%20Yolov5%2C%20with%20improvements%20of%2033.9%25%20in%0AAP%400.5%20and%2026.9%25%20in%20AP%40%5B0.5%3A0.95%5D%20on%20the%20aphid%20test%20set.%20In%20addition%2C%20the%20aphid%0Acounting%20test%20results%20using%20our%20proposed%20counting%20confidence%20evaluation%20system%0Ashow%20significant%20improvements%20over%20the%20static%20counting%20method%2C%20closely%20aligning%0Awith%20manual%20counting%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10357v1&entry.124074799=Read"},
{"title": "Multidimensional Byte Pair Encoding: Shortened Sequences for Improved\n  Visual Data Generation", "author": "Tim Elsner and Paula Usinger and Julius Nehring-Wirxel and Gregor Kobsik and Victor Czech and Yanjiang He and Isaak Lim and Leif Kobbelt", "abstract": "  In language processing, transformers benefit greatly from text being\ncondensed. This is achieved through a larger vocabulary that captures word\nfragments instead of plain characters. This is often done with Byte Pair\nEncoding. In the context of images, tokenisation of visual data is usually\nlimited to regular grids obtained from quantisation methods, without global\ncontent awareness. Our work improves tokenisation of visual data by bringing\nByte Pair Encoding from 1D to multiple dimensions, as a complementary add-on to\nexisting compression. We achieve this through counting constellations of token\npairs and replacing the most frequent token pair with a newly introduced token.\nThe multidimensionality only increases the computation time by a factor of 2\nfor images, making it applicable even to large datasets like ImageNet within\nminutes on consumer hardware. This is a lossless preprocessing step. Our\nevaluation shows improved training and inference performance of transformers on\nvisual data achieved by compressing frequent constellations of tokens: The\nresulting sequences are shorter, with more uniformly distributed information\ncontent, e.g. condensing empty regions in an image into single tokens. As our\nexperiments show, these condensed sequences are easier to process. We\nadditionally introduce a strategy to amplify this compression further by\nclustering the vocabulary.\n", "link": "http://arxiv.org/abs/2411.10281v1", "date": "2024-11-15", "relevancy": 2.2336, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5689}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5518}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multidimensional%20Byte%20Pair%20Encoding%3A%20Shortened%20Sequences%20for%20Improved%0A%20%20Visual%20Data%20Generation&body=Title%3A%20Multidimensional%20Byte%20Pair%20Encoding%3A%20Shortened%20Sequences%20for%20Improved%0A%20%20Visual%20Data%20Generation%0AAuthor%3A%20Tim%20Elsner%20and%20Paula%20Usinger%20and%20Julius%20Nehring-Wirxel%20and%20Gregor%20Kobsik%20and%20Victor%20Czech%20and%20Yanjiang%20He%20and%20Isaak%20Lim%20and%20Leif%20Kobbelt%0AAbstract%3A%20%20%20In%20language%20processing%2C%20transformers%20benefit%20greatly%20from%20text%20being%0Acondensed.%20This%20is%20achieved%20through%20a%20larger%20vocabulary%20that%20captures%20word%0Afragments%20instead%20of%20plain%20characters.%20This%20is%20often%20done%20with%20Byte%20Pair%0AEncoding.%20In%20the%20context%20of%20images%2C%20tokenisation%20of%20visual%20data%20is%20usually%0Alimited%20to%20regular%20grids%20obtained%20from%20quantisation%20methods%2C%20without%20global%0Acontent%20awareness.%20Our%20work%20improves%20tokenisation%20of%20visual%20data%20by%20bringing%0AByte%20Pair%20Encoding%20from%201D%20to%20multiple%20dimensions%2C%20as%20a%20complementary%20add-on%20to%0Aexisting%20compression.%20We%20achieve%20this%20through%20counting%20constellations%20of%20token%0Apairs%20and%20replacing%20the%20most%20frequent%20token%20pair%20with%20a%20newly%20introduced%20token.%0AThe%20multidimensionality%20only%20increases%20the%20computation%20time%20by%20a%20factor%20of%202%0Afor%20images%2C%20making%20it%20applicable%20even%20to%20large%20datasets%20like%20ImageNet%20within%0Aminutes%20on%20consumer%20hardware.%20This%20is%20a%20lossless%20preprocessing%20step.%20Our%0Aevaluation%20shows%20improved%20training%20and%20inference%20performance%20of%20transformers%20on%0Avisual%20data%20achieved%20by%20compressing%20frequent%20constellations%20of%20tokens%3A%20The%0Aresulting%20sequences%20are%20shorter%2C%20with%20more%20uniformly%20distributed%20information%0Acontent%2C%20e.g.%20condensing%20empty%20regions%20in%20an%20image%20into%20single%20tokens.%20As%20our%0Aexperiments%20show%2C%20these%20condensed%20sequences%20are%20easier%20to%20process.%20We%0Aadditionally%20introduce%20a%20strategy%20to%20amplify%20this%20compression%20further%20by%0Aclustering%20the%20vocabulary.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10281v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultidimensional%2520Byte%2520Pair%2520Encoding%253A%2520Shortened%2520Sequences%2520for%2520Improved%250A%2520%2520Visual%2520Data%2520Generation%26entry.906535625%3DTim%2520Elsner%2520and%2520Paula%2520Usinger%2520and%2520Julius%2520Nehring-Wirxel%2520and%2520Gregor%2520Kobsik%2520and%2520Victor%2520Czech%2520and%2520Yanjiang%2520He%2520and%2520Isaak%2520Lim%2520and%2520Leif%2520Kobbelt%26entry.1292438233%3D%2520%2520In%2520language%2520processing%252C%2520transformers%2520benefit%2520greatly%2520from%2520text%2520being%250Acondensed.%2520This%2520is%2520achieved%2520through%2520a%2520larger%2520vocabulary%2520that%2520captures%2520word%250Afragments%2520instead%2520of%2520plain%2520characters.%2520This%2520is%2520often%2520done%2520with%2520Byte%2520Pair%250AEncoding.%2520In%2520the%2520context%2520of%2520images%252C%2520tokenisation%2520of%2520visual%2520data%2520is%2520usually%250Alimited%2520to%2520regular%2520grids%2520obtained%2520from%2520quantisation%2520methods%252C%2520without%2520global%250Acontent%2520awareness.%2520Our%2520work%2520improves%2520tokenisation%2520of%2520visual%2520data%2520by%2520bringing%250AByte%2520Pair%2520Encoding%2520from%25201D%2520to%2520multiple%2520dimensions%252C%2520as%2520a%2520complementary%2520add-on%2520to%250Aexisting%2520compression.%2520We%2520achieve%2520this%2520through%2520counting%2520constellations%2520of%2520token%250Apairs%2520and%2520replacing%2520the%2520most%2520frequent%2520token%2520pair%2520with%2520a%2520newly%2520introduced%2520token.%250AThe%2520multidimensionality%2520only%2520increases%2520the%2520computation%2520time%2520by%2520a%2520factor%2520of%25202%250Afor%2520images%252C%2520making%2520it%2520applicable%2520even%2520to%2520large%2520datasets%2520like%2520ImageNet%2520within%250Aminutes%2520on%2520consumer%2520hardware.%2520This%2520is%2520a%2520lossless%2520preprocessing%2520step.%2520Our%250Aevaluation%2520shows%2520improved%2520training%2520and%2520inference%2520performance%2520of%2520transformers%2520on%250Avisual%2520data%2520achieved%2520by%2520compressing%2520frequent%2520constellations%2520of%2520tokens%253A%2520The%250Aresulting%2520sequences%2520are%2520shorter%252C%2520with%2520more%2520uniformly%2520distributed%2520information%250Acontent%252C%2520e.g.%2520condensing%2520empty%2520regions%2520in%2520an%2520image%2520into%2520single%2520tokens.%2520As%2520our%250Aexperiments%2520show%252C%2520these%2520condensed%2520sequences%2520are%2520easier%2520to%2520process.%2520We%250Aadditionally%2520introduce%2520a%2520strategy%2520to%2520amplify%2520this%2520compression%2520further%2520by%250Aclustering%2520the%2520vocabulary.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10281v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multidimensional%20Byte%20Pair%20Encoding%3A%20Shortened%20Sequences%20for%20Improved%0A%20%20Visual%20Data%20Generation&entry.906535625=Tim%20Elsner%20and%20Paula%20Usinger%20and%20Julius%20Nehring-Wirxel%20and%20Gregor%20Kobsik%20and%20Victor%20Czech%20and%20Yanjiang%20He%20and%20Isaak%20Lim%20and%20Leif%20Kobbelt&entry.1292438233=%20%20In%20language%20processing%2C%20transformers%20benefit%20greatly%20from%20text%20being%0Acondensed.%20This%20is%20achieved%20through%20a%20larger%20vocabulary%20that%20captures%20word%0Afragments%20instead%20of%20plain%20characters.%20This%20is%20often%20done%20with%20Byte%20Pair%0AEncoding.%20In%20the%20context%20of%20images%2C%20tokenisation%20of%20visual%20data%20is%20usually%0Alimited%20to%20regular%20grids%20obtained%20from%20quantisation%20methods%2C%20without%20global%0Acontent%20awareness.%20Our%20work%20improves%20tokenisation%20of%20visual%20data%20by%20bringing%0AByte%20Pair%20Encoding%20from%201D%20to%20multiple%20dimensions%2C%20as%20a%20complementary%20add-on%20to%0Aexisting%20compression.%20We%20achieve%20this%20through%20counting%20constellations%20of%20token%0Apairs%20and%20replacing%20the%20most%20frequent%20token%20pair%20with%20a%20newly%20introduced%20token.%0AThe%20multidimensionality%20only%20increases%20the%20computation%20time%20by%20a%20factor%20of%202%0Afor%20images%2C%20making%20it%20applicable%20even%20to%20large%20datasets%20like%20ImageNet%20within%0Aminutes%20on%20consumer%20hardware.%20This%20is%20a%20lossless%20preprocessing%20step.%20Our%0Aevaluation%20shows%20improved%20training%20and%20inference%20performance%20of%20transformers%20on%0Avisual%20data%20achieved%20by%20compressing%20frequent%20constellations%20of%20tokens%3A%20The%0Aresulting%20sequences%20are%20shorter%2C%20with%20more%20uniformly%20distributed%20information%0Acontent%2C%20e.g.%20condensing%20empty%20regions%20in%20an%20image%20into%20single%20tokens.%20As%20our%0Aexperiments%20show%2C%20these%20condensed%20sequences%20are%20easier%20to%20process.%20We%0Aadditionally%20introduce%20a%20strategy%20to%20amplify%20this%20compression%20further%20by%0Aclustering%20the%20vocabulary.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10281v1&entry.124074799=Read"},
{"title": "M3TR: Generalist HD Map Construction with Variable Map Priors", "author": "Fabian Immel and Richard Fehler and Frank Bieder and Jan-Hendrik Pauls and Christoph Stiller", "abstract": "  Autonomous vehicles require road information for their operation, usually in\nform of HD maps. Since offline maps eventually become outdated or may only be\npartially available, online HD map construction methods have been proposed to\ninfer map information from live sensor data. A key issue remains how to exploit\nsuch partial or outdated map information as a prior. We introduce M3TR\n(Multi-Masking Map Transformer), a generalist approach for HD map construction\nboth with and without map priors. We address shortcomings in ground truth\ngeneration for Argoverse 2 and nuScenes and propose the first realistic\nscenarios with semantically diverse map priors. Examining various query\ndesigns, we use an improved method for integrating prior map elements into a HD\nmap construction model, increasing performance by +4.3 mAP. Finally, we show\nthat training across all prior scenarios yields a single Generalist model,\nwhose performance is on par with previous Expert models that can handle only\none specific type of map prior. M3TR thus is the first model capable of\nleveraging variable map priors, making it suitable for real-world deployment.\nCode is available at https://github.com/immel-f/m3tr\n", "link": "http://arxiv.org/abs/2411.10316v1", "date": "2024-11-15", "relevancy": 2.2061, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5614}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5446}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M3TR%3A%20Generalist%20HD%20Map%20Construction%20with%20Variable%20Map%20Priors&body=Title%3A%20M3TR%3A%20Generalist%20HD%20Map%20Construction%20with%20Variable%20Map%20Priors%0AAuthor%3A%20Fabian%20Immel%20and%20Richard%20Fehler%20and%20Frank%20Bieder%20and%20Jan-Hendrik%20Pauls%20and%20Christoph%20Stiller%0AAbstract%3A%20%20%20Autonomous%20vehicles%20require%20road%20information%20for%20their%20operation%2C%20usually%20in%0Aform%20of%20HD%20maps.%20Since%20offline%20maps%20eventually%20become%20outdated%20or%20may%20only%20be%0Apartially%20available%2C%20online%20HD%20map%20construction%20methods%20have%20been%20proposed%20to%0Ainfer%20map%20information%20from%20live%20sensor%20data.%20A%20key%20issue%20remains%20how%20to%20exploit%0Asuch%20partial%20or%20outdated%20map%20information%20as%20a%20prior.%20We%20introduce%20M3TR%0A%28Multi-Masking%20Map%20Transformer%29%2C%20a%20generalist%20approach%20for%20HD%20map%20construction%0Aboth%20with%20and%20without%20map%20priors.%20We%20address%20shortcomings%20in%20ground%20truth%0Ageneration%20for%20Argoverse%202%20and%20nuScenes%20and%20propose%20the%20first%20realistic%0Ascenarios%20with%20semantically%20diverse%20map%20priors.%20Examining%20various%20query%0Adesigns%2C%20we%20use%20an%20improved%20method%20for%20integrating%20prior%20map%20elements%20into%20a%20HD%0Amap%20construction%20model%2C%20increasing%20performance%20by%20%2B4.3%20mAP.%20Finally%2C%20we%20show%0Athat%20training%20across%20all%20prior%20scenarios%20yields%20a%20single%20Generalist%20model%2C%0Awhose%20performance%20is%20on%20par%20with%20previous%20Expert%20models%20that%20can%20handle%20only%0Aone%20specific%20type%20of%20map%20prior.%20M3TR%20thus%20is%20the%20first%20model%20capable%20of%0Aleveraging%20variable%20map%20priors%2C%20making%20it%20suitable%20for%20real-world%20deployment.%0ACode%20is%20available%20at%20https%3A//github.com/immel-f/m3tr%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10316v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM3TR%253A%2520Generalist%2520HD%2520Map%2520Construction%2520with%2520Variable%2520Map%2520Priors%26entry.906535625%3DFabian%2520Immel%2520and%2520Richard%2520Fehler%2520and%2520Frank%2520Bieder%2520and%2520Jan-Hendrik%2520Pauls%2520and%2520Christoph%2520Stiller%26entry.1292438233%3D%2520%2520Autonomous%2520vehicles%2520require%2520road%2520information%2520for%2520their%2520operation%252C%2520usually%2520in%250Aform%2520of%2520HD%2520maps.%2520Since%2520offline%2520maps%2520eventually%2520become%2520outdated%2520or%2520may%2520only%2520be%250Apartially%2520available%252C%2520online%2520HD%2520map%2520construction%2520methods%2520have%2520been%2520proposed%2520to%250Ainfer%2520map%2520information%2520from%2520live%2520sensor%2520data.%2520A%2520key%2520issue%2520remains%2520how%2520to%2520exploit%250Asuch%2520partial%2520or%2520outdated%2520map%2520information%2520as%2520a%2520prior.%2520We%2520introduce%2520M3TR%250A%2528Multi-Masking%2520Map%2520Transformer%2529%252C%2520a%2520generalist%2520approach%2520for%2520HD%2520map%2520construction%250Aboth%2520with%2520and%2520without%2520map%2520priors.%2520We%2520address%2520shortcomings%2520in%2520ground%2520truth%250Ageneration%2520for%2520Argoverse%25202%2520and%2520nuScenes%2520and%2520propose%2520the%2520first%2520realistic%250Ascenarios%2520with%2520semantically%2520diverse%2520map%2520priors.%2520Examining%2520various%2520query%250Adesigns%252C%2520we%2520use%2520an%2520improved%2520method%2520for%2520integrating%2520prior%2520map%2520elements%2520into%2520a%2520HD%250Amap%2520construction%2520model%252C%2520increasing%2520performance%2520by%2520%252B4.3%2520mAP.%2520Finally%252C%2520we%2520show%250Athat%2520training%2520across%2520all%2520prior%2520scenarios%2520yields%2520a%2520single%2520Generalist%2520model%252C%250Awhose%2520performance%2520is%2520on%2520par%2520with%2520previous%2520Expert%2520models%2520that%2520can%2520handle%2520only%250Aone%2520specific%2520type%2520of%2520map%2520prior.%2520M3TR%2520thus%2520is%2520the%2520first%2520model%2520capable%2520of%250Aleveraging%2520variable%2520map%2520priors%252C%2520making%2520it%2520suitable%2520for%2520real-world%2520deployment.%250ACode%2520is%2520available%2520at%2520https%253A//github.com/immel-f/m3tr%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10316v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M3TR%3A%20Generalist%20HD%20Map%20Construction%20with%20Variable%20Map%20Priors&entry.906535625=Fabian%20Immel%20and%20Richard%20Fehler%20and%20Frank%20Bieder%20and%20Jan-Hendrik%20Pauls%20and%20Christoph%20Stiller&entry.1292438233=%20%20Autonomous%20vehicles%20require%20road%20information%20for%20their%20operation%2C%20usually%20in%0Aform%20of%20HD%20maps.%20Since%20offline%20maps%20eventually%20become%20outdated%20or%20may%20only%20be%0Apartially%20available%2C%20online%20HD%20map%20construction%20methods%20have%20been%20proposed%20to%0Ainfer%20map%20information%20from%20live%20sensor%20data.%20A%20key%20issue%20remains%20how%20to%20exploit%0Asuch%20partial%20or%20outdated%20map%20information%20as%20a%20prior.%20We%20introduce%20M3TR%0A%28Multi-Masking%20Map%20Transformer%29%2C%20a%20generalist%20approach%20for%20HD%20map%20construction%0Aboth%20with%20and%20without%20map%20priors.%20We%20address%20shortcomings%20in%20ground%20truth%0Ageneration%20for%20Argoverse%202%20and%20nuScenes%20and%20propose%20the%20first%20realistic%0Ascenarios%20with%20semantically%20diverse%20map%20priors.%20Examining%20various%20query%0Adesigns%2C%20we%20use%20an%20improved%20method%20for%20integrating%20prior%20map%20elements%20into%20a%20HD%0Amap%20construction%20model%2C%20increasing%20performance%20by%20%2B4.3%20mAP.%20Finally%2C%20we%20show%0Athat%20training%20across%20all%20prior%20scenarios%20yields%20a%20single%20Generalist%20model%2C%0Awhose%20performance%20is%20on%20par%20with%20previous%20Expert%20models%20that%20can%20handle%20only%0Aone%20specific%20type%20of%20map%20prior.%20M3TR%20thus%20is%20the%20first%20model%20capable%20of%0Aleveraging%20variable%20map%20priors%2C%20making%20it%20suitable%20for%20real-world%20deployment.%0ACode%20is%20available%20at%20https%3A//github.com/immel-f/m3tr%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10316v1&entry.124074799=Read"},
{"title": "Label Cluster Chains for Multi-Label Classification", "author": "Elaine Cec\u00edlia Gatto and Felipe Nakano Kenji and Jesse Read and Mauri Ferrandin and Ricardo Cerri and Celine Vens", "abstract": "  Multi-label classification is a type of supervised machine learning that can\nsimultaneously assign multiple labels to an instance. To solve this task, some\nmethods divide the original problem into several sub-problems (local approach),\nothers learn all labels at once (global approach), and others combine several\nclassifiers (ensemble approach). Regardless of the approach used, exploring and\nlearning label correlations is important to improve the classifier predictions.\nEnsemble of Classifier Chains (ECC) is a well-known multi-label method that\nconsiders label correlations and can achieve good overall performance on\nseveral multi-label datasets and evaluation measures. However, one of the\nchallenges when working with ECC is the high dimensionality of the label space,\nwhich can impose limitations for fully-cascaded chains as the complexity\nincreases regarding feature space expansion. To improve classifier chains, we\npropose a method to chain disjoint correlated label clusters obtained by\napplying a partition method in the label space. During the training phase, the\nground truth labels of each cluster are used as new features for all of the\nfollowing clusters. During the test phase, the predicted labels of clusters are\nused as new features for all the following clusters. Our proposal, called Label\nCluster Chains for Multi-Label Classification (LCC-ML), uses multi-label Random\nForests as base classifiers in each cluster, combining their predictions to\nobtain a final multi-label classification. Our proposal obtained better results\ncompared to the original ECC. This shows that learning and chaining disjoint\ncorrelated label clusters can better explore and learn label correlations.\n", "link": "http://arxiv.org/abs/2411.00514v2", "date": "2024-11-15", "relevancy": 2.1952, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4451}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4388}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Label%20Cluster%20Chains%20for%20Multi-Label%20Classification&body=Title%3A%20Label%20Cluster%20Chains%20for%20Multi-Label%20Classification%0AAuthor%3A%20Elaine%20Cec%C3%ADlia%20Gatto%20and%20Felipe%20Nakano%20Kenji%20and%20Jesse%20Read%20and%20Mauri%20Ferrandin%20and%20Ricardo%20Cerri%20and%20Celine%20Vens%0AAbstract%3A%20%20%20Multi-label%20classification%20is%20a%20type%20of%20supervised%20machine%20learning%20that%20can%0Asimultaneously%20assign%20multiple%20labels%20to%20an%20instance.%20To%20solve%20this%20task%2C%20some%0Amethods%20divide%20the%20original%20problem%20into%20several%20sub-problems%20%28local%20approach%29%2C%0Aothers%20learn%20all%20labels%20at%20once%20%28global%20approach%29%2C%20and%20others%20combine%20several%0Aclassifiers%20%28ensemble%20approach%29.%20Regardless%20of%20the%20approach%20used%2C%20exploring%20and%0Alearning%20label%20correlations%20is%20important%20to%20improve%20the%20classifier%20predictions.%0AEnsemble%20of%20Classifier%20Chains%20%28ECC%29%20is%20a%20well-known%20multi-label%20method%20that%0Aconsiders%20label%20correlations%20and%20can%20achieve%20good%20overall%20performance%20on%0Aseveral%20multi-label%20datasets%20and%20evaluation%20measures.%20However%2C%20one%20of%20the%0Achallenges%20when%20working%20with%20ECC%20is%20the%20high%20dimensionality%20of%20the%20label%20space%2C%0Awhich%20can%20impose%20limitations%20for%20fully-cascaded%20chains%20as%20the%20complexity%0Aincreases%20regarding%20feature%20space%20expansion.%20To%20improve%20classifier%20chains%2C%20we%0Apropose%20a%20method%20to%20chain%20disjoint%20correlated%20label%20clusters%20obtained%20by%0Aapplying%20a%20partition%20method%20in%20the%20label%20space.%20During%20the%20training%20phase%2C%20the%0Aground%20truth%20labels%20of%20each%20cluster%20are%20used%20as%20new%20features%20for%20all%20of%20the%0Afollowing%20clusters.%20During%20the%20test%20phase%2C%20the%20predicted%20labels%20of%20clusters%20are%0Aused%20as%20new%20features%20for%20all%20the%20following%20clusters.%20Our%20proposal%2C%20called%20Label%0ACluster%20Chains%20for%20Multi-Label%20Classification%20%28LCC-ML%29%2C%20uses%20multi-label%20Random%0AForests%20as%20base%20classifiers%20in%20each%20cluster%2C%20combining%20their%20predictions%20to%0Aobtain%20a%20final%20multi-label%20classification.%20Our%20proposal%20obtained%20better%20results%0Acompared%20to%20the%20original%20ECC.%20This%20shows%20that%20learning%20and%20chaining%20disjoint%0Acorrelated%20label%20clusters%20can%20better%20explore%20and%20learn%20label%20correlations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.00514v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLabel%2520Cluster%2520Chains%2520for%2520Multi-Label%2520Classification%26entry.906535625%3DElaine%2520Cec%25C3%25ADlia%2520Gatto%2520and%2520Felipe%2520Nakano%2520Kenji%2520and%2520Jesse%2520Read%2520and%2520Mauri%2520Ferrandin%2520and%2520Ricardo%2520Cerri%2520and%2520Celine%2520Vens%26entry.1292438233%3D%2520%2520Multi-label%2520classification%2520is%2520a%2520type%2520of%2520supervised%2520machine%2520learning%2520that%2520can%250Asimultaneously%2520assign%2520multiple%2520labels%2520to%2520an%2520instance.%2520To%2520solve%2520this%2520task%252C%2520some%250Amethods%2520divide%2520the%2520original%2520problem%2520into%2520several%2520sub-problems%2520%2528local%2520approach%2529%252C%250Aothers%2520learn%2520all%2520labels%2520at%2520once%2520%2528global%2520approach%2529%252C%2520and%2520others%2520combine%2520several%250Aclassifiers%2520%2528ensemble%2520approach%2529.%2520Regardless%2520of%2520the%2520approach%2520used%252C%2520exploring%2520and%250Alearning%2520label%2520correlations%2520is%2520important%2520to%2520improve%2520the%2520classifier%2520predictions.%250AEnsemble%2520of%2520Classifier%2520Chains%2520%2528ECC%2529%2520is%2520a%2520well-known%2520multi-label%2520method%2520that%250Aconsiders%2520label%2520correlations%2520and%2520can%2520achieve%2520good%2520overall%2520performance%2520on%250Aseveral%2520multi-label%2520datasets%2520and%2520evaluation%2520measures.%2520However%252C%2520one%2520of%2520the%250Achallenges%2520when%2520working%2520with%2520ECC%2520is%2520the%2520high%2520dimensionality%2520of%2520the%2520label%2520space%252C%250Awhich%2520can%2520impose%2520limitations%2520for%2520fully-cascaded%2520chains%2520as%2520the%2520complexity%250Aincreases%2520regarding%2520feature%2520space%2520expansion.%2520To%2520improve%2520classifier%2520chains%252C%2520we%250Apropose%2520a%2520method%2520to%2520chain%2520disjoint%2520correlated%2520label%2520clusters%2520obtained%2520by%250Aapplying%2520a%2520partition%2520method%2520in%2520the%2520label%2520space.%2520During%2520the%2520training%2520phase%252C%2520the%250Aground%2520truth%2520labels%2520of%2520each%2520cluster%2520are%2520used%2520as%2520new%2520features%2520for%2520all%2520of%2520the%250Afollowing%2520clusters.%2520During%2520the%2520test%2520phase%252C%2520the%2520predicted%2520labels%2520of%2520clusters%2520are%250Aused%2520as%2520new%2520features%2520for%2520all%2520the%2520following%2520clusters.%2520Our%2520proposal%252C%2520called%2520Label%250ACluster%2520Chains%2520for%2520Multi-Label%2520Classification%2520%2528LCC-ML%2529%252C%2520uses%2520multi-label%2520Random%250AForests%2520as%2520base%2520classifiers%2520in%2520each%2520cluster%252C%2520combining%2520their%2520predictions%2520to%250Aobtain%2520a%2520final%2520multi-label%2520classification.%2520Our%2520proposal%2520obtained%2520better%2520results%250Acompared%2520to%2520the%2520original%2520ECC.%2520This%2520shows%2520that%2520learning%2520and%2520chaining%2520disjoint%250Acorrelated%2520label%2520clusters%2520can%2520better%2520explore%2520and%2520learn%2520label%2520correlations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.00514v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Label%20Cluster%20Chains%20for%20Multi-Label%20Classification&entry.906535625=Elaine%20Cec%C3%ADlia%20Gatto%20and%20Felipe%20Nakano%20Kenji%20and%20Jesse%20Read%20and%20Mauri%20Ferrandin%20and%20Ricardo%20Cerri%20and%20Celine%20Vens&entry.1292438233=%20%20Multi-label%20classification%20is%20a%20type%20of%20supervised%20machine%20learning%20that%20can%0Asimultaneously%20assign%20multiple%20labels%20to%20an%20instance.%20To%20solve%20this%20task%2C%20some%0Amethods%20divide%20the%20original%20problem%20into%20several%20sub-problems%20%28local%20approach%29%2C%0Aothers%20learn%20all%20labels%20at%20once%20%28global%20approach%29%2C%20and%20others%20combine%20several%0Aclassifiers%20%28ensemble%20approach%29.%20Regardless%20of%20the%20approach%20used%2C%20exploring%20and%0Alearning%20label%20correlations%20is%20important%20to%20improve%20the%20classifier%20predictions.%0AEnsemble%20of%20Classifier%20Chains%20%28ECC%29%20is%20a%20well-known%20multi-label%20method%20that%0Aconsiders%20label%20correlations%20and%20can%20achieve%20good%20overall%20performance%20on%0Aseveral%20multi-label%20datasets%20and%20evaluation%20measures.%20However%2C%20one%20of%20the%0Achallenges%20when%20working%20with%20ECC%20is%20the%20high%20dimensionality%20of%20the%20label%20space%2C%0Awhich%20can%20impose%20limitations%20for%20fully-cascaded%20chains%20as%20the%20complexity%0Aincreases%20regarding%20feature%20space%20expansion.%20To%20improve%20classifier%20chains%2C%20we%0Apropose%20a%20method%20to%20chain%20disjoint%20correlated%20label%20clusters%20obtained%20by%0Aapplying%20a%20partition%20method%20in%20the%20label%20space.%20During%20the%20training%20phase%2C%20the%0Aground%20truth%20labels%20of%20each%20cluster%20are%20used%20as%20new%20features%20for%20all%20of%20the%0Afollowing%20clusters.%20During%20the%20test%20phase%2C%20the%20predicted%20labels%20of%20clusters%20are%0Aused%20as%20new%20features%20for%20all%20the%20following%20clusters.%20Our%20proposal%2C%20called%20Label%0ACluster%20Chains%20for%20Multi-Label%20Classification%20%28LCC-ML%29%2C%20uses%20multi-label%20Random%0AForests%20as%20base%20classifiers%20in%20each%20cluster%2C%20combining%20their%20predictions%20to%0Aobtain%20a%20final%20multi-label%20classification.%20Our%20proposal%20obtained%20better%20results%0Acompared%20to%20the%20original%20ECC.%20This%20shows%20that%20learning%20and%20chaining%20disjoint%0Acorrelated%20label%20clusters%20can%20better%20explore%20and%20learn%20label%20correlations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.00514v2&entry.124074799=Read"},
{"title": "Enhancing the Reasoning Ability of Multimodal Large Language Models via\n  Mixed Preference Optimization", "author": "Weiyun Wang and Zhe Chen and Wenhai Wang and Yue Cao and Yangzhou Liu and Zhangwei Gao and Jinguo Zhu and Xizhou Zhu and Lewei Lu and Yu Qiao and Jifeng Dai", "abstract": "  Existing open-source multimodal large language models (MLLMs) generally\nfollow a training process involving pre-training and supervised fine-tuning.\nHowever, these models suffer from distribution shifts, which limit their\nmultimodal reasoning, particularly in the Chain-of-Thought (CoT) performance.\nTo address this, we introduce a preference optimization (PO) process to enhance\nthe multimodal reasoning capabilities of MLLMs. Specifically, (1) on the data\nside, we design an automated preference data construction pipeline to create\nMMPR, a high-quality, large-scale multimodal reasoning preference dataset. and\n(2) on the model side, we explore integrating PO with MLLMs, developing a\nsimple yet effective method, termed Mixed Preference Optimization (MPO), which\nboosts multimodal CoT performance. Our approach demonstrates improved\nperformance across multiple benchmarks, particularly in multimodal reasoning\ntasks. Notably, our model, InternVL2-8B-MPO, achieves an accuracy of 67.0 on\nMathVista, outperforming InternVL2-8B by 8.7 points and achieving performance\ncomparable to the 10x larger InternVL2-76B. We hope this study could inspire\nfurther advancements in MLLMs. Code, data, and model shall be publicly\nreleased.\n", "link": "http://arxiv.org/abs/2411.10442v1", "date": "2024-11-15", "relevancy": 2.1739, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5521}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.548}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20the%20Reasoning%20Ability%20of%20Multimodal%20Large%20Language%20Models%20via%0A%20%20Mixed%20Preference%20Optimization&body=Title%3A%20Enhancing%20the%20Reasoning%20Ability%20of%20Multimodal%20Large%20Language%20Models%20via%0A%20%20Mixed%20Preference%20Optimization%0AAuthor%3A%20Weiyun%20Wang%20and%20Zhe%20Chen%20and%20Wenhai%20Wang%20and%20Yue%20Cao%20and%20Yangzhou%20Liu%20and%20Zhangwei%20Gao%20and%20Jinguo%20Zhu%20and%20Xizhou%20Zhu%20and%20Lewei%20Lu%20and%20Yu%20Qiao%20and%20Jifeng%20Dai%0AAbstract%3A%20%20%20Existing%20open-source%20multimodal%20large%20language%20models%20%28MLLMs%29%20generally%0Afollow%20a%20training%20process%20involving%20pre-training%20and%20supervised%20fine-tuning.%0AHowever%2C%20these%20models%20suffer%20from%20distribution%20shifts%2C%20which%20limit%20their%0Amultimodal%20reasoning%2C%20particularly%20in%20the%20Chain-of-Thought%20%28CoT%29%20performance.%0ATo%20address%20this%2C%20we%20introduce%20a%20preference%20optimization%20%28PO%29%20process%20to%20enhance%0Athe%20multimodal%20reasoning%20capabilities%20of%20MLLMs.%20Specifically%2C%20%281%29%20on%20the%20data%0Aside%2C%20we%20design%20an%20automated%20preference%20data%20construction%20pipeline%20to%20create%0AMMPR%2C%20a%20high-quality%2C%20large-scale%20multimodal%20reasoning%20preference%20dataset.%20and%0A%282%29%20on%20the%20model%20side%2C%20we%20explore%20integrating%20PO%20with%20MLLMs%2C%20developing%20a%0Asimple%20yet%20effective%20method%2C%20termed%20Mixed%20Preference%20Optimization%20%28MPO%29%2C%20which%0Aboosts%20multimodal%20CoT%20performance.%20Our%20approach%20demonstrates%20improved%0Aperformance%20across%20multiple%20benchmarks%2C%20particularly%20in%20multimodal%20reasoning%0Atasks.%20Notably%2C%20our%20model%2C%20InternVL2-8B-MPO%2C%20achieves%20an%20accuracy%20of%2067.0%20on%0AMathVista%2C%20outperforming%20InternVL2-8B%20by%208.7%20points%20and%20achieving%20performance%0Acomparable%20to%20the%2010x%20larger%20InternVL2-76B.%20We%20hope%20this%20study%20could%20inspire%0Afurther%20advancements%20in%20MLLMs.%20Code%2C%20data%2C%20and%20model%20shall%20be%20publicly%0Areleased.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10442v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520the%2520Reasoning%2520Ability%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520via%250A%2520%2520Mixed%2520Preference%2520Optimization%26entry.906535625%3DWeiyun%2520Wang%2520and%2520Zhe%2520Chen%2520and%2520Wenhai%2520Wang%2520and%2520Yue%2520Cao%2520and%2520Yangzhou%2520Liu%2520and%2520Zhangwei%2520Gao%2520and%2520Jinguo%2520Zhu%2520and%2520Xizhou%2520Zhu%2520and%2520Lewei%2520Lu%2520and%2520Yu%2520Qiao%2520and%2520Jifeng%2520Dai%26entry.1292438233%3D%2520%2520Existing%2520open-source%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520generally%250Afollow%2520a%2520training%2520process%2520involving%2520pre-training%2520and%2520supervised%2520fine-tuning.%250AHowever%252C%2520these%2520models%2520suffer%2520from%2520distribution%2520shifts%252C%2520which%2520limit%2520their%250Amultimodal%2520reasoning%252C%2520particularly%2520in%2520the%2520Chain-of-Thought%2520%2528CoT%2529%2520performance.%250ATo%2520address%2520this%252C%2520we%2520introduce%2520a%2520preference%2520optimization%2520%2528PO%2529%2520process%2520to%2520enhance%250Athe%2520multimodal%2520reasoning%2520capabilities%2520of%2520MLLMs.%2520Specifically%252C%2520%25281%2529%2520on%2520the%2520data%250Aside%252C%2520we%2520design%2520an%2520automated%2520preference%2520data%2520construction%2520pipeline%2520to%2520create%250AMMPR%252C%2520a%2520high-quality%252C%2520large-scale%2520multimodal%2520reasoning%2520preference%2520dataset.%2520and%250A%25282%2529%2520on%2520the%2520model%2520side%252C%2520we%2520explore%2520integrating%2520PO%2520with%2520MLLMs%252C%2520developing%2520a%250Asimple%2520yet%2520effective%2520method%252C%2520termed%2520Mixed%2520Preference%2520Optimization%2520%2528MPO%2529%252C%2520which%250Aboosts%2520multimodal%2520CoT%2520performance.%2520Our%2520approach%2520demonstrates%2520improved%250Aperformance%2520across%2520multiple%2520benchmarks%252C%2520particularly%2520in%2520multimodal%2520reasoning%250Atasks.%2520Notably%252C%2520our%2520model%252C%2520InternVL2-8B-MPO%252C%2520achieves%2520an%2520accuracy%2520of%252067.0%2520on%250AMathVista%252C%2520outperforming%2520InternVL2-8B%2520by%25208.7%2520points%2520and%2520achieving%2520performance%250Acomparable%2520to%2520the%252010x%2520larger%2520InternVL2-76B.%2520We%2520hope%2520this%2520study%2520could%2520inspire%250Afurther%2520advancements%2520in%2520MLLMs.%2520Code%252C%2520data%252C%2520and%2520model%2520shall%2520be%2520publicly%250Areleased.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10442v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20the%20Reasoning%20Ability%20of%20Multimodal%20Large%20Language%20Models%20via%0A%20%20Mixed%20Preference%20Optimization&entry.906535625=Weiyun%20Wang%20and%20Zhe%20Chen%20and%20Wenhai%20Wang%20and%20Yue%20Cao%20and%20Yangzhou%20Liu%20and%20Zhangwei%20Gao%20and%20Jinguo%20Zhu%20and%20Xizhou%20Zhu%20and%20Lewei%20Lu%20and%20Yu%20Qiao%20and%20Jifeng%20Dai&entry.1292438233=%20%20Existing%20open-source%20multimodal%20large%20language%20models%20%28MLLMs%29%20generally%0Afollow%20a%20training%20process%20involving%20pre-training%20and%20supervised%20fine-tuning.%0AHowever%2C%20these%20models%20suffer%20from%20distribution%20shifts%2C%20which%20limit%20their%0Amultimodal%20reasoning%2C%20particularly%20in%20the%20Chain-of-Thought%20%28CoT%29%20performance.%0ATo%20address%20this%2C%20we%20introduce%20a%20preference%20optimization%20%28PO%29%20process%20to%20enhance%0Athe%20multimodal%20reasoning%20capabilities%20of%20MLLMs.%20Specifically%2C%20%281%29%20on%20the%20data%0Aside%2C%20we%20design%20an%20automated%20preference%20data%20construction%20pipeline%20to%20create%0AMMPR%2C%20a%20high-quality%2C%20large-scale%20multimodal%20reasoning%20preference%20dataset.%20and%0A%282%29%20on%20the%20model%20side%2C%20we%20explore%20integrating%20PO%20with%20MLLMs%2C%20developing%20a%0Asimple%20yet%20effective%20method%2C%20termed%20Mixed%20Preference%20Optimization%20%28MPO%29%2C%20which%0Aboosts%20multimodal%20CoT%20performance.%20Our%20approach%20demonstrates%20improved%0Aperformance%20across%20multiple%20benchmarks%2C%20particularly%20in%20multimodal%20reasoning%0Atasks.%20Notably%2C%20our%20model%2C%20InternVL2-8B-MPO%2C%20achieves%20an%20accuracy%20of%2067.0%20on%0AMathVista%2C%20outperforming%20InternVL2-8B%20by%208.7%20points%20and%20achieving%20performance%0Acomparable%20to%20the%2010x%20larger%20InternVL2-76B.%20We%20hope%20this%20study%20could%20inspire%0Afurther%20advancements%20in%20MLLMs.%20Code%2C%20data%2C%20and%20model%20shall%20be%20publicly%0Areleased.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10442v1&entry.124074799=Read"},
{"title": "Forming Auxiliary High-confident Instance-level Loss to Promote Learning\n  from Label Proportions", "author": "Tianhao Ma and Han Chen and Juncheng Hu and Yungang Zhu and Ximing Li", "abstract": "  Learning from label proportions (LLP), i.e., a challenging weakly-supervised\nlearning task, aims to train a classifier by using bags of instances and the\nproportions of classes within bags, rather than annotated labels for each\ninstance. Beyond the traditional bag-level loss, the mainstream methodology of\nLLP is to incorporate an auxiliary instance-level loss with pseudo-labels\nformed by predictions. Unfortunately, we empirically observed that the\npseudo-labels are are often inaccurate due to over-smoothing, especially for\nthe scenarios with large bag sizes, hurting the classifier induction. To\nalleviate this problem, we suggest a novel LLP method, namely Learning from\nLabel Proportions with Auxiliary High-confident Instance-level Loss\n(L^2P-AHIL). Specifically, we propose a dual entropy-based weight (DEW) method\nto adaptively measure the confidences of pseudo-labels. It simultaneously\nemphasizes accurate predictions at the bag level and avoids overly smoothed\npredictions. We then form high-confident instance-level loss with DEW, and\njointly optimize it with the bag-level loss in a self-training manner. The\nexperimental results on benchmark datasets show that L^2P-AHIL can surpass the\nexisting baseline methods, and the performance gain can be more significant as\nthe bag size increases.\n", "link": "http://arxiv.org/abs/2411.10364v1", "date": "2024-11-15", "relevancy": 2.162, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5698}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5391}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5118}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forming%20Auxiliary%20High-confident%20Instance-level%20Loss%20to%20Promote%20Learning%0A%20%20from%20Label%20Proportions&body=Title%3A%20Forming%20Auxiliary%20High-confident%20Instance-level%20Loss%20to%20Promote%20Learning%0A%20%20from%20Label%20Proportions%0AAuthor%3A%20Tianhao%20Ma%20and%20Han%20Chen%20and%20Juncheng%20Hu%20and%20Yungang%20Zhu%20and%20Ximing%20Li%0AAbstract%3A%20%20%20Learning%20from%20label%20proportions%20%28LLP%29%2C%20i.e.%2C%20a%20challenging%20weakly-supervised%0Alearning%20task%2C%20aims%20to%20train%20a%20classifier%20by%20using%20bags%20of%20instances%20and%20the%0Aproportions%20of%20classes%20within%20bags%2C%20rather%20than%20annotated%20labels%20for%20each%0Ainstance.%20Beyond%20the%20traditional%20bag-level%20loss%2C%20the%20mainstream%20methodology%20of%0ALLP%20is%20to%20incorporate%20an%20auxiliary%20instance-level%20loss%20with%20pseudo-labels%0Aformed%20by%20predictions.%20Unfortunately%2C%20we%20empirically%20observed%20that%20the%0Apseudo-labels%20are%20are%20often%20inaccurate%20due%20to%20over-smoothing%2C%20especially%20for%0Athe%20scenarios%20with%20large%20bag%20sizes%2C%20hurting%20the%20classifier%20induction.%20To%0Aalleviate%20this%20problem%2C%20we%20suggest%20a%20novel%20LLP%20method%2C%20namely%20Learning%20from%0ALabel%20Proportions%20with%20Auxiliary%20High-confident%20Instance-level%20Loss%0A%28L%5E2P-AHIL%29.%20Specifically%2C%20we%20propose%20a%20dual%20entropy-based%20weight%20%28DEW%29%20method%0Ato%20adaptively%20measure%20the%20confidences%20of%20pseudo-labels.%20It%20simultaneously%0Aemphasizes%20accurate%20predictions%20at%20the%20bag%20level%20and%20avoids%20overly%20smoothed%0Apredictions.%20We%20then%20form%20high-confident%20instance-level%20loss%20with%20DEW%2C%20and%0Ajointly%20optimize%20it%20with%20the%20bag-level%20loss%20in%20a%20self-training%20manner.%20The%0Aexperimental%20results%20on%20benchmark%20datasets%20show%20that%20L%5E2P-AHIL%20can%20surpass%20the%0Aexisting%20baseline%20methods%2C%20and%20the%20performance%20gain%20can%20be%20more%20significant%20as%0Athe%20bag%20size%20increases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10364v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForming%2520Auxiliary%2520High-confident%2520Instance-level%2520Loss%2520to%2520Promote%2520Learning%250A%2520%2520from%2520Label%2520Proportions%26entry.906535625%3DTianhao%2520Ma%2520and%2520Han%2520Chen%2520and%2520Juncheng%2520Hu%2520and%2520Yungang%2520Zhu%2520and%2520Ximing%2520Li%26entry.1292438233%3D%2520%2520Learning%2520from%2520label%2520proportions%2520%2528LLP%2529%252C%2520i.e.%252C%2520a%2520challenging%2520weakly-supervised%250Alearning%2520task%252C%2520aims%2520to%2520train%2520a%2520classifier%2520by%2520using%2520bags%2520of%2520instances%2520and%2520the%250Aproportions%2520of%2520classes%2520within%2520bags%252C%2520rather%2520than%2520annotated%2520labels%2520for%2520each%250Ainstance.%2520Beyond%2520the%2520traditional%2520bag-level%2520loss%252C%2520the%2520mainstream%2520methodology%2520of%250ALLP%2520is%2520to%2520incorporate%2520an%2520auxiliary%2520instance-level%2520loss%2520with%2520pseudo-labels%250Aformed%2520by%2520predictions.%2520Unfortunately%252C%2520we%2520empirically%2520observed%2520that%2520the%250Apseudo-labels%2520are%2520are%2520often%2520inaccurate%2520due%2520to%2520over-smoothing%252C%2520especially%2520for%250Athe%2520scenarios%2520with%2520large%2520bag%2520sizes%252C%2520hurting%2520the%2520classifier%2520induction.%2520To%250Aalleviate%2520this%2520problem%252C%2520we%2520suggest%2520a%2520novel%2520LLP%2520method%252C%2520namely%2520Learning%2520from%250ALabel%2520Proportions%2520with%2520Auxiliary%2520High-confident%2520Instance-level%2520Loss%250A%2528L%255E2P-AHIL%2529.%2520Specifically%252C%2520we%2520propose%2520a%2520dual%2520entropy-based%2520weight%2520%2528DEW%2529%2520method%250Ato%2520adaptively%2520measure%2520the%2520confidences%2520of%2520pseudo-labels.%2520It%2520simultaneously%250Aemphasizes%2520accurate%2520predictions%2520at%2520the%2520bag%2520level%2520and%2520avoids%2520overly%2520smoothed%250Apredictions.%2520We%2520then%2520form%2520high-confident%2520instance-level%2520loss%2520with%2520DEW%252C%2520and%250Ajointly%2520optimize%2520it%2520with%2520the%2520bag-level%2520loss%2520in%2520a%2520self-training%2520manner.%2520The%250Aexperimental%2520results%2520on%2520benchmark%2520datasets%2520show%2520that%2520L%255E2P-AHIL%2520can%2520surpass%2520the%250Aexisting%2520baseline%2520methods%252C%2520and%2520the%2520performance%2520gain%2520can%2520be%2520more%2520significant%2520as%250Athe%2520bag%2520size%2520increases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10364v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forming%20Auxiliary%20High-confident%20Instance-level%20Loss%20to%20Promote%20Learning%0A%20%20from%20Label%20Proportions&entry.906535625=Tianhao%20Ma%20and%20Han%20Chen%20and%20Juncheng%20Hu%20and%20Yungang%20Zhu%20and%20Ximing%20Li&entry.1292438233=%20%20Learning%20from%20label%20proportions%20%28LLP%29%2C%20i.e.%2C%20a%20challenging%20weakly-supervised%0Alearning%20task%2C%20aims%20to%20train%20a%20classifier%20by%20using%20bags%20of%20instances%20and%20the%0Aproportions%20of%20classes%20within%20bags%2C%20rather%20than%20annotated%20labels%20for%20each%0Ainstance.%20Beyond%20the%20traditional%20bag-level%20loss%2C%20the%20mainstream%20methodology%20of%0ALLP%20is%20to%20incorporate%20an%20auxiliary%20instance-level%20loss%20with%20pseudo-labels%0Aformed%20by%20predictions.%20Unfortunately%2C%20we%20empirically%20observed%20that%20the%0Apseudo-labels%20are%20are%20often%20inaccurate%20due%20to%20over-smoothing%2C%20especially%20for%0Athe%20scenarios%20with%20large%20bag%20sizes%2C%20hurting%20the%20classifier%20induction.%20To%0Aalleviate%20this%20problem%2C%20we%20suggest%20a%20novel%20LLP%20method%2C%20namely%20Learning%20from%0ALabel%20Proportions%20with%20Auxiliary%20High-confident%20Instance-level%20Loss%0A%28L%5E2P-AHIL%29.%20Specifically%2C%20we%20propose%20a%20dual%20entropy-based%20weight%20%28DEW%29%20method%0Ato%20adaptively%20measure%20the%20confidences%20of%20pseudo-labels.%20It%20simultaneously%0Aemphasizes%20accurate%20predictions%20at%20the%20bag%20level%20and%20avoids%20overly%20smoothed%0Apredictions.%20We%20then%20form%20high-confident%20instance-level%20loss%20with%20DEW%2C%20and%0Ajointly%20optimize%20it%20with%20the%20bag-level%20loss%20in%20a%20self-training%20manner.%20The%0Aexperimental%20results%20on%20benchmark%20datasets%20show%20that%20L%5E2P-AHIL%20can%20surpass%20the%0Aexisting%20baseline%20methods%2C%20and%20the%20performance%20gain%20can%20be%20more%20significant%20as%0Athe%20bag%20size%20increases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10364v1&entry.124074799=Read"},
{"title": "Generative AI in Multimodal User Interfaces: Trends, Challenges, and\n  Cross-Platform Adaptability", "author": "J. Bieniek and M. Rahouti and D. C. Verma", "abstract": "  As the boundaries of human computer interaction expand, Generative AI emerges\nas a key driver in reshaping user interfaces, introducing new possibilities for\npersonalized, multimodal and cross-platform interactions. This integration\nreflects a growing demand for more adaptive and intuitive user interfaces that\ncan accommodate diverse input types such as text, voice and video, and deliver\nseamless experiences across devices. This paper explores the integration of\ngenerative AI in modern user interfaces, examining historical developments and\nfocusing on multimodal interaction, cross-platform adaptability and dynamic\npersonalization. A central theme is the interface dilemma, which addresses the\nchallenge of designing effective interactions for multimodal large language\nmodels, assessing the trade-offs between graphical, voice-based and immersive\ninterfaces. The paper further evaluates lightweight frameworks tailored for\nmobile platforms, spotlighting the role of mobile hardware in enabling scalable\nmultimodal AI. Technical and ethical challenges, including context retention,\nprivacy concerns and balancing cloud and on-device processing are thoroughly\nexamined. Finally, the paper outlines future directions such as emotionally\nadaptive interfaces, predictive AI driven user interfaces and real-time\ncollaborative systems, underscoring generative AI's potential to redefine\nadaptive user-centric interfaces across platforms.\n", "link": "http://arxiv.org/abs/2411.10234v1", "date": "2024-11-15", "relevancy": 2.161, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5645}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5269}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%20in%20Multimodal%20User%20Interfaces%3A%20Trends%2C%20Challenges%2C%20and%0A%20%20Cross-Platform%20Adaptability&body=Title%3A%20Generative%20AI%20in%20Multimodal%20User%20Interfaces%3A%20Trends%2C%20Challenges%2C%20and%0A%20%20Cross-Platform%20Adaptability%0AAuthor%3A%20J.%20Bieniek%20and%20M.%20Rahouti%20and%20D.%20C.%20Verma%0AAbstract%3A%20%20%20As%20the%20boundaries%20of%20human%20computer%20interaction%20expand%2C%20Generative%20AI%20emerges%0Aas%20a%20key%20driver%20in%20reshaping%20user%20interfaces%2C%20introducing%20new%20possibilities%20for%0Apersonalized%2C%20multimodal%20and%20cross-platform%20interactions.%20This%20integration%0Areflects%20a%20growing%20demand%20for%20more%20adaptive%20and%20intuitive%20user%20interfaces%20that%0Acan%20accommodate%20diverse%20input%20types%20such%20as%20text%2C%20voice%20and%20video%2C%20and%20deliver%0Aseamless%20experiences%20across%20devices.%20This%20paper%20explores%20the%20integration%20of%0Agenerative%20AI%20in%20modern%20user%20interfaces%2C%20examining%20historical%20developments%20and%0Afocusing%20on%20multimodal%20interaction%2C%20cross-platform%20adaptability%20and%20dynamic%0Apersonalization.%20A%20central%20theme%20is%20the%20interface%20dilemma%2C%20which%20addresses%20the%0Achallenge%20of%20designing%20effective%20interactions%20for%20multimodal%20large%20language%0Amodels%2C%20assessing%20the%20trade-offs%20between%20graphical%2C%20voice-based%20and%20immersive%0Ainterfaces.%20The%20paper%20further%20evaluates%20lightweight%20frameworks%20tailored%20for%0Amobile%20platforms%2C%20spotlighting%20the%20role%20of%20mobile%20hardware%20in%20enabling%20scalable%0Amultimodal%20AI.%20Technical%20and%20ethical%20challenges%2C%20including%20context%20retention%2C%0Aprivacy%20concerns%20and%20balancing%20cloud%20and%20on-device%20processing%20are%20thoroughly%0Aexamined.%20Finally%2C%20the%20paper%20outlines%20future%20directions%20such%20as%20emotionally%0Aadaptive%20interfaces%2C%20predictive%20AI%20driven%20user%20interfaces%20and%20real-time%0Acollaborative%20systems%2C%20underscoring%20generative%20AI%27s%20potential%20to%20redefine%0Aadaptive%20user-centric%20interfaces%20across%20platforms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10234v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%2520in%2520Multimodal%2520User%2520Interfaces%253A%2520Trends%252C%2520Challenges%252C%2520and%250A%2520%2520Cross-Platform%2520Adaptability%26entry.906535625%3DJ.%2520Bieniek%2520and%2520M.%2520Rahouti%2520and%2520D.%2520C.%2520Verma%26entry.1292438233%3D%2520%2520As%2520the%2520boundaries%2520of%2520human%2520computer%2520interaction%2520expand%252C%2520Generative%2520AI%2520emerges%250Aas%2520a%2520key%2520driver%2520in%2520reshaping%2520user%2520interfaces%252C%2520introducing%2520new%2520possibilities%2520for%250Apersonalized%252C%2520multimodal%2520and%2520cross-platform%2520interactions.%2520This%2520integration%250Areflects%2520a%2520growing%2520demand%2520for%2520more%2520adaptive%2520and%2520intuitive%2520user%2520interfaces%2520that%250Acan%2520accommodate%2520diverse%2520input%2520types%2520such%2520as%2520text%252C%2520voice%2520and%2520video%252C%2520and%2520deliver%250Aseamless%2520experiences%2520across%2520devices.%2520This%2520paper%2520explores%2520the%2520integration%2520of%250Agenerative%2520AI%2520in%2520modern%2520user%2520interfaces%252C%2520examining%2520historical%2520developments%2520and%250Afocusing%2520on%2520multimodal%2520interaction%252C%2520cross-platform%2520adaptability%2520and%2520dynamic%250Apersonalization.%2520A%2520central%2520theme%2520is%2520the%2520interface%2520dilemma%252C%2520which%2520addresses%2520the%250Achallenge%2520of%2520designing%2520effective%2520interactions%2520for%2520multimodal%2520large%2520language%250Amodels%252C%2520assessing%2520the%2520trade-offs%2520between%2520graphical%252C%2520voice-based%2520and%2520immersive%250Ainterfaces.%2520The%2520paper%2520further%2520evaluates%2520lightweight%2520frameworks%2520tailored%2520for%250Amobile%2520platforms%252C%2520spotlighting%2520the%2520role%2520of%2520mobile%2520hardware%2520in%2520enabling%2520scalable%250Amultimodal%2520AI.%2520Technical%2520and%2520ethical%2520challenges%252C%2520including%2520context%2520retention%252C%250Aprivacy%2520concerns%2520and%2520balancing%2520cloud%2520and%2520on-device%2520processing%2520are%2520thoroughly%250Aexamined.%2520Finally%252C%2520the%2520paper%2520outlines%2520future%2520directions%2520such%2520as%2520emotionally%250Aadaptive%2520interfaces%252C%2520predictive%2520AI%2520driven%2520user%2520interfaces%2520and%2520real-time%250Acollaborative%2520systems%252C%2520underscoring%2520generative%2520AI%2527s%2520potential%2520to%2520redefine%250Aadaptive%2520user-centric%2520interfaces%2520across%2520platforms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10234v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%20in%20Multimodal%20User%20Interfaces%3A%20Trends%2C%20Challenges%2C%20and%0A%20%20Cross-Platform%20Adaptability&entry.906535625=J.%20Bieniek%20and%20M.%20Rahouti%20and%20D.%20C.%20Verma&entry.1292438233=%20%20As%20the%20boundaries%20of%20human%20computer%20interaction%20expand%2C%20Generative%20AI%20emerges%0Aas%20a%20key%20driver%20in%20reshaping%20user%20interfaces%2C%20introducing%20new%20possibilities%20for%0Apersonalized%2C%20multimodal%20and%20cross-platform%20interactions.%20This%20integration%0Areflects%20a%20growing%20demand%20for%20more%20adaptive%20and%20intuitive%20user%20interfaces%20that%0Acan%20accommodate%20diverse%20input%20types%20such%20as%20text%2C%20voice%20and%20video%2C%20and%20deliver%0Aseamless%20experiences%20across%20devices.%20This%20paper%20explores%20the%20integration%20of%0Agenerative%20AI%20in%20modern%20user%20interfaces%2C%20examining%20historical%20developments%20and%0Afocusing%20on%20multimodal%20interaction%2C%20cross-platform%20adaptability%20and%20dynamic%0Apersonalization.%20A%20central%20theme%20is%20the%20interface%20dilemma%2C%20which%20addresses%20the%0Achallenge%20of%20designing%20effective%20interactions%20for%20multimodal%20large%20language%0Amodels%2C%20assessing%20the%20trade-offs%20between%20graphical%2C%20voice-based%20and%20immersive%0Ainterfaces.%20The%20paper%20further%20evaluates%20lightweight%20frameworks%20tailored%20for%0Amobile%20platforms%2C%20spotlighting%20the%20role%20of%20mobile%20hardware%20in%20enabling%20scalable%0Amultimodal%20AI.%20Technical%20and%20ethical%20challenges%2C%20including%20context%20retention%2C%0Aprivacy%20concerns%20and%20balancing%20cloud%20and%20on-device%20processing%20are%20thoroughly%0Aexamined.%20Finally%2C%20the%20paper%20outlines%20future%20directions%20such%20as%20emotionally%0Aadaptive%20interfaces%2C%20predictive%20AI%20driven%20user%20interfaces%20and%20real-time%0Acollaborative%20systems%2C%20underscoring%20generative%20AI%27s%20potential%20to%20redefine%0Aadaptive%20user-centric%20interfaces%20across%20platforms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10234v1&entry.124074799=Read"},
{"title": "CLCE: An Approach to Refining Cross-Entropy and Contrastive Learning for\n  Optimized Learning Fusion", "author": "Zijun Long and George Killick and Lipeng Zhuang and Gerardo Aragon-Camarasa and Zaiqiao Meng and Richard Mccreadie", "abstract": "  State-of-the-art pre-trained image models predominantly adopt a two-stage\napproach: initial unsupervised pre-training on large-scale datasets followed by\ntask-specific fine-tuning using Cross-Entropy loss~(CE). However, it has been\ndemonstrated that CE can compromise model generalization and stability. While\nrecent works employing contrastive learning address some of these limitations\nby enhancing the quality of embeddings and producing better decision\nboundaries, they often overlook the importance of hard negative mining and rely\non resource intensive and slow training using large sample batches. To counter\nthese issues, we introduce a novel approach named CLCE, which integrates\nLabel-Aware Contrastive Learning with CE. Our approach not only maintains the\nstrengths of both loss functions but also leverages hard negative mining in a\nsynergistic way to enhance performance. Experimental results demonstrate that\nCLCE significantly outperforms CE in Top-1 accuracy across twelve benchmarks,\nachieving gains of up to 3.52% in few-shot learning scenarios and 3.41% in\ntransfer learning settings with the BEiT-3 model. Importantly, our proposed\nCLCE approach effectively mitigates the dependency of contrastive learning on\nlarge batch sizes such as 4096 samples per batch, a limitation that has\npreviously constrained the application of contrastive learning in\nbudget-limited hardware environments.\n", "link": "http://arxiv.org/abs/2402.14551v2", "date": "2024-11-15", "relevancy": 2.1355, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5509}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5232}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLCE%3A%20An%20Approach%20to%20Refining%20Cross-Entropy%20and%20Contrastive%20Learning%20for%0A%20%20Optimized%20Learning%20Fusion&body=Title%3A%20CLCE%3A%20An%20Approach%20to%20Refining%20Cross-Entropy%20and%20Contrastive%20Learning%20for%0A%20%20Optimized%20Learning%20Fusion%0AAuthor%3A%20Zijun%20Long%20and%20George%20Killick%20and%20Lipeng%20Zhuang%20and%20Gerardo%20Aragon-Camarasa%20and%20Zaiqiao%20Meng%20and%20Richard%20Mccreadie%0AAbstract%3A%20%20%20State-of-the-art%20pre-trained%20image%20models%20predominantly%20adopt%20a%20two-stage%0Aapproach%3A%20initial%20unsupervised%20pre-training%20on%20large-scale%20datasets%20followed%20by%0Atask-specific%20fine-tuning%20using%20Cross-Entropy%20loss~%28CE%29.%20However%2C%20it%20has%20been%0Ademonstrated%20that%20CE%20can%20compromise%20model%20generalization%20and%20stability.%20While%0Arecent%20works%20employing%20contrastive%20learning%20address%20some%20of%20these%20limitations%0Aby%20enhancing%20the%20quality%20of%20embeddings%20and%20producing%20better%20decision%0Aboundaries%2C%20they%20often%20overlook%20the%20importance%20of%20hard%20negative%20mining%20and%20rely%0Aon%20resource%20intensive%20and%20slow%20training%20using%20large%20sample%20batches.%20To%20counter%0Athese%20issues%2C%20we%20introduce%20a%20novel%20approach%20named%20CLCE%2C%20which%20integrates%0ALabel-Aware%20Contrastive%20Learning%20with%20CE.%20Our%20approach%20not%20only%20maintains%20the%0Astrengths%20of%20both%20loss%20functions%20but%20also%20leverages%20hard%20negative%20mining%20in%20a%0Asynergistic%20way%20to%20enhance%20performance.%20Experimental%20results%20demonstrate%20that%0ACLCE%20significantly%20outperforms%20CE%20in%20Top-1%20accuracy%20across%20twelve%20benchmarks%2C%0Aachieving%20gains%20of%20up%20to%203.52%25%20in%20few-shot%20learning%20scenarios%20and%203.41%25%20in%0Atransfer%20learning%20settings%20with%20the%20BEiT-3%20model.%20Importantly%2C%20our%20proposed%0ACLCE%20approach%20effectively%20mitigates%20the%20dependency%20of%20contrastive%20learning%20on%0Alarge%20batch%20sizes%20such%20as%204096%20samples%20per%20batch%2C%20a%20limitation%20that%20has%0Apreviously%20constrained%20the%20application%20of%20contrastive%20learning%20in%0Abudget-limited%20hardware%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14551v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLCE%253A%2520An%2520Approach%2520to%2520Refining%2520Cross-Entropy%2520and%2520Contrastive%2520Learning%2520for%250A%2520%2520Optimized%2520Learning%2520Fusion%26entry.906535625%3DZijun%2520Long%2520and%2520George%2520Killick%2520and%2520Lipeng%2520Zhuang%2520and%2520Gerardo%2520Aragon-Camarasa%2520and%2520Zaiqiao%2520Meng%2520and%2520Richard%2520Mccreadie%26entry.1292438233%3D%2520%2520State-of-the-art%2520pre-trained%2520image%2520models%2520predominantly%2520adopt%2520a%2520two-stage%250Aapproach%253A%2520initial%2520unsupervised%2520pre-training%2520on%2520large-scale%2520datasets%2520followed%2520by%250Atask-specific%2520fine-tuning%2520using%2520Cross-Entropy%2520loss~%2528CE%2529.%2520However%252C%2520it%2520has%2520been%250Ademonstrated%2520that%2520CE%2520can%2520compromise%2520model%2520generalization%2520and%2520stability.%2520While%250Arecent%2520works%2520employing%2520contrastive%2520learning%2520address%2520some%2520of%2520these%2520limitations%250Aby%2520enhancing%2520the%2520quality%2520of%2520embeddings%2520and%2520producing%2520better%2520decision%250Aboundaries%252C%2520they%2520often%2520overlook%2520the%2520importance%2520of%2520hard%2520negative%2520mining%2520and%2520rely%250Aon%2520resource%2520intensive%2520and%2520slow%2520training%2520using%2520large%2520sample%2520batches.%2520To%2520counter%250Athese%2520issues%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520named%2520CLCE%252C%2520which%2520integrates%250ALabel-Aware%2520Contrastive%2520Learning%2520with%2520CE.%2520Our%2520approach%2520not%2520only%2520maintains%2520the%250Astrengths%2520of%2520both%2520loss%2520functions%2520but%2520also%2520leverages%2520hard%2520negative%2520mining%2520in%2520a%250Asynergistic%2520way%2520to%2520enhance%2520performance.%2520Experimental%2520results%2520demonstrate%2520that%250ACLCE%2520significantly%2520outperforms%2520CE%2520in%2520Top-1%2520accuracy%2520across%2520twelve%2520benchmarks%252C%250Aachieving%2520gains%2520of%2520up%2520to%25203.52%2525%2520in%2520few-shot%2520learning%2520scenarios%2520and%25203.41%2525%2520in%250Atransfer%2520learning%2520settings%2520with%2520the%2520BEiT-3%2520model.%2520Importantly%252C%2520our%2520proposed%250ACLCE%2520approach%2520effectively%2520mitigates%2520the%2520dependency%2520of%2520contrastive%2520learning%2520on%250Alarge%2520batch%2520sizes%2520such%2520as%25204096%2520samples%2520per%2520batch%252C%2520a%2520limitation%2520that%2520has%250Apreviously%2520constrained%2520the%2520application%2520of%2520contrastive%2520learning%2520in%250Abudget-limited%2520hardware%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14551v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLCE%3A%20An%20Approach%20to%20Refining%20Cross-Entropy%20and%20Contrastive%20Learning%20for%0A%20%20Optimized%20Learning%20Fusion&entry.906535625=Zijun%20Long%20and%20George%20Killick%20and%20Lipeng%20Zhuang%20and%20Gerardo%20Aragon-Camarasa%20and%20Zaiqiao%20Meng%20and%20Richard%20Mccreadie&entry.1292438233=%20%20State-of-the-art%20pre-trained%20image%20models%20predominantly%20adopt%20a%20two-stage%0Aapproach%3A%20initial%20unsupervised%20pre-training%20on%20large-scale%20datasets%20followed%20by%0Atask-specific%20fine-tuning%20using%20Cross-Entropy%20loss~%28CE%29.%20However%2C%20it%20has%20been%0Ademonstrated%20that%20CE%20can%20compromise%20model%20generalization%20and%20stability.%20While%0Arecent%20works%20employing%20contrastive%20learning%20address%20some%20of%20these%20limitations%0Aby%20enhancing%20the%20quality%20of%20embeddings%20and%20producing%20better%20decision%0Aboundaries%2C%20they%20often%20overlook%20the%20importance%20of%20hard%20negative%20mining%20and%20rely%0Aon%20resource%20intensive%20and%20slow%20training%20using%20large%20sample%20batches.%20To%20counter%0Athese%20issues%2C%20we%20introduce%20a%20novel%20approach%20named%20CLCE%2C%20which%20integrates%0ALabel-Aware%20Contrastive%20Learning%20with%20CE.%20Our%20approach%20not%20only%20maintains%20the%0Astrengths%20of%20both%20loss%20functions%20but%20also%20leverages%20hard%20negative%20mining%20in%20a%0Asynergistic%20way%20to%20enhance%20performance.%20Experimental%20results%20demonstrate%20that%0ACLCE%20significantly%20outperforms%20CE%20in%20Top-1%20accuracy%20across%20twelve%20benchmarks%2C%0Aachieving%20gains%20of%20up%20to%203.52%25%20in%20few-shot%20learning%20scenarios%20and%203.41%25%20in%0Atransfer%20learning%20settings%20with%20the%20BEiT-3%20model.%20Importantly%2C%20our%20proposed%0ACLCE%20approach%20effectively%20mitigates%20the%20dependency%20of%20contrastive%20learning%20on%0Alarge%20batch%20sizes%20such%20as%204096%20samples%20per%20batch%2C%20a%20limitation%20that%20has%0Apreviously%20constrained%20the%20application%20of%20contrastive%20learning%20in%0Abudget-limited%20hardware%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14551v2&entry.124074799=Read"},
{"title": "Modification Takes Courage: Seamless Image Stitching via\n  Reference-Driven Inpainting", "author": "Ziqi Xie and Xiao Lai and Weidong Zhao and Xianhui Liu and Wenlong Hou", "abstract": "  Current image stitching methods often produce noticeable seams in challenging\nscenarios such as uneven hue and large parallax. To tackle this problem, we\npropose the Reference-Driven Inpainting Stitcher (RDIStitcher), which\nreformulates the image fusion and rectangling as a reference-based inpainting\nmodel, incorporating a larger modification fusion area and stronger\nmodification intensity than previous methods. Furthermore, we introduce a\nself-supervised model training method, which enables the implementation of\nRDIStitcher without requiring labeled data by fine-tuning a Text-to-Image (T2I)\ndiffusion model. Recognizing difficulties in assessing the quality of stitched\nimages, we present the Multimodal Large Language Models (MLLMs)-based metrics,\noffering a new perspective on evaluating stitched image quality. Compared to\nthe state-of-the-art (SOTA) method, extensive experiments demonstrate that our\nmethod significantly enhances content coherence and seamless transitions in the\nstitched images. Especially in the zero-shot experiments, our method exhibits\nstrong generalization capabilities. Code:\nhttps://github.com/yayoyo66/RDIStitcher\n", "link": "http://arxiv.org/abs/2411.10309v1", "date": "2024-11-15", "relevancy": 2.1287, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5357}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5322}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modification%20Takes%20Courage%3A%20Seamless%20Image%20Stitching%20via%0A%20%20Reference-Driven%20Inpainting&body=Title%3A%20Modification%20Takes%20Courage%3A%20Seamless%20Image%20Stitching%20via%0A%20%20Reference-Driven%20Inpainting%0AAuthor%3A%20Ziqi%20Xie%20and%20Xiao%20Lai%20and%20Weidong%20Zhao%20and%20Xianhui%20Liu%20and%20Wenlong%20Hou%0AAbstract%3A%20%20%20Current%20image%20stitching%20methods%20often%20produce%20noticeable%20seams%20in%20challenging%0Ascenarios%20such%20as%20uneven%20hue%20and%20large%20parallax.%20To%20tackle%20this%20problem%2C%20we%0Apropose%20the%20Reference-Driven%20Inpainting%20Stitcher%20%28RDIStitcher%29%2C%20which%0Areformulates%20the%20image%20fusion%20and%20rectangling%20as%20a%20reference-based%20inpainting%0Amodel%2C%20incorporating%20a%20larger%20modification%20fusion%20area%20and%20stronger%0Amodification%20intensity%20than%20previous%20methods.%20Furthermore%2C%20we%20introduce%20a%0Aself-supervised%20model%20training%20method%2C%20which%20enables%20the%20implementation%20of%0ARDIStitcher%20without%20requiring%20labeled%20data%20by%20fine-tuning%20a%20Text-to-Image%20%28T2I%29%0Adiffusion%20model.%20Recognizing%20difficulties%20in%20assessing%20the%20quality%20of%20stitched%0Aimages%2C%20we%20present%20the%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29-based%20metrics%2C%0Aoffering%20a%20new%20perspective%20on%20evaluating%20stitched%20image%20quality.%20Compared%20to%0Athe%20state-of-the-art%20%28SOTA%29%20method%2C%20extensive%20experiments%20demonstrate%20that%20our%0Amethod%20significantly%20enhances%20content%20coherence%20and%20seamless%20transitions%20in%20the%0Astitched%20images.%20Especially%20in%20the%20zero-shot%20experiments%2C%20our%20method%20exhibits%0Astrong%20generalization%20capabilities.%20Code%3A%0Ahttps%3A//github.com/yayoyo66/RDIStitcher%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10309v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModification%2520Takes%2520Courage%253A%2520Seamless%2520Image%2520Stitching%2520via%250A%2520%2520Reference-Driven%2520Inpainting%26entry.906535625%3DZiqi%2520Xie%2520and%2520Xiao%2520Lai%2520and%2520Weidong%2520Zhao%2520and%2520Xianhui%2520Liu%2520and%2520Wenlong%2520Hou%26entry.1292438233%3D%2520%2520Current%2520image%2520stitching%2520methods%2520often%2520produce%2520noticeable%2520seams%2520in%2520challenging%250Ascenarios%2520such%2520as%2520uneven%2520hue%2520and%2520large%2520parallax.%2520To%2520tackle%2520this%2520problem%252C%2520we%250Apropose%2520the%2520Reference-Driven%2520Inpainting%2520Stitcher%2520%2528RDIStitcher%2529%252C%2520which%250Areformulates%2520the%2520image%2520fusion%2520and%2520rectangling%2520as%2520a%2520reference-based%2520inpainting%250Amodel%252C%2520incorporating%2520a%2520larger%2520modification%2520fusion%2520area%2520and%2520stronger%250Amodification%2520intensity%2520than%2520previous%2520methods.%2520Furthermore%252C%2520we%2520introduce%2520a%250Aself-supervised%2520model%2520training%2520method%252C%2520which%2520enables%2520the%2520implementation%2520of%250ARDIStitcher%2520without%2520requiring%2520labeled%2520data%2520by%2520fine-tuning%2520a%2520Text-to-Image%2520%2528T2I%2529%250Adiffusion%2520model.%2520Recognizing%2520difficulties%2520in%2520assessing%2520the%2520quality%2520of%2520stitched%250Aimages%252C%2520we%2520present%2520the%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529-based%2520metrics%252C%250Aoffering%2520a%2520new%2520perspective%2520on%2520evaluating%2520stitched%2520image%2520quality.%2520Compared%2520to%250Athe%2520state-of-the-art%2520%2528SOTA%2529%2520method%252C%2520extensive%2520experiments%2520demonstrate%2520that%2520our%250Amethod%2520significantly%2520enhances%2520content%2520coherence%2520and%2520seamless%2520transitions%2520in%2520the%250Astitched%2520images.%2520Especially%2520in%2520the%2520zero-shot%2520experiments%252C%2520our%2520method%2520exhibits%250Astrong%2520generalization%2520capabilities.%2520Code%253A%250Ahttps%253A//github.com/yayoyo66/RDIStitcher%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10309v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modification%20Takes%20Courage%3A%20Seamless%20Image%20Stitching%20via%0A%20%20Reference-Driven%20Inpainting&entry.906535625=Ziqi%20Xie%20and%20Xiao%20Lai%20and%20Weidong%20Zhao%20and%20Xianhui%20Liu%20and%20Wenlong%20Hou&entry.1292438233=%20%20Current%20image%20stitching%20methods%20often%20produce%20noticeable%20seams%20in%20challenging%0Ascenarios%20such%20as%20uneven%20hue%20and%20large%20parallax.%20To%20tackle%20this%20problem%2C%20we%0Apropose%20the%20Reference-Driven%20Inpainting%20Stitcher%20%28RDIStitcher%29%2C%20which%0Areformulates%20the%20image%20fusion%20and%20rectangling%20as%20a%20reference-based%20inpainting%0Amodel%2C%20incorporating%20a%20larger%20modification%20fusion%20area%20and%20stronger%0Amodification%20intensity%20than%20previous%20methods.%20Furthermore%2C%20we%20introduce%20a%0Aself-supervised%20model%20training%20method%2C%20which%20enables%20the%20implementation%20of%0ARDIStitcher%20without%20requiring%20labeled%20data%20by%20fine-tuning%20a%20Text-to-Image%20%28T2I%29%0Adiffusion%20model.%20Recognizing%20difficulties%20in%20assessing%20the%20quality%20of%20stitched%0Aimages%2C%20we%20present%20the%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29-based%20metrics%2C%0Aoffering%20a%20new%20perspective%20on%20evaluating%20stitched%20image%20quality.%20Compared%20to%0Athe%20state-of-the-art%20%28SOTA%29%20method%2C%20extensive%20experiments%20demonstrate%20that%20our%0Amethod%20significantly%20enhances%20content%20coherence%20and%20seamless%20transitions%20in%20the%0Astitched%20images.%20Especially%20in%20the%20zero-shot%20experiments%2C%20our%20method%20exhibits%0Astrong%20generalization%20capabilities.%20Code%3A%0Ahttps%3A//github.com/yayoyo66/RDIStitcher%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10309v1&entry.124074799=Read"},
{"title": "Training Deep 3D Convolutional Neural Networks to Extract BSM Physics\n  Parameters Directly from HEP Data: a Proof-of-Concept Study Using Monte Carlo\n  Simulations", "author": "S. Dubey and T. E. Browder and S. Kohani and R. Mandal and A. Sibidanov and R. Sinha", "abstract": "  We report on a novel application of computer vision techniques to extract\nbeyond the Standard Model parameters directly from high energy physics flavor\ndata. We propose a simple but novel data representation that transforms the\nangular and kinematic distributions into \"quasi-images\", which are used to\ntrain a convolutional neural network to perform regression tasks, similar to\nfitting. As a proof-of-concept, we train a 34-layer Residual Neural Network to\nregress on these images and determine information about the Wilson Coefficient\n$C_{9}$ in Monte Carlo simulations of $B^0 \\rightarrow K^{*0}\\mu^{+}\\mu^{-}$\ndecays. The method described here can be generalized and may find applicability\nacross a variety of experiments.\n", "link": "http://arxiv.org/abs/2311.13060v3", "date": "2024-11-15", "relevancy": 2.1281, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5522}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5345}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20Deep%203D%20Convolutional%20Neural%20Networks%20to%20Extract%20BSM%20Physics%0A%20%20Parameters%20Directly%20from%20HEP%20Data%3A%20a%20Proof-of-Concept%20Study%20Using%20Monte%20Carlo%0A%20%20Simulations&body=Title%3A%20Training%20Deep%203D%20Convolutional%20Neural%20Networks%20to%20Extract%20BSM%20Physics%0A%20%20Parameters%20Directly%20from%20HEP%20Data%3A%20a%20Proof-of-Concept%20Study%20Using%20Monte%20Carlo%0A%20%20Simulations%0AAuthor%3A%20S.%20Dubey%20and%20T.%20E.%20Browder%20and%20S.%20Kohani%20and%20R.%20Mandal%20and%20A.%20Sibidanov%20and%20R.%20Sinha%0AAbstract%3A%20%20%20We%20report%20on%20a%20novel%20application%20of%20computer%20vision%20techniques%20to%20extract%0Abeyond%20the%20Standard%20Model%20parameters%20directly%20from%20high%20energy%20physics%20flavor%0Adata.%20We%20propose%20a%20simple%20but%20novel%20data%20representation%20that%20transforms%20the%0Aangular%20and%20kinematic%20distributions%20into%20%22quasi-images%22%2C%20which%20are%20used%20to%0Atrain%20a%20convolutional%20neural%20network%20to%20perform%20regression%20tasks%2C%20similar%20to%0Afitting.%20As%20a%20proof-of-concept%2C%20we%20train%20a%2034-layer%20Residual%20Neural%20Network%20to%0Aregress%20on%20these%20images%20and%20determine%20information%20about%20the%20Wilson%20Coefficient%0A%24C_%7B9%7D%24%20in%20Monte%20Carlo%20simulations%20of%20%24B%5E0%20%5Crightarrow%20K%5E%7B%2A0%7D%5Cmu%5E%7B%2B%7D%5Cmu%5E%7B-%7D%24%0Adecays.%20The%20method%20described%20here%20can%20be%20generalized%20and%20may%20find%20applicability%0Aacross%20a%20variety%20of%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.13060v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520Deep%25203D%2520Convolutional%2520Neural%2520Networks%2520to%2520Extract%2520BSM%2520Physics%250A%2520%2520Parameters%2520Directly%2520from%2520HEP%2520Data%253A%2520a%2520Proof-of-Concept%2520Study%2520Using%2520Monte%2520Carlo%250A%2520%2520Simulations%26entry.906535625%3DS.%2520Dubey%2520and%2520T.%2520E.%2520Browder%2520and%2520S.%2520Kohani%2520and%2520R.%2520Mandal%2520and%2520A.%2520Sibidanov%2520and%2520R.%2520Sinha%26entry.1292438233%3D%2520%2520We%2520report%2520on%2520a%2520novel%2520application%2520of%2520computer%2520vision%2520techniques%2520to%2520extract%250Abeyond%2520the%2520Standard%2520Model%2520parameters%2520directly%2520from%2520high%2520energy%2520physics%2520flavor%250Adata.%2520We%2520propose%2520a%2520simple%2520but%2520novel%2520data%2520representation%2520that%2520transforms%2520the%250Aangular%2520and%2520kinematic%2520distributions%2520into%2520%2522quasi-images%2522%252C%2520which%2520are%2520used%2520to%250Atrain%2520a%2520convolutional%2520neural%2520network%2520to%2520perform%2520regression%2520tasks%252C%2520similar%2520to%250Afitting.%2520As%2520a%2520proof-of-concept%252C%2520we%2520train%2520a%252034-layer%2520Residual%2520Neural%2520Network%2520to%250Aregress%2520on%2520these%2520images%2520and%2520determine%2520information%2520about%2520the%2520Wilson%2520Coefficient%250A%2524C_%257B9%257D%2524%2520in%2520Monte%2520Carlo%2520simulations%2520of%2520%2524B%255E0%2520%255Crightarrow%2520K%255E%257B%252A0%257D%255Cmu%255E%257B%252B%257D%255Cmu%255E%257B-%257D%2524%250Adecays.%2520The%2520method%2520described%2520here%2520can%2520be%2520generalized%2520and%2520may%2520find%2520applicability%250Aacross%2520a%2520variety%2520of%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.13060v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20Deep%203D%20Convolutional%20Neural%20Networks%20to%20Extract%20BSM%20Physics%0A%20%20Parameters%20Directly%20from%20HEP%20Data%3A%20a%20Proof-of-Concept%20Study%20Using%20Monte%20Carlo%0A%20%20Simulations&entry.906535625=S.%20Dubey%20and%20T.%20E.%20Browder%20and%20S.%20Kohani%20and%20R.%20Mandal%20and%20A.%20Sibidanov%20and%20R.%20Sinha&entry.1292438233=%20%20We%20report%20on%20a%20novel%20application%20of%20computer%20vision%20techniques%20to%20extract%0Abeyond%20the%20Standard%20Model%20parameters%20directly%20from%20high%20energy%20physics%20flavor%0Adata.%20We%20propose%20a%20simple%20but%20novel%20data%20representation%20that%20transforms%20the%0Aangular%20and%20kinematic%20distributions%20into%20%22quasi-images%22%2C%20which%20are%20used%20to%0Atrain%20a%20convolutional%20neural%20network%20to%20perform%20regression%20tasks%2C%20similar%20to%0Afitting.%20As%20a%20proof-of-concept%2C%20we%20train%20a%2034-layer%20Residual%20Neural%20Network%20to%0Aregress%20on%20these%20images%20and%20determine%20information%20about%20the%20Wilson%20Coefficient%0A%24C_%7B9%7D%24%20in%20Monte%20Carlo%20simulations%20of%20%24B%5E0%20%5Crightarrow%20K%5E%7B%2A0%7D%5Cmu%5E%7B%2B%7D%5Cmu%5E%7B-%7D%24%0Adecays.%20The%20method%20described%20here%20can%20be%20generalized%20and%20may%20find%20applicability%0Aacross%20a%20variety%20of%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.13060v3&entry.124074799=Read"},
{"title": "Unlocking Real-Time Fluorescence Lifetime Imaging: Multi-Pixel\n  Parallelism for FPGA-Accelerated Processing", "author": "Ismail Erbas and Aporva Amarnath and Vikas Pandey and Karthik Swaminathan and Naigang Wang and Xavier Intes", "abstract": "  Fluorescence lifetime imaging (FLI) is a widely used technique in the\nbiomedical field for measuring the decay times of fluorescent molecules,\nproviding insights into metabolic states, protein interactions, and\nligand-receptor bindings. However, its broader application in fast biological\nprocesses, such as dynamic activity monitoring, and clinical use, such as in\nguided surgery, is limited by long data acquisition times and computationally\ndemanding data processing. While deep learning has reduced post-processing\ntimes, time-resolved data acquisition remains a bottleneck for real-time\napplications. To address this, we propose a method to achieve real-time FLI\nusing an FPGA-based hardware accelerator. Specifically, we implemented a\nGRU-based sequence-to-sequence (Seq2Seq) model on an FPGA board compatible with\ntime-resolved cameras. The GRU model balances accurate processing with the\nresource constraints of FPGAs, which have limited DSP units and BRAM. The\nlimited memory and computational resources on the FPGA require efficient\nscheduling of operations and memory allocation to deploy deep learning models\nfor low-latency applications. We address these challenges by using STOMP, a\nqueue-based discrete-event simulator that automates and optimizes task\nscheduling and memory management on hardware. By integrating a GRU-based\nSeq2Seq model and its compressed version, called Seq2SeqLite, generated through\nknowledge distillation, we were able to process multiple pixels in parallel,\nreducing latency compared to sequential processing. We explore various levels\nof parallelism to achieve an optimal balance between performance and resource\nutilization. Our results indicate that the proposed techniques achieved a 17.7x\nand 52.0x speedup over manual scheduling for the Seq2Seq model and the\nSeq2SeqLite model, respectively.\n", "link": "http://arxiv.org/abs/2410.07364v2", "date": "2024-11-15", "relevancy": 2.1201, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5707}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5456}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4982}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unlocking%20Real-Time%20Fluorescence%20Lifetime%20Imaging%3A%20Multi-Pixel%0A%20%20Parallelism%20for%20FPGA-Accelerated%20Processing&body=Title%3A%20Unlocking%20Real-Time%20Fluorescence%20Lifetime%20Imaging%3A%20Multi-Pixel%0A%20%20Parallelism%20for%20FPGA-Accelerated%20Processing%0AAuthor%3A%20Ismail%20Erbas%20and%20Aporva%20Amarnath%20and%20Vikas%20Pandey%20and%20Karthik%20Swaminathan%20and%20Naigang%20Wang%20and%20Xavier%20Intes%0AAbstract%3A%20%20%20Fluorescence%20lifetime%20imaging%20%28FLI%29%20is%20a%20widely%20used%20technique%20in%20the%0Abiomedical%20field%20for%20measuring%20the%20decay%20times%20of%20fluorescent%20molecules%2C%0Aproviding%20insights%20into%20metabolic%20states%2C%20protein%20interactions%2C%20and%0Aligand-receptor%20bindings.%20However%2C%20its%20broader%20application%20in%20fast%20biological%0Aprocesses%2C%20such%20as%20dynamic%20activity%20monitoring%2C%20and%20clinical%20use%2C%20such%20as%20in%0Aguided%20surgery%2C%20is%20limited%20by%20long%20data%20acquisition%20times%20and%20computationally%0Ademanding%20data%20processing.%20While%20deep%20learning%20has%20reduced%20post-processing%0Atimes%2C%20time-resolved%20data%20acquisition%20remains%20a%20bottleneck%20for%20real-time%0Aapplications.%20To%20address%20this%2C%20we%20propose%20a%20method%20to%20achieve%20real-time%20FLI%0Ausing%20an%20FPGA-based%20hardware%20accelerator.%20Specifically%2C%20we%20implemented%20a%0AGRU-based%20sequence-to-sequence%20%28Seq2Seq%29%20model%20on%20an%20FPGA%20board%20compatible%20with%0Atime-resolved%20cameras.%20The%20GRU%20model%20balances%20accurate%20processing%20with%20the%0Aresource%20constraints%20of%20FPGAs%2C%20which%20have%20limited%20DSP%20units%20and%20BRAM.%20The%0Alimited%20memory%20and%20computational%20resources%20on%20the%20FPGA%20require%20efficient%0Ascheduling%20of%20operations%20and%20memory%20allocation%20to%20deploy%20deep%20learning%20models%0Afor%20low-latency%20applications.%20We%20address%20these%20challenges%20by%20using%20STOMP%2C%20a%0Aqueue-based%20discrete-event%20simulator%20that%20automates%20and%20optimizes%20task%0Ascheduling%20and%20memory%20management%20on%20hardware.%20By%20integrating%20a%20GRU-based%0ASeq2Seq%20model%20and%20its%20compressed%20version%2C%20called%20Seq2SeqLite%2C%20generated%20through%0Aknowledge%20distillation%2C%20we%20were%20able%20to%20process%20multiple%20pixels%20in%20parallel%2C%0Areducing%20latency%20compared%20to%20sequential%20processing.%20We%20explore%20various%20levels%0Aof%20parallelism%20to%20achieve%20an%20optimal%20balance%20between%20performance%20and%20resource%0Autilization.%20Our%20results%20indicate%20that%20the%20proposed%20techniques%20achieved%20a%2017.7x%0Aand%2052.0x%20speedup%20over%20manual%20scheduling%20for%20the%20Seq2Seq%20model%20and%20the%0ASeq2SeqLite%20model%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07364v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnlocking%2520Real-Time%2520Fluorescence%2520Lifetime%2520Imaging%253A%2520Multi-Pixel%250A%2520%2520Parallelism%2520for%2520FPGA-Accelerated%2520Processing%26entry.906535625%3DIsmail%2520Erbas%2520and%2520Aporva%2520Amarnath%2520and%2520Vikas%2520Pandey%2520and%2520Karthik%2520Swaminathan%2520and%2520Naigang%2520Wang%2520and%2520Xavier%2520Intes%26entry.1292438233%3D%2520%2520Fluorescence%2520lifetime%2520imaging%2520%2528FLI%2529%2520is%2520a%2520widely%2520used%2520technique%2520in%2520the%250Abiomedical%2520field%2520for%2520measuring%2520the%2520decay%2520times%2520of%2520fluorescent%2520molecules%252C%250Aproviding%2520insights%2520into%2520metabolic%2520states%252C%2520protein%2520interactions%252C%2520and%250Aligand-receptor%2520bindings.%2520However%252C%2520its%2520broader%2520application%2520in%2520fast%2520biological%250Aprocesses%252C%2520such%2520as%2520dynamic%2520activity%2520monitoring%252C%2520and%2520clinical%2520use%252C%2520such%2520as%2520in%250Aguided%2520surgery%252C%2520is%2520limited%2520by%2520long%2520data%2520acquisition%2520times%2520and%2520computationally%250Ademanding%2520data%2520processing.%2520While%2520deep%2520learning%2520has%2520reduced%2520post-processing%250Atimes%252C%2520time-resolved%2520data%2520acquisition%2520remains%2520a%2520bottleneck%2520for%2520real-time%250Aapplications.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520method%2520to%2520achieve%2520real-time%2520FLI%250Ausing%2520an%2520FPGA-based%2520hardware%2520accelerator.%2520Specifically%252C%2520we%2520implemented%2520a%250AGRU-based%2520sequence-to-sequence%2520%2528Seq2Seq%2529%2520model%2520on%2520an%2520FPGA%2520board%2520compatible%2520with%250Atime-resolved%2520cameras.%2520The%2520GRU%2520model%2520balances%2520accurate%2520processing%2520with%2520the%250Aresource%2520constraints%2520of%2520FPGAs%252C%2520which%2520have%2520limited%2520DSP%2520units%2520and%2520BRAM.%2520The%250Alimited%2520memory%2520and%2520computational%2520resources%2520on%2520the%2520FPGA%2520require%2520efficient%250Ascheduling%2520of%2520operations%2520and%2520memory%2520allocation%2520to%2520deploy%2520deep%2520learning%2520models%250Afor%2520low-latency%2520applications.%2520We%2520address%2520these%2520challenges%2520by%2520using%2520STOMP%252C%2520a%250Aqueue-based%2520discrete-event%2520simulator%2520that%2520automates%2520and%2520optimizes%2520task%250Ascheduling%2520and%2520memory%2520management%2520on%2520hardware.%2520By%2520integrating%2520a%2520GRU-based%250ASeq2Seq%2520model%2520and%2520its%2520compressed%2520version%252C%2520called%2520Seq2SeqLite%252C%2520generated%2520through%250Aknowledge%2520distillation%252C%2520we%2520were%2520able%2520to%2520process%2520multiple%2520pixels%2520in%2520parallel%252C%250Areducing%2520latency%2520compared%2520to%2520sequential%2520processing.%2520We%2520explore%2520various%2520levels%250Aof%2520parallelism%2520to%2520achieve%2520an%2520optimal%2520balance%2520between%2520performance%2520and%2520resource%250Autilization.%2520Our%2520results%2520indicate%2520that%2520the%2520proposed%2520techniques%2520achieved%2520a%252017.7x%250Aand%252052.0x%2520speedup%2520over%2520manual%2520scheduling%2520for%2520the%2520Seq2Seq%2520model%2520and%2520the%250ASeq2SeqLite%2520model%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07364v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unlocking%20Real-Time%20Fluorescence%20Lifetime%20Imaging%3A%20Multi-Pixel%0A%20%20Parallelism%20for%20FPGA-Accelerated%20Processing&entry.906535625=Ismail%20Erbas%20and%20Aporva%20Amarnath%20and%20Vikas%20Pandey%20and%20Karthik%20Swaminathan%20and%20Naigang%20Wang%20and%20Xavier%20Intes&entry.1292438233=%20%20Fluorescence%20lifetime%20imaging%20%28FLI%29%20is%20a%20widely%20used%20technique%20in%20the%0Abiomedical%20field%20for%20measuring%20the%20decay%20times%20of%20fluorescent%20molecules%2C%0Aproviding%20insights%20into%20metabolic%20states%2C%20protein%20interactions%2C%20and%0Aligand-receptor%20bindings.%20However%2C%20its%20broader%20application%20in%20fast%20biological%0Aprocesses%2C%20such%20as%20dynamic%20activity%20monitoring%2C%20and%20clinical%20use%2C%20such%20as%20in%0Aguided%20surgery%2C%20is%20limited%20by%20long%20data%20acquisition%20times%20and%20computationally%0Ademanding%20data%20processing.%20While%20deep%20learning%20has%20reduced%20post-processing%0Atimes%2C%20time-resolved%20data%20acquisition%20remains%20a%20bottleneck%20for%20real-time%0Aapplications.%20To%20address%20this%2C%20we%20propose%20a%20method%20to%20achieve%20real-time%20FLI%0Ausing%20an%20FPGA-based%20hardware%20accelerator.%20Specifically%2C%20we%20implemented%20a%0AGRU-based%20sequence-to-sequence%20%28Seq2Seq%29%20model%20on%20an%20FPGA%20board%20compatible%20with%0Atime-resolved%20cameras.%20The%20GRU%20model%20balances%20accurate%20processing%20with%20the%0Aresource%20constraints%20of%20FPGAs%2C%20which%20have%20limited%20DSP%20units%20and%20BRAM.%20The%0Alimited%20memory%20and%20computational%20resources%20on%20the%20FPGA%20require%20efficient%0Ascheduling%20of%20operations%20and%20memory%20allocation%20to%20deploy%20deep%20learning%20models%0Afor%20low-latency%20applications.%20We%20address%20these%20challenges%20by%20using%20STOMP%2C%20a%0Aqueue-based%20discrete-event%20simulator%20that%20automates%20and%20optimizes%20task%0Ascheduling%20and%20memory%20management%20on%20hardware.%20By%20integrating%20a%20GRU-based%0ASeq2Seq%20model%20and%20its%20compressed%20version%2C%20called%20Seq2SeqLite%2C%20generated%20through%0Aknowledge%20distillation%2C%20we%20were%20able%20to%20process%20multiple%20pixels%20in%20parallel%2C%0Areducing%20latency%20compared%20to%20sequential%20processing.%20We%20explore%20various%20levels%0Aof%20parallelism%20to%20achieve%20an%20optimal%20balance%20between%20performance%20and%20resource%0Autilization.%20Our%20results%20indicate%20that%20the%20proposed%20techniques%20achieved%20a%2017.7x%0Aand%2052.0x%20speedup%20over%20manual%20scheduling%20for%20the%20Seq2Seq%20model%20and%20the%0ASeq2SeqLite%20model%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07364v2&entry.124074799=Read"},
{"title": "Pretrained ViTs Yield Versatile Representations For Medical Images", "author": "Christos Matsoukas and Johan Fredin Haslum and Moein Sorkhei and Magnus S\u00f6derberg and Kevin Smith", "abstract": "  Convolutional Neural Networks (CNNs) have reigned for a decade as the de\nfacto approach to automated medical image diagnosis, pushing the\nstate-of-the-art in classification, detection and segmentation tasks. Over the\nlast years, vision transformers (ViTs) have appeared as a competitive\nalternative to CNNs, yielding impressive levels of performance in the natural\nimage domain, while possessing several interesting properties that could prove\nbeneficial for medical imaging tasks. In this work, we explore the benefits and\ndrawbacks of transformer-based models for medical image classification. We\nconduct a series of experiments on several standard 2D medical image benchmark\ndatasets and tasks. Our findings show that, while CNNs perform better if\ntrained from scratch, off-the-shelf vision transformers can perform on par with\nCNNs when pretrained on ImageNet, both in a supervised and self-supervised\nsetting, rendering them as a viable alternative to CNNs.\n", "link": "http://arxiv.org/abs/2303.07034v3", "date": "2024-11-15", "relevancy": 2.1038, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5297}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5265}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pretrained%20ViTs%20Yield%20Versatile%20Representations%20For%20Medical%20Images&body=Title%3A%20Pretrained%20ViTs%20Yield%20Versatile%20Representations%20For%20Medical%20Images%0AAuthor%3A%20Christos%20Matsoukas%20and%20Johan%20Fredin%20Haslum%20and%20Moein%20Sorkhei%20and%20Magnus%20S%C3%B6derberg%20and%20Kevin%20Smith%0AAbstract%3A%20%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20reigned%20for%20a%20decade%20as%20the%20de%0Afacto%20approach%20to%20automated%20medical%20image%20diagnosis%2C%20pushing%20the%0Astate-of-the-art%20in%20classification%2C%20detection%20and%20segmentation%20tasks.%20Over%20the%0Alast%20years%2C%20vision%20transformers%20%28ViTs%29%20have%20appeared%20as%20a%20competitive%0Aalternative%20to%20CNNs%2C%20yielding%20impressive%20levels%20of%20performance%20in%20the%20natural%0Aimage%20domain%2C%20while%20possessing%20several%20interesting%20properties%20that%20could%20prove%0Abeneficial%20for%20medical%20imaging%20tasks.%20In%20this%20work%2C%20we%20explore%20the%20benefits%20and%0Adrawbacks%20of%20transformer-based%20models%20for%20medical%20image%20classification.%20We%0Aconduct%20a%20series%20of%20experiments%20on%20several%20standard%202D%20medical%20image%20benchmark%0Adatasets%20and%20tasks.%20Our%20findings%20show%20that%2C%20while%20CNNs%20perform%20better%20if%0Atrained%20from%20scratch%2C%20off-the-shelf%20vision%20transformers%20can%20perform%20on%20par%20with%0ACNNs%20when%20pretrained%20on%20ImageNet%2C%20both%20in%20a%20supervised%20and%20self-supervised%0Asetting%2C%20rendering%20them%20as%20a%20viable%20alternative%20to%20CNNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.07034v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPretrained%2520ViTs%2520Yield%2520Versatile%2520Representations%2520For%2520Medical%2520Images%26entry.906535625%3DChristos%2520Matsoukas%2520and%2520Johan%2520Fredin%2520Haslum%2520and%2520Moein%2520Sorkhei%2520and%2520Magnus%2520S%25C3%25B6derberg%2520and%2520Kevin%2520Smith%26entry.1292438233%3D%2520%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520have%2520reigned%2520for%2520a%2520decade%2520as%2520the%2520de%250Afacto%2520approach%2520to%2520automated%2520medical%2520image%2520diagnosis%252C%2520pushing%2520the%250Astate-of-the-art%2520in%2520classification%252C%2520detection%2520and%2520segmentation%2520tasks.%2520Over%2520the%250Alast%2520years%252C%2520vision%2520transformers%2520%2528ViTs%2529%2520have%2520appeared%2520as%2520a%2520competitive%250Aalternative%2520to%2520CNNs%252C%2520yielding%2520impressive%2520levels%2520of%2520performance%2520in%2520the%2520natural%250Aimage%2520domain%252C%2520while%2520possessing%2520several%2520interesting%2520properties%2520that%2520could%2520prove%250Abeneficial%2520for%2520medical%2520imaging%2520tasks.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%2520benefits%2520and%250Adrawbacks%2520of%2520transformer-based%2520models%2520for%2520medical%2520image%2520classification.%2520We%250Aconduct%2520a%2520series%2520of%2520experiments%2520on%2520several%2520standard%25202D%2520medical%2520image%2520benchmark%250Adatasets%2520and%2520tasks.%2520Our%2520findings%2520show%2520that%252C%2520while%2520CNNs%2520perform%2520better%2520if%250Atrained%2520from%2520scratch%252C%2520off-the-shelf%2520vision%2520transformers%2520can%2520perform%2520on%2520par%2520with%250ACNNs%2520when%2520pretrained%2520on%2520ImageNet%252C%2520both%2520in%2520a%2520supervised%2520and%2520self-supervised%250Asetting%252C%2520rendering%2520them%2520as%2520a%2520viable%2520alternative%2520to%2520CNNs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.07034v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pretrained%20ViTs%20Yield%20Versatile%20Representations%20For%20Medical%20Images&entry.906535625=Christos%20Matsoukas%20and%20Johan%20Fredin%20Haslum%20and%20Moein%20Sorkhei%20and%20Magnus%20S%C3%B6derberg%20and%20Kevin%20Smith&entry.1292438233=%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20reigned%20for%20a%20decade%20as%20the%20de%0Afacto%20approach%20to%20automated%20medical%20image%20diagnosis%2C%20pushing%20the%0Astate-of-the-art%20in%20classification%2C%20detection%20and%20segmentation%20tasks.%20Over%20the%0Alast%20years%2C%20vision%20transformers%20%28ViTs%29%20have%20appeared%20as%20a%20competitive%0Aalternative%20to%20CNNs%2C%20yielding%20impressive%20levels%20of%20performance%20in%20the%20natural%0Aimage%20domain%2C%20while%20possessing%20several%20interesting%20properties%20that%20could%20prove%0Abeneficial%20for%20medical%20imaging%20tasks.%20In%20this%20work%2C%20we%20explore%20the%20benefits%20and%0Adrawbacks%20of%20transformer-based%20models%20for%20medical%20image%20classification.%20We%0Aconduct%20a%20series%20of%20experiments%20on%20several%20standard%202D%20medical%20image%20benchmark%0Adatasets%20and%20tasks.%20Our%20findings%20show%20that%2C%20while%20CNNs%20perform%20better%20if%0Atrained%20from%20scratch%2C%20off-the-shelf%20vision%20transformers%20can%20perform%20on%20par%20with%0ACNNs%20when%20pretrained%20on%20ImageNet%2C%20both%20in%20a%20supervised%20and%20self-supervised%0Asetting%2C%20rendering%20them%20as%20a%20viable%20alternative%20to%20CNNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.07034v3&entry.124074799=Read"},
{"title": "Low-Latency Task-Oriented Communications with Multi-Round, Multi-Task\n  Deep Learning", "author": "Yalin E. Sagduyu and Tugba Erpek and Aylin Yener and Sennur Ulukus", "abstract": "  In this paper, we address task-oriented (or goal-oriented) communications\nwhere an encoder at the transmitter learns compressed latent representations of\ndata, which are then transmitted over a wireless channel. At the receiver, a\ndecoder performs a machine learning task, specifically for classifying the\nreceived signals. The deep neural networks corresponding to the encoder-decoder\npair are jointly trained, taking both channel and data characteristics into\naccount. Our objective is to achieve high accuracy in completing the underlying\ntask while minimizing the number of channel uses determined by the encoder's\noutput size. To this end, we propose a multi-round, multi-task learning (MRMTL)\napproach for the dynamic update of channel uses in multi-round transmissions.\nThe transmitter incrementally sends an increasing number of encoded samples\nover the channel based on the feedback from the receiver, and the receiver\nutilizes the signals from a previous round to enhance the task performance,\nrather than only considering the latest transmission. This approach employs\nmulti-task learning to jointly optimize accuracy across varying number of\nchannel uses, treating each configuration as a distinct task. By evaluating the\nconfidence of the receiver in task decisions, MRMTL decides on whether to\nallocate additional channel uses in multiple rounds. We characterize both the\naccuracy and the delay (total number of channel uses) of MRMTL, demonstrating\nthat it achieves the accuracy close to that of conventional methods requiring\nlarge numbers of channel uses, but with reduced delay by incorporating signals\nfrom a prior round. We consider the CIFAR-10 dataset, convolutional neural\nnetwork architectures, and AWGN and Rayleigh channel models for performance\nevaluation. We show that MRMTL significantly improves the efficiency of\ntask-oriented communications, balancing accuracy and latency effectively.\n", "link": "http://arxiv.org/abs/2411.10385v1", "date": "2024-11-15", "relevancy": 2.085, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5347}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5294}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5077}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Latency%20Task-Oriented%20Communications%20with%20Multi-Round%2C%20Multi-Task%0A%20%20Deep%20Learning&body=Title%3A%20Low-Latency%20Task-Oriented%20Communications%20with%20Multi-Round%2C%20Multi-Task%0A%20%20Deep%20Learning%0AAuthor%3A%20Yalin%20E.%20Sagduyu%20and%20Tugba%20Erpek%20and%20Aylin%20Yener%20and%20Sennur%20Ulukus%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20address%20task-oriented%20%28or%20goal-oriented%29%20communications%0Awhere%20an%20encoder%20at%20the%20transmitter%20learns%20compressed%20latent%20representations%20of%0Adata%2C%20which%20are%20then%20transmitted%20over%20a%20wireless%20channel.%20At%20the%20receiver%2C%20a%0Adecoder%20performs%20a%20machine%20learning%20task%2C%20specifically%20for%20classifying%20the%0Areceived%20signals.%20The%20deep%20neural%20networks%20corresponding%20to%20the%20encoder-decoder%0Apair%20are%20jointly%20trained%2C%20taking%20both%20channel%20and%20data%20characteristics%20into%0Aaccount.%20Our%20objective%20is%20to%20achieve%20high%20accuracy%20in%20completing%20the%20underlying%0Atask%20while%20minimizing%20the%20number%20of%20channel%20uses%20determined%20by%20the%20encoder%27s%0Aoutput%20size.%20To%20this%20end%2C%20we%20propose%20a%20multi-round%2C%20multi-task%20learning%20%28MRMTL%29%0Aapproach%20for%20the%20dynamic%20update%20of%20channel%20uses%20in%20multi-round%20transmissions.%0AThe%20transmitter%20incrementally%20sends%20an%20increasing%20number%20of%20encoded%20samples%0Aover%20the%20channel%20based%20on%20the%20feedback%20from%20the%20receiver%2C%20and%20the%20receiver%0Autilizes%20the%20signals%20from%20a%20previous%20round%20to%20enhance%20the%20task%20performance%2C%0Arather%20than%20only%20considering%20the%20latest%20transmission.%20This%20approach%20employs%0Amulti-task%20learning%20to%20jointly%20optimize%20accuracy%20across%20varying%20number%20of%0Achannel%20uses%2C%20treating%20each%20configuration%20as%20a%20distinct%20task.%20By%20evaluating%20the%0Aconfidence%20of%20the%20receiver%20in%20task%20decisions%2C%20MRMTL%20decides%20on%20whether%20to%0Aallocate%20additional%20channel%20uses%20in%20multiple%20rounds.%20We%20characterize%20both%20the%0Aaccuracy%20and%20the%20delay%20%28total%20number%20of%20channel%20uses%29%20of%20MRMTL%2C%20demonstrating%0Athat%20it%20achieves%20the%20accuracy%20close%20to%20that%20of%20conventional%20methods%20requiring%0Alarge%20numbers%20of%20channel%20uses%2C%20but%20with%20reduced%20delay%20by%20incorporating%20signals%0Afrom%20a%20prior%20round.%20We%20consider%20the%20CIFAR-10%20dataset%2C%20convolutional%20neural%0Anetwork%20architectures%2C%20and%20AWGN%20and%20Rayleigh%20channel%20models%20for%20performance%0Aevaluation.%20We%20show%20that%20MRMTL%20significantly%20improves%20the%20efficiency%20of%0Atask-oriented%20communications%2C%20balancing%20accuracy%20and%20latency%20effectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10385v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Latency%2520Task-Oriented%2520Communications%2520with%2520Multi-Round%252C%2520Multi-Task%250A%2520%2520Deep%2520Learning%26entry.906535625%3DYalin%2520E.%2520Sagduyu%2520and%2520Tugba%2520Erpek%2520and%2520Aylin%2520Yener%2520and%2520Sennur%2520Ulukus%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520address%2520task-oriented%2520%2528or%2520goal-oriented%2529%2520communications%250Awhere%2520an%2520encoder%2520at%2520the%2520transmitter%2520learns%2520compressed%2520latent%2520representations%2520of%250Adata%252C%2520which%2520are%2520then%2520transmitted%2520over%2520a%2520wireless%2520channel.%2520At%2520the%2520receiver%252C%2520a%250Adecoder%2520performs%2520a%2520machine%2520learning%2520task%252C%2520specifically%2520for%2520classifying%2520the%250Areceived%2520signals.%2520The%2520deep%2520neural%2520networks%2520corresponding%2520to%2520the%2520encoder-decoder%250Apair%2520are%2520jointly%2520trained%252C%2520taking%2520both%2520channel%2520and%2520data%2520characteristics%2520into%250Aaccount.%2520Our%2520objective%2520is%2520to%2520achieve%2520high%2520accuracy%2520in%2520completing%2520the%2520underlying%250Atask%2520while%2520minimizing%2520the%2520number%2520of%2520channel%2520uses%2520determined%2520by%2520the%2520encoder%2527s%250Aoutput%2520size.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520multi-round%252C%2520multi-task%2520learning%2520%2528MRMTL%2529%250Aapproach%2520for%2520the%2520dynamic%2520update%2520of%2520channel%2520uses%2520in%2520multi-round%2520transmissions.%250AThe%2520transmitter%2520incrementally%2520sends%2520an%2520increasing%2520number%2520of%2520encoded%2520samples%250Aover%2520the%2520channel%2520based%2520on%2520the%2520feedback%2520from%2520the%2520receiver%252C%2520and%2520the%2520receiver%250Autilizes%2520the%2520signals%2520from%2520a%2520previous%2520round%2520to%2520enhance%2520the%2520task%2520performance%252C%250Arather%2520than%2520only%2520considering%2520the%2520latest%2520transmission.%2520This%2520approach%2520employs%250Amulti-task%2520learning%2520to%2520jointly%2520optimize%2520accuracy%2520across%2520varying%2520number%2520of%250Achannel%2520uses%252C%2520treating%2520each%2520configuration%2520as%2520a%2520distinct%2520task.%2520By%2520evaluating%2520the%250Aconfidence%2520of%2520the%2520receiver%2520in%2520task%2520decisions%252C%2520MRMTL%2520decides%2520on%2520whether%2520to%250Aallocate%2520additional%2520channel%2520uses%2520in%2520multiple%2520rounds.%2520We%2520characterize%2520both%2520the%250Aaccuracy%2520and%2520the%2520delay%2520%2528total%2520number%2520of%2520channel%2520uses%2529%2520of%2520MRMTL%252C%2520demonstrating%250Athat%2520it%2520achieves%2520the%2520accuracy%2520close%2520to%2520that%2520of%2520conventional%2520methods%2520requiring%250Alarge%2520numbers%2520of%2520channel%2520uses%252C%2520but%2520with%2520reduced%2520delay%2520by%2520incorporating%2520signals%250Afrom%2520a%2520prior%2520round.%2520We%2520consider%2520the%2520CIFAR-10%2520dataset%252C%2520convolutional%2520neural%250Anetwork%2520architectures%252C%2520and%2520AWGN%2520and%2520Rayleigh%2520channel%2520models%2520for%2520performance%250Aevaluation.%2520We%2520show%2520that%2520MRMTL%2520significantly%2520improves%2520the%2520efficiency%2520of%250Atask-oriented%2520communications%252C%2520balancing%2520accuracy%2520and%2520latency%2520effectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10385v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Latency%20Task-Oriented%20Communications%20with%20Multi-Round%2C%20Multi-Task%0A%20%20Deep%20Learning&entry.906535625=Yalin%20E.%20Sagduyu%20and%20Tugba%20Erpek%20and%20Aylin%20Yener%20and%20Sennur%20Ulukus&entry.1292438233=%20%20In%20this%20paper%2C%20we%20address%20task-oriented%20%28or%20goal-oriented%29%20communications%0Awhere%20an%20encoder%20at%20the%20transmitter%20learns%20compressed%20latent%20representations%20of%0Adata%2C%20which%20are%20then%20transmitted%20over%20a%20wireless%20channel.%20At%20the%20receiver%2C%20a%0Adecoder%20performs%20a%20machine%20learning%20task%2C%20specifically%20for%20classifying%20the%0Areceived%20signals.%20The%20deep%20neural%20networks%20corresponding%20to%20the%20encoder-decoder%0Apair%20are%20jointly%20trained%2C%20taking%20both%20channel%20and%20data%20characteristics%20into%0Aaccount.%20Our%20objective%20is%20to%20achieve%20high%20accuracy%20in%20completing%20the%20underlying%0Atask%20while%20minimizing%20the%20number%20of%20channel%20uses%20determined%20by%20the%20encoder%27s%0Aoutput%20size.%20To%20this%20end%2C%20we%20propose%20a%20multi-round%2C%20multi-task%20learning%20%28MRMTL%29%0Aapproach%20for%20the%20dynamic%20update%20of%20channel%20uses%20in%20multi-round%20transmissions.%0AThe%20transmitter%20incrementally%20sends%20an%20increasing%20number%20of%20encoded%20samples%0Aover%20the%20channel%20based%20on%20the%20feedback%20from%20the%20receiver%2C%20and%20the%20receiver%0Autilizes%20the%20signals%20from%20a%20previous%20round%20to%20enhance%20the%20task%20performance%2C%0Arather%20than%20only%20considering%20the%20latest%20transmission.%20This%20approach%20employs%0Amulti-task%20learning%20to%20jointly%20optimize%20accuracy%20across%20varying%20number%20of%0Achannel%20uses%2C%20treating%20each%20configuration%20as%20a%20distinct%20task.%20By%20evaluating%20the%0Aconfidence%20of%20the%20receiver%20in%20task%20decisions%2C%20MRMTL%20decides%20on%20whether%20to%0Aallocate%20additional%20channel%20uses%20in%20multiple%20rounds.%20We%20characterize%20both%20the%0Aaccuracy%20and%20the%20delay%20%28total%20number%20of%20channel%20uses%29%20of%20MRMTL%2C%20demonstrating%0Athat%20it%20achieves%20the%20accuracy%20close%20to%20that%20of%20conventional%20methods%20requiring%0Alarge%20numbers%20of%20channel%20uses%2C%20but%20with%20reduced%20delay%20by%20incorporating%20signals%0Afrom%20a%20prior%20round.%20We%20consider%20the%20CIFAR-10%20dataset%2C%20convolutional%20neural%0Anetwork%20architectures%2C%20and%20AWGN%20and%20Rayleigh%20channel%20models%20for%20performance%0Aevaluation.%20We%20show%20that%20MRMTL%20significantly%20improves%20the%20efficiency%20of%0Atask-oriented%20communications%2C%20balancing%20accuracy%20and%20latency%20effectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10385v1&entry.124074799=Read"},
{"title": "Moving Forward: A Review of Autonomous Driving Software and Hardware\n  Systems", "author": "Xu Wang and Mohammad Ali Maleki and Muhammad Waqar Azhar and Pedro Trancoso", "abstract": "  With their potential to significantly reduce traffic accidents, enhance road\nsafety, optimize traffic flow, and decrease congestion, autonomous driving\nsystems are a major focus of research and development in recent years. Beyond\nthese immediate benefits, they offer long-term advantages in promoting\nsustainable transportation by reducing emissions and fuel consumption.\nAchieving a high level of autonomy across diverse conditions requires a\ncomprehensive understanding of the environment. This is accomplished by\nprocessing data from sensors such as cameras, radars, and LiDARs through a\nsoftware stack that relies heavily on machine learning algorithms. These ML\nmodels demand significant computational resources and involve large-scale data\nmovement, presenting challenges for hardware to execute them efficiently and at\nhigh speed. In this survey, we first outline and highlight the key components\nof self-driving systems, covering input sensors, commonly used datasets,\nsimulation platforms, and the software architecture. We then explore the\nunderlying hardware platforms that support the execution of these software\nsystems. By presenting a comprehensive view of autonomous driving systems and\ntheir increasing demands, particularly for higher levels of autonomy, we\nanalyze the performance and efficiency of scaled-up off-the-shelf GPU/CPU-based\nsystems, emphasizing the challenges within the computational components.\nThrough examples showcasing the diverse computational and memory requirements\nin the software stack, we demonstrate how more specialized hardware and\nprocessing closer to memory can enable more efficient execution with lower\nlatency. Finally, based on current trends and future demands, we conclude by\nspeculating what a future hardware platform for autonomous driving might look\nlike.\n", "link": "http://arxiv.org/abs/2411.10291v1", "date": "2024-11-15", "relevancy": 2.0699, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.538}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5133}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Moving%20Forward%3A%20A%20Review%20of%20Autonomous%20Driving%20Software%20and%20Hardware%0A%20%20Systems&body=Title%3A%20Moving%20Forward%3A%20A%20Review%20of%20Autonomous%20Driving%20Software%20and%20Hardware%0A%20%20Systems%0AAuthor%3A%20Xu%20Wang%20and%20Mohammad%20Ali%20Maleki%20and%20Muhammad%20Waqar%20Azhar%20and%20Pedro%20Trancoso%0AAbstract%3A%20%20%20With%20their%20potential%20to%20significantly%20reduce%20traffic%20accidents%2C%20enhance%20road%0Asafety%2C%20optimize%20traffic%20flow%2C%20and%20decrease%20congestion%2C%20autonomous%20driving%0Asystems%20are%20a%20major%20focus%20of%20research%20and%20development%20in%20recent%20years.%20Beyond%0Athese%20immediate%20benefits%2C%20they%20offer%20long-term%20advantages%20in%20promoting%0Asustainable%20transportation%20by%20reducing%20emissions%20and%20fuel%20consumption.%0AAchieving%20a%20high%20level%20of%20autonomy%20across%20diverse%20conditions%20requires%20a%0Acomprehensive%20understanding%20of%20the%20environment.%20This%20is%20accomplished%20by%0Aprocessing%20data%20from%20sensors%20such%20as%20cameras%2C%20radars%2C%20and%20LiDARs%20through%20a%0Asoftware%20stack%20that%20relies%20heavily%20on%20machine%20learning%20algorithms.%20These%20ML%0Amodels%20demand%20significant%20computational%20resources%20and%20involve%20large-scale%20data%0Amovement%2C%20presenting%20challenges%20for%20hardware%20to%20execute%20them%20efficiently%20and%20at%0Ahigh%20speed.%20In%20this%20survey%2C%20we%20first%20outline%20and%20highlight%20the%20key%20components%0Aof%20self-driving%20systems%2C%20covering%20input%20sensors%2C%20commonly%20used%20datasets%2C%0Asimulation%20platforms%2C%20and%20the%20software%20architecture.%20We%20then%20explore%20the%0Aunderlying%20hardware%20platforms%20that%20support%20the%20execution%20of%20these%20software%0Asystems.%20By%20presenting%20a%20comprehensive%20view%20of%20autonomous%20driving%20systems%20and%0Atheir%20increasing%20demands%2C%20particularly%20for%20higher%20levels%20of%20autonomy%2C%20we%0Aanalyze%20the%20performance%20and%20efficiency%20of%20scaled-up%20off-the-shelf%20GPU/CPU-based%0Asystems%2C%20emphasizing%20the%20challenges%20within%20the%20computational%20components.%0AThrough%20examples%20showcasing%20the%20diverse%20computational%20and%20memory%20requirements%0Ain%20the%20software%20stack%2C%20we%20demonstrate%20how%20more%20specialized%20hardware%20and%0Aprocessing%20closer%20to%20memory%20can%20enable%20more%20efficient%20execution%20with%20lower%0Alatency.%20Finally%2C%20based%20on%20current%20trends%20and%20future%20demands%2C%20we%20conclude%20by%0Aspeculating%20what%20a%20future%20hardware%20platform%20for%20autonomous%20driving%20might%20look%0Alike.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10291v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoving%2520Forward%253A%2520A%2520Review%2520of%2520Autonomous%2520Driving%2520Software%2520and%2520Hardware%250A%2520%2520Systems%26entry.906535625%3DXu%2520Wang%2520and%2520Mohammad%2520Ali%2520Maleki%2520and%2520Muhammad%2520Waqar%2520Azhar%2520and%2520Pedro%2520Trancoso%26entry.1292438233%3D%2520%2520With%2520their%2520potential%2520to%2520significantly%2520reduce%2520traffic%2520accidents%252C%2520enhance%2520road%250Asafety%252C%2520optimize%2520traffic%2520flow%252C%2520and%2520decrease%2520congestion%252C%2520autonomous%2520driving%250Asystems%2520are%2520a%2520major%2520focus%2520of%2520research%2520and%2520development%2520in%2520recent%2520years.%2520Beyond%250Athese%2520immediate%2520benefits%252C%2520they%2520offer%2520long-term%2520advantages%2520in%2520promoting%250Asustainable%2520transportation%2520by%2520reducing%2520emissions%2520and%2520fuel%2520consumption.%250AAchieving%2520a%2520high%2520level%2520of%2520autonomy%2520across%2520diverse%2520conditions%2520requires%2520a%250Acomprehensive%2520understanding%2520of%2520the%2520environment.%2520This%2520is%2520accomplished%2520by%250Aprocessing%2520data%2520from%2520sensors%2520such%2520as%2520cameras%252C%2520radars%252C%2520and%2520LiDARs%2520through%2520a%250Asoftware%2520stack%2520that%2520relies%2520heavily%2520on%2520machine%2520learning%2520algorithms.%2520These%2520ML%250Amodels%2520demand%2520significant%2520computational%2520resources%2520and%2520involve%2520large-scale%2520data%250Amovement%252C%2520presenting%2520challenges%2520for%2520hardware%2520to%2520execute%2520them%2520efficiently%2520and%2520at%250Ahigh%2520speed.%2520In%2520this%2520survey%252C%2520we%2520first%2520outline%2520and%2520highlight%2520the%2520key%2520components%250Aof%2520self-driving%2520systems%252C%2520covering%2520input%2520sensors%252C%2520commonly%2520used%2520datasets%252C%250Asimulation%2520platforms%252C%2520and%2520the%2520software%2520architecture.%2520We%2520then%2520explore%2520the%250Aunderlying%2520hardware%2520platforms%2520that%2520support%2520the%2520execution%2520of%2520these%2520software%250Asystems.%2520By%2520presenting%2520a%2520comprehensive%2520view%2520of%2520autonomous%2520driving%2520systems%2520and%250Atheir%2520increasing%2520demands%252C%2520particularly%2520for%2520higher%2520levels%2520of%2520autonomy%252C%2520we%250Aanalyze%2520the%2520performance%2520and%2520efficiency%2520of%2520scaled-up%2520off-the-shelf%2520GPU/CPU-based%250Asystems%252C%2520emphasizing%2520the%2520challenges%2520within%2520the%2520computational%2520components.%250AThrough%2520examples%2520showcasing%2520the%2520diverse%2520computational%2520and%2520memory%2520requirements%250Ain%2520the%2520software%2520stack%252C%2520we%2520demonstrate%2520how%2520more%2520specialized%2520hardware%2520and%250Aprocessing%2520closer%2520to%2520memory%2520can%2520enable%2520more%2520efficient%2520execution%2520with%2520lower%250Alatency.%2520Finally%252C%2520based%2520on%2520current%2520trends%2520and%2520future%2520demands%252C%2520we%2520conclude%2520by%250Aspeculating%2520what%2520a%2520future%2520hardware%2520platform%2520for%2520autonomous%2520driving%2520might%2520look%250Alike.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10291v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Moving%20Forward%3A%20A%20Review%20of%20Autonomous%20Driving%20Software%20and%20Hardware%0A%20%20Systems&entry.906535625=Xu%20Wang%20and%20Mohammad%20Ali%20Maleki%20and%20Muhammad%20Waqar%20Azhar%20and%20Pedro%20Trancoso&entry.1292438233=%20%20With%20their%20potential%20to%20significantly%20reduce%20traffic%20accidents%2C%20enhance%20road%0Asafety%2C%20optimize%20traffic%20flow%2C%20and%20decrease%20congestion%2C%20autonomous%20driving%0Asystems%20are%20a%20major%20focus%20of%20research%20and%20development%20in%20recent%20years.%20Beyond%0Athese%20immediate%20benefits%2C%20they%20offer%20long-term%20advantages%20in%20promoting%0Asustainable%20transportation%20by%20reducing%20emissions%20and%20fuel%20consumption.%0AAchieving%20a%20high%20level%20of%20autonomy%20across%20diverse%20conditions%20requires%20a%0Acomprehensive%20understanding%20of%20the%20environment.%20This%20is%20accomplished%20by%0Aprocessing%20data%20from%20sensors%20such%20as%20cameras%2C%20radars%2C%20and%20LiDARs%20through%20a%0Asoftware%20stack%20that%20relies%20heavily%20on%20machine%20learning%20algorithms.%20These%20ML%0Amodels%20demand%20significant%20computational%20resources%20and%20involve%20large-scale%20data%0Amovement%2C%20presenting%20challenges%20for%20hardware%20to%20execute%20them%20efficiently%20and%20at%0Ahigh%20speed.%20In%20this%20survey%2C%20we%20first%20outline%20and%20highlight%20the%20key%20components%0Aof%20self-driving%20systems%2C%20covering%20input%20sensors%2C%20commonly%20used%20datasets%2C%0Asimulation%20platforms%2C%20and%20the%20software%20architecture.%20We%20then%20explore%20the%0Aunderlying%20hardware%20platforms%20that%20support%20the%20execution%20of%20these%20software%0Asystems.%20By%20presenting%20a%20comprehensive%20view%20of%20autonomous%20driving%20systems%20and%0Atheir%20increasing%20demands%2C%20particularly%20for%20higher%20levels%20of%20autonomy%2C%20we%0Aanalyze%20the%20performance%20and%20efficiency%20of%20scaled-up%20off-the-shelf%20GPU/CPU-based%0Asystems%2C%20emphasizing%20the%20challenges%20within%20the%20computational%20components.%0AThrough%20examples%20showcasing%20the%20diverse%20computational%20and%20memory%20requirements%0Ain%20the%20software%20stack%2C%20we%20demonstrate%20how%20more%20specialized%20hardware%20and%0Aprocessing%20closer%20to%20memory%20can%20enable%20more%20efficient%20execution%20with%20lower%0Alatency.%20Finally%2C%20based%20on%20current%20trends%20and%20future%20demands%2C%20we%20conclude%20by%0Aspeculating%20what%20a%20future%20hardware%20platform%20for%20autonomous%20driving%20might%20look%0Alike.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10291v1&entry.124074799=Read"},
{"title": "ScribbleVS: Scribble-Supervised Medical Image Segmentation via Dynamic\n  Competitive Pseudo Label Selection", "author": "Tao Wang and Xinlin Zhang and Yuanbin Chen and Yuanbo Zhou and Longxuan Zhao and Tao Tan and Tong Tong", "abstract": "  In clinical medicine, precise image segmentation can provide substantial\nsupport to clinicians. However, achieving such precision often requires a large\namount of finely annotated data, which can be costly. Scribble annotation\npresents a more efficient alternative, boosting labeling efficiency. However,\nutilizing such minimal supervision for medical image segmentation training,\nespecially with scribble annotations, poses significant challenges. To address\nthese challenges, we introduce ScribbleVS, a novel framework that leverages\nscribble annotations. We introduce a Regional Pseudo Labels Diffusion Module to\nexpand the scope of supervision and reduce the impact of noise present in\npseudo labels. Additionally, we propose a Dynamic Competitive Selection module\nfor enhanced refinement in selecting pseudo labels. Experiments conducted on\nthe ACDC and MSCMRseg datasets have demonstrated promising results, achieving\nperformance levels that even exceed those of fully supervised methodologies.\nThe codes of this study are available at\nhttps://github.com/ortonwang/ScribbleVS.\n", "link": "http://arxiv.org/abs/2411.10237v1", "date": "2024-11-15", "relevancy": 2.0675, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5533}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5097}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ScribbleVS%3A%20Scribble-Supervised%20Medical%20Image%20Segmentation%20via%20Dynamic%0A%20%20Competitive%20Pseudo%20Label%20Selection&body=Title%3A%20ScribbleVS%3A%20Scribble-Supervised%20Medical%20Image%20Segmentation%20via%20Dynamic%0A%20%20Competitive%20Pseudo%20Label%20Selection%0AAuthor%3A%20Tao%20Wang%20and%20Xinlin%20Zhang%20and%20Yuanbin%20Chen%20and%20Yuanbo%20Zhou%20and%20Longxuan%20Zhao%20and%20Tao%20Tan%20and%20Tong%20Tong%0AAbstract%3A%20%20%20In%20clinical%20medicine%2C%20precise%20image%20segmentation%20can%20provide%20substantial%0Asupport%20to%20clinicians.%20However%2C%20achieving%20such%20precision%20often%20requires%20a%20large%0Aamount%20of%20finely%20annotated%20data%2C%20which%20can%20be%20costly.%20Scribble%20annotation%0Apresents%20a%20more%20efficient%20alternative%2C%20boosting%20labeling%20efficiency.%20However%2C%0Autilizing%20such%20minimal%20supervision%20for%20medical%20image%20segmentation%20training%2C%0Aespecially%20with%20scribble%20annotations%2C%20poses%20significant%20challenges.%20To%20address%0Athese%20challenges%2C%20we%20introduce%20ScribbleVS%2C%20a%20novel%20framework%20that%20leverages%0Ascribble%20annotations.%20We%20introduce%20a%20Regional%20Pseudo%20Labels%20Diffusion%20Module%20to%0Aexpand%20the%20scope%20of%20supervision%20and%20reduce%20the%20impact%20of%20noise%20present%20in%0Apseudo%20labels.%20Additionally%2C%20we%20propose%20a%20Dynamic%20Competitive%20Selection%20module%0Afor%20enhanced%20refinement%20in%20selecting%20pseudo%20labels.%20Experiments%20conducted%20on%0Athe%20ACDC%20and%20MSCMRseg%20datasets%20have%20demonstrated%20promising%20results%2C%20achieving%0Aperformance%20levels%20that%20even%20exceed%20those%20of%20fully%20supervised%20methodologies.%0AThe%20codes%20of%20this%20study%20are%20available%20at%0Ahttps%3A//github.com/ortonwang/ScribbleVS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10237v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScribbleVS%253A%2520Scribble-Supervised%2520Medical%2520Image%2520Segmentation%2520via%2520Dynamic%250A%2520%2520Competitive%2520Pseudo%2520Label%2520Selection%26entry.906535625%3DTao%2520Wang%2520and%2520Xinlin%2520Zhang%2520and%2520Yuanbin%2520Chen%2520and%2520Yuanbo%2520Zhou%2520and%2520Longxuan%2520Zhao%2520and%2520Tao%2520Tan%2520and%2520Tong%2520Tong%26entry.1292438233%3D%2520%2520In%2520clinical%2520medicine%252C%2520precise%2520image%2520segmentation%2520can%2520provide%2520substantial%250Asupport%2520to%2520clinicians.%2520However%252C%2520achieving%2520such%2520precision%2520often%2520requires%2520a%2520large%250Aamount%2520of%2520finely%2520annotated%2520data%252C%2520which%2520can%2520be%2520costly.%2520Scribble%2520annotation%250Apresents%2520a%2520more%2520efficient%2520alternative%252C%2520boosting%2520labeling%2520efficiency.%2520However%252C%250Autilizing%2520such%2520minimal%2520supervision%2520for%2520medical%2520image%2520segmentation%2520training%252C%250Aespecially%2520with%2520scribble%2520annotations%252C%2520poses%2520significant%2520challenges.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520introduce%2520ScribbleVS%252C%2520a%2520novel%2520framework%2520that%2520leverages%250Ascribble%2520annotations.%2520We%2520introduce%2520a%2520Regional%2520Pseudo%2520Labels%2520Diffusion%2520Module%2520to%250Aexpand%2520the%2520scope%2520of%2520supervision%2520and%2520reduce%2520the%2520impact%2520of%2520noise%2520present%2520in%250Apseudo%2520labels.%2520Additionally%252C%2520we%2520propose%2520a%2520Dynamic%2520Competitive%2520Selection%2520module%250Afor%2520enhanced%2520refinement%2520in%2520selecting%2520pseudo%2520labels.%2520Experiments%2520conducted%2520on%250Athe%2520ACDC%2520and%2520MSCMRseg%2520datasets%2520have%2520demonstrated%2520promising%2520results%252C%2520achieving%250Aperformance%2520levels%2520that%2520even%2520exceed%2520those%2520of%2520fully%2520supervised%2520methodologies.%250AThe%2520codes%2520of%2520this%2520study%2520are%2520available%2520at%250Ahttps%253A//github.com/ortonwang/ScribbleVS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10237v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ScribbleVS%3A%20Scribble-Supervised%20Medical%20Image%20Segmentation%20via%20Dynamic%0A%20%20Competitive%20Pseudo%20Label%20Selection&entry.906535625=Tao%20Wang%20and%20Xinlin%20Zhang%20and%20Yuanbin%20Chen%20and%20Yuanbo%20Zhou%20and%20Longxuan%20Zhao%20and%20Tao%20Tan%20and%20Tong%20Tong&entry.1292438233=%20%20In%20clinical%20medicine%2C%20precise%20image%20segmentation%20can%20provide%20substantial%0Asupport%20to%20clinicians.%20However%2C%20achieving%20such%20precision%20often%20requires%20a%20large%0Aamount%20of%20finely%20annotated%20data%2C%20which%20can%20be%20costly.%20Scribble%20annotation%0Apresents%20a%20more%20efficient%20alternative%2C%20boosting%20labeling%20efficiency.%20However%2C%0Autilizing%20such%20minimal%20supervision%20for%20medical%20image%20segmentation%20training%2C%0Aespecially%20with%20scribble%20annotations%2C%20poses%20significant%20challenges.%20To%20address%0Athese%20challenges%2C%20we%20introduce%20ScribbleVS%2C%20a%20novel%20framework%20that%20leverages%0Ascribble%20annotations.%20We%20introduce%20a%20Regional%20Pseudo%20Labels%20Diffusion%20Module%20to%0Aexpand%20the%20scope%20of%20supervision%20and%20reduce%20the%20impact%20of%20noise%20present%20in%0Apseudo%20labels.%20Additionally%2C%20we%20propose%20a%20Dynamic%20Competitive%20Selection%20module%0Afor%20enhanced%20refinement%20in%20selecting%20pseudo%20labels.%20Experiments%20conducted%20on%0Athe%20ACDC%20and%20MSCMRseg%20datasets%20have%20demonstrated%20promising%20results%2C%20achieving%0Aperformance%20levels%20that%20even%20exceed%20those%20of%20fully%20supervised%20methodologies.%0AThe%20codes%20of%20this%20study%20are%20available%20at%0Ahttps%3A//github.com/ortonwang/ScribbleVS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10237v1&entry.124074799=Read"},
{"title": "Melanoma Detection with Uncertainty Quantification", "author": "SangHyuk Kim and Edward Gaibor and Brian Matejek and Daniel Haehn", "abstract": "  Early detection of melanoma is crucial for improving survival rates. Current\ndetection tools often utilize data-driven machine learning methods but often\noverlook the full integration of multiple datasets. We combine publicly\navailable datasets to enhance data diversity, allowing numerous experiments to\ntrain and evaluate various classifiers. We then calibrate them to minimize\nmisdiagnoses by incorporating uncertainty quantification. Our experiments on\nbenchmark datasets show accuracies of up to 93.2% before and 97.8% after\napplying uncertainty-based rejection, leading to a reduction in misdiagnoses by\nover 40.5%. Our code and data are publicly available, and a web-based interface\nfor quick melanoma detection of user-supplied images is also provided.\n", "link": "http://arxiv.org/abs/2411.10322v1", "date": "2024-11-15", "relevancy": 2.0558, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5678}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5296}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4768}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Melanoma%20Detection%20with%20Uncertainty%20Quantification&body=Title%3A%20Melanoma%20Detection%20with%20Uncertainty%20Quantification%0AAuthor%3A%20SangHyuk%20Kim%20and%20Edward%20Gaibor%20and%20Brian%20Matejek%20and%20Daniel%20Haehn%0AAbstract%3A%20%20%20Early%20detection%20of%20melanoma%20is%20crucial%20for%20improving%20survival%20rates.%20Current%0Adetection%20tools%20often%20utilize%20data-driven%20machine%20learning%20methods%20but%20often%0Aoverlook%20the%20full%20integration%20of%20multiple%20datasets.%20We%20combine%20publicly%0Aavailable%20datasets%20to%20enhance%20data%20diversity%2C%20allowing%20numerous%20experiments%20to%0Atrain%20and%20evaluate%20various%20classifiers.%20We%20then%20calibrate%20them%20to%20minimize%0Amisdiagnoses%20by%20incorporating%20uncertainty%20quantification.%20Our%20experiments%20on%0Abenchmark%20datasets%20show%20accuracies%20of%20up%20to%2093.2%25%20before%20and%2097.8%25%20after%0Aapplying%20uncertainty-based%20rejection%2C%20leading%20to%20a%20reduction%20in%20misdiagnoses%20by%0Aover%2040.5%25.%20Our%20code%20and%20data%20are%20publicly%20available%2C%20and%20a%20web-based%20interface%0Afor%20quick%20melanoma%20detection%20of%20user-supplied%20images%20is%20also%20provided.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10322v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMelanoma%2520Detection%2520with%2520Uncertainty%2520Quantification%26entry.906535625%3DSangHyuk%2520Kim%2520and%2520Edward%2520Gaibor%2520and%2520Brian%2520Matejek%2520and%2520Daniel%2520Haehn%26entry.1292438233%3D%2520%2520Early%2520detection%2520of%2520melanoma%2520is%2520crucial%2520for%2520improving%2520survival%2520rates.%2520Current%250Adetection%2520tools%2520often%2520utilize%2520data-driven%2520machine%2520learning%2520methods%2520but%2520often%250Aoverlook%2520the%2520full%2520integration%2520of%2520multiple%2520datasets.%2520We%2520combine%2520publicly%250Aavailable%2520datasets%2520to%2520enhance%2520data%2520diversity%252C%2520allowing%2520numerous%2520experiments%2520to%250Atrain%2520and%2520evaluate%2520various%2520classifiers.%2520We%2520then%2520calibrate%2520them%2520to%2520minimize%250Amisdiagnoses%2520by%2520incorporating%2520uncertainty%2520quantification.%2520Our%2520experiments%2520on%250Abenchmark%2520datasets%2520show%2520accuracies%2520of%2520up%2520to%252093.2%2525%2520before%2520and%252097.8%2525%2520after%250Aapplying%2520uncertainty-based%2520rejection%252C%2520leading%2520to%2520a%2520reduction%2520in%2520misdiagnoses%2520by%250Aover%252040.5%2525.%2520Our%2520code%2520and%2520data%2520are%2520publicly%2520available%252C%2520and%2520a%2520web-based%2520interface%250Afor%2520quick%2520melanoma%2520detection%2520of%2520user-supplied%2520images%2520is%2520also%2520provided.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10322v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Melanoma%20Detection%20with%20Uncertainty%20Quantification&entry.906535625=SangHyuk%20Kim%20and%20Edward%20Gaibor%20and%20Brian%20Matejek%20and%20Daniel%20Haehn&entry.1292438233=%20%20Early%20detection%20of%20melanoma%20is%20crucial%20for%20improving%20survival%20rates.%20Current%0Adetection%20tools%20often%20utilize%20data-driven%20machine%20learning%20methods%20but%20often%0Aoverlook%20the%20full%20integration%20of%20multiple%20datasets.%20We%20combine%20publicly%0Aavailable%20datasets%20to%20enhance%20data%20diversity%2C%20allowing%20numerous%20experiments%20to%0Atrain%20and%20evaluate%20various%20classifiers.%20We%20then%20calibrate%20them%20to%20minimize%0Amisdiagnoses%20by%20incorporating%20uncertainty%20quantification.%20Our%20experiments%20on%0Abenchmark%20datasets%20show%20accuracies%20of%20up%20to%2093.2%25%20before%20and%2097.8%25%20after%0Aapplying%20uncertainty-based%20rejection%2C%20leading%20to%20a%20reduction%20in%20misdiagnoses%20by%0Aover%2040.5%25.%20Our%20code%20and%20data%20are%20publicly%20available%2C%20and%20a%20web-based%20interface%0Afor%20quick%20melanoma%20detection%20of%20user-supplied%20images%20is%20also%20provided.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10322v1&entry.124074799=Read"},
{"title": "CE-SSL: Computation-Efficient Semi-Supervised Learning for ECG-based\n  Cardiovascular Diseases Detection", "author": "Rushuang Zhou and Lei Clifton and Zijun Liu and Kannie W. Y. Chan and David A. Clifton and Yuan-Ting Zhang and Yining Dong", "abstract": "  The label scarcity problem is the main challenge that hinders the wide\napplication of deep learning systems in automatic cardiovascular diseases\n(CVDs) detection using electrocardiography (ECG). Tuning pre-trained models\nalleviates this problem by transferring knowledge learned from large datasets\nto downstream small datasets. However, bottlenecks in computational efficiency\nand detection performance limit its clinical applications. It is difficult to\nimprove the detection performance without significantly sacrificing the\ncomputational efficiency during model training. Here, we propose a\ncomputation-efficient semi-supervised learning paradigm (CE-SSL) for robust and\ncomputation-efficient CVDs detection using ECG. It enables a robust adaptation\nof pre-trained models on downstream datasets with limited supervision and high\ncomputational efficiency. First, a random-deactivation technique is developed\nto achieve robust and fast low-rank adaptation of pre-trained weights.\nSubsequently, we propose a one-shot rank allocation module to determine the\noptimal ranks for the update matrices of the pre-trained weights. Finally, a\nlightweight semi-supervised learning pipeline is introduced to enhance model\nperformance by leveraging labeled and unlabeled data with high computational\nefficiency. Extensive experiments on four downstream datasets demonstrate that\nCE-SSL not only outperforms the state-of-the-art methods in multi-label CVDs\ndetection but also consumes fewer GPU footprints, training time, and parameter\nstorage space. As such, this paradigm provides an effective solution for\nachieving high computational efficiency and robust detection performance in the\nclinical applications of pre-trained models under limited supervision. Code and\nSupplementary Materials are available at https://github.com/KAZABANA/CE-SSL\n", "link": "http://arxiv.org/abs/2406.14377v2", "date": "2024-11-15", "relevancy": 2.0484, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5268}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5208}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CE-SSL%3A%20Computation-Efficient%20Semi-Supervised%20Learning%20for%20ECG-based%0A%20%20Cardiovascular%20Diseases%20Detection&body=Title%3A%20CE-SSL%3A%20Computation-Efficient%20Semi-Supervised%20Learning%20for%20ECG-based%0A%20%20Cardiovascular%20Diseases%20Detection%0AAuthor%3A%20Rushuang%20Zhou%20and%20Lei%20Clifton%20and%20Zijun%20Liu%20and%20Kannie%20W.%20Y.%20Chan%20and%20David%20A.%20Clifton%20and%20Yuan-Ting%20Zhang%20and%20Yining%20Dong%0AAbstract%3A%20%20%20The%20label%20scarcity%20problem%20is%20the%20main%20challenge%20that%20hinders%20the%20wide%0Aapplication%20of%20deep%20learning%20systems%20in%20automatic%20cardiovascular%20diseases%0A%28CVDs%29%20detection%20using%20electrocardiography%20%28ECG%29.%20Tuning%20pre-trained%20models%0Aalleviates%20this%20problem%20by%20transferring%20knowledge%20learned%20from%20large%20datasets%0Ato%20downstream%20small%20datasets.%20However%2C%20bottlenecks%20in%20computational%20efficiency%0Aand%20detection%20performance%20limit%20its%20clinical%20applications.%20It%20is%20difficult%20to%0Aimprove%20the%20detection%20performance%20without%20significantly%20sacrificing%20the%0Acomputational%20efficiency%20during%20model%20training.%20Here%2C%20we%20propose%20a%0Acomputation-efficient%20semi-supervised%20learning%20paradigm%20%28CE-SSL%29%20for%20robust%20and%0Acomputation-efficient%20CVDs%20detection%20using%20ECG.%20It%20enables%20a%20robust%20adaptation%0Aof%20pre-trained%20models%20on%20downstream%20datasets%20with%20limited%20supervision%20and%20high%0Acomputational%20efficiency.%20First%2C%20a%20random-deactivation%20technique%20is%20developed%0Ato%20achieve%20robust%20and%20fast%20low-rank%20adaptation%20of%20pre-trained%20weights.%0ASubsequently%2C%20we%20propose%20a%20one-shot%20rank%20allocation%20module%20to%20determine%20the%0Aoptimal%20ranks%20for%20the%20update%20matrices%20of%20the%20pre-trained%20weights.%20Finally%2C%20a%0Alightweight%20semi-supervised%20learning%20pipeline%20is%20introduced%20to%20enhance%20model%0Aperformance%20by%20leveraging%20labeled%20and%20unlabeled%20data%20with%20high%20computational%0Aefficiency.%20Extensive%20experiments%20on%20four%20downstream%20datasets%20demonstrate%20that%0ACE-SSL%20not%20only%20outperforms%20the%20state-of-the-art%20methods%20in%20multi-label%20CVDs%0Adetection%20but%20also%20consumes%20fewer%20GPU%20footprints%2C%20training%20time%2C%20and%20parameter%0Astorage%20space.%20As%20such%2C%20this%20paradigm%20provides%20an%20effective%20solution%20for%0Aachieving%20high%20computational%20efficiency%20and%20robust%20detection%20performance%20in%20the%0Aclinical%20applications%20of%20pre-trained%20models%20under%20limited%20supervision.%20Code%20and%0ASupplementary%20Materials%20are%20available%20at%20https%3A//github.com/KAZABANA/CE-SSL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14377v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCE-SSL%253A%2520Computation-Efficient%2520Semi-Supervised%2520Learning%2520for%2520ECG-based%250A%2520%2520Cardiovascular%2520Diseases%2520Detection%26entry.906535625%3DRushuang%2520Zhou%2520and%2520Lei%2520Clifton%2520and%2520Zijun%2520Liu%2520and%2520Kannie%2520W.%2520Y.%2520Chan%2520and%2520David%2520A.%2520Clifton%2520and%2520Yuan-Ting%2520Zhang%2520and%2520Yining%2520Dong%26entry.1292438233%3D%2520%2520The%2520label%2520scarcity%2520problem%2520is%2520the%2520main%2520challenge%2520that%2520hinders%2520the%2520wide%250Aapplication%2520of%2520deep%2520learning%2520systems%2520in%2520automatic%2520cardiovascular%2520diseases%250A%2528CVDs%2529%2520detection%2520using%2520electrocardiography%2520%2528ECG%2529.%2520Tuning%2520pre-trained%2520models%250Aalleviates%2520this%2520problem%2520by%2520transferring%2520knowledge%2520learned%2520from%2520large%2520datasets%250Ato%2520downstream%2520small%2520datasets.%2520However%252C%2520bottlenecks%2520in%2520computational%2520efficiency%250Aand%2520detection%2520performance%2520limit%2520its%2520clinical%2520applications.%2520It%2520is%2520difficult%2520to%250Aimprove%2520the%2520detection%2520performance%2520without%2520significantly%2520sacrificing%2520the%250Acomputational%2520efficiency%2520during%2520model%2520training.%2520Here%252C%2520we%2520propose%2520a%250Acomputation-efficient%2520semi-supervised%2520learning%2520paradigm%2520%2528CE-SSL%2529%2520for%2520robust%2520and%250Acomputation-efficient%2520CVDs%2520detection%2520using%2520ECG.%2520It%2520enables%2520a%2520robust%2520adaptation%250Aof%2520pre-trained%2520models%2520on%2520downstream%2520datasets%2520with%2520limited%2520supervision%2520and%2520high%250Acomputational%2520efficiency.%2520First%252C%2520a%2520random-deactivation%2520technique%2520is%2520developed%250Ato%2520achieve%2520robust%2520and%2520fast%2520low-rank%2520adaptation%2520of%2520pre-trained%2520weights.%250ASubsequently%252C%2520we%2520propose%2520a%2520one-shot%2520rank%2520allocation%2520module%2520to%2520determine%2520the%250Aoptimal%2520ranks%2520for%2520the%2520update%2520matrices%2520of%2520the%2520pre-trained%2520weights.%2520Finally%252C%2520a%250Alightweight%2520semi-supervised%2520learning%2520pipeline%2520is%2520introduced%2520to%2520enhance%2520model%250Aperformance%2520by%2520leveraging%2520labeled%2520and%2520unlabeled%2520data%2520with%2520high%2520computational%250Aefficiency.%2520Extensive%2520experiments%2520on%2520four%2520downstream%2520datasets%2520demonstrate%2520that%250ACE-SSL%2520not%2520only%2520outperforms%2520the%2520state-of-the-art%2520methods%2520in%2520multi-label%2520CVDs%250Adetection%2520but%2520also%2520consumes%2520fewer%2520GPU%2520footprints%252C%2520training%2520time%252C%2520and%2520parameter%250Astorage%2520space.%2520As%2520such%252C%2520this%2520paradigm%2520provides%2520an%2520effective%2520solution%2520for%250Aachieving%2520high%2520computational%2520efficiency%2520and%2520robust%2520detection%2520performance%2520in%2520the%250Aclinical%2520applications%2520of%2520pre-trained%2520models%2520under%2520limited%2520supervision.%2520Code%2520and%250ASupplementary%2520Materials%2520are%2520available%2520at%2520https%253A//github.com/KAZABANA/CE-SSL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14377v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CE-SSL%3A%20Computation-Efficient%20Semi-Supervised%20Learning%20for%20ECG-based%0A%20%20Cardiovascular%20Diseases%20Detection&entry.906535625=Rushuang%20Zhou%20and%20Lei%20Clifton%20and%20Zijun%20Liu%20and%20Kannie%20W.%20Y.%20Chan%20and%20David%20A.%20Clifton%20and%20Yuan-Ting%20Zhang%20and%20Yining%20Dong&entry.1292438233=%20%20The%20label%20scarcity%20problem%20is%20the%20main%20challenge%20that%20hinders%20the%20wide%0Aapplication%20of%20deep%20learning%20systems%20in%20automatic%20cardiovascular%20diseases%0A%28CVDs%29%20detection%20using%20electrocardiography%20%28ECG%29.%20Tuning%20pre-trained%20models%0Aalleviates%20this%20problem%20by%20transferring%20knowledge%20learned%20from%20large%20datasets%0Ato%20downstream%20small%20datasets.%20However%2C%20bottlenecks%20in%20computational%20efficiency%0Aand%20detection%20performance%20limit%20its%20clinical%20applications.%20It%20is%20difficult%20to%0Aimprove%20the%20detection%20performance%20without%20significantly%20sacrificing%20the%0Acomputational%20efficiency%20during%20model%20training.%20Here%2C%20we%20propose%20a%0Acomputation-efficient%20semi-supervised%20learning%20paradigm%20%28CE-SSL%29%20for%20robust%20and%0Acomputation-efficient%20CVDs%20detection%20using%20ECG.%20It%20enables%20a%20robust%20adaptation%0Aof%20pre-trained%20models%20on%20downstream%20datasets%20with%20limited%20supervision%20and%20high%0Acomputational%20efficiency.%20First%2C%20a%20random-deactivation%20technique%20is%20developed%0Ato%20achieve%20robust%20and%20fast%20low-rank%20adaptation%20of%20pre-trained%20weights.%0ASubsequently%2C%20we%20propose%20a%20one-shot%20rank%20allocation%20module%20to%20determine%20the%0Aoptimal%20ranks%20for%20the%20update%20matrices%20of%20the%20pre-trained%20weights.%20Finally%2C%20a%0Alightweight%20semi-supervised%20learning%20pipeline%20is%20introduced%20to%20enhance%20model%0Aperformance%20by%20leveraging%20labeled%20and%20unlabeled%20data%20with%20high%20computational%0Aefficiency.%20Extensive%20experiments%20on%20four%20downstream%20datasets%20demonstrate%20that%0ACE-SSL%20not%20only%20outperforms%20the%20state-of-the-art%20methods%20in%20multi-label%20CVDs%0Adetection%20but%20also%20consumes%20fewer%20GPU%20footprints%2C%20training%20time%2C%20and%20parameter%0Astorage%20space.%20As%20such%2C%20this%20paradigm%20provides%20an%20effective%20solution%20for%0Aachieving%20high%20computational%20efficiency%20and%20robust%20detection%20performance%20in%20the%0Aclinical%20applications%20of%20pre-trained%20models%20under%20limited%20supervision.%20Code%20and%0ASupplementary%20Materials%20are%20available%20at%20https%3A//github.com/KAZABANA/CE-SSL%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14377v2&entry.124074799=Read"},
{"title": "Deep Learning for Micro-Scale Crack Detection on Imbalanced Datasets\n  Using Key Point Localization", "author": "Fatahlla Moreh and Yusuf Hasan and Bilal Zahid Hussain and Mohammad Ammar and Sven Tomforde", "abstract": "  Internal crack detection has been a subject of focus in structural health\nmonitoring. By focusing on crack detection in structural datasets, it is\ndemonstrated that deep learning (DL) methods can effectively analyze seismic\nwave fields interacting with micro-scale cracks, which are beyond the\nresolution of conventional visual inspection. This work explores a novel\napplication of DL-based key point detection technique, where cracks are\nlocalized by predicting the coordinates of four key points that define a\nbounding region of the crack. The study not only opens new research directions\nfor non-visual applications but also effectively mitigates the impact of\nimbalanced data which poses a challenge for previous DL models, as it can be\nbiased toward predicting the majority class (non-crack regions). Popular DL\ntechniques, such as the Inception blocks, are used and investigated. The model\nshows an overall reduction in loss when applied to micro-scale crack detection\nand is reflected in the lower average deviation between the location of actual\nand predicted cracks, with an average Intersection over Union (IoU) being 0.511\nfor all micro cracks (greater than 0.00 micrometers) and 0.631 for larger micro\ncracks (greater than 4 micrometers).\n", "link": "http://arxiv.org/abs/2411.10389v1", "date": "2024-11-15", "relevancy": 2.0452, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5639}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4777}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4722}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20for%20Micro-Scale%20Crack%20Detection%20on%20Imbalanced%20Datasets%0A%20%20Using%20Key%20Point%20Localization&body=Title%3A%20Deep%20Learning%20for%20Micro-Scale%20Crack%20Detection%20on%20Imbalanced%20Datasets%0A%20%20Using%20Key%20Point%20Localization%0AAuthor%3A%20Fatahlla%20Moreh%20and%20Yusuf%20Hasan%20and%20Bilal%20Zahid%20Hussain%20and%20Mohammad%20Ammar%20and%20Sven%20Tomforde%0AAbstract%3A%20%20%20Internal%20crack%20detection%20has%20been%20a%20subject%20of%20focus%20in%20structural%20health%0Amonitoring.%20By%20focusing%20on%20crack%20detection%20in%20structural%20datasets%2C%20it%20is%0Ademonstrated%20that%20deep%20learning%20%28DL%29%20methods%20can%20effectively%20analyze%20seismic%0Awave%20fields%20interacting%20with%20micro-scale%20cracks%2C%20which%20are%20beyond%20the%0Aresolution%20of%20conventional%20visual%20inspection.%20This%20work%20explores%20a%20novel%0Aapplication%20of%20DL-based%20key%20point%20detection%20technique%2C%20where%20cracks%20are%0Alocalized%20by%20predicting%20the%20coordinates%20of%20four%20key%20points%20that%20define%20a%0Abounding%20region%20of%20the%20crack.%20The%20study%20not%20only%20opens%20new%20research%20directions%0Afor%20non-visual%20applications%20but%20also%20effectively%20mitigates%20the%20impact%20of%0Aimbalanced%20data%20which%20poses%20a%20challenge%20for%20previous%20DL%20models%2C%20as%20it%20can%20be%0Abiased%20toward%20predicting%20the%20majority%20class%20%28non-crack%20regions%29.%20Popular%20DL%0Atechniques%2C%20such%20as%20the%20Inception%20blocks%2C%20are%20used%20and%20investigated.%20The%20model%0Ashows%20an%20overall%20reduction%20in%20loss%20when%20applied%20to%20micro-scale%20crack%20detection%0Aand%20is%20reflected%20in%20the%20lower%20average%20deviation%20between%20the%20location%20of%20actual%0Aand%20predicted%20cracks%2C%20with%20an%20average%20Intersection%20over%20Union%20%28IoU%29%20being%200.511%0Afor%20all%20micro%20cracks%20%28greater%20than%200.00%20micrometers%29%20and%200.631%20for%20larger%20micro%0Acracks%20%28greater%20than%204%20micrometers%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10389v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520for%2520Micro-Scale%2520Crack%2520Detection%2520on%2520Imbalanced%2520Datasets%250A%2520%2520Using%2520Key%2520Point%2520Localization%26entry.906535625%3DFatahlla%2520Moreh%2520and%2520Yusuf%2520Hasan%2520and%2520Bilal%2520Zahid%2520Hussain%2520and%2520Mohammad%2520Ammar%2520and%2520Sven%2520Tomforde%26entry.1292438233%3D%2520%2520Internal%2520crack%2520detection%2520has%2520been%2520a%2520subject%2520of%2520focus%2520in%2520structural%2520health%250Amonitoring.%2520By%2520focusing%2520on%2520crack%2520detection%2520in%2520structural%2520datasets%252C%2520it%2520is%250Ademonstrated%2520that%2520deep%2520learning%2520%2528DL%2529%2520methods%2520can%2520effectively%2520analyze%2520seismic%250Awave%2520fields%2520interacting%2520with%2520micro-scale%2520cracks%252C%2520which%2520are%2520beyond%2520the%250Aresolution%2520of%2520conventional%2520visual%2520inspection.%2520This%2520work%2520explores%2520a%2520novel%250Aapplication%2520of%2520DL-based%2520key%2520point%2520detection%2520technique%252C%2520where%2520cracks%2520are%250Alocalized%2520by%2520predicting%2520the%2520coordinates%2520of%2520four%2520key%2520points%2520that%2520define%2520a%250Abounding%2520region%2520of%2520the%2520crack.%2520The%2520study%2520not%2520only%2520opens%2520new%2520research%2520directions%250Afor%2520non-visual%2520applications%2520but%2520also%2520effectively%2520mitigates%2520the%2520impact%2520of%250Aimbalanced%2520data%2520which%2520poses%2520a%2520challenge%2520for%2520previous%2520DL%2520models%252C%2520as%2520it%2520can%2520be%250Abiased%2520toward%2520predicting%2520the%2520majority%2520class%2520%2528non-crack%2520regions%2529.%2520Popular%2520DL%250Atechniques%252C%2520such%2520as%2520the%2520Inception%2520blocks%252C%2520are%2520used%2520and%2520investigated.%2520The%2520model%250Ashows%2520an%2520overall%2520reduction%2520in%2520loss%2520when%2520applied%2520to%2520micro-scale%2520crack%2520detection%250Aand%2520is%2520reflected%2520in%2520the%2520lower%2520average%2520deviation%2520between%2520the%2520location%2520of%2520actual%250Aand%2520predicted%2520cracks%252C%2520with%2520an%2520average%2520Intersection%2520over%2520Union%2520%2528IoU%2529%2520being%25200.511%250Afor%2520all%2520micro%2520cracks%2520%2528greater%2520than%25200.00%2520micrometers%2529%2520and%25200.631%2520for%2520larger%2520micro%250Acracks%2520%2528greater%2520than%25204%2520micrometers%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10389v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20for%20Micro-Scale%20Crack%20Detection%20on%20Imbalanced%20Datasets%0A%20%20Using%20Key%20Point%20Localization&entry.906535625=Fatahlla%20Moreh%20and%20Yusuf%20Hasan%20and%20Bilal%20Zahid%20Hussain%20and%20Mohammad%20Ammar%20and%20Sven%20Tomforde&entry.1292438233=%20%20Internal%20crack%20detection%20has%20been%20a%20subject%20of%20focus%20in%20structural%20health%0Amonitoring.%20By%20focusing%20on%20crack%20detection%20in%20structural%20datasets%2C%20it%20is%0Ademonstrated%20that%20deep%20learning%20%28DL%29%20methods%20can%20effectively%20analyze%20seismic%0Awave%20fields%20interacting%20with%20micro-scale%20cracks%2C%20which%20are%20beyond%20the%0Aresolution%20of%20conventional%20visual%20inspection.%20This%20work%20explores%20a%20novel%0Aapplication%20of%20DL-based%20key%20point%20detection%20technique%2C%20where%20cracks%20are%0Alocalized%20by%20predicting%20the%20coordinates%20of%20four%20key%20points%20that%20define%20a%0Abounding%20region%20of%20the%20crack.%20The%20study%20not%20only%20opens%20new%20research%20directions%0Afor%20non-visual%20applications%20but%20also%20effectively%20mitigates%20the%20impact%20of%0Aimbalanced%20data%20which%20poses%20a%20challenge%20for%20previous%20DL%20models%2C%20as%20it%20can%20be%0Abiased%20toward%20predicting%20the%20majority%20class%20%28non-crack%20regions%29.%20Popular%20DL%0Atechniques%2C%20such%20as%20the%20Inception%20blocks%2C%20are%20used%20and%20investigated.%20The%20model%0Ashows%20an%20overall%20reduction%20in%20loss%20when%20applied%20to%20micro-scale%20crack%20detection%0Aand%20is%20reflected%20in%20the%20lower%20average%20deviation%20between%20the%20location%20of%20actual%0Aand%20predicted%20cracks%2C%20with%20an%20average%20Intersection%20over%20Union%20%28IoU%29%20being%200.511%0Afor%20all%20micro%20cracks%20%28greater%20than%200.00%20micrometers%29%20and%200.631%20for%20larger%20micro%0Acracks%20%28greater%20than%204%20micrometers%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10389v1&entry.124074799=Read"},
{"title": "The Dawn of GUI Agent: A Preliminary Case Study with Claude 3.5 Computer\n  Use", "author": "Siyuan Hu and Mingyu Ouyang and Difei Gao and Mike Zheng Shou", "abstract": "  The recently released model, Claude 3.5 Computer Use, stands out as the first\nfrontier AI model to offer computer use in public beta as a graphical user\ninterface (GUI) agent. As an early beta, its capability in the real-world\ncomplex environment remains unknown. In this case study to explore Claude 3.5\nComputer Use, we curate and organize a collection of carefully designed tasks\nspanning a variety of domains and software. Observations from these cases\ndemonstrate Claude 3.5 Computer Use's unprecedented ability in end-to-end\nlanguage to desktop actions. Along with this study, we provide an\nout-of-the-box agent framework for deploying API-based GUI automation models\nwith easy implementation. Our case studies aim to showcase a groundwork of\ncapabilities and limitations of Claude 3.5 Computer Use with detailed analyses\nand bring to the fore questions about planning, action, and critic, which must\nbe considered for future improvement. We hope this preliminary exploration will\ninspire future research into the GUI agent community. All the test cases in the\npaper can be tried through the project:\nhttps://github.com/showlab/computer_use_ootb.\n", "link": "http://arxiv.org/abs/2411.10323v1", "date": "2024-11-15", "relevancy": 2.0389, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.548}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5158}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Dawn%20of%20GUI%20Agent%3A%20A%20Preliminary%20Case%20Study%20with%20Claude%203.5%20Computer%0A%20%20Use&body=Title%3A%20The%20Dawn%20of%20GUI%20Agent%3A%20A%20Preliminary%20Case%20Study%20with%20Claude%203.5%20Computer%0A%20%20Use%0AAuthor%3A%20Siyuan%20Hu%20and%20Mingyu%20Ouyang%20and%20Difei%20Gao%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20%20%20The%20recently%20released%20model%2C%20Claude%203.5%20Computer%20Use%2C%20stands%20out%20as%20the%20first%0Afrontier%20AI%20model%20to%20offer%20computer%20use%20in%20public%20beta%20as%20a%20graphical%20user%0Ainterface%20%28GUI%29%20agent.%20As%20an%20early%20beta%2C%20its%20capability%20in%20the%20real-world%0Acomplex%20environment%20remains%20unknown.%20In%20this%20case%20study%20to%20explore%20Claude%203.5%0AComputer%20Use%2C%20we%20curate%20and%20organize%20a%20collection%20of%20carefully%20designed%20tasks%0Aspanning%20a%20variety%20of%20domains%20and%20software.%20Observations%20from%20these%20cases%0Ademonstrate%20Claude%203.5%20Computer%20Use%27s%20unprecedented%20ability%20in%20end-to-end%0Alanguage%20to%20desktop%20actions.%20Along%20with%20this%20study%2C%20we%20provide%20an%0Aout-of-the-box%20agent%20framework%20for%20deploying%20API-based%20GUI%20automation%20models%0Awith%20easy%20implementation.%20Our%20case%20studies%20aim%20to%20showcase%20a%20groundwork%20of%0Acapabilities%20and%20limitations%20of%20Claude%203.5%20Computer%20Use%20with%20detailed%20analyses%0Aand%20bring%20to%20the%20fore%20questions%20about%20planning%2C%20action%2C%20and%20critic%2C%20which%20must%0Abe%20considered%20for%20future%20improvement.%20We%20hope%20this%20preliminary%20exploration%20will%0Ainspire%20future%20research%20into%20the%20GUI%20agent%20community.%20All%20the%20test%20cases%20in%20the%0Apaper%20can%20be%20tried%20through%20the%20project%3A%0Ahttps%3A//github.com/showlab/computer_use_ootb.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10323v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Dawn%2520of%2520GUI%2520Agent%253A%2520A%2520Preliminary%2520Case%2520Study%2520with%2520Claude%25203.5%2520Computer%250A%2520%2520Use%26entry.906535625%3DSiyuan%2520Hu%2520and%2520Mingyu%2520Ouyang%2520and%2520Difei%2520Gao%2520and%2520Mike%2520Zheng%2520Shou%26entry.1292438233%3D%2520%2520The%2520recently%2520released%2520model%252C%2520Claude%25203.5%2520Computer%2520Use%252C%2520stands%2520out%2520as%2520the%2520first%250Afrontier%2520AI%2520model%2520to%2520offer%2520computer%2520use%2520in%2520public%2520beta%2520as%2520a%2520graphical%2520user%250Ainterface%2520%2528GUI%2529%2520agent.%2520As%2520an%2520early%2520beta%252C%2520its%2520capability%2520in%2520the%2520real-world%250Acomplex%2520environment%2520remains%2520unknown.%2520In%2520this%2520case%2520study%2520to%2520explore%2520Claude%25203.5%250AComputer%2520Use%252C%2520we%2520curate%2520and%2520organize%2520a%2520collection%2520of%2520carefully%2520designed%2520tasks%250Aspanning%2520a%2520variety%2520of%2520domains%2520and%2520software.%2520Observations%2520from%2520these%2520cases%250Ademonstrate%2520Claude%25203.5%2520Computer%2520Use%2527s%2520unprecedented%2520ability%2520in%2520end-to-end%250Alanguage%2520to%2520desktop%2520actions.%2520Along%2520with%2520this%2520study%252C%2520we%2520provide%2520an%250Aout-of-the-box%2520agent%2520framework%2520for%2520deploying%2520API-based%2520GUI%2520automation%2520models%250Awith%2520easy%2520implementation.%2520Our%2520case%2520studies%2520aim%2520to%2520showcase%2520a%2520groundwork%2520of%250Acapabilities%2520and%2520limitations%2520of%2520Claude%25203.5%2520Computer%2520Use%2520with%2520detailed%2520analyses%250Aand%2520bring%2520to%2520the%2520fore%2520questions%2520about%2520planning%252C%2520action%252C%2520and%2520critic%252C%2520which%2520must%250Abe%2520considered%2520for%2520future%2520improvement.%2520We%2520hope%2520this%2520preliminary%2520exploration%2520will%250Ainspire%2520future%2520research%2520into%2520the%2520GUI%2520agent%2520community.%2520All%2520the%2520test%2520cases%2520in%2520the%250Apaper%2520can%2520be%2520tried%2520through%2520the%2520project%253A%250Ahttps%253A//github.com/showlab/computer_use_ootb.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10323v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Dawn%20of%20GUI%20Agent%3A%20A%20Preliminary%20Case%20Study%20with%20Claude%203.5%20Computer%0A%20%20Use&entry.906535625=Siyuan%20Hu%20and%20Mingyu%20Ouyang%20and%20Difei%20Gao%20and%20Mike%20Zheng%20Shou&entry.1292438233=%20%20The%20recently%20released%20model%2C%20Claude%203.5%20Computer%20Use%2C%20stands%20out%20as%20the%20first%0Afrontier%20AI%20model%20to%20offer%20computer%20use%20in%20public%20beta%20as%20a%20graphical%20user%0Ainterface%20%28GUI%29%20agent.%20As%20an%20early%20beta%2C%20its%20capability%20in%20the%20real-world%0Acomplex%20environment%20remains%20unknown.%20In%20this%20case%20study%20to%20explore%20Claude%203.5%0AComputer%20Use%2C%20we%20curate%20and%20organize%20a%20collection%20of%20carefully%20designed%20tasks%0Aspanning%20a%20variety%20of%20domains%20and%20software.%20Observations%20from%20these%20cases%0Ademonstrate%20Claude%203.5%20Computer%20Use%27s%20unprecedented%20ability%20in%20end-to-end%0Alanguage%20to%20desktop%20actions.%20Along%20with%20this%20study%2C%20we%20provide%20an%0Aout-of-the-box%20agent%20framework%20for%20deploying%20API-based%20GUI%20automation%20models%0Awith%20easy%20implementation.%20Our%20case%20studies%20aim%20to%20showcase%20a%20groundwork%20of%0Acapabilities%20and%20limitations%20of%20Claude%203.5%20Computer%20Use%20with%20detailed%20analyses%0Aand%20bring%20to%20the%20fore%20questions%20about%20planning%2C%20action%2C%20and%20critic%2C%20which%20must%0Abe%20considered%20for%20future%20improvement.%20We%20hope%20this%20preliminary%20exploration%20will%0Ainspire%20future%20research%20into%20the%20GUI%20agent%20community.%20All%20the%20test%20cases%20in%20the%0Apaper%20can%20be%20tried%20through%20the%20project%3A%0Ahttps%3A//github.com/showlab/computer_use_ootb.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10323v1&entry.124074799=Read"},
{"title": "Generation of synthetic gait data: application to multiple sclerosis\n  patients' gait patterns", "author": "Klervi Le Gall and Lise Bellanger and David Laplaud", "abstract": "  Multiple sclerosis (MS) is the leading cause of severe non-traumatic\ndisability in young adults and its incidence is increasing worldwide. The\nvariability of gait impairment in MS necessitates the development of a\nnon-invasive, sensitive, and cost-effective tool for quantitative gait\nevaluation. The eGait movement sensor, designed to characterize human gait\nthrough unit quaternion time series (QTS) representing hip rotations, is a\npromising approach. However, the small sample sizes typical of clinical studies\npose challenges for the stability of gait data analysis tools. To address these\nchallenges, this article presents two key scientific contributions. First, a\ncomprehensive framework is proposed for transforming QTS data into a form that\npreserves the essential geometric properties of gait while enabling the use of\nany tabular synthetic data generation method. Second, a synthetic data\ngeneration method is introduced, based on nearest neighbors weighting, which\nproduces high-fidelity synthetic QTS data suitable for small datasets and\nprivate data environments. The effectiveness of the proposed method, is\ndemonstrated through its application to MS gait data, showing very good\nfidelity and respect of the initial geometry of the data. Thanks to this work,\nwe are able to produce synthetic data sets and work on the stability of\nclustering methods.\n", "link": "http://arxiv.org/abs/2411.10377v1", "date": "2024-11-15", "relevancy": 2.0363, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5256}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5111}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generation%20of%20synthetic%20gait%20data%3A%20application%20to%20multiple%20sclerosis%0A%20%20patients%27%20gait%20patterns&body=Title%3A%20Generation%20of%20synthetic%20gait%20data%3A%20application%20to%20multiple%20sclerosis%0A%20%20patients%27%20gait%20patterns%0AAuthor%3A%20Klervi%20Le%20Gall%20and%20Lise%20Bellanger%20and%20David%20Laplaud%0AAbstract%3A%20%20%20Multiple%20sclerosis%20%28MS%29%20is%20the%20leading%20cause%20of%20severe%20non-traumatic%0Adisability%20in%20young%20adults%20and%20its%20incidence%20is%20increasing%20worldwide.%20The%0Avariability%20of%20gait%20impairment%20in%20MS%20necessitates%20the%20development%20of%20a%0Anon-invasive%2C%20sensitive%2C%20and%20cost-effective%20tool%20for%20quantitative%20gait%0Aevaluation.%20The%20eGait%20movement%20sensor%2C%20designed%20to%20characterize%20human%20gait%0Athrough%20unit%20quaternion%20time%20series%20%28QTS%29%20representing%20hip%20rotations%2C%20is%20a%0Apromising%20approach.%20However%2C%20the%20small%20sample%20sizes%20typical%20of%20clinical%20studies%0Apose%20challenges%20for%20the%20stability%20of%20gait%20data%20analysis%20tools.%20To%20address%20these%0Achallenges%2C%20this%20article%20presents%20two%20key%20scientific%20contributions.%20First%2C%20a%0Acomprehensive%20framework%20is%20proposed%20for%20transforming%20QTS%20data%20into%20a%20form%20that%0Apreserves%20the%20essential%20geometric%20properties%20of%20gait%20while%20enabling%20the%20use%20of%0Aany%20tabular%20synthetic%20data%20generation%20method.%20Second%2C%20a%20synthetic%20data%0Ageneration%20method%20is%20introduced%2C%20based%20on%20nearest%20neighbors%20weighting%2C%20which%0Aproduces%20high-fidelity%20synthetic%20QTS%20data%20suitable%20for%20small%20datasets%20and%0Aprivate%20data%20environments.%20The%20effectiveness%20of%20the%20proposed%20method%2C%20is%0Ademonstrated%20through%20its%20application%20to%20MS%20gait%20data%2C%20showing%20very%20good%0Afidelity%20and%20respect%20of%20the%20initial%20geometry%20of%20the%20data.%20Thanks%20to%20this%20work%2C%0Awe%20are%20able%20to%20produce%20synthetic%20data%20sets%20and%20work%20on%20the%20stability%20of%0Aclustering%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10377v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneration%2520of%2520synthetic%2520gait%2520data%253A%2520application%2520to%2520multiple%2520sclerosis%250A%2520%2520patients%2527%2520gait%2520patterns%26entry.906535625%3DKlervi%2520Le%2520Gall%2520and%2520Lise%2520Bellanger%2520and%2520David%2520Laplaud%26entry.1292438233%3D%2520%2520Multiple%2520sclerosis%2520%2528MS%2529%2520is%2520the%2520leading%2520cause%2520of%2520severe%2520non-traumatic%250Adisability%2520in%2520young%2520adults%2520and%2520its%2520incidence%2520is%2520increasing%2520worldwide.%2520The%250Avariability%2520of%2520gait%2520impairment%2520in%2520MS%2520necessitates%2520the%2520development%2520of%2520a%250Anon-invasive%252C%2520sensitive%252C%2520and%2520cost-effective%2520tool%2520for%2520quantitative%2520gait%250Aevaluation.%2520The%2520eGait%2520movement%2520sensor%252C%2520designed%2520to%2520characterize%2520human%2520gait%250Athrough%2520unit%2520quaternion%2520time%2520series%2520%2528QTS%2529%2520representing%2520hip%2520rotations%252C%2520is%2520a%250Apromising%2520approach.%2520However%252C%2520the%2520small%2520sample%2520sizes%2520typical%2520of%2520clinical%2520studies%250Apose%2520challenges%2520for%2520the%2520stability%2520of%2520gait%2520data%2520analysis%2520tools.%2520To%2520address%2520these%250Achallenges%252C%2520this%2520article%2520presents%2520two%2520key%2520scientific%2520contributions.%2520First%252C%2520a%250Acomprehensive%2520framework%2520is%2520proposed%2520for%2520transforming%2520QTS%2520data%2520into%2520a%2520form%2520that%250Apreserves%2520the%2520essential%2520geometric%2520properties%2520of%2520gait%2520while%2520enabling%2520the%2520use%2520of%250Aany%2520tabular%2520synthetic%2520data%2520generation%2520method.%2520Second%252C%2520a%2520synthetic%2520data%250Ageneration%2520method%2520is%2520introduced%252C%2520based%2520on%2520nearest%2520neighbors%2520weighting%252C%2520which%250Aproduces%2520high-fidelity%2520synthetic%2520QTS%2520data%2520suitable%2520for%2520small%2520datasets%2520and%250Aprivate%2520data%2520environments.%2520The%2520effectiveness%2520of%2520the%2520proposed%2520method%252C%2520is%250Ademonstrated%2520through%2520its%2520application%2520to%2520MS%2520gait%2520data%252C%2520showing%2520very%2520good%250Afidelity%2520and%2520respect%2520of%2520the%2520initial%2520geometry%2520of%2520the%2520data.%2520Thanks%2520to%2520this%2520work%252C%250Awe%2520are%2520able%2520to%2520produce%2520synthetic%2520data%2520sets%2520and%2520work%2520on%2520the%2520stability%2520of%250Aclustering%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10377v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generation%20of%20synthetic%20gait%20data%3A%20application%20to%20multiple%20sclerosis%0A%20%20patients%27%20gait%20patterns&entry.906535625=Klervi%20Le%20Gall%20and%20Lise%20Bellanger%20and%20David%20Laplaud&entry.1292438233=%20%20Multiple%20sclerosis%20%28MS%29%20is%20the%20leading%20cause%20of%20severe%20non-traumatic%0Adisability%20in%20young%20adults%20and%20its%20incidence%20is%20increasing%20worldwide.%20The%0Avariability%20of%20gait%20impairment%20in%20MS%20necessitates%20the%20development%20of%20a%0Anon-invasive%2C%20sensitive%2C%20and%20cost-effective%20tool%20for%20quantitative%20gait%0Aevaluation.%20The%20eGait%20movement%20sensor%2C%20designed%20to%20characterize%20human%20gait%0Athrough%20unit%20quaternion%20time%20series%20%28QTS%29%20representing%20hip%20rotations%2C%20is%20a%0Apromising%20approach.%20However%2C%20the%20small%20sample%20sizes%20typical%20of%20clinical%20studies%0Apose%20challenges%20for%20the%20stability%20of%20gait%20data%20analysis%20tools.%20To%20address%20these%0Achallenges%2C%20this%20article%20presents%20two%20key%20scientific%20contributions.%20First%2C%20a%0Acomprehensive%20framework%20is%20proposed%20for%20transforming%20QTS%20data%20into%20a%20form%20that%0Apreserves%20the%20essential%20geometric%20properties%20of%20gait%20while%20enabling%20the%20use%20of%0Aany%20tabular%20synthetic%20data%20generation%20method.%20Second%2C%20a%20synthetic%20data%0Ageneration%20method%20is%20introduced%2C%20based%20on%20nearest%20neighbors%20weighting%2C%20which%0Aproduces%20high-fidelity%20synthetic%20QTS%20data%20suitable%20for%20small%20datasets%20and%0Aprivate%20data%20environments.%20The%20effectiveness%20of%20the%20proposed%20method%2C%20is%0Ademonstrated%20through%20its%20application%20to%20MS%20gait%20data%2C%20showing%20very%20good%0Afidelity%20and%20respect%20of%20the%20initial%20geometry%20of%20the%20data.%20Thanks%20to%20this%20work%2C%0Awe%20are%20able%20to%20produce%20synthetic%20data%20sets%20and%20work%20on%20the%20stability%20of%0Aclustering%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10377v1&entry.124074799=Read"},
{"title": "Safe Text-to-Image Generation: Simply Sanitize the Prompt Embedding", "author": "Huming Qiu and Guanxu Chen and Mi Zhang and Min Yang", "abstract": "  In recent years, text-to-image (T2I) generation models have made significant\nprogress in generating high-quality images that align with text descriptions.\nHowever, these models also face the risk of unsafe generation, potentially\nproducing harmful content that violates usage policies, such as explicit\nmaterial. Existing safe generation methods typically focus on suppressing\ninappropriate content by erasing undesired concepts from visual\nrepresentations, while neglecting to sanitize the textual representation.\nAlthough these methods help mitigate the risk of misuse to certain extent,\ntheir robustness remains insufficient when dealing with adversarial attacks.\n  Given that semantic consistency between input text and output image is a\nfundamental requirement for T2I models, we identify that textual\nrepresentations (i.e., prompt embeddings) are likely the primary source of\nunsafe generation. To this end, we propose a vision-agnostic safe generation\nframework, Embedding Sanitizer (ES), which focuses on erasing inappropriate\nconcepts from prompt embeddings and uses the sanitized embeddings to guide the\nmodel for safe generation. ES is applied to the output of the text encoder as a\nplug-and-play module, enabling seamless integration with different T2I models\nas well as other safeguards. In addition, ES's unique scoring mechanism assigns\na score to each token in the prompt to indicate its potential harmfulness, and\ndynamically adjusts the sanitization intensity to balance defensive performance\nand generation quality. Through extensive evaluation on five prompt benchmarks,\nour approach achieves state-of-the-art robustness by sanitizing the source\n(prompt embedding) of unsafe generation compared to nine baseline methods. It\nsignificantly outperforms existing safeguards in terms of interpretability and\ncontrollability while maintaining generation quality.\n", "link": "http://arxiv.org/abs/2411.10329v1", "date": "2024-11-15", "relevancy": 2.0216, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5258}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4921}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safe%20Text-to-Image%20Generation%3A%20Simply%20Sanitize%20the%20Prompt%20Embedding&body=Title%3A%20Safe%20Text-to-Image%20Generation%3A%20Simply%20Sanitize%20the%20Prompt%20Embedding%0AAuthor%3A%20Huming%20Qiu%20and%20Guanxu%20Chen%20and%20Mi%20Zhang%20and%20Min%20Yang%0AAbstract%3A%20%20%20In%20recent%20years%2C%20text-to-image%20%28T2I%29%20generation%20models%20have%20made%20significant%0Aprogress%20in%20generating%20high-quality%20images%20that%20align%20with%20text%20descriptions.%0AHowever%2C%20these%20models%20also%20face%20the%20risk%20of%20unsafe%20generation%2C%20potentially%0Aproducing%20harmful%20content%20that%20violates%20usage%20policies%2C%20such%20as%20explicit%0Amaterial.%20Existing%20safe%20generation%20methods%20typically%20focus%20on%20suppressing%0Ainappropriate%20content%20by%20erasing%20undesired%20concepts%20from%20visual%0Arepresentations%2C%20while%20neglecting%20to%20sanitize%20the%20textual%20representation.%0AAlthough%20these%20methods%20help%20mitigate%20the%20risk%20of%20misuse%20to%20certain%20extent%2C%0Atheir%20robustness%20remains%20insufficient%20when%20dealing%20with%20adversarial%20attacks.%0A%20%20Given%20that%20semantic%20consistency%20between%20input%20text%20and%20output%20image%20is%20a%0Afundamental%20requirement%20for%20T2I%20models%2C%20we%20identify%20that%20textual%0Arepresentations%20%28i.e.%2C%20prompt%20embeddings%29%20are%20likely%20the%20primary%20source%20of%0Aunsafe%20generation.%20To%20this%20end%2C%20we%20propose%20a%20vision-agnostic%20safe%20generation%0Aframework%2C%20Embedding%20Sanitizer%20%28ES%29%2C%20which%20focuses%20on%20erasing%20inappropriate%0Aconcepts%20from%20prompt%20embeddings%20and%20uses%20the%20sanitized%20embeddings%20to%20guide%20the%0Amodel%20for%20safe%20generation.%20ES%20is%20applied%20to%20the%20output%20of%20the%20text%20encoder%20as%20a%0Aplug-and-play%20module%2C%20enabling%20seamless%20integration%20with%20different%20T2I%20models%0Aas%20well%20as%20other%20safeguards.%20In%20addition%2C%20ES%27s%20unique%20scoring%20mechanism%20assigns%0Aa%20score%20to%20each%20token%20in%20the%20prompt%20to%20indicate%20its%20potential%20harmfulness%2C%20and%0Adynamically%20adjusts%20the%20sanitization%20intensity%20to%20balance%20defensive%20performance%0Aand%20generation%20quality.%20Through%20extensive%20evaluation%20on%20five%20prompt%20benchmarks%2C%0Aour%20approach%20achieves%20state-of-the-art%20robustness%20by%20sanitizing%20the%20source%0A%28prompt%20embedding%29%20of%20unsafe%20generation%20compared%20to%20nine%20baseline%20methods.%20It%0Asignificantly%20outperforms%20existing%20safeguards%20in%20terms%20of%20interpretability%20and%0Acontrollability%20while%20maintaining%20generation%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10329v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafe%2520Text-to-Image%2520Generation%253A%2520Simply%2520Sanitize%2520the%2520Prompt%2520Embedding%26entry.906535625%3DHuming%2520Qiu%2520and%2520Guanxu%2520Chen%2520and%2520Mi%2520Zhang%2520and%2520Min%2520Yang%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520text-to-image%2520%2528T2I%2529%2520generation%2520models%2520have%2520made%2520significant%250Aprogress%2520in%2520generating%2520high-quality%2520images%2520that%2520align%2520with%2520text%2520descriptions.%250AHowever%252C%2520these%2520models%2520also%2520face%2520the%2520risk%2520of%2520unsafe%2520generation%252C%2520potentially%250Aproducing%2520harmful%2520content%2520that%2520violates%2520usage%2520policies%252C%2520such%2520as%2520explicit%250Amaterial.%2520Existing%2520safe%2520generation%2520methods%2520typically%2520focus%2520on%2520suppressing%250Ainappropriate%2520content%2520by%2520erasing%2520undesired%2520concepts%2520from%2520visual%250Arepresentations%252C%2520while%2520neglecting%2520to%2520sanitize%2520the%2520textual%2520representation.%250AAlthough%2520these%2520methods%2520help%2520mitigate%2520the%2520risk%2520of%2520misuse%2520to%2520certain%2520extent%252C%250Atheir%2520robustness%2520remains%2520insufficient%2520when%2520dealing%2520with%2520adversarial%2520attacks.%250A%2520%2520Given%2520that%2520semantic%2520consistency%2520between%2520input%2520text%2520and%2520output%2520image%2520is%2520a%250Afundamental%2520requirement%2520for%2520T2I%2520models%252C%2520we%2520identify%2520that%2520textual%250Arepresentations%2520%2528i.e.%252C%2520prompt%2520embeddings%2529%2520are%2520likely%2520the%2520primary%2520source%2520of%250Aunsafe%2520generation.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520vision-agnostic%2520safe%2520generation%250Aframework%252C%2520Embedding%2520Sanitizer%2520%2528ES%2529%252C%2520which%2520focuses%2520on%2520erasing%2520inappropriate%250Aconcepts%2520from%2520prompt%2520embeddings%2520and%2520uses%2520the%2520sanitized%2520embeddings%2520to%2520guide%2520the%250Amodel%2520for%2520safe%2520generation.%2520ES%2520is%2520applied%2520to%2520the%2520output%2520of%2520the%2520text%2520encoder%2520as%2520a%250Aplug-and-play%2520module%252C%2520enabling%2520seamless%2520integration%2520with%2520different%2520T2I%2520models%250Aas%2520well%2520as%2520other%2520safeguards.%2520In%2520addition%252C%2520ES%2527s%2520unique%2520scoring%2520mechanism%2520assigns%250Aa%2520score%2520to%2520each%2520token%2520in%2520the%2520prompt%2520to%2520indicate%2520its%2520potential%2520harmfulness%252C%2520and%250Adynamically%2520adjusts%2520the%2520sanitization%2520intensity%2520to%2520balance%2520defensive%2520performance%250Aand%2520generation%2520quality.%2520Through%2520extensive%2520evaluation%2520on%2520five%2520prompt%2520benchmarks%252C%250Aour%2520approach%2520achieves%2520state-of-the-art%2520robustness%2520by%2520sanitizing%2520the%2520source%250A%2528prompt%2520embedding%2529%2520of%2520unsafe%2520generation%2520compared%2520to%2520nine%2520baseline%2520methods.%2520It%250Asignificantly%2520outperforms%2520existing%2520safeguards%2520in%2520terms%2520of%2520interpretability%2520and%250Acontrollability%2520while%2520maintaining%2520generation%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10329v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20Text-to-Image%20Generation%3A%20Simply%20Sanitize%20the%20Prompt%20Embedding&entry.906535625=Huming%20Qiu%20and%20Guanxu%20Chen%20and%20Mi%20Zhang%20and%20Min%20Yang&entry.1292438233=%20%20In%20recent%20years%2C%20text-to-image%20%28T2I%29%20generation%20models%20have%20made%20significant%0Aprogress%20in%20generating%20high-quality%20images%20that%20align%20with%20text%20descriptions.%0AHowever%2C%20these%20models%20also%20face%20the%20risk%20of%20unsafe%20generation%2C%20potentially%0Aproducing%20harmful%20content%20that%20violates%20usage%20policies%2C%20such%20as%20explicit%0Amaterial.%20Existing%20safe%20generation%20methods%20typically%20focus%20on%20suppressing%0Ainappropriate%20content%20by%20erasing%20undesired%20concepts%20from%20visual%0Arepresentations%2C%20while%20neglecting%20to%20sanitize%20the%20textual%20representation.%0AAlthough%20these%20methods%20help%20mitigate%20the%20risk%20of%20misuse%20to%20certain%20extent%2C%0Atheir%20robustness%20remains%20insufficient%20when%20dealing%20with%20adversarial%20attacks.%0A%20%20Given%20that%20semantic%20consistency%20between%20input%20text%20and%20output%20image%20is%20a%0Afundamental%20requirement%20for%20T2I%20models%2C%20we%20identify%20that%20textual%0Arepresentations%20%28i.e.%2C%20prompt%20embeddings%29%20are%20likely%20the%20primary%20source%20of%0Aunsafe%20generation.%20To%20this%20end%2C%20we%20propose%20a%20vision-agnostic%20safe%20generation%0Aframework%2C%20Embedding%20Sanitizer%20%28ES%29%2C%20which%20focuses%20on%20erasing%20inappropriate%0Aconcepts%20from%20prompt%20embeddings%20and%20uses%20the%20sanitized%20embeddings%20to%20guide%20the%0Amodel%20for%20safe%20generation.%20ES%20is%20applied%20to%20the%20output%20of%20the%20text%20encoder%20as%20a%0Aplug-and-play%20module%2C%20enabling%20seamless%20integration%20with%20different%20T2I%20models%0Aas%20well%20as%20other%20safeguards.%20In%20addition%2C%20ES%27s%20unique%20scoring%20mechanism%20assigns%0Aa%20score%20to%20each%20token%20in%20the%20prompt%20to%20indicate%20its%20potential%20harmfulness%2C%20and%0Adynamically%20adjusts%20the%20sanitization%20intensity%20to%20balance%20defensive%20performance%0Aand%20generation%20quality.%20Through%20extensive%20evaluation%20on%20five%20prompt%20benchmarks%2C%0Aour%20approach%20achieves%20state-of-the-art%20robustness%20by%20sanitizing%20the%20source%0A%28prompt%20embedding%29%20of%20unsafe%20generation%20compared%20to%20nine%20baseline%20methods.%20It%0Asignificantly%20outperforms%20existing%20safeguards%20in%20terms%20of%20interpretability%20and%0Acontrollability%20while%20maintaining%20generation%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10329v1&entry.124074799=Read"},
{"title": "Mitigating Hallucination in Multimodal Large Language Model via\n  Hallucination-targeted Direct Preference Optimization", "author": "Yuhan Fu and Ruobing Xie and Xingwu Sun and Zhanhui Kang and Xirong Li", "abstract": "  Multimodal Large Language Models (MLLMs) are known to hallucinate, which\nlimits their practical applications. Recent works have attempted to apply\nDirect Preference Optimization (DPO) to enhance the performance of MLLMs, but\nhave shown inconsistent improvements in mitigating hallucinations. To address\nthis issue more effectively, we introduce Hallucination-targeted Direct\nPreference Optimization (HDPO) to reduce hallucinations in MLLMs. Unlike\nprevious approaches, our method tackles hallucinations from their diverse forms\nand causes. Specifically, we develop three types of preference pair data\ntargeting the following causes of MLLM hallucinations: (1) insufficient visual\ncapabilities, (2) long context generation, and (3) multimodal conflicts.\nExperimental results demonstrate that our method achieves superior performance\nacross multiple hallucination evaluation datasets, surpassing most\nstate-of-the-art (SOTA) methods and highlighting the potential of our approach.\nAblation studies and in-depth analyses further confirm the effectiveness of our\nmethod and suggest the potential for further improvements through scaling up.\n", "link": "http://arxiv.org/abs/2411.10436v1", "date": "2024-11-15", "relevancy": 2.0181, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5272}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5012}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4988}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Hallucination%20in%20Multimodal%20Large%20Language%20Model%20via%0A%20%20Hallucination-targeted%20Direct%20Preference%20Optimization&body=Title%3A%20Mitigating%20Hallucination%20in%20Multimodal%20Large%20Language%20Model%20via%0A%20%20Hallucination-targeted%20Direct%20Preference%20Optimization%0AAuthor%3A%20Yuhan%20Fu%20and%20Ruobing%20Xie%20and%20Xingwu%20Sun%20and%20Zhanhui%20Kang%20and%20Xirong%20Li%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20are%20known%20to%20hallucinate%2C%20which%0Alimits%20their%20practical%20applications.%20Recent%20works%20have%20attempted%20to%20apply%0ADirect%20Preference%20Optimization%20%28DPO%29%20to%20enhance%20the%20performance%20of%20MLLMs%2C%20but%0Ahave%20shown%20inconsistent%20improvements%20in%20mitigating%20hallucinations.%20To%20address%0Athis%20issue%20more%20effectively%2C%20we%20introduce%20Hallucination-targeted%20Direct%0APreference%20Optimization%20%28HDPO%29%20to%20reduce%20hallucinations%20in%20MLLMs.%20Unlike%0Aprevious%20approaches%2C%20our%20method%20tackles%20hallucinations%20from%20their%20diverse%20forms%0Aand%20causes.%20Specifically%2C%20we%20develop%20three%20types%20of%20preference%20pair%20data%0Atargeting%20the%20following%20causes%20of%20MLLM%20hallucinations%3A%20%281%29%20insufficient%20visual%0Acapabilities%2C%20%282%29%20long%20context%20generation%2C%20and%20%283%29%20multimodal%20conflicts.%0AExperimental%20results%20demonstrate%20that%20our%20method%20achieves%20superior%20performance%0Aacross%20multiple%20hallucination%20evaluation%20datasets%2C%20surpassing%20most%0Astate-of-the-art%20%28SOTA%29%20methods%20and%20highlighting%20the%20potential%20of%20our%20approach.%0AAblation%20studies%20and%20in-depth%20analyses%20further%20confirm%20the%20effectiveness%20of%20our%0Amethod%20and%20suggest%20the%20potential%20for%20further%20improvements%20through%20scaling%20up.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10436v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Hallucination%2520in%2520Multimodal%2520Large%2520Language%2520Model%2520via%250A%2520%2520Hallucination-targeted%2520Direct%2520Preference%2520Optimization%26entry.906535625%3DYuhan%2520Fu%2520and%2520Ruobing%2520Xie%2520and%2520Xingwu%2520Sun%2520and%2520Zhanhui%2520Kang%2520and%2520Xirong%2520Li%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520are%2520known%2520to%2520hallucinate%252C%2520which%250Alimits%2520their%2520practical%2520applications.%2520Recent%2520works%2520have%2520attempted%2520to%2520apply%250ADirect%2520Preference%2520Optimization%2520%2528DPO%2529%2520to%2520enhance%2520the%2520performance%2520of%2520MLLMs%252C%2520but%250Ahave%2520shown%2520inconsistent%2520improvements%2520in%2520mitigating%2520hallucinations.%2520To%2520address%250Athis%2520issue%2520more%2520effectively%252C%2520we%2520introduce%2520Hallucination-targeted%2520Direct%250APreference%2520Optimization%2520%2528HDPO%2529%2520to%2520reduce%2520hallucinations%2520in%2520MLLMs.%2520Unlike%250Aprevious%2520approaches%252C%2520our%2520method%2520tackles%2520hallucinations%2520from%2520their%2520diverse%2520forms%250Aand%2520causes.%2520Specifically%252C%2520we%2520develop%2520three%2520types%2520of%2520preference%2520pair%2520data%250Atargeting%2520the%2520following%2520causes%2520of%2520MLLM%2520hallucinations%253A%2520%25281%2529%2520insufficient%2520visual%250Acapabilities%252C%2520%25282%2529%2520long%2520context%2520generation%252C%2520and%2520%25283%2529%2520multimodal%2520conflicts.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520method%2520achieves%2520superior%2520performance%250Aacross%2520multiple%2520hallucination%2520evaluation%2520datasets%252C%2520surpassing%2520most%250Astate-of-the-art%2520%2528SOTA%2529%2520methods%2520and%2520highlighting%2520the%2520potential%2520of%2520our%2520approach.%250AAblation%2520studies%2520and%2520in-depth%2520analyses%2520further%2520confirm%2520the%2520effectiveness%2520of%2520our%250Amethod%2520and%2520suggest%2520the%2520potential%2520for%2520further%2520improvements%2520through%2520scaling%2520up.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10436v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Hallucination%20in%20Multimodal%20Large%20Language%20Model%20via%0A%20%20Hallucination-targeted%20Direct%20Preference%20Optimization&entry.906535625=Yuhan%20Fu%20and%20Ruobing%20Xie%20and%20Xingwu%20Sun%20and%20Zhanhui%20Kang%20and%20Xirong%20Li&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20are%20known%20to%20hallucinate%2C%20which%0Alimits%20their%20practical%20applications.%20Recent%20works%20have%20attempted%20to%20apply%0ADirect%20Preference%20Optimization%20%28DPO%29%20to%20enhance%20the%20performance%20of%20MLLMs%2C%20but%0Ahave%20shown%20inconsistent%20improvements%20in%20mitigating%20hallucinations.%20To%20address%0Athis%20issue%20more%20effectively%2C%20we%20introduce%20Hallucination-targeted%20Direct%0APreference%20Optimization%20%28HDPO%29%20to%20reduce%20hallucinations%20in%20MLLMs.%20Unlike%0Aprevious%20approaches%2C%20our%20method%20tackles%20hallucinations%20from%20their%20diverse%20forms%0Aand%20causes.%20Specifically%2C%20we%20develop%20three%20types%20of%20preference%20pair%20data%0Atargeting%20the%20following%20causes%20of%20MLLM%20hallucinations%3A%20%281%29%20insufficient%20visual%0Acapabilities%2C%20%282%29%20long%20context%20generation%2C%20and%20%283%29%20multimodal%20conflicts.%0AExperimental%20results%20demonstrate%20that%20our%20method%20achieves%20superior%20performance%0Aacross%20multiple%20hallucination%20evaluation%20datasets%2C%20surpassing%20most%0Astate-of-the-art%20%28SOTA%29%20methods%20and%20highlighting%20the%20potential%20of%20our%20approach.%0AAblation%20studies%20and%20in-depth%20analyses%20further%20confirm%20the%20effectiveness%20of%20our%0Amethod%20and%20suggest%20the%20potential%20for%20further%20improvements%20through%20scaling%20up.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10436v1&entry.124074799=Read"},
{"title": "Energy-Aware Predictive Motion Planning for Autonomous Vehicles Using a\n  Hybrid Zonotope Constraint Representation", "author": "Joshua A. Robbins and Andrew F. Thompson and Sean Brennan and Herschel C. Pangborn", "abstract": "  Uncrewed aerial systems have tightly coupled energy and motion dynamics which\nmust be accounted for by onboard planning algorithms. This work proposes a\nstrategy for coupled motion and energy planning using model predictive control\n(MPC). A reduced-order linear time-invariant model of coupled energy and motion\ndynamics is presented. Constrained zonotopes are used to represent state and\ninput constraints, and hybrid zonotopes are used to represent non-convex\nconstraints tied to a map of the environment. The structures of these\nconstraint representations are exploited within a mixed-integer quadratic\nprogram solver tailored to MPC motion planning problems. Results apply the\nproposed methodology to coupled motion and energy utilization planning problems\nfor 1) a hybrid-electric vehicle that must restrict engine usage when flying\nover regions with noise restrictions, and 2) an electric package delivery drone\nthat must track waysets with both position and battery state of charge\nrequirements. By leveraging the structure-exploiting solver, the proposed\nmixed-integer MPC formulations can be implemented in real time.\n", "link": "http://arxiv.org/abs/2411.03189v2", "date": "2024-11-15", "relevancy": 2.0033, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5359}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4941}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Energy-Aware%20Predictive%20Motion%20Planning%20for%20Autonomous%20Vehicles%20Using%20a%0A%20%20Hybrid%20Zonotope%20Constraint%20Representation&body=Title%3A%20Energy-Aware%20Predictive%20Motion%20Planning%20for%20Autonomous%20Vehicles%20Using%20a%0A%20%20Hybrid%20Zonotope%20Constraint%20Representation%0AAuthor%3A%20Joshua%20A.%20Robbins%20and%20Andrew%20F.%20Thompson%20and%20Sean%20Brennan%20and%20Herschel%20C.%20Pangborn%0AAbstract%3A%20%20%20Uncrewed%20aerial%20systems%20have%20tightly%20coupled%20energy%20and%20motion%20dynamics%20which%0Amust%20be%20accounted%20for%20by%20onboard%20planning%20algorithms.%20This%20work%20proposes%20a%0Astrategy%20for%20coupled%20motion%20and%20energy%20planning%20using%20model%20predictive%20control%0A%28MPC%29.%20A%20reduced-order%20linear%20time-invariant%20model%20of%20coupled%20energy%20and%20motion%0Adynamics%20is%20presented.%20Constrained%20zonotopes%20are%20used%20to%20represent%20state%20and%0Ainput%20constraints%2C%20and%20hybrid%20zonotopes%20are%20used%20to%20represent%20non-convex%0Aconstraints%20tied%20to%20a%20map%20of%20the%20environment.%20The%20structures%20of%20these%0Aconstraint%20representations%20are%20exploited%20within%20a%20mixed-integer%20quadratic%0Aprogram%20solver%20tailored%20to%20MPC%20motion%20planning%20problems.%20Results%20apply%20the%0Aproposed%20methodology%20to%20coupled%20motion%20and%20energy%20utilization%20planning%20problems%0Afor%201%29%20a%20hybrid-electric%20vehicle%20that%20must%20restrict%20engine%20usage%20when%20flying%0Aover%20regions%20with%20noise%20restrictions%2C%20and%202%29%20an%20electric%20package%20delivery%20drone%0Athat%20must%20track%20waysets%20with%20both%20position%20and%20battery%20state%20of%20charge%0Arequirements.%20By%20leveraging%20the%20structure-exploiting%20solver%2C%20the%20proposed%0Amixed-integer%20MPC%20formulations%20can%20be%20implemented%20in%20real%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03189v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnergy-Aware%2520Predictive%2520Motion%2520Planning%2520for%2520Autonomous%2520Vehicles%2520Using%2520a%250A%2520%2520Hybrid%2520Zonotope%2520Constraint%2520Representation%26entry.906535625%3DJoshua%2520A.%2520Robbins%2520and%2520Andrew%2520F.%2520Thompson%2520and%2520Sean%2520Brennan%2520and%2520Herschel%2520C.%2520Pangborn%26entry.1292438233%3D%2520%2520Uncrewed%2520aerial%2520systems%2520have%2520tightly%2520coupled%2520energy%2520and%2520motion%2520dynamics%2520which%250Amust%2520be%2520accounted%2520for%2520by%2520onboard%2520planning%2520algorithms.%2520This%2520work%2520proposes%2520a%250Astrategy%2520for%2520coupled%2520motion%2520and%2520energy%2520planning%2520using%2520model%2520predictive%2520control%250A%2528MPC%2529.%2520A%2520reduced-order%2520linear%2520time-invariant%2520model%2520of%2520coupled%2520energy%2520and%2520motion%250Adynamics%2520is%2520presented.%2520Constrained%2520zonotopes%2520are%2520used%2520to%2520represent%2520state%2520and%250Ainput%2520constraints%252C%2520and%2520hybrid%2520zonotopes%2520are%2520used%2520to%2520represent%2520non-convex%250Aconstraints%2520tied%2520to%2520a%2520map%2520of%2520the%2520environment.%2520The%2520structures%2520of%2520these%250Aconstraint%2520representations%2520are%2520exploited%2520within%2520a%2520mixed-integer%2520quadratic%250Aprogram%2520solver%2520tailored%2520to%2520MPC%2520motion%2520planning%2520problems.%2520Results%2520apply%2520the%250Aproposed%2520methodology%2520to%2520coupled%2520motion%2520and%2520energy%2520utilization%2520planning%2520problems%250Afor%25201%2529%2520a%2520hybrid-electric%2520vehicle%2520that%2520must%2520restrict%2520engine%2520usage%2520when%2520flying%250Aover%2520regions%2520with%2520noise%2520restrictions%252C%2520and%25202%2529%2520an%2520electric%2520package%2520delivery%2520drone%250Athat%2520must%2520track%2520waysets%2520with%2520both%2520position%2520and%2520battery%2520state%2520of%2520charge%250Arequirements.%2520By%2520leveraging%2520the%2520structure-exploiting%2520solver%252C%2520the%2520proposed%250Amixed-integer%2520MPC%2520formulations%2520can%2520be%2520implemented%2520in%2520real%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03189v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Energy-Aware%20Predictive%20Motion%20Planning%20for%20Autonomous%20Vehicles%20Using%20a%0A%20%20Hybrid%20Zonotope%20Constraint%20Representation&entry.906535625=Joshua%20A.%20Robbins%20and%20Andrew%20F.%20Thompson%20and%20Sean%20Brennan%20and%20Herschel%20C.%20Pangborn&entry.1292438233=%20%20Uncrewed%20aerial%20systems%20have%20tightly%20coupled%20energy%20and%20motion%20dynamics%20which%0Amust%20be%20accounted%20for%20by%20onboard%20planning%20algorithms.%20This%20work%20proposes%20a%0Astrategy%20for%20coupled%20motion%20and%20energy%20planning%20using%20model%20predictive%20control%0A%28MPC%29.%20A%20reduced-order%20linear%20time-invariant%20model%20of%20coupled%20energy%20and%20motion%0Adynamics%20is%20presented.%20Constrained%20zonotopes%20are%20used%20to%20represent%20state%20and%0Ainput%20constraints%2C%20and%20hybrid%20zonotopes%20are%20used%20to%20represent%20non-convex%0Aconstraints%20tied%20to%20a%20map%20of%20the%20environment.%20The%20structures%20of%20these%0Aconstraint%20representations%20are%20exploited%20within%20a%20mixed-integer%20quadratic%0Aprogram%20solver%20tailored%20to%20MPC%20motion%20planning%20problems.%20Results%20apply%20the%0Aproposed%20methodology%20to%20coupled%20motion%20and%20energy%20utilization%20planning%20problems%0Afor%201%29%20a%20hybrid-electric%20vehicle%20that%20must%20restrict%20engine%20usage%20when%20flying%0Aover%20regions%20with%20noise%20restrictions%2C%20and%202%29%20an%20electric%20package%20delivery%20drone%0Athat%20must%20track%20waysets%20with%20both%20position%20and%20battery%20state%20of%20charge%0Arequirements.%20By%20leveraging%20the%20structure-exploiting%20solver%2C%20the%20proposed%0Amixed-integer%20MPC%20formulations%20can%20be%20implemented%20in%20real%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03189v2&entry.124074799=Read"},
{"title": "Private Counterfactual Retrieval With Immutable Features", "author": "Shreya Meel and Pasan Dissanayake and Mohamed Nomeir and Sanghamitra Dutta and Sennur Ulukus", "abstract": "  In a classification task, counterfactual explanations provide the minimum\nchange needed for an input to be classified into a favorable class. We consider\nthe problem of privately retrieving the exact closest counterfactual from a\ndatabase of accepted samples while enforcing that certain features of the input\nsample cannot be changed, i.e., they are \\emph{immutable}. An applicant (user)\nwhose feature vector is rejected by a machine learning model wants to retrieve\nthe sample closest to them in the database without altering a private subset of\ntheir features, which constitutes the immutable set. While doing this, the user\nshould keep their feature vector, immutable set and the resulting\ncounterfactual index information-theoretically private from the institution. We\nrefer to this as immutable private counterfactual retrieval (I-PCR) problem\nwhich generalizes PCR to a more practical setting. In this paper, we propose\ntwo I-PCR schemes by leveraging techniques from private information retrieval\n(PIR) and characterize their communication costs. Further, we quantify the\ninformation that the user learns about the database and compare it for the\nproposed schemes.\n", "link": "http://arxiv.org/abs/2411.10429v1", "date": "2024-11-15", "relevancy": 1.9891, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4047}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3974}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3913}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Private%20Counterfactual%20Retrieval%20With%20Immutable%20Features&body=Title%3A%20Private%20Counterfactual%20Retrieval%20With%20Immutable%20Features%0AAuthor%3A%20Shreya%20Meel%20and%20Pasan%20Dissanayake%20and%20Mohamed%20Nomeir%20and%20Sanghamitra%20Dutta%20and%20Sennur%20Ulukus%0AAbstract%3A%20%20%20In%20a%20classification%20task%2C%20counterfactual%20explanations%20provide%20the%20minimum%0Achange%20needed%20for%20an%20input%20to%20be%20classified%20into%20a%20favorable%20class.%20We%20consider%0Athe%20problem%20of%20privately%20retrieving%20the%20exact%20closest%20counterfactual%20from%20a%0Adatabase%20of%20accepted%20samples%20while%20enforcing%20that%20certain%20features%20of%20the%20input%0Asample%20cannot%20be%20changed%2C%20i.e.%2C%20they%20are%20%5Cemph%7Bimmutable%7D.%20An%20applicant%20%28user%29%0Awhose%20feature%20vector%20is%20rejected%20by%20a%20machine%20learning%20model%20wants%20to%20retrieve%0Athe%20sample%20closest%20to%20them%20in%20the%20database%20without%20altering%20a%20private%20subset%20of%0Atheir%20features%2C%20which%20constitutes%20the%20immutable%20set.%20While%20doing%20this%2C%20the%20user%0Ashould%20keep%20their%20feature%20vector%2C%20immutable%20set%20and%20the%20resulting%0Acounterfactual%20index%20information-theoretically%20private%20from%20the%20institution.%20We%0Arefer%20to%20this%20as%20immutable%20private%20counterfactual%20retrieval%20%28I-PCR%29%20problem%0Awhich%20generalizes%20PCR%20to%20a%20more%20practical%20setting.%20In%20this%20paper%2C%20we%20propose%0Atwo%20I-PCR%20schemes%20by%20leveraging%20techniques%20from%20private%20information%20retrieval%0A%28PIR%29%20and%20characterize%20their%20communication%20costs.%20Further%2C%20we%20quantify%20the%0Ainformation%20that%20the%20user%20learns%20about%20the%20database%20and%20compare%20it%20for%20the%0Aproposed%20schemes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10429v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrivate%2520Counterfactual%2520Retrieval%2520With%2520Immutable%2520Features%26entry.906535625%3DShreya%2520Meel%2520and%2520Pasan%2520Dissanayake%2520and%2520Mohamed%2520Nomeir%2520and%2520Sanghamitra%2520Dutta%2520and%2520Sennur%2520Ulukus%26entry.1292438233%3D%2520%2520In%2520a%2520classification%2520task%252C%2520counterfactual%2520explanations%2520provide%2520the%2520minimum%250Achange%2520needed%2520for%2520an%2520input%2520to%2520be%2520classified%2520into%2520a%2520favorable%2520class.%2520We%2520consider%250Athe%2520problem%2520of%2520privately%2520retrieving%2520the%2520exact%2520closest%2520counterfactual%2520from%2520a%250Adatabase%2520of%2520accepted%2520samples%2520while%2520enforcing%2520that%2520certain%2520features%2520of%2520the%2520input%250Asample%2520cannot%2520be%2520changed%252C%2520i.e.%252C%2520they%2520are%2520%255Cemph%257Bimmutable%257D.%2520An%2520applicant%2520%2528user%2529%250Awhose%2520feature%2520vector%2520is%2520rejected%2520by%2520a%2520machine%2520learning%2520model%2520wants%2520to%2520retrieve%250Athe%2520sample%2520closest%2520to%2520them%2520in%2520the%2520database%2520without%2520altering%2520a%2520private%2520subset%2520of%250Atheir%2520features%252C%2520which%2520constitutes%2520the%2520immutable%2520set.%2520While%2520doing%2520this%252C%2520the%2520user%250Ashould%2520keep%2520their%2520feature%2520vector%252C%2520immutable%2520set%2520and%2520the%2520resulting%250Acounterfactual%2520index%2520information-theoretically%2520private%2520from%2520the%2520institution.%2520We%250Arefer%2520to%2520this%2520as%2520immutable%2520private%2520counterfactual%2520retrieval%2520%2528I-PCR%2529%2520problem%250Awhich%2520generalizes%2520PCR%2520to%2520a%2520more%2520practical%2520setting.%2520In%2520this%2520paper%252C%2520we%2520propose%250Atwo%2520I-PCR%2520schemes%2520by%2520leveraging%2520techniques%2520from%2520private%2520information%2520retrieval%250A%2528PIR%2529%2520and%2520characterize%2520their%2520communication%2520costs.%2520Further%252C%2520we%2520quantify%2520the%250Ainformation%2520that%2520the%2520user%2520learns%2520about%2520the%2520database%2520and%2520compare%2520it%2520for%2520the%250Aproposed%2520schemes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10429v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Private%20Counterfactual%20Retrieval%20With%20Immutable%20Features&entry.906535625=Shreya%20Meel%20and%20Pasan%20Dissanayake%20and%20Mohamed%20Nomeir%20and%20Sanghamitra%20Dutta%20and%20Sennur%20Ulukus&entry.1292438233=%20%20In%20a%20classification%20task%2C%20counterfactual%20explanations%20provide%20the%20minimum%0Achange%20needed%20for%20an%20input%20to%20be%20classified%20into%20a%20favorable%20class.%20We%20consider%0Athe%20problem%20of%20privately%20retrieving%20the%20exact%20closest%20counterfactual%20from%20a%0Adatabase%20of%20accepted%20samples%20while%20enforcing%20that%20certain%20features%20of%20the%20input%0Asample%20cannot%20be%20changed%2C%20i.e.%2C%20they%20are%20%5Cemph%7Bimmutable%7D.%20An%20applicant%20%28user%29%0Awhose%20feature%20vector%20is%20rejected%20by%20a%20machine%20learning%20model%20wants%20to%20retrieve%0Athe%20sample%20closest%20to%20them%20in%20the%20database%20without%20altering%20a%20private%20subset%20of%0Atheir%20features%2C%20which%20constitutes%20the%20immutable%20set.%20While%20doing%20this%2C%20the%20user%0Ashould%20keep%20their%20feature%20vector%2C%20immutable%20set%20and%20the%20resulting%0Acounterfactual%20index%20information-theoretically%20private%20from%20the%20institution.%20We%0Arefer%20to%20this%20as%20immutable%20private%20counterfactual%20retrieval%20%28I-PCR%29%20problem%0Awhich%20generalizes%20PCR%20to%20a%20more%20practical%20setting.%20In%20this%20paper%2C%20we%20propose%0Atwo%20I-PCR%20schemes%20by%20leveraging%20techniques%20from%20private%20information%20retrieval%0A%28PIR%29%20and%20characterize%20their%20communication%20costs.%20Further%2C%20we%20quantify%20the%0Ainformation%20that%20the%20user%20learns%20about%20the%20database%20and%20compare%20it%20for%20the%0Aproposed%20schemes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10429v1&entry.124074799=Read"},
{"title": "Advancing Building Energy Modeling with Large Language Models:\n  Exploration and Case Studies", "author": "Liang Zhang and Zhelun Chen and Vitaly Ford", "abstract": "  The rapid progression in artificial intelligence has facilitated the\nemergence of large language models like ChatGPT, offering potential\napplications extending into specialized engineering modeling, especially\nphysics-based building energy modeling. This paper investigates the innovative\nintegration of large language models with building energy modeling software,\nfocusing specifically on the fusion of ChatGPT with EnergyPlus. A literature\nreview is first conducted to reveal a growing trend of incorporating large\nlanguage models in engineering modeling, albeit limited research on their\napplication in building energy modeling. We underscore the potential of large\nlanguage models in addressing building energy modeling challenges and outline\npotential applications including simulation input generation, simulation output\nanalysis and visualization, conducting error analysis, co-simulation,\nsimulation knowledge extraction and training, and simulation optimization.\nThree case studies reveal the transformative potential of large language models\nin automating and optimizing building energy modeling tasks, underscoring the\npivotal role of artificial intelligence in advancing sustainable building\npractices and energy efficiency. The case studies demonstrate that selecting\nthe right large language model techniques is essential to enhance performance\nand reduce engineering efforts. The findings advocate a multidisciplinary\napproach in future artificial intelligence research, with implications\nextending beyond building energy modeling to other specialized engineering\nmodeling.\n", "link": "http://arxiv.org/abs/2402.09579v2", "date": "2024-11-15", "relevancy": 1.9838, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4992}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4953}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Building%20Energy%20Modeling%20with%20Large%20Language%20Models%3A%0A%20%20Exploration%20and%20Case%20Studies&body=Title%3A%20Advancing%20Building%20Energy%20Modeling%20with%20Large%20Language%20Models%3A%0A%20%20Exploration%20and%20Case%20Studies%0AAuthor%3A%20Liang%20Zhang%20and%20Zhelun%20Chen%20and%20Vitaly%20Ford%0AAbstract%3A%20%20%20The%20rapid%20progression%20in%20artificial%20intelligence%20has%20facilitated%20the%0Aemergence%20of%20large%20language%20models%20like%20ChatGPT%2C%20offering%20potential%0Aapplications%20extending%20into%20specialized%20engineering%20modeling%2C%20especially%0Aphysics-based%20building%20energy%20modeling.%20This%20paper%20investigates%20the%20innovative%0Aintegration%20of%20large%20language%20models%20with%20building%20energy%20modeling%20software%2C%0Afocusing%20specifically%20on%20the%20fusion%20of%20ChatGPT%20with%20EnergyPlus.%20A%20literature%0Areview%20is%20first%20conducted%20to%20reveal%20a%20growing%20trend%20of%20incorporating%20large%0Alanguage%20models%20in%20engineering%20modeling%2C%20albeit%20limited%20research%20on%20their%0Aapplication%20in%20building%20energy%20modeling.%20We%20underscore%20the%20potential%20of%20large%0Alanguage%20models%20in%20addressing%20building%20energy%20modeling%20challenges%20and%20outline%0Apotential%20applications%20including%20simulation%20input%20generation%2C%20simulation%20output%0Aanalysis%20and%20visualization%2C%20conducting%20error%20analysis%2C%20co-simulation%2C%0Asimulation%20knowledge%20extraction%20and%20training%2C%20and%20simulation%20optimization.%0AThree%20case%20studies%20reveal%20the%20transformative%20potential%20of%20large%20language%20models%0Ain%20automating%20and%20optimizing%20building%20energy%20modeling%20tasks%2C%20underscoring%20the%0Apivotal%20role%20of%20artificial%20intelligence%20in%20advancing%20sustainable%20building%0Apractices%20and%20energy%20efficiency.%20The%20case%20studies%20demonstrate%20that%20selecting%0Athe%20right%20large%20language%20model%20techniques%20is%20essential%20to%20enhance%20performance%0Aand%20reduce%20engineering%20efforts.%20The%20findings%20advocate%20a%20multidisciplinary%0Aapproach%20in%20future%20artificial%20intelligence%20research%2C%20with%20implications%0Aextending%20beyond%20building%20energy%20modeling%20to%20other%20specialized%20engineering%0Amodeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.09579v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Building%2520Energy%2520Modeling%2520with%2520Large%2520Language%2520Models%253A%250A%2520%2520Exploration%2520and%2520Case%2520Studies%26entry.906535625%3DLiang%2520Zhang%2520and%2520Zhelun%2520Chen%2520and%2520Vitaly%2520Ford%26entry.1292438233%3D%2520%2520The%2520rapid%2520progression%2520in%2520artificial%2520intelligence%2520has%2520facilitated%2520the%250Aemergence%2520of%2520large%2520language%2520models%2520like%2520ChatGPT%252C%2520offering%2520potential%250Aapplications%2520extending%2520into%2520specialized%2520engineering%2520modeling%252C%2520especially%250Aphysics-based%2520building%2520energy%2520modeling.%2520This%2520paper%2520investigates%2520the%2520innovative%250Aintegration%2520of%2520large%2520language%2520models%2520with%2520building%2520energy%2520modeling%2520software%252C%250Afocusing%2520specifically%2520on%2520the%2520fusion%2520of%2520ChatGPT%2520with%2520EnergyPlus.%2520A%2520literature%250Areview%2520is%2520first%2520conducted%2520to%2520reveal%2520a%2520growing%2520trend%2520of%2520incorporating%2520large%250Alanguage%2520models%2520in%2520engineering%2520modeling%252C%2520albeit%2520limited%2520research%2520on%2520their%250Aapplication%2520in%2520building%2520energy%2520modeling.%2520We%2520underscore%2520the%2520potential%2520of%2520large%250Alanguage%2520models%2520in%2520addressing%2520building%2520energy%2520modeling%2520challenges%2520and%2520outline%250Apotential%2520applications%2520including%2520simulation%2520input%2520generation%252C%2520simulation%2520output%250Aanalysis%2520and%2520visualization%252C%2520conducting%2520error%2520analysis%252C%2520co-simulation%252C%250Asimulation%2520knowledge%2520extraction%2520and%2520training%252C%2520and%2520simulation%2520optimization.%250AThree%2520case%2520studies%2520reveal%2520the%2520transformative%2520potential%2520of%2520large%2520language%2520models%250Ain%2520automating%2520and%2520optimizing%2520building%2520energy%2520modeling%2520tasks%252C%2520underscoring%2520the%250Apivotal%2520role%2520of%2520artificial%2520intelligence%2520in%2520advancing%2520sustainable%2520building%250Apractices%2520and%2520energy%2520efficiency.%2520The%2520case%2520studies%2520demonstrate%2520that%2520selecting%250Athe%2520right%2520large%2520language%2520model%2520techniques%2520is%2520essential%2520to%2520enhance%2520performance%250Aand%2520reduce%2520engineering%2520efforts.%2520The%2520findings%2520advocate%2520a%2520multidisciplinary%250Aapproach%2520in%2520future%2520artificial%2520intelligence%2520research%252C%2520with%2520implications%250Aextending%2520beyond%2520building%2520energy%2520modeling%2520to%2520other%2520specialized%2520engineering%250Amodeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.09579v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Building%20Energy%20Modeling%20with%20Large%20Language%20Models%3A%0A%20%20Exploration%20and%20Case%20Studies&entry.906535625=Liang%20Zhang%20and%20Zhelun%20Chen%20and%20Vitaly%20Ford&entry.1292438233=%20%20The%20rapid%20progression%20in%20artificial%20intelligence%20has%20facilitated%20the%0Aemergence%20of%20large%20language%20models%20like%20ChatGPT%2C%20offering%20potential%0Aapplications%20extending%20into%20specialized%20engineering%20modeling%2C%20especially%0Aphysics-based%20building%20energy%20modeling.%20This%20paper%20investigates%20the%20innovative%0Aintegration%20of%20large%20language%20models%20with%20building%20energy%20modeling%20software%2C%0Afocusing%20specifically%20on%20the%20fusion%20of%20ChatGPT%20with%20EnergyPlus.%20A%20literature%0Areview%20is%20first%20conducted%20to%20reveal%20a%20growing%20trend%20of%20incorporating%20large%0Alanguage%20models%20in%20engineering%20modeling%2C%20albeit%20limited%20research%20on%20their%0Aapplication%20in%20building%20energy%20modeling.%20We%20underscore%20the%20potential%20of%20large%0Alanguage%20models%20in%20addressing%20building%20energy%20modeling%20challenges%20and%20outline%0Apotential%20applications%20including%20simulation%20input%20generation%2C%20simulation%20output%0Aanalysis%20and%20visualization%2C%20conducting%20error%20analysis%2C%20co-simulation%2C%0Asimulation%20knowledge%20extraction%20and%20training%2C%20and%20simulation%20optimization.%0AThree%20case%20studies%20reveal%20the%20transformative%20potential%20of%20large%20language%20models%0Ain%20automating%20and%20optimizing%20building%20energy%20modeling%20tasks%2C%20underscoring%20the%0Apivotal%20role%20of%20artificial%20intelligence%20in%20advancing%20sustainable%20building%0Apractices%20and%20energy%20efficiency.%20The%20case%20studies%20demonstrate%20that%20selecting%0Athe%20right%20large%20language%20model%20techniques%20is%20essential%20to%20enhance%20performance%0Aand%20reduce%20engineering%20efforts.%20The%20findings%20advocate%20a%20multidisciplinary%0Aapproach%20in%20future%20artificial%20intelligence%20research%2C%20with%20implications%0Aextending%20beyond%20building%20energy%20modeling%20to%20other%20specialized%20engineering%0Amodeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09579v2&entry.124074799=Read"},
{"title": "Efficient Neural Hybrid System Learning and Transition System\n  Abstraction for Dynamical Systems", "author": "Yejiang Yang and Zihao Mo and Weiming Xiang", "abstract": "  This paper proposes a neural network hybrid modeling framework for dynamics\nlearning to promote an interpretable, computationally efficient way of dynamics\nlearning and system identification. First, a low-level model will be trained to\nlearn the system dynamics, which utilizes multiple simple neural networks to\napproximate the local dynamics generated from data-driven partitions. Then,\nbased on the low-level model, a high-level model will be trained to abstract\nthe low-level neural hybrid system model into a transition system that allows\nComputational Tree Logic Verification to promote the model's ability with human\ninteraction and verification efficiency.\n", "link": "http://arxiv.org/abs/2411.10240v1", "date": "2024-11-15", "relevancy": 1.9735, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5182}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4958}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Neural%20Hybrid%20System%20Learning%20and%20Transition%20System%0A%20%20Abstraction%20for%20Dynamical%20Systems&body=Title%3A%20Efficient%20Neural%20Hybrid%20System%20Learning%20and%20Transition%20System%0A%20%20Abstraction%20for%20Dynamical%20Systems%0AAuthor%3A%20Yejiang%20Yang%20and%20Zihao%20Mo%20and%20Weiming%20Xiang%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20neural%20network%20hybrid%20modeling%20framework%20for%20dynamics%0Alearning%20to%20promote%20an%20interpretable%2C%20computationally%20efficient%20way%20of%20dynamics%0Alearning%20and%20system%20identification.%20First%2C%20a%20low-level%20model%20will%20be%20trained%20to%0Alearn%20the%20system%20dynamics%2C%20which%20utilizes%20multiple%20simple%20neural%20networks%20to%0Aapproximate%20the%20local%20dynamics%20generated%20from%20data-driven%20partitions.%20Then%2C%0Abased%20on%20the%20low-level%20model%2C%20a%20high-level%20model%20will%20be%20trained%20to%20abstract%0Athe%20low-level%20neural%20hybrid%20system%20model%20into%20a%20transition%20system%20that%20allows%0AComputational%20Tree%20Logic%20Verification%20to%20promote%20the%20model%27s%20ability%20with%20human%0Ainteraction%20and%20verification%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10240v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Neural%2520Hybrid%2520System%2520Learning%2520and%2520Transition%2520System%250A%2520%2520Abstraction%2520for%2520Dynamical%2520Systems%26entry.906535625%3DYejiang%2520Yang%2520and%2520Zihao%2520Mo%2520and%2520Weiming%2520Xiang%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520neural%2520network%2520hybrid%2520modeling%2520framework%2520for%2520dynamics%250Alearning%2520to%2520promote%2520an%2520interpretable%252C%2520computationally%2520efficient%2520way%2520of%2520dynamics%250Alearning%2520and%2520system%2520identification.%2520First%252C%2520a%2520low-level%2520model%2520will%2520be%2520trained%2520to%250Alearn%2520the%2520system%2520dynamics%252C%2520which%2520utilizes%2520multiple%2520simple%2520neural%2520networks%2520to%250Aapproximate%2520the%2520local%2520dynamics%2520generated%2520from%2520data-driven%2520partitions.%2520Then%252C%250Abased%2520on%2520the%2520low-level%2520model%252C%2520a%2520high-level%2520model%2520will%2520be%2520trained%2520to%2520abstract%250Athe%2520low-level%2520neural%2520hybrid%2520system%2520model%2520into%2520a%2520transition%2520system%2520that%2520allows%250AComputational%2520Tree%2520Logic%2520Verification%2520to%2520promote%2520the%2520model%2527s%2520ability%2520with%2520human%250Ainteraction%2520and%2520verification%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10240v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Neural%20Hybrid%20System%20Learning%20and%20Transition%20System%0A%20%20Abstraction%20for%20Dynamical%20Systems&entry.906535625=Yejiang%20Yang%20and%20Zihao%20Mo%20and%20Weiming%20Xiang&entry.1292438233=%20%20This%20paper%20proposes%20a%20neural%20network%20hybrid%20modeling%20framework%20for%20dynamics%0Alearning%20to%20promote%20an%20interpretable%2C%20computationally%20efficient%20way%20of%20dynamics%0Alearning%20and%20system%20identification.%20First%2C%20a%20low-level%20model%20will%20be%20trained%20to%0Alearn%20the%20system%20dynamics%2C%20which%20utilizes%20multiple%20simple%20neural%20networks%20to%0Aapproximate%20the%20local%20dynamics%20generated%20from%20data-driven%20partitions.%20Then%2C%0Abased%20on%20the%20low-level%20model%2C%20a%20high-level%20model%20will%20be%20trained%20to%20abstract%0Athe%20low-level%20neural%20hybrid%20system%20model%20into%20a%20transition%20system%20that%20allows%0AComputational%20Tree%20Logic%20Verification%20to%20promote%20the%20model%27s%20ability%20with%20human%0Ainteraction%20and%20verification%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10240v1&entry.124074799=Read"},
{"title": "Evaluating Creativity and Deception in Large Language Models: A\n  Simulation Framework for Multi-Agent Balderdash", "author": "Parsa Hejabi and Elnaz Rahmati and Alireza S. Ziabari and Preni Golazizian and Jesse Thomason and Morteza Dehghani", "abstract": "  Large Language Models (LLMs) have shown impressive capabilities in complex\ntasks and interactive environments, yet their creativity remains underexplored.\nThis paper introduces a simulation framework utilizing the game Balderdash to\nevaluate both the creativity and logical reasoning of LLMs. In Balderdash,\nplayers generate fictitious definitions for obscure terms to deceive others\nwhile identifying correct definitions. Our framework enables multiple LLM\nagents to participate in this game, assessing their ability to produce\nplausible definitions and strategize based on game rules and history. We\nimplemented a centralized game engine featuring various LLMs as participants\nand a judge LLM to evaluate semantic equivalence. Through a series of\nexperiments, we analyzed the performance of different LLMs, examining metrics\nsuch as True Definition Ratio, Deception Ratio, and Correct Guess Ratio. The\nresults provide insights into the creative and deceptive capabilities of LLMs,\nhighlighting their strengths and areas for improvement. Specifically, the study\nreveals that infrequent vocabulary in LLMs' input leads to poor reasoning on\ngame rules and historical context\n(https://github.com/ParsaHejabi/Simulation-Framework-for-Multi-Agent-Balderdash).\n", "link": "http://arxiv.org/abs/2411.10422v1", "date": "2024-11-15", "relevancy": 1.9732, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4937}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4937}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Creativity%20and%20Deception%20in%20Large%20Language%20Models%3A%20A%0A%20%20Simulation%20Framework%20for%20Multi-Agent%20Balderdash&body=Title%3A%20Evaluating%20Creativity%20and%20Deception%20in%20Large%20Language%20Models%3A%20A%0A%20%20Simulation%20Framework%20for%20Multi-Agent%20Balderdash%0AAuthor%3A%20Parsa%20Hejabi%20and%20Elnaz%20Rahmati%20and%20Alireza%20S.%20Ziabari%20and%20Preni%20Golazizian%20and%20Jesse%20Thomason%20and%20Morteza%20Dehghani%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20impressive%20capabilities%20in%20complex%0Atasks%20and%20interactive%20environments%2C%20yet%20their%20creativity%20remains%20underexplored.%0AThis%20paper%20introduces%20a%20simulation%20framework%20utilizing%20the%20game%20Balderdash%20to%0Aevaluate%20both%20the%20creativity%20and%20logical%20reasoning%20of%20LLMs.%20In%20Balderdash%2C%0Aplayers%20generate%20fictitious%20definitions%20for%20obscure%20terms%20to%20deceive%20others%0Awhile%20identifying%20correct%20definitions.%20Our%20framework%20enables%20multiple%20LLM%0Aagents%20to%20participate%20in%20this%20game%2C%20assessing%20their%20ability%20to%20produce%0Aplausible%20definitions%20and%20strategize%20based%20on%20game%20rules%20and%20history.%20We%0Aimplemented%20a%20centralized%20game%20engine%20featuring%20various%20LLMs%20as%20participants%0Aand%20a%20judge%20LLM%20to%20evaluate%20semantic%20equivalence.%20Through%20a%20series%20of%0Aexperiments%2C%20we%20analyzed%20the%20performance%20of%20different%20LLMs%2C%20examining%20metrics%0Asuch%20as%20True%20Definition%20Ratio%2C%20Deception%20Ratio%2C%20and%20Correct%20Guess%20Ratio.%20The%0Aresults%20provide%20insights%20into%20the%20creative%20and%20deceptive%20capabilities%20of%20LLMs%2C%0Ahighlighting%20their%20strengths%20and%20areas%20for%20improvement.%20Specifically%2C%20the%20study%0Areveals%20that%20infrequent%20vocabulary%20in%20LLMs%27%20input%20leads%20to%20poor%20reasoning%20on%0Agame%20rules%20and%20historical%20context%0A%28https%3A//github.com/ParsaHejabi/Simulation-Framework-for-Multi-Agent-Balderdash%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10422v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Creativity%2520and%2520Deception%2520in%2520Large%2520Language%2520Models%253A%2520A%250A%2520%2520Simulation%2520Framework%2520for%2520Multi-Agent%2520Balderdash%26entry.906535625%3DParsa%2520Hejabi%2520and%2520Elnaz%2520Rahmati%2520and%2520Alireza%2520S.%2520Ziabari%2520and%2520Preni%2520Golazizian%2520and%2520Jesse%2520Thomason%2520and%2520Morteza%2520Dehghani%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520impressive%2520capabilities%2520in%2520complex%250Atasks%2520and%2520interactive%2520environments%252C%2520yet%2520their%2520creativity%2520remains%2520underexplored.%250AThis%2520paper%2520introduces%2520a%2520simulation%2520framework%2520utilizing%2520the%2520game%2520Balderdash%2520to%250Aevaluate%2520both%2520the%2520creativity%2520and%2520logical%2520reasoning%2520of%2520LLMs.%2520In%2520Balderdash%252C%250Aplayers%2520generate%2520fictitious%2520definitions%2520for%2520obscure%2520terms%2520to%2520deceive%2520others%250Awhile%2520identifying%2520correct%2520definitions.%2520Our%2520framework%2520enables%2520multiple%2520LLM%250Aagents%2520to%2520participate%2520in%2520this%2520game%252C%2520assessing%2520their%2520ability%2520to%2520produce%250Aplausible%2520definitions%2520and%2520strategize%2520based%2520on%2520game%2520rules%2520and%2520history.%2520We%250Aimplemented%2520a%2520centralized%2520game%2520engine%2520featuring%2520various%2520LLMs%2520as%2520participants%250Aand%2520a%2520judge%2520LLM%2520to%2520evaluate%2520semantic%2520equivalence.%2520Through%2520a%2520series%2520of%250Aexperiments%252C%2520we%2520analyzed%2520the%2520performance%2520of%2520different%2520LLMs%252C%2520examining%2520metrics%250Asuch%2520as%2520True%2520Definition%2520Ratio%252C%2520Deception%2520Ratio%252C%2520and%2520Correct%2520Guess%2520Ratio.%2520The%250Aresults%2520provide%2520insights%2520into%2520the%2520creative%2520and%2520deceptive%2520capabilities%2520of%2520LLMs%252C%250Ahighlighting%2520their%2520strengths%2520and%2520areas%2520for%2520improvement.%2520Specifically%252C%2520the%2520study%250Areveals%2520that%2520infrequent%2520vocabulary%2520in%2520LLMs%2527%2520input%2520leads%2520to%2520poor%2520reasoning%2520on%250Agame%2520rules%2520and%2520historical%2520context%250A%2528https%253A//github.com/ParsaHejabi/Simulation-Framework-for-Multi-Agent-Balderdash%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10422v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Creativity%20and%20Deception%20in%20Large%20Language%20Models%3A%20A%0A%20%20Simulation%20Framework%20for%20Multi-Agent%20Balderdash&entry.906535625=Parsa%20Hejabi%20and%20Elnaz%20Rahmati%20and%20Alireza%20S.%20Ziabari%20and%20Preni%20Golazizian%20and%20Jesse%20Thomason%20and%20Morteza%20Dehghani&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20impressive%20capabilities%20in%20complex%0Atasks%20and%20interactive%20environments%2C%20yet%20their%20creativity%20remains%20underexplored.%0AThis%20paper%20introduces%20a%20simulation%20framework%20utilizing%20the%20game%20Balderdash%20to%0Aevaluate%20both%20the%20creativity%20and%20logical%20reasoning%20of%20LLMs.%20In%20Balderdash%2C%0Aplayers%20generate%20fictitious%20definitions%20for%20obscure%20terms%20to%20deceive%20others%0Awhile%20identifying%20correct%20definitions.%20Our%20framework%20enables%20multiple%20LLM%0Aagents%20to%20participate%20in%20this%20game%2C%20assessing%20their%20ability%20to%20produce%0Aplausible%20definitions%20and%20strategize%20based%20on%20game%20rules%20and%20history.%20We%0Aimplemented%20a%20centralized%20game%20engine%20featuring%20various%20LLMs%20as%20participants%0Aand%20a%20judge%20LLM%20to%20evaluate%20semantic%20equivalence.%20Through%20a%20series%20of%0Aexperiments%2C%20we%20analyzed%20the%20performance%20of%20different%20LLMs%2C%20examining%20metrics%0Asuch%20as%20True%20Definition%20Ratio%2C%20Deception%20Ratio%2C%20and%20Correct%20Guess%20Ratio.%20The%0Aresults%20provide%20insights%20into%20the%20creative%20and%20deceptive%20capabilities%20of%20LLMs%2C%0Ahighlighting%20their%20strengths%20and%20areas%20for%20improvement.%20Specifically%2C%20the%20study%0Areveals%20that%20infrequent%20vocabulary%20in%20LLMs%27%20input%20leads%20to%20poor%20reasoning%20on%0Agame%20rules%20and%20historical%20context%0A%28https%3A//github.com/ParsaHejabi/Simulation-Framework-for-Multi-Agent-Balderdash%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10422v1&entry.124074799=Read"},
{"title": "MARS: Unleashing the Power of Variance Reduction for Training Large\n  Models", "author": "Huizhuo Yuan and Yifeng Liu and Shuang Wu and Xun Zhou and Quanquan Gu", "abstract": "  Training deep neural networks--and more recently, large models--demands\nefficient and scalable optimizers. Adaptive gradient algorithms like Adam,\nAdamW, and their variants have been central to this task. Despite the\ndevelopment of numerous variance reduction algorithms in the past decade aimed\nat accelerating stochastic optimization in both convex and nonconvex settings,\nvariance reduction has not found widespread success in training deep neural\nnetworks or large language models. Consequently, it has remained a less favored\napproach in modern AI. In this paper, to unleash the power of variance\nreduction for efficient training of large models, we propose a unified\noptimization framework, MARS (Make vAriance Reduction Shine), which reconciles\npreconditioned gradient methods with variance reduction via a scaled stochastic\nrecursive momentum technique. Within our framework, we introduce three\ninstances of MARS that leverage preconditioned gradient updates based on AdamW,\nLion, and Shampoo, respectively. We also draw a connection between our\nalgorithms and existing optimizers. Experimental results on training GPT-2\nmodels indicate that MARS consistently outperforms AdamW by a large margin.\n", "link": "http://arxiv.org/abs/2411.10438v1", "date": "2024-11-15", "relevancy": 1.9664, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5015}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.491}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4882}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MARS%3A%20Unleashing%20the%20Power%20of%20Variance%20Reduction%20for%20Training%20Large%0A%20%20Models&body=Title%3A%20MARS%3A%20Unleashing%20the%20Power%20of%20Variance%20Reduction%20for%20Training%20Large%0A%20%20Models%0AAuthor%3A%20Huizhuo%20Yuan%20and%20Yifeng%20Liu%20and%20Shuang%20Wu%20and%20Xun%20Zhou%20and%20Quanquan%20Gu%0AAbstract%3A%20%20%20Training%20deep%20neural%20networks--and%20more%20recently%2C%20large%20models--demands%0Aefficient%20and%20scalable%20optimizers.%20Adaptive%20gradient%20algorithms%20like%20Adam%2C%0AAdamW%2C%20and%20their%20variants%20have%20been%20central%20to%20this%20task.%20Despite%20the%0Adevelopment%20of%20numerous%20variance%20reduction%20algorithms%20in%20the%20past%20decade%20aimed%0Aat%20accelerating%20stochastic%20optimization%20in%20both%20convex%20and%20nonconvex%20settings%2C%0Avariance%20reduction%20has%20not%20found%20widespread%20success%20in%20training%20deep%20neural%0Anetworks%20or%20large%20language%20models.%20Consequently%2C%20it%20has%20remained%20a%20less%20favored%0Aapproach%20in%20modern%20AI.%20In%20this%20paper%2C%20to%20unleash%20the%20power%20of%20variance%0Areduction%20for%20efficient%20training%20of%20large%20models%2C%20we%20propose%20a%20unified%0Aoptimization%20framework%2C%20MARS%20%28Make%20vAriance%20Reduction%20Shine%29%2C%20which%20reconciles%0Apreconditioned%20gradient%20methods%20with%20variance%20reduction%20via%20a%20scaled%20stochastic%0Arecursive%20momentum%20technique.%20Within%20our%20framework%2C%20we%20introduce%20three%0Ainstances%20of%20MARS%20that%20leverage%20preconditioned%20gradient%20updates%20based%20on%20AdamW%2C%0ALion%2C%20and%20Shampoo%2C%20respectively.%20We%20also%20draw%20a%20connection%20between%20our%0Aalgorithms%20and%20existing%20optimizers.%20Experimental%20results%20on%20training%20GPT-2%0Amodels%20indicate%20that%20MARS%20consistently%20outperforms%20AdamW%20by%20a%20large%20margin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10438v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMARS%253A%2520Unleashing%2520the%2520Power%2520of%2520Variance%2520Reduction%2520for%2520Training%2520Large%250A%2520%2520Models%26entry.906535625%3DHuizhuo%2520Yuan%2520and%2520Yifeng%2520Liu%2520and%2520Shuang%2520Wu%2520and%2520Xun%2520Zhou%2520and%2520Quanquan%2520Gu%26entry.1292438233%3D%2520%2520Training%2520deep%2520neural%2520networks--and%2520more%2520recently%252C%2520large%2520models--demands%250Aefficient%2520and%2520scalable%2520optimizers.%2520Adaptive%2520gradient%2520algorithms%2520like%2520Adam%252C%250AAdamW%252C%2520and%2520their%2520variants%2520have%2520been%2520central%2520to%2520this%2520task.%2520Despite%2520the%250Adevelopment%2520of%2520numerous%2520variance%2520reduction%2520algorithms%2520in%2520the%2520past%2520decade%2520aimed%250Aat%2520accelerating%2520stochastic%2520optimization%2520in%2520both%2520convex%2520and%2520nonconvex%2520settings%252C%250Avariance%2520reduction%2520has%2520not%2520found%2520widespread%2520success%2520in%2520training%2520deep%2520neural%250Anetworks%2520or%2520large%2520language%2520models.%2520Consequently%252C%2520it%2520has%2520remained%2520a%2520less%2520favored%250Aapproach%2520in%2520modern%2520AI.%2520In%2520this%2520paper%252C%2520to%2520unleash%2520the%2520power%2520of%2520variance%250Areduction%2520for%2520efficient%2520training%2520of%2520large%2520models%252C%2520we%2520propose%2520a%2520unified%250Aoptimization%2520framework%252C%2520MARS%2520%2528Make%2520vAriance%2520Reduction%2520Shine%2529%252C%2520which%2520reconciles%250Apreconditioned%2520gradient%2520methods%2520with%2520variance%2520reduction%2520via%2520a%2520scaled%2520stochastic%250Arecursive%2520momentum%2520technique.%2520Within%2520our%2520framework%252C%2520we%2520introduce%2520three%250Ainstances%2520of%2520MARS%2520that%2520leverage%2520preconditioned%2520gradient%2520updates%2520based%2520on%2520AdamW%252C%250ALion%252C%2520and%2520Shampoo%252C%2520respectively.%2520We%2520also%2520draw%2520a%2520connection%2520between%2520our%250Aalgorithms%2520and%2520existing%2520optimizers.%2520Experimental%2520results%2520on%2520training%2520GPT-2%250Amodels%2520indicate%2520that%2520MARS%2520consistently%2520outperforms%2520AdamW%2520by%2520a%2520large%2520margin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10438v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MARS%3A%20Unleashing%20the%20Power%20of%20Variance%20Reduction%20for%20Training%20Large%0A%20%20Models&entry.906535625=Huizhuo%20Yuan%20and%20Yifeng%20Liu%20and%20Shuang%20Wu%20and%20Xun%20Zhou%20and%20Quanquan%20Gu&entry.1292438233=%20%20Training%20deep%20neural%20networks--and%20more%20recently%2C%20large%20models--demands%0Aefficient%20and%20scalable%20optimizers.%20Adaptive%20gradient%20algorithms%20like%20Adam%2C%0AAdamW%2C%20and%20their%20variants%20have%20been%20central%20to%20this%20task.%20Despite%20the%0Adevelopment%20of%20numerous%20variance%20reduction%20algorithms%20in%20the%20past%20decade%20aimed%0Aat%20accelerating%20stochastic%20optimization%20in%20both%20convex%20and%20nonconvex%20settings%2C%0Avariance%20reduction%20has%20not%20found%20widespread%20success%20in%20training%20deep%20neural%0Anetworks%20or%20large%20language%20models.%20Consequently%2C%20it%20has%20remained%20a%20less%20favored%0Aapproach%20in%20modern%20AI.%20In%20this%20paper%2C%20to%20unleash%20the%20power%20of%20variance%0Areduction%20for%20efficient%20training%20of%20large%20models%2C%20we%20propose%20a%20unified%0Aoptimization%20framework%2C%20MARS%20%28Make%20vAriance%20Reduction%20Shine%29%2C%20which%20reconciles%0Apreconditioned%20gradient%20methods%20with%20variance%20reduction%20via%20a%20scaled%20stochastic%0Arecursive%20momentum%20technique.%20Within%20our%20framework%2C%20we%20introduce%20three%0Ainstances%20of%20MARS%20that%20leverage%20preconditioned%20gradient%20updates%20based%20on%20AdamW%2C%0ALion%2C%20and%20Shampoo%2C%20respectively.%20We%20also%20draw%20a%20connection%20between%20our%0Aalgorithms%20and%20existing%20optimizers.%20Experimental%20results%20on%20training%20GPT-2%0Amodels%20indicate%20that%20MARS%20consistently%20outperforms%20AdamW%20by%20a%20large%20margin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10438v1&entry.124074799=Read"},
{"title": "Measuring Non-Adversarial Reproduction of Training Data in Large\n  Language Models", "author": "Michael Aerni and Javier Rando and Edoardo Debenedetti and Nicholas Carlini and Daphne Ippolito and Florian Tram\u00e8r", "abstract": "  Large language models memorize parts of their training data. Memorizing short\nsnippets and facts is required to answer questions about the world and to be\nfluent in any language. But models have also been shown to reproduce long\nverbatim sequences of memorized text when prompted by a motivated adversary. In\nthis work, we investigate an intermediate regime of memorization that we call\nnon-adversarial reproduction, where we quantify the overlap between model\nresponses and pretraining data when responding to natural and benign prompts.\nFor a variety of innocuous prompt categories (e.g., writing a letter or a\ntutorial), we show that up to 15% of the text output by popular conversational\nlanguage models overlaps with snippets from the Internet. In worst cases, we\nfind generations where 100% of the content can be found exactly online. For the\nsame tasks, we find that human-written text has far less overlap with Internet\ndata. We further study whether prompting strategies can close this reproduction\ngap between models and humans. While appropriate prompting can reduce\nnon-adversarial reproduction on average, we find that mitigating worst-case\nreproduction of training data requires stronger defenses -- even for benign\ninteractions.\n", "link": "http://arxiv.org/abs/2411.10242v1", "date": "2024-11-15", "relevancy": 1.955, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4907}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4907}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Measuring%20Non-Adversarial%20Reproduction%20of%20Training%20Data%20in%20Large%0A%20%20Language%20Models&body=Title%3A%20Measuring%20Non-Adversarial%20Reproduction%20of%20Training%20Data%20in%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Michael%20Aerni%20and%20Javier%20Rando%20and%20Edoardo%20Debenedetti%20and%20Nicholas%20Carlini%20and%20Daphne%20Ippolito%20and%20Florian%20Tram%C3%A8r%0AAbstract%3A%20%20%20Large%20language%20models%20memorize%20parts%20of%20their%20training%20data.%20Memorizing%20short%0Asnippets%20and%20facts%20is%20required%20to%20answer%20questions%20about%20the%20world%20and%20to%20be%0Afluent%20in%20any%20language.%20But%20models%20have%20also%20been%20shown%20to%20reproduce%20long%0Averbatim%20sequences%20of%20memorized%20text%20when%20prompted%20by%20a%20motivated%20adversary.%20In%0Athis%20work%2C%20we%20investigate%20an%20intermediate%20regime%20of%20memorization%20that%20we%20call%0Anon-adversarial%20reproduction%2C%20where%20we%20quantify%20the%20overlap%20between%20model%0Aresponses%20and%20pretraining%20data%20when%20responding%20to%20natural%20and%20benign%20prompts.%0AFor%20a%20variety%20of%20innocuous%20prompt%20categories%20%28e.g.%2C%20writing%20a%20letter%20or%20a%0Atutorial%29%2C%20we%20show%20that%20up%20to%2015%25%20of%20the%20text%20output%20by%20popular%20conversational%0Alanguage%20models%20overlaps%20with%20snippets%20from%20the%20Internet.%20In%20worst%20cases%2C%20we%0Afind%20generations%20where%20100%25%20of%20the%20content%20can%20be%20found%20exactly%20online.%20For%20the%0Asame%20tasks%2C%20we%20find%20that%20human-written%20text%20has%20far%20less%20overlap%20with%20Internet%0Adata.%20We%20further%20study%20whether%20prompting%20strategies%20can%20close%20this%20reproduction%0Agap%20between%20models%20and%20humans.%20While%20appropriate%20prompting%20can%20reduce%0Anon-adversarial%20reproduction%20on%20average%2C%20we%20find%20that%20mitigating%20worst-case%0Areproduction%20of%20training%20data%20requires%20stronger%20defenses%20--%20even%20for%20benign%0Ainteractions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10242v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeasuring%2520Non-Adversarial%2520Reproduction%2520of%2520Training%2520Data%2520in%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DMichael%2520Aerni%2520and%2520Javier%2520Rando%2520and%2520Edoardo%2520Debenedetti%2520and%2520Nicholas%2520Carlini%2520and%2520Daphne%2520Ippolito%2520and%2520Florian%2520Tram%25C3%25A8r%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520memorize%2520parts%2520of%2520their%2520training%2520data.%2520Memorizing%2520short%250Asnippets%2520and%2520facts%2520is%2520required%2520to%2520answer%2520questions%2520about%2520the%2520world%2520and%2520to%2520be%250Afluent%2520in%2520any%2520language.%2520But%2520models%2520have%2520also%2520been%2520shown%2520to%2520reproduce%2520long%250Averbatim%2520sequences%2520of%2520memorized%2520text%2520when%2520prompted%2520by%2520a%2520motivated%2520adversary.%2520In%250Athis%2520work%252C%2520we%2520investigate%2520an%2520intermediate%2520regime%2520of%2520memorization%2520that%2520we%2520call%250Anon-adversarial%2520reproduction%252C%2520where%2520we%2520quantify%2520the%2520overlap%2520between%2520model%250Aresponses%2520and%2520pretraining%2520data%2520when%2520responding%2520to%2520natural%2520and%2520benign%2520prompts.%250AFor%2520a%2520variety%2520of%2520innocuous%2520prompt%2520categories%2520%2528e.g.%252C%2520writing%2520a%2520letter%2520or%2520a%250Atutorial%2529%252C%2520we%2520show%2520that%2520up%2520to%252015%2525%2520of%2520the%2520text%2520output%2520by%2520popular%2520conversational%250Alanguage%2520models%2520overlaps%2520with%2520snippets%2520from%2520the%2520Internet.%2520In%2520worst%2520cases%252C%2520we%250Afind%2520generations%2520where%2520100%2525%2520of%2520the%2520content%2520can%2520be%2520found%2520exactly%2520online.%2520For%2520the%250Asame%2520tasks%252C%2520we%2520find%2520that%2520human-written%2520text%2520has%2520far%2520less%2520overlap%2520with%2520Internet%250Adata.%2520We%2520further%2520study%2520whether%2520prompting%2520strategies%2520can%2520close%2520this%2520reproduction%250Agap%2520between%2520models%2520and%2520humans.%2520While%2520appropriate%2520prompting%2520can%2520reduce%250Anon-adversarial%2520reproduction%2520on%2520average%252C%2520we%2520find%2520that%2520mitigating%2520worst-case%250Areproduction%2520of%2520training%2520data%2520requires%2520stronger%2520defenses%2520--%2520even%2520for%2520benign%250Ainteractions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10242v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Measuring%20Non-Adversarial%20Reproduction%20of%20Training%20Data%20in%20Large%0A%20%20Language%20Models&entry.906535625=Michael%20Aerni%20and%20Javier%20Rando%20and%20Edoardo%20Debenedetti%20and%20Nicholas%20Carlini%20and%20Daphne%20Ippolito%20and%20Florian%20Tram%C3%A8r&entry.1292438233=%20%20Large%20language%20models%20memorize%20parts%20of%20their%20training%20data.%20Memorizing%20short%0Asnippets%20and%20facts%20is%20required%20to%20answer%20questions%20about%20the%20world%20and%20to%20be%0Afluent%20in%20any%20language.%20But%20models%20have%20also%20been%20shown%20to%20reproduce%20long%0Averbatim%20sequences%20of%20memorized%20text%20when%20prompted%20by%20a%20motivated%20adversary.%20In%0Athis%20work%2C%20we%20investigate%20an%20intermediate%20regime%20of%20memorization%20that%20we%20call%0Anon-adversarial%20reproduction%2C%20where%20we%20quantify%20the%20overlap%20between%20model%0Aresponses%20and%20pretraining%20data%20when%20responding%20to%20natural%20and%20benign%20prompts.%0AFor%20a%20variety%20of%20innocuous%20prompt%20categories%20%28e.g.%2C%20writing%20a%20letter%20or%20a%0Atutorial%29%2C%20we%20show%20that%20up%20to%2015%25%20of%20the%20text%20output%20by%20popular%20conversational%0Alanguage%20models%20overlaps%20with%20snippets%20from%20the%20Internet.%20In%20worst%20cases%2C%20we%0Afind%20generations%20where%20100%25%20of%20the%20content%20can%20be%20found%20exactly%20online.%20For%20the%0Asame%20tasks%2C%20we%20find%20that%20human-written%20text%20has%20far%20less%20overlap%20with%20Internet%0Adata.%20We%20further%20study%20whether%20prompting%20strategies%20can%20close%20this%20reproduction%0Agap%20between%20models%20and%20humans.%20While%20appropriate%20prompting%20can%20reduce%0Anon-adversarial%20reproduction%20on%20average%2C%20we%20find%20that%20mitigating%20worst-case%0Areproduction%20of%20training%20data%20requires%20stronger%20defenses%20--%20even%20for%20benign%0Ainteractions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10242v1&entry.124074799=Read"},
{"title": "Recurrent Neural Goodness-of-Fit Test for Time Series", "author": "Aoran Zhang and Wenbin Zhou and Liyan Xie and Shixiang Zhu", "abstract": "  Time series data are crucial across diverse domains such as finance and\nhealthcare, where accurate forecasting and decision-making rely on advanced\nmodeling techniques. While generative models have shown great promise in\ncapturing the intricate dynamics inherent in time series, evaluating their\nperformance remains a major challenge. Traditional evaluation metrics fall\nshort due to the temporal dependencies and potential high dimensionality of the\nfeatures. In this paper, we propose the REcurrent NeurAL (RENAL)\nGoodness-of-Fit test, a novel and statistically rigorous framework for\nevaluating generative time series models. By leveraging recurrent neural\nnetworks, we transform the time series into conditionally independent data\npairs, enabling the application of a chi-square-based goodness-of-fit test to\nthe temporal dependencies within the data. This approach offers a robust,\ntheoretically grounded solution for assessing the quality of generative models,\nparticularly in settings with limited time sequences. We demonstrate the\nefficacy of our method across both synthetic and real-world datasets,\noutperforming existing methods in terms of reliability and accuracy. Our method\nfills a critical gap in the evaluation of time series generative models,\noffering a tool that is both practical and adaptable to high-stakes\napplications.\n", "link": "http://arxiv.org/abs/2410.13986v3", "date": "2024-11-15", "relevancy": 1.9344, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5041}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4854}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4736}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recurrent%20Neural%20Goodness-of-Fit%20Test%20for%20Time%20Series&body=Title%3A%20Recurrent%20Neural%20Goodness-of-Fit%20Test%20for%20Time%20Series%0AAuthor%3A%20Aoran%20Zhang%20and%20Wenbin%20Zhou%20and%20Liyan%20Xie%20and%20Shixiang%20Zhu%0AAbstract%3A%20%20%20Time%20series%20data%20are%20crucial%20across%20diverse%20domains%20such%20as%20finance%20and%0Ahealthcare%2C%20where%20accurate%20forecasting%20and%20decision-making%20rely%20on%20advanced%0Amodeling%20techniques.%20While%20generative%20models%20have%20shown%20great%20promise%20in%0Acapturing%20the%20intricate%20dynamics%20inherent%20in%20time%20series%2C%20evaluating%20their%0Aperformance%20remains%20a%20major%20challenge.%20Traditional%20evaluation%20metrics%20fall%0Ashort%20due%20to%20the%20temporal%20dependencies%20and%20potential%20high%20dimensionality%20of%20the%0Afeatures.%20In%20this%20paper%2C%20we%20propose%20the%20REcurrent%20NeurAL%20%28RENAL%29%0AGoodness-of-Fit%20test%2C%20a%20novel%20and%20statistically%20rigorous%20framework%20for%0Aevaluating%20generative%20time%20series%20models.%20By%20leveraging%20recurrent%20neural%0Anetworks%2C%20we%20transform%20the%20time%20series%20into%20conditionally%20independent%20data%0Apairs%2C%20enabling%20the%20application%20of%20a%20chi-square-based%20goodness-of-fit%20test%20to%0Athe%20temporal%20dependencies%20within%20the%20data.%20This%20approach%20offers%20a%20robust%2C%0Atheoretically%20grounded%20solution%20for%20assessing%20the%20quality%20of%20generative%20models%2C%0Aparticularly%20in%20settings%20with%20limited%20time%20sequences.%20We%20demonstrate%20the%0Aefficacy%20of%20our%20method%20across%20both%20synthetic%20and%20real-world%20datasets%2C%0Aoutperforming%20existing%20methods%20in%20terms%20of%20reliability%20and%20accuracy.%20Our%20method%0Afills%20a%20critical%20gap%20in%20the%20evaluation%20of%20time%20series%20generative%20models%2C%0Aoffering%20a%20tool%20that%20is%20both%20practical%20and%20adaptable%20to%20high-stakes%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13986v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecurrent%2520Neural%2520Goodness-of-Fit%2520Test%2520for%2520Time%2520Series%26entry.906535625%3DAoran%2520Zhang%2520and%2520Wenbin%2520Zhou%2520and%2520Liyan%2520Xie%2520and%2520Shixiang%2520Zhu%26entry.1292438233%3D%2520%2520Time%2520series%2520data%2520are%2520crucial%2520across%2520diverse%2520domains%2520such%2520as%2520finance%2520and%250Ahealthcare%252C%2520where%2520accurate%2520forecasting%2520and%2520decision-making%2520rely%2520on%2520advanced%250Amodeling%2520techniques.%2520While%2520generative%2520models%2520have%2520shown%2520great%2520promise%2520in%250Acapturing%2520the%2520intricate%2520dynamics%2520inherent%2520in%2520time%2520series%252C%2520evaluating%2520their%250Aperformance%2520remains%2520a%2520major%2520challenge.%2520Traditional%2520evaluation%2520metrics%2520fall%250Ashort%2520due%2520to%2520the%2520temporal%2520dependencies%2520and%2520potential%2520high%2520dimensionality%2520of%2520the%250Afeatures.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%2520REcurrent%2520NeurAL%2520%2528RENAL%2529%250AGoodness-of-Fit%2520test%252C%2520a%2520novel%2520and%2520statistically%2520rigorous%2520framework%2520for%250Aevaluating%2520generative%2520time%2520series%2520models.%2520By%2520leveraging%2520recurrent%2520neural%250Anetworks%252C%2520we%2520transform%2520the%2520time%2520series%2520into%2520conditionally%2520independent%2520data%250Apairs%252C%2520enabling%2520the%2520application%2520of%2520a%2520chi-square-based%2520goodness-of-fit%2520test%2520to%250Athe%2520temporal%2520dependencies%2520within%2520the%2520data.%2520This%2520approach%2520offers%2520a%2520robust%252C%250Atheoretically%2520grounded%2520solution%2520for%2520assessing%2520the%2520quality%2520of%2520generative%2520models%252C%250Aparticularly%2520in%2520settings%2520with%2520limited%2520time%2520sequences.%2520We%2520demonstrate%2520the%250Aefficacy%2520of%2520our%2520method%2520across%2520both%2520synthetic%2520and%2520real-world%2520datasets%252C%250Aoutperforming%2520existing%2520methods%2520in%2520terms%2520of%2520reliability%2520and%2520accuracy.%2520Our%2520method%250Afills%2520a%2520critical%2520gap%2520in%2520the%2520evaluation%2520of%2520time%2520series%2520generative%2520models%252C%250Aoffering%2520a%2520tool%2520that%2520is%2520both%2520practical%2520and%2520adaptable%2520to%2520high-stakes%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13986v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recurrent%20Neural%20Goodness-of-Fit%20Test%20for%20Time%20Series&entry.906535625=Aoran%20Zhang%20and%20Wenbin%20Zhou%20and%20Liyan%20Xie%20and%20Shixiang%20Zhu&entry.1292438233=%20%20Time%20series%20data%20are%20crucial%20across%20diverse%20domains%20such%20as%20finance%20and%0Ahealthcare%2C%20where%20accurate%20forecasting%20and%20decision-making%20rely%20on%20advanced%0Amodeling%20techniques.%20While%20generative%20models%20have%20shown%20great%20promise%20in%0Acapturing%20the%20intricate%20dynamics%20inherent%20in%20time%20series%2C%20evaluating%20their%0Aperformance%20remains%20a%20major%20challenge.%20Traditional%20evaluation%20metrics%20fall%0Ashort%20due%20to%20the%20temporal%20dependencies%20and%20potential%20high%20dimensionality%20of%20the%0Afeatures.%20In%20this%20paper%2C%20we%20propose%20the%20REcurrent%20NeurAL%20%28RENAL%29%0AGoodness-of-Fit%20test%2C%20a%20novel%20and%20statistically%20rigorous%20framework%20for%0Aevaluating%20generative%20time%20series%20models.%20By%20leveraging%20recurrent%20neural%0Anetworks%2C%20we%20transform%20the%20time%20series%20into%20conditionally%20independent%20data%0Apairs%2C%20enabling%20the%20application%20of%20a%20chi-square-based%20goodness-of-fit%20test%20to%0Athe%20temporal%20dependencies%20within%20the%20data.%20This%20approach%20offers%20a%20robust%2C%0Atheoretically%20grounded%20solution%20for%20assessing%20the%20quality%20of%20generative%20models%2C%0Aparticularly%20in%20settings%20with%20limited%20time%20sequences.%20We%20demonstrate%20the%0Aefficacy%20of%20our%20method%20across%20both%20synthetic%20and%20real-world%20datasets%2C%0Aoutperforming%20existing%20methods%20in%20terms%20of%20reliability%20and%20accuracy.%20Our%20method%0Afills%20a%20critical%20gap%20in%20the%20evaluation%20of%20time%20series%20generative%20models%2C%0Aoffering%20a%20tool%20that%20is%20both%20practical%20and%20adaptable%20to%20high-stakes%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13986v3&entry.124074799=Read"},
{"title": "Improved Canonicalization for Model Agnostic Equivariance", "author": "Siba Smarak Panigrahi and Arnab Kumar Mondal", "abstract": "  This work introduces a novel approach to achieving architecture-agnostic\nequivariance in deep learning, particularly addressing the limitations of\ntraditional layerwise equivariant architectures and the inefficiencies of the\nexisting architecture-agnostic methods. Building equivariant models using\ntraditional methods requires designing equivariant versions of existing models\nand training them from scratch, a process that is both impractical and\nresource-intensive. Canonicalization has emerged as a promising alternative for\ninducing equivariance without altering model architecture, but it suffers from\nthe need for highly expressive and expensive equivariant networks to learn\ncanonical orientations accurately. We propose a new optimization-based method\nthat employs any non-equivariant network for canonicalization. Our method uses\ncontrastive learning to efficiently learn a canonical orientation and offers\nmore flexibility for the choice of canonicalization network. We empirically\ndemonstrate that this approach outperforms existing methods in achieving\nequivariance for large pretrained models and significantly speeds up the\ncanonicalization process, making it up to 2 times faster.\n", "link": "http://arxiv.org/abs/2405.14089v2", "date": "2024-11-15", "relevancy": 1.9246, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5014}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4778}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4764}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20Canonicalization%20for%20Model%20Agnostic%20Equivariance&body=Title%3A%20Improved%20Canonicalization%20for%20Model%20Agnostic%20Equivariance%0AAuthor%3A%20Siba%20Smarak%20Panigrahi%20and%20Arnab%20Kumar%20Mondal%0AAbstract%3A%20%20%20This%20work%20introduces%20a%20novel%20approach%20to%20achieving%20architecture-agnostic%0Aequivariance%20in%20deep%20learning%2C%20particularly%20addressing%20the%20limitations%20of%0Atraditional%20layerwise%20equivariant%20architectures%20and%20the%20inefficiencies%20of%20the%0Aexisting%20architecture-agnostic%20methods.%20Building%20equivariant%20models%20using%0Atraditional%20methods%20requires%20designing%20equivariant%20versions%20of%20existing%20models%0Aand%20training%20them%20from%20scratch%2C%20a%20process%20that%20is%20both%20impractical%20and%0Aresource-intensive.%20Canonicalization%20has%20emerged%20as%20a%20promising%20alternative%20for%0Ainducing%20equivariance%20without%20altering%20model%20architecture%2C%20but%20it%20suffers%20from%0Athe%20need%20for%20highly%20expressive%20and%20expensive%20equivariant%20networks%20to%20learn%0Acanonical%20orientations%20accurately.%20We%20propose%20a%20new%20optimization-based%20method%0Athat%20employs%20any%20non-equivariant%20network%20for%20canonicalization.%20Our%20method%20uses%0Acontrastive%20learning%20to%20efficiently%20learn%20a%20canonical%20orientation%20and%20offers%0Amore%20flexibility%20for%20the%20choice%20of%20canonicalization%20network.%20We%20empirically%0Ademonstrate%20that%20this%20approach%20outperforms%20existing%20methods%20in%20achieving%0Aequivariance%20for%20large%20pretrained%20models%20and%20significantly%20speeds%20up%20the%0Acanonicalization%20process%2C%20making%20it%20up%20to%202%20times%20faster.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14089v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520Canonicalization%2520for%2520Model%2520Agnostic%2520Equivariance%26entry.906535625%3DSiba%2520Smarak%2520Panigrahi%2520and%2520Arnab%2520Kumar%2520Mondal%26entry.1292438233%3D%2520%2520This%2520work%2520introduces%2520a%2520novel%2520approach%2520to%2520achieving%2520architecture-agnostic%250Aequivariance%2520in%2520deep%2520learning%252C%2520particularly%2520addressing%2520the%2520limitations%2520of%250Atraditional%2520layerwise%2520equivariant%2520architectures%2520and%2520the%2520inefficiencies%2520of%2520the%250Aexisting%2520architecture-agnostic%2520methods.%2520Building%2520equivariant%2520models%2520using%250Atraditional%2520methods%2520requires%2520designing%2520equivariant%2520versions%2520of%2520existing%2520models%250Aand%2520training%2520them%2520from%2520scratch%252C%2520a%2520process%2520that%2520is%2520both%2520impractical%2520and%250Aresource-intensive.%2520Canonicalization%2520has%2520emerged%2520as%2520a%2520promising%2520alternative%2520for%250Ainducing%2520equivariance%2520without%2520altering%2520model%2520architecture%252C%2520but%2520it%2520suffers%2520from%250Athe%2520need%2520for%2520highly%2520expressive%2520and%2520expensive%2520equivariant%2520networks%2520to%2520learn%250Acanonical%2520orientations%2520accurately.%2520We%2520propose%2520a%2520new%2520optimization-based%2520method%250Athat%2520employs%2520any%2520non-equivariant%2520network%2520for%2520canonicalization.%2520Our%2520method%2520uses%250Acontrastive%2520learning%2520to%2520efficiently%2520learn%2520a%2520canonical%2520orientation%2520and%2520offers%250Amore%2520flexibility%2520for%2520the%2520choice%2520of%2520canonicalization%2520network.%2520We%2520empirically%250Ademonstrate%2520that%2520this%2520approach%2520outperforms%2520existing%2520methods%2520in%2520achieving%250Aequivariance%2520for%2520large%2520pretrained%2520models%2520and%2520significantly%2520speeds%2520up%2520the%250Acanonicalization%2520process%252C%2520making%2520it%2520up%2520to%25202%2520times%2520faster.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14089v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20Canonicalization%20for%20Model%20Agnostic%20Equivariance&entry.906535625=Siba%20Smarak%20Panigrahi%20and%20Arnab%20Kumar%20Mondal&entry.1292438233=%20%20This%20work%20introduces%20a%20novel%20approach%20to%20achieving%20architecture-agnostic%0Aequivariance%20in%20deep%20learning%2C%20particularly%20addressing%20the%20limitations%20of%0Atraditional%20layerwise%20equivariant%20architectures%20and%20the%20inefficiencies%20of%20the%0Aexisting%20architecture-agnostic%20methods.%20Building%20equivariant%20models%20using%0Atraditional%20methods%20requires%20designing%20equivariant%20versions%20of%20existing%20models%0Aand%20training%20them%20from%20scratch%2C%20a%20process%20that%20is%20both%20impractical%20and%0Aresource-intensive.%20Canonicalization%20has%20emerged%20as%20a%20promising%20alternative%20for%0Ainducing%20equivariance%20without%20altering%20model%20architecture%2C%20but%20it%20suffers%20from%0Athe%20need%20for%20highly%20expressive%20and%20expensive%20equivariant%20networks%20to%20learn%0Acanonical%20orientations%20accurately.%20We%20propose%20a%20new%20optimization-based%20method%0Athat%20employs%20any%20non-equivariant%20network%20for%20canonicalization.%20Our%20method%20uses%0Acontrastive%20learning%20to%20efficiently%20learn%20a%20canonical%20orientation%20and%20offers%0Amore%20flexibility%20for%20the%20choice%20of%20canonicalization%20network.%20We%20empirically%0Ademonstrate%20that%20this%20approach%20outperforms%20existing%20methods%20in%20achieving%0Aequivariance%20for%20large%20pretrained%20models%20and%20significantly%20speeds%20up%20the%0Acanonicalization%20process%2C%20making%20it%20up%20to%202%20times%20faster.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14089v2&entry.124074799=Read"},
{"title": "A Survey of Event Causality Identification: Principles, Taxonomy,\n  Challenges, and Assessment", "author": "Zefan Zeng and Qing Cheng and Xingchen Hu and Yuehang Si and Zhong Liu", "abstract": "  Event Causality Identification (ECI) has become a crucial task in Natural\nLanguage Processing (NLP), aimed at automatically extracting causalities from\ntextual data. In this survey, we systematically address the foundational\nprinciples, technical frameworks, and challenges of ECI, offering a\ncomprehensive taxonomy to categorize and clarify current research\nmethodologies, as well as a quantitative assessment of existing models. We\nfirst establish a conceptual framework for ECI, outlining key definitions,\nproblem formulations, and evaluation standards. Our taxonomy classifies ECI\nmethods according to the two primary tasks of sentence-level (SECI) and\ndocument-level (DECI) event causality identification. For SECI, we examine\nfeature pattern-based matching, deep semantic encoding, causal knowledge\npre-training and prompt-based fine-tuning, and external knowledge enhancement\nmethods. For DECI, we highlight approaches focused on event graph reasoning and\nprompt-based techniques to address the complexity of cross-sentence causal\ninference. Additionally, we analyze the strengths, limitations, and open\nchallenges of each approach. We further conduct an extensive quantitative\nevaluation of various ECI methods on two benchmark datasets. Finally, we\nexplore future research directions, highlighting promising pathways to overcome\ncurrent limitations and broaden ECI applications.\n", "link": "http://arxiv.org/abs/2411.10371v1", "date": "2024-11-15", "relevancy": 1.9194, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4882}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4882}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Event%20Causality%20Identification%3A%20Principles%2C%20Taxonomy%2C%0A%20%20Challenges%2C%20and%20Assessment&body=Title%3A%20A%20Survey%20of%20Event%20Causality%20Identification%3A%20Principles%2C%20Taxonomy%2C%0A%20%20Challenges%2C%20and%20Assessment%0AAuthor%3A%20Zefan%20Zeng%20and%20Qing%20Cheng%20and%20Xingchen%20Hu%20and%20Yuehang%20Si%20and%20Zhong%20Liu%0AAbstract%3A%20%20%20Event%20Causality%20Identification%20%28ECI%29%20has%20become%20a%20crucial%20task%20in%20Natural%0ALanguage%20Processing%20%28NLP%29%2C%20aimed%20at%20automatically%20extracting%20causalities%20from%0Atextual%20data.%20In%20this%20survey%2C%20we%20systematically%20address%20the%20foundational%0Aprinciples%2C%20technical%20frameworks%2C%20and%20challenges%20of%20ECI%2C%20offering%20a%0Acomprehensive%20taxonomy%20to%20categorize%20and%20clarify%20current%20research%0Amethodologies%2C%20as%20well%20as%20a%20quantitative%20assessment%20of%20existing%20models.%20We%0Afirst%20establish%20a%20conceptual%20framework%20for%20ECI%2C%20outlining%20key%20definitions%2C%0Aproblem%20formulations%2C%20and%20evaluation%20standards.%20Our%20taxonomy%20classifies%20ECI%0Amethods%20according%20to%20the%20two%20primary%20tasks%20of%20sentence-level%20%28SECI%29%20and%0Adocument-level%20%28DECI%29%20event%20causality%20identification.%20For%20SECI%2C%20we%20examine%0Afeature%20pattern-based%20matching%2C%20deep%20semantic%20encoding%2C%20causal%20knowledge%0Apre-training%20and%20prompt-based%20fine-tuning%2C%20and%20external%20knowledge%20enhancement%0Amethods.%20For%20DECI%2C%20we%20highlight%20approaches%20focused%20on%20event%20graph%20reasoning%20and%0Aprompt-based%20techniques%20to%20address%20the%20complexity%20of%20cross-sentence%20causal%0Ainference.%20Additionally%2C%20we%20analyze%20the%20strengths%2C%20limitations%2C%20and%20open%0Achallenges%20of%20each%20approach.%20We%20further%20conduct%20an%20extensive%20quantitative%0Aevaluation%20of%20various%20ECI%20methods%20on%20two%20benchmark%20datasets.%20Finally%2C%20we%0Aexplore%20future%20research%20directions%2C%20highlighting%20promising%20pathways%20to%20overcome%0Acurrent%20limitations%20and%20broaden%20ECI%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10371v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520Event%2520Causality%2520Identification%253A%2520Principles%252C%2520Taxonomy%252C%250A%2520%2520Challenges%252C%2520and%2520Assessment%26entry.906535625%3DZefan%2520Zeng%2520and%2520Qing%2520Cheng%2520and%2520Xingchen%2520Hu%2520and%2520Yuehang%2520Si%2520and%2520Zhong%2520Liu%26entry.1292438233%3D%2520%2520Event%2520Causality%2520Identification%2520%2528ECI%2529%2520has%2520become%2520a%2520crucial%2520task%2520in%2520Natural%250ALanguage%2520Processing%2520%2528NLP%2529%252C%2520aimed%2520at%2520automatically%2520extracting%2520causalities%2520from%250Atextual%2520data.%2520In%2520this%2520survey%252C%2520we%2520systematically%2520address%2520the%2520foundational%250Aprinciples%252C%2520technical%2520frameworks%252C%2520and%2520challenges%2520of%2520ECI%252C%2520offering%2520a%250Acomprehensive%2520taxonomy%2520to%2520categorize%2520and%2520clarify%2520current%2520research%250Amethodologies%252C%2520as%2520well%2520as%2520a%2520quantitative%2520assessment%2520of%2520existing%2520models.%2520We%250Afirst%2520establish%2520a%2520conceptual%2520framework%2520for%2520ECI%252C%2520outlining%2520key%2520definitions%252C%250Aproblem%2520formulations%252C%2520and%2520evaluation%2520standards.%2520Our%2520taxonomy%2520classifies%2520ECI%250Amethods%2520according%2520to%2520the%2520two%2520primary%2520tasks%2520of%2520sentence-level%2520%2528SECI%2529%2520and%250Adocument-level%2520%2528DECI%2529%2520event%2520causality%2520identification.%2520For%2520SECI%252C%2520we%2520examine%250Afeature%2520pattern-based%2520matching%252C%2520deep%2520semantic%2520encoding%252C%2520causal%2520knowledge%250Apre-training%2520and%2520prompt-based%2520fine-tuning%252C%2520and%2520external%2520knowledge%2520enhancement%250Amethods.%2520For%2520DECI%252C%2520we%2520highlight%2520approaches%2520focused%2520on%2520event%2520graph%2520reasoning%2520and%250Aprompt-based%2520techniques%2520to%2520address%2520the%2520complexity%2520of%2520cross-sentence%2520causal%250Ainference.%2520Additionally%252C%2520we%2520analyze%2520the%2520strengths%252C%2520limitations%252C%2520and%2520open%250Achallenges%2520of%2520each%2520approach.%2520We%2520further%2520conduct%2520an%2520extensive%2520quantitative%250Aevaluation%2520of%2520various%2520ECI%2520methods%2520on%2520two%2520benchmark%2520datasets.%2520Finally%252C%2520we%250Aexplore%2520future%2520research%2520directions%252C%2520highlighting%2520promising%2520pathways%2520to%2520overcome%250Acurrent%2520limitations%2520and%2520broaden%2520ECI%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10371v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Event%20Causality%20Identification%3A%20Principles%2C%20Taxonomy%2C%0A%20%20Challenges%2C%20and%20Assessment&entry.906535625=Zefan%20Zeng%20and%20Qing%20Cheng%20and%20Xingchen%20Hu%20and%20Yuehang%20Si%20and%20Zhong%20Liu&entry.1292438233=%20%20Event%20Causality%20Identification%20%28ECI%29%20has%20become%20a%20crucial%20task%20in%20Natural%0ALanguage%20Processing%20%28NLP%29%2C%20aimed%20at%20automatically%20extracting%20causalities%20from%0Atextual%20data.%20In%20this%20survey%2C%20we%20systematically%20address%20the%20foundational%0Aprinciples%2C%20technical%20frameworks%2C%20and%20challenges%20of%20ECI%2C%20offering%20a%0Acomprehensive%20taxonomy%20to%20categorize%20and%20clarify%20current%20research%0Amethodologies%2C%20as%20well%20as%20a%20quantitative%20assessment%20of%20existing%20models.%20We%0Afirst%20establish%20a%20conceptual%20framework%20for%20ECI%2C%20outlining%20key%20definitions%2C%0Aproblem%20formulations%2C%20and%20evaluation%20standards.%20Our%20taxonomy%20classifies%20ECI%0Amethods%20according%20to%20the%20two%20primary%20tasks%20of%20sentence-level%20%28SECI%29%20and%0Adocument-level%20%28DECI%29%20event%20causality%20identification.%20For%20SECI%2C%20we%20examine%0Afeature%20pattern-based%20matching%2C%20deep%20semantic%20encoding%2C%20causal%20knowledge%0Apre-training%20and%20prompt-based%20fine-tuning%2C%20and%20external%20knowledge%20enhancement%0Amethods.%20For%20DECI%2C%20we%20highlight%20approaches%20focused%20on%20event%20graph%20reasoning%20and%0Aprompt-based%20techniques%20to%20address%20the%20complexity%20of%20cross-sentence%20causal%0Ainference.%20Additionally%2C%20we%20analyze%20the%20strengths%2C%20limitations%2C%20and%20open%0Achallenges%20of%20each%20approach.%20We%20further%20conduct%20an%20extensive%20quantitative%0Aevaluation%20of%20various%20ECI%20methods%20on%20two%20benchmark%20datasets.%20Finally%2C%20we%0Aexplore%20future%20research%20directions%2C%20highlighting%20promising%20pathways%20to%20overcome%0Acurrent%20limitations%20and%20broaden%20ECI%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10371v1&entry.124074799=Read"},
{"title": "MDHP-Net: Detecting Injection Attacks on In-vehicle Network using\n  Multi-Dimensional Hawkes Process and Temporal Model", "author": "Qi Liu and Yanchen Liu and Ruifeng Li and Chenhong Cao and Yufeng Li and Xingyu Li and Peng Wang and Runhan Feng", "abstract": "  The integration of intelligent and connected technologies in modern vehicles,\nwhile offering enhanced functionalities through Electronic Control Unit and\ninterfaces like OBD-II and telematics, also exposes the vehicle's in-vehicle\nnetwork (IVN) to potential cyberattacks. In this paper, we consider a specific\ntype of cyberattack known as the injection attack. As demonstrated by empirical\ndata from real-world cybersecurity adversarial competitions(available at\nhttps://mimic2024.xctf.org.cn/race/qwmimic2024 ), these injection attacks have\nexcitation effect over time, gradually manipulating network traffic and\ndisrupting the vehicle's normal functioning, ultimately compromising both its\nstability and safety. To profile the abnormal behavior of attackers, we propose\na novel injection attack detector to extract long-term features of attack\nbehavior. Specifically, we first provide a theoretical analysis of modeling the\ntime-excitation effects of the attack using Multi-Dimensional Hawkes Process\n(MDHP). A gradient descent solver specifically tailored for MDHP, MDHP-GDS, is\ndeveloped to accurately estimate optimal MDHP parameters. We then propose an\ninjection attack detector, MDHP-Net, which integrates optimal MDHP parameters\nwith MDHP-LSTM blocks to enhance temporal feature extraction. By introducing\nMDHP parameters, MDHP-Net captures complex temporal features that standard Long\nShort-Term Memory (LSTM) cannot, enriching temporal dependencies within our\ncustomized structure. Extensive evaluations demonstrate the effectiveness of\nour proposed detection approach.\n", "link": "http://arxiv.org/abs/2411.10258v1", "date": "2024-11-15", "relevancy": 1.9092, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4893}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4718}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MDHP-Net%3A%20Detecting%20Injection%20Attacks%20on%20In-vehicle%20Network%20using%0A%20%20Multi-Dimensional%20Hawkes%20Process%20and%20Temporal%20Model&body=Title%3A%20MDHP-Net%3A%20Detecting%20Injection%20Attacks%20on%20In-vehicle%20Network%20using%0A%20%20Multi-Dimensional%20Hawkes%20Process%20and%20Temporal%20Model%0AAuthor%3A%20Qi%20Liu%20and%20Yanchen%20Liu%20and%20Ruifeng%20Li%20and%20Chenhong%20Cao%20and%20Yufeng%20Li%20and%20Xingyu%20Li%20and%20Peng%20Wang%20and%20Runhan%20Feng%0AAbstract%3A%20%20%20The%20integration%20of%20intelligent%20and%20connected%20technologies%20in%20modern%20vehicles%2C%0Awhile%20offering%20enhanced%20functionalities%20through%20Electronic%20Control%20Unit%20and%0Ainterfaces%20like%20OBD-II%20and%20telematics%2C%20also%20exposes%20the%20vehicle%27s%20in-vehicle%0Anetwork%20%28IVN%29%20to%20potential%20cyberattacks.%20In%20this%20paper%2C%20we%20consider%20a%20specific%0Atype%20of%20cyberattack%20known%20as%20the%20injection%20attack.%20As%20demonstrated%20by%20empirical%0Adata%20from%20real-world%20cybersecurity%20adversarial%20competitions%28available%20at%0Ahttps%3A//mimic2024.xctf.org.cn/race/qwmimic2024%20%29%2C%20these%20injection%20attacks%20have%0Aexcitation%20effect%20over%20time%2C%20gradually%20manipulating%20network%20traffic%20and%0Adisrupting%20the%20vehicle%27s%20normal%20functioning%2C%20ultimately%20compromising%20both%20its%0Astability%20and%20safety.%20To%20profile%20the%20abnormal%20behavior%20of%20attackers%2C%20we%20propose%0Aa%20novel%20injection%20attack%20detector%20to%20extract%20long-term%20features%20of%20attack%0Abehavior.%20Specifically%2C%20we%20first%20provide%20a%20theoretical%20analysis%20of%20modeling%20the%0Atime-excitation%20effects%20of%20the%20attack%20using%20Multi-Dimensional%20Hawkes%20Process%0A%28MDHP%29.%20A%20gradient%20descent%20solver%20specifically%20tailored%20for%20MDHP%2C%20MDHP-GDS%2C%20is%0Adeveloped%20to%20accurately%20estimate%20optimal%20MDHP%20parameters.%20We%20then%20propose%20an%0Ainjection%20attack%20detector%2C%20MDHP-Net%2C%20which%20integrates%20optimal%20MDHP%20parameters%0Awith%20MDHP-LSTM%20blocks%20to%20enhance%20temporal%20feature%20extraction.%20By%20introducing%0AMDHP%20parameters%2C%20MDHP-Net%20captures%20complex%20temporal%20features%20that%20standard%20Long%0AShort-Term%20Memory%20%28LSTM%29%20cannot%2C%20enriching%20temporal%20dependencies%20within%20our%0Acustomized%20structure.%20Extensive%20evaluations%20demonstrate%20the%20effectiveness%20of%0Aour%20proposed%20detection%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10258v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMDHP-Net%253A%2520Detecting%2520Injection%2520Attacks%2520on%2520In-vehicle%2520Network%2520using%250A%2520%2520Multi-Dimensional%2520Hawkes%2520Process%2520and%2520Temporal%2520Model%26entry.906535625%3DQi%2520Liu%2520and%2520Yanchen%2520Liu%2520and%2520Ruifeng%2520Li%2520and%2520Chenhong%2520Cao%2520and%2520Yufeng%2520Li%2520and%2520Xingyu%2520Li%2520and%2520Peng%2520Wang%2520and%2520Runhan%2520Feng%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520intelligent%2520and%2520connected%2520technologies%2520in%2520modern%2520vehicles%252C%250Awhile%2520offering%2520enhanced%2520functionalities%2520through%2520Electronic%2520Control%2520Unit%2520and%250Ainterfaces%2520like%2520OBD-II%2520and%2520telematics%252C%2520also%2520exposes%2520the%2520vehicle%2527s%2520in-vehicle%250Anetwork%2520%2528IVN%2529%2520to%2520potential%2520cyberattacks.%2520In%2520this%2520paper%252C%2520we%2520consider%2520a%2520specific%250Atype%2520of%2520cyberattack%2520known%2520as%2520the%2520injection%2520attack.%2520As%2520demonstrated%2520by%2520empirical%250Adata%2520from%2520real-world%2520cybersecurity%2520adversarial%2520competitions%2528available%2520at%250Ahttps%253A//mimic2024.xctf.org.cn/race/qwmimic2024%2520%2529%252C%2520these%2520injection%2520attacks%2520have%250Aexcitation%2520effect%2520over%2520time%252C%2520gradually%2520manipulating%2520network%2520traffic%2520and%250Adisrupting%2520the%2520vehicle%2527s%2520normal%2520functioning%252C%2520ultimately%2520compromising%2520both%2520its%250Astability%2520and%2520safety.%2520To%2520profile%2520the%2520abnormal%2520behavior%2520of%2520attackers%252C%2520we%2520propose%250Aa%2520novel%2520injection%2520attack%2520detector%2520to%2520extract%2520long-term%2520features%2520of%2520attack%250Abehavior.%2520Specifically%252C%2520we%2520first%2520provide%2520a%2520theoretical%2520analysis%2520of%2520modeling%2520the%250Atime-excitation%2520effects%2520of%2520the%2520attack%2520using%2520Multi-Dimensional%2520Hawkes%2520Process%250A%2528MDHP%2529.%2520A%2520gradient%2520descent%2520solver%2520specifically%2520tailored%2520for%2520MDHP%252C%2520MDHP-GDS%252C%2520is%250Adeveloped%2520to%2520accurately%2520estimate%2520optimal%2520MDHP%2520parameters.%2520We%2520then%2520propose%2520an%250Ainjection%2520attack%2520detector%252C%2520MDHP-Net%252C%2520which%2520integrates%2520optimal%2520MDHP%2520parameters%250Awith%2520MDHP-LSTM%2520blocks%2520to%2520enhance%2520temporal%2520feature%2520extraction.%2520By%2520introducing%250AMDHP%2520parameters%252C%2520MDHP-Net%2520captures%2520complex%2520temporal%2520features%2520that%2520standard%2520Long%250AShort-Term%2520Memory%2520%2528LSTM%2529%2520cannot%252C%2520enriching%2520temporal%2520dependencies%2520within%2520our%250Acustomized%2520structure.%2520Extensive%2520evaluations%2520demonstrate%2520the%2520effectiveness%2520of%250Aour%2520proposed%2520detection%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10258v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MDHP-Net%3A%20Detecting%20Injection%20Attacks%20on%20In-vehicle%20Network%20using%0A%20%20Multi-Dimensional%20Hawkes%20Process%20and%20Temporal%20Model&entry.906535625=Qi%20Liu%20and%20Yanchen%20Liu%20and%20Ruifeng%20Li%20and%20Chenhong%20Cao%20and%20Yufeng%20Li%20and%20Xingyu%20Li%20and%20Peng%20Wang%20and%20Runhan%20Feng&entry.1292438233=%20%20The%20integration%20of%20intelligent%20and%20connected%20technologies%20in%20modern%20vehicles%2C%0Awhile%20offering%20enhanced%20functionalities%20through%20Electronic%20Control%20Unit%20and%0Ainterfaces%20like%20OBD-II%20and%20telematics%2C%20also%20exposes%20the%20vehicle%27s%20in-vehicle%0Anetwork%20%28IVN%29%20to%20potential%20cyberattacks.%20In%20this%20paper%2C%20we%20consider%20a%20specific%0Atype%20of%20cyberattack%20known%20as%20the%20injection%20attack.%20As%20demonstrated%20by%20empirical%0Adata%20from%20real-world%20cybersecurity%20adversarial%20competitions%28available%20at%0Ahttps%3A//mimic2024.xctf.org.cn/race/qwmimic2024%20%29%2C%20these%20injection%20attacks%20have%0Aexcitation%20effect%20over%20time%2C%20gradually%20manipulating%20network%20traffic%20and%0Adisrupting%20the%20vehicle%27s%20normal%20functioning%2C%20ultimately%20compromising%20both%20its%0Astability%20and%20safety.%20To%20profile%20the%20abnormal%20behavior%20of%20attackers%2C%20we%20propose%0Aa%20novel%20injection%20attack%20detector%20to%20extract%20long-term%20features%20of%20attack%0Abehavior.%20Specifically%2C%20we%20first%20provide%20a%20theoretical%20analysis%20of%20modeling%20the%0Atime-excitation%20effects%20of%20the%20attack%20using%20Multi-Dimensional%20Hawkes%20Process%0A%28MDHP%29.%20A%20gradient%20descent%20solver%20specifically%20tailored%20for%20MDHP%2C%20MDHP-GDS%2C%20is%0Adeveloped%20to%20accurately%20estimate%20optimal%20MDHP%20parameters.%20We%20then%20propose%20an%0Ainjection%20attack%20detector%2C%20MDHP-Net%2C%20which%20integrates%20optimal%20MDHP%20parameters%0Awith%20MDHP-LSTM%20blocks%20to%20enhance%20temporal%20feature%20extraction.%20By%20introducing%0AMDHP%20parameters%2C%20MDHP-Net%20captures%20complex%20temporal%20features%20that%20standard%20Long%0AShort-Term%20Memory%20%28LSTM%29%20cannot%2C%20enriching%20temporal%20dependencies%20within%20our%0Acustomized%20structure.%20Extensive%20evaluations%20demonstrate%20the%20effectiveness%20of%0Aour%20proposed%20detection%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10258v1&entry.124074799=Read"},
{"title": "Artificial Intelligence in Pediatric Echocardiography: Exploring\n  Challenges, Opportunities, and Clinical Applications with Explainable AI and\n  Federated Learning", "author": "Mohammed Yaseen Jabarulla and Theodor Uden and Thomas Jack and Philipp Beerbaum and Steffen Oeltze-Jafra", "abstract": "  Pediatric heart diseases present a broad spectrum of congenital and acquired\ndiseases. More complex congenital malformations require a differentiated and\nmultimodal decision-making process, usually including echocardiography as a\ncentral imaging method. Artificial intelligence (AI) offers considerable\npromise for clinicians by facilitating automated interpretation of pediatric\nechocardiography data. However, adapting AI technologies for pediatric\nechocardiography analysis has challenges such as limited public data\navailability, data privacy, and AI model transparency. Recently, researchers\nhave focused on disruptive technologies, such as federated learning (FL) and\nexplainable AI (XAI), to improve automatic diagnostic and decision support\nworkflows. This study offers a comprehensive overview of the limitations and\nopportunities of AI in pediatric echocardiography, emphasizing the synergistic\nworkflow and role of XAI and FL, identifying research gaps, and exploring\npotential future developments. Additionally, three relevant clinical use cases\ndemonstrate the functionality of XAI and FL with a focus on (i) view\nrecognition, (ii) disease classification, (iii) segmentation of cardiac\nstructures, and (iv) quantitative assessment of cardiac function.\n", "link": "http://arxiv.org/abs/2411.10255v1", "date": "2024-11-15", "relevancy": 1.9069, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4992}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4722}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4722}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Artificial%20Intelligence%20in%20Pediatric%20Echocardiography%3A%20Exploring%0A%20%20Challenges%2C%20Opportunities%2C%20and%20Clinical%20Applications%20with%20Explainable%20AI%20and%0A%20%20Federated%20Learning&body=Title%3A%20Artificial%20Intelligence%20in%20Pediatric%20Echocardiography%3A%20Exploring%0A%20%20Challenges%2C%20Opportunities%2C%20and%20Clinical%20Applications%20with%20Explainable%20AI%20and%0A%20%20Federated%20Learning%0AAuthor%3A%20Mohammed%20Yaseen%20Jabarulla%20and%20Theodor%20Uden%20and%20Thomas%20Jack%20and%20Philipp%20Beerbaum%20and%20Steffen%20Oeltze-Jafra%0AAbstract%3A%20%20%20Pediatric%20heart%20diseases%20present%20a%20broad%20spectrum%20of%20congenital%20and%20acquired%0Adiseases.%20More%20complex%20congenital%20malformations%20require%20a%20differentiated%20and%0Amultimodal%20decision-making%20process%2C%20usually%20including%20echocardiography%20as%20a%0Acentral%20imaging%20method.%20Artificial%20intelligence%20%28AI%29%20offers%20considerable%0Apromise%20for%20clinicians%20by%20facilitating%20automated%20interpretation%20of%20pediatric%0Aechocardiography%20data.%20However%2C%20adapting%20AI%20technologies%20for%20pediatric%0Aechocardiography%20analysis%20has%20challenges%20such%20as%20limited%20public%20data%0Aavailability%2C%20data%20privacy%2C%20and%20AI%20model%20transparency.%20Recently%2C%20researchers%0Ahave%20focused%20on%20disruptive%20technologies%2C%20such%20as%20federated%20learning%20%28FL%29%20and%0Aexplainable%20AI%20%28XAI%29%2C%20to%20improve%20automatic%20diagnostic%20and%20decision%20support%0Aworkflows.%20This%20study%20offers%20a%20comprehensive%20overview%20of%20the%20limitations%20and%0Aopportunities%20of%20AI%20in%20pediatric%20echocardiography%2C%20emphasizing%20the%20synergistic%0Aworkflow%20and%20role%20of%20XAI%20and%20FL%2C%20identifying%20research%20gaps%2C%20and%20exploring%0Apotential%20future%20developments.%20Additionally%2C%20three%20relevant%20clinical%20use%20cases%0Ademonstrate%20the%20functionality%20of%20XAI%20and%20FL%20with%20a%20focus%20on%20%28i%29%20view%0Arecognition%2C%20%28ii%29%20disease%20classification%2C%20%28iii%29%20segmentation%20of%20cardiac%0Astructures%2C%20and%20%28iv%29%20quantitative%20assessment%20of%20cardiac%20function.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10255v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArtificial%2520Intelligence%2520in%2520Pediatric%2520Echocardiography%253A%2520Exploring%250A%2520%2520Challenges%252C%2520Opportunities%252C%2520and%2520Clinical%2520Applications%2520with%2520Explainable%2520AI%2520and%250A%2520%2520Federated%2520Learning%26entry.906535625%3DMohammed%2520Yaseen%2520Jabarulla%2520and%2520Theodor%2520Uden%2520and%2520Thomas%2520Jack%2520and%2520Philipp%2520Beerbaum%2520and%2520Steffen%2520Oeltze-Jafra%26entry.1292438233%3D%2520%2520Pediatric%2520heart%2520diseases%2520present%2520a%2520broad%2520spectrum%2520of%2520congenital%2520and%2520acquired%250Adiseases.%2520More%2520complex%2520congenital%2520malformations%2520require%2520a%2520differentiated%2520and%250Amultimodal%2520decision-making%2520process%252C%2520usually%2520including%2520echocardiography%2520as%2520a%250Acentral%2520imaging%2520method.%2520Artificial%2520intelligence%2520%2528AI%2529%2520offers%2520considerable%250Apromise%2520for%2520clinicians%2520by%2520facilitating%2520automated%2520interpretation%2520of%2520pediatric%250Aechocardiography%2520data.%2520However%252C%2520adapting%2520AI%2520technologies%2520for%2520pediatric%250Aechocardiography%2520analysis%2520has%2520challenges%2520such%2520as%2520limited%2520public%2520data%250Aavailability%252C%2520data%2520privacy%252C%2520and%2520AI%2520model%2520transparency.%2520Recently%252C%2520researchers%250Ahave%2520focused%2520on%2520disruptive%2520technologies%252C%2520such%2520as%2520federated%2520learning%2520%2528FL%2529%2520and%250Aexplainable%2520AI%2520%2528XAI%2529%252C%2520to%2520improve%2520automatic%2520diagnostic%2520and%2520decision%2520support%250Aworkflows.%2520This%2520study%2520offers%2520a%2520comprehensive%2520overview%2520of%2520the%2520limitations%2520and%250Aopportunities%2520of%2520AI%2520in%2520pediatric%2520echocardiography%252C%2520emphasizing%2520the%2520synergistic%250Aworkflow%2520and%2520role%2520of%2520XAI%2520and%2520FL%252C%2520identifying%2520research%2520gaps%252C%2520and%2520exploring%250Apotential%2520future%2520developments.%2520Additionally%252C%2520three%2520relevant%2520clinical%2520use%2520cases%250Ademonstrate%2520the%2520functionality%2520of%2520XAI%2520and%2520FL%2520with%2520a%2520focus%2520on%2520%2528i%2529%2520view%250Arecognition%252C%2520%2528ii%2529%2520disease%2520classification%252C%2520%2528iii%2529%2520segmentation%2520of%2520cardiac%250Astructures%252C%2520and%2520%2528iv%2529%2520quantitative%2520assessment%2520of%2520cardiac%2520function.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10255v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Artificial%20Intelligence%20in%20Pediatric%20Echocardiography%3A%20Exploring%0A%20%20Challenges%2C%20Opportunities%2C%20and%20Clinical%20Applications%20with%20Explainable%20AI%20and%0A%20%20Federated%20Learning&entry.906535625=Mohammed%20Yaseen%20Jabarulla%20and%20Theodor%20Uden%20and%20Thomas%20Jack%20and%20Philipp%20Beerbaum%20and%20Steffen%20Oeltze-Jafra&entry.1292438233=%20%20Pediatric%20heart%20diseases%20present%20a%20broad%20spectrum%20of%20congenital%20and%20acquired%0Adiseases.%20More%20complex%20congenital%20malformations%20require%20a%20differentiated%20and%0Amultimodal%20decision-making%20process%2C%20usually%20including%20echocardiography%20as%20a%0Acentral%20imaging%20method.%20Artificial%20intelligence%20%28AI%29%20offers%20considerable%0Apromise%20for%20clinicians%20by%20facilitating%20automated%20interpretation%20of%20pediatric%0Aechocardiography%20data.%20However%2C%20adapting%20AI%20technologies%20for%20pediatric%0Aechocardiography%20analysis%20has%20challenges%20such%20as%20limited%20public%20data%0Aavailability%2C%20data%20privacy%2C%20and%20AI%20model%20transparency.%20Recently%2C%20researchers%0Ahave%20focused%20on%20disruptive%20technologies%2C%20such%20as%20federated%20learning%20%28FL%29%20and%0Aexplainable%20AI%20%28XAI%29%2C%20to%20improve%20automatic%20diagnostic%20and%20decision%20support%0Aworkflows.%20This%20study%20offers%20a%20comprehensive%20overview%20of%20the%20limitations%20and%0Aopportunities%20of%20AI%20in%20pediatric%20echocardiography%2C%20emphasizing%20the%20synergistic%0Aworkflow%20and%20role%20of%20XAI%20and%20FL%2C%20identifying%20research%20gaps%2C%20and%20exploring%0Apotential%20future%20developments.%20Additionally%2C%20three%20relevant%20clinical%20use%20cases%0Ademonstrate%20the%20functionality%20of%20XAI%20and%20FL%20with%20a%20focus%20on%20%28i%29%20view%0Arecognition%2C%20%28ii%29%20disease%20classification%2C%20%28iii%29%20segmentation%20of%20cardiac%0Astructures%2C%20and%20%28iv%29%20quantitative%20assessment%20of%20cardiac%20function.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10255v1&entry.124074799=Read"},
{"title": "Scaling Law for Post-training after Model Pruning", "author": "Xiaodong Chen and Yuxuan Hu and Jing Zhang and Xiaokang Zhang and Cuiping Li and Hong Chen", "abstract": "  Large language models (LLMs) based on the Transformer architecture are widely\nemployed across various domains and tasks. However, their increasing size\nimposes significant hardware demands, limiting practical deployment. To\nmitigate this, model pruning techniques have been developed to create more\nefficient models while maintaining high performance. Despite this,\npost-training after pruning is crucial for performance recovery and can be\nresource-intensive. This paper investigates the post-training requirements of\npruned LLMs and introduces a scaling law to determine the optimal amount of\npost-training data. Post-training experiments with the Llama-3 and Qwen-2.5\nseries models, pruned using depth pruning, width pruning, and 2:4\nsemi-structured pruning, show that higher pruning ratios necessitate more\npost-training data for performance recovery, whereas larger LLMs require less.\nThe proposed scaling law predicts a model's loss based on its parameter counts\nbefore and after pruning, as well as the post-training token counts.\nFurthermore, we find that the scaling law established from smaller LLMs can be\nreliably extrapolated to larger LLMs. This work provides valuable insights into\nthe post-training of pruned LLMs and offers a practical scaling law for\noptimizing post-training data usage.\n", "link": "http://arxiv.org/abs/2411.10272v1", "date": "2024-11-15", "relevancy": 1.8933, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5392}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4653}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Law%20for%20Post-training%20after%20Model%20Pruning&body=Title%3A%20Scaling%20Law%20for%20Post-training%20after%20Model%20Pruning%0AAuthor%3A%20Xiaodong%20Chen%20and%20Yuxuan%20Hu%20and%20Jing%20Zhang%20and%20Xiaokang%20Zhang%20and%20Cuiping%20Li%20and%20Hong%20Chen%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20based%20on%20the%20Transformer%20architecture%20are%20widely%0Aemployed%20across%20various%20domains%20and%20tasks.%20However%2C%20their%20increasing%20size%0Aimposes%20significant%20hardware%20demands%2C%20limiting%20practical%20deployment.%20To%0Amitigate%20this%2C%20model%20pruning%20techniques%20have%20been%20developed%20to%20create%20more%0Aefficient%20models%20while%20maintaining%20high%20performance.%20Despite%20this%2C%0Apost-training%20after%20pruning%20is%20crucial%20for%20performance%20recovery%20and%20can%20be%0Aresource-intensive.%20This%20paper%20investigates%20the%20post-training%20requirements%20of%0Apruned%20LLMs%20and%20introduces%20a%20scaling%20law%20to%20determine%20the%20optimal%20amount%20of%0Apost-training%20data.%20Post-training%20experiments%20with%20the%20Llama-3%20and%20Qwen-2.5%0Aseries%20models%2C%20pruned%20using%20depth%20pruning%2C%20width%20pruning%2C%20and%202%3A4%0Asemi-structured%20pruning%2C%20show%20that%20higher%20pruning%20ratios%20necessitate%20more%0Apost-training%20data%20for%20performance%20recovery%2C%20whereas%20larger%20LLMs%20require%20less.%0AThe%20proposed%20scaling%20law%20predicts%20a%20model%27s%20loss%20based%20on%20its%20parameter%20counts%0Abefore%20and%20after%20pruning%2C%20as%20well%20as%20the%20post-training%20token%20counts.%0AFurthermore%2C%20we%20find%20that%20the%20scaling%20law%20established%20from%20smaller%20LLMs%20can%20be%0Areliably%20extrapolated%20to%20larger%20LLMs.%20This%20work%20provides%20valuable%20insights%20into%0Athe%20post-training%20of%20pruned%20LLMs%20and%20offers%20a%20practical%20scaling%20law%20for%0Aoptimizing%20post-training%20data%20usage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10272v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Law%2520for%2520Post-training%2520after%2520Model%2520Pruning%26entry.906535625%3DXiaodong%2520Chen%2520and%2520Yuxuan%2520Hu%2520and%2520Jing%2520Zhang%2520and%2520Xiaokang%2520Zhang%2520and%2520Cuiping%2520Li%2520and%2520Hong%2520Chen%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520based%2520on%2520the%2520Transformer%2520architecture%2520are%2520widely%250Aemployed%2520across%2520various%2520domains%2520and%2520tasks.%2520However%252C%2520their%2520increasing%2520size%250Aimposes%2520significant%2520hardware%2520demands%252C%2520limiting%2520practical%2520deployment.%2520To%250Amitigate%2520this%252C%2520model%2520pruning%2520techniques%2520have%2520been%2520developed%2520to%2520create%2520more%250Aefficient%2520models%2520while%2520maintaining%2520high%2520performance.%2520Despite%2520this%252C%250Apost-training%2520after%2520pruning%2520is%2520crucial%2520for%2520performance%2520recovery%2520and%2520can%2520be%250Aresource-intensive.%2520This%2520paper%2520investigates%2520the%2520post-training%2520requirements%2520of%250Apruned%2520LLMs%2520and%2520introduces%2520a%2520scaling%2520law%2520to%2520determine%2520the%2520optimal%2520amount%2520of%250Apost-training%2520data.%2520Post-training%2520experiments%2520with%2520the%2520Llama-3%2520and%2520Qwen-2.5%250Aseries%2520models%252C%2520pruned%2520using%2520depth%2520pruning%252C%2520width%2520pruning%252C%2520and%25202%253A4%250Asemi-structured%2520pruning%252C%2520show%2520that%2520higher%2520pruning%2520ratios%2520necessitate%2520more%250Apost-training%2520data%2520for%2520performance%2520recovery%252C%2520whereas%2520larger%2520LLMs%2520require%2520less.%250AThe%2520proposed%2520scaling%2520law%2520predicts%2520a%2520model%2527s%2520loss%2520based%2520on%2520its%2520parameter%2520counts%250Abefore%2520and%2520after%2520pruning%252C%2520as%2520well%2520as%2520the%2520post-training%2520token%2520counts.%250AFurthermore%252C%2520we%2520find%2520that%2520the%2520scaling%2520law%2520established%2520from%2520smaller%2520LLMs%2520can%2520be%250Areliably%2520extrapolated%2520to%2520larger%2520LLMs.%2520This%2520work%2520provides%2520valuable%2520insights%2520into%250Athe%2520post-training%2520of%2520pruned%2520LLMs%2520and%2520offers%2520a%2520practical%2520scaling%2520law%2520for%250Aoptimizing%2520post-training%2520data%2520usage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10272v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Law%20for%20Post-training%20after%20Model%20Pruning&entry.906535625=Xiaodong%20Chen%20and%20Yuxuan%20Hu%20and%20Jing%20Zhang%20and%20Xiaokang%20Zhang%20and%20Cuiping%20Li%20and%20Hong%20Chen&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20based%20on%20the%20Transformer%20architecture%20are%20widely%0Aemployed%20across%20various%20domains%20and%20tasks.%20However%2C%20their%20increasing%20size%0Aimposes%20significant%20hardware%20demands%2C%20limiting%20practical%20deployment.%20To%0Amitigate%20this%2C%20model%20pruning%20techniques%20have%20been%20developed%20to%20create%20more%0Aefficient%20models%20while%20maintaining%20high%20performance.%20Despite%20this%2C%0Apost-training%20after%20pruning%20is%20crucial%20for%20performance%20recovery%20and%20can%20be%0Aresource-intensive.%20This%20paper%20investigates%20the%20post-training%20requirements%20of%0Apruned%20LLMs%20and%20introduces%20a%20scaling%20law%20to%20determine%20the%20optimal%20amount%20of%0Apost-training%20data.%20Post-training%20experiments%20with%20the%20Llama-3%20and%20Qwen-2.5%0Aseries%20models%2C%20pruned%20using%20depth%20pruning%2C%20width%20pruning%2C%20and%202%3A4%0Asemi-structured%20pruning%2C%20show%20that%20higher%20pruning%20ratios%20necessitate%20more%0Apost-training%20data%20for%20performance%20recovery%2C%20whereas%20larger%20LLMs%20require%20less.%0AThe%20proposed%20scaling%20law%20predicts%20a%20model%27s%20loss%20based%20on%20its%20parameter%20counts%0Abefore%20and%20after%20pruning%2C%20as%20well%20as%20the%20post-training%20token%20counts.%0AFurthermore%2C%20we%20find%20that%20the%20scaling%20law%20established%20from%20smaller%20LLMs%20can%20be%0Areliably%20extrapolated%20to%20larger%20LLMs.%20This%20work%20provides%20valuable%20insights%20into%0Athe%20post-training%20of%20pruned%20LLMs%20and%20offers%20a%20practical%20scaling%20law%20for%0Aoptimizing%20post-training%20data%20usage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10272v1&entry.124074799=Read"},
{"title": "Provocation: Who benefits from \"inclusion\" in Generative AI?", "author": "Samantha Dalal and Siobhan Mackenzie Hall and Nari Johnson", "abstract": "  The demands for accurate and representative generative AI systems means there\nis an increased demand on participatory evaluation structures. While these\nparticipatory structures are paramount to to ensure non-dominant values,\nknowledge and material culture are also reflected in AI models and the media\nthey generate, we argue that dominant structures of community participation in\nAI development and evaluation are not explicit enough about the benefits and\nharms that members of socially marginalized groups may experience as a result\nof their participation. Without explicit interrogation of these benefits by AI\ndevelopers, as a community we may remain blind to the immensity of systemic\nchange that is needed as well. To support this provocation, we present a\nspeculative case study, developed from our own collective experiences as AI\nresearchers. We use this speculative context to itemize the barriers that need\nto be overcome in order for the proposed benefits to marginalized communities\nto be realized, and harms mitigated.\n", "link": "http://arxiv.org/abs/2411.09102v2", "date": "2024-11-15", "relevancy": 1.8642, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4958}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4709}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Provocation%3A%20Who%20benefits%20from%20%22inclusion%22%20in%20Generative%20AI%3F&body=Title%3A%20Provocation%3A%20Who%20benefits%20from%20%22inclusion%22%20in%20Generative%20AI%3F%0AAuthor%3A%20Samantha%20Dalal%20and%20Siobhan%20Mackenzie%20Hall%20and%20Nari%20Johnson%0AAbstract%3A%20%20%20The%20demands%20for%20accurate%20and%20representative%20generative%20AI%20systems%20means%20there%0Ais%20an%20increased%20demand%20on%20participatory%20evaluation%20structures.%20While%20these%0Aparticipatory%20structures%20are%20paramount%20to%20to%20ensure%20non-dominant%20values%2C%0Aknowledge%20and%20material%20culture%20are%20also%20reflected%20in%20AI%20models%20and%20the%20media%0Athey%20generate%2C%20we%20argue%20that%20dominant%20structures%20of%20community%20participation%20in%0AAI%20development%20and%20evaluation%20are%20not%20explicit%20enough%20about%20the%20benefits%20and%0Aharms%20that%20members%20of%20socially%20marginalized%20groups%20may%20experience%20as%20a%20result%0Aof%20their%20participation.%20Without%20explicit%20interrogation%20of%20these%20benefits%20by%20AI%0Adevelopers%2C%20as%20a%20community%20we%20may%20remain%20blind%20to%20the%20immensity%20of%20systemic%0Achange%20that%20is%20needed%20as%20well.%20To%20support%20this%20provocation%2C%20we%20present%20a%0Aspeculative%20case%20study%2C%20developed%20from%20our%20own%20collective%20experiences%20as%20AI%0Aresearchers.%20We%20use%20this%20speculative%20context%20to%20itemize%20the%20barriers%20that%20need%0Ato%20be%20overcome%20in%20order%20for%20the%20proposed%20benefits%20to%20marginalized%20communities%0Ato%20be%20realized%2C%20and%20harms%20mitigated.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09102v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProvocation%253A%2520Who%2520benefits%2520from%2520%2522inclusion%2522%2520in%2520Generative%2520AI%253F%26entry.906535625%3DSamantha%2520Dalal%2520and%2520Siobhan%2520Mackenzie%2520Hall%2520and%2520Nari%2520Johnson%26entry.1292438233%3D%2520%2520The%2520demands%2520for%2520accurate%2520and%2520representative%2520generative%2520AI%2520systems%2520means%2520there%250Ais%2520an%2520increased%2520demand%2520on%2520participatory%2520evaluation%2520structures.%2520While%2520these%250Aparticipatory%2520structures%2520are%2520paramount%2520to%2520to%2520ensure%2520non-dominant%2520values%252C%250Aknowledge%2520and%2520material%2520culture%2520are%2520also%2520reflected%2520in%2520AI%2520models%2520and%2520the%2520media%250Athey%2520generate%252C%2520we%2520argue%2520that%2520dominant%2520structures%2520of%2520community%2520participation%2520in%250AAI%2520development%2520and%2520evaluation%2520are%2520not%2520explicit%2520enough%2520about%2520the%2520benefits%2520and%250Aharms%2520that%2520members%2520of%2520socially%2520marginalized%2520groups%2520may%2520experience%2520as%2520a%2520result%250Aof%2520their%2520participation.%2520Without%2520explicit%2520interrogation%2520of%2520these%2520benefits%2520by%2520AI%250Adevelopers%252C%2520as%2520a%2520community%2520we%2520may%2520remain%2520blind%2520to%2520the%2520immensity%2520of%2520systemic%250Achange%2520that%2520is%2520needed%2520as%2520well.%2520To%2520support%2520this%2520provocation%252C%2520we%2520present%2520a%250Aspeculative%2520case%2520study%252C%2520developed%2520from%2520our%2520own%2520collective%2520experiences%2520as%2520AI%250Aresearchers.%2520We%2520use%2520this%2520speculative%2520context%2520to%2520itemize%2520the%2520barriers%2520that%2520need%250Ato%2520be%2520overcome%2520in%2520order%2520for%2520the%2520proposed%2520benefits%2520to%2520marginalized%2520communities%250Ato%2520be%2520realized%252C%2520and%2520harms%2520mitigated.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09102v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Provocation%3A%20Who%20benefits%20from%20%22inclusion%22%20in%20Generative%20AI%3F&entry.906535625=Samantha%20Dalal%20and%20Siobhan%20Mackenzie%20Hall%20and%20Nari%20Johnson&entry.1292438233=%20%20The%20demands%20for%20accurate%20and%20representative%20generative%20AI%20systems%20means%20there%0Ais%20an%20increased%20demand%20on%20participatory%20evaluation%20structures.%20While%20these%0Aparticipatory%20structures%20are%20paramount%20to%20to%20ensure%20non-dominant%20values%2C%0Aknowledge%20and%20material%20culture%20are%20also%20reflected%20in%20AI%20models%20and%20the%20media%0Athey%20generate%2C%20we%20argue%20that%20dominant%20structures%20of%20community%20participation%20in%0AAI%20development%20and%20evaluation%20are%20not%20explicit%20enough%20about%20the%20benefits%20and%0Aharms%20that%20members%20of%20socially%20marginalized%20groups%20may%20experience%20as%20a%20result%0Aof%20their%20participation.%20Without%20explicit%20interrogation%20of%20these%20benefits%20by%20AI%0Adevelopers%2C%20as%20a%20community%20we%20may%20remain%20blind%20to%20the%20immensity%20of%20systemic%0Achange%20that%20is%20needed%20as%20well.%20To%20support%20this%20provocation%2C%20we%20present%20a%0Aspeculative%20case%20study%2C%20developed%20from%20our%20own%20collective%20experiences%20as%20AI%0Aresearchers.%20We%20use%20this%20speculative%20context%20to%20itemize%20the%20barriers%20that%20need%0Ato%20be%20overcome%20in%20order%20for%20the%20proposed%20benefits%20to%20marginalized%20communities%0Ato%20be%20realized%2C%20and%20harms%20mitigated.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09102v2&entry.124074799=Read"},
{"title": "BMP: Bridging the Gap between B-Spline and Movement Primitives", "author": "Weiran Liao and Ge Li and Hongyi Zhou and Rudolf Lioutikov and Gerhard Neumann", "abstract": "  This work introduces B-spline Movement Primitives (BMPs), a new Movement\nPrimitive (MP) variant that leverages B-splines for motion representation.\nB-splines are a well-known concept in motion planning due to their ability to\ngenerate complex, smooth trajectories with only a few control points while\nsatisfying boundary conditions, i.e., passing through a specified desired\nposition with desired velocity. However, current usages of B-splines tend to\nignore the higher-order statistics in trajectory distributions, which limits\ntheir usage in imitation learning (IL) and reinforcement learning (RL), where\nmodeling trajectory distribution is essential. In contrast, MPs are commonly\nused in IL and RL for their capacity to capture trajectory likelihoods and\ncorrelations. However, MPs are constrained by their abilities to satisfy\nboundary conditions and usually need extra terms in learning objectives to\nsatisfy velocity constraints. By reformulating B-splines as MPs, represented\nthrough basis functions and weight parameters, BMPs combine the strengths of\nboth approaches, allowing B-splines to capture higher-order statistics while\nretaining their ability to satisfy boundary conditions. Empirical results in IL\nand RL demonstrate that BMPs broaden the applicability of B-splines in robot\nlearning and offer greater expressiveness compared to existing MP variants.\n", "link": "http://arxiv.org/abs/2411.10336v1", "date": "2024-11-15", "relevancy": 1.861, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.498}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4664}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BMP%3A%20Bridging%20the%20Gap%20between%20B-Spline%20and%20Movement%20Primitives&body=Title%3A%20BMP%3A%20Bridging%20the%20Gap%20between%20B-Spline%20and%20Movement%20Primitives%0AAuthor%3A%20Weiran%20Liao%20and%20Ge%20Li%20and%20Hongyi%20Zhou%20and%20Rudolf%20Lioutikov%20and%20Gerhard%20Neumann%0AAbstract%3A%20%20%20This%20work%20introduces%20B-spline%20Movement%20Primitives%20%28BMPs%29%2C%20a%20new%20Movement%0APrimitive%20%28MP%29%20variant%20that%20leverages%20B-splines%20for%20motion%20representation.%0AB-splines%20are%20a%20well-known%20concept%20in%20motion%20planning%20due%20to%20their%20ability%20to%0Agenerate%20complex%2C%20smooth%20trajectories%20with%20only%20a%20few%20control%20points%20while%0Asatisfying%20boundary%20conditions%2C%20i.e.%2C%20passing%20through%20a%20specified%20desired%0Aposition%20with%20desired%20velocity.%20However%2C%20current%20usages%20of%20B-splines%20tend%20to%0Aignore%20the%20higher-order%20statistics%20in%20trajectory%20distributions%2C%20which%20limits%0Atheir%20usage%20in%20imitation%20learning%20%28IL%29%20and%20reinforcement%20learning%20%28RL%29%2C%20where%0Amodeling%20trajectory%20distribution%20is%20essential.%20In%20contrast%2C%20MPs%20are%20commonly%0Aused%20in%20IL%20and%20RL%20for%20their%20capacity%20to%20capture%20trajectory%20likelihoods%20and%0Acorrelations.%20However%2C%20MPs%20are%20constrained%20by%20their%20abilities%20to%20satisfy%0Aboundary%20conditions%20and%20usually%20need%20extra%20terms%20in%20learning%20objectives%20to%0Asatisfy%20velocity%20constraints.%20By%20reformulating%20B-splines%20as%20MPs%2C%20represented%0Athrough%20basis%20functions%20and%20weight%20parameters%2C%20BMPs%20combine%20the%20strengths%20of%0Aboth%20approaches%2C%20allowing%20B-splines%20to%20capture%20higher-order%20statistics%20while%0Aretaining%20their%20ability%20to%20satisfy%20boundary%20conditions.%20Empirical%20results%20in%20IL%0Aand%20RL%20demonstrate%20that%20BMPs%20broaden%20the%20applicability%20of%20B-splines%20in%20robot%0Alearning%20and%20offer%20greater%20expressiveness%20compared%20to%20existing%20MP%20variants.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10336v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBMP%253A%2520Bridging%2520the%2520Gap%2520between%2520B-Spline%2520and%2520Movement%2520Primitives%26entry.906535625%3DWeiran%2520Liao%2520and%2520Ge%2520Li%2520and%2520Hongyi%2520Zhou%2520and%2520Rudolf%2520Lioutikov%2520and%2520Gerhard%2520Neumann%26entry.1292438233%3D%2520%2520This%2520work%2520introduces%2520B-spline%2520Movement%2520Primitives%2520%2528BMPs%2529%252C%2520a%2520new%2520Movement%250APrimitive%2520%2528MP%2529%2520variant%2520that%2520leverages%2520B-splines%2520for%2520motion%2520representation.%250AB-splines%2520are%2520a%2520well-known%2520concept%2520in%2520motion%2520planning%2520due%2520to%2520their%2520ability%2520to%250Agenerate%2520complex%252C%2520smooth%2520trajectories%2520with%2520only%2520a%2520few%2520control%2520points%2520while%250Asatisfying%2520boundary%2520conditions%252C%2520i.e.%252C%2520passing%2520through%2520a%2520specified%2520desired%250Aposition%2520with%2520desired%2520velocity.%2520However%252C%2520current%2520usages%2520of%2520B-splines%2520tend%2520to%250Aignore%2520the%2520higher-order%2520statistics%2520in%2520trajectory%2520distributions%252C%2520which%2520limits%250Atheir%2520usage%2520in%2520imitation%2520learning%2520%2528IL%2529%2520and%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520where%250Amodeling%2520trajectory%2520distribution%2520is%2520essential.%2520In%2520contrast%252C%2520MPs%2520are%2520commonly%250Aused%2520in%2520IL%2520and%2520RL%2520for%2520their%2520capacity%2520to%2520capture%2520trajectory%2520likelihoods%2520and%250Acorrelations.%2520However%252C%2520MPs%2520are%2520constrained%2520by%2520their%2520abilities%2520to%2520satisfy%250Aboundary%2520conditions%2520and%2520usually%2520need%2520extra%2520terms%2520in%2520learning%2520objectives%2520to%250Asatisfy%2520velocity%2520constraints.%2520By%2520reformulating%2520B-splines%2520as%2520MPs%252C%2520represented%250Athrough%2520basis%2520functions%2520and%2520weight%2520parameters%252C%2520BMPs%2520combine%2520the%2520strengths%2520of%250Aboth%2520approaches%252C%2520allowing%2520B-splines%2520to%2520capture%2520higher-order%2520statistics%2520while%250Aretaining%2520their%2520ability%2520to%2520satisfy%2520boundary%2520conditions.%2520Empirical%2520results%2520in%2520IL%250Aand%2520RL%2520demonstrate%2520that%2520BMPs%2520broaden%2520the%2520applicability%2520of%2520B-splines%2520in%2520robot%250Alearning%2520and%2520offer%2520greater%2520expressiveness%2520compared%2520to%2520existing%2520MP%2520variants.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10336v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BMP%3A%20Bridging%20the%20Gap%20between%20B-Spline%20and%20Movement%20Primitives&entry.906535625=Weiran%20Liao%20and%20Ge%20Li%20and%20Hongyi%20Zhou%20and%20Rudolf%20Lioutikov%20and%20Gerhard%20Neumann&entry.1292438233=%20%20This%20work%20introduces%20B-spline%20Movement%20Primitives%20%28BMPs%29%2C%20a%20new%20Movement%0APrimitive%20%28MP%29%20variant%20that%20leverages%20B-splines%20for%20motion%20representation.%0AB-splines%20are%20a%20well-known%20concept%20in%20motion%20planning%20due%20to%20their%20ability%20to%0Agenerate%20complex%2C%20smooth%20trajectories%20with%20only%20a%20few%20control%20points%20while%0Asatisfying%20boundary%20conditions%2C%20i.e.%2C%20passing%20through%20a%20specified%20desired%0Aposition%20with%20desired%20velocity.%20However%2C%20current%20usages%20of%20B-splines%20tend%20to%0Aignore%20the%20higher-order%20statistics%20in%20trajectory%20distributions%2C%20which%20limits%0Atheir%20usage%20in%20imitation%20learning%20%28IL%29%20and%20reinforcement%20learning%20%28RL%29%2C%20where%0Amodeling%20trajectory%20distribution%20is%20essential.%20In%20contrast%2C%20MPs%20are%20commonly%0Aused%20in%20IL%20and%20RL%20for%20their%20capacity%20to%20capture%20trajectory%20likelihoods%20and%0Acorrelations.%20However%2C%20MPs%20are%20constrained%20by%20their%20abilities%20to%20satisfy%0Aboundary%20conditions%20and%20usually%20need%20extra%20terms%20in%20learning%20objectives%20to%0Asatisfy%20velocity%20constraints.%20By%20reformulating%20B-splines%20as%20MPs%2C%20represented%0Athrough%20basis%20functions%20and%20weight%20parameters%2C%20BMPs%20combine%20the%20strengths%20of%0Aboth%20approaches%2C%20allowing%20B-splines%20to%20capture%20higher-order%20statistics%20while%0Aretaining%20their%20ability%20to%20satisfy%20boundary%20conditions.%20Empirical%20results%20in%20IL%0Aand%20RL%20demonstrate%20that%20BMPs%20broaden%20the%20applicability%20of%20B-splines%20in%20robot%0Alearning%20and%20offer%20greater%20expressiveness%20compared%20to%20existing%20MP%20variants.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10336v1&entry.124074799=Read"},
{"title": "Large Language Model-Based Interpretable Machine Learning Control in\n  Building Energy Systems", "author": "Liang Zhang and Zhelun Chen", "abstract": "  The potential of Machine Learning Control (MLC) in HVAC systems is hindered\nby its opaque nature and inference mechanisms, which is challenging for users\nand modelers to fully comprehend, ultimately leading to a lack of trust in\nMLC-based decision-making. To address this challenge, this paper investigates\nand explores Interpretable Machine Learning (IML), a branch of Machine Learning\n(ML) that enhances transparency and understanding of models and their\ninferences, to improve the credibility of MLC and its industrial application in\nHVAC systems. Specifically, we developed an innovative framework that combines\nthe principles of Shapley values and the in-context learning feature of Large\nLanguage Models (LLMs). While the Shapley values are instrumental in dissecting\nthe contributions of various features in ML models, LLM provides an in-depth\nunderstanding of the non-data-driven or rule-based elements in MLC; combining\nthem, LLM further packages these insights into a coherent, human-understandable\nnarrative. The paper presents a case study to demonstrate the feasibility of\nthe developed IML framework for model predictive control-based precooling under\ndemand response events in a virtual testbed. The results indicate that the\ndeveloped framework generates and explains the control signals in accordance\nwith the rule-based rationale.\n", "link": "http://arxiv.org/abs/2402.09584v2", "date": "2024-11-15", "relevancy": 1.8532, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4752}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4609}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Model-Based%20Interpretable%20Machine%20Learning%20Control%20in%0A%20%20Building%20Energy%20Systems&body=Title%3A%20Large%20Language%20Model-Based%20Interpretable%20Machine%20Learning%20Control%20in%0A%20%20Building%20Energy%20Systems%0AAuthor%3A%20Liang%20Zhang%20and%20Zhelun%20Chen%0AAbstract%3A%20%20%20The%20potential%20of%20Machine%20Learning%20Control%20%28MLC%29%20in%20HVAC%20systems%20is%20hindered%0Aby%20its%20opaque%20nature%20and%20inference%20mechanisms%2C%20which%20is%20challenging%20for%20users%0Aand%20modelers%20to%20fully%20comprehend%2C%20ultimately%20leading%20to%20a%20lack%20of%20trust%20in%0AMLC-based%20decision-making.%20To%20address%20this%20challenge%2C%20this%20paper%20investigates%0Aand%20explores%20Interpretable%20Machine%20Learning%20%28IML%29%2C%20a%20branch%20of%20Machine%20Learning%0A%28ML%29%20that%20enhances%20transparency%20and%20understanding%20of%20models%20and%20their%0Ainferences%2C%20to%20improve%20the%20credibility%20of%20MLC%20and%20its%20industrial%20application%20in%0AHVAC%20systems.%20Specifically%2C%20we%20developed%20an%20innovative%20framework%20that%20combines%0Athe%20principles%20of%20Shapley%20values%20and%20the%20in-context%20learning%20feature%20of%20Large%0ALanguage%20Models%20%28LLMs%29.%20While%20the%20Shapley%20values%20are%20instrumental%20in%20dissecting%0Athe%20contributions%20of%20various%20features%20in%20ML%20models%2C%20LLM%20provides%20an%20in-depth%0Aunderstanding%20of%20the%20non-data-driven%20or%20rule-based%20elements%20in%20MLC%3B%20combining%0Athem%2C%20LLM%20further%20packages%20these%20insights%20into%20a%20coherent%2C%20human-understandable%0Anarrative.%20The%20paper%20presents%20a%20case%20study%20to%20demonstrate%20the%20feasibility%20of%0Athe%20developed%20IML%20framework%20for%20model%20predictive%20control-based%20precooling%20under%0Ademand%20response%20events%20in%20a%20virtual%20testbed.%20The%20results%20indicate%20that%20the%0Adeveloped%20framework%20generates%20and%20explains%20the%20control%20signals%20in%20accordance%0Awith%20the%20rule-based%20rationale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.09584v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Model-Based%2520Interpretable%2520Machine%2520Learning%2520Control%2520in%250A%2520%2520Building%2520Energy%2520Systems%26entry.906535625%3DLiang%2520Zhang%2520and%2520Zhelun%2520Chen%26entry.1292438233%3D%2520%2520The%2520potential%2520of%2520Machine%2520Learning%2520Control%2520%2528MLC%2529%2520in%2520HVAC%2520systems%2520is%2520hindered%250Aby%2520its%2520opaque%2520nature%2520and%2520inference%2520mechanisms%252C%2520which%2520is%2520challenging%2520for%2520users%250Aand%2520modelers%2520to%2520fully%2520comprehend%252C%2520ultimately%2520leading%2520to%2520a%2520lack%2520of%2520trust%2520in%250AMLC-based%2520decision-making.%2520To%2520address%2520this%2520challenge%252C%2520this%2520paper%2520investigates%250Aand%2520explores%2520Interpretable%2520Machine%2520Learning%2520%2528IML%2529%252C%2520a%2520branch%2520of%2520Machine%2520Learning%250A%2528ML%2529%2520that%2520enhances%2520transparency%2520and%2520understanding%2520of%2520models%2520and%2520their%250Ainferences%252C%2520to%2520improve%2520the%2520credibility%2520of%2520MLC%2520and%2520its%2520industrial%2520application%2520in%250AHVAC%2520systems.%2520Specifically%252C%2520we%2520developed%2520an%2520innovative%2520framework%2520that%2520combines%250Athe%2520principles%2520of%2520Shapley%2520values%2520and%2520the%2520in-context%2520learning%2520feature%2520of%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529.%2520While%2520the%2520Shapley%2520values%2520are%2520instrumental%2520in%2520dissecting%250Athe%2520contributions%2520of%2520various%2520features%2520in%2520ML%2520models%252C%2520LLM%2520provides%2520an%2520in-depth%250Aunderstanding%2520of%2520the%2520non-data-driven%2520or%2520rule-based%2520elements%2520in%2520MLC%253B%2520combining%250Athem%252C%2520LLM%2520further%2520packages%2520these%2520insights%2520into%2520a%2520coherent%252C%2520human-understandable%250Anarrative.%2520The%2520paper%2520presents%2520a%2520case%2520study%2520to%2520demonstrate%2520the%2520feasibility%2520of%250Athe%2520developed%2520IML%2520framework%2520for%2520model%2520predictive%2520control-based%2520precooling%2520under%250Ademand%2520response%2520events%2520in%2520a%2520virtual%2520testbed.%2520The%2520results%2520indicate%2520that%2520the%250Adeveloped%2520framework%2520generates%2520and%2520explains%2520the%2520control%2520signals%2520in%2520accordance%250Awith%2520the%2520rule-based%2520rationale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.09584v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Model-Based%20Interpretable%20Machine%20Learning%20Control%20in%0A%20%20Building%20Energy%20Systems&entry.906535625=Liang%20Zhang%20and%20Zhelun%20Chen&entry.1292438233=%20%20The%20potential%20of%20Machine%20Learning%20Control%20%28MLC%29%20in%20HVAC%20systems%20is%20hindered%0Aby%20its%20opaque%20nature%20and%20inference%20mechanisms%2C%20which%20is%20challenging%20for%20users%0Aand%20modelers%20to%20fully%20comprehend%2C%20ultimately%20leading%20to%20a%20lack%20of%20trust%20in%0AMLC-based%20decision-making.%20To%20address%20this%20challenge%2C%20this%20paper%20investigates%0Aand%20explores%20Interpretable%20Machine%20Learning%20%28IML%29%2C%20a%20branch%20of%20Machine%20Learning%0A%28ML%29%20that%20enhances%20transparency%20and%20understanding%20of%20models%20and%20their%0Ainferences%2C%20to%20improve%20the%20credibility%20of%20MLC%20and%20its%20industrial%20application%20in%0AHVAC%20systems.%20Specifically%2C%20we%20developed%20an%20innovative%20framework%20that%20combines%0Athe%20principles%20of%20Shapley%20values%20and%20the%20in-context%20learning%20feature%20of%20Large%0ALanguage%20Models%20%28LLMs%29.%20While%20the%20Shapley%20values%20are%20instrumental%20in%20dissecting%0Athe%20contributions%20of%20various%20features%20in%20ML%20models%2C%20LLM%20provides%20an%20in-depth%0Aunderstanding%20of%20the%20non-data-driven%20or%20rule-based%20elements%20in%20MLC%3B%20combining%0Athem%2C%20LLM%20further%20packages%20these%20insights%20into%20a%20coherent%2C%20human-understandable%0Anarrative.%20The%20paper%20presents%20a%20case%20study%20to%20demonstrate%20the%20feasibility%20of%0Athe%20developed%20IML%20framework%20for%20model%20predictive%20control-based%20precooling%20under%0Ademand%20response%20events%20in%20a%20virtual%20testbed.%20The%20results%20indicate%20that%20the%0Adeveloped%20framework%20generates%20and%20explains%20the%20control%20signals%20in%20accordance%0Awith%20the%20rule-based%20rationale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09584v2&entry.124074799=Read"},
{"title": "CNN-Based Classification of Persian Miniature Paintings from Five\n  Renowned Schools", "author": "Mojtaba Shahi and Roozbeh Rajabi and Farnaz Masoumzadeh", "abstract": "  This article addresses the gap in computational painting analysis focused on\nPersian miniature painting, a rich cultural and artistic heritage. It\nintroduces a novel approach using Convolutional Neural Networks (CNN) to\nclassify Persian miniatures from five schools: Herat, Tabriz-e Avval, Shiraz-e\nAvval, Tabriz-e Dovvom, and Qajar. The method achieves an average accuracy of\nover 91%. A meticulously curated dataset captures the distinct features of each\nschool, with a patch-based CNN approach classifying image segments\nindependently before merging results for enhanced accuracy. This research\ncontributes significantly to digital art analysis, providing detailed insights\ninto the dataset, CNN architecture, training, and validation processes. It\nhighlights the potential for future advancements in automated art analysis,\nbridging machine learning, art history, and digital humanities, thereby aiding\nthe preservation and understanding of Persian cultural heritage.\n", "link": "http://arxiv.org/abs/2411.10330v1", "date": "2024-11-15", "relevancy": 1.8453, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4686}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4679}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CNN-Based%20Classification%20of%20Persian%20Miniature%20Paintings%20from%20Five%0A%20%20Renowned%20Schools&body=Title%3A%20CNN-Based%20Classification%20of%20Persian%20Miniature%20Paintings%20from%20Five%0A%20%20Renowned%20Schools%0AAuthor%3A%20Mojtaba%20Shahi%20and%20Roozbeh%20Rajabi%20and%20Farnaz%20Masoumzadeh%0AAbstract%3A%20%20%20This%20article%20addresses%20the%20gap%20in%20computational%20painting%20analysis%20focused%20on%0APersian%20miniature%20painting%2C%20a%20rich%20cultural%20and%20artistic%20heritage.%20It%0Aintroduces%20a%20novel%20approach%20using%20Convolutional%20Neural%20Networks%20%28CNN%29%20to%0Aclassify%20Persian%20miniatures%20from%20five%20schools%3A%20Herat%2C%20Tabriz-e%20Avval%2C%20Shiraz-e%0AAvval%2C%20Tabriz-e%20Dovvom%2C%20and%20Qajar.%20The%20method%20achieves%20an%20average%20accuracy%20of%0Aover%2091%25.%20A%20meticulously%20curated%20dataset%20captures%20the%20distinct%20features%20of%20each%0Aschool%2C%20with%20a%20patch-based%20CNN%20approach%20classifying%20image%20segments%0Aindependently%20before%20merging%20results%20for%20enhanced%20accuracy.%20This%20research%0Acontributes%20significantly%20to%20digital%20art%20analysis%2C%20providing%20detailed%20insights%0Ainto%20the%20dataset%2C%20CNN%20architecture%2C%20training%2C%20and%20validation%20processes.%20It%0Ahighlights%20the%20potential%20for%20future%20advancements%20in%20automated%20art%20analysis%2C%0Abridging%20machine%20learning%2C%20art%20history%2C%20and%20digital%20humanities%2C%20thereby%20aiding%0Athe%20preservation%20and%20understanding%20of%20Persian%20cultural%20heritage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10330v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCNN-Based%2520Classification%2520of%2520Persian%2520Miniature%2520Paintings%2520from%2520Five%250A%2520%2520Renowned%2520Schools%26entry.906535625%3DMojtaba%2520Shahi%2520and%2520Roozbeh%2520Rajabi%2520and%2520Farnaz%2520Masoumzadeh%26entry.1292438233%3D%2520%2520This%2520article%2520addresses%2520the%2520gap%2520in%2520computational%2520painting%2520analysis%2520focused%2520on%250APersian%2520miniature%2520painting%252C%2520a%2520rich%2520cultural%2520and%2520artistic%2520heritage.%2520It%250Aintroduces%2520a%2520novel%2520approach%2520using%2520Convolutional%2520Neural%2520Networks%2520%2528CNN%2529%2520to%250Aclassify%2520Persian%2520miniatures%2520from%2520five%2520schools%253A%2520Herat%252C%2520Tabriz-e%2520Avval%252C%2520Shiraz-e%250AAvval%252C%2520Tabriz-e%2520Dovvom%252C%2520and%2520Qajar.%2520The%2520method%2520achieves%2520an%2520average%2520accuracy%2520of%250Aover%252091%2525.%2520A%2520meticulously%2520curated%2520dataset%2520captures%2520the%2520distinct%2520features%2520of%2520each%250Aschool%252C%2520with%2520a%2520patch-based%2520CNN%2520approach%2520classifying%2520image%2520segments%250Aindependently%2520before%2520merging%2520results%2520for%2520enhanced%2520accuracy.%2520This%2520research%250Acontributes%2520significantly%2520to%2520digital%2520art%2520analysis%252C%2520providing%2520detailed%2520insights%250Ainto%2520the%2520dataset%252C%2520CNN%2520architecture%252C%2520training%252C%2520and%2520validation%2520processes.%2520It%250Ahighlights%2520the%2520potential%2520for%2520future%2520advancements%2520in%2520automated%2520art%2520analysis%252C%250Abridging%2520machine%2520learning%252C%2520art%2520history%252C%2520and%2520digital%2520humanities%252C%2520thereby%2520aiding%250Athe%2520preservation%2520and%2520understanding%2520of%2520Persian%2520cultural%2520heritage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10330v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CNN-Based%20Classification%20of%20Persian%20Miniature%20Paintings%20from%20Five%0A%20%20Renowned%20Schools&entry.906535625=Mojtaba%20Shahi%20and%20Roozbeh%20Rajabi%20and%20Farnaz%20Masoumzadeh&entry.1292438233=%20%20This%20article%20addresses%20the%20gap%20in%20computational%20painting%20analysis%20focused%20on%0APersian%20miniature%20painting%2C%20a%20rich%20cultural%20and%20artistic%20heritage.%20It%0Aintroduces%20a%20novel%20approach%20using%20Convolutional%20Neural%20Networks%20%28CNN%29%20to%0Aclassify%20Persian%20miniatures%20from%20five%20schools%3A%20Herat%2C%20Tabriz-e%20Avval%2C%20Shiraz-e%0AAvval%2C%20Tabriz-e%20Dovvom%2C%20and%20Qajar.%20The%20method%20achieves%20an%20average%20accuracy%20of%0Aover%2091%25.%20A%20meticulously%20curated%20dataset%20captures%20the%20distinct%20features%20of%20each%0Aschool%2C%20with%20a%20patch-based%20CNN%20approach%20classifying%20image%20segments%0Aindependently%20before%20merging%20results%20for%20enhanced%20accuracy.%20This%20research%0Acontributes%20significantly%20to%20digital%20art%20analysis%2C%20providing%20detailed%20insights%0Ainto%20the%20dataset%2C%20CNN%20architecture%2C%20training%2C%20and%20validation%20processes.%20It%0Ahighlights%20the%20potential%20for%20future%20advancements%20in%20automated%20art%20analysis%2C%0Abridging%20machine%20learning%2C%20art%20history%2C%20and%20digital%20humanities%2C%20thereby%20aiding%0Athe%20preservation%20and%20understanding%20of%20Persian%20cultural%20heritage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10330v1&entry.124074799=Read"},
{"title": "KPC-cF: Aspect-Based Sentiment Analysis via Implicit-Feature Alignment\n  with Corpus Filtering", "author": "Kibeom Nam", "abstract": "  Investigations into Aspect-Based Sentiment Analysis (ABSA) for Korean\nindustrial reviews are notably lacking in the existing literature. Our research\nproposes an intuitive and effective framework for ABSA in low-resource\nlanguages such as Korean. It optimizes prediction labels by integrating\ntranslated benchmark and unlabeled Korean data. Using a model fine-tuned on\ntranslated data, we pseudo-labeled the actual Korean NLI set. Subsequently, we\napplied LaBSE and \\MSP{}-based filtering to this pseudo-NLI set as implicit\nfeature, enhancing Aspect Category Detection and Polarity determination through\nadditional training. Incorporating dual filtering, this model bridged dataset\ngaps, achieving positive results in Korean ABSA with minimal resources. Through\nadditional data injection pipelines, our approach aims to utilize high-resource\ndata and construct effective models within communities, whether corporate or\nindividual, in low-resource language countries. Compared to English ABSA, our\nframework showed an approximately 3\\% difference in F1 scores and accuracy. We\nrelease the dataset and our code for Korean ABSA, at this link.\n", "link": "http://arxiv.org/abs/2407.00342v4", "date": "2024-11-15", "relevancy": 1.8329, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4677}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4626}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KPC-cF%3A%20Aspect-Based%20Sentiment%20Analysis%20via%20Implicit-Feature%20Alignment%0A%20%20with%20Corpus%20Filtering&body=Title%3A%20KPC-cF%3A%20Aspect-Based%20Sentiment%20Analysis%20via%20Implicit-Feature%20Alignment%0A%20%20with%20Corpus%20Filtering%0AAuthor%3A%20Kibeom%20Nam%0AAbstract%3A%20%20%20Investigations%20into%20Aspect-Based%20Sentiment%20Analysis%20%28ABSA%29%20for%20Korean%0Aindustrial%20reviews%20are%20notably%20lacking%20in%20the%20existing%20literature.%20Our%20research%0Aproposes%20an%20intuitive%20and%20effective%20framework%20for%20ABSA%20in%20low-resource%0Alanguages%20such%20as%20Korean.%20It%20optimizes%20prediction%20labels%20by%20integrating%0Atranslated%20benchmark%20and%20unlabeled%20Korean%20data.%20Using%20a%20model%20fine-tuned%20on%0Atranslated%20data%2C%20we%20pseudo-labeled%20the%20actual%20Korean%20NLI%20set.%20Subsequently%2C%20we%0Aapplied%20LaBSE%20and%20%5CMSP%7B%7D-based%20filtering%20to%20this%20pseudo-NLI%20set%20as%20implicit%0Afeature%2C%20enhancing%20Aspect%20Category%20Detection%20and%20Polarity%20determination%20through%0Aadditional%20training.%20Incorporating%20dual%20filtering%2C%20this%20model%20bridged%20dataset%0Agaps%2C%20achieving%20positive%20results%20in%20Korean%20ABSA%20with%20minimal%20resources.%20Through%0Aadditional%20data%20injection%20pipelines%2C%20our%20approach%20aims%20to%20utilize%20high-resource%0Adata%20and%20construct%20effective%20models%20within%20communities%2C%20whether%20corporate%20or%0Aindividual%2C%20in%20low-resource%20language%20countries.%20Compared%20to%20English%20ABSA%2C%20our%0Aframework%20showed%20an%20approximately%203%5C%25%20difference%20in%20F1%20scores%20and%20accuracy.%20We%0Arelease%20the%20dataset%20and%20our%20code%20for%20Korean%20ABSA%2C%20at%20this%20link.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.00342v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKPC-cF%253A%2520Aspect-Based%2520Sentiment%2520Analysis%2520via%2520Implicit-Feature%2520Alignment%250A%2520%2520with%2520Corpus%2520Filtering%26entry.906535625%3DKibeom%2520Nam%26entry.1292438233%3D%2520%2520Investigations%2520into%2520Aspect-Based%2520Sentiment%2520Analysis%2520%2528ABSA%2529%2520for%2520Korean%250Aindustrial%2520reviews%2520are%2520notably%2520lacking%2520in%2520the%2520existing%2520literature.%2520Our%2520research%250Aproposes%2520an%2520intuitive%2520and%2520effective%2520framework%2520for%2520ABSA%2520in%2520low-resource%250Alanguages%2520such%2520as%2520Korean.%2520It%2520optimizes%2520prediction%2520labels%2520by%2520integrating%250Atranslated%2520benchmark%2520and%2520unlabeled%2520Korean%2520data.%2520Using%2520a%2520model%2520fine-tuned%2520on%250Atranslated%2520data%252C%2520we%2520pseudo-labeled%2520the%2520actual%2520Korean%2520NLI%2520set.%2520Subsequently%252C%2520we%250Aapplied%2520LaBSE%2520and%2520%255CMSP%257B%257D-based%2520filtering%2520to%2520this%2520pseudo-NLI%2520set%2520as%2520implicit%250Afeature%252C%2520enhancing%2520Aspect%2520Category%2520Detection%2520and%2520Polarity%2520determination%2520through%250Aadditional%2520training.%2520Incorporating%2520dual%2520filtering%252C%2520this%2520model%2520bridged%2520dataset%250Agaps%252C%2520achieving%2520positive%2520results%2520in%2520Korean%2520ABSA%2520with%2520minimal%2520resources.%2520Through%250Aadditional%2520data%2520injection%2520pipelines%252C%2520our%2520approach%2520aims%2520to%2520utilize%2520high-resource%250Adata%2520and%2520construct%2520effective%2520models%2520within%2520communities%252C%2520whether%2520corporate%2520or%250Aindividual%252C%2520in%2520low-resource%2520language%2520countries.%2520Compared%2520to%2520English%2520ABSA%252C%2520our%250Aframework%2520showed%2520an%2520approximately%25203%255C%2525%2520difference%2520in%2520F1%2520scores%2520and%2520accuracy.%2520We%250Arelease%2520the%2520dataset%2520and%2520our%2520code%2520for%2520Korean%2520ABSA%252C%2520at%2520this%2520link.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.00342v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KPC-cF%3A%20Aspect-Based%20Sentiment%20Analysis%20via%20Implicit-Feature%20Alignment%0A%20%20with%20Corpus%20Filtering&entry.906535625=Kibeom%20Nam&entry.1292438233=%20%20Investigations%20into%20Aspect-Based%20Sentiment%20Analysis%20%28ABSA%29%20for%20Korean%0Aindustrial%20reviews%20are%20notably%20lacking%20in%20the%20existing%20literature.%20Our%20research%0Aproposes%20an%20intuitive%20and%20effective%20framework%20for%20ABSA%20in%20low-resource%0Alanguages%20such%20as%20Korean.%20It%20optimizes%20prediction%20labels%20by%20integrating%0Atranslated%20benchmark%20and%20unlabeled%20Korean%20data.%20Using%20a%20model%20fine-tuned%20on%0Atranslated%20data%2C%20we%20pseudo-labeled%20the%20actual%20Korean%20NLI%20set.%20Subsequently%2C%20we%0Aapplied%20LaBSE%20and%20%5CMSP%7B%7D-based%20filtering%20to%20this%20pseudo-NLI%20set%20as%20implicit%0Afeature%2C%20enhancing%20Aspect%20Category%20Detection%20and%20Polarity%20determination%20through%0Aadditional%20training.%20Incorporating%20dual%20filtering%2C%20this%20model%20bridged%20dataset%0Agaps%2C%20achieving%20positive%20results%20in%20Korean%20ABSA%20with%20minimal%20resources.%20Through%0Aadditional%20data%20injection%20pipelines%2C%20our%20approach%20aims%20to%20utilize%20high-resource%0Adata%20and%20construct%20effective%20models%20within%20communities%2C%20whether%20corporate%20or%0Aindividual%2C%20in%20low-resource%20language%20countries.%20Compared%20to%20English%20ABSA%2C%20our%0Aframework%20showed%20an%20approximately%203%5C%25%20difference%20in%20F1%20scores%20and%20accuracy.%20We%0Arelease%20the%20dataset%20and%20our%20code%20for%20Korean%20ABSA%2C%20at%20this%20link.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.00342v4&entry.124074799=Read"},
{"title": "Continual Adversarial Reinforcement Learning (CARL) of False Data\n  Injection detection: forgetting and explainability", "author": "Pooja Aslami and Kejun Chen and Timothy M. Hansen and Malik Hassanaly", "abstract": "  False data injection attacks (FDIAs) on smart inverters are a growing concern\nlinked to increased renewable energy production. While data-based FDIA\ndetection methods are also actively developed, we show that they remain\nvulnerable to impactful and stealthy adversarial examples that can be crafted\nusing Reinforcement Learning (RL). We propose to include such adversarial\nexamples in data-based detection training procedure via a continual adversarial\nRL (CARL) approach. This way, one can pinpoint the deficiencies of data-based\ndetection, thereby offering explainability during their incremental\nimprovement. We show that a continual learning implementation is subject to\ncatastrophic forgetting, and additionally show that forgetting can be addressed\nby employing a joint training strategy on all generated FDIA scenarios.\n", "link": "http://arxiv.org/abs/2411.10367v1", "date": "2024-11-15", "relevancy": 1.8284, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4679}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4561}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Adversarial%20Reinforcement%20Learning%20%28CARL%29%20of%20False%20Data%0A%20%20Injection%20detection%3A%20forgetting%20and%20explainability&body=Title%3A%20Continual%20Adversarial%20Reinforcement%20Learning%20%28CARL%29%20of%20False%20Data%0A%20%20Injection%20detection%3A%20forgetting%20and%20explainability%0AAuthor%3A%20Pooja%20Aslami%20and%20Kejun%20Chen%20and%20Timothy%20M.%20Hansen%20and%20Malik%20Hassanaly%0AAbstract%3A%20%20%20False%20data%20injection%20attacks%20%28FDIAs%29%20on%20smart%20inverters%20are%20a%20growing%20concern%0Alinked%20to%20increased%20renewable%20energy%20production.%20While%20data-based%20FDIA%0Adetection%20methods%20are%20also%20actively%20developed%2C%20we%20show%20that%20they%20remain%0Avulnerable%20to%20impactful%20and%20stealthy%20adversarial%20examples%20that%20can%20be%20crafted%0Ausing%20Reinforcement%20Learning%20%28RL%29.%20We%20propose%20to%20include%20such%20adversarial%0Aexamples%20in%20data-based%20detection%20training%20procedure%20via%20a%20continual%20adversarial%0ARL%20%28CARL%29%20approach.%20This%20way%2C%20one%20can%20pinpoint%20the%20deficiencies%20of%20data-based%0Adetection%2C%20thereby%20offering%20explainability%20during%20their%20incremental%0Aimprovement.%20We%20show%20that%20a%20continual%20learning%20implementation%20is%20subject%20to%0Acatastrophic%20forgetting%2C%20and%20additionally%20show%20that%20forgetting%20can%20be%20addressed%0Aby%20employing%20a%20joint%20training%20strategy%20on%20all%20generated%20FDIA%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10367v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Adversarial%2520Reinforcement%2520Learning%2520%2528CARL%2529%2520of%2520False%2520Data%250A%2520%2520Injection%2520detection%253A%2520forgetting%2520and%2520explainability%26entry.906535625%3DPooja%2520Aslami%2520and%2520Kejun%2520Chen%2520and%2520Timothy%2520M.%2520Hansen%2520and%2520Malik%2520Hassanaly%26entry.1292438233%3D%2520%2520False%2520data%2520injection%2520attacks%2520%2528FDIAs%2529%2520on%2520smart%2520inverters%2520are%2520a%2520growing%2520concern%250Alinked%2520to%2520increased%2520renewable%2520energy%2520production.%2520While%2520data-based%2520FDIA%250Adetection%2520methods%2520are%2520also%2520actively%2520developed%252C%2520we%2520show%2520that%2520they%2520remain%250Avulnerable%2520to%2520impactful%2520and%2520stealthy%2520adversarial%2520examples%2520that%2520can%2520be%2520crafted%250Ausing%2520Reinforcement%2520Learning%2520%2528RL%2529.%2520We%2520propose%2520to%2520include%2520such%2520adversarial%250Aexamples%2520in%2520data-based%2520detection%2520training%2520procedure%2520via%2520a%2520continual%2520adversarial%250ARL%2520%2528CARL%2529%2520approach.%2520This%2520way%252C%2520one%2520can%2520pinpoint%2520the%2520deficiencies%2520of%2520data-based%250Adetection%252C%2520thereby%2520offering%2520explainability%2520during%2520their%2520incremental%250Aimprovement.%2520We%2520show%2520that%2520a%2520continual%2520learning%2520implementation%2520is%2520subject%2520to%250Acatastrophic%2520forgetting%252C%2520and%2520additionally%2520show%2520that%2520forgetting%2520can%2520be%2520addressed%250Aby%2520employing%2520a%2520joint%2520training%2520strategy%2520on%2520all%2520generated%2520FDIA%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10367v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Adversarial%20Reinforcement%20Learning%20%28CARL%29%20of%20False%20Data%0A%20%20Injection%20detection%3A%20forgetting%20and%20explainability&entry.906535625=Pooja%20Aslami%20and%20Kejun%20Chen%20and%20Timothy%20M.%20Hansen%20and%20Malik%20Hassanaly&entry.1292438233=%20%20False%20data%20injection%20attacks%20%28FDIAs%29%20on%20smart%20inverters%20are%20a%20growing%20concern%0Alinked%20to%20increased%20renewable%20energy%20production.%20While%20data-based%20FDIA%0Adetection%20methods%20are%20also%20actively%20developed%2C%20we%20show%20that%20they%20remain%0Avulnerable%20to%20impactful%20and%20stealthy%20adversarial%20examples%20that%20can%20be%20crafted%0Ausing%20Reinforcement%20Learning%20%28RL%29.%20We%20propose%20to%20include%20such%20adversarial%0Aexamples%20in%20data-based%20detection%20training%20procedure%20via%20a%20continual%20adversarial%0ARL%20%28CARL%29%20approach.%20This%20way%2C%20one%20can%20pinpoint%20the%20deficiencies%20of%20data-based%0Adetection%2C%20thereby%20offering%20explainability%20during%20their%20incremental%0Aimprovement.%20We%20show%20that%20a%20continual%20learning%20implementation%20is%20subject%20to%0Acatastrophic%20forgetting%2C%20and%20additionally%20show%20that%20forgetting%20can%20be%20addressed%0Aby%20employing%20a%20joint%20training%20strategy%20on%20all%20generated%20FDIA%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10367v1&entry.124074799=Read"},
{"title": "Framework for Co-distillation Driven Federated Learning to Address Class\n  Imbalance in Healthcare", "author": "Suraj Racha and Shubh Gupta and Humaira Firdowse and Aastik Solanki and Ganesh Ramakrishnan and Kshitij S. Jadhav", "abstract": "  Federated Learning (FL) is a pioneering approach in distributed machine\nlearning, enabling collaborative model training across multiple clients while\nretaining data privacy. However, the inherent heterogeneity due to imbalanced\nresource representations across multiple clients poses significant challenges,\noften introducing bias towards the majority class. This issue is particularly\nprevalent in healthcare settings, where hospitals acting as clients share\nmedical images. To address class imbalance and reduce bias, we propose a\nco-distillation driven framework in a federated healthcare setting. Unlike\ntraditional federated setups with a designated server client, our framework\npromotes knowledge sharing among clients to collectively improve learning\noutcomes. Our experiments demonstrate that in a federated healthcare setting,\nco-distillation outperforms other federated methods in handling class\nimbalance. Additionally, we demonstrate that our framework has the least\nstandard deviation with increasing imbalance while outperforming other\nbaselines, signifying the robustness of our framework for FL in healthcare.\n", "link": "http://arxiv.org/abs/2411.10383v1", "date": "2024-11-15", "relevancy": 1.8177, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4665}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4528}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Framework%20for%20Co-distillation%20Driven%20Federated%20Learning%20to%20Address%20Class%0A%20%20Imbalance%20in%20Healthcare&body=Title%3A%20Framework%20for%20Co-distillation%20Driven%20Federated%20Learning%20to%20Address%20Class%0A%20%20Imbalance%20in%20Healthcare%0AAuthor%3A%20Suraj%20Racha%20and%20Shubh%20Gupta%20and%20Humaira%20Firdowse%20and%20Aastik%20Solanki%20and%20Ganesh%20Ramakrishnan%20and%20Kshitij%20S.%20Jadhav%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20is%20a%20pioneering%20approach%20in%20distributed%20machine%0Alearning%2C%20enabling%20collaborative%20model%20training%20across%20multiple%20clients%20while%0Aretaining%20data%20privacy.%20However%2C%20the%20inherent%20heterogeneity%20due%20to%20imbalanced%0Aresource%20representations%20across%20multiple%20clients%20poses%20significant%20challenges%2C%0Aoften%20introducing%20bias%20towards%20the%20majority%20class.%20This%20issue%20is%20particularly%0Aprevalent%20in%20healthcare%20settings%2C%20where%20hospitals%20acting%20as%20clients%20share%0Amedical%20images.%20To%20address%20class%20imbalance%20and%20reduce%20bias%2C%20we%20propose%20a%0Aco-distillation%20driven%20framework%20in%20a%20federated%20healthcare%20setting.%20Unlike%0Atraditional%20federated%20setups%20with%20a%20designated%20server%20client%2C%20our%20framework%0Apromotes%20knowledge%20sharing%20among%20clients%20to%20collectively%20improve%20learning%0Aoutcomes.%20Our%20experiments%20demonstrate%20that%20in%20a%20federated%20healthcare%20setting%2C%0Aco-distillation%20outperforms%20other%20federated%20methods%20in%20handling%20class%0Aimbalance.%20Additionally%2C%20we%20demonstrate%20that%20our%20framework%20has%20the%20least%0Astandard%20deviation%20with%20increasing%20imbalance%20while%20outperforming%20other%0Abaselines%2C%20signifying%20the%20robustness%20of%20our%20framework%20for%20FL%20in%20healthcare.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10383v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFramework%2520for%2520Co-distillation%2520Driven%2520Federated%2520Learning%2520to%2520Address%2520Class%250A%2520%2520Imbalance%2520in%2520Healthcare%26entry.906535625%3DSuraj%2520Racha%2520and%2520Shubh%2520Gupta%2520and%2520Humaira%2520Firdowse%2520and%2520Aastik%2520Solanki%2520and%2520Ganesh%2520Ramakrishnan%2520and%2520Kshitij%2520S.%2520Jadhav%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520is%2520a%2520pioneering%2520approach%2520in%2520distributed%2520machine%250Alearning%252C%2520enabling%2520collaborative%2520model%2520training%2520across%2520multiple%2520clients%2520while%250Aretaining%2520data%2520privacy.%2520However%252C%2520the%2520inherent%2520heterogeneity%2520due%2520to%2520imbalanced%250Aresource%2520representations%2520across%2520multiple%2520clients%2520poses%2520significant%2520challenges%252C%250Aoften%2520introducing%2520bias%2520towards%2520the%2520majority%2520class.%2520This%2520issue%2520is%2520particularly%250Aprevalent%2520in%2520healthcare%2520settings%252C%2520where%2520hospitals%2520acting%2520as%2520clients%2520share%250Amedical%2520images.%2520To%2520address%2520class%2520imbalance%2520and%2520reduce%2520bias%252C%2520we%2520propose%2520a%250Aco-distillation%2520driven%2520framework%2520in%2520a%2520federated%2520healthcare%2520setting.%2520Unlike%250Atraditional%2520federated%2520setups%2520with%2520a%2520designated%2520server%2520client%252C%2520our%2520framework%250Apromotes%2520knowledge%2520sharing%2520among%2520clients%2520to%2520collectively%2520improve%2520learning%250Aoutcomes.%2520Our%2520experiments%2520demonstrate%2520that%2520in%2520a%2520federated%2520healthcare%2520setting%252C%250Aco-distillation%2520outperforms%2520other%2520federated%2520methods%2520in%2520handling%2520class%250Aimbalance.%2520Additionally%252C%2520we%2520demonstrate%2520that%2520our%2520framework%2520has%2520the%2520least%250Astandard%2520deviation%2520with%2520increasing%2520imbalance%2520while%2520outperforming%2520other%250Abaselines%252C%2520signifying%2520the%2520robustness%2520of%2520our%2520framework%2520for%2520FL%2520in%2520healthcare.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10383v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Framework%20for%20Co-distillation%20Driven%20Federated%20Learning%20to%20Address%20Class%0A%20%20Imbalance%20in%20Healthcare&entry.906535625=Suraj%20Racha%20and%20Shubh%20Gupta%20and%20Humaira%20Firdowse%20and%20Aastik%20Solanki%20and%20Ganesh%20Ramakrishnan%20and%20Kshitij%20S.%20Jadhav&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20is%20a%20pioneering%20approach%20in%20distributed%20machine%0Alearning%2C%20enabling%20collaborative%20model%20training%20across%20multiple%20clients%20while%0Aretaining%20data%20privacy.%20However%2C%20the%20inherent%20heterogeneity%20due%20to%20imbalanced%0Aresource%20representations%20across%20multiple%20clients%20poses%20significant%20challenges%2C%0Aoften%20introducing%20bias%20towards%20the%20majority%20class.%20This%20issue%20is%20particularly%0Aprevalent%20in%20healthcare%20settings%2C%20where%20hospitals%20acting%20as%20clients%20share%0Amedical%20images.%20To%20address%20class%20imbalance%20and%20reduce%20bias%2C%20we%20propose%20a%0Aco-distillation%20driven%20framework%20in%20a%20federated%20healthcare%20setting.%20Unlike%0Atraditional%20federated%20setups%20with%20a%20designated%20server%20client%2C%20our%20framework%0Apromotes%20knowledge%20sharing%20among%20clients%20to%20collectively%20improve%20learning%0Aoutcomes.%20Our%20experiments%20demonstrate%20that%20in%20a%20federated%20healthcare%20setting%2C%0Aco-distillation%20outperforms%20other%20federated%20methods%20in%20handling%20class%0Aimbalance.%20Additionally%2C%20we%20demonstrate%20that%20our%20framework%20has%20the%20least%0Astandard%20deviation%20with%20increasing%20imbalance%20while%20outperforming%20other%0Abaselines%2C%20signifying%20the%20robustness%20of%20our%20framework%20for%20FL%20in%20healthcare.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10383v1&entry.124074799=Read"},
{"title": "Exploring GPU-to-GPU Communication: Insights into Supercomputer\n  Interconnects", "author": "Daniele De Sensi and Lorenzo Pichetti and Flavio Vella and Tiziano De Matteis and Zebin Ren and Luigi Fusco and Matteo Turisini and Daniele Cesarini and Kurt Lust and Animesh Trivedi and Duncan Roweth and Filippo Spiga and Salvatore Di Girolamo and Torsten Hoefler", "abstract": "  Multi-GPU nodes are increasingly common in the rapidly evolving landscape of\nexascale supercomputers. On these systems, GPUs on the same node are connected\nthrough dedicated networks, with bandwidths up to a few terabits per second.\nHowever, gauging performance expectations and maximizing system efficiency is\nchallenging due to different technologies, design options, and software layers.\nThis paper comprehensively characterizes three supercomputers - Alps, Leonardo,\nand LUMI - each with a unique architecture and design. We focus on performance\nevaluation of intra-node and inter-node interconnects on up to 4096 GPUs, using\na mix of intra-node and inter-node benchmarks. By analyzing its limitations and\nopportunities, we aim to offer practical guidance to researchers, system\narchitects, and software developers dealing with multi-GPU supercomputing. Our\nresults show that there is untapped bandwidth, and there are still many\nopportunities for optimization, ranging from network to software optimization.\n", "link": "http://arxiv.org/abs/2408.14090v2", "date": "2024-11-15", "relevancy": 1.7942, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4579}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4467}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20GPU-to-GPU%20Communication%3A%20Insights%20into%20Supercomputer%0A%20%20Interconnects&body=Title%3A%20Exploring%20GPU-to-GPU%20Communication%3A%20Insights%20into%20Supercomputer%0A%20%20Interconnects%0AAuthor%3A%20Daniele%20De%20Sensi%20and%20Lorenzo%20Pichetti%20and%20Flavio%20Vella%20and%20Tiziano%20De%20Matteis%20and%20Zebin%20Ren%20and%20Luigi%20Fusco%20and%20Matteo%20Turisini%20and%20Daniele%20Cesarini%20and%20Kurt%20Lust%20and%20Animesh%20Trivedi%20and%20Duncan%20Roweth%20and%20Filippo%20Spiga%20and%20Salvatore%20Di%20Girolamo%20and%20Torsten%20Hoefler%0AAbstract%3A%20%20%20Multi-GPU%20nodes%20are%20increasingly%20common%20in%20the%20rapidly%20evolving%20landscape%20of%0Aexascale%20supercomputers.%20On%20these%20systems%2C%20GPUs%20on%20the%20same%20node%20are%20connected%0Athrough%20dedicated%20networks%2C%20with%20bandwidths%20up%20to%20a%20few%20terabits%20per%20second.%0AHowever%2C%20gauging%20performance%20expectations%20and%20maximizing%20system%20efficiency%20is%0Achallenging%20due%20to%20different%20technologies%2C%20design%20options%2C%20and%20software%20layers.%0AThis%20paper%20comprehensively%20characterizes%20three%20supercomputers%20-%20Alps%2C%20Leonardo%2C%0Aand%20LUMI%20-%20each%20with%20a%20unique%20architecture%20and%20design.%20We%20focus%20on%20performance%0Aevaluation%20of%20intra-node%20and%20inter-node%20interconnects%20on%20up%20to%204096%20GPUs%2C%20using%0Aa%20mix%20of%20intra-node%20and%20inter-node%20benchmarks.%20By%20analyzing%20its%20limitations%20and%0Aopportunities%2C%20we%20aim%20to%20offer%20practical%20guidance%20to%20researchers%2C%20system%0Aarchitects%2C%20and%20software%20developers%20dealing%20with%20multi-GPU%20supercomputing.%20Our%0Aresults%20show%20that%20there%20is%20untapped%20bandwidth%2C%20and%20there%20are%20still%20many%0Aopportunities%20for%20optimization%2C%20ranging%20from%20network%20to%20software%20optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14090v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520GPU-to-GPU%2520Communication%253A%2520Insights%2520into%2520Supercomputer%250A%2520%2520Interconnects%26entry.906535625%3DDaniele%2520De%2520Sensi%2520and%2520Lorenzo%2520Pichetti%2520and%2520Flavio%2520Vella%2520and%2520Tiziano%2520De%2520Matteis%2520and%2520Zebin%2520Ren%2520and%2520Luigi%2520Fusco%2520and%2520Matteo%2520Turisini%2520and%2520Daniele%2520Cesarini%2520and%2520Kurt%2520Lust%2520and%2520Animesh%2520Trivedi%2520and%2520Duncan%2520Roweth%2520and%2520Filippo%2520Spiga%2520and%2520Salvatore%2520Di%2520Girolamo%2520and%2520Torsten%2520Hoefler%26entry.1292438233%3D%2520%2520Multi-GPU%2520nodes%2520are%2520increasingly%2520common%2520in%2520the%2520rapidly%2520evolving%2520landscape%2520of%250Aexascale%2520supercomputers.%2520On%2520these%2520systems%252C%2520GPUs%2520on%2520the%2520same%2520node%2520are%2520connected%250Athrough%2520dedicated%2520networks%252C%2520with%2520bandwidths%2520up%2520to%2520a%2520few%2520terabits%2520per%2520second.%250AHowever%252C%2520gauging%2520performance%2520expectations%2520and%2520maximizing%2520system%2520efficiency%2520is%250Achallenging%2520due%2520to%2520different%2520technologies%252C%2520design%2520options%252C%2520and%2520software%2520layers.%250AThis%2520paper%2520comprehensively%2520characterizes%2520three%2520supercomputers%2520-%2520Alps%252C%2520Leonardo%252C%250Aand%2520LUMI%2520-%2520each%2520with%2520a%2520unique%2520architecture%2520and%2520design.%2520We%2520focus%2520on%2520performance%250Aevaluation%2520of%2520intra-node%2520and%2520inter-node%2520interconnects%2520on%2520up%2520to%25204096%2520GPUs%252C%2520using%250Aa%2520mix%2520of%2520intra-node%2520and%2520inter-node%2520benchmarks.%2520By%2520analyzing%2520its%2520limitations%2520and%250Aopportunities%252C%2520we%2520aim%2520to%2520offer%2520practical%2520guidance%2520to%2520researchers%252C%2520system%250Aarchitects%252C%2520and%2520software%2520developers%2520dealing%2520with%2520multi-GPU%2520supercomputing.%2520Our%250Aresults%2520show%2520that%2520there%2520is%2520untapped%2520bandwidth%252C%2520and%2520there%2520are%2520still%2520many%250Aopportunities%2520for%2520optimization%252C%2520ranging%2520from%2520network%2520to%2520software%2520optimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14090v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20GPU-to-GPU%20Communication%3A%20Insights%20into%20Supercomputer%0A%20%20Interconnects&entry.906535625=Daniele%20De%20Sensi%20and%20Lorenzo%20Pichetti%20and%20Flavio%20Vella%20and%20Tiziano%20De%20Matteis%20and%20Zebin%20Ren%20and%20Luigi%20Fusco%20and%20Matteo%20Turisini%20and%20Daniele%20Cesarini%20and%20Kurt%20Lust%20and%20Animesh%20Trivedi%20and%20Duncan%20Roweth%20and%20Filippo%20Spiga%20and%20Salvatore%20Di%20Girolamo%20and%20Torsten%20Hoefler&entry.1292438233=%20%20Multi-GPU%20nodes%20are%20increasingly%20common%20in%20the%20rapidly%20evolving%20landscape%20of%0Aexascale%20supercomputers.%20On%20these%20systems%2C%20GPUs%20on%20the%20same%20node%20are%20connected%0Athrough%20dedicated%20networks%2C%20with%20bandwidths%20up%20to%20a%20few%20terabits%20per%20second.%0AHowever%2C%20gauging%20performance%20expectations%20and%20maximizing%20system%20efficiency%20is%0Achallenging%20due%20to%20different%20technologies%2C%20design%20options%2C%20and%20software%20layers.%0AThis%20paper%20comprehensively%20characterizes%20three%20supercomputers%20-%20Alps%2C%20Leonardo%2C%0Aand%20LUMI%20-%20each%20with%20a%20unique%20architecture%20and%20design.%20We%20focus%20on%20performance%0Aevaluation%20of%20intra-node%20and%20inter-node%20interconnects%20on%20up%20to%204096%20GPUs%2C%20using%0Aa%20mix%20of%20intra-node%20and%20inter-node%20benchmarks.%20By%20analyzing%20its%20limitations%20and%0Aopportunities%2C%20we%20aim%20to%20offer%20practical%20guidance%20to%20researchers%2C%20system%0Aarchitects%2C%20and%20software%20developers%20dealing%20with%20multi-GPU%20supercomputing.%20Our%0Aresults%20show%20that%20there%20is%20untapped%20bandwidth%2C%20and%20there%20are%20still%20many%0Aopportunities%20for%20optimization%2C%20ranging%20from%20network%20to%20software%20optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14090v2&entry.124074799=Read"},
{"title": "Swarm Characteristics Classification Using Neural Networks", "author": "Donald W. Peltier III and Isaac Kaminer and Abram Clark and Marko Orescanin", "abstract": "  Understanding the characteristics of swarming autonomous agents is critical\nfor defense and security applications. This article presents a study on using\nsupervised neural network time series classification (NN TSC) to predict key\nattributes and tactics of swarming autonomous agents for military contexts.\nSpecifically, NN TSC is applied to infer two binary attributes - communication\nand proportional navigation - which combine to define four mutually exclusive\nswarm tactics. We identify a gap in literature on using NNs for swarm\nclassification and demonstrate the effectiveness of NN TSC in rapidly deducing\nintelligence about attacking swarms to inform counter-maneuvers. Through\nsimulated swarm-vs-swarm engagements, we evaluate NN TSC performance in terms\nof observation window requirements, noise robustness, and scalability to swarm\nsize. Key findings show NNs can predict swarm behaviors with 97% accuracy using\nshort observation windows of 20 time steps, while also demonstrating graceful\ndegradation down to 80% accuracy under 50% noise, as well as excellent\nscalability to swarm sizes from 10 to 100 agents. These capabilities are\npromising for real-time decision-making support in defense scenarios by rapidly\ninferring insights about swarm behavior.\n", "link": "http://arxiv.org/abs/2403.19572v2", "date": "2024-11-15", "relevancy": 1.7768, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4679}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4445}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Swarm%20Characteristics%20Classification%20Using%20Neural%20Networks&body=Title%3A%20Swarm%20Characteristics%20Classification%20Using%20Neural%20Networks%0AAuthor%3A%20Donald%20W.%20Peltier%20III%20and%20Isaac%20Kaminer%20and%20Abram%20Clark%20and%20Marko%20Orescanin%0AAbstract%3A%20%20%20Understanding%20the%20characteristics%20of%20swarming%20autonomous%20agents%20is%20critical%0Afor%20defense%20and%20security%20applications.%20This%20article%20presents%20a%20study%20on%20using%0Asupervised%20neural%20network%20time%20series%20classification%20%28NN%20TSC%29%20to%20predict%20key%0Aattributes%20and%20tactics%20of%20swarming%20autonomous%20agents%20for%20military%20contexts.%0ASpecifically%2C%20NN%20TSC%20is%20applied%20to%20infer%20two%20binary%20attributes%20-%20communication%0Aand%20proportional%20navigation%20-%20which%20combine%20to%20define%20four%20mutually%20exclusive%0Aswarm%20tactics.%20We%20identify%20a%20gap%20in%20literature%20on%20using%20NNs%20for%20swarm%0Aclassification%20and%20demonstrate%20the%20effectiveness%20of%20NN%20TSC%20in%20rapidly%20deducing%0Aintelligence%20about%20attacking%20swarms%20to%20inform%20counter-maneuvers.%20Through%0Asimulated%20swarm-vs-swarm%20engagements%2C%20we%20evaluate%20NN%20TSC%20performance%20in%20terms%0Aof%20observation%20window%20requirements%2C%20noise%20robustness%2C%20and%20scalability%20to%20swarm%0Asize.%20Key%20findings%20show%20NNs%20can%20predict%20swarm%20behaviors%20with%2097%25%20accuracy%20using%0Ashort%20observation%20windows%20of%2020%20time%20steps%2C%20while%20also%20demonstrating%20graceful%0Adegradation%20down%20to%2080%25%20accuracy%20under%2050%25%20noise%2C%20as%20well%20as%20excellent%0Ascalability%20to%20swarm%20sizes%20from%2010%20to%20100%20agents.%20These%20capabilities%20are%0Apromising%20for%20real-time%20decision-making%20support%20in%20defense%20scenarios%20by%20rapidly%0Ainferring%20insights%20about%20swarm%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19572v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSwarm%2520Characteristics%2520Classification%2520Using%2520Neural%2520Networks%26entry.906535625%3DDonald%2520W.%2520Peltier%2520III%2520and%2520Isaac%2520Kaminer%2520and%2520Abram%2520Clark%2520and%2520Marko%2520Orescanin%26entry.1292438233%3D%2520%2520Understanding%2520the%2520characteristics%2520of%2520swarming%2520autonomous%2520agents%2520is%2520critical%250Afor%2520defense%2520and%2520security%2520applications.%2520This%2520article%2520presents%2520a%2520study%2520on%2520using%250Asupervised%2520neural%2520network%2520time%2520series%2520classification%2520%2528NN%2520TSC%2529%2520to%2520predict%2520key%250Aattributes%2520and%2520tactics%2520of%2520swarming%2520autonomous%2520agents%2520for%2520military%2520contexts.%250ASpecifically%252C%2520NN%2520TSC%2520is%2520applied%2520to%2520infer%2520two%2520binary%2520attributes%2520-%2520communication%250Aand%2520proportional%2520navigation%2520-%2520which%2520combine%2520to%2520define%2520four%2520mutually%2520exclusive%250Aswarm%2520tactics.%2520We%2520identify%2520a%2520gap%2520in%2520literature%2520on%2520using%2520NNs%2520for%2520swarm%250Aclassification%2520and%2520demonstrate%2520the%2520effectiveness%2520of%2520NN%2520TSC%2520in%2520rapidly%2520deducing%250Aintelligence%2520about%2520attacking%2520swarms%2520to%2520inform%2520counter-maneuvers.%2520Through%250Asimulated%2520swarm-vs-swarm%2520engagements%252C%2520we%2520evaluate%2520NN%2520TSC%2520performance%2520in%2520terms%250Aof%2520observation%2520window%2520requirements%252C%2520noise%2520robustness%252C%2520and%2520scalability%2520to%2520swarm%250Asize.%2520Key%2520findings%2520show%2520NNs%2520can%2520predict%2520swarm%2520behaviors%2520with%252097%2525%2520accuracy%2520using%250Ashort%2520observation%2520windows%2520of%252020%2520time%2520steps%252C%2520while%2520also%2520demonstrating%2520graceful%250Adegradation%2520down%2520to%252080%2525%2520accuracy%2520under%252050%2525%2520noise%252C%2520as%2520well%2520as%2520excellent%250Ascalability%2520to%2520swarm%2520sizes%2520from%252010%2520to%2520100%2520agents.%2520These%2520capabilities%2520are%250Apromising%2520for%2520real-time%2520decision-making%2520support%2520in%2520defense%2520scenarios%2520by%2520rapidly%250Ainferring%2520insights%2520about%2520swarm%2520behavior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.19572v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Swarm%20Characteristics%20Classification%20Using%20Neural%20Networks&entry.906535625=Donald%20W.%20Peltier%20III%20and%20Isaac%20Kaminer%20and%20Abram%20Clark%20and%20Marko%20Orescanin&entry.1292438233=%20%20Understanding%20the%20characteristics%20of%20swarming%20autonomous%20agents%20is%20critical%0Afor%20defense%20and%20security%20applications.%20This%20article%20presents%20a%20study%20on%20using%0Asupervised%20neural%20network%20time%20series%20classification%20%28NN%20TSC%29%20to%20predict%20key%0Aattributes%20and%20tactics%20of%20swarming%20autonomous%20agents%20for%20military%20contexts.%0ASpecifically%2C%20NN%20TSC%20is%20applied%20to%20infer%20two%20binary%20attributes%20-%20communication%0Aand%20proportional%20navigation%20-%20which%20combine%20to%20define%20four%20mutually%20exclusive%0Aswarm%20tactics.%20We%20identify%20a%20gap%20in%20literature%20on%20using%20NNs%20for%20swarm%0Aclassification%20and%20demonstrate%20the%20effectiveness%20of%20NN%20TSC%20in%20rapidly%20deducing%0Aintelligence%20about%20attacking%20swarms%20to%20inform%20counter-maneuvers.%20Through%0Asimulated%20swarm-vs-swarm%20engagements%2C%20we%20evaluate%20NN%20TSC%20performance%20in%20terms%0Aof%20observation%20window%20requirements%2C%20noise%20robustness%2C%20and%20scalability%20to%20swarm%0Asize.%20Key%20findings%20show%20NNs%20can%20predict%20swarm%20behaviors%20with%2097%25%20accuracy%20using%0Ashort%20observation%20windows%20of%2020%20time%20steps%2C%20while%20also%20demonstrating%20graceful%0Adegradation%20down%20to%2080%25%20accuracy%20under%2050%25%20noise%2C%20as%20well%20as%20excellent%0Ascalability%20to%20swarm%20sizes%20from%2010%20to%20100%20agents.%20These%20capabilities%20are%0Apromising%20for%20real-time%20decision-making%20support%20in%20defense%20scenarios%20by%20rapidly%0Ainferring%20insights%20about%20swarm%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19572v2&entry.124074799=Read"},
{"title": "Optimization-based Prompt Injection Attack to LLM-as-a-Judge", "author": "Jiawen Shi and Zenghui Yuan and Yinuo Liu and Yue Huang and Pan Zhou and Lichao Sun and Neil Zhenqiang Gong", "abstract": "  LLM-as-a-Judge uses a large language model (LLM) to select the best response\nfrom a set of candidates for a given question. LLM-as-a-Judge has many\napplications such as LLM-powered search, reinforcement learning with AI\nfeedback (RLAIF), and tool selection. In this work, we propose JudgeDeceiver,\nan optimization-based prompt injection attack to LLM-as-a-Judge. JudgeDeceiver\ninjects a carefully crafted sequence into an attacker-controlled candidate\nresponse such that LLM-as-a-Judge selects the candidate response for an\nattacker-chosen question no matter what other candidate responses are.\nSpecifically, we formulate finding such sequence as an optimization problem and\npropose a gradient based method to approximately solve it. Our extensive\nevaluation shows that JudgeDeceive is highly effective, and is much more\neffective than existing prompt injection attacks that manually craft the\ninjected sequences and jailbreak attacks when extended to our problem. We also\nshow the effectiveness of JudgeDeceiver in three case studies, i.e.,\nLLM-powered search, RLAIF, and tool selection. Moreover, we consider defenses\nincluding known-answer detection, perplexity detection, and perplexity windowed\ndetection. Our results show these defenses are insufficient, highlighting the\nurgent need for developing new defense strategies. Our implementation is\navailable at this repository: https://github.com/ShiJiawenwen/JudgeDeceiver.\n", "link": "http://arxiv.org/abs/2403.17710v3", "date": "2024-11-15", "relevancy": 1.7758, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4754}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4383}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimization-based%20Prompt%20Injection%20Attack%20to%20LLM-as-a-Judge&body=Title%3A%20Optimization-based%20Prompt%20Injection%20Attack%20to%20LLM-as-a-Judge%0AAuthor%3A%20Jiawen%20Shi%20and%20Zenghui%20Yuan%20and%20Yinuo%20Liu%20and%20Yue%20Huang%20and%20Pan%20Zhou%20and%20Lichao%20Sun%20and%20Neil%20Zhenqiang%20Gong%0AAbstract%3A%20%20%20LLM-as-a-Judge%20uses%20a%20large%20language%20model%20%28LLM%29%20to%20select%20the%20best%20response%0Afrom%20a%20set%20of%20candidates%20for%20a%20given%20question.%20LLM-as-a-Judge%20has%20many%0Aapplications%20such%20as%20LLM-powered%20search%2C%20reinforcement%20learning%20with%20AI%0Afeedback%20%28RLAIF%29%2C%20and%20tool%20selection.%20In%20this%20work%2C%20we%20propose%20JudgeDeceiver%2C%0Aan%20optimization-based%20prompt%20injection%20attack%20to%20LLM-as-a-Judge.%20JudgeDeceiver%0Ainjects%20a%20carefully%20crafted%20sequence%20into%20an%20attacker-controlled%20candidate%0Aresponse%20such%20that%20LLM-as-a-Judge%20selects%20the%20candidate%20response%20for%20an%0Aattacker-chosen%20question%20no%20matter%20what%20other%20candidate%20responses%20are.%0ASpecifically%2C%20we%20formulate%20finding%20such%20sequence%20as%20an%20optimization%20problem%20and%0Apropose%20a%20gradient%20based%20method%20to%20approximately%20solve%20it.%20Our%20extensive%0Aevaluation%20shows%20that%20JudgeDeceive%20is%20highly%20effective%2C%20and%20is%20much%20more%0Aeffective%20than%20existing%20prompt%20injection%20attacks%20that%20manually%20craft%20the%0Ainjected%20sequences%20and%20jailbreak%20attacks%20when%20extended%20to%20our%20problem.%20We%20also%0Ashow%20the%20effectiveness%20of%20JudgeDeceiver%20in%20three%20case%20studies%2C%20i.e.%2C%0ALLM-powered%20search%2C%20RLAIF%2C%20and%20tool%20selection.%20Moreover%2C%20we%20consider%20defenses%0Aincluding%20known-answer%20detection%2C%20perplexity%20detection%2C%20and%20perplexity%20windowed%0Adetection.%20Our%20results%20show%20these%20defenses%20are%20insufficient%2C%20highlighting%20the%0Aurgent%20need%20for%20developing%20new%20defense%20strategies.%20Our%20implementation%20is%0Aavailable%20at%20this%20repository%3A%20https%3A//github.com/ShiJiawenwen/JudgeDeceiver.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17710v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimization-based%2520Prompt%2520Injection%2520Attack%2520to%2520LLM-as-a-Judge%26entry.906535625%3DJiawen%2520Shi%2520and%2520Zenghui%2520Yuan%2520and%2520Yinuo%2520Liu%2520and%2520Yue%2520Huang%2520and%2520Pan%2520Zhou%2520and%2520Lichao%2520Sun%2520and%2520Neil%2520Zhenqiang%2520Gong%26entry.1292438233%3D%2520%2520LLM-as-a-Judge%2520uses%2520a%2520large%2520language%2520model%2520%2528LLM%2529%2520to%2520select%2520the%2520best%2520response%250Afrom%2520a%2520set%2520of%2520candidates%2520for%2520a%2520given%2520question.%2520LLM-as-a-Judge%2520has%2520many%250Aapplications%2520such%2520as%2520LLM-powered%2520search%252C%2520reinforcement%2520learning%2520with%2520AI%250Afeedback%2520%2528RLAIF%2529%252C%2520and%2520tool%2520selection.%2520In%2520this%2520work%252C%2520we%2520propose%2520JudgeDeceiver%252C%250Aan%2520optimization-based%2520prompt%2520injection%2520attack%2520to%2520LLM-as-a-Judge.%2520JudgeDeceiver%250Ainjects%2520a%2520carefully%2520crafted%2520sequence%2520into%2520an%2520attacker-controlled%2520candidate%250Aresponse%2520such%2520that%2520LLM-as-a-Judge%2520selects%2520the%2520candidate%2520response%2520for%2520an%250Aattacker-chosen%2520question%2520no%2520matter%2520what%2520other%2520candidate%2520responses%2520are.%250ASpecifically%252C%2520we%2520formulate%2520finding%2520such%2520sequence%2520as%2520an%2520optimization%2520problem%2520and%250Apropose%2520a%2520gradient%2520based%2520method%2520to%2520approximately%2520solve%2520it.%2520Our%2520extensive%250Aevaluation%2520shows%2520that%2520JudgeDeceive%2520is%2520highly%2520effective%252C%2520and%2520is%2520much%2520more%250Aeffective%2520than%2520existing%2520prompt%2520injection%2520attacks%2520that%2520manually%2520craft%2520the%250Ainjected%2520sequences%2520and%2520jailbreak%2520attacks%2520when%2520extended%2520to%2520our%2520problem.%2520We%2520also%250Ashow%2520the%2520effectiveness%2520of%2520JudgeDeceiver%2520in%2520three%2520case%2520studies%252C%2520i.e.%252C%250ALLM-powered%2520search%252C%2520RLAIF%252C%2520and%2520tool%2520selection.%2520Moreover%252C%2520we%2520consider%2520defenses%250Aincluding%2520known-answer%2520detection%252C%2520perplexity%2520detection%252C%2520and%2520perplexity%2520windowed%250Adetection.%2520Our%2520results%2520show%2520these%2520defenses%2520are%2520insufficient%252C%2520highlighting%2520the%250Aurgent%2520need%2520for%2520developing%2520new%2520defense%2520strategies.%2520Our%2520implementation%2520is%250Aavailable%2520at%2520this%2520repository%253A%2520https%253A//github.com/ShiJiawenwen/JudgeDeceiver.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.17710v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimization-based%20Prompt%20Injection%20Attack%20to%20LLM-as-a-Judge&entry.906535625=Jiawen%20Shi%20and%20Zenghui%20Yuan%20and%20Yinuo%20Liu%20and%20Yue%20Huang%20and%20Pan%20Zhou%20and%20Lichao%20Sun%20and%20Neil%20Zhenqiang%20Gong&entry.1292438233=%20%20LLM-as-a-Judge%20uses%20a%20large%20language%20model%20%28LLM%29%20to%20select%20the%20best%20response%0Afrom%20a%20set%20of%20candidates%20for%20a%20given%20question.%20LLM-as-a-Judge%20has%20many%0Aapplications%20such%20as%20LLM-powered%20search%2C%20reinforcement%20learning%20with%20AI%0Afeedback%20%28RLAIF%29%2C%20and%20tool%20selection.%20In%20this%20work%2C%20we%20propose%20JudgeDeceiver%2C%0Aan%20optimization-based%20prompt%20injection%20attack%20to%20LLM-as-a-Judge.%20JudgeDeceiver%0Ainjects%20a%20carefully%20crafted%20sequence%20into%20an%20attacker-controlled%20candidate%0Aresponse%20such%20that%20LLM-as-a-Judge%20selects%20the%20candidate%20response%20for%20an%0Aattacker-chosen%20question%20no%20matter%20what%20other%20candidate%20responses%20are.%0ASpecifically%2C%20we%20formulate%20finding%20such%20sequence%20as%20an%20optimization%20problem%20and%0Apropose%20a%20gradient%20based%20method%20to%20approximately%20solve%20it.%20Our%20extensive%0Aevaluation%20shows%20that%20JudgeDeceive%20is%20highly%20effective%2C%20and%20is%20much%20more%0Aeffective%20than%20existing%20prompt%20injection%20attacks%20that%20manually%20craft%20the%0Ainjected%20sequences%20and%20jailbreak%20attacks%20when%20extended%20to%20our%20problem.%20We%20also%0Ashow%20the%20effectiveness%20of%20JudgeDeceiver%20in%20three%20case%20studies%2C%20i.e.%2C%0ALLM-powered%20search%2C%20RLAIF%2C%20and%20tool%20selection.%20Moreover%2C%20we%20consider%20defenses%0Aincluding%20known-answer%20detection%2C%20perplexity%20detection%2C%20and%20perplexity%20windowed%0Adetection.%20Our%20results%20show%20these%20defenses%20are%20insufficient%2C%20highlighting%20the%0Aurgent%20need%20for%20developing%20new%20defense%20strategies.%20Our%20implementation%20is%0Aavailable%20at%20this%20repository%3A%20https%3A//github.com/ShiJiawenwen/JudgeDeceiver.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17710v3&entry.124074799=Read"},
{"title": "Repurposing Stable Diffusion Attention for Training-Free Unsupervised\n  Interactive Segmentation", "author": "Markus Karmann and Onay Urfalioglu", "abstract": "  Recent progress in interactive point prompt based Image Segmentation allows\nto significantly reduce the manual effort to obtain high quality semantic\nlabels. State-of-the-art unsupervised methods use self-supervised pre-trained\nmodels to obtain pseudo-labels which are used in training a prompt-based\nsegmentation model. In this paper, we propose a novel unsupervised and\ntraining-free approach based solely on the self-attention of Stable Diffusion.\nWe interpret the self-attention tensor as a Markov transition operator, which\nenables us to iteratively construct a Markov chain. Pixel-wise counting of the\nrequired number of iterations along the Markov-chain to reach a relative\nprobability threshold yields a Markov-iteration-map, which we simply call a\nMarkov-map. Compared to the raw attention maps, we show that our proposed\nMarkov-map has less noise, sharper semantic boundaries and more uniform values\nwithin semantically similar regions. We integrate the Markov-map in a simple\nyet effective truncated nearest neighbor framework to obtain interactive point\nprompt based segmentation. Despite being training-free, we experimentally show\nthat our approach yields excellent results in terms of Number of Clicks (NoC),\neven outperforming state-of-the-art training based unsupervised methods in most\nof the datasets.\n", "link": "http://arxiv.org/abs/2411.10411v1", "date": "2024-11-15", "relevancy": 1.7666, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6261}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5594}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Repurposing%20Stable%20Diffusion%20Attention%20for%20Training-Free%20Unsupervised%0A%20%20Interactive%20Segmentation&body=Title%3A%20Repurposing%20Stable%20Diffusion%20Attention%20for%20Training-Free%20Unsupervised%0A%20%20Interactive%20Segmentation%0AAuthor%3A%20Markus%20Karmann%20and%20Onay%20Urfalioglu%0AAbstract%3A%20%20%20Recent%20progress%20in%20interactive%20point%20prompt%20based%20Image%20Segmentation%20allows%0Ato%20significantly%20reduce%20the%20manual%20effort%20to%20obtain%20high%20quality%20semantic%0Alabels.%20State-of-the-art%20unsupervised%20methods%20use%20self-supervised%20pre-trained%0Amodels%20to%20obtain%20pseudo-labels%20which%20are%20used%20in%20training%20a%20prompt-based%0Asegmentation%20model.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20unsupervised%20and%0Atraining-free%20approach%20based%20solely%20on%20the%20self-attention%20of%20Stable%20Diffusion.%0AWe%20interpret%20the%20self-attention%20tensor%20as%20a%20Markov%20transition%20operator%2C%20which%0Aenables%20us%20to%20iteratively%20construct%20a%20Markov%20chain.%20Pixel-wise%20counting%20of%20the%0Arequired%20number%20of%20iterations%20along%20the%20Markov-chain%20to%20reach%20a%20relative%0Aprobability%20threshold%20yields%20a%20Markov-iteration-map%2C%20which%20we%20simply%20call%20a%0AMarkov-map.%20Compared%20to%20the%20raw%20attention%20maps%2C%20we%20show%20that%20our%20proposed%0AMarkov-map%20has%20less%20noise%2C%20sharper%20semantic%20boundaries%20and%20more%20uniform%20values%0Awithin%20semantically%20similar%20regions.%20We%20integrate%20the%20Markov-map%20in%20a%20simple%0Ayet%20effective%20truncated%20nearest%20neighbor%20framework%20to%20obtain%20interactive%20point%0Aprompt%20based%20segmentation.%20Despite%20being%20training-free%2C%20we%20experimentally%20show%0Athat%20our%20approach%20yields%20excellent%20results%20in%20terms%20of%20Number%20of%20Clicks%20%28NoC%29%2C%0Aeven%20outperforming%20state-of-the-art%20training%20based%20unsupervised%20methods%20in%20most%0Aof%20the%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10411v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepurposing%2520Stable%2520Diffusion%2520Attention%2520for%2520Training-Free%2520Unsupervised%250A%2520%2520Interactive%2520Segmentation%26entry.906535625%3DMarkus%2520Karmann%2520and%2520Onay%2520Urfalioglu%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520interactive%2520point%2520prompt%2520based%2520Image%2520Segmentation%2520allows%250Ato%2520significantly%2520reduce%2520the%2520manual%2520effort%2520to%2520obtain%2520high%2520quality%2520semantic%250Alabels.%2520State-of-the-art%2520unsupervised%2520methods%2520use%2520self-supervised%2520pre-trained%250Amodels%2520to%2520obtain%2520pseudo-labels%2520which%2520are%2520used%2520in%2520training%2520a%2520prompt-based%250Asegmentation%2520model.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520unsupervised%2520and%250Atraining-free%2520approach%2520based%2520solely%2520on%2520the%2520self-attention%2520of%2520Stable%2520Diffusion.%250AWe%2520interpret%2520the%2520self-attention%2520tensor%2520as%2520a%2520Markov%2520transition%2520operator%252C%2520which%250Aenables%2520us%2520to%2520iteratively%2520construct%2520a%2520Markov%2520chain.%2520Pixel-wise%2520counting%2520of%2520the%250Arequired%2520number%2520of%2520iterations%2520along%2520the%2520Markov-chain%2520to%2520reach%2520a%2520relative%250Aprobability%2520threshold%2520yields%2520a%2520Markov-iteration-map%252C%2520which%2520we%2520simply%2520call%2520a%250AMarkov-map.%2520Compared%2520to%2520the%2520raw%2520attention%2520maps%252C%2520we%2520show%2520that%2520our%2520proposed%250AMarkov-map%2520has%2520less%2520noise%252C%2520sharper%2520semantic%2520boundaries%2520and%2520more%2520uniform%2520values%250Awithin%2520semantically%2520similar%2520regions.%2520We%2520integrate%2520the%2520Markov-map%2520in%2520a%2520simple%250Ayet%2520effective%2520truncated%2520nearest%2520neighbor%2520framework%2520to%2520obtain%2520interactive%2520point%250Aprompt%2520based%2520segmentation.%2520Despite%2520being%2520training-free%252C%2520we%2520experimentally%2520show%250Athat%2520our%2520approach%2520yields%2520excellent%2520results%2520in%2520terms%2520of%2520Number%2520of%2520Clicks%2520%2528NoC%2529%252C%250Aeven%2520outperforming%2520state-of-the-art%2520training%2520based%2520unsupervised%2520methods%2520in%2520most%250Aof%2520the%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10411v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Repurposing%20Stable%20Diffusion%20Attention%20for%20Training-Free%20Unsupervised%0A%20%20Interactive%20Segmentation&entry.906535625=Markus%20Karmann%20and%20Onay%20Urfalioglu&entry.1292438233=%20%20Recent%20progress%20in%20interactive%20point%20prompt%20based%20Image%20Segmentation%20allows%0Ato%20significantly%20reduce%20the%20manual%20effort%20to%20obtain%20high%20quality%20semantic%0Alabels.%20State-of-the-art%20unsupervised%20methods%20use%20self-supervised%20pre-trained%0Amodels%20to%20obtain%20pseudo-labels%20which%20are%20used%20in%20training%20a%20prompt-based%0Asegmentation%20model.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20unsupervised%20and%0Atraining-free%20approach%20based%20solely%20on%20the%20self-attention%20of%20Stable%20Diffusion.%0AWe%20interpret%20the%20self-attention%20tensor%20as%20a%20Markov%20transition%20operator%2C%20which%0Aenables%20us%20to%20iteratively%20construct%20a%20Markov%20chain.%20Pixel-wise%20counting%20of%20the%0Arequired%20number%20of%20iterations%20along%20the%20Markov-chain%20to%20reach%20a%20relative%0Aprobability%20threshold%20yields%20a%20Markov-iteration-map%2C%20which%20we%20simply%20call%20a%0AMarkov-map.%20Compared%20to%20the%20raw%20attention%20maps%2C%20we%20show%20that%20our%20proposed%0AMarkov-map%20has%20less%20noise%2C%20sharper%20semantic%20boundaries%20and%20more%20uniform%20values%0Awithin%20semantically%20similar%20regions.%20We%20integrate%20the%20Markov-map%20in%20a%20simple%0Ayet%20effective%20truncated%20nearest%20neighbor%20framework%20to%20obtain%20interactive%20point%0Aprompt%20based%20segmentation.%20Despite%20being%20training-free%2C%20we%20experimentally%20show%0Athat%20our%20approach%20yields%20excellent%20results%20in%20terms%20of%20Number%20of%20Clicks%20%28NoC%29%2C%0Aeven%20outperforming%20state-of-the-art%20training%20based%20unsupervised%20methods%20in%20most%0Aof%20the%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10411v1&entry.124074799=Read"},
{"title": "Open LLMs are Necessary for Current Private Adaptations and Outperform\n  their Closed Alternatives", "author": "Vincent Hanke and Tom Blanchard and Franziska Boenisch and Iyiola Emmanuel Olatunji and Michael Backes and Adam Dziedzic", "abstract": "  While open Large Language Models (LLMs) have made significant progress, they\nstill fall short of matching the performance of their closed, proprietary\ncounterparts, making the latter attractive even for the use on highly private\ndata. Recently, various new methods have been proposed to adapt closed LLMs to\nprivate data without leaking private information to third parties and/or the\nLLM provider. In this work, we analyze the privacy protection and performance\nof the four most recent methods for private adaptation of closed LLMs. By\nexamining their threat models and thoroughly comparing their performance under\ndifferent privacy levels according to differential privacy (DP), various LLM\narchitectures, and multiple datasets for classification and generation tasks,\nwe find that: (1) all the methods leak query data, i.e., the (potentially\nsensitive) user data that is queried at inference time, to the LLM provider,\n(2) three out of four methods also leak large fractions of private training\ndata to the LLM provider while the method that protects private data requires a\nlocal open LLM, (3) all the methods exhibit lower performance compared to three\nprivate gradient-based adaptation methods for local open LLMs, and (4) the\nprivate adaptation methods for closed LLMs incur higher monetary training and\nquery costs than running the alternative methods on local open LLMs. This\nyields the conclusion that, to achieve truly privacy-preserving LLM adaptations\nthat yield high performance and more privacy at lower costs, taking into\naccount current methods and models, one should use open LLMs.\n", "link": "http://arxiv.org/abs/2411.05818v2", "date": "2024-11-15", "relevancy": 1.766, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4544}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.434}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4316}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open%20LLMs%20are%20Necessary%20for%20Current%20Private%20Adaptations%20and%20Outperform%0A%20%20their%20Closed%20Alternatives&body=Title%3A%20Open%20LLMs%20are%20Necessary%20for%20Current%20Private%20Adaptations%20and%20Outperform%0A%20%20their%20Closed%20Alternatives%0AAuthor%3A%20Vincent%20Hanke%20and%20Tom%20Blanchard%20and%20Franziska%20Boenisch%20and%20Iyiola%20Emmanuel%20Olatunji%20and%20Michael%20Backes%20and%20Adam%20Dziedzic%0AAbstract%3A%20%20%20While%20open%20Large%20Language%20Models%20%28LLMs%29%20have%20made%20significant%20progress%2C%20they%0Astill%20fall%20short%20of%20matching%20the%20performance%20of%20their%20closed%2C%20proprietary%0Acounterparts%2C%20making%20the%20latter%20attractive%20even%20for%20the%20use%20on%20highly%20private%0Adata.%20Recently%2C%20various%20new%20methods%20have%20been%20proposed%20to%20adapt%20closed%20LLMs%20to%0Aprivate%20data%20without%20leaking%20private%20information%20to%20third%20parties%20and/or%20the%0ALLM%20provider.%20In%20this%20work%2C%20we%20analyze%20the%20privacy%20protection%20and%20performance%0Aof%20the%20four%20most%20recent%20methods%20for%20private%20adaptation%20of%20closed%20LLMs.%20By%0Aexamining%20their%20threat%20models%20and%20thoroughly%20comparing%20their%20performance%20under%0Adifferent%20privacy%20levels%20according%20to%20differential%20privacy%20%28DP%29%2C%20various%20LLM%0Aarchitectures%2C%20and%20multiple%20datasets%20for%20classification%20and%20generation%20tasks%2C%0Awe%20find%20that%3A%20%281%29%20all%20the%20methods%20leak%20query%20data%2C%20i.e.%2C%20the%20%28potentially%0Asensitive%29%20user%20data%20that%20is%20queried%20at%20inference%20time%2C%20to%20the%20LLM%20provider%2C%0A%282%29%20three%20out%20of%20four%20methods%20also%20leak%20large%20fractions%20of%20private%20training%0Adata%20to%20the%20LLM%20provider%20while%20the%20method%20that%20protects%20private%20data%20requires%20a%0Alocal%20open%20LLM%2C%20%283%29%20all%20the%20methods%20exhibit%20lower%20performance%20compared%20to%20three%0Aprivate%20gradient-based%20adaptation%20methods%20for%20local%20open%20LLMs%2C%20and%20%284%29%20the%0Aprivate%20adaptation%20methods%20for%20closed%20LLMs%20incur%20higher%20monetary%20training%20and%0Aquery%20costs%20than%20running%20the%20alternative%20methods%20on%20local%20open%20LLMs.%20This%0Ayields%20the%20conclusion%20that%2C%20to%20achieve%20truly%20privacy-preserving%20LLM%20adaptations%0Athat%20yield%20high%20performance%20and%20more%20privacy%20at%20lower%20costs%2C%20taking%20into%0Aaccount%20current%20methods%20and%20models%2C%20one%20should%20use%20open%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05818v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen%2520LLMs%2520are%2520Necessary%2520for%2520Current%2520Private%2520Adaptations%2520and%2520Outperform%250A%2520%2520their%2520Closed%2520Alternatives%26entry.906535625%3DVincent%2520Hanke%2520and%2520Tom%2520Blanchard%2520and%2520Franziska%2520Boenisch%2520and%2520Iyiola%2520Emmanuel%2520Olatunji%2520and%2520Michael%2520Backes%2520and%2520Adam%2520Dziedzic%26entry.1292438233%3D%2520%2520While%2520open%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520made%2520significant%2520progress%252C%2520they%250Astill%2520fall%2520short%2520of%2520matching%2520the%2520performance%2520of%2520their%2520closed%252C%2520proprietary%250Acounterparts%252C%2520making%2520the%2520latter%2520attractive%2520even%2520for%2520the%2520use%2520on%2520highly%2520private%250Adata.%2520Recently%252C%2520various%2520new%2520methods%2520have%2520been%2520proposed%2520to%2520adapt%2520closed%2520LLMs%2520to%250Aprivate%2520data%2520without%2520leaking%2520private%2520information%2520to%2520third%2520parties%2520and/or%2520the%250ALLM%2520provider.%2520In%2520this%2520work%252C%2520we%2520analyze%2520the%2520privacy%2520protection%2520and%2520performance%250Aof%2520the%2520four%2520most%2520recent%2520methods%2520for%2520private%2520adaptation%2520of%2520closed%2520LLMs.%2520By%250Aexamining%2520their%2520threat%2520models%2520and%2520thoroughly%2520comparing%2520their%2520performance%2520under%250Adifferent%2520privacy%2520levels%2520according%2520to%2520differential%2520privacy%2520%2528DP%2529%252C%2520various%2520LLM%250Aarchitectures%252C%2520and%2520multiple%2520datasets%2520for%2520classification%2520and%2520generation%2520tasks%252C%250Awe%2520find%2520that%253A%2520%25281%2529%2520all%2520the%2520methods%2520leak%2520query%2520data%252C%2520i.e.%252C%2520the%2520%2528potentially%250Asensitive%2529%2520user%2520data%2520that%2520is%2520queried%2520at%2520inference%2520time%252C%2520to%2520the%2520LLM%2520provider%252C%250A%25282%2529%2520three%2520out%2520of%2520four%2520methods%2520also%2520leak%2520large%2520fractions%2520of%2520private%2520training%250Adata%2520to%2520the%2520LLM%2520provider%2520while%2520the%2520method%2520that%2520protects%2520private%2520data%2520requires%2520a%250Alocal%2520open%2520LLM%252C%2520%25283%2529%2520all%2520the%2520methods%2520exhibit%2520lower%2520performance%2520compared%2520to%2520three%250Aprivate%2520gradient-based%2520adaptation%2520methods%2520for%2520local%2520open%2520LLMs%252C%2520and%2520%25284%2529%2520the%250Aprivate%2520adaptation%2520methods%2520for%2520closed%2520LLMs%2520incur%2520higher%2520monetary%2520training%2520and%250Aquery%2520costs%2520than%2520running%2520the%2520alternative%2520methods%2520on%2520local%2520open%2520LLMs.%2520This%250Ayields%2520the%2520conclusion%2520that%252C%2520to%2520achieve%2520truly%2520privacy-preserving%2520LLM%2520adaptations%250Athat%2520yield%2520high%2520performance%2520and%2520more%2520privacy%2520at%2520lower%2520costs%252C%2520taking%2520into%250Aaccount%2520current%2520methods%2520and%2520models%252C%2520one%2520should%2520use%2520open%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05818v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open%20LLMs%20are%20Necessary%20for%20Current%20Private%20Adaptations%20and%20Outperform%0A%20%20their%20Closed%20Alternatives&entry.906535625=Vincent%20Hanke%20and%20Tom%20Blanchard%20and%20Franziska%20Boenisch%20and%20Iyiola%20Emmanuel%20Olatunji%20and%20Michael%20Backes%20and%20Adam%20Dziedzic&entry.1292438233=%20%20While%20open%20Large%20Language%20Models%20%28LLMs%29%20have%20made%20significant%20progress%2C%20they%0Astill%20fall%20short%20of%20matching%20the%20performance%20of%20their%20closed%2C%20proprietary%0Acounterparts%2C%20making%20the%20latter%20attractive%20even%20for%20the%20use%20on%20highly%20private%0Adata.%20Recently%2C%20various%20new%20methods%20have%20been%20proposed%20to%20adapt%20closed%20LLMs%20to%0Aprivate%20data%20without%20leaking%20private%20information%20to%20third%20parties%20and/or%20the%0ALLM%20provider.%20In%20this%20work%2C%20we%20analyze%20the%20privacy%20protection%20and%20performance%0Aof%20the%20four%20most%20recent%20methods%20for%20private%20adaptation%20of%20closed%20LLMs.%20By%0Aexamining%20their%20threat%20models%20and%20thoroughly%20comparing%20their%20performance%20under%0Adifferent%20privacy%20levels%20according%20to%20differential%20privacy%20%28DP%29%2C%20various%20LLM%0Aarchitectures%2C%20and%20multiple%20datasets%20for%20classification%20and%20generation%20tasks%2C%0Awe%20find%20that%3A%20%281%29%20all%20the%20methods%20leak%20query%20data%2C%20i.e.%2C%20the%20%28potentially%0Asensitive%29%20user%20data%20that%20is%20queried%20at%20inference%20time%2C%20to%20the%20LLM%20provider%2C%0A%282%29%20three%20out%20of%20four%20methods%20also%20leak%20large%20fractions%20of%20private%20training%0Adata%20to%20the%20LLM%20provider%20while%20the%20method%20that%20protects%20private%20data%20requires%20a%0Alocal%20open%20LLM%2C%20%283%29%20all%20the%20methods%20exhibit%20lower%20performance%20compared%20to%20three%0Aprivate%20gradient-based%20adaptation%20methods%20for%20local%20open%20LLMs%2C%20and%20%284%29%20the%0Aprivate%20adaptation%20methods%20for%20closed%20LLMs%20incur%20higher%20monetary%20training%20and%0Aquery%20costs%20than%20running%20the%20alternative%20methods%20on%20local%20open%20LLMs.%20This%0Ayields%20the%20conclusion%20that%2C%20to%20achieve%20truly%20privacy-preserving%20LLM%20adaptations%0Athat%20yield%20high%20performance%20and%20more%20privacy%20at%20lower%20costs%2C%20taking%20into%0Aaccount%20current%20methods%20and%20models%2C%20one%20should%20use%20open%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05818v2&entry.124074799=Read"},
{"title": "Coniferest: a complete active anomaly detection framework", "author": "M. V. Kornilov and V. S. Korolev and K. L. Malanchev and A. D. Lavrukhina and E. Russeil and T. A. Semenikhin and E. Gangler and E. E. O. Ishida and M. V. Pruzhinskaya and A. A. Volnova and S. Sreejith", "abstract": "  We present coniferest, an open source generic purpose active anomaly\ndetection framework written in Python. The package design and implemented\nalgorithms are described. Currently, static outlier detection analysis is\nsupported via the Isolation forest algorithm. Moreover, Active Anomaly\nDiscovery (AAD) and Pineforest algorithms are available to tackle active\nanomaly detection problems. The algorithms and package performance are\nevaluated on a series of synthetic datasets. We also describe a few success\ncases which resulted from applying the package to real astronomical data in\nactive anomaly detection tasks within the SNAD project.\n", "link": "http://arxiv.org/abs/2410.17142v2", "date": "2024-11-15", "relevancy": 1.7055, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4333}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4249}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Coniferest%3A%20a%20complete%20active%20anomaly%20detection%20framework&body=Title%3A%20Coniferest%3A%20a%20complete%20active%20anomaly%20detection%20framework%0AAuthor%3A%20M.%20V.%20Kornilov%20and%20V.%20S.%20Korolev%20and%20K.%20L.%20Malanchev%20and%20A.%20D.%20Lavrukhina%20and%20E.%20Russeil%20and%20T.%20A.%20Semenikhin%20and%20E.%20Gangler%20and%20E.%20E.%20O.%20Ishida%20and%20M.%20V.%20Pruzhinskaya%20and%20A.%20A.%20Volnova%20and%20S.%20Sreejith%0AAbstract%3A%20%20%20We%20present%20coniferest%2C%20an%20open%20source%20generic%20purpose%20active%20anomaly%0Adetection%20framework%20written%20in%20Python.%20The%20package%20design%20and%20implemented%0Aalgorithms%20are%20described.%20Currently%2C%20static%20outlier%20detection%20analysis%20is%0Asupported%20via%20the%20Isolation%20forest%20algorithm.%20Moreover%2C%20Active%20Anomaly%0ADiscovery%20%28AAD%29%20and%20Pineforest%20algorithms%20are%20available%20to%20tackle%20active%0Aanomaly%20detection%20problems.%20The%20algorithms%20and%20package%20performance%20are%0Aevaluated%20on%20a%20series%20of%20synthetic%20datasets.%20We%20also%20describe%20a%20few%20success%0Acases%20which%20resulted%20from%20applying%20the%20package%20to%20real%20astronomical%20data%20in%0Aactive%20anomaly%20detection%20tasks%20within%20the%20SNAD%20project.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17142v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConiferest%253A%2520a%2520complete%2520active%2520anomaly%2520detection%2520framework%26entry.906535625%3DM.%2520V.%2520Kornilov%2520and%2520V.%2520S.%2520Korolev%2520and%2520K.%2520L.%2520Malanchev%2520and%2520A.%2520D.%2520Lavrukhina%2520and%2520E.%2520Russeil%2520and%2520T.%2520A.%2520Semenikhin%2520and%2520E.%2520Gangler%2520and%2520E.%2520E.%2520O.%2520Ishida%2520and%2520M.%2520V.%2520Pruzhinskaya%2520and%2520A.%2520A.%2520Volnova%2520and%2520S.%2520Sreejith%26entry.1292438233%3D%2520%2520We%2520present%2520coniferest%252C%2520an%2520open%2520source%2520generic%2520purpose%2520active%2520anomaly%250Adetection%2520framework%2520written%2520in%2520Python.%2520The%2520package%2520design%2520and%2520implemented%250Aalgorithms%2520are%2520described.%2520Currently%252C%2520static%2520outlier%2520detection%2520analysis%2520is%250Asupported%2520via%2520the%2520Isolation%2520forest%2520algorithm.%2520Moreover%252C%2520Active%2520Anomaly%250ADiscovery%2520%2528AAD%2529%2520and%2520Pineforest%2520algorithms%2520are%2520available%2520to%2520tackle%2520active%250Aanomaly%2520detection%2520problems.%2520The%2520algorithms%2520and%2520package%2520performance%2520are%250Aevaluated%2520on%2520a%2520series%2520of%2520synthetic%2520datasets.%2520We%2520also%2520describe%2520a%2520few%2520success%250Acases%2520which%2520resulted%2520from%2520applying%2520the%2520package%2520to%2520real%2520astronomical%2520data%2520in%250Aactive%2520anomaly%2520detection%2520tasks%2520within%2520the%2520SNAD%2520project.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17142v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coniferest%3A%20a%20complete%20active%20anomaly%20detection%20framework&entry.906535625=M.%20V.%20Kornilov%20and%20V.%20S.%20Korolev%20and%20K.%20L.%20Malanchev%20and%20A.%20D.%20Lavrukhina%20and%20E.%20Russeil%20and%20T.%20A.%20Semenikhin%20and%20E.%20Gangler%20and%20E.%20E.%20O.%20Ishida%20and%20M.%20V.%20Pruzhinskaya%20and%20A.%20A.%20Volnova%20and%20S.%20Sreejith&entry.1292438233=%20%20We%20present%20coniferest%2C%20an%20open%20source%20generic%20purpose%20active%20anomaly%0Adetection%20framework%20written%20in%20Python.%20The%20package%20design%20and%20implemented%0Aalgorithms%20are%20described.%20Currently%2C%20static%20outlier%20detection%20analysis%20is%0Asupported%20via%20the%20Isolation%20forest%20algorithm.%20Moreover%2C%20Active%20Anomaly%0ADiscovery%20%28AAD%29%20and%20Pineforest%20algorithms%20are%20available%20to%20tackle%20active%0Aanomaly%20detection%20problems.%20The%20algorithms%20and%20package%20performance%20are%0Aevaluated%20on%20a%20series%20of%20synthetic%20datasets.%20We%20also%20describe%20a%20few%20success%0Acases%20which%20resulted%20from%20applying%20the%20package%20to%20real%20astronomical%20data%20in%0Aactive%20anomaly%20detection%20tasks%20within%20the%20SNAD%20project.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17142v2&entry.124074799=Read"},
{"title": "Learning Diffusion Priors from Observations by Expectation Maximization", "author": "Fran\u00e7ois Rozet and G\u00e9r\u00f4me Andry and Fran\u00e7ois Lanusse and Gilles Louppe", "abstract": "  Diffusion models recently proved to be remarkable priors for Bayesian inverse\nproblems. However, training these models typically requires access to large\namounts of clean data, which could prove difficult in some settings. In this\nwork, we present a novel method based on the expectation-maximization algorithm\nfor training diffusion models from incomplete and noisy observations only.\nUnlike previous works, our method leads to proper diffusion models, which is\ncrucial for downstream tasks. As part of our method, we propose and motivate an\nimproved posterior sampling scheme for unconditional diffusion models. We\npresent empirical evidence supporting the effectiveness of our method.\n", "link": "http://arxiv.org/abs/2405.13712v4", "date": "2024-11-15", "relevancy": 1.6563, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5897}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5426}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Diffusion%20Priors%20from%20Observations%20by%20Expectation%20Maximization&body=Title%3A%20Learning%20Diffusion%20Priors%20from%20Observations%20by%20Expectation%20Maximization%0AAuthor%3A%20Fran%C3%A7ois%20Rozet%20and%20G%C3%A9r%C3%B4me%20Andry%20and%20Fran%C3%A7ois%20Lanusse%20and%20Gilles%20Louppe%0AAbstract%3A%20%20%20Diffusion%20models%20recently%20proved%20to%20be%20remarkable%20priors%20for%20Bayesian%20inverse%0Aproblems.%20However%2C%20training%20these%20models%20typically%20requires%20access%20to%20large%0Aamounts%20of%20clean%20data%2C%20which%20could%20prove%20difficult%20in%20some%20settings.%20In%20this%0Awork%2C%20we%20present%20a%20novel%20method%20based%20on%20the%20expectation-maximization%20algorithm%0Afor%20training%20diffusion%20models%20from%20incomplete%20and%20noisy%20observations%20only.%0AUnlike%20previous%20works%2C%20our%20method%20leads%20to%20proper%20diffusion%20models%2C%20which%20is%0Acrucial%20for%20downstream%20tasks.%20As%20part%20of%20our%20method%2C%20we%20propose%20and%20motivate%20an%0Aimproved%20posterior%20sampling%20scheme%20for%20unconditional%20diffusion%20models.%20We%0Apresent%20empirical%20evidence%20supporting%20the%20effectiveness%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13712v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Diffusion%2520Priors%2520from%2520Observations%2520by%2520Expectation%2520Maximization%26entry.906535625%3DFran%25C3%25A7ois%2520Rozet%2520and%2520G%25C3%25A9r%25C3%25B4me%2520Andry%2520and%2520Fran%25C3%25A7ois%2520Lanusse%2520and%2520Gilles%2520Louppe%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520recently%2520proved%2520to%2520be%2520remarkable%2520priors%2520for%2520Bayesian%2520inverse%250Aproblems.%2520However%252C%2520training%2520these%2520models%2520typically%2520requires%2520access%2520to%2520large%250Aamounts%2520of%2520clean%2520data%252C%2520which%2520could%2520prove%2520difficult%2520in%2520some%2520settings.%2520In%2520this%250Awork%252C%2520we%2520present%2520a%2520novel%2520method%2520based%2520on%2520the%2520expectation-maximization%2520algorithm%250Afor%2520training%2520diffusion%2520models%2520from%2520incomplete%2520and%2520noisy%2520observations%2520only.%250AUnlike%2520previous%2520works%252C%2520our%2520method%2520leads%2520to%2520proper%2520diffusion%2520models%252C%2520which%2520is%250Acrucial%2520for%2520downstream%2520tasks.%2520As%2520part%2520of%2520our%2520method%252C%2520we%2520propose%2520and%2520motivate%2520an%250Aimproved%2520posterior%2520sampling%2520scheme%2520for%2520unconditional%2520diffusion%2520models.%2520We%250Apresent%2520empirical%2520evidence%2520supporting%2520the%2520effectiveness%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13712v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Diffusion%20Priors%20from%20Observations%20by%20Expectation%20Maximization&entry.906535625=Fran%C3%A7ois%20Rozet%20and%20G%C3%A9r%C3%B4me%20Andry%20and%20Fran%C3%A7ois%20Lanusse%20and%20Gilles%20Louppe&entry.1292438233=%20%20Diffusion%20models%20recently%20proved%20to%20be%20remarkable%20priors%20for%20Bayesian%20inverse%0Aproblems.%20However%2C%20training%20these%20models%20typically%20requires%20access%20to%20large%0Aamounts%20of%20clean%20data%2C%20which%20could%20prove%20difficult%20in%20some%20settings.%20In%20this%0Awork%2C%20we%20present%20a%20novel%20method%20based%20on%20the%20expectation-maximization%20algorithm%0Afor%20training%20diffusion%20models%20from%20incomplete%20and%20noisy%20observations%20only.%0AUnlike%20previous%20works%2C%20our%20method%20leads%20to%20proper%20diffusion%20models%2C%20which%20is%0Acrucial%20for%20downstream%20tasks.%20As%20part%20of%20our%20method%2C%20we%20propose%20and%20motivate%20an%0Aimproved%20posterior%20sampling%20scheme%20for%20unconditional%20diffusion%20models.%20We%0Apresent%20empirical%20evidence%20supporting%20the%20effectiveness%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13712v4&entry.124074799=Read"},
{"title": "Disclosure of AI-Generated News Increases Engagement but Does Not Reduce\n  Aversion, Despite Positive Quality Ratings", "author": "Fabrizio Gilardi and Sabrina Di Lorenzo and Juri Ezzaini and Beryl Santa and Benjamin Streiff and Eric Zurfluh and Emma Hoes", "abstract": "  The advancement of artificial intelligence (AI) has led to its application in\nmany areas, including news media. The integration of AI in journalism presents\nboth opportunities and risks for democracy, making it crucial to understand\npublic reception of and engagement with AI-generated news, as it may directly\ninfluence political knowledge and trust. This preregistered study investigates\n(i) the perceived quality of AI-assisted and AI-generated versus\nhuman-generated news articles, (ii) whether disclosure of AI's involvement in\ngenerating these news articles influences engagement with them, and (iii)\nwhether such awareness affects the willingness to read AI-generated articles in\nthe future. We employed a between-subjects survey experiment with 599\nparticipants from the German-speaking part of Switzerland, who evaluated the\ncredibility, readability, and expertise of news articles. These articles were\neither written by journalists (control group), rewritten by AI (AI-assisted\ngroup), or entirely generated by AI (AI-generated group). Our results indicate\nthat all news articles, regardless of whether they were written by journalists\nor AI, were perceived to be of equal quality. When participants in the\ntreatment groups were subsequently made aware of AI's involvement in generating\nthe articles, they expressed a higher willingness to engage with (i.e.,\ncontinue reading) the articles than participants in the control group. However,\nthey were not more willing to read AI-generated news in the future. These\nresults suggest that aversion to AI usage in news media is not primarily rooted\nin a perceived lack of quality, and that by disclosing using AI, journalists\ncould attract more immediate engagement with their content, at least in the\nshort term.\n", "link": "http://arxiv.org/abs/2409.03500v2", "date": "2024-11-15", "relevancy": 1.6297, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4232}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.407}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.3918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disclosure%20of%20AI-Generated%20News%20Increases%20Engagement%20but%20Does%20Not%20Reduce%0A%20%20Aversion%2C%20Despite%20Positive%20Quality%20Ratings&body=Title%3A%20Disclosure%20of%20AI-Generated%20News%20Increases%20Engagement%20but%20Does%20Not%20Reduce%0A%20%20Aversion%2C%20Despite%20Positive%20Quality%20Ratings%0AAuthor%3A%20Fabrizio%20Gilardi%20and%20Sabrina%20Di%20Lorenzo%20and%20Juri%20Ezzaini%20and%20Beryl%20Santa%20and%20Benjamin%20Streiff%20and%20Eric%20Zurfluh%20and%20Emma%20Hoes%0AAbstract%3A%20%20%20The%20advancement%20of%20artificial%20intelligence%20%28AI%29%20has%20led%20to%20its%20application%20in%0Amany%20areas%2C%20including%20news%20media.%20The%20integration%20of%20AI%20in%20journalism%20presents%0Aboth%20opportunities%20and%20risks%20for%20democracy%2C%20making%20it%20crucial%20to%20understand%0Apublic%20reception%20of%20and%20engagement%20with%20AI-generated%20news%2C%20as%20it%20may%20directly%0Ainfluence%20political%20knowledge%20and%20trust.%20This%20preregistered%20study%20investigates%0A%28i%29%20the%20perceived%20quality%20of%20AI-assisted%20and%20AI-generated%20versus%0Ahuman-generated%20news%20articles%2C%20%28ii%29%20whether%20disclosure%20of%20AI%27s%20involvement%20in%0Agenerating%20these%20news%20articles%20influences%20engagement%20with%20them%2C%20and%20%28iii%29%0Awhether%20such%20awareness%20affects%20the%20willingness%20to%20read%20AI-generated%20articles%20in%0Athe%20future.%20We%20employed%20a%20between-subjects%20survey%20experiment%20with%20599%0Aparticipants%20from%20the%20German-speaking%20part%20of%20Switzerland%2C%20who%20evaluated%20the%0Acredibility%2C%20readability%2C%20and%20expertise%20of%20news%20articles.%20These%20articles%20were%0Aeither%20written%20by%20journalists%20%28control%20group%29%2C%20rewritten%20by%20AI%20%28AI-assisted%0Agroup%29%2C%20or%20entirely%20generated%20by%20AI%20%28AI-generated%20group%29.%20Our%20results%20indicate%0Athat%20all%20news%20articles%2C%20regardless%20of%20whether%20they%20were%20written%20by%20journalists%0Aor%20AI%2C%20were%20perceived%20to%20be%20of%20equal%20quality.%20When%20participants%20in%20the%0Atreatment%20groups%20were%20subsequently%20made%20aware%20of%20AI%27s%20involvement%20in%20generating%0Athe%20articles%2C%20they%20expressed%20a%20higher%20willingness%20to%20engage%20with%20%28i.e.%2C%0Acontinue%20reading%29%20the%20articles%20than%20participants%20in%20the%20control%20group.%20However%2C%0Athey%20were%20not%20more%20willing%20to%20read%20AI-generated%20news%20in%20the%20future.%20These%0Aresults%20suggest%20that%20aversion%20to%20AI%20usage%20in%20news%20media%20is%20not%20primarily%20rooted%0Ain%20a%20perceived%20lack%20of%20quality%2C%20and%20that%20by%20disclosing%20using%20AI%2C%20journalists%0Acould%20attract%20more%20immediate%20engagement%20with%20their%20content%2C%20at%20least%20in%20the%0Ashort%20term.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03500v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisclosure%2520of%2520AI-Generated%2520News%2520Increases%2520Engagement%2520but%2520Does%2520Not%2520Reduce%250A%2520%2520Aversion%252C%2520Despite%2520Positive%2520Quality%2520Ratings%26entry.906535625%3DFabrizio%2520Gilardi%2520and%2520Sabrina%2520Di%2520Lorenzo%2520and%2520Juri%2520Ezzaini%2520and%2520Beryl%2520Santa%2520and%2520Benjamin%2520Streiff%2520and%2520Eric%2520Zurfluh%2520and%2520Emma%2520Hoes%26entry.1292438233%3D%2520%2520The%2520advancement%2520of%2520artificial%2520intelligence%2520%2528AI%2529%2520has%2520led%2520to%2520its%2520application%2520in%250Amany%2520areas%252C%2520including%2520news%2520media.%2520The%2520integration%2520of%2520AI%2520in%2520journalism%2520presents%250Aboth%2520opportunities%2520and%2520risks%2520for%2520democracy%252C%2520making%2520it%2520crucial%2520to%2520understand%250Apublic%2520reception%2520of%2520and%2520engagement%2520with%2520AI-generated%2520news%252C%2520as%2520it%2520may%2520directly%250Ainfluence%2520political%2520knowledge%2520and%2520trust.%2520This%2520preregistered%2520study%2520investigates%250A%2528i%2529%2520the%2520perceived%2520quality%2520of%2520AI-assisted%2520and%2520AI-generated%2520versus%250Ahuman-generated%2520news%2520articles%252C%2520%2528ii%2529%2520whether%2520disclosure%2520of%2520AI%2527s%2520involvement%2520in%250Agenerating%2520these%2520news%2520articles%2520influences%2520engagement%2520with%2520them%252C%2520and%2520%2528iii%2529%250Awhether%2520such%2520awareness%2520affects%2520the%2520willingness%2520to%2520read%2520AI-generated%2520articles%2520in%250Athe%2520future.%2520We%2520employed%2520a%2520between-subjects%2520survey%2520experiment%2520with%2520599%250Aparticipants%2520from%2520the%2520German-speaking%2520part%2520of%2520Switzerland%252C%2520who%2520evaluated%2520the%250Acredibility%252C%2520readability%252C%2520and%2520expertise%2520of%2520news%2520articles.%2520These%2520articles%2520were%250Aeither%2520written%2520by%2520journalists%2520%2528control%2520group%2529%252C%2520rewritten%2520by%2520AI%2520%2528AI-assisted%250Agroup%2529%252C%2520or%2520entirely%2520generated%2520by%2520AI%2520%2528AI-generated%2520group%2529.%2520Our%2520results%2520indicate%250Athat%2520all%2520news%2520articles%252C%2520regardless%2520of%2520whether%2520they%2520were%2520written%2520by%2520journalists%250Aor%2520AI%252C%2520were%2520perceived%2520to%2520be%2520of%2520equal%2520quality.%2520When%2520participants%2520in%2520the%250Atreatment%2520groups%2520were%2520subsequently%2520made%2520aware%2520of%2520AI%2527s%2520involvement%2520in%2520generating%250Athe%2520articles%252C%2520they%2520expressed%2520a%2520higher%2520willingness%2520to%2520engage%2520with%2520%2528i.e.%252C%250Acontinue%2520reading%2529%2520the%2520articles%2520than%2520participants%2520in%2520the%2520control%2520group.%2520However%252C%250Athey%2520were%2520not%2520more%2520willing%2520to%2520read%2520AI-generated%2520news%2520in%2520the%2520future.%2520These%250Aresults%2520suggest%2520that%2520aversion%2520to%2520AI%2520usage%2520in%2520news%2520media%2520is%2520not%2520primarily%2520rooted%250Ain%2520a%2520perceived%2520lack%2520of%2520quality%252C%2520and%2520that%2520by%2520disclosing%2520using%2520AI%252C%2520journalists%250Acould%2520attract%2520more%2520immediate%2520engagement%2520with%2520their%2520content%252C%2520at%2520least%2520in%2520the%250Ashort%2520term.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03500v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disclosure%20of%20AI-Generated%20News%20Increases%20Engagement%20but%20Does%20Not%20Reduce%0A%20%20Aversion%2C%20Despite%20Positive%20Quality%20Ratings&entry.906535625=Fabrizio%20Gilardi%20and%20Sabrina%20Di%20Lorenzo%20and%20Juri%20Ezzaini%20and%20Beryl%20Santa%20and%20Benjamin%20Streiff%20and%20Eric%20Zurfluh%20and%20Emma%20Hoes&entry.1292438233=%20%20The%20advancement%20of%20artificial%20intelligence%20%28AI%29%20has%20led%20to%20its%20application%20in%0Amany%20areas%2C%20including%20news%20media.%20The%20integration%20of%20AI%20in%20journalism%20presents%0Aboth%20opportunities%20and%20risks%20for%20democracy%2C%20making%20it%20crucial%20to%20understand%0Apublic%20reception%20of%20and%20engagement%20with%20AI-generated%20news%2C%20as%20it%20may%20directly%0Ainfluence%20political%20knowledge%20and%20trust.%20This%20preregistered%20study%20investigates%0A%28i%29%20the%20perceived%20quality%20of%20AI-assisted%20and%20AI-generated%20versus%0Ahuman-generated%20news%20articles%2C%20%28ii%29%20whether%20disclosure%20of%20AI%27s%20involvement%20in%0Agenerating%20these%20news%20articles%20influences%20engagement%20with%20them%2C%20and%20%28iii%29%0Awhether%20such%20awareness%20affects%20the%20willingness%20to%20read%20AI-generated%20articles%20in%0Athe%20future.%20We%20employed%20a%20between-subjects%20survey%20experiment%20with%20599%0Aparticipants%20from%20the%20German-speaking%20part%20of%20Switzerland%2C%20who%20evaluated%20the%0Acredibility%2C%20readability%2C%20and%20expertise%20of%20news%20articles.%20These%20articles%20were%0Aeither%20written%20by%20journalists%20%28control%20group%29%2C%20rewritten%20by%20AI%20%28AI-assisted%0Agroup%29%2C%20or%20entirely%20generated%20by%20AI%20%28AI-generated%20group%29.%20Our%20results%20indicate%0Athat%20all%20news%20articles%2C%20regardless%20of%20whether%20they%20were%20written%20by%20journalists%0Aor%20AI%2C%20were%20perceived%20to%20be%20of%20equal%20quality.%20When%20participants%20in%20the%0Atreatment%20groups%20were%20subsequently%20made%20aware%20of%20AI%27s%20involvement%20in%20generating%0Athe%20articles%2C%20they%20expressed%20a%20higher%20willingness%20to%20engage%20with%20%28i.e.%2C%0Acontinue%20reading%29%20the%20articles%20than%20participants%20in%20the%20control%20group.%20However%2C%0Athey%20were%20not%20more%20willing%20to%20read%20AI-generated%20news%20in%20the%20future.%20These%0Aresults%20suggest%20that%20aversion%20to%20AI%20usage%20in%20news%20media%20is%20not%20primarily%20rooted%0Ain%20a%20perceived%20lack%20of%20quality%2C%20and%20that%20by%20disclosing%20using%20AI%2C%20journalists%0Acould%20attract%20more%20immediate%20engagement%20with%20their%20content%2C%20at%20least%20in%20the%0Ashort%20term.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03500v2&entry.124074799=Read"},
{"title": "The ParClusterers Benchmark Suite (PCBS): A Fine-Grained Analysis of\n  Scalable Graph Clustering", "author": "Shangdi Yu and Jessica Shi and Jamison Meindl and David Eisenstat and Xiaoen Ju and Sasan Tavakkol and Laxman Dhulipala and Jakub \u0141\u0105cki and Vahab Mirrokni and Julian Shun", "abstract": "  We introduce the ParClusterers Benchmark Suite (PCBS) -- a collection of\nhighly scalable parallel graph clustering algorithms and benchmarking tools\nthat streamline comparing different graph clustering algorithms and\nimplementations.\n  The benchmark includes clustering algorithms that target a wide range of\nmodern clustering use cases, including community detection, classification, and\ndense subgraph mining.\n  The benchmark toolkit makes it easy to run and evaluate multiple instances of\ndifferent clustering algorithms, which can be useful for fine-tuning the\nperformance of clustering on a given task, and for comparing different\nclustering algorithms based on different metrics of interest, including\nclustering quality and running time.\n  Using PCBS, we evaluate a broad collection of real-world graph clustering\ndatasets. Somewhat surprisingly, we find that the best quality results are\nobtained by algorithms that not included in many popular graph clustering\ntoolkits. The PCBS provides a standardized way to evaluate and judge the\nquality-performance tradeoffs of the active research area of scalable graph\nclustering algorithms. We believe it will help enable fair, accurate, and\nnuanced evaluation of graph clustering algorithms in the future.\n", "link": "http://arxiv.org/abs/2411.10290v1", "date": "2024-11-15", "relevancy": 1.6082, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4027}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4022}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20ParClusterers%20Benchmark%20Suite%20%28PCBS%29%3A%20A%20Fine-Grained%20Analysis%20of%0A%20%20Scalable%20Graph%20Clustering&body=Title%3A%20The%20ParClusterers%20Benchmark%20Suite%20%28PCBS%29%3A%20A%20Fine-Grained%20Analysis%20of%0A%20%20Scalable%20Graph%20Clustering%0AAuthor%3A%20Shangdi%20Yu%20and%20Jessica%20Shi%20and%20Jamison%20Meindl%20and%20David%20Eisenstat%20and%20Xiaoen%20Ju%20and%20Sasan%20Tavakkol%20and%20Laxman%20Dhulipala%20and%20Jakub%20%C5%81%C4%85cki%20and%20Vahab%20Mirrokni%20and%20Julian%20Shun%0AAbstract%3A%20%20%20We%20introduce%20the%20ParClusterers%20Benchmark%20Suite%20%28PCBS%29%20--%20a%20collection%20of%0Ahighly%20scalable%20parallel%20graph%20clustering%20algorithms%20and%20benchmarking%20tools%0Athat%20streamline%20comparing%20different%20graph%20clustering%20algorithms%20and%0Aimplementations.%0A%20%20The%20benchmark%20includes%20clustering%20algorithms%20that%20target%20a%20wide%20range%20of%0Amodern%20clustering%20use%20cases%2C%20including%20community%20detection%2C%20classification%2C%20and%0Adense%20subgraph%20mining.%0A%20%20The%20benchmark%20toolkit%20makes%20it%20easy%20to%20run%20and%20evaluate%20multiple%20instances%20of%0Adifferent%20clustering%20algorithms%2C%20which%20can%20be%20useful%20for%20fine-tuning%20the%0Aperformance%20of%20clustering%20on%20a%20given%20task%2C%20and%20for%20comparing%20different%0Aclustering%20algorithms%20based%20on%20different%20metrics%20of%20interest%2C%20including%0Aclustering%20quality%20and%20running%20time.%0A%20%20Using%20PCBS%2C%20we%20evaluate%20a%20broad%20collection%20of%20real-world%20graph%20clustering%0Adatasets.%20Somewhat%20surprisingly%2C%20we%20find%20that%20the%20best%20quality%20results%20are%0Aobtained%20by%20algorithms%20that%20not%20included%20in%20many%20popular%20graph%20clustering%0Atoolkits.%20The%20PCBS%20provides%20a%20standardized%20way%20to%20evaluate%20and%20judge%20the%0Aquality-performance%20tradeoffs%20of%20the%20active%20research%20area%20of%20scalable%20graph%0Aclustering%20algorithms.%20We%20believe%20it%20will%20help%20enable%20fair%2C%20accurate%2C%20and%0Anuanced%20evaluation%20of%20graph%20clustering%20algorithms%20in%20the%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10290v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520ParClusterers%2520Benchmark%2520Suite%2520%2528PCBS%2529%253A%2520A%2520Fine-Grained%2520Analysis%2520of%250A%2520%2520Scalable%2520Graph%2520Clustering%26entry.906535625%3DShangdi%2520Yu%2520and%2520Jessica%2520Shi%2520and%2520Jamison%2520Meindl%2520and%2520David%2520Eisenstat%2520and%2520Xiaoen%2520Ju%2520and%2520Sasan%2520Tavakkol%2520and%2520Laxman%2520Dhulipala%2520and%2520Jakub%2520%25C5%2581%25C4%2585cki%2520and%2520Vahab%2520Mirrokni%2520and%2520Julian%2520Shun%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520ParClusterers%2520Benchmark%2520Suite%2520%2528PCBS%2529%2520--%2520a%2520collection%2520of%250Ahighly%2520scalable%2520parallel%2520graph%2520clustering%2520algorithms%2520and%2520benchmarking%2520tools%250Athat%2520streamline%2520comparing%2520different%2520graph%2520clustering%2520algorithms%2520and%250Aimplementations.%250A%2520%2520The%2520benchmark%2520includes%2520clustering%2520algorithms%2520that%2520target%2520a%2520wide%2520range%2520of%250Amodern%2520clustering%2520use%2520cases%252C%2520including%2520community%2520detection%252C%2520classification%252C%2520and%250Adense%2520subgraph%2520mining.%250A%2520%2520The%2520benchmark%2520toolkit%2520makes%2520it%2520easy%2520to%2520run%2520and%2520evaluate%2520multiple%2520instances%2520of%250Adifferent%2520clustering%2520algorithms%252C%2520which%2520can%2520be%2520useful%2520for%2520fine-tuning%2520the%250Aperformance%2520of%2520clustering%2520on%2520a%2520given%2520task%252C%2520and%2520for%2520comparing%2520different%250Aclustering%2520algorithms%2520based%2520on%2520different%2520metrics%2520of%2520interest%252C%2520including%250Aclustering%2520quality%2520and%2520running%2520time.%250A%2520%2520Using%2520PCBS%252C%2520we%2520evaluate%2520a%2520broad%2520collection%2520of%2520real-world%2520graph%2520clustering%250Adatasets.%2520Somewhat%2520surprisingly%252C%2520we%2520find%2520that%2520the%2520best%2520quality%2520results%2520are%250Aobtained%2520by%2520algorithms%2520that%2520not%2520included%2520in%2520many%2520popular%2520graph%2520clustering%250Atoolkits.%2520The%2520PCBS%2520provides%2520a%2520standardized%2520way%2520to%2520evaluate%2520and%2520judge%2520the%250Aquality-performance%2520tradeoffs%2520of%2520the%2520active%2520research%2520area%2520of%2520scalable%2520graph%250Aclustering%2520algorithms.%2520We%2520believe%2520it%2520will%2520help%2520enable%2520fair%252C%2520accurate%252C%2520and%250Anuanced%2520evaluation%2520of%2520graph%2520clustering%2520algorithms%2520in%2520the%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10290v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20ParClusterers%20Benchmark%20Suite%20%28PCBS%29%3A%20A%20Fine-Grained%20Analysis%20of%0A%20%20Scalable%20Graph%20Clustering&entry.906535625=Shangdi%20Yu%20and%20Jessica%20Shi%20and%20Jamison%20Meindl%20and%20David%20Eisenstat%20and%20Xiaoen%20Ju%20and%20Sasan%20Tavakkol%20and%20Laxman%20Dhulipala%20and%20Jakub%20%C5%81%C4%85cki%20and%20Vahab%20Mirrokni%20and%20Julian%20Shun&entry.1292438233=%20%20We%20introduce%20the%20ParClusterers%20Benchmark%20Suite%20%28PCBS%29%20--%20a%20collection%20of%0Ahighly%20scalable%20parallel%20graph%20clustering%20algorithms%20and%20benchmarking%20tools%0Athat%20streamline%20comparing%20different%20graph%20clustering%20algorithms%20and%0Aimplementations.%0A%20%20The%20benchmark%20includes%20clustering%20algorithms%20that%20target%20a%20wide%20range%20of%0Amodern%20clustering%20use%20cases%2C%20including%20community%20detection%2C%20classification%2C%20and%0Adense%20subgraph%20mining.%0A%20%20The%20benchmark%20toolkit%20makes%20it%20easy%20to%20run%20and%20evaluate%20multiple%20instances%20of%0Adifferent%20clustering%20algorithms%2C%20which%20can%20be%20useful%20for%20fine-tuning%20the%0Aperformance%20of%20clustering%20on%20a%20given%20task%2C%20and%20for%20comparing%20different%0Aclustering%20algorithms%20based%20on%20different%20metrics%20of%20interest%2C%20including%0Aclustering%20quality%20and%20running%20time.%0A%20%20Using%20PCBS%2C%20we%20evaluate%20a%20broad%20collection%20of%20real-world%20graph%20clustering%0Adatasets.%20Somewhat%20surprisingly%2C%20we%20find%20that%20the%20best%20quality%20results%20are%0Aobtained%20by%20algorithms%20that%20not%20included%20in%20many%20popular%20graph%20clustering%0Atoolkits.%20The%20PCBS%20provides%20a%20standardized%20way%20to%20evaluate%20and%20judge%20the%0Aquality-performance%20tradeoffs%20of%20the%20active%20research%20area%20of%20scalable%20graph%0Aclustering%20algorithms.%20We%20believe%20it%20will%20help%20enable%20fair%2C%20accurate%2C%20and%0Anuanced%20evaluation%20of%20graph%20clustering%20algorithms%20in%20the%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10290v1&entry.124074799=Read"},
{"title": "BiDense: Binarization for Dense Prediction", "author": "Rui Yin and Haotong Qin and Yulun Zhang and Wenbo Li and Yong Guo and Jianjun Zhu and Cheng Wang and Biao Jia", "abstract": "  Dense prediction is a critical task in computer vision. However, previous\nmethods often require extensive computational resources, which hinders their\nreal-world application. In this paper, we propose BiDense, a generalized binary\nneural network (BNN) designed for efficient and accurate dense prediction\ntasks. BiDense incorporates two key techniques: the Distribution-adaptive\nBinarizer (DAB) and the Channel-adaptive Full-precision Bypass (CFB). The DAB\nadaptively calculates thresholds and scaling factors for binarization,\neffectively retaining more information within BNNs. Meanwhile, the CFB\nfacilitates full-precision bypassing for binary convolutional layers undergoing\nvarious channel size transformations, which enhances the propagation of\nreal-valued signals and minimizes information loss. By leveraging these\ntechniques, BiDense preserves more real-valued information, enabling more\naccurate and detailed dense predictions in BNNs. Extensive experiments\ndemonstrate that our framework achieves performance levels comparable to\nfull-precision models while significantly reducing memory usage and\ncomputational costs.\n", "link": "http://arxiv.org/abs/2411.10346v1", "date": "2024-11-15", "relevancy": 1.5886, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5418}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5157}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5128}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BiDense%3A%20Binarization%20for%20Dense%20Prediction&body=Title%3A%20BiDense%3A%20Binarization%20for%20Dense%20Prediction%0AAuthor%3A%20Rui%20Yin%20and%20Haotong%20Qin%20and%20Yulun%20Zhang%20and%20Wenbo%20Li%20and%20Yong%20Guo%20and%20Jianjun%20Zhu%20and%20Cheng%20Wang%20and%20Biao%20Jia%0AAbstract%3A%20%20%20Dense%20prediction%20is%20a%20critical%20task%20in%20computer%20vision.%20However%2C%20previous%0Amethods%20often%20require%20extensive%20computational%20resources%2C%20which%20hinders%20their%0Areal-world%20application.%20In%20this%20paper%2C%20we%20propose%20BiDense%2C%20a%20generalized%20binary%0Aneural%20network%20%28BNN%29%20designed%20for%20efficient%20and%20accurate%20dense%20prediction%0Atasks.%20BiDense%20incorporates%20two%20key%20techniques%3A%20the%20Distribution-adaptive%0ABinarizer%20%28DAB%29%20and%20the%20Channel-adaptive%20Full-precision%20Bypass%20%28CFB%29.%20The%20DAB%0Aadaptively%20calculates%20thresholds%20and%20scaling%20factors%20for%20binarization%2C%0Aeffectively%20retaining%20more%20information%20within%20BNNs.%20Meanwhile%2C%20the%20CFB%0Afacilitates%20full-precision%20bypassing%20for%20binary%20convolutional%20layers%20undergoing%0Avarious%20channel%20size%20transformations%2C%20which%20enhances%20the%20propagation%20of%0Areal-valued%20signals%20and%20minimizes%20information%20loss.%20By%20leveraging%20these%0Atechniques%2C%20BiDense%20preserves%20more%20real-valued%20information%2C%20enabling%20more%0Aaccurate%20and%20detailed%20dense%20predictions%20in%20BNNs.%20Extensive%20experiments%0Ademonstrate%20that%20our%20framework%20achieves%20performance%20levels%20comparable%20to%0Afull-precision%20models%20while%20significantly%20reducing%20memory%20usage%20and%0Acomputational%20costs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10346v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiDense%253A%2520Binarization%2520for%2520Dense%2520Prediction%26entry.906535625%3DRui%2520Yin%2520and%2520Haotong%2520Qin%2520and%2520Yulun%2520Zhang%2520and%2520Wenbo%2520Li%2520and%2520Yong%2520Guo%2520and%2520Jianjun%2520Zhu%2520and%2520Cheng%2520Wang%2520and%2520Biao%2520Jia%26entry.1292438233%3D%2520%2520Dense%2520prediction%2520is%2520a%2520critical%2520task%2520in%2520computer%2520vision.%2520However%252C%2520previous%250Amethods%2520often%2520require%2520extensive%2520computational%2520resources%252C%2520which%2520hinders%2520their%250Areal-world%2520application.%2520In%2520this%2520paper%252C%2520we%2520propose%2520BiDense%252C%2520a%2520generalized%2520binary%250Aneural%2520network%2520%2528BNN%2529%2520designed%2520for%2520efficient%2520and%2520accurate%2520dense%2520prediction%250Atasks.%2520BiDense%2520incorporates%2520two%2520key%2520techniques%253A%2520the%2520Distribution-adaptive%250ABinarizer%2520%2528DAB%2529%2520and%2520the%2520Channel-adaptive%2520Full-precision%2520Bypass%2520%2528CFB%2529.%2520The%2520DAB%250Aadaptively%2520calculates%2520thresholds%2520and%2520scaling%2520factors%2520for%2520binarization%252C%250Aeffectively%2520retaining%2520more%2520information%2520within%2520BNNs.%2520Meanwhile%252C%2520the%2520CFB%250Afacilitates%2520full-precision%2520bypassing%2520for%2520binary%2520convolutional%2520layers%2520undergoing%250Avarious%2520channel%2520size%2520transformations%252C%2520which%2520enhances%2520the%2520propagation%2520of%250Areal-valued%2520signals%2520and%2520minimizes%2520information%2520loss.%2520By%2520leveraging%2520these%250Atechniques%252C%2520BiDense%2520preserves%2520more%2520real-valued%2520information%252C%2520enabling%2520more%250Aaccurate%2520and%2520detailed%2520dense%2520predictions%2520in%2520BNNs.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520our%2520framework%2520achieves%2520performance%2520levels%2520comparable%2520to%250Afull-precision%2520models%2520while%2520significantly%2520reducing%2520memory%2520usage%2520and%250Acomputational%2520costs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10346v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BiDense%3A%20Binarization%20for%20Dense%20Prediction&entry.906535625=Rui%20Yin%20and%20Haotong%20Qin%20and%20Yulun%20Zhang%20and%20Wenbo%20Li%20and%20Yong%20Guo%20and%20Jianjun%20Zhu%20and%20Cheng%20Wang%20and%20Biao%20Jia&entry.1292438233=%20%20Dense%20prediction%20is%20a%20critical%20task%20in%20computer%20vision.%20However%2C%20previous%0Amethods%20often%20require%20extensive%20computational%20resources%2C%20which%20hinders%20their%0Areal-world%20application.%20In%20this%20paper%2C%20we%20propose%20BiDense%2C%20a%20generalized%20binary%0Aneural%20network%20%28BNN%29%20designed%20for%20efficient%20and%20accurate%20dense%20prediction%0Atasks.%20BiDense%20incorporates%20two%20key%20techniques%3A%20the%20Distribution-adaptive%0ABinarizer%20%28DAB%29%20and%20the%20Channel-adaptive%20Full-precision%20Bypass%20%28CFB%29.%20The%20DAB%0Aadaptively%20calculates%20thresholds%20and%20scaling%20factors%20for%20binarization%2C%0Aeffectively%20retaining%20more%20information%20within%20BNNs.%20Meanwhile%2C%20the%20CFB%0Afacilitates%20full-precision%20bypassing%20for%20binary%20convolutional%20layers%20undergoing%0Avarious%20channel%20size%20transformations%2C%20which%20enhances%20the%20propagation%20of%0Areal-valued%20signals%20and%20minimizes%20information%20loss.%20By%20leveraging%20these%0Atechniques%2C%20BiDense%20preserves%20more%20real-valued%20information%2C%20enabling%20more%0Aaccurate%20and%20detailed%20dense%20predictions%20in%20BNNs.%20Extensive%20experiments%0Ademonstrate%20that%20our%20framework%20achieves%20performance%20levels%20comparable%20to%0Afull-precision%20models%20while%20significantly%20reducing%20memory%20usage%20and%0Acomputational%20costs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10346v1&entry.124074799=Read"},
{"title": "Mechanisms of Generative Image-to-Image Translation Networks", "author": "Guangzong Chen and Mingui Sun and Zhi-Hong Mao and Kangni Liu and Wenyan Jia", "abstract": "  Generative Adversarial Networks (GANs) are a class of neural networks that\nhave been widely used in the field of image-to-image translation. In this\npaper, we propose a streamlined image-to-image translation network with a\nsimpler architecture compared to existing models. We investigate the\nrelationship between GANs and autoencoders and provide an explanation for the\nefficacy of employing only the GAN component for tasks involving image\ntranslation. We show that adversarial for GAN models yields results comparable\nto those of existing methods without additional complex loss penalties.\nSubsequently, we elucidate the rationale behind this phenomenon. We also\nincorporate experimental results to demonstrate the validity of our findings.\n", "link": "http://arxiv.org/abs/2411.10368v1", "date": "2024-11-15", "relevancy": 1.5852, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5349}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5279}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mechanisms%20of%20Generative%20Image-to-Image%20Translation%20Networks&body=Title%3A%20Mechanisms%20of%20Generative%20Image-to-Image%20Translation%20Networks%0AAuthor%3A%20Guangzong%20Chen%20and%20Mingui%20Sun%20and%20Zhi-Hong%20Mao%20and%20Kangni%20Liu%20and%20Wenyan%20Jia%0AAbstract%3A%20%20%20Generative%20Adversarial%20Networks%20%28GANs%29%20are%20a%20class%20of%20neural%20networks%20that%0Ahave%20been%20widely%20used%20in%20the%20field%20of%20image-to-image%20translation.%20In%20this%0Apaper%2C%20we%20propose%20a%20streamlined%20image-to-image%20translation%20network%20with%20a%0Asimpler%20architecture%20compared%20to%20existing%20models.%20We%20investigate%20the%0Arelationship%20between%20GANs%20and%20autoencoders%20and%20provide%20an%20explanation%20for%20the%0Aefficacy%20of%20employing%20only%20the%20GAN%20component%20for%20tasks%20involving%20image%0Atranslation.%20We%20show%20that%20adversarial%20for%20GAN%20models%20yields%20results%20comparable%0Ato%20those%20of%20existing%20methods%20without%20additional%20complex%20loss%20penalties.%0ASubsequently%2C%20we%20elucidate%20the%20rationale%20behind%20this%20phenomenon.%20We%20also%0Aincorporate%20experimental%20results%20to%20demonstrate%20the%20validity%20of%20our%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10368v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMechanisms%2520of%2520Generative%2520Image-to-Image%2520Translation%2520Networks%26entry.906535625%3DGuangzong%2520Chen%2520and%2520Mingui%2520Sun%2520and%2520Zhi-Hong%2520Mao%2520and%2520Kangni%2520Liu%2520and%2520Wenyan%2520Jia%26entry.1292438233%3D%2520%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%2520are%2520a%2520class%2520of%2520neural%2520networks%2520that%250Ahave%2520been%2520widely%2520used%2520in%2520the%2520field%2520of%2520image-to-image%2520translation.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520streamlined%2520image-to-image%2520translation%2520network%2520with%2520a%250Asimpler%2520architecture%2520compared%2520to%2520existing%2520models.%2520We%2520investigate%2520the%250Arelationship%2520between%2520GANs%2520and%2520autoencoders%2520and%2520provide%2520an%2520explanation%2520for%2520the%250Aefficacy%2520of%2520employing%2520only%2520the%2520GAN%2520component%2520for%2520tasks%2520involving%2520image%250Atranslation.%2520We%2520show%2520that%2520adversarial%2520for%2520GAN%2520models%2520yields%2520results%2520comparable%250Ato%2520those%2520of%2520existing%2520methods%2520without%2520additional%2520complex%2520loss%2520penalties.%250ASubsequently%252C%2520we%2520elucidate%2520the%2520rationale%2520behind%2520this%2520phenomenon.%2520We%2520also%250Aincorporate%2520experimental%2520results%2520to%2520demonstrate%2520the%2520validity%2520of%2520our%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10368v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mechanisms%20of%20Generative%20Image-to-Image%20Translation%20Networks&entry.906535625=Guangzong%20Chen%20and%20Mingui%20Sun%20and%20Zhi-Hong%20Mao%20and%20Kangni%20Liu%20and%20Wenyan%20Jia&entry.1292438233=%20%20Generative%20Adversarial%20Networks%20%28GANs%29%20are%20a%20class%20of%20neural%20networks%20that%0Ahave%20been%20widely%20used%20in%20the%20field%20of%20image-to-image%20translation.%20In%20this%0Apaper%2C%20we%20propose%20a%20streamlined%20image-to-image%20translation%20network%20with%20a%0Asimpler%20architecture%20compared%20to%20existing%20models.%20We%20investigate%20the%0Arelationship%20between%20GANs%20and%20autoencoders%20and%20provide%20an%20explanation%20for%20the%0Aefficacy%20of%20employing%20only%20the%20GAN%20component%20for%20tasks%20involving%20image%0Atranslation.%20We%20show%20that%20adversarial%20for%20GAN%20models%20yields%20results%20comparable%0Ato%20those%20of%20existing%20methods%20without%20additional%20complex%20loss%20penalties.%0ASubsequently%2C%20we%20elucidate%20the%20rationale%20behind%20this%20phenomenon.%20We%20also%0Aincorporate%20experimental%20results%20to%20demonstrate%20the%20validity%20of%20our%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10368v1&entry.124074799=Read"},
{"title": "Weakly-Supervised Multimodal Learning on MIMIC-CXR", "author": "Andrea Agostini and Daphn\u00e9 Chopard and Yang Meng and Norbert Fortin and Babak Shahbaba and Stephan Mandt and Thomas M. Sutter and Julia E. Vogt", "abstract": "  Multimodal data integration and label scarcity pose significant challenges\nfor machine learning in medical settings. To address these issues, we conduct\nan in-depth evaluation of the newly proposed Multimodal Variational\nMixture-of-Experts (MMVM) VAE on the challenging MIMIC-CXR dataset. Our\nanalysis demonstrates that the MMVM VAE consistently outperforms other\nmultimodal VAEs and fully supervised approaches, highlighting its strong\npotential for real-world medical applications.\n", "link": "http://arxiv.org/abs/2411.10356v1", "date": "2024-11-15", "relevancy": 1.5744, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5356}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5171}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weakly-Supervised%20Multimodal%20Learning%20on%20MIMIC-CXR&body=Title%3A%20Weakly-Supervised%20Multimodal%20Learning%20on%20MIMIC-CXR%0AAuthor%3A%20Andrea%20Agostini%20and%20Daphn%C3%A9%20Chopard%20and%20Yang%20Meng%20and%20Norbert%20Fortin%20and%20Babak%20Shahbaba%20and%20Stephan%20Mandt%20and%20Thomas%20M.%20Sutter%20and%20Julia%20E.%20Vogt%0AAbstract%3A%20%20%20Multimodal%20data%20integration%20and%20label%20scarcity%20pose%20significant%20challenges%0Afor%20machine%20learning%20in%20medical%20settings.%20To%20address%20these%20issues%2C%20we%20conduct%0Aan%20in-depth%20evaluation%20of%20the%20newly%20proposed%20Multimodal%20Variational%0AMixture-of-Experts%20%28MMVM%29%20VAE%20on%20the%20challenging%20MIMIC-CXR%20dataset.%20Our%0Aanalysis%20demonstrates%20that%20the%20MMVM%20VAE%20consistently%20outperforms%20other%0Amultimodal%20VAEs%20and%20fully%20supervised%20approaches%2C%20highlighting%20its%20strong%0Apotential%20for%20real-world%20medical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10356v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeakly-Supervised%2520Multimodal%2520Learning%2520on%2520MIMIC-CXR%26entry.906535625%3DAndrea%2520Agostini%2520and%2520Daphn%25C3%25A9%2520Chopard%2520and%2520Yang%2520Meng%2520and%2520Norbert%2520Fortin%2520and%2520Babak%2520Shahbaba%2520and%2520Stephan%2520Mandt%2520and%2520Thomas%2520M.%2520Sutter%2520and%2520Julia%2520E.%2520Vogt%26entry.1292438233%3D%2520%2520Multimodal%2520data%2520integration%2520and%2520label%2520scarcity%2520pose%2520significant%2520challenges%250Afor%2520machine%2520learning%2520in%2520medical%2520settings.%2520To%2520address%2520these%2520issues%252C%2520we%2520conduct%250Aan%2520in-depth%2520evaluation%2520of%2520the%2520newly%2520proposed%2520Multimodal%2520Variational%250AMixture-of-Experts%2520%2528MMVM%2529%2520VAE%2520on%2520the%2520challenging%2520MIMIC-CXR%2520dataset.%2520Our%250Aanalysis%2520demonstrates%2520that%2520the%2520MMVM%2520VAE%2520consistently%2520outperforms%2520other%250Amultimodal%2520VAEs%2520and%2520fully%2520supervised%2520approaches%252C%2520highlighting%2520its%2520strong%250Apotential%2520for%2520real-world%2520medical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10356v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weakly-Supervised%20Multimodal%20Learning%20on%20MIMIC-CXR&entry.906535625=Andrea%20Agostini%20and%20Daphn%C3%A9%20Chopard%20and%20Yang%20Meng%20and%20Norbert%20Fortin%20and%20Babak%20Shahbaba%20and%20Stephan%20Mandt%20and%20Thomas%20M.%20Sutter%20and%20Julia%20E.%20Vogt&entry.1292438233=%20%20Multimodal%20data%20integration%20and%20label%20scarcity%20pose%20significant%20challenges%0Afor%20machine%20learning%20in%20medical%20settings.%20To%20address%20these%20issues%2C%20we%20conduct%0Aan%20in-depth%20evaluation%20of%20the%20newly%20proposed%20Multimodal%20Variational%0AMixture-of-Experts%20%28MMVM%29%20VAE%20on%20the%20challenging%20MIMIC-CXR%20dataset.%20Our%0Aanalysis%20demonstrates%20that%20the%20MMVM%20VAE%20consistently%20outperforms%20other%0Amultimodal%20VAEs%20and%20fully%20supervised%20approaches%2C%20highlighting%20its%20strong%0Apotential%20for%20real-world%20medical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10356v1&entry.124074799=Read"},
{"title": "On the Foundation Model for Cardiac MRI Reconstruction", "author": "Chi Zhang and Michael Loecher and Cagan Alkan and Mahmut Yurt and Shreyas S. Vasanawala and Daniel B. Ennis", "abstract": "  In recent years, machine learning (ML) based reconstruction has been widely\ninvestigated and employed in cardiac magnetic resonance (CMR) imaging. ML-based\nreconstructions can deliver clinically acceptable image quality under\nsubstantially accelerated scans. ML-based reconstruction, however, also\nrequires substantial data and computational time to train the neural network,\nwhich is often optimized for a fixed acceleration rate or image contrast. In\npractice, imaging parameters are often tuned to best suit the diagnosis, which\nmay differ from the training data. This can result in degraded image quality,\nand multiple trained networks are needed to fulfill the clinical demands. In\nthis study, we propose a foundation model that uses adaptive unrolling,\nchannel-shifting, and Pattern and Contrast-Prompt-UNet (PCP-UNet) to tackle the\nproblem. In particular, the undersampled data goes through a different number\nof unrolled iterations according to its acceleration rate. Channel-shifting\nimproves reconstructed data quality. The PCP-UNet is equipped with an image\ncontrast and sampling pattern prompt. In vivo CMR experiments were performed\nusing mixed combinations of image contrasts, acceleration rates, and\n(under)sampling patterns. The proposed foundation model has significantly\nimproved image quality for a wide range of CMR protocols and outperforms the\nconventional ML-based method.\n", "link": "http://arxiv.org/abs/2411.10403v1", "date": "2024-11-15", "relevancy": 1.5576, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5292}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5178}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5158}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Foundation%20Model%20for%20Cardiac%20MRI%20Reconstruction&body=Title%3A%20On%20the%20Foundation%20Model%20for%20Cardiac%20MRI%20Reconstruction%0AAuthor%3A%20Chi%20Zhang%20and%20Michael%20Loecher%20and%20Cagan%20Alkan%20and%20Mahmut%20Yurt%20and%20Shreyas%20S.%20Vasanawala%20and%20Daniel%20B.%20Ennis%0AAbstract%3A%20%20%20In%20recent%20years%2C%20machine%20learning%20%28ML%29%20based%20reconstruction%20has%20been%20widely%0Ainvestigated%20and%20employed%20in%20cardiac%20magnetic%20resonance%20%28CMR%29%20imaging.%20ML-based%0Areconstructions%20can%20deliver%20clinically%20acceptable%20image%20quality%20under%0Asubstantially%20accelerated%20scans.%20ML-based%20reconstruction%2C%20however%2C%20also%0Arequires%20substantial%20data%20and%20computational%20time%20to%20train%20the%20neural%20network%2C%0Awhich%20is%20often%20optimized%20for%20a%20fixed%20acceleration%20rate%20or%20image%20contrast.%20In%0Apractice%2C%20imaging%20parameters%20are%20often%20tuned%20to%20best%20suit%20the%20diagnosis%2C%20which%0Amay%20differ%20from%20the%20training%20data.%20This%20can%20result%20in%20degraded%20image%20quality%2C%0Aand%20multiple%20trained%20networks%20are%20needed%20to%20fulfill%20the%20clinical%20demands.%20In%0Athis%20study%2C%20we%20propose%20a%20foundation%20model%20that%20uses%20adaptive%20unrolling%2C%0Achannel-shifting%2C%20and%20Pattern%20and%20Contrast-Prompt-UNet%20%28PCP-UNet%29%20to%20tackle%20the%0Aproblem.%20In%20particular%2C%20the%20undersampled%20data%20goes%20through%20a%20different%20number%0Aof%20unrolled%20iterations%20according%20to%20its%20acceleration%20rate.%20Channel-shifting%0Aimproves%20reconstructed%20data%20quality.%20The%20PCP-UNet%20is%20equipped%20with%20an%20image%0Acontrast%20and%20sampling%20pattern%20prompt.%20In%20vivo%20CMR%20experiments%20were%20performed%0Ausing%20mixed%20combinations%20of%20image%20contrasts%2C%20acceleration%20rates%2C%20and%0A%28under%29sampling%20patterns.%20The%20proposed%20foundation%20model%20has%20significantly%0Aimproved%20image%20quality%20for%20a%20wide%20range%20of%20CMR%20protocols%20and%20outperforms%20the%0Aconventional%20ML-based%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10403v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Foundation%2520Model%2520for%2520Cardiac%2520MRI%2520Reconstruction%26entry.906535625%3DChi%2520Zhang%2520and%2520Michael%2520Loecher%2520and%2520Cagan%2520Alkan%2520and%2520Mahmut%2520Yurt%2520and%2520Shreyas%2520S.%2520Vasanawala%2520and%2520Daniel%2520B.%2520Ennis%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520machine%2520learning%2520%2528ML%2529%2520based%2520reconstruction%2520has%2520been%2520widely%250Ainvestigated%2520and%2520employed%2520in%2520cardiac%2520magnetic%2520resonance%2520%2528CMR%2529%2520imaging.%2520ML-based%250Areconstructions%2520can%2520deliver%2520clinically%2520acceptable%2520image%2520quality%2520under%250Asubstantially%2520accelerated%2520scans.%2520ML-based%2520reconstruction%252C%2520however%252C%2520also%250Arequires%2520substantial%2520data%2520and%2520computational%2520time%2520to%2520train%2520the%2520neural%2520network%252C%250Awhich%2520is%2520often%2520optimized%2520for%2520a%2520fixed%2520acceleration%2520rate%2520or%2520image%2520contrast.%2520In%250Apractice%252C%2520imaging%2520parameters%2520are%2520often%2520tuned%2520to%2520best%2520suit%2520the%2520diagnosis%252C%2520which%250Amay%2520differ%2520from%2520the%2520training%2520data.%2520This%2520can%2520result%2520in%2520degraded%2520image%2520quality%252C%250Aand%2520multiple%2520trained%2520networks%2520are%2520needed%2520to%2520fulfill%2520the%2520clinical%2520demands.%2520In%250Athis%2520study%252C%2520we%2520propose%2520a%2520foundation%2520model%2520that%2520uses%2520adaptive%2520unrolling%252C%250Achannel-shifting%252C%2520and%2520Pattern%2520and%2520Contrast-Prompt-UNet%2520%2528PCP-UNet%2529%2520to%2520tackle%2520the%250Aproblem.%2520In%2520particular%252C%2520the%2520undersampled%2520data%2520goes%2520through%2520a%2520different%2520number%250Aof%2520unrolled%2520iterations%2520according%2520to%2520its%2520acceleration%2520rate.%2520Channel-shifting%250Aimproves%2520reconstructed%2520data%2520quality.%2520The%2520PCP-UNet%2520is%2520equipped%2520with%2520an%2520image%250Acontrast%2520and%2520sampling%2520pattern%2520prompt.%2520In%2520vivo%2520CMR%2520experiments%2520were%2520performed%250Ausing%2520mixed%2520combinations%2520of%2520image%2520contrasts%252C%2520acceleration%2520rates%252C%2520and%250A%2528under%2529sampling%2520patterns.%2520The%2520proposed%2520foundation%2520model%2520has%2520significantly%250Aimproved%2520image%2520quality%2520for%2520a%2520wide%2520range%2520of%2520CMR%2520protocols%2520and%2520outperforms%2520the%250Aconventional%2520ML-based%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10403v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Foundation%20Model%20for%20Cardiac%20MRI%20Reconstruction&entry.906535625=Chi%20Zhang%20and%20Michael%20Loecher%20and%20Cagan%20Alkan%20and%20Mahmut%20Yurt%20and%20Shreyas%20S.%20Vasanawala%20and%20Daniel%20B.%20Ennis&entry.1292438233=%20%20In%20recent%20years%2C%20machine%20learning%20%28ML%29%20based%20reconstruction%20has%20been%20widely%0Ainvestigated%20and%20employed%20in%20cardiac%20magnetic%20resonance%20%28CMR%29%20imaging.%20ML-based%0Areconstructions%20can%20deliver%20clinically%20acceptable%20image%20quality%20under%0Asubstantially%20accelerated%20scans.%20ML-based%20reconstruction%2C%20however%2C%20also%0Arequires%20substantial%20data%20and%20computational%20time%20to%20train%20the%20neural%20network%2C%0Awhich%20is%20often%20optimized%20for%20a%20fixed%20acceleration%20rate%20or%20image%20contrast.%20In%0Apractice%2C%20imaging%20parameters%20are%20often%20tuned%20to%20best%20suit%20the%20diagnosis%2C%20which%0Amay%20differ%20from%20the%20training%20data.%20This%20can%20result%20in%20degraded%20image%20quality%2C%0Aand%20multiple%20trained%20networks%20are%20needed%20to%20fulfill%20the%20clinical%20demands.%20In%0Athis%20study%2C%20we%20propose%20a%20foundation%20model%20that%20uses%20adaptive%20unrolling%2C%0Achannel-shifting%2C%20and%20Pattern%20and%20Contrast-Prompt-UNet%20%28PCP-UNet%29%20to%20tackle%20the%0Aproblem.%20In%20particular%2C%20the%20undersampled%20data%20goes%20through%20a%20different%20number%0Aof%20unrolled%20iterations%20according%20to%20its%20acceleration%20rate.%20Channel-shifting%0Aimproves%20reconstructed%20data%20quality.%20The%20PCP-UNet%20is%20equipped%20with%20an%20image%0Acontrast%20and%20sampling%20pattern%20prompt.%20In%20vivo%20CMR%20experiments%20were%20performed%0Ausing%20mixed%20combinations%20of%20image%20contrasts%2C%20acceleration%20rates%2C%20and%0A%28under%29sampling%20patterns.%20The%20proposed%20foundation%20model%20has%20significantly%0Aimproved%20image%20quality%20for%20a%20wide%20range%20of%20CMR%20protocols%20and%20outperforms%20the%0Aconventional%20ML-based%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10403v1&entry.124074799=Read"},
{"title": "Bitcoin Research with a Transaction Graph Dataset", "author": "Hugo Schnoering and Michalis Vazirgiannis", "abstract": "  Bitcoin, launched in 2008 by Satoshi Nakamoto, established a new digital\neconomy where value can be stored and transferred in a fully decentralized\nmanner - alleviating the need for a central authority. This paper introduces a\nlarge scale dataset in the form of a transactions graph representing\ntransactions between Bitcoin users along with a set of tasks and baselines. The\ngraph includes 252 million nodes and 785 million edges, covering a time span of\nnearly 13 years of and 670 million transactions. Each node and edge is\ntimestamped. As for supervised tasks we provide two labeled sets i. a 33,000\nnodes based on entity type and ii. nearly 100,000 Bitcoin addresses labeled\nwith an entity name and an entity type. This is the largest publicly available\ndata set of bitcoin transactions designed to facilitate advanced research and\nexploration in this domain, overcoming the limitations of existing datasets.\nVarious graph neural network models are trained to predict node labels,\nestablishing a baseline for future research. In addition, several use cases are\npresented to demonstrate the dataset's applicability beyond Bitcoin analysis.\nFinally, all data and source code is made publicly available to enable\nreproducibility of the results.\n", "link": "http://arxiv.org/abs/2411.10325v1", "date": "2024-11-15", "relevancy": 1.5551, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3997}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3912}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bitcoin%20Research%20with%20a%20Transaction%20Graph%20Dataset&body=Title%3A%20Bitcoin%20Research%20with%20a%20Transaction%20Graph%20Dataset%0AAuthor%3A%20Hugo%20Schnoering%20and%20Michalis%20Vazirgiannis%0AAbstract%3A%20%20%20Bitcoin%2C%20launched%20in%202008%20by%20Satoshi%20Nakamoto%2C%20established%20a%20new%20digital%0Aeconomy%20where%20value%20can%20be%20stored%20and%20transferred%20in%20a%20fully%20decentralized%0Amanner%20-%20alleviating%20the%20need%20for%20a%20central%20authority.%20This%20paper%20introduces%20a%0Alarge%20scale%20dataset%20in%20the%20form%20of%20a%20transactions%20graph%20representing%0Atransactions%20between%20Bitcoin%20users%20along%20with%20a%20set%20of%20tasks%20and%20baselines.%20The%0Agraph%20includes%20252%20million%20nodes%20and%20785%20million%20edges%2C%20covering%20a%20time%20span%20of%0Anearly%2013%20years%20of%20and%20670%20million%20transactions.%20Each%20node%20and%20edge%20is%0Atimestamped.%20As%20for%20supervised%20tasks%20we%20provide%20two%20labeled%20sets%20i.%20a%2033%2C000%0Anodes%20based%20on%20entity%20type%20and%20ii.%20nearly%20100%2C000%20Bitcoin%20addresses%20labeled%0Awith%20an%20entity%20name%20and%20an%20entity%20type.%20This%20is%20the%20largest%20publicly%20available%0Adata%20set%20of%20bitcoin%20transactions%20designed%20to%20facilitate%20advanced%20research%20and%0Aexploration%20in%20this%20domain%2C%20overcoming%20the%20limitations%20of%20existing%20datasets.%0AVarious%20graph%20neural%20network%20models%20are%20trained%20to%20predict%20node%20labels%2C%0Aestablishing%20a%20baseline%20for%20future%20research.%20In%20addition%2C%20several%20use%20cases%20are%0Apresented%20to%20demonstrate%20the%20dataset%27s%20applicability%20beyond%20Bitcoin%20analysis.%0AFinally%2C%20all%20data%20and%20source%20code%20is%20made%20publicly%20available%20to%20enable%0Areproducibility%20of%20the%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10325v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBitcoin%2520Research%2520with%2520a%2520Transaction%2520Graph%2520Dataset%26entry.906535625%3DHugo%2520Schnoering%2520and%2520Michalis%2520Vazirgiannis%26entry.1292438233%3D%2520%2520Bitcoin%252C%2520launched%2520in%25202008%2520by%2520Satoshi%2520Nakamoto%252C%2520established%2520a%2520new%2520digital%250Aeconomy%2520where%2520value%2520can%2520be%2520stored%2520and%2520transferred%2520in%2520a%2520fully%2520decentralized%250Amanner%2520-%2520alleviating%2520the%2520need%2520for%2520a%2520central%2520authority.%2520This%2520paper%2520introduces%2520a%250Alarge%2520scale%2520dataset%2520in%2520the%2520form%2520of%2520a%2520transactions%2520graph%2520representing%250Atransactions%2520between%2520Bitcoin%2520users%2520along%2520with%2520a%2520set%2520of%2520tasks%2520and%2520baselines.%2520The%250Agraph%2520includes%2520252%2520million%2520nodes%2520and%2520785%2520million%2520edges%252C%2520covering%2520a%2520time%2520span%2520of%250Anearly%252013%2520years%2520of%2520and%2520670%2520million%2520transactions.%2520Each%2520node%2520and%2520edge%2520is%250Atimestamped.%2520As%2520for%2520supervised%2520tasks%2520we%2520provide%2520two%2520labeled%2520sets%2520i.%2520a%252033%252C000%250Anodes%2520based%2520on%2520entity%2520type%2520and%2520ii.%2520nearly%2520100%252C000%2520Bitcoin%2520addresses%2520labeled%250Awith%2520an%2520entity%2520name%2520and%2520an%2520entity%2520type.%2520This%2520is%2520the%2520largest%2520publicly%2520available%250Adata%2520set%2520of%2520bitcoin%2520transactions%2520designed%2520to%2520facilitate%2520advanced%2520research%2520and%250Aexploration%2520in%2520this%2520domain%252C%2520overcoming%2520the%2520limitations%2520of%2520existing%2520datasets.%250AVarious%2520graph%2520neural%2520network%2520models%2520are%2520trained%2520to%2520predict%2520node%2520labels%252C%250Aestablishing%2520a%2520baseline%2520for%2520future%2520research.%2520In%2520addition%252C%2520several%2520use%2520cases%2520are%250Apresented%2520to%2520demonstrate%2520the%2520dataset%2527s%2520applicability%2520beyond%2520Bitcoin%2520analysis.%250AFinally%252C%2520all%2520data%2520and%2520source%2520code%2520is%2520made%2520publicly%2520available%2520to%2520enable%250Areproducibility%2520of%2520the%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10325v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bitcoin%20Research%20with%20a%20Transaction%20Graph%20Dataset&entry.906535625=Hugo%20Schnoering%20and%20Michalis%20Vazirgiannis&entry.1292438233=%20%20Bitcoin%2C%20launched%20in%202008%20by%20Satoshi%20Nakamoto%2C%20established%20a%20new%20digital%0Aeconomy%20where%20value%20can%20be%20stored%20and%20transferred%20in%20a%20fully%20decentralized%0Amanner%20-%20alleviating%20the%20need%20for%20a%20central%20authority.%20This%20paper%20introduces%20a%0Alarge%20scale%20dataset%20in%20the%20form%20of%20a%20transactions%20graph%20representing%0Atransactions%20between%20Bitcoin%20users%20along%20with%20a%20set%20of%20tasks%20and%20baselines.%20The%0Agraph%20includes%20252%20million%20nodes%20and%20785%20million%20edges%2C%20covering%20a%20time%20span%20of%0Anearly%2013%20years%20of%20and%20670%20million%20transactions.%20Each%20node%20and%20edge%20is%0Atimestamped.%20As%20for%20supervised%20tasks%20we%20provide%20two%20labeled%20sets%20i.%20a%2033%2C000%0Anodes%20based%20on%20entity%20type%20and%20ii.%20nearly%20100%2C000%20Bitcoin%20addresses%20labeled%0Awith%20an%20entity%20name%20and%20an%20entity%20type.%20This%20is%20the%20largest%20publicly%20available%0Adata%20set%20of%20bitcoin%20transactions%20designed%20to%20facilitate%20advanced%20research%20and%0Aexploration%20in%20this%20domain%2C%20overcoming%20the%20limitations%20of%20existing%20datasets.%0AVarious%20graph%20neural%20network%20models%20are%20trained%20to%20predict%20node%20labels%2C%0Aestablishing%20a%20baseline%20for%20future%20research.%20In%20addition%2C%20several%20use%20cases%20are%0Apresented%20to%20demonstrate%20the%20dataset%27s%20applicability%20beyond%20Bitcoin%20analysis.%0AFinally%2C%20all%20data%20and%20source%20code%20is%20made%20publicly%20available%20to%20enable%0Areproducibility%20of%20the%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10325v1&entry.124074799=Read"},
{"title": "DCD: Discriminative and Consistent Representation Distillation", "author": "Nikolaos Giakoumoglou and Tania Stathaki", "abstract": "  Knowledge Distillation (KD) aims to transfer knowledge from a large teacher\nmodel to a smaller student model. While contrastive learning has shown promise\nin self-supervised learning by creating discriminative representations, its\napplication in knowledge distillation remains limited and focuses primarily on\ndiscrimination, neglecting the structural relationships captured by the teacher\nmodel. To address this limitation, we propose Discriminative and Consistent\nDistillation (DCD), which employs a contrastive loss along with a consistency\nregularization to minimize the discrepancy between the distributions of teacher\nand student representations. Our method introduces learnable temperature and\nbias parameters that adapt during training to balance these complementary\nobjectives, replacing the fixed hyperparameters commonly used in contrastive\nlearning approaches. Through extensive experiments on CIFAR-100 and ImageNet\nILSVRC-2012, we demonstrate that DCD achieves state-of-the-art performance,\nwith the student model sometimes surpassing the teacher's accuracy.\nFurthermore, we show that DCD's learned representations exhibit superior\ncross-dataset generalization when transferred to Tiny ImageNet and STL-10. Code\nis available at https://github.com/giakoumoglou/distillers.\n", "link": "http://arxiv.org/abs/2407.11802v3", "date": "2024-11-15", "relevancy": 1.4944, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5035}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.499}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DCD%3A%20Discriminative%20and%20Consistent%20Representation%20Distillation&body=Title%3A%20DCD%3A%20Discriminative%20and%20Consistent%20Representation%20Distillation%0AAuthor%3A%20Nikolaos%20Giakoumoglou%20and%20Tania%20Stathaki%0AAbstract%3A%20%20%20Knowledge%20Distillation%20%28KD%29%20aims%20to%20transfer%20knowledge%20from%20a%20large%20teacher%0Amodel%20to%20a%20smaller%20student%20model.%20While%20contrastive%20learning%20has%20shown%20promise%0Ain%20self-supervised%20learning%20by%20creating%20discriminative%20representations%2C%20its%0Aapplication%20in%20knowledge%20distillation%20remains%20limited%20and%20focuses%20primarily%20on%0Adiscrimination%2C%20neglecting%20the%20structural%20relationships%20captured%20by%20the%20teacher%0Amodel.%20To%20address%20this%20limitation%2C%20we%20propose%20Discriminative%20and%20Consistent%0ADistillation%20%28DCD%29%2C%20which%20employs%20a%20contrastive%20loss%20along%20with%20a%20consistency%0Aregularization%20to%20minimize%20the%20discrepancy%20between%20the%20distributions%20of%20teacher%0Aand%20student%20representations.%20Our%20method%20introduces%20learnable%20temperature%20and%0Abias%20parameters%20that%20adapt%20during%20training%20to%20balance%20these%20complementary%0Aobjectives%2C%20replacing%20the%20fixed%20hyperparameters%20commonly%20used%20in%20contrastive%0Alearning%20approaches.%20Through%20extensive%20experiments%20on%20CIFAR-100%20and%20ImageNet%0AILSVRC-2012%2C%20we%20demonstrate%20that%20DCD%20achieves%20state-of-the-art%20performance%2C%0Awith%20the%20student%20model%20sometimes%20surpassing%20the%20teacher%27s%20accuracy.%0AFurthermore%2C%20we%20show%20that%20DCD%27s%20learned%20representations%20exhibit%20superior%0Across-dataset%20generalization%20when%20transferred%20to%20Tiny%20ImageNet%20and%20STL-10.%20Code%0Ais%20available%20at%20https%3A//github.com/giakoumoglou/distillers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11802v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDCD%253A%2520Discriminative%2520and%2520Consistent%2520Representation%2520Distillation%26entry.906535625%3DNikolaos%2520Giakoumoglou%2520and%2520Tania%2520Stathaki%26entry.1292438233%3D%2520%2520Knowledge%2520Distillation%2520%2528KD%2529%2520aims%2520to%2520transfer%2520knowledge%2520from%2520a%2520large%2520teacher%250Amodel%2520to%2520a%2520smaller%2520student%2520model.%2520While%2520contrastive%2520learning%2520has%2520shown%2520promise%250Ain%2520self-supervised%2520learning%2520by%2520creating%2520discriminative%2520representations%252C%2520its%250Aapplication%2520in%2520knowledge%2520distillation%2520remains%2520limited%2520and%2520focuses%2520primarily%2520on%250Adiscrimination%252C%2520neglecting%2520the%2520structural%2520relationships%2520captured%2520by%2520the%2520teacher%250Amodel.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520Discriminative%2520and%2520Consistent%250ADistillation%2520%2528DCD%2529%252C%2520which%2520employs%2520a%2520contrastive%2520loss%2520along%2520with%2520a%2520consistency%250Aregularization%2520to%2520minimize%2520the%2520discrepancy%2520between%2520the%2520distributions%2520of%2520teacher%250Aand%2520student%2520representations.%2520Our%2520method%2520introduces%2520learnable%2520temperature%2520and%250Abias%2520parameters%2520that%2520adapt%2520during%2520training%2520to%2520balance%2520these%2520complementary%250Aobjectives%252C%2520replacing%2520the%2520fixed%2520hyperparameters%2520commonly%2520used%2520in%2520contrastive%250Alearning%2520approaches.%2520Through%2520extensive%2520experiments%2520on%2520CIFAR-100%2520and%2520ImageNet%250AILSVRC-2012%252C%2520we%2520demonstrate%2520that%2520DCD%2520achieves%2520state-of-the-art%2520performance%252C%250Awith%2520the%2520student%2520model%2520sometimes%2520surpassing%2520the%2520teacher%2527s%2520accuracy.%250AFurthermore%252C%2520we%2520show%2520that%2520DCD%2527s%2520learned%2520representations%2520exhibit%2520superior%250Across-dataset%2520generalization%2520when%2520transferred%2520to%2520Tiny%2520ImageNet%2520and%2520STL-10.%2520Code%250Ais%2520available%2520at%2520https%253A//github.com/giakoumoglou/distillers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11802v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DCD%3A%20Discriminative%20and%20Consistent%20Representation%20Distillation&entry.906535625=Nikolaos%20Giakoumoglou%20and%20Tania%20Stathaki&entry.1292438233=%20%20Knowledge%20Distillation%20%28KD%29%20aims%20to%20transfer%20knowledge%20from%20a%20large%20teacher%0Amodel%20to%20a%20smaller%20student%20model.%20While%20contrastive%20learning%20has%20shown%20promise%0Ain%20self-supervised%20learning%20by%20creating%20discriminative%20representations%2C%20its%0Aapplication%20in%20knowledge%20distillation%20remains%20limited%20and%20focuses%20primarily%20on%0Adiscrimination%2C%20neglecting%20the%20structural%20relationships%20captured%20by%20the%20teacher%0Amodel.%20To%20address%20this%20limitation%2C%20we%20propose%20Discriminative%20and%20Consistent%0ADistillation%20%28DCD%29%2C%20which%20employs%20a%20contrastive%20loss%20along%20with%20a%20consistency%0Aregularization%20to%20minimize%20the%20discrepancy%20between%20the%20distributions%20of%20teacher%0Aand%20student%20representations.%20Our%20method%20introduces%20learnable%20temperature%20and%0Abias%20parameters%20that%20adapt%20during%20training%20to%20balance%20these%20complementary%0Aobjectives%2C%20replacing%20the%20fixed%20hyperparameters%20commonly%20used%20in%20contrastive%0Alearning%20approaches.%20Through%20extensive%20experiments%20on%20CIFAR-100%20and%20ImageNet%0AILSVRC-2012%2C%20we%20demonstrate%20that%20DCD%20achieves%20state-of-the-art%20performance%2C%0Awith%20the%20student%20model%20sometimes%20surpassing%20the%20teacher%27s%20accuracy.%0AFurthermore%2C%20we%20show%20that%20DCD%27s%20learned%20representations%20exhibit%20superior%0Across-dataset%20generalization%20when%20transferred%20to%20Tiny%20ImageNet%20and%20STL-10.%20Code%0Ais%20available%20at%20https%3A//github.com/giakoumoglou/distillers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11802v3&entry.124074799=Read"},
{"title": "Systolic Arrays and Structured Pruning Co-design for Efficient\n  Transformers in Edge Systems", "author": "Pedro Palacios and Rafael Medina and Jean-Luc Rouas and Giovanni Ansaloni and David Atienza", "abstract": "  Efficient deployment of resource-intensive transformers on edge devices\nnecessitates cross-stack optimization. We thus study the interrelation between\nstructured pruning and systolic acceleration, matching the size of pruned\nblocks with the systolic array dimensions. In this setting, computations of\npruned weight blocks can be skipped, reducing run-time and energy consumption,\nbut potentially impacting quality of service (QoS). To evaluate the trade-offs\nbetween systolic array size and sparsity opportunities, we present a novel\nco-design framework that integrates algorithmic optimization, system\nsimulation, and hardware design. Targeting speech recognition using\ntransformers as a case study, we analyze how configuration choices across the\nstack affect performance metrics. Results demonstrate that structured pruning\non systems featuring systolic array acceleration can effectively increase\nperformance, while maintaining high QoS levels. Up to 26% system-wide speedups\ndue to structured pruning were measured, with only 1.4% word error rate\ndegradation on the standard Librispeech dataset.\n", "link": "http://arxiv.org/abs/2411.10285v1", "date": "2024-11-15", "relevancy": 1.4545, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5325}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4776}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Systolic%20Arrays%20and%20Structured%20Pruning%20Co-design%20for%20Efficient%0A%20%20Transformers%20in%20Edge%20Systems&body=Title%3A%20Systolic%20Arrays%20and%20Structured%20Pruning%20Co-design%20for%20Efficient%0A%20%20Transformers%20in%20Edge%20Systems%0AAuthor%3A%20Pedro%20Palacios%20and%20Rafael%20Medina%20and%20Jean-Luc%20Rouas%20and%20Giovanni%20Ansaloni%20and%20David%20Atienza%0AAbstract%3A%20%20%20Efficient%20deployment%20of%20resource-intensive%20transformers%20on%20edge%20devices%0Anecessitates%20cross-stack%20optimization.%20We%20thus%20study%20the%20interrelation%20between%0Astructured%20pruning%20and%20systolic%20acceleration%2C%20matching%20the%20size%20of%20pruned%0Ablocks%20with%20the%20systolic%20array%20dimensions.%20In%20this%20setting%2C%20computations%20of%0Apruned%20weight%20blocks%20can%20be%20skipped%2C%20reducing%20run-time%20and%20energy%20consumption%2C%0Abut%20potentially%20impacting%20quality%20of%20service%20%28QoS%29.%20To%20evaluate%20the%20trade-offs%0Abetween%20systolic%20array%20size%20and%20sparsity%20opportunities%2C%20we%20present%20a%20novel%0Aco-design%20framework%20that%20integrates%20algorithmic%20optimization%2C%20system%0Asimulation%2C%20and%20hardware%20design.%20Targeting%20speech%20recognition%20using%0Atransformers%20as%20a%20case%20study%2C%20we%20analyze%20how%20configuration%20choices%20across%20the%0Astack%20affect%20performance%20metrics.%20Results%20demonstrate%20that%20structured%20pruning%0Aon%20systems%20featuring%20systolic%20array%20acceleration%20can%20effectively%20increase%0Aperformance%2C%20while%20maintaining%20high%20QoS%20levels.%20Up%20to%2026%25%20system-wide%20speedups%0Adue%20to%20structured%20pruning%20were%20measured%2C%20with%20only%201.4%25%20word%20error%20rate%0Adegradation%20on%20the%20standard%20Librispeech%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10285v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSystolic%2520Arrays%2520and%2520Structured%2520Pruning%2520Co-design%2520for%2520Efficient%250A%2520%2520Transformers%2520in%2520Edge%2520Systems%26entry.906535625%3DPedro%2520Palacios%2520and%2520Rafael%2520Medina%2520and%2520Jean-Luc%2520Rouas%2520and%2520Giovanni%2520Ansaloni%2520and%2520David%2520Atienza%26entry.1292438233%3D%2520%2520Efficient%2520deployment%2520of%2520resource-intensive%2520transformers%2520on%2520edge%2520devices%250Anecessitates%2520cross-stack%2520optimization.%2520We%2520thus%2520study%2520the%2520interrelation%2520between%250Astructured%2520pruning%2520and%2520systolic%2520acceleration%252C%2520matching%2520the%2520size%2520of%2520pruned%250Ablocks%2520with%2520the%2520systolic%2520array%2520dimensions.%2520In%2520this%2520setting%252C%2520computations%2520of%250Apruned%2520weight%2520blocks%2520can%2520be%2520skipped%252C%2520reducing%2520run-time%2520and%2520energy%2520consumption%252C%250Abut%2520potentially%2520impacting%2520quality%2520of%2520service%2520%2528QoS%2529.%2520To%2520evaluate%2520the%2520trade-offs%250Abetween%2520systolic%2520array%2520size%2520and%2520sparsity%2520opportunities%252C%2520we%2520present%2520a%2520novel%250Aco-design%2520framework%2520that%2520integrates%2520algorithmic%2520optimization%252C%2520system%250Asimulation%252C%2520and%2520hardware%2520design.%2520Targeting%2520speech%2520recognition%2520using%250Atransformers%2520as%2520a%2520case%2520study%252C%2520we%2520analyze%2520how%2520configuration%2520choices%2520across%2520the%250Astack%2520affect%2520performance%2520metrics.%2520Results%2520demonstrate%2520that%2520structured%2520pruning%250Aon%2520systems%2520featuring%2520systolic%2520array%2520acceleration%2520can%2520effectively%2520increase%250Aperformance%252C%2520while%2520maintaining%2520high%2520QoS%2520levels.%2520Up%2520to%252026%2525%2520system-wide%2520speedups%250Adue%2520to%2520structured%2520pruning%2520were%2520measured%252C%2520with%2520only%25201.4%2525%2520word%2520error%2520rate%250Adegradation%2520on%2520the%2520standard%2520Librispeech%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10285v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Systolic%20Arrays%20and%20Structured%20Pruning%20Co-design%20for%20Efficient%0A%20%20Transformers%20in%20Edge%20Systems&entry.906535625=Pedro%20Palacios%20and%20Rafael%20Medina%20and%20Jean-Luc%20Rouas%20and%20Giovanni%20Ansaloni%20and%20David%20Atienza&entry.1292438233=%20%20Efficient%20deployment%20of%20resource-intensive%20transformers%20on%20edge%20devices%0Anecessitates%20cross-stack%20optimization.%20We%20thus%20study%20the%20interrelation%20between%0Astructured%20pruning%20and%20systolic%20acceleration%2C%20matching%20the%20size%20of%20pruned%0Ablocks%20with%20the%20systolic%20array%20dimensions.%20In%20this%20setting%2C%20computations%20of%0Apruned%20weight%20blocks%20can%20be%20skipped%2C%20reducing%20run-time%20and%20energy%20consumption%2C%0Abut%20potentially%20impacting%20quality%20of%20service%20%28QoS%29.%20To%20evaluate%20the%20trade-offs%0Abetween%20systolic%20array%20size%20and%20sparsity%20opportunities%2C%20we%20present%20a%20novel%0Aco-design%20framework%20that%20integrates%20algorithmic%20optimization%2C%20system%0Asimulation%2C%20and%20hardware%20design.%20Targeting%20speech%20recognition%20using%0Atransformers%20as%20a%20case%20study%2C%20we%20analyze%20how%20configuration%20choices%20across%20the%0Astack%20affect%20performance%20metrics.%20Results%20demonstrate%20that%20structured%20pruning%0Aon%20systems%20featuring%20systolic%20array%20acceleration%20can%20effectively%20increase%0Aperformance%2C%20while%20maintaining%20high%20QoS%20levels.%20Up%20to%2026%25%20system-wide%20speedups%0Adue%20to%20structured%20pruning%20were%20measured%2C%20with%20only%201.4%25%20word%20error%20rate%0Adegradation%20on%20the%20standard%20Librispeech%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10285v1&entry.124074799=Read"},
{"title": "Demo: Multi-Modal Seizure Prediction System", "author": "Ali Saeizadeh and Pietro Brach del Prever and Douglas Schonholtz and Raffaele Guida and Emrecan Demirors and Jorge M. Jimenez and Pedram Johari and Tommaso Melodia", "abstract": "  This demo presents SeizNet, an innovative system for predicting epileptic\nseizures benefiting from a multi-modal sensor network and utilizing Deep\nLearning (DL) techniques. Epilepsy affects approximately 65 million people\nworldwide, many of whom experience drug-resistant seizures. SeizNet aims at\nproviding highly accurate alerts, allowing individuals to take preventive\nmeasures without being disturbed by false alarms. SeizNet uses a combination of\ndata collected through either invasive (intracranial electroencephalogram\n(iEEG)) or non-invasive (electroencephalogram (EEG) and electrocardiogram\n(ECG)) sensors, and processed by advanced DL algorithms that are optimized for\nreal-time inference at the edge, ensuring privacy and minimizing data\ntransmission. SeizNet achieves > 97% accuracy in seizure prediction while\nkeeping the size and energy restrictions of an implantable device.\n", "link": "http://arxiv.org/abs/2411.05817v2", "date": "2024-11-15", "relevancy": 1.452, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5266}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4802}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Demo%3A%20Multi-Modal%20Seizure%20Prediction%20System&body=Title%3A%20Demo%3A%20Multi-Modal%20Seizure%20Prediction%20System%0AAuthor%3A%20Ali%20Saeizadeh%20and%20Pietro%20Brach%20del%20Prever%20and%20Douglas%20Schonholtz%20and%20Raffaele%20Guida%20and%20Emrecan%20Demirors%20and%20Jorge%20M.%20Jimenez%20and%20Pedram%20Johari%20and%20Tommaso%20Melodia%0AAbstract%3A%20%20%20This%20demo%20presents%20SeizNet%2C%20an%20innovative%20system%20for%20predicting%20epileptic%0Aseizures%20benefiting%20from%20a%20multi-modal%20sensor%20network%20and%20utilizing%20Deep%0ALearning%20%28DL%29%20techniques.%20Epilepsy%20affects%20approximately%2065%20million%20people%0Aworldwide%2C%20many%20of%20whom%20experience%20drug-resistant%20seizures.%20SeizNet%20aims%20at%0Aproviding%20highly%20accurate%20alerts%2C%20allowing%20individuals%20to%20take%20preventive%0Ameasures%20without%20being%20disturbed%20by%20false%20alarms.%20SeizNet%20uses%20a%20combination%20of%0Adata%20collected%20through%20either%20invasive%20%28intracranial%20electroencephalogram%0A%28iEEG%29%29%20or%20non-invasive%20%28electroencephalogram%20%28EEG%29%20and%20electrocardiogram%0A%28ECG%29%29%20sensors%2C%20and%20processed%20by%20advanced%20DL%20algorithms%20that%20are%20optimized%20for%0Areal-time%20inference%20at%20the%20edge%2C%20ensuring%20privacy%20and%20minimizing%20data%0Atransmission.%20SeizNet%20achieves%20%3E%2097%25%20accuracy%20in%20seizure%20prediction%20while%0Akeeping%20the%20size%20and%20energy%20restrictions%20of%20an%20implantable%20device.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05817v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDemo%253A%2520Multi-Modal%2520Seizure%2520Prediction%2520System%26entry.906535625%3DAli%2520Saeizadeh%2520and%2520Pietro%2520Brach%2520del%2520Prever%2520and%2520Douglas%2520Schonholtz%2520and%2520Raffaele%2520Guida%2520and%2520Emrecan%2520Demirors%2520and%2520Jorge%2520M.%2520Jimenez%2520and%2520Pedram%2520Johari%2520and%2520Tommaso%2520Melodia%26entry.1292438233%3D%2520%2520This%2520demo%2520presents%2520SeizNet%252C%2520an%2520innovative%2520system%2520for%2520predicting%2520epileptic%250Aseizures%2520benefiting%2520from%2520a%2520multi-modal%2520sensor%2520network%2520and%2520utilizing%2520Deep%250ALearning%2520%2528DL%2529%2520techniques.%2520Epilepsy%2520affects%2520approximately%252065%2520million%2520people%250Aworldwide%252C%2520many%2520of%2520whom%2520experience%2520drug-resistant%2520seizures.%2520SeizNet%2520aims%2520at%250Aproviding%2520highly%2520accurate%2520alerts%252C%2520allowing%2520individuals%2520to%2520take%2520preventive%250Ameasures%2520without%2520being%2520disturbed%2520by%2520false%2520alarms.%2520SeizNet%2520uses%2520a%2520combination%2520of%250Adata%2520collected%2520through%2520either%2520invasive%2520%2528intracranial%2520electroencephalogram%250A%2528iEEG%2529%2529%2520or%2520non-invasive%2520%2528electroencephalogram%2520%2528EEG%2529%2520and%2520electrocardiogram%250A%2528ECG%2529%2529%2520sensors%252C%2520and%2520processed%2520by%2520advanced%2520DL%2520algorithms%2520that%2520are%2520optimized%2520for%250Areal-time%2520inference%2520at%2520the%2520edge%252C%2520ensuring%2520privacy%2520and%2520minimizing%2520data%250Atransmission.%2520SeizNet%2520achieves%2520%253E%252097%2525%2520accuracy%2520in%2520seizure%2520prediction%2520while%250Akeeping%2520the%2520size%2520and%2520energy%2520restrictions%2520of%2520an%2520implantable%2520device.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05817v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Demo%3A%20Multi-Modal%20Seizure%20Prediction%20System&entry.906535625=Ali%20Saeizadeh%20and%20Pietro%20Brach%20del%20Prever%20and%20Douglas%20Schonholtz%20and%20Raffaele%20Guida%20and%20Emrecan%20Demirors%20and%20Jorge%20M.%20Jimenez%20and%20Pedram%20Johari%20and%20Tommaso%20Melodia&entry.1292438233=%20%20This%20demo%20presents%20SeizNet%2C%20an%20innovative%20system%20for%20predicting%20epileptic%0Aseizures%20benefiting%20from%20a%20multi-modal%20sensor%20network%20and%20utilizing%20Deep%0ALearning%20%28DL%29%20techniques.%20Epilepsy%20affects%20approximately%2065%20million%20people%0Aworldwide%2C%20many%20of%20whom%20experience%20drug-resistant%20seizures.%20SeizNet%20aims%20at%0Aproviding%20highly%20accurate%20alerts%2C%20allowing%20individuals%20to%20take%20preventive%0Ameasures%20without%20being%20disturbed%20by%20false%20alarms.%20SeizNet%20uses%20a%20combination%20of%0Adata%20collected%20through%20either%20invasive%20%28intracranial%20electroencephalogram%0A%28iEEG%29%29%20or%20non-invasive%20%28electroencephalogram%20%28EEG%29%20and%20electrocardiogram%0A%28ECG%29%29%20sensors%2C%20and%20processed%20by%20advanced%20DL%20algorithms%20that%20are%20optimized%20for%0Areal-time%20inference%20at%20the%20edge%2C%20ensuring%20privacy%20and%20minimizing%20data%0Atransmission.%20SeizNet%20achieves%20%3E%2097%25%20accuracy%20in%20seizure%20prediction%20while%0Akeeping%20the%20size%20and%20energy%20restrictions%20of%20an%20implantable%20device.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05817v2&entry.124074799=Read"},
{"title": "Inconsistencies In Consistency Models: Better ODE Solving Does Not Imply\n  Better Samples", "author": "No\u00ebl Vouitsis and Rasa Hosseinzadeh and Brendan Leigh Ross and Valentin Villecroze and Satya Krishna Gorti and Jesse C. Cresswell and Gabriel Loaiza-Ganem", "abstract": "  Although diffusion models can generate remarkably high-quality samples, they\nare intrinsically bottlenecked by their expensive iterative sampling procedure.\nConsistency models (CMs) have recently emerged as a promising diffusion model\ndistillation method, reducing the cost of sampling by generating high-fidelity\nsamples in just a few iterations. Consistency model distillation aims to solve\nthe probability flow ordinary differential equation (ODE) defined by an\nexisting diffusion model. CMs are not directly trained to minimize error\nagainst an ODE solver, rather they use a more computationally tractable\nobjective. As a way to study how effectively CMs solve the probability flow\nODE, and the effect that any induced error has on the quality of generated\nsamples, we introduce Direct CMs, which \\textit{directly} minimize this error.\nIntriguingly, we find that Direct CMs reduce the ODE solving error compared to\nCMs but also result in significantly worse sample quality, calling into\nquestion why exactly CMs work well in the first place. Full code is available\nat: https://github.com/layer6ai-labs/direct-cms.\n", "link": "http://arxiv.org/abs/2411.08954v2", "date": "2024-11-15", "relevancy": 1.4294, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5231}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.471}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.46}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inconsistencies%20In%20Consistency%20Models%3A%20Better%20ODE%20Solving%20Does%20Not%20Imply%0A%20%20Better%20Samples&body=Title%3A%20Inconsistencies%20In%20Consistency%20Models%3A%20Better%20ODE%20Solving%20Does%20Not%20Imply%0A%20%20Better%20Samples%0AAuthor%3A%20No%C3%ABl%20Vouitsis%20and%20Rasa%20Hosseinzadeh%20and%20Brendan%20Leigh%20Ross%20and%20Valentin%20Villecroze%20and%20Satya%20Krishna%20Gorti%20and%20Jesse%20C.%20Cresswell%20and%20Gabriel%20Loaiza-Ganem%0AAbstract%3A%20%20%20Although%20diffusion%20models%20can%20generate%20remarkably%20high-quality%20samples%2C%20they%0Aare%20intrinsically%20bottlenecked%20by%20their%20expensive%20iterative%20sampling%20procedure.%0AConsistency%20models%20%28CMs%29%20have%20recently%20emerged%20as%20a%20promising%20diffusion%20model%0Adistillation%20method%2C%20reducing%20the%20cost%20of%20sampling%20by%20generating%20high-fidelity%0Asamples%20in%20just%20a%20few%20iterations.%20Consistency%20model%20distillation%20aims%20to%20solve%0Athe%20probability%20flow%20ordinary%20differential%20equation%20%28ODE%29%20defined%20by%20an%0Aexisting%20diffusion%20model.%20CMs%20are%20not%20directly%20trained%20to%20minimize%20error%0Aagainst%20an%20ODE%20solver%2C%20rather%20they%20use%20a%20more%20computationally%20tractable%0Aobjective.%20As%20a%20way%20to%20study%20how%20effectively%20CMs%20solve%20the%20probability%20flow%0AODE%2C%20and%20the%20effect%20that%20any%20induced%20error%20has%20on%20the%20quality%20of%20generated%0Asamples%2C%20we%20introduce%20Direct%20CMs%2C%20which%20%5Ctextit%7Bdirectly%7D%20minimize%20this%20error.%0AIntriguingly%2C%20we%20find%20that%20Direct%20CMs%20reduce%20the%20ODE%20solving%20error%20compared%20to%0ACMs%20but%20also%20result%20in%20significantly%20worse%20sample%20quality%2C%20calling%20into%0Aquestion%20why%20exactly%20CMs%20work%20well%20in%20the%20first%20place.%20Full%20code%20is%20available%0Aat%3A%20https%3A//github.com/layer6ai-labs/direct-cms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08954v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInconsistencies%2520In%2520Consistency%2520Models%253A%2520Better%2520ODE%2520Solving%2520Does%2520Not%2520Imply%250A%2520%2520Better%2520Samples%26entry.906535625%3DNo%25C3%25ABl%2520Vouitsis%2520and%2520Rasa%2520Hosseinzadeh%2520and%2520Brendan%2520Leigh%2520Ross%2520and%2520Valentin%2520Villecroze%2520and%2520Satya%2520Krishna%2520Gorti%2520and%2520Jesse%2520C.%2520Cresswell%2520and%2520Gabriel%2520Loaiza-Ganem%26entry.1292438233%3D%2520%2520Although%2520diffusion%2520models%2520can%2520generate%2520remarkably%2520high-quality%2520samples%252C%2520they%250Aare%2520intrinsically%2520bottlenecked%2520by%2520their%2520expensive%2520iterative%2520sampling%2520procedure.%250AConsistency%2520models%2520%2528CMs%2529%2520have%2520recently%2520emerged%2520as%2520a%2520promising%2520diffusion%2520model%250Adistillation%2520method%252C%2520reducing%2520the%2520cost%2520of%2520sampling%2520by%2520generating%2520high-fidelity%250Asamples%2520in%2520just%2520a%2520few%2520iterations.%2520Consistency%2520model%2520distillation%2520aims%2520to%2520solve%250Athe%2520probability%2520flow%2520ordinary%2520differential%2520equation%2520%2528ODE%2529%2520defined%2520by%2520an%250Aexisting%2520diffusion%2520model.%2520CMs%2520are%2520not%2520directly%2520trained%2520to%2520minimize%2520error%250Aagainst%2520an%2520ODE%2520solver%252C%2520rather%2520they%2520use%2520a%2520more%2520computationally%2520tractable%250Aobjective.%2520As%2520a%2520way%2520to%2520study%2520how%2520effectively%2520CMs%2520solve%2520the%2520probability%2520flow%250AODE%252C%2520and%2520the%2520effect%2520that%2520any%2520induced%2520error%2520has%2520on%2520the%2520quality%2520of%2520generated%250Asamples%252C%2520we%2520introduce%2520Direct%2520CMs%252C%2520which%2520%255Ctextit%257Bdirectly%257D%2520minimize%2520this%2520error.%250AIntriguingly%252C%2520we%2520find%2520that%2520Direct%2520CMs%2520reduce%2520the%2520ODE%2520solving%2520error%2520compared%2520to%250ACMs%2520but%2520also%2520result%2520in%2520significantly%2520worse%2520sample%2520quality%252C%2520calling%2520into%250Aquestion%2520why%2520exactly%2520CMs%2520work%2520well%2520in%2520the%2520first%2520place.%2520Full%2520code%2520is%2520available%250Aat%253A%2520https%253A//github.com/layer6ai-labs/direct-cms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08954v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inconsistencies%20In%20Consistency%20Models%3A%20Better%20ODE%20Solving%20Does%20Not%20Imply%0A%20%20Better%20Samples&entry.906535625=No%C3%ABl%20Vouitsis%20and%20Rasa%20Hosseinzadeh%20and%20Brendan%20Leigh%20Ross%20and%20Valentin%20Villecroze%20and%20Satya%20Krishna%20Gorti%20and%20Jesse%20C.%20Cresswell%20and%20Gabriel%20Loaiza-Ganem&entry.1292438233=%20%20Although%20diffusion%20models%20can%20generate%20remarkably%20high-quality%20samples%2C%20they%0Aare%20intrinsically%20bottlenecked%20by%20their%20expensive%20iterative%20sampling%20procedure.%0AConsistency%20models%20%28CMs%29%20have%20recently%20emerged%20as%20a%20promising%20diffusion%20model%0Adistillation%20method%2C%20reducing%20the%20cost%20of%20sampling%20by%20generating%20high-fidelity%0Asamples%20in%20just%20a%20few%20iterations.%20Consistency%20model%20distillation%20aims%20to%20solve%0Athe%20probability%20flow%20ordinary%20differential%20equation%20%28ODE%29%20defined%20by%20an%0Aexisting%20diffusion%20model.%20CMs%20are%20not%20directly%20trained%20to%20minimize%20error%0Aagainst%20an%20ODE%20solver%2C%20rather%20they%20use%20a%20more%20computationally%20tractable%0Aobjective.%20As%20a%20way%20to%20study%20how%20effectively%20CMs%20solve%20the%20probability%20flow%0AODE%2C%20and%20the%20effect%20that%20any%20induced%20error%20has%20on%20the%20quality%20of%20generated%0Asamples%2C%20we%20introduce%20Direct%20CMs%2C%20which%20%5Ctextit%7Bdirectly%7D%20minimize%20this%20error.%0AIntriguingly%2C%20we%20find%20that%20Direct%20CMs%20reduce%20the%20ODE%20solving%20error%20compared%20to%0ACMs%20but%20also%20result%20in%20significantly%20worse%20sample%20quality%2C%20calling%20into%0Aquestion%20why%20exactly%20CMs%20work%20well%20in%20the%20first%20place.%20Full%20code%20is%20available%0Aat%3A%20https%3A//github.com/layer6ai-labs/direct-cms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08954v2&entry.124074799=Read"},
{"title": "Towards Sample-Efficiency and Generalization of Transfer and Inverse\n  Reinforcement Learning: A Comprehensive Literature Review", "author": "Hossein Hassani and Roozbeh Razavi-Far and Mehrdad Saif and Liang Lin", "abstract": "  Reinforcement learning (RL) is a sub-domain of machine learning, mainly\nconcerned with solving sequential decision-making problems by a learning agent\nthat interacts with the decision environment to improve its behavior through\nthe reward it receives from the environment. This learning paradigm is,\nhowever, well-known for being time-consuming due to the necessity of collecting\na large amount of data, making RL suffer from sample inefficiency and difficult\ngeneralization. Furthermore, the construction of an explicit reward function\nthat accounts for the trade-off between multiple desiderata of a decision\nproblem is often a laborious task. These challenges have been recently\naddressed utilizing transfer and inverse reinforcement learning (T-IRL). In\nthis regard, this paper is devoted to a comprehensive review of realizing the\nsample efficiency and generalization of RL algorithms through T-IRL. Following\na brief introduction to RL, the fundamental T-IRL methods are presented and the\nmost recent advancements in each research field have been extensively reviewed.\nOur findings denote that a majority of recent research works have dealt with\nthe aforementioned challenges by utilizing human-in-the-loop and sim-to-real\nstrategies for the efficient transfer of knowledge from source domains to the\ntarget domain under the transfer learning scheme. Under the IRL structure,\ntraining schemes that require a low number of experience transitions and\nextension of such frameworks to multi-agent and multi-intention problems have\nbeen the priority of researchers in recent years.\n", "link": "http://arxiv.org/abs/2411.10268v1", "date": "2024-11-15", "relevancy": 1.4137, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4774}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.471}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Sample-Efficiency%20and%20Generalization%20of%20Transfer%20and%20Inverse%0A%20%20Reinforcement%20Learning%3A%20A%20Comprehensive%20Literature%20Review&body=Title%3A%20Towards%20Sample-Efficiency%20and%20Generalization%20of%20Transfer%20and%20Inverse%0A%20%20Reinforcement%20Learning%3A%20A%20Comprehensive%20Literature%20Review%0AAuthor%3A%20Hossein%20Hassani%20and%20Roozbeh%20Razavi-Far%20and%20Mehrdad%20Saif%20and%20Liang%20Lin%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20is%20a%20sub-domain%20of%20machine%20learning%2C%20mainly%0Aconcerned%20with%20solving%20sequential%20decision-making%20problems%20by%20a%20learning%20agent%0Athat%20interacts%20with%20the%20decision%20environment%20to%20improve%20its%20behavior%20through%0Athe%20reward%20it%20receives%20from%20the%20environment.%20This%20learning%20paradigm%20is%2C%0Ahowever%2C%20well-known%20for%20being%20time-consuming%20due%20to%20the%20necessity%20of%20collecting%0Aa%20large%20amount%20of%20data%2C%20making%20RL%20suffer%20from%20sample%20inefficiency%20and%20difficult%0Ageneralization.%20Furthermore%2C%20the%20construction%20of%20an%20explicit%20reward%20function%0Athat%20accounts%20for%20the%20trade-off%20between%20multiple%20desiderata%20of%20a%20decision%0Aproblem%20is%20often%20a%20laborious%20task.%20These%20challenges%20have%20been%20recently%0Aaddressed%20utilizing%20transfer%20and%20inverse%20reinforcement%20learning%20%28T-IRL%29.%20In%0Athis%20regard%2C%20this%20paper%20is%20devoted%20to%20a%20comprehensive%20review%20of%20realizing%20the%0Asample%20efficiency%20and%20generalization%20of%20RL%20algorithms%20through%20T-IRL.%20Following%0Aa%20brief%20introduction%20to%20RL%2C%20the%20fundamental%20T-IRL%20methods%20are%20presented%20and%20the%0Amost%20recent%20advancements%20in%20each%20research%20field%20have%20been%20extensively%20reviewed.%0AOur%20findings%20denote%20that%20a%20majority%20of%20recent%20research%20works%20have%20dealt%20with%0Athe%20aforementioned%20challenges%20by%20utilizing%20human-in-the-loop%20and%20sim-to-real%0Astrategies%20for%20the%20efficient%20transfer%20of%20knowledge%20from%20source%20domains%20to%20the%0Atarget%20domain%20under%20the%20transfer%20learning%20scheme.%20Under%20the%20IRL%20structure%2C%0Atraining%20schemes%20that%20require%20a%20low%20number%20of%20experience%20transitions%20and%0Aextension%20of%20such%20frameworks%20to%20multi-agent%20and%20multi-intention%20problems%20have%0Abeen%20the%20priority%20of%20researchers%20in%20recent%20years.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10268v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Sample-Efficiency%2520and%2520Generalization%2520of%2520Transfer%2520and%2520Inverse%250A%2520%2520Reinforcement%2520Learning%253A%2520A%2520Comprehensive%2520Literature%2520Review%26entry.906535625%3DHossein%2520Hassani%2520and%2520Roozbeh%2520Razavi-Far%2520and%2520Mehrdad%2520Saif%2520and%2520Liang%2520Lin%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520is%2520a%2520sub-domain%2520of%2520machine%2520learning%252C%2520mainly%250Aconcerned%2520with%2520solving%2520sequential%2520decision-making%2520problems%2520by%2520a%2520learning%2520agent%250Athat%2520interacts%2520with%2520the%2520decision%2520environment%2520to%2520improve%2520its%2520behavior%2520through%250Athe%2520reward%2520it%2520receives%2520from%2520the%2520environment.%2520This%2520learning%2520paradigm%2520is%252C%250Ahowever%252C%2520well-known%2520for%2520being%2520time-consuming%2520due%2520to%2520the%2520necessity%2520of%2520collecting%250Aa%2520large%2520amount%2520of%2520data%252C%2520making%2520RL%2520suffer%2520from%2520sample%2520inefficiency%2520and%2520difficult%250Ageneralization.%2520Furthermore%252C%2520the%2520construction%2520of%2520an%2520explicit%2520reward%2520function%250Athat%2520accounts%2520for%2520the%2520trade-off%2520between%2520multiple%2520desiderata%2520of%2520a%2520decision%250Aproblem%2520is%2520often%2520a%2520laborious%2520task.%2520These%2520challenges%2520have%2520been%2520recently%250Aaddressed%2520utilizing%2520transfer%2520and%2520inverse%2520reinforcement%2520learning%2520%2528T-IRL%2529.%2520In%250Athis%2520regard%252C%2520this%2520paper%2520is%2520devoted%2520to%2520a%2520comprehensive%2520review%2520of%2520realizing%2520the%250Asample%2520efficiency%2520and%2520generalization%2520of%2520RL%2520algorithms%2520through%2520T-IRL.%2520Following%250Aa%2520brief%2520introduction%2520to%2520RL%252C%2520the%2520fundamental%2520T-IRL%2520methods%2520are%2520presented%2520and%2520the%250Amost%2520recent%2520advancements%2520in%2520each%2520research%2520field%2520have%2520been%2520extensively%2520reviewed.%250AOur%2520findings%2520denote%2520that%2520a%2520majority%2520of%2520recent%2520research%2520works%2520have%2520dealt%2520with%250Athe%2520aforementioned%2520challenges%2520by%2520utilizing%2520human-in-the-loop%2520and%2520sim-to-real%250Astrategies%2520for%2520the%2520efficient%2520transfer%2520of%2520knowledge%2520from%2520source%2520domains%2520to%2520the%250Atarget%2520domain%2520under%2520the%2520transfer%2520learning%2520scheme.%2520Under%2520the%2520IRL%2520structure%252C%250Atraining%2520schemes%2520that%2520require%2520a%2520low%2520number%2520of%2520experience%2520transitions%2520and%250Aextension%2520of%2520such%2520frameworks%2520to%2520multi-agent%2520and%2520multi-intention%2520problems%2520have%250Abeen%2520the%2520priority%2520of%2520researchers%2520in%2520recent%2520years.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10268v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Sample-Efficiency%20and%20Generalization%20of%20Transfer%20and%20Inverse%0A%20%20Reinforcement%20Learning%3A%20A%20Comprehensive%20Literature%20Review&entry.906535625=Hossein%20Hassani%20and%20Roozbeh%20Razavi-Far%20and%20Mehrdad%20Saif%20and%20Liang%20Lin&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20is%20a%20sub-domain%20of%20machine%20learning%2C%20mainly%0Aconcerned%20with%20solving%20sequential%20decision-making%20problems%20by%20a%20learning%20agent%0Athat%20interacts%20with%20the%20decision%20environment%20to%20improve%20its%20behavior%20through%0Athe%20reward%20it%20receives%20from%20the%20environment.%20This%20learning%20paradigm%20is%2C%0Ahowever%2C%20well-known%20for%20being%20time-consuming%20due%20to%20the%20necessity%20of%20collecting%0Aa%20large%20amount%20of%20data%2C%20making%20RL%20suffer%20from%20sample%20inefficiency%20and%20difficult%0Ageneralization.%20Furthermore%2C%20the%20construction%20of%20an%20explicit%20reward%20function%0Athat%20accounts%20for%20the%20trade-off%20between%20multiple%20desiderata%20of%20a%20decision%0Aproblem%20is%20often%20a%20laborious%20task.%20These%20challenges%20have%20been%20recently%0Aaddressed%20utilizing%20transfer%20and%20inverse%20reinforcement%20learning%20%28T-IRL%29.%20In%0Athis%20regard%2C%20this%20paper%20is%20devoted%20to%20a%20comprehensive%20review%20of%20realizing%20the%0Asample%20efficiency%20and%20generalization%20of%20RL%20algorithms%20through%20T-IRL.%20Following%0Aa%20brief%20introduction%20to%20RL%2C%20the%20fundamental%20T-IRL%20methods%20are%20presented%20and%20the%0Amost%20recent%20advancements%20in%20each%20research%20field%20have%20been%20extensively%20reviewed.%0AOur%20findings%20denote%20that%20a%20majority%20of%20recent%20research%20works%20have%20dealt%20with%0Athe%20aforementioned%20challenges%20by%20utilizing%20human-in-the-loop%20and%20sim-to-real%0Astrategies%20for%20the%20efficient%20transfer%20of%20knowledge%20from%20source%20domains%20to%20the%0Atarget%20domain%20under%20the%20transfer%20learning%20scheme.%20Under%20the%20IRL%20structure%2C%0Atraining%20schemes%20that%20require%20a%20low%20number%20of%20experience%20transitions%20and%0Aextension%20of%20such%20frameworks%20to%20multi-agent%20and%20multi-intention%20problems%20have%0Abeen%20the%20priority%20of%20researchers%20in%20recent%20years.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10268v1&entry.124074799=Read"},
{"title": "Uncertainty in Supply Chain Digital Twins: A Quantum-Classical Hybrid\n  Approach", "author": "Abdullah Abdullah and Fannya Ratana Sandjaja and Ayesha Abdul Majeed and Gyan Wickremasinghe and Karen Rafferty and Vishal Sharma", "abstract": "  This study investigates uncertainty quantification (UQ) using\nquantum-classical hybrid machine learning (ML) models for applications in\ncomplex and dynamic fields, such as attaining resiliency in supply chain\ndigital twins and financial risk assessment. Although quantum feature\ntransformations have been integrated into ML models for complex data tasks, a\ngap exists in determining their impact on UQ within their hybrid architectures\n(quantum-classical approach). This work applies existing UQ techniques for\ndifferent models within a hybrid framework, examining how quantum feature\ntransformation affects uncertainty propagation. Increasing qubits from 4 to 16\nshows varied model responsiveness to outlier detection (OD) samples, which is a\ncritical factor for resilient decision-making in dynamic environments. This\nwork shows how quantum computing techniques can transform data features for UQ,\nparticularly when combined with traditional methods.\n", "link": "http://arxiv.org/abs/2411.10254v1", "date": "2024-11-15", "relevancy": 1.4059, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4935}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4659}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty%20in%20Supply%20Chain%20Digital%20Twins%3A%20A%20Quantum-Classical%20Hybrid%0A%20%20Approach&body=Title%3A%20Uncertainty%20in%20Supply%20Chain%20Digital%20Twins%3A%20A%20Quantum-Classical%20Hybrid%0A%20%20Approach%0AAuthor%3A%20Abdullah%20Abdullah%20and%20Fannya%20Ratana%20Sandjaja%20and%20Ayesha%20Abdul%20Majeed%20and%20Gyan%20Wickremasinghe%20and%20Karen%20Rafferty%20and%20Vishal%20Sharma%0AAbstract%3A%20%20%20This%20study%20investigates%20uncertainty%20quantification%20%28UQ%29%20using%0Aquantum-classical%20hybrid%20machine%20learning%20%28ML%29%20models%20for%20applications%20in%0Acomplex%20and%20dynamic%20fields%2C%20such%20as%20attaining%20resiliency%20in%20supply%20chain%0Adigital%20twins%20and%20financial%20risk%20assessment.%20Although%20quantum%20feature%0Atransformations%20have%20been%20integrated%20into%20ML%20models%20for%20complex%20data%20tasks%2C%20a%0Agap%20exists%20in%20determining%20their%20impact%20on%20UQ%20within%20their%20hybrid%20architectures%0A%28quantum-classical%20approach%29.%20This%20work%20applies%20existing%20UQ%20techniques%20for%0Adifferent%20models%20within%20a%20hybrid%20framework%2C%20examining%20how%20quantum%20feature%0Atransformation%20affects%20uncertainty%20propagation.%20Increasing%20qubits%20from%204%20to%2016%0Ashows%20varied%20model%20responsiveness%20to%20outlier%20detection%20%28OD%29%20samples%2C%20which%20is%20a%0Acritical%20factor%20for%20resilient%20decision-making%20in%20dynamic%20environments.%20This%0Awork%20shows%20how%20quantum%20computing%20techniques%20can%20transform%20data%20features%20for%20UQ%2C%0Aparticularly%20when%20combined%20with%20traditional%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10254v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty%2520in%2520Supply%2520Chain%2520Digital%2520Twins%253A%2520A%2520Quantum-Classical%2520Hybrid%250A%2520%2520Approach%26entry.906535625%3DAbdullah%2520Abdullah%2520and%2520Fannya%2520Ratana%2520Sandjaja%2520and%2520Ayesha%2520Abdul%2520Majeed%2520and%2520Gyan%2520Wickremasinghe%2520and%2520Karen%2520Rafferty%2520and%2520Vishal%2520Sharma%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520uncertainty%2520quantification%2520%2528UQ%2529%2520using%250Aquantum-classical%2520hybrid%2520machine%2520learning%2520%2528ML%2529%2520models%2520for%2520applications%2520in%250Acomplex%2520and%2520dynamic%2520fields%252C%2520such%2520as%2520attaining%2520resiliency%2520in%2520supply%2520chain%250Adigital%2520twins%2520and%2520financial%2520risk%2520assessment.%2520Although%2520quantum%2520feature%250Atransformations%2520have%2520been%2520integrated%2520into%2520ML%2520models%2520for%2520complex%2520data%2520tasks%252C%2520a%250Agap%2520exists%2520in%2520determining%2520their%2520impact%2520on%2520UQ%2520within%2520their%2520hybrid%2520architectures%250A%2528quantum-classical%2520approach%2529.%2520This%2520work%2520applies%2520existing%2520UQ%2520techniques%2520for%250Adifferent%2520models%2520within%2520a%2520hybrid%2520framework%252C%2520examining%2520how%2520quantum%2520feature%250Atransformation%2520affects%2520uncertainty%2520propagation.%2520Increasing%2520qubits%2520from%25204%2520to%252016%250Ashows%2520varied%2520model%2520responsiveness%2520to%2520outlier%2520detection%2520%2528OD%2529%2520samples%252C%2520which%2520is%2520a%250Acritical%2520factor%2520for%2520resilient%2520decision-making%2520in%2520dynamic%2520environments.%2520This%250Awork%2520shows%2520how%2520quantum%2520computing%2520techniques%2520can%2520transform%2520data%2520features%2520for%2520UQ%252C%250Aparticularly%2520when%2520combined%2520with%2520traditional%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10254v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty%20in%20Supply%20Chain%20Digital%20Twins%3A%20A%20Quantum-Classical%20Hybrid%0A%20%20Approach&entry.906535625=Abdullah%20Abdullah%20and%20Fannya%20Ratana%20Sandjaja%20and%20Ayesha%20Abdul%20Majeed%20and%20Gyan%20Wickremasinghe%20and%20Karen%20Rafferty%20and%20Vishal%20Sharma&entry.1292438233=%20%20This%20study%20investigates%20uncertainty%20quantification%20%28UQ%29%20using%0Aquantum-classical%20hybrid%20machine%20learning%20%28ML%29%20models%20for%20applications%20in%0Acomplex%20and%20dynamic%20fields%2C%20such%20as%20attaining%20resiliency%20in%20supply%20chain%0Adigital%20twins%20and%20financial%20risk%20assessment.%20Although%20quantum%20feature%0Atransformations%20have%20been%20integrated%20into%20ML%20models%20for%20complex%20data%20tasks%2C%20a%0Agap%20exists%20in%20determining%20their%20impact%20on%20UQ%20within%20their%20hybrid%20architectures%0A%28quantum-classical%20approach%29.%20This%20work%20applies%20existing%20UQ%20techniques%20for%0Adifferent%20models%20within%20a%20hybrid%20framework%2C%20examining%20how%20quantum%20feature%0Atransformation%20affects%20uncertainty%20propagation.%20Increasing%20qubits%20from%204%20to%2016%0Ashows%20varied%20model%20responsiveness%20to%20outlier%20detection%20%28OD%29%20samples%2C%20which%20is%20a%0Acritical%20factor%20for%20resilient%20decision-making%20in%20dynamic%20environments.%20This%0Awork%20shows%20how%20quantum%20computing%20techniques%20can%20transform%20data%20features%20for%20UQ%2C%0Aparticularly%20when%20combined%20with%20traditional%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10254v1&entry.124074799=Read"},
{"title": "Learning rheological parameters of non-Newtonian fluids from velocimetry\n  data", "author": "Alexandros Kontogiannis and Richard Hodgkinson and Emily L. Manchester", "abstract": "  We solve a Bayesian inverse Navier-Stokes (N-S) problem that assimilates\nvelocimetry data in order to jointly reconstruct the flow field and learn the\nunknown N-S parameters. By incorporating a Carreau shear-thinning viscosity\nmodel into the N-S problem, we devise an algorithm that learns the most likely\nCarreau parameters of a shear-thinning fluid, and estimates their\nuncertainties, from velocimetry data alone. We then conduct a flow-MRI\nexperiment to obtain velocimetry data of an axisymmetric laminar jet through an\nidealised medical device (FDA nozzle) for a blood analogue fluid. We show that\nthe algorithm can successfully reconstruct the flow field by learning the most\nlikely Carreau parameters, and that the learned parameters are in very good\nagreement with rheometry measurements. The algorithm accepts any algebraic\neffective viscosity model, as long as the model is differentiable, and it can\nbe extended to more complicated non-Newtonian fluids (e.g. Oldroyd-B fluid) if\na viscoelastic model is incorporated into the N-S problem.\n", "link": "http://arxiv.org/abs/2408.02604v2", "date": "2024-11-15", "relevancy": 1.3926, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5008}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4771}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20rheological%20parameters%20of%20non-Newtonian%20fluids%20from%20velocimetry%0A%20%20data&body=Title%3A%20Learning%20rheological%20parameters%20of%20non-Newtonian%20fluids%20from%20velocimetry%0A%20%20data%0AAuthor%3A%20Alexandros%20Kontogiannis%20and%20Richard%20Hodgkinson%20and%20Emily%20L.%20Manchester%0AAbstract%3A%20%20%20We%20solve%20a%20Bayesian%20inverse%20Navier-Stokes%20%28N-S%29%20problem%20that%20assimilates%0Avelocimetry%20data%20in%20order%20to%20jointly%20reconstruct%20the%20flow%20field%20and%20learn%20the%0Aunknown%20N-S%20parameters.%20By%20incorporating%20a%20Carreau%20shear-thinning%20viscosity%0Amodel%20into%20the%20N-S%20problem%2C%20we%20devise%20an%20algorithm%20that%20learns%20the%20most%20likely%0ACarreau%20parameters%20of%20a%20shear-thinning%20fluid%2C%20and%20estimates%20their%0Auncertainties%2C%20from%20velocimetry%20data%20alone.%20We%20then%20conduct%20a%20flow-MRI%0Aexperiment%20to%20obtain%20velocimetry%20data%20of%20an%20axisymmetric%20laminar%20jet%20through%20an%0Aidealised%20medical%20device%20%28FDA%20nozzle%29%20for%20a%20blood%20analogue%20fluid.%20We%20show%20that%0Athe%20algorithm%20can%20successfully%20reconstruct%20the%20flow%20field%20by%20learning%20the%20most%0Alikely%20Carreau%20parameters%2C%20and%20that%20the%20learned%20parameters%20are%20in%20very%20good%0Aagreement%20with%20rheometry%20measurements.%20The%20algorithm%20accepts%20any%20algebraic%0Aeffective%20viscosity%20model%2C%20as%20long%20as%20the%20model%20is%20differentiable%2C%20and%20it%20can%0Abe%20extended%20to%20more%20complicated%20non-Newtonian%20fluids%20%28e.g.%20Oldroyd-B%20fluid%29%20if%0Aa%20viscoelastic%20model%20is%20incorporated%20into%20the%20N-S%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02604v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520rheological%2520parameters%2520of%2520non-Newtonian%2520fluids%2520from%2520velocimetry%250A%2520%2520data%26entry.906535625%3DAlexandros%2520Kontogiannis%2520and%2520Richard%2520Hodgkinson%2520and%2520Emily%2520L.%2520Manchester%26entry.1292438233%3D%2520%2520We%2520solve%2520a%2520Bayesian%2520inverse%2520Navier-Stokes%2520%2528N-S%2529%2520problem%2520that%2520assimilates%250Avelocimetry%2520data%2520in%2520order%2520to%2520jointly%2520reconstruct%2520the%2520flow%2520field%2520and%2520learn%2520the%250Aunknown%2520N-S%2520parameters.%2520By%2520incorporating%2520a%2520Carreau%2520shear-thinning%2520viscosity%250Amodel%2520into%2520the%2520N-S%2520problem%252C%2520we%2520devise%2520an%2520algorithm%2520that%2520learns%2520the%2520most%2520likely%250ACarreau%2520parameters%2520of%2520a%2520shear-thinning%2520fluid%252C%2520and%2520estimates%2520their%250Auncertainties%252C%2520from%2520velocimetry%2520data%2520alone.%2520We%2520then%2520conduct%2520a%2520flow-MRI%250Aexperiment%2520to%2520obtain%2520velocimetry%2520data%2520of%2520an%2520axisymmetric%2520laminar%2520jet%2520through%2520an%250Aidealised%2520medical%2520device%2520%2528FDA%2520nozzle%2529%2520for%2520a%2520blood%2520analogue%2520fluid.%2520We%2520show%2520that%250Athe%2520algorithm%2520can%2520successfully%2520reconstruct%2520the%2520flow%2520field%2520by%2520learning%2520the%2520most%250Alikely%2520Carreau%2520parameters%252C%2520and%2520that%2520the%2520learned%2520parameters%2520are%2520in%2520very%2520good%250Aagreement%2520with%2520rheometry%2520measurements.%2520The%2520algorithm%2520accepts%2520any%2520algebraic%250Aeffective%2520viscosity%2520model%252C%2520as%2520long%2520as%2520the%2520model%2520is%2520differentiable%252C%2520and%2520it%2520can%250Abe%2520extended%2520to%2520more%2520complicated%2520non-Newtonian%2520fluids%2520%2528e.g.%2520Oldroyd-B%2520fluid%2529%2520if%250Aa%2520viscoelastic%2520model%2520is%2520incorporated%2520into%2520the%2520N-S%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02604v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20rheological%20parameters%20of%20non-Newtonian%20fluids%20from%20velocimetry%0A%20%20data&entry.906535625=Alexandros%20Kontogiannis%20and%20Richard%20Hodgkinson%20and%20Emily%20L.%20Manchester&entry.1292438233=%20%20We%20solve%20a%20Bayesian%20inverse%20Navier-Stokes%20%28N-S%29%20problem%20that%20assimilates%0Avelocimetry%20data%20in%20order%20to%20jointly%20reconstruct%20the%20flow%20field%20and%20learn%20the%0Aunknown%20N-S%20parameters.%20By%20incorporating%20a%20Carreau%20shear-thinning%20viscosity%0Amodel%20into%20the%20N-S%20problem%2C%20we%20devise%20an%20algorithm%20that%20learns%20the%20most%20likely%0ACarreau%20parameters%20of%20a%20shear-thinning%20fluid%2C%20and%20estimates%20their%0Auncertainties%2C%20from%20velocimetry%20data%20alone.%20We%20then%20conduct%20a%20flow-MRI%0Aexperiment%20to%20obtain%20velocimetry%20data%20of%20an%20axisymmetric%20laminar%20jet%20through%20an%0Aidealised%20medical%20device%20%28FDA%20nozzle%29%20for%20a%20blood%20analogue%20fluid.%20We%20show%20that%0Athe%20algorithm%20can%20successfully%20reconstruct%20the%20flow%20field%20by%20learning%20the%20most%0Alikely%20Carreau%20parameters%2C%20and%20that%20the%20learned%20parameters%20are%20in%20very%20good%0Aagreement%20with%20rheometry%20measurements.%20The%20algorithm%20accepts%20any%20algebraic%0Aeffective%20viscosity%20model%2C%20as%20long%20as%20the%20model%20is%20differentiable%2C%20and%20it%20can%0Abe%20extended%20to%20more%20complicated%20non-Newtonian%20fluids%20%28e.g.%20Oldroyd-B%20fluid%29%20if%0Aa%20viscoelastic%20model%20is%20incorporated%20into%20the%20N-S%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02604v2&entry.124074799=Read"},
{"title": "The Spatial Complexity of Optical Computing and How to Reduce It", "author": "Yandong Li and Francesco Monticone", "abstract": "  Similar to algorithms, which consume time and memory to run, hardware\nrequires resources to function. For devices processing physical waves,\nimplementing operations needs sufficient \"space,\" as dictated by wave physics.\nHow much space is needed to perform a certain function is a fundamental\nquestion in optics, with recent research addressing it for given mathematical\noperations, but not for more general computing tasks, e.g., classification.\nInspired by computational complexity theory, we study the \"spatial complexity\"\nof optical computing systems in terms of scaling laws - specifically, how their\nphysical dimensions must scale as the dimension of the mathematical operation\nincreases - and propose a new paradigm for designing optical computing systems:\nspace-efficient neuromorphic optics, based on structural sparsity constraints\nand neural pruning methods motivated by wave physics (notably, the concept of\n\"overlapping nonlocality\"). On two mainstream platforms, free-space optics and\non-chip integrated photonics, our methods demonstrate substantial size\nreductions (to 1%-10% the size of conventional designs) with minimal compromise\non performance. Our theoretical and computational results reveal a trend of\ndiminishing returns on accuracy as structure dimensions increase, providing a\nnew perspective for interpreting and approaching the ultimate limits of optical\ncomputing - a balanced trade-off between device size and accuracy.\n", "link": "http://arxiv.org/abs/2411.10435v1", "date": "2024-11-15", "relevancy": 1.3909, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.468}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4635}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Spatial%20Complexity%20of%20Optical%20Computing%20and%20How%20to%20Reduce%20It&body=Title%3A%20The%20Spatial%20Complexity%20of%20Optical%20Computing%20and%20How%20to%20Reduce%20It%0AAuthor%3A%20Yandong%20Li%20and%20Francesco%20Monticone%0AAbstract%3A%20%20%20Similar%20to%20algorithms%2C%20which%20consume%20time%20and%20memory%20to%20run%2C%20hardware%0Arequires%20resources%20to%20function.%20For%20devices%20processing%20physical%20waves%2C%0Aimplementing%20operations%20needs%20sufficient%20%22space%2C%22%20as%20dictated%20by%20wave%20physics.%0AHow%20much%20space%20is%20needed%20to%20perform%20a%20certain%20function%20is%20a%20fundamental%0Aquestion%20in%20optics%2C%20with%20recent%20research%20addressing%20it%20for%20given%20mathematical%0Aoperations%2C%20but%20not%20for%20more%20general%20computing%20tasks%2C%20e.g.%2C%20classification.%0AInspired%20by%20computational%20complexity%20theory%2C%20we%20study%20the%20%22spatial%20complexity%22%0Aof%20optical%20computing%20systems%20in%20terms%20of%20scaling%20laws%20-%20specifically%2C%20how%20their%0Aphysical%20dimensions%20must%20scale%20as%20the%20dimension%20of%20the%20mathematical%20operation%0Aincreases%20-%20and%20propose%20a%20new%20paradigm%20for%20designing%20optical%20computing%20systems%3A%0Aspace-efficient%20neuromorphic%20optics%2C%20based%20on%20structural%20sparsity%20constraints%0Aand%20neural%20pruning%20methods%20motivated%20by%20wave%20physics%20%28notably%2C%20the%20concept%20of%0A%22overlapping%20nonlocality%22%29.%20On%20two%20mainstream%20platforms%2C%20free-space%20optics%20and%0Aon-chip%20integrated%20photonics%2C%20our%20methods%20demonstrate%20substantial%20size%0Areductions%20%28to%201%25-10%25%20the%20size%20of%20conventional%20designs%29%20with%20minimal%20compromise%0Aon%20performance.%20Our%20theoretical%20and%20computational%20results%20reveal%20a%20trend%20of%0Adiminishing%20returns%20on%20accuracy%20as%20structure%20dimensions%20increase%2C%20providing%20a%0Anew%20perspective%20for%20interpreting%20and%20approaching%20the%20ultimate%20limits%20of%20optical%0Acomputing%20-%20a%20balanced%20trade-off%20between%20device%20size%20and%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10435v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Spatial%2520Complexity%2520of%2520Optical%2520Computing%2520and%2520How%2520to%2520Reduce%2520It%26entry.906535625%3DYandong%2520Li%2520and%2520Francesco%2520Monticone%26entry.1292438233%3D%2520%2520Similar%2520to%2520algorithms%252C%2520which%2520consume%2520time%2520and%2520memory%2520to%2520run%252C%2520hardware%250Arequires%2520resources%2520to%2520function.%2520For%2520devices%2520processing%2520physical%2520waves%252C%250Aimplementing%2520operations%2520needs%2520sufficient%2520%2522space%252C%2522%2520as%2520dictated%2520by%2520wave%2520physics.%250AHow%2520much%2520space%2520is%2520needed%2520to%2520perform%2520a%2520certain%2520function%2520is%2520a%2520fundamental%250Aquestion%2520in%2520optics%252C%2520with%2520recent%2520research%2520addressing%2520it%2520for%2520given%2520mathematical%250Aoperations%252C%2520but%2520not%2520for%2520more%2520general%2520computing%2520tasks%252C%2520e.g.%252C%2520classification.%250AInspired%2520by%2520computational%2520complexity%2520theory%252C%2520we%2520study%2520the%2520%2522spatial%2520complexity%2522%250Aof%2520optical%2520computing%2520systems%2520in%2520terms%2520of%2520scaling%2520laws%2520-%2520specifically%252C%2520how%2520their%250Aphysical%2520dimensions%2520must%2520scale%2520as%2520the%2520dimension%2520of%2520the%2520mathematical%2520operation%250Aincreases%2520-%2520and%2520propose%2520a%2520new%2520paradigm%2520for%2520designing%2520optical%2520computing%2520systems%253A%250Aspace-efficient%2520neuromorphic%2520optics%252C%2520based%2520on%2520structural%2520sparsity%2520constraints%250Aand%2520neural%2520pruning%2520methods%2520motivated%2520by%2520wave%2520physics%2520%2528notably%252C%2520the%2520concept%2520of%250A%2522overlapping%2520nonlocality%2522%2529.%2520On%2520two%2520mainstream%2520platforms%252C%2520free-space%2520optics%2520and%250Aon-chip%2520integrated%2520photonics%252C%2520our%2520methods%2520demonstrate%2520substantial%2520size%250Areductions%2520%2528to%25201%2525-10%2525%2520the%2520size%2520of%2520conventional%2520designs%2529%2520with%2520minimal%2520compromise%250Aon%2520performance.%2520Our%2520theoretical%2520and%2520computational%2520results%2520reveal%2520a%2520trend%2520of%250Adiminishing%2520returns%2520on%2520accuracy%2520as%2520structure%2520dimensions%2520increase%252C%2520providing%2520a%250Anew%2520perspective%2520for%2520interpreting%2520and%2520approaching%2520the%2520ultimate%2520limits%2520of%2520optical%250Acomputing%2520-%2520a%2520balanced%2520trade-off%2520between%2520device%2520size%2520and%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10435v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Spatial%20Complexity%20of%20Optical%20Computing%20and%20How%20to%20Reduce%20It&entry.906535625=Yandong%20Li%20and%20Francesco%20Monticone&entry.1292438233=%20%20Similar%20to%20algorithms%2C%20which%20consume%20time%20and%20memory%20to%20run%2C%20hardware%0Arequires%20resources%20to%20function.%20For%20devices%20processing%20physical%20waves%2C%0Aimplementing%20operations%20needs%20sufficient%20%22space%2C%22%20as%20dictated%20by%20wave%20physics.%0AHow%20much%20space%20is%20needed%20to%20perform%20a%20certain%20function%20is%20a%20fundamental%0Aquestion%20in%20optics%2C%20with%20recent%20research%20addressing%20it%20for%20given%20mathematical%0Aoperations%2C%20but%20not%20for%20more%20general%20computing%20tasks%2C%20e.g.%2C%20classification.%0AInspired%20by%20computational%20complexity%20theory%2C%20we%20study%20the%20%22spatial%20complexity%22%0Aof%20optical%20computing%20systems%20in%20terms%20of%20scaling%20laws%20-%20specifically%2C%20how%20their%0Aphysical%20dimensions%20must%20scale%20as%20the%20dimension%20of%20the%20mathematical%20operation%0Aincreases%20-%20and%20propose%20a%20new%20paradigm%20for%20designing%20optical%20computing%20systems%3A%0Aspace-efficient%20neuromorphic%20optics%2C%20based%20on%20structural%20sparsity%20constraints%0Aand%20neural%20pruning%20methods%20motivated%20by%20wave%20physics%20%28notably%2C%20the%20concept%20of%0A%22overlapping%20nonlocality%22%29.%20On%20two%20mainstream%20platforms%2C%20free-space%20optics%20and%0Aon-chip%20integrated%20photonics%2C%20our%20methods%20demonstrate%20substantial%20size%0Areductions%20%28to%201%25-10%25%20the%20size%20of%20conventional%20designs%29%20with%20minimal%20compromise%0Aon%20performance.%20Our%20theoretical%20and%20computational%20results%20reveal%20a%20trend%20of%0Adiminishing%20returns%20on%20accuracy%20as%20structure%20dimensions%20increase%2C%20providing%20a%0Anew%20perspective%20for%20interpreting%20and%20approaching%20the%20ultimate%20limits%20of%20optical%0Acomputing%20-%20a%20balanced%20trade-off%20between%20device%20size%20and%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10435v1&entry.124074799=Read"},
{"title": "Domain Adaptation-based Edge Computing for Cross-Conditions Fault\n  Diagnosis", "author": "Yanzhi Wang and Chu Wang and Jinhong Wu and Ziyang Yu and Qi Zhou", "abstract": "  Fault diagnosis technology supports the healthy operation of mechanical\nequipment. However, the variations conditions during the operation of\nmechanical equipment lead to significant disparities in data distribution,\nposing challenges to fault diagnosis. Furthermore, when deploying applications,\ntraditional methods often encounter issues such as latency and data security.\nTherefore, conducting fault diagnosis and deploying application methods under\ncross-operating conditions holds significant value. This paper proposes a\ndomain adaptation-based lightweight fault diagnosis framework for edge\ncomputing scenarios. Incorporating the local maximum mean discrepancy into\nknowledge transfer aligns the feature distributions of different domains in a\nhigh-dimensional feature space, to discover a common feature space across\ndomains. The acquired fault diagnosis expertise from the cloud-model is\ntransferred to the lightweight edge-model using adaptation knowledge transfer\nmethods. While ensuring real-time diagnostic capabilities, accurate fault\ndiagnosis is achieved across working conditions. We conducted validation\nexperiments on the NVIDIA Jetson Xavier NX kit. In terms of diagnostic\nperformance, the proposed method significantly improved diagnostic accuracy,\nwith average increases of 34.44% and 17.33% compared to the comparison method,\nrespectively. Regarding lightweight effectiveness, proposed method achieved an\naverage inference speed increase of 80.47%. Additionally, compared to the\ncloud-model, the parameter count of the edge-model decreased by 96.37%, while\nthe Flops decreased by 83.08%.\n", "link": "http://arxiv.org/abs/2411.10340v1", "date": "2024-11-15", "relevancy": 1.3691, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5113}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4419}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain%20Adaptation-based%20Edge%20Computing%20for%20Cross-Conditions%20Fault%0A%20%20Diagnosis&body=Title%3A%20Domain%20Adaptation-based%20Edge%20Computing%20for%20Cross-Conditions%20Fault%0A%20%20Diagnosis%0AAuthor%3A%20Yanzhi%20Wang%20and%20Chu%20Wang%20and%20Jinhong%20Wu%20and%20Ziyang%20Yu%20and%20Qi%20Zhou%0AAbstract%3A%20%20%20Fault%20diagnosis%20technology%20supports%20the%20healthy%20operation%20of%20mechanical%0Aequipment.%20However%2C%20the%20variations%20conditions%20during%20the%20operation%20of%0Amechanical%20equipment%20lead%20to%20significant%20disparities%20in%20data%20distribution%2C%0Aposing%20challenges%20to%20fault%20diagnosis.%20Furthermore%2C%20when%20deploying%20applications%2C%0Atraditional%20methods%20often%20encounter%20issues%20such%20as%20latency%20and%20data%20security.%0ATherefore%2C%20conducting%20fault%20diagnosis%20and%20deploying%20application%20methods%20under%0Across-operating%20conditions%20holds%20significant%20value.%20This%20paper%20proposes%20a%0Adomain%20adaptation-based%20lightweight%20fault%20diagnosis%20framework%20for%20edge%0Acomputing%20scenarios.%20Incorporating%20the%20local%20maximum%20mean%20discrepancy%20into%0Aknowledge%20transfer%20aligns%20the%20feature%20distributions%20of%20different%20domains%20in%20a%0Ahigh-dimensional%20feature%20space%2C%20to%20discover%20a%20common%20feature%20space%20across%0Adomains.%20The%20acquired%20fault%20diagnosis%20expertise%20from%20the%20cloud-model%20is%0Atransferred%20to%20the%20lightweight%20edge-model%20using%20adaptation%20knowledge%20transfer%0Amethods.%20While%20ensuring%20real-time%20diagnostic%20capabilities%2C%20accurate%20fault%0Adiagnosis%20is%20achieved%20across%20working%20conditions.%20We%20conducted%20validation%0Aexperiments%20on%20the%20NVIDIA%20Jetson%20Xavier%20NX%20kit.%20In%20terms%20of%20diagnostic%0Aperformance%2C%20the%20proposed%20method%20significantly%20improved%20diagnostic%20accuracy%2C%0Awith%20average%20increases%20of%2034.44%25%20and%2017.33%25%20compared%20to%20the%20comparison%20method%2C%0Arespectively.%20Regarding%20lightweight%20effectiveness%2C%20proposed%20method%20achieved%20an%0Aaverage%20inference%20speed%20increase%20of%2080.47%25.%20Additionally%2C%20compared%20to%20the%0Acloud-model%2C%20the%20parameter%20count%20of%20the%20edge-model%20decreased%20by%2096.37%25%2C%20while%0Athe%20Flops%20decreased%20by%2083.08%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10340v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain%2520Adaptation-based%2520Edge%2520Computing%2520for%2520Cross-Conditions%2520Fault%250A%2520%2520Diagnosis%26entry.906535625%3DYanzhi%2520Wang%2520and%2520Chu%2520Wang%2520and%2520Jinhong%2520Wu%2520and%2520Ziyang%2520Yu%2520and%2520Qi%2520Zhou%26entry.1292438233%3D%2520%2520Fault%2520diagnosis%2520technology%2520supports%2520the%2520healthy%2520operation%2520of%2520mechanical%250Aequipment.%2520However%252C%2520the%2520variations%2520conditions%2520during%2520the%2520operation%2520of%250Amechanical%2520equipment%2520lead%2520to%2520significant%2520disparities%2520in%2520data%2520distribution%252C%250Aposing%2520challenges%2520to%2520fault%2520diagnosis.%2520Furthermore%252C%2520when%2520deploying%2520applications%252C%250Atraditional%2520methods%2520often%2520encounter%2520issues%2520such%2520as%2520latency%2520and%2520data%2520security.%250ATherefore%252C%2520conducting%2520fault%2520diagnosis%2520and%2520deploying%2520application%2520methods%2520under%250Across-operating%2520conditions%2520holds%2520significant%2520value.%2520This%2520paper%2520proposes%2520a%250Adomain%2520adaptation-based%2520lightweight%2520fault%2520diagnosis%2520framework%2520for%2520edge%250Acomputing%2520scenarios.%2520Incorporating%2520the%2520local%2520maximum%2520mean%2520discrepancy%2520into%250Aknowledge%2520transfer%2520aligns%2520the%2520feature%2520distributions%2520of%2520different%2520domains%2520in%2520a%250Ahigh-dimensional%2520feature%2520space%252C%2520to%2520discover%2520a%2520common%2520feature%2520space%2520across%250Adomains.%2520The%2520acquired%2520fault%2520diagnosis%2520expertise%2520from%2520the%2520cloud-model%2520is%250Atransferred%2520to%2520the%2520lightweight%2520edge-model%2520using%2520adaptation%2520knowledge%2520transfer%250Amethods.%2520While%2520ensuring%2520real-time%2520diagnostic%2520capabilities%252C%2520accurate%2520fault%250Adiagnosis%2520is%2520achieved%2520across%2520working%2520conditions.%2520We%2520conducted%2520validation%250Aexperiments%2520on%2520the%2520NVIDIA%2520Jetson%2520Xavier%2520NX%2520kit.%2520In%2520terms%2520of%2520diagnostic%250Aperformance%252C%2520the%2520proposed%2520method%2520significantly%2520improved%2520diagnostic%2520accuracy%252C%250Awith%2520average%2520increases%2520of%252034.44%2525%2520and%252017.33%2525%2520compared%2520to%2520the%2520comparison%2520method%252C%250Arespectively.%2520Regarding%2520lightweight%2520effectiveness%252C%2520proposed%2520method%2520achieved%2520an%250Aaverage%2520inference%2520speed%2520increase%2520of%252080.47%2525.%2520Additionally%252C%2520compared%2520to%2520the%250Acloud-model%252C%2520the%2520parameter%2520count%2520of%2520the%2520edge-model%2520decreased%2520by%252096.37%2525%252C%2520while%250Athe%2520Flops%2520decreased%2520by%252083.08%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10340v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain%20Adaptation-based%20Edge%20Computing%20for%20Cross-Conditions%20Fault%0A%20%20Diagnosis&entry.906535625=Yanzhi%20Wang%20and%20Chu%20Wang%20and%20Jinhong%20Wu%20and%20Ziyang%20Yu%20and%20Qi%20Zhou&entry.1292438233=%20%20Fault%20diagnosis%20technology%20supports%20the%20healthy%20operation%20of%20mechanical%0Aequipment.%20However%2C%20the%20variations%20conditions%20during%20the%20operation%20of%0Amechanical%20equipment%20lead%20to%20significant%20disparities%20in%20data%20distribution%2C%0Aposing%20challenges%20to%20fault%20diagnosis.%20Furthermore%2C%20when%20deploying%20applications%2C%0Atraditional%20methods%20often%20encounter%20issues%20such%20as%20latency%20and%20data%20security.%0ATherefore%2C%20conducting%20fault%20diagnosis%20and%20deploying%20application%20methods%20under%0Across-operating%20conditions%20holds%20significant%20value.%20This%20paper%20proposes%20a%0Adomain%20adaptation-based%20lightweight%20fault%20diagnosis%20framework%20for%20edge%0Acomputing%20scenarios.%20Incorporating%20the%20local%20maximum%20mean%20discrepancy%20into%0Aknowledge%20transfer%20aligns%20the%20feature%20distributions%20of%20different%20domains%20in%20a%0Ahigh-dimensional%20feature%20space%2C%20to%20discover%20a%20common%20feature%20space%20across%0Adomains.%20The%20acquired%20fault%20diagnosis%20expertise%20from%20the%20cloud-model%20is%0Atransferred%20to%20the%20lightweight%20edge-model%20using%20adaptation%20knowledge%20transfer%0Amethods.%20While%20ensuring%20real-time%20diagnostic%20capabilities%2C%20accurate%20fault%0Adiagnosis%20is%20achieved%20across%20working%20conditions.%20We%20conducted%20validation%0Aexperiments%20on%20the%20NVIDIA%20Jetson%20Xavier%20NX%20kit.%20In%20terms%20of%20diagnostic%0Aperformance%2C%20the%20proposed%20method%20significantly%20improved%20diagnostic%20accuracy%2C%0Awith%20average%20increases%20of%2034.44%25%20and%2017.33%25%20compared%20to%20the%20comparison%20method%2C%0Arespectively.%20Regarding%20lightweight%20effectiveness%2C%20proposed%20method%20achieved%20an%0Aaverage%20inference%20speed%20increase%20of%2080.47%25.%20Additionally%2C%20compared%20to%20the%0Acloud-model%2C%20the%20parameter%20count%20of%20the%20edge-model%20decreased%20by%2096.37%25%2C%20while%0Athe%20Flops%20decreased%20by%2083.08%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10340v1&entry.124074799=Read"},
{"title": "Towards Automatic Evaluation of Task-Oriented Dialogue Flows", "author": "Mehrnoosh Mirtaheri and Nikhil Varghese and Chandra Khatri and Amol Kelkar", "abstract": "  Task-oriented dialogue systems rely on predefined conversation schemes\n(dialogue flows) often represented as directed acyclic graphs. These flows can\nbe manually designed or automatically generated from previously recorded\nconversations. Due to variations in domain expertise or reliance on different\nsets of prior conversations, these dialogue flows can manifest in significantly\ndifferent graph structures. Despite their importance, there is no standard\nmethod for evaluating the quality of dialogue flows. We introduce FuDGE (Fuzzy\nDialogue-Graph Edit Distance), a novel metric that evaluates dialogue flows by\nassessing their structural complexity and representational coverage of the\nconversation data. FuDGE measures how well individual conversations align with\na flow and, consequently, how well a set of conversations is represented by the\nflow overall. Through extensive experiments on manually configured flows and\nflows generated by automated techniques, we demonstrate the effectiveness of\nFuDGE and its evaluation framework. By standardizing and optimizing dialogue\nflows, FuDGE enables conversational designers and automated techniques to\nachieve higher levels of efficiency and automation.\n", "link": "http://arxiv.org/abs/2411.10416v1", "date": "2024-11-15", "relevancy": 1.3478, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4815}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4627}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Automatic%20Evaluation%20of%20Task-Oriented%20Dialogue%20Flows&body=Title%3A%20Towards%20Automatic%20Evaluation%20of%20Task-Oriented%20Dialogue%20Flows%0AAuthor%3A%20Mehrnoosh%20Mirtaheri%20and%20Nikhil%20Varghese%20and%20Chandra%20Khatri%20and%20Amol%20Kelkar%0AAbstract%3A%20%20%20Task-oriented%20dialogue%20systems%20rely%20on%20predefined%20conversation%20schemes%0A%28dialogue%20flows%29%20often%20represented%20as%20directed%20acyclic%20graphs.%20These%20flows%20can%0Abe%20manually%20designed%20or%20automatically%20generated%20from%20previously%20recorded%0Aconversations.%20Due%20to%20variations%20in%20domain%20expertise%20or%20reliance%20on%20different%0Asets%20of%20prior%20conversations%2C%20these%20dialogue%20flows%20can%20manifest%20in%20significantly%0Adifferent%20graph%20structures.%20Despite%20their%20importance%2C%20there%20is%20no%20standard%0Amethod%20for%20evaluating%20the%20quality%20of%20dialogue%20flows.%20We%20introduce%20FuDGE%20%28Fuzzy%0ADialogue-Graph%20Edit%20Distance%29%2C%20a%20novel%20metric%20that%20evaluates%20dialogue%20flows%20by%0Aassessing%20their%20structural%20complexity%20and%20representational%20coverage%20of%20the%0Aconversation%20data.%20FuDGE%20measures%20how%20well%20individual%20conversations%20align%20with%0Aa%20flow%20and%2C%20consequently%2C%20how%20well%20a%20set%20of%20conversations%20is%20represented%20by%20the%0Aflow%20overall.%20Through%20extensive%20experiments%20on%20manually%20configured%20flows%20and%0Aflows%20generated%20by%20automated%20techniques%2C%20we%20demonstrate%20the%20effectiveness%20of%0AFuDGE%20and%20its%20evaluation%20framework.%20By%20standardizing%20and%20optimizing%20dialogue%0Aflows%2C%20FuDGE%20enables%20conversational%20designers%20and%20automated%20techniques%20to%0Aachieve%20higher%20levels%20of%20efficiency%20and%20automation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10416v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Automatic%2520Evaluation%2520of%2520Task-Oriented%2520Dialogue%2520Flows%26entry.906535625%3DMehrnoosh%2520Mirtaheri%2520and%2520Nikhil%2520Varghese%2520and%2520Chandra%2520Khatri%2520and%2520Amol%2520Kelkar%26entry.1292438233%3D%2520%2520Task-oriented%2520dialogue%2520systems%2520rely%2520on%2520predefined%2520conversation%2520schemes%250A%2528dialogue%2520flows%2529%2520often%2520represented%2520as%2520directed%2520acyclic%2520graphs.%2520These%2520flows%2520can%250Abe%2520manually%2520designed%2520or%2520automatically%2520generated%2520from%2520previously%2520recorded%250Aconversations.%2520Due%2520to%2520variations%2520in%2520domain%2520expertise%2520or%2520reliance%2520on%2520different%250Asets%2520of%2520prior%2520conversations%252C%2520these%2520dialogue%2520flows%2520can%2520manifest%2520in%2520significantly%250Adifferent%2520graph%2520structures.%2520Despite%2520their%2520importance%252C%2520there%2520is%2520no%2520standard%250Amethod%2520for%2520evaluating%2520the%2520quality%2520of%2520dialogue%2520flows.%2520We%2520introduce%2520FuDGE%2520%2528Fuzzy%250ADialogue-Graph%2520Edit%2520Distance%2529%252C%2520a%2520novel%2520metric%2520that%2520evaluates%2520dialogue%2520flows%2520by%250Aassessing%2520their%2520structural%2520complexity%2520and%2520representational%2520coverage%2520of%2520the%250Aconversation%2520data.%2520FuDGE%2520measures%2520how%2520well%2520individual%2520conversations%2520align%2520with%250Aa%2520flow%2520and%252C%2520consequently%252C%2520how%2520well%2520a%2520set%2520of%2520conversations%2520is%2520represented%2520by%2520the%250Aflow%2520overall.%2520Through%2520extensive%2520experiments%2520on%2520manually%2520configured%2520flows%2520and%250Aflows%2520generated%2520by%2520automated%2520techniques%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%250AFuDGE%2520and%2520its%2520evaluation%2520framework.%2520By%2520standardizing%2520and%2520optimizing%2520dialogue%250Aflows%252C%2520FuDGE%2520enables%2520conversational%2520designers%2520and%2520automated%2520techniques%2520to%250Aachieve%2520higher%2520levels%2520of%2520efficiency%2520and%2520automation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10416v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Automatic%20Evaluation%20of%20Task-Oriented%20Dialogue%20Flows&entry.906535625=Mehrnoosh%20Mirtaheri%20and%20Nikhil%20Varghese%20and%20Chandra%20Khatri%20and%20Amol%20Kelkar&entry.1292438233=%20%20Task-oriented%20dialogue%20systems%20rely%20on%20predefined%20conversation%20schemes%0A%28dialogue%20flows%29%20often%20represented%20as%20directed%20acyclic%20graphs.%20These%20flows%20can%0Abe%20manually%20designed%20or%20automatically%20generated%20from%20previously%20recorded%0Aconversations.%20Due%20to%20variations%20in%20domain%20expertise%20or%20reliance%20on%20different%0Asets%20of%20prior%20conversations%2C%20these%20dialogue%20flows%20can%20manifest%20in%20significantly%0Adifferent%20graph%20structures.%20Despite%20their%20importance%2C%20there%20is%20no%20standard%0Amethod%20for%20evaluating%20the%20quality%20of%20dialogue%20flows.%20We%20introduce%20FuDGE%20%28Fuzzy%0ADialogue-Graph%20Edit%20Distance%29%2C%20a%20novel%20metric%20that%20evaluates%20dialogue%20flows%20by%0Aassessing%20their%20structural%20complexity%20and%20representational%20coverage%20of%20the%0Aconversation%20data.%20FuDGE%20measures%20how%20well%20individual%20conversations%20align%20with%0Aa%20flow%20and%2C%20consequently%2C%20how%20well%20a%20set%20of%20conversations%20is%20represented%20by%20the%0Aflow%20overall.%20Through%20extensive%20experiments%20on%20manually%20configured%20flows%20and%0Aflows%20generated%20by%20automated%20techniques%2C%20we%20demonstrate%20the%20effectiveness%20of%0AFuDGE%20and%20its%20evaluation%20framework.%20By%20standardizing%20and%20optimizing%20dialogue%0Aflows%2C%20FuDGE%20enables%20conversational%20designers%20and%20automated%20techniques%20to%0Aachieve%20higher%20levels%20of%20efficiency%20and%20automation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10416v1&entry.124074799=Read"},
{"title": "ColorEdit: Training-free Image-Guided Color editing with diffusion model", "author": "Xingxi Yin and Zhi Li and Jingfeng Zhang and Chenglin Li and Yin Zhang", "abstract": "  Text-to-image (T2I) diffusion models, with their impressive generative\ncapabilities, have been adopted for image editing tasks, demonstrating\nremarkable efficacy. However, due to attention leakage and collision between\nthe cross-attention map of the object and the new color attribute from the text\nprompt, text-guided image editing methods may fail to change the color of an\nobject, resulting in a misalignment between the resulting image and the text\nprompt. In this paper, we conduct an in-depth analysis on the process of\ntext-guided image synthesizing and what semantic information different\ncross-attention blocks have learned. We observe that the visual representation\nof an object is determined in the up-block of the diffusion model in the early\nstage of the denoising process, and color adjustment can be achieved through\nvalue matrices alignment in the cross-attention layer. Based on our findings,\nwe propose a straightforward, yet stable, and effective image-guided method to\nmodify the color of an object without requiring any additional fine-tuning or\ntraining. Lastly, we present a benchmark dataset called COLORBENCH, the first\nbenchmark to evaluate the performance of color change methods. Extensive\nexperiments validate the effectiveness of our method in object-level color\nediting and surpass the performance of popular text-guided image editing\napproaches in both synthesized and real images.\n", "link": "http://arxiv.org/abs/2411.10232v1", "date": "2024-11-15", "relevancy": 1.2779, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.672}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.623}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6217}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ColorEdit%3A%20Training-free%20Image-Guided%20Color%20editing%20with%20diffusion%20model&body=Title%3A%20ColorEdit%3A%20Training-free%20Image-Guided%20Color%20editing%20with%20diffusion%20model%0AAuthor%3A%20Xingxi%20Yin%20and%20Zhi%20Li%20and%20Jingfeng%20Zhang%20and%20Chenglin%20Li%20and%20Yin%20Zhang%0AAbstract%3A%20%20%20Text-to-image%20%28T2I%29%20diffusion%20models%2C%20with%20their%20impressive%20generative%0Acapabilities%2C%20have%20been%20adopted%20for%20image%20editing%20tasks%2C%20demonstrating%0Aremarkable%20efficacy.%20However%2C%20due%20to%20attention%20leakage%20and%20collision%20between%0Athe%20cross-attention%20map%20of%20the%20object%20and%20the%20new%20color%20attribute%20from%20the%20text%0Aprompt%2C%20text-guided%20image%20editing%20methods%20may%20fail%20to%20change%20the%20color%20of%20an%0Aobject%2C%20resulting%20in%20a%20misalignment%20between%20the%20resulting%20image%20and%20the%20text%0Aprompt.%20In%20this%20paper%2C%20we%20conduct%20an%20in-depth%20analysis%20on%20the%20process%20of%0Atext-guided%20image%20synthesizing%20and%20what%20semantic%20information%20different%0Across-attention%20blocks%20have%20learned.%20We%20observe%20that%20the%20visual%20representation%0Aof%20an%20object%20is%20determined%20in%20the%20up-block%20of%20the%20diffusion%20model%20in%20the%20early%0Astage%20of%20the%20denoising%20process%2C%20and%20color%20adjustment%20can%20be%20achieved%20through%0Avalue%20matrices%20alignment%20in%20the%20cross-attention%20layer.%20Based%20on%20our%20findings%2C%0Awe%20propose%20a%20straightforward%2C%20yet%20stable%2C%20and%20effective%20image-guided%20method%20to%0Amodify%20the%20color%20of%20an%20object%20without%20requiring%20any%20additional%20fine-tuning%20or%0Atraining.%20Lastly%2C%20we%20present%20a%20benchmark%20dataset%20called%20COLORBENCH%2C%20the%20first%0Abenchmark%20to%20evaluate%20the%20performance%20of%20color%20change%20methods.%20Extensive%0Aexperiments%20validate%20the%20effectiveness%20of%20our%20method%20in%20object-level%20color%0Aediting%20and%20surpass%20the%20performance%20of%20popular%20text-guided%20image%20editing%0Aapproaches%20in%20both%20synthesized%20and%20real%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10232v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DColorEdit%253A%2520Training-free%2520Image-Guided%2520Color%2520editing%2520with%2520diffusion%2520model%26entry.906535625%3DXingxi%2520Yin%2520and%2520Zhi%2520Li%2520and%2520Jingfeng%2520Zhang%2520and%2520Chenglin%2520Li%2520and%2520Yin%2520Zhang%26entry.1292438233%3D%2520%2520Text-to-image%2520%2528T2I%2529%2520diffusion%2520models%252C%2520with%2520their%2520impressive%2520generative%250Acapabilities%252C%2520have%2520been%2520adopted%2520for%2520image%2520editing%2520tasks%252C%2520demonstrating%250Aremarkable%2520efficacy.%2520However%252C%2520due%2520to%2520attention%2520leakage%2520and%2520collision%2520between%250Athe%2520cross-attention%2520map%2520of%2520the%2520object%2520and%2520the%2520new%2520color%2520attribute%2520from%2520the%2520text%250Aprompt%252C%2520text-guided%2520image%2520editing%2520methods%2520may%2520fail%2520to%2520change%2520the%2520color%2520of%2520an%250Aobject%252C%2520resulting%2520in%2520a%2520misalignment%2520between%2520the%2520resulting%2520image%2520and%2520the%2520text%250Aprompt.%2520In%2520this%2520paper%252C%2520we%2520conduct%2520an%2520in-depth%2520analysis%2520on%2520the%2520process%2520of%250Atext-guided%2520image%2520synthesizing%2520and%2520what%2520semantic%2520information%2520different%250Across-attention%2520blocks%2520have%2520learned.%2520We%2520observe%2520that%2520the%2520visual%2520representation%250Aof%2520an%2520object%2520is%2520determined%2520in%2520the%2520up-block%2520of%2520the%2520diffusion%2520model%2520in%2520the%2520early%250Astage%2520of%2520the%2520denoising%2520process%252C%2520and%2520color%2520adjustment%2520can%2520be%2520achieved%2520through%250Avalue%2520matrices%2520alignment%2520in%2520the%2520cross-attention%2520layer.%2520Based%2520on%2520our%2520findings%252C%250Awe%2520propose%2520a%2520straightforward%252C%2520yet%2520stable%252C%2520and%2520effective%2520image-guided%2520method%2520to%250Amodify%2520the%2520color%2520of%2520an%2520object%2520without%2520requiring%2520any%2520additional%2520fine-tuning%2520or%250Atraining.%2520Lastly%252C%2520we%2520present%2520a%2520benchmark%2520dataset%2520called%2520COLORBENCH%252C%2520the%2520first%250Abenchmark%2520to%2520evaluate%2520the%2520performance%2520of%2520color%2520change%2520methods.%2520Extensive%250Aexperiments%2520validate%2520the%2520effectiveness%2520of%2520our%2520method%2520in%2520object-level%2520color%250Aediting%2520and%2520surpass%2520the%2520performance%2520of%2520popular%2520text-guided%2520image%2520editing%250Aapproaches%2520in%2520both%2520synthesized%2520and%2520real%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10232v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ColorEdit%3A%20Training-free%20Image-Guided%20Color%20editing%20with%20diffusion%20model&entry.906535625=Xingxi%20Yin%20and%20Zhi%20Li%20and%20Jingfeng%20Zhang%20and%20Chenglin%20Li%20and%20Yin%20Zhang&entry.1292438233=%20%20Text-to-image%20%28T2I%29%20diffusion%20models%2C%20with%20their%20impressive%20generative%0Acapabilities%2C%20have%20been%20adopted%20for%20image%20editing%20tasks%2C%20demonstrating%0Aremarkable%20efficacy.%20However%2C%20due%20to%20attention%20leakage%20and%20collision%20between%0Athe%20cross-attention%20map%20of%20the%20object%20and%20the%20new%20color%20attribute%20from%20the%20text%0Aprompt%2C%20text-guided%20image%20editing%20methods%20may%20fail%20to%20change%20the%20color%20of%20an%0Aobject%2C%20resulting%20in%20a%20misalignment%20between%20the%20resulting%20image%20and%20the%20text%0Aprompt.%20In%20this%20paper%2C%20we%20conduct%20an%20in-depth%20analysis%20on%20the%20process%20of%0Atext-guided%20image%20synthesizing%20and%20what%20semantic%20information%20different%0Across-attention%20blocks%20have%20learned.%20We%20observe%20that%20the%20visual%20representation%0Aof%20an%20object%20is%20determined%20in%20the%20up-block%20of%20the%20diffusion%20model%20in%20the%20early%0Astage%20of%20the%20denoising%20process%2C%20and%20color%20adjustment%20can%20be%20achieved%20through%0Avalue%20matrices%20alignment%20in%20the%20cross-attention%20layer.%20Based%20on%20our%20findings%2C%0Awe%20propose%20a%20straightforward%2C%20yet%20stable%2C%20and%20effective%20image-guided%20method%20to%0Amodify%20the%20color%20of%20an%20object%20without%20requiring%20any%20additional%20fine-tuning%20or%0Atraining.%20Lastly%2C%20we%20present%20a%20benchmark%20dataset%20called%20COLORBENCH%2C%20the%20first%0Abenchmark%20to%20evaluate%20the%20performance%20of%20color%20change%20methods.%20Extensive%0Aexperiments%20validate%20the%20effectiveness%20of%20our%20method%20in%20object-level%20color%0Aediting%20and%20surpass%20the%20performance%20of%20popular%20text-guided%20image%20editing%0Aapproaches%20in%20both%20synthesized%20and%20real%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10232v1&entry.124074799=Read"},
{"title": "A Realistic Collimated X-Ray Image Simulation Pipeline", "author": "Benjamin El-Zein and Dominik Eckert and Thomas Weber and Maximilian Rohleder and Ludwig Ritschl and Steffen Kappler and Andreas Maier", "abstract": "  Collimator detection remains a challenging task in X-ray systems with\nunreliable or non-available information about the detectors position relative\nto the source. This paper presents a physically motivated image processing\npipeline for simulating the characteristics of collimator shadows in X-ray\nimages. By generating randomized labels for collimator shapes and locations,\nincorporating scattered radiation simulation, and including Poisson noise, the\npipeline enables the expansion of limited datasets for training deep neural\nnetworks. We validate the proposed pipeline by a qualitative and quantitative\ncomparison against real collimator shadows. Furthermore, it is demonstrated\nthat utilizing simulated data within our deep learning framework not only\nserves as a suitable substitute for actual collimators but also enhances the\ngeneralization performance when applied to real-world data.\n", "link": "http://arxiv.org/abs/2411.10308v1", "date": "2024-11-15", "relevancy": 0.9849, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5189}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4835}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Realistic%20Collimated%20X-Ray%20Image%20Simulation%20Pipeline&body=Title%3A%20A%20Realistic%20Collimated%20X-Ray%20Image%20Simulation%20Pipeline%0AAuthor%3A%20Benjamin%20El-Zein%20and%20Dominik%20Eckert%20and%20Thomas%20Weber%20and%20Maximilian%20Rohleder%20and%20Ludwig%20Ritschl%20and%20Steffen%20Kappler%20and%20Andreas%20Maier%0AAbstract%3A%20%20%20Collimator%20detection%20remains%20a%20challenging%20task%20in%20X-ray%20systems%20with%0Aunreliable%20or%20non-available%20information%20about%20the%20detectors%20position%20relative%0Ato%20the%20source.%20This%20paper%20presents%20a%20physically%20motivated%20image%20processing%0Apipeline%20for%20simulating%20the%20characteristics%20of%20collimator%20shadows%20in%20X-ray%0Aimages.%20By%20generating%20randomized%20labels%20for%20collimator%20shapes%20and%20locations%2C%0Aincorporating%20scattered%20radiation%20simulation%2C%20and%20including%20Poisson%20noise%2C%20the%0Apipeline%20enables%20the%20expansion%20of%20limited%20datasets%20for%20training%20deep%20neural%0Anetworks.%20We%20validate%20the%20proposed%20pipeline%20by%20a%20qualitative%20and%20quantitative%0Acomparison%20against%20real%20collimator%20shadows.%20Furthermore%2C%20it%20is%20demonstrated%0Athat%20utilizing%20simulated%20data%20within%20our%20deep%20learning%20framework%20not%20only%0Aserves%20as%20a%20suitable%20substitute%20for%20actual%20collimators%20but%20also%20enhances%20the%0Ageneralization%20performance%20when%20applied%20to%20real-world%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10308v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Realistic%2520Collimated%2520X-Ray%2520Image%2520Simulation%2520Pipeline%26entry.906535625%3DBenjamin%2520El-Zein%2520and%2520Dominik%2520Eckert%2520and%2520Thomas%2520Weber%2520and%2520Maximilian%2520Rohleder%2520and%2520Ludwig%2520Ritschl%2520and%2520Steffen%2520Kappler%2520and%2520Andreas%2520Maier%26entry.1292438233%3D%2520%2520Collimator%2520detection%2520remains%2520a%2520challenging%2520task%2520in%2520X-ray%2520systems%2520with%250Aunreliable%2520or%2520non-available%2520information%2520about%2520the%2520detectors%2520position%2520relative%250Ato%2520the%2520source.%2520This%2520paper%2520presents%2520a%2520physically%2520motivated%2520image%2520processing%250Apipeline%2520for%2520simulating%2520the%2520characteristics%2520of%2520collimator%2520shadows%2520in%2520X-ray%250Aimages.%2520By%2520generating%2520randomized%2520labels%2520for%2520collimator%2520shapes%2520and%2520locations%252C%250Aincorporating%2520scattered%2520radiation%2520simulation%252C%2520and%2520including%2520Poisson%2520noise%252C%2520the%250Apipeline%2520enables%2520the%2520expansion%2520of%2520limited%2520datasets%2520for%2520training%2520deep%2520neural%250Anetworks.%2520We%2520validate%2520the%2520proposed%2520pipeline%2520by%2520a%2520qualitative%2520and%2520quantitative%250Acomparison%2520against%2520real%2520collimator%2520shadows.%2520Furthermore%252C%2520it%2520is%2520demonstrated%250Athat%2520utilizing%2520simulated%2520data%2520within%2520our%2520deep%2520learning%2520framework%2520not%2520only%250Aserves%2520as%2520a%2520suitable%2520substitute%2520for%2520actual%2520collimators%2520but%2520also%2520enhances%2520the%250Ageneralization%2520performance%2520when%2520applied%2520to%2520real-world%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10308v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Realistic%20Collimated%20X-Ray%20Image%20Simulation%20Pipeline&entry.906535625=Benjamin%20El-Zein%20and%20Dominik%20Eckert%20and%20Thomas%20Weber%20and%20Maximilian%20Rohleder%20and%20Ludwig%20Ritschl%20and%20Steffen%20Kappler%20and%20Andreas%20Maier&entry.1292438233=%20%20Collimator%20detection%20remains%20a%20challenging%20task%20in%20X-ray%20systems%20with%0Aunreliable%20or%20non-available%20information%20about%20the%20detectors%20position%20relative%0Ato%20the%20source.%20This%20paper%20presents%20a%20physically%20motivated%20image%20processing%0Apipeline%20for%20simulating%20the%20characteristics%20of%20collimator%20shadows%20in%20X-ray%0Aimages.%20By%20generating%20randomized%20labels%20for%20collimator%20shapes%20and%20locations%2C%0Aincorporating%20scattered%20radiation%20simulation%2C%20and%20including%20Poisson%20noise%2C%20the%0Apipeline%20enables%20the%20expansion%20of%20limited%20datasets%20for%20training%20deep%20neural%0Anetworks.%20We%20validate%20the%20proposed%20pipeline%20by%20a%20qualitative%20and%20quantitative%0Acomparison%20against%20real%20collimator%20shadows.%20Furthermore%2C%20it%20is%20demonstrated%0Athat%20utilizing%20simulated%20data%20within%20our%20deep%20learning%20framework%20not%20only%0Aserves%20as%20a%20suitable%20substitute%20for%20actual%20collimators%20but%20also%20enhances%20the%0Ageneralization%20performance%20when%20applied%20to%20real-world%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10308v1&entry.124074799=Read"},
{"title": "Risk Sources and Risk Management Measures in Support of Standards for\n  General-Purpose AI Systems", "author": "Rokas Gipi\u0161kis and Ayrton San Joaquin and Ze Shen Chin and Adrian Regenfu\u00df and Ariel Gil and Koen Holtman", "abstract": "  There is an urgent need to identify both short and long-term risks from newly\nemerging types of Artificial Intelligence (AI), as well as available risk\nmanagement measures. In response, and to support global efforts in regulating\nAI and writing safety standards, we compile an extensive catalog of risk\nsources and risk management measures for general-purpose AI (GPAI) systems,\ncomplete with descriptions and supporting examples where relevant. This work\ninvolves identifying technical, operational, and societal risks across model\ndevelopment, training, and deployment stages, as well as surveying established\nand experimental methods for managing these risks. To the best of our\nknowledge, this paper is the first of its kind to provide extensive\ndocumentation of both GPAI risk sources and risk management measures that are\ndescriptive, self-contained and neutral with respect to any existing regulatory\nframework. This work intends to help AI providers, standards experts,\nresearchers, policymakers, and regulators in identifying and mitigating\nsystemic risks from GPAI systems. For this reason, the catalog is released\nunder a public domain license for ease of direct use by stakeholders in AI\ngovernance and standards.\n", "link": "http://arxiv.org/abs/2410.23472v2", "date": "2024-11-15", "relevancy": 1.2843, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4338}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4335}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Risk%20Sources%20and%20Risk%20Management%20Measures%20in%20Support%20of%20Standards%20for%0A%20%20General-Purpose%20AI%20Systems&body=Title%3A%20Risk%20Sources%20and%20Risk%20Management%20Measures%20in%20Support%20of%20Standards%20for%0A%20%20General-Purpose%20AI%20Systems%0AAuthor%3A%20Rokas%20Gipi%C5%A1kis%20and%20Ayrton%20San%20Joaquin%20and%20Ze%20Shen%20Chin%20and%20Adrian%20Regenfu%C3%9F%20and%20Ariel%20Gil%20and%20Koen%20Holtman%0AAbstract%3A%20%20%20There%20is%20an%20urgent%20need%20to%20identify%20both%20short%20and%20long-term%20risks%20from%20newly%0Aemerging%20types%20of%20Artificial%20Intelligence%20%28AI%29%2C%20as%20well%20as%20available%20risk%0Amanagement%20measures.%20In%20response%2C%20and%20to%20support%20global%20efforts%20in%20regulating%0AAI%20and%20writing%20safety%20standards%2C%20we%20compile%20an%20extensive%20catalog%20of%20risk%0Asources%20and%20risk%20management%20measures%20for%20general-purpose%20AI%20%28GPAI%29%20systems%2C%0Acomplete%20with%20descriptions%20and%20supporting%20examples%20where%20relevant.%20This%20work%0Ainvolves%20identifying%20technical%2C%20operational%2C%20and%20societal%20risks%20across%20model%0Adevelopment%2C%20training%2C%20and%20deployment%20stages%2C%20as%20well%20as%20surveying%20established%0Aand%20experimental%20methods%20for%20managing%20these%20risks.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20paper%20is%20the%20first%20of%20its%20kind%20to%20provide%20extensive%0Adocumentation%20of%20both%20GPAI%20risk%20sources%20and%20risk%20management%20measures%20that%20are%0Adescriptive%2C%20self-contained%20and%20neutral%20with%20respect%20to%20any%20existing%20regulatory%0Aframework.%20This%20work%20intends%20to%20help%20AI%20providers%2C%20standards%20experts%2C%0Aresearchers%2C%20policymakers%2C%20and%20regulators%20in%20identifying%20and%20mitigating%0Asystemic%20risks%20from%20GPAI%20systems.%20For%20this%20reason%2C%20the%20catalog%20is%20released%0Aunder%20a%20public%20domain%20license%20for%20ease%20of%20direct%20use%20by%20stakeholders%20in%20AI%0Agovernance%20and%20standards.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23472v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRisk%2520Sources%2520and%2520Risk%2520Management%2520Measures%2520in%2520Support%2520of%2520Standards%2520for%250A%2520%2520General-Purpose%2520AI%2520Systems%26entry.906535625%3DRokas%2520Gipi%25C5%25A1kis%2520and%2520Ayrton%2520San%2520Joaquin%2520and%2520Ze%2520Shen%2520Chin%2520and%2520Adrian%2520Regenfu%25C3%259F%2520and%2520Ariel%2520Gil%2520and%2520Koen%2520Holtman%26entry.1292438233%3D%2520%2520There%2520is%2520an%2520urgent%2520need%2520to%2520identify%2520both%2520short%2520and%2520long-term%2520risks%2520from%2520newly%250Aemerging%2520types%2520of%2520Artificial%2520Intelligence%2520%2528AI%2529%252C%2520as%2520well%2520as%2520available%2520risk%250Amanagement%2520measures.%2520In%2520response%252C%2520and%2520to%2520support%2520global%2520efforts%2520in%2520regulating%250AAI%2520and%2520writing%2520safety%2520standards%252C%2520we%2520compile%2520an%2520extensive%2520catalog%2520of%2520risk%250Asources%2520and%2520risk%2520management%2520measures%2520for%2520general-purpose%2520AI%2520%2528GPAI%2529%2520systems%252C%250Acomplete%2520with%2520descriptions%2520and%2520supporting%2520examples%2520where%2520relevant.%2520This%2520work%250Ainvolves%2520identifying%2520technical%252C%2520operational%252C%2520and%2520societal%2520risks%2520across%2520model%250Adevelopment%252C%2520training%252C%2520and%2520deployment%2520stages%252C%2520as%2520well%2520as%2520surveying%2520established%250Aand%2520experimental%2520methods%2520for%2520managing%2520these%2520risks.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520this%2520paper%2520is%2520the%2520first%2520of%2520its%2520kind%2520to%2520provide%2520extensive%250Adocumentation%2520of%2520both%2520GPAI%2520risk%2520sources%2520and%2520risk%2520management%2520measures%2520that%2520are%250Adescriptive%252C%2520self-contained%2520and%2520neutral%2520with%2520respect%2520to%2520any%2520existing%2520regulatory%250Aframework.%2520This%2520work%2520intends%2520to%2520help%2520AI%2520providers%252C%2520standards%2520experts%252C%250Aresearchers%252C%2520policymakers%252C%2520and%2520regulators%2520in%2520identifying%2520and%2520mitigating%250Asystemic%2520risks%2520from%2520GPAI%2520systems.%2520For%2520this%2520reason%252C%2520the%2520catalog%2520is%2520released%250Aunder%2520a%2520public%2520domain%2520license%2520for%2520ease%2520of%2520direct%2520use%2520by%2520stakeholders%2520in%2520AI%250Agovernance%2520and%2520standards.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23472v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Risk%20Sources%20and%20Risk%20Management%20Measures%20in%20Support%20of%20Standards%20for%0A%20%20General-Purpose%20AI%20Systems&entry.906535625=Rokas%20Gipi%C5%A1kis%20and%20Ayrton%20San%20Joaquin%20and%20Ze%20Shen%20Chin%20and%20Adrian%20Regenfu%C3%9F%20and%20Ariel%20Gil%20and%20Koen%20Holtman&entry.1292438233=%20%20There%20is%20an%20urgent%20need%20to%20identify%20both%20short%20and%20long-term%20risks%20from%20newly%0Aemerging%20types%20of%20Artificial%20Intelligence%20%28AI%29%2C%20as%20well%20as%20available%20risk%0Amanagement%20measures.%20In%20response%2C%20and%20to%20support%20global%20efforts%20in%20regulating%0AAI%20and%20writing%20safety%20standards%2C%20we%20compile%20an%20extensive%20catalog%20of%20risk%0Asources%20and%20risk%20management%20measures%20for%20general-purpose%20AI%20%28GPAI%29%20systems%2C%0Acomplete%20with%20descriptions%20and%20supporting%20examples%20where%20relevant.%20This%20work%0Ainvolves%20identifying%20technical%2C%20operational%2C%20and%20societal%20risks%20across%20model%0Adevelopment%2C%20training%2C%20and%20deployment%20stages%2C%20as%20well%20as%20surveying%20established%0Aand%20experimental%20methods%20for%20managing%20these%20risks.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20paper%20is%20the%20first%20of%20its%20kind%20to%20provide%20extensive%0Adocumentation%20of%20both%20GPAI%20risk%20sources%20and%20risk%20management%20measures%20that%20are%0Adescriptive%2C%20self-contained%20and%20neutral%20with%20respect%20to%20any%20existing%20regulatory%0Aframework.%20This%20work%20intends%20to%20help%20AI%20providers%2C%20standards%20experts%2C%0Aresearchers%2C%20policymakers%2C%20and%20regulators%20in%20identifying%20and%20mitigating%0Asystemic%20risks%20from%20GPAI%20systems.%20For%20this%20reason%2C%20the%20catalog%20is%20released%0Aunder%20a%20public%20domain%20license%20for%20ease%20of%20direct%20use%20by%20stakeholders%20in%20AI%0Agovernance%20and%20standards.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23472v2&entry.124074799=Read"},
{"title": "On the Cost of Model-Serving Frameworks: An Experimental Evaluation", "author": "Pasquale De Rosa and Y\u00e9rom-David Bromberg and Pascal Felber and Djob Mvondo and Valerio Schiavoni", "abstract": "  In machine learning (ML), the inference phase is the process of applying\npre-trained models to new, unseen data with the objective of making\npredictions. During the inference phase, end-users interact with ML services to\ngain insights, recommendations, or actions based on the input data. For this\nreason, serving strategies are nowadays crucial for deploying and managing\nmodels in production environments effectively. These strategies ensure that\nmodels are available, scalable, reliable, and performant for real-world\napplications, such as time series forecasting, image classification, natural\nlanguage processing, and so on. In this paper, we evaluate the performances of\nfive widely-used model serving frameworks (TensorFlow Serving, TorchServe,\nMLServer, MLflow, and BentoML) under four different scenarios (malware\ndetection, cryptocoin prices forecasting, image classification, and sentiment\nanalysis). We demonstrate that TensorFlow Serving is able to outperform all the\nother frameworks in serving deep learning (DL) models. Moreover, we show that\nDL-specific frameworks (TensorFlow Serving and TorchServe) display\nsignificantly lower latencies than the three general-purpose ML frameworks\n(BentoML, MLFlow, and MLServer).\n", "link": "http://arxiv.org/abs/2411.10337v1", "date": "2024-11-15", "relevancy": 0.9898, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.518}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4837}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Cost%20of%20Model-Serving%20Frameworks%3A%20An%20Experimental%20Evaluation&body=Title%3A%20On%20the%20Cost%20of%20Model-Serving%20Frameworks%3A%20An%20Experimental%20Evaluation%0AAuthor%3A%20Pasquale%20De%20Rosa%20and%20Y%C3%A9rom-David%20Bromberg%20and%20Pascal%20Felber%20and%20Djob%20Mvondo%20and%20Valerio%20Schiavoni%0AAbstract%3A%20%20%20In%20machine%20learning%20%28ML%29%2C%20the%20inference%20phase%20is%20the%20process%20of%20applying%0Apre-trained%20models%20to%20new%2C%20unseen%20data%20with%20the%20objective%20of%20making%0Apredictions.%20During%20the%20inference%20phase%2C%20end-users%20interact%20with%20ML%20services%20to%0Again%20insights%2C%20recommendations%2C%20or%20actions%20based%20on%20the%20input%20data.%20For%20this%0Areason%2C%20serving%20strategies%20are%20nowadays%20crucial%20for%20deploying%20and%20managing%0Amodels%20in%20production%20environments%20effectively.%20These%20strategies%20ensure%20that%0Amodels%20are%20available%2C%20scalable%2C%20reliable%2C%20and%20performant%20for%20real-world%0Aapplications%2C%20such%20as%20time%20series%20forecasting%2C%20image%20classification%2C%20natural%0Alanguage%20processing%2C%20and%20so%20on.%20In%20this%20paper%2C%20we%20evaluate%20the%20performances%20of%0Afive%20widely-used%20model%20serving%20frameworks%20%28TensorFlow%20Serving%2C%20TorchServe%2C%0AMLServer%2C%20MLflow%2C%20and%20BentoML%29%20under%20four%20different%20scenarios%20%28malware%0Adetection%2C%20cryptocoin%20prices%20forecasting%2C%20image%20classification%2C%20and%20sentiment%0Aanalysis%29.%20We%20demonstrate%20that%20TensorFlow%20Serving%20is%20able%20to%20outperform%20all%20the%0Aother%20frameworks%20in%20serving%20deep%20learning%20%28DL%29%20models.%20Moreover%2C%20we%20show%20that%0ADL-specific%20frameworks%20%28TensorFlow%20Serving%20and%20TorchServe%29%20display%0Asignificantly%20lower%20latencies%20than%20the%20three%20general-purpose%20ML%20frameworks%0A%28BentoML%2C%20MLFlow%2C%20and%20MLServer%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10337v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Cost%2520of%2520Model-Serving%2520Frameworks%253A%2520An%2520Experimental%2520Evaluation%26entry.906535625%3DPasquale%2520De%2520Rosa%2520and%2520Y%25C3%25A9rom-David%2520Bromberg%2520and%2520Pascal%2520Felber%2520and%2520Djob%2520Mvondo%2520and%2520Valerio%2520Schiavoni%26entry.1292438233%3D%2520%2520In%2520machine%2520learning%2520%2528ML%2529%252C%2520the%2520inference%2520phase%2520is%2520the%2520process%2520of%2520applying%250Apre-trained%2520models%2520to%2520new%252C%2520unseen%2520data%2520with%2520the%2520objective%2520of%2520making%250Apredictions.%2520During%2520the%2520inference%2520phase%252C%2520end-users%2520interact%2520with%2520ML%2520services%2520to%250Again%2520insights%252C%2520recommendations%252C%2520or%2520actions%2520based%2520on%2520the%2520input%2520data.%2520For%2520this%250Areason%252C%2520serving%2520strategies%2520are%2520nowadays%2520crucial%2520for%2520deploying%2520and%2520managing%250Amodels%2520in%2520production%2520environments%2520effectively.%2520These%2520strategies%2520ensure%2520that%250Amodels%2520are%2520available%252C%2520scalable%252C%2520reliable%252C%2520and%2520performant%2520for%2520real-world%250Aapplications%252C%2520such%2520as%2520time%2520series%2520forecasting%252C%2520image%2520classification%252C%2520natural%250Alanguage%2520processing%252C%2520and%2520so%2520on.%2520In%2520this%2520paper%252C%2520we%2520evaluate%2520the%2520performances%2520of%250Afive%2520widely-used%2520model%2520serving%2520frameworks%2520%2528TensorFlow%2520Serving%252C%2520TorchServe%252C%250AMLServer%252C%2520MLflow%252C%2520and%2520BentoML%2529%2520under%2520four%2520different%2520scenarios%2520%2528malware%250Adetection%252C%2520cryptocoin%2520prices%2520forecasting%252C%2520image%2520classification%252C%2520and%2520sentiment%250Aanalysis%2529.%2520We%2520demonstrate%2520that%2520TensorFlow%2520Serving%2520is%2520able%2520to%2520outperform%2520all%2520the%250Aother%2520frameworks%2520in%2520serving%2520deep%2520learning%2520%2528DL%2529%2520models.%2520Moreover%252C%2520we%2520show%2520that%250ADL-specific%2520frameworks%2520%2528TensorFlow%2520Serving%2520and%2520TorchServe%2529%2520display%250Asignificantly%2520lower%2520latencies%2520than%2520the%2520three%2520general-purpose%2520ML%2520frameworks%250A%2528BentoML%252C%2520MLFlow%252C%2520and%2520MLServer%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10337v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Cost%20of%20Model-Serving%20Frameworks%3A%20An%20Experimental%20Evaluation&entry.906535625=Pasquale%20De%20Rosa%20and%20Y%C3%A9rom-David%20Bromberg%20and%20Pascal%20Felber%20and%20Djob%20Mvondo%20and%20Valerio%20Schiavoni&entry.1292438233=%20%20In%20machine%20learning%20%28ML%29%2C%20the%20inference%20phase%20is%20the%20process%20of%20applying%0Apre-trained%20models%20to%20new%2C%20unseen%20data%20with%20the%20objective%20of%20making%0Apredictions.%20During%20the%20inference%20phase%2C%20end-users%20interact%20with%20ML%20services%20to%0Again%20insights%2C%20recommendations%2C%20or%20actions%20based%20on%20the%20input%20data.%20For%20this%0Areason%2C%20serving%20strategies%20are%20nowadays%20crucial%20for%20deploying%20and%20managing%0Amodels%20in%20production%20environments%20effectively.%20These%20strategies%20ensure%20that%0Amodels%20are%20available%2C%20scalable%2C%20reliable%2C%20and%20performant%20for%20real-world%0Aapplications%2C%20such%20as%20time%20series%20forecasting%2C%20image%20classification%2C%20natural%0Alanguage%20processing%2C%20and%20so%20on.%20In%20this%20paper%2C%20we%20evaluate%20the%20performances%20of%0Afive%20widely-used%20model%20serving%20frameworks%20%28TensorFlow%20Serving%2C%20TorchServe%2C%0AMLServer%2C%20MLflow%2C%20and%20BentoML%29%20under%20four%20different%20scenarios%20%28malware%0Adetection%2C%20cryptocoin%20prices%20forecasting%2C%20image%20classification%2C%20and%20sentiment%0Aanalysis%29.%20We%20demonstrate%20that%20TensorFlow%20Serving%20is%20able%20to%20outperform%20all%20the%0Aother%20frameworks%20in%20serving%20deep%20learning%20%28DL%29%20models.%20Moreover%2C%20we%20show%20that%0ADL-specific%20frameworks%20%28TensorFlow%20Serving%20and%20TorchServe%29%20display%0Asignificantly%20lower%20latencies%20than%20the%20three%20general-purpose%20ML%20frameworks%0A%28BentoML%2C%20MLFlow%2C%20and%20MLServer%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10337v1&entry.124074799=Read"},
{"title": "Mitigating Parameter Degeneracy using Joint Conditional Diffusion Model\n  for WECC Composite Load Model in Power Systems", "author": "Feiqin Zhu and Dmitrii Torbunov and Yihui Ren and Zhongjing Jiang and Tianqiao Zhao and Amirthagunaraj Yogarathnam and Meng Yue", "abstract": "  Data-driven modeling for dynamic systems has gained widespread attention in\nrecent years. Its inverse formulation, parameter estimation, aims to infer the\ninherent model parameters from observations. However, parameter degeneracy,\nwhere different combinations of parameters yield the same observable output,\nposes a critical barrier to accurately and uniquely identifying model\nparameters. In the context of WECC composite load model (CLM) in power systems,\nutility practitioners have observed that CLM parameters carefully selected for\none fault event may not perform satisfactorily in another fault. Here, we\ninnovate a joint conditional diffusion model-based inverse problem solver\n(JCDI), that incorporates a joint conditioning architecture with simultaneous\ninputs of multi-event observations to improve parameter generalizability.\nSimulation studies on the WECC CLM show that the proposed JCDI effectively\nreduces uncertainties of degenerate parameters, thus the parameter estimation\nerror is decreased by 42.1% compared to a single-event learning scheme. This\nenables the model to achieve high accuracy in predicting power trajectories\nunder different fault events, including electronic load tripping and motor\nstalling, outperforming standard deep reinforcement learning and supervised\nlearning approaches. We anticipate this work will contribute to mitigating\nparameter degeneracy in system dynamics, providing a general parameter\nestimation framework across various scientific domains.\n", "link": "http://arxiv.org/abs/2411.10431v1", "date": "2024-11-15", "relevancy": 1.0445, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5361}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5204}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Parameter%20Degeneracy%20using%20Joint%20Conditional%20Diffusion%20Model%0A%20%20for%20WECC%20Composite%20Load%20Model%20in%20Power%20Systems&body=Title%3A%20Mitigating%20Parameter%20Degeneracy%20using%20Joint%20Conditional%20Diffusion%20Model%0A%20%20for%20WECC%20Composite%20Load%20Model%20in%20Power%20Systems%0AAuthor%3A%20Feiqin%20Zhu%20and%20Dmitrii%20Torbunov%20and%20Yihui%20Ren%20and%20Zhongjing%20Jiang%20and%20Tianqiao%20Zhao%20and%20Amirthagunaraj%20Yogarathnam%20and%20Meng%20Yue%0AAbstract%3A%20%20%20Data-driven%20modeling%20for%20dynamic%20systems%20has%20gained%20widespread%20attention%20in%0Arecent%20years.%20Its%20inverse%20formulation%2C%20parameter%20estimation%2C%20aims%20to%20infer%20the%0Ainherent%20model%20parameters%20from%20observations.%20However%2C%20parameter%20degeneracy%2C%0Awhere%20different%20combinations%20of%20parameters%20yield%20the%20same%20observable%20output%2C%0Aposes%20a%20critical%20barrier%20to%20accurately%20and%20uniquely%20identifying%20model%0Aparameters.%20In%20the%20context%20of%20WECC%20composite%20load%20model%20%28CLM%29%20in%20power%20systems%2C%0Autility%20practitioners%20have%20observed%20that%20CLM%20parameters%20carefully%20selected%20for%0Aone%20fault%20event%20may%20not%20perform%20satisfactorily%20in%20another%20fault.%20Here%2C%20we%0Ainnovate%20a%20joint%20conditional%20diffusion%20model-based%20inverse%20problem%20solver%0A%28JCDI%29%2C%20that%20incorporates%20a%20joint%20conditioning%20architecture%20with%20simultaneous%0Ainputs%20of%20multi-event%20observations%20to%20improve%20parameter%20generalizability.%0ASimulation%20studies%20on%20the%20WECC%20CLM%20show%20that%20the%20proposed%20JCDI%20effectively%0Areduces%20uncertainties%20of%20degenerate%20parameters%2C%20thus%20the%20parameter%20estimation%0Aerror%20is%20decreased%20by%2042.1%25%20compared%20to%20a%20single-event%20learning%20scheme.%20This%0Aenables%20the%20model%20to%20achieve%20high%20accuracy%20in%20predicting%20power%20trajectories%0Aunder%20different%20fault%20events%2C%20including%20electronic%20load%20tripping%20and%20motor%0Astalling%2C%20outperforming%20standard%20deep%20reinforcement%20learning%20and%20supervised%0Alearning%20approaches.%20We%20anticipate%20this%20work%20will%20contribute%20to%20mitigating%0Aparameter%20degeneracy%20in%20system%20dynamics%2C%20providing%20a%20general%20parameter%0Aestimation%20framework%20across%20various%20scientific%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10431v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Parameter%2520Degeneracy%2520using%2520Joint%2520Conditional%2520Diffusion%2520Model%250A%2520%2520for%2520WECC%2520Composite%2520Load%2520Model%2520in%2520Power%2520Systems%26entry.906535625%3DFeiqin%2520Zhu%2520and%2520Dmitrii%2520Torbunov%2520and%2520Yihui%2520Ren%2520and%2520Zhongjing%2520Jiang%2520and%2520Tianqiao%2520Zhao%2520and%2520Amirthagunaraj%2520Yogarathnam%2520and%2520Meng%2520Yue%26entry.1292438233%3D%2520%2520Data-driven%2520modeling%2520for%2520dynamic%2520systems%2520has%2520gained%2520widespread%2520attention%2520in%250Arecent%2520years.%2520Its%2520inverse%2520formulation%252C%2520parameter%2520estimation%252C%2520aims%2520to%2520infer%2520the%250Ainherent%2520model%2520parameters%2520from%2520observations.%2520However%252C%2520parameter%2520degeneracy%252C%250Awhere%2520different%2520combinations%2520of%2520parameters%2520yield%2520the%2520same%2520observable%2520output%252C%250Aposes%2520a%2520critical%2520barrier%2520to%2520accurately%2520and%2520uniquely%2520identifying%2520model%250Aparameters.%2520In%2520the%2520context%2520of%2520WECC%2520composite%2520load%2520model%2520%2528CLM%2529%2520in%2520power%2520systems%252C%250Autility%2520practitioners%2520have%2520observed%2520that%2520CLM%2520parameters%2520carefully%2520selected%2520for%250Aone%2520fault%2520event%2520may%2520not%2520perform%2520satisfactorily%2520in%2520another%2520fault.%2520Here%252C%2520we%250Ainnovate%2520a%2520joint%2520conditional%2520diffusion%2520model-based%2520inverse%2520problem%2520solver%250A%2528JCDI%2529%252C%2520that%2520incorporates%2520a%2520joint%2520conditioning%2520architecture%2520with%2520simultaneous%250Ainputs%2520of%2520multi-event%2520observations%2520to%2520improve%2520parameter%2520generalizability.%250ASimulation%2520studies%2520on%2520the%2520WECC%2520CLM%2520show%2520that%2520the%2520proposed%2520JCDI%2520effectively%250Areduces%2520uncertainties%2520of%2520degenerate%2520parameters%252C%2520thus%2520the%2520parameter%2520estimation%250Aerror%2520is%2520decreased%2520by%252042.1%2525%2520compared%2520to%2520a%2520single-event%2520learning%2520scheme.%2520This%250Aenables%2520the%2520model%2520to%2520achieve%2520high%2520accuracy%2520in%2520predicting%2520power%2520trajectories%250Aunder%2520different%2520fault%2520events%252C%2520including%2520electronic%2520load%2520tripping%2520and%2520motor%250Astalling%252C%2520outperforming%2520standard%2520deep%2520reinforcement%2520learning%2520and%2520supervised%250Alearning%2520approaches.%2520We%2520anticipate%2520this%2520work%2520will%2520contribute%2520to%2520mitigating%250Aparameter%2520degeneracy%2520in%2520system%2520dynamics%252C%2520providing%2520a%2520general%2520parameter%250Aestimation%2520framework%2520across%2520various%2520scientific%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10431v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Parameter%20Degeneracy%20using%20Joint%20Conditional%20Diffusion%20Model%0A%20%20for%20WECC%20Composite%20Load%20Model%20in%20Power%20Systems&entry.906535625=Feiqin%20Zhu%20and%20Dmitrii%20Torbunov%20and%20Yihui%20Ren%20and%20Zhongjing%20Jiang%20and%20Tianqiao%20Zhao%20and%20Amirthagunaraj%20Yogarathnam%20and%20Meng%20Yue&entry.1292438233=%20%20Data-driven%20modeling%20for%20dynamic%20systems%20has%20gained%20widespread%20attention%20in%0Arecent%20years.%20Its%20inverse%20formulation%2C%20parameter%20estimation%2C%20aims%20to%20infer%20the%0Ainherent%20model%20parameters%20from%20observations.%20However%2C%20parameter%20degeneracy%2C%0Awhere%20different%20combinations%20of%20parameters%20yield%20the%20same%20observable%20output%2C%0Aposes%20a%20critical%20barrier%20to%20accurately%20and%20uniquely%20identifying%20model%0Aparameters.%20In%20the%20context%20of%20WECC%20composite%20load%20model%20%28CLM%29%20in%20power%20systems%2C%0Autility%20practitioners%20have%20observed%20that%20CLM%20parameters%20carefully%20selected%20for%0Aone%20fault%20event%20may%20not%20perform%20satisfactorily%20in%20another%20fault.%20Here%2C%20we%0Ainnovate%20a%20joint%20conditional%20diffusion%20model-based%20inverse%20problem%20solver%0A%28JCDI%29%2C%20that%20incorporates%20a%20joint%20conditioning%20architecture%20with%20simultaneous%0Ainputs%20of%20multi-event%20observations%20to%20improve%20parameter%20generalizability.%0ASimulation%20studies%20on%20the%20WECC%20CLM%20show%20that%20the%20proposed%20JCDI%20effectively%0Areduces%20uncertainties%20of%20degenerate%20parameters%2C%20thus%20the%20parameter%20estimation%0Aerror%20is%20decreased%20by%2042.1%25%20compared%20to%20a%20single-event%20learning%20scheme.%20This%0Aenables%20the%20model%20to%20achieve%20high%20accuracy%20in%20predicting%20power%20trajectories%0Aunder%20different%20fault%20events%2C%20including%20electronic%20load%20tripping%20and%20motor%0Astalling%2C%20outperforming%20standard%20deep%20reinforcement%20learning%20and%20supervised%0Alearning%20approaches.%20We%20anticipate%20this%20work%20will%20contribute%20to%20mitigating%0Aparameter%20degeneracy%20in%20system%20dynamics%2C%20providing%20a%20general%20parameter%0Aestimation%20framework%20across%20various%20scientific%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10431v1&entry.124074799=Read"},
{"title": "The Unreasonable Effectiveness of Guidance for Diffusion Models", "author": "Tim Kaiser and Nikolas Adaloglou and Markus Kollmann", "abstract": "  Guidance is an error-correcting technique used to improve the perceptual\nquality of images generated by diffusion models. Typically, the correction is\nachieved by linear extrapolation, using an auxiliary diffusion model that has\nlower performance than the primary model. Using a 2D toy example, we show that\nit is highly beneficial when the auxiliary model exhibits similar errors as the\nprimary one but stronger. We verify this finding in higher dimensions, where we\nshow that competitive generative performance to state-of-the-art guidance\nmethods can be achieved when the auxiliary model differs from the primary one\nonly by having stronger weight regularization. As an independent contribution,\nwe investigate whether upweighting long-range spatial dependencies improves\nvisual fidelity. The result is a novel guidance method, which we call sliding\nwindow guidance (SWG), that guides the primary model with itself by\nconstraining its receptive field. Intriguingly, SWG aligns better with human\npreferences than state-of-the-art guidance methods while requiring neither\ntraining, architectural modifications, nor class conditioning. The code will be\nreleased.\n", "link": "http://arxiv.org/abs/2411.10257v1", "date": "2024-11-15", "relevancy": 1.1462, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5782}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5731}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Unreasonable%20Effectiveness%20of%20Guidance%20for%20Diffusion%20Models&body=Title%3A%20The%20Unreasonable%20Effectiveness%20of%20Guidance%20for%20Diffusion%20Models%0AAuthor%3A%20Tim%20Kaiser%20and%20Nikolas%20Adaloglou%20and%20Markus%20Kollmann%0AAbstract%3A%20%20%20Guidance%20is%20an%20error-correcting%20technique%20used%20to%20improve%20the%20perceptual%0Aquality%20of%20images%20generated%20by%20diffusion%20models.%20Typically%2C%20the%20correction%20is%0Aachieved%20by%20linear%20extrapolation%2C%20using%20an%20auxiliary%20diffusion%20model%20that%20has%0Alower%20performance%20than%20the%20primary%20model.%20Using%20a%202D%20toy%20example%2C%20we%20show%20that%0Ait%20is%20highly%20beneficial%20when%20the%20auxiliary%20model%20exhibits%20similar%20errors%20as%20the%0Aprimary%20one%20but%20stronger.%20We%20verify%20this%20finding%20in%20higher%20dimensions%2C%20where%20we%0Ashow%20that%20competitive%20generative%20performance%20to%20state-of-the-art%20guidance%0Amethods%20can%20be%20achieved%20when%20the%20auxiliary%20model%20differs%20from%20the%20primary%20one%0Aonly%20by%20having%20stronger%20weight%20regularization.%20As%20an%20independent%20contribution%2C%0Awe%20investigate%20whether%20upweighting%20long-range%20spatial%20dependencies%20improves%0Avisual%20fidelity.%20The%20result%20is%20a%20novel%20guidance%20method%2C%20which%20we%20call%20sliding%0Awindow%20guidance%20%28SWG%29%2C%20that%20guides%20the%20primary%20model%20with%20itself%20by%0Aconstraining%20its%20receptive%20field.%20Intriguingly%2C%20SWG%20aligns%20better%20with%20human%0Apreferences%20than%20state-of-the-art%20guidance%20methods%20while%20requiring%20neither%0Atraining%2C%20architectural%20modifications%2C%20nor%20class%20conditioning.%20The%20code%20will%20be%0Areleased.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10257v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Unreasonable%2520Effectiveness%2520of%2520Guidance%2520for%2520Diffusion%2520Models%26entry.906535625%3DTim%2520Kaiser%2520and%2520Nikolas%2520Adaloglou%2520and%2520Markus%2520Kollmann%26entry.1292438233%3D%2520%2520Guidance%2520is%2520an%2520error-correcting%2520technique%2520used%2520to%2520improve%2520the%2520perceptual%250Aquality%2520of%2520images%2520generated%2520by%2520diffusion%2520models.%2520Typically%252C%2520the%2520correction%2520is%250Aachieved%2520by%2520linear%2520extrapolation%252C%2520using%2520an%2520auxiliary%2520diffusion%2520model%2520that%2520has%250Alower%2520performance%2520than%2520the%2520primary%2520model.%2520Using%2520a%25202D%2520toy%2520example%252C%2520we%2520show%2520that%250Ait%2520is%2520highly%2520beneficial%2520when%2520the%2520auxiliary%2520model%2520exhibits%2520similar%2520errors%2520as%2520the%250Aprimary%2520one%2520but%2520stronger.%2520We%2520verify%2520this%2520finding%2520in%2520higher%2520dimensions%252C%2520where%2520we%250Ashow%2520that%2520competitive%2520generative%2520performance%2520to%2520state-of-the-art%2520guidance%250Amethods%2520can%2520be%2520achieved%2520when%2520the%2520auxiliary%2520model%2520differs%2520from%2520the%2520primary%2520one%250Aonly%2520by%2520having%2520stronger%2520weight%2520regularization.%2520As%2520an%2520independent%2520contribution%252C%250Awe%2520investigate%2520whether%2520upweighting%2520long-range%2520spatial%2520dependencies%2520improves%250Avisual%2520fidelity.%2520The%2520result%2520is%2520a%2520novel%2520guidance%2520method%252C%2520which%2520we%2520call%2520sliding%250Awindow%2520guidance%2520%2528SWG%2529%252C%2520that%2520guides%2520the%2520primary%2520model%2520with%2520itself%2520by%250Aconstraining%2520its%2520receptive%2520field.%2520Intriguingly%252C%2520SWG%2520aligns%2520better%2520with%2520human%250Apreferences%2520than%2520state-of-the-art%2520guidance%2520methods%2520while%2520requiring%2520neither%250Atraining%252C%2520architectural%2520modifications%252C%2520nor%2520class%2520conditioning.%2520The%2520code%2520will%2520be%250Areleased.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10257v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Unreasonable%20Effectiveness%20of%20Guidance%20for%20Diffusion%20Models&entry.906535625=Tim%20Kaiser%20and%20Nikolas%20Adaloglou%20and%20Markus%20Kollmann&entry.1292438233=%20%20Guidance%20is%20an%20error-correcting%20technique%20used%20to%20improve%20the%20perceptual%0Aquality%20of%20images%20generated%20by%20diffusion%20models.%20Typically%2C%20the%20correction%20is%0Aachieved%20by%20linear%20extrapolation%2C%20using%20an%20auxiliary%20diffusion%20model%20that%20has%0Alower%20performance%20than%20the%20primary%20model.%20Using%20a%202D%20toy%20example%2C%20we%20show%20that%0Ait%20is%20highly%20beneficial%20when%20the%20auxiliary%20model%20exhibits%20similar%20errors%20as%20the%0Aprimary%20one%20but%20stronger.%20We%20verify%20this%20finding%20in%20higher%20dimensions%2C%20where%20we%0Ashow%20that%20competitive%20generative%20performance%20to%20state-of-the-art%20guidance%0Amethods%20can%20be%20achieved%20when%20the%20auxiliary%20model%20differs%20from%20the%20primary%20one%0Aonly%20by%20having%20stronger%20weight%20regularization.%20As%20an%20independent%20contribution%2C%0Awe%20investigate%20whether%20upweighting%20long-range%20spatial%20dependencies%20improves%0Avisual%20fidelity.%20The%20result%20is%20a%20novel%20guidance%20method%2C%20which%20we%20call%20sliding%0Awindow%20guidance%20%28SWG%29%2C%20that%20guides%20the%20primary%20model%20with%20itself%20by%0Aconstraining%20its%20receptive%20field.%20Intriguingly%2C%20SWG%20aligns%20better%20with%20human%0Apreferences%20than%20state-of-the-art%20guidance%20methods%20while%20requiring%20neither%0Atraining%2C%20architectural%20modifications%2C%20nor%20class%20conditioning.%20The%20code%20will%20be%0Areleased.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10257v1&entry.124074799=Read"},
{"title": "Multiscale Dubuc: A New Similarity Measure for Time Series", "author": "Mahsa Khazaei and Azim Ahmadzadeh and Krishna Rukmini Puthucode", "abstract": "  Quantifying similarities between time series in a meaningful way remains a\nchallenge in time series analysis, despite many advances in the field. Most\nreal-world solutions still rely on a few popular measures, such as Euclidean\nDistance (EuD), Longest Common Subsequence (LCSS), and Dynamic Time Warping\n(DTW). The strengths and weaknesses of these measures have been studied\nextensively, and incremental improvements have been proposed. In this study,\nhowever, we present a different similarity measure that fuses the notion of\nDubuc's variation from fractal analysis with the Intersection-over-Union (IoU)\nmeasure which is widely used in object recognition (also known as the Jaccard\nIndex). In this proof-of-concept paper, we introduce the Multiscale Dubuc\nDistance (MDD) measure and prove that it is a metric, possessing desirable\nproperties such as the triangle inequality. We use 95 datasets from the UCR\nTime Series Classification Archive to compare MDD's performance with EuD, LCSS,\nand DTW. Our experiments show that MDD's overall success, without any\ncase-specific customization, is comparable to DTW with optimized window sizes\nper dataset. We also highlight several datasets where MDD's performance\nimproves significantly when its single parameter is customized. This\ncustomization serves as a powerful tool for gauging MDD's sensitivity to noise.\nLastly, we show that MDD's running time is linear in the length of the time\nseries, which is crucial for real-world applications involving very large\ndatasets.\n", "link": "http://arxiv.org/abs/2411.10418v1", "date": "2024-11-15", "relevancy": 0.9327, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4725}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4719}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multiscale%20Dubuc%3A%20A%20New%20Similarity%20Measure%20for%20Time%20Series&body=Title%3A%20Multiscale%20Dubuc%3A%20A%20New%20Similarity%20Measure%20for%20Time%20Series%0AAuthor%3A%20Mahsa%20Khazaei%20and%20Azim%20Ahmadzadeh%20and%20Krishna%20Rukmini%20Puthucode%0AAbstract%3A%20%20%20Quantifying%20similarities%20between%20time%20series%20in%20a%20meaningful%20way%20remains%20a%0Achallenge%20in%20time%20series%20analysis%2C%20despite%20many%20advances%20in%20the%20field.%20Most%0Areal-world%20solutions%20still%20rely%20on%20a%20few%20popular%20measures%2C%20such%20as%20Euclidean%0ADistance%20%28EuD%29%2C%20Longest%20Common%20Subsequence%20%28LCSS%29%2C%20and%20Dynamic%20Time%20Warping%0A%28DTW%29.%20The%20strengths%20and%20weaknesses%20of%20these%20measures%20have%20been%20studied%0Aextensively%2C%20and%20incremental%20improvements%20have%20been%20proposed.%20In%20this%20study%2C%0Ahowever%2C%20we%20present%20a%20different%20similarity%20measure%20that%20fuses%20the%20notion%20of%0ADubuc%27s%20variation%20from%20fractal%20analysis%20with%20the%20Intersection-over-Union%20%28IoU%29%0Ameasure%20which%20is%20widely%20used%20in%20object%20recognition%20%28also%20known%20as%20the%20Jaccard%0AIndex%29.%20In%20this%20proof-of-concept%20paper%2C%20we%20introduce%20the%20Multiscale%20Dubuc%0ADistance%20%28MDD%29%20measure%20and%20prove%20that%20it%20is%20a%20metric%2C%20possessing%20desirable%0Aproperties%20such%20as%20the%20triangle%20inequality.%20We%20use%2095%20datasets%20from%20the%20UCR%0ATime%20Series%20Classification%20Archive%20to%20compare%20MDD%27s%20performance%20with%20EuD%2C%20LCSS%2C%0Aand%20DTW.%20Our%20experiments%20show%20that%20MDD%27s%20overall%20success%2C%20without%20any%0Acase-specific%20customization%2C%20is%20comparable%20to%20DTW%20with%20optimized%20window%20sizes%0Aper%20dataset.%20We%20also%20highlight%20several%20datasets%20where%20MDD%27s%20performance%0Aimproves%20significantly%20when%20its%20single%20parameter%20is%20customized.%20This%0Acustomization%20serves%20as%20a%20powerful%20tool%20for%20gauging%20MDD%27s%20sensitivity%20to%20noise.%0ALastly%2C%20we%20show%20that%20MDD%27s%20running%20time%20is%20linear%20in%20the%20length%20of%20the%20time%0Aseries%2C%20which%20is%20crucial%20for%20real-world%20applications%20involving%20very%20large%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10418v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiscale%2520Dubuc%253A%2520A%2520New%2520Similarity%2520Measure%2520for%2520Time%2520Series%26entry.906535625%3DMahsa%2520Khazaei%2520and%2520Azim%2520Ahmadzadeh%2520and%2520Krishna%2520Rukmini%2520Puthucode%26entry.1292438233%3D%2520%2520Quantifying%2520similarities%2520between%2520time%2520series%2520in%2520a%2520meaningful%2520way%2520remains%2520a%250Achallenge%2520in%2520time%2520series%2520analysis%252C%2520despite%2520many%2520advances%2520in%2520the%2520field.%2520Most%250Areal-world%2520solutions%2520still%2520rely%2520on%2520a%2520few%2520popular%2520measures%252C%2520such%2520as%2520Euclidean%250ADistance%2520%2528EuD%2529%252C%2520Longest%2520Common%2520Subsequence%2520%2528LCSS%2529%252C%2520and%2520Dynamic%2520Time%2520Warping%250A%2528DTW%2529.%2520The%2520strengths%2520and%2520weaknesses%2520of%2520these%2520measures%2520have%2520been%2520studied%250Aextensively%252C%2520and%2520incremental%2520improvements%2520have%2520been%2520proposed.%2520In%2520this%2520study%252C%250Ahowever%252C%2520we%2520present%2520a%2520different%2520similarity%2520measure%2520that%2520fuses%2520the%2520notion%2520of%250ADubuc%2527s%2520variation%2520from%2520fractal%2520analysis%2520with%2520the%2520Intersection-over-Union%2520%2528IoU%2529%250Ameasure%2520which%2520is%2520widely%2520used%2520in%2520object%2520recognition%2520%2528also%2520known%2520as%2520the%2520Jaccard%250AIndex%2529.%2520In%2520this%2520proof-of-concept%2520paper%252C%2520we%2520introduce%2520the%2520Multiscale%2520Dubuc%250ADistance%2520%2528MDD%2529%2520measure%2520and%2520prove%2520that%2520it%2520is%2520a%2520metric%252C%2520possessing%2520desirable%250Aproperties%2520such%2520as%2520the%2520triangle%2520inequality.%2520We%2520use%252095%2520datasets%2520from%2520the%2520UCR%250ATime%2520Series%2520Classification%2520Archive%2520to%2520compare%2520MDD%2527s%2520performance%2520with%2520EuD%252C%2520LCSS%252C%250Aand%2520DTW.%2520Our%2520experiments%2520show%2520that%2520MDD%2527s%2520overall%2520success%252C%2520without%2520any%250Acase-specific%2520customization%252C%2520is%2520comparable%2520to%2520DTW%2520with%2520optimized%2520window%2520sizes%250Aper%2520dataset.%2520We%2520also%2520highlight%2520several%2520datasets%2520where%2520MDD%2527s%2520performance%250Aimproves%2520significantly%2520when%2520its%2520single%2520parameter%2520is%2520customized.%2520This%250Acustomization%2520serves%2520as%2520a%2520powerful%2520tool%2520for%2520gauging%2520MDD%2527s%2520sensitivity%2520to%2520noise.%250ALastly%252C%2520we%2520show%2520that%2520MDD%2527s%2520running%2520time%2520is%2520linear%2520in%2520the%2520length%2520of%2520the%2520time%250Aseries%252C%2520which%2520is%2520crucial%2520for%2520real-world%2520applications%2520involving%2520very%2520large%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10418v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multiscale%20Dubuc%3A%20A%20New%20Similarity%20Measure%20for%20Time%20Series&entry.906535625=Mahsa%20Khazaei%20and%20Azim%20Ahmadzadeh%20and%20Krishna%20Rukmini%20Puthucode&entry.1292438233=%20%20Quantifying%20similarities%20between%20time%20series%20in%20a%20meaningful%20way%20remains%20a%0Achallenge%20in%20time%20series%20analysis%2C%20despite%20many%20advances%20in%20the%20field.%20Most%0Areal-world%20solutions%20still%20rely%20on%20a%20few%20popular%20measures%2C%20such%20as%20Euclidean%0ADistance%20%28EuD%29%2C%20Longest%20Common%20Subsequence%20%28LCSS%29%2C%20and%20Dynamic%20Time%20Warping%0A%28DTW%29.%20The%20strengths%20and%20weaknesses%20of%20these%20measures%20have%20been%20studied%0Aextensively%2C%20and%20incremental%20improvements%20have%20been%20proposed.%20In%20this%20study%2C%0Ahowever%2C%20we%20present%20a%20different%20similarity%20measure%20that%20fuses%20the%20notion%20of%0ADubuc%27s%20variation%20from%20fractal%20analysis%20with%20the%20Intersection-over-Union%20%28IoU%29%0Ameasure%20which%20is%20widely%20used%20in%20object%20recognition%20%28also%20known%20as%20the%20Jaccard%0AIndex%29.%20In%20this%20proof-of-concept%20paper%2C%20we%20introduce%20the%20Multiscale%20Dubuc%0ADistance%20%28MDD%29%20measure%20and%20prove%20that%20it%20is%20a%20metric%2C%20possessing%20desirable%0Aproperties%20such%20as%20the%20triangle%20inequality.%20We%20use%2095%20datasets%20from%20the%20UCR%0ATime%20Series%20Classification%20Archive%20to%20compare%20MDD%27s%20performance%20with%20EuD%2C%20LCSS%2C%0Aand%20DTW.%20Our%20experiments%20show%20that%20MDD%27s%20overall%20success%2C%20without%20any%0Acase-specific%20customization%2C%20is%20comparable%20to%20DTW%20with%20optimized%20window%20sizes%0Aper%20dataset.%20We%20also%20highlight%20several%20datasets%20where%20MDD%27s%20performance%0Aimproves%20significantly%20when%20its%20single%20parameter%20is%20customized.%20This%0Acustomization%20serves%20as%20a%20powerful%20tool%20for%20gauging%20MDD%27s%20sensitivity%20to%20noise.%0ALastly%2C%20we%20show%20that%20MDD%27s%20running%20time%20is%20linear%20in%20the%20length%20of%20the%20time%0Aseries%2C%20which%20is%20crucial%20for%20real-world%20applications%20involving%20very%20large%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10418v1&entry.124074799=Read"},
{"title": "Denoising Diffusion Planner: Learning Complex Paths from Low-Quality\n  Demonstrations", "author": "Michiel Nikken and Nicol\u00f2 Botteghi and Wesley Roozing and Federico Califano", "abstract": "  Denoising Diffusion Probabilistic Models (DDPMs) are powerful generative deep\nlearning models that have been very successful at image generation, and, very\nrecently, in path planning and control. In this paper, we investigate how to\nleverage the generalization and conditional sampling capabilities of DDPMs to\ngenerate complex paths for a robotic end effector. We show that training a DDPM\nwith synthetic and low-quality demonstrations is sufficient for generating\nnontrivial paths reaching arbitrary targets and avoiding obstacles.\nAdditionally, we investigate different strategies for conditional sampling\ncombining classifier-free and classifier-guided approaches. Eventually, we\ndeploy the DDPM in a receding-horizon control scheme to enhance its planning\ncapabilities. The Denoising Diffusion Planner is experimentally validated\nthrough various experiments on a Franka Emika Panda robot.\n", "link": "http://arxiv.org/abs/2410.21497v2", "date": "2024-11-15", "relevancy": 1.1096, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5654}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5545}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Denoising%20Diffusion%20Planner%3A%20Learning%20Complex%20Paths%20from%20Low-Quality%0A%20%20Demonstrations&body=Title%3A%20Denoising%20Diffusion%20Planner%3A%20Learning%20Complex%20Paths%20from%20Low-Quality%0A%20%20Demonstrations%0AAuthor%3A%20Michiel%20Nikken%20and%20Nicol%C3%B2%20Botteghi%20and%20Wesley%20Roozing%20and%20Federico%20Califano%0AAbstract%3A%20%20%20Denoising%20Diffusion%20Probabilistic%20Models%20%28DDPMs%29%20are%20powerful%20generative%20deep%0Alearning%20models%20that%20have%20been%20very%20successful%20at%20image%20generation%2C%20and%2C%20very%0Arecently%2C%20in%20path%20planning%20and%20control.%20In%20this%20paper%2C%20we%20investigate%20how%20to%0Aleverage%20the%20generalization%20and%20conditional%20sampling%20capabilities%20of%20DDPMs%20to%0Agenerate%20complex%20paths%20for%20a%20robotic%20end%20effector.%20We%20show%20that%20training%20a%20DDPM%0Awith%20synthetic%20and%20low-quality%20demonstrations%20is%20sufficient%20for%20generating%0Anontrivial%20paths%20reaching%20arbitrary%20targets%20and%20avoiding%20obstacles.%0AAdditionally%2C%20we%20investigate%20different%20strategies%20for%20conditional%20sampling%0Acombining%20classifier-free%20and%20classifier-guided%20approaches.%20Eventually%2C%20we%0Adeploy%20the%20DDPM%20in%20a%20receding-horizon%20control%20scheme%20to%20enhance%20its%20planning%0Acapabilities.%20The%20Denoising%20Diffusion%20Planner%20is%20experimentally%20validated%0Athrough%20various%20experiments%20on%20a%20Franka%20Emika%20Panda%20robot.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21497v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDenoising%2520Diffusion%2520Planner%253A%2520Learning%2520Complex%2520Paths%2520from%2520Low-Quality%250A%2520%2520Demonstrations%26entry.906535625%3DMichiel%2520Nikken%2520and%2520Nicol%25C3%25B2%2520Botteghi%2520and%2520Wesley%2520Roozing%2520and%2520Federico%2520Califano%26entry.1292438233%3D%2520%2520Denoising%2520Diffusion%2520Probabilistic%2520Models%2520%2528DDPMs%2529%2520are%2520powerful%2520generative%2520deep%250Alearning%2520models%2520that%2520have%2520been%2520very%2520successful%2520at%2520image%2520generation%252C%2520and%252C%2520very%250Arecently%252C%2520in%2520path%2520planning%2520and%2520control.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520how%2520to%250Aleverage%2520the%2520generalization%2520and%2520conditional%2520sampling%2520capabilities%2520of%2520DDPMs%2520to%250Agenerate%2520complex%2520paths%2520for%2520a%2520robotic%2520end%2520effector.%2520We%2520show%2520that%2520training%2520a%2520DDPM%250Awith%2520synthetic%2520and%2520low-quality%2520demonstrations%2520is%2520sufficient%2520for%2520generating%250Anontrivial%2520paths%2520reaching%2520arbitrary%2520targets%2520and%2520avoiding%2520obstacles.%250AAdditionally%252C%2520we%2520investigate%2520different%2520strategies%2520for%2520conditional%2520sampling%250Acombining%2520classifier-free%2520and%2520classifier-guided%2520approaches.%2520Eventually%252C%2520we%250Adeploy%2520the%2520DDPM%2520in%2520a%2520receding-horizon%2520control%2520scheme%2520to%2520enhance%2520its%2520planning%250Acapabilities.%2520The%2520Denoising%2520Diffusion%2520Planner%2520is%2520experimentally%2520validated%250Athrough%2520various%2520experiments%2520on%2520a%2520Franka%2520Emika%2520Panda%2520robot.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21497v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Denoising%20Diffusion%20Planner%3A%20Learning%20Complex%20Paths%20from%20Low-Quality%0A%20%20Demonstrations&entry.906535625=Michiel%20Nikken%20and%20Nicol%C3%B2%20Botteghi%20and%20Wesley%20Roozing%20and%20Federico%20Califano&entry.1292438233=%20%20Denoising%20Diffusion%20Probabilistic%20Models%20%28DDPMs%29%20are%20powerful%20generative%20deep%0Alearning%20models%20that%20have%20been%20very%20successful%20at%20image%20generation%2C%20and%2C%20very%0Arecently%2C%20in%20path%20planning%20and%20control.%20In%20this%20paper%2C%20we%20investigate%20how%20to%0Aleverage%20the%20generalization%20and%20conditional%20sampling%20capabilities%20of%20DDPMs%20to%0Agenerate%20complex%20paths%20for%20a%20robotic%20end%20effector.%20We%20show%20that%20training%20a%20DDPM%0Awith%20synthetic%20and%20low-quality%20demonstrations%20is%20sufficient%20for%20generating%0Anontrivial%20paths%20reaching%20arbitrary%20targets%20and%20avoiding%20obstacles.%0AAdditionally%2C%20we%20investigate%20different%20strategies%20for%20conditional%20sampling%0Acombining%20classifier-free%20and%20classifier-guided%20approaches.%20Eventually%2C%20we%0Adeploy%20the%20DDPM%20in%20a%20receding-horizon%20control%20scheme%20to%20enhance%20its%20planning%0Acapabilities.%20The%20Denoising%20Diffusion%20Planner%20is%20experimentally%20validated%0Athrough%20various%20experiments%20on%20a%20Franka%20Emika%20Panda%20robot.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21497v2&entry.124074799=Read"},
{"title": "Harnessing Machine Learning for Single-Shot Measurement of Free Electron\n  Laser Pulse Power", "author": "Till Korten and Vladimir Rybnikov and Mathias Vogt and Juliane Roensch-Schulenburg and Peter Steinbach and Najmeh Mirian", "abstract": "  Electron beam accelerators are essential in many scientific and technological\nfields. Their operation relies heavily on the stability and precision of the\nelectron beam. Traditional diagnostic techniques encounter difficulties in\naddressing the complex and dynamic nature of electron beams. Particularly in\nthe context of free-electron lasers (FELs), it is fundamentally impossible to\nmeasure the lasing-on and lasingoff electron power profiles for a single\nelectron bunch. This is a crucial hurdle in the exact reconstruction of the\nphoton pulse profile. To overcome this hurdle, we developed a machine learning\nmodel that predicts the temporal power profile of the electron bunch in the\nlasing-off regime using machine parameters that can be obtained when lasing is\non. The model was statistically validated and showed superior predictions\ncompared to the state-of-the-art batch calibrations. The work we present here\nis a critical element for a virtual pulse reconstruction diagnostic (VPRD) tool\ndesigned to reconstruct the power profile of individual photon pulses without\nrequiring repeated measurements in the lasing-off regime. This promises to\nsignificantly enhance the diagnostic capabilities in FELs at large.\n", "link": "http://arxiv.org/abs/2411.09468v2", "date": "2024-11-15", "relevancy": 1.3348, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.45}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4455}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harnessing%20Machine%20Learning%20for%20Single-Shot%20Measurement%20of%20Free%20Electron%0A%20%20Laser%20Pulse%20Power&body=Title%3A%20Harnessing%20Machine%20Learning%20for%20Single-Shot%20Measurement%20of%20Free%20Electron%0A%20%20Laser%20Pulse%20Power%0AAuthor%3A%20Till%20Korten%20and%20Vladimir%20Rybnikov%20and%20Mathias%20Vogt%20and%20Juliane%20Roensch-Schulenburg%20and%20Peter%20Steinbach%20and%20Najmeh%20Mirian%0AAbstract%3A%20%20%20Electron%20beam%20accelerators%20are%20essential%20in%20many%20scientific%20and%20technological%0Afields.%20Their%20operation%20relies%20heavily%20on%20the%20stability%20and%20precision%20of%20the%0Aelectron%20beam.%20Traditional%20diagnostic%20techniques%20encounter%20difficulties%20in%0Aaddressing%20the%20complex%20and%20dynamic%20nature%20of%20electron%20beams.%20Particularly%20in%0Athe%20context%20of%20free-electron%20lasers%20%28FELs%29%2C%20it%20is%20fundamentally%20impossible%20to%0Ameasure%20the%20lasing-on%20and%20lasingoff%20electron%20power%20profiles%20for%20a%20single%0Aelectron%20bunch.%20This%20is%20a%20crucial%20hurdle%20in%20the%20exact%20reconstruction%20of%20the%0Aphoton%20pulse%20profile.%20To%20overcome%20this%20hurdle%2C%20we%20developed%20a%20machine%20learning%0Amodel%20that%20predicts%20the%20temporal%20power%20profile%20of%20the%20electron%20bunch%20in%20the%0Alasing-off%20regime%20using%20machine%20parameters%20that%20can%20be%20obtained%20when%20lasing%20is%0Aon.%20The%20model%20was%20statistically%20validated%20and%20showed%20superior%20predictions%0Acompared%20to%20the%20state-of-the-art%20batch%20calibrations.%20The%20work%20we%20present%20here%0Ais%20a%20critical%20element%20for%20a%20virtual%20pulse%20reconstruction%20diagnostic%20%28VPRD%29%20tool%0Adesigned%20to%20reconstruct%20the%20power%20profile%20of%20individual%20photon%20pulses%20without%0Arequiring%20repeated%20measurements%20in%20the%20lasing-off%20regime.%20This%20promises%20to%0Asignificantly%20enhance%20the%20diagnostic%20capabilities%20in%20FELs%20at%20large.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09468v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarnessing%2520Machine%2520Learning%2520for%2520Single-Shot%2520Measurement%2520of%2520Free%2520Electron%250A%2520%2520Laser%2520Pulse%2520Power%26entry.906535625%3DTill%2520Korten%2520and%2520Vladimir%2520Rybnikov%2520and%2520Mathias%2520Vogt%2520and%2520Juliane%2520Roensch-Schulenburg%2520and%2520Peter%2520Steinbach%2520and%2520Najmeh%2520Mirian%26entry.1292438233%3D%2520%2520Electron%2520beam%2520accelerators%2520are%2520essential%2520in%2520many%2520scientific%2520and%2520technological%250Afields.%2520Their%2520operation%2520relies%2520heavily%2520on%2520the%2520stability%2520and%2520precision%2520of%2520the%250Aelectron%2520beam.%2520Traditional%2520diagnostic%2520techniques%2520encounter%2520difficulties%2520in%250Aaddressing%2520the%2520complex%2520and%2520dynamic%2520nature%2520of%2520electron%2520beams.%2520Particularly%2520in%250Athe%2520context%2520of%2520free-electron%2520lasers%2520%2528FELs%2529%252C%2520it%2520is%2520fundamentally%2520impossible%2520to%250Ameasure%2520the%2520lasing-on%2520and%2520lasingoff%2520electron%2520power%2520profiles%2520for%2520a%2520single%250Aelectron%2520bunch.%2520This%2520is%2520a%2520crucial%2520hurdle%2520in%2520the%2520exact%2520reconstruction%2520of%2520the%250Aphoton%2520pulse%2520profile.%2520To%2520overcome%2520this%2520hurdle%252C%2520we%2520developed%2520a%2520machine%2520learning%250Amodel%2520that%2520predicts%2520the%2520temporal%2520power%2520profile%2520of%2520the%2520electron%2520bunch%2520in%2520the%250Alasing-off%2520regime%2520using%2520machine%2520parameters%2520that%2520can%2520be%2520obtained%2520when%2520lasing%2520is%250Aon.%2520The%2520model%2520was%2520statistically%2520validated%2520and%2520showed%2520superior%2520predictions%250Acompared%2520to%2520the%2520state-of-the-art%2520batch%2520calibrations.%2520The%2520work%2520we%2520present%2520here%250Ais%2520a%2520critical%2520element%2520for%2520a%2520virtual%2520pulse%2520reconstruction%2520diagnostic%2520%2528VPRD%2529%2520tool%250Adesigned%2520to%2520reconstruct%2520the%2520power%2520profile%2520of%2520individual%2520photon%2520pulses%2520without%250Arequiring%2520repeated%2520measurements%2520in%2520the%2520lasing-off%2520regime.%2520This%2520promises%2520to%250Asignificantly%2520enhance%2520the%2520diagnostic%2520capabilities%2520in%2520FELs%2520at%2520large.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09468v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20Machine%20Learning%20for%20Single-Shot%20Measurement%20of%20Free%20Electron%0A%20%20Laser%20Pulse%20Power&entry.906535625=Till%20Korten%20and%20Vladimir%20Rybnikov%20and%20Mathias%20Vogt%20and%20Juliane%20Roensch-Schulenburg%20and%20Peter%20Steinbach%20and%20Najmeh%20Mirian&entry.1292438233=%20%20Electron%20beam%20accelerators%20are%20essential%20in%20many%20scientific%20and%20technological%0Afields.%20Their%20operation%20relies%20heavily%20on%20the%20stability%20and%20precision%20of%20the%0Aelectron%20beam.%20Traditional%20diagnostic%20techniques%20encounter%20difficulties%20in%0Aaddressing%20the%20complex%20and%20dynamic%20nature%20of%20electron%20beams.%20Particularly%20in%0Athe%20context%20of%20free-electron%20lasers%20%28FELs%29%2C%20it%20is%20fundamentally%20impossible%20to%0Ameasure%20the%20lasing-on%20and%20lasingoff%20electron%20power%20profiles%20for%20a%20single%0Aelectron%20bunch.%20This%20is%20a%20crucial%20hurdle%20in%20the%20exact%20reconstruction%20of%20the%0Aphoton%20pulse%20profile.%20To%20overcome%20this%20hurdle%2C%20we%20developed%20a%20machine%20learning%0Amodel%20that%20predicts%20the%20temporal%20power%20profile%20of%20the%20electron%20bunch%20in%20the%0Alasing-off%20regime%20using%20machine%20parameters%20that%20can%20be%20obtained%20when%20lasing%20is%0Aon.%20The%20model%20was%20statistically%20validated%20and%20showed%20superior%20predictions%0Acompared%20to%20the%20state-of-the-art%20batch%20calibrations.%20The%20work%20we%20present%20here%0Ais%20a%20critical%20element%20for%20a%20virtual%20pulse%20reconstruction%20diagnostic%20%28VPRD%29%20tool%0Adesigned%20to%20reconstruct%20the%20power%20profile%20of%20individual%20photon%20pulses%20without%0Arequiring%20repeated%20measurements%20in%20the%20lasing-off%20regime.%20This%20promises%20to%0Asignificantly%20enhance%20the%20diagnostic%20capabilities%20in%20FELs%20at%20large.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09468v2&entry.124074799=Read"},
{"title": "Temporal Patterns of Multiple Long-Term Conditions in Individuals with\n  Intellectual Disability Living in Wales: An Unsupervised Clustering Approach\n  to Disease Trajectories", "author": "Rania Kousovista and Georgina Cosma and Emeka Abakasanga and Ashley Akbari and Francesco Zaccardi and Gyuchan Thomas Jun and Reza Kiani and Satheesh Gangadharan", "abstract": "  Identifying and understanding the co-occurrence of multiple long-term\nconditions (MLTC) in individuals with intellectual disabilities (ID) is vital\nfor effective healthcare management. These individuals often face earlier onset\nand higher prevalence of MLTCs, yet specific co-occurrence patterns remain\nunexplored. This study applies an unsupervised approach to characterise MLTC\nclusters based on shared disease trajectories using electronic health records\n(EHRs) from 13069 individuals with ID in Wales (2000-2021). Disease\nassociations and temporal directionality were assessed, followed by spectral\nclustering to group shared trajectories. The population consisted of 52.3%\nmales and 47.7% females, with an average of 4.5 conditions per patient. Males\nunder 45 formed a single cluster dominated by neurological conditions (32.4%),\nwhile males above 45 had three clusters, the largest characterised circulatory\n(51.8%). Females under 45 formed one cluster with digestive conditions (24.6%)\nas most prevalent, while those aged 45 and older showed two clusters: one\ndominated by circulatory (34.1%), and the other by digestive (25.9%) and\nmusculoskeletal (21.9%) system conditions. Mental illness, epilepsy, and reflux\nwere common across groups. These clusters offer insights into disease\nprogression in individuals with ID, informing targeted interventions and\npersonalised healthcare strategies.\n", "link": "http://arxiv.org/abs/2411.08894v2", "date": "2024-11-15", "relevancy": 1.1572, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4038}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.387}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporal%20Patterns%20of%20Multiple%20Long-Term%20Conditions%20in%20Individuals%20with%0A%20%20Intellectual%20Disability%20Living%20in%20Wales%3A%20An%20Unsupervised%20Clustering%20Approach%0A%20%20to%20Disease%20Trajectories&body=Title%3A%20Temporal%20Patterns%20of%20Multiple%20Long-Term%20Conditions%20in%20Individuals%20with%0A%20%20Intellectual%20Disability%20Living%20in%20Wales%3A%20An%20Unsupervised%20Clustering%20Approach%0A%20%20to%20Disease%20Trajectories%0AAuthor%3A%20Rania%20Kousovista%20and%20Georgina%20Cosma%20and%20Emeka%20Abakasanga%20and%20Ashley%20Akbari%20and%20Francesco%20Zaccardi%20and%20Gyuchan%20Thomas%20Jun%20and%20Reza%20Kiani%20and%20Satheesh%20Gangadharan%0AAbstract%3A%20%20%20Identifying%20and%20understanding%20the%20co-occurrence%20of%20multiple%20long-term%0Aconditions%20%28MLTC%29%20in%20individuals%20with%20intellectual%20disabilities%20%28ID%29%20is%20vital%0Afor%20effective%20healthcare%20management.%20These%20individuals%20often%20face%20earlier%20onset%0Aand%20higher%20prevalence%20of%20MLTCs%2C%20yet%20specific%20co-occurrence%20patterns%20remain%0Aunexplored.%20This%20study%20applies%20an%20unsupervised%20approach%20to%20characterise%20MLTC%0Aclusters%20based%20on%20shared%20disease%20trajectories%20using%20electronic%20health%20records%0A%28EHRs%29%20from%2013069%20individuals%20with%20ID%20in%20Wales%20%282000-2021%29.%20Disease%0Aassociations%20and%20temporal%20directionality%20were%20assessed%2C%20followed%20by%20spectral%0Aclustering%20to%20group%20shared%20trajectories.%20The%20population%20consisted%20of%2052.3%25%0Amales%20and%2047.7%25%20females%2C%20with%20an%20average%20of%204.5%20conditions%20per%20patient.%20Males%0Aunder%2045%20formed%20a%20single%20cluster%20dominated%20by%20neurological%20conditions%20%2832.4%25%29%2C%0Awhile%20males%20above%2045%20had%20three%20clusters%2C%20the%20largest%20characterised%20circulatory%0A%2851.8%25%29.%20Females%20under%2045%20formed%20one%20cluster%20with%20digestive%20conditions%20%2824.6%25%29%0Aas%20most%20prevalent%2C%20while%20those%20aged%2045%20and%20older%20showed%20two%20clusters%3A%20one%0Adominated%20by%20circulatory%20%2834.1%25%29%2C%20and%20the%20other%20by%20digestive%20%2825.9%25%29%20and%0Amusculoskeletal%20%2821.9%25%29%20system%20conditions.%20Mental%20illness%2C%20epilepsy%2C%20and%20reflux%0Awere%20common%20across%20groups.%20These%20clusters%20offer%20insights%20into%20disease%0Aprogression%20in%20individuals%20with%20ID%2C%20informing%20targeted%20interventions%20and%0Apersonalised%20healthcare%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08894v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporal%2520Patterns%2520of%2520Multiple%2520Long-Term%2520Conditions%2520in%2520Individuals%2520with%250A%2520%2520Intellectual%2520Disability%2520Living%2520in%2520Wales%253A%2520An%2520Unsupervised%2520Clustering%2520Approach%250A%2520%2520to%2520Disease%2520Trajectories%26entry.906535625%3DRania%2520Kousovista%2520and%2520Georgina%2520Cosma%2520and%2520Emeka%2520Abakasanga%2520and%2520Ashley%2520Akbari%2520and%2520Francesco%2520Zaccardi%2520and%2520Gyuchan%2520Thomas%2520Jun%2520and%2520Reza%2520Kiani%2520and%2520Satheesh%2520Gangadharan%26entry.1292438233%3D%2520%2520Identifying%2520and%2520understanding%2520the%2520co-occurrence%2520of%2520multiple%2520long-term%250Aconditions%2520%2528MLTC%2529%2520in%2520individuals%2520with%2520intellectual%2520disabilities%2520%2528ID%2529%2520is%2520vital%250Afor%2520effective%2520healthcare%2520management.%2520These%2520individuals%2520often%2520face%2520earlier%2520onset%250Aand%2520higher%2520prevalence%2520of%2520MLTCs%252C%2520yet%2520specific%2520co-occurrence%2520patterns%2520remain%250Aunexplored.%2520This%2520study%2520applies%2520an%2520unsupervised%2520approach%2520to%2520characterise%2520MLTC%250Aclusters%2520based%2520on%2520shared%2520disease%2520trajectories%2520using%2520electronic%2520health%2520records%250A%2528EHRs%2529%2520from%252013069%2520individuals%2520with%2520ID%2520in%2520Wales%2520%25282000-2021%2529.%2520Disease%250Aassociations%2520and%2520temporal%2520directionality%2520were%2520assessed%252C%2520followed%2520by%2520spectral%250Aclustering%2520to%2520group%2520shared%2520trajectories.%2520The%2520population%2520consisted%2520of%252052.3%2525%250Amales%2520and%252047.7%2525%2520females%252C%2520with%2520an%2520average%2520of%25204.5%2520conditions%2520per%2520patient.%2520Males%250Aunder%252045%2520formed%2520a%2520single%2520cluster%2520dominated%2520by%2520neurological%2520conditions%2520%252832.4%2525%2529%252C%250Awhile%2520males%2520above%252045%2520had%2520three%2520clusters%252C%2520the%2520largest%2520characterised%2520circulatory%250A%252851.8%2525%2529.%2520Females%2520under%252045%2520formed%2520one%2520cluster%2520with%2520digestive%2520conditions%2520%252824.6%2525%2529%250Aas%2520most%2520prevalent%252C%2520while%2520those%2520aged%252045%2520and%2520older%2520showed%2520two%2520clusters%253A%2520one%250Adominated%2520by%2520circulatory%2520%252834.1%2525%2529%252C%2520and%2520the%2520other%2520by%2520digestive%2520%252825.9%2525%2529%2520and%250Amusculoskeletal%2520%252821.9%2525%2529%2520system%2520conditions.%2520Mental%2520illness%252C%2520epilepsy%252C%2520and%2520reflux%250Awere%2520common%2520across%2520groups.%2520These%2520clusters%2520offer%2520insights%2520into%2520disease%250Aprogression%2520in%2520individuals%2520with%2520ID%252C%2520informing%2520targeted%2520interventions%2520and%250Apersonalised%2520healthcare%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08894v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporal%20Patterns%20of%20Multiple%20Long-Term%20Conditions%20in%20Individuals%20with%0A%20%20Intellectual%20Disability%20Living%20in%20Wales%3A%20An%20Unsupervised%20Clustering%20Approach%0A%20%20to%20Disease%20Trajectories&entry.906535625=Rania%20Kousovista%20and%20Georgina%20Cosma%20and%20Emeka%20Abakasanga%20and%20Ashley%20Akbari%20and%20Francesco%20Zaccardi%20and%20Gyuchan%20Thomas%20Jun%20and%20Reza%20Kiani%20and%20Satheesh%20Gangadharan&entry.1292438233=%20%20Identifying%20and%20understanding%20the%20co-occurrence%20of%20multiple%20long-term%0Aconditions%20%28MLTC%29%20in%20individuals%20with%20intellectual%20disabilities%20%28ID%29%20is%20vital%0Afor%20effective%20healthcare%20management.%20These%20individuals%20often%20face%20earlier%20onset%0Aand%20higher%20prevalence%20of%20MLTCs%2C%20yet%20specific%20co-occurrence%20patterns%20remain%0Aunexplored.%20This%20study%20applies%20an%20unsupervised%20approach%20to%20characterise%20MLTC%0Aclusters%20based%20on%20shared%20disease%20trajectories%20using%20electronic%20health%20records%0A%28EHRs%29%20from%2013069%20individuals%20with%20ID%20in%20Wales%20%282000-2021%29.%20Disease%0Aassociations%20and%20temporal%20directionality%20were%20assessed%2C%20followed%20by%20spectral%0Aclustering%20to%20group%20shared%20trajectories.%20The%20population%20consisted%20of%2052.3%25%0Amales%20and%2047.7%25%20females%2C%20with%20an%20average%20of%204.5%20conditions%20per%20patient.%20Males%0Aunder%2045%20formed%20a%20single%20cluster%20dominated%20by%20neurological%20conditions%20%2832.4%25%29%2C%0Awhile%20males%20above%2045%20had%20three%20clusters%2C%20the%20largest%20characterised%20circulatory%0A%2851.8%25%29.%20Females%20under%2045%20formed%20one%20cluster%20with%20digestive%20conditions%20%2824.6%25%29%0Aas%20most%20prevalent%2C%20while%20those%20aged%2045%20and%20older%20showed%20two%20clusters%3A%20one%0Adominated%20by%20circulatory%20%2834.1%25%29%2C%20and%20the%20other%20by%20digestive%20%2825.9%25%29%20and%0Amusculoskeletal%20%2821.9%25%29%20system%20conditions.%20Mental%20illness%2C%20epilepsy%2C%20and%20reflux%0Awere%20common%20across%20groups.%20These%20clusters%20offer%20insights%20into%20disease%0Aprogression%20in%20individuals%20with%20ID%2C%20informing%20targeted%20interventions%20and%0Apersonalised%20healthcare%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08894v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


