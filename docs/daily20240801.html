<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240731.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Expressive Whole-Body 3D Gaussian Avatar", "author": "Gyeongsik Moon and Takaaki Shiratori and Shunsuke Saito", "abstract": "  Facial expression and hand motions are necessary to express our emotions and\ninteract with the world. Nevertheless, most of the 3D human avatars modeled\nfrom a casually captured video only support body motions without facial\nexpressions and hand motions.In this work, we present ExAvatar, an expressive\nwhole-body 3D human avatar learned from a short monocular video. We design\nExAvatar as a combination of the whole-body parametric mesh model (SMPL-X) and\n3D Gaussian Splatting (3DGS). The main challenges are 1) a limited diversity of\nfacial expressions and poses in the video and 2) the absence of 3D\nobservations, such as 3D scans and RGBD images. The limited diversity in the\nvideo makes animations with novel facial expressions and poses non-trivial. In\naddition, the absence of 3D observations could cause significant ambiguity in\nhuman parts that are not observed in the video, which can result in noticeable\nartifacts under novel motions. To address them, we introduce our hybrid\nrepresentation of the mesh and 3D Gaussians. Our hybrid representation treats\neach 3D Gaussian as a vertex on the surface with pre-defined connectivity\ninformation (i.e., triangle faces) between them following the mesh topology of\nSMPL-X. It makes our ExAvatar animatable with novel facial expressions by\ndriven by the facial expression space of SMPL-X. In addition, by using\nconnectivity-based regularizers, we significantly reduce artifacts in novel\nfacial expressions and poses.\n", "link": "http://arxiv.org/abs/2407.21686v1", "date": "2024-07-31", "relevancy": 3.569, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7699}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7699}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6017}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Expressive%20Whole-Body%203D%20Gaussian%20Avatar&body=Title%3A%20Expressive%20Whole-Body%203D%20Gaussian%20Avatar%0AAuthor%3A%20Gyeongsik%20Moon%20and%20Takaaki%20Shiratori%20and%20Shunsuke%20Saito%0AAbstract%3A%20%20%20Facial%20expression%20and%20hand%20motions%20are%20necessary%20to%20express%20our%20emotions%20and%0Ainteract%20with%20the%20world.%20Nevertheless%2C%20most%20of%20the%203D%20human%20avatars%20modeled%0Afrom%20a%20casually%20captured%20video%20only%20support%20body%20motions%20without%20facial%0Aexpressions%20and%20hand%20motions.In%20this%20work%2C%20we%20present%20ExAvatar%2C%20an%20expressive%0Awhole-body%203D%20human%20avatar%20learned%20from%20a%20short%20monocular%20video.%20We%20design%0AExAvatar%20as%20a%20combination%20of%20the%20whole-body%20parametric%20mesh%20model%20%28SMPL-X%29%20and%0A3D%20Gaussian%20Splatting%20%283DGS%29.%20The%20main%20challenges%20are%201%29%20a%20limited%20diversity%20of%0Afacial%20expressions%20and%20poses%20in%20the%20video%20and%202%29%20the%20absence%20of%203D%0Aobservations%2C%20such%20as%203D%20scans%20and%20RGBD%20images.%20The%20limited%20diversity%20in%20the%0Avideo%20makes%20animations%20with%20novel%20facial%20expressions%20and%20poses%20non-trivial.%20In%0Aaddition%2C%20the%20absence%20of%203D%20observations%20could%20cause%20significant%20ambiguity%20in%0Ahuman%20parts%20that%20are%20not%20observed%20in%20the%20video%2C%20which%20can%20result%20in%20noticeable%0Aartifacts%20under%20novel%20motions.%20To%20address%20them%2C%20we%20introduce%20our%20hybrid%0Arepresentation%20of%20the%20mesh%20and%203D%20Gaussians.%20Our%20hybrid%20representation%20treats%0Aeach%203D%20Gaussian%20as%20a%20vertex%20on%20the%20surface%20with%20pre-defined%20connectivity%0Ainformation%20%28i.e.%2C%20triangle%20faces%29%20between%20them%20following%20the%20mesh%20topology%20of%0ASMPL-X.%20It%20makes%20our%20ExAvatar%20animatable%20with%20novel%20facial%20expressions%20by%0Adriven%20by%20the%20facial%20expression%20space%20of%20SMPL-X.%20In%20addition%2C%20by%20using%0Aconnectivity-based%20regularizers%2C%20we%20significantly%20reduce%20artifacts%20in%20novel%0Afacial%20expressions%20and%20poses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21686v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExpressive%2520Whole-Body%25203D%2520Gaussian%2520Avatar%26entry.906535625%3DGyeongsik%2520Moon%2520and%2520Takaaki%2520Shiratori%2520and%2520Shunsuke%2520Saito%26entry.1292438233%3D%2520%2520Facial%2520expression%2520and%2520hand%2520motions%2520are%2520necessary%2520to%2520express%2520our%2520emotions%2520and%250Ainteract%2520with%2520the%2520world.%2520Nevertheless%252C%2520most%2520of%2520the%25203D%2520human%2520avatars%2520modeled%250Afrom%2520a%2520casually%2520captured%2520video%2520only%2520support%2520body%2520motions%2520without%2520facial%250Aexpressions%2520and%2520hand%2520motions.In%2520this%2520work%252C%2520we%2520present%2520ExAvatar%252C%2520an%2520expressive%250Awhole-body%25203D%2520human%2520avatar%2520learned%2520from%2520a%2520short%2520monocular%2520video.%2520We%2520design%250AExAvatar%2520as%2520a%2520combination%2520of%2520the%2520whole-body%2520parametric%2520mesh%2520model%2520%2528SMPL-X%2529%2520and%250A3D%2520Gaussian%2520Splatting%2520%25283DGS%2529.%2520The%2520main%2520challenges%2520are%25201%2529%2520a%2520limited%2520diversity%2520of%250Afacial%2520expressions%2520and%2520poses%2520in%2520the%2520video%2520and%25202%2529%2520the%2520absence%2520of%25203D%250Aobservations%252C%2520such%2520as%25203D%2520scans%2520and%2520RGBD%2520images.%2520The%2520limited%2520diversity%2520in%2520the%250Avideo%2520makes%2520animations%2520with%2520novel%2520facial%2520expressions%2520and%2520poses%2520non-trivial.%2520In%250Aaddition%252C%2520the%2520absence%2520of%25203D%2520observations%2520could%2520cause%2520significant%2520ambiguity%2520in%250Ahuman%2520parts%2520that%2520are%2520not%2520observed%2520in%2520the%2520video%252C%2520which%2520can%2520result%2520in%2520noticeable%250Aartifacts%2520under%2520novel%2520motions.%2520To%2520address%2520them%252C%2520we%2520introduce%2520our%2520hybrid%250Arepresentation%2520of%2520the%2520mesh%2520and%25203D%2520Gaussians.%2520Our%2520hybrid%2520representation%2520treats%250Aeach%25203D%2520Gaussian%2520as%2520a%2520vertex%2520on%2520the%2520surface%2520with%2520pre-defined%2520connectivity%250Ainformation%2520%2528i.e.%252C%2520triangle%2520faces%2529%2520between%2520them%2520following%2520the%2520mesh%2520topology%2520of%250ASMPL-X.%2520It%2520makes%2520our%2520ExAvatar%2520animatable%2520with%2520novel%2520facial%2520expressions%2520by%250Adriven%2520by%2520the%2520facial%2520expression%2520space%2520of%2520SMPL-X.%2520In%2520addition%252C%2520by%2520using%250Aconnectivity-based%2520regularizers%252C%2520we%2520significantly%2520reduce%2520artifacts%2520in%2520novel%250Afacial%2520expressions%2520and%2520poses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21686v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Expressive%20Whole-Body%203D%20Gaussian%20Avatar&entry.906535625=Gyeongsik%20Moon%20and%20Takaaki%20Shiratori%20and%20Shunsuke%20Saito&entry.1292438233=%20%20Facial%20expression%20and%20hand%20motions%20are%20necessary%20to%20express%20our%20emotions%20and%0Ainteract%20with%20the%20world.%20Nevertheless%2C%20most%20of%20the%203D%20human%20avatars%20modeled%0Afrom%20a%20casually%20captured%20video%20only%20support%20body%20motions%20without%20facial%0Aexpressions%20and%20hand%20motions.In%20this%20work%2C%20we%20present%20ExAvatar%2C%20an%20expressive%0Awhole-body%203D%20human%20avatar%20learned%20from%20a%20short%20monocular%20video.%20We%20design%0AExAvatar%20as%20a%20combination%20of%20the%20whole-body%20parametric%20mesh%20model%20%28SMPL-X%29%20and%0A3D%20Gaussian%20Splatting%20%283DGS%29.%20The%20main%20challenges%20are%201%29%20a%20limited%20diversity%20of%0Afacial%20expressions%20and%20poses%20in%20the%20video%20and%202%29%20the%20absence%20of%203D%0Aobservations%2C%20such%20as%203D%20scans%20and%20RGBD%20images.%20The%20limited%20diversity%20in%20the%0Avideo%20makes%20animations%20with%20novel%20facial%20expressions%20and%20poses%20non-trivial.%20In%0Aaddition%2C%20the%20absence%20of%203D%20observations%20could%20cause%20significant%20ambiguity%20in%0Ahuman%20parts%20that%20are%20not%20observed%20in%20the%20video%2C%20which%20can%20result%20in%20noticeable%0Aartifacts%20under%20novel%20motions.%20To%20address%20them%2C%20we%20introduce%20our%20hybrid%0Arepresentation%20of%20the%20mesh%20and%203D%20Gaussians.%20Our%20hybrid%20representation%20treats%0Aeach%203D%20Gaussian%20as%20a%20vertex%20on%20the%20surface%20with%20pre-defined%20connectivity%0Ainformation%20%28i.e.%2C%20triangle%20faces%29%20between%20them%20following%20the%20mesh%20topology%20of%0ASMPL-X.%20It%20makes%20our%20ExAvatar%20animatable%20with%20novel%20facial%20expressions%20by%0Adriven%20by%20the%20facial%20expression%20space%20of%20SMPL-X.%20In%20addition%2C%20by%20using%0Aconnectivity-based%20regularizers%2C%20we%20significantly%20reduce%20artifacts%20in%20novel%0Afacial%20expressions%20and%20poses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21686v1&entry.124074799=Read"},
{"title": "iMatching: Imperative Correspondence Learning", "author": "Zitong Zhan and Dasong Gao and Yun-Jou Lin and Youjie Xia and Chen Wang", "abstract": "  Learning feature correspondence is a foundational task in computer vision,\nholding immense importance for downstream applications such as visual odometry\nand 3D reconstruction. Despite recent progress in data-driven models, feature\ncorrespondence learning is still limited by the lack of accurate per-pixel\ncorrespondence labels. To overcome this difficulty, we introduce a new\nself-supervised scheme, imperative learning (IL), for training feature\ncorrespondence. It enables correspondence learning on arbitrary uninterrupted\nvideos without any camera pose or depth labels, heralding a new era for\nself-supervised correspondence learning. Specifically, we formulated the\nproblem of correspondence learning as a bilevel optimization, which takes the\nreprojection error from bundle adjustment as a supervisory signal for the\nmodel. To avoid large memory and computation overhead, we leverage the\nstationary point to effectively back-propagate the implicit gradients through\nbundle adjustment. Through extensive experiments, we demonstrate superior\nperformance on tasks including feature matching and pose estimation, in which\nwe obtained an average of 30% accuracy gain over the state-of-the-art matching\nmodels. This preprint corresponds to the Accepted Manuscript in European\nConference on Computer Vision (ECCV) 2024.\n", "link": "http://arxiv.org/abs/2312.02141v2", "date": "2024-07-31", "relevancy": 2.804, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6023}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5586}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20iMatching%3A%20Imperative%20Correspondence%20Learning&body=Title%3A%20iMatching%3A%20Imperative%20Correspondence%20Learning%0AAuthor%3A%20Zitong%20Zhan%20and%20Dasong%20Gao%20and%20Yun-Jou%20Lin%20and%20Youjie%20Xia%20and%20Chen%20Wang%0AAbstract%3A%20%20%20Learning%20feature%20correspondence%20is%20a%20foundational%20task%20in%20computer%20vision%2C%0Aholding%20immense%20importance%20for%20downstream%20applications%20such%20as%20visual%20odometry%0Aand%203D%20reconstruction.%20Despite%20recent%20progress%20in%20data-driven%20models%2C%20feature%0Acorrespondence%20learning%20is%20still%20limited%20by%20the%20lack%20of%20accurate%20per-pixel%0Acorrespondence%20labels.%20To%20overcome%20this%20difficulty%2C%20we%20introduce%20a%20new%0Aself-supervised%20scheme%2C%20imperative%20learning%20%28IL%29%2C%20for%20training%20feature%0Acorrespondence.%20It%20enables%20correspondence%20learning%20on%20arbitrary%20uninterrupted%0Avideos%20without%20any%20camera%20pose%20or%20depth%20labels%2C%20heralding%20a%20new%20era%20for%0Aself-supervised%20correspondence%20learning.%20Specifically%2C%20we%20formulated%20the%0Aproblem%20of%20correspondence%20learning%20as%20a%20bilevel%20optimization%2C%20which%20takes%20the%0Areprojection%20error%20from%20bundle%20adjustment%20as%20a%20supervisory%20signal%20for%20the%0Amodel.%20To%20avoid%20large%20memory%20and%20computation%20overhead%2C%20we%20leverage%20the%0Astationary%20point%20to%20effectively%20back-propagate%20the%20implicit%20gradients%20through%0Abundle%20adjustment.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20superior%0Aperformance%20on%20tasks%20including%20feature%20matching%20and%20pose%20estimation%2C%20in%20which%0Awe%20obtained%20an%20average%20of%2030%25%20accuracy%20gain%20over%20the%20state-of-the-art%20matching%0Amodels.%20This%20preprint%20corresponds%20to%20the%20Accepted%20Manuscript%20in%20European%0AConference%20on%20Computer%20Vision%20%28ECCV%29%202024.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02141v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DiMatching%253A%2520Imperative%2520Correspondence%2520Learning%26entry.906535625%3DZitong%2520Zhan%2520and%2520Dasong%2520Gao%2520and%2520Yun-Jou%2520Lin%2520and%2520Youjie%2520Xia%2520and%2520Chen%2520Wang%26entry.1292438233%3D%2520%2520Learning%2520feature%2520correspondence%2520is%2520a%2520foundational%2520task%2520in%2520computer%2520vision%252C%250Aholding%2520immense%2520importance%2520for%2520downstream%2520applications%2520such%2520as%2520visual%2520odometry%250Aand%25203D%2520reconstruction.%2520Despite%2520recent%2520progress%2520in%2520data-driven%2520models%252C%2520feature%250Acorrespondence%2520learning%2520is%2520still%2520limited%2520by%2520the%2520lack%2520of%2520accurate%2520per-pixel%250Acorrespondence%2520labels.%2520To%2520overcome%2520this%2520difficulty%252C%2520we%2520introduce%2520a%2520new%250Aself-supervised%2520scheme%252C%2520imperative%2520learning%2520%2528IL%2529%252C%2520for%2520training%2520feature%250Acorrespondence.%2520It%2520enables%2520correspondence%2520learning%2520on%2520arbitrary%2520uninterrupted%250Avideos%2520without%2520any%2520camera%2520pose%2520or%2520depth%2520labels%252C%2520heralding%2520a%2520new%2520era%2520for%250Aself-supervised%2520correspondence%2520learning.%2520Specifically%252C%2520we%2520formulated%2520the%250Aproblem%2520of%2520correspondence%2520learning%2520as%2520a%2520bilevel%2520optimization%252C%2520which%2520takes%2520the%250Areprojection%2520error%2520from%2520bundle%2520adjustment%2520as%2520a%2520supervisory%2520signal%2520for%2520the%250Amodel.%2520To%2520avoid%2520large%2520memory%2520and%2520computation%2520overhead%252C%2520we%2520leverage%2520the%250Astationary%2520point%2520to%2520effectively%2520back-propagate%2520the%2520implicit%2520gradients%2520through%250Abundle%2520adjustment.%2520Through%2520extensive%2520experiments%252C%2520we%2520demonstrate%2520superior%250Aperformance%2520on%2520tasks%2520including%2520feature%2520matching%2520and%2520pose%2520estimation%252C%2520in%2520which%250Awe%2520obtained%2520an%2520average%2520of%252030%2525%2520accuracy%2520gain%2520over%2520the%2520state-of-the-art%2520matching%250Amodels.%2520This%2520preprint%2520corresponds%2520to%2520the%2520Accepted%2520Manuscript%2520in%2520European%250AConference%2520on%2520Computer%2520Vision%2520%2528ECCV%2529%25202024.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.02141v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=iMatching%3A%20Imperative%20Correspondence%20Learning&entry.906535625=Zitong%20Zhan%20and%20Dasong%20Gao%20and%20Yun-Jou%20Lin%20and%20Youjie%20Xia%20and%20Chen%20Wang&entry.1292438233=%20%20Learning%20feature%20correspondence%20is%20a%20foundational%20task%20in%20computer%20vision%2C%0Aholding%20immense%20importance%20for%20downstream%20applications%20such%20as%20visual%20odometry%0Aand%203D%20reconstruction.%20Despite%20recent%20progress%20in%20data-driven%20models%2C%20feature%0Acorrespondence%20learning%20is%20still%20limited%20by%20the%20lack%20of%20accurate%20per-pixel%0Acorrespondence%20labels.%20To%20overcome%20this%20difficulty%2C%20we%20introduce%20a%20new%0Aself-supervised%20scheme%2C%20imperative%20learning%20%28IL%29%2C%20for%20training%20feature%0Acorrespondence.%20It%20enables%20correspondence%20learning%20on%20arbitrary%20uninterrupted%0Avideos%20without%20any%20camera%20pose%20or%20depth%20labels%2C%20heralding%20a%20new%20era%20for%0Aself-supervised%20correspondence%20learning.%20Specifically%2C%20we%20formulated%20the%0Aproblem%20of%20correspondence%20learning%20as%20a%20bilevel%20optimization%2C%20which%20takes%20the%0Areprojection%20error%20from%20bundle%20adjustment%20as%20a%20supervisory%20signal%20for%20the%0Amodel.%20To%20avoid%20large%20memory%20and%20computation%20overhead%2C%20we%20leverage%20the%0Astationary%20point%20to%20effectively%20back-propagate%20the%20implicit%20gradients%20through%0Abundle%20adjustment.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20superior%0Aperformance%20on%20tasks%20including%20feature%20matching%20and%20pose%20estimation%2C%20in%20which%0Awe%20obtained%20an%20average%20of%2030%25%20accuracy%20gain%20over%20the%20state-of-the-art%20matching%0Amodels.%20This%20preprint%20corresponds%20to%20the%20Accepted%20Manuscript%20in%20European%0AConference%20on%20Computer%20Vision%20%28ECCV%29%202024.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02141v2&entry.124074799=Read"},
{"title": "3D-GRES: Generalized 3D Referring Expression Segmentation", "author": "Changli Wu and Yihang Liu and Jiayi Ji and Yiwei Ma and Haowei Wang and Gen Luo and Henghui Ding and Xiaoshuai Sun and Rongrong Ji", "abstract": "  3D Referring Expression Segmentation (3D-RES) is dedicated to segmenting a\nspecific instance within a 3D space based on a natural language description.\nHowever, current approaches are limited to segmenting a single target,\nrestricting the versatility of the task. To overcome this limitation, we\nintroduce Generalized 3D Referring Expression Segmentation (3D-GRES), which\nextends the capability to segment any number of instances based on natural\nlanguage instructions. In addressing this broader task, we propose the\nMulti-Query Decoupled Interaction Network (MDIN), designed to break down\nmulti-object segmentation tasks into simpler, individual segmentations. MDIN\ncomprises two fundamental components: Text-driven Sparse Queries (TSQ) and\nMulti-object Decoupling Optimization (MDO). TSQ generates sparse point cloud\nfeatures distributed over key targets as the initialization for queries.\nMeanwhile, MDO is tasked with assigning each target in multi-object scenarios\nto different queries while maintaining their semantic consistency. To adapt to\nthis new task, we build a new dataset, namely Multi3DRes. Our comprehensive\nevaluations on this dataset demonstrate substantial enhancements over existing\nmodels, thus charting a new path for intricate multi-object 3D scene\ncomprehension. The benchmark and code are available at\nhttps://github.com/sosppxo/MDIN.\n", "link": "http://arxiv.org/abs/2407.20664v2", "date": "2024-07-31", "relevancy": 2.7887, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5605}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5595}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D-GRES%3A%20Generalized%203D%20Referring%20Expression%20Segmentation&body=Title%3A%203D-GRES%3A%20Generalized%203D%20Referring%20Expression%20Segmentation%0AAuthor%3A%20Changli%20Wu%20and%20Yihang%20Liu%20and%20Jiayi%20Ji%20and%20Yiwei%20Ma%20and%20Haowei%20Wang%20and%20Gen%20Luo%20and%20Henghui%20Ding%20and%20Xiaoshuai%20Sun%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%203D%20Referring%20Expression%20Segmentation%20%283D-RES%29%20is%20dedicated%20to%20segmenting%20a%0Aspecific%20instance%20within%20a%203D%20space%20based%20on%20a%20natural%20language%20description.%0AHowever%2C%20current%20approaches%20are%20limited%20to%20segmenting%20a%20single%20target%2C%0Arestricting%20the%20versatility%20of%20the%20task.%20To%20overcome%20this%20limitation%2C%20we%0Aintroduce%20Generalized%203D%20Referring%20Expression%20Segmentation%20%283D-GRES%29%2C%20which%0Aextends%20the%20capability%20to%20segment%20any%20number%20of%20instances%20based%20on%20natural%0Alanguage%20instructions.%20In%20addressing%20this%20broader%20task%2C%20we%20propose%20the%0AMulti-Query%20Decoupled%20Interaction%20Network%20%28MDIN%29%2C%20designed%20to%20break%20down%0Amulti-object%20segmentation%20tasks%20into%20simpler%2C%20individual%20segmentations.%20MDIN%0Acomprises%20two%20fundamental%20components%3A%20Text-driven%20Sparse%20Queries%20%28TSQ%29%20and%0AMulti-object%20Decoupling%20Optimization%20%28MDO%29.%20TSQ%20generates%20sparse%20point%20cloud%0Afeatures%20distributed%20over%20key%20targets%20as%20the%20initialization%20for%20queries.%0AMeanwhile%2C%20MDO%20is%20tasked%20with%20assigning%20each%20target%20in%20multi-object%20scenarios%0Ato%20different%20queries%20while%20maintaining%20their%20semantic%20consistency.%20To%20adapt%20to%0Athis%20new%20task%2C%20we%20build%20a%20new%20dataset%2C%20namely%20Multi3DRes.%20Our%20comprehensive%0Aevaluations%20on%20this%20dataset%20demonstrate%20substantial%20enhancements%20over%20existing%0Amodels%2C%20thus%20charting%20a%20new%20path%20for%20intricate%20multi-object%203D%20scene%0Acomprehension.%20The%20benchmark%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/sosppxo/MDIN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20664v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D-GRES%253A%2520Generalized%25203D%2520Referring%2520Expression%2520Segmentation%26entry.906535625%3DChangli%2520Wu%2520and%2520Yihang%2520Liu%2520and%2520Jiayi%2520Ji%2520and%2520Yiwei%2520Ma%2520and%2520Haowei%2520Wang%2520and%2520Gen%2520Luo%2520and%2520Henghui%2520Ding%2520and%2520Xiaoshuai%2520Sun%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%25203D%2520Referring%2520Expression%2520Segmentation%2520%25283D-RES%2529%2520is%2520dedicated%2520to%2520segmenting%2520a%250Aspecific%2520instance%2520within%2520a%25203D%2520space%2520based%2520on%2520a%2520natural%2520language%2520description.%250AHowever%252C%2520current%2520approaches%2520are%2520limited%2520to%2520segmenting%2520a%2520single%2520target%252C%250Arestricting%2520the%2520versatility%2520of%2520the%2520task.%2520To%2520overcome%2520this%2520limitation%252C%2520we%250Aintroduce%2520Generalized%25203D%2520Referring%2520Expression%2520Segmentation%2520%25283D-GRES%2529%252C%2520which%250Aextends%2520the%2520capability%2520to%2520segment%2520any%2520number%2520of%2520instances%2520based%2520on%2520natural%250Alanguage%2520instructions.%2520In%2520addressing%2520this%2520broader%2520task%252C%2520we%2520propose%2520the%250AMulti-Query%2520Decoupled%2520Interaction%2520Network%2520%2528MDIN%2529%252C%2520designed%2520to%2520break%2520down%250Amulti-object%2520segmentation%2520tasks%2520into%2520simpler%252C%2520individual%2520segmentations.%2520MDIN%250Acomprises%2520two%2520fundamental%2520components%253A%2520Text-driven%2520Sparse%2520Queries%2520%2528TSQ%2529%2520and%250AMulti-object%2520Decoupling%2520Optimization%2520%2528MDO%2529.%2520TSQ%2520generates%2520sparse%2520point%2520cloud%250Afeatures%2520distributed%2520over%2520key%2520targets%2520as%2520the%2520initialization%2520for%2520queries.%250AMeanwhile%252C%2520MDO%2520is%2520tasked%2520with%2520assigning%2520each%2520target%2520in%2520multi-object%2520scenarios%250Ato%2520different%2520queries%2520while%2520maintaining%2520their%2520semantic%2520consistency.%2520To%2520adapt%2520to%250Athis%2520new%2520task%252C%2520we%2520build%2520a%2520new%2520dataset%252C%2520namely%2520Multi3DRes.%2520Our%2520comprehensive%250Aevaluations%2520on%2520this%2520dataset%2520demonstrate%2520substantial%2520enhancements%2520over%2520existing%250Amodels%252C%2520thus%2520charting%2520a%2520new%2520path%2520for%2520intricate%2520multi-object%25203D%2520scene%250Acomprehension.%2520The%2520benchmark%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/sosppxo/MDIN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20664v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-GRES%3A%20Generalized%203D%20Referring%20Expression%20Segmentation&entry.906535625=Changli%20Wu%20and%20Yihang%20Liu%20and%20Jiayi%20Ji%20and%20Yiwei%20Ma%20and%20Haowei%20Wang%20and%20Gen%20Luo%20and%20Henghui%20Ding%20and%20Xiaoshuai%20Sun%20and%20Rongrong%20Ji&entry.1292438233=%20%203D%20Referring%20Expression%20Segmentation%20%283D-RES%29%20is%20dedicated%20to%20segmenting%20a%0Aspecific%20instance%20within%20a%203D%20space%20based%20on%20a%20natural%20language%20description.%0AHowever%2C%20current%20approaches%20are%20limited%20to%20segmenting%20a%20single%20target%2C%0Arestricting%20the%20versatility%20of%20the%20task.%20To%20overcome%20this%20limitation%2C%20we%0Aintroduce%20Generalized%203D%20Referring%20Expression%20Segmentation%20%283D-GRES%29%2C%20which%0Aextends%20the%20capability%20to%20segment%20any%20number%20of%20instances%20based%20on%20natural%0Alanguage%20instructions.%20In%20addressing%20this%20broader%20task%2C%20we%20propose%20the%0AMulti-Query%20Decoupled%20Interaction%20Network%20%28MDIN%29%2C%20designed%20to%20break%20down%0Amulti-object%20segmentation%20tasks%20into%20simpler%2C%20individual%20segmentations.%20MDIN%0Acomprises%20two%20fundamental%20components%3A%20Text-driven%20Sparse%20Queries%20%28TSQ%29%20and%0AMulti-object%20Decoupling%20Optimization%20%28MDO%29.%20TSQ%20generates%20sparse%20point%20cloud%0Afeatures%20distributed%20over%20key%20targets%20as%20the%20initialization%20for%20queries.%0AMeanwhile%2C%20MDO%20is%20tasked%20with%20assigning%20each%20target%20in%20multi-object%20scenarios%0Ato%20different%20queries%20while%20maintaining%20their%20semantic%20consistency.%20To%20adapt%20to%0Athis%20new%20task%2C%20we%20build%20a%20new%20dataset%2C%20namely%20Multi3DRes.%20Our%20comprehensive%0Aevaluations%20on%20this%20dataset%20demonstrate%20substantial%20enhancements%20over%20existing%0Amodels%2C%20thus%20charting%20a%20new%20path%20for%20intricate%20multi-object%203D%20scene%0Acomprehension.%20The%20benchmark%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/sosppxo/MDIN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20664v2&entry.124074799=Read"},
{"title": "MTA-CLIP: Language-Guided Semantic Segmentation with Mask-Text Alignment", "author": "Anurag Das and Xinting Hu and Li Jiang and Bernt Schiele", "abstract": "  Recent approaches have shown that large-scale vision-language models such as\nCLIP can improve semantic segmentation performance. These methods typically aim\nfor pixel-level vision-language alignment, but often rely on low resolution\nimage features from CLIP, resulting in class ambiguities along boundaries.\nMoreover, the global scene representations in CLIP text embeddings do not\ndirectly correlate with the local and detailed pixel-level features, making\nmeaningful alignment more difficult. To address these limitations, we introduce\nMTA-CLIP, a novel framework employing mask-level vision-language alignment.\nSpecifically, we first propose Mask-Text Decoder that enhances the mask\nrepresentations using rich textual data with the CLIP language model.\nSubsequently, it aligns mask representations with text embeddings using\nMask-to-Text Contrastive Learning. Furthermore, we introduce MaskText Prompt\nLearning, utilizing multiple context-specific prompts for text embeddings to\ncapture diverse class representations across masks. Overall, MTA-CLIP achieves\nstate-of-the-art, surpassing prior works by an average of 2.8% and 1.3% on on\nstandard benchmark datasets, ADE20k and Cityscapes, respectively.\n", "link": "http://arxiv.org/abs/2407.21654v1", "date": "2024-07-31", "relevancy": 2.729, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6273}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5063}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5037}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MTA-CLIP%3A%20Language-Guided%20Semantic%20Segmentation%20with%20Mask-Text%20Alignment&body=Title%3A%20MTA-CLIP%3A%20Language-Guided%20Semantic%20Segmentation%20with%20Mask-Text%20Alignment%0AAuthor%3A%20Anurag%20Das%20and%20Xinting%20Hu%20and%20Li%20Jiang%20and%20Bernt%20Schiele%0AAbstract%3A%20%20%20Recent%20approaches%20have%20shown%20that%20large-scale%20vision-language%20models%20such%20as%0ACLIP%20can%20improve%20semantic%20segmentation%20performance.%20These%20methods%20typically%20aim%0Afor%20pixel-level%20vision-language%20alignment%2C%20but%20often%20rely%20on%20low%20resolution%0Aimage%20features%20from%20CLIP%2C%20resulting%20in%20class%20ambiguities%20along%20boundaries.%0AMoreover%2C%20the%20global%20scene%20representations%20in%20CLIP%20text%20embeddings%20do%20not%0Adirectly%20correlate%20with%20the%20local%20and%20detailed%20pixel-level%20features%2C%20making%0Ameaningful%20alignment%20more%20difficult.%20To%20address%20these%20limitations%2C%20we%20introduce%0AMTA-CLIP%2C%20a%20novel%20framework%20employing%20mask-level%20vision-language%20alignment.%0ASpecifically%2C%20we%20first%20propose%20Mask-Text%20Decoder%20that%20enhances%20the%20mask%0Arepresentations%20using%20rich%20textual%20data%20with%20the%20CLIP%20language%20model.%0ASubsequently%2C%20it%20aligns%20mask%20representations%20with%20text%20embeddings%20using%0AMask-to-Text%20Contrastive%20Learning.%20Furthermore%2C%20we%20introduce%20MaskText%20Prompt%0ALearning%2C%20utilizing%20multiple%20context-specific%20prompts%20for%20text%20embeddings%20to%0Acapture%20diverse%20class%20representations%20across%20masks.%20Overall%2C%20MTA-CLIP%20achieves%0Astate-of-the-art%2C%20surpassing%20prior%20works%20by%20an%20average%20of%202.8%25%20and%201.3%25%20on%20on%0Astandard%20benchmark%20datasets%2C%20ADE20k%20and%20Cityscapes%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21654v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMTA-CLIP%253A%2520Language-Guided%2520Semantic%2520Segmentation%2520with%2520Mask-Text%2520Alignment%26entry.906535625%3DAnurag%2520Das%2520and%2520Xinting%2520Hu%2520and%2520Li%2520Jiang%2520and%2520Bernt%2520Schiele%26entry.1292438233%3D%2520%2520Recent%2520approaches%2520have%2520shown%2520that%2520large-scale%2520vision-language%2520models%2520such%2520as%250ACLIP%2520can%2520improve%2520semantic%2520segmentation%2520performance.%2520These%2520methods%2520typically%2520aim%250Afor%2520pixel-level%2520vision-language%2520alignment%252C%2520but%2520often%2520rely%2520on%2520low%2520resolution%250Aimage%2520features%2520from%2520CLIP%252C%2520resulting%2520in%2520class%2520ambiguities%2520along%2520boundaries.%250AMoreover%252C%2520the%2520global%2520scene%2520representations%2520in%2520CLIP%2520text%2520embeddings%2520do%2520not%250Adirectly%2520correlate%2520with%2520the%2520local%2520and%2520detailed%2520pixel-level%2520features%252C%2520making%250Ameaningful%2520alignment%2520more%2520difficult.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%250AMTA-CLIP%252C%2520a%2520novel%2520framework%2520employing%2520mask-level%2520vision-language%2520alignment.%250ASpecifically%252C%2520we%2520first%2520propose%2520Mask-Text%2520Decoder%2520that%2520enhances%2520the%2520mask%250Arepresentations%2520using%2520rich%2520textual%2520data%2520with%2520the%2520CLIP%2520language%2520model.%250ASubsequently%252C%2520it%2520aligns%2520mask%2520representations%2520with%2520text%2520embeddings%2520using%250AMask-to-Text%2520Contrastive%2520Learning.%2520Furthermore%252C%2520we%2520introduce%2520MaskText%2520Prompt%250ALearning%252C%2520utilizing%2520multiple%2520context-specific%2520prompts%2520for%2520text%2520embeddings%2520to%250Acapture%2520diverse%2520class%2520representations%2520across%2520masks.%2520Overall%252C%2520MTA-CLIP%2520achieves%250Astate-of-the-art%252C%2520surpassing%2520prior%2520works%2520by%2520an%2520average%2520of%25202.8%2525%2520and%25201.3%2525%2520on%2520on%250Astandard%2520benchmark%2520datasets%252C%2520ADE20k%2520and%2520Cityscapes%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21654v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MTA-CLIP%3A%20Language-Guided%20Semantic%20Segmentation%20with%20Mask-Text%20Alignment&entry.906535625=Anurag%20Das%20and%20Xinting%20Hu%20and%20Li%20Jiang%20and%20Bernt%20Schiele&entry.1292438233=%20%20Recent%20approaches%20have%20shown%20that%20large-scale%20vision-language%20models%20such%20as%0ACLIP%20can%20improve%20semantic%20segmentation%20performance.%20These%20methods%20typically%20aim%0Afor%20pixel-level%20vision-language%20alignment%2C%20but%20often%20rely%20on%20low%20resolution%0Aimage%20features%20from%20CLIP%2C%20resulting%20in%20class%20ambiguities%20along%20boundaries.%0AMoreover%2C%20the%20global%20scene%20representations%20in%20CLIP%20text%20embeddings%20do%20not%0Adirectly%20correlate%20with%20the%20local%20and%20detailed%20pixel-level%20features%2C%20making%0Ameaningful%20alignment%20more%20difficult.%20To%20address%20these%20limitations%2C%20we%20introduce%0AMTA-CLIP%2C%20a%20novel%20framework%20employing%20mask-level%20vision-language%20alignment.%0ASpecifically%2C%20we%20first%20propose%20Mask-Text%20Decoder%20that%20enhances%20the%20mask%0Arepresentations%20using%20rich%20textual%20data%20with%20the%20CLIP%20language%20model.%0ASubsequently%2C%20it%20aligns%20mask%20representations%20with%20text%20embeddings%20using%0AMask-to-Text%20Contrastive%20Learning.%20Furthermore%2C%20we%20introduce%20MaskText%20Prompt%0ALearning%2C%20utilizing%20multiple%20context-specific%20prompts%20for%20text%20embeddings%20to%0Acapture%20diverse%20class%20representations%20across%20masks.%20Overall%2C%20MTA-CLIP%20achieves%0Astate-of-the-art%2C%20surpassing%20prior%20works%20by%20an%20average%20of%202.8%25%20and%201.3%25%20on%20on%0Astandard%20benchmark%20datasets%2C%20ADE20k%20and%20Cityscapes%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21654v1&entry.124074799=Read"},
{"title": "InScope: A New Real-world 3D Infrastructure-side Collaborative\n  Perception Dataset for Open Traffic Scenarios", "author": "Xiaofei Zhang and Yining Li and Jinping Wang and Xiangyi Qin and Ying Shen and Zhengping Fan and Xiaojun Tan", "abstract": "  Perception systems of autonomous vehicles are susceptible to occlusion,\nespecially when examined from a vehicle-centric perspective. Such occlusion can\nlead to overlooked object detections, e.g., larger vehicles such as trucks or\nbuses may create blind spots where cyclists or pedestrians could be obscured,\naccentuating the safety concerns associated with such perception system\nlimitations. To mitigate these challenges, the vehicle-to-everything (V2X)\nparadigm suggests employing an infrastructure-side perception system (IPS) to\ncomplement autonomous vehicles with a broader perceptual scope. Nevertheless,\nthe scarcity of real-world 3D infrastructure-side datasets constrains the\nadvancement of V2X technologies. To bridge these gaps, this paper introduces a\nnew 3D infrastructure-side collaborative perception dataset, abbreviated as\ninscope. Notably, InScope is the first dataset dedicated to addressing\nocclusion challenges by strategically deploying multiple-position Light\nDetection and Ranging (LiDAR) systems on the infrastructure side. Specifically,\nInScope encapsulates a 20-day capture duration with 303 tracking trajectories\nand 187,787 3D bounding boxes annotated by experts. Through analysis of\nbenchmarks, four different benchmarks are presented for open traffic scenarios,\nincluding collaborative 3D object detection, multisource data fusion, data\ndomain transfer, and 3D multiobject tracking tasks. Additionally, a new metric\nis designed to quantify the impact of occlusion, facilitating the evaluation of\ndetection degradation ratios among various algorithms. The Experimental\nfindings showcase the enhanced performance of leveraging InScope to assist in\ndetecting and tracking 3D multiobjects in real-world scenarios, particularly in\ntracking obscured, small, and distant objects. The dataset and benchmarks are\navailable at https://github.com/xf-zh/InScope.\n", "link": "http://arxiv.org/abs/2407.21581v1", "date": "2024-07-31", "relevancy": 2.6999, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5622}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5289}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InScope%3A%20A%20New%20Real-world%203D%20Infrastructure-side%20Collaborative%0A%20%20Perception%20Dataset%20for%20Open%20Traffic%20Scenarios&body=Title%3A%20InScope%3A%20A%20New%20Real-world%203D%20Infrastructure-side%20Collaborative%0A%20%20Perception%20Dataset%20for%20Open%20Traffic%20Scenarios%0AAuthor%3A%20Xiaofei%20Zhang%20and%20Yining%20Li%20and%20Jinping%20Wang%20and%20Xiangyi%20Qin%20and%20Ying%20Shen%20and%20Zhengping%20Fan%20and%20Xiaojun%20Tan%0AAbstract%3A%20%20%20Perception%20systems%20of%20autonomous%20vehicles%20are%20susceptible%20to%20occlusion%2C%0Aespecially%20when%20examined%20from%20a%20vehicle-centric%20perspective.%20Such%20occlusion%20can%0Alead%20to%20overlooked%20object%20detections%2C%20e.g.%2C%20larger%20vehicles%20such%20as%20trucks%20or%0Abuses%20may%20create%20blind%20spots%20where%20cyclists%20or%20pedestrians%20could%20be%20obscured%2C%0Aaccentuating%20the%20safety%20concerns%20associated%20with%20such%20perception%20system%0Alimitations.%20To%20mitigate%20these%20challenges%2C%20the%20vehicle-to-everything%20%28V2X%29%0Aparadigm%20suggests%20employing%20an%20infrastructure-side%20perception%20system%20%28IPS%29%20to%0Acomplement%20autonomous%20vehicles%20with%20a%20broader%20perceptual%20scope.%20Nevertheless%2C%0Athe%20scarcity%20of%20real-world%203D%20infrastructure-side%20datasets%20constrains%20the%0Aadvancement%20of%20V2X%20technologies.%20To%20bridge%20these%20gaps%2C%20this%20paper%20introduces%20a%0Anew%203D%20infrastructure-side%20collaborative%20perception%20dataset%2C%20abbreviated%20as%0Ainscope.%20Notably%2C%20InScope%20is%20the%20first%20dataset%20dedicated%20to%20addressing%0Aocclusion%20challenges%20by%20strategically%20deploying%20multiple-position%20Light%0ADetection%20and%20Ranging%20%28LiDAR%29%20systems%20on%20the%20infrastructure%20side.%20Specifically%2C%0AInScope%20encapsulates%20a%2020-day%20capture%20duration%20with%20303%20tracking%20trajectories%0Aand%20187%2C787%203D%20bounding%20boxes%20annotated%20by%20experts.%20Through%20analysis%20of%0Abenchmarks%2C%20four%20different%20benchmarks%20are%20presented%20for%20open%20traffic%20scenarios%2C%0Aincluding%20collaborative%203D%20object%20detection%2C%20multisource%20data%20fusion%2C%20data%0Adomain%20transfer%2C%20and%203D%20multiobject%20tracking%20tasks.%20Additionally%2C%20a%20new%20metric%0Ais%20designed%20to%20quantify%20the%20impact%20of%20occlusion%2C%20facilitating%20the%20evaluation%20of%0Adetection%20degradation%20ratios%20among%20various%20algorithms.%20The%20Experimental%0Afindings%20showcase%20the%20enhanced%20performance%20of%20leveraging%20InScope%20to%20assist%20in%0Adetecting%20and%20tracking%203D%20multiobjects%20in%20real-world%20scenarios%2C%20particularly%20in%0Atracking%20obscured%2C%20small%2C%20and%20distant%20objects.%20The%20dataset%20and%20benchmarks%20are%0Aavailable%20at%20https%3A//github.com/xf-zh/InScope.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21581v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInScope%253A%2520A%2520New%2520Real-world%25203D%2520Infrastructure-side%2520Collaborative%250A%2520%2520Perception%2520Dataset%2520for%2520Open%2520Traffic%2520Scenarios%26entry.906535625%3DXiaofei%2520Zhang%2520and%2520Yining%2520Li%2520and%2520Jinping%2520Wang%2520and%2520Xiangyi%2520Qin%2520and%2520Ying%2520Shen%2520and%2520Zhengping%2520Fan%2520and%2520Xiaojun%2520Tan%26entry.1292438233%3D%2520%2520Perception%2520systems%2520of%2520autonomous%2520vehicles%2520are%2520susceptible%2520to%2520occlusion%252C%250Aespecially%2520when%2520examined%2520from%2520a%2520vehicle-centric%2520perspective.%2520Such%2520occlusion%2520can%250Alead%2520to%2520overlooked%2520object%2520detections%252C%2520e.g.%252C%2520larger%2520vehicles%2520such%2520as%2520trucks%2520or%250Abuses%2520may%2520create%2520blind%2520spots%2520where%2520cyclists%2520or%2520pedestrians%2520could%2520be%2520obscured%252C%250Aaccentuating%2520the%2520safety%2520concerns%2520associated%2520with%2520such%2520perception%2520system%250Alimitations.%2520To%2520mitigate%2520these%2520challenges%252C%2520the%2520vehicle-to-everything%2520%2528V2X%2529%250Aparadigm%2520suggests%2520employing%2520an%2520infrastructure-side%2520perception%2520system%2520%2528IPS%2529%2520to%250Acomplement%2520autonomous%2520vehicles%2520with%2520a%2520broader%2520perceptual%2520scope.%2520Nevertheless%252C%250Athe%2520scarcity%2520of%2520real-world%25203D%2520infrastructure-side%2520datasets%2520constrains%2520the%250Aadvancement%2520of%2520V2X%2520technologies.%2520To%2520bridge%2520these%2520gaps%252C%2520this%2520paper%2520introduces%2520a%250Anew%25203D%2520infrastructure-side%2520collaborative%2520perception%2520dataset%252C%2520abbreviated%2520as%250Ainscope.%2520Notably%252C%2520InScope%2520is%2520the%2520first%2520dataset%2520dedicated%2520to%2520addressing%250Aocclusion%2520challenges%2520by%2520strategically%2520deploying%2520multiple-position%2520Light%250ADetection%2520and%2520Ranging%2520%2528LiDAR%2529%2520systems%2520on%2520the%2520infrastructure%2520side.%2520Specifically%252C%250AInScope%2520encapsulates%2520a%252020-day%2520capture%2520duration%2520with%2520303%2520tracking%2520trajectories%250Aand%2520187%252C787%25203D%2520bounding%2520boxes%2520annotated%2520by%2520experts.%2520Through%2520analysis%2520of%250Abenchmarks%252C%2520four%2520different%2520benchmarks%2520are%2520presented%2520for%2520open%2520traffic%2520scenarios%252C%250Aincluding%2520collaborative%25203D%2520object%2520detection%252C%2520multisource%2520data%2520fusion%252C%2520data%250Adomain%2520transfer%252C%2520and%25203D%2520multiobject%2520tracking%2520tasks.%2520Additionally%252C%2520a%2520new%2520metric%250Ais%2520designed%2520to%2520quantify%2520the%2520impact%2520of%2520occlusion%252C%2520facilitating%2520the%2520evaluation%2520of%250Adetection%2520degradation%2520ratios%2520among%2520various%2520algorithms.%2520The%2520Experimental%250Afindings%2520showcase%2520the%2520enhanced%2520performance%2520of%2520leveraging%2520InScope%2520to%2520assist%2520in%250Adetecting%2520and%2520tracking%25203D%2520multiobjects%2520in%2520real-world%2520scenarios%252C%2520particularly%2520in%250Atracking%2520obscured%252C%2520small%252C%2520and%2520distant%2520objects.%2520The%2520dataset%2520and%2520benchmarks%2520are%250Aavailable%2520at%2520https%253A//github.com/xf-zh/InScope.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21581v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InScope%3A%20A%20New%20Real-world%203D%20Infrastructure-side%20Collaborative%0A%20%20Perception%20Dataset%20for%20Open%20Traffic%20Scenarios&entry.906535625=Xiaofei%20Zhang%20and%20Yining%20Li%20and%20Jinping%20Wang%20and%20Xiangyi%20Qin%20and%20Ying%20Shen%20and%20Zhengping%20Fan%20and%20Xiaojun%20Tan&entry.1292438233=%20%20Perception%20systems%20of%20autonomous%20vehicles%20are%20susceptible%20to%20occlusion%2C%0Aespecially%20when%20examined%20from%20a%20vehicle-centric%20perspective.%20Such%20occlusion%20can%0Alead%20to%20overlooked%20object%20detections%2C%20e.g.%2C%20larger%20vehicles%20such%20as%20trucks%20or%0Abuses%20may%20create%20blind%20spots%20where%20cyclists%20or%20pedestrians%20could%20be%20obscured%2C%0Aaccentuating%20the%20safety%20concerns%20associated%20with%20such%20perception%20system%0Alimitations.%20To%20mitigate%20these%20challenges%2C%20the%20vehicle-to-everything%20%28V2X%29%0Aparadigm%20suggests%20employing%20an%20infrastructure-side%20perception%20system%20%28IPS%29%20to%0Acomplement%20autonomous%20vehicles%20with%20a%20broader%20perceptual%20scope.%20Nevertheless%2C%0Athe%20scarcity%20of%20real-world%203D%20infrastructure-side%20datasets%20constrains%20the%0Aadvancement%20of%20V2X%20technologies.%20To%20bridge%20these%20gaps%2C%20this%20paper%20introduces%20a%0Anew%203D%20infrastructure-side%20collaborative%20perception%20dataset%2C%20abbreviated%20as%0Ainscope.%20Notably%2C%20InScope%20is%20the%20first%20dataset%20dedicated%20to%20addressing%0Aocclusion%20challenges%20by%20strategically%20deploying%20multiple-position%20Light%0ADetection%20and%20Ranging%20%28LiDAR%29%20systems%20on%20the%20infrastructure%20side.%20Specifically%2C%0AInScope%20encapsulates%20a%2020-day%20capture%20duration%20with%20303%20tracking%20trajectories%0Aand%20187%2C787%203D%20bounding%20boxes%20annotated%20by%20experts.%20Through%20analysis%20of%0Abenchmarks%2C%20four%20different%20benchmarks%20are%20presented%20for%20open%20traffic%20scenarios%2C%0Aincluding%20collaborative%203D%20object%20detection%2C%20multisource%20data%20fusion%2C%20data%0Adomain%20transfer%2C%20and%203D%20multiobject%20tracking%20tasks.%20Additionally%2C%20a%20new%20metric%0Ais%20designed%20to%20quantify%20the%20impact%20of%20occlusion%2C%20facilitating%20the%20evaluation%20of%0Adetection%20degradation%20ratios%20among%20various%20algorithms.%20The%20Experimental%0Afindings%20showcase%20the%20enhanced%20performance%20of%20leveraging%20InScope%20to%20assist%20in%0Adetecting%20and%20tracking%203D%20multiobjects%20in%20real-world%20scenarios%2C%20particularly%20in%0Atracking%20obscured%2C%20small%2C%20and%20distant%20objects.%20The%20dataset%20and%20benchmarks%20are%0Aavailable%20at%20https%3A//github.com/xf-zh/InScope.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21581v1&entry.124074799=Read"},
{"title": "Breaking the Hourglass Phenomenon of Residual Quantization: Enhancing\n  the Upper Bound of Generative Retrieval", "author": "Zhirui Kuai and Zuxu Chen and Huimu Wang and Mingming Li and Dadong Miao and Binbin Wang and Xusong Chen and Li Kuang and Yuxing Han and Jiaxing Wang and Guoyu Tang and Lin Liu and Songlin Wang and Jingwei Zhuo", "abstract": "  Generative retrieval (GR) has emerged as a transformative paradigm in search\nand recommender systems, leveraging numeric-based identifier representations to\nenhance efficiency and generalization. Notably, methods like TIGER employing\nResidual Quantization-based Semantic Identifiers (RQ-SID), have shown\nsignificant promise in e-commerce scenarios by effectively managing item IDs.\nHowever, a critical issue termed the \"\\textbf{Hourglass}\" phenomenon, occurs in\nRQ-SID, where intermediate codebook tokens become overly concentrated,\nhindering the full utilization of generative retrieval methods. This paper\nanalyses and addresses this problem by identifying data sparsity and\nlong-tailed distribution as the primary causes. Through comprehensive\nexperiments and detailed ablation studies, we analyze the impact of these\nfactors on codebook utilization and data distribution. Our findings reveal that\nthe \"Hourglass\" phenomenon substantially impacts the performance of RQ-SID in\ngenerative retrieval. We propose effective solutions to mitigate this issue,\nthereby significantly enhancing the effectiveness of generative retrieval in\nreal-world E-commerce applications.\n", "link": "http://arxiv.org/abs/2407.21488v1", "date": "2024-07-31", "relevancy": 2.6367, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5524}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5152}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breaking%20the%20Hourglass%20Phenomenon%20of%20Residual%20Quantization%3A%20Enhancing%0A%20%20the%20Upper%20Bound%20of%20Generative%20Retrieval&body=Title%3A%20Breaking%20the%20Hourglass%20Phenomenon%20of%20Residual%20Quantization%3A%20Enhancing%0A%20%20the%20Upper%20Bound%20of%20Generative%20Retrieval%0AAuthor%3A%20Zhirui%20Kuai%20and%20Zuxu%20Chen%20and%20Huimu%20Wang%20and%20Mingming%20Li%20and%20Dadong%20Miao%20and%20Binbin%20Wang%20and%20Xusong%20Chen%20and%20Li%20Kuang%20and%20Yuxing%20Han%20and%20Jiaxing%20Wang%20and%20Guoyu%20Tang%20and%20Lin%20Liu%20and%20Songlin%20Wang%20and%20Jingwei%20Zhuo%0AAbstract%3A%20%20%20Generative%20retrieval%20%28GR%29%20has%20emerged%20as%20a%20transformative%20paradigm%20in%20search%0Aand%20recommender%20systems%2C%20leveraging%20numeric-based%20identifier%20representations%20to%0Aenhance%20efficiency%20and%20generalization.%20Notably%2C%20methods%20like%20TIGER%20employing%0AResidual%20Quantization-based%20Semantic%20Identifiers%20%28RQ-SID%29%2C%20have%20shown%0Asignificant%20promise%20in%20e-commerce%20scenarios%20by%20effectively%20managing%20item%20IDs.%0AHowever%2C%20a%20critical%20issue%20termed%20the%20%22%5Ctextbf%7BHourglass%7D%22%20phenomenon%2C%20occurs%20in%0ARQ-SID%2C%20where%20intermediate%20codebook%20tokens%20become%20overly%20concentrated%2C%0Ahindering%20the%20full%20utilization%20of%20generative%20retrieval%20methods.%20This%20paper%0Aanalyses%20and%20addresses%20this%20problem%20by%20identifying%20data%20sparsity%20and%0Along-tailed%20distribution%20as%20the%20primary%20causes.%20Through%20comprehensive%0Aexperiments%20and%20detailed%20ablation%20studies%2C%20we%20analyze%20the%20impact%20of%20these%0Afactors%20on%20codebook%20utilization%20and%20data%20distribution.%20Our%20findings%20reveal%20that%0Athe%20%22Hourglass%22%20phenomenon%20substantially%20impacts%20the%20performance%20of%20RQ-SID%20in%0Agenerative%20retrieval.%20We%20propose%20effective%20solutions%20to%20mitigate%20this%20issue%2C%0Athereby%20significantly%20enhancing%20the%20effectiveness%20of%20generative%20retrieval%20in%0Areal-world%20E-commerce%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21488v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreaking%2520the%2520Hourglass%2520Phenomenon%2520of%2520Residual%2520Quantization%253A%2520Enhancing%250A%2520%2520the%2520Upper%2520Bound%2520of%2520Generative%2520Retrieval%26entry.906535625%3DZhirui%2520Kuai%2520and%2520Zuxu%2520Chen%2520and%2520Huimu%2520Wang%2520and%2520Mingming%2520Li%2520and%2520Dadong%2520Miao%2520and%2520Binbin%2520Wang%2520and%2520Xusong%2520Chen%2520and%2520Li%2520Kuang%2520and%2520Yuxing%2520Han%2520and%2520Jiaxing%2520Wang%2520and%2520Guoyu%2520Tang%2520and%2520Lin%2520Liu%2520and%2520Songlin%2520Wang%2520and%2520Jingwei%2520Zhuo%26entry.1292438233%3D%2520%2520Generative%2520retrieval%2520%2528GR%2529%2520has%2520emerged%2520as%2520a%2520transformative%2520paradigm%2520in%2520search%250Aand%2520recommender%2520systems%252C%2520leveraging%2520numeric-based%2520identifier%2520representations%2520to%250Aenhance%2520efficiency%2520and%2520generalization.%2520Notably%252C%2520methods%2520like%2520TIGER%2520employing%250AResidual%2520Quantization-based%2520Semantic%2520Identifiers%2520%2528RQ-SID%2529%252C%2520have%2520shown%250Asignificant%2520promise%2520in%2520e-commerce%2520scenarios%2520by%2520effectively%2520managing%2520item%2520IDs.%250AHowever%252C%2520a%2520critical%2520issue%2520termed%2520the%2520%2522%255Ctextbf%257BHourglass%257D%2522%2520phenomenon%252C%2520occurs%2520in%250ARQ-SID%252C%2520where%2520intermediate%2520codebook%2520tokens%2520become%2520overly%2520concentrated%252C%250Ahindering%2520the%2520full%2520utilization%2520of%2520generative%2520retrieval%2520methods.%2520This%2520paper%250Aanalyses%2520and%2520addresses%2520this%2520problem%2520by%2520identifying%2520data%2520sparsity%2520and%250Along-tailed%2520distribution%2520as%2520the%2520primary%2520causes.%2520Through%2520comprehensive%250Aexperiments%2520and%2520detailed%2520ablation%2520studies%252C%2520we%2520analyze%2520the%2520impact%2520of%2520these%250Afactors%2520on%2520codebook%2520utilization%2520and%2520data%2520distribution.%2520Our%2520findings%2520reveal%2520that%250Athe%2520%2522Hourglass%2522%2520phenomenon%2520substantially%2520impacts%2520the%2520performance%2520of%2520RQ-SID%2520in%250Agenerative%2520retrieval.%2520We%2520propose%2520effective%2520solutions%2520to%2520mitigate%2520this%2520issue%252C%250Athereby%2520significantly%2520enhancing%2520the%2520effectiveness%2520of%2520generative%2520retrieval%2520in%250Areal-world%2520E-commerce%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21488v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breaking%20the%20Hourglass%20Phenomenon%20of%20Residual%20Quantization%3A%20Enhancing%0A%20%20the%20Upper%20Bound%20of%20Generative%20Retrieval&entry.906535625=Zhirui%20Kuai%20and%20Zuxu%20Chen%20and%20Huimu%20Wang%20and%20Mingming%20Li%20and%20Dadong%20Miao%20and%20Binbin%20Wang%20and%20Xusong%20Chen%20and%20Li%20Kuang%20and%20Yuxing%20Han%20and%20Jiaxing%20Wang%20and%20Guoyu%20Tang%20and%20Lin%20Liu%20and%20Songlin%20Wang%20and%20Jingwei%20Zhuo&entry.1292438233=%20%20Generative%20retrieval%20%28GR%29%20has%20emerged%20as%20a%20transformative%20paradigm%20in%20search%0Aand%20recommender%20systems%2C%20leveraging%20numeric-based%20identifier%20representations%20to%0Aenhance%20efficiency%20and%20generalization.%20Notably%2C%20methods%20like%20TIGER%20employing%0AResidual%20Quantization-based%20Semantic%20Identifiers%20%28RQ-SID%29%2C%20have%20shown%0Asignificant%20promise%20in%20e-commerce%20scenarios%20by%20effectively%20managing%20item%20IDs.%0AHowever%2C%20a%20critical%20issue%20termed%20the%20%22%5Ctextbf%7BHourglass%7D%22%20phenomenon%2C%20occurs%20in%0ARQ-SID%2C%20where%20intermediate%20codebook%20tokens%20become%20overly%20concentrated%2C%0Ahindering%20the%20full%20utilization%20of%20generative%20retrieval%20methods.%20This%20paper%0Aanalyses%20and%20addresses%20this%20problem%20by%20identifying%20data%20sparsity%20and%0Along-tailed%20distribution%20as%20the%20primary%20causes.%20Through%20comprehensive%0Aexperiments%20and%20detailed%20ablation%20studies%2C%20we%20analyze%20the%20impact%20of%20these%0Afactors%20on%20codebook%20utilization%20and%20data%20distribution.%20Our%20findings%20reveal%20that%0Athe%20%22Hourglass%22%20phenomenon%20substantially%20impacts%20the%20performance%20of%20RQ-SID%20in%0Agenerative%20retrieval.%20We%20propose%20effective%20solutions%20to%20mitigate%20this%20issue%2C%0Athereby%20significantly%20enhancing%20the%20effectiveness%20of%20generative%20retrieval%20in%0Areal-world%20E-commerce%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21488v1&entry.124074799=Read"},
{"title": "EZSR: Event-based Zero-Shot Recognition", "author": "Yan Yang and Liyuan Pan and Dongxu Li and Liu Liu", "abstract": "  This paper studies zero-shot object recognition using event camera data.\nGuided by CLIP, which is pre-trained on RGB images, existing approaches achieve\nzero-shot object recognition by maximizing embedding similarities between event\ndata encoded by an event encoder and RGB images encoded by the CLIP image\nencoder. Alternatively, several methods learn RGB frame reconstructions from\nevent data for the CLIP image encoder. However, these approaches often result\nin suboptimal zero-shot performance.\n  This study develops an event encoder without relying on additional\nreconstruction networks. We theoretically analyze the performance bottlenecks\nof previous approaches: global similarity-based objective (i.e., maximizing the\nembedding similarities) cause semantic misalignments between the learned event\nembedding space and the CLIP text embedding space due to the degree of freedom.\nTo mitigate the issue, we explore a scalar-wise regularization strategy.\nFurthermore, to scale up the number of events and RGB data pairs for training,\nwe also propose a pipeline for synthesizing event data from static RGB images.\n  Experimentally, our data synthesis strategy exhibits an attractive scaling\nproperty, and our method achieves superior zero-shot object recognition\nperformance on extensive standard benchmark datasets, even compared with past\nsupervised learning approaches. For example, we achieve 47.84% zero-shot\naccuracy on the N-ImageNet dataset.\n", "link": "http://arxiv.org/abs/2407.21616v1", "date": "2024-07-31", "relevancy": 2.614, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5452}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5122}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EZSR%3A%20Event-based%20Zero-Shot%20Recognition&body=Title%3A%20EZSR%3A%20Event-based%20Zero-Shot%20Recognition%0AAuthor%3A%20Yan%20Yang%20and%20Liyuan%20Pan%20and%20Dongxu%20Li%20and%20Liu%20Liu%0AAbstract%3A%20%20%20This%20paper%20studies%20zero-shot%20object%20recognition%20using%20event%20camera%20data.%0AGuided%20by%20CLIP%2C%20which%20is%20pre-trained%20on%20RGB%20images%2C%20existing%20approaches%20achieve%0Azero-shot%20object%20recognition%20by%20maximizing%20embedding%20similarities%20between%20event%0Adata%20encoded%20by%20an%20event%20encoder%20and%20RGB%20images%20encoded%20by%20the%20CLIP%20image%0Aencoder.%20Alternatively%2C%20several%20methods%20learn%20RGB%20frame%20reconstructions%20from%0Aevent%20data%20for%20the%20CLIP%20image%20encoder.%20However%2C%20these%20approaches%20often%20result%0Ain%20suboptimal%20zero-shot%20performance.%0A%20%20This%20study%20develops%20an%20event%20encoder%20without%20relying%20on%20additional%0Areconstruction%20networks.%20We%20theoretically%20analyze%20the%20performance%20bottlenecks%0Aof%20previous%20approaches%3A%20global%20similarity-based%20objective%20%28i.e.%2C%20maximizing%20the%0Aembedding%20similarities%29%20cause%20semantic%20misalignments%20between%20the%20learned%20event%0Aembedding%20space%20and%20the%20CLIP%20text%20embedding%20space%20due%20to%20the%20degree%20of%20freedom.%0ATo%20mitigate%20the%20issue%2C%20we%20explore%20a%20scalar-wise%20regularization%20strategy.%0AFurthermore%2C%20to%20scale%20up%20the%20number%20of%20events%20and%20RGB%20data%20pairs%20for%20training%2C%0Awe%20also%20propose%20a%20pipeline%20for%20synthesizing%20event%20data%20from%20static%20RGB%20images.%0A%20%20Experimentally%2C%20our%20data%20synthesis%20strategy%20exhibits%20an%20attractive%20scaling%0Aproperty%2C%20and%20our%20method%20achieves%20superior%20zero-shot%20object%20recognition%0Aperformance%20on%20extensive%20standard%20benchmark%20datasets%2C%20even%20compared%20with%20past%0Asupervised%20learning%20approaches.%20For%20example%2C%20we%20achieve%2047.84%25%20zero-shot%0Aaccuracy%20on%20the%20N-ImageNet%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21616v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEZSR%253A%2520Event-based%2520Zero-Shot%2520Recognition%26entry.906535625%3DYan%2520Yang%2520and%2520Liyuan%2520Pan%2520and%2520Dongxu%2520Li%2520and%2520Liu%2520Liu%26entry.1292438233%3D%2520%2520This%2520paper%2520studies%2520zero-shot%2520object%2520recognition%2520using%2520event%2520camera%2520data.%250AGuided%2520by%2520CLIP%252C%2520which%2520is%2520pre-trained%2520on%2520RGB%2520images%252C%2520existing%2520approaches%2520achieve%250Azero-shot%2520object%2520recognition%2520by%2520maximizing%2520embedding%2520similarities%2520between%2520event%250Adata%2520encoded%2520by%2520an%2520event%2520encoder%2520and%2520RGB%2520images%2520encoded%2520by%2520the%2520CLIP%2520image%250Aencoder.%2520Alternatively%252C%2520several%2520methods%2520learn%2520RGB%2520frame%2520reconstructions%2520from%250Aevent%2520data%2520for%2520the%2520CLIP%2520image%2520encoder.%2520However%252C%2520these%2520approaches%2520often%2520result%250Ain%2520suboptimal%2520zero-shot%2520performance.%250A%2520%2520This%2520study%2520develops%2520an%2520event%2520encoder%2520without%2520relying%2520on%2520additional%250Areconstruction%2520networks.%2520We%2520theoretically%2520analyze%2520the%2520performance%2520bottlenecks%250Aof%2520previous%2520approaches%253A%2520global%2520similarity-based%2520objective%2520%2528i.e.%252C%2520maximizing%2520the%250Aembedding%2520similarities%2529%2520cause%2520semantic%2520misalignments%2520between%2520the%2520learned%2520event%250Aembedding%2520space%2520and%2520the%2520CLIP%2520text%2520embedding%2520space%2520due%2520to%2520the%2520degree%2520of%2520freedom.%250ATo%2520mitigate%2520the%2520issue%252C%2520we%2520explore%2520a%2520scalar-wise%2520regularization%2520strategy.%250AFurthermore%252C%2520to%2520scale%2520up%2520the%2520number%2520of%2520events%2520and%2520RGB%2520data%2520pairs%2520for%2520training%252C%250Awe%2520also%2520propose%2520a%2520pipeline%2520for%2520synthesizing%2520event%2520data%2520from%2520static%2520RGB%2520images.%250A%2520%2520Experimentally%252C%2520our%2520data%2520synthesis%2520strategy%2520exhibits%2520an%2520attractive%2520scaling%250Aproperty%252C%2520and%2520our%2520method%2520achieves%2520superior%2520zero-shot%2520object%2520recognition%250Aperformance%2520on%2520extensive%2520standard%2520benchmark%2520datasets%252C%2520even%2520compared%2520with%2520past%250Asupervised%2520learning%2520approaches.%2520For%2520example%252C%2520we%2520achieve%252047.84%2525%2520zero-shot%250Aaccuracy%2520on%2520the%2520N-ImageNet%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21616v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EZSR%3A%20Event-based%20Zero-Shot%20Recognition&entry.906535625=Yan%20Yang%20and%20Liyuan%20Pan%20and%20Dongxu%20Li%20and%20Liu%20Liu&entry.1292438233=%20%20This%20paper%20studies%20zero-shot%20object%20recognition%20using%20event%20camera%20data.%0AGuided%20by%20CLIP%2C%20which%20is%20pre-trained%20on%20RGB%20images%2C%20existing%20approaches%20achieve%0Azero-shot%20object%20recognition%20by%20maximizing%20embedding%20similarities%20between%20event%0Adata%20encoded%20by%20an%20event%20encoder%20and%20RGB%20images%20encoded%20by%20the%20CLIP%20image%0Aencoder.%20Alternatively%2C%20several%20methods%20learn%20RGB%20frame%20reconstructions%20from%0Aevent%20data%20for%20the%20CLIP%20image%20encoder.%20However%2C%20these%20approaches%20often%20result%0Ain%20suboptimal%20zero-shot%20performance.%0A%20%20This%20study%20develops%20an%20event%20encoder%20without%20relying%20on%20additional%0Areconstruction%20networks.%20We%20theoretically%20analyze%20the%20performance%20bottlenecks%0Aof%20previous%20approaches%3A%20global%20similarity-based%20objective%20%28i.e.%2C%20maximizing%20the%0Aembedding%20similarities%29%20cause%20semantic%20misalignments%20between%20the%20learned%20event%0Aembedding%20space%20and%20the%20CLIP%20text%20embedding%20space%20due%20to%20the%20degree%20of%20freedom.%0ATo%20mitigate%20the%20issue%2C%20we%20explore%20a%20scalar-wise%20regularization%20strategy.%0AFurthermore%2C%20to%20scale%20up%20the%20number%20of%20events%20and%20RGB%20data%20pairs%20for%20training%2C%0Awe%20also%20propose%20a%20pipeline%20for%20synthesizing%20event%20data%20from%20static%20RGB%20images.%0A%20%20Experimentally%2C%20our%20data%20synthesis%20strategy%20exhibits%20an%20attractive%20scaling%0Aproperty%2C%20and%20our%20method%20achieves%20superior%20zero-shot%20object%20recognition%0Aperformance%20on%20extensive%20standard%20benchmark%20datasets%2C%20even%20compared%20with%20past%0Asupervised%20learning%20approaches.%20For%20example%2C%20we%20achieve%2047.84%25%20zero-shot%0Aaccuracy%20on%20the%20N-ImageNet%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21616v1&entry.124074799=Read"},
{"title": "PIPsUS: Self-Supervised Point Tracking in Ultrasound", "author": "Wanwen Chen and Adam Schmidt and Eitan Prisman and Septimiu E Salcudean", "abstract": "  Finding point-level correspondences is a fundamental problem in ultrasound\n(US), since it can enable US landmark tracking for intraoperative image\nguidance in different surgeries, including head and neck. Most existing US\ntracking methods, e.g., those based on optical flow or feature matching, were\ninitially designed for RGB images before being applied to US. Therefore domain\nshift can impact their performance. Training could be supervised by\nground-truth correspondences, but these are expensive to acquire in US. To\nsolve these problems, we propose a self-supervised pixel-level tracking model\ncalled PIPsUS. Our model can track an arbitrary number of points in one forward\npass and exploits temporal information by considering multiple, instead of just\nconsecutive, frames. We developed a new self-supervised training strategy that\nutilizes a long-term point-tracking model trained for RGB images as a teacher\nto guide the model to learn realistic motions and use data augmentation to\nenforce tracking from US appearance. We evaluate our method on neck and oral US\nand echocardiography, showing higher point tracking accuracy when compared with\nfast normalized cross-correlation and tuned optical flow. Code will be\navailable once the paper is accepted.\n", "link": "http://arxiv.org/abs/2403.04969v2", "date": "2024-07-31", "relevancy": 2.5862, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5371}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5356}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PIPsUS%3A%20Self-Supervised%20Point%20Tracking%20in%20Ultrasound&body=Title%3A%20PIPsUS%3A%20Self-Supervised%20Point%20Tracking%20in%20Ultrasound%0AAuthor%3A%20Wanwen%20Chen%20and%20Adam%20Schmidt%20and%20Eitan%20Prisman%20and%20Septimiu%20E%20Salcudean%0AAbstract%3A%20%20%20Finding%20point-level%20correspondences%20is%20a%20fundamental%20problem%20in%20ultrasound%0A%28US%29%2C%20since%20it%20can%20enable%20US%20landmark%20tracking%20for%20intraoperative%20image%0Aguidance%20in%20different%20surgeries%2C%20including%20head%20and%20neck.%20Most%20existing%20US%0Atracking%20methods%2C%20e.g.%2C%20those%20based%20on%20optical%20flow%20or%20feature%20matching%2C%20were%0Ainitially%20designed%20for%20RGB%20images%20before%20being%20applied%20to%20US.%20Therefore%20domain%0Ashift%20can%20impact%20their%20performance.%20Training%20could%20be%20supervised%20by%0Aground-truth%20correspondences%2C%20but%20these%20are%20expensive%20to%20acquire%20in%20US.%20To%0Asolve%20these%20problems%2C%20we%20propose%20a%20self-supervised%20pixel-level%20tracking%20model%0Acalled%20PIPsUS.%20Our%20model%20can%20track%20an%20arbitrary%20number%20of%20points%20in%20one%20forward%0Apass%20and%20exploits%20temporal%20information%20by%20considering%20multiple%2C%20instead%20of%20just%0Aconsecutive%2C%20frames.%20We%20developed%20a%20new%20self-supervised%20training%20strategy%20that%0Autilizes%20a%20long-term%20point-tracking%20model%20trained%20for%20RGB%20images%20as%20a%20teacher%0Ato%20guide%20the%20model%20to%20learn%20realistic%20motions%20and%20use%20data%20augmentation%20to%0Aenforce%20tracking%20from%20US%20appearance.%20We%20evaluate%20our%20method%20on%20neck%20and%20oral%20US%0Aand%20echocardiography%2C%20showing%20higher%20point%20tracking%20accuracy%20when%20compared%20with%0Afast%20normalized%20cross-correlation%20and%20tuned%20optical%20flow.%20Code%20will%20be%0Aavailable%20once%20the%20paper%20is%20accepted.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04969v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPIPsUS%253A%2520Self-Supervised%2520Point%2520Tracking%2520in%2520Ultrasound%26entry.906535625%3DWanwen%2520Chen%2520and%2520Adam%2520Schmidt%2520and%2520Eitan%2520Prisman%2520and%2520Septimiu%2520E%2520Salcudean%26entry.1292438233%3D%2520%2520Finding%2520point-level%2520correspondences%2520is%2520a%2520fundamental%2520problem%2520in%2520ultrasound%250A%2528US%2529%252C%2520since%2520it%2520can%2520enable%2520US%2520landmark%2520tracking%2520for%2520intraoperative%2520image%250Aguidance%2520in%2520different%2520surgeries%252C%2520including%2520head%2520and%2520neck.%2520Most%2520existing%2520US%250Atracking%2520methods%252C%2520e.g.%252C%2520those%2520based%2520on%2520optical%2520flow%2520or%2520feature%2520matching%252C%2520were%250Ainitially%2520designed%2520for%2520RGB%2520images%2520before%2520being%2520applied%2520to%2520US.%2520Therefore%2520domain%250Ashift%2520can%2520impact%2520their%2520performance.%2520Training%2520could%2520be%2520supervised%2520by%250Aground-truth%2520correspondences%252C%2520but%2520these%2520are%2520expensive%2520to%2520acquire%2520in%2520US.%2520To%250Asolve%2520these%2520problems%252C%2520we%2520propose%2520a%2520self-supervised%2520pixel-level%2520tracking%2520model%250Acalled%2520PIPsUS.%2520Our%2520model%2520can%2520track%2520an%2520arbitrary%2520number%2520of%2520points%2520in%2520one%2520forward%250Apass%2520and%2520exploits%2520temporal%2520information%2520by%2520considering%2520multiple%252C%2520instead%2520of%2520just%250Aconsecutive%252C%2520frames.%2520We%2520developed%2520a%2520new%2520self-supervised%2520training%2520strategy%2520that%250Autilizes%2520a%2520long-term%2520point-tracking%2520model%2520trained%2520for%2520RGB%2520images%2520as%2520a%2520teacher%250Ato%2520guide%2520the%2520model%2520to%2520learn%2520realistic%2520motions%2520and%2520use%2520data%2520augmentation%2520to%250Aenforce%2520tracking%2520from%2520US%2520appearance.%2520We%2520evaluate%2520our%2520method%2520on%2520neck%2520and%2520oral%2520US%250Aand%2520echocardiography%252C%2520showing%2520higher%2520point%2520tracking%2520accuracy%2520when%2520compared%2520with%250Afast%2520normalized%2520cross-correlation%2520and%2520tuned%2520optical%2520flow.%2520Code%2520will%2520be%250Aavailable%2520once%2520the%2520paper%2520is%2520accepted.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.04969v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PIPsUS%3A%20Self-Supervised%20Point%20Tracking%20in%20Ultrasound&entry.906535625=Wanwen%20Chen%20and%20Adam%20Schmidt%20and%20Eitan%20Prisman%20and%20Septimiu%20E%20Salcudean&entry.1292438233=%20%20Finding%20point-level%20correspondences%20is%20a%20fundamental%20problem%20in%20ultrasound%0A%28US%29%2C%20since%20it%20can%20enable%20US%20landmark%20tracking%20for%20intraoperative%20image%0Aguidance%20in%20different%20surgeries%2C%20including%20head%20and%20neck.%20Most%20existing%20US%0Atracking%20methods%2C%20e.g.%2C%20those%20based%20on%20optical%20flow%20or%20feature%20matching%2C%20were%0Ainitially%20designed%20for%20RGB%20images%20before%20being%20applied%20to%20US.%20Therefore%20domain%0Ashift%20can%20impact%20their%20performance.%20Training%20could%20be%20supervised%20by%0Aground-truth%20correspondences%2C%20but%20these%20are%20expensive%20to%20acquire%20in%20US.%20To%0Asolve%20these%20problems%2C%20we%20propose%20a%20self-supervised%20pixel-level%20tracking%20model%0Acalled%20PIPsUS.%20Our%20model%20can%20track%20an%20arbitrary%20number%20of%20points%20in%20one%20forward%0Apass%20and%20exploits%20temporal%20information%20by%20considering%20multiple%2C%20instead%20of%20just%0Aconsecutive%2C%20frames.%20We%20developed%20a%20new%20self-supervised%20training%20strategy%20that%0Autilizes%20a%20long-term%20point-tracking%20model%20trained%20for%20RGB%20images%20as%20a%20teacher%0Ato%20guide%20the%20model%20to%20learn%20realistic%20motions%20and%20use%20data%20augmentation%20to%0Aenforce%20tracking%20from%20US%20appearance.%20We%20evaluate%20our%20method%20on%20neck%20and%20oral%20US%0Aand%20echocardiography%2C%20showing%20higher%20point%20tracking%20accuracy%20when%20compared%20with%0Afast%20normalized%20cross-correlation%20and%20tuned%20optical%20flow.%20Code%20will%20be%0Aavailable%20once%20the%20paper%20is%20accepted.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04969v2&entry.124074799=Read"},
{"title": "Enhancing Partially Spoofed Audio Localization with Boundary-aware\n  Attention Mechanism", "author": "Jiafeng Zhong and Bin Li and Jiangyan Yi", "abstract": "  The task of partially spoofed audio localization aims to accurately determine\naudio authenticity at a frame level. Although some works have achieved\nencouraging results, utilizing boundary information within a single model\nremains an unexplored research topic. In this work, we propose a novel method\ncalled Boundary-aware Attention Mechanism (BAM). Specifically, it consists of\ntwo core modules: Boundary Enhancement and Boundary Frame-wise Attention. The\nformer assembles the intra-frame and inter-frame information to extract\ndiscriminative boundary features that are subsequently used for boundary\nposition detection and authenticity decision, while the latter leverages\nboundary prediction results to explicitly control the feature interaction\nbetween frames, which achieves effective discrimination between real and fake\nframes. Experimental results on PartialSpoof database demonstrate our proposed\nmethod achieves the best performance. The code is available at\nhttps://github.com/media-sec-lab/BAM.\n", "link": "http://arxiv.org/abs/2407.21611v1", "date": "2024-07-31", "relevancy": 2.4963, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5607}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.469}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Partially%20Spoofed%20Audio%20Localization%20with%20Boundary-aware%0A%20%20Attention%20Mechanism&body=Title%3A%20Enhancing%20Partially%20Spoofed%20Audio%20Localization%20with%20Boundary-aware%0A%20%20Attention%20Mechanism%0AAuthor%3A%20Jiafeng%20Zhong%20and%20Bin%20Li%20and%20Jiangyan%20Yi%0AAbstract%3A%20%20%20The%20task%20of%20partially%20spoofed%20audio%20localization%20aims%20to%20accurately%20determine%0Aaudio%20authenticity%20at%20a%20frame%20level.%20Although%20some%20works%20have%20achieved%0Aencouraging%20results%2C%20utilizing%20boundary%20information%20within%20a%20single%20model%0Aremains%20an%20unexplored%20research%20topic.%20In%20this%20work%2C%20we%20propose%20a%20novel%20method%0Acalled%20Boundary-aware%20Attention%20Mechanism%20%28BAM%29.%20Specifically%2C%20it%20consists%20of%0Atwo%20core%20modules%3A%20Boundary%20Enhancement%20and%20Boundary%20Frame-wise%20Attention.%20The%0Aformer%20assembles%20the%20intra-frame%20and%20inter-frame%20information%20to%20extract%0Adiscriminative%20boundary%20features%20that%20are%20subsequently%20used%20for%20boundary%0Aposition%20detection%20and%20authenticity%20decision%2C%20while%20the%20latter%20leverages%0Aboundary%20prediction%20results%20to%20explicitly%20control%20the%20feature%20interaction%0Abetween%20frames%2C%20which%20achieves%20effective%20discrimination%20between%20real%20and%20fake%0Aframes.%20Experimental%20results%20on%20PartialSpoof%20database%20demonstrate%20our%20proposed%0Amethod%20achieves%20the%20best%20performance.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/media-sec-lab/BAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21611v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Partially%2520Spoofed%2520Audio%2520Localization%2520with%2520Boundary-aware%250A%2520%2520Attention%2520Mechanism%26entry.906535625%3DJiafeng%2520Zhong%2520and%2520Bin%2520Li%2520and%2520Jiangyan%2520Yi%26entry.1292438233%3D%2520%2520The%2520task%2520of%2520partially%2520spoofed%2520audio%2520localization%2520aims%2520to%2520accurately%2520determine%250Aaudio%2520authenticity%2520at%2520a%2520frame%2520level.%2520Although%2520some%2520works%2520have%2520achieved%250Aencouraging%2520results%252C%2520utilizing%2520boundary%2520information%2520within%2520a%2520single%2520model%250Aremains%2520an%2520unexplored%2520research%2520topic.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520method%250Acalled%2520Boundary-aware%2520Attention%2520Mechanism%2520%2528BAM%2529.%2520Specifically%252C%2520it%2520consists%2520of%250Atwo%2520core%2520modules%253A%2520Boundary%2520Enhancement%2520and%2520Boundary%2520Frame-wise%2520Attention.%2520The%250Aformer%2520assembles%2520the%2520intra-frame%2520and%2520inter-frame%2520information%2520to%2520extract%250Adiscriminative%2520boundary%2520features%2520that%2520are%2520subsequently%2520used%2520for%2520boundary%250Aposition%2520detection%2520and%2520authenticity%2520decision%252C%2520while%2520the%2520latter%2520leverages%250Aboundary%2520prediction%2520results%2520to%2520explicitly%2520control%2520the%2520feature%2520interaction%250Abetween%2520frames%252C%2520which%2520achieves%2520effective%2520discrimination%2520between%2520real%2520and%2520fake%250Aframes.%2520Experimental%2520results%2520on%2520PartialSpoof%2520database%2520demonstrate%2520our%2520proposed%250Amethod%2520achieves%2520the%2520best%2520performance.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/media-sec-lab/BAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21611v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Partially%20Spoofed%20Audio%20Localization%20with%20Boundary-aware%0A%20%20Attention%20Mechanism&entry.906535625=Jiafeng%20Zhong%20and%20Bin%20Li%20and%20Jiangyan%20Yi&entry.1292438233=%20%20The%20task%20of%20partially%20spoofed%20audio%20localization%20aims%20to%20accurately%20determine%0Aaudio%20authenticity%20at%20a%20frame%20level.%20Although%20some%20works%20have%20achieved%0Aencouraging%20results%2C%20utilizing%20boundary%20information%20within%20a%20single%20model%0Aremains%20an%20unexplored%20research%20topic.%20In%20this%20work%2C%20we%20propose%20a%20novel%20method%0Acalled%20Boundary-aware%20Attention%20Mechanism%20%28BAM%29.%20Specifically%2C%20it%20consists%20of%0Atwo%20core%20modules%3A%20Boundary%20Enhancement%20and%20Boundary%20Frame-wise%20Attention.%20The%0Aformer%20assembles%20the%20intra-frame%20and%20inter-frame%20information%20to%20extract%0Adiscriminative%20boundary%20features%20that%20are%20subsequently%20used%20for%20boundary%0Aposition%20detection%20and%20authenticity%20decision%2C%20while%20the%20latter%20leverages%0Aboundary%20prediction%20results%20to%20explicitly%20control%20the%20feature%20interaction%0Abetween%20frames%2C%20which%20achieves%20effective%20discrimination%20between%20real%20and%20fake%0Aframes.%20Experimental%20results%20on%20PartialSpoof%20database%20demonstrate%20our%20proposed%0Amethod%20achieves%20the%20best%20performance.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/media-sec-lab/BAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21611v1&entry.124074799=Read"},
{"title": "Transferring to Real-World Layouts: A Depth-aware Framework for Scene\n  Adaptation", "author": "Mu Chen and Zhedong Zheng and Yi Yang", "abstract": "  Scene segmentation via unsupervised domain adaptation (UDA) enables the\ntransfer of knowledge acquired from source synthetic data to real-world target\ndata, which largely reduces the need for manual pixel-level annotations in the\ntarget domain. To facilitate domain-invariant feature learning, existing\nmethods typically mix data from both the source domain and target domain by\nsimply copying and pasting the pixels. Such vanilla methods are usually\nsub-optimal since they do not take into account how well the mixed layouts\ncorrespond to real-world scenarios. Real-world scenarios are with an inherent\nlayout. We observe that semantic categories, such as sidewalks, buildings, and\nsky, display relatively consistent depth distributions, and could be clearly\ndistinguished in a depth map. Based on such observation, we propose a\ndepth-aware framework to explicitly leverage depth estimation to mix the\ncategories and facilitate the two complementary tasks, i.e., segmentation and\ndepth learning in an end-to-end manner. In particular, the framework contains a\nDepth-guided Contextual Filter (DCF) forndata augmentation and a cross-task\nencoder for contextual learning. DCF simulates the real-world layouts, while\nthe cross-task encoder further adaptively fuses the complementing features\nbetween two tasks. Besides, it is worth noting that several public datasets do\nnot provide depth annotation. Therefore, we leverage the off-the-shelf depth\nestimation network to generate the pseudo depth. Extensive experiments show\nthat our proposed methods, even with pseudo depth, achieve competitive\nperformance on two widely-used bench-marks, i.e. 77.7 mIoU on GTA to Cityscapes\nand 69.3 mIoU on Synthia to Cityscapes.\n", "link": "http://arxiv.org/abs/2311.12682v2", "date": "2024-07-31", "relevancy": 2.4947, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6515}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6161}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transferring%20to%20Real-World%20Layouts%3A%20A%20Depth-aware%20Framework%20for%20Scene%0A%20%20Adaptation&body=Title%3A%20Transferring%20to%20Real-World%20Layouts%3A%20A%20Depth-aware%20Framework%20for%20Scene%0A%20%20Adaptation%0AAuthor%3A%20Mu%20Chen%20and%20Zhedong%20Zheng%20and%20Yi%20Yang%0AAbstract%3A%20%20%20Scene%20segmentation%20via%20unsupervised%20domain%20adaptation%20%28UDA%29%20enables%20the%0Atransfer%20of%20knowledge%20acquired%20from%20source%20synthetic%20data%20to%20real-world%20target%0Adata%2C%20which%20largely%20reduces%20the%20need%20for%20manual%20pixel-level%20annotations%20in%20the%0Atarget%20domain.%20To%20facilitate%20domain-invariant%20feature%20learning%2C%20existing%0Amethods%20typically%20mix%20data%20from%20both%20the%20source%20domain%20and%20target%20domain%20by%0Asimply%20copying%20and%20pasting%20the%20pixels.%20Such%20vanilla%20methods%20are%20usually%0Asub-optimal%20since%20they%20do%20not%20take%20into%20account%20how%20well%20the%20mixed%20layouts%0Acorrespond%20to%20real-world%20scenarios.%20Real-world%20scenarios%20are%20with%20an%20inherent%0Alayout.%20We%20observe%20that%20semantic%20categories%2C%20such%20as%20sidewalks%2C%20buildings%2C%20and%0Asky%2C%20display%20relatively%20consistent%20depth%20distributions%2C%20and%20could%20be%20clearly%0Adistinguished%20in%20a%20depth%20map.%20Based%20on%20such%20observation%2C%20we%20propose%20a%0Adepth-aware%20framework%20to%20explicitly%20leverage%20depth%20estimation%20to%20mix%20the%0Acategories%20and%20facilitate%20the%20two%20complementary%20tasks%2C%20i.e.%2C%20segmentation%20and%0Adepth%20learning%20in%20an%20end-to-end%20manner.%20In%20particular%2C%20the%20framework%20contains%20a%0ADepth-guided%20Contextual%20Filter%20%28DCF%29%20forndata%20augmentation%20and%20a%20cross-task%0Aencoder%20for%20contextual%20learning.%20DCF%20simulates%20the%20real-world%20layouts%2C%20while%0Athe%20cross-task%20encoder%20further%20adaptively%20fuses%20the%20complementing%20features%0Abetween%20two%20tasks.%20Besides%2C%20it%20is%20worth%20noting%20that%20several%20public%20datasets%20do%0Anot%20provide%20depth%20annotation.%20Therefore%2C%20we%20leverage%20the%20off-the-shelf%20depth%0Aestimation%20network%20to%20generate%20the%20pseudo%20depth.%20Extensive%20experiments%20show%0Athat%20our%20proposed%20methods%2C%20even%20with%20pseudo%20depth%2C%20achieve%20competitive%0Aperformance%20on%20two%20widely-used%20bench-marks%2C%20i.e.%2077.7%20mIoU%20on%20GTA%20to%20Cityscapes%0Aand%2069.3%20mIoU%20on%20Synthia%20to%20Cityscapes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.12682v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransferring%2520to%2520Real-World%2520Layouts%253A%2520A%2520Depth-aware%2520Framework%2520for%2520Scene%250A%2520%2520Adaptation%26entry.906535625%3DMu%2520Chen%2520and%2520Zhedong%2520Zheng%2520and%2520Yi%2520Yang%26entry.1292438233%3D%2520%2520Scene%2520segmentation%2520via%2520unsupervised%2520domain%2520adaptation%2520%2528UDA%2529%2520enables%2520the%250Atransfer%2520of%2520knowledge%2520acquired%2520from%2520source%2520synthetic%2520data%2520to%2520real-world%2520target%250Adata%252C%2520which%2520largely%2520reduces%2520the%2520need%2520for%2520manual%2520pixel-level%2520annotations%2520in%2520the%250Atarget%2520domain.%2520To%2520facilitate%2520domain-invariant%2520feature%2520learning%252C%2520existing%250Amethods%2520typically%2520mix%2520data%2520from%2520both%2520the%2520source%2520domain%2520and%2520target%2520domain%2520by%250Asimply%2520copying%2520and%2520pasting%2520the%2520pixels.%2520Such%2520vanilla%2520methods%2520are%2520usually%250Asub-optimal%2520since%2520they%2520do%2520not%2520take%2520into%2520account%2520how%2520well%2520the%2520mixed%2520layouts%250Acorrespond%2520to%2520real-world%2520scenarios.%2520Real-world%2520scenarios%2520are%2520with%2520an%2520inherent%250Alayout.%2520We%2520observe%2520that%2520semantic%2520categories%252C%2520such%2520as%2520sidewalks%252C%2520buildings%252C%2520and%250Asky%252C%2520display%2520relatively%2520consistent%2520depth%2520distributions%252C%2520and%2520could%2520be%2520clearly%250Adistinguished%2520in%2520a%2520depth%2520map.%2520Based%2520on%2520such%2520observation%252C%2520we%2520propose%2520a%250Adepth-aware%2520framework%2520to%2520explicitly%2520leverage%2520depth%2520estimation%2520to%2520mix%2520the%250Acategories%2520and%2520facilitate%2520the%2520two%2520complementary%2520tasks%252C%2520i.e.%252C%2520segmentation%2520and%250Adepth%2520learning%2520in%2520an%2520end-to-end%2520manner.%2520In%2520particular%252C%2520the%2520framework%2520contains%2520a%250ADepth-guided%2520Contextual%2520Filter%2520%2528DCF%2529%2520forndata%2520augmentation%2520and%2520a%2520cross-task%250Aencoder%2520for%2520contextual%2520learning.%2520DCF%2520simulates%2520the%2520real-world%2520layouts%252C%2520while%250Athe%2520cross-task%2520encoder%2520further%2520adaptively%2520fuses%2520the%2520complementing%2520features%250Abetween%2520two%2520tasks.%2520Besides%252C%2520it%2520is%2520worth%2520noting%2520that%2520several%2520public%2520datasets%2520do%250Anot%2520provide%2520depth%2520annotation.%2520Therefore%252C%2520we%2520leverage%2520the%2520off-the-shelf%2520depth%250Aestimation%2520network%2520to%2520generate%2520the%2520pseudo%2520depth.%2520Extensive%2520experiments%2520show%250Athat%2520our%2520proposed%2520methods%252C%2520even%2520with%2520pseudo%2520depth%252C%2520achieve%2520competitive%250Aperformance%2520on%2520two%2520widely-used%2520bench-marks%252C%2520i.e.%252077.7%2520mIoU%2520on%2520GTA%2520to%2520Cityscapes%250Aand%252069.3%2520mIoU%2520on%2520Synthia%2520to%2520Cityscapes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.12682v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transferring%20to%20Real-World%20Layouts%3A%20A%20Depth-aware%20Framework%20for%20Scene%0A%20%20Adaptation&entry.906535625=Mu%20Chen%20and%20Zhedong%20Zheng%20and%20Yi%20Yang&entry.1292438233=%20%20Scene%20segmentation%20via%20unsupervised%20domain%20adaptation%20%28UDA%29%20enables%20the%0Atransfer%20of%20knowledge%20acquired%20from%20source%20synthetic%20data%20to%20real-world%20target%0Adata%2C%20which%20largely%20reduces%20the%20need%20for%20manual%20pixel-level%20annotations%20in%20the%0Atarget%20domain.%20To%20facilitate%20domain-invariant%20feature%20learning%2C%20existing%0Amethods%20typically%20mix%20data%20from%20both%20the%20source%20domain%20and%20target%20domain%20by%0Asimply%20copying%20and%20pasting%20the%20pixels.%20Such%20vanilla%20methods%20are%20usually%0Asub-optimal%20since%20they%20do%20not%20take%20into%20account%20how%20well%20the%20mixed%20layouts%0Acorrespond%20to%20real-world%20scenarios.%20Real-world%20scenarios%20are%20with%20an%20inherent%0Alayout.%20We%20observe%20that%20semantic%20categories%2C%20such%20as%20sidewalks%2C%20buildings%2C%20and%0Asky%2C%20display%20relatively%20consistent%20depth%20distributions%2C%20and%20could%20be%20clearly%0Adistinguished%20in%20a%20depth%20map.%20Based%20on%20such%20observation%2C%20we%20propose%20a%0Adepth-aware%20framework%20to%20explicitly%20leverage%20depth%20estimation%20to%20mix%20the%0Acategories%20and%20facilitate%20the%20two%20complementary%20tasks%2C%20i.e.%2C%20segmentation%20and%0Adepth%20learning%20in%20an%20end-to-end%20manner.%20In%20particular%2C%20the%20framework%20contains%20a%0ADepth-guided%20Contextual%20Filter%20%28DCF%29%20forndata%20augmentation%20and%20a%20cross-task%0Aencoder%20for%20contextual%20learning.%20DCF%20simulates%20the%20real-world%20layouts%2C%20while%0Athe%20cross-task%20encoder%20further%20adaptively%20fuses%20the%20complementing%20features%0Abetween%20two%20tasks.%20Besides%2C%20it%20is%20worth%20noting%20that%20several%20public%20datasets%20do%0Anot%20provide%20depth%20annotation.%20Therefore%2C%20we%20leverage%20the%20off-the-shelf%20depth%0Aestimation%20network%20to%20generate%20the%20pseudo%20depth.%20Extensive%20experiments%20show%0Athat%20our%20proposed%20methods%2C%20even%20with%20pseudo%20depth%2C%20achieve%20competitive%0Aperformance%20on%20two%20widely-used%20bench-marks%2C%20i.e.%2077.7%20mIoU%20on%20GTA%20to%20Cityscapes%0Aand%2069.3%20mIoU%20on%20Synthia%20to%20Cityscapes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.12682v2&entry.124074799=Read"},
{"title": "Voxel Scene Graph for Intracranial Hemorrhage", "author": "Antoine P. Sanner and Nils F. Grauhan and Marc A. Brockmann and Ahmed E. Othman and Anirban Mukhopadhyay", "abstract": "  Patients with Intracranial Hemorrhage (ICH) face a potentially\nlife-threatening condition, and patient-centered individualized treatment\nremains challenging due to possible clinical complications. Deep-Learning-based\nmethods can efficiently analyze the routinely acquired head CTs to support the\nclinical decision-making. The majority of early work focuses on the detection\nand segmentation of ICH, but do not model the complex relations between ICH and\nadjacent brain structures. In this work, we design a tailored object detection\nmethod for ICH, which we unite with segmentation-grounded Scene Graph\nGeneration (SGG) methods to learn a holistic representation of the clinical\ncerebral scene. To the best of our knowledge, this is the first application of\nSGG for 3D voxel images. We evaluate our method on two head-CT datasets and\ndemonstrate that our model can recall up to 74% of clinically relevant\nrelations. This work lays the foundation towards SGG for 3D voxel data. The\ngenerated Scene Graphs can already provide insights for the clinician, but are\nalso valuable for all downstream tasks as a compact and interpretable\nrepresentation.\n", "link": "http://arxiv.org/abs/2407.21580v1", "date": "2024-07-31", "relevancy": 2.4721, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5175}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4829}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4829}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Voxel%20Scene%20Graph%20for%20Intracranial%20Hemorrhage&body=Title%3A%20Voxel%20Scene%20Graph%20for%20Intracranial%20Hemorrhage%0AAuthor%3A%20Antoine%20P.%20Sanner%20and%20Nils%20F.%20Grauhan%20and%20Marc%20A.%20Brockmann%20and%20Ahmed%20E.%20Othman%20and%20Anirban%20Mukhopadhyay%0AAbstract%3A%20%20%20Patients%20with%20Intracranial%20Hemorrhage%20%28ICH%29%20face%20a%20potentially%0Alife-threatening%20condition%2C%20and%20patient-centered%20individualized%20treatment%0Aremains%20challenging%20due%20to%20possible%20clinical%20complications.%20Deep-Learning-based%0Amethods%20can%20efficiently%20analyze%20the%20routinely%20acquired%20head%20CTs%20to%20support%20the%0Aclinical%20decision-making.%20The%20majority%20of%20early%20work%20focuses%20on%20the%20detection%0Aand%20segmentation%20of%20ICH%2C%20but%20do%20not%20model%20the%20complex%20relations%20between%20ICH%20and%0Aadjacent%20brain%20structures.%20In%20this%20work%2C%20we%20design%20a%20tailored%20object%20detection%0Amethod%20for%20ICH%2C%20which%20we%20unite%20with%20segmentation-grounded%20Scene%20Graph%0AGeneration%20%28SGG%29%20methods%20to%20learn%20a%20holistic%20representation%20of%20the%20clinical%0Acerebral%20scene.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20application%20of%0ASGG%20for%203D%20voxel%20images.%20We%20evaluate%20our%20method%20on%20two%20head-CT%20datasets%20and%0Ademonstrate%20that%20our%20model%20can%20recall%20up%20to%2074%25%20of%20clinically%20relevant%0Arelations.%20This%20work%20lays%20the%20foundation%20towards%20SGG%20for%203D%20voxel%20data.%20The%0Agenerated%20Scene%20Graphs%20can%20already%20provide%20insights%20for%20the%20clinician%2C%20but%20are%0Aalso%20valuable%20for%20all%20downstream%20tasks%20as%20a%20compact%20and%20interpretable%0Arepresentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21580v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVoxel%2520Scene%2520Graph%2520for%2520Intracranial%2520Hemorrhage%26entry.906535625%3DAntoine%2520P.%2520Sanner%2520and%2520Nils%2520F.%2520Grauhan%2520and%2520Marc%2520A.%2520Brockmann%2520and%2520Ahmed%2520E.%2520Othman%2520and%2520Anirban%2520Mukhopadhyay%26entry.1292438233%3D%2520%2520Patients%2520with%2520Intracranial%2520Hemorrhage%2520%2528ICH%2529%2520face%2520a%2520potentially%250Alife-threatening%2520condition%252C%2520and%2520patient-centered%2520individualized%2520treatment%250Aremains%2520challenging%2520due%2520to%2520possible%2520clinical%2520complications.%2520Deep-Learning-based%250Amethods%2520can%2520efficiently%2520analyze%2520the%2520routinely%2520acquired%2520head%2520CTs%2520to%2520support%2520the%250Aclinical%2520decision-making.%2520The%2520majority%2520of%2520early%2520work%2520focuses%2520on%2520the%2520detection%250Aand%2520segmentation%2520of%2520ICH%252C%2520but%2520do%2520not%2520model%2520the%2520complex%2520relations%2520between%2520ICH%2520and%250Aadjacent%2520brain%2520structures.%2520In%2520this%2520work%252C%2520we%2520design%2520a%2520tailored%2520object%2520detection%250Amethod%2520for%2520ICH%252C%2520which%2520we%2520unite%2520with%2520segmentation-grounded%2520Scene%2520Graph%250AGeneration%2520%2528SGG%2529%2520methods%2520to%2520learn%2520a%2520holistic%2520representation%2520of%2520the%2520clinical%250Acerebral%2520scene.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520application%2520of%250ASGG%2520for%25203D%2520voxel%2520images.%2520We%2520evaluate%2520our%2520method%2520on%2520two%2520head-CT%2520datasets%2520and%250Ademonstrate%2520that%2520our%2520model%2520can%2520recall%2520up%2520to%252074%2525%2520of%2520clinically%2520relevant%250Arelations.%2520This%2520work%2520lays%2520the%2520foundation%2520towards%2520SGG%2520for%25203D%2520voxel%2520data.%2520The%250Agenerated%2520Scene%2520Graphs%2520can%2520already%2520provide%2520insights%2520for%2520the%2520clinician%252C%2520but%2520are%250Aalso%2520valuable%2520for%2520all%2520downstream%2520tasks%2520as%2520a%2520compact%2520and%2520interpretable%250Arepresentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21580v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Voxel%20Scene%20Graph%20for%20Intracranial%20Hemorrhage&entry.906535625=Antoine%20P.%20Sanner%20and%20Nils%20F.%20Grauhan%20and%20Marc%20A.%20Brockmann%20and%20Ahmed%20E.%20Othman%20and%20Anirban%20Mukhopadhyay&entry.1292438233=%20%20Patients%20with%20Intracranial%20Hemorrhage%20%28ICH%29%20face%20a%20potentially%0Alife-threatening%20condition%2C%20and%20patient-centered%20individualized%20treatment%0Aremains%20challenging%20due%20to%20possible%20clinical%20complications.%20Deep-Learning-based%0Amethods%20can%20efficiently%20analyze%20the%20routinely%20acquired%20head%20CTs%20to%20support%20the%0Aclinical%20decision-making.%20The%20majority%20of%20early%20work%20focuses%20on%20the%20detection%0Aand%20segmentation%20of%20ICH%2C%20but%20do%20not%20model%20the%20complex%20relations%20between%20ICH%20and%0Aadjacent%20brain%20structures.%20In%20this%20work%2C%20we%20design%20a%20tailored%20object%20detection%0Amethod%20for%20ICH%2C%20which%20we%20unite%20with%20segmentation-grounded%20Scene%20Graph%0AGeneration%20%28SGG%29%20methods%20to%20learn%20a%20holistic%20representation%20of%20the%20clinical%0Acerebral%20scene.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20application%20of%0ASGG%20for%203D%20voxel%20images.%20We%20evaluate%20our%20method%20on%20two%20head-CT%20datasets%20and%0Ademonstrate%20that%20our%20model%20can%20recall%20up%20to%2074%25%20of%20clinically%20relevant%0Arelations.%20This%20work%20lays%20the%20foundation%20towards%20SGG%20for%203D%20voxel%20data.%20The%0Agenerated%20Scene%20Graphs%20can%20already%20provide%20insights%20for%20the%20clinician%2C%20but%20are%0Aalso%20valuable%20for%20all%20downstream%20tasks%20as%20a%20compact%20and%20interpretable%0Arepresentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21580v1&entry.124074799=Read"},
{"title": "TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue\n  System with Transfer Capabilities", "author": "Ming Zhang and Caishuang Huang and Yilong Wu and Shichun Liu and Huiyuan Zheng and Yurui Dong and Yujiong Shen and Shihan Dou and Jun Zhao and Junjie Ye and Qi Zhang and Tao Gui and Xuanjing Huang", "abstract": "  Task-oriented dialogue (TOD) systems aim to efficiently handle task-oriented\nconversations, including information gathering. How to utilize ToD accurately,\nefficiently and effectively for information gathering has always been a\ncritical and challenging task. Recent studies have demonstrated that Large\nLanguage Models (LLMs) excel in dialogue, instruction generation, and\nreasoning, and can significantly enhance the performance of TOD through\nfine-tuning. However, current datasets primarily cater to user-led systems and\nare limited to predefined specific scenarios and slots, thereby necessitating\nimprovements in the proactiveness, diversity, and capabilities of TOD. In this\nstudy, we present a detailed multi-domain task-oriented data construction\nprocess for conversations, and a Chinese dialogue dataset generated based on\nthis process, \\textbf{TransferTOD}, which authentically simulates human-machine\ndialogues in 30 popular life service scenarios. Leveraging this dataset, we\ntrained a \\textbf{TransferTOD-7B} model using full-parameter fine-tuning,\nshowcasing notable abilities in slot filling and questioning. Our work has\ndemonstrated its strong generalization capabilities in various downstream\nscenarios, significantly enhancing both data utilization efficiency and system\nperformance. The data is released in\nhttps://github.com/KongLongGeFDU/TransferTOD.\n", "link": "http://arxiv.org/abs/2407.21693v1", "date": "2024-07-31", "relevancy": 2.4653, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5303}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4835}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TransferTOD%3A%20A%20Generalizable%20Chinese%20Multi-Domain%20Task-Oriented%20Dialogue%0A%20%20System%20with%20Transfer%20Capabilities&body=Title%3A%20TransferTOD%3A%20A%20Generalizable%20Chinese%20Multi-Domain%20Task-Oriented%20Dialogue%0A%20%20System%20with%20Transfer%20Capabilities%0AAuthor%3A%20Ming%20Zhang%20and%20Caishuang%20Huang%20and%20Yilong%20Wu%20and%20Shichun%20Liu%20and%20Huiyuan%20Zheng%20and%20Yurui%20Dong%20and%20Yujiong%20Shen%20and%20Shihan%20Dou%20and%20Jun%20Zhao%20and%20Junjie%20Ye%20and%20Qi%20Zhang%20and%20Tao%20Gui%20and%20Xuanjing%20Huang%0AAbstract%3A%20%20%20Task-oriented%20dialogue%20%28TOD%29%20systems%20aim%20to%20efficiently%20handle%20task-oriented%0Aconversations%2C%20including%20information%20gathering.%20How%20to%20utilize%20ToD%20accurately%2C%0Aefficiently%20and%20effectively%20for%20information%20gathering%20has%20always%20been%20a%0Acritical%20and%20challenging%20task.%20Recent%20studies%20have%20demonstrated%20that%20Large%0ALanguage%20Models%20%28LLMs%29%20excel%20in%20dialogue%2C%20instruction%20generation%2C%20and%0Areasoning%2C%20and%20can%20significantly%20enhance%20the%20performance%20of%20TOD%20through%0Afine-tuning.%20However%2C%20current%20datasets%20primarily%20cater%20to%20user-led%20systems%20and%0Aare%20limited%20to%20predefined%20specific%20scenarios%20and%20slots%2C%20thereby%20necessitating%0Aimprovements%20in%20the%20proactiveness%2C%20diversity%2C%20and%20capabilities%20of%20TOD.%20In%20this%0Astudy%2C%20we%20present%20a%20detailed%20multi-domain%20task-oriented%20data%20construction%0Aprocess%20for%20conversations%2C%20and%20a%20Chinese%20dialogue%20dataset%20generated%20based%20on%0Athis%20process%2C%20%5Ctextbf%7BTransferTOD%7D%2C%20which%20authentically%20simulates%20human-machine%0Adialogues%20in%2030%20popular%20life%20service%20scenarios.%20Leveraging%20this%20dataset%2C%20we%0Atrained%20a%20%5Ctextbf%7BTransferTOD-7B%7D%20model%20using%20full-parameter%20fine-tuning%2C%0Ashowcasing%20notable%20abilities%20in%20slot%20filling%20and%20questioning.%20Our%20work%20has%0Ademonstrated%20its%20strong%20generalization%20capabilities%20in%20various%20downstream%0Ascenarios%2C%20significantly%20enhancing%20both%20data%20utilization%20efficiency%20and%20system%0Aperformance.%20The%20data%20is%20released%20in%0Ahttps%3A//github.com/KongLongGeFDU/TransferTOD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21693v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransferTOD%253A%2520A%2520Generalizable%2520Chinese%2520Multi-Domain%2520Task-Oriented%2520Dialogue%250A%2520%2520System%2520with%2520Transfer%2520Capabilities%26entry.906535625%3DMing%2520Zhang%2520and%2520Caishuang%2520Huang%2520and%2520Yilong%2520Wu%2520and%2520Shichun%2520Liu%2520and%2520Huiyuan%2520Zheng%2520and%2520Yurui%2520Dong%2520and%2520Yujiong%2520Shen%2520and%2520Shihan%2520Dou%2520and%2520Jun%2520Zhao%2520and%2520Junjie%2520Ye%2520and%2520Qi%2520Zhang%2520and%2520Tao%2520Gui%2520and%2520Xuanjing%2520Huang%26entry.1292438233%3D%2520%2520Task-oriented%2520dialogue%2520%2528TOD%2529%2520systems%2520aim%2520to%2520efficiently%2520handle%2520task-oriented%250Aconversations%252C%2520including%2520information%2520gathering.%2520How%2520to%2520utilize%2520ToD%2520accurately%252C%250Aefficiently%2520and%2520effectively%2520for%2520information%2520gathering%2520has%2520always%2520been%2520a%250Acritical%2520and%2520challenging%2520task.%2520Recent%2520studies%2520have%2520demonstrated%2520that%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520excel%2520in%2520dialogue%252C%2520instruction%2520generation%252C%2520and%250Areasoning%252C%2520and%2520can%2520significantly%2520enhance%2520the%2520performance%2520of%2520TOD%2520through%250Afine-tuning.%2520However%252C%2520current%2520datasets%2520primarily%2520cater%2520to%2520user-led%2520systems%2520and%250Aare%2520limited%2520to%2520predefined%2520specific%2520scenarios%2520and%2520slots%252C%2520thereby%2520necessitating%250Aimprovements%2520in%2520the%2520proactiveness%252C%2520diversity%252C%2520and%2520capabilities%2520of%2520TOD.%2520In%2520this%250Astudy%252C%2520we%2520present%2520a%2520detailed%2520multi-domain%2520task-oriented%2520data%2520construction%250Aprocess%2520for%2520conversations%252C%2520and%2520a%2520Chinese%2520dialogue%2520dataset%2520generated%2520based%2520on%250Athis%2520process%252C%2520%255Ctextbf%257BTransferTOD%257D%252C%2520which%2520authentically%2520simulates%2520human-machine%250Adialogues%2520in%252030%2520popular%2520life%2520service%2520scenarios.%2520Leveraging%2520this%2520dataset%252C%2520we%250Atrained%2520a%2520%255Ctextbf%257BTransferTOD-7B%257D%2520model%2520using%2520full-parameter%2520fine-tuning%252C%250Ashowcasing%2520notable%2520abilities%2520in%2520slot%2520filling%2520and%2520questioning.%2520Our%2520work%2520has%250Ademonstrated%2520its%2520strong%2520generalization%2520capabilities%2520in%2520various%2520downstream%250Ascenarios%252C%2520significantly%2520enhancing%2520both%2520data%2520utilization%2520efficiency%2520and%2520system%250Aperformance.%2520The%2520data%2520is%2520released%2520in%250Ahttps%253A//github.com/KongLongGeFDU/TransferTOD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21693v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TransferTOD%3A%20A%20Generalizable%20Chinese%20Multi-Domain%20Task-Oriented%20Dialogue%0A%20%20System%20with%20Transfer%20Capabilities&entry.906535625=Ming%20Zhang%20and%20Caishuang%20Huang%20and%20Yilong%20Wu%20and%20Shichun%20Liu%20and%20Huiyuan%20Zheng%20and%20Yurui%20Dong%20and%20Yujiong%20Shen%20and%20Shihan%20Dou%20and%20Jun%20Zhao%20and%20Junjie%20Ye%20and%20Qi%20Zhang%20and%20Tao%20Gui%20and%20Xuanjing%20Huang&entry.1292438233=%20%20Task-oriented%20dialogue%20%28TOD%29%20systems%20aim%20to%20efficiently%20handle%20task-oriented%0Aconversations%2C%20including%20information%20gathering.%20How%20to%20utilize%20ToD%20accurately%2C%0Aefficiently%20and%20effectively%20for%20information%20gathering%20has%20always%20been%20a%0Acritical%20and%20challenging%20task.%20Recent%20studies%20have%20demonstrated%20that%20Large%0ALanguage%20Models%20%28LLMs%29%20excel%20in%20dialogue%2C%20instruction%20generation%2C%20and%0Areasoning%2C%20and%20can%20significantly%20enhance%20the%20performance%20of%20TOD%20through%0Afine-tuning.%20However%2C%20current%20datasets%20primarily%20cater%20to%20user-led%20systems%20and%0Aare%20limited%20to%20predefined%20specific%20scenarios%20and%20slots%2C%20thereby%20necessitating%0Aimprovements%20in%20the%20proactiveness%2C%20diversity%2C%20and%20capabilities%20of%20TOD.%20In%20this%0Astudy%2C%20we%20present%20a%20detailed%20multi-domain%20task-oriented%20data%20construction%0Aprocess%20for%20conversations%2C%20and%20a%20Chinese%20dialogue%20dataset%20generated%20based%20on%0Athis%20process%2C%20%5Ctextbf%7BTransferTOD%7D%2C%20which%20authentically%20simulates%20human-machine%0Adialogues%20in%2030%20popular%20life%20service%20scenarios.%20Leveraging%20this%20dataset%2C%20we%0Atrained%20a%20%5Ctextbf%7BTransferTOD-7B%7D%20model%20using%20full-parameter%20fine-tuning%2C%0Ashowcasing%20notable%20abilities%20in%20slot%20filling%20and%20questioning.%20Our%20work%20has%0Ademonstrated%20its%20strong%20generalization%20capabilities%20in%20various%20downstream%0Ascenarios%2C%20significantly%20enhancing%20both%20data%20utilization%20efficiency%20and%20system%0Aperformance.%20The%20data%20is%20released%20in%0Ahttps%3A//github.com/KongLongGeFDU/TransferTOD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21693v1&entry.124074799=Read"},
{"title": "Spatial Transformer Network YOLO Model for Agricultural Object Detection", "author": "Yash Zambre and Ekdev Rajkitkul and Akshatha Mohan and Joshua Peeples", "abstract": "  Object detection plays a crucial role in the field of computer vision by\nautonomously identifying and locating objects of interest. The You Only Look\nOnce (YOLO) model is an effective single-shot detector. However, YOLO faces\nchallenges in cluttered or partially occluded scenes and can struggle with\nsmall, low-contrast objects. We propose a new method that integrates spatial\ntransformer networks (STNs) into YOLO to improve performance. The proposed\nSTN-YOLO aims to enhance the model's effectiveness by focusing on important\nareas of the image and improving the spatial invariance of the model before the\ndetection process. Our proposed method improved object detection performance\nboth qualitatively and quantitatively. We explore the impact of different\nlocalization networks within the STN module as well as the robustness of the\nmodel across different spatial transformations. We apply the STN-YOLO on\nbenchmark datasets for Agricultural object detection as well as a new dataset\nfrom a state-of-the-art plant phenotyping greenhouse facility. Our code and\ndataset are publicly available.\n", "link": "http://arxiv.org/abs/2407.21652v1", "date": "2024-07-31", "relevancy": 2.4541, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4969}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4902}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4854}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial%20Transformer%20Network%20YOLO%20Model%20for%20Agricultural%20Object%20Detection&body=Title%3A%20Spatial%20Transformer%20Network%20YOLO%20Model%20for%20Agricultural%20Object%20Detection%0AAuthor%3A%20Yash%20Zambre%20and%20Ekdev%20Rajkitkul%20and%20Akshatha%20Mohan%20and%20Joshua%20Peeples%0AAbstract%3A%20%20%20Object%20detection%20plays%20a%20crucial%20role%20in%20the%20field%20of%20computer%20vision%20by%0Aautonomously%20identifying%20and%20locating%20objects%20of%20interest.%20The%20You%20Only%20Look%0AOnce%20%28YOLO%29%20model%20is%20an%20effective%20single-shot%20detector.%20However%2C%20YOLO%20faces%0Achallenges%20in%20cluttered%20or%20partially%20occluded%20scenes%20and%20can%20struggle%20with%0Asmall%2C%20low-contrast%20objects.%20We%20propose%20a%20new%20method%20that%20integrates%20spatial%0Atransformer%20networks%20%28STNs%29%20into%20YOLO%20to%20improve%20performance.%20The%20proposed%0ASTN-YOLO%20aims%20to%20enhance%20the%20model%27s%20effectiveness%20by%20focusing%20on%20important%0Aareas%20of%20the%20image%20and%20improving%20the%20spatial%20invariance%20of%20the%20model%20before%20the%0Adetection%20process.%20Our%20proposed%20method%20improved%20object%20detection%20performance%0Aboth%20qualitatively%20and%20quantitatively.%20We%20explore%20the%20impact%20of%20different%0Alocalization%20networks%20within%20the%20STN%20module%20as%20well%20as%20the%20robustness%20of%20the%0Amodel%20across%20different%20spatial%20transformations.%20We%20apply%20the%20STN-YOLO%20on%0Abenchmark%20datasets%20for%20Agricultural%20object%20detection%20as%20well%20as%20a%20new%20dataset%0Afrom%20a%20state-of-the-art%20plant%20phenotyping%20greenhouse%20facility.%20Our%20code%20and%0Adataset%20are%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21652v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial%2520Transformer%2520Network%2520YOLO%2520Model%2520for%2520Agricultural%2520Object%2520Detection%26entry.906535625%3DYash%2520Zambre%2520and%2520Ekdev%2520Rajkitkul%2520and%2520Akshatha%2520Mohan%2520and%2520Joshua%2520Peeples%26entry.1292438233%3D%2520%2520Object%2520detection%2520plays%2520a%2520crucial%2520role%2520in%2520the%2520field%2520of%2520computer%2520vision%2520by%250Aautonomously%2520identifying%2520and%2520locating%2520objects%2520of%2520interest.%2520The%2520You%2520Only%2520Look%250AOnce%2520%2528YOLO%2529%2520model%2520is%2520an%2520effective%2520single-shot%2520detector.%2520However%252C%2520YOLO%2520faces%250Achallenges%2520in%2520cluttered%2520or%2520partially%2520occluded%2520scenes%2520and%2520can%2520struggle%2520with%250Asmall%252C%2520low-contrast%2520objects.%2520We%2520propose%2520a%2520new%2520method%2520that%2520integrates%2520spatial%250Atransformer%2520networks%2520%2528STNs%2529%2520into%2520YOLO%2520to%2520improve%2520performance.%2520The%2520proposed%250ASTN-YOLO%2520aims%2520to%2520enhance%2520the%2520model%2527s%2520effectiveness%2520by%2520focusing%2520on%2520important%250Aareas%2520of%2520the%2520image%2520and%2520improving%2520the%2520spatial%2520invariance%2520of%2520the%2520model%2520before%2520the%250Adetection%2520process.%2520Our%2520proposed%2520method%2520improved%2520object%2520detection%2520performance%250Aboth%2520qualitatively%2520and%2520quantitatively.%2520We%2520explore%2520the%2520impact%2520of%2520different%250Alocalization%2520networks%2520within%2520the%2520STN%2520module%2520as%2520well%2520as%2520the%2520robustness%2520of%2520the%250Amodel%2520across%2520different%2520spatial%2520transformations.%2520We%2520apply%2520the%2520STN-YOLO%2520on%250Abenchmark%2520datasets%2520for%2520Agricultural%2520object%2520detection%2520as%2520well%2520as%2520a%2520new%2520dataset%250Afrom%2520a%2520state-of-the-art%2520plant%2520phenotyping%2520greenhouse%2520facility.%2520Our%2520code%2520and%250Adataset%2520are%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21652v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial%20Transformer%20Network%20YOLO%20Model%20for%20Agricultural%20Object%20Detection&entry.906535625=Yash%20Zambre%20and%20Ekdev%20Rajkitkul%20and%20Akshatha%20Mohan%20and%20Joshua%20Peeples&entry.1292438233=%20%20Object%20detection%20plays%20a%20crucial%20role%20in%20the%20field%20of%20computer%20vision%20by%0Aautonomously%20identifying%20and%20locating%20objects%20of%20interest.%20The%20You%20Only%20Look%0AOnce%20%28YOLO%29%20model%20is%20an%20effective%20single-shot%20detector.%20However%2C%20YOLO%20faces%0Achallenges%20in%20cluttered%20or%20partially%20occluded%20scenes%20and%20can%20struggle%20with%0Asmall%2C%20low-contrast%20objects.%20We%20propose%20a%20new%20method%20that%20integrates%20spatial%0Atransformer%20networks%20%28STNs%29%20into%20YOLO%20to%20improve%20performance.%20The%20proposed%0ASTN-YOLO%20aims%20to%20enhance%20the%20model%27s%20effectiveness%20by%20focusing%20on%20important%0Aareas%20of%20the%20image%20and%20improving%20the%20spatial%20invariance%20of%20the%20model%20before%20the%0Adetection%20process.%20Our%20proposed%20method%20improved%20object%20detection%20performance%0Aboth%20qualitatively%20and%20quantitatively.%20We%20explore%20the%20impact%20of%20different%0Alocalization%20networks%20within%20the%20STN%20module%20as%20well%20as%20the%20robustness%20of%20the%0Amodel%20across%20different%20spatial%20transformations.%20We%20apply%20the%20STN-YOLO%20on%0Abenchmark%20datasets%20for%20Agricultural%20object%20detection%20as%20well%20as%20a%20new%20dataset%0Afrom%20a%20state-of-the-art%20plant%20phenotyping%20greenhouse%20facility.%20Our%20code%20and%0Adataset%20are%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21652v1&entry.124074799=Read"},
{"title": "Tabular Data Augmentation for Machine Learning: Progress and Prospects\n  of Embracing Generative AI", "author": "Lingxi Cui and Huan Li and Ke Chen and Lidan Shou and Gang Chen", "abstract": "  Machine learning (ML) on tabular data is ubiquitous, yet obtaining abundant\nhigh-quality tabular data for model training remains a significant obstacle.\nNumerous works have focused on tabular data augmentation (TDA) to enhance the\noriginal table with additional data, thereby improving downstream ML tasks.\nRecently, there has been a growing interest in leveraging the capabilities of\ngenerative AI for TDA. Therefore, we believe it is time to provide a\ncomprehensive review of the progress and future prospects of TDA, with a\nparticular emphasis on the trending generative AI. Specifically, we present an\narchitectural view of the TDA pipeline, comprising three main procedures:\npre-augmentation, augmentation, and post-augmentation. Pre-augmentation\nencompasses preparation tasks that facilitate subsequent TDA, including error\nhandling, table annotation, table simplification, table representation, table\nindexing, table navigation, schema matching, and entity matching. Augmentation\nsystematically analyzes current TDA methods, categorized into retrieval-based\nmethods, which retrieve external data, and generation-based methods, which\ngenerate synthetic data. We further subdivide these methods based on the\ngranularity of the augmentation process at the row, column, cell, and table\nlevels. Post-augmentation focuses on the datasets, evaluation and optimization\naspects of TDA. We also summarize current trends and future directions for TDA,\nhighlighting promising opportunities in the era of generative AI. In addition,\nthe accompanying papers and related resources are continuously updated and\nmaintained in the GitHub repository at\nhttps://github.com/SuDIS-ZJU/awesome-tabular-data-augmentation to reflect\nongoing advancements in the field.\n", "link": "http://arxiv.org/abs/2407.21523v1", "date": "2024-07-31", "relevancy": 2.4361, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4973}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4856}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tabular%20Data%20Augmentation%20for%20Machine%20Learning%3A%20Progress%20and%20Prospects%0A%20%20of%20Embracing%20Generative%20AI&body=Title%3A%20Tabular%20Data%20Augmentation%20for%20Machine%20Learning%3A%20Progress%20and%20Prospects%0A%20%20of%20Embracing%20Generative%20AI%0AAuthor%3A%20Lingxi%20Cui%20and%20Huan%20Li%20and%20Ke%20Chen%20and%20Lidan%20Shou%20and%20Gang%20Chen%0AAbstract%3A%20%20%20Machine%20learning%20%28ML%29%20on%20tabular%20data%20is%20ubiquitous%2C%20yet%20obtaining%20abundant%0Ahigh-quality%20tabular%20data%20for%20model%20training%20remains%20a%20significant%20obstacle.%0ANumerous%20works%20have%20focused%20on%20tabular%20data%20augmentation%20%28TDA%29%20to%20enhance%20the%0Aoriginal%20table%20with%20additional%20data%2C%20thereby%20improving%20downstream%20ML%20tasks.%0ARecently%2C%20there%20has%20been%20a%20growing%20interest%20in%20leveraging%20the%20capabilities%20of%0Agenerative%20AI%20for%20TDA.%20Therefore%2C%20we%20believe%20it%20is%20time%20to%20provide%20a%0Acomprehensive%20review%20of%20the%20progress%20and%20future%20prospects%20of%20TDA%2C%20with%20a%0Aparticular%20emphasis%20on%20the%20trending%20generative%20AI.%20Specifically%2C%20we%20present%20an%0Aarchitectural%20view%20of%20the%20TDA%20pipeline%2C%20comprising%20three%20main%20procedures%3A%0Apre-augmentation%2C%20augmentation%2C%20and%20post-augmentation.%20Pre-augmentation%0Aencompasses%20preparation%20tasks%20that%20facilitate%20subsequent%20TDA%2C%20including%20error%0Ahandling%2C%20table%20annotation%2C%20table%20simplification%2C%20table%20representation%2C%20table%0Aindexing%2C%20table%20navigation%2C%20schema%20matching%2C%20and%20entity%20matching.%20Augmentation%0Asystematically%20analyzes%20current%20TDA%20methods%2C%20categorized%20into%20retrieval-based%0Amethods%2C%20which%20retrieve%20external%20data%2C%20and%20generation-based%20methods%2C%20which%0Agenerate%20synthetic%20data.%20We%20further%20subdivide%20these%20methods%20based%20on%20the%0Agranularity%20of%20the%20augmentation%20process%20at%20the%20row%2C%20column%2C%20cell%2C%20and%20table%0Alevels.%20Post-augmentation%20focuses%20on%20the%20datasets%2C%20evaluation%20and%20optimization%0Aaspects%20of%20TDA.%20We%20also%20summarize%20current%20trends%20and%20future%20directions%20for%20TDA%2C%0Ahighlighting%20promising%20opportunities%20in%20the%20era%20of%20generative%20AI.%20In%20addition%2C%0Athe%20accompanying%20papers%20and%20related%20resources%20are%20continuously%20updated%20and%0Amaintained%20in%20the%20GitHub%20repository%20at%0Ahttps%3A//github.com/SuDIS-ZJU/awesome-tabular-data-augmentation%20to%20reflect%0Aongoing%20advancements%20in%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21523v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTabular%2520Data%2520Augmentation%2520for%2520Machine%2520Learning%253A%2520Progress%2520and%2520Prospects%250A%2520%2520of%2520Embracing%2520Generative%2520AI%26entry.906535625%3DLingxi%2520Cui%2520and%2520Huan%2520Li%2520and%2520Ke%2520Chen%2520and%2520Lidan%2520Shou%2520and%2520Gang%2520Chen%26entry.1292438233%3D%2520%2520Machine%2520learning%2520%2528ML%2529%2520on%2520tabular%2520data%2520is%2520ubiquitous%252C%2520yet%2520obtaining%2520abundant%250Ahigh-quality%2520tabular%2520data%2520for%2520model%2520training%2520remains%2520a%2520significant%2520obstacle.%250ANumerous%2520works%2520have%2520focused%2520on%2520tabular%2520data%2520augmentation%2520%2528TDA%2529%2520to%2520enhance%2520the%250Aoriginal%2520table%2520with%2520additional%2520data%252C%2520thereby%2520improving%2520downstream%2520ML%2520tasks.%250ARecently%252C%2520there%2520has%2520been%2520a%2520growing%2520interest%2520in%2520leveraging%2520the%2520capabilities%2520of%250Agenerative%2520AI%2520for%2520TDA.%2520Therefore%252C%2520we%2520believe%2520it%2520is%2520time%2520to%2520provide%2520a%250Acomprehensive%2520review%2520of%2520the%2520progress%2520and%2520future%2520prospects%2520of%2520TDA%252C%2520with%2520a%250Aparticular%2520emphasis%2520on%2520the%2520trending%2520generative%2520AI.%2520Specifically%252C%2520we%2520present%2520an%250Aarchitectural%2520view%2520of%2520the%2520TDA%2520pipeline%252C%2520comprising%2520three%2520main%2520procedures%253A%250Apre-augmentation%252C%2520augmentation%252C%2520and%2520post-augmentation.%2520Pre-augmentation%250Aencompasses%2520preparation%2520tasks%2520that%2520facilitate%2520subsequent%2520TDA%252C%2520including%2520error%250Ahandling%252C%2520table%2520annotation%252C%2520table%2520simplification%252C%2520table%2520representation%252C%2520table%250Aindexing%252C%2520table%2520navigation%252C%2520schema%2520matching%252C%2520and%2520entity%2520matching.%2520Augmentation%250Asystematically%2520analyzes%2520current%2520TDA%2520methods%252C%2520categorized%2520into%2520retrieval-based%250Amethods%252C%2520which%2520retrieve%2520external%2520data%252C%2520and%2520generation-based%2520methods%252C%2520which%250Agenerate%2520synthetic%2520data.%2520We%2520further%2520subdivide%2520these%2520methods%2520based%2520on%2520the%250Agranularity%2520of%2520the%2520augmentation%2520process%2520at%2520the%2520row%252C%2520column%252C%2520cell%252C%2520and%2520table%250Alevels.%2520Post-augmentation%2520focuses%2520on%2520the%2520datasets%252C%2520evaluation%2520and%2520optimization%250Aaspects%2520of%2520TDA.%2520We%2520also%2520summarize%2520current%2520trends%2520and%2520future%2520directions%2520for%2520TDA%252C%250Ahighlighting%2520promising%2520opportunities%2520in%2520the%2520era%2520of%2520generative%2520AI.%2520In%2520addition%252C%250Athe%2520accompanying%2520papers%2520and%2520related%2520resources%2520are%2520continuously%2520updated%2520and%250Amaintained%2520in%2520the%2520GitHub%2520repository%2520at%250Ahttps%253A//github.com/SuDIS-ZJU/awesome-tabular-data-augmentation%2520to%2520reflect%250Aongoing%2520advancements%2520in%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21523v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tabular%20Data%20Augmentation%20for%20Machine%20Learning%3A%20Progress%20and%20Prospects%0A%20%20of%20Embracing%20Generative%20AI&entry.906535625=Lingxi%20Cui%20and%20Huan%20Li%20and%20Ke%20Chen%20and%20Lidan%20Shou%20and%20Gang%20Chen&entry.1292438233=%20%20Machine%20learning%20%28ML%29%20on%20tabular%20data%20is%20ubiquitous%2C%20yet%20obtaining%20abundant%0Ahigh-quality%20tabular%20data%20for%20model%20training%20remains%20a%20significant%20obstacle.%0ANumerous%20works%20have%20focused%20on%20tabular%20data%20augmentation%20%28TDA%29%20to%20enhance%20the%0Aoriginal%20table%20with%20additional%20data%2C%20thereby%20improving%20downstream%20ML%20tasks.%0ARecently%2C%20there%20has%20been%20a%20growing%20interest%20in%20leveraging%20the%20capabilities%20of%0Agenerative%20AI%20for%20TDA.%20Therefore%2C%20we%20believe%20it%20is%20time%20to%20provide%20a%0Acomprehensive%20review%20of%20the%20progress%20and%20future%20prospects%20of%20TDA%2C%20with%20a%0Aparticular%20emphasis%20on%20the%20trending%20generative%20AI.%20Specifically%2C%20we%20present%20an%0Aarchitectural%20view%20of%20the%20TDA%20pipeline%2C%20comprising%20three%20main%20procedures%3A%0Apre-augmentation%2C%20augmentation%2C%20and%20post-augmentation.%20Pre-augmentation%0Aencompasses%20preparation%20tasks%20that%20facilitate%20subsequent%20TDA%2C%20including%20error%0Ahandling%2C%20table%20annotation%2C%20table%20simplification%2C%20table%20representation%2C%20table%0Aindexing%2C%20table%20navigation%2C%20schema%20matching%2C%20and%20entity%20matching.%20Augmentation%0Asystematically%20analyzes%20current%20TDA%20methods%2C%20categorized%20into%20retrieval-based%0Amethods%2C%20which%20retrieve%20external%20data%2C%20and%20generation-based%20methods%2C%20which%0Agenerate%20synthetic%20data.%20We%20further%20subdivide%20these%20methods%20based%20on%20the%0Agranularity%20of%20the%20augmentation%20process%20at%20the%20row%2C%20column%2C%20cell%2C%20and%20table%0Alevels.%20Post-augmentation%20focuses%20on%20the%20datasets%2C%20evaluation%20and%20optimization%0Aaspects%20of%20TDA.%20We%20also%20summarize%20current%20trends%20and%20future%20directions%20for%20TDA%2C%0Ahighlighting%20promising%20opportunities%20in%20the%20era%20of%20generative%20AI.%20In%20addition%2C%0Athe%20accompanying%20papers%20and%20related%20resources%20are%20continuously%20updated%20and%0Amaintained%20in%20the%20GitHub%20repository%20at%0Ahttps%3A//github.com/SuDIS-ZJU/awesome-tabular-data-augmentation%20to%20reflect%0Aongoing%20advancements%20in%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21523v1&entry.124074799=Read"},
{"title": "HGOE: Hybrid External and Internal Graph Outlier Exposure for Graph\n  Out-of-Distribution Detection", "author": "Junwei He and Qianqian Xu and Yangbangyan Jiang and Zitai Wang and Yuchen Sun and Qingming Huang", "abstract": "  With the progressive advancements in deep graph learning, out-of-distribution\n(OOD) detection for graph data has emerged as a critical challenge. While the\nefficacy of auxiliary datasets in enhancing OOD detection has been extensively\nstudied for image and text data, such approaches have not yet been explored for\ngraph data. Unlike Euclidean data, graph data exhibits greater diversity but\nlower robustness to perturbations, complicating the integration of outliers. To\ntackle these challenges, we propose the introduction of \\textbf{H}ybrid\nExternal and Internal \\textbf{G}raph \\textbf{O}utlier \\textbf{E}xposure (HGOE)\nto improve graph OOD detection performance. Our framework involves using\nrealistic external graph data from various domains and synthesizing internal\noutliers within ID subgroups to address the poor robustness and presence of OOD\nsamples within the ID class. Furthermore, we develop a boundary-aware OE loss\nthat adaptively assigns weights to outliers, maximizing the use of high-quality\nOOD samples while minimizing the impact of low-quality ones. Our proposed HGOE\nframework is model-agnostic and designed to enhance the effectiveness of\nexisting graph OOD detection models. Experimental results demonstrate that our\nHGOE framework can significantly improve the performance of existing OOD\ndetection models across all 8 real datasets.\n", "link": "http://arxiv.org/abs/2407.21742v1", "date": "2024-07-31", "relevancy": 2.4112, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4974}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4759}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4734}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HGOE%3A%20Hybrid%20External%20and%20Internal%20Graph%20Outlier%20Exposure%20for%20Graph%0A%20%20Out-of-Distribution%20Detection&body=Title%3A%20HGOE%3A%20Hybrid%20External%20and%20Internal%20Graph%20Outlier%20Exposure%20for%20Graph%0A%20%20Out-of-Distribution%20Detection%0AAuthor%3A%20Junwei%20He%20and%20Qianqian%20Xu%20and%20Yangbangyan%20Jiang%20and%20Zitai%20Wang%20and%20Yuchen%20Sun%20and%20Qingming%20Huang%0AAbstract%3A%20%20%20With%20the%20progressive%20advancements%20in%20deep%20graph%20learning%2C%20out-of-distribution%0A%28OOD%29%20detection%20for%20graph%20data%20has%20emerged%20as%20a%20critical%20challenge.%20While%20the%0Aefficacy%20of%20auxiliary%20datasets%20in%20enhancing%20OOD%20detection%20has%20been%20extensively%0Astudied%20for%20image%20and%20text%20data%2C%20such%20approaches%20have%20not%20yet%20been%20explored%20for%0Agraph%20data.%20Unlike%20Euclidean%20data%2C%20graph%20data%20exhibits%20greater%20diversity%20but%0Alower%20robustness%20to%20perturbations%2C%20complicating%20the%20integration%20of%20outliers.%20To%0Atackle%20these%20challenges%2C%20we%20propose%20the%20introduction%20of%20%5Ctextbf%7BH%7Dybrid%0AExternal%20and%20Internal%20%5Ctextbf%7BG%7Draph%20%5Ctextbf%7BO%7Dutlier%20%5Ctextbf%7BE%7Dxposure%20%28HGOE%29%0Ato%20improve%20graph%20OOD%20detection%20performance.%20Our%20framework%20involves%20using%0Arealistic%20external%20graph%20data%20from%20various%20domains%20and%20synthesizing%20internal%0Aoutliers%20within%20ID%20subgroups%20to%20address%20the%20poor%20robustness%20and%20presence%20of%20OOD%0Asamples%20within%20the%20ID%20class.%20Furthermore%2C%20we%20develop%20a%20boundary-aware%20OE%20loss%0Athat%20adaptively%20assigns%20weights%20to%20outliers%2C%20maximizing%20the%20use%20of%20high-quality%0AOOD%20samples%20while%20minimizing%20the%20impact%20of%20low-quality%20ones.%20Our%20proposed%20HGOE%0Aframework%20is%20model-agnostic%20and%20designed%20to%20enhance%20the%20effectiveness%20of%0Aexisting%20graph%20OOD%20detection%20models.%20Experimental%20results%20demonstrate%20that%20our%0AHGOE%20framework%20can%20significantly%20improve%20the%20performance%20of%20existing%20OOD%0Adetection%20models%20across%20all%208%20real%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21742v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHGOE%253A%2520Hybrid%2520External%2520and%2520Internal%2520Graph%2520Outlier%2520Exposure%2520for%2520Graph%250A%2520%2520Out-of-Distribution%2520Detection%26entry.906535625%3DJunwei%2520He%2520and%2520Qianqian%2520Xu%2520and%2520Yangbangyan%2520Jiang%2520and%2520Zitai%2520Wang%2520and%2520Yuchen%2520Sun%2520and%2520Qingming%2520Huang%26entry.1292438233%3D%2520%2520With%2520the%2520progressive%2520advancements%2520in%2520deep%2520graph%2520learning%252C%2520out-of-distribution%250A%2528OOD%2529%2520detection%2520for%2520graph%2520data%2520has%2520emerged%2520as%2520a%2520critical%2520challenge.%2520While%2520the%250Aefficacy%2520of%2520auxiliary%2520datasets%2520in%2520enhancing%2520OOD%2520detection%2520has%2520been%2520extensively%250Astudied%2520for%2520image%2520and%2520text%2520data%252C%2520such%2520approaches%2520have%2520not%2520yet%2520been%2520explored%2520for%250Agraph%2520data.%2520Unlike%2520Euclidean%2520data%252C%2520graph%2520data%2520exhibits%2520greater%2520diversity%2520but%250Alower%2520robustness%2520to%2520perturbations%252C%2520complicating%2520the%2520integration%2520of%2520outliers.%2520To%250Atackle%2520these%2520challenges%252C%2520we%2520propose%2520the%2520introduction%2520of%2520%255Ctextbf%257BH%257Dybrid%250AExternal%2520and%2520Internal%2520%255Ctextbf%257BG%257Draph%2520%255Ctextbf%257BO%257Dutlier%2520%255Ctextbf%257BE%257Dxposure%2520%2528HGOE%2529%250Ato%2520improve%2520graph%2520OOD%2520detection%2520performance.%2520Our%2520framework%2520involves%2520using%250Arealistic%2520external%2520graph%2520data%2520from%2520various%2520domains%2520and%2520synthesizing%2520internal%250Aoutliers%2520within%2520ID%2520subgroups%2520to%2520address%2520the%2520poor%2520robustness%2520and%2520presence%2520of%2520OOD%250Asamples%2520within%2520the%2520ID%2520class.%2520Furthermore%252C%2520we%2520develop%2520a%2520boundary-aware%2520OE%2520loss%250Athat%2520adaptively%2520assigns%2520weights%2520to%2520outliers%252C%2520maximizing%2520the%2520use%2520of%2520high-quality%250AOOD%2520samples%2520while%2520minimizing%2520the%2520impact%2520of%2520low-quality%2520ones.%2520Our%2520proposed%2520HGOE%250Aframework%2520is%2520model-agnostic%2520and%2520designed%2520to%2520enhance%2520the%2520effectiveness%2520of%250Aexisting%2520graph%2520OOD%2520detection%2520models.%2520Experimental%2520results%2520demonstrate%2520that%2520our%250AHGOE%2520framework%2520can%2520significantly%2520improve%2520the%2520performance%2520of%2520existing%2520OOD%250Adetection%2520models%2520across%2520all%25208%2520real%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21742v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HGOE%3A%20Hybrid%20External%20and%20Internal%20Graph%20Outlier%20Exposure%20for%20Graph%0A%20%20Out-of-Distribution%20Detection&entry.906535625=Junwei%20He%20and%20Qianqian%20Xu%20and%20Yangbangyan%20Jiang%20and%20Zitai%20Wang%20and%20Yuchen%20Sun%20and%20Qingming%20Huang&entry.1292438233=%20%20With%20the%20progressive%20advancements%20in%20deep%20graph%20learning%2C%20out-of-distribution%0A%28OOD%29%20detection%20for%20graph%20data%20has%20emerged%20as%20a%20critical%20challenge.%20While%20the%0Aefficacy%20of%20auxiliary%20datasets%20in%20enhancing%20OOD%20detection%20has%20been%20extensively%0Astudied%20for%20image%20and%20text%20data%2C%20such%20approaches%20have%20not%20yet%20been%20explored%20for%0Agraph%20data.%20Unlike%20Euclidean%20data%2C%20graph%20data%20exhibits%20greater%20diversity%20but%0Alower%20robustness%20to%20perturbations%2C%20complicating%20the%20integration%20of%20outliers.%20To%0Atackle%20these%20challenges%2C%20we%20propose%20the%20introduction%20of%20%5Ctextbf%7BH%7Dybrid%0AExternal%20and%20Internal%20%5Ctextbf%7BG%7Draph%20%5Ctextbf%7BO%7Dutlier%20%5Ctextbf%7BE%7Dxposure%20%28HGOE%29%0Ato%20improve%20graph%20OOD%20detection%20performance.%20Our%20framework%20involves%20using%0Arealistic%20external%20graph%20data%20from%20various%20domains%20and%20synthesizing%20internal%0Aoutliers%20within%20ID%20subgroups%20to%20address%20the%20poor%20robustness%20and%20presence%20of%20OOD%0Asamples%20within%20the%20ID%20class.%20Furthermore%2C%20we%20develop%20a%20boundary-aware%20OE%20loss%0Athat%20adaptively%20assigns%20weights%20to%20outliers%2C%20maximizing%20the%20use%20of%20high-quality%0AOOD%20samples%20while%20minimizing%20the%20impact%20of%20low-quality%20ones.%20Our%20proposed%20HGOE%0Aframework%20is%20model-agnostic%20and%20designed%20to%20enhance%20the%20effectiveness%20of%0Aexisting%20graph%20OOD%20detection%20models.%20Experimental%20results%20demonstrate%20that%20our%0AHGOE%20framework%20can%20significantly%20improve%20the%20performance%20of%20existing%20OOD%0Adetection%20models%20across%20all%208%20real%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21742v1&entry.124074799=Read"},
{"title": "FedADMM-InSa: An Inexact and Self-Adaptive ADMM for Federated Learning", "author": "Yongcun Song and Ziqi Wang and Enrique Zuazua", "abstract": "  Federated learning (FL) is a promising framework for learning from\ndistributed data while maintaining privacy. The development of efficient FL\nalgorithms encounters various challenges, including heterogeneous data and\nsystems, limited communication capacities, and constrained local computational\nresources. Recently developed FedADMM methods show great resilience to both\ndata and system heterogeneity. However, they still suffer from performance\ndeterioration if the hyperparameters are not carefully tuned. To address this\nissue, we propose an inexact and self-adaptive FedADMM algorithm, termed\nFedADMM-InSa. First, we design an inexactness criterion for the clients' local\nupdates to eliminate the need for empirically setting the local training\naccuracy. This inexactness criterion can be assessed by each client\nindependently based on its unique condition, thereby reducing the local\ncomputational cost and mitigating the undesirable straggle effect. The\nconvergence of the resulting inexact ADMM is proved under the assumption of\nstrongly convex loss functions. Additionally, we present a self-adaptive scheme\nthat dynamically adjusts each client's penalty parameter, enhancing algorithm\nrobustness by mitigating the need for empirical penalty parameter choices for\neach client. Extensive numerical experiments on both synthetic and real-world\ndatasets are conducted. As validated by some numerical tests, our proposed\nalgorithm can reduce the clients' local computational load significantly and\nalso accelerate the learning process compared to the vanilla FedADMM.\n", "link": "http://arxiv.org/abs/2402.13989v3", "date": "2024-07-31", "relevancy": 2.3568, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4927}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4622}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedADMM-InSa%3A%20An%20Inexact%20and%20Self-Adaptive%20ADMM%20for%20Federated%20Learning&body=Title%3A%20FedADMM-InSa%3A%20An%20Inexact%20and%20Self-Adaptive%20ADMM%20for%20Federated%20Learning%0AAuthor%3A%20Yongcun%20Song%20and%20Ziqi%20Wang%20and%20Enrique%20Zuazua%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20is%20a%20promising%20framework%20for%20learning%20from%0Adistributed%20data%20while%20maintaining%20privacy.%20The%20development%20of%20efficient%20FL%0Aalgorithms%20encounters%20various%20challenges%2C%20including%20heterogeneous%20data%20and%0Asystems%2C%20limited%20communication%20capacities%2C%20and%20constrained%20local%20computational%0Aresources.%20Recently%20developed%20FedADMM%20methods%20show%20great%20resilience%20to%20both%0Adata%20and%20system%20heterogeneity.%20However%2C%20they%20still%20suffer%20from%20performance%0Adeterioration%20if%20the%20hyperparameters%20are%20not%20carefully%20tuned.%20To%20address%20this%0Aissue%2C%20we%20propose%20an%20inexact%20and%20self-adaptive%20FedADMM%20algorithm%2C%20termed%0AFedADMM-InSa.%20First%2C%20we%20design%20an%20inexactness%20criterion%20for%20the%20clients%27%20local%0Aupdates%20to%20eliminate%20the%20need%20for%20empirically%20setting%20the%20local%20training%0Aaccuracy.%20This%20inexactness%20criterion%20can%20be%20assessed%20by%20each%20client%0Aindependently%20based%20on%20its%20unique%20condition%2C%20thereby%20reducing%20the%20local%0Acomputational%20cost%20and%20mitigating%20the%20undesirable%20straggle%20effect.%20The%0Aconvergence%20of%20the%20resulting%20inexact%20ADMM%20is%20proved%20under%20the%20assumption%20of%0Astrongly%20convex%20loss%20functions.%20Additionally%2C%20we%20present%20a%20self-adaptive%20scheme%0Athat%20dynamically%20adjusts%20each%20client%27s%20penalty%20parameter%2C%20enhancing%20algorithm%0Arobustness%20by%20mitigating%20the%20need%20for%20empirical%20penalty%20parameter%20choices%20for%0Aeach%20client.%20Extensive%20numerical%20experiments%20on%20both%20synthetic%20and%20real-world%0Adatasets%20are%20conducted.%20As%20validated%20by%20some%20numerical%20tests%2C%20our%20proposed%0Aalgorithm%20can%20reduce%20the%20clients%27%20local%20computational%20load%20significantly%20and%0Aalso%20accelerate%20the%20learning%20process%20compared%20to%20the%20vanilla%20FedADMM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.13989v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedADMM-InSa%253A%2520An%2520Inexact%2520and%2520Self-Adaptive%2520ADMM%2520for%2520Federated%2520Learning%26entry.906535625%3DYongcun%2520Song%2520and%2520Ziqi%2520Wang%2520and%2520Enrique%2520Zuazua%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520is%2520a%2520promising%2520framework%2520for%2520learning%2520from%250Adistributed%2520data%2520while%2520maintaining%2520privacy.%2520The%2520development%2520of%2520efficient%2520FL%250Aalgorithms%2520encounters%2520various%2520challenges%252C%2520including%2520heterogeneous%2520data%2520and%250Asystems%252C%2520limited%2520communication%2520capacities%252C%2520and%2520constrained%2520local%2520computational%250Aresources.%2520Recently%2520developed%2520FedADMM%2520methods%2520show%2520great%2520resilience%2520to%2520both%250Adata%2520and%2520system%2520heterogeneity.%2520However%252C%2520they%2520still%2520suffer%2520from%2520performance%250Adeterioration%2520if%2520the%2520hyperparameters%2520are%2520not%2520carefully%2520tuned.%2520To%2520address%2520this%250Aissue%252C%2520we%2520propose%2520an%2520inexact%2520and%2520self-adaptive%2520FedADMM%2520algorithm%252C%2520termed%250AFedADMM-InSa.%2520First%252C%2520we%2520design%2520an%2520inexactness%2520criterion%2520for%2520the%2520clients%2527%2520local%250Aupdates%2520to%2520eliminate%2520the%2520need%2520for%2520empirically%2520setting%2520the%2520local%2520training%250Aaccuracy.%2520This%2520inexactness%2520criterion%2520can%2520be%2520assessed%2520by%2520each%2520client%250Aindependently%2520based%2520on%2520its%2520unique%2520condition%252C%2520thereby%2520reducing%2520the%2520local%250Acomputational%2520cost%2520and%2520mitigating%2520the%2520undesirable%2520straggle%2520effect.%2520The%250Aconvergence%2520of%2520the%2520resulting%2520inexact%2520ADMM%2520is%2520proved%2520under%2520the%2520assumption%2520of%250Astrongly%2520convex%2520loss%2520functions.%2520Additionally%252C%2520we%2520present%2520a%2520self-adaptive%2520scheme%250Athat%2520dynamically%2520adjusts%2520each%2520client%2527s%2520penalty%2520parameter%252C%2520enhancing%2520algorithm%250Arobustness%2520by%2520mitigating%2520the%2520need%2520for%2520empirical%2520penalty%2520parameter%2520choices%2520for%250Aeach%2520client.%2520Extensive%2520numerical%2520experiments%2520on%2520both%2520synthetic%2520and%2520real-world%250Adatasets%2520are%2520conducted.%2520As%2520validated%2520by%2520some%2520numerical%2520tests%252C%2520our%2520proposed%250Aalgorithm%2520can%2520reduce%2520the%2520clients%2527%2520local%2520computational%2520load%2520significantly%2520and%250Aalso%2520accelerate%2520the%2520learning%2520process%2520compared%2520to%2520the%2520vanilla%2520FedADMM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.13989v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedADMM-InSa%3A%20An%20Inexact%20and%20Self-Adaptive%20ADMM%20for%20Federated%20Learning&entry.906535625=Yongcun%20Song%20and%20Ziqi%20Wang%20and%20Enrique%20Zuazua&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20is%20a%20promising%20framework%20for%20learning%20from%0Adistributed%20data%20while%20maintaining%20privacy.%20The%20development%20of%20efficient%20FL%0Aalgorithms%20encounters%20various%20challenges%2C%20including%20heterogeneous%20data%20and%0Asystems%2C%20limited%20communication%20capacities%2C%20and%20constrained%20local%20computational%0Aresources.%20Recently%20developed%20FedADMM%20methods%20show%20great%20resilience%20to%20both%0Adata%20and%20system%20heterogeneity.%20However%2C%20they%20still%20suffer%20from%20performance%0Adeterioration%20if%20the%20hyperparameters%20are%20not%20carefully%20tuned.%20To%20address%20this%0Aissue%2C%20we%20propose%20an%20inexact%20and%20self-adaptive%20FedADMM%20algorithm%2C%20termed%0AFedADMM-InSa.%20First%2C%20we%20design%20an%20inexactness%20criterion%20for%20the%20clients%27%20local%0Aupdates%20to%20eliminate%20the%20need%20for%20empirically%20setting%20the%20local%20training%0Aaccuracy.%20This%20inexactness%20criterion%20can%20be%20assessed%20by%20each%20client%0Aindependently%20based%20on%20its%20unique%20condition%2C%20thereby%20reducing%20the%20local%0Acomputational%20cost%20and%20mitigating%20the%20undesirable%20straggle%20effect.%20The%0Aconvergence%20of%20the%20resulting%20inexact%20ADMM%20is%20proved%20under%20the%20assumption%20of%0Astrongly%20convex%20loss%20functions.%20Additionally%2C%20we%20present%20a%20self-adaptive%20scheme%0Athat%20dynamically%20adjusts%20each%20client%27s%20penalty%20parameter%2C%20enhancing%20algorithm%0Arobustness%20by%20mitigating%20the%20need%20for%20empirical%20penalty%20parameter%20choices%20for%0Aeach%20client.%20Extensive%20numerical%20experiments%20on%20both%20synthetic%20and%20real-world%0Adatasets%20are%20conducted.%20As%20validated%20by%20some%20numerical%20tests%2C%20our%20proposed%0Aalgorithm%20can%20reduce%20the%20clients%27%20local%20computational%20load%20significantly%20and%0Aalso%20accelerate%20the%20learning%20process%20compared%20to%20the%20vanilla%20FedADMM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13989v3&entry.124074799=Read"},
{"title": "Berkeley Humanoid: A Research Platform for Learning-based Control", "author": "Qiayuan Liao and Bike Zhang and Xuanyu Huang and Xiaoyu Huang and Zhongyu Li and Koushil Sreenath", "abstract": "  We introduce Berkeley Humanoid, a reliable and low-cost mid-scale humanoid\nresearch platform for learning-based control. Our lightweight, in-house-built\nrobot is designed specifically for learning algorithms with low simulation\ncomplexity, anthropomorphic motion, and high reliability against falls. The\nrobot's narrow sim-to-real gap enables agile and robust locomotion across\nvarious terrains in outdoor environments, achieved with a simple reinforcement\nlearning controller using light domain randomization. Furthermore, we\ndemonstrate the robot traversing for hundreds of meters, walking on a steep\nunpaved trail, and hopping with single and double legs as a testimony to its\nhigh performance in dynamical walking. Capable of omnidirectional locomotion\nand withstanding large perturbations with a compact setup, our system aims for\nscalable, sim-to-real deployment of learning-based humanoid systems. Please\ncheck http://berkeley-humanoid.com for more details.\n", "link": "http://arxiv.org/abs/2407.21781v1", "date": "2024-07-31", "relevancy": 2.3048, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6357}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5773}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Berkeley%20Humanoid%3A%20A%20Research%20Platform%20for%20Learning-based%20Control&body=Title%3A%20Berkeley%20Humanoid%3A%20A%20Research%20Platform%20for%20Learning-based%20Control%0AAuthor%3A%20Qiayuan%20Liao%20and%20Bike%20Zhang%20and%20Xuanyu%20Huang%20and%20Xiaoyu%20Huang%20and%20Zhongyu%20Li%20and%20Koushil%20Sreenath%0AAbstract%3A%20%20%20We%20introduce%20Berkeley%20Humanoid%2C%20a%20reliable%20and%20low-cost%20mid-scale%20humanoid%0Aresearch%20platform%20for%20learning-based%20control.%20Our%20lightweight%2C%20in-house-built%0Arobot%20is%20designed%20specifically%20for%20learning%20algorithms%20with%20low%20simulation%0Acomplexity%2C%20anthropomorphic%20motion%2C%20and%20high%20reliability%20against%20falls.%20The%0Arobot%27s%20narrow%20sim-to-real%20gap%20enables%20agile%20and%20robust%20locomotion%20across%0Avarious%20terrains%20in%20outdoor%20environments%2C%20achieved%20with%20a%20simple%20reinforcement%0Alearning%20controller%20using%20light%20domain%20randomization.%20Furthermore%2C%20we%0Ademonstrate%20the%20robot%20traversing%20for%20hundreds%20of%20meters%2C%20walking%20on%20a%20steep%0Aunpaved%20trail%2C%20and%20hopping%20with%20single%20and%20double%20legs%20as%20a%20testimony%20to%20its%0Ahigh%20performance%20in%20dynamical%20walking.%20Capable%20of%20omnidirectional%20locomotion%0Aand%20withstanding%20large%20perturbations%20with%20a%20compact%20setup%2C%20our%20system%20aims%20for%0Ascalable%2C%20sim-to-real%20deployment%20of%20learning-based%20humanoid%20systems.%20Please%0Acheck%20http%3A//berkeley-humanoid.com%20for%20more%20details.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21781v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBerkeley%2520Humanoid%253A%2520A%2520Research%2520Platform%2520for%2520Learning-based%2520Control%26entry.906535625%3DQiayuan%2520Liao%2520and%2520Bike%2520Zhang%2520and%2520Xuanyu%2520Huang%2520and%2520Xiaoyu%2520Huang%2520and%2520Zhongyu%2520Li%2520and%2520Koushil%2520Sreenath%26entry.1292438233%3D%2520%2520We%2520introduce%2520Berkeley%2520Humanoid%252C%2520a%2520reliable%2520and%2520low-cost%2520mid-scale%2520humanoid%250Aresearch%2520platform%2520for%2520learning-based%2520control.%2520Our%2520lightweight%252C%2520in-house-built%250Arobot%2520is%2520designed%2520specifically%2520for%2520learning%2520algorithms%2520with%2520low%2520simulation%250Acomplexity%252C%2520anthropomorphic%2520motion%252C%2520and%2520high%2520reliability%2520against%2520falls.%2520The%250Arobot%2527s%2520narrow%2520sim-to-real%2520gap%2520enables%2520agile%2520and%2520robust%2520locomotion%2520across%250Avarious%2520terrains%2520in%2520outdoor%2520environments%252C%2520achieved%2520with%2520a%2520simple%2520reinforcement%250Alearning%2520controller%2520using%2520light%2520domain%2520randomization.%2520Furthermore%252C%2520we%250Ademonstrate%2520the%2520robot%2520traversing%2520for%2520hundreds%2520of%2520meters%252C%2520walking%2520on%2520a%2520steep%250Aunpaved%2520trail%252C%2520and%2520hopping%2520with%2520single%2520and%2520double%2520legs%2520as%2520a%2520testimony%2520to%2520its%250Ahigh%2520performance%2520in%2520dynamical%2520walking.%2520Capable%2520of%2520omnidirectional%2520locomotion%250Aand%2520withstanding%2520large%2520perturbations%2520with%2520a%2520compact%2520setup%252C%2520our%2520system%2520aims%2520for%250Ascalable%252C%2520sim-to-real%2520deployment%2520of%2520learning-based%2520humanoid%2520systems.%2520Please%250Acheck%2520http%253A//berkeley-humanoid.com%2520for%2520more%2520details.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21781v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Berkeley%20Humanoid%3A%20A%20Research%20Platform%20for%20Learning-based%20Control&entry.906535625=Qiayuan%20Liao%20and%20Bike%20Zhang%20and%20Xuanyu%20Huang%20and%20Xiaoyu%20Huang%20and%20Zhongyu%20Li%20and%20Koushil%20Sreenath&entry.1292438233=%20%20We%20introduce%20Berkeley%20Humanoid%2C%20a%20reliable%20and%20low-cost%20mid-scale%20humanoid%0Aresearch%20platform%20for%20learning-based%20control.%20Our%20lightweight%2C%20in-house-built%0Arobot%20is%20designed%20specifically%20for%20learning%20algorithms%20with%20low%20simulation%0Acomplexity%2C%20anthropomorphic%20motion%2C%20and%20high%20reliability%20against%20falls.%20The%0Arobot%27s%20narrow%20sim-to-real%20gap%20enables%20agile%20and%20robust%20locomotion%20across%0Avarious%20terrains%20in%20outdoor%20environments%2C%20achieved%20with%20a%20simple%20reinforcement%0Alearning%20controller%20using%20light%20domain%20randomization.%20Furthermore%2C%20we%0Ademonstrate%20the%20robot%20traversing%20for%20hundreds%20of%20meters%2C%20walking%20on%20a%20steep%0Aunpaved%20trail%2C%20and%20hopping%20with%20single%20and%20double%20legs%20as%20a%20testimony%20to%20its%0Ahigh%20performance%20in%20dynamical%20walking.%20Capable%20of%20omnidirectional%20locomotion%0Aand%20withstanding%20large%20perturbations%20with%20a%20compact%20setup%2C%20our%20system%20aims%20for%0Ascalable%2C%20sim-to-real%20deployment%20of%20learning-based%20humanoid%20systems.%20Please%0Acheck%20http%3A//berkeley-humanoid.com%20for%20more%20details.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21781v1&entry.124074799=Read"},
{"title": "Beat this! Accurate beat tracking without DBN postprocessing", "author": "Francesco Foscarin and Jan Schl\u00fcter and Gerhard Widmer", "abstract": "  We propose a system for tracking beats and downbeats with two objectives:\ngenerality across a diverse music range, and high accuracy. We achieve\ngenerality by training on multiple datasets -- including solo instrument\nrecordings, pieces with time signature changes, and classical music with high\ntempo variations -- and by removing the commonly used Dynamic Bayesian Network\n(DBN) postprocessing, which introduces constraints on the meter and tempo. For\nhigh accuracy, among other improvements, we develop a loss function tolerant to\nsmall time shifts of annotations, and an architecture alternating convolutions\nwith transformers either over frequency or time. Our system surpasses the\ncurrent state of the art in F1 score despite using no DBN. However, it can\nstill fail, especially for difficult and underrepresented genres, and performs\nworse on continuity metrics, so we publish our model, code, and preprocessed\ndatasets, and invite others to beat this.\n", "link": "http://arxiv.org/abs/2407.21658v1", "date": "2024-07-31", "relevancy": 2.2983, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4637}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4637}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beat%20this%21%20Accurate%20beat%20tracking%20without%20DBN%20postprocessing&body=Title%3A%20Beat%20this%21%20Accurate%20beat%20tracking%20without%20DBN%20postprocessing%0AAuthor%3A%20Francesco%20Foscarin%20and%20Jan%20Schl%C3%BCter%20and%20Gerhard%20Widmer%0AAbstract%3A%20%20%20We%20propose%20a%20system%20for%20tracking%20beats%20and%20downbeats%20with%20two%20objectives%3A%0Agenerality%20across%20a%20diverse%20music%20range%2C%20and%20high%20accuracy.%20We%20achieve%0Agenerality%20by%20training%20on%20multiple%20datasets%20--%20including%20solo%20instrument%0Arecordings%2C%20pieces%20with%20time%20signature%20changes%2C%20and%20classical%20music%20with%20high%0Atempo%20variations%20--%20and%20by%20removing%20the%20commonly%20used%20Dynamic%20Bayesian%20Network%0A%28DBN%29%20postprocessing%2C%20which%20introduces%20constraints%20on%20the%20meter%20and%20tempo.%20For%0Ahigh%20accuracy%2C%20among%20other%20improvements%2C%20we%20develop%20a%20loss%20function%20tolerant%20to%0Asmall%20time%20shifts%20of%20annotations%2C%20and%20an%20architecture%20alternating%20convolutions%0Awith%20transformers%20either%20over%20frequency%20or%20time.%20Our%20system%20surpasses%20the%0Acurrent%20state%20of%20the%20art%20in%20F1%20score%20despite%20using%20no%20DBN.%20However%2C%20it%20can%0Astill%20fail%2C%20especially%20for%20difficult%20and%20underrepresented%20genres%2C%20and%20performs%0Aworse%20on%20continuity%20metrics%2C%20so%20we%20publish%20our%20model%2C%20code%2C%20and%20preprocessed%0Adatasets%2C%20and%20invite%20others%20to%20beat%20this.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21658v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeat%2520this%2521%2520Accurate%2520beat%2520tracking%2520without%2520DBN%2520postprocessing%26entry.906535625%3DFrancesco%2520Foscarin%2520and%2520Jan%2520Schl%25C3%25BCter%2520and%2520Gerhard%2520Widmer%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520system%2520for%2520tracking%2520beats%2520and%2520downbeats%2520with%2520two%2520objectives%253A%250Agenerality%2520across%2520a%2520diverse%2520music%2520range%252C%2520and%2520high%2520accuracy.%2520We%2520achieve%250Agenerality%2520by%2520training%2520on%2520multiple%2520datasets%2520--%2520including%2520solo%2520instrument%250Arecordings%252C%2520pieces%2520with%2520time%2520signature%2520changes%252C%2520and%2520classical%2520music%2520with%2520high%250Atempo%2520variations%2520--%2520and%2520by%2520removing%2520the%2520commonly%2520used%2520Dynamic%2520Bayesian%2520Network%250A%2528DBN%2529%2520postprocessing%252C%2520which%2520introduces%2520constraints%2520on%2520the%2520meter%2520and%2520tempo.%2520For%250Ahigh%2520accuracy%252C%2520among%2520other%2520improvements%252C%2520we%2520develop%2520a%2520loss%2520function%2520tolerant%2520to%250Asmall%2520time%2520shifts%2520of%2520annotations%252C%2520and%2520an%2520architecture%2520alternating%2520convolutions%250Awith%2520transformers%2520either%2520over%2520frequency%2520or%2520time.%2520Our%2520system%2520surpasses%2520the%250Acurrent%2520state%2520of%2520the%2520art%2520in%2520F1%2520score%2520despite%2520using%2520no%2520DBN.%2520However%252C%2520it%2520can%250Astill%2520fail%252C%2520especially%2520for%2520difficult%2520and%2520underrepresented%2520genres%252C%2520and%2520performs%250Aworse%2520on%2520continuity%2520metrics%252C%2520so%2520we%2520publish%2520our%2520model%252C%2520code%252C%2520and%2520preprocessed%250Adatasets%252C%2520and%2520invite%2520others%2520to%2520beat%2520this.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21658v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beat%20this%21%20Accurate%20beat%20tracking%20without%20DBN%20postprocessing&entry.906535625=Francesco%20Foscarin%20and%20Jan%20Schl%C3%BCter%20and%20Gerhard%20Widmer&entry.1292438233=%20%20We%20propose%20a%20system%20for%20tracking%20beats%20and%20downbeats%20with%20two%20objectives%3A%0Agenerality%20across%20a%20diverse%20music%20range%2C%20and%20high%20accuracy.%20We%20achieve%0Agenerality%20by%20training%20on%20multiple%20datasets%20--%20including%20solo%20instrument%0Arecordings%2C%20pieces%20with%20time%20signature%20changes%2C%20and%20classical%20music%20with%20high%0Atempo%20variations%20--%20and%20by%20removing%20the%20commonly%20used%20Dynamic%20Bayesian%20Network%0A%28DBN%29%20postprocessing%2C%20which%20introduces%20constraints%20on%20the%20meter%20and%20tempo.%20For%0Ahigh%20accuracy%2C%20among%20other%20improvements%2C%20we%20develop%20a%20loss%20function%20tolerant%20to%0Asmall%20time%20shifts%20of%20annotations%2C%20and%20an%20architecture%20alternating%20convolutions%0Awith%20transformers%20either%20over%20frequency%20or%20time.%20Our%20system%20surpasses%20the%0Acurrent%20state%20of%20the%20art%20in%20F1%20score%20despite%20using%20no%20DBN.%20However%2C%20it%20can%0Astill%20fail%2C%20especially%20for%20difficult%20and%20underrepresented%20genres%2C%20and%20performs%0Aworse%20on%20continuity%20metrics%2C%20so%20we%20publish%20our%20model%2C%20code%2C%20and%20preprocessed%0Adatasets%2C%20and%20invite%20others%20to%20beat%20this.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21658v1&entry.124074799=Read"},
{"title": "Robust Simultaneous Multislice MRI Reconstruction Using Deep Generative\n  Priors", "author": "Shoujin Huang and Guanxiong Luo and Yuwan Wang and Kexin Yang and Lingyan Zhang and Jingzhe Liu and Hua Guo and Min Wang and Mengye Lyu", "abstract": "  Simultaneous multislice (SMS) imaging is a powerful technique for\naccelerating magnetic resonance imaging (MRI) acquisitions. However, SMS\nreconstruction remains challenging due to the complex signal interactions\nbetween and within the excited slices. This study presents a robust SMS MRI\nreconstruction method using deep generative priors. Starting from Gaussian\nnoise, we leverage denoising diffusion probabilistic models (DDPM) to gradually\nrecover the individual slices through reverse diffusion iterations while\nimposing data consistency from the measured k-space under readout concatenation\nframework. The posterior sampling procedure is designed such that the DDPM\ntraining can be performed on single-slice images without special adjustments\nfor SMS tasks. Additionally, our method integrates a low-frequency enhancement\n(LFE) module to address a practical issue that SMS-accelerated fast spin echo\n(FSE) and echo-planar imaging (EPI) sequences cannot easily embed\nautocalibration signals. Extensive experiments demonstrate that our approach\nconsistently outperforms existing methods and generalizes well to unseen\ndatasets. The code is available at https://github.com/Solor-pikachu/ROGER after\nthe review process.\n", "link": "http://arxiv.org/abs/2407.21600v1", "date": "2024-07-31", "relevancy": 2.2945, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5898}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5693}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Simultaneous%20Multislice%20MRI%20Reconstruction%20Using%20Deep%20Generative%0A%20%20Priors&body=Title%3A%20Robust%20Simultaneous%20Multislice%20MRI%20Reconstruction%20Using%20Deep%20Generative%0A%20%20Priors%0AAuthor%3A%20Shoujin%20Huang%20and%20Guanxiong%20Luo%20and%20Yuwan%20Wang%20and%20Kexin%20Yang%20and%20Lingyan%20Zhang%20and%20Jingzhe%20Liu%20and%20Hua%20Guo%20and%20Min%20Wang%20and%20Mengye%20Lyu%0AAbstract%3A%20%20%20Simultaneous%20multislice%20%28SMS%29%20imaging%20is%20a%20powerful%20technique%20for%0Aaccelerating%20magnetic%20resonance%20imaging%20%28MRI%29%20acquisitions.%20However%2C%20SMS%0Areconstruction%20remains%20challenging%20due%20to%20the%20complex%20signal%20interactions%0Abetween%20and%20within%20the%20excited%20slices.%20This%20study%20presents%20a%20robust%20SMS%20MRI%0Areconstruction%20method%20using%20deep%20generative%20priors.%20Starting%20from%20Gaussian%0Anoise%2C%20we%20leverage%20denoising%20diffusion%20probabilistic%20models%20%28DDPM%29%20to%20gradually%0Arecover%20the%20individual%20slices%20through%20reverse%20diffusion%20iterations%20while%0Aimposing%20data%20consistency%20from%20the%20measured%20k-space%20under%20readout%20concatenation%0Aframework.%20The%20posterior%20sampling%20procedure%20is%20designed%20such%20that%20the%20DDPM%0Atraining%20can%20be%20performed%20on%20single-slice%20images%20without%20special%20adjustments%0Afor%20SMS%20tasks.%20Additionally%2C%20our%20method%20integrates%20a%20low-frequency%20enhancement%0A%28LFE%29%20module%20to%20address%20a%20practical%20issue%20that%20SMS-accelerated%20fast%20spin%20echo%0A%28FSE%29%20and%20echo-planar%20imaging%20%28EPI%29%20sequences%20cannot%20easily%20embed%0Aautocalibration%20signals.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%0Aconsistently%20outperforms%20existing%20methods%20and%20generalizes%20well%20to%20unseen%0Adatasets.%20The%20code%20is%20available%20at%20https%3A//github.com/Solor-pikachu/ROGER%20after%0Athe%20review%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21600v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Simultaneous%2520Multislice%2520MRI%2520Reconstruction%2520Using%2520Deep%2520Generative%250A%2520%2520Priors%26entry.906535625%3DShoujin%2520Huang%2520and%2520Guanxiong%2520Luo%2520and%2520Yuwan%2520Wang%2520and%2520Kexin%2520Yang%2520and%2520Lingyan%2520Zhang%2520and%2520Jingzhe%2520Liu%2520and%2520Hua%2520Guo%2520and%2520Min%2520Wang%2520and%2520Mengye%2520Lyu%26entry.1292438233%3D%2520%2520Simultaneous%2520multislice%2520%2528SMS%2529%2520imaging%2520is%2520a%2520powerful%2520technique%2520for%250Aaccelerating%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520acquisitions.%2520However%252C%2520SMS%250Areconstruction%2520remains%2520challenging%2520due%2520to%2520the%2520complex%2520signal%2520interactions%250Abetween%2520and%2520within%2520the%2520excited%2520slices.%2520This%2520study%2520presents%2520a%2520robust%2520SMS%2520MRI%250Areconstruction%2520method%2520using%2520deep%2520generative%2520priors.%2520Starting%2520from%2520Gaussian%250Anoise%252C%2520we%2520leverage%2520denoising%2520diffusion%2520probabilistic%2520models%2520%2528DDPM%2529%2520to%2520gradually%250Arecover%2520the%2520individual%2520slices%2520through%2520reverse%2520diffusion%2520iterations%2520while%250Aimposing%2520data%2520consistency%2520from%2520the%2520measured%2520k-space%2520under%2520readout%2520concatenation%250Aframework.%2520The%2520posterior%2520sampling%2520procedure%2520is%2520designed%2520such%2520that%2520the%2520DDPM%250Atraining%2520can%2520be%2520performed%2520on%2520single-slice%2520images%2520without%2520special%2520adjustments%250Afor%2520SMS%2520tasks.%2520Additionally%252C%2520our%2520method%2520integrates%2520a%2520low-frequency%2520enhancement%250A%2528LFE%2529%2520module%2520to%2520address%2520a%2520practical%2520issue%2520that%2520SMS-accelerated%2520fast%2520spin%2520echo%250A%2528FSE%2529%2520and%2520echo-planar%2520imaging%2520%2528EPI%2529%2520sequences%2520cannot%2520easily%2520embed%250Aautocalibration%2520signals.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520approach%250Aconsistently%2520outperforms%2520existing%2520methods%2520and%2520generalizes%2520well%2520to%2520unseen%250Adatasets.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/Solor-pikachu/ROGER%2520after%250Athe%2520review%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21600v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Simultaneous%20Multislice%20MRI%20Reconstruction%20Using%20Deep%20Generative%0A%20%20Priors&entry.906535625=Shoujin%20Huang%20and%20Guanxiong%20Luo%20and%20Yuwan%20Wang%20and%20Kexin%20Yang%20and%20Lingyan%20Zhang%20and%20Jingzhe%20Liu%20and%20Hua%20Guo%20and%20Min%20Wang%20and%20Mengye%20Lyu&entry.1292438233=%20%20Simultaneous%20multislice%20%28SMS%29%20imaging%20is%20a%20powerful%20technique%20for%0Aaccelerating%20magnetic%20resonance%20imaging%20%28MRI%29%20acquisitions.%20However%2C%20SMS%0Areconstruction%20remains%20challenging%20due%20to%20the%20complex%20signal%20interactions%0Abetween%20and%20within%20the%20excited%20slices.%20This%20study%20presents%20a%20robust%20SMS%20MRI%0Areconstruction%20method%20using%20deep%20generative%20priors.%20Starting%20from%20Gaussian%0Anoise%2C%20we%20leverage%20denoising%20diffusion%20probabilistic%20models%20%28DDPM%29%20to%20gradually%0Arecover%20the%20individual%20slices%20through%20reverse%20diffusion%20iterations%20while%0Aimposing%20data%20consistency%20from%20the%20measured%20k-space%20under%20readout%20concatenation%0Aframework.%20The%20posterior%20sampling%20procedure%20is%20designed%20such%20that%20the%20DDPM%0Atraining%20can%20be%20performed%20on%20single-slice%20images%20without%20special%20adjustments%0Afor%20SMS%20tasks.%20Additionally%2C%20our%20method%20integrates%20a%20low-frequency%20enhancement%0A%28LFE%29%20module%20to%20address%20a%20practical%20issue%20that%20SMS-accelerated%20fast%20spin%20echo%0A%28FSE%29%20and%20echo-planar%20imaging%20%28EPI%29%20sequences%20cannot%20easily%20embed%0Aautocalibration%20signals.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%0Aconsistently%20outperforms%20existing%20methods%20and%20generalizes%20well%20to%20unseen%0Adatasets.%20The%20code%20is%20available%20at%20https%3A//github.com/Solor-pikachu/ROGER%20after%0Athe%20review%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21600v1&entry.124074799=Read"},
{"title": "Green Edge AI: A Contemporary Survey", "author": "Yuyi Mao and Xianghao Yu and Kaibin Huang and Ying-Jun Angela Zhang and Jun Zhang", "abstract": "  Artificial intelligence (AI) technologies have emerged as pivotal enablers\nacross a multitude of industries largely due to their significant resurgence\nover the past decade. The transformative power of AI is primarily derived from\nthe utilization of deep neural networks (DNNs), which require extensive data\nfor training and substantial computational resources for processing.\nConsequently, DNN models are typically trained and deployed on resource-rich\ncloud servers. However, due to potential latency issues associated with cloud\ncommunications, deep learning (DL) workflows are increasingly being\ntransitioned to wireless edge networks in proximity to end-user devices (EUDs).\nThis shift is designed to support latency-sensitive applications and has given\nrise to a new paradigm of edge AI, which will play a critical role in upcoming\nsixth-generation (6G) networks to support ubiquitous AI applications. Despite\nits considerable potential, edge AI faces substantial challenges, mostly due to\nthe dichotomy between the resource limitations of wireless edge networks and\nthe resource-intensive nature of DL. Specifically, the acquisition of\nlarge-scale data, as well as the training and inference processes of DNNs, can\nrapidly deplete the battery energy of EUDs. This necessitates an\nenergy-conscious approach to edge AI to ensure both optimal and sustainable\nperformance. In this paper, we present a contemporary survey on green edge AI.\nWe commence by analyzing the principal energy consumption components of edge AI\nsystems to identify the fundamental design principles of green edge AI. Guided\nby these principles, we then explore energy-efficient design methodologies for\nthe three critical tasks in edge AI systems, including training data\nacquisition, edge training, and edge inference. Finally, we underscore\npotential future research directions to further enhance the energy efficiency\nof edge AI.\n", "link": "http://arxiv.org/abs/2312.00333v2", "date": "2024-07-31", "relevancy": 2.2907, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.473}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.453}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Green%20Edge%20AI%3A%20A%20Contemporary%20Survey&body=Title%3A%20Green%20Edge%20AI%3A%20A%20Contemporary%20Survey%0AAuthor%3A%20Yuyi%20Mao%20and%20Xianghao%20Yu%20and%20Kaibin%20Huang%20and%20Ying-Jun%20Angela%20Zhang%20and%20Jun%20Zhang%0AAbstract%3A%20%20%20Artificial%20intelligence%20%28AI%29%20technologies%20have%20emerged%20as%20pivotal%20enablers%0Aacross%20a%20multitude%20of%20industries%20largely%20due%20to%20their%20significant%20resurgence%0Aover%20the%20past%20decade.%20The%20transformative%20power%20of%20AI%20is%20primarily%20derived%20from%0Athe%20utilization%20of%20deep%20neural%20networks%20%28DNNs%29%2C%20which%20require%20extensive%20data%0Afor%20training%20and%20substantial%20computational%20resources%20for%20processing.%0AConsequently%2C%20DNN%20models%20are%20typically%20trained%20and%20deployed%20on%20resource-rich%0Acloud%20servers.%20However%2C%20due%20to%20potential%20latency%20issues%20associated%20with%20cloud%0Acommunications%2C%20deep%20learning%20%28DL%29%20workflows%20are%20increasingly%20being%0Atransitioned%20to%20wireless%20edge%20networks%20in%20proximity%20to%20end-user%20devices%20%28EUDs%29.%0AThis%20shift%20is%20designed%20to%20support%20latency-sensitive%20applications%20and%20has%20given%0Arise%20to%20a%20new%20paradigm%20of%20edge%20AI%2C%20which%20will%20play%20a%20critical%20role%20in%20upcoming%0Asixth-generation%20%286G%29%20networks%20to%20support%20ubiquitous%20AI%20applications.%20Despite%0Aits%20considerable%20potential%2C%20edge%20AI%20faces%20substantial%20challenges%2C%20mostly%20due%20to%0Athe%20dichotomy%20between%20the%20resource%20limitations%20of%20wireless%20edge%20networks%20and%0Athe%20resource-intensive%20nature%20of%20DL.%20Specifically%2C%20the%20acquisition%20of%0Alarge-scale%20data%2C%20as%20well%20as%20the%20training%20and%20inference%20processes%20of%20DNNs%2C%20can%0Arapidly%20deplete%20the%20battery%20energy%20of%20EUDs.%20This%20necessitates%20an%0Aenergy-conscious%20approach%20to%20edge%20AI%20to%20ensure%20both%20optimal%20and%20sustainable%0Aperformance.%20In%20this%20paper%2C%20we%20present%20a%20contemporary%20survey%20on%20green%20edge%20AI.%0AWe%20commence%20by%20analyzing%20the%20principal%20energy%20consumption%20components%20of%20edge%20AI%0Asystems%20to%20identify%20the%20fundamental%20design%20principles%20of%20green%20edge%20AI.%20Guided%0Aby%20these%20principles%2C%20we%20then%20explore%20energy-efficient%20design%20methodologies%20for%0Athe%20three%20critical%20tasks%20in%20edge%20AI%20systems%2C%20including%20training%20data%0Aacquisition%2C%20edge%20training%2C%20and%20edge%20inference.%20Finally%2C%20we%20underscore%0Apotential%20future%20research%20directions%20to%20further%20enhance%20the%20energy%20efficiency%0Aof%20edge%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00333v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGreen%2520Edge%2520AI%253A%2520A%2520Contemporary%2520Survey%26entry.906535625%3DYuyi%2520Mao%2520and%2520Xianghao%2520Yu%2520and%2520Kaibin%2520Huang%2520and%2520Ying-Jun%2520Angela%2520Zhang%2520and%2520Jun%2520Zhang%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520%2528AI%2529%2520technologies%2520have%2520emerged%2520as%2520pivotal%2520enablers%250Aacross%2520a%2520multitude%2520of%2520industries%2520largely%2520due%2520to%2520their%2520significant%2520resurgence%250Aover%2520the%2520past%2520decade.%2520The%2520transformative%2520power%2520of%2520AI%2520is%2520primarily%2520derived%2520from%250Athe%2520utilization%2520of%2520deep%2520neural%2520networks%2520%2528DNNs%2529%252C%2520which%2520require%2520extensive%2520data%250Afor%2520training%2520and%2520substantial%2520computational%2520resources%2520for%2520processing.%250AConsequently%252C%2520DNN%2520models%2520are%2520typically%2520trained%2520and%2520deployed%2520on%2520resource-rich%250Acloud%2520servers.%2520However%252C%2520due%2520to%2520potential%2520latency%2520issues%2520associated%2520with%2520cloud%250Acommunications%252C%2520deep%2520learning%2520%2528DL%2529%2520workflows%2520are%2520increasingly%2520being%250Atransitioned%2520to%2520wireless%2520edge%2520networks%2520in%2520proximity%2520to%2520end-user%2520devices%2520%2528EUDs%2529.%250AThis%2520shift%2520is%2520designed%2520to%2520support%2520latency-sensitive%2520applications%2520and%2520has%2520given%250Arise%2520to%2520a%2520new%2520paradigm%2520of%2520edge%2520AI%252C%2520which%2520will%2520play%2520a%2520critical%2520role%2520in%2520upcoming%250Asixth-generation%2520%25286G%2529%2520networks%2520to%2520support%2520ubiquitous%2520AI%2520applications.%2520Despite%250Aits%2520considerable%2520potential%252C%2520edge%2520AI%2520faces%2520substantial%2520challenges%252C%2520mostly%2520due%2520to%250Athe%2520dichotomy%2520between%2520the%2520resource%2520limitations%2520of%2520wireless%2520edge%2520networks%2520and%250Athe%2520resource-intensive%2520nature%2520of%2520DL.%2520Specifically%252C%2520the%2520acquisition%2520of%250Alarge-scale%2520data%252C%2520as%2520well%2520as%2520the%2520training%2520and%2520inference%2520processes%2520of%2520DNNs%252C%2520can%250Arapidly%2520deplete%2520the%2520battery%2520energy%2520of%2520EUDs.%2520This%2520necessitates%2520an%250Aenergy-conscious%2520approach%2520to%2520edge%2520AI%2520to%2520ensure%2520both%2520optimal%2520and%2520sustainable%250Aperformance.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520contemporary%2520survey%2520on%2520green%2520edge%2520AI.%250AWe%2520commence%2520by%2520analyzing%2520the%2520principal%2520energy%2520consumption%2520components%2520of%2520edge%2520AI%250Asystems%2520to%2520identify%2520the%2520fundamental%2520design%2520principles%2520of%2520green%2520edge%2520AI.%2520Guided%250Aby%2520these%2520principles%252C%2520we%2520then%2520explore%2520energy-efficient%2520design%2520methodologies%2520for%250Athe%2520three%2520critical%2520tasks%2520in%2520edge%2520AI%2520systems%252C%2520including%2520training%2520data%250Aacquisition%252C%2520edge%2520training%252C%2520and%2520edge%2520inference.%2520Finally%252C%2520we%2520underscore%250Apotential%2520future%2520research%2520directions%2520to%2520further%2520enhance%2520the%2520energy%2520efficiency%250Aof%2520edge%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.00333v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Green%20Edge%20AI%3A%20A%20Contemporary%20Survey&entry.906535625=Yuyi%20Mao%20and%20Xianghao%20Yu%20and%20Kaibin%20Huang%20and%20Ying-Jun%20Angela%20Zhang%20and%20Jun%20Zhang&entry.1292438233=%20%20Artificial%20intelligence%20%28AI%29%20technologies%20have%20emerged%20as%20pivotal%20enablers%0Aacross%20a%20multitude%20of%20industries%20largely%20due%20to%20their%20significant%20resurgence%0Aover%20the%20past%20decade.%20The%20transformative%20power%20of%20AI%20is%20primarily%20derived%20from%0Athe%20utilization%20of%20deep%20neural%20networks%20%28DNNs%29%2C%20which%20require%20extensive%20data%0Afor%20training%20and%20substantial%20computational%20resources%20for%20processing.%0AConsequently%2C%20DNN%20models%20are%20typically%20trained%20and%20deployed%20on%20resource-rich%0Acloud%20servers.%20However%2C%20due%20to%20potential%20latency%20issues%20associated%20with%20cloud%0Acommunications%2C%20deep%20learning%20%28DL%29%20workflows%20are%20increasingly%20being%0Atransitioned%20to%20wireless%20edge%20networks%20in%20proximity%20to%20end-user%20devices%20%28EUDs%29.%0AThis%20shift%20is%20designed%20to%20support%20latency-sensitive%20applications%20and%20has%20given%0Arise%20to%20a%20new%20paradigm%20of%20edge%20AI%2C%20which%20will%20play%20a%20critical%20role%20in%20upcoming%0Asixth-generation%20%286G%29%20networks%20to%20support%20ubiquitous%20AI%20applications.%20Despite%0Aits%20considerable%20potential%2C%20edge%20AI%20faces%20substantial%20challenges%2C%20mostly%20due%20to%0Athe%20dichotomy%20between%20the%20resource%20limitations%20of%20wireless%20edge%20networks%20and%0Athe%20resource-intensive%20nature%20of%20DL.%20Specifically%2C%20the%20acquisition%20of%0Alarge-scale%20data%2C%20as%20well%20as%20the%20training%20and%20inference%20processes%20of%20DNNs%2C%20can%0Arapidly%20deplete%20the%20battery%20energy%20of%20EUDs.%20This%20necessitates%20an%0Aenergy-conscious%20approach%20to%20edge%20AI%20to%20ensure%20both%20optimal%20and%20sustainable%0Aperformance.%20In%20this%20paper%2C%20we%20present%20a%20contemporary%20survey%20on%20green%20edge%20AI.%0AWe%20commence%20by%20analyzing%20the%20principal%20energy%20consumption%20components%20of%20edge%20AI%0Asystems%20to%20identify%20the%20fundamental%20design%20principles%20of%20green%20edge%20AI.%20Guided%0Aby%20these%20principles%2C%20we%20then%20explore%20energy-efficient%20design%20methodologies%20for%0Athe%20three%20critical%20tasks%20in%20edge%20AI%20systems%2C%20including%20training%20data%0Aacquisition%2C%20edge%20training%2C%20and%20edge%20inference.%20Finally%2C%20we%20underscore%0Apotential%20future%20research%20directions%20to%20further%20enhance%20the%20energy%20efficiency%0Aof%20edge%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00333v2&entry.124074799=Read"},
{"title": "Novel Hybrid Integrated Pix2Pix and WGAN Model with Gradient Penalty for\n  Binary Images Denoising", "author": "Luca Tirel and Ali Mohamed Ali and Hashim A. Hashim", "abstract": "  This paper introduces a novel approach to image denoising that leverages the\nadvantages of Generative Adversarial Networks (GANs). Specifically, we propose\na model that combines elements of the Pix2Pix model and the Wasserstein GAN\n(WGAN) with Gradient Penalty (WGAN-GP). This hybrid framework seeks to\ncapitalize on the denoising capabilities of conditional GANs, as demonstrated\nin the Pix2Pix model, while mitigating the need for an exhaustive search for\noptimal hyperparameters that could potentially ruin the stability of the\nlearning process. In the proposed method, the GAN's generator is employed to\nproduce denoised images, harnessing the power of a conditional GAN for noise\nreduction. Simultaneously, the implementation of the Lipschitz continuity\nconstraint during updates, as featured in WGAN-GP, aids in reducing\nsusceptibility to mode collapse. This innovative design allows the proposed\nmodel to benefit from the strong points of both Pix2Pix and WGAN-GP, generating\nsuperior denoising results while ensuring training stability. Drawing on\nprevious work on image-to-image translation and GAN stabilization techniques,\nthe proposed research highlights the potential of GANs as a general-purpose\nsolution for denoising. The paper details the development and testing of this\nmodel, showcasing its effectiveness through numerical experiments. The dataset\nwas created by adding synthetic noise to clean images. Numerical results based\non real-world dataset validation underscore the efficacy of this approach in\nimage-denoising tasks, exhibiting significant enhancements over traditional\ntechniques. Notably, the proposed model demonstrates strong generalization\ncapabilities, performing effectively even when trained with synthetic noise.\n", "link": "http://arxiv.org/abs/2407.11865v2", "date": "2024-07-31", "relevancy": 2.2821, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5885}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.558}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Novel%20Hybrid%20Integrated%20Pix2Pix%20and%20WGAN%20Model%20with%20Gradient%20Penalty%20for%0A%20%20Binary%20Images%20Denoising&body=Title%3A%20Novel%20Hybrid%20Integrated%20Pix2Pix%20and%20WGAN%20Model%20with%20Gradient%20Penalty%20for%0A%20%20Binary%20Images%20Denoising%0AAuthor%3A%20Luca%20Tirel%20and%20Ali%20Mohamed%20Ali%20and%20Hashim%20A.%20Hashim%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20approach%20to%20image%20denoising%20that%20leverages%20the%0Aadvantages%20of%20Generative%20Adversarial%20Networks%20%28GANs%29.%20Specifically%2C%20we%20propose%0Aa%20model%20that%20combines%20elements%20of%20the%20Pix2Pix%20model%20and%20the%20Wasserstein%20GAN%0A%28WGAN%29%20with%20Gradient%20Penalty%20%28WGAN-GP%29.%20This%20hybrid%20framework%20seeks%20to%0Acapitalize%20on%20the%20denoising%20capabilities%20of%20conditional%20GANs%2C%20as%20demonstrated%0Ain%20the%20Pix2Pix%20model%2C%20while%20mitigating%20the%20need%20for%20an%20exhaustive%20search%20for%0Aoptimal%20hyperparameters%20that%20could%20potentially%20ruin%20the%20stability%20of%20the%0Alearning%20process.%20In%20the%20proposed%20method%2C%20the%20GAN%27s%20generator%20is%20employed%20to%0Aproduce%20denoised%20images%2C%20harnessing%20the%20power%20of%20a%20conditional%20GAN%20for%20noise%0Areduction.%20Simultaneously%2C%20the%20implementation%20of%20the%20Lipschitz%20continuity%0Aconstraint%20during%20updates%2C%20as%20featured%20in%20WGAN-GP%2C%20aids%20in%20reducing%0Asusceptibility%20to%20mode%20collapse.%20This%20innovative%20design%20allows%20the%20proposed%0Amodel%20to%20benefit%20from%20the%20strong%20points%20of%20both%20Pix2Pix%20and%20WGAN-GP%2C%20generating%0Asuperior%20denoising%20results%20while%20ensuring%20training%20stability.%20Drawing%20on%0Aprevious%20work%20on%20image-to-image%20translation%20and%20GAN%20stabilization%20techniques%2C%0Athe%20proposed%20research%20highlights%20the%20potential%20of%20GANs%20as%20a%20general-purpose%0Asolution%20for%20denoising.%20The%20paper%20details%20the%20development%20and%20testing%20of%20this%0Amodel%2C%20showcasing%20its%20effectiveness%20through%20numerical%20experiments.%20The%20dataset%0Awas%20created%20by%20adding%20synthetic%20noise%20to%20clean%20images.%20Numerical%20results%20based%0Aon%20real-world%20dataset%20validation%20underscore%20the%20efficacy%20of%20this%20approach%20in%0Aimage-denoising%20tasks%2C%20exhibiting%20significant%20enhancements%20over%20traditional%0Atechniques.%20Notably%2C%20the%20proposed%20model%20demonstrates%20strong%20generalization%0Acapabilities%2C%20performing%20effectively%20even%20when%20trained%20with%20synthetic%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11865v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNovel%2520Hybrid%2520Integrated%2520Pix2Pix%2520and%2520WGAN%2520Model%2520with%2520Gradient%2520Penalty%2520for%250A%2520%2520Binary%2520Images%2520Denoising%26entry.906535625%3DLuca%2520Tirel%2520and%2520Ali%2520Mohamed%2520Ali%2520and%2520Hashim%2520A.%2520Hashim%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520approach%2520to%2520image%2520denoising%2520that%2520leverages%2520the%250Aadvantages%2520of%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529.%2520Specifically%252C%2520we%2520propose%250Aa%2520model%2520that%2520combines%2520elements%2520of%2520the%2520Pix2Pix%2520model%2520and%2520the%2520Wasserstein%2520GAN%250A%2528WGAN%2529%2520with%2520Gradient%2520Penalty%2520%2528WGAN-GP%2529.%2520This%2520hybrid%2520framework%2520seeks%2520to%250Acapitalize%2520on%2520the%2520denoising%2520capabilities%2520of%2520conditional%2520GANs%252C%2520as%2520demonstrated%250Ain%2520the%2520Pix2Pix%2520model%252C%2520while%2520mitigating%2520the%2520need%2520for%2520an%2520exhaustive%2520search%2520for%250Aoptimal%2520hyperparameters%2520that%2520could%2520potentially%2520ruin%2520the%2520stability%2520of%2520the%250Alearning%2520process.%2520In%2520the%2520proposed%2520method%252C%2520the%2520GAN%2527s%2520generator%2520is%2520employed%2520to%250Aproduce%2520denoised%2520images%252C%2520harnessing%2520the%2520power%2520of%2520a%2520conditional%2520GAN%2520for%2520noise%250Areduction.%2520Simultaneously%252C%2520the%2520implementation%2520of%2520the%2520Lipschitz%2520continuity%250Aconstraint%2520during%2520updates%252C%2520as%2520featured%2520in%2520WGAN-GP%252C%2520aids%2520in%2520reducing%250Asusceptibility%2520to%2520mode%2520collapse.%2520This%2520innovative%2520design%2520allows%2520the%2520proposed%250Amodel%2520to%2520benefit%2520from%2520the%2520strong%2520points%2520of%2520both%2520Pix2Pix%2520and%2520WGAN-GP%252C%2520generating%250Asuperior%2520denoising%2520results%2520while%2520ensuring%2520training%2520stability.%2520Drawing%2520on%250Aprevious%2520work%2520on%2520image-to-image%2520translation%2520and%2520GAN%2520stabilization%2520techniques%252C%250Athe%2520proposed%2520research%2520highlights%2520the%2520potential%2520of%2520GANs%2520as%2520a%2520general-purpose%250Asolution%2520for%2520denoising.%2520The%2520paper%2520details%2520the%2520development%2520and%2520testing%2520of%2520this%250Amodel%252C%2520showcasing%2520its%2520effectiveness%2520through%2520numerical%2520experiments.%2520The%2520dataset%250Awas%2520created%2520by%2520adding%2520synthetic%2520noise%2520to%2520clean%2520images.%2520Numerical%2520results%2520based%250Aon%2520real-world%2520dataset%2520validation%2520underscore%2520the%2520efficacy%2520of%2520this%2520approach%2520in%250Aimage-denoising%2520tasks%252C%2520exhibiting%2520significant%2520enhancements%2520over%2520traditional%250Atechniques.%2520Notably%252C%2520the%2520proposed%2520model%2520demonstrates%2520strong%2520generalization%250Acapabilities%252C%2520performing%2520effectively%2520even%2520when%2520trained%2520with%2520synthetic%2520noise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11865v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Novel%20Hybrid%20Integrated%20Pix2Pix%20and%20WGAN%20Model%20with%20Gradient%20Penalty%20for%0A%20%20Binary%20Images%20Denoising&entry.906535625=Luca%20Tirel%20and%20Ali%20Mohamed%20Ali%20and%20Hashim%20A.%20Hashim&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20approach%20to%20image%20denoising%20that%20leverages%20the%0Aadvantages%20of%20Generative%20Adversarial%20Networks%20%28GANs%29.%20Specifically%2C%20we%20propose%0Aa%20model%20that%20combines%20elements%20of%20the%20Pix2Pix%20model%20and%20the%20Wasserstein%20GAN%0A%28WGAN%29%20with%20Gradient%20Penalty%20%28WGAN-GP%29.%20This%20hybrid%20framework%20seeks%20to%0Acapitalize%20on%20the%20denoising%20capabilities%20of%20conditional%20GANs%2C%20as%20demonstrated%0Ain%20the%20Pix2Pix%20model%2C%20while%20mitigating%20the%20need%20for%20an%20exhaustive%20search%20for%0Aoptimal%20hyperparameters%20that%20could%20potentially%20ruin%20the%20stability%20of%20the%0Alearning%20process.%20In%20the%20proposed%20method%2C%20the%20GAN%27s%20generator%20is%20employed%20to%0Aproduce%20denoised%20images%2C%20harnessing%20the%20power%20of%20a%20conditional%20GAN%20for%20noise%0Areduction.%20Simultaneously%2C%20the%20implementation%20of%20the%20Lipschitz%20continuity%0Aconstraint%20during%20updates%2C%20as%20featured%20in%20WGAN-GP%2C%20aids%20in%20reducing%0Asusceptibility%20to%20mode%20collapse.%20This%20innovative%20design%20allows%20the%20proposed%0Amodel%20to%20benefit%20from%20the%20strong%20points%20of%20both%20Pix2Pix%20and%20WGAN-GP%2C%20generating%0Asuperior%20denoising%20results%20while%20ensuring%20training%20stability.%20Drawing%20on%0Aprevious%20work%20on%20image-to-image%20translation%20and%20GAN%20stabilization%20techniques%2C%0Athe%20proposed%20research%20highlights%20the%20potential%20of%20GANs%20as%20a%20general-purpose%0Asolution%20for%20denoising.%20The%20paper%20details%20the%20development%20and%20testing%20of%20this%0Amodel%2C%20showcasing%20its%20effectiveness%20through%20numerical%20experiments.%20The%20dataset%0Awas%20created%20by%20adding%20synthetic%20noise%20to%20clean%20images.%20Numerical%20results%20based%0Aon%20real-world%20dataset%20validation%20underscore%20the%20efficacy%20of%20this%20approach%20in%0Aimage-denoising%20tasks%2C%20exhibiting%20significant%20enhancements%20over%20traditional%0Atechniques.%20Notably%2C%20the%20proposed%20model%20demonstrates%20strong%20generalization%0Acapabilities%2C%20performing%20effectively%20even%20when%20trained%20with%20synthetic%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11865v2&entry.124074799=Read"},
{"title": "Textual Query-Driven Mask Transformer for Domain Generalized\n  Segmentation", "author": "Byeonghyun Pak and Byeongju Woo and Sunghwan Kim and Dae-hwan Kim and Hoseong Kim", "abstract": "  In this paper, we introduce a method to tackle Domain Generalized Semantic\nSegmentation (DGSS) by utilizing domain-invariant semantic knowledge from text\nembeddings of vision-language models. We employ the text embeddings as object\nqueries within a transformer-based segmentation framework (textual object\nqueries). These queries are regarded as a domain-invariant basis for pixel\ngrouping in DGSS. To leverage the power of textual object queries, we introduce\na novel framework named the textual query-driven mask transformer (tqdm). Our\ntqdm aims to (1) generate textual object queries that maximally encode\ndomain-invariant semantics and (2) enhance the semantic clarity of dense visual\nfeatures. Additionally, we suggest three regularization losses to improve the\nefficacy of tqdm by aligning between visual and textual features. By utilizing\nour method, the model can comprehend inherent semantic information for classes\nof interest, enabling it to generalize to extreme domains (e.g., sketch style).\nOur tqdm achieves 68.9 mIoU on GTA5$\\rightarrow$Cityscapes, outperforming the\nprior state-of-the-art method by 2.5 mIoU. The project page is available at\nhttps://byeonghyunpak.github.io/tqdm.\n", "link": "http://arxiv.org/abs/2407.09033v2", "date": "2024-07-31", "relevancy": 2.2802, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5775}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5692}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Textual%20Query-Driven%20Mask%20Transformer%20for%20Domain%20Generalized%0A%20%20Segmentation&body=Title%3A%20Textual%20Query-Driven%20Mask%20Transformer%20for%20Domain%20Generalized%0A%20%20Segmentation%0AAuthor%3A%20Byeonghyun%20Pak%20and%20Byeongju%20Woo%20and%20Sunghwan%20Kim%20and%20Dae-hwan%20Kim%20and%20Hoseong%20Kim%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20a%20method%20to%20tackle%20Domain%20Generalized%20Semantic%0ASegmentation%20%28DGSS%29%20by%20utilizing%20domain-invariant%20semantic%20knowledge%20from%20text%0Aembeddings%20of%20vision-language%20models.%20We%20employ%20the%20text%20embeddings%20as%20object%0Aqueries%20within%20a%20transformer-based%20segmentation%20framework%20%28textual%20object%0Aqueries%29.%20These%20queries%20are%20regarded%20as%20a%20domain-invariant%20basis%20for%20pixel%0Agrouping%20in%20DGSS.%20To%20leverage%20the%20power%20of%20textual%20object%20queries%2C%20we%20introduce%0Aa%20novel%20framework%20named%20the%20textual%20query-driven%20mask%20transformer%20%28tqdm%29.%20Our%0Atqdm%20aims%20to%20%281%29%20generate%20textual%20object%20queries%20that%20maximally%20encode%0Adomain-invariant%20semantics%20and%20%282%29%20enhance%20the%20semantic%20clarity%20of%20dense%20visual%0Afeatures.%20Additionally%2C%20we%20suggest%20three%20regularization%20losses%20to%20improve%20the%0Aefficacy%20of%20tqdm%20by%20aligning%20between%20visual%20and%20textual%20features.%20By%20utilizing%0Aour%20method%2C%20the%20model%20can%20comprehend%20inherent%20semantic%20information%20for%20classes%0Aof%20interest%2C%20enabling%20it%20to%20generalize%20to%20extreme%20domains%20%28e.g.%2C%20sketch%20style%29.%0AOur%20tqdm%20achieves%2068.9%20mIoU%20on%20GTA5%24%5Crightarrow%24Cityscapes%2C%20outperforming%20the%0Aprior%20state-of-the-art%20method%20by%202.5%20mIoU.%20The%20project%20page%20is%20available%20at%0Ahttps%3A//byeonghyunpak.github.io/tqdm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09033v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTextual%2520Query-Driven%2520Mask%2520Transformer%2520for%2520Domain%2520Generalized%250A%2520%2520Segmentation%26entry.906535625%3DByeonghyun%2520Pak%2520and%2520Byeongju%2520Woo%2520and%2520Sunghwan%2520Kim%2520and%2520Dae-hwan%2520Kim%2520and%2520Hoseong%2520Kim%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520method%2520to%2520tackle%2520Domain%2520Generalized%2520Semantic%250ASegmentation%2520%2528DGSS%2529%2520by%2520utilizing%2520domain-invariant%2520semantic%2520knowledge%2520from%2520text%250Aembeddings%2520of%2520vision-language%2520models.%2520We%2520employ%2520the%2520text%2520embeddings%2520as%2520object%250Aqueries%2520within%2520a%2520transformer-based%2520segmentation%2520framework%2520%2528textual%2520object%250Aqueries%2529.%2520These%2520queries%2520are%2520regarded%2520as%2520a%2520domain-invariant%2520basis%2520for%2520pixel%250Agrouping%2520in%2520DGSS.%2520To%2520leverage%2520the%2520power%2520of%2520textual%2520object%2520queries%252C%2520we%2520introduce%250Aa%2520novel%2520framework%2520named%2520the%2520textual%2520query-driven%2520mask%2520transformer%2520%2528tqdm%2529.%2520Our%250Atqdm%2520aims%2520to%2520%25281%2529%2520generate%2520textual%2520object%2520queries%2520that%2520maximally%2520encode%250Adomain-invariant%2520semantics%2520and%2520%25282%2529%2520enhance%2520the%2520semantic%2520clarity%2520of%2520dense%2520visual%250Afeatures.%2520Additionally%252C%2520we%2520suggest%2520three%2520regularization%2520losses%2520to%2520improve%2520the%250Aefficacy%2520of%2520tqdm%2520by%2520aligning%2520between%2520visual%2520and%2520textual%2520features.%2520By%2520utilizing%250Aour%2520method%252C%2520the%2520model%2520can%2520comprehend%2520inherent%2520semantic%2520information%2520for%2520classes%250Aof%2520interest%252C%2520enabling%2520it%2520to%2520generalize%2520to%2520extreme%2520domains%2520%2528e.g.%252C%2520sketch%2520style%2529.%250AOur%2520tqdm%2520achieves%252068.9%2520mIoU%2520on%2520GTA5%2524%255Crightarrow%2524Cityscapes%252C%2520outperforming%2520the%250Aprior%2520state-of-the-art%2520method%2520by%25202.5%2520mIoU.%2520The%2520project%2520page%2520is%2520available%2520at%250Ahttps%253A//byeonghyunpak.github.io/tqdm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09033v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Textual%20Query-Driven%20Mask%20Transformer%20for%20Domain%20Generalized%0A%20%20Segmentation&entry.906535625=Byeonghyun%20Pak%20and%20Byeongju%20Woo%20and%20Sunghwan%20Kim%20and%20Dae-hwan%20Kim%20and%20Hoseong%20Kim&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20a%20method%20to%20tackle%20Domain%20Generalized%20Semantic%0ASegmentation%20%28DGSS%29%20by%20utilizing%20domain-invariant%20semantic%20knowledge%20from%20text%0Aembeddings%20of%20vision-language%20models.%20We%20employ%20the%20text%20embeddings%20as%20object%0Aqueries%20within%20a%20transformer-based%20segmentation%20framework%20%28textual%20object%0Aqueries%29.%20These%20queries%20are%20regarded%20as%20a%20domain-invariant%20basis%20for%20pixel%0Agrouping%20in%20DGSS.%20To%20leverage%20the%20power%20of%20textual%20object%20queries%2C%20we%20introduce%0Aa%20novel%20framework%20named%20the%20textual%20query-driven%20mask%20transformer%20%28tqdm%29.%20Our%0Atqdm%20aims%20to%20%281%29%20generate%20textual%20object%20queries%20that%20maximally%20encode%0Adomain-invariant%20semantics%20and%20%282%29%20enhance%20the%20semantic%20clarity%20of%20dense%20visual%0Afeatures.%20Additionally%2C%20we%20suggest%20three%20regularization%20losses%20to%20improve%20the%0Aefficacy%20of%20tqdm%20by%20aligning%20between%20visual%20and%20textual%20features.%20By%20utilizing%0Aour%20method%2C%20the%20model%20can%20comprehend%20inherent%20semantic%20information%20for%20classes%0Aof%20interest%2C%20enabling%20it%20to%20generalize%20to%20extreme%20domains%20%28e.g.%2C%20sketch%20style%29.%0AOur%20tqdm%20achieves%2068.9%20mIoU%20on%20GTA5%24%5Crightarrow%24Cityscapes%2C%20outperforming%20the%0Aprior%20state-of-the-art%20method%20by%202.5%20mIoU.%20The%20project%20page%20is%20available%20at%0Ahttps%3A//byeonghyunpak.github.io/tqdm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09033v2&entry.124074799=Read"},
{"title": "Manifold learning in Wasserstein space", "author": "Keaton Hamm and Caroline Moosm\u00fcller and Bernhard Schmitzer and Matthew Thorpe", "abstract": "  This paper aims at building the theoretical foundations for manifold learning\nalgorithms in the space of absolutely continuous probability measures on a\ncompact and convex subset of $\\mathbb{R}^d$, metrized with the Wasserstein-2\ndistance $\\mathrm{W}$. We begin by introducing a construction of submanifolds\n$\\Lambda$ of probability measures equipped with metric $\\mathrm{W}_\\Lambda$,\nthe geodesic restriction of $W$ to $\\Lambda$. In contrast to other\nconstructions, these submanifolds are not necessarily flat, but still allow for\nlocal linearizations in a similar fashion to Riemannian submanifolds of\n$\\mathbb{R}^d$. We then show how the latent manifold structure of\n$(\\Lambda,\\mathrm{W}_{\\Lambda})$ can be learned from samples\n$\\{\\lambda_i\\}_{i=1}^N$ of $\\Lambda$ and pairwise extrinsic Wasserstein\ndistances $\\mathrm{W}$ only. In particular, we show that the metric space\n$(\\Lambda,\\mathrm{W}_{\\Lambda})$ can be asymptotically recovered in the sense\nof Gromov--Wasserstein from a graph with nodes $\\{\\lambda_i\\}_{i=1}^N$ and edge\nweights $W(\\lambda_i,\\lambda_j)$. In addition, we demonstrate how the tangent\nspace at a sample $\\lambda$ can be asymptotically recovered via spectral\nanalysis of a suitable \"covariance operator\" using optimal transport maps from\n$\\lambda$ to sufficiently close and diverse samples $\\{\\lambda_i\\}_{i=1}^N$.\nThe paper closes with some explicit constructions of submanifolds $\\Lambda$ and\nnumerical examples on the recovery of tangent spaces through spectral analysis.\n", "link": "http://arxiv.org/abs/2311.08549v2", "date": "2024-07-31", "relevancy": 2.2706, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4637}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4538}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Manifold%20learning%20in%20Wasserstein%20space&body=Title%3A%20Manifold%20learning%20in%20Wasserstein%20space%0AAuthor%3A%20Keaton%20Hamm%20and%20Caroline%20Moosm%C3%BCller%20and%20Bernhard%20Schmitzer%20and%20Matthew%20Thorpe%0AAbstract%3A%20%20%20This%20paper%20aims%20at%20building%20the%20theoretical%20foundations%20for%20manifold%20learning%0Aalgorithms%20in%20the%20space%20of%20absolutely%20continuous%20probability%20measures%20on%20a%0Acompact%20and%20convex%20subset%20of%20%24%5Cmathbb%7BR%7D%5Ed%24%2C%20metrized%20with%20the%20Wasserstein-2%0Adistance%20%24%5Cmathrm%7BW%7D%24.%20We%20begin%20by%20introducing%20a%20construction%20of%20submanifolds%0A%24%5CLambda%24%20of%20probability%20measures%20equipped%20with%20metric%20%24%5Cmathrm%7BW%7D_%5CLambda%24%2C%0Athe%20geodesic%20restriction%20of%20%24W%24%20to%20%24%5CLambda%24.%20In%20contrast%20to%20other%0Aconstructions%2C%20these%20submanifolds%20are%20not%20necessarily%20flat%2C%20but%20still%20allow%20for%0Alocal%20linearizations%20in%20a%20similar%20fashion%20to%20Riemannian%20submanifolds%20of%0A%24%5Cmathbb%7BR%7D%5Ed%24.%20We%20then%20show%20how%20the%20latent%20manifold%20structure%20of%0A%24%28%5CLambda%2C%5Cmathrm%7BW%7D_%7B%5CLambda%7D%29%24%20can%20be%20learned%20from%20samples%0A%24%5C%7B%5Clambda_i%5C%7D_%7Bi%3D1%7D%5EN%24%20of%20%24%5CLambda%24%20and%20pairwise%20extrinsic%20Wasserstein%0Adistances%20%24%5Cmathrm%7BW%7D%24%20only.%20In%20particular%2C%20we%20show%20that%20the%20metric%20space%0A%24%28%5CLambda%2C%5Cmathrm%7BW%7D_%7B%5CLambda%7D%29%24%20can%20be%20asymptotically%20recovered%20in%20the%20sense%0Aof%20Gromov--Wasserstein%20from%20a%20graph%20with%20nodes%20%24%5C%7B%5Clambda_i%5C%7D_%7Bi%3D1%7D%5EN%24%20and%20edge%0Aweights%20%24W%28%5Clambda_i%2C%5Clambda_j%29%24.%20In%20addition%2C%20we%20demonstrate%20how%20the%20tangent%0Aspace%20at%20a%20sample%20%24%5Clambda%24%20can%20be%20asymptotically%20recovered%20via%20spectral%0Aanalysis%20of%20a%20suitable%20%22covariance%20operator%22%20using%20optimal%20transport%20maps%20from%0A%24%5Clambda%24%20to%20sufficiently%20close%20and%20diverse%20samples%20%24%5C%7B%5Clambda_i%5C%7D_%7Bi%3D1%7D%5EN%24.%0AThe%20paper%20closes%20with%20some%20explicit%20constructions%20of%20submanifolds%20%24%5CLambda%24%20and%0Anumerical%20examples%20on%20the%20recovery%20of%20tangent%20spaces%20through%20spectral%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.08549v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DManifold%2520learning%2520in%2520Wasserstein%2520space%26entry.906535625%3DKeaton%2520Hamm%2520and%2520Caroline%2520Moosm%25C3%25BCller%2520and%2520Bernhard%2520Schmitzer%2520and%2520Matthew%2520Thorpe%26entry.1292438233%3D%2520%2520This%2520paper%2520aims%2520at%2520building%2520the%2520theoretical%2520foundations%2520for%2520manifold%2520learning%250Aalgorithms%2520in%2520the%2520space%2520of%2520absolutely%2520continuous%2520probability%2520measures%2520on%2520a%250Acompact%2520and%2520convex%2520subset%2520of%2520%2524%255Cmathbb%257BR%257D%255Ed%2524%252C%2520metrized%2520with%2520the%2520Wasserstein-2%250Adistance%2520%2524%255Cmathrm%257BW%257D%2524.%2520We%2520begin%2520by%2520introducing%2520a%2520construction%2520of%2520submanifolds%250A%2524%255CLambda%2524%2520of%2520probability%2520measures%2520equipped%2520with%2520metric%2520%2524%255Cmathrm%257BW%257D_%255CLambda%2524%252C%250Athe%2520geodesic%2520restriction%2520of%2520%2524W%2524%2520to%2520%2524%255CLambda%2524.%2520In%2520contrast%2520to%2520other%250Aconstructions%252C%2520these%2520submanifolds%2520are%2520not%2520necessarily%2520flat%252C%2520but%2520still%2520allow%2520for%250Alocal%2520linearizations%2520in%2520a%2520similar%2520fashion%2520to%2520Riemannian%2520submanifolds%2520of%250A%2524%255Cmathbb%257BR%257D%255Ed%2524.%2520We%2520then%2520show%2520how%2520the%2520latent%2520manifold%2520structure%2520of%250A%2524%2528%255CLambda%252C%255Cmathrm%257BW%257D_%257B%255CLambda%257D%2529%2524%2520can%2520be%2520learned%2520from%2520samples%250A%2524%255C%257B%255Clambda_i%255C%257D_%257Bi%253D1%257D%255EN%2524%2520of%2520%2524%255CLambda%2524%2520and%2520pairwise%2520extrinsic%2520Wasserstein%250Adistances%2520%2524%255Cmathrm%257BW%257D%2524%2520only.%2520In%2520particular%252C%2520we%2520show%2520that%2520the%2520metric%2520space%250A%2524%2528%255CLambda%252C%255Cmathrm%257BW%257D_%257B%255CLambda%257D%2529%2524%2520can%2520be%2520asymptotically%2520recovered%2520in%2520the%2520sense%250Aof%2520Gromov--Wasserstein%2520from%2520a%2520graph%2520with%2520nodes%2520%2524%255C%257B%255Clambda_i%255C%257D_%257Bi%253D1%257D%255EN%2524%2520and%2520edge%250Aweights%2520%2524W%2528%255Clambda_i%252C%255Clambda_j%2529%2524.%2520In%2520addition%252C%2520we%2520demonstrate%2520how%2520the%2520tangent%250Aspace%2520at%2520a%2520sample%2520%2524%255Clambda%2524%2520can%2520be%2520asymptotically%2520recovered%2520via%2520spectral%250Aanalysis%2520of%2520a%2520suitable%2520%2522covariance%2520operator%2522%2520using%2520optimal%2520transport%2520maps%2520from%250A%2524%255Clambda%2524%2520to%2520sufficiently%2520close%2520and%2520diverse%2520samples%2520%2524%255C%257B%255Clambda_i%255C%257D_%257Bi%253D1%257D%255EN%2524.%250AThe%2520paper%2520closes%2520with%2520some%2520explicit%2520constructions%2520of%2520submanifolds%2520%2524%255CLambda%2524%2520and%250Anumerical%2520examples%2520on%2520the%2520recovery%2520of%2520tangent%2520spaces%2520through%2520spectral%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.08549v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Manifold%20learning%20in%20Wasserstein%20space&entry.906535625=Keaton%20Hamm%20and%20Caroline%20Moosm%C3%BCller%20and%20Bernhard%20Schmitzer%20and%20Matthew%20Thorpe&entry.1292438233=%20%20This%20paper%20aims%20at%20building%20the%20theoretical%20foundations%20for%20manifold%20learning%0Aalgorithms%20in%20the%20space%20of%20absolutely%20continuous%20probability%20measures%20on%20a%0Acompact%20and%20convex%20subset%20of%20%24%5Cmathbb%7BR%7D%5Ed%24%2C%20metrized%20with%20the%20Wasserstein-2%0Adistance%20%24%5Cmathrm%7BW%7D%24.%20We%20begin%20by%20introducing%20a%20construction%20of%20submanifolds%0A%24%5CLambda%24%20of%20probability%20measures%20equipped%20with%20metric%20%24%5Cmathrm%7BW%7D_%5CLambda%24%2C%0Athe%20geodesic%20restriction%20of%20%24W%24%20to%20%24%5CLambda%24.%20In%20contrast%20to%20other%0Aconstructions%2C%20these%20submanifolds%20are%20not%20necessarily%20flat%2C%20but%20still%20allow%20for%0Alocal%20linearizations%20in%20a%20similar%20fashion%20to%20Riemannian%20submanifolds%20of%0A%24%5Cmathbb%7BR%7D%5Ed%24.%20We%20then%20show%20how%20the%20latent%20manifold%20structure%20of%0A%24%28%5CLambda%2C%5Cmathrm%7BW%7D_%7B%5CLambda%7D%29%24%20can%20be%20learned%20from%20samples%0A%24%5C%7B%5Clambda_i%5C%7D_%7Bi%3D1%7D%5EN%24%20of%20%24%5CLambda%24%20and%20pairwise%20extrinsic%20Wasserstein%0Adistances%20%24%5Cmathrm%7BW%7D%24%20only.%20In%20particular%2C%20we%20show%20that%20the%20metric%20space%0A%24%28%5CLambda%2C%5Cmathrm%7BW%7D_%7B%5CLambda%7D%29%24%20can%20be%20asymptotically%20recovered%20in%20the%20sense%0Aof%20Gromov--Wasserstein%20from%20a%20graph%20with%20nodes%20%24%5C%7B%5Clambda_i%5C%7D_%7Bi%3D1%7D%5EN%24%20and%20edge%0Aweights%20%24W%28%5Clambda_i%2C%5Clambda_j%29%24.%20In%20addition%2C%20we%20demonstrate%20how%20the%20tangent%0Aspace%20at%20a%20sample%20%24%5Clambda%24%20can%20be%20asymptotically%20recovered%20via%20spectral%0Aanalysis%20of%20a%20suitable%20%22covariance%20operator%22%20using%20optimal%20transport%20maps%20from%0A%24%5Clambda%24%20to%20sufficiently%20close%20and%20diverse%20samples%20%24%5C%7B%5Clambda_i%5C%7D_%7Bi%3D1%7D%5EN%24.%0AThe%20paper%20closes%20with%20some%20explicit%20constructions%20of%20submanifolds%20%24%5CLambda%24%20and%0Anumerical%20examples%20on%20the%20recovery%20of%20tangent%20spaces%20through%20spectral%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.08549v2&entry.124074799=Read"},
{"title": "A Federated Learning-Friendly Approach for Parameter-Efficient\n  Fine-Tuning of SAM in 3D Segmentation", "author": "Mothilal Asokan and Joseph Geo Benjamin and Mohammad Yaqub and Karthik Nandakumar", "abstract": "  Adapting foundation models for medical image analysis requires finetuning\nthem on a considerable amount of data because of extreme distribution shifts\nbetween natural (source) data used for pretraining and medical (target) data.\nHowever, collecting task-specific medical data for such finetuning at a central\nlocation raises many privacy concerns. Although Federated learning (FL)\nprovides an effective means for training on private decentralized data,\ncommunication costs in federating large foundation models can quickly become a\nsignificant bottleneck, impacting the solution's scalability. In this work, we\naddress this problem of efficient communication while ensuring effective\nlearning in FL by combining the strengths of Parameter-Efficient Fine-tuning\n(PEFT) with FL. Specifically, we study plug-and-play Low-Rank Adapters (LoRA)\nin a federated manner to adapt the Segment Anything Model (SAM) for 3D medical\nimage segmentation. Unlike prior works that utilize LoRA and finetune the\nentire decoder, we critically analyze the contribution of each granular\ncomponent of SAM on finetuning performance. Thus, we identify specific layers\nto be federated that are very efficient in terms of communication cost while\nproducing on-par accuracy. Our experiments show that retaining the parameters\nof the SAM model (including most of the decoder) in their original state during\nadaptation is beneficial because fine-tuning on small datasets tends to distort\nthe inherent capabilities of the underlying foundation model. On Fed-KiTS, our\napproach decreases communication cost (~48x) compared to full fine-tuning while\nincreasing performance (~6% Dice score) in 3D segmentation tasks. Our approach\nperforms similar to SAMed while achieving ~2.8x reduction in communication and\nparameters to be finetuned. We further validate our approach with experiments\non Fed-IXI and Prostate MRI datasets.\n", "link": "http://arxiv.org/abs/2407.21739v1", "date": "2024-07-31", "relevancy": 2.2646, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5804}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5647}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Federated%20Learning-Friendly%20Approach%20for%20Parameter-Efficient%0A%20%20Fine-Tuning%20of%20SAM%20in%203D%20Segmentation&body=Title%3A%20A%20Federated%20Learning-Friendly%20Approach%20for%20Parameter-Efficient%0A%20%20Fine-Tuning%20of%20SAM%20in%203D%20Segmentation%0AAuthor%3A%20Mothilal%20Asokan%20and%20Joseph%20Geo%20Benjamin%20and%20Mohammad%20Yaqub%20and%20Karthik%20Nandakumar%0AAbstract%3A%20%20%20Adapting%20foundation%20models%20for%20medical%20image%20analysis%20requires%20finetuning%0Athem%20on%20a%20considerable%20amount%20of%20data%20because%20of%20extreme%20distribution%20shifts%0Abetween%20natural%20%28source%29%20data%20used%20for%20pretraining%20and%20medical%20%28target%29%20data.%0AHowever%2C%20collecting%20task-specific%20medical%20data%20for%20such%20finetuning%20at%20a%20central%0Alocation%20raises%20many%20privacy%20concerns.%20Although%20Federated%20learning%20%28FL%29%0Aprovides%20an%20effective%20means%20for%20training%20on%20private%20decentralized%20data%2C%0Acommunication%20costs%20in%20federating%20large%20foundation%20models%20can%20quickly%20become%20a%0Asignificant%20bottleneck%2C%20impacting%20the%20solution%27s%20scalability.%20In%20this%20work%2C%20we%0Aaddress%20this%20problem%20of%20efficient%20communication%20while%20ensuring%20effective%0Alearning%20in%20FL%20by%20combining%20the%20strengths%20of%20Parameter-Efficient%20Fine-tuning%0A%28PEFT%29%20with%20FL.%20Specifically%2C%20we%20study%20plug-and-play%20Low-Rank%20Adapters%20%28LoRA%29%0Ain%20a%20federated%20manner%20to%20adapt%20the%20Segment%20Anything%20Model%20%28SAM%29%20for%203D%20medical%0Aimage%20segmentation.%20Unlike%20prior%20works%20that%20utilize%20LoRA%20and%20finetune%20the%0Aentire%20decoder%2C%20we%20critically%20analyze%20the%20contribution%20of%20each%20granular%0Acomponent%20of%20SAM%20on%20finetuning%20performance.%20Thus%2C%20we%20identify%20specific%20layers%0Ato%20be%20federated%20that%20are%20very%20efficient%20in%20terms%20of%20communication%20cost%20while%0Aproducing%20on-par%20accuracy.%20Our%20experiments%20show%20that%20retaining%20the%20parameters%0Aof%20the%20SAM%20model%20%28including%20most%20of%20the%20decoder%29%20in%20their%20original%20state%20during%0Aadaptation%20is%20beneficial%20because%20fine-tuning%20on%20small%20datasets%20tends%20to%20distort%0Athe%20inherent%20capabilities%20of%20the%20underlying%20foundation%20model.%20On%20Fed-KiTS%2C%20our%0Aapproach%20decreases%20communication%20cost%20%28~48x%29%20compared%20to%20full%20fine-tuning%20while%0Aincreasing%20performance%20%28~6%25%20Dice%20score%29%20in%203D%20segmentation%20tasks.%20Our%20approach%0Aperforms%20similar%20to%20SAMed%20while%20achieving%20~2.8x%20reduction%20in%20communication%20and%0Aparameters%20to%20be%20finetuned.%20We%20further%20validate%20our%20approach%20with%20experiments%0Aon%20Fed-IXI%20and%20Prostate%20MRI%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21739v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Federated%2520Learning-Friendly%2520Approach%2520for%2520Parameter-Efficient%250A%2520%2520Fine-Tuning%2520of%2520SAM%2520in%25203D%2520Segmentation%26entry.906535625%3DMothilal%2520Asokan%2520and%2520Joseph%2520Geo%2520Benjamin%2520and%2520Mohammad%2520Yaqub%2520and%2520Karthik%2520Nandakumar%26entry.1292438233%3D%2520%2520Adapting%2520foundation%2520models%2520for%2520medical%2520image%2520analysis%2520requires%2520finetuning%250Athem%2520on%2520a%2520considerable%2520amount%2520of%2520data%2520because%2520of%2520extreme%2520distribution%2520shifts%250Abetween%2520natural%2520%2528source%2529%2520data%2520used%2520for%2520pretraining%2520and%2520medical%2520%2528target%2529%2520data.%250AHowever%252C%2520collecting%2520task-specific%2520medical%2520data%2520for%2520such%2520finetuning%2520at%2520a%2520central%250Alocation%2520raises%2520many%2520privacy%2520concerns.%2520Although%2520Federated%2520learning%2520%2528FL%2529%250Aprovides%2520an%2520effective%2520means%2520for%2520training%2520on%2520private%2520decentralized%2520data%252C%250Acommunication%2520costs%2520in%2520federating%2520large%2520foundation%2520models%2520can%2520quickly%2520become%2520a%250Asignificant%2520bottleneck%252C%2520impacting%2520the%2520solution%2527s%2520scalability.%2520In%2520this%2520work%252C%2520we%250Aaddress%2520this%2520problem%2520of%2520efficient%2520communication%2520while%2520ensuring%2520effective%250Alearning%2520in%2520FL%2520by%2520combining%2520the%2520strengths%2520of%2520Parameter-Efficient%2520Fine-tuning%250A%2528PEFT%2529%2520with%2520FL.%2520Specifically%252C%2520we%2520study%2520plug-and-play%2520Low-Rank%2520Adapters%2520%2528LoRA%2529%250Ain%2520a%2520federated%2520manner%2520to%2520adapt%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520for%25203D%2520medical%250Aimage%2520segmentation.%2520Unlike%2520prior%2520works%2520that%2520utilize%2520LoRA%2520and%2520finetune%2520the%250Aentire%2520decoder%252C%2520we%2520critically%2520analyze%2520the%2520contribution%2520of%2520each%2520granular%250Acomponent%2520of%2520SAM%2520on%2520finetuning%2520performance.%2520Thus%252C%2520we%2520identify%2520specific%2520layers%250Ato%2520be%2520federated%2520that%2520are%2520very%2520efficient%2520in%2520terms%2520of%2520communication%2520cost%2520while%250Aproducing%2520on-par%2520accuracy.%2520Our%2520experiments%2520show%2520that%2520retaining%2520the%2520parameters%250Aof%2520the%2520SAM%2520model%2520%2528including%2520most%2520of%2520the%2520decoder%2529%2520in%2520their%2520original%2520state%2520during%250Aadaptation%2520is%2520beneficial%2520because%2520fine-tuning%2520on%2520small%2520datasets%2520tends%2520to%2520distort%250Athe%2520inherent%2520capabilities%2520of%2520the%2520underlying%2520foundation%2520model.%2520On%2520Fed-KiTS%252C%2520our%250Aapproach%2520decreases%2520communication%2520cost%2520%2528~48x%2529%2520compared%2520to%2520full%2520fine-tuning%2520while%250Aincreasing%2520performance%2520%2528~6%2525%2520Dice%2520score%2529%2520in%25203D%2520segmentation%2520tasks.%2520Our%2520approach%250Aperforms%2520similar%2520to%2520SAMed%2520while%2520achieving%2520~2.8x%2520reduction%2520in%2520communication%2520and%250Aparameters%2520to%2520be%2520finetuned.%2520We%2520further%2520validate%2520our%2520approach%2520with%2520experiments%250Aon%2520Fed-IXI%2520and%2520Prostate%2520MRI%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21739v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Federated%20Learning-Friendly%20Approach%20for%20Parameter-Efficient%0A%20%20Fine-Tuning%20of%20SAM%20in%203D%20Segmentation&entry.906535625=Mothilal%20Asokan%20and%20Joseph%20Geo%20Benjamin%20and%20Mohammad%20Yaqub%20and%20Karthik%20Nandakumar&entry.1292438233=%20%20Adapting%20foundation%20models%20for%20medical%20image%20analysis%20requires%20finetuning%0Athem%20on%20a%20considerable%20amount%20of%20data%20because%20of%20extreme%20distribution%20shifts%0Abetween%20natural%20%28source%29%20data%20used%20for%20pretraining%20and%20medical%20%28target%29%20data.%0AHowever%2C%20collecting%20task-specific%20medical%20data%20for%20such%20finetuning%20at%20a%20central%0Alocation%20raises%20many%20privacy%20concerns.%20Although%20Federated%20learning%20%28FL%29%0Aprovides%20an%20effective%20means%20for%20training%20on%20private%20decentralized%20data%2C%0Acommunication%20costs%20in%20federating%20large%20foundation%20models%20can%20quickly%20become%20a%0Asignificant%20bottleneck%2C%20impacting%20the%20solution%27s%20scalability.%20In%20this%20work%2C%20we%0Aaddress%20this%20problem%20of%20efficient%20communication%20while%20ensuring%20effective%0Alearning%20in%20FL%20by%20combining%20the%20strengths%20of%20Parameter-Efficient%20Fine-tuning%0A%28PEFT%29%20with%20FL.%20Specifically%2C%20we%20study%20plug-and-play%20Low-Rank%20Adapters%20%28LoRA%29%0Ain%20a%20federated%20manner%20to%20adapt%20the%20Segment%20Anything%20Model%20%28SAM%29%20for%203D%20medical%0Aimage%20segmentation.%20Unlike%20prior%20works%20that%20utilize%20LoRA%20and%20finetune%20the%0Aentire%20decoder%2C%20we%20critically%20analyze%20the%20contribution%20of%20each%20granular%0Acomponent%20of%20SAM%20on%20finetuning%20performance.%20Thus%2C%20we%20identify%20specific%20layers%0Ato%20be%20federated%20that%20are%20very%20efficient%20in%20terms%20of%20communication%20cost%20while%0Aproducing%20on-par%20accuracy.%20Our%20experiments%20show%20that%20retaining%20the%20parameters%0Aof%20the%20SAM%20model%20%28including%20most%20of%20the%20decoder%29%20in%20their%20original%20state%20during%0Aadaptation%20is%20beneficial%20because%20fine-tuning%20on%20small%20datasets%20tends%20to%20distort%0Athe%20inherent%20capabilities%20of%20the%20underlying%20foundation%20model.%20On%20Fed-KiTS%2C%20our%0Aapproach%20decreases%20communication%20cost%20%28~48x%29%20compared%20to%20full%20fine-tuning%20while%0Aincreasing%20performance%20%28~6%25%20Dice%20score%29%20in%203D%20segmentation%20tasks.%20Our%20approach%0Aperforms%20similar%20to%20SAMed%20while%20achieving%20~2.8x%20reduction%20in%20communication%20and%0Aparameters%20to%20be%20finetuned.%20We%20further%20validate%20our%20approach%20with%20experiments%0Aon%20Fed-IXI%20and%20Prostate%20MRI%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21739v1&entry.124074799=Read"},
{"title": "Unifying Event-based Flow, Stereo and Depth Estimation via Feature\n  Similarity Matching", "author": "Pengjie Zhang and Lin Zhu and Lizhi Wang and Hua Huang", "abstract": "  As an emerging vision sensor, the event camera has gained popularity in\nvarious vision tasks such as optical flow estimation, stereo matching, and\ndepth estimation due to its high-speed, sparse, and asynchronous event streams.\nUnlike traditional approaches that use specialized architectures for each\nspecific task, we propose a unified framework, EventMatch, that reformulates\nthese tasks as an event-based dense correspondence matching problem, allowing\nthem to be solved with a single model by directly comparing feature\nsimilarities. By utilizing a shared feature similarities module, which\nintegrates knowledge from other event flows via temporal or spatial\ninteractions, and distinct task heads, our network can concurrently perform\noptical flow estimation from temporal inputs (e.g., two segments of event\nstreams in the temporal domain) and stereo matching from spatial inputs (e.g.,\ntwo segments of event streams from different viewpoints in the spatial domain).\nMoreover, we further demonstrate that our unified model inherently supports\ncross-task transfer since the architecture and parameters are shared across\ntasks. Without the need for retraining on each task, our model can effectively\nhandle both optical flow and disparity estimation simultaneously. The\nexperiment conducted on the DSEC benchmark demonstrates that our model exhibits\nsuperior performance in both optical flow and disparity estimation tasks,\noutperforming existing state-of-the-art methods. Our unified approach not only\nadvances event-based models but also opens new possibilities for cross-task\ntransfer and inter-task fusion in both spatial and temporal dimensions. Our\ncode will be available later.\n", "link": "http://arxiv.org/abs/2407.21735v1", "date": "2024-07-31", "relevancy": 2.2464, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5801}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5657}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unifying%20Event-based%20Flow%2C%20Stereo%20and%20Depth%20Estimation%20via%20Feature%0A%20%20Similarity%20Matching&body=Title%3A%20Unifying%20Event-based%20Flow%2C%20Stereo%20and%20Depth%20Estimation%20via%20Feature%0A%20%20Similarity%20Matching%0AAuthor%3A%20Pengjie%20Zhang%20and%20Lin%20Zhu%20and%20Lizhi%20Wang%20and%20Hua%20Huang%0AAbstract%3A%20%20%20As%20an%20emerging%20vision%20sensor%2C%20the%20event%20camera%20has%20gained%20popularity%20in%0Avarious%20vision%20tasks%20such%20as%20optical%20flow%20estimation%2C%20stereo%20matching%2C%20and%0Adepth%20estimation%20due%20to%20its%20high-speed%2C%20sparse%2C%20and%20asynchronous%20event%20streams.%0AUnlike%20traditional%20approaches%20that%20use%20specialized%20architectures%20for%20each%0Aspecific%20task%2C%20we%20propose%20a%20unified%20framework%2C%20EventMatch%2C%20that%20reformulates%0Athese%20tasks%20as%20an%20event-based%20dense%20correspondence%20matching%20problem%2C%20allowing%0Athem%20to%20be%20solved%20with%20a%20single%20model%20by%20directly%20comparing%20feature%0Asimilarities.%20By%20utilizing%20a%20shared%20feature%20similarities%20module%2C%20which%0Aintegrates%20knowledge%20from%20other%20event%20flows%20via%20temporal%20or%20spatial%0Ainteractions%2C%20and%20distinct%20task%20heads%2C%20our%20network%20can%20concurrently%20perform%0Aoptical%20flow%20estimation%20from%20temporal%20inputs%20%28e.g.%2C%20two%20segments%20of%20event%0Astreams%20in%20the%20temporal%20domain%29%20and%20stereo%20matching%20from%20spatial%20inputs%20%28e.g.%2C%0Atwo%20segments%20of%20event%20streams%20from%20different%20viewpoints%20in%20the%20spatial%20domain%29.%0AMoreover%2C%20we%20further%20demonstrate%20that%20our%20unified%20model%20inherently%20supports%0Across-task%20transfer%20since%20the%20architecture%20and%20parameters%20are%20shared%20across%0Atasks.%20Without%20the%20need%20for%20retraining%20on%20each%20task%2C%20our%20model%20can%20effectively%0Ahandle%20both%20optical%20flow%20and%20disparity%20estimation%20simultaneously.%20The%0Aexperiment%20conducted%20on%20the%20DSEC%20benchmark%20demonstrates%20that%20our%20model%20exhibits%0Asuperior%20performance%20in%20both%20optical%20flow%20and%20disparity%20estimation%20tasks%2C%0Aoutperforming%20existing%20state-of-the-art%20methods.%20Our%20unified%20approach%20not%20only%0Aadvances%20event-based%20models%20but%20also%20opens%20new%20possibilities%20for%20cross-task%0Atransfer%20and%20inter-task%20fusion%20in%20both%20spatial%20and%20temporal%20dimensions.%20Our%0Acode%20will%20be%20available%20later.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21735v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnifying%2520Event-based%2520Flow%252C%2520Stereo%2520and%2520Depth%2520Estimation%2520via%2520Feature%250A%2520%2520Similarity%2520Matching%26entry.906535625%3DPengjie%2520Zhang%2520and%2520Lin%2520Zhu%2520and%2520Lizhi%2520Wang%2520and%2520Hua%2520Huang%26entry.1292438233%3D%2520%2520As%2520an%2520emerging%2520vision%2520sensor%252C%2520the%2520event%2520camera%2520has%2520gained%2520popularity%2520in%250Avarious%2520vision%2520tasks%2520such%2520as%2520optical%2520flow%2520estimation%252C%2520stereo%2520matching%252C%2520and%250Adepth%2520estimation%2520due%2520to%2520its%2520high-speed%252C%2520sparse%252C%2520and%2520asynchronous%2520event%2520streams.%250AUnlike%2520traditional%2520approaches%2520that%2520use%2520specialized%2520architectures%2520for%2520each%250Aspecific%2520task%252C%2520we%2520propose%2520a%2520unified%2520framework%252C%2520EventMatch%252C%2520that%2520reformulates%250Athese%2520tasks%2520as%2520an%2520event-based%2520dense%2520correspondence%2520matching%2520problem%252C%2520allowing%250Athem%2520to%2520be%2520solved%2520with%2520a%2520single%2520model%2520by%2520directly%2520comparing%2520feature%250Asimilarities.%2520By%2520utilizing%2520a%2520shared%2520feature%2520similarities%2520module%252C%2520which%250Aintegrates%2520knowledge%2520from%2520other%2520event%2520flows%2520via%2520temporal%2520or%2520spatial%250Ainteractions%252C%2520and%2520distinct%2520task%2520heads%252C%2520our%2520network%2520can%2520concurrently%2520perform%250Aoptical%2520flow%2520estimation%2520from%2520temporal%2520inputs%2520%2528e.g.%252C%2520two%2520segments%2520of%2520event%250Astreams%2520in%2520the%2520temporal%2520domain%2529%2520and%2520stereo%2520matching%2520from%2520spatial%2520inputs%2520%2528e.g.%252C%250Atwo%2520segments%2520of%2520event%2520streams%2520from%2520different%2520viewpoints%2520in%2520the%2520spatial%2520domain%2529.%250AMoreover%252C%2520we%2520further%2520demonstrate%2520that%2520our%2520unified%2520model%2520inherently%2520supports%250Across-task%2520transfer%2520since%2520the%2520architecture%2520and%2520parameters%2520are%2520shared%2520across%250Atasks.%2520Without%2520the%2520need%2520for%2520retraining%2520on%2520each%2520task%252C%2520our%2520model%2520can%2520effectively%250Ahandle%2520both%2520optical%2520flow%2520and%2520disparity%2520estimation%2520simultaneously.%2520The%250Aexperiment%2520conducted%2520on%2520the%2520DSEC%2520benchmark%2520demonstrates%2520that%2520our%2520model%2520exhibits%250Asuperior%2520performance%2520in%2520both%2520optical%2520flow%2520and%2520disparity%2520estimation%2520tasks%252C%250Aoutperforming%2520existing%2520state-of-the-art%2520methods.%2520Our%2520unified%2520approach%2520not%2520only%250Aadvances%2520event-based%2520models%2520but%2520also%2520opens%2520new%2520possibilities%2520for%2520cross-task%250Atransfer%2520and%2520inter-task%2520fusion%2520in%2520both%2520spatial%2520and%2520temporal%2520dimensions.%2520Our%250Acode%2520will%2520be%2520available%2520later.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21735v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unifying%20Event-based%20Flow%2C%20Stereo%20and%20Depth%20Estimation%20via%20Feature%0A%20%20Similarity%20Matching&entry.906535625=Pengjie%20Zhang%20and%20Lin%20Zhu%20and%20Lizhi%20Wang%20and%20Hua%20Huang&entry.1292438233=%20%20As%20an%20emerging%20vision%20sensor%2C%20the%20event%20camera%20has%20gained%20popularity%20in%0Avarious%20vision%20tasks%20such%20as%20optical%20flow%20estimation%2C%20stereo%20matching%2C%20and%0Adepth%20estimation%20due%20to%20its%20high-speed%2C%20sparse%2C%20and%20asynchronous%20event%20streams.%0AUnlike%20traditional%20approaches%20that%20use%20specialized%20architectures%20for%20each%0Aspecific%20task%2C%20we%20propose%20a%20unified%20framework%2C%20EventMatch%2C%20that%20reformulates%0Athese%20tasks%20as%20an%20event-based%20dense%20correspondence%20matching%20problem%2C%20allowing%0Athem%20to%20be%20solved%20with%20a%20single%20model%20by%20directly%20comparing%20feature%0Asimilarities.%20By%20utilizing%20a%20shared%20feature%20similarities%20module%2C%20which%0Aintegrates%20knowledge%20from%20other%20event%20flows%20via%20temporal%20or%20spatial%0Ainteractions%2C%20and%20distinct%20task%20heads%2C%20our%20network%20can%20concurrently%20perform%0Aoptical%20flow%20estimation%20from%20temporal%20inputs%20%28e.g.%2C%20two%20segments%20of%20event%0Astreams%20in%20the%20temporal%20domain%29%20and%20stereo%20matching%20from%20spatial%20inputs%20%28e.g.%2C%0Atwo%20segments%20of%20event%20streams%20from%20different%20viewpoints%20in%20the%20spatial%20domain%29.%0AMoreover%2C%20we%20further%20demonstrate%20that%20our%20unified%20model%20inherently%20supports%0Across-task%20transfer%20since%20the%20architecture%20and%20parameters%20are%20shared%20across%0Atasks.%20Without%20the%20need%20for%20retraining%20on%20each%20task%2C%20our%20model%20can%20effectively%0Ahandle%20both%20optical%20flow%20and%20disparity%20estimation%20simultaneously.%20The%0Aexperiment%20conducted%20on%20the%20DSEC%20benchmark%20demonstrates%20that%20our%20model%20exhibits%0Asuperior%20performance%20in%20both%20optical%20flow%20and%20disparity%20estimation%20tasks%2C%0Aoutperforming%20existing%20state-of-the-art%20methods.%20Our%20unified%20approach%20not%20only%0Aadvances%20event-based%20models%20but%20also%20opens%20new%20possibilities%20for%20cross-task%0Atransfer%20and%20inter-task%20fusion%20in%20both%20spatial%20and%20temporal%20dimensions.%20Our%0Acode%20will%20be%20available%20later.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21735v1&entry.124074799=Read"},
{"title": "Explainable and Controllable Motion Curve Guided Cardiac Ultrasound\n  Video Generation", "author": "Junxuan Yu and Rusi Chen and Yongsong Zhou and Yanlin Chen and Yaofei Duan and Yuhao Huang and Han Zhou and Tan Tao and Xin Yang and Dong Ni", "abstract": "  Echocardiography video is a primary modality for diagnosing heart diseases,\nbut the limited data poses challenges for both clinical teaching and machine\nlearning training. Recently, video generative models have emerged as a\npromising strategy to alleviate this issue. However, previous methods often\nrelied on holistic conditions during generation, hindering the flexible\nmovement control over specific cardiac structures. In this context, we propose\nan explainable and controllable method for echocardiography video generation,\ntaking an initial frame and a motion curve as guidance. Our contributions are\nthree-fold. First, we extract motion information from each heart substructure\nto construct motion curves, enabling the diffusion model to synthesize\ncustomized echocardiography videos by modifying these curves. Second, we\npropose the structure-to-motion alignment module, which can map semantic\nfeatures onto motion curves across cardiac structures. Third, The\nposition-aware attention mechanism is designed to enhance video consistency\nutilizing Gaussian masks with structural position information. Extensive\nexperiments on three echocardiography datasets show that our method outperforms\nothers regarding fidelity and consistency. The full code will be released at\nhttps://github.com/mlmi-2024-72/ECM.\n", "link": "http://arxiv.org/abs/2407.21490v1", "date": "2024-07-31", "relevancy": 2.2383, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5855}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5722}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explainable%20and%20Controllable%20Motion%20Curve%20Guided%20Cardiac%20Ultrasound%0A%20%20Video%20Generation&body=Title%3A%20Explainable%20and%20Controllable%20Motion%20Curve%20Guided%20Cardiac%20Ultrasound%0A%20%20Video%20Generation%0AAuthor%3A%20Junxuan%20Yu%20and%20Rusi%20Chen%20and%20Yongsong%20Zhou%20and%20Yanlin%20Chen%20and%20Yaofei%20Duan%20and%20Yuhao%20Huang%20and%20Han%20Zhou%20and%20Tan%20Tao%20and%20Xin%20Yang%20and%20Dong%20Ni%0AAbstract%3A%20%20%20Echocardiography%20video%20is%20a%20primary%20modality%20for%20diagnosing%20heart%20diseases%2C%0Abut%20the%20limited%20data%20poses%20challenges%20for%20both%20clinical%20teaching%20and%20machine%0Alearning%20training.%20Recently%2C%20video%20generative%20models%20have%20emerged%20as%20a%0Apromising%20strategy%20to%20alleviate%20this%20issue.%20However%2C%20previous%20methods%20often%0Arelied%20on%20holistic%20conditions%20during%20generation%2C%20hindering%20the%20flexible%0Amovement%20control%20over%20specific%20cardiac%20structures.%20In%20this%20context%2C%20we%20propose%0Aan%20explainable%20and%20controllable%20method%20for%20echocardiography%20video%20generation%2C%0Ataking%20an%20initial%20frame%20and%20a%20motion%20curve%20as%20guidance.%20Our%20contributions%20are%0Athree-fold.%20First%2C%20we%20extract%20motion%20information%20from%20each%20heart%20substructure%0Ato%20construct%20motion%20curves%2C%20enabling%20the%20diffusion%20model%20to%20synthesize%0Acustomized%20echocardiography%20videos%20by%20modifying%20these%20curves.%20Second%2C%20we%0Apropose%20the%20structure-to-motion%20alignment%20module%2C%20which%20can%20map%20semantic%0Afeatures%20onto%20motion%20curves%20across%20cardiac%20structures.%20Third%2C%20The%0Aposition-aware%20attention%20mechanism%20is%20designed%20to%20enhance%20video%20consistency%0Autilizing%20Gaussian%20masks%20with%20structural%20position%20information.%20Extensive%0Aexperiments%20on%20three%20echocardiography%20datasets%20show%20that%20our%20method%20outperforms%0Aothers%20regarding%20fidelity%20and%20consistency.%20The%20full%20code%20will%20be%20released%20at%0Ahttps%3A//github.com/mlmi-2024-72/ECM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21490v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplainable%2520and%2520Controllable%2520Motion%2520Curve%2520Guided%2520Cardiac%2520Ultrasound%250A%2520%2520Video%2520Generation%26entry.906535625%3DJunxuan%2520Yu%2520and%2520Rusi%2520Chen%2520and%2520Yongsong%2520Zhou%2520and%2520Yanlin%2520Chen%2520and%2520Yaofei%2520Duan%2520and%2520Yuhao%2520Huang%2520and%2520Han%2520Zhou%2520and%2520Tan%2520Tao%2520and%2520Xin%2520Yang%2520and%2520Dong%2520Ni%26entry.1292438233%3D%2520%2520Echocardiography%2520video%2520is%2520a%2520primary%2520modality%2520for%2520diagnosing%2520heart%2520diseases%252C%250Abut%2520the%2520limited%2520data%2520poses%2520challenges%2520for%2520both%2520clinical%2520teaching%2520and%2520machine%250Alearning%2520training.%2520Recently%252C%2520video%2520generative%2520models%2520have%2520emerged%2520as%2520a%250Apromising%2520strategy%2520to%2520alleviate%2520this%2520issue.%2520However%252C%2520previous%2520methods%2520often%250Arelied%2520on%2520holistic%2520conditions%2520during%2520generation%252C%2520hindering%2520the%2520flexible%250Amovement%2520control%2520over%2520specific%2520cardiac%2520structures.%2520In%2520this%2520context%252C%2520we%2520propose%250Aan%2520explainable%2520and%2520controllable%2520method%2520for%2520echocardiography%2520video%2520generation%252C%250Ataking%2520an%2520initial%2520frame%2520and%2520a%2520motion%2520curve%2520as%2520guidance.%2520Our%2520contributions%2520are%250Athree-fold.%2520First%252C%2520we%2520extract%2520motion%2520information%2520from%2520each%2520heart%2520substructure%250Ato%2520construct%2520motion%2520curves%252C%2520enabling%2520the%2520diffusion%2520model%2520to%2520synthesize%250Acustomized%2520echocardiography%2520videos%2520by%2520modifying%2520these%2520curves.%2520Second%252C%2520we%250Apropose%2520the%2520structure-to-motion%2520alignment%2520module%252C%2520which%2520can%2520map%2520semantic%250Afeatures%2520onto%2520motion%2520curves%2520across%2520cardiac%2520structures.%2520Third%252C%2520The%250Aposition-aware%2520attention%2520mechanism%2520is%2520designed%2520to%2520enhance%2520video%2520consistency%250Autilizing%2520Gaussian%2520masks%2520with%2520structural%2520position%2520information.%2520Extensive%250Aexperiments%2520on%2520three%2520echocardiography%2520datasets%2520show%2520that%2520our%2520method%2520outperforms%250Aothers%2520regarding%2520fidelity%2520and%2520consistency.%2520The%2520full%2520code%2520will%2520be%2520released%2520at%250Ahttps%253A//github.com/mlmi-2024-72/ECM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21490v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explainable%20and%20Controllable%20Motion%20Curve%20Guided%20Cardiac%20Ultrasound%0A%20%20Video%20Generation&entry.906535625=Junxuan%20Yu%20and%20Rusi%20Chen%20and%20Yongsong%20Zhou%20and%20Yanlin%20Chen%20and%20Yaofei%20Duan%20and%20Yuhao%20Huang%20and%20Han%20Zhou%20and%20Tan%20Tao%20and%20Xin%20Yang%20and%20Dong%20Ni&entry.1292438233=%20%20Echocardiography%20video%20is%20a%20primary%20modality%20for%20diagnosing%20heart%20diseases%2C%0Abut%20the%20limited%20data%20poses%20challenges%20for%20both%20clinical%20teaching%20and%20machine%0Alearning%20training.%20Recently%2C%20video%20generative%20models%20have%20emerged%20as%20a%0Apromising%20strategy%20to%20alleviate%20this%20issue.%20However%2C%20previous%20methods%20often%0Arelied%20on%20holistic%20conditions%20during%20generation%2C%20hindering%20the%20flexible%0Amovement%20control%20over%20specific%20cardiac%20structures.%20In%20this%20context%2C%20we%20propose%0Aan%20explainable%20and%20controllable%20method%20for%20echocardiography%20video%20generation%2C%0Ataking%20an%20initial%20frame%20and%20a%20motion%20curve%20as%20guidance.%20Our%20contributions%20are%0Athree-fold.%20First%2C%20we%20extract%20motion%20information%20from%20each%20heart%20substructure%0Ato%20construct%20motion%20curves%2C%20enabling%20the%20diffusion%20model%20to%20synthesize%0Acustomized%20echocardiography%20videos%20by%20modifying%20these%20curves.%20Second%2C%20we%0Apropose%20the%20structure-to-motion%20alignment%20module%2C%20which%20can%20map%20semantic%0Afeatures%20onto%20motion%20curves%20across%20cardiac%20structures.%20Third%2C%20The%0Aposition-aware%20attention%20mechanism%20is%20designed%20to%20enhance%20video%20consistency%0Autilizing%20Gaussian%20masks%20with%20structural%20position%20information.%20Extensive%0Aexperiments%20on%20three%20echocardiography%20datasets%20show%20that%20our%20method%20outperforms%0Aothers%20regarding%20fidelity%20and%20consistency.%20The%20full%20code%20will%20be%20released%20at%0Ahttps%3A//github.com/mlmi-2024-72/ECM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21490v1&entry.124074799=Read"},
{"title": "ICAGC 2024: Inspirational and Convincing Audio Generation Challenge 2024", "author": "Ruibo Fu and Rui Liu and Chunyu Qiang and Yingming Gao and Yi Lu and Shuchen Shi and Tao Wang and Ya Li and Zhengqi Wen and Chen Zhang and Hui Bu and Yukun Liu and Xin Qi and Guanjun Li", "abstract": "  The Inspirational and Convincing Audio Generation Challenge 2024 (ICAGC 2024)\nis part of the ISCSLP 2024 Competitions and Challenges track. While current\ntext-to-speech (TTS) technology can generate high-quality audio, its ability to\nconvey complex emotions and controlled detail content remains limited. This\nconstraint leads to a discrepancy between the generated audio and human\nsubjective perception in practical applications like companion robots for\nchildren and marketing bots. The core issue lies in the inconsistency between\nhigh-quality audio generation and the ultimate human subjective experience.\nTherefore, this challenge aims to enhance the persuasiveness and acceptability\nof synthesized audio, focusing on human alignment convincing and inspirational\naudio generation. A total of 19 teams have registered for the challenge, and\nthe results of the competition and the competition are described in this paper.\n", "link": "http://arxiv.org/abs/2407.12038v2", "date": "2024-07-31", "relevancy": 2.2352, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4579}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4472}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ICAGC%202024%3A%20Inspirational%20and%20Convincing%20Audio%20Generation%20Challenge%202024&body=Title%3A%20ICAGC%202024%3A%20Inspirational%20and%20Convincing%20Audio%20Generation%20Challenge%202024%0AAuthor%3A%20Ruibo%20Fu%20and%20Rui%20Liu%20and%20Chunyu%20Qiang%20and%20Yingming%20Gao%20and%20Yi%20Lu%20and%20Shuchen%20Shi%20and%20Tao%20Wang%20and%20Ya%20Li%20and%20Zhengqi%20Wen%20and%20Chen%20Zhang%20and%20Hui%20Bu%20and%20Yukun%20Liu%20and%20Xin%20Qi%20and%20Guanjun%20Li%0AAbstract%3A%20%20%20The%20Inspirational%20and%20Convincing%20Audio%20Generation%20Challenge%202024%20%28ICAGC%202024%29%0Ais%20part%20of%20the%20ISCSLP%202024%20Competitions%20and%20Challenges%20track.%20While%20current%0Atext-to-speech%20%28TTS%29%20technology%20can%20generate%20high-quality%20audio%2C%20its%20ability%20to%0Aconvey%20complex%20emotions%20and%20controlled%20detail%20content%20remains%20limited.%20This%0Aconstraint%20leads%20to%20a%20discrepancy%20between%20the%20generated%20audio%20and%20human%0Asubjective%20perception%20in%20practical%20applications%20like%20companion%20robots%20for%0Achildren%20and%20marketing%20bots.%20The%20core%20issue%20lies%20in%20the%20inconsistency%20between%0Ahigh-quality%20audio%20generation%20and%20the%20ultimate%20human%20subjective%20experience.%0ATherefore%2C%20this%20challenge%20aims%20to%20enhance%20the%20persuasiveness%20and%20acceptability%0Aof%20synthesized%20audio%2C%20focusing%20on%20human%20alignment%20convincing%20and%20inspirational%0Aaudio%20generation.%20A%20total%20of%2019%20teams%20have%20registered%20for%20the%20challenge%2C%20and%0Athe%20results%20of%20the%20competition%20and%20the%20competition%20are%20described%20in%20this%20paper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12038v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DICAGC%25202024%253A%2520Inspirational%2520and%2520Convincing%2520Audio%2520Generation%2520Challenge%25202024%26entry.906535625%3DRuibo%2520Fu%2520and%2520Rui%2520Liu%2520and%2520Chunyu%2520Qiang%2520and%2520Yingming%2520Gao%2520and%2520Yi%2520Lu%2520and%2520Shuchen%2520Shi%2520and%2520Tao%2520Wang%2520and%2520Ya%2520Li%2520and%2520Zhengqi%2520Wen%2520and%2520Chen%2520Zhang%2520and%2520Hui%2520Bu%2520and%2520Yukun%2520Liu%2520and%2520Xin%2520Qi%2520and%2520Guanjun%2520Li%26entry.1292438233%3D%2520%2520The%2520Inspirational%2520and%2520Convincing%2520Audio%2520Generation%2520Challenge%25202024%2520%2528ICAGC%25202024%2529%250Ais%2520part%2520of%2520the%2520ISCSLP%25202024%2520Competitions%2520and%2520Challenges%2520track.%2520While%2520current%250Atext-to-speech%2520%2528TTS%2529%2520technology%2520can%2520generate%2520high-quality%2520audio%252C%2520its%2520ability%2520to%250Aconvey%2520complex%2520emotions%2520and%2520controlled%2520detail%2520content%2520remains%2520limited.%2520This%250Aconstraint%2520leads%2520to%2520a%2520discrepancy%2520between%2520the%2520generated%2520audio%2520and%2520human%250Asubjective%2520perception%2520in%2520practical%2520applications%2520like%2520companion%2520robots%2520for%250Achildren%2520and%2520marketing%2520bots.%2520The%2520core%2520issue%2520lies%2520in%2520the%2520inconsistency%2520between%250Ahigh-quality%2520audio%2520generation%2520and%2520the%2520ultimate%2520human%2520subjective%2520experience.%250ATherefore%252C%2520this%2520challenge%2520aims%2520to%2520enhance%2520the%2520persuasiveness%2520and%2520acceptability%250Aof%2520synthesized%2520audio%252C%2520focusing%2520on%2520human%2520alignment%2520convincing%2520and%2520inspirational%250Aaudio%2520generation.%2520A%2520total%2520of%252019%2520teams%2520have%2520registered%2520for%2520the%2520challenge%252C%2520and%250Athe%2520results%2520of%2520the%2520competition%2520and%2520the%2520competition%2520are%2520described%2520in%2520this%2520paper.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12038v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ICAGC%202024%3A%20Inspirational%20and%20Convincing%20Audio%20Generation%20Challenge%202024&entry.906535625=Ruibo%20Fu%20and%20Rui%20Liu%20and%20Chunyu%20Qiang%20and%20Yingming%20Gao%20and%20Yi%20Lu%20and%20Shuchen%20Shi%20and%20Tao%20Wang%20and%20Ya%20Li%20and%20Zhengqi%20Wen%20and%20Chen%20Zhang%20and%20Hui%20Bu%20and%20Yukun%20Liu%20and%20Xin%20Qi%20and%20Guanjun%20Li&entry.1292438233=%20%20The%20Inspirational%20and%20Convincing%20Audio%20Generation%20Challenge%202024%20%28ICAGC%202024%29%0Ais%20part%20of%20the%20ISCSLP%202024%20Competitions%20and%20Challenges%20track.%20While%20current%0Atext-to-speech%20%28TTS%29%20technology%20can%20generate%20high-quality%20audio%2C%20its%20ability%20to%0Aconvey%20complex%20emotions%20and%20controlled%20detail%20content%20remains%20limited.%20This%0Aconstraint%20leads%20to%20a%20discrepancy%20between%20the%20generated%20audio%20and%20human%0Asubjective%20perception%20in%20practical%20applications%20like%20companion%20robots%20for%0Achildren%20and%20marketing%20bots.%20The%20core%20issue%20lies%20in%20the%20inconsistency%20between%0Ahigh-quality%20audio%20generation%20and%20the%20ultimate%20human%20subjective%20experience.%0ATherefore%2C%20this%20challenge%20aims%20to%20enhance%20the%20persuasiveness%20and%20acceptability%0Aof%20synthesized%20audio%2C%20focusing%20on%20human%20alignment%20convincing%20and%20inspirational%0Aaudio%20generation.%20A%20total%20of%2019%20teams%20have%20registered%20for%20the%20challenge%2C%20and%0Athe%20results%20of%20the%20competition%20and%20the%20competition%20are%20described%20in%20this%20paper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12038v2&entry.124074799=Read"},
{"title": "PP-TIL: Personalized Planning for Autonomous Driving with Instance-based\n  Transfer Imitation Learning", "author": "Fangze Lin and Ying He and Fei Yu", "abstract": "  Personalized motion planning holds significant importance within urban\nautomated driving, catering to the unique requirements of individual users.\nNevertheless, prior endeavors have frequently encountered difficulties in\nsimultaneously addressing two crucial aspects: personalized planning within\nintricate urban settings and enhancing planning performance through data\nutilization. The challenge arises from the expensive and limited nature of user\ndata, coupled with the scene state space tending towards infinity. These\nfactors contribute to overfitting and poor generalization problems during model\ntraining. Henceforth, we propose an instance-based transfer imitation learning\napproach. This method facilitates knowledge transfer from extensive expert\ndomain data to the user domain, presenting a fundamental resolution to these\nissues. We initially train a pre-trained model using large-scale expert data.\nSubsequently, during the fine-tuning phase, we feed the batch data, which\ncomprises expert and user data. Employing the inverse reinforcement learning\ntechnique, we extract the style feature distribution from user demonstrations,\nconstructing the regularization term for the approximation of user style. In\nour experiments, we conducted extensive evaluations of the proposed method.\nCompared to the baseline methods, our approach mitigates the overfitting issue\ncaused by sparse user data. Furthermore, we discovered that integrating the\ndriving model with a differentiable nonlinear optimizer as a safety protection\nlayer for end-to-end personalized fine-tuning results in superior planning\nperformance.\n", "link": "http://arxiv.org/abs/2407.18569v2", "date": "2024-07-31", "relevancy": 2.2212, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5571}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5553}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PP-TIL%3A%20Personalized%20Planning%20for%20Autonomous%20Driving%20with%20Instance-based%0A%20%20Transfer%20Imitation%20Learning&body=Title%3A%20PP-TIL%3A%20Personalized%20Planning%20for%20Autonomous%20Driving%20with%20Instance-based%0A%20%20Transfer%20Imitation%20Learning%0AAuthor%3A%20Fangze%20Lin%20and%20Ying%20He%20and%20Fei%20Yu%0AAbstract%3A%20%20%20Personalized%20motion%20planning%20holds%20significant%20importance%20within%20urban%0Aautomated%20driving%2C%20catering%20to%20the%20unique%20requirements%20of%20individual%20users.%0ANevertheless%2C%20prior%20endeavors%20have%20frequently%20encountered%20difficulties%20in%0Asimultaneously%20addressing%20two%20crucial%20aspects%3A%20personalized%20planning%20within%0Aintricate%20urban%20settings%20and%20enhancing%20planning%20performance%20through%20data%0Autilization.%20The%20challenge%20arises%20from%20the%20expensive%20and%20limited%20nature%20of%20user%0Adata%2C%20coupled%20with%20the%20scene%20state%20space%20tending%20towards%20infinity.%20These%0Afactors%20contribute%20to%20overfitting%20and%20poor%20generalization%20problems%20during%20model%0Atraining.%20Henceforth%2C%20we%20propose%20an%20instance-based%20transfer%20imitation%20learning%0Aapproach.%20This%20method%20facilitates%20knowledge%20transfer%20from%20extensive%20expert%0Adomain%20data%20to%20the%20user%20domain%2C%20presenting%20a%20fundamental%20resolution%20to%20these%0Aissues.%20We%20initially%20train%20a%20pre-trained%20model%20using%20large-scale%20expert%20data.%0ASubsequently%2C%20during%20the%20fine-tuning%20phase%2C%20we%20feed%20the%20batch%20data%2C%20which%0Acomprises%20expert%20and%20user%20data.%20Employing%20the%20inverse%20reinforcement%20learning%0Atechnique%2C%20we%20extract%20the%20style%20feature%20distribution%20from%20user%20demonstrations%2C%0Aconstructing%20the%20regularization%20term%20for%20the%20approximation%20of%20user%20style.%20In%0Aour%20experiments%2C%20we%20conducted%20extensive%20evaluations%20of%20the%20proposed%20method.%0ACompared%20to%20the%20baseline%20methods%2C%20our%20approach%20mitigates%20the%20overfitting%20issue%0Acaused%20by%20sparse%20user%20data.%20Furthermore%2C%20we%20discovered%20that%20integrating%20the%0Adriving%20model%20with%20a%20differentiable%20nonlinear%20optimizer%20as%20a%20safety%20protection%0Alayer%20for%20end-to-end%20personalized%20fine-tuning%20results%20in%20superior%20planning%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18569v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPP-TIL%253A%2520Personalized%2520Planning%2520for%2520Autonomous%2520Driving%2520with%2520Instance-based%250A%2520%2520Transfer%2520Imitation%2520Learning%26entry.906535625%3DFangze%2520Lin%2520and%2520Ying%2520He%2520and%2520Fei%2520Yu%26entry.1292438233%3D%2520%2520Personalized%2520motion%2520planning%2520holds%2520significant%2520importance%2520within%2520urban%250Aautomated%2520driving%252C%2520catering%2520to%2520the%2520unique%2520requirements%2520of%2520individual%2520users.%250ANevertheless%252C%2520prior%2520endeavors%2520have%2520frequently%2520encountered%2520difficulties%2520in%250Asimultaneously%2520addressing%2520two%2520crucial%2520aspects%253A%2520personalized%2520planning%2520within%250Aintricate%2520urban%2520settings%2520and%2520enhancing%2520planning%2520performance%2520through%2520data%250Autilization.%2520The%2520challenge%2520arises%2520from%2520the%2520expensive%2520and%2520limited%2520nature%2520of%2520user%250Adata%252C%2520coupled%2520with%2520the%2520scene%2520state%2520space%2520tending%2520towards%2520infinity.%2520These%250Afactors%2520contribute%2520to%2520overfitting%2520and%2520poor%2520generalization%2520problems%2520during%2520model%250Atraining.%2520Henceforth%252C%2520we%2520propose%2520an%2520instance-based%2520transfer%2520imitation%2520learning%250Aapproach.%2520This%2520method%2520facilitates%2520knowledge%2520transfer%2520from%2520extensive%2520expert%250Adomain%2520data%2520to%2520the%2520user%2520domain%252C%2520presenting%2520a%2520fundamental%2520resolution%2520to%2520these%250Aissues.%2520We%2520initially%2520train%2520a%2520pre-trained%2520model%2520using%2520large-scale%2520expert%2520data.%250ASubsequently%252C%2520during%2520the%2520fine-tuning%2520phase%252C%2520we%2520feed%2520the%2520batch%2520data%252C%2520which%250Acomprises%2520expert%2520and%2520user%2520data.%2520Employing%2520the%2520inverse%2520reinforcement%2520learning%250Atechnique%252C%2520we%2520extract%2520the%2520style%2520feature%2520distribution%2520from%2520user%2520demonstrations%252C%250Aconstructing%2520the%2520regularization%2520term%2520for%2520the%2520approximation%2520of%2520user%2520style.%2520In%250Aour%2520experiments%252C%2520we%2520conducted%2520extensive%2520evaluations%2520of%2520the%2520proposed%2520method.%250ACompared%2520to%2520the%2520baseline%2520methods%252C%2520our%2520approach%2520mitigates%2520the%2520overfitting%2520issue%250Acaused%2520by%2520sparse%2520user%2520data.%2520Furthermore%252C%2520we%2520discovered%2520that%2520integrating%2520the%250Adriving%2520model%2520with%2520a%2520differentiable%2520nonlinear%2520optimizer%2520as%2520a%2520safety%2520protection%250Alayer%2520for%2520end-to-end%2520personalized%2520fine-tuning%2520results%2520in%2520superior%2520planning%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18569v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PP-TIL%3A%20Personalized%20Planning%20for%20Autonomous%20Driving%20with%20Instance-based%0A%20%20Transfer%20Imitation%20Learning&entry.906535625=Fangze%20Lin%20and%20Ying%20He%20and%20Fei%20Yu&entry.1292438233=%20%20Personalized%20motion%20planning%20holds%20significant%20importance%20within%20urban%0Aautomated%20driving%2C%20catering%20to%20the%20unique%20requirements%20of%20individual%20users.%0ANevertheless%2C%20prior%20endeavors%20have%20frequently%20encountered%20difficulties%20in%0Asimultaneously%20addressing%20two%20crucial%20aspects%3A%20personalized%20planning%20within%0Aintricate%20urban%20settings%20and%20enhancing%20planning%20performance%20through%20data%0Autilization.%20The%20challenge%20arises%20from%20the%20expensive%20and%20limited%20nature%20of%20user%0Adata%2C%20coupled%20with%20the%20scene%20state%20space%20tending%20towards%20infinity.%20These%0Afactors%20contribute%20to%20overfitting%20and%20poor%20generalization%20problems%20during%20model%0Atraining.%20Henceforth%2C%20we%20propose%20an%20instance-based%20transfer%20imitation%20learning%0Aapproach.%20This%20method%20facilitates%20knowledge%20transfer%20from%20extensive%20expert%0Adomain%20data%20to%20the%20user%20domain%2C%20presenting%20a%20fundamental%20resolution%20to%20these%0Aissues.%20We%20initially%20train%20a%20pre-trained%20model%20using%20large-scale%20expert%20data.%0ASubsequently%2C%20during%20the%20fine-tuning%20phase%2C%20we%20feed%20the%20batch%20data%2C%20which%0Acomprises%20expert%20and%20user%20data.%20Employing%20the%20inverse%20reinforcement%20learning%0Atechnique%2C%20we%20extract%20the%20style%20feature%20distribution%20from%20user%20demonstrations%2C%0Aconstructing%20the%20regularization%20term%20for%20the%20approximation%20of%20user%20style.%20In%0Aour%20experiments%2C%20we%20conducted%20extensive%20evaluations%20of%20the%20proposed%20method.%0ACompared%20to%20the%20baseline%20methods%2C%20our%20approach%20mitigates%20the%20overfitting%20issue%0Acaused%20by%20sparse%20user%20data.%20Furthermore%2C%20we%20discovered%20that%20integrating%20the%0Adriving%20model%20with%20a%20differentiable%20nonlinear%20optimizer%20as%20a%20safety%20protection%0Alayer%20for%20end-to-end%20personalized%20fine-tuning%20results%20in%20superior%20planning%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18569v2&entry.124074799=Read"},
{"title": "ShieldGemma: Generative AI Content Moderation Based on Gemma", "author": "Wenjun Zeng and Yuchi Liu and Ryan Mullins and Ludovic Peran and Joe Fernandez and Hamza Harkous and Karthik Narasimhan and Drew Proud and Piyush Kumar and Bhaktipriya Radharapu and Olivia Sturman and Oscar Wahltinez", "abstract": "  We present ShieldGemma, a comprehensive suite of LLM-based safety content\nmoderation models built upon Gemma2. These models provide robust,\nstate-of-the-art predictions of safety risks across key harm types (sexually\nexplicit, dangerous content, harassment, hate speech) in both user input and\nLLM-generated output. By evaluating on both public and internal benchmarks, we\ndemonstrate superior performance compared to existing models, such as Llama\nGuard (+10.8\\% AU-PRC on public benchmarks) and WildCard (+4.3\\%).\nAdditionally, we present a novel LLM-based data curation pipeline, adaptable to\na variety of safety-related tasks and beyond. We have shown strong\ngeneralization performance for model trained mainly on synthetic data. By\nreleasing ShieldGemma, we provide a valuable resource to the research\ncommunity, advancing LLM safety and enabling the creation of more effective\ncontent moderation solutions for developers.\n", "link": "http://arxiv.org/abs/2407.21772v1", "date": "2024-07-31", "relevancy": 2.2143, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4568}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4388}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ShieldGemma%3A%20Generative%20AI%20Content%20Moderation%20Based%20on%20Gemma&body=Title%3A%20ShieldGemma%3A%20Generative%20AI%20Content%20Moderation%20Based%20on%20Gemma%0AAuthor%3A%20Wenjun%20Zeng%20and%20Yuchi%20Liu%20and%20Ryan%20Mullins%20and%20Ludovic%20Peran%20and%20Joe%20Fernandez%20and%20Hamza%20Harkous%20and%20Karthik%20Narasimhan%20and%20Drew%20Proud%20and%20Piyush%20Kumar%20and%20Bhaktipriya%20Radharapu%20and%20Olivia%20Sturman%20and%20Oscar%20Wahltinez%0AAbstract%3A%20%20%20We%20present%20ShieldGemma%2C%20a%20comprehensive%20suite%20of%20LLM-based%20safety%20content%0Amoderation%20models%20built%20upon%20Gemma2.%20These%20models%20provide%20robust%2C%0Astate-of-the-art%20predictions%20of%20safety%20risks%20across%20key%20harm%20types%20%28sexually%0Aexplicit%2C%20dangerous%20content%2C%20harassment%2C%20hate%20speech%29%20in%20both%20user%20input%20and%0ALLM-generated%20output.%20By%20evaluating%20on%20both%20public%20and%20internal%20benchmarks%2C%20we%0Ademonstrate%20superior%20performance%20compared%20to%20existing%20models%2C%20such%20as%20Llama%0AGuard%20%28%2B10.8%5C%25%20AU-PRC%20on%20public%20benchmarks%29%20and%20WildCard%20%28%2B4.3%5C%25%29.%0AAdditionally%2C%20we%20present%20a%20novel%20LLM-based%20data%20curation%20pipeline%2C%20adaptable%20to%0Aa%20variety%20of%20safety-related%20tasks%20and%20beyond.%20We%20have%20shown%20strong%0Ageneralization%20performance%20for%20model%20trained%20mainly%20on%20synthetic%20data.%20By%0Areleasing%20ShieldGemma%2C%20we%20provide%20a%20valuable%20resource%20to%20the%20research%0Acommunity%2C%20advancing%20LLM%20safety%20and%20enabling%20the%20creation%20of%20more%20effective%0Acontent%20moderation%20solutions%20for%20developers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21772v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShieldGemma%253A%2520Generative%2520AI%2520Content%2520Moderation%2520Based%2520on%2520Gemma%26entry.906535625%3DWenjun%2520Zeng%2520and%2520Yuchi%2520Liu%2520and%2520Ryan%2520Mullins%2520and%2520Ludovic%2520Peran%2520and%2520Joe%2520Fernandez%2520and%2520Hamza%2520Harkous%2520and%2520Karthik%2520Narasimhan%2520and%2520Drew%2520Proud%2520and%2520Piyush%2520Kumar%2520and%2520Bhaktipriya%2520Radharapu%2520and%2520Olivia%2520Sturman%2520and%2520Oscar%2520Wahltinez%26entry.1292438233%3D%2520%2520We%2520present%2520ShieldGemma%252C%2520a%2520comprehensive%2520suite%2520of%2520LLM-based%2520safety%2520content%250Amoderation%2520models%2520built%2520upon%2520Gemma2.%2520These%2520models%2520provide%2520robust%252C%250Astate-of-the-art%2520predictions%2520of%2520safety%2520risks%2520across%2520key%2520harm%2520types%2520%2528sexually%250Aexplicit%252C%2520dangerous%2520content%252C%2520harassment%252C%2520hate%2520speech%2529%2520in%2520both%2520user%2520input%2520and%250ALLM-generated%2520output.%2520By%2520evaluating%2520on%2520both%2520public%2520and%2520internal%2520benchmarks%252C%2520we%250Ademonstrate%2520superior%2520performance%2520compared%2520to%2520existing%2520models%252C%2520such%2520as%2520Llama%250AGuard%2520%2528%252B10.8%255C%2525%2520AU-PRC%2520on%2520public%2520benchmarks%2529%2520and%2520WildCard%2520%2528%252B4.3%255C%2525%2529.%250AAdditionally%252C%2520we%2520present%2520a%2520novel%2520LLM-based%2520data%2520curation%2520pipeline%252C%2520adaptable%2520to%250Aa%2520variety%2520of%2520safety-related%2520tasks%2520and%2520beyond.%2520We%2520have%2520shown%2520strong%250Ageneralization%2520performance%2520for%2520model%2520trained%2520mainly%2520on%2520synthetic%2520data.%2520By%250Areleasing%2520ShieldGemma%252C%2520we%2520provide%2520a%2520valuable%2520resource%2520to%2520the%2520research%250Acommunity%252C%2520advancing%2520LLM%2520safety%2520and%2520enabling%2520the%2520creation%2520of%2520more%2520effective%250Acontent%2520moderation%2520solutions%2520for%2520developers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21772v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ShieldGemma%3A%20Generative%20AI%20Content%20Moderation%20Based%20on%20Gemma&entry.906535625=Wenjun%20Zeng%20and%20Yuchi%20Liu%20and%20Ryan%20Mullins%20and%20Ludovic%20Peran%20and%20Joe%20Fernandez%20and%20Hamza%20Harkous%20and%20Karthik%20Narasimhan%20and%20Drew%20Proud%20and%20Piyush%20Kumar%20and%20Bhaktipriya%20Radharapu%20and%20Olivia%20Sturman%20and%20Oscar%20Wahltinez&entry.1292438233=%20%20We%20present%20ShieldGemma%2C%20a%20comprehensive%20suite%20of%20LLM-based%20safety%20content%0Amoderation%20models%20built%20upon%20Gemma2.%20These%20models%20provide%20robust%2C%0Astate-of-the-art%20predictions%20of%20safety%20risks%20across%20key%20harm%20types%20%28sexually%0Aexplicit%2C%20dangerous%20content%2C%20harassment%2C%20hate%20speech%29%20in%20both%20user%20input%20and%0ALLM-generated%20output.%20By%20evaluating%20on%20both%20public%20and%20internal%20benchmarks%2C%20we%0Ademonstrate%20superior%20performance%20compared%20to%20existing%20models%2C%20such%20as%20Llama%0AGuard%20%28%2B10.8%5C%25%20AU-PRC%20on%20public%20benchmarks%29%20and%20WildCard%20%28%2B4.3%5C%25%29.%0AAdditionally%2C%20we%20present%20a%20novel%20LLM-based%20data%20curation%20pipeline%2C%20adaptable%20to%0Aa%20variety%20of%20safety-related%20tasks%20and%20beyond.%20We%20have%20shown%20strong%0Ageneralization%20performance%20for%20model%20trained%20mainly%20on%20synthetic%20data.%20By%0Areleasing%20ShieldGemma%2C%20we%20provide%20a%20valuable%20resource%20to%20the%20research%0Acommunity%2C%20advancing%20LLM%20safety%20and%20enabling%20the%20creation%20of%20more%20effective%0Acontent%20moderation%20solutions%20for%20developers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21772v1&entry.124074799=Read"},
{"title": "Learning Video Context as Interleaved Multimodal Sequences", "author": "Kevin Qinghong Lin and Pengchuan Zhang and Difei Gao and Xide Xia and Joya Chen and Ziteng Gao and Jinheng Xie and Xuhong Xiao and Mike Zheng Shou", "abstract": "  Narrative videos, such as movies, pose significant challenges in video\nunderstanding due to their rich contexts (characters, dialogues, storylines)\nand diverse demands (identify who, relationship, and reason). In this paper, we\nintroduce MovieSeq, a multimodal language model developed to address the wide\nrange of challenges in understanding video contexts. Our core idea is to\nrepresent videos as interleaved multimodal sequences (including images, plots,\nvideos, and subtitles), either by linking external knowledge databases or using\noffline models (such as whisper for subtitles). Through instruction-tuning,\nthis approach empowers the language model to interact with videos using\ninterleaved multimodal instructions. For example, instead of solely relying on\nvideo as input, we jointly provide character photos alongside their names and\ndialogues, allowing the model to associate these elements and generate more\ncomprehensive responses. To demonstrate its effectiveness, we validate\nMovieSeq's performance on six datasets (LVU, MAD, Movienet, CMD, TVC, MovieQA)\nacross five settings (video classification, audio description, video-text\nretrieval, video captioning, and video question-answering). The code will be\npublic at https://github.com/showlab/MovieSeq.\n", "link": "http://arxiv.org/abs/2407.21757v1", "date": "2024-07-31", "relevancy": 2.2082, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5608}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5543}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Video%20Context%20as%20Interleaved%20Multimodal%20Sequences&body=Title%3A%20Learning%20Video%20Context%20as%20Interleaved%20Multimodal%20Sequences%0AAuthor%3A%20Kevin%20Qinghong%20Lin%20and%20Pengchuan%20Zhang%20and%20Difei%20Gao%20and%20Xide%20Xia%20and%20Joya%20Chen%20and%20Ziteng%20Gao%20and%20Jinheng%20Xie%20and%20Xuhong%20Xiao%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20%20%20Narrative%20videos%2C%20such%20as%20movies%2C%20pose%20significant%20challenges%20in%20video%0Aunderstanding%20due%20to%20their%20rich%20contexts%20%28characters%2C%20dialogues%2C%20storylines%29%0Aand%20diverse%20demands%20%28identify%20who%2C%20relationship%2C%20and%20reason%29.%20In%20this%20paper%2C%20we%0Aintroduce%20MovieSeq%2C%20a%20multimodal%20language%20model%20developed%20to%20address%20the%20wide%0Arange%20of%20challenges%20in%20understanding%20video%20contexts.%20Our%20core%20idea%20is%20to%0Arepresent%20videos%20as%20interleaved%20multimodal%20sequences%20%28including%20images%2C%20plots%2C%0Avideos%2C%20and%20subtitles%29%2C%20either%20by%20linking%20external%20knowledge%20databases%20or%20using%0Aoffline%20models%20%28such%20as%20whisper%20for%20subtitles%29.%20Through%20instruction-tuning%2C%0Athis%20approach%20empowers%20the%20language%20model%20to%20interact%20with%20videos%20using%0Ainterleaved%20multimodal%20instructions.%20For%20example%2C%20instead%20of%20solely%20relying%20on%0Avideo%20as%20input%2C%20we%20jointly%20provide%20character%20photos%20alongside%20their%20names%20and%0Adialogues%2C%20allowing%20the%20model%20to%20associate%20these%20elements%20and%20generate%20more%0Acomprehensive%20responses.%20To%20demonstrate%20its%20effectiveness%2C%20we%20validate%0AMovieSeq%27s%20performance%20on%20six%20datasets%20%28LVU%2C%20MAD%2C%20Movienet%2C%20CMD%2C%20TVC%2C%20MovieQA%29%0Aacross%20five%20settings%20%28video%20classification%2C%20audio%20description%2C%20video-text%0Aretrieval%2C%20video%20captioning%2C%20and%20video%20question-answering%29.%20The%20code%20will%20be%0Apublic%20at%20https%3A//github.com/showlab/MovieSeq.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21757v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Video%2520Context%2520as%2520Interleaved%2520Multimodal%2520Sequences%26entry.906535625%3DKevin%2520Qinghong%2520Lin%2520and%2520Pengchuan%2520Zhang%2520and%2520Difei%2520Gao%2520and%2520Xide%2520Xia%2520and%2520Joya%2520Chen%2520and%2520Ziteng%2520Gao%2520and%2520Jinheng%2520Xie%2520and%2520Xuhong%2520Xiao%2520and%2520Mike%2520Zheng%2520Shou%26entry.1292438233%3D%2520%2520Narrative%2520videos%252C%2520such%2520as%2520movies%252C%2520pose%2520significant%2520challenges%2520in%2520video%250Aunderstanding%2520due%2520to%2520their%2520rich%2520contexts%2520%2528characters%252C%2520dialogues%252C%2520storylines%2529%250Aand%2520diverse%2520demands%2520%2528identify%2520who%252C%2520relationship%252C%2520and%2520reason%2529.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520MovieSeq%252C%2520a%2520multimodal%2520language%2520model%2520developed%2520to%2520address%2520the%2520wide%250Arange%2520of%2520challenges%2520in%2520understanding%2520video%2520contexts.%2520Our%2520core%2520idea%2520is%2520to%250Arepresent%2520videos%2520as%2520interleaved%2520multimodal%2520sequences%2520%2528including%2520images%252C%2520plots%252C%250Avideos%252C%2520and%2520subtitles%2529%252C%2520either%2520by%2520linking%2520external%2520knowledge%2520databases%2520or%2520using%250Aoffline%2520models%2520%2528such%2520as%2520whisper%2520for%2520subtitles%2529.%2520Through%2520instruction-tuning%252C%250Athis%2520approach%2520empowers%2520the%2520language%2520model%2520to%2520interact%2520with%2520videos%2520using%250Ainterleaved%2520multimodal%2520instructions.%2520For%2520example%252C%2520instead%2520of%2520solely%2520relying%2520on%250Avideo%2520as%2520input%252C%2520we%2520jointly%2520provide%2520character%2520photos%2520alongside%2520their%2520names%2520and%250Adialogues%252C%2520allowing%2520the%2520model%2520to%2520associate%2520these%2520elements%2520and%2520generate%2520more%250Acomprehensive%2520responses.%2520To%2520demonstrate%2520its%2520effectiveness%252C%2520we%2520validate%250AMovieSeq%2527s%2520performance%2520on%2520six%2520datasets%2520%2528LVU%252C%2520MAD%252C%2520Movienet%252C%2520CMD%252C%2520TVC%252C%2520MovieQA%2529%250Aacross%2520five%2520settings%2520%2528video%2520classification%252C%2520audio%2520description%252C%2520video-text%250Aretrieval%252C%2520video%2520captioning%252C%2520and%2520video%2520question-answering%2529.%2520The%2520code%2520will%2520be%250Apublic%2520at%2520https%253A//github.com/showlab/MovieSeq.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21757v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Video%20Context%20as%20Interleaved%20Multimodal%20Sequences&entry.906535625=Kevin%20Qinghong%20Lin%20and%20Pengchuan%20Zhang%20and%20Difei%20Gao%20and%20Xide%20Xia%20and%20Joya%20Chen%20and%20Ziteng%20Gao%20and%20Jinheng%20Xie%20and%20Xuhong%20Xiao%20and%20Mike%20Zheng%20Shou&entry.1292438233=%20%20Narrative%20videos%2C%20such%20as%20movies%2C%20pose%20significant%20challenges%20in%20video%0Aunderstanding%20due%20to%20their%20rich%20contexts%20%28characters%2C%20dialogues%2C%20storylines%29%0Aand%20diverse%20demands%20%28identify%20who%2C%20relationship%2C%20and%20reason%29.%20In%20this%20paper%2C%20we%0Aintroduce%20MovieSeq%2C%20a%20multimodal%20language%20model%20developed%20to%20address%20the%20wide%0Arange%20of%20challenges%20in%20understanding%20video%20contexts.%20Our%20core%20idea%20is%20to%0Arepresent%20videos%20as%20interleaved%20multimodal%20sequences%20%28including%20images%2C%20plots%2C%0Avideos%2C%20and%20subtitles%29%2C%20either%20by%20linking%20external%20knowledge%20databases%20or%20using%0Aoffline%20models%20%28such%20as%20whisper%20for%20subtitles%29.%20Through%20instruction-tuning%2C%0Athis%20approach%20empowers%20the%20language%20model%20to%20interact%20with%20videos%20using%0Ainterleaved%20multimodal%20instructions.%20For%20example%2C%20instead%20of%20solely%20relying%20on%0Avideo%20as%20input%2C%20we%20jointly%20provide%20character%20photos%20alongside%20their%20names%20and%0Adialogues%2C%20allowing%20the%20model%20to%20associate%20these%20elements%20and%20generate%20more%0Acomprehensive%20responses.%20To%20demonstrate%20its%20effectiveness%2C%20we%20validate%0AMovieSeq%27s%20performance%20on%20six%20datasets%20%28LVU%2C%20MAD%2C%20Movienet%2C%20CMD%2C%20TVC%2C%20MovieQA%29%0Aacross%20five%20settings%20%28video%20classification%2C%20audio%20description%2C%20video-text%0Aretrieval%2C%20video%20captioning%2C%20and%20video%20question-answering%29.%20The%20code%20will%20be%0Apublic%20at%20https%3A//github.com/showlab/MovieSeq.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21757v1&entry.124074799=Read"},
{"title": "Human-Machine Co-Adaptation for Robot-Assisted Rehabilitation via\n  Dual-Agent Multiple Model Reinforcement Learning (DAMMRL)", "author": "Yang An and Yaqi Li and Hongwei Wang and Rob Duffield and Steven W. Su", "abstract": "  This study introduces a novel approach to robot-assisted ankle rehabilitation\nby proposing a Dual-Agent Multiple Model Reinforcement Learning (DAMMRL)\nframework, leveraging multiple model adaptive control (MMAC) and co-adaptive\ncontrol strategies. In robot-assisted rehabilitation, one of the key challenges\nis modelling human behaviour due to the complexity of human cognition and\nphysiological systems. Traditional single-model approaches often fail to\ncapture the dynamics of human-machine interactions. Our research employs a\nmultiple model strategy, using simple sub-models to approximate complex human\nresponses during rehabilitation tasks, tailored to varying levels of patient\nincapacity. The proposed system's versatility is demonstrated in real\nexperiments and simulated environments. Feasibility and potential were\nevaluated with 13 healthy young subjects, yielding promising results that\naffirm the anticipated benefits of the approach. This study not only introduces\na new paradigm for robot-assisted ankle rehabilitation but also opens the way\nfor future research in adaptive, patient-centred therapeutic interventions.\n", "link": "http://arxiv.org/abs/2407.21734v1", "date": "2024-07-31", "relevancy": 2.182, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5924}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5393}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-Machine%20Co-Adaptation%20for%20Robot-Assisted%20Rehabilitation%20via%0A%20%20Dual-Agent%20Multiple%20Model%20Reinforcement%20Learning%20%28DAMMRL%29&body=Title%3A%20Human-Machine%20Co-Adaptation%20for%20Robot-Assisted%20Rehabilitation%20via%0A%20%20Dual-Agent%20Multiple%20Model%20Reinforcement%20Learning%20%28DAMMRL%29%0AAuthor%3A%20Yang%20An%20and%20Yaqi%20Li%20and%20Hongwei%20Wang%20and%20Rob%20Duffield%20and%20Steven%20W.%20Su%0AAbstract%3A%20%20%20This%20study%20introduces%20a%20novel%20approach%20to%20robot-assisted%20ankle%20rehabilitation%0Aby%20proposing%20a%20Dual-Agent%20Multiple%20Model%20Reinforcement%20Learning%20%28DAMMRL%29%0Aframework%2C%20leveraging%20multiple%20model%20adaptive%20control%20%28MMAC%29%20and%20co-adaptive%0Acontrol%20strategies.%20In%20robot-assisted%20rehabilitation%2C%20one%20of%20the%20key%20challenges%0Ais%20modelling%20human%20behaviour%20due%20to%20the%20complexity%20of%20human%20cognition%20and%0Aphysiological%20systems.%20Traditional%20single-model%20approaches%20often%20fail%20to%0Acapture%20the%20dynamics%20of%20human-machine%20interactions.%20Our%20research%20employs%20a%0Amultiple%20model%20strategy%2C%20using%20simple%20sub-models%20to%20approximate%20complex%20human%0Aresponses%20during%20rehabilitation%20tasks%2C%20tailored%20to%20varying%20levels%20of%20patient%0Aincapacity.%20The%20proposed%20system%27s%20versatility%20is%20demonstrated%20in%20real%0Aexperiments%20and%20simulated%20environments.%20Feasibility%20and%20potential%20were%0Aevaluated%20with%2013%20healthy%20young%20subjects%2C%20yielding%20promising%20results%20that%0Aaffirm%20the%20anticipated%20benefits%20of%20the%20approach.%20This%20study%20not%20only%20introduces%0Aa%20new%20paradigm%20for%20robot-assisted%20ankle%20rehabilitation%20but%20also%20opens%20the%20way%0Afor%20future%20research%20in%20adaptive%2C%20patient-centred%20therapeutic%20interventions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21734v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-Machine%2520Co-Adaptation%2520for%2520Robot-Assisted%2520Rehabilitation%2520via%250A%2520%2520Dual-Agent%2520Multiple%2520Model%2520Reinforcement%2520Learning%2520%2528DAMMRL%2529%26entry.906535625%3DYang%2520An%2520and%2520Yaqi%2520Li%2520and%2520Hongwei%2520Wang%2520and%2520Rob%2520Duffield%2520and%2520Steven%2520W.%2520Su%26entry.1292438233%3D%2520%2520This%2520study%2520introduces%2520a%2520novel%2520approach%2520to%2520robot-assisted%2520ankle%2520rehabilitation%250Aby%2520proposing%2520a%2520Dual-Agent%2520Multiple%2520Model%2520Reinforcement%2520Learning%2520%2528DAMMRL%2529%250Aframework%252C%2520leveraging%2520multiple%2520model%2520adaptive%2520control%2520%2528MMAC%2529%2520and%2520co-adaptive%250Acontrol%2520strategies.%2520In%2520robot-assisted%2520rehabilitation%252C%2520one%2520of%2520the%2520key%2520challenges%250Ais%2520modelling%2520human%2520behaviour%2520due%2520to%2520the%2520complexity%2520of%2520human%2520cognition%2520and%250Aphysiological%2520systems.%2520Traditional%2520single-model%2520approaches%2520often%2520fail%2520to%250Acapture%2520the%2520dynamics%2520of%2520human-machine%2520interactions.%2520Our%2520research%2520employs%2520a%250Amultiple%2520model%2520strategy%252C%2520using%2520simple%2520sub-models%2520to%2520approximate%2520complex%2520human%250Aresponses%2520during%2520rehabilitation%2520tasks%252C%2520tailored%2520to%2520varying%2520levels%2520of%2520patient%250Aincapacity.%2520The%2520proposed%2520system%2527s%2520versatility%2520is%2520demonstrated%2520in%2520real%250Aexperiments%2520and%2520simulated%2520environments.%2520Feasibility%2520and%2520potential%2520were%250Aevaluated%2520with%252013%2520healthy%2520young%2520subjects%252C%2520yielding%2520promising%2520results%2520that%250Aaffirm%2520the%2520anticipated%2520benefits%2520of%2520the%2520approach.%2520This%2520study%2520not%2520only%2520introduces%250Aa%2520new%2520paradigm%2520for%2520robot-assisted%2520ankle%2520rehabilitation%2520but%2520also%2520opens%2520the%2520way%250Afor%2520future%2520research%2520in%2520adaptive%252C%2520patient-centred%2520therapeutic%2520interventions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21734v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-Machine%20Co-Adaptation%20for%20Robot-Assisted%20Rehabilitation%20via%0A%20%20Dual-Agent%20Multiple%20Model%20Reinforcement%20Learning%20%28DAMMRL%29&entry.906535625=Yang%20An%20and%20Yaqi%20Li%20and%20Hongwei%20Wang%20and%20Rob%20Duffield%20and%20Steven%20W.%20Su&entry.1292438233=%20%20This%20study%20introduces%20a%20novel%20approach%20to%20robot-assisted%20ankle%20rehabilitation%0Aby%20proposing%20a%20Dual-Agent%20Multiple%20Model%20Reinforcement%20Learning%20%28DAMMRL%29%0Aframework%2C%20leveraging%20multiple%20model%20adaptive%20control%20%28MMAC%29%20and%20co-adaptive%0Acontrol%20strategies.%20In%20robot-assisted%20rehabilitation%2C%20one%20of%20the%20key%20challenges%0Ais%20modelling%20human%20behaviour%20due%20to%20the%20complexity%20of%20human%20cognition%20and%0Aphysiological%20systems.%20Traditional%20single-model%20approaches%20often%20fail%20to%0Acapture%20the%20dynamics%20of%20human-machine%20interactions.%20Our%20research%20employs%20a%0Amultiple%20model%20strategy%2C%20using%20simple%20sub-models%20to%20approximate%20complex%20human%0Aresponses%20during%20rehabilitation%20tasks%2C%20tailored%20to%20varying%20levels%20of%20patient%0Aincapacity.%20The%20proposed%20system%27s%20versatility%20is%20demonstrated%20in%20real%0Aexperiments%20and%20simulated%20environments.%20Feasibility%20and%20potential%20were%0Aevaluated%20with%2013%20healthy%20young%20subjects%2C%20yielding%20promising%20results%20that%0Aaffirm%20the%20anticipated%20benefits%20of%20the%20approach.%20This%20study%20not%20only%20introduces%0Aa%20new%20paradigm%20for%20robot-assisted%20ankle%20rehabilitation%20but%20also%20opens%20the%20way%0Afor%20future%20research%20in%20adaptive%2C%20patient-centred%20therapeutic%20interventions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21734v1&entry.124074799=Read"},
{"title": "The Impacts of AI Avatar Appearance and Disclosure on User Motivation", "author": "Boele Visser and Peter van der Putten and Amirhossein Zohrehvand", "abstract": "  This study examines the influence of perceived AI features on user motivation\nin virtual interactions. AI avatars, being disclosed as being an AI, or\nembodying specific genders, could be used in user-AI interactions. Leveraging\ninsights from AI and avatar research, we explore how AI disclosure and gender\naffect user motivation. We conducted a game-based experiment involving over\n72,500 participants who solved search problems alone or with an AI companion.\nDifferent groups experienced varying AI appearances and disclosures. We\nmeasured play intensity. Results revealed that the presence of another avatar\nled to less intense play compared to solo play. Disclosure of the avatar as AI\nheightened effort intensity compared to non-disclosed AI companions.\nAdditionally, a masculine AI appearance reduced effort intensity.\n", "link": "http://arxiv.org/abs/2407.21521v1", "date": "2024-07-31", "relevancy": 2.177, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4454}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.441}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4197}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Impacts%20of%20AI%20Avatar%20Appearance%20and%20Disclosure%20on%20User%20Motivation&body=Title%3A%20The%20Impacts%20of%20AI%20Avatar%20Appearance%20and%20Disclosure%20on%20User%20Motivation%0AAuthor%3A%20Boele%20Visser%20and%20Peter%20van%20der%20Putten%20and%20Amirhossein%20Zohrehvand%0AAbstract%3A%20%20%20This%20study%20examines%20the%20influence%20of%20perceived%20AI%20features%20on%20user%20motivation%0Ain%20virtual%20interactions.%20AI%20avatars%2C%20being%20disclosed%20as%20being%20an%20AI%2C%20or%0Aembodying%20specific%20genders%2C%20could%20be%20used%20in%20user-AI%20interactions.%20Leveraging%0Ainsights%20from%20AI%20and%20avatar%20research%2C%20we%20explore%20how%20AI%20disclosure%20and%20gender%0Aaffect%20user%20motivation.%20We%20conducted%20a%20game-based%20experiment%20involving%20over%0A72%2C500%20participants%20who%20solved%20search%20problems%20alone%20or%20with%20an%20AI%20companion.%0ADifferent%20groups%20experienced%20varying%20AI%20appearances%20and%20disclosures.%20We%0Ameasured%20play%20intensity.%20Results%20revealed%20that%20the%20presence%20of%20another%20avatar%0Aled%20to%20less%20intense%20play%20compared%20to%20solo%20play.%20Disclosure%20of%20the%20avatar%20as%20AI%0Aheightened%20effort%20intensity%20compared%20to%20non-disclosed%20AI%20companions.%0AAdditionally%2C%20a%20masculine%20AI%20appearance%20reduced%20effort%20intensity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21521v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Impacts%2520of%2520AI%2520Avatar%2520Appearance%2520and%2520Disclosure%2520on%2520User%2520Motivation%26entry.906535625%3DBoele%2520Visser%2520and%2520Peter%2520van%2520der%2520Putten%2520and%2520Amirhossein%2520Zohrehvand%26entry.1292438233%3D%2520%2520This%2520study%2520examines%2520the%2520influence%2520of%2520perceived%2520AI%2520features%2520on%2520user%2520motivation%250Ain%2520virtual%2520interactions.%2520AI%2520avatars%252C%2520being%2520disclosed%2520as%2520being%2520an%2520AI%252C%2520or%250Aembodying%2520specific%2520genders%252C%2520could%2520be%2520used%2520in%2520user-AI%2520interactions.%2520Leveraging%250Ainsights%2520from%2520AI%2520and%2520avatar%2520research%252C%2520we%2520explore%2520how%2520AI%2520disclosure%2520and%2520gender%250Aaffect%2520user%2520motivation.%2520We%2520conducted%2520a%2520game-based%2520experiment%2520involving%2520over%250A72%252C500%2520participants%2520who%2520solved%2520search%2520problems%2520alone%2520or%2520with%2520an%2520AI%2520companion.%250ADifferent%2520groups%2520experienced%2520varying%2520AI%2520appearances%2520and%2520disclosures.%2520We%250Ameasured%2520play%2520intensity.%2520Results%2520revealed%2520that%2520the%2520presence%2520of%2520another%2520avatar%250Aled%2520to%2520less%2520intense%2520play%2520compared%2520to%2520solo%2520play.%2520Disclosure%2520of%2520the%2520avatar%2520as%2520AI%250Aheightened%2520effort%2520intensity%2520compared%2520to%2520non-disclosed%2520AI%2520companions.%250AAdditionally%252C%2520a%2520masculine%2520AI%2520appearance%2520reduced%2520effort%2520intensity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21521v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Impacts%20of%20AI%20Avatar%20Appearance%20and%20Disclosure%20on%20User%20Motivation&entry.906535625=Boele%20Visser%20and%20Peter%20van%20der%20Putten%20and%20Amirhossein%20Zohrehvand&entry.1292438233=%20%20This%20study%20examines%20the%20influence%20of%20perceived%20AI%20features%20on%20user%20motivation%0Ain%20virtual%20interactions.%20AI%20avatars%2C%20being%20disclosed%20as%20being%20an%20AI%2C%20or%0Aembodying%20specific%20genders%2C%20could%20be%20used%20in%20user-AI%20interactions.%20Leveraging%0Ainsights%20from%20AI%20and%20avatar%20research%2C%20we%20explore%20how%20AI%20disclosure%20and%20gender%0Aaffect%20user%20motivation.%20We%20conducted%20a%20game-based%20experiment%20involving%20over%0A72%2C500%20participants%20who%20solved%20search%20problems%20alone%20or%20with%20an%20AI%20companion.%0ADifferent%20groups%20experienced%20varying%20AI%20appearances%20and%20disclosures.%20We%0Ameasured%20play%20intensity.%20Results%20revealed%20that%20the%20presence%20of%20another%20avatar%0Aled%20to%20less%20intense%20play%20compared%20to%20solo%20play.%20Disclosure%20of%20the%20avatar%20as%20AI%0Aheightened%20effort%20intensity%20compared%20to%20non-disclosed%20AI%20companions.%0AAdditionally%2C%20a%20masculine%20AI%20appearance%20reduced%20effort%20intensity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21521v1&entry.124074799=Read"},
{"title": "A Simple Low-bit Quantization Framework for Video Snapshot Compressive\n  Imaging", "author": "Miao Cao and Lishun Wang and Huan Wang and Xin Yuan", "abstract": "  Video Snapshot Compressive Imaging (SCI) aims to use a low-speed 2D camera to\ncapture high-speed scene as snapshot compressed measurements, followed by a\nreconstruction algorithm to reconstruct the high-speed video frames.\nState-of-the-art (SOTA) deep learning-based algorithms have achieved impressive\nperformance, yet with heavy computational workload. Network quantization is a\npromising way to reduce computational cost. However, a direct low-bit\nquantization will bring large performance drop. To address this challenge, in\nthis paper, we propose a simple low-bit quantization framework (dubbed Q-SCI)\nfor the end-to-end deep learning-based video SCI reconstruction methods which\nusually consist of a feature extraction, feature enhancement, and video\nreconstruction module. Specifically, we first design a high-quality feature\nextraction module and a precise video reconstruction module to extract and\npropagate high-quality features in the low-bit quantized model. In addition, to\nalleviate the information distortion of the Transformer branch in the quantized\nfeature enhancement module, we introduce a shift operation on the query and key\ndistributions to further bridge the performance gap. Comprehensive experimental\nresults manifest that our Q-SCI framework can achieve superior performance,\ne.g., 4-bit quantized EfficientSCI-S derived by our Q-SCI framework can\ntheoretically accelerate the real-valued EfficientSCI-S by 7.8X with only 2.3%\nperformance gap on the simulation testing datasets. Code is available at\nhttps://github.com/mcao92/QuantizedSCI.\n", "link": "http://arxiv.org/abs/2407.21517v1", "date": "2024-07-31", "relevancy": 2.1703, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5519}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5493}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5305}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Simple%20Low-bit%20Quantization%20Framework%20for%20Video%20Snapshot%20Compressive%0A%20%20Imaging&body=Title%3A%20A%20Simple%20Low-bit%20Quantization%20Framework%20for%20Video%20Snapshot%20Compressive%0A%20%20Imaging%0AAuthor%3A%20Miao%20Cao%20and%20Lishun%20Wang%20and%20Huan%20Wang%20and%20Xin%20Yuan%0AAbstract%3A%20%20%20Video%20Snapshot%20Compressive%20Imaging%20%28SCI%29%20aims%20to%20use%20a%20low-speed%202D%20camera%20to%0Acapture%20high-speed%20scene%20as%20snapshot%20compressed%20measurements%2C%20followed%20by%20a%0Areconstruction%20algorithm%20to%20reconstruct%20the%20high-speed%20video%20frames.%0AState-of-the-art%20%28SOTA%29%20deep%20learning-based%20algorithms%20have%20achieved%20impressive%0Aperformance%2C%20yet%20with%20heavy%20computational%20workload.%20Network%20quantization%20is%20a%0Apromising%20way%20to%20reduce%20computational%20cost.%20However%2C%20a%20direct%20low-bit%0Aquantization%20will%20bring%20large%20performance%20drop.%20To%20address%20this%20challenge%2C%20in%0Athis%20paper%2C%20we%20propose%20a%20simple%20low-bit%20quantization%20framework%20%28dubbed%20Q-SCI%29%0Afor%20the%20end-to-end%20deep%20learning-based%20video%20SCI%20reconstruction%20methods%20which%0Ausually%20consist%20of%20a%20feature%20extraction%2C%20feature%20enhancement%2C%20and%20video%0Areconstruction%20module.%20Specifically%2C%20we%20first%20design%20a%20high-quality%20feature%0Aextraction%20module%20and%20a%20precise%20video%20reconstruction%20module%20to%20extract%20and%0Apropagate%20high-quality%20features%20in%20the%20low-bit%20quantized%20model.%20In%20addition%2C%20to%0Aalleviate%20the%20information%20distortion%20of%20the%20Transformer%20branch%20in%20the%20quantized%0Afeature%20enhancement%20module%2C%20we%20introduce%20a%20shift%20operation%20on%20the%20query%20and%20key%0Adistributions%20to%20further%20bridge%20the%20performance%20gap.%20Comprehensive%20experimental%0Aresults%20manifest%20that%20our%20Q-SCI%20framework%20can%20achieve%20superior%20performance%2C%0Ae.g.%2C%204-bit%20quantized%20EfficientSCI-S%20derived%20by%20our%20Q-SCI%20framework%20can%0Atheoretically%20accelerate%20the%20real-valued%20EfficientSCI-S%20by%207.8X%20with%20only%202.3%25%0Aperformance%20gap%20on%20the%20simulation%20testing%20datasets.%20Code%20is%20available%20at%0Ahttps%3A//github.com/mcao92/QuantizedSCI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21517v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Simple%2520Low-bit%2520Quantization%2520Framework%2520for%2520Video%2520Snapshot%2520Compressive%250A%2520%2520Imaging%26entry.906535625%3DMiao%2520Cao%2520and%2520Lishun%2520Wang%2520and%2520Huan%2520Wang%2520and%2520Xin%2520Yuan%26entry.1292438233%3D%2520%2520Video%2520Snapshot%2520Compressive%2520Imaging%2520%2528SCI%2529%2520aims%2520to%2520use%2520a%2520low-speed%25202D%2520camera%2520to%250Acapture%2520high-speed%2520scene%2520as%2520snapshot%2520compressed%2520measurements%252C%2520followed%2520by%2520a%250Areconstruction%2520algorithm%2520to%2520reconstruct%2520the%2520high-speed%2520video%2520frames.%250AState-of-the-art%2520%2528SOTA%2529%2520deep%2520learning-based%2520algorithms%2520have%2520achieved%2520impressive%250Aperformance%252C%2520yet%2520with%2520heavy%2520computational%2520workload.%2520Network%2520quantization%2520is%2520a%250Apromising%2520way%2520to%2520reduce%2520computational%2520cost.%2520However%252C%2520a%2520direct%2520low-bit%250Aquantization%2520will%2520bring%2520large%2520performance%2520drop.%2520To%2520address%2520this%2520challenge%252C%2520in%250Athis%2520paper%252C%2520we%2520propose%2520a%2520simple%2520low-bit%2520quantization%2520framework%2520%2528dubbed%2520Q-SCI%2529%250Afor%2520the%2520end-to-end%2520deep%2520learning-based%2520video%2520SCI%2520reconstruction%2520methods%2520which%250Ausually%2520consist%2520of%2520a%2520feature%2520extraction%252C%2520feature%2520enhancement%252C%2520and%2520video%250Areconstruction%2520module.%2520Specifically%252C%2520we%2520first%2520design%2520a%2520high-quality%2520feature%250Aextraction%2520module%2520and%2520a%2520precise%2520video%2520reconstruction%2520module%2520to%2520extract%2520and%250Apropagate%2520high-quality%2520features%2520in%2520the%2520low-bit%2520quantized%2520model.%2520In%2520addition%252C%2520to%250Aalleviate%2520the%2520information%2520distortion%2520of%2520the%2520Transformer%2520branch%2520in%2520the%2520quantized%250Afeature%2520enhancement%2520module%252C%2520we%2520introduce%2520a%2520shift%2520operation%2520on%2520the%2520query%2520and%2520key%250Adistributions%2520to%2520further%2520bridge%2520the%2520performance%2520gap.%2520Comprehensive%2520experimental%250Aresults%2520manifest%2520that%2520our%2520Q-SCI%2520framework%2520can%2520achieve%2520superior%2520performance%252C%250Ae.g.%252C%25204-bit%2520quantized%2520EfficientSCI-S%2520derived%2520by%2520our%2520Q-SCI%2520framework%2520can%250Atheoretically%2520accelerate%2520the%2520real-valued%2520EfficientSCI-S%2520by%25207.8X%2520with%2520only%25202.3%2525%250Aperformance%2520gap%2520on%2520the%2520simulation%2520testing%2520datasets.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/mcao92/QuantizedSCI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21517v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Simple%20Low-bit%20Quantization%20Framework%20for%20Video%20Snapshot%20Compressive%0A%20%20Imaging&entry.906535625=Miao%20Cao%20and%20Lishun%20Wang%20and%20Huan%20Wang%20and%20Xin%20Yuan&entry.1292438233=%20%20Video%20Snapshot%20Compressive%20Imaging%20%28SCI%29%20aims%20to%20use%20a%20low-speed%202D%20camera%20to%0Acapture%20high-speed%20scene%20as%20snapshot%20compressed%20measurements%2C%20followed%20by%20a%0Areconstruction%20algorithm%20to%20reconstruct%20the%20high-speed%20video%20frames.%0AState-of-the-art%20%28SOTA%29%20deep%20learning-based%20algorithms%20have%20achieved%20impressive%0Aperformance%2C%20yet%20with%20heavy%20computational%20workload.%20Network%20quantization%20is%20a%0Apromising%20way%20to%20reduce%20computational%20cost.%20However%2C%20a%20direct%20low-bit%0Aquantization%20will%20bring%20large%20performance%20drop.%20To%20address%20this%20challenge%2C%20in%0Athis%20paper%2C%20we%20propose%20a%20simple%20low-bit%20quantization%20framework%20%28dubbed%20Q-SCI%29%0Afor%20the%20end-to-end%20deep%20learning-based%20video%20SCI%20reconstruction%20methods%20which%0Ausually%20consist%20of%20a%20feature%20extraction%2C%20feature%20enhancement%2C%20and%20video%0Areconstruction%20module.%20Specifically%2C%20we%20first%20design%20a%20high-quality%20feature%0Aextraction%20module%20and%20a%20precise%20video%20reconstruction%20module%20to%20extract%20and%0Apropagate%20high-quality%20features%20in%20the%20low-bit%20quantized%20model.%20In%20addition%2C%20to%0Aalleviate%20the%20information%20distortion%20of%20the%20Transformer%20branch%20in%20the%20quantized%0Afeature%20enhancement%20module%2C%20we%20introduce%20a%20shift%20operation%20on%20the%20query%20and%20key%0Adistributions%20to%20further%20bridge%20the%20performance%20gap.%20Comprehensive%20experimental%0Aresults%20manifest%20that%20our%20Q-SCI%20framework%20can%20achieve%20superior%20performance%2C%0Ae.g.%2C%204-bit%20quantized%20EfficientSCI-S%20derived%20by%20our%20Q-SCI%20framework%20can%0Atheoretically%20accelerate%20the%20real-valued%20EfficientSCI-S%20by%207.8X%20with%20only%202.3%25%0Aperformance%20gap%20on%20the%20simulation%20testing%20datasets.%20Code%20is%20available%20at%0Ahttps%3A//github.com/mcao92/QuantizedSCI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21517v1&entry.124074799=Read"},
{"title": "Evaluating SAM2's Role in Camouflaged Object Detection: From SAM to SAM2", "author": "Lv Tang and Bo Li", "abstract": "  The Segment Anything Model (SAM), introduced by Meta AI Research as a generic\nobject segmentation model, quickly garnered widespread attention and\nsignificantly influenced the academic community. To extend its application to\nvideo, Meta further develops Segment Anything Model 2 (SAM2), a unified model\ncapable of both video and image segmentation. SAM2 shows notable improvements\nover its predecessor in terms of applicable domains, promptable segmentation\naccuracy, and running speed. However, this report reveals a decline in SAM2's\nability to perceive different objects in images without prompts in its auto\nmode, compared to SAM. Specifically, we employ the challenging task of\ncamouflaged object detection to assess this performance decrease, hoping to\ninspire further exploration of the SAM model family by researchers. The results\nof this paper are provided in \\url{https://github.com/luckybird1994/SAMCOD}.\n", "link": "http://arxiv.org/abs/2407.21596v1", "date": "2024-07-31", "relevancy": 2.146, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5663}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5226}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20SAM2%27s%20Role%20in%20Camouflaged%20Object%20Detection%3A%20From%20SAM%20to%20SAM2&body=Title%3A%20Evaluating%20SAM2%27s%20Role%20in%20Camouflaged%20Object%20Detection%3A%20From%20SAM%20to%20SAM2%0AAuthor%3A%20Lv%20Tang%20and%20Bo%20Li%0AAbstract%3A%20%20%20The%20Segment%20Anything%20Model%20%28SAM%29%2C%20introduced%20by%20Meta%20AI%20Research%20as%20a%20generic%0Aobject%20segmentation%20model%2C%20quickly%20garnered%20widespread%20attention%20and%0Asignificantly%20influenced%20the%20academic%20community.%20To%20extend%20its%20application%20to%0Avideo%2C%20Meta%20further%20develops%20Segment%20Anything%20Model%202%20%28SAM2%29%2C%20a%20unified%20model%0Acapable%20of%20both%20video%20and%20image%20segmentation.%20SAM2%20shows%20notable%20improvements%0Aover%20its%20predecessor%20in%20terms%20of%20applicable%20domains%2C%20promptable%20segmentation%0Aaccuracy%2C%20and%20running%20speed.%20However%2C%20this%20report%20reveals%20a%20decline%20in%20SAM2%27s%0Aability%20to%20perceive%20different%20objects%20in%20images%20without%20prompts%20in%20its%20auto%0Amode%2C%20compared%20to%20SAM.%20Specifically%2C%20we%20employ%20the%20challenging%20task%20of%0Acamouflaged%20object%20detection%20to%20assess%20this%20performance%20decrease%2C%20hoping%20to%0Ainspire%20further%20exploration%20of%20the%20SAM%20model%20family%20by%20researchers.%20The%20results%0Aof%20this%20paper%20are%20provided%20in%20%5Curl%7Bhttps%3A//github.com/luckybird1994/SAMCOD%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21596v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520SAM2%2527s%2520Role%2520in%2520Camouflaged%2520Object%2520Detection%253A%2520From%2520SAM%2520to%2520SAM2%26entry.906535625%3DLv%2520Tang%2520and%2520Bo%2520Li%26entry.1292438233%3D%2520%2520The%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%252C%2520introduced%2520by%2520Meta%2520AI%2520Research%2520as%2520a%2520generic%250Aobject%2520segmentation%2520model%252C%2520quickly%2520garnered%2520widespread%2520attention%2520and%250Asignificantly%2520influenced%2520the%2520academic%2520community.%2520To%2520extend%2520its%2520application%2520to%250Avideo%252C%2520Meta%2520further%2520develops%2520Segment%2520Anything%2520Model%25202%2520%2528SAM2%2529%252C%2520a%2520unified%2520model%250Acapable%2520of%2520both%2520video%2520and%2520image%2520segmentation.%2520SAM2%2520shows%2520notable%2520improvements%250Aover%2520its%2520predecessor%2520in%2520terms%2520of%2520applicable%2520domains%252C%2520promptable%2520segmentation%250Aaccuracy%252C%2520and%2520running%2520speed.%2520However%252C%2520this%2520report%2520reveals%2520a%2520decline%2520in%2520SAM2%2527s%250Aability%2520to%2520perceive%2520different%2520objects%2520in%2520images%2520without%2520prompts%2520in%2520its%2520auto%250Amode%252C%2520compared%2520to%2520SAM.%2520Specifically%252C%2520we%2520employ%2520the%2520challenging%2520task%2520of%250Acamouflaged%2520object%2520detection%2520to%2520assess%2520this%2520performance%2520decrease%252C%2520hoping%2520to%250Ainspire%2520further%2520exploration%2520of%2520the%2520SAM%2520model%2520family%2520by%2520researchers.%2520The%2520results%250Aof%2520this%2520paper%2520are%2520provided%2520in%2520%255Curl%257Bhttps%253A//github.com/luckybird1994/SAMCOD%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21596v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20SAM2%27s%20Role%20in%20Camouflaged%20Object%20Detection%3A%20From%20SAM%20to%20SAM2&entry.906535625=Lv%20Tang%20and%20Bo%20Li&entry.1292438233=%20%20The%20Segment%20Anything%20Model%20%28SAM%29%2C%20introduced%20by%20Meta%20AI%20Research%20as%20a%20generic%0Aobject%20segmentation%20model%2C%20quickly%20garnered%20widespread%20attention%20and%0Asignificantly%20influenced%20the%20academic%20community.%20To%20extend%20its%20application%20to%0Avideo%2C%20Meta%20further%20develops%20Segment%20Anything%20Model%202%20%28SAM2%29%2C%20a%20unified%20model%0Acapable%20of%20both%20video%20and%20image%20segmentation.%20SAM2%20shows%20notable%20improvements%0Aover%20its%20predecessor%20in%20terms%20of%20applicable%20domains%2C%20promptable%20segmentation%0Aaccuracy%2C%20and%20running%20speed.%20However%2C%20this%20report%20reveals%20a%20decline%20in%20SAM2%27s%0Aability%20to%20perceive%20different%20objects%20in%20images%20without%20prompts%20in%20its%20auto%0Amode%2C%20compared%20to%20SAM.%20Specifically%2C%20we%20employ%20the%20challenging%20task%20of%0Acamouflaged%20object%20detection%20to%20assess%20this%20performance%20decrease%2C%20hoping%20to%0Ainspire%20further%20exploration%20of%20the%20SAM%20model%20family%20by%20researchers.%20The%20results%0Aof%20this%20paper%20are%20provided%20in%20%5Curl%7Bhttps%3A//github.com/luckybird1994/SAMCOD%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21596v1&entry.124074799=Read"},
{"title": "Skeleton-Based Action Recognition with Spatial-Structural Graph\n  Convolution", "author": "Jingyao Wang and Emmanuel Bergeret and Issam Falih", "abstract": "  Human Activity Recognition (HAR) is a field of study that focuses on\nidentifying and classifying human activities. Skeleton-based Human Activity\nRecognition has received much attention in recent years, where Graph\nConvolutional Network (GCN) based method is widely used and has achieved\nremarkable results. However, the representation of skeleton data and the issue\nof over-smoothing in GCN still need to be studied. 1). Compared to central\nnodes, edge nodes can only aggregate limited neighbor information, and\ndifferent edge nodes of the human body are always structurally related.\nHowever, the information from edge nodes is crucial for fine-grained activity\nrecognition. 2). The Graph Convolutional Network suffers from a significant\nover-smoothing issue, causing nodes to become increasingly similar as the\nnumber of network layers increases. Based on these two ideas, we propose a\ntwo-stream graph convolution method called Spatial-Structural GCN (SpSt-GCN).\nSpatial GCN performs information aggregation based on the topological structure\nof the human body, and structural GCN performs differentiation based on the\nsimilarity of edge node sequences. The spatial connection is fixed, and the\nhuman skeleton naturally maintains this topology regardless of the actions\nperformed by humans. However, the structural connection is dynamic and depends\non the type of movement the human body is performing. Based on this idea, we\nalso propose an entirely data-driven structural connection, which greatly\nincreases flexibility. We evaluate our method on two large-scale datasets,\ni.e., NTU RGB+D and NTU RGB+D 120. The proposed method achieves good results\nwhile being efficient.\n", "link": "http://arxiv.org/abs/2407.21525v1", "date": "2024-07-31", "relevancy": 2.1259, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5392}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5292}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Skeleton-Based%20Action%20Recognition%20with%20Spatial-Structural%20Graph%0A%20%20Convolution&body=Title%3A%20Skeleton-Based%20Action%20Recognition%20with%20Spatial-Structural%20Graph%0A%20%20Convolution%0AAuthor%3A%20Jingyao%20Wang%20and%20Emmanuel%20Bergeret%20and%20Issam%20Falih%0AAbstract%3A%20%20%20Human%20Activity%20Recognition%20%28HAR%29%20is%20a%20field%20of%20study%20that%20focuses%20on%0Aidentifying%20and%20classifying%20human%20activities.%20Skeleton-based%20Human%20Activity%0ARecognition%20has%20received%20much%20attention%20in%20recent%20years%2C%20where%20Graph%0AConvolutional%20Network%20%28GCN%29%20based%20method%20is%20widely%20used%20and%20has%20achieved%0Aremarkable%20results.%20However%2C%20the%20representation%20of%20skeleton%20data%20and%20the%20issue%0Aof%20over-smoothing%20in%20GCN%20still%20need%20to%20be%20studied.%201%29.%20Compared%20to%20central%0Anodes%2C%20edge%20nodes%20can%20only%20aggregate%20limited%20neighbor%20information%2C%20and%0Adifferent%20edge%20nodes%20of%20the%20human%20body%20are%20always%20structurally%20related.%0AHowever%2C%20the%20information%20from%20edge%20nodes%20is%20crucial%20for%20fine-grained%20activity%0Arecognition.%202%29.%20The%20Graph%20Convolutional%20Network%20suffers%20from%20a%20significant%0Aover-smoothing%20issue%2C%20causing%20nodes%20to%20become%20increasingly%20similar%20as%20the%0Anumber%20of%20network%20layers%20increases.%20Based%20on%20these%20two%20ideas%2C%20we%20propose%20a%0Atwo-stream%20graph%20convolution%20method%20called%20Spatial-Structural%20GCN%20%28SpSt-GCN%29.%0ASpatial%20GCN%20performs%20information%20aggregation%20based%20on%20the%20topological%20structure%0Aof%20the%20human%20body%2C%20and%20structural%20GCN%20performs%20differentiation%20based%20on%20the%0Asimilarity%20of%20edge%20node%20sequences.%20The%20spatial%20connection%20is%20fixed%2C%20and%20the%0Ahuman%20skeleton%20naturally%20maintains%20this%20topology%20regardless%20of%20the%20actions%0Aperformed%20by%20humans.%20However%2C%20the%20structural%20connection%20is%20dynamic%20and%20depends%0Aon%20the%20type%20of%20movement%20the%20human%20body%20is%20performing.%20Based%20on%20this%20idea%2C%20we%0Aalso%20propose%20an%20entirely%20data-driven%20structural%20connection%2C%20which%20greatly%0Aincreases%20flexibility.%20We%20evaluate%20our%20method%20on%20two%20large-scale%20datasets%2C%0Ai.e.%2C%20NTU%20RGB%2BD%20and%20NTU%20RGB%2BD%20120.%20The%20proposed%20method%20achieves%20good%20results%0Awhile%20being%20efficient.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21525v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkeleton-Based%2520Action%2520Recognition%2520with%2520Spatial-Structural%2520Graph%250A%2520%2520Convolution%26entry.906535625%3DJingyao%2520Wang%2520and%2520Emmanuel%2520Bergeret%2520and%2520Issam%2520Falih%26entry.1292438233%3D%2520%2520Human%2520Activity%2520Recognition%2520%2528HAR%2529%2520is%2520a%2520field%2520of%2520study%2520that%2520focuses%2520on%250Aidentifying%2520and%2520classifying%2520human%2520activities.%2520Skeleton-based%2520Human%2520Activity%250ARecognition%2520has%2520received%2520much%2520attention%2520in%2520recent%2520years%252C%2520where%2520Graph%250AConvolutional%2520Network%2520%2528GCN%2529%2520based%2520method%2520is%2520widely%2520used%2520and%2520has%2520achieved%250Aremarkable%2520results.%2520However%252C%2520the%2520representation%2520of%2520skeleton%2520data%2520and%2520the%2520issue%250Aof%2520over-smoothing%2520in%2520GCN%2520still%2520need%2520to%2520be%2520studied.%25201%2529.%2520Compared%2520to%2520central%250Anodes%252C%2520edge%2520nodes%2520can%2520only%2520aggregate%2520limited%2520neighbor%2520information%252C%2520and%250Adifferent%2520edge%2520nodes%2520of%2520the%2520human%2520body%2520are%2520always%2520structurally%2520related.%250AHowever%252C%2520the%2520information%2520from%2520edge%2520nodes%2520is%2520crucial%2520for%2520fine-grained%2520activity%250Arecognition.%25202%2529.%2520The%2520Graph%2520Convolutional%2520Network%2520suffers%2520from%2520a%2520significant%250Aover-smoothing%2520issue%252C%2520causing%2520nodes%2520to%2520become%2520increasingly%2520similar%2520as%2520the%250Anumber%2520of%2520network%2520layers%2520increases.%2520Based%2520on%2520these%2520two%2520ideas%252C%2520we%2520propose%2520a%250Atwo-stream%2520graph%2520convolution%2520method%2520called%2520Spatial-Structural%2520GCN%2520%2528SpSt-GCN%2529.%250ASpatial%2520GCN%2520performs%2520information%2520aggregation%2520based%2520on%2520the%2520topological%2520structure%250Aof%2520the%2520human%2520body%252C%2520and%2520structural%2520GCN%2520performs%2520differentiation%2520based%2520on%2520the%250Asimilarity%2520of%2520edge%2520node%2520sequences.%2520The%2520spatial%2520connection%2520is%2520fixed%252C%2520and%2520the%250Ahuman%2520skeleton%2520naturally%2520maintains%2520this%2520topology%2520regardless%2520of%2520the%2520actions%250Aperformed%2520by%2520humans.%2520However%252C%2520the%2520structural%2520connection%2520is%2520dynamic%2520and%2520depends%250Aon%2520the%2520type%2520of%2520movement%2520the%2520human%2520body%2520is%2520performing.%2520Based%2520on%2520this%2520idea%252C%2520we%250Aalso%2520propose%2520an%2520entirely%2520data-driven%2520structural%2520connection%252C%2520which%2520greatly%250Aincreases%2520flexibility.%2520We%2520evaluate%2520our%2520method%2520on%2520two%2520large-scale%2520datasets%252C%250Ai.e.%252C%2520NTU%2520RGB%252BD%2520and%2520NTU%2520RGB%252BD%2520120.%2520The%2520proposed%2520method%2520achieves%2520good%2520results%250Awhile%2520being%2520efficient.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21525v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Skeleton-Based%20Action%20Recognition%20with%20Spatial-Structural%20Graph%0A%20%20Convolution&entry.906535625=Jingyao%20Wang%20and%20Emmanuel%20Bergeret%20and%20Issam%20Falih&entry.1292438233=%20%20Human%20Activity%20Recognition%20%28HAR%29%20is%20a%20field%20of%20study%20that%20focuses%20on%0Aidentifying%20and%20classifying%20human%20activities.%20Skeleton-based%20Human%20Activity%0ARecognition%20has%20received%20much%20attention%20in%20recent%20years%2C%20where%20Graph%0AConvolutional%20Network%20%28GCN%29%20based%20method%20is%20widely%20used%20and%20has%20achieved%0Aremarkable%20results.%20However%2C%20the%20representation%20of%20skeleton%20data%20and%20the%20issue%0Aof%20over-smoothing%20in%20GCN%20still%20need%20to%20be%20studied.%201%29.%20Compared%20to%20central%0Anodes%2C%20edge%20nodes%20can%20only%20aggregate%20limited%20neighbor%20information%2C%20and%0Adifferent%20edge%20nodes%20of%20the%20human%20body%20are%20always%20structurally%20related.%0AHowever%2C%20the%20information%20from%20edge%20nodes%20is%20crucial%20for%20fine-grained%20activity%0Arecognition.%202%29.%20The%20Graph%20Convolutional%20Network%20suffers%20from%20a%20significant%0Aover-smoothing%20issue%2C%20causing%20nodes%20to%20become%20increasingly%20similar%20as%20the%0Anumber%20of%20network%20layers%20increases.%20Based%20on%20these%20two%20ideas%2C%20we%20propose%20a%0Atwo-stream%20graph%20convolution%20method%20called%20Spatial-Structural%20GCN%20%28SpSt-GCN%29.%0ASpatial%20GCN%20performs%20information%20aggregation%20based%20on%20the%20topological%20structure%0Aof%20the%20human%20body%2C%20and%20structural%20GCN%20performs%20differentiation%20based%20on%20the%0Asimilarity%20of%20edge%20node%20sequences.%20The%20spatial%20connection%20is%20fixed%2C%20and%20the%0Ahuman%20skeleton%20naturally%20maintains%20this%20topology%20regardless%20of%20the%20actions%0Aperformed%20by%20humans.%20However%2C%20the%20structural%20connection%20is%20dynamic%20and%20depends%0Aon%20the%20type%20of%20movement%20the%20human%20body%20is%20performing.%20Based%20on%20this%20idea%2C%20we%0Aalso%20propose%20an%20entirely%20data-driven%20structural%20connection%2C%20which%20greatly%0Aincreases%20flexibility.%20We%20evaluate%20our%20method%20on%20two%20large-scale%20datasets%2C%0Ai.e.%2C%20NTU%20RGB%2BD%20and%20NTU%20RGB%2BD%20120.%20The%20proposed%20method%20achieves%20good%20results%0Awhile%20being%20efficient.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21525v1&entry.124074799=Read"},
{"title": "Pedestrian Inertial Navigation: An Overview of Model and Data-Driven\n  Approaches", "author": "Itzik Klein", "abstract": "  The task of indoor positioning is fundamental to several applications,\nincluding navigation, healthcare, location-based services, and security. An\nemerging field is inertial navigation for pedestrians, which relies only on\ninertial sensors for positioning. In this paper, we present inertial pedestrian\nnavigation models and learning approaches. Among these, are methods and\nalgorithms for shoe-mounted inertial sensors and pedestrian dead reckoning\n(PDR) with unconstrained inertial sensors. We also address three categories of\ndata-driven PDR strategies: activity-assisted, hybrid approaches, and\nlearning-based frameworks.\n", "link": "http://arxiv.org/abs/2407.21676v1", "date": "2024-07-31", "relevancy": 2.1181, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.548}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5474}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pedestrian%20Inertial%20Navigation%3A%20An%20Overview%20of%20Model%20and%20Data-Driven%0A%20%20Approaches&body=Title%3A%20Pedestrian%20Inertial%20Navigation%3A%20An%20Overview%20of%20Model%20and%20Data-Driven%0A%20%20Approaches%0AAuthor%3A%20Itzik%20Klein%0AAbstract%3A%20%20%20The%20task%20of%20indoor%20positioning%20is%20fundamental%20to%20several%20applications%2C%0Aincluding%20navigation%2C%20healthcare%2C%20location-based%20services%2C%20and%20security.%20An%0Aemerging%20field%20is%20inertial%20navigation%20for%20pedestrians%2C%20which%20relies%20only%20on%0Ainertial%20sensors%20for%20positioning.%20In%20this%20paper%2C%20we%20present%20inertial%20pedestrian%0Anavigation%20models%20and%20learning%20approaches.%20Among%20these%2C%20are%20methods%20and%0Aalgorithms%20for%20shoe-mounted%20inertial%20sensors%20and%20pedestrian%20dead%20reckoning%0A%28PDR%29%20with%20unconstrained%20inertial%20sensors.%20We%20also%20address%20three%20categories%20of%0Adata-driven%20PDR%20strategies%3A%20activity-assisted%2C%20hybrid%20approaches%2C%20and%0Alearning-based%20frameworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21676v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPedestrian%2520Inertial%2520Navigation%253A%2520An%2520Overview%2520of%2520Model%2520and%2520Data-Driven%250A%2520%2520Approaches%26entry.906535625%3DItzik%2520Klein%26entry.1292438233%3D%2520%2520The%2520task%2520of%2520indoor%2520positioning%2520is%2520fundamental%2520to%2520several%2520applications%252C%250Aincluding%2520navigation%252C%2520healthcare%252C%2520location-based%2520services%252C%2520and%2520security.%2520An%250Aemerging%2520field%2520is%2520inertial%2520navigation%2520for%2520pedestrians%252C%2520which%2520relies%2520only%2520on%250Ainertial%2520sensors%2520for%2520positioning.%2520In%2520this%2520paper%252C%2520we%2520present%2520inertial%2520pedestrian%250Anavigation%2520models%2520and%2520learning%2520approaches.%2520Among%2520these%252C%2520are%2520methods%2520and%250Aalgorithms%2520for%2520shoe-mounted%2520inertial%2520sensors%2520and%2520pedestrian%2520dead%2520reckoning%250A%2528PDR%2529%2520with%2520unconstrained%2520inertial%2520sensors.%2520We%2520also%2520address%2520three%2520categories%2520of%250Adata-driven%2520PDR%2520strategies%253A%2520activity-assisted%252C%2520hybrid%2520approaches%252C%2520and%250Alearning-based%2520frameworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21676v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pedestrian%20Inertial%20Navigation%3A%20An%20Overview%20of%20Model%20and%20Data-Driven%0A%20%20Approaches&entry.906535625=Itzik%20Klein&entry.1292438233=%20%20The%20task%20of%20indoor%20positioning%20is%20fundamental%20to%20several%20applications%2C%0Aincluding%20navigation%2C%20healthcare%2C%20location-based%20services%2C%20and%20security.%20An%0Aemerging%20field%20is%20inertial%20navigation%20for%20pedestrians%2C%20which%20relies%20only%20on%0Ainertial%20sensors%20for%20positioning.%20In%20this%20paper%2C%20we%20present%20inertial%20pedestrian%0Anavigation%20models%20and%20learning%20approaches.%20Among%20these%2C%20are%20methods%20and%0Aalgorithms%20for%20shoe-mounted%20inertial%20sensors%20and%20pedestrian%20dead%20reckoning%0A%28PDR%29%20with%20unconstrained%20inertial%20sensors.%20We%20also%20address%20three%20categories%20of%0Adata-driven%20PDR%20strategies%3A%20activity-assisted%2C%20hybrid%20approaches%2C%20and%0Alearning-based%20frameworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21676v1&entry.124074799=Read"},
{"title": "V-RECS, a Low-Cost LLM4VIS Recommender with Explanations, Captioning and\n  Suggestions", "author": "Luca Podo and Marco Angelini and Paola Velardi", "abstract": "  NL2VIS (natural language to visualization) is a promising and recent research\narea that involves interpreting natural language queries and translating them\ninto visualizations that accurately represent the underlying data. As we\nnavigate the era of big data, NL2VIS holds considerable application potential\nsince it greatly facilitates data exploration by non-expert users. Following\nthe increasingly widespread usage of generative AI in NL2VIS applications, in\nthis paper we present V-RECS, the first LLM-based Visual Recommender augmented\nwith explanations(E), captioning(C), and suggestions(S) for further data\nexploration. V-RECS' visualization narratives facilitate both response\nverification and data exploration by non-expert users. Furthermore, our\nproposed solution mitigates computational, controllability, and cost issues\nassociated with using powerful LLMs by leveraging a methodology to effectively\nfine-tune small models. To generate insightful visualization narratives, we use\nChain-of-Thoughts (CoT), a prompt engineering technique to help LLM identify\nand generate the logical steps to produce a correct answer. Since CoT is\nreported to perform poorly with small LLMs, we adopted a strategy in which a\nlarge LLM (GPT-4), acting as a Teacher, generates CoT-based instructions to\nfine-tune a small model, Llama-2-7B, which plays the role of a Student.\nExtensive experiments-based on a framework for the quantitative evaluation of\nAI-based visualizations and on manual assessment by a group of\nparticipants-show that V-RECS achieves performance scores comparable to GPT-4,\nat a much lower cost. The efficacy of the V-RECS teacher-student paradigm is\nalso demonstrated by the fact that the un-tuned Llama fails to perform the task\nin the vast majority of test cases. We release V-RECS for the visualization\ncommunity to assist visualization designers throughout the entire visualization\ngeneration process.\n", "link": "http://arxiv.org/abs/2406.15259v2", "date": "2024-07-31", "relevancy": 2.1172, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5342}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5301}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V-RECS%2C%20a%20Low-Cost%20LLM4VIS%20Recommender%20with%20Explanations%2C%20Captioning%20and%0A%20%20Suggestions&body=Title%3A%20V-RECS%2C%20a%20Low-Cost%20LLM4VIS%20Recommender%20with%20Explanations%2C%20Captioning%20and%0A%20%20Suggestions%0AAuthor%3A%20Luca%20Podo%20and%20Marco%20Angelini%20and%20Paola%20Velardi%0AAbstract%3A%20%20%20NL2VIS%20%28natural%20language%20to%20visualization%29%20is%20a%20promising%20and%20recent%20research%0Aarea%20that%20involves%20interpreting%20natural%20language%20queries%20and%20translating%20them%0Ainto%20visualizations%20that%20accurately%20represent%20the%20underlying%20data.%20As%20we%0Anavigate%20the%20era%20of%20big%20data%2C%20NL2VIS%20holds%20considerable%20application%20potential%0Asince%20it%20greatly%20facilitates%20data%20exploration%20by%20non-expert%20users.%20Following%0Athe%20increasingly%20widespread%20usage%20of%20generative%20AI%20in%20NL2VIS%20applications%2C%20in%0Athis%20paper%20we%20present%20V-RECS%2C%20the%20first%20LLM-based%20Visual%20Recommender%20augmented%0Awith%20explanations%28E%29%2C%20captioning%28C%29%2C%20and%20suggestions%28S%29%20for%20further%20data%0Aexploration.%20V-RECS%27%20visualization%20narratives%20facilitate%20both%20response%0Averification%20and%20data%20exploration%20by%20non-expert%20users.%20Furthermore%2C%20our%0Aproposed%20solution%20mitigates%20computational%2C%20controllability%2C%20and%20cost%20issues%0Aassociated%20with%20using%20powerful%20LLMs%20by%20leveraging%20a%20methodology%20to%20effectively%0Afine-tune%20small%20models.%20To%20generate%20insightful%20visualization%20narratives%2C%20we%20use%0AChain-of-Thoughts%20%28CoT%29%2C%20a%20prompt%20engineering%20technique%20to%20help%20LLM%20identify%0Aand%20generate%20the%20logical%20steps%20to%20produce%20a%20correct%20answer.%20Since%20CoT%20is%0Areported%20to%20perform%20poorly%20with%20small%20LLMs%2C%20we%20adopted%20a%20strategy%20in%20which%20a%0Alarge%20LLM%20%28GPT-4%29%2C%20acting%20as%20a%20Teacher%2C%20generates%20CoT-based%20instructions%20to%0Afine-tune%20a%20small%20model%2C%20Llama-2-7B%2C%20which%20plays%20the%20role%20of%20a%20Student.%0AExtensive%20experiments-based%20on%20a%20framework%20for%20the%20quantitative%20evaluation%20of%0AAI-based%20visualizations%20and%20on%20manual%20assessment%20by%20a%20group%20of%0Aparticipants-show%20that%20V-RECS%20achieves%20performance%20scores%20comparable%20to%20GPT-4%2C%0Aat%20a%20much%20lower%20cost.%20The%20efficacy%20of%20the%20V-RECS%20teacher-student%20paradigm%20is%0Aalso%20demonstrated%20by%20the%20fact%20that%20the%20un-tuned%20Llama%20fails%20to%20perform%20the%20task%0Ain%20the%20vast%20majority%20of%20test%20cases.%20We%20release%20V-RECS%20for%20the%20visualization%0Acommunity%20to%20assist%20visualization%20designers%20throughout%20the%20entire%20visualization%0Ageneration%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15259v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV-RECS%252C%2520a%2520Low-Cost%2520LLM4VIS%2520Recommender%2520with%2520Explanations%252C%2520Captioning%2520and%250A%2520%2520Suggestions%26entry.906535625%3DLuca%2520Podo%2520and%2520Marco%2520Angelini%2520and%2520Paola%2520Velardi%26entry.1292438233%3D%2520%2520NL2VIS%2520%2528natural%2520language%2520to%2520visualization%2529%2520is%2520a%2520promising%2520and%2520recent%2520research%250Aarea%2520that%2520involves%2520interpreting%2520natural%2520language%2520queries%2520and%2520translating%2520them%250Ainto%2520visualizations%2520that%2520accurately%2520represent%2520the%2520underlying%2520data.%2520As%2520we%250Anavigate%2520the%2520era%2520of%2520big%2520data%252C%2520NL2VIS%2520holds%2520considerable%2520application%2520potential%250Asince%2520it%2520greatly%2520facilitates%2520data%2520exploration%2520by%2520non-expert%2520users.%2520Following%250Athe%2520increasingly%2520widespread%2520usage%2520of%2520generative%2520AI%2520in%2520NL2VIS%2520applications%252C%2520in%250Athis%2520paper%2520we%2520present%2520V-RECS%252C%2520the%2520first%2520LLM-based%2520Visual%2520Recommender%2520augmented%250Awith%2520explanations%2528E%2529%252C%2520captioning%2528C%2529%252C%2520and%2520suggestions%2528S%2529%2520for%2520further%2520data%250Aexploration.%2520V-RECS%2527%2520visualization%2520narratives%2520facilitate%2520both%2520response%250Averification%2520and%2520data%2520exploration%2520by%2520non-expert%2520users.%2520Furthermore%252C%2520our%250Aproposed%2520solution%2520mitigates%2520computational%252C%2520controllability%252C%2520and%2520cost%2520issues%250Aassociated%2520with%2520using%2520powerful%2520LLMs%2520by%2520leveraging%2520a%2520methodology%2520to%2520effectively%250Afine-tune%2520small%2520models.%2520To%2520generate%2520insightful%2520visualization%2520narratives%252C%2520we%2520use%250AChain-of-Thoughts%2520%2528CoT%2529%252C%2520a%2520prompt%2520engineering%2520technique%2520to%2520help%2520LLM%2520identify%250Aand%2520generate%2520the%2520logical%2520steps%2520to%2520produce%2520a%2520correct%2520answer.%2520Since%2520CoT%2520is%250Areported%2520to%2520perform%2520poorly%2520with%2520small%2520LLMs%252C%2520we%2520adopted%2520a%2520strategy%2520in%2520which%2520a%250Alarge%2520LLM%2520%2528GPT-4%2529%252C%2520acting%2520as%2520a%2520Teacher%252C%2520generates%2520CoT-based%2520instructions%2520to%250Afine-tune%2520a%2520small%2520model%252C%2520Llama-2-7B%252C%2520which%2520plays%2520the%2520role%2520of%2520a%2520Student.%250AExtensive%2520experiments-based%2520on%2520a%2520framework%2520for%2520the%2520quantitative%2520evaluation%2520of%250AAI-based%2520visualizations%2520and%2520on%2520manual%2520assessment%2520by%2520a%2520group%2520of%250Aparticipants-show%2520that%2520V-RECS%2520achieves%2520performance%2520scores%2520comparable%2520to%2520GPT-4%252C%250Aat%2520a%2520much%2520lower%2520cost.%2520The%2520efficacy%2520of%2520the%2520V-RECS%2520teacher-student%2520paradigm%2520is%250Aalso%2520demonstrated%2520by%2520the%2520fact%2520that%2520the%2520un-tuned%2520Llama%2520fails%2520to%2520perform%2520the%2520task%250Ain%2520the%2520vast%2520majority%2520of%2520test%2520cases.%2520We%2520release%2520V-RECS%2520for%2520the%2520visualization%250Acommunity%2520to%2520assist%2520visualization%2520designers%2520throughout%2520the%2520entire%2520visualization%250Ageneration%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15259v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V-RECS%2C%20a%20Low-Cost%20LLM4VIS%20Recommender%20with%20Explanations%2C%20Captioning%20and%0A%20%20Suggestions&entry.906535625=Luca%20Podo%20and%20Marco%20Angelini%20and%20Paola%20Velardi&entry.1292438233=%20%20NL2VIS%20%28natural%20language%20to%20visualization%29%20is%20a%20promising%20and%20recent%20research%0Aarea%20that%20involves%20interpreting%20natural%20language%20queries%20and%20translating%20them%0Ainto%20visualizations%20that%20accurately%20represent%20the%20underlying%20data.%20As%20we%0Anavigate%20the%20era%20of%20big%20data%2C%20NL2VIS%20holds%20considerable%20application%20potential%0Asince%20it%20greatly%20facilitates%20data%20exploration%20by%20non-expert%20users.%20Following%0Athe%20increasingly%20widespread%20usage%20of%20generative%20AI%20in%20NL2VIS%20applications%2C%20in%0Athis%20paper%20we%20present%20V-RECS%2C%20the%20first%20LLM-based%20Visual%20Recommender%20augmented%0Awith%20explanations%28E%29%2C%20captioning%28C%29%2C%20and%20suggestions%28S%29%20for%20further%20data%0Aexploration.%20V-RECS%27%20visualization%20narratives%20facilitate%20both%20response%0Averification%20and%20data%20exploration%20by%20non-expert%20users.%20Furthermore%2C%20our%0Aproposed%20solution%20mitigates%20computational%2C%20controllability%2C%20and%20cost%20issues%0Aassociated%20with%20using%20powerful%20LLMs%20by%20leveraging%20a%20methodology%20to%20effectively%0Afine-tune%20small%20models.%20To%20generate%20insightful%20visualization%20narratives%2C%20we%20use%0AChain-of-Thoughts%20%28CoT%29%2C%20a%20prompt%20engineering%20technique%20to%20help%20LLM%20identify%0Aand%20generate%20the%20logical%20steps%20to%20produce%20a%20correct%20answer.%20Since%20CoT%20is%0Areported%20to%20perform%20poorly%20with%20small%20LLMs%2C%20we%20adopted%20a%20strategy%20in%20which%20a%0Alarge%20LLM%20%28GPT-4%29%2C%20acting%20as%20a%20Teacher%2C%20generates%20CoT-based%20instructions%20to%0Afine-tune%20a%20small%20model%2C%20Llama-2-7B%2C%20which%20plays%20the%20role%20of%20a%20Student.%0AExtensive%20experiments-based%20on%20a%20framework%20for%20the%20quantitative%20evaluation%20of%0AAI-based%20visualizations%20and%20on%20manual%20assessment%20by%20a%20group%20of%0Aparticipants-show%20that%20V-RECS%20achieves%20performance%20scores%20comparable%20to%20GPT-4%2C%0Aat%20a%20much%20lower%20cost.%20The%20efficacy%20of%20the%20V-RECS%20teacher-student%20paradigm%20is%0Aalso%20demonstrated%20by%20the%20fact%20that%20the%20un-tuned%20Llama%20fails%20to%20perform%20the%20task%0Ain%20the%20vast%20majority%20of%20test%20cases.%20We%20release%20V-RECS%20for%20the%20visualization%0Acommunity%20to%20assist%20visualization%20designers%20throughout%20the%20entire%20visualization%0Ageneration%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15259v2&entry.124074799=Read"},
{"title": "Intention-Aware Planner for Robust and Safe Aerial Tracking", "author": "Qiuyu Ren and Huan Yu and Jiajun Dai and Zhi Zheng and Jun Meng and Li Xu and Chao Xu and Fei Gao and Yanjun Cao", "abstract": "  Autonomous target tracking with quadrotors has wide applications in many\nscenarios, such as cinematographic follow-up shooting or suspect chasing.\nTarget motion prediction is necessary when designing the tracking planner.\nHowever, the widely used constant velocity or constant rotation assumption can\nnot fully capture the dynamics of the target. The tracker may fail when the\ntarget happens to move aggressively, such as sudden turn or deceleration. In\nthis paper, we propose an intention-aware planner by additionally considering\nthe intention of the target to enhance safety and robustness in aerial tracking\napplications. Firstly, a designated intention prediction method is proposed,\nwhich combines a user-defined potential assessment function and a state\nobservation function. A reachable region is generated to specifically evaluate\nthe turning intentions. Then we design an intention-driven hybrid A* method to\npredict the future possible positions for the target. Finally, an\nintention-aware optimization approach is designed to generate a\nspatial-temporal optimal trajectory, allowing the tracker to perceive\nunexpected situations from the target. Benchmark comparisons and real-world\nexperiments are conducted to validate the performance of our method.\n", "link": "http://arxiv.org/abs/2309.08854v4", "date": "2024-07-31", "relevancy": 2.1063, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5424}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5341}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intention-Aware%20Planner%20for%20Robust%20and%20Safe%20Aerial%20Tracking&body=Title%3A%20Intention-Aware%20Planner%20for%20Robust%20and%20Safe%20Aerial%20Tracking%0AAuthor%3A%20Qiuyu%20Ren%20and%20Huan%20Yu%20and%20Jiajun%20Dai%20and%20Zhi%20Zheng%20and%20Jun%20Meng%20and%20Li%20Xu%20and%20Chao%20Xu%20and%20Fei%20Gao%20and%20Yanjun%20Cao%0AAbstract%3A%20%20%20Autonomous%20target%20tracking%20with%20quadrotors%20has%20wide%20applications%20in%20many%0Ascenarios%2C%20such%20as%20cinematographic%20follow-up%20shooting%20or%20suspect%20chasing.%0ATarget%20motion%20prediction%20is%20necessary%20when%20designing%20the%20tracking%20planner.%0AHowever%2C%20the%20widely%20used%20constant%20velocity%20or%20constant%20rotation%20assumption%20can%0Anot%20fully%20capture%20the%20dynamics%20of%20the%20target.%20The%20tracker%20may%20fail%20when%20the%0Atarget%20happens%20to%20move%20aggressively%2C%20such%20as%20sudden%20turn%20or%20deceleration.%20In%0Athis%20paper%2C%20we%20propose%20an%20intention-aware%20planner%20by%20additionally%20considering%0Athe%20intention%20of%20the%20target%20to%20enhance%20safety%20and%20robustness%20in%20aerial%20tracking%0Aapplications.%20Firstly%2C%20a%20designated%20intention%20prediction%20method%20is%20proposed%2C%0Awhich%20combines%20a%20user-defined%20potential%20assessment%20function%20and%20a%20state%0Aobservation%20function.%20A%20reachable%20region%20is%20generated%20to%20specifically%20evaluate%0Athe%20turning%20intentions.%20Then%20we%20design%20an%20intention-driven%20hybrid%20A%2A%20method%20to%0Apredict%20the%20future%20possible%20positions%20for%20the%20target.%20Finally%2C%20an%0Aintention-aware%20optimization%20approach%20is%20designed%20to%20generate%20a%0Aspatial-temporal%20optimal%20trajectory%2C%20allowing%20the%20tracker%20to%20perceive%0Aunexpected%20situations%20from%20the%20target.%20Benchmark%20comparisons%20and%20real-world%0Aexperiments%20are%20conducted%20to%20validate%20the%20performance%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.08854v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntention-Aware%2520Planner%2520for%2520Robust%2520and%2520Safe%2520Aerial%2520Tracking%26entry.906535625%3DQiuyu%2520Ren%2520and%2520Huan%2520Yu%2520and%2520Jiajun%2520Dai%2520and%2520Zhi%2520Zheng%2520and%2520Jun%2520Meng%2520and%2520Li%2520Xu%2520and%2520Chao%2520Xu%2520and%2520Fei%2520Gao%2520and%2520Yanjun%2520Cao%26entry.1292438233%3D%2520%2520Autonomous%2520target%2520tracking%2520with%2520quadrotors%2520has%2520wide%2520applications%2520in%2520many%250Ascenarios%252C%2520such%2520as%2520cinematographic%2520follow-up%2520shooting%2520or%2520suspect%2520chasing.%250ATarget%2520motion%2520prediction%2520is%2520necessary%2520when%2520designing%2520the%2520tracking%2520planner.%250AHowever%252C%2520the%2520widely%2520used%2520constant%2520velocity%2520or%2520constant%2520rotation%2520assumption%2520can%250Anot%2520fully%2520capture%2520the%2520dynamics%2520of%2520the%2520target.%2520The%2520tracker%2520may%2520fail%2520when%2520the%250Atarget%2520happens%2520to%2520move%2520aggressively%252C%2520such%2520as%2520sudden%2520turn%2520or%2520deceleration.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520an%2520intention-aware%2520planner%2520by%2520additionally%2520considering%250Athe%2520intention%2520of%2520the%2520target%2520to%2520enhance%2520safety%2520and%2520robustness%2520in%2520aerial%2520tracking%250Aapplications.%2520Firstly%252C%2520a%2520designated%2520intention%2520prediction%2520method%2520is%2520proposed%252C%250Awhich%2520combines%2520a%2520user-defined%2520potential%2520assessment%2520function%2520and%2520a%2520state%250Aobservation%2520function.%2520A%2520reachable%2520region%2520is%2520generated%2520to%2520specifically%2520evaluate%250Athe%2520turning%2520intentions.%2520Then%2520we%2520design%2520an%2520intention-driven%2520hybrid%2520A%252A%2520method%2520to%250Apredict%2520the%2520future%2520possible%2520positions%2520for%2520the%2520target.%2520Finally%252C%2520an%250Aintention-aware%2520optimization%2520approach%2520is%2520designed%2520to%2520generate%2520a%250Aspatial-temporal%2520optimal%2520trajectory%252C%2520allowing%2520the%2520tracker%2520to%2520perceive%250Aunexpected%2520situations%2520from%2520the%2520target.%2520Benchmark%2520comparisons%2520and%2520real-world%250Aexperiments%2520are%2520conducted%2520to%2520validate%2520the%2520performance%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.08854v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intention-Aware%20Planner%20for%20Robust%20and%20Safe%20Aerial%20Tracking&entry.906535625=Qiuyu%20Ren%20and%20Huan%20Yu%20and%20Jiajun%20Dai%20and%20Zhi%20Zheng%20and%20Jun%20Meng%20and%20Li%20Xu%20and%20Chao%20Xu%20and%20Fei%20Gao%20and%20Yanjun%20Cao&entry.1292438233=%20%20Autonomous%20target%20tracking%20with%20quadrotors%20has%20wide%20applications%20in%20many%0Ascenarios%2C%20such%20as%20cinematographic%20follow-up%20shooting%20or%20suspect%20chasing.%0ATarget%20motion%20prediction%20is%20necessary%20when%20designing%20the%20tracking%20planner.%0AHowever%2C%20the%20widely%20used%20constant%20velocity%20or%20constant%20rotation%20assumption%20can%0Anot%20fully%20capture%20the%20dynamics%20of%20the%20target.%20The%20tracker%20may%20fail%20when%20the%0Atarget%20happens%20to%20move%20aggressively%2C%20such%20as%20sudden%20turn%20or%20deceleration.%20In%0Athis%20paper%2C%20we%20propose%20an%20intention-aware%20planner%20by%20additionally%20considering%0Athe%20intention%20of%20the%20target%20to%20enhance%20safety%20and%20robustness%20in%20aerial%20tracking%0Aapplications.%20Firstly%2C%20a%20designated%20intention%20prediction%20method%20is%20proposed%2C%0Awhich%20combines%20a%20user-defined%20potential%20assessment%20function%20and%20a%20state%0Aobservation%20function.%20A%20reachable%20region%20is%20generated%20to%20specifically%20evaluate%0Athe%20turning%20intentions.%20Then%20we%20design%20an%20intention-driven%20hybrid%20A%2A%20method%20to%0Apredict%20the%20future%20possible%20positions%20for%20the%20target.%20Finally%2C%20an%0Aintention-aware%20optimization%20approach%20is%20designed%20to%20generate%20a%0Aspatial-temporal%20optimal%20trajectory%2C%20allowing%20the%20tracker%20to%20perceive%0Aunexpected%20situations%20from%20the%20target.%20Benchmark%20comparisons%20and%20real-world%0Aexperiments%20are%20conducted%20to%20validate%20the%20performance%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.08854v4&entry.124074799=Read"},
{"title": "Dynamic Object Queries for Transformer-based Incremental Object\n  Detection", "author": "Jichuan Zhang and Wei Li and Shuang Cheng and Ya-Li Li and Shengjin Wang", "abstract": "  Incremental object detection (IOD) aims to sequentially learn new classes,\nwhile maintaining the capability to locate and identify old ones. As the\ntraining data arrives with annotations only with new classes, IOD suffers from\ncatastrophic forgetting. Prior methodologies mainly tackle the forgetting issue\nthrough knowledge distillation and exemplar replay, ignoring the conflict\nbetween limited model capacity and increasing knowledge. In this paper, we\nexplore \\textit{dynamic object queries} for incremental object detection built\non Transformer architecture. We propose the \\textbf{Dy}namic object\n\\textbf{Q}uery-based \\textbf{DE}tection \\textbf{TR}ansformer (DyQ-DETR), which\nincrementally expands the model representation ability to achieve\nstability-plasticity tradeoff. First, a new set of learnable object queries are\nfed into the decoder to represent new classes. These new object queries are\naggregated with those from previous phases to adapt both old and new knowledge\nwell. Second, we propose the isolated bipartite matching for object queries in\ndifferent phases, based on disentangled self-attention. The interaction among\nthe object queries at different phases is eliminated to reduce inter-class\nconfusion. Thanks to the separate supervision and computation over object\nqueries, we further present the risk-balanced partial calibration for effective\nexemplar replay. Extensive experiments demonstrate that DyQ-DETR significantly\nsurpasses the state-of-the-art methods, with limited parameter overhead. Code\nwill be made publicly available.\n", "link": "http://arxiv.org/abs/2407.21687v1", "date": "2024-07-31", "relevancy": 2.1006, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5293}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5285}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Object%20Queries%20for%20Transformer-based%20Incremental%20Object%0A%20%20Detection&body=Title%3A%20Dynamic%20Object%20Queries%20for%20Transformer-based%20Incremental%20Object%0A%20%20Detection%0AAuthor%3A%20Jichuan%20Zhang%20and%20Wei%20Li%20and%20Shuang%20Cheng%20and%20Ya-Li%20Li%20and%20Shengjin%20Wang%0AAbstract%3A%20%20%20Incremental%20object%20detection%20%28IOD%29%20aims%20to%20sequentially%20learn%20new%20classes%2C%0Awhile%20maintaining%20the%20capability%20to%20locate%20and%20identify%20old%20ones.%20As%20the%0Atraining%20data%20arrives%20with%20annotations%20only%20with%20new%20classes%2C%20IOD%20suffers%20from%0Acatastrophic%20forgetting.%20Prior%20methodologies%20mainly%20tackle%20the%20forgetting%20issue%0Athrough%20knowledge%20distillation%20and%20exemplar%20replay%2C%20ignoring%20the%20conflict%0Abetween%20limited%20model%20capacity%20and%20increasing%20knowledge.%20In%20this%20paper%2C%20we%0Aexplore%20%5Ctextit%7Bdynamic%20object%20queries%7D%20for%20incremental%20object%20detection%20built%0Aon%20Transformer%20architecture.%20We%20propose%20the%20%5Ctextbf%7BDy%7Dnamic%20object%0A%5Ctextbf%7BQ%7Duery-based%20%5Ctextbf%7BDE%7Dtection%20%5Ctextbf%7BTR%7Dansformer%20%28DyQ-DETR%29%2C%20which%0Aincrementally%20expands%20the%20model%20representation%20ability%20to%20achieve%0Astability-plasticity%20tradeoff.%20First%2C%20a%20new%20set%20of%20learnable%20object%20queries%20are%0Afed%20into%20the%20decoder%20to%20represent%20new%20classes.%20These%20new%20object%20queries%20are%0Aaggregated%20with%20those%20from%20previous%20phases%20to%20adapt%20both%20old%20and%20new%20knowledge%0Awell.%20Second%2C%20we%20propose%20the%20isolated%20bipartite%20matching%20for%20object%20queries%20in%0Adifferent%20phases%2C%20based%20on%20disentangled%20self-attention.%20The%20interaction%20among%0Athe%20object%20queries%20at%20different%20phases%20is%20eliminated%20to%20reduce%20inter-class%0Aconfusion.%20Thanks%20to%20the%20separate%20supervision%20and%20computation%20over%20object%0Aqueries%2C%20we%20further%20present%20the%20risk-balanced%20partial%20calibration%20for%20effective%0Aexemplar%20replay.%20Extensive%20experiments%20demonstrate%20that%20DyQ-DETR%20significantly%0Asurpasses%20the%20state-of-the-art%20methods%2C%20with%20limited%20parameter%20overhead.%20Code%0Awill%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21687v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Object%2520Queries%2520for%2520Transformer-based%2520Incremental%2520Object%250A%2520%2520Detection%26entry.906535625%3DJichuan%2520Zhang%2520and%2520Wei%2520Li%2520and%2520Shuang%2520Cheng%2520and%2520Ya-Li%2520Li%2520and%2520Shengjin%2520Wang%26entry.1292438233%3D%2520%2520Incremental%2520object%2520detection%2520%2528IOD%2529%2520aims%2520to%2520sequentially%2520learn%2520new%2520classes%252C%250Awhile%2520maintaining%2520the%2520capability%2520to%2520locate%2520and%2520identify%2520old%2520ones.%2520As%2520the%250Atraining%2520data%2520arrives%2520with%2520annotations%2520only%2520with%2520new%2520classes%252C%2520IOD%2520suffers%2520from%250Acatastrophic%2520forgetting.%2520Prior%2520methodologies%2520mainly%2520tackle%2520the%2520forgetting%2520issue%250Athrough%2520knowledge%2520distillation%2520and%2520exemplar%2520replay%252C%2520ignoring%2520the%2520conflict%250Abetween%2520limited%2520model%2520capacity%2520and%2520increasing%2520knowledge.%2520In%2520this%2520paper%252C%2520we%250Aexplore%2520%255Ctextit%257Bdynamic%2520object%2520queries%257D%2520for%2520incremental%2520object%2520detection%2520built%250Aon%2520Transformer%2520architecture.%2520We%2520propose%2520the%2520%255Ctextbf%257BDy%257Dnamic%2520object%250A%255Ctextbf%257BQ%257Duery-based%2520%255Ctextbf%257BDE%257Dtection%2520%255Ctextbf%257BTR%257Dansformer%2520%2528DyQ-DETR%2529%252C%2520which%250Aincrementally%2520expands%2520the%2520model%2520representation%2520ability%2520to%2520achieve%250Astability-plasticity%2520tradeoff.%2520First%252C%2520a%2520new%2520set%2520of%2520learnable%2520object%2520queries%2520are%250Afed%2520into%2520the%2520decoder%2520to%2520represent%2520new%2520classes.%2520These%2520new%2520object%2520queries%2520are%250Aaggregated%2520with%2520those%2520from%2520previous%2520phases%2520to%2520adapt%2520both%2520old%2520and%2520new%2520knowledge%250Awell.%2520Second%252C%2520we%2520propose%2520the%2520isolated%2520bipartite%2520matching%2520for%2520object%2520queries%2520in%250Adifferent%2520phases%252C%2520based%2520on%2520disentangled%2520self-attention.%2520The%2520interaction%2520among%250Athe%2520object%2520queries%2520at%2520different%2520phases%2520is%2520eliminated%2520to%2520reduce%2520inter-class%250Aconfusion.%2520Thanks%2520to%2520the%2520separate%2520supervision%2520and%2520computation%2520over%2520object%250Aqueries%252C%2520we%2520further%2520present%2520the%2520risk-balanced%2520partial%2520calibration%2520for%2520effective%250Aexemplar%2520replay.%2520Extensive%2520experiments%2520demonstrate%2520that%2520DyQ-DETR%2520significantly%250Asurpasses%2520the%2520state-of-the-art%2520methods%252C%2520with%2520limited%2520parameter%2520overhead.%2520Code%250Awill%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21687v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Object%20Queries%20for%20Transformer-based%20Incremental%20Object%0A%20%20Detection&entry.906535625=Jichuan%20Zhang%20and%20Wei%20Li%20and%20Shuang%20Cheng%20and%20Ya-Li%20Li%20and%20Shengjin%20Wang&entry.1292438233=%20%20Incremental%20object%20detection%20%28IOD%29%20aims%20to%20sequentially%20learn%20new%20classes%2C%0Awhile%20maintaining%20the%20capability%20to%20locate%20and%20identify%20old%20ones.%20As%20the%0Atraining%20data%20arrives%20with%20annotations%20only%20with%20new%20classes%2C%20IOD%20suffers%20from%0Acatastrophic%20forgetting.%20Prior%20methodologies%20mainly%20tackle%20the%20forgetting%20issue%0Athrough%20knowledge%20distillation%20and%20exemplar%20replay%2C%20ignoring%20the%20conflict%0Abetween%20limited%20model%20capacity%20and%20increasing%20knowledge.%20In%20this%20paper%2C%20we%0Aexplore%20%5Ctextit%7Bdynamic%20object%20queries%7D%20for%20incremental%20object%20detection%20built%0Aon%20Transformer%20architecture.%20We%20propose%20the%20%5Ctextbf%7BDy%7Dnamic%20object%0A%5Ctextbf%7BQ%7Duery-based%20%5Ctextbf%7BDE%7Dtection%20%5Ctextbf%7BTR%7Dansformer%20%28DyQ-DETR%29%2C%20which%0Aincrementally%20expands%20the%20model%20representation%20ability%20to%20achieve%0Astability-plasticity%20tradeoff.%20First%2C%20a%20new%20set%20of%20learnable%20object%20queries%20are%0Afed%20into%20the%20decoder%20to%20represent%20new%20classes.%20These%20new%20object%20queries%20are%0Aaggregated%20with%20those%20from%20previous%20phases%20to%20adapt%20both%20old%20and%20new%20knowledge%0Awell.%20Second%2C%20we%20propose%20the%20isolated%20bipartite%20matching%20for%20object%20queries%20in%0Adifferent%20phases%2C%20based%20on%20disentangled%20self-attention.%20The%20interaction%20among%0Athe%20object%20queries%20at%20different%20phases%20is%20eliminated%20to%20reduce%20inter-class%0Aconfusion.%20Thanks%20to%20the%20separate%20supervision%20and%20computation%20over%20object%0Aqueries%2C%20we%20further%20present%20the%20risk-balanced%20partial%20calibration%20for%20effective%0Aexemplar%20replay.%20Extensive%20experiments%20demonstrate%20that%20DyQ-DETR%20significantly%0Asurpasses%20the%20state-of-the-art%20methods%2C%20with%20limited%20parameter%20overhead.%20Code%0Awill%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21687v1&entry.124074799=Read"},
{"title": "MSA2Net: Multi-scale Adaptive Attention-guided Network for Medical Image\n  Segmentation", "author": "Sina Ghorbani Kolahi and Seyed Kamal Chaharsooghi and Toktam Khatibi and Afshin Bozorgpour and Reza Azad and Moein Heidari and Ilker Hacihaliloglu and Dorit Merhof", "abstract": "  Medical image segmentation involves identifying and separating object\ninstances in a medical image to delineate various tissues and structures, a\ntask complicated by the significant variations in size, shape, and density of\nthese features. Convolutional neural networks (CNNs) have traditionally been\nused for this task but have limitations in capturing long-range dependencies.\nTransformers, equipped with self-attention mechanisms, aim to address this\nproblem. However, in medical image segmentation it is beneficial to merge both\nlocal and global features to effectively integrate feature maps across various\nscales, capturing both detailed features and broader semantic elements for\ndealing with variations in structures. In this paper, we introduce MSA2Net, a\nnew deep segmentation framework featuring an expedient design of\nskip-connections. These connections facilitate feature fusion by dynamically\nweighting and combining coarse-grained encoder features with fine-grained\ndecoder feature maps. Specifically, we propose a Multi-Scale Adaptive Spatial\nAttention Gate (MASAG), which dynamically adjusts the receptive field (Local\nand Global contextual information) to ensure that spatially relevant features\nare selectively highlighted while minimizing background distractions. Extensive\nevaluations involving dermatology, and radiological datasets demonstrate that\nour MSA2Net outperforms state-of-the-art (SOTA) works or matches their\nperformance. The source code is publicly available at\nhttps://github.com/xmindflow/MSA-2Net.\n", "link": "http://arxiv.org/abs/2407.21640v1", "date": "2024-07-31", "relevancy": 2.1, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5487}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5152}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MSA2Net%3A%20Multi-scale%20Adaptive%20Attention-guided%20Network%20for%20Medical%20Image%0A%20%20Segmentation&body=Title%3A%20MSA2Net%3A%20Multi-scale%20Adaptive%20Attention-guided%20Network%20for%20Medical%20Image%0A%20%20Segmentation%0AAuthor%3A%20Sina%20Ghorbani%20Kolahi%20and%20Seyed%20Kamal%20Chaharsooghi%20and%20Toktam%20Khatibi%20and%20Afshin%20Bozorgpour%20and%20Reza%20Azad%20and%20Moein%20Heidari%20and%20Ilker%20Hacihaliloglu%20and%20Dorit%20Merhof%0AAbstract%3A%20%20%20Medical%20image%20segmentation%20involves%20identifying%20and%20separating%20object%0Ainstances%20in%20a%20medical%20image%20to%20delineate%20various%20tissues%20and%20structures%2C%20a%0Atask%20complicated%20by%20the%20significant%20variations%20in%20size%2C%20shape%2C%20and%20density%20of%0Athese%20features.%20Convolutional%20neural%20networks%20%28CNNs%29%20have%20traditionally%20been%0Aused%20for%20this%20task%20but%20have%20limitations%20in%20capturing%20long-range%20dependencies.%0ATransformers%2C%20equipped%20with%20self-attention%20mechanisms%2C%20aim%20to%20address%20this%0Aproblem.%20However%2C%20in%20medical%20image%20segmentation%20it%20is%20beneficial%20to%20merge%20both%0Alocal%20and%20global%20features%20to%20effectively%20integrate%20feature%20maps%20across%20various%0Ascales%2C%20capturing%20both%20detailed%20features%20and%20broader%20semantic%20elements%20for%0Adealing%20with%20variations%20in%20structures.%20In%20this%20paper%2C%20we%20introduce%20MSA2Net%2C%20a%0Anew%20deep%20segmentation%20framework%20featuring%20an%20expedient%20design%20of%0Askip-connections.%20These%20connections%20facilitate%20feature%20fusion%20by%20dynamically%0Aweighting%20and%20combining%20coarse-grained%20encoder%20features%20with%20fine-grained%0Adecoder%20feature%20maps.%20Specifically%2C%20we%20propose%20a%20Multi-Scale%20Adaptive%20Spatial%0AAttention%20Gate%20%28MASAG%29%2C%20which%20dynamically%20adjusts%20the%20receptive%20field%20%28Local%0Aand%20Global%20contextual%20information%29%20to%20ensure%20that%20spatially%20relevant%20features%0Aare%20selectively%20highlighted%20while%20minimizing%20background%20distractions.%20Extensive%0Aevaluations%20involving%20dermatology%2C%20and%20radiological%20datasets%20demonstrate%20that%0Aour%20MSA2Net%20outperforms%20state-of-the-art%20%28SOTA%29%20works%20or%20matches%20their%0Aperformance.%20The%20source%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/xmindflow/MSA-2Net.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21640v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMSA2Net%253A%2520Multi-scale%2520Adaptive%2520Attention-guided%2520Network%2520for%2520Medical%2520Image%250A%2520%2520Segmentation%26entry.906535625%3DSina%2520Ghorbani%2520Kolahi%2520and%2520Seyed%2520Kamal%2520Chaharsooghi%2520and%2520Toktam%2520Khatibi%2520and%2520Afshin%2520Bozorgpour%2520and%2520Reza%2520Azad%2520and%2520Moein%2520Heidari%2520and%2520Ilker%2520Hacihaliloglu%2520and%2520Dorit%2520Merhof%26entry.1292438233%3D%2520%2520Medical%2520image%2520segmentation%2520involves%2520identifying%2520and%2520separating%2520object%250Ainstances%2520in%2520a%2520medical%2520image%2520to%2520delineate%2520various%2520tissues%2520and%2520structures%252C%2520a%250Atask%2520complicated%2520by%2520the%2520significant%2520variations%2520in%2520size%252C%2520shape%252C%2520and%2520density%2520of%250Athese%2520features.%2520Convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520have%2520traditionally%2520been%250Aused%2520for%2520this%2520task%2520but%2520have%2520limitations%2520in%2520capturing%2520long-range%2520dependencies.%250ATransformers%252C%2520equipped%2520with%2520self-attention%2520mechanisms%252C%2520aim%2520to%2520address%2520this%250Aproblem.%2520However%252C%2520in%2520medical%2520image%2520segmentation%2520it%2520is%2520beneficial%2520to%2520merge%2520both%250Alocal%2520and%2520global%2520features%2520to%2520effectively%2520integrate%2520feature%2520maps%2520across%2520various%250Ascales%252C%2520capturing%2520both%2520detailed%2520features%2520and%2520broader%2520semantic%2520elements%2520for%250Adealing%2520with%2520variations%2520in%2520structures.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520MSA2Net%252C%2520a%250Anew%2520deep%2520segmentation%2520framework%2520featuring%2520an%2520expedient%2520design%2520of%250Askip-connections.%2520These%2520connections%2520facilitate%2520feature%2520fusion%2520by%2520dynamically%250Aweighting%2520and%2520combining%2520coarse-grained%2520encoder%2520features%2520with%2520fine-grained%250Adecoder%2520feature%2520maps.%2520Specifically%252C%2520we%2520propose%2520a%2520Multi-Scale%2520Adaptive%2520Spatial%250AAttention%2520Gate%2520%2528MASAG%2529%252C%2520which%2520dynamically%2520adjusts%2520the%2520receptive%2520field%2520%2528Local%250Aand%2520Global%2520contextual%2520information%2529%2520to%2520ensure%2520that%2520spatially%2520relevant%2520features%250Aare%2520selectively%2520highlighted%2520while%2520minimizing%2520background%2520distractions.%2520Extensive%250Aevaluations%2520involving%2520dermatology%252C%2520and%2520radiological%2520datasets%2520demonstrate%2520that%250Aour%2520MSA2Net%2520outperforms%2520state-of-the-art%2520%2528SOTA%2529%2520works%2520or%2520matches%2520their%250Aperformance.%2520The%2520source%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/xmindflow/MSA-2Net.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21640v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MSA2Net%3A%20Multi-scale%20Adaptive%20Attention-guided%20Network%20for%20Medical%20Image%0A%20%20Segmentation&entry.906535625=Sina%20Ghorbani%20Kolahi%20and%20Seyed%20Kamal%20Chaharsooghi%20and%20Toktam%20Khatibi%20and%20Afshin%20Bozorgpour%20and%20Reza%20Azad%20and%20Moein%20Heidari%20and%20Ilker%20Hacihaliloglu%20and%20Dorit%20Merhof&entry.1292438233=%20%20Medical%20image%20segmentation%20involves%20identifying%20and%20separating%20object%0Ainstances%20in%20a%20medical%20image%20to%20delineate%20various%20tissues%20and%20structures%2C%20a%0Atask%20complicated%20by%20the%20significant%20variations%20in%20size%2C%20shape%2C%20and%20density%20of%0Athese%20features.%20Convolutional%20neural%20networks%20%28CNNs%29%20have%20traditionally%20been%0Aused%20for%20this%20task%20but%20have%20limitations%20in%20capturing%20long-range%20dependencies.%0ATransformers%2C%20equipped%20with%20self-attention%20mechanisms%2C%20aim%20to%20address%20this%0Aproblem.%20However%2C%20in%20medical%20image%20segmentation%20it%20is%20beneficial%20to%20merge%20both%0Alocal%20and%20global%20features%20to%20effectively%20integrate%20feature%20maps%20across%20various%0Ascales%2C%20capturing%20both%20detailed%20features%20and%20broader%20semantic%20elements%20for%0Adealing%20with%20variations%20in%20structures.%20In%20this%20paper%2C%20we%20introduce%20MSA2Net%2C%20a%0Anew%20deep%20segmentation%20framework%20featuring%20an%20expedient%20design%20of%0Askip-connections.%20These%20connections%20facilitate%20feature%20fusion%20by%20dynamically%0Aweighting%20and%20combining%20coarse-grained%20encoder%20features%20with%20fine-grained%0Adecoder%20feature%20maps.%20Specifically%2C%20we%20propose%20a%20Multi-Scale%20Adaptive%20Spatial%0AAttention%20Gate%20%28MASAG%29%2C%20which%20dynamically%20adjusts%20the%20receptive%20field%20%28Local%0Aand%20Global%20contextual%20information%29%20to%20ensure%20that%20spatially%20relevant%20features%0Aare%20selectively%20highlighted%20while%20minimizing%20background%20distractions.%20Extensive%0Aevaluations%20involving%20dermatology%2C%20and%20radiological%20datasets%20demonstrate%20that%0Aour%20MSA2Net%20outperforms%20state-of-the-art%20%28SOTA%29%20works%20or%20matches%20their%0Aperformance.%20The%20source%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/xmindflow/MSA-2Net.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21640v1&entry.124074799=Read"},
{"title": "Occam Gradient Descent", "author": "B. N. Kausik", "abstract": "  Deep learning neural network models must be large enough to adapt to their\nproblem domain, while small enough to avoid overfitting training data during\ngradient descent. To balance these competing demands, overprovisioned deep\nlearning models such as transformers are trained for a single epoch on large\ndata sets, and hence inefficient with both computing resources and training\ndata. In response to these inefficiencies, we exploit learning theory to derive\nOccam Gradient Descent, an algorithm that interleaves adaptive reduction of\nmodel size to minimize generalization error, with gradient descent on model\nweights to minimize fitting error. In contrast, traditional gradient descent\ngreedily minimizes fitting error without regard to generalization error. Our\nalgorithm simultaneously descends the space of weights and topological size of\nany neural network without modification. With respect to loss, compute and\nmodel size, our experiments show (a) on image classification benchmarks, linear\nand convolutional neural networks trained with Occam Gradient Descent\noutperform traditional gradient descent with or without post-train pruning; (b)\non a range of tabular data classification tasks, neural networks trained with\nOccam Gradient Descent outperform traditional gradient descent, as well as\nRandom Forests; (c) on natural language transformers, Occam Gradient Descent\noutperforms traditional gradient descent.\n", "link": "http://arxiv.org/abs/2405.20194v4", "date": "2024-07-31", "relevancy": 2.0874, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5339}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5311}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Occam%20Gradient%20Descent&body=Title%3A%20Occam%20Gradient%20Descent%0AAuthor%3A%20B.%20N.%20Kausik%0AAbstract%3A%20%20%20Deep%20learning%20neural%20network%20models%20must%20be%20large%20enough%20to%20adapt%20to%20their%0Aproblem%20domain%2C%20while%20small%20enough%20to%20avoid%20overfitting%20training%20data%20during%0Agradient%20descent.%20To%20balance%20these%20competing%20demands%2C%20overprovisioned%20deep%0Alearning%20models%20such%20as%20transformers%20are%20trained%20for%20a%20single%20epoch%20on%20large%0Adata%20sets%2C%20and%20hence%20inefficient%20with%20both%20computing%20resources%20and%20training%0Adata.%20In%20response%20to%20these%20inefficiencies%2C%20we%20exploit%20learning%20theory%20to%20derive%0AOccam%20Gradient%20Descent%2C%20an%20algorithm%20that%20interleaves%20adaptive%20reduction%20of%0Amodel%20size%20to%20minimize%20generalization%20error%2C%20with%20gradient%20descent%20on%20model%0Aweights%20to%20minimize%20fitting%20error.%20In%20contrast%2C%20traditional%20gradient%20descent%0Agreedily%20minimizes%20fitting%20error%20without%20regard%20to%20generalization%20error.%20Our%0Aalgorithm%20simultaneously%20descends%20the%20space%20of%20weights%20and%20topological%20size%20of%0Aany%20neural%20network%20without%20modification.%20With%20respect%20to%20loss%2C%20compute%20and%0Amodel%20size%2C%20our%20experiments%20show%20%28a%29%20on%20image%20classification%20benchmarks%2C%20linear%0Aand%20convolutional%20neural%20networks%20trained%20with%20Occam%20Gradient%20Descent%0Aoutperform%20traditional%20gradient%20descent%20with%20or%20without%20post-train%20pruning%3B%20%28b%29%0Aon%20a%20range%20of%20tabular%20data%20classification%20tasks%2C%20neural%20networks%20trained%20with%0AOccam%20Gradient%20Descent%20outperform%20traditional%20gradient%20descent%2C%20as%20well%20as%0ARandom%20Forests%3B%20%28c%29%20on%20natural%20language%20transformers%2C%20Occam%20Gradient%20Descent%0Aoutperforms%20traditional%20gradient%20descent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20194v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOccam%2520Gradient%2520Descent%26entry.906535625%3DB.%2520N.%2520Kausik%26entry.1292438233%3D%2520%2520Deep%2520learning%2520neural%2520network%2520models%2520must%2520be%2520large%2520enough%2520to%2520adapt%2520to%2520their%250Aproblem%2520domain%252C%2520while%2520small%2520enough%2520to%2520avoid%2520overfitting%2520training%2520data%2520during%250Agradient%2520descent.%2520To%2520balance%2520these%2520competing%2520demands%252C%2520overprovisioned%2520deep%250Alearning%2520models%2520such%2520as%2520transformers%2520are%2520trained%2520for%2520a%2520single%2520epoch%2520on%2520large%250Adata%2520sets%252C%2520and%2520hence%2520inefficient%2520with%2520both%2520computing%2520resources%2520and%2520training%250Adata.%2520In%2520response%2520to%2520these%2520inefficiencies%252C%2520we%2520exploit%2520learning%2520theory%2520to%2520derive%250AOccam%2520Gradient%2520Descent%252C%2520an%2520algorithm%2520that%2520interleaves%2520adaptive%2520reduction%2520of%250Amodel%2520size%2520to%2520minimize%2520generalization%2520error%252C%2520with%2520gradient%2520descent%2520on%2520model%250Aweights%2520to%2520minimize%2520fitting%2520error.%2520In%2520contrast%252C%2520traditional%2520gradient%2520descent%250Agreedily%2520minimizes%2520fitting%2520error%2520without%2520regard%2520to%2520generalization%2520error.%2520Our%250Aalgorithm%2520simultaneously%2520descends%2520the%2520space%2520of%2520weights%2520and%2520topological%2520size%2520of%250Aany%2520neural%2520network%2520without%2520modification.%2520With%2520respect%2520to%2520loss%252C%2520compute%2520and%250Amodel%2520size%252C%2520our%2520experiments%2520show%2520%2528a%2529%2520on%2520image%2520classification%2520benchmarks%252C%2520linear%250Aand%2520convolutional%2520neural%2520networks%2520trained%2520with%2520Occam%2520Gradient%2520Descent%250Aoutperform%2520traditional%2520gradient%2520descent%2520with%2520or%2520without%2520post-train%2520pruning%253B%2520%2528b%2529%250Aon%2520a%2520range%2520of%2520tabular%2520data%2520classification%2520tasks%252C%2520neural%2520networks%2520trained%2520with%250AOccam%2520Gradient%2520Descent%2520outperform%2520traditional%2520gradient%2520descent%252C%2520as%2520well%2520as%250ARandom%2520Forests%253B%2520%2528c%2529%2520on%2520natural%2520language%2520transformers%252C%2520Occam%2520Gradient%2520Descent%250Aoutperforms%2520traditional%2520gradient%2520descent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20194v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Occam%20Gradient%20Descent&entry.906535625=B.%20N.%20Kausik&entry.1292438233=%20%20Deep%20learning%20neural%20network%20models%20must%20be%20large%20enough%20to%20adapt%20to%20their%0Aproblem%20domain%2C%20while%20small%20enough%20to%20avoid%20overfitting%20training%20data%20during%0Agradient%20descent.%20To%20balance%20these%20competing%20demands%2C%20overprovisioned%20deep%0Alearning%20models%20such%20as%20transformers%20are%20trained%20for%20a%20single%20epoch%20on%20large%0Adata%20sets%2C%20and%20hence%20inefficient%20with%20both%20computing%20resources%20and%20training%0Adata.%20In%20response%20to%20these%20inefficiencies%2C%20we%20exploit%20learning%20theory%20to%20derive%0AOccam%20Gradient%20Descent%2C%20an%20algorithm%20that%20interleaves%20adaptive%20reduction%20of%0Amodel%20size%20to%20minimize%20generalization%20error%2C%20with%20gradient%20descent%20on%20model%0Aweights%20to%20minimize%20fitting%20error.%20In%20contrast%2C%20traditional%20gradient%20descent%0Agreedily%20minimizes%20fitting%20error%20without%20regard%20to%20generalization%20error.%20Our%0Aalgorithm%20simultaneously%20descends%20the%20space%20of%20weights%20and%20topological%20size%20of%0Aany%20neural%20network%20without%20modification.%20With%20respect%20to%20loss%2C%20compute%20and%0Amodel%20size%2C%20our%20experiments%20show%20%28a%29%20on%20image%20classification%20benchmarks%2C%20linear%0Aand%20convolutional%20neural%20networks%20trained%20with%20Occam%20Gradient%20Descent%0Aoutperform%20traditional%20gradient%20descent%20with%20or%20without%20post-train%20pruning%3B%20%28b%29%0Aon%20a%20range%20of%20tabular%20data%20classification%20tasks%2C%20neural%20networks%20trained%20with%0AOccam%20Gradient%20Descent%20outperform%20traditional%20gradient%20descent%2C%20as%20well%20as%0ARandom%20Forests%3B%20%28c%29%20on%20natural%20language%20transformers%2C%20Occam%20Gradient%20Descent%0Aoutperforms%20traditional%20gradient%20descent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20194v4&entry.124074799=Read"},
{"title": "Paying More Attention to Image: A Training-Free Method for Alleviating\n  Hallucination in LVLMs", "author": "Shi Liu and Kecheng Zheng and Wei Chen", "abstract": "  Existing Large Vision-Language Models (LVLMs) primarily align image features\nof vision encoder with Large Language Models (LLMs) to leverage their superior\ntext generation capabilities. However, the scale disparity between vision\nencoder and language model may led to LLMs assuming a predominant role in\nmulti-modal comprehension. This imbalance in LVLMs may result in the instances\nof hallucinatory. Concretely, LVLMs may generate consistent descriptions with\nor without visual input, indicating that certain outputs are influenced solely\nby context text. We refer to this phenomenon as \"text inertia.\" To counteract\nthis issue, we introduce a training-free algorithm to find an equilibrium point\nbetween image comprehension and language inference. Specifically, we adaptively\ninvolve adjusting and amplifying the attention weights assigned to image\ntokens, thereby granting greater prominence to visual elements. Meanwhile, we\nsubtract the logits of multi-modal inputs from ones of pure text input, which\ncan help LVLMs be not biased towards LLMs. By enhancing images tokens and\nreducing the stubborn output of LLM, we can let LVLM pay more attention to\nimages, towards alleviating text inertia and reducing the hallucination in\nLVLMs. Our extensive experiments shows that this method substantially reduces\nthe frequency of hallucinatory outputs in various LVLMs in terms of different\nmetrics. Project page is available at https://lalbj.github.io/projects/PAI/.\n", "link": "http://arxiv.org/abs/2407.21771v1", "date": "2024-07-31", "relevancy": 2.0796, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5244}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5239}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5141}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Paying%20More%20Attention%20to%20Image%3A%20A%20Training-Free%20Method%20for%20Alleviating%0A%20%20Hallucination%20in%20LVLMs&body=Title%3A%20Paying%20More%20Attention%20to%20Image%3A%20A%20Training-Free%20Method%20for%20Alleviating%0A%20%20Hallucination%20in%20LVLMs%0AAuthor%3A%20Shi%20Liu%20and%20Kecheng%20Zheng%20and%20Wei%20Chen%0AAbstract%3A%20%20%20Existing%20Large%20Vision-Language%20Models%20%28LVLMs%29%20primarily%20align%20image%20features%0Aof%20vision%20encoder%20with%20Large%20Language%20Models%20%28LLMs%29%20to%20leverage%20their%20superior%0Atext%20generation%20capabilities.%20However%2C%20the%20scale%20disparity%20between%20vision%0Aencoder%20and%20language%20model%20may%20led%20to%20LLMs%20assuming%20a%20predominant%20role%20in%0Amulti-modal%20comprehension.%20This%20imbalance%20in%20LVLMs%20may%20result%20in%20the%20instances%0Aof%20hallucinatory.%20Concretely%2C%20LVLMs%20may%20generate%20consistent%20descriptions%20with%0Aor%20without%20visual%20input%2C%20indicating%20that%20certain%20outputs%20are%20influenced%20solely%0Aby%20context%20text.%20We%20refer%20to%20this%20phenomenon%20as%20%22text%20inertia.%22%20To%20counteract%0Athis%20issue%2C%20we%20introduce%20a%20training-free%20algorithm%20to%20find%20an%20equilibrium%20point%0Abetween%20image%20comprehension%20and%20language%20inference.%20Specifically%2C%20we%20adaptively%0Ainvolve%20adjusting%20and%20amplifying%20the%20attention%20weights%20assigned%20to%20image%0Atokens%2C%20thereby%20granting%20greater%20prominence%20to%20visual%20elements.%20Meanwhile%2C%20we%0Asubtract%20the%20logits%20of%20multi-modal%20inputs%20from%20ones%20of%20pure%20text%20input%2C%20which%0Acan%20help%20LVLMs%20be%20not%20biased%20towards%20LLMs.%20By%20enhancing%20images%20tokens%20and%0Areducing%20the%20stubborn%20output%20of%20LLM%2C%20we%20can%20let%20LVLM%20pay%20more%20attention%20to%0Aimages%2C%20towards%20alleviating%20text%20inertia%20and%20reducing%20the%20hallucination%20in%0ALVLMs.%20Our%20extensive%20experiments%20shows%20that%20this%20method%20substantially%20reduces%0Athe%20frequency%20of%20hallucinatory%20outputs%20in%20various%20LVLMs%20in%20terms%20of%20different%0Ametrics.%20Project%20page%20is%20available%20at%20https%3A//lalbj.github.io/projects/PAI/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21771v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPaying%2520More%2520Attention%2520to%2520Image%253A%2520A%2520Training-Free%2520Method%2520for%2520Alleviating%250A%2520%2520Hallucination%2520in%2520LVLMs%26entry.906535625%3DShi%2520Liu%2520and%2520Kecheng%2520Zheng%2520and%2520Wei%2520Chen%26entry.1292438233%3D%2520%2520Existing%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520primarily%2520align%2520image%2520features%250Aof%2520vision%2520encoder%2520with%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520leverage%2520their%2520superior%250Atext%2520generation%2520capabilities.%2520However%252C%2520the%2520scale%2520disparity%2520between%2520vision%250Aencoder%2520and%2520language%2520model%2520may%2520led%2520to%2520LLMs%2520assuming%2520a%2520predominant%2520role%2520in%250Amulti-modal%2520comprehension.%2520This%2520imbalance%2520in%2520LVLMs%2520may%2520result%2520in%2520the%2520instances%250Aof%2520hallucinatory.%2520Concretely%252C%2520LVLMs%2520may%2520generate%2520consistent%2520descriptions%2520with%250Aor%2520without%2520visual%2520input%252C%2520indicating%2520that%2520certain%2520outputs%2520are%2520influenced%2520solely%250Aby%2520context%2520text.%2520We%2520refer%2520to%2520this%2520phenomenon%2520as%2520%2522text%2520inertia.%2522%2520To%2520counteract%250Athis%2520issue%252C%2520we%2520introduce%2520a%2520training-free%2520algorithm%2520to%2520find%2520an%2520equilibrium%2520point%250Abetween%2520image%2520comprehension%2520and%2520language%2520inference.%2520Specifically%252C%2520we%2520adaptively%250Ainvolve%2520adjusting%2520and%2520amplifying%2520the%2520attention%2520weights%2520assigned%2520to%2520image%250Atokens%252C%2520thereby%2520granting%2520greater%2520prominence%2520to%2520visual%2520elements.%2520Meanwhile%252C%2520we%250Asubtract%2520the%2520logits%2520of%2520multi-modal%2520inputs%2520from%2520ones%2520of%2520pure%2520text%2520input%252C%2520which%250Acan%2520help%2520LVLMs%2520be%2520not%2520biased%2520towards%2520LLMs.%2520By%2520enhancing%2520images%2520tokens%2520and%250Areducing%2520the%2520stubborn%2520output%2520of%2520LLM%252C%2520we%2520can%2520let%2520LVLM%2520pay%2520more%2520attention%2520to%250Aimages%252C%2520towards%2520alleviating%2520text%2520inertia%2520and%2520reducing%2520the%2520hallucination%2520in%250ALVLMs.%2520Our%2520extensive%2520experiments%2520shows%2520that%2520this%2520method%2520substantially%2520reduces%250Athe%2520frequency%2520of%2520hallucinatory%2520outputs%2520in%2520various%2520LVLMs%2520in%2520terms%2520of%2520different%250Ametrics.%2520Project%2520page%2520is%2520available%2520at%2520https%253A//lalbj.github.io/projects/PAI/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21771v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Paying%20More%20Attention%20to%20Image%3A%20A%20Training-Free%20Method%20for%20Alleviating%0A%20%20Hallucination%20in%20LVLMs&entry.906535625=Shi%20Liu%20and%20Kecheng%20Zheng%20and%20Wei%20Chen&entry.1292438233=%20%20Existing%20Large%20Vision-Language%20Models%20%28LVLMs%29%20primarily%20align%20image%20features%0Aof%20vision%20encoder%20with%20Large%20Language%20Models%20%28LLMs%29%20to%20leverage%20their%20superior%0Atext%20generation%20capabilities.%20However%2C%20the%20scale%20disparity%20between%20vision%0Aencoder%20and%20language%20model%20may%20led%20to%20LLMs%20assuming%20a%20predominant%20role%20in%0Amulti-modal%20comprehension.%20This%20imbalance%20in%20LVLMs%20may%20result%20in%20the%20instances%0Aof%20hallucinatory.%20Concretely%2C%20LVLMs%20may%20generate%20consistent%20descriptions%20with%0Aor%20without%20visual%20input%2C%20indicating%20that%20certain%20outputs%20are%20influenced%20solely%0Aby%20context%20text.%20We%20refer%20to%20this%20phenomenon%20as%20%22text%20inertia.%22%20To%20counteract%0Athis%20issue%2C%20we%20introduce%20a%20training-free%20algorithm%20to%20find%20an%20equilibrium%20point%0Abetween%20image%20comprehension%20and%20language%20inference.%20Specifically%2C%20we%20adaptively%0Ainvolve%20adjusting%20and%20amplifying%20the%20attention%20weights%20assigned%20to%20image%0Atokens%2C%20thereby%20granting%20greater%20prominence%20to%20visual%20elements.%20Meanwhile%2C%20we%0Asubtract%20the%20logits%20of%20multi-modal%20inputs%20from%20ones%20of%20pure%20text%20input%2C%20which%0Acan%20help%20LVLMs%20be%20not%20biased%20towards%20LLMs.%20By%20enhancing%20images%20tokens%20and%0Areducing%20the%20stubborn%20output%20of%20LLM%2C%20we%20can%20let%20LVLM%20pay%20more%20attention%20to%0Aimages%2C%20towards%20alleviating%20text%20inertia%20and%20reducing%20the%20hallucination%20in%0ALVLMs.%20Our%20extensive%20experiments%20shows%20that%20this%20method%20substantially%20reduces%0Athe%20frequency%20of%20hallucinatory%20outputs%20in%20various%20LVLMs%20in%20terms%20of%20different%0Ametrics.%20Project%20page%20is%20available%20at%20https%3A//lalbj.github.io/projects/PAI/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21771v1&entry.124074799=Read"},
{"title": "Generalized Out-of-Distribution Detection and Beyond in Vision Language\n  Model Era: A Survey", "author": "Atsuyuki Miyai and Jingkang Yang and Jingyang Zhang and Yifei Ming and Yueqian Lin and Qing Yu and Go Irie and Shafiq Joty and Yixuan Li and Hai Li and Ziwei Liu and Toshihiko Yamasaki and Kiyoharu Aizawa", "abstract": "  Detecting out-of-distribution (OOD) samples is crucial for ensuring the\nsafety of machine learning systems and has shaped the field of OOD detection.\nMeanwhile, several other problems are closely related to OOD detection,\nincluding anomaly detection (AD), novelty detection (ND), open set recognition\n(OSR), and outlier detection (OD). To unify these problems, a generalized OOD\ndetection framework was proposed, taxonomically categorizing these five\nproblems. However, Vision Language Models (VLMs) such as CLIP have\nsignificantly changed the paradigm and blurred the boundaries between these\nfields, again confusing researchers. In this survey, we first present a\ngeneralized OOD detection v2, encapsulating the evolution of AD, ND, OSR, OOD\ndetection, and OD in the VLM era. Our framework reveals that, with some field\ninactivity and integration, the demanding challenges have become OOD detection\nand AD. In addition, we also highlight the significant shift in the definition,\nproblem settings, and benchmarks; we thus feature a comprehensive review of the\nmethodology for OOD detection, including the discussion over other related\ntasks to clarify their relationship to OOD detection. Finally, we explore the\nadvancements in the emerging Large Vision Language Model (LVLM) era, such as\nGPT-4V. We conclude this survey with open challenges and future directions.\n", "link": "http://arxiv.org/abs/2407.21794v1", "date": "2024-07-31", "relevancy": 2.0784, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5252}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5192}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20Out-of-Distribution%20Detection%20and%20Beyond%20in%20Vision%20Language%0A%20%20Model%20Era%3A%20A%20Survey&body=Title%3A%20Generalized%20Out-of-Distribution%20Detection%20and%20Beyond%20in%20Vision%20Language%0A%20%20Model%20Era%3A%20A%20Survey%0AAuthor%3A%20Atsuyuki%20Miyai%20and%20Jingkang%20Yang%20and%20Jingyang%20Zhang%20and%20Yifei%20Ming%20and%20Yueqian%20Lin%20and%20Qing%20Yu%20and%20Go%20Irie%20and%20Shafiq%20Joty%20and%20Yixuan%20Li%20and%20Hai%20Li%20and%20Ziwei%20Liu%20and%20Toshihiko%20Yamasaki%20and%20Kiyoharu%20Aizawa%0AAbstract%3A%20%20%20Detecting%20out-of-distribution%20%28OOD%29%20samples%20is%20crucial%20for%20ensuring%20the%0Asafety%20of%20machine%20learning%20systems%20and%20has%20shaped%20the%20field%20of%20OOD%20detection.%0AMeanwhile%2C%20several%20other%20problems%20are%20closely%20related%20to%20OOD%20detection%2C%0Aincluding%20anomaly%20detection%20%28AD%29%2C%20novelty%20detection%20%28ND%29%2C%20open%20set%20recognition%0A%28OSR%29%2C%20and%20outlier%20detection%20%28OD%29.%20To%20unify%20these%20problems%2C%20a%20generalized%20OOD%0Adetection%20framework%20was%20proposed%2C%20taxonomically%20categorizing%20these%20five%0Aproblems.%20However%2C%20Vision%20Language%20Models%20%28VLMs%29%20such%20as%20CLIP%20have%0Asignificantly%20changed%20the%20paradigm%20and%20blurred%20the%20boundaries%20between%20these%0Afields%2C%20again%20confusing%20researchers.%20In%20this%20survey%2C%20we%20first%20present%20a%0Ageneralized%20OOD%20detection%20v2%2C%20encapsulating%20the%20evolution%20of%20AD%2C%20ND%2C%20OSR%2C%20OOD%0Adetection%2C%20and%20OD%20in%20the%20VLM%20era.%20Our%20framework%20reveals%20that%2C%20with%20some%20field%0Ainactivity%20and%20integration%2C%20the%20demanding%20challenges%20have%20become%20OOD%20detection%0Aand%20AD.%20In%20addition%2C%20we%20also%20highlight%20the%20significant%20shift%20in%20the%20definition%2C%0Aproblem%20settings%2C%20and%20benchmarks%3B%20we%20thus%20feature%20a%20comprehensive%20review%20of%20the%0Amethodology%20for%20OOD%20detection%2C%20including%20the%20discussion%20over%20other%20related%0Atasks%20to%20clarify%20their%20relationship%20to%20OOD%20detection.%20Finally%2C%20we%20explore%20the%0Aadvancements%20in%20the%20emerging%20Large%20Vision%20Language%20Model%20%28LVLM%29%20era%2C%20such%20as%0AGPT-4V.%20We%20conclude%20this%20survey%20with%20open%20challenges%20and%20future%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21794v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520Out-of-Distribution%2520Detection%2520and%2520Beyond%2520in%2520Vision%2520Language%250A%2520%2520Model%2520Era%253A%2520A%2520Survey%26entry.906535625%3DAtsuyuki%2520Miyai%2520and%2520Jingkang%2520Yang%2520and%2520Jingyang%2520Zhang%2520and%2520Yifei%2520Ming%2520and%2520Yueqian%2520Lin%2520and%2520Qing%2520Yu%2520and%2520Go%2520Irie%2520and%2520Shafiq%2520Joty%2520and%2520Yixuan%2520Li%2520and%2520Hai%2520Li%2520and%2520Ziwei%2520Liu%2520and%2520Toshihiko%2520Yamasaki%2520and%2520Kiyoharu%2520Aizawa%26entry.1292438233%3D%2520%2520Detecting%2520out-of-distribution%2520%2528OOD%2529%2520samples%2520is%2520crucial%2520for%2520ensuring%2520the%250Asafety%2520of%2520machine%2520learning%2520systems%2520and%2520has%2520shaped%2520the%2520field%2520of%2520OOD%2520detection.%250AMeanwhile%252C%2520several%2520other%2520problems%2520are%2520closely%2520related%2520to%2520OOD%2520detection%252C%250Aincluding%2520anomaly%2520detection%2520%2528AD%2529%252C%2520novelty%2520detection%2520%2528ND%2529%252C%2520open%2520set%2520recognition%250A%2528OSR%2529%252C%2520and%2520outlier%2520detection%2520%2528OD%2529.%2520To%2520unify%2520these%2520problems%252C%2520a%2520generalized%2520OOD%250Adetection%2520framework%2520was%2520proposed%252C%2520taxonomically%2520categorizing%2520these%2520five%250Aproblems.%2520However%252C%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520such%2520as%2520CLIP%2520have%250Asignificantly%2520changed%2520the%2520paradigm%2520and%2520blurred%2520the%2520boundaries%2520between%2520these%250Afields%252C%2520again%2520confusing%2520researchers.%2520In%2520this%2520survey%252C%2520we%2520first%2520present%2520a%250Ageneralized%2520OOD%2520detection%2520v2%252C%2520encapsulating%2520the%2520evolution%2520of%2520AD%252C%2520ND%252C%2520OSR%252C%2520OOD%250Adetection%252C%2520and%2520OD%2520in%2520the%2520VLM%2520era.%2520Our%2520framework%2520reveals%2520that%252C%2520with%2520some%2520field%250Ainactivity%2520and%2520integration%252C%2520the%2520demanding%2520challenges%2520have%2520become%2520OOD%2520detection%250Aand%2520AD.%2520In%2520addition%252C%2520we%2520also%2520highlight%2520the%2520significant%2520shift%2520in%2520the%2520definition%252C%250Aproblem%2520settings%252C%2520and%2520benchmarks%253B%2520we%2520thus%2520feature%2520a%2520comprehensive%2520review%2520of%2520the%250Amethodology%2520for%2520OOD%2520detection%252C%2520including%2520the%2520discussion%2520over%2520other%2520related%250Atasks%2520to%2520clarify%2520their%2520relationship%2520to%2520OOD%2520detection.%2520Finally%252C%2520we%2520explore%2520the%250Aadvancements%2520in%2520the%2520emerging%2520Large%2520Vision%2520Language%2520Model%2520%2528LVLM%2529%2520era%252C%2520such%2520as%250AGPT-4V.%2520We%2520conclude%2520this%2520survey%2520with%2520open%2520challenges%2520and%2520future%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21794v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20Out-of-Distribution%20Detection%20and%20Beyond%20in%20Vision%20Language%0A%20%20Model%20Era%3A%20A%20Survey&entry.906535625=Atsuyuki%20Miyai%20and%20Jingkang%20Yang%20and%20Jingyang%20Zhang%20and%20Yifei%20Ming%20and%20Yueqian%20Lin%20and%20Qing%20Yu%20and%20Go%20Irie%20and%20Shafiq%20Joty%20and%20Yixuan%20Li%20and%20Hai%20Li%20and%20Ziwei%20Liu%20and%20Toshihiko%20Yamasaki%20and%20Kiyoharu%20Aizawa&entry.1292438233=%20%20Detecting%20out-of-distribution%20%28OOD%29%20samples%20is%20crucial%20for%20ensuring%20the%0Asafety%20of%20machine%20learning%20systems%20and%20has%20shaped%20the%20field%20of%20OOD%20detection.%0AMeanwhile%2C%20several%20other%20problems%20are%20closely%20related%20to%20OOD%20detection%2C%0Aincluding%20anomaly%20detection%20%28AD%29%2C%20novelty%20detection%20%28ND%29%2C%20open%20set%20recognition%0A%28OSR%29%2C%20and%20outlier%20detection%20%28OD%29.%20To%20unify%20these%20problems%2C%20a%20generalized%20OOD%0Adetection%20framework%20was%20proposed%2C%20taxonomically%20categorizing%20these%20five%0Aproblems.%20However%2C%20Vision%20Language%20Models%20%28VLMs%29%20such%20as%20CLIP%20have%0Asignificantly%20changed%20the%20paradigm%20and%20blurred%20the%20boundaries%20between%20these%0Afields%2C%20again%20confusing%20researchers.%20In%20this%20survey%2C%20we%20first%20present%20a%0Ageneralized%20OOD%20detection%20v2%2C%20encapsulating%20the%20evolution%20of%20AD%2C%20ND%2C%20OSR%2C%20OOD%0Adetection%2C%20and%20OD%20in%20the%20VLM%20era.%20Our%20framework%20reveals%20that%2C%20with%20some%20field%0Ainactivity%20and%20integration%2C%20the%20demanding%20challenges%20have%20become%20OOD%20detection%0Aand%20AD.%20In%20addition%2C%20we%20also%20highlight%20the%20significant%20shift%20in%20the%20definition%2C%0Aproblem%20settings%2C%20and%20benchmarks%3B%20we%20thus%20feature%20a%20comprehensive%20review%20of%20the%0Amethodology%20for%20OOD%20detection%2C%20including%20the%20discussion%20over%20other%20related%0Atasks%20to%20clarify%20their%20relationship%20to%20OOD%20detection.%20Finally%2C%20we%20explore%20the%0Aadvancements%20in%20the%20emerging%20Large%20Vision%20Language%20Model%20%28LVLM%29%20era%2C%20such%20as%0AGPT-4V.%20We%20conclude%20this%20survey%20with%20open%20challenges%20and%20future%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21794v1&entry.124074799=Read"},
{"title": "Learning to Plan for Language Modeling from Unlabeled Data", "author": "Nathan Cornille and Marie-Francine Moens and Florian Mai", "abstract": "  By training to predict the next token in an unlabeled corpus, large language\nmodels learn to perform many tasks without any labeled data. However, their\nnext-token-prediction objective arguably limits their performance in scenarios\nthat require planning, such as writing a coherent article. In this paper, we\ntrain a module for planning the future writing process via a self-supervised\nlearning objective. Given the textual context, this planning module learns to\npredict future abstract writing actions, which correspond to centroids in a\nclustered text embedding space. By conditioning on these actions, our model\nextends the successful language model formula to more abstract planning in an\nunsupervised way. Empirically, we demonstrate that our method improves language\nmodeling performance in general, particularly with respect to the text\nstructure. Because our framework uses a planner module that is unsupervised and\nexternal to the language model, new planner modules can be trained at large\nscale and easily be shared with the community.\n", "link": "http://arxiv.org/abs/2404.00614v2", "date": "2024-07-31", "relevancy": 2.0611, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5528}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4928}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Plan%20for%20Language%20Modeling%20from%20Unlabeled%20Data&body=Title%3A%20Learning%20to%20Plan%20for%20Language%20Modeling%20from%20Unlabeled%20Data%0AAuthor%3A%20Nathan%20Cornille%20and%20Marie-Francine%20Moens%20and%20Florian%20Mai%0AAbstract%3A%20%20%20By%20training%20to%20predict%20the%20next%20token%20in%20an%20unlabeled%20corpus%2C%20large%20language%0Amodels%20learn%20to%20perform%20many%20tasks%20without%20any%20labeled%20data.%20However%2C%20their%0Anext-token-prediction%20objective%20arguably%20limits%20their%20performance%20in%20scenarios%0Athat%20require%20planning%2C%20such%20as%20writing%20a%20coherent%20article.%20In%20this%20paper%2C%20we%0Atrain%20a%20module%20for%20planning%20the%20future%20writing%20process%20via%20a%20self-supervised%0Alearning%20objective.%20Given%20the%20textual%20context%2C%20this%20planning%20module%20learns%20to%0Apredict%20future%20abstract%20writing%20actions%2C%20which%20correspond%20to%20centroids%20in%20a%0Aclustered%20text%20embedding%20space.%20By%20conditioning%20on%20these%20actions%2C%20our%20model%0Aextends%20the%20successful%20language%20model%20formula%20to%20more%20abstract%20planning%20in%20an%0Aunsupervised%20way.%20Empirically%2C%20we%20demonstrate%20that%20our%20method%20improves%20language%0Amodeling%20performance%20in%20general%2C%20particularly%20with%20respect%20to%20the%20text%0Astructure.%20Because%20our%20framework%20uses%20a%20planner%20module%20that%20is%20unsupervised%20and%0Aexternal%20to%20the%20language%20model%2C%20new%20planner%20modules%20can%20be%20trained%20at%20large%0Ascale%20and%20easily%20be%20shared%20with%20the%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00614v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Plan%2520for%2520Language%2520Modeling%2520from%2520Unlabeled%2520Data%26entry.906535625%3DNathan%2520Cornille%2520and%2520Marie-Francine%2520Moens%2520and%2520Florian%2520Mai%26entry.1292438233%3D%2520%2520By%2520training%2520to%2520predict%2520the%2520next%2520token%2520in%2520an%2520unlabeled%2520corpus%252C%2520large%2520language%250Amodels%2520learn%2520to%2520perform%2520many%2520tasks%2520without%2520any%2520labeled%2520data.%2520However%252C%2520their%250Anext-token-prediction%2520objective%2520arguably%2520limits%2520their%2520performance%2520in%2520scenarios%250Athat%2520require%2520planning%252C%2520such%2520as%2520writing%2520a%2520coherent%2520article.%2520In%2520this%2520paper%252C%2520we%250Atrain%2520a%2520module%2520for%2520planning%2520the%2520future%2520writing%2520process%2520via%2520a%2520self-supervised%250Alearning%2520objective.%2520Given%2520the%2520textual%2520context%252C%2520this%2520planning%2520module%2520learns%2520to%250Apredict%2520future%2520abstract%2520writing%2520actions%252C%2520which%2520correspond%2520to%2520centroids%2520in%2520a%250Aclustered%2520text%2520embedding%2520space.%2520By%2520conditioning%2520on%2520these%2520actions%252C%2520our%2520model%250Aextends%2520the%2520successful%2520language%2520model%2520formula%2520to%2520more%2520abstract%2520planning%2520in%2520an%250Aunsupervised%2520way.%2520Empirically%252C%2520we%2520demonstrate%2520that%2520our%2520method%2520improves%2520language%250Amodeling%2520performance%2520in%2520general%252C%2520particularly%2520with%2520respect%2520to%2520the%2520text%250Astructure.%2520Because%2520our%2520framework%2520uses%2520a%2520planner%2520module%2520that%2520is%2520unsupervised%2520and%250Aexternal%2520to%2520the%2520language%2520model%252C%2520new%2520planner%2520modules%2520can%2520be%2520trained%2520at%2520large%250Ascale%2520and%2520easily%2520be%2520shared%2520with%2520the%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.00614v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Plan%20for%20Language%20Modeling%20from%20Unlabeled%20Data&entry.906535625=Nathan%20Cornille%20and%20Marie-Francine%20Moens%20and%20Florian%20Mai&entry.1292438233=%20%20By%20training%20to%20predict%20the%20next%20token%20in%20an%20unlabeled%20corpus%2C%20large%20language%0Amodels%20learn%20to%20perform%20many%20tasks%20without%20any%20labeled%20data.%20However%2C%20their%0Anext-token-prediction%20objective%20arguably%20limits%20their%20performance%20in%20scenarios%0Athat%20require%20planning%2C%20such%20as%20writing%20a%20coherent%20article.%20In%20this%20paper%2C%20we%0Atrain%20a%20module%20for%20planning%20the%20future%20writing%20process%20via%20a%20self-supervised%0Alearning%20objective.%20Given%20the%20textual%20context%2C%20this%20planning%20module%20learns%20to%0Apredict%20future%20abstract%20writing%20actions%2C%20which%20correspond%20to%20centroids%20in%20a%0Aclustered%20text%20embedding%20space.%20By%20conditioning%20on%20these%20actions%2C%20our%20model%0Aextends%20the%20successful%20language%20model%20formula%20to%20more%20abstract%20planning%20in%20an%0Aunsupervised%20way.%20Empirically%2C%20we%20demonstrate%20that%20our%20method%20improves%20language%0Amodeling%20performance%20in%20general%2C%20particularly%20with%20respect%20to%20the%20text%0Astructure.%20Because%20our%20framework%20uses%20a%20planner%20module%20that%20is%20unsupervised%20and%0Aexternal%20to%20the%20language%20model%2C%20new%20planner%20modules%20can%20be%20trained%20at%20large%0Ascale%20and%20easily%20be%20shared%20with%20the%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00614v2&entry.124074799=Read"},
{"title": "Tulip Agent -- Enabling LLM-Based Agents to Solve Tasks Using Large Tool\n  Libraries", "author": "Felix Ocker and Daniel Tanneberg and Julian Eggert and Michael Gienger", "abstract": "  We introduce tulip agent, an architecture for autonomous LLM-based agents\nwith Create, Read, Update, and Delete access to a tool library containing a\npotentially large number of tools. In contrast to state-of-the-art\nimplementations, tulip agent does not encode the descriptions of all available\ntools in the system prompt, which counts against the model's context window, or\nembed the entire prompt for retrieving suitable tools. Instead, the tulip agent\ncan recursively search for suitable tools in its extensible tool library,\nimplemented exemplarily as a vector store. The tulip agent architecture\nsignificantly reduces inference costs, allows using even large tool libraries,\nand enables the agent to adapt and extend its set of tools. We evaluate the\narchitecture with several ablation studies in a mathematics context and\ndemonstrate its generalizability with an application to robotics. A reference\nimplementation and the benchmark are available at\ngithub.com/HRI-EU/tulip_agent.\n", "link": "http://arxiv.org/abs/2407.21778v1", "date": "2024-07-31", "relevancy": 2.0604, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5514}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5111}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tulip%20Agent%20--%20Enabling%20LLM-Based%20Agents%20to%20Solve%20Tasks%20Using%20Large%20Tool%0A%20%20Libraries&body=Title%3A%20Tulip%20Agent%20--%20Enabling%20LLM-Based%20Agents%20to%20Solve%20Tasks%20Using%20Large%20Tool%0A%20%20Libraries%0AAuthor%3A%20Felix%20Ocker%20and%20Daniel%20Tanneberg%20and%20Julian%20Eggert%20and%20Michael%20Gienger%0AAbstract%3A%20%20%20We%20introduce%20tulip%20agent%2C%20an%20architecture%20for%20autonomous%20LLM-based%20agents%0Awith%20Create%2C%20Read%2C%20Update%2C%20and%20Delete%20access%20to%20a%20tool%20library%20containing%20a%0Apotentially%20large%20number%20of%20tools.%20In%20contrast%20to%20state-of-the-art%0Aimplementations%2C%20tulip%20agent%20does%20not%20encode%20the%20descriptions%20of%20all%20available%0Atools%20in%20the%20system%20prompt%2C%20which%20counts%20against%20the%20model%27s%20context%20window%2C%20or%0Aembed%20the%20entire%20prompt%20for%20retrieving%20suitable%20tools.%20Instead%2C%20the%20tulip%20agent%0Acan%20recursively%20search%20for%20suitable%20tools%20in%20its%20extensible%20tool%20library%2C%0Aimplemented%20exemplarily%20as%20a%20vector%20store.%20The%20tulip%20agent%20architecture%0Asignificantly%20reduces%20inference%20costs%2C%20allows%20using%20even%20large%20tool%20libraries%2C%0Aand%20enables%20the%20agent%20to%20adapt%20and%20extend%20its%20set%20of%20tools.%20We%20evaluate%20the%0Aarchitecture%20with%20several%20ablation%20studies%20in%20a%20mathematics%20context%20and%0Ademonstrate%20its%20generalizability%20with%20an%20application%20to%20robotics.%20A%20reference%0Aimplementation%20and%20the%20benchmark%20are%20available%20at%0Agithub.com/HRI-EU/tulip_agent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21778v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTulip%2520Agent%2520--%2520Enabling%2520LLM-Based%2520Agents%2520to%2520Solve%2520Tasks%2520Using%2520Large%2520Tool%250A%2520%2520Libraries%26entry.906535625%3DFelix%2520Ocker%2520and%2520Daniel%2520Tanneberg%2520and%2520Julian%2520Eggert%2520and%2520Michael%2520Gienger%26entry.1292438233%3D%2520%2520We%2520introduce%2520tulip%2520agent%252C%2520an%2520architecture%2520for%2520autonomous%2520LLM-based%2520agents%250Awith%2520Create%252C%2520Read%252C%2520Update%252C%2520and%2520Delete%2520access%2520to%2520a%2520tool%2520library%2520containing%2520a%250Apotentially%2520large%2520number%2520of%2520tools.%2520In%2520contrast%2520to%2520state-of-the-art%250Aimplementations%252C%2520tulip%2520agent%2520does%2520not%2520encode%2520the%2520descriptions%2520of%2520all%2520available%250Atools%2520in%2520the%2520system%2520prompt%252C%2520which%2520counts%2520against%2520the%2520model%2527s%2520context%2520window%252C%2520or%250Aembed%2520the%2520entire%2520prompt%2520for%2520retrieving%2520suitable%2520tools.%2520Instead%252C%2520the%2520tulip%2520agent%250Acan%2520recursively%2520search%2520for%2520suitable%2520tools%2520in%2520its%2520extensible%2520tool%2520library%252C%250Aimplemented%2520exemplarily%2520as%2520a%2520vector%2520store.%2520The%2520tulip%2520agent%2520architecture%250Asignificantly%2520reduces%2520inference%2520costs%252C%2520allows%2520using%2520even%2520large%2520tool%2520libraries%252C%250Aand%2520enables%2520the%2520agent%2520to%2520adapt%2520and%2520extend%2520its%2520set%2520of%2520tools.%2520We%2520evaluate%2520the%250Aarchitecture%2520with%2520several%2520ablation%2520studies%2520in%2520a%2520mathematics%2520context%2520and%250Ademonstrate%2520its%2520generalizability%2520with%2520an%2520application%2520to%2520robotics.%2520A%2520reference%250Aimplementation%2520and%2520the%2520benchmark%2520are%2520available%2520at%250Agithub.com/HRI-EU/tulip_agent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21778v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tulip%20Agent%20--%20Enabling%20LLM-Based%20Agents%20to%20Solve%20Tasks%20Using%20Large%20Tool%0A%20%20Libraries&entry.906535625=Felix%20Ocker%20and%20Daniel%20Tanneberg%20and%20Julian%20Eggert%20and%20Michael%20Gienger&entry.1292438233=%20%20We%20introduce%20tulip%20agent%2C%20an%20architecture%20for%20autonomous%20LLM-based%20agents%0Awith%20Create%2C%20Read%2C%20Update%2C%20and%20Delete%20access%20to%20a%20tool%20library%20containing%20a%0Apotentially%20large%20number%20of%20tools.%20In%20contrast%20to%20state-of-the-art%0Aimplementations%2C%20tulip%20agent%20does%20not%20encode%20the%20descriptions%20of%20all%20available%0Atools%20in%20the%20system%20prompt%2C%20which%20counts%20against%20the%20model%27s%20context%20window%2C%20or%0Aembed%20the%20entire%20prompt%20for%20retrieving%20suitable%20tools.%20Instead%2C%20the%20tulip%20agent%0Acan%20recursively%20search%20for%20suitable%20tools%20in%20its%20extensible%20tool%20library%2C%0Aimplemented%20exemplarily%20as%20a%20vector%20store.%20The%20tulip%20agent%20architecture%0Asignificantly%20reduces%20inference%20costs%2C%20allows%20using%20even%20large%20tool%20libraries%2C%0Aand%20enables%20the%20agent%20to%20adapt%20and%20extend%20its%20set%20of%20tools.%20We%20evaluate%20the%0Aarchitecture%20with%20several%20ablation%20studies%20in%20a%20mathematics%20context%20and%0Ademonstrate%20its%20generalizability%20with%20an%20application%20to%20robotics.%20A%20reference%0Aimplementation%20and%20the%20benchmark%20are%20available%20at%0Agithub.com/HRI-EU/tulip_agent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21778v1&entry.124074799=Read"},
{"title": "XMeCap: Meme Caption Generation with Sub-Image Adaptability", "author": "Yuyan Chen and Songzhou Yan and Zhihong Zhu and Zhixu Li and Yanghua Xiao", "abstract": "  Humor, deeply rooted in societal meanings and cultural details, poses a\nunique challenge for machines. While advances have been made in natural\nlanguage processing, real-world humor often thrives in a multi-modal context,\nencapsulated distinctively by memes. This paper poses a particular emphasis on\nthe impact of multi-images on meme captioning. After that, we introduce the\n\\textsc{XMeCap} framework, a novel approach that adopts supervised fine-tuning\nand reinforcement learning based on an innovative reward model, which factors\nin both global and local similarities between visuals and text. Our results,\nbenchmarked against contemporary models, manifest a marked improvement in\ncaption generation for both single-image and multi-image memes, as well as\ndifferent meme categories. \\textsc{XMeCap} achieves an average evaluation score\nof 75.85 for single-image memes and 66.32 for multi-image memes, outperforming\nthe best baseline by 3.71\\% and 4.82\\%, respectively. This research not only\nestablishes a new frontier in meme-related studies but also underscores the\npotential of machines in understanding and generating humor in a multi-modal\nsetting.\n", "link": "http://arxiv.org/abs/2407.17152v2", "date": "2024-07-31", "relevancy": 2.0485, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5303}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5093}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4736}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XMeCap%3A%20Meme%20Caption%20Generation%20with%20Sub-Image%20Adaptability&body=Title%3A%20XMeCap%3A%20Meme%20Caption%20Generation%20with%20Sub-Image%20Adaptability%0AAuthor%3A%20Yuyan%20Chen%20and%20Songzhou%20Yan%20and%20Zhihong%20Zhu%20and%20Zhixu%20Li%20and%20Yanghua%20Xiao%0AAbstract%3A%20%20%20Humor%2C%20deeply%20rooted%20in%20societal%20meanings%20and%20cultural%20details%2C%20poses%20a%0Aunique%20challenge%20for%20machines.%20While%20advances%20have%20been%20made%20in%20natural%0Alanguage%20processing%2C%20real-world%20humor%20often%20thrives%20in%20a%20multi-modal%20context%2C%0Aencapsulated%20distinctively%20by%20memes.%20This%20paper%20poses%20a%20particular%20emphasis%20on%0Athe%20impact%20of%20multi-images%20on%20meme%20captioning.%20After%20that%2C%20we%20introduce%20the%0A%5Ctextsc%7BXMeCap%7D%20framework%2C%20a%20novel%20approach%20that%20adopts%20supervised%20fine-tuning%0Aand%20reinforcement%20learning%20based%20on%20an%20innovative%20reward%20model%2C%20which%20factors%0Ain%20both%20global%20and%20local%20similarities%20between%20visuals%20and%20text.%20Our%20results%2C%0Abenchmarked%20against%20contemporary%20models%2C%20manifest%20a%20marked%20improvement%20in%0Acaption%20generation%20for%20both%20single-image%20and%20multi-image%20memes%2C%20as%20well%20as%0Adifferent%20meme%20categories.%20%5Ctextsc%7BXMeCap%7D%20achieves%20an%20average%20evaluation%20score%0Aof%2075.85%20for%20single-image%20memes%20and%2066.32%20for%20multi-image%20memes%2C%20outperforming%0Athe%20best%20baseline%20by%203.71%5C%25%20and%204.82%5C%25%2C%20respectively.%20This%20research%20not%20only%0Aestablishes%20a%20new%20frontier%20in%20meme-related%20studies%20but%20also%20underscores%20the%0Apotential%20of%20machines%20in%20understanding%20and%20generating%20humor%20in%20a%20multi-modal%0Asetting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17152v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXMeCap%253A%2520Meme%2520Caption%2520Generation%2520with%2520Sub-Image%2520Adaptability%26entry.906535625%3DYuyan%2520Chen%2520and%2520Songzhou%2520Yan%2520and%2520Zhihong%2520Zhu%2520and%2520Zhixu%2520Li%2520and%2520Yanghua%2520Xiao%26entry.1292438233%3D%2520%2520Humor%252C%2520deeply%2520rooted%2520in%2520societal%2520meanings%2520and%2520cultural%2520details%252C%2520poses%2520a%250Aunique%2520challenge%2520for%2520machines.%2520While%2520advances%2520have%2520been%2520made%2520in%2520natural%250Alanguage%2520processing%252C%2520real-world%2520humor%2520often%2520thrives%2520in%2520a%2520multi-modal%2520context%252C%250Aencapsulated%2520distinctively%2520by%2520memes.%2520This%2520paper%2520poses%2520a%2520particular%2520emphasis%2520on%250Athe%2520impact%2520of%2520multi-images%2520on%2520meme%2520captioning.%2520After%2520that%252C%2520we%2520introduce%2520the%250A%255Ctextsc%257BXMeCap%257D%2520framework%252C%2520a%2520novel%2520approach%2520that%2520adopts%2520supervised%2520fine-tuning%250Aand%2520reinforcement%2520learning%2520based%2520on%2520an%2520innovative%2520reward%2520model%252C%2520which%2520factors%250Ain%2520both%2520global%2520and%2520local%2520similarities%2520between%2520visuals%2520and%2520text.%2520Our%2520results%252C%250Abenchmarked%2520against%2520contemporary%2520models%252C%2520manifest%2520a%2520marked%2520improvement%2520in%250Acaption%2520generation%2520for%2520both%2520single-image%2520and%2520multi-image%2520memes%252C%2520as%2520well%2520as%250Adifferent%2520meme%2520categories.%2520%255Ctextsc%257BXMeCap%257D%2520achieves%2520an%2520average%2520evaluation%2520score%250Aof%252075.85%2520for%2520single-image%2520memes%2520and%252066.32%2520for%2520multi-image%2520memes%252C%2520outperforming%250Athe%2520best%2520baseline%2520by%25203.71%255C%2525%2520and%25204.82%255C%2525%252C%2520respectively.%2520This%2520research%2520not%2520only%250Aestablishes%2520a%2520new%2520frontier%2520in%2520meme-related%2520studies%2520but%2520also%2520underscores%2520the%250Apotential%2520of%2520machines%2520in%2520understanding%2520and%2520generating%2520humor%2520in%2520a%2520multi-modal%250Asetting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17152v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XMeCap%3A%20Meme%20Caption%20Generation%20with%20Sub-Image%20Adaptability&entry.906535625=Yuyan%20Chen%20and%20Songzhou%20Yan%20and%20Zhihong%20Zhu%20and%20Zhixu%20Li%20and%20Yanghua%20Xiao&entry.1292438233=%20%20Humor%2C%20deeply%20rooted%20in%20societal%20meanings%20and%20cultural%20details%2C%20poses%20a%0Aunique%20challenge%20for%20machines.%20While%20advances%20have%20been%20made%20in%20natural%0Alanguage%20processing%2C%20real-world%20humor%20often%20thrives%20in%20a%20multi-modal%20context%2C%0Aencapsulated%20distinctively%20by%20memes.%20This%20paper%20poses%20a%20particular%20emphasis%20on%0Athe%20impact%20of%20multi-images%20on%20meme%20captioning.%20After%20that%2C%20we%20introduce%20the%0A%5Ctextsc%7BXMeCap%7D%20framework%2C%20a%20novel%20approach%20that%20adopts%20supervised%20fine-tuning%0Aand%20reinforcement%20learning%20based%20on%20an%20innovative%20reward%20model%2C%20which%20factors%0Ain%20both%20global%20and%20local%20similarities%20between%20visuals%20and%20text.%20Our%20results%2C%0Abenchmarked%20against%20contemporary%20models%2C%20manifest%20a%20marked%20improvement%20in%0Acaption%20generation%20for%20both%20single-image%20and%20multi-image%20memes%2C%20as%20well%20as%0Adifferent%20meme%20categories.%20%5Ctextsc%7BXMeCap%7D%20achieves%20an%20average%20evaluation%20score%0Aof%2075.85%20for%20single-image%20memes%20and%2066.32%20for%20multi-image%20memes%2C%20outperforming%0Athe%20best%20baseline%20by%203.71%5C%25%20and%204.82%5C%25%2C%20respectively.%20This%20research%20not%20only%0Aestablishes%20a%20new%20frontier%20in%20meme-related%20studies%20but%20also%20underscores%20the%0Apotential%20of%20machines%20in%20understanding%20and%20generating%20humor%20in%20a%20multi-modal%0Asetting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17152v2&entry.124074799=Read"},
{"title": "SpaER: Learning Spatio-temporal Equivariant Representations for Fetal\n  Brain Motion Tracking", "author": "Jian Wang and Razieh Faghihpirayesh and Polina Golland and Ali Gholipour", "abstract": "  In this paper, we introduce SpaER, a pioneering method for fetal motion\ntracking that leverages equivariant filters and self-attention mechanisms to\neffectively learn spatio-temporal representations. Different from conventional\napproaches that statically estimate fetal brain motions from pairs of images,\nour method dynamically tracks the rigid movement patterns of the fetal head\nacross temporal and spatial dimensions. Specifically, we first develop an\nequivariant neural network that efficiently learns rigid motion sequences\nthrough low-dimensional spatial representations of images. Subsequently, we\nlearn spatio-temporal representations by incorporating time encoding and\nself-attention neural network layers. This approach allows for the capture of\nlong-term dependencies of fetal brain motion and addresses alignment errors due\nto contrast changes and severe motion artifacts. Our model also provides a\ngeometric deformation estimation that properly addresses image distortions\namong all time frames. To the best of our knowledge, our approach is the first\nto learn spatial-temporal representations via deep neural networks for fetal\nmotion tracking without data augmentation. We validated our model using real\nfetal echo-planar images with simulated and real motions. Our method carries\nsignificant potential value in accurately measuring, tracking, and correcting\nfetal motion in fetal MRI sequences.\n", "link": "http://arxiv.org/abs/2407.20198v3", "date": "2024-07-31", "relevancy": 2.0429, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5155}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5122}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpaER%3A%20Learning%20Spatio-temporal%20Equivariant%20Representations%20for%20Fetal%0A%20%20Brain%20Motion%20Tracking&body=Title%3A%20SpaER%3A%20Learning%20Spatio-temporal%20Equivariant%20Representations%20for%20Fetal%0A%20%20Brain%20Motion%20Tracking%0AAuthor%3A%20Jian%20Wang%20and%20Razieh%20Faghihpirayesh%20and%20Polina%20Golland%20and%20Ali%20Gholipour%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20SpaER%2C%20a%20pioneering%20method%20for%20fetal%20motion%0Atracking%20that%20leverages%20equivariant%20filters%20and%20self-attention%20mechanisms%20to%0Aeffectively%20learn%20spatio-temporal%20representations.%20Different%20from%20conventional%0Aapproaches%20that%20statically%20estimate%20fetal%20brain%20motions%20from%20pairs%20of%20images%2C%0Aour%20method%20dynamically%20tracks%20the%20rigid%20movement%20patterns%20of%20the%20fetal%20head%0Aacross%20temporal%20and%20spatial%20dimensions.%20Specifically%2C%20we%20first%20develop%20an%0Aequivariant%20neural%20network%20that%20efficiently%20learns%20rigid%20motion%20sequences%0Athrough%20low-dimensional%20spatial%20representations%20of%20images.%20Subsequently%2C%20we%0Alearn%20spatio-temporal%20representations%20by%20incorporating%20time%20encoding%20and%0Aself-attention%20neural%20network%20layers.%20This%20approach%20allows%20for%20the%20capture%20of%0Along-term%20dependencies%20of%20fetal%20brain%20motion%20and%20addresses%20alignment%20errors%20due%0Ato%20contrast%20changes%20and%20severe%20motion%20artifacts.%20Our%20model%20also%20provides%20a%0Ageometric%20deformation%20estimation%20that%20properly%20addresses%20image%20distortions%0Aamong%20all%20time%20frames.%20To%20the%20best%20of%20our%20knowledge%2C%20our%20approach%20is%20the%20first%0Ato%20learn%20spatial-temporal%20representations%20via%20deep%20neural%20networks%20for%20fetal%0Amotion%20tracking%20without%20data%20augmentation.%20We%20validated%20our%20model%20using%20real%0Afetal%20echo-planar%20images%20with%20simulated%20and%20real%20motions.%20Our%20method%20carries%0Asignificant%20potential%20value%20in%20accurately%20measuring%2C%20tracking%2C%20and%20correcting%0Afetal%20motion%20in%20fetal%20MRI%20sequences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20198v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpaER%253A%2520Learning%2520Spatio-temporal%2520Equivariant%2520Representations%2520for%2520Fetal%250A%2520%2520Brain%2520Motion%2520Tracking%26entry.906535625%3DJian%2520Wang%2520and%2520Razieh%2520Faghihpirayesh%2520and%2520Polina%2520Golland%2520and%2520Ali%2520Gholipour%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520SpaER%252C%2520a%2520pioneering%2520method%2520for%2520fetal%2520motion%250Atracking%2520that%2520leverages%2520equivariant%2520filters%2520and%2520self-attention%2520mechanisms%2520to%250Aeffectively%2520learn%2520spatio-temporal%2520representations.%2520Different%2520from%2520conventional%250Aapproaches%2520that%2520statically%2520estimate%2520fetal%2520brain%2520motions%2520from%2520pairs%2520of%2520images%252C%250Aour%2520method%2520dynamically%2520tracks%2520the%2520rigid%2520movement%2520patterns%2520of%2520the%2520fetal%2520head%250Aacross%2520temporal%2520and%2520spatial%2520dimensions.%2520Specifically%252C%2520we%2520first%2520develop%2520an%250Aequivariant%2520neural%2520network%2520that%2520efficiently%2520learns%2520rigid%2520motion%2520sequences%250Athrough%2520low-dimensional%2520spatial%2520representations%2520of%2520images.%2520Subsequently%252C%2520we%250Alearn%2520spatio-temporal%2520representations%2520by%2520incorporating%2520time%2520encoding%2520and%250Aself-attention%2520neural%2520network%2520layers.%2520This%2520approach%2520allows%2520for%2520the%2520capture%2520of%250Along-term%2520dependencies%2520of%2520fetal%2520brain%2520motion%2520and%2520addresses%2520alignment%2520errors%2520due%250Ato%2520contrast%2520changes%2520and%2520severe%2520motion%2520artifacts.%2520Our%2520model%2520also%2520provides%2520a%250Ageometric%2520deformation%2520estimation%2520that%2520properly%2520addresses%2520image%2520distortions%250Aamong%2520all%2520time%2520frames.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520our%2520approach%2520is%2520the%2520first%250Ato%2520learn%2520spatial-temporal%2520representations%2520via%2520deep%2520neural%2520networks%2520for%2520fetal%250Amotion%2520tracking%2520without%2520data%2520augmentation.%2520We%2520validated%2520our%2520model%2520using%2520real%250Afetal%2520echo-planar%2520images%2520with%2520simulated%2520and%2520real%2520motions.%2520Our%2520method%2520carries%250Asignificant%2520potential%2520value%2520in%2520accurately%2520measuring%252C%2520tracking%252C%2520and%2520correcting%250Afetal%2520motion%2520in%2520fetal%2520MRI%2520sequences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20198v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpaER%3A%20Learning%20Spatio-temporal%20Equivariant%20Representations%20for%20Fetal%0A%20%20Brain%20Motion%20Tracking&entry.906535625=Jian%20Wang%20and%20Razieh%20Faghihpirayesh%20and%20Polina%20Golland%20and%20Ali%20Gholipour&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20SpaER%2C%20a%20pioneering%20method%20for%20fetal%20motion%0Atracking%20that%20leverages%20equivariant%20filters%20and%20self-attention%20mechanisms%20to%0Aeffectively%20learn%20spatio-temporal%20representations.%20Different%20from%20conventional%0Aapproaches%20that%20statically%20estimate%20fetal%20brain%20motions%20from%20pairs%20of%20images%2C%0Aour%20method%20dynamically%20tracks%20the%20rigid%20movement%20patterns%20of%20the%20fetal%20head%0Aacross%20temporal%20and%20spatial%20dimensions.%20Specifically%2C%20we%20first%20develop%20an%0Aequivariant%20neural%20network%20that%20efficiently%20learns%20rigid%20motion%20sequences%0Athrough%20low-dimensional%20spatial%20representations%20of%20images.%20Subsequently%2C%20we%0Alearn%20spatio-temporal%20representations%20by%20incorporating%20time%20encoding%20and%0Aself-attention%20neural%20network%20layers.%20This%20approach%20allows%20for%20the%20capture%20of%0Along-term%20dependencies%20of%20fetal%20brain%20motion%20and%20addresses%20alignment%20errors%20due%0Ato%20contrast%20changes%20and%20severe%20motion%20artifacts.%20Our%20model%20also%20provides%20a%0Ageometric%20deformation%20estimation%20that%20properly%20addresses%20image%20distortions%0Aamong%20all%20time%20frames.%20To%20the%20best%20of%20our%20knowledge%2C%20our%20approach%20is%20the%20first%0Ato%20learn%20spatial-temporal%20representations%20via%20deep%20neural%20networks%20for%20fetal%0Amotion%20tracking%20without%20data%20augmentation.%20We%20validated%20our%20model%20using%20real%0Afetal%20echo-planar%20images%20with%20simulated%20and%20real%20motions.%20Our%20method%20carries%0Asignificant%20potential%20value%20in%20accurately%20measuring%2C%20tracking%2C%20and%20correcting%0Afetal%20motion%20in%20fetal%20MRI%20sequences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20198v3&entry.124074799=Read"},
{"title": "Adaptive Mix for Semi-Supervised Medical Image Segmentation", "author": "Zhiqiang Shen and Peng Cao and Junming Su and Jinzhu Yang and Osmar R. Zaiane", "abstract": "  Mix-up is a key technique for consistency regularization-based\nsemi-supervised learning methods, generating strong-perturbed samples for\nstrong-weak pseudo-supervision. Existing mix-up operations are performed either\nrandomly or with predefined rules, such as replacing low-confidence patches\nwith high-confidence ones. The former lacks control over the perturbation\ndegree, leading to overfitting on randomly perturbed samples, while the latter\ntends to generate images with trivial perturbations, both of which limit the\neffectiveness of consistency learning. This paper aims to answer the following\nquestion: How can image mix-up perturbation be adaptively performed during\ntraining? To this end, we propose an Adaptive Mix algorithm (AdaMix) for image\nmix-up in a self-paced learning manner. Given that, in general, a model's\nperformance gradually improves during training, AdaMix is equipped with a\nself-paced curriculum that, in the initial training stage, provides relatively\nsimple perturbed samples and then gradually increases the difficulty of\nperturbed images by adaptively controlling the perturbation degree based on the\nmodel's learning state estimated by a self-paced regularize. We develop three\nframeworks with our AdaMix, i.e., AdaMix-ST, AdaMix-MT, and AdaMix-CT, for\nsemi-supervised medical image segmentation. Extensive experiments on three\npublic datasets, including both 2D and 3D modalities, show that the proposed\nframeworks are capable of achieving superior performance. For example, compared\nwith the state-of-the-art, AdaMix-CT achieves relative improvements of 2.62% in\nDice and 48.25% in average surface distance on the ACDC dataset with 10%\nlabeled data. The results demonstrate that mix-up operations with dynamically\nadjusted perturbation strength based on the segmentation model's state can\nsignificantly enhance the effectiveness of consistency regularization.\n", "link": "http://arxiv.org/abs/2407.21586v1", "date": "2024-07-31", "relevancy": 2.0384, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5302}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5143}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Mix%20for%20Semi-Supervised%20Medical%20Image%20Segmentation&body=Title%3A%20Adaptive%20Mix%20for%20Semi-Supervised%20Medical%20Image%20Segmentation%0AAuthor%3A%20Zhiqiang%20Shen%20and%20Peng%20Cao%20and%20Junming%20Su%20and%20Jinzhu%20Yang%20and%20Osmar%20R.%20Zaiane%0AAbstract%3A%20%20%20Mix-up%20is%20a%20key%20technique%20for%20consistency%20regularization-based%0Asemi-supervised%20learning%20methods%2C%20generating%20strong-perturbed%20samples%20for%0Astrong-weak%20pseudo-supervision.%20Existing%20mix-up%20operations%20are%20performed%20either%0Arandomly%20or%20with%20predefined%20rules%2C%20such%20as%20replacing%20low-confidence%20patches%0Awith%20high-confidence%20ones.%20The%20former%20lacks%20control%20over%20the%20perturbation%0Adegree%2C%20leading%20to%20overfitting%20on%20randomly%20perturbed%20samples%2C%20while%20the%20latter%0Atends%20to%20generate%20images%20with%20trivial%20perturbations%2C%20both%20of%20which%20limit%20the%0Aeffectiveness%20of%20consistency%20learning.%20This%20paper%20aims%20to%20answer%20the%20following%0Aquestion%3A%20How%20can%20image%20mix-up%20perturbation%20be%20adaptively%20performed%20during%0Atraining%3F%20To%20this%20end%2C%20we%20propose%20an%20Adaptive%20Mix%20algorithm%20%28AdaMix%29%20for%20image%0Amix-up%20in%20a%20self-paced%20learning%20manner.%20Given%20that%2C%20in%20general%2C%20a%20model%27s%0Aperformance%20gradually%20improves%20during%20training%2C%20AdaMix%20is%20equipped%20with%20a%0Aself-paced%20curriculum%20that%2C%20in%20the%20initial%20training%20stage%2C%20provides%20relatively%0Asimple%20perturbed%20samples%20and%20then%20gradually%20increases%20the%20difficulty%20of%0Aperturbed%20images%20by%20adaptively%20controlling%20the%20perturbation%20degree%20based%20on%20the%0Amodel%27s%20learning%20state%20estimated%20by%20a%20self-paced%20regularize.%20We%20develop%20three%0Aframeworks%20with%20our%20AdaMix%2C%20i.e.%2C%20AdaMix-ST%2C%20AdaMix-MT%2C%20and%20AdaMix-CT%2C%20for%0Asemi-supervised%20medical%20image%20segmentation.%20Extensive%20experiments%20on%20three%0Apublic%20datasets%2C%20including%20both%202D%20and%203D%20modalities%2C%20show%20that%20the%20proposed%0Aframeworks%20are%20capable%20of%20achieving%20superior%20performance.%20For%20example%2C%20compared%0Awith%20the%20state-of-the-art%2C%20AdaMix-CT%20achieves%20relative%20improvements%20of%202.62%25%20in%0ADice%20and%2048.25%25%20in%20average%20surface%20distance%20on%20the%20ACDC%20dataset%20with%2010%25%0Alabeled%20data.%20The%20results%20demonstrate%20that%20mix-up%20operations%20with%20dynamically%0Aadjusted%20perturbation%20strength%20based%20on%20the%20segmentation%20model%27s%20state%20can%0Asignificantly%20enhance%20the%20effectiveness%20of%20consistency%20regularization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21586v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Mix%2520for%2520Semi-Supervised%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DZhiqiang%2520Shen%2520and%2520Peng%2520Cao%2520and%2520Junming%2520Su%2520and%2520Jinzhu%2520Yang%2520and%2520Osmar%2520R.%2520Zaiane%26entry.1292438233%3D%2520%2520Mix-up%2520is%2520a%2520key%2520technique%2520for%2520consistency%2520regularization-based%250Asemi-supervised%2520learning%2520methods%252C%2520generating%2520strong-perturbed%2520samples%2520for%250Astrong-weak%2520pseudo-supervision.%2520Existing%2520mix-up%2520operations%2520are%2520performed%2520either%250Arandomly%2520or%2520with%2520predefined%2520rules%252C%2520such%2520as%2520replacing%2520low-confidence%2520patches%250Awith%2520high-confidence%2520ones.%2520The%2520former%2520lacks%2520control%2520over%2520the%2520perturbation%250Adegree%252C%2520leading%2520to%2520overfitting%2520on%2520randomly%2520perturbed%2520samples%252C%2520while%2520the%2520latter%250Atends%2520to%2520generate%2520images%2520with%2520trivial%2520perturbations%252C%2520both%2520of%2520which%2520limit%2520the%250Aeffectiveness%2520of%2520consistency%2520learning.%2520This%2520paper%2520aims%2520to%2520answer%2520the%2520following%250Aquestion%253A%2520How%2520can%2520image%2520mix-up%2520perturbation%2520be%2520adaptively%2520performed%2520during%250Atraining%253F%2520To%2520this%2520end%252C%2520we%2520propose%2520an%2520Adaptive%2520Mix%2520algorithm%2520%2528AdaMix%2529%2520for%2520image%250Amix-up%2520in%2520a%2520self-paced%2520learning%2520manner.%2520Given%2520that%252C%2520in%2520general%252C%2520a%2520model%2527s%250Aperformance%2520gradually%2520improves%2520during%2520training%252C%2520AdaMix%2520is%2520equipped%2520with%2520a%250Aself-paced%2520curriculum%2520that%252C%2520in%2520the%2520initial%2520training%2520stage%252C%2520provides%2520relatively%250Asimple%2520perturbed%2520samples%2520and%2520then%2520gradually%2520increases%2520the%2520difficulty%2520of%250Aperturbed%2520images%2520by%2520adaptively%2520controlling%2520the%2520perturbation%2520degree%2520based%2520on%2520the%250Amodel%2527s%2520learning%2520state%2520estimated%2520by%2520a%2520self-paced%2520regularize.%2520We%2520develop%2520three%250Aframeworks%2520with%2520our%2520AdaMix%252C%2520i.e.%252C%2520AdaMix-ST%252C%2520AdaMix-MT%252C%2520and%2520AdaMix-CT%252C%2520for%250Asemi-supervised%2520medical%2520image%2520segmentation.%2520Extensive%2520experiments%2520on%2520three%250Apublic%2520datasets%252C%2520including%2520both%25202D%2520and%25203D%2520modalities%252C%2520show%2520that%2520the%2520proposed%250Aframeworks%2520are%2520capable%2520of%2520achieving%2520superior%2520performance.%2520For%2520example%252C%2520compared%250Awith%2520the%2520state-of-the-art%252C%2520AdaMix-CT%2520achieves%2520relative%2520improvements%2520of%25202.62%2525%2520in%250ADice%2520and%252048.25%2525%2520in%2520average%2520surface%2520distance%2520on%2520the%2520ACDC%2520dataset%2520with%252010%2525%250Alabeled%2520data.%2520The%2520results%2520demonstrate%2520that%2520mix-up%2520operations%2520with%2520dynamically%250Aadjusted%2520perturbation%2520strength%2520based%2520on%2520the%2520segmentation%2520model%2527s%2520state%2520can%250Asignificantly%2520enhance%2520the%2520effectiveness%2520of%2520consistency%2520regularization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21586v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Mix%20for%20Semi-Supervised%20Medical%20Image%20Segmentation&entry.906535625=Zhiqiang%20Shen%20and%20Peng%20Cao%20and%20Junming%20Su%20and%20Jinzhu%20Yang%20and%20Osmar%20R.%20Zaiane&entry.1292438233=%20%20Mix-up%20is%20a%20key%20technique%20for%20consistency%20regularization-based%0Asemi-supervised%20learning%20methods%2C%20generating%20strong-perturbed%20samples%20for%0Astrong-weak%20pseudo-supervision.%20Existing%20mix-up%20operations%20are%20performed%20either%0Arandomly%20or%20with%20predefined%20rules%2C%20such%20as%20replacing%20low-confidence%20patches%0Awith%20high-confidence%20ones.%20The%20former%20lacks%20control%20over%20the%20perturbation%0Adegree%2C%20leading%20to%20overfitting%20on%20randomly%20perturbed%20samples%2C%20while%20the%20latter%0Atends%20to%20generate%20images%20with%20trivial%20perturbations%2C%20both%20of%20which%20limit%20the%0Aeffectiveness%20of%20consistency%20learning.%20This%20paper%20aims%20to%20answer%20the%20following%0Aquestion%3A%20How%20can%20image%20mix-up%20perturbation%20be%20adaptively%20performed%20during%0Atraining%3F%20To%20this%20end%2C%20we%20propose%20an%20Adaptive%20Mix%20algorithm%20%28AdaMix%29%20for%20image%0Amix-up%20in%20a%20self-paced%20learning%20manner.%20Given%20that%2C%20in%20general%2C%20a%20model%27s%0Aperformance%20gradually%20improves%20during%20training%2C%20AdaMix%20is%20equipped%20with%20a%0Aself-paced%20curriculum%20that%2C%20in%20the%20initial%20training%20stage%2C%20provides%20relatively%0Asimple%20perturbed%20samples%20and%20then%20gradually%20increases%20the%20difficulty%20of%0Aperturbed%20images%20by%20adaptively%20controlling%20the%20perturbation%20degree%20based%20on%20the%0Amodel%27s%20learning%20state%20estimated%20by%20a%20self-paced%20regularize.%20We%20develop%20three%0Aframeworks%20with%20our%20AdaMix%2C%20i.e.%2C%20AdaMix-ST%2C%20AdaMix-MT%2C%20and%20AdaMix-CT%2C%20for%0Asemi-supervised%20medical%20image%20segmentation.%20Extensive%20experiments%20on%20three%0Apublic%20datasets%2C%20including%20both%202D%20and%203D%20modalities%2C%20show%20that%20the%20proposed%0Aframeworks%20are%20capable%20of%20achieving%20superior%20performance.%20For%20example%2C%20compared%0Awith%20the%20state-of-the-art%2C%20AdaMix-CT%20achieves%20relative%20improvements%20of%202.62%25%20in%0ADice%20and%2048.25%25%20in%20average%20surface%20distance%20on%20the%20ACDC%20dataset%20with%2010%25%0Alabeled%20data.%20The%20results%20demonstrate%20that%20mix-up%20operations%20with%20dynamically%0Aadjusted%20perturbation%20strength%20based%20on%20the%20segmentation%20model%27s%20state%20can%0Asignificantly%20enhance%20the%20effectiveness%20of%20consistency%20regularization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21586v1&entry.124074799=Read"},
{"title": "Generative Sentiment Analysis via Latent Category Distribution and\n  Constrained Decoding", "author": "Jun Zhou and Dongyang Yu and Kamran Aziz and Fangfang Su and Qing Zhang and Fei Li and Donghong Ji", "abstract": "  Fine-grained sentiment analysis involves extracting and organizing sentiment\nelements from textual data. However, existing approaches often overlook issues\nof category semantic inclusion and overlap, as well as inherent structural\npatterns within the target sequence. This study introduces a generative\nsentiment analysis model. To address the challenges related to category\nsemantic inclusion and overlap, a latent category distribution variable is\nintroduced. By reconstructing the input of a variational autoencoder, the model\nlearns the intensity of the relationship between categories and text, thereby\nimproving sequence generation. Additionally, a trie data structure and\nconstrained decoding strategy are utilized to exploit structural patterns,\nwhich in turn reduces the search space and regularizes the generation process.\nExperimental results on the Restaurant-ACOS and Laptop-ACOS datasets\ndemonstrate a significant performance improvement compared to baseline models.\nAblation experiments further confirm the effectiveness of latent category\ndistribution and constrained decoding strategy.\n", "link": "http://arxiv.org/abs/2407.21560v1", "date": "2024-07-31", "relevancy": 2.0319, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5123}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.508}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5037}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Sentiment%20Analysis%20via%20Latent%20Category%20Distribution%20and%0A%20%20Constrained%20Decoding&body=Title%3A%20Generative%20Sentiment%20Analysis%20via%20Latent%20Category%20Distribution%20and%0A%20%20Constrained%20Decoding%0AAuthor%3A%20Jun%20Zhou%20and%20Dongyang%20Yu%20and%20Kamran%20Aziz%20and%20Fangfang%20Su%20and%20Qing%20Zhang%20and%20Fei%20Li%20and%20Donghong%20Ji%0AAbstract%3A%20%20%20Fine-grained%20sentiment%20analysis%20involves%20extracting%20and%20organizing%20sentiment%0Aelements%20from%20textual%20data.%20However%2C%20existing%20approaches%20often%20overlook%20issues%0Aof%20category%20semantic%20inclusion%20and%20overlap%2C%20as%20well%20as%20inherent%20structural%0Apatterns%20within%20the%20target%20sequence.%20This%20study%20introduces%20a%20generative%0Asentiment%20analysis%20model.%20To%20address%20the%20challenges%20related%20to%20category%0Asemantic%20inclusion%20and%20overlap%2C%20a%20latent%20category%20distribution%20variable%20is%0Aintroduced.%20By%20reconstructing%20the%20input%20of%20a%20variational%20autoencoder%2C%20the%20model%0Alearns%20the%20intensity%20of%20the%20relationship%20between%20categories%20and%20text%2C%20thereby%0Aimproving%20sequence%20generation.%20Additionally%2C%20a%20trie%20data%20structure%20and%0Aconstrained%20decoding%20strategy%20are%20utilized%20to%20exploit%20structural%20patterns%2C%0Awhich%20in%20turn%20reduces%20the%20search%20space%20and%20regularizes%20the%20generation%20process.%0AExperimental%20results%20on%20the%20Restaurant-ACOS%20and%20Laptop-ACOS%20datasets%0Ademonstrate%20a%20significant%20performance%20improvement%20compared%20to%20baseline%20models.%0AAblation%20experiments%20further%20confirm%20the%20effectiveness%20of%20latent%20category%0Adistribution%20and%20constrained%20decoding%20strategy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21560v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Sentiment%2520Analysis%2520via%2520Latent%2520Category%2520Distribution%2520and%250A%2520%2520Constrained%2520Decoding%26entry.906535625%3DJun%2520Zhou%2520and%2520Dongyang%2520Yu%2520and%2520Kamran%2520Aziz%2520and%2520Fangfang%2520Su%2520and%2520Qing%2520Zhang%2520and%2520Fei%2520Li%2520and%2520Donghong%2520Ji%26entry.1292438233%3D%2520%2520Fine-grained%2520sentiment%2520analysis%2520involves%2520extracting%2520and%2520organizing%2520sentiment%250Aelements%2520from%2520textual%2520data.%2520However%252C%2520existing%2520approaches%2520often%2520overlook%2520issues%250Aof%2520category%2520semantic%2520inclusion%2520and%2520overlap%252C%2520as%2520well%2520as%2520inherent%2520structural%250Apatterns%2520within%2520the%2520target%2520sequence.%2520This%2520study%2520introduces%2520a%2520generative%250Asentiment%2520analysis%2520model.%2520To%2520address%2520the%2520challenges%2520related%2520to%2520category%250Asemantic%2520inclusion%2520and%2520overlap%252C%2520a%2520latent%2520category%2520distribution%2520variable%2520is%250Aintroduced.%2520By%2520reconstructing%2520the%2520input%2520of%2520a%2520variational%2520autoencoder%252C%2520the%2520model%250Alearns%2520the%2520intensity%2520of%2520the%2520relationship%2520between%2520categories%2520and%2520text%252C%2520thereby%250Aimproving%2520sequence%2520generation.%2520Additionally%252C%2520a%2520trie%2520data%2520structure%2520and%250Aconstrained%2520decoding%2520strategy%2520are%2520utilized%2520to%2520exploit%2520structural%2520patterns%252C%250Awhich%2520in%2520turn%2520reduces%2520the%2520search%2520space%2520and%2520regularizes%2520the%2520generation%2520process.%250AExperimental%2520results%2520on%2520the%2520Restaurant-ACOS%2520and%2520Laptop-ACOS%2520datasets%250Ademonstrate%2520a%2520significant%2520performance%2520improvement%2520compared%2520to%2520baseline%2520models.%250AAblation%2520experiments%2520further%2520confirm%2520the%2520effectiveness%2520of%2520latent%2520category%250Adistribution%2520and%2520constrained%2520decoding%2520strategy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21560v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Sentiment%20Analysis%20via%20Latent%20Category%20Distribution%20and%0A%20%20Constrained%20Decoding&entry.906535625=Jun%20Zhou%20and%20Dongyang%20Yu%20and%20Kamran%20Aziz%20and%20Fangfang%20Su%20and%20Qing%20Zhang%20and%20Fei%20Li%20and%20Donghong%20Ji&entry.1292438233=%20%20Fine-grained%20sentiment%20analysis%20involves%20extracting%20and%20organizing%20sentiment%0Aelements%20from%20textual%20data.%20However%2C%20existing%20approaches%20often%20overlook%20issues%0Aof%20category%20semantic%20inclusion%20and%20overlap%2C%20as%20well%20as%20inherent%20structural%0Apatterns%20within%20the%20target%20sequence.%20This%20study%20introduces%20a%20generative%0Asentiment%20analysis%20model.%20To%20address%20the%20challenges%20related%20to%20category%0Asemantic%20inclusion%20and%20overlap%2C%20a%20latent%20category%20distribution%20variable%20is%0Aintroduced.%20By%20reconstructing%20the%20input%20of%20a%20variational%20autoencoder%2C%20the%20model%0Alearns%20the%20intensity%20of%20the%20relationship%20between%20categories%20and%20text%2C%20thereby%0Aimproving%20sequence%20generation.%20Additionally%2C%20a%20trie%20data%20structure%20and%0Aconstrained%20decoding%20strategy%20are%20utilized%20to%20exploit%20structural%20patterns%2C%0Awhich%20in%20turn%20reduces%20the%20search%20space%20and%20regularizes%20the%20generation%20process.%0AExperimental%20results%20on%20the%20Restaurant-ACOS%20and%20Laptop-ACOS%20datasets%0Ademonstrate%20a%20significant%20performance%20improvement%20compared%20to%20baseline%20models.%0AAblation%20experiments%20further%20confirm%20the%20effectiveness%20of%20latent%20category%0Adistribution%20and%20constrained%20decoding%20strategy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21560v1&entry.124074799=Read"},
{"title": "A Performance Study of LLM-Generated Code on Leetcode", "author": "Tristan Coignion and Cl\u00e9ment Quinton and Romain Rouvoy", "abstract": "  This study evaluates the efficiency of code generation by Large Language\nModels (LLMs) and measures their performance against human-crafted solutions\nusing a dataset from Leetcode. We compare 18 LLMs, considering factors such as\nmodel temperature and success rate, and their impact on code performance. This\nresearch introduces a novel method for measuring and comparing the speed of\nLLM-generated code, revealing that LLMs produce code with comparable\nperformance, irrespective of the adopted LLM. We also find that LLMs are\ncapable of generating code that is, on average, more efficient than the code\nwritten by humans. The paper further discusses the use of Leetcode as a\nbenchmarking dataset, the limitations imposed by potential data contamination,\nand the platform's measurement reliability. We believe that our findings\ncontribute to a better understanding of LLM capabilities in code generation and\nset the stage for future optimizations in the field.\n", "link": "http://arxiv.org/abs/2407.21579v1", "date": "2024-07-31", "relevancy": 2.0313, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4156}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4029}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4002}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Performance%20Study%20of%20LLM-Generated%20Code%20on%20Leetcode&body=Title%3A%20A%20Performance%20Study%20of%20LLM-Generated%20Code%20on%20Leetcode%0AAuthor%3A%20Tristan%20Coignion%20and%20Cl%C3%A9ment%20Quinton%20and%20Romain%20Rouvoy%0AAbstract%3A%20%20%20This%20study%20evaluates%20the%20efficiency%20of%20code%20generation%20by%20Large%20Language%0AModels%20%28LLMs%29%20and%20measures%20their%20performance%20against%20human-crafted%20solutions%0Ausing%20a%20dataset%20from%20Leetcode.%20We%20compare%2018%20LLMs%2C%20considering%20factors%20such%20as%0Amodel%20temperature%20and%20success%20rate%2C%20and%20their%20impact%20on%20code%20performance.%20This%0Aresearch%20introduces%20a%20novel%20method%20for%20measuring%20and%20comparing%20the%20speed%20of%0ALLM-generated%20code%2C%20revealing%20that%20LLMs%20produce%20code%20with%20comparable%0Aperformance%2C%20irrespective%20of%20the%20adopted%20LLM.%20We%20also%20find%20that%20LLMs%20are%0Acapable%20of%20generating%20code%20that%20is%2C%20on%20average%2C%20more%20efficient%20than%20the%20code%0Awritten%20by%20humans.%20The%20paper%20further%20discusses%20the%20use%20of%20Leetcode%20as%20a%0Abenchmarking%20dataset%2C%20the%20limitations%20imposed%20by%20potential%20data%20contamination%2C%0Aand%20the%20platform%27s%20measurement%20reliability.%20We%20believe%20that%20our%20findings%0Acontribute%20to%20a%20better%20understanding%20of%20LLM%20capabilities%20in%20code%20generation%20and%0Aset%20the%20stage%20for%20future%20optimizations%20in%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21579v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Performance%2520Study%2520of%2520LLM-Generated%2520Code%2520on%2520Leetcode%26entry.906535625%3DTristan%2520Coignion%2520and%2520Cl%25C3%25A9ment%2520Quinton%2520and%2520Romain%2520Rouvoy%26entry.1292438233%3D%2520%2520This%2520study%2520evaluates%2520the%2520efficiency%2520of%2520code%2520generation%2520by%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520and%2520measures%2520their%2520performance%2520against%2520human-crafted%2520solutions%250Ausing%2520a%2520dataset%2520from%2520Leetcode.%2520We%2520compare%252018%2520LLMs%252C%2520considering%2520factors%2520such%2520as%250Amodel%2520temperature%2520and%2520success%2520rate%252C%2520and%2520their%2520impact%2520on%2520code%2520performance.%2520This%250Aresearch%2520introduces%2520a%2520novel%2520method%2520for%2520measuring%2520and%2520comparing%2520the%2520speed%2520of%250ALLM-generated%2520code%252C%2520revealing%2520that%2520LLMs%2520produce%2520code%2520with%2520comparable%250Aperformance%252C%2520irrespective%2520of%2520the%2520adopted%2520LLM.%2520We%2520also%2520find%2520that%2520LLMs%2520are%250Acapable%2520of%2520generating%2520code%2520that%2520is%252C%2520on%2520average%252C%2520more%2520efficient%2520than%2520the%2520code%250Awritten%2520by%2520humans.%2520The%2520paper%2520further%2520discusses%2520the%2520use%2520of%2520Leetcode%2520as%2520a%250Abenchmarking%2520dataset%252C%2520the%2520limitations%2520imposed%2520by%2520potential%2520data%2520contamination%252C%250Aand%2520the%2520platform%2527s%2520measurement%2520reliability.%2520We%2520believe%2520that%2520our%2520findings%250Acontribute%2520to%2520a%2520better%2520understanding%2520of%2520LLM%2520capabilities%2520in%2520code%2520generation%2520and%250Aset%2520the%2520stage%2520for%2520future%2520optimizations%2520in%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21579v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Performance%20Study%20of%20LLM-Generated%20Code%20on%20Leetcode&entry.906535625=Tristan%20Coignion%20and%20Cl%C3%A9ment%20Quinton%20and%20Romain%20Rouvoy&entry.1292438233=%20%20This%20study%20evaluates%20the%20efficiency%20of%20code%20generation%20by%20Large%20Language%0AModels%20%28LLMs%29%20and%20measures%20their%20performance%20against%20human-crafted%20solutions%0Ausing%20a%20dataset%20from%20Leetcode.%20We%20compare%2018%20LLMs%2C%20considering%20factors%20such%20as%0Amodel%20temperature%20and%20success%20rate%2C%20and%20their%20impact%20on%20code%20performance.%20This%0Aresearch%20introduces%20a%20novel%20method%20for%20measuring%20and%20comparing%20the%20speed%20of%0ALLM-generated%20code%2C%20revealing%20that%20LLMs%20produce%20code%20with%20comparable%0Aperformance%2C%20irrespective%20of%20the%20adopted%20LLM.%20We%20also%20find%20that%20LLMs%20are%0Acapable%20of%20generating%20code%20that%20is%2C%20on%20average%2C%20more%20efficient%20than%20the%20code%0Awritten%20by%20humans.%20The%20paper%20further%20discusses%20the%20use%20of%20Leetcode%20as%20a%0Abenchmarking%20dataset%2C%20the%20limitations%20imposed%20by%20potential%20data%20contamination%2C%0Aand%20the%20platform%27s%20measurement%20reliability.%20We%20believe%20that%20our%20findings%0Acontribute%20to%20a%20better%20understanding%20of%20LLM%20capabilities%20in%20code%20generation%20and%0Aset%20the%20stage%20for%20future%20optimizations%20in%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21579v1&entry.124074799=Read"},
{"title": "Mitral Regurgitation Recogniton based on Unsupervised\n  Out-of-Distribution Detection with Residual Diffusion Amplification", "author": "Zhe Liu and Xiliang Zhu and Tong Han and Yuhao Huang and Jian Wang and Lian Liu and Fang Wang and Dong Ni and Zhongshan Gou and Xin Yang", "abstract": "  Mitral regurgitation (MR) is a serious heart valve disease. Early and\naccurate diagnosis of MR via ultrasound video is critical for timely clinical\ndecision-making and surgical intervention. However, manual MR diagnosis heavily\nrelies on the operator's experience, which may cause misdiagnosis and\ninter-observer variability. Since MR data is limited and has large intra-class\nvariability, we propose an unsupervised out-of-distribution (OOD) detection\nmethod to identify MR rather than building a deep classifier. To our knowledge,\nwe are the first to explore OOD in MR ultrasound videos. Our method consists of\na feature extractor, a feature reconstruction model, and a residual\naccumulation amplification algorithm. The feature extractor obtains features\nfrom the video clips and feeds them into the feature reconstruction model to\nrestore the original features. The residual accumulation amplification\nalgorithm then iteratively performs noise feature reconstruction, amplifying\nthe reconstructed error of OOD features. This algorithm is straightforward yet\nefficient and can seamlessly integrate as a plug-and-play component in\nreconstruction-based OOD detection methods. We validated the proposed method on\na large ultrasound dataset containing 893 non-MR and 267 MR videos.\nExperimental results show that our OOD detection method can effectively\nidentify MR samples.\n", "link": "http://arxiv.org/abs/2407.21497v1", "date": "2024-07-31", "relevancy": 2.0301, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.555}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5059}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitral%20Regurgitation%20Recogniton%20based%20on%20Unsupervised%0A%20%20Out-of-Distribution%20Detection%20with%20Residual%20Diffusion%20Amplification&body=Title%3A%20Mitral%20Regurgitation%20Recogniton%20based%20on%20Unsupervised%0A%20%20Out-of-Distribution%20Detection%20with%20Residual%20Diffusion%20Amplification%0AAuthor%3A%20Zhe%20Liu%20and%20Xiliang%20Zhu%20and%20Tong%20Han%20and%20Yuhao%20Huang%20and%20Jian%20Wang%20and%20Lian%20Liu%20and%20Fang%20Wang%20and%20Dong%20Ni%20and%20Zhongshan%20Gou%20and%20Xin%20Yang%0AAbstract%3A%20%20%20Mitral%20regurgitation%20%28MR%29%20is%20a%20serious%20heart%20valve%20disease.%20Early%20and%0Aaccurate%20diagnosis%20of%20MR%20via%20ultrasound%20video%20is%20critical%20for%20timely%20clinical%0Adecision-making%20and%20surgical%20intervention.%20However%2C%20manual%20MR%20diagnosis%20heavily%0Arelies%20on%20the%20operator%27s%20experience%2C%20which%20may%20cause%20misdiagnosis%20and%0Ainter-observer%20variability.%20Since%20MR%20data%20is%20limited%20and%20has%20large%20intra-class%0Avariability%2C%20we%20propose%20an%20unsupervised%20out-of-distribution%20%28OOD%29%20detection%0Amethod%20to%20identify%20MR%20rather%20than%20building%20a%20deep%20classifier.%20To%20our%20knowledge%2C%0Awe%20are%20the%20first%20to%20explore%20OOD%20in%20MR%20ultrasound%20videos.%20Our%20method%20consists%20of%0Aa%20feature%20extractor%2C%20a%20feature%20reconstruction%20model%2C%20and%20a%20residual%0Aaccumulation%20amplification%20algorithm.%20The%20feature%20extractor%20obtains%20features%0Afrom%20the%20video%20clips%20and%20feeds%20them%20into%20the%20feature%20reconstruction%20model%20to%0Arestore%20the%20original%20features.%20The%20residual%20accumulation%20amplification%0Aalgorithm%20then%20iteratively%20performs%20noise%20feature%20reconstruction%2C%20amplifying%0Athe%20reconstructed%20error%20of%20OOD%20features.%20This%20algorithm%20is%20straightforward%20yet%0Aefficient%20and%20can%20seamlessly%20integrate%20as%20a%20plug-and-play%20component%20in%0Areconstruction-based%20OOD%20detection%20methods.%20We%20validated%20the%20proposed%20method%20on%0Aa%20large%20ultrasound%20dataset%20containing%20893%20non-MR%20and%20267%20MR%20videos.%0AExperimental%20results%20show%20that%20our%20OOD%20detection%20method%20can%20effectively%0Aidentify%20MR%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21497v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitral%2520Regurgitation%2520Recogniton%2520based%2520on%2520Unsupervised%250A%2520%2520Out-of-Distribution%2520Detection%2520with%2520Residual%2520Diffusion%2520Amplification%26entry.906535625%3DZhe%2520Liu%2520and%2520Xiliang%2520Zhu%2520and%2520Tong%2520Han%2520and%2520Yuhao%2520Huang%2520and%2520Jian%2520Wang%2520and%2520Lian%2520Liu%2520and%2520Fang%2520Wang%2520and%2520Dong%2520Ni%2520and%2520Zhongshan%2520Gou%2520and%2520Xin%2520Yang%26entry.1292438233%3D%2520%2520Mitral%2520regurgitation%2520%2528MR%2529%2520is%2520a%2520serious%2520heart%2520valve%2520disease.%2520Early%2520and%250Aaccurate%2520diagnosis%2520of%2520MR%2520via%2520ultrasound%2520video%2520is%2520critical%2520for%2520timely%2520clinical%250Adecision-making%2520and%2520surgical%2520intervention.%2520However%252C%2520manual%2520MR%2520diagnosis%2520heavily%250Arelies%2520on%2520the%2520operator%2527s%2520experience%252C%2520which%2520may%2520cause%2520misdiagnosis%2520and%250Ainter-observer%2520variability.%2520Since%2520MR%2520data%2520is%2520limited%2520and%2520has%2520large%2520intra-class%250Avariability%252C%2520we%2520propose%2520an%2520unsupervised%2520out-of-distribution%2520%2528OOD%2529%2520detection%250Amethod%2520to%2520identify%2520MR%2520rather%2520than%2520building%2520a%2520deep%2520classifier.%2520To%2520our%2520knowledge%252C%250Awe%2520are%2520the%2520first%2520to%2520explore%2520OOD%2520in%2520MR%2520ultrasound%2520videos.%2520Our%2520method%2520consists%2520of%250Aa%2520feature%2520extractor%252C%2520a%2520feature%2520reconstruction%2520model%252C%2520and%2520a%2520residual%250Aaccumulation%2520amplification%2520algorithm.%2520The%2520feature%2520extractor%2520obtains%2520features%250Afrom%2520the%2520video%2520clips%2520and%2520feeds%2520them%2520into%2520the%2520feature%2520reconstruction%2520model%2520to%250Arestore%2520the%2520original%2520features.%2520The%2520residual%2520accumulation%2520amplification%250Aalgorithm%2520then%2520iteratively%2520performs%2520noise%2520feature%2520reconstruction%252C%2520amplifying%250Athe%2520reconstructed%2520error%2520of%2520OOD%2520features.%2520This%2520algorithm%2520is%2520straightforward%2520yet%250Aefficient%2520and%2520can%2520seamlessly%2520integrate%2520as%2520a%2520plug-and-play%2520component%2520in%250Areconstruction-based%2520OOD%2520detection%2520methods.%2520We%2520validated%2520the%2520proposed%2520method%2520on%250Aa%2520large%2520ultrasound%2520dataset%2520containing%2520893%2520non-MR%2520and%2520267%2520MR%2520videos.%250AExperimental%2520results%2520show%2520that%2520our%2520OOD%2520detection%2520method%2520can%2520effectively%250Aidentify%2520MR%2520samples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21497v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitral%20Regurgitation%20Recogniton%20based%20on%20Unsupervised%0A%20%20Out-of-Distribution%20Detection%20with%20Residual%20Diffusion%20Amplification&entry.906535625=Zhe%20Liu%20and%20Xiliang%20Zhu%20and%20Tong%20Han%20and%20Yuhao%20Huang%20and%20Jian%20Wang%20and%20Lian%20Liu%20and%20Fang%20Wang%20and%20Dong%20Ni%20and%20Zhongshan%20Gou%20and%20Xin%20Yang&entry.1292438233=%20%20Mitral%20regurgitation%20%28MR%29%20is%20a%20serious%20heart%20valve%20disease.%20Early%20and%0Aaccurate%20diagnosis%20of%20MR%20via%20ultrasound%20video%20is%20critical%20for%20timely%20clinical%0Adecision-making%20and%20surgical%20intervention.%20However%2C%20manual%20MR%20diagnosis%20heavily%0Arelies%20on%20the%20operator%27s%20experience%2C%20which%20may%20cause%20misdiagnosis%20and%0Ainter-observer%20variability.%20Since%20MR%20data%20is%20limited%20and%20has%20large%20intra-class%0Avariability%2C%20we%20propose%20an%20unsupervised%20out-of-distribution%20%28OOD%29%20detection%0Amethod%20to%20identify%20MR%20rather%20than%20building%20a%20deep%20classifier.%20To%20our%20knowledge%2C%0Awe%20are%20the%20first%20to%20explore%20OOD%20in%20MR%20ultrasound%20videos.%20Our%20method%20consists%20of%0Aa%20feature%20extractor%2C%20a%20feature%20reconstruction%20model%2C%20and%20a%20residual%0Aaccumulation%20amplification%20algorithm.%20The%20feature%20extractor%20obtains%20features%0Afrom%20the%20video%20clips%20and%20feeds%20them%20into%20the%20feature%20reconstruction%20model%20to%0Arestore%20the%20original%20features.%20The%20residual%20accumulation%20amplification%0Aalgorithm%20then%20iteratively%20performs%20noise%20feature%20reconstruction%2C%20amplifying%0Athe%20reconstructed%20error%20of%20OOD%20features.%20This%20algorithm%20is%20straightforward%20yet%0Aefficient%20and%20can%20seamlessly%20integrate%20as%20a%20plug-and-play%20component%20in%0Areconstruction-based%20OOD%20detection%20methods.%20We%20validated%20the%20proposed%20method%20on%0Aa%20large%20ultrasound%20dataset%20containing%20893%20non-MR%20and%20267%20MR%20videos.%0AExperimental%20results%20show%20that%20our%20OOD%20detection%20method%20can%20effectively%0Aidentify%20MR%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21497v1&entry.124074799=Read"},
{"title": "Quaternion-based Adaptive Backstepping Fast Terminal Sliding Mode\n  Control for Quadrotor UAVs with Finite Time Convergence", "author": "Arezo Shevidi and Hashim A. Hashim", "abstract": "  This paper proposes a novel quaternion-based approach for tracking the\ntranslation (position and linear velocity) and rotation (attitude and angular\nvelocity) trajectories of underactuated Unmanned Aerial Vehicles (UAVs).\nQuadrotor UAVs are challenging regarding accuracy, singularity, and\nuncertainties issues. Controllers designed based on unit-quaternion are\nsingularity-free for attitude representation compared to other methods (e.g.,\nEuler angles), which fail to represent the vehicle's attitude at multiple\norientations. Quaternion-based Adaptive Backstepping Control (ABC) and Adaptive\nFast Terminal Sliding Mode Control (AFTSMC) are proposed to address a set of\nchallenging problems. A quaternion-based ABC, a superior recursive approach, is\nproposed to generate the necessary thrust handling unknown uncertainties and\nUAV translation trajectory tracking. Next, a quaternion-based AFTSMC is\ndeveloped to overcome parametric uncertainties, avoid singularity, and ensure\nfast convergence in a finite time. Moreover, the proposed AFTSMC is able to\nsignificantly minimize control signal chattering, which is the main reason for\nactuator failure and provide smooth and accurate rotational control input. To\nensure the robustness of the proposed approach, the designed control algorithms\nhave been validated considering unknown time-variant parametric uncertainties\nand significant initialization errors. The proposed techniques has been\ncompared to state-of-the-art control technique. Keywords: Adaptive Backstepping\nControl (ABC), Adaptive Fast Terminal Sliding Mode Control (AFTSMC),\nUnit-quaternion, Unmanned Aerial Vehicles, Singularity Free, Pose Control\n", "link": "http://arxiv.org/abs/2407.01275v2", "date": "2024-07-31", "relevancy": 2.0267, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5182}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5008}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quaternion-based%20Adaptive%20Backstepping%20Fast%20Terminal%20Sliding%20Mode%0A%20%20Control%20for%20Quadrotor%20UAVs%20with%20Finite%20Time%20Convergence&body=Title%3A%20Quaternion-based%20Adaptive%20Backstepping%20Fast%20Terminal%20Sliding%20Mode%0A%20%20Control%20for%20Quadrotor%20UAVs%20with%20Finite%20Time%20Convergence%0AAuthor%3A%20Arezo%20Shevidi%20and%20Hashim%20A.%20Hashim%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20novel%20quaternion-based%20approach%20for%20tracking%20the%0Atranslation%20%28position%20and%20linear%20velocity%29%20and%20rotation%20%28attitude%20and%20angular%0Avelocity%29%20trajectories%20of%20underactuated%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29.%0AQuadrotor%20UAVs%20are%20challenging%20regarding%20accuracy%2C%20singularity%2C%20and%0Auncertainties%20issues.%20Controllers%20designed%20based%20on%20unit-quaternion%20are%0Asingularity-free%20for%20attitude%20representation%20compared%20to%20other%20methods%20%28e.g.%2C%0AEuler%20angles%29%2C%20which%20fail%20to%20represent%20the%20vehicle%27s%20attitude%20at%20multiple%0Aorientations.%20Quaternion-based%20Adaptive%20Backstepping%20Control%20%28ABC%29%20and%20Adaptive%0AFast%20Terminal%20Sliding%20Mode%20Control%20%28AFTSMC%29%20are%20proposed%20to%20address%20a%20set%20of%0Achallenging%20problems.%20A%20quaternion-based%20ABC%2C%20a%20superior%20recursive%20approach%2C%20is%0Aproposed%20to%20generate%20the%20necessary%20thrust%20handling%20unknown%20uncertainties%20and%0AUAV%20translation%20trajectory%20tracking.%20Next%2C%20a%20quaternion-based%20AFTSMC%20is%0Adeveloped%20to%20overcome%20parametric%20uncertainties%2C%20avoid%20singularity%2C%20and%20ensure%0Afast%20convergence%20in%20a%20finite%20time.%20Moreover%2C%20the%20proposed%20AFTSMC%20is%20able%20to%0Asignificantly%20minimize%20control%20signal%20chattering%2C%20which%20is%20the%20main%20reason%20for%0Aactuator%20failure%20and%20provide%20smooth%20and%20accurate%20rotational%20control%20input.%20To%0Aensure%20the%20robustness%20of%20the%20proposed%20approach%2C%20the%20designed%20control%20algorithms%0Ahave%20been%20validated%20considering%20unknown%20time-variant%20parametric%20uncertainties%0Aand%20significant%20initialization%20errors.%20The%20proposed%20techniques%20has%20been%0Acompared%20to%20state-of-the-art%20control%20technique.%20Keywords%3A%20Adaptive%20Backstepping%0AControl%20%28ABC%29%2C%20Adaptive%20Fast%20Terminal%20Sliding%20Mode%20Control%20%28AFTSMC%29%2C%0AUnit-quaternion%2C%20Unmanned%20Aerial%20Vehicles%2C%20Singularity%20Free%2C%20Pose%20Control%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01275v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuaternion-based%2520Adaptive%2520Backstepping%2520Fast%2520Terminal%2520Sliding%2520Mode%250A%2520%2520Control%2520for%2520Quadrotor%2520UAVs%2520with%2520Finite%2520Time%2520Convergence%26entry.906535625%3DArezo%2520Shevidi%2520and%2520Hashim%2520A.%2520Hashim%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520novel%2520quaternion-based%2520approach%2520for%2520tracking%2520the%250Atranslation%2520%2528position%2520and%2520linear%2520velocity%2529%2520and%2520rotation%2520%2528attitude%2520and%2520angular%250Avelocity%2529%2520trajectories%2520of%2520underactuated%2520Unmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529.%250AQuadrotor%2520UAVs%2520are%2520challenging%2520regarding%2520accuracy%252C%2520singularity%252C%2520and%250Auncertainties%2520issues.%2520Controllers%2520designed%2520based%2520on%2520unit-quaternion%2520are%250Asingularity-free%2520for%2520attitude%2520representation%2520compared%2520to%2520other%2520methods%2520%2528e.g.%252C%250AEuler%2520angles%2529%252C%2520which%2520fail%2520to%2520represent%2520the%2520vehicle%2527s%2520attitude%2520at%2520multiple%250Aorientations.%2520Quaternion-based%2520Adaptive%2520Backstepping%2520Control%2520%2528ABC%2529%2520and%2520Adaptive%250AFast%2520Terminal%2520Sliding%2520Mode%2520Control%2520%2528AFTSMC%2529%2520are%2520proposed%2520to%2520address%2520a%2520set%2520of%250Achallenging%2520problems.%2520A%2520quaternion-based%2520ABC%252C%2520a%2520superior%2520recursive%2520approach%252C%2520is%250Aproposed%2520to%2520generate%2520the%2520necessary%2520thrust%2520handling%2520unknown%2520uncertainties%2520and%250AUAV%2520translation%2520trajectory%2520tracking.%2520Next%252C%2520a%2520quaternion-based%2520AFTSMC%2520is%250Adeveloped%2520to%2520overcome%2520parametric%2520uncertainties%252C%2520avoid%2520singularity%252C%2520and%2520ensure%250Afast%2520convergence%2520in%2520a%2520finite%2520time.%2520Moreover%252C%2520the%2520proposed%2520AFTSMC%2520is%2520able%2520to%250Asignificantly%2520minimize%2520control%2520signal%2520chattering%252C%2520which%2520is%2520the%2520main%2520reason%2520for%250Aactuator%2520failure%2520and%2520provide%2520smooth%2520and%2520accurate%2520rotational%2520control%2520input.%2520To%250Aensure%2520the%2520robustness%2520of%2520the%2520proposed%2520approach%252C%2520the%2520designed%2520control%2520algorithms%250Ahave%2520been%2520validated%2520considering%2520unknown%2520time-variant%2520parametric%2520uncertainties%250Aand%2520significant%2520initialization%2520errors.%2520The%2520proposed%2520techniques%2520has%2520been%250Acompared%2520to%2520state-of-the-art%2520control%2520technique.%2520Keywords%253A%2520Adaptive%2520Backstepping%250AControl%2520%2528ABC%2529%252C%2520Adaptive%2520Fast%2520Terminal%2520Sliding%2520Mode%2520Control%2520%2528AFTSMC%2529%252C%250AUnit-quaternion%252C%2520Unmanned%2520Aerial%2520Vehicles%252C%2520Singularity%2520Free%252C%2520Pose%2520Control%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01275v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quaternion-based%20Adaptive%20Backstepping%20Fast%20Terminal%20Sliding%20Mode%0A%20%20Control%20for%20Quadrotor%20UAVs%20with%20Finite%20Time%20Convergence&entry.906535625=Arezo%20Shevidi%20and%20Hashim%20A.%20Hashim&entry.1292438233=%20%20This%20paper%20proposes%20a%20novel%20quaternion-based%20approach%20for%20tracking%20the%0Atranslation%20%28position%20and%20linear%20velocity%29%20and%20rotation%20%28attitude%20and%20angular%0Avelocity%29%20trajectories%20of%20underactuated%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29.%0AQuadrotor%20UAVs%20are%20challenging%20regarding%20accuracy%2C%20singularity%2C%20and%0Auncertainties%20issues.%20Controllers%20designed%20based%20on%20unit-quaternion%20are%0Asingularity-free%20for%20attitude%20representation%20compared%20to%20other%20methods%20%28e.g.%2C%0AEuler%20angles%29%2C%20which%20fail%20to%20represent%20the%20vehicle%27s%20attitude%20at%20multiple%0Aorientations.%20Quaternion-based%20Adaptive%20Backstepping%20Control%20%28ABC%29%20and%20Adaptive%0AFast%20Terminal%20Sliding%20Mode%20Control%20%28AFTSMC%29%20are%20proposed%20to%20address%20a%20set%20of%0Achallenging%20problems.%20A%20quaternion-based%20ABC%2C%20a%20superior%20recursive%20approach%2C%20is%0Aproposed%20to%20generate%20the%20necessary%20thrust%20handling%20unknown%20uncertainties%20and%0AUAV%20translation%20trajectory%20tracking.%20Next%2C%20a%20quaternion-based%20AFTSMC%20is%0Adeveloped%20to%20overcome%20parametric%20uncertainties%2C%20avoid%20singularity%2C%20and%20ensure%0Afast%20convergence%20in%20a%20finite%20time.%20Moreover%2C%20the%20proposed%20AFTSMC%20is%20able%20to%0Asignificantly%20minimize%20control%20signal%20chattering%2C%20which%20is%20the%20main%20reason%20for%0Aactuator%20failure%20and%20provide%20smooth%20and%20accurate%20rotational%20control%20input.%20To%0Aensure%20the%20robustness%20of%20the%20proposed%20approach%2C%20the%20designed%20control%20algorithms%0Ahave%20been%20validated%20considering%20unknown%20time-variant%20parametric%20uncertainties%0Aand%20significant%20initialization%20errors.%20The%20proposed%20techniques%20has%20been%0Acompared%20to%20state-of-the-art%20control%20technique.%20Keywords%3A%20Adaptive%20Backstepping%0AControl%20%28ABC%29%2C%20Adaptive%20Fast%20Terminal%20Sliding%20Mode%20Control%20%28AFTSMC%29%2C%0AUnit-quaternion%2C%20Unmanned%20Aerial%20Vehicles%2C%20Singularity%20Free%2C%20Pose%20Control%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01275v2&entry.124074799=Read"},
{"title": "Posterior-Variance-Based Error Quantification for Inverse Problems in\n  Imaging", "author": "Dominik Narnhofer and Andreas Habring and Martin Holler and Thomas Pock", "abstract": "  In this work, a method for obtaining pixel-wise error bounds in Bayesian\nregularization of inverse imaging problems is introduced. The proposed method\nemploys estimates of the posterior variance together with techniques from\nconformal prediction in order to obtain coverage guarantees for the error\nbounds, without making any assumption on the underlying data distribution. It\nis generally applicable to Bayesian regularization approaches, independent,\ne.g., of the concrete choice of the prior. Furthermore, the coverage guarantees\ncan also be obtained in case only approximate sampling from the posterior is\npossible. With this in particular, the proposed framework is able to\nincorporate any learned prior in a black-box manner. Guaranteed coverage\nwithout assumptions on the underlying distributions is only achievable since\nthe magnitude of the error bounds is, in general, unknown in advance.\nNevertheless, experiments with multiple regularization approaches presented in\nthe paper confirm that in practice, the obtained error bounds are rather tight.\nFor realizing the numerical experiments, also a novel primal-dual Langevin\nalgorithm for sampling from non-smooth distributions is introduced in this\nwork.\n", "link": "http://arxiv.org/abs/2212.12499v2", "date": "2024-07-31", "relevancy": 2.0091, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5143}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5075}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Posterior-Variance-Based%20Error%20Quantification%20for%20Inverse%20Problems%20in%0A%20%20Imaging&body=Title%3A%20Posterior-Variance-Based%20Error%20Quantification%20for%20Inverse%20Problems%20in%0A%20%20Imaging%0AAuthor%3A%20Dominik%20Narnhofer%20and%20Andreas%20Habring%20and%20Martin%20Holler%20and%20Thomas%20Pock%0AAbstract%3A%20%20%20In%20this%20work%2C%20a%20method%20for%20obtaining%20pixel-wise%20error%20bounds%20in%20Bayesian%0Aregularization%20of%20inverse%20imaging%20problems%20is%20introduced.%20The%20proposed%20method%0Aemploys%20estimates%20of%20the%20posterior%20variance%20together%20with%20techniques%20from%0Aconformal%20prediction%20in%20order%20to%20obtain%20coverage%20guarantees%20for%20the%20error%0Abounds%2C%20without%20making%20any%20assumption%20on%20the%20underlying%20data%20distribution.%20It%0Ais%20generally%20applicable%20to%20Bayesian%20regularization%20approaches%2C%20independent%2C%0Ae.g.%2C%20of%20the%20concrete%20choice%20of%20the%20prior.%20Furthermore%2C%20the%20coverage%20guarantees%0Acan%20also%20be%20obtained%20in%20case%20only%20approximate%20sampling%20from%20the%20posterior%20is%0Apossible.%20With%20this%20in%20particular%2C%20the%20proposed%20framework%20is%20able%20to%0Aincorporate%20any%20learned%20prior%20in%20a%20black-box%20manner.%20Guaranteed%20coverage%0Awithout%20assumptions%20on%20the%20underlying%20distributions%20is%20only%20achievable%20since%0Athe%20magnitude%20of%20the%20error%20bounds%20is%2C%20in%20general%2C%20unknown%20in%20advance.%0ANevertheless%2C%20experiments%20with%20multiple%20regularization%20approaches%20presented%20in%0Athe%20paper%20confirm%20that%20in%20practice%2C%20the%20obtained%20error%20bounds%20are%20rather%20tight.%0AFor%20realizing%20the%20numerical%20experiments%2C%20also%20a%20novel%20primal-dual%20Langevin%0Aalgorithm%20for%20sampling%20from%20non-smooth%20distributions%20is%20introduced%20in%20this%0Awork.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.12499v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPosterior-Variance-Based%2520Error%2520Quantification%2520for%2520Inverse%2520Problems%2520in%250A%2520%2520Imaging%26entry.906535625%3DDominik%2520Narnhofer%2520and%2520Andreas%2520Habring%2520and%2520Martin%2520Holler%2520and%2520Thomas%2520Pock%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520a%2520method%2520for%2520obtaining%2520pixel-wise%2520error%2520bounds%2520in%2520Bayesian%250Aregularization%2520of%2520inverse%2520imaging%2520problems%2520is%2520introduced.%2520The%2520proposed%2520method%250Aemploys%2520estimates%2520of%2520the%2520posterior%2520variance%2520together%2520with%2520techniques%2520from%250Aconformal%2520prediction%2520in%2520order%2520to%2520obtain%2520coverage%2520guarantees%2520for%2520the%2520error%250Abounds%252C%2520without%2520making%2520any%2520assumption%2520on%2520the%2520underlying%2520data%2520distribution.%2520It%250Ais%2520generally%2520applicable%2520to%2520Bayesian%2520regularization%2520approaches%252C%2520independent%252C%250Ae.g.%252C%2520of%2520the%2520concrete%2520choice%2520of%2520the%2520prior.%2520Furthermore%252C%2520the%2520coverage%2520guarantees%250Acan%2520also%2520be%2520obtained%2520in%2520case%2520only%2520approximate%2520sampling%2520from%2520the%2520posterior%2520is%250Apossible.%2520With%2520this%2520in%2520particular%252C%2520the%2520proposed%2520framework%2520is%2520able%2520to%250Aincorporate%2520any%2520learned%2520prior%2520in%2520a%2520black-box%2520manner.%2520Guaranteed%2520coverage%250Awithout%2520assumptions%2520on%2520the%2520underlying%2520distributions%2520is%2520only%2520achievable%2520since%250Athe%2520magnitude%2520of%2520the%2520error%2520bounds%2520is%252C%2520in%2520general%252C%2520unknown%2520in%2520advance.%250ANevertheless%252C%2520experiments%2520with%2520multiple%2520regularization%2520approaches%2520presented%2520in%250Athe%2520paper%2520confirm%2520that%2520in%2520practice%252C%2520the%2520obtained%2520error%2520bounds%2520are%2520rather%2520tight.%250AFor%2520realizing%2520the%2520numerical%2520experiments%252C%2520also%2520a%2520novel%2520primal-dual%2520Langevin%250Aalgorithm%2520for%2520sampling%2520from%2520non-smooth%2520distributions%2520is%2520introduced%2520in%2520this%250Awork.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2212.12499v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Posterior-Variance-Based%20Error%20Quantification%20for%20Inverse%20Problems%20in%0A%20%20Imaging&entry.906535625=Dominik%20Narnhofer%20and%20Andreas%20Habring%20and%20Martin%20Holler%20and%20Thomas%20Pock&entry.1292438233=%20%20In%20this%20work%2C%20a%20method%20for%20obtaining%20pixel-wise%20error%20bounds%20in%20Bayesian%0Aregularization%20of%20inverse%20imaging%20problems%20is%20introduced.%20The%20proposed%20method%0Aemploys%20estimates%20of%20the%20posterior%20variance%20together%20with%20techniques%20from%0Aconformal%20prediction%20in%20order%20to%20obtain%20coverage%20guarantees%20for%20the%20error%0Abounds%2C%20without%20making%20any%20assumption%20on%20the%20underlying%20data%20distribution.%20It%0Ais%20generally%20applicable%20to%20Bayesian%20regularization%20approaches%2C%20independent%2C%0Ae.g.%2C%20of%20the%20concrete%20choice%20of%20the%20prior.%20Furthermore%2C%20the%20coverage%20guarantees%0Acan%20also%20be%20obtained%20in%20case%20only%20approximate%20sampling%20from%20the%20posterior%20is%0Apossible.%20With%20this%20in%20particular%2C%20the%20proposed%20framework%20is%20able%20to%0Aincorporate%20any%20learned%20prior%20in%20a%20black-box%20manner.%20Guaranteed%20coverage%0Awithout%20assumptions%20on%20the%20underlying%20distributions%20is%20only%20achievable%20since%0Athe%20magnitude%20of%20the%20error%20bounds%20is%2C%20in%20general%2C%20unknown%20in%20advance.%0ANevertheless%2C%20experiments%20with%20multiple%20regularization%20approaches%20presented%20in%0Athe%20paper%20confirm%20that%20in%20practice%2C%20the%20obtained%20error%20bounds%20are%20rather%20tight.%0AFor%20realizing%20the%20numerical%20experiments%2C%20also%20a%20novel%20primal-dual%20Langevin%0Aalgorithm%20for%20sampling%20from%20non-smooth%20distributions%20is%20introduced%20in%20this%0Awork.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.12499v2&entry.124074799=Read"},
{"title": "RepGhost: A Hardware-Efficient Ghost Module via Re-parameterization", "author": "Chengpeng Chen and Zichao Guo and Haien Zeng and Pengfei Xiong and Jian Dong", "abstract": "  Feature reuse has been a key technique in light-weight convolutional neural\nnetworks (CNNs) architecture design. Current methods usually utilize a\nconcatenation operator to keep large channel numbers cheaply (thus large\nnetwork capacity) by reusing feature maps from other layers. Although\nconcatenation is parameters- and FLOPs-free, its computational cost on hardware\ndevices is non-negligible. To address this, this paper provides a new\nperspective to realize feature reuse implicitly and more efficiently instead of\nconcatenation. A novel hardware-efficient RepGhost module is proposed for\nimplicit feature reuse via reparameterization, instead of using concatenation\noperator. Based on the RepGhost module, we develop our efficient RepGhost\nbottleneck and RepGhostNet. Experiments on ImageNet and COCO benchmarks\ndemonstrate that our RepGhostNet is much more effective and efficient than\nGhostNet and MobileNetV3 on mobile devices. Specially, our RepGhostNet\nsurpasses GhostNet 0.5x by 2.5% Top-1 accuracy on ImageNet dataset with less\nparameters and comparable latency on an ARM-based mobile device. Code and model\nweights are available at https://github.com/ChengpengChen/RepGhost.\n", "link": "http://arxiv.org/abs/2211.06088v2", "date": "2024-07-31", "relevancy": 2.0084, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5033}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5015}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RepGhost%3A%20A%20Hardware-Efficient%20Ghost%20Module%20via%20Re-parameterization&body=Title%3A%20RepGhost%3A%20A%20Hardware-Efficient%20Ghost%20Module%20via%20Re-parameterization%0AAuthor%3A%20Chengpeng%20Chen%20and%20Zichao%20Guo%20and%20Haien%20Zeng%20and%20Pengfei%20Xiong%20and%20Jian%20Dong%0AAbstract%3A%20%20%20Feature%20reuse%20has%20been%20a%20key%20technique%20in%20light-weight%20convolutional%20neural%0Anetworks%20%28CNNs%29%20architecture%20design.%20Current%20methods%20usually%20utilize%20a%0Aconcatenation%20operator%20to%20keep%20large%20channel%20numbers%20cheaply%20%28thus%20large%0Anetwork%20capacity%29%20by%20reusing%20feature%20maps%20from%20other%20layers.%20Although%0Aconcatenation%20is%20parameters-%20and%20FLOPs-free%2C%20its%20computational%20cost%20on%20hardware%0Adevices%20is%20non-negligible.%20To%20address%20this%2C%20this%20paper%20provides%20a%20new%0Aperspective%20to%20realize%20feature%20reuse%20implicitly%20and%20more%20efficiently%20instead%20of%0Aconcatenation.%20A%20novel%20hardware-efficient%20RepGhost%20module%20is%20proposed%20for%0Aimplicit%20feature%20reuse%20via%20reparameterization%2C%20instead%20of%20using%20concatenation%0Aoperator.%20Based%20on%20the%20RepGhost%20module%2C%20we%20develop%20our%20efficient%20RepGhost%0Abottleneck%20and%20RepGhostNet.%20Experiments%20on%20ImageNet%20and%20COCO%20benchmarks%0Ademonstrate%20that%20our%20RepGhostNet%20is%20much%20more%20effective%20and%20efficient%20than%0AGhostNet%20and%20MobileNetV3%20on%20mobile%20devices.%20Specially%2C%20our%20RepGhostNet%0Asurpasses%20GhostNet%200.5x%20by%202.5%25%20Top-1%20accuracy%20on%20ImageNet%20dataset%20with%20less%0Aparameters%20and%20comparable%20latency%20on%20an%20ARM-based%20mobile%20device.%20Code%20and%20model%0Aweights%20are%20available%20at%20https%3A//github.com/ChengpengChen/RepGhost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.06088v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepGhost%253A%2520A%2520Hardware-Efficient%2520Ghost%2520Module%2520via%2520Re-parameterization%26entry.906535625%3DChengpeng%2520Chen%2520and%2520Zichao%2520Guo%2520and%2520Haien%2520Zeng%2520and%2520Pengfei%2520Xiong%2520and%2520Jian%2520Dong%26entry.1292438233%3D%2520%2520Feature%2520reuse%2520has%2520been%2520a%2520key%2520technique%2520in%2520light-weight%2520convolutional%2520neural%250Anetworks%2520%2528CNNs%2529%2520architecture%2520design.%2520Current%2520methods%2520usually%2520utilize%2520a%250Aconcatenation%2520operator%2520to%2520keep%2520large%2520channel%2520numbers%2520cheaply%2520%2528thus%2520large%250Anetwork%2520capacity%2529%2520by%2520reusing%2520feature%2520maps%2520from%2520other%2520layers.%2520Although%250Aconcatenation%2520is%2520parameters-%2520and%2520FLOPs-free%252C%2520its%2520computational%2520cost%2520on%2520hardware%250Adevices%2520is%2520non-negligible.%2520To%2520address%2520this%252C%2520this%2520paper%2520provides%2520a%2520new%250Aperspective%2520to%2520realize%2520feature%2520reuse%2520implicitly%2520and%2520more%2520efficiently%2520instead%2520of%250Aconcatenation.%2520A%2520novel%2520hardware-efficient%2520RepGhost%2520module%2520is%2520proposed%2520for%250Aimplicit%2520feature%2520reuse%2520via%2520reparameterization%252C%2520instead%2520of%2520using%2520concatenation%250Aoperator.%2520Based%2520on%2520the%2520RepGhost%2520module%252C%2520we%2520develop%2520our%2520efficient%2520RepGhost%250Abottleneck%2520and%2520RepGhostNet.%2520Experiments%2520on%2520ImageNet%2520and%2520COCO%2520benchmarks%250Ademonstrate%2520that%2520our%2520RepGhostNet%2520is%2520much%2520more%2520effective%2520and%2520efficient%2520than%250AGhostNet%2520and%2520MobileNetV3%2520on%2520mobile%2520devices.%2520Specially%252C%2520our%2520RepGhostNet%250Asurpasses%2520GhostNet%25200.5x%2520by%25202.5%2525%2520Top-1%2520accuracy%2520on%2520ImageNet%2520dataset%2520with%2520less%250Aparameters%2520and%2520comparable%2520latency%2520on%2520an%2520ARM-based%2520mobile%2520device.%2520Code%2520and%2520model%250Aweights%2520are%2520available%2520at%2520https%253A//github.com/ChengpengChen/RepGhost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2211.06088v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RepGhost%3A%20A%20Hardware-Efficient%20Ghost%20Module%20via%20Re-parameterization&entry.906535625=Chengpeng%20Chen%20and%20Zichao%20Guo%20and%20Haien%20Zeng%20and%20Pengfei%20Xiong%20and%20Jian%20Dong&entry.1292438233=%20%20Feature%20reuse%20has%20been%20a%20key%20technique%20in%20light-weight%20convolutional%20neural%0Anetworks%20%28CNNs%29%20architecture%20design.%20Current%20methods%20usually%20utilize%20a%0Aconcatenation%20operator%20to%20keep%20large%20channel%20numbers%20cheaply%20%28thus%20large%0Anetwork%20capacity%29%20by%20reusing%20feature%20maps%20from%20other%20layers.%20Although%0Aconcatenation%20is%20parameters-%20and%20FLOPs-free%2C%20its%20computational%20cost%20on%20hardware%0Adevices%20is%20non-negligible.%20To%20address%20this%2C%20this%20paper%20provides%20a%20new%0Aperspective%20to%20realize%20feature%20reuse%20implicitly%20and%20more%20efficiently%20instead%20of%0Aconcatenation.%20A%20novel%20hardware-efficient%20RepGhost%20module%20is%20proposed%20for%0Aimplicit%20feature%20reuse%20via%20reparameterization%2C%20instead%20of%20using%20concatenation%0Aoperator.%20Based%20on%20the%20RepGhost%20module%2C%20we%20develop%20our%20efficient%20RepGhost%0Abottleneck%20and%20RepGhostNet.%20Experiments%20on%20ImageNet%20and%20COCO%20benchmarks%0Ademonstrate%20that%20our%20RepGhostNet%20is%20much%20more%20effective%20and%20efficient%20than%0AGhostNet%20and%20MobileNetV3%20on%20mobile%20devices.%20Specially%2C%20our%20RepGhostNet%0Asurpasses%20GhostNet%200.5x%20by%202.5%25%20Top-1%20accuracy%20on%20ImageNet%20dataset%20with%20less%0Aparameters%20and%20comparable%20latency%20on%20an%20ARM-based%20mobile%20device.%20Code%20and%20model%0Aweights%20are%20available%20at%20https%3A//github.com/ChengpengChen/RepGhost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.06088v2&entry.124074799=Read"},
{"title": "Multi-Site Class-Incremental Learning with Weighted Experts in\n  Echocardiography", "author": "Kit M. Bransby and Woo-jin Cho Kim and Jorge Oliveira and Alex Thorley and Arian Beqiri and Alberto Gomez and Agisilaos Chartsias", "abstract": "  Building an echocardiography view classifier that maintains performance in\nreal-life cases requires diverse multi-site data, and frequent updates with\nnewly available data to mitigate model drift. Simply fine-tuning on new\ndatasets results in \"catastrophic forgetting\", and cannot adapt to variations\nof view labels between sites. Alternatively, collecting all data on a single\nserver and re-training may not be feasible as data sharing agreements may\nrestrict image transfer, or datasets may only become available at different\ntimes. Furthermore, time and cost associated with re-training grows with every\nnew dataset. We propose a class-incremental learning method which learns an\nexpert network for each dataset, and combines all expert networks with a score\nfusion model. The influence of ``unqualified experts'' is minimised by\nweighting each contribution with a learnt in-distribution score. These weights\npromote transparency as the contribution of each expert is known during\ninference. Instead of using the original images, we use learned features from\neach dataset, which are easier to share and raise fewer licensing and privacy\nconcerns. We validate our work on six datasets from multiple sites,\ndemonstrating significant reductions in training time while improving view\nclassification performance.\n", "link": "http://arxiv.org/abs/2407.21577v1", "date": "2024-07-31", "relevancy": 2.0071, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5079}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5008}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Site%20Class-Incremental%20Learning%20with%20Weighted%20Experts%20in%0A%20%20Echocardiography&body=Title%3A%20Multi-Site%20Class-Incremental%20Learning%20with%20Weighted%20Experts%20in%0A%20%20Echocardiography%0AAuthor%3A%20Kit%20M.%20Bransby%20and%20Woo-jin%20Cho%20Kim%20and%20Jorge%20Oliveira%20and%20Alex%20Thorley%20and%20Arian%20Beqiri%20and%20Alberto%20Gomez%20and%20Agisilaos%20Chartsias%0AAbstract%3A%20%20%20Building%20an%20echocardiography%20view%20classifier%20that%20maintains%20performance%20in%0Areal-life%20cases%20requires%20diverse%20multi-site%20data%2C%20and%20frequent%20updates%20with%0Anewly%20available%20data%20to%20mitigate%20model%20drift.%20Simply%20fine-tuning%20on%20new%0Adatasets%20results%20in%20%22catastrophic%20forgetting%22%2C%20and%20cannot%20adapt%20to%20variations%0Aof%20view%20labels%20between%20sites.%20Alternatively%2C%20collecting%20all%20data%20on%20a%20single%0Aserver%20and%20re-training%20may%20not%20be%20feasible%20as%20data%20sharing%20agreements%20may%0Arestrict%20image%20transfer%2C%20or%20datasets%20may%20only%20become%20available%20at%20different%0Atimes.%20Furthermore%2C%20time%20and%20cost%20associated%20with%20re-training%20grows%20with%20every%0Anew%20dataset.%20We%20propose%20a%20class-incremental%20learning%20method%20which%20learns%20an%0Aexpert%20network%20for%20each%20dataset%2C%20and%20combines%20all%20expert%20networks%20with%20a%20score%0Afusion%20model.%20The%20influence%20of%20%60%60unqualified%20experts%27%27%20is%20minimised%20by%0Aweighting%20each%20contribution%20with%20a%20learnt%20in-distribution%20score.%20These%20weights%0Apromote%20transparency%20as%20the%20contribution%20of%20each%20expert%20is%20known%20during%0Ainference.%20Instead%20of%20using%20the%20original%20images%2C%20we%20use%20learned%20features%20from%0Aeach%20dataset%2C%20which%20are%20easier%20to%20share%20and%20raise%20fewer%20licensing%20and%20privacy%0Aconcerns.%20We%20validate%20our%20work%20on%20six%20datasets%20from%20multiple%20sites%2C%0Ademonstrating%20significant%20reductions%20in%20training%20time%20while%20improving%20view%0Aclassification%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21577v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Site%2520Class-Incremental%2520Learning%2520with%2520Weighted%2520Experts%2520in%250A%2520%2520Echocardiography%26entry.906535625%3DKit%2520M.%2520Bransby%2520and%2520Woo-jin%2520Cho%2520Kim%2520and%2520Jorge%2520Oliveira%2520and%2520Alex%2520Thorley%2520and%2520Arian%2520Beqiri%2520and%2520Alberto%2520Gomez%2520and%2520Agisilaos%2520Chartsias%26entry.1292438233%3D%2520%2520Building%2520an%2520echocardiography%2520view%2520classifier%2520that%2520maintains%2520performance%2520in%250Areal-life%2520cases%2520requires%2520diverse%2520multi-site%2520data%252C%2520and%2520frequent%2520updates%2520with%250Anewly%2520available%2520data%2520to%2520mitigate%2520model%2520drift.%2520Simply%2520fine-tuning%2520on%2520new%250Adatasets%2520results%2520in%2520%2522catastrophic%2520forgetting%2522%252C%2520and%2520cannot%2520adapt%2520to%2520variations%250Aof%2520view%2520labels%2520between%2520sites.%2520Alternatively%252C%2520collecting%2520all%2520data%2520on%2520a%2520single%250Aserver%2520and%2520re-training%2520may%2520not%2520be%2520feasible%2520as%2520data%2520sharing%2520agreements%2520may%250Arestrict%2520image%2520transfer%252C%2520or%2520datasets%2520may%2520only%2520become%2520available%2520at%2520different%250Atimes.%2520Furthermore%252C%2520time%2520and%2520cost%2520associated%2520with%2520re-training%2520grows%2520with%2520every%250Anew%2520dataset.%2520We%2520propose%2520a%2520class-incremental%2520learning%2520method%2520which%2520learns%2520an%250Aexpert%2520network%2520for%2520each%2520dataset%252C%2520and%2520combines%2520all%2520expert%2520networks%2520with%2520a%2520score%250Afusion%2520model.%2520The%2520influence%2520of%2520%2560%2560unqualified%2520experts%2527%2527%2520is%2520minimised%2520by%250Aweighting%2520each%2520contribution%2520with%2520a%2520learnt%2520in-distribution%2520score.%2520These%2520weights%250Apromote%2520transparency%2520as%2520the%2520contribution%2520of%2520each%2520expert%2520is%2520known%2520during%250Ainference.%2520Instead%2520of%2520using%2520the%2520original%2520images%252C%2520we%2520use%2520learned%2520features%2520from%250Aeach%2520dataset%252C%2520which%2520are%2520easier%2520to%2520share%2520and%2520raise%2520fewer%2520licensing%2520and%2520privacy%250Aconcerns.%2520We%2520validate%2520our%2520work%2520on%2520six%2520datasets%2520from%2520multiple%2520sites%252C%250Ademonstrating%2520significant%2520reductions%2520in%2520training%2520time%2520while%2520improving%2520view%250Aclassification%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21577v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Site%20Class-Incremental%20Learning%20with%20Weighted%20Experts%20in%0A%20%20Echocardiography&entry.906535625=Kit%20M.%20Bransby%20and%20Woo-jin%20Cho%20Kim%20and%20Jorge%20Oliveira%20and%20Alex%20Thorley%20and%20Arian%20Beqiri%20and%20Alberto%20Gomez%20and%20Agisilaos%20Chartsias&entry.1292438233=%20%20Building%20an%20echocardiography%20view%20classifier%20that%20maintains%20performance%20in%0Areal-life%20cases%20requires%20diverse%20multi-site%20data%2C%20and%20frequent%20updates%20with%0Anewly%20available%20data%20to%20mitigate%20model%20drift.%20Simply%20fine-tuning%20on%20new%0Adatasets%20results%20in%20%22catastrophic%20forgetting%22%2C%20and%20cannot%20adapt%20to%20variations%0Aof%20view%20labels%20between%20sites.%20Alternatively%2C%20collecting%20all%20data%20on%20a%20single%0Aserver%20and%20re-training%20may%20not%20be%20feasible%20as%20data%20sharing%20agreements%20may%0Arestrict%20image%20transfer%2C%20or%20datasets%20may%20only%20become%20available%20at%20different%0Atimes.%20Furthermore%2C%20time%20and%20cost%20associated%20with%20re-training%20grows%20with%20every%0Anew%20dataset.%20We%20propose%20a%20class-incremental%20learning%20method%20which%20learns%20an%0Aexpert%20network%20for%20each%20dataset%2C%20and%20combines%20all%20expert%20networks%20with%20a%20score%0Afusion%20model.%20The%20influence%20of%20%60%60unqualified%20experts%27%27%20is%20minimised%20by%0Aweighting%20each%20contribution%20with%20a%20learnt%20in-distribution%20score.%20These%20weights%0Apromote%20transparency%20as%20the%20contribution%20of%20each%20expert%20is%20known%20during%0Ainference.%20Instead%20of%20using%20the%20original%20images%2C%20we%20use%20learned%20features%20from%0Aeach%20dataset%2C%20which%20are%20easier%20to%20share%20and%20raise%20fewer%20licensing%20and%20privacy%0Aconcerns.%20We%20validate%20our%20work%20on%20six%20datasets%20from%20multiple%20sites%2C%0Ademonstrating%20significant%20reductions%20in%20training%20time%20while%20improving%20view%0Aclassification%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21577v1&entry.124074799=Read"},
{"title": "Leveraging Self-Supervised Learning for Fetal Cardiac Planes\n  Classification using Ultrasound Scan Videos", "author": "Joseph Geo Benjamin and Mothilal Asokan and Amna Alhosani and Hussain Alasmawi and Werner Gerhard Diehl and Leanne Bricker and Karthik Nandakumar and Mohammad Yaqub", "abstract": "  Self-supervised learning (SSL) methods are popular since they can address\nsituations with limited annotated data by directly utilising the underlying\ndata distribution. However, the adoption of such methods is not explored enough\nin ultrasound (US) imaging, especially for fetal assessment. We investigate the\npotential of dual-encoder SSL in utilizing unlabelled US video data to improve\nthe performance of challenging downstream Standard Fetal Cardiac Planes (SFCP)\nclassification using limited labelled 2D US images. We study 7 SSL approaches\nbased on reconstruction, contrastive loss, distillation, and information theory\nand evaluate them extensively on a large private US dataset. Our observations\nand findings are consolidated from more than 500 downstream training\nexperiments under different settings. Our primary observation shows that for\nSSL training, the variance of the dataset is more crucial than its size because\nit allows the model to learn generalisable representations, which improve the\nperformance of downstream tasks. Overall, the BarlowTwins method shows robust\nperformance, irrespective of the training settings and data variations, when\nused as an initialisation for downstream tasks. Notably, full fine-tuning with\n1% of labelled data outperforms ImageNet initialisation by 12% in F1-score and\noutperforms other SSL initialisations by at least 4% in F1-score, thus making\nit a promising candidate for transfer learning from US video to image data.\n", "link": "http://arxiv.org/abs/2407.21738v1", "date": "2024-07-31", "relevancy": 2.0056, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5206}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4908}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Self-Supervised%20Learning%20for%20Fetal%20Cardiac%20Planes%0A%20%20Classification%20using%20Ultrasound%20Scan%20Videos&body=Title%3A%20Leveraging%20Self-Supervised%20Learning%20for%20Fetal%20Cardiac%20Planes%0A%20%20Classification%20using%20Ultrasound%20Scan%20Videos%0AAuthor%3A%20Joseph%20Geo%20Benjamin%20and%20Mothilal%20Asokan%20and%20Amna%20Alhosani%20and%20Hussain%20Alasmawi%20and%20Werner%20Gerhard%20Diehl%20and%20Leanne%20Bricker%20and%20Karthik%20Nandakumar%20and%20Mohammad%20Yaqub%0AAbstract%3A%20%20%20Self-supervised%20learning%20%28SSL%29%20methods%20are%20popular%20since%20they%20can%20address%0Asituations%20with%20limited%20annotated%20data%20by%20directly%20utilising%20the%20underlying%0Adata%20distribution.%20However%2C%20the%20adoption%20of%20such%20methods%20is%20not%20explored%20enough%0Ain%20ultrasound%20%28US%29%20imaging%2C%20especially%20for%20fetal%20assessment.%20We%20investigate%20the%0Apotential%20of%20dual-encoder%20SSL%20in%20utilizing%20unlabelled%20US%20video%20data%20to%20improve%0Athe%20performance%20of%20challenging%20downstream%20Standard%20Fetal%20Cardiac%20Planes%20%28SFCP%29%0Aclassification%20using%20limited%20labelled%202D%20US%20images.%20We%20study%207%20SSL%20approaches%0Abased%20on%20reconstruction%2C%20contrastive%20loss%2C%20distillation%2C%20and%20information%20theory%0Aand%20evaluate%20them%20extensively%20on%20a%20large%20private%20US%20dataset.%20Our%20observations%0Aand%20findings%20are%20consolidated%20from%20more%20than%20500%20downstream%20training%0Aexperiments%20under%20different%20settings.%20Our%20primary%20observation%20shows%20that%20for%0ASSL%20training%2C%20the%20variance%20of%20the%20dataset%20is%20more%20crucial%20than%20its%20size%20because%0Ait%20allows%20the%20model%20to%20learn%20generalisable%20representations%2C%20which%20improve%20the%0Aperformance%20of%20downstream%20tasks.%20Overall%2C%20the%20BarlowTwins%20method%20shows%20robust%0Aperformance%2C%20irrespective%20of%20the%20training%20settings%20and%20data%20variations%2C%20when%0Aused%20as%20an%20initialisation%20for%20downstream%20tasks.%20Notably%2C%20full%20fine-tuning%20with%0A1%25%20of%20labelled%20data%20outperforms%20ImageNet%20initialisation%20by%2012%25%20in%20F1-score%20and%0Aoutperforms%20other%20SSL%20initialisations%20by%20at%20least%204%25%20in%20F1-score%2C%20thus%20making%0Ait%20a%20promising%20candidate%20for%20transfer%20learning%20from%20US%20video%20to%20image%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21738v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Self-Supervised%2520Learning%2520for%2520Fetal%2520Cardiac%2520Planes%250A%2520%2520Classification%2520using%2520Ultrasound%2520Scan%2520Videos%26entry.906535625%3DJoseph%2520Geo%2520Benjamin%2520and%2520Mothilal%2520Asokan%2520and%2520Amna%2520Alhosani%2520and%2520Hussain%2520Alasmawi%2520and%2520Werner%2520Gerhard%2520Diehl%2520and%2520Leanne%2520Bricker%2520and%2520Karthik%2520Nandakumar%2520and%2520Mohammad%2520Yaqub%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520%2528SSL%2529%2520methods%2520are%2520popular%2520since%2520they%2520can%2520address%250Asituations%2520with%2520limited%2520annotated%2520data%2520by%2520directly%2520utilising%2520the%2520underlying%250Adata%2520distribution.%2520However%252C%2520the%2520adoption%2520of%2520such%2520methods%2520is%2520not%2520explored%2520enough%250Ain%2520ultrasound%2520%2528US%2529%2520imaging%252C%2520especially%2520for%2520fetal%2520assessment.%2520We%2520investigate%2520the%250Apotential%2520of%2520dual-encoder%2520SSL%2520in%2520utilizing%2520unlabelled%2520US%2520video%2520data%2520to%2520improve%250Athe%2520performance%2520of%2520challenging%2520downstream%2520Standard%2520Fetal%2520Cardiac%2520Planes%2520%2528SFCP%2529%250Aclassification%2520using%2520limited%2520labelled%25202D%2520US%2520images.%2520We%2520study%25207%2520SSL%2520approaches%250Abased%2520on%2520reconstruction%252C%2520contrastive%2520loss%252C%2520distillation%252C%2520and%2520information%2520theory%250Aand%2520evaluate%2520them%2520extensively%2520on%2520a%2520large%2520private%2520US%2520dataset.%2520Our%2520observations%250Aand%2520findings%2520are%2520consolidated%2520from%2520more%2520than%2520500%2520downstream%2520training%250Aexperiments%2520under%2520different%2520settings.%2520Our%2520primary%2520observation%2520shows%2520that%2520for%250ASSL%2520training%252C%2520the%2520variance%2520of%2520the%2520dataset%2520is%2520more%2520crucial%2520than%2520its%2520size%2520because%250Ait%2520allows%2520the%2520model%2520to%2520learn%2520generalisable%2520representations%252C%2520which%2520improve%2520the%250Aperformance%2520of%2520downstream%2520tasks.%2520Overall%252C%2520the%2520BarlowTwins%2520method%2520shows%2520robust%250Aperformance%252C%2520irrespective%2520of%2520the%2520training%2520settings%2520and%2520data%2520variations%252C%2520when%250Aused%2520as%2520an%2520initialisation%2520for%2520downstream%2520tasks.%2520Notably%252C%2520full%2520fine-tuning%2520with%250A1%2525%2520of%2520labelled%2520data%2520outperforms%2520ImageNet%2520initialisation%2520by%252012%2525%2520in%2520F1-score%2520and%250Aoutperforms%2520other%2520SSL%2520initialisations%2520by%2520at%2520least%25204%2525%2520in%2520F1-score%252C%2520thus%2520making%250Ait%2520a%2520promising%2520candidate%2520for%2520transfer%2520learning%2520from%2520US%2520video%2520to%2520image%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21738v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Self-Supervised%20Learning%20for%20Fetal%20Cardiac%20Planes%0A%20%20Classification%20using%20Ultrasound%20Scan%20Videos&entry.906535625=Joseph%20Geo%20Benjamin%20and%20Mothilal%20Asokan%20and%20Amna%20Alhosani%20and%20Hussain%20Alasmawi%20and%20Werner%20Gerhard%20Diehl%20and%20Leanne%20Bricker%20and%20Karthik%20Nandakumar%20and%20Mohammad%20Yaqub&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20methods%20are%20popular%20since%20they%20can%20address%0Asituations%20with%20limited%20annotated%20data%20by%20directly%20utilising%20the%20underlying%0Adata%20distribution.%20However%2C%20the%20adoption%20of%20such%20methods%20is%20not%20explored%20enough%0Ain%20ultrasound%20%28US%29%20imaging%2C%20especially%20for%20fetal%20assessment.%20We%20investigate%20the%0Apotential%20of%20dual-encoder%20SSL%20in%20utilizing%20unlabelled%20US%20video%20data%20to%20improve%0Athe%20performance%20of%20challenging%20downstream%20Standard%20Fetal%20Cardiac%20Planes%20%28SFCP%29%0Aclassification%20using%20limited%20labelled%202D%20US%20images.%20We%20study%207%20SSL%20approaches%0Abased%20on%20reconstruction%2C%20contrastive%20loss%2C%20distillation%2C%20and%20information%20theory%0Aand%20evaluate%20them%20extensively%20on%20a%20large%20private%20US%20dataset.%20Our%20observations%0Aand%20findings%20are%20consolidated%20from%20more%20than%20500%20downstream%20training%0Aexperiments%20under%20different%20settings.%20Our%20primary%20observation%20shows%20that%20for%0ASSL%20training%2C%20the%20variance%20of%20the%20dataset%20is%20more%20crucial%20than%20its%20size%20because%0Ait%20allows%20the%20model%20to%20learn%20generalisable%20representations%2C%20which%20improve%20the%0Aperformance%20of%20downstream%20tasks.%20Overall%2C%20the%20BarlowTwins%20method%20shows%20robust%0Aperformance%2C%20irrespective%20of%20the%20training%20settings%20and%20data%20variations%2C%20when%0Aused%20as%20an%20initialisation%20for%20downstream%20tasks.%20Notably%2C%20full%20fine-tuning%20with%0A1%25%20of%20labelled%20data%20outperforms%20ImageNet%20initialisation%20by%2012%25%20in%20F1-score%20and%0Aoutperforms%20other%20SSL%20initialisations%20by%20at%20least%204%25%20in%20F1-score%2C%20thus%20making%0Ait%20a%20promising%20candidate%20for%20transfer%20learning%20from%20US%20video%20to%20image%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21738v1&entry.124074799=Read"},
{"title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling", "author": "Bradley Brown and Jordan Juravsky and Ryan Ehrlich and Ronald Clark and Quoc V. Le and Christopher R\u00e9 and Azalia Mirhoseini", "abstract": "  Scaling the amount of compute used to train language models has dramatically\nimproved their capabilities. However, when it comes to inference, we often\nlimit the amount of compute to only one attempt per problem. Here, we explore\ninference compute as another axis for scaling by increasing the number of\ngenerated samples. Across multiple tasks and models, we observe that coverage -\nthe fraction of problems solved by any attempt - scales with the number of\nsamples over four orders of magnitude. In domains like coding and formal\nproofs, where all answers can be automatically verified, these increases in\ncoverage directly translate into improved performance. When we apply repeated\nsampling to SWE-bench Lite, the fraction of issues solved with\nDeepSeek-V2-Coder-Instruct increases from 15.9% with one sample to 56% with 250\nsamples, outperforming the single-attempt state-of-the-art of 43% which uses\nmore capable frontier models. Moreover, using current API pricing, amplifying\nthe cheaper DeepSeek model with five samples is more cost-effective and solves\nmore issues than paying a premium for one sample from GPT-4o or Claude 3.5\nSonnet. Interestingly, the relationship between coverage and the number of\nsamples is often log-linear and can be modelled with an exponentiated power\nlaw, suggesting the existence of inference-time scaling laws. Finally, we find\nthat identifying correct samples out of many generations remains an important\ndirection for future research in domains without automatic verifiers. When\nsolving math word problems from GSM8K and MATH, coverage with Llama-3 models\ngrows to over 95% with 10,000 samples. However, common methods to pick correct\nsolutions from a sample collection, such as majority voting or reward models,\nplateau beyond several hundred samples and fail to fully scale with the sample\nbudget.\n", "link": "http://arxiv.org/abs/2407.21787v1", "date": "2024-07-31", "relevancy": 1.9821, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5141}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4968}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Monkeys%3A%20Scaling%20Inference%20Compute%20with%20Repeated%20Sampling&body=Title%3A%20Large%20Language%20Monkeys%3A%20Scaling%20Inference%20Compute%20with%20Repeated%20Sampling%0AAuthor%3A%20Bradley%20Brown%20and%20Jordan%20Juravsky%20and%20Ryan%20Ehrlich%20and%20Ronald%20Clark%20and%20Quoc%20V.%20Le%20and%20Christopher%20R%C3%A9%20and%20Azalia%20Mirhoseini%0AAbstract%3A%20%20%20Scaling%20the%20amount%20of%20compute%20used%20to%20train%20language%20models%20has%20dramatically%0Aimproved%20their%20capabilities.%20However%2C%20when%20it%20comes%20to%20inference%2C%20we%20often%0Alimit%20the%20amount%20of%20compute%20to%20only%20one%20attempt%20per%20problem.%20Here%2C%20we%20explore%0Ainference%20compute%20as%20another%20axis%20for%20scaling%20by%20increasing%20the%20number%20of%0Agenerated%20samples.%20Across%20multiple%20tasks%20and%20models%2C%20we%20observe%20that%20coverage%20-%0Athe%20fraction%20of%20problems%20solved%20by%20any%20attempt%20-%20scales%20with%20the%20number%20of%0Asamples%20over%20four%20orders%20of%20magnitude.%20In%20domains%20like%20coding%20and%20formal%0Aproofs%2C%20where%20all%20answers%20can%20be%20automatically%20verified%2C%20these%20increases%20in%0Acoverage%20directly%20translate%20into%20improved%20performance.%20When%20we%20apply%20repeated%0Asampling%20to%20SWE-bench%20Lite%2C%20the%20fraction%20of%20issues%20solved%20with%0ADeepSeek-V2-Coder-Instruct%20increases%20from%2015.9%25%20with%20one%20sample%20to%2056%25%20with%20250%0Asamples%2C%20outperforming%20the%20single-attempt%20state-of-the-art%20of%2043%25%20which%20uses%0Amore%20capable%20frontier%20models.%20Moreover%2C%20using%20current%20API%20pricing%2C%20amplifying%0Athe%20cheaper%20DeepSeek%20model%20with%20five%20samples%20is%20more%20cost-effective%20and%20solves%0Amore%20issues%20than%20paying%20a%20premium%20for%20one%20sample%20from%20GPT-4o%20or%20Claude%203.5%0ASonnet.%20Interestingly%2C%20the%20relationship%20between%20coverage%20and%20the%20number%20of%0Asamples%20is%20often%20log-linear%20and%20can%20be%20modelled%20with%20an%20exponentiated%20power%0Alaw%2C%20suggesting%20the%20existence%20of%20inference-time%20scaling%20laws.%20Finally%2C%20we%20find%0Athat%20identifying%20correct%20samples%20out%20of%20many%20generations%20remains%20an%20important%0Adirection%20for%20future%20research%20in%20domains%20without%20automatic%20verifiers.%20When%0Asolving%20math%20word%20problems%20from%20GSM8K%20and%20MATH%2C%20coverage%20with%20Llama-3%20models%0Agrows%20to%20over%2095%25%20with%2010%2C000%20samples.%20However%2C%20common%20methods%20to%20pick%20correct%0Asolutions%20from%20a%20sample%20collection%2C%20such%20as%20majority%20voting%20or%20reward%20models%2C%0Aplateau%20beyond%20several%20hundred%20samples%20and%20fail%20to%20fully%20scale%20with%20the%20sample%0Abudget.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21787v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Monkeys%253A%2520Scaling%2520Inference%2520Compute%2520with%2520Repeated%2520Sampling%26entry.906535625%3DBradley%2520Brown%2520and%2520Jordan%2520Juravsky%2520and%2520Ryan%2520Ehrlich%2520and%2520Ronald%2520Clark%2520and%2520Quoc%2520V.%2520Le%2520and%2520Christopher%2520R%25C3%25A9%2520and%2520Azalia%2520Mirhoseini%26entry.1292438233%3D%2520%2520Scaling%2520the%2520amount%2520of%2520compute%2520used%2520to%2520train%2520language%2520models%2520has%2520dramatically%250Aimproved%2520their%2520capabilities.%2520However%252C%2520when%2520it%2520comes%2520to%2520inference%252C%2520we%2520often%250Alimit%2520the%2520amount%2520of%2520compute%2520to%2520only%2520one%2520attempt%2520per%2520problem.%2520Here%252C%2520we%2520explore%250Ainference%2520compute%2520as%2520another%2520axis%2520for%2520scaling%2520by%2520increasing%2520the%2520number%2520of%250Agenerated%2520samples.%2520Across%2520multiple%2520tasks%2520and%2520models%252C%2520we%2520observe%2520that%2520coverage%2520-%250Athe%2520fraction%2520of%2520problems%2520solved%2520by%2520any%2520attempt%2520-%2520scales%2520with%2520the%2520number%2520of%250Asamples%2520over%2520four%2520orders%2520of%2520magnitude.%2520In%2520domains%2520like%2520coding%2520and%2520formal%250Aproofs%252C%2520where%2520all%2520answers%2520can%2520be%2520automatically%2520verified%252C%2520these%2520increases%2520in%250Acoverage%2520directly%2520translate%2520into%2520improved%2520performance.%2520When%2520we%2520apply%2520repeated%250Asampling%2520to%2520SWE-bench%2520Lite%252C%2520the%2520fraction%2520of%2520issues%2520solved%2520with%250ADeepSeek-V2-Coder-Instruct%2520increases%2520from%252015.9%2525%2520with%2520one%2520sample%2520to%252056%2525%2520with%2520250%250Asamples%252C%2520outperforming%2520the%2520single-attempt%2520state-of-the-art%2520of%252043%2525%2520which%2520uses%250Amore%2520capable%2520frontier%2520models.%2520Moreover%252C%2520using%2520current%2520API%2520pricing%252C%2520amplifying%250Athe%2520cheaper%2520DeepSeek%2520model%2520with%2520five%2520samples%2520is%2520more%2520cost-effective%2520and%2520solves%250Amore%2520issues%2520than%2520paying%2520a%2520premium%2520for%2520one%2520sample%2520from%2520GPT-4o%2520or%2520Claude%25203.5%250ASonnet.%2520Interestingly%252C%2520the%2520relationship%2520between%2520coverage%2520and%2520the%2520number%2520of%250Asamples%2520is%2520often%2520log-linear%2520and%2520can%2520be%2520modelled%2520with%2520an%2520exponentiated%2520power%250Alaw%252C%2520suggesting%2520the%2520existence%2520of%2520inference-time%2520scaling%2520laws.%2520Finally%252C%2520we%2520find%250Athat%2520identifying%2520correct%2520samples%2520out%2520of%2520many%2520generations%2520remains%2520an%2520important%250Adirection%2520for%2520future%2520research%2520in%2520domains%2520without%2520automatic%2520verifiers.%2520When%250Asolving%2520math%2520word%2520problems%2520from%2520GSM8K%2520and%2520MATH%252C%2520coverage%2520with%2520Llama-3%2520models%250Agrows%2520to%2520over%252095%2525%2520with%252010%252C000%2520samples.%2520However%252C%2520common%2520methods%2520to%2520pick%2520correct%250Asolutions%2520from%2520a%2520sample%2520collection%252C%2520such%2520as%2520majority%2520voting%2520or%2520reward%2520models%252C%250Aplateau%2520beyond%2520several%2520hundred%2520samples%2520and%2520fail%2520to%2520fully%2520scale%2520with%2520the%2520sample%250Abudget.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21787v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Monkeys%3A%20Scaling%20Inference%20Compute%20with%20Repeated%20Sampling&entry.906535625=Bradley%20Brown%20and%20Jordan%20Juravsky%20and%20Ryan%20Ehrlich%20and%20Ronald%20Clark%20and%20Quoc%20V.%20Le%20and%20Christopher%20R%C3%A9%20and%20Azalia%20Mirhoseini&entry.1292438233=%20%20Scaling%20the%20amount%20of%20compute%20used%20to%20train%20language%20models%20has%20dramatically%0Aimproved%20their%20capabilities.%20However%2C%20when%20it%20comes%20to%20inference%2C%20we%20often%0Alimit%20the%20amount%20of%20compute%20to%20only%20one%20attempt%20per%20problem.%20Here%2C%20we%20explore%0Ainference%20compute%20as%20another%20axis%20for%20scaling%20by%20increasing%20the%20number%20of%0Agenerated%20samples.%20Across%20multiple%20tasks%20and%20models%2C%20we%20observe%20that%20coverage%20-%0Athe%20fraction%20of%20problems%20solved%20by%20any%20attempt%20-%20scales%20with%20the%20number%20of%0Asamples%20over%20four%20orders%20of%20magnitude.%20In%20domains%20like%20coding%20and%20formal%0Aproofs%2C%20where%20all%20answers%20can%20be%20automatically%20verified%2C%20these%20increases%20in%0Acoverage%20directly%20translate%20into%20improved%20performance.%20When%20we%20apply%20repeated%0Asampling%20to%20SWE-bench%20Lite%2C%20the%20fraction%20of%20issues%20solved%20with%0ADeepSeek-V2-Coder-Instruct%20increases%20from%2015.9%25%20with%20one%20sample%20to%2056%25%20with%20250%0Asamples%2C%20outperforming%20the%20single-attempt%20state-of-the-art%20of%2043%25%20which%20uses%0Amore%20capable%20frontier%20models.%20Moreover%2C%20using%20current%20API%20pricing%2C%20amplifying%0Athe%20cheaper%20DeepSeek%20model%20with%20five%20samples%20is%20more%20cost-effective%20and%20solves%0Amore%20issues%20than%20paying%20a%20premium%20for%20one%20sample%20from%20GPT-4o%20or%20Claude%203.5%0ASonnet.%20Interestingly%2C%20the%20relationship%20between%20coverage%20and%20the%20number%20of%0Asamples%20is%20often%20log-linear%20and%20can%20be%20modelled%20with%20an%20exponentiated%20power%0Alaw%2C%20suggesting%20the%20existence%20of%20inference-time%20scaling%20laws.%20Finally%2C%20we%20find%0Athat%20identifying%20correct%20samples%20out%20of%20many%20generations%20remains%20an%20important%0Adirection%20for%20future%20research%20in%20domains%20without%20automatic%20verifiers.%20When%0Asolving%20math%20word%20problems%20from%20GSM8K%20and%20MATH%2C%20coverage%20with%20Llama-3%20models%0Agrows%20to%20over%2095%25%20with%2010%2C000%20samples.%20However%2C%20common%20methods%20to%20pick%20correct%0Asolutions%20from%20a%20sample%20collection%2C%20such%20as%20majority%20voting%20or%20reward%20models%2C%0Aplateau%20beyond%20several%20hundred%20samples%20and%20fail%20to%20fully%20scale%20with%20the%20sample%0Abudget.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21787v1&entry.124074799=Read"},
{"title": "Open-Vocabulary Audio-Visual Semantic Segmentation", "author": "Ruohao Guo and Liao Qu and Dantong Niu and Yanyu Qi and Wenzhen Yue and Ji Shi and Bowei Xing and Xianghua Ying", "abstract": "  Audio-visual semantic segmentation (AVSS) aims to segment and classify\nsounding objects in videos with acoustic cues. However, most approaches operate\non the close-set assumption and only identify pre-defined categories from\ntraining data, lacking the generalization ability to detect novel categories in\npractical applications. In this paper, we introduce a new task: open-vocabulary\naudio-visual semantic segmentation, extending AVSS task to open-world scenarios\nbeyond the annotated label space. This is a more challenging task that requires\nrecognizing all categories, even those that have never been seen nor heard\nduring training. Moreover, we propose the first open-vocabulary AVSS framework,\nOV-AVSS, which mainly consists of two parts: 1) a universal sound source\nlocalization module to perform audio-visual fusion and locate all potential\nsounding objects and 2) an open-vocabulary classification module to predict\ncategories with the help of the prior knowledge from large-scale pre-trained\nvision-language models. To properly evaluate the open-vocabulary AVSS, we split\nzero-shot training and testing subsets based on the AVSBench-semantic\nbenchmark, namely AVSBench-OV. Extensive experiments demonstrate the strong\nsegmentation and zero-shot generalization ability of our model on all\ncategories. On the AVSBench-OV dataset, OV-AVSS achieves 55.43% mIoU on base\ncategories and 29.14% mIoU on novel categories, exceeding the state-of-the-art\nzero-shot method by 41.88%/20.61% and open-vocabulary method by 10.2%/11.6%.\nThe code is available at https://github.com/ruohaoguo/ovavss.\n", "link": "http://arxiv.org/abs/2407.21721v1", "date": "2024-07-31", "relevancy": 1.9795, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4977}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4971}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-Vocabulary%20Audio-Visual%20Semantic%20Segmentation&body=Title%3A%20Open-Vocabulary%20Audio-Visual%20Semantic%20Segmentation%0AAuthor%3A%20Ruohao%20Guo%20and%20Liao%20Qu%20and%20Dantong%20Niu%20and%20Yanyu%20Qi%20and%20Wenzhen%20Yue%20and%20Ji%20Shi%20and%20Bowei%20Xing%20and%20Xianghua%20Ying%0AAbstract%3A%20%20%20Audio-visual%20semantic%20segmentation%20%28AVSS%29%20aims%20to%20segment%20and%20classify%0Asounding%20objects%20in%20videos%20with%20acoustic%20cues.%20However%2C%20most%20approaches%20operate%0Aon%20the%20close-set%20assumption%20and%20only%20identify%20pre-defined%20categories%20from%0Atraining%20data%2C%20lacking%20the%20generalization%20ability%20to%20detect%20novel%20categories%20in%0Apractical%20applications.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20task%3A%20open-vocabulary%0Aaudio-visual%20semantic%20segmentation%2C%20extending%20AVSS%20task%20to%20open-world%20scenarios%0Abeyond%20the%20annotated%20label%20space.%20This%20is%20a%20more%20challenging%20task%20that%20requires%0Arecognizing%20all%20categories%2C%20even%20those%20that%20have%20never%20been%20seen%20nor%20heard%0Aduring%20training.%20Moreover%2C%20we%20propose%20the%20first%20open-vocabulary%20AVSS%20framework%2C%0AOV-AVSS%2C%20which%20mainly%20consists%20of%20two%20parts%3A%201%29%20a%20universal%20sound%20source%0Alocalization%20module%20to%20perform%20audio-visual%20fusion%20and%20locate%20all%20potential%0Asounding%20objects%20and%202%29%20an%20open-vocabulary%20classification%20module%20to%20predict%0Acategories%20with%20the%20help%20of%20the%20prior%20knowledge%20from%20large-scale%20pre-trained%0Avision-language%20models.%20To%20properly%20evaluate%20the%20open-vocabulary%20AVSS%2C%20we%20split%0Azero-shot%20training%20and%20testing%20subsets%20based%20on%20the%20AVSBench-semantic%0Abenchmark%2C%20namely%20AVSBench-OV.%20Extensive%20experiments%20demonstrate%20the%20strong%0Asegmentation%20and%20zero-shot%20generalization%20ability%20of%20our%20model%20on%20all%0Acategories.%20On%20the%20AVSBench-OV%20dataset%2C%20OV-AVSS%20achieves%2055.43%25%20mIoU%20on%20base%0Acategories%20and%2029.14%25%20mIoU%20on%20novel%20categories%2C%20exceeding%20the%20state-of-the-art%0Azero-shot%20method%20by%2041.88%25/20.61%25%20and%20open-vocabulary%20method%20by%2010.2%25/11.6%25.%0AThe%20code%20is%20available%20at%20https%3A//github.com/ruohaoguo/ovavss.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21721v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-Vocabulary%2520Audio-Visual%2520Semantic%2520Segmentation%26entry.906535625%3DRuohao%2520Guo%2520and%2520Liao%2520Qu%2520and%2520Dantong%2520Niu%2520and%2520Yanyu%2520Qi%2520and%2520Wenzhen%2520Yue%2520and%2520Ji%2520Shi%2520and%2520Bowei%2520Xing%2520and%2520Xianghua%2520Ying%26entry.1292438233%3D%2520%2520Audio-visual%2520semantic%2520segmentation%2520%2528AVSS%2529%2520aims%2520to%2520segment%2520and%2520classify%250Asounding%2520objects%2520in%2520videos%2520with%2520acoustic%2520cues.%2520However%252C%2520most%2520approaches%2520operate%250Aon%2520the%2520close-set%2520assumption%2520and%2520only%2520identify%2520pre-defined%2520categories%2520from%250Atraining%2520data%252C%2520lacking%2520the%2520generalization%2520ability%2520to%2520detect%2520novel%2520categories%2520in%250Apractical%2520applications.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520new%2520task%253A%2520open-vocabulary%250Aaudio-visual%2520semantic%2520segmentation%252C%2520extending%2520AVSS%2520task%2520to%2520open-world%2520scenarios%250Abeyond%2520the%2520annotated%2520label%2520space.%2520This%2520is%2520a%2520more%2520challenging%2520task%2520that%2520requires%250Arecognizing%2520all%2520categories%252C%2520even%2520those%2520that%2520have%2520never%2520been%2520seen%2520nor%2520heard%250Aduring%2520training.%2520Moreover%252C%2520we%2520propose%2520the%2520first%2520open-vocabulary%2520AVSS%2520framework%252C%250AOV-AVSS%252C%2520which%2520mainly%2520consists%2520of%2520two%2520parts%253A%25201%2529%2520a%2520universal%2520sound%2520source%250Alocalization%2520module%2520to%2520perform%2520audio-visual%2520fusion%2520and%2520locate%2520all%2520potential%250Asounding%2520objects%2520and%25202%2529%2520an%2520open-vocabulary%2520classification%2520module%2520to%2520predict%250Acategories%2520with%2520the%2520help%2520of%2520the%2520prior%2520knowledge%2520from%2520large-scale%2520pre-trained%250Avision-language%2520models.%2520To%2520properly%2520evaluate%2520the%2520open-vocabulary%2520AVSS%252C%2520we%2520split%250Azero-shot%2520training%2520and%2520testing%2520subsets%2520based%2520on%2520the%2520AVSBench-semantic%250Abenchmark%252C%2520namely%2520AVSBench-OV.%2520Extensive%2520experiments%2520demonstrate%2520the%2520strong%250Asegmentation%2520and%2520zero-shot%2520generalization%2520ability%2520of%2520our%2520model%2520on%2520all%250Acategories.%2520On%2520the%2520AVSBench-OV%2520dataset%252C%2520OV-AVSS%2520achieves%252055.43%2525%2520mIoU%2520on%2520base%250Acategories%2520and%252029.14%2525%2520mIoU%2520on%2520novel%2520categories%252C%2520exceeding%2520the%2520state-of-the-art%250Azero-shot%2520method%2520by%252041.88%2525/20.61%2525%2520and%2520open-vocabulary%2520method%2520by%252010.2%2525/11.6%2525.%250AThe%2520code%2520is%2520available%2520at%2520https%253A//github.com/ruohaoguo/ovavss.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21721v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-Vocabulary%20Audio-Visual%20Semantic%20Segmentation&entry.906535625=Ruohao%20Guo%20and%20Liao%20Qu%20and%20Dantong%20Niu%20and%20Yanyu%20Qi%20and%20Wenzhen%20Yue%20and%20Ji%20Shi%20and%20Bowei%20Xing%20and%20Xianghua%20Ying&entry.1292438233=%20%20Audio-visual%20semantic%20segmentation%20%28AVSS%29%20aims%20to%20segment%20and%20classify%0Asounding%20objects%20in%20videos%20with%20acoustic%20cues.%20However%2C%20most%20approaches%20operate%0Aon%20the%20close-set%20assumption%20and%20only%20identify%20pre-defined%20categories%20from%0Atraining%20data%2C%20lacking%20the%20generalization%20ability%20to%20detect%20novel%20categories%20in%0Apractical%20applications.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20task%3A%20open-vocabulary%0Aaudio-visual%20semantic%20segmentation%2C%20extending%20AVSS%20task%20to%20open-world%20scenarios%0Abeyond%20the%20annotated%20label%20space.%20This%20is%20a%20more%20challenging%20task%20that%20requires%0Arecognizing%20all%20categories%2C%20even%20those%20that%20have%20never%20been%20seen%20nor%20heard%0Aduring%20training.%20Moreover%2C%20we%20propose%20the%20first%20open-vocabulary%20AVSS%20framework%2C%0AOV-AVSS%2C%20which%20mainly%20consists%20of%20two%20parts%3A%201%29%20a%20universal%20sound%20source%0Alocalization%20module%20to%20perform%20audio-visual%20fusion%20and%20locate%20all%20potential%0Asounding%20objects%20and%202%29%20an%20open-vocabulary%20classification%20module%20to%20predict%0Acategories%20with%20the%20help%20of%20the%20prior%20knowledge%20from%20large-scale%20pre-trained%0Avision-language%20models.%20To%20properly%20evaluate%20the%20open-vocabulary%20AVSS%2C%20we%20split%0Azero-shot%20training%20and%20testing%20subsets%20based%20on%20the%20AVSBench-semantic%0Abenchmark%2C%20namely%20AVSBench-OV.%20Extensive%20experiments%20demonstrate%20the%20strong%0Asegmentation%20and%20zero-shot%20generalization%20ability%20of%20our%20model%20on%20all%0Acategories.%20On%20the%20AVSBench-OV%20dataset%2C%20OV-AVSS%20achieves%2055.43%25%20mIoU%20on%20base%0Acategories%20and%2029.14%25%20mIoU%20on%20novel%20categories%2C%20exceeding%20the%20state-of-the-art%0Azero-shot%20method%20by%2041.88%25/20.61%25%20and%20open-vocabulary%20method%20by%2010.2%25/11.6%25.%0AThe%20code%20is%20available%20at%20https%3A//github.com/ruohaoguo/ovavss.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21721v1&entry.124074799=Read"},
{"title": "A Survey on Self-Supervised Graph Foundation Models: Knowledge-Based\n  Perspective", "author": "Ziwen Zhao and Yixin Su and Yuhua Li and Yixiong Zou and Ruixuan Li and Rui Zhang", "abstract": "  Graph self-supervised learning (SSL) is now a go-to method for pre-training\ngraph foundation models (GFMs). There is a wide variety of knowledge patterns\nembedded in the graph data, such as node properties and clusters, which are\ncrucial to learning generalized representations for GFMs. However, existing\nsurveys of GFMs have several shortcomings: they lack comprehensiveness\nregarding the most recent progress, have unclear categorization of\nself-supervised methods, and take a limited architecture-based perspective that\nis restricted to only certain types of graph models. As the ultimate goal of\nGFMs is to learn generalized graph knowledge, we provide a comprehensive survey\nof self-supervised GFMs from a novel knowledge-based perspective. We propose a\nknowledge-based taxonomy, which categorizes self-supervised graph models by the\nspecific graph knowledge utilized. Our taxonomy consists of microscopic (nodes,\nlinks, etc.), mesoscopic (context, clusters, etc.), and macroscopic knowledge\n(global structure, manifolds, etc.). It covers a total of 9 knowledge\ncategories and more than 25 pretext tasks for pre-training GFMs, as well as\nvarious downstream task generalization strategies. Such a knowledge-based\ntaxonomy allows us to re-examine graph models based on new architectures more\nclearly, such as graph language models, as well as provide more in-depth\ninsights for constructing GFMs.\n", "link": "http://arxiv.org/abs/2403.16137v2", "date": "2024-07-31", "relevancy": 1.9753, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5599}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4484}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Self-Supervised%20Graph%20Foundation%20Models%3A%20Knowledge-Based%0A%20%20Perspective&body=Title%3A%20A%20Survey%20on%20Self-Supervised%20Graph%20Foundation%20Models%3A%20Knowledge-Based%0A%20%20Perspective%0AAuthor%3A%20Ziwen%20Zhao%20and%20Yixin%20Su%20and%20Yuhua%20Li%20and%20Yixiong%20Zou%20and%20Ruixuan%20Li%20and%20Rui%20Zhang%0AAbstract%3A%20%20%20Graph%20self-supervised%20learning%20%28SSL%29%20is%20now%20a%20go-to%20method%20for%20pre-training%0Agraph%20foundation%20models%20%28GFMs%29.%20There%20is%20a%20wide%20variety%20of%20knowledge%20patterns%0Aembedded%20in%20the%20graph%20data%2C%20such%20as%20node%20properties%20and%20clusters%2C%20which%20are%0Acrucial%20to%20learning%20generalized%20representations%20for%20GFMs.%20However%2C%20existing%0Asurveys%20of%20GFMs%20have%20several%20shortcomings%3A%20they%20lack%20comprehensiveness%0Aregarding%20the%20most%20recent%20progress%2C%20have%20unclear%20categorization%20of%0Aself-supervised%20methods%2C%20and%20take%20a%20limited%20architecture-based%20perspective%20that%0Ais%20restricted%20to%20only%20certain%20types%20of%20graph%20models.%20As%20the%20ultimate%20goal%20of%0AGFMs%20is%20to%20learn%20generalized%20graph%20knowledge%2C%20we%20provide%20a%20comprehensive%20survey%0Aof%20self-supervised%20GFMs%20from%20a%20novel%20knowledge-based%20perspective.%20We%20propose%20a%0Aknowledge-based%20taxonomy%2C%20which%20categorizes%20self-supervised%20graph%20models%20by%20the%0Aspecific%20graph%20knowledge%20utilized.%20Our%20taxonomy%20consists%20of%20microscopic%20%28nodes%2C%0Alinks%2C%20etc.%29%2C%20mesoscopic%20%28context%2C%20clusters%2C%20etc.%29%2C%20and%20macroscopic%20knowledge%0A%28global%20structure%2C%20manifolds%2C%20etc.%29.%20It%20covers%20a%20total%20of%209%20knowledge%0Acategories%20and%20more%20than%2025%20pretext%20tasks%20for%20pre-training%20GFMs%2C%20as%20well%20as%0Avarious%20downstream%20task%20generalization%20strategies.%20Such%20a%20knowledge-based%0Ataxonomy%20allows%20us%20to%20re-examine%20graph%20models%20based%20on%20new%20architectures%20more%0Aclearly%2C%20such%20as%20graph%20language%20models%2C%20as%20well%20as%20provide%20more%20in-depth%0Ainsights%20for%20constructing%20GFMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16137v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Self-Supervised%2520Graph%2520Foundation%2520Models%253A%2520Knowledge-Based%250A%2520%2520Perspective%26entry.906535625%3DZiwen%2520Zhao%2520and%2520Yixin%2520Su%2520and%2520Yuhua%2520Li%2520and%2520Yixiong%2520Zou%2520and%2520Ruixuan%2520Li%2520and%2520Rui%2520Zhang%26entry.1292438233%3D%2520%2520Graph%2520self-supervised%2520learning%2520%2528SSL%2529%2520is%2520now%2520a%2520go-to%2520method%2520for%2520pre-training%250Agraph%2520foundation%2520models%2520%2528GFMs%2529.%2520There%2520is%2520a%2520wide%2520variety%2520of%2520knowledge%2520patterns%250Aembedded%2520in%2520the%2520graph%2520data%252C%2520such%2520as%2520node%2520properties%2520and%2520clusters%252C%2520which%2520are%250Acrucial%2520to%2520learning%2520generalized%2520representations%2520for%2520GFMs.%2520However%252C%2520existing%250Asurveys%2520of%2520GFMs%2520have%2520several%2520shortcomings%253A%2520they%2520lack%2520comprehensiveness%250Aregarding%2520the%2520most%2520recent%2520progress%252C%2520have%2520unclear%2520categorization%2520of%250Aself-supervised%2520methods%252C%2520and%2520take%2520a%2520limited%2520architecture-based%2520perspective%2520that%250Ais%2520restricted%2520to%2520only%2520certain%2520types%2520of%2520graph%2520models.%2520As%2520the%2520ultimate%2520goal%2520of%250AGFMs%2520is%2520to%2520learn%2520generalized%2520graph%2520knowledge%252C%2520we%2520provide%2520a%2520comprehensive%2520survey%250Aof%2520self-supervised%2520GFMs%2520from%2520a%2520novel%2520knowledge-based%2520perspective.%2520We%2520propose%2520a%250Aknowledge-based%2520taxonomy%252C%2520which%2520categorizes%2520self-supervised%2520graph%2520models%2520by%2520the%250Aspecific%2520graph%2520knowledge%2520utilized.%2520Our%2520taxonomy%2520consists%2520of%2520microscopic%2520%2528nodes%252C%250Alinks%252C%2520etc.%2529%252C%2520mesoscopic%2520%2528context%252C%2520clusters%252C%2520etc.%2529%252C%2520and%2520macroscopic%2520knowledge%250A%2528global%2520structure%252C%2520manifolds%252C%2520etc.%2529.%2520It%2520covers%2520a%2520total%2520of%25209%2520knowledge%250Acategories%2520and%2520more%2520than%252025%2520pretext%2520tasks%2520for%2520pre-training%2520GFMs%252C%2520as%2520well%2520as%250Avarious%2520downstream%2520task%2520generalization%2520strategies.%2520Such%2520a%2520knowledge-based%250Ataxonomy%2520allows%2520us%2520to%2520re-examine%2520graph%2520models%2520based%2520on%2520new%2520architectures%2520more%250Aclearly%252C%2520such%2520as%2520graph%2520language%2520models%252C%2520as%2520well%2520as%2520provide%2520more%2520in-depth%250Ainsights%2520for%2520constructing%2520GFMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16137v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Self-Supervised%20Graph%20Foundation%20Models%3A%20Knowledge-Based%0A%20%20Perspective&entry.906535625=Ziwen%20Zhao%20and%20Yixin%20Su%20and%20Yuhua%20Li%20and%20Yixiong%20Zou%20and%20Ruixuan%20Li%20and%20Rui%20Zhang&entry.1292438233=%20%20Graph%20self-supervised%20learning%20%28SSL%29%20is%20now%20a%20go-to%20method%20for%20pre-training%0Agraph%20foundation%20models%20%28GFMs%29.%20There%20is%20a%20wide%20variety%20of%20knowledge%20patterns%0Aembedded%20in%20the%20graph%20data%2C%20such%20as%20node%20properties%20and%20clusters%2C%20which%20are%0Acrucial%20to%20learning%20generalized%20representations%20for%20GFMs.%20However%2C%20existing%0Asurveys%20of%20GFMs%20have%20several%20shortcomings%3A%20they%20lack%20comprehensiveness%0Aregarding%20the%20most%20recent%20progress%2C%20have%20unclear%20categorization%20of%0Aself-supervised%20methods%2C%20and%20take%20a%20limited%20architecture-based%20perspective%20that%0Ais%20restricted%20to%20only%20certain%20types%20of%20graph%20models.%20As%20the%20ultimate%20goal%20of%0AGFMs%20is%20to%20learn%20generalized%20graph%20knowledge%2C%20we%20provide%20a%20comprehensive%20survey%0Aof%20self-supervised%20GFMs%20from%20a%20novel%20knowledge-based%20perspective.%20We%20propose%20a%0Aknowledge-based%20taxonomy%2C%20which%20categorizes%20self-supervised%20graph%20models%20by%20the%0Aspecific%20graph%20knowledge%20utilized.%20Our%20taxonomy%20consists%20of%20microscopic%20%28nodes%2C%0Alinks%2C%20etc.%29%2C%20mesoscopic%20%28context%2C%20clusters%2C%20etc.%29%2C%20and%20macroscopic%20knowledge%0A%28global%20structure%2C%20manifolds%2C%20etc.%29.%20It%20covers%20a%20total%20of%209%20knowledge%0Acategories%20and%20more%20than%2025%20pretext%20tasks%20for%20pre-training%20GFMs%2C%20as%20well%20as%0Avarious%20downstream%20task%20generalization%20strategies.%20Such%20a%20knowledge-based%0Ataxonomy%20allows%20us%20to%20re-examine%20graph%20models%20based%20on%20new%20architectures%20more%0Aclearly%2C%20such%20as%20graph%20language%20models%2C%20as%20well%20as%20provide%20more%20in-depth%0Ainsights%20for%20constructing%20GFMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16137v2&entry.124074799=Read"},
{"title": "The Llama 3 Herd of Models", "author": "Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aurelien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and Charlotte Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Canton Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Frank Zhang and Gabriel Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Graeme Nail and Gregoire Mialon and Guan Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel Kloumann and Ishan Misra and Ivan Evtimov and Jade Copet and Jaewon Lee and Jan Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and Jelmer van der Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Junteng Jia and Kalyan Vasuden Alwala and Kartikeya Upasani and Kate Plawiak and Ke Li and Kenneth Heafield and Kevin Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuenley Chiu and Kunal Bhalla and Lauren Rantala-Yeary and Laurens van der Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and Mahesh Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Mathew Oldham and Mathieu Rita and Maya Pavlova and Melanie Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Nikolay Bashlykov and Nikolay Bogoychev and Niladri Chatterji and Olivier Duchenne and Onur \u00c7elebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasic and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohit Girdhar and Rohit Patel and Romain Sauvestre and Ronnie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and Saghar Hosseini and Sahana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and Sharath Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and Sydney Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Virginie Do and Vish Vogeti and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whitney Meers and Xavier Martinet and Xiaodong Wang and Xiaoqing Ellen Tan and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yi Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zheng Yan and Zhengxing Chen and Zoe Papakipos and Aaditya Singh and Aaron Grattafiori and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adithya Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alex Vaughan and Alexei Baevski and Allie Feinstein and Amanda Kallet and Amit Sangani and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Franco and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Benjamin Leonhardi and Bernie Huang and Beth Loyd and Beto De Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Damon Civin and Dana Beaty and Daniel Kreymer and Daniel Li and Danny Wyatt and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Firat Ozgenel and Francesco Caggioni and Francisco Guzm\u00e1n and Frank Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and Govind Thattai and Grant Herman and Grigory Sizov and  Guangyi and  Zhang and Guna Lakshminarayanan and Hamid Shojanazeri and Han Zou and Hannah Wang and Hanwen Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Igor Molybog and Igor Tufanov and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kai Wu and Kam Hou U and Karan Saxena and Karthik Prasad and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and Kaushik Veeraraghavan and Kelly Michelena and Keqian Li and Kun Huang and Kunal Chawla and Kushal Lakhotia and Kyle Huang and Lailin Chen and Lakshya Garg and Lavender A and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and Maria Tsimpoukelli and Martynas Mankus and Matan Hasson and Matthew Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and Michael L. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Munish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navyata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikolay Pavlovich Laptev and Ning Dong and Ning Zhang and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pedro Rittner and Philip Bontrager and Pierre Roux and Piotr Dollar and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Rohan Maheswari and Russ Howes and Ruty Rinott and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Shaun Lindsay and Sheng Feng and Shenghao Lin and Shengxin Cindy Zha and Shiva Shankar and Shuqiang Zhang and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and Soji Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Gupta and Sungmin Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and Tal Remez and Tamar Glaser and Tamara Best and Thilo Kohler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and Vlad Ionescu and Vlad Poenaru and Vlad Tiberiu Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xiaocheng Tang and Xiaofang Wang and Xiaojian Wu and Xiaolan Wang and Xide Xia and Xilun Wu and Xinbo Gao and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and  Yu and  Wang and Yuchen Hao and Yundi Qian and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao", "abstract": "  Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment.\n", "link": "http://arxiv.org/abs/2407.21783v1", "date": "2024-07-31", "relevancy": 1.9476, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.531}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.469}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.45}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Llama%203%20Herd%20of%20Models&body=Title%3A%20The%20Llama%203%20Herd%20of%20Models%0AAuthor%3A%20Abhimanyu%20Dubey%20and%20Abhinav%20Jauhri%20and%20Abhinav%20Pandey%20and%20Abhishek%20Kadian%20and%20Ahmad%20Al-Dahle%20and%20Aiesha%20Letman%20and%20Akhil%20Mathur%20and%20Alan%20Schelten%20and%20Amy%20Yang%20and%20Angela%20Fan%20and%20Anirudh%20Goyal%20and%20Anthony%20Hartshorn%20and%20Aobo%20Yang%20and%20Archi%20Mitra%20and%20Archie%20Sravankumar%20and%20Artem%20Korenev%20and%20Arthur%20Hinsvark%20and%20Arun%20Rao%20and%20Aston%20Zhang%20and%20Aurelien%20Rodriguez%20and%20Austen%20Gregerson%20and%20Ava%20Spataru%20and%20Baptiste%20Roziere%20and%20Bethany%20Biron%20and%20Binh%20Tang%20and%20Bobbie%20Chern%20and%20Charlotte%20Caucheteux%20and%20Chaya%20Nayak%20and%20Chloe%20Bi%20and%20Chris%20Marra%20and%20Chris%20McConnell%20and%20Christian%20Keller%20and%20Christophe%20Touret%20and%20Chunyang%20Wu%20and%20Corinne%20Wong%20and%20Cristian%20Canton%20Ferrer%20and%20Cyrus%20Nikolaidis%20and%20Damien%20Allonsius%20and%20Daniel%20Song%20and%20Danielle%20Pintz%20and%20Danny%20Livshits%20and%20David%20Esiobu%20and%20Dhruv%20Choudhary%20and%20Dhruv%20Mahajan%20and%20Diego%20Garcia-Olano%20and%20Diego%20Perino%20and%20Dieuwke%20Hupkes%20and%20Egor%20Lakomkin%20and%20Ehab%20AlBadawy%20and%20Elina%20Lobanova%20and%20Emily%20Dinan%20and%20Eric%20Michael%20Smith%20and%20Filip%20Radenovic%20and%20Frank%20Zhang%20and%20Gabriel%20Synnaeve%20and%20Gabrielle%20Lee%20and%20Georgia%20Lewis%20Anderson%20and%20Graeme%20Nail%20and%20Gregoire%20Mialon%20and%20Guan%20Pang%20and%20Guillem%20Cucurell%20and%20Hailey%20Nguyen%20and%20Hannah%20Korevaar%20and%20Hu%20Xu%20and%20Hugo%20Touvron%20and%20Iliyan%20Zarov%20and%20Imanol%20Arrieta%20Ibarra%20and%20Isabel%20Kloumann%20and%20Ishan%20Misra%20and%20Ivan%20Evtimov%20and%20Jade%20Copet%20and%20Jaewon%20Lee%20and%20Jan%20Geffert%20and%20Jana%20Vranes%20and%20Jason%20Park%20and%20Jay%20Mahadeokar%20and%20Jeet%20Shah%20and%20Jelmer%20van%20der%20Linde%20and%20Jennifer%20Billock%20and%20Jenny%20Hong%20and%20Jenya%20Lee%20and%20Jeremy%20Fu%20and%20Jianfeng%20Chi%20and%20Jianyu%20Huang%20and%20Jiawen%20Liu%20and%20Jie%20Wang%20and%20Jiecao%20Yu%20and%20Joanna%20Bitton%20and%20Joe%20Spisak%20and%20Jongsoo%20Park%20and%20Joseph%20Rocca%20and%20Joshua%20Johnstun%20and%20Joshua%20Saxe%20and%20Junteng%20Jia%20and%20Kalyan%20Vasuden%20Alwala%20and%20Kartikeya%20Upasani%20and%20Kate%20Plawiak%20and%20Ke%20Li%20and%20Kenneth%20Heafield%20and%20Kevin%20Stone%20and%20Khalid%20El-Arini%20and%20Krithika%20Iyer%20and%20Kshitiz%20Malik%20and%20Kuenley%20Chiu%20and%20Kunal%20Bhalla%20and%20Lauren%20Rantala-Yeary%20and%20Laurens%20van%20der%20Maaten%20and%20Lawrence%20Chen%20and%20Liang%20Tan%20and%20Liz%20Jenkins%20and%20Louis%20Martin%20and%20Lovish%20Madaan%20and%20Lubo%20Malo%20and%20Lukas%20Blecher%20and%20Lukas%20Landzaat%20and%20Luke%20de%20Oliveira%20and%20Madeline%20Muzzi%20and%20Mahesh%20Pasupuleti%20and%20Mannat%20Singh%20and%20Manohar%20Paluri%20and%20Marcin%20Kardas%20and%20Mathew%20Oldham%20and%20Mathieu%20Rita%20and%20Maya%20Pavlova%20and%20Melanie%20Kambadur%20and%20Mike%20Lewis%20and%20Min%20Si%20and%20Mitesh%20Kumar%20Singh%20and%20Mona%20Hassan%20and%20Naman%20Goyal%20and%20Narjes%20Torabi%20and%20Nikolay%20Bashlykov%20and%20Nikolay%20Bogoychev%20and%20Niladri%20Chatterji%20and%20Olivier%20Duchenne%20and%20Onur%20%C3%87elebi%20and%20Patrick%20Alrassy%20and%20Pengchuan%20Zhang%20and%20Pengwei%20Li%20and%20Petar%20Vasic%20and%20Peter%20Weng%20and%20Prajjwal%20Bhargava%20and%20Pratik%20Dubal%20and%20Praveen%20Krishnan%20and%20Punit%20Singh%20Koura%20and%20Puxin%20Xu%20and%20Qing%20He%20and%20Qingxiao%20Dong%20and%20Ragavan%20Srinivasan%20and%20Raj%20Ganapathy%20and%20Ramon%20Calderer%20and%20Ricardo%20Silveira%20Cabral%20and%20Robert%20Stojnic%20and%20Roberta%20Raileanu%20and%20Rohit%20Girdhar%20and%20Rohit%20Patel%20and%20Romain%20Sauvestre%20and%20Ronnie%20Polidoro%20and%20Roshan%20Sumbaly%20and%20Ross%20Taylor%20and%20Ruan%20Silva%20and%20Rui%20Hou%20and%20Rui%20Wang%20and%20Saghar%20Hosseini%20and%20Sahana%20Chennabasappa%20and%20Sanjay%20Singh%20and%20Sean%20Bell%20and%20Seohyun%20Sonia%20Kim%20and%20Sergey%20Edunov%20and%20Shaoliang%20Nie%20and%20Sharan%20Narang%20and%20Sharath%20Raparthy%20and%20Sheng%20Shen%20and%20Shengye%20Wan%20and%20Shruti%20Bhosale%20and%20Shun%20Zhang%20and%20Simon%20Vandenhende%20and%20Soumya%20Batra%20and%20Spencer%20Whitman%20and%20Sten%20Sootla%20and%20Stephane%20Collot%20and%20Suchin%20Gururangan%20and%20Sydney%20Borodinsky%20and%20Tamar%20Herman%20and%20Tara%20Fowler%20and%20Tarek%20Sheasha%20and%20Thomas%20Georgiou%20and%20Thomas%20Scialom%20and%20Tobias%20Speckbacher%20and%20Todor%20Mihaylov%20and%20Tong%20Xiao%20and%20Ujjwal%20Karn%20and%20Vedanuj%20Goswami%20and%20Vibhor%20Gupta%20and%20Vignesh%20Ramanathan%20and%20Viktor%20Kerkez%20and%20Vincent%20Gonguet%20and%20Virginie%20Do%20and%20Vish%20Vogeti%20and%20Vladan%20Petrovic%20and%20Weiwei%20Chu%20and%20Wenhan%20Xiong%20and%20Wenyin%20Fu%20and%20Whitney%20Meers%20and%20Xavier%20Martinet%20and%20Xiaodong%20Wang%20and%20Xiaoqing%20Ellen%20Tan%20and%20Xinfeng%20Xie%20and%20Xuchao%20Jia%20and%20Xuewei%20Wang%20and%20Yaelle%20Goldschlag%20and%20Yashesh%20Gaur%20and%20Yasmine%20Babaei%20and%20Yi%20Wen%20and%20Yiwen%20Song%20and%20Yuchen%20Zhang%20and%20Yue%20Li%20and%20Yuning%20Mao%20and%20Zacharie%20Delpierre%20Coudert%20and%20Zheng%20Yan%20and%20Zhengxing%20Chen%20and%20Zoe%20Papakipos%20and%20Aaditya%20Singh%20and%20Aaron%20Grattafiori%20and%20Abha%20Jain%20and%20Adam%20Kelsey%20and%20Adam%20Shajnfeld%20and%20Adithya%20Gangidi%20and%20Adolfo%20Victoria%20and%20Ahuva%20Goldstand%20and%20Ajay%20Menon%20and%20Ajay%20Sharma%20and%20Alex%20Boesenberg%20and%20Alex%20Vaughan%20and%20Alexei%20Baevski%20and%20Allie%20Feinstein%20and%20Amanda%20Kallet%20and%20Amit%20Sangani%20and%20Anam%20Yunus%20and%20Andrei%20Lupu%20and%20Andres%20Alvarado%20and%20Andrew%20Caples%20and%20Andrew%20Gu%20and%20Andrew%20Ho%20and%20Andrew%20Poulton%20and%20Andrew%20Ryan%20and%20Ankit%20Ramchandani%20and%20Annie%20Franco%20and%20Aparajita%20Saraf%20and%20Arkabandhu%20Chowdhury%20and%20Ashley%20Gabriel%20and%20Ashwin%20Bharambe%20and%20Assaf%20Eisenman%20and%20Azadeh%20Yazdan%20and%20Beau%20James%20and%20Ben%20Maurer%20and%20Benjamin%20Leonhardi%20and%20Bernie%20Huang%20and%20Beth%20Loyd%20and%20Beto%20De%20Paola%20and%20Bhargavi%20Paranjape%20and%20Bing%20Liu%20and%20Bo%20Wu%20and%20Boyu%20Ni%20and%20Braden%20Hancock%20and%20Bram%20Wasti%20and%20Brandon%20Spence%20and%20Brani%20Stojkovic%20and%20Brian%20Gamido%20and%20Britt%20Montalvo%20and%20Carl%20Parker%20and%20Carly%20Burton%20and%20Catalina%20Mejia%20and%20Changhan%20Wang%20and%20Changkyu%20Kim%20and%20Chao%20Zhou%20and%20Chester%20Hu%20and%20Ching-Hsiang%20Chu%20and%20Chris%20Cai%20and%20Chris%20Tindal%20and%20Christoph%20Feichtenhofer%20and%20Damon%20Civin%20and%20Dana%20Beaty%20and%20Daniel%20Kreymer%20and%20Daniel%20Li%20and%20Danny%20Wyatt%20and%20David%20Adkins%20and%20David%20Xu%20and%20Davide%20Testuggine%20and%20Delia%20David%20and%20Devi%20Parikh%20and%20Diana%20Liskovich%20and%20Didem%20Foss%20and%20Dingkang%20Wang%20and%20Duc%20Le%20and%20Dustin%20Holland%20and%20Edward%20Dowling%20and%20Eissa%20Jamil%20and%20Elaine%20Montgomery%20and%20Eleonora%20Presani%20and%20Emily%20Hahn%20and%20Emily%20Wood%20and%20Erik%20Brinkman%20and%20Esteban%20Arcaute%20and%20Evan%20Dunbar%20and%20Evan%20Smothers%20and%20Fei%20Sun%20and%20Felix%20Kreuk%20and%20Feng%20Tian%20and%20Firat%20Ozgenel%20and%20Francesco%20Caggioni%20and%20Francisco%20Guzm%C3%A1n%20and%20Frank%20Kanayet%20and%20Frank%20Seide%20and%20Gabriela%20Medina%20Florez%20and%20Gabriella%20Schwarz%20and%20Gada%20Badeer%20and%20Georgia%20Swee%20and%20Gil%20Halpern%20and%20Govind%20Thattai%20and%20Grant%20Herman%20and%20Grigory%20Sizov%20and%20%20Guangyi%20and%20%20Zhang%20and%20Guna%20Lakshminarayanan%20and%20Hamid%20Shojanazeri%20and%20Han%20Zou%20and%20Hannah%20Wang%20and%20Hanwen%20Zha%20and%20Haroun%20Habeeb%20and%20Harrison%20Rudolph%20and%20Helen%20Suk%20and%20Henry%20Aspegren%20and%20Hunter%20Goldman%20and%20Igor%20Molybog%20and%20Igor%20Tufanov%20and%20Irina-Elena%20Veliche%20and%20Itai%20Gat%20and%20Jake%20Weissman%20and%20James%20Geboski%20and%20James%20Kohli%20and%20Japhet%20Asher%20and%20Jean-Baptiste%20Gaya%20and%20Jeff%20Marcus%20and%20Jeff%20Tang%20and%20Jennifer%20Chan%20and%20Jenny%20Zhen%20and%20Jeremy%20Reizenstein%20and%20Jeremy%20Teboul%20and%20Jessica%20Zhong%20and%20Jian%20Jin%20and%20Jingyi%20Yang%20and%20Joe%20Cummings%20and%20Jon%20Carvill%20and%20Jon%20Shepard%20and%20Jonathan%20McPhie%20and%20Jonathan%20Torres%20and%20Josh%20Ginsburg%20and%20Junjie%20Wang%20and%20Kai%20Wu%20and%20Kam%20Hou%20U%20and%20Karan%20Saxena%20and%20Karthik%20Prasad%20and%20Kartikay%20Khandelwal%20and%20Katayoun%20Zand%20and%20Kathy%20Matosich%20and%20Kaushik%20Veeraraghavan%20and%20Kelly%20Michelena%20and%20Keqian%20Li%20and%20Kun%20Huang%20and%20Kunal%20Chawla%20and%20Kushal%20Lakhotia%20and%20Kyle%20Huang%20and%20Lailin%20Chen%20and%20Lakshya%20Garg%20and%20Lavender%20A%20and%20Leandro%20Silva%20and%20Lee%20Bell%20and%20Lei%20Zhang%20and%20Liangpeng%20Guo%20and%20Licheng%20Yu%20and%20Liron%20Moshkovich%20and%20Luca%20Wehrstedt%20and%20Madian%20Khabsa%20and%20Manav%20Avalani%20and%20Manish%20Bhatt%20and%20Maria%20Tsimpoukelli%20and%20Martynas%20Mankus%20and%20Matan%20Hasson%20and%20Matthew%20Lennie%20and%20Matthias%20Reso%20and%20Maxim%20Groshev%20and%20Maxim%20Naumov%20and%20Maya%20Lathi%20and%20Meghan%20Keneally%20and%20Michael%20L.%20Seltzer%20and%20Michal%20Valko%20and%20Michelle%20Restrepo%20and%20Mihir%20Patel%20and%20Mik%20Vyatskov%20and%20Mikayel%20Samvelyan%20and%20Mike%20Clark%20and%20Mike%20Macey%20and%20Mike%20Wang%20and%20Miquel%20Jubert%20Hermoso%20and%20Mo%20Metanat%20and%20Mohammad%20Rastegari%20and%20Munish%20Bansal%20and%20Nandhini%20Santhanam%20and%20Natascha%20Parks%20and%20Natasha%20White%20and%20Navyata%20Bawa%20and%20Nayan%20Singhal%20and%20Nick%20Egebo%20and%20Nicolas%20Usunier%20and%20Nikolay%20Pavlovich%20Laptev%20and%20Ning%20Dong%20and%20Ning%20Zhang%20and%20Norman%20Cheng%20and%20Oleg%20Chernoguz%20and%20Olivia%20Hart%20and%20Omkar%20Salpekar%20and%20Ozlem%20Kalinli%20and%20Parkin%20Kent%20and%20Parth%20Parekh%20and%20Paul%20Saab%20and%20Pavan%20Balaji%20and%20Pedro%20Rittner%20and%20Philip%20Bontrager%20and%20Pierre%20Roux%20and%20Piotr%20Dollar%20and%20Polina%20Zvyagina%20and%20Prashant%20Ratanchandani%20and%20Pritish%20Yuvraj%20and%20Qian%20Liang%20and%20Rachad%20Alao%20and%20Rachel%20Rodriguez%20and%20Rafi%20Ayub%20and%20Raghotham%20Murthy%20and%20Raghu%20Nayani%20and%20Rahul%20Mitra%20and%20Raymond%20Li%20and%20Rebekkah%20Hogan%20and%20Robin%20Battey%20and%20Rocky%20Wang%20and%20Rohan%20Maheswari%20and%20Russ%20Howes%20and%20Ruty%20Rinott%20and%20Sai%20Jayesh%20Bondu%20and%20Samyak%20Datta%20and%20Sara%20Chugh%20and%20Sara%20Hunt%20and%20Sargun%20Dhillon%20and%20Sasha%20Sidorov%20and%20Satadru%20Pan%20and%20Saurabh%20Verma%20and%20Seiji%20Yamamoto%20and%20Sharadh%20Ramaswamy%20and%20Shaun%20Lindsay%20and%20Shaun%20Lindsay%20and%20Sheng%20Feng%20and%20Shenghao%20Lin%20and%20Shengxin%20Cindy%20Zha%20and%20Shiva%20Shankar%20and%20Shuqiang%20Zhang%20and%20Shuqiang%20Zhang%20and%20Sinong%20Wang%20and%20Sneha%20Agarwal%20and%20Soji%20Sajuyigbe%20and%20Soumith%20Chintala%20and%20Stephanie%20Max%20and%20Stephen%20Chen%20and%20Steve%20Kehoe%20and%20Steve%20Satterfield%20and%20Sudarshan%20Govindaprasad%20and%20Sumit%20Gupta%20and%20Sungmin%20Cho%20and%20Sunny%20Virk%20and%20Suraj%20Subramanian%20and%20Sy%20Choudhury%20and%20Sydney%20Goldman%20and%20Tal%20Remez%20and%20Tamar%20Glaser%20and%20Tamara%20Best%20and%20Thilo%20Kohler%20and%20Thomas%20Robinson%20and%20Tianhe%20Li%20and%20Tianjun%20Zhang%20and%20Tim%20Matthews%20and%20Timothy%20Chou%20and%20Tzook%20Shaked%20and%20Varun%20Vontimitta%20and%20Victoria%20Ajayi%20and%20Victoria%20Montanez%20and%20Vijai%20Mohan%20and%20Vinay%20Satish%20Kumar%20and%20Vishal%20Mangla%20and%20Vlad%20Ionescu%20and%20Vlad%20Poenaru%20and%20Vlad%20Tiberiu%20Mihailescu%20and%20Vladimir%20Ivanov%20and%20Wei%20Li%20and%20Wenchen%20Wang%20and%20Wenwen%20Jiang%20and%20Wes%20Bouaziz%20and%20Will%20Constable%20and%20Xiaocheng%20Tang%20and%20Xiaofang%20Wang%20and%20Xiaojian%20Wu%20and%20Xiaolan%20Wang%20and%20Xide%20Xia%20and%20Xilun%20Wu%20and%20Xinbo%20Gao%20and%20Yanjun%20Chen%20and%20Ye%20Hu%20and%20Ye%20Jia%20and%20Ye%20Qi%20and%20Yenda%20Li%20and%20Yilin%20Zhang%20and%20Ying%20Zhang%20and%20Yossi%20Adi%20and%20Youngjin%20Nam%20and%20%20Yu%20and%20%20Wang%20and%20Yuchen%20Hao%20and%20Yundi%20Qian%20and%20Yuzi%20He%20and%20Zach%20Rait%20and%20Zachary%20DeVito%20and%20Zef%20Rosnbrick%20and%20Zhaoduo%20Wen%20and%20Zhenyu%20Yang%20and%20Zhiwei%20Zhao%0AAbstract%3A%20%20%20Modern%20artificial%20intelligence%20%28AI%29%20systems%20are%20powered%20by%20foundation%20models.%0AThis%20paper%20presents%20a%20new%20set%20of%20foundation%20models%2C%20called%20Llama%203.%20It%20is%20a%0Aherd%20of%20language%20models%20that%20natively%20support%20multilinguality%2C%20coding%2C%0Areasoning%2C%20and%20tool%20usage.%20Our%20largest%20model%20is%20a%20dense%20Transformer%20with%20405B%0Aparameters%20and%20a%20context%20window%20of%20up%20to%20128K%20tokens.%20This%20paper%20presents%20an%0Aextensive%20empirical%20evaluation%20of%20Llama%203.%20We%20find%20that%20Llama%203%20delivers%0Acomparable%20quality%20to%20leading%20language%20models%20such%20as%20GPT-4%20on%20a%20plethora%20of%0Atasks.%20We%20publicly%20release%20Llama%203%2C%20including%20pre-trained%20and%20post-trained%0Aversions%20of%20the%20405B%20parameter%20language%20model%20and%20our%20Llama%20Guard%203%20model%20for%0Ainput%20and%20output%20safety.%20The%20paper%20also%20presents%20the%20results%20of%20experiments%20in%0Awhich%20we%20integrate%20image%2C%20video%2C%20and%20speech%20capabilities%20into%20Llama%203%20via%20a%0Acompositional%20approach.%20We%20observe%20this%20approach%20performs%20competitively%20with%0Athe%20state-of-the-art%20on%20image%2C%20video%2C%20and%20speech%20recognition%20tasks.%20The%0Aresulting%20models%20are%20not%20yet%20being%20broadly%20released%20as%20they%20are%20still%20under%0Adevelopment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21783v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Llama%25203%2520Herd%2520of%2520Models%26entry.906535625%3DAbhimanyu%2520Dubey%2520and%2520Abhinav%2520Jauhri%2520and%2520Abhinav%2520Pandey%2520and%2520Abhishek%2520Kadian%2520and%2520Ahmad%2520Al-Dahle%2520and%2520Aiesha%2520Letman%2520and%2520Akhil%2520Mathur%2520and%2520Alan%2520Schelten%2520and%2520Amy%2520Yang%2520and%2520Angela%2520Fan%2520and%2520Anirudh%2520Goyal%2520and%2520Anthony%2520Hartshorn%2520and%2520Aobo%2520Yang%2520and%2520Archi%2520Mitra%2520and%2520Archie%2520Sravankumar%2520and%2520Artem%2520Korenev%2520and%2520Arthur%2520Hinsvark%2520and%2520Arun%2520Rao%2520and%2520Aston%2520Zhang%2520and%2520Aurelien%2520Rodriguez%2520and%2520Austen%2520Gregerson%2520and%2520Ava%2520Spataru%2520and%2520Baptiste%2520Roziere%2520and%2520Bethany%2520Biron%2520and%2520Binh%2520Tang%2520and%2520Bobbie%2520Chern%2520and%2520Charlotte%2520Caucheteux%2520and%2520Chaya%2520Nayak%2520and%2520Chloe%2520Bi%2520and%2520Chris%2520Marra%2520and%2520Chris%2520McConnell%2520and%2520Christian%2520Keller%2520and%2520Christophe%2520Touret%2520and%2520Chunyang%2520Wu%2520and%2520Corinne%2520Wong%2520and%2520Cristian%2520Canton%2520Ferrer%2520and%2520Cyrus%2520Nikolaidis%2520and%2520Damien%2520Allonsius%2520and%2520Daniel%2520Song%2520and%2520Danielle%2520Pintz%2520and%2520Danny%2520Livshits%2520and%2520David%2520Esiobu%2520and%2520Dhruv%2520Choudhary%2520and%2520Dhruv%2520Mahajan%2520and%2520Diego%2520Garcia-Olano%2520and%2520Diego%2520Perino%2520and%2520Dieuwke%2520Hupkes%2520and%2520Egor%2520Lakomkin%2520and%2520Ehab%2520AlBadawy%2520and%2520Elina%2520Lobanova%2520and%2520Emily%2520Dinan%2520and%2520Eric%2520Michael%2520Smith%2520and%2520Filip%2520Radenovic%2520and%2520Frank%2520Zhang%2520and%2520Gabriel%2520Synnaeve%2520and%2520Gabrielle%2520Lee%2520and%2520Georgia%2520Lewis%2520Anderson%2520and%2520Graeme%2520Nail%2520and%2520Gregoire%2520Mialon%2520and%2520Guan%2520Pang%2520and%2520Guillem%2520Cucurell%2520and%2520Hailey%2520Nguyen%2520and%2520Hannah%2520Korevaar%2520and%2520Hu%2520Xu%2520and%2520Hugo%2520Touvron%2520and%2520Iliyan%2520Zarov%2520and%2520Imanol%2520Arrieta%2520Ibarra%2520and%2520Isabel%2520Kloumann%2520and%2520Ishan%2520Misra%2520and%2520Ivan%2520Evtimov%2520and%2520Jade%2520Copet%2520and%2520Jaewon%2520Lee%2520and%2520Jan%2520Geffert%2520and%2520Jana%2520Vranes%2520and%2520Jason%2520Park%2520and%2520Jay%2520Mahadeokar%2520and%2520Jeet%2520Shah%2520and%2520Jelmer%2520van%2520der%2520Linde%2520and%2520Jennifer%2520Billock%2520and%2520Jenny%2520Hong%2520and%2520Jenya%2520Lee%2520and%2520Jeremy%2520Fu%2520and%2520Jianfeng%2520Chi%2520and%2520Jianyu%2520Huang%2520and%2520Jiawen%2520Liu%2520and%2520Jie%2520Wang%2520and%2520Jiecao%2520Yu%2520and%2520Joanna%2520Bitton%2520and%2520Joe%2520Spisak%2520and%2520Jongsoo%2520Park%2520and%2520Joseph%2520Rocca%2520and%2520Joshua%2520Johnstun%2520and%2520Joshua%2520Saxe%2520and%2520Junteng%2520Jia%2520and%2520Kalyan%2520Vasuden%2520Alwala%2520and%2520Kartikeya%2520Upasani%2520and%2520Kate%2520Plawiak%2520and%2520Ke%2520Li%2520and%2520Kenneth%2520Heafield%2520and%2520Kevin%2520Stone%2520and%2520Khalid%2520El-Arini%2520and%2520Krithika%2520Iyer%2520and%2520Kshitiz%2520Malik%2520and%2520Kuenley%2520Chiu%2520and%2520Kunal%2520Bhalla%2520and%2520Lauren%2520Rantala-Yeary%2520and%2520Laurens%2520van%2520der%2520Maaten%2520and%2520Lawrence%2520Chen%2520and%2520Liang%2520Tan%2520and%2520Liz%2520Jenkins%2520and%2520Louis%2520Martin%2520and%2520Lovish%2520Madaan%2520and%2520Lubo%2520Malo%2520and%2520Lukas%2520Blecher%2520and%2520Lukas%2520Landzaat%2520and%2520Luke%2520de%2520Oliveira%2520and%2520Madeline%2520Muzzi%2520and%2520Mahesh%2520Pasupuleti%2520and%2520Mannat%2520Singh%2520and%2520Manohar%2520Paluri%2520and%2520Marcin%2520Kardas%2520and%2520Mathew%2520Oldham%2520and%2520Mathieu%2520Rita%2520and%2520Maya%2520Pavlova%2520and%2520Melanie%2520Kambadur%2520and%2520Mike%2520Lewis%2520and%2520Min%2520Si%2520and%2520Mitesh%2520Kumar%2520Singh%2520and%2520Mona%2520Hassan%2520and%2520Naman%2520Goyal%2520and%2520Narjes%2520Torabi%2520and%2520Nikolay%2520Bashlykov%2520and%2520Nikolay%2520Bogoychev%2520and%2520Niladri%2520Chatterji%2520and%2520Olivier%2520Duchenne%2520and%2520Onur%2520%25C3%2587elebi%2520and%2520Patrick%2520Alrassy%2520and%2520Pengchuan%2520Zhang%2520and%2520Pengwei%2520Li%2520and%2520Petar%2520Vasic%2520and%2520Peter%2520Weng%2520and%2520Prajjwal%2520Bhargava%2520and%2520Pratik%2520Dubal%2520and%2520Praveen%2520Krishnan%2520and%2520Punit%2520Singh%2520Koura%2520and%2520Puxin%2520Xu%2520and%2520Qing%2520He%2520and%2520Qingxiao%2520Dong%2520and%2520Ragavan%2520Srinivasan%2520and%2520Raj%2520Ganapathy%2520and%2520Ramon%2520Calderer%2520and%2520Ricardo%2520Silveira%2520Cabral%2520and%2520Robert%2520Stojnic%2520and%2520Roberta%2520Raileanu%2520and%2520Rohit%2520Girdhar%2520and%2520Rohit%2520Patel%2520and%2520Romain%2520Sauvestre%2520and%2520Ronnie%2520Polidoro%2520and%2520Roshan%2520Sumbaly%2520and%2520Ross%2520Taylor%2520and%2520Ruan%2520Silva%2520and%2520Rui%2520Hou%2520and%2520Rui%2520Wang%2520and%2520Saghar%2520Hosseini%2520and%2520Sahana%2520Chennabasappa%2520and%2520Sanjay%2520Singh%2520and%2520Sean%2520Bell%2520and%2520Seohyun%2520Sonia%2520Kim%2520and%2520Sergey%2520Edunov%2520and%2520Shaoliang%2520Nie%2520and%2520Sharan%2520Narang%2520and%2520Sharath%2520Raparthy%2520and%2520Sheng%2520Shen%2520and%2520Shengye%2520Wan%2520and%2520Shruti%2520Bhosale%2520and%2520Shun%2520Zhang%2520and%2520Simon%2520Vandenhende%2520and%2520Soumya%2520Batra%2520and%2520Spencer%2520Whitman%2520and%2520Sten%2520Sootla%2520and%2520Stephane%2520Collot%2520and%2520Suchin%2520Gururangan%2520and%2520Sydney%2520Borodinsky%2520and%2520Tamar%2520Herman%2520and%2520Tara%2520Fowler%2520and%2520Tarek%2520Sheasha%2520and%2520Thomas%2520Georgiou%2520and%2520Thomas%2520Scialom%2520and%2520Tobias%2520Speckbacher%2520and%2520Todor%2520Mihaylov%2520and%2520Tong%2520Xiao%2520and%2520Ujjwal%2520Karn%2520and%2520Vedanuj%2520Goswami%2520and%2520Vibhor%2520Gupta%2520and%2520Vignesh%2520Ramanathan%2520and%2520Viktor%2520Kerkez%2520and%2520Vincent%2520Gonguet%2520and%2520Virginie%2520Do%2520and%2520Vish%2520Vogeti%2520and%2520Vladan%2520Petrovic%2520and%2520Weiwei%2520Chu%2520and%2520Wenhan%2520Xiong%2520and%2520Wenyin%2520Fu%2520and%2520Whitney%2520Meers%2520and%2520Xavier%2520Martinet%2520and%2520Xiaodong%2520Wang%2520and%2520Xiaoqing%2520Ellen%2520Tan%2520and%2520Xinfeng%2520Xie%2520and%2520Xuchao%2520Jia%2520and%2520Xuewei%2520Wang%2520and%2520Yaelle%2520Goldschlag%2520and%2520Yashesh%2520Gaur%2520and%2520Yasmine%2520Babaei%2520and%2520Yi%2520Wen%2520and%2520Yiwen%2520Song%2520and%2520Yuchen%2520Zhang%2520and%2520Yue%2520Li%2520and%2520Yuning%2520Mao%2520and%2520Zacharie%2520Delpierre%2520Coudert%2520and%2520Zheng%2520Yan%2520and%2520Zhengxing%2520Chen%2520and%2520Zoe%2520Papakipos%2520and%2520Aaditya%2520Singh%2520and%2520Aaron%2520Grattafiori%2520and%2520Abha%2520Jain%2520and%2520Adam%2520Kelsey%2520and%2520Adam%2520Shajnfeld%2520and%2520Adithya%2520Gangidi%2520and%2520Adolfo%2520Victoria%2520and%2520Ahuva%2520Goldstand%2520and%2520Ajay%2520Menon%2520and%2520Ajay%2520Sharma%2520and%2520Alex%2520Boesenberg%2520and%2520Alex%2520Vaughan%2520and%2520Alexei%2520Baevski%2520and%2520Allie%2520Feinstein%2520and%2520Amanda%2520Kallet%2520and%2520Amit%2520Sangani%2520and%2520Anam%2520Yunus%2520and%2520Andrei%2520Lupu%2520and%2520Andres%2520Alvarado%2520and%2520Andrew%2520Caples%2520and%2520Andrew%2520Gu%2520and%2520Andrew%2520Ho%2520and%2520Andrew%2520Poulton%2520and%2520Andrew%2520Ryan%2520and%2520Ankit%2520Ramchandani%2520and%2520Annie%2520Franco%2520and%2520Aparajita%2520Saraf%2520and%2520Arkabandhu%2520Chowdhury%2520and%2520Ashley%2520Gabriel%2520and%2520Ashwin%2520Bharambe%2520and%2520Assaf%2520Eisenman%2520and%2520Azadeh%2520Yazdan%2520and%2520Beau%2520James%2520and%2520Ben%2520Maurer%2520and%2520Benjamin%2520Leonhardi%2520and%2520Bernie%2520Huang%2520and%2520Beth%2520Loyd%2520and%2520Beto%2520De%2520Paola%2520and%2520Bhargavi%2520Paranjape%2520and%2520Bing%2520Liu%2520and%2520Bo%2520Wu%2520and%2520Boyu%2520Ni%2520and%2520Braden%2520Hancock%2520and%2520Bram%2520Wasti%2520and%2520Brandon%2520Spence%2520and%2520Brani%2520Stojkovic%2520and%2520Brian%2520Gamido%2520and%2520Britt%2520Montalvo%2520and%2520Carl%2520Parker%2520and%2520Carly%2520Burton%2520and%2520Catalina%2520Mejia%2520and%2520Changhan%2520Wang%2520and%2520Changkyu%2520Kim%2520and%2520Chao%2520Zhou%2520and%2520Chester%2520Hu%2520and%2520Ching-Hsiang%2520Chu%2520and%2520Chris%2520Cai%2520and%2520Chris%2520Tindal%2520and%2520Christoph%2520Feichtenhofer%2520and%2520Damon%2520Civin%2520and%2520Dana%2520Beaty%2520and%2520Daniel%2520Kreymer%2520and%2520Daniel%2520Li%2520and%2520Danny%2520Wyatt%2520and%2520David%2520Adkins%2520and%2520David%2520Xu%2520and%2520Davide%2520Testuggine%2520and%2520Delia%2520David%2520and%2520Devi%2520Parikh%2520and%2520Diana%2520Liskovich%2520and%2520Didem%2520Foss%2520and%2520Dingkang%2520Wang%2520and%2520Duc%2520Le%2520and%2520Dustin%2520Holland%2520and%2520Edward%2520Dowling%2520and%2520Eissa%2520Jamil%2520and%2520Elaine%2520Montgomery%2520and%2520Eleonora%2520Presani%2520and%2520Emily%2520Hahn%2520and%2520Emily%2520Wood%2520and%2520Erik%2520Brinkman%2520and%2520Esteban%2520Arcaute%2520and%2520Evan%2520Dunbar%2520and%2520Evan%2520Smothers%2520and%2520Fei%2520Sun%2520and%2520Felix%2520Kreuk%2520and%2520Feng%2520Tian%2520and%2520Firat%2520Ozgenel%2520and%2520Francesco%2520Caggioni%2520and%2520Francisco%2520Guzm%25C3%25A1n%2520and%2520Frank%2520Kanayet%2520and%2520Frank%2520Seide%2520and%2520Gabriela%2520Medina%2520Florez%2520and%2520Gabriella%2520Schwarz%2520and%2520Gada%2520Badeer%2520and%2520Georgia%2520Swee%2520and%2520Gil%2520Halpern%2520and%2520Govind%2520Thattai%2520and%2520Grant%2520Herman%2520and%2520Grigory%2520Sizov%2520and%2520%2520Guangyi%2520and%2520%2520Zhang%2520and%2520Guna%2520Lakshminarayanan%2520and%2520Hamid%2520Shojanazeri%2520and%2520Han%2520Zou%2520and%2520Hannah%2520Wang%2520and%2520Hanwen%2520Zha%2520and%2520Haroun%2520Habeeb%2520and%2520Harrison%2520Rudolph%2520and%2520Helen%2520Suk%2520and%2520Henry%2520Aspegren%2520and%2520Hunter%2520Goldman%2520and%2520Igor%2520Molybog%2520and%2520Igor%2520Tufanov%2520and%2520Irina-Elena%2520Veliche%2520and%2520Itai%2520Gat%2520and%2520Jake%2520Weissman%2520and%2520James%2520Geboski%2520and%2520James%2520Kohli%2520and%2520Japhet%2520Asher%2520and%2520Jean-Baptiste%2520Gaya%2520and%2520Jeff%2520Marcus%2520and%2520Jeff%2520Tang%2520and%2520Jennifer%2520Chan%2520and%2520Jenny%2520Zhen%2520and%2520Jeremy%2520Reizenstein%2520and%2520Jeremy%2520Teboul%2520and%2520Jessica%2520Zhong%2520and%2520Jian%2520Jin%2520and%2520Jingyi%2520Yang%2520and%2520Joe%2520Cummings%2520and%2520Jon%2520Carvill%2520and%2520Jon%2520Shepard%2520and%2520Jonathan%2520McPhie%2520and%2520Jonathan%2520Torres%2520and%2520Josh%2520Ginsburg%2520and%2520Junjie%2520Wang%2520and%2520Kai%2520Wu%2520and%2520Kam%2520Hou%2520U%2520and%2520Karan%2520Saxena%2520and%2520Karthik%2520Prasad%2520and%2520Kartikay%2520Khandelwal%2520and%2520Katayoun%2520Zand%2520and%2520Kathy%2520Matosich%2520and%2520Kaushik%2520Veeraraghavan%2520and%2520Kelly%2520Michelena%2520and%2520Keqian%2520Li%2520and%2520Kun%2520Huang%2520and%2520Kunal%2520Chawla%2520and%2520Kushal%2520Lakhotia%2520and%2520Kyle%2520Huang%2520and%2520Lailin%2520Chen%2520and%2520Lakshya%2520Garg%2520and%2520Lavender%2520A%2520and%2520Leandro%2520Silva%2520and%2520Lee%2520Bell%2520and%2520Lei%2520Zhang%2520and%2520Liangpeng%2520Guo%2520and%2520Licheng%2520Yu%2520and%2520Liron%2520Moshkovich%2520and%2520Luca%2520Wehrstedt%2520and%2520Madian%2520Khabsa%2520and%2520Manav%2520Avalani%2520and%2520Manish%2520Bhatt%2520and%2520Maria%2520Tsimpoukelli%2520and%2520Martynas%2520Mankus%2520and%2520Matan%2520Hasson%2520and%2520Matthew%2520Lennie%2520and%2520Matthias%2520Reso%2520and%2520Maxim%2520Groshev%2520and%2520Maxim%2520Naumov%2520and%2520Maya%2520Lathi%2520and%2520Meghan%2520Keneally%2520and%2520Michael%2520L.%2520Seltzer%2520and%2520Michal%2520Valko%2520and%2520Michelle%2520Restrepo%2520and%2520Mihir%2520Patel%2520and%2520Mik%2520Vyatskov%2520and%2520Mikayel%2520Samvelyan%2520and%2520Mike%2520Clark%2520and%2520Mike%2520Macey%2520and%2520Mike%2520Wang%2520and%2520Miquel%2520Jubert%2520Hermoso%2520and%2520Mo%2520Metanat%2520and%2520Mohammad%2520Rastegari%2520and%2520Munish%2520Bansal%2520and%2520Nandhini%2520Santhanam%2520and%2520Natascha%2520Parks%2520and%2520Natasha%2520White%2520and%2520Navyata%2520Bawa%2520and%2520Nayan%2520Singhal%2520and%2520Nick%2520Egebo%2520and%2520Nicolas%2520Usunier%2520and%2520Nikolay%2520Pavlovich%2520Laptev%2520and%2520Ning%2520Dong%2520and%2520Ning%2520Zhang%2520and%2520Norman%2520Cheng%2520and%2520Oleg%2520Chernoguz%2520and%2520Olivia%2520Hart%2520and%2520Omkar%2520Salpekar%2520and%2520Ozlem%2520Kalinli%2520and%2520Parkin%2520Kent%2520and%2520Parth%2520Parekh%2520and%2520Paul%2520Saab%2520and%2520Pavan%2520Balaji%2520and%2520Pedro%2520Rittner%2520and%2520Philip%2520Bontrager%2520and%2520Pierre%2520Roux%2520and%2520Piotr%2520Dollar%2520and%2520Polina%2520Zvyagina%2520and%2520Prashant%2520Ratanchandani%2520and%2520Pritish%2520Yuvraj%2520and%2520Qian%2520Liang%2520and%2520Rachad%2520Alao%2520and%2520Rachel%2520Rodriguez%2520and%2520Rafi%2520Ayub%2520and%2520Raghotham%2520Murthy%2520and%2520Raghu%2520Nayani%2520and%2520Rahul%2520Mitra%2520and%2520Raymond%2520Li%2520and%2520Rebekkah%2520Hogan%2520and%2520Robin%2520Battey%2520and%2520Rocky%2520Wang%2520and%2520Rohan%2520Maheswari%2520and%2520Russ%2520Howes%2520and%2520Ruty%2520Rinott%2520and%2520Sai%2520Jayesh%2520Bondu%2520and%2520Samyak%2520Datta%2520and%2520Sara%2520Chugh%2520and%2520Sara%2520Hunt%2520and%2520Sargun%2520Dhillon%2520and%2520Sasha%2520Sidorov%2520and%2520Satadru%2520Pan%2520and%2520Saurabh%2520Verma%2520and%2520Seiji%2520Yamamoto%2520and%2520Sharadh%2520Ramaswamy%2520and%2520Shaun%2520Lindsay%2520and%2520Shaun%2520Lindsay%2520and%2520Sheng%2520Feng%2520and%2520Shenghao%2520Lin%2520and%2520Shengxin%2520Cindy%2520Zha%2520and%2520Shiva%2520Shankar%2520and%2520Shuqiang%2520Zhang%2520and%2520Shuqiang%2520Zhang%2520and%2520Sinong%2520Wang%2520and%2520Sneha%2520Agarwal%2520and%2520Soji%2520Sajuyigbe%2520and%2520Soumith%2520Chintala%2520and%2520Stephanie%2520Max%2520and%2520Stephen%2520Chen%2520and%2520Steve%2520Kehoe%2520and%2520Steve%2520Satterfield%2520and%2520Sudarshan%2520Govindaprasad%2520and%2520Sumit%2520Gupta%2520and%2520Sungmin%2520Cho%2520and%2520Sunny%2520Virk%2520and%2520Suraj%2520Subramanian%2520and%2520Sy%2520Choudhury%2520and%2520Sydney%2520Goldman%2520and%2520Tal%2520Remez%2520and%2520Tamar%2520Glaser%2520and%2520Tamara%2520Best%2520and%2520Thilo%2520Kohler%2520and%2520Thomas%2520Robinson%2520and%2520Tianhe%2520Li%2520and%2520Tianjun%2520Zhang%2520and%2520Tim%2520Matthews%2520and%2520Timothy%2520Chou%2520and%2520Tzook%2520Shaked%2520and%2520Varun%2520Vontimitta%2520and%2520Victoria%2520Ajayi%2520and%2520Victoria%2520Montanez%2520and%2520Vijai%2520Mohan%2520and%2520Vinay%2520Satish%2520Kumar%2520and%2520Vishal%2520Mangla%2520and%2520Vlad%2520Ionescu%2520and%2520Vlad%2520Poenaru%2520and%2520Vlad%2520Tiberiu%2520Mihailescu%2520and%2520Vladimir%2520Ivanov%2520and%2520Wei%2520Li%2520and%2520Wenchen%2520Wang%2520and%2520Wenwen%2520Jiang%2520and%2520Wes%2520Bouaziz%2520and%2520Will%2520Constable%2520and%2520Xiaocheng%2520Tang%2520and%2520Xiaofang%2520Wang%2520and%2520Xiaojian%2520Wu%2520and%2520Xiaolan%2520Wang%2520and%2520Xide%2520Xia%2520and%2520Xilun%2520Wu%2520and%2520Xinbo%2520Gao%2520and%2520Yanjun%2520Chen%2520and%2520Ye%2520Hu%2520and%2520Ye%2520Jia%2520and%2520Ye%2520Qi%2520and%2520Yenda%2520Li%2520and%2520Yilin%2520Zhang%2520and%2520Ying%2520Zhang%2520and%2520Yossi%2520Adi%2520and%2520Youngjin%2520Nam%2520and%2520%2520Yu%2520and%2520%2520Wang%2520and%2520Yuchen%2520Hao%2520and%2520Yundi%2520Qian%2520and%2520Yuzi%2520He%2520and%2520Zach%2520Rait%2520and%2520Zachary%2520DeVito%2520and%2520Zef%2520Rosnbrick%2520and%2520Zhaoduo%2520Wen%2520and%2520Zhenyu%2520Yang%2520and%2520Zhiwei%2520Zhao%26entry.1292438233%3D%2520%2520Modern%2520artificial%2520intelligence%2520%2528AI%2529%2520systems%2520are%2520powered%2520by%2520foundation%2520models.%250AThis%2520paper%2520presents%2520a%2520new%2520set%2520of%2520foundation%2520models%252C%2520called%2520Llama%25203.%2520It%2520is%2520a%250Aherd%2520of%2520language%2520models%2520that%2520natively%2520support%2520multilinguality%252C%2520coding%252C%250Areasoning%252C%2520and%2520tool%2520usage.%2520Our%2520largest%2520model%2520is%2520a%2520dense%2520Transformer%2520with%2520405B%250Aparameters%2520and%2520a%2520context%2520window%2520of%2520up%2520to%2520128K%2520tokens.%2520This%2520paper%2520presents%2520an%250Aextensive%2520empirical%2520evaluation%2520of%2520Llama%25203.%2520We%2520find%2520that%2520Llama%25203%2520delivers%250Acomparable%2520quality%2520to%2520leading%2520language%2520models%2520such%2520as%2520GPT-4%2520on%2520a%2520plethora%2520of%250Atasks.%2520We%2520publicly%2520release%2520Llama%25203%252C%2520including%2520pre-trained%2520and%2520post-trained%250Aversions%2520of%2520the%2520405B%2520parameter%2520language%2520model%2520and%2520our%2520Llama%2520Guard%25203%2520model%2520for%250Ainput%2520and%2520output%2520safety.%2520The%2520paper%2520also%2520presents%2520the%2520results%2520of%2520experiments%2520in%250Awhich%2520we%2520integrate%2520image%252C%2520video%252C%2520and%2520speech%2520capabilities%2520into%2520Llama%25203%2520via%2520a%250Acompositional%2520approach.%2520We%2520observe%2520this%2520approach%2520performs%2520competitively%2520with%250Athe%2520state-of-the-art%2520on%2520image%252C%2520video%252C%2520and%2520speech%2520recognition%2520tasks.%2520The%250Aresulting%2520models%2520are%2520not%2520yet%2520being%2520broadly%2520released%2520as%2520they%2520are%2520still%2520under%250Adevelopment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21783v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Llama%203%20Herd%20of%20Models&entry.906535625=Abhimanyu%20Dubey%20and%20Abhinav%20Jauhri%20and%20Abhinav%20Pandey%20and%20Abhishek%20Kadian%20and%20Ahmad%20Al-Dahle%20and%20Aiesha%20Letman%20and%20Akhil%20Mathur%20and%20Alan%20Schelten%20and%20Amy%20Yang%20and%20Angela%20Fan%20and%20Anirudh%20Goyal%20and%20Anthony%20Hartshorn%20and%20Aobo%20Yang%20and%20Archi%20Mitra%20and%20Archie%20Sravankumar%20and%20Artem%20Korenev%20and%20Arthur%20Hinsvark%20and%20Arun%20Rao%20and%20Aston%20Zhang%20and%20Aurelien%20Rodriguez%20and%20Austen%20Gregerson%20and%20Ava%20Spataru%20and%20Baptiste%20Roziere%20and%20Bethany%20Biron%20and%20Binh%20Tang%20and%20Bobbie%20Chern%20and%20Charlotte%20Caucheteux%20and%20Chaya%20Nayak%20and%20Chloe%20Bi%20and%20Chris%20Marra%20and%20Chris%20McConnell%20and%20Christian%20Keller%20and%20Christophe%20Touret%20and%20Chunyang%20Wu%20and%20Corinne%20Wong%20and%20Cristian%20Canton%20Ferrer%20and%20Cyrus%20Nikolaidis%20and%20Damien%20Allonsius%20and%20Daniel%20Song%20and%20Danielle%20Pintz%20and%20Danny%20Livshits%20and%20David%20Esiobu%20and%20Dhruv%20Choudhary%20and%20Dhruv%20Mahajan%20and%20Diego%20Garcia-Olano%20and%20Diego%20Perino%20and%20Dieuwke%20Hupkes%20and%20Egor%20Lakomkin%20and%20Ehab%20AlBadawy%20and%20Elina%20Lobanova%20and%20Emily%20Dinan%20and%20Eric%20Michael%20Smith%20and%20Filip%20Radenovic%20and%20Frank%20Zhang%20and%20Gabriel%20Synnaeve%20and%20Gabrielle%20Lee%20and%20Georgia%20Lewis%20Anderson%20and%20Graeme%20Nail%20and%20Gregoire%20Mialon%20and%20Guan%20Pang%20and%20Guillem%20Cucurell%20and%20Hailey%20Nguyen%20and%20Hannah%20Korevaar%20and%20Hu%20Xu%20and%20Hugo%20Touvron%20and%20Iliyan%20Zarov%20and%20Imanol%20Arrieta%20Ibarra%20and%20Isabel%20Kloumann%20and%20Ishan%20Misra%20and%20Ivan%20Evtimov%20and%20Jade%20Copet%20and%20Jaewon%20Lee%20and%20Jan%20Geffert%20and%20Jana%20Vranes%20and%20Jason%20Park%20and%20Jay%20Mahadeokar%20and%20Jeet%20Shah%20and%20Jelmer%20van%20der%20Linde%20and%20Jennifer%20Billock%20and%20Jenny%20Hong%20and%20Jenya%20Lee%20and%20Jeremy%20Fu%20and%20Jianfeng%20Chi%20and%20Jianyu%20Huang%20and%20Jiawen%20Liu%20and%20Jie%20Wang%20and%20Jiecao%20Yu%20and%20Joanna%20Bitton%20and%20Joe%20Spisak%20and%20Jongsoo%20Park%20and%20Joseph%20Rocca%20and%20Joshua%20Johnstun%20and%20Joshua%20Saxe%20and%20Junteng%20Jia%20and%20Kalyan%20Vasuden%20Alwala%20and%20Kartikeya%20Upasani%20and%20Kate%20Plawiak%20and%20Ke%20Li%20and%20Kenneth%20Heafield%20and%20Kevin%20Stone%20and%20Khalid%20El-Arini%20and%20Krithika%20Iyer%20and%20Kshitiz%20Malik%20and%20Kuenley%20Chiu%20and%20Kunal%20Bhalla%20and%20Lauren%20Rantala-Yeary%20and%20Laurens%20van%20der%20Maaten%20and%20Lawrence%20Chen%20and%20Liang%20Tan%20and%20Liz%20Jenkins%20and%20Louis%20Martin%20and%20Lovish%20Madaan%20and%20Lubo%20Malo%20and%20Lukas%20Blecher%20and%20Lukas%20Landzaat%20and%20Luke%20de%20Oliveira%20and%20Madeline%20Muzzi%20and%20Mahesh%20Pasupuleti%20and%20Mannat%20Singh%20and%20Manohar%20Paluri%20and%20Marcin%20Kardas%20and%20Mathew%20Oldham%20and%20Mathieu%20Rita%20and%20Maya%20Pavlova%20and%20Melanie%20Kambadur%20and%20Mike%20Lewis%20and%20Min%20Si%20and%20Mitesh%20Kumar%20Singh%20and%20Mona%20Hassan%20and%20Naman%20Goyal%20and%20Narjes%20Torabi%20and%20Nikolay%20Bashlykov%20and%20Nikolay%20Bogoychev%20and%20Niladri%20Chatterji%20and%20Olivier%20Duchenne%20and%20Onur%20%C3%87elebi%20and%20Patrick%20Alrassy%20and%20Pengchuan%20Zhang%20and%20Pengwei%20Li%20and%20Petar%20Vasic%20and%20Peter%20Weng%20and%20Prajjwal%20Bhargava%20and%20Pratik%20Dubal%20and%20Praveen%20Krishnan%20and%20Punit%20Singh%20Koura%20and%20Puxin%20Xu%20and%20Qing%20He%20and%20Qingxiao%20Dong%20and%20Ragavan%20Srinivasan%20and%20Raj%20Ganapathy%20and%20Ramon%20Calderer%20and%20Ricardo%20Silveira%20Cabral%20and%20Robert%20Stojnic%20and%20Roberta%20Raileanu%20and%20Rohit%20Girdhar%20and%20Rohit%20Patel%20and%20Romain%20Sauvestre%20and%20Ronnie%20Polidoro%20and%20Roshan%20Sumbaly%20and%20Ross%20Taylor%20and%20Ruan%20Silva%20and%20Rui%20Hou%20and%20Rui%20Wang%20and%20Saghar%20Hosseini%20and%20Sahana%20Chennabasappa%20and%20Sanjay%20Singh%20and%20Sean%20Bell%20and%20Seohyun%20Sonia%20Kim%20and%20Sergey%20Edunov%20and%20Shaoliang%20Nie%20and%20Sharan%20Narang%20and%20Sharath%20Raparthy%20and%20Sheng%20Shen%20and%20Shengye%20Wan%20and%20Shruti%20Bhosale%20and%20Shun%20Zhang%20and%20Simon%20Vandenhende%20and%20Soumya%20Batra%20and%20Spencer%20Whitman%20and%20Sten%20Sootla%20and%20Stephane%20Collot%20and%20Suchin%20Gururangan%20and%20Sydney%20Borodinsky%20and%20Tamar%20Herman%20and%20Tara%20Fowler%20and%20Tarek%20Sheasha%20and%20Thomas%20Georgiou%20and%20Thomas%20Scialom%20and%20Tobias%20Speckbacher%20and%20Todor%20Mihaylov%20and%20Tong%20Xiao%20and%20Ujjwal%20Karn%20and%20Vedanuj%20Goswami%20and%20Vibhor%20Gupta%20and%20Vignesh%20Ramanathan%20and%20Viktor%20Kerkez%20and%20Vincent%20Gonguet%20and%20Virginie%20Do%20and%20Vish%20Vogeti%20and%20Vladan%20Petrovic%20and%20Weiwei%20Chu%20and%20Wenhan%20Xiong%20and%20Wenyin%20Fu%20and%20Whitney%20Meers%20and%20Xavier%20Martinet%20and%20Xiaodong%20Wang%20and%20Xiaoqing%20Ellen%20Tan%20and%20Xinfeng%20Xie%20and%20Xuchao%20Jia%20and%20Xuewei%20Wang%20and%20Yaelle%20Goldschlag%20and%20Yashesh%20Gaur%20and%20Yasmine%20Babaei%20and%20Yi%20Wen%20and%20Yiwen%20Song%20and%20Yuchen%20Zhang%20and%20Yue%20Li%20and%20Yuning%20Mao%20and%20Zacharie%20Delpierre%20Coudert%20and%20Zheng%20Yan%20and%20Zhengxing%20Chen%20and%20Zoe%20Papakipos%20and%20Aaditya%20Singh%20and%20Aaron%20Grattafiori%20and%20Abha%20Jain%20and%20Adam%20Kelsey%20and%20Adam%20Shajnfeld%20and%20Adithya%20Gangidi%20and%20Adolfo%20Victoria%20and%20Ahuva%20Goldstand%20and%20Ajay%20Menon%20and%20Ajay%20Sharma%20and%20Alex%20Boesenberg%20and%20Alex%20Vaughan%20and%20Alexei%20Baevski%20and%20Allie%20Feinstein%20and%20Amanda%20Kallet%20and%20Amit%20Sangani%20and%20Anam%20Yunus%20and%20Andrei%20Lupu%20and%20Andres%20Alvarado%20and%20Andrew%20Caples%20and%20Andrew%20Gu%20and%20Andrew%20Ho%20and%20Andrew%20Poulton%20and%20Andrew%20Ryan%20and%20Ankit%20Ramchandani%20and%20Annie%20Franco%20and%20Aparajita%20Saraf%20and%20Arkabandhu%20Chowdhury%20and%20Ashley%20Gabriel%20and%20Ashwin%20Bharambe%20and%20Assaf%20Eisenman%20and%20Azadeh%20Yazdan%20and%20Beau%20James%20and%20Ben%20Maurer%20and%20Benjamin%20Leonhardi%20and%20Bernie%20Huang%20and%20Beth%20Loyd%20and%20Beto%20De%20Paola%20and%20Bhargavi%20Paranjape%20and%20Bing%20Liu%20and%20Bo%20Wu%20and%20Boyu%20Ni%20and%20Braden%20Hancock%20and%20Bram%20Wasti%20and%20Brandon%20Spence%20and%20Brani%20Stojkovic%20and%20Brian%20Gamido%20and%20Britt%20Montalvo%20and%20Carl%20Parker%20and%20Carly%20Burton%20and%20Catalina%20Mejia%20and%20Changhan%20Wang%20and%20Changkyu%20Kim%20and%20Chao%20Zhou%20and%20Chester%20Hu%20and%20Ching-Hsiang%20Chu%20and%20Chris%20Cai%20and%20Chris%20Tindal%20and%20Christoph%20Feichtenhofer%20and%20Damon%20Civin%20and%20Dana%20Beaty%20and%20Daniel%20Kreymer%20and%20Daniel%20Li%20and%20Danny%20Wyatt%20and%20David%20Adkins%20and%20David%20Xu%20and%20Davide%20Testuggine%20and%20Delia%20David%20and%20Devi%20Parikh%20and%20Diana%20Liskovich%20and%20Didem%20Foss%20and%20Dingkang%20Wang%20and%20Duc%20Le%20and%20Dustin%20Holland%20and%20Edward%20Dowling%20and%20Eissa%20Jamil%20and%20Elaine%20Montgomery%20and%20Eleonora%20Presani%20and%20Emily%20Hahn%20and%20Emily%20Wood%20and%20Erik%20Brinkman%20and%20Esteban%20Arcaute%20and%20Evan%20Dunbar%20and%20Evan%20Smothers%20and%20Fei%20Sun%20and%20Felix%20Kreuk%20and%20Feng%20Tian%20and%20Firat%20Ozgenel%20and%20Francesco%20Caggioni%20and%20Francisco%20Guzm%C3%A1n%20and%20Frank%20Kanayet%20and%20Frank%20Seide%20and%20Gabriela%20Medina%20Florez%20and%20Gabriella%20Schwarz%20and%20Gada%20Badeer%20and%20Georgia%20Swee%20and%20Gil%20Halpern%20and%20Govind%20Thattai%20and%20Grant%20Herman%20and%20Grigory%20Sizov%20and%20%20Guangyi%20and%20%20Zhang%20and%20Guna%20Lakshminarayanan%20and%20Hamid%20Shojanazeri%20and%20Han%20Zou%20and%20Hannah%20Wang%20and%20Hanwen%20Zha%20and%20Haroun%20Habeeb%20and%20Harrison%20Rudolph%20and%20Helen%20Suk%20and%20Henry%20Aspegren%20and%20Hunter%20Goldman%20and%20Igor%20Molybog%20and%20Igor%20Tufanov%20and%20Irina-Elena%20Veliche%20and%20Itai%20Gat%20and%20Jake%20Weissman%20and%20James%20Geboski%20and%20James%20Kohli%20and%20Japhet%20Asher%20and%20Jean-Baptiste%20Gaya%20and%20Jeff%20Marcus%20and%20Jeff%20Tang%20and%20Jennifer%20Chan%20and%20Jenny%20Zhen%20and%20Jeremy%20Reizenstein%20and%20Jeremy%20Teboul%20and%20Jessica%20Zhong%20and%20Jian%20Jin%20and%20Jingyi%20Yang%20and%20Joe%20Cummings%20and%20Jon%20Carvill%20and%20Jon%20Shepard%20and%20Jonathan%20McPhie%20and%20Jonathan%20Torres%20and%20Josh%20Ginsburg%20and%20Junjie%20Wang%20and%20Kai%20Wu%20and%20Kam%20Hou%20U%20and%20Karan%20Saxena%20and%20Karthik%20Prasad%20and%20Kartikay%20Khandelwal%20and%20Katayoun%20Zand%20and%20Kathy%20Matosich%20and%20Kaushik%20Veeraraghavan%20and%20Kelly%20Michelena%20and%20Keqian%20Li%20and%20Kun%20Huang%20and%20Kunal%20Chawla%20and%20Kushal%20Lakhotia%20and%20Kyle%20Huang%20and%20Lailin%20Chen%20and%20Lakshya%20Garg%20and%20Lavender%20A%20and%20Leandro%20Silva%20and%20Lee%20Bell%20and%20Lei%20Zhang%20and%20Liangpeng%20Guo%20and%20Licheng%20Yu%20and%20Liron%20Moshkovich%20and%20Luca%20Wehrstedt%20and%20Madian%20Khabsa%20and%20Manav%20Avalani%20and%20Manish%20Bhatt%20and%20Maria%20Tsimpoukelli%20and%20Martynas%20Mankus%20and%20Matan%20Hasson%20and%20Matthew%20Lennie%20and%20Matthias%20Reso%20and%20Maxim%20Groshev%20and%20Maxim%20Naumov%20and%20Maya%20Lathi%20and%20Meghan%20Keneally%20and%20Michael%20L.%20Seltzer%20and%20Michal%20Valko%20and%20Michelle%20Restrepo%20and%20Mihir%20Patel%20and%20Mik%20Vyatskov%20and%20Mikayel%20Samvelyan%20and%20Mike%20Clark%20and%20Mike%20Macey%20and%20Mike%20Wang%20and%20Miquel%20Jubert%20Hermoso%20and%20Mo%20Metanat%20and%20Mohammad%20Rastegari%20and%20Munish%20Bansal%20and%20Nandhini%20Santhanam%20and%20Natascha%20Parks%20and%20Natasha%20White%20and%20Navyata%20Bawa%20and%20Nayan%20Singhal%20and%20Nick%20Egebo%20and%20Nicolas%20Usunier%20and%20Nikolay%20Pavlovich%20Laptev%20and%20Ning%20Dong%20and%20Ning%20Zhang%20and%20Norman%20Cheng%20and%20Oleg%20Chernoguz%20and%20Olivia%20Hart%20and%20Omkar%20Salpekar%20and%20Ozlem%20Kalinli%20and%20Parkin%20Kent%20and%20Parth%20Parekh%20and%20Paul%20Saab%20and%20Pavan%20Balaji%20and%20Pedro%20Rittner%20and%20Philip%20Bontrager%20and%20Pierre%20Roux%20and%20Piotr%20Dollar%20and%20Polina%20Zvyagina%20and%20Prashant%20Ratanchandani%20and%20Pritish%20Yuvraj%20and%20Qian%20Liang%20and%20Rachad%20Alao%20and%20Rachel%20Rodriguez%20and%20Rafi%20Ayub%20and%20Raghotham%20Murthy%20and%20Raghu%20Nayani%20and%20Rahul%20Mitra%20and%20Raymond%20Li%20and%20Rebekkah%20Hogan%20and%20Robin%20Battey%20and%20Rocky%20Wang%20and%20Rohan%20Maheswari%20and%20Russ%20Howes%20and%20Ruty%20Rinott%20and%20Sai%20Jayesh%20Bondu%20and%20Samyak%20Datta%20and%20Sara%20Chugh%20and%20Sara%20Hunt%20and%20Sargun%20Dhillon%20and%20Sasha%20Sidorov%20and%20Satadru%20Pan%20and%20Saurabh%20Verma%20and%20Seiji%20Yamamoto%20and%20Sharadh%20Ramaswamy%20and%20Shaun%20Lindsay%20and%20Shaun%20Lindsay%20and%20Sheng%20Feng%20and%20Shenghao%20Lin%20and%20Shengxin%20Cindy%20Zha%20and%20Shiva%20Shankar%20and%20Shuqiang%20Zhang%20and%20Shuqiang%20Zhang%20and%20Sinong%20Wang%20and%20Sneha%20Agarwal%20and%20Soji%20Sajuyigbe%20and%20Soumith%20Chintala%20and%20Stephanie%20Max%20and%20Stephen%20Chen%20and%20Steve%20Kehoe%20and%20Steve%20Satterfield%20and%20Sudarshan%20Govindaprasad%20and%20Sumit%20Gupta%20and%20Sungmin%20Cho%20and%20Sunny%20Virk%20and%20Suraj%20Subramanian%20and%20Sy%20Choudhury%20and%20Sydney%20Goldman%20and%20Tal%20Remez%20and%20Tamar%20Glaser%20and%20Tamara%20Best%20and%20Thilo%20Kohler%20and%20Thomas%20Robinson%20and%20Tianhe%20Li%20and%20Tianjun%20Zhang%20and%20Tim%20Matthews%20and%20Timothy%20Chou%20and%20Tzook%20Shaked%20and%20Varun%20Vontimitta%20and%20Victoria%20Ajayi%20and%20Victoria%20Montanez%20and%20Vijai%20Mohan%20and%20Vinay%20Satish%20Kumar%20and%20Vishal%20Mangla%20and%20Vlad%20Ionescu%20and%20Vlad%20Poenaru%20and%20Vlad%20Tiberiu%20Mihailescu%20and%20Vladimir%20Ivanov%20and%20Wei%20Li%20and%20Wenchen%20Wang%20and%20Wenwen%20Jiang%20and%20Wes%20Bouaziz%20and%20Will%20Constable%20and%20Xiaocheng%20Tang%20and%20Xiaofang%20Wang%20and%20Xiaojian%20Wu%20and%20Xiaolan%20Wang%20and%20Xide%20Xia%20and%20Xilun%20Wu%20and%20Xinbo%20Gao%20and%20Yanjun%20Chen%20and%20Ye%20Hu%20and%20Ye%20Jia%20and%20Ye%20Qi%20and%20Yenda%20Li%20and%20Yilin%20Zhang%20and%20Ying%20Zhang%20and%20Yossi%20Adi%20and%20Youngjin%20Nam%20and%20%20Yu%20and%20%20Wang%20and%20Yuchen%20Hao%20and%20Yundi%20Qian%20and%20Yuzi%20He%20and%20Zach%20Rait%20and%20Zachary%20DeVito%20and%20Zef%20Rosnbrick%20and%20Zhaoduo%20Wen%20and%20Zhenyu%20Yang%20and%20Zhiwei%20Zhao&entry.1292438233=%20%20Modern%20artificial%20intelligence%20%28AI%29%20systems%20are%20powered%20by%20foundation%20models.%0AThis%20paper%20presents%20a%20new%20set%20of%20foundation%20models%2C%20called%20Llama%203.%20It%20is%20a%0Aherd%20of%20language%20models%20that%20natively%20support%20multilinguality%2C%20coding%2C%0Areasoning%2C%20and%20tool%20usage.%20Our%20largest%20model%20is%20a%20dense%20Transformer%20with%20405B%0Aparameters%20and%20a%20context%20window%20of%20up%20to%20128K%20tokens.%20This%20paper%20presents%20an%0Aextensive%20empirical%20evaluation%20of%20Llama%203.%20We%20find%20that%20Llama%203%20delivers%0Acomparable%20quality%20to%20leading%20language%20models%20such%20as%20GPT-4%20on%20a%20plethora%20of%0Atasks.%20We%20publicly%20release%20Llama%203%2C%20including%20pre-trained%20and%20post-trained%0Aversions%20of%20the%20405B%20parameter%20language%20model%20and%20our%20Llama%20Guard%203%20model%20for%0Ainput%20and%20output%20safety.%20The%20paper%20also%20presents%20the%20results%20of%20experiments%20in%0Awhich%20we%20integrate%20image%2C%20video%2C%20and%20speech%20capabilities%20into%20Llama%203%20via%20a%0Acompositional%20approach.%20We%20observe%20this%20approach%20performs%20competitively%20with%0Athe%20state-of-the-art%20on%20image%2C%20video%2C%20and%20speech%20recognition%20tasks.%20The%0Aresulting%20models%20are%20not%20yet%20being%20broadly%20released%20as%20they%20are%20still%20under%0Adevelopment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21783v1&entry.124074799=Read"},
{"title": "Enhancing and Assessing Instruction-Following with Fine-Grained\n  Instruction Variants", "author": "Jiuding Yang and Weidong Guo and Kaitong Yang and Xiangyang Li and Zhuwei Rao and Yu Xu and Di Niu", "abstract": "  The effective alignment of Large Language Models (LLMs) with precise\ninstructions is essential for their application in diverse real-world\nscenarios. Current methods focus on enhancing the diversity and complexity of\ntraining and evaluation samples, yet they fall short in accurately assessing\nLLMs' ability to follow similar instruction variants. We introduce an effective\ndata augmentation technique that decomposes complex instructions into simpler\nsub-components, modifies these, and reconstructs them into new variants,\nthereby preserves the original instruction's context and complexity while\nintroducing variability, which is critical for training and evaluating LLMs'\ninstruction-following precision. We developed the DeMoRecon dataset using this\nmethod to both fine-tune and evaluate LLMs. Our findings show that LLMs\nfine-tuned with DeMoRecon will gain significant performance boost on both ours\nand commonly used instructions-following benchmarks.\n", "link": "http://arxiv.org/abs/2406.11301v2", "date": "2024-07-31", "relevancy": 1.9472, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4903}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.489}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20and%20Assessing%20Instruction-Following%20with%20Fine-Grained%0A%20%20Instruction%20Variants&body=Title%3A%20Enhancing%20and%20Assessing%20Instruction-Following%20with%20Fine-Grained%0A%20%20Instruction%20Variants%0AAuthor%3A%20Jiuding%20Yang%20and%20Weidong%20Guo%20and%20Kaitong%20Yang%20and%20Xiangyang%20Li%20and%20Zhuwei%20Rao%20and%20Yu%20Xu%20and%20Di%20Niu%0AAbstract%3A%20%20%20The%20effective%20alignment%20of%20Large%20Language%20Models%20%28LLMs%29%20with%20precise%0Ainstructions%20is%20essential%20for%20their%20application%20in%20diverse%20real-world%0Ascenarios.%20Current%20methods%20focus%20on%20enhancing%20the%20diversity%20and%20complexity%20of%0Atraining%20and%20evaluation%20samples%2C%20yet%20they%20fall%20short%20in%20accurately%20assessing%0ALLMs%27%20ability%20to%20follow%20similar%20instruction%20variants.%20We%20introduce%20an%20effective%0Adata%20augmentation%20technique%20that%20decomposes%20complex%20instructions%20into%20simpler%0Asub-components%2C%20modifies%20these%2C%20and%20reconstructs%20them%20into%20new%20variants%2C%0Athereby%20preserves%20the%20original%20instruction%27s%20context%20and%20complexity%20while%0Aintroducing%20variability%2C%20which%20is%20critical%20for%20training%20and%20evaluating%20LLMs%27%0Ainstruction-following%20precision.%20We%20developed%20the%20DeMoRecon%20dataset%20using%20this%0Amethod%20to%20both%20fine-tune%20and%20evaluate%20LLMs.%20Our%20findings%20show%20that%20LLMs%0Afine-tuned%20with%20DeMoRecon%20will%20gain%20significant%20performance%20boost%20on%20both%20ours%0Aand%20commonly%20used%20instructions-following%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11301v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520and%2520Assessing%2520Instruction-Following%2520with%2520Fine-Grained%250A%2520%2520Instruction%2520Variants%26entry.906535625%3DJiuding%2520Yang%2520and%2520Weidong%2520Guo%2520and%2520Kaitong%2520Yang%2520and%2520Xiangyang%2520Li%2520and%2520Zhuwei%2520Rao%2520and%2520Yu%2520Xu%2520and%2520Di%2520Niu%26entry.1292438233%3D%2520%2520The%2520effective%2520alignment%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520precise%250Ainstructions%2520is%2520essential%2520for%2520their%2520application%2520in%2520diverse%2520real-world%250Ascenarios.%2520Current%2520methods%2520focus%2520on%2520enhancing%2520the%2520diversity%2520and%2520complexity%2520of%250Atraining%2520and%2520evaluation%2520samples%252C%2520yet%2520they%2520fall%2520short%2520in%2520accurately%2520assessing%250ALLMs%2527%2520ability%2520to%2520follow%2520similar%2520instruction%2520variants.%2520We%2520introduce%2520an%2520effective%250Adata%2520augmentation%2520technique%2520that%2520decomposes%2520complex%2520instructions%2520into%2520simpler%250Asub-components%252C%2520modifies%2520these%252C%2520and%2520reconstructs%2520them%2520into%2520new%2520variants%252C%250Athereby%2520preserves%2520the%2520original%2520instruction%2527s%2520context%2520and%2520complexity%2520while%250Aintroducing%2520variability%252C%2520which%2520is%2520critical%2520for%2520training%2520and%2520evaluating%2520LLMs%2527%250Ainstruction-following%2520precision.%2520We%2520developed%2520the%2520DeMoRecon%2520dataset%2520using%2520this%250Amethod%2520to%2520both%2520fine-tune%2520and%2520evaluate%2520LLMs.%2520Our%2520findings%2520show%2520that%2520LLMs%250Afine-tuned%2520with%2520DeMoRecon%2520will%2520gain%2520significant%2520performance%2520boost%2520on%2520both%2520ours%250Aand%2520commonly%2520used%2520instructions-following%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11301v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20and%20Assessing%20Instruction-Following%20with%20Fine-Grained%0A%20%20Instruction%20Variants&entry.906535625=Jiuding%20Yang%20and%20Weidong%20Guo%20and%20Kaitong%20Yang%20and%20Xiangyang%20Li%20and%20Zhuwei%20Rao%20and%20Yu%20Xu%20and%20Di%20Niu&entry.1292438233=%20%20The%20effective%20alignment%20of%20Large%20Language%20Models%20%28LLMs%29%20with%20precise%0Ainstructions%20is%20essential%20for%20their%20application%20in%20diverse%20real-world%0Ascenarios.%20Current%20methods%20focus%20on%20enhancing%20the%20diversity%20and%20complexity%20of%0Atraining%20and%20evaluation%20samples%2C%20yet%20they%20fall%20short%20in%20accurately%20assessing%0ALLMs%27%20ability%20to%20follow%20similar%20instruction%20variants.%20We%20introduce%20an%20effective%0Adata%20augmentation%20technique%20that%20decomposes%20complex%20instructions%20into%20simpler%0Asub-components%2C%20modifies%20these%2C%20and%20reconstructs%20them%20into%20new%20variants%2C%0Athereby%20preserves%20the%20original%20instruction%27s%20context%20and%20complexity%20while%0Aintroducing%20variability%2C%20which%20is%20critical%20for%20training%20and%20evaluating%20LLMs%27%0Ainstruction-following%20precision.%20We%20developed%20the%20DeMoRecon%20dataset%20using%20this%0Amethod%20to%20both%20fine-tune%20and%20evaluate%20LLMs.%20Our%20findings%20show%20that%20LLMs%0Afine-tuned%20with%20DeMoRecon%20will%20gain%20significant%20performance%20boost%20on%20both%20ours%0Aand%20commonly%20used%20instructions-following%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11301v2&entry.124074799=Read"},
{"title": "Deep Reinforcement Learning for Sim-to-Real Policy Transfer of VTOL-UAVs\n  Offshore Docking Operations", "author": "Ali M. Ali and Aryaman Gupta and Hashim A. Hashim", "abstract": "  This paper proposes a novel Reinforcement Learning (RL) approach for\nsim-to-real policy transfer of Vertical Take-Off and Landing Unmanned Aerial\nVehicle (VTOL-UAV). The proposed approach is designed for VTOL-UAV landing on\noffshore docking stations in maritime operations. VTOL-UAVs in maritime\noperations encounter limitations in their operational range, primarily stemming\nfrom constraints imposed by their battery capacity. The concept of autonomous\nlanding on a charging platform presents an intriguing prospect for mitigating\nthese limitations by facilitating battery charging and data transfer. However,\ncurrent Deep Reinforcement Learning (DRL) methods exhibit drawbacks, including\nlengthy training times, and modest success rates. In this paper, we tackle\nthese concerns comprehensively by decomposing the landing procedure into a\nsequence of more manageable but analogous tasks in terms of an approach phase\nand a landing phase. The proposed architecture utilizes a model-based control\nscheme for the approach phase, where the VTOL-UAV is approaching the offshore\ndocking station. In the Landing phase, DRL agents were trained offline to learn\nthe optimal policy to dock on the offshore station. The Joint North Sea Wave\nProject (JONSWAP) spectrum model has been employed to create a wave model for\neach episode, enhancing policy generalization for sim2real transfer. A set of\nDRL algorithms have been tested through numerical simulations including\nvalue-based agents and policy-based agents such as Deep \\textit{Q} Networks\n(DQN) and Proximal Policy Optimization (PPO) respectively. The numerical\nexperiments show that the PPO agent can learn complicated and efficient\npolicies to land in uncertain environments, which in turn enhances the\nlikelihood of successful sim-to-real transfer.\n", "link": "http://arxiv.org/abs/2406.00887v2", "date": "2024-07-31", "relevancy": 1.947, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5294}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5029}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Reinforcement%20Learning%20for%20Sim-to-Real%20Policy%20Transfer%20of%20VTOL-UAVs%0A%20%20Offshore%20Docking%20Operations&body=Title%3A%20Deep%20Reinforcement%20Learning%20for%20Sim-to-Real%20Policy%20Transfer%20of%20VTOL-UAVs%0A%20%20Offshore%20Docking%20Operations%0AAuthor%3A%20Ali%20M.%20Ali%20and%20Aryaman%20Gupta%20and%20Hashim%20A.%20Hashim%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20novel%20Reinforcement%20Learning%20%28RL%29%20approach%20for%0Asim-to-real%20policy%20transfer%20of%20Vertical%20Take-Off%20and%20Landing%20Unmanned%20Aerial%0AVehicle%20%28VTOL-UAV%29.%20The%20proposed%20approach%20is%20designed%20for%20VTOL-UAV%20landing%20on%0Aoffshore%20docking%20stations%20in%20maritime%20operations.%20VTOL-UAVs%20in%20maritime%0Aoperations%20encounter%20limitations%20in%20their%20operational%20range%2C%20primarily%20stemming%0Afrom%20constraints%20imposed%20by%20their%20battery%20capacity.%20The%20concept%20of%20autonomous%0Alanding%20on%20a%20charging%20platform%20presents%20an%20intriguing%20prospect%20for%20mitigating%0Athese%20limitations%20by%20facilitating%20battery%20charging%20and%20data%20transfer.%20However%2C%0Acurrent%20Deep%20Reinforcement%20Learning%20%28DRL%29%20methods%20exhibit%20drawbacks%2C%20including%0Alengthy%20training%20times%2C%20and%20modest%20success%20rates.%20In%20this%20paper%2C%20we%20tackle%0Athese%20concerns%20comprehensively%20by%20decomposing%20the%20landing%20procedure%20into%20a%0Asequence%20of%20more%20manageable%20but%20analogous%20tasks%20in%20terms%20of%20an%20approach%20phase%0Aand%20a%20landing%20phase.%20The%20proposed%20architecture%20utilizes%20a%20model-based%20control%0Ascheme%20for%20the%20approach%20phase%2C%20where%20the%20VTOL-UAV%20is%20approaching%20the%20offshore%0Adocking%20station.%20In%20the%20Landing%20phase%2C%20DRL%20agents%20were%20trained%20offline%20to%20learn%0Athe%20optimal%20policy%20to%20dock%20on%20the%20offshore%20station.%20The%20Joint%20North%20Sea%20Wave%0AProject%20%28JONSWAP%29%20spectrum%20model%20has%20been%20employed%20to%20create%20a%20wave%20model%20for%0Aeach%20episode%2C%20enhancing%20policy%20generalization%20for%20sim2real%20transfer.%20A%20set%20of%0ADRL%20algorithms%20have%20been%20tested%20through%20numerical%20simulations%20including%0Avalue-based%20agents%20and%20policy-based%20agents%20such%20as%20Deep%20%5Ctextit%7BQ%7D%20Networks%0A%28DQN%29%20and%20Proximal%20Policy%20Optimization%20%28PPO%29%20respectively.%20The%20numerical%0Aexperiments%20show%20that%20the%20PPO%20agent%20can%20learn%20complicated%20and%20efficient%0Apolicies%20to%20land%20in%20uncertain%20environments%2C%20which%20in%20turn%20enhances%20the%0Alikelihood%20of%20successful%20sim-to-real%20transfer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.00887v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Reinforcement%2520Learning%2520for%2520Sim-to-Real%2520Policy%2520Transfer%2520of%2520VTOL-UAVs%250A%2520%2520Offshore%2520Docking%2520Operations%26entry.906535625%3DAli%2520M.%2520Ali%2520and%2520Aryaman%2520Gupta%2520and%2520Hashim%2520A.%2520Hashim%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520novel%2520Reinforcement%2520Learning%2520%2528RL%2529%2520approach%2520for%250Asim-to-real%2520policy%2520transfer%2520of%2520Vertical%2520Take-Off%2520and%2520Landing%2520Unmanned%2520Aerial%250AVehicle%2520%2528VTOL-UAV%2529.%2520The%2520proposed%2520approach%2520is%2520designed%2520for%2520VTOL-UAV%2520landing%2520on%250Aoffshore%2520docking%2520stations%2520in%2520maritime%2520operations.%2520VTOL-UAVs%2520in%2520maritime%250Aoperations%2520encounter%2520limitations%2520in%2520their%2520operational%2520range%252C%2520primarily%2520stemming%250Afrom%2520constraints%2520imposed%2520by%2520their%2520battery%2520capacity.%2520The%2520concept%2520of%2520autonomous%250Alanding%2520on%2520a%2520charging%2520platform%2520presents%2520an%2520intriguing%2520prospect%2520for%2520mitigating%250Athese%2520limitations%2520by%2520facilitating%2520battery%2520charging%2520and%2520data%2520transfer.%2520However%252C%250Acurrent%2520Deep%2520Reinforcement%2520Learning%2520%2528DRL%2529%2520methods%2520exhibit%2520drawbacks%252C%2520including%250Alengthy%2520training%2520times%252C%2520and%2520modest%2520success%2520rates.%2520In%2520this%2520paper%252C%2520we%2520tackle%250Athese%2520concerns%2520comprehensively%2520by%2520decomposing%2520the%2520landing%2520procedure%2520into%2520a%250Asequence%2520of%2520more%2520manageable%2520but%2520analogous%2520tasks%2520in%2520terms%2520of%2520an%2520approach%2520phase%250Aand%2520a%2520landing%2520phase.%2520The%2520proposed%2520architecture%2520utilizes%2520a%2520model-based%2520control%250Ascheme%2520for%2520the%2520approach%2520phase%252C%2520where%2520the%2520VTOL-UAV%2520is%2520approaching%2520the%2520offshore%250Adocking%2520station.%2520In%2520the%2520Landing%2520phase%252C%2520DRL%2520agents%2520were%2520trained%2520offline%2520to%2520learn%250Athe%2520optimal%2520policy%2520to%2520dock%2520on%2520the%2520offshore%2520station.%2520The%2520Joint%2520North%2520Sea%2520Wave%250AProject%2520%2528JONSWAP%2529%2520spectrum%2520model%2520has%2520been%2520employed%2520to%2520create%2520a%2520wave%2520model%2520for%250Aeach%2520episode%252C%2520enhancing%2520policy%2520generalization%2520for%2520sim2real%2520transfer.%2520A%2520set%2520of%250ADRL%2520algorithms%2520have%2520been%2520tested%2520through%2520numerical%2520simulations%2520including%250Avalue-based%2520agents%2520and%2520policy-based%2520agents%2520such%2520as%2520Deep%2520%255Ctextit%257BQ%257D%2520Networks%250A%2528DQN%2529%2520and%2520Proximal%2520Policy%2520Optimization%2520%2528PPO%2529%2520respectively.%2520The%2520numerical%250Aexperiments%2520show%2520that%2520the%2520PPO%2520agent%2520can%2520learn%2520complicated%2520and%2520efficient%250Apolicies%2520to%2520land%2520in%2520uncertain%2520environments%252C%2520which%2520in%2520turn%2520enhances%2520the%250Alikelihood%2520of%2520successful%2520sim-to-real%2520transfer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.00887v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Reinforcement%20Learning%20for%20Sim-to-Real%20Policy%20Transfer%20of%20VTOL-UAVs%0A%20%20Offshore%20Docking%20Operations&entry.906535625=Ali%20M.%20Ali%20and%20Aryaman%20Gupta%20and%20Hashim%20A.%20Hashim&entry.1292438233=%20%20This%20paper%20proposes%20a%20novel%20Reinforcement%20Learning%20%28RL%29%20approach%20for%0Asim-to-real%20policy%20transfer%20of%20Vertical%20Take-Off%20and%20Landing%20Unmanned%20Aerial%0AVehicle%20%28VTOL-UAV%29.%20The%20proposed%20approach%20is%20designed%20for%20VTOL-UAV%20landing%20on%0Aoffshore%20docking%20stations%20in%20maritime%20operations.%20VTOL-UAVs%20in%20maritime%0Aoperations%20encounter%20limitations%20in%20their%20operational%20range%2C%20primarily%20stemming%0Afrom%20constraints%20imposed%20by%20their%20battery%20capacity.%20The%20concept%20of%20autonomous%0Alanding%20on%20a%20charging%20platform%20presents%20an%20intriguing%20prospect%20for%20mitigating%0Athese%20limitations%20by%20facilitating%20battery%20charging%20and%20data%20transfer.%20However%2C%0Acurrent%20Deep%20Reinforcement%20Learning%20%28DRL%29%20methods%20exhibit%20drawbacks%2C%20including%0Alengthy%20training%20times%2C%20and%20modest%20success%20rates.%20In%20this%20paper%2C%20we%20tackle%0Athese%20concerns%20comprehensively%20by%20decomposing%20the%20landing%20procedure%20into%20a%0Asequence%20of%20more%20manageable%20but%20analogous%20tasks%20in%20terms%20of%20an%20approach%20phase%0Aand%20a%20landing%20phase.%20The%20proposed%20architecture%20utilizes%20a%20model-based%20control%0Ascheme%20for%20the%20approach%20phase%2C%20where%20the%20VTOL-UAV%20is%20approaching%20the%20offshore%0Adocking%20station.%20In%20the%20Landing%20phase%2C%20DRL%20agents%20were%20trained%20offline%20to%20learn%0Athe%20optimal%20policy%20to%20dock%20on%20the%20offshore%20station.%20The%20Joint%20North%20Sea%20Wave%0AProject%20%28JONSWAP%29%20spectrum%20model%20has%20been%20employed%20to%20create%20a%20wave%20model%20for%0Aeach%20episode%2C%20enhancing%20policy%20generalization%20for%20sim2real%20transfer.%20A%20set%20of%0ADRL%20algorithms%20have%20been%20tested%20through%20numerical%20simulations%20including%0Avalue-based%20agents%20and%20policy-based%20agents%20such%20as%20Deep%20%5Ctextit%7BQ%7D%20Networks%0A%28DQN%29%20and%20Proximal%20Policy%20Optimization%20%28PPO%29%20respectively.%20The%20numerical%0Aexperiments%20show%20that%20the%20PPO%20agent%20can%20learn%20complicated%20and%20efficient%0Apolicies%20to%20land%20in%20uncertain%20environments%2C%20which%20in%20turn%20enhances%20the%0Alikelihood%20of%20successful%20sim-to-real%20transfer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.00887v2&entry.124074799=Read"},
{"title": "Empirical Capacity Model for Self-Attention Neural Networks", "author": "Aki H\u00e4rm\u00e4 and Marcin Pietrasik and Anna Wilbik", "abstract": "  Large pretrained self-attention neural networks, or transformers, have been\nvery successful in various tasks recently. The performance of a model on a\ngiven task depends on its ability to memorize and generalize the training data.\nLarge transformer models, which may have billions of parameters, in theory have\na huge capacity to memorize content. However, the current algorithms for the\noptimization fall short of the theoretical capacity, and the capacity is also\nhighly dependent on the content. In this paper, we focus on the memory capacity\nof these models obtained using common training algorithms and synthetic\ntraining data. Based on the results, we derive an empirical capacity model\n(ECM) for a generic transformer. The ECM can be used to design task-specific\ntransformer models with an optimal number of parameters in cases where the\ntarget memorization capability of the task can be defined.\n", "link": "http://arxiv.org/abs/2407.15425v2", "date": "2024-07-31", "relevancy": 1.9442, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.537}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4865}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Empirical%20Capacity%20Model%20for%20Self-Attention%20Neural%20Networks&body=Title%3A%20Empirical%20Capacity%20Model%20for%20Self-Attention%20Neural%20Networks%0AAuthor%3A%20Aki%20H%C3%A4rm%C3%A4%20and%20Marcin%20Pietrasik%20and%20Anna%20Wilbik%0AAbstract%3A%20%20%20Large%20pretrained%20self-attention%20neural%20networks%2C%20or%20transformers%2C%20have%20been%0Avery%20successful%20in%20various%20tasks%20recently.%20The%20performance%20of%20a%20model%20on%20a%0Agiven%20task%20depends%20on%20its%20ability%20to%20memorize%20and%20generalize%20the%20training%20data.%0ALarge%20transformer%20models%2C%20which%20may%20have%20billions%20of%20parameters%2C%20in%20theory%20have%0Aa%20huge%20capacity%20to%20memorize%20content.%20However%2C%20the%20current%20algorithms%20for%20the%0Aoptimization%20fall%20short%20of%20the%20theoretical%20capacity%2C%20and%20the%20capacity%20is%20also%0Ahighly%20dependent%20on%20the%20content.%20In%20this%20paper%2C%20we%20focus%20on%20the%20memory%20capacity%0Aof%20these%20models%20obtained%20using%20common%20training%20algorithms%20and%20synthetic%0Atraining%20data.%20Based%20on%20the%20results%2C%20we%20derive%20an%20empirical%20capacity%20model%0A%28ECM%29%20for%20a%20generic%20transformer.%20The%20ECM%20can%20be%20used%20to%20design%20task-specific%0Atransformer%20models%20with%20an%20optimal%20number%20of%20parameters%20in%20cases%20where%20the%0Atarget%20memorization%20capability%20of%20the%20task%20can%20be%20defined.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15425v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmpirical%2520Capacity%2520Model%2520for%2520Self-Attention%2520Neural%2520Networks%26entry.906535625%3DAki%2520H%25C3%25A4rm%25C3%25A4%2520and%2520Marcin%2520Pietrasik%2520and%2520Anna%2520Wilbik%26entry.1292438233%3D%2520%2520Large%2520pretrained%2520self-attention%2520neural%2520networks%252C%2520or%2520transformers%252C%2520have%2520been%250Avery%2520successful%2520in%2520various%2520tasks%2520recently.%2520The%2520performance%2520of%2520a%2520model%2520on%2520a%250Agiven%2520task%2520depends%2520on%2520its%2520ability%2520to%2520memorize%2520and%2520generalize%2520the%2520training%2520data.%250ALarge%2520transformer%2520models%252C%2520which%2520may%2520have%2520billions%2520of%2520parameters%252C%2520in%2520theory%2520have%250Aa%2520huge%2520capacity%2520to%2520memorize%2520content.%2520However%252C%2520the%2520current%2520algorithms%2520for%2520the%250Aoptimization%2520fall%2520short%2520of%2520the%2520theoretical%2520capacity%252C%2520and%2520the%2520capacity%2520is%2520also%250Ahighly%2520dependent%2520on%2520the%2520content.%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%2520the%2520memory%2520capacity%250Aof%2520these%2520models%2520obtained%2520using%2520common%2520training%2520algorithms%2520and%2520synthetic%250Atraining%2520data.%2520Based%2520on%2520the%2520results%252C%2520we%2520derive%2520an%2520empirical%2520capacity%2520model%250A%2528ECM%2529%2520for%2520a%2520generic%2520transformer.%2520The%2520ECM%2520can%2520be%2520used%2520to%2520design%2520task-specific%250Atransformer%2520models%2520with%2520an%2520optimal%2520number%2520of%2520parameters%2520in%2520cases%2520where%2520the%250Atarget%2520memorization%2520capability%2520of%2520the%2520task%2520can%2520be%2520defined.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15425v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empirical%20Capacity%20Model%20for%20Self-Attention%20Neural%20Networks&entry.906535625=Aki%20H%C3%A4rm%C3%A4%20and%20Marcin%20Pietrasik%20and%20Anna%20Wilbik&entry.1292438233=%20%20Large%20pretrained%20self-attention%20neural%20networks%2C%20or%20transformers%2C%20have%20been%0Avery%20successful%20in%20various%20tasks%20recently.%20The%20performance%20of%20a%20model%20on%20a%0Agiven%20task%20depends%20on%20its%20ability%20to%20memorize%20and%20generalize%20the%20training%20data.%0ALarge%20transformer%20models%2C%20which%20may%20have%20billions%20of%20parameters%2C%20in%20theory%20have%0Aa%20huge%20capacity%20to%20memorize%20content.%20However%2C%20the%20current%20algorithms%20for%20the%0Aoptimization%20fall%20short%20of%20the%20theoretical%20capacity%2C%20and%20the%20capacity%20is%20also%0Ahighly%20dependent%20on%20the%20content.%20In%20this%20paper%2C%20we%20focus%20on%20the%20memory%20capacity%0Aof%20these%20models%20obtained%20using%20common%20training%20algorithms%20and%20synthetic%0Atraining%20data.%20Based%20on%20the%20results%2C%20we%20derive%20an%20empirical%20capacity%20model%0A%28ECM%29%20for%20a%20generic%20transformer.%20The%20ECM%20can%20be%20used%20to%20design%20task-specific%0Atransformer%20models%20with%20an%20optimal%20number%20of%20parameters%20in%20cases%20where%20the%0Atarget%20memorization%20capability%20of%20the%20task%20can%20be%20defined.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15425v2&entry.124074799=Read"},
{"title": "Black box meta-learning intrinsic rewards for sparse-reward environments", "author": "Octavio Pappalardo and Rodrigo Ramele and Juan Miguel Santos", "abstract": "  Despite the successes and progress of deep reinforcement learning over the\nlast decade, several challenges remain that hinder its broader application.\nSome fundamental aspects to improve include data efficiency, generalization\ncapability, and ability to learn in sparse-reward environments, which often\nrequire human-designed dense rewards. Meta-learning has emerged as a promising\napproach to address these issues by optimizing components of the learning\nalgorithm to meet desired characteristics. Additionally, a different line of\nwork has extensively studied the use of intrinsic rewards to enhance the\nexploration capabilities of algorithms. This work investigates how\nmeta-learning can improve the training signal received by RL agents. The focus\nis on meta-learning intrinsic rewards under a framework that doesn't rely on\nthe use of meta-gradients. We analyze and compare this approach to the use of\nextrinsic rewards and a meta-learned advantage function. The developed\nalgorithms are evaluated on distributions of continuous control tasks with both\nparametric and non-parametric variations, and with only sparse rewards\naccessible for the evaluation tasks.\n", "link": "http://arxiv.org/abs/2407.21546v1", "date": "2024-07-31", "relevancy": 1.9315, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4889}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4807}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4777}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Black%20box%20meta-learning%20intrinsic%20rewards%20for%20sparse-reward%20environments&body=Title%3A%20Black%20box%20meta-learning%20intrinsic%20rewards%20for%20sparse-reward%20environments%0AAuthor%3A%20Octavio%20Pappalardo%20and%20Rodrigo%20Ramele%20and%20Juan%20Miguel%20Santos%0AAbstract%3A%20%20%20Despite%20the%20successes%20and%20progress%20of%20deep%20reinforcement%20learning%20over%20the%0Alast%20decade%2C%20several%20challenges%20remain%20that%20hinder%20its%20broader%20application.%0ASome%20fundamental%20aspects%20to%20improve%20include%20data%20efficiency%2C%20generalization%0Acapability%2C%20and%20ability%20to%20learn%20in%20sparse-reward%20environments%2C%20which%20often%0Arequire%20human-designed%20dense%20rewards.%20Meta-learning%20has%20emerged%20as%20a%20promising%0Aapproach%20to%20address%20these%20issues%20by%20optimizing%20components%20of%20the%20learning%0Aalgorithm%20to%20meet%20desired%20characteristics.%20Additionally%2C%20a%20different%20line%20of%0Awork%20has%20extensively%20studied%20the%20use%20of%20intrinsic%20rewards%20to%20enhance%20the%0Aexploration%20capabilities%20of%20algorithms.%20This%20work%20investigates%20how%0Ameta-learning%20can%20improve%20the%20training%20signal%20received%20by%20RL%20agents.%20The%20focus%0Ais%20on%20meta-learning%20intrinsic%20rewards%20under%20a%20framework%20that%20doesn%27t%20rely%20on%0Athe%20use%20of%20meta-gradients.%20We%20analyze%20and%20compare%20this%20approach%20to%20the%20use%20of%0Aextrinsic%20rewards%20and%20a%20meta-learned%20advantage%20function.%20The%20developed%0Aalgorithms%20are%20evaluated%20on%20distributions%20of%20continuous%20control%20tasks%20with%20both%0Aparametric%20and%20non-parametric%20variations%2C%20and%20with%20only%20sparse%20rewards%0Aaccessible%20for%20the%20evaluation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21546v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlack%2520box%2520meta-learning%2520intrinsic%2520rewards%2520for%2520sparse-reward%2520environments%26entry.906535625%3DOctavio%2520Pappalardo%2520and%2520Rodrigo%2520Ramele%2520and%2520Juan%2520Miguel%2520Santos%26entry.1292438233%3D%2520%2520Despite%2520the%2520successes%2520and%2520progress%2520of%2520deep%2520reinforcement%2520learning%2520over%2520the%250Alast%2520decade%252C%2520several%2520challenges%2520remain%2520that%2520hinder%2520its%2520broader%2520application.%250ASome%2520fundamental%2520aspects%2520to%2520improve%2520include%2520data%2520efficiency%252C%2520generalization%250Acapability%252C%2520and%2520ability%2520to%2520learn%2520in%2520sparse-reward%2520environments%252C%2520which%2520often%250Arequire%2520human-designed%2520dense%2520rewards.%2520Meta-learning%2520has%2520emerged%2520as%2520a%2520promising%250Aapproach%2520to%2520address%2520these%2520issues%2520by%2520optimizing%2520components%2520of%2520the%2520learning%250Aalgorithm%2520to%2520meet%2520desired%2520characteristics.%2520Additionally%252C%2520a%2520different%2520line%2520of%250Awork%2520has%2520extensively%2520studied%2520the%2520use%2520of%2520intrinsic%2520rewards%2520to%2520enhance%2520the%250Aexploration%2520capabilities%2520of%2520algorithms.%2520This%2520work%2520investigates%2520how%250Ameta-learning%2520can%2520improve%2520the%2520training%2520signal%2520received%2520by%2520RL%2520agents.%2520The%2520focus%250Ais%2520on%2520meta-learning%2520intrinsic%2520rewards%2520under%2520a%2520framework%2520that%2520doesn%2527t%2520rely%2520on%250Athe%2520use%2520of%2520meta-gradients.%2520We%2520analyze%2520and%2520compare%2520this%2520approach%2520to%2520the%2520use%2520of%250Aextrinsic%2520rewards%2520and%2520a%2520meta-learned%2520advantage%2520function.%2520The%2520developed%250Aalgorithms%2520are%2520evaluated%2520on%2520distributions%2520of%2520continuous%2520control%2520tasks%2520with%2520both%250Aparametric%2520and%2520non-parametric%2520variations%252C%2520and%2520with%2520only%2520sparse%2520rewards%250Aaccessible%2520for%2520the%2520evaluation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21546v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Black%20box%20meta-learning%20intrinsic%20rewards%20for%20sparse-reward%20environments&entry.906535625=Octavio%20Pappalardo%20and%20Rodrigo%20Ramele%20and%20Juan%20Miguel%20Santos&entry.1292438233=%20%20Despite%20the%20successes%20and%20progress%20of%20deep%20reinforcement%20learning%20over%20the%0Alast%20decade%2C%20several%20challenges%20remain%20that%20hinder%20its%20broader%20application.%0ASome%20fundamental%20aspects%20to%20improve%20include%20data%20efficiency%2C%20generalization%0Acapability%2C%20and%20ability%20to%20learn%20in%20sparse-reward%20environments%2C%20which%20often%0Arequire%20human-designed%20dense%20rewards.%20Meta-learning%20has%20emerged%20as%20a%20promising%0Aapproach%20to%20address%20these%20issues%20by%20optimizing%20components%20of%20the%20learning%0Aalgorithm%20to%20meet%20desired%20characteristics.%20Additionally%2C%20a%20different%20line%20of%0Awork%20has%20extensively%20studied%20the%20use%20of%20intrinsic%20rewards%20to%20enhance%20the%0Aexploration%20capabilities%20of%20algorithms.%20This%20work%20investigates%20how%0Ameta-learning%20can%20improve%20the%20training%20signal%20received%20by%20RL%20agents.%20The%20focus%0Ais%20on%20meta-learning%20intrinsic%20rewards%20under%20a%20framework%20that%20doesn%27t%20rely%20on%0Athe%20use%20of%20meta-gradients.%20We%20analyze%20and%20compare%20this%20approach%20to%20the%20use%20of%0Aextrinsic%20rewards%20and%20a%20meta-learned%20advantage%20function.%20The%20developed%0Aalgorithms%20are%20evaluated%20on%20distributions%20of%20continuous%20control%20tasks%20with%20both%0Aparametric%20and%20non-parametric%20variations%2C%20and%20with%20only%20sparse%20rewards%0Aaccessible%20for%20the%20evaluation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21546v1&entry.124074799=Read"},
{"title": "Vision-Language Model Based Handwriting Verification", "author": "Mihir Chauhan and Abhishek Satbhai and Mohammad Abuzar Hashemi and Mir Basheer Ali and Bina Ramamurthy and Mingchen Gao and Siwei Lyu and Sargur Srihari", "abstract": "  Handwriting Verification is a critical in document forensics. Deep learning\nbased approaches often face skepticism from forensic document examiners due to\ntheir lack of explainability and reliance on extensive training data and\nhandcrafted features. This paper explores using Vision Language Models (VLMs),\nsuch as OpenAI's GPT-4o and Google's PaliGemma, to address these challenges. By\nleveraging their Visual Question Answering capabilities and 0-shot\nChain-of-Thought (CoT) reasoning, our goal is to provide clear,\nhuman-understandable explanations for model decisions. Our experiments on the\nCEDAR handwriting dataset demonstrate that VLMs offer enhanced\ninterpretability, reduce the need for large training datasets, and adapt better\nto diverse handwriting styles. However, results show that the CNN-based\nResNet-18 architecture outperforms the 0-shot CoT prompt engineering approach\nwith GPT-4o (Accuracy: 70%) and supervised fine-tuned PaliGemma (Accuracy:\n71%), achieving an accuracy of 84% on the CEDAR AND dataset. These findings\nhighlight the potential of VLMs in generating human-interpretable decisions\nwhile underscoring the need for further advancements to match the performance\nof specialized deep learning models.\n", "link": "http://arxiv.org/abs/2407.21788v1", "date": "2024-07-31", "relevancy": 1.9292, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4858}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4818}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4814}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-Language%20Model%20Based%20Handwriting%20Verification&body=Title%3A%20Vision-Language%20Model%20Based%20Handwriting%20Verification%0AAuthor%3A%20Mihir%20Chauhan%20and%20Abhishek%20Satbhai%20and%20Mohammad%20Abuzar%20Hashemi%20and%20Mir%20Basheer%20Ali%20and%20Bina%20Ramamurthy%20and%20Mingchen%20Gao%20and%20Siwei%20Lyu%20and%20Sargur%20Srihari%0AAbstract%3A%20%20%20Handwriting%20Verification%20is%20a%20critical%20in%20document%20forensics.%20Deep%20learning%0Abased%20approaches%20often%20face%20skepticism%20from%20forensic%20document%20examiners%20due%20to%0Atheir%20lack%20of%20explainability%20and%20reliance%20on%20extensive%20training%20data%20and%0Ahandcrafted%20features.%20This%20paper%20explores%20using%20Vision%20Language%20Models%20%28VLMs%29%2C%0Asuch%20as%20OpenAI%27s%20GPT-4o%20and%20Google%27s%20PaliGemma%2C%20to%20address%20these%20challenges.%20By%0Aleveraging%20their%20Visual%20Question%20Answering%20capabilities%20and%200-shot%0AChain-of-Thought%20%28CoT%29%20reasoning%2C%20our%20goal%20is%20to%20provide%20clear%2C%0Ahuman-understandable%20explanations%20for%20model%20decisions.%20Our%20experiments%20on%20the%0ACEDAR%20handwriting%20dataset%20demonstrate%20that%20VLMs%20offer%20enhanced%0Ainterpretability%2C%20reduce%20the%20need%20for%20large%20training%20datasets%2C%20and%20adapt%20better%0Ato%20diverse%20handwriting%20styles.%20However%2C%20results%20show%20that%20the%20CNN-based%0AResNet-18%20architecture%20outperforms%20the%200-shot%20CoT%20prompt%20engineering%20approach%0Awith%20GPT-4o%20%28Accuracy%3A%2070%25%29%20and%20supervised%20fine-tuned%20PaliGemma%20%28Accuracy%3A%0A71%25%29%2C%20achieving%20an%20accuracy%20of%2084%25%20on%20the%20CEDAR%20AND%20dataset.%20These%20findings%0Ahighlight%20the%20potential%20of%20VLMs%20in%20generating%20human-interpretable%20decisions%0Awhile%20underscoring%20the%20need%20for%20further%20advancements%20to%20match%20the%20performance%0Aof%20specialized%20deep%20learning%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21788v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-Language%2520Model%2520Based%2520Handwriting%2520Verification%26entry.906535625%3DMihir%2520Chauhan%2520and%2520Abhishek%2520Satbhai%2520and%2520Mohammad%2520Abuzar%2520Hashemi%2520and%2520Mir%2520Basheer%2520Ali%2520and%2520Bina%2520Ramamurthy%2520and%2520Mingchen%2520Gao%2520and%2520Siwei%2520Lyu%2520and%2520Sargur%2520Srihari%26entry.1292438233%3D%2520%2520Handwriting%2520Verification%2520is%2520a%2520critical%2520in%2520document%2520forensics.%2520Deep%2520learning%250Abased%2520approaches%2520often%2520face%2520skepticism%2520from%2520forensic%2520document%2520examiners%2520due%2520to%250Atheir%2520lack%2520of%2520explainability%2520and%2520reliance%2520on%2520extensive%2520training%2520data%2520and%250Ahandcrafted%2520features.%2520This%2520paper%2520explores%2520using%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%252C%250Asuch%2520as%2520OpenAI%2527s%2520GPT-4o%2520and%2520Google%2527s%2520PaliGemma%252C%2520to%2520address%2520these%2520challenges.%2520By%250Aleveraging%2520their%2520Visual%2520Question%2520Answering%2520capabilities%2520and%25200-shot%250AChain-of-Thought%2520%2528CoT%2529%2520reasoning%252C%2520our%2520goal%2520is%2520to%2520provide%2520clear%252C%250Ahuman-understandable%2520explanations%2520for%2520model%2520decisions.%2520Our%2520experiments%2520on%2520the%250ACEDAR%2520handwriting%2520dataset%2520demonstrate%2520that%2520VLMs%2520offer%2520enhanced%250Ainterpretability%252C%2520reduce%2520the%2520need%2520for%2520large%2520training%2520datasets%252C%2520and%2520adapt%2520better%250Ato%2520diverse%2520handwriting%2520styles.%2520However%252C%2520results%2520show%2520that%2520the%2520CNN-based%250AResNet-18%2520architecture%2520outperforms%2520the%25200-shot%2520CoT%2520prompt%2520engineering%2520approach%250Awith%2520GPT-4o%2520%2528Accuracy%253A%252070%2525%2529%2520and%2520supervised%2520fine-tuned%2520PaliGemma%2520%2528Accuracy%253A%250A71%2525%2529%252C%2520achieving%2520an%2520accuracy%2520of%252084%2525%2520on%2520the%2520CEDAR%2520AND%2520dataset.%2520These%2520findings%250Ahighlight%2520the%2520potential%2520of%2520VLMs%2520in%2520generating%2520human-interpretable%2520decisions%250Awhile%2520underscoring%2520the%2520need%2520for%2520further%2520advancements%2520to%2520match%2520the%2520performance%250Aof%2520specialized%2520deep%2520learning%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21788v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-Language%20Model%20Based%20Handwriting%20Verification&entry.906535625=Mihir%20Chauhan%20and%20Abhishek%20Satbhai%20and%20Mohammad%20Abuzar%20Hashemi%20and%20Mir%20Basheer%20Ali%20and%20Bina%20Ramamurthy%20and%20Mingchen%20Gao%20and%20Siwei%20Lyu%20and%20Sargur%20Srihari&entry.1292438233=%20%20Handwriting%20Verification%20is%20a%20critical%20in%20document%20forensics.%20Deep%20learning%0Abased%20approaches%20often%20face%20skepticism%20from%20forensic%20document%20examiners%20due%20to%0Atheir%20lack%20of%20explainability%20and%20reliance%20on%20extensive%20training%20data%20and%0Ahandcrafted%20features.%20This%20paper%20explores%20using%20Vision%20Language%20Models%20%28VLMs%29%2C%0Asuch%20as%20OpenAI%27s%20GPT-4o%20and%20Google%27s%20PaliGemma%2C%20to%20address%20these%20challenges.%20By%0Aleveraging%20their%20Visual%20Question%20Answering%20capabilities%20and%200-shot%0AChain-of-Thought%20%28CoT%29%20reasoning%2C%20our%20goal%20is%20to%20provide%20clear%2C%0Ahuman-understandable%20explanations%20for%20model%20decisions.%20Our%20experiments%20on%20the%0ACEDAR%20handwriting%20dataset%20demonstrate%20that%20VLMs%20offer%20enhanced%0Ainterpretability%2C%20reduce%20the%20need%20for%20large%20training%20datasets%2C%20and%20adapt%20better%0Ato%20diverse%20handwriting%20styles.%20However%2C%20results%20show%20that%20the%20CNN-based%0AResNet-18%20architecture%20outperforms%20the%200-shot%20CoT%20prompt%20engineering%20approach%0Awith%20GPT-4o%20%28Accuracy%3A%2070%25%29%20and%20supervised%20fine-tuned%20PaliGemma%20%28Accuracy%3A%0A71%25%29%2C%20achieving%20an%20accuracy%20of%2084%25%20on%20the%20CEDAR%20AND%20dataset.%20These%20findings%0Ahighlight%20the%20potential%20of%20VLMs%20in%20generating%20human-interpretable%20decisions%0Awhile%20underscoring%20the%20need%20for%20further%20advancements%20to%20match%20the%20performance%0Aof%20specialized%20deep%20learning%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21788v1&entry.124074799=Read"},
{"title": "Measuring What Matters: Intrinsic Distance Preservation as a Robust\n  Metric for Embedding Quality", "author": "Steven N. Hart and Thomas E. Tavolara", "abstract": "  Unsupervised embeddings are fundamental to numerous machine learning\napplications, yet their evaluation remains a challenging task. Traditional\nassessment methods often rely on extrinsic variables, such as performance in\ndownstream tasks, which can introduce confounding factors and mask the true\nquality of embeddings. This paper introduces the Intrinsic Distance\nPreservation Evaluation (IDPE) method, a novel approach for assessing embedding\nquality based on the preservation of Mahalanobis distances between data points\nin the original and embedded spaces. We demonstrate the limitations of\nextrinsic evaluation methods through a simple example, highlighting how they\ncan lead to misleading conclusions about embedding quality. IDPE addresses\nthese issues by providing a task-independent measure of how well embeddings\npreserve the intrinsic structure of the original data. Our method leverages\nefficient similarity search techniques to make it applicable to large-scale\ndatasets. We compare IDPE with established intrinsic metrics like\ntrustworthiness and continuity, as well as extrinsic metrics such as Average\nRank and Mean Reciprocal Rank. Our results show that IDPE offers a more\ncomprehensive and reliable assessment of embedding quality across various\nscenarios. We evaluate PCA and t-SNE embeddings using IDPE, revealing insights\ninto their performance that are not captured by traditional metrics. This work\ncontributes to the field by providing a robust, efficient, and interpretable\nmethod for embedding evaluation. IDPE's focus on intrinsic properties offers a\nvaluable tool for researchers and practitioners seeking to develop and assess\nhigh-quality embeddings for diverse machine learning applications.\n", "link": "http://arxiv.org/abs/2407.21590v1", "date": "2024-07-31", "relevancy": 1.9151, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4876}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4839}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4701}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Measuring%20What%20Matters%3A%20Intrinsic%20Distance%20Preservation%20as%20a%20Robust%0A%20%20Metric%20for%20Embedding%20Quality&body=Title%3A%20Measuring%20What%20Matters%3A%20Intrinsic%20Distance%20Preservation%20as%20a%20Robust%0A%20%20Metric%20for%20Embedding%20Quality%0AAuthor%3A%20Steven%20N.%20Hart%20and%20Thomas%20E.%20Tavolara%0AAbstract%3A%20%20%20Unsupervised%20embeddings%20are%20fundamental%20to%20numerous%20machine%20learning%0Aapplications%2C%20yet%20their%20evaluation%20remains%20a%20challenging%20task.%20Traditional%0Aassessment%20methods%20often%20rely%20on%20extrinsic%20variables%2C%20such%20as%20performance%20in%0Adownstream%20tasks%2C%20which%20can%20introduce%20confounding%20factors%20and%20mask%20the%20true%0Aquality%20of%20embeddings.%20This%20paper%20introduces%20the%20Intrinsic%20Distance%0APreservation%20Evaluation%20%28IDPE%29%20method%2C%20a%20novel%20approach%20for%20assessing%20embedding%0Aquality%20based%20on%20the%20preservation%20of%20Mahalanobis%20distances%20between%20data%20points%0Ain%20the%20original%20and%20embedded%20spaces.%20We%20demonstrate%20the%20limitations%20of%0Aextrinsic%20evaluation%20methods%20through%20a%20simple%20example%2C%20highlighting%20how%20they%0Acan%20lead%20to%20misleading%20conclusions%20about%20embedding%20quality.%20IDPE%20addresses%0Athese%20issues%20by%20providing%20a%20task-independent%20measure%20of%20how%20well%20embeddings%0Apreserve%20the%20intrinsic%20structure%20of%20the%20original%20data.%20Our%20method%20leverages%0Aefficient%20similarity%20search%20techniques%20to%20make%20it%20applicable%20to%20large-scale%0Adatasets.%20We%20compare%20IDPE%20with%20established%20intrinsic%20metrics%20like%0Atrustworthiness%20and%20continuity%2C%20as%20well%20as%20extrinsic%20metrics%20such%20as%20Average%0ARank%20and%20Mean%20Reciprocal%20Rank.%20Our%20results%20show%20that%20IDPE%20offers%20a%20more%0Acomprehensive%20and%20reliable%20assessment%20of%20embedding%20quality%20across%20various%0Ascenarios.%20We%20evaluate%20PCA%20and%20t-SNE%20embeddings%20using%20IDPE%2C%20revealing%20insights%0Ainto%20their%20performance%20that%20are%20not%20captured%20by%20traditional%20metrics.%20This%20work%0Acontributes%20to%20the%20field%20by%20providing%20a%20robust%2C%20efficient%2C%20and%20interpretable%0Amethod%20for%20embedding%20evaluation.%20IDPE%27s%20focus%20on%20intrinsic%20properties%20offers%20a%0Avaluable%20tool%20for%20researchers%20and%20practitioners%20seeking%20to%20develop%20and%20assess%0Ahigh-quality%20embeddings%20for%20diverse%20machine%20learning%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21590v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeasuring%2520What%2520Matters%253A%2520Intrinsic%2520Distance%2520Preservation%2520as%2520a%2520Robust%250A%2520%2520Metric%2520for%2520Embedding%2520Quality%26entry.906535625%3DSteven%2520N.%2520Hart%2520and%2520Thomas%2520E.%2520Tavolara%26entry.1292438233%3D%2520%2520Unsupervised%2520embeddings%2520are%2520fundamental%2520to%2520numerous%2520machine%2520learning%250Aapplications%252C%2520yet%2520their%2520evaluation%2520remains%2520a%2520challenging%2520task.%2520Traditional%250Aassessment%2520methods%2520often%2520rely%2520on%2520extrinsic%2520variables%252C%2520such%2520as%2520performance%2520in%250Adownstream%2520tasks%252C%2520which%2520can%2520introduce%2520confounding%2520factors%2520and%2520mask%2520the%2520true%250Aquality%2520of%2520embeddings.%2520This%2520paper%2520introduces%2520the%2520Intrinsic%2520Distance%250APreservation%2520Evaluation%2520%2528IDPE%2529%2520method%252C%2520a%2520novel%2520approach%2520for%2520assessing%2520embedding%250Aquality%2520based%2520on%2520the%2520preservation%2520of%2520Mahalanobis%2520distances%2520between%2520data%2520points%250Ain%2520the%2520original%2520and%2520embedded%2520spaces.%2520We%2520demonstrate%2520the%2520limitations%2520of%250Aextrinsic%2520evaluation%2520methods%2520through%2520a%2520simple%2520example%252C%2520highlighting%2520how%2520they%250Acan%2520lead%2520to%2520misleading%2520conclusions%2520about%2520embedding%2520quality.%2520IDPE%2520addresses%250Athese%2520issues%2520by%2520providing%2520a%2520task-independent%2520measure%2520of%2520how%2520well%2520embeddings%250Apreserve%2520the%2520intrinsic%2520structure%2520of%2520the%2520original%2520data.%2520Our%2520method%2520leverages%250Aefficient%2520similarity%2520search%2520techniques%2520to%2520make%2520it%2520applicable%2520to%2520large-scale%250Adatasets.%2520We%2520compare%2520IDPE%2520with%2520established%2520intrinsic%2520metrics%2520like%250Atrustworthiness%2520and%2520continuity%252C%2520as%2520well%2520as%2520extrinsic%2520metrics%2520such%2520as%2520Average%250ARank%2520and%2520Mean%2520Reciprocal%2520Rank.%2520Our%2520results%2520show%2520that%2520IDPE%2520offers%2520a%2520more%250Acomprehensive%2520and%2520reliable%2520assessment%2520of%2520embedding%2520quality%2520across%2520various%250Ascenarios.%2520We%2520evaluate%2520PCA%2520and%2520t-SNE%2520embeddings%2520using%2520IDPE%252C%2520revealing%2520insights%250Ainto%2520their%2520performance%2520that%2520are%2520not%2520captured%2520by%2520traditional%2520metrics.%2520This%2520work%250Acontributes%2520to%2520the%2520field%2520by%2520providing%2520a%2520robust%252C%2520efficient%252C%2520and%2520interpretable%250Amethod%2520for%2520embedding%2520evaluation.%2520IDPE%2527s%2520focus%2520on%2520intrinsic%2520properties%2520offers%2520a%250Avaluable%2520tool%2520for%2520researchers%2520and%2520practitioners%2520seeking%2520to%2520develop%2520and%2520assess%250Ahigh-quality%2520embeddings%2520for%2520diverse%2520machine%2520learning%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21590v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Measuring%20What%20Matters%3A%20Intrinsic%20Distance%20Preservation%20as%20a%20Robust%0A%20%20Metric%20for%20Embedding%20Quality&entry.906535625=Steven%20N.%20Hart%20and%20Thomas%20E.%20Tavolara&entry.1292438233=%20%20Unsupervised%20embeddings%20are%20fundamental%20to%20numerous%20machine%20learning%0Aapplications%2C%20yet%20their%20evaluation%20remains%20a%20challenging%20task.%20Traditional%0Aassessment%20methods%20often%20rely%20on%20extrinsic%20variables%2C%20such%20as%20performance%20in%0Adownstream%20tasks%2C%20which%20can%20introduce%20confounding%20factors%20and%20mask%20the%20true%0Aquality%20of%20embeddings.%20This%20paper%20introduces%20the%20Intrinsic%20Distance%0APreservation%20Evaluation%20%28IDPE%29%20method%2C%20a%20novel%20approach%20for%20assessing%20embedding%0Aquality%20based%20on%20the%20preservation%20of%20Mahalanobis%20distances%20between%20data%20points%0Ain%20the%20original%20and%20embedded%20spaces.%20We%20demonstrate%20the%20limitations%20of%0Aextrinsic%20evaluation%20methods%20through%20a%20simple%20example%2C%20highlighting%20how%20they%0Acan%20lead%20to%20misleading%20conclusions%20about%20embedding%20quality.%20IDPE%20addresses%0Athese%20issues%20by%20providing%20a%20task-independent%20measure%20of%20how%20well%20embeddings%0Apreserve%20the%20intrinsic%20structure%20of%20the%20original%20data.%20Our%20method%20leverages%0Aefficient%20similarity%20search%20techniques%20to%20make%20it%20applicable%20to%20large-scale%0Adatasets.%20We%20compare%20IDPE%20with%20established%20intrinsic%20metrics%20like%0Atrustworthiness%20and%20continuity%2C%20as%20well%20as%20extrinsic%20metrics%20such%20as%20Average%0ARank%20and%20Mean%20Reciprocal%20Rank.%20Our%20results%20show%20that%20IDPE%20offers%20a%20more%0Acomprehensive%20and%20reliable%20assessment%20of%20embedding%20quality%20across%20various%0Ascenarios.%20We%20evaluate%20PCA%20and%20t-SNE%20embeddings%20using%20IDPE%2C%20revealing%20insights%0Ainto%20their%20performance%20that%20are%20not%20captured%20by%20traditional%20metrics.%20This%20work%0Acontributes%20to%20the%20field%20by%20providing%20a%20robust%2C%20efficient%2C%20and%20interpretable%0Amethod%20for%20embedding%20evaluation.%20IDPE%27s%20focus%20on%20intrinsic%20properties%20offers%20a%0Avaluable%20tool%20for%20researchers%20and%20practitioners%20seeking%20to%20develop%20and%20assess%0Ahigh-quality%20embeddings%20for%20diverse%20machine%20learning%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21590v1&entry.124074799=Read"},
{"title": "TRGR: Transmissive RIS-aided Gait Recognition Through Walls", "author": "Yunlong Huang and Junshuo Liu and Jianan Zhang and Tiebin Mi and Xin Shi and Robert Caiming Qiu", "abstract": "  Gait recognition with radio frequency (RF) signals enables many potential\napplications requiring accurate identification. However, current systems\nrequire individuals to be within a line-of-sight (LOS) environment and struggle\nwith low signal-to-noise ratio (SNR) when signals traverse concrete and thick\nwalls. To address these challenges, we present TRGR, a novel transmissive\nreconfigurable intelligent surface (RIS)-aided gait recognition system. TRGR\ncan recognize human identities through walls using only the magnitude\nmeasurements of channel state information (CSI) from a pair of transceivers.\nSpecifically, by leveraging transmissive RIS alongside a configuration\nalternating optimization algorithm, TRGR enhances wall penetration and signal\nquality, enabling accurate gait recognition. Furthermore, a residual\nconvolution network (RCNN) is proposed as the backbone network to learn robust\nhuman information. Experimental results confirm the efficacy of transmissive\nRIS, highlighting the significant potential of transmissive RIS in enhancing\nRF-based gait recognition systems. Extensive experiment results show that TRGR\nachieves an average accuracy of 97.88\\% in identifying persons when signals\ntraverse concrete walls, demonstrating the effectiveness and robustness of\nTRGR.\n", "link": "http://arxiv.org/abs/2407.21566v1", "date": "2024-07-31", "relevancy": 1.9104, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5411}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4448}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TRGR%3A%20Transmissive%20RIS-aided%20Gait%20Recognition%20Through%20Walls&body=Title%3A%20TRGR%3A%20Transmissive%20RIS-aided%20Gait%20Recognition%20Through%20Walls%0AAuthor%3A%20Yunlong%20Huang%20and%20Junshuo%20Liu%20and%20Jianan%20Zhang%20and%20Tiebin%20Mi%20and%20Xin%20Shi%20and%20Robert%20Caiming%20Qiu%0AAbstract%3A%20%20%20Gait%20recognition%20with%20radio%20frequency%20%28RF%29%20signals%20enables%20many%20potential%0Aapplications%20requiring%20accurate%20identification.%20However%2C%20current%20systems%0Arequire%20individuals%20to%20be%20within%20a%20line-of-sight%20%28LOS%29%20environment%20and%20struggle%0Awith%20low%20signal-to-noise%20ratio%20%28SNR%29%20when%20signals%20traverse%20concrete%20and%20thick%0Awalls.%20To%20address%20these%20challenges%2C%20we%20present%20TRGR%2C%20a%20novel%20transmissive%0Areconfigurable%20intelligent%20surface%20%28RIS%29-aided%20gait%20recognition%20system.%20TRGR%0Acan%20recognize%20human%20identities%20through%20walls%20using%20only%20the%20magnitude%0Ameasurements%20of%20channel%20state%20information%20%28CSI%29%20from%20a%20pair%20of%20transceivers.%0ASpecifically%2C%20by%20leveraging%20transmissive%20RIS%20alongside%20a%20configuration%0Aalternating%20optimization%20algorithm%2C%20TRGR%20enhances%20wall%20penetration%20and%20signal%0Aquality%2C%20enabling%20accurate%20gait%20recognition.%20Furthermore%2C%20a%20residual%0Aconvolution%20network%20%28RCNN%29%20is%20proposed%20as%20the%20backbone%20network%20to%20learn%20robust%0Ahuman%20information.%20Experimental%20results%20confirm%20the%20efficacy%20of%20transmissive%0ARIS%2C%20highlighting%20the%20significant%20potential%20of%20transmissive%20RIS%20in%20enhancing%0ARF-based%20gait%20recognition%20systems.%20Extensive%20experiment%20results%20show%20that%20TRGR%0Aachieves%20an%20average%20accuracy%20of%2097.88%5C%25%20in%20identifying%20persons%20when%20signals%0Atraverse%20concrete%20walls%2C%20demonstrating%20the%20effectiveness%20and%20robustness%20of%0ATRGR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21566v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTRGR%253A%2520Transmissive%2520RIS-aided%2520Gait%2520Recognition%2520Through%2520Walls%26entry.906535625%3DYunlong%2520Huang%2520and%2520Junshuo%2520Liu%2520and%2520Jianan%2520Zhang%2520and%2520Tiebin%2520Mi%2520and%2520Xin%2520Shi%2520and%2520Robert%2520Caiming%2520Qiu%26entry.1292438233%3D%2520%2520Gait%2520recognition%2520with%2520radio%2520frequency%2520%2528RF%2529%2520signals%2520enables%2520many%2520potential%250Aapplications%2520requiring%2520accurate%2520identification.%2520However%252C%2520current%2520systems%250Arequire%2520individuals%2520to%2520be%2520within%2520a%2520line-of-sight%2520%2528LOS%2529%2520environment%2520and%2520struggle%250Awith%2520low%2520signal-to-noise%2520ratio%2520%2528SNR%2529%2520when%2520signals%2520traverse%2520concrete%2520and%2520thick%250Awalls.%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%2520TRGR%252C%2520a%2520novel%2520transmissive%250Areconfigurable%2520intelligent%2520surface%2520%2528RIS%2529-aided%2520gait%2520recognition%2520system.%2520TRGR%250Acan%2520recognize%2520human%2520identities%2520through%2520walls%2520using%2520only%2520the%2520magnitude%250Ameasurements%2520of%2520channel%2520state%2520information%2520%2528CSI%2529%2520from%2520a%2520pair%2520of%2520transceivers.%250ASpecifically%252C%2520by%2520leveraging%2520transmissive%2520RIS%2520alongside%2520a%2520configuration%250Aalternating%2520optimization%2520algorithm%252C%2520TRGR%2520enhances%2520wall%2520penetration%2520and%2520signal%250Aquality%252C%2520enabling%2520accurate%2520gait%2520recognition.%2520Furthermore%252C%2520a%2520residual%250Aconvolution%2520network%2520%2528RCNN%2529%2520is%2520proposed%2520as%2520the%2520backbone%2520network%2520to%2520learn%2520robust%250Ahuman%2520information.%2520Experimental%2520results%2520confirm%2520the%2520efficacy%2520of%2520transmissive%250ARIS%252C%2520highlighting%2520the%2520significant%2520potential%2520of%2520transmissive%2520RIS%2520in%2520enhancing%250ARF-based%2520gait%2520recognition%2520systems.%2520Extensive%2520experiment%2520results%2520show%2520that%2520TRGR%250Aachieves%2520an%2520average%2520accuracy%2520of%252097.88%255C%2525%2520in%2520identifying%2520persons%2520when%2520signals%250Atraverse%2520concrete%2520walls%252C%2520demonstrating%2520the%2520effectiveness%2520and%2520robustness%2520of%250ATRGR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21566v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRGR%3A%20Transmissive%20RIS-aided%20Gait%20Recognition%20Through%20Walls&entry.906535625=Yunlong%20Huang%20and%20Junshuo%20Liu%20and%20Jianan%20Zhang%20and%20Tiebin%20Mi%20and%20Xin%20Shi%20and%20Robert%20Caiming%20Qiu&entry.1292438233=%20%20Gait%20recognition%20with%20radio%20frequency%20%28RF%29%20signals%20enables%20many%20potential%0Aapplications%20requiring%20accurate%20identification.%20However%2C%20current%20systems%0Arequire%20individuals%20to%20be%20within%20a%20line-of-sight%20%28LOS%29%20environment%20and%20struggle%0Awith%20low%20signal-to-noise%20ratio%20%28SNR%29%20when%20signals%20traverse%20concrete%20and%20thick%0Awalls.%20To%20address%20these%20challenges%2C%20we%20present%20TRGR%2C%20a%20novel%20transmissive%0Areconfigurable%20intelligent%20surface%20%28RIS%29-aided%20gait%20recognition%20system.%20TRGR%0Acan%20recognize%20human%20identities%20through%20walls%20using%20only%20the%20magnitude%0Ameasurements%20of%20channel%20state%20information%20%28CSI%29%20from%20a%20pair%20of%20transceivers.%0ASpecifically%2C%20by%20leveraging%20transmissive%20RIS%20alongside%20a%20configuration%0Aalternating%20optimization%20algorithm%2C%20TRGR%20enhances%20wall%20penetration%20and%20signal%0Aquality%2C%20enabling%20accurate%20gait%20recognition.%20Furthermore%2C%20a%20residual%0Aconvolution%20network%20%28RCNN%29%20is%20proposed%20as%20the%20backbone%20network%20to%20learn%20robust%0Ahuman%20information.%20Experimental%20results%20confirm%20the%20efficacy%20of%20transmissive%0ARIS%2C%20highlighting%20the%20significant%20potential%20of%20transmissive%20RIS%20in%20enhancing%0ARF-based%20gait%20recognition%20systems.%20Extensive%20experiment%20results%20show%20that%20TRGR%0Aachieves%20an%20average%20accuracy%20of%2097.88%5C%25%20in%20identifying%20persons%20when%20signals%0Atraverse%20concrete%20walls%2C%20demonstrating%20the%20effectiveness%20and%20robustness%20of%0ATRGR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21566v1&entry.124074799=Read"},
{"title": "Deep Learning for Options Trading: An End-To-End Approach", "author": "Wee Ling Tan and Stephen Roberts and Stefan Zohren", "abstract": "  We introduce a novel approach to options trading strategies using a highly\nscalable and data-driven machine learning algorithm. In contrast to traditional\napproaches that often require specifications of underlying market dynamics or\nassumptions on an option pricing model, our models depart fundamentally from\nthe need for these prerequisites, directly learning non-trivial mappings from\nmarket data to optimal trading signals. Backtesting on more than a decade of\noption contracts for equities listed on the S&P 100, we demonstrate that deep\nlearning models trained according to our end-to-end approach exhibit\nsignificant improvements in risk-adjusted performance over existing rules-based\ntrading strategies. We find that incorporating turnover regularization into the\nmodels leads to further performance enhancements at prohibitively high levels\nof transaction costs.\n", "link": "http://arxiv.org/abs/2407.21791v1", "date": "2024-07-31", "relevancy": 1.9082, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5155}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4511}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20for%20Options%20Trading%3A%20An%20End-To-End%20Approach&body=Title%3A%20Deep%20Learning%20for%20Options%20Trading%3A%20An%20End-To-End%20Approach%0AAuthor%3A%20Wee%20Ling%20Tan%20and%20Stephen%20Roberts%20and%20Stefan%20Zohren%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20approach%20to%20options%20trading%20strategies%20using%20a%20highly%0Ascalable%20and%20data-driven%20machine%20learning%20algorithm.%20In%20contrast%20to%20traditional%0Aapproaches%20that%20often%20require%20specifications%20of%20underlying%20market%20dynamics%20or%0Aassumptions%20on%20an%20option%20pricing%20model%2C%20our%20models%20depart%20fundamentally%20from%0Athe%20need%20for%20these%20prerequisites%2C%20directly%20learning%20non-trivial%20mappings%20from%0Amarket%20data%20to%20optimal%20trading%20signals.%20Backtesting%20on%20more%20than%20a%20decade%20of%0Aoption%20contracts%20for%20equities%20listed%20on%20the%20S%26P%20100%2C%20we%20demonstrate%20that%20deep%0Alearning%20models%20trained%20according%20to%20our%20end-to-end%20approach%20exhibit%0Asignificant%20improvements%20in%20risk-adjusted%20performance%20over%20existing%20rules-based%0Atrading%20strategies.%20We%20find%20that%20incorporating%20turnover%20regularization%20into%20the%0Amodels%20leads%20to%20further%20performance%20enhancements%20at%20prohibitively%20high%20levels%0Aof%20transaction%20costs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21791v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520for%2520Options%2520Trading%253A%2520An%2520End-To-End%2520Approach%26entry.906535625%3DWee%2520Ling%2520Tan%2520and%2520Stephen%2520Roberts%2520and%2520Stefan%2520Zohren%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520approach%2520to%2520options%2520trading%2520strategies%2520using%2520a%2520highly%250Ascalable%2520and%2520data-driven%2520machine%2520learning%2520algorithm.%2520In%2520contrast%2520to%2520traditional%250Aapproaches%2520that%2520often%2520require%2520specifications%2520of%2520underlying%2520market%2520dynamics%2520or%250Aassumptions%2520on%2520an%2520option%2520pricing%2520model%252C%2520our%2520models%2520depart%2520fundamentally%2520from%250Athe%2520need%2520for%2520these%2520prerequisites%252C%2520directly%2520learning%2520non-trivial%2520mappings%2520from%250Amarket%2520data%2520to%2520optimal%2520trading%2520signals.%2520Backtesting%2520on%2520more%2520than%2520a%2520decade%2520of%250Aoption%2520contracts%2520for%2520equities%2520listed%2520on%2520the%2520S%2526P%2520100%252C%2520we%2520demonstrate%2520that%2520deep%250Alearning%2520models%2520trained%2520according%2520to%2520our%2520end-to-end%2520approach%2520exhibit%250Asignificant%2520improvements%2520in%2520risk-adjusted%2520performance%2520over%2520existing%2520rules-based%250Atrading%2520strategies.%2520We%2520find%2520that%2520incorporating%2520turnover%2520regularization%2520into%2520the%250Amodels%2520leads%2520to%2520further%2520performance%2520enhancements%2520at%2520prohibitively%2520high%2520levels%250Aof%2520transaction%2520costs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21791v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20for%20Options%20Trading%3A%20An%20End-To-End%20Approach&entry.906535625=Wee%20Ling%20Tan%20and%20Stephen%20Roberts%20and%20Stefan%20Zohren&entry.1292438233=%20%20We%20introduce%20a%20novel%20approach%20to%20options%20trading%20strategies%20using%20a%20highly%0Ascalable%20and%20data-driven%20machine%20learning%20algorithm.%20In%20contrast%20to%20traditional%0Aapproaches%20that%20often%20require%20specifications%20of%20underlying%20market%20dynamics%20or%0Aassumptions%20on%20an%20option%20pricing%20model%2C%20our%20models%20depart%20fundamentally%20from%0Athe%20need%20for%20these%20prerequisites%2C%20directly%20learning%20non-trivial%20mappings%20from%0Amarket%20data%20to%20optimal%20trading%20signals.%20Backtesting%20on%20more%20than%20a%20decade%20of%0Aoption%20contracts%20for%20equities%20listed%20on%20the%20S%26P%20100%2C%20we%20demonstrate%20that%20deep%0Alearning%20models%20trained%20according%20to%20our%20end-to-end%20approach%20exhibit%0Asignificant%20improvements%20in%20risk-adjusted%20performance%20over%20existing%20rules-based%0Atrading%20strategies.%20We%20find%20that%20incorporating%20turnover%20regularization%20into%20the%0Amodels%20leads%20to%20further%20performance%20enhancements%20at%20prohibitively%20high%20levels%0Aof%20transaction%20costs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21791v1&entry.124074799=Read"},
{"title": "Stable Audio Open", "author": "Zach Evans and Julian D. Parker and CJ Carr and Zack Zukowski and Josiah Taylor and Jordi Pons", "abstract": "  Open generative models are vitally important for the community, allowing for\nfine-tunes and serving as baselines when presenting new models. However, most\ncurrent text-to-audio models are private and not accessible for artists and\nresearchers to build upon. Here we describe the architecture and training\nprocess of a new open-weights text-to-audio model trained with Creative Commons\ndata. Our evaluation shows that the model's performance is competitive with the\nstate-of-the-art across various metrics. Notably, the reported FDopenl3 results\n(measuring the realism of the generations) showcase its potential for\nhigh-quality stereo sound synthesis at 44.1kHz.\n", "link": "http://arxiv.org/abs/2407.14358v2", "date": "2024-07-31", "relevancy": 1.9075, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5006}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4876}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stable%20Audio%20Open&body=Title%3A%20Stable%20Audio%20Open%0AAuthor%3A%20Zach%20Evans%20and%20Julian%20D.%20Parker%20and%20CJ%20Carr%20and%20Zack%20Zukowski%20and%20Josiah%20Taylor%20and%20Jordi%20Pons%0AAbstract%3A%20%20%20Open%20generative%20models%20are%20vitally%20important%20for%20the%20community%2C%20allowing%20for%0Afine-tunes%20and%20serving%20as%20baselines%20when%20presenting%20new%20models.%20However%2C%20most%0Acurrent%20text-to-audio%20models%20are%20private%20and%20not%20accessible%20for%20artists%20and%0Aresearchers%20to%20build%20upon.%20Here%20we%20describe%20the%20architecture%20and%20training%0Aprocess%20of%20a%20new%20open-weights%20text-to-audio%20model%20trained%20with%20Creative%20Commons%0Adata.%20Our%20evaluation%20shows%20that%20the%20model%27s%20performance%20is%20competitive%20with%20the%0Astate-of-the-art%20across%20various%20metrics.%20Notably%2C%20the%20reported%20FDopenl3%20results%0A%28measuring%20the%20realism%20of%20the%20generations%29%20showcase%20its%20potential%20for%0Ahigh-quality%20stereo%20sound%20synthesis%20at%2044.1kHz.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14358v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStable%2520Audio%2520Open%26entry.906535625%3DZach%2520Evans%2520and%2520Julian%2520D.%2520Parker%2520and%2520CJ%2520Carr%2520and%2520Zack%2520Zukowski%2520and%2520Josiah%2520Taylor%2520and%2520Jordi%2520Pons%26entry.1292438233%3D%2520%2520Open%2520generative%2520models%2520are%2520vitally%2520important%2520for%2520the%2520community%252C%2520allowing%2520for%250Afine-tunes%2520and%2520serving%2520as%2520baselines%2520when%2520presenting%2520new%2520models.%2520However%252C%2520most%250Acurrent%2520text-to-audio%2520models%2520are%2520private%2520and%2520not%2520accessible%2520for%2520artists%2520and%250Aresearchers%2520to%2520build%2520upon.%2520Here%2520we%2520describe%2520the%2520architecture%2520and%2520training%250Aprocess%2520of%2520a%2520new%2520open-weights%2520text-to-audio%2520model%2520trained%2520with%2520Creative%2520Commons%250Adata.%2520Our%2520evaluation%2520shows%2520that%2520the%2520model%2527s%2520performance%2520is%2520competitive%2520with%2520the%250Astate-of-the-art%2520across%2520various%2520metrics.%2520Notably%252C%2520the%2520reported%2520FDopenl3%2520results%250A%2528measuring%2520the%2520realism%2520of%2520the%2520generations%2529%2520showcase%2520its%2520potential%2520for%250Ahigh-quality%2520stereo%2520sound%2520synthesis%2520at%252044.1kHz.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14358v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stable%20Audio%20Open&entry.906535625=Zach%20Evans%20and%20Julian%20D.%20Parker%20and%20CJ%20Carr%20and%20Zack%20Zukowski%20and%20Josiah%20Taylor%20and%20Jordi%20Pons&entry.1292438233=%20%20Open%20generative%20models%20are%20vitally%20important%20for%20the%20community%2C%20allowing%20for%0Afine-tunes%20and%20serving%20as%20baselines%20when%20presenting%20new%20models.%20However%2C%20most%0Acurrent%20text-to-audio%20models%20are%20private%20and%20not%20accessible%20for%20artists%20and%0Aresearchers%20to%20build%20upon.%20Here%20we%20describe%20the%20architecture%20and%20training%0Aprocess%20of%20a%20new%20open-weights%20text-to-audio%20model%20trained%20with%20Creative%20Commons%0Adata.%20Our%20evaluation%20shows%20that%20the%20model%27s%20performance%20is%20competitive%20with%20the%0Astate-of-the-art%20across%20various%20metrics.%20Notably%2C%20the%20reported%20FDopenl3%20results%0A%28measuring%20the%20realism%20of%20the%20generations%29%20showcase%20its%20potential%20for%0Ahigh-quality%20stereo%20sound%20synthesis%20at%2044.1kHz.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14358v2&entry.124074799=Read"},
{"title": "Ironing the Graphs: Toward a Correct Geometric Analysis of Large-Scale\n  Graphs", "author": "Saloua Naama and Kav\u00e9 Salamatian and Francesco Bronzino", "abstract": "  Graph embedding approaches attempt to project graphs into geometric entities,\ni.e, manifolds. The idea is that the geometric properties of the projected\nmanifolds are helpful in the inference of graph properties. However, if the\nchoice of the embedding manifold is incorrectly performed, it can lead to\nincorrect geometric inference. In this paper, we argue that the classical\nembedding techniques cannot lead to correct geometric interpretation as they\nmiss the curvature at each point, of manifold. We advocate that for doing\ncorrect geometric interpretation the embedding of graph should be done over\nregular constant curvature manifolds. To this end, we present an embedding\napproach, the discrete Ricci flow graph embedding (dRfge) based on the discrete\nRicci flow that adapts the distance between nodes in a graph so that the graph\ncan be embedded onto a constant curvature manifold that is homogeneous and\nisotropic, i.e., all directions are equivalent and distances comparable,\nresulting in correct geometric interpretations. A major contribution of this\npaper is that for the first time, we prove the convergence of discrete Ricci\nflow to a constant curvature and stable distance metrics over the edges. A\ndrawback of using the discrete Ricci flow is the high computational complexity\nthat prevented its usage in large-scale graph analysis. Another contribution of\nthis paper is a new algorithmic solution that makes it feasible to calculate\nthe Ricci flow for graphs of up to 50k nodes, and beyond. The intuitions behind\nthe discrete Ricci flow make it possible to obtain new insights into the\nstructure of large-scale graphs. We demonstrate this through a case study on\nanalyzing the internet connectivity structure between countries at the BGP\nlevel.\n", "link": "http://arxiv.org/abs/2407.21609v1", "date": "2024-07-31", "relevancy": 1.9039, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5026}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4932}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ironing%20the%20Graphs%3A%20Toward%20a%20Correct%20Geometric%20Analysis%20of%20Large-Scale%0A%20%20Graphs&body=Title%3A%20Ironing%20the%20Graphs%3A%20Toward%20a%20Correct%20Geometric%20Analysis%20of%20Large-Scale%0A%20%20Graphs%0AAuthor%3A%20Saloua%20Naama%20and%20Kav%C3%A9%20Salamatian%20and%20Francesco%20Bronzino%0AAbstract%3A%20%20%20Graph%20embedding%20approaches%20attempt%20to%20project%20graphs%20into%20geometric%20entities%2C%0Ai.e%2C%20manifolds.%20The%20idea%20is%20that%20the%20geometric%20properties%20of%20the%20projected%0Amanifolds%20are%20helpful%20in%20the%20inference%20of%20graph%20properties.%20However%2C%20if%20the%0Achoice%20of%20the%20embedding%20manifold%20is%20incorrectly%20performed%2C%20it%20can%20lead%20to%0Aincorrect%20geometric%20inference.%20In%20this%20paper%2C%20we%20argue%20that%20the%20classical%0Aembedding%20techniques%20cannot%20lead%20to%20correct%20geometric%20interpretation%20as%20they%0Amiss%20the%20curvature%20at%20each%20point%2C%20of%20manifold.%20We%20advocate%20that%20for%20doing%0Acorrect%20geometric%20interpretation%20the%20embedding%20of%20graph%20should%20be%20done%20over%0Aregular%20constant%20curvature%20manifolds.%20To%20this%20end%2C%20we%20present%20an%20embedding%0Aapproach%2C%20the%20discrete%20Ricci%20flow%20graph%20embedding%20%28dRfge%29%20based%20on%20the%20discrete%0ARicci%20flow%20that%20adapts%20the%20distance%20between%20nodes%20in%20a%20graph%20so%20that%20the%20graph%0Acan%20be%20embedded%20onto%20a%20constant%20curvature%20manifold%20that%20is%20homogeneous%20and%0Aisotropic%2C%20i.e.%2C%20all%20directions%20are%20equivalent%20and%20distances%20comparable%2C%0Aresulting%20in%20correct%20geometric%20interpretations.%20A%20major%20contribution%20of%20this%0Apaper%20is%20that%20for%20the%20first%20time%2C%20we%20prove%20the%20convergence%20of%20discrete%20Ricci%0Aflow%20to%20a%20constant%20curvature%20and%20stable%20distance%20metrics%20over%20the%20edges.%20A%0Adrawback%20of%20using%20the%20discrete%20Ricci%20flow%20is%20the%20high%20computational%20complexity%0Athat%20prevented%20its%20usage%20in%20large-scale%20graph%20analysis.%20Another%20contribution%20of%0Athis%20paper%20is%20a%20new%20algorithmic%20solution%20that%20makes%20it%20feasible%20to%20calculate%0Athe%20Ricci%20flow%20for%20graphs%20of%20up%20to%2050k%20nodes%2C%20and%20beyond.%20The%20intuitions%20behind%0Athe%20discrete%20Ricci%20flow%20make%20it%20possible%20to%20obtain%20new%20insights%20into%20the%0Astructure%20of%20large-scale%20graphs.%20We%20demonstrate%20this%20through%20a%20case%20study%20on%0Aanalyzing%20the%20internet%20connectivity%20structure%20between%20countries%20at%20the%20BGP%0Alevel.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIroning%2520the%2520Graphs%253A%2520Toward%2520a%2520Correct%2520Geometric%2520Analysis%2520of%2520Large-Scale%250A%2520%2520Graphs%26entry.906535625%3DSaloua%2520Naama%2520and%2520Kav%25C3%25A9%2520Salamatian%2520and%2520Francesco%2520Bronzino%26entry.1292438233%3D%2520%2520Graph%2520embedding%2520approaches%2520attempt%2520to%2520project%2520graphs%2520into%2520geometric%2520entities%252C%250Ai.e%252C%2520manifolds.%2520The%2520idea%2520is%2520that%2520the%2520geometric%2520properties%2520of%2520the%2520projected%250Amanifolds%2520are%2520helpful%2520in%2520the%2520inference%2520of%2520graph%2520properties.%2520However%252C%2520if%2520the%250Achoice%2520of%2520the%2520embedding%2520manifold%2520is%2520incorrectly%2520performed%252C%2520it%2520can%2520lead%2520to%250Aincorrect%2520geometric%2520inference.%2520In%2520this%2520paper%252C%2520we%2520argue%2520that%2520the%2520classical%250Aembedding%2520techniques%2520cannot%2520lead%2520to%2520correct%2520geometric%2520interpretation%2520as%2520they%250Amiss%2520the%2520curvature%2520at%2520each%2520point%252C%2520of%2520manifold.%2520We%2520advocate%2520that%2520for%2520doing%250Acorrect%2520geometric%2520interpretation%2520the%2520embedding%2520of%2520graph%2520should%2520be%2520done%2520over%250Aregular%2520constant%2520curvature%2520manifolds.%2520To%2520this%2520end%252C%2520we%2520present%2520an%2520embedding%250Aapproach%252C%2520the%2520discrete%2520Ricci%2520flow%2520graph%2520embedding%2520%2528dRfge%2529%2520based%2520on%2520the%2520discrete%250ARicci%2520flow%2520that%2520adapts%2520the%2520distance%2520between%2520nodes%2520in%2520a%2520graph%2520so%2520that%2520the%2520graph%250Acan%2520be%2520embedded%2520onto%2520a%2520constant%2520curvature%2520manifold%2520that%2520is%2520homogeneous%2520and%250Aisotropic%252C%2520i.e.%252C%2520all%2520directions%2520are%2520equivalent%2520and%2520distances%2520comparable%252C%250Aresulting%2520in%2520correct%2520geometric%2520interpretations.%2520A%2520major%2520contribution%2520of%2520this%250Apaper%2520is%2520that%2520for%2520the%2520first%2520time%252C%2520we%2520prove%2520the%2520convergence%2520of%2520discrete%2520Ricci%250Aflow%2520to%2520a%2520constant%2520curvature%2520and%2520stable%2520distance%2520metrics%2520over%2520the%2520edges.%2520A%250Adrawback%2520of%2520using%2520the%2520discrete%2520Ricci%2520flow%2520is%2520the%2520high%2520computational%2520complexity%250Athat%2520prevented%2520its%2520usage%2520in%2520large-scale%2520graph%2520analysis.%2520Another%2520contribution%2520of%250Athis%2520paper%2520is%2520a%2520new%2520algorithmic%2520solution%2520that%2520makes%2520it%2520feasible%2520to%2520calculate%250Athe%2520Ricci%2520flow%2520for%2520graphs%2520of%2520up%2520to%252050k%2520nodes%252C%2520and%2520beyond.%2520The%2520intuitions%2520behind%250Athe%2520discrete%2520Ricci%2520flow%2520make%2520it%2520possible%2520to%2520obtain%2520new%2520insights%2520into%2520the%250Astructure%2520of%2520large-scale%2520graphs.%2520We%2520demonstrate%2520this%2520through%2520a%2520case%2520study%2520on%250Aanalyzing%2520the%2520internet%2520connectivity%2520structure%2520between%2520countries%2520at%2520the%2520BGP%250Alevel.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ironing%20the%20Graphs%3A%20Toward%20a%20Correct%20Geometric%20Analysis%20of%20Large-Scale%0A%20%20Graphs&entry.906535625=Saloua%20Naama%20and%20Kav%C3%A9%20Salamatian%20and%20Francesco%20Bronzino&entry.1292438233=%20%20Graph%20embedding%20approaches%20attempt%20to%20project%20graphs%20into%20geometric%20entities%2C%0Ai.e%2C%20manifolds.%20The%20idea%20is%20that%20the%20geometric%20properties%20of%20the%20projected%0Amanifolds%20are%20helpful%20in%20the%20inference%20of%20graph%20properties.%20However%2C%20if%20the%0Achoice%20of%20the%20embedding%20manifold%20is%20incorrectly%20performed%2C%20it%20can%20lead%20to%0Aincorrect%20geometric%20inference.%20In%20this%20paper%2C%20we%20argue%20that%20the%20classical%0Aembedding%20techniques%20cannot%20lead%20to%20correct%20geometric%20interpretation%20as%20they%0Amiss%20the%20curvature%20at%20each%20point%2C%20of%20manifold.%20We%20advocate%20that%20for%20doing%0Acorrect%20geometric%20interpretation%20the%20embedding%20of%20graph%20should%20be%20done%20over%0Aregular%20constant%20curvature%20manifolds.%20To%20this%20end%2C%20we%20present%20an%20embedding%0Aapproach%2C%20the%20discrete%20Ricci%20flow%20graph%20embedding%20%28dRfge%29%20based%20on%20the%20discrete%0ARicci%20flow%20that%20adapts%20the%20distance%20between%20nodes%20in%20a%20graph%20so%20that%20the%20graph%0Acan%20be%20embedded%20onto%20a%20constant%20curvature%20manifold%20that%20is%20homogeneous%20and%0Aisotropic%2C%20i.e.%2C%20all%20directions%20are%20equivalent%20and%20distances%20comparable%2C%0Aresulting%20in%20correct%20geometric%20interpretations.%20A%20major%20contribution%20of%20this%0Apaper%20is%20that%20for%20the%20first%20time%2C%20we%20prove%20the%20convergence%20of%20discrete%20Ricci%0Aflow%20to%20a%20constant%20curvature%20and%20stable%20distance%20metrics%20over%20the%20edges.%20A%0Adrawback%20of%20using%20the%20discrete%20Ricci%20flow%20is%20the%20high%20computational%20complexity%0Athat%20prevented%20its%20usage%20in%20large-scale%20graph%20analysis.%20Another%20contribution%20of%0Athis%20paper%20is%20a%20new%20algorithmic%20solution%20that%20makes%20it%20feasible%20to%20calculate%0Athe%20Ricci%20flow%20for%20graphs%20of%20up%20to%2050k%20nodes%2C%20and%20beyond.%20The%20intuitions%20behind%0Athe%20discrete%20Ricci%20flow%20make%20it%20possible%20to%20obtain%20new%20insights%20into%20the%0Astructure%20of%20large-scale%20graphs.%20We%20demonstrate%20this%20through%20a%20case%20study%20on%0Aanalyzing%20the%20internet%20connectivity%20structure%20between%20countries%20at%20the%20BGP%0Alevel.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21609v1&entry.124074799=Read"},
{"title": "Tora: Trajectory-oriented Diffusion Transformer for Video Generation", "author": "Zhenghao Zhang and Junchao Liao and Menghao Li and Long Qin and Weizhi Wang", "abstract": "  Recent advancements in Diffusion Transformer (DiT) have demonstrated\nremarkable proficiency in producing high-quality video content. Nonetheless,\nthe potential of transformer-based diffusion models for effectively generating\nvideos with controllable motion remains an area of limited exploration. This\npaper introduces Tora, the first trajectory-oriented DiT framework that\nintegrates textual, visual, and trajectory conditions concurrently for video\ngeneration. Specifically, Tora consists of a Trajectory Extractor~(TE), a\nSpatial-Temporal DiT, and a Motion-guidance Fuser~(MGF). The TE encodes\narbitrary trajectories into hierarchical spacetime motion patches with a 3D\nvideo compression network. The MGF integrates the motion patches into the DiT\nblocks to generate consistent videos following trajectories. Our design aligns\nseamlessly with DiT's scalability, allowing precise control of video content's\ndynamics with diverse durations, aspect ratios, and resolutions. Extensive\nexperiments demonstrate Tora's excellence in achieving high motion fidelity,\nwhile also meticulously simulating the movement of the physical world. Page can\nbe found at https://ali-videoai.github.io/tora_video.\n", "link": "http://arxiv.org/abs/2407.21705v1", "date": "2024-07-31", "relevancy": 1.9018, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6823}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6235}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6118}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tora%3A%20Trajectory-oriented%20Diffusion%20Transformer%20for%20Video%20Generation&body=Title%3A%20Tora%3A%20Trajectory-oriented%20Diffusion%20Transformer%20for%20Video%20Generation%0AAuthor%3A%20Zhenghao%20Zhang%20and%20Junchao%20Liao%20and%20Menghao%20Li%20and%20Long%20Qin%20and%20Weizhi%20Wang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Diffusion%20Transformer%20%28DiT%29%20have%20demonstrated%0Aremarkable%20proficiency%20in%20producing%20high-quality%20video%20content.%20Nonetheless%2C%0Athe%20potential%20of%20transformer-based%20diffusion%20models%20for%20effectively%20generating%0Avideos%20with%20controllable%20motion%20remains%20an%20area%20of%20limited%20exploration.%20This%0Apaper%20introduces%20Tora%2C%20the%20first%20trajectory-oriented%20DiT%20framework%20that%0Aintegrates%20textual%2C%20visual%2C%20and%20trajectory%20conditions%20concurrently%20for%20video%0Ageneration.%20Specifically%2C%20Tora%20consists%20of%20a%20Trajectory%20Extractor~%28TE%29%2C%20a%0ASpatial-Temporal%20DiT%2C%20and%20a%20Motion-guidance%20Fuser~%28MGF%29.%20The%20TE%20encodes%0Aarbitrary%20trajectories%20into%20hierarchical%20spacetime%20motion%20patches%20with%20a%203D%0Avideo%20compression%20network.%20The%20MGF%20integrates%20the%20motion%20patches%20into%20the%20DiT%0Ablocks%20to%20generate%20consistent%20videos%20following%20trajectories.%20Our%20design%20aligns%0Aseamlessly%20with%20DiT%27s%20scalability%2C%20allowing%20precise%20control%20of%20video%20content%27s%0Adynamics%20with%20diverse%20durations%2C%20aspect%20ratios%2C%20and%20resolutions.%20Extensive%0Aexperiments%20demonstrate%20Tora%27s%20excellence%20in%20achieving%20high%20motion%20fidelity%2C%0Awhile%20also%20meticulously%20simulating%20the%20movement%20of%20the%20physical%20world.%20Page%20can%0Abe%20found%20at%20https%3A//ali-videoai.github.io/tora_video.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21705v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTora%253A%2520Trajectory-oriented%2520Diffusion%2520Transformer%2520for%2520Video%2520Generation%26entry.906535625%3DZhenghao%2520Zhang%2520and%2520Junchao%2520Liao%2520and%2520Menghao%2520Li%2520and%2520Long%2520Qin%2520and%2520Weizhi%2520Wang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Diffusion%2520Transformer%2520%2528DiT%2529%2520have%2520demonstrated%250Aremarkable%2520proficiency%2520in%2520producing%2520high-quality%2520video%2520content.%2520Nonetheless%252C%250Athe%2520potential%2520of%2520transformer-based%2520diffusion%2520models%2520for%2520effectively%2520generating%250Avideos%2520with%2520controllable%2520motion%2520remains%2520an%2520area%2520of%2520limited%2520exploration.%2520This%250Apaper%2520introduces%2520Tora%252C%2520the%2520first%2520trajectory-oriented%2520DiT%2520framework%2520that%250Aintegrates%2520textual%252C%2520visual%252C%2520and%2520trajectory%2520conditions%2520concurrently%2520for%2520video%250Ageneration.%2520Specifically%252C%2520Tora%2520consists%2520of%2520a%2520Trajectory%2520Extractor~%2528TE%2529%252C%2520a%250ASpatial-Temporal%2520DiT%252C%2520and%2520a%2520Motion-guidance%2520Fuser~%2528MGF%2529.%2520The%2520TE%2520encodes%250Aarbitrary%2520trajectories%2520into%2520hierarchical%2520spacetime%2520motion%2520patches%2520with%2520a%25203D%250Avideo%2520compression%2520network.%2520The%2520MGF%2520integrates%2520the%2520motion%2520patches%2520into%2520the%2520DiT%250Ablocks%2520to%2520generate%2520consistent%2520videos%2520following%2520trajectories.%2520Our%2520design%2520aligns%250Aseamlessly%2520with%2520DiT%2527s%2520scalability%252C%2520allowing%2520precise%2520control%2520of%2520video%2520content%2527s%250Adynamics%2520with%2520diverse%2520durations%252C%2520aspect%2520ratios%252C%2520and%2520resolutions.%2520Extensive%250Aexperiments%2520demonstrate%2520Tora%2527s%2520excellence%2520in%2520achieving%2520high%2520motion%2520fidelity%252C%250Awhile%2520also%2520meticulously%2520simulating%2520the%2520movement%2520of%2520the%2520physical%2520world.%2520Page%2520can%250Abe%2520found%2520at%2520https%253A//ali-videoai.github.io/tora_video.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21705v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tora%3A%20Trajectory-oriented%20Diffusion%20Transformer%20for%20Video%20Generation&entry.906535625=Zhenghao%20Zhang%20and%20Junchao%20Liao%20and%20Menghao%20Li%20and%20Long%20Qin%20and%20Weizhi%20Wang&entry.1292438233=%20%20Recent%20advancements%20in%20Diffusion%20Transformer%20%28DiT%29%20have%20demonstrated%0Aremarkable%20proficiency%20in%20producing%20high-quality%20video%20content.%20Nonetheless%2C%0Athe%20potential%20of%20transformer-based%20diffusion%20models%20for%20effectively%20generating%0Avideos%20with%20controllable%20motion%20remains%20an%20area%20of%20limited%20exploration.%20This%0Apaper%20introduces%20Tora%2C%20the%20first%20trajectory-oriented%20DiT%20framework%20that%0Aintegrates%20textual%2C%20visual%2C%20and%20trajectory%20conditions%20concurrently%20for%20video%0Ageneration.%20Specifically%2C%20Tora%20consists%20of%20a%20Trajectory%20Extractor~%28TE%29%2C%20a%0ASpatial-Temporal%20DiT%2C%20and%20a%20Motion-guidance%20Fuser~%28MGF%29.%20The%20TE%20encodes%0Aarbitrary%20trajectories%20into%20hierarchical%20spacetime%20motion%20patches%20with%20a%203D%0Avideo%20compression%20network.%20The%20MGF%20integrates%20the%20motion%20patches%20into%20the%20DiT%0Ablocks%20to%20generate%20consistent%20videos%20following%20trajectories.%20Our%20design%20aligns%0Aseamlessly%20with%20DiT%27s%20scalability%2C%20allowing%20precise%20control%20of%20video%20content%27s%0Adynamics%20with%20diverse%20durations%2C%20aspect%20ratios%2C%20and%20resolutions.%20Extensive%0Aexperiments%20demonstrate%20Tora%27s%20excellence%20in%20achieving%20high%20motion%20fidelity%2C%0Awhile%20also%20meticulously%20simulating%20the%20movement%20of%20the%20physical%20world.%20Page%20can%0Abe%20found%20at%20https%3A//ali-videoai.github.io/tora_video.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21705v1&entry.124074799=Read"},
{"title": "Expanding the Medical Decathlon dataset: segmentation of colon and\n  colorectal cancer from computed tomography images", "author": "I. M. Chernenkiy and Y. A. Drach and S. R. Mustakimova and V. V. Kazantseva and N. A. Ushakov and S. K. Efetov and M. V. Feldsherov", "abstract": "  Colorectal cancer is the third-most common cancer in the Western Hemisphere.\nThe segmentation of colorectal and colorectal cancer by computed tomography is\nan urgent problem in medicine. Indeed, a system capable of solving this problem\nwill enable the detection of colorectal cancer at early stages of the disease,\nfacilitate the search for pathology by the radiologist, and significantly\naccelerate the process of diagnosing the disease. However, scientific\npublications on medical image processing mostly use closed, non-public data.\nThis paper presents an extension of the Medical Decathlon dataset with\ncolorectal markups in order to improve the quality of segmentation algorithms.\nAn experienced radiologist validated the data, categorized it into subsets by\nquality, and published it in the public domain. Based on the obtained results,\nwe trained neural network models of the UNet architecture with 5-part\ncross-validation and achieved a Dice metric quality of $0.6988 \\pm 0.3$. The\npublished markups will improve the quality of colorectal cancer detection and\nsimplify the radiologist's job for study description.\n", "link": "http://arxiv.org/abs/2407.21516v1", "date": "2024-07-31", "relevancy": 1.9006, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.523}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4656}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Expanding%20the%20Medical%20Decathlon%20dataset%3A%20segmentation%20of%20colon%20and%0A%20%20colorectal%20cancer%20from%20computed%20tomography%20images&body=Title%3A%20Expanding%20the%20Medical%20Decathlon%20dataset%3A%20segmentation%20of%20colon%20and%0A%20%20colorectal%20cancer%20from%20computed%20tomography%20images%0AAuthor%3A%20I.%20M.%20Chernenkiy%20and%20Y.%20A.%20Drach%20and%20S.%20R.%20Mustakimova%20and%20V.%20V.%20Kazantseva%20and%20N.%20A.%20Ushakov%20and%20S.%20K.%20Efetov%20and%20M.%20V.%20Feldsherov%0AAbstract%3A%20%20%20Colorectal%20cancer%20is%20the%20third-most%20common%20cancer%20in%20the%20Western%20Hemisphere.%0AThe%20segmentation%20of%20colorectal%20and%20colorectal%20cancer%20by%20computed%20tomography%20is%0Aan%20urgent%20problem%20in%20medicine.%20Indeed%2C%20a%20system%20capable%20of%20solving%20this%20problem%0Awill%20enable%20the%20detection%20of%20colorectal%20cancer%20at%20early%20stages%20of%20the%20disease%2C%0Afacilitate%20the%20search%20for%20pathology%20by%20the%20radiologist%2C%20and%20significantly%0Aaccelerate%20the%20process%20of%20diagnosing%20the%20disease.%20However%2C%20scientific%0Apublications%20on%20medical%20image%20processing%20mostly%20use%20closed%2C%20non-public%20data.%0AThis%20paper%20presents%20an%20extension%20of%20the%20Medical%20Decathlon%20dataset%20with%0Acolorectal%20markups%20in%20order%20to%20improve%20the%20quality%20of%20segmentation%20algorithms.%0AAn%20experienced%20radiologist%20validated%20the%20data%2C%20categorized%20it%20into%20subsets%20by%0Aquality%2C%20and%20published%20it%20in%20the%20public%20domain.%20Based%20on%20the%20obtained%20results%2C%0Awe%20trained%20neural%20network%20models%20of%20the%20UNet%20architecture%20with%205-part%0Across-validation%20and%20achieved%20a%20Dice%20metric%20quality%20of%20%240.6988%20%5Cpm%200.3%24.%20The%0Apublished%20markups%20will%20improve%20the%20quality%20of%20colorectal%20cancer%20detection%20and%0Asimplify%20the%20radiologist%27s%20job%20for%20study%20description.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21516v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExpanding%2520the%2520Medical%2520Decathlon%2520dataset%253A%2520segmentation%2520of%2520colon%2520and%250A%2520%2520colorectal%2520cancer%2520from%2520computed%2520tomography%2520images%26entry.906535625%3DI.%2520M.%2520Chernenkiy%2520and%2520Y.%2520A.%2520Drach%2520and%2520S.%2520R.%2520Mustakimova%2520and%2520V.%2520V.%2520Kazantseva%2520and%2520N.%2520A.%2520Ushakov%2520and%2520S.%2520K.%2520Efetov%2520and%2520M.%2520V.%2520Feldsherov%26entry.1292438233%3D%2520%2520Colorectal%2520cancer%2520is%2520the%2520third-most%2520common%2520cancer%2520in%2520the%2520Western%2520Hemisphere.%250AThe%2520segmentation%2520of%2520colorectal%2520and%2520colorectal%2520cancer%2520by%2520computed%2520tomography%2520is%250Aan%2520urgent%2520problem%2520in%2520medicine.%2520Indeed%252C%2520a%2520system%2520capable%2520of%2520solving%2520this%2520problem%250Awill%2520enable%2520the%2520detection%2520of%2520colorectal%2520cancer%2520at%2520early%2520stages%2520of%2520the%2520disease%252C%250Afacilitate%2520the%2520search%2520for%2520pathology%2520by%2520the%2520radiologist%252C%2520and%2520significantly%250Aaccelerate%2520the%2520process%2520of%2520diagnosing%2520the%2520disease.%2520However%252C%2520scientific%250Apublications%2520on%2520medical%2520image%2520processing%2520mostly%2520use%2520closed%252C%2520non-public%2520data.%250AThis%2520paper%2520presents%2520an%2520extension%2520of%2520the%2520Medical%2520Decathlon%2520dataset%2520with%250Acolorectal%2520markups%2520in%2520order%2520to%2520improve%2520the%2520quality%2520of%2520segmentation%2520algorithms.%250AAn%2520experienced%2520radiologist%2520validated%2520the%2520data%252C%2520categorized%2520it%2520into%2520subsets%2520by%250Aquality%252C%2520and%2520published%2520it%2520in%2520the%2520public%2520domain.%2520Based%2520on%2520the%2520obtained%2520results%252C%250Awe%2520trained%2520neural%2520network%2520models%2520of%2520the%2520UNet%2520architecture%2520with%25205-part%250Across-validation%2520and%2520achieved%2520a%2520Dice%2520metric%2520quality%2520of%2520%25240.6988%2520%255Cpm%25200.3%2524.%2520The%250Apublished%2520markups%2520will%2520improve%2520the%2520quality%2520of%2520colorectal%2520cancer%2520detection%2520and%250Asimplify%2520the%2520radiologist%2527s%2520job%2520for%2520study%2520description.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21516v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Expanding%20the%20Medical%20Decathlon%20dataset%3A%20segmentation%20of%20colon%20and%0A%20%20colorectal%20cancer%20from%20computed%20tomography%20images&entry.906535625=I.%20M.%20Chernenkiy%20and%20Y.%20A.%20Drach%20and%20S.%20R.%20Mustakimova%20and%20V.%20V.%20Kazantseva%20and%20N.%20A.%20Ushakov%20and%20S.%20K.%20Efetov%20and%20M.%20V.%20Feldsherov&entry.1292438233=%20%20Colorectal%20cancer%20is%20the%20third-most%20common%20cancer%20in%20the%20Western%20Hemisphere.%0AThe%20segmentation%20of%20colorectal%20and%20colorectal%20cancer%20by%20computed%20tomography%20is%0Aan%20urgent%20problem%20in%20medicine.%20Indeed%2C%20a%20system%20capable%20of%20solving%20this%20problem%0Awill%20enable%20the%20detection%20of%20colorectal%20cancer%20at%20early%20stages%20of%20the%20disease%2C%0Afacilitate%20the%20search%20for%20pathology%20by%20the%20radiologist%2C%20and%20significantly%0Aaccelerate%20the%20process%20of%20diagnosing%20the%20disease.%20However%2C%20scientific%0Apublications%20on%20medical%20image%20processing%20mostly%20use%20closed%2C%20non-public%20data.%0AThis%20paper%20presents%20an%20extension%20of%20the%20Medical%20Decathlon%20dataset%20with%0Acolorectal%20markups%20in%20order%20to%20improve%20the%20quality%20of%20segmentation%20algorithms.%0AAn%20experienced%20radiologist%20validated%20the%20data%2C%20categorized%20it%20into%20subsets%20by%0Aquality%2C%20and%20published%20it%20in%20the%20public%20domain.%20Based%20on%20the%20obtained%20results%2C%0Awe%20trained%20neural%20network%20models%20of%20the%20UNet%20architecture%20with%205-part%0Across-validation%20and%20achieved%20a%20Dice%20metric%20quality%20of%20%240.6988%20%5Cpm%200.3%24.%20The%0Apublished%20markups%20will%20improve%20the%20quality%20of%20colorectal%20cancer%20detection%20and%0Asimplify%20the%20radiologist%27s%20job%20for%20study%20description.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21516v1&entry.124074799=Read"},
{"title": "MaskUno: Switch-Split Block For Enhancing Instance Segmentation", "author": "Jawad Haidar and Marc Mouawad and Imad Elhajj and Daniel Asmar", "abstract": "  Instance segmentation is an advanced form of image segmentation which, beyond\ntraditional segmentation, requires identifying individual instances of\nrepeating objects in a scene. Mask R-CNN is the most common architecture for\ninstance segmentation, and improvements to this architecture include steps such\nas benefiting from bounding box refinements, adding semantics, or backbone\nenhancements. In all the proposed variations to date, the problem of competing\nkernels (each class aims to maximize its own accuracy) persists when models try\nto synchronously learn numerous classes. In this paper, we propose mitigating\nthis problem by replacing mask prediction with a Switch-Split block that\nprocesses refined ROIs, classifies them, and assigns them to specialized mask\npredictors. We name the method MaskUno and test it on various models from the\nliterature, which are then trained on multiple classes using the benchmark COCO\ndataset. An increase in the mean Average Precision (mAP) of 2.03% was observed\nfor the high-performing DetectoRS when trained on 80 classes. MaskUno proved to\nenhance the mAP of instance segmentation models regardless of the number and\ntyp\n", "link": "http://arxiv.org/abs/2407.21498v1", "date": "2024-07-31", "relevancy": 1.8978, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4776}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.475}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4726}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MaskUno%3A%20Switch-Split%20Block%20For%20Enhancing%20Instance%20Segmentation&body=Title%3A%20MaskUno%3A%20Switch-Split%20Block%20For%20Enhancing%20Instance%20Segmentation%0AAuthor%3A%20Jawad%20Haidar%20and%20Marc%20Mouawad%20and%20Imad%20Elhajj%20and%20Daniel%20Asmar%0AAbstract%3A%20%20%20Instance%20segmentation%20is%20an%20advanced%20form%20of%20image%20segmentation%20which%2C%20beyond%0Atraditional%20segmentation%2C%20requires%20identifying%20individual%20instances%20of%0Arepeating%20objects%20in%20a%20scene.%20Mask%20R-CNN%20is%20the%20most%20common%20architecture%20for%0Ainstance%20segmentation%2C%20and%20improvements%20to%20this%20architecture%20include%20steps%20such%0Aas%20benefiting%20from%20bounding%20box%20refinements%2C%20adding%20semantics%2C%20or%20backbone%0Aenhancements.%20In%20all%20the%20proposed%20variations%20to%20date%2C%20the%20problem%20of%20competing%0Akernels%20%28each%20class%20aims%20to%20maximize%20its%20own%20accuracy%29%20persists%20when%20models%20try%0Ato%20synchronously%20learn%20numerous%20classes.%20In%20this%20paper%2C%20we%20propose%20mitigating%0Athis%20problem%20by%20replacing%20mask%20prediction%20with%20a%20Switch-Split%20block%20that%0Aprocesses%20refined%20ROIs%2C%20classifies%20them%2C%20and%20assigns%20them%20to%20specialized%20mask%0Apredictors.%20We%20name%20the%20method%20MaskUno%20and%20test%20it%20on%20various%20models%20from%20the%0Aliterature%2C%20which%20are%20then%20trained%20on%20multiple%20classes%20using%20the%20benchmark%20COCO%0Adataset.%20An%20increase%20in%20the%20mean%20Average%20Precision%20%28mAP%29%20of%202.03%25%20was%20observed%0Afor%20the%20high-performing%20DetectoRS%20when%20trained%20on%2080%20classes.%20MaskUno%20proved%20to%0Aenhance%20the%20mAP%20of%20instance%20segmentation%20models%20regardless%20of%20the%20number%20and%0Atyp%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21498v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaskUno%253A%2520Switch-Split%2520Block%2520For%2520Enhancing%2520Instance%2520Segmentation%26entry.906535625%3DJawad%2520Haidar%2520and%2520Marc%2520Mouawad%2520and%2520Imad%2520Elhajj%2520and%2520Daniel%2520Asmar%26entry.1292438233%3D%2520%2520Instance%2520segmentation%2520is%2520an%2520advanced%2520form%2520of%2520image%2520segmentation%2520which%252C%2520beyond%250Atraditional%2520segmentation%252C%2520requires%2520identifying%2520individual%2520instances%2520of%250Arepeating%2520objects%2520in%2520a%2520scene.%2520Mask%2520R-CNN%2520is%2520the%2520most%2520common%2520architecture%2520for%250Ainstance%2520segmentation%252C%2520and%2520improvements%2520to%2520this%2520architecture%2520include%2520steps%2520such%250Aas%2520benefiting%2520from%2520bounding%2520box%2520refinements%252C%2520adding%2520semantics%252C%2520or%2520backbone%250Aenhancements.%2520In%2520all%2520the%2520proposed%2520variations%2520to%2520date%252C%2520the%2520problem%2520of%2520competing%250Akernels%2520%2528each%2520class%2520aims%2520to%2520maximize%2520its%2520own%2520accuracy%2529%2520persists%2520when%2520models%2520try%250Ato%2520synchronously%2520learn%2520numerous%2520classes.%2520In%2520this%2520paper%252C%2520we%2520propose%2520mitigating%250Athis%2520problem%2520by%2520replacing%2520mask%2520prediction%2520with%2520a%2520Switch-Split%2520block%2520that%250Aprocesses%2520refined%2520ROIs%252C%2520classifies%2520them%252C%2520and%2520assigns%2520them%2520to%2520specialized%2520mask%250Apredictors.%2520We%2520name%2520the%2520method%2520MaskUno%2520and%2520test%2520it%2520on%2520various%2520models%2520from%2520the%250Aliterature%252C%2520which%2520are%2520then%2520trained%2520on%2520multiple%2520classes%2520using%2520the%2520benchmark%2520COCO%250Adataset.%2520An%2520increase%2520in%2520the%2520mean%2520Average%2520Precision%2520%2528mAP%2529%2520of%25202.03%2525%2520was%2520observed%250Afor%2520the%2520high-performing%2520DetectoRS%2520when%2520trained%2520on%252080%2520classes.%2520MaskUno%2520proved%2520to%250Aenhance%2520the%2520mAP%2520of%2520instance%2520segmentation%2520models%2520regardless%2520of%2520the%2520number%2520and%250Atyp%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21498v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MaskUno%3A%20Switch-Split%20Block%20For%20Enhancing%20Instance%20Segmentation&entry.906535625=Jawad%20Haidar%20and%20Marc%20Mouawad%20and%20Imad%20Elhajj%20and%20Daniel%20Asmar&entry.1292438233=%20%20Instance%20segmentation%20is%20an%20advanced%20form%20of%20image%20segmentation%20which%2C%20beyond%0Atraditional%20segmentation%2C%20requires%20identifying%20individual%20instances%20of%0Arepeating%20objects%20in%20a%20scene.%20Mask%20R-CNN%20is%20the%20most%20common%20architecture%20for%0Ainstance%20segmentation%2C%20and%20improvements%20to%20this%20architecture%20include%20steps%20such%0Aas%20benefiting%20from%20bounding%20box%20refinements%2C%20adding%20semantics%2C%20or%20backbone%0Aenhancements.%20In%20all%20the%20proposed%20variations%20to%20date%2C%20the%20problem%20of%20competing%0Akernels%20%28each%20class%20aims%20to%20maximize%20its%20own%20accuracy%29%20persists%20when%20models%20try%0Ato%20synchronously%20learn%20numerous%20classes.%20In%20this%20paper%2C%20we%20propose%20mitigating%0Athis%20problem%20by%20replacing%20mask%20prediction%20with%20a%20Switch-Split%20block%20that%0Aprocesses%20refined%20ROIs%2C%20classifies%20them%2C%20and%20assigns%20them%20to%20specialized%20mask%0Apredictors.%20We%20name%20the%20method%20MaskUno%20and%20test%20it%20on%20various%20models%20from%20the%0Aliterature%2C%20which%20are%20then%20trained%20on%20multiple%20classes%20using%20the%20benchmark%20COCO%0Adataset.%20An%20increase%20in%20the%20mean%20Average%20Precision%20%28mAP%29%20of%202.03%25%20was%20observed%0Afor%20the%20high-performing%20DetectoRS%20when%20trained%20on%2080%20classes.%20MaskUno%20proved%20to%0Aenhance%20the%20mAP%20of%20instance%20segmentation%20models%20regardless%20of%20the%20number%20and%0Atyp%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21498v1&entry.124074799=Read"},
{"title": "MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM\n  Fine-Tuning", "author": "Yupeng Chen and Senmiao Wang and Zhihang Lin and Zeyu Qin and Yushun Zhang and Tian Ding and Ruoyu Sun", "abstract": "  Recently, large language models (LLMs) have demonstrated remarkable\ncapabilities in a wide range of tasks. Typically, an LLM is pre-trained on\nlarge corpora and subsequently fine-tuned on task-specific datasets. However,\nduring fine-tuning, LLMs may forget the knowledge acquired in the pre-training\nstage, leading to a decline in general capabilities. To address this issue, we\npropose a new fine-tuning algorithm termed Momentum-Filtered Optimizer (MoFO).\nThe key idea of MoFO is to iteratively select and update the model parameters\nwith the largest momentum magnitudes. Compared to full-parameter training, MoFO\nachieves similar fine-tuning performance while keeping parameters closer to the\npre-trained model, thereby mitigating knowledge forgetting. Unlike most\nexisting methods for forgetting mitigation, MoFO combines the following two\nadvantages. First, MoFO does not require access to pre-training data. This\nmakes MoFO particularly suitable for fine-tuning scenarios where pre-training\ndata is unavailable, such as fine-tuning checkpoint-only open-source LLMs.\nSecond, MoFO does not alter the original loss function. This could avoid\nimpairing the model performance on the fine-tuning tasks. We validate MoFO\nthrough rigorous convergence analysis and extensive experiments, demonstrating\nits superiority over existing methods in mitigating forgetting and enhancing\nfine-tuning performance.\n", "link": "http://arxiv.org/abs/2407.20999v2", "date": "2024-07-31", "relevancy": 1.8977, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4779}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4746}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4709}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoFO%3A%20Momentum-Filtered%20Optimizer%20for%20Mitigating%20Forgetting%20in%20LLM%0A%20%20Fine-Tuning&body=Title%3A%20MoFO%3A%20Momentum-Filtered%20Optimizer%20for%20Mitigating%20Forgetting%20in%20LLM%0A%20%20Fine-Tuning%0AAuthor%3A%20Yupeng%20Chen%20and%20Senmiao%20Wang%20and%20Zhihang%20Lin%20and%20Zeyu%20Qin%20and%20Yushun%20Zhang%20and%20Tian%20Ding%20and%20Ruoyu%20Sun%0AAbstract%3A%20%20%20Recently%2C%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%0Acapabilities%20in%20a%20wide%20range%20of%20tasks.%20Typically%2C%20an%20LLM%20is%20pre-trained%20on%0Alarge%20corpora%20and%20subsequently%20fine-tuned%20on%20task-specific%20datasets.%20However%2C%0Aduring%20fine-tuning%2C%20LLMs%20may%20forget%20the%20knowledge%20acquired%20in%20the%20pre-training%0Astage%2C%20leading%20to%20a%20decline%20in%20general%20capabilities.%20To%20address%20this%20issue%2C%20we%0Apropose%20a%20new%20fine-tuning%20algorithm%20termed%20Momentum-Filtered%20Optimizer%20%28MoFO%29.%0AThe%20key%20idea%20of%20MoFO%20is%20to%20iteratively%20select%20and%20update%20the%20model%20parameters%0Awith%20the%20largest%20momentum%20magnitudes.%20Compared%20to%20full-parameter%20training%2C%20MoFO%0Aachieves%20similar%20fine-tuning%20performance%20while%20keeping%20parameters%20closer%20to%20the%0Apre-trained%20model%2C%20thereby%20mitigating%20knowledge%20forgetting.%20Unlike%20most%0Aexisting%20methods%20for%20forgetting%20mitigation%2C%20MoFO%20combines%20the%20following%20two%0Aadvantages.%20First%2C%20MoFO%20does%20not%20require%20access%20to%20pre-training%20data.%20This%0Amakes%20MoFO%20particularly%20suitable%20for%20fine-tuning%20scenarios%20where%20pre-training%0Adata%20is%20unavailable%2C%20such%20as%20fine-tuning%20checkpoint-only%20open-source%20LLMs.%0ASecond%2C%20MoFO%20does%20not%20alter%20the%20original%20loss%20function.%20This%20could%20avoid%0Aimpairing%20the%20model%20performance%20on%20the%20fine-tuning%20tasks.%20We%20validate%20MoFO%0Athrough%20rigorous%20convergence%20analysis%20and%20extensive%20experiments%2C%20demonstrating%0Aits%20superiority%20over%20existing%20methods%20in%20mitigating%20forgetting%20and%20enhancing%0Afine-tuning%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20999v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoFO%253A%2520Momentum-Filtered%2520Optimizer%2520for%2520Mitigating%2520Forgetting%2520in%2520LLM%250A%2520%2520Fine-Tuning%26entry.906535625%3DYupeng%2520Chen%2520and%2520Senmiao%2520Wang%2520and%2520Zhihang%2520Lin%2520and%2520Zeyu%2520Qin%2520and%2520Yushun%2520Zhang%2520and%2520Tian%2520Ding%2520and%2520Ruoyu%2520Sun%26entry.1292438233%3D%2520%2520Recently%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%250Acapabilities%2520in%2520a%2520wide%2520range%2520of%2520tasks.%2520Typically%252C%2520an%2520LLM%2520is%2520pre-trained%2520on%250Alarge%2520corpora%2520and%2520subsequently%2520fine-tuned%2520on%2520task-specific%2520datasets.%2520However%252C%250Aduring%2520fine-tuning%252C%2520LLMs%2520may%2520forget%2520the%2520knowledge%2520acquired%2520in%2520the%2520pre-training%250Astage%252C%2520leading%2520to%2520a%2520decline%2520in%2520general%2520capabilities.%2520To%2520address%2520this%2520issue%252C%2520we%250Apropose%2520a%2520new%2520fine-tuning%2520algorithm%2520termed%2520Momentum-Filtered%2520Optimizer%2520%2528MoFO%2529.%250AThe%2520key%2520idea%2520of%2520MoFO%2520is%2520to%2520iteratively%2520select%2520and%2520update%2520the%2520model%2520parameters%250Awith%2520the%2520largest%2520momentum%2520magnitudes.%2520Compared%2520to%2520full-parameter%2520training%252C%2520MoFO%250Aachieves%2520similar%2520fine-tuning%2520performance%2520while%2520keeping%2520parameters%2520closer%2520to%2520the%250Apre-trained%2520model%252C%2520thereby%2520mitigating%2520knowledge%2520forgetting.%2520Unlike%2520most%250Aexisting%2520methods%2520for%2520forgetting%2520mitigation%252C%2520MoFO%2520combines%2520the%2520following%2520two%250Aadvantages.%2520First%252C%2520MoFO%2520does%2520not%2520require%2520access%2520to%2520pre-training%2520data.%2520This%250Amakes%2520MoFO%2520particularly%2520suitable%2520for%2520fine-tuning%2520scenarios%2520where%2520pre-training%250Adata%2520is%2520unavailable%252C%2520such%2520as%2520fine-tuning%2520checkpoint-only%2520open-source%2520LLMs.%250ASecond%252C%2520MoFO%2520does%2520not%2520alter%2520the%2520original%2520loss%2520function.%2520This%2520could%2520avoid%250Aimpairing%2520the%2520model%2520performance%2520on%2520the%2520fine-tuning%2520tasks.%2520We%2520validate%2520MoFO%250Athrough%2520rigorous%2520convergence%2520analysis%2520and%2520extensive%2520experiments%252C%2520demonstrating%250Aits%2520superiority%2520over%2520existing%2520methods%2520in%2520mitigating%2520forgetting%2520and%2520enhancing%250Afine-tuning%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20999v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoFO%3A%20Momentum-Filtered%20Optimizer%20for%20Mitigating%20Forgetting%20in%20LLM%0A%20%20Fine-Tuning&entry.906535625=Yupeng%20Chen%20and%20Senmiao%20Wang%20and%20Zhihang%20Lin%20and%20Zeyu%20Qin%20and%20Yushun%20Zhang%20and%20Tian%20Ding%20and%20Ruoyu%20Sun&entry.1292438233=%20%20Recently%2C%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%0Acapabilities%20in%20a%20wide%20range%20of%20tasks.%20Typically%2C%20an%20LLM%20is%20pre-trained%20on%0Alarge%20corpora%20and%20subsequently%20fine-tuned%20on%20task-specific%20datasets.%20However%2C%0Aduring%20fine-tuning%2C%20LLMs%20may%20forget%20the%20knowledge%20acquired%20in%20the%20pre-training%0Astage%2C%20leading%20to%20a%20decline%20in%20general%20capabilities.%20To%20address%20this%20issue%2C%20we%0Apropose%20a%20new%20fine-tuning%20algorithm%20termed%20Momentum-Filtered%20Optimizer%20%28MoFO%29.%0AThe%20key%20idea%20of%20MoFO%20is%20to%20iteratively%20select%20and%20update%20the%20model%20parameters%0Awith%20the%20largest%20momentum%20magnitudes.%20Compared%20to%20full-parameter%20training%2C%20MoFO%0Aachieves%20similar%20fine-tuning%20performance%20while%20keeping%20parameters%20closer%20to%20the%0Apre-trained%20model%2C%20thereby%20mitigating%20knowledge%20forgetting.%20Unlike%20most%0Aexisting%20methods%20for%20forgetting%20mitigation%2C%20MoFO%20combines%20the%20following%20two%0Aadvantages.%20First%2C%20MoFO%20does%20not%20require%20access%20to%20pre-training%20data.%20This%0Amakes%20MoFO%20particularly%20suitable%20for%20fine-tuning%20scenarios%20where%20pre-training%0Adata%20is%20unavailable%2C%20such%20as%20fine-tuning%20checkpoint-only%20open-source%20LLMs.%0ASecond%2C%20MoFO%20does%20not%20alter%20the%20original%20loss%20function.%20This%20could%20avoid%0Aimpairing%20the%20model%20performance%20on%20the%20fine-tuning%20tasks.%20We%20validate%20MoFO%0Athrough%20rigorous%20convergence%20analysis%20and%20extensive%20experiments%2C%20demonstrating%0Aits%20superiority%20over%20existing%20methods%20in%20mitigating%20forgetting%20and%20enhancing%0Afine-tuning%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20999v2&entry.124074799=Read"},
{"title": "Optimal Decision Tree and Adaptive Submodular Ranking with Noisy\n  Outcomes", "author": "Su Jia and Fatemeh Navidi and Viswanath Nagarajan and R. Ravi", "abstract": "  In pool-based active learning, the learner is given an unlabeled data set and\naims to efficiently learn the unknown hypothesis by querying the labels of the\ndata points. This can be formulated as the classical Optimal Decision Tree\n(ODT) problem: Given a set of tests, a set of hypotheses, and an outcome for\neach pair of test and hypothesis, our objective is to find a low-cost testing\nprocedure (i.e., decision tree) that identifies the true hypothesis. This\noptimization problem has been extensively studied under the assumption that\neach test generates a deterministic outcome. However, in numerous applications,\nfor example, clinical trials, the outcomes may be uncertain, which renders the\nideas from the deterministic setting invalid. In this work, we study a\nfundamental variant of the ODT problem in which some test outcomes are noisy,\neven in the more general case where the noise is persistent, i.e., repeating a\ntest gives the same noisy output. Our approximation algorithms provide\nguarantees that are nearly best possible and hold for the general case of a\nlarge number of noisy outcomes per test or per hypothesis where the performance\ndegrades continuously with this number. We numerically evaluated our algorithms\nfor identifying toxic chemicals and learning linear classifiers, and observed\nthat our algorithms have costs very close to the information-theoretic minimum.\n", "link": "http://arxiv.org/abs/2312.15357v2", "date": "2024-07-31", "relevancy": 1.8908, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4732}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4729}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Decision%20Tree%20and%20Adaptive%20Submodular%20Ranking%20with%20Noisy%0A%20%20Outcomes&body=Title%3A%20Optimal%20Decision%20Tree%20and%20Adaptive%20Submodular%20Ranking%20with%20Noisy%0A%20%20Outcomes%0AAuthor%3A%20Su%20Jia%20and%20Fatemeh%20Navidi%20and%20Viswanath%20Nagarajan%20and%20R.%20Ravi%0AAbstract%3A%20%20%20In%20pool-based%20active%20learning%2C%20the%20learner%20is%20given%20an%20unlabeled%20data%20set%20and%0Aaims%20to%20efficiently%20learn%20the%20unknown%20hypothesis%20by%20querying%20the%20labels%20of%20the%0Adata%20points.%20This%20can%20be%20formulated%20as%20the%20classical%20Optimal%20Decision%20Tree%0A%28ODT%29%20problem%3A%20Given%20a%20set%20of%20tests%2C%20a%20set%20of%20hypotheses%2C%20and%20an%20outcome%20for%0Aeach%20pair%20of%20test%20and%20hypothesis%2C%20our%20objective%20is%20to%20find%20a%20low-cost%20testing%0Aprocedure%20%28i.e.%2C%20decision%20tree%29%20that%20identifies%20the%20true%20hypothesis.%20This%0Aoptimization%20problem%20has%20been%20extensively%20studied%20under%20the%20assumption%20that%0Aeach%20test%20generates%20a%20deterministic%20outcome.%20However%2C%20in%20numerous%20applications%2C%0Afor%20example%2C%20clinical%20trials%2C%20the%20outcomes%20may%20be%20uncertain%2C%20which%20renders%20the%0Aideas%20from%20the%20deterministic%20setting%20invalid.%20In%20this%20work%2C%20we%20study%20a%0Afundamental%20variant%20of%20the%20ODT%20problem%20in%20which%20some%20test%20outcomes%20are%20noisy%2C%0Aeven%20in%20the%20more%20general%20case%20where%20the%20noise%20is%20persistent%2C%20i.e.%2C%20repeating%20a%0Atest%20gives%20the%20same%20noisy%20output.%20Our%20approximation%20algorithms%20provide%0Aguarantees%20that%20are%20nearly%20best%20possible%20and%20hold%20for%20the%20general%20case%20of%20a%0Alarge%20number%20of%20noisy%20outcomes%20per%20test%20or%20per%20hypothesis%20where%20the%20performance%0Adegrades%20continuously%20with%20this%20number.%20We%20numerically%20evaluated%20our%20algorithms%0Afor%20identifying%20toxic%20chemicals%20and%20learning%20linear%20classifiers%2C%20and%20observed%0Athat%20our%20algorithms%20have%20costs%20very%20close%20to%20the%20information-theoretic%20minimum.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.15357v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Decision%2520Tree%2520and%2520Adaptive%2520Submodular%2520Ranking%2520with%2520Noisy%250A%2520%2520Outcomes%26entry.906535625%3DSu%2520Jia%2520and%2520Fatemeh%2520Navidi%2520and%2520Viswanath%2520Nagarajan%2520and%2520R.%2520Ravi%26entry.1292438233%3D%2520%2520In%2520pool-based%2520active%2520learning%252C%2520the%2520learner%2520is%2520given%2520an%2520unlabeled%2520data%2520set%2520and%250Aaims%2520to%2520efficiently%2520learn%2520the%2520unknown%2520hypothesis%2520by%2520querying%2520the%2520labels%2520of%2520the%250Adata%2520points.%2520This%2520can%2520be%2520formulated%2520as%2520the%2520classical%2520Optimal%2520Decision%2520Tree%250A%2528ODT%2529%2520problem%253A%2520Given%2520a%2520set%2520of%2520tests%252C%2520a%2520set%2520of%2520hypotheses%252C%2520and%2520an%2520outcome%2520for%250Aeach%2520pair%2520of%2520test%2520and%2520hypothesis%252C%2520our%2520objective%2520is%2520to%2520find%2520a%2520low-cost%2520testing%250Aprocedure%2520%2528i.e.%252C%2520decision%2520tree%2529%2520that%2520identifies%2520the%2520true%2520hypothesis.%2520This%250Aoptimization%2520problem%2520has%2520been%2520extensively%2520studied%2520under%2520the%2520assumption%2520that%250Aeach%2520test%2520generates%2520a%2520deterministic%2520outcome.%2520However%252C%2520in%2520numerous%2520applications%252C%250Afor%2520example%252C%2520clinical%2520trials%252C%2520the%2520outcomes%2520may%2520be%2520uncertain%252C%2520which%2520renders%2520the%250Aideas%2520from%2520the%2520deterministic%2520setting%2520invalid.%2520In%2520this%2520work%252C%2520we%2520study%2520a%250Afundamental%2520variant%2520of%2520the%2520ODT%2520problem%2520in%2520which%2520some%2520test%2520outcomes%2520are%2520noisy%252C%250Aeven%2520in%2520the%2520more%2520general%2520case%2520where%2520the%2520noise%2520is%2520persistent%252C%2520i.e.%252C%2520repeating%2520a%250Atest%2520gives%2520the%2520same%2520noisy%2520output.%2520Our%2520approximation%2520algorithms%2520provide%250Aguarantees%2520that%2520are%2520nearly%2520best%2520possible%2520and%2520hold%2520for%2520the%2520general%2520case%2520of%2520a%250Alarge%2520number%2520of%2520noisy%2520outcomes%2520per%2520test%2520or%2520per%2520hypothesis%2520where%2520the%2520performance%250Adegrades%2520continuously%2520with%2520this%2520number.%2520We%2520numerically%2520evaluated%2520our%2520algorithms%250Afor%2520identifying%2520toxic%2520chemicals%2520and%2520learning%2520linear%2520classifiers%252C%2520and%2520observed%250Athat%2520our%2520algorithms%2520have%2520costs%2520very%2520close%2520to%2520the%2520information-theoretic%2520minimum.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.15357v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Decision%20Tree%20and%20Adaptive%20Submodular%20Ranking%20with%20Noisy%0A%20%20Outcomes&entry.906535625=Su%20Jia%20and%20Fatemeh%20Navidi%20and%20Viswanath%20Nagarajan%20and%20R.%20Ravi&entry.1292438233=%20%20In%20pool-based%20active%20learning%2C%20the%20learner%20is%20given%20an%20unlabeled%20data%20set%20and%0Aaims%20to%20efficiently%20learn%20the%20unknown%20hypothesis%20by%20querying%20the%20labels%20of%20the%0Adata%20points.%20This%20can%20be%20formulated%20as%20the%20classical%20Optimal%20Decision%20Tree%0A%28ODT%29%20problem%3A%20Given%20a%20set%20of%20tests%2C%20a%20set%20of%20hypotheses%2C%20and%20an%20outcome%20for%0Aeach%20pair%20of%20test%20and%20hypothesis%2C%20our%20objective%20is%20to%20find%20a%20low-cost%20testing%0Aprocedure%20%28i.e.%2C%20decision%20tree%29%20that%20identifies%20the%20true%20hypothesis.%20This%0Aoptimization%20problem%20has%20been%20extensively%20studied%20under%20the%20assumption%20that%0Aeach%20test%20generates%20a%20deterministic%20outcome.%20However%2C%20in%20numerous%20applications%2C%0Afor%20example%2C%20clinical%20trials%2C%20the%20outcomes%20may%20be%20uncertain%2C%20which%20renders%20the%0Aideas%20from%20the%20deterministic%20setting%20invalid.%20In%20this%20work%2C%20we%20study%20a%0Afundamental%20variant%20of%20the%20ODT%20problem%20in%20which%20some%20test%20outcomes%20are%20noisy%2C%0Aeven%20in%20the%20more%20general%20case%20where%20the%20noise%20is%20persistent%2C%20i.e.%2C%20repeating%20a%0Atest%20gives%20the%20same%20noisy%20output.%20Our%20approximation%20algorithms%20provide%0Aguarantees%20that%20are%20nearly%20best%20possible%20and%20hold%20for%20the%20general%20case%20of%20a%0Alarge%20number%20of%20noisy%20outcomes%20per%20test%20or%20per%20hypothesis%20where%20the%20performance%0Adegrades%20continuously%20with%20this%20number.%20We%20numerically%20evaluated%20our%20algorithms%0Afor%20identifying%20toxic%20chemicals%20and%20learning%20linear%20classifiers%2C%20and%20observed%0Athat%20our%20algorithms%20have%20costs%20very%20close%20to%20the%20information-theoretic%20minimum.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.15357v2&entry.124074799=Read"},
{"title": "UMMAN: Unsupervised Multi-graph Merge Adversarial Network for Disease\n  Prediction Based on Intestinal Flora", "author": "Dingkun Liu and Hongjie Zhou and Yilu Qu and Huimei Zhang and Yongdong Xu", "abstract": "  The abundance of intestinal flora is closely related to human diseases, but\ndiseases are not caused by a single gut microbe. Instead, they result from the\ncomplex interplay of numerous microbial entities. This intricate and implicit\nconnection among gut microbes poses a significant challenge for disease\nprediction using abundance information from OTU data. Recently, several methods\nhave shown potential in predicting corresponding diseases. However, these\nmethods fail to learn the inner association among gut microbes from different\nhosts, leading to unsatisfactory performance. In this paper, we present a novel\narchitecture, Unsupervised Multi-graph Merge Adversarial Network (UMMAN). UMMAN\ncan obtain the embeddings of nodes in the Multi-Graph in an unsupervised\nscenario, so that it helps learn the multiplex association. Our method is the\nfirst to combine Graph Neural Network with the task of intestinal flora disease\nprediction. We employ complex relation-types to construct the Original-Graph\nand disrupt the relationships among nodes to generate corresponding\nShuffled-Graph. We introduce the Node Feature Global Integration (NFGI) module\nto represent the global features of the graph. Furthermore, we design a joint\nloss comprising adversarial loss and hybrid attention loss to ensure that the\nreal graph embedding aligns closely with the Original-Graph and diverges from\nthe Shuffled-Graph. Comprehensive experiments on five classical OTU gut\nmicrobiome datasets demonstrate the effectiveness and stability of our method.\n(We will release our code soon.)\n", "link": "http://arxiv.org/abs/2407.21714v1", "date": "2024-07-31", "relevancy": 1.8885, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4817}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.47}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4635}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UMMAN%3A%20Unsupervised%20Multi-graph%20Merge%20Adversarial%20Network%20for%20Disease%0A%20%20Prediction%20Based%20on%20Intestinal%20Flora&body=Title%3A%20UMMAN%3A%20Unsupervised%20Multi-graph%20Merge%20Adversarial%20Network%20for%20Disease%0A%20%20Prediction%20Based%20on%20Intestinal%20Flora%0AAuthor%3A%20Dingkun%20Liu%20and%20Hongjie%20Zhou%20and%20Yilu%20Qu%20and%20Huimei%20Zhang%20and%20Yongdong%20Xu%0AAbstract%3A%20%20%20The%20abundance%20of%20intestinal%20flora%20is%20closely%20related%20to%20human%20diseases%2C%20but%0Adiseases%20are%20not%20caused%20by%20a%20single%20gut%20microbe.%20Instead%2C%20they%20result%20from%20the%0Acomplex%20interplay%20of%20numerous%20microbial%20entities.%20This%20intricate%20and%20implicit%0Aconnection%20among%20gut%20microbes%20poses%20a%20significant%20challenge%20for%20disease%0Aprediction%20using%20abundance%20information%20from%20OTU%20data.%20Recently%2C%20several%20methods%0Ahave%20shown%20potential%20in%20predicting%20corresponding%20diseases.%20However%2C%20these%0Amethods%20fail%20to%20learn%20the%20inner%20association%20among%20gut%20microbes%20from%20different%0Ahosts%2C%20leading%20to%20unsatisfactory%20performance.%20In%20this%20paper%2C%20we%20present%20a%20novel%0Aarchitecture%2C%20Unsupervised%20Multi-graph%20Merge%20Adversarial%20Network%20%28UMMAN%29.%20UMMAN%0Acan%20obtain%20the%20embeddings%20of%20nodes%20in%20the%20Multi-Graph%20in%20an%20unsupervised%0Ascenario%2C%20so%20that%20it%20helps%20learn%20the%20multiplex%20association.%20Our%20method%20is%20the%0Afirst%20to%20combine%20Graph%20Neural%20Network%20with%20the%20task%20of%20intestinal%20flora%20disease%0Aprediction.%20We%20employ%20complex%20relation-types%20to%20construct%20the%20Original-Graph%0Aand%20disrupt%20the%20relationships%20among%20nodes%20to%20generate%20corresponding%0AShuffled-Graph.%20We%20introduce%20the%20Node%20Feature%20Global%20Integration%20%28NFGI%29%20module%0Ato%20represent%20the%20global%20features%20of%20the%20graph.%20Furthermore%2C%20we%20design%20a%20joint%0Aloss%20comprising%20adversarial%20loss%20and%20hybrid%20attention%20loss%20to%20ensure%20that%20the%0Areal%20graph%20embedding%20aligns%20closely%20with%20the%20Original-Graph%20and%20diverges%20from%0Athe%20Shuffled-Graph.%20Comprehensive%20experiments%20on%20five%20classical%20OTU%20gut%0Amicrobiome%20datasets%20demonstrate%20the%20effectiveness%20and%20stability%20of%20our%20method.%0A%28We%20will%20release%20our%20code%20soon.%29%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21714v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUMMAN%253A%2520Unsupervised%2520Multi-graph%2520Merge%2520Adversarial%2520Network%2520for%2520Disease%250A%2520%2520Prediction%2520Based%2520on%2520Intestinal%2520Flora%26entry.906535625%3DDingkun%2520Liu%2520and%2520Hongjie%2520Zhou%2520and%2520Yilu%2520Qu%2520and%2520Huimei%2520Zhang%2520and%2520Yongdong%2520Xu%26entry.1292438233%3D%2520%2520The%2520abundance%2520of%2520intestinal%2520flora%2520is%2520closely%2520related%2520to%2520human%2520diseases%252C%2520but%250Adiseases%2520are%2520not%2520caused%2520by%2520a%2520single%2520gut%2520microbe.%2520Instead%252C%2520they%2520result%2520from%2520the%250Acomplex%2520interplay%2520of%2520numerous%2520microbial%2520entities.%2520This%2520intricate%2520and%2520implicit%250Aconnection%2520among%2520gut%2520microbes%2520poses%2520a%2520significant%2520challenge%2520for%2520disease%250Aprediction%2520using%2520abundance%2520information%2520from%2520OTU%2520data.%2520Recently%252C%2520several%2520methods%250Ahave%2520shown%2520potential%2520in%2520predicting%2520corresponding%2520diseases.%2520However%252C%2520these%250Amethods%2520fail%2520to%2520learn%2520the%2520inner%2520association%2520among%2520gut%2520microbes%2520from%2520different%250Ahosts%252C%2520leading%2520to%2520unsatisfactory%2520performance.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%250Aarchitecture%252C%2520Unsupervised%2520Multi-graph%2520Merge%2520Adversarial%2520Network%2520%2528UMMAN%2529.%2520UMMAN%250Acan%2520obtain%2520the%2520embeddings%2520of%2520nodes%2520in%2520the%2520Multi-Graph%2520in%2520an%2520unsupervised%250Ascenario%252C%2520so%2520that%2520it%2520helps%2520learn%2520the%2520multiplex%2520association.%2520Our%2520method%2520is%2520the%250Afirst%2520to%2520combine%2520Graph%2520Neural%2520Network%2520with%2520the%2520task%2520of%2520intestinal%2520flora%2520disease%250Aprediction.%2520We%2520employ%2520complex%2520relation-types%2520to%2520construct%2520the%2520Original-Graph%250Aand%2520disrupt%2520the%2520relationships%2520among%2520nodes%2520to%2520generate%2520corresponding%250AShuffled-Graph.%2520We%2520introduce%2520the%2520Node%2520Feature%2520Global%2520Integration%2520%2528NFGI%2529%2520module%250Ato%2520represent%2520the%2520global%2520features%2520of%2520the%2520graph.%2520Furthermore%252C%2520we%2520design%2520a%2520joint%250Aloss%2520comprising%2520adversarial%2520loss%2520and%2520hybrid%2520attention%2520loss%2520to%2520ensure%2520that%2520the%250Areal%2520graph%2520embedding%2520aligns%2520closely%2520with%2520the%2520Original-Graph%2520and%2520diverges%2520from%250Athe%2520Shuffled-Graph.%2520Comprehensive%2520experiments%2520on%2520five%2520classical%2520OTU%2520gut%250Amicrobiome%2520datasets%2520demonstrate%2520the%2520effectiveness%2520and%2520stability%2520of%2520our%2520method.%250A%2528We%2520will%2520release%2520our%2520code%2520soon.%2529%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21714v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UMMAN%3A%20Unsupervised%20Multi-graph%20Merge%20Adversarial%20Network%20for%20Disease%0A%20%20Prediction%20Based%20on%20Intestinal%20Flora&entry.906535625=Dingkun%20Liu%20and%20Hongjie%20Zhou%20and%20Yilu%20Qu%20and%20Huimei%20Zhang%20and%20Yongdong%20Xu&entry.1292438233=%20%20The%20abundance%20of%20intestinal%20flora%20is%20closely%20related%20to%20human%20diseases%2C%20but%0Adiseases%20are%20not%20caused%20by%20a%20single%20gut%20microbe.%20Instead%2C%20they%20result%20from%20the%0Acomplex%20interplay%20of%20numerous%20microbial%20entities.%20This%20intricate%20and%20implicit%0Aconnection%20among%20gut%20microbes%20poses%20a%20significant%20challenge%20for%20disease%0Aprediction%20using%20abundance%20information%20from%20OTU%20data.%20Recently%2C%20several%20methods%0Ahave%20shown%20potential%20in%20predicting%20corresponding%20diseases.%20However%2C%20these%0Amethods%20fail%20to%20learn%20the%20inner%20association%20among%20gut%20microbes%20from%20different%0Ahosts%2C%20leading%20to%20unsatisfactory%20performance.%20In%20this%20paper%2C%20we%20present%20a%20novel%0Aarchitecture%2C%20Unsupervised%20Multi-graph%20Merge%20Adversarial%20Network%20%28UMMAN%29.%20UMMAN%0Acan%20obtain%20the%20embeddings%20of%20nodes%20in%20the%20Multi-Graph%20in%20an%20unsupervised%0Ascenario%2C%20so%20that%20it%20helps%20learn%20the%20multiplex%20association.%20Our%20method%20is%20the%0Afirst%20to%20combine%20Graph%20Neural%20Network%20with%20the%20task%20of%20intestinal%20flora%20disease%0Aprediction.%20We%20employ%20complex%20relation-types%20to%20construct%20the%20Original-Graph%0Aand%20disrupt%20the%20relationships%20among%20nodes%20to%20generate%20corresponding%0AShuffled-Graph.%20We%20introduce%20the%20Node%20Feature%20Global%20Integration%20%28NFGI%29%20module%0Ato%20represent%20the%20global%20features%20of%20the%20graph.%20Furthermore%2C%20we%20design%20a%20joint%0Aloss%20comprising%20adversarial%20loss%20and%20hybrid%20attention%20loss%20to%20ensure%20that%20the%0Areal%20graph%20embedding%20aligns%20closely%20with%20the%20Original-Graph%20and%20diverges%20from%0Athe%20Shuffled-Graph.%20Comprehensive%20experiments%20on%20five%20classical%20OTU%20gut%0Amicrobiome%20datasets%20demonstrate%20the%20effectiveness%20and%20stability%20of%20our%20method.%0A%28We%20will%20release%20our%20code%20soon.%29%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21714v1&entry.124074799=Read"},
{"title": "Human interaction classifier for LLM based chatbot", "author": "Diego Mart\u00edn and Jordi Sanchez and Xavier Vizca\u00edno", "abstract": "  This study investigates different approaches to classify human interactions\nin an artificial intelligence-based environment, specifically for Applus+\nIDIADA's intelligent agent AIDA. The main objective is to develop a classifier\nthat accurately identifies the type of interaction received (Conversation,\nServices, or Document Translation) to direct requests to the appropriate\nchannel and provide a more specialized and efficient service. Various models\nare compared, including LLM-based classifiers, KNN using Titan and Cohere\nembeddings, SVM, and artificial neural networks. Results show that SVM and ANN\nmodels with Cohere embeddings achieve the best overall performance, with\nsuperior F1 scores and faster execution times compared to LLM-based approaches.\nThe study concludes that the SVM model with Cohere embeddings is the most\nsuitable option for classifying human interactions in the AIDA environment,\noffering an optimal balance between accuracy and computational efficiency.\n", "link": "http://arxiv.org/abs/2407.21647v1", "date": "2024-07-31", "relevancy": 1.8703, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4699}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4681}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human%20interaction%20classifier%20for%20LLM%20based%20chatbot&body=Title%3A%20Human%20interaction%20classifier%20for%20LLM%20based%20chatbot%0AAuthor%3A%20Diego%20Mart%C3%ADn%20and%20Jordi%20Sanchez%20and%20Xavier%20Vizca%C3%ADno%0AAbstract%3A%20%20%20This%20study%20investigates%20different%20approaches%20to%20classify%20human%20interactions%0Ain%20an%20artificial%20intelligence-based%20environment%2C%20specifically%20for%20Applus%2B%0AIDIADA%27s%20intelligent%20agent%20AIDA.%20The%20main%20objective%20is%20to%20develop%20a%20classifier%0Athat%20accurately%20identifies%20the%20type%20of%20interaction%20received%20%28Conversation%2C%0AServices%2C%20or%20Document%20Translation%29%20to%20direct%20requests%20to%20the%20appropriate%0Achannel%20and%20provide%20a%20more%20specialized%20and%20efficient%20service.%20Various%20models%0Aare%20compared%2C%20including%20LLM-based%20classifiers%2C%20KNN%20using%20Titan%20and%20Cohere%0Aembeddings%2C%20SVM%2C%20and%20artificial%20neural%20networks.%20Results%20show%20that%20SVM%20and%20ANN%0Amodels%20with%20Cohere%20embeddings%20achieve%20the%20best%20overall%20performance%2C%20with%0Asuperior%20F1%20scores%20and%20faster%20execution%20times%20compared%20to%20LLM-based%20approaches.%0AThe%20study%20concludes%20that%20the%20SVM%20model%20with%20Cohere%20embeddings%20is%20the%20most%0Asuitable%20option%20for%20classifying%20human%20interactions%20in%20the%20AIDA%20environment%2C%0Aoffering%20an%20optimal%20balance%20between%20accuracy%20and%20computational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21647v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman%2520interaction%2520classifier%2520for%2520LLM%2520based%2520chatbot%26entry.906535625%3DDiego%2520Mart%25C3%25ADn%2520and%2520Jordi%2520Sanchez%2520and%2520Xavier%2520Vizca%25C3%25ADno%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520different%2520approaches%2520to%2520classify%2520human%2520interactions%250Ain%2520an%2520artificial%2520intelligence-based%2520environment%252C%2520specifically%2520for%2520Applus%252B%250AIDIADA%2527s%2520intelligent%2520agent%2520AIDA.%2520The%2520main%2520objective%2520is%2520to%2520develop%2520a%2520classifier%250Athat%2520accurately%2520identifies%2520the%2520type%2520of%2520interaction%2520received%2520%2528Conversation%252C%250AServices%252C%2520or%2520Document%2520Translation%2529%2520to%2520direct%2520requests%2520to%2520the%2520appropriate%250Achannel%2520and%2520provide%2520a%2520more%2520specialized%2520and%2520efficient%2520service.%2520Various%2520models%250Aare%2520compared%252C%2520including%2520LLM-based%2520classifiers%252C%2520KNN%2520using%2520Titan%2520and%2520Cohere%250Aembeddings%252C%2520SVM%252C%2520and%2520artificial%2520neural%2520networks.%2520Results%2520show%2520that%2520SVM%2520and%2520ANN%250Amodels%2520with%2520Cohere%2520embeddings%2520achieve%2520the%2520best%2520overall%2520performance%252C%2520with%250Asuperior%2520F1%2520scores%2520and%2520faster%2520execution%2520times%2520compared%2520to%2520LLM-based%2520approaches.%250AThe%2520study%2520concludes%2520that%2520the%2520SVM%2520model%2520with%2520Cohere%2520embeddings%2520is%2520the%2520most%250Asuitable%2520option%2520for%2520classifying%2520human%2520interactions%2520in%2520the%2520AIDA%2520environment%252C%250Aoffering%2520an%2520optimal%2520balance%2520between%2520accuracy%2520and%2520computational%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21647v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human%20interaction%20classifier%20for%20LLM%20based%20chatbot&entry.906535625=Diego%20Mart%C3%ADn%20and%20Jordi%20Sanchez%20and%20Xavier%20Vizca%C3%ADno&entry.1292438233=%20%20This%20study%20investigates%20different%20approaches%20to%20classify%20human%20interactions%0Ain%20an%20artificial%20intelligence-based%20environment%2C%20specifically%20for%20Applus%2B%0AIDIADA%27s%20intelligent%20agent%20AIDA.%20The%20main%20objective%20is%20to%20develop%20a%20classifier%0Athat%20accurately%20identifies%20the%20type%20of%20interaction%20received%20%28Conversation%2C%0AServices%2C%20or%20Document%20Translation%29%20to%20direct%20requests%20to%20the%20appropriate%0Achannel%20and%20provide%20a%20more%20specialized%20and%20efficient%20service.%20Various%20models%0Aare%20compared%2C%20including%20LLM-based%20classifiers%2C%20KNN%20using%20Titan%20and%20Cohere%0Aembeddings%2C%20SVM%2C%20and%20artificial%20neural%20networks.%20Results%20show%20that%20SVM%20and%20ANN%0Amodels%20with%20Cohere%20embeddings%20achieve%20the%20best%20overall%20performance%2C%20with%0Asuperior%20F1%20scores%20and%20faster%20execution%20times%20compared%20to%20LLM-based%20approaches.%0AThe%20study%20concludes%20that%20the%20SVM%20model%20with%20Cohere%20embeddings%20is%20the%20most%0Asuitable%20option%20for%20classifying%20human%20interactions%20in%20the%20AIDA%20environment%2C%0Aoffering%20an%20optimal%20balance%20between%20accuracy%20and%20computational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21647v1&entry.124074799=Read"},
{"title": "Can LLMs Understand Computer Networks? Towards a Virtual System\n  Administrator", "author": "Denis Donadel and Francesco Marchiori and Luca Pajola and Mauro Conti", "abstract": "  Recent advancements in Artificial Intelligence, and particularly Large\nLanguage Models (LLMs), offer promising prospects for aiding system\nadministrators in managing the complexity of modern networks. However, despite\nthis potential, a significant gap exists in the literature regarding the extent\nto which LLMs can understand computer networks. Without empirical evidence,\nsystem administrators might rely on these models without assurance of their\nefficacy in performing network-related tasks accurately.\n  In this paper, we are the first to conduct an exhaustive study on LLMs'\ncomprehension of computer networks. We formulate several research questions to\ndetermine whether LLMs can provide correct answers when supplied with a network\ntopology and questions on it. To assess them, we developed a thorough framework\nfor evaluating LLMs' capabilities in various network-related tasks. We evaluate\nour framework on multiple computer networks employing proprietary (e.g., GPT4)\nand open-source (e.g., Llama2) models. Our findings in general purpose LLMs\nusing a zero-shot scenario demonstrate promising results, with the best model\nachieving an average accuracy of 79.3%. Proprietary LLMs achieve noteworthy\nresults in small and medium networks, while challenges persist in comprehending\ncomplex network topologies, particularly for open-source models. Moreover, we\nprovide insight into how prompt engineering can enhance the accuracy of some\ntasks.\n", "link": "http://arxiv.org/abs/2404.12689v2", "date": "2024-07-31", "relevancy": 1.8624, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4703}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4688}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20LLMs%20Understand%20Computer%20Networks%3F%20Towards%20a%20Virtual%20System%0A%20%20Administrator&body=Title%3A%20Can%20LLMs%20Understand%20Computer%20Networks%3F%20Towards%20a%20Virtual%20System%0A%20%20Administrator%0AAuthor%3A%20Denis%20Donadel%20and%20Francesco%20Marchiori%20and%20Luca%20Pajola%20and%20Mauro%20Conti%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Artificial%20Intelligence%2C%20and%20particularly%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20offer%20promising%20prospects%20for%20aiding%20system%0Aadministrators%20in%20managing%20the%20complexity%20of%20modern%20networks.%20However%2C%20despite%0Athis%20potential%2C%20a%20significant%20gap%20exists%20in%20the%20literature%20regarding%20the%20extent%0Ato%20which%20LLMs%20can%20understand%20computer%20networks.%20Without%20empirical%20evidence%2C%0Asystem%20administrators%20might%20rely%20on%20these%20models%20without%20assurance%20of%20their%0Aefficacy%20in%20performing%20network-related%20tasks%20accurately.%0A%20%20In%20this%20paper%2C%20we%20are%20the%20first%20to%20conduct%20an%20exhaustive%20study%20on%20LLMs%27%0Acomprehension%20of%20computer%20networks.%20We%20formulate%20several%20research%20questions%20to%0Adetermine%20whether%20LLMs%20can%20provide%20correct%20answers%20when%20supplied%20with%20a%20network%0Atopology%20and%20questions%20on%20it.%20To%20assess%20them%2C%20we%20developed%20a%20thorough%20framework%0Afor%20evaluating%20LLMs%27%20capabilities%20in%20various%20network-related%20tasks.%20We%20evaluate%0Aour%20framework%20on%20multiple%20computer%20networks%20employing%20proprietary%20%28e.g.%2C%20GPT4%29%0Aand%20open-source%20%28e.g.%2C%20Llama2%29%20models.%20Our%20findings%20in%20general%20purpose%20LLMs%0Ausing%20a%20zero-shot%20scenario%20demonstrate%20promising%20results%2C%20with%20the%20best%20model%0Aachieving%20an%20average%20accuracy%20of%2079.3%25.%20Proprietary%20LLMs%20achieve%20noteworthy%0Aresults%20in%20small%20and%20medium%20networks%2C%20while%20challenges%20persist%20in%20comprehending%0Acomplex%20network%20topologies%2C%20particularly%20for%20open-source%20models.%20Moreover%2C%20we%0Aprovide%20insight%20into%20how%20prompt%20engineering%20can%20enhance%20the%20accuracy%20of%20some%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12689v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520LLMs%2520Understand%2520Computer%2520Networks%253F%2520Towards%2520a%2520Virtual%2520System%250A%2520%2520Administrator%26entry.906535625%3DDenis%2520Donadel%2520and%2520Francesco%2520Marchiori%2520and%2520Luca%2520Pajola%2520and%2520Mauro%2520Conti%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Artificial%2520Intelligence%252C%2520and%2520particularly%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%252C%2520offer%2520promising%2520prospects%2520for%2520aiding%2520system%250Aadministrators%2520in%2520managing%2520the%2520complexity%2520of%2520modern%2520networks.%2520However%252C%2520despite%250Athis%2520potential%252C%2520a%2520significant%2520gap%2520exists%2520in%2520the%2520literature%2520regarding%2520the%2520extent%250Ato%2520which%2520LLMs%2520can%2520understand%2520computer%2520networks.%2520Without%2520empirical%2520evidence%252C%250Asystem%2520administrators%2520might%2520rely%2520on%2520these%2520models%2520without%2520assurance%2520of%2520their%250Aefficacy%2520in%2520performing%2520network-related%2520tasks%2520accurately.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520are%2520the%2520first%2520to%2520conduct%2520an%2520exhaustive%2520study%2520on%2520LLMs%2527%250Acomprehension%2520of%2520computer%2520networks.%2520We%2520formulate%2520several%2520research%2520questions%2520to%250Adetermine%2520whether%2520LLMs%2520can%2520provide%2520correct%2520answers%2520when%2520supplied%2520with%2520a%2520network%250Atopology%2520and%2520questions%2520on%2520it.%2520To%2520assess%2520them%252C%2520we%2520developed%2520a%2520thorough%2520framework%250Afor%2520evaluating%2520LLMs%2527%2520capabilities%2520in%2520various%2520network-related%2520tasks.%2520We%2520evaluate%250Aour%2520framework%2520on%2520multiple%2520computer%2520networks%2520employing%2520proprietary%2520%2528e.g.%252C%2520GPT4%2529%250Aand%2520open-source%2520%2528e.g.%252C%2520Llama2%2529%2520models.%2520Our%2520findings%2520in%2520general%2520purpose%2520LLMs%250Ausing%2520a%2520zero-shot%2520scenario%2520demonstrate%2520promising%2520results%252C%2520with%2520the%2520best%2520model%250Aachieving%2520an%2520average%2520accuracy%2520of%252079.3%2525.%2520Proprietary%2520LLMs%2520achieve%2520noteworthy%250Aresults%2520in%2520small%2520and%2520medium%2520networks%252C%2520while%2520challenges%2520persist%2520in%2520comprehending%250Acomplex%2520network%2520topologies%252C%2520particularly%2520for%2520open-source%2520models.%2520Moreover%252C%2520we%250Aprovide%2520insight%2520into%2520how%2520prompt%2520engineering%2520can%2520enhance%2520the%2520accuracy%2520of%2520some%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.12689v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20LLMs%20Understand%20Computer%20Networks%3F%20Towards%20a%20Virtual%20System%0A%20%20Administrator&entry.906535625=Denis%20Donadel%20and%20Francesco%20Marchiori%20and%20Luca%20Pajola%20and%20Mauro%20Conti&entry.1292438233=%20%20Recent%20advancements%20in%20Artificial%20Intelligence%2C%20and%20particularly%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20offer%20promising%20prospects%20for%20aiding%20system%0Aadministrators%20in%20managing%20the%20complexity%20of%20modern%20networks.%20However%2C%20despite%0Athis%20potential%2C%20a%20significant%20gap%20exists%20in%20the%20literature%20regarding%20the%20extent%0Ato%20which%20LLMs%20can%20understand%20computer%20networks.%20Without%20empirical%20evidence%2C%0Asystem%20administrators%20might%20rely%20on%20these%20models%20without%20assurance%20of%20their%0Aefficacy%20in%20performing%20network-related%20tasks%20accurately.%0A%20%20In%20this%20paper%2C%20we%20are%20the%20first%20to%20conduct%20an%20exhaustive%20study%20on%20LLMs%27%0Acomprehension%20of%20computer%20networks.%20We%20formulate%20several%20research%20questions%20to%0Adetermine%20whether%20LLMs%20can%20provide%20correct%20answers%20when%20supplied%20with%20a%20network%0Atopology%20and%20questions%20on%20it.%20To%20assess%20them%2C%20we%20developed%20a%20thorough%20framework%0Afor%20evaluating%20LLMs%27%20capabilities%20in%20various%20network-related%20tasks.%20We%20evaluate%0Aour%20framework%20on%20multiple%20computer%20networks%20employing%20proprietary%20%28e.g.%2C%20GPT4%29%0Aand%20open-source%20%28e.g.%2C%20Llama2%29%20models.%20Our%20findings%20in%20general%20purpose%20LLMs%0Ausing%20a%20zero-shot%20scenario%20demonstrate%20promising%20results%2C%20with%20the%20best%20model%0Aachieving%20an%20average%20accuracy%20of%2079.3%25.%20Proprietary%20LLMs%20achieve%20noteworthy%0Aresults%20in%20small%20and%20medium%20networks%2C%20while%20challenges%20persist%20in%20comprehending%0Acomplex%20network%20topologies%2C%20particularly%20for%20open-source%20models.%20Moreover%2C%20we%0Aprovide%20insight%20into%20how%20prompt%20engineering%20can%20enhance%20the%20accuracy%20of%20some%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12689v2&entry.124074799=Read"},
{"title": "Neural Cellular Automata for Lightweight, Robust and Explainable\n  Classification of White Blood Cell Images", "author": "Michael Deutges and Ario Sadafi and Nassir Navab and Carsten Marr", "abstract": "  Diagnosis of hematological malignancies depends on accurate identification of\nwhite blood cells in peripheral blood smears. Deep learning techniques are\nemerging as a viable solution to scale and optimize this process by automatic\ncell classification. However, these techniques face several challenges such as\nlimited generalizability, sensitivity to domain shifts, and lack of\nexplainability. Here, we introduce a novel approach for white blood cell\nclassification based on neural cellular automata (NCA). We test our approach on\nthree datasets of white blood cell images and show that we achieve competitive\nperformance compared to conventional methods. Our NCA-based method is\nsignificantly smaller in terms of parameters and exhibits robustness to domain\nshifts. Furthermore, the architecture is inherently explainable, providing\ninsights into the decision process for each classification, which helps to\nunderstand and validate model predictions. Our results demonstrate that NCA can\nbe used for image classification, and that they address key challenges of\nconventional methods, indicating a high potential for applicability in clinical\npractice.\n", "link": "http://arxiv.org/abs/2404.05584v2", "date": "2024-07-31", "relevancy": 1.8459, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.464}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4598}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Cellular%20Automata%20for%20Lightweight%2C%20Robust%20and%20Explainable%0A%20%20Classification%20of%20White%20Blood%20Cell%20Images&body=Title%3A%20Neural%20Cellular%20Automata%20for%20Lightweight%2C%20Robust%20and%20Explainable%0A%20%20Classification%20of%20White%20Blood%20Cell%20Images%0AAuthor%3A%20Michael%20Deutges%20and%20Ario%20Sadafi%20and%20Nassir%20Navab%20and%20Carsten%20Marr%0AAbstract%3A%20%20%20Diagnosis%20of%20hematological%20malignancies%20depends%20on%20accurate%20identification%20of%0Awhite%20blood%20cells%20in%20peripheral%20blood%20smears.%20Deep%20learning%20techniques%20are%0Aemerging%20as%20a%20viable%20solution%20to%20scale%20and%20optimize%20this%20process%20by%20automatic%0Acell%20classification.%20However%2C%20these%20techniques%20face%20several%20challenges%20such%20as%0Alimited%20generalizability%2C%20sensitivity%20to%20domain%20shifts%2C%20and%20lack%20of%0Aexplainability.%20Here%2C%20we%20introduce%20a%20novel%20approach%20for%20white%20blood%20cell%0Aclassification%20based%20on%20neural%20cellular%20automata%20%28NCA%29.%20We%20test%20our%20approach%20on%0Athree%20datasets%20of%20white%20blood%20cell%20images%20and%20show%20that%20we%20achieve%20competitive%0Aperformance%20compared%20to%20conventional%20methods.%20Our%20NCA-based%20method%20is%0Asignificantly%20smaller%20in%20terms%20of%20parameters%20and%20exhibits%20robustness%20to%20domain%0Ashifts.%20Furthermore%2C%20the%20architecture%20is%20inherently%20explainable%2C%20providing%0Ainsights%20into%20the%20decision%20process%20for%20each%20classification%2C%20which%20helps%20to%0Aunderstand%20and%20validate%20model%20predictions.%20Our%20results%20demonstrate%20that%20NCA%20can%0Abe%20used%20for%20image%20classification%2C%20and%20that%20they%20address%20key%20challenges%20of%0Aconventional%20methods%2C%20indicating%20a%20high%20potential%20for%20applicability%20in%20clinical%0Apractice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05584v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Cellular%2520Automata%2520for%2520Lightweight%252C%2520Robust%2520and%2520Explainable%250A%2520%2520Classification%2520of%2520White%2520Blood%2520Cell%2520Images%26entry.906535625%3DMichael%2520Deutges%2520and%2520Ario%2520Sadafi%2520and%2520Nassir%2520Navab%2520and%2520Carsten%2520Marr%26entry.1292438233%3D%2520%2520Diagnosis%2520of%2520hematological%2520malignancies%2520depends%2520on%2520accurate%2520identification%2520of%250Awhite%2520blood%2520cells%2520in%2520peripheral%2520blood%2520smears.%2520Deep%2520learning%2520techniques%2520are%250Aemerging%2520as%2520a%2520viable%2520solution%2520to%2520scale%2520and%2520optimize%2520this%2520process%2520by%2520automatic%250Acell%2520classification.%2520However%252C%2520these%2520techniques%2520face%2520several%2520challenges%2520such%2520as%250Alimited%2520generalizability%252C%2520sensitivity%2520to%2520domain%2520shifts%252C%2520and%2520lack%2520of%250Aexplainability.%2520Here%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520for%2520white%2520blood%2520cell%250Aclassification%2520based%2520on%2520neural%2520cellular%2520automata%2520%2528NCA%2529.%2520We%2520test%2520our%2520approach%2520on%250Athree%2520datasets%2520of%2520white%2520blood%2520cell%2520images%2520and%2520show%2520that%2520we%2520achieve%2520competitive%250Aperformance%2520compared%2520to%2520conventional%2520methods.%2520Our%2520NCA-based%2520method%2520is%250Asignificantly%2520smaller%2520in%2520terms%2520of%2520parameters%2520and%2520exhibits%2520robustness%2520to%2520domain%250Ashifts.%2520Furthermore%252C%2520the%2520architecture%2520is%2520inherently%2520explainable%252C%2520providing%250Ainsights%2520into%2520the%2520decision%2520process%2520for%2520each%2520classification%252C%2520which%2520helps%2520to%250Aunderstand%2520and%2520validate%2520model%2520predictions.%2520Our%2520results%2520demonstrate%2520that%2520NCA%2520can%250Abe%2520used%2520for%2520image%2520classification%252C%2520and%2520that%2520they%2520address%2520key%2520challenges%2520of%250Aconventional%2520methods%252C%2520indicating%2520a%2520high%2520potential%2520for%2520applicability%2520in%2520clinical%250Apractice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.05584v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Cellular%20Automata%20for%20Lightweight%2C%20Robust%20and%20Explainable%0A%20%20Classification%20of%20White%20Blood%20Cell%20Images&entry.906535625=Michael%20Deutges%20and%20Ario%20Sadafi%20and%20Nassir%20Navab%20and%20Carsten%20Marr&entry.1292438233=%20%20Diagnosis%20of%20hematological%20malignancies%20depends%20on%20accurate%20identification%20of%0Awhite%20blood%20cells%20in%20peripheral%20blood%20smears.%20Deep%20learning%20techniques%20are%0Aemerging%20as%20a%20viable%20solution%20to%20scale%20and%20optimize%20this%20process%20by%20automatic%0Acell%20classification.%20However%2C%20these%20techniques%20face%20several%20challenges%20such%20as%0Alimited%20generalizability%2C%20sensitivity%20to%20domain%20shifts%2C%20and%20lack%20of%0Aexplainability.%20Here%2C%20we%20introduce%20a%20novel%20approach%20for%20white%20blood%20cell%0Aclassification%20based%20on%20neural%20cellular%20automata%20%28NCA%29.%20We%20test%20our%20approach%20on%0Athree%20datasets%20of%20white%20blood%20cell%20images%20and%20show%20that%20we%20achieve%20competitive%0Aperformance%20compared%20to%20conventional%20methods.%20Our%20NCA-based%20method%20is%0Asignificantly%20smaller%20in%20terms%20of%20parameters%20and%20exhibits%20robustness%20to%20domain%0Ashifts.%20Furthermore%2C%20the%20architecture%20is%20inherently%20explainable%2C%20providing%0Ainsights%20into%20the%20decision%20process%20for%20each%20classification%2C%20which%20helps%20to%0Aunderstand%20and%20validate%20model%20predictions.%20Our%20results%20demonstrate%20that%20NCA%20can%0Abe%20used%20for%20image%20classification%2C%20and%20that%20they%20address%20key%20challenges%20of%0Aconventional%20methods%2C%20indicating%20a%20high%20potential%20for%20applicability%20in%20clinical%0Apractice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05584v2&entry.124074799=Read"},
{"title": "Analysis of Functional Insufficiencies and Triggering Conditions to\n  Improve the SOTIF of an MPC-based Trajectory Planner", "author": "Mirko Conrad and Georg Schildbach", "abstract": "  Automated and autonomous driving has made a significatnt technological leap\nover the past decade. In this process, the complexity of algorithms used for\nvehicle control has grown significantly. Model Predictive Control (MPC) is a\nprominent example, which has gained enormous popularity and is now widely used\nfor vehicle motion planning and control. However, safety concerns constrain its\npractical application, especially since traditional procedures of functional\nsafety (FS), with its universal standard ISO26262, reach their limits.\nConcomitantly, the new aspect of safety-of-the-intended-Function (SOTIF) has\nmoved into the center of attention, whose standard, ISO21448, has only been\nreleased in 2022. Thus, experience with SOTIF is low and few case studies are\navailable in industry and research. Hence this paper aims to make two main\ncontributions: (1) an analysis of the SOTIF for a generic MPC-based trajectory\nplanner and (2) an interpretation and concrete application of the generic\nprocedures described in ISO21448 for determining functional insufficiencies\n(FIs) and triggering conditions (TCs). Particular novelties of the paper\ninclude an approach for the out-of-context development of SOTIF-related\nelements (SOTIF-EooC), a compilation of important FIs and TCs for a MPC-based\ntrajectory planner, and an optimized safety concept based on the identified FIs\nand TCs for the MPC-based trajectory planner.\n", "link": "http://arxiv.org/abs/2407.21569v1", "date": "2024-07-31", "relevancy": 1.8433, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5021}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4557}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analysis%20of%20Functional%20Insufficiencies%20and%20Triggering%20Conditions%20to%0A%20%20Improve%20the%20SOTIF%20of%20an%20MPC-based%20Trajectory%20Planner&body=Title%3A%20Analysis%20of%20Functional%20Insufficiencies%20and%20Triggering%20Conditions%20to%0A%20%20Improve%20the%20SOTIF%20of%20an%20MPC-based%20Trajectory%20Planner%0AAuthor%3A%20Mirko%20Conrad%20and%20Georg%20Schildbach%0AAbstract%3A%20%20%20Automated%20and%20autonomous%20driving%20has%20made%20a%20significatnt%20technological%20leap%0Aover%20the%20past%20decade.%20In%20this%20process%2C%20the%20complexity%20of%20algorithms%20used%20for%0Avehicle%20control%20has%20grown%20significantly.%20Model%20Predictive%20Control%20%28MPC%29%20is%20a%0Aprominent%20example%2C%20which%20has%20gained%20enormous%20popularity%20and%20is%20now%20widely%20used%0Afor%20vehicle%20motion%20planning%20and%20control.%20However%2C%20safety%20concerns%20constrain%20its%0Apractical%20application%2C%20especially%20since%20traditional%20procedures%20of%20functional%0Asafety%20%28FS%29%2C%20with%20its%20universal%20standard%20ISO26262%2C%20reach%20their%20limits.%0AConcomitantly%2C%20the%20new%20aspect%20of%20safety-of-the-intended-Function%20%28SOTIF%29%20has%0Amoved%20into%20the%20center%20of%20attention%2C%20whose%20standard%2C%20ISO21448%2C%20has%20only%20been%0Areleased%20in%202022.%20Thus%2C%20experience%20with%20SOTIF%20is%20low%20and%20few%20case%20studies%20are%0Aavailable%20in%20industry%20and%20research.%20Hence%20this%20paper%20aims%20to%20make%20two%20main%0Acontributions%3A%20%281%29%20an%20analysis%20of%20the%20SOTIF%20for%20a%20generic%20MPC-based%20trajectory%0Aplanner%20and%20%282%29%20an%20interpretation%20and%20concrete%20application%20of%20the%20generic%0Aprocedures%20described%20in%20ISO21448%20for%20determining%20functional%20insufficiencies%0A%28FIs%29%20and%20triggering%20conditions%20%28TCs%29.%20Particular%20novelties%20of%20the%20paper%0Ainclude%20an%20approach%20for%20the%20out-of-context%20development%20of%20SOTIF-related%0Aelements%20%28SOTIF-EooC%29%2C%20a%20compilation%20of%20important%20FIs%20and%20TCs%20for%20a%20MPC-based%0Atrajectory%20planner%2C%20and%20an%20optimized%20safety%20concept%20based%20on%20the%20identified%20FIs%0Aand%20TCs%20for%20the%20MPC-based%20trajectory%20planner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21569v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalysis%2520of%2520Functional%2520Insufficiencies%2520and%2520Triggering%2520Conditions%2520to%250A%2520%2520Improve%2520the%2520SOTIF%2520of%2520an%2520MPC-based%2520Trajectory%2520Planner%26entry.906535625%3DMirko%2520Conrad%2520and%2520Georg%2520Schildbach%26entry.1292438233%3D%2520%2520Automated%2520and%2520autonomous%2520driving%2520has%2520made%2520a%2520significatnt%2520technological%2520leap%250Aover%2520the%2520past%2520decade.%2520In%2520this%2520process%252C%2520the%2520complexity%2520of%2520algorithms%2520used%2520for%250Avehicle%2520control%2520has%2520grown%2520significantly.%2520Model%2520Predictive%2520Control%2520%2528MPC%2529%2520is%2520a%250Aprominent%2520example%252C%2520which%2520has%2520gained%2520enormous%2520popularity%2520and%2520is%2520now%2520widely%2520used%250Afor%2520vehicle%2520motion%2520planning%2520and%2520control.%2520However%252C%2520safety%2520concerns%2520constrain%2520its%250Apractical%2520application%252C%2520especially%2520since%2520traditional%2520procedures%2520of%2520functional%250Asafety%2520%2528FS%2529%252C%2520with%2520its%2520universal%2520standard%2520ISO26262%252C%2520reach%2520their%2520limits.%250AConcomitantly%252C%2520the%2520new%2520aspect%2520of%2520safety-of-the-intended-Function%2520%2528SOTIF%2529%2520has%250Amoved%2520into%2520the%2520center%2520of%2520attention%252C%2520whose%2520standard%252C%2520ISO21448%252C%2520has%2520only%2520been%250Areleased%2520in%25202022.%2520Thus%252C%2520experience%2520with%2520SOTIF%2520is%2520low%2520and%2520few%2520case%2520studies%2520are%250Aavailable%2520in%2520industry%2520and%2520research.%2520Hence%2520this%2520paper%2520aims%2520to%2520make%2520two%2520main%250Acontributions%253A%2520%25281%2529%2520an%2520analysis%2520of%2520the%2520SOTIF%2520for%2520a%2520generic%2520MPC-based%2520trajectory%250Aplanner%2520and%2520%25282%2529%2520an%2520interpretation%2520and%2520concrete%2520application%2520of%2520the%2520generic%250Aprocedures%2520described%2520in%2520ISO21448%2520for%2520determining%2520functional%2520insufficiencies%250A%2528FIs%2529%2520and%2520triggering%2520conditions%2520%2528TCs%2529.%2520Particular%2520novelties%2520of%2520the%2520paper%250Ainclude%2520an%2520approach%2520for%2520the%2520out-of-context%2520development%2520of%2520SOTIF-related%250Aelements%2520%2528SOTIF-EooC%2529%252C%2520a%2520compilation%2520of%2520important%2520FIs%2520and%2520TCs%2520for%2520a%2520MPC-based%250Atrajectory%2520planner%252C%2520and%2520an%2520optimized%2520safety%2520concept%2520based%2520on%2520the%2520identified%2520FIs%250Aand%2520TCs%2520for%2520the%2520MPC-based%2520trajectory%2520planner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21569v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analysis%20of%20Functional%20Insufficiencies%20and%20Triggering%20Conditions%20to%0A%20%20Improve%20the%20SOTIF%20of%20an%20MPC-based%20Trajectory%20Planner&entry.906535625=Mirko%20Conrad%20and%20Georg%20Schildbach&entry.1292438233=%20%20Automated%20and%20autonomous%20driving%20has%20made%20a%20significatnt%20technological%20leap%0Aover%20the%20past%20decade.%20In%20this%20process%2C%20the%20complexity%20of%20algorithms%20used%20for%0Avehicle%20control%20has%20grown%20significantly.%20Model%20Predictive%20Control%20%28MPC%29%20is%20a%0Aprominent%20example%2C%20which%20has%20gained%20enormous%20popularity%20and%20is%20now%20widely%20used%0Afor%20vehicle%20motion%20planning%20and%20control.%20However%2C%20safety%20concerns%20constrain%20its%0Apractical%20application%2C%20especially%20since%20traditional%20procedures%20of%20functional%0Asafety%20%28FS%29%2C%20with%20its%20universal%20standard%20ISO26262%2C%20reach%20their%20limits.%0AConcomitantly%2C%20the%20new%20aspect%20of%20safety-of-the-intended-Function%20%28SOTIF%29%20has%0Amoved%20into%20the%20center%20of%20attention%2C%20whose%20standard%2C%20ISO21448%2C%20has%20only%20been%0Areleased%20in%202022.%20Thus%2C%20experience%20with%20SOTIF%20is%20low%20and%20few%20case%20studies%20are%0Aavailable%20in%20industry%20and%20research.%20Hence%20this%20paper%20aims%20to%20make%20two%20main%0Acontributions%3A%20%281%29%20an%20analysis%20of%20the%20SOTIF%20for%20a%20generic%20MPC-based%20trajectory%0Aplanner%20and%20%282%29%20an%20interpretation%20and%20concrete%20application%20of%20the%20generic%0Aprocedures%20described%20in%20ISO21448%20for%20determining%20functional%20insufficiencies%0A%28FIs%29%20and%20triggering%20conditions%20%28TCs%29.%20Particular%20novelties%20of%20the%20paper%0Ainclude%20an%20approach%20for%20the%20out-of-context%20development%20of%20SOTIF-related%0Aelements%20%28SOTIF-EooC%29%2C%20a%20compilation%20of%20important%20FIs%20and%20TCs%20for%20a%20MPC-based%0Atrajectory%20planner%2C%20and%20an%20optimized%20safety%20concept%20based%20on%20the%20identified%20FIs%0Aand%20TCs%20for%20the%20MPC-based%20trajectory%20planner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21569v1&entry.124074799=Read"},
{"title": "DGInStyle: Domain-Generalizable Semantic Segmentation with Image\n  Diffusion Models and Stylized Semantic Control", "author": "Yuru Jia and Lukas Hoyer and Shengyu Huang and Tianfu Wang and Luc Van Gool and Konrad Schindler and Anton Obukhov", "abstract": "  Large, pretrained latent diffusion models (LDMs) have demonstrated an\nextraordinary ability to generate creative content, specialize to user data\nthrough few-shot fine-tuning, and condition their output on other modalities,\nsuch as semantic maps. However, are they usable as large-scale data generators,\ne.g., to improve tasks in the perception stack, like semantic segmentation? We\ninvestigate this question in the context of autonomous driving, and answer it\nwith a resounding \"yes\". We propose an efficient data generation pipeline\ntermed DGInStyle. First, we examine the problem of specializing a pretrained\nLDM to semantically-controlled generation within a narrow domain. Second, we\npropose a Style Swap technique to endow the rich generative prior with the\nlearned semantic control. Third, we design a Multi-resolution Latent Fusion\ntechnique to overcome the bias of LDMs towards dominant objects. Using\nDGInStyle, we generate a diverse dataset of street scenes, train a\ndomain-agnostic semantic segmentation model on it, and evaluate the model on\nmultiple popular autonomous driving datasets. Our approach consistently\nincreases the performance of several domain generalization methods compared to\nthe previous state-of-the-art methods. The source code and the generated\ndataset are available at https://dginstyle.github.io.\n", "link": "http://arxiv.org/abs/2312.03048v3", "date": "2024-07-31", "relevancy": 1.839, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6337}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6145}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6041}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DGInStyle%3A%20Domain-Generalizable%20Semantic%20Segmentation%20with%20Image%0A%20%20Diffusion%20Models%20and%20Stylized%20Semantic%20Control&body=Title%3A%20DGInStyle%3A%20Domain-Generalizable%20Semantic%20Segmentation%20with%20Image%0A%20%20Diffusion%20Models%20and%20Stylized%20Semantic%20Control%0AAuthor%3A%20Yuru%20Jia%20and%20Lukas%20Hoyer%20and%20Shengyu%20Huang%20and%20Tianfu%20Wang%20and%20Luc%20Van%20Gool%20and%20Konrad%20Schindler%20and%20Anton%20Obukhov%0AAbstract%3A%20%20%20Large%2C%20pretrained%20latent%20diffusion%20models%20%28LDMs%29%20have%20demonstrated%20an%0Aextraordinary%20ability%20to%20generate%20creative%20content%2C%20specialize%20to%20user%20data%0Athrough%20few-shot%20fine-tuning%2C%20and%20condition%20their%20output%20on%20other%20modalities%2C%0Asuch%20as%20semantic%20maps.%20However%2C%20are%20they%20usable%20as%20large-scale%20data%20generators%2C%0Ae.g.%2C%20to%20improve%20tasks%20in%20the%20perception%20stack%2C%20like%20semantic%20segmentation%3F%20We%0Ainvestigate%20this%20question%20in%20the%20context%20of%20autonomous%20driving%2C%20and%20answer%20it%0Awith%20a%20resounding%20%22yes%22.%20We%20propose%20an%20efficient%20data%20generation%20pipeline%0Atermed%20DGInStyle.%20First%2C%20we%20examine%20the%20problem%20of%20specializing%20a%20pretrained%0ALDM%20to%20semantically-controlled%20generation%20within%20a%20narrow%20domain.%20Second%2C%20we%0Apropose%20a%20Style%20Swap%20technique%20to%20endow%20the%20rich%20generative%20prior%20with%20the%0Alearned%20semantic%20control.%20Third%2C%20we%20design%20a%20Multi-resolution%20Latent%20Fusion%0Atechnique%20to%20overcome%20the%20bias%20of%20LDMs%20towards%20dominant%20objects.%20Using%0ADGInStyle%2C%20we%20generate%20a%20diverse%20dataset%20of%20street%20scenes%2C%20train%20a%0Adomain-agnostic%20semantic%20segmentation%20model%20on%20it%2C%20and%20evaluate%20the%20model%20on%0Amultiple%20popular%20autonomous%20driving%20datasets.%20Our%20approach%20consistently%0Aincreases%20the%20performance%20of%20several%20domain%20generalization%20methods%20compared%20to%0Athe%20previous%20state-of-the-art%20methods.%20The%20source%20code%20and%20the%20generated%0Adataset%20are%20available%20at%20https%3A//dginstyle.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.03048v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDGInStyle%253A%2520Domain-Generalizable%2520Semantic%2520Segmentation%2520with%2520Image%250A%2520%2520Diffusion%2520Models%2520and%2520Stylized%2520Semantic%2520Control%26entry.906535625%3DYuru%2520Jia%2520and%2520Lukas%2520Hoyer%2520and%2520Shengyu%2520Huang%2520and%2520Tianfu%2520Wang%2520and%2520Luc%2520Van%2520Gool%2520and%2520Konrad%2520Schindler%2520and%2520Anton%2520Obukhov%26entry.1292438233%3D%2520%2520Large%252C%2520pretrained%2520latent%2520diffusion%2520models%2520%2528LDMs%2529%2520have%2520demonstrated%2520an%250Aextraordinary%2520ability%2520to%2520generate%2520creative%2520content%252C%2520specialize%2520to%2520user%2520data%250Athrough%2520few-shot%2520fine-tuning%252C%2520and%2520condition%2520their%2520output%2520on%2520other%2520modalities%252C%250Asuch%2520as%2520semantic%2520maps.%2520However%252C%2520are%2520they%2520usable%2520as%2520large-scale%2520data%2520generators%252C%250Ae.g.%252C%2520to%2520improve%2520tasks%2520in%2520the%2520perception%2520stack%252C%2520like%2520semantic%2520segmentation%253F%2520We%250Ainvestigate%2520this%2520question%2520in%2520the%2520context%2520of%2520autonomous%2520driving%252C%2520and%2520answer%2520it%250Awith%2520a%2520resounding%2520%2522yes%2522.%2520We%2520propose%2520an%2520efficient%2520data%2520generation%2520pipeline%250Atermed%2520DGInStyle.%2520First%252C%2520we%2520examine%2520the%2520problem%2520of%2520specializing%2520a%2520pretrained%250ALDM%2520to%2520semantically-controlled%2520generation%2520within%2520a%2520narrow%2520domain.%2520Second%252C%2520we%250Apropose%2520a%2520Style%2520Swap%2520technique%2520to%2520endow%2520the%2520rich%2520generative%2520prior%2520with%2520the%250Alearned%2520semantic%2520control.%2520Third%252C%2520we%2520design%2520a%2520Multi-resolution%2520Latent%2520Fusion%250Atechnique%2520to%2520overcome%2520the%2520bias%2520of%2520LDMs%2520towards%2520dominant%2520objects.%2520Using%250ADGInStyle%252C%2520we%2520generate%2520a%2520diverse%2520dataset%2520of%2520street%2520scenes%252C%2520train%2520a%250Adomain-agnostic%2520semantic%2520segmentation%2520model%2520on%2520it%252C%2520and%2520evaluate%2520the%2520model%2520on%250Amultiple%2520popular%2520autonomous%2520driving%2520datasets.%2520Our%2520approach%2520consistently%250Aincreases%2520the%2520performance%2520of%2520several%2520domain%2520generalization%2520methods%2520compared%2520to%250Athe%2520previous%2520state-of-the-art%2520methods.%2520The%2520source%2520code%2520and%2520the%2520generated%250Adataset%2520are%2520available%2520at%2520https%253A//dginstyle.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.03048v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DGInStyle%3A%20Domain-Generalizable%20Semantic%20Segmentation%20with%20Image%0A%20%20Diffusion%20Models%20and%20Stylized%20Semantic%20Control&entry.906535625=Yuru%20Jia%20and%20Lukas%20Hoyer%20and%20Shengyu%20Huang%20and%20Tianfu%20Wang%20and%20Luc%20Van%20Gool%20and%20Konrad%20Schindler%20and%20Anton%20Obukhov&entry.1292438233=%20%20Large%2C%20pretrained%20latent%20diffusion%20models%20%28LDMs%29%20have%20demonstrated%20an%0Aextraordinary%20ability%20to%20generate%20creative%20content%2C%20specialize%20to%20user%20data%0Athrough%20few-shot%20fine-tuning%2C%20and%20condition%20their%20output%20on%20other%20modalities%2C%0Asuch%20as%20semantic%20maps.%20However%2C%20are%20they%20usable%20as%20large-scale%20data%20generators%2C%0Ae.g.%2C%20to%20improve%20tasks%20in%20the%20perception%20stack%2C%20like%20semantic%20segmentation%3F%20We%0Ainvestigate%20this%20question%20in%20the%20context%20of%20autonomous%20driving%2C%20and%20answer%20it%0Awith%20a%20resounding%20%22yes%22.%20We%20propose%20an%20efficient%20data%20generation%20pipeline%0Atermed%20DGInStyle.%20First%2C%20we%20examine%20the%20problem%20of%20specializing%20a%20pretrained%0ALDM%20to%20semantically-controlled%20generation%20within%20a%20narrow%20domain.%20Second%2C%20we%0Apropose%20a%20Style%20Swap%20technique%20to%20endow%20the%20rich%20generative%20prior%20with%20the%0Alearned%20semantic%20control.%20Third%2C%20we%20design%20a%20Multi-resolution%20Latent%20Fusion%0Atechnique%20to%20overcome%20the%20bias%20of%20LDMs%20towards%20dominant%20objects.%20Using%0ADGInStyle%2C%20we%20generate%20a%20diverse%20dataset%20of%20street%20scenes%2C%20train%20a%0Adomain-agnostic%20semantic%20segmentation%20model%20on%20it%2C%20and%20evaluate%20the%20model%20on%0Amultiple%20popular%20autonomous%20driving%20datasets.%20Our%20approach%20consistently%0Aincreases%20the%20performance%20of%20several%20domain%20generalization%20methods%20compared%20to%0Athe%20previous%20state-of-the-art%20methods.%20The%20source%20code%20and%20the%20generated%0Adataset%20are%20available%20at%20https%3A//dginstyle.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.03048v3&entry.124074799=Read"},
{"title": "Quality Control for Radiology Report Generation Models via Auxiliary\n  Auditing Components", "author": "Hermione Warr and Yasin Ibrahim and Daniel R. McGowan and Konstantinos Kamnitsas", "abstract": "  Automation of medical image interpretation could alleviate bottlenecks in\ndiagnostic workflows, and has become of particular interest in recent years due\nto advancements in natural language processing. Great strides have been made\ntowards automated radiology report generation via AI, yet ensuring clinical\naccuracy in generated reports is a significant challenge, hindering deployment\nof such methods in clinical practice. In this work we propose a quality control\nframework for assessing the reliability of AI-generated radiology reports with\nrespect to semantics of diagnostic importance using modular auxiliary auditing\ncomponents (AC). Evaluating our pipeline on the MIMIC-CXR dataset, our findings\nshow that incorporating ACs in the form of disease-classifiers can enable\nauditing that identifies more reliable reports, resulting in higher F1 scores\ncompared to unfiltered generated reports. Additionally, leveraging the\nconfidence of the AC labels further improves the audit's effectiveness.\n", "link": "http://arxiv.org/abs/2407.21638v1", "date": "2024-07-31", "relevancy": 1.8348, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4893}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4601}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quality%20Control%20for%20Radiology%20Report%20Generation%20Models%20via%20Auxiliary%0A%20%20Auditing%20Components&body=Title%3A%20Quality%20Control%20for%20Radiology%20Report%20Generation%20Models%20via%20Auxiliary%0A%20%20Auditing%20Components%0AAuthor%3A%20Hermione%20Warr%20and%20Yasin%20Ibrahim%20and%20Daniel%20R.%20McGowan%20and%20Konstantinos%20Kamnitsas%0AAbstract%3A%20%20%20Automation%20of%20medical%20image%20interpretation%20could%20alleviate%20bottlenecks%20in%0Adiagnostic%20workflows%2C%20and%20has%20become%20of%20particular%20interest%20in%20recent%20years%20due%0Ato%20advancements%20in%20natural%20language%20processing.%20Great%20strides%20have%20been%20made%0Atowards%20automated%20radiology%20report%20generation%20via%20AI%2C%20yet%20ensuring%20clinical%0Aaccuracy%20in%20generated%20reports%20is%20a%20significant%20challenge%2C%20hindering%20deployment%0Aof%20such%20methods%20in%20clinical%20practice.%20In%20this%20work%20we%20propose%20a%20quality%20control%0Aframework%20for%20assessing%20the%20reliability%20of%20AI-generated%20radiology%20reports%20with%0Arespect%20to%20semantics%20of%20diagnostic%20importance%20using%20modular%20auxiliary%20auditing%0Acomponents%20%28AC%29.%20Evaluating%20our%20pipeline%20on%20the%20MIMIC-CXR%20dataset%2C%20our%20findings%0Ashow%20that%20incorporating%20ACs%20in%20the%20form%20of%20disease-classifiers%20can%20enable%0Aauditing%20that%20identifies%20more%20reliable%20reports%2C%20resulting%20in%20higher%20F1%20scores%0Acompared%20to%20unfiltered%20generated%20reports.%20Additionally%2C%20leveraging%20the%0Aconfidence%20of%20the%20AC%20labels%20further%20improves%20the%20audit%27s%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21638v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuality%2520Control%2520for%2520Radiology%2520Report%2520Generation%2520Models%2520via%2520Auxiliary%250A%2520%2520Auditing%2520Components%26entry.906535625%3DHermione%2520Warr%2520and%2520Yasin%2520Ibrahim%2520and%2520Daniel%2520R.%2520McGowan%2520and%2520Konstantinos%2520Kamnitsas%26entry.1292438233%3D%2520%2520Automation%2520of%2520medical%2520image%2520interpretation%2520could%2520alleviate%2520bottlenecks%2520in%250Adiagnostic%2520workflows%252C%2520and%2520has%2520become%2520of%2520particular%2520interest%2520in%2520recent%2520years%2520due%250Ato%2520advancements%2520in%2520natural%2520language%2520processing.%2520Great%2520strides%2520have%2520been%2520made%250Atowards%2520automated%2520radiology%2520report%2520generation%2520via%2520AI%252C%2520yet%2520ensuring%2520clinical%250Aaccuracy%2520in%2520generated%2520reports%2520is%2520a%2520significant%2520challenge%252C%2520hindering%2520deployment%250Aof%2520such%2520methods%2520in%2520clinical%2520practice.%2520In%2520this%2520work%2520we%2520propose%2520a%2520quality%2520control%250Aframework%2520for%2520assessing%2520the%2520reliability%2520of%2520AI-generated%2520radiology%2520reports%2520with%250Arespect%2520to%2520semantics%2520of%2520diagnostic%2520importance%2520using%2520modular%2520auxiliary%2520auditing%250Acomponents%2520%2528AC%2529.%2520Evaluating%2520our%2520pipeline%2520on%2520the%2520MIMIC-CXR%2520dataset%252C%2520our%2520findings%250Ashow%2520that%2520incorporating%2520ACs%2520in%2520the%2520form%2520of%2520disease-classifiers%2520can%2520enable%250Aauditing%2520that%2520identifies%2520more%2520reliable%2520reports%252C%2520resulting%2520in%2520higher%2520F1%2520scores%250Acompared%2520to%2520unfiltered%2520generated%2520reports.%2520Additionally%252C%2520leveraging%2520the%250Aconfidence%2520of%2520the%2520AC%2520labels%2520further%2520improves%2520the%2520audit%2527s%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21638v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quality%20Control%20for%20Radiology%20Report%20Generation%20Models%20via%20Auxiliary%0A%20%20Auditing%20Components&entry.906535625=Hermione%20Warr%20and%20Yasin%20Ibrahim%20and%20Daniel%20R.%20McGowan%20and%20Konstantinos%20Kamnitsas&entry.1292438233=%20%20Automation%20of%20medical%20image%20interpretation%20could%20alleviate%20bottlenecks%20in%0Adiagnostic%20workflows%2C%20and%20has%20become%20of%20particular%20interest%20in%20recent%20years%20due%0Ato%20advancements%20in%20natural%20language%20processing.%20Great%20strides%20have%20been%20made%0Atowards%20automated%20radiology%20report%20generation%20via%20AI%2C%20yet%20ensuring%20clinical%0Aaccuracy%20in%20generated%20reports%20is%20a%20significant%20challenge%2C%20hindering%20deployment%0Aof%20such%20methods%20in%20clinical%20practice.%20In%20this%20work%20we%20propose%20a%20quality%20control%0Aframework%20for%20assessing%20the%20reliability%20of%20AI-generated%20radiology%20reports%20with%0Arespect%20to%20semantics%20of%20diagnostic%20importance%20using%20modular%20auxiliary%20auditing%0Acomponents%20%28AC%29.%20Evaluating%20our%20pipeline%20on%20the%20MIMIC-CXR%20dataset%2C%20our%20findings%0Ashow%20that%20incorporating%20ACs%20in%20the%20form%20of%20disease-classifiers%20can%20enable%0Aauditing%20that%20identifies%20more%20reliable%20reports%2C%20resulting%20in%20higher%20F1%20scores%0Acompared%20to%20unfiltered%20generated%20reports.%20Additionally%2C%20leveraging%20the%0Aconfidence%20of%20the%20AC%20labels%20further%20improves%20the%20audit%27s%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21638v1&entry.124074799=Read"},
{"title": "Diversifying AI: Towards Creative Chess with AlphaZero", "author": "Tom Zahavy and Vivek Veeriah and Shaobo Hou and Kevin Waugh and Matthew Lai and Edouard Leurent and Nenad Tomasev and Lisa Schut and Demis Hassabis and Satinder Singh", "abstract": "  In recent years, Artificial Intelligence (AI) systems have surpassed human\nintelligence in a variety of computational tasks. However, AI systems, like\nhumans, make mistakes, have blind spots, hallucinate, and struggle to\ngeneralize to new situations. This work explores whether AI can benefit from\ncreative decision-making mechanisms when pushed to the limits of its\ncomputational rationality. In particular, we investigate whether a team of\ndiverse AI systems can outperform a single AI in challenging tasks by\ngenerating more ideas as a group and then selecting the best ones. We study\nthis question in the game of chess, the so-called drosophila of AI. We build on\nAlphaZero (AZ) and extend it to represent a league of agents via a\nlatent-conditioned architecture, which we call AZ_db. We train AZ_db to\ngenerate a wider range of ideas using behavioral diversity techniques and\nselect the most promising ones with sub-additive planning. Our experiments\nsuggest that AZ_db plays chess in diverse ways, solves more puzzles as a group\nand outperforms a more homogeneous team. Notably, AZ_db solves twice as many\nchallenging puzzles as AZ, including the challenging Penrose positions. When\nplaying chess from different openings, we notice that players in AZ_db\nspecialize in different openings, and that selecting a player for each opening\nusing sub-additive planning results in a 50 Elo improvement over AZ. Our\nfindings suggest that diversity bonuses emerge in teams of AI agents, just as\nthey do in teams of humans and that diversity is a valuable asset in solving\ncomputationally hard problems.\n", "link": "http://arxiv.org/abs/2308.09175v3", "date": "2024-07-31", "relevancy": 1.834, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5056}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4628}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diversifying%20AI%3A%20Towards%20Creative%20Chess%20with%20AlphaZero&body=Title%3A%20Diversifying%20AI%3A%20Towards%20Creative%20Chess%20with%20AlphaZero%0AAuthor%3A%20Tom%20Zahavy%20and%20Vivek%20Veeriah%20and%20Shaobo%20Hou%20and%20Kevin%20Waugh%20and%20Matthew%20Lai%20and%20Edouard%20Leurent%20and%20Nenad%20Tomasev%20and%20Lisa%20Schut%20and%20Demis%20Hassabis%20and%20Satinder%20Singh%0AAbstract%3A%20%20%20In%20recent%20years%2C%20Artificial%20Intelligence%20%28AI%29%20systems%20have%20surpassed%20human%0Aintelligence%20in%20a%20variety%20of%20computational%20tasks.%20However%2C%20AI%20systems%2C%20like%0Ahumans%2C%20make%20mistakes%2C%20have%20blind%20spots%2C%20hallucinate%2C%20and%20struggle%20to%0Ageneralize%20to%20new%20situations.%20This%20work%20explores%20whether%20AI%20can%20benefit%20from%0Acreative%20decision-making%20mechanisms%20when%20pushed%20to%20the%20limits%20of%20its%0Acomputational%20rationality.%20In%20particular%2C%20we%20investigate%20whether%20a%20team%20of%0Adiverse%20AI%20systems%20can%20outperform%20a%20single%20AI%20in%20challenging%20tasks%20by%0Agenerating%20more%20ideas%20as%20a%20group%20and%20then%20selecting%20the%20best%20ones.%20We%20study%0Athis%20question%20in%20the%20game%20of%20chess%2C%20the%20so-called%20drosophila%20of%20AI.%20We%20build%20on%0AAlphaZero%20%28AZ%29%20and%20extend%20it%20to%20represent%20a%20league%20of%20agents%20via%20a%0Alatent-conditioned%20architecture%2C%20which%20we%20call%20AZ_db.%20We%20train%20AZ_db%20to%0Agenerate%20a%20wider%20range%20of%20ideas%20using%20behavioral%20diversity%20techniques%20and%0Aselect%20the%20most%20promising%20ones%20with%20sub-additive%20planning.%20Our%20experiments%0Asuggest%20that%20AZ_db%20plays%20chess%20in%20diverse%20ways%2C%20solves%20more%20puzzles%20as%20a%20group%0Aand%20outperforms%20a%20more%20homogeneous%20team.%20Notably%2C%20AZ_db%20solves%20twice%20as%20many%0Achallenging%20puzzles%20as%20AZ%2C%20including%20the%20challenging%20Penrose%20positions.%20When%0Aplaying%20chess%20from%20different%20openings%2C%20we%20notice%20that%20players%20in%20AZ_db%0Aspecialize%20in%20different%20openings%2C%20and%20that%20selecting%20a%20player%20for%20each%20opening%0Ausing%20sub-additive%20planning%20results%20in%20a%2050%20Elo%20improvement%20over%20AZ.%20Our%0Afindings%20suggest%20that%20diversity%20bonuses%20emerge%20in%20teams%20of%20AI%20agents%2C%20just%20as%0Athey%20do%20in%20teams%20of%20humans%20and%20that%20diversity%20is%20a%20valuable%20asset%20in%20solving%0Acomputationally%20hard%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.09175v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiversifying%2520AI%253A%2520Towards%2520Creative%2520Chess%2520with%2520AlphaZero%26entry.906535625%3DTom%2520Zahavy%2520and%2520Vivek%2520Veeriah%2520and%2520Shaobo%2520Hou%2520and%2520Kevin%2520Waugh%2520and%2520Matthew%2520Lai%2520and%2520Edouard%2520Leurent%2520and%2520Nenad%2520Tomasev%2520and%2520Lisa%2520Schut%2520and%2520Demis%2520Hassabis%2520and%2520Satinder%2520Singh%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520Artificial%2520Intelligence%2520%2528AI%2529%2520systems%2520have%2520surpassed%2520human%250Aintelligence%2520in%2520a%2520variety%2520of%2520computational%2520tasks.%2520However%252C%2520AI%2520systems%252C%2520like%250Ahumans%252C%2520make%2520mistakes%252C%2520have%2520blind%2520spots%252C%2520hallucinate%252C%2520and%2520struggle%2520to%250Ageneralize%2520to%2520new%2520situations.%2520This%2520work%2520explores%2520whether%2520AI%2520can%2520benefit%2520from%250Acreative%2520decision-making%2520mechanisms%2520when%2520pushed%2520to%2520the%2520limits%2520of%2520its%250Acomputational%2520rationality.%2520In%2520particular%252C%2520we%2520investigate%2520whether%2520a%2520team%2520of%250Adiverse%2520AI%2520systems%2520can%2520outperform%2520a%2520single%2520AI%2520in%2520challenging%2520tasks%2520by%250Agenerating%2520more%2520ideas%2520as%2520a%2520group%2520and%2520then%2520selecting%2520the%2520best%2520ones.%2520We%2520study%250Athis%2520question%2520in%2520the%2520game%2520of%2520chess%252C%2520the%2520so-called%2520drosophila%2520of%2520AI.%2520We%2520build%2520on%250AAlphaZero%2520%2528AZ%2529%2520and%2520extend%2520it%2520to%2520represent%2520a%2520league%2520of%2520agents%2520via%2520a%250Alatent-conditioned%2520architecture%252C%2520which%2520we%2520call%2520AZ_db.%2520We%2520train%2520AZ_db%2520to%250Agenerate%2520a%2520wider%2520range%2520of%2520ideas%2520using%2520behavioral%2520diversity%2520techniques%2520and%250Aselect%2520the%2520most%2520promising%2520ones%2520with%2520sub-additive%2520planning.%2520Our%2520experiments%250Asuggest%2520that%2520AZ_db%2520plays%2520chess%2520in%2520diverse%2520ways%252C%2520solves%2520more%2520puzzles%2520as%2520a%2520group%250Aand%2520outperforms%2520a%2520more%2520homogeneous%2520team.%2520Notably%252C%2520AZ_db%2520solves%2520twice%2520as%2520many%250Achallenging%2520puzzles%2520as%2520AZ%252C%2520including%2520the%2520challenging%2520Penrose%2520positions.%2520When%250Aplaying%2520chess%2520from%2520different%2520openings%252C%2520we%2520notice%2520that%2520players%2520in%2520AZ_db%250Aspecialize%2520in%2520different%2520openings%252C%2520and%2520that%2520selecting%2520a%2520player%2520for%2520each%2520opening%250Ausing%2520sub-additive%2520planning%2520results%2520in%2520a%252050%2520Elo%2520improvement%2520over%2520AZ.%2520Our%250Afindings%2520suggest%2520that%2520diversity%2520bonuses%2520emerge%2520in%2520teams%2520of%2520AI%2520agents%252C%2520just%2520as%250Athey%2520do%2520in%2520teams%2520of%2520humans%2520and%2520that%2520diversity%2520is%2520a%2520valuable%2520asset%2520in%2520solving%250Acomputationally%2520hard%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.09175v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diversifying%20AI%3A%20Towards%20Creative%20Chess%20with%20AlphaZero&entry.906535625=Tom%20Zahavy%20and%20Vivek%20Veeriah%20and%20Shaobo%20Hou%20and%20Kevin%20Waugh%20and%20Matthew%20Lai%20and%20Edouard%20Leurent%20and%20Nenad%20Tomasev%20and%20Lisa%20Schut%20and%20Demis%20Hassabis%20and%20Satinder%20Singh&entry.1292438233=%20%20In%20recent%20years%2C%20Artificial%20Intelligence%20%28AI%29%20systems%20have%20surpassed%20human%0Aintelligence%20in%20a%20variety%20of%20computational%20tasks.%20However%2C%20AI%20systems%2C%20like%0Ahumans%2C%20make%20mistakes%2C%20have%20blind%20spots%2C%20hallucinate%2C%20and%20struggle%20to%0Ageneralize%20to%20new%20situations.%20This%20work%20explores%20whether%20AI%20can%20benefit%20from%0Acreative%20decision-making%20mechanisms%20when%20pushed%20to%20the%20limits%20of%20its%0Acomputational%20rationality.%20In%20particular%2C%20we%20investigate%20whether%20a%20team%20of%0Adiverse%20AI%20systems%20can%20outperform%20a%20single%20AI%20in%20challenging%20tasks%20by%0Agenerating%20more%20ideas%20as%20a%20group%20and%20then%20selecting%20the%20best%20ones.%20We%20study%0Athis%20question%20in%20the%20game%20of%20chess%2C%20the%20so-called%20drosophila%20of%20AI.%20We%20build%20on%0AAlphaZero%20%28AZ%29%20and%20extend%20it%20to%20represent%20a%20league%20of%20agents%20via%20a%0Alatent-conditioned%20architecture%2C%20which%20we%20call%20AZ_db.%20We%20train%20AZ_db%20to%0Agenerate%20a%20wider%20range%20of%20ideas%20using%20behavioral%20diversity%20techniques%20and%0Aselect%20the%20most%20promising%20ones%20with%20sub-additive%20planning.%20Our%20experiments%0Asuggest%20that%20AZ_db%20plays%20chess%20in%20diverse%20ways%2C%20solves%20more%20puzzles%20as%20a%20group%0Aand%20outperforms%20a%20more%20homogeneous%20team.%20Notably%2C%20AZ_db%20solves%20twice%20as%20many%0Achallenging%20puzzles%20as%20AZ%2C%20including%20the%20challenging%20Penrose%20positions.%20When%0Aplaying%20chess%20from%20different%20openings%2C%20we%20notice%20that%20players%20in%20AZ_db%0Aspecialize%20in%20different%20openings%2C%20and%20that%20selecting%20a%20player%20for%20each%20opening%0Ausing%20sub-additive%20planning%20results%20in%20a%2050%20Elo%20improvement%20over%20AZ.%20Our%0Afindings%20suggest%20that%20diversity%20bonuses%20emerge%20in%20teams%20of%20AI%20agents%2C%20just%20as%0Athey%20do%20in%20teams%20of%20humans%20and%20that%20diversity%20is%20a%20valuable%20asset%20in%20solving%0Acomputationally%20hard%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.09175v3&entry.124074799=Read"},
{"title": "MicroMIL: Graph-based Contextual Multiple Instance Learning for Patient\n  Diagnosis Using Microscopy Images", "author": "JongWoo Kim and Bryan Wong and YoungSin Ko and MunYong Yi", "abstract": "  Current histopathology research has primarily focused on using whole-slide\nimages (WSIs) produced by scanners with weakly-supervised multiple instance\nlearning (MIL). However, WSIs are costly, memory-intensive, and require\nextensive analysis time. As an alternative, microscopy-based analysis offers\ncost and memory efficiency, though microscopy images face issues with unknown\nabsolute positions and redundant images due to multiple captures from the\nsubjective perspectives of pathologists. To this end, we introduce MicroMIL, a\nweakly-supervised MIL framework specifically built to address these challenges\nby dynamically clustering images using deep cluster embedding (DCE) and Gumbel\nSoftmax for representative image extraction. Graph edges are then constructed\nfrom the upper triangular similarity matrix, with nodes connected to their most\nsimilar neighbors, and a graph neural network (GNN) is utilized to capture\nlocal and diverse areas of contextual information. Unlike existing graph-based\nMIL methods designed for WSIs that require absolute positions, MicroMIL\nefficiently handles the graph edges without this need. Extensive evaluations on\nreal-world colon cancer (Seegene) and public BreakHis datasets demonstrate that\nMicroMIL outperforms state-of-the-art (SOTA) methods, offering a robust and\nefficient solution for patient diagnosis using microscopy images. The code is\navailable at https://anonymous.4open.science/r/MicroMIL-6C7C\n", "link": "http://arxiv.org/abs/2407.21604v1", "date": "2024-07-31", "relevancy": 1.8335, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4606}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4568}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MicroMIL%3A%20Graph-based%20Contextual%20Multiple%20Instance%20Learning%20for%20Patient%0A%20%20Diagnosis%20Using%20Microscopy%20Images&body=Title%3A%20MicroMIL%3A%20Graph-based%20Contextual%20Multiple%20Instance%20Learning%20for%20Patient%0A%20%20Diagnosis%20Using%20Microscopy%20Images%0AAuthor%3A%20JongWoo%20Kim%20and%20Bryan%20Wong%20and%20YoungSin%20Ko%20and%20MunYong%20Yi%0AAbstract%3A%20%20%20Current%20histopathology%20research%20has%20primarily%20focused%20on%20using%20whole-slide%0Aimages%20%28WSIs%29%20produced%20by%20scanners%20with%20weakly-supervised%20multiple%20instance%0Alearning%20%28MIL%29.%20However%2C%20WSIs%20are%20costly%2C%20memory-intensive%2C%20and%20require%0Aextensive%20analysis%20time.%20As%20an%20alternative%2C%20microscopy-based%20analysis%20offers%0Acost%20and%20memory%20efficiency%2C%20though%20microscopy%20images%20face%20issues%20with%20unknown%0Aabsolute%20positions%20and%20redundant%20images%20due%20to%20multiple%20captures%20from%20the%0Asubjective%20perspectives%20of%20pathologists.%20To%20this%20end%2C%20we%20introduce%20MicroMIL%2C%20a%0Aweakly-supervised%20MIL%20framework%20specifically%20built%20to%20address%20these%20challenges%0Aby%20dynamically%20clustering%20images%20using%20deep%20cluster%20embedding%20%28DCE%29%20and%20Gumbel%0ASoftmax%20for%20representative%20image%20extraction.%20Graph%20edges%20are%20then%20constructed%0Afrom%20the%20upper%20triangular%20similarity%20matrix%2C%20with%20nodes%20connected%20to%20their%20most%0Asimilar%20neighbors%2C%20and%20a%20graph%20neural%20network%20%28GNN%29%20is%20utilized%20to%20capture%0Alocal%20and%20diverse%20areas%20of%20contextual%20information.%20Unlike%20existing%20graph-based%0AMIL%20methods%20designed%20for%20WSIs%20that%20require%20absolute%20positions%2C%20MicroMIL%0Aefficiently%20handles%20the%20graph%20edges%20without%20this%20need.%20Extensive%20evaluations%20on%0Areal-world%20colon%20cancer%20%28Seegene%29%20and%20public%20BreakHis%20datasets%20demonstrate%20that%0AMicroMIL%20outperforms%20state-of-the-art%20%28SOTA%29%20methods%2C%20offering%20a%20robust%20and%0Aefficient%20solution%20for%20patient%20diagnosis%20using%20microscopy%20images.%20The%20code%20is%0Aavailable%20at%20https%3A//anonymous.4open.science/r/MicroMIL-6C7C%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21604v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMicroMIL%253A%2520Graph-based%2520Contextual%2520Multiple%2520Instance%2520Learning%2520for%2520Patient%250A%2520%2520Diagnosis%2520Using%2520Microscopy%2520Images%26entry.906535625%3DJongWoo%2520Kim%2520and%2520Bryan%2520Wong%2520and%2520YoungSin%2520Ko%2520and%2520MunYong%2520Yi%26entry.1292438233%3D%2520%2520Current%2520histopathology%2520research%2520has%2520primarily%2520focused%2520on%2520using%2520whole-slide%250Aimages%2520%2528WSIs%2529%2520produced%2520by%2520scanners%2520with%2520weakly-supervised%2520multiple%2520instance%250Alearning%2520%2528MIL%2529.%2520However%252C%2520WSIs%2520are%2520costly%252C%2520memory-intensive%252C%2520and%2520require%250Aextensive%2520analysis%2520time.%2520As%2520an%2520alternative%252C%2520microscopy-based%2520analysis%2520offers%250Acost%2520and%2520memory%2520efficiency%252C%2520though%2520microscopy%2520images%2520face%2520issues%2520with%2520unknown%250Aabsolute%2520positions%2520and%2520redundant%2520images%2520due%2520to%2520multiple%2520captures%2520from%2520the%250Asubjective%2520perspectives%2520of%2520pathologists.%2520To%2520this%2520end%252C%2520we%2520introduce%2520MicroMIL%252C%2520a%250Aweakly-supervised%2520MIL%2520framework%2520specifically%2520built%2520to%2520address%2520these%2520challenges%250Aby%2520dynamically%2520clustering%2520images%2520using%2520deep%2520cluster%2520embedding%2520%2528DCE%2529%2520and%2520Gumbel%250ASoftmax%2520for%2520representative%2520image%2520extraction.%2520Graph%2520edges%2520are%2520then%2520constructed%250Afrom%2520the%2520upper%2520triangular%2520similarity%2520matrix%252C%2520with%2520nodes%2520connected%2520to%2520their%2520most%250Asimilar%2520neighbors%252C%2520and%2520a%2520graph%2520neural%2520network%2520%2528GNN%2529%2520is%2520utilized%2520to%2520capture%250Alocal%2520and%2520diverse%2520areas%2520of%2520contextual%2520information.%2520Unlike%2520existing%2520graph-based%250AMIL%2520methods%2520designed%2520for%2520WSIs%2520that%2520require%2520absolute%2520positions%252C%2520MicroMIL%250Aefficiently%2520handles%2520the%2520graph%2520edges%2520without%2520this%2520need.%2520Extensive%2520evaluations%2520on%250Areal-world%2520colon%2520cancer%2520%2528Seegene%2529%2520and%2520public%2520BreakHis%2520datasets%2520demonstrate%2520that%250AMicroMIL%2520outperforms%2520state-of-the-art%2520%2528SOTA%2529%2520methods%252C%2520offering%2520a%2520robust%2520and%250Aefficient%2520solution%2520for%2520patient%2520diagnosis%2520using%2520microscopy%2520images.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//anonymous.4open.science/r/MicroMIL-6C7C%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21604v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MicroMIL%3A%20Graph-based%20Contextual%20Multiple%20Instance%20Learning%20for%20Patient%0A%20%20Diagnosis%20Using%20Microscopy%20Images&entry.906535625=JongWoo%20Kim%20and%20Bryan%20Wong%20and%20YoungSin%20Ko%20and%20MunYong%20Yi&entry.1292438233=%20%20Current%20histopathology%20research%20has%20primarily%20focused%20on%20using%20whole-slide%0Aimages%20%28WSIs%29%20produced%20by%20scanners%20with%20weakly-supervised%20multiple%20instance%0Alearning%20%28MIL%29.%20However%2C%20WSIs%20are%20costly%2C%20memory-intensive%2C%20and%20require%0Aextensive%20analysis%20time.%20As%20an%20alternative%2C%20microscopy-based%20analysis%20offers%0Acost%20and%20memory%20efficiency%2C%20though%20microscopy%20images%20face%20issues%20with%20unknown%0Aabsolute%20positions%20and%20redundant%20images%20due%20to%20multiple%20captures%20from%20the%0Asubjective%20perspectives%20of%20pathologists.%20To%20this%20end%2C%20we%20introduce%20MicroMIL%2C%20a%0Aweakly-supervised%20MIL%20framework%20specifically%20built%20to%20address%20these%20challenges%0Aby%20dynamically%20clustering%20images%20using%20deep%20cluster%20embedding%20%28DCE%29%20and%20Gumbel%0ASoftmax%20for%20representative%20image%20extraction.%20Graph%20edges%20are%20then%20constructed%0Afrom%20the%20upper%20triangular%20similarity%20matrix%2C%20with%20nodes%20connected%20to%20their%20most%0Asimilar%20neighbors%2C%20and%20a%20graph%20neural%20network%20%28GNN%29%20is%20utilized%20to%20capture%0Alocal%20and%20diverse%20areas%20of%20contextual%20information.%20Unlike%20existing%20graph-based%0AMIL%20methods%20designed%20for%20WSIs%20that%20require%20absolute%20positions%2C%20MicroMIL%0Aefficiently%20handles%20the%20graph%20edges%20without%20this%20need.%20Extensive%20evaluations%20on%0Areal-world%20colon%20cancer%20%28Seegene%29%20and%20public%20BreakHis%20datasets%20demonstrate%20that%0AMicroMIL%20outperforms%20state-of-the-art%20%28SOTA%29%20methods%2C%20offering%20a%20robust%20and%0Aefficient%20solution%20for%20patient%20diagnosis%20using%20microscopy%20images.%20The%20code%20is%0Aavailable%20at%20https%3A//anonymous.4open.science/r/MicroMIL-6C7C%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21604v1&entry.124074799=Read"},
{"title": "Naeural AI OS -- Decentralized ubiquitous computing MLOps execution\n  engine", "author": "Beatrice Milik and Stefan Saraev and Cristian Bleotiu and Radu Lupaescu and Bogdan Hobeanu and Andrei Ionut Damian", "abstract": "  Over the past few years, ubiquitous, or pervasive computing has gained\npopularity as the primary approach for a wide range of applications, including\nenterprise-grade systems, consumer applications, and gaming systems. Ubiquitous\ncomputing refers to the integration of computing technologies into everyday\nobjects and environments, creating a network of interconnected devices that can\ncommunicate with each other and with humans. By using ubiquitous computing\ntechnologies, communities can become more connected and efficient, with members\nable to communicate and collaborate more easily. This enabled\ninterconnectedness and collaboration can lead to a more successful and\nsustainable community. The spread of ubiquitous computing, however, has\nemphasized the importance of automated learning and smart applications in\ngeneral. Even though there have been significant strides in Artificial\nIntelligence and Deep Learning, large scale adoption has been hesitant due to\nmounting pressure on expensive and highly complex cloud numerical-compute\ninfrastructures. Adopting, and even developing, practical machine learning\nsystems can come with prohibitive costs, not only in terms of complex\ninfrastructures but also of solid expertise in Data Science and Machine\nLearning. In this paper we present an innovative approach for low-code\ndevelopment and deployment of end-to-end AI cooperative application pipelines.\nWe address infrastructure allocation, costs, and secure job distribution in a\nfully decentralized global cooperative community based on tokenized economics.\n", "link": "http://arxiv.org/abs/2306.08708v3", "date": "2024-07-31", "relevancy": 1.812, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4628}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4561}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Naeural%20AI%20OS%20--%20Decentralized%20ubiquitous%20computing%20MLOps%20execution%0A%20%20engine&body=Title%3A%20Naeural%20AI%20OS%20--%20Decentralized%20ubiquitous%20computing%20MLOps%20execution%0A%20%20engine%0AAuthor%3A%20Beatrice%20Milik%20and%20Stefan%20Saraev%20and%20Cristian%20Bleotiu%20and%20Radu%20Lupaescu%20and%20Bogdan%20Hobeanu%20and%20Andrei%20Ionut%20Damian%0AAbstract%3A%20%20%20Over%20the%20past%20few%20years%2C%20ubiquitous%2C%20or%20pervasive%20computing%20has%20gained%0Apopularity%20as%20the%20primary%20approach%20for%20a%20wide%20range%20of%20applications%2C%20including%0Aenterprise-grade%20systems%2C%20consumer%20applications%2C%20and%20gaming%20systems.%20Ubiquitous%0Acomputing%20refers%20to%20the%20integration%20of%20computing%20technologies%20into%20everyday%0Aobjects%20and%20environments%2C%20creating%20a%20network%20of%20interconnected%20devices%20that%20can%0Acommunicate%20with%20each%20other%20and%20with%20humans.%20By%20using%20ubiquitous%20computing%0Atechnologies%2C%20communities%20can%20become%20more%20connected%20and%20efficient%2C%20with%20members%0Aable%20to%20communicate%20and%20collaborate%20more%20easily.%20This%20enabled%0Ainterconnectedness%20and%20collaboration%20can%20lead%20to%20a%20more%20successful%20and%0Asustainable%20community.%20The%20spread%20of%20ubiquitous%20computing%2C%20however%2C%20has%0Aemphasized%20the%20importance%20of%20automated%20learning%20and%20smart%20applications%20in%0Ageneral.%20Even%20though%20there%20have%20been%20significant%20strides%20in%20Artificial%0AIntelligence%20and%20Deep%20Learning%2C%20large%20scale%20adoption%20has%20been%20hesitant%20due%20to%0Amounting%20pressure%20on%20expensive%20and%20highly%20complex%20cloud%20numerical-compute%0Ainfrastructures.%20Adopting%2C%20and%20even%20developing%2C%20practical%20machine%20learning%0Asystems%20can%20come%20with%20prohibitive%20costs%2C%20not%20only%20in%20terms%20of%20complex%0Ainfrastructures%20but%20also%20of%20solid%20expertise%20in%20Data%20Science%20and%20Machine%0ALearning.%20In%20this%20paper%20we%20present%20an%20innovative%20approach%20for%20low-code%0Adevelopment%20and%20deployment%20of%20end-to-end%20AI%20cooperative%20application%20pipelines.%0AWe%20address%20infrastructure%20allocation%2C%20costs%2C%20and%20secure%20job%20distribution%20in%20a%0Afully%20decentralized%20global%20cooperative%20community%20based%20on%20tokenized%20economics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.08708v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNaeural%2520AI%2520OS%2520--%2520Decentralized%2520ubiquitous%2520computing%2520MLOps%2520execution%250A%2520%2520engine%26entry.906535625%3DBeatrice%2520Milik%2520and%2520Stefan%2520Saraev%2520and%2520Cristian%2520Bleotiu%2520and%2520Radu%2520Lupaescu%2520and%2520Bogdan%2520Hobeanu%2520and%2520Andrei%2520Ionut%2520Damian%26entry.1292438233%3D%2520%2520Over%2520the%2520past%2520few%2520years%252C%2520ubiquitous%252C%2520or%2520pervasive%2520computing%2520has%2520gained%250Apopularity%2520as%2520the%2520primary%2520approach%2520for%2520a%2520wide%2520range%2520of%2520applications%252C%2520including%250Aenterprise-grade%2520systems%252C%2520consumer%2520applications%252C%2520and%2520gaming%2520systems.%2520Ubiquitous%250Acomputing%2520refers%2520to%2520the%2520integration%2520of%2520computing%2520technologies%2520into%2520everyday%250Aobjects%2520and%2520environments%252C%2520creating%2520a%2520network%2520of%2520interconnected%2520devices%2520that%2520can%250Acommunicate%2520with%2520each%2520other%2520and%2520with%2520humans.%2520By%2520using%2520ubiquitous%2520computing%250Atechnologies%252C%2520communities%2520can%2520become%2520more%2520connected%2520and%2520efficient%252C%2520with%2520members%250Aable%2520to%2520communicate%2520and%2520collaborate%2520more%2520easily.%2520This%2520enabled%250Ainterconnectedness%2520and%2520collaboration%2520can%2520lead%2520to%2520a%2520more%2520successful%2520and%250Asustainable%2520community.%2520The%2520spread%2520of%2520ubiquitous%2520computing%252C%2520however%252C%2520has%250Aemphasized%2520the%2520importance%2520of%2520automated%2520learning%2520and%2520smart%2520applications%2520in%250Ageneral.%2520Even%2520though%2520there%2520have%2520been%2520significant%2520strides%2520in%2520Artificial%250AIntelligence%2520and%2520Deep%2520Learning%252C%2520large%2520scale%2520adoption%2520has%2520been%2520hesitant%2520due%2520to%250Amounting%2520pressure%2520on%2520expensive%2520and%2520highly%2520complex%2520cloud%2520numerical-compute%250Ainfrastructures.%2520Adopting%252C%2520and%2520even%2520developing%252C%2520practical%2520machine%2520learning%250Asystems%2520can%2520come%2520with%2520prohibitive%2520costs%252C%2520not%2520only%2520in%2520terms%2520of%2520complex%250Ainfrastructures%2520but%2520also%2520of%2520solid%2520expertise%2520in%2520Data%2520Science%2520and%2520Machine%250ALearning.%2520In%2520this%2520paper%2520we%2520present%2520an%2520innovative%2520approach%2520for%2520low-code%250Adevelopment%2520and%2520deployment%2520of%2520end-to-end%2520AI%2520cooperative%2520application%2520pipelines.%250AWe%2520address%2520infrastructure%2520allocation%252C%2520costs%252C%2520and%2520secure%2520job%2520distribution%2520in%2520a%250Afully%2520decentralized%2520global%2520cooperative%2520community%2520based%2520on%2520tokenized%2520economics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.08708v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Naeural%20AI%20OS%20--%20Decentralized%20ubiquitous%20computing%20MLOps%20execution%0A%20%20engine&entry.906535625=Beatrice%20Milik%20and%20Stefan%20Saraev%20and%20Cristian%20Bleotiu%20and%20Radu%20Lupaescu%20and%20Bogdan%20Hobeanu%20and%20Andrei%20Ionut%20Damian&entry.1292438233=%20%20Over%20the%20past%20few%20years%2C%20ubiquitous%2C%20or%20pervasive%20computing%20has%20gained%0Apopularity%20as%20the%20primary%20approach%20for%20a%20wide%20range%20of%20applications%2C%20including%0Aenterprise-grade%20systems%2C%20consumer%20applications%2C%20and%20gaming%20systems.%20Ubiquitous%0Acomputing%20refers%20to%20the%20integration%20of%20computing%20technologies%20into%20everyday%0Aobjects%20and%20environments%2C%20creating%20a%20network%20of%20interconnected%20devices%20that%20can%0Acommunicate%20with%20each%20other%20and%20with%20humans.%20By%20using%20ubiquitous%20computing%0Atechnologies%2C%20communities%20can%20become%20more%20connected%20and%20efficient%2C%20with%20members%0Aable%20to%20communicate%20and%20collaborate%20more%20easily.%20This%20enabled%0Ainterconnectedness%20and%20collaboration%20can%20lead%20to%20a%20more%20successful%20and%0Asustainable%20community.%20The%20spread%20of%20ubiquitous%20computing%2C%20however%2C%20has%0Aemphasized%20the%20importance%20of%20automated%20learning%20and%20smart%20applications%20in%0Ageneral.%20Even%20though%20there%20have%20been%20significant%20strides%20in%20Artificial%0AIntelligence%20and%20Deep%20Learning%2C%20large%20scale%20adoption%20has%20been%20hesitant%20due%20to%0Amounting%20pressure%20on%20expensive%20and%20highly%20complex%20cloud%20numerical-compute%0Ainfrastructures.%20Adopting%2C%20and%20even%20developing%2C%20practical%20machine%20learning%0Asystems%20can%20come%20with%20prohibitive%20costs%2C%20not%20only%20in%20terms%20of%20complex%0Ainfrastructures%20but%20also%20of%20solid%20expertise%20in%20Data%20Science%20and%20Machine%0ALearning.%20In%20this%20paper%20we%20present%20an%20innovative%20approach%20for%20low-code%0Adevelopment%20and%20deployment%20of%20end-to-end%20AI%20cooperative%20application%20pipelines.%0AWe%20address%20infrastructure%20allocation%2C%20costs%2C%20and%20secure%20job%20distribution%20in%20a%0Afully%20decentralized%20global%20cooperative%20community%20based%20on%20tokenized%20economics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.08708v3&entry.124074799=Read"},
{"title": "Iterative Ensemble Training with Anti-Gradient Control for Mitigating\n  Memorization in Diffusion Models", "author": "Xiao Liu and Xiaoliu Guan and Yu Wu and Jiaxu Miao", "abstract": "  Diffusion models, known for their tremendous ability to generate novel and\nhigh-quality samples, have recently raised concerns due to their data\nmemorization behavior, which poses privacy risks. Recent approaches for memory\nmitigation either only focused on the text modality problem in cross-modal\ngeneration tasks or utilized data augmentation strategies. In this paper, we\npropose a novel training framework for diffusion models from the perspective of\nvisual modality, which is more generic and fundamental for mitigating\nmemorization. To facilitate forgetting of stored information in diffusion model\nparameters, we propose an iterative ensemble training strategy by splitting the\ndata into multiple shards for training multiple models and intermittently\naggregating these model parameters. Moreover, practical analysis of losses\nillustrates that the training loss for easily memorable images tends to be\nobviously lower. Thus, we propose an anti-gradient control method to exclude\nthe sample with a lower loss value from the current mini-batch to avoid\nmemorizing. Extensive experiments and analysis on four datasets are conducted\nto illustrate the effectiveness of our method, and results show that our method\nsuccessfully reduces memory capacity while even improving the performance\nslightly. Moreover, to save the computing cost, we successfully apply our\nmethod to fine-tune the well-trained diffusion models by limited epochs,\ndemonstrating the applicability of our method. Code is available in\nhttps://github.com/liuxiao-guan/IET_AGC.\n", "link": "http://arxiv.org/abs/2407.15328v2", "date": "2024-07-31", "relevancy": 1.8065, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6127}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6109}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Iterative%20Ensemble%20Training%20with%20Anti-Gradient%20Control%20for%20Mitigating%0A%20%20Memorization%20in%20Diffusion%20Models&body=Title%3A%20Iterative%20Ensemble%20Training%20with%20Anti-Gradient%20Control%20for%20Mitigating%0A%20%20Memorization%20in%20Diffusion%20Models%0AAuthor%3A%20Xiao%20Liu%20and%20Xiaoliu%20Guan%20and%20Yu%20Wu%20and%20Jiaxu%20Miao%0AAbstract%3A%20%20%20Diffusion%20models%2C%20known%20for%20their%20tremendous%20ability%20to%20generate%20novel%20and%0Ahigh-quality%20samples%2C%20have%20recently%20raised%20concerns%20due%20to%20their%20data%0Amemorization%20behavior%2C%20which%20poses%20privacy%20risks.%20Recent%20approaches%20for%20memory%0Amitigation%20either%20only%20focused%20on%20the%20text%20modality%20problem%20in%20cross-modal%0Ageneration%20tasks%20or%20utilized%20data%20augmentation%20strategies.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20training%20framework%20for%20diffusion%20models%20from%20the%20perspective%20of%0Avisual%20modality%2C%20which%20is%20more%20generic%20and%20fundamental%20for%20mitigating%0Amemorization.%20To%20facilitate%20forgetting%20of%20stored%20information%20in%20diffusion%20model%0Aparameters%2C%20we%20propose%20an%20iterative%20ensemble%20training%20strategy%20by%20splitting%20the%0Adata%20into%20multiple%20shards%20for%20training%20multiple%20models%20and%20intermittently%0Aaggregating%20these%20model%20parameters.%20Moreover%2C%20practical%20analysis%20of%20losses%0Aillustrates%20that%20the%20training%20loss%20for%20easily%20memorable%20images%20tends%20to%20be%0Aobviously%20lower.%20Thus%2C%20we%20propose%20an%20anti-gradient%20control%20method%20to%20exclude%0Athe%20sample%20with%20a%20lower%20loss%20value%20from%20the%20current%20mini-batch%20to%20avoid%0Amemorizing.%20Extensive%20experiments%20and%20analysis%20on%20four%20datasets%20are%20conducted%0Ato%20illustrate%20the%20effectiveness%20of%20our%20method%2C%20and%20results%20show%20that%20our%20method%0Asuccessfully%20reduces%20memory%20capacity%20while%20even%20improving%20the%20performance%0Aslightly.%20Moreover%2C%20to%20save%20the%20computing%20cost%2C%20we%20successfully%20apply%20our%0Amethod%20to%20fine-tune%20the%20well-trained%20diffusion%20models%20by%20limited%20epochs%2C%0Ademonstrating%20the%20applicability%20of%20our%20method.%20Code%20is%20available%20in%0Ahttps%3A//github.com/liuxiao-guan/IET_AGC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15328v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIterative%2520Ensemble%2520Training%2520with%2520Anti-Gradient%2520Control%2520for%2520Mitigating%250A%2520%2520Memorization%2520in%2520Diffusion%2520Models%26entry.906535625%3DXiao%2520Liu%2520and%2520Xiaoliu%2520Guan%2520and%2520Yu%2520Wu%2520and%2520Jiaxu%2520Miao%26entry.1292438233%3D%2520%2520Diffusion%2520models%252C%2520known%2520for%2520their%2520tremendous%2520ability%2520to%2520generate%2520novel%2520and%250Ahigh-quality%2520samples%252C%2520have%2520recently%2520raised%2520concerns%2520due%2520to%2520their%2520data%250Amemorization%2520behavior%252C%2520which%2520poses%2520privacy%2520risks.%2520Recent%2520approaches%2520for%2520memory%250Amitigation%2520either%2520only%2520focused%2520on%2520the%2520text%2520modality%2520problem%2520in%2520cross-modal%250Ageneration%2520tasks%2520or%2520utilized%2520data%2520augmentation%2520strategies.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520novel%2520training%2520framework%2520for%2520diffusion%2520models%2520from%2520the%2520perspective%2520of%250Avisual%2520modality%252C%2520which%2520is%2520more%2520generic%2520and%2520fundamental%2520for%2520mitigating%250Amemorization.%2520To%2520facilitate%2520forgetting%2520of%2520stored%2520information%2520in%2520diffusion%2520model%250Aparameters%252C%2520we%2520propose%2520an%2520iterative%2520ensemble%2520training%2520strategy%2520by%2520splitting%2520the%250Adata%2520into%2520multiple%2520shards%2520for%2520training%2520multiple%2520models%2520and%2520intermittently%250Aaggregating%2520these%2520model%2520parameters.%2520Moreover%252C%2520practical%2520analysis%2520of%2520losses%250Aillustrates%2520that%2520the%2520training%2520loss%2520for%2520easily%2520memorable%2520images%2520tends%2520to%2520be%250Aobviously%2520lower.%2520Thus%252C%2520we%2520propose%2520an%2520anti-gradient%2520control%2520method%2520to%2520exclude%250Athe%2520sample%2520with%2520a%2520lower%2520loss%2520value%2520from%2520the%2520current%2520mini-batch%2520to%2520avoid%250Amemorizing.%2520Extensive%2520experiments%2520and%2520analysis%2520on%2520four%2520datasets%2520are%2520conducted%250Ato%2520illustrate%2520the%2520effectiveness%2520of%2520our%2520method%252C%2520and%2520results%2520show%2520that%2520our%2520method%250Asuccessfully%2520reduces%2520memory%2520capacity%2520while%2520even%2520improving%2520the%2520performance%250Aslightly.%2520Moreover%252C%2520to%2520save%2520the%2520computing%2520cost%252C%2520we%2520successfully%2520apply%2520our%250Amethod%2520to%2520fine-tune%2520the%2520well-trained%2520diffusion%2520models%2520by%2520limited%2520epochs%252C%250Ademonstrating%2520the%2520applicability%2520of%2520our%2520method.%2520Code%2520is%2520available%2520in%250Ahttps%253A//github.com/liuxiao-guan/IET_AGC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15328v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Iterative%20Ensemble%20Training%20with%20Anti-Gradient%20Control%20for%20Mitigating%0A%20%20Memorization%20in%20Diffusion%20Models&entry.906535625=Xiao%20Liu%20and%20Xiaoliu%20Guan%20and%20Yu%20Wu%20and%20Jiaxu%20Miao&entry.1292438233=%20%20Diffusion%20models%2C%20known%20for%20their%20tremendous%20ability%20to%20generate%20novel%20and%0Ahigh-quality%20samples%2C%20have%20recently%20raised%20concerns%20due%20to%20their%20data%0Amemorization%20behavior%2C%20which%20poses%20privacy%20risks.%20Recent%20approaches%20for%20memory%0Amitigation%20either%20only%20focused%20on%20the%20text%20modality%20problem%20in%20cross-modal%0Ageneration%20tasks%20or%20utilized%20data%20augmentation%20strategies.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20training%20framework%20for%20diffusion%20models%20from%20the%20perspective%20of%0Avisual%20modality%2C%20which%20is%20more%20generic%20and%20fundamental%20for%20mitigating%0Amemorization.%20To%20facilitate%20forgetting%20of%20stored%20information%20in%20diffusion%20model%0Aparameters%2C%20we%20propose%20an%20iterative%20ensemble%20training%20strategy%20by%20splitting%20the%0Adata%20into%20multiple%20shards%20for%20training%20multiple%20models%20and%20intermittently%0Aaggregating%20these%20model%20parameters.%20Moreover%2C%20practical%20analysis%20of%20losses%0Aillustrates%20that%20the%20training%20loss%20for%20easily%20memorable%20images%20tends%20to%20be%0Aobviously%20lower.%20Thus%2C%20we%20propose%20an%20anti-gradient%20control%20method%20to%20exclude%0Athe%20sample%20with%20a%20lower%20loss%20value%20from%20the%20current%20mini-batch%20to%20avoid%0Amemorizing.%20Extensive%20experiments%20and%20analysis%20on%20four%20datasets%20are%20conducted%0Ato%20illustrate%20the%20effectiveness%20of%20our%20method%2C%20and%20results%20show%20that%20our%20method%0Asuccessfully%20reduces%20memory%20capacity%20while%20even%20improving%20the%20performance%0Aslightly.%20Moreover%2C%20to%20save%20the%20computing%20cost%2C%20we%20successfully%20apply%20our%0Amethod%20to%20fine-tune%20the%20well-trained%20diffusion%20models%20by%20limited%20epochs%2C%0Ademonstrating%20the%20applicability%20of%20our%20method.%20Code%20is%20available%20in%0Ahttps%3A//github.com/liuxiao-guan/IET_AGC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15328v2&entry.124074799=Read"},
{"title": "Language-Grounded Dynamic Scene Graphs for Interactive Object Search\n  with Mobile Manipulation", "author": "Daniel Honerkamp and Martin B\u00fcchner and Fabien Despinoy and Tim Welschehold and Abhinav Valada", "abstract": "  To fully leverage the capabilities of mobile manipulation robots, it is\nimperative that they are able to autonomously execute long-horizon tasks in\nlarge unexplored environments. While large language models (LLMs) have shown\nemergent reasoning skills on arbitrary tasks, existing work primarily\nconcentrates on explored environments, typically focusing on either navigation\nor manipulation tasks in isolation. In this work, we propose MoMa-LLM, a novel\napproach that grounds language models within structured representations derived\nfrom open-vocabulary scene graphs, dynamically updated as the environment is\nexplored. We tightly interleave these representations with an object-centric\naction space. Given object detections, the resulting approach is zero-shot,\nopen-vocabulary, and readily extendable to a spectrum of mobile manipulation\nand household robotic tasks. We demonstrate the effectiveness of MoMa-LLM in a\nnovel semantic interactive search task in large realistic indoor environments.\nIn extensive experiments in both simulation and the real world, we show\nsubstantially improved search efficiency compared to conventional baselines and\nstate-of-the-art approaches, as well as its applicability to more abstract\ntasks. We make the code publicly available at\nhttp://moma-llm.cs.uni-freiburg.de.\n", "link": "http://arxiv.org/abs/2403.08605v4", "date": "2024-07-31", "relevancy": 1.7917, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.7405}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6016}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language-Grounded%20Dynamic%20Scene%20Graphs%20for%20Interactive%20Object%20Search%0A%20%20with%20Mobile%20Manipulation&body=Title%3A%20Language-Grounded%20Dynamic%20Scene%20Graphs%20for%20Interactive%20Object%20Search%0A%20%20with%20Mobile%20Manipulation%0AAuthor%3A%20Daniel%20Honerkamp%20and%20Martin%20B%C3%BCchner%20and%20Fabien%20Despinoy%20and%20Tim%20Welschehold%20and%20Abhinav%20Valada%0AAbstract%3A%20%20%20To%20fully%20leverage%20the%20capabilities%20of%20mobile%20manipulation%20robots%2C%20it%20is%0Aimperative%20that%20they%20are%20able%20to%20autonomously%20execute%20long-horizon%20tasks%20in%0Alarge%20unexplored%20environments.%20While%20large%20language%20models%20%28LLMs%29%20have%20shown%0Aemergent%20reasoning%20skills%20on%20arbitrary%20tasks%2C%20existing%20work%20primarily%0Aconcentrates%20on%20explored%20environments%2C%20typically%20focusing%20on%20either%20navigation%0Aor%20manipulation%20tasks%20in%20isolation.%20In%20this%20work%2C%20we%20propose%20MoMa-LLM%2C%20a%20novel%0Aapproach%20that%20grounds%20language%20models%20within%20structured%20representations%20derived%0Afrom%20open-vocabulary%20scene%20graphs%2C%20dynamically%20updated%20as%20the%20environment%20is%0Aexplored.%20We%20tightly%20interleave%20these%20representations%20with%20an%20object-centric%0Aaction%20space.%20Given%20object%20detections%2C%20the%20resulting%20approach%20is%20zero-shot%2C%0Aopen-vocabulary%2C%20and%20readily%20extendable%20to%20a%20spectrum%20of%20mobile%20manipulation%0Aand%20household%20robotic%20tasks.%20We%20demonstrate%20the%20effectiveness%20of%20MoMa-LLM%20in%20a%0Anovel%20semantic%20interactive%20search%20task%20in%20large%20realistic%20indoor%20environments.%0AIn%20extensive%20experiments%20in%20both%20simulation%20and%20the%20real%20world%2C%20we%20show%0Asubstantially%20improved%20search%20efficiency%20compared%20to%20conventional%20baselines%20and%0Astate-of-the-art%20approaches%2C%20as%20well%20as%20its%20applicability%20to%20more%20abstract%0Atasks.%20We%20make%20the%20code%20publicly%20available%20at%0Ahttp%3A//moma-llm.cs.uni-freiburg.de.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08605v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage-Grounded%2520Dynamic%2520Scene%2520Graphs%2520for%2520Interactive%2520Object%2520Search%250A%2520%2520with%2520Mobile%2520Manipulation%26entry.906535625%3DDaniel%2520Honerkamp%2520and%2520Martin%2520B%25C3%25BCchner%2520and%2520Fabien%2520Despinoy%2520and%2520Tim%2520Welschehold%2520and%2520Abhinav%2520Valada%26entry.1292438233%3D%2520%2520To%2520fully%2520leverage%2520the%2520capabilities%2520of%2520mobile%2520manipulation%2520robots%252C%2520it%2520is%250Aimperative%2520that%2520they%2520are%2520able%2520to%2520autonomously%2520execute%2520long-horizon%2520tasks%2520in%250Alarge%2520unexplored%2520environments.%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%250Aemergent%2520reasoning%2520skills%2520on%2520arbitrary%2520tasks%252C%2520existing%2520work%2520primarily%250Aconcentrates%2520on%2520explored%2520environments%252C%2520typically%2520focusing%2520on%2520either%2520navigation%250Aor%2520manipulation%2520tasks%2520in%2520isolation.%2520In%2520this%2520work%252C%2520we%2520propose%2520MoMa-LLM%252C%2520a%2520novel%250Aapproach%2520that%2520grounds%2520language%2520models%2520within%2520structured%2520representations%2520derived%250Afrom%2520open-vocabulary%2520scene%2520graphs%252C%2520dynamically%2520updated%2520as%2520the%2520environment%2520is%250Aexplored.%2520We%2520tightly%2520interleave%2520these%2520representations%2520with%2520an%2520object-centric%250Aaction%2520space.%2520Given%2520object%2520detections%252C%2520the%2520resulting%2520approach%2520is%2520zero-shot%252C%250Aopen-vocabulary%252C%2520and%2520readily%2520extendable%2520to%2520a%2520spectrum%2520of%2520mobile%2520manipulation%250Aand%2520household%2520robotic%2520tasks.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520MoMa-LLM%2520in%2520a%250Anovel%2520semantic%2520interactive%2520search%2520task%2520in%2520large%2520realistic%2520indoor%2520environments.%250AIn%2520extensive%2520experiments%2520in%2520both%2520simulation%2520and%2520the%2520real%2520world%252C%2520we%2520show%250Asubstantially%2520improved%2520search%2520efficiency%2520compared%2520to%2520conventional%2520baselines%2520and%250Astate-of-the-art%2520approaches%252C%2520as%2520well%2520as%2520its%2520applicability%2520to%2520more%2520abstract%250Atasks.%2520We%2520make%2520the%2520code%2520publicly%2520available%2520at%250Ahttp%253A//moma-llm.cs.uni-freiburg.de.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.08605v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language-Grounded%20Dynamic%20Scene%20Graphs%20for%20Interactive%20Object%20Search%0A%20%20with%20Mobile%20Manipulation&entry.906535625=Daniel%20Honerkamp%20and%20Martin%20B%C3%BCchner%20and%20Fabien%20Despinoy%20and%20Tim%20Welschehold%20and%20Abhinav%20Valada&entry.1292438233=%20%20To%20fully%20leverage%20the%20capabilities%20of%20mobile%20manipulation%20robots%2C%20it%20is%0Aimperative%20that%20they%20are%20able%20to%20autonomously%20execute%20long-horizon%20tasks%20in%0Alarge%20unexplored%20environments.%20While%20large%20language%20models%20%28LLMs%29%20have%20shown%0Aemergent%20reasoning%20skills%20on%20arbitrary%20tasks%2C%20existing%20work%20primarily%0Aconcentrates%20on%20explored%20environments%2C%20typically%20focusing%20on%20either%20navigation%0Aor%20manipulation%20tasks%20in%20isolation.%20In%20this%20work%2C%20we%20propose%20MoMa-LLM%2C%20a%20novel%0Aapproach%20that%20grounds%20language%20models%20within%20structured%20representations%20derived%0Afrom%20open-vocabulary%20scene%20graphs%2C%20dynamically%20updated%20as%20the%20environment%20is%0Aexplored.%20We%20tightly%20interleave%20these%20representations%20with%20an%20object-centric%0Aaction%20space.%20Given%20object%20detections%2C%20the%20resulting%20approach%20is%20zero-shot%2C%0Aopen-vocabulary%2C%20and%20readily%20extendable%20to%20a%20spectrum%20of%20mobile%20manipulation%0Aand%20household%20robotic%20tasks.%20We%20demonstrate%20the%20effectiveness%20of%20MoMa-LLM%20in%20a%0Anovel%20semantic%20interactive%20search%20task%20in%20large%20realistic%20indoor%20environments.%0AIn%20extensive%20experiments%20in%20both%20simulation%20and%20the%20real%20world%2C%20we%20show%0Asubstantially%20improved%20search%20efficiency%20compared%20to%20conventional%20baselines%20and%0Astate-of-the-art%20approaches%2C%20as%20well%20as%20its%20applicability%20to%20more%20abstract%0Atasks.%20We%20make%20the%20code%20publicly%20available%20at%0Ahttp%3A//moma-llm.cs.uni-freiburg.de.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08605v4&entry.124074799=Read"},
{"title": "Between the AI and Me: Analysing Listeners' Perspectives on AI- and\n  Human-Composed Progressive Metal Music", "author": "Pedro Sarmento and Jackson Loth and Mathieu Barthet", "abstract": "  Generative AI models have recently blossomed, significantly impacting\nartistic and musical traditions. Research investigating how humans interact\nwith and deem these models is therefore crucial. Through a listening and\nreflection study, we explore participants' perspectives on AI- vs\nhuman-generated progressive metal, in symbolic format, using rock music as a\ncontrol group. AI-generated examples were produced by ProgGP, a\nTransformer-based model. We propose a mixed methods approach to assess the\neffects of generation type (human vs. AI), genre (progressive metal vs. rock),\nand curation process (random vs. cherry-picked). This combines quantitative\nfeedback on genre congruence, preference, creativity, consistency, playability,\nhumanness, and repeatability, and qualitative feedback to provide insights into\nlisteners' experiences. A total of 32 progressive metal fans completed the\nstudy. Our findings validate the use of fine-tuning to achieve genre-specific\nspecialization in AI music generation, as listeners could distinguish between\nAI-generated rock and progressive metal. Despite some AI-generated excerpts\nreceiving similar ratings to human music, listeners exhibited a preference for\nhuman compositions. Thematic analysis identified key features for genre and AI\nvs. human distinctions. Finally, we consider the ethical implications of our\nwork in promoting musical data diversity within MIR research by focusing on an\nunder-explored genre.\n", "link": "http://arxiv.org/abs/2407.21615v1", "date": "2024-07-31", "relevancy": 1.7772, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4591}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4342}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4326}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Between%20the%20AI%20and%20Me%3A%20Analysing%20Listeners%27%20Perspectives%20on%20AI-%20and%0A%20%20Human-Composed%20Progressive%20Metal%20Music&body=Title%3A%20Between%20the%20AI%20and%20Me%3A%20Analysing%20Listeners%27%20Perspectives%20on%20AI-%20and%0A%20%20Human-Composed%20Progressive%20Metal%20Music%0AAuthor%3A%20Pedro%20Sarmento%20and%20Jackson%20Loth%20and%20Mathieu%20Barthet%0AAbstract%3A%20%20%20Generative%20AI%20models%20have%20recently%20blossomed%2C%20significantly%20impacting%0Aartistic%20and%20musical%20traditions.%20Research%20investigating%20how%20humans%20interact%0Awith%20and%20deem%20these%20models%20is%20therefore%20crucial.%20Through%20a%20listening%20and%0Areflection%20study%2C%20we%20explore%20participants%27%20perspectives%20on%20AI-%20vs%0Ahuman-generated%20progressive%20metal%2C%20in%20symbolic%20format%2C%20using%20rock%20music%20as%20a%0Acontrol%20group.%20AI-generated%20examples%20were%20produced%20by%20ProgGP%2C%20a%0ATransformer-based%20model.%20We%20propose%20a%20mixed%20methods%20approach%20to%20assess%20the%0Aeffects%20of%20generation%20type%20%28human%20vs.%20AI%29%2C%20genre%20%28progressive%20metal%20vs.%20rock%29%2C%0Aand%20curation%20process%20%28random%20vs.%20cherry-picked%29.%20This%20combines%20quantitative%0Afeedback%20on%20genre%20congruence%2C%20preference%2C%20creativity%2C%20consistency%2C%20playability%2C%0Ahumanness%2C%20and%20repeatability%2C%20and%20qualitative%20feedback%20to%20provide%20insights%20into%0Alisteners%27%20experiences.%20A%20total%20of%2032%20progressive%20metal%20fans%20completed%20the%0Astudy.%20Our%20findings%20validate%20the%20use%20of%20fine-tuning%20to%20achieve%20genre-specific%0Aspecialization%20in%20AI%20music%20generation%2C%20as%20listeners%20could%20distinguish%20between%0AAI-generated%20rock%20and%20progressive%20metal.%20Despite%20some%20AI-generated%20excerpts%0Areceiving%20similar%20ratings%20to%20human%20music%2C%20listeners%20exhibited%20a%20preference%20for%0Ahuman%20compositions.%20Thematic%20analysis%20identified%20key%20features%20for%20genre%20and%20AI%0Avs.%20human%20distinctions.%20Finally%2C%20we%20consider%20the%20ethical%20implications%20of%20our%0Awork%20in%20promoting%20musical%20data%20diversity%20within%20MIR%20research%20by%20focusing%20on%20an%0Aunder-explored%20genre.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21615v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBetween%2520the%2520AI%2520and%2520Me%253A%2520Analysing%2520Listeners%2527%2520Perspectives%2520on%2520AI-%2520and%250A%2520%2520Human-Composed%2520Progressive%2520Metal%2520Music%26entry.906535625%3DPedro%2520Sarmento%2520and%2520Jackson%2520Loth%2520and%2520Mathieu%2520Barthet%26entry.1292438233%3D%2520%2520Generative%2520AI%2520models%2520have%2520recently%2520blossomed%252C%2520significantly%2520impacting%250Aartistic%2520and%2520musical%2520traditions.%2520Research%2520investigating%2520how%2520humans%2520interact%250Awith%2520and%2520deem%2520these%2520models%2520is%2520therefore%2520crucial.%2520Through%2520a%2520listening%2520and%250Areflection%2520study%252C%2520we%2520explore%2520participants%2527%2520perspectives%2520on%2520AI-%2520vs%250Ahuman-generated%2520progressive%2520metal%252C%2520in%2520symbolic%2520format%252C%2520using%2520rock%2520music%2520as%2520a%250Acontrol%2520group.%2520AI-generated%2520examples%2520were%2520produced%2520by%2520ProgGP%252C%2520a%250ATransformer-based%2520model.%2520We%2520propose%2520a%2520mixed%2520methods%2520approach%2520to%2520assess%2520the%250Aeffects%2520of%2520generation%2520type%2520%2528human%2520vs.%2520AI%2529%252C%2520genre%2520%2528progressive%2520metal%2520vs.%2520rock%2529%252C%250Aand%2520curation%2520process%2520%2528random%2520vs.%2520cherry-picked%2529.%2520This%2520combines%2520quantitative%250Afeedback%2520on%2520genre%2520congruence%252C%2520preference%252C%2520creativity%252C%2520consistency%252C%2520playability%252C%250Ahumanness%252C%2520and%2520repeatability%252C%2520and%2520qualitative%2520feedback%2520to%2520provide%2520insights%2520into%250Alisteners%2527%2520experiences.%2520A%2520total%2520of%252032%2520progressive%2520metal%2520fans%2520completed%2520the%250Astudy.%2520Our%2520findings%2520validate%2520the%2520use%2520of%2520fine-tuning%2520to%2520achieve%2520genre-specific%250Aspecialization%2520in%2520AI%2520music%2520generation%252C%2520as%2520listeners%2520could%2520distinguish%2520between%250AAI-generated%2520rock%2520and%2520progressive%2520metal.%2520Despite%2520some%2520AI-generated%2520excerpts%250Areceiving%2520similar%2520ratings%2520to%2520human%2520music%252C%2520listeners%2520exhibited%2520a%2520preference%2520for%250Ahuman%2520compositions.%2520Thematic%2520analysis%2520identified%2520key%2520features%2520for%2520genre%2520and%2520AI%250Avs.%2520human%2520distinctions.%2520Finally%252C%2520we%2520consider%2520the%2520ethical%2520implications%2520of%2520our%250Awork%2520in%2520promoting%2520musical%2520data%2520diversity%2520within%2520MIR%2520research%2520by%2520focusing%2520on%2520an%250Aunder-explored%2520genre.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21615v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Between%20the%20AI%20and%20Me%3A%20Analysing%20Listeners%27%20Perspectives%20on%20AI-%20and%0A%20%20Human-Composed%20Progressive%20Metal%20Music&entry.906535625=Pedro%20Sarmento%20and%20Jackson%20Loth%20and%20Mathieu%20Barthet&entry.1292438233=%20%20Generative%20AI%20models%20have%20recently%20blossomed%2C%20significantly%20impacting%0Aartistic%20and%20musical%20traditions.%20Research%20investigating%20how%20humans%20interact%0Awith%20and%20deem%20these%20models%20is%20therefore%20crucial.%20Through%20a%20listening%20and%0Areflection%20study%2C%20we%20explore%20participants%27%20perspectives%20on%20AI-%20vs%0Ahuman-generated%20progressive%20metal%2C%20in%20symbolic%20format%2C%20using%20rock%20music%20as%20a%0Acontrol%20group.%20AI-generated%20examples%20were%20produced%20by%20ProgGP%2C%20a%0ATransformer-based%20model.%20We%20propose%20a%20mixed%20methods%20approach%20to%20assess%20the%0Aeffects%20of%20generation%20type%20%28human%20vs.%20AI%29%2C%20genre%20%28progressive%20metal%20vs.%20rock%29%2C%0Aand%20curation%20process%20%28random%20vs.%20cherry-picked%29.%20This%20combines%20quantitative%0Afeedback%20on%20genre%20congruence%2C%20preference%2C%20creativity%2C%20consistency%2C%20playability%2C%0Ahumanness%2C%20and%20repeatability%2C%20and%20qualitative%20feedback%20to%20provide%20insights%20into%0Alisteners%27%20experiences.%20A%20total%20of%2032%20progressive%20metal%20fans%20completed%20the%0Astudy.%20Our%20findings%20validate%20the%20use%20of%20fine-tuning%20to%20achieve%20genre-specific%0Aspecialization%20in%20AI%20music%20generation%2C%20as%20listeners%20could%20distinguish%20between%0AAI-generated%20rock%20and%20progressive%20metal.%20Despite%20some%20AI-generated%20excerpts%0Areceiving%20similar%20ratings%20to%20human%20music%2C%20listeners%20exhibited%20a%20preference%20for%0Ahuman%20compositions.%20Thematic%20analysis%20identified%20key%20features%20for%20genre%20and%20AI%0Avs.%20human%20distinctions.%20Finally%2C%20we%20consider%20the%20ethical%20implications%20of%20our%0Awork%20in%20promoting%20musical%20data%20diversity%20within%20MIR%20research%20by%20focusing%20on%20an%0Aunder-explored%20genre.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21615v1&entry.124074799=Read"},
{"title": "Analysis of Total Variation Minimization for Clustered Federated\n  Learning", "author": "A. Jung", "abstract": "  A key challenge in federated learning applications is the statistical\nheterogeneity of local datasets. Clustered federated learning addresses this\nchallenge by identifying clusters of local datasets that are approximately\nhomogeneous. One recent approach to clustered federated learning is generalized\ntotal variation minimization (GTVMin). This approach requires a similarity\ngraph which can be obtained by domain expertise or in a data-driven fashion via\ngraph learning techniques. Under a widely applicable clustering assumption, we\nderive an upper bound the deviation between GTVMin solutions and their\ncluster-wise averages. This bound provides valuable insights into the\neffectiveness and robustness of GTVMin in addressing statistical heterogeneity\nwithin federated learning environments.\n", "link": "http://arxiv.org/abs/2403.06298v2", "date": "2024-07-31", "relevancy": 1.7433, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4417}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4388}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4305}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analysis%20of%20Total%20Variation%20Minimization%20for%20Clustered%20Federated%0A%20%20Learning&body=Title%3A%20Analysis%20of%20Total%20Variation%20Minimization%20for%20Clustered%20Federated%0A%20%20Learning%0AAuthor%3A%20A.%20Jung%0AAbstract%3A%20%20%20A%20key%20challenge%20in%20federated%20learning%20applications%20is%20the%20statistical%0Aheterogeneity%20of%20local%20datasets.%20Clustered%20federated%20learning%20addresses%20this%0Achallenge%20by%20identifying%20clusters%20of%20local%20datasets%20that%20are%20approximately%0Ahomogeneous.%20One%20recent%20approach%20to%20clustered%20federated%20learning%20is%20generalized%0Atotal%20variation%20minimization%20%28GTVMin%29.%20This%20approach%20requires%20a%20similarity%0Agraph%20which%20can%20be%20obtained%20by%20domain%20expertise%20or%20in%20a%20data-driven%20fashion%20via%0Agraph%20learning%20techniques.%20Under%20a%20widely%20applicable%20clustering%20assumption%2C%20we%0Aderive%20an%20upper%20bound%20the%20deviation%20between%20GTVMin%20solutions%20and%20their%0Acluster-wise%20averages.%20This%20bound%20provides%20valuable%20insights%20into%20the%0Aeffectiveness%20and%20robustness%20of%20GTVMin%20in%20addressing%20statistical%20heterogeneity%0Awithin%20federated%20learning%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06298v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalysis%2520of%2520Total%2520Variation%2520Minimization%2520for%2520Clustered%2520Federated%250A%2520%2520Learning%26entry.906535625%3DA.%2520Jung%26entry.1292438233%3D%2520%2520A%2520key%2520challenge%2520in%2520federated%2520learning%2520applications%2520is%2520the%2520statistical%250Aheterogeneity%2520of%2520local%2520datasets.%2520Clustered%2520federated%2520learning%2520addresses%2520this%250Achallenge%2520by%2520identifying%2520clusters%2520of%2520local%2520datasets%2520that%2520are%2520approximately%250Ahomogeneous.%2520One%2520recent%2520approach%2520to%2520clustered%2520federated%2520learning%2520is%2520generalized%250Atotal%2520variation%2520minimization%2520%2528GTVMin%2529.%2520This%2520approach%2520requires%2520a%2520similarity%250Agraph%2520which%2520can%2520be%2520obtained%2520by%2520domain%2520expertise%2520or%2520in%2520a%2520data-driven%2520fashion%2520via%250Agraph%2520learning%2520techniques.%2520Under%2520a%2520widely%2520applicable%2520clustering%2520assumption%252C%2520we%250Aderive%2520an%2520upper%2520bound%2520the%2520deviation%2520between%2520GTVMin%2520solutions%2520and%2520their%250Acluster-wise%2520averages.%2520This%2520bound%2520provides%2520valuable%2520insights%2520into%2520the%250Aeffectiveness%2520and%2520robustness%2520of%2520GTVMin%2520in%2520addressing%2520statistical%2520heterogeneity%250Awithin%2520federated%2520learning%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06298v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analysis%20of%20Total%20Variation%20Minimization%20for%20Clustered%20Federated%0A%20%20Learning&entry.906535625=A.%20Jung&entry.1292438233=%20%20A%20key%20challenge%20in%20federated%20learning%20applications%20is%20the%20statistical%0Aheterogeneity%20of%20local%20datasets.%20Clustered%20federated%20learning%20addresses%20this%0Achallenge%20by%20identifying%20clusters%20of%20local%20datasets%20that%20are%20approximately%0Ahomogeneous.%20One%20recent%20approach%20to%20clustered%20federated%20learning%20is%20generalized%0Atotal%20variation%20minimization%20%28GTVMin%29.%20This%20approach%20requires%20a%20similarity%0Agraph%20which%20can%20be%20obtained%20by%20domain%20expertise%20or%20in%20a%20data-driven%20fashion%20via%0Agraph%20learning%20techniques.%20Under%20a%20widely%20applicable%20clustering%20assumption%2C%20we%0Aderive%20an%20upper%20bound%20the%20deviation%20between%20GTVMin%20solutions%20and%20their%0Acluster-wise%20averages.%20This%20bound%20provides%20valuable%20insights%20into%20the%0Aeffectiveness%20and%20robustness%20of%20GTVMin%20in%20addressing%20statistical%20heterogeneity%0Awithin%20federated%20learning%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06298v2&entry.124074799=Read"},
{"title": "Learning Tactile Insertion in the Real World", "author": "Daniel Palenicek and Theo Gruner and Tim Schneider and Alina B\u00f6hm and Janis Lenz and Inga Pfenning and Eric Kr\u00e4mer and Jan Peters", "abstract": "  Humans have exceptional tactile sensing capabilities, which they can leverage\nto solve challenging, partially observable tasks that cannot be solved from\nvisual observation alone. Research in tactile sensing attempts to unlock this\nnew input modality for robots. Lately, these sensors have become cheaper and,\nthus, widely available. At the same time, the question of how to integrate them\ninto control loops is still an active area of research, with central challenges\nbeing partial observability and the contact-rich nature of manipulation tasks.\nIn this study, we propose to use Reinforcement Learning to learn an end-to-end\npolicy, mapping directly from tactile sensor readings to actions. Specifically,\nwe use Dreamer-v3 on a challenging, partially observable robotic insertion task\nwith a Franka Research 3, both in simulation and on a real system. For the real\nsetup, we built a robotic platform capable of resetting itself fully\nautonomously, allowing for extensive training runs without human supervision.\nOur preliminary results indicate that Dreamer is capable of utilizing tactile\ninputs to solve robotic manipulation tasks in simulation and reality.\nFurthermore, we find that providing the robot with tactile feedback generally\nimproves task performance, though, in our setup, we do not yet include other\nsensing modalities. In the future, we plan to utilize our platform to evaluate\na wide range of other Reinforcement Learning algorithms on tactile tasks.\n", "link": "http://arxiv.org/abs/2405.00383v2", "date": "2024-07-31", "relevancy": 1.7147, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6116}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6007}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Tactile%20Insertion%20in%20the%20Real%20World&body=Title%3A%20Learning%20Tactile%20Insertion%20in%20the%20Real%20World%0AAuthor%3A%20Daniel%20Palenicek%20and%20Theo%20Gruner%20and%20Tim%20Schneider%20and%20Alina%20B%C3%B6hm%20and%20Janis%20Lenz%20and%20Inga%20Pfenning%20and%20Eric%20Kr%C3%A4mer%20and%20Jan%20Peters%0AAbstract%3A%20%20%20Humans%20have%20exceptional%20tactile%20sensing%20capabilities%2C%20which%20they%20can%20leverage%0Ato%20solve%20challenging%2C%20partially%20observable%20tasks%20that%20cannot%20be%20solved%20from%0Avisual%20observation%20alone.%20Research%20in%20tactile%20sensing%20attempts%20to%20unlock%20this%0Anew%20input%20modality%20for%20robots.%20Lately%2C%20these%20sensors%20have%20become%20cheaper%20and%2C%0Athus%2C%20widely%20available.%20At%20the%20same%20time%2C%20the%20question%20of%20how%20to%20integrate%20them%0Ainto%20control%20loops%20is%20still%20an%20active%20area%20of%20research%2C%20with%20central%20challenges%0Abeing%20partial%20observability%20and%20the%20contact-rich%20nature%20of%20manipulation%20tasks.%0AIn%20this%20study%2C%20we%20propose%20to%20use%20Reinforcement%20Learning%20to%20learn%20an%20end-to-end%0Apolicy%2C%20mapping%20directly%20from%20tactile%20sensor%20readings%20to%20actions.%20Specifically%2C%0Awe%20use%20Dreamer-v3%20on%20a%20challenging%2C%20partially%20observable%20robotic%20insertion%20task%0Awith%20a%20Franka%20Research%203%2C%20both%20in%20simulation%20and%20on%20a%20real%20system.%20For%20the%20real%0Asetup%2C%20we%20built%20a%20robotic%20platform%20capable%20of%20resetting%20itself%20fully%0Aautonomously%2C%20allowing%20for%20extensive%20training%20runs%20without%20human%20supervision.%0AOur%20preliminary%20results%20indicate%20that%20Dreamer%20is%20capable%20of%20utilizing%20tactile%0Ainputs%20to%20solve%20robotic%20manipulation%20tasks%20in%20simulation%20and%20reality.%0AFurthermore%2C%20we%20find%20that%20providing%20the%20robot%20with%20tactile%20feedback%20generally%0Aimproves%20task%20performance%2C%20though%2C%20in%20our%20setup%2C%20we%20do%20not%20yet%20include%20other%0Asensing%20modalities.%20In%20the%20future%2C%20we%20plan%20to%20utilize%20our%20platform%20to%20evaluate%0Aa%20wide%20range%20of%20other%20Reinforcement%20Learning%20algorithms%20on%20tactile%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00383v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Tactile%2520Insertion%2520in%2520the%2520Real%2520World%26entry.906535625%3DDaniel%2520Palenicek%2520and%2520Theo%2520Gruner%2520and%2520Tim%2520Schneider%2520and%2520Alina%2520B%25C3%25B6hm%2520and%2520Janis%2520Lenz%2520and%2520Inga%2520Pfenning%2520and%2520Eric%2520Kr%25C3%25A4mer%2520and%2520Jan%2520Peters%26entry.1292438233%3D%2520%2520Humans%2520have%2520exceptional%2520tactile%2520sensing%2520capabilities%252C%2520which%2520they%2520can%2520leverage%250Ato%2520solve%2520challenging%252C%2520partially%2520observable%2520tasks%2520that%2520cannot%2520be%2520solved%2520from%250Avisual%2520observation%2520alone.%2520Research%2520in%2520tactile%2520sensing%2520attempts%2520to%2520unlock%2520this%250Anew%2520input%2520modality%2520for%2520robots.%2520Lately%252C%2520these%2520sensors%2520have%2520become%2520cheaper%2520and%252C%250Athus%252C%2520widely%2520available.%2520At%2520the%2520same%2520time%252C%2520the%2520question%2520of%2520how%2520to%2520integrate%2520them%250Ainto%2520control%2520loops%2520is%2520still%2520an%2520active%2520area%2520of%2520research%252C%2520with%2520central%2520challenges%250Abeing%2520partial%2520observability%2520and%2520the%2520contact-rich%2520nature%2520of%2520manipulation%2520tasks.%250AIn%2520this%2520study%252C%2520we%2520propose%2520to%2520use%2520Reinforcement%2520Learning%2520to%2520learn%2520an%2520end-to-end%250Apolicy%252C%2520mapping%2520directly%2520from%2520tactile%2520sensor%2520readings%2520to%2520actions.%2520Specifically%252C%250Awe%2520use%2520Dreamer-v3%2520on%2520a%2520challenging%252C%2520partially%2520observable%2520robotic%2520insertion%2520task%250Awith%2520a%2520Franka%2520Research%25203%252C%2520both%2520in%2520simulation%2520and%2520on%2520a%2520real%2520system.%2520For%2520the%2520real%250Asetup%252C%2520we%2520built%2520a%2520robotic%2520platform%2520capable%2520of%2520resetting%2520itself%2520fully%250Aautonomously%252C%2520allowing%2520for%2520extensive%2520training%2520runs%2520without%2520human%2520supervision.%250AOur%2520preliminary%2520results%2520indicate%2520that%2520Dreamer%2520is%2520capable%2520of%2520utilizing%2520tactile%250Ainputs%2520to%2520solve%2520robotic%2520manipulation%2520tasks%2520in%2520simulation%2520and%2520reality.%250AFurthermore%252C%2520we%2520find%2520that%2520providing%2520the%2520robot%2520with%2520tactile%2520feedback%2520generally%250Aimproves%2520task%2520performance%252C%2520though%252C%2520in%2520our%2520setup%252C%2520we%2520do%2520not%2520yet%2520include%2520other%250Asensing%2520modalities.%2520In%2520the%2520future%252C%2520we%2520plan%2520to%2520utilize%2520our%2520platform%2520to%2520evaluate%250Aa%2520wide%2520range%2520of%2520other%2520Reinforcement%2520Learning%2520algorithms%2520on%2520tactile%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.00383v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Tactile%20Insertion%20in%20the%20Real%20World&entry.906535625=Daniel%20Palenicek%20and%20Theo%20Gruner%20and%20Tim%20Schneider%20and%20Alina%20B%C3%B6hm%20and%20Janis%20Lenz%20and%20Inga%20Pfenning%20and%20Eric%20Kr%C3%A4mer%20and%20Jan%20Peters&entry.1292438233=%20%20Humans%20have%20exceptional%20tactile%20sensing%20capabilities%2C%20which%20they%20can%20leverage%0Ato%20solve%20challenging%2C%20partially%20observable%20tasks%20that%20cannot%20be%20solved%20from%0Avisual%20observation%20alone.%20Research%20in%20tactile%20sensing%20attempts%20to%20unlock%20this%0Anew%20input%20modality%20for%20robots.%20Lately%2C%20these%20sensors%20have%20become%20cheaper%20and%2C%0Athus%2C%20widely%20available.%20At%20the%20same%20time%2C%20the%20question%20of%20how%20to%20integrate%20them%0Ainto%20control%20loops%20is%20still%20an%20active%20area%20of%20research%2C%20with%20central%20challenges%0Abeing%20partial%20observability%20and%20the%20contact-rich%20nature%20of%20manipulation%20tasks.%0AIn%20this%20study%2C%20we%20propose%20to%20use%20Reinforcement%20Learning%20to%20learn%20an%20end-to-end%0Apolicy%2C%20mapping%20directly%20from%20tactile%20sensor%20readings%20to%20actions.%20Specifically%2C%0Awe%20use%20Dreamer-v3%20on%20a%20challenging%2C%20partially%20observable%20robotic%20insertion%20task%0Awith%20a%20Franka%20Research%203%2C%20both%20in%20simulation%20and%20on%20a%20real%20system.%20For%20the%20real%0Asetup%2C%20we%20built%20a%20robotic%20platform%20capable%20of%20resetting%20itself%20fully%0Aautonomously%2C%20allowing%20for%20extensive%20training%20runs%20without%20human%20supervision.%0AOur%20preliminary%20results%20indicate%20that%20Dreamer%20is%20capable%20of%20utilizing%20tactile%0Ainputs%20to%20solve%20robotic%20manipulation%20tasks%20in%20simulation%20and%20reality.%0AFurthermore%2C%20we%20find%20that%20providing%20the%20robot%20with%20tactile%20feedback%20generally%0Aimproves%20task%20performance%2C%20though%2C%20in%20our%20setup%2C%20we%20do%20not%20yet%20include%20other%0Asensing%20modalities.%20In%20the%20future%2C%20we%20plan%20to%20utilize%20our%20platform%20to%20evaluate%0Aa%20wide%20range%20of%20other%20Reinforcement%20Learning%20algorithms%20on%20tactile%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00383v2&entry.124074799=Read"},
{"title": "A State-of-the-Art Review of Computational Models for Analyzing\n  Longitudinal Wearable Sensor Data in Healthcare", "author": "Paula Lago", "abstract": "  Wearable devices are increasingly used as tools for biomedical research, as\nthe continuous stream of behavioral and physiological data they collect can\nprovide insights about our health in everyday contexts. Long-term tracking,\ndefined in the timescale of months of year, can provide insights of patterns\nand changes as indicators of health changes. These insights can make medicine\nand healthcare more predictive, preventive, personalized, and participative\n(The 4P's). However, the challenges in modeling, understanding and processing\nlongitudinal data are a significant barrier to their adoption in research\nstudies and clinical settings. In this paper, we review and discuss three\nmodels used to make sense of longitudinal data: routines, rhythms and stability\nmetrics. We present the challenges associated with the processing and analysis\nof longitudinal wearable sensor data, with a special focus on how to handle the\ndifferent temporal dynamics at various granularities. We then discuss current\nlimitations and identify directions for future work. This review is essential\nto the advancement of computational modeling and analysis of longitudinal\nsensor data for pervasive healthcare.\n", "link": "http://arxiv.org/abs/2407.21665v1", "date": "2024-07-31", "relevancy": 1.3745, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4943}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4508}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20State-of-the-Art%20Review%20of%20Computational%20Models%20for%20Analyzing%0A%20%20Longitudinal%20Wearable%20Sensor%20Data%20in%20Healthcare&body=Title%3A%20A%20State-of-the-Art%20Review%20of%20Computational%20Models%20for%20Analyzing%0A%20%20Longitudinal%20Wearable%20Sensor%20Data%20in%20Healthcare%0AAuthor%3A%20Paula%20Lago%0AAbstract%3A%20%20%20Wearable%20devices%20are%20increasingly%20used%20as%20tools%20for%20biomedical%20research%2C%20as%0Athe%20continuous%20stream%20of%20behavioral%20and%20physiological%20data%20they%20collect%20can%0Aprovide%20insights%20about%20our%20health%20in%20everyday%20contexts.%20Long-term%20tracking%2C%0Adefined%20in%20the%20timescale%20of%20months%20of%20year%2C%20can%20provide%20insights%20of%20patterns%0Aand%20changes%20as%20indicators%20of%20health%20changes.%20These%20insights%20can%20make%20medicine%0Aand%20healthcare%20more%20predictive%2C%20preventive%2C%20personalized%2C%20and%20participative%0A%28The%204P%27s%29.%20However%2C%20the%20challenges%20in%20modeling%2C%20understanding%20and%20processing%0Alongitudinal%20data%20are%20a%20significant%20barrier%20to%20their%20adoption%20in%20research%0Astudies%20and%20clinical%20settings.%20In%20this%20paper%2C%20we%20review%20and%20discuss%20three%0Amodels%20used%20to%20make%20sense%20of%20longitudinal%20data%3A%20routines%2C%20rhythms%20and%20stability%0Ametrics.%20We%20present%20the%20challenges%20associated%20with%20the%20processing%20and%20analysis%0Aof%20longitudinal%20wearable%20sensor%20data%2C%20with%20a%20special%20focus%20on%20how%20to%20handle%20the%0Adifferent%20temporal%20dynamics%20at%20various%20granularities.%20We%20then%20discuss%20current%0Alimitations%20and%20identify%20directions%20for%20future%20work.%20This%20review%20is%20essential%0Ato%20the%20advancement%20of%20computational%20modeling%20and%20analysis%20of%20longitudinal%0Asensor%20data%20for%20pervasive%20healthcare.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520State-of-the-Art%2520Review%2520of%2520Computational%2520Models%2520for%2520Analyzing%250A%2520%2520Longitudinal%2520Wearable%2520Sensor%2520Data%2520in%2520Healthcare%26entry.906535625%3DPaula%2520Lago%26entry.1292438233%3D%2520%2520Wearable%2520devices%2520are%2520increasingly%2520used%2520as%2520tools%2520for%2520biomedical%2520research%252C%2520as%250Athe%2520continuous%2520stream%2520of%2520behavioral%2520and%2520physiological%2520data%2520they%2520collect%2520can%250Aprovide%2520insights%2520about%2520our%2520health%2520in%2520everyday%2520contexts.%2520Long-term%2520tracking%252C%250Adefined%2520in%2520the%2520timescale%2520of%2520months%2520of%2520year%252C%2520can%2520provide%2520insights%2520of%2520patterns%250Aand%2520changes%2520as%2520indicators%2520of%2520health%2520changes.%2520These%2520insights%2520can%2520make%2520medicine%250Aand%2520healthcare%2520more%2520predictive%252C%2520preventive%252C%2520personalized%252C%2520and%2520participative%250A%2528The%25204P%2527s%2529.%2520However%252C%2520the%2520challenges%2520in%2520modeling%252C%2520understanding%2520and%2520processing%250Alongitudinal%2520data%2520are%2520a%2520significant%2520barrier%2520to%2520their%2520adoption%2520in%2520research%250Astudies%2520and%2520clinical%2520settings.%2520In%2520this%2520paper%252C%2520we%2520review%2520and%2520discuss%2520three%250Amodels%2520used%2520to%2520make%2520sense%2520of%2520longitudinal%2520data%253A%2520routines%252C%2520rhythms%2520and%2520stability%250Ametrics.%2520We%2520present%2520the%2520challenges%2520associated%2520with%2520the%2520processing%2520and%2520analysis%250Aof%2520longitudinal%2520wearable%2520sensor%2520data%252C%2520with%2520a%2520special%2520focus%2520on%2520how%2520to%2520handle%2520the%250Adifferent%2520temporal%2520dynamics%2520at%2520various%2520granularities.%2520We%2520then%2520discuss%2520current%250Alimitations%2520and%2520identify%2520directions%2520for%2520future%2520work.%2520This%2520review%2520is%2520essential%250Ato%2520the%2520advancement%2520of%2520computational%2520modeling%2520and%2520analysis%2520of%2520longitudinal%250Asensor%2520data%2520for%2520pervasive%2520healthcare.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20State-of-the-Art%20Review%20of%20Computational%20Models%20for%20Analyzing%0A%20%20Longitudinal%20Wearable%20Sensor%20Data%20in%20Healthcare&entry.906535625=Paula%20Lago&entry.1292438233=%20%20Wearable%20devices%20are%20increasingly%20used%20as%20tools%20for%20biomedical%20research%2C%20as%0Athe%20continuous%20stream%20of%20behavioral%20and%20physiological%20data%20they%20collect%20can%0Aprovide%20insights%20about%20our%20health%20in%20everyday%20contexts.%20Long-term%20tracking%2C%0Adefined%20in%20the%20timescale%20of%20months%20of%20year%2C%20can%20provide%20insights%20of%20patterns%0Aand%20changes%20as%20indicators%20of%20health%20changes.%20These%20insights%20can%20make%20medicine%0Aand%20healthcare%20more%20predictive%2C%20preventive%2C%20personalized%2C%20and%20participative%0A%28The%204P%27s%29.%20However%2C%20the%20challenges%20in%20modeling%2C%20understanding%20and%20processing%0Alongitudinal%20data%20are%20a%20significant%20barrier%20to%20their%20adoption%20in%20research%0Astudies%20and%20clinical%20settings.%20In%20this%20paper%2C%20we%20review%20and%20discuss%20three%0Amodels%20used%20to%20make%20sense%20of%20longitudinal%20data%3A%20routines%2C%20rhythms%20and%20stability%0Ametrics.%20We%20present%20the%20challenges%20associated%20with%20the%20processing%20and%20analysis%0Aof%20longitudinal%20wearable%20sensor%20data%2C%20with%20a%20special%20focus%20on%20how%20to%20handle%20the%0Adifferent%20temporal%20dynamics%20at%20various%20granularities.%20We%20then%20discuss%20current%0Alimitations%20and%20identify%20directions%20for%20future%20work.%20This%20review%20is%20essential%0Ato%20the%20advancement%20of%20computational%20modeling%20and%20analysis%20of%20longitudinal%0Asensor%20data%20for%20pervasive%20healthcare.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21665v1&entry.124074799=Read"},
{"title": "Comgra: A Tool for Analyzing and Debugging Neural Networks", "author": "Florian Dietz and Sophie Fellenz and Dietrich Klakow and Marius Kloft", "abstract": "  Neural Networks are notoriously difficult to inspect. We introduce comgra, an\nopen source python library for use with PyTorch. Comgra extracts data about the\ninternal activations of a model and organizes it in a GUI (graphical user\ninterface). It can show both summary statistics and individual data points,\ncompare early and late stages of training, focus on individual samples of\ninterest, and visualize the flow of the gradient through the network. This\nmakes it possible to inspect the model's behavior from many different angles\nand save time by rapidly testing different hypotheses without having to rerun\nit. Comgra has applications for debugging, neural architecture design, and\nmechanistic interpretability. We publish our library through Python Package\nIndex (PyPI) and provide code, documentation, and tutorials at\nhttps://github.com/FlorianDietz/comgra.\n", "link": "http://arxiv.org/abs/2407.21656v1", "date": "2024-07-31", "relevancy": 1.701, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4427}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4143}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comgra%3A%20A%20Tool%20for%20Analyzing%20and%20Debugging%20Neural%20Networks&body=Title%3A%20Comgra%3A%20A%20Tool%20for%20Analyzing%20and%20Debugging%20Neural%20Networks%0AAuthor%3A%20Florian%20Dietz%20and%20Sophie%20Fellenz%20and%20Dietrich%20Klakow%20and%20Marius%20Kloft%0AAbstract%3A%20%20%20Neural%20Networks%20are%20notoriously%20difficult%20to%20inspect.%20We%20introduce%20comgra%2C%20an%0Aopen%20source%20python%20library%20for%20use%20with%20PyTorch.%20Comgra%20extracts%20data%20about%20the%0Ainternal%20activations%20of%20a%20model%20and%20organizes%20it%20in%20a%20GUI%20%28graphical%20user%0Ainterface%29.%20It%20can%20show%20both%20summary%20statistics%20and%20individual%20data%20points%2C%0Acompare%20early%20and%20late%20stages%20of%20training%2C%20focus%20on%20individual%20samples%20of%0Ainterest%2C%20and%20visualize%20the%20flow%20of%20the%20gradient%20through%20the%20network.%20This%0Amakes%20it%20possible%20to%20inspect%20the%20model%27s%20behavior%20from%20many%20different%20angles%0Aand%20save%20time%20by%20rapidly%20testing%20different%20hypotheses%20without%20having%20to%20rerun%0Ait.%20Comgra%20has%20applications%20for%20debugging%2C%20neural%20architecture%20design%2C%20and%0Amechanistic%20interpretability.%20We%20publish%20our%20library%20through%20Python%20Package%0AIndex%20%28PyPI%29%20and%20provide%20code%2C%20documentation%2C%20and%20tutorials%20at%0Ahttps%3A//github.com/FlorianDietz/comgra.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21656v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComgra%253A%2520A%2520Tool%2520for%2520Analyzing%2520and%2520Debugging%2520Neural%2520Networks%26entry.906535625%3DFlorian%2520Dietz%2520and%2520Sophie%2520Fellenz%2520and%2520Dietrich%2520Klakow%2520and%2520Marius%2520Kloft%26entry.1292438233%3D%2520%2520Neural%2520Networks%2520are%2520notoriously%2520difficult%2520to%2520inspect.%2520We%2520introduce%2520comgra%252C%2520an%250Aopen%2520source%2520python%2520library%2520for%2520use%2520with%2520PyTorch.%2520Comgra%2520extracts%2520data%2520about%2520the%250Ainternal%2520activations%2520of%2520a%2520model%2520and%2520organizes%2520it%2520in%2520a%2520GUI%2520%2528graphical%2520user%250Ainterface%2529.%2520It%2520can%2520show%2520both%2520summary%2520statistics%2520and%2520individual%2520data%2520points%252C%250Acompare%2520early%2520and%2520late%2520stages%2520of%2520training%252C%2520focus%2520on%2520individual%2520samples%2520of%250Ainterest%252C%2520and%2520visualize%2520the%2520flow%2520of%2520the%2520gradient%2520through%2520the%2520network.%2520This%250Amakes%2520it%2520possible%2520to%2520inspect%2520the%2520model%2527s%2520behavior%2520from%2520many%2520different%2520angles%250Aand%2520save%2520time%2520by%2520rapidly%2520testing%2520different%2520hypotheses%2520without%2520having%2520to%2520rerun%250Ait.%2520Comgra%2520has%2520applications%2520for%2520debugging%252C%2520neural%2520architecture%2520design%252C%2520and%250Amechanistic%2520interpretability.%2520We%2520publish%2520our%2520library%2520through%2520Python%2520Package%250AIndex%2520%2528PyPI%2529%2520and%2520provide%2520code%252C%2520documentation%252C%2520and%2520tutorials%2520at%250Ahttps%253A//github.com/FlorianDietz/comgra.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21656v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comgra%3A%20A%20Tool%20for%20Analyzing%20and%20Debugging%20Neural%20Networks&entry.906535625=Florian%20Dietz%20and%20Sophie%20Fellenz%20and%20Dietrich%20Klakow%20and%20Marius%20Kloft&entry.1292438233=%20%20Neural%20Networks%20are%20notoriously%20difficult%20to%20inspect.%20We%20introduce%20comgra%2C%20an%0Aopen%20source%20python%20library%20for%20use%20with%20PyTorch.%20Comgra%20extracts%20data%20about%20the%0Ainternal%20activations%20of%20a%20model%20and%20organizes%20it%20in%20a%20GUI%20%28graphical%20user%0Ainterface%29.%20It%20can%20show%20both%20summary%20statistics%20and%20individual%20data%20points%2C%0Acompare%20early%20and%20late%20stages%20of%20training%2C%20focus%20on%20individual%20samples%20of%0Ainterest%2C%20and%20visualize%20the%20flow%20of%20the%20gradient%20through%20the%20network.%20This%0Amakes%20it%20possible%20to%20inspect%20the%20model%27s%20behavior%20from%20many%20different%20angles%0Aand%20save%20time%20by%20rapidly%20testing%20different%20hypotheses%20without%20having%20to%20rerun%0Ait.%20Comgra%20has%20applications%20for%20debugging%2C%20neural%20architecture%20design%2C%20and%0Amechanistic%20interpretability.%20We%20publish%20our%20library%20through%20Python%20Package%0AIndex%20%28PyPI%29%20and%20provide%20code%2C%20documentation%2C%20and%20tutorials%20at%0Ahttps%3A//github.com/FlorianDietz/comgra.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21656v1&entry.124074799=Read"},
{"title": "Universal Approximation Theory: Foundations for Parallelism in Neural\n  Networks", "author": "Wei Wang and Qing Li", "abstract": "  Neural networks are increasingly evolving towards training large models with\nbig data, a method that has demonstrated superior performance across many\ntasks. However, this approach introduces an urgent problem: current deep\nlearning models are predominantly serial, meaning that as the number of network\nlayers increases, so do the training and inference times. This is unacceptable\nif deep learning is to continue advancing. Therefore, this paper proposes a\ndeep learning parallelization strategy based on the Universal Approximation\nTheorem (UAT). From this foundation, we designed a parallel network called\nPara-Former to test our theory. Unlike traditional serial models, the inference\ntime of Para-Former does not increase with the number of layers, significantly\naccelerating the inference speed of multi-layer networks. Experimental results\nvalidate the effectiveness of this network.\n", "link": "http://arxiv.org/abs/2407.21670v1", "date": "2024-07-31", "relevancy": 1.5137, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.527}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5074}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4944}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Universal%20Approximation%20Theory%3A%20Foundations%20for%20Parallelism%20in%20Neural%0A%20%20Networks&body=Title%3A%20Universal%20Approximation%20Theory%3A%20Foundations%20for%20Parallelism%20in%20Neural%0A%20%20Networks%0AAuthor%3A%20Wei%20Wang%20and%20Qing%20Li%0AAbstract%3A%20%20%20Neural%20networks%20are%20increasingly%20evolving%20towards%20training%20large%20models%20with%0Abig%20data%2C%20a%20method%20that%20has%20demonstrated%20superior%20performance%20across%20many%0Atasks.%20However%2C%20this%20approach%20introduces%20an%20urgent%20problem%3A%20current%20deep%0Alearning%20models%20are%20predominantly%20serial%2C%20meaning%20that%20as%20the%20number%20of%20network%0Alayers%20increases%2C%20so%20do%20the%20training%20and%20inference%20times.%20This%20is%20unacceptable%0Aif%20deep%20learning%20is%20to%20continue%20advancing.%20Therefore%2C%20this%20paper%20proposes%20a%0Adeep%20learning%20parallelization%20strategy%20based%20on%20the%20Universal%20Approximation%0ATheorem%20%28UAT%29.%20From%20this%20foundation%2C%20we%20designed%20a%20parallel%20network%20called%0APara-Former%20to%20test%20our%20theory.%20Unlike%20traditional%20serial%20models%2C%20the%20inference%0Atime%20of%20Para-Former%20does%20not%20increase%20with%20the%20number%20of%20layers%2C%20significantly%0Aaccelerating%20the%20inference%20speed%20of%20multi-layer%20networks.%20Experimental%20results%0Avalidate%20the%20effectiveness%20of%20this%20network.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21670v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniversal%2520Approximation%2520Theory%253A%2520Foundations%2520for%2520Parallelism%2520in%2520Neural%250A%2520%2520Networks%26entry.906535625%3DWei%2520Wang%2520and%2520Qing%2520Li%26entry.1292438233%3D%2520%2520Neural%2520networks%2520are%2520increasingly%2520evolving%2520towards%2520training%2520large%2520models%2520with%250Abig%2520data%252C%2520a%2520method%2520that%2520has%2520demonstrated%2520superior%2520performance%2520across%2520many%250Atasks.%2520However%252C%2520this%2520approach%2520introduces%2520an%2520urgent%2520problem%253A%2520current%2520deep%250Alearning%2520models%2520are%2520predominantly%2520serial%252C%2520meaning%2520that%2520as%2520the%2520number%2520of%2520network%250Alayers%2520increases%252C%2520so%2520do%2520the%2520training%2520and%2520inference%2520times.%2520This%2520is%2520unacceptable%250Aif%2520deep%2520learning%2520is%2520to%2520continue%2520advancing.%2520Therefore%252C%2520this%2520paper%2520proposes%2520a%250Adeep%2520learning%2520parallelization%2520strategy%2520based%2520on%2520the%2520Universal%2520Approximation%250ATheorem%2520%2528UAT%2529.%2520From%2520this%2520foundation%252C%2520we%2520designed%2520a%2520parallel%2520network%2520called%250APara-Former%2520to%2520test%2520our%2520theory.%2520Unlike%2520traditional%2520serial%2520models%252C%2520the%2520inference%250Atime%2520of%2520Para-Former%2520does%2520not%2520increase%2520with%2520the%2520number%2520of%2520layers%252C%2520significantly%250Aaccelerating%2520the%2520inference%2520speed%2520of%2520multi-layer%2520networks.%2520Experimental%2520results%250Avalidate%2520the%2520effectiveness%2520of%2520this%2520network.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21670v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Universal%20Approximation%20Theory%3A%20Foundations%20for%20Parallelism%20in%20Neural%0A%20%20Networks&entry.906535625=Wei%20Wang%20and%20Qing%20Li&entry.1292438233=%20%20Neural%20networks%20are%20increasingly%20evolving%20towards%20training%20large%20models%20with%0Abig%20data%2C%20a%20method%20that%20has%20demonstrated%20superior%20performance%20across%20many%0Atasks.%20However%2C%20this%20approach%20introduces%20an%20urgent%20problem%3A%20current%20deep%0Alearning%20models%20are%20predominantly%20serial%2C%20meaning%20that%20as%20the%20number%20of%20network%0Alayers%20increases%2C%20so%20do%20the%20training%20and%20inference%20times.%20This%20is%20unacceptable%0Aif%20deep%20learning%20is%20to%20continue%20advancing.%20Therefore%2C%20this%20paper%20proposes%20a%0Adeep%20learning%20parallelization%20strategy%20based%20on%20the%20Universal%20Approximation%0ATheorem%20%28UAT%29.%20From%20this%20foundation%2C%20we%20designed%20a%20parallel%20network%20called%0APara-Former%20to%20test%20our%20theory.%20Unlike%20traditional%20serial%20models%2C%20the%20inference%0Atime%20of%20Para-Former%20does%20not%20increase%20with%20the%20number%20of%20layers%2C%20significantly%0Aaccelerating%20the%20inference%20speed%20of%20multi-layer%20networks.%20Experimental%20results%0Avalidate%20the%20effectiveness%20of%20this%20network.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21670v1&entry.124074799=Read"},
{"title": "Higher order quantum reservoir computing for non-intrusive reduced-order\n  models", "author": "Vinamr Jain and Romit Maulik", "abstract": "  Forecasting dynamical systems is of importance to numerous real-world\napplications. When possible, dynamical systems forecasts are constructed based\non first-principles-based models such as through the use of differential\nequations. When these equations are unknown, non-intrusive techniques must be\nutilized to build predictive models from data alone. Machine learning (ML)\nmethods have recently been used for such tasks. Moreover, ML methods provide\nthe added advantage of significant reductions in time-to-solution for\npredictions in contrast with first-principle based models. However, many\nstate-of-the-art ML-based methods for forecasting rely on neural networks,\nwhich may be expensive to train and necessitate requirements for large amounts\nof memory. In this work, we propose a quantum mechanics inspired ML modeling\nstrategy for learning nonlinear dynamical systems that provides data-driven\nforecasts for complex dynamical systems with reduced training time and memory\ncosts. This approach, denoted the quantum reservoir computing technique (QRC),\nis a hybrid quantum-classical framework employing an ensemble of interconnected\nsmall quantum systems via classical linear feedback connections. By mapping the\ndynamical state to a suitable quantum representation amenable to unitary\noperations, QRC is able to predict complex nonlinear dynamical systems in a\nstable and accurate manner. We demonstrate the efficacy of this framework\nthrough benchmark forecasts of the NOAA Optimal Interpolation Sea Surface\nTemperature dataset and compare the performance of QRC to other ML methods.\n", "link": "http://arxiv.org/abs/2407.21602v1", "date": "2024-07-31", "relevancy": 0.9471, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4824}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.477}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Higher%20order%20quantum%20reservoir%20computing%20for%20non-intrusive%20reduced-order%0A%20%20models&body=Title%3A%20Higher%20order%20quantum%20reservoir%20computing%20for%20non-intrusive%20reduced-order%0A%20%20models%0AAuthor%3A%20Vinamr%20Jain%20and%20Romit%20Maulik%0AAbstract%3A%20%20%20Forecasting%20dynamical%20systems%20is%20of%20importance%20to%20numerous%20real-world%0Aapplications.%20When%20possible%2C%20dynamical%20systems%20forecasts%20are%20constructed%20based%0Aon%20first-principles-based%20models%20such%20as%20through%20the%20use%20of%20differential%0Aequations.%20When%20these%20equations%20are%20unknown%2C%20non-intrusive%20techniques%20must%20be%0Autilized%20to%20build%20predictive%20models%20from%20data%20alone.%20Machine%20learning%20%28ML%29%0Amethods%20have%20recently%20been%20used%20for%20such%20tasks.%20Moreover%2C%20ML%20methods%20provide%0Athe%20added%20advantage%20of%20significant%20reductions%20in%20time-to-solution%20for%0Apredictions%20in%20contrast%20with%20first-principle%20based%20models.%20However%2C%20many%0Astate-of-the-art%20ML-based%20methods%20for%20forecasting%20rely%20on%20neural%20networks%2C%0Awhich%20may%20be%20expensive%20to%20train%20and%20necessitate%20requirements%20for%20large%20amounts%0Aof%20memory.%20In%20this%20work%2C%20we%20propose%20a%20quantum%20mechanics%20inspired%20ML%20modeling%0Astrategy%20for%20learning%20nonlinear%20dynamical%20systems%20that%20provides%20data-driven%0Aforecasts%20for%20complex%20dynamical%20systems%20with%20reduced%20training%20time%20and%20memory%0Acosts.%20This%20approach%2C%20denoted%20the%20quantum%20reservoir%20computing%20technique%20%28QRC%29%2C%0Ais%20a%20hybrid%20quantum-classical%20framework%20employing%20an%20ensemble%20of%20interconnected%0Asmall%20quantum%20systems%20via%20classical%20linear%20feedback%20connections.%20By%20mapping%20the%0Adynamical%20state%20to%20a%20suitable%20quantum%20representation%20amenable%20to%20unitary%0Aoperations%2C%20QRC%20is%20able%20to%20predict%20complex%20nonlinear%20dynamical%20systems%20in%20a%0Astable%20and%20accurate%20manner.%20We%20demonstrate%20the%20efficacy%20of%20this%20framework%0Athrough%20benchmark%20forecasts%20of%20the%20NOAA%20Optimal%20Interpolation%20Sea%20Surface%0ATemperature%20dataset%20and%20compare%20the%20performance%20of%20QRC%20to%20other%20ML%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21602v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigher%2520order%2520quantum%2520reservoir%2520computing%2520for%2520non-intrusive%2520reduced-order%250A%2520%2520models%26entry.906535625%3DVinamr%2520Jain%2520and%2520Romit%2520Maulik%26entry.1292438233%3D%2520%2520Forecasting%2520dynamical%2520systems%2520is%2520of%2520importance%2520to%2520numerous%2520real-world%250Aapplications.%2520When%2520possible%252C%2520dynamical%2520systems%2520forecasts%2520are%2520constructed%2520based%250Aon%2520first-principles-based%2520models%2520such%2520as%2520through%2520the%2520use%2520of%2520differential%250Aequations.%2520When%2520these%2520equations%2520are%2520unknown%252C%2520non-intrusive%2520techniques%2520must%2520be%250Autilized%2520to%2520build%2520predictive%2520models%2520from%2520data%2520alone.%2520Machine%2520learning%2520%2528ML%2529%250Amethods%2520have%2520recently%2520been%2520used%2520for%2520such%2520tasks.%2520Moreover%252C%2520ML%2520methods%2520provide%250Athe%2520added%2520advantage%2520of%2520significant%2520reductions%2520in%2520time-to-solution%2520for%250Apredictions%2520in%2520contrast%2520with%2520first-principle%2520based%2520models.%2520However%252C%2520many%250Astate-of-the-art%2520ML-based%2520methods%2520for%2520forecasting%2520rely%2520on%2520neural%2520networks%252C%250Awhich%2520may%2520be%2520expensive%2520to%2520train%2520and%2520necessitate%2520requirements%2520for%2520large%2520amounts%250Aof%2520memory.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520quantum%2520mechanics%2520inspired%2520ML%2520modeling%250Astrategy%2520for%2520learning%2520nonlinear%2520dynamical%2520systems%2520that%2520provides%2520data-driven%250Aforecasts%2520for%2520complex%2520dynamical%2520systems%2520with%2520reduced%2520training%2520time%2520and%2520memory%250Acosts.%2520This%2520approach%252C%2520denoted%2520the%2520quantum%2520reservoir%2520computing%2520technique%2520%2528QRC%2529%252C%250Ais%2520a%2520hybrid%2520quantum-classical%2520framework%2520employing%2520an%2520ensemble%2520of%2520interconnected%250Asmall%2520quantum%2520systems%2520via%2520classical%2520linear%2520feedback%2520connections.%2520By%2520mapping%2520the%250Adynamical%2520state%2520to%2520a%2520suitable%2520quantum%2520representation%2520amenable%2520to%2520unitary%250Aoperations%252C%2520QRC%2520is%2520able%2520to%2520predict%2520complex%2520nonlinear%2520dynamical%2520systems%2520in%2520a%250Astable%2520and%2520accurate%2520manner.%2520We%2520demonstrate%2520the%2520efficacy%2520of%2520this%2520framework%250Athrough%2520benchmark%2520forecasts%2520of%2520the%2520NOAA%2520Optimal%2520Interpolation%2520Sea%2520Surface%250ATemperature%2520dataset%2520and%2520compare%2520the%2520performance%2520of%2520QRC%2520to%2520other%2520ML%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21602v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Higher%20order%20quantum%20reservoir%20computing%20for%20non-intrusive%20reduced-order%0A%20%20models&entry.906535625=Vinamr%20Jain%20and%20Romit%20Maulik&entry.1292438233=%20%20Forecasting%20dynamical%20systems%20is%20of%20importance%20to%20numerous%20real-world%0Aapplications.%20When%20possible%2C%20dynamical%20systems%20forecasts%20are%20constructed%20based%0Aon%20first-principles-based%20models%20such%20as%20through%20the%20use%20of%20differential%0Aequations.%20When%20these%20equations%20are%20unknown%2C%20non-intrusive%20techniques%20must%20be%0Autilized%20to%20build%20predictive%20models%20from%20data%20alone.%20Machine%20learning%20%28ML%29%0Amethods%20have%20recently%20been%20used%20for%20such%20tasks.%20Moreover%2C%20ML%20methods%20provide%0Athe%20added%20advantage%20of%20significant%20reductions%20in%20time-to-solution%20for%0Apredictions%20in%20contrast%20with%20first-principle%20based%20models.%20However%2C%20many%0Astate-of-the-art%20ML-based%20methods%20for%20forecasting%20rely%20on%20neural%20networks%2C%0Awhich%20may%20be%20expensive%20to%20train%20and%20necessitate%20requirements%20for%20large%20amounts%0Aof%20memory.%20In%20this%20work%2C%20we%20propose%20a%20quantum%20mechanics%20inspired%20ML%20modeling%0Astrategy%20for%20learning%20nonlinear%20dynamical%20systems%20that%20provides%20data-driven%0Aforecasts%20for%20complex%20dynamical%20systems%20with%20reduced%20training%20time%20and%20memory%0Acosts.%20This%20approach%2C%20denoted%20the%20quantum%20reservoir%20computing%20technique%20%28QRC%29%2C%0Ais%20a%20hybrid%20quantum-classical%20framework%20employing%20an%20ensemble%20of%20interconnected%0Asmall%20quantum%20systems%20via%20classical%20linear%20feedback%20connections.%20By%20mapping%20the%0Adynamical%20state%20to%20a%20suitable%20quantum%20representation%20amenable%20to%20unitary%0Aoperations%2C%20QRC%20is%20able%20to%20predict%20complex%20nonlinear%20dynamical%20systems%20in%20a%0Astable%20and%20accurate%20manner.%20We%20demonstrate%20the%20efficacy%20of%20this%20framework%0Athrough%20benchmark%20forecasts%20of%20the%20NOAA%20Optimal%20Interpolation%20Sea%20Surface%0ATemperature%20dataset%20and%20compare%20the%20performance%20of%20QRC%20to%20other%20ML%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21602v1&entry.124074799=Read"},
{"title": "Lyapunov weights to convey the meaning of time in physics-informed\n  neural networks", "author": "Gabriel Turinici", "abstract": "  Time is not a dimension as the others. In Physics-Informed Neural Networks\n(PINN) several proposals attempted to adapt the time sampling or time weighting\nto take into account the specifics of this special dimension. But these\nproposals are not principled and need guidance to be used. We explain here\ntheoretically why the Lyapunov exponents give actionable insights and propose a\nweighting scheme to automatically adapt to chaotic, periodic or stable\ndynamics. We characterize theoretically the best weighting scheme under\ncomputational constraints as a cumulative exponential integral of the local\nLyapunov exponent estimators and show that it performs well in practice under\nthe regimes mentioned above.\n", "link": "http://arxiv.org/abs/2407.21642v1", "date": "2024-07-31", "relevancy": 1.2442, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.454}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4132}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3996}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lyapunov%20weights%20to%20convey%20the%20meaning%20of%20time%20in%20physics-informed%0A%20%20neural%20networks&body=Title%3A%20Lyapunov%20weights%20to%20convey%20the%20meaning%20of%20time%20in%20physics-informed%0A%20%20neural%20networks%0AAuthor%3A%20Gabriel%20Turinici%0AAbstract%3A%20%20%20Time%20is%20not%20a%20dimension%20as%20the%20others.%20In%20Physics-Informed%20Neural%20Networks%0A%28PINN%29%20several%20proposals%20attempted%20to%20adapt%20the%20time%20sampling%20or%20time%20weighting%0Ato%20take%20into%20account%20the%20specifics%20of%20this%20special%20dimension.%20But%20these%0Aproposals%20are%20not%20principled%20and%20need%20guidance%20to%20be%20used.%20We%20explain%20here%0Atheoretically%20why%20the%20Lyapunov%20exponents%20give%20actionable%20insights%20and%20propose%20a%0Aweighting%20scheme%20to%20automatically%20adapt%20to%20chaotic%2C%20periodic%20or%20stable%0Adynamics.%20We%20characterize%20theoretically%20the%20best%20weighting%20scheme%20under%0Acomputational%20constraints%20as%20a%20cumulative%20exponential%20integral%20of%20the%20local%0ALyapunov%20exponent%20estimators%20and%20show%20that%20it%20performs%20well%20in%20practice%20under%0Athe%20regimes%20mentioned%20above.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21642v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLyapunov%2520weights%2520to%2520convey%2520the%2520meaning%2520of%2520time%2520in%2520physics-informed%250A%2520%2520neural%2520networks%26entry.906535625%3DGabriel%2520Turinici%26entry.1292438233%3D%2520%2520Time%2520is%2520not%2520a%2520dimension%2520as%2520the%2520others.%2520In%2520Physics-Informed%2520Neural%2520Networks%250A%2528PINN%2529%2520several%2520proposals%2520attempted%2520to%2520adapt%2520the%2520time%2520sampling%2520or%2520time%2520weighting%250Ato%2520take%2520into%2520account%2520the%2520specifics%2520of%2520this%2520special%2520dimension.%2520But%2520these%250Aproposals%2520are%2520not%2520principled%2520and%2520need%2520guidance%2520to%2520be%2520used.%2520We%2520explain%2520here%250Atheoretically%2520why%2520the%2520Lyapunov%2520exponents%2520give%2520actionable%2520insights%2520and%2520propose%2520a%250Aweighting%2520scheme%2520to%2520automatically%2520adapt%2520to%2520chaotic%252C%2520periodic%2520or%2520stable%250Adynamics.%2520We%2520characterize%2520theoretically%2520the%2520best%2520weighting%2520scheme%2520under%250Acomputational%2520constraints%2520as%2520a%2520cumulative%2520exponential%2520integral%2520of%2520the%2520local%250ALyapunov%2520exponent%2520estimators%2520and%2520show%2520that%2520it%2520performs%2520well%2520in%2520practice%2520under%250Athe%2520regimes%2520mentioned%2520above.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21642v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lyapunov%20weights%20to%20convey%20the%20meaning%20of%20time%20in%20physics-informed%0A%20%20neural%20networks&entry.906535625=Gabriel%20Turinici&entry.1292438233=%20%20Time%20is%20not%20a%20dimension%20as%20the%20others.%20In%20Physics-Informed%20Neural%20Networks%0A%28PINN%29%20several%20proposals%20attempted%20to%20adapt%20the%20time%20sampling%20or%20time%20weighting%0Ato%20take%20into%20account%20the%20specifics%20of%20this%20special%20dimension.%20But%20these%0Aproposals%20are%20not%20principled%20and%20need%20guidance%20to%20be%20used.%20We%20explain%20here%0Atheoretically%20why%20the%20Lyapunov%20exponents%20give%20actionable%20insights%20and%20propose%20a%0Aweighting%20scheme%20to%20automatically%20adapt%20to%20chaotic%2C%20periodic%20or%20stable%0Adynamics.%20We%20characterize%20theoretically%20the%20best%20weighting%20scheme%20under%0Acomputational%20constraints%20as%20a%20cumulative%20exponential%20integral%20of%20the%20local%0ALyapunov%20exponent%20estimators%20and%20show%20that%20it%20performs%20well%20in%20practice%20under%0Athe%20regimes%20mentioned%20above.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21642v1&entry.124074799=Read"},
{"title": "Maverick: Efficient and Accurate Coreference Resolution Defying Recent\n  Trends", "author": "Giuliano Martinelli and Edoardo Barba and Roberto Navigli", "abstract": "  Large autoregressive generative models have emerged as the cornerstone for\nachieving the highest performance across several Natural Language Processing\ntasks. However, the urge to attain superior results has, at times, led to the\npremature replacement of carefully designed task-specific approaches without\nexhaustive experimentation. The Coreference Resolution task is no exception;\nall recent state-of-the-art solutions adopt large generative autoregressive\nmodels that outperform encoder-based discriminative systems. In this work,we\nchallenge this recent trend by introducing Maverick, a carefully designed - yet\nsimple - pipeline, which enables running a state-of-the-art Coreference\nResolution system within the constraints of an academic budget, outperforming\nmodels with up to 13 billion parameters with as few as 500 million parameters.\nMaverick achieves state-of-the-art performance on the CoNLL-2012 benchmark,\ntraining with up to 0.006x the memory resources and obtaining a 170x faster\ninference compared to previous state-of-the-art systems. We extensively\nvalidate the robustness of the Maverick framework with an array of diverse\nexperiments, reporting improvements over prior systems in data-scarce,\nlong-document, and out-of-domain settings. We release our code and models for\nresearch purposes at https://github.com/SapienzaNLP/maverick-coref.\n", "link": "http://arxiv.org/abs/2407.21489v1", "date": "2024-07-31", "relevancy": 1.4602, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5065}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4892}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Maverick%3A%20Efficient%20and%20Accurate%20Coreference%20Resolution%20Defying%20Recent%0A%20%20Trends&body=Title%3A%20Maverick%3A%20Efficient%20and%20Accurate%20Coreference%20Resolution%20Defying%20Recent%0A%20%20Trends%0AAuthor%3A%20Giuliano%20Martinelli%20and%20Edoardo%20Barba%20and%20Roberto%20Navigli%0AAbstract%3A%20%20%20Large%20autoregressive%20generative%20models%20have%20emerged%20as%20the%20cornerstone%20for%0Aachieving%20the%20highest%20performance%20across%20several%20Natural%20Language%20Processing%0Atasks.%20However%2C%20the%20urge%20to%20attain%20superior%20results%20has%2C%20at%20times%2C%20led%20to%20the%0Apremature%20replacement%20of%20carefully%20designed%20task-specific%20approaches%20without%0Aexhaustive%20experimentation.%20The%20Coreference%20Resolution%20task%20is%20no%20exception%3B%0Aall%20recent%20state-of-the-art%20solutions%20adopt%20large%20generative%20autoregressive%0Amodels%20that%20outperform%20encoder-based%20discriminative%20systems.%20In%20this%20work%2Cwe%0Achallenge%20this%20recent%20trend%20by%20introducing%20Maverick%2C%20a%20carefully%20designed%20-%20yet%0Asimple%20-%20pipeline%2C%20which%20enables%20running%20a%20state-of-the-art%20Coreference%0AResolution%20system%20within%20the%20constraints%20of%20an%20academic%20budget%2C%20outperforming%0Amodels%20with%20up%20to%2013%20billion%20parameters%20with%20as%20few%20as%20500%20million%20parameters.%0AMaverick%20achieves%20state-of-the-art%20performance%20on%20the%20CoNLL-2012%20benchmark%2C%0Atraining%20with%20up%20to%200.006x%20the%20memory%20resources%20and%20obtaining%20a%20170x%20faster%0Ainference%20compared%20to%20previous%20state-of-the-art%20systems.%20We%20extensively%0Avalidate%20the%20robustness%20of%20the%20Maverick%20framework%20with%20an%20array%20of%20diverse%0Aexperiments%2C%20reporting%20improvements%20over%20prior%20systems%20in%20data-scarce%2C%0Along-document%2C%20and%20out-of-domain%20settings.%20We%20release%20our%20code%20and%20models%20for%0Aresearch%20purposes%20at%20https%3A//github.com/SapienzaNLP/maverick-coref.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21489v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaverick%253A%2520Efficient%2520and%2520Accurate%2520Coreference%2520Resolution%2520Defying%2520Recent%250A%2520%2520Trends%26entry.906535625%3DGiuliano%2520Martinelli%2520and%2520Edoardo%2520Barba%2520and%2520Roberto%2520Navigli%26entry.1292438233%3D%2520%2520Large%2520autoregressive%2520generative%2520models%2520have%2520emerged%2520as%2520the%2520cornerstone%2520for%250Aachieving%2520the%2520highest%2520performance%2520across%2520several%2520Natural%2520Language%2520Processing%250Atasks.%2520However%252C%2520the%2520urge%2520to%2520attain%2520superior%2520results%2520has%252C%2520at%2520times%252C%2520led%2520to%2520the%250Apremature%2520replacement%2520of%2520carefully%2520designed%2520task-specific%2520approaches%2520without%250Aexhaustive%2520experimentation.%2520The%2520Coreference%2520Resolution%2520task%2520is%2520no%2520exception%253B%250Aall%2520recent%2520state-of-the-art%2520solutions%2520adopt%2520large%2520generative%2520autoregressive%250Amodels%2520that%2520outperform%2520encoder-based%2520discriminative%2520systems.%2520In%2520this%2520work%252Cwe%250Achallenge%2520this%2520recent%2520trend%2520by%2520introducing%2520Maverick%252C%2520a%2520carefully%2520designed%2520-%2520yet%250Asimple%2520-%2520pipeline%252C%2520which%2520enables%2520running%2520a%2520state-of-the-art%2520Coreference%250AResolution%2520system%2520within%2520the%2520constraints%2520of%2520an%2520academic%2520budget%252C%2520outperforming%250Amodels%2520with%2520up%2520to%252013%2520billion%2520parameters%2520with%2520as%2520few%2520as%2520500%2520million%2520parameters.%250AMaverick%2520achieves%2520state-of-the-art%2520performance%2520on%2520the%2520CoNLL-2012%2520benchmark%252C%250Atraining%2520with%2520up%2520to%25200.006x%2520the%2520memory%2520resources%2520and%2520obtaining%2520a%2520170x%2520faster%250Ainference%2520compared%2520to%2520previous%2520state-of-the-art%2520systems.%2520We%2520extensively%250Avalidate%2520the%2520robustness%2520of%2520the%2520Maverick%2520framework%2520with%2520an%2520array%2520of%2520diverse%250Aexperiments%252C%2520reporting%2520improvements%2520over%2520prior%2520systems%2520in%2520data-scarce%252C%250Along-document%252C%2520and%2520out-of-domain%2520settings.%2520We%2520release%2520our%2520code%2520and%2520models%2520for%250Aresearch%2520purposes%2520at%2520https%253A//github.com/SapienzaNLP/maverick-coref.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21489v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Maverick%3A%20Efficient%20and%20Accurate%20Coreference%20Resolution%20Defying%20Recent%0A%20%20Trends&entry.906535625=Giuliano%20Martinelli%20and%20Edoardo%20Barba%20and%20Roberto%20Navigli&entry.1292438233=%20%20Large%20autoregressive%20generative%20models%20have%20emerged%20as%20the%20cornerstone%20for%0Aachieving%20the%20highest%20performance%20across%20several%20Natural%20Language%20Processing%0Atasks.%20However%2C%20the%20urge%20to%20attain%20superior%20results%20has%2C%20at%20times%2C%20led%20to%20the%0Apremature%20replacement%20of%20carefully%20designed%20task-specific%20approaches%20without%0Aexhaustive%20experimentation.%20The%20Coreference%20Resolution%20task%20is%20no%20exception%3B%0Aall%20recent%20state-of-the-art%20solutions%20adopt%20large%20generative%20autoregressive%0Amodels%20that%20outperform%20encoder-based%20discriminative%20systems.%20In%20this%20work%2Cwe%0Achallenge%20this%20recent%20trend%20by%20introducing%20Maverick%2C%20a%20carefully%20designed%20-%20yet%0Asimple%20-%20pipeline%2C%20which%20enables%20running%20a%20state-of-the-art%20Coreference%0AResolution%20system%20within%20the%20constraints%20of%20an%20academic%20budget%2C%20outperforming%0Amodels%20with%20up%20to%2013%20billion%20parameters%20with%20as%20few%20as%20500%20million%20parameters.%0AMaverick%20achieves%20state-of-the-art%20performance%20on%20the%20CoNLL-2012%20benchmark%2C%0Atraining%20with%20up%20to%200.006x%20the%20memory%20resources%20and%20obtaining%20a%20170x%20faster%0Ainference%20compared%20to%20previous%20state-of-the-art%20systems.%20We%20extensively%0Avalidate%20the%20robustness%20of%20the%20Maverick%20framework%20with%20an%20array%20of%20diverse%0Aexperiments%2C%20reporting%20improvements%20over%20prior%20systems%20in%20data-scarce%2C%0Along-document%2C%20and%20out-of-domain%20settings.%20We%20release%20our%20code%20and%20models%20for%0Aresearch%20purposes%20at%20https%3A//github.com/SapienzaNLP/maverick-coref.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21489v1&entry.124074799=Read"},
{"title": "Enhancing Interpretability of Vertebrae Fracture Grading using\n  Human-interpretable Prototypes", "author": "Poulami Sinhamahapatra and Suprosanna Shit and Anjany Sekuboyina and Malek Husseini and David Schinz and Nicolas Lenhart and Joern Menze and Jan Kirschke and Karsten Roscher and Stephan Guennemann", "abstract": "  Vertebral fracture grading classifies the severity of vertebral fractures,\nwhich is a challenging task in medical imaging and has recently attracted Deep\nLearning (DL) models. Only a few works attempted to make such models\nhuman-interpretable despite the need for transparency and trustworthiness in\ncritical use cases like DL-assisted medical diagnosis. Moreover, such models\neither rely on post-hoc methods or additional annotations. In this work, we\npropose a novel interpretable-by-design method, ProtoVerse, to find relevant\nsub-parts of vertebral fractures (prototypes) that reliably explain the model's\ndecision in a human-understandable way. Specifically, we introduce a novel\ndiversity-promoting loss to mitigate prototype repetitions in small datasets\nwith intricate semantics. We have experimented with the VerSe'19 dataset and\noutperformed the existing prototype-based method. Further, our model provides\nsuperior interpretability against the post-hoc method. Importantly, expert\nradiologists validated the visual interpretability of our results, showing\nclinical applicability.\n", "link": "http://arxiv.org/abs/2404.02830v2", "date": "2024-07-31", "relevancy": 1.4638, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4927}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4886}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Interpretability%20of%20Vertebrae%20Fracture%20Grading%20using%0A%20%20Human-interpretable%20Prototypes&body=Title%3A%20Enhancing%20Interpretability%20of%20Vertebrae%20Fracture%20Grading%20using%0A%20%20Human-interpretable%20Prototypes%0AAuthor%3A%20Poulami%20Sinhamahapatra%20and%20Suprosanna%20Shit%20and%20Anjany%20Sekuboyina%20and%20Malek%20Husseini%20and%20David%20Schinz%20and%20Nicolas%20Lenhart%20and%20Joern%20Menze%20and%20Jan%20Kirschke%20and%20Karsten%20Roscher%20and%20Stephan%20Guennemann%0AAbstract%3A%20%20%20Vertebral%20fracture%20grading%20classifies%20the%20severity%20of%20vertebral%20fractures%2C%0Awhich%20is%20a%20challenging%20task%20in%20medical%20imaging%20and%20has%20recently%20attracted%20Deep%0ALearning%20%28DL%29%20models.%20Only%20a%20few%20works%20attempted%20to%20make%20such%20models%0Ahuman-interpretable%20despite%20the%20need%20for%20transparency%20and%20trustworthiness%20in%0Acritical%20use%20cases%20like%20DL-assisted%20medical%20diagnosis.%20Moreover%2C%20such%20models%0Aeither%20rely%20on%20post-hoc%20methods%20or%20additional%20annotations.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20interpretable-by-design%20method%2C%20ProtoVerse%2C%20to%20find%20relevant%0Asub-parts%20of%20vertebral%20fractures%20%28prototypes%29%20that%20reliably%20explain%20the%20model%27s%0Adecision%20in%20a%20human-understandable%20way.%20Specifically%2C%20we%20introduce%20a%20novel%0Adiversity-promoting%20loss%20to%20mitigate%20prototype%20repetitions%20in%20small%20datasets%0Awith%20intricate%20semantics.%20We%20have%20experimented%20with%20the%20VerSe%2719%20dataset%20and%0Aoutperformed%20the%20existing%20prototype-based%20method.%20Further%2C%20our%20model%20provides%0Asuperior%20interpretability%20against%20the%20post-hoc%20method.%20Importantly%2C%20expert%0Aradiologists%20validated%20the%20visual%20interpretability%20of%20our%20results%2C%20showing%0Aclinical%20applicability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02830v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Interpretability%2520of%2520Vertebrae%2520Fracture%2520Grading%2520using%250A%2520%2520Human-interpretable%2520Prototypes%26entry.906535625%3DPoulami%2520Sinhamahapatra%2520and%2520Suprosanna%2520Shit%2520and%2520Anjany%2520Sekuboyina%2520and%2520Malek%2520Husseini%2520and%2520David%2520Schinz%2520and%2520Nicolas%2520Lenhart%2520and%2520Joern%2520Menze%2520and%2520Jan%2520Kirschke%2520and%2520Karsten%2520Roscher%2520and%2520Stephan%2520Guennemann%26entry.1292438233%3D%2520%2520Vertebral%2520fracture%2520grading%2520classifies%2520the%2520severity%2520of%2520vertebral%2520fractures%252C%250Awhich%2520is%2520a%2520challenging%2520task%2520in%2520medical%2520imaging%2520and%2520has%2520recently%2520attracted%2520Deep%250ALearning%2520%2528DL%2529%2520models.%2520Only%2520a%2520few%2520works%2520attempted%2520to%2520make%2520such%2520models%250Ahuman-interpretable%2520despite%2520the%2520need%2520for%2520transparency%2520and%2520trustworthiness%2520in%250Acritical%2520use%2520cases%2520like%2520DL-assisted%2520medical%2520diagnosis.%2520Moreover%252C%2520such%2520models%250Aeither%2520rely%2520on%2520post-hoc%2520methods%2520or%2520additional%2520annotations.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520novel%2520interpretable-by-design%2520method%252C%2520ProtoVerse%252C%2520to%2520find%2520relevant%250Asub-parts%2520of%2520vertebral%2520fractures%2520%2528prototypes%2529%2520that%2520reliably%2520explain%2520the%2520model%2527s%250Adecision%2520in%2520a%2520human-understandable%2520way.%2520Specifically%252C%2520we%2520introduce%2520a%2520novel%250Adiversity-promoting%2520loss%2520to%2520mitigate%2520prototype%2520repetitions%2520in%2520small%2520datasets%250Awith%2520intricate%2520semantics.%2520We%2520have%2520experimented%2520with%2520the%2520VerSe%252719%2520dataset%2520and%250Aoutperformed%2520the%2520existing%2520prototype-based%2520method.%2520Further%252C%2520our%2520model%2520provides%250Asuperior%2520interpretability%2520against%2520the%2520post-hoc%2520method.%2520Importantly%252C%2520expert%250Aradiologists%2520validated%2520the%2520visual%2520interpretability%2520of%2520our%2520results%252C%2520showing%250Aclinical%2520applicability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.02830v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Interpretability%20of%20Vertebrae%20Fracture%20Grading%20using%0A%20%20Human-interpretable%20Prototypes&entry.906535625=Poulami%20Sinhamahapatra%20and%20Suprosanna%20Shit%20and%20Anjany%20Sekuboyina%20and%20Malek%20Husseini%20and%20David%20Schinz%20and%20Nicolas%20Lenhart%20and%20Joern%20Menze%20and%20Jan%20Kirschke%20and%20Karsten%20Roscher%20and%20Stephan%20Guennemann&entry.1292438233=%20%20Vertebral%20fracture%20grading%20classifies%20the%20severity%20of%20vertebral%20fractures%2C%0Awhich%20is%20a%20challenging%20task%20in%20medical%20imaging%20and%20has%20recently%20attracted%20Deep%0ALearning%20%28DL%29%20models.%20Only%20a%20few%20works%20attempted%20to%20make%20such%20models%0Ahuman-interpretable%20despite%20the%20need%20for%20transparency%20and%20trustworthiness%20in%0Acritical%20use%20cases%20like%20DL-assisted%20medical%20diagnosis.%20Moreover%2C%20such%20models%0Aeither%20rely%20on%20post-hoc%20methods%20or%20additional%20annotations.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20interpretable-by-design%20method%2C%20ProtoVerse%2C%20to%20find%20relevant%0Asub-parts%20of%20vertebral%20fractures%20%28prototypes%29%20that%20reliably%20explain%20the%20model%27s%0Adecision%20in%20a%20human-understandable%20way.%20Specifically%2C%20we%20introduce%20a%20novel%0Adiversity-promoting%20loss%20to%20mitigate%20prototype%20repetitions%20in%20small%20datasets%0Awith%20intricate%20semantics.%20We%20have%20experimented%20with%20the%20VerSe%2719%20dataset%20and%0Aoutperformed%20the%20existing%20prototype-based%20method.%20Further%2C%20our%20model%20provides%0Asuperior%20interpretability%20against%20the%20post-hoc%20method.%20Importantly%2C%20expert%0Aradiologists%20validated%20the%20visual%20interpretability%20of%20our%20results%2C%20showing%0Aclinical%20applicability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02830v2&entry.124074799=Read"},
{"title": "Pediatric Wrist Fracture Detection in X-rays via YOLOv10 Algorithm and\n  Dual Label Assignment System", "author": "Ammar Ahmed and Abdul Manaf", "abstract": "  Wrist fractures are highly prevalent among children and can significantly\nimpact their daily activities, such as attending school, participating in\nsports, and performing basic self-care tasks. If not treated properly, these\nfractures can result in chronic pain, reduced wrist functionality, and other\nlong-term complications. Recently, advancements in object detection have shown\npromise in enhancing fracture detection, with systems achieving accuracy\ncomparable to, or even surpassing, that of human radiologists. The YOLO series,\nin particular, has demonstrated notable success in this domain. This study is\nthe first to provide a thorough evaluation of various YOLOv10 variants to\nassess their performance in detecting pediatric wrist fractures using the\nGRAZPEDWRI-DX dataset. It investigates how changes in model complexity, scaling\nthe architecture, and implementing a dual-label assignment strategy can enhance\ndetection performance. Experimental results indicate that our trained model\nachieved mean average precision (mAP@50-95) of 51.9\\% surpassing the current\nYOLOv9 benchmark of 43.3\\% on this dataset. This represents an improvement of\n8.6\\%. The implementation code is publicly available at\nhttps://github.com/ammarlodhi255/YOLOv10-Fracture-Detection\n", "link": "http://arxiv.org/abs/2407.15689v2", "date": "2024-07-31", "relevancy": 1.2957, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4556}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4301}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4232}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pediatric%20Wrist%20Fracture%20Detection%20in%20X-rays%20via%20YOLOv10%20Algorithm%20and%0A%20%20Dual%20Label%20Assignment%20System&body=Title%3A%20Pediatric%20Wrist%20Fracture%20Detection%20in%20X-rays%20via%20YOLOv10%20Algorithm%20and%0A%20%20Dual%20Label%20Assignment%20System%0AAuthor%3A%20Ammar%20Ahmed%20and%20Abdul%20Manaf%0AAbstract%3A%20%20%20Wrist%20fractures%20are%20highly%20prevalent%20among%20children%20and%20can%20significantly%0Aimpact%20their%20daily%20activities%2C%20such%20as%20attending%20school%2C%20participating%20in%0Asports%2C%20and%20performing%20basic%20self-care%20tasks.%20If%20not%20treated%20properly%2C%20these%0Afractures%20can%20result%20in%20chronic%20pain%2C%20reduced%20wrist%20functionality%2C%20and%20other%0Along-term%20complications.%20Recently%2C%20advancements%20in%20object%20detection%20have%20shown%0Apromise%20in%20enhancing%20fracture%20detection%2C%20with%20systems%20achieving%20accuracy%0Acomparable%20to%2C%20or%20even%20surpassing%2C%20that%20of%20human%20radiologists.%20The%20YOLO%20series%2C%0Ain%20particular%2C%20has%20demonstrated%20notable%20success%20in%20this%20domain.%20This%20study%20is%0Athe%20first%20to%20provide%20a%20thorough%20evaluation%20of%20various%20YOLOv10%20variants%20to%0Aassess%20their%20performance%20in%20detecting%20pediatric%20wrist%20fractures%20using%20the%0AGRAZPEDWRI-DX%20dataset.%20It%20investigates%20how%20changes%20in%20model%20complexity%2C%20scaling%0Athe%20architecture%2C%20and%20implementing%20a%20dual-label%20assignment%20strategy%20can%20enhance%0Adetection%20performance.%20Experimental%20results%20indicate%20that%20our%20trained%20model%0Aachieved%20mean%20average%20precision%20%28mAP%4050-95%29%20of%2051.9%5C%25%20surpassing%20the%20current%0AYOLOv9%20benchmark%20of%2043.3%5C%25%20on%20this%20dataset.%20This%20represents%20an%20improvement%20of%0A8.6%5C%25.%20The%20implementation%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/ammarlodhi255/YOLOv10-Fracture-Detection%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15689v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPediatric%2520Wrist%2520Fracture%2520Detection%2520in%2520X-rays%2520via%2520YOLOv10%2520Algorithm%2520and%250A%2520%2520Dual%2520Label%2520Assignment%2520System%26entry.906535625%3DAmmar%2520Ahmed%2520and%2520Abdul%2520Manaf%26entry.1292438233%3D%2520%2520Wrist%2520fractures%2520are%2520highly%2520prevalent%2520among%2520children%2520and%2520can%2520significantly%250Aimpact%2520their%2520daily%2520activities%252C%2520such%2520as%2520attending%2520school%252C%2520participating%2520in%250Asports%252C%2520and%2520performing%2520basic%2520self-care%2520tasks.%2520If%2520not%2520treated%2520properly%252C%2520these%250Afractures%2520can%2520result%2520in%2520chronic%2520pain%252C%2520reduced%2520wrist%2520functionality%252C%2520and%2520other%250Along-term%2520complications.%2520Recently%252C%2520advancements%2520in%2520object%2520detection%2520have%2520shown%250Apromise%2520in%2520enhancing%2520fracture%2520detection%252C%2520with%2520systems%2520achieving%2520accuracy%250Acomparable%2520to%252C%2520or%2520even%2520surpassing%252C%2520that%2520of%2520human%2520radiologists.%2520The%2520YOLO%2520series%252C%250Ain%2520particular%252C%2520has%2520demonstrated%2520notable%2520success%2520in%2520this%2520domain.%2520This%2520study%2520is%250Athe%2520first%2520to%2520provide%2520a%2520thorough%2520evaluation%2520of%2520various%2520YOLOv10%2520variants%2520to%250Aassess%2520their%2520performance%2520in%2520detecting%2520pediatric%2520wrist%2520fractures%2520using%2520the%250AGRAZPEDWRI-DX%2520dataset.%2520It%2520investigates%2520how%2520changes%2520in%2520model%2520complexity%252C%2520scaling%250Athe%2520architecture%252C%2520and%2520implementing%2520a%2520dual-label%2520assignment%2520strategy%2520can%2520enhance%250Adetection%2520performance.%2520Experimental%2520results%2520indicate%2520that%2520our%2520trained%2520model%250Aachieved%2520mean%2520average%2520precision%2520%2528mAP%254050-95%2529%2520of%252051.9%255C%2525%2520surpassing%2520the%2520current%250AYOLOv9%2520benchmark%2520of%252043.3%255C%2525%2520on%2520this%2520dataset.%2520This%2520represents%2520an%2520improvement%2520of%250A8.6%255C%2525.%2520The%2520implementation%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/ammarlodhi255/YOLOv10-Fracture-Detection%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15689v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pediatric%20Wrist%20Fracture%20Detection%20in%20X-rays%20via%20YOLOv10%20Algorithm%20and%0A%20%20Dual%20Label%20Assignment%20System&entry.906535625=Ammar%20Ahmed%20and%20Abdul%20Manaf&entry.1292438233=%20%20Wrist%20fractures%20are%20highly%20prevalent%20among%20children%20and%20can%20significantly%0Aimpact%20their%20daily%20activities%2C%20such%20as%20attending%20school%2C%20participating%20in%0Asports%2C%20and%20performing%20basic%20self-care%20tasks.%20If%20not%20treated%20properly%2C%20these%0Afractures%20can%20result%20in%20chronic%20pain%2C%20reduced%20wrist%20functionality%2C%20and%20other%0Along-term%20complications.%20Recently%2C%20advancements%20in%20object%20detection%20have%20shown%0Apromise%20in%20enhancing%20fracture%20detection%2C%20with%20systems%20achieving%20accuracy%0Acomparable%20to%2C%20or%20even%20surpassing%2C%20that%20of%20human%20radiologists.%20The%20YOLO%20series%2C%0Ain%20particular%2C%20has%20demonstrated%20notable%20success%20in%20this%20domain.%20This%20study%20is%0Athe%20first%20to%20provide%20a%20thorough%20evaluation%20of%20various%20YOLOv10%20variants%20to%0Aassess%20their%20performance%20in%20detecting%20pediatric%20wrist%20fractures%20using%20the%0AGRAZPEDWRI-DX%20dataset.%20It%20investigates%20how%20changes%20in%20model%20complexity%2C%20scaling%0Athe%20architecture%2C%20and%20implementing%20a%20dual-label%20assignment%20strategy%20can%20enhance%0Adetection%20performance.%20Experimental%20results%20indicate%20that%20our%20trained%20model%0Aachieved%20mean%20average%20precision%20%28mAP%4050-95%29%20of%2051.9%5C%25%20surpassing%20the%20current%0AYOLOv9%20benchmark%20of%2043.3%5C%25%20on%20this%20dataset.%20This%20represents%20an%20improvement%20of%0A8.6%5C%25.%20The%20implementation%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/ammarlodhi255/YOLOv10-Fracture-Detection%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15689v2&entry.124074799=Read"},
{"title": "Extended Fiducial Inference: Toward an Automated Process of Statistical\n  Inference", "author": "Faming Liang and Sehwan Kim and Yan Sun", "abstract": "  While fiducial inference was widely considered a big blunder by R.A. Fisher,\nthe goal he initially set --`inferring the uncertainty of model parameters on\nthe basis of observations' -- has been continually pursued by many\nstatisticians. To this end, we develop a new statistical inference method\ncalled extended Fiducial inference (EFI). The new method achieves the goal of\nfiducial inference by leveraging advanced statistical computing techniques\nwhile remaining scalable for big data. EFI involves jointly imputing random\nerrors realized in observations using stochastic gradient Markov chain Monte\nCarlo and estimating the inverse function using a sparse deep neural network\n(DNN). The consistency of the sparse DNN estimator ensures that the uncertainty\nembedded in observations is properly propagated to model parameters through the\nestimated inverse function, thereby validating downstream statistical\ninference. Compared to frequentist and Bayesian methods, EFI offers significant\nadvantages in parameter estimation and hypothesis testing. Specifically, EFI\nprovides higher fidelity in parameter estimation, especially when outliers are\npresent in the observations; and eliminates the need for theoretical reference\ndistributions in hypothesis testing, thereby automating the statistical\ninference process. EFI also provides an innovative framework for\nsemi-supervised learning.\n", "link": "http://arxiv.org/abs/2407.21622v1", "date": "2024-07-31", "relevancy": 1.4707, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5025}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5025}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extended%20Fiducial%20Inference%3A%20Toward%20an%20Automated%20Process%20of%20Statistical%0A%20%20Inference&body=Title%3A%20Extended%20Fiducial%20Inference%3A%20Toward%20an%20Automated%20Process%20of%20Statistical%0A%20%20Inference%0AAuthor%3A%20Faming%20Liang%20and%20Sehwan%20Kim%20and%20Yan%20Sun%0AAbstract%3A%20%20%20While%20fiducial%20inference%20was%20widely%20considered%20a%20big%20blunder%20by%20R.A.%20Fisher%2C%0Athe%20goal%20he%20initially%20set%20--%60inferring%20the%20uncertainty%20of%20model%20parameters%20on%0Athe%20basis%20of%20observations%27%20--%20has%20been%20continually%20pursued%20by%20many%0Astatisticians.%20To%20this%20end%2C%20we%20develop%20a%20new%20statistical%20inference%20method%0Acalled%20extended%20Fiducial%20inference%20%28EFI%29.%20The%20new%20method%20achieves%20the%20goal%20of%0Afiducial%20inference%20by%20leveraging%20advanced%20statistical%20computing%20techniques%0Awhile%20remaining%20scalable%20for%20big%20data.%20EFI%20involves%20jointly%20imputing%20random%0Aerrors%20realized%20in%20observations%20using%20stochastic%20gradient%20Markov%20chain%20Monte%0ACarlo%20and%20estimating%20the%20inverse%20function%20using%20a%20sparse%20deep%20neural%20network%0A%28DNN%29.%20The%20consistency%20of%20the%20sparse%20DNN%20estimator%20ensures%20that%20the%20uncertainty%0Aembedded%20in%20observations%20is%20properly%20propagated%20to%20model%20parameters%20through%20the%0Aestimated%20inverse%20function%2C%20thereby%20validating%20downstream%20statistical%0Ainference.%20Compared%20to%20frequentist%20and%20Bayesian%20methods%2C%20EFI%20offers%20significant%0Aadvantages%20in%20parameter%20estimation%20and%20hypothesis%20testing.%20Specifically%2C%20EFI%0Aprovides%20higher%20fidelity%20in%20parameter%20estimation%2C%20especially%20when%20outliers%20are%0Apresent%20in%20the%20observations%3B%20and%20eliminates%20the%20need%20for%20theoretical%20reference%0Adistributions%20in%20hypothesis%20testing%2C%20thereby%20automating%20the%20statistical%0Ainference%20process.%20EFI%20also%20provides%20an%20innovative%20framework%20for%0Asemi-supervised%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21622v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtended%2520Fiducial%2520Inference%253A%2520Toward%2520an%2520Automated%2520Process%2520of%2520Statistical%250A%2520%2520Inference%26entry.906535625%3DFaming%2520Liang%2520and%2520Sehwan%2520Kim%2520and%2520Yan%2520Sun%26entry.1292438233%3D%2520%2520While%2520fiducial%2520inference%2520was%2520widely%2520considered%2520a%2520big%2520blunder%2520by%2520R.A.%2520Fisher%252C%250Athe%2520goal%2520he%2520initially%2520set%2520--%2560inferring%2520the%2520uncertainty%2520of%2520model%2520parameters%2520on%250Athe%2520basis%2520of%2520observations%2527%2520--%2520has%2520been%2520continually%2520pursued%2520by%2520many%250Astatisticians.%2520To%2520this%2520end%252C%2520we%2520develop%2520a%2520new%2520statistical%2520inference%2520method%250Acalled%2520extended%2520Fiducial%2520inference%2520%2528EFI%2529.%2520The%2520new%2520method%2520achieves%2520the%2520goal%2520of%250Afiducial%2520inference%2520by%2520leveraging%2520advanced%2520statistical%2520computing%2520techniques%250Awhile%2520remaining%2520scalable%2520for%2520big%2520data.%2520EFI%2520involves%2520jointly%2520imputing%2520random%250Aerrors%2520realized%2520in%2520observations%2520using%2520stochastic%2520gradient%2520Markov%2520chain%2520Monte%250ACarlo%2520and%2520estimating%2520the%2520inverse%2520function%2520using%2520a%2520sparse%2520deep%2520neural%2520network%250A%2528DNN%2529.%2520The%2520consistency%2520of%2520the%2520sparse%2520DNN%2520estimator%2520ensures%2520that%2520the%2520uncertainty%250Aembedded%2520in%2520observations%2520is%2520properly%2520propagated%2520to%2520model%2520parameters%2520through%2520the%250Aestimated%2520inverse%2520function%252C%2520thereby%2520validating%2520downstream%2520statistical%250Ainference.%2520Compared%2520to%2520frequentist%2520and%2520Bayesian%2520methods%252C%2520EFI%2520offers%2520significant%250Aadvantages%2520in%2520parameter%2520estimation%2520and%2520hypothesis%2520testing.%2520Specifically%252C%2520EFI%250Aprovides%2520higher%2520fidelity%2520in%2520parameter%2520estimation%252C%2520especially%2520when%2520outliers%2520are%250Apresent%2520in%2520the%2520observations%253B%2520and%2520eliminates%2520the%2520need%2520for%2520theoretical%2520reference%250Adistributions%2520in%2520hypothesis%2520testing%252C%2520thereby%2520automating%2520the%2520statistical%250Ainference%2520process.%2520EFI%2520also%2520provides%2520an%2520innovative%2520framework%2520for%250Asemi-supervised%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21622v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extended%20Fiducial%20Inference%3A%20Toward%20an%20Automated%20Process%20of%20Statistical%0A%20%20Inference&entry.906535625=Faming%20Liang%20and%20Sehwan%20Kim%20and%20Yan%20Sun&entry.1292438233=%20%20While%20fiducial%20inference%20was%20widely%20considered%20a%20big%20blunder%20by%20R.A.%20Fisher%2C%0Athe%20goal%20he%20initially%20set%20--%60inferring%20the%20uncertainty%20of%20model%20parameters%20on%0Athe%20basis%20of%20observations%27%20--%20has%20been%20continually%20pursued%20by%20many%0Astatisticians.%20To%20this%20end%2C%20we%20develop%20a%20new%20statistical%20inference%20method%0Acalled%20extended%20Fiducial%20inference%20%28EFI%29.%20The%20new%20method%20achieves%20the%20goal%20of%0Afiducial%20inference%20by%20leveraging%20advanced%20statistical%20computing%20techniques%0Awhile%20remaining%20scalable%20for%20big%20data.%20EFI%20involves%20jointly%20imputing%20random%0Aerrors%20realized%20in%20observations%20using%20stochastic%20gradient%20Markov%20chain%20Monte%0ACarlo%20and%20estimating%20the%20inverse%20function%20using%20a%20sparse%20deep%20neural%20network%0A%28DNN%29.%20The%20consistency%20of%20the%20sparse%20DNN%20estimator%20ensures%20that%20the%20uncertainty%0Aembedded%20in%20observations%20is%20properly%20propagated%20to%20model%20parameters%20through%20the%0Aestimated%20inverse%20function%2C%20thereby%20validating%20downstream%20statistical%0Ainference.%20Compared%20to%20frequentist%20and%20Bayesian%20methods%2C%20EFI%20offers%20significant%0Aadvantages%20in%20parameter%20estimation%20and%20hypothesis%20testing.%20Specifically%2C%20EFI%0Aprovides%20higher%20fidelity%20in%20parameter%20estimation%2C%20especially%20when%20outliers%20are%0Apresent%20in%20the%20observations%3B%20and%20eliminates%20the%20need%20for%20theoretical%20reference%0Adistributions%20in%20hypothesis%20testing%2C%20thereby%20automating%20the%20statistical%0Ainference%20process.%20EFI%20also%20provides%20an%20innovative%20framework%20for%0Asemi-supervised%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21622v1&entry.124074799=Read"},
{"title": "Artificial Intelligence Approaches for Energy Efficiency: A Review", "author": "Alberto Pasqualetto and Lorenzo Serafini and Michele Sprocatti", "abstract": "  United Nations set Sustainable Development Goals and this paper focuses on\n7th (Affordable and Clean Energy), 9th (Industries, Innovation and\nInfrastructure), and 13th (Climate Action) goals. Climate change is a major\nconcern in our society; for this reason, a current global objective is to\nreduce energy waste. This work summarizes all main approaches towards energy\nefficiency using Artificial Intelligence with a particular focus on multi-agent\nsystems to create smart buildings. It mentions the tight relationship between\nAI, especially IoT, and Big Data. It explains the application of AI to anomaly\ndetection in smart buildings and a possible classification of Intelligent\nEnergy Management Systems: Direct and Indirect. Finally, some drawbacks of AI\napproaches and some possible future research focuses are proposed.\n", "link": "http://arxiv.org/abs/2407.21726v1", "date": "2024-07-31", "relevancy": 1.1585, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.3903}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3866}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Artificial%20Intelligence%20Approaches%20for%20Energy%20Efficiency%3A%20A%20Review&body=Title%3A%20Artificial%20Intelligence%20Approaches%20for%20Energy%20Efficiency%3A%20A%20Review%0AAuthor%3A%20Alberto%20Pasqualetto%20and%20Lorenzo%20Serafini%20and%20Michele%20Sprocatti%0AAbstract%3A%20%20%20United%20Nations%20set%20Sustainable%20Development%20Goals%20and%20this%20paper%20focuses%20on%0A7th%20%28Affordable%20and%20Clean%20Energy%29%2C%209th%20%28Industries%2C%20Innovation%20and%0AInfrastructure%29%2C%20and%2013th%20%28Climate%20Action%29%20goals.%20Climate%20change%20is%20a%20major%0Aconcern%20in%20our%20society%3B%20for%20this%20reason%2C%20a%20current%20global%20objective%20is%20to%0Areduce%20energy%20waste.%20This%20work%20summarizes%20all%20main%20approaches%20towards%20energy%0Aefficiency%20using%20Artificial%20Intelligence%20with%20a%20particular%20focus%20on%20multi-agent%0Asystems%20to%20create%20smart%20buildings.%20It%20mentions%20the%20tight%20relationship%20between%0AAI%2C%20especially%20IoT%2C%20and%20Big%20Data.%20It%20explains%20the%20application%20of%20AI%20to%20anomaly%0Adetection%20in%20smart%20buildings%20and%20a%20possible%20classification%20of%20Intelligent%0AEnergy%20Management%20Systems%3A%20Direct%20and%20Indirect.%20Finally%2C%20some%20drawbacks%20of%20AI%0Aapproaches%20and%20some%20possible%20future%20research%20focuses%20are%20proposed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21726v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArtificial%2520Intelligence%2520Approaches%2520for%2520Energy%2520Efficiency%253A%2520A%2520Review%26entry.906535625%3DAlberto%2520Pasqualetto%2520and%2520Lorenzo%2520Serafini%2520and%2520Michele%2520Sprocatti%26entry.1292438233%3D%2520%2520United%2520Nations%2520set%2520Sustainable%2520Development%2520Goals%2520and%2520this%2520paper%2520focuses%2520on%250A7th%2520%2528Affordable%2520and%2520Clean%2520Energy%2529%252C%25209th%2520%2528Industries%252C%2520Innovation%2520and%250AInfrastructure%2529%252C%2520and%252013th%2520%2528Climate%2520Action%2529%2520goals.%2520Climate%2520change%2520is%2520a%2520major%250Aconcern%2520in%2520our%2520society%253B%2520for%2520this%2520reason%252C%2520a%2520current%2520global%2520objective%2520is%2520to%250Areduce%2520energy%2520waste.%2520This%2520work%2520summarizes%2520all%2520main%2520approaches%2520towards%2520energy%250Aefficiency%2520using%2520Artificial%2520Intelligence%2520with%2520a%2520particular%2520focus%2520on%2520multi-agent%250Asystems%2520to%2520create%2520smart%2520buildings.%2520It%2520mentions%2520the%2520tight%2520relationship%2520between%250AAI%252C%2520especially%2520IoT%252C%2520and%2520Big%2520Data.%2520It%2520explains%2520the%2520application%2520of%2520AI%2520to%2520anomaly%250Adetection%2520in%2520smart%2520buildings%2520and%2520a%2520possible%2520classification%2520of%2520Intelligent%250AEnergy%2520Management%2520Systems%253A%2520Direct%2520and%2520Indirect.%2520Finally%252C%2520some%2520drawbacks%2520of%2520AI%250Aapproaches%2520and%2520some%2520possible%2520future%2520research%2520focuses%2520are%2520proposed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21726v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Artificial%20Intelligence%20Approaches%20for%20Energy%20Efficiency%3A%20A%20Review&entry.906535625=Alberto%20Pasqualetto%20and%20Lorenzo%20Serafini%20and%20Michele%20Sprocatti&entry.1292438233=%20%20United%20Nations%20set%20Sustainable%20Development%20Goals%20and%20this%20paper%20focuses%20on%0A7th%20%28Affordable%20and%20Clean%20Energy%29%2C%209th%20%28Industries%2C%20Innovation%20and%0AInfrastructure%29%2C%20and%2013th%20%28Climate%20Action%29%20goals.%20Climate%20change%20is%20a%20major%0Aconcern%20in%20our%20society%3B%20for%20this%20reason%2C%20a%20current%20global%20objective%20is%20to%0Areduce%20energy%20waste.%20This%20work%20summarizes%20all%20main%20approaches%20towards%20energy%0Aefficiency%20using%20Artificial%20Intelligence%20with%20a%20particular%20focus%20on%20multi-agent%0Asystems%20to%20create%20smart%20buildings.%20It%20mentions%20the%20tight%20relationship%20between%0AAI%2C%20especially%20IoT%2C%20and%20Big%20Data.%20It%20explains%20the%20application%20of%20AI%20to%20anomaly%0Adetection%20in%20smart%20buildings%20and%20a%20possible%20classification%20of%20Intelligent%0AEnergy%20Management%20Systems%3A%20Direct%20and%20Indirect.%20Finally%2C%20some%20drawbacks%20of%20AI%0Aapproaches%20and%20some%20possible%20future%20research%20focuses%20are%20proposed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21726v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


