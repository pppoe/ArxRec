<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20260105.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "360-GeoGS: Geometrically Consistent Feed-Forward 3D Gaussian Splatting Reconstruction for 360 Images", "author": "Jiaqi Yao and Zhongmiao Yan and Jingyi Xu and Songpengcheng Xia and Yan Xiang and Ling Pei", "abstract": "3D scene reconstruction is fundamental for spatial intelligence applications such as AR, robotics, and digital twins. Traditional multi-view stereo struggles with sparse viewpoints or low-texture regions, while neural rendering approaches, though capable of producing high-quality results, require per-scene optimization and lack real-time efficiency. Explicit 3D Gaussian Splatting (3DGS) enables efficient rendering, but most feed-forward variants focus on visual quality rather than geometric consistency, limiting accurate surface reconstruction and overall reliability in spatial perception tasks. This paper presents a novel feed-forward 3DGS framework for 360 images, capable of generating geometrically consistent Gaussian primitives while maintaining high rendering quality. A Depth-Normal geometric regularization is introduced to couple rendered depth gradients with normal information, supervising Gaussian rotation, scale, and position to improve point cloud and surface accuracy. Experimental results show that the proposed method maintains high rendering quality while significantly improving geometric consistency, providing an effective solution for 3D reconstruction in spatial perception tasks.", "link": "http://arxiv.org/abs/2601.02102v1", "date": "2026-01-05", "relevancy": 3.6037, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7493}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7144}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20360-GeoGS%3A%20Geometrically%20Consistent%20Feed-Forward%203D%20Gaussian%20Splatting%20Reconstruction%20for%20360%20Images&body=Title%3A%20360-GeoGS%3A%20Geometrically%20Consistent%20Feed-Forward%203D%20Gaussian%20Splatting%20Reconstruction%20for%20360%20Images%0AAuthor%3A%20Jiaqi%20Yao%20and%20Zhongmiao%20Yan%20and%20Jingyi%20Xu%20and%20Songpengcheng%20Xia%20and%20Yan%20Xiang%20and%20Ling%20Pei%0AAbstract%3A%203D%20scene%20reconstruction%20is%20fundamental%20for%20spatial%20intelligence%20applications%20such%20as%20AR%2C%20robotics%2C%20and%20digital%20twins.%20Traditional%20multi-view%20stereo%20struggles%20with%20sparse%20viewpoints%20or%20low-texture%20regions%2C%20while%20neural%20rendering%20approaches%2C%20though%20capable%20of%20producing%20high-quality%20results%2C%20require%20per-scene%20optimization%20and%20lack%20real-time%20efficiency.%20Explicit%203D%20Gaussian%20Splatting%20%283DGS%29%20enables%20efficient%20rendering%2C%20but%20most%20feed-forward%20variants%20focus%20on%20visual%20quality%20rather%20than%20geometric%20consistency%2C%20limiting%20accurate%20surface%20reconstruction%20and%20overall%20reliability%20in%20spatial%20perception%20tasks.%20This%20paper%20presents%20a%20novel%20feed-forward%203DGS%20framework%20for%20360%20images%2C%20capable%20of%20generating%20geometrically%20consistent%20Gaussian%20primitives%20while%20maintaining%20high%20rendering%20quality.%20A%20Depth-Normal%20geometric%20regularization%20is%20introduced%20to%20couple%20rendered%20depth%20gradients%20with%20normal%20information%2C%20supervising%20Gaussian%20rotation%2C%20scale%2C%20and%20position%20to%20improve%20point%20cloud%20and%20surface%20accuracy.%20Experimental%20results%20show%20that%20the%20proposed%20method%20maintains%20high%20rendering%20quality%20while%20significantly%20improving%20geometric%20consistency%2C%20providing%20an%20effective%20solution%20for%203D%20reconstruction%20in%20spatial%20perception%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02102v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D360-GeoGS%253A%2520Geometrically%2520Consistent%2520Feed-Forward%25203D%2520Gaussian%2520Splatting%2520Reconstruction%2520for%2520360%2520Images%26entry.906535625%3DJiaqi%2520Yao%2520and%2520Zhongmiao%2520Yan%2520and%2520Jingyi%2520Xu%2520and%2520Songpengcheng%2520Xia%2520and%2520Yan%2520Xiang%2520and%2520Ling%2520Pei%26entry.1292438233%3D3D%2520scene%2520reconstruction%2520is%2520fundamental%2520for%2520spatial%2520intelligence%2520applications%2520such%2520as%2520AR%252C%2520robotics%252C%2520and%2520digital%2520twins.%2520Traditional%2520multi-view%2520stereo%2520struggles%2520with%2520sparse%2520viewpoints%2520or%2520low-texture%2520regions%252C%2520while%2520neural%2520rendering%2520approaches%252C%2520though%2520capable%2520of%2520producing%2520high-quality%2520results%252C%2520require%2520per-scene%2520optimization%2520and%2520lack%2520real-time%2520efficiency.%2520Explicit%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520enables%2520efficient%2520rendering%252C%2520but%2520most%2520feed-forward%2520variants%2520focus%2520on%2520visual%2520quality%2520rather%2520than%2520geometric%2520consistency%252C%2520limiting%2520accurate%2520surface%2520reconstruction%2520and%2520overall%2520reliability%2520in%2520spatial%2520perception%2520tasks.%2520This%2520paper%2520presents%2520a%2520novel%2520feed-forward%25203DGS%2520framework%2520for%2520360%2520images%252C%2520capable%2520of%2520generating%2520geometrically%2520consistent%2520Gaussian%2520primitives%2520while%2520maintaining%2520high%2520rendering%2520quality.%2520A%2520Depth-Normal%2520geometric%2520regularization%2520is%2520introduced%2520to%2520couple%2520rendered%2520depth%2520gradients%2520with%2520normal%2520information%252C%2520supervising%2520Gaussian%2520rotation%252C%2520scale%252C%2520and%2520position%2520to%2520improve%2520point%2520cloud%2520and%2520surface%2520accuracy.%2520Experimental%2520results%2520show%2520that%2520the%2520proposed%2520method%2520maintains%2520high%2520rendering%2520quality%2520while%2520significantly%2520improving%2520geometric%2520consistency%252C%2520providing%2520an%2520effective%2520solution%2520for%25203D%2520reconstruction%2520in%2520spatial%2520perception%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02102v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=360-GeoGS%3A%20Geometrically%20Consistent%20Feed-Forward%203D%20Gaussian%20Splatting%20Reconstruction%20for%20360%20Images&entry.906535625=Jiaqi%20Yao%20and%20Zhongmiao%20Yan%20and%20Jingyi%20Xu%20and%20Songpengcheng%20Xia%20and%20Yan%20Xiang%20and%20Ling%20Pei&entry.1292438233=3D%20scene%20reconstruction%20is%20fundamental%20for%20spatial%20intelligence%20applications%20such%20as%20AR%2C%20robotics%2C%20and%20digital%20twins.%20Traditional%20multi-view%20stereo%20struggles%20with%20sparse%20viewpoints%20or%20low-texture%20regions%2C%20while%20neural%20rendering%20approaches%2C%20though%20capable%20of%20producing%20high-quality%20results%2C%20require%20per-scene%20optimization%20and%20lack%20real-time%20efficiency.%20Explicit%203D%20Gaussian%20Splatting%20%283DGS%29%20enables%20efficient%20rendering%2C%20but%20most%20feed-forward%20variants%20focus%20on%20visual%20quality%20rather%20than%20geometric%20consistency%2C%20limiting%20accurate%20surface%20reconstruction%20and%20overall%20reliability%20in%20spatial%20perception%20tasks.%20This%20paper%20presents%20a%20novel%20feed-forward%203DGS%20framework%20for%20360%20images%2C%20capable%20of%20generating%20geometrically%20consistent%20Gaussian%20primitives%20while%20maintaining%20high%20rendering%20quality.%20A%20Depth-Normal%20geometric%20regularization%20is%20introduced%20to%20couple%20rendered%20depth%20gradients%20with%20normal%20information%2C%20supervising%20Gaussian%20rotation%2C%20scale%2C%20and%20position%20to%20improve%20point%20cloud%20and%20surface%20accuracy.%20Experimental%20results%20show%20that%20the%20proposed%20method%20maintains%20high%20rendering%20quality%20while%20significantly%20improving%20geometric%20consistency%2C%20providing%20an%20effective%20solution%20for%203D%20reconstruction%20in%20spatial%20perception%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2601.02102v1&entry.124074799=Read"},
{"title": "PMGS: Reconstruction of Projectile Motion Across Large Spatiotemporal Spans via 3D Gaussian Splatting", "author": "Yijun Xu and Jingrui Zhang and Yuhan Chen and Dingwen Wang and Lei Yu and Chu He", "abstract": "Modeling complex rigid motion across large spatiotemporal spans remains an unresolved challenge in dynamic reconstruction. Existing paradigms are mainly confined to short-term, small-scale deformation and offer limited consideration for physical consistency. This study proposes PMGS, focusing on reconstructing Projectile Motion via 3D Gaussian Splatting. The workflow comprises two stages: 1) Target Modeling: achieving object-centralized reconstruction through dynamic scene decomposition and an improved point density control; 2) Motion Recovery: restoring full motion sequences by learning per-frame SE(3) poses. We introduce an acceleration consistency constraint to bridge Newtonian mechanics and pose estimation, and design a dynamic simulated annealing strategy that adaptively schedules learning rates based on motion states. Furthermore, we devise a Kalman fusion scheme to optimize error accumulation from multi-source observations to mitigate disturbances. Experiments show PMGS's superior performance in reconstructing high-speed nonlinear rigid motion compared to mainstream dynamic methods.", "link": "http://arxiv.org/abs/2508.02660v3", "date": "2026-01-05", "relevancy": 3.3917, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6813}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6796}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PMGS%3A%20Reconstruction%20of%20Projectile%20Motion%20Across%20Large%20Spatiotemporal%20Spans%20via%203D%20Gaussian%20Splatting&body=Title%3A%20PMGS%3A%20Reconstruction%20of%20Projectile%20Motion%20Across%20Large%20Spatiotemporal%20Spans%20via%203D%20Gaussian%20Splatting%0AAuthor%3A%20Yijun%20Xu%20and%20Jingrui%20Zhang%20and%20Yuhan%20Chen%20and%20Dingwen%20Wang%20and%20Lei%20Yu%20and%20Chu%20He%0AAbstract%3A%20Modeling%20complex%20rigid%20motion%20across%20large%20spatiotemporal%20spans%20remains%20an%20unresolved%20challenge%20in%20dynamic%20reconstruction.%20Existing%20paradigms%20are%20mainly%20confined%20to%20short-term%2C%20small-scale%20deformation%20and%20offer%20limited%20consideration%20for%20physical%20consistency.%20This%20study%20proposes%20PMGS%2C%20focusing%20on%20reconstructing%20Projectile%20Motion%20via%203D%20Gaussian%20Splatting.%20The%20workflow%20comprises%20two%20stages%3A%201%29%20Target%20Modeling%3A%20achieving%20object-centralized%20reconstruction%20through%20dynamic%20scene%20decomposition%20and%20an%20improved%20point%20density%20control%3B%202%29%20Motion%20Recovery%3A%20restoring%20full%20motion%20sequences%20by%20learning%20per-frame%20SE%283%29%20poses.%20We%20introduce%20an%20acceleration%20consistency%20constraint%20to%20bridge%20Newtonian%20mechanics%20and%20pose%20estimation%2C%20and%20design%20a%20dynamic%20simulated%20annealing%20strategy%20that%20adaptively%20schedules%20learning%20rates%20based%20on%20motion%20states.%20Furthermore%2C%20we%20devise%20a%20Kalman%20fusion%20scheme%20to%20optimize%20error%20accumulation%20from%20multi-source%20observations%20to%20mitigate%20disturbances.%20Experiments%20show%20PMGS%27s%20superior%20performance%20in%20reconstructing%20high-speed%20nonlinear%20rigid%20motion%20compared%20to%20mainstream%20dynamic%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2508.02660v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPMGS%253A%2520Reconstruction%2520of%2520Projectile%2520Motion%2520Across%2520Large%2520Spatiotemporal%2520Spans%2520via%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DYijun%2520Xu%2520and%2520Jingrui%2520Zhang%2520and%2520Yuhan%2520Chen%2520and%2520Dingwen%2520Wang%2520and%2520Lei%2520Yu%2520and%2520Chu%2520He%26entry.1292438233%3DModeling%2520complex%2520rigid%2520motion%2520across%2520large%2520spatiotemporal%2520spans%2520remains%2520an%2520unresolved%2520challenge%2520in%2520dynamic%2520reconstruction.%2520Existing%2520paradigms%2520are%2520mainly%2520confined%2520to%2520short-term%252C%2520small-scale%2520deformation%2520and%2520offer%2520limited%2520consideration%2520for%2520physical%2520consistency.%2520This%2520study%2520proposes%2520PMGS%252C%2520focusing%2520on%2520reconstructing%2520Projectile%2520Motion%2520via%25203D%2520Gaussian%2520Splatting.%2520The%2520workflow%2520comprises%2520two%2520stages%253A%25201%2529%2520Target%2520Modeling%253A%2520achieving%2520object-centralized%2520reconstruction%2520through%2520dynamic%2520scene%2520decomposition%2520and%2520an%2520improved%2520point%2520density%2520control%253B%25202%2529%2520Motion%2520Recovery%253A%2520restoring%2520full%2520motion%2520sequences%2520by%2520learning%2520per-frame%2520SE%25283%2529%2520poses.%2520We%2520introduce%2520an%2520acceleration%2520consistency%2520constraint%2520to%2520bridge%2520Newtonian%2520mechanics%2520and%2520pose%2520estimation%252C%2520and%2520design%2520a%2520dynamic%2520simulated%2520annealing%2520strategy%2520that%2520adaptively%2520schedules%2520learning%2520rates%2520based%2520on%2520motion%2520states.%2520Furthermore%252C%2520we%2520devise%2520a%2520Kalman%2520fusion%2520scheme%2520to%2520optimize%2520error%2520accumulation%2520from%2520multi-source%2520observations%2520to%2520mitigate%2520disturbances.%2520Experiments%2520show%2520PMGS%2527s%2520superior%2520performance%2520in%2520reconstructing%2520high-speed%2520nonlinear%2520rigid%2520motion%2520compared%2520to%2520mainstream%2520dynamic%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02660v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PMGS%3A%20Reconstruction%20of%20Projectile%20Motion%20Across%20Large%20Spatiotemporal%20Spans%20via%203D%20Gaussian%20Splatting&entry.906535625=Yijun%20Xu%20and%20Jingrui%20Zhang%20and%20Yuhan%20Chen%20and%20Dingwen%20Wang%20and%20Lei%20Yu%20and%20Chu%20He&entry.1292438233=Modeling%20complex%20rigid%20motion%20across%20large%20spatiotemporal%20spans%20remains%20an%20unresolved%20challenge%20in%20dynamic%20reconstruction.%20Existing%20paradigms%20are%20mainly%20confined%20to%20short-term%2C%20small-scale%20deformation%20and%20offer%20limited%20consideration%20for%20physical%20consistency.%20This%20study%20proposes%20PMGS%2C%20focusing%20on%20reconstructing%20Projectile%20Motion%20via%203D%20Gaussian%20Splatting.%20The%20workflow%20comprises%20two%20stages%3A%201%29%20Target%20Modeling%3A%20achieving%20object-centralized%20reconstruction%20through%20dynamic%20scene%20decomposition%20and%20an%20improved%20point%20density%20control%3B%202%29%20Motion%20Recovery%3A%20restoring%20full%20motion%20sequences%20by%20learning%20per-frame%20SE%283%29%20poses.%20We%20introduce%20an%20acceleration%20consistency%20constraint%20to%20bridge%20Newtonian%20mechanics%20and%20pose%20estimation%2C%20and%20design%20a%20dynamic%20simulated%20annealing%20strategy%20that%20adaptively%20schedules%20learning%20rates%20based%20on%20motion%20states.%20Furthermore%2C%20we%20devise%20a%20Kalman%20fusion%20scheme%20to%20optimize%20error%20accumulation%20from%20multi-source%20observations%20to%20mitigate%20disturbances.%20Experiments%20show%20PMGS%27s%20superior%20performance%20in%20reconstructing%20high-speed%20nonlinear%20rigid%20motion%20compared%20to%20mainstream%20dynamic%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2508.02660v3&entry.124074799=Read"},
{"title": "InpaintHuman: Reconstructing Occluded Humans with Multi-Scale UV Mapping and Identity-Preserving Diffusion Inpainting", "author": "Jinlong Fan and Shanshan Zhao and Liang Zheng and Jing Zhang and Yuxiang Yang and Mingming Gong", "abstract": "Reconstructing complete and animatable 3D human avatars from monocular videos remains challenging, particularly under severe occlusions. While 3D Gaussian Splatting has enabled photorealistic human rendering, existing methods struggle with incomplete observations, often producing corrupted geometry and temporal inconsistencies. We present InpaintHuman, a novel method for generating high-fidelity, complete, and animatable avatars from occluded monocular videos. Our approach introduces two key innovations: (i) a multi-scale UV-parameterized representation with hierarchical coarse-to-fine feature interpolation, enabling robust reconstruction of occluded regions while preserving geometric details; and (ii) an identity-preserving diffusion inpainting module that integrates textual inversion with semantic-conditioned guidance for subject-specific, temporally coherent completion. Unlike SDS-based methods, our approach employs direct pixel-level supervision to ensure identity fidelity. Experiments on synthetic benchmarks (PeopleSnapshot, ZJU-MoCap) and real-world scenarios (OcMotion) demonstrate competitive performance with consistent improvements in reconstruction quality across diverse poses and viewpoints.", "link": "http://arxiv.org/abs/2601.02098v1", "date": "2026-01-05", "relevancy": 3.2356, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6863}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.644}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InpaintHuman%3A%20Reconstructing%20Occluded%20Humans%20with%20Multi-Scale%20UV%20Mapping%20and%20Identity-Preserving%20Diffusion%20Inpainting&body=Title%3A%20InpaintHuman%3A%20Reconstructing%20Occluded%20Humans%20with%20Multi-Scale%20UV%20Mapping%20and%20Identity-Preserving%20Diffusion%20Inpainting%0AAuthor%3A%20Jinlong%20Fan%20and%20Shanshan%20Zhao%20and%20Liang%20Zheng%20and%20Jing%20Zhang%20and%20Yuxiang%20Yang%20and%20Mingming%20Gong%0AAbstract%3A%20Reconstructing%20complete%20and%20animatable%203D%20human%20avatars%20from%20monocular%20videos%20remains%20challenging%2C%20particularly%20under%20severe%20occlusions.%20While%203D%20Gaussian%20Splatting%20has%20enabled%20photorealistic%20human%20rendering%2C%20existing%20methods%20struggle%20with%20incomplete%20observations%2C%20often%20producing%20corrupted%20geometry%20and%20temporal%20inconsistencies.%20We%20present%20InpaintHuman%2C%20a%20novel%20method%20for%20generating%20high-fidelity%2C%20complete%2C%20and%20animatable%20avatars%20from%20occluded%20monocular%20videos.%20Our%20approach%20introduces%20two%20key%20innovations%3A%20%28i%29%20a%20multi-scale%20UV-parameterized%20representation%20with%20hierarchical%20coarse-to-fine%20feature%20interpolation%2C%20enabling%20robust%20reconstruction%20of%20occluded%20regions%20while%20preserving%20geometric%20details%3B%20and%20%28ii%29%20an%20identity-preserving%20diffusion%20inpainting%20module%20that%20integrates%20textual%20inversion%20with%20semantic-conditioned%20guidance%20for%20subject-specific%2C%20temporally%20coherent%20completion.%20Unlike%20SDS-based%20methods%2C%20our%20approach%20employs%20direct%20pixel-level%20supervision%20to%20ensure%20identity%20fidelity.%20Experiments%20on%20synthetic%20benchmarks%20%28PeopleSnapshot%2C%20ZJU-MoCap%29%20and%20real-world%20scenarios%20%28OcMotion%29%20demonstrate%20competitive%20performance%20with%20consistent%20improvements%20in%20reconstruction%20quality%20across%20diverse%20poses%20and%20viewpoints.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02098v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInpaintHuman%253A%2520Reconstructing%2520Occluded%2520Humans%2520with%2520Multi-Scale%2520UV%2520Mapping%2520and%2520Identity-Preserving%2520Diffusion%2520Inpainting%26entry.906535625%3DJinlong%2520Fan%2520and%2520Shanshan%2520Zhao%2520and%2520Liang%2520Zheng%2520and%2520Jing%2520Zhang%2520and%2520Yuxiang%2520Yang%2520and%2520Mingming%2520Gong%26entry.1292438233%3DReconstructing%2520complete%2520and%2520animatable%25203D%2520human%2520avatars%2520from%2520monocular%2520videos%2520remains%2520challenging%252C%2520particularly%2520under%2520severe%2520occlusions.%2520While%25203D%2520Gaussian%2520Splatting%2520has%2520enabled%2520photorealistic%2520human%2520rendering%252C%2520existing%2520methods%2520struggle%2520with%2520incomplete%2520observations%252C%2520often%2520producing%2520corrupted%2520geometry%2520and%2520temporal%2520inconsistencies.%2520We%2520present%2520InpaintHuman%252C%2520a%2520novel%2520method%2520for%2520generating%2520high-fidelity%252C%2520complete%252C%2520and%2520animatable%2520avatars%2520from%2520occluded%2520monocular%2520videos.%2520Our%2520approach%2520introduces%2520two%2520key%2520innovations%253A%2520%2528i%2529%2520a%2520multi-scale%2520UV-parameterized%2520representation%2520with%2520hierarchical%2520coarse-to-fine%2520feature%2520interpolation%252C%2520enabling%2520robust%2520reconstruction%2520of%2520occluded%2520regions%2520while%2520preserving%2520geometric%2520details%253B%2520and%2520%2528ii%2529%2520an%2520identity-preserving%2520diffusion%2520inpainting%2520module%2520that%2520integrates%2520textual%2520inversion%2520with%2520semantic-conditioned%2520guidance%2520for%2520subject-specific%252C%2520temporally%2520coherent%2520completion.%2520Unlike%2520SDS-based%2520methods%252C%2520our%2520approach%2520employs%2520direct%2520pixel-level%2520supervision%2520to%2520ensure%2520identity%2520fidelity.%2520Experiments%2520on%2520synthetic%2520benchmarks%2520%2528PeopleSnapshot%252C%2520ZJU-MoCap%2529%2520and%2520real-world%2520scenarios%2520%2528OcMotion%2529%2520demonstrate%2520competitive%2520performance%2520with%2520consistent%2520improvements%2520in%2520reconstruction%2520quality%2520across%2520diverse%2520poses%2520and%2520viewpoints.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02098v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InpaintHuman%3A%20Reconstructing%20Occluded%20Humans%20with%20Multi-Scale%20UV%20Mapping%20and%20Identity-Preserving%20Diffusion%20Inpainting&entry.906535625=Jinlong%20Fan%20and%20Shanshan%20Zhao%20and%20Liang%20Zheng%20and%20Jing%20Zhang%20and%20Yuxiang%20Yang%20and%20Mingming%20Gong&entry.1292438233=Reconstructing%20complete%20and%20animatable%203D%20human%20avatars%20from%20monocular%20videos%20remains%20challenging%2C%20particularly%20under%20severe%20occlusions.%20While%203D%20Gaussian%20Splatting%20has%20enabled%20photorealistic%20human%20rendering%2C%20existing%20methods%20struggle%20with%20incomplete%20observations%2C%20often%20producing%20corrupted%20geometry%20and%20temporal%20inconsistencies.%20We%20present%20InpaintHuman%2C%20a%20novel%20method%20for%20generating%20high-fidelity%2C%20complete%2C%20and%20animatable%20avatars%20from%20occluded%20monocular%20videos.%20Our%20approach%20introduces%20two%20key%20innovations%3A%20%28i%29%20a%20multi-scale%20UV-parameterized%20representation%20with%20hierarchical%20coarse-to-fine%20feature%20interpolation%2C%20enabling%20robust%20reconstruction%20of%20occluded%20regions%20while%20preserving%20geometric%20details%3B%20and%20%28ii%29%20an%20identity-preserving%20diffusion%20inpainting%20module%20that%20integrates%20textual%20inversion%20with%20semantic-conditioned%20guidance%20for%20subject-specific%2C%20temporally%20coherent%20completion.%20Unlike%20SDS-based%20methods%2C%20our%20approach%20employs%20direct%20pixel-level%20supervision%20to%20ensure%20identity%20fidelity.%20Experiments%20on%20synthetic%20benchmarks%20%28PeopleSnapshot%2C%20ZJU-MoCap%29%20and%20real-world%20scenarios%20%28OcMotion%29%20demonstrate%20competitive%20performance%20with%20consistent%20improvements%20in%20reconstruction%20quality%20across%20diverse%20poses%20and%20viewpoints.&entry.1838667208=http%3A//arxiv.org/abs/2601.02098v1&entry.124074799=Read"},
{"title": "Joint Semantic and Rendering Enhancements in 3D Gaussian Modeling with Anisotropic Local Encoding", "author": "Jingming He and Chongyi Li and Shiqi Wang and Sam Kwong", "abstract": "Recent works propose extending 3DGS with semantic feature vectors for simultaneous semantic segmentation and image rendering. However, these methods often treat the semantic and rendering branches separately, relying solely on 2D supervision while ignoring the 3D Gaussian geometry. Moreover, current adaptive strategies adapt the Gaussian set depending solely on rendering gradients, which can be insufficient in subtle or textureless regions. In this work, we propose a joint enhancement framework for 3D semantic Gaussian modeling that synergizes both semantic and rendering branches. Firstly, unlike conventional point cloud shape encoding, we introduce an anisotropic 3D Gaussian Chebyshev descriptor using the Laplace-Beltrami operator to capture fine-grained 3D shape details, thereby distinguishing objects with similar appearances and reducing reliance on potentially noisy 2D guidance. In addition, without relying solely on rendering gradient, we adaptively adjust Gaussian allocation and spherical harmonics with local semantic and shape signals, enhancing rendering efficiency through selective resource allocation. Finally, we employ a cross-scene knowledge transfer module to continuously update learned shape patterns, enabling faster convergence and robust representations without relearning shape information from scratch for each new scene. Experiments on multiple datasets demonstrate improvements in segmentation accuracy and rendering quality while maintaining high rendering frame rates.", "link": "http://arxiv.org/abs/2601.02339v1", "date": "2026-01-05", "relevancy": 3.1181, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6475}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6186}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6048}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%20Semantic%20and%20Rendering%20Enhancements%20in%203D%20Gaussian%20Modeling%20with%20Anisotropic%20Local%20Encoding&body=Title%3A%20Joint%20Semantic%20and%20Rendering%20Enhancements%20in%203D%20Gaussian%20Modeling%20with%20Anisotropic%20Local%20Encoding%0AAuthor%3A%20Jingming%20He%20and%20Chongyi%20Li%20and%20Shiqi%20Wang%20and%20Sam%20Kwong%0AAbstract%3A%20Recent%20works%20propose%20extending%203DGS%20with%20semantic%20feature%20vectors%20for%20simultaneous%20semantic%20segmentation%20and%20image%20rendering.%20However%2C%20these%20methods%20often%20treat%20the%20semantic%20and%20rendering%20branches%20separately%2C%20relying%20solely%20on%202D%20supervision%20while%20ignoring%20the%203D%20Gaussian%20geometry.%20Moreover%2C%20current%20adaptive%20strategies%20adapt%20the%20Gaussian%20set%20depending%20solely%20on%20rendering%20gradients%2C%20which%20can%20be%20insufficient%20in%20subtle%20or%20textureless%20regions.%20In%20this%20work%2C%20we%20propose%20a%20joint%20enhancement%20framework%20for%203D%20semantic%20Gaussian%20modeling%20that%20synergizes%20both%20semantic%20and%20rendering%20branches.%20Firstly%2C%20unlike%20conventional%20point%20cloud%20shape%20encoding%2C%20we%20introduce%20an%20anisotropic%203D%20Gaussian%20Chebyshev%20descriptor%20using%20the%20Laplace-Beltrami%20operator%20to%20capture%20fine-grained%203D%20shape%20details%2C%20thereby%20distinguishing%20objects%20with%20similar%20appearances%20and%20reducing%20reliance%20on%20potentially%20noisy%202D%20guidance.%20In%20addition%2C%20without%20relying%20solely%20on%20rendering%20gradient%2C%20we%20adaptively%20adjust%20Gaussian%20allocation%20and%20spherical%20harmonics%20with%20local%20semantic%20and%20shape%20signals%2C%20enhancing%20rendering%20efficiency%20through%20selective%20resource%20allocation.%20Finally%2C%20we%20employ%20a%20cross-scene%20knowledge%20transfer%20module%20to%20continuously%20update%20learned%20shape%20patterns%2C%20enabling%20faster%20convergence%20and%20robust%20representations%20without%20relearning%20shape%20information%20from%20scratch%20for%20each%20new%20scene.%20Experiments%20on%20multiple%20datasets%20demonstrate%20improvements%20in%20segmentation%20accuracy%20and%20rendering%20quality%20while%20maintaining%20high%20rendering%20frame%20rates.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02339v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%2520Semantic%2520and%2520Rendering%2520Enhancements%2520in%25203D%2520Gaussian%2520Modeling%2520with%2520Anisotropic%2520Local%2520Encoding%26entry.906535625%3DJingming%2520He%2520and%2520Chongyi%2520Li%2520and%2520Shiqi%2520Wang%2520and%2520Sam%2520Kwong%26entry.1292438233%3DRecent%2520works%2520propose%2520extending%25203DGS%2520with%2520semantic%2520feature%2520vectors%2520for%2520simultaneous%2520semantic%2520segmentation%2520and%2520image%2520rendering.%2520However%252C%2520these%2520methods%2520often%2520treat%2520the%2520semantic%2520and%2520rendering%2520branches%2520separately%252C%2520relying%2520solely%2520on%25202D%2520supervision%2520while%2520ignoring%2520the%25203D%2520Gaussian%2520geometry.%2520Moreover%252C%2520current%2520adaptive%2520strategies%2520adapt%2520the%2520Gaussian%2520set%2520depending%2520solely%2520on%2520rendering%2520gradients%252C%2520which%2520can%2520be%2520insufficient%2520in%2520subtle%2520or%2520textureless%2520regions.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520joint%2520enhancement%2520framework%2520for%25203D%2520semantic%2520Gaussian%2520modeling%2520that%2520synergizes%2520both%2520semantic%2520and%2520rendering%2520branches.%2520Firstly%252C%2520unlike%2520conventional%2520point%2520cloud%2520shape%2520encoding%252C%2520we%2520introduce%2520an%2520anisotropic%25203D%2520Gaussian%2520Chebyshev%2520descriptor%2520using%2520the%2520Laplace-Beltrami%2520operator%2520to%2520capture%2520fine-grained%25203D%2520shape%2520details%252C%2520thereby%2520distinguishing%2520objects%2520with%2520similar%2520appearances%2520and%2520reducing%2520reliance%2520on%2520potentially%2520noisy%25202D%2520guidance.%2520In%2520addition%252C%2520without%2520relying%2520solely%2520on%2520rendering%2520gradient%252C%2520we%2520adaptively%2520adjust%2520Gaussian%2520allocation%2520and%2520spherical%2520harmonics%2520with%2520local%2520semantic%2520and%2520shape%2520signals%252C%2520enhancing%2520rendering%2520efficiency%2520through%2520selective%2520resource%2520allocation.%2520Finally%252C%2520we%2520employ%2520a%2520cross-scene%2520knowledge%2520transfer%2520module%2520to%2520continuously%2520update%2520learned%2520shape%2520patterns%252C%2520enabling%2520faster%2520convergence%2520and%2520robust%2520representations%2520without%2520relearning%2520shape%2520information%2520from%2520scratch%2520for%2520each%2520new%2520scene.%2520Experiments%2520on%2520multiple%2520datasets%2520demonstrate%2520improvements%2520in%2520segmentation%2520accuracy%2520and%2520rendering%2520quality%2520while%2520maintaining%2520high%2520rendering%2520frame%2520rates.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02339v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Semantic%20and%20Rendering%20Enhancements%20in%203D%20Gaussian%20Modeling%20with%20Anisotropic%20Local%20Encoding&entry.906535625=Jingming%20He%20and%20Chongyi%20Li%20and%20Shiqi%20Wang%20and%20Sam%20Kwong&entry.1292438233=Recent%20works%20propose%20extending%203DGS%20with%20semantic%20feature%20vectors%20for%20simultaneous%20semantic%20segmentation%20and%20image%20rendering.%20However%2C%20these%20methods%20often%20treat%20the%20semantic%20and%20rendering%20branches%20separately%2C%20relying%20solely%20on%202D%20supervision%20while%20ignoring%20the%203D%20Gaussian%20geometry.%20Moreover%2C%20current%20adaptive%20strategies%20adapt%20the%20Gaussian%20set%20depending%20solely%20on%20rendering%20gradients%2C%20which%20can%20be%20insufficient%20in%20subtle%20or%20textureless%20regions.%20In%20this%20work%2C%20we%20propose%20a%20joint%20enhancement%20framework%20for%203D%20semantic%20Gaussian%20modeling%20that%20synergizes%20both%20semantic%20and%20rendering%20branches.%20Firstly%2C%20unlike%20conventional%20point%20cloud%20shape%20encoding%2C%20we%20introduce%20an%20anisotropic%203D%20Gaussian%20Chebyshev%20descriptor%20using%20the%20Laplace-Beltrami%20operator%20to%20capture%20fine-grained%203D%20shape%20details%2C%20thereby%20distinguishing%20objects%20with%20similar%20appearances%20and%20reducing%20reliance%20on%20potentially%20noisy%202D%20guidance.%20In%20addition%2C%20without%20relying%20solely%20on%20rendering%20gradient%2C%20we%20adaptively%20adjust%20Gaussian%20allocation%20and%20spherical%20harmonics%20with%20local%20semantic%20and%20shape%20signals%2C%20enhancing%20rendering%20efficiency%20through%20selective%20resource%20allocation.%20Finally%2C%20we%20employ%20a%20cross-scene%20knowledge%20transfer%20module%20to%20continuously%20update%20learned%20shape%20patterns%2C%20enabling%20faster%20convergence%20and%20robust%20representations%20without%20relearning%20shape%20information%20from%20scratch%20for%20each%20new%20scene.%20Experiments%20on%20multiple%20datasets%20demonstrate%20improvements%20in%20segmentation%20accuracy%20and%20rendering%20quality%20while%20maintaining%20high%20rendering%20frame%20rates.&entry.1838667208=http%3A//arxiv.org/abs/2601.02339v1&entry.124074799=Read"},
{"title": "DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies", "author": "Renke Wang and Zhenyu Zhang and Ying Tai and Jian Yang", "abstract": "Human mesh recovery from multi-view images faces a fundamental challenge: real-world datasets contain imperfect ground-truth annotations that bias the models' training, while synthetic data with precise supervision suffers from domain gap. In this paper, we propose DiffProxy, a novel framework that generates multi-view consistent human proxies for mesh recovery. Central to DiffProxy is leveraging the diffusion-based generative priors to bridge the synthetic training and real-world generalization. Its key innovations include: (1) a multi-conditional mechanism for generating multi-view consistent, pixel-aligned human proxies; (2) a hand refinement module that incorporates flexible visual prompts to enhance local details; and (3) an uncertainty-aware test-time scaling method that increases robustness to challenging cases during optimization. These designs ensure that the mesh recovery process effectively benefits from the precise synthetic ground truth and generative advantages of the diffusion-based pipeline. Trained entirely on synthetic data, DiffProxy achieves state-of-the-art performance across five real-world benchmarks, demonstrating strong zero-shot generalization particularly on challenging scenarios with occlusions and partial views. Project page: https://wrk226.github.io/DiffProxy.html", "link": "http://arxiv.org/abs/2601.02267v1", "date": "2026-01-05", "relevancy": 3.0857, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6367}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6189}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5958}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffProxy%3A%20Multi-View%20Human%20Mesh%20Recovery%20via%20Diffusion-Generated%20Dense%20Proxies&body=Title%3A%20DiffProxy%3A%20Multi-View%20Human%20Mesh%20Recovery%20via%20Diffusion-Generated%20Dense%20Proxies%0AAuthor%3A%20Renke%20Wang%20and%20Zhenyu%20Zhang%20and%20Ying%20Tai%20and%20Jian%20Yang%0AAbstract%3A%20Human%20mesh%20recovery%20from%20multi-view%20images%20faces%20a%20fundamental%20challenge%3A%20real-world%20datasets%20contain%20imperfect%20ground-truth%20annotations%20that%20bias%20the%20models%27%20training%2C%20while%20synthetic%20data%20with%20precise%20supervision%20suffers%20from%20domain%20gap.%20In%20this%20paper%2C%20we%20propose%20DiffProxy%2C%20a%20novel%20framework%20that%20generates%20multi-view%20consistent%20human%20proxies%20for%20mesh%20recovery.%20Central%20to%20DiffProxy%20is%20leveraging%20the%20diffusion-based%20generative%20priors%20to%20bridge%20the%20synthetic%20training%20and%20real-world%20generalization.%20Its%20key%20innovations%20include%3A%20%281%29%20a%20multi-conditional%20mechanism%20for%20generating%20multi-view%20consistent%2C%20pixel-aligned%20human%20proxies%3B%20%282%29%20a%20hand%20refinement%20module%20that%20incorporates%20flexible%20visual%20prompts%20to%20enhance%20local%20details%3B%20and%20%283%29%20an%20uncertainty-aware%20test-time%20scaling%20method%20that%20increases%20robustness%20to%20challenging%20cases%20during%20optimization.%20These%20designs%20ensure%20that%20the%20mesh%20recovery%20process%20effectively%20benefits%20from%20the%20precise%20synthetic%20ground%20truth%20and%20generative%20advantages%20of%20the%20diffusion-based%20pipeline.%20Trained%20entirely%20on%20synthetic%20data%2C%20DiffProxy%20achieves%20state-of-the-art%20performance%20across%20five%20real-world%20benchmarks%2C%20demonstrating%20strong%20zero-shot%20generalization%20particularly%20on%20challenging%20scenarios%20with%20occlusions%20and%20partial%20views.%20Project%20page%3A%20https%3A//wrk226.github.io/DiffProxy.html%0ALink%3A%20http%3A//arxiv.org/abs/2601.02267v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffProxy%253A%2520Multi-View%2520Human%2520Mesh%2520Recovery%2520via%2520Diffusion-Generated%2520Dense%2520Proxies%26entry.906535625%3DRenke%2520Wang%2520and%2520Zhenyu%2520Zhang%2520and%2520Ying%2520Tai%2520and%2520Jian%2520Yang%26entry.1292438233%3DHuman%2520mesh%2520recovery%2520from%2520multi-view%2520images%2520faces%2520a%2520fundamental%2520challenge%253A%2520real-world%2520datasets%2520contain%2520imperfect%2520ground-truth%2520annotations%2520that%2520bias%2520the%2520models%2527%2520training%252C%2520while%2520synthetic%2520data%2520with%2520precise%2520supervision%2520suffers%2520from%2520domain%2520gap.%2520In%2520this%2520paper%252C%2520we%2520propose%2520DiffProxy%252C%2520a%2520novel%2520framework%2520that%2520generates%2520multi-view%2520consistent%2520human%2520proxies%2520for%2520mesh%2520recovery.%2520Central%2520to%2520DiffProxy%2520is%2520leveraging%2520the%2520diffusion-based%2520generative%2520priors%2520to%2520bridge%2520the%2520synthetic%2520training%2520and%2520real-world%2520generalization.%2520Its%2520key%2520innovations%2520include%253A%2520%25281%2529%2520a%2520multi-conditional%2520mechanism%2520for%2520generating%2520multi-view%2520consistent%252C%2520pixel-aligned%2520human%2520proxies%253B%2520%25282%2529%2520a%2520hand%2520refinement%2520module%2520that%2520incorporates%2520flexible%2520visual%2520prompts%2520to%2520enhance%2520local%2520details%253B%2520and%2520%25283%2529%2520an%2520uncertainty-aware%2520test-time%2520scaling%2520method%2520that%2520increases%2520robustness%2520to%2520challenging%2520cases%2520during%2520optimization.%2520These%2520designs%2520ensure%2520that%2520the%2520mesh%2520recovery%2520process%2520effectively%2520benefits%2520from%2520the%2520precise%2520synthetic%2520ground%2520truth%2520and%2520generative%2520advantages%2520of%2520the%2520diffusion-based%2520pipeline.%2520Trained%2520entirely%2520on%2520synthetic%2520data%252C%2520DiffProxy%2520achieves%2520state-of-the-art%2520performance%2520across%2520five%2520real-world%2520benchmarks%252C%2520demonstrating%2520strong%2520zero-shot%2520generalization%2520particularly%2520on%2520challenging%2520scenarios%2520with%2520occlusions%2520and%2520partial%2520views.%2520Project%2520page%253A%2520https%253A//wrk226.github.io/DiffProxy.html%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02267v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffProxy%3A%20Multi-View%20Human%20Mesh%20Recovery%20via%20Diffusion-Generated%20Dense%20Proxies&entry.906535625=Renke%20Wang%20and%20Zhenyu%20Zhang%20and%20Ying%20Tai%20and%20Jian%20Yang&entry.1292438233=Human%20mesh%20recovery%20from%20multi-view%20images%20faces%20a%20fundamental%20challenge%3A%20real-world%20datasets%20contain%20imperfect%20ground-truth%20annotations%20that%20bias%20the%20models%27%20training%2C%20while%20synthetic%20data%20with%20precise%20supervision%20suffers%20from%20domain%20gap.%20In%20this%20paper%2C%20we%20propose%20DiffProxy%2C%20a%20novel%20framework%20that%20generates%20multi-view%20consistent%20human%20proxies%20for%20mesh%20recovery.%20Central%20to%20DiffProxy%20is%20leveraging%20the%20diffusion-based%20generative%20priors%20to%20bridge%20the%20synthetic%20training%20and%20real-world%20generalization.%20Its%20key%20innovations%20include%3A%20%281%29%20a%20multi-conditional%20mechanism%20for%20generating%20multi-view%20consistent%2C%20pixel-aligned%20human%20proxies%3B%20%282%29%20a%20hand%20refinement%20module%20that%20incorporates%20flexible%20visual%20prompts%20to%20enhance%20local%20details%3B%20and%20%283%29%20an%20uncertainty-aware%20test-time%20scaling%20method%20that%20increases%20robustness%20to%20challenging%20cases%20during%20optimization.%20These%20designs%20ensure%20that%20the%20mesh%20recovery%20process%20effectively%20benefits%20from%20the%20precise%20synthetic%20ground%20truth%20and%20generative%20advantages%20of%20the%20diffusion-based%20pipeline.%20Trained%20entirely%20on%20synthetic%20data%2C%20DiffProxy%20achieves%20state-of-the-art%20performance%20across%20five%20real-world%20benchmarks%2C%20demonstrating%20strong%20zero-shot%20generalization%20particularly%20on%20challenging%20scenarios%20with%20occlusions%20and%20partial%20views.%20Project%20page%3A%20https%3A//wrk226.github.io/DiffProxy.html&entry.1838667208=http%3A//arxiv.org/abs/2601.02267v1&entry.124074799=Read"},
{"title": "Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better", "author": "Dianyi Wang and Wei Song and Yikun Wang and Siyuan Wang and Kaicheng Yu and Zhongyu Wei and Jiaqi Wang", "abstract": "Typical large vision-language models (LVLMs) apply autoregressive supervision solely to textual sequences, without fully incorporating the visual modality into the learning process. This results in three key limitations: (1) an inability to utilize images without accompanying captions, (2) the risk that captions omit critical visual details, and (3) the challenge that certain vision-centric content cannot be adequately conveyed through text. As a result, current LVLMs often prioritize vision-to-language alignment while potentially overlooking fine-grained visual information. While some prior works have explored autoregressive image generation, effectively leveraging autoregressive visual supervision to enhance image understanding remains an open challenge. In this paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR), which enables joint learning of visual and textual modalities within a unified autoregressive framework. We show that autoregressively reconstructing the raw visual appearance of images does not enhance and may even impair multimodal understanding. In contrast, autoregressively reconstructing the semantic representation of images consistently improves comprehension. Notably, we find that even when models are given continuous image features as input, they can effectively reconstruct discrete semantic tokens, resulting in stable and consistent improvements across a wide range of multimodal understanding benchmarks. Our approach delivers significant performance gains across varying data scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is available at https://github.com/AlenjandroWang/ASVR.", "link": "http://arxiv.org/abs/2506.09040v2", "date": "2026-01-05", "relevancy": 3.0341, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6173}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6173}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autoregressive%20Semantic%20Visual%20Reconstruction%20Helps%20VLMs%20Understand%20Better&body=Title%3A%20Autoregressive%20Semantic%20Visual%20Reconstruction%20Helps%20VLMs%20Understand%20Better%0AAuthor%3A%20Dianyi%20Wang%20and%20Wei%20Song%20and%20Yikun%20Wang%20and%20Siyuan%20Wang%20and%20Kaicheng%20Yu%20and%20Zhongyu%20Wei%20and%20Jiaqi%20Wang%0AAbstract%3A%20Typical%20large%20vision-language%20models%20%28LVLMs%29%20apply%20autoregressive%20supervision%20solely%20to%20textual%20sequences%2C%20without%20fully%20incorporating%20the%20visual%20modality%20into%20the%20learning%20process.%20This%20results%20in%20three%20key%20limitations%3A%20%281%29%20an%20inability%20to%20utilize%20images%20without%20accompanying%20captions%2C%20%282%29%20the%20risk%20that%20captions%20omit%20critical%20visual%20details%2C%20and%20%283%29%20the%20challenge%20that%20certain%20vision-centric%20content%20cannot%20be%20adequately%20conveyed%20through%20text.%20As%20a%20result%2C%20current%20LVLMs%20often%20prioritize%20vision-to-language%20alignment%20while%20potentially%20overlooking%20fine-grained%20visual%20information.%20While%20some%20prior%20works%20have%20explored%20autoregressive%20image%20generation%2C%20effectively%20leveraging%20autoregressive%20visual%20supervision%20to%20enhance%20image%20understanding%20remains%20an%20open%20challenge.%20In%20this%20paper%2C%20we%20introduce%20Autoregressive%20Semantic%20Visual%20Reconstruction%20%28ASVR%29%2C%20which%20enables%20joint%20learning%20of%20visual%20and%20textual%20modalities%20within%20a%20unified%20autoregressive%20framework.%20We%20show%20that%20autoregressively%20reconstructing%20the%20raw%20visual%20appearance%20of%20images%20does%20not%20enhance%20and%20may%20even%20impair%20multimodal%20understanding.%20In%20contrast%2C%20autoregressively%20reconstructing%20the%20semantic%20representation%20of%20images%20consistently%20improves%20comprehension.%20Notably%2C%20we%20find%20that%20even%20when%20models%20are%20given%20continuous%20image%20features%20as%20input%2C%20they%20can%20effectively%20reconstruct%20discrete%20semantic%20tokens%2C%20resulting%20in%20stable%20and%20consistent%20improvements%20across%20a%20wide%20range%20of%20multimodal%20understanding%20benchmarks.%20Our%20approach%20delivers%20significant%20performance%20gains%20across%20varying%20data%20scales%20%28556k-2M%29%20and%20types%20of%20LLM%20bacbones.%20Specifically%2C%20ASVR%20improves%20LLaVA-1.5%20by%205%25%20in%20average%20scores%20across%2014%20multimodal%20benchmarks.%20The%20code%20is%20available%20at%20https%3A//github.com/AlenjandroWang/ASVR.%0ALink%3A%20http%3A//arxiv.org/abs/2506.09040v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoregressive%2520Semantic%2520Visual%2520Reconstruction%2520Helps%2520VLMs%2520Understand%2520Better%26entry.906535625%3DDianyi%2520Wang%2520and%2520Wei%2520Song%2520and%2520Yikun%2520Wang%2520and%2520Siyuan%2520Wang%2520and%2520Kaicheng%2520Yu%2520and%2520Zhongyu%2520Wei%2520and%2520Jiaqi%2520Wang%26entry.1292438233%3DTypical%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520apply%2520autoregressive%2520supervision%2520solely%2520to%2520textual%2520sequences%252C%2520without%2520fully%2520incorporating%2520the%2520visual%2520modality%2520into%2520the%2520learning%2520process.%2520This%2520results%2520in%2520three%2520key%2520limitations%253A%2520%25281%2529%2520an%2520inability%2520to%2520utilize%2520images%2520without%2520accompanying%2520captions%252C%2520%25282%2529%2520the%2520risk%2520that%2520captions%2520omit%2520critical%2520visual%2520details%252C%2520and%2520%25283%2529%2520the%2520challenge%2520that%2520certain%2520vision-centric%2520content%2520cannot%2520be%2520adequately%2520conveyed%2520through%2520text.%2520As%2520a%2520result%252C%2520current%2520LVLMs%2520often%2520prioritize%2520vision-to-language%2520alignment%2520while%2520potentially%2520overlooking%2520fine-grained%2520visual%2520information.%2520While%2520some%2520prior%2520works%2520have%2520explored%2520autoregressive%2520image%2520generation%252C%2520effectively%2520leveraging%2520autoregressive%2520visual%2520supervision%2520to%2520enhance%2520image%2520understanding%2520remains%2520an%2520open%2520challenge.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Autoregressive%2520Semantic%2520Visual%2520Reconstruction%2520%2528ASVR%2529%252C%2520which%2520enables%2520joint%2520learning%2520of%2520visual%2520and%2520textual%2520modalities%2520within%2520a%2520unified%2520autoregressive%2520framework.%2520We%2520show%2520that%2520autoregressively%2520reconstructing%2520the%2520raw%2520visual%2520appearance%2520of%2520images%2520does%2520not%2520enhance%2520and%2520may%2520even%2520impair%2520multimodal%2520understanding.%2520In%2520contrast%252C%2520autoregressively%2520reconstructing%2520the%2520semantic%2520representation%2520of%2520images%2520consistently%2520improves%2520comprehension.%2520Notably%252C%2520we%2520find%2520that%2520even%2520when%2520models%2520are%2520given%2520continuous%2520image%2520features%2520as%2520input%252C%2520they%2520can%2520effectively%2520reconstruct%2520discrete%2520semantic%2520tokens%252C%2520resulting%2520in%2520stable%2520and%2520consistent%2520improvements%2520across%2520a%2520wide%2520range%2520of%2520multimodal%2520understanding%2520benchmarks.%2520Our%2520approach%2520delivers%2520significant%2520performance%2520gains%2520across%2520varying%2520data%2520scales%2520%2528556k-2M%2529%2520and%2520types%2520of%2520LLM%2520bacbones.%2520Specifically%252C%2520ASVR%2520improves%2520LLaVA-1.5%2520by%25205%2525%2520in%2520average%2520scores%2520across%252014%2520multimodal%2520benchmarks.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/AlenjandroWang/ASVR.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09040v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autoregressive%20Semantic%20Visual%20Reconstruction%20Helps%20VLMs%20Understand%20Better&entry.906535625=Dianyi%20Wang%20and%20Wei%20Song%20and%20Yikun%20Wang%20and%20Siyuan%20Wang%20and%20Kaicheng%20Yu%20and%20Zhongyu%20Wei%20and%20Jiaqi%20Wang&entry.1292438233=Typical%20large%20vision-language%20models%20%28LVLMs%29%20apply%20autoregressive%20supervision%20solely%20to%20textual%20sequences%2C%20without%20fully%20incorporating%20the%20visual%20modality%20into%20the%20learning%20process.%20This%20results%20in%20three%20key%20limitations%3A%20%281%29%20an%20inability%20to%20utilize%20images%20without%20accompanying%20captions%2C%20%282%29%20the%20risk%20that%20captions%20omit%20critical%20visual%20details%2C%20and%20%283%29%20the%20challenge%20that%20certain%20vision-centric%20content%20cannot%20be%20adequately%20conveyed%20through%20text.%20As%20a%20result%2C%20current%20LVLMs%20often%20prioritize%20vision-to-language%20alignment%20while%20potentially%20overlooking%20fine-grained%20visual%20information.%20While%20some%20prior%20works%20have%20explored%20autoregressive%20image%20generation%2C%20effectively%20leveraging%20autoregressive%20visual%20supervision%20to%20enhance%20image%20understanding%20remains%20an%20open%20challenge.%20In%20this%20paper%2C%20we%20introduce%20Autoregressive%20Semantic%20Visual%20Reconstruction%20%28ASVR%29%2C%20which%20enables%20joint%20learning%20of%20visual%20and%20textual%20modalities%20within%20a%20unified%20autoregressive%20framework.%20We%20show%20that%20autoregressively%20reconstructing%20the%20raw%20visual%20appearance%20of%20images%20does%20not%20enhance%20and%20may%20even%20impair%20multimodal%20understanding.%20In%20contrast%2C%20autoregressively%20reconstructing%20the%20semantic%20representation%20of%20images%20consistently%20improves%20comprehension.%20Notably%2C%20we%20find%20that%20even%20when%20models%20are%20given%20continuous%20image%20features%20as%20input%2C%20they%20can%20effectively%20reconstruct%20discrete%20semantic%20tokens%2C%20resulting%20in%20stable%20and%20consistent%20improvements%20across%20a%20wide%20range%20of%20multimodal%20understanding%20benchmarks.%20Our%20approach%20delivers%20significant%20performance%20gains%20across%20varying%20data%20scales%20%28556k-2M%29%20and%20types%20of%20LLM%20bacbones.%20Specifically%2C%20ASVR%20improves%20LLaVA-1.5%20by%205%25%20in%20average%20scores%20across%2014%20multimodal%20benchmarks.%20The%20code%20is%20available%20at%20https%3A//github.com/AlenjandroWang/ASVR.&entry.1838667208=http%3A//arxiv.org/abs/2506.09040v2&entry.124074799=Read"},
{"title": "SLGNet: Synergizing Structural Priors and Language-Guided Modulation for Multimodal Object Detection", "author": "Xiantai Xiang and Guangyao Zhou and Zixiao Wen and Wenshuai Li and Ben Niu and Feng Wang and Lijia Huang and Qiantong Wang and Yuhan Liu and Zongxu Pan and Yuxin Hu", "abstract": "Multimodal object detection leveraging RGB and Infrared (IR) images is pivotal for robust perception in all-weather scenarios. While recent adapter-based approaches efficiently transfer RGB-pretrained foundation models to this task, they often prioritize model efficiency at the expense of cross-modal structural consistency. Consequently, critical structural cues are frequently lost when significant domain gaps arise, such as in high-contrast or nighttime environments. Moreover, conventional static multimodal fusion mechanisms typically lack environmental awareness, resulting in suboptimal adaptation and constrained detection performance under complex, dynamic scene variations. To address these limitations, we propose SLGNet, a parameter-efficient framework that synergizes hierarchical structural priors and language-guided modulation within a frozen Vision Transformer (ViT)-based foundation model. Specifically, we design a Structure-Aware Adapter to extract hierarchical structural representations from both modalities and dynamically inject them into the ViT to compensate for structural degradation inherent in ViT-based backbones. Furthermore, we propose a Language-Guided Modulation module that exploits VLM-driven structured captions to dynamically recalibrate visual features, thereby endowing the model with robust environmental awareness. Extensive experiments on the LLVIP, FLIR, KAIST, and DroneVehicle datasets demonstrate that SLGNet establishes new state-of-the-art performance. Notably, on the LLVIP benchmark, our method achieves an mAP of 66.1, while reducing trainable parameters by approximately 87% compared to traditional full fine-tuning. This confirms SLGNet as a robust and efficient solution for multimodal perception.", "link": "http://arxiv.org/abs/2601.02249v1", "date": "2026-01-05", "relevancy": 2.9958, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6339}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5852}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SLGNet%3A%20Synergizing%20Structural%20Priors%20and%20Language-Guided%20Modulation%20for%20Multimodal%20Object%20Detection&body=Title%3A%20SLGNet%3A%20Synergizing%20Structural%20Priors%20and%20Language-Guided%20Modulation%20for%20Multimodal%20Object%20Detection%0AAuthor%3A%20Xiantai%20Xiang%20and%20Guangyao%20Zhou%20and%20Zixiao%20Wen%20and%20Wenshuai%20Li%20and%20Ben%20Niu%20and%20Feng%20Wang%20and%20Lijia%20Huang%20and%20Qiantong%20Wang%20and%20Yuhan%20Liu%20and%20Zongxu%20Pan%20and%20Yuxin%20Hu%0AAbstract%3A%20Multimodal%20object%20detection%20leveraging%20RGB%20and%20Infrared%20%28IR%29%20images%20is%20pivotal%20for%20robust%20perception%20in%20all-weather%20scenarios.%20While%20recent%20adapter-based%20approaches%20efficiently%20transfer%20RGB-pretrained%20foundation%20models%20to%20this%20task%2C%20they%20often%20prioritize%20model%20efficiency%20at%20the%20expense%20of%20cross-modal%20structural%20consistency.%20Consequently%2C%20critical%20structural%20cues%20are%20frequently%20lost%20when%20significant%20domain%20gaps%20arise%2C%20such%20as%20in%20high-contrast%20or%20nighttime%20environments.%20Moreover%2C%20conventional%20static%20multimodal%20fusion%20mechanisms%20typically%20lack%20environmental%20awareness%2C%20resulting%20in%20suboptimal%20adaptation%20and%20constrained%20detection%20performance%20under%20complex%2C%20dynamic%20scene%20variations.%20To%20address%20these%20limitations%2C%20we%20propose%20SLGNet%2C%20a%20parameter-efficient%20framework%20that%20synergizes%20hierarchical%20structural%20priors%20and%20language-guided%20modulation%20within%20a%20frozen%20Vision%20Transformer%20%28ViT%29-based%20foundation%20model.%20Specifically%2C%20we%20design%20a%20Structure-Aware%20Adapter%20to%20extract%20hierarchical%20structural%20representations%20from%20both%20modalities%20and%20dynamically%20inject%20them%20into%20the%20ViT%20to%20compensate%20for%20structural%20degradation%20inherent%20in%20ViT-based%20backbones.%20Furthermore%2C%20we%20propose%20a%20Language-Guided%20Modulation%20module%20that%20exploits%20VLM-driven%20structured%20captions%20to%20dynamically%20recalibrate%20visual%20features%2C%20thereby%20endowing%20the%20model%20with%20robust%20environmental%20awareness.%20Extensive%20experiments%20on%20the%20LLVIP%2C%20FLIR%2C%20KAIST%2C%20and%20DroneVehicle%20datasets%20demonstrate%20that%20SLGNet%20establishes%20new%20state-of-the-art%20performance.%20Notably%2C%20on%20the%20LLVIP%20benchmark%2C%20our%20method%20achieves%20an%20mAP%20of%2066.1%2C%20while%20reducing%20trainable%20parameters%20by%20approximately%2087%25%20compared%20to%20traditional%20full%20fine-tuning.%20This%20confirms%20SLGNet%20as%20a%20robust%20and%20efficient%20solution%20for%20multimodal%20perception.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02249v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSLGNet%253A%2520Synergizing%2520Structural%2520Priors%2520and%2520Language-Guided%2520Modulation%2520for%2520Multimodal%2520Object%2520Detection%26entry.906535625%3DXiantai%2520Xiang%2520and%2520Guangyao%2520Zhou%2520and%2520Zixiao%2520Wen%2520and%2520Wenshuai%2520Li%2520and%2520Ben%2520Niu%2520and%2520Feng%2520Wang%2520and%2520Lijia%2520Huang%2520and%2520Qiantong%2520Wang%2520and%2520Yuhan%2520Liu%2520and%2520Zongxu%2520Pan%2520and%2520Yuxin%2520Hu%26entry.1292438233%3DMultimodal%2520object%2520detection%2520leveraging%2520RGB%2520and%2520Infrared%2520%2528IR%2529%2520images%2520is%2520pivotal%2520for%2520robust%2520perception%2520in%2520all-weather%2520scenarios.%2520While%2520recent%2520adapter-based%2520approaches%2520efficiently%2520transfer%2520RGB-pretrained%2520foundation%2520models%2520to%2520this%2520task%252C%2520they%2520often%2520prioritize%2520model%2520efficiency%2520at%2520the%2520expense%2520of%2520cross-modal%2520structural%2520consistency.%2520Consequently%252C%2520critical%2520structural%2520cues%2520are%2520frequently%2520lost%2520when%2520significant%2520domain%2520gaps%2520arise%252C%2520such%2520as%2520in%2520high-contrast%2520or%2520nighttime%2520environments.%2520Moreover%252C%2520conventional%2520static%2520multimodal%2520fusion%2520mechanisms%2520typically%2520lack%2520environmental%2520awareness%252C%2520resulting%2520in%2520suboptimal%2520adaptation%2520and%2520constrained%2520detection%2520performance%2520under%2520complex%252C%2520dynamic%2520scene%2520variations.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520SLGNet%252C%2520a%2520parameter-efficient%2520framework%2520that%2520synergizes%2520hierarchical%2520structural%2520priors%2520and%2520language-guided%2520modulation%2520within%2520a%2520frozen%2520Vision%2520Transformer%2520%2528ViT%2529-based%2520foundation%2520model.%2520Specifically%252C%2520we%2520design%2520a%2520Structure-Aware%2520Adapter%2520to%2520extract%2520hierarchical%2520structural%2520representations%2520from%2520both%2520modalities%2520and%2520dynamically%2520inject%2520them%2520into%2520the%2520ViT%2520to%2520compensate%2520for%2520structural%2520degradation%2520inherent%2520in%2520ViT-based%2520backbones.%2520Furthermore%252C%2520we%2520propose%2520a%2520Language-Guided%2520Modulation%2520module%2520that%2520exploits%2520VLM-driven%2520structured%2520captions%2520to%2520dynamically%2520recalibrate%2520visual%2520features%252C%2520thereby%2520endowing%2520the%2520model%2520with%2520robust%2520environmental%2520awareness.%2520Extensive%2520experiments%2520on%2520the%2520LLVIP%252C%2520FLIR%252C%2520KAIST%252C%2520and%2520DroneVehicle%2520datasets%2520demonstrate%2520that%2520SLGNet%2520establishes%2520new%2520state-of-the-art%2520performance.%2520Notably%252C%2520on%2520the%2520LLVIP%2520benchmark%252C%2520our%2520method%2520achieves%2520an%2520mAP%2520of%252066.1%252C%2520while%2520reducing%2520trainable%2520parameters%2520by%2520approximately%252087%2525%2520compared%2520to%2520traditional%2520full%2520fine-tuning.%2520This%2520confirms%2520SLGNet%2520as%2520a%2520robust%2520and%2520efficient%2520solution%2520for%2520multimodal%2520perception.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02249v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLGNet%3A%20Synergizing%20Structural%20Priors%20and%20Language-Guided%20Modulation%20for%20Multimodal%20Object%20Detection&entry.906535625=Xiantai%20Xiang%20and%20Guangyao%20Zhou%20and%20Zixiao%20Wen%20and%20Wenshuai%20Li%20and%20Ben%20Niu%20and%20Feng%20Wang%20and%20Lijia%20Huang%20and%20Qiantong%20Wang%20and%20Yuhan%20Liu%20and%20Zongxu%20Pan%20and%20Yuxin%20Hu&entry.1292438233=Multimodal%20object%20detection%20leveraging%20RGB%20and%20Infrared%20%28IR%29%20images%20is%20pivotal%20for%20robust%20perception%20in%20all-weather%20scenarios.%20While%20recent%20adapter-based%20approaches%20efficiently%20transfer%20RGB-pretrained%20foundation%20models%20to%20this%20task%2C%20they%20often%20prioritize%20model%20efficiency%20at%20the%20expense%20of%20cross-modal%20structural%20consistency.%20Consequently%2C%20critical%20structural%20cues%20are%20frequently%20lost%20when%20significant%20domain%20gaps%20arise%2C%20such%20as%20in%20high-contrast%20or%20nighttime%20environments.%20Moreover%2C%20conventional%20static%20multimodal%20fusion%20mechanisms%20typically%20lack%20environmental%20awareness%2C%20resulting%20in%20suboptimal%20adaptation%20and%20constrained%20detection%20performance%20under%20complex%2C%20dynamic%20scene%20variations.%20To%20address%20these%20limitations%2C%20we%20propose%20SLGNet%2C%20a%20parameter-efficient%20framework%20that%20synergizes%20hierarchical%20structural%20priors%20and%20language-guided%20modulation%20within%20a%20frozen%20Vision%20Transformer%20%28ViT%29-based%20foundation%20model.%20Specifically%2C%20we%20design%20a%20Structure-Aware%20Adapter%20to%20extract%20hierarchical%20structural%20representations%20from%20both%20modalities%20and%20dynamically%20inject%20them%20into%20the%20ViT%20to%20compensate%20for%20structural%20degradation%20inherent%20in%20ViT-based%20backbones.%20Furthermore%2C%20we%20propose%20a%20Language-Guided%20Modulation%20module%20that%20exploits%20VLM-driven%20structured%20captions%20to%20dynamically%20recalibrate%20visual%20features%2C%20thereby%20endowing%20the%20model%20with%20robust%20environmental%20awareness.%20Extensive%20experiments%20on%20the%20LLVIP%2C%20FLIR%2C%20KAIST%2C%20and%20DroneVehicle%20datasets%20demonstrate%20that%20SLGNet%20establishes%20new%20state-of-the-art%20performance.%20Notably%2C%20on%20the%20LLVIP%20benchmark%2C%20our%20method%20achieves%20an%20mAP%20of%2066.1%2C%20while%20reducing%20trainable%20parameters%20by%20approximately%2087%25%20compared%20to%20traditional%20full%20fine-tuning.%20This%20confirms%20SLGNet%20as%20a%20robust%20and%20efficient%20solution%20for%20multimodal%20perception.&entry.1838667208=http%3A//arxiv.org/abs/2601.02249v1&entry.124074799=Read"},
{"title": "Leveraging 2D-VLM for Label-Free 3D Segmentation in Large-Scale Outdoor Scene Understanding", "author": "Toshihiko Nishimura and Hirofumi Abe and Kazuhiko Murasaki and Taiga Yoshida and Ryuichi Tanida", "abstract": "This paper presents a novel 3D semantic segmentation method for large-scale point cloud data that does not require annotated 3D training data or paired RGB images. The proposed approach projects 3D point clouds onto 2D images using virtual cameras and performs semantic segmentation via a foundation 2D model guided by natural language prompts. 3D segmentation is achieved by aggregating predictions from multiple viewpoints through weighted voting. Our method outperforms existing training-free approaches and achieves segmentation accuracy comparable to supervised methods. Moreover, it supports open-vocabulary recognition, enabling users to detect objects using arbitrary text queries, thus overcoming the limitations of traditional supervised approaches.", "link": "http://arxiv.org/abs/2601.02029v1", "date": "2026-01-05", "relevancy": 2.9417, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5944}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5944}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%202D-VLM%20for%20Label-Free%203D%20Segmentation%20in%20Large-Scale%20Outdoor%20Scene%20Understanding&body=Title%3A%20Leveraging%202D-VLM%20for%20Label-Free%203D%20Segmentation%20in%20Large-Scale%20Outdoor%20Scene%20Understanding%0AAuthor%3A%20Toshihiko%20Nishimura%20and%20Hirofumi%20Abe%20and%20Kazuhiko%20Murasaki%20and%20Taiga%20Yoshida%20and%20Ryuichi%20Tanida%0AAbstract%3A%20This%20paper%20presents%20a%20novel%203D%20semantic%20segmentation%20method%20for%20large-scale%20point%20cloud%20data%20that%20does%20not%20require%20annotated%203D%20training%20data%20or%20paired%20RGB%20images.%20The%20proposed%20approach%20projects%203D%20point%20clouds%20onto%202D%20images%20using%20virtual%20cameras%20and%20performs%20semantic%20segmentation%20via%20a%20foundation%202D%20model%20guided%20by%20natural%20language%20prompts.%203D%20segmentation%20is%20achieved%20by%20aggregating%20predictions%20from%20multiple%20viewpoints%20through%20weighted%20voting.%20Our%20method%20outperforms%20existing%20training-free%20approaches%20and%20achieves%20segmentation%20accuracy%20comparable%20to%20supervised%20methods.%20Moreover%2C%20it%20supports%20open-vocabulary%20recognition%2C%20enabling%20users%20to%20detect%20objects%20using%20arbitrary%20text%20queries%2C%20thus%20overcoming%20the%20limitations%20of%20traditional%20supervised%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02029v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%25202D-VLM%2520for%2520Label-Free%25203D%2520Segmentation%2520in%2520Large-Scale%2520Outdoor%2520Scene%2520Understanding%26entry.906535625%3DToshihiko%2520Nishimura%2520and%2520Hirofumi%2520Abe%2520and%2520Kazuhiko%2520Murasaki%2520and%2520Taiga%2520Yoshida%2520and%2520Ryuichi%2520Tanida%26entry.1292438233%3DThis%2520paper%2520presents%2520a%2520novel%25203D%2520semantic%2520segmentation%2520method%2520for%2520large-scale%2520point%2520cloud%2520data%2520that%2520does%2520not%2520require%2520annotated%25203D%2520training%2520data%2520or%2520paired%2520RGB%2520images.%2520The%2520proposed%2520approach%2520projects%25203D%2520point%2520clouds%2520onto%25202D%2520images%2520using%2520virtual%2520cameras%2520and%2520performs%2520semantic%2520segmentation%2520via%2520a%2520foundation%25202D%2520model%2520guided%2520by%2520natural%2520language%2520prompts.%25203D%2520segmentation%2520is%2520achieved%2520by%2520aggregating%2520predictions%2520from%2520multiple%2520viewpoints%2520through%2520weighted%2520voting.%2520Our%2520method%2520outperforms%2520existing%2520training-free%2520approaches%2520and%2520achieves%2520segmentation%2520accuracy%2520comparable%2520to%2520supervised%2520methods.%2520Moreover%252C%2520it%2520supports%2520open-vocabulary%2520recognition%252C%2520enabling%2520users%2520to%2520detect%2520objects%2520using%2520arbitrary%2520text%2520queries%252C%2520thus%2520overcoming%2520the%2520limitations%2520of%2520traditional%2520supervised%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02029v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%202D-VLM%20for%20Label-Free%203D%20Segmentation%20in%20Large-Scale%20Outdoor%20Scene%20Understanding&entry.906535625=Toshihiko%20Nishimura%20and%20Hirofumi%20Abe%20and%20Kazuhiko%20Murasaki%20and%20Taiga%20Yoshida%20and%20Ryuichi%20Tanida&entry.1292438233=This%20paper%20presents%20a%20novel%203D%20semantic%20segmentation%20method%20for%20large-scale%20point%20cloud%20data%20that%20does%20not%20require%20annotated%203D%20training%20data%20or%20paired%20RGB%20images.%20The%20proposed%20approach%20projects%203D%20point%20clouds%20onto%202D%20images%20using%20virtual%20cameras%20and%20performs%20semantic%20segmentation%20via%20a%20foundation%202D%20model%20guided%20by%20natural%20language%20prompts.%203D%20segmentation%20is%20achieved%20by%20aggregating%20predictions%20from%20multiple%20viewpoints%20through%20weighted%20voting.%20Our%20method%20outperforms%20existing%20training-free%20approaches%20and%20achieves%20segmentation%20accuracy%20comparable%20to%20supervised%20methods.%20Moreover%2C%20it%20supports%20open-vocabulary%20recognition%2C%20enabling%20users%20to%20detect%20objects%20using%20arbitrary%20text%20queries%2C%20thus%20overcoming%20the%20limitations%20of%20traditional%20supervised%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2601.02029v1&entry.124074799=Read"},
{"title": "Thinking with Blueprints: Assisting Vision-Language Models in Spatial Reasoning via Structured Object Representation", "author": "Weijian Ma and Shizhao Sun and Tianyu Yu and Ruiyu Wang and Tat-Seng Chua and Jiang Bian", "abstract": "Spatial reasoning -- the ability to perceive and reason about relationships in space -- advances vision-language models (VLMs) from visual perception toward spatial semantic understanding. Existing approaches either revisit local image patches, improving fine-grained perception but weakening global spatial awareness, or mark isolated coordinates, which capture object locations but overlook their overall organization. In this work, we integrate the cognitive concept of an object-centric blueprint into VLMs to enhance spatial reasoning. Given an image and a question, the model first constructs a JSON-style blueprint that records the positions, sizes, and attributes of relevant objects, and then reasons over this structured representation to produce the final answer. To achieve this, we introduce three key techniques: (1) blueprint-embedded reasoning traces for supervised fine-tuning to elicit basic reasoning skills; (2) blueprint-aware rewards in reinforcement learning to encourage the blueprint to include an appropriate number of objects and to align final answers with this causal reasoning; and (3) anti-shortcut data augmentation that applies targeted perturbations to images and questions, discouraging reliance on superficial visual or linguistic cues. Experiments show that our method consistently outperforms existing VLMs and specialized spatial reasoning models.", "link": "http://arxiv.org/abs/2601.01984v1", "date": "2026-01-05", "relevancy": 2.926, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6036}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6036}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thinking%20with%20Blueprints%3A%20Assisting%20Vision-Language%20Models%20in%20Spatial%20Reasoning%20via%20Structured%20Object%20Representation&body=Title%3A%20Thinking%20with%20Blueprints%3A%20Assisting%20Vision-Language%20Models%20in%20Spatial%20Reasoning%20via%20Structured%20Object%20Representation%0AAuthor%3A%20Weijian%20Ma%20and%20Shizhao%20Sun%20and%20Tianyu%20Yu%20and%20Ruiyu%20Wang%20and%20Tat-Seng%20Chua%20and%20Jiang%20Bian%0AAbstract%3A%20Spatial%20reasoning%20--%20the%20ability%20to%20perceive%20and%20reason%20about%20relationships%20in%20space%20--%20advances%20vision-language%20models%20%28VLMs%29%20from%20visual%20perception%20toward%20spatial%20semantic%20understanding.%20Existing%20approaches%20either%20revisit%20local%20image%20patches%2C%20improving%20fine-grained%20perception%20but%20weakening%20global%20spatial%20awareness%2C%20or%20mark%20isolated%20coordinates%2C%20which%20capture%20object%20locations%20but%20overlook%20their%20overall%20organization.%20In%20this%20work%2C%20we%20integrate%20the%20cognitive%20concept%20of%20an%20object-centric%20blueprint%20into%20VLMs%20to%20enhance%20spatial%20reasoning.%20Given%20an%20image%20and%20a%20question%2C%20the%20model%20first%20constructs%20a%20JSON-style%20blueprint%20that%20records%20the%20positions%2C%20sizes%2C%20and%20attributes%20of%20relevant%20objects%2C%20and%20then%20reasons%20over%20this%20structured%20representation%20to%20produce%20the%20final%20answer.%20To%20achieve%20this%2C%20we%20introduce%20three%20key%20techniques%3A%20%281%29%20blueprint-embedded%20reasoning%20traces%20for%20supervised%20fine-tuning%20to%20elicit%20basic%20reasoning%20skills%3B%20%282%29%20blueprint-aware%20rewards%20in%20reinforcement%20learning%20to%20encourage%20the%20blueprint%20to%20include%20an%20appropriate%20number%20of%20objects%20and%20to%20align%20final%20answers%20with%20this%20causal%20reasoning%3B%20and%20%283%29%20anti-shortcut%20data%20augmentation%20that%20applies%20targeted%20perturbations%20to%20images%20and%20questions%2C%20discouraging%20reliance%20on%20superficial%20visual%20or%20linguistic%20cues.%20Experiments%20show%20that%20our%20method%20consistently%20outperforms%20existing%20VLMs%20and%20specialized%20spatial%20reasoning%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2601.01984v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThinking%2520with%2520Blueprints%253A%2520Assisting%2520Vision-Language%2520Models%2520in%2520Spatial%2520Reasoning%2520via%2520Structured%2520Object%2520Representation%26entry.906535625%3DWeijian%2520Ma%2520and%2520Shizhao%2520Sun%2520and%2520Tianyu%2520Yu%2520and%2520Ruiyu%2520Wang%2520and%2520Tat-Seng%2520Chua%2520and%2520Jiang%2520Bian%26entry.1292438233%3DSpatial%2520reasoning%2520--%2520the%2520ability%2520to%2520perceive%2520and%2520reason%2520about%2520relationships%2520in%2520space%2520--%2520advances%2520vision-language%2520models%2520%2528VLMs%2529%2520from%2520visual%2520perception%2520toward%2520spatial%2520semantic%2520understanding.%2520Existing%2520approaches%2520either%2520revisit%2520local%2520image%2520patches%252C%2520improving%2520fine-grained%2520perception%2520but%2520weakening%2520global%2520spatial%2520awareness%252C%2520or%2520mark%2520isolated%2520coordinates%252C%2520which%2520capture%2520object%2520locations%2520but%2520overlook%2520their%2520overall%2520organization.%2520In%2520this%2520work%252C%2520we%2520integrate%2520the%2520cognitive%2520concept%2520of%2520an%2520object-centric%2520blueprint%2520into%2520VLMs%2520to%2520enhance%2520spatial%2520reasoning.%2520Given%2520an%2520image%2520and%2520a%2520question%252C%2520the%2520model%2520first%2520constructs%2520a%2520JSON-style%2520blueprint%2520that%2520records%2520the%2520positions%252C%2520sizes%252C%2520and%2520attributes%2520of%2520relevant%2520objects%252C%2520and%2520then%2520reasons%2520over%2520this%2520structured%2520representation%2520to%2520produce%2520the%2520final%2520answer.%2520To%2520achieve%2520this%252C%2520we%2520introduce%2520three%2520key%2520techniques%253A%2520%25281%2529%2520blueprint-embedded%2520reasoning%2520traces%2520for%2520supervised%2520fine-tuning%2520to%2520elicit%2520basic%2520reasoning%2520skills%253B%2520%25282%2529%2520blueprint-aware%2520rewards%2520in%2520reinforcement%2520learning%2520to%2520encourage%2520the%2520blueprint%2520to%2520include%2520an%2520appropriate%2520number%2520of%2520objects%2520and%2520to%2520align%2520final%2520answers%2520with%2520this%2520causal%2520reasoning%253B%2520and%2520%25283%2529%2520anti-shortcut%2520data%2520augmentation%2520that%2520applies%2520targeted%2520perturbations%2520to%2520images%2520and%2520questions%252C%2520discouraging%2520reliance%2520on%2520superficial%2520visual%2520or%2520linguistic%2520cues.%2520Experiments%2520show%2520that%2520our%2520method%2520consistently%2520outperforms%2520existing%2520VLMs%2520and%2520specialized%2520spatial%2520reasoning%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.01984v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thinking%20with%20Blueprints%3A%20Assisting%20Vision-Language%20Models%20in%20Spatial%20Reasoning%20via%20Structured%20Object%20Representation&entry.906535625=Weijian%20Ma%20and%20Shizhao%20Sun%20and%20Tianyu%20Yu%20and%20Ruiyu%20Wang%20and%20Tat-Seng%20Chua%20and%20Jiang%20Bian&entry.1292438233=Spatial%20reasoning%20--%20the%20ability%20to%20perceive%20and%20reason%20about%20relationships%20in%20space%20--%20advances%20vision-language%20models%20%28VLMs%29%20from%20visual%20perception%20toward%20spatial%20semantic%20understanding.%20Existing%20approaches%20either%20revisit%20local%20image%20patches%2C%20improving%20fine-grained%20perception%20but%20weakening%20global%20spatial%20awareness%2C%20or%20mark%20isolated%20coordinates%2C%20which%20capture%20object%20locations%20but%20overlook%20their%20overall%20organization.%20In%20this%20work%2C%20we%20integrate%20the%20cognitive%20concept%20of%20an%20object-centric%20blueprint%20into%20VLMs%20to%20enhance%20spatial%20reasoning.%20Given%20an%20image%20and%20a%20question%2C%20the%20model%20first%20constructs%20a%20JSON-style%20blueprint%20that%20records%20the%20positions%2C%20sizes%2C%20and%20attributes%20of%20relevant%20objects%2C%20and%20then%20reasons%20over%20this%20structured%20representation%20to%20produce%20the%20final%20answer.%20To%20achieve%20this%2C%20we%20introduce%20three%20key%20techniques%3A%20%281%29%20blueprint-embedded%20reasoning%20traces%20for%20supervised%20fine-tuning%20to%20elicit%20basic%20reasoning%20skills%3B%20%282%29%20blueprint-aware%20rewards%20in%20reinforcement%20learning%20to%20encourage%20the%20blueprint%20to%20include%20an%20appropriate%20number%20of%20objects%20and%20to%20align%20final%20answers%20with%20this%20causal%20reasoning%3B%20and%20%283%29%20anti-shortcut%20data%20augmentation%20that%20applies%20targeted%20perturbations%20to%20images%20and%20questions%2C%20discouraging%20reliance%20on%20superficial%20visual%20or%20linguistic%20cues.%20Experiments%20show%20that%20our%20method%20consistently%20outperforms%20existing%20VLMs%20and%20specialized%20spatial%20reasoning%20models.&entry.1838667208=http%3A//arxiv.org/abs/2601.01984v1&entry.124074799=Read"},
{"title": "MagicFight: Personalized Martial Arts Combat Video Generation", "author": "Jiancheng Huang and Mingfu Yan and Songyan Chen and Yi Huang and Shifeng Chen", "abstract": "Amid the surge in generic text-to-video generation, the field of personalized human video generation has witnessed notable advancements, primarily concentrated on single-person scenarios. However, to our knowledge, the domain of two-person interactions, particularly in the context of martial arts combat, remains uncharted. We identify a significant gap: existing models for single-person dancing generation prove insufficient for capturing the subtleties and complexities of two engaged fighters, resulting in challenges such as identity confusion, anomalous limbs, and action mismatches. To address this, we introduce a pioneering new task, Personalized Martial Arts Combat Video Generation. Our approach, MagicFight, is specifically crafted to overcome these hurdles. Given this pioneering task, we face a lack of appropriate datasets. Thus, we generate a bespoke dataset using the game physics engine Unity, meticulously crafting a multitude of 3D characters, martial arts moves, and scenes designed to represent the diversity of combat. MagicFight refines and adapts existing models and strategies to generate high-fidelity two-person combat videos that maintain individual identities and ensure seamless, coherent action sequences, thereby laying the groundwork for future innovations in the realm of interactive video content creation.\n  Website: https://MingfuYAN.github.io/MagicFight/\n  Dataset: https://huggingface.co/datasets/MingfuYAN/KungFu-Fiesta", "link": "http://arxiv.org/abs/2601.02107v1", "date": "2026-01-05", "relevancy": 2.9188, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6272}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5701}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MagicFight%3A%20Personalized%20Martial%20Arts%20Combat%20Video%20Generation&body=Title%3A%20MagicFight%3A%20Personalized%20Martial%20Arts%20Combat%20Video%20Generation%0AAuthor%3A%20Jiancheng%20Huang%20and%20Mingfu%20Yan%20and%20Songyan%20Chen%20and%20Yi%20Huang%20and%20Shifeng%20Chen%0AAbstract%3A%20Amid%20the%20surge%20in%20generic%20text-to-video%20generation%2C%20the%20field%20of%20personalized%20human%20video%20generation%20has%20witnessed%20notable%20advancements%2C%20primarily%20concentrated%20on%20single-person%20scenarios.%20However%2C%20to%20our%20knowledge%2C%20the%20domain%20of%20two-person%20interactions%2C%20particularly%20in%20the%20context%20of%20martial%20arts%20combat%2C%20remains%20uncharted.%20We%20identify%20a%20significant%20gap%3A%20existing%20models%20for%20single-person%20dancing%20generation%20prove%20insufficient%20for%20capturing%20the%20subtleties%20and%20complexities%20of%20two%20engaged%20fighters%2C%20resulting%20in%20challenges%20such%20as%20identity%20confusion%2C%20anomalous%20limbs%2C%20and%20action%20mismatches.%20To%20address%20this%2C%20we%20introduce%20a%20pioneering%20new%20task%2C%20Personalized%20Martial%20Arts%20Combat%20Video%20Generation.%20Our%20approach%2C%20MagicFight%2C%20is%20specifically%20crafted%20to%20overcome%20these%20hurdles.%20Given%20this%20pioneering%20task%2C%20we%20face%20a%20lack%20of%20appropriate%20datasets.%20Thus%2C%20we%20generate%20a%20bespoke%20dataset%20using%20the%20game%20physics%20engine%20Unity%2C%20meticulously%20crafting%20a%20multitude%20of%203D%20characters%2C%20martial%20arts%20moves%2C%20and%20scenes%20designed%20to%20represent%20the%20diversity%20of%20combat.%20MagicFight%20refines%20and%20adapts%20existing%20models%20and%20strategies%20to%20generate%20high-fidelity%20two-person%20combat%20videos%20that%20maintain%20individual%20identities%20and%20ensure%20seamless%2C%20coherent%20action%20sequences%2C%20thereby%20laying%20the%20groundwork%20for%20future%20innovations%20in%20the%20realm%20of%20interactive%20video%20content%20creation.%0A%20%20Website%3A%20https%3A//MingfuYAN.github.io/MagicFight/%0A%20%20Dataset%3A%20https%3A//huggingface.co/datasets/MingfuYAN/KungFu-Fiesta%0ALink%3A%20http%3A//arxiv.org/abs/2601.02107v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMagicFight%253A%2520Personalized%2520Martial%2520Arts%2520Combat%2520Video%2520Generation%26entry.906535625%3DJiancheng%2520Huang%2520and%2520Mingfu%2520Yan%2520and%2520Songyan%2520Chen%2520and%2520Yi%2520Huang%2520and%2520Shifeng%2520Chen%26entry.1292438233%3DAmid%2520the%2520surge%2520in%2520generic%2520text-to-video%2520generation%252C%2520the%2520field%2520of%2520personalized%2520human%2520video%2520generation%2520has%2520witnessed%2520notable%2520advancements%252C%2520primarily%2520concentrated%2520on%2520single-person%2520scenarios.%2520However%252C%2520to%2520our%2520knowledge%252C%2520the%2520domain%2520of%2520two-person%2520interactions%252C%2520particularly%2520in%2520the%2520context%2520of%2520martial%2520arts%2520combat%252C%2520remains%2520uncharted.%2520We%2520identify%2520a%2520significant%2520gap%253A%2520existing%2520models%2520for%2520single-person%2520dancing%2520generation%2520prove%2520insufficient%2520for%2520capturing%2520the%2520subtleties%2520and%2520complexities%2520of%2520two%2520engaged%2520fighters%252C%2520resulting%2520in%2520challenges%2520such%2520as%2520identity%2520confusion%252C%2520anomalous%2520limbs%252C%2520and%2520action%2520mismatches.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520pioneering%2520new%2520task%252C%2520Personalized%2520Martial%2520Arts%2520Combat%2520Video%2520Generation.%2520Our%2520approach%252C%2520MagicFight%252C%2520is%2520specifically%2520crafted%2520to%2520overcome%2520these%2520hurdles.%2520Given%2520this%2520pioneering%2520task%252C%2520we%2520face%2520a%2520lack%2520of%2520appropriate%2520datasets.%2520Thus%252C%2520we%2520generate%2520a%2520bespoke%2520dataset%2520using%2520the%2520game%2520physics%2520engine%2520Unity%252C%2520meticulously%2520crafting%2520a%2520multitude%2520of%25203D%2520characters%252C%2520martial%2520arts%2520moves%252C%2520and%2520scenes%2520designed%2520to%2520represent%2520the%2520diversity%2520of%2520combat.%2520MagicFight%2520refines%2520and%2520adapts%2520existing%2520models%2520and%2520strategies%2520to%2520generate%2520high-fidelity%2520two-person%2520combat%2520videos%2520that%2520maintain%2520individual%2520identities%2520and%2520ensure%2520seamless%252C%2520coherent%2520action%2520sequences%252C%2520thereby%2520laying%2520the%2520groundwork%2520for%2520future%2520innovations%2520in%2520the%2520realm%2520of%2520interactive%2520video%2520content%2520creation.%250A%2520%2520Website%253A%2520https%253A//MingfuYAN.github.io/MagicFight/%250A%2520%2520Dataset%253A%2520https%253A//huggingface.co/datasets/MingfuYAN/KungFu-Fiesta%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02107v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MagicFight%3A%20Personalized%20Martial%20Arts%20Combat%20Video%20Generation&entry.906535625=Jiancheng%20Huang%20and%20Mingfu%20Yan%20and%20Songyan%20Chen%20and%20Yi%20Huang%20and%20Shifeng%20Chen&entry.1292438233=Amid%20the%20surge%20in%20generic%20text-to-video%20generation%2C%20the%20field%20of%20personalized%20human%20video%20generation%20has%20witnessed%20notable%20advancements%2C%20primarily%20concentrated%20on%20single-person%20scenarios.%20However%2C%20to%20our%20knowledge%2C%20the%20domain%20of%20two-person%20interactions%2C%20particularly%20in%20the%20context%20of%20martial%20arts%20combat%2C%20remains%20uncharted.%20We%20identify%20a%20significant%20gap%3A%20existing%20models%20for%20single-person%20dancing%20generation%20prove%20insufficient%20for%20capturing%20the%20subtleties%20and%20complexities%20of%20two%20engaged%20fighters%2C%20resulting%20in%20challenges%20such%20as%20identity%20confusion%2C%20anomalous%20limbs%2C%20and%20action%20mismatches.%20To%20address%20this%2C%20we%20introduce%20a%20pioneering%20new%20task%2C%20Personalized%20Martial%20Arts%20Combat%20Video%20Generation.%20Our%20approach%2C%20MagicFight%2C%20is%20specifically%20crafted%20to%20overcome%20these%20hurdles.%20Given%20this%20pioneering%20task%2C%20we%20face%20a%20lack%20of%20appropriate%20datasets.%20Thus%2C%20we%20generate%20a%20bespoke%20dataset%20using%20the%20game%20physics%20engine%20Unity%2C%20meticulously%20crafting%20a%20multitude%20of%203D%20characters%2C%20martial%20arts%20moves%2C%20and%20scenes%20designed%20to%20represent%20the%20diversity%20of%20combat.%20MagicFight%20refines%20and%20adapts%20existing%20models%20and%20strategies%20to%20generate%20high-fidelity%20two-person%20combat%20videos%20that%20maintain%20individual%20identities%20and%20ensure%20seamless%2C%20coherent%20action%20sequences%2C%20thereby%20laying%20the%20groundwork%20for%20future%20innovations%20in%20the%20realm%20of%20interactive%20video%20content%20creation.%0A%20%20Website%3A%20https%3A//MingfuYAN.github.io/MagicFight/%0A%20%20Dataset%3A%20https%3A//huggingface.co/datasets/MingfuYAN/KungFu-Fiesta&entry.1838667208=http%3A//arxiv.org/abs/2601.02107v1&entry.124074799=Read"},
{"title": "TopoLoRA-SAM: Topology-Aware Parameter-Efficient Adaptation of Foundation Segmenters for Thin-Structure and Cross-Domain Binary Semantic Segmentation", "author": "Salim Khazem", "abstract": "Foundation segmentation models such as the Segment Anything Model (SAM) exhibit strong zero-shot generalization through large-scale pretraining, but adapting them to domain-specific semantic segmentation remains challenging, particularly for thin structures (e.g., retinal vessels) and noisy modalities (e.g., SAR imagery). Full fine-tuning is computationally expensive and risks catastrophic forgetting. We propose \\textbf{TopoLoRA-SAM}, a topology-aware and parameter-efficient adaptation framework for binary semantic segmentation. TopoLoRA-SAM injects Low-Rank Adaptation (LoRA) into the frozen ViT encoder, augmented with a lightweight spatial convolutional adapter and optional topology-aware supervision via differentiable clDice. We evaluate our approach on five benchmarks spanning retinal vessel segmentation (DRIVE, STARE, CHASE\\_DB1), polyp segmentation (Kvasir-SEG), and SAR sea/land segmentation (SL-SSDD), comparing against U-Net, DeepLabV3+, SegFormer, and Mask2Former. TopoLoRA-SAM achieves the best retina-average Dice and the best overall average Dice across datasets, while training only \\textbf{5.2\\%} of model parameters ($\\sim$4.9M). On the challenging CHASE\\_DB1 dataset, our method substantially improves segmentation accuracy and robustness, demonstrating that topology-aware parameter-efficient adaptation can match or exceed fully fine-tuned specialist models. Code is available at : https://github.com/salimkhazem/Seglab.git", "link": "http://arxiv.org/abs/2601.02273v1", "date": "2026-01-05", "relevancy": 2.8756, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6098}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5626}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TopoLoRA-SAM%3A%20Topology-Aware%20Parameter-Efficient%20Adaptation%20of%20Foundation%20Segmenters%20for%20Thin-Structure%20and%20Cross-Domain%20Binary%20Semantic%20Segmentation&body=Title%3A%20TopoLoRA-SAM%3A%20Topology-Aware%20Parameter-Efficient%20Adaptation%20of%20Foundation%20Segmenters%20for%20Thin-Structure%20and%20Cross-Domain%20Binary%20Semantic%20Segmentation%0AAuthor%3A%20Salim%20Khazem%0AAbstract%3A%20Foundation%20segmentation%20models%20such%20as%20the%20Segment%20Anything%20Model%20%28SAM%29%20exhibit%20strong%20zero-shot%20generalization%20through%20large-scale%20pretraining%2C%20but%20adapting%20them%20to%20domain-specific%20semantic%20segmentation%20remains%20challenging%2C%20particularly%20for%20thin%20structures%20%28e.g.%2C%20retinal%20vessels%29%20and%20noisy%20modalities%20%28e.g.%2C%20SAR%20imagery%29.%20Full%20fine-tuning%20is%20computationally%20expensive%20and%20risks%20catastrophic%20forgetting.%20We%20propose%20%5Ctextbf%7BTopoLoRA-SAM%7D%2C%20a%20topology-aware%20and%20parameter-efficient%20adaptation%20framework%20for%20binary%20semantic%20segmentation.%20TopoLoRA-SAM%20injects%20Low-Rank%20Adaptation%20%28LoRA%29%20into%20the%20frozen%20ViT%20encoder%2C%20augmented%20with%20a%20lightweight%20spatial%20convolutional%20adapter%20and%20optional%20topology-aware%20supervision%20via%20differentiable%20clDice.%20We%20evaluate%20our%20approach%20on%20five%20benchmarks%20spanning%20retinal%20vessel%20segmentation%20%28DRIVE%2C%20STARE%2C%20CHASE%5C_DB1%29%2C%20polyp%20segmentation%20%28Kvasir-SEG%29%2C%20and%20SAR%20sea/land%20segmentation%20%28SL-SSDD%29%2C%20comparing%20against%20U-Net%2C%20DeepLabV3%2B%2C%20SegFormer%2C%20and%20Mask2Former.%20TopoLoRA-SAM%20achieves%20the%20best%20retina-average%20Dice%20and%20the%20best%20overall%20average%20Dice%20across%20datasets%2C%20while%20training%20only%20%5Ctextbf%7B5.2%5C%25%7D%20of%20model%20parameters%20%28%24%5Csim%244.9M%29.%20On%20the%20challenging%20CHASE%5C_DB1%20dataset%2C%20our%20method%20substantially%20improves%20segmentation%20accuracy%20and%20robustness%2C%20demonstrating%20that%20topology-aware%20parameter-efficient%20adaptation%20can%20match%20or%20exceed%20fully%20fine-tuned%20specialist%20models.%20Code%20is%20available%20at%20%3A%20https%3A//github.com/salimkhazem/Seglab.git%0ALink%3A%20http%3A//arxiv.org/abs/2601.02273v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopoLoRA-SAM%253A%2520Topology-Aware%2520Parameter-Efficient%2520Adaptation%2520of%2520Foundation%2520Segmenters%2520for%2520Thin-Structure%2520and%2520Cross-Domain%2520Binary%2520Semantic%2520Segmentation%26entry.906535625%3DSalim%2520Khazem%26entry.1292438233%3DFoundation%2520segmentation%2520models%2520such%2520as%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520exhibit%2520strong%2520zero-shot%2520generalization%2520through%2520large-scale%2520pretraining%252C%2520but%2520adapting%2520them%2520to%2520domain-specific%2520semantic%2520segmentation%2520remains%2520challenging%252C%2520particularly%2520for%2520thin%2520structures%2520%2528e.g.%252C%2520retinal%2520vessels%2529%2520and%2520noisy%2520modalities%2520%2528e.g.%252C%2520SAR%2520imagery%2529.%2520Full%2520fine-tuning%2520is%2520computationally%2520expensive%2520and%2520risks%2520catastrophic%2520forgetting.%2520We%2520propose%2520%255Ctextbf%257BTopoLoRA-SAM%257D%252C%2520a%2520topology-aware%2520and%2520parameter-efficient%2520adaptation%2520framework%2520for%2520binary%2520semantic%2520segmentation.%2520TopoLoRA-SAM%2520injects%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520into%2520the%2520frozen%2520ViT%2520encoder%252C%2520augmented%2520with%2520a%2520lightweight%2520spatial%2520convolutional%2520adapter%2520and%2520optional%2520topology-aware%2520supervision%2520via%2520differentiable%2520clDice.%2520We%2520evaluate%2520our%2520approach%2520on%2520five%2520benchmarks%2520spanning%2520retinal%2520vessel%2520segmentation%2520%2528DRIVE%252C%2520STARE%252C%2520CHASE%255C_DB1%2529%252C%2520polyp%2520segmentation%2520%2528Kvasir-SEG%2529%252C%2520and%2520SAR%2520sea/land%2520segmentation%2520%2528SL-SSDD%2529%252C%2520comparing%2520against%2520U-Net%252C%2520DeepLabV3%252B%252C%2520SegFormer%252C%2520and%2520Mask2Former.%2520TopoLoRA-SAM%2520achieves%2520the%2520best%2520retina-average%2520Dice%2520and%2520the%2520best%2520overall%2520average%2520Dice%2520across%2520datasets%252C%2520while%2520training%2520only%2520%255Ctextbf%257B5.2%255C%2525%257D%2520of%2520model%2520parameters%2520%2528%2524%255Csim%25244.9M%2529.%2520On%2520the%2520challenging%2520CHASE%255C_DB1%2520dataset%252C%2520our%2520method%2520substantially%2520improves%2520segmentation%2520accuracy%2520and%2520robustness%252C%2520demonstrating%2520that%2520topology-aware%2520parameter-efficient%2520adaptation%2520can%2520match%2520or%2520exceed%2520fully%2520fine-tuned%2520specialist%2520models.%2520Code%2520is%2520available%2520at%2520%253A%2520https%253A//github.com/salimkhazem/Seglab.git%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02273v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TopoLoRA-SAM%3A%20Topology-Aware%20Parameter-Efficient%20Adaptation%20of%20Foundation%20Segmenters%20for%20Thin-Structure%20and%20Cross-Domain%20Binary%20Semantic%20Segmentation&entry.906535625=Salim%20Khazem&entry.1292438233=Foundation%20segmentation%20models%20such%20as%20the%20Segment%20Anything%20Model%20%28SAM%29%20exhibit%20strong%20zero-shot%20generalization%20through%20large-scale%20pretraining%2C%20but%20adapting%20them%20to%20domain-specific%20semantic%20segmentation%20remains%20challenging%2C%20particularly%20for%20thin%20structures%20%28e.g.%2C%20retinal%20vessels%29%20and%20noisy%20modalities%20%28e.g.%2C%20SAR%20imagery%29.%20Full%20fine-tuning%20is%20computationally%20expensive%20and%20risks%20catastrophic%20forgetting.%20We%20propose%20%5Ctextbf%7BTopoLoRA-SAM%7D%2C%20a%20topology-aware%20and%20parameter-efficient%20adaptation%20framework%20for%20binary%20semantic%20segmentation.%20TopoLoRA-SAM%20injects%20Low-Rank%20Adaptation%20%28LoRA%29%20into%20the%20frozen%20ViT%20encoder%2C%20augmented%20with%20a%20lightweight%20spatial%20convolutional%20adapter%20and%20optional%20topology-aware%20supervision%20via%20differentiable%20clDice.%20We%20evaluate%20our%20approach%20on%20five%20benchmarks%20spanning%20retinal%20vessel%20segmentation%20%28DRIVE%2C%20STARE%2C%20CHASE%5C_DB1%29%2C%20polyp%20segmentation%20%28Kvasir-SEG%29%2C%20and%20SAR%20sea/land%20segmentation%20%28SL-SSDD%29%2C%20comparing%20against%20U-Net%2C%20DeepLabV3%2B%2C%20SegFormer%2C%20and%20Mask2Former.%20TopoLoRA-SAM%20achieves%20the%20best%20retina-average%20Dice%20and%20the%20best%20overall%20average%20Dice%20across%20datasets%2C%20while%20training%20only%20%5Ctextbf%7B5.2%5C%25%7D%20of%20model%20parameters%20%28%24%5Csim%244.9M%29.%20On%20the%20challenging%20CHASE%5C_DB1%20dataset%2C%20our%20method%20substantially%20improves%20segmentation%20accuracy%20and%20robustness%2C%20demonstrating%20that%20topology-aware%20parameter-efficient%20adaptation%20can%20match%20or%20exceed%20fully%20fine-tuned%20specialist%20models.%20Code%20is%20available%20at%20%3A%20https%3A//github.com/salimkhazem/Seglab.git&entry.1838667208=http%3A//arxiv.org/abs/2601.02273v1&entry.124074799=Read"},
{"title": "SketchRodGS: Sketch-based Extraction of Slender Geometries for Animating Gaussian Splatting Scenes", "author": "Haato Watanabe and Nobuyuki Umetani", "abstract": "Physics simulation of slender elastic objects often requires discretization as a polyline. However, constructing a polyline from Gaussian splatting is challenging as Gaussian splatting lacks connectivity information and the configuration of Gaussian primitives contains much noise. This paper presents a method to extract a polyline representation of the slender part of the objects in a Gaussian splatting scene from the user's sketching input. Our method robustly constructs a polyline mesh that represents the slender parts using the screen-space shortest path analysis that can be efficiently solved using dynamic programming. We demonstrate the effectiveness of our approach in several in-the-wild examples.", "link": "http://arxiv.org/abs/2601.02072v1", "date": "2026-01-05", "relevancy": 2.8483, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5773}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5717}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SketchRodGS%3A%20Sketch-based%20Extraction%20of%20Slender%20Geometries%20for%20Animating%20Gaussian%20Splatting%20Scenes&body=Title%3A%20SketchRodGS%3A%20Sketch-based%20Extraction%20of%20Slender%20Geometries%20for%20Animating%20Gaussian%20Splatting%20Scenes%0AAuthor%3A%20Haato%20Watanabe%20and%20Nobuyuki%20Umetani%0AAbstract%3A%20Physics%20simulation%20of%20slender%20elastic%20objects%20often%20requires%20discretization%20as%20a%20polyline.%20However%2C%20constructing%20a%20polyline%20from%20Gaussian%20splatting%20is%20challenging%20as%20Gaussian%20splatting%20lacks%20connectivity%20information%20and%20the%20configuration%20of%20Gaussian%20primitives%20contains%20much%20noise.%20This%20paper%20presents%20a%20method%20to%20extract%20a%20polyline%20representation%20of%20the%20slender%20part%20of%20the%20objects%20in%20a%20Gaussian%20splatting%20scene%20from%20the%20user%27s%20sketching%20input.%20Our%20method%20robustly%20constructs%20a%20polyline%20mesh%20that%20represents%20the%20slender%20parts%20using%20the%20screen-space%20shortest%20path%20analysis%20that%20can%20be%20efficiently%20solved%20using%20dynamic%20programming.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%20in%20several%20in-the-wild%20examples.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02072v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSketchRodGS%253A%2520Sketch-based%2520Extraction%2520of%2520Slender%2520Geometries%2520for%2520Animating%2520Gaussian%2520Splatting%2520Scenes%26entry.906535625%3DHaato%2520Watanabe%2520and%2520Nobuyuki%2520Umetani%26entry.1292438233%3DPhysics%2520simulation%2520of%2520slender%2520elastic%2520objects%2520often%2520requires%2520discretization%2520as%2520a%2520polyline.%2520However%252C%2520constructing%2520a%2520polyline%2520from%2520Gaussian%2520splatting%2520is%2520challenging%2520as%2520Gaussian%2520splatting%2520lacks%2520connectivity%2520information%2520and%2520the%2520configuration%2520of%2520Gaussian%2520primitives%2520contains%2520much%2520noise.%2520This%2520paper%2520presents%2520a%2520method%2520to%2520extract%2520a%2520polyline%2520representation%2520of%2520the%2520slender%2520part%2520of%2520the%2520objects%2520in%2520a%2520Gaussian%2520splatting%2520scene%2520from%2520the%2520user%2527s%2520sketching%2520input.%2520Our%2520method%2520robustly%2520constructs%2520a%2520polyline%2520mesh%2520that%2520represents%2520the%2520slender%2520parts%2520using%2520the%2520screen-space%2520shortest%2520path%2520analysis%2520that%2520can%2520be%2520efficiently%2520solved%2520using%2520dynamic%2520programming.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520in%2520several%2520in-the-wild%2520examples.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02072v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SketchRodGS%3A%20Sketch-based%20Extraction%20of%20Slender%20Geometries%20for%20Animating%20Gaussian%20Splatting%20Scenes&entry.906535625=Haato%20Watanabe%20and%20Nobuyuki%20Umetani&entry.1292438233=Physics%20simulation%20of%20slender%20elastic%20objects%20often%20requires%20discretization%20as%20a%20polyline.%20However%2C%20constructing%20a%20polyline%20from%20Gaussian%20splatting%20is%20challenging%20as%20Gaussian%20splatting%20lacks%20connectivity%20information%20and%20the%20configuration%20of%20Gaussian%20primitives%20contains%20much%20noise.%20This%20paper%20presents%20a%20method%20to%20extract%20a%20polyline%20representation%20of%20the%20slender%20part%20of%20the%20objects%20in%20a%20Gaussian%20splatting%20scene%20from%20the%20user%27s%20sketching%20input.%20Our%20method%20robustly%20constructs%20a%20polyline%20mesh%20that%20represents%20the%20slender%20parts%20using%20the%20screen-space%20shortest%20path%20analysis%20that%20can%20be%20efficiently%20solved%20using%20dynamic%20programming.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%20in%20several%20in-the-wild%20examples.&entry.1838667208=http%3A//arxiv.org/abs/2601.02072v1&entry.124074799=Read"},
{"title": "TD3Net: A temporal densely connected multi-dilated convolutional network for lipreading", "author": "Byung Hoon Lee and Wooseok Shin and Sung Won Han", "abstract": "The word-level lipreading approach typically employs a two-stage framework with separate frontend and backend architectures to model dynamic lip movements. Each component has been extensively studied, and in the backend architecture, temporal convolutional networks (TCNs) have been widely adopted in state-of-the-art methods. Recently, dense skip connections have been introduced in TCNs to mitigate the limited density of the receptive field, thereby improving the modeling of complex temporal representations. However, their performance remains constrained owing to potential information loss regarding the continuous nature of lip movements, caused by blind spots in the receptive field. To address this limitation, we propose TD3Net, a temporal densely connected multi-dilated convolutional network that combines dense skip connections and multi-dilated temporal convolutions as the backend architecture. TD3Net covers a wide and dense receptive field without blind spots by applying different dilation factors to skip-connected features. Experimental results on a word-level lipreading task using two large publicly available datasets, Lip Reading in the Wild (LRW) and LRW-1000, indicate that the proposed method achieves performance comparable to state-of-the-art methods. It achieved higher accuracy with fewer parameters and lower floating-point operations compared to existing TCN-based backend architectures. Moreover, visualization results suggest that our approach effectively utilizes diverse temporal features while preserving temporal continuity, presenting notable advantages in lipreading systems. The code is available at our GitHub repository (https://github.com/Leebh-kor/TD3Net).", "link": "http://arxiv.org/abs/2506.16073v4", "date": "2026-01-05", "relevancy": 2.7993, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5963}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5633}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TD3Net%3A%20A%20temporal%20densely%20connected%20multi-dilated%20convolutional%20network%20for%20lipreading&body=Title%3A%20TD3Net%3A%20A%20temporal%20densely%20connected%20multi-dilated%20convolutional%20network%20for%20lipreading%0AAuthor%3A%20Byung%20Hoon%20Lee%20and%20Wooseok%20Shin%20and%20Sung%20Won%20Han%0AAbstract%3A%20The%20word-level%20lipreading%20approach%20typically%20employs%20a%20two-stage%20framework%20with%20separate%20frontend%20and%20backend%20architectures%20to%20model%20dynamic%20lip%20movements.%20Each%20component%20has%20been%20extensively%20studied%2C%20and%20in%20the%20backend%20architecture%2C%20temporal%20convolutional%20networks%20%28TCNs%29%20have%20been%20widely%20adopted%20in%20state-of-the-art%20methods.%20Recently%2C%20dense%20skip%20connections%20have%20been%20introduced%20in%20TCNs%20to%20mitigate%20the%20limited%20density%20of%20the%20receptive%20field%2C%20thereby%20improving%20the%20modeling%20of%20complex%20temporal%20representations.%20However%2C%20their%20performance%20remains%20constrained%20owing%20to%20potential%20information%20loss%20regarding%20the%20continuous%20nature%20of%20lip%20movements%2C%20caused%20by%20blind%20spots%20in%20the%20receptive%20field.%20To%20address%20this%20limitation%2C%20we%20propose%20TD3Net%2C%20a%20temporal%20densely%20connected%20multi-dilated%20convolutional%20network%20that%20combines%20dense%20skip%20connections%20and%20multi-dilated%20temporal%20convolutions%20as%20the%20backend%20architecture.%20TD3Net%20covers%20a%20wide%20and%20dense%20receptive%20field%20without%20blind%20spots%20by%20applying%20different%20dilation%20factors%20to%20skip-connected%20features.%20Experimental%20results%20on%20a%20word-level%20lipreading%20task%20using%20two%20large%20publicly%20available%20datasets%2C%20Lip%20Reading%20in%20the%20Wild%20%28LRW%29%20and%20LRW-1000%2C%20indicate%20that%20the%20proposed%20method%20achieves%20performance%20comparable%20to%20state-of-the-art%20methods.%20It%20achieved%20higher%20accuracy%20with%20fewer%20parameters%20and%20lower%20floating-point%20operations%20compared%20to%20existing%20TCN-based%20backend%20architectures.%20Moreover%2C%20visualization%20results%20suggest%20that%20our%20approach%20effectively%20utilizes%20diverse%20temporal%20features%20while%20preserving%20temporal%20continuity%2C%20presenting%20notable%20advantages%20in%20lipreading%20systems.%20The%20code%20is%20available%20at%20our%20GitHub%20repository%20%28https%3A//github.com/Leebh-kor/TD3Net%29.%0ALink%3A%20http%3A//arxiv.org/abs/2506.16073v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTD3Net%253A%2520A%2520temporal%2520densely%2520connected%2520multi-dilated%2520convolutional%2520network%2520for%2520lipreading%26entry.906535625%3DByung%2520Hoon%2520Lee%2520and%2520Wooseok%2520Shin%2520and%2520Sung%2520Won%2520Han%26entry.1292438233%3DThe%2520word-level%2520lipreading%2520approach%2520typically%2520employs%2520a%2520two-stage%2520framework%2520with%2520separate%2520frontend%2520and%2520backend%2520architectures%2520to%2520model%2520dynamic%2520lip%2520movements.%2520Each%2520component%2520has%2520been%2520extensively%2520studied%252C%2520and%2520in%2520the%2520backend%2520architecture%252C%2520temporal%2520convolutional%2520networks%2520%2528TCNs%2529%2520have%2520been%2520widely%2520adopted%2520in%2520state-of-the-art%2520methods.%2520Recently%252C%2520dense%2520skip%2520connections%2520have%2520been%2520introduced%2520in%2520TCNs%2520to%2520mitigate%2520the%2520limited%2520density%2520of%2520the%2520receptive%2520field%252C%2520thereby%2520improving%2520the%2520modeling%2520of%2520complex%2520temporal%2520representations.%2520However%252C%2520their%2520performance%2520remains%2520constrained%2520owing%2520to%2520potential%2520information%2520loss%2520regarding%2520the%2520continuous%2520nature%2520of%2520lip%2520movements%252C%2520caused%2520by%2520blind%2520spots%2520in%2520the%2520receptive%2520field.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520TD3Net%252C%2520a%2520temporal%2520densely%2520connected%2520multi-dilated%2520convolutional%2520network%2520that%2520combines%2520dense%2520skip%2520connections%2520and%2520multi-dilated%2520temporal%2520convolutions%2520as%2520the%2520backend%2520architecture.%2520TD3Net%2520covers%2520a%2520wide%2520and%2520dense%2520receptive%2520field%2520without%2520blind%2520spots%2520by%2520applying%2520different%2520dilation%2520factors%2520to%2520skip-connected%2520features.%2520Experimental%2520results%2520on%2520a%2520word-level%2520lipreading%2520task%2520using%2520two%2520large%2520publicly%2520available%2520datasets%252C%2520Lip%2520Reading%2520in%2520the%2520Wild%2520%2528LRW%2529%2520and%2520LRW-1000%252C%2520indicate%2520that%2520the%2520proposed%2520method%2520achieves%2520performance%2520comparable%2520to%2520state-of-the-art%2520methods.%2520It%2520achieved%2520higher%2520accuracy%2520with%2520fewer%2520parameters%2520and%2520lower%2520floating-point%2520operations%2520compared%2520to%2520existing%2520TCN-based%2520backend%2520architectures.%2520Moreover%252C%2520visualization%2520results%2520suggest%2520that%2520our%2520approach%2520effectively%2520utilizes%2520diverse%2520temporal%2520features%2520while%2520preserving%2520temporal%2520continuity%252C%2520presenting%2520notable%2520advantages%2520in%2520lipreading%2520systems.%2520The%2520code%2520is%2520available%2520at%2520our%2520GitHub%2520repository%2520%2528https%253A//github.com/Leebh-kor/TD3Net%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.16073v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TD3Net%3A%20A%20temporal%20densely%20connected%20multi-dilated%20convolutional%20network%20for%20lipreading&entry.906535625=Byung%20Hoon%20Lee%20and%20Wooseok%20Shin%20and%20Sung%20Won%20Han&entry.1292438233=The%20word-level%20lipreading%20approach%20typically%20employs%20a%20two-stage%20framework%20with%20separate%20frontend%20and%20backend%20architectures%20to%20model%20dynamic%20lip%20movements.%20Each%20component%20has%20been%20extensively%20studied%2C%20and%20in%20the%20backend%20architecture%2C%20temporal%20convolutional%20networks%20%28TCNs%29%20have%20been%20widely%20adopted%20in%20state-of-the-art%20methods.%20Recently%2C%20dense%20skip%20connections%20have%20been%20introduced%20in%20TCNs%20to%20mitigate%20the%20limited%20density%20of%20the%20receptive%20field%2C%20thereby%20improving%20the%20modeling%20of%20complex%20temporal%20representations.%20However%2C%20their%20performance%20remains%20constrained%20owing%20to%20potential%20information%20loss%20regarding%20the%20continuous%20nature%20of%20lip%20movements%2C%20caused%20by%20blind%20spots%20in%20the%20receptive%20field.%20To%20address%20this%20limitation%2C%20we%20propose%20TD3Net%2C%20a%20temporal%20densely%20connected%20multi-dilated%20convolutional%20network%20that%20combines%20dense%20skip%20connections%20and%20multi-dilated%20temporal%20convolutions%20as%20the%20backend%20architecture.%20TD3Net%20covers%20a%20wide%20and%20dense%20receptive%20field%20without%20blind%20spots%20by%20applying%20different%20dilation%20factors%20to%20skip-connected%20features.%20Experimental%20results%20on%20a%20word-level%20lipreading%20task%20using%20two%20large%20publicly%20available%20datasets%2C%20Lip%20Reading%20in%20the%20Wild%20%28LRW%29%20and%20LRW-1000%2C%20indicate%20that%20the%20proposed%20method%20achieves%20performance%20comparable%20to%20state-of-the-art%20methods.%20It%20achieved%20higher%20accuracy%20with%20fewer%20parameters%20and%20lower%20floating-point%20operations%20compared%20to%20existing%20TCN-based%20backend%20architectures.%20Moreover%2C%20visualization%20results%20suggest%20that%20our%20approach%20effectively%20utilizes%20diverse%20temporal%20features%20while%20preserving%20temporal%20continuity%2C%20presenting%20notable%20advantages%20in%20lipreading%20systems.%20The%20code%20is%20available%20at%20our%20GitHub%20repository%20%28https%3A//github.com/Leebh-kor/TD3Net%29.&entry.1838667208=http%3A//arxiv.org/abs/2506.16073v4&entry.124074799=Read"},
{"title": "CADMorph: Geometry-Driven Parametric CAD Editing via a Plan-Generate-Verify Loop", "author": "Weijian Ma and Shizhao Sun and Ruiyu Wang and Jiang Bian", "abstract": "A Computer-Aided Design (CAD) model encodes an object in two coupled forms: a parametric construction sequence and its resulting visible geometric shape. During iterative design, adjustments to the geometric shape inevitably require synchronized edits to the underlying parametric sequence, called geometry-driven parametric CAD editing. The task calls for 1) preserving the original sequence's structure, 2) ensuring each edit's semantic validity, and 3) maintaining high shape fidelity to the target shape, all under scarce editing data triplets. We present CADMorph, an iterative plan-generate-verify framework that orchestrates pretrained domain-specific foundation models during inference: a parameter-to-shape (P2S) latent diffusion model and a masked-parameter-prediction (MPP) model. In the planning stage, cross-attention maps from the P2S model pinpoint the segments that need modification and offer editing masks. The MPP model then infills these masks with semantically valid edits in the generation stage. During verification, the P2S model embeds each candidate sequence in shape-latent space, measures its distance to the target shape, and selects the closest one. The three stages leverage the inherent geometric consciousness and design knowledge in pretrained priors, and thus tackle structure preservation, semantic validity, and shape fidelity respectively. Besides, both P2S and MPP models are trained without triplet data, bypassing the data-scarcity bottleneck. CADMorph surpasses GPT-4o and specialized CAD baselines, and supports downstream applications such as iterative editing and reverse-engineering enhancement.", "link": "http://arxiv.org/abs/2512.11480v2", "date": "2026-01-05", "relevancy": 2.7625, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5619}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5557}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CADMorph%3A%20Geometry-Driven%20Parametric%20CAD%20Editing%20via%20a%20Plan-Generate-Verify%20Loop&body=Title%3A%20CADMorph%3A%20Geometry-Driven%20Parametric%20CAD%20Editing%20via%20a%20Plan-Generate-Verify%20Loop%0AAuthor%3A%20Weijian%20Ma%20and%20Shizhao%20Sun%20and%20Ruiyu%20Wang%20and%20Jiang%20Bian%0AAbstract%3A%20A%20Computer-Aided%20Design%20%28CAD%29%20model%20encodes%20an%20object%20in%20two%20coupled%20forms%3A%20a%20parametric%20construction%20sequence%20and%20its%20resulting%20visible%20geometric%20shape.%20During%20iterative%20design%2C%20adjustments%20to%20the%20geometric%20shape%20inevitably%20require%20synchronized%20edits%20to%20the%20underlying%20parametric%20sequence%2C%20called%20geometry-driven%20parametric%20CAD%20editing.%20The%20task%20calls%20for%201%29%20preserving%20the%20original%20sequence%27s%20structure%2C%202%29%20ensuring%20each%20edit%27s%20semantic%20validity%2C%20and%203%29%20maintaining%20high%20shape%20fidelity%20to%20the%20target%20shape%2C%20all%20under%20scarce%20editing%20data%20triplets.%20We%20present%20CADMorph%2C%20an%20iterative%20plan-generate-verify%20framework%20that%20orchestrates%20pretrained%20domain-specific%20foundation%20models%20during%20inference%3A%20a%20parameter-to-shape%20%28P2S%29%20latent%20diffusion%20model%20and%20a%20masked-parameter-prediction%20%28MPP%29%20model.%20In%20the%20planning%20stage%2C%20cross-attention%20maps%20from%20the%20P2S%20model%20pinpoint%20the%20segments%20that%20need%20modification%20and%20offer%20editing%20masks.%20The%20MPP%20model%20then%20infills%20these%20masks%20with%20semantically%20valid%20edits%20in%20the%20generation%20stage.%20During%20verification%2C%20the%20P2S%20model%20embeds%20each%20candidate%20sequence%20in%20shape-latent%20space%2C%20measures%20its%20distance%20to%20the%20target%20shape%2C%20and%20selects%20the%20closest%20one.%20The%20three%20stages%20leverage%20the%20inherent%20geometric%20consciousness%20and%20design%20knowledge%20in%20pretrained%20priors%2C%20and%20thus%20tackle%20structure%20preservation%2C%20semantic%20validity%2C%20and%20shape%20fidelity%20respectively.%20Besides%2C%20both%20P2S%20and%20MPP%20models%20are%20trained%20without%20triplet%20data%2C%20bypassing%20the%20data-scarcity%20bottleneck.%20CADMorph%20surpasses%20GPT-4o%20and%20specialized%20CAD%20baselines%2C%20and%20supports%20downstream%20applications%20such%20as%20iterative%20editing%20and%20reverse-engineering%20enhancement.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11480v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCADMorph%253A%2520Geometry-Driven%2520Parametric%2520CAD%2520Editing%2520via%2520a%2520Plan-Generate-Verify%2520Loop%26entry.906535625%3DWeijian%2520Ma%2520and%2520Shizhao%2520Sun%2520and%2520Ruiyu%2520Wang%2520and%2520Jiang%2520Bian%26entry.1292438233%3DA%2520Computer-Aided%2520Design%2520%2528CAD%2529%2520model%2520encodes%2520an%2520object%2520in%2520two%2520coupled%2520forms%253A%2520a%2520parametric%2520construction%2520sequence%2520and%2520its%2520resulting%2520visible%2520geometric%2520shape.%2520During%2520iterative%2520design%252C%2520adjustments%2520to%2520the%2520geometric%2520shape%2520inevitably%2520require%2520synchronized%2520edits%2520to%2520the%2520underlying%2520parametric%2520sequence%252C%2520called%2520geometry-driven%2520parametric%2520CAD%2520editing.%2520The%2520task%2520calls%2520for%25201%2529%2520preserving%2520the%2520original%2520sequence%2527s%2520structure%252C%25202%2529%2520ensuring%2520each%2520edit%2527s%2520semantic%2520validity%252C%2520and%25203%2529%2520maintaining%2520high%2520shape%2520fidelity%2520to%2520the%2520target%2520shape%252C%2520all%2520under%2520scarce%2520editing%2520data%2520triplets.%2520We%2520present%2520CADMorph%252C%2520an%2520iterative%2520plan-generate-verify%2520framework%2520that%2520orchestrates%2520pretrained%2520domain-specific%2520foundation%2520models%2520during%2520inference%253A%2520a%2520parameter-to-shape%2520%2528P2S%2529%2520latent%2520diffusion%2520model%2520and%2520a%2520masked-parameter-prediction%2520%2528MPP%2529%2520model.%2520In%2520the%2520planning%2520stage%252C%2520cross-attention%2520maps%2520from%2520the%2520P2S%2520model%2520pinpoint%2520the%2520segments%2520that%2520need%2520modification%2520and%2520offer%2520editing%2520masks.%2520The%2520MPP%2520model%2520then%2520infills%2520these%2520masks%2520with%2520semantically%2520valid%2520edits%2520in%2520the%2520generation%2520stage.%2520During%2520verification%252C%2520the%2520P2S%2520model%2520embeds%2520each%2520candidate%2520sequence%2520in%2520shape-latent%2520space%252C%2520measures%2520its%2520distance%2520to%2520the%2520target%2520shape%252C%2520and%2520selects%2520the%2520closest%2520one.%2520The%2520three%2520stages%2520leverage%2520the%2520inherent%2520geometric%2520consciousness%2520and%2520design%2520knowledge%2520in%2520pretrained%2520priors%252C%2520and%2520thus%2520tackle%2520structure%2520preservation%252C%2520semantic%2520validity%252C%2520and%2520shape%2520fidelity%2520respectively.%2520Besides%252C%2520both%2520P2S%2520and%2520MPP%2520models%2520are%2520trained%2520without%2520triplet%2520data%252C%2520bypassing%2520the%2520data-scarcity%2520bottleneck.%2520CADMorph%2520surpasses%2520GPT-4o%2520and%2520specialized%2520CAD%2520baselines%252C%2520and%2520supports%2520downstream%2520applications%2520such%2520as%2520iterative%2520editing%2520and%2520reverse-engineering%2520enhancement.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11480v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CADMorph%3A%20Geometry-Driven%20Parametric%20CAD%20Editing%20via%20a%20Plan-Generate-Verify%20Loop&entry.906535625=Weijian%20Ma%20and%20Shizhao%20Sun%20and%20Ruiyu%20Wang%20and%20Jiang%20Bian&entry.1292438233=A%20Computer-Aided%20Design%20%28CAD%29%20model%20encodes%20an%20object%20in%20two%20coupled%20forms%3A%20a%20parametric%20construction%20sequence%20and%20its%20resulting%20visible%20geometric%20shape.%20During%20iterative%20design%2C%20adjustments%20to%20the%20geometric%20shape%20inevitably%20require%20synchronized%20edits%20to%20the%20underlying%20parametric%20sequence%2C%20called%20geometry-driven%20parametric%20CAD%20editing.%20The%20task%20calls%20for%201%29%20preserving%20the%20original%20sequence%27s%20structure%2C%202%29%20ensuring%20each%20edit%27s%20semantic%20validity%2C%20and%203%29%20maintaining%20high%20shape%20fidelity%20to%20the%20target%20shape%2C%20all%20under%20scarce%20editing%20data%20triplets.%20We%20present%20CADMorph%2C%20an%20iterative%20plan-generate-verify%20framework%20that%20orchestrates%20pretrained%20domain-specific%20foundation%20models%20during%20inference%3A%20a%20parameter-to-shape%20%28P2S%29%20latent%20diffusion%20model%20and%20a%20masked-parameter-prediction%20%28MPP%29%20model.%20In%20the%20planning%20stage%2C%20cross-attention%20maps%20from%20the%20P2S%20model%20pinpoint%20the%20segments%20that%20need%20modification%20and%20offer%20editing%20masks.%20The%20MPP%20model%20then%20infills%20these%20masks%20with%20semantically%20valid%20edits%20in%20the%20generation%20stage.%20During%20verification%2C%20the%20P2S%20model%20embeds%20each%20candidate%20sequence%20in%20shape-latent%20space%2C%20measures%20its%20distance%20to%20the%20target%20shape%2C%20and%20selects%20the%20closest%20one.%20The%20three%20stages%20leverage%20the%20inherent%20geometric%20consciousness%20and%20design%20knowledge%20in%20pretrained%20priors%2C%20and%20thus%20tackle%20structure%20preservation%2C%20semantic%20validity%2C%20and%20shape%20fidelity%20respectively.%20Besides%2C%20both%20P2S%20and%20MPP%20models%20are%20trained%20without%20triplet%20data%2C%20bypassing%20the%20data-scarcity%20bottleneck.%20CADMorph%20surpasses%20GPT-4o%20and%20specialized%20CAD%20baselines%2C%20and%20supports%20downstream%20applications%20such%20as%20iterative%20editing%20and%20reverse-engineering%20enhancement.&entry.1838667208=http%3A//arxiv.org/abs/2512.11480v2&entry.124074799=Read"},
{"title": "PrevMatch: Revisiting and Maximizing Temporal Knowledge in Semi-Supervised Semantic Segmentation", "author": "Wooseok Shin and Hyun Joon Park and Jin Sob Kim and Juan Yun and Se Hong Park and Sung Won Han", "abstract": "In semi-supervised semantic segmentation, the Mean Teacher- and co-training-based approaches are employed to mitigate confirmation bias and coupling problems. However, despite their high performance, these approaches frequently involve complex training pipelines and a substantial computational burden, limiting the scalability and compatibility of these methods. In this paper, we propose a PrevMatch framework that effectively mitigates the aforementioned limitations by maximizing the utilization of the temporal knowledge obtained during the training process. The PrevMatch framework relies on two core strategies: (1) we reconsider the use of temporal knowledge and thus directly utilize previous models obtained during training to generate additional pseudo-label guidance, referred to as previous guidance. (2) we design a highly randomized ensemble strategy to maximize the effectiveness of the previous guidance. PrevMatch, a simple yet effective plug-in method, can be seamlessly integrated into existing semi-supervised learning frameworks with minimal computational overhead. Experimental results on three benchmark semantic segmentation datasets show that incorporating PrevMatch into existing methods significantly improves their performance. Furthermore, our analysis indicates that PrevMatch facilitates stable optimization during training, resulting in improved generalization performance.", "link": "http://arxiv.org/abs/2405.20610v2", "date": "2026-01-05", "relevancy": 2.7545, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5682}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.552}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PrevMatch%3A%20Revisiting%20and%20Maximizing%20Temporal%20Knowledge%20in%20Semi-Supervised%20Semantic%20Segmentation&body=Title%3A%20PrevMatch%3A%20Revisiting%20and%20Maximizing%20Temporal%20Knowledge%20in%20Semi-Supervised%20Semantic%20Segmentation%0AAuthor%3A%20Wooseok%20Shin%20and%20Hyun%20Joon%20Park%20and%20Jin%20Sob%20Kim%20and%20Juan%20Yun%20and%20Se%20Hong%20Park%20and%20Sung%20Won%20Han%0AAbstract%3A%20In%20semi-supervised%20semantic%20segmentation%2C%20the%20Mean%20Teacher-%20and%20co-training-based%20approaches%20are%20employed%20to%20mitigate%20confirmation%20bias%20and%20coupling%20problems.%20However%2C%20despite%20their%20high%20performance%2C%20these%20approaches%20frequently%20involve%20complex%20training%20pipelines%20and%20a%20substantial%20computational%20burden%2C%20limiting%20the%20scalability%20and%20compatibility%20of%20these%20methods.%20In%20this%20paper%2C%20we%20propose%20a%20PrevMatch%20framework%20that%20effectively%20mitigates%20the%20aforementioned%20limitations%20by%20maximizing%20the%20utilization%20of%20the%20temporal%20knowledge%20obtained%20during%20the%20training%20process.%20The%20PrevMatch%20framework%20relies%20on%20two%20core%20strategies%3A%20%281%29%20we%20reconsider%20the%20use%20of%20temporal%20knowledge%20and%20thus%20directly%20utilize%20previous%20models%20obtained%20during%20training%20to%20generate%20additional%20pseudo-label%20guidance%2C%20referred%20to%20as%20previous%20guidance.%20%282%29%20we%20design%20a%20highly%20randomized%20ensemble%20strategy%20to%20maximize%20the%20effectiveness%20of%20the%20previous%20guidance.%20PrevMatch%2C%20a%20simple%20yet%20effective%20plug-in%20method%2C%20can%20be%20seamlessly%20integrated%20into%20existing%20semi-supervised%20learning%20frameworks%20with%20minimal%20computational%20overhead.%20Experimental%20results%20on%20three%20benchmark%20semantic%20segmentation%20datasets%20show%20that%20incorporating%20PrevMatch%20into%20existing%20methods%20significantly%20improves%20their%20performance.%20Furthermore%2C%20our%20analysis%20indicates%20that%20PrevMatch%20facilitates%20stable%20optimization%20during%20training%2C%20resulting%20in%20improved%20generalization%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2405.20610v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrevMatch%253A%2520Revisiting%2520and%2520Maximizing%2520Temporal%2520Knowledge%2520in%2520Semi-Supervised%2520Semantic%2520Segmentation%26entry.906535625%3DWooseok%2520Shin%2520and%2520Hyun%2520Joon%2520Park%2520and%2520Jin%2520Sob%2520Kim%2520and%2520Juan%2520Yun%2520and%2520Se%2520Hong%2520Park%2520and%2520Sung%2520Won%2520Han%26entry.1292438233%3DIn%2520semi-supervised%2520semantic%2520segmentation%252C%2520the%2520Mean%2520Teacher-%2520and%2520co-training-based%2520approaches%2520are%2520employed%2520to%2520mitigate%2520confirmation%2520bias%2520and%2520coupling%2520problems.%2520However%252C%2520despite%2520their%2520high%2520performance%252C%2520these%2520approaches%2520frequently%2520involve%2520complex%2520training%2520pipelines%2520and%2520a%2520substantial%2520computational%2520burden%252C%2520limiting%2520the%2520scalability%2520and%2520compatibility%2520of%2520these%2520methods.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520PrevMatch%2520framework%2520that%2520effectively%2520mitigates%2520the%2520aforementioned%2520limitations%2520by%2520maximizing%2520the%2520utilization%2520of%2520the%2520temporal%2520knowledge%2520obtained%2520during%2520the%2520training%2520process.%2520The%2520PrevMatch%2520framework%2520relies%2520on%2520two%2520core%2520strategies%253A%2520%25281%2529%2520we%2520reconsider%2520the%2520use%2520of%2520temporal%2520knowledge%2520and%2520thus%2520directly%2520utilize%2520previous%2520models%2520obtained%2520during%2520training%2520to%2520generate%2520additional%2520pseudo-label%2520guidance%252C%2520referred%2520to%2520as%2520previous%2520guidance.%2520%25282%2529%2520we%2520design%2520a%2520highly%2520randomized%2520ensemble%2520strategy%2520to%2520maximize%2520the%2520effectiveness%2520of%2520the%2520previous%2520guidance.%2520PrevMatch%252C%2520a%2520simple%2520yet%2520effective%2520plug-in%2520method%252C%2520can%2520be%2520seamlessly%2520integrated%2520into%2520existing%2520semi-supervised%2520learning%2520frameworks%2520with%2520minimal%2520computational%2520overhead.%2520Experimental%2520results%2520on%2520three%2520benchmark%2520semantic%2520segmentation%2520datasets%2520show%2520that%2520incorporating%2520PrevMatch%2520into%2520existing%2520methods%2520significantly%2520improves%2520their%2520performance.%2520Furthermore%252C%2520our%2520analysis%2520indicates%2520that%2520PrevMatch%2520facilitates%2520stable%2520optimization%2520during%2520training%252C%2520resulting%2520in%2520improved%2520generalization%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20610v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PrevMatch%3A%20Revisiting%20and%20Maximizing%20Temporal%20Knowledge%20in%20Semi-Supervised%20Semantic%20Segmentation&entry.906535625=Wooseok%20Shin%20and%20Hyun%20Joon%20Park%20and%20Jin%20Sob%20Kim%20and%20Juan%20Yun%20and%20Se%20Hong%20Park%20and%20Sung%20Won%20Han&entry.1292438233=In%20semi-supervised%20semantic%20segmentation%2C%20the%20Mean%20Teacher-%20and%20co-training-based%20approaches%20are%20employed%20to%20mitigate%20confirmation%20bias%20and%20coupling%20problems.%20However%2C%20despite%20their%20high%20performance%2C%20these%20approaches%20frequently%20involve%20complex%20training%20pipelines%20and%20a%20substantial%20computational%20burden%2C%20limiting%20the%20scalability%20and%20compatibility%20of%20these%20methods.%20In%20this%20paper%2C%20we%20propose%20a%20PrevMatch%20framework%20that%20effectively%20mitigates%20the%20aforementioned%20limitations%20by%20maximizing%20the%20utilization%20of%20the%20temporal%20knowledge%20obtained%20during%20the%20training%20process.%20The%20PrevMatch%20framework%20relies%20on%20two%20core%20strategies%3A%20%281%29%20we%20reconsider%20the%20use%20of%20temporal%20knowledge%20and%20thus%20directly%20utilize%20previous%20models%20obtained%20during%20training%20to%20generate%20additional%20pseudo-label%20guidance%2C%20referred%20to%20as%20previous%20guidance.%20%282%29%20we%20design%20a%20highly%20randomized%20ensemble%20strategy%20to%20maximize%20the%20effectiveness%20of%20the%20previous%20guidance.%20PrevMatch%2C%20a%20simple%20yet%20effective%20plug-in%20method%2C%20can%20be%20seamlessly%20integrated%20into%20existing%20semi-supervised%20learning%20frameworks%20with%20minimal%20computational%20overhead.%20Experimental%20results%20on%20three%20benchmark%20semantic%20segmentation%20datasets%20show%20that%20incorporating%20PrevMatch%20into%20existing%20methods%20significantly%20improves%20their%20performance.%20Furthermore%2C%20our%20analysis%20indicates%20that%20PrevMatch%20facilitates%20stable%20optimization%20during%20training%2C%20resulting%20in%20improved%20generalization%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2405.20610v2&entry.124074799=Read"},
{"title": "ULTra: Unveiling Latent Token Interpretability in Transformer-Based Understanding and Segmentation", "author": "Hesam Hosseini and Ghazal Hosseini Mighan and Amirabbas Afzali and Sajjad Amini and Amir Houmansadr", "abstract": "Transformers have revolutionized Computer Vision (CV) through self-attention mechanisms. However, their complexity makes latent token representations difficult to interpret. We introduce ULTra, a framework for interpreting Transformer embeddings and uncovering meaningful semantic patterns within them. ULTra enables unsupervised semantic segmentation using pre-trained models without requiring fine-tuning. Additionally, we propose a self-supervised training approach that refines segmentation performance by learning an external transformation matrix without modifying the underlying model. Our method achieves state-of-the-art performance in unsupervised semantic segmentation, outperforming existing segmentation methods. Furthermore, we validate ULTra for model interpretation on both synthetic and real-world scenarios, including Object Selection and interpretable text summarization using LLMs, demonstrating its broad applicability in explaining the semantic structure of latent token representations.", "link": "http://arxiv.org/abs/2411.12589v3", "date": "2026-01-05", "relevancy": 2.7408, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.552}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5462}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ULTra%3A%20Unveiling%20Latent%20Token%20Interpretability%20in%20Transformer-Based%20Understanding%20and%20Segmentation&body=Title%3A%20ULTra%3A%20Unveiling%20Latent%20Token%20Interpretability%20in%20Transformer-Based%20Understanding%20and%20Segmentation%0AAuthor%3A%20Hesam%20Hosseini%20and%20Ghazal%20Hosseini%20Mighan%20and%20Amirabbas%20Afzali%20and%20Sajjad%20Amini%20and%20Amir%20Houmansadr%0AAbstract%3A%20Transformers%20have%20revolutionized%20Computer%20Vision%20%28CV%29%20through%20self-attention%20mechanisms.%20However%2C%20their%20complexity%20makes%20latent%20token%20representations%20difficult%20to%20interpret.%20We%20introduce%20ULTra%2C%20a%20framework%20for%20interpreting%20Transformer%20embeddings%20and%20uncovering%20meaningful%20semantic%20patterns%20within%20them.%20ULTra%20enables%20unsupervised%20semantic%20segmentation%20using%20pre-trained%20models%20without%20requiring%20fine-tuning.%20Additionally%2C%20we%20propose%20a%20self-supervised%20training%20approach%20that%20refines%20segmentation%20performance%20by%20learning%20an%20external%20transformation%20matrix%20without%20modifying%20the%20underlying%20model.%20Our%20method%20achieves%20state-of-the-art%20performance%20in%20unsupervised%20semantic%20segmentation%2C%20outperforming%20existing%20segmentation%20methods.%20Furthermore%2C%20we%20validate%20ULTra%20for%20model%20interpretation%20on%20both%20synthetic%20and%20real-world%20scenarios%2C%20including%20Object%20Selection%20and%20interpretable%20text%20summarization%20using%20LLMs%2C%20demonstrating%20its%20broad%20applicability%20in%20explaining%20the%20semantic%20structure%20of%20latent%20token%20representations.%0ALink%3A%20http%3A//arxiv.org/abs/2411.12589v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DULTra%253A%2520Unveiling%2520Latent%2520Token%2520Interpretability%2520in%2520Transformer-Based%2520Understanding%2520and%2520Segmentation%26entry.906535625%3DHesam%2520Hosseini%2520and%2520Ghazal%2520Hosseini%2520Mighan%2520and%2520Amirabbas%2520Afzali%2520and%2520Sajjad%2520Amini%2520and%2520Amir%2520Houmansadr%26entry.1292438233%3DTransformers%2520have%2520revolutionized%2520Computer%2520Vision%2520%2528CV%2529%2520through%2520self-attention%2520mechanisms.%2520However%252C%2520their%2520complexity%2520makes%2520latent%2520token%2520representations%2520difficult%2520to%2520interpret.%2520We%2520introduce%2520ULTra%252C%2520a%2520framework%2520for%2520interpreting%2520Transformer%2520embeddings%2520and%2520uncovering%2520meaningful%2520semantic%2520patterns%2520within%2520them.%2520ULTra%2520enables%2520unsupervised%2520semantic%2520segmentation%2520using%2520pre-trained%2520models%2520without%2520requiring%2520fine-tuning.%2520Additionally%252C%2520we%2520propose%2520a%2520self-supervised%2520training%2520approach%2520that%2520refines%2520segmentation%2520performance%2520by%2520learning%2520an%2520external%2520transformation%2520matrix%2520without%2520modifying%2520the%2520underlying%2520model.%2520Our%2520method%2520achieves%2520state-of-the-art%2520performance%2520in%2520unsupervised%2520semantic%2520segmentation%252C%2520outperforming%2520existing%2520segmentation%2520methods.%2520Furthermore%252C%2520we%2520validate%2520ULTra%2520for%2520model%2520interpretation%2520on%2520both%2520synthetic%2520and%2520real-world%2520scenarios%252C%2520including%2520Object%2520Selection%2520and%2520interpretable%2520text%2520summarization%2520using%2520LLMs%252C%2520demonstrating%2520its%2520broad%2520applicability%2520in%2520explaining%2520the%2520semantic%2520structure%2520of%2520latent%2520token%2520representations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.12589v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ULTra%3A%20Unveiling%20Latent%20Token%20Interpretability%20in%20Transformer-Based%20Understanding%20and%20Segmentation&entry.906535625=Hesam%20Hosseini%20and%20Ghazal%20Hosseini%20Mighan%20and%20Amirabbas%20Afzali%20and%20Sajjad%20Amini%20and%20Amir%20Houmansadr&entry.1292438233=Transformers%20have%20revolutionized%20Computer%20Vision%20%28CV%29%20through%20self-attention%20mechanisms.%20However%2C%20their%20complexity%20makes%20latent%20token%20representations%20difficult%20to%20interpret.%20We%20introduce%20ULTra%2C%20a%20framework%20for%20interpreting%20Transformer%20embeddings%20and%20uncovering%20meaningful%20semantic%20patterns%20within%20them.%20ULTra%20enables%20unsupervised%20semantic%20segmentation%20using%20pre-trained%20models%20without%20requiring%20fine-tuning.%20Additionally%2C%20we%20propose%20a%20self-supervised%20training%20approach%20that%20refines%20segmentation%20performance%20by%20learning%20an%20external%20transformation%20matrix%20without%20modifying%20the%20underlying%20model.%20Our%20method%20achieves%20state-of-the-art%20performance%20in%20unsupervised%20semantic%20segmentation%2C%20outperforming%20existing%20segmentation%20methods.%20Furthermore%2C%20we%20validate%20ULTra%20for%20model%20interpretation%20on%20both%20synthetic%20and%20real-world%20scenarios%2C%20including%20Object%20Selection%20and%20interpretable%20text%20summarization%20using%20LLMs%2C%20demonstrating%20its%20broad%20applicability%20in%20explaining%20the%20semantic%20structure%20of%20latent%20token%20representations.&entry.1838667208=http%3A//arxiv.org/abs/2411.12589v3&entry.124074799=Read"},
{"title": "DatBench: Discriminative, Faithful, and Efficient VLM Evaluations", "author": "Siddharth Joshi and Haoli Yin and Rishabh Adiga and Ricardo Monti and Aldo Carranza and Alex Fang and Alvin Deng and Amro Abbas and Brett Larsen and Cody Blakeney and Darren Teh and David Schwab and Fan Pan and Haakon Mongstad and Jack Urbanek and Jason Lee and Jason Telanoff and Josh Wills and Kaleigh Mentzer and Luke Merrick and Parth Doshi and Paul Burstein and Pratyush Maini and Scott Loftin and Spandan Das and Tony Jiang and Vineeth Dorna and Zhengping Wang and Bogdan Gaza and Ari Morcos and Matthew Leavitt", "abstract": "Empirical evaluation serves as the primary compass guiding research progress in foundation models. Despite a large body of work focused on training frontier vision-language models (VLMs), approaches to their evaluation remain nascent. To guide their maturation, we propose three desiderata that evaluations should satisfy: (1) faithfulness to the modality and application, (2) discriminability between models of varying quality, and (3) efficiency in compute. Through this lens, we identify critical failure modes that violate faithfulness and discriminability, misrepresenting model capabilities: (i) multiple-choice formats reward guessing, poorly reflect downstream use cases, and saturate early as models improve; (ii) blindly solvable questions, which can be answered without images, constitute up to 70% of some evaluations; and (iii) mislabeled or ambiguous samples compromise up to 42% of examples in certain datasets. Regarding efficiency, the computational burden of evaluating frontier models has become prohibitive: by some accounts, nearly 20% of development compute is devoted to evaluation alone. Rather than discarding existing benchmarks, we curate them via transformation and filtering to maximize fidelity and discriminability. We find that converting multiple-choice questions to generative tasks reveals sharp capability drops of up to 35%. In addition, filtering blindly solvable and mislabeled samples improves discriminative power while simultaneously reducing computational cost. We release DatBench-Full, a cleaned evaluation suite of 33 datasets spanning nine VLM capabilities, and DatBench, a discriminative subset that achieves 13x average speedup (up to 50x) while closely matching the discriminative power of the original datasets. Our work outlines a path toward evaluation practices that are both rigorous and sustainable as VLMs continue to scale.", "link": "http://arxiv.org/abs/2601.02316v1", "date": "2026-01-05", "relevancy": 2.7172, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5563}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5563}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DatBench%3A%20Discriminative%2C%20Faithful%2C%20and%20Efficient%20VLM%20Evaluations&body=Title%3A%20DatBench%3A%20Discriminative%2C%20Faithful%2C%20and%20Efficient%20VLM%20Evaluations%0AAuthor%3A%20Siddharth%20Joshi%20and%20Haoli%20Yin%20and%20Rishabh%20Adiga%20and%20Ricardo%20Monti%20and%20Aldo%20Carranza%20and%20Alex%20Fang%20and%20Alvin%20Deng%20and%20Amro%20Abbas%20and%20Brett%20Larsen%20and%20Cody%20Blakeney%20and%20Darren%20Teh%20and%20David%20Schwab%20and%20Fan%20Pan%20and%20Haakon%20Mongstad%20and%20Jack%20Urbanek%20and%20Jason%20Lee%20and%20Jason%20Telanoff%20and%20Josh%20Wills%20and%20Kaleigh%20Mentzer%20and%20Luke%20Merrick%20and%20Parth%20Doshi%20and%20Paul%20Burstein%20and%20Pratyush%20Maini%20and%20Scott%20Loftin%20and%20Spandan%20Das%20and%20Tony%20Jiang%20and%20Vineeth%20Dorna%20and%20Zhengping%20Wang%20and%20Bogdan%20Gaza%20and%20Ari%20Morcos%20and%20Matthew%20Leavitt%0AAbstract%3A%20Empirical%20evaluation%20serves%20as%20the%20primary%20compass%20guiding%20research%20progress%20in%20foundation%20models.%20Despite%20a%20large%20body%20of%20work%20focused%20on%20training%20frontier%20vision-language%20models%20%28VLMs%29%2C%20approaches%20to%20their%20evaluation%20remain%20nascent.%20To%20guide%20their%20maturation%2C%20we%20propose%20three%20desiderata%20that%20evaluations%20should%20satisfy%3A%20%281%29%20faithfulness%20to%20the%20modality%20and%20application%2C%20%282%29%20discriminability%20between%20models%20of%20varying%20quality%2C%20and%20%283%29%20efficiency%20in%20compute.%20Through%20this%20lens%2C%20we%20identify%20critical%20failure%20modes%20that%20violate%20faithfulness%20and%20discriminability%2C%20misrepresenting%20model%20capabilities%3A%20%28i%29%20multiple-choice%20formats%20reward%20guessing%2C%20poorly%20reflect%20downstream%20use%20cases%2C%20and%20saturate%20early%20as%20models%20improve%3B%20%28ii%29%20blindly%20solvable%20questions%2C%20which%20can%20be%20answered%20without%20images%2C%20constitute%20up%20to%2070%25%20of%20some%20evaluations%3B%20and%20%28iii%29%20mislabeled%20or%20ambiguous%20samples%20compromise%20up%20to%2042%25%20of%20examples%20in%20certain%20datasets.%20Regarding%20efficiency%2C%20the%20computational%20burden%20of%20evaluating%20frontier%20models%20has%20become%20prohibitive%3A%20by%20some%20accounts%2C%20nearly%2020%25%20of%20development%20compute%20is%20devoted%20to%20evaluation%20alone.%20Rather%20than%20discarding%20existing%20benchmarks%2C%20we%20curate%20them%20via%20transformation%20and%20filtering%20to%20maximize%20fidelity%20and%20discriminability.%20We%20find%20that%20converting%20multiple-choice%20questions%20to%20generative%20tasks%20reveals%20sharp%20capability%20drops%20of%20up%20to%2035%25.%20In%20addition%2C%20filtering%20blindly%20solvable%20and%20mislabeled%20samples%20improves%20discriminative%20power%20while%20simultaneously%20reducing%20computational%20cost.%20We%20release%20DatBench-Full%2C%20a%20cleaned%20evaluation%20suite%20of%2033%20datasets%20spanning%20nine%20VLM%20capabilities%2C%20and%20DatBench%2C%20a%20discriminative%20subset%20that%20achieves%2013x%20average%20speedup%20%28up%20to%2050x%29%20while%20closely%20matching%20the%20discriminative%20power%20of%20the%20original%20datasets.%20Our%20work%20outlines%20a%20path%20toward%20evaluation%20practices%20that%20are%20both%20rigorous%20and%20sustainable%20as%20VLMs%20continue%20to%20scale.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02316v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDatBench%253A%2520Discriminative%252C%2520Faithful%252C%2520and%2520Efficient%2520VLM%2520Evaluations%26entry.906535625%3DSiddharth%2520Joshi%2520and%2520Haoli%2520Yin%2520and%2520Rishabh%2520Adiga%2520and%2520Ricardo%2520Monti%2520and%2520Aldo%2520Carranza%2520and%2520Alex%2520Fang%2520and%2520Alvin%2520Deng%2520and%2520Amro%2520Abbas%2520and%2520Brett%2520Larsen%2520and%2520Cody%2520Blakeney%2520and%2520Darren%2520Teh%2520and%2520David%2520Schwab%2520and%2520Fan%2520Pan%2520and%2520Haakon%2520Mongstad%2520and%2520Jack%2520Urbanek%2520and%2520Jason%2520Lee%2520and%2520Jason%2520Telanoff%2520and%2520Josh%2520Wills%2520and%2520Kaleigh%2520Mentzer%2520and%2520Luke%2520Merrick%2520and%2520Parth%2520Doshi%2520and%2520Paul%2520Burstein%2520and%2520Pratyush%2520Maini%2520and%2520Scott%2520Loftin%2520and%2520Spandan%2520Das%2520and%2520Tony%2520Jiang%2520and%2520Vineeth%2520Dorna%2520and%2520Zhengping%2520Wang%2520and%2520Bogdan%2520Gaza%2520and%2520Ari%2520Morcos%2520and%2520Matthew%2520Leavitt%26entry.1292438233%3DEmpirical%2520evaluation%2520serves%2520as%2520the%2520primary%2520compass%2520guiding%2520research%2520progress%2520in%2520foundation%2520models.%2520Despite%2520a%2520large%2520body%2520of%2520work%2520focused%2520on%2520training%2520frontier%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520approaches%2520to%2520their%2520evaluation%2520remain%2520nascent.%2520To%2520guide%2520their%2520maturation%252C%2520we%2520propose%2520three%2520desiderata%2520that%2520evaluations%2520should%2520satisfy%253A%2520%25281%2529%2520faithfulness%2520to%2520the%2520modality%2520and%2520application%252C%2520%25282%2529%2520discriminability%2520between%2520models%2520of%2520varying%2520quality%252C%2520and%2520%25283%2529%2520efficiency%2520in%2520compute.%2520Through%2520this%2520lens%252C%2520we%2520identify%2520critical%2520failure%2520modes%2520that%2520violate%2520faithfulness%2520and%2520discriminability%252C%2520misrepresenting%2520model%2520capabilities%253A%2520%2528i%2529%2520multiple-choice%2520formats%2520reward%2520guessing%252C%2520poorly%2520reflect%2520downstream%2520use%2520cases%252C%2520and%2520saturate%2520early%2520as%2520models%2520improve%253B%2520%2528ii%2529%2520blindly%2520solvable%2520questions%252C%2520which%2520can%2520be%2520answered%2520without%2520images%252C%2520constitute%2520up%2520to%252070%2525%2520of%2520some%2520evaluations%253B%2520and%2520%2528iii%2529%2520mislabeled%2520or%2520ambiguous%2520samples%2520compromise%2520up%2520to%252042%2525%2520of%2520examples%2520in%2520certain%2520datasets.%2520Regarding%2520efficiency%252C%2520the%2520computational%2520burden%2520of%2520evaluating%2520frontier%2520models%2520has%2520become%2520prohibitive%253A%2520by%2520some%2520accounts%252C%2520nearly%252020%2525%2520of%2520development%2520compute%2520is%2520devoted%2520to%2520evaluation%2520alone.%2520Rather%2520than%2520discarding%2520existing%2520benchmarks%252C%2520we%2520curate%2520them%2520via%2520transformation%2520and%2520filtering%2520to%2520maximize%2520fidelity%2520and%2520discriminability.%2520We%2520find%2520that%2520converting%2520multiple-choice%2520questions%2520to%2520generative%2520tasks%2520reveals%2520sharp%2520capability%2520drops%2520of%2520up%2520to%252035%2525.%2520In%2520addition%252C%2520filtering%2520blindly%2520solvable%2520and%2520mislabeled%2520samples%2520improves%2520discriminative%2520power%2520while%2520simultaneously%2520reducing%2520computational%2520cost.%2520We%2520release%2520DatBench-Full%252C%2520a%2520cleaned%2520evaluation%2520suite%2520of%252033%2520datasets%2520spanning%2520nine%2520VLM%2520capabilities%252C%2520and%2520DatBench%252C%2520a%2520discriminative%2520subset%2520that%2520achieves%252013x%2520average%2520speedup%2520%2528up%2520to%252050x%2529%2520while%2520closely%2520matching%2520the%2520discriminative%2520power%2520of%2520the%2520original%2520datasets.%2520Our%2520work%2520outlines%2520a%2520path%2520toward%2520evaluation%2520practices%2520that%2520are%2520both%2520rigorous%2520and%2520sustainable%2520as%2520VLMs%2520continue%2520to%2520scale.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02316v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DatBench%3A%20Discriminative%2C%20Faithful%2C%20and%20Efficient%20VLM%20Evaluations&entry.906535625=Siddharth%20Joshi%20and%20Haoli%20Yin%20and%20Rishabh%20Adiga%20and%20Ricardo%20Monti%20and%20Aldo%20Carranza%20and%20Alex%20Fang%20and%20Alvin%20Deng%20and%20Amro%20Abbas%20and%20Brett%20Larsen%20and%20Cody%20Blakeney%20and%20Darren%20Teh%20and%20David%20Schwab%20and%20Fan%20Pan%20and%20Haakon%20Mongstad%20and%20Jack%20Urbanek%20and%20Jason%20Lee%20and%20Jason%20Telanoff%20and%20Josh%20Wills%20and%20Kaleigh%20Mentzer%20and%20Luke%20Merrick%20and%20Parth%20Doshi%20and%20Paul%20Burstein%20and%20Pratyush%20Maini%20and%20Scott%20Loftin%20and%20Spandan%20Das%20and%20Tony%20Jiang%20and%20Vineeth%20Dorna%20and%20Zhengping%20Wang%20and%20Bogdan%20Gaza%20and%20Ari%20Morcos%20and%20Matthew%20Leavitt&entry.1292438233=Empirical%20evaluation%20serves%20as%20the%20primary%20compass%20guiding%20research%20progress%20in%20foundation%20models.%20Despite%20a%20large%20body%20of%20work%20focused%20on%20training%20frontier%20vision-language%20models%20%28VLMs%29%2C%20approaches%20to%20their%20evaluation%20remain%20nascent.%20To%20guide%20their%20maturation%2C%20we%20propose%20three%20desiderata%20that%20evaluations%20should%20satisfy%3A%20%281%29%20faithfulness%20to%20the%20modality%20and%20application%2C%20%282%29%20discriminability%20between%20models%20of%20varying%20quality%2C%20and%20%283%29%20efficiency%20in%20compute.%20Through%20this%20lens%2C%20we%20identify%20critical%20failure%20modes%20that%20violate%20faithfulness%20and%20discriminability%2C%20misrepresenting%20model%20capabilities%3A%20%28i%29%20multiple-choice%20formats%20reward%20guessing%2C%20poorly%20reflect%20downstream%20use%20cases%2C%20and%20saturate%20early%20as%20models%20improve%3B%20%28ii%29%20blindly%20solvable%20questions%2C%20which%20can%20be%20answered%20without%20images%2C%20constitute%20up%20to%2070%25%20of%20some%20evaluations%3B%20and%20%28iii%29%20mislabeled%20or%20ambiguous%20samples%20compromise%20up%20to%2042%25%20of%20examples%20in%20certain%20datasets.%20Regarding%20efficiency%2C%20the%20computational%20burden%20of%20evaluating%20frontier%20models%20has%20become%20prohibitive%3A%20by%20some%20accounts%2C%20nearly%2020%25%20of%20development%20compute%20is%20devoted%20to%20evaluation%20alone.%20Rather%20than%20discarding%20existing%20benchmarks%2C%20we%20curate%20them%20via%20transformation%20and%20filtering%20to%20maximize%20fidelity%20and%20discriminability.%20We%20find%20that%20converting%20multiple-choice%20questions%20to%20generative%20tasks%20reveals%20sharp%20capability%20drops%20of%20up%20to%2035%25.%20In%20addition%2C%20filtering%20blindly%20solvable%20and%20mislabeled%20samples%20improves%20discriminative%20power%20while%20simultaneously%20reducing%20computational%20cost.%20We%20release%20DatBench-Full%2C%20a%20cleaned%20evaluation%20suite%20of%2033%20datasets%20spanning%20nine%20VLM%20capabilities%2C%20and%20DatBench%2C%20a%20discriminative%20subset%20that%20achieves%2013x%20average%20speedup%20%28up%20to%2050x%29%20while%20closely%20matching%20the%20discriminative%20power%20of%20the%20original%20datasets.%20Our%20work%20outlines%20a%20path%20toward%20evaluation%20practices%20that%20are%20both%20rigorous%20and%20sustainable%20as%20VLMs%20continue%20to%20scale.&entry.1838667208=http%3A//arxiv.org/abs/2601.02316v1&entry.124074799=Read"},
{"title": "VALLR: Visual ASR Language Model for Lip Reading", "author": "Marshall Thomas and Edward Fish and Richard Bowden", "abstract": "Lip Reading, or Visual Automatic Speech Recognition (V-ASR), is a complex task requiring the interpretation of spoken language exclusively from visual cues, primarily lip movements and facial expressions. This task is especially challenging due to the absence of auditory information and the inherent ambiguity when visually distinguishing phonemes that have overlapping visemes where different phonemes appear identical on the lips. Current methods typically attempt to predict words or characters directly from these visual cues, but this approach frequently encounters high error rates due to coarticulation effects and viseme ambiguity. We propose a novel two-stage, phoneme-centric framework for Visual Automatic Speech Recognition (V-ASR) that addresses these longstanding challenges. First, our model predicts a compact sequence of phonemes from visual inputs using a Video Transformer with a CTC head, thereby reducing the task complexity and achieving robust speaker invariance. This phoneme output then serves as the input to a fine-tuned Large Language Model (LLM), which reconstructs coherent words and sentences by leveraging broader linguistic context. Unlike existing methods that either predict words directly-often faltering on visually similar phonemes-or rely on large-scale multimodal pre-training, our approach explicitly encodes intermediate linguistic structure while remaining highly data efficient. We demonstrate state-of-the-art performance on two challenging datasets, LRS2 and LRS3, where our method achieves significant reductions in Word Error Rate (WER) achieving a SOTA WER of 18.7 on LRS3 despite using 99.4% less labelled data than the next best approach.", "link": "http://arxiv.org/abs/2503.21408v2", "date": "2026-01-05", "relevancy": 2.7122, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5473}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5473}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VALLR%3A%20Visual%20ASR%20Language%20Model%20for%20Lip%20Reading&body=Title%3A%20VALLR%3A%20Visual%20ASR%20Language%20Model%20for%20Lip%20Reading%0AAuthor%3A%20Marshall%20Thomas%20and%20Edward%20Fish%20and%20Richard%20Bowden%0AAbstract%3A%20Lip%20Reading%2C%20or%20Visual%20Automatic%20Speech%20Recognition%20%28V-ASR%29%2C%20is%20a%20complex%20task%20requiring%20the%20interpretation%20of%20spoken%20language%20exclusively%20from%20visual%20cues%2C%20primarily%20lip%20movements%20and%20facial%20expressions.%20This%20task%20is%20especially%20challenging%20due%20to%20the%20absence%20of%20auditory%20information%20and%20the%20inherent%20ambiguity%20when%20visually%20distinguishing%20phonemes%20that%20have%20overlapping%20visemes%20where%20different%20phonemes%20appear%20identical%20on%20the%20lips.%20Current%20methods%20typically%20attempt%20to%20predict%20words%20or%20characters%20directly%20from%20these%20visual%20cues%2C%20but%20this%20approach%20frequently%20encounters%20high%20error%20rates%20due%20to%20coarticulation%20effects%20and%20viseme%20ambiguity.%20We%20propose%20a%20novel%20two-stage%2C%20phoneme-centric%20framework%20for%20Visual%20Automatic%20Speech%20Recognition%20%28V-ASR%29%20that%20addresses%20these%20longstanding%20challenges.%20First%2C%20our%20model%20predicts%20a%20compact%20sequence%20of%20phonemes%20from%20visual%20inputs%20using%20a%20Video%20Transformer%20with%20a%20CTC%20head%2C%20thereby%20reducing%20the%20task%20complexity%20and%20achieving%20robust%20speaker%20invariance.%20This%20phoneme%20output%20then%20serves%20as%20the%20input%20to%20a%20fine-tuned%20Large%20Language%20Model%20%28LLM%29%2C%20which%20reconstructs%20coherent%20words%20and%20sentences%20by%20leveraging%20broader%20linguistic%20context.%20Unlike%20existing%20methods%20that%20either%20predict%20words%20directly-often%20faltering%20on%20visually%20similar%20phonemes-or%20rely%20on%20large-scale%20multimodal%20pre-training%2C%20our%20approach%20explicitly%20encodes%20intermediate%20linguistic%20structure%20while%20remaining%20highly%20data%20efficient.%20We%20demonstrate%20state-of-the-art%20performance%20on%20two%20challenging%20datasets%2C%20LRS2%20and%20LRS3%2C%20where%20our%20method%20achieves%20significant%20reductions%20in%20Word%20Error%20Rate%20%28WER%29%20achieving%20a%20SOTA%20WER%20of%2018.7%20on%20LRS3%20despite%20using%2099.4%25%20less%20labelled%20data%20than%20the%20next%20best%20approach.%0ALink%3A%20http%3A//arxiv.org/abs/2503.21408v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVALLR%253A%2520Visual%2520ASR%2520Language%2520Model%2520for%2520Lip%2520Reading%26entry.906535625%3DMarshall%2520Thomas%2520and%2520Edward%2520Fish%2520and%2520Richard%2520Bowden%26entry.1292438233%3DLip%2520Reading%252C%2520or%2520Visual%2520Automatic%2520Speech%2520Recognition%2520%2528V-ASR%2529%252C%2520is%2520a%2520complex%2520task%2520requiring%2520the%2520interpretation%2520of%2520spoken%2520language%2520exclusively%2520from%2520visual%2520cues%252C%2520primarily%2520lip%2520movements%2520and%2520facial%2520expressions.%2520This%2520task%2520is%2520especially%2520challenging%2520due%2520to%2520the%2520absence%2520of%2520auditory%2520information%2520and%2520the%2520inherent%2520ambiguity%2520when%2520visually%2520distinguishing%2520phonemes%2520that%2520have%2520overlapping%2520visemes%2520where%2520different%2520phonemes%2520appear%2520identical%2520on%2520the%2520lips.%2520Current%2520methods%2520typically%2520attempt%2520to%2520predict%2520words%2520or%2520characters%2520directly%2520from%2520these%2520visual%2520cues%252C%2520but%2520this%2520approach%2520frequently%2520encounters%2520high%2520error%2520rates%2520due%2520to%2520coarticulation%2520effects%2520and%2520viseme%2520ambiguity.%2520We%2520propose%2520a%2520novel%2520two-stage%252C%2520phoneme-centric%2520framework%2520for%2520Visual%2520Automatic%2520Speech%2520Recognition%2520%2528V-ASR%2529%2520that%2520addresses%2520these%2520longstanding%2520challenges.%2520First%252C%2520our%2520model%2520predicts%2520a%2520compact%2520sequence%2520of%2520phonemes%2520from%2520visual%2520inputs%2520using%2520a%2520Video%2520Transformer%2520with%2520a%2520CTC%2520head%252C%2520thereby%2520reducing%2520the%2520task%2520complexity%2520and%2520achieving%2520robust%2520speaker%2520invariance.%2520This%2520phoneme%2520output%2520then%2520serves%2520as%2520the%2520input%2520to%2520a%2520fine-tuned%2520Large%2520Language%2520Model%2520%2528LLM%2529%252C%2520which%2520reconstructs%2520coherent%2520words%2520and%2520sentences%2520by%2520leveraging%2520broader%2520linguistic%2520context.%2520Unlike%2520existing%2520methods%2520that%2520either%2520predict%2520words%2520directly-often%2520faltering%2520on%2520visually%2520similar%2520phonemes-or%2520rely%2520on%2520large-scale%2520multimodal%2520pre-training%252C%2520our%2520approach%2520explicitly%2520encodes%2520intermediate%2520linguistic%2520structure%2520while%2520remaining%2520highly%2520data%2520efficient.%2520We%2520demonstrate%2520state-of-the-art%2520performance%2520on%2520two%2520challenging%2520datasets%252C%2520LRS2%2520and%2520LRS3%252C%2520where%2520our%2520method%2520achieves%2520significant%2520reductions%2520in%2520Word%2520Error%2520Rate%2520%2528WER%2529%2520achieving%2520a%2520SOTA%2520WER%2520of%252018.7%2520on%2520LRS3%2520despite%2520using%252099.4%2525%2520less%2520labelled%2520data%2520than%2520the%2520next%2520best%2520approach.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.21408v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VALLR%3A%20Visual%20ASR%20Language%20Model%20for%20Lip%20Reading&entry.906535625=Marshall%20Thomas%20and%20Edward%20Fish%20and%20Richard%20Bowden&entry.1292438233=Lip%20Reading%2C%20or%20Visual%20Automatic%20Speech%20Recognition%20%28V-ASR%29%2C%20is%20a%20complex%20task%20requiring%20the%20interpretation%20of%20spoken%20language%20exclusively%20from%20visual%20cues%2C%20primarily%20lip%20movements%20and%20facial%20expressions.%20This%20task%20is%20especially%20challenging%20due%20to%20the%20absence%20of%20auditory%20information%20and%20the%20inherent%20ambiguity%20when%20visually%20distinguishing%20phonemes%20that%20have%20overlapping%20visemes%20where%20different%20phonemes%20appear%20identical%20on%20the%20lips.%20Current%20methods%20typically%20attempt%20to%20predict%20words%20or%20characters%20directly%20from%20these%20visual%20cues%2C%20but%20this%20approach%20frequently%20encounters%20high%20error%20rates%20due%20to%20coarticulation%20effects%20and%20viseme%20ambiguity.%20We%20propose%20a%20novel%20two-stage%2C%20phoneme-centric%20framework%20for%20Visual%20Automatic%20Speech%20Recognition%20%28V-ASR%29%20that%20addresses%20these%20longstanding%20challenges.%20First%2C%20our%20model%20predicts%20a%20compact%20sequence%20of%20phonemes%20from%20visual%20inputs%20using%20a%20Video%20Transformer%20with%20a%20CTC%20head%2C%20thereby%20reducing%20the%20task%20complexity%20and%20achieving%20robust%20speaker%20invariance.%20This%20phoneme%20output%20then%20serves%20as%20the%20input%20to%20a%20fine-tuned%20Large%20Language%20Model%20%28LLM%29%2C%20which%20reconstructs%20coherent%20words%20and%20sentences%20by%20leveraging%20broader%20linguistic%20context.%20Unlike%20existing%20methods%20that%20either%20predict%20words%20directly-often%20faltering%20on%20visually%20similar%20phonemes-or%20rely%20on%20large-scale%20multimodal%20pre-training%2C%20our%20approach%20explicitly%20encodes%20intermediate%20linguistic%20structure%20while%20remaining%20highly%20data%20efficient.%20We%20demonstrate%20state-of-the-art%20performance%20on%20two%20challenging%20datasets%2C%20LRS2%20and%20LRS3%2C%20where%20our%20method%20achieves%20significant%20reductions%20in%20Word%20Error%20Rate%20%28WER%29%20achieving%20a%20SOTA%20WER%20of%2018.7%20on%20LRS3%20despite%20using%2099.4%25%20less%20labelled%20data%20than%20the%20next%20best%20approach.&entry.1838667208=http%3A//arxiv.org/abs/2503.21408v2&entry.124074799=Read"},
{"title": "PhysSFI-Net: Physics-informed Geometric Learning of Skeletal and Facial Interactions for Orthognathic Surgical Outcome Prediction", "author": "Jiahao Bao and Huazhen Liu and Yu Zhuang and Leran Tao and Xinyu Xu and Yongtao Shi and Mengjia Cheng and Yiming Wang and Congshuang Ku and Ting Zeng and Yilang Du and Siyi Chen and Shunyao Shen and Suncheng Xiang and Hongbo Yu", "abstract": "Orthognathic surgery repositions jaw bones to restore occlusion and enhance facial aesthetics. Accurate simulation of postoperative facial morphology is essential for preoperative planning. However, traditional biomechanical models are computationally expensive, while geometric deep learning approaches often lack interpretability. In this study, we develop and validate a physics-informed geometric deep learning framework named PhysSFI-Net for precise prediction of soft tissue deformation following orthognathic surgery. PhysSFI-Net consists of three components: a hierarchical graph module with craniofacial and surgical plan encoders combined with attention mechanisms to extract skeletal-facial interaction features; a Long Short-Term Memory (LSTM)-based sequential predictor for incremental soft tissue deformation; and a biomechanics-inspired module for high-resolution facial surface reconstruction. Model performance was assessed using point cloud shape error (Hausdorff distance), surface deviation error, and landmark localization error (Euclidean distances of craniomaxillofacial landmarks) between predicted facial shapes and corresponding ground truths. A total of 135 patients who underwent combined orthodontic and orthognathic treatment were included for model training and validation. Quantitative analysis demonstrated that PhysSFI-Net achieved a point cloud shape error of 1.070 +/- 0.088 mm, a surface deviation error of 1.296 +/- 0.349 mm, and a landmark localization error of 2.445 +/- 1.326 mm. Comparative experiments indicated that PhysSFI-Net outperformed the state-of-the-art method ACMT-Net in prediction accuracy. In conclusion, PhysSFI-Net enables interpretable, high-resolution prediction of postoperative facial morphology with superior accuracy, showing strong potential for clinical application in orthognathic surgical planning and simulation.", "link": "http://arxiv.org/abs/2601.02088v1", "date": "2026-01-05", "relevancy": 2.6359, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5375}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5344}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhysSFI-Net%3A%20Physics-informed%20Geometric%20Learning%20of%20Skeletal%20and%20Facial%20Interactions%20for%20Orthognathic%20Surgical%20Outcome%20Prediction&body=Title%3A%20PhysSFI-Net%3A%20Physics-informed%20Geometric%20Learning%20of%20Skeletal%20and%20Facial%20Interactions%20for%20Orthognathic%20Surgical%20Outcome%20Prediction%0AAuthor%3A%20Jiahao%20Bao%20and%20Huazhen%20Liu%20and%20Yu%20Zhuang%20and%20Leran%20Tao%20and%20Xinyu%20Xu%20and%20Yongtao%20Shi%20and%20Mengjia%20Cheng%20and%20Yiming%20Wang%20and%20Congshuang%20Ku%20and%20Ting%20Zeng%20and%20Yilang%20Du%20and%20Siyi%20Chen%20and%20Shunyao%20Shen%20and%20Suncheng%20Xiang%20and%20Hongbo%20Yu%0AAbstract%3A%20Orthognathic%20surgery%20repositions%20jaw%20bones%20to%20restore%20occlusion%20and%20enhance%20facial%20aesthetics.%20Accurate%20simulation%20of%20postoperative%20facial%20morphology%20is%20essential%20for%20preoperative%20planning.%20However%2C%20traditional%20biomechanical%20models%20are%20computationally%20expensive%2C%20while%20geometric%20deep%20learning%20approaches%20often%20lack%20interpretability.%20In%20this%20study%2C%20we%20develop%20and%20validate%20a%20physics-informed%20geometric%20deep%20learning%20framework%20named%20PhysSFI-Net%20for%20precise%20prediction%20of%20soft%20tissue%20deformation%20following%20orthognathic%20surgery.%20PhysSFI-Net%20consists%20of%20three%20components%3A%20a%20hierarchical%20graph%20module%20with%20craniofacial%20and%20surgical%20plan%20encoders%20combined%20with%20attention%20mechanisms%20to%20extract%20skeletal-facial%20interaction%20features%3B%20a%20Long%20Short-Term%20Memory%20%28LSTM%29-based%20sequential%20predictor%20for%20incremental%20soft%20tissue%20deformation%3B%20and%20a%20biomechanics-inspired%20module%20for%20high-resolution%20facial%20surface%20reconstruction.%20Model%20performance%20was%20assessed%20using%20point%20cloud%20shape%20error%20%28Hausdorff%20distance%29%2C%20surface%20deviation%20error%2C%20and%20landmark%20localization%20error%20%28Euclidean%20distances%20of%20craniomaxillofacial%20landmarks%29%20between%20predicted%20facial%20shapes%20and%20corresponding%20ground%20truths.%20A%20total%20of%20135%20patients%20who%20underwent%20combined%20orthodontic%20and%20orthognathic%20treatment%20were%20included%20for%20model%20training%20and%20validation.%20Quantitative%20analysis%20demonstrated%20that%20PhysSFI-Net%20achieved%20a%20point%20cloud%20shape%20error%20of%201.070%20%2B/-%200.088%20mm%2C%20a%20surface%20deviation%20error%20of%201.296%20%2B/-%200.349%20mm%2C%20and%20a%20landmark%20localization%20error%20of%202.445%20%2B/-%201.326%20mm.%20Comparative%20experiments%20indicated%20that%20PhysSFI-Net%20outperformed%20the%20state-of-the-art%20method%20ACMT-Net%20in%20prediction%20accuracy.%20In%20conclusion%2C%20PhysSFI-Net%20enables%20interpretable%2C%20high-resolution%20prediction%20of%20postoperative%20facial%20morphology%20with%20superior%20accuracy%2C%20showing%20strong%20potential%20for%20clinical%20application%20in%20orthognathic%20surgical%20planning%20and%20simulation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02088v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysSFI-Net%253A%2520Physics-informed%2520Geometric%2520Learning%2520of%2520Skeletal%2520and%2520Facial%2520Interactions%2520for%2520Orthognathic%2520Surgical%2520Outcome%2520Prediction%26entry.906535625%3DJiahao%2520Bao%2520and%2520Huazhen%2520Liu%2520and%2520Yu%2520Zhuang%2520and%2520Leran%2520Tao%2520and%2520Xinyu%2520Xu%2520and%2520Yongtao%2520Shi%2520and%2520Mengjia%2520Cheng%2520and%2520Yiming%2520Wang%2520and%2520Congshuang%2520Ku%2520and%2520Ting%2520Zeng%2520and%2520Yilang%2520Du%2520and%2520Siyi%2520Chen%2520and%2520Shunyao%2520Shen%2520and%2520Suncheng%2520Xiang%2520and%2520Hongbo%2520Yu%26entry.1292438233%3DOrthognathic%2520surgery%2520repositions%2520jaw%2520bones%2520to%2520restore%2520occlusion%2520and%2520enhance%2520facial%2520aesthetics.%2520Accurate%2520simulation%2520of%2520postoperative%2520facial%2520morphology%2520is%2520essential%2520for%2520preoperative%2520planning.%2520However%252C%2520traditional%2520biomechanical%2520models%2520are%2520computationally%2520expensive%252C%2520while%2520geometric%2520deep%2520learning%2520approaches%2520often%2520lack%2520interpretability.%2520In%2520this%2520study%252C%2520we%2520develop%2520and%2520validate%2520a%2520physics-informed%2520geometric%2520deep%2520learning%2520framework%2520named%2520PhysSFI-Net%2520for%2520precise%2520prediction%2520of%2520soft%2520tissue%2520deformation%2520following%2520orthognathic%2520surgery.%2520PhysSFI-Net%2520consists%2520of%2520three%2520components%253A%2520a%2520hierarchical%2520graph%2520module%2520with%2520craniofacial%2520and%2520surgical%2520plan%2520encoders%2520combined%2520with%2520attention%2520mechanisms%2520to%2520extract%2520skeletal-facial%2520interaction%2520features%253B%2520a%2520Long%2520Short-Term%2520Memory%2520%2528LSTM%2529-based%2520sequential%2520predictor%2520for%2520incremental%2520soft%2520tissue%2520deformation%253B%2520and%2520a%2520biomechanics-inspired%2520module%2520for%2520high-resolution%2520facial%2520surface%2520reconstruction.%2520Model%2520performance%2520was%2520assessed%2520using%2520point%2520cloud%2520shape%2520error%2520%2528Hausdorff%2520distance%2529%252C%2520surface%2520deviation%2520error%252C%2520and%2520landmark%2520localization%2520error%2520%2528Euclidean%2520distances%2520of%2520craniomaxillofacial%2520landmarks%2529%2520between%2520predicted%2520facial%2520shapes%2520and%2520corresponding%2520ground%2520truths.%2520A%2520total%2520of%2520135%2520patients%2520who%2520underwent%2520combined%2520orthodontic%2520and%2520orthognathic%2520treatment%2520were%2520included%2520for%2520model%2520training%2520and%2520validation.%2520Quantitative%2520analysis%2520demonstrated%2520that%2520PhysSFI-Net%2520achieved%2520a%2520point%2520cloud%2520shape%2520error%2520of%25201.070%2520%252B/-%25200.088%2520mm%252C%2520a%2520surface%2520deviation%2520error%2520of%25201.296%2520%252B/-%25200.349%2520mm%252C%2520and%2520a%2520landmark%2520localization%2520error%2520of%25202.445%2520%252B/-%25201.326%2520mm.%2520Comparative%2520experiments%2520indicated%2520that%2520PhysSFI-Net%2520outperformed%2520the%2520state-of-the-art%2520method%2520ACMT-Net%2520in%2520prediction%2520accuracy.%2520In%2520conclusion%252C%2520PhysSFI-Net%2520enables%2520interpretable%252C%2520high-resolution%2520prediction%2520of%2520postoperative%2520facial%2520morphology%2520with%2520superior%2520accuracy%252C%2520showing%2520strong%2520potential%2520for%2520clinical%2520application%2520in%2520orthognathic%2520surgical%2520planning%2520and%2520simulation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02088v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhysSFI-Net%3A%20Physics-informed%20Geometric%20Learning%20of%20Skeletal%20and%20Facial%20Interactions%20for%20Orthognathic%20Surgical%20Outcome%20Prediction&entry.906535625=Jiahao%20Bao%20and%20Huazhen%20Liu%20and%20Yu%20Zhuang%20and%20Leran%20Tao%20and%20Xinyu%20Xu%20and%20Yongtao%20Shi%20and%20Mengjia%20Cheng%20and%20Yiming%20Wang%20and%20Congshuang%20Ku%20and%20Ting%20Zeng%20and%20Yilang%20Du%20and%20Siyi%20Chen%20and%20Shunyao%20Shen%20and%20Suncheng%20Xiang%20and%20Hongbo%20Yu&entry.1292438233=Orthognathic%20surgery%20repositions%20jaw%20bones%20to%20restore%20occlusion%20and%20enhance%20facial%20aesthetics.%20Accurate%20simulation%20of%20postoperative%20facial%20morphology%20is%20essential%20for%20preoperative%20planning.%20However%2C%20traditional%20biomechanical%20models%20are%20computationally%20expensive%2C%20while%20geometric%20deep%20learning%20approaches%20often%20lack%20interpretability.%20In%20this%20study%2C%20we%20develop%20and%20validate%20a%20physics-informed%20geometric%20deep%20learning%20framework%20named%20PhysSFI-Net%20for%20precise%20prediction%20of%20soft%20tissue%20deformation%20following%20orthognathic%20surgery.%20PhysSFI-Net%20consists%20of%20three%20components%3A%20a%20hierarchical%20graph%20module%20with%20craniofacial%20and%20surgical%20plan%20encoders%20combined%20with%20attention%20mechanisms%20to%20extract%20skeletal-facial%20interaction%20features%3B%20a%20Long%20Short-Term%20Memory%20%28LSTM%29-based%20sequential%20predictor%20for%20incremental%20soft%20tissue%20deformation%3B%20and%20a%20biomechanics-inspired%20module%20for%20high-resolution%20facial%20surface%20reconstruction.%20Model%20performance%20was%20assessed%20using%20point%20cloud%20shape%20error%20%28Hausdorff%20distance%29%2C%20surface%20deviation%20error%2C%20and%20landmark%20localization%20error%20%28Euclidean%20distances%20of%20craniomaxillofacial%20landmarks%29%20between%20predicted%20facial%20shapes%20and%20corresponding%20ground%20truths.%20A%20total%20of%20135%20patients%20who%20underwent%20combined%20orthodontic%20and%20orthognathic%20treatment%20were%20included%20for%20model%20training%20and%20validation.%20Quantitative%20analysis%20demonstrated%20that%20PhysSFI-Net%20achieved%20a%20point%20cloud%20shape%20error%20of%201.070%20%2B/-%200.088%20mm%2C%20a%20surface%20deviation%20error%20of%201.296%20%2B/-%200.349%20mm%2C%20and%20a%20landmark%20localization%20error%20of%202.445%20%2B/-%201.326%20mm.%20Comparative%20experiments%20indicated%20that%20PhysSFI-Net%20outperformed%20the%20state-of-the-art%20method%20ACMT-Net%20in%20prediction%20accuracy.%20In%20conclusion%2C%20PhysSFI-Net%20enables%20interpretable%2C%20high-resolution%20prediction%20of%20postoperative%20facial%20morphology%20with%20superior%20accuracy%2C%20showing%20strong%20potential%20for%20clinical%20application%20in%20orthognathic%20surgical%20planning%20and%20simulation.&entry.1838667208=http%3A//arxiv.org/abs/2601.02088v1&entry.124074799=Read"},
{"title": "Matrix Manifold Neural Networks++", "author": "Xuan Son Nguyen and Shuo Yang and Aymeric Histace", "abstract": "Deep neural networks (DNNs) on Riemannian manifolds have garnered increasing interest in various applied areas. For instance, DNNs on spherical and hyperbolic manifolds have been designed to solve a wide range of computer vision and nature language processing tasks. One of the key factors that contribute to the success of these networks is that spherical and hyperbolic manifolds have the rich algebraic structures of gyrogroups and gyrovector spaces. This enables principled and effective generalizations of the most successful DNNs to these manifolds. Recently, some works have shown that many concepts in the theory of gyrogroups and gyrovector spaces can also be generalized to matrix manifolds such as Symmetric Positive Definite (SPD) and Grassmann manifolds. As a result, some building blocks for SPD and Grassmann neural networks, e.g., isometric models and multinomial logistic regression (MLR) can be derived in a way that is fully analogous to their spherical and hyperbolic counterparts. Building upon these works, we design fully-connected (FC) and convolutional layers for SPD neural networks. We also develop MLR on Symmetric Positive Semi-definite (SPSD) manifolds, and propose a method for performing backpropagation with the Grassmann logarithmic map in the projector perspective. We demonstrate the effectiveness of the proposed approach in the human action recognition and node classification tasks.", "link": "http://arxiv.org/abs/2405.19206v2", "date": "2026-01-05", "relevancy": 2.6189, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5657}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.513}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Matrix%20Manifold%20Neural%20Networks%2B%2B&body=Title%3A%20Matrix%20Manifold%20Neural%20Networks%2B%2B%0AAuthor%3A%20Xuan%20Son%20Nguyen%20and%20Shuo%20Yang%20and%20Aymeric%20Histace%0AAbstract%3A%20Deep%20neural%20networks%20%28DNNs%29%20on%20Riemannian%20manifolds%20have%20garnered%20increasing%20interest%20in%20various%20applied%20areas.%20For%20instance%2C%20DNNs%20on%20spherical%20and%20hyperbolic%20manifolds%20have%20been%20designed%20to%20solve%20a%20wide%20range%20of%20computer%20vision%20and%20nature%20language%20processing%20tasks.%20One%20of%20the%20key%20factors%20that%20contribute%20to%20the%20success%20of%20these%20networks%20is%20that%20spherical%20and%20hyperbolic%20manifolds%20have%20the%20rich%20algebraic%20structures%20of%20gyrogroups%20and%20gyrovector%20spaces.%20This%20enables%20principled%20and%20effective%20generalizations%20of%20the%20most%20successful%20DNNs%20to%20these%20manifolds.%20Recently%2C%20some%20works%20have%20shown%20that%20many%20concepts%20in%20the%20theory%20of%20gyrogroups%20and%20gyrovector%20spaces%20can%20also%20be%20generalized%20to%20matrix%20manifolds%20such%20as%20Symmetric%20Positive%20Definite%20%28SPD%29%20and%20Grassmann%20manifolds.%20As%20a%20result%2C%20some%20building%20blocks%20for%20SPD%20and%20Grassmann%20neural%20networks%2C%20e.g.%2C%20isometric%20models%20and%20multinomial%20logistic%20regression%20%28MLR%29%20can%20be%20derived%20in%20a%20way%20that%20is%20fully%20analogous%20to%20their%20spherical%20and%20hyperbolic%20counterparts.%20Building%20upon%20these%20works%2C%20we%20design%20fully-connected%20%28FC%29%20and%20convolutional%20layers%20for%20SPD%20neural%20networks.%20We%20also%20develop%20MLR%20on%20Symmetric%20Positive%20Semi-definite%20%28SPSD%29%20manifolds%2C%20and%20propose%20a%20method%20for%20performing%20backpropagation%20with%20the%20Grassmann%20logarithmic%20map%20in%20the%20projector%20perspective.%20We%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20approach%20in%20the%20human%20action%20recognition%20and%20node%20classification%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2405.19206v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMatrix%2520Manifold%2520Neural%2520Networks%252B%252B%26entry.906535625%3DXuan%2520Son%2520Nguyen%2520and%2520Shuo%2520Yang%2520and%2520Aymeric%2520Histace%26entry.1292438233%3DDeep%2520neural%2520networks%2520%2528DNNs%2529%2520on%2520Riemannian%2520manifolds%2520have%2520garnered%2520increasing%2520interest%2520in%2520various%2520applied%2520areas.%2520For%2520instance%252C%2520DNNs%2520on%2520spherical%2520and%2520hyperbolic%2520manifolds%2520have%2520been%2520designed%2520to%2520solve%2520a%2520wide%2520range%2520of%2520computer%2520vision%2520and%2520nature%2520language%2520processing%2520tasks.%2520One%2520of%2520the%2520key%2520factors%2520that%2520contribute%2520to%2520the%2520success%2520of%2520these%2520networks%2520is%2520that%2520spherical%2520and%2520hyperbolic%2520manifolds%2520have%2520the%2520rich%2520algebraic%2520structures%2520of%2520gyrogroups%2520and%2520gyrovector%2520spaces.%2520This%2520enables%2520principled%2520and%2520effective%2520generalizations%2520of%2520the%2520most%2520successful%2520DNNs%2520to%2520these%2520manifolds.%2520Recently%252C%2520some%2520works%2520have%2520shown%2520that%2520many%2520concepts%2520in%2520the%2520theory%2520of%2520gyrogroups%2520and%2520gyrovector%2520spaces%2520can%2520also%2520be%2520generalized%2520to%2520matrix%2520manifolds%2520such%2520as%2520Symmetric%2520Positive%2520Definite%2520%2528SPD%2529%2520and%2520Grassmann%2520manifolds.%2520As%2520a%2520result%252C%2520some%2520building%2520blocks%2520for%2520SPD%2520and%2520Grassmann%2520neural%2520networks%252C%2520e.g.%252C%2520isometric%2520models%2520and%2520multinomial%2520logistic%2520regression%2520%2528MLR%2529%2520can%2520be%2520derived%2520in%2520a%2520way%2520that%2520is%2520fully%2520analogous%2520to%2520their%2520spherical%2520and%2520hyperbolic%2520counterparts.%2520Building%2520upon%2520these%2520works%252C%2520we%2520design%2520fully-connected%2520%2528FC%2529%2520and%2520convolutional%2520layers%2520for%2520SPD%2520neural%2520networks.%2520We%2520also%2520develop%2520MLR%2520on%2520Symmetric%2520Positive%2520Semi-definite%2520%2528SPSD%2529%2520manifolds%252C%2520and%2520propose%2520a%2520method%2520for%2520performing%2520backpropagation%2520with%2520the%2520Grassmann%2520logarithmic%2520map%2520in%2520the%2520projector%2520perspective.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520approach%2520in%2520the%2520human%2520action%2520recognition%2520and%2520node%2520classification%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19206v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Matrix%20Manifold%20Neural%20Networks%2B%2B&entry.906535625=Xuan%20Son%20Nguyen%20and%20Shuo%20Yang%20and%20Aymeric%20Histace&entry.1292438233=Deep%20neural%20networks%20%28DNNs%29%20on%20Riemannian%20manifolds%20have%20garnered%20increasing%20interest%20in%20various%20applied%20areas.%20For%20instance%2C%20DNNs%20on%20spherical%20and%20hyperbolic%20manifolds%20have%20been%20designed%20to%20solve%20a%20wide%20range%20of%20computer%20vision%20and%20nature%20language%20processing%20tasks.%20One%20of%20the%20key%20factors%20that%20contribute%20to%20the%20success%20of%20these%20networks%20is%20that%20spherical%20and%20hyperbolic%20manifolds%20have%20the%20rich%20algebraic%20structures%20of%20gyrogroups%20and%20gyrovector%20spaces.%20This%20enables%20principled%20and%20effective%20generalizations%20of%20the%20most%20successful%20DNNs%20to%20these%20manifolds.%20Recently%2C%20some%20works%20have%20shown%20that%20many%20concepts%20in%20the%20theory%20of%20gyrogroups%20and%20gyrovector%20spaces%20can%20also%20be%20generalized%20to%20matrix%20manifolds%20such%20as%20Symmetric%20Positive%20Definite%20%28SPD%29%20and%20Grassmann%20manifolds.%20As%20a%20result%2C%20some%20building%20blocks%20for%20SPD%20and%20Grassmann%20neural%20networks%2C%20e.g.%2C%20isometric%20models%20and%20multinomial%20logistic%20regression%20%28MLR%29%20can%20be%20derived%20in%20a%20way%20that%20is%20fully%20analogous%20to%20their%20spherical%20and%20hyperbolic%20counterparts.%20Building%20upon%20these%20works%2C%20we%20design%20fully-connected%20%28FC%29%20and%20convolutional%20layers%20for%20SPD%20neural%20networks.%20We%20also%20develop%20MLR%20on%20Symmetric%20Positive%20Semi-definite%20%28SPSD%29%20manifolds%2C%20and%20propose%20a%20method%20for%20performing%20backpropagation%20with%20the%20Grassmann%20logarithmic%20map%20in%20the%20projector%20perspective.%20We%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20approach%20in%20the%20human%20action%20recognition%20and%20node%20classification%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2405.19206v2&entry.124074799=Read"},
{"title": "QuIC: A Quantum-Inspired Interaction Classifier for Revitalizing Shallow CNNs in Fine-Grained Recognition", "author": "Cheng Ying Wu and Yen Jui Chang", "abstract": "Deploying deep learning models for Fine-Grained Visual Classification (FGVC) on resource-constrained edge devices remains a significant challenge. While deep architectures achieve high accuracy on benchmarks like CUB-200-2011, their computational cost is often prohibitive. Conversely, shallow networks (e.g., AlexNet, VGG) offer efficiency but fail to distinguish visually similar sub-categories. This is because standard Global Average Pooling (GAP) heads capture only first-order statistics, missing the subtle high-order feature interactions required for FGVC. While Bilinear CNNs address this, they suffer from high feature dimensionality and instability during training. To bridge this gap, we propose the Quantum-inspired Interaction Classifier (QuIC). Drawing inspiration from quantum mechanics, QuIC models feature channels as interacting quantum states and captures second-order feature covariance via a learnable observable operator. Designed as a lightweight, plug-and-play module, QuIC supports stable, single-stage end-to-end training without exploding feature dimensions. Experimental results demonstrate that QuIC significantly revitalizes shallow backbones: it boosts the Top-1 accuracy of VGG16 by nearly 20% and outperforms state-of-the-art attention mechanisms (SE-Block) on ResNet18. Qualitative analysis, including t-SNE visualization, further confirms that QuIC resolves ambiguous cases by explicitly attending to fine-grained discriminative features and enforcing compact intra-class clustering.", "link": "http://arxiv.org/abs/2601.02189v1", "date": "2026-01-05", "relevancy": 2.6065, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5383}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.515}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QuIC%3A%20A%20Quantum-Inspired%20Interaction%20Classifier%20for%20Revitalizing%20Shallow%20CNNs%20in%20Fine-Grained%20Recognition&body=Title%3A%20QuIC%3A%20A%20Quantum-Inspired%20Interaction%20Classifier%20for%20Revitalizing%20Shallow%20CNNs%20in%20Fine-Grained%20Recognition%0AAuthor%3A%20Cheng%20Ying%20Wu%20and%20Yen%20Jui%20Chang%0AAbstract%3A%20Deploying%20deep%20learning%20models%20for%20Fine-Grained%20Visual%20Classification%20%28FGVC%29%20on%20resource-constrained%20edge%20devices%20remains%20a%20significant%20challenge.%20While%20deep%20architectures%20achieve%20high%20accuracy%20on%20benchmarks%20like%20CUB-200-2011%2C%20their%20computational%20cost%20is%20often%20prohibitive.%20Conversely%2C%20shallow%20networks%20%28e.g.%2C%20AlexNet%2C%20VGG%29%20offer%20efficiency%20but%20fail%20to%20distinguish%20visually%20similar%20sub-categories.%20This%20is%20because%20standard%20Global%20Average%20Pooling%20%28GAP%29%20heads%20capture%20only%20first-order%20statistics%2C%20missing%20the%20subtle%20high-order%20feature%20interactions%20required%20for%20FGVC.%20While%20Bilinear%20CNNs%20address%20this%2C%20they%20suffer%20from%20high%20feature%20dimensionality%20and%20instability%20during%20training.%20To%20bridge%20this%20gap%2C%20we%20propose%20the%20Quantum-inspired%20Interaction%20Classifier%20%28QuIC%29.%20Drawing%20inspiration%20from%20quantum%20mechanics%2C%20QuIC%20models%20feature%20channels%20as%20interacting%20quantum%20states%20and%20captures%20second-order%20feature%20covariance%20via%20a%20learnable%20observable%20operator.%20Designed%20as%20a%20lightweight%2C%20plug-and-play%20module%2C%20QuIC%20supports%20stable%2C%20single-stage%20end-to-end%20training%20without%20exploding%20feature%20dimensions.%20Experimental%20results%20demonstrate%20that%20QuIC%20significantly%20revitalizes%20shallow%20backbones%3A%20it%20boosts%20the%20Top-1%20accuracy%20of%20VGG16%20by%20nearly%2020%25%20and%20outperforms%20state-of-the-art%20attention%20mechanisms%20%28SE-Block%29%20on%20ResNet18.%20Qualitative%20analysis%2C%20including%20t-SNE%20visualization%2C%20further%20confirms%20that%20QuIC%20resolves%20ambiguous%20cases%20by%20explicitly%20attending%20to%20fine-grained%20discriminative%20features%20and%20enforcing%20compact%20intra-class%20clustering.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02189v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuIC%253A%2520A%2520Quantum-Inspired%2520Interaction%2520Classifier%2520for%2520Revitalizing%2520Shallow%2520CNNs%2520in%2520Fine-Grained%2520Recognition%26entry.906535625%3DCheng%2520Ying%2520Wu%2520and%2520Yen%2520Jui%2520Chang%26entry.1292438233%3DDeploying%2520deep%2520learning%2520models%2520for%2520Fine-Grained%2520Visual%2520Classification%2520%2528FGVC%2529%2520on%2520resource-constrained%2520edge%2520devices%2520remains%2520a%2520significant%2520challenge.%2520While%2520deep%2520architectures%2520achieve%2520high%2520accuracy%2520on%2520benchmarks%2520like%2520CUB-200-2011%252C%2520their%2520computational%2520cost%2520is%2520often%2520prohibitive.%2520Conversely%252C%2520shallow%2520networks%2520%2528e.g.%252C%2520AlexNet%252C%2520VGG%2529%2520offer%2520efficiency%2520but%2520fail%2520to%2520distinguish%2520visually%2520similar%2520sub-categories.%2520This%2520is%2520because%2520standard%2520Global%2520Average%2520Pooling%2520%2528GAP%2529%2520heads%2520capture%2520only%2520first-order%2520statistics%252C%2520missing%2520the%2520subtle%2520high-order%2520feature%2520interactions%2520required%2520for%2520FGVC.%2520While%2520Bilinear%2520CNNs%2520address%2520this%252C%2520they%2520suffer%2520from%2520high%2520feature%2520dimensionality%2520and%2520instability%2520during%2520training.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520the%2520Quantum-inspired%2520Interaction%2520Classifier%2520%2528QuIC%2529.%2520Drawing%2520inspiration%2520from%2520quantum%2520mechanics%252C%2520QuIC%2520models%2520feature%2520channels%2520as%2520interacting%2520quantum%2520states%2520and%2520captures%2520second-order%2520feature%2520covariance%2520via%2520a%2520learnable%2520observable%2520operator.%2520Designed%2520as%2520a%2520lightweight%252C%2520plug-and-play%2520module%252C%2520QuIC%2520supports%2520stable%252C%2520single-stage%2520end-to-end%2520training%2520without%2520exploding%2520feature%2520dimensions.%2520Experimental%2520results%2520demonstrate%2520that%2520QuIC%2520significantly%2520revitalizes%2520shallow%2520backbones%253A%2520it%2520boosts%2520the%2520Top-1%2520accuracy%2520of%2520VGG16%2520by%2520nearly%252020%2525%2520and%2520outperforms%2520state-of-the-art%2520attention%2520mechanisms%2520%2528SE-Block%2529%2520on%2520ResNet18.%2520Qualitative%2520analysis%252C%2520including%2520t-SNE%2520visualization%252C%2520further%2520confirms%2520that%2520QuIC%2520resolves%2520ambiguous%2520cases%2520by%2520explicitly%2520attending%2520to%2520fine-grained%2520discriminative%2520features%2520and%2520enforcing%2520compact%2520intra-class%2520clustering.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02189v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QuIC%3A%20A%20Quantum-Inspired%20Interaction%20Classifier%20for%20Revitalizing%20Shallow%20CNNs%20in%20Fine-Grained%20Recognition&entry.906535625=Cheng%20Ying%20Wu%20and%20Yen%20Jui%20Chang&entry.1292438233=Deploying%20deep%20learning%20models%20for%20Fine-Grained%20Visual%20Classification%20%28FGVC%29%20on%20resource-constrained%20edge%20devices%20remains%20a%20significant%20challenge.%20While%20deep%20architectures%20achieve%20high%20accuracy%20on%20benchmarks%20like%20CUB-200-2011%2C%20their%20computational%20cost%20is%20often%20prohibitive.%20Conversely%2C%20shallow%20networks%20%28e.g.%2C%20AlexNet%2C%20VGG%29%20offer%20efficiency%20but%20fail%20to%20distinguish%20visually%20similar%20sub-categories.%20This%20is%20because%20standard%20Global%20Average%20Pooling%20%28GAP%29%20heads%20capture%20only%20first-order%20statistics%2C%20missing%20the%20subtle%20high-order%20feature%20interactions%20required%20for%20FGVC.%20While%20Bilinear%20CNNs%20address%20this%2C%20they%20suffer%20from%20high%20feature%20dimensionality%20and%20instability%20during%20training.%20To%20bridge%20this%20gap%2C%20we%20propose%20the%20Quantum-inspired%20Interaction%20Classifier%20%28QuIC%29.%20Drawing%20inspiration%20from%20quantum%20mechanics%2C%20QuIC%20models%20feature%20channels%20as%20interacting%20quantum%20states%20and%20captures%20second-order%20feature%20covariance%20via%20a%20learnable%20observable%20operator.%20Designed%20as%20a%20lightweight%2C%20plug-and-play%20module%2C%20QuIC%20supports%20stable%2C%20single-stage%20end-to-end%20training%20without%20exploding%20feature%20dimensions.%20Experimental%20results%20demonstrate%20that%20QuIC%20significantly%20revitalizes%20shallow%20backbones%3A%20it%20boosts%20the%20Top-1%20accuracy%20of%20VGG16%20by%20nearly%2020%25%20and%20outperforms%20state-of-the-art%20attention%20mechanisms%20%28SE-Block%29%20on%20ResNet18.%20Qualitative%20analysis%2C%20including%20t-SNE%20visualization%2C%20further%20confirms%20that%20QuIC%20resolves%20ambiguous%20cases%20by%20explicitly%20attending%20to%20fine-grained%20discriminative%20features%20and%20enforcing%20compact%20intra-class%20clustering.&entry.1838667208=http%3A//arxiv.org/abs/2601.02189v1&entry.124074799=Read"},
{"title": "Parameter-Efficient Domain Adaption for CSI Crowd-Counting via Self-Supervised Learning with Adapter Modules", "author": "Oliver Custance and Saad Khan and Simon Parkinson and Quan Z. Sheng", "abstract": "Device-free crowd-counting using WiFi Channel State Information (CSI) is a key enabling technology for a new generation of privacy-preserving Internet of Things (IoT) applications. However, practical deployment is severely hampered by the domain shift problem, where models trained in one environment fail to generalise to another. To overcome this, we propose a novel two-stage framework centred on a CSI-ResNet-A architecture. This model is pre-trained via self-supervised contrastive learning to learn domain-invariant representations and leverages lightweight Adapter modules for highly efficient fine-tuning. The resulting event sequence is then processed by a stateful counting machine to produce a final, stable occupancy estimate. We validate our framework extensively. On our WiFlow dataset, our unsupervised approach excels in a 10-shot learning scenario, achieving a final Mean Absolute Error (MAE) of just 0.44--a task where supervised baselines fail. To formally quantify robustness, we introduce the Generalisation Index (GI), on which our model scores near-perfectly, confirming its ability to generalise. Furthermore, our framework sets a new state-of-the-art public WiAR benchmark with 98.8\\% accuracy. Our ablation studies reveal the core strength of our design: adapter-based fine-tuning achieves performance within 1\\% of a full fine-tune (98.84\\% vs. 99.67\\%) while training 97.2\\% fewer parameters. Our work provides a practical and scalable solution for developing robust sensing systems ready for real-world IoT deployments.", "link": "http://arxiv.org/abs/2601.02203v1", "date": "2026-01-05", "relevancy": 2.5832, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5182}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5181}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5136}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parameter-Efficient%20Domain%20Adaption%20for%20CSI%20Crowd-Counting%20via%20Self-Supervised%20Learning%20with%20Adapter%20Modules&body=Title%3A%20Parameter-Efficient%20Domain%20Adaption%20for%20CSI%20Crowd-Counting%20via%20Self-Supervised%20Learning%20with%20Adapter%20Modules%0AAuthor%3A%20Oliver%20Custance%20and%20Saad%20Khan%20and%20Simon%20Parkinson%20and%20Quan%20Z.%20Sheng%0AAbstract%3A%20Device-free%20crowd-counting%20using%20WiFi%20Channel%20State%20Information%20%28CSI%29%20is%20a%20key%20enabling%20technology%20for%20a%20new%20generation%20of%20privacy-preserving%20Internet%20of%20Things%20%28IoT%29%20applications.%20However%2C%20practical%20deployment%20is%20severely%20hampered%20by%20the%20domain%20shift%20problem%2C%20where%20models%20trained%20in%20one%20environment%20fail%20to%20generalise%20to%20another.%20To%20overcome%20this%2C%20we%20propose%20a%20novel%20two-stage%20framework%20centred%20on%20a%20CSI-ResNet-A%20architecture.%20This%20model%20is%20pre-trained%20via%20self-supervised%20contrastive%20learning%20to%20learn%20domain-invariant%20representations%20and%20leverages%20lightweight%20Adapter%20modules%20for%20highly%20efficient%20fine-tuning.%20The%20resulting%20event%20sequence%20is%20then%20processed%20by%20a%20stateful%20counting%20machine%20to%20produce%20a%20final%2C%20stable%20occupancy%20estimate.%20We%20validate%20our%20framework%20extensively.%20On%20our%20WiFlow%20dataset%2C%20our%20unsupervised%20approach%20excels%20in%20a%2010-shot%20learning%20scenario%2C%20achieving%20a%20final%20Mean%20Absolute%20Error%20%28MAE%29%20of%20just%200.44--a%20task%20where%20supervised%20baselines%20fail.%20To%20formally%20quantify%20robustness%2C%20we%20introduce%20the%20Generalisation%20Index%20%28GI%29%2C%20on%20which%20our%20model%20scores%20near-perfectly%2C%20confirming%20its%20ability%20to%20generalise.%20Furthermore%2C%20our%20framework%20sets%20a%20new%20state-of-the-art%20public%20WiAR%20benchmark%20with%2098.8%5C%25%20accuracy.%20Our%20ablation%20studies%20reveal%20the%20core%20strength%20of%20our%20design%3A%20adapter-based%20fine-tuning%20achieves%20performance%20within%201%5C%25%20of%20a%20full%20fine-tune%20%2898.84%5C%25%20vs.%2099.67%5C%25%29%20while%20training%2097.2%5C%25%20fewer%20parameters.%20Our%20work%20provides%20a%20practical%20and%20scalable%20solution%20for%20developing%20robust%20sensing%20systems%20ready%20for%20real-world%20IoT%20deployments.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02203v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParameter-Efficient%2520Domain%2520Adaption%2520for%2520CSI%2520Crowd-Counting%2520via%2520Self-Supervised%2520Learning%2520with%2520Adapter%2520Modules%26entry.906535625%3DOliver%2520Custance%2520and%2520Saad%2520Khan%2520and%2520Simon%2520Parkinson%2520and%2520Quan%2520Z.%2520Sheng%26entry.1292438233%3DDevice-free%2520crowd-counting%2520using%2520WiFi%2520Channel%2520State%2520Information%2520%2528CSI%2529%2520is%2520a%2520key%2520enabling%2520technology%2520for%2520a%2520new%2520generation%2520of%2520privacy-preserving%2520Internet%2520of%2520Things%2520%2528IoT%2529%2520applications.%2520However%252C%2520practical%2520deployment%2520is%2520severely%2520hampered%2520by%2520the%2520domain%2520shift%2520problem%252C%2520where%2520models%2520trained%2520in%2520one%2520environment%2520fail%2520to%2520generalise%2520to%2520another.%2520To%2520overcome%2520this%252C%2520we%2520propose%2520a%2520novel%2520two-stage%2520framework%2520centred%2520on%2520a%2520CSI-ResNet-A%2520architecture.%2520This%2520model%2520is%2520pre-trained%2520via%2520self-supervised%2520contrastive%2520learning%2520to%2520learn%2520domain-invariant%2520representations%2520and%2520leverages%2520lightweight%2520Adapter%2520modules%2520for%2520highly%2520efficient%2520fine-tuning.%2520The%2520resulting%2520event%2520sequence%2520is%2520then%2520processed%2520by%2520a%2520stateful%2520counting%2520machine%2520to%2520produce%2520a%2520final%252C%2520stable%2520occupancy%2520estimate.%2520We%2520validate%2520our%2520framework%2520extensively.%2520On%2520our%2520WiFlow%2520dataset%252C%2520our%2520unsupervised%2520approach%2520excels%2520in%2520a%252010-shot%2520learning%2520scenario%252C%2520achieving%2520a%2520final%2520Mean%2520Absolute%2520Error%2520%2528MAE%2529%2520of%2520just%25200.44--a%2520task%2520where%2520supervised%2520baselines%2520fail.%2520To%2520formally%2520quantify%2520robustness%252C%2520we%2520introduce%2520the%2520Generalisation%2520Index%2520%2528GI%2529%252C%2520on%2520which%2520our%2520model%2520scores%2520near-perfectly%252C%2520confirming%2520its%2520ability%2520to%2520generalise.%2520Furthermore%252C%2520our%2520framework%2520sets%2520a%2520new%2520state-of-the-art%2520public%2520WiAR%2520benchmark%2520with%252098.8%255C%2525%2520accuracy.%2520Our%2520ablation%2520studies%2520reveal%2520the%2520core%2520strength%2520of%2520our%2520design%253A%2520adapter-based%2520fine-tuning%2520achieves%2520performance%2520within%25201%255C%2525%2520of%2520a%2520full%2520fine-tune%2520%252898.84%255C%2525%2520vs.%252099.67%255C%2525%2529%2520while%2520training%252097.2%255C%2525%2520fewer%2520parameters.%2520Our%2520work%2520provides%2520a%2520practical%2520and%2520scalable%2520solution%2520for%2520developing%2520robust%2520sensing%2520systems%2520ready%2520for%2520real-world%2520IoT%2520deployments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02203v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parameter-Efficient%20Domain%20Adaption%20for%20CSI%20Crowd-Counting%20via%20Self-Supervised%20Learning%20with%20Adapter%20Modules&entry.906535625=Oliver%20Custance%20and%20Saad%20Khan%20and%20Simon%20Parkinson%20and%20Quan%20Z.%20Sheng&entry.1292438233=Device-free%20crowd-counting%20using%20WiFi%20Channel%20State%20Information%20%28CSI%29%20is%20a%20key%20enabling%20technology%20for%20a%20new%20generation%20of%20privacy-preserving%20Internet%20of%20Things%20%28IoT%29%20applications.%20However%2C%20practical%20deployment%20is%20severely%20hampered%20by%20the%20domain%20shift%20problem%2C%20where%20models%20trained%20in%20one%20environment%20fail%20to%20generalise%20to%20another.%20To%20overcome%20this%2C%20we%20propose%20a%20novel%20two-stage%20framework%20centred%20on%20a%20CSI-ResNet-A%20architecture.%20This%20model%20is%20pre-trained%20via%20self-supervised%20contrastive%20learning%20to%20learn%20domain-invariant%20representations%20and%20leverages%20lightweight%20Adapter%20modules%20for%20highly%20efficient%20fine-tuning.%20The%20resulting%20event%20sequence%20is%20then%20processed%20by%20a%20stateful%20counting%20machine%20to%20produce%20a%20final%2C%20stable%20occupancy%20estimate.%20We%20validate%20our%20framework%20extensively.%20On%20our%20WiFlow%20dataset%2C%20our%20unsupervised%20approach%20excels%20in%20a%2010-shot%20learning%20scenario%2C%20achieving%20a%20final%20Mean%20Absolute%20Error%20%28MAE%29%20of%20just%200.44--a%20task%20where%20supervised%20baselines%20fail.%20To%20formally%20quantify%20robustness%2C%20we%20introduce%20the%20Generalisation%20Index%20%28GI%29%2C%20on%20which%20our%20model%20scores%20near-perfectly%2C%20confirming%20its%20ability%20to%20generalise.%20Furthermore%2C%20our%20framework%20sets%20a%20new%20state-of-the-art%20public%20WiAR%20benchmark%20with%2098.8%5C%25%20accuracy.%20Our%20ablation%20studies%20reveal%20the%20core%20strength%20of%20our%20design%3A%20adapter-based%20fine-tuning%20achieves%20performance%20within%201%5C%25%20of%20a%20full%20fine-tune%20%2898.84%5C%25%20vs.%2099.67%5C%25%29%20while%20training%2097.2%5C%25%20fewer%20parameters.%20Our%20work%20provides%20a%20practical%20and%20scalable%20solution%20for%20developing%20robust%20sensing%20systems%20ready%20for%20real-world%20IoT%20deployments.&entry.1838667208=http%3A//arxiv.org/abs/2601.02203v1&entry.124074799=Read"},
{"title": "MotionAdapter: Video Motion Transfer via Content-Aware Attention Customization", "author": "Zhexin Zhang and Yifeng Zhu and Yangyang Xu and Long Chen and Yong Du and Shengfeng He and Jun Yu", "abstract": "Recent advances in diffusion-based text-to-video models, particularly those built on the diffusion transformer architecture, have achieved remarkable progress in generating high-quality and temporally coherent videos. However, transferring complex motions between videos remains challenging. In this work, we present MotionAdapter, a content-aware motion transfer framework that enables robust and semantically aligned motion transfer within DiT-based T2V models. Our key insight is that effective motion transfer requires \\romannumeral1) explicit disentanglement of motion from appearance and \\romannumeral 2) adaptive customization of motion to target content. MotionAdapter first isolates motion by analyzing cross-frame attention within 3D full-attention modules to extract attention-derived motion fields. To bridge the semantic gap between reference and target videos, we further introduce a DINO-guided motion customization module that rearranges and refines motion fields based on content correspondences. The customized motion field is then used to guide the DiT denoising process, ensuring that the synthesized video inherits the reference motion while preserving target appearance and semantics. Extensive experiments demonstrate that MotionAdapter outperforms state-of-the-art methods in both qualitative and quantitative evaluations. Moreover, MotionAdapter naturally supports complex motion transfer and motion editing tasks such as zooming.", "link": "http://arxiv.org/abs/2601.01955v1", "date": "2026-01-05", "relevancy": 2.572, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7582}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6244}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MotionAdapter%3A%20Video%20Motion%20Transfer%20via%20Content-Aware%20Attention%20Customization&body=Title%3A%20MotionAdapter%3A%20Video%20Motion%20Transfer%20via%20Content-Aware%20Attention%20Customization%0AAuthor%3A%20Zhexin%20Zhang%20and%20Yifeng%20Zhu%20and%20Yangyang%20Xu%20and%20Long%20Chen%20and%20Yong%20Du%20and%20Shengfeng%20He%20and%20Jun%20Yu%0AAbstract%3A%20Recent%20advances%20in%20diffusion-based%20text-to-video%20models%2C%20particularly%20those%20built%20on%20the%20diffusion%20transformer%20architecture%2C%20have%20achieved%20remarkable%20progress%20in%20generating%20high-quality%20and%20temporally%20coherent%20videos.%20However%2C%20transferring%20complex%20motions%20between%20videos%20remains%20challenging.%20In%20this%20work%2C%20we%20present%20MotionAdapter%2C%20a%20content-aware%20motion%20transfer%20framework%20that%20enables%20robust%20and%20semantically%20aligned%20motion%20transfer%20within%20DiT-based%20T2V%20models.%20Our%20key%20insight%20is%20that%20effective%20motion%20transfer%20requires%20%5Cromannumeral1%29%20explicit%20disentanglement%20of%20motion%20from%20appearance%20and%20%5Cromannumeral%202%29%20adaptive%20customization%20of%20motion%20to%20target%20content.%20MotionAdapter%20first%20isolates%20motion%20by%20analyzing%20cross-frame%20attention%20within%203D%20full-attention%20modules%20to%20extract%20attention-derived%20motion%20fields.%20To%20bridge%20the%20semantic%20gap%20between%20reference%20and%20target%20videos%2C%20we%20further%20introduce%20a%20DINO-guided%20motion%20customization%20module%20that%20rearranges%20and%20refines%20motion%20fields%20based%20on%20content%20correspondences.%20The%20customized%20motion%20field%20is%20then%20used%20to%20guide%20the%20DiT%20denoising%20process%2C%20ensuring%20that%20the%20synthesized%20video%20inherits%20the%20reference%20motion%20while%20preserving%20target%20appearance%20and%20semantics.%20Extensive%20experiments%20demonstrate%20that%20MotionAdapter%20outperforms%20state-of-the-art%20methods%20in%20both%20qualitative%20and%20quantitative%20evaluations.%20Moreover%2C%20MotionAdapter%20naturally%20supports%20complex%20motion%20transfer%20and%20motion%20editing%20tasks%20such%20as%20zooming.%0ALink%3A%20http%3A//arxiv.org/abs/2601.01955v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotionAdapter%253A%2520Video%2520Motion%2520Transfer%2520via%2520Content-Aware%2520Attention%2520Customization%26entry.906535625%3DZhexin%2520Zhang%2520and%2520Yifeng%2520Zhu%2520and%2520Yangyang%2520Xu%2520and%2520Long%2520Chen%2520and%2520Yong%2520Du%2520and%2520Shengfeng%2520He%2520and%2520Jun%2520Yu%26entry.1292438233%3DRecent%2520advances%2520in%2520diffusion-based%2520text-to-video%2520models%252C%2520particularly%2520those%2520built%2520on%2520the%2520diffusion%2520transformer%2520architecture%252C%2520have%2520achieved%2520remarkable%2520progress%2520in%2520generating%2520high-quality%2520and%2520temporally%2520coherent%2520videos.%2520However%252C%2520transferring%2520complex%2520motions%2520between%2520videos%2520remains%2520challenging.%2520In%2520this%2520work%252C%2520we%2520present%2520MotionAdapter%252C%2520a%2520content-aware%2520motion%2520transfer%2520framework%2520that%2520enables%2520robust%2520and%2520semantically%2520aligned%2520motion%2520transfer%2520within%2520DiT-based%2520T2V%2520models.%2520Our%2520key%2520insight%2520is%2520that%2520effective%2520motion%2520transfer%2520requires%2520%255Cromannumeral1%2529%2520explicit%2520disentanglement%2520of%2520motion%2520from%2520appearance%2520and%2520%255Cromannumeral%25202%2529%2520adaptive%2520customization%2520of%2520motion%2520to%2520target%2520content.%2520MotionAdapter%2520first%2520isolates%2520motion%2520by%2520analyzing%2520cross-frame%2520attention%2520within%25203D%2520full-attention%2520modules%2520to%2520extract%2520attention-derived%2520motion%2520fields.%2520To%2520bridge%2520the%2520semantic%2520gap%2520between%2520reference%2520and%2520target%2520videos%252C%2520we%2520further%2520introduce%2520a%2520DINO-guided%2520motion%2520customization%2520module%2520that%2520rearranges%2520and%2520refines%2520motion%2520fields%2520based%2520on%2520content%2520correspondences.%2520The%2520customized%2520motion%2520field%2520is%2520then%2520used%2520to%2520guide%2520the%2520DiT%2520denoising%2520process%252C%2520ensuring%2520that%2520the%2520synthesized%2520video%2520inherits%2520the%2520reference%2520motion%2520while%2520preserving%2520target%2520appearance%2520and%2520semantics.%2520Extensive%2520experiments%2520demonstrate%2520that%2520MotionAdapter%2520outperforms%2520state-of-the-art%2520methods%2520in%2520both%2520qualitative%2520and%2520quantitative%2520evaluations.%2520Moreover%252C%2520MotionAdapter%2520naturally%2520supports%2520complex%2520motion%2520transfer%2520and%2520motion%2520editing%2520tasks%2520such%2520as%2520zooming.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.01955v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MotionAdapter%3A%20Video%20Motion%20Transfer%20via%20Content-Aware%20Attention%20Customization&entry.906535625=Zhexin%20Zhang%20and%20Yifeng%20Zhu%20and%20Yangyang%20Xu%20and%20Long%20Chen%20and%20Yong%20Du%20and%20Shengfeng%20He%20and%20Jun%20Yu&entry.1292438233=Recent%20advances%20in%20diffusion-based%20text-to-video%20models%2C%20particularly%20those%20built%20on%20the%20diffusion%20transformer%20architecture%2C%20have%20achieved%20remarkable%20progress%20in%20generating%20high-quality%20and%20temporally%20coherent%20videos.%20However%2C%20transferring%20complex%20motions%20between%20videos%20remains%20challenging.%20In%20this%20work%2C%20we%20present%20MotionAdapter%2C%20a%20content-aware%20motion%20transfer%20framework%20that%20enables%20robust%20and%20semantically%20aligned%20motion%20transfer%20within%20DiT-based%20T2V%20models.%20Our%20key%20insight%20is%20that%20effective%20motion%20transfer%20requires%20%5Cromannumeral1%29%20explicit%20disentanglement%20of%20motion%20from%20appearance%20and%20%5Cromannumeral%202%29%20adaptive%20customization%20of%20motion%20to%20target%20content.%20MotionAdapter%20first%20isolates%20motion%20by%20analyzing%20cross-frame%20attention%20within%203D%20full-attention%20modules%20to%20extract%20attention-derived%20motion%20fields.%20To%20bridge%20the%20semantic%20gap%20between%20reference%20and%20target%20videos%2C%20we%20further%20introduce%20a%20DINO-guided%20motion%20customization%20module%20that%20rearranges%20and%20refines%20motion%20fields%20based%20on%20content%20correspondences.%20The%20customized%20motion%20field%20is%20then%20used%20to%20guide%20the%20DiT%20denoising%20process%2C%20ensuring%20that%20the%20synthesized%20video%20inherits%20the%20reference%20motion%20while%20preserving%20target%20appearance%20and%20semantics.%20Extensive%20experiments%20demonstrate%20that%20MotionAdapter%20outperforms%20state-of-the-art%20methods%20in%20both%20qualitative%20and%20quantitative%20evaluations.%20Moreover%2C%20MotionAdapter%20naturally%20supports%20complex%20motion%20transfer%20and%20motion%20editing%20tasks%20such%20as%20zooming.&entry.1838667208=http%3A//arxiv.org/abs/2601.01955v1&entry.124074799=Read"},
{"title": "Genie Sim 3.0 : A High-Fidelity Comprehensive Simulation Platform for Humanoid Robot", "author": "Chenghao Yin and Da Huang and Di Yang and Jichao Wang and Nanshu Zhao and Chen Xu and Wenjun Sun and Linjie Hou and Zhijun Li and Junhui Wu and Zhaobo Liu and Zhen Xiao and Sheng Zhang and Lei Bao and Rui Feng and Zhenquan Pang and Jiayu Li and Qian Wang and Maoqing Yao", "abstract": "The development of robust and generalizable robot learning models is critically contingent upon the availability of large-scale, diverse training data and reliable evaluation benchmarks. Collecting data in the physical world poses prohibitive costs and scalability challenges, and prevailing simulation benchmarks frequently suffer from fragmentation, narrow scope, or insufficient fidelity to enable effective sim-to-real transfer. To address these challenges, we introduce Genie Sim 3.0, a unified simulation platform for robotic manipulation. We present Genie Sim Generator, a large language model (LLM)-powered tool that constructs high-fidelity scenes from natural language instructions. Its principal strength resides in rapid and multi-dimensional generalization, facilitating the synthesis of diverse environments to support scalable data collection and robust policy evaluation. We introduce the first benchmark that pioneers the application of LLM for automated evaluation. It leverages LLM to mass-generate evaluation scenarios and employs Vision-Language Model (VLM) to establish an automated assessment pipeline. We also release an open-source dataset comprising more than 10,000 hours of synthetic data across over 200 tasks. Through systematic experimentation, we validate the robust zero-shot sim-to-real transfer capability of our open-source dataset, demonstrating that synthetic data can server as an effective substitute for real-world data under controlled conditions for scalable policy training. For code and dataset details, please refer to: https://github.com/AgibotTech/genie_sim.", "link": "http://arxiv.org/abs/2601.02078v1", "date": "2026-01-05", "relevancy": 2.5716, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.7091}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6075}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Genie%20Sim%203.0%20%3A%20A%20High-Fidelity%20Comprehensive%20Simulation%20Platform%20for%20Humanoid%20Robot&body=Title%3A%20Genie%20Sim%203.0%20%3A%20A%20High-Fidelity%20Comprehensive%20Simulation%20Platform%20for%20Humanoid%20Robot%0AAuthor%3A%20Chenghao%20Yin%20and%20Da%20Huang%20and%20Di%20Yang%20and%20Jichao%20Wang%20and%20Nanshu%20Zhao%20and%20Chen%20Xu%20and%20Wenjun%20Sun%20and%20Linjie%20Hou%20and%20Zhijun%20Li%20and%20Junhui%20Wu%20and%20Zhaobo%20Liu%20and%20Zhen%20Xiao%20and%20Sheng%20Zhang%20and%20Lei%20Bao%20and%20Rui%20Feng%20and%20Zhenquan%20Pang%20and%20Jiayu%20Li%20and%20Qian%20Wang%20and%20Maoqing%20Yao%0AAbstract%3A%20The%20development%20of%20robust%20and%20generalizable%20robot%20learning%20models%20is%20critically%20contingent%20upon%20the%20availability%20of%20large-scale%2C%20diverse%20training%20data%20and%20reliable%20evaluation%20benchmarks.%20Collecting%20data%20in%20the%20physical%20world%20poses%20prohibitive%20costs%20and%20scalability%20challenges%2C%20and%20prevailing%20simulation%20benchmarks%20frequently%20suffer%20from%20fragmentation%2C%20narrow%20scope%2C%20or%20insufficient%20fidelity%20to%20enable%20effective%20sim-to-real%20transfer.%20To%20address%20these%20challenges%2C%20we%20introduce%20Genie%20Sim%203.0%2C%20a%20unified%20simulation%20platform%20for%20robotic%20manipulation.%20We%20present%20Genie%20Sim%20Generator%2C%20a%20large%20language%20model%20%28LLM%29-powered%20tool%20that%20constructs%20high-fidelity%20scenes%20from%20natural%20language%20instructions.%20Its%20principal%20strength%20resides%20in%20rapid%20and%20multi-dimensional%20generalization%2C%20facilitating%20the%20synthesis%20of%20diverse%20environments%20to%20support%20scalable%20data%20collection%20and%20robust%20policy%20evaluation.%20We%20introduce%20the%20first%20benchmark%20that%20pioneers%20the%20application%20of%20LLM%20for%20automated%20evaluation.%20It%20leverages%20LLM%20to%20mass-generate%20evaluation%20scenarios%20and%20employs%20Vision-Language%20Model%20%28VLM%29%20to%20establish%20an%20automated%20assessment%20pipeline.%20We%20also%20release%20an%20open-source%20dataset%20comprising%20more%20than%2010%2C000%20hours%20of%20synthetic%20data%20across%20over%20200%20tasks.%20Through%20systematic%20experimentation%2C%20we%20validate%20the%20robust%20zero-shot%20sim-to-real%20transfer%20capability%20of%20our%20open-source%20dataset%2C%20demonstrating%20that%20synthetic%20data%20can%20server%20as%20an%20effective%20substitute%20for%20real-world%20data%20under%20controlled%20conditions%20for%20scalable%20policy%20training.%20For%20code%20and%20dataset%20details%2C%20please%20refer%20to%3A%20https%3A//github.com/AgibotTech/genie_sim.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02078v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenie%2520Sim%25203.0%2520%253A%2520A%2520High-Fidelity%2520Comprehensive%2520Simulation%2520Platform%2520for%2520Humanoid%2520Robot%26entry.906535625%3DChenghao%2520Yin%2520and%2520Da%2520Huang%2520and%2520Di%2520Yang%2520and%2520Jichao%2520Wang%2520and%2520Nanshu%2520Zhao%2520and%2520Chen%2520Xu%2520and%2520Wenjun%2520Sun%2520and%2520Linjie%2520Hou%2520and%2520Zhijun%2520Li%2520and%2520Junhui%2520Wu%2520and%2520Zhaobo%2520Liu%2520and%2520Zhen%2520Xiao%2520and%2520Sheng%2520Zhang%2520and%2520Lei%2520Bao%2520and%2520Rui%2520Feng%2520and%2520Zhenquan%2520Pang%2520and%2520Jiayu%2520Li%2520and%2520Qian%2520Wang%2520and%2520Maoqing%2520Yao%26entry.1292438233%3DThe%2520development%2520of%2520robust%2520and%2520generalizable%2520robot%2520learning%2520models%2520is%2520critically%2520contingent%2520upon%2520the%2520availability%2520of%2520large-scale%252C%2520diverse%2520training%2520data%2520and%2520reliable%2520evaluation%2520benchmarks.%2520Collecting%2520data%2520in%2520the%2520physical%2520world%2520poses%2520prohibitive%2520costs%2520and%2520scalability%2520challenges%252C%2520and%2520prevailing%2520simulation%2520benchmarks%2520frequently%2520suffer%2520from%2520fragmentation%252C%2520narrow%2520scope%252C%2520or%2520insufficient%2520fidelity%2520to%2520enable%2520effective%2520sim-to-real%2520transfer.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520Genie%2520Sim%25203.0%252C%2520a%2520unified%2520simulation%2520platform%2520for%2520robotic%2520manipulation.%2520We%2520present%2520Genie%2520Sim%2520Generator%252C%2520a%2520large%2520language%2520model%2520%2528LLM%2529-powered%2520tool%2520that%2520constructs%2520high-fidelity%2520scenes%2520from%2520natural%2520language%2520instructions.%2520Its%2520principal%2520strength%2520resides%2520in%2520rapid%2520and%2520multi-dimensional%2520generalization%252C%2520facilitating%2520the%2520synthesis%2520of%2520diverse%2520environments%2520to%2520support%2520scalable%2520data%2520collection%2520and%2520robust%2520policy%2520evaluation.%2520We%2520introduce%2520the%2520first%2520benchmark%2520that%2520pioneers%2520the%2520application%2520of%2520LLM%2520for%2520automated%2520evaluation.%2520It%2520leverages%2520LLM%2520to%2520mass-generate%2520evaluation%2520scenarios%2520and%2520employs%2520Vision-Language%2520Model%2520%2528VLM%2529%2520to%2520establish%2520an%2520automated%2520assessment%2520pipeline.%2520We%2520also%2520release%2520an%2520open-source%2520dataset%2520comprising%2520more%2520than%252010%252C000%2520hours%2520of%2520synthetic%2520data%2520across%2520over%2520200%2520tasks.%2520Through%2520systematic%2520experimentation%252C%2520we%2520validate%2520the%2520robust%2520zero-shot%2520sim-to-real%2520transfer%2520capability%2520of%2520our%2520open-source%2520dataset%252C%2520demonstrating%2520that%2520synthetic%2520data%2520can%2520server%2520as%2520an%2520effective%2520substitute%2520for%2520real-world%2520data%2520under%2520controlled%2520conditions%2520for%2520scalable%2520policy%2520training.%2520For%2520code%2520and%2520dataset%2520details%252C%2520please%2520refer%2520to%253A%2520https%253A//github.com/AgibotTech/genie_sim.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02078v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Genie%20Sim%203.0%20%3A%20A%20High-Fidelity%20Comprehensive%20Simulation%20Platform%20for%20Humanoid%20Robot&entry.906535625=Chenghao%20Yin%20and%20Da%20Huang%20and%20Di%20Yang%20and%20Jichao%20Wang%20and%20Nanshu%20Zhao%20and%20Chen%20Xu%20and%20Wenjun%20Sun%20and%20Linjie%20Hou%20and%20Zhijun%20Li%20and%20Junhui%20Wu%20and%20Zhaobo%20Liu%20and%20Zhen%20Xiao%20and%20Sheng%20Zhang%20and%20Lei%20Bao%20and%20Rui%20Feng%20and%20Zhenquan%20Pang%20and%20Jiayu%20Li%20and%20Qian%20Wang%20and%20Maoqing%20Yao&entry.1292438233=The%20development%20of%20robust%20and%20generalizable%20robot%20learning%20models%20is%20critically%20contingent%20upon%20the%20availability%20of%20large-scale%2C%20diverse%20training%20data%20and%20reliable%20evaluation%20benchmarks.%20Collecting%20data%20in%20the%20physical%20world%20poses%20prohibitive%20costs%20and%20scalability%20challenges%2C%20and%20prevailing%20simulation%20benchmarks%20frequently%20suffer%20from%20fragmentation%2C%20narrow%20scope%2C%20or%20insufficient%20fidelity%20to%20enable%20effective%20sim-to-real%20transfer.%20To%20address%20these%20challenges%2C%20we%20introduce%20Genie%20Sim%203.0%2C%20a%20unified%20simulation%20platform%20for%20robotic%20manipulation.%20We%20present%20Genie%20Sim%20Generator%2C%20a%20large%20language%20model%20%28LLM%29-powered%20tool%20that%20constructs%20high-fidelity%20scenes%20from%20natural%20language%20instructions.%20Its%20principal%20strength%20resides%20in%20rapid%20and%20multi-dimensional%20generalization%2C%20facilitating%20the%20synthesis%20of%20diverse%20environments%20to%20support%20scalable%20data%20collection%20and%20robust%20policy%20evaluation.%20We%20introduce%20the%20first%20benchmark%20that%20pioneers%20the%20application%20of%20LLM%20for%20automated%20evaluation.%20It%20leverages%20LLM%20to%20mass-generate%20evaluation%20scenarios%20and%20employs%20Vision-Language%20Model%20%28VLM%29%20to%20establish%20an%20automated%20assessment%20pipeline.%20We%20also%20release%20an%20open-source%20dataset%20comprising%20more%20than%2010%2C000%20hours%20of%20synthetic%20data%20across%20over%20200%20tasks.%20Through%20systematic%20experimentation%2C%20we%20validate%20the%20robust%20zero-shot%20sim-to-real%20transfer%20capability%20of%20our%20open-source%20dataset%2C%20demonstrating%20that%20synthetic%20data%20can%20server%20as%20an%20effective%20substitute%20for%20real-world%20data%20under%20controlled%20conditions%20for%20scalable%20policy%20training.%20For%20code%20and%20dataset%20details%2C%20please%20refer%20to%3A%20https%3A//github.com/AgibotTech/genie_sim.&entry.1838667208=http%3A//arxiv.org/abs/2601.02078v1&entry.124074799=Read"},
{"title": "Exploring Approaches for Detecting Memorization of Recommender System Data in Large Language Models", "author": "Antonio Colacicco and Vito Guida and Dario Di Palma and Fedelucio Narducci and Tommaso Di Noia", "abstract": "Large Language Models (LLMs) are increasingly applied in recommendation scenarios due to their strong natural language understanding and generation capabilities. However, they are trained on vast corpora whose contents are not publicly disclosed, raising concerns about data leakage. Recent work has shown that the MovieLens-1M dataset is memorized by both the LLaMA and OpenAI model families, but the extraction of such memorized data has so far relied exclusively on manual prompt engineering. In this paper, we pose three main questions: Is it possible to enhance manual prompting? Can LLM memorization be detected through methods beyond manual prompting? And can the detection of data leakage be automated? To address these questions, we evaluate three approaches: (i) jailbreak prompt engineering; (ii) unsupervised latent knowledge discovery, probing internal activations via Contrast-Consistent Search (CCS) and Cluster-Norm; and (iii) Automatic Prompt Engineering (APE), which frames prompt discovery as a meta-learning process that iteratively refines candidate instructions. Experiments on MovieLens-1M using LLaMA models show that jailbreak prompting does not improve the retrieval of memorized items and remains inconsistent; CCS reliably distinguishes genuine from fabricated movie titles but fails on numerical user and rating data; and APE retrieves item-level information with moderate success yet struggles to recover numerical interactions. These findings suggest that automatically optimizing prompts is the most promising strategy for extracting memorized samples.", "link": "http://arxiv.org/abs/2601.02002v1", "date": "2026-01-05", "relevancy": 2.5455, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.516}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.516}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Approaches%20for%20Detecting%20Memorization%20of%20Recommender%20System%20Data%20in%20Large%20Language%20Models&body=Title%3A%20Exploring%20Approaches%20for%20Detecting%20Memorization%20of%20Recommender%20System%20Data%20in%20Large%20Language%20Models%0AAuthor%3A%20Antonio%20Colacicco%20and%20Vito%20Guida%20and%20Dario%20Di%20Palma%20and%20Fedelucio%20Narducci%20and%20Tommaso%20Di%20Noia%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20applied%20in%20recommendation%20scenarios%20due%20to%20their%20strong%20natural%20language%20understanding%20and%20generation%20capabilities.%20However%2C%20they%20are%20trained%20on%20vast%20corpora%20whose%20contents%20are%20not%20publicly%20disclosed%2C%20raising%20concerns%20about%20data%20leakage.%20Recent%20work%20has%20shown%20that%20the%20MovieLens-1M%20dataset%20is%20memorized%20by%20both%20the%20LLaMA%20and%20OpenAI%20model%20families%2C%20but%20the%20extraction%20of%20such%20memorized%20data%20has%20so%20far%20relied%20exclusively%20on%20manual%20prompt%20engineering.%20In%20this%20paper%2C%20we%20pose%20three%20main%20questions%3A%20Is%20it%20possible%20to%20enhance%20manual%20prompting%3F%20Can%20LLM%20memorization%20be%20detected%20through%20methods%20beyond%20manual%20prompting%3F%20And%20can%20the%20detection%20of%20data%20leakage%20be%20automated%3F%20To%20address%20these%20questions%2C%20we%20evaluate%20three%20approaches%3A%20%28i%29%20jailbreak%20prompt%20engineering%3B%20%28ii%29%20unsupervised%20latent%20knowledge%20discovery%2C%20probing%20internal%20activations%20via%20Contrast-Consistent%20Search%20%28CCS%29%20and%20Cluster-Norm%3B%20and%20%28iii%29%20Automatic%20Prompt%20Engineering%20%28APE%29%2C%20which%20frames%20prompt%20discovery%20as%20a%20meta-learning%20process%20that%20iteratively%20refines%20candidate%20instructions.%20Experiments%20on%20MovieLens-1M%20using%20LLaMA%20models%20show%20that%20jailbreak%20prompting%20does%20not%20improve%20the%20retrieval%20of%20memorized%20items%20and%20remains%20inconsistent%3B%20CCS%20reliably%20distinguishes%20genuine%20from%20fabricated%20movie%20titles%20but%20fails%20on%20numerical%20user%20and%20rating%20data%3B%20and%20APE%20retrieves%20item-level%20information%20with%20moderate%20success%20yet%20struggles%20to%20recover%20numerical%20interactions.%20These%20findings%20suggest%20that%20automatically%20optimizing%20prompts%20is%20the%20most%20promising%20strategy%20for%20extracting%20memorized%20samples.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02002v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Approaches%2520for%2520Detecting%2520Memorization%2520of%2520Recommender%2520System%2520Data%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DAntonio%2520Colacicco%2520and%2520Vito%2520Guida%2520and%2520Dario%2520Di%2520Palma%2520and%2520Fedelucio%2520Narducci%2520and%2520Tommaso%2520Di%2520Noia%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520increasingly%2520applied%2520in%2520recommendation%2520scenarios%2520due%2520to%2520their%2520strong%2520natural%2520language%2520understanding%2520and%2520generation%2520capabilities.%2520However%252C%2520they%2520are%2520trained%2520on%2520vast%2520corpora%2520whose%2520contents%2520are%2520not%2520publicly%2520disclosed%252C%2520raising%2520concerns%2520about%2520data%2520leakage.%2520Recent%2520work%2520has%2520shown%2520that%2520the%2520MovieLens-1M%2520dataset%2520is%2520memorized%2520by%2520both%2520the%2520LLaMA%2520and%2520OpenAI%2520model%2520families%252C%2520but%2520the%2520extraction%2520of%2520such%2520memorized%2520data%2520has%2520so%2520far%2520relied%2520exclusively%2520on%2520manual%2520prompt%2520engineering.%2520In%2520this%2520paper%252C%2520we%2520pose%2520three%2520main%2520questions%253A%2520Is%2520it%2520possible%2520to%2520enhance%2520manual%2520prompting%253F%2520Can%2520LLM%2520memorization%2520be%2520detected%2520through%2520methods%2520beyond%2520manual%2520prompting%253F%2520And%2520can%2520the%2520detection%2520of%2520data%2520leakage%2520be%2520automated%253F%2520To%2520address%2520these%2520questions%252C%2520we%2520evaluate%2520three%2520approaches%253A%2520%2528i%2529%2520jailbreak%2520prompt%2520engineering%253B%2520%2528ii%2529%2520unsupervised%2520latent%2520knowledge%2520discovery%252C%2520probing%2520internal%2520activations%2520via%2520Contrast-Consistent%2520Search%2520%2528CCS%2529%2520and%2520Cluster-Norm%253B%2520and%2520%2528iii%2529%2520Automatic%2520Prompt%2520Engineering%2520%2528APE%2529%252C%2520which%2520frames%2520prompt%2520discovery%2520as%2520a%2520meta-learning%2520process%2520that%2520iteratively%2520refines%2520candidate%2520instructions.%2520Experiments%2520on%2520MovieLens-1M%2520using%2520LLaMA%2520models%2520show%2520that%2520jailbreak%2520prompting%2520does%2520not%2520improve%2520the%2520retrieval%2520of%2520memorized%2520items%2520and%2520remains%2520inconsistent%253B%2520CCS%2520reliably%2520distinguishes%2520genuine%2520from%2520fabricated%2520movie%2520titles%2520but%2520fails%2520on%2520numerical%2520user%2520and%2520rating%2520data%253B%2520and%2520APE%2520retrieves%2520item-level%2520information%2520with%2520moderate%2520success%2520yet%2520struggles%2520to%2520recover%2520numerical%2520interactions.%2520These%2520findings%2520suggest%2520that%2520automatically%2520optimizing%2520prompts%2520is%2520the%2520most%2520promising%2520strategy%2520for%2520extracting%2520memorized%2520samples.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02002v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Approaches%20for%20Detecting%20Memorization%20of%20Recommender%20System%20Data%20in%20Large%20Language%20Models&entry.906535625=Antonio%20Colacicco%20and%20Vito%20Guida%20and%20Dario%20Di%20Palma%20and%20Fedelucio%20Narducci%20and%20Tommaso%20Di%20Noia&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20applied%20in%20recommendation%20scenarios%20due%20to%20their%20strong%20natural%20language%20understanding%20and%20generation%20capabilities.%20However%2C%20they%20are%20trained%20on%20vast%20corpora%20whose%20contents%20are%20not%20publicly%20disclosed%2C%20raising%20concerns%20about%20data%20leakage.%20Recent%20work%20has%20shown%20that%20the%20MovieLens-1M%20dataset%20is%20memorized%20by%20both%20the%20LLaMA%20and%20OpenAI%20model%20families%2C%20but%20the%20extraction%20of%20such%20memorized%20data%20has%20so%20far%20relied%20exclusively%20on%20manual%20prompt%20engineering.%20In%20this%20paper%2C%20we%20pose%20three%20main%20questions%3A%20Is%20it%20possible%20to%20enhance%20manual%20prompting%3F%20Can%20LLM%20memorization%20be%20detected%20through%20methods%20beyond%20manual%20prompting%3F%20And%20can%20the%20detection%20of%20data%20leakage%20be%20automated%3F%20To%20address%20these%20questions%2C%20we%20evaluate%20three%20approaches%3A%20%28i%29%20jailbreak%20prompt%20engineering%3B%20%28ii%29%20unsupervised%20latent%20knowledge%20discovery%2C%20probing%20internal%20activations%20via%20Contrast-Consistent%20Search%20%28CCS%29%20and%20Cluster-Norm%3B%20and%20%28iii%29%20Automatic%20Prompt%20Engineering%20%28APE%29%2C%20which%20frames%20prompt%20discovery%20as%20a%20meta-learning%20process%20that%20iteratively%20refines%20candidate%20instructions.%20Experiments%20on%20MovieLens-1M%20using%20LLaMA%20models%20show%20that%20jailbreak%20prompting%20does%20not%20improve%20the%20retrieval%20of%20memorized%20items%20and%20remains%20inconsistent%3B%20CCS%20reliably%20distinguishes%20genuine%20from%20fabricated%20movie%20titles%20but%20fails%20on%20numerical%20user%20and%20rating%20data%3B%20and%20APE%20retrieves%20item-level%20information%20with%20moderate%20success%20yet%20struggles%20to%20recover%20numerical%20interactions.%20These%20findings%20suggest%20that%20automatically%20optimizing%20prompts%20is%20the%20most%20promising%20strategy%20for%20extracting%20memorized%20samples.&entry.1838667208=http%3A//arxiv.org/abs/2601.02002v1&entry.124074799=Read"},
{"title": "Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting", "author": "Muxi Diao and Lele Yang and Wuxuan Gong and Yutong Zhang and Zhonghao Yan and Yufei Han and Kongming Liang and Weiran Xu and Zhanyu Ma", "abstract": "Supervised Fine-Tuning (SFT) is the standard paradigm for domain adaptation, yet it frequently incurs the cost of catastrophic forgetting. In sharp contrast, on-policy Reinforcement Learning (RL) effectively preserves general capabilities. We investigate this discrepancy and identify a fundamental distributional gap: while RL aligns with the model's internal belief, SFT forces the model to fit external supervision. This mismatch often manifests as \"Confident Conflicts\" tokens characterized by low probability but low entropy. In these instances, the model is highly confident in its own prediction but is forced to learn a divergent ground truth, triggering destructive gradient updates. To address this, we propose Entropy-Adaptive Fine-Tuning (EAFT). Unlike methods relying solely on prediction probability, EAFT utilizes token-level entropy as a gating mechanism to distinguish between epistemic uncertainty and knowledge conflict. This allows the model to learn from uncertain samples while suppressing gradients on conflicting data. Extensive experiments on Qwen and GLM series (ranging from 4B to 32B parameters) across mathematical, medical, and agentic domains confirm our hypothesis. EAFT consistently matches the downstream performance of standard SFT while significantly mitigating the degradation of general capabilities.", "link": "http://arxiv.org/abs/2601.02151v1", "date": "2026-01-05", "relevancy": 2.5343, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5109}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5074}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Entropy-Adaptive%20Fine-Tuning%3A%20Resolving%20Confident%20Conflicts%20to%20Mitigate%20Forgetting&body=Title%3A%20Entropy-Adaptive%20Fine-Tuning%3A%20Resolving%20Confident%20Conflicts%20to%20Mitigate%20Forgetting%0AAuthor%3A%20Muxi%20Diao%20and%20Lele%20Yang%20and%20Wuxuan%20Gong%20and%20Yutong%20Zhang%20and%20Zhonghao%20Yan%20and%20Yufei%20Han%20and%20Kongming%20Liang%20and%20Weiran%20Xu%20and%20Zhanyu%20Ma%0AAbstract%3A%20Supervised%20Fine-Tuning%20%28SFT%29%20is%20the%20standard%20paradigm%20for%20domain%20adaptation%2C%20yet%20it%20frequently%20incurs%20the%20cost%20of%20catastrophic%20forgetting.%20In%20sharp%20contrast%2C%20on-policy%20Reinforcement%20Learning%20%28RL%29%20effectively%20preserves%20general%20capabilities.%20We%20investigate%20this%20discrepancy%20and%20identify%20a%20fundamental%20distributional%20gap%3A%20while%20RL%20aligns%20with%20the%20model%27s%20internal%20belief%2C%20SFT%20forces%20the%20model%20to%20fit%20external%20supervision.%20This%20mismatch%20often%20manifests%20as%20%22Confident%20Conflicts%22%20tokens%20characterized%20by%20low%20probability%20but%20low%20entropy.%20In%20these%20instances%2C%20the%20model%20is%20highly%20confident%20in%20its%20own%20prediction%20but%20is%20forced%20to%20learn%20a%20divergent%20ground%20truth%2C%20triggering%20destructive%20gradient%20updates.%20To%20address%20this%2C%20we%20propose%20Entropy-Adaptive%20Fine-Tuning%20%28EAFT%29.%20Unlike%20methods%20relying%20solely%20on%20prediction%20probability%2C%20EAFT%20utilizes%20token-level%20entropy%20as%20a%20gating%20mechanism%20to%20distinguish%20between%20epistemic%20uncertainty%20and%20knowledge%20conflict.%20This%20allows%20the%20model%20to%20learn%20from%20uncertain%20samples%20while%20suppressing%20gradients%20on%20conflicting%20data.%20Extensive%20experiments%20on%20Qwen%20and%20GLM%20series%20%28ranging%20from%204B%20to%2032B%20parameters%29%20across%20mathematical%2C%20medical%2C%20and%20agentic%20domains%20confirm%20our%20hypothesis.%20EAFT%20consistently%20matches%20the%20downstream%20performance%20of%20standard%20SFT%20while%20significantly%20mitigating%20the%20degradation%20of%20general%20capabilities.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02151v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEntropy-Adaptive%2520Fine-Tuning%253A%2520Resolving%2520Confident%2520Conflicts%2520to%2520Mitigate%2520Forgetting%26entry.906535625%3DMuxi%2520Diao%2520and%2520Lele%2520Yang%2520and%2520Wuxuan%2520Gong%2520and%2520Yutong%2520Zhang%2520and%2520Zhonghao%2520Yan%2520and%2520Yufei%2520Han%2520and%2520Kongming%2520Liang%2520and%2520Weiran%2520Xu%2520and%2520Zhanyu%2520Ma%26entry.1292438233%3DSupervised%2520Fine-Tuning%2520%2528SFT%2529%2520is%2520the%2520standard%2520paradigm%2520for%2520domain%2520adaptation%252C%2520yet%2520it%2520frequently%2520incurs%2520the%2520cost%2520of%2520catastrophic%2520forgetting.%2520In%2520sharp%2520contrast%252C%2520on-policy%2520Reinforcement%2520Learning%2520%2528RL%2529%2520effectively%2520preserves%2520general%2520capabilities.%2520We%2520investigate%2520this%2520discrepancy%2520and%2520identify%2520a%2520fundamental%2520distributional%2520gap%253A%2520while%2520RL%2520aligns%2520with%2520the%2520model%2527s%2520internal%2520belief%252C%2520SFT%2520forces%2520the%2520model%2520to%2520fit%2520external%2520supervision.%2520This%2520mismatch%2520often%2520manifests%2520as%2520%2522Confident%2520Conflicts%2522%2520tokens%2520characterized%2520by%2520low%2520probability%2520but%2520low%2520entropy.%2520In%2520these%2520instances%252C%2520the%2520model%2520is%2520highly%2520confident%2520in%2520its%2520own%2520prediction%2520but%2520is%2520forced%2520to%2520learn%2520a%2520divergent%2520ground%2520truth%252C%2520triggering%2520destructive%2520gradient%2520updates.%2520To%2520address%2520this%252C%2520we%2520propose%2520Entropy-Adaptive%2520Fine-Tuning%2520%2528EAFT%2529.%2520Unlike%2520methods%2520relying%2520solely%2520on%2520prediction%2520probability%252C%2520EAFT%2520utilizes%2520token-level%2520entropy%2520as%2520a%2520gating%2520mechanism%2520to%2520distinguish%2520between%2520epistemic%2520uncertainty%2520and%2520knowledge%2520conflict.%2520This%2520allows%2520the%2520model%2520to%2520learn%2520from%2520uncertain%2520samples%2520while%2520suppressing%2520gradients%2520on%2520conflicting%2520data.%2520Extensive%2520experiments%2520on%2520Qwen%2520and%2520GLM%2520series%2520%2528ranging%2520from%25204B%2520to%252032B%2520parameters%2529%2520across%2520mathematical%252C%2520medical%252C%2520and%2520agentic%2520domains%2520confirm%2520our%2520hypothesis.%2520EAFT%2520consistently%2520matches%2520the%2520downstream%2520performance%2520of%2520standard%2520SFT%2520while%2520significantly%2520mitigating%2520the%2520degradation%2520of%2520general%2520capabilities.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02151v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Entropy-Adaptive%20Fine-Tuning%3A%20Resolving%20Confident%20Conflicts%20to%20Mitigate%20Forgetting&entry.906535625=Muxi%20Diao%20and%20Lele%20Yang%20and%20Wuxuan%20Gong%20and%20Yutong%20Zhang%20and%20Zhonghao%20Yan%20and%20Yufei%20Han%20and%20Kongming%20Liang%20and%20Weiran%20Xu%20and%20Zhanyu%20Ma&entry.1292438233=Supervised%20Fine-Tuning%20%28SFT%29%20is%20the%20standard%20paradigm%20for%20domain%20adaptation%2C%20yet%20it%20frequently%20incurs%20the%20cost%20of%20catastrophic%20forgetting.%20In%20sharp%20contrast%2C%20on-policy%20Reinforcement%20Learning%20%28RL%29%20effectively%20preserves%20general%20capabilities.%20We%20investigate%20this%20discrepancy%20and%20identify%20a%20fundamental%20distributional%20gap%3A%20while%20RL%20aligns%20with%20the%20model%27s%20internal%20belief%2C%20SFT%20forces%20the%20model%20to%20fit%20external%20supervision.%20This%20mismatch%20often%20manifests%20as%20%22Confident%20Conflicts%22%20tokens%20characterized%20by%20low%20probability%20but%20low%20entropy.%20In%20these%20instances%2C%20the%20model%20is%20highly%20confident%20in%20its%20own%20prediction%20but%20is%20forced%20to%20learn%20a%20divergent%20ground%20truth%2C%20triggering%20destructive%20gradient%20updates.%20To%20address%20this%2C%20we%20propose%20Entropy-Adaptive%20Fine-Tuning%20%28EAFT%29.%20Unlike%20methods%20relying%20solely%20on%20prediction%20probability%2C%20EAFT%20utilizes%20token-level%20entropy%20as%20a%20gating%20mechanism%20to%20distinguish%20between%20epistemic%20uncertainty%20and%20knowledge%20conflict.%20This%20allows%20the%20model%20to%20learn%20from%20uncertain%20samples%20while%20suppressing%20gradients%20on%20conflicting%20data.%20Extensive%20experiments%20on%20Qwen%20and%20GLM%20series%20%28ranging%20from%204B%20to%2032B%20parameters%29%20across%20mathematical%2C%20medical%2C%20and%20agentic%20domains%20confirm%20our%20hypothesis.%20EAFT%20consistently%20matches%20the%20downstream%20performance%20of%20standard%20SFT%20while%20significantly%20mitigating%20the%20degradation%20of%20general%20capabilities.&entry.1838667208=http%3A//arxiv.org/abs/2601.02151v1&entry.124074799=Read"},
{"title": "Rank-based Geographical Regularization: Revisiting Contrastive Self-Supervised Learning for Multispectral Remote Sensing Imagery", "author": "Tom Burgert and Leonard Hackel and Paolo Rota and Beg\u00fcm Demir", "abstract": "Self-supervised learning (SSL) has become a powerful paradigm for learning from large, unlabeled datasets, particularly in computer vision (CV). However, applying SSL to multispectral remote sensing (RS) images presents unique challenges and opportunities due to the geographical and temporal variability of the data. In this paper, we introduce GeoRank, a novel regularization method for contrastive SSL that improves upon prior techniques by directly optimizing spherical distances to embed geographical relationships into the learned feature space. GeoRank outperforms or matches prior methods that integrate geographical metadata and consistently improves diverse contrastive SSL algorithms (e.g., BYOL, DINO). Beyond this, we present a systematic investigation of key adaptations of contrastive SSL for multispectral RS images, including the effectiveness of data augmentations, the impact of dataset cardinality and image size on performance, and the task dependency of temporal views. Code is available at https://github.com/tomburgert/georank.", "link": "http://arxiv.org/abs/2601.02289v1", "date": "2026-01-05", "relevancy": 2.5335, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5332}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5013}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rank-based%20Geographical%20Regularization%3A%20Revisiting%20Contrastive%20Self-Supervised%20Learning%20for%20Multispectral%20Remote%20Sensing%20Imagery&body=Title%3A%20Rank-based%20Geographical%20Regularization%3A%20Revisiting%20Contrastive%20Self-Supervised%20Learning%20for%20Multispectral%20Remote%20Sensing%20Imagery%0AAuthor%3A%20Tom%20Burgert%20and%20Leonard%20Hackel%20and%20Paolo%20Rota%20and%20Beg%C3%BCm%20Demir%0AAbstract%3A%20Self-supervised%20learning%20%28SSL%29%20has%20become%20a%20powerful%20paradigm%20for%20learning%20from%20large%2C%20unlabeled%20datasets%2C%20particularly%20in%20computer%20vision%20%28CV%29.%20However%2C%20applying%20SSL%20to%20multispectral%20remote%20sensing%20%28RS%29%20images%20presents%20unique%20challenges%20and%20opportunities%20due%20to%20the%20geographical%20and%20temporal%20variability%20of%20the%20data.%20In%20this%20paper%2C%20we%20introduce%20GeoRank%2C%20a%20novel%20regularization%20method%20for%20contrastive%20SSL%20that%20improves%20upon%20prior%20techniques%20by%20directly%20optimizing%20spherical%20distances%20to%20embed%20geographical%20relationships%20into%20the%20learned%20feature%20space.%20GeoRank%20outperforms%20or%20matches%20prior%20methods%20that%20integrate%20geographical%20metadata%20and%20consistently%20improves%20diverse%20contrastive%20SSL%20algorithms%20%28e.g.%2C%20BYOL%2C%20DINO%29.%20Beyond%20this%2C%20we%20present%20a%20systematic%20investigation%20of%20key%20adaptations%20of%20contrastive%20SSL%20for%20multispectral%20RS%20images%2C%20including%20the%20effectiveness%20of%20data%20augmentations%2C%20the%20impact%20of%20dataset%20cardinality%20and%20image%20size%20on%20performance%2C%20and%20the%20task%20dependency%20of%20temporal%20views.%20Code%20is%20available%20at%20https%3A//github.com/tomburgert/georank.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02289v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRank-based%2520Geographical%2520Regularization%253A%2520Revisiting%2520Contrastive%2520Self-Supervised%2520Learning%2520for%2520Multispectral%2520Remote%2520Sensing%2520Imagery%26entry.906535625%3DTom%2520Burgert%2520and%2520Leonard%2520Hackel%2520and%2520Paolo%2520Rota%2520and%2520Beg%25C3%25BCm%2520Demir%26entry.1292438233%3DSelf-supervised%2520learning%2520%2528SSL%2529%2520has%2520become%2520a%2520powerful%2520paradigm%2520for%2520learning%2520from%2520large%252C%2520unlabeled%2520datasets%252C%2520particularly%2520in%2520computer%2520vision%2520%2528CV%2529.%2520However%252C%2520applying%2520SSL%2520to%2520multispectral%2520remote%2520sensing%2520%2528RS%2529%2520images%2520presents%2520unique%2520challenges%2520and%2520opportunities%2520due%2520to%2520the%2520geographical%2520and%2520temporal%2520variability%2520of%2520the%2520data.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520GeoRank%252C%2520a%2520novel%2520regularization%2520method%2520for%2520contrastive%2520SSL%2520that%2520improves%2520upon%2520prior%2520techniques%2520by%2520directly%2520optimizing%2520spherical%2520distances%2520to%2520embed%2520geographical%2520relationships%2520into%2520the%2520learned%2520feature%2520space.%2520GeoRank%2520outperforms%2520or%2520matches%2520prior%2520methods%2520that%2520integrate%2520geographical%2520metadata%2520and%2520consistently%2520improves%2520diverse%2520contrastive%2520SSL%2520algorithms%2520%2528e.g.%252C%2520BYOL%252C%2520DINO%2529.%2520Beyond%2520this%252C%2520we%2520present%2520a%2520systematic%2520investigation%2520of%2520key%2520adaptations%2520of%2520contrastive%2520SSL%2520for%2520multispectral%2520RS%2520images%252C%2520including%2520the%2520effectiveness%2520of%2520data%2520augmentations%252C%2520the%2520impact%2520of%2520dataset%2520cardinality%2520and%2520image%2520size%2520on%2520performance%252C%2520and%2520the%2520task%2520dependency%2520of%2520temporal%2520views.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/tomburgert/georank.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02289v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rank-based%20Geographical%20Regularization%3A%20Revisiting%20Contrastive%20Self-Supervised%20Learning%20for%20Multispectral%20Remote%20Sensing%20Imagery&entry.906535625=Tom%20Burgert%20and%20Leonard%20Hackel%20and%20Paolo%20Rota%20and%20Beg%C3%BCm%20Demir&entry.1292438233=Self-supervised%20learning%20%28SSL%29%20has%20become%20a%20powerful%20paradigm%20for%20learning%20from%20large%2C%20unlabeled%20datasets%2C%20particularly%20in%20computer%20vision%20%28CV%29.%20However%2C%20applying%20SSL%20to%20multispectral%20remote%20sensing%20%28RS%29%20images%20presents%20unique%20challenges%20and%20opportunities%20due%20to%20the%20geographical%20and%20temporal%20variability%20of%20the%20data.%20In%20this%20paper%2C%20we%20introduce%20GeoRank%2C%20a%20novel%20regularization%20method%20for%20contrastive%20SSL%20that%20improves%20upon%20prior%20techniques%20by%20directly%20optimizing%20spherical%20distances%20to%20embed%20geographical%20relationships%20into%20the%20learned%20feature%20space.%20GeoRank%20outperforms%20or%20matches%20prior%20methods%20that%20integrate%20geographical%20metadata%20and%20consistently%20improves%20diverse%20contrastive%20SSL%20algorithms%20%28e.g.%2C%20BYOL%2C%20DINO%29.%20Beyond%20this%2C%20we%20present%20a%20systematic%20investigation%20of%20key%20adaptations%20of%20contrastive%20SSL%20for%20multispectral%20RS%20images%2C%20including%20the%20effectiveness%20of%20data%20augmentations%2C%20the%20impact%20of%20dataset%20cardinality%20and%20image%20size%20on%20performance%2C%20and%20the%20task%20dependency%20of%20temporal%20views.%20Code%20is%20available%20at%20https%3A//github.com/tomburgert/georank.&entry.1838667208=http%3A//arxiv.org/abs/2601.02289v1&entry.124074799=Read"},
{"title": "SingingBot: An Avatar-Driven System for Robotic Face Singing Performance", "author": "Zhuoxiong Xu and Xuanchen Li and Yuhao Cheng and Fei Xu and Yichao Yan and Xiaokang Yang", "abstract": "Equipping robotic faces with singing capabilities is crucial for empathetic Human-Robot Interaction. However, existing robotic face driving research primarily focuses on conversations or mimicking static expressions, struggling to meet the high demands for continuous emotional expression and coherence in singing. To address this, we propose a novel avatar-driven framework for appealing robotic singing. We first leverage portrait video generation models embedded with extensive human priors to synthesize vivid singing avatars, providing reliable expression and emotion guidance. Subsequently, these facial features are transferred to the robot via semantic-oriented mapping functions that span a wide expression space. Furthermore, to quantitatively evaluate the emotional richness of robotic singing, we propose the Emotion Dynamic Range metric to measure the emotional breadth within the Valence-Arousal space, revealing that a broad emotional spectrum is crucial for appealing performances. Comprehensive experiments prove that our method achieves rich emotional expressions while maintaining lip-audio synchronization, significantly outperforming existing approaches.", "link": "http://arxiv.org/abs/2601.02125v1", "date": "2026-01-05", "relevancy": 2.5321, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5178}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5178}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SingingBot%3A%20An%20Avatar-Driven%20System%20for%20Robotic%20Face%20Singing%20Performance&body=Title%3A%20SingingBot%3A%20An%20Avatar-Driven%20System%20for%20Robotic%20Face%20Singing%20Performance%0AAuthor%3A%20Zhuoxiong%20Xu%20and%20Xuanchen%20Li%20and%20Yuhao%20Cheng%20and%20Fei%20Xu%20and%20Yichao%20Yan%20and%20Xiaokang%20Yang%0AAbstract%3A%20Equipping%20robotic%20faces%20with%20singing%20capabilities%20is%20crucial%20for%20empathetic%20Human-Robot%20Interaction.%20However%2C%20existing%20robotic%20face%20driving%20research%20primarily%20focuses%20on%20conversations%20or%20mimicking%20static%20expressions%2C%20struggling%20to%20meet%20the%20high%20demands%20for%20continuous%20emotional%20expression%20and%20coherence%20in%20singing.%20To%20address%20this%2C%20we%20propose%20a%20novel%20avatar-driven%20framework%20for%20appealing%20robotic%20singing.%20We%20first%20leverage%20portrait%20video%20generation%20models%20embedded%20with%20extensive%20human%20priors%20to%20synthesize%20vivid%20singing%20avatars%2C%20providing%20reliable%20expression%20and%20emotion%20guidance.%20Subsequently%2C%20these%20facial%20features%20are%20transferred%20to%20the%20robot%20via%20semantic-oriented%20mapping%20functions%20that%20span%20a%20wide%20expression%20space.%20Furthermore%2C%20to%20quantitatively%20evaluate%20the%20emotional%20richness%20of%20robotic%20singing%2C%20we%20propose%20the%20Emotion%20Dynamic%20Range%20metric%20to%20measure%20the%20emotional%20breadth%20within%20the%20Valence-Arousal%20space%2C%20revealing%20that%20a%20broad%20emotional%20spectrum%20is%20crucial%20for%20appealing%20performances.%20Comprehensive%20experiments%20prove%20that%20our%20method%20achieves%20rich%20emotional%20expressions%20while%20maintaining%20lip-audio%20synchronization%2C%20significantly%20outperforming%20existing%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02125v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingingBot%253A%2520An%2520Avatar-Driven%2520System%2520for%2520Robotic%2520Face%2520Singing%2520Performance%26entry.906535625%3DZhuoxiong%2520Xu%2520and%2520Xuanchen%2520Li%2520and%2520Yuhao%2520Cheng%2520and%2520Fei%2520Xu%2520and%2520Yichao%2520Yan%2520and%2520Xiaokang%2520Yang%26entry.1292438233%3DEquipping%2520robotic%2520faces%2520with%2520singing%2520capabilities%2520is%2520crucial%2520for%2520empathetic%2520Human-Robot%2520Interaction.%2520However%252C%2520existing%2520robotic%2520face%2520driving%2520research%2520primarily%2520focuses%2520on%2520conversations%2520or%2520mimicking%2520static%2520expressions%252C%2520struggling%2520to%2520meet%2520the%2520high%2520demands%2520for%2520continuous%2520emotional%2520expression%2520and%2520coherence%2520in%2520singing.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520novel%2520avatar-driven%2520framework%2520for%2520appealing%2520robotic%2520singing.%2520We%2520first%2520leverage%2520portrait%2520video%2520generation%2520models%2520embedded%2520with%2520extensive%2520human%2520priors%2520to%2520synthesize%2520vivid%2520singing%2520avatars%252C%2520providing%2520reliable%2520expression%2520and%2520emotion%2520guidance.%2520Subsequently%252C%2520these%2520facial%2520features%2520are%2520transferred%2520to%2520the%2520robot%2520via%2520semantic-oriented%2520mapping%2520functions%2520that%2520span%2520a%2520wide%2520expression%2520space.%2520Furthermore%252C%2520to%2520quantitatively%2520evaluate%2520the%2520emotional%2520richness%2520of%2520robotic%2520singing%252C%2520we%2520propose%2520the%2520Emotion%2520Dynamic%2520Range%2520metric%2520to%2520measure%2520the%2520emotional%2520breadth%2520within%2520the%2520Valence-Arousal%2520space%252C%2520revealing%2520that%2520a%2520broad%2520emotional%2520spectrum%2520is%2520crucial%2520for%2520appealing%2520performances.%2520Comprehensive%2520experiments%2520prove%2520that%2520our%2520method%2520achieves%2520rich%2520emotional%2520expressions%2520while%2520maintaining%2520lip-audio%2520synchronization%252C%2520significantly%2520outperforming%2520existing%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02125v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SingingBot%3A%20An%20Avatar-Driven%20System%20for%20Robotic%20Face%20Singing%20Performance&entry.906535625=Zhuoxiong%20Xu%20and%20Xuanchen%20Li%20and%20Yuhao%20Cheng%20and%20Fei%20Xu%20and%20Yichao%20Yan%20and%20Xiaokang%20Yang&entry.1292438233=Equipping%20robotic%20faces%20with%20singing%20capabilities%20is%20crucial%20for%20empathetic%20Human-Robot%20Interaction.%20However%2C%20existing%20robotic%20face%20driving%20research%20primarily%20focuses%20on%20conversations%20or%20mimicking%20static%20expressions%2C%20struggling%20to%20meet%20the%20high%20demands%20for%20continuous%20emotional%20expression%20and%20coherence%20in%20singing.%20To%20address%20this%2C%20we%20propose%20a%20novel%20avatar-driven%20framework%20for%20appealing%20robotic%20singing.%20We%20first%20leverage%20portrait%20video%20generation%20models%20embedded%20with%20extensive%20human%20priors%20to%20synthesize%20vivid%20singing%20avatars%2C%20providing%20reliable%20expression%20and%20emotion%20guidance.%20Subsequently%2C%20these%20facial%20features%20are%20transferred%20to%20the%20robot%20via%20semantic-oriented%20mapping%20functions%20that%20span%20a%20wide%20expression%20space.%20Furthermore%2C%20to%20quantitatively%20evaluate%20the%20emotional%20richness%20of%20robotic%20singing%2C%20we%20propose%20the%20Emotion%20Dynamic%20Range%20metric%20to%20measure%20the%20emotional%20breadth%20within%20the%20Valence-Arousal%20space%2C%20revealing%20that%20a%20broad%20emotional%20spectrum%20is%20crucial%20for%20appealing%20performances.%20Comprehensive%20experiments%20prove%20that%20our%20method%20achieves%20rich%20emotional%20expressions%20while%20maintaining%20lip-audio%20synchronization%2C%20significantly%20outperforming%20existing%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2601.02125v1&entry.124074799=Read"},
{"title": "HeadLighter: Disentangling Illumination in Generative 3D Gaussian Heads via Lightstage Captures", "author": "Yating Wang and Yuan Sun and Xuan Wang and Ran Yi and Boyao Zhou and Yipengjing Sun and Hongyu Liu and Yinuo Wang and Lizhuang Ma", "abstract": "Recent 3D-aware head generative models based on 3D Gaussian Splatting achieve real-time, photorealistic and view-consistent head synthesis. However, a fundamental limitation persists: the deep entanglement of illumination and intrinsic appearance prevents controllable relighting. Existing disentanglement methods rely on strong assumptions to enable weakly supervised learning, which restricts their capacity for complex illumination. To address this challenge, we introduce HeadLighter, a novel supervised framework that learns a physically plausible decomposition of appearance and illumination in head generative models. Specifically, we design a dual-branch architecture that separately models lighting-invariant head attributes and physically grounded rendering components. A progressive disentanglement training is employed to gradually inject head appearance priors into the generative architecture, supervised by multi-view images captured under controlled light conditions with a light stage setup. We further introduce a distillation strategy to generate high-quality normals for realistic rendering. Experiments demonstrate that our method preserves high-quality generation and real-time rendering, while simultaneously supporting explicit lighting and viewpoint editing. We will publicly release our code and dataset.", "link": "http://arxiv.org/abs/2601.02103v1", "date": "2026-01-05", "relevancy": 2.5319, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6345}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6345}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HeadLighter%3A%20Disentangling%20Illumination%20in%20Generative%203D%20Gaussian%20Heads%20via%20Lightstage%20Captures&body=Title%3A%20HeadLighter%3A%20Disentangling%20Illumination%20in%20Generative%203D%20Gaussian%20Heads%20via%20Lightstage%20Captures%0AAuthor%3A%20Yating%20Wang%20and%20Yuan%20Sun%20and%20Xuan%20Wang%20and%20Ran%20Yi%20and%20Boyao%20Zhou%20and%20Yipengjing%20Sun%20and%20Hongyu%20Liu%20and%20Yinuo%20Wang%20and%20Lizhuang%20Ma%0AAbstract%3A%20Recent%203D-aware%20head%20generative%20models%20based%20on%203D%20Gaussian%20Splatting%20achieve%20real-time%2C%20photorealistic%20and%20view-consistent%20head%20synthesis.%20However%2C%20a%20fundamental%20limitation%20persists%3A%20the%20deep%20entanglement%20of%20illumination%20and%20intrinsic%20appearance%20prevents%20controllable%20relighting.%20Existing%20disentanglement%20methods%20rely%20on%20strong%20assumptions%20to%20enable%20weakly%20supervised%20learning%2C%20which%20restricts%20their%20capacity%20for%20complex%20illumination.%20To%20address%20this%20challenge%2C%20we%20introduce%20HeadLighter%2C%20a%20novel%20supervised%20framework%20that%20learns%20a%20physically%20plausible%20decomposition%20of%20appearance%20and%20illumination%20in%20head%20generative%20models.%20Specifically%2C%20we%20design%20a%20dual-branch%20architecture%20that%20separately%20models%20lighting-invariant%20head%20attributes%20and%20physically%20grounded%20rendering%20components.%20A%20progressive%20disentanglement%20training%20is%20employed%20to%20gradually%20inject%20head%20appearance%20priors%20into%20the%20generative%20architecture%2C%20supervised%20by%20multi-view%20images%20captured%20under%20controlled%20light%20conditions%20with%20a%20light%20stage%20setup.%20We%20further%20introduce%20a%20distillation%20strategy%20to%20generate%20high-quality%20normals%20for%20realistic%20rendering.%20Experiments%20demonstrate%20that%20our%20method%20preserves%20high-quality%20generation%20and%20real-time%20rendering%2C%20while%20simultaneously%20supporting%20explicit%20lighting%20and%20viewpoint%20editing.%20We%20will%20publicly%20release%20our%20code%20and%20dataset.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02103v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeadLighter%253A%2520Disentangling%2520Illumination%2520in%2520Generative%25203D%2520Gaussian%2520Heads%2520via%2520Lightstage%2520Captures%26entry.906535625%3DYating%2520Wang%2520and%2520Yuan%2520Sun%2520and%2520Xuan%2520Wang%2520and%2520Ran%2520Yi%2520and%2520Boyao%2520Zhou%2520and%2520Yipengjing%2520Sun%2520and%2520Hongyu%2520Liu%2520and%2520Yinuo%2520Wang%2520and%2520Lizhuang%2520Ma%26entry.1292438233%3DRecent%25203D-aware%2520head%2520generative%2520models%2520based%2520on%25203D%2520Gaussian%2520Splatting%2520achieve%2520real-time%252C%2520photorealistic%2520and%2520view-consistent%2520head%2520synthesis.%2520However%252C%2520a%2520fundamental%2520limitation%2520persists%253A%2520the%2520deep%2520entanglement%2520of%2520illumination%2520and%2520intrinsic%2520appearance%2520prevents%2520controllable%2520relighting.%2520Existing%2520disentanglement%2520methods%2520rely%2520on%2520strong%2520assumptions%2520to%2520enable%2520weakly%2520supervised%2520learning%252C%2520which%2520restricts%2520their%2520capacity%2520for%2520complex%2520illumination.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520HeadLighter%252C%2520a%2520novel%2520supervised%2520framework%2520that%2520learns%2520a%2520physically%2520plausible%2520decomposition%2520of%2520appearance%2520and%2520illumination%2520in%2520head%2520generative%2520models.%2520Specifically%252C%2520we%2520design%2520a%2520dual-branch%2520architecture%2520that%2520separately%2520models%2520lighting-invariant%2520head%2520attributes%2520and%2520physically%2520grounded%2520rendering%2520components.%2520A%2520progressive%2520disentanglement%2520training%2520is%2520employed%2520to%2520gradually%2520inject%2520head%2520appearance%2520priors%2520into%2520the%2520generative%2520architecture%252C%2520supervised%2520by%2520multi-view%2520images%2520captured%2520under%2520controlled%2520light%2520conditions%2520with%2520a%2520light%2520stage%2520setup.%2520We%2520further%2520introduce%2520a%2520distillation%2520strategy%2520to%2520generate%2520high-quality%2520normals%2520for%2520realistic%2520rendering.%2520Experiments%2520demonstrate%2520that%2520our%2520method%2520preserves%2520high-quality%2520generation%2520and%2520real-time%2520rendering%252C%2520while%2520simultaneously%2520supporting%2520explicit%2520lighting%2520and%2520viewpoint%2520editing.%2520We%2520will%2520publicly%2520release%2520our%2520code%2520and%2520dataset.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02103v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HeadLighter%3A%20Disentangling%20Illumination%20in%20Generative%203D%20Gaussian%20Heads%20via%20Lightstage%20Captures&entry.906535625=Yating%20Wang%20and%20Yuan%20Sun%20and%20Xuan%20Wang%20and%20Ran%20Yi%20and%20Boyao%20Zhou%20and%20Yipengjing%20Sun%20and%20Hongyu%20Liu%20and%20Yinuo%20Wang%20and%20Lizhuang%20Ma&entry.1292438233=Recent%203D-aware%20head%20generative%20models%20based%20on%203D%20Gaussian%20Splatting%20achieve%20real-time%2C%20photorealistic%20and%20view-consistent%20head%20synthesis.%20However%2C%20a%20fundamental%20limitation%20persists%3A%20the%20deep%20entanglement%20of%20illumination%20and%20intrinsic%20appearance%20prevents%20controllable%20relighting.%20Existing%20disentanglement%20methods%20rely%20on%20strong%20assumptions%20to%20enable%20weakly%20supervised%20learning%2C%20which%20restricts%20their%20capacity%20for%20complex%20illumination.%20To%20address%20this%20challenge%2C%20we%20introduce%20HeadLighter%2C%20a%20novel%20supervised%20framework%20that%20learns%20a%20physically%20plausible%20decomposition%20of%20appearance%20and%20illumination%20in%20head%20generative%20models.%20Specifically%2C%20we%20design%20a%20dual-branch%20architecture%20that%20separately%20models%20lighting-invariant%20head%20attributes%20and%20physically%20grounded%20rendering%20components.%20A%20progressive%20disentanglement%20training%20is%20employed%20to%20gradually%20inject%20head%20appearance%20priors%20into%20the%20generative%20architecture%2C%20supervised%20by%20multi-view%20images%20captured%20under%20controlled%20light%20conditions%20with%20a%20light%20stage%20setup.%20We%20further%20introduce%20a%20distillation%20strategy%20to%20generate%20high-quality%20normals%20for%20realistic%20rendering.%20Experiments%20demonstrate%20that%20our%20method%20preserves%20high-quality%20generation%20and%20real-time%20rendering%2C%20while%20simultaneously%20supporting%20explicit%20lighting%20and%20viewpoint%20editing.%20We%20will%20publicly%20release%20our%20code%20and%20dataset.&entry.1838667208=http%3A//arxiv.org/abs/2601.02103v1&entry.124074799=Read"},
{"title": "MCD-Net: A Lightweight Deep Learning Baseline for Optical-Only Moraine Segmentation", "author": "Zhehuan Cao and Fiseha Berhanu Tesema and Ping Fu and Jianfeng Ren and Ahmed Nasr", "abstract": "Glacial segmentation is essential for reconstructing past glacier dynamics and evaluating climate-driven landscape change. However, weak optical contrast and the limited availability of high-resolution DEMs hinder automated mapping. This study introduces the first large-scale optical-only moraine segmentation dataset, comprising 3,340 manually annotated high-resolution images from Google Earth covering glaciated regions of Sichuan and Yunnan, China. We develop MCD-Net, a lightweight baseline that integrates a MobileNetV2 encoder, a Convolutional Block Attention Module (CBAM), and a DeepLabV3+ decoder. Benchmarking against deeper backbones (ResNet152, Xception) shows that MCD-Net achieves 62.3\\% mean Intersection over Union (mIoU) and 72.8\\% Dice coefficient while reducing computational cost by more than 60\\%. Although ridge delineation remains constrained by sub-pixel width and spectral ambiguity, the results demonstrate that optical imagery alone can provide reliable moraine-body segmentation. The dataset and code are publicly available at https://github.com/Lyra-alpha/MCD-Net, establishing a reproducible benchmark for moraine-specific segmentation and offering a deployable baseline for high-altitude glacial monitoring.", "link": "http://arxiv.org/abs/2601.02091v1", "date": "2026-01-05", "relevancy": 2.4952, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5171}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4942}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MCD-Net%3A%20A%20Lightweight%20Deep%20Learning%20Baseline%20for%20Optical-Only%20Moraine%20Segmentation&body=Title%3A%20MCD-Net%3A%20A%20Lightweight%20Deep%20Learning%20Baseline%20for%20Optical-Only%20Moraine%20Segmentation%0AAuthor%3A%20Zhehuan%20Cao%20and%20Fiseha%20Berhanu%20Tesema%20and%20Ping%20Fu%20and%20Jianfeng%20Ren%20and%20Ahmed%20Nasr%0AAbstract%3A%20Glacial%20segmentation%20is%20essential%20for%20reconstructing%20past%20glacier%20dynamics%20and%20evaluating%20climate-driven%20landscape%20change.%20However%2C%20weak%20optical%20contrast%20and%20the%20limited%20availability%20of%20high-resolution%20DEMs%20hinder%20automated%20mapping.%20This%20study%20introduces%20the%20first%20large-scale%20optical-only%20moraine%20segmentation%20dataset%2C%20comprising%203%2C340%20manually%20annotated%20high-resolution%20images%20from%20Google%20Earth%20covering%20glaciated%20regions%20of%20Sichuan%20and%20Yunnan%2C%20China.%20We%20develop%20MCD-Net%2C%20a%20lightweight%20baseline%20that%20integrates%20a%20MobileNetV2%20encoder%2C%20a%20Convolutional%20Block%20Attention%20Module%20%28CBAM%29%2C%20and%20a%20DeepLabV3%2B%20decoder.%20Benchmarking%20against%20deeper%20backbones%20%28ResNet152%2C%20Xception%29%20shows%20that%20MCD-Net%20achieves%2062.3%5C%25%20mean%20Intersection%20over%20Union%20%28mIoU%29%20and%2072.8%5C%25%20Dice%20coefficient%20while%20reducing%20computational%20cost%20by%20more%20than%2060%5C%25.%20Although%20ridge%20delineation%20remains%20constrained%20by%20sub-pixel%20width%20and%20spectral%20ambiguity%2C%20the%20results%20demonstrate%20that%20optical%20imagery%20alone%20can%20provide%20reliable%20moraine-body%20segmentation.%20The%20dataset%20and%20code%20are%20publicly%20available%20at%20https%3A//github.com/Lyra-alpha/MCD-Net%2C%20establishing%20a%20reproducible%20benchmark%20for%20moraine-specific%20segmentation%20and%20offering%20a%20deployable%20baseline%20for%20high-altitude%20glacial%20monitoring.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02091v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMCD-Net%253A%2520A%2520Lightweight%2520Deep%2520Learning%2520Baseline%2520for%2520Optical-Only%2520Moraine%2520Segmentation%26entry.906535625%3DZhehuan%2520Cao%2520and%2520Fiseha%2520Berhanu%2520Tesema%2520and%2520Ping%2520Fu%2520and%2520Jianfeng%2520Ren%2520and%2520Ahmed%2520Nasr%26entry.1292438233%3DGlacial%2520segmentation%2520is%2520essential%2520for%2520reconstructing%2520past%2520glacier%2520dynamics%2520and%2520evaluating%2520climate-driven%2520landscape%2520change.%2520However%252C%2520weak%2520optical%2520contrast%2520and%2520the%2520limited%2520availability%2520of%2520high-resolution%2520DEMs%2520hinder%2520automated%2520mapping.%2520This%2520study%2520introduces%2520the%2520first%2520large-scale%2520optical-only%2520moraine%2520segmentation%2520dataset%252C%2520comprising%25203%252C340%2520manually%2520annotated%2520high-resolution%2520images%2520from%2520Google%2520Earth%2520covering%2520glaciated%2520regions%2520of%2520Sichuan%2520and%2520Yunnan%252C%2520China.%2520We%2520develop%2520MCD-Net%252C%2520a%2520lightweight%2520baseline%2520that%2520integrates%2520a%2520MobileNetV2%2520encoder%252C%2520a%2520Convolutional%2520Block%2520Attention%2520Module%2520%2528CBAM%2529%252C%2520and%2520a%2520DeepLabV3%252B%2520decoder.%2520Benchmarking%2520against%2520deeper%2520backbones%2520%2528ResNet152%252C%2520Xception%2529%2520shows%2520that%2520MCD-Net%2520achieves%252062.3%255C%2525%2520mean%2520Intersection%2520over%2520Union%2520%2528mIoU%2529%2520and%252072.8%255C%2525%2520Dice%2520coefficient%2520while%2520reducing%2520computational%2520cost%2520by%2520more%2520than%252060%255C%2525.%2520Although%2520ridge%2520delineation%2520remains%2520constrained%2520by%2520sub-pixel%2520width%2520and%2520spectral%2520ambiguity%252C%2520the%2520results%2520demonstrate%2520that%2520optical%2520imagery%2520alone%2520can%2520provide%2520reliable%2520moraine-body%2520segmentation.%2520The%2520dataset%2520and%2520code%2520are%2520publicly%2520available%2520at%2520https%253A//github.com/Lyra-alpha/MCD-Net%252C%2520establishing%2520a%2520reproducible%2520benchmark%2520for%2520moraine-specific%2520segmentation%2520and%2520offering%2520a%2520deployable%2520baseline%2520for%2520high-altitude%2520glacial%2520monitoring.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02091v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCD-Net%3A%20A%20Lightweight%20Deep%20Learning%20Baseline%20for%20Optical-Only%20Moraine%20Segmentation&entry.906535625=Zhehuan%20Cao%20and%20Fiseha%20Berhanu%20Tesema%20and%20Ping%20Fu%20and%20Jianfeng%20Ren%20and%20Ahmed%20Nasr&entry.1292438233=Glacial%20segmentation%20is%20essential%20for%20reconstructing%20past%20glacier%20dynamics%20and%20evaluating%20climate-driven%20landscape%20change.%20However%2C%20weak%20optical%20contrast%20and%20the%20limited%20availability%20of%20high-resolution%20DEMs%20hinder%20automated%20mapping.%20This%20study%20introduces%20the%20first%20large-scale%20optical-only%20moraine%20segmentation%20dataset%2C%20comprising%203%2C340%20manually%20annotated%20high-resolution%20images%20from%20Google%20Earth%20covering%20glaciated%20regions%20of%20Sichuan%20and%20Yunnan%2C%20China.%20We%20develop%20MCD-Net%2C%20a%20lightweight%20baseline%20that%20integrates%20a%20MobileNetV2%20encoder%2C%20a%20Convolutional%20Block%20Attention%20Module%20%28CBAM%29%2C%20and%20a%20DeepLabV3%2B%20decoder.%20Benchmarking%20against%20deeper%20backbones%20%28ResNet152%2C%20Xception%29%20shows%20that%20MCD-Net%20achieves%2062.3%5C%25%20mean%20Intersection%20over%20Union%20%28mIoU%29%20and%2072.8%5C%25%20Dice%20coefficient%20while%20reducing%20computational%20cost%20by%20more%20than%2060%5C%25.%20Although%20ridge%20delineation%20remains%20constrained%20by%20sub-pixel%20width%20and%20spectral%20ambiguity%2C%20the%20results%20demonstrate%20that%20optical%20imagery%20alone%20can%20provide%20reliable%20moraine-body%20segmentation.%20The%20dataset%20and%20code%20are%20publicly%20available%20at%20https%3A//github.com/Lyra-alpha/MCD-Net%2C%20establishing%20a%20reproducible%20benchmark%20for%20moraine-specific%20segmentation%20and%20offering%20a%20deployable%20baseline%20for%20high-altitude%20glacial%20monitoring.&entry.1838667208=http%3A//arxiv.org/abs/2601.02091v1&entry.124074799=Read"},
{"title": "Dancing Points: Synthesizing Ballroom Dancing with Three-Point Inputs", "author": "Peizhuo Li and Sebastian Starke and Yuting Ye and Olga Sorkine-Hornung", "abstract": "Ballroom dancing is a structured yet expressive motion category. Its highly diverse movement and complex interactions between leader and follower dancers make the understanding and synthesis challenging. We demonstrate that the three-point trajectory available from a virtual reality (VR) device can effectively serve as a dancer's motion descriptor, simplifying the modeling and synthesis of interplay between dancers' full-body motions down to sparse trajectories. Thanks to the low dimensionality, we can employ an efficient MLP network to predict the follower's three-point trajectory directly from the leader's three-point input for certain types of ballroom dancing, addressing the challenge of modeling high-dimensional full-body interaction. It also prevents our method from overfitting thanks to its compact yet explicit representation. By leveraging the inherent structure of the movements and carefully planning the autoregressive procedure, we show a deterministic neural network is able to translate three-point trajectories into a virtual embodied avatar, which is typically considered under-constrained and requires generative models for common motions. In addition, we demonstrate this deterministic approach generalizes beyond small, structured datasets like ballroom dancing, and performs robustly on larger, more diverse datasets such as LaFAN. Our method provides a computationally- and data-efficient solution, opening new possibilities for immersive paired dancing applications. Code and pre-trained models for this paper are available at https://peizhuoli.github.io/dancing-points.", "link": "http://arxiv.org/abs/2601.02096v1", "date": "2026-01-05", "relevancy": 2.4935, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5834}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dancing%20Points%3A%20Synthesizing%20Ballroom%20Dancing%20with%20Three-Point%20Inputs&body=Title%3A%20Dancing%20Points%3A%20Synthesizing%20Ballroom%20Dancing%20with%20Three-Point%20Inputs%0AAuthor%3A%20Peizhuo%20Li%20and%20Sebastian%20Starke%20and%20Yuting%20Ye%20and%20Olga%20Sorkine-Hornung%0AAbstract%3A%20Ballroom%20dancing%20is%20a%20structured%20yet%20expressive%20motion%20category.%20Its%20highly%20diverse%20movement%20and%20complex%20interactions%20between%20leader%20and%20follower%20dancers%20make%20the%20understanding%20and%20synthesis%20challenging.%20We%20demonstrate%20that%20the%20three-point%20trajectory%20available%20from%20a%20virtual%20reality%20%28VR%29%20device%20can%20effectively%20serve%20as%20a%20dancer%27s%20motion%20descriptor%2C%20simplifying%20the%20modeling%20and%20synthesis%20of%20interplay%20between%20dancers%27%20full-body%20motions%20down%20to%20sparse%20trajectories.%20Thanks%20to%20the%20low%20dimensionality%2C%20we%20can%20employ%20an%20efficient%20MLP%20network%20to%20predict%20the%20follower%27s%20three-point%20trajectory%20directly%20from%20the%20leader%27s%20three-point%20input%20for%20certain%20types%20of%20ballroom%20dancing%2C%20addressing%20the%20challenge%20of%20modeling%20high-dimensional%20full-body%20interaction.%20It%20also%20prevents%20our%20method%20from%20overfitting%20thanks%20to%20its%20compact%20yet%20explicit%20representation.%20By%20leveraging%20the%20inherent%20structure%20of%20the%20movements%20and%20carefully%20planning%20the%20autoregressive%20procedure%2C%20we%20show%20a%20deterministic%20neural%20network%20is%20able%20to%20translate%20three-point%20trajectories%20into%20a%20virtual%20embodied%20avatar%2C%20which%20is%20typically%20considered%20under-constrained%20and%20requires%20generative%20models%20for%20common%20motions.%20In%20addition%2C%20we%20demonstrate%20this%20deterministic%20approach%20generalizes%20beyond%20small%2C%20structured%20datasets%20like%20ballroom%20dancing%2C%20and%20performs%20robustly%20on%20larger%2C%20more%20diverse%20datasets%20such%20as%20LaFAN.%20Our%20method%20provides%20a%20computationally-%20and%20data-efficient%20solution%2C%20opening%20new%20possibilities%20for%20immersive%20paired%20dancing%20applications.%20Code%20and%20pre-trained%20models%20for%20this%20paper%20are%20available%20at%20https%3A//peizhuoli.github.io/dancing-points.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02096v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDancing%2520Points%253A%2520Synthesizing%2520Ballroom%2520Dancing%2520with%2520Three-Point%2520Inputs%26entry.906535625%3DPeizhuo%2520Li%2520and%2520Sebastian%2520Starke%2520and%2520Yuting%2520Ye%2520and%2520Olga%2520Sorkine-Hornung%26entry.1292438233%3DBallroom%2520dancing%2520is%2520a%2520structured%2520yet%2520expressive%2520motion%2520category.%2520Its%2520highly%2520diverse%2520movement%2520and%2520complex%2520interactions%2520between%2520leader%2520and%2520follower%2520dancers%2520make%2520the%2520understanding%2520and%2520synthesis%2520challenging.%2520We%2520demonstrate%2520that%2520the%2520three-point%2520trajectory%2520available%2520from%2520a%2520virtual%2520reality%2520%2528VR%2529%2520device%2520can%2520effectively%2520serve%2520as%2520a%2520dancer%2527s%2520motion%2520descriptor%252C%2520simplifying%2520the%2520modeling%2520and%2520synthesis%2520of%2520interplay%2520between%2520dancers%2527%2520full-body%2520motions%2520down%2520to%2520sparse%2520trajectories.%2520Thanks%2520to%2520the%2520low%2520dimensionality%252C%2520we%2520can%2520employ%2520an%2520efficient%2520MLP%2520network%2520to%2520predict%2520the%2520follower%2527s%2520three-point%2520trajectory%2520directly%2520from%2520the%2520leader%2527s%2520three-point%2520input%2520for%2520certain%2520types%2520of%2520ballroom%2520dancing%252C%2520addressing%2520the%2520challenge%2520of%2520modeling%2520high-dimensional%2520full-body%2520interaction.%2520It%2520also%2520prevents%2520our%2520method%2520from%2520overfitting%2520thanks%2520to%2520its%2520compact%2520yet%2520explicit%2520representation.%2520By%2520leveraging%2520the%2520inherent%2520structure%2520of%2520the%2520movements%2520and%2520carefully%2520planning%2520the%2520autoregressive%2520procedure%252C%2520we%2520show%2520a%2520deterministic%2520neural%2520network%2520is%2520able%2520to%2520translate%2520three-point%2520trajectories%2520into%2520a%2520virtual%2520embodied%2520avatar%252C%2520which%2520is%2520typically%2520considered%2520under-constrained%2520and%2520requires%2520generative%2520models%2520for%2520common%2520motions.%2520In%2520addition%252C%2520we%2520demonstrate%2520this%2520deterministic%2520approach%2520generalizes%2520beyond%2520small%252C%2520structured%2520datasets%2520like%2520ballroom%2520dancing%252C%2520and%2520performs%2520robustly%2520on%2520larger%252C%2520more%2520diverse%2520datasets%2520such%2520as%2520LaFAN.%2520Our%2520method%2520provides%2520a%2520computationally-%2520and%2520data-efficient%2520solution%252C%2520opening%2520new%2520possibilities%2520for%2520immersive%2520paired%2520dancing%2520applications.%2520Code%2520and%2520pre-trained%2520models%2520for%2520this%2520paper%2520are%2520available%2520at%2520https%253A//peizhuoli.github.io/dancing-points.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02096v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dancing%20Points%3A%20Synthesizing%20Ballroom%20Dancing%20with%20Three-Point%20Inputs&entry.906535625=Peizhuo%20Li%20and%20Sebastian%20Starke%20and%20Yuting%20Ye%20and%20Olga%20Sorkine-Hornung&entry.1292438233=Ballroom%20dancing%20is%20a%20structured%20yet%20expressive%20motion%20category.%20Its%20highly%20diverse%20movement%20and%20complex%20interactions%20between%20leader%20and%20follower%20dancers%20make%20the%20understanding%20and%20synthesis%20challenging.%20We%20demonstrate%20that%20the%20three-point%20trajectory%20available%20from%20a%20virtual%20reality%20%28VR%29%20device%20can%20effectively%20serve%20as%20a%20dancer%27s%20motion%20descriptor%2C%20simplifying%20the%20modeling%20and%20synthesis%20of%20interplay%20between%20dancers%27%20full-body%20motions%20down%20to%20sparse%20trajectories.%20Thanks%20to%20the%20low%20dimensionality%2C%20we%20can%20employ%20an%20efficient%20MLP%20network%20to%20predict%20the%20follower%27s%20three-point%20trajectory%20directly%20from%20the%20leader%27s%20three-point%20input%20for%20certain%20types%20of%20ballroom%20dancing%2C%20addressing%20the%20challenge%20of%20modeling%20high-dimensional%20full-body%20interaction.%20It%20also%20prevents%20our%20method%20from%20overfitting%20thanks%20to%20its%20compact%20yet%20explicit%20representation.%20By%20leveraging%20the%20inherent%20structure%20of%20the%20movements%20and%20carefully%20planning%20the%20autoregressive%20procedure%2C%20we%20show%20a%20deterministic%20neural%20network%20is%20able%20to%20translate%20three-point%20trajectories%20into%20a%20virtual%20embodied%20avatar%2C%20which%20is%20typically%20considered%20under-constrained%20and%20requires%20generative%20models%20for%20common%20motions.%20In%20addition%2C%20we%20demonstrate%20this%20deterministic%20approach%20generalizes%20beyond%20small%2C%20structured%20datasets%20like%20ballroom%20dancing%2C%20and%20performs%20robustly%20on%20larger%2C%20more%20diverse%20datasets%20such%20as%20LaFAN.%20Our%20method%20provides%20a%20computationally-%20and%20data-efficient%20solution%2C%20opening%20new%20possibilities%20for%20immersive%20paired%20dancing%20applications.%20Code%20and%20pre-trained%20models%20for%20this%20paper%20are%20available%20at%20https%3A//peizhuoli.github.io/dancing-points.&entry.1838667208=http%3A//arxiv.org/abs/2601.02096v1&entry.124074799=Read"},
{"title": "A Universal and Robust Framework for Multiple Gas Recognition Based-on Spherical Normalization-Coupled Mahalanobis Algorithm", "author": "Shuai Chen and Yang Song and Chen Wang and Ziran Wang", "abstract": "Electronic nose (E-nose) systems face two interconnected challenges in open-set gas recognition: feature distribution shift caused by signal drift and decision boundary failure induced by unknown gas interference. Existing methods predominantly rely on Euclidean distance or conventional classifiers, failing to account for anisotropic feature distributions and dynamic signal intensity variations. To address these issues, this study proposes the Spherical Normalization coupled Mahalanobis (SNM) module, a universal post-processing module for open-set gas recognition. First, it achieves geometric decoupling through cascaded batch and L2 normalization, projecting features onto a unit hypersphere to eliminate signal intensity fluctuations. Second, it utilizes Mahalanobis distance to construct adaptive ellipsoidal decision boundaries that conform to the anisotropic feature geometry. The architecture-agnostic SNM-Module seamlessly integrates with mainstream backbones including Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), and Transformer. Experiments on the public Vergara dataset demonstrate that the Transformer+SNM configuration achieves near-theoretical-limit performance in discriminating among multiple target gases, with an AUROC of 0.9977 and an unknown gas detection rate of 99.57% at 5% false positive rate, significantly outperforming state-of-the-art methods with a 3.0% AUROC improvement and 91.0% standard deviation reduction compared to Class Anchor Clustering (CAC). The module maintains exceptional robustness across five sensor positions, with standard deviations below 0.0028. This work effectively addresses the critical challenge of simultaneously achieving high accuracy and high stability in open-set gas recognition, providing solid support for industrial E-nose deployment.", "link": "http://arxiv.org/abs/2512.22792v2", "date": "2026-01-05", "relevancy": 2.4463, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4914}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4886}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Universal%20and%20Robust%20Framework%20for%20Multiple%20Gas%20Recognition%20Based-on%20Spherical%20Normalization-Coupled%20Mahalanobis%20Algorithm&body=Title%3A%20A%20Universal%20and%20Robust%20Framework%20for%20Multiple%20Gas%20Recognition%20Based-on%20Spherical%20Normalization-Coupled%20Mahalanobis%20Algorithm%0AAuthor%3A%20Shuai%20Chen%20and%20Yang%20Song%20and%20Chen%20Wang%20and%20Ziran%20Wang%0AAbstract%3A%20Electronic%20nose%20%28E-nose%29%20systems%20face%20two%20interconnected%20challenges%20in%20open-set%20gas%20recognition%3A%20feature%20distribution%20shift%20caused%20by%20signal%20drift%20and%20decision%20boundary%20failure%20induced%20by%20unknown%20gas%20interference.%20Existing%20methods%20predominantly%20rely%20on%20Euclidean%20distance%20or%20conventional%20classifiers%2C%20failing%20to%20account%20for%20anisotropic%20feature%20distributions%20and%20dynamic%20signal%20intensity%20variations.%20To%20address%20these%20issues%2C%20this%20study%20proposes%20the%20Spherical%20Normalization%20coupled%20Mahalanobis%20%28SNM%29%20module%2C%20a%20universal%20post-processing%20module%20for%20open-set%20gas%20recognition.%20First%2C%20it%20achieves%20geometric%20decoupling%20through%20cascaded%20batch%20and%20L2%20normalization%2C%20projecting%20features%20onto%20a%20unit%20hypersphere%20to%20eliminate%20signal%20intensity%20fluctuations.%20Second%2C%20it%20utilizes%20Mahalanobis%20distance%20to%20construct%20adaptive%20ellipsoidal%20decision%20boundaries%20that%20conform%20to%20the%20anisotropic%20feature%20geometry.%20The%20architecture-agnostic%20SNM-Module%20seamlessly%20integrates%20with%20mainstream%20backbones%20including%20Convolutional%20Neural%20Network%20%28CNN%29%2C%20Recurrent%20Neural%20Network%20%28RNN%29%2C%20and%20Transformer.%20Experiments%20on%20the%20public%20Vergara%20dataset%20demonstrate%20that%20the%20Transformer%2BSNM%20configuration%20achieves%20near-theoretical-limit%20performance%20in%20discriminating%20among%20multiple%20target%20gases%2C%20with%20an%20AUROC%20of%200.9977%20and%20an%20unknown%20gas%20detection%20rate%20of%2099.57%25%20at%205%25%20false%20positive%20rate%2C%20significantly%20outperforming%20state-of-the-art%20methods%20with%20a%203.0%25%20AUROC%20improvement%20and%2091.0%25%20standard%20deviation%20reduction%20compared%20to%20Class%20Anchor%20Clustering%20%28CAC%29.%20The%20module%20maintains%20exceptional%20robustness%20across%20five%20sensor%20positions%2C%20with%20standard%20deviations%20below%200.0028.%20This%20work%20effectively%20addresses%20the%20critical%20challenge%20of%20simultaneously%20achieving%20high%20accuracy%20and%20high%20stability%20in%20open-set%20gas%20recognition%2C%20providing%20solid%20support%20for%20industrial%20E-nose%20deployment.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22792v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Universal%2520and%2520Robust%2520Framework%2520for%2520Multiple%2520Gas%2520Recognition%2520Based-on%2520Spherical%2520Normalization-Coupled%2520Mahalanobis%2520Algorithm%26entry.906535625%3DShuai%2520Chen%2520and%2520Yang%2520Song%2520and%2520Chen%2520Wang%2520and%2520Ziran%2520Wang%26entry.1292438233%3DElectronic%2520nose%2520%2528E-nose%2529%2520systems%2520face%2520two%2520interconnected%2520challenges%2520in%2520open-set%2520gas%2520recognition%253A%2520feature%2520distribution%2520shift%2520caused%2520by%2520signal%2520drift%2520and%2520decision%2520boundary%2520failure%2520induced%2520by%2520unknown%2520gas%2520interference.%2520Existing%2520methods%2520predominantly%2520rely%2520on%2520Euclidean%2520distance%2520or%2520conventional%2520classifiers%252C%2520failing%2520to%2520account%2520for%2520anisotropic%2520feature%2520distributions%2520and%2520dynamic%2520signal%2520intensity%2520variations.%2520To%2520address%2520these%2520issues%252C%2520this%2520study%2520proposes%2520the%2520Spherical%2520Normalization%2520coupled%2520Mahalanobis%2520%2528SNM%2529%2520module%252C%2520a%2520universal%2520post-processing%2520module%2520for%2520open-set%2520gas%2520recognition.%2520First%252C%2520it%2520achieves%2520geometric%2520decoupling%2520through%2520cascaded%2520batch%2520and%2520L2%2520normalization%252C%2520projecting%2520features%2520onto%2520a%2520unit%2520hypersphere%2520to%2520eliminate%2520signal%2520intensity%2520fluctuations.%2520Second%252C%2520it%2520utilizes%2520Mahalanobis%2520distance%2520to%2520construct%2520adaptive%2520ellipsoidal%2520decision%2520boundaries%2520that%2520conform%2520to%2520the%2520anisotropic%2520feature%2520geometry.%2520The%2520architecture-agnostic%2520SNM-Module%2520seamlessly%2520integrates%2520with%2520mainstream%2520backbones%2520including%2520Convolutional%2520Neural%2520Network%2520%2528CNN%2529%252C%2520Recurrent%2520Neural%2520Network%2520%2528RNN%2529%252C%2520and%2520Transformer.%2520Experiments%2520on%2520the%2520public%2520Vergara%2520dataset%2520demonstrate%2520that%2520the%2520Transformer%252BSNM%2520configuration%2520achieves%2520near-theoretical-limit%2520performance%2520in%2520discriminating%2520among%2520multiple%2520target%2520gases%252C%2520with%2520an%2520AUROC%2520of%25200.9977%2520and%2520an%2520unknown%2520gas%2520detection%2520rate%2520of%252099.57%2525%2520at%25205%2525%2520false%2520positive%2520rate%252C%2520significantly%2520outperforming%2520state-of-the-art%2520methods%2520with%2520a%25203.0%2525%2520AUROC%2520improvement%2520and%252091.0%2525%2520standard%2520deviation%2520reduction%2520compared%2520to%2520Class%2520Anchor%2520Clustering%2520%2528CAC%2529.%2520The%2520module%2520maintains%2520exceptional%2520robustness%2520across%2520five%2520sensor%2520positions%252C%2520with%2520standard%2520deviations%2520below%25200.0028.%2520This%2520work%2520effectively%2520addresses%2520the%2520critical%2520challenge%2520of%2520simultaneously%2520achieving%2520high%2520accuracy%2520and%2520high%2520stability%2520in%2520open-set%2520gas%2520recognition%252C%2520providing%2520solid%2520support%2520for%2520industrial%2520E-nose%2520deployment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22792v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Universal%20and%20Robust%20Framework%20for%20Multiple%20Gas%20Recognition%20Based-on%20Spherical%20Normalization-Coupled%20Mahalanobis%20Algorithm&entry.906535625=Shuai%20Chen%20and%20Yang%20Song%20and%20Chen%20Wang%20and%20Ziran%20Wang&entry.1292438233=Electronic%20nose%20%28E-nose%29%20systems%20face%20two%20interconnected%20challenges%20in%20open-set%20gas%20recognition%3A%20feature%20distribution%20shift%20caused%20by%20signal%20drift%20and%20decision%20boundary%20failure%20induced%20by%20unknown%20gas%20interference.%20Existing%20methods%20predominantly%20rely%20on%20Euclidean%20distance%20or%20conventional%20classifiers%2C%20failing%20to%20account%20for%20anisotropic%20feature%20distributions%20and%20dynamic%20signal%20intensity%20variations.%20To%20address%20these%20issues%2C%20this%20study%20proposes%20the%20Spherical%20Normalization%20coupled%20Mahalanobis%20%28SNM%29%20module%2C%20a%20universal%20post-processing%20module%20for%20open-set%20gas%20recognition.%20First%2C%20it%20achieves%20geometric%20decoupling%20through%20cascaded%20batch%20and%20L2%20normalization%2C%20projecting%20features%20onto%20a%20unit%20hypersphere%20to%20eliminate%20signal%20intensity%20fluctuations.%20Second%2C%20it%20utilizes%20Mahalanobis%20distance%20to%20construct%20adaptive%20ellipsoidal%20decision%20boundaries%20that%20conform%20to%20the%20anisotropic%20feature%20geometry.%20The%20architecture-agnostic%20SNM-Module%20seamlessly%20integrates%20with%20mainstream%20backbones%20including%20Convolutional%20Neural%20Network%20%28CNN%29%2C%20Recurrent%20Neural%20Network%20%28RNN%29%2C%20and%20Transformer.%20Experiments%20on%20the%20public%20Vergara%20dataset%20demonstrate%20that%20the%20Transformer%2BSNM%20configuration%20achieves%20near-theoretical-limit%20performance%20in%20discriminating%20among%20multiple%20target%20gases%2C%20with%20an%20AUROC%20of%200.9977%20and%20an%20unknown%20gas%20detection%20rate%20of%2099.57%25%20at%205%25%20false%20positive%20rate%2C%20significantly%20outperforming%20state-of-the-art%20methods%20with%20a%203.0%25%20AUROC%20improvement%20and%2091.0%25%20standard%20deviation%20reduction%20compared%20to%20Class%20Anchor%20Clustering%20%28CAC%29.%20The%20module%20maintains%20exceptional%20robustness%20across%20five%20sensor%20positions%2C%20with%20standard%20deviations%20below%200.0028.%20This%20work%20effectively%20addresses%20the%20critical%20challenge%20of%20simultaneously%20achieving%20high%20accuracy%20and%20high%20stability%20in%20open-set%20gas%20recognition%2C%20providing%20solid%20support%20for%20industrial%20E-nose%20deployment.&entry.1838667208=http%3A//arxiv.org/abs/2512.22792v2&entry.124074799=Read"},
{"title": "VINO: A Unified Visual Generator with Interleaved OmniModal Context", "author": "Junyi Chen and Tong He and Zhoujie Fu and Pengfei Wan and Kun Gai and Weicai Ye", "abstract": "We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.", "link": "http://arxiv.org/abs/2601.02358v1", "date": "2026-01-05", "relevancy": 2.4394, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6513}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5862}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VINO%3A%20A%20Unified%20Visual%20Generator%20with%20Interleaved%20OmniModal%20Context&body=Title%3A%20VINO%3A%20A%20Unified%20Visual%20Generator%20with%20Interleaved%20OmniModal%20Context%0AAuthor%3A%20Junyi%20Chen%20and%20Tong%20He%20and%20Zhoujie%20Fu%20and%20Pengfei%20Wan%20and%20Kun%20Gai%20and%20Weicai%20Ye%0AAbstract%3A%20We%20present%20VINO%2C%20a%20unified%20visual%20generator%20that%20performs%20image%20and%20video%20generation%20and%20editing%20within%20a%20single%20framework.%20Instead%20of%20relying%20on%20task-specific%20models%20or%20independent%20modules%20for%20each%20modality%2C%20VINO%20uses%20a%20shared%20diffusion%20backbone%20that%20conditions%20on%20text%2C%20images%20and%20videos%2C%20enabling%20a%20broad%20range%20of%20visual%20creation%20and%20editing%20tasks%20under%20one%20model.%20Specifically%2C%20VINO%20couples%20a%20vision-language%20model%20%28VLM%29%20with%20a%20Multimodal%20Diffusion%20Transformer%20%28MMDiT%29%2C%20where%20multimodal%20inputs%20are%20encoded%20as%20interleaved%20conditioning%20tokens%2C%20and%20then%20used%20to%20guide%20the%20diffusion%20process.%20This%20design%20supports%20multi-reference%20grounding%2C%20long-form%20instruction%20following%2C%20and%20coherent%20identity%20preservation%20across%20static%20and%20dynamic%20content%2C%20while%20avoiding%20modality-specific%20architectural%20components.%20To%20train%20such%20a%20unified%20system%2C%20we%20introduce%20a%20multi-stage%20training%20pipeline%20that%20progressively%20expands%20a%20video%20generation%20base%20model%20into%20a%20unified%2C%20multi-task%20generator%20capable%20of%20both%20image%20and%20video%20input%20and%20output.%20Across%20diverse%20generation%20and%20editing%20benchmarks%2C%20VINO%20demonstrates%20strong%20visual%20quality%2C%20faithful%20instruction%20following%2C%20improved%20reference%20and%20attribute%20preservation%2C%20and%20more%20controllable%20multi-identity%20edits.%20Our%20results%20highlight%20a%20practical%20path%20toward%20scalable%20unified%20visual%20generation%2C%20and%20the%20promise%20of%20interleaved%2C%20in-context%20computation%20as%20a%20foundation%20for%20general-purpose%20visual%20creation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02358v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVINO%253A%2520A%2520Unified%2520Visual%2520Generator%2520with%2520Interleaved%2520OmniModal%2520Context%26entry.906535625%3DJunyi%2520Chen%2520and%2520Tong%2520He%2520and%2520Zhoujie%2520Fu%2520and%2520Pengfei%2520Wan%2520and%2520Kun%2520Gai%2520and%2520Weicai%2520Ye%26entry.1292438233%3DWe%2520present%2520VINO%252C%2520a%2520unified%2520visual%2520generator%2520that%2520performs%2520image%2520and%2520video%2520generation%2520and%2520editing%2520within%2520a%2520single%2520framework.%2520Instead%2520of%2520relying%2520on%2520task-specific%2520models%2520or%2520independent%2520modules%2520for%2520each%2520modality%252C%2520VINO%2520uses%2520a%2520shared%2520diffusion%2520backbone%2520that%2520conditions%2520on%2520text%252C%2520images%2520and%2520videos%252C%2520enabling%2520a%2520broad%2520range%2520of%2520visual%2520creation%2520and%2520editing%2520tasks%2520under%2520one%2520model.%2520Specifically%252C%2520VINO%2520couples%2520a%2520vision-language%2520model%2520%2528VLM%2529%2520with%2520a%2520Multimodal%2520Diffusion%2520Transformer%2520%2528MMDiT%2529%252C%2520where%2520multimodal%2520inputs%2520are%2520encoded%2520as%2520interleaved%2520conditioning%2520tokens%252C%2520and%2520then%2520used%2520to%2520guide%2520the%2520diffusion%2520process.%2520This%2520design%2520supports%2520multi-reference%2520grounding%252C%2520long-form%2520instruction%2520following%252C%2520and%2520coherent%2520identity%2520preservation%2520across%2520static%2520and%2520dynamic%2520content%252C%2520while%2520avoiding%2520modality-specific%2520architectural%2520components.%2520To%2520train%2520such%2520a%2520unified%2520system%252C%2520we%2520introduce%2520a%2520multi-stage%2520training%2520pipeline%2520that%2520progressively%2520expands%2520a%2520video%2520generation%2520base%2520model%2520into%2520a%2520unified%252C%2520multi-task%2520generator%2520capable%2520of%2520both%2520image%2520and%2520video%2520input%2520and%2520output.%2520Across%2520diverse%2520generation%2520and%2520editing%2520benchmarks%252C%2520VINO%2520demonstrates%2520strong%2520visual%2520quality%252C%2520faithful%2520instruction%2520following%252C%2520improved%2520reference%2520and%2520attribute%2520preservation%252C%2520and%2520more%2520controllable%2520multi-identity%2520edits.%2520Our%2520results%2520highlight%2520a%2520practical%2520path%2520toward%2520scalable%2520unified%2520visual%2520generation%252C%2520and%2520the%2520promise%2520of%2520interleaved%252C%2520in-context%2520computation%2520as%2520a%2520foundation%2520for%2520general-purpose%2520visual%2520creation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02358v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VINO%3A%20A%20Unified%20Visual%20Generator%20with%20Interleaved%20OmniModal%20Context&entry.906535625=Junyi%20Chen%20and%20Tong%20He%20and%20Zhoujie%20Fu%20and%20Pengfei%20Wan%20and%20Kun%20Gai%20and%20Weicai%20Ye&entry.1292438233=We%20present%20VINO%2C%20a%20unified%20visual%20generator%20that%20performs%20image%20and%20video%20generation%20and%20editing%20within%20a%20single%20framework.%20Instead%20of%20relying%20on%20task-specific%20models%20or%20independent%20modules%20for%20each%20modality%2C%20VINO%20uses%20a%20shared%20diffusion%20backbone%20that%20conditions%20on%20text%2C%20images%20and%20videos%2C%20enabling%20a%20broad%20range%20of%20visual%20creation%20and%20editing%20tasks%20under%20one%20model.%20Specifically%2C%20VINO%20couples%20a%20vision-language%20model%20%28VLM%29%20with%20a%20Multimodal%20Diffusion%20Transformer%20%28MMDiT%29%2C%20where%20multimodal%20inputs%20are%20encoded%20as%20interleaved%20conditioning%20tokens%2C%20and%20then%20used%20to%20guide%20the%20diffusion%20process.%20This%20design%20supports%20multi-reference%20grounding%2C%20long-form%20instruction%20following%2C%20and%20coherent%20identity%20preservation%20across%20static%20and%20dynamic%20content%2C%20while%20avoiding%20modality-specific%20architectural%20components.%20To%20train%20such%20a%20unified%20system%2C%20we%20introduce%20a%20multi-stage%20training%20pipeline%20that%20progressively%20expands%20a%20video%20generation%20base%20model%20into%20a%20unified%2C%20multi-task%20generator%20capable%20of%20both%20image%20and%20video%20input%20and%20output.%20Across%20diverse%20generation%20and%20editing%20benchmarks%2C%20VINO%20demonstrates%20strong%20visual%20quality%2C%20faithful%20instruction%20following%2C%20improved%20reference%20and%20attribute%20preservation%2C%20and%20more%20controllable%20multi-identity%20edits.%20Our%20results%20highlight%20a%20practical%20path%20toward%20scalable%20unified%20visual%20generation%2C%20and%20the%20promise%20of%20interleaved%2C%20in-context%20computation%20as%20a%20foundation%20for%20general-purpose%20visual%20creation.&entry.1838667208=http%3A//arxiv.org/abs/2601.02358v1&entry.124074799=Read"},
{"title": "Explore the Ideology of Deep Learning in ENSO Forecasts", "author": "Yanhai Gan and Yipeng Chen and Ning Li and Xingguo Liu and Junyu Dong and Xianyao Chen", "abstract": "The El Ni{~n}o-Southern Oscillation (ENSO) exerts profound influence on global climate variability, yet its prediction remains a grand challenge. Recent advances in deep learning have significantly improved forecasting skill, but the opacity of these models hampers scientific trust and operational deployment. Here, we introduce a mathematically grounded interpretability framework based on bounded variation function. By rescuing the \"dead\" neurons from the saturation zone of the activation function, we enhance the model's expressive capacity. Our analysis reveals that ENSO predictability emerges dominantly from the tropical Pacific, with contributions from the Indian and Atlantic Oceans, consistent with physical understanding. Controlled experiments affirm the robustness of our method and its alignment with established predictors. Notably, we probe the persistent Spring Predictability Barrier (SPB), finding that despite expanded sensitivity during spring, predictive performance declines-likely due to suboptimal variable selection. These results suggest that incorporating additional ocean-atmosphere variables may help transcend SPB limitations and advance long-range ENSO prediction.", "link": "http://arxiv.org/abs/2601.02050v1", "date": "2026-01-05", "relevancy": 2.4382, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4954}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4838}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explore%20the%20Ideology%20of%20Deep%20Learning%20in%20ENSO%20Forecasts&body=Title%3A%20Explore%20the%20Ideology%20of%20Deep%20Learning%20in%20ENSO%20Forecasts%0AAuthor%3A%20Yanhai%20Gan%20and%20Yipeng%20Chen%20and%20Ning%20Li%20and%20Xingguo%20Liu%20and%20Junyu%20Dong%20and%20Xianyao%20Chen%0AAbstract%3A%20The%20El%20Ni%7B~n%7Do-Southern%20Oscillation%20%28ENSO%29%20exerts%20profound%20influence%20on%20global%20climate%20variability%2C%20yet%20its%20prediction%20remains%20a%20grand%20challenge.%20Recent%20advances%20in%20deep%20learning%20have%20significantly%20improved%20forecasting%20skill%2C%20but%20the%20opacity%20of%20these%20models%20hampers%20scientific%20trust%20and%20operational%20deployment.%20Here%2C%20we%20introduce%20a%20mathematically%20grounded%20interpretability%20framework%20based%20on%20bounded%20variation%20function.%20By%20rescuing%20the%20%22dead%22%20neurons%20from%20the%20saturation%20zone%20of%20the%20activation%20function%2C%20we%20enhance%20the%20model%27s%20expressive%20capacity.%20Our%20analysis%20reveals%20that%20ENSO%20predictability%20emerges%20dominantly%20from%20the%20tropical%20Pacific%2C%20with%20contributions%20from%20the%20Indian%20and%20Atlantic%20Oceans%2C%20consistent%20with%20physical%20understanding.%20Controlled%20experiments%20affirm%20the%20robustness%20of%20our%20method%20and%20its%20alignment%20with%20established%20predictors.%20Notably%2C%20we%20probe%20the%20persistent%20Spring%20Predictability%20Barrier%20%28SPB%29%2C%20finding%20that%20despite%20expanded%20sensitivity%20during%20spring%2C%20predictive%20performance%20declines-likely%20due%20to%20suboptimal%20variable%20selection.%20These%20results%20suggest%20that%20incorporating%20additional%20ocean-atmosphere%20variables%20may%20help%20transcend%20SPB%20limitations%20and%20advance%20long-range%20ENSO%20prediction.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02050v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplore%2520the%2520Ideology%2520of%2520Deep%2520Learning%2520in%2520ENSO%2520Forecasts%26entry.906535625%3DYanhai%2520Gan%2520and%2520Yipeng%2520Chen%2520and%2520Ning%2520Li%2520and%2520Xingguo%2520Liu%2520and%2520Junyu%2520Dong%2520and%2520Xianyao%2520Chen%26entry.1292438233%3DThe%2520El%2520Ni%257B~n%257Do-Southern%2520Oscillation%2520%2528ENSO%2529%2520exerts%2520profound%2520influence%2520on%2520global%2520climate%2520variability%252C%2520yet%2520its%2520prediction%2520remains%2520a%2520grand%2520challenge.%2520Recent%2520advances%2520in%2520deep%2520learning%2520have%2520significantly%2520improved%2520forecasting%2520skill%252C%2520but%2520the%2520opacity%2520of%2520these%2520models%2520hampers%2520scientific%2520trust%2520and%2520operational%2520deployment.%2520Here%252C%2520we%2520introduce%2520a%2520mathematically%2520grounded%2520interpretability%2520framework%2520based%2520on%2520bounded%2520variation%2520function.%2520By%2520rescuing%2520the%2520%2522dead%2522%2520neurons%2520from%2520the%2520saturation%2520zone%2520of%2520the%2520activation%2520function%252C%2520we%2520enhance%2520the%2520model%2527s%2520expressive%2520capacity.%2520Our%2520analysis%2520reveals%2520that%2520ENSO%2520predictability%2520emerges%2520dominantly%2520from%2520the%2520tropical%2520Pacific%252C%2520with%2520contributions%2520from%2520the%2520Indian%2520and%2520Atlantic%2520Oceans%252C%2520consistent%2520with%2520physical%2520understanding.%2520Controlled%2520experiments%2520affirm%2520the%2520robustness%2520of%2520our%2520method%2520and%2520its%2520alignment%2520with%2520established%2520predictors.%2520Notably%252C%2520we%2520probe%2520the%2520persistent%2520Spring%2520Predictability%2520Barrier%2520%2528SPB%2529%252C%2520finding%2520that%2520despite%2520expanded%2520sensitivity%2520during%2520spring%252C%2520predictive%2520performance%2520declines-likely%2520due%2520to%2520suboptimal%2520variable%2520selection.%2520These%2520results%2520suggest%2520that%2520incorporating%2520additional%2520ocean-atmosphere%2520variables%2520may%2520help%2520transcend%2520SPB%2520limitations%2520and%2520advance%2520long-range%2520ENSO%2520prediction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02050v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explore%20the%20Ideology%20of%20Deep%20Learning%20in%20ENSO%20Forecasts&entry.906535625=Yanhai%20Gan%20and%20Yipeng%20Chen%20and%20Ning%20Li%20and%20Xingguo%20Liu%20and%20Junyu%20Dong%20and%20Xianyao%20Chen&entry.1292438233=The%20El%20Ni%7B~n%7Do-Southern%20Oscillation%20%28ENSO%29%20exerts%20profound%20influence%20on%20global%20climate%20variability%2C%20yet%20its%20prediction%20remains%20a%20grand%20challenge.%20Recent%20advances%20in%20deep%20learning%20have%20significantly%20improved%20forecasting%20skill%2C%20but%20the%20opacity%20of%20these%20models%20hampers%20scientific%20trust%20and%20operational%20deployment.%20Here%2C%20we%20introduce%20a%20mathematically%20grounded%20interpretability%20framework%20based%20on%20bounded%20variation%20function.%20By%20rescuing%20the%20%22dead%22%20neurons%20from%20the%20saturation%20zone%20of%20the%20activation%20function%2C%20we%20enhance%20the%20model%27s%20expressive%20capacity.%20Our%20analysis%20reveals%20that%20ENSO%20predictability%20emerges%20dominantly%20from%20the%20tropical%20Pacific%2C%20with%20contributions%20from%20the%20Indian%20and%20Atlantic%20Oceans%2C%20consistent%20with%20physical%20understanding.%20Controlled%20experiments%20affirm%20the%20robustness%20of%20our%20method%20and%20its%20alignment%20with%20established%20predictors.%20Notably%2C%20we%20probe%20the%20persistent%20Spring%20Predictability%20Barrier%20%28SPB%29%2C%20finding%20that%20despite%20expanded%20sensitivity%20during%20spring%2C%20predictive%20performance%20declines-likely%20due%20to%20suboptimal%20variable%20selection.%20These%20results%20suggest%20that%20incorporating%20additional%20ocean-atmosphere%20variables%20may%20help%20transcend%20SPB%20limitations%20and%20advance%20long-range%20ENSO%20prediction.&entry.1838667208=http%3A//arxiv.org/abs/2601.02050v1&entry.124074799=Read"},
{"title": "InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams", "author": "Shuai Yuan and Yantai Yang and Xiaotian Yang and Xupeng Zhang and Zhonghao Zhao and Lingming Zhang and Zhipeng Zhang", "abstract": "The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinite-horizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, a causal visual geometry transformer that operationalizes the concept of a rolling memory through a bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively ``rolling'' the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such a system is its performance over a truly infinite horizon, a capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT", "link": "http://arxiv.org/abs/2601.02281v1", "date": "2026-01-05", "relevancy": 2.4355, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6196}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6045}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InfiniteVGGT%3A%20Visual%20Geometry%20Grounded%20Transformer%20for%20Endless%20Streams&body=Title%3A%20InfiniteVGGT%3A%20Visual%20Geometry%20Grounded%20Transformer%20for%20Endless%20Streams%0AAuthor%3A%20Shuai%20Yuan%20and%20Yantai%20Yang%20and%20Xiaotian%20Yang%20and%20Xupeng%20Zhang%20and%20Zhonghao%20Zhao%20and%20Lingming%20Zhang%20and%20Zhipeng%20Zhang%0AAbstract%3A%20The%20grand%20vision%20of%20enabling%20persistent%2C%20large-scale%203D%20visual%20geometry%20understanding%20is%20shackled%20by%20the%20irreconcilable%20demands%20of%20scalability%20and%20long-term%20stability.%20While%20offline%20models%20like%20VGGT%20achieve%20inspiring%20geometry%20capability%2C%20their%20batch-based%20nature%20renders%20them%20irrelevant%20for%20live%20systems.%20Streaming%20architectures%2C%20though%20the%20intended%20solution%20for%20live%20operation%2C%20have%20proven%20inadequate.%20Existing%20methods%20either%20fail%20to%20support%20truly%20infinite-horizon%20inputs%20or%20suffer%20from%20catastrophic%20drift%20over%20long%20sequences.%20We%20shatter%20this%20long-standing%20dilemma%20with%20InfiniteVGGT%2C%20a%20causal%20visual%20geometry%20transformer%20that%20operationalizes%20the%20concept%20of%20a%20rolling%20memory%20through%20a%20bounded%20yet%20adaptive%20and%20perpetually%20expressive%20KV%20cache.%20Capitalizing%20on%20this%2C%20we%20devise%20a%20training-free%2C%20attention-agnostic%20pruning%20strategy%20that%20intelligently%20discards%20obsolete%20information%2C%20effectively%20%60%60rolling%27%27%20the%20memory%20forward%20with%20each%20new%20frame.%20Fully%20compatible%20with%20FlashAttention%2C%20InfiniteVGGT%20finally%20alleviates%20the%20compromise%2C%20enabling%20infinite-horizon%20streaming%20while%20outperforming%20existing%20streaming%20methods%20in%20long-term%20stability.%20The%20ultimate%20test%20for%20such%20a%20system%20is%20its%20performance%20over%20a%20truly%20infinite%20horizon%2C%20a%20capability%20that%20has%20been%20impossible%20to%20rigorously%20validate%20due%20to%20the%20lack%20of%20extremely%20long-term%2C%20continuous%20benchmarks.%20To%20address%20this%20critical%20gap%2C%20we%20introduce%20the%20Long3D%20benchmark%2C%20which%2C%20for%20the%20first%20time%2C%20enables%20a%20rigorous%20evaluation%20of%20continuous%203D%20geometry%20estimation%20on%20sequences%20about%2010%2C000%20frames.%20This%20provides%20the%20definitive%20evaluation%20platform%20for%20future%20research%20in%20long-term%203D%20geometry%20understanding.%20Code%20is%20available%20at%3A%20https%3A//github.com/AutoLab-SAI-SJTU/InfiniteVGGT%0ALink%3A%20http%3A//arxiv.org/abs/2601.02281v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfiniteVGGT%253A%2520Visual%2520Geometry%2520Grounded%2520Transformer%2520for%2520Endless%2520Streams%26entry.906535625%3DShuai%2520Yuan%2520and%2520Yantai%2520Yang%2520and%2520Xiaotian%2520Yang%2520and%2520Xupeng%2520Zhang%2520and%2520Zhonghao%2520Zhao%2520and%2520Lingming%2520Zhang%2520and%2520Zhipeng%2520Zhang%26entry.1292438233%3DThe%2520grand%2520vision%2520of%2520enabling%2520persistent%252C%2520large-scale%25203D%2520visual%2520geometry%2520understanding%2520is%2520shackled%2520by%2520the%2520irreconcilable%2520demands%2520of%2520scalability%2520and%2520long-term%2520stability.%2520While%2520offline%2520models%2520like%2520VGGT%2520achieve%2520inspiring%2520geometry%2520capability%252C%2520their%2520batch-based%2520nature%2520renders%2520them%2520irrelevant%2520for%2520live%2520systems.%2520Streaming%2520architectures%252C%2520though%2520the%2520intended%2520solution%2520for%2520live%2520operation%252C%2520have%2520proven%2520inadequate.%2520Existing%2520methods%2520either%2520fail%2520to%2520support%2520truly%2520infinite-horizon%2520inputs%2520or%2520suffer%2520from%2520catastrophic%2520drift%2520over%2520long%2520sequences.%2520We%2520shatter%2520this%2520long-standing%2520dilemma%2520with%2520InfiniteVGGT%252C%2520a%2520causal%2520visual%2520geometry%2520transformer%2520that%2520operationalizes%2520the%2520concept%2520of%2520a%2520rolling%2520memory%2520through%2520a%2520bounded%2520yet%2520adaptive%2520and%2520perpetually%2520expressive%2520KV%2520cache.%2520Capitalizing%2520on%2520this%252C%2520we%2520devise%2520a%2520training-free%252C%2520attention-agnostic%2520pruning%2520strategy%2520that%2520intelligently%2520discards%2520obsolete%2520information%252C%2520effectively%2520%2560%2560rolling%2527%2527%2520the%2520memory%2520forward%2520with%2520each%2520new%2520frame.%2520Fully%2520compatible%2520with%2520FlashAttention%252C%2520InfiniteVGGT%2520finally%2520alleviates%2520the%2520compromise%252C%2520enabling%2520infinite-horizon%2520streaming%2520while%2520outperforming%2520existing%2520streaming%2520methods%2520in%2520long-term%2520stability.%2520The%2520ultimate%2520test%2520for%2520such%2520a%2520system%2520is%2520its%2520performance%2520over%2520a%2520truly%2520infinite%2520horizon%252C%2520a%2520capability%2520that%2520has%2520been%2520impossible%2520to%2520rigorously%2520validate%2520due%2520to%2520the%2520lack%2520of%2520extremely%2520long-term%252C%2520continuous%2520benchmarks.%2520To%2520address%2520this%2520critical%2520gap%252C%2520we%2520introduce%2520the%2520Long3D%2520benchmark%252C%2520which%252C%2520for%2520the%2520first%2520time%252C%2520enables%2520a%2520rigorous%2520evaluation%2520of%2520continuous%25203D%2520geometry%2520estimation%2520on%2520sequences%2520about%252010%252C000%2520frames.%2520This%2520provides%2520the%2520definitive%2520evaluation%2520platform%2520for%2520future%2520research%2520in%2520long-term%25203D%2520geometry%2520understanding.%2520Code%2520is%2520available%2520at%253A%2520https%253A//github.com/AutoLab-SAI-SJTU/InfiniteVGGT%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02281v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InfiniteVGGT%3A%20Visual%20Geometry%20Grounded%20Transformer%20for%20Endless%20Streams&entry.906535625=Shuai%20Yuan%20and%20Yantai%20Yang%20and%20Xiaotian%20Yang%20and%20Xupeng%20Zhang%20and%20Zhonghao%20Zhao%20and%20Lingming%20Zhang%20and%20Zhipeng%20Zhang&entry.1292438233=The%20grand%20vision%20of%20enabling%20persistent%2C%20large-scale%203D%20visual%20geometry%20understanding%20is%20shackled%20by%20the%20irreconcilable%20demands%20of%20scalability%20and%20long-term%20stability.%20While%20offline%20models%20like%20VGGT%20achieve%20inspiring%20geometry%20capability%2C%20their%20batch-based%20nature%20renders%20them%20irrelevant%20for%20live%20systems.%20Streaming%20architectures%2C%20though%20the%20intended%20solution%20for%20live%20operation%2C%20have%20proven%20inadequate.%20Existing%20methods%20either%20fail%20to%20support%20truly%20infinite-horizon%20inputs%20or%20suffer%20from%20catastrophic%20drift%20over%20long%20sequences.%20We%20shatter%20this%20long-standing%20dilemma%20with%20InfiniteVGGT%2C%20a%20causal%20visual%20geometry%20transformer%20that%20operationalizes%20the%20concept%20of%20a%20rolling%20memory%20through%20a%20bounded%20yet%20adaptive%20and%20perpetually%20expressive%20KV%20cache.%20Capitalizing%20on%20this%2C%20we%20devise%20a%20training-free%2C%20attention-agnostic%20pruning%20strategy%20that%20intelligently%20discards%20obsolete%20information%2C%20effectively%20%60%60rolling%27%27%20the%20memory%20forward%20with%20each%20new%20frame.%20Fully%20compatible%20with%20FlashAttention%2C%20InfiniteVGGT%20finally%20alleviates%20the%20compromise%2C%20enabling%20infinite-horizon%20streaming%20while%20outperforming%20existing%20streaming%20methods%20in%20long-term%20stability.%20The%20ultimate%20test%20for%20such%20a%20system%20is%20its%20performance%20over%20a%20truly%20infinite%20horizon%2C%20a%20capability%20that%20has%20been%20impossible%20to%20rigorously%20validate%20due%20to%20the%20lack%20of%20extremely%20long-term%2C%20continuous%20benchmarks.%20To%20address%20this%20critical%20gap%2C%20we%20introduce%20the%20Long3D%20benchmark%2C%20which%2C%20for%20the%20first%20time%2C%20enables%20a%20rigorous%20evaluation%20of%20continuous%203D%20geometry%20estimation%20on%20sequences%20about%2010%2C000%20frames.%20This%20provides%20the%20definitive%20evaluation%20platform%20for%20future%20research%20in%20long-term%203D%20geometry%20understanding.%20Code%20is%20available%20at%3A%20https%3A//github.com/AutoLab-SAI-SJTU/InfiniteVGGT&entry.1838667208=http%3A//arxiv.org/abs/2601.02281v1&entry.124074799=Read"},
{"title": "ChaosBench-Logic: A Benchmark for Logical and Symbolic Reasoning on Chaotic Dynamical Systems", "author": "Noel Thomas", "abstract": "Large language models (LLMs) excel at natural language tasks but remain brittle in domains requiring precise logical and symbolic reasoning. Chaotic dynamical systems provide an especially demanding test because chaos is deterministic yet often misinterpreted as randomness or complexity. We introduce ChaosBench-Logic, a benchmark that evaluates LLM reasoning across 30 diverse dynamical systems using a unified first-order logic (FOL) ontology. Each system is annotated with truth assignments for 11 semantic predicates, and 621 questions are generated across seven reasoning categories, including multi-hop implications, cross-system analogies, counterfactual reasoning, bias probes, and multi-turn dialogues. We define metrics for logical accuracy, implication consistency, dialogue coherence, and contradiction, and we release an open-source evaluation pipeline. Initial experiments show that frontier LLMs such as GPT-4, Claude 3.5 Sonnet, Gemini 2.5 Flash, and the open-source LLaMA-3 70B achieve 91-94% per-item accuracy, yet still score 0% on compositional items and exhibit fragile global coherence. Dialogue-level accuracy ranges from 53.1% (GPT-4 CoT) to 75.5% (LLaMA-3 zero-shot). ChaosBench-Logic provides a rigorous testbed for diagnosing such failures and a foundation for developing neuro-symbolic approaches that improve scientific reasoning in LLMs.", "link": "http://arxiv.org/abs/2601.01982v1", "date": "2026-01-05", "relevancy": 2.4243, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4896}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4825}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChaosBench-Logic%3A%20A%20Benchmark%20for%20Logical%20and%20Symbolic%20Reasoning%20on%20Chaotic%20Dynamical%20Systems&body=Title%3A%20ChaosBench-Logic%3A%20A%20Benchmark%20for%20Logical%20and%20Symbolic%20Reasoning%20on%20Chaotic%20Dynamical%20Systems%0AAuthor%3A%20Noel%20Thomas%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20excel%20at%20natural%20language%20tasks%20but%20remain%20brittle%20in%20domains%20requiring%20precise%20logical%20and%20symbolic%20reasoning.%20Chaotic%20dynamical%20systems%20provide%20an%20especially%20demanding%20test%20because%20chaos%20is%20deterministic%20yet%20often%20misinterpreted%20as%20randomness%20or%20complexity.%20We%20introduce%20ChaosBench-Logic%2C%20a%20benchmark%20that%20evaluates%20LLM%20reasoning%20across%2030%20diverse%20dynamical%20systems%20using%20a%20unified%20first-order%20logic%20%28FOL%29%20ontology.%20Each%20system%20is%20annotated%20with%20truth%20assignments%20for%2011%20semantic%20predicates%2C%20and%20621%20questions%20are%20generated%20across%20seven%20reasoning%20categories%2C%20including%20multi-hop%20implications%2C%20cross-system%20analogies%2C%20counterfactual%20reasoning%2C%20bias%20probes%2C%20and%20multi-turn%20dialogues.%20We%20define%20metrics%20for%20logical%20accuracy%2C%20implication%20consistency%2C%20dialogue%20coherence%2C%20and%20contradiction%2C%20and%20we%20release%20an%20open-source%20evaluation%20pipeline.%20Initial%20experiments%20show%20that%20frontier%20LLMs%20such%20as%20GPT-4%2C%20Claude%203.5%20Sonnet%2C%20Gemini%202.5%20Flash%2C%20and%20the%20open-source%20LLaMA-3%2070B%20achieve%2091-94%25%20per-item%20accuracy%2C%20yet%20still%20score%200%25%20on%20compositional%20items%20and%20exhibit%20fragile%20global%20coherence.%20Dialogue-level%20accuracy%20ranges%20from%2053.1%25%20%28GPT-4%20CoT%29%20to%2075.5%25%20%28LLaMA-3%20zero-shot%29.%20ChaosBench-Logic%20provides%20a%20rigorous%20testbed%20for%20diagnosing%20such%20failures%20and%20a%20foundation%20for%20developing%20neuro-symbolic%20approaches%20that%20improve%20scientific%20reasoning%20in%20LLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2601.01982v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChaosBench-Logic%253A%2520A%2520Benchmark%2520for%2520Logical%2520and%2520Symbolic%2520Reasoning%2520on%2520Chaotic%2520Dynamical%2520Systems%26entry.906535625%3DNoel%2520Thomas%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520excel%2520at%2520natural%2520language%2520tasks%2520but%2520remain%2520brittle%2520in%2520domains%2520requiring%2520precise%2520logical%2520and%2520symbolic%2520reasoning.%2520Chaotic%2520dynamical%2520systems%2520provide%2520an%2520especially%2520demanding%2520test%2520because%2520chaos%2520is%2520deterministic%2520yet%2520often%2520misinterpreted%2520as%2520randomness%2520or%2520complexity.%2520We%2520introduce%2520ChaosBench-Logic%252C%2520a%2520benchmark%2520that%2520evaluates%2520LLM%2520reasoning%2520across%252030%2520diverse%2520dynamical%2520systems%2520using%2520a%2520unified%2520first-order%2520logic%2520%2528FOL%2529%2520ontology.%2520Each%2520system%2520is%2520annotated%2520with%2520truth%2520assignments%2520for%252011%2520semantic%2520predicates%252C%2520and%2520621%2520questions%2520are%2520generated%2520across%2520seven%2520reasoning%2520categories%252C%2520including%2520multi-hop%2520implications%252C%2520cross-system%2520analogies%252C%2520counterfactual%2520reasoning%252C%2520bias%2520probes%252C%2520and%2520multi-turn%2520dialogues.%2520We%2520define%2520metrics%2520for%2520logical%2520accuracy%252C%2520implication%2520consistency%252C%2520dialogue%2520coherence%252C%2520and%2520contradiction%252C%2520and%2520we%2520release%2520an%2520open-source%2520evaluation%2520pipeline.%2520Initial%2520experiments%2520show%2520that%2520frontier%2520LLMs%2520such%2520as%2520GPT-4%252C%2520Claude%25203.5%2520Sonnet%252C%2520Gemini%25202.5%2520Flash%252C%2520and%2520the%2520open-source%2520LLaMA-3%252070B%2520achieve%252091-94%2525%2520per-item%2520accuracy%252C%2520yet%2520still%2520score%25200%2525%2520on%2520compositional%2520items%2520and%2520exhibit%2520fragile%2520global%2520coherence.%2520Dialogue-level%2520accuracy%2520ranges%2520from%252053.1%2525%2520%2528GPT-4%2520CoT%2529%2520to%252075.5%2525%2520%2528LLaMA-3%2520zero-shot%2529.%2520ChaosBench-Logic%2520provides%2520a%2520rigorous%2520testbed%2520for%2520diagnosing%2520such%2520failures%2520and%2520a%2520foundation%2520for%2520developing%2520neuro-symbolic%2520approaches%2520that%2520improve%2520scientific%2520reasoning%2520in%2520LLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.01982v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChaosBench-Logic%3A%20A%20Benchmark%20for%20Logical%20and%20Symbolic%20Reasoning%20on%20Chaotic%20Dynamical%20Systems&entry.906535625=Noel%20Thomas&entry.1292438233=Large%20language%20models%20%28LLMs%29%20excel%20at%20natural%20language%20tasks%20but%20remain%20brittle%20in%20domains%20requiring%20precise%20logical%20and%20symbolic%20reasoning.%20Chaotic%20dynamical%20systems%20provide%20an%20especially%20demanding%20test%20because%20chaos%20is%20deterministic%20yet%20often%20misinterpreted%20as%20randomness%20or%20complexity.%20We%20introduce%20ChaosBench-Logic%2C%20a%20benchmark%20that%20evaluates%20LLM%20reasoning%20across%2030%20diverse%20dynamical%20systems%20using%20a%20unified%20first-order%20logic%20%28FOL%29%20ontology.%20Each%20system%20is%20annotated%20with%20truth%20assignments%20for%2011%20semantic%20predicates%2C%20and%20621%20questions%20are%20generated%20across%20seven%20reasoning%20categories%2C%20including%20multi-hop%20implications%2C%20cross-system%20analogies%2C%20counterfactual%20reasoning%2C%20bias%20probes%2C%20and%20multi-turn%20dialogues.%20We%20define%20metrics%20for%20logical%20accuracy%2C%20implication%20consistency%2C%20dialogue%20coherence%2C%20and%20contradiction%2C%20and%20we%20release%20an%20open-source%20evaluation%20pipeline.%20Initial%20experiments%20show%20that%20frontier%20LLMs%20such%20as%20GPT-4%2C%20Claude%203.5%20Sonnet%2C%20Gemini%202.5%20Flash%2C%20and%20the%20open-source%20LLaMA-3%2070B%20achieve%2091-94%25%20per-item%20accuracy%2C%20yet%20still%20score%200%25%20on%20compositional%20items%20and%20exhibit%20fragile%20global%20coherence.%20Dialogue-level%20accuracy%20ranges%20from%2053.1%25%20%28GPT-4%20CoT%29%20to%2075.5%25%20%28LLaMA-3%20zero-shot%29.%20ChaosBench-Logic%20provides%20a%20rigorous%20testbed%20for%20diagnosing%20such%20failures%20and%20a%20foundation%20for%20developing%20neuro-symbolic%20approaches%20that%20improve%20scientific%20reasoning%20in%20LLMs.&entry.1838667208=http%3A//arxiv.org/abs/2601.01982v1&entry.124074799=Read"},
{"title": "Quantized SO(3)-Equivariant Graph Neural Networks for Efficient Molecular Property Prediction", "author": "Haoyu Zhou and Ping Xue and Tianfan Fu and Hao Zhang", "abstract": "Deploying 3D graph neural networks (GNNs) that are equivariant to 3D rotations (the group SO(3)) on edge devices is challenging due to their high computational cost. This paper addresses the problem by compressing and accelerating an SO(3)-equivariant GNN using low-bit quantization techniques. Specifically, we introduce three innovations for quantized equivariant transformers: (1) a magnitude-direction decoupled quantization scheme that separately quantizes the norm and orientation of equivariant (vector) features, (2) a branch-separated quantization-aware training strategy that treats invariant and equivariant feature channels differently in an attention-based $SO(3)$-GNN, and (3) a robustness-enhancing attention normalization mechanism that stabilizes low-precision attention computations. Experiments on the QM9 and rMD17 molecular benchmarks demonstrate that our 8-bit models achieve accuracy on energy and force predictions comparable to full-precision baselines with markedly improved efficiency. We also conduct ablation studies to quantify the contribution of each component to maintain accuracy and equivariance under quantization, using the Local error of equivariance (LEE) metric. The proposed techniques enable the deployment of symmetry-aware GNNs in practical chemistry applications with 2.37--2.73x faster inference and 4x smaller model size, without sacrificing accuracy or physical symmetry.", "link": "http://arxiv.org/abs/2601.02213v1", "date": "2026-01-05", "relevancy": 2.4133, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4928}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4823}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantized%20SO%283%29-Equivariant%20Graph%20Neural%20Networks%20for%20Efficient%20Molecular%20Property%20Prediction&body=Title%3A%20Quantized%20SO%283%29-Equivariant%20Graph%20Neural%20Networks%20for%20Efficient%20Molecular%20Property%20Prediction%0AAuthor%3A%20Haoyu%20Zhou%20and%20Ping%20Xue%20and%20Tianfan%20Fu%20and%20Hao%20Zhang%0AAbstract%3A%20Deploying%203D%20graph%20neural%20networks%20%28GNNs%29%20that%20are%20equivariant%20to%203D%20rotations%20%28the%20group%20SO%283%29%29%20on%20edge%20devices%20is%20challenging%20due%20to%20their%20high%20computational%20cost.%20This%20paper%20addresses%20the%20problem%20by%20compressing%20and%20accelerating%20an%20SO%283%29-equivariant%20GNN%20using%20low-bit%20quantization%20techniques.%20Specifically%2C%20we%20introduce%20three%20innovations%20for%20quantized%20equivariant%20transformers%3A%20%281%29%20a%20magnitude-direction%20decoupled%20quantization%20scheme%20that%20separately%20quantizes%20the%20norm%20and%20orientation%20of%20equivariant%20%28vector%29%20features%2C%20%282%29%20a%20branch-separated%20quantization-aware%20training%20strategy%20that%20treats%20invariant%20and%20equivariant%20feature%20channels%20differently%20in%20an%20attention-based%20%24SO%283%29%24-GNN%2C%20and%20%283%29%20a%20robustness-enhancing%20attention%20normalization%20mechanism%20that%20stabilizes%20low-precision%20attention%20computations.%20Experiments%20on%20the%20QM9%20and%20rMD17%20molecular%20benchmarks%20demonstrate%20that%20our%208-bit%20models%20achieve%20accuracy%20on%20energy%20and%20force%20predictions%20comparable%20to%20full-precision%20baselines%20with%20markedly%20improved%20efficiency.%20We%20also%20conduct%20ablation%20studies%20to%20quantify%20the%20contribution%20of%20each%20component%20to%20maintain%20accuracy%20and%20equivariance%20under%20quantization%2C%20using%20the%20Local%20error%20of%20equivariance%20%28LEE%29%20metric.%20The%20proposed%20techniques%20enable%20the%20deployment%20of%20symmetry-aware%20GNNs%20in%20practical%20chemistry%20applications%20with%202.37--2.73x%20faster%20inference%20and%204x%20smaller%20model%20size%2C%20without%20sacrificing%20accuracy%20or%20physical%20symmetry.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02213v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantized%2520SO%25283%2529-Equivariant%2520Graph%2520Neural%2520Networks%2520for%2520Efficient%2520Molecular%2520Property%2520Prediction%26entry.906535625%3DHaoyu%2520Zhou%2520and%2520Ping%2520Xue%2520and%2520Tianfan%2520Fu%2520and%2520Hao%2520Zhang%26entry.1292438233%3DDeploying%25203D%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520that%2520are%2520equivariant%2520to%25203D%2520rotations%2520%2528the%2520group%2520SO%25283%2529%2529%2520on%2520edge%2520devices%2520is%2520challenging%2520due%2520to%2520their%2520high%2520computational%2520cost.%2520This%2520paper%2520addresses%2520the%2520problem%2520by%2520compressing%2520and%2520accelerating%2520an%2520SO%25283%2529-equivariant%2520GNN%2520using%2520low-bit%2520quantization%2520techniques.%2520Specifically%252C%2520we%2520introduce%2520three%2520innovations%2520for%2520quantized%2520equivariant%2520transformers%253A%2520%25281%2529%2520a%2520magnitude-direction%2520decoupled%2520quantization%2520scheme%2520that%2520separately%2520quantizes%2520the%2520norm%2520and%2520orientation%2520of%2520equivariant%2520%2528vector%2529%2520features%252C%2520%25282%2529%2520a%2520branch-separated%2520quantization-aware%2520training%2520strategy%2520that%2520treats%2520invariant%2520and%2520equivariant%2520feature%2520channels%2520differently%2520in%2520an%2520attention-based%2520%2524SO%25283%2529%2524-GNN%252C%2520and%2520%25283%2529%2520a%2520robustness-enhancing%2520attention%2520normalization%2520mechanism%2520that%2520stabilizes%2520low-precision%2520attention%2520computations.%2520Experiments%2520on%2520the%2520QM9%2520and%2520rMD17%2520molecular%2520benchmarks%2520demonstrate%2520that%2520our%25208-bit%2520models%2520achieve%2520accuracy%2520on%2520energy%2520and%2520force%2520predictions%2520comparable%2520to%2520full-precision%2520baselines%2520with%2520markedly%2520improved%2520efficiency.%2520We%2520also%2520conduct%2520ablation%2520studies%2520to%2520quantify%2520the%2520contribution%2520of%2520each%2520component%2520to%2520maintain%2520accuracy%2520and%2520equivariance%2520under%2520quantization%252C%2520using%2520the%2520Local%2520error%2520of%2520equivariance%2520%2528LEE%2529%2520metric.%2520The%2520proposed%2520techniques%2520enable%2520the%2520deployment%2520of%2520symmetry-aware%2520GNNs%2520in%2520practical%2520chemistry%2520applications%2520with%25202.37--2.73x%2520faster%2520inference%2520and%25204x%2520smaller%2520model%2520size%252C%2520without%2520sacrificing%2520accuracy%2520or%2520physical%2520symmetry.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02213v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantized%20SO%283%29-Equivariant%20Graph%20Neural%20Networks%20for%20Efficient%20Molecular%20Property%20Prediction&entry.906535625=Haoyu%20Zhou%20and%20Ping%20Xue%20and%20Tianfan%20Fu%20and%20Hao%20Zhang&entry.1292438233=Deploying%203D%20graph%20neural%20networks%20%28GNNs%29%20that%20are%20equivariant%20to%203D%20rotations%20%28the%20group%20SO%283%29%29%20on%20edge%20devices%20is%20challenging%20due%20to%20their%20high%20computational%20cost.%20This%20paper%20addresses%20the%20problem%20by%20compressing%20and%20accelerating%20an%20SO%283%29-equivariant%20GNN%20using%20low-bit%20quantization%20techniques.%20Specifically%2C%20we%20introduce%20three%20innovations%20for%20quantized%20equivariant%20transformers%3A%20%281%29%20a%20magnitude-direction%20decoupled%20quantization%20scheme%20that%20separately%20quantizes%20the%20norm%20and%20orientation%20of%20equivariant%20%28vector%29%20features%2C%20%282%29%20a%20branch-separated%20quantization-aware%20training%20strategy%20that%20treats%20invariant%20and%20equivariant%20feature%20channels%20differently%20in%20an%20attention-based%20%24SO%283%29%24-GNN%2C%20and%20%283%29%20a%20robustness-enhancing%20attention%20normalization%20mechanism%20that%20stabilizes%20low-precision%20attention%20computations.%20Experiments%20on%20the%20QM9%20and%20rMD17%20molecular%20benchmarks%20demonstrate%20that%20our%208-bit%20models%20achieve%20accuracy%20on%20energy%20and%20force%20predictions%20comparable%20to%20full-precision%20baselines%20with%20markedly%20improved%20efficiency.%20We%20also%20conduct%20ablation%20studies%20to%20quantify%20the%20contribution%20of%20each%20component%20to%20maintain%20accuracy%20and%20equivariance%20under%20quantization%2C%20using%20the%20Local%20error%20of%20equivariance%20%28LEE%29%20metric.%20The%20proposed%20techniques%20enable%20the%20deployment%20of%20symmetry-aware%20GNNs%20in%20practical%20chemistry%20applications%20with%202.37--2.73x%20faster%20inference%20and%204x%20smaller%20model%20size%2C%20without%20sacrificing%20accuracy%20or%20physical%20symmetry.&entry.1838667208=http%3A//arxiv.org/abs/2601.02213v1&entry.124074799=Read"},
{"title": "Coward: Collision-based Watermark for Proactive Federated Backdoor Detection", "author": "Wenjie Li and Siying Gu and Yiming Li and Kangjie Chen and Zhili Chen and Tianwei Zhang and Shu-Tao Xia and Dacheng Tao", "abstract": "Backdoor detection is currently the mainstream defense against backdoor attacks in federated learning (FL), where a small number of malicious clients can upload poisoned updates to compromise the federated global model. Existing backdoor detection techniques fall into two categories, passive and proactive, depending on whether the server proactively intervenes in the training process. However, both of them have inherent limitations in practice: passive detection methods are disrupted by common non-i.i.d. data distributions and random participation of FL clients, whereas current proactive detection methods are misled by an inevitable out-of-distribution (OOD) bias because they rely on backdoor coexistence effects. To address these issues, we introduce a novel proactive detection method dubbed Coward, inspired by our discovery of multi-backdoor collision effects, in which consecutively planted, distinct backdoors significantly suppress earlier ones. Correspondingly, we modify the federated global model by injecting a carefully designed backdoor-collided watermark, implemented via regulated dual-mapping learning on OOD data. This design not only enables an inverted detection paradigm compared to existing proactive methods, thereby naturally counteracting the adverse impact of OOD prediction bias, but also introduces a low-disruptive training intervention that inherently limits the strength of OOD bias, leading to significantly fewer misjudgments. Extensive experiments on benchmark datasets show that Coward achieves state-of-the-art detection performance, effectively alleviates OOD prediction bias, and remains robust against potential adaptive attacks. The code for our method is available at https://github.com/still2009/cowardFL.", "link": "http://arxiv.org/abs/2508.02115v3", "date": "2026-01-05", "relevancy": 2.4124, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4912}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4877}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Coward%3A%20Collision-based%20Watermark%20for%20Proactive%20Federated%20Backdoor%20Detection&body=Title%3A%20Coward%3A%20Collision-based%20Watermark%20for%20Proactive%20Federated%20Backdoor%20Detection%0AAuthor%3A%20Wenjie%20Li%20and%20Siying%20Gu%20and%20Yiming%20Li%20and%20Kangjie%20Chen%20and%20Zhili%20Chen%20and%20Tianwei%20Zhang%20and%20Shu-Tao%20Xia%20and%20Dacheng%20Tao%0AAbstract%3A%20Backdoor%20detection%20is%20currently%20the%20mainstream%20defense%20against%20backdoor%20attacks%20in%20federated%20learning%20%28FL%29%2C%20where%20a%20small%20number%20of%20malicious%20clients%20can%20upload%20poisoned%20updates%20to%20compromise%20the%20federated%20global%20model.%20Existing%20backdoor%20detection%20techniques%20fall%20into%20two%20categories%2C%20passive%20and%20proactive%2C%20depending%20on%20whether%20the%20server%20proactively%20intervenes%20in%20the%20training%20process.%20However%2C%20both%20of%20them%20have%20inherent%20limitations%20in%20practice%3A%20passive%20detection%20methods%20are%20disrupted%20by%20common%20non-i.i.d.%20data%20distributions%20and%20random%20participation%20of%20FL%20clients%2C%20whereas%20current%20proactive%20detection%20methods%20are%20misled%20by%20an%20inevitable%20out-of-distribution%20%28OOD%29%20bias%20because%20they%20rely%20on%20backdoor%20coexistence%20effects.%20To%20address%20these%20issues%2C%20we%20introduce%20a%20novel%20proactive%20detection%20method%20dubbed%20Coward%2C%20inspired%20by%20our%20discovery%20of%20multi-backdoor%20collision%20effects%2C%20in%20which%20consecutively%20planted%2C%20distinct%20backdoors%20significantly%20suppress%20earlier%20ones.%20Correspondingly%2C%20we%20modify%20the%20federated%20global%20model%20by%20injecting%20a%20carefully%20designed%20backdoor-collided%20watermark%2C%20implemented%20via%20regulated%20dual-mapping%20learning%20on%20OOD%20data.%20This%20design%20not%20only%20enables%20an%20inverted%20detection%20paradigm%20compared%20to%20existing%20proactive%20methods%2C%20thereby%20naturally%20counteracting%20the%20adverse%20impact%20of%20OOD%20prediction%20bias%2C%20but%20also%20introduces%20a%20low-disruptive%20training%20intervention%20that%20inherently%20limits%20the%20strength%20of%20OOD%20bias%2C%20leading%20to%20significantly%20fewer%20misjudgments.%20Extensive%20experiments%20on%20benchmark%20datasets%20show%20that%20Coward%20achieves%20state-of-the-art%20detection%20performance%2C%20effectively%20alleviates%20OOD%20prediction%20bias%2C%20and%20remains%20robust%20against%20potential%20adaptive%20attacks.%20The%20code%20for%20our%20method%20is%20available%20at%20https%3A//github.com/still2009/cowardFL.%0ALink%3A%20http%3A//arxiv.org/abs/2508.02115v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoward%253A%2520Collision-based%2520Watermark%2520for%2520Proactive%2520Federated%2520Backdoor%2520Detection%26entry.906535625%3DWenjie%2520Li%2520and%2520Siying%2520Gu%2520and%2520Yiming%2520Li%2520and%2520Kangjie%2520Chen%2520and%2520Zhili%2520Chen%2520and%2520Tianwei%2520Zhang%2520and%2520Shu-Tao%2520Xia%2520and%2520Dacheng%2520Tao%26entry.1292438233%3DBackdoor%2520detection%2520is%2520currently%2520the%2520mainstream%2520defense%2520against%2520backdoor%2520attacks%2520in%2520federated%2520learning%2520%2528FL%2529%252C%2520where%2520a%2520small%2520number%2520of%2520malicious%2520clients%2520can%2520upload%2520poisoned%2520updates%2520to%2520compromise%2520the%2520federated%2520global%2520model.%2520Existing%2520backdoor%2520detection%2520techniques%2520fall%2520into%2520two%2520categories%252C%2520passive%2520and%2520proactive%252C%2520depending%2520on%2520whether%2520the%2520server%2520proactively%2520intervenes%2520in%2520the%2520training%2520process.%2520However%252C%2520both%2520of%2520them%2520have%2520inherent%2520limitations%2520in%2520practice%253A%2520passive%2520detection%2520methods%2520are%2520disrupted%2520by%2520common%2520non-i.i.d.%2520data%2520distributions%2520and%2520random%2520participation%2520of%2520FL%2520clients%252C%2520whereas%2520current%2520proactive%2520detection%2520methods%2520are%2520misled%2520by%2520an%2520inevitable%2520out-of-distribution%2520%2528OOD%2529%2520bias%2520because%2520they%2520rely%2520on%2520backdoor%2520coexistence%2520effects.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520a%2520novel%2520proactive%2520detection%2520method%2520dubbed%2520Coward%252C%2520inspired%2520by%2520our%2520discovery%2520of%2520multi-backdoor%2520collision%2520effects%252C%2520in%2520which%2520consecutively%2520planted%252C%2520distinct%2520backdoors%2520significantly%2520suppress%2520earlier%2520ones.%2520Correspondingly%252C%2520we%2520modify%2520the%2520federated%2520global%2520model%2520by%2520injecting%2520a%2520carefully%2520designed%2520backdoor-collided%2520watermark%252C%2520implemented%2520via%2520regulated%2520dual-mapping%2520learning%2520on%2520OOD%2520data.%2520This%2520design%2520not%2520only%2520enables%2520an%2520inverted%2520detection%2520paradigm%2520compared%2520to%2520existing%2520proactive%2520methods%252C%2520thereby%2520naturally%2520counteracting%2520the%2520adverse%2520impact%2520of%2520OOD%2520prediction%2520bias%252C%2520but%2520also%2520introduces%2520a%2520low-disruptive%2520training%2520intervention%2520that%2520inherently%2520limits%2520the%2520strength%2520of%2520OOD%2520bias%252C%2520leading%2520to%2520significantly%2520fewer%2520misjudgments.%2520Extensive%2520experiments%2520on%2520benchmark%2520datasets%2520show%2520that%2520Coward%2520achieves%2520state-of-the-art%2520detection%2520performance%252C%2520effectively%2520alleviates%2520OOD%2520prediction%2520bias%252C%2520and%2520remains%2520robust%2520against%2520potential%2520adaptive%2520attacks.%2520The%2520code%2520for%2520our%2520method%2520is%2520available%2520at%2520https%253A//github.com/still2009/cowardFL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02115v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coward%3A%20Collision-based%20Watermark%20for%20Proactive%20Federated%20Backdoor%20Detection&entry.906535625=Wenjie%20Li%20and%20Siying%20Gu%20and%20Yiming%20Li%20and%20Kangjie%20Chen%20and%20Zhili%20Chen%20and%20Tianwei%20Zhang%20and%20Shu-Tao%20Xia%20and%20Dacheng%20Tao&entry.1292438233=Backdoor%20detection%20is%20currently%20the%20mainstream%20defense%20against%20backdoor%20attacks%20in%20federated%20learning%20%28FL%29%2C%20where%20a%20small%20number%20of%20malicious%20clients%20can%20upload%20poisoned%20updates%20to%20compromise%20the%20federated%20global%20model.%20Existing%20backdoor%20detection%20techniques%20fall%20into%20two%20categories%2C%20passive%20and%20proactive%2C%20depending%20on%20whether%20the%20server%20proactively%20intervenes%20in%20the%20training%20process.%20However%2C%20both%20of%20them%20have%20inherent%20limitations%20in%20practice%3A%20passive%20detection%20methods%20are%20disrupted%20by%20common%20non-i.i.d.%20data%20distributions%20and%20random%20participation%20of%20FL%20clients%2C%20whereas%20current%20proactive%20detection%20methods%20are%20misled%20by%20an%20inevitable%20out-of-distribution%20%28OOD%29%20bias%20because%20they%20rely%20on%20backdoor%20coexistence%20effects.%20To%20address%20these%20issues%2C%20we%20introduce%20a%20novel%20proactive%20detection%20method%20dubbed%20Coward%2C%20inspired%20by%20our%20discovery%20of%20multi-backdoor%20collision%20effects%2C%20in%20which%20consecutively%20planted%2C%20distinct%20backdoors%20significantly%20suppress%20earlier%20ones.%20Correspondingly%2C%20we%20modify%20the%20federated%20global%20model%20by%20injecting%20a%20carefully%20designed%20backdoor-collided%20watermark%2C%20implemented%20via%20regulated%20dual-mapping%20learning%20on%20OOD%20data.%20This%20design%20not%20only%20enables%20an%20inverted%20detection%20paradigm%20compared%20to%20existing%20proactive%20methods%2C%20thereby%20naturally%20counteracting%20the%20adverse%20impact%20of%20OOD%20prediction%20bias%2C%20but%20also%20introduces%20a%20low-disruptive%20training%20intervention%20that%20inherently%20limits%20the%20strength%20of%20OOD%20bias%2C%20leading%20to%20significantly%20fewer%20misjudgments.%20Extensive%20experiments%20on%20benchmark%20datasets%20show%20that%20Coward%20achieves%20state-of-the-art%20detection%20performance%2C%20effectively%20alleviates%20OOD%20prediction%20bias%2C%20and%20remains%20robust%20against%20potential%20adaptive%20attacks.%20The%20code%20for%20our%20method%20is%20available%20at%20https%3A//github.com/still2009/cowardFL.&entry.1838667208=http%3A//arxiv.org/abs/2508.02115v3&entry.124074799=Read"},
{"title": "Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey", "author": "Shengyue Guan and Jindong Wang and Jiang Bian and Bin Zhu and Jian-guang Lou and Haoyi Xiong", "abstract": "This survey examines evaluation methods for large language model (LLM)-based agents in multi-turn conversational settings. Using a PRISMA-inspired framework, we systematically reviewed nearly 250 scholarly sources, capturing the state of the art from various venues of publication, and establishing a solid foundation for our analysis. Our study offers a structured approach by developing two interrelated taxonomy systems: one that defines \\emph{what to evaluate} and another that explains \\emph{how to evaluate}. The first taxonomy identifies key components of LLM-based agents for multi-turn conversations and their evaluation dimensions, including task completion, response quality, user experience, memory and context retention, as well as planning and tool integration. These components ensure that the performance of conversational agents is assessed in a holistic and meaningful manner. The second taxonomy system focuses on the evaluation methodologies. It categorizes approaches into annotation-based evaluations, automated metrics, hybrid strategies that combine human assessments with quantitative measures, and self-judging methods utilizing LLMs. This framework not only captures traditional metrics derived from language understanding, such as BLEU and ROUGE scores, but also incorporates advanced techniques that reflect the dynamic, interactive nature of multi-turn dialogues.", "link": "http://arxiv.org/abs/2503.22458v2", "date": "2026-01-05", "relevancy": 2.3482, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4775}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4775}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20LLM-based%20Agents%20for%20Multi-Turn%20Conversations%3A%20A%20Survey&body=Title%3A%20Evaluating%20LLM-based%20Agents%20for%20Multi-Turn%20Conversations%3A%20A%20Survey%0AAuthor%3A%20Shengyue%20Guan%20and%20Jindong%20Wang%20and%20Jiang%20Bian%20and%20Bin%20Zhu%20and%20Jian-guang%20Lou%20and%20Haoyi%20Xiong%0AAbstract%3A%20This%20survey%20examines%20evaluation%20methods%20for%20large%20language%20model%20%28LLM%29-based%20agents%20in%20multi-turn%20conversational%20settings.%20Using%20a%20PRISMA-inspired%20framework%2C%20we%20systematically%20reviewed%20nearly%20250%20scholarly%20sources%2C%20capturing%20the%20state%20of%20the%20art%20from%20various%20venues%20of%20publication%2C%20and%20establishing%20a%20solid%20foundation%20for%20our%20analysis.%20Our%20study%20offers%20a%20structured%20approach%20by%20developing%20two%20interrelated%20taxonomy%20systems%3A%20one%20that%20defines%20%5Cemph%7Bwhat%20to%20evaluate%7D%20and%20another%20that%20explains%20%5Cemph%7Bhow%20to%20evaluate%7D.%20The%20first%20taxonomy%20identifies%20key%20components%20of%20LLM-based%20agents%20for%20multi-turn%20conversations%20and%20their%20evaluation%20dimensions%2C%20including%20task%20completion%2C%20response%20quality%2C%20user%20experience%2C%20memory%20and%20context%20retention%2C%20as%20well%20as%20planning%20and%20tool%20integration.%20These%20components%20ensure%20that%20the%20performance%20of%20conversational%20agents%20is%20assessed%20in%20a%20holistic%20and%20meaningful%20manner.%20The%20second%20taxonomy%20system%20focuses%20on%20the%20evaluation%20methodologies.%20It%20categorizes%20approaches%20into%20annotation-based%20evaluations%2C%20automated%20metrics%2C%20hybrid%20strategies%20that%20combine%20human%20assessments%20with%20quantitative%20measures%2C%20and%20self-judging%20methods%20utilizing%20LLMs.%20This%20framework%20not%20only%20captures%20traditional%20metrics%20derived%20from%20language%20understanding%2C%20such%20as%20BLEU%20and%20ROUGE%20scores%2C%20but%20also%20incorporates%20advanced%20techniques%20that%20reflect%20the%20dynamic%2C%20interactive%20nature%20of%20multi-turn%20dialogues.%0ALink%3A%20http%3A//arxiv.org/abs/2503.22458v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520LLM-based%2520Agents%2520for%2520Multi-Turn%2520Conversations%253A%2520A%2520Survey%26entry.906535625%3DShengyue%2520Guan%2520and%2520Jindong%2520Wang%2520and%2520Jiang%2520Bian%2520and%2520Bin%2520Zhu%2520and%2520Jian-guang%2520Lou%2520and%2520Haoyi%2520Xiong%26entry.1292438233%3DThis%2520survey%2520examines%2520evaluation%2520methods%2520for%2520large%2520language%2520model%2520%2528LLM%2529-based%2520agents%2520in%2520multi-turn%2520conversational%2520settings.%2520Using%2520a%2520PRISMA-inspired%2520framework%252C%2520we%2520systematically%2520reviewed%2520nearly%2520250%2520scholarly%2520sources%252C%2520capturing%2520the%2520state%2520of%2520the%2520art%2520from%2520various%2520venues%2520of%2520publication%252C%2520and%2520establishing%2520a%2520solid%2520foundation%2520for%2520our%2520analysis.%2520Our%2520study%2520offers%2520a%2520structured%2520approach%2520by%2520developing%2520two%2520interrelated%2520taxonomy%2520systems%253A%2520one%2520that%2520defines%2520%255Cemph%257Bwhat%2520to%2520evaluate%257D%2520and%2520another%2520that%2520explains%2520%255Cemph%257Bhow%2520to%2520evaluate%257D.%2520The%2520first%2520taxonomy%2520identifies%2520key%2520components%2520of%2520LLM-based%2520agents%2520for%2520multi-turn%2520conversations%2520and%2520their%2520evaluation%2520dimensions%252C%2520including%2520task%2520completion%252C%2520response%2520quality%252C%2520user%2520experience%252C%2520memory%2520and%2520context%2520retention%252C%2520as%2520well%2520as%2520planning%2520and%2520tool%2520integration.%2520These%2520components%2520ensure%2520that%2520the%2520performance%2520of%2520conversational%2520agents%2520is%2520assessed%2520in%2520a%2520holistic%2520and%2520meaningful%2520manner.%2520The%2520second%2520taxonomy%2520system%2520focuses%2520on%2520the%2520evaluation%2520methodologies.%2520It%2520categorizes%2520approaches%2520into%2520annotation-based%2520evaluations%252C%2520automated%2520metrics%252C%2520hybrid%2520strategies%2520that%2520combine%2520human%2520assessments%2520with%2520quantitative%2520measures%252C%2520and%2520self-judging%2520methods%2520utilizing%2520LLMs.%2520This%2520framework%2520not%2520only%2520captures%2520traditional%2520metrics%2520derived%2520from%2520language%2520understanding%252C%2520such%2520as%2520BLEU%2520and%2520ROUGE%2520scores%252C%2520but%2520also%2520incorporates%2520advanced%2520techniques%2520that%2520reflect%2520the%2520dynamic%252C%2520interactive%2520nature%2520of%2520multi-turn%2520dialogues.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.22458v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20LLM-based%20Agents%20for%20Multi-Turn%20Conversations%3A%20A%20Survey&entry.906535625=Shengyue%20Guan%20and%20Jindong%20Wang%20and%20Jiang%20Bian%20and%20Bin%20Zhu%20and%20Jian-guang%20Lou%20and%20Haoyi%20Xiong&entry.1292438233=This%20survey%20examines%20evaluation%20methods%20for%20large%20language%20model%20%28LLM%29-based%20agents%20in%20multi-turn%20conversational%20settings.%20Using%20a%20PRISMA-inspired%20framework%2C%20we%20systematically%20reviewed%20nearly%20250%20scholarly%20sources%2C%20capturing%20the%20state%20of%20the%20art%20from%20various%20venues%20of%20publication%2C%20and%20establishing%20a%20solid%20foundation%20for%20our%20analysis.%20Our%20study%20offers%20a%20structured%20approach%20by%20developing%20two%20interrelated%20taxonomy%20systems%3A%20one%20that%20defines%20%5Cemph%7Bwhat%20to%20evaluate%7D%20and%20another%20that%20explains%20%5Cemph%7Bhow%20to%20evaluate%7D.%20The%20first%20taxonomy%20identifies%20key%20components%20of%20LLM-based%20agents%20for%20multi-turn%20conversations%20and%20their%20evaluation%20dimensions%2C%20including%20task%20completion%2C%20response%20quality%2C%20user%20experience%2C%20memory%20and%20context%20retention%2C%20as%20well%20as%20planning%20and%20tool%20integration.%20These%20components%20ensure%20that%20the%20performance%20of%20conversational%20agents%20is%20assessed%20in%20a%20holistic%20and%20meaningful%20manner.%20The%20second%20taxonomy%20system%20focuses%20on%20the%20evaluation%20methodologies.%20It%20categorizes%20approaches%20into%20annotation-based%20evaluations%2C%20automated%20metrics%2C%20hybrid%20strategies%20that%20combine%20human%20assessments%20with%20quantitative%20measures%2C%20and%20self-judging%20methods%20utilizing%20LLMs.%20This%20framework%20not%20only%20captures%20traditional%20metrics%20derived%20from%20language%20understanding%2C%20such%20as%20BLEU%20and%20ROUGE%20scores%2C%20but%20also%20incorporates%20advanced%20techniques%20that%20reflect%20the%20dynamic%2C%20interactive%20nature%20of%20multi-turn%20dialogues.&entry.1838667208=http%3A//arxiv.org/abs/2503.22458v2&entry.124074799=Read"},
{"title": "Towards Fair In-Context Learning with Tabular Foundation Models", "author": "Patrik Kenfack and Samira Ebrahimi Kahou and Ulrich A\u00efvodji", "abstract": "Transformer-based tabular foundation models have recently demonstrated promising in-context learning (ICL) performance on structured data, emerging as competitive alternatives to gradient-boosted trees. However, the fairness implications of this new paradigm remain largely unexplored. We present the first investigation of fairness in tabular ICL, evaluating three recently proposed foundation models--TabPFNv2, TabICL, and TabDPT--on multiple benchmark datasets. To mitigate biases, we explore three pre-processing fairness-enhancing methods: correlation removal (decorrelating input features from the sensitive attribute), group-balanced sample selection (ensuring equal representation of protected groups in context examples), and uncertainty-based sample selection (prioritizing context examples with high sensitive-attribute prediction uncertainty). Our experiments show that the uncertainty-based strategy consistently improves group fairness metrics (e.g., demographic parity, equalized odds, and equal opportunity) with minimal impact on predictive accuracy. We release our code to facilitate reproducibility https://github.com/patrikken/Fair-TabICL.", "link": "http://arxiv.org/abs/2505.09503v4", "date": "2026-01-05", "relevancy": 2.3475, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4853}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4616}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Fair%20In-Context%20Learning%20with%20Tabular%20Foundation%20Models&body=Title%3A%20Towards%20Fair%20In-Context%20Learning%20with%20Tabular%20Foundation%20Models%0AAuthor%3A%20Patrik%20Kenfack%20and%20Samira%20Ebrahimi%20Kahou%20and%20Ulrich%20A%C3%AFvodji%0AAbstract%3A%20Transformer-based%20tabular%20foundation%20models%20have%20recently%20demonstrated%20promising%20in-context%20learning%20%28ICL%29%20performance%20on%20structured%20data%2C%20emerging%20as%20competitive%20alternatives%20to%20gradient-boosted%20trees.%20However%2C%20the%20fairness%20implications%20of%20this%20new%20paradigm%20remain%20largely%20unexplored.%20We%20present%20the%20first%20investigation%20of%20fairness%20in%20tabular%20ICL%2C%20evaluating%20three%20recently%20proposed%20foundation%20models--TabPFNv2%2C%20TabICL%2C%20and%20TabDPT--on%20multiple%20benchmark%20datasets.%20To%20mitigate%20biases%2C%20we%20explore%20three%20pre-processing%20fairness-enhancing%20methods%3A%20correlation%20removal%20%28decorrelating%20input%20features%20from%20the%20sensitive%20attribute%29%2C%20group-balanced%20sample%20selection%20%28ensuring%20equal%20representation%20of%20protected%20groups%20in%20context%20examples%29%2C%20and%20uncertainty-based%20sample%20selection%20%28prioritizing%20context%20examples%20with%20high%20sensitive-attribute%20prediction%20uncertainty%29.%20Our%20experiments%20show%20that%20the%20uncertainty-based%20strategy%20consistently%20improves%20group%20fairness%20metrics%20%28e.g.%2C%20demographic%20parity%2C%20equalized%20odds%2C%20and%20equal%20opportunity%29%20with%20minimal%20impact%20on%20predictive%20accuracy.%20We%20release%20our%20code%20to%20facilitate%20reproducibility%20https%3A//github.com/patrikken/Fair-TabICL.%0ALink%3A%20http%3A//arxiv.org/abs/2505.09503v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Fair%2520In-Context%2520Learning%2520with%2520Tabular%2520Foundation%2520Models%26entry.906535625%3DPatrik%2520Kenfack%2520and%2520Samira%2520Ebrahimi%2520Kahou%2520and%2520Ulrich%2520A%25C3%25AFvodji%26entry.1292438233%3DTransformer-based%2520tabular%2520foundation%2520models%2520have%2520recently%2520demonstrated%2520promising%2520in-context%2520learning%2520%2528ICL%2529%2520performance%2520on%2520structured%2520data%252C%2520emerging%2520as%2520competitive%2520alternatives%2520to%2520gradient-boosted%2520trees.%2520However%252C%2520the%2520fairness%2520implications%2520of%2520this%2520new%2520paradigm%2520remain%2520largely%2520unexplored.%2520We%2520present%2520the%2520first%2520investigation%2520of%2520fairness%2520in%2520tabular%2520ICL%252C%2520evaluating%2520three%2520recently%2520proposed%2520foundation%2520models--TabPFNv2%252C%2520TabICL%252C%2520and%2520TabDPT--on%2520multiple%2520benchmark%2520datasets.%2520To%2520mitigate%2520biases%252C%2520we%2520explore%2520three%2520pre-processing%2520fairness-enhancing%2520methods%253A%2520correlation%2520removal%2520%2528decorrelating%2520input%2520features%2520from%2520the%2520sensitive%2520attribute%2529%252C%2520group-balanced%2520sample%2520selection%2520%2528ensuring%2520equal%2520representation%2520of%2520protected%2520groups%2520in%2520context%2520examples%2529%252C%2520and%2520uncertainty-based%2520sample%2520selection%2520%2528prioritizing%2520context%2520examples%2520with%2520high%2520sensitive-attribute%2520prediction%2520uncertainty%2529.%2520Our%2520experiments%2520show%2520that%2520the%2520uncertainty-based%2520strategy%2520consistently%2520improves%2520group%2520fairness%2520metrics%2520%2528e.g.%252C%2520demographic%2520parity%252C%2520equalized%2520odds%252C%2520and%2520equal%2520opportunity%2529%2520with%2520minimal%2520impact%2520on%2520predictive%2520accuracy.%2520We%2520release%2520our%2520code%2520to%2520facilitate%2520reproducibility%2520https%253A//github.com/patrikken/Fair-TabICL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09503v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Fair%20In-Context%20Learning%20with%20Tabular%20Foundation%20Models&entry.906535625=Patrik%20Kenfack%20and%20Samira%20Ebrahimi%20Kahou%20and%20Ulrich%20A%C3%AFvodji&entry.1292438233=Transformer-based%20tabular%20foundation%20models%20have%20recently%20demonstrated%20promising%20in-context%20learning%20%28ICL%29%20performance%20on%20structured%20data%2C%20emerging%20as%20competitive%20alternatives%20to%20gradient-boosted%20trees.%20However%2C%20the%20fairness%20implications%20of%20this%20new%20paradigm%20remain%20largely%20unexplored.%20We%20present%20the%20first%20investigation%20of%20fairness%20in%20tabular%20ICL%2C%20evaluating%20three%20recently%20proposed%20foundation%20models--TabPFNv2%2C%20TabICL%2C%20and%20TabDPT--on%20multiple%20benchmark%20datasets.%20To%20mitigate%20biases%2C%20we%20explore%20three%20pre-processing%20fairness-enhancing%20methods%3A%20correlation%20removal%20%28decorrelating%20input%20features%20from%20the%20sensitive%20attribute%29%2C%20group-balanced%20sample%20selection%20%28ensuring%20equal%20representation%20of%20protected%20groups%20in%20context%20examples%29%2C%20and%20uncertainty-based%20sample%20selection%20%28prioritizing%20context%20examples%20with%20high%20sensitive-attribute%20prediction%20uncertainty%29.%20Our%20experiments%20show%20that%20the%20uncertainty-based%20strategy%20consistently%20improves%20group%20fairness%20metrics%20%28e.g.%2C%20demographic%20parity%2C%20equalized%20odds%2C%20and%20equal%20opportunity%29%20with%20minimal%20impact%20on%20predictive%20accuracy.%20We%20release%20our%20code%20to%20facilitate%20reproducibility%20https%3A//github.com/patrikken/Fair-TabICL.&entry.1838667208=http%3A//arxiv.org/abs/2505.09503v4&entry.124074799=Read"},
{"title": "CORE: Code-based Inverse Self-Training Framework with Graph Expansion for Virtual Agents", "author": "Keyu Wang and Bingchen Miao and Wendong Bu and Yu Wu and Juncheng Li and Shengyu Zhang and Wenqiao Zhang and Siliang Tang and Jun Xiao and Yueting Zhuang", "abstract": "The development of Multimodal Virtual Agents has made significant progress through the integration of Multimodal Large Language Models. However, mainstream training paradigms face key challenges: Behavior Cloning is simple and effective through imitation but suffers from low behavioral diversity, while Reinforcement Learning is capable of discovering novel strategies through exploration but heavily relies on manually designed reward functions. To address the conflict between these two methods, we present CORE, a Code-based Inverse Self-Training Framework with Graph Expansion that bridges imitation and exploration, offering a novel training framework that promotes behavioral diversity while eliminating the reliance on manually reward design. Specifically, we introduce Semantic Code Abstraction to automatically infers reward functions from expert demonstrations without manual design. The inferred reward function, referred to as the Label Function, is executable code that verifies one key step within a task. Building on this, we propose Strategy Graph Expansion to enhance in-domain behavioral diversity, which constructs a multi-path graph called Strategy Graph that captures diverse valid solutions beyond expert demonstrations. Furthermore, we introduce Trajectory-Guided Extrapolation, which enriches out-of-domain behavioral diversity by utilizing both successful and failed trajectories to expand the task space. Experiments on Web and Android platforms demonstrate that CORE significantly improves both overall performance and generalization, highlighting its potential as a robust and generalizable training paradigm for building powerful virtual agents.", "link": "http://arxiv.org/abs/2601.02201v1", "date": "2026-01-05", "relevancy": 2.3179, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5924}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5785}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CORE%3A%20Code-based%20Inverse%20Self-Training%20Framework%20with%20Graph%20Expansion%20for%20Virtual%20Agents&body=Title%3A%20CORE%3A%20Code-based%20Inverse%20Self-Training%20Framework%20with%20Graph%20Expansion%20for%20Virtual%20Agents%0AAuthor%3A%20Keyu%20Wang%20and%20Bingchen%20Miao%20and%20Wendong%20Bu%20and%20Yu%20Wu%20and%20Juncheng%20Li%20and%20Shengyu%20Zhang%20and%20Wenqiao%20Zhang%20and%20Siliang%20Tang%20and%20Jun%20Xiao%20and%20Yueting%20Zhuang%0AAbstract%3A%20The%20development%20of%20Multimodal%20Virtual%20Agents%20has%20made%20significant%20progress%20through%20the%20integration%20of%20Multimodal%20Large%20Language%20Models.%20However%2C%20mainstream%20training%20paradigms%20face%20key%20challenges%3A%20Behavior%20Cloning%20is%20simple%20and%20effective%20through%20imitation%20but%20suffers%20from%20low%20behavioral%20diversity%2C%20while%20Reinforcement%20Learning%20is%20capable%20of%20discovering%20novel%20strategies%20through%20exploration%20but%20heavily%20relies%20on%20manually%20designed%20reward%20functions.%20To%20address%20the%20conflict%20between%20these%20two%20methods%2C%20we%20present%20CORE%2C%20a%20Code-based%20Inverse%20Self-Training%20Framework%20with%20Graph%20Expansion%20that%20bridges%20imitation%20and%20exploration%2C%20offering%20a%20novel%20training%20framework%20that%20promotes%20behavioral%20diversity%20while%20eliminating%20the%20reliance%20on%20manually%20reward%20design.%20Specifically%2C%20we%20introduce%20Semantic%20Code%20Abstraction%20to%20automatically%20infers%20reward%20functions%20from%20expert%20demonstrations%20without%20manual%20design.%20The%20inferred%20reward%20function%2C%20referred%20to%20as%20the%20Label%20Function%2C%20is%20executable%20code%20that%20verifies%20one%20key%20step%20within%20a%20task.%20Building%20on%20this%2C%20we%20propose%20Strategy%20Graph%20Expansion%20to%20enhance%20in-domain%20behavioral%20diversity%2C%20which%20constructs%20a%20multi-path%20graph%20called%20Strategy%20Graph%20that%20captures%20diverse%20valid%20solutions%20beyond%20expert%20demonstrations.%20Furthermore%2C%20we%20introduce%20Trajectory-Guided%20Extrapolation%2C%20which%20enriches%20out-of-domain%20behavioral%20diversity%20by%20utilizing%20both%20successful%20and%20failed%20trajectories%20to%20expand%20the%20task%20space.%20Experiments%20on%20Web%20and%20Android%20platforms%20demonstrate%20that%20CORE%20significantly%20improves%20both%20overall%20performance%20and%20generalization%2C%20highlighting%20its%20potential%20as%20a%20robust%20and%20generalizable%20training%20paradigm%20for%20building%20powerful%20virtual%20agents.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02201v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCORE%253A%2520Code-based%2520Inverse%2520Self-Training%2520Framework%2520with%2520Graph%2520Expansion%2520for%2520Virtual%2520Agents%26entry.906535625%3DKeyu%2520Wang%2520and%2520Bingchen%2520Miao%2520and%2520Wendong%2520Bu%2520and%2520Yu%2520Wu%2520and%2520Juncheng%2520Li%2520and%2520Shengyu%2520Zhang%2520and%2520Wenqiao%2520Zhang%2520and%2520Siliang%2520Tang%2520and%2520Jun%2520Xiao%2520and%2520Yueting%2520Zhuang%26entry.1292438233%3DThe%2520development%2520of%2520Multimodal%2520Virtual%2520Agents%2520has%2520made%2520significant%2520progress%2520through%2520the%2520integration%2520of%2520Multimodal%2520Large%2520Language%2520Models.%2520However%252C%2520mainstream%2520training%2520paradigms%2520face%2520key%2520challenges%253A%2520Behavior%2520Cloning%2520is%2520simple%2520and%2520effective%2520through%2520imitation%2520but%2520suffers%2520from%2520low%2520behavioral%2520diversity%252C%2520while%2520Reinforcement%2520Learning%2520is%2520capable%2520of%2520discovering%2520novel%2520strategies%2520through%2520exploration%2520but%2520heavily%2520relies%2520on%2520manually%2520designed%2520reward%2520functions.%2520To%2520address%2520the%2520conflict%2520between%2520these%2520two%2520methods%252C%2520we%2520present%2520CORE%252C%2520a%2520Code-based%2520Inverse%2520Self-Training%2520Framework%2520with%2520Graph%2520Expansion%2520that%2520bridges%2520imitation%2520and%2520exploration%252C%2520offering%2520a%2520novel%2520training%2520framework%2520that%2520promotes%2520behavioral%2520diversity%2520while%2520eliminating%2520the%2520reliance%2520on%2520manually%2520reward%2520design.%2520Specifically%252C%2520we%2520introduce%2520Semantic%2520Code%2520Abstraction%2520to%2520automatically%2520infers%2520reward%2520functions%2520from%2520expert%2520demonstrations%2520without%2520manual%2520design.%2520The%2520inferred%2520reward%2520function%252C%2520referred%2520to%2520as%2520the%2520Label%2520Function%252C%2520is%2520executable%2520code%2520that%2520verifies%2520one%2520key%2520step%2520within%2520a%2520task.%2520Building%2520on%2520this%252C%2520we%2520propose%2520Strategy%2520Graph%2520Expansion%2520to%2520enhance%2520in-domain%2520behavioral%2520diversity%252C%2520which%2520constructs%2520a%2520multi-path%2520graph%2520called%2520Strategy%2520Graph%2520that%2520captures%2520diverse%2520valid%2520solutions%2520beyond%2520expert%2520demonstrations.%2520Furthermore%252C%2520we%2520introduce%2520Trajectory-Guided%2520Extrapolation%252C%2520which%2520enriches%2520out-of-domain%2520behavioral%2520diversity%2520by%2520utilizing%2520both%2520successful%2520and%2520failed%2520trajectories%2520to%2520expand%2520the%2520task%2520space.%2520Experiments%2520on%2520Web%2520and%2520Android%2520platforms%2520demonstrate%2520that%2520CORE%2520significantly%2520improves%2520both%2520overall%2520performance%2520and%2520generalization%252C%2520highlighting%2520its%2520potential%2520as%2520a%2520robust%2520and%2520generalizable%2520training%2520paradigm%2520for%2520building%2520powerful%2520virtual%2520agents.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02201v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CORE%3A%20Code-based%20Inverse%20Self-Training%20Framework%20with%20Graph%20Expansion%20for%20Virtual%20Agents&entry.906535625=Keyu%20Wang%20and%20Bingchen%20Miao%20and%20Wendong%20Bu%20and%20Yu%20Wu%20and%20Juncheng%20Li%20and%20Shengyu%20Zhang%20and%20Wenqiao%20Zhang%20and%20Siliang%20Tang%20and%20Jun%20Xiao%20and%20Yueting%20Zhuang&entry.1292438233=The%20development%20of%20Multimodal%20Virtual%20Agents%20has%20made%20significant%20progress%20through%20the%20integration%20of%20Multimodal%20Large%20Language%20Models.%20However%2C%20mainstream%20training%20paradigms%20face%20key%20challenges%3A%20Behavior%20Cloning%20is%20simple%20and%20effective%20through%20imitation%20but%20suffers%20from%20low%20behavioral%20diversity%2C%20while%20Reinforcement%20Learning%20is%20capable%20of%20discovering%20novel%20strategies%20through%20exploration%20but%20heavily%20relies%20on%20manually%20designed%20reward%20functions.%20To%20address%20the%20conflict%20between%20these%20two%20methods%2C%20we%20present%20CORE%2C%20a%20Code-based%20Inverse%20Self-Training%20Framework%20with%20Graph%20Expansion%20that%20bridges%20imitation%20and%20exploration%2C%20offering%20a%20novel%20training%20framework%20that%20promotes%20behavioral%20diversity%20while%20eliminating%20the%20reliance%20on%20manually%20reward%20design.%20Specifically%2C%20we%20introduce%20Semantic%20Code%20Abstraction%20to%20automatically%20infers%20reward%20functions%20from%20expert%20demonstrations%20without%20manual%20design.%20The%20inferred%20reward%20function%2C%20referred%20to%20as%20the%20Label%20Function%2C%20is%20executable%20code%20that%20verifies%20one%20key%20step%20within%20a%20task.%20Building%20on%20this%2C%20we%20propose%20Strategy%20Graph%20Expansion%20to%20enhance%20in-domain%20behavioral%20diversity%2C%20which%20constructs%20a%20multi-path%20graph%20called%20Strategy%20Graph%20that%20captures%20diverse%20valid%20solutions%20beyond%20expert%20demonstrations.%20Furthermore%2C%20we%20introduce%20Trajectory-Guided%20Extrapolation%2C%20which%20enriches%20out-of-domain%20behavioral%20diversity%20by%20utilizing%20both%20successful%20and%20failed%20trajectories%20to%20expand%20the%20task%20space.%20Experiments%20on%20Web%20and%20Android%20platforms%20demonstrate%20that%20CORE%20significantly%20improves%20both%20overall%20performance%20and%20generalization%2C%20highlighting%20its%20potential%20as%20a%20robust%20and%20generalizable%20training%20paradigm%20for%20building%20powerful%20virtual%20agents.&entry.1838667208=http%3A//arxiv.org/abs/2601.02201v1&entry.124074799=Read"},
{"title": "Inferring Network Evolutionary History via Structure-State Coupled Learning", "author": "En Xu and Shihe Zhou and Huandong Wang and Jingtao Ding and Yong Li", "abstract": "Inferring a network's evolutionary history from a single final snapshot with limited temporal annotations is fundamental yet challenging. Existing approaches predominantly rely on topology alone, which often provides insufficient and noisy cues. This paper leverages network steady-state dynamics -- converged node states under a given dynamical process -- as an additional and widely accessible observation for network evolution history inference. We propose CS$^2$, which explicitly models structure-state coupling to capture how topology modulates steady states and how the two signals jointly improve edge discrimination for formation-order recovery. Experiments on six real temporal networks, evaluated under multiple dynamical processes, show that CS$^2$ consistently outperforms strong baselines, improving pairwise edge precedence accuracy by 4.0% on average and global ordering consistency (Spearman-$\u03c1$) by 7.7% on average. CS$^2$ also more faithfully recovers macroscopic evolution trajectories such as clustering formation, degree heterogeneity, and hub growth. Moreover, a steady-state-only variant remains competitive when reliable topology is limited, highlighting steady states as an independent signal for evolution inference.", "link": "http://arxiv.org/abs/2601.02121v1", "date": "2026-01-05", "relevancy": 2.2967, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4931}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4436}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inferring%20Network%20Evolutionary%20History%20via%20Structure-State%20Coupled%20Learning&body=Title%3A%20Inferring%20Network%20Evolutionary%20History%20via%20Structure-State%20Coupled%20Learning%0AAuthor%3A%20En%20Xu%20and%20Shihe%20Zhou%20and%20Huandong%20Wang%20and%20Jingtao%20Ding%20and%20Yong%20Li%0AAbstract%3A%20Inferring%20a%20network%27s%20evolutionary%20history%20from%20a%20single%20final%20snapshot%20with%20limited%20temporal%20annotations%20is%20fundamental%20yet%20challenging.%20Existing%20approaches%20predominantly%20rely%20on%20topology%20alone%2C%20which%20often%20provides%20insufficient%20and%20noisy%20cues.%20This%20paper%20leverages%20network%20steady-state%20dynamics%20--%20converged%20node%20states%20under%20a%20given%20dynamical%20process%20--%20as%20an%20additional%20and%20widely%20accessible%20observation%20for%20network%20evolution%20history%20inference.%20We%20propose%20CS%24%5E2%24%2C%20which%20explicitly%20models%20structure-state%20coupling%20to%20capture%20how%20topology%20modulates%20steady%20states%20and%20how%20the%20two%20signals%20jointly%20improve%20edge%20discrimination%20for%20formation-order%20recovery.%20Experiments%20on%20six%20real%20temporal%20networks%2C%20evaluated%20under%20multiple%20dynamical%20processes%2C%20show%20that%20CS%24%5E2%24%20consistently%20outperforms%20strong%20baselines%2C%20improving%20pairwise%20edge%20precedence%20accuracy%20by%204.0%25%20on%20average%20and%20global%20ordering%20consistency%20%28Spearman-%24%CF%81%24%29%20by%207.7%25%20on%20average.%20CS%24%5E2%24%20also%20more%20faithfully%20recovers%20macroscopic%20evolution%20trajectories%20such%20as%20clustering%20formation%2C%20degree%20heterogeneity%2C%20and%20hub%20growth.%20Moreover%2C%20a%20steady-state-only%20variant%20remains%20competitive%20when%20reliable%20topology%20is%20limited%2C%20highlighting%20steady%20states%20as%20an%20independent%20signal%20for%20evolution%20inference.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02121v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInferring%2520Network%2520Evolutionary%2520History%2520via%2520Structure-State%2520Coupled%2520Learning%26entry.906535625%3DEn%2520Xu%2520and%2520Shihe%2520Zhou%2520and%2520Huandong%2520Wang%2520and%2520Jingtao%2520Ding%2520and%2520Yong%2520Li%26entry.1292438233%3DInferring%2520a%2520network%2527s%2520evolutionary%2520history%2520from%2520a%2520single%2520final%2520snapshot%2520with%2520limited%2520temporal%2520annotations%2520is%2520fundamental%2520yet%2520challenging.%2520Existing%2520approaches%2520predominantly%2520rely%2520on%2520topology%2520alone%252C%2520which%2520often%2520provides%2520insufficient%2520and%2520noisy%2520cues.%2520This%2520paper%2520leverages%2520network%2520steady-state%2520dynamics%2520--%2520converged%2520node%2520states%2520under%2520a%2520given%2520dynamical%2520process%2520--%2520as%2520an%2520additional%2520and%2520widely%2520accessible%2520observation%2520for%2520network%2520evolution%2520history%2520inference.%2520We%2520propose%2520CS%2524%255E2%2524%252C%2520which%2520explicitly%2520models%2520structure-state%2520coupling%2520to%2520capture%2520how%2520topology%2520modulates%2520steady%2520states%2520and%2520how%2520the%2520two%2520signals%2520jointly%2520improve%2520edge%2520discrimination%2520for%2520formation-order%2520recovery.%2520Experiments%2520on%2520six%2520real%2520temporal%2520networks%252C%2520evaluated%2520under%2520multiple%2520dynamical%2520processes%252C%2520show%2520that%2520CS%2524%255E2%2524%2520consistently%2520outperforms%2520strong%2520baselines%252C%2520improving%2520pairwise%2520edge%2520precedence%2520accuracy%2520by%25204.0%2525%2520on%2520average%2520and%2520global%2520ordering%2520consistency%2520%2528Spearman-%2524%25CF%2581%2524%2529%2520by%25207.7%2525%2520on%2520average.%2520CS%2524%255E2%2524%2520also%2520more%2520faithfully%2520recovers%2520macroscopic%2520evolution%2520trajectories%2520such%2520as%2520clustering%2520formation%252C%2520degree%2520heterogeneity%252C%2520and%2520hub%2520growth.%2520Moreover%252C%2520a%2520steady-state-only%2520variant%2520remains%2520competitive%2520when%2520reliable%2520topology%2520is%2520limited%252C%2520highlighting%2520steady%2520states%2520as%2520an%2520independent%2520signal%2520for%2520evolution%2520inference.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02121v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inferring%20Network%20Evolutionary%20History%20via%20Structure-State%20Coupled%20Learning&entry.906535625=En%20Xu%20and%20Shihe%20Zhou%20and%20Huandong%20Wang%20and%20Jingtao%20Ding%20and%20Yong%20Li&entry.1292438233=Inferring%20a%20network%27s%20evolutionary%20history%20from%20a%20single%20final%20snapshot%20with%20limited%20temporal%20annotations%20is%20fundamental%20yet%20challenging.%20Existing%20approaches%20predominantly%20rely%20on%20topology%20alone%2C%20which%20often%20provides%20insufficient%20and%20noisy%20cues.%20This%20paper%20leverages%20network%20steady-state%20dynamics%20--%20converged%20node%20states%20under%20a%20given%20dynamical%20process%20--%20as%20an%20additional%20and%20widely%20accessible%20observation%20for%20network%20evolution%20history%20inference.%20We%20propose%20CS%24%5E2%24%2C%20which%20explicitly%20models%20structure-state%20coupling%20to%20capture%20how%20topology%20modulates%20steady%20states%20and%20how%20the%20two%20signals%20jointly%20improve%20edge%20discrimination%20for%20formation-order%20recovery.%20Experiments%20on%20six%20real%20temporal%20networks%2C%20evaluated%20under%20multiple%20dynamical%20processes%2C%20show%20that%20CS%24%5E2%24%20consistently%20outperforms%20strong%20baselines%2C%20improving%20pairwise%20edge%20precedence%20accuracy%20by%204.0%25%20on%20average%20and%20global%20ordering%20consistency%20%28Spearman-%24%CF%81%24%29%20by%207.7%25%20on%20average.%20CS%24%5E2%24%20also%20more%20faithfully%20recovers%20macroscopic%20evolution%20trajectories%20such%20as%20clustering%20formation%2C%20degree%20heterogeneity%2C%20and%20hub%20growth.%20Moreover%2C%20a%20steady-state-only%20variant%20remains%20competitive%20when%20reliable%20topology%20is%20limited%2C%20highlighting%20steady%20states%20as%20an%20independent%20signal%20for%20evolution%20inference.&entry.1838667208=http%3A//arxiv.org/abs/2601.02121v1&entry.124074799=Read"},
{"title": "AlignVTOFF: Texture-Spatial Feature Alignment for High-Fidelity Virtual Try-Off", "author": "Yihan Zhu and Mengying Ge", "abstract": "Virtual Try-Off (VTOFF) is a challenging multimodal image generation task that aims to synthesize high-fidelity flat-lay garments under complex geometric deformation and rich high-frequency textures. Existing methods often rely on lightweight modules for fast feature extraction, which struggles to preserve structured patterns and fine-grained details, leading to texture attenuation during generation.To address these issues, we propose AlignVTOFF, a novel parallel U-Net framework built upon a Reference U-Net and Texture-Spatial Feature Alignment (TSFA). The Reference U-Net performs multi-scale feature extraction and enhances geometric fidelity, enabling robust modeling of deformation while retaining complex structured patterns. TSFA then injects the reference garment features into a frozen denoising U-Net via a hybrid attention design, consisting of a trainable cross-attention module and a frozen self-attention module. This design explicitly aligns texture and spatial cues and alleviates the loss of high-frequency information during the denoising process.Extensive experiments across multiple settings demonstrate that AlignVTOFF consistently outperforms state-of-the-art methods, producing flat-lay garment results with improved structural realism and high-frequency detail fidelity.", "link": "http://arxiv.org/abs/2601.02038v1", "date": "2026-01-05", "relevancy": 2.2966, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.599}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5595}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AlignVTOFF%3A%20Texture-Spatial%20Feature%20Alignment%20for%20High-Fidelity%20Virtual%20Try-Off&body=Title%3A%20AlignVTOFF%3A%20Texture-Spatial%20Feature%20Alignment%20for%20High-Fidelity%20Virtual%20Try-Off%0AAuthor%3A%20Yihan%20Zhu%20and%20Mengying%20Ge%0AAbstract%3A%20Virtual%20Try-Off%20%28VTOFF%29%20is%20a%20challenging%20multimodal%20image%20generation%20task%20that%20aims%20to%20synthesize%20high-fidelity%20flat-lay%20garments%20under%20complex%20geometric%20deformation%20and%20rich%20high-frequency%20textures.%20Existing%20methods%20often%20rely%20on%20lightweight%20modules%20for%20fast%20feature%20extraction%2C%20which%20struggles%20to%20preserve%20structured%20patterns%20and%20fine-grained%20details%2C%20leading%20to%20texture%20attenuation%20during%20generation.To%20address%20these%20issues%2C%20we%20propose%20AlignVTOFF%2C%20a%20novel%20parallel%20U-Net%20framework%20built%20upon%20a%20Reference%20U-Net%20and%20Texture-Spatial%20Feature%20Alignment%20%28TSFA%29.%20The%20Reference%20U-Net%20performs%20multi-scale%20feature%20extraction%20and%20enhances%20geometric%20fidelity%2C%20enabling%20robust%20modeling%20of%20deformation%20while%20retaining%20complex%20structured%20patterns.%20TSFA%20then%20injects%20the%20reference%20garment%20features%20into%20a%20frozen%20denoising%20U-Net%20via%20a%20hybrid%20attention%20design%2C%20consisting%20of%20a%20trainable%20cross-attention%20module%20and%20a%20frozen%20self-attention%20module.%20This%20design%20explicitly%20aligns%20texture%20and%20spatial%20cues%20and%20alleviates%20the%20loss%20of%20high-frequency%20information%20during%20the%20denoising%20process.Extensive%20experiments%20across%20multiple%20settings%20demonstrate%20that%20AlignVTOFF%20consistently%20outperforms%20state-of-the-art%20methods%2C%20producing%20flat-lay%20garment%20results%20with%20improved%20structural%20realism%20and%20high-frequency%20detail%20fidelity.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02038v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlignVTOFF%253A%2520Texture-Spatial%2520Feature%2520Alignment%2520for%2520High-Fidelity%2520Virtual%2520Try-Off%26entry.906535625%3DYihan%2520Zhu%2520and%2520Mengying%2520Ge%26entry.1292438233%3DVirtual%2520Try-Off%2520%2528VTOFF%2529%2520is%2520a%2520challenging%2520multimodal%2520image%2520generation%2520task%2520that%2520aims%2520to%2520synthesize%2520high-fidelity%2520flat-lay%2520garments%2520under%2520complex%2520geometric%2520deformation%2520and%2520rich%2520high-frequency%2520textures.%2520Existing%2520methods%2520often%2520rely%2520on%2520lightweight%2520modules%2520for%2520fast%2520feature%2520extraction%252C%2520which%2520struggles%2520to%2520preserve%2520structured%2520patterns%2520and%2520fine-grained%2520details%252C%2520leading%2520to%2520texture%2520attenuation%2520during%2520generation.To%2520address%2520these%2520issues%252C%2520we%2520propose%2520AlignVTOFF%252C%2520a%2520novel%2520parallel%2520U-Net%2520framework%2520built%2520upon%2520a%2520Reference%2520U-Net%2520and%2520Texture-Spatial%2520Feature%2520Alignment%2520%2528TSFA%2529.%2520The%2520Reference%2520U-Net%2520performs%2520multi-scale%2520feature%2520extraction%2520and%2520enhances%2520geometric%2520fidelity%252C%2520enabling%2520robust%2520modeling%2520of%2520deformation%2520while%2520retaining%2520complex%2520structured%2520patterns.%2520TSFA%2520then%2520injects%2520the%2520reference%2520garment%2520features%2520into%2520a%2520frozen%2520denoising%2520U-Net%2520via%2520a%2520hybrid%2520attention%2520design%252C%2520consisting%2520of%2520a%2520trainable%2520cross-attention%2520module%2520and%2520a%2520frozen%2520self-attention%2520module.%2520This%2520design%2520explicitly%2520aligns%2520texture%2520and%2520spatial%2520cues%2520and%2520alleviates%2520the%2520loss%2520of%2520high-frequency%2520information%2520during%2520the%2520denoising%2520process.Extensive%2520experiments%2520across%2520multiple%2520settings%2520demonstrate%2520that%2520AlignVTOFF%2520consistently%2520outperforms%2520state-of-the-art%2520methods%252C%2520producing%2520flat-lay%2520garment%2520results%2520with%2520improved%2520structural%2520realism%2520and%2520high-frequency%2520detail%2520fidelity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02038v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AlignVTOFF%3A%20Texture-Spatial%20Feature%20Alignment%20for%20High-Fidelity%20Virtual%20Try-Off&entry.906535625=Yihan%20Zhu%20and%20Mengying%20Ge&entry.1292438233=Virtual%20Try-Off%20%28VTOFF%29%20is%20a%20challenging%20multimodal%20image%20generation%20task%20that%20aims%20to%20synthesize%20high-fidelity%20flat-lay%20garments%20under%20complex%20geometric%20deformation%20and%20rich%20high-frequency%20textures.%20Existing%20methods%20often%20rely%20on%20lightweight%20modules%20for%20fast%20feature%20extraction%2C%20which%20struggles%20to%20preserve%20structured%20patterns%20and%20fine-grained%20details%2C%20leading%20to%20texture%20attenuation%20during%20generation.To%20address%20these%20issues%2C%20we%20propose%20AlignVTOFF%2C%20a%20novel%20parallel%20U-Net%20framework%20built%20upon%20a%20Reference%20U-Net%20and%20Texture-Spatial%20Feature%20Alignment%20%28TSFA%29.%20The%20Reference%20U-Net%20performs%20multi-scale%20feature%20extraction%20and%20enhances%20geometric%20fidelity%2C%20enabling%20robust%20modeling%20of%20deformation%20while%20retaining%20complex%20structured%20patterns.%20TSFA%20then%20injects%20the%20reference%20garment%20features%20into%20a%20frozen%20denoising%20U-Net%20via%20a%20hybrid%20attention%20design%2C%20consisting%20of%20a%20trainable%20cross-attention%20module%20and%20a%20frozen%20self-attention%20module.%20This%20design%20explicitly%20aligns%20texture%20and%20spatial%20cues%20and%20alleviates%20the%20loss%20of%20high-frequency%20information%20during%20the%20denoising%20process.Extensive%20experiments%20across%20multiple%20settings%20demonstrate%20that%20AlignVTOFF%20consistently%20outperforms%20state-of-the-art%20methods%2C%20producing%20flat-lay%20garment%20results%20with%20improved%20structural%20realism%20and%20high-frequency%20detail%20fidelity.&entry.1838667208=http%3A//arxiv.org/abs/2601.02038v1&entry.124074799=Read"},
{"title": "360DVO: Deep Visual Odometry for Monocular 360-Degree Camera", "author": "Xiaopeng Guo and Yinzhe Xu and Huajian Huang and Sai-Kit Yeung", "abstract": "Monocular omnidirectional visual odometry (OVO) systems leverage 360-degree cameras to overcome field-of-view limitations of perspective VO systems. However, existing methods, reliant on handcrafted features or photometric objectives, often lack robustness in challenging scenarios, such as aggressive motion and varying illumination. To address this, we present 360DVO, the first deep learning-based OVO framework. Our approach introduces a distortion-aware spherical feature extractor (DAS-Feat) that adaptively learns distortion-resistant features from 360-degree images. These sparse feature patches are then used to establish constraints for effective pose estimation within a novel omnidirectional differentiable bundle adjustment (ODBA) module. To facilitate evaluation in realistic settings, we also contribute a new real-world OVO benchmark. Extensive experiments on this benchmark and public synthetic datasets (TartanAir V2 and 360VO) demonstrate that 360DVO surpasses state-of-the-art baselines (including 360VO and OpenVSLAM), improving robustness by 50% and accuracy by 37.5%. Homepage: https://chris1004336379.github.io/360DVO-homepage", "link": "http://arxiv.org/abs/2601.02309v1", "date": "2026-01-05", "relevancy": 2.2919, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6092}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5692}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20360DVO%3A%20Deep%20Visual%20Odometry%20for%20Monocular%20360-Degree%20Camera&body=Title%3A%20360DVO%3A%20Deep%20Visual%20Odometry%20for%20Monocular%20360-Degree%20Camera%0AAuthor%3A%20Xiaopeng%20Guo%20and%20Yinzhe%20Xu%20and%20Huajian%20Huang%20and%20Sai-Kit%20Yeung%0AAbstract%3A%20Monocular%20omnidirectional%20visual%20odometry%20%28OVO%29%20systems%20leverage%20360-degree%20cameras%20to%20overcome%20field-of-view%20limitations%20of%20perspective%20VO%20systems.%20However%2C%20existing%20methods%2C%20reliant%20on%20handcrafted%20features%20or%20photometric%20objectives%2C%20often%20lack%20robustness%20in%20challenging%20scenarios%2C%20such%20as%20aggressive%20motion%20and%20varying%20illumination.%20To%20address%20this%2C%20we%20present%20360DVO%2C%20the%20first%20deep%20learning-based%20OVO%20framework.%20Our%20approach%20introduces%20a%20distortion-aware%20spherical%20feature%20extractor%20%28DAS-Feat%29%20that%20adaptively%20learns%20distortion-resistant%20features%20from%20360-degree%20images.%20These%20sparse%20feature%20patches%20are%20then%20used%20to%20establish%20constraints%20for%20effective%20pose%20estimation%20within%20a%20novel%20omnidirectional%20differentiable%20bundle%20adjustment%20%28ODBA%29%20module.%20To%20facilitate%20evaluation%20in%20realistic%20settings%2C%20we%20also%20contribute%20a%20new%20real-world%20OVO%20benchmark.%20Extensive%20experiments%20on%20this%20benchmark%20and%20public%20synthetic%20datasets%20%28TartanAir%20V2%20and%20360VO%29%20demonstrate%20that%20360DVO%20surpasses%20state-of-the-art%20baselines%20%28including%20360VO%20and%20OpenVSLAM%29%2C%20improving%20robustness%20by%2050%25%20and%20accuracy%20by%2037.5%25.%20Homepage%3A%20https%3A//chris1004336379.github.io/360DVO-homepage%0ALink%3A%20http%3A//arxiv.org/abs/2601.02309v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D360DVO%253A%2520Deep%2520Visual%2520Odometry%2520for%2520Monocular%2520360-Degree%2520Camera%26entry.906535625%3DXiaopeng%2520Guo%2520and%2520Yinzhe%2520Xu%2520and%2520Huajian%2520Huang%2520and%2520Sai-Kit%2520Yeung%26entry.1292438233%3DMonocular%2520omnidirectional%2520visual%2520odometry%2520%2528OVO%2529%2520systems%2520leverage%2520360-degree%2520cameras%2520to%2520overcome%2520field-of-view%2520limitations%2520of%2520perspective%2520VO%2520systems.%2520However%252C%2520existing%2520methods%252C%2520reliant%2520on%2520handcrafted%2520features%2520or%2520photometric%2520objectives%252C%2520often%2520lack%2520robustness%2520in%2520challenging%2520scenarios%252C%2520such%2520as%2520aggressive%2520motion%2520and%2520varying%2520illumination.%2520To%2520address%2520this%252C%2520we%2520present%2520360DVO%252C%2520the%2520first%2520deep%2520learning-based%2520OVO%2520framework.%2520Our%2520approach%2520introduces%2520a%2520distortion-aware%2520spherical%2520feature%2520extractor%2520%2528DAS-Feat%2529%2520that%2520adaptively%2520learns%2520distortion-resistant%2520features%2520from%2520360-degree%2520images.%2520These%2520sparse%2520feature%2520patches%2520are%2520then%2520used%2520to%2520establish%2520constraints%2520for%2520effective%2520pose%2520estimation%2520within%2520a%2520novel%2520omnidirectional%2520differentiable%2520bundle%2520adjustment%2520%2528ODBA%2529%2520module.%2520To%2520facilitate%2520evaluation%2520in%2520realistic%2520settings%252C%2520we%2520also%2520contribute%2520a%2520new%2520real-world%2520OVO%2520benchmark.%2520Extensive%2520experiments%2520on%2520this%2520benchmark%2520and%2520public%2520synthetic%2520datasets%2520%2528TartanAir%2520V2%2520and%2520360VO%2529%2520demonstrate%2520that%2520360DVO%2520surpasses%2520state-of-the-art%2520baselines%2520%2528including%2520360VO%2520and%2520OpenVSLAM%2529%252C%2520improving%2520robustness%2520by%252050%2525%2520and%2520accuracy%2520by%252037.5%2525.%2520Homepage%253A%2520https%253A//chris1004336379.github.io/360DVO-homepage%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02309v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=360DVO%3A%20Deep%20Visual%20Odometry%20for%20Monocular%20360-Degree%20Camera&entry.906535625=Xiaopeng%20Guo%20and%20Yinzhe%20Xu%20and%20Huajian%20Huang%20and%20Sai-Kit%20Yeung&entry.1292438233=Monocular%20omnidirectional%20visual%20odometry%20%28OVO%29%20systems%20leverage%20360-degree%20cameras%20to%20overcome%20field-of-view%20limitations%20of%20perspective%20VO%20systems.%20However%2C%20existing%20methods%2C%20reliant%20on%20handcrafted%20features%20or%20photometric%20objectives%2C%20often%20lack%20robustness%20in%20challenging%20scenarios%2C%20such%20as%20aggressive%20motion%20and%20varying%20illumination.%20To%20address%20this%2C%20we%20present%20360DVO%2C%20the%20first%20deep%20learning-based%20OVO%20framework.%20Our%20approach%20introduces%20a%20distortion-aware%20spherical%20feature%20extractor%20%28DAS-Feat%29%20that%20adaptively%20learns%20distortion-resistant%20features%20from%20360-degree%20images.%20These%20sparse%20feature%20patches%20are%20then%20used%20to%20establish%20constraints%20for%20effective%20pose%20estimation%20within%20a%20novel%20omnidirectional%20differentiable%20bundle%20adjustment%20%28ODBA%29%20module.%20To%20facilitate%20evaluation%20in%20realistic%20settings%2C%20we%20also%20contribute%20a%20new%20real-world%20OVO%20benchmark.%20Extensive%20experiments%20on%20this%20benchmark%20and%20public%20synthetic%20datasets%20%28TartanAir%20V2%20and%20360VO%29%20demonstrate%20that%20360DVO%20surpasses%20state-of-the-art%20baselines%20%28including%20360VO%20and%20OpenVSLAM%29%2C%20improving%20robustness%20by%2050%25%20and%20accuracy%20by%2037.5%25.%20Homepage%3A%20https%3A//chris1004336379.github.io/360DVO-homepage&entry.1838667208=http%3A//arxiv.org/abs/2601.02309v1&entry.124074799=Read"},
{"title": "Volume-Consistent Kneading-Based Deformation Manufacturing for Material-Efficient Shaping", "author": "Lei Li and Jiale Gong and Ziyang Li and Hong Wang", "abstract": "Conventional subtractive manufacturing inevitably involves material loss during geometric realization, while additive manufacturing still suffers from limitations in surface quality, process continuity, and productivity when fabricating complex geometries. To address these challenges, this paper proposes a volume-consistent kneading-based forming method for plastic materials, enabling continuous and controllable three-dimensional deformation under mass conservation. An integrated kneading-based manufacturing system is developed, in which geometry-aware kneading command generation, layer-wise kneading execution, and in-process point-cloud scanning are tightly coupled to form a closed-loop workflow of scanning, forming, and feedback compensation. Target geometries are analyzed through layer-wise point-cloud processing and classified into enveloping and non-enveloping types. Accordingly, an Envelope Shaping First strategy and a Similar Gradient Method are adopted to ensure stable material flow and continuous deformation. An RMSE-based compensation scheme is further introduced to correct systematic geometric deviations induced by elastic rebound and material redistribution. Experimental validation on five representative geometries demonstrates high geometric fidelity, with material utilization consistently exceeding 98%. The results indicate that kneading-based forming provides a promising alternative manufacturing paradigm for low-waste, customizable production.", "link": "http://arxiv.org/abs/2511.22042v2", "date": "2026-01-05", "relevancy": 2.2735, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.4596}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.457}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Volume-Consistent%20Kneading-Based%20Deformation%20Manufacturing%20for%20Material-Efficient%20Shaping&body=Title%3A%20Volume-Consistent%20Kneading-Based%20Deformation%20Manufacturing%20for%20Material-Efficient%20Shaping%0AAuthor%3A%20Lei%20Li%20and%20Jiale%20Gong%20and%20Ziyang%20Li%20and%20Hong%20Wang%0AAbstract%3A%20Conventional%20subtractive%20manufacturing%20inevitably%20involves%20material%20loss%20during%20geometric%20realization%2C%20while%20additive%20manufacturing%20still%20suffers%20from%20limitations%20in%20surface%20quality%2C%20process%20continuity%2C%20and%20productivity%20when%20fabricating%20complex%20geometries.%20To%20address%20these%20challenges%2C%20this%20paper%20proposes%20a%20volume-consistent%20kneading-based%20forming%20method%20for%20plastic%20materials%2C%20enabling%20continuous%20and%20controllable%20three-dimensional%20deformation%20under%20mass%20conservation.%20An%20integrated%20kneading-based%20manufacturing%20system%20is%20developed%2C%20in%20which%20geometry-aware%20kneading%20command%20generation%2C%20layer-wise%20kneading%20execution%2C%20and%20in-process%20point-cloud%20scanning%20are%20tightly%20coupled%20to%20form%20a%20closed-loop%20workflow%20of%20scanning%2C%20forming%2C%20and%20feedback%20compensation.%20Target%20geometries%20are%20analyzed%20through%20layer-wise%20point-cloud%20processing%20and%20classified%20into%20enveloping%20and%20non-enveloping%20types.%20Accordingly%2C%20an%20Envelope%20Shaping%20First%20strategy%20and%20a%20Similar%20Gradient%20Method%20are%20adopted%20to%20ensure%20stable%20material%20flow%20and%20continuous%20deformation.%20An%20RMSE-based%20compensation%20scheme%20is%20further%20introduced%20to%20correct%20systematic%20geometric%20deviations%20induced%20by%20elastic%20rebound%20and%20material%20redistribution.%20Experimental%20validation%20on%20five%20representative%20geometries%20demonstrates%20high%20geometric%20fidelity%2C%20with%20material%20utilization%20consistently%20exceeding%2098%25.%20The%20results%20indicate%20that%20kneading-based%20forming%20provides%20a%20promising%20alternative%20manufacturing%20paradigm%20for%20low-waste%2C%20customizable%20production.%0ALink%3A%20http%3A//arxiv.org/abs/2511.22042v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVolume-Consistent%2520Kneading-Based%2520Deformation%2520Manufacturing%2520for%2520Material-Efficient%2520Shaping%26entry.906535625%3DLei%2520Li%2520and%2520Jiale%2520Gong%2520and%2520Ziyang%2520Li%2520and%2520Hong%2520Wang%26entry.1292438233%3DConventional%2520subtractive%2520manufacturing%2520inevitably%2520involves%2520material%2520loss%2520during%2520geometric%2520realization%252C%2520while%2520additive%2520manufacturing%2520still%2520suffers%2520from%2520limitations%2520in%2520surface%2520quality%252C%2520process%2520continuity%252C%2520and%2520productivity%2520when%2520fabricating%2520complex%2520geometries.%2520To%2520address%2520these%2520challenges%252C%2520this%2520paper%2520proposes%2520a%2520volume-consistent%2520kneading-based%2520forming%2520method%2520for%2520plastic%2520materials%252C%2520enabling%2520continuous%2520and%2520controllable%2520three-dimensional%2520deformation%2520under%2520mass%2520conservation.%2520An%2520integrated%2520kneading-based%2520manufacturing%2520system%2520is%2520developed%252C%2520in%2520which%2520geometry-aware%2520kneading%2520command%2520generation%252C%2520layer-wise%2520kneading%2520execution%252C%2520and%2520in-process%2520point-cloud%2520scanning%2520are%2520tightly%2520coupled%2520to%2520form%2520a%2520closed-loop%2520workflow%2520of%2520scanning%252C%2520forming%252C%2520and%2520feedback%2520compensation.%2520Target%2520geometries%2520are%2520analyzed%2520through%2520layer-wise%2520point-cloud%2520processing%2520and%2520classified%2520into%2520enveloping%2520and%2520non-enveloping%2520types.%2520Accordingly%252C%2520an%2520Envelope%2520Shaping%2520First%2520strategy%2520and%2520a%2520Similar%2520Gradient%2520Method%2520are%2520adopted%2520to%2520ensure%2520stable%2520material%2520flow%2520and%2520continuous%2520deformation.%2520An%2520RMSE-based%2520compensation%2520scheme%2520is%2520further%2520introduced%2520to%2520correct%2520systematic%2520geometric%2520deviations%2520induced%2520by%2520elastic%2520rebound%2520and%2520material%2520redistribution.%2520Experimental%2520validation%2520on%2520five%2520representative%2520geometries%2520demonstrates%2520high%2520geometric%2520fidelity%252C%2520with%2520material%2520utilization%2520consistently%2520exceeding%252098%2525.%2520The%2520results%2520indicate%2520that%2520kneading-based%2520forming%2520provides%2520a%2520promising%2520alternative%2520manufacturing%2520paradigm%2520for%2520low-waste%252C%2520customizable%2520production.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.22042v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Volume-Consistent%20Kneading-Based%20Deformation%20Manufacturing%20for%20Material-Efficient%20Shaping&entry.906535625=Lei%20Li%20and%20Jiale%20Gong%20and%20Ziyang%20Li%20and%20Hong%20Wang&entry.1292438233=Conventional%20subtractive%20manufacturing%20inevitably%20involves%20material%20loss%20during%20geometric%20realization%2C%20while%20additive%20manufacturing%20still%20suffers%20from%20limitations%20in%20surface%20quality%2C%20process%20continuity%2C%20and%20productivity%20when%20fabricating%20complex%20geometries.%20To%20address%20these%20challenges%2C%20this%20paper%20proposes%20a%20volume-consistent%20kneading-based%20forming%20method%20for%20plastic%20materials%2C%20enabling%20continuous%20and%20controllable%20three-dimensional%20deformation%20under%20mass%20conservation.%20An%20integrated%20kneading-based%20manufacturing%20system%20is%20developed%2C%20in%20which%20geometry-aware%20kneading%20command%20generation%2C%20layer-wise%20kneading%20execution%2C%20and%20in-process%20point-cloud%20scanning%20are%20tightly%20coupled%20to%20form%20a%20closed-loop%20workflow%20of%20scanning%2C%20forming%2C%20and%20feedback%20compensation.%20Target%20geometries%20are%20analyzed%20through%20layer-wise%20point-cloud%20processing%20and%20classified%20into%20enveloping%20and%20non-enveloping%20types.%20Accordingly%2C%20an%20Envelope%20Shaping%20First%20strategy%20and%20a%20Similar%20Gradient%20Method%20are%20adopted%20to%20ensure%20stable%20material%20flow%20and%20continuous%20deformation.%20An%20RMSE-based%20compensation%20scheme%20is%20further%20introduced%20to%20correct%20systematic%20geometric%20deviations%20induced%20by%20elastic%20rebound%20and%20material%20redistribution.%20Experimental%20validation%20on%20five%20representative%20geometries%20demonstrates%20high%20geometric%20fidelity%2C%20with%20material%20utilization%20consistently%20exceeding%2098%25.%20The%20results%20indicate%20that%20kneading-based%20forming%20provides%20a%20promising%20alternative%20manufacturing%20paradigm%20for%20low-waste%2C%20customizable%20production.&entry.1838667208=http%3A//arxiv.org/abs/2511.22042v2&entry.124074799=Read"},
{"title": "ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors", "author": "Kaede Shiohara and Toshihiko Yamasaki and Vladislav Golyanik", "abstract": "Detecting unknown deepfake manipulations remains one of the most challenging problems in face forgery detection. Current state-of-the-art approaches fail to generalize to unseen manipulations, as they primarily rely on supervised training with existing deepfakes or pseudo-fakes, which leads to overfitting to specific forgery patterns. In contrast, self-supervised methods offer greater potential for generalization, but existing work struggles to learn discriminative representations only from self-supervision. In this paper, we propose ExposeAnyone, a fully self-supervised approach based on a diffusion model that generates expression sequences from audio. The key idea is, once the model is personalized to specific subjects using reference sets, it can compute the identity distances between suspected videos and personalized subjects via diffusion reconstruction errors, enabling person-of-interest face forgery detection. Extensive experiments demonstrate that 1) our method outperforms the previous state-of-the-art method by 4.22 percentage points in the average AUC on DF-TIMIT, DFDCP, KoDF, and IDForge datasets, 2) our model is also capable of detecting Sora2-generated videos, where the previous approaches perform poorly, and 3) our method is highly robust to corruptions such as blur and compression, highlighting the applicability in real-world face forgery detection.", "link": "http://arxiv.org/abs/2601.02359v1", "date": "2026-01-05", "relevancy": 2.271, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5986}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5596}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ExposeAnyone%3A%20Personalized%20Audio-to-Expression%20Diffusion%20Models%20Are%20Robust%20Zero-Shot%20Face%20Forgery%20Detectors&body=Title%3A%20ExposeAnyone%3A%20Personalized%20Audio-to-Expression%20Diffusion%20Models%20Are%20Robust%20Zero-Shot%20Face%20Forgery%20Detectors%0AAuthor%3A%20Kaede%20Shiohara%20and%20Toshihiko%20Yamasaki%20and%20Vladislav%20Golyanik%0AAbstract%3A%20Detecting%20unknown%20deepfake%20manipulations%20remains%20one%20of%20the%20most%20challenging%20problems%20in%20face%20forgery%20detection.%20Current%20state-of-the-art%20approaches%20fail%20to%20generalize%20to%20unseen%20manipulations%2C%20as%20they%20primarily%20rely%20on%20supervised%20training%20with%20existing%20deepfakes%20or%20pseudo-fakes%2C%20which%20leads%20to%20overfitting%20to%20specific%20forgery%20patterns.%20In%20contrast%2C%20self-supervised%20methods%20offer%20greater%20potential%20for%20generalization%2C%20but%20existing%20work%20struggles%20to%20learn%20discriminative%20representations%20only%20from%20self-supervision.%20In%20this%20paper%2C%20we%20propose%20ExposeAnyone%2C%20a%20fully%20self-supervised%20approach%20based%20on%20a%20diffusion%20model%20that%20generates%20expression%20sequences%20from%20audio.%20The%20key%20idea%20is%2C%20once%20the%20model%20is%20personalized%20to%20specific%20subjects%20using%20reference%20sets%2C%20it%20can%20compute%20the%20identity%20distances%20between%20suspected%20videos%20and%20personalized%20subjects%20via%20diffusion%20reconstruction%20errors%2C%20enabling%20person-of-interest%20face%20forgery%20detection.%20Extensive%20experiments%20demonstrate%20that%201%29%20our%20method%20outperforms%20the%20previous%20state-of-the-art%20method%20by%204.22%20percentage%20points%20in%20the%20average%20AUC%20on%20DF-TIMIT%2C%20DFDCP%2C%20KoDF%2C%20and%20IDForge%20datasets%2C%202%29%20our%20model%20is%20also%20capable%20of%20detecting%20Sora2-generated%20videos%2C%20where%20the%20previous%20approaches%20perform%20poorly%2C%20and%203%29%20our%20method%20is%20highly%20robust%20to%20corruptions%20such%20as%20blur%20and%20compression%2C%20highlighting%20the%20applicability%20in%20real-world%20face%20forgery%20detection.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02359v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExposeAnyone%253A%2520Personalized%2520Audio-to-Expression%2520Diffusion%2520Models%2520Are%2520Robust%2520Zero-Shot%2520Face%2520Forgery%2520Detectors%26entry.906535625%3DKaede%2520Shiohara%2520and%2520Toshihiko%2520Yamasaki%2520and%2520Vladislav%2520Golyanik%26entry.1292438233%3DDetecting%2520unknown%2520deepfake%2520manipulations%2520remains%2520one%2520of%2520the%2520most%2520challenging%2520problems%2520in%2520face%2520forgery%2520detection.%2520Current%2520state-of-the-art%2520approaches%2520fail%2520to%2520generalize%2520to%2520unseen%2520manipulations%252C%2520as%2520they%2520primarily%2520rely%2520on%2520supervised%2520training%2520with%2520existing%2520deepfakes%2520or%2520pseudo-fakes%252C%2520which%2520leads%2520to%2520overfitting%2520to%2520specific%2520forgery%2520patterns.%2520In%2520contrast%252C%2520self-supervised%2520methods%2520offer%2520greater%2520potential%2520for%2520generalization%252C%2520but%2520existing%2520work%2520struggles%2520to%2520learn%2520discriminative%2520representations%2520only%2520from%2520self-supervision.%2520In%2520this%2520paper%252C%2520we%2520propose%2520ExposeAnyone%252C%2520a%2520fully%2520self-supervised%2520approach%2520based%2520on%2520a%2520diffusion%2520model%2520that%2520generates%2520expression%2520sequences%2520from%2520audio.%2520The%2520key%2520idea%2520is%252C%2520once%2520the%2520model%2520is%2520personalized%2520to%2520specific%2520subjects%2520using%2520reference%2520sets%252C%2520it%2520can%2520compute%2520the%2520identity%2520distances%2520between%2520suspected%2520videos%2520and%2520personalized%2520subjects%2520via%2520diffusion%2520reconstruction%2520errors%252C%2520enabling%2520person-of-interest%2520face%2520forgery%2520detection.%2520Extensive%2520experiments%2520demonstrate%2520that%25201%2529%2520our%2520method%2520outperforms%2520the%2520previous%2520state-of-the-art%2520method%2520by%25204.22%2520percentage%2520points%2520in%2520the%2520average%2520AUC%2520on%2520DF-TIMIT%252C%2520DFDCP%252C%2520KoDF%252C%2520and%2520IDForge%2520datasets%252C%25202%2529%2520our%2520model%2520is%2520also%2520capable%2520of%2520detecting%2520Sora2-generated%2520videos%252C%2520where%2520the%2520previous%2520approaches%2520perform%2520poorly%252C%2520and%25203%2529%2520our%2520method%2520is%2520highly%2520robust%2520to%2520corruptions%2520such%2520as%2520blur%2520and%2520compression%252C%2520highlighting%2520the%2520applicability%2520in%2520real-world%2520face%2520forgery%2520detection.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02359v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ExposeAnyone%3A%20Personalized%20Audio-to-Expression%20Diffusion%20Models%20Are%20Robust%20Zero-Shot%20Face%20Forgery%20Detectors&entry.906535625=Kaede%20Shiohara%20and%20Toshihiko%20Yamasaki%20and%20Vladislav%20Golyanik&entry.1292438233=Detecting%20unknown%20deepfake%20manipulations%20remains%20one%20of%20the%20most%20challenging%20problems%20in%20face%20forgery%20detection.%20Current%20state-of-the-art%20approaches%20fail%20to%20generalize%20to%20unseen%20manipulations%2C%20as%20they%20primarily%20rely%20on%20supervised%20training%20with%20existing%20deepfakes%20or%20pseudo-fakes%2C%20which%20leads%20to%20overfitting%20to%20specific%20forgery%20patterns.%20In%20contrast%2C%20self-supervised%20methods%20offer%20greater%20potential%20for%20generalization%2C%20but%20existing%20work%20struggles%20to%20learn%20discriminative%20representations%20only%20from%20self-supervision.%20In%20this%20paper%2C%20we%20propose%20ExposeAnyone%2C%20a%20fully%20self-supervised%20approach%20based%20on%20a%20diffusion%20model%20that%20generates%20expression%20sequences%20from%20audio.%20The%20key%20idea%20is%2C%20once%20the%20model%20is%20personalized%20to%20specific%20subjects%20using%20reference%20sets%2C%20it%20can%20compute%20the%20identity%20distances%20between%20suspected%20videos%20and%20personalized%20subjects%20via%20diffusion%20reconstruction%20errors%2C%20enabling%20person-of-interest%20face%20forgery%20detection.%20Extensive%20experiments%20demonstrate%20that%201%29%20our%20method%20outperforms%20the%20previous%20state-of-the-art%20method%20by%204.22%20percentage%20points%20in%20the%20average%20AUC%20on%20DF-TIMIT%2C%20DFDCP%2C%20KoDF%2C%20and%20IDForge%20datasets%2C%202%29%20our%20model%20is%20also%20capable%20of%20detecting%20Sora2-generated%20videos%2C%20where%20the%20previous%20approaches%20perform%20poorly%2C%20and%203%29%20our%20method%20is%20highly%20robust%20to%20corruptions%20such%20as%20blur%20and%20compression%2C%20highlighting%20the%20applicability%20in%20real-world%20face%20forgery%20detection.&entry.1838667208=http%3A//arxiv.org/abs/2601.02359v1&entry.124074799=Read"},
{"title": "Seeing the Unseen: Zooming in the Dark with Event Cameras", "author": "Dachun Kai and Zeyu Xiao and Huyue Zhu and Jiaxiao Wang and Yueyi Zhang and Xiaoyan Sun", "abstract": "This paper addresses low-light video super-resolution (LVSR), aiming to restore high-resolution videos from low-light, low-resolution (LR) inputs. Existing LVSR methods often struggle to recover fine details due to limited contrast and insufficient high-frequency information. To overcome these challenges, we present RetinexEVSR, the first event-driven LVSR framework that leverages high-contrast event signals and Retinex-inspired priors to enhance video quality under low-light scenarios. Unlike previous approaches that directly fuse degraded signals, RetinexEVSR introduces a novel bidirectional cross-modal fusion strategy to extract and integrate meaningful cues from noisy event data and degraded RGB frames. Specifically, an illumination-guided event enhancement module is designed to progressively refine event features using illumination maps derived from the Retinex model, thereby suppressing low-light artifacts while preserving high-contrast details. Furthermore, we propose an event-guided reflectance enhancement module that utilizes the enhanced event features to dynamically recover reflectance details via a multi-scale fusion mechanism. Experimental results show that our RetinexEVSR achieves state-of-the-art performance on three datasets. Notably, on the SDSD benchmark, our method can get up to 2.95 dB gain while reducing runtime by 65% compared to prior event-based methods. Code: https://github.com/DachunKai/RetinexEVSR.", "link": "http://arxiv.org/abs/2601.02206v1", "date": "2026-01-05", "relevancy": 2.2642, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5757}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5748}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%20the%20Unseen%3A%20Zooming%20in%20the%20Dark%20with%20Event%20Cameras&body=Title%3A%20Seeing%20the%20Unseen%3A%20Zooming%20in%20the%20Dark%20with%20Event%20Cameras%0AAuthor%3A%20Dachun%20Kai%20and%20Zeyu%20Xiao%20and%20Huyue%20Zhu%20and%20Jiaxiao%20Wang%20and%20Yueyi%20Zhang%20and%20Xiaoyan%20Sun%0AAbstract%3A%20This%20paper%20addresses%20low-light%20video%20super-resolution%20%28LVSR%29%2C%20aiming%20to%20restore%20high-resolution%20videos%20from%20low-light%2C%20low-resolution%20%28LR%29%20inputs.%20Existing%20LVSR%20methods%20often%20struggle%20to%20recover%20fine%20details%20due%20to%20limited%20contrast%20and%20insufficient%20high-frequency%20information.%20To%20overcome%20these%20challenges%2C%20we%20present%20RetinexEVSR%2C%20the%20first%20event-driven%20LVSR%20framework%20that%20leverages%20high-contrast%20event%20signals%20and%20Retinex-inspired%20priors%20to%20enhance%20video%20quality%20under%20low-light%20scenarios.%20Unlike%20previous%20approaches%20that%20directly%20fuse%20degraded%20signals%2C%20RetinexEVSR%20introduces%20a%20novel%20bidirectional%20cross-modal%20fusion%20strategy%20to%20extract%20and%20integrate%20meaningful%20cues%20from%20noisy%20event%20data%20and%20degraded%20RGB%20frames.%20Specifically%2C%20an%20illumination-guided%20event%20enhancement%20module%20is%20designed%20to%20progressively%20refine%20event%20features%20using%20illumination%20maps%20derived%20from%20the%20Retinex%20model%2C%20thereby%20suppressing%20low-light%20artifacts%20while%20preserving%20high-contrast%20details.%20Furthermore%2C%20we%20propose%20an%20event-guided%20reflectance%20enhancement%20module%20that%20utilizes%20the%20enhanced%20event%20features%20to%20dynamically%20recover%20reflectance%20details%20via%20a%20multi-scale%20fusion%20mechanism.%20Experimental%20results%20show%20that%20our%20RetinexEVSR%20achieves%20state-of-the-art%20performance%20on%20three%20datasets.%20Notably%2C%20on%20the%20SDSD%20benchmark%2C%20our%20method%20can%20get%20up%20to%202.95%20dB%20gain%20while%20reducing%20runtime%20by%2065%25%20compared%20to%20prior%20event-based%20methods.%20Code%3A%20https%3A//github.com/DachunKai/RetinexEVSR.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02206v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%2520the%2520Unseen%253A%2520Zooming%2520in%2520the%2520Dark%2520with%2520Event%2520Cameras%26entry.906535625%3DDachun%2520Kai%2520and%2520Zeyu%2520Xiao%2520and%2520Huyue%2520Zhu%2520and%2520Jiaxiao%2520Wang%2520and%2520Yueyi%2520Zhang%2520and%2520Xiaoyan%2520Sun%26entry.1292438233%3DThis%2520paper%2520addresses%2520low-light%2520video%2520super-resolution%2520%2528LVSR%2529%252C%2520aiming%2520to%2520restore%2520high-resolution%2520videos%2520from%2520low-light%252C%2520low-resolution%2520%2528LR%2529%2520inputs.%2520Existing%2520LVSR%2520methods%2520often%2520struggle%2520to%2520recover%2520fine%2520details%2520due%2520to%2520limited%2520contrast%2520and%2520insufficient%2520high-frequency%2520information.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520present%2520RetinexEVSR%252C%2520the%2520first%2520event-driven%2520LVSR%2520framework%2520that%2520leverages%2520high-contrast%2520event%2520signals%2520and%2520Retinex-inspired%2520priors%2520to%2520enhance%2520video%2520quality%2520under%2520low-light%2520scenarios.%2520Unlike%2520previous%2520approaches%2520that%2520directly%2520fuse%2520degraded%2520signals%252C%2520RetinexEVSR%2520introduces%2520a%2520novel%2520bidirectional%2520cross-modal%2520fusion%2520strategy%2520to%2520extract%2520and%2520integrate%2520meaningful%2520cues%2520from%2520noisy%2520event%2520data%2520and%2520degraded%2520RGB%2520frames.%2520Specifically%252C%2520an%2520illumination-guided%2520event%2520enhancement%2520module%2520is%2520designed%2520to%2520progressively%2520refine%2520event%2520features%2520using%2520illumination%2520maps%2520derived%2520from%2520the%2520Retinex%2520model%252C%2520thereby%2520suppressing%2520low-light%2520artifacts%2520while%2520preserving%2520high-contrast%2520details.%2520Furthermore%252C%2520we%2520propose%2520an%2520event-guided%2520reflectance%2520enhancement%2520module%2520that%2520utilizes%2520the%2520enhanced%2520event%2520features%2520to%2520dynamically%2520recover%2520reflectance%2520details%2520via%2520a%2520multi-scale%2520fusion%2520mechanism.%2520Experimental%2520results%2520show%2520that%2520our%2520RetinexEVSR%2520achieves%2520state-of-the-art%2520performance%2520on%2520three%2520datasets.%2520Notably%252C%2520on%2520the%2520SDSD%2520benchmark%252C%2520our%2520method%2520can%2520get%2520up%2520to%25202.95%2520dB%2520gain%2520while%2520reducing%2520runtime%2520by%252065%2525%2520compared%2520to%2520prior%2520event-based%2520methods.%2520Code%253A%2520https%253A//github.com/DachunKai/RetinexEVSR.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02206v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20the%20Unseen%3A%20Zooming%20in%20the%20Dark%20with%20Event%20Cameras&entry.906535625=Dachun%20Kai%20and%20Zeyu%20Xiao%20and%20Huyue%20Zhu%20and%20Jiaxiao%20Wang%20and%20Yueyi%20Zhang%20and%20Xiaoyan%20Sun&entry.1292438233=This%20paper%20addresses%20low-light%20video%20super-resolution%20%28LVSR%29%2C%20aiming%20to%20restore%20high-resolution%20videos%20from%20low-light%2C%20low-resolution%20%28LR%29%20inputs.%20Existing%20LVSR%20methods%20often%20struggle%20to%20recover%20fine%20details%20due%20to%20limited%20contrast%20and%20insufficient%20high-frequency%20information.%20To%20overcome%20these%20challenges%2C%20we%20present%20RetinexEVSR%2C%20the%20first%20event-driven%20LVSR%20framework%20that%20leverages%20high-contrast%20event%20signals%20and%20Retinex-inspired%20priors%20to%20enhance%20video%20quality%20under%20low-light%20scenarios.%20Unlike%20previous%20approaches%20that%20directly%20fuse%20degraded%20signals%2C%20RetinexEVSR%20introduces%20a%20novel%20bidirectional%20cross-modal%20fusion%20strategy%20to%20extract%20and%20integrate%20meaningful%20cues%20from%20noisy%20event%20data%20and%20degraded%20RGB%20frames.%20Specifically%2C%20an%20illumination-guided%20event%20enhancement%20module%20is%20designed%20to%20progressively%20refine%20event%20features%20using%20illumination%20maps%20derived%20from%20the%20Retinex%20model%2C%20thereby%20suppressing%20low-light%20artifacts%20while%20preserving%20high-contrast%20details.%20Furthermore%2C%20we%20propose%20an%20event-guided%20reflectance%20enhancement%20module%20that%20utilizes%20the%20enhanced%20event%20features%20to%20dynamically%20recover%20reflectance%20details%20via%20a%20multi-scale%20fusion%20mechanism.%20Experimental%20results%20show%20that%20our%20RetinexEVSR%20achieves%20state-of-the-art%20performance%20on%20three%20datasets.%20Notably%2C%20on%20the%20SDSD%20benchmark%2C%20our%20method%20can%20get%20up%20to%202.95%20dB%20gain%20while%20reducing%20runtime%20by%2065%25%20compared%20to%20prior%20event-based%20methods.%20Code%3A%20https%3A//github.com/DachunKai/RetinexEVSR.&entry.1838667208=http%3A//arxiv.org/abs/2601.02206v1&entry.124074799=Read"},
{"title": "Unsupervised Stereo via Multi-Baseline Geometry-Consistent Self-Training", "author": "Peng Xu and Zhiyu Xiang and Tingming Bai and Tianyu Pu and Kai Wang and Chaojie Ji and Zhihao Yang and Eryun Liu", "abstract": "Photometric loss and pseudo-label-based self-training are two widely used methods for training stereo networks on unlabeled data. However, they both struggle to provide accurate supervision in occluded regions. The former lacks valid correspondences, while the latter's pseudo labels are often unreliable. To overcome these limitations, we present S$^3$, a simple yet effective framework based on multi-baseline geometry consistency. Unlike conventional self-training where teacher and student share identical stereo pairs, S$^3$ assigns them different target images, introducing natural visibility asymmetry. Regions occluded in the student's view often remain visible and matchable to the teacher, enabling reliable pseudo labels even in regions where photometric supervision fails. The teacher's disparities are rescaled to align with the student's baseline and used to guide student learning. An occlusion-aware weighting strategy is further proposed to mitigate unreliable supervision in teacher-occluded regions and to encourage the student to learn robust occlusion completion. To support training, we construct MBS20K, a multi-baseline stereo dataset synthesized using the CARLA simulator. Extensive experiments demonstrate that S$^3$ provides effective supervision in both occluded and non-occluded regions, achieves strong generalization performance, and surpasses previous state-of-the-art methods on the KITTI 2015 and 2012 benchmarks.", "link": "http://arxiv.org/abs/2508.10838v2", "date": "2026-01-05", "relevancy": 2.2638, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.581}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5586}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Stereo%20via%20Multi-Baseline%20Geometry-Consistent%20Self-Training&body=Title%3A%20Unsupervised%20Stereo%20via%20Multi-Baseline%20Geometry-Consistent%20Self-Training%0AAuthor%3A%20Peng%20Xu%20and%20Zhiyu%20Xiang%20and%20Tingming%20Bai%20and%20Tianyu%20Pu%20and%20Kai%20Wang%20and%20Chaojie%20Ji%20and%20Zhihao%20Yang%20and%20Eryun%20Liu%0AAbstract%3A%20Photometric%20loss%20and%20pseudo-label-based%20self-training%20are%20two%20widely%20used%20methods%20for%20training%20stereo%20networks%20on%20unlabeled%20data.%20However%2C%20they%20both%20struggle%20to%20provide%20accurate%20supervision%20in%20occluded%20regions.%20The%20former%20lacks%20valid%20correspondences%2C%20while%20the%20latter%27s%20pseudo%20labels%20are%20often%20unreliable.%20To%20overcome%20these%20limitations%2C%20we%20present%20S%24%5E3%24%2C%20a%20simple%20yet%20effective%20framework%20based%20on%20multi-baseline%20geometry%20consistency.%20Unlike%20conventional%20self-training%20where%20teacher%20and%20student%20share%20identical%20stereo%20pairs%2C%20S%24%5E3%24%20assigns%20them%20different%20target%20images%2C%20introducing%20natural%20visibility%20asymmetry.%20Regions%20occluded%20in%20the%20student%27s%20view%20often%20remain%20visible%20and%20matchable%20to%20the%20teacher%2C%20enabling%20reliable%20pseudo%20labels%20even%20in%20regions%20where%20photometric%20supervision%20fails.%20The%20teacher%27s%20disparities%20are%20rescaled%20to%20align%20with%20the%20student%27s%20baseline%20and%20used%20to%20guide%20student%20learning.%20An%20occlusion-aware%20weighting%20strategy%20is%20further%20proposed%20to%20mitigate%20unreliable%20supervision%20in%20teacher-occluded%20regions%20and%20to%20encourage%20the%20student%20to%20learn%20robust%20occlusion%20completion.%20To%20support%20training%2C%20we%20construct%20MBS20K%2C%20a%20multi-baseline%20stereo%20dataset%20synthesized%20using%20the%20CARLA%20simulator.%20Extensive%20experiments%20demonstrate%20that%20S%24%5E3%24%20provides%20effective%20supervision%20in%20both%20occluded%20and%20non-occluded%20regions%2C%20achieves%20strong%20generalization%20performance%2C%20and%20surpasses%20previous%20state-of-the-art%20methods%20on%20the%20KITTI%202015%20and%202012%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2508.10838v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Stereo%2520via%2520Multi-Baseline%2520Geometry-Consistent%2520Self-Training%26entry.906535625%3DPeng%2520Xu%2520and%2520Zhiyu%2520Xiang%2520and%2520Tingming%2520Bai%2520and%2520Tianyu%2520Pu%2520and%2520Kai%2520Wang%2520and%2520Chaojie%2520Ji%2520and%2520Zhihao%2520Yang%2520and%2520Eryun%2520Liu%26entry.1292438233%3DPhotometric%2520loss%2520and%2520pseudo-label-based%2520self-training%2520are%2520two%2520widely%2520used%2520methods%2520for%2520training%2520stereo%2520networks%2520on%2520unlabeled%2520data.%2520However%252C%2520they%2520both%2520struggle%2520to%2520provide%2520accurate%2520supervision%2520in%2520occluded%2520regions.%2520The%2520former%2520lacks%2520valid%2520correspondences%252C%2520while%2520the%2520latter%2527s%2520pseudo%2520labels%2520are%2520often%2520unreliable.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520present%2520S%2524%255E3%2524%252C%2520a%2520simple%2520yet%2520effective%2520framework%2520based%2520on%2520multi-baseline%2520geometry%2520consistency.%2520Unlike%2520conventional%2520self-training%2520where%2520teacher%2520and%2520student%2520share%2520identical%2520stereo%2520pairs%252C%2520S%2524%255E3%2524%2520assigns%2520them%2520different%2520target%2520images%252C%2520introducing%2520natural%2520visibility%2520asymmetry.%2520Regions%2520occluded%2520in%2520the%2520student%2527s%2520view%2520often%2520remain%2520visible%2520and%2520matchable%2520to%2520the%2520teacher%252C%2520enabling%2520reliable%2520pseudo%2520labels%2520even%2520in%2520regions%2520where%2520photometric%2520supervision%2520fails.%2520The%2520teacher%2527s%2520disparities%2520are%2520rescaled%2520to%2520align%2520with%2520the%2520student%2527s%2520baseline%2520and%2520used%2520to%2520guide%2520student%2520learning.%2520An%2520occlusion-aware%2520weighting%2520strategy%2520is%2520further%2520proposed%2520to%2520mitigate%2520unreliable%2520supervision%2520in%2520teacher-occluded%2520regions%2520and%2520to%2520encourage%2520the%2520student%2520to%2520learn%2520robust%2520occlusion%2520completion.%2520To%2520support%2520training%252C%2520we%2520construct%2520MBS20K%252C%2520a%2520multi-baseline%2520stereo%2520dataset%2520synthesized%2520using%2520the%2520CARLA%2520simulator.%2520Extensive%2520experiments%2520demonstrate%2520that%2520S%2524%255E3%2524%2520provides%2520effective%2520supervision%2520in%2520both%2520occluded%2520and%2520non-occluded%2520regions%252C%2520achieves%2520strong%2520generalization%2520performance%252C%2520and%2520surpasses%2520previous%2520state-of-the-art%2520methods%2520on%2520the%2520KITTI%25202015%2520and%25202012%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10838v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Stereo%20via%20Multi-Baseline%20Geometry-Consistent%20Self-Training&entry.906535625=Peng%20Xu%20and%20Zhiyu%20Xiang%20and%20Tingming%20Bai%20and%20Tianyu%20Pu%20and%20Kai%20Wang%20and%20Chaojie%20Ji%20and%20Zhihao%20Yang%20and%20Eryun%20Liu&entry.1292438233=Photometric%20loss%20and%20pseudo-label-based%20self-training%20are%20two%20widely%20used%20methods%20for%20training%20stereo%20networks%20on%20unlabeled%20data.%20However%2C%20they%20both%20struggle%20to%20provide%20accurate%20supervision%20in%20occluded%20regions.%20The%20former%20lacks%20valid%20correspondences%2C%20while%20the%20latter%27s%20pseudo%20labels%20are%20often%20unreliable.%20To%20overcome%20these%20limitations%2C%20we%20present%20S%24%5E3%24%2C%20a%20simple%20yet%20effective%20framework%20based%20on%20multi-baseline%20geometry%20consistency.%20Unlike%20conventional%20self-training%20where%20teacher%20and%20student%20share%20identical%20stereo%20pairs%2C%20S%24%5E3%24%20assigns%20them%20different%20target%20images%2C%20introducing%20natural%20visibility%20asymmetry.%20Regions%20occluded%20in%20the%20student%27s%20view%20often%20remain%20visible%20and%20matchable%20to%20the%20teacher%2C%20enabling%20reliable%20pseudo%20labels%20even%20in%20regions%20where%20photometric%20supervision%20fails.%20The%20teacher%27s%20disparities%20are%20rescaled%20to%20align%20with%20the%20student%27s%20baseline%20and%20used%20to%20guide%20student%20learning.%20An%20occlusion-aware%20weighting%20strategy%20is%20further%20proposed%20to%20mitigate%20unreliable%20supervision%20in%20teacher-occluded%20regions%20and%20to%20encourage%20the%20student%20to%20learn%20robust%20occlusion%20completion.%20To%20support%20training%2C%20we%20construct%20MBS20K%2C%20a%20multi-baseline%20stereo%20dataset%20synthesized%20using%20the%20CARLA%20simulator.%20Extensive%20experiments%20demonstrate%20that%20S%24%5E3%24%20provides%20effective%20supervision%20in%20both%20occluded%20and%20non-occluded%20regions%2C%20achieves%20strong%20generalization%20performance%2C%20and%20surpasses%20previous%20state-of-the-art%20methods%20on%20the%20KITTI%202015%20and%202012%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2508.10838v2&entry.124074799=Read"},
{"title": "RingMo-Agent: A Unified Remote Sensing Foundation Model for Multi-Platform and Multi-Modal Reasoning", "author": "Huiyang Hu and Peijin Wang and Yingchao Feng and Kaiwen Wei and Wenxin Yin and Wenhui Diao and Mengyu Wang and Hanbo Bi and Kaiyue Kang and Tong Ling and Kun Fu and Xian Sun", "abstract": "Remote sensing (RS) images from multiple modalities and platforms exhibit diverse details due to differences in sensor characteristics and imaging perspectives. Existing vision-language research in RS largely relies on relatively homogeneous data sources. Moreover, they still remain limited to conventional visual perception tasks such as classification or captioning. As a result, these methods fail to serve as a unified and standalone framework capable of effectively handling RS imagery from diverse sources in real-world applications. To address these issues, we propose RingMo-Agent, a model designed to handle multi-modal and multi-platform data that performs perception and reasoning tasks based on user textual instructions. Compared with existing models, RingMo-Agent 1) is supported by a large-scale vision-language dataset named RS-VL3M, comprising over 3 million image-text pairs, spanning optical, SAR, and infrared (IR) modalities collected from both satellite and UAV platforms, covering perception and challenging reasoning tasks; 2) learns modality adaptive representations by incorporating separated embedding layers to construct isolated features for heterogeneous modalities and reduce cross-modal interference; 3) unifies task modeling by introducing task-specific tokens and employing a token-based high-dimensional hidden state decoding mechanism designed for long-horizon spatial tasks. Extensive experiments on various RS vision-language tasks demonstrate that RingMo-Agent not only proves effective in both visual understanding and sophisticated analytical tasks, but also exhibits strong generalizability across different platforms and sensing modalities.", "link": "http://arxiv.org/abs/2507.20776v2", "date": "2026-01-05", "relevancy": 2.2571, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.603}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5565}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RingMo-Agent%3A%20A%20Unified%20Remote%20Sensing%20Foundation%20Model%20for%20Multi-Platform%20and%20Multi-Modal%20Reasoning&body=Title%3A%20RingMo-Agent%3A%20A%20Unified%20Remote%20Sensing%20Foundation%20Model%20for%20Multi-Platform%20and%20Multi-Modal%20Reasoning%0AAuthor%3A%20Huiyang%20Hu%20and%20Peijin%20Wang%20and%20Yingchao%20Feng%20and%20Kaiwen%20Wei%20and%20Wenxin%20Yin%20and%20Wenhui%20Diao%20and%20Mengyu%20Wang%20and%20Hanbo%20Bi%20and%20Kaiyue%20Kang%20and%20Tong%20Ling%20and%20Kun%20Fu%20and%20Xian%20Sun%0AAbstract%3A%20Remote%20sensing%20%28RS%29%20images%20from%20multiple%20modalities%20and%20platforms%20exhibit%20diverse%20details%20due%20to%20differences%20in%20sensor%20characteristics%20and%20imaging%20perspectives.%20Existing%20vision-language%20research%20in%20RS%20largely%20relies%20on%20relatively%20homogeneous%20data%20sources.%20Moreover%2C%20they%20still%20remain%20limited%20to%20conventional%20visual%20perception%20tasks%20such%20as%20classification%20or%20captioning.%20As%20a%20result%2C%20these%20methods%20fail%20to%20serve%20as%20a%20unified%20and%20standalone%20framework%20capable%20of%20effectively%20handling%20RS%20imagery%20from%20diverse%20sources%20in%20real-world%20applications.%20To%20address%20these%20issues%2C%20we%20propose%20RingMo-Agent%2C%20a%20model%20designed%20to%20handle%20multi-modal%20and%20multi-platform%20data%20that%20performs%20perception%20and%20reasoning%20tasks%20based%20on%20user%20textual%20instructions.%20Compared%20with%20existing%20models%2C%20RingMo-Agent%201%29%20is%20supported%20by%20a%20large-scale%20vision-language%20dataset%20named%20RS-VL3M%2C%20comprising%20over%203%20million%20image-text%20pairs%2C%20spanning%20optical%2C%20SAR%2C%20and%20infrared%20%28IR%29%20modalities%20collected%20from%20both%20satellite%20and%20UAV%20platforms%2C%20covering%20perception%20and%20challenging%20reasoning%20tasks%3B%202%29%20learns%20modality%20adaptive%20representations%20by%20incorporating%20separated%20embedding%20layers%20to%20construct%20isolated%20features%20for%20heterogeneous%20modalities%20and%20reduce%20cross-modal%20interference%3B%203%29%20unifies%20task%20modeling%20by%20introducing%20task-specific%20tokens%20and%20employing%20a%20token-based%20high-dimensional%20hidden%20state%20decoding%20mechanism%20designed%20for%20long-horizon%20spatial%20tasks.%20Extensive%20experiments%20on%20various%20RS%20vision-language%20tasks%20demonstrate%20that%20RingMo-Agent%20not%20only%20proves%20effective%20in%20both%20visual%20understanding%20and%20sophisticated%20analytical%20tasks%2C%20but%20also%20exhibits%20strong%20generalizability%20across%20different%20platforms%20and%20sensing%20modalities.%0ALink%3A%20http%3A//arxiv.org/abs/2507.20776v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRingMo-Agent%253A%2520A%2520Unified%2520Remote%2520Sensing%2520Foundation%2520Model%2520for%2520Multi-Platform%2520and%2520Multi-Modal%2520Reasoning%26entry.906535625%3DHuiyang%2520Hu%2520and%2520Peijin%2520Wang%2520and%2520Yingchao%2520Feng%2520and%2520Kaiwen%2520Wei%2520and%2520Wenxin%2520Yin%2520and%2520Wenhui%2520Diao%2520and%2520Mengyu%2520Wang%2520and%2520Hanbo%2520Bi%2520and%2520Kaiyue%2520Kang%2520and%2520Tong%2520Ling%2520and%2520Kun%2520Fu%2520and%2520Xian%2520Sun%26entry.1292438233%3DRemote%2520sensing%2520%2528RS%2529%2520images%2520from%2520multiple%2520modalities%2520and%2520platforms%2520exhibit%2520diverse%2520details%2520due%2520to%2520differences%2520in%2520sensor%2520characteristics%2520and%2520imaging%2520perspectives.%2520Existing%2520vision-language%2520research%2520in%2520RS%2520largely%2520relies%2520on%2520relatively%2520homogeneous%2520data%2520sources.%2520Moreover%252C%2520they%2520still%2520remain%2520limited%2520to%2520conventional%2520visual%2520perception%2520tasks%2520such%2520as%2520classification%2520or%2520captioning.%2520As%2520a%2520result%252C%2520these%2520methods%2520fail%2520to%2520serve%2520as%2520a%2520unified%2520and%2520standalone%2520framework%2520capable%2520of%2520effectively%2520handling%2520RS%2520imagery%2520from%2520diverse%2520sources%2520in%2520real-world%2520applications.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520RingMo-Agent%252C%2520a%2520model%2520designed%2520to%2520handle%2520multi-modal%2520and%2520multi-platform%2520data%2520that%2520performs%2520perception%2520and%2520reasoning%2520tasks%2520based%2520on%2520user%2520textual%2520instructions.%2520Compared%2520with%2520existing%2520models%252C%2520RingMo-Agent%25201%2529%2520is%2520supported%2520by%2520a%2520large-scale%2520vision-language%2520dataset%2520named%2520RS-VL3M%252C%2520comprising%2520over%25203%2520million%2520image-text%2520pairs%252C%2520spanning%2520optical%252C%2520SAR%252C%2520and%2520infrared%2520%2528IR%2529%2520modalities%2520collected%2520from%2520both%2520satellite%2520and%2520UAV%2520platforms%252C%2520covering%2520perception%2520and%2520challenging%2520reasoning%2520tasks%253B%25202%2529%2520learns%2520modality%2520adaptive%2520representations%2520by%2520incorporating%2520separated%2520embedding%2520layers%2520to%2520construct%2520isolated%2520features%2520for%2520heterogeneous%2520modalities%2520and%2520reduce%2520cross-modal%2520interference%253B%25203%2529%2520unifies%2520task%2520modeling%2520by%2520introducing%2520task-specific%2520tokens%2520and%2520employing%2520a%2520token-based%2520high-dimensional%2520hidden%2520state%2520decoding%2520mechanism%2520designed%2520for%2520long-horizon%2520spatial%2520tasks.%2520Extensive%2520experiments%2520on%2520various%2520RS%2520vision-language%2520tasks%2520demonstrate%2520that%2520RingMo-Agent%2520not%2520only%2520proves%2520effective%2520in%2520both%2520visual%2520understanding%2520and%2520sophisticated%2520analytical%2520tasks%252C%2520but%2520also%2520exhibits%2520strong%2520generalizability%2520across%2520different%2520platforms%2520and%2520sensing%2520modalities.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20776v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RingMo-Agent%3A%20A%20Unified%20Remote%20Sensing%20Foundation%20Model%20for%20Multi-Platform%20and%20Multi-Modal%20Reasoning&entry.906535625=Huiyang%20Hu%20and%20Peijin%20Wang%20and%20Yingchao%20Feng%20and%20Kaiwen%20Wei%20and%20Wenxin%20Yin%20and%20Wenhui%20Diao%20and%20Mengyu%20Wang%20and%20Hanbo%20Bi%20and%20Kaiyue%20Kang%20and%20Tong%20Ling%20and%20Kun%20Fu%20and%20Xian%20Sun&entry.1292438233=Remote%20sensing%20%28RS%29%20images%20from%20multiple%20modalities%20and%20platforms%20exhibit%20diverse%20details%20due%20to%20differences%20in%20sensor%20characteristics%20and%20imaging%20perspectives.%20Existing%20vision-language%20research%20in%20RS%20largely%20relies%20on%20relatively%20homogeneous%20data%20sources.%20Moreover%2C%20they%20still%20remain%20limited%20to%20conventional%20visual%20perception%20tasks%20such%20as%20classification%20or%20captioning.%20As%20a%20result%2C%20these%20methods%20fail%20to%20serve%20as%20a%20unified%20and%20standalone%20framework%20capable%20of%20effectively%20handling%20RS%20imagery%20from%20diverse%20sources%20in%20real-world%20applications.%20To%20address%20these%20issues%2C%20we%20propose%20RingMo-Agent%2C%20a%20model%20designed%20to%20handle%20multi-modal%20and%20multi-platform%20data%20that%20performs%20perception%20and%20reasoning%20tasks%20based%20on%20user%20textual%20instructions.%20Compared%20with%20existing%20models%2C%20RingMo-Agent%201%29%20is%20supported%20by%20a%20large-scale%20vision-language%20dataset%20named%20RS-VL3M%2C%20comprising%20over%203%20million%20image-text%20pairs%2C%20spanning%20optical%2C%20SAR%2C%20and%20infrared%20%28IR%29%20modalities%20collected%20from%20both%20satellite%20and%20UAV%20platforms%2C%20covering%20perception%20and%20challenging%20reasoning%20tasks%3B%202%29%20learns%20modality%20adaptive%20representations%20by%20incorporating%20separated%20embedding%20layers%20to%20construct%20isolated%20features%20for%20heterogeneous%20modalities%20and%20reduce%20cross-modal%20interference%3B%203%29%20unifies%20task%20modeling%20by%20introducing%20task-specific%20tokens%20and%20employing%20a%20token-based%20high-dimensional%20hidden%20state%20decoding%20mechanism%20designed%20for%20long-horizon%20spatial%20tasks.%20Extensive%20experiments%20on%20various%20RS%20vision-language%20tasks%20demonstrate%20that%20RingMo-Agent%20not%20only%20proves%20effective%20in%20both%20visual%20understanding%20and%20sophisticated%20analytical%20tasks%2C%20but%20also%20exhibits%20strong%20generalizability%20across%20different%20platforms%20and%20sensing%20modalities.&entry.1838667208=http%3A//arxiv.org/abs/2507.20776v2&entry.124074799=Read"},
{"title": "SAM-aware Test-time Adaptation for Universal Medical Image Segmentation", "author": "Jianghao Wu and Yicheng Wu and Yutong Xie and Wenjia Bai and You Zhang and Feilong Tang and Yulong Li and Imran Razzak and Daniel F Schmidt and Yasmeen George", "abstract": "Leveraging the Segment Anything Model (SAM) for medical image segmentation remains challenging due to its limited adaptability across diverse medical domains. Although fine-tuned variants, such as MedSAM, improve performance in scenarios similar to the training modalities or organs, they may lack generalizability to unseen data. To overcome this limitation, we propose SAM-aware Test-time Adaptation (SAM-TTA), a lightweight and flexible framework that preserves SAM's inherent generalization ability while enhancing segmentation accuracy for medical images. SAM-TTA tackles two major challenges: (1) input-level discrepancy caused by channel mismatches between natural and medical images, and (2) semantic-level discrepancy due to different object characteristics in natural versus medical images (e.g., with clear boundaries vs. ambiguous structures). To this end, we introduce two complementary components: a self-adaptive Bezier Curve-based Transformation (SBCT), which maps single-channel medical images into SAM-compatible three-channel images via a few learnable parameters to be optimized at test time; and IoU-guided Multi-scale Adaptation (IMA), which leverages SAM's intrinsic IoU scores to enforce high output confidence, dual-scale prediction consistency, and intermediate feature consistency, to improve semantic-level alignments. Extensive experiments on eight public medical image segmentation tasks, covering six grayscale and two color (endoscopic) tasks, demonstrate that SAM-TTA consistently outperforms state-of-the-art test-time adaptation methods. Notably, on six grayscale datasets, SAM-TTA even surpasses fully fine-tuned models, achieving significant Dice improvements (i.e., average 4.8% and 7.4% gains over MedSAM and SAM-Med2D) and establishing a new paradigm for universal medical image segmentation. Code is available at https://github.com/JianghaoWu/SAM-TTA.", "link": "http://arxiv.org/abs/2506.05221v2", "date": "2026-01-05", "relevancy": 2.2562, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6126}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5414}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAM-aware%20Test-time%20Adaptation%20for%20Universal%20Medical%20Image%20Segmentation&body=Title%3A%20SAM-aware%20Test-time%20Adaptation%20for%20Universal%20Medical%20Image%20Segmentation%0AAuthor%3A%20Jianghao%20Wu%20and%20Yicheng%20Wu%20and%20Yutong%20Xie%20and%20Wenjia%20Bai%20and%20You%20Zhang%20and%20Feilong%20Tang%20and%20Yulong%20Li%20and%20Imran%20Razzak%20and%20Daniel%20F%20Schmidt%20and%20Yasmeen%20George%0AAbstract%3A%20Leveraging%20the%20Segment%20Anything%20Model%20%28SAM%29%20for%20medical%20image%20segmentation%20remains%20challenging%20due%20to%20its%20limited%20adaptability%20across%20diverse%20medical%20domains.%20Although%20fine-tuned%20variants%2C%20such%20as%20MedSAM%2C%20improve%20performance%20in%20scenarios%20similar%20to%20the%20training%20modalities%20or%20organs%2C%20they%20may%20lack%20generalizability%20to%20unseen%20data.%20To%20overcome%20this%20limitation%2C%20we%20propose%20SAM-aware%20Test-time%20Adaptation%20%28SAM-TTA%29%2C%20a%20lightweight%20and%20flexible%20framework%20that%20preserves%20SAM%27s%20inherent%20generalization%20ability%20while%20enhancing%20segmentation%20accuracy%20for%20medical%20images.%20SAM-TTA%20tackles%20two%20major%20challenges%3A%20%281%29%20input-level%20discrepancy%20caused%20by%20channel%20mismatches%20between%20natural%20and%20medical%20images%2C%20and%20%282%29%20semantic-level%20discrepancy%20due%20to%20different%20object%20characteristics%20in%20natural%20versus%20medical%20images%20%28e.g.%2C%20with%20clear%20boundaries%20vs.%20ambiguous%20structures%29.%20To%20this%20end%2C%20we%20introduce%20two%20complementary%20components%3A%20a%20self-adaptive%20Bezier%20Curve-based%20Transformation%20%28SBCT%29%2C%20which%20maps%20single-channel%20medical%20images%20into%20SAM-compatible%20three-channel%20images%20via%20a%20few%20learnable%20parameters%20to%20be%20optimized%20at%20test%20time%3B%20and%20IoU-guided%20Multi-scale%20Adaptation%20%28IMA%29%2C%20which%20leverages%20SAM%27s%20intrinsic%20IoU%20scores%20to%20enforce%20high%20output%20confidence%2C%20dual-scale%20prediction%20consistency%2C%20and%20intermediate%20feature%20consistency%2C%20to%20improve%20semantic-level%20alignments.%20Extensive%20experiments%20on%20eight%20public%20medical%20image%20segmentation%20tasks%2C%20covering%20six%20grayscale%20and%20two%20color%20%28endoscopic%29%20tasks%2C%20demonstrate%20that%20SAM-TTA%20consistently%20outperforms%20state-of-the-art%20test-time%20adaptation%20methods.%20Notably%2C%20on%20six%20grayscale%20datasets%2C%20SAM-TTA%20even%20surpasses%20fully%20fine-tuned%20models%2C%20achieving%20significant%20Dice%20improvements%20%28i.e.%2C%20average%204.8%25%20and%207.4%25%20gains%20over%20MedSAM%20and%20SAM-Med2D%29%20and%20establishing%20a%20new%20paradigm%20for%20universal%20medical%20image%20segmentation.%20Code%20is%20available%20at%20https%3A//github.com/JianghaoWu/SAM-TTA.%0ALink%3A%20http%3A//arxiv.org/abs/2506.05221v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAM-aware%2520Test-time%2520Adaptation%2520for%2520Universal%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DJianghao%2520Wu%2520and%2520Yicheng%2520Wu%2520and%2520Yutong%2520Xie%2520and%2520Wenjia%2520Bai%2520and%2520You%2520Zhang%2520and%2520Feilong%2520Tang%2520and%2520Yulong%2520Li%2520and%2520Imran%2520Razzak%2520and%2520Daniel%2520F%2520Schmidt%2520and%2520Yasmeen%2520George%26entry.1292438233%3DLeveraging%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520for%2520medical%2520image%2520segmentation%2520remains%2520challenging%2520due%2520to%2520its%2520limited%2520adaptability%2520across%2520diverse%2520medical%2520domains.%2520Although%2520fine-tuned%2520variants%252C%2520such%2520as%2520MedSAM%252C%2520improve%2520performance%2520in%2520scenarios%2520similar%2520to%2520the%2520training%2520modalities%2520or%2520organs%252C%2520they%2520may%2520lack%2520generalizability%2520to%2520unseen%2520data.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520SAM-aware%2520Test-time%2520Adaptation%2520%2528SAM-TTA%2529%252C%2520a%2520lightweight%2520and%2520flexible%2520framework%2520that%2520preserves%2520SAM%2527s%2520inherent%2520generalization%2520ability%2520while%2520enhancing%2520segmentation%2520accuracy%2520for%2520medical%2520images.%2520SAM-TTA%2520tackles%2520two%2520major%2520challenges%253A%2520%25281%2529%2520input-level%2520discrepancy%2520caused%2520by%2520channel%2520mismatches%2520between%2520natural%2520and%2520medical%2520images%252C%2520and%2520%25282%2529%2520semantic-level%2520discrepancy%2520due%2520to%2520different%2520object%2520characteristics%2520in%2520natural%2520versus%2520medical%2520images%2520%2528e.g.%252C%2520with%2520clear%2520boundaries%2520vs.%2520ambiguous%2520structures%2529.%2520To%2520this%2520end%252C%2520we%2520introduce%2520two%2520complementary%2520components%253A%2520a%2520self-adaptive%2520Bezier%2520Curve-based%2520Transformation%2520%2528SBCT%2529%252C%2520which%2520maps%2520single-channel%2520medical%2520images%2520into%2520SAM-compatible%2520three-channel%2520images%2520via%2520a%2520few%2520learnable%2520parameters%2520to%2520be%2520optimized%2520at%2520test%2520time%253B%2520and%2520IoU-guided%2520Multi-scale%2520Adaptation%2520%2528IMA%2529%252C%2520which%2520leverages%2520SAM%2527s%2520intrinsic%2520IoU%2520scores%2520to%2520enforce%2520high%2520output%2520confidence%252C%2520dual-scale%2520prediction%2520consistency%252C%2520and%2520intermediate%2520feature%2520consistency%252C%2520to%2520improve%2520semantic-level%2520alignments.%2520Extensive%2520experiments%2520on%2520eight%2520public%2520medical%2520image%2520segmentation%2520tasks%252C%2520covering%2520six%2520grayscale%2520and%2520two%2520color%2520%2528endoscopic%2529%2520tasks%252C%2520demonstrate%2520that%2520SAM-TTA%2520consistently%2520outperforms%2520state-of-the-art%2520test-time%2520adaptation%2520methods.%2520Notably%252C%2520on%2520six%2520grayscale%2520datasets%252C%2520SAM-TTA%2520even%2520surpasses%2520fully%2520fine-tuned%2520models%252C%2520achieving%2520significant%2520Dice%2520improvements%2520%2528i.e.%252C%2520average%25204.8%2525%2520and%25207.4%2525%2520gains%2520over%2520MedSAM%2520and%2520SAM-Med2D%2529%2520and%2520establishing%2520a%2520new%2520paradigm%2520for%2520universal%2520medical%2520image%2520segmentation.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/JianghaoWu/SAM-TTA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05221v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM-aware%20Test-time%20Adaptation%20for%20Universal%20Medical%20Image%20Segmentation&entry.906535625=Jianghao%20Wu%20and%20Yicheng%20Wu%20and%20Yutong%20Xie%20and%20Wenjia%20Bai%20and%20You%20Zhang%20and%20Feilong%20Tang%20and%20Yulong%20Li%20and%20Imran%20Razzak%20and%20Daniel%20F%20Schmidt%20and%20Yasmeen%20George&entry.1292438233=Leveraging%20the%20Segment%20Anything%20Model%20%28SAM%29%20for%20medical%20image%20segmentation%20remains%20challenging%20due%20to%20its%20limited%20adaptability%20across%20diverse%20medical%20domains.%20Although%20fine-tuned%20variants%2C%20such%20as%20MedSAM%2C%20improve%20performance%20in%20scenarios%20similar%20to%20the%20training%20modalities%20or%20organs%2C%20they%20may%20lack%20generalizability%20to%20unseen%20data.%20To%20overcome%20this%20limitation%2C%20we%20propose%20SAM-aware%20Test-time%20Adaptation%20%28SAM-TTA%29%2C%20a%20lightweight%20and%20flexible%20framework%20that%20preserves%20SAM%27s%20inherent%20generalization%20ability%20while%20enhancing%20segmentation%20accuracy%20for%20medical%20images.%20SAM-TTA%20tackles%20two%20major%20challenges%3A%20%281%29%20input-level%20discrepancy%20caused%20by%20channel%20mismatches%20between%20natural%20and%20medical%20images%2C%20and%20%282%29%20semantic-level%20discrepancy%20due%20to%20different%20object%20characteristics%20in%20natural%20versus%20medical%20images%20%28e.g.%2C%20with%20clear%20boundaries%20vs.%20ambiguous%20structures%29.%20To%20this%20end%2C%20we%20introduce%20two%20complementary%20components%3A%20a%20self-adaptive%20Bezier%20Curve-based%20Transformation%20%28SBCT%29%2C%20which%20maps%20single-channel%20medical%20images%20into%20SAM-compatible%20three-channel%20images%20via%20a%20few%20learnable%20parameters%20to%20be%20optimized%20at%20test%20time%3B%20and%20IoU-guided%20Multi-scale%20Adaptation%20%28IMA%29%2C%20which%20leverages%20SAM%27s%20intrinsic%20IoU%20scores%20to%20enforce%20high%20output%20confidence%2C%20dual-scale%20prediction%20consistency%2C%20and%20intermediate%20feature%20consistency%2C%20to%20improve%20semantic-level%20alignments.%20Extensive%20experiments%20on%20eight%20public%20medical%20image%20segmentation%20tasks%2C%20covering%20six%20grayscale%20and%20two%20color%20%28endoscopic%29%20tasks%2C%20demonstrate%20that%20SAM-TTA%20consistently%20outperforms%20state-of-the-art%20test-time%20adaptation%20methods.%20Notably%2C%20on%20six%20grayscale%20datasets%2C%20SAM-TTA%20even%20surpasses%20fully%20fine-tuned%20models%2C%20achieving%20significant%20Dice%20improvements%20%28i.e.%2C%20average%204.8%25%20and%207.4%25%20gains%20over%20MedSAM%20and%20SAM-Med2D%29%20and%20establishing%20a%20new%20paradigm%20for%20universal%20medical%20image%20segmentation.%20Code%20is%20available%20at%20https%3A//github.com/JianghaoWu/SAM-TTA.&entry.1838667208=http%3A//arxiv.org/abs/2506.05221v2&entry.124074799=Read"},
{"title": "CMDAR: A Chinese Multi-scene Dynamic Audio Reasoning Benchmark with Diverse Challenges", "author": "Hui Li and Changhao Jiang and Hongyu Wang and Ming Zhang and Jiajun Sun and Zhixiong Yang and Yifei Cao and Shihan Dou and Xiaoran Fan and Baoyu Fan and Tao Ji and Tao Gui and Qi Zhang and Xuanjing Huang", "abstract": "The ability to reason from audio, including speech, environmental sounds, and music, is essential for AI agents to interact effectively in real-world scenarios. Existing benchmarks mainly focus on static or single-scene settings and English audio data and do not fully capture scenarios where multiple speakers, unfolding events, and heterogeneous audio sources interact. To address these challenges, we introduce CMDAR, a Chinese benchmark for evaluating models on complex, multi-scene, and dynamically evolving audio reasoning tasks. CMDAR comprises 3,000 carefully curated question-answer pairs linked to diverse audio clips, covering five categories of complex reasoning and spanning three question types. We benchmark 26 state-of-the-art audio language models on CMDAR and observe that they exhibit limitations in complex reasoning tasks. In CMDAR-main, Qwen2.5-Omni achieves 76.67% accuracy, whereas GPT-4o Audio reaches 68.47%. However, GPT-4o Audio substantially outperforms Qwen2.5-Omni on the more challenging multiple-choice with multiple audios and open-ended tasks. And we provide detail analysis corresponding suggestions for the future development of large audio language models.", "link": "http://arxiv.org/abs/2509.22461v2", "date": "2026-01-05", "relevancy": 2.2497, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5783}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5783}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CMDAR%3A%20A%20Chinese%20Multi-scene%20Dynamic%20Audio%20Reasoning%20Benchmark%20with%20Diverse%20Challenges&body=Title%3A%20CMDAR%3A%20A%20Chinese%20Multi-scene%20Dynamic%20Audio%20Reasoning%20Benchmark%20with%20Diverse%20Challenges%0AAuthor%3A%20Hui%20Li%20and%20Changhao%20Jiang%20and%20Hongyu%20Wang%20and%20Ming%20Zhang%20and%20Jiajun%20Sun%20and%20Zhixiong%20Yang%20and%20Yifei%20Cao%20and%20Shihan%20Dou%20and%20Xiaoran%20Fan%20and%20Baoyu%20Fan%20and%20Tao%20Ji%20and%20Tao%20Gui%20and%20Qi%20Zhang%20and%20Xuanjing%20Huang%0AAbstract%3A%20The%20ability%20to%20reason%20from%20audio%2C%20including%20speech%2C%20environmental%20sounds%2C%20and%20music%2C%20is%20essential%20for%20AI%20agents%20to%20interact%20effectively%20in%20real-world%20scenarios.%20Existing%20benchmarks%20mainly%20focus%20on%20static%20or%20single-scene%20settings%20and%20English%20audio%20data%20and%20do%20not%20fully%20capture%20scenarios%20where%20multiple%20speakers%2C%20unfolding%20events%2C%20and%20heterogeneous%20audio%20sources%20interact.%20To%20address%20these%20challenges%2C%20we%20introduce%20CMDAR%2C%20a%20Chinese%20benchmark%20for%20evaluating%20models%20on%20complex%2C%20multi-scene%2C%20and%20dynamically%20evolving%20audio%20reasoning%20tasks.%20CMDAR%20comprises%203%2C000%20carefully%20curated%20question-answer%20pairs%20linked%20to%20diverse%20audio%20clips%2C%20covering%20five%20categories%20of%20complex%20reasoning%20and%20spanning%20three%20question%20types.%20We%20benchmark%2026%20state-of-the-art%20audio%20language%20models%20on%20CMDAR%20and%20observe%20that%20they%20exhibit%20limitations%20in%20complex%20reasoning%20tasks.%20In%20CMDAR-main%2C%20Qwen2.5-Omni%20achieves%2076.67%25%20accuracy%2C%20whereas%20GPT-4o%20Audio%20reaches%2068.47%25.%20However%2C%20GPT-4o%20Audio%20substantially%20outperforms%20Qwen2.5-Omni%20on%20the%20more%20challenging%20multiple-choice%20with%20multiple%20audios%20and%20open-ended%20tasks.%20And%20we%20provide%20detail%20analysis%20corresponding%20suggestions%20for%20the%20future%20development%20of%20large%20audio%20language%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2509.22461v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCMDAR%253A%2520A%2520Chinese%2520Multi-scene%2520Dynamic%2520Audio%2520Reasoning%2520Benchmark%2520with%2520Diverse%2520Challenges%26entry.906535625%3DHui%2520Li%2520and%2520Changhao%2520Jiang%2520and%2520Hongyu%2520Wang%2520and%2520Ming%2520Zhang%2520and%2520Jiajun%2520Sun%2520and%2520Zhixiong%2520Yang%2520and%2520Yifei%2520Cao%2520and%2520Shihan%2520Dou%2520and%2520Xiaoran%2520Fan%2520and%2520Baoyu%2520Fan%2520and%2520Tao%2520Ji%2520and%2520Tao%2520Gui%2520and%2520Qi%2520Zhang%2520and%2520Xuanjing%2520Huang%26entry.1292438233%3DThe%2520ability%2520to%2520reason%2520from%2520audio%252C%2520including%2520speech%252C%2520environmental%2520sounds%252C%2520and%2520music%252C%2520is%2520essential%2520for%2520AI%2520agents%2520to%2520interact%2520effectively%2520in%2520real-world%2520scenarios.%2520Existing%2520benchmarks%2520mainly%2520focus%2520on%2520static%2520or%2520single-scene%2520settings%2520and%2520English%2520audio%2520data%2520and%2520do%2520not%2520fully%2520capture%2520scenarios%2520where%2520multiple%2520speakers%252C%2520unfolding%2520events%252C%2520and%2520heterogeneous%2520audio%2520sources%2520interact.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520CMDAR%252C%2520a%2520Chinese%2520benchmark%2520for%2520evaluating%2520models%2520on%2520complex%252C%2520multi-scene%252C%2520and%2520dynamically%2520evolving%2520audio%2520reasoning%2520tasks.%2520CMDAR%2520comprises%25203%252C000%2520carefully%2520curated%2520question-answer%2520pairs%2520linked%2520to%2520diverse%2520audio%2520clips%252C%2520covering%2520five%2520categories%2520of%2520complex%2520reasoning%2520and%2520spanning%2520three%2520question%2520types.%2520We%2520benchmark%252026%2520state-of-the-art%2520audio%2520language%2520models%2520on%2520CMDAR%2520and%2520observe%2520that%2520they%2520exhibit%2520limitations%2520in%2520complex%2520reasoning%2520tasks.%2520In%2520CMDAR-main%252C%2520Qwen2.5-Omni%2520achieves%252076.67%2525%2520accuracy%252C%2520whereas%2520GPT-4o%2520Audio%2520reaches%252068.47%2525.%2520However%252C%2520GPT-4o%2520Audio%2520substantially%2520outperforms%2520Qwen2.5-Omni%2520on%2520the%2520more%2520challenging%2520multiple-choice%2520with%2520multiple%2520audios%2520and%2520open-ended%2520tasks.%2520And%2520we%2520provide%2520detail%2520analysis%2520corresponding%2520suggestions%2520for%2520the%2520future%2520development%2520of%2520large%2520audio%2520language%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22461v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CMDAR%3A%20A%20Chinese%20Multi-scene%20Dynamic%20Audio%20Reasoning%20Benchmark%20with%20Diverse%20Challenges&entry.906535625=Hui%20Li%20and%20Changhao%20Jiang%20and%20Hongyu%20Wang%20and%20Ming%20Zhang%20and%20Jiajun%20Sun%20and%20Zhixiong%20Yang%20and%20Yifei%20Cao%20and%20Shihan%20Dou%20and%20Xiaoran%20Fan%20and%20Baoyu%20Fan%20and%20Tao%20Ji%20and%20Tao%20Gui%20and%20Qi%20Zhang%20and%20Xuanjing%20Huang&entry.1292438233=The%20ability%20to%20reason%20from%20audio%2C%20including%20speech%2C%20environmental%20sounds%2C%20and%20music%2C%20is%20essential%20for%20AI%20agents%20to%20interact%20effectively%20in%20real-world%20scenarios.%20Existing%20benchmarks%20mainly%20focus%20on%20static%20or%20single-scene%20settings%20and%20English%20audio%20data%20and%20do%20not%20fully%20capture%20scenarios%20where%20multiple%20speakers%2C%20unfolding%20events%2C%20and%20heterogeneous%20audio%20sources%20interact.%20To%20address%20these%20challenges%2C%20we%20introduce%20CMDAR%2C%20a%20Chinese%20benchmark%20for%20evaluating%20models%20on%20complex%2C%20multi-scene%2C%20and%20dynamically%20evolving%20audio%20reasoning%20tasks.%20CMDAR%20comprises%203%2C000%20carefully%20curated%20question-answer%20pairs%20linked%20to%20diverse%20audio%20clips%2C%20covering%20five%20categories%20of%20complex%20reasoning%20and%20spanning%20three%20question%20types.%20We%20benchmark%2026%20state-of-the-art%20audio%20language%20models%20on%20CMDAR%20and%20observe%20that%20they%20exhibit%20limitations%20in%20complex%20reasoning%20tasks.%20In%20CMDAR-main%2C%20Qwen2.5-Omni%20achieves%2076.67%25%20accuracy%2C%20whereas%20GPT-4o%20Audio%20reaches%2068.47%25.%20However%2C%20GPT-4o%20Audio%20substantially%20outperforms%20Qwen2.5-Omni%20on%20the%20more%20challenging%20multiple-choice%20with%20multiple%20audios%20and%20open-ended%20tasks.%20And%20we%20provide%20detail%20analysis%20corresponding%20suggestions%20for%20the%20future%20development%20of%20large%20audio%20language%20models.&entry.1838667208=http%3A//arxiv.org/abs/2509.22461v2&entry.124074799=Read"},
{"title": "Diminishing Returns in Self-Supervised Learning", "author": "Oli Bridge and Huey Sun and Botond Branyicskai-Nagy and Charles D'Ornano and Shomit Basu", "abstract": "Transformer-based architectures have become a dominant paradigm in vision and language, but their success is often attributed to large model capacity and massive training data. In this work, we examine how self-supervised pre-training, intermediate fine-tuning, and downstream fine-tuning interact in a low-capacity regime, using a 5M-parameter Vision Transformer for semantic segmentation. Across multiple data scales, we find that masked image modeling pre-training and downstream fine-tuning reliably improve performance, but with clear diminishing returns as supervision increases. In contrast, inserting an intermediate classification fine-tuning stage consistently degrades downstream performance, with the largest drops occurring precisely where pre-training is most effective. Through an analysis of patch-level representation geometry, we show that classification-based intermediate supervision actively interferes with representations learned during pre-training by collapsing spatial structure critical for dense prediction. These results indicate that, in small models, the geometry of supervision matters more than the number of training stages: misaligned intermediate objectives can negate the benefits of pre-training rather than amplify them.", "link": "http://arxiv.org/abs/2512.03862v2", "date": "2026-01-05", "relevancy": 2.2457, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5747}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5685}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diminishing%20Returns%20in%20Self-Supervised%20Learning&body=Title%3A%20Diminishing%20Returns%20in%20Self-Supervised%20Learning%0AAuthor%3A%20Oli%20Bridge%20and%20Huey%20Sun%20and%20Botond%20Branyicskai-Nagy%20and%20Charles%20D%27Ornano%20and%20Shomit%20Basu%0AAbstract%3A%20Transformer-based%20architectures%20have%20become%20a%20dominant%20paradigm%20in%20vision%20and%20language%2C%20but%20their%20success%20is%20often%20attributed%20to%20large%20model%20capacity%20and%20massive%20training%20data.%20In%20this%20work%2C%20we%20examine%20how%20self-supervised%20pre-training%2C%20intermediate%20fine-tuning%2C%20and%20downstream%20fine-tuning%20interact%20in%20a%20low-capacity%20regime%2C%20using%20a%205M-parameter%20Vision%20Transformer%20for%20semantic%20segmentation.%20Across%20multiple%20data%20scales%2C%20we%20find%20that%20masked%20image%20modeling%20pre-training%20and%20downstream%20fine-tuning%20reliably%20improve%20performance%2C%20but%20with%20clear%20diminishing%20returns%20as%20supervision%20increases.%20In%20contrast%2C%20inserting%20an%20intermediate%20classification%20fine-tuning%20stage%20consistently%20degrades%20downstream%20performance%2C%20with%20the%20largest%20drops%20occurring%20precisely%20where%20pre-training%20is%20most%20effective.%20Through%20an%20analysis%20of%20patch-level%20representation%20geometry%2C%20we%20show%20that%20classification-based%20intermediate%20supervision%20actively%20interferes%20with%20representations%20learned%20during%20pre-training%20by%20collapsing%20spatial%20structure%20critical%20for%20dense%20prediction.%20These%20results%20indicate%20that%2C%20in%20small%20models%2C%20the%20geometry%20of%20supervision%20matters%20more%20than%20the%20number%20of%20training%20stages%3A%20misaligned%20intermediate%20objectives%20can%20negate%20the%20benefits%20of%20pre-training%20rather%20than%20amplify%20them.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03862v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiminishing%2520Returns%2520in%2520Self-Supervised%2520Learning%26entry.906535625%3DOli%2520Bridge%2520and%2520Huey%2520Sun%2520and%2520Botond%2520Branyicskai-Nagy%2520and%2520Charles%2520D%2527Ornano%2520and%2520Shomit%2520Basu%26entry.1292438233%3DTransformer-based%2520architectures%2520have%2520become%2520a%2520dominant%2520paradigm%2520in%2520vision%2520and%2520language%252C%2520but%2520their%2520success%2520is%2520often%2520attributed%2520to%2520large%2520model%2520capacity%2520and%2520massive%2520training%2520data.%2520In%2520this%2520work%252C%2520we%2520examine%2520how%2520self-supervised%2520pre-training%252C%2520intermediate%2520fine-tuning%252C%2520and%2520downstream%2520fine-tuning%2520interact%2520in%2520a%2520low-capacity%2520regime%252C%2520using%2520a%25205M-parameter%2520Vision%2520Transformer%2520for%2520semantic%2520segmentation.%2520Across%2520multiple%2520data%2520scales%252C%2520we%2520find%2520that%2520masked%2520image%2520modeling%2520pre-training%2520and%2520downstream%2520fine-tuning%2520reliably%2520improve%2520performance%252C%2520but%2520with%2520clear%2520diminishing%2520returns%2520as%2520supervision%2520increases.%2520In%2520contrast%252C%2520inserting%2520an%2520intermediate%2520classification%2520fine-tuning%2520stage%2520consistently%2520degrades%2520downstream%2520performance%252C%2520with%2520the%2520largest%2520drops%2520occurring%2520precisely%2520where%2520pre-training%2520is%2520most%2520effective.%2520Through%2520an%2520analysis%2520of%2520patch-level%2520representation%2520geometry%252C%2520we%2520show%2520that%2520classification-based%2520intermediate%2520supervision%2520actively%2520interferes%2520with%2520representations%2520learned%2520during%2520pre-training%2520by%2520collapsing%2520spatial%2520structure%2520critical%2520for%2520dense%2520prediction.%2520These%2520results%2520indicate%2520that%252C%2520in%2520small%2520models%252C%2520the%2520geometry%2520of%2520supervision%2520matters%2520more%2520than%2520the%2520number%2520of%2520training%2520stages%253A%2520misaligned%2520intermediate%2520objectives%2520can%2520negate%2520the%2520benefits%2520of%2520pre-training%2520rather%2520than%2520amplify%2520them.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03862v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diminishing%20Returns%20in%20Self-Supervised%20Learning&entry.906535625=Oli%20Bridge%20and%20Huey%20Sun%20and%20Botond%20Branyicskai-Nagy%20and%20Charles%20D%27Ornano%20and%20Shomit%20Basu&entry.1292438233=Transformer-based%20architectures%20have%20become%20a%20dominant%20paradigm%20in%20vision%20and%20language%2C%20but%20their%20success%20is%20often%20attributed%20to%20large%20model%20capacity%20and%20massive%20training%20data.%20In%20this%20work%2C%20we%20examine%20how%20self-supervised%20pre-training%2C%20intermediate%20fine-tuning%2C%20and%20downstream%20fine-tuning%20interact%20in%20a%20low-capacity%20regime%2C%20using%20a%205M-parameter%20Vision%20Transformer%20for%20semantic%20segmentation.%20Across%20multiple%20data%20scales%2C%20we%20find%20that%20masked%20image%20modeling%20pre-training%20and%20downstream%20fine-tuning%20reliably%20improve%20performance%2C%20but%20with%20clear%20diminishing%20returns%20as%20supervision%20increases.%20In%20contrast%2C%20inserting%20an%20intermediate%20classification%20fine-tuning%20stage%20consistently%20degrades%20downstream%20performance%2C%20with%20the%20largest%20drops%20occurring%20precisely%20where%20pre-training%20is%20most%20effective.%20Through%20an%20analysis%20of%20patch-level%20representation%20geometry%2C%20we%20show%20that%20classification-based%20intermediate%20supervision%20actively%20interferes%20with%20representations%20learned%20during%20pre-training%20by%20collapsing%20spatial%20structure%20critical%20for%20dense%20prediction.%20These%20results%20indicate%20that%2C%20in%20small%20models%2C%20the%20geometry%20of%20supervision%20matters%20more%20than%20the%20number%20of%20training%20stages%3A%20misaligned%20intermediate%20objectives%20can%20negate%20the%20benefits%20of%20pre-training%20rather%20than%20amplify%20them.&entry.1838667208=http%3A//arxiv.org/abs/2512.03862v2&entry.124074799=Read"},
{"title": "VIBE: Visual Instruction Based Editor", "author": "Grigorii Alekseenko and Aleksandr Gordeev and Irina Tolstykh and Bulat Suleimanov and Vladimir Dokholyan and Georgii Fedorov and Sergey Yakubson and Aleksandra Tsybina and Mikhail Chernyshov and Maksim Kuprashevich", "abstract": "Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems. However, only a limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents a compact, high-throughput instruction-based image editing pipeline that uses a modern 2B-parameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation.", "link": "http://arxiv.org/abs/2601.02242v1", "date": "2026-01-05", "relevancy": 2.2343, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5676}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5643}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VIBE%3A%20Visual%20Instruction%20Based%20Editor&body=Title%3A%20VIBE%3A%20Visual%20Instruction%20Based%20Editor%0AAuthor%3A%20Grigorii%20Alekseenko%20and%20Aleksandr%20Gordeev%20and%20Irina%20Tolstykh%20and%20Bulat%20Suleimanov%20and%20Vladimir%20Dokholyan%20and%20Georgii%20Fedorov%20and%20Sergey%20Yakubson%20and%20Aleksandra%20Tsybina%20and%20Mikhail%20Chernyshov%20and%20Maksim%20Kuprashevich%0AAbstract%3A%20Instruction-based%20image%20editing%20is%20among%20the%20fastest%20developing%20areas%20in%20generative%20AI.%20Over%20the%20past%20year%2C%20the%20field%20has%20reached%20a%20new%20level%2C%20with%20dozens%20of%20open-source%20models%20released%20alongside%20highly%20capable%20commercial%20systems.%20However%2C%20only%20a%20limited%20number%20of%20open-source%20approaches%20currently%20achieve%20real-world%20quality.%20In%20addition%2C%20diffusion%20backbones%2C%20the%20dominant%20choice%20for%20these%20pipelines%2C%20are%20often%20large%20and%20computationally%20expensive%20for%20many%20deployments%20and%20research%20settings%2C%20with%20widely%20used%20variants%20typically%20containing%206B%20to%2020B%20parameters.%20This%20paper%20presents%20a%20compact%2C%20high-throughput%20instruction-based%20image%20editing%20pipeline%20that%20uses%20a%20modern%202B-parameter%20Qwen3-VL%20model%20to%20guide%20the%20editing%20process%20and%20the%201.6B-parameter%20diffusion%20model%20Sana1.5%20for%20image%20generation.%20Our%20design%20decisions%20across%20architecture%2C%20data%20processing%2C%20training%20configuration%2C%20and%20evaluation%20target%20low-cost%20inference%20and%20strict%20source%20consistency%20while%20maintaining%20high%20quality%20across%20the%20major%20edit%20categories%20feasible%20at%20this%20scale.%20Evaluated%20on%20the%20ImgEdit%20and%20GEdit%20benchmarks%2C%20the%20proposed%20method%20matches%20or%20exceeds%20the%20performance%20of%20substantially%20heavier%20baselines%2C%20including%20models%20with%20several%20times%20as%20many%20parameters%20and%20higher%20inference%20cost%2C%20and%20is%20particularly%20strong%20on%20edits%20that%20require%20preserving%20the%20input%20image%2C%20such%20as%20an%20attribute%20adjustment%2C%20object%20removal%2C%20background%20edits%2C%20and%20targeted%20replacement.%20The%20model%20fits%20within%2024%20GB%20of%20GPU%20memory%20and%20generates%20edited%20images%20at%20up%20to%202K%20resolution%20in%20approximately%204%20seconds%20on%20an%20NVIDIA%20H100%20in%20BF16%2C%20without%20additional%20inference%20optimizations%20or%20distillation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02242v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVIBE%253A%2520Visual%2520Instruction%2520Based%2520Editor%26entry.906535625%3DGrigorii%2520Alekseenko%2520and%2520Aleksandr%2520Gordeev%2520and%2520Irina%2520Tolstykh%2520and%2520Bulat%2520Suleimanov%2520and%2520Vladimir%2520Dokholyan%2520and%2520Georgii%2520Fedorov%2520and%2520Sergey%2520Yakubson%2520and%2520Aleksandra%2520Tsybina%2520and%2520Mikhail%2520Chernyshov%2520and%2520Maksim%2520Kuprashevich%26entry.1292438233%3DInstruction-based%2520image%2520editing%2520is%2520among%2520the%2520fastest%2520developing%2520areas%2520in%2520generative%2520AI.%2520Over%2520the%2520past%2520year%252C%2520the%2520field%2520has%2520reached%2520a%2520new%2520level%252C%2520with%2520dozens%2520of%2520open-source%2520models%2520released%2520alongside%2520highly%2520capable%2520commercial%2520systems.%2520However%252C%2520only%2520a%2520limited%2520number%2520of%2520open-source%2520approaches%2520currently%2520achieve%2520real-world%2520quality.%2520In%2520addition%252C%2520diffusion%2520backbones%252C%2520the%2520dominant%2520choice%2520for%2520these%2520pipelines%252C%2520are%2520often%2520large%2520and%2520computationally%2520expensive%2520for%2520many%2520deployments%2520and%2520research%2520settings%252C%2520with%2520widely%2520used%2520variants%2520typically%2520containing%25206B%2520to%252020B%2520parameters.%2520This%2520paper%2520presents%2520a%2520compact%252C%2520high-throughput%2520instruction-based%2520image%2520editing%2520pipeline%2520that%2520uses%2520a%2520modern%25202B-parameter%2520Qwen3-VL%2520model%2520to%2520guide%2520the%2520editing%2520process%2520and%2520the%25201.6B-parameter%2520diffusion%2520model%2520Sana1.5%2520for%2520image%2520generation.%2520Our%2520design%2520decisions%2520across%2520architecture%252C%2520data%2520processing%252C%2520training%2520configuration%252C%2520and%2520evaluation%2520target%2520low-cost%2520inference%2520and%2520strict%2520source%2520consistency%2520while%2520maintaining%2520high%2520quality%2520across%2520the%2520major%2520edit%2520categories%2520feasible%2520at%2520this%2520scale.%2520Evaluated%2520on%2520the%2520ImgEdit%2520and%2520GEdit%2520benchmarks%252C%2520the%2520proposed%2520method%2520matches%2520or%2520exceeds%2520the%2520performance%2520of%2520substantially%2520heavier%2520baselines%252C%2520including%2520models%2520with%2520several%2520times%2520as%2520many%2520parameters%2520and%2520higher%2520inference%2520cost%252C%2520and%2520is%2520particularly%2520strong%2520on%2520edits%2520that%2520require%2520preserving%2520the%2520input%2520image%252C%2520such%2520as%2520an%2520attribute%2520adjustment%252C%2520object%2520removal%252C%2520background%2520edits%252C%2520and%2520targeted%2520replacement.%2520The%2520model%2520fits%2520within%252024%2520GB%2520of%2520GPU%2520memory%2520and%2520generates%2520edited%2520images%2520at%2520up%2520to%25202K%2520resolution%2520in%2520approximately%25204%2520seconds%2520on%2520an%2520NVIDIA%2520H100%2520in%2520BF16%252C%2520without%2520additional%2520inference%2520optimizations%2520or%2520distillation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02242v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIBE%3A%20Visual%20Instruction%20Based%20Editor&entry.906535625=Grigorii%20Alekseenko%20and%20Aleksandr%20Gordeev%20and%20Irina%20Tolstykh%20and%20Bulat%20Suleimanov%20and%20Vladimir%20Dokholyan%20and%20Georgii%20Fedorov%20and%20Sergey%20Yakubson%20and%20Aleksandra%20Tsybina%20and%20Mikhail%20Chernyshov%20and%20Maksim%20Kuprashevich&entry.1292438233=Instruction-based%20image%20editing%20is%20among%20the%20fastest%20developing%20areas%20in%20generative%20AI.%20Over%20the%20past%20year%2C%20the%20field%20has%20reached%20a%20new%20level%2C%20with%20dozens%20of%20open-source%20models%20released%20alongside%20highly%20capable%20commercial%20systems.%20However%2C%20only%20a%20limited%20number%20of%20open-source%20approaches%20currently%20achieve%20real-world%20quality.%20In%20addition%2C%20diffusion%20backbones%2C%20the%20dominant%20choice%20for%20these%20pipelines%2C%20are%20often%20large%20and%20computationally%20expensive%20for%20many%20deployments%20and%20research%20settings%2C%20with%20widely%20used%20variants%20typically%20containing%206B%20to%2020B%20parameters.%20This%20paper%20presents%20a%20compact%2C%20high-throughput%20instruction-based%20image%20editing%20pipeline%20that%20uses%20a%20modern%202B-parameter%20Qwen3-VL%20model%20to%20guide%20the%20editing%20process%20and%20the%201.6B-parameter%20diffusion%20model%20Sana1.5%20for%20image%20generation.%20Our%20design%20decisions%20across%20architecture%2C%20data%20processing%2C%20training%20configuration%2C%20and%20evaluation%20target%20low-cost%20inference%20and%20strict%20source%20consistency%20while%20maintaining%20high%20quality%20across%20the%20major%20edit%20categories%20feasible%20at%20this%20scale.%20Evaluated%20on%20the%20ImgEdit%20and%20GEdit%20benchmarks%2C%20the%20proposed%20method%20matches%20or%20exceeds%20the%20performance%20of%20substantially%20heavier%20baselines%2C%20including%20models%20with%20several%20times%20as%20many%20parameters%20and%20higher%20inference%20cost%2C%20and%20is%20particularly%20strong%20on%20edits%20that%20require%20preserving%20the%20input%20image%2C%20such%20as%20an%20attribute%20adjustment%2C%20object%20removal%2C%20background%20edits%2C%20and%20targeted%20replacement.%20The%20model%20fits%20within%2024%20GB%20of%20GPU%20memory%20and%20generates%20edited%20images%20at%20up%20to%202K%20resolution%20in%20approximately%204%20seconds%20on%20an%20NVIDIA%20H100%20in%20BF16%2C%20without%20additional%20inference%20optimizations%20or%20distillation.&entry.1838667208=http%3A//arxiv.org/abs/2601.02242v1&entry.124074799=Read"},
{"title": "LTLBench: Towards Benchmarks for Evaluating Temporal Reasoning in Large Language Models", "author": "Weizhi Tang and Kwabena Nuamah and Vaishak Belle", "abstract": "Temporal Reasoning (TR) is a critical ability for LLMs to understand and reason over temporal information and relationships between events. To study the TR ability in LLMs, prior works provide different ways for evaluating various aspects of TR ability. In this work, we propose an alternative perspective for evaluating TR ability by leveraging Linear Temporal Logic (LTL), and develop a pipeline to automatically synthesize challenges for assessing the TR ability of LLMs. Based on this pipeline, we construct a dataset, namely LTLBench, consisting of $2000$ TR challenges, and benchmark 12 LLMs across 5 different methods. Furthermore, we conduct additional experiments to investigate the impact of increasing the number of formula operators and events on both LLM performance and the complexity of TR problems. We also perform qualitative analyses of their reasoning processes and the effects of varying the number of events and formula operators, which reveal 3 main issues in their temporal reasoning processes and the unexpected performance changes observed as problem complexity increases. We expect this work to provide valuable insights into the TR ability of LLMs.", "link": "http://arxiv.org/abs/2407.05434v3", "date": "2026-01-05", "relevancy": 2.2324, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4468}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4468}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LTLBench%3A%20Towards%20Benchmarks%20for%20Evaluating%20Temporal%20Reasoning%20in%20Large%20Language%20Models&body=Title%3A%20LTLBench%3A%20Towards%20Benchmarks%20for%20Evaluating%20Temporal%20Reasoning%20in%20Large%20Language%20Models%0AAuthor%3A%20Weizhi%20Tang%20and%20Kwabena%20Nuamah%20and%20Vaishak%20Belle%0AAbstract%3A%20Temporal%20Reasoning%20%28TR%29%20is%20a%20critical%20ability%20for%20LLMs%20to%20understand%20and%20reason%20over%20temporal%20information%20and%20relationships%20between%20events.%20To%20study%20the%20TR%20ability%20in%20LLMs%2C%20prior%20works%20provide%20different%20ways%20for%20evaluating%20various%20aspects%20of%20TR%20ability.%20In%20this%20work%2C%20we%20propose%20an%20alternative%20perspective%20for%20evaluating%20TR%20ability%20by%20leveraging%20Linear%20Temporal%20Logic%20%28LTL%29%2C%20and%20develop%20a%20pipeline%20to%20automatically%20synthesize%20challenges%20for%20assessing%20the%20TR%20ability%20of%20LLMs.%20Based%20on%20this%20pipeline%2C%20we%20construct%20a%20dataset%2C%20namely%20LTLBench%2C%20consisting%20of%20%242000%24%20TR%20challenges%2C%20and%20benchmark%2012%20LLMs%20across%205%20different%20methods.%20Furthermore%2C%20we%20conduct%20additional%20experiments%20to%20investigate%20the%20impact%20of%20increasing%20the%20number%20of%20formula%20operators%20and%20events%20on%20both%20LLM%20performance%20and%20the%20complexity%20of%20TR%20problems.%20We%20also%20perform%20qualitative%20analyses%20of%20their%20reasoning%20processes%20and%20the%20effects%20of%20varying%20the%20number%20of%20events%20and%20formula%20operators%2C%20which%20reveal%203%20main%20issues%20in%20their%20temporal%20reasoning%20processes%20and%20the%20unexpected%20performance%20changes%20observed%20as%20problem%20complexity%20increases.%20We%20expect%20this%20work%20to%20provide%20valuable%20insights%20into%20the%20TR%20ability%20of%20LLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2407.05434v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLTLBench%253A%2520Towards%2520Benchmarks%2520for%2520Evaluating%2520Temporal%2520Reasoning%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DWeizhi%2520Tang%2520and%2520Kwabena%2520Nuamah%2520and%2520Vaishak%2520Belle%26entry.1292438233%3DTemporal%2520Reasoning%2520%2528TR%2529%2520is%2520a%2520critical%2520ability%2520for%2520LLMs%2520to%2520understand%2520and%2520reason%2520over%2520temporal%2520information%2520and%2520relationships%2520between%2520events.%2520To%2520study%2520the%2520TR%2520ability%2520in%2520LLMs%252C%2520prior%2520works%2520provide%2520different%2520ways%2520for%2520evaluating%2520various%2520aspects%2520of%2520TR%2520ability.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520alternative%2520perspective%2520for%2520evaluating%2520TR%2520ability%2520by%2520leveraging%2520Linear%2520Temporal%2520Logic%2520%2528LTL%2529%252C%2520and%2520develop%2520a%2520pipeline%2520to%2520automatically%2520synthesize%2520challenges%2520for%2520assessing%2520the%2520TR%2520ability%2520of%2520LLMs.%2520Based%2520on%2520this%2520pipeline%252C%2520we%2520construct%2520a%2520dataset%252C%2520namely%2520LTLBench%252C%2520consisting%2520of%2520%25242000%2524%2520TR%2520challenges%252C%2520and%2520benchmark%252012%2520LLMs%2520across%25205%2520different%2520methods.%2520Furthermore%252C%2520we%2520conduct%2520additional%2520experiments%2520to%2520investigate%2520the%2520impact%2520of%2520increasing%2520the%2520number%2520of%2520formula%2520operators%2520and%2520events%2520on%2520both%2520LLM%2520performance%2520and%2520the%2520complexity%2520of%2520TR%2520problems.%2520We%2520also%2520perform%2520qualitative%2520analyses%2520of%2520their%2520reasoning%2520processes%2520and%2520the%2520effects%2520of%2520varying%2520the%2520number%2520of%2520events%2520and%2520formula%2520operators%252C%2520which%2520reveal%25203%2520main%2520issues%2520in%2520their%2520temporal%2520reasoning%2520processes%2520and%2520the%2520unexpected%2520performance%2520changes%2520observed%2520as%2520problem%2520complexity%2520increases.%2520We%2520expect%2520this%2520work%2520to%2520provide%2520valuable%2520insights%2520into%2520the%2520TR%2520ability%2520of%2520LLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05434v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LTLBench%3A%20Towards%20Benchmarks%20for%20Evaluating%20Temporal%20Reasoning%20in%20Large%20Language%20Models&entry.906535625=Weizhi%20Tang%20and%20Kwabena%20Nuamah%20and%20Vaishak%20Belle&entry.1292438233=Temporal%20Reasoning%20%28TR%29%20is%20a%20critical%20ability%20for%20LLMs%20to%20understand%20and%20reason%20over%20temporal%20information%20and%20relationships%20between%20events.%20To%20study%20the%20TR%20ability%20in%20LLMs%2C%20prior%20works%20provide%20different%20ways%20for%20evaluating%20various%20aspects%20of%20TR%20ability.%20In%20this%20work%2C%20we%20propose%20an%20alternative%20perspective%20for%20evaluating%20TR%20ability%20by%20leveraging%20Linear%20Temporal%20Logic%20%28LTL%29%2C%20and%20develop%20a%20pipeline%20to%20automatically%20synthesize%20challenges%20for%20assessing%20the%20TR%20ability%20of%20LLMs.%20Based%20on%20this%20pipeline%2C%20we%20construct%20a%20dataset%2C%20namely%20LTLBench%2C%20consisting%20of%20%242000%24%20TR%20challenges%2C%20and%20benchmark%2012%20LLMs%20across%205%20different%20methods.%20Furthermore%2C%20we%20conduct%20additional%20experiments%20to%20investigate%20the%20impact%20of%20increasing%20the%20number%20of%20formula%20operators%20and%20events%20on%20both%20LLM%20performance%20and%20the%20complexity%20of%20TR%20problems.%20We%20also%20perform%20qualitative%20analyses%20of%20their%20reasoning%20processes%20and%20the%20effects%20of%20varying%20the%20number%20of%20events%20and%20formula%20operators%2C%20which%20reveal%203%20main%20issues%20in%20their%20temporal%20reasoning%20processes%20and%20the%20unexpected%20performance%20changes%20observed%20as%20problem%20complexity%20increases.%20We%20expect%20this%20work%20to%20provide%20valuable%20insights%20into%20the%20TR%20ability%20of%20LLMs.&entry.1838667208=http%3A//arxiv.org/abs/2407.05434v3&entry.124074799=Read"},
{"title": "Realistic adversarial scenario generation via human-like pedestrian model for autonomous vehicle control parameter optimisation", "author": "Yueyang Wang and Mehmet Dogar and Gustav Markkula", "abstract": "Autonomous vehicles (AVs) are rapidly advancing and are expected to play a central role in future mobility. Ensuring their safe deployment requires reliable interaction with other road users, not least pedestrians. Direct testing on public roads is costly and unsafe for rare but critical interactions, making simulation a practical alternative. Within simulation-based testing, adversarial scenarios are widely used to probe safety limits, but many prioritise difficulty over realism, producing exaggerated behaviours which may result in AV controllers that are overly conservative. We propose an alternative method, instead using a cognitively inspired pedestrian model featuring both inter-individual and intra-individual variability to generate behaviourally plausible adversarial scenarios. We provide a proof of concept demonstration of this method's potential for AV control optimisation, in closed-loop testing and tuning of an AV controller. Our results show that replacing the rule-based CARLA pedestrian with the human-like model yields more realistic gap acceptance patterns and smoother vehicle decelerations. Unsafe interactions occur only for certain pedestrian individuals and conditions, underscoring the importance of human variability in AV testing. Adversarial scenarios generated by this model can be used to optimise AV control towards safer and more efficient behaviour. Overall, this work illustrates how incorporating human-like road user models into simulation-based adversarial testing can enhance the credibility of AV evaluation and provide a practical basis to behaviourally informed controller optimisation.", "link": "http://arxiv.org/abs/2601.02082v1", "date": "2026-01-05", "relevancy": 2.2246, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.576}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5677}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Realistic%20adversarial%20scenario%20generation%20via%20human-like%20pedestrian%20model%20for%20autonomous%20vehicle%20control%20parameter%20optimisation&body=Title%3A%20Realistic%20adversarial%20scenario%20generation%20via%20human-like%20pedestrian%20model%20for%20autonomous%20vehicle%20control%20parameter%20optimisation%0AAuthor%3A%20Yueyang%20Wang%20and%20Mehmet%20Dogar%20and%20Gustav%20Markkula%0AAbstract%3A%20Autonomous%20vehicles%20%28AVs%29%20are%20rapidly%20advancing%20and%20are%20expected%20to%20play%20a%20central%20role%20in%20future%20mobility.%20Ensuring%20their%20safe%20deployment%20requires%20reliable%20interaction%20with%20other%20road%20users%2C%20not%20least%20pedestrians.%20Direct%20testing%20on%20public%20roads%20is%20costly%20and%20unsafe%20for%20rare%20but%20critical%20interactions%2C%20making%20simulation%20a%20practical%20alternative.%20Within%20simulation-based%20testing%2C%20adversarial%20scenarios%20are%20widely%20used%20to%20probe%20safety%20limits%2C%20but%20many%20prioritise%20difficulty%20over%20realism%2C%20producing%20exaggerated%20behaviours%20which%20may%20result%20in%20AV%20controllers%20that%20are%20overly%20conservative.%20We%20propose%20an%20alternative%20method%2C%20instead%20using%20a%20cognitively%20inspired%20pedestrian%20model%20featuring%20both%20inter-individual%20and%20intra-individual%20variability%20to%20generate%20behaviourally%20plausible%20adversarial%20scenarios.%20We%20provide%20a%20proof%20of%20concept%20demonstration%20of%20this%20method%27s%20potential%20for%20AV%20control%20optimisation%2C%20in%20closed-loop%20testing%20and%20tuning%20of%20an%20AV%20controller.%20Our%20results%20show%20that%20replacing%20the%20rule-based%20CARLA%20pedestrian%20with%20the%20human-like%20model%20yields%20more%20realistic%20gap%20acceptance%20patterns%20and%20smoother%20vehicle%20decelerations.%20Unsafe%20interactions%20occur%20only%20for%20certain%20pedestrian%20individuals%20and%20conditions%2C%20underscoring%20the%20importance%20of%20human%20variability%20in%20AV%20testing.%20Adversarial%20scenarios%20generated%20by%20this%20model%20can%20be%20used%20to%20optimise%20AV%20control%20towards%20safer%20and%20more%20efficient%20behaviour.%20Overall%2C%20this%20work%20illustrates%20how%20incorporating%20human-like%20road%20user%20models%20into%20simulation-based%20adversarial%20testing%20can%20enhance%20the%20credibility%20of%20AV%20evaluation%20and%20provide%20a%20practical%20basis%20to%20behaviourally%20informed%20controller%20optimisation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02082v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRealistic%2520adversarial%2520scenario%2520generation%2520via%2520human-like%2520pedestrian%2520model%2520for%2520autonomous%2520vehicle%2520control%2520parameter%2520optimisation%26entry.906535625%3DYueyang%2520Wang%2520and%2520Mehmet%2520Dogar%2520and%2520Gustav%2520Markkula%26entry.1292438233%3DAutonomous%2520vehicles%2520%2528AVs%2529%2520are%2520rapidly%2520advancing%2520and%2520are%2520expected%2520to%2520play%2520a%2520central%2520role%2520in%2520future%2520mobility.%2520Ensuring%2520their%2520safe%2520deployment%2520requires%2520reliable%2520interaction%2520with%2520other%2520road%2520users%252C%2520not%2520least%2520pedestrians.%2520Direct%2520testing%2520on%2520public%2520roads%2520is%2520costly%2520and%2520unsafe%2520for%2520rare%2520but%2520critical%2520interactions%252C%2520making%2520simulation%2520a%2520practical%2520alternative.%2520Within%2520simulation-based%2520testing%252C%2520adversarial%2520scenarios%2520are%2520widely%2520used%2520to%2520probe%2520safety%2520limits%252C%2520but%2520many%2520prioritise%2520difficulty%2520over%2520realism%252C%2520producing%2520exaggerated%2520behaviours%2520which%2520may%2520result%2520in%2520AV%2520controllers%2520that%2520are%2520overly%2520conservative.%2520We%2520propose%2520an%2520alternative%2520method%252C%2520instead%2520using%2520a%2520cognitively%2520inspired%2520pedestrian%2520model%2520featuring%2520both%2520inter-individual%2520and%2520intra-individual%2520variability%2520to%2520generate%2520behaviourally%2520plausible%2520adversarial%2520scenarios.%2520We%2520provide%2520a%2520proof%2520of%2520concept%2520demonstration%2520of%2520this%2520method%2527s%2520potential%2520for%2520AV%2520control%2520optimisation%252C%2520in%2520closed-loop%2520testing%2520and%2520tuning%2520of%2520an%2520AV%2520controller.%2520Our%2520results%2520show%2520that%2520replacing%2520the%2520rule-based%2520CARLA%2520pedestrian%2520with%2520the%2520human-like%2520model%2520yields%2520more%2520realistic%2520gap%2520acceptance%2520patterns%2520and%2520smoother%2520vehicle%2520decelerations.%2520Unsafe%2520interactions%2520occur%2520only%2520for%2520certain%2520pedestrian%2520individuals%2520and%2520conditions%252C%2520underscoring%2520the%2520importance%2520of%2520human%2520variability%2520in%2520AV%2520testing.%2520Adversarial%2520scenarios%2520generated%2520by%2520this%2520model%2520can%2520be%2520used%2520to%2520optimise%2520AV%2520control%2520towards%2520safer%2520and%2520more%2520efficient%2520behaviour.%2520Overall%252C%2520this%2520work%2520illustrates%2520how%2520incorporating%2520human-like%2520road%2520user%2520models%2520into%2520simulation-based%2520adversarial%2520testing%2520can%2520enhance%2520the%2520credibility%2520of%2520AV%2520evaluation%2520and%2520provide%2520a%2520practical%2520basis%2520to%2520behaviourally%2520informed%2520controller%2520optimisation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02082v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Realistic%20adversarial%20scenario%20generation%20via%20human-like%20pedestrian%20model%20for%20autonomous%20vehicle%20control%20parameter%20optimisation&entry.906535625=Yueyang%20Wang%20and%20Mehmet%20Dogar%20and%20Gustav%20Markkula&entry.1292438233=Autonomous%20vehicles%20%28AVs%29%20are%20rapidly%20advancing%20and%20are%20expected%20to%20play%20a%20central%20role%20in%20future%20mobility.%20Ensuring%20their%20safe%20deployment%20requires%20reliable%20interaction%20with%20other%20road%20users%2C%20not%20least%20pedestrians.%20Direct%20testing%20on%20public%20roads%20is%20costly%20and%20unsafe%20for%20rare%20but%20critical%20interactions%2C%20making%20simulation%20a%20practical%20alternative.%20Within%20simulation-based%20testing%2C%20adversarial%20scenarios%20are%20widely%20used%20to%20probe%20safety%20limits%2C%20but%20many%20prioritise%20difficulty%20over%20realism%2C%20producing%20exaggerated%20behaviours%20which%20may%20result%20in%20AV%20controllers%20that%20are%20overly%20conservative.%20We%20propose%20an%20alternative%20method%2C%20instead%20using%20a%20cognitively%20inspired%20pedestrian%20model%20featuring%20both%20inter-individual%20and%20intra-individual%20variability%20to%20generate%20behaviourally%20plausible%20adversarial%20scenarios.%20We%20provide%20a%20proof%20of%20concept%20demonstration%20of%20this%20method%27s%20potential%20for%20AV%20control%20optimisation%2C%20in%20closed-loop%20testing%20and%20tuning%20of%20an%20AV%20controller.%20Our%20results%20show%20that%20replacing%20the%20rule-based%20CARLA%20pedestrian%20with%20the%20human-like%20model%20yields%20more%20realistic%20gap%20acceptance%20patterns%20and%20smoother%20vehicle%20decelerations.%20Unsafe%20interactions%20occur%20only%20for%20certain%20pedestrian%20individuals%20and%20conditions%2C%20underscoring%20the%20importance%20of%20human%20variability%20in%20AV%20testing.%20Adversarial%20scenarios%20generated%20by%20this%20model%20can%20be%20used%20to%20optimise%20AV%20control%20towards%20safer%20and%20more%20efficient%20behaviour.%20Overall%2C%20this%20work%20illustrates%20how%20incorporating%20human-like%20road%20user%20models%20into%20simulation-based%20adversarial%20testing%20can%20enhance%20the%20credibility%20of%20AV%20evaluation%20and%20provide%20a%20practical%20basis%20to%20behaviourally%20informed%20controller%20optimisation.&entry.1838667208=http%3A//arxiv.org/abs/2601.02082v1&entry.124074799=Read"},
{"title": "Exploring Diversity, Novelty, and Popularity Bias in ChatGPT's Recommendations", "author": "Dario Di Palma and Giovanni Maria Biancofiore and Vito Walter Anelli and Fedelucio Narducci and Tommaso Di Noia", "abstract": "ChatGPT has emerged as a versatile tool, demonstrating capabilities across diverse domains. Given these successes, the Recommender Systems (RSs) community has begun investigating its applications within recommendation scenarios primarily focusing on accuracy. While the integration of ChatGPT into RSs has garnered significant attention, a comprehensive analysis of its performance across various dimensions remains largely unexplored. Specifically, the capabilities of providing diverse and novel recommendations or exploring potential biases such as popularity bias have not been thoroughly examined. As the use of these models continues to expand, understanding these aspects is crucial for enhancing user satisfaction and achieving long-term personalization.\n  This study investigates the recommendations provided by ChatGPT-3.5 and ChatGPT-4 by assessing ChatGPT's capabilities in terms of diversity, novelty, and popularity bias. We evaluate these models on three distinct datasets and assess their performance in Top-N recommendation and cold-start scenarios. The findings reveal that ChatGPT-4 matches or surpasses traditional recommenders, demonstrating the ability to balance novelty and diversity in recommendations. Furthermore, in the cold-start scenario, ChatGPT models exhibit superior performance in both accuracy and novelty, suggesting they can be particularly beneficial for new users. This research highlights the strengths and limitations of ChatGPT's recommendations, offering new perspectives on the capacity of these models to provide recommendations beyond accuracy-focused metrics.", "link": "http://arxiv.org/abs/2601.01997v1", "date": "2026-01-05", "relevancy": 2.2209, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4624}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4363}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4339}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Diversity%2C%20Novelty%2C%20and%20Popularity%20Bias%20in%20ChatGPT%27s%20Recommendations&body=Title%3A%20Exploring%20Diversity%2C%20Novelty%2C%20and%20Popularity%20Bias%20in%20ChatGPT%27s%20Recommendations%0AAuthor%3A%20Dario%20Di%20Palma%20and%20Giovanni%20Maria%20Biancofiore%20and%20Vito%20Walter%20Anelli%20and%20Fedelucio%20Narducci%20and%20Tommaso%20Di%20Noia%0AAbstract%3A%20ChatGPT%20has%20emerged%20as%20a%20versatile%20tool%2C%20demonstrating%20capabilities%20across%20diverse%20domains.%20Given%20these%20successes%2C%20the%20Recommender%20Systems%20%28RSs%29%20community%20has%20begun%20investigating%20its%20applications%20within%20recommendation%20scenarios%20primarily%20focusing%20on%20accuracy.%20While%20the%20integration%20of%20ChatGPT%20into%20RSs%20has%20garnered%20significant%20attention%2C%20a%20comprehensive%20analysis%20of%20its%20performance%20across%20various%20dimensions%20remains%20largely%20unexplored.%20Specifically%2C%20the%20capabilities%20of%20providing%20diverse%20and%20novel%20recommendations%20or%20exploring%20potential%20biases%20such%20as%20popularity%20bias%20have%20not%20been%20thoroughly%20examined.%20As%20the%20use%20of%20these%20models%20continues%20to%20expand%2C%20understanding%20these%20aspects%20is%20crucial%20for%20enhancing%20user%20satisfaction%20and%20achieving%20long-term%20personalization.%0A%20%20This%20study%20investigates%20the%20recommendations%20provided%20by%20ChatGPT-3.5%20and%20ChatGPT-4%20by%20assessing%20ChatGPT%27s%20capabilities%20in%20terms%20of%20diversity%2C%20novelty%2C%20and%20popularity%20bias.%20We%20evaluate%20these%20models%20on%20three%20distinct%20datasets%20and%20assess%20their%20performance%20in%20Top-N%20recommendation%20and%20cold-start%20scenarios.%20The%20findings%20reveal%20that%20ChatGPT-4%20matches%20or%20surpasses%20traditional%20recommenders%2C%20demonstrating%20the%20ability%20to%20balance%20novelty%20and%20diversity%20in%20recommendations.%20Furthermore%2C%20in%20the%20cold-start%20scenario%2C%20ChatGPT%20models%20exhibit%20superior%20performance%20in%20both%20accuracy%20and%20novelty%2C%20suggesting%20they%20can%20be%20particularly%20beneficial%20for%20new%20users.%20This%20research%20highlights%20the%20strengths%20and%20limitations%20of%20ChatGPT%27s%20recommendations%2C%20offering%20new%20perspectives%20on%20the%20capacity%20of%20these%20models%20to%20provide%20recommendations%20beyond%20accuracy-focused%20metrics.%0ALink%3A%20http%3A//arxiv.org/abs/2601.01997v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Diversity%252C%2520Novelty%252C%2520and%2520Popularity%2520Bias%2520in%2520ChatGPT%2527s%2520Recommendations%26entry.906535625%3DDario%2520Di%2520Palma%2520and%2520Giovanni%2520Maria%2520Biancofiore%2520and%2520Vito%2520Walter%2520Anelli%2520and%2520Fedelucio%2520Narducci%2520and%2520Tommaso%2520Di%2520Noia%26entry.1292438233%3DChatGPT%2520has%2520emerged%2520as%2520a%2520versatile%2520tool%252C%2520demonstrating%2520capabilities%2520across%2520diverse%2520domains.%2520Given%2520these%2520successes%252C%2520the%2520Recommender%2520Systems%2520%2528RSs%2529%2520community%2520has%2520begun%2520investigating%2520its%2520applications%2520within%2520recommendation%2520scenarios%2520primarily%2520focusing%2520on%2520accuracy.%2520While%2520the%2520integration%2520of%2520ChatGPT%2520into%2520RSs%2520has%2520garnered%2520significant%2520attention%252C%2520a%2520comprehensive%2520analysis%2520of%2520its%2520performance%2520across%2520various%2520dimensions%2520remains%2520largely%2520unexplored.%2520Specifically%252C%2520the%2520capabilities%2520of%2520providing%2520diverse%2520and%2520novel%2520recommendations%2520or%2520exploring%2520potential%2520biases%2520such%2520as%2520popularity%2520bias%2520have%2520not%2520been%2520thoroughly%2520examined.%2520As%2520the%2520use%2520of%2520these%2520models%2520continues%2520to%2520expand%252C%2520understanding%2520these%2520aspects%2520is%2520crucial%2520for%2520enhancing%2520user%2520satisfaction%2520and%2520achieving%2520long-term%2520personalization.%250A%2520%2520This%2520study%2520investigates%2520the%2520recommendations%2520provided%2520by%2520ChatGPT-3.5%2520and%2520ChatGPT-4%2520by%2520assessing%2520ChatGPT%2527s%2520capabilities%2520in%2520terms%2520of%2520diversity%252C%2520novelty%252C%2520and%2520popularity%2520bias.%2520We%2520evaluate%2520these%2520models%2520on%2520three%2520distinct%2520datasets%2520and%2520assess%2520their%2520performance%2520in%2520Top-N%2520recommendation%2520and%2520cold-start%2520scenarios.%2520The%2520findings%2520reveal%2520that%2520ChatGPT-4%2520matches%2520or%2520surpasses%2520traditional%2520recommenders%252C%2520demonstrating%2520the%2520ability%2520to%2520balance%2520novelty%2520and%2520diversity%2520in%2520recommendations.%2520Furthermore%252C%2520in%2520the%2520cold-start%2520scenario%252C%2520ChatGPT%2520models%2520exhibit%2520superior%2520performance%2520in%2520both%2520accuracy%2520and%2520novelty%252C%2520suggesting%2520they%2520can%2520be%2520particularly%2520beneficial%2520for%2520new%2520users.%2520This%2520research%2520highlights%2520the%2520strengths%2520and%2520limitations%2520of%2520ChatGPT%2527s%2520recommendations%252C%2520offering%2520new%2520perspectives%2520on%2520the%2520capacity%2520of%2520these%2520models%2520to%2520provide%2520recommendations%2520beyond%2520accuracy-focused%2520metrics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.01997v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Diversity%2C%20Novelty%2C%20and%20Popularity%20Bias%20in%20ChatGPT%27s%20Recommendations&entry.906535625=Dario%20Di%20Palma%20and%20Giovanni%20Maria%20Biancofiore%20and%20Vito%20Walter%20Anelli%20and%20Fedelucio%20Narducci%20and%20Tommaso%20Di%20Noia&entry.1292438233=ChatGPT%20has%20emerged%20as%20a%20versatile%20tool%2C%20demonstrating%20capabilities%20across%20diverse%20domains.%20Given%20these%20successes%2C%20the%20Recommender%20Systems%20%28RSs%29%20community%20has%20begun%20investigating%20its%20applications%20within%20recommendation%20scenarios%20primarily%20focusing%20on%20accuracy.%20While%20the%20integration%20of%20ChatGPT%20into%20RSs%20has%20garnered%20significant%20attention%2C%20a%20comprehensive%20analysis%20of%20its%20performance%20across%20various%20dimensions%20remains%20largely%20unexplored.%20Specifically%2C%20the%20capabilities%20of%20providing%20diverse%20and%20novel%20recommendations%20or%20exploring%20potential%20biases%20such%20as%20popularity%20bias%20have%20not%20been%20thoroughly%20examined.%20As%20the%20use%20of%20these%20models%20continues%20to%20expand%2C%20understanding%20these%20aspects%20is%20crucial%20for%20enhancing%20user%20satisfaction%20and%20achieving%20long-term%20personalization.%0A%20%20This%20study%20investigates%20the%20recommendations%20provided%20by%20ChatGPT-3.5%20and%20ChatGPT-4%20by%20assessing%20ChatGPT%27s%20capabilities%20in%20terms%20of%20diversity%2C%20novelty%2C%20and%20popularity%20bias.%20We%20evaluate%20these%20models%20on%20three%20distinct%20datasets%20and%20assess%20their%20performance%20in%20Top-N%20recommendation%20and%20cold-start%20scenarios.%20The%20findings%20reveal%20that%20ChatGPT-4%20matches%20or%20surpasses%20traditional%20recommenders%2C%20demonstrating%20the%20ability%20to%20balance%20novelty%20and%20diversity%20in%20recommendations.%20Furthermore%2C%20in%20the%20cold-start%20scenario%2C%20ChatGPT%20models%20exhibit%20superior%20performance%20in%20both%20accuracy%20and%20novelty%2C%20suggesting%20they%20can%20be%20particularly%20beneficial%20for%20new%20users.%20This%20research%20highlights%20the%20strengths%20and%20limitations%20of%20ChatGPT%27s%20recommendations%2C%20offering%20new%20perspectives%20on%20the%20capacity%20of%20these%20models%20to%20provide%20recommendations%20beyond%20accuracy-focused%20metrics.&entry.1838667208=http%3A//arxiv.org/abs/2601.01997v1&entry.124074799=Read"},
{"title": "Comparison of generalised additive models and neural networks in applications: A systematic review", "author": "Jessica Doohan and Lucas Kook and Kevin Burke", "abstract": "Neural networks have become a popular tool in predictive modelling, more commonly associated with machine learning and artificial intelligence than with statistics. Generalised Additive Models (GAMs) are flexible non-linear statistical models that retain interpretability. Both are state-of-the-art in their own right, with their respective advantages and disadvantages. This paper analyses how these two model classes have performed on real-world tabular data. Following PRISMA guidelines, we conducted a systematic review of papers that performed empirical comparisons of GAMs and neural networks. Eligible papers were identified, yielding 143 papers, with 430 datasets. Key attributes at both paper and dataset levels were extracted and reported. Beyond summarising comparisons, we analyse reported performance metrics using mixed-effects modelling to investigate potential characteristics that can explain and quantify observed differences, including application area, study year, sample size, number of predictors, and neural network complexity. Across datasets, no consistent evidence of superiority was found for either GAMs or neural networks when considering the most frequently reported metrics (RMSE, $R^2$, and AUC). Neural networks tended to outperform in larger datasets and in those with more predictors, but this advantage narrowed over time. Conversely, GAMs remained competitive, particularly in smaller data settings, while retaining interpretability. Reporting of dataset characteristics and neural network complexity was incomplete in much of the literature, limiting transparency and reproducibility. This review highlights that GAMs and neural networks should be viewed as complementary approaches rather than competitors. For many tabular applications, the performance trade-off is modest, and interpretability may favour GAMs.", "link": "http://arxiv.org/abs/2510.24601v2", "date": "2026-01-05", "relevancy": 2.2145, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4737}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4291}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparison%20of%20generalised%20additive%20models%20and%20neural%20networks%20in%20applications%3A%20A%20systematic%20review&body=Title%3A%20Comparison%20of%20generalised%20additive%20models%20and%20neural%20networks%20in%20applications%3A%20A%20systematic%20review%0AAuthor%3A%20Jessica%20Doohan%20and%20Lucas%20Kook%20and%20Kevin%20Burke%0AAbstract%3A%20Neural%20networks%20have%20become%20a%20popular%20tool%20in%20predictive%20modelling%2C%20more%20commonly%20associated%20with%20machine%20learning%20and%20artificial%20intelligence%20than%20with%20statistics.%20Generalised%20Additive%20Models%20%28GAMs%29%20are%20flexible%20non-linear%20statistical%20models%20that%20retain%20interpretability.%20Both%20are%20state-of-the-art%20in%20their%20own%20right%2C%20with%20their%20respective%20advantages%20and%20disadvantages.%20This%20paper%20analyses%20how%20these%20two%20model%20classes%20have%20performed%20on%20real-world%20tabular%20data.%20Following%20PRISMA%20guidelines%2C%20we%20conducted%20a%20systematic%20review%20of%20papers%20that%20performed%20empirical%20comparisons%20of%20GAMs%20and%20neural%20networks.%20Eligible%20papers%20were%20identified%2C%20yielding%20143%20papers%2C%20with%20430%20datasets.%20Key%20attributes%20at%20both%20paper%20and%20dataset%20levels%20were%20extracted%20and%20reported.%20Beyond%20summarising%20comparisons%2C%20we%20analyse%20reported%20performance%20metrics%20using%20mixed-effects%20modelling%20to%20investigate%20potential%20characteristics%20that%20can%20explain%20and%20quantify%20observed%20differences%2C%20including%20application%20area%2C%20study%20year%2C%20sample%20size%2C%20number%20of%20predictors%2C%20and%20neural%20network%20complexity.%20Across%20datasets%2C%20no%20consistent%20evidence%20of%20superiority%20was%20found%20for%20either%20GAMs%20or%20neural%20networks%20when%20considering%20the%20most%20frequently%20reported%20metrics%20%28RMSE%2C%20%24R%5E2%24%2C%20and%20AUC%29.%20Neural%20networks%20tended%20to%20outperform%20in%20larger%20datasets%20and%20in%20those%20with%20more%20predictors%2C%20but%20this%20advantage%20narrowed%20over%20time.%20Conversely%2C%20GAMs%20remained%20competitive%2C%20particularly%20in%20smaller%20data%20settings%2C%20while%20retaining%20interpretability.%20Reporting%20of%20dataset%20characteristics%20and%20neural%20network%20complexity%20was%20incomplete%20in%20much%20of%20the%20literature%2C%20limiting%20transparency%20and%20reproducibility.%20This%20review%20highlights%20that%20GAMs%20and%20neural%20networks%20should%20be%20viewed%20as%20complementary%20approaches%20rather%20than%20competitors.%20For%20many%20tabular%20applications%2C%20the%20performance%20trade-off%20is%20modest%2C%20and%20interpretability%20may%20favour%20GAMs.%0ALink%3A%20http%3A//arxiv.org/abs/2510.24601v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparison%2520of%2520generalised%2520additive%2520models%2520and%2520neural%2520networks%2520in%2520applications%253A%2520A%2520systematic%2520review%26entry.906535625%3DJessica%2520Doohan%2520and%2520Lucas%2520Kook%2520and%2520Kevin%2520Burke%26entry.1292438233%3DNeural%2520networks%2520have%2520become%2520a%2520popular%2520tool%2520in%2520predictive%2520modelling%252C%2520more%2520commonly%2520associated%2520with%2520machine%2520learning%2520and%2520artificial%2520intelligence%2520than%2520with%2520statistics.%2520Generalised%2520Additive%2520Models%2520%2528GAMs%2529%2520are%2520flexible%2520non-linear%2520statistical%2520models%2520that%2520retain%2520interpretability.%2520Both%2520are%2520state-of-the-art%2520in%2520their%2520own%2520right%252C%2520with%2520their%2520respective%2520advantages%2520and%2520disadvantages.%2520This%2520paper%2520analyses%2520how%2520these%2520two%2520model%2520classes%2520have%2520performed%2520on%2520real-world%2520tabular%2520data.%2520Following%2520PRISMA%2520guidelines%252C%2520we%2520conducted%2520a%2520systematic%2520review%2520of%2520papers%2520that%2520performed%2520empirical%2520comparisons%2520of%2520GAMs%2520and%2520neural%2520networks.%2520Eligible%2520papers%2520were%2520identified%252C%2520yielding%2520143%2520papers%252C%2520with%2520430%2520datasets.%2520Key%2520attributes%2520at%2520both%2520paper%2520and%2520dataset%2520levels%2520were%2520extracted%2520and%2520reported.%2520Beyond%2520summarising%2520comparisons%252C%2520we%2520analyse%2520reported%2520performance%2520metrics%2520using%2520mixed-effects%2520modelling%2520to%2520investigate%2520potential%2520characteristics%2520that%2520can%2520explain%2520and%2520quantify%2520observed%2520differences%252C%2520including%2520application%2520area%252C%2520study%2520year%252C%2520sample%2520size%252C%2520number%2520of%2520predictors%252C%2520and%2520neural%2520network%2520complexity.%2520Across%2520datasets%252C%2520no%2520consistent%2520evidence%2520of%2520superiority%2520was%2520found%2520for%2520either%2520GAMs%2520or%2520neural%2520networks%2520when%2520considering%2520the%2520most%2520frequently%2520reported%2520metrics%2520%2528RMSE%252C%2520%2524R%255E2%2524%252C%2520and%2520AUC%2529.%2520Neural%2520networks%2520tended%2520to%2520outperform%2520in%2520larger%2520datasets%2520and%2520in%2520those%2520with%2520more%2520predictors%252C%2520but%2520this%2520advantage%2520narrowed%2520over%2520time.%2520Conversely%252C%2520GAMs%2520remained%2520competitive%252C%2520particularly%2520in%2520smaller%2520data%2520settings%252C%2520while%2520retaining%2520interpretability.%2520Reporting%2520of%2520dataset%2520characteristics%2520and%2520neural%2520network%2520complexity%2520was%2520incomplete%2520in%2520much%2520of%2520the%2520literature%252C%2520limiting%2520transparency%2520and%2520reproducibility.%2520This%2520review%2520highlights%2520that%2520GAMs%2520and%2520neural%2520networks%2520should%2520be%2520viewed%2520as%2520complementary%2520approaches%2520rather%2520than%2520competitors.%2520For%2520many%2520tabular%2520applications%252C%2520the%2520performance%2520trade-off%2520is%2520modest%252C%2520and%2520interpretability%2520may%2520favour%2520GAMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.24601v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparison%20of%20generalised%20additive%20models%20and%20neural%20networks%20in%20applications%3A%20A%20systematic%20review&entry.906535625=Jessica%20Doohan%20and%20Lucas%20Kook%20and%20Kevin%20Burke&entry.1292438233=Neural%20networks%20have%20become%20a%20popular%20tool%20in%20predictive%20modelling%2C%20more%20commonly%20associated%20with%20machine%20learning%20and%20artificial%20intelligence%20than%20with%20statistics.%20Generalised%20Additive%20Models%20%28GAMs%29%20are%20flexible%20non-linear%20statistical%20models%20that%20retain%20interpretability.%20Both%20are%20state-of-the-art%20in%20their%20own%20right%2C%20with%20their%20respective%20advantages%20and%20disadvantages.%20This%20paper%20analyses%20how%20these%20two%20model%20classes%20have%20performed%20on%20real-world%20tabular%20data.%20Following%20PRISMA%20guidelines%2C%20we%20conducted%20a%20systematic%20review%20of%20papers%20that%20performed%20empirical%20comparisons%20of%20GAMs%20and%20neural%20networks.%20Eligible%20papers%20were%20identified%2C%20yielding%20143%20papers%2C%20with%20430%20datasets.%20Key%20attributes%20at%20both%20paper%20and%20dataset%20levels%20were%20extracted%20and%20reported.%20Beyond%20summarising%20comparisons%2C%20we%20analyse%20reported%20performance%20metrics%20using%20mixed-effects%20modelling%20to%20investigate%20potential%20characteristics%20that%20can%20explain%20and%20quantify%20observed%20differences%2C%20including%20application%20area%2C%20study%20year%2C%20sample%20size%2C%20number%20of%20predictors%2C%20and%20neural%20network%20complexity.%20Across%20datasets%2C%20no%20consistent%20evidence%20of%20superiority%20was%20found%20for%20either%20GAMs%20or%20neural%20networks%20when%20considering%20the%20most%20frequently%20reported%20metrics%20%28RMSE%2C%20%24R%5E2%24%2C%20and%20AUC%29.%20Neural%20networks%20tended%20to%20outperform%20in%20larger%20datasets%20and%20in%20those%20with%20more%20predictors%2C%20but%20this%20advantage%20narrowed%20over%20time.%20Conversely%2C%20GAMs%20remained%20competitive%2C%20particularly%20in%20smaller%20data%20settings%2C%20while%20retaining%20interpretability.%20Reporting%20of%20dataset%20characteristics%20and%20neural%20network%20complexity%20was%20incomplete%20in%20much%20of%20the%20literature%2C%20limiting%20transparency%20and%20reproducibility.%20This%20review%20highlights%20that%20GAMs%20and%20neural%20networks%20should%20be%20viewed%20as%20complementary%20approaches%20rather%20than%20competitors.%20For%20many%20tabular%20applications%2C%20the%20performance%20trade-off%20is%20modest%2C%20and%20interpretability%20may%20favour%20GAMs.&entry.1838667208=http%3A//arxiv.org/abs/2510.24601v2&entry.124074799=Read"},
{"title": "Learning with Monotone Adversarial Corruptions", "author": "Kasper Green Larsen and Chirag Pabbaraju and Abhishek Shetty", "abstract": "We study the extent to which standard machine learning algorithms rely on exchangeability and independence of data by introducing a monotone adversarial corruption model. In this model, an adversary, upon looking at a \"clean\" i.i.d. dataset, inserts additional \"corrupted\" points of their choice into the dataset. These added points are constrained to be monotone corruptions, in that they get labeled according to the ground-truth target function. Perhaps surprisingly, we demonstrate that in this setting, all known optimal learning algorithms for binary classification can be made to achieve suboptimal expected error on a new independent test point drawn from the same distribution as the clean dataset. On the other hand, we show that uniform convergence-based algorithms do not degrade in their guarantees. Our results showcase how optimal learning algorithms break down in the face of seemingly helpful monotone corruptions, exposing their overreliance on exchangeability.", "link": "http://arxiv.org/abs/2601.02193v1", "date": "2026-01-05", "relevancy": 2.211, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4674}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4459}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20with%20Monotone%20Adversarial%20Corruptions&body=Title%3A%20Learning%20with%20Monotone%20Adversarial%20Corruptions%0AAuthor%3A%20Kasper%20Green%20Larsen%20and%20Chirag%20Pabbaraju%20and%20Abhishek%20Shetty%0AAbstract%3A%20We%20study%20the%20extent%20to%20which%20standard%20machine%20learning%20algorithms%20rely%20on%20exchangeability%20and%20independence%20of%20data%20by%20introducing%20a%20monotone%20adversarial%20corruption%20model.%20In%20this%20model%2C%20an%20adversary%2C%20upon%20looking%20at%20a%20%22clean%22%20i.i.d.%20dataset%2C%20inserts%20additional%20%22corrupted%22%20points%20of%20their%20choice%20into%20the%20dataset.%20These%20added%20points%20are%20constrained%20to%20be%20monotone%20corruptions%2C%20in%20that%20they%20get%20labeled%20according%20to%20the%20ground-truth%20target%20function.%20Perhaps%20surprisingly%2C%20we%20demonstrate%20that%20in%20this%20setting%2C%20all%20known%20optimal%20learning%20algorithms%20for%20binary%20classification%20can%20be%20made%20to%20achieve%20suboptimal%20expected%20error%20on%20a%20new%20independent%20test%20point%20drawn%20from%20the%20same%20distribution%20as%20the%20clean%20dataset.%20On%20the%20other%20hand%2C%20we%20show%20that%20uniform%20convergence-based%20algorithms%20do%20not%20degrade%20in%20their%20guarantees.%20Our%20results%20showcase%20how%20optimal%20learning%20algorithms%20break%20down%20in%20the%20face%20of%20seemingly%20helpful%20monotone%20corruptions%2C%20exposing%20their%20overreliance%20on%20exchangeability.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02193v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520with%2520Monotone%2520Adversarial%2520Corruptions%26entry.906535625%3DKasper%2520Green%2520Larsen%2520and%2520Chirag%2520Pabbaraju%2520and%2520Abhishek%2520Shetty%26entry.1292438233%3DWe%2520study%2520the%2520extent%2520to%2520which%2520standard%2520machine%2520learning%2520algorithms%2520rely%2520on%2520exchangeability%2520and%2520independence%2520of%2520data%2520by%2520introducing%2520a%2520monotone%2520adversarial%2520corruption%2520model.%2520In%2520this%2520model%252C%2520an%2520adversary%252C%2520upon%2520looking%2520at%2520a%2520%2522clean%2522%2520i.i.d.%2520dataset%252C%2520inserts%2520additional%2520%2522corrupted%2522%2520points%2520of%2520their%2520choice%2520into%2520the%2520dataset.%2520These%2520added%2520points%2520are%2520constrained%2520to%2520be%2520monotone%2520corruptions%252C%2520in%2520that%2520they%2520get%2520labeled%2520according%2520to%2520the%2520ground-truth%2520target%2520function.%2520Perhaps%2520surprisingly%252C%2520we%2520demonstrate%2520that%2520in%2520this%2520setting%252C%2520all%2520known%2520optimal%2520learning%2520algorithms%2520for%2520binary%2520classification%2520can%2520be%2520made%2520to%2520achieve%2520suboptimal%2520expected%2520error%2520on%2520a%2520new%2520independent%2520test%2520point%2520drawn%2520from%2520the%2520same%2520distribution%2520as%2520the%2520clean%2520dataset.%2520On%2520the%2520other%2520hand%252C%2520we%2520show%2520that%2520uniform%2520convergence-based%2520algorithms%2520do%2520not%2520degrade%2520in%2520their%2520guarantees.%2520Our%2520results%2520showcase%2520how%2520optimal%2520learning%2520algorithms%2520break%2520down%2520in%2520the%2520face%2520of%2520seemingly%2520helpful%2520monotone%2520corruptions%252C%2520exposing%2520their%2520overreliance%2520on%2520exchangeability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02193v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20with%20Monotone%20Adversarial%20Corruptions&entry.906535625=Kasper%20Green%20Larsen%20and%20Chirag%20Pabbaraju%20and%20Abhishek%20Shetty&entry.1292438233=We%20study%20the%20extent%20to%20which%20standard%20machine%20learning%20algorithms%20rely%20on%20exchangeability%20and%20independence%20of%20data%20by%20introducing%20a%20monotone%20adversarial%20corruption%20model.%20In%20this%20model%2C%20an%20adversary%2C%20upon%20looking%20at%20a%20%22clean%22%20i.i.d.%20dataset%2C%20inserts%20additional%20%22corrupted%22%20points%20of%20their%20choice%20into%20the%20dataset.%20These%20added%20points%20are%20constrained%20to%20be%20monotone%20corruptions%2C%20in%20that%20they%20get%20labeled%20according%20to%20the%20ground-truth%20target%20function.%20Perhaps%20surprisingly%2C%20we%20demonstrate%20that%20in%20this%20setting%2C%20all%20known%20optimal%20learning%20algorithms%20for%20binary%20classification%20can%20be%20made%20to%20achieve%20suboptimal%20expected%20error%20on%20a%20new%20independent%20test%20point%20drawn%20from%20the%20same%20distribution%20as%20the%20clean%20dataset.%20On%20the%20other%20hand%2C%20we%20show%20that%20uniform%20convergence-based%20algorithms%20do%20not%20degrade%20in%20their%20guarantees.%20Our%20results%20showcase%20how%20optimal%20learning%20algorithms%20break%20down%20in%20the%20face%20of%20seemingly%20helpful%20monotone%20corruptions%2C%20exposing%20their%20overreliance%20on%20exchangeability.&entry.1838667208=http%3A//arxiv.org/abs/2601.02193v1&entry.124074799=Read"},
{"title": "AI-enhanced tuning of quantum dot Hamiltonians toward Majorana modes", "author": "Mateusz Krawczyk and Jaros\u0142aw Paw\u0142owski", "abstract": "We propose a neural network-based model capable of learning the broad landscape of working regimes in quantum dot simulators, and using this knowledge to autotune these devices - based on transport measurements - toward obtaining Majorana modes in the structure. The model is trained in an unsupervised manner on synthetic data in the form of conductance maps, using a physics-informed loss that incorporates key properties of Majorana zero modes. We show that, with appropriate training, a deep vision-transformer network can efficiently memorize relation between Hamiltonian parameters and structures on conductance maps and use it to propose parameters update for a quantum dot chain that drive the system toward topological phase. Starting from a broad range of initial detunings in parameter space, a single update step is sufficient to generate nontrivial zero modes. Moreover, by enabling an iterative tuning procedure - where the system acquires updated conductance maps at each step - we demonstrate that the method can address a much larger region of the parameter space.", "link": "http://arxiv.org/abs/2601.02149v1", "date": "2026-01-05", "relevancy": 2.2005, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4498}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4399}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-enhanced%20tuning%20of%20quantum%20dot%20Hamiltonians%20toward%20Majorana%20modes&body=Title%3A%20AI-enhanced%20tuning%20of%20quantum%20dot%20Hamiltonians%20toward%20Majorana%20modes%0AAuthor%3A%20Mateusz%20Krawczyk%20and%20Jaros%C5%82aw%20Paw%C5%82owski%0AAbstract%3A%20We%20propose%20a%20neural%20network-based%20model%20capable%20of%20learning%20the%20broad%20landscape%20of%20working%20regimes%20in%20quantum%20dot%20simulators%2C%20and%20using%20this%20knowledge%20to%20autotune%20these%20devices%20-%20based%20on%20transport%20measurements%20-%20toward%20obtaining%20Majorana%20modes%20in%20the%20structure.%20The%20model%20is%20trained%20in%20an%20unsupervised%20manner%20on%20synthetic%20data%20in%20the%20form%20of%20conductance%20maps%2C%20using%20a%20physics-informed%20loss%20that%20incorporates%20key%20properties%20of%20Majorana%20zero%20modes.%20We%20show%20that%2C%20with%20appropriate%20training%2C%20a%20deep%20vision-transformer%20network%20can%20efficiently%20memorize%20relation%20between%20Hamiltonian%20parameters%20and%20structures%20on%20conductance%20maps%20and%20use%20it%20to%20propose%20parameters%20update%20for%20a%20quantum%20dot%20chain%20that%20drive%20the%20system%20toward%20topological%20phase.%20Starting%20from%20a%20broad%20range%20of%20initial%20detunings%20in%20parameter%20space%2C%20a%20single%20update%20step%20is%20sufficient%20to%20generate%20nontrivial%20zero%20modes.%20Moreover%2C%20by%20enabling%20an%20iterative%20tuning%20procedure%20-%20where%20the%20system%20acquires%20updated%20conductance%20maps%20at%20each%20step%20-%20we%20demonstrate%20that%20the%20method%20can%20address%20a%20much%20larger%20region%20of%20the%20parameter%20space.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02149v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-enhanced%2520tuning%2520of%2520quantum%2520dot%2520Hamiltonians%2520toward%2520Majorana%2520modes%26entry.906535625%3DMateusz%2520Krawczyk%2520and%2520Jaros%25C5%2582aw%2520Paw%25C5%2582owski%26entry.1292438233%3DWe%2520propose%2520a%2520neural%2520network-based%2520model%2520capable%2520of%2520learning%2520the%2520broad%2520landscape%2520of%2520working%2520regimes%2520in%2520quantum%2520dot%2520simulators%252C%2520and%2520using%2520this%2520knowledge%2520to%2520autotune%2520these%2520devices%2520-%2520based%2520on%2520transport%2520measurements%2520-%2520toward%2520obtaining%2520Majorana%2520modes%2520in%2520the%2520structure.%2520The%2520model%2520is%2520trained%2520in%2520an%2520unsupervised%2520manner%2520on%2520synthetic%2520data%2520in%2520the%2520form%2520of%2520conductance%2520maps%252C%2520using%2520a%2520physics-informed%2520loss%2520that%2520incorporates%2520key%2520properties%2520of%2520Majorana%2520zero%2520modes.%2520We%2520show%2520that%252C%2520with%2520appropriate%2520training%252C%2520a%2520deep%2520vision-transformer%2520network%2520can%2520efficiently%2520memorize%2520relation%2520between%2520Hamiltonian%2520parameters%2520and%2520structures%2520on%2520conductance%2520maps%2520and%2520use%2520it%2520to%2520propose%2520parameters%2520update%2520for%2520a%2520quantum%2520dot%2520chain%2520that%2520drive%2520the%2520system%2520toward%2520topological%2520phase.%2520Starting%2520from%2520a%2520broad%2520range%2520of%2520initial%2520detunings%2520in%2520parameter%2520space%252C%2520a%2520single%2520update%2520step%2520is%2520sufficient%2520to%2520generate%2520nontrivial%2520zero%2520modes.%2520Moreover%252C%2520by%2520enabling%2520an%2520iterative%2520tuning%2520procedure%2520-%2520where%2520the%2520system%2520acquires%2520updated%2520conductance%2520maps%2520at%2520each%2520step%2520-%2520we%2520demonstrate%2520that%2520the%2520method%2520can%2520address%2520a%2520much%2520larger%2520region%2520of%2520the%2520parameter%2520space.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02149v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-enhanced%20tuning%20of%20quantum%20dot%20Hamiltonians%20toward%20Majorana%20modes&entry.906535625=Mateusz%20Krawczyk%20and%20Jaros%C5%82aw%20Paw%C5%82owski&entry.1292438233=We%20propose%20a%20neural%20network-based%20model%20capable%20of%20learning%20the%20broad%20landscape%20of%20working%20regimes%20in%20quantum%20dot%20simulators%2C%20and%20using%20this%20knowledge%20to%20autotune%20these%20devices%20-%20based%20on%20transport%20measurements%20-%20toward%20obtaining%20Majorana%20modes%20in%20the%20structure.%20The%20model%20is%20trained%20in%20an%20unsupervised%20manner%20on%20synthetic%20data%20in%20the%20form%20of%20conductance%20maps%2C%20using%20a%20physics-informed%20loss%20that%20incorporates%20key%20properties%20of%20Majorana%20zero%20modes.%20We%20show%20that%2C%20with%20appropriate%20training%2C%20a%20deep%20vision-transformer%20network%20can%20efficiently%20memorize%20relation%20between%20Hamiltonian%20parameters%20and%20structures%20on%20conductance%20maps%20and%20use%20it%20to%20propose%20parameters%20update%20for%20a%20quantum%20dot%20chain%20that%20drive%20the%20system%20toward%20topological%20phase.%20Starting%20from%20a%20broad%20range%20of%20initial%20detunings%20in%20parameter%20space%2C%20a%20single%20update%20step%20is%20sufficient%20to%20generate%20nontrivial%20zero%20modes.%20Moreover%2C%20by%20enabling%20an%20iterative%20tuning%20procedure%20-%20where%20the%20system%20acquires%20updated%20conductance%20maps%20at%20each%20step%20-%20we%20demonstrate%20that%20the%20method%20can%20address%20a%20much%20larger%20region%20of%20the%20parameter%20space.&entry.1838667208=http%3A//arxiv.org/abs/2601.02149v1&entry.124074799=Read"},
{"title": "A Differentiable Adversarial Framework for Task-Aware Data Subsampling", "author": "Jiacheng Lyu and Bihua Bao", "abstract": "The proliferation of large-scale datasets poses a major computational challenge to model training. The traditional data subsampling method works as a static, task independent preprocessing step which usually discards information that is critical to downstream prediction. In this paper, we introduces the antagonistic soft selection subsampling (ASSS) framework as is a novel paradigm that reconstructs data reduction into a differentiable end-to-end learning problem. ASSS uses the adversarial game between selector network and task network, and selector network learning assigns continuous importance weights to samples. This direct optimization implemented by Gumbel-Softmax relaxation allows the selector to identify and retain samples with the maximum amount of information for a specific task target under the guidance of the loss function that balances the fidelity and sparsity of the prediction. Theoretical analysis links this framework with the information bottleneck principle. Comprehensive experiments on four large-scale real world datasets show that ASSS has always been better than heuristic subsampling baselines such as clustering and nearest neighbor thinning in maintaining model performance. It is worth noting that ASSS can not only match, but also sometimes exceed the training performance of the entire dataset, showcasing the effect of intelligent denoising. This work establishes task aware data subsampling as a learnable component, providing a principled solution for effective large-scale data learning.", "link": "http://arxiv.org/abs/2601.02081v1", "date": "2026-01-05", "relevancy": 2.1983, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5821}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5382}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4967}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Differentiable%20Adversarial%20Framework%20for%20Task-Aware%20Data%20Subsampling&body=Title%3A%20A%20Differentiable%20Adversarial%20Framework%20for%20Task-Aware%20Data%20Subsampling%0AAuthor%3A%20Jiacheng%20Lyu%20and%20Bihua%20Bao%0AAbstract%3A%20The%20proliferation%20of%20large-scale%20datasets%20poses%20a%20major%20computational%20challenge%20to%20model%20training.%20The%20traditional%20data%20subsampling%20method%20works%20as%20a%20static%2C%20task%20independent%20preprocessing%20step%20which%20usually%20discards%20information%20that%20is%20critical%20to%20downstream%20prediction.%20In%20this%20paper%2C%20we%20introduces%20the%20antagonistic%20soft%20selection%20subsampling%20%28ASSS%29%20framework%20as%20is%20a%20novel%20paradigm%20that%20reconstructs%20data%20reduction%20into%20a%20differentiable%20end-to-end%20learning%20problem.%20ASSS%20uses%20the%20adversarial%20game%20between%20selector%20network%20and%20task%20network%2C%20and%20selector%20network%20learning%20assigns%20continuous%20importance%20weights%20to%20samples.%20This%20direct%20optimization%20implemented%20by%20Gumbel-Softmax%20relaxation%20allows%20the%20selector%20to%20identify%20and%20retain%20samples%20with%20the%20maximum%20amount%20of%20information%20for%20a%20specific%20task%20target%20under%20the%20guidance%20of%20the%20loss%20function%20that%20balances%20the%20fidelity%20and%20sparsity%20of%20the%20prediction.%20Theoretical%20analysis%20links%20this%20framework%20with%20the%20information%20bottleneck%20principle.%20Comprehensive%20experiments%20on%20four%20large-scale%20real%20world%20datasets%20show%20that%20ASSS%20has%20always%20been%20better%20than%20heuristic%20subsampling%20baselines%20such%20as%20clustering%20and%20nearest%20neighbor%20thinning%20in%20maintaining%20model%20performance.%20It%20is%20worth%20noting%20that%20ASSS%20can%20not%20only%20match%2C%20but%20also%20sometimes%20exceed%20the%20training%20performance%20of%20the%20entire%20dataset%2C%20showcasing%20the%20effect%20of%20intelligent%20denoising.%20This%20work%20establishes%20task%20aware%20data%20subsampling%20as%20a%20learnable%20component%2C%20providing%20a%20principled%20solution%20for%20effective%20large-scale%20data%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02081v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Differentiable%2520Adversarial%2520Framework%2520for%2520Task-Aware%2520Data%2520Subsampling%26entry.906535625%3DJiacheng%2520Lyu%2520and%2520Bihua%2520Bao%26entry.1292438233%3DThe%2520proliferation%2520of%2520large-scale%2520datasets%2520poses%2520a%2520major%2520computational%2520challenge%2520to%2520model%2520training.%2520The%2520traditional%2520data%2520subsampling%2520method%2520works%2520as%2520a%2520static%252C%2520task%2520independent%2520preprocessing%2520step%2520which%2520usually%2520discards%2520information%2520that%2520is%2520critical%2520to%2520downstream%2520prediction.%2520In%2520this%2520paper%252C%2520we%2520introduces%2520the%2520antagonistic%2520soft%2520selection%2520subsampling%2520%2528ASSS%2529%2520framework%2520as%2520is%2520a%2520novel%2520paradigm%2520that%2520reconstructs%2520data%2520reduction%2520into%2520a%2520differentiable%2520end-to-end%2520learning%2520problem.%2520ASSS%2520uses%2520the%2520adversarial%2520game%2520between%2520selector%2520network%2520and%2520task%2520network%252C%2520and%2520selector%2520network%2520learning%2520assigns%2520continuous%2520importance%2520weights%2520to%2520samples.%2520This%2520direct%2520optimization%2520implemented%2520by%2520Gumbel-Softmax%2520relaxation%2520allows%2520the%2520selector%2520to%2520identify%2520and%2520retain%2520samples%2520with%2520the%2520maximum%2520amount%2520of%2520information%2520for%2520a%2520specific%2520task%2520target%2520under%2520the%2520guidance%2520of%2520the%2520loss%2520function%2520that%2520balances%2520the%2520fidelity%2520and%2520sparsity%2520of%2520the%2520prediction.%2520Theoretical%2520analysis%2520links%2520this%2520framework%2520with%2520the%2520information%2520bottleneck%2520principle.%2520Comprehensive%2520experiments%2520on%2520four%2520large-scale%2520real%2520world%2520datasets%2520show%2520that%2520ASSS%2520has%2520always%2520been%2520better%2520than%2520heuristic%2520subsampling%2520baselines%2520such%2520as%2520clustering%2520and%2520nearest%2520neighbor%2520thinning%2520in%2520maintaining%2520model%2520performance.%2520It%2520is%2520worth%2520noting%2520that%2520ASSS%2520can%2520not%2520only%2520match%252C%2520but%2520also%2520sometimes%2520exceed%2520the%2520training%2520performance%2520of%2520the%2520entire%2520dataset%252C%2520showcasing%2520the%2520effect%2520of%2520intelligent%2520denoising.%2520This%2520work%2520establishes%2520task%2520aware%2520data%2520subsampling%2520as%2520a%2520learnable%2520component%252C%2520providing%2520a%2520principled%2520solution%2520for%2520effective%2520large-scale%2520data%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02081v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Differentiable%20Adversarial%20Framework%20for%20Task-Aware%20Data%20Subsampling&entry.906535625=Jiacheng%20Lyu%20and%20Bihua%20Bao&entry.1292438233=The%20proliferation%20of%20large-scale%20datasets%20poses%20a%20major%20computational%20challenge%20to%20model%20training.%20The%20traditional%20data%20subsampling%20method%20works%20as%20a%20static%2C%20task%20independent%20preprocessing%20step%20which%20usually%20discards%20information%20that%20is%20critical%20to%20downstream%20prediction.%20In%20this%20paper%2C%20we%20introduces%20the%20antagonistic%20soft%20selection%20subsampling%20%28ASSS%29%20framework%20as%20is%20a%20novel%20paradigm%20that%20reconstructs%20data%20reduction%20into%20a%20differentiable%20end-to-end%20learning%20problem.%20ASSS%20uses%20the%20adversarial%20game%20between%20selector%20network%20and%20task%20network%2C%20and%20selector%20network%20learning%20assigns%20continuous%20importance%20weights%20to%20samples.%20This%20direct%20optimization%20implemented%20by%20Gumbel-Softmax%20relaxation%20allows%20the%20selector%20to%20identify%20and%20retain%20samples%20with%20the%20maximum%20amount%20of%20information%20for%20a%20specific%20task%20target%20under%20the%20guidance%20of%20the%20loss%20function%20that%20balances%20the%20fidelity%20and%20sparsity%20of%20the%20prediction.%20Theoretical%20analysis%20links%20this%20framework%20with%20the%20information%20bottleneck%20principle.%20Comprehensive%20experiments%20on%20four%20large-scale%20real%20world%20datasets%20show%20that%20ASSS%20has%20always%20been%20better%20than%20heuristic%20subsampling%20baselines%20such%20as%20clustering%20and%20nearest%20neighbor%20thinning%20in%20maintaining%20model%20performance.%20It%20is%20worth%20noting%20that%20ASSS%20can%20not%20only%20match%2C%20but%20also%20sometimes%20exceed%20the%20training%20performance%20of%20the%20entire%20dataset%2C%20showcasing%20the%20effect%20of%20intelligent%20denoising.%20This%20work%20establishes%20task%20aware%20data%20subsampling%20as%20a%20learnable%20component%2C%20providing%20a%20principled%20solution%20for%20effective%20large-scale%20data%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2601.02081v1&entry.124074799=Read"},
{"title": "Perish or Flourish? A Holistic Evaluation of Large Language Models for Code Generation in Functional Programming", "author": "Nguyet-Anh H. Lang and Eric Lang and Thanh Le-Cong and Bach Le and Quyet-Thang Huynh", "abstract": "Functional programming provides strong foundations for developing reliable and secure software systems, yet its adoption remains not widespread due to the steep learning curve. Recent advances in Large Language Models (LLMs) for code generation present new opportunities to lower these barriers. However, extensive evaluations of LLMs largely focus on imperative programming languages, and their capabilities in functional programming languages (FP) remain underexplored. To address this gap, we introduce FPEval, a holistic evaluation framework built on FPBench, a new benchmark of 721 programming tasks across three difficulty levels on three mainstream FP languages: Haskell, Ocaml and Scala. FPEval provides compehensive evaluation infrastructures with both test validations with comprehensive test suites and static analysis tools to assess both functional correctness and code style and maintainability. Using this framework, we evaluate state-of-the-art LLMs, including GPT-3.5, GPT-4o, and GPT-5, for code generation in functional programming languages and Java as an imperative baseline. Our results demonstrate that LLM performance in functional programming improves substantially with model advancement; however, error rates remain significantly higher in purely functional languages (Haskell and OCaml) than in hybrid (Scala) or imperative (Java) languages. Moreover, LLMs frequently generate non-idiomatic functional code that follows imperative patterns, raising concerns about code style and long-term maintainability. Finally, we show that LLMs can partially self-repair both correctness and quality issues when provided with static analysis feedback and hand-crafted instructions for common types of issues.", "link": "http://arxiv.org/abs/2601.02060v1", "date": "2026-01-05", "relevancy": 2.1971, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4399}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4392}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perish%20or%20Flourish%3F%20A%20Holistic%20Evaluation%20of%20Large%20Language%20Models%20for%20Code%20Generation%20in%20Functional%20Programming&body=Title%3A%20Perish%20or%20Flourish%3F%20A%20Holistic%20Evaluation%20of%20Large%20Language%20Models%20for%20Code%20Generation%20in%20Functional%20Programming%0AAuthor%3A%20Nguyet-Anh%20H.%20Lang%20and%20Eric%20Lang%20and%20Thanh%20Le-Cong%20and%20Bach%20Le%20and%20Quyet-Thang%20Huynh%0AAbstract%3A%20Functional%20programming%20provides%20strong%20foundations%20for%20developing%20reliable%20and%20secure%20software%20systems%2C%20yet%20its%20adoption%20remains%20not%20widespread%20due%20to%20the%20steep%20learning%20curve.%20Recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20for%20code%20generation%20present%20new%20opportunities%20to%20lower%20these%20barriers.%20However%2C%20extensive%20evaluations%20of%20LLMs%20largely%20focus%20on%20imperative%20programming%20languages%2C%20and%20their%20capabilities%20in%20functional%20programming%20languages%20%28FP%29%20remain%20underexplored.%20To%20address%20this%20gap%2C%20we%20introduce%20FPEval%2C%20a%20holistic%20evaluation%20framework%20built%20on%20FPBench%2C%20a%20new%20benchmark%20of%20721%20programming%20tasks%20across%20three%20difficulty%20levels%20on%20three%20mainstream%20FP%20languages%3A%20Haskell%2C%20Ocaml%20and%20Scala.%20FPEval%20provides%20compehensive%20evaluation%20infrastructures%20with%20both%20test%20validations%20with%20comprehensive%20test%20suites%20and%20static%20analysis%20tools%20to%20assess%20both%20functional%20correctness%20and%20code%20style%20and%20maintainability.%20Using%20this%20framework%2C%20we%20evaluate%20state-of-the-art%20LLMs%2C%20including%20GPT-3.5%2C%20GPT-4o%2C%20and%20GPT-5%2C%20for%20code%20generation%20in%20functional%20programming%20languages%20and%20Java%20as%20an%20imperative%20baseline.%20Our%20results%20demonstrate%20that%20LLM%20performance%20in%20functional%20programming%20improves%20substantially%20with%20model%20advancement%3B%20however%2C%20error%20rates%20remain%20significantly%20higher%20in%20purely%20functional%20languages%20%28Haskell%20and%20OCaml%29%20than%20in%20hybrid%20%28Scala%29%20or%20imperative%20%28Java%29%20languages.%20Moreover%2C%20LLMs%20frequently%20generate%20non-idiomatic%20functional%20code%20that%20follows%20imperative%20patterns%2C%20raising%20concerns%20about%20code%20style%20and%20long-term%20maintainability.%20Finally%2C%20we%20show%20that%20LLMs%20can%20partially%20self-repair%20both%20correctness%20and%20quality%20issues%20when%20provided%20with%20static%20analysis%20feedback%20and%20hand-crafted%20instructions%20for%20common%20types%20of%20issues.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02060v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerish%2520or%2520Flourish%253F%2520A%2520Holistic%2520Evaluation%2520of%2520Large%2520Language%2520Models%2520for%2520Code%2520Generation%2520in%2520Functional%2520Programming%26entry.906535625%3DNguyet-Anh%2520H.%2520Lang%2520and%2520Eric%2520Lang%2520and%2520Thanh%2520Le-Cong%2520and%2520Bach%2520Le%2520and%2520Quyet-Thang%2520Huynh%26entry.1292438233%3DFunctional%2520programming%2520provides%2520strong%2520foundations%2520for%2520developing%2520reliable%2520and%2520secure%2520software%2520systems%252C%2520yet%2520its%2520adoption%2520remains%2520not%2520widespread%2520due%2520to%2520the%2520steep%2520learning%2520curve.%2520Recent%2520advances%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520for%2520code%2520generation%2520present%2520new%2520opportunities%2520to%2520lower%2520these%2520barriers.%2520However%252C%2520extensive%2520evaluations%2520of%2520LLMs%2520largely%2520focus%2520on%2520imperative%2520programming%2520languages%252C%2520and%2520their%2520capabilities%2520in%2520functional%2520programming%2520languages%2520%2528FP%2529%2520remain%2520underexplored.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520FPEval%252C%2520a%2520holistic%2520evaluation%2520framework%2520built%2520on%2520FPBench%252C%2520a%2520new%2520benchmark%2520of%2520721%2520programming%2520tasks%2520across%2520three%2520difficulty%2520levels%2520on%2520three%2520mainstream%2520FP%2520languages%253A%2520Haskell%252C%2520Ocaml%2520and%2520Scala.%2520FPEval%2520provides%2520compehensive%2520evaluation%2520infrastructures%2520with%2520both%2520test%2520validations%2520with%2520comprehensive%2520test%2520suites%2520and%2520static%2520analysis%2520tools%2520to%2520assess%2520both%2520functional%2520correctness%2520and%2520code%2520style%2520and%2520maintainability.%2520Using%2520this%2520framework%252C%2520we%2520evaluate%2520state-of-the-art%2520LLMs%252C%2520including%2520GPT-3.5%252C%2520GPT-4o%252C%2520and%2520GPT-5%252C%2520for%2520code%2520generation%2520in%2520functional%2520programming%2520languages%2520and%2520Java%2520as%2520an%2520imperative%2520baseline.%2520Our%2520results%2520demonstrate%2520that%2520LLM%2520performance%2520in%2520functional%2520programming%2520improves%2520substantially%2520with%2520model%2520advancement%253B%2520however%252C%2520error%2520rates%2520remain%2520significantly%2520higher%2520in%2520purely%2520functional%2520languages%2520%2528Haskell%2520and%2520OCaml%2529%2520than%2520in%2520hybrid%2520%2528Scala%2529%2520or%2520imperative%2520%2528Java%2529%2520languages.%2520Moreover%252C%2520LLMs%2520frequently%2520generate%2520non-idiomatic%2520functional%2520code%2520that%2520follows%2520imperative%2520patterns%252C%2520raising%2520concerns%2520about%2520code%2520style%2520and%2520long-term%2520maintainability.%2520Finally%252C%2520we%2520show%2520that%2520LLMs%2520can%2520partially%2520self-repair%2520both%2520correctness%2520and%2520quality%2520issues%2520when%2520provided%2520with%2520static%2520analysis%2520feedback%2520and%2520hand-crafted%2520instructions%2520for%2520common%2520types%2520of%2520issues.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02060v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perish%20or%20Flourish%3F%20A%20Holistic%20Evaluation%20of%20Large%20Language%20Models%20for%20Code%20Generation%20in%20Functional%20Programming&entry.906535625=Nguyet-Anh%20H.%20Lang%20and%20Eric%20Lang%20and%20Thanh%20Le-Cong%20and%20Bach%20Le%20and%20Quyet-Thang%20Huynh&entry.1292438233=Functional%20programming%20provides%20strong%20foundations%20for%20developing%20reliable%20and%20secure%20software%20systems%2C%20yet%20its%20adoption%20remains%20not%20widespread%20due%20to%20the%20steep%20learning%20curve.%20Recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20for%20code%20generation%20present%20new%20opportunities%20to%20lower%20these%20barriers.%20However%2C%20extensive%20evaluations%20of%20LLMs%20largely%20focus%20on%20imperative%20programming%20languages%2C%20and%20their%20capabilities%20in%20functional%20programming%20languages%20%28FP%29%20remain%20underexplored.%20To%20address%20this%20gap%2C%20we%20introduce%20FPEval%2C%20a%20holistic%20evaluation%20framework%20built%20on%20FPBench%2C%20a%20new%20benchmark%20of%20721%20programming%20tasks%20across%20three%20difficulty%20levels%20on%20three%20mainstream%20FP%20languages%3A%20Haskell%2C%20Ocaml%20and%20Scala.%20FPEval%20provides%20compehensive%20evaluation%20infrastructures%20with%20both%20test%20validations%20with%20comprehensive%20test%20suites%20and%20static%20analysis%20tools%20to%20assess%20both%20functional%20correctness%20and%20code%20style%20and%20maintainability.%20Using%20this%20framework%2C%20we%20evaluate%20state-of-the-art%20LLMs%2C%20including%20GPT-3.5%2C%20GPT-4o%2C%20and%20GPT-5%2C%20for%20code%20generation%20in%20functional%20programming%20languages%20and%20Java%20as%20an%20imperative%20baseline.%20Our%20results%20demonstrate%20that%20LLM%20performance%20in%20functional%20programming%20improves%20substantially%20with%20model%20advancement%3B%20however%2C%20error%20rates%20remain%20significantly%20higher%20in%20purely%20functional%20languages%20%28Haskell%20and%20OCaml%29%20than%20in%20hybrid%20%28Scala%29%20or%20imperative%20%28Java%29%20languages.%20Moreover%2C%20LLMs%20frequently%20generate%20non-idiomatic%20functional%20code%20that%20follows%20imperative%20patterns%2C%20raising%20concerns%20about%20code%20style%20and%20long-term%20maintainability.%20Finally%2C%20we%20show%20that%20LLMs%20can%20partially%20self-repair%20both%20correctness%20and%20quality%20issues%20when%20provided%20with%20static%20analysis%20feedback%20and%20hand-crafted%20instructions%20for%20common%20types%20of%20issues.&entry.1838667208=http%3A//arxiv.org/abs/2601.02060v1&entry.124074799=Read"},
{"title": "Towards Any-Quality Image Segmentation via Generative and Adaptive Latent Space Enhancement", "author": "Guangqian Guo and Aixi Ren and Yong Guo and Xuehui Yu and Jiacheng Tian and Wenli Li and Yaoxing Wang and Shan Gao", "abstract": "Segment Anything Models (SAMs), known for their exceptional zero-shot segmentation performance, have garnered significant attention in the research community. Nevertheless, their performance drops significantly on severely degraded, low-quality images, limiting their effectiveness in real-world scenarios. To address this, we propose GleSAM++, which utilizes Generative Latent space Enhancement to boost robustness on low-quality images, thus enabling generalization across various image qualities. Additionally, to improve compatibility between the pre-trained diffusion model and the segmentation framework, we introduce two techniques, i.e., Feature Distribution Alignment (FDA) and Channel Replication and Expansion (CRE). However, the above components lack explicit guidance regarding the degree of degradation. The model is forced to implicitly fit a complex noise distribution that spans conditions from mild noise to severe artifacts, which substantially increases the learning burden and leads to suboptimal reconstructions. To address this issue, we further introduce a Degradation-aware Adaptive Enhancement (DAE) mechanism. The key principle of DAE is to decouple the reconstruction process for arbitrary-quality features into two stages: degradation-level prediction and degradation-aware reconstruction. Our method can be applied to pre-trained SAM and SAM2 with only minimal additional learnable parameters, allowing for efficient optimization. Extensive experiments demonstrate that GleSAM++ significantly improves segmentation robustness on complex degradations while maintaining generalization to clear images. Furthermore, GleSAM++ also performs well on unseen degradations, underscoring the versatility of our approach and dataset.", "link": "http://arxiv.org/abs/2601.02018v1", "date": "2026-01-05", "relevancy": 2.1921, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5717}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5446}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Any-Quality%20Image%20Segmentation%20via%20Generative%20and%20Adaptive%20Latent%20Space%20Enhancement&body=Title%3A%20Towards%20Any-Quality%20Image%20Segmentation%20via%20Generative%20and%20Adaptive%20Latent%20Space%20Enhancement%0AAuthor%3A%20Guangqian%20Guo%20and%20Aixi%20Ren%20and%20Yong%20Guo%20and%20Xuehui%20Yu%20and%20Jiacheng%20Tian%20and%20Wenli%20Li%20and%20Yaoxing%20Wang%20and%20Shan%20Gao%0AAbstract%3A%20Segment%20Anything%20Models%20%28SAMs%29%2C%20known%20for%20their%20exceptional%20zero-shot%20segmentation%20performance%2C%20have%20garnered%20significant%20attention%20in%20the%20research%20community.%20Nevertheless%2C%20their%20performance%20drops%20significantly%20on%20severely%20degraded%2C%20low-quality%20images%2C%20limiting%20their%20effectiveness%20in%20real-world%20scenarios.%20To%20address%20this%2C%20we%20propose%20GleSAM%2B%2B%2C%20which%20utilizes%20Generative%20Latent%20space%20Enhancement%20to%20boost%20robustness%20on%20low-quality%20images%2C%20thus%20enabling%20generalization%20across%20various%20image%20qualities.%20Additionally%2C%20to%20improve%20compatibility%20between%20the%20pre-trained%20diffusion%20model%20and%20the%20segmentation%20framework%2C%20we%20introduce%20two%20techniques%2C%20i.e.%2C%20Feature%20Distribution%20Alignment%20%28FDA%29%20and%20Channel%20Replication%20and%20Expansion%20%28CRE%29.%20However%2C%20the%20above%20components%20lack%20explicit%20guidance%20regarding%20the%20degree%20of%20degradation.%20The%20model%20is%20forced%20to%20implicitly%20fit%20a%20complex%20noise%20distribution%20that%20spans%20conditions%20from%20mild%20noise%20to%20severe%20artifacts%2C%20which%20substantially%20increases%20the%20learning%20burden%20and%20leads%20to%20suboptimal%20reconstructions.%20To%20address%20this%20issue%2C%20we%20further%20introduce%20a%20Degradation-aware%20Adaptive%20Enhancement%20%28DAE%29%20mechanism.%20The%20key%20principle%20of%20DAE%20is%20to%20decouple%20the%20reconstruction%20process%20for%20arbitrary-quality%20features%20into%20two%20stages%3A%20degradation-level%20prediction%20and%20degradation-aware%20reconstruction.%20Our%20method%20can%20be%20applied%20to%20pre-trained%20SAM%20and%20SAM2%20with%20only%20minimal%20additional%20learnable%20parameters%2C%20allowing%20for%20efficient%20optimization.%20Extensive%20experiments%20demonstrate%20that%20GleSAM%2B%2B%20significantly%20improves%20segmentation%20robustness%20on%20complex%20degradations%20while%20maintaining%20generalization%20to%20clear%20images.%20Furthermore%2C%20GleSAM%2B%2B%20also%20performs%20well%20on%20unseen%20degradations%2C%20underscoring%20the%20versatility%20of%20our%20approach%20and%20dataset.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02018v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Any-Quality%2520Image%2520Segmentation%2520via%2520Generative%2520and%2520Adaptive%2520Latent%2520Space%2520Enhancement%26entry.906535625%3DGuangqian%2520Guo%2520and%2520Aixi%2520Ren%2520and%2520Yong%2520Guo%2520and%2520Xuehui%2520Yu%2520and%2520Jiacheng%2520Tian%2520and%2520Wenli%2520Li%2520and%2520Yaoxing%2520Wang%2520and%2520Shan%2520Gao%26entry.1292438233%3DSegment%2520Anything%2520Models%2520%2528SAMs%2529%252C%2520known%2520for%2520their%2520exceptional%2520zero-shot%2520segmentation%2520performance%252C%2520have%2520garnered%2520significant%2520attention%2520in%2520the%2520research%2520community.%2520Nevertheless%252C%2520their%2520performance%2520drops%2520significantly%2520on%2520severely%2520degraded%252C%2520low-quality%2520images%252C%2520limiting%2520their%2520effectiveness%2520in%2520real-world%2520scenarios.%2520To%2520address%2520this%252C%2520we%2520propose%2520GleSAM%252B%252B%252C%2520which%2520utilizes%2520Generative%2520Latent%2520space%2520Enhancement%2520to%2520boost%2520robustness%2520on%2520low-quality%2520images%252C%2520thus%2520enabling%2520generalization%2520across%2520various%2520image%2520qualities.%2520Additionally%252C%2520to%2520improve%2520compatibility%2520between%2520the%2520pre-trained%2520diffusion%2520model%2520and%2520the%2520segmentation%2520framework%252C%2520we%2520introduce%2520two%2520techniques%252C%2520i.e.%252C%2520Feature%2520Distribution%2520Alignment%2520%2528FDA%2529%2520and%2520Channel%2520Replication%2520and%2520Expansion%2520%2528CRE%2529.%2520However%252C%2520the%2520above%2520components%2520lack%2520explicit%2520guidance%2520regarding%2520the%2520degree%2520of%2520degradation.%2520The%2520model%2520is%2520forced%2520to%2520implicitly%2520fit%2520a%2520complex%2520noise%2520distribution%2520that%2520spans%2520conditions%2520from%2520mild%2520noise%2520to%2520severe%2520artifacts%252C%2520which%2520substantially%2520increases%2520the%2520learning%2520burden%2520and%2520leads%2520to%2520suboptimal%2520reconstructions.%2520To%2520address%2520this%2520issue%252C%2520we%2520further%2520introduce%2520a%2520Degradation-aware%2520Adaptive%2520Enhancement%2520%2528DAE%2529%2520mechanism.%2520The%2520key%2520principle%2520of%2520DAE%2520is%2520to%2520decouple%2520the%2520reconstruction%2520process%2520for%2520arbitrary-quality%2520features%2520into%2520two%2520stages%253A%2520degradation-level%2520prediction%2520and%2520degradation-aware%2520reconstruction.%2520Our%2520method%2520can%2520be%2520applied%2520to%2520pre-trained%2520SAM%2520and%2520SAM2%2520with%2520only%2520minimal%2520additional%2520learnable%2520parameters%252C%2520allowing%2520for%2520efficient%2520optimization.%2520Extensive%2520experiments%2520demonstrate%2520that%2520GleSAM%252B%252B%2520significantly%2520improves%2520segmentation%2520robustness%2520on%2520complex%2520degradations%2520while%2520maintaining%2520generalization%2520to%2520clear%2520images.%2520Furthermore%252C%2520GleSAM%252B%252B%2520also%2520performs%2520well%2520on%2520unseen%2520degradations%252C%2520underscoring%2520the%2520versatility%2520of%2520our%2520approach%2520and%2520dataset.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02018v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Any-Quality%20Image%20Segmentation%20via%20Generative%20and%20Adaptive%20Latent%20Space%20Enhancement&entry.906535625=Guangqian%20Guo%20and%20Aixi%20Ren%20and%20Yong%20Guo%20and%20Xuehui%20Yu%20and%20Jiacheng%20Tian%20and%20Wenli%20Li%20and%20Yaoxing%20Wang%20and%20Shan%20Gao&entry.1292438233=Segment%20Anything%20Models%20%28SAMs%29%2C%20known%20for%20their%20exceptional%20zero-shot%20segmentation%20performance%2C%20have%20garnered%20significant%20attention%20in%20the%20research%20community.%20Nevertheless%2C%20their%20performance%20drops%20significantly%20on%20severely%20degraded%2C%20low-quality%20images%2C%20limiting%20their%20effectiveness%20in%20real-world%20scenarios.%20To%20address%20this%2C%20we%20propose%20GleSAM%2B%2B%2C%20which%20utilizes%20Generative%20Latent%20space%20Enhancement%20to%20boost%20robustness%20on%20low-quality%20images%2C%20thus%20enabling%20generalization%20across%20various%20image%20qualities.%20Additionally%2C%20to%20improve%20compatibility%20between%20the%20pre-trained%20diffusion%20model%20and%20the%20segmentation%20framework%2C%20we%20introduce%20two%20techniques%2C%20i.e.%2C%20Feature%20Distribution%20Alignment%20%28FDA%29%20and%20Channel%20Replication%20and%20Expansion%20%28CRE%29.%20However%2C%20the%20above%20components%20lack%20explicit%20guidance%20regarding%20the%20degree%20of%20degradation.%20The%20model%20is%20forced%20to%20implicitly%20fit%20a%20complex%20noise%20distribution%20that%20spans%20conditions%20from%20mild%20noise%20to%20severe%20artifacts%2C%20which%20substantially%20increases%20the%20learning%20burden%20and%20leads%20to%20suboptimal%20reconstructions.%20To%20address%20this%20issue%2C%20we%20further%20introduce%20a%20Degradation-aware%20Adaptive%20Enhancement%20%28DAE%29%20mechanism.%20The%20key%20principle%20of%20DAE%20is%20to%20decouple%20the%20reconstruction%20process%20for%20arbitrary-quality%20features%20into%20two%20stages%3A%20degradation-level%20prediction%20and%20degradation-aware%20reconstruction.%20Our%20method%20can%20be%20applied%20to%20pre-trained%20SAM%20and%20SAM2%20with%20only%20minimal%20additional%20learnable%20parameters%2C%20allowing%20for%20efficient%20optimization.%20Extensive%20experiments%20demonstrate%20that%20GleSAM%2B%2B%20significantly%20improves%20segmentation%20robustness%20on%20complex%20degradations%20while%20maintaining%20generalization%20to%20clear%20images.%20Furthermore%2C%20GleSAM%2B%2B%20also%20performs%20well%20on%20unseen%20degradations%2C%20underscoring%20the%20versatility%20of%20our%20approach%20and%20dataset.&entry.1838667208=http%3A//arxiv.org/abs/2601.02018v1&entry.124074799=Read"},
{"title": "Efficient Unrolled Networks for Large-Scale 3D Inverse Problems", "author": "Romain Vo and Juli\u00e1n Tachella", "abstract": "Deep learning-based methods have revolutionized the field of imaging inverse problems, yielding state-of-the-art performance across various imaging domains. The best performing networks incorporate the imaging operator within the network architecture, typically in the form of deep unrolling. However, in large-scale problems, such as 3D imaging, most existing methods fail to incorporate the operator in the architecture due to the prohibitive amount of memory required by global forward operators, which hinder typical patching strategies. In this work, we present a domain partitioning strategy and normal operator approximations that enable the training of end-to-end reconstruction models incorporating forward operators of arbitrarily large problems into their architecture. The proposed method achieves state-of-the-art performance on 3D X-ray cone-beam tomography and 3D multi-coil accelerated MRI, while requiring only a single GPU for both training and inference.", "link": "http://arxiv.org/abs/2601.02141v1", "date": "2026-01-05", "relevancy": 2.1913, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5592}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5424}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Unrolled%20Networks%20for%20Large-Scale%203D%20Inverse%20Problems&body=Title%3A%20Efficient%20Unrolled%20Networks%20for%20Large-Scale%203D%20Inverse%20Problems%0AAuthor%3A%20Romain%20Vo%20and%20Juli%C3%A1n%20Tachella%0AAbstract%3A%20Deep%20learning-based%20methods%20have%20revolutionized%20the%20field%20of%20imaging%20inverse%20problems%2C%20yielding%20state-of-the-art%20performance%20across%20various%20imaging%20domains.%20The%20best%20performing%20networks%20incorporate%20the%20imaging%20operator%20within%20the%20network%20architecture%2C%20typically%20in%20the%20form%20of%20deep%20unrolling.%20However%2C%20in%20large-scale%20problems%2C%20such%20as%203D%20imaging%2C%20most%20existing%20methods%20fail%20to%20incorporate%20the%20operator%20in%20the%20architecture%20due%20to%20the%20prohibitive%20amount%20of%20memory%20required%20by%20global%20forward%20operators%2C%20which%20hinder%20typical%20patching%20strategies.%20In%20this%20work%2C%20we%20present%20a%20domain%20partitioning%20strategy%20and%20normal%20operator%20approximations%20that%20enable%20the%20training%20of%20end-to-end%20reconstruction%20models%20incorporating%20forward%20operators%20of%20arbitrarily%20large%20problems%20into%20their%20architecture.%20The%20proposed%20method%20achieves%20state-of-the-art%20performance%20on%203D%20X-ray%20cone-beam%20tomography%20and%203D%20multi-coil%20accelerated%20MRI%2C%20while%20requiring%20only%20a%20single%20GPU%20for%20both%20training%20and%20inference.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02141v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Unrolled%2520Networks%2520for%2520Large-Scale%25203D%2520Inverse%2520Problems%26entry.906535625%3DRomain%2520Vo%2520and%2520Juli%25C3%25A1n%2520Tachella%26entry.1292438233%3DDeep%2520learning-based%2520methods%2520have%2520revolutionized%2520the%2520field%2520of%2520imaging%2520inverse%2520problems%252C%2520yielding%2520state-of-the-art%2520performance%2520across%2520various%2520imaging%2520domains.%2520The%2520best%2520performing%2520networks%2520incorporate%2520the%2520imaging%2520operator%2520within%2520the%2520network%2520architecture%252C%2520typically%2520in%2520the%2520form%2520of%2520deep%2520unrolling.%2520However%252C%2520in%2520large-scale%2520problems%252C%2520such%2520as%25203D%2520imaging%252C%2520most%2520existing%2520methods%2520fail%2520to%2520incorporate%2520the%2520operator%2520in%2520the%2520architecture%2520due%2520to%2520the%2520prohibitive%2520amount%2520of%2520memory%2520required%2520by%2520global%2520forward%2520operators%252C%2520which%2520hinder%2520typical%2520patching%2520strategies.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520domain%2520partitioning%2520strategy%2520and%2520normal%2520operator%2520approximations%2520that%2520enable%2520the%2520training%2520of%2520end-to-end%2520reconstruction%2520models%2520incorporating%2520forward%2520operators%2520of%2520arbitrarily%2520large%2520problems%2520into%2520their%2520architecture.%2520The%2520proposed%2520method%2520achieves%2520state-of-the-art%2520performance%2520on%25203D%2520X-ray%2520cone-beam%2520tomography%2520and%25203D%2520multi-coil%2520accelerated%2520MRI%252C%2520while%2520requiring%2520only%2520a%2520single%2520GPU%2520for%2520both%2520training%2520and%2520inference.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02141v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Unrolled%20Networks%20for%20Large-Scale%203D%20Inverse%20Problems&entry.906535625=Romain%20Vo%20and%20Juli%C3%A1n%20Tachella&entry.1292438233=Deep%20learning-based%20methods%20have%20revolutionized%20the%20field%20of%20imaging%20inverse%20problems%2C%20yielding%20state-of-the-art%20performance%20across%20various%20imaging%20domains.%20The%20best%20performing%20networks%20incorporate%20the%20imaging%20operator%20within%20the%20network%20architecture%2C%20typically%20in%20the%20form%20of%20deep%20unrolling.%20However%2C%20in%20large-scale%20problems%2C%20such%20as%203D%20imaging%2C%20most%20existing%20methods%20fail%20to%20incorporate%20the%20operator%20in%20the%20architecture%20due%20to%20the%20prohibitive%20amount%20of%20memory%20required%20by%20global%20forward%20operators%2C%20which%20hinder%20typical%20patching%20strategies.%20In%20this%20work%2C%20we%20present%20a%20domain%20partitioning%20strategy%20and%20normal%20operator%20approximations%20that%20enable%20the%20training%20of%20end-to-end%20reconstruction%20models%20incorporating%20forward%20operators%20of%20arbitrarily%20large%20problems%20into%20their%20architecture.%20The%20proposed%20method%20achieves%20state-of-the-art%20performance%20on%203D%20X-ray%20cone-beam%20tomography%20and%203D%20multi-coil%20accelerated%20MRI%2C%20while%20requiring%20only%20a%20single%20GPU%20for%20both%20training%20and%20inference.&entry.1838667208=http%3A//arxiv.org/abs/2601.02141v1&entry.124074799=Read"},
{"title": "From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition", "author": "Yiqing Zhou and Yu Lei and Shuzheng Si and Qingyan Sun and Wei Wang and Yifei Wu and Hao Wen and Gang Chen and Fanchao Qi and Maosong Sun", "abstract": "Managing extensive context remains a critical bottleneck for Large Language Models (LLMs), particularly in applications like long-document question answering and autonomous agents where lengthy inputs incur high computational costs and introduce noise. Existing compression techniques often disrupt local coherence through discrete token removal or rely on implicit latent encoding that suffers from positional bias and incompatibility with closed-source APIs. To address these limitations, we introduce the EDU-based Context Compressor, a novel explicit compression framework designed to preserve both global structure and fine-grained details. Our approach reformulates context compression as a structure-then-select process. First, our LingoEDU transforms linear text into a structural relation tree of Elementary Discourse Units (EDUs) which are anchored strictly to source indices to eliminate hallucination. Second, a lightweight ranking module selects query-relevant sub-trees for linearization. To rigorously evaluate structural understanding, we release StructBench, a manually annotated dataset of 248 diverse documents. Empirical results demonstrate that our method achieves state-of-the-art structural prediction accuracy and significantly outperforms frontier LLMs while reducing costs. Furthermore, our structure-aware compression substantially enhances performance across downstream tasks ranging from long-context tasks to complex Deep Search scenarios.", "link": "http://arxiv.org/abs/2512.14244v4", "date": "2026-01-05", "relevancy": 2.1828, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5496}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5496}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Context%20to%20EDUs%3A%20Faithful%20and%20Structured%20Context%20Compression%20via%20Elementary%20Discourse%20Unit%20Decomposition&body=Title%3A%20From%20Context%20to%20EDUs%3A%20Faithful%20and%20Structured%20Context%20Compression%20via%20Elementary%20Discourse%20Unit%20Decomposition%0AAuthor%3A%20Yiqing%20Zhou%20and%20Yu%20Lei%20and%20Shuzheng%20Si%20and%20Qingyan%20Sun%20and%20Wei%20Wang%20and%20Yifei%20Wu%20and%20Hao%20Wen%20and%20Gang%20Chen%20and%20Fanchao%20Qi%20and%20Maosong%20Sun%0AAbstract%3A%20Managing%20extensive%20context%20remains%20a%20critical%20bottleneck%20for%20Large%20Language%20Models%20%28LLMs%29%2C%20particularly%20in%20applications%20like%20long-document%20question%20answering%20and%20autonomous%20agents%20where%20lengthy%20inputs%20incur%20high%20computational%20costs%20and%20introduce%20noise.%20Existing%20compression%20techniques%20often%20disrupt%20local%20coherence%20through%20discrete%20token%20removal%20or%20rely%20on%20implicit%20latent%20encoding%20that%20suffers%20from%20positional%20bias%20and%20incompatibility%20with%20closed-source%20APIs.%20To%20address%20these%20limitations%2C%20we%20introduce%20the%20EDU-based%20Context%20Compressor%2C%20a%20novel%20explicit%20compression%20framework%20designed%20to%20preserve%20both%20global%20structure%20and%20fine-grained%20details.%20Our%20approach%20reformulates%20context%20compression%20as%20a%20structure-then-select%20process.%20First%2C%20our%20LingoEDU%20transforms%20linear%20text%20into%20a%20structural%20relation%20tree%20of%20Elementary%20Discourse%20Units%20%28EDUs%29%20which%20are%20anchored%20strictly%20to%20source%20indices%20to%20eliminate%20hallucination.%20Second%2C%20a%20lightweight%20ranking%20module%20selects%20query-relevant%20sub-trees%20for%20linearization.%20To%20rigorously%20evaluate%20structural%20understanding%2C%20we%20release%20StructBench%2C%20a%20manually%20annotated%20dataset%20of%20248%20diverse%20documents.%20Empirical%20results%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20structural%20prediction%20accuracy%20and%20significantly%20outperforms%20frontier%20LLMs%20while%20reducing%20costs.%20Furthermore%2C%20our%20structure-aware%20compression%20substantially%20enhances%20performance%20across%20downstream%20tasks%20ranging%20from%20long-context%20tasks%20to%20complex%20Deep%20Search%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14244v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Context%2520to%2520EDUs%253A%2520Faithful%2520and%2520Structured%2520Context%2520Compression%2520via%2520Elementary%2520Discourse%2520Unit%2520Decomposition%26entry.906535625%3DYiqing%2520Zhou%2520and%2520Yu%2520Lei%2520and%2520Shuzheng%2520Si%2520and%2520Qingyan%2520Sun%2520and%2520Wei%2520Wang%2520and%2520Yifei%2520Wu%2520and%2520Hao%2520Wen%2520and%2520Gang%2520Chen%2520and%2520Fanchao%2520Qi%2520and%2520Maosong%2520Sun%26entry.1292438233%3DManaging%2520extensive%2520context%2520remains%2520a%2520critical%2520bottleneck%2520for%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520particularly%2520in%2520applications%2520like%2520long-document%2520question%2520answering%2520and%2520autonomous%2520agents%2520where%2520lengthy%2520inputs%2520incur%2520high%2520computational%2520costs%2520and%2520introduce%2520noise.%2520Existing%2520compression%2520techniques%2520often%2520disrupt%2520local%2520coherence%2520through%2520discrete%2520token%2520removal%2520or%2520rely%2520on%2520implicit%2520latent%2520encoding%2520that%2520suffers%2520from%2520positional%2520bias%2520and%2520incompatibility%2520with%2520closed-source%2520APIs.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520the%2520EDU-based%2520Context%2520Compressor%252C%2520a%2520novel%2520explicit%2520compression%2520framework%2520designed%2520to%2520preserve%2520both%2520global%2520structure%2520and%2520fine-grained%2520details.%2520Our%2520approach%2520reformulates%2520context%2520compression%2520as%2520a%2520structure-then-select%2520process.%2520First%252C%2520our%2520LingoEDU%2520transforms%2520linear%2520text%2520into%2520a%2520structural%2520relation%2520tree%2520of%2520Elementary%2520Discourse%2520Units%2520%2528EDUs%2529%2520which%2520are%2520anchored%2520strictly%2520to%2520source%2520indices%2520to%2520eliminate%2520hallucination.%2520Second%252C%2520a%2520lightweight%2520ranking%2520module%2520selects%2520query-relevant%2520sub-trees%2520for%2520linearization.%2520To%2520rigorously%2520evaluate%2520structural%2520understanding%252C%2520we%2520release%2520StructBench%252C%2520a%2520manually%2520annotated%2520dataset%2520of%2520248%2520diverse%2520documents.%2520Empirical%2520results%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520structural%2520prediction%2520accuracy%2520and%2520significantly%2520outperforms%2520frontier%2520LLMs%2520while%2520reducing%2520costs.%2520Furthermore%252C%2520our%2520structure-aware%2520compression%2520substantially%2520enhances%2520performance%2520across%2520downstream%2520tasks%2520ranging%2520from%2520long-context%2520tasks%2520to%2520complex%2520Deep%2520Search%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14244v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Context%20to%20EDUs%3A%20Faithful%20and%20Structured%20Context%20Compression%20via%20Elementary%20Discourse%20Unit%20Decomposition&entry.906535625=Yiqing%20Zhou%20and%20Yu%20Lei%20and%20Shuzheng%20Si%20and%20Qingyan%20Sun%20and%20Wei%20Wang%20and%20Yifei%20Wu%20and%20Hao%20Wen%20and%20Gang%20Chen%20and%20Fanchao%20Qi%20and%20Maosong%20Sun&entry.1292438233=Managing%20extensive%20context%20remains%20a%20critical%20bottleneck%20for%20Large%20Language%20Models%20%28LLMs%29%2C%20particularly%20in%20applications%20like%20long-document%20question%20answering%20and%20autonomous%20agents%20where%20lengthy%20inputs%20incur%20high%20computational%20costs%20and%20introduce%20noise.%20Existing%20compression%20techniques%20often%20disrupt%20local%20coherence%20through%20discrete%20token%20removal%20or%20rely%20on%20implicit%20latent%20encoding%20that%20suffers%20from%20positional%20bias%20and%20incompatibility%20with%20closed-source%20APIs.%20To%20address%20these%20limitations%2C%20we%20introduce%20the%20EDU-based%20Context%20Compressor%2C%20a%20novel%20explicit%20compression%20framework%20designed%20to%20preserve%20both%20global%20structure%20and%20fine-grained%20details.%20Our%20approach%20reformulates%20context%20compression%20as%20a%20structure-then-select%20process.%20First%2C%20our%20LingoEDU%20transforms%20linear%20text%20into%20a%20structural%20relation%20tree%20of%20Elementary%20Discourse%20Units%20%28EDUs%29%20which%20are%20anchored%20strictly%20to%20source%20indices%20to%20eliminate%20hallucination.%20Second%2C%20a%20lightweight%20ranking%20module%20selects%20query-relevant%20sub-trees%20for%20linearization.%20To%20rigorously%20evaluate%20structural%20understanding%2C%20we%20release%20StructBench%2C%20a%20manually%20annotated%20dataset%20of%20248%20diverse%20documents.%20Empirical%20results%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20structural%20prediction%20accuracy%20and%20significantly%20outperforms%20frontier%20LLMs%20while%20reducing%20costs.%20Furthermore%2C%20our%20structure-aware%20compression%20substantially%20enhances%20performance%20across%20downstream%20tasks%20ranging%20from%20long-context%20tasks%20to%20complex%20Deep%20Search%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2512.14244v4&entry.124074799=Read"},
{"title": "Answering from Sure to Uncertain: Uncertainty-Aware Curriculum Learning for Video Question Answering", "author": "Haopeng Li and Mohammed Bennamoun and Jun Liu and Hossein Rahmani and Qiuhong Ke", "abstract": "While significant advancements have been made in video question answering (VideoQA), the potential benefits of enhancing model generalization through tailored difficulty scheduling have been largely overlooked in existing research. This paper seeks to bridge that gap by incorporating VideoQA into a curriculum learning (CL) framework that progressively trains models from simpler to more complex data. Recognizing that conventional self-paced CL methods rely on training loss for difficulty measurement, which might not accurately reflect the intricacies of video-question pairs, we introduce the concept of uncertainty-aware CL. Here, uncertainty serves as the guiding principle for dynamically adjusting the difficulty. Furthermore, we address the challenge posed by uncertainty by presenting a probabilistic modeling approach for VideoQA. Specifically, we conceptualize VideoQA as a stochastic computation graph, where the hidden representations are treated as stochastic variables. This yields two distinct types of uncertainty: one related to the inherent uncertainty in the data and another pertaining to the model's confidence. In practice, we seamlessly integrate the VideoQA model into our framework and conduct comprehensive experiments. The findings affirm that our approach not only achieves enhanced performance but also effectively quantifies uncertainty in the context of VideoQA.", "link": "http://arxiv.org/abs/2401.01510v2", "date": "2026-01-05", "relevancy": 2.1755, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5663}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.563}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Answering%20from%20Sure%20to%20Uncertain%3A%20Uncertainty-Aware%20Curriculum%20Learning%20for%20Video%20Question%20Answering&body=Title%3A%20Answering%20from%20Sure%20to%20Uncertain%3A%20Uncertainty-Aware%20Curriculum%20Learning%20for%20Video%20Question%20Answering%0AAuthor%3A%20Haopeng%20Li%20and%20Mohammed%20Bennamoun%20and%20Jun%20Liu%20and%20Hossein%20Rahmani%20and%20Qiuhong%20Ke%0AAbstract%3A%20While%20significant%20advancements%20have%20been%20made%20in%20video%20question%20answering%20%28VideoQA%29%2C%20the%20potential%20benefits%20of%20enhancing%20model%20generalization%20through%20tailored%20difficulty%20scheduling%20have%20been%20largely%20overlooked%20in%20existing%20research.%20This%20paper%20seeks%20to%20bridge%20that%20gap%20by%20incorporating%20VideoQA%20into%20a%20curriculum%20learning%20%28CL%29%20framework%20that%20progressively%20trains%20models%20from%20simpler%20to%20more%20complex%20data.%20Recognizing%20that%20conventional%20self-paced%20CL%20methods%20rely%20on%20training%20loss%20for%20difficulty%20measurement%2C%20which%20might%20not%20accurately%20reflect%20the%20intricacies%20of%20video-question%20pairs%2C%20we%20introduce%20the%20concept%20of%20uncertainty-aware%20CL.%20Here%2C%20uncertainty%20serves%20as%20the%20guiding%20principle%20for%20dynamically%20adjusting%20the%20difficulty.%20Furthermore%2C%20we%20address%20the%20challenge%20posed%20by%20uncertainty%20by%20presenting%20a%20probabilistic%20modeling%20approach%20for%20VideoQA.%20Specifically%2C%20we%20conceptualize%20VideoQA%20as%20a%20stochastic%20computation%20graph%2C%20where%20the%20hidden%20representations%20are%20treated%20as%20stochastic%20variables.%20This%20yields%20two%20distinct%20types%20of%20uncertainty%3A%20one%20related%20to%20the%20inherent%20uncertainty%20in%20the%20data%20and%20another%20pertaining%20to%20the%20model%27s%20confidence.%20In%20practice%2C%20we%20seamlessly%20integrate%20the%20VideoQA%20model%20into%20our%20framework%20and%20conduct%20comprehensive%20experiments.%20The%20findings%20affirm%20that%20our%20approach%20not%20only%20achieves%20enhanced%20performance%20but%20also%20effectively%20quantifies%20uncertainty%20in%20the%20context%20of%20VideoQA.%0ALink%3A%20http%3A//arxiv.org/abs/2401.01510v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnswering%2520from%2520Sure%2520to%2520Uncertain%253A%2520Uncertainty-Aware%2520Curriculum%2520Learning%2520for%2520Video%2520Question%2520Answering%26entry.906535625%3DHaopeng%2520Li%2520and%2520Mohammed%2520Bennamoun%2520and%2520Jun%2520Liu%2520and%2520Hossein%2520Rahmani%2520and%2520Qiuhong%2520Ke%26entry.1292438233%3DWhile%2520significant%2520advancements%2520have%2520been%2520made%2520in%2520video%2520question%2520answering%2520%2528VideoQA%2529%252C%2520the%2520potential%2520benefits%2520of%2520enhancing%2520model%2520generalization%2520through%2520tailored%2520difficulty%2520scheduling%2520have%2520been%2520largely%2520overlooked%2520in%2520existing%2520research.%2520This%2520paper%2520seeks%2520to%2520bridge%2520that%2520gap%2520by%2520incorporating%2520VideoQA%2520into%2520a%2520curriculum%2520learning%2520%2528CL%2529%2520framework%2520that%2520progressively%2520trains%2520models%2520from%2520simpler%2520to%2520more%2520complex%2520data.%2520Recognizing%2520that%2520conventional%2520self-paced%2520CL%2520methods%2520rely%2520on%2520training%2520loss%2520for%2520difficulty%2520measurement%252C%2520which%2520might%2520not%2520accurately%2520reflect%2520the%2520intricacies%2520of%2520video-question%2520pairs%252C%2520we%2520introduce%2520the%2520concept%2520of%2520uncertainty-aware%2520CL.%2520Here%252C%2520uncertainty%2520serves%2520as%2520the%2520guiding%2520principle%2520for%2520dynamically%2520adjusting%2520the%2520difficulty.%2520Furthermore%252C%2520we%2520address%2520the%2520challenge%2520posed%2520by%2520uncertainty%2520by%2520presenting%2520a%2520probabilistic%2520modeling%2520approach%2520for%2520VideoQA.%2520Specifically%252C%2520we%2520conceptualize%2520VideoQA%2520as%2520a%2520stochastic%2520computation%2520graph%252C%2520where%2520the%2520hidden%2520representations%2520are%2520treated%2520as%2520stochastic%2520variables.%2520This%2520yields%2520two%2520distinct%2520types%2520of%2520uncertainty%253A%2520one%2520related%2520to%2520the%2520inherent%2520uncertainty%2520in%2520the%2520data%2520and%2520another%2520pertaining%2520to%2520the%2520model%2527s%2520confidence.%2520In%2520practice%252C%2520we%2520seamlessly%2520integrate%2520the%2520VideoQA%2520model%2520into%2520our%2520framework%2520and%2520conduct%2520comprehensive%2520experiments.%2520The%2520findings%2520affirm%2520that%2520our%2520approach%2520not%2520only%2520achieves%2520enhanced%2520performance%2520but%2520also%2520effectively%2520quantifies%2520uncertainty%2520in%2520the%2520context%2520of%2520VideoQA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.01510v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Answering%20from%20Sure%20to%20Uncertain%3A%20Uncertainty-Aware%20Curriculum%20Learning%20for%20Video%20Question%20Answering&entry.906535625=Haopeng%20Li%20and%20Mohammed%20Bennamoun%20and%20Jun%20Liu%20and%20Hossein%20Rahmani%20and%20Qiuhong%20Ke&entry.1292438233=While%20significant%20advancements%20have%20been%20made%20in%20video%20question%20answering%20%28VideoQA%29%2C%20the%20potential%20benefits%20of%20enhancing%20model%20generalization%20through%20tailored%20difficulty%20scheduling%20have%20been%20largely%20overlooked%20in%20existing%20research.%20This%20paper%20seeks%20to%20bridge%20that%20gap%20by%20incorporating%20VideoQA%20into%20a%20curriculum%20learning%20%28CL%29%20framework%20that%20progressively%20trains%20models%20from%20simpler%20to%20more%20complex%20data.%20Recognizing%20that%20conventional%20self-paced%20CL%20methods%20rely%20on%20training%20loss%20for%20difficulty%20measurement%2C%20which%20might%20not%20accurately%20reflect%20the%20intricacies%20of%20video-question%20pairs%2C%20we%20introduce%20the%20concept%20of%20uncertainty-aware%20CL.%20Here%2C%20uncertainty%20serves%20as%20the%20guiding%20principle%20for%20dynamically%20adjusting%20the%20difficulty.%20Furthermore%2C%20we%20address%20the%20challenge%20posed%20by%20uncertainty%20by%20presenting%20a%20probabilistic%20modeling%20approach%20for%20VideoQA.%20Specifically%2C%20we%20conceptualize%20VideoQA%20as%20a%20stochastic%20computation%20graph%2C%20where%20the%20hidden%20representations%20are%20treated%20as%20stochastic%20variables.%20This%20yields%20two%20distinct%20types%20of%20uncertainty%3A%20one%20related%20to%20the%20inherent%20uncertainty%20in%20the%20data%20and%20another%20pertaining%20to%20the%20model%27s%20confidence.%20In%20practice%2C%20we%20seamlessly%20integrate%20the%20VideoQA%20model%20into%20our%20framework%20and%20conduct%20comprehensive%20experiments.%20The%20findings%20affirm%20that%20our%20approach%20not%20only%20achieves%20enhanced%20performance%20but%20also%20effectively%20quantifies%20uncertainty%20in%20the%20context%20of%20VideoQA.&entry.1838667208=http%3A//arxiv.org/abs/2401.01510v2&entry.124074799=Read"},
{"title": "Language as a Wave Phenomenon: Iso-Energetic Phase-Locking and Semantic Interference in Neural Networks", "author": "Alper Y\u0131ld\u0131r\u0131m and \u0130brahim Y\u00fcceda\u011f", "abstract": "Conventional deep learning paradigms rely on metabolically expensive magnitude-based representations, rendering them fundamentally incompatible with passive photonic hardware. We introduce PRISM, a sequence modeling architecture that bridges high-level reasoning and physical constraints by enforcing an Iso-Energetic (Unity Gain) principle, compelling the network to encode semantic information exclusively in the phase angle. Validated on the WMT14 translation benchmark, PRISM achieves a 0.799 COMET score, demonstrating that phase-based reasoning competes with standard Transformers (0.821) and functionally matches unconstrained spectral baselines like FNet (0.805), despite enforcing strict energy constraints and requiring 11.5% fewer parameters. Furthermore, to verify hardware feasibility, we simulate a Holographic Backpropagation mechanism on a noisy, 4-bit optical correlator. Ablation studies reveal a substantial performance gain (48.4% vs. 62.4%) over a frozen baseline, proving that the proposed phase-steering mechanism actively optimizes physical parameters under strict energy constraints. These results establish an existence proof that ultra-low-power, passive optical hardware can support high-level linguistic intelligence without sacrificing representational capacity.", "link": "http://arxiv.org/abs/2512.01208v2", "date": "2026-01-05", "relevancy": 2.1734, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5448}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5448}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20as%20a%20Wave%20Phenomenon%3A%20Iso-Energetic%20Phase-Locking%20and%20Semantic%20Interference%20in%20Neural%20Networks&body=Title%3A%20Language%20as%20a%20Wave%20Phenomenon%3A%20Iso-Energetic%20Phase-Locking%20and%20Semantic%20Interference%20in%20Neural%20Networks%0AAuthor%3A%20Alper%20Y%C4%B1ld%C4%B1r%C4%B1m%20and%20%C4%B0brahim%20Y%C3%BCceda%C4%9F%0AAbstract%3A%20Conventional%20deep%20learning%20paradigms%20rely%20on%20metabolically%20expensive%20magnitude-based%20representations%2C%20rendering%20them%20fundamentally%20incompatible%20with%20passive%20photonic%20hardware.%20We%20introduce%20PRISM%2C%20a%20sequence%20modeling%20architecture%20that%20bridges%20high-level%20reasoning%20and%20physical%20constraints%20by%20enforcing%20an%20Iso-Energetic%20%28Unity%20Gain%29%20principle%2C%20compelling%20the%20network%20to%20encode%20semantic%20information%20exclusively%20in%20the%20phase%20angle.%20Validated%20on%20the%20WMT14%20translation%20benchmark%2C%20PRISM%20achieves%20a%200.799%20COMET%20score%2C%20demonstrating%20that%20phase-based%20reasoning%20competes%20with%20standard%20Transformers%20%280.821%29%20and%20functionally%20matches%20unconstrained%20spectral%20baselines%20like%20FNet%20%280.805%29%2C%20despite%20enforcing%20strict%20energy%20constraints%20and%20requiring%2011.5%25%20fewer%20parameters.%20Furthermore%2C%20to%20verify%20hardware%20feasibility%2C%20we%20simulate%20a%20Holographic%20Backpropagation%20mechanism%20on%20a%20noisy%2C%204-bit%20optical%20correlator.%20Ablation%20studies%20reveal%20a%20substantial%20performance%20gain%20%2848.4%25%20vs.%2062.4%25%29%20over%20a%20frozen%20baseline%2C%20proving%20that%20the%20proposed%20phase-steering%20mechanism%20actively%20optimizes%20physical%20parameters%20under%20strict%20energy%20constraints.%20These%20results%20establish%20an%20existence%20proof%20that%20ultra-low-power%2C%20passive%20optical%20hardware%20can%20support%20high-level%20linguistic%20intelligence%20without%20sacrificing%20representational%20capacity.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01208v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520as%2520a%2520Wave%2520Phenomenon%253A%2520Iso-Energetic%2520Phase-Locking%2520and%2520Semantic%2520Interference%2520in%2520Neural%2520Networks%26entry.906535625%3DAlper%2520Y%25C4%25B1ld%25C4%25B1r%25C4%25B1m%2520and%2520%25C4%25B0brahim%2520Y%25C3%25BCceda%25C4%259F%26entry.1292438233%3DConventional%2520deep%2520learning%2520paradigms%2520rely%2520on%2520metabolically%2520expensive%2520magnitude-based%2520representations%252C%2520rendering%2520them%2520fundamentally%2520incompatible%2520with%2520passive%2520photonic%2520hardware.%2520We%2520introduce%2520PRISM%252C%2520a%2520sequence%2520modeling%2520architecture%2520that%2520bridges%2520high-level%2520reasoning%2520and%2520physical%2520constraints%2520by%2520enforcing%2520an%2520Iso-Energetic%2520%2528Unity%2520Gain%2529%2520principle%252C%2520compelling%2520the%2520network%2520to%2520encode%2520semantic%2520information%2520exclusively%2520in%2520the%2520phase%2520angle.%2520Validated%2520on%2520the%2520WMT14%2520translation%2520benchmark%252C%2520PRISM%2520achieves%2520a%25200.799%2520COMET%2520score%252C%2520demonstrating%2520that%2520phase-based%2520reasoning%2520competes%2520with%2520standard%2520Transformers%2520%25280.821%2529%2520and%2520functionally%2520matches%2520unconstrained%2520spectral%2520baselines%2520like%2520FNet%2520%25280.805%2529%252C%2520despite%2520enforcing%2520strict%2520energy%2520constraints%2520and%2520requiring%252011.5%2525%2520fewer%2520parameters.%2520Furthermore%252C%2520to%2520verify%2520hardware%2520feasibility%252C%2520we%2520simulate%2520a%2520Holographic%2520Backpropagation%2520mechanism%2520on%2520a%2520noisy%252C%25204-bit%2520optical%2520correlator.%2520Ablation%2520studies%2520reveal%2520a%2520substantial%2520performance%2520gain%2520%252848.4%2525%2520vs.%252062.4%2525%2529%2520over%2520a%2520frozen%2520baseline%252C%2520proving%2520that%2520the%2520proposed%2520phase-steering%2520mechanism%2520actively%2520optimizes%2520physical%2520parameters%2520under%2520strict%2520energy%2520constraints.%2520These%2520results%2520establish%2520an%2520existence%2520proof%2520that%2520ultra-low-power%252C%2520passive%2520optical%2520hardware%2520can%2520support%2520high-level%2520linguistic%2520intelligence%2520without%2520sacrificing%2520representational%2520capacity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01208v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20as%20a%20Wave%20Phenomenon%3A%20Iso-Energetic%20Phase-Locking%20and%20Semantic%20Interference%20in%20Neural%20Networks&entry.906535625=Alper%20Y%C4%B1ld%C4%B1r%C4%B1m%20and%20%C4%B0brahim%20Y%C3%BCceda%C4%9F&entry.1292438233=Conventional%20deep%20learning%20paradigms%20rely%20on%20metabolically%20expensive%20magnitude-based%20representations%2C%20rendering%20them%20fundamentally%20incompatible%20with%20passive%20photonic%20hardware.%20We%20introduce%20PRISM%2C%20a%20sequence%20modeling%20architecture%20that%20bridges%20high-level%20reasoning%20and%20physical%20constraints%20by%20enforcing%20an%20Iso-Energetic%20%28Unity%20Gain%29%20principle%2C%20compelling%20the%20network%20to%20encode%20semantic%20information%20exclusively%20in%20the%20phase%20angle.%20Validated%20on%20the%20WMT14%20translation%20benchmark%2C%20PRISM%20achieves%20a%200.799%20COMET%20score%2C%20demonstrating%20that%20phase-based%20reasoning%20competes%20with%20standard%20Transformers%20%280.821%29%20and%20functionally%20matches%20unconstrained%20spectral%20baselines%20like%20FNet%20%280.805%29%2C%20despite%20enforcing%20strict%20energy%20constraints%20and%20requiring%2011.5%25%20fewer%20parameters.%20Furthermore%2C%20to%20verify%20hardware%20feasibility%2C%20we%20simulate%20a%20Holographic%20Backpropagation%20mechanism%20on%20a%20noisy%2C%204-bit%20optical%20correlator.%20Ablation%20studies%20reveal%20a%20substantial%20performance%20gain%20%2848.4%25%20vs.%2062.4%25%29%20over%20a%20frozen%20baseline%2C%20proving%20that%20the%20proposed%20phase-steering%20mechanism%20actively%20optimizes%20physical%20parameters%20under%20strict%20energy%20constraints.%20These%20results%20establish%20an%20existence%20proof%20that%20ultra-low-power%2C%20passive%20optical%20hardware%20can%20support%20high-level%20linguistic%20intelligence%20without%20sacrificing%20representational%20capacity.&entry.1838667208=http%3A//arxiv.org/abs/2512.01208v2&entry.124074799=Read"},
{"title": "Learning Evolving Latent Strategies for Multi-Agent Language Systems without Model Fine-Tuning", "author": "Wenlong Tang", "abstract": "This study proposes a multi-agent language framework that enables continual strategy evolution without fine-tuning the language model's parameters. The core idea is to liberate the latent vectors of abstract concepts from traditional static semantic representations, allowing them to be continuously updated through environmental interaction and reinforcement feedback. We construct a dual-loop architecture: the behavior loop adjusts action preferences based on environmental rewards, while the language loop updates the external latent vectors by reflecting on the semantic embeddings of generated text.\n  Together, these mechanisms allow agents to develop stable and disentangled strategic styles over long-horizon multi-round interactions. Experiments show that agents' latent spaces exhibit clear convergence trajectories under reflection-driven updates, along with structured shifts at critical moments. Moreover, the system demonstrates an emergent ability to implicitly infer and continually adapt to emotional agents, even without shared rewards. These results indicate that, without modifying model parameters, an external latent space can provide language agents with a low-cost, scalable, and interpretable form of abstract strategic representation.", "link": "http://arxiv.org/abs/2512.20629v3", "date": "2026-01-05", "relevancy": 2.1686, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5439}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5416}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5406}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Evolving%20Latent%20Strategies%20for%20Multi-Agent%20Language%20Systems%20without%20Model%20Fine-Tuning&body=Title%3A%20Learning%20Evolving%20Latent%20Strategies%20for%20Multi-Agent%20Language%20Systems%20without%20Model%20Fine-Tuning%0AAuthor%3A%20Wenlong%20Tang%0AAbstract%3A%20This%20study%20proposes%20a%20multi-agent%20language%20framework%20that%20enables%20continual%20strategy%20evolution%20without%20fine-tuning%20the%20language%20model%27s%20parameters.%20The%20core%20idea%20is%20to%20liberate%20the%20latent%20vectors%20of%20abstract%20concepts%20from%20traditional%20static%20semantic%20representations%2C%20allowing%20them%20to%20be%20continuously%20updated%20through%20environmental%20interaction%20and%20reinforcement%20feedback.%20We%20construct%20a%20dual-loop%20architecture%3A%20the%20behavior%20loop%20adjusts%20action%20preferences%20based%20on%20environmental%20rewards%2C%20while%20the%20language%20loop%20updates%20the%20external%20latent%20vectors%20by%20reflecting%20on%20the%20semantic%20embeddings%20of%20generated%20text.%0A%20%20Together%2C%20these%20mechanisms%20allow%20agents%20to%20develop%20stable%20and%20disentangled%20strategic%20styles%20over%20long-horizon%20multi-round%20interactions.%20Experiments%20show%20that%20agents%27%20latent%20spaces%20exhibit%20clear%20convergence%20trajectories%20under%20reflection-driven%20updates%2C%20along%20with%20structured%20shifts%20at%20critical%20moments.%20Moreover%2C%20the%20system%20demonstrates%20an%20emergent%20ability%20to%20implicitly%20infer%20and%20continually%20adapt%20to%20emotional%20agents%2C%20even%20without%20shared%20rewards.%20These%20results%20indicate%20that%2C%20without%20modifying%20model%20parameters%2C%20an%20external%20latent%20space%20can%20provide%20language%20agents%20with%20a%20low-cost%2C%20scalable%2C%20and%20interpretable%20form%20of%20abstract%20strategic%20representation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20629v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Evolving%2520Latent%2520Strategies%2520for%2520Multi-Agent%2520Language%2520Systems%2520without%2520Model%2520Fine-Tuning%26entry.906535625%3DWenlong%2520Tang%26entry.1292438233%3DThis%2520study%2520proposes%2520a%2520multi-agent%2520language%2520framework%2520that%2520enables%2520continual%2520strategy%2520evolution%2520without%2520fine-tuning%2520the%2520language%2520model%2527s%2520parameters.%2520The%2520core%2520idea%2520is%2520to%2520liberate%2520the%2520latent%2520vectors%2520of%2520abstract%2520concepts%2520from%2520traditional%2520static%2520semantic%2520representations%252C%2520allowing%2520them%2520to%2520be%2520continuously%2520updated%2520through%2520environmental%2520interaction%2520and%2520reinforcement%2520feedback.%2520We%2520construct%2520a%2520dual-loop%2520architecture%253A%2520the%2520behavior%2520loop%2520adjusts%2520action%2520preferences%2520based%2520on%2520environmental%2520rewards%252C%2520while%2520the%2520language%2520loop%2520updates%2520the%2520external%2520latent%2520vectors%2520by%2520reflecting%2520on%2520the%2520semantic%2520embeddings%2520of%2520generated%2520text.%250A%2520%2520Together%252C%2520these%2520mechanisms%2520allow%2520agents%2520to%2520develop%2520stable%2520and%2520disentangled%2520strategic%2520styles%2520over%2520long-horizon%2520multi-round%2520interactions.%2520Experiments%2520show%2520that%2520agents%2527%2520latent%2520spaces%2520exhibit%2520clear%2520convergence%2520trajectories%2520under%2520reflection-driven%2520updates%252C%2520along%2520with%2520structured%2520shifts%2520at%2520critical%2520moments.%2520Moreover%252C%2520the%2520system%2520demonstrates%2520an%2520emergent%2520ability%2520to%2520implicitly%2520infer%2520and%2520continually%2520adapt%2520to%2520emotional%2520agents%252C%2520even%2520without%2520shared%2520rewards.%2520These%2520results%2520indicate%2520that%252C%2520without%2520modifying%2520model%2520parameters%252C%2520an%2520external%2520latent%2520space%2520can%2520provide%2520language%2520agents%2520with%2520a%2520low-cost%252C%2520scalable%252C%2520and%2520interpretable%2520form%2520of%2520abstract%2520strategic%2520representation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20629v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Evolving%20Latent%20Strategies%20for%20Multi-Agent%20Language%20Systems%20without%20Model%20Fine-Tuning&entry.906535625=Wenlong%20Tang&entry.1292438233=This%20study%20proposes%20a%20multi-agent%20language%20framework%20that%20enables%20continual%20strategy%20evolution%20without%20fine-tuning%20the%20language%20model%27s%20parameters.%20The%20core%20idea%20is%20to%20liberate%20the%20latent%20vectors%20of%20abstract%20concepts%20from%20traditional%20static%20semantic%20representations%2C%20allowing%20them%20to%20be%20continuously%20updated%20through%20environmental%20interaction%20and%20reinforcement%20feedback.%20We%20construct%20a%20dual-loop%20architecture%3A%20the%20behavior%20loop%20adjusts%20action%20preferences%20based%20on%20environmental%20rewards%2C%20while%20the%20language%20loop%20updates%20the%20external%20latent%20vectors%20by%20reflecting%20on%20the%20semantic%20embeddings%20of%20generated%20text.%0A%20%20Together%2C%20these%20mechanisms%20allow%20agents%20to%20develop%20stable%20and%20disentangled%20strategic%20styles%20over%20long-horizon%20multi-round%20interactions.%20Experiments%20show%20that%20agents%27%20latent%20spaces%20exhibit%20clear%20convergence%20trajectories%20under%20reflection-driven%20updates%2C%20along%20with%20structured%20shifts%20at%20critical%20moments.%20Moreover%2C%20the%20system%20demonstrates%20an%20emergent%20ability%20to%20implicitly%20infer%20and%20continually%20adapt%20to%20emotional%20agents%2C%20even%20without%20shared%20rewards.%20These%20results%20indicate%20that%2C%20without%20modifying%20model%20parameters%2C%20an%20external%20latent%20space%20can%20provide%20language%20agents%20with%20a%20low-cost%2C%20scalable%2C%20and%20interpretable%20form%20of%20abstract%20strategic%20representation.&entry.1838667208=http%3A//arxiv.org/abs/2512.20629v3&entry.124074799=Read"},
{"title": "Foundation models on the bridge: Semantic hazard detection and safety maneuvers for maritime autonomy with vision-language models", "author": "Kim Alexander Christensen and Andreas Gudahl Tufte and Alexey Gusev and Rohan Sinha and Milan Ganai and Ole Andreas Alsos and Marco Pavone and Martin Steinert", "abstract": "The draft IMO MASS Code requires autonomous and remotely supervised maritime vessels to detect departures from their operational design domain, enter a predefined fallback that notifies the operator, permit immediate human override, and avoid changing the voyage plan without approval. Meeting these obligations in the alert-to-takeover gap calls for a short-horizon, human-overridable fallback maneuver. Classical maritime autonomy stacks struggle when the correct action depends on meaning (e.g., diver-down flag means people in the water, fire close by means hazard). We argue (i) that vision-language models (VLMs) provide semantic awareness for such out-of-distribution situations, and (ii) that a fast-slow anomaly pipeline with a short-horizon, human-overridable fallback maneuver makes this practical in the handover window. We introduce Semantic Lookout, a camera-only, candidate-constrained VLM fallback maneuver selector that selects one cautious action (or station-keeping) from water-valid, world-anchored trajectories under continuous human authority. On 40 harbor scenes we measure per-call scene understanding and latency, alignment with human consensus (model majority-of-three voting), short-horizon risk-relief on fire hazard scenes, and an on-water alert->fallback maneuver->operator handover. Sub-10 s models retain most of the awareness of slower state-of-the-art models. The fallback maneuver selector outperforms geometry-only baselines and increases standoff distance on fire scenes. A field run verifies end-to-end operation. These results support VLMs as semantic fallback maneuver selectors compatible with the draft IMO MASS Code, within practical latency budgets, and motivate future work on domain-adapted, hybrid autonomy that pairs foundation-model semantics with multi-sensor bird's-eye-view perception and short-horizon replanning. Website: kimachristensen.github.io/bridge_policy", "link": "http://arxiv.org/abs/2512.24470v2", "date": "2026-01-05", "relevancy": 2.1684, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5882}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5329}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foundation%20models%20on%20the%20bridge%3A%20Semantic%20hazard%20detection%20and%20safety%20maneuvers%20for%20maritime%20autonomy%20with%20vision-language%20models&body=Title%3A%20Foundation%20models%20on%20the%20bridge%3A%20Semantic%20hazard%20detection%20and%20safety%20maneuvers%20for%20maritime%20autonomy%20with%20vision-language%20models%0AAuthor%3A%20Kim%20Alexander%20Christensen%20and%20Andreas%20Gudahl%20Tufte%20and%20Alexey%20Gusev%20and%20Rohan%20Sinha%20and%20Milan%20Ganai%20and%20Ole%20Andreas%20Alsos%20and%20Marco%20Pavone%20and%20Martin%20Steinert%0AAbstract%3A%20The%20draft%20IMO%20MASS%20Code%20requires%20autonomous%20and%20remotely%20supervised%20maritime%20vessels%20to%20detect%20departures%20from%20their%20operational%20design%20domain%2C%20enter%20a%20predefined%20fallback%20that%20notifies%20the%20operator%2C%20permit%20immediate%20human%20override%2C%20and%20avoid%20changing%20the%20voyage%20plan%20without%20approval.%20Meeting%20these%20obligations%20in%20the%20alert-to-takeover%20gap%20calls%20for%20a%20short-horizon%2C%20human-overridable%20fallback%20maneuver.%20Classical%20maritime%20autonomy%20stacks%20struggle%20when%20the%20correct%20action%20depends%20on%20meaning%20%28e.g.%2C%20diver-down%20flag%20means%20people%20in%20the%20water%2C%20fire%20close%20by%20means%20hazard%29.%20We%20argue%20%28i%29%20that%20vision-language%20models%20%28VLMs%29%20provide%20semantic%20awareness%20for%20such%20out-of-distribution%20situations%2C%20and%20%28ii%29%20that%20a%20fast-slow%20anomaly%20pipeline%20with%20a%20short-horizon%2C%20human-overridable%20fallback%20maneuver%20makes%20this%20practical%20in%20the%20handover%20window.%20We%20introduce%20Semantic%20Lookout%2C%20a%20camera-only%2C%20candidate-constrained%20VLM%20fallback%20maneuver%20selector%20that%20selects%20one%20cautious%20action%20%28or%20station-keeping%29%20from%20water-valid%2C%20world-anchored%20trajectories%20under%20continuous%20human%20authority.%20On%2040%20harbor%20scenes%20we%20measure%20per-call%20scene%20understanding%20and%20latency%2C%20alignment%20with%20human%20consensus%20%28model%20majority-of-three%20voting%29%2C%20short-horizon%20risk-relief%20on%20fire%20hazard%20scenes%2C%20and%20an%20on-water%20alert-%3Efallback%20maneuver-%3Eoperator%20handover.%20Sub-10%20s%20models%20retain%20most%20of%20the%20awareness%20of%20slower%20state-of-the-art%20models.%20The%20fallback%20maneuver%20selector%20outperforms%20geometry-only%20baselines%20and%20increases%20standoff%20distance%20on%20fire%20scenes.%20A%20field%20run%20verifies%20end-to-end%20operation.%20These%20results%20support%20VLMs%20as%20semantic%20fallback%20maneuver%20selectors%20compatible%20with%20the%20draft%20IMO%20MASS%20Code%2C%20within%20practical%20latency%20budgets%2C%20and%20motivate%20future%20work%20on%20domain-adapted%2C%20hybrid%20autonomy%20that%20pairs%20foundation-model%20semantics%20with%20multi-sensor%20bird%27s-eye-view%20perception%20and%20short-horizon%20replanning.%20Website%3A%20kimachristensen.github.io/bridge_policy%0ALink%3A%20http%3A//arxiv.org/abs/2512.24470v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundation%2520models%2520on%2520the%2520bridge%253A%2520Semantic%2520hazard%2520detection%2520and%2520safety%2520maneuvers%2520for%2520maritime%2520autonomy%2520with%2520vision-language%2520models%26entry.906535625%3DKim%2520Alexander%2520Christensen%2520and%2520Andreas%2520Gudahl%2520Tufte%2520and%2520Alexey%2520Gusev%2520and%2520Rohan%2520Sinha%2520and%2520Milan%2520Ganai%2520and%2520Ole%2520Andreas%2520Alsos%2520and%2520Marco%2520Pavone%2520and%2520Martin%2520Steinert%26entry.1292438233%3DThe%2520draft%2520IMO%2520MASS%2520Code%2520requires%2520autonomous%2520and%2520remotely%2520supervised%2520maritime%2520vessels%2520to%2520detect%2520departures%2520from%2520their%2520operational%2520design%2520domain%252C%2520enter%2520a%2520predefined%2520fallback%2520that%2520notifies%2520the%2520operator%252C%2520permit%2520immediate%2520human%2520override%252C%2520and%2520avoid%2520changing%2520the%2520voyage%2520plan%2520without%2520approval.%2520Meeting%2520these%2520obligations%2520in%2520the%2520alert-to-takeover%2520gap%2520calls%2520for%2520a%2520short-horizon%252C%2520human-overridable%2520fallback%2520maneuver.%2520Classical%2520maritime%2520autonomy%2520stacks%2520struggle%2520when%2520the%2520correct%2520action%2520depends%2520on%2520meaning%2520%2528e.g.%252C%2520diver-down%2520flag%2520means%2520people%2520in%2520the%2520water%252C%2520fire%2520close%2520by%2520means%2520hazard%2529.%2520We%2520argue%2520%2528i%2529%2520that%2520vision-language%2520models%2520%2528VLMs%2529%2520provide%2520semantic%2520awareness%2520for%2520such%2520out-of-distribution%2520situations%252C%2520and%2520%2528ii%2529%2520that%2520a%2520fast-slow%2520anomaly%2520pipeline%2520with%2520a%2520short-horizon%252C%2520human-overridable%2520fallback%2520maneuver%2520makes%2520this%2520practical%2520in%2520the%2520handover%2520window.%2520We%2520introduce%2520Semantic%2520Lookout%252C%2520a%2520camera-only%252C%2520candidate-constrained%2520VLM%2520fallback%2520maneuver%2520selector%2520that%2520selects%2520one%2520cautious%2520action%2520%2528or%2520station-keeping%2529%2520from%2520water-valid%252C%2520world-anchored%2520trajectories%2520under%2520continuous%2520human%2520authority.%2520On%252040%2520harbor%2520scenes%2520we%2520measure%2520per-call%2520scene%2520understanding%2520and%2520latency%252C%2520alignment%2520with%2520human%2520consensus%2520%2528model%2520majority-of-three%2520voting%2529%252C%2520short-horizon%2520risk-relief%2520on%2520fire%2520hazard%2520scenes%252C%2520and%2520an%2520on-water%2520alert-%253Efallback%2520maneuver-%253Eoperator%2520handover.%2520Sub-10%2520s%2520models%2520retain%2520most%2520of%2520the%2520awareness%2520of%2520slower%2520state-of-the-art%2520models.%2520The%2520fallback%2520maneuver%2520selector%2520outperforms%2520geometry-only%2520baselines%2520and%2520increases%2520standoff%2520distance%2520on%2520fire%2520scenes.%2520A%2520field%2520run%2520verifies%2520end-to-end%2520operation.%2520These%2520results%2520support%2520VLMs%2520as%2520semantic%2520fallback%2520maneuver%2520selectors%2520compatible%2520with%2520the%2520draft%2520IMO%2520MASS%2520Code%252C%2520within%2520practical%2520latency%2520budgets%252C%2520and%2520motivate%2520future%2520work%2520on%2520domain-adapted%252C%2520hybrid%2520autonomy%2520that%2520pairs%2520foundation-model%2520semantics%2520with%2520multi-sensor%2520bird%2527s-eye-view%2520perception%2520and%2520short-horizon%2520replanning.%2520Website%253A%2520kimachristensen.github.io/bridge_policy%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.24470v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundation%20models%20on%20the%20bridge%3A%20Semantic%20hazard%20detection%20and%20safety%20maneuvers%20for%20maritime%20autonomy%20with%20vision-language%20models&entry.906535625=Kim%20Alexander%20Christensen%20and%20Andreas%20Gudahl%20Tufte%20and%20Alexey%20Gusev%20and%20Rohan%20Sinha%20and%20Milan%20Ganai%20and%20Ole%20Andreas%20Alsos%20and%20Marco%20Pavone%20and%20Martin%20Steinert&entry.1292438233=The%20draft%20IMO%20MASS%20Code%20requires%20autonomous%20and%20remotely%20supervised%20maritime%20vessels%20to%20detect%20departures%20from%20their%20operational%20design%20domain%2C%20enter%20a%20predefined%20fallback%20that%20notifies%20the%20operator%2C%20permit%20immediate%20human%20override%2C%20and%20avoid%20changing%20the%20voyage%20plan%20without%20approval.%20Meeting%20these%20obligations%20in%20the%20alert-to-takeover%20gap%20calls%20for%20a%20short-horizon%2C%20human-overridable%20fallback%20maneuver.%20Classical%20maritime%20autonomy%20stacks%20struggle%20when%20the%20correct%20action%20depends%20on%20meaning%20%28e.g.%2C%20diver-down%20flag%20means%20people%20in%20the%20water%2C%20fire%20close%20by%20means%20hazard%29.%20We%20argue%20%28i%29%20that%20vision-language%20models%20%28VLMs%29%20provide%20semantic%20awareness%20for%20such%20out-of-distribution%20situations%2C%20and%20%28ii%29%20that%20a%20fast-slow%20anomaly%20pipeline%20with%20a%20short-horizon%2C%20human-overridable%20fallback%20maneuver%20makes%20this%20practical%20in%20the%20handover%20window.%20We%20introduce%20Semantic%20Lookout%2C%20a%20camera-only%2C%20candidate-constrained%20VLM%20fallback%20maneuver%20selector%20that%20selects%20one%20cautious%20action%20%28or%20station-keeping%29%20from%20water-valid%2C%20world-anchored%20trajectories%20under%20continuous%20human%20authority.%20On%2040%20harbor%20scenes%20we%20measure%20per-call%20scene%20understanding%20and%20latency%2C%20alignment%20with%20human%20consensus%20%28model%20majority-of-three%20voting%29%2C%20short-horizon%20risk-relief%20on%20fire%20hazard%20scenes%2C%20and%20an%20on-water%20alert-%3Efallback%20maneuver-%3Eoperator%20handover.%20Sub-10%20s%20models%20retain%20most%20of%20the%20awareness%20of%20slower%20state-of-the-art%20models.%20The%20fallback%20maneuver%20selector%20outperforms%20geometry-only%20baselines%20and%20increases%20standoff%20distance%20on%20fire%20scenes.%20A%20field%20run%20verifies%20end-to-end%20operation.%20These%20results%20support%20VLMs%20as%20semantic%20fallback%20maneuver%20selectors%20compatible%20with%20the%20draft%20IMO%20MASS%20Code%2C%20within%20practical%20latency%20budgets%2C%20and%20motivate%20future%20work%20on%20domain-adapted%2C%20hybrid%20autonomy%20that%20pairs%20foundation-model%20semantics%20with%20multi-sensor%20bird%27s-eye-view%20perception%20and%20short-horizon%20replanning.%20Website%3A%20kimachristensen.github.io/bridge_policy&entry.1838667208=http%3A//arxiv.org/abs/2512.24470v2&entry.124074799=Read"},
{"title": "LIMOncello: Iterated Error-State Kalman Filter on the SGal(3) Manifold for Fast LiDAR-Inertial Odometry", "author": "Carlos P\u00e9rez-Ruiz and Joan Sol\u00e0", "abstract": "This work introduces LIMOncello, a tightly coupled LiDAR-Inertial Odometry system that models 6-DoF motion on the $\\mathrm{SGal}(3)$ manifold within an iterated error-state Kalman filter backend. Compared to state representations defined on $\\mathrm{SO}(3)\\times\\mathbb{R}^6$, the use of $\\mathrm{SGal}(3)$ provides a coherent and numerically stable discrete-time propagation model that helps limit drift in low-observability conditions.\n  LIMOncello also includes a lightweight incremental i-Octree mapping backend that enables faster updates and substantially lower memory usage than incremental kd-tree style map structures, without relying on locality-restricted search heuristics. Experiments on multiple real-world datasets show that LIMOncello achieves competitive accuracy while improving robustness in geometrically sparse environments. The system maintains real-time performance with stable memory growth and is released as an extensible open-source implementation at https://github.com/CPerezRuiz335/LIMOncello.", "link": "http://arxiv.org/abs/2512.19567v2", "date": "2026-01-05", "relevancy": 2.1547, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5567}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5306}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LIMOncello%3A%20Iterated%20Error-State%20Kalman%20Filter%20on%20the%20SGal%283%29%20Manifold%20for%20Fast%20LiDAR-Inertial%20Odometry&body=Title%3A%20LIMOncello%3A%20Iterated%20Error-State%20Kalman%20Filter%20on%20the%20SGal%283%29%20Manifold%20for%20Fast%20LiDAR-Inertial%20Odometry%0AAuthor%3A%20Carlos%20P%C3%A9rez-Ruiz%20and%20Joan%20Sol%C3%A0%0AAbstract%3A%20This%20work%20introduces%20LIMOncello%2C%20a%20tightly%20coupled%20LiDAR-Inertial%20Odometry%20system%20that%20models%206-DoF%20motion%20on%20the%20%24%5Cmathrm%7BSGal%7D%283%29%24%20manifold%20within%20an%20iterated%20error-state%20Kalman%20filter%20backend.%20Compared%20to%20state%20representations%20defined%20on%20%24%5Cmathrm%7BSO%7D%283%29%5Ctimes%5Cmathbb%7BR%7D%5E6%24%2C%20the%20use%20of%20%24%5Cmathrm%7BSGal%7D%283%29%24%20provides%20a%20coherent%20and%20numerically%20stable%20discrete-time%20propagation%20model%20that%20helps%20limit%20drift%20in%20low-observability%20conditions.%0A%20%20LIMOncello%20also%20includes%20a%20lightweight%20incremental%20i-Octree%20mapping%20backend%20that%20enables%20faster%20updates%20and%20substantially%20lower%20memory%20usage%20than%20incremental%20kd-tree%20style%20map%20structures%2C%20without%20relying%20on%20locality-restricted%20search%20heuristics.%20Experiments%20on%20multiple%20real-world%20datasets%20show%20that%20LIMOncello%20achieves%20competitive%20accuracy%20while%20improving%20robustness%20in%20geometrically%20sparse%20environments.%20The%20system%20maintains%20real-time%20performance%20with%20stable%20memory%20growth%20and%20is%20released%20as%20an%20extensible%20open-source%20implementation%20at%20https%3A//github.com/CPerezRuiz335/LIMOncello.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19567v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLIMOncello%253A%2520Iterated%2520Error-State%2520Kalman%2520Filter%2520on%2520the%2520SGal%25283%2529%2520Manifold%2520for%2520Fast%2520LiDAR-Inertial%2520Odometry%26entry.906535625%3DCarlos%2520P%25C3%25A9rez-Ruiz%2520and%2520Joan%2520Sol%25C3%25A0%26entry.1292438233%3DThis%2520work%2520introduces%2520LIMOncello%252C%2520a%2520tightly%2520coupled%2520LiDAR-Inertial%2520Odometry%2520system%2520that%2520models%25206-DoF%2520motion%2520on%2520the%2520%2524%255Cmathrm%257BSGal%257D%25283%2529%2524%2520manifold%2520within%2520an%2520iterated%2520error-state%2520Kalman%2520filter%2520backend.%2520Compared%2520to%2520state%2520representations%2520defined%2520on%2520%2524%255Cmathrm%257BSO%257D%25283%2529%255Ctimes%255Cmathbb%257BR%257D%255E6%2524%252C%2520the%2520use%2520of%2520%2524%255Cmathrm%257BSGal%257D%25283%2529%2524%2520provides%2520a%2520coherent%2520and%2520numerically%2520stable%2520discrete-time%2520propagation%2520model%2520that%2520helps%2520limit%2520drift%2520in%2520low-observability%2520conditions.%250A%2520%2520LIMOncello%2520also%2520includes%2520a%2520lightweight%2520incremental%2520i-Octree%2520mapping%2520backend%2520that%2520enables%2520faster%2520updates%2520and%2520substantially%2520lower%2520memory%2520usage%2520than%2520incremental%2520kd-tree%2520style%2520map%2520structures%252C%2520without%2520relying%2520on%2520locality-restricted%2520search%2520heuristics.%2520Experiments%2520on%2520multiple%2520real-world%2520datasets%2520show%2520that%2520LIMOncello%2520achieves%2520competitive%2520accuracy%2520while%2520improving%2520robustness%2520in%2520geometrically%2520sparse%2520environments.%2520The%2520system%2520maintains%2520real-time%2520performance%2520with%2520stable%2520memory%2520growth%2520and%2520is%2520released%2520as%2520an%2520extensible%2520open-source%2520implementation%2520at%2520https%253A//github.com/CPerezRuiz335/LIMOncello.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19567v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LIMOncello%3A%20Iterated%20Error-State%20Kalman%20Filter%20on%20the%20SGal%283%29%20Manifold%20for%20Fast%20LiDAR-Inertial%20Odometry&entry.906535625=Carlos%20P%C3%A9rez-Ruiz%20and%20Joan%20Sol%C3%A0&entry.1292438233=This%20work%20introduces%20LIMOncello%2C%20a%20tightly%20coupled%20LiDAR-Inertial%20Odometry%20system%20that%20models%206-DoF%20motion%20on%20the%20%24%5Cmathrm%7BSGal%7D%283%29%24%20manifold%20within%20an%20iterated%20error-state%20Kalman%20filter%20backend.%20Compared%20to%20state%20representations%20defined%20on%20%24%5Cmathrm%7BSO%7D%283%29%5Ctimes%5Cmathbb%7BR%7D%5E6%24%2C%20the%20use%20of%20%24%5Cmathrm%7BSGal%7D%283%29%24%20provides%20a%20coherent%20and%20numerically%20stable%20discrete-time%20propagation%20model%20that%20helps%20limit%20drift%20in%20low-observability%20conditions.%0A%20%20LIMOncello%20also%20includes%20a%20lightweight%20incremental%20i-Octree%20mapping%20backend%20that%20enables%20faster%20updates%20and%20substantially%20lower%20memory%20usage%20than%20incremental%20kd-tree%20style%20map%20structures%2C%20without%20relying%20on%20locality-restricted%20search%20heuristics.%20Experiments%20on%20multiple%20real-world%20datasets%20show%20that%20LIMOncello%20achieves%20competitive%20accuracy%20while%20improving%20robustness%20in%20geometrically%20sparse%20environments.%20The%20system%20maintains%20real-time%20performance%20with%20stable%20memory%20growth%20and%20is%20released%20as%20an%20extensible%20open-source%20implementation%20at%20https%3A//github.com/CPerezRuiz335/LIMOncello.&entry.1838667208=http%3A//arxiv.org/abs/2512.19567v2&entry.124074799=Read"},
{"title": "API: Empowering Generalizable Real-World Image Dehazing via Adaptive Patch Importance Learning", "author": "Chen Zhu and Huiwen Zhang and Yujie Li and Mu He and Xiaotian Qiao", "abstract": "Real-world image dehazing is a fundamental yet challenging task in low-level vision. Existing learning-based methods often suffer from significant performance degradation when applied to complex real-world hazy scenes, primarily due to limited training data and the intrinsic complexity of haze density distributions.To address these challenges, we introduce a novel Adaptive Patch Importance-aware (API) framework for generalizable real-world image dehazing. Specifically, our framework consists of an Automatic Haze Generation (AHG) module and a Density-aware Haze Removal (DHR) module. AHG provides a hybrid data augmentation strategy by generating realistic and diverse hazy images as additional high-quality training data. DHR considers hazy regions with varying haze density distributions for generalizable real-world image dehazing in an adaptive patch importance-aware manner. To alleviate the ambiguity of the dehazed image details, we further introduce a new Multi-Negative Contrastive Dehazing (MNCD) loss, which fully utilizes information from multiple negative samples across both spatial and frequency domains. Extensive experiments demonstrate that our framework achieves state-of-the-art performance across multiple real-world benchmarks, delivering strong results in both quantitative metrics and qualitative visual quality, and exhibiting robust generalization across diverse haze distributions.", "link": "http://arxiv.org/abs/2601.01992v1", "date": "2026-01-05", "relevancy": 2.1542, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5691}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5341}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20API%3A%20Empowering%20Generalizable%20Real-World%20Image%20Dehazing%20via%20Adaptive%20Patch%20Importance%20Learning&body=Title%3A%20API%3A%20Empowering%20Generalizable%20Real-World%20Image%20Dehazing%20via%20Adaptive%20Patch%20Importance%20Learning%0AAuthor%3A%20Chen%20Zhu%20and%20Huiwen%20Zhang%20and%20Yujie%20Li%20and%20Mu%20He%20and%20Xiaotian%20Qiao%0AAbstract%3A%20Real-world%20image%20dehazing%20is%20a%20fundamental%20yet%20challenging%20task%20in%20low-level%20vision.%20Existing%20learning-based%20methods%20often%20suffer%20from%20significant%20performance%20degradation%20when%20applied%20to%20complex%20real-world%20hazy%20scenes%2C%20primarily%20due%20to%20limited%20training%20data%20and%20the%20intrinsic%20complexity%20of%20haze%20density%20distributions.To%20address%20these%20challenges%2C%20we%20introduce%20a%20novel%20Adaptive%20Patch%20Importance-aware%20%28API%29%20framework%20for%20generalizable%20real-world%20image%20dehazing.%20Specifically%2C%20our%20framework%20consists%20of%20an%20Automatic%20Haze%20Generation%20%28AHG%29%20module%20and%20a%20Density-aware%20Haze%20Removal%20%28DHR%29%20module.%20AHG%20provides%20a%20hybrid%20data%20augmentation%20strategy%20by%20generating%20realistic%20and%20diverse%20hazy%20images%20as%20additional%20high-quality%20training%20data.%20DHR%20considers%20hazy%20regions%20with%20varying%20haze%20density%20distributions%20for%20generalizable%20real-world%20image%20dehazing%20in%20an%20adaptive%20patch%20importance-aware%20manner.%20To%20alleviate%20the%20ambiguity%20of%20the%20dehazed%20image%20details%2C%20we%20further%20introduce%20a%20new%20Multi-Negative%20Contrastive%20Dehazing%20%28MNCD%29%20loss%2C%20which%20fully%20utilizes%20information%20from%20multiple%20negative%20samples%20across%20both%20spatial%20and%20frequency%20domains.%20Extensive%20experiments%20demonstrate%20that%20our%20framework%20achieves%20state-of-the-art%20performance%20across%20multiple%20real-world%20benchmarks%2C%20delivering%20strong%20results%20in%20both%20quantitative%20metrics%20and%20qualitative%20visual%20quality%2C%20and%20exhibiting%20robust%20generalization%20across%20diverse%20haze%20distributions.%0ALink%3A%20http%3A//arxiv.org/abs/2601.01992v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAPI%253A%2520Empowering%2520Generalizable%2520Real-World%2520Image%2520Dehazing%2520via%2520Adaptive%2520Patch%2520Importance%2520Learning%26entry.906535625%3DChen%2520Zhu%2520and%2520Huiwen%2520Zhang%2520and%2520Yujie%2520Li%2520and%2520Mu%2520He%2520and%2520Xiaotian%2520Qiao%26entry.1292438233%3DReal-world%2520image%2520dehazing%2520is%2520a%2520fundamental%2520yet%2520challenging%2520task%2520in%2520low-level%2520vision.%2520Existing%2520learning-based%2520methods%2520often%2520suffer%2520from%2520significant%2520performance%2520degradation%2520when%2520applied%2520to%2520complex%2520real-world%2520hazy%2520scenes%252C%2520primarily%2520due%2520to%2520limited%2520training%2520data%2520and%2520the%2520intrinsic%2520complexity%2520of%2520haze%2520density%2520distributions.To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520novel%2520Adaptive%2520Patch%2520Importance-aware%2520%2528API%2529%2520framework%2520for%2520generalizable%2520real-world%2520image%2520dehazing.%2520Specifically%252C%2520our%2520framework%2520consists%2520of%2520an%2520Automatic%2520Haze%2520Generation%2520%2528AHG%2529%2520module%2520and%2520a%2520Density-aware%2520Haze%2520Removal%2520%2528DHR%2529%2520module.%2520AHG%2520provides%2520a%2520hybrid%2520data%2520augmentation%2520strategy%2520by%2520generating%2520realistic%2520and%2520diverse%2520hazy%2520images%2520as%2520additional%2520high-quality%2520training%2520data.%2520DHR%2520considers%2520hazy%2520regions%2520with%2520varying%2520haze%2520density%2520distributions%2520for%2520generalizable%2520real-world%2520image%2520dehazing%2520in%2520an%2520adaptive%2520patch%2520importance-aware%2520manner.%2520To%2520alleviate%2520the%2520ambiguity%2520of%2520the%2520dehazed%2520image%2520details%252C%2520we%2520further%2520introduce%2520a%2520new%2520Multi-Negative%2520Contrastive%2520Dehazing%2520%2528MNCD%2529%2520loss%252C%2520which%2520fully%2520utilizes%2520information%2520from%2520multiple%2520negative%2520samples%2520across%2520both%2520spatial%2520and%2520frequency%2520domains.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520framework%2520achieves%2520state-of-the-art%2520performance%2520across%2520multiple%2520real-world%2520benchmarks%252C%2520delivering%2520strong%2520results%2520in%2520both%2520quantitative%2520metrics%2520and%2520qualitative%2520visual%2520quality%252C%2520and%2520exhibiting%2520robust%2520generalization%2520across%2520diverse%2520haze%2520distributions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.01992v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=API%3A%20Empowering%20Generalizable%20Real-World%20Image%20Dehazing%20via%20Adaptive%20Patch%20Importance%20Learning&entry.906535625=Chen%20Zhu%20and%20Huiwen%20Zhang%20and%20Yujie%20Li%20and%20Mu%20He%20and%20Xiaotian%20Qiao&entry.1292438233=Real-world%20image%20dehazing%20is%20a%20fundamental%20yet%20challenging%20task%20in%20low-level%20vision.%20Existing%20learning-based%20methods%20often%20suffer%20from%20significant%20performance%20degradation%20when%20applied%20to%20complex%20real-world%20hazy%20scenes%2C%20primarily%20due%20to%20limited%20training%20data%20and%20the%20intrinsic%20complexity%20of%20haze%20density%20distributions.To%20address%20these%20challenges%2C%20we%20introduce%20a%20novel%20Adaptive%20Patch%20Importance-aware%20%28API%29%20framework%20for%20generalizable%20real-world%20image%20dehazing.%20Specifically%2C%20our%20framework%20consists%20of%20an%20Automatic%20Haze%20Generation%20%28AHG%29%20module%20and%20a%20Density-aware%20Haze%20Removal%20%28DHR%29%20module.%20AHG%20provides%20a%20hybrid%20data%20augmentation%20strategy%20by%20generating%20realistic%20and%20diverse%20hazy%20images%20as%20additional%20high-quality%20training%20data.%20DHR%20considers%20hazy%20regions%20with%20varying%20haze%20density%20distributions%20for%20generalizable%20real-world%20image%20dehazing%20in%20an%20adaptive%20patch%20importance-aware%20manner.%20To%20alleviate%20the%20ambiguity%20of%20the%20dehazed%20image%20details%2C%20we%20further%20introduce%20a%20new%20Multi-Negative%20Contrastive%20Dehazing%20%28MNCD%29%20loss%2C%20which%20fully%20utilizes%20information%20from%20multiple%20negative%20samples%20across%20both%20spatial%20and%20frequency%20domains.%20Extensive%20experiments%20demonstrate%20that%20our%20framework%20achieves%20state-of-the-art%20performance%20across%20multiple%20real-world%20benchmarks%2C%20delivering%20strong%20results%20in%20both%20quantitative%20metrics%20and%20qualitative%20visual%20quality%2C%20and%20exhibiting%20robust%20generalization%20across%20diverse%20haze%20distributions.&entry.1838667208=http%3A//arxiv.org/abs/2601.01992v1&entry.124074799=Read"},
{"title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach", "author": "Matthias Bartolo and Dylan Seychell and Gabriel Hili and Matthew Montebello and Carl James Debono and Saviour Formosa and Konstantinos Makantasis", "abstract": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.", "link": "http://arxiv.org/abs/2601.02016v1", "date": "2026-01-05", "relevancy": 2.153, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5665}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5461}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Object%20Detection%20with%20Privileged%20Information%3A%20A%20Model-Agnostic%20Teacher-Student%20Approach&body=Title%3A%20Enhancing%20Object%20Detection%20with%20Privileged%20Information%3A%20A%20Model-Agnostic%20Teacher-Student%20Approach%0AAuthor%3A%20Matthias%20Bartolo%20and%20Dylan%20Seychell%20and%20Gabriel%20Hili%20and%20Matthew%20Montebello%20and%20Carl%20James%20Debono%20and%20Saviour%20Formosa%20and%20Konstantinos%20Makantasis%0AAbstract%3A%20This%20paper%20investigates%20the%20integration%20of%20the%20Learning%20Using%20Privileged%20Information%20%28LUPI%29%20paradigm%20in%20object%20detection%20to%20exploit%20fine-grained%2C%20descriptive%20information%20available%20during%20training%20but%20not%20at%20inference.%20We%20introduce%20a%20general%2C%20model-agnostic%20methodology%20for%20injecting%20privileged%20information-such%20as%20bounding%20box%20masks%2C%20saliency%20maps%2C%20and%20depth%20cues-into%20deep%20learning-based%20object%20detectors%20through%20a%20teacher-student%20architecture.%20Experiments%20are%20conducted%20across%20five%20state-of-the-art%20object%20detection%20models%20and%20multiple%20public%20benchmarks%2C%20including%20UAV-based%20litter%20detection%20datasets%20and%20Pascal%20VOC%202012%2C%20to%20assess%20the%20impact%20on%20accuracy%2C%20generalization%2C%20and%20computational%20efficiency.%20Our%20results%20demonstrate%20that%20LUPI-trained%20students%20consistently%20outperform%20their%20baseline%20counterparts%2C%20achieving%20significant%20boosts%20in%20detection%20accuracy%20with%20no%20increase%20in%20inference%20complexity%20or%20model%20size.%20Performance%20improvements%20are%20especially%20marked%20for%20medium%20and%20large%20objects%2C%20while%20ablation%20studies%20reveal%20that%20intermediate%20weighting%20of%20teacher%20guidance%20optimally%20balances%20learning%20from%20privileged%20and%20standard%20inputs.%20The%20findings%20affirm%20that%20the%20LUPI%20framework%20provides%20an%20effective%20and%20practical%20strategy%20for%20advancing%20object%20detection%20systems%20in%20both%20resource-constrained%20and%20real-world%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02016v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Object%2520Detection%2520with%2520Privileged%2520Information%253A%2520A%2520Model-Agnostic%2520Teacher-Student%2520Approach%26entry.906535625%3DMatthias%2520Bartolo%2520and%2520Dylan%2520Seychell%2520and%2520Gabriel%2520Hili%2520and%2520Matthew%2520Montebello%2520and%2520Carl%2520James%2520Debono%2520and%2520Saviour%2520Formosa%2520and%2520Konstantinos%2520Makantasis%26entry.1292438233%3DThis%2520paper%2520investigates%2520the%2520integration%2520of%2520the%2520Learning%2520Using%2520Privileged%2520Information%2520%2528LUPI%2529%2520paradigm%2520in%2520object%2520detection%2520to%2520exploit%2520fine-grained%252C%2520descriptive%2520information%2520available%2520during%2520training%2520but%2520not%2520at%2520inference.%2520We%2520introduce%2520a%2520general%252C%2520model-agnostic%2520methodology%2520for%2520injecting%2520privileged%2520information-such%2520as%2520bounding%2520box%2520masks%252C%2520saliency%2520maps%252C%2520and%2520depth%2520cues-into%2520deep%2520learning-based%2520object%2520detectors%2520through%2520a%2520teacher-student%2520architecture.%2520Experiments%2520are%2520conducted%2520across%2520five%2520state-of-the-art%2520object%2520detection%2520models%2520and%2520multiple%2520public%2520benchmarks%252C%2520including%2520UAV-based%2520litter%2520detection%2520datasets%2520and%2520Pascal%2520VOC%25202012%252C%2520to%2520assess%2520the%2520impact%2520on%2520accuracy%252C%2520generalization%252C%2520and%2520computational%2520efficiency.%2520Our%2520results%2520demonstrate%2520that%2520LUPI-trained%2520students%2520consistently%2520outperform%2520their%2520baseline%2520counterparts%252C%2520achieving%2520significant%2520boosts%2520in%2520detection%2520accuracy%2520with%2520no%2520increase%2520in%2520inference%2520complexity%2520or%2520model%2520size.%2520Performance%2520improvements%2520are%2520especially%2520marked%2520for%2520medium%2520and%2520large%2520objects%252C%2520while%2520ablation%2520studies%2520reveal%2520that%2520intermediate%2520weighting%2520of%2520teacher%2520guidance%2520optimally%2520balances%2520learning%2520from%2520privileged%2520and%2520standard%2520inputs.%2520The%2520findings%2520affirm%2520that%2520the%2520LUPI%2520framework%2520provides%2520an%2520effective%2520and%2520practical%2520strategy%2520for%2520advancing%2520object%2520detection%2520systems%2520in%2520both%2520resource-constrained%2520and%2520real-world%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02016v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Object%20Detection%20with%20Privileged%20Information%3A%20A%20Model-Agnostic%20Teacher-Student%20Approach&entry.906535625=Matthias%20Bartolo%20and%20Dylan%20Seychell%20and%20Gabriel%20Hili%20and%20Matthew%20Montebello%20and%20Carl%20James%20Debono%20and%20Saviour%20Formosa%20and%20Konstantinos%20Makantasis&entry.1292438233=This%20paper%20investigates%20the%20integration%20of%20the%20Learning%20Using%20Privileged%20Information%20%28LUPI%29%20paradigm%20in%20object%20detection%20to%20exploit%20fine-grained%2C%20descriptive%20information%20available%20during%20training%20but%20not%20at%20inference.%20We%20introduce%20a%20general%2C%20model-agnostic%20methodology%20for%20injecting%20privileged%20information-such%20as%20bounding%20box%20masks%2C%20saliency%20maps%2C%20and%20depth%20cues-into%20deep%20learning-based%20object%20detectors%20through%20a%20teacher-student%20architecture.%20Experiments%20are%20conducted%20across%20five%20state-of-the-art%20object%20detection%20models%20and%20multiple%20public%20benchmarks%2C%20including%20UAV-based%20litter%20detection%20datasets%20and%20Pascal%20VOC%202012%2C%20to%20assess%20the%20impact%20on%20accuracy%2C%20generalization%2C%20and%20computational%20efficiency.%20Our%20results%20demonstrate%20that%20LUPI-trained%20students%20consistently%20outperform%20their%20baseline%20counterparts%2C%20achieving%20significant%20boosts%20in%20detection%20accuracy%20with%20no%20increase%20in%20inference%20complexity%20or%20model%20size.%20Performance%20improvements%20are%20especially%20marked%20for%20medium%20and%20large%20objects%2C%20while%20ablation%20studies%20reveal%20that%20intermediate%20weighting%20of%20teacher%20guidance%20optimally%20balances%20learning%20from%20privileged%20and%20standard%20inputs.%20The%20findings%20affirm%20that%20the%20LUPI%20framework%20provides%20an%20effective%20and%20practical%20strategy%20for%20advancing%20object%20detection%20systems%20in%20both%20resource-constrained%20and%20real-world%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2601.02016v1&entry.124074799=Read"},
{"title": "FMVP: Masked Flow Matching for Adversarial Video Purification", "author": "Duoxun Tang and Xueyi Zhang and Chak Hin Wang and Xi Xiao and Dasen Dai and Xinhang Jiang and Wentao Shi and Rui Li and Qing Li", "abstract": "Video recognition models remain vulnerable to adversarial attacks, while existing diffusion-based purification methods suffer from inefficient sampling and curved trajectories. Directly regressing clean videos from adversarial inputs often fails to recover faithful content due to the subtle nature of perturbations; this necessitates physically shattering the adversarial structure. Therefore, we propose Flow Matching for Adversarial Video Purification FMVP. FMVP physically shatters global adversarial structures via a masking strategy and reconstructs clean video dynamics using Conditional Flow Matching (CFM) with an inpainting objective. To further decouple semantic content from adversarial noise, we design a Frequency-Gated Loss (FGL) that explicitly suppresses high-frequency adversarial residuals while preserving low-frequency fidelity. We design Attack-Aware and Generalist training paradigms to handle known and unknown threats, respectively. Extensive experiments on UCF-101 and HMDB-51 demonstrate that FMVP outperforms state-of-the-art methods (DiffPure, Defense Patterns (DP), Temporal Shuffling (TS) and FlowPure), achieving robust accuracy exceeding 87% against PGD and 89% against CW attacks. Furthermore, FMVP demonstrates superior robustness against adaptive attacks (DiffHammer) and functions as a zero-shot adversarial detector, attaining detection accuracies of 98% for PGD and 79% for highly imperceptible CW attacks.", "link": "http://arxiv.org/abs/2601.02228v1", "date": "2026-01-05", "relevancy": 2.1466, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5499}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5346}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FMVP%3A%20Masked%20Flow%20Matching%20for%20Adversarial%20Video%20Purification&body=Title%3A%20FMVP%3A%20Masked%20Flow%20Matching%20for%20Adversarial%20Video%20Purification%0AAuthor%3A%20Duoxun%20Tang%20and%20Xueyi%20Zhang%20and%20Chak%20Hin%20Wang%20and%20Xi%20Xiao%20and%20Dasen%20Dai%20and%20Xinhang%20Jiang%20and%20Wentao%20Shi%20and%20Rui%20Li%20and%20Qing%20Li%0AAbstract%3A%20Video%20recognition%20models%20remain%20vulnerable%20to%20adversarial%20attacks%2C%20while%20existing%20diffusion-based%20purification%20methods%20suffer%20from%20inefficient%20sampling%20and%20curved%20trajectories.%20Directly%20regressing%20clean%20videos%20from%20adversarial%20inputs%20often%20fails%20to%20recover%20faithful%20content%20due%20to%20the%20subtle%20nature%20of%20perturbations%3B%20this%20necessitates%20physically%20shattering%20the%20adversarial%20structure.%20Therefore%2C%20we%20propose%20Flow%20Matching%20for%20Adversarial%20Video%20Purification%20FMVP.%20FMVP%20physically%20shatters%20global%20adversarial%20structures%20via%20a%20masking%20strategy%20and%20reconstructs%20clean%20video%20dynamics%20using%20Conditional%20Flow%20Matching%20%28CFM%29%20with%20an%20inpainting%20objective.%20To%20further%20decouple%20semantic%20content%20from%20adversarial%20noise%2C%20we%20design%20a%20Frequency-Gated%20Loss%20%28FGL%29%20that%20explicitly%20suppresses%20high-frequency%20adversarial%20residuals%20while%20preserving%20low-frequency%20fidelity.%20We%20design%20Attack-Aware%20and%20Generalist%20training%20paradigms%20to%20handle%20known%20and%20unknown%20threats%2C%20respectively.%20Extensive%20experiments%20on%20UCF-101%20and%20HMDB-51%20demonstrate%20that%20FMVP%20outperforms%20state-of-the-art%20methods%20%28DiffPure%2C%20Defense%20Patterns%20%28DP%29%2C%20Temporal%20Shuffling%20%28TS%29%20and%20FlowPure%29%2C%20achieving%20robust%20accuracy%20exceeding%2087%25%20against%20PGD%20and%2089%25%20against%20CW%20attacks.%20Furthermore%2C%20FMVP%20demonstrates%20superior%20robustness%20against%20adaptive%20attacks%20%28DiffHammer%29%20and%20functions%20as%20a%20zero-shot%20adversarial%20detector%2C%20attaining%20detection%20accuracies%20of%2098%25%20for%20PGD%20and%2079%25%20for%20highly%20imperceptible%20CW%20attacks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02228v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFMVP%253A%2520Masked%2520Flow%2520Matching%2520for%2520Adversarial%2520Video%2520Purification%26entry.906535625%3DDuoxun%2520Tang%2520and%2520Xueyi%2520Zhang%2520and%2520Chak%2520Hin%2520Wang%2520and%2520Xi%2520Xiao%2520and%2520Dasen%2520Dai%2520and%2520Xinhang%2520Jiang%2520and%2520Wentao%2520Shi%2520and%2520Rui%2520Li%2520and%2520Qing%2520Li%26entry.1292438233%3DVideo%2520recognition%2520models%2520remain%2520vulnerable%2520to%2520adversarial%2520attacks%252C%2520while%2520existing%2520diffusion-based%2520purification%2520methods%2520suffer%2520from%2520inefficient%2520sampling%2520and%2520curved%2520trajectories.%2520Directly%2520regressing%2520clean%2520videos%2520from%2520adversarial%2520inputs%2520often%2520fails%2520to%2520recover%2520faithful%2520content%2520due%2520to%2520the%2520subtle%2520nature%2520of%2520perturbations%253B%2520this%2520necessitates%2520physically%2520shattering%2520the%2520adversarial%2520structure.%2520Therefore%252C%2520we%2520propose%2520Flow%2520Matching%2520for%2520Adversarial%2520Video%2520Purification%2520FMVP.%2520FMVP%2520physically%2520shatters%2520global%2520adversarial%2520structures%2520via%2520a%2520masking%2520strategy%2520and%2520reconstructs%2520clean%2520video%2520dynamics%2520using%2520Conditional%2520Flow%2520Matching%2520%2528CFM%2529%2520with%2520an%2520inpainting%2520objective.%2520To%2520further%2520decouple%2520semantic%2520content%2520from%2520adversarial%2520noise%252C%2520we%2520design%2520a%2520Frequency-Gated%2520Loss%2520%2528FGL%2529%2520that%2520explicitly%2520suppresses%2520high-frequency%2520adversarial%2520residuals%2520while%2520preserving%2520low-frequency%2520fidelity.%2520We%2520design%2520Attack-Aware%2520and%2520Generalist%2520training%2520paradigms%2520to%2520handle%2520known%2520and%2520unknown%2520threats%252C%2520respectively.%2520Extensive%2520experiments%2520on%2520UCF-101%2520and%2520HMDB-51%2520demonstrate%2520that%2520FMVP%2520outperforms%2520state-of-the-art%2520methods%2520%2528DiffPure%252C%2520Defense%2520Patterns%2520%2528DP%2529%252C%2520Temporal%2520Shuffling%2520%2528TS%2529%2520and%2520FlowPure%2529%252C%2520achieving%2520robust%2520accuracy%2520exceeding%252087%2525%2520against%2520PGD%2520and%252089%2525%2520against%2520CW%2520attacks.%2520Furthermore%252C%2520FMVP%2520demonstrates%2520superior%2520robustness%2520against%2520adaptive%2520attacks%2520%2528DiffHammer%2529%2520and%2520functions%2520as%2520a%2520zero-shot%2520adversarial%2520detector%252C%2520attaining%2520detection%2520accuracies%2520of%252098%2525%2520for%2520PGD%2520and%252079%2525%2520for%2520highly%2520imperceptible%2520CW%2520attacks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02228v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FMVP%3A%20Masked%20Flow%20Matching%20for%20Adversarial%20Video%20Purification&entry.906535625=Duoxun%20Tang%20and%20Xueyi%20Zhang%20and%20Chak%20Hin%20Wang%20and%20Xi%20Xiao%20and%20Dasen%20Dai%20and%20Xinhang%20Jiang%20and%20Wentao%20Shi%20and%20Rui%20Li%20and%20Qing%20Li&entry.1292438233=Video%20recognition%20models%20remain%20vulnerable%20to%20adversarial%20attacks%2C%20while%20existing%20diffusion-based%20purification%20methods%20suffer%20from%20inefficient%20sampling%20and%20curved%20trajectories.%20Directly%20regressing%20clean%20videos%20from%20adversarial%20inputs%20often%20fails%20to%20recover%20faithful%20content%20due%20to%20the%20subtle%20nature%20of%20perturbations%3B%20this%20necessitates%20physically%20shattering%20the%20adversarial%20structure.%20Therefore%2C%20we%20propose%20Flow%20Matching%20for%20Adversarial%20Video%20Purification%20FMVP.%20FMVP%20physically%20shatters%20global%20adversarial%20structures%20via%20a%20masking%20strategy%20and%20reconstructs%20clean%20video%20dynamics%20using%20Conditional%20Flow%20Matching%20%28CFM%29%20with%20an%20inpainting%20objective.%20To%20further%20decouple%20semantic%20content%20from%20adversarial%20noise%2C%20we%20design%20a%20Frequency-Gated%20Loss%20%28FGL%29%20that%20explicitly%20suppresses%20high-frequency%20adversarial%20residuals%20while%20preserving%20low-frequency%20fidelity.%20We%20design%20Attack-Aware%20and%20Generalist%20training%20paradigms%20to%20handle%20known%20and%20unknown%20threats%2C%20respectively.%20Extensive%20experiments%20on%20UCF-101%20and%20HMDB-51%20demonstrate%20that%20FMVP%20outperforms%20state-of-the-art%20methods%20%28DiffPure%2C%20Defense%20Patterns%20%28DP%29%2C%20Temporal%20Shuffling%20%28TS%29%20and%20FlowPure%29%2C%20achieving%20robust%20accuracy%20exceeding%2087%25%20against%20PGD%20and%2089%25%20against%20CW%20attacks.%20Furthermore%2C%20FMVP%20demonstrates%20superior%20robustness%20against%20adaptive%20attacks%20%28DiffHammer%29%20and%20functions%20as%20a%20zero-shot%20adversarial%20detector%2C%20attaining%20detection%20accuracies%20of%2098%25%20for%20PGD%20and%2079%25%20for%20highly%20imperceptible%20CW%20attacks.&entry.1838667208=http%3A//arxiv.org/abs/2601.02228v1&entry.124074799=Read"},
{"title": "DeCode: Decoupling Content and Delivery for Medical QA", "author": "Po-Jen Ko and Chen-Han Tsai and Yu-Shao Peng", "abstract": "Large language models (LLMs) exhibit strong medical knowledge and can generate factually accurate responses. However, existing models often fail to account for individual patient contexts, producing answers that are clinically correct yet poorly aligned with patients' needs. In this work, we introduce DeCode, a training-free, model-agnostic framework that adapts existing LLMs to produce contextualized answers in clinical settings. We evaluate DeCode on OpenAI HealthBench, a comprehensive and challenging benchmark designed to assess clinical relevance and validity of LLM responses. DeCode improves the previous state of the art from $28.4\\%$ to $49.8\\%$, corresponding to a $75\\%$ relative improvement. Experimental results suggest the effectiveness of DeCode in improving clinical question answering of LLMs.", "link": "http://arxiv.org/abs/2601.02123v1", "date": "2026-01-05", "relevancy": 2.145, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5385}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5385}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5249}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeCode%3A%20Decoupling%20Content%20and%20Delivery%20for%20Medical%20QA&body=Title%3A%20DeCode%3A%20Decoupling%20Content%20and%20Delivery%20for%20Medical%20QA%0AAuthor%3A%20Po-Jen%20Ko%20and%20Chen-Han%20Tsai%20and%20Yu-Shao%20Peng%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20exhibit%20strong%20medical%20knowledge%20and%20can%20generate%20factually%20accurate%20responses.%20However%2C%20existing%20models%20often%20fail%20to%20account%20for%20individual%20patient%20contexts%2C%20producing%20answers%20that%20are%20clinically%20correct%20yet%20poorly%20aligned%20with%20patients%27%20needs.%20In%20this%20work%2C%20we%20introduce%20DeCode%2C%20a%20training-free%2C%20model-agnostic%20framework%20that%20adapts%20existing%20LLMs%20to%20produce%20contextualized%20answers%20in%20clinical%20settings.%20We%20evaluate%20DeCode%20on%20OpenAI%20HealthBench%2C%20a%20comprehensive%20and%20challenging%20benchmark%20designed%20to%20assess%20clinical%20relevance%20and%20validity%20of%20LLM%20responses.%20DeCode%20improves%20the%20previous%20state%20of%20the%20art%20from%20%2428.4%5C%25%24%20to%20%2449.8%5C%25%24%2C%20corresponding%20to%20a%20%2475%5C%25%24%20relative%20improvement.%20Experimental%20results%20suggest%20the%20effectiveness%20of%20DeCode%20in%20improving%20clinical%20question%20answering%20of%20LLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02123v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeCode%253A%2520Decoupling%2520Content%2520and%2520Delivery%2520for%2520Medical%2520QA%26entry.906535625%3DPo-Jen%2520Ko%2520and%2520Chen-Han%2520Tsai%2520and%2520Yu-Shao%2520Peng%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520exhibit%2520strong%2520medical%2520knowledge%2520and%2520can%2520generate%2520factually%2520accurate%2520responses.%2520However%252C%2520existing%2520models%2520often%2520fail%2520to%2520account%2520for%2520individual%2520patient%2520contexts%252C%2520producing%2520answers%2520that%2520are%2520clinically%2520correct%2520yet%2520poorly%2520aligned%2520with%2520patients%2527%2520needs.%2520In%2520this%2520work%252C%2520we%2520introduce%2520DeCode%252C%2520a%2520training-free%252C%2520model-agnostic%2520framework%2520that%2520adapts%2520existing%2520LLMs%2520to%2520produce%2520contextualized%2520answers%2520in%2520clinical%2520settings.%2520We%2520evaluate%2520DeCode%2520on%2520OpenAI%2520HealthBench%252C%2520a%2520comprehensive%2520and%2520challenging%2520benchmark%2520designed%2520to%2520assess%2520clinical%2520relevance%2520and%2520validity%2520of%2520LLM%2520responses.%2520DeCode%2520improves%2520the%2520previous%2520state%2520of%2520the%2520art%2520from%2520%252428.4%255C%2525%2524%2520to%2520%252449.8%255C%2525%2524%252C%2520corresponding%2520to%2520a%2520%252475%255C%2525%2524%2520relative%2520improvement.%2520Experimental%2520results%2520suggest%2520the%2520effectiveness%2520of%2520DeCode%2520in%2520improving%2520clinical%2520question%2520answering%2520of%2520LLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02123v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeCode%3A%20Decoupling%20Content%20and%20Delivery%20for%20Medical%20QA&entry.906535625=Po-Jen%20Ko%20and%20Chen-Han%20Tsai%20and%20Yu-Shao%20Peng&entry.1292438233=Large%20language%20models%20%28LLMs%29%20exhibit%20strong%20medical%20knowledge%20and%20can%20generate%20factually%20accurate%20responses.%20However%2C%20existing%20models%20often%20fail%20to%20account%20for%20individual%20patient%20contexts%2C%20producing%20answers%20that%20are%20clinically%20correct%20yet%20poorly%20aligned%20with%20patients%27%20needs.%20In%20this%20work%2C%20we%20introduce%20DeCode%2C%20a%20training-free%2C%20model-agnostic%20framework%20that%20adapts%20existing%20LLMs%20to%20produce%20contextualized%20answers%20in%20clinical%20settings.%20We%20evaluate%20DeCode%20on%20OpenAI%20HealthBench%2C%20a%20comprehensive%20and%20challenging%20benchmark%20designed%20to%20assess%20clinical%20relevance%20and%20validity%20of%20LLM%20responses.%20DeCode%20improves%20the%20previous%20state%20of%20the%20art%20from%20%2428.4%5C%25%24%20to%20%2449.8%5C%25%24%2C%20corresponding%20to%20a%20%2475%5C%25%24%20relative%20improvement.%20Experimental%20results%20suggest%20the%20effectiveness%20of%20DeCode%20in%20improving%20clinical%20question%20answering%20of%20LLMs.&entry.1838667208=http%3A//arxiv.org/abs/2601.02123v1&entry.124074799=Read"},
{"title": "Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping", "author": "Saurabh Kaushik and Lalit Maurya and Beth Tellman", "abstract": "Geo-Foundation Models (GFMs), have proven effective in diverse downstream applications, including semantic segmentation, classification, and regression tasks. However, in case of flood mapping using Sen1Flood11 dataset as a downstream task, GFMs struggles to outperform the baseline U-Net, highlighting model's limitation in capturing critical local nuances. To address this, we present the Prithvi-Complementary Adaptive Fusion Encoder (CAFE), which integrate Prithvi GFM pretrained encoder with a parallel CNN residual branch enhanced by Convolutional Attention Modules (CAM). Prithvi-CAFE enables fast and efficient fine-tuning through adapters in Prithvi and performs multi-scale, multi-level fusion with CNN features, capturing critical local details while preserving long-range dependencies. We achieve state-of-the-art results on two comprehensive flood mapping datasets: Sen1Flood11 and FloodPlanet. On Sen1Flood11 test data, Prithvi-CAFE (IoU 83.41) outperforms the original Prithvi (IoU 82.50) and other major GFMs (TerraMind 82.90, DOFA 81.54, spectralGPT: 81.02). The improvement is even more pronounced on the hold-out test site, where Prithvi-CAFE achieves an IoU of 81.37 compared to the baseline U-Net (70.57) and original Prithvi (72.42). On FloodPlanet, Prithvi-CAFE also surpasses the baseline U-Net and other GFMs, achieving an IoU of 64.70 compared to U-Net (60.14), Terramind (62.33), DOFA (59.15) and Prithvi 2.0 (61.91). Our proposed simple yet effective Prithvi-CAFE demonstrates strong potential for improving segmentation tasks where multi-channel and multi-modal data provide complementary information and local details are critical. The code is released on \\href{https://github.com/Sk-2103/Prithvi-CAFE}{Prithvi-CAFE Github}", "link": "http://arxiv.org/abs/2601.02315v1", "date": "2026-01-05", "relevancy": 2.1193, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5584}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5135}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prithvi-Complimentary%20Adaptive%20Fusion%20Encoder%20%28CAFE%29%3A%20unlocking%20full-potential%20for%20flood%20inundation%20mapping&body=Title%3A%20Prithvi-Complimentary%20Adaptive%20Fusion%20Encoder%20%28CAFE%29%3A%20unlocking%20full-potential%20for%20flood%20inundation%20mapping%0AAuthor%3A%20Saurabh%20Kaushik%20and%20Lalit%20Maurya%20and%20Beth%20Tellman%0AAbstract%3A%20Geo-Foundation%20Models%20%28GFMs%29%2C%20have%20proven%20effective%20in%20diverse%20downstream%20applications%2C%20including%20semantic%20segmentation%2C%20classification%2C%20and%20regression%20tasks.%20However%2C%20in%20case%20of%20flood%20mapping%20using%20Sen1Flood11%20dataset%20as%20a%20downstream%20task%2C%20GFMs%20struggles%20to%20outperform%20the%20baseline%20U-Net%2C%20highlighting%20model%27s%20limitation%20in%20capturing%20critical%20local%20nuances.%20To%20address%20this%2C%20we%20present%20the%20Prithvi-Complementary%20Adaptive%20Fusion%20Encoder%20%28CAFE%29%2C%20which%20integrate%20Prithvi%20GFM%20pretrained%20encoder%20with%20a%20parallel%20CNN%20residual%20branch%20enhanced%20by%20Convolutional%20Attention%20Modules%20%28CAM%29.%20Prithvi-CAFE%20enables%20fast%20and%20efficient%20fine-tuning%20through%20adapters%20in%20Prithvi%20and%20performs%20multi-scale%2C%20multi-level%20fusion%20with%20CNN%20features%2C%20capturing%20critical%20local%20details%20while%20preserving%20long-range%20dependencies.%20We%20achieve%20state-of-the-art%20results%20on%20two%20comprehensive%20flood%20mapping%20datasets%3A%20Sen1Flood11%20and%20FloodPlanet.%20On%20Sen1Flood11%20test%20data%2C%20Prithvi-CAFE%20%28IoU%2083.41%29%20outperforms%20the%20original%20Prithvi%20%28IoU%2082.50%29%20and%20other%20major%20GFMs%20%28TerraMind%2082.90%2C%20DOFA%2081.54%2C%20spectralGPT%3A%2081.02%29.%20The%20improvement%20is%20even%20more%20pronounced%20on%20the%20hold-out%20test%20site%2C%20where%20Prithvi-CAFE%20achieves%20an%20IoU%20of%2081.37%20compared%20to%20the%20baseline%20U-Net%20%2870.57%29%20and%20original%20Prithvi%20%2872.42%29.%20On%20FloodPlanet%2C%20Prithvi-CAFE%20also%20surpasses%20the%20baseline%20U-Net%20and%20other%20GFMs%2C%20achieving%20an%20IoU%20of%2064.70%20compared%20to%20U-Net%20%2860.14%29%2C%20Terramind%20%2862.33%29%2C%20DOFA%20%2859.15%29%20and%20Prithvi%202.0%20%2861.91%29.%20Our%20proposed%20simple%20yet%20effective%20Prithvi-CAFE%20demonstrates%20strong%20potential%20for%20improving%20segmentation%20tasks%20where%20multi-channel%20and%20multi-modal%20data%20provide%20complementary%20information%20and%20local%20details%20are%20critical.%20The%20code%20is%20released%20on%20%5Chref%7Bhttps%3A//github.com/Sk-2103/Prithvi-CAFE%7D%7BPrithvi-CAFE%20Github%7D%0ALink%3A%20http%3A//arxiv.org/abs/2601.02315v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrithvi-Complimentary%2520Adaptive%2520Fusion%2520Encoder%2520%2528CAFE%2529%253A%2520unlocking%2520full-potential%2520for%2520flood%2520inundation%2520mapping%26entry.906535625%3DSaurabh%2520Kaushik%2520and%2520Lalit%2520Maurya%2520and%2520Beth%2520Tellman%26entry.1292438233%3DGeo-Foundation%2520Models%2520%2528GFMs%2529%252C%2520have%2520proven%2520effective%2520in%2520diverse%2520downstream%2520applications%252C%2520including%2520semantic%2520segmentation%252C%2520classification%252C%2520and%2520regression%2520tasks.%2520However%252C%2520in%2520case%2520of%2520flood%2520mapping%2520using%2520Sen1Flood11%2520dataset%2520as%2520a%2520downstream%2520task%252C%2520GFMs%2520struggles%2520to%2520outperform%2520the%2520baseline%2520U-Net%252C%2520highlighting%2520model%2527s%2520limitation%2520in%2520capturing%2520critical%2520local%2520nuances.%2520To%2520address%2520this%252C%2520we%2520present%2520the%2520Prithvi-Complementary%2520Adaptive%2520Fusion%2520Encoder%2520%2528CAFE%2529%252C%2520which%2520integrate%2520Prithvi%2520GFM%2520pretrained%2520encoder%2520with%2520a%2520parallel%2520CNN%2520residual%2520branch%2520enhanced%2520by%2520Convolutional%2520Attention%2520Modules%2520%2528CAM%2529.%2520Prithvi-CAFE%2520enables%2520fast%2520and%2520efficient%2520fine-tuning%2520through%2520adapters%2520in%2520Prithvi%2520and%2520performs%2520multi-scale%252C%2520multi-level%2520fusion%2520with%2520CNN%2520features%252C%2520capturing%2520critical%2520local%2520details%2520while%2520preserving%2520long-range%2520dependencies.%2520We%2520achieve%2520state-of-the-art%2520results%2520on%2520two%2520comprehensive%2520flood%2520mapping%2520datasets%253A%2520Sen1Flood11%2520and%2520FloodPlanet.%2520On%2520Sen1Flood11%2520test%2520data%252C%2520Prithvi-CAFE%2520%2528IoU%252083.41%2529%2520outperforms%2520the%2520original%2520Prithvi%2520%2528IoU%252082.50%2529%2520and%2520other%2520major%2520GFMs%2520%2528TerraMind%252082.90%252C%2520DOFA%252081.54%252C%2520spectralGPT%253A%252081.02%2529.%2520The%2520improvement%2520is%2520even%2520more%2520pronounced%2520on%2520the%2520hold-out%2520test%2520site%252C%2520where%2520Prithvi-CAFE%2520achieves%2520an%2520IoU%2520of%252081.37%2520compared%2520to%2520the%2520baseline%2520U-Net%2520%252870.57%2529%2520and%2520original%2520Prithvi%2520%252872.42%2529.%2520On%2520FloodPlanet%252C%2520Prithvi-CAFE%2520also%2520surpasses%2520the%2520baseline%2520U-Net%2520and%2520other%2520GFMs%252C%2520achieving%2520an%2520IoU%2520of%252064.70%2520compared%2520to%2520U-Net%2520%252860.14%2529%252C%2520Terramind%2520%252862.33%2529%252C%2520DOFA%2520%252859.15%2529%2520and%2520Prithvi%25202.0%2520%252861.91%2529.%2520Our%2520proposed%2520simple%2520yet%2520effective%2520Prithvi-CAFE%2520demonstrates%2520strong%2520potential%2520for%2520improving%2520segmentation%2520tasks%2520where%2520multi-channel%2520and%2520multi-modal%2520data%2520provide%2520complementary%2520information%2520and%2520local%2520details%2520are%2520critical.%2520The%2520code%2520is%2520released%2520on%2520%255Chref%257Bhttps%253A//github.com/Sk-2103/Prithvi-CAFE%257D%257BPrithvi-CAFE%2520Github%257D%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02315v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prithvi-Complimentary%20Adaptive%20Fusion%20Encoder%20%28CAFE%29%3A%20unlocking%20full-potential%20for%20flood%20inundation%20mapping&entry.906535625=Saurabh%20Kaushik%20and%20Lalit%20Maurya%20and%20Beth%20Tellman&entry.1292438233=Geo-Foundation%20Models%20%28GFMs%29%2C%20have%20proven%20effective%20in%20diverse%20downstream%20applications%2C%20including%20semantic%20segmentation%2C%20classification%2C%20and%20regression%20tasks.%20However%2C%20in%20case%20of%20flood%20mapping%20using%20Sen1Flood11%20dataset%20as%20a%20downstream%20task%2C%20GFMs%20struggles%20to%20outperform%20the%20baseline%20U-Net%2C%20highlighting%20model%27s%20limitation%20in%20capturing%20critical%20local%20nuances.%20To%20address%20this%2C%20we%20present%20the%20Prithvi-Complementary%20Adaptive%20Fusion%20Encoder%20%28CAFE%29%2C%20which%20integrate%20Prithvi%20GFM%20pretrained%20encoder%20with%20a%20parallel%20CNN%20residual%20branch%20enhanced%20by%20Convolutional%20Attention%20Modules%20%28CAM%29.%20Prithvi-CAFE%20enables%20fast%20and%20efficient%20fine-tuning%20through%20adapters%20in%20Prithvi%20and%20performs%20multi-scale%2C%20multi-level%20fusion%20with%20CNN%20features%2C%20capturing%20critical%20local%20details%20while%20preserving%20long-range%20dependencies.%20We%20achieve%20state-of-the-art%20results%20on%20two%20comprehensive%20flood%20mapping%20datasets%3A%20Sen1Flood11%20and%20FloodPlanet.%20On%20Sen1Flood11%20test%20data%2C%20Prithvi-CAFE%20%28IoU%2083.41%29%20outperforms%20the%20original%20Prithvi%20%28IoU%2082.50%29%20and%20other%20major%20GFMs%20%28TerraMind%2082.90%2C%20DOFA%2081.54%2C%20spectralGPT%3A%2081.02%29.%20The%20improvement%20is%20even%20more%20pronounced%20on%20the%20hold-out%20test%20site%2C%20where%20Prithvi-CAFE%20achieves%20an%20IoU%20of%2081.37%20compared%20to%20the%20baseline%20U-Net%20%2870.57%29%20and%20original%20Prithvi%20%2872.42%29.%20On%20FloodPlanet%2C%20Prithvi-CAFE%20also%20surpasses%20the%20baseline%20U-Net%20and%20other%20GFMs%2C%20achieving%20an%20IoU%20of%2064.70%20compared%20to%20U-Net%20%2860.14%29%2C%20Terramind%20%2862.33%29%2C%20DOFA%20%2859.15%29%20and%20Prithvi%202.0%20%2861.91%29.%20Our%20proposed%20simple%20yet%20effective%20Prithvi-CAFE%20demonstrates%20strong%20potential%20for%20improving%20segmentation%20tasks%20where%20multi-channel%20and%20multi-modal%20data%20provide%20complementary%20information%20and%20local%20details%20are%20critical.%20The%20code%20is%20released%20on%20%5Chref%7Bhttps%3A//github.com/Sk-2103/Prithvi-CAFE%7D%7BPrithvi-CAFE%20Github%7D&entry.1838667208=http%3A//arxiv.org/abs/2601.02315v1&entry.124074799=Read"},
{"title": "Deferred Commitment Decoding for Diffusion Language Models with Confidence-Aware Sliding Windows", "author": "Yingte Shu and Yuchuan Tian and Chao Xu and Yunhe Wang and Hanting Chen", "abstract": "Diffusion language models (DLMs) have recently emerged as a strong alternative to autoregressive models by enabling parallel text generation. To improve inference efficiency and KV-cache compatibility, prior work commonly adopts block-based diffusion, decoding tokens block by block. However, this paradigm suffers from a structural limitation that we term Boundary-Induced Context Truncation (BICT): undecoded tokens near block boundaries are forced to commit without access to nearby future context, even when such context could substantially reduce uncertainty. This limitation degrades decoding confidence and generation quality, especially for tasks requiring precise reasoning, such as mathematical problem solving and code generation. We propose Deferred Commitment Decoding (DCD), a novel, training-free decoding strategy that mitigates this issue. DCD maintains a confidence-aware sliding window over masked tokens, resolving low-uncertainty tokens early while deferring high-uncertainty tokens until sufficient contextual evidence becomes available. This design enables effective bidirectional information flow within the decoding window without sacrificing efficiency. Extensive experiments across multiple diffusion language models, benchmarks, and caching configurations show that DCD improves generation accuracy by 1.39% with comparable time on average compared to fixed block-based diffusion methods, with the most significant improvement reaching 9.0%. These results demonstrate that deferring token commitment based on uncertainty is a simple yet effective principle for improving both the quality and efficiency of diffusion language model decoding.", "link": "http://arxiv.org/abs/2601.02076v1", "date": "2026-01-05", "relevancy": 2.1061, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6118}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5139}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deferred%20Commitment%20Decoding%20for%20Diffusion%20Language%20Models%20with%20Confidence-Aware%20Sliding%20Windows&body=Title%3A%20Deferred%20Commitment%20Decoding%20for%20Diffusion%20Language%20Models%20with%20Confidence-Aware%20Sliding%20Windows%0AAuthor%3A%20Yingte%20Shu%20and%20Yuchuan%20Tian%20and%20Chao%20Xu%20and%20Yunhe%20Wang%20and%20Hanting%20Chen%0AAbstract%3A%20Diffusion%20language%20models%20%28DLMs%29%20have%20recently%20emerged%20as%20a%20strong%20alternative%20to%20autoregressive%20models%20by%20enabling%20parallel%20text%20generation.%20To%20improve%20inference%20efficiency%20and%20KV-cache%20compatibility%2C%20prior%20work%20commonly%20adopts%20block-based%20diffusion%2C%20decoding%20tokens%20block%20by%20block.%20However%2C%20this%20paradigm%20suffers%20from%20a%20structural%20limitation%20that%20we%20term%20Boundary-Induced%20Context%20Truncation%20%28BICT%29%3A%20undecoded%20tokens%20near%20block%20boundaries%20are%20forced%20to%20commit%20without%20access%20to%20nearby%20future%20context%2C%20even%20when%20such%20context%20could%20substantially%20reduce%20uncertainty.%20This%20limitation%20degrades%20decoding%20confidence%20and%20generation%20quality%2C%20especially%20for%20tasks%20requiring%20precise%20reasoning%2C%20such%20as%20mathematical%20problem%20solving%20and%20code%20generation.%20We%20propose%20Deferred%20Commitment%20Decoding%20%28DCD%29%2C%20a%20novel%2C%20training-free%20decoding%20strategy%20that%20mitigates%20this%20issue.%20DCD%20maintains%20a%20confidence-aware%20sliding%20window%20over%20masked%20tokens%2C%20resolving%20low-uncertainty%20tokens%20early%20while%20deferring%20high-uncertainty%20tokens%20until%20sufficient%20contextual%20evidence%20becomes%20available.%20This%20design%20enables%20effective%20bidirectional%20information%20flow%20within%20the%20decoding%20window%20without%20sacrificing%20efficiency.%20Extensive%20experiments%20across%20multiple%20diffusion%20language%20models%2C%20benchmarks%2C%20and%20caching%20configurations%20show%20that%20DCD%20improves%20generation%20accuracy%20by%201.39%25%20with%20comparable%20time%20on%20average%20compared%20to%20fixed%20block-based%20diffusion%20methods%2C%20with%20the%20most%20significant%20improvement%20reaching%209.0%25.%20These%20results%20demonstrate%20that%20deferring%20token%20commitment%20based%20on%20uncertainty%20is%20a%20simple%20yet%20effective%20principle%20for%20improving%20both%20the%20quality%20and%20efficiency%20of%20diffusion%20language%20model%20decoding.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02076v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeferred%2520Commitment%2520Decoding%2520for%2520Diffusion%2520Language%2520Models%2520with%2520Confidence-Aware%2520Sliding%2520Windows%26entry.906535625%3DYingte%2520Shu%2520and%2520Yuchuan%2520Tian%2520and%2520Chao%2520Xu%2520and%2520Yunhe%2520Wang%2520and%2520Hanting%2520Chen%26entry.1292438233%3DDiffusion%2520language%2520models%2520%2528DLMs%2529%2520have%2520recently%2520emerged%2520as%2520a%2520strong%2520alternative%2520to%2520autoregressive%2520models%2520by%2520enabling%2520parallel%2520text%2520generation.%2520To%2520improve%2520inference%2520efficiency%2520and%2520KV-cache%2520compatibility%252C%2520prior%2520work%2520commonly%2520adopts%2520block-based%2520diffusion%252C%2520decoding%2520tokens%2520block%2520by%2520block.%2520However%252C%2520this%2520paradigm%2520suffers%2520from%2520a%2520structural%2520limitation%2520that%2520we%2520term%2520Boundary-Induced%2520Context%2520Truncation%2520%2528BICT%2529%253A%2520undecoded%2520tokens%2520near%2520block%2520boundaries%2520are%2520forced%2520to%2520commit%2520without%2520access%2520to%2520nearby%2520future%2520context%252C%2520even%2520when%2520such%2520context%2520could%2520substantially%2520reduce%2520uncertainty.%2520This%2520limitation%2520degrades%2520decoding%2520confidence%2520and%2520generation%2520quality%252C%2520especially%2520for%2520tasks%2520requiring%2520precise%2520reasoning%252C%2520such%2520as%2520mathematical%2520problem%2520solving%2520and%2520code%2520generation.%2520We%2520propose%2520Deferred%2520Commitment%2520Decoding%2520%2528DCD%2529%252C%2520a%2520novel%252C%2520training-free%2520decoding%2520strategy%2520that%2520mitigates%2520this%2520issue.%2520DCD%2520maintains%2520a%2520confidence-aware%2520sliding%2520window%2520over%2520masked%2520tokens%252C%2520resolving%2520low-uncertainty%2520tokens%2520early%2520while%2520deferring%2520high-uncertainty%2520tokens%2520until%2520sufficient%2520contextual%2520evidence%2520becomes%2520available.%2520This%2520design%2520enables%2520effective%2520bidirectional%2520information%2520flow%2520within%2520the%2520decoding%2520window%2520without%2520sacrificing%2520efficiency.%2520Extensive%2520experiments%2520across%2520multiple%2520diffusion%2520language%2520models%252C%2520benchmarks%252C%2520and%2520caching%2520configurations%2520show%2520that%2520DCD%2520improves%2520generation%2520accuracy%2520by%25201.39%2525%2520with%2520comparable%2520time%2520on%2520average%2520compared%2520to%2520fixed%2520block-based%2520diffusion%2520methods%252C%2520with%2520the%2520most%2520significant%2520improvement%2520reaching%25209.0%2525.%2520These%2520results%2520demonstrate%2520that%2520deferring%2520token%2520commitment%2520based%2520on%2520uncertainty%2520is%2520a%2520simple%2520yet%2520effective%2520principle%2520for%2520improving%2520both%2520the%2520quality%2520and%2520efficiency%2520of%2520diffusion%2520language%2520model%2520decoding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02076v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deferred%20Commitment%20Decoding%20for%20Diffusion%20Language%20Models%20with%20Confidence-Aware%20Sliding%20Windows&entry.906535625=Yingte%20Shu%20and%20Yuchuan%20Tian%20and%20Chao%20Xu%20and%20Yunhe%20Wang%20and%20Hanting%20Chen&entry.1292438233=Diffusion%20language%20models%20%28DLMs%29%20have%20recently%20emerged%20as%20a%20strong%20alternative%20to%20autoregressive%20models%20by%20enabling%20parallel%20text%20generation.%20To%20improve%20inference%20efficiency%20and%20KV-cache%20compatibility%2C%20prior%20work%20commonly%20adopts%20block-based%20diffusion%2C%20decoding%20tokens%20block%20by%20block.%20However%2C%20this%20paradigm%20suffers%20from%20a%20structural%20limitation%20that%20we%20term%20Boundary-Induced%20Context%20Truncation%20%28BICT%29%3A%20undecoded%20tokens%20near%20block%20boundaries%20are%20forced%20to%20commit%20without%20access%20to%20nearby%20future%20context%2C%20even%20when%20such%20context%20could%20substantially%20reduce%20uncertainty.%20This%20limitation%20degrades%20decoding%20confidence%20and%20generation%20quality%2C%20especially%20for%20tasks%20requiring%20precise%20reasoning%2C%20such%20as%20mathematical%20problem%20solving%20and%20code%20generation.%20We%20propose%20Deferred%20Commitment%20Decoding%20%28DCD%29%2C%20a%20novel%2C%20training-free%20decoding%20strategy%20that%20mitigates%20this%20issue.%20DCD%20maintains%20a%20confidence-aware%20sliding%20window%20over%20masked%20tokens%2C%20resolving%20low-uncertainty%20tokens%20early%20while%20deferring%20high-uncertainty%20tokens%20until%20sufficient%20contextual%20evidence%20becomes%20available.%20This%20design%20enables%20effective%20bidirectional%20information%20flow%20within%20the%20decoding%20window%20without%20sacrificing%20efficiency.%20Extensive%20experiments%20across%20multiple%20diffusion%20language%20models%2C%20benchmarks%2C%20and%20caching%20configurations%20show%20that%20DCD%20improves%20generation%20accuracy%20by%201.39%25%20with%20comparable%20time%20on%20average%20compared%20to%20fixed%20block-based%20diffusion%20methods%2C%20with%20the%20most%20significant%20improvement%20reaching%209.0%25.%20These%20results%20demonstrate%20that%20deferring%20token%20commitment%20based%20on%20uncertainty%20is%20a%20simple%20yet%20effective%20principle%20for%20improving%20both%20the%20quality%20and%20efficiency%20of%20diffusion%20language%20model%20decoding.&entry.1838667208=http%3A//arxiv.org/abs/2601.02076v1&entry.124074799=Read"},
{"title": "Hunting for \"Oddballs\" with Machine Learning: Detecting Anomalous Exoplanets Using a Deep-Learned Low-Dimensional Representation of Transit Spectra with Autoencoders", "author": "Alexander Roman and Emilie Panek and Roy T. Forestano and Eyup B. Unlu and Katia Matcheva and Konstantin T. Matchev", "abstract": "This study explores the application of autoencoder-based machine learning techniques for anomaly detection to identify exoplanet atmospheres with unconventional chemical signatures using a low-dimensional data representation. We use the Atmospheric Big Challenge (ABC) database, a publicly available dataset with over 100,000 simulated exoplanet spectra, to construct an anomaly detection scenario by defining CO2-rich atmospheres as anomalies and CO2-poor atmospheres as the normal class. We benchmarked four different anomaly detection strategies: Autoencoder Reconstruction Loss, One-Class Support Vector Machine (1 class-SVM), K-means Clustering, and Local Outlier Factor (LOF). Each method was evaluated in both the original spectral space and the autoencoder's latent space using Receiver Operating Characteristic (ROC) curves and Area Under the Curve (AUC) metrics. To test the performance of the different methods under realistic conditions, we introduced Gaussian noise levels ranging from 10 to 50 ppm. Our results indicate that anomaly detection is consistently more effective when performed within the latent space across all noise levels. Specifically, K-means clustering in the latent space emerged as a stable and high-performing method. We demonstrate that this anomaly detection approach is robust to noise levels up to 30 ppm (consistent with realistic space-based observations) and remains viable even at 50 ppm when leveraging latent space representations. On the other hand, the performance of the anomaly detection methods applied directly in the raw spectral space degrades significantly with increasing the level of noise. This suggests that autoencoder-driven dimensionality reduction offers a robust methodology for flagging chemically anomalous targets in large-scale surveys where exhaustive retrievals are computationally prohibitive.", "link": "http://arxiv.org/abs/2601.02324v1", "date": "2026-01-05", "relevancy": 2.1045, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5348}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5215}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5193}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hunting%20for%20%22Oddballs%22%20with%20Machine%20Learning%3A%20Detecting%20Anomalous%20Exoplanets%20Using%20a%20Deep-Learned%20Low-Dimensional%20Representation%20of%20Transit%20Spectra%20with%20Autoencoders&body=Title%3A%20Hunting%20for%20%22Oddballs%22%20with%20Machine%20Learning%3A%20Detecting%20Anomalous%20Exoplanets%20Using%20a%20Deep-Learned%20Low-Dimensional%20Representation%20of%20Transit%20Spectra%20with%20Autoencoders%0AAuthor%3A%20Alexander%20Roman%20and%20Emilie%20Panek%20and%20Roy%20T.%20Forestano%20and%20Eyup%20B.%20Unlu%20and%20Katia%20Matcheva%20and%20Konstantin%20T.%20Matchev%0AAbstract%3A%20This%20study%20explores%20the%20application%20of%20autoencoder-based%20machine%20learning%20techniques%20for%20anomaly%20detection%20to%20identify%20exoplanet%20atmospheres%20with%20unconventional%20chemical%20signatures%20using%20a%20low-dimensional%20data%20representation.%20We%20use%20the%20Atmospheric%20Big%20Challenge%20%28ABC%29%20database%2C%20a%20publicly%20available%20dataset%20with%20over%20100%2C000%20simulated%20exoplanet%20spectra%2C%20to%20construct%20an%20anomaly%20detection%20scenario%20by%20defining%20CO2-rich%20atmospheres%20as%20anomalies%20and%20CO2-poor%20atmospheres%20as%20the%20normal%20class.%20We%20benchmarked%20four%20different%20anomaly%20detection%20strategies%3A%20Autoencoder%20Reconstruction%20Loss%2C%20One-Class%20Support%20Vector%20Machine%20%281%20class-SVM%29%2C%20K-means%20Clustering%2C%20and%20Local%20Outlier%20Factor%20%28LOF%29.%20Each%20method%20was%20evaluated%20in%20both%20the%20original%20spectral%20space%20and%20the%20autoencoder%27s%20latent%20space%20using%20Receiver%20Operating%20Characteristic%20%28ROC%29%20curves%20and%20Area%20Under%20the%20Curve%20%28AUC%29%20metrics.%20To%20test%20the%20performance%20of%20the%20different%20methods%20under%20realistic%20conditions%2C%20we%20introduced%20Gaussian%20noise%20levels%20ranging%20from%2010%20to%2050%20ppm.%20Our%20results%20indicate%20that%20anomaly%20detection%20is%20consistently%20more%20effective%20when%20performed%20within%20the%20latent%20space%20across%20all%20noise%20levels.%20Specifically%2C%20K-means%20clustering%20in%20the%20latent%20space%20emerged%20as%20a%20stable%20and%20high-performing%20method.%20We%20demonstrate%20that%20this%20anomaly%20detection%20approach%20is%20robust%20to%20noise%20levels%20up%20to%2030%20ppm%20%28consistent%20with%20realistic%20space-based%20observations%29%20and%20remains%20viable%20even%20at%2050%20ppm%20when%20leveraging%20latent%20space%20representations.%20On%20the%20other%20hand%2C%20the%20performance%20of%20the%20anomaly%20detection%20methods%20applied%20directly%20in%20the%20raw%20spectral%20space%20degrades%20significantly%20with%20increasing%20the%20level%20of%20noise.%20This%20suggests%20that%20autoencoder-driven%20dimensionality%20reduction%20offers%20a%20robust%20methodology%20for%20flagging%20chemically%20anomalous%20targets%20in%20large-scale%20surveys%20where%20exhaustive%20retrievals%20are%20computationally%20prohibitive.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02324v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHunting%2520for%2520%2522Oddballs%2522%2520with%2520Machine%2520Learning%253A%2520Detecting%2520Anomalous%2520Exoplanets%2520Using%2520a%2520Deep-Learned%2520Low-Dimensional%2520Representation%2520of%2520Transit%2520Spectra%2520with%2520Autoencoders%26entry.906535625%3DAlexander%2520Roman%2520and%2520Emilie%2520Panek%2520and%2520Roy%2520T.%2520Forestano%2520and%2520Eyup%2520B.%2520Unlu%2520and%2520Katia%2520Matcheva%2520and%2520Konstantin%2520T.%2520Matchev%26entry.1292438233%3DThis%2520study%2520explores%2520the%2520application%2520of%2520autoencoder-based%2520machine%2520learning%2520techniques%2520for%2520anomaly%2520detection%2520to%2520identify%2520exoplanet%2520atmospheres%2520with%2520unconventional%2520chemical%2520signatures%2520using%2520a%2520low-dimensional%2520data%2520representation.%2520We%2520use%2520the%2520Atmospheric%2520Big%2520Challenge%2520%2528ABC%2529%2520database%252C%2520a%2520publicly%2520available%2520dataset%2520with%2520over%2520100%252C000%2520simulated%2520exoplanet%2520spectra%252C%2520to%2520construct%2520an%2520anomaly%2520detection%2520scenario%2520by%2520defining%2520CO2-rich%2520atmospheres%2520as%2520anomalies%2520and%2520CO2-poor%2520atmospheres%2520as%2520the%2520normal%2520class.%2520We%2520benchmarked%2520four%2520different%2520anomaly%2520detection%2520strategies%253A%2520Autoencoder%2520Reconstruction%2520Loss%252C%2520One-Class%2520Support%2520Vector%2520Machine%2520%25281%2520class-SVM%2529%252C%2520K-means%2520Clustering%252C%2520and%2520Local%2520Outlier%2520Factor%2520%2528LOF%2529.%2520Each%2520method%2520was%2520evaluated%2520in%2520both%2520the%2520original%2520spectral%2520space%2520and%2520the%2520autoencoder%2527s%2520latent%2520space%2520using%2520Receiver%2520Operating%2520Characteristic%2520%2528ROC%2529%2520curves%2520and%2520Area%2520Under%2520the%2520Curve%2520%2528AUC%2529%2520metrics.%2520To%2520test%2520the%2520performance%2520of%2520the%2520different%2520methods%2520under%2520realistic%2520conditions%252C%2520we%2520introduced%2520Gaussian%2520noise%2520levels%2520ranging%2520from%252010%2520to%252050%2520ppm.%2520Our%2520results%2520indicate%2520that%2520anomaly%2520detection%2520is%2520consistently%2520more%2520effective%2520when%2520performed%2520within%2520the%2520latent%2520space%2520across%2520all%2520noise%2520levels.%2520Specifically%252C%2520K-means%2520clustering%2520in%2520the%2520latent%2520space%2520emerged%2520as%2520a%2520stable%2520and%2520high-performing%2520method.%2520We%2520demonstrate%2520that%2520this%2520anomaly%2520detection%2520approach%2520is%2520robust%2520to%2520noise%2520levels%2520up%2520to%252030%2520ppm%2520%2528consistent%2520with%2520realistic%2520space-based%2520observations%2529%2520and%2520remains%2520viable%2520even%2520at%252050%2520ppm%2520when%2520leveraging%2520latent%2520space%2520representations.%2520On%2520the%2520other%2520hand%252C%2520the%2520performance%2520of%2520the%2520anomaly%2520detection%2520methods%2520applied%2520directly%2520in%2520the%2520raw%2520spectral%2520space%2520degrades%2520significantly%2520with%2520increasing%2520the%2520level%2520of%2520noise.%2520This%2520suggests%2520that%2520autoencoder-driven%2520dimensionality%2520reduction%2520offers%2520a%2520robust%2520methodology%2520for%2520flagging%2520chemically%2520anomalous%2520targets%2520in%2520large-scale%2520surveys%2520where%2520exhaustive%2520retrievals%2520are%2520computationally%2520prohibitive.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02324v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hunting%20for%20%22Oddballs%22%20with%20Machine%20Learning%3A%20Detecting%20Anomalous%20Exoplanets%20Using%20a%20Deep-Learned%20Low-Dimensional%20Representation%20of%20Transit%20Spectra%20with%20Autoencoders&entry.906535625=Alexander%20Roman%20and%20Emilie%20Panek%20and%20Roy%20T.%20Forestano%20and%20Eyup%20B.%20Unlu%20and%20Katia%20Matcheva%20and%20Konstantin%20T.%20Matchev&entry.1292438233=This%20study%20explores%20the%20application%20of%20autoencoder-based%20machine%20learning%20techniques%20for%20anomaly%20detection%20to%20identify%20exoplanet%20atmospheres%20with%20unconventional%20chemical%20signatures%20using%20a%20low-dimensional%20data%20representation.%20We%20use%20the%20Atmospheric%20Big%20Challenge%20%28ABC%29%20database%2C%20a%20publicly%20available%20dataset%20with%20over%20100%2C000%20simulated%20exoplanet%20spectra%2C%20to%20construct%20an%20anomaly%20detection%20scenario%20by%20defining%20CO2-rich%20atmospheres%20as%20anomalies%20and%20CO2-poor%20atmospheres%20as%20the%20normal%20class.%20We%20benchmarked%20four%20different%20anomaly%20detection%20strategies%3A%20Autoencoder%20Reconstruction%20Loss%2C%20One-Class%20Support%20Vector%20Machine%20%281%20class-SVM%29%2C%20K-means%20Clustering%2C%20and%20Local%20Outlier%20Factor%20%28LOF%29.%20Each%20method%20was%20evaluated%20in%20both%20the%20original%20spectral%20space%20and%20the%20autoencoder%27s%20latent%20space%20using%20Receiver%20Operating%20Characteristic%20%28ROC%29%20curves%20and%20Area%20Under%20the%20Curve%20%28AUC%29%20metrics.%20To%20test%20the%20performance%20of%20the%20different%20methods%20under%20realistic%20conditions%2C%20we%20introduced%20Gaussian%20noise%20levels%20ranging%20from%2010%20to%2050%20ppm.%20Our%20results%20indicate%20that%20anomaly%20detection%20is%20consistently%20more%20effective%20when%20performed%20within%20the%20latent%20space%20across%20all%20noise%20levels.%20Specifically%2C%20K-means%20clustering%20in%20the%20latent%20space%20emerged%20as%20a%20stable%20and%20high-performing%20method.%20We%20demonstrate%20that%20this%20anomaly%20detection%20approach%20is%20robust%20to%20noise%20levels%20up%20to%2030%20ppm%20%28consistent%20with%20realistic%20space-based%20observations%29%20and%20remains%20viable%20even%20at%2050%20ppm%20when%20leveraging%20latent%20space%20representations.%20On%20the%20other%20hand%2C%20the%20performance%20of%20the%20anomaly%20detection%20methods%20applied%20directly%20in%20the%20raw%20spectral%20space%20degrades%20significantly%20with%20increasing%20the%20level%20of%20noise.%20This%20suggests%20that%20autoencoder-driven%20dimensionality%20reduction%20offers%20a%20robust%20methodology%20for%20flagging%20chemically%20anomalous%20targets%20in%20large-scale%20surveys%20where%20exhaustive%20retrievals%20are%20computationally%20prohibitive.&entry.1838667208=http%3A//arxiv.org/abs/2601.02324v1&entry.124074799=Read"},
{"title": "SerpentFlow: Generative Unpaired Domain Alignment via Shared-Structure Decomposition", "author": "Julie Keisler and Anastase Alexandre Charantonis and Yannig Goude and Boutheina Oueslati and Claire Monteleoni", "abstract": "Domain alignment refers broadly to learning correspondences between data distributions from distinct domains. In this work, we focus on a setting where domains share underlying structural patterns despite differences in their specific realizations. The task is particularly challenging in the absence of paired observations, which removes direct supervision across domains. We introduce a generative framework, called SerpentFlow (SharEd-structuRe decomPosition for gEnerative domaiN adapTation), for unpaired domain alignment. SerpentFlow decomposes data within a latent space into a shared component common to both domains and a domain-specific one. By isolating the shared structure and replacing the domain-specific component with stochastic noise, we construct synthetic training pairs between shared representations and target-domain samples, thereby enabling the use of conditional generative models that are traditionally restricted to paired settings. We apply this approach to super-resolution tasks, where the shared component naturally corresponds to low-frequency content while high-frequency details capture domain-specific variability. The cutoff frequency separating low- and high-frequency components is determined automatically using a classifier-based criterion, ensuring a data-driven and domain-adaptive decomposition. By generating pseudo-pairs that preserve low-frequency structures while injecting stochastic high-frequency realizations, we learn the conditional distribution of the target domain given the shared representation. We implement SerpentFlow using Flow Matching as the generative pipeline, although the framework is compatible with other conditional generative approaches. Experiments on synthetic images, physical process simulations, and a climate downscaling task demonstrate that the method effectively reconstructs high-frequency structures consistent with underlying low-frequency patterns, supporting shared-structure decomposition as an effective strategy for unpaired domain alignment.", "link": "http://arxiv.org/abs/2601.01979v1", "date": "2026-01-05", "relevancy": 2.1008, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5789}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5176}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SerpentFlow%3A%20Generative%20Unpaired%20Domain%20Alignment%20via%20Shared-Structure%20Decomposition&body=Title%3A%20SerpentFlow%3A%20Generative%20Unpaired%20Domain%20Alignment%20via%20Shared-Structure%20Decomposition%0AAuthor%3A%20Julie%20Keisler%20and%20Anastase%20Alexandre%20Charantonis%20and%20Yannig%20Goude%20and%20Boutheina%20Oueslati%20and%20Claire%20Monteleoni%0AAbstract%3A%20Domain%20alignment%20refers%20broadly%20to%20learning%20correspondences%20between%20data%20distributions%20from%20distinct%20domains.%20In%20this%20work%2C%20we%20focus%20on%20a%20setting%20where%20domains%20share%20underlying%20structural%20patterns%20despite%20differences%20in%20their%20specific%20realizations.%20The%20task%20is%20particularly%20challenging%20in%20the%20absence%20of%20paired%20observations%2C%20which%20removes%20direct%20supervision%20across%20domains.%20We%20introduce%20a%20generative%20framework%2C%20called%20SerpentFlow%20%28SharEd-structuRe%20decomPosition%20for%20gEnerative%20domaiN%20adapTation%29%2C%20for%20unpaired%20domain%20alignment.%20SerpentFlow%20decomposes%20data%20within%20a%20latent%20space%20into%20a%20shared%20component%20common%20to%20both%20domains%20and%20a%20domain-specific%20one.%20By%20isolating%20the%20shared%20structure%20and%20replacing%20the%20domain-specific%20component%20with%20stochastic%20noise%2C%20we%20construct%20synthetic%20training%20pairs%20between%20shared%20representations%20and%20target-domain%20samples%2C%20thereby%20enabling%20the%20use%20of%20conditional%20generative%20models%20that%20are%20traditionally%20restricted%20to%20paired%20settings.%20We%20apply%20this%20approach%20to%20super-resolution%20tasks%2C%20where%20the%20shared%20component%20naturally%20corresponds%20to%20low-frequency%20content%20while%20high-frequency%20details%20capture%20domain-specific%20variability.%20The%20cutoff%20frequency%20separating%20low-%20and%20high-frequency%20components%20is%20determined%20automatically%20using%20a%20classifier-based%20criterion%2C%20ensuring%20a%20data-driven%20and%20domain-adaptive%20decomposition.%20By%20generating%20pseudo-pairs%20that%20preserve%20low-frequency%20structures%20while%20injecting%20stochastic%20high-frequency%20realizations%2C%20we%20learn%20the%20conditional%20distribution%20of%20the%20target%20domain%20given%20the%20shared%20representation.%20We%20implement%20SerpentFlow%20using%20Flow%20Matching%20as%20the%20generative%20pipeline%2C%20although%20the%20framework%20is%20compatible%20with%20other%20conditional%20generative%20approaches.%20Experiments%20on%20synthetic%20images%2C%20physical%20process%20simulations%2C%20and%20a%20climate%20downscaling%20task%20demonstrate%20that%20the%20method%20effectively%20reconstructs%20high-frequency%20structures%20consistent%20with%20underlying%20low-frequency%20patterns%2C%20supporting%20shared-structure%20decomposition%20as%20an%20effective%20strategy%20for%20unpaired%20domain%20alignment.%0ALink%3A%20http%3A//arxiv.org/abs/2601.01979v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSerpentFlow%253A%2520Generative%2520Unpaired%2520Domain%2520Alignment%2520via%2520Shared-Structure%2520Decomposition%26entry.906535625%3DJulie%2520Keisler%2520and%2520Anastase%2520Alexandre%2520Charantonis%2520and%2520Yannig%2520Goude%2520and%2520Boutheina%2520Oueslati%2520and%2520Claire%2520Monteleoni%26entry.1292438233%3DDomain%2520alignment%2520refers%2520broadly%2520to%2520learning%2520correspondences%2520between%2520data%2520distributions%2520from%2520distinct%2520domains.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520a%2520setting%2520where%2520domains%2520share%2520underlying%2520structural%2520patterns%2520despite%2520differences%2520in%2520their%2520specific%2520realizations.%2520The%2520task%2520is%2520particularly%2520challenging%2520in%2520the%2520absence%2520of%2520paired%2520observations%252C%2520which%2520removes%2520direct%2520supervision%2520across%2520domains.%2520We%2520introduce%2520a%2520generative%2520framework%252C%2520called%2520SerpentFlow%2520%2528SharEd-structuRe%2520decomPosition%2520for%2520gEnerative%2520domaiN%2520adapTation%2529%252C%2520for%2520unpaired%2520domain%2520alignment.%2520SerpentFlow%2520decomposes%2520data%2520within%2520a%2520latent%2520space%2520into%2520a%2520shared%2520component%2520common%2520to%2520both%2520domains%2520and%2520a%2520domain-specific%2520one.%2520By%2520isolating%2520the%2520shared%2520structure%2520and%2520replacing%2520the%2520domain-specific%2520component%2520with%2520stochastic%2520noise%252C%2520we%2520construct%2520synthetic%2520training%2520pairs%2520between%2520shared%2520representations%2520and%2520target-domain%2520samples%252C%2520thereby%2520enabling%2520the%2520use%2520of%2520conditional%2520generative%2520models%2520that%2520are%2520traditionally%2520restricted%2520to%2520paired%2520settings.%2520We%2520apply%2520this%2520approach%2520to%2520super-resolution%2520tasks%252C%2520where%2520the%2520shared%2520component%2520naturally%2520corresponds%2520to%2520low-frequency%2520content%2520while%2520high-frequency%2520details%2520capture%2520domain-specific%2520variability.%2520The%2520cutoff%2520frequency%2520separating%2520low-%2520and%2520high-frequency%2520components%2520is%2520determined%2520automatically%2520using%2520a%2520classifier-based%2520criterion%252C%2520ensuring%2520a%2520data-driven%2520and%2520domain-adaptive%2520decomposition.%2520By%2520generating%2520pseudo-pairs%2520that%2520preserve%2520low-frequency%2520structures%2520while%2520injecting%2520stochastic%2520high-frequency%2520realizations%252C%2520we%2520learn%2520the%2520conditional%2520distribution%2520of%2520the%2520target%2520domain%2520given%2520the%2520shared%2520representation.%2520We%2520implement%2520SerpentFlow%2520using%2520Flow%2520Matching%2520as%2520the%2520generative%2520pipeline%252C%2520although%2520the%2520framework%2520is%2520compatible%2520with%2520other%2520conditional%2520generative%2520approaches.%2520Experiments%2520on%2520synthetic%2520images%252C%2520physical%2520process%2520simulations%252C%2520and%2520a%2520climate%2520downscaling%2520task%2520demonstrate%2520that%2520the%2520method%2520effectively%2520reconstructs%2520high-frequency%2520structures%2520consistent%2520with%2520underlying%2520low-frequency%2520patterns%252C%2520supporting%2520shared-structure%2520decomposition%2520as%2520an%2520effective%2520strategy%2520for%2520unpaired%2520domain%2520alignment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.01979v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SerpentFlow%3A%20Generative%20Unpaired%20Domain%20Alignment%20via%20Shared-Structure%20Decomposition&entry.906535625=Julie%20Keisler%20and%20Anastase%20Alexandre%20Charantonis%20and%20Yannig%20Goude%20and%20Boutheina%20Oueslati%20and%20Claire%20Monteleoni&entry.1292438233=Domain%20alignment%20refers%20broadly%20to%20learning%20correspondences%20between%20data%20distributions%20from%20distinct%20domains.%20In%20this%20work%2C%20we%20focus%20on%20a%20setting%20where%20domains%20share%20underlying%20structural%20patterns%20despite%20differences%20in%20their%20specific%20realizations.%20The%20task%20is%20particularly%20challenging%20in%20the%20absence%20of%20paired%20observations%2C%20which%20removes%20direct%20supervision%20across%20domains.%20We%20introduce%20a%20generative%20framework%2C%20called%20SerpentFlow%20%28SharEd-structuRe%20decomPosition%20for%20gEnerative%20domaiN%20adapTation%29%2C%20for%20unpaired%20domain%20alignment.%20SerpentFlow%20decomposes%20data%20within%20a%20latent%20space%20into%20a%20shared%20component%20common%20to%20both%20domains%20and%20a%20domain-specific%20one.%20By%20isolating%20the%20shared%20structure%20and%20replacing%20the%20domain-specific%20component%20with%20stochastic%20noise%2C%20we%20construct%20synthetic%20training%20pairs%20between%20shared%20representations%20and%20target-domain%20samples%2C%20thereby%20enabling%20the%20use%20of%20conditional%20generative%20models%20that%20are%20traditionally%20restricted%20to%20paired%20settings.%20We%20apply%20this%20approach%20to%20super-resolution%20tasks%2C%20where%20the%20shared%20component%20naturally%20corresponds%20to%20low-frequency%20content%20while%20high-frequency%20details%20capture%20domain-specific%20variability.%20The%20cutoff%20frequency%20separating%20low-%20and%20high-frequency%20components%20is%20determined%20automatically%20using%20a%20classifier-based%20criterion%2C%20ensuring%20a%20data-driven%20and%20domain-adaptive%20decomposition.%20By%20generating%20pseudo-pairs%20that%20preserve%20low-frequency%20structures%20while%20injecting%20stochastic%20high-frequency%20realizations%2C%20we%20learn%20the%20conditional%20distribution%20of%20the%20target%20domain%20given%20the%20shared%20representation.%20We%20implement%20SerpentFlow%20using%20Flow%20Matching%20as%20the%20generative%20pipeline%2C%20although%20the%20framework%20is%20compatible%20with%20other%20conditional%20generative%20approaches.%20Experiments%20on%20synthetic%20images%2C%20physical%20process%20simulations%2C%20and%20a%20climate%20downscaling%20task%20demonstrate%20that%20the%20method%20effectively%20reconstructs%20high-frequency%20structures%20consistent%20with%20underlying%20low-frequency%20patterns%2C%20supporting%20shared-structure%20decomposition%20as%20an%20effective%20strategy%20for%20unpaired%20domain%20alignment.&entry.1838667208=http%3A//arxiv.org/abs/2601.01979v1&entry.124074799=Read"},
{"title": "Bayesian uncertainty-aware deep learning with noisy labels: Tackling annotation ambiguity in EEG seizure detection", "author": "Deeksha M. Shama and Archana Venkataraman", "abstract": "Deep learning is advancing EEG processing for automated epileptic seizure detection and onset zone localization, yet its performance relies heavily on high-quality annotated training data. However, scalp EEG is susceptible to high noise levels, which in turn leads to imprecise annotations of the seizure timing and characteristics. This \"label noise\" presents a significant challenge in model training and generalization. In this paper, we introduce Bayesian UncertaiNty-aware Deep Learning (BUNDL), a novel algorithm that informs a deep learning model of label ambiguities, thereby enhancing the robustness of seizure detection systems. By integrating domain knowledge into an underlying Bayesian framework, we derive a novel KL-divergence-based loss function that capitalizes on uncertainty to better learn seizure characteristics from scalp EEG. Thus, BUNDL offers a straightforward and model-agnostic method for training deep neural networks with noisy training labels that does not add any parameters to existing architectures. Additionally, we explore the impact of improved detection system on the task of automated onset zone localization. We validate BUNDL using a comprehensive simulated EEG dataset and two publicly available datasets collected by Temple University Hospital (TUH) and Boston Children's Hospital (CHB-MIT). Results show that BUNDL consistently identifies noisy labels and improves the robustness of three base models under various label noise conditions. We also evaluate cross-site generalizability and quantify computational cost of all methods. Ultimately, BUNDL presents as a reliable method that can be seamlessly integrated with existing deep models used in clinical practice, enabling the training of trustworthy models for epilepsy evaluation.", "link": "http://arxiv.org/abs/2410.19815v2", "date": "2026-01-05", "relevancy": 2.0999, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6121}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.509}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20uncertainty-aware%20deep%20learning%20with%20noisy%20labels%3A%20Tackling%20annotation%20ambiguity%20in%20EEG%20seizure%20detection&body=Title%3A%20Bayesian%20uncertainty-aware%20deep%20learning%20with%20noisy%20labels%3A%20Tackling%20annotation%20ambiguity%20in%20EEG%20seizure%20detection%0AAuthor%3A%20Deeksha%20M.%20Shama%20and%20Archana%20Venkataraman%0AAbstract%3A%20Deep%20learning%20is%20advancing%20EEG%20processing%20for%20automated%20epileptic%20seizure%20detection%20and%20onset%20zone%20localization%2C%20yet%20its%20performance%20relies%20heavily%20on%20high-quality%20annotated%20training%20data.%20However%2C%20scalp%20EEG%20is%20susceptible%20to%20high%20noise%20levels%2C%20which%20in%20turn%20leads%20to%20imprecise%20annotations%20of%20the%20seizure%20timing%20and%20characteristics.%20This%20%22label%20noise%22%20presents%20a%20significant%20challenge%20in%20model%20training%20and%20generalization.%20In%20this%20paper%2C%20we%20introduce%20Bayesian%20UncertaiNty-aware%20Deep%20Learning%20%28BUNDL%29%2C%20a%20novel%20algorithm%20that%20informs%20a%20deep%20learning%20model%20of%20label%20ambiguities%2C%20thereby%20enhancing%20the%20robustness%20of%20seizure%20detection%20systems.%20By%20integrating%20domain%20knowledge%20into%20an%20underlying%20Bayesian%20framework%2C%20we%20derive%20a%20novel%20KL-divergence-based%20loss%20function%20that%20capitalizes%20on%20uncertainty%20to%20better%20learn%20seizure%20characteristics%20from%20scalp%20EEG.%20Thus%2C%20BUNDL%20offers%20a%20straightforward%20and%20model-agnostic%20method%20for%20training%20deep%20neural%20networks%20with%20noisy%20training%20labels%20that%20does%20not%20add%20any%20parameters%20to%20existing%20architectures.%20Additionally%2C%20we%20explore%20the%20impact%20of%20improved%20detection%20system%20on%20the%20task%20of%20automated%20onset%20zone%20localization.%20We%20validate%20BUNDL%20using%20a%20comprehensive%20simulated%20EEG%20dataset%20and%20two%20publicly%20available%20datasets%20collected%20by%20Temple%20University%20Hospital%20%28TUH%29%20and%20Boston%20Children%27s%20Hospital%20%28CHB-MIT%29.%20Results%20show%20that%20BUNDL%20consistently%20identifies%20noisy%20labels%20and%20improves%20the%20robustness%20of%20three%20base%20models%20under%20various%20label%20noise%20conditions.%20We%20also%20evaluate%20cross-site%20generalizability%20and%20quantify%20computational%20cost%20of%20all%20methods.%20Ultimately%2C%20BUNDL%20presents%20as%20a%20reliable%20method%20that%20can%20be%20seamlessly%20integrated%20with%20existing%20deep%20models%20used%20in%20clinical%20practice%2C%20enabling%20the%20training%20of%20trustworthy%20models%20for%20epilepsy%20evaluation.%0ALink%3A%20http%3A//arxiv.org/abs/2410.19815v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520uncertainty-aware%2520deep%2520learning%2520with%2520noisy%2520labels%253A%2520Tackling%2520annotation%2520ambiguity%2520in%2520EEG%2520seizure%2520detection%26entry.906535625%3DDeeksha%2520M.%2520Shama%2520and%2520Archana%2520Venkataraman%26entry.1292438233%3DDeep%2520learning%2520is%2520advancing%2520EEG%2520processing%2520for%2520automated%2520epileptic%2520seizure%2520detection%2520and%2520onset%2520zone%2520localization%252C%2520yet%2520its%2520performance%2520relies%2520heavily%2520on%2520high-quality%2520annotated%2520training%2520data.%2520However%252C%2520scalp%2520EEG%2520is%2520susceptible%2520to%2520high%2520noise%2520levels%252C%2520which%2520in%2520turn%2520leads%2520to%2520imprecise%2520annotations%2520of%2520the%2520seizure%2520timing%2520and%2520characteristics.%2520This%2520%2522label%2520noise%2522%2520presents%2520a%2520significant%2520challenge%2520in%2520model%2520training%2520and%2520generalization.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Bayesian%2520UncertaiNty-aware%2520Deep%2520Learning%2520%2528BUNDL%2529%252C%2520a%2520novel%2520algorithm%2520that%2520informs%2520a%2520deep%2520learning%2520model%2520of%2520label%2520ambiguities%252C%2520thereby%2520enhancing%2520the%2520robustness%2520of%2520seizure%2520detection%2520systems.%2520By%2520integrating%2520domain%2520knowledge%2520into%2520an%2520underlying%2520Bayesian%2520framework%252C%2520we%2520derive%2520a%2520novel%2520KL-divergence-based%2520loss%2520function%2520that%2520capitalizes%2520on%2520uncertainty%2520to%2520better%2520learn%2520seizure%2520characteristics%2520from%2520scalp%2520EEG.%2520Thus%252C%2520BUNDL%2520offers%2520a%2520straightforward%2520and%2520model-agnostic%2520method%2520for%2520training%2520deep%2520neural%2520networks%2520with%2520noisy%2520training%2520labels%2520that%2520does%2520not%2520add%2520any%2520parameters%2520to%2520existing%2520architectures.%2520Additionally%252C%2520we%2520explore%2520the%2520impact%2520of%2520improved%2520detection%2520system%2520on%2520the%2520task%2520of%2520automated%2520onset%2520zone%2520localization.%2520We%2520validate%2520BUNDL%2520using%2520a%2520comprehensive%2520simulated%2520EEG%2520dataset%2520and%2520two%2520publicly%2520available%2520datasets%2520collected%2520by%2520Temple%2520University%2520Hospital%2520%2528TUH%2529%2520and%2520Boston%2520Children%2527s%2520Hospital%2520%2528CHB-MIT%2529.%2520Results%2520show%2520that%2520BUNDL%2520consistently%2520identifies%2520noisy%2520labels%2520and%2520improves%2520the%2520robustness%2520of%2520three%2520base%2520models%2520under%2520various%2520label%2520noise%2520conditions.%2520We%2520also%2520evaluate%2520cross-site%2520generalizability%2520and%2520quantify%2520computational%2520cost%2520of%2520all%2520methods.%2520Ultimately%252C%2520BUNDL%2520presents%2520as%2520a%2520reliable%2520method%2520that%2520can%2520be%2520seamlessly%2520integrated%2520with%2520existing%2520deep%2520models%2520used%2520in%2520clinical%2520practice%252C%2520enabling%2520the%2520training%2520of%2520trustworthy%2520models%2520for%2520epilepsy%2520evaluation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19815v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20uncertainty-aware%20deep%20learning%20with%20noisy%20labels%3A%20Tackling%20annotation%20ambiguity%20in%20EEG%20seizure%20detection&entry.906535625=Deeksha%20M.%20Shama%20and%20Archana%20Venkataraman&entry.1292438233=Deep%20learning%20is%20advancing%20EEG%20processing%20for%20automated%20epileptic%20seizure%20detection%20and%20onset%20zone%20localization%2C%20yet%20its%20performance%20relies%20heavily%20on%20high-quality%20annotated%20training%20data.%20However%2C%20scalp%20EEG%20is%20susceptible%20to%20high%20noise%20levels%2C%20which%20in%20turn%20leads%20to%20imprecise%20annotations%20of%20the%20seizure%20timing%20and%20characteristics.%20This%20%22label%20noise%22%20presents%20a%20significant%20challenge%20in%20model%20training%20and%20generalization.%20In%20this%20paper%2C%20we%20introduce%20Bayesian%20UncertaiNty-aware%20Deep%20Learning%20%28BUNDL%29%2C%20a%20novel%20algorithm%20that%20informs%20a%20deep%20learning%20model%20of%20label%20ambiguities%2C%20thereby%20enhancing%20the%20robustness%20of%20seizure%20detection%20systems.%20By%20integrating%20domain%20knowledge%20into%20an%20underlying%20Bayesian%20framework%2C%20we%20derive%20a%20novel%20KL-divergence-based%20loss%20function%20that%20capitalizes%20on%20uncertainty%20to%20better%20learn%20seizure%20characteristics%20from%20scalp%20EEG.%20Thus%2C%20BUNDL%20offers%20a%20straightforward%20and%20model-agnostic%20method%20for%20training%20deep%20neural%20networks%20with%20noisy%20training%20labels%20that%20does%20not%20add%20any%20parameters%20to%20existing%20architectures.%20Additionally%2C%20we%20explore%20the%20impact%20of%20improved%20detection%20system%20on%20the%20task%20of%20automated%20onset%20zone%20localization.%20We%20validate%20BUNDL%20using%20a%20comprehensive%20simulated%20EEG%20dataset%20and%20two%20publicly%20available%20datasets%20collected%20by%20Temple%20University%20Hospital%20%28TUH%29%20and%20Boston%20Children%27s%20Hospital%20%28CHB-MIT%29.%20Results%20show%20that%20BUNDL%20consistently%20identifies%20noisy%20labels%20and%20improves%20the%20robustness%20of%20three%20base%20models%20under%20various%20label%20noise%20conditions.%20We%20also%20evaluate%20cross-site%20generalizability%20and%20quantify%20computational%20cost%20of%20all%20methods.%20Ultimately%2C%20BUNDL%20presents%20as%20a%20reliable%20method%20that%20can%20be%20seamlessly%20integrated%20with%20existing%20deep%20models%20used%20in%20clinical%20practice%2C%20enabling%20the%20training%20of%20trustworthy%20models%20for%20epilepsy%20evaluation.&entry.1838667208=http%3A//arxiv.org/abs/2410.19815v2&entry.124074799=Read"},
{"title": "Prior-Guided DETR for Ultrasound Nodule Detection", "author": "Jingjing Wang and Zhuo Xiao and Xinning Yao and Bo Liu and Lijuan Niu and Xiangzhi Bai and Fugen Zhou", "abstract": "Accurate detection of ultrasound nodules is essential for the early diagnosis and treatment of thyroid and breast cancers. However, this task remains challenging due to irregular nodule shapes, indistinct boundaries, substantial scale variations, and the presence of speckle noise that degrades structural visibility. To address these challenges, we propose a prior-guided DETR framework specifically designed for ultrasound nodule detection. Instead of relying on purely data-driven feature learning, the proposed framework progressively incorporates different prior knowledge at multiple stages of the network. First, a Spatially-adaptive Deformable FFN with Prior Regularization (SDFPR) is embedded into the CNN backbone to inject geometric priors into deformable sampling, stabilizing feature extraction for irregular and blurred nodules. Second, a Multi-scale Spatial-Frequency Feature Mixer (MSFFM) is designed to extract multi-scale structural priors, where spatial-domain processing emphasizes contour continuity and boundary cues, while frequency-domain modeling captures global morphology and suppresses speckle noise. Furthermore, a Dense Feature Interaction (DFI) mechanism propagates and exploits these prior-modulated features across all encoder layers, enabling the decoder to enhance query refinement under consistent geometric and structural guidance. Experiments conducted on two clinically collected thyroid ultrasound datasets (Thyroid I and Thyroid II) and two public benchmarks (TN3K and BUSI) for thyroid and breast nodules demonstrate that the proposed method achieves superior accuracy compared with 18 detection methods, particularly in detecting morphologically complex nodules.The source code is publicly available at https://github.com/wjj1wjj/Ultrasound-DETR.", "link": "http://arxiv.org/abs/2601.02212v1", "date": "2026-01-05", "relevancy": 2.0999, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5379}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5239}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5209}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prior-Guided%20DETR%20for%20Ultrasound%20Nodule%20Detection&body=Title%3A%20Prior-Guided%20DETR%20for%20Ultrasound%20Nodule%20Detection%0AAuthor%3A%20Jingjing%20Wang%20and%20Zhuo%20Xiao%20and%20Xinning%20Yao%20and%20Bo%20Liu%20and%20Lijuan%20Niu%20and%20Xiangzhi%20Bai%20and%20Fugen%20Zhou%0AAbstract%3A%20Accurate%20detection%20of%20ultrasound%20nodules%20is%20essential%20for%20the%20early%20diagnosis%20and%20treatment%20of%20thyroid%20and%20breast%20cancers.%20However%2C%20this%20task%20remains%20challenging%20due%20to%20irregular%20nodule%20shapes%2C%20indistinct%20boundaries%2C%20substantial%20scale%20variations%2C%20and%20the%20presence%20of%20speckle%20noise%20that%20degrades%20structural%20visibility.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20prior-guided%20DETR%20framework%20specifically%20designed%20for%20ultrasound%20nodule%20detection.%20Instead%20of%20relying%20on%20purely%20data-driven%20feature%20learning%2C%20the%20proposed%20framework%20progressively%20incorporates%20different%20prior%20knowledge%20at%20multiple%20stages%20of%20the%20network.%20First%2C%20a%20Spatially-adaptive%20Deformable%20FFN%20with%20Prior%20Regularization%20%28SDFPR%29%20is%20embedded%20into%20the%20CNN%20backbone%20to%20inject%20geometric%20priors%20into%20deformable%20sampling%2C%20stabilizing%20feature%20extraction%20for%20irregular%20and%20blurred%20nodules.%20Second%2C%20a%20Multi-scale%20Spatial-Frequency%20Feature%20Mixer%20%28MSFFM%29%20is%20designed%20to%20extract%20multi-scale%20structural%20priors%2C%20where%20spatial-domain%20processing%20emphasizes%20contour%20continuity%20and%20boundary%20cues%2C%20while%20frequency-domain%20modeling%20captures%20global%20morphology%20and%20suppresses%20speckle%20noise.%20Furthermore%2C%20a%20Dense%20Feature%20Interaction%20%28DFI%29%20mechanism%20propagates%20and%20exploits%20these%20prior-modulated%20features%20across%20all%20encoder%20layers%2C%20enabling%20the%20decoder%20to%20enhance%20query%20refinement%20under%20consistent%20geometric%20and%20structural%20guidance.%20Experiments%20conducted%20on%20two%20clinically%20collected%20thyroid%20ultrasound%20datasets%20%28Thyroid%20I%20and%20Thyroid%20II%29%20and%20two%20public%20benchmarks%20%28TN3K%20and%20BUSI%29%20for%20thyroid%20and%20breast%20nodules%20demonstrate%20that%20the%20proposed%20method%20achieves%20superior%20accuracy%20compared%20with%2018%20detection%20methods%2C%20particularly%20in%20detecting%20morphologically%20complex%20nodules.The%20source%20code%20is%20publicly%20available%20at%20https%3A//github.com/wjj1wjj/Ultrasound-DETR.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02212v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrior-Guided%2520DETR%2520for%2520Ultrasound%2520Nodule%2520Detection%26entry.906535625%3DJingjing%2520Wang%2520and%2520Zhuo%2520Xiao%2520and%2520Xinning%2520Yao%2520and%2520Bo%2520Liu%2520and%2520Lijuan%2520Niu%2520and%2520Xiangzhi%2520Bai%2520and%2520Fugen%2520Zhou%26entry.1292438233%3DAccurate%2520detection%2520of%2520ultrasound%2520nodules%2520is%2520essential%2520for%2520the%2520early%2520diagnosis%2520and%2520treatment%2520of%2520thyroid%2520and%2520breast%2520cancers.%2520However%252C%2520this%2520task%2520remains%2520challenging%2520due%2520to%2520irregular%2520nodule%2520shapes%252C%2520indistinct%2520boundaries%252C%2520substantial%2520scale%2520variations%252C%2520and%2520the%2520presence%2520of%2520speckle%2520noise%2520that%2520degrades%2520structural%2520visibility.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520prior-guided%2520DETR%2520framework%2520specifically%2520designed%2520for%2520ultrasound%2520nodule%2520detection.%2520Instead%2520of%2520relying%2520on%2520purely%2520data-driven%2520feature%2520learning%252C%2520the%2520proposed%2520framework%2520progressively%2520incorporates%2520different%2520prior%2520knowledge%2520at%2520multiple%2520stages%2520of%2520the%2520network.%2520First%252C%2520a%2520Spatially-adaptive%2520Deformable%2520FFN%2520with%2520Prior%2520Regularization%2520%2528SDFPR%2529%2520is%2520embedded%2520into%2520the%2520CNN%2520backbone%2520to%2520inject%2520geometric%2520priors%2520into%2520deformable%2520sampling%252C%2520stabilizing%2520feature%2520extraction%2520for%2520irregular%2520and%2520blurred%2520nodules.%2520Second%252C%2520a%2520Multi-scale%2520Spatial-Frequency%2520Feature%2520Mixer%2520%2528MSFFM%2529%2520is%2520designed%2520to%2520extract%2520multi-scale%2520structural%2520priors%252C%2520where%2520spatial-domain%2520processing%2520emphasizes%2520contour%2520continuity%2520and%2520boundary%2520cues%252C%2520while%2520frequency-domain%2520modeling%2520captures%2520global%2520morphology%2520and%2520suppresses%2520speckle%2520noise.%2520Furthermore%252C%2520a%2520Dense%2520Feature%2520Interaction%2520%2528DFI%2529%2520mechanism%2520propagates%2520and%2520exploits%2520these%2520prior-modulated%2520features%2520across%2520all%2520encoder%2520layers%252C%2520enabling%2520the%2520decoder%2520to%2520enhance%2520query%2520refinement%2520under%2520consistent%2520geometric%2520and%2520structural%2520guidance.%2520Experiments%2520conducted%2520on%2520two%2520clinically%2520collected%2520thyroid%2520ultrasound%2520datasets%2520%2528Thyroid%2520I%2520and%2520Thyroid%2520II%2529%2520and%2520two%2520public%2520benchmarks%2520%2528TN3K%2520and%2520BUSI%2529%2520for%2520thyroid%2520and%2520breast%2520nodules%2520demonstrate%2520that%2520the%2520proposed%2520method%2520achieves%2520superior%2520accuracy%2520compared%2520with%252018%2520detection%2520methods%252C%2520particularly%2520in%2520detecting%2520morphologically%2520complex%2520nodules.The%2520source%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/wjj1wjj/Ultrasound-DETR.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02212v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prior-Guided%20DETR%20for%20Ultrasound%20Nodule%20Detection&entry.906535625=Jingjing%20Wang%20and%20Zhuo%20Xiao%20and%20Xinning%20Yao%20and%20Bo%20Liu%20and%20Lijuan%20Niu%20and%20Xiangzhi%20Bai%20and%20Fugen%20Zhou&entry.1292438233=Accurate%20detection%20of%20ultrasound%20nodules%20is%20essential%20for%20the%20early%20diagnosis%20and%20treatment%20of%20thyroid%20and%20breast%20cancers.%20However%2C%20this%20task%20remains%20challenging%20due%20to%20irregular%20nodule%20shapes%2C%20indistinct%20boundaries%2C%20substantial%20scale%20variations%2C%20and%20the%20presence%20of%20speckle%20noise%20that%20degrades%20structural%20visibility.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20prior-guided%20DETR%20framework%20specifically%20designed%20for%20ultrasound%20nodule%20detection.%20Instead%20of%20relying%20on%20purely%20data-driven%20feature%20learning%2C%20the%20proposed%20framework%20progressively%20incorporates%20different%20prior%20knowledge%20at%20multiple%20stages%20of%20the%20network.%20First%2C%20a%20Spatially-adaptive%20Deformable%20FFN%20with%20Prior%20Regularization%20%28SDFPR%29%20is%20embedded%20into%20the%20CNN%20backbone%20to%20inject%20geometric%20priors%20into%20deformable%20sampling%2C%20stabilizing%20feature%20extraction%20for%20irregular%20and%20blurred%20nodules.%20Second%2C%20a%20Multi-scale%20Spatial-Frequency%20Feature%20Mixer%20%28MSFFM%29%20is%20designed%20to%20extract%20multi-scale%20structural%20priors%2C%20where%20spatial-domain%20processing%20emphasizes%20contour%20continuity%20and%20boundary%20cues%2C%20while%20frequency-domain%20modeling%20captures%20global%20morphology%20and%20suppresses%20speckle%20noise.%20Furthermore%2C%20a%20Dense%20Feature%20Interaction%20%28DFI%29%20mechanism%20propagates%20and%20exploits%20these%20prior-modulated%20features%20across%20all%20encoder%20layers%2C%20enabling%20the%20decoder%20to%20enhance%20query%20refinement%20under%20consistent%20geometric%20and%20structural%20guidance.%20Experiments%20conducted%20on%20two%20clinically%20collected%20thyroid%20ultrasound%20datasets%20%28Thyroid%20I%20and%20Thyroid%20II%29%20and%20two%20public%20benchmarks%20%28TN3K%20and%20BUSI%29%20for%20thyroid%20and%20breast%20nodules%20demonstrate%20that%20the%20proposed%20method%20achieves%20superior%20accuracy%20compared%20with%2018%20detection%20methods%2C%20particularly%20in%20detecting%20morphologically%20complex%20nodules.The%20source%20code%20is%20publicly%20available%20at%20https%3A//github.com/wjj1wjj/Ultrasound-DETR.&entry.1838667208=http%3A//arxiv.org/abs/2601.02212v1&entry.124074799=Read"},
{"title": "Not All Needles Are Found: How Fact Distribution and Don't Make It Up Prompts Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context LLMs", "author": "Amirali Ebrahimzadeh and Seyyed M. Salili", "abstract": "Large language models (LLMs) increasingly support very long input contexts. Yet it remains unclear how reliably they extract and infer information at scale. Performance varies with context length and strongly interacts with how information is distributed in real-world corpora. Motivated by these observations, we study how fact placement, corpus-level fact distributions, and Don't Make It Up prompts influence model behavior. We introduce an extended needle-in-a-haystack benchmark across four production-scale models: Gemini-2.5-flash, ChatGPT-5-mini, Claude-4.5-haiku, and Deepseek-v3.2-chat. Unlike prior work, we separately evaluate literal extraction, logical inference, and hallucination risk. Our study considers both positional effects and realistic distributions of evidence across long contexts, as well as prompts that explicitly discourage fabrication. We find that longer contexts alone do not guarantee better performance and can be detrimental when relevant evidence is diluted or widely dispersed. Performance varies substantially across models: some show severe degradation under realistic conditions, while others remain more robust at longer context lengths. Anti-hallucination (AH) instructions can make some models overly conservative, sharply reducing accuracy in literal extraction and logical inference. While we do not directly compare retrieval-augmented generation (RAG) and cache-augmented generation (CAG), our results suggest many failures stem from ineffective context utilization. Models often struggle to identify and prioritize relevant information even when it is present. These findings have direct practical implications, as enterprise workflows increasingly involve pasting large volumes of unfiltered documents into LLM prompts. Effective context length and model-specific robustness to long contexts are therefore critical for reliable LLM deployment in research and business.", "link": "http://arxiv.org/abs/2601.02023v1", "date": "2026-01-05", "relevancy": 2.0991, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.529}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.529}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Not%20All%20Needles%20Are%20Found%3A%20How%20Fact%20Distribution%20and%20Don%27t%20Make%20It%20Up%20Prompts%20Shape%20Literal%20Extraction%2C%20Logical%20Inference%2C%20and%20Hallucination%20Risks%20in%20Long-Context%20LLMs&body=Title%3A%20Not%20All%20Needles%20Are%20Found%3A%20How%20Fact%20Distribution%20and%20Don%27t%20Make%20It%20Up%20Prompts%20Shape%20Literal%20Extraction%2C%20Logical%20Inference%2C%20and%20Hallucination%20Risks%20in%20Long-Context%20LLMs%0AAuthor%3A%20Amirali%20Ebrahimzadeh%20and%20Seyyed%20M.%20Salili%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20increasingly%20support%20very%20long%20input%20contexts.%20Yet%20it%20remains%20unclear%20how%20reliably%20they%20extract%20and%20infer%20information%20at%20scale.%20Performance%20varies%20with%20context%20length%20and%20strongly%20interacts%20with%20how%20information%20is%20distributed%20in%20real-world%20corpora.%20Motivated%20by%20these%20observations%2C%20we%20study%20how%20fact%20placement%2C%20corpus-level%20fact%20distributions%2C%20and%20Don%27t%20Make%20It%20Up%20prompts%20influence%20model%20behavior.%20We%20introduce%20an%20extended%20needle-in-a-haystack%20benchmark%20across%20four%20production-scale%20models%3A%20Gemini-2.5-flash%2C%20ChatGPT-5-mini%2C%20Claude-4.5-haiku%2C%20and%20Deepseek-v3.2-chat.%20Unlike%20prior%20work%2C%20we%20separately%20evaluate%20literal%20extraction%2C%20logical%20inference%2C%20and%20hallucination%20risk.%20Our%20study%20considers%20both%20positional%20effects%20and%20realistic%20distributions%20of%20evidence%20across%20long%20contexts%2C%20as%20well%20as%20prompts%20that%20explicitly%20discourage%20fabrication.%20We%20find%20that%20longer%20contexts%20alone%20do%20not%20guarantee%20better%20performance%20and%20can%20be%20detrimental%20when%20relevant%20evidence%20is%20diluted%20or%20widely%20dispersed.%20Performance%20varies%20substantially%20across%20models%3A%20some%20show%20severe%20degradation%20under%20realistic%20conditions%2C%20while%20others%20remain%20more%20robust%20at%20longer%20context%20lengths.%20Anti-hallucination%20%28AH%29%20instructions%20can%20make%20some%20models%20overly%20conservative%2C%20sharply%20reducing%20accuracy%20in%20literal%20extraction%20and%20logical%20inference.%20While%20we%20do%20not%20directly%20compare%20retrieval-augmented%20generation%20%28RAG%29%20and%20cache-augmented%20generation%20%28CAG%29%2C%20our%20results%20suggest%20many%20failures%20stem%20from%20ineffective%20context%20utilization.%20Models%20often%20struggle%20to%20identify%20and%20prioritize%20relevant%20information%20even%20when%20it%20is%20present.%20These%20findings%20have%20direct%20practical%20implications%2C%20as%20enterprise%20workflows%20increasingly%20involve%20pasting%20large%20volumes%20of%20unfiltered%20documents%20into%20LLM%20prompts.%20Effective%20context%20length%20and%20model-specific%20robustness%20to%20long%20contexts%20are%20therefore%20critical%20for%20reliable%20LLM%20deployment%20in%20research%20and%20business.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02023v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNot%2520All%2520Needles%2520Are%2520Found%253A%2520How%2520Fact%2520Distribution%2520and%2520Don%2527t%2520Make%2520It%2520Up%2520Prompts%2520Shape%2520Literal%2520Extraction%252C%2520Logical%2520Inference%252C%2520and%2520Hallucination%2520Risks%2520in%2520Long-Context%2520LLMs%26entry.906535625%3DAmirali%2520Ebrahimzadeh%2520and%2520Seyyed%2520M.%2520Salili%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520increasingly%2520support%2520very%2520long%2520input%2520contexts.%2520Yet%2520it%2520remains%2520unclear%2520how%2520reliably%2520they%2520extract%2520and%2520infer%2520information%2520at%2520scale.%2520Performance%2520varies%2520with%2520context%2520length%2520and%2520strongly%2520interacts%2520with%2520how%2520information%2520is%2520distributed%2520in%2520real-world%2520corpora.%2520Motivated%2520by%2520these%2520observations%252C%2520we%2520study%2520how%2520fact%2520placement%252C%2520corpus-level%2520fact%2520distributions%252C%2520and%2520Don%2527t%2520Make%2520It%2520Up%2520prompts%2520influence%2520model%2520behavior.%2520We%2520introduce%2520an%2520extended%2520needle-in-a-haystack%2520benchmark%2520across%2520four%2520production-scale%2520models%253A%2520Gemini-2.5-flash%252C%2520ChatGPT-5-mini%252C%2520Claude-4.5-haiku%252C%2520and%2520Deepseek-v3.2-chat.%2520Unlike%2520prior%2520work%252C%2520we%2520separately%2520evaluate%2520literal%2520extraction%252C%2520logical%2520inference%252C%2520and%2520hallucination%2520risk.%2520Our%2520study%2520considers%2520both%2520positional%2520effects%2520and%2520realistic%2520distributions%2520of%2520evidence%2520across%2520long%2520contexts%252C%2520as%2520well%2520as%2520prompts%2520that%2520explicitly%2520discourage%2520fabrication.%2520We%2520find%2520that%2520longer%2520contexts%2520alone%2520do%2520not%2520guarantee%2520better%2520performance%2520and%2520can%2520be%2520detrimental%2520when%2520relevant%2520evidence%2520is%2520diluted%2520or%2520widely%2520dispersed.%2520Performance%2520varies%2520substantially%2520across%2520models%253A%2520some%2520show%2520severe%2520degradation%2520under%2520realistic%2520conditions%252C%2520while%2520others%2520remain%2520more%2520robust%2520at%2520longer%2520context%2520lengths.%2520Anti-hallucination%2520%2528AH%2529%2520instructions%2520can%2520make%2520some%2520models%2520overly%2520conservative%252C%2520sharply%2520reducing%2520accuracy%2520in%2520literal%2520extraction%2520and%2520logical%2520inference.%2520While%2520we%2520do%2520not%2520directly%2520compare%2520retrieval-augmented%2520generation%2520%2528RAG%2529%2520and%2520cache-augmented%2520generation%2520%2528CAG%2529%252C%2520our%2520results%2520suggest%2520many%2520failures%2520stem%2520from%2520ineffective%2520context%2520utilization.%2520Models%2520often%2520struggle%2520to%2520identify%2520and%2520prioritize%2520relevant%2520information%2520even%2520when%2520it%2520is%2520present.%2520These%2520findings%2520have%2520direct%2520practical%2520implications%252C%2520as%2520enterprise%2520workflows%2520increasingly%2520involve%2520pasting%2520large%2520volumes%2520of%2520unfiltered%2520documents%2520into%2520LLM%2520prompts.%2520Effective%2520context%2520length%2520and%2520model-specific%2520robustness%2520to%2520long%2520contexts%2520are%2520therefore%2520critical%2520for%2520reliable%2520LLM%2520deployment%2520in%2520research%2520and%2520business.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02023v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Not%20All%20Needles%20Are%20Found%3A%20How%20Fact%20Distribution%20and%20Don%27t%20Make%20It%20Up%20Prompts%20Shape%20Literal%20Extraction%2C%20Logical%20Inference%2C%20and%20Hallucination%20Risks%20in%20Long-Context%20LLMs&entry.906535625=Amirali%20Ebrahimzadeh%20and%20Seyyed%20M.%20Salili&entry.1292438233=Large%20language%20models%20%28LLMs%29%20increasingly%20support%20very%20long%20input%20contexts.%20Yet%20it%20remains%20unclear%20how%20reliably%20they%20extract%20and%20infer%20information%20at%20scale.%20Performance%20varies%20with%20context%20length%20and%20strongly%20interacts%20with%20how%20information%20is%20distributed%20in%20real-world%20corpora.%20Motivated%20by%20these%20observations%2C%20we%20study%20how%20fact%20placement%2C%20corpus-level%20fact%20distributions%2C%20and%20Don%27t%20Make%20It%20Up%20prompts%20influence%20model%20behavior.%20We%20introduce%20an%20extended%20needle-in-a-haystack%20benchmark%20across%20four%20production-scale%20models%3A%20Gemini-2.5-flash%2C%20ChatGPT-5-mini%2C%20Claude-4.5-haiku%2C%20and%20Deepseek-v3.2-chat.%20Unlike%20prior%20work%2C%20we%20separately%20evaluate%20literal%20extraction%2C%20logical%20inference%2C%20and%20hallucination%20risk.%20Our%20study%20considers%20both%20positional%20effects%20and%20realistic%20distributions%20of%20evidence%20across%20long%20contexts%2C%20as%20well%20as%20prompts%20that%20explicitly%20discourage%20fabrication.%20We%20find%20that%20longer%20contexts%20alone%20do%20not%20guarantee%20better%20performance%20and%20can%20be%20detrimental%20when%20relevant%20evidence%20is%20diluted%20or%20widely%20dispersed.%20Performance%20varies%20substantially%20across%20models%3A%20some%20show%20severe%20degradation%20under%20realistic%20conditions%2C%20while%20others%20remain%20more%20robust%20at%20longer%20context%20lengths.%20Anti-hallucination%20%28AH%29%20instructions%20can%20make%20some%20models%20overly%20conservative%2C%20sharply%20reducing%20accuracy%20in%20literal%20extraction%20and%20logical%20inference.%20While%20we%20do%20not%20directly%20compare%20retrieval-augmented%20generation%20%28RAG%29%20and%20cache-augmented%20generation%20%28CAG%29%2C%20our%20results%20suggest%20many%20failures%20stem%20from%20ineffective%20context%20utilization.%20Models%20often%20struggle%20to%20identify%20and%20prioritize%20relevant%20information%20even%20when%20it%20is%20present.%20These%20findings%20have%20direct%20practical%20implications%2C%20as%20enterprise%20workflows%20increasingly%20involve%20pasting%20large%20volumes%20of%20unfiltered%20documents%20into%20LLM%20prompts.%20Effective%20context%20length%20and%20model-specific%20robustness%20to%20long%20contexts%20are%20therefore%20critical%20for%20reliable%20LLM%20deployment%20in%20research%20and%20business.&entry.1838667208=http%3A//arxiv.org/abs/2601.02023v1&entry.124074799=Read"},
{"title": "v-PuNNs: van der Put Neural Networks for Transparent Ultrametric Representation Learning", "author": "Gnankan Landry Regis N'guessan", "abstract": "Conventional deep learning models embed data in Euclidean space $\\mathbb{R}^d$, a poor fit for strictly hierarchical objects such as taxa, word senses, or file systems. We introduce van der Put Neural Networks (v-PuNNs), the first architecture whose neurons are characteristic functions of p-adic balls in $\\mathbb{Z}_p$. Under our Transparent Ultrametric Representation Learning (TURL) principle every weight is itself a p-adic number, giving exact subtree semantics. A new Finite Hierarchical Approximation Theorem shows that a depth-K v-PuNN with $\\sum_{j=0}^{K-1}p^{\\,j}$ neurons universally represents any K-level tree. Because gradients vanish in this discrete space, we propose Valuation-Adaptive Perturbation Optimization (VAPO), with a fast deterministic variant (HiPaN-DS) and a moment-based one (HiPaN / Adam-VAPO). On three canonical benchmarks our CPU-only implementation sets new state-of-the-art: WordNet nouns (52,427 leaves) 99.96% leaf accuracy in 16 min; GO molecular-function 96.9% leaf / 100% root in 50 s; NCBI Mammalia Spearman $\u03c1= -0.96$ with true taxonomic distance. The learned metric is perfectly ultrametric (zero triangle violations), and its fractal and information-theoretic properties are analyzed. Beyond classification we derive structural invariants for quantum systems (HiPaQ) and controllable generative codes for tabular data (Tab-HiPaN). v-PuNNs therefore bridge number theory and deep learning, offering exact, interpretable, and efficient models for hierarchical data.", "link": "http://arxiv.org/abs/2508.01010v2", "date": "2026-01-05", "relevancy": 2.099, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5361}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5266}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20v-PuNNs%3A%20van%20der%20Put%20Neural%20Networks%20for%20Transparent%20Ultrametric%20Representation%20Learning&body=Title%3A%20v-PuNNs%3A%20van%20der%20Put%20Neural%20Networks%20for%20Transparent%20Ultrametric%20Representation%20Learning%0AAuthor%3A%20Gnankan%20Landry%20Regis%20N%27guessan%0AAbstract%3A%20Conventional%20deep%20learning%20models%20embed%20data%20in%20Euclidean%20space%20%24%5Cmathbb%7BR%7D%5Ed%24%2C%20a%20poor%20fit%20for%20strictly%20hierarchical%20objects%20such%20as%20taxa%2C%20word%20senses%2C%20or%20file%20systems.%20We%20introduce%20van%20der%20Put%20Neural%20Networks%20%28v-PuNNs%29%2C%20the%20first%20architecture%20whose%20neurons%20are%20characteristic%20functions%20of%20p-adic%20balls%20in%20%24%5Cmathbb%7BZ%7D_p%24.%20Under%20our%20Transparent%20Ultrametric%20Representation%20Learning%20%28TURL%29%20principle%20every%20weight%20is%20itself%20a%20p-adic%20number%2C%20giving%20exact%20subtree%20semantics.%20A%20new%20Finite%20Hierarchical%20Approximation%20Theorem%20shows%20that%20a%20depth-K%20v-PuNN%20with%20%24%5Csum_%7Bj%3D0%7D%5E%7BK-1%7Dp%5E%7B%5C%2Cj%7D%24%20neurons%20universally%20represents%20any%20K-level%20tree.%20Because%20gradients%20vanish%20in%20this%20discrete%20space%2C%20we%20propose%20Valuation-Adaptive%20Perturbation%20Optimization%20%28VAPO%29%2C%20with%20a%20fast%20deterministic%20variant%20%28HiPaN-DS%29%20and%20a%20moment-based%20one%20%28HiPaN%20/%20Adam-VAPO%29.%20On%20three%20canonical%20benchmarks%20our%20CPU-only%20implementation%20sets%20new%20state-of-the-art%3A%20WordNet%20nouns%20%2852%2C427%20leaves%29%2099.96%25%20leaf%20accuracy%20in%2016%20min%3B%20GO%20molecular-function%2096.9%25%20leaf%20/%20100%25%20root%20in%2050%20s%3B%20NCBI%20Mammalia%20Spearman%20%24%CF%81%3D%20-0.96%24%20with%20true%20taxonomic%20distance.%20The%20learned%20metric%20is%20perfectly%20ultrametric%20%28zero%20triangle%20violations%29%2C%20and%20its%20fractal%20and%20information-theoretic%20properties%20are%20analyzed.%20Beyond%20classification%20we%20derive%20structural%20invariants%20for%20quantum%20systems%20%28HiPaQ%29%20and%20controllable%20generative%20codes%20for%20tabular%20data%20%28Tab-HiPaN%29.%20v-PuNNs%20therefore%20bridge%20number%20theory%20and%20deep%20learning%2C%20offering%20exact%2C%20interpretable%2C%20and%20efficient%20models%20for%20hierarchical%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2508.01010v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dv-PuNNs%253A%2520van%2520der%2520Put%2520Neural%2520Networks%2520for%2520Transparent%2520Ultrametric%2520Representation%2520Learning%26entry.906535625%3DGnankan%2520Landry%2520Regis%2520N%2527guessan%26entry.1292438233%3DConventional%2520deep%2520learning%2520models%2520embed%2520data%2520in%2520Euclidean%2520space%2520%2524%255Cmathbb%257BR%257D%255Ed%2524%252C%2520a%2520poor%2520fit%2520for%2520strictly%2520hierarchical%2520objects%2520such%2520as%2520taxa%252C%2520word%2520senses%252C%2520or%2520file%2520systems.%2520We%2520introduce%2520van%2520der%2520Put%2520Neural%2520Networks%2520%2528v-PuNNs%2529%252C%2520the%2520first%2520architecture%2520whose%2520neurons%2520are%2520characteristic%2520functions%2520of%2520p-adic%2520balls%2520in%2520%2524%255Cmathbb%257BZ%257D_p%2524.%2520Under%2520our%2520Transparent%2520Ultrametric%2520Representation%2520Learning%2520%2528TURL%2529%2520principle%2520every%2520weight%2520is%2520itself%2520a%2520p-adic%2520number%252C%2520giving%2520exact%2520subtree%2520semantics.%2520A%2520new%2520Finite%2520Hierarchical%2520Approximation%2520Theorem%2520shows%2520that%2520a%2520depth-K%2520v-PuNN%2520with%2520%2524%255Csum_%257Bj%253D0%257D%255E%257BK-1%257Dp%255E%257B%255C%252Cj%257D%2524%2520neurons%2520universally%2520represents%2520any%2520K-level%2520tree.%2520Because%2520gradients%2520vanish%2520in%2520this%2520discrete%2520space%252C%2520we%2520propose%2520Valuation-Adaptive%2520Perturbation%2520Optimization%2520%2528VAPO%2529%252C%2520with%2520a%2520fast%2520deterministic%2520variant%2520%2528HiPaN-DS%2529%2520and%2520a%2520moment-based%2520one%2520%2528HiPaN%2520/%2520Adam-VAPO%2529.%2520On%2520three%2520canonical%2520benchmarks%2520our%2520CPU-only%2520implementation%2520sets%2520new%2520state-of-the-art%253A%2520WordNet%2520nouns%2520%252852%252C427%2520leaves%2529%252099.96%2525%2520leaf%2520accuracy%2520in%252016%2520min%253B%2520GO%2520molecular-function%252096.9%2525%2520leaf%2520/%2520100%2525%2520root%2520in%252050%2520s%253B%2520NCBI%2520Mammalia%2520Spearman%2520%2524%25CF%2581%253D%2520-0.96%2524%2520with%2520true%2520taxonomic%2520distance.%2520The%2520learned%2520metric%2520is%2520perfectly%2520ultrametric%2520%2528zero%2520triangle%2520violations%2529%252C%2520and%2520its%2520fractal%2520and%2520information-theoretic%2520properties%2520are%2520analyzed.%2520Beyond%2520classification%2520we%2520derive%2520structural%2520invariants%2520for%2520quantum%2520systems%2520%2528HiPaQ%2529%2520and%2520controllable%2520generative%2520codes%2520for%2520tabular%2520data%2520%2528Tab-HiPaN%2529.%2520v-PuNNs%2520therefore%2520bridge%2520number%2520theory%2520and%2520deep%2520learning%252C%2520offering%2520exact%252C%2520interpretable%252C%2520and%2520efficient%2520models%2520for%2520hierarchical%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.01010v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=v-PuNNs%3A%20van%20der%20Put%20Neural%20Networks%20for%20Transparent%20Ultrametric%20Representation%20Learning&entry.906535625=Gnankan%20Landry%20Regis%20N%27guessan&entry.1292438233=Conventional%20deep%20learning%20models%20embed%20data%20in%20Euclidean%20space%20%24%5Cmathbb%7BR%7D%5Ed%24%2C%20a%20poor%20fit%20for%20strictly%20hierarchical%20objects%20such%20as%20taxa%2C%20word%20senses%2C%20or%20file%20systems.%20We%20introduce%20van%20der%20Put%20Neural%20Networks%20%28v-PuNNs%29%2C%20the%20first%20architecture%20whose%20neurons%20are%20characteristic%20functions%20of%20p-adic%20balls%20in%20%24%5Cmathbb%7BZ%7D_p%24.%20Under%20our%20Transparent%20Ultrametric%20Representation%20Learning%20%28TURL%29%20principle%20every%20weight%20is%20itself%20a%20p-adic%20number%2C%20giving%20exact%20subtree%20semantics.%20A%20new%20Finite%20Hierarchical%20Approximation%20Theorem%20shows%20that%20a%20depth-K%20v-PuNN%20with%20%24%5Csum_%7Bj%3D0%7D%5E%7BK-1%7Dp%5E%7B%5C%2Cj%7D%24%20neurons%20universally%20represents%20any%20K-level%20tree.%20Because%20gradients%20vanish%20in%20this%20discrete%20space%2C%20we%20propose%20Valuation-Adaptive%20Perturbation%20Optimization%20%28VAPO%29%2C%20with%20a%20fast%20deterministic%20variant%20%28HiPaN-DS%29%20and%20a%20moment-based%20one%20%28HiPaN%20/%20Adam-VAPO%29.%20On%20three%20canonical%20benchmarks%20our%20CPU-only%20implementation%20sets%20new%20state-of-the-art%3A%20WordNet%20nouns%20%2852%2C427%20leaves%29%2099.96%25%20leaf%20accuracy%20in%2016%20min%3B%20GO%20molecular-function%2096.9%25%20leaf%20/%20100%25%20root%20in%2050%20s%3B%20NCBI%20Mammalia%20Spearman%20%24%CF%81%3D%20-0.96%24%20with%20true%20taxonomic%20distance.%20The%20learned%20metric%20is%20perfectly%20ultrametric%20%28zero%20triangle%20violations%29%2C%20and%20its%20fractal%20and%20information-theoretic%20properties%20are%20analyzed.%20Beyond%20classification%20we%20derive%20structural%20invariants%20for%20quantum%20systems%20%28HiPaQ%29%20and%20controllable%20generative%20codes%20for%20tabular%20data%20%28Tab-HiPaN%29.%20v-PuNNs%20therefore%20bridge%20number%20theory%20and%20deep%20learning%2C%20offering%20exact%2C%20interpretable%2C%20and%20efficient%20models%20for%20hierarchical%20data.&entry.1838667208=http%3A//arxiv.org/abs/2508.01010v2&entry.124074799=Read"},
{"title": "Car Drag Coefficient Prediction from 3D Point Clouds Using a Slice-Based Surrogate Model", "author": "Utkarsh Singh and Absaar Ali and Adarsh Roy", "abstract": "The automotive industry's pursuit of enhanced fuel economy and performance necessitates efficient aerodynamic design. However, traditional evaluation methods such as computational fluid dynamics (CFD) and wind tunnel testing are resource intensive, hindering rapid iteration in the early design stages. Machine learning-based surrogate models offer a promising alternative, yet many existing approaches suffer from high computational complexity, limited interpretability, or insufficient accuracy for detailed geometric inputs. This paper introduces a novel lightweight surrogate model for the prediction of the aerodynamic drag coefficient (Cd) based on a sequential slice-wise processing of the geometry of the 3D vehicle. Inspired by medical imaging, 3D point clouds of vehicles are decomposed into an ordered sequence of 2D cross-sectional slices along the stream-wise axis. Each slice is encoded by a lightweight PointNet2D module, and the sequence of slice embeddings is processed by a bidirectional LSTM to capture longitudinal geometric evolution. The model, trained and evaluated on the DrivAerNet++ dataset, achieves a high coefficient of determination (R^2 > 0.9528) and a low mean absolute error (MAE approx 6.046 x 10^{-3}) in Cd prediction. With an inference time of approximately 0.025 seconds per sample on a consumer-grade GPU, our approach provides fast, accurate, and interpretable aerodynamic feedback, facilitating more agile and informed automotive design exploration.", "link": "http://arxiv.org/abs/2601.02112v1", "date": "2026-01-05", "relevancy": 2.0934, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5383}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5141}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Car%20Drag%20Coefficient%20Prediction%20from%203D%20Point%20Clouds%20Using%20a%20Slice-Based%20Surrogate%20Model&body=Title%3A%20Car%20Drag%20Coefficient%20Prediction%20from%203D%20Point%20Clouds%20Using%20a%20Slice-Based%20Surrogate%20Model%0AAuthor%3A%20Utkarsh%20Singh%20and%20Absaar%20Ali%20and%20Adarsh%20Roy%0AAbstract%3A%20The%20automotive%20industry%27s%20pursuit%20of%20enhanced%20fuel%20economy%20and%20performance%20necessitates%20efficient%20aerodynamic%20design.%20However%2C%20traditional%20evaluation%20methods%20such%20as%20computational%20fluid%20dynamics%20%28CFD%29%20and%20wind%20tunnel%20testing%20are%20resource%20intensive%2C%20hindering%20rapid%20iteration%20in%20the%20early%20design%20stages.%20Machine%20learning-based%20surrogate%20models%20offer%20a%20promising%20alternative%2C%20yet%20many%20existing%20approaches%20suffer%20from%20high%20computational%20complexity%2C%20limited%20interpretability%2C%20or%20insufficient%20accuracy%20for%20detailed%20geometric%20inputs.%20This%20paper%20introduces%20a%20novel%20lightweight%20surrogate%20model%20for%20the%20prediction%20of%20the%20aerodynamic%20drag%20coefficient%20%28Cd%29%20based%20on%20a%20sequential%20slice-wise%20processing%20of%20the%20geometry%20of%20the%203D%20vehicle.%20Inspired%20by%20medical%20imaging%2C%203D%20point%20clouds%20of%20vehicles%20are%20decomposed%20into%20an%20ordered%20sequence%20of%202D%20cross-sectional%20slices%20along%20the%20stream-wise%20axis.%20Each%20slice%20is%20encoded%20by%20a%20lightweight%20PointNet2D%20module%2C%20and%20the%20sequence%20of%20slice%20embeddings%20is%20processed%20by%20a%20bidirectional%20LSTM%20to%20capture%20longitudinal%20geometric%20evolution.%20The%20model%2C%20trained%20and%20evaluated%20on%20the%20DrivAerNet%2B%2B%20dataset%2C%20achieves%20a%20high%20coefficient%20of%20determination%20%28R%5E2%20%3E%200.9528%29%20and%20a%20low%20mean%20absolute%20error%20%28MAE%20approx%206.046%20x%2010%5E%7B-3%7D%29%20in%20Cd%20prediction.%20With%20an%20inference%20time%20of%20approximately%200.025%20seconds%20per%20sample%20on%20a%20consumer-grade%20GPU%2C%20our%20approach%20provides%20fast%2C%20accurate%2C%20and%20interpretable%20aerodynamic%20feedback%2C%20facilitating%20more%20agile%20and%20informed%20automotive%20design%20exploration.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02112v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCar%2520Drag%2520Coefficient%2520Prediction%2520from%25203D%2520Point%2520Clouds%2520Using%2520a%2520Slice-Based%2520Surrogate%2520Model%26entry.906535625%3DUtkarsh%2520Singh%2520and%2520Absaar%2520Ali%2520and%2520Adarsh%2520Roy%26entry.1292438233%3DThe%2520automotive%2520industry%2527s%2520pursuit%2520of%2520enhanced%2520fuel%2520economy%2520and%2520performance%2520necessitates%2520efficient%2520aerodynamic%2520design.%2520However%252C%2520traditional%2520evaluation%2520methods%2520such%2520as%2520computational%2520fluid%2520dynamics%2520%2528CFD%2529%2520and%2520wind%2520tunnel%2520testing%2520are%2520resource%2520intensive%252C%2520hindering%2520rapid%2520iteration%2520in%2520the%2520early%2520design%2520stages.%2520Machine%2520learning-based%2520surrogate%2520models%2520offer%2520a%2520promising%2520alternative%252C%2520yet%2520many%2520existing%2520approaches%2520suffer%2520from%2520high%2520computational%2520complexity%252C%2520limited%2520interpretability%252C%2520or%2520insufficient%2520accuracy%2520for%2520detailed%2520geometric%2520inputs.%2520This%2520paper%2520introduces%2520a%2520novel%2520lightweight%2520surrogate%2520model%2520for%2520the%2520prediction%2520of%2520the%2520aerodynamic%2520drag%2520coefficient%2520%2528Cd%2529%2520based%2520on%2520a%2520sequential%2520slice-wise%2520processing%2520of%2520the%2520geometry%2520of%2520the%25203D%2520vehicle.%2520Inspired%2520by%2520medical%2520imaging%252C%25203D%2520point%2520clouds%2520of%2520vehicles%2520are%2520decomposed%2520into%2520an%2520ordered%2520sequence%2520of%25202D%2520cross-sectional%2520slices%2520along%2520the%2520stream-wise%2520axis.%2520Each%2520slice%2520is%2520encoded%2520by%2520a%2520lightweight%2520PointNet2D%2520module%252C%2520and%2520the%2520sequence%2520of%2520slice%2520embeddings%2520is%2520processed%2520by%2520a%2520bidirectional%2520LSTM%2520to%2520capture%2520longitudinal%2520geometric%2520evolution.%2520The%2520model%252C%2520trained%2520and%2520evaluated%2520on%2520the%2520DrivAerNet%252B%252B%2520dataset%252C%2520achieves%2520a%2520high%2520coefficient%2520of%2520determination%2520%2528R%255E2%2520%253E%25200.9528%2529%2520and%2520a%2520low%2520mean%2520absolute%2520error%2520%2528MAE%2520approx%25206.046%2520x%252010%255E%257B-3%257D%2529%2520in%2520Cd%2520prediction.%2520With%2520an%2520inference%2520time%2520of%2520approximately%25200.025%2520seconds%2520per%2520sample%2520on%2520a%2520consumer-grade%2520GPU%252C%2520our%2520approach%2520provides%2520fast%252C%2520accurate%252C%2520and%2520interpretable%2520aerodynamic%2520feedback%252C%2520facilitating%2520more%2520agile%2520and%2520informed%2520automotive%2520design%2520exploration.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02112v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Car%20Drag%20Coefficient%20Prediction%20from%203D%20Point%20Clouds%20Using%20a%20Slice-Based%20Surrogate%20Model&entry.906535625=Utkarsh%20Singh%20and%20Absaar%20Ali%20and%20Adarsh%20Roy&entry.1292438233=The%20automotive%20industry%27s%20pursuit%20of%20enhanced%20fuel%20economy%20and%20performance%20necessitates%20efficient%20aerodynamic%20design.%20However%2C%20traditional%20evaluation%20methods%20such%20as%20computational%20fluid%20dynamics%20%28CFD%29%20and%20wind%20tunnel%20testing%20are%20resource%20intensive%2C%20hindering%20rapid%20iteration%20in%20the%20early%20design%20stages.%20Machine%20learning-based%20surrogate%20models%20offer%20a%20promising%20alternative%2C%20yet%20many%20existing%20approaches%20suffer%20from%20high%20computational%20complexity%2C%20limited%20interpretability%2C%20or%20insufficient%20accuracy%20for%20detailed%20geometric%20inputs.%20This%20paper%20introduces%20a%20novel%20lightweight%20surrogate%20model%20for%20the%20prediction%20of%20the%20aerodynamic%20drag%20coefficient%20%28Cd%29%20based%20on%20a%20sequential%20slice-wise%20processing%20of%20the%20geometry%20of%20the%203D%20vehicle.%20Inspired%20by%20medical%20imaging%2C%203D%20point%20clouds%20of%20vehicles%20are%20decomposed%20into%20an%20ordered%20sequence%20of%202D%20cross-sectional%20slices%20along%20the%20stream-wise%20axis.%20Each%20slice%20is%20encoded%20by%20a%20lightweight%20PointNet2D%20module%2C%20and%20the%20sequence%20of%20slice%20embeddings%20is%20processed%20by%20a%20bidirectional%20LSTM%20to%20capture%20longitudinal%20geometric%20evolution.%20The%20model%2C%20trained%20and%20evaluated%20on%20the%20DrivAerNet%2B%2B%20dataset%2C%20achieves%20a%20high%20coefficient%20of%20determination%20%28R%5E2%20%3E%200.9528%29%20and%20a%20low%20mean%20absolute%20error%20%28MAE%20approx%206.046%20x%2010%5E%7B-3%7D%29%20in%20Cd%20prediction.%20With%20an%20inference%20time%20of%20approximately%200.025%20seconds%20per%20sample%20on%20a%20consumer-grade%20GPU%2C%20our%20approach%20provides%20fast%2C%20accurate%2C%20and%20interpretable%20aerodynamic%20feedback%2C%20facilitating%20more%20agile%20and%20informed%20automotive%20design%20exploration.&entry.1838667208=http%3A//arxiv.org/abs/2601.02112v1&entry.124074799=Read"},
{"title": "Training More Robust Classification Model via Discriminative Loss and Gaussian Noise Injection", "author": "Hai-Vy Nguyen and Fabrice Gamboa and Sixin Zhang and Reda Chhaibi and Serge Gratton and Thierry Giaccone", "abstract": "Robustness of deep neural networks to input noise remains a critical challenge, as naive noise injection often degrades accuracy on clean (uncorrupted) data. We propose a novel training framework that addresses this trade-off through two complementary objectives. First, we introduce a loss function applied at the penultimate layer that explicitly enforces intra-class compactness and increases the margin to analytically defined decision boundaries. This enhances feature discriminativeness and class separability for clean data. Second, we propose a class-wise feature alignment mechanism that brings noisy data clusters closer to their clean counterparts. Furthermore, we provide a theoretical analysis demonstrating that improving feature stability under additive Gaussian noise implicitly reduces the curvature of the softmax loss landscape in input space, as measured by Hessian eigenvalues.This thus naturally enhances robustness without explicit curvature penalties. Conversely, we also theoretically show that lower curvatures lead to more robust models. We validate the effectiveness of our method on standard benchmarks and our custom dataset. Our approach significantly reinforces model robustness to various perturbations while maintaining high accuracy on clean data, advancing the understanding and practice of noise-robust deep learning.", "link": "http://arxiv.org/abs/2405.18499v4", "date": "2026-01-05", "relevancy": 2.0927, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5305}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5242}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5024}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20More%20Robust%20Classification%20Model%20via%20Discriminative%20Loss%20and%20Gaussian%20Noise%20Injection&body=Title%3A%20Training%20More%20Robust%20Classification%20Model%20via%20Discriminative%20Loss%20and%20Gaussian%20Noise%20Injection%0AAuthor%3A%20Hai-Vy%20Nguyen%20and%20Fabrice%20Gamboa%20and%20Sixin%20Zhang%20and%20Reda%20Chhaibi%20and%20Serge%20Gratton%20and%20Thierry%20Giaccone%0AAbstract%3A%20Robustness%20of%20deep%20neural%20networks%20to%20input%20noise%20remains%20a%20critical%20challenge%2C%20as%20naive%20noise%20injection%20often%20degrades%20accuracy%20on%20clean%20%28uncorrupted%29%20data.%20We%20propose%20a%20novel%20training%20framework%20that%20addresses%20this%20trade-off%20through%20two%20complementary%20objectives.%20First%2C%20we%20introduce%20a%20loss%20function%20applied%20at%20the%20penultimate%20layer%20that%20explicitly%20enforces%20intra-class%20compactness%20and%20increases%20the%20margin%20to%20analytically%20defined%20decision%20boundaries.%20This%20enhances%20feature%20discriminativeness%20and%20class%20separability%20for%20clean%20data.%20Second%2C%20we%20propose%20a%20class-wise%20feature%20alignment%20mechanism%20that%20brings%20noisy%20data%20clusters%20closer%20to%20their%20clean%20counterparts.%20Furthermore%2C%20we%20provide%20a%20theoretical%20analysis%20demonstrating%20that%20improving%20feature%20stability%20under%20additive%20Gaussian%20noise%20implicitly%20reduces%20the%20curvature%20of%20the%20softmax%20loss%20landscape%20in%20input%20space%2C%20as%20measured%20by%20Hessian%20eigenvalues.This%20thus%20naturally%20enhances%20robustness%20without%20explicit%20curvature%20penalties.%20Conversely%2C%20we%20also%20theoretically%20show%20that%20lower%20curvatures%20lead%20to%20more%20robust%20models.%20We%20validate%20the%20effectiveness%20of%20our%20method%20on%20standard%20benchmarks%20and%20our%20custom%20dataset.%20Our%20approach%20significantly%20reinforces%20model%20robustness%20to%20various%20perturbations%20while%20maintaining%20high%20accuracy%20on%20clean%20data%2C%20advancing%20the%20understanding%20and%20practice%20of%20noise-robust%20deep%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2405.18499v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520More%2520Robust%2520Classification%2520Model%2520via%2520Discriminative%2520Loss%2520and%2520Gaussian%2520Noise%2520Injection%26entry.906535625%3DHai-Vy%2520Nguyen%2520and%2520Fabrice%2520Gamboa%2520and%2520Sixin%2520Zhang%2520and%2520Reda%2520Chhaibi%2520and%2520Serge%2520Gratton%2520and%2520Thierry%2520Giaccone%26entry.1292438233%3DRobustness%2520of%2520deep%2520neural%2520networks%2520to%2520input%2520noise%2520remains%2520a%2520critical%2520challenge%252C%2520as%2520naive%2520noise%2520injection%2520often%2520degrades%2520accuracy%2520on%2520clean%2520%2528uncorrupted%2529%2520data.%2520We%2520propose%2520a%2520novel%2520training%2520framework%2520that%2520addresses%2520this%2520trade-off%2520through%2520two%2520complementary%2520objectives.%2520First%252C%2520we%2520introduce%2520a%2520loss%2520function%2520applied%2520at%2520the%2520penultimate%2520layer%2520that%2520explicitly%2520enforces%2520intra-class%2520compactness%2520and%2520increases%2520the%2520margin%2520to%2520analytically%2520defined%2520decision%2520boundaries.%2520This%2520enhances%2520feature%2520discriminativeness%2520and%2520class%2520separability%2520for%2520clean%2520data.%2520Second%252C%2520we%2520propose%2520a%2520class-wise%2520feature%2520alignment%2520mechanism%2520that%2520brings%2520noisy%2520data%2520clusters%2520closer%2520to%2520their%2520clean%2520counterparts.%2520Furthermore%252C%2520we%2520provide%2520a%2520theoretical%2520analysis%2520demonstrating%2520that%2520improving%2520feature%2520stability%2520under%2520additive%2520Gaussian%2520noise%2520implicitly%2520reduces%2520the%2520curvature%2520of%2520the%2520softmax%2520loss%2520landscape%2520in%2520input%2520space%252C%2520as%2520measured%2520by%2520Hessian%2520eigenvalues.This%2520thus%2520naturally%2520enhances%2520robustness%2520without%2520explicit%2520curvature%2520penalties.%2520Conversely%252C%2520we%2520also%2520theoretically%2520show%2520that%2520lower%2520curvatures%2520lead%2520to%2520more%2520robust%2520models.%2520We%2520validate%2520the%2520effectiveness%2520of%2520our%2520method%2520on%2520standard%2520benchmarks%2520and%2520our%2520custom%2520dataset.%2520Our%2520approach%2520significantly%2520reinforces%2520model%2520robustness%2520to%2520various%2520perturbations%2520while%2520maintaining%2520high%2520accuracy%2520on%2520clean%2520data%252C%2520advancing%2520the%2520understanding%2520and%2520practice%2520of%2520noise-robust%2520deep%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18499v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20More%20Robust%20Classification%20Model%20via%20Discriminative%20Loss%20and%20Gaussian%20Noise%20Injection&entry.906535625=Hai-Vy%20Nguyen%20and%20Fabrice%20Gamboa%20and%20Sixin%20Zhang%20and%20Reda%20Chhaibi%20and%20Serge%20Gratton%20and%20Thierry%20Giaccone&entry.1292438233=Robustness%20of%20deep%20neural%20networks%20to%20input%20noise%20remains%20a%20critical%20challenge%2C%20as%20naive%20noise%20injection%20often%20degrades%20accuracy%20on%20clean%20%28uncorrupted%29%20data.%20We%20propose%20a%20novel%20training%20framework%20that%20addresses%20this%20trade-off%20through%20two%20complementary%20objectives.%20First%2C%20we%20introduce%20a%20loss%20function%20applied%20at%20the%20penultimate%20layer%20that%20explicitly%20enforces%20intra-class%20compactness%20and%20increases%20the%20margin%20to%20analytically%20defined%20decision%20boundaries.%20This%20enhances%20feature%20discriminativeness%20and%20class%20separability%20for%20clean%20data.%20Second%2C%20we%20propose%20a%20class-wise%20feature%20alignment%20mechanism%20that%20brings%20noisy%20data%20clusters%20closer%20to%20their%20clean%20counterparts.%20Furthermore%2C%20we%20provide%20a%20theoretical%20analysis%20demonstrating%20that%20improving%20feature%20stability%20under%20additive%20Gaussian%20noise%20implicitly%20reduces%20the%20curvature%20of%20the%20softmax%20loss%20landscape%20in%20input%20space%2C%20as%20measured%20by%20Hessian%20eigenvalues.This%20thus%20naturally%20enhances%20robustness%20without%20explicit%20curvature%20penalties.%20Conversely%2C%20we%20also%20theoretically%20show%20that%20lower%20curvatures%20lead%20to%20more%20robust%20models.%20We%20validate%20the%20effectiveness%20of%20our%20method%20on%20standard%20benchmarks%20and%20our%20custom%20dataset.%20Our%20approach%20significantly%20reinforces%20model%20robustness%20to%20various%20perturbations%20while%20maintaining%20high%20accuracy%20on%20clean%20data%2C%20advancing%20the%20understanding%20and%20practice%20of%20noise-robust%20deep%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2405.18499v4&entry.124074799=Read"},
{"title": "VIT-Ped: Visionary Intention Transformer for Pedestrian Behavior Analysis", "author": "Aly R. Elkammar and Karim M. Gamaleldin and Catherine M. Elias", "abstract": "Pedestrian Intention prediction is one of the key technologies in the transition from level 3 to level 4 autonomous driving. To understand pedestrian crossing behaviour, several elements and features should be taken into consideration to make the roads of tomorrow safer for everybody. We introduce a transformer / video vision transformer based algorithm of different sizes which uses different data modalities .We evaluated our algorithms on popular pedestrian behaviour dataset, JAAD, and have reached SOTA performance and passed the SOTA in metrics like Accuracy, AUC and F1-score. The advantages brought by different model design choices are investigated via extensive ablation studies.", "link": "http://arxiv.org/abs/2601.01989v1", "date": "2026-01-05", "relevancy": 2.0914, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.551}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5302}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VIT-Ped%3A%20Visionary%20Intention%20Transformer%20for%20Pedestrian%20Behavior%20Analysis&body=Title%3A%20VIT-Ped%3A%20Visionary%20Intention%20Transformer%20for%20Pedestrian%20Behavior%20Analysis%0AAuthor%3A%20Aly%20R.%20Elkammar%20and%20Karim%20M.%20Gamaleldin%20and%20Catherine%20M.%20Elias%0AAbstract%3A%20Pedestrian%20Intention%20prediction%20is%20one%20of%20the%20key%20technologies%20in%20the%20transition%20from%20level%203%20to%20level%204%20autonomous%20driving.%20To%20understand%20pedestrian%20crossing%20behaviour%2C%20several%20elements%20and%20features%20should%20be%20taken%20into%20consideration%20to%20make%20the%20roads%20of%20tomorrow%20safer%20for%20everybody.%20We%20introduce%20a%20transformer%20/%20video%20vision%20transformer%20based%20algorithm%20of%20different%20sizes%20which%20uses%20different%20data%20modalities%20.We%20evaluated%20our%20algorithms%20on%20popular%20pedestrian%20behaviour%20dataset%2C%20JAAD%2C%20and%20have%20reached%20SOTA%20performance%20and%20passed%20the%20SOTA%20in%20metrics%20like%20Accuracy%2C%20AUC%20and%20F1-score.%20The%20advantages%20brought%20by%20different%20model%20design%20choices%20are%20investigated%20via%20extensive%20ablation%20studies.%0ALink%3A%20http%3A//arxiv.org/abs/2601.01989v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVIT-Ped%253A%2520Visionary%2520Intention%2520Transformer%2520for%2520Pedestrian%2520Behavior%2520Analysis%26entry.906535625%3DAly%2520R.%2520Elkammar%2520and%2520Karim%2520M.%2520Gamaleldin%2520and%2520Catherine%2520M.%2520Elias%26entry.1292438233%3DPedestrian%2520Intention%2520prediction%2520is%2520one%2520of%2520the%2520key%2520technologies%2520in%2520the%2520transition%2520from%2520level%25203%2520to%2520level%25204%2520autonomous%2520driving.%2520To%2520understand%2520pedestrian%2520crossing%2520behaviour%252C%2520several%2520elements%2520and%2520features%2520should%2520be%2520taken%2520into%2520consideration%2520to%2520make%2520the%2520roads%2520of%2520tomorrow%2520safer%2520for%2520everybody.%2520We%2520introduce%2520a%2520transformer%2520/%2520video%2520vision%2520transformer%2520based%2520algorithm%2520of%2520different%2520sizes%2520which%2520uses%2520different%2520data%2520modalities%2520.We%2520evaluated%2520our%2520algorithms%2520on%2520popular%2520pedestrian%2520behaviour%2520dataset%252C%2520JAAD%252C%2520and%2520have%2520reached%2520SOTA%2520performance%2520and%2520passed%2520the%2520SOTA%2520in%2520metrics%2520like%2520Accuracy%252C%2520AUC%2520and%2520F1-score.%2520The%2520advantages%2520brought%2520by%2520different%2520model%2520design%2520choices%2520are%2520investigated%2520via%2520extensive%2520ablation%2520studies.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.01989v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIT-Ped%3A%20Visionary%20Intention%20Transformer%20for%20Pedestrian%20Behavior%20Analysis&entry.906535625=Aly%20R.%20Elkammar%20and%20Karim%20M.%20Gamaleldin%20and%20Catherine%20M.%20Elias&entry.1292438233=Pedestrian%20Intention%20prediction%20is%20one%20of%20the%20key%20technologies%20in%20the%20transition%20from%20level%203%20to%20level%204%20autonomous%20driving.%20To%20understand%20pedestrian%20crossing%20behaviour%2C%20several%20elements%20and%20features%20should%20be%20taken%20into%20consideration%20to%20make%20the%20roads%20of%20tomorrow%20safer%20for%20everybody.%20We%20introduce%20a%20transformer%20/%20video%20vision%20transformer%20based%20algorithm%20of%20different%20sizes%20which%20uses%20different%20data%20modalities%20.We%20evaluated%20our%20algorithms%20on%20popular%20pedestrian%20behaviour%20dataset%2C%20JAAD%2C%20and%20have%20reached%20SOTA%20performance%20and%20passed%20the%20SOTA%20in%20metrics%20like%20Accuracy%2C%20AUC%20and%20F1-score.%20The%20advantages%20brought%20by%20different%20model%20design%20choices%20are%20investigated%20via%20extensive%20ablation%20studies.&entry.1838667208=http%3A//arxiv.org/abs/2601.01989v1&entry.124074799=Read"},
{"title": "LION-DG: Layer-Informed Initialization with Deep Gradient Protocols for Accelerated Neural Network Training", "author": "Hyunjun Kim", "abstract": "Weight initialization remains decisive for neural network optimization, yet existing methods are largely layer-agnostic. We study initialization for deeply-supervised architectures with auxiliary classifiers, where untrained auxiliary heads can destabilize early training through gradient interference.\n  We propose LION-DG, a layer-informed initialization that zero-initializes auxiliary classifier heads while applying standard He-initialization to the backbone. We prove that this implements Gradient Awakening: auxiliary gradients are exactly zero at initialization, then phase in naturally as weights grow -- providing an implicit warmup without hyperparameters.\n  Experiments on CIFAR-10 and CIFAR-100 with DenseNet-DS and ResNet-DS architectures demonstrate: (1) DenseNet-DS: +8.3% faster convergence on CIFAR-10 with comparable accuracy, (2) Hybrid approach: Combining LSUV with LION-DG achieves best accuracy (81.92% on CIFAR-10), (3) ResNet-DS: Positive speedup on CIFAR-100 (+11.3%) with side-tap auxiliary design.\n  We identify architecture-specific trade-offs and provide clear guidelines for practitioners. LION-DG is simple, requires zero hyperparameters, and adds no computational overhead.", "link": "http://arxiv.org/abs/2601.02105v1", "date": "2026-01-05", "relevancy": 2.0876, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5698}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5062}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LION-DG%3A%20Layer-Informed%20Initialization%20with%20Deep%20Gradient%20Protocols%20for%20Accelerated%20Neural%20Network%20Training&body=Title%3A%20LION-DG%3A%20Layer-Informed%20Initialization%20with%20Deep%20Gradient%20Protocols%20for%20Accelerated%20Neural%20Network%20Training%0AAuthor%3A%20Hyunjun%20Kim%0AAbstract%3A%20Weight%20initialization%20remains%20decisive%20for%20neural%20network%20optimization%2C%20yet%20existing%20methods%20are%20largely%20layer-agnostic.%20We%20study%20initialization%20for%20deeply-supervised%20architectures%20with%20auxiliary%20classifiers%2C%20where%20untrained%20auxiliary%20heads%20can%20destabilize%20early%20training%20through%20gradient%20interference.%0A%20%20We%20propose%20LION-DG%2C%20a%20layer-informed%20initialization%20that%20zero-initializes%20auxiliary%20classifier%20heads%20while%20applying%20standard%20He-initialization%20to%20the%20backbone.%20We%20prove%20that%20this%20implements%20Gradient%20Awakening%3A%20auxiliary%20gradients%20are%20exactly%20zero%20at%20initialization%2C%20then%20phase%20in%20naturally%20as%20weights%20grow%20--%20providing%20an%20implicit%20warmup%20without%20hyperparameters.%0A%20%20Experiments%20on%20CIFAR-10%20and%20CIFAR-100%20with%20DenseNet-DS%20and%20ResNet-DS%20architectures%20demonstrate%3A%20%281%29%20DenseNet-DS%3A%20%2B8.3%25%20faster%20convergence%20on%20CIFAR-10%20with%20comparable%20accuracy%2C%20%282%29%20Hybrid%20approach%3A%20Combining%20LSUV%20with%20LION-DG%20achieves%20best%20accuracy%20%2881.92%25%20on%20CIFAR-10%29%2C%20%283%29%20ResNet-DS%3A%20Positive%20speedup%20on%20CIFAR-100%20%28%2B11.3%25%29%20with%20side-tap%20auxiliary%20design.%0A%20%20We%20identify%20architecture-specific%20trade-offs%20and%20provide%20clear%20guidelines%20for%20practitioners.%20LION-DG%20is%20simple%2C%20requires%20zero%20hyperparameters%2C%20and%20adds%20no%20computational%20overhead.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02105v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLION-DG%253A%2520Layer-Informed%2520Initialization%2520with%2520Deep%2520Gradient%2520Protocols%2520for%2520Accelerated%2520Neural%2520Network%2520Training%26entry.906535625%3DHyunjun%2520Kim%26entry.1292438233%3DWeight%2520initialization%2520remains%2520decisive%2520for%2520neural%2520network%2520optimization%252C%2520yet%2520existing%2520methods%2520are%2520largely%2520layer-agnostic.%2520We%2520study%2520initialization%2520for%2520deeply-supervised%2520architectures%2520with%2520auxiliary%2520classifiers%252C%2520where%2520untrained%2520auxiliary%2520heads%2520can%2520destabilize%2520early%2520training%2520through%2520gradient%2520interference.%250A%2520%2520We%2520propose%2520LION-DG%252C%2520a%2520layer-informed%2520initialization%2520that%2520zero-initializes%2520auxiliary%2520classifier%2520heads%2520while%2520applying%2520standard%2520He-initialization%2520to%2520the%2520backbone.%2520We%2520prove%2520that%2520this%2520implements%2520Gradient%2520Awakening%253A%2520auxiliary%2520gradients%2520are%2520exactly%2520zero%2520at%2520initialization%252C%2520then%2520phase%2520in%2520naturally%2520as%2520weights%2520grow%2520--%2520providing%2520an%2520implicit%2520warmup%2520without%2520hyperparameters.%250A%2520%2520Experiments%2520on%2520CIFAR-10%2520and%2520CIFAR-100%2520with%2520DenseNet-DS%2520and%2520ResNet-DS%2520architectures%2520demonstrate%253A%2520%25281%2529%2520DenseNet-DS%253A%2520%252B8.3%2525%2520faster%2520convergence%2520on%2520CIFAR-10%2520with%2520comparable%2520accuracy%252C%2520%25282%2529%2520Hybrid%2520approach%253A%2520Combining%2520LSUV%2520with%2520LION-DG%2520achieves%2520best%2520accuracy%2520%252881.92%2525%2520on%2520CIFAR-10%2529%252C%2520%25283%2529%2520ResNet-DS%253A%2520Positive%2520speedup%2520on%2520CIFAR-100%2520%2528%252B11.3%2525%2529%2520with%2520side-tap%2520auxiliary%2520design.%250A%2520%2520We%2520identify%2520architecture-specific%2520trade-offs%2520and%2520provide%2520clear%2520guidelines%2520for%2520practitioners.%2520LION-DG%2520is%2520simple%252C%2520requires%2520zero%2520hyperparameters%252C%2520and%2520adds%2520no%2520computational%2520overhead.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02105v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LION-DG%3A%20Layer-Informed%20Initialization%20with%20Deep%20Gradient%20Protocols%20for%20Accelerated%20Neural%20Network%20Training&entry.906535625=Hyunjun%20Kim&entry.1292438233=Weight%20initialization%20remains%20decisive%20for%20neural%20network%20optimization%2C%20yet%20existing%20methods%20are%20largely%20layer-agnostic.%20We%20study%20initialization%20for%20deeply-supervised%20architectures%20with%20auxiliary%20classifiers%2C%20where%20untrained%20auxiliary%20heads%20can%20destabilize%20early%20training%20through%20gradient%20interference.%0A%20%20We%20propose%20LION-DG%2C%20a%20layer-informed%20initialization%20that%20zero-initializes%20auxiliary%20classifier%20heads%20while%20applying%20standard%20He-initialization%20to%20the%20backbone.%20We%20prove%20that%20this%20implements%20Gradient%20Awakening%3A%20auxiliary%20gradients%20are%20exactly%20zero%20at%20initialization%2C%20then%20phase%20in%20naturally%20as%20weights%20grow%20--%20providing%20an%20implicit%20warmup%20without%20hyperparameters.%0A%20%20Experiments%20on%20CIFAR-10%20and%20CIFAR-100%20with%20DenseNet-DS%20and%20ResNet-DS%20architectures%20demonstrate%3A%20%281%29%20DenseNet-DS%3A%20%2B8.3%25%20faster%20convergence%20on%20CIFAR-10%20with%20comparable%20accuracy%2C%20%282%29%20Hybrid%20approach%3A%20Combining%20LSUV%20with%20LION-DG%20achieves%20best%20accuracy%20%2881.92%25%20on%20CIFAR-10%29%2C%20%283%29%20ResNet-DS%3A%20Positive%20speedup%20on%20CIFAR-100%20%28%2B11.3%25%29%20with%20side-tap%20auxiliary%20design.%0A%20%20We%20identify%20architecture-specific%20trade-offs%20and%20provide%20clear%20guidelines%20for%20practitioners.%20LION-DG%20is%20simple%2C%20requires%20zero%20hyperparameters%2C%20and%20adds%20no%20computational%20overhead.&entry.1838667208=http%3A//arxiv.org/abs/2601.02105v1&entry.124074799=Read"},
{"title": "BiPrompt: Bilateral Prompt Optimization for Visual and Textual Debiasing in Vision-Language Models", "author": "Sunny Gupta and Shounak Das and Amit Sethi", "abstract": "Vision language foundation models such as CLIP exhibit impressive zero-shot generalization yet remain vulnerable to spurious correlations across visual and textual modalities. Existing debiasing approaches often address a single modality either visual or textual leading to partial robustness and unstable adaptation under distribution shifts. We propose a bilateral prompt optimization framework (BiPrompt) that simultaneously mitigates non-causal feature reliance in both modalities during test-time adaptation. On the visual side, it employs structured attention-guided erasure to suppress background activations and enforce orthogonal prediction consistency between causal and spurious regions. On the textual side, it introduces balanced prompt normalization, a learnable re-centering mechanism that aligns class embeddings toward an isotropic semantic space. Together, these modules jointly minimize conditional mutual information between spurious cues and predictions, steering the model toward causal, domain invariant reasoning without retraining or domain supervision. Extensive evaluations on real-world and synthetic bias benchmarks demonstrate consistent improvements in both average and worst-group accuracies over prior test-time debiasing methods, establishing a lightweight yet effective path toward trustworthy and causally grounded vision-language adaptation.", "link": "http://arxiv.org/abs/2601.02147v1", "date": "2026-01-05", "relevancy": 2.0862, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5439}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5183}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5158}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BiPrompt%3A%20Bilateral%20Prompt%20Optimization%20for%20Visual%20and%20Textual%20Debiasing%20in%20Vision-Language%20Models&body=Title%3A%20BiPrompt%3A%20Bilateral%20Prompt%20Optimization%20for%20Visual%20and%20Textual%20Debiasing%20in%20Vision-Language%20Models%0AAuthor%3A%20Sunny%20Gupta%20and%20Shounak%20Das%20and%20Amit%20Sethi%0AAbstract%3A%20Vision%20language%20foundation%20models%20such%20as%20CLIP%20exhibit%20impressive%20zero-shot%20generalization%20yet%20remain%20vulnerable%20to%20spurious%20correlations%20across%20visual%20and%20textual%20modalities.%20Existing%20debiasing%20approaches%20often%20address%20a%20single%20modality%20either%20visual%20or%20textual%20leading%20to%20partial%20robustness%20and%20unstable%20adaptation%20under%20distribution%20shifts.%20We%20propose%20a%20bilateral%20prompt%20optimization%20framework%20%28BiPrompt%29%20that%20simultaneously%20mitigates%20non-causal%20feature%20reliance%20in%20both%20modalities%20during%20test-time%20adaptation.%20On%20the%20visual%20side%2C%20it%20employs%20structured%20attention-guided%20erasure%20to%20suppress%20background%20activations%20and%20enforce%20orthogonal%20prediction%20consistency%20between%20causal%20and%20spurious%20regions.%20On%20the%20textual%20side%2C%20it%20introduces%20balanced%20prompt%20normalization%2C%20a%20learnable%20re-centering%20mechanism%20that%20aligns%20class%20embeddings%20toward%20an%20isotropic%20semantic%20space.%20Together%2C%20these%20modules%20jointly%20minimize%20conditional%20mutual%20information%20between%20spurious%20cues%20and%20predictions%2C%20steering%20the%20model%20toward%20causal%2C%20domain%20invariant%20reasoning%20without%20retraining%20or%20domain%20supervision.%20Extensive%20evaluations%20on%20real-world%20and%20synthetic%20bias%20benchmarks%20demonstrate%20consistent%20improvements%20in%20both%20average%20and%20worst-group%20accuracies%20over%20prior%20test-time%20debiasing%20methods%2C%20establishing%20a%20lightweight%20yet%20effective%20path%20toward%20trustworthy%20and%20causally%20grounded%20vision-language%20adaptation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02147v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiPrompt%253A%2520Bilateral%2520Prompt%2520Optimization%2520for%2520Visual%2520and%2520Textual%2520Debiasing%2520in%2520Vision-Language%2520Models%26entry.906535625%3DSunny%2520Gupta%2520and%2520Shounak%2520Das%2520and%2520Amit%2520Sethi%26entry.1292438233%3DVision%2520language%2520foundation%2520models%2520such%2520as%2520CLIP%2520exhibit%2520impressive%2520zero-shot%2520generalization%2520yet%2520remain%2520vulnerable%2520to%2520spurious%2520correlations%2520across%2520visual%2520and%2520textual%2520modalities.%2520Existing%2520debiasing%2520approaches%2520often%2520address%2520a%2520single%2520modality%2520either%2520visual%2520or%2520textual%2520leading%2520to%2520partial%2520robustness%2520and%2520unstable%2520adaptation%2520under%2520distribution%2520shifts.%2520We%2520propose%2520a%2520bilateral%2520prompt%2520optimization%2520framework%2520%2528BiPrompt%2529%2520that%2520simultaneously%2520mitigates%2520non-causal%2520feature%2520reliance%2520in%2520both%2520modalities%2520during%2520test-time%2520adaptation.%2520On%2520the%2520visual%2520side%252C%2520it%2520employs%2520structured%2520attention-guided%2520erasure%2520to%2520suppress%2520background%2520activations%2520and%2520enforce%2520orthogonal%2520prediction%2520consistency%2520between%2520causal%2520and%2520spurious%2520regions.%2520On%2520the%2520textual%2520side%252C%2520it%2520introduces%2520balanced%2520prompt%2520normalization%252C%2520a%2520learnable%2520re-centering%2520mechanism%2520that%2520aligns%2520class%2520embeddings%2520toward%2520an%2520isotropic%2520semantic%2520space.%2520Together%252C%2520these%2520modules%2520jointly%2520minimize%2520conditional%2520mutual%2520information%2520between%2520spurious%2520cues%2520and%2520predictions%252C%2520steering%2520the%2520model%2520toward%2520causal%252C%2520domain%2520invariant%2520reasoning%2520without%2520retraining%2520or%2520domain%2520supervision.%2520Extensive%2520evaluations%2520on%2520real-world%2520and%2520synthetic%2520bias%2520benchmarks%2520demonstrate%2520consistent%2520improvements%2520in%2520both%2520average%2520and%2520worst-group%2520accuracies%2520over%2520prior%2520test-time%2520debiasing%2520methods%252C%2520establishing%2520a%2520lightweight%2520yet%2520effective%2520path%2520toward%2520trustworthy%2520and%2520causally%2520grounded%2520vision-language%2520adaptation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02147v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BiPrompt%3A%20Bilateral%20Prompt%20Optimization%20for%20Visual%20and%20Textual%20Debiasing%20in%20Vision-Language%20Models&entry.906535625=Sunny%20Gupta%20and%20Shounak%20Das%20and%20Amit%20Sethi&entry.1292438233=Vision%20language%20foundation%20models%20such%20as%20CLIP%20exhibit%20impressive%20zero-shot%20generalization%20yet%20remain%20vulnerable%20to%20spurious%20correlations%20across%20visual%20and%20textual%20modalities.%20Existing%20debiasing%20approaches%20often%20address%20a%20single%20modality%20either%20visual%20or%20textual%20leading%20to%20partial%20robustness%20and%20unstable%20adaptation%20under%20distribution%20shifts.%20We%20propose%20a%20bilateral%20prompt%20optimization%20framework%20%28BiPrompt%29%20that%20simultaneously%20mitigates%20non-causal%20feature%20reliance%20in%20both%20modalities%20during%20test-time%20adaptation.%20On%20the%20visual%20side%2C%20it%20employs%20structured%20attention-guided%20erasure%20to%20suppress%20background%20activations%20and%20enforce%20orthogonal%20prediction%20consistency%20between%20causal%20and%20spurious%20regions.%20On%20the%20textual%20side%2C%20it%20introduces%20balanced%20prompt%20normalization%2C%20a%20learnable%20re-centering%20mechanism%20that%20aligns%20class%20embeddings%20toward%20an%20isotropic%20semantic%20space.%20Together%2C%20these%20modules%20jointly%20minimize%20conditional%20mutual%20information%20between%20spurious%20cues%20and%20predictions%2C%20steering%20the%20model%20toward%20causal%2C%20domain%20invariant%20reasoning%20without%20retraining%20or%20domain%20supervision.%20Extensive%20evaluations%20on%20real-world%20and%20synthetic%20bias%20benchmarks%20demonstrate%20consistent%20improvements%20in%20both%20average%20and%20worst-group%20accuracies%20over%20prior%20test-time%20debiasing%20methods%2C%20establishing%20a%20lightweight%20yet%20effective%20path%20toward%20trustworthy%20and%20causally%20grounded%20vision-language%20adaptation.&entry.1838667208=http%3A//arxiv.org/abs/2601.02147v1&entry.124074799=Read"},
{"title": "Face Normal Estimation from Rags to Riches", "author": "Meng Wang and Wenjing Dai and Jiawan Zhang and Xiaojie Guo", "abstract": "Although recent approaches to face normal estimation have achieved promising results, their effectiveness heavily depends on large-scale paired data for training. This paper concentrates on relieving this requirement via developing a coarse-to-fine normal estimator. Concretely, our method first trains a neat model from a small dataset to produce coarse face normals that perform as guidance (called exemplars) for the following refinement. A self-attention mechanism is employed to capture long-range dependencies, thus remedying severe local artifacts left in estimated coarse facial normals. Then, a refinement network is customized for the sake of mapping input face images together with corresponding exemplars to fine-grained high-quality facial normals. Such a logical function split can significantly cut the requirement of massive paired data and computational resource. Extensive experiments and ablation studies are conducted to demonstrate the efficacy of our design and reveal its superiority over state-of-the-art methods in terms of both training expense as well as estimation quality. Our code and models are open-sourced at: https://github.com/AutoHDR/FNR2R.git.", "link": "http://arxiv.org/abs/2601.01950v1", "date": "2026-01-05", "relevancy": 2.082, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5367}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5222}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Face%20Normal%20Estimation%20from%20Rags%20to%20Riches&body=Title%3A%20Face%20Normal%20Estimation%20from%20Rags%20to%20Riches%0AAuthor%3A%20Meng%20Wang%20and%20Wenjing%20Dai%20and%20Jiawan%20Zhang%20and%20Xiaojie%20Guo%0AAbstract%3A%20Although%20recent%20approaches%20to%20face%20normal%20estimation%20have%20achieved%20promising%20results%2C%20their%20effectiveness%20heavily%20depends%20on%20large-scale%20paired%20data%20for%20training.%20This%20paper%20concentrates%20on%20relieving%20this%20requirement%20via%20developing%20a%20coarse-to-fine%20normal%20estimator.%20Concretely%2C%20our%20method%20first%20trains%20a%20neat%20model%20from%20a%20small%20dataset%20to%20produce%20coarse%20face%20normals%20that%20perform%20as%20guidance%20%28called%20exemplars%29%20for%20the%20following%20refinement.%20A%20self-attention%20mechanism%20is%20employed%20to%20capture%20long-range%20dependencies%2C%20thus%20remedying%20severe%20local%20artifacts%20left%20in%20estimated%20coarse%20facial%20normals.%20Then%2C%20a%20refinement%20network%20is%20customized%20for%20the%20sake%20of%20mapping%20input%20face%20images%20together%20with%20corresponding%20exemplars%20to%20fine-grained%20high-quality%20facial%20normals.%20Such%20a%20logical%20function%20split%20can%20significantly%20cut%20the%20requirement%20of%20massive%20paired%20data%20and%20computational%20resource.%20Extensive%20experiments%20and%20ablation%20studies%20are%20conducted%20to%20demonstrate%20the%20efficacy%20of%20our%20design%20and%20reveal%20its%20superiority%20over%20state-of-the-art%20methods%20in%20terms%20of%20both%20training%20expense%20as%20well%20as%20estimation%20quality.%20Our%20code%20and%20models%20are%20open-sourced%20at%3A%20https%3A//github.com/AutoHDR/FNR2R.git.%0ALink%3A%20http%3A//arxiv.org/abs/2601.01950v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFace%2520Normal%2520Estimation%2520from%2520Rags%2520to%2520Riches%26entry.906535625%3DMeng%2520Wang%2520and%2520Wenjing%2520Dai%2520and%2520Jiawan%2520Zhang%2520and%2520Xiaojie%2520Guo%26entry.1292438233%3DAlthough%2520recent%2520approaches%2520to%2520face%2520normal%2520estimation%2520have%2520achieved%2520promising%2520results%252C%2520their%2520effectiveness%2520heavily%2520depends%2520on%2520large-scale%2520paired%2520data%2520for%2520training.%2520This%2520paper%2520concentrates%2520on%2520relieving%2520this%2520requirement%2520via%2520developing%2520a%2520coarse-to-fine%2520normal%2520estimator.%2520Concretely%252C%2520our%2520method%2520first%2520trains%2520a%2520neat%2520model%2520from%2520a%2520small%2520dataset%2520to%2520produce%2520coarse%2520face%2520normals%2520that%2520perform%2520as%2520guidance%2520%2528called%2520exemplars%2529%2520for%2520the%2520following%2520refinement.%2520A%2520self-attention%2520mechanism%2520is%2520employed%2520to%2520capture%2520long-range%2520dependencies%252C%2520thus%2520remedying%2520severe%2520local%2520artifacts%2520left%2520in%2520estimated%2520coarse%2520facial%2520normals.%2520Then%252C%2520a%2520refinement%2520network%2520is%2520customized%2520for%2520the%2520sake%2520of%2520mapping%2520input%2520face%2520images%2520together%2520with%2520corresponding%2520exemplars%2520to%2520fine-grained%2520high-quality%2520facial%2520normals.%2520Such%2520a%2520logical%2520function%2520split%2520can%2520significantly%2520cut%2520the%2520requirement%2520of%2520massive%2520paired%2520data%2520and%2520computational%2520resource.%2520Extensive%2520experiments%2520and%2520ablation%2520studies%2520are%2520conducted%2520to%2520demonstrate%2520the%2520efficacy%2520of%2520our%2520design%2520and%2520reveal%2520its%2520superiority%2520over%2520state-of-the-art%2520methods%2520in%2520terms%2520of%2520both%2520training%2520expense%2520as%2520well%2520as%2520estimation%2520quality.%2520Our%2520code%2520and%2520models%2520are%2520open-sourced%2520at%253A%2520https%253A//github.com/AutoHDR/FNR2R.git.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.01950v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Face%20Normal%20Estimation%20from%20Rags%20to%20Riches&entry.906535625=Meng%20Wang%20and%20Wenjing%20Dai%20and%20Jiawan%20Zhang%20and%20Xiaojie%20Guo&entry.1292438233=Although%20recent%20approaches%20to%20face%20normal%20estimation%20have%20achieved%20promising%20results%2C%20their%20effectiveness%20heavily%20depends%20on%20large-scale%20paired%20data%20for%20training.%20This%20paper%20concentrates%20on%20relieving%20this%20requirement%20via%20developing%20a%20coarse-to-fine%20normal%20estimator.%20Concretely%2C%20our%20method%20first%20trains%20a%20neat%20model%20from%20a%20small%20dataset%20to%20produce%20coarse%20face%20normals%20that%20perform%20as%20guidance%20%28called%20exemplars%29%20for%20the%20following%20refinement.%20A%20self-attention%20mechanism%20is%20employed%20to%20capture%20long-range%20dependencies%2C%20thus%20remedying%20severe%20local%20artifacts%20left%20in%20estimated%20coarse%20facial%20normals.%20Then%2C%20a%20refinement%20network%20is%20customized%20for%20the%20sake%20of%20mapping%20input%20face%20images%20together%20with%20corresponding%20exemplars%20to%20fine-grained%20high-quality%20facial%20normals.%20Such%20a%20logical%20function%20split%20can%20significantly%20cut%20the%20requirement%20of%20massive%20paired%20data%20and%20computational%20resource.%20Extensive%20experiments%20and%20ablation%20studies%20are%20conducted%20to%20demonstrate%20the%20efficacy%20of%20our%20design%20and%20reveal%20its%20superiority%20over%20state-of-the-art%20methods%20in%20terms%20of%20both%20training%20expense%20as%20well%20as%20estimation%20quality.%20Our%20code%20and%20models%20are%20open-sourced%20at%3A%20https%3A//github.com/AutoHDR/FNR2R.git.&entry.1838667208=http%3A//arxiv.org/abs/2601.01950v1&entry.124074799=Read"},
{"title": "A Comparative Study of Custom CNNs, Pre-trained Models, and Transfer Learning Across Multiple Visual Datasets", "author": "Annoor Sharara Akhand", "abstract": "Convolutional Neural Networks (CNNs) are a standard approach for visual recognition due to their capacity to learn hierarchical representations from raw pixels. In practice, practitioners often choose among (i) training a compact custom CNN from scratch, (ii) using a large pre-trained CNN as a fixed feature extractor, and (iii) performing transfer learning via partial or full fine-tuning of a pre-trained backbone. This report presents a controlled comparison of these three paradigms across five real-world image classification datasets spanning road-surface defect recognition, agricultural variety identification, fruit/leaf disease recognition, pedestrian walkway encroachment recognition, and unauthorized vehicle recognition. Models are evaluated using accuracy and macro F1-score, complemented by efficiency metrics including training time per epoch and parameter counts. The results show that transfer learning consistently yields the strongest predictive performance, while the custom CNN provides an attractive efficiency--accuracy trade-off, especially when compute and memory budgets are constrained.", "link": "http://arxiv.org/abs/2601.02246v1", "date": "2026-01-05", "relevancy": 2.078, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5198}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5198}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comparative%20Study%20of%20Custom%20CNNs%2C%20Pre-trained%20Models%2C%20and%20Transfer%20Learning%20Across%20Multiple%20Visual%20Datasets&body=Title%3A%20A%20Comparative%20Study%20of%20Custom%20CNNs%2C%20Pre-trained%20Models%2C%20and%20Transfer%20Learning%20Across%20Multiple%20Visual%20Datasets%0AAuthor%3A%20Annoor%20Sharara%20Akhand%0AAbstract%3A%20Convolutional%20Neural%20Networks%20%28CNNs%29%20are%20a%20standard%20approach%20for%20visual%20recognition%20due%20to%20their%20capacity%20to%20learn%20hierarchical%20representations%20from%20raw%20pixels.%20In%20practice%2C%20practitioners%20often%20choose%20among%20%28i%29%20training%20a%20compact%20custom%20CNN%20from%20scratch%2C%20%28ii%29%20using%20a%20large%20pre-trained%20CNN%20as%20a%20fixed%20feature%20extractor%2C%20and%20%28iii%29%20performing%20transfer%20learning%20via%20partial%20or%20full%20fine-tuning%20of%20a%20pre-trained%20backbone.%20This%20report%20presents%20a%20controlled%20comparison%20of%20these%20three%20paradigms%20across%20five%20real-world%20image%20classification%20datasets%20spanning%20road-surface%20defect%20recognition%2C%20agricultural%20variety%20identification%2C%20fruit/leaf%20disease%20recognition%2C%20pedestrian%20walkway%20encroachment%20recognition%2C%20and%20unauthorized%20vehicle%20recognition.%20Models%20are%20evaluated%20using%20accuracy%20and%20macro%20F1-score%2C%20complemented%20by%20efficiency%20metrics%20including%20training%20time%20per%20epoch%20and%20parameter%20counts.%20The%20results%20show%20that%20transfer%20learning%20consistently%20yields%20the%20strongest%20predictive%20performance%2C%20while%20the%20custom%20CNN%20provides%20an%20attractive%20efficiency--accuracy%20trade-off%2C%20especially%20when%20compute%20and%20memory%20budgets%20are%20constrained.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02246v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comparative%2520Study%2520of%2520Custom%2520CNNs%252C%2520Pre-trained%2520Models%252C%2520and%2520Transfer%2520Learning%2520Across%2520Multiple%2520Visual%2520Datasets%26entry.906535625%3DAnnoor%2520Sharara%2520Akhand%26entry.1292438233%3DConvolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520are%2520a%2520standard%2520approach%2520for%2520visual%2520recognition%2520due%2520to%2520their%2520capacity%2520to%2520learn%2520hierarchical%2520representations%2520from%2520raw%2520pixels.%2520In%2520practice%252C%2520practitioners%2520often%2520choose%2520among%2520%2528i%2529%2520training%2520a%2520compact%2520custom%2520CNN%2520from%2520scratch%252C%2520%2528ii%2529%2520using%2520a%2520large%2520pre-trained%2520CNN%2520as%2520a%2520fixed%2520feature%2520extractor%252C%2520and%2520%2528iii%2529%2520performing%2520transfer%2520learning%2520via%2520partial%2520or%2520full%2520fine-tuning%2520of%2520a%2520pre-trained%2520backbone.%2520This%2520report%2520presents%2520a%2520controlled%2520comparison%2520of%2520these%2520three%2520paradigms%2520across%2520five%2520real-world%2520image%2520classification%2520datasets%2520spanning%2520road-surface%2520defect%2520recognition%252C%2520agricultural%2520variety%2520identification%252C%2520fruit/leaf%2520disease%2520recognition%252C%2520pedestrian%2520walkway%2520encroachment%2520recognition%252C%2520and%2520unauthorized%2520vehicle%2520recognition.%2520Models%2520are%2520evaluated%2520using%2520accuracy%2520and%2520macro%2520F1-score%252C%2520complemented%2520by%2520efficiency%2520metrics%2520including%2520training%2520time%2520per%2520epoch%2520and%2520parameter%2520counts.%2520The%2520results%2520show%2520that%2520transfer%2520learning%2520consistently%2520yields%2520the%2520strongest%2520predictive%2520performance%252C%2520while%2520the%2520custom%2520CNN%2520provides%2520an%2520attractive%2520efficiency--accuracy%2520trade-off%252C%2520especially%2520when%2520compute%2520and%2520memory%2520budgets%2520are%2520constrained.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02246v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comparative%20Study%20of%20Custom%20CNNs%2C%20Pre-trained%20Models%2C%20and%20Transfer%20Learning%20Across%20Multiple%20Visual%20Datasets&entry.906535625=Annoor%20Sharara%20Akhand&entry.1292438233=Convolutional%20Neural%20Networks%20%28CNNs%29%20are%20a%20standard%20approach%20for%20visual%20recognition%20due%20to%20their%20capacity%20to%20learn%20hierarchical%20representations%20from%20raw%20pixels.%20In%20practice%2C%20practitioners%20often%20choose%20among%20%28i%29%20training%20a%20compact%20custom%20CNN%20from%20scratch%2C%20%28ii%29%20using%20a%20large%20pre-trained%20CNN%20as%20a%20fixed%20feature%20extractor%2C%20and%20%28iii%29%20performing%20transfer%20learning%20via%20partial%20or%20full%20fine-tuning%20of%20a%20pre-trained%20backbone.%20This%20report%20presents%20a%20controlled%20comparison%20of%20these%20three%20paradigms%20across%20five%20real-world%20image%20classification%20datasets%20spanning%20road-surface%20defect%20recognition%2C%20agricultural%20variety%20identification%2C%20fruit/leaf%20disease%20recognition%2C%20pedestrian%20walkway%20encroachment%20recognition%2C%20and%20unauthorized%20vehicle%20recognition.%20Models%20are%20evaluated%20using%20accuracy%20and%20macro%20F1-score%2C%20complemented%20by%20efficiency%20metrics%20including%20training%20time%20per%20epoch%20and%20parameter%20counts.%20The%20results%20show%20that%20transfer%20learning%20consistently%20yields%20the%20strongest%20predictive%20performance%2C%20while%20the%20custom%20CNN%20provides%20an%20attractive%20efficiency--accuracy%20trade-off%2C%20especially%20when%20compute%20and%20memory%20budgets%20are%20constrained.&entry.1838667208=http%3A//arxiv.org/abs/2601.02246v1&entry.124074799=Read"},
{"title": "Ideal Observer for Segmentation of Dead Leaves Images", "author": "Swantje Mahncke and Malte Ott", "abstract": "The human visual environment is comprised of different surfaces that are distributed in space. The parts of a scene that are visible at any one time are governed by the occlusion of overlapping objects. In this work we consider \"dead leaves\" models, which replicate these occlusions when generating images by layering objects on top of each other. A dead leaves model is a generative model comprised of distributions for object position, shape, color and texture. An image is generated from a dead leaves model by sampling objects (\"leaves\") from these distributions until a stopping criterion is reached, usually when the image is fully covered or until a given number of leaves was sampled. Here, we describe a theoretical approach, based on previous work, to derive a Bayesian ideal observer for the partition of a given set of pixels based on independent dead leaves model distributions. Extending previous work, we provide step-by-step explanations for the computation of the posterior probability as well as describe factors that determine the feasibility of practically applying this computation. The dead leaves image model and the associated ideal observer can be applied to study segmentation decisions in a limited number of pixels, providing a principled upper-bound on performance, to which humans and vision algorithms could be compared.", "link": "http://arxiv.org/abs/2512.05539v2", "date": "2026-01-05", "relevancy": 2.0721, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5271}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.517}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ideal%20Observer%20for%20Segmentation%20of%20Dead%20Leaves%20Images&body=Title%3A%20Ideal%20Observer%20for%20Segmentation%20of%20Dead%20Leaves%20Images%0AAuthor%3A%20Swantje%20Mahncke%20and%20Malte%20Ott%0AAbstract%3A%20The%20human%20visual%20environment%20is%20comprised%20of%20different%20surfaces%20that%20are%20distributed%20in%20space.%20The%20parts%20of%20a%20scene%20that%20are%20visible%20at%20any%20one%20time%20are%20governed%20by%20the%20occlusion%20of%20overlapping%20objects.%20In%20this%20work%20we%20consider%20%22dead%20leaves%22%20models%2C%20which%20replicate%20these%20occlusions%20when%20generating%20images%20by%20layering%20objects%20on%20top%20of%20each%20other.%20A%20dead%20leaves%20model%20is%20a%20generative%20model%20comprised%20of%20distributions%20for%20object%20position%2C%20shape%2C%20color%20and%20texture.%20An%20image%20is%20generated%20from%20a%20dead%20leaves%20model%20by%20sampling%20objects%20%28%22leaves%22%29%20from%20these%20distributions%20until%20a%20stopping%20criterion%20is%20reached%2C%20usually%20when%20the%20image%20is%20fully%20covered%20or%20until%20a%20given%20number%20of%20leaves%20was%20sampled.%20Here%2C%20we%20describe%20a%20theoretical%20approach%2C%20based%20on%20previous%20work%2C%20to%20derive%20a%20Bayesian%20ideal%20observer%20for%20the%20partition%20of%20a%20given%20set%20of%20pixels%20based%20on%20independent%20dead%20leaves%20model%20distributions.%20Extending%20previous%20work%2C%20we%20provide%20step-by-step%20explanations%20for%20the%20computation%20of%20the%20posterior%20probability%20as%20well%20as%20describe%20factors%20that%20determine%20the%20feasibility%20of%20practically%20applying%20this%20computation.%20The%20dead%20leaves%20image%20model%20and%20the%20associated%20ideal%20observer%20can%20be%20applied%20to%20study%20segmentation%20decisions%20in%20a%20limited%20number%20of%20pixels%2C%20providing%20a%20principled%20upper-bound%20on%20performance%2C%20to%20which%20humans%20and%20vision%20algorithms%20could%20be%20compared.%0ALink%3A%20http%3A//arxiv.org/abs/2512.05539v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdeal%2520Observer%2520for%2520Segmentation%2520of%2520Dead%2520Leaves%2520Images%26entry.906535625%3DSwantje%2520Mahncke%2520and%2520Malte%2520Ott%26entry.1292438233%3DThe%2520human%2520visual%2520environment%2520is%2520comprised%2520of%2520different%2520surfaces%2520that%2520are%2520distributed%2520in%2520space.%2520The%2520parts%2520of%2520a%2520scene%2520that%2520are%2520visible%2520at%2520any%2520one%2520time%2520are%2520governed%2520by%2520the%2520occlusion%2520of%2520overlapping%2520objects.%2520In%2520this%2520work%2520we%2520consider%2520%2522dead%2520leaves%2522%2520models%252C%2520which%2520replicate%2520these%2520occlusions%2520when%2520generating%2520images%2520by%2520layering%2520objects%2520on%2520top%2520of%2520each%2520other.%2520A%2520dead%2520leaves%2520model%2520is%2520a%2520generative%2520model%2520comprised%2520of%2520distributions%2520for%2520object%2520position%252C%2520shape%252C%2520color%2520and%2520texture.%2520An%2520image%2520is%2520generated%2520from%2520a%2520dead%2520leaves%2520model%2520by%2520sampling%2520objects%2520%2528%2522leaves%2522%2529%2520from%2520these%2520distributions%2520until%2520a%2520stopping%2520criterion%2520is%2520reached%252C%2520usually%2520when%2520the%2520image%2520is%2520fully%2520covered%2520or%2520until%2520a%2520given%2520number%2520of%2520leaves%2520was%2520sampled.%2520Here%252C%2520we%2520describe%2520a%2520theoretical%2520approach%252C%2520based%2520on%2520previous%2520work%252C%2520to%2520derive%2520a%2520Bayesian%2520ideal%2520observer%2520for%2520the%2520partition%2520of%2520a%2520given%2520set%2520of%2520pixels%2520based%2520on%2520independent%2520dead%2520leaves%2520model%2520distributions.%2520Extending%2520previous%2520work%252C%2520we%2520provide%2520step-by-step%2520explanations%2520for%2520the%2520computation%2520of%2520the%2520posterior%2520probability%2520as%2520well%2520as%2520describe%2520factors%2520that%2520determine%2520the%2520feasibility%2520of%2520practically%2520applying%2520this%2520computation.%2520The%2520dead%2520leaves%2520image%2520model%2520and%2520the%2520associated%2520ideal%2520observer%2520can%2520be%2520applied%2520to%2520study%2520segmentation%2520decisions%2520in%2520a%2520limited%2520number%2520of%2520pixels%252C%2520providing%2520a%2520principled%2520upper-bound%2520on%2520performance%252C%2520to%2520which%2520humans%2520and%2520vision%2520algorithms%2520could%2520be%2520compared.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.05539v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ideal%20Observer%20for%20Segmentation%20of%20Dead%20Leaves%20Images&entry.906535625=Swantje%20Mahncke%20and%20Malte%20Ott&entry.1292438233=The%20human%20visual%20environment%20is%20comprised%20of%20different%20surfaces%20that%20are%20distributed%20in%20space.%20The%20parts%20of%20a%20scene%20that%20are%20visible%20at%20any%20one%20time%20are%20governed%20by%20the%20occlusion%20of%20overlapping%20objects.%20In%20this%20work%20we%20consider%20%22dead%20leaves%22%20models%2C%20which%20replicate%20these%20occlusions%20when%20generating%20images%20by%20layering%20objects%20on%20top%20of%20each%20other.%20A%20dead%20leaves%20model%20is%20a%20generative%20model%20comprised%20of%20distributions%20for%20object%20position%2C%20shape%2C%20color%20and%20texture.%20An%20image%20is%20generated%20from%20a%20dead%20leaves%20model%20by%20sampling%20objects%20%28%22leaves%22%29%20from%20these%20distributions%20until%20a%20stopping%20criterion%20is%20reached%2C%20usually%20when%20the%20image%20is%20fully%20covered%20or%20until%20a%20given%20number%20of%20leaves%20was%20sampled.%20Here%2C%20we%20describe%20a%20theoretical%20approach%2C%20based%20on%20previous%20work%2C%20to%20derive%20a%20Bayesian%20ideal%20observer%20for%20the%20partition%20of%20a%20given%20set%20of%20pixels%20based%20on%20independent%20dead%20leaves%20model%20distributions.%20Extending%20previous%20work%2C%20we%20provide%20step-by-step%20explanations%20for%20the%20computation%20of%20the%20posterior%20probability%20as%20well%20as%20describe%20factors%20that%20determine%20the%20feasibility%20of%20practically%20applying%20this%20computation.%20The%20dead%20leaves%20image%20model%20and%20the%20associated%20ideal%20observer%20can%20be%20applied%20to%20study%20segmentation%20decisions%20in%20a%20limited%20number%20of%20pixels%2C%20providing%20a%20principled%20upper-bound%20on%20performance%2C%20to%20which%20humans%20and%20vision%20algorithms%20could%20be%20compared.&entry.1838667208=http%3A//arxiv.org/abs/2512.05539v2&entry.124074799=Read"},
{"title": "XAI-MeD: Explainable Knowledge Guided Neuro-Symbolic Framework for Domain Generalization and Rare Class Detection in Medical Imaging", "author": "Midhat Urooj and Ayan Banerjee and Sandeep Gupta", "abstract": "Explainability domain generalization and rare class reliability are critical challenges in medical AI where deep models often fail under real world distribution shifts and exhibit bias against infrequent clinical conditions This paper introduces XAIMeD an explainable medical AI framework that integrates clinically accurate expert knowledge into deep learning through a unified neuro symbolic architecture XAIMeD is designed to improve robustness under distribution shift enhance rare class sensitivity and deliver transparent clinically aligned interpretations The framework encodes clinical expertise as logical connectives over atomic medical propositions transforming them into machine checkable class specific rules Their diagnostic utility is quantified through weighted feature satisfaction scores enabling a symbolic reasoning branch that complements neural predictions A confidence weighted fusion integrates symbolic and deep outputs while a Hunt inspired adaptive routing mechanism guided by Entropy Imbalance Gain EIG and Rare Class Gini mitigates class imbalance high intra class variability and uncertainty We evaluate XAIMeD across diverse modalities on four challenging tasks i Seizure Onset Zone SOZ localization from rs fMRI ii Diabetic Retinopathy grading across 6 multicenter datasets demonstrate substantial performance improvements including 6 percent gains in cross domain generalization and a 10 percent improved rare class F1 score far outperforming state of the art deep learning baselines Ablation studies confirm that the clinically grounded symbolic components act as effective regularizers ensuring robustness to distribution shifts XAIMeD thus provides a principled clinically faithful and interpretable approach to multimodal medical AI.", "link": "http://arxiv.org/abs/2601.02008v1", "date": "2026-01-05", "relevancy": 2.0686, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5801}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.509}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XAI-MeD%3A%20Explainable%20Knowledge%20Guided%20Neuro-Symbolic%20Framework%20for%20Domain%20Generalization%20and%20Rare%20Class%20Detection%20in%20Medical%20Imaging&body=Title%3A%20XAI-MeD%3A%20Explainable%20Knowledge%20Guided%20Neuro-Symbolic%20Framework%20for%20Domain%20Generalization%20and%20Rare%20Class%20Detection%20in%20Medical%20Imaging%0AAuthor%3A%20Midhat%20Urooj%20and%20Ayan%20Banerjee%20and%20Sandeep%20Gupta%0AAbstract%3A%20Explainability%20domain%20generalization%20and%20rare%20class%20reliability%20are%20critical%20challenges%20in%20medical%20AI%20where%20deep%20models%20often%20fail%20under%20real%20world%20distribution%20shifts%20and%20exhibit%20bias%20against%20infrequent%20clinical%20conditions%20This%20paper%20introduces%20XAIMeD%20an%20explainable%20medical%20AI%20framework%20that%20integrates%20clinically%20accurate%20expert%20knowledge%20into%20deep%20learning%20through%20a%20unified%20neuro%20symbolic%20architecture%20XAIMeD%20is%20designed%20to%20improve%20robustness%20under%20distribution%20shift%20enhance%20rare%20class%20sensitivity%20and%20deliver%20transparent%20clinically%20aligned%20interpretations%20The%20framework%20encodes%20clinical%20expertise%20as%20logical%20connectives%20over%20atomic%20medical%20propositions%20transforming%20them%20into%20machine%20checkable%20class%20specific%20rules%20Their%20diagnostic%20utility%20is%20quantified%20through%20weighted%20feature%20satisfaction%20scores%20enabling%20a%20symbolic%20reasoning%20branch%20that%20complements%20neural%20predictions%20A%20confidence%20weighted%20fusion%20integrates%20symbolic%20and%20deep%20outputs%20while%20a%20Hunt%20inspired%20adaptive%20routing%20mechanism%20guided%20by%20Entropy%20Imbalance%20Gain%20EIG%20and%20Rare%20Class%20Gini%20mitigates%20class%20imbalance%20high%20intra%20class%20variability%20and%20uncertainty%20We%20evaluate%20XAIMeD%20across%20diverse%20modalities%20on%20four%20challenging%20tasks%20i%20Seizure%20Onset%20Zone%20SOZ%20localization%20from%20rs%20fMRI%20ii%20Diabetic%20Retinopathy%20grading%20across%206%20multicenter%20datasets%20demonstrate%20substantial%20performance%20improvements%20including%206%20percent%20gains%20in%20cross%20domain%20generalization%20and%20a%2010%20percent%20improved%20rare%20class%20F1%20score%20far%20outperforming%20state%20of%20the%20art%20deep%20learning%20baselines%20Ablation%20studies%20confirm%20that%20the%20clinically%20grounded%20symbolic%20components%20act%20as%20effective%20regularizers%20ensuring%20robustness%20to%20distribution%20shifts%20XAIMeD%20thus%20provides%20a%20principled%20clinically%20faithful%20and%20interpretable%20approach%20to%20multimodal%20medical%20AI.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02008v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXAI-MeD%253A%2520Explainable%2520Knowledge%2520Guided%2520Neuro-Symbolic%2520Framework%2520for%2520Domain%2520Generalization%2520and%2520Rare%2520Class%2520Detection%2520in%2520Medical%2520Imaging%26entry.906535625%3DMidhat%2520Urooj%2520and%2520Ayan%2520Banerjee%2520and%2520Sandeep%2520Gupta%26entry.1292438233%3DExplainability%2520domain%2520generalization%2520and%2520rare%2520class%2520reliability%2520are%2520critical%2520challenges%2520in%2520medical%2520AI%2520where%2520deep%2520models%2520often%2520fail%2520under%2520real%2520world%2520distribution%2520shifts%2520and%2520exhibit%2520bias%2520against%2520infrequent%2520clinical%2520conditions%2520This%2520paper%2520introduces%2520XAIMeD%2520an%2520explainable%2520medical%2520AI%2520framework%2520that%2520integrates%2520clinically%2520accurate%2520expert%2520knowledge%2520into%2520deep%2520learning%2520through%2520a%2520unified%2520neuro%2520symbolic%2520architecture%2520XAIMeD%2520is%2520designed%2520to%2520improve%2520robustness%2520under%2520distribution%2520shift%2520enhance%2520rare%2520class%2520sensitivity%2520and%2520deliver%2520transparent%2520clinically%2520aligned%2520interpretations%2520The%2520framework%2520encodes%2520clinical%2520expertise%2520as%2520logical%2520connectives%2520over%2520atomic%2520medical%2520propositions%2520transforming%2520them%2520into%2520machine%2520checkable%2520class%2520specific%2520rules%2520Their%2520diagnostic%2520utility%2520is%2520quantified%2520through%2520weighted%2520feature%2520satisfaction%2520scores%2520enabling%2520a%2520symbolic%2520reasoning%2520branch%2520that%2520complements%2520neural%2520predictions%2520A%2520confidence%2520weighted%2520fusion%2520integrates%2520symbolic%2520and%2520deep%2520outputs%2520while%2520a%2520Hunt%2520inspired%2520adaptive%2520routing%2520mechanism%2520guided%2520by%2520Entropy%2520Imbalance%2520Gain%2520EIG%2520and%2520Rare%2520Class%2520Gini%2520mitigates%2520class%2520imbalance%2520high%2520intra%2520class%2520variability%2520and%2520uncertainty%2520We%2520evaluate%2520XAIMeD%2520across%2520diverse%2520modalities%2520on%2520four%2520challenging%2520tasks%2520i%2520Seizure%2520Onset%2520Zone%2520SOZ%2520localization%2520from%2520rs%2520fMRI%2520ii%2520Diabetic%2520Retinopathy%2520grading%2520across%25206%2520multicenter%2520datasets%2520demonstrate%2520substantial%2520performance%2520improvements%2520including%25206%2520percent%2520gains%2520in%2520cross%2520domain%2520generalization%2520and%2520a%252010%2520percent%2520improved%2520rare%2520class%2520F1%2520score%2520far%2520outperforming%2520state%2520of%2520the%2520art%2520deep%2520learning%2520baselines%2520Ablation%2520studies%2520confirm%2520that%2520the%2520clinically%2520grounded%2520symbolic%2520components%2520act%2520as%2520effective%2520regularizers%2520ensuring%2520robustness%2520to%2520distribution%2520shifts%2520XAIMeD%2520thus%2520provides%2520a%2520principled%2520clinically%2520faithful%2520and%2520interpretable%2520approach%2520to%2520multimodal%2520medical%2520AI.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02008v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XAI-MeD%3A%20Explainable%20Knowledge%20Guided%20Neuro-Symbolic%20Framework%20for%20Domain%20Generalization%20and%20Rare%20Class%20Detection%20in%20Medical%20Imaging&entry.906535625=Midhat%20Urooj%20and%20Ayan%20Banerjee%20and%20Sandeep%20Gupta&entry.1292438233=Explainability%20domain%20generalization%20and%20rare%20class%20reliability%20are%20critical%20challenges%20in%20medical%20AI%20where%20deep%20models%20often%20fail%20under%20real%20world%20distribution%20shifts%20and%20exhibit%20bias%20against%20infrequent%20clinical%20conditions%20This%20paper%20introduces%20XAIMeD%20an%20explainable%20medical%20AI%20framework%20that%20integrates%20clinically%20accurate%20expert%20knowledge%20into%20deep%20learning%20through%20a%20unified%20neuro%20symbolic%20architecture%20XAIMeD%20is%20designed%20to%20improve%20robustness%20under%20distribution%20shift%20enhance%20rare%20class%20sensitivity%20and%20deliver%20transparent%20clinically%20aligned%20interpretations%20The%20framework%20encodes%20clinical%20expertise%20as%20logical%20connectives%20over%20atomic%20medical%20propositions%20transforming%20them%20into%20machine%20checkable%20class%20specific%20rules%20Their%20diagnostic%20utility%20is%20quantified%20through%20weighted%20feature%20satisfaction%20scores%20enabling%20a%20symbolic%20reasoning%20branch%20that%20complements%20neural%20predictions%20A%20confidence%20weighted%20fusion%20integrates%20symbolic%20and%20deep%20outputs%20while%20a%20Hunt%20inspired%20adaptive%20routing%20mechanism%20guided%20by%20Entropy%20Imbalance%20Gain%20EIG%20and%20Rare%20Class%20Gini%20mitigates%20class%20imbalance%20high%20intra%20class%20variability%20and%20uncertainty%20We%20evaluate%20XAIMeD%20across%20diverse%20modalities%20on%20four%20challenging%20tasks%20i%20Seizure%20Onset%20Zone%20SOZ%20localization%20from%20rs%20fMRI%20ii%20Diabetic%20Retinopathy%20grading%20across%206%20multicenter%20datasets%20demonstrate%20substantial%20performance%20improvements%20including%206%20percent%20gains%20in%20cross%20domain%20generalization%20and%20a%2010%20percent%20improved%20rare%20class%20F1%20score%20far%20outperforming%20state%20of%20the%20art%20deep%20learning%20baselines%20Ablation%20studies%20confirm%20that%20the%20clinically%20grounded%20symbolic%20components%20act%20as%20effective%20regularizers%20ensuring%20robustness%20to%20distribution%20shifts%20XAIMeD%20thus%20provides%20a%20principled%20clinically%20faithful%20and%20interpretable%20approach%20to%20multimodal%20medical%20AI.&entry.1838667208=http%3A//arxiv.org/abs/2601.02008v1&entry.124074799=Read"},
{"title": "Cost-Efficient Cross-Lingual Retrieval-Augmented Generation for Low-Resource Languages: A Case Study in Bengali Agricultural Advisory", "author": "Md. Asif Hossain and Nabil Subhan and Mantasha Rahman Mahi and Jannatul Ferdous Nabila", "abstract": "Access to reliable agricultural advisory remains limited in many developing regions due to a persistent language barrier: authoritative agricultural manuals are predominantly written in English, while farmers primarily communicate in low-resource local languages such as Bengali. Although recent advances in Large Language Models (LLMs) enable natural language interaction, direct generation in low-resource languages often exhibits poor fluency and factual inconsistency, while cloud-based solutions remain cost-prohibitive. This paper presents a cost-efficient, cross-lingual Retrieval-Augmented Generation (RAG) framework for Bengali agricultural advisory that emphasizes factual grounding and practical deployability. The proposed system adopts a translation-centric architecture in which Bengali user queries are translated into English, enriched through domain-specific keyword injection to align colloquial farmer terminology with scientific nomenclature, and answered via dense vector retrieval over a curated corpus of English agricultural manuals (FAO, IRRI). The generated English response is subsequently translated back into Bengali to ensure accessibility. The system is implemented entirely using open-source models and operates on consumer-grade hardware without reliance on paid APIs. Experimental evaluation demonstrates reliable source-grounded responses, robust rejection of out-of-domain queries, and an average end-to-end latency below 20 seconds. The results indicate that cross-lingual retrieval combined with controlled translation offers a practical and scalable solution for agricultural knowledge access in low-resource language settings", "link": "http://arxiv.org/abs/2601.02065v1", "date": "2026-01-05", "relevancy": 1.716, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4361}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4264}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cost-Efficient%20Cross-Lingual%20Retrieval-Augmented%20Generation%20for%20Low-Resource%20Languages%3A%20A%20Case%20Study%20in%20Bengali%20Agricultural%20Advisory&body=Title%3A%20Cost-Efficient%20Cross-Lingual%20Retrieval-Augmented%20Generation%20for%20Low-Resource%20Languages%3A%20A%20Case%20Study%20in%20Bengali%20Agricultural%20Advisory%0AAuthor%3A%20Md.%20Asif%20Hossain%20and%20Nabil%20Subhan%20and%20Mantasha%20Rahman%20Mahi%20and%20Jannatul%20Ferdous%20Nabila%0AAbstract%3A%20Access%20to%20reliable%20agricultural%20advisory%20remains%20limited%20in%20many%20developing%20regions%20due%20to%20a%20persistent%20language%20barrier%3A%20authoritative%20agricultural%20manuals%20are%20predominantly%20written%20in%20English%2C%20while%20farmers%20primarily%20communicate%20in%20low-resource%20local%20languages%20such%20as%20Bengali.%20Although%20recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20enable%20natural%20language%20interaction%2C%20direct%20generation%20in%20low-resource%20languages%20often%20exhibits%20poor%20fluency%20and%20factual%20inconsistency%2C%20while%20cloud-based%20solutions%20remain%20cost-prohibitive.%20This%20paper%20presents%20a%20cost-efficient%2C%20cross-lingual%20Retrieval-Augmented%20Generation%20%28RAG%29%20framework%20for%20Bengali%20agricultural%20advisory%20that%20emphasizes%20factual%20grounding%20and%20practical%20deployability.%20The%20proposed%20system%20adopts%20a%20translation-centric%20architecture%20in%20which%20Bengali%20user%20queries%20are%20translated%20into%20English%2C%20enriched%20through%20domain-specific%20keyword%20injection%20to%20align%20colloquial%20farmer%20terminology%20with%20scientific%20nomenclature%2C%20and%20answered%20via%20dense%20vector%20retrieval%20over%20a%20curated%20corpus%20of%20English%20agricultural%20manuals%20%28FAO%2C%20IRRI%29.%20The%20generated%20English%20response%20is%20subsequently%20translated%20back%20into%20Bengali%20to%20ensure%20accessibility.%20The%20system%20is%20implemented%20entirely%20using%20open-source%20models%20and%20operates%20on%20consumer-grade%20hardware%20without%20reliance%20on%20paid%20APIs.%20Experimental%20evaluation%20demonstrates%20reliable%20source-grounded%20responses%2C%20robust%20rejection%20of%20out-of-domain%20queries%2C%20and%20an%20average%20end-to-end%20latency%20below%2020%20seconds.%20The%20results%20indicate%20that%20cross-lingual%20retrieval%20combined%20with%20controlled%20translation%20offers%20a%20practical%20and%20scalable%20solution%20for%20agricultural%20knowledge%20access%20in%20low-resource%20language%20settings%0ALink%3A%20http%3A//arxiv.org/abs/2601.02065v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCost-Efficient%2520Cross-Lingual%2520Retrieval-Augmented%2520Generation%2520for%2520Low-Resource%2520Languages%253A%2520A%2520Case%2520Study%2520in%2520Bengali%2520Agricultural%2520Advisory%26entry.906535625%3DMd.%2520Asif%2520Hossain%2520and%2520Nabil%2520Subhan%2520and%2520Mantasha%2520Rahman%2520Mahi%2520and%2520Jannatul%2520Ferdous%2520Nabila%26entry.1292438233%3DAccess%2520to%2520reliable%2520agricultural%2520advisory%2520remains%2520limited%2520in%2520many%2520developing%2520regions%2520due%2520to%2520a%2520persistent%2520language%2520barrier%253A%2520authoritative%2520agricultural%2520manuals%2520are%2520predominantly%2520written%2520in%2520English%252C%2520while%2520farmers%2520primarily%2520communicate%2520in%2520low-resource%2520local%2520languages%2520such%2520as%2520Bengali.%2520Although%2520recent%2520advances%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520enable%2520natural%2520language%2520interaction%252C%2520direct%2520generation%2520in%2520low-resource%2520languages%2520often%2520exhibits%2520poor%2520fluency%2520and%2520factual%2520inconsistency%252C%2520while%2520cloud-based%2520solutions%2520remain%2520cost-prohibitive.%2520This%2520paper%2520presents%2520a%2520cost-efficient%252C%2520cross-lingual%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520framework%2520for%2520Bengali%2520agricultural%2520advisory%2520that%2520emphasizes%2520factual%2520grounding%2520and%2520practical%2520deployability.%2520The%2520proposed%2520system%2520adopts%2520a%2520translation-centric%2520architecture%2520in%2520which%2520Bengali%2520user%2520queries%2520are%2520translated%2520into%2520English%252C%2520enriched%2520through%2520domain-specific%2520keyword%2520injection%2520to%2520align%2520colloquial%2520farmer%2520terminology%2520with%2520scientific%2520nomenclature%252C%2520and%2520answered%2520via%2520dense%2520vector%2520retrieval%2520over%2520a%2520curated%2520corpus%2520of%2520English%2520agricultural%2520manuals%2520%2528FAO%252C%2520IRRI%2529.%2520The%2520generated%2520English%2520response%2520is%2520subsequently%2520translated%2520back%2520into%2520Bengali%2520to%2520ensure%2520accessibility.%2520The%2520system%2520is%2520implemented%2520entirely%2520using%2520open-source%2520models%2520and%2520operates%2520on%2520consumer-grade%2520hardware%2520without%2520reliance%2520on%2520paid%2520APIs.%2520Experimental%2520evaluation%2520demonstrates%2520reliable%2520source-grounded%2520responses%252C%2520robust%2520rejection%2520of%2520out-of-domain%2520queries%252C%2520and%2520an%2520average%2520end-to-end%2520latency%2520below%252020%2520seconds.%2520The%2520results%2520indicate%2520that%2520cross-lingual%2520retrieval%2520combined%2520with%2520controlled%2520translation%2520offers%2520a%2520practical%2520and%2520scalable%2520solution%2520for%2520agricultural%2520knowledge%2520access%2520in%2520low-resource%2520language%2520settings%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02065v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cost-Efficient%20Cross-Lingual%20Retrieval-Augmented%20Generation%20for%20Low-Resource%20Languages%3A%20A%20Case%20Study%20in%20Bengali%20Agricultural%20Advisory&entry.906535625=Md.%20Asif%20Hossain%20and%20Nabil%20Subhan%20and%20Mantasha%20Rahman%20Mahi%20and%20Jannatul%20Ferdous%20Nabila&entry.1292438233=Access%20to%20reliable%20agricultural%20advisory%20remains%20limited%20in%20many%20developing%20regions%20due%20to%20a%20persistent%20language%20barrier%3A%20authoritative%20agricultural%20manuals%20are%20predominantly%20written%20in%20English%2C%20while%20farmers%20primarily%20communicate%20in%20low-resource%20local%20languages%20such%20as%20Bengali.%20Although%20recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20enable%20natural%20language%20interaction%2C%20direct%20generation%20in%20low-resource%20languages%20often%20exhibits%20poor%20fluency%20and%20factual%20inconsistency%2C%20while%20cloud-based%20solutions%20remain%20cost-prohibitive.%20This%20paper%20presents%20a%20cost-efficient%2C%20cross-lingual%20Retrieval-Augmented%20Generation%20%28RAG%29%20framework%20for%20Bengali%20agricultural%20advisory%20that%20emphasizes%20factual%20grounding%20and%20practical%20deployability.%20The%20proposed%20system%20adopts%20a%20translation-centric%20architecture%20in%20which%20Bengali%20user%20queries%20are%20translated%20into%20English%2C%20enriched%20through%20domain-specific%20keyword%20injection%20to%20align%20colloquial%20farmer%20terminology%20with%20scientific%20nomenclature%2C%20and%20answered%20via%20dense%20vector%20retrieval%20over%20a%20curated%20corpus%20of%20English%20agricultural%20manuals%20%28FAO%2C%20IRRI%29.%20The%20generated%20English%20response%20is%20subsequently%20translated%20back%20into%20Bengali%20to%20ensure%20accessibility.%20The%20system%20is%20implemented%20entirely%20using%20open-source%20models%20and%20operates%20on%20consumer-grade%20hardware%20without%20reliance%20on%20paid%20APIs.%20Experimental%20evaluation%20demonstrates%20reliable%20source-grounded%20responses%2C%20robust%20rejection%20of%20out-of-domain%20queries%2C%20and%20an%20average%20end-to-end%20latency%20below%2020%20seconds.%20The%20results%20indicate%20that%20cross-lingual%20retrieval%20combined%20with%20controlled%20translation%20offers%20a%20practical%20and%20scalable%20solution%20for%20agricultural%20knowledge%20access%20in%20low-resource%20language%20settings&entry.1838667208=http%3A//arxiv.org/abs/2601.02065v1&entry.124074799=Read"},
{"title": "CangLing-KnowFlow: A Unified Knowledge-and-Flow-fused Agent for Comprehensive Remote Sensing Applications", "author": "Zhengchao Chen and Haoran Wang and Jing Yao and Pedram Ghamisi and Jun Zhou and Peter M. Atkinson and Bing Zhang", "abstract": "The automated and intelligent processing of massive remote sensing (RS) datasets is critical in Earth observation (EO). Existing automated systems are normally task-specific, lacking a unified framework to manage diverse, end-to-end workflows--from data preprocessing to advanced interpretation--across diverse RS applications. To address this gap, this paper introduces CangLing-KnowFlow, a unified intelligent agent framework that integrates a Procedural Knowledge Base (PKB), Dynamic Workflow Adjustment, and an Evolutionary Memory Module. The PKB, comprising 1,008 expert-validated workflow cases across 162 practical RS tasks, guides planning and substantially reduces hallucinations common in general-purpose agents. During runtime failures, the Dynamic Workflow Adjustment autonomously diagnoses and replans recovery strategies, while the Evolutionary Memory Module continuously learns from these events, iteratively enhancing the agent's knowledge and performance. This synergy enables CangLing-KnowFlow to adapt, learn, and operate reliably across diverse, complex tasks. We evaluated CangLing-KnowFlow on the KnowFlow-Bench, a novel benchmark of 324 workflows inspired by real-world applications, testing its performance across 13 top Large Language Model (LLM) backbones, from open-source to commercial. Across all complex tasks, CangLing-KnowFlow surpassed the Reflexion baseline by at least 4% in Task Success Rate. As the first most comprehensive validation along this emerging field, this research demonstrates the great potential of CangLing-KnowFlow as a robust, efficient, and scalable automated solution for complex EO challenges by leveraging expert knowledge (Knowledge) into adaptive and verifiable procedures (Flow).", "link": "http://arxiv.org/abs/2512.15231v2", "date": "2026-01-05", "relevancy": 1.5164, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5392}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5079}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CangLing-KnowFlow%3A%20A%20Unified%20Knowledge-and-Flow-fused%20Agent%20for%20Comprehensive%20Remote%20Sensing%20Applications&body=Title%3A%20CangLing-KnowFlow%3A%20A%20Unified%20Knowledge-and-Flow-fused%20Agent%20for%20Comprehensive%20Remote%20Sensing%20Applications%0AAuthor%3A%20Zhengchao%20Chen%20and%20Haoran%20Wang%20and%20Jing%20Yao%20and%20Pedram%20Ghamisi%20and%20Jun%20Zhou%20and%20Peter%20M.%20Atkinson%20and%20Bing%20Zhang%0AAbstract%3A%20The%20automated%20and%20intelligent%20processing%20of%20massive%20remote%20sensing%20%28RS%29%20datasets%20is%20critical%20in%20Earth%20observation%20%28EO%29.%20Existing%20automated%20systems%20are%20normally%20task-specific%2C%20lacking%20a%20unified%20framework%20to%20manage%20diverse%2C%20end-to-end%20workflows--from%20data%20preprocessing%20to%20advanced%20interpretation--across%20diverse%20RS%20applications.%20To%20address%20this%20gap%2C%20this%20paper%20introduces%20CangLing-KnowFlow%2C%20a%20unified%20intelligent%20agent%20framework%20that%20integrates%20a%20Procedural%20Knowledge%20Base%20%28PKB%29%2C%20Dynamic%20Workflow%20Adjustment%2C%20and%20an%20Evolutionary%20Memory%20Module.%20The%20PKB%2C%20comprising%201%2C008%20expert-validated%20workflow%20cases%20across%20162%20practical%20RS%20tasks%2C%20guides%20planning%20and%20substantially%20reduces%20hallucinations%20common%20in%20general-purpose%20agents.%20During%20runtime%20failures%2C%20the%20Dynamic%20Workflow%20Adjustment%20autonomously%20diagnoses%20and%20replans%20recovery%20strategies%2C%20while%20the%20Evolutionary%20Memory%20Module%20continuously%20learns%20from%20these%20events%2C%20iteratively%20enhancing%20the%20agent%27s%20knowledge%20and%20performance.%20This%20synergy%20enables%20CangLing-KnowFlow%20to%20adapt%2C%20learn%2C%20and%20operate%20reliably%20across%20diverse%2C%20complex%20tasks.%20We%20evaluated%20CangLing-KnowFlow%20on%20the%20KnowFlow-Bench%2C%20a%20novel%20benchmark%20of%20324%20workflows%20inspired%20by%20real-world%20applications%2C%20testing%20its%20performance%20across%2013%20top%20Large%20Language%20Model%20%28LLM%29%20backbones%2C%20from%20open-source%20to%20commercial.%20Across%20all%20complex%20tasks%2C%20CangLing-KnowFlow%20surpassed%20the%20Reflexion%20baseline%20by%20at%20least%204%25%20in%20Task%20Success%20Rate.%20As%20the%20first%20most%20comprehensive%20validation%20along%20this%20emerging%20field%2C%20this%20research%20demonstrates%20the%20great%20potential%20of%20CangLing-KnowFlow%20as%20a%20robust%2C%20efficient%2C%20and%20scalable%20automated%20solution%20for%20complex%20EO%20challenges%20by%20leveraging%20expert%20knowledge%20%28Knowledge%29%20into%20adaptive%20and%20verifiable%20procedures%20%28Flow%29.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15231v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCangLing-KnowFlow%253A%2520A%2520Unified%2520Knowledge-and-Flow-fused%2520Agent%2520for%2520Comprehensive%2520Remote%2520Sensing%2520Applications%26entry.906535625%3DZhengchao%2520Chen%2520and%2520Haoran%2520Wang%2520and%2520Jing%2520Yao%2520and%2520Pedram%2520Ghamisi%2520and%2520Jun%2520Zhou%2520and%2520Peter%2520M.%2520Atkinson%2520and%2520Bing%2520Zhang%26entry.1292438233%3DThe%2520automated%2520and%2520intelligent%2520processing%2520of%2520massive%2520remote%2520sensing%2520%2528RS%2529%2520datasets%2520is%2520critical%2520in%2520Earth%2520observation%2520%2528EO%2529.%2520Existing%2520automated%2520systems%2520are%2520normally%2520task-specific%252C%2520lacking%2520a%2520unified%2520framework%2520to%2520manage%2520diverse%252C%2520end-to-end%2520workflows--from%2520data%2520preprocessing%2520to%2520advanced%2520interpretation--across%2520diverse%2520RS%2520applications.%2520To%2520address%2520this%2520gap%252C%2520this%2520paper%2520introduces%2520CangLing-KnowFlow%252C%2520a%2520unified%2520intelligent%2520agent%2520framework%2520that%2520integrates%2520a%2520Procedural%2520Knowledge%2520Base%2520%2528PKB%2529%252C%2520Dynamic%2520Workflow%2520Adjustment%252C%2520and%2520an%2520Evolutionary%2520Memory%2520Module.%2520The%2520PKB%252C%2520comprising%25201%252C008%2520expert-validated%2520workflow%2520cases%2520across%2520162%2520practical%2520RS%2520tasks%252C%2520guides%2520planning%2520and%2520substantially%2520reduces%2520hallucinations%2520common%2520in%2520general-purpose%2520agents.%2520During%2520runtime%2520failures%252C%2520the%2520Dynamic%2520Workflow%2520Adjustment%2520autonomously%2520diagnoses%2520and%2520replans%2520recovery%2520strategies%252C%2520while%2520the%2520Evolutionary%2520Memory%2520Module%2520continuously%2520learns%2520from%2520these%2520events%252C%2520iteratively%2520enhancing%2520the%2520agent%2527s%2520knowledge%2520and%2520performance.%2520This%2520synergy%2520enables%2520CangLing-KnowFlow%2520to%2520adapt%252C%2520learn%252C%2520and%2520operate%2520reliably%2520across%2520diverse%252C%2520complex%2520tasks.%2520We%2520evaluated%2520CangLing-KnowFlow%2520on%2520the%2520KnowFlow-Bench%252C%2520a%2520novel%2520benchmark%2520of%2520324%2520workflows%2520inspired%2520by%2520real-world%2520applications%252C%2520testing%2520its%2520performance%2520across%252013%2520top%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520backbones%252C%2520from%2520open-source%2520to%2520commercial.%2520Across%2520all%2520complex%2520tasks%252C%2520CangLing-KnowFlow%2520surpassed%2520the%2520Reflexion%2520baseline%2520by%2520at%2520least%25204%2525%2520in%2520Task%2520Success%2520Rate.%2520As%2520the%2520first%2520most%2520comprehensive%2520validation%2520along%2520this%2520emerging%2520field%252C%2520this%2520research%2520demonstrates%2520the%2520great%2520potential%2520of%2520CangLing-KnowFlow%2520as%2520a%2520robust%252C%2520efficient%252C%2520and%2520scalable%2520automated%2520solution%2520for%2520complex%2520EO%2520challenges%2520by%2520leveraging%2520expert%2520knowledge%2520%2528Knowledge%2529%2520into%2520adaptive%2520and%2520verifiable%2520procedures%2520%2528Flow%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15231v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CangLing-KnowFlow%3A%20A%20Unified%20Knowledge-and-Flow-fused%20Agent%20for%20Comprehensive%20Remote%20Sensing%20Applications&entry.906535625=Zhengchao%20Chen%20and%20Haoran%20Wang%20and%20Jing%20Yao%20and%20Pedram%20Ghamisi%20and%20Jun%20Zhou%20and%20Peter%20M.%20Atkinson%20and%20Bing%20Zhang&entry.1292438233=The%20automated%20and%20intelligent%20processing%20of%20massive%20remote%20sensing%20%28RS%29%20datasets%20is%20critical%20in%20Earth%20observation%20%28EO%29.%20Existing%20automated%20systems%20are%20normally%20task-specific%2C%20lacking%20a%20unified%20framework%20to%20manage%20diverse%2C%20end-to-end%20workflows--from%20data%20preprocessing%20to%20advanced%20interpretation--across%20diverse%20RS%20applications.%20To%20address%20this%20gap%2C%20this%20paper%20introduces%20CangLing-KnowFlow%2C%20a%20unified%20intelligent%20agent%20framework%20that%20integrates%20a%20Procedural%20Knowledge%20Base%20%28PKB%29%2C%20Dynamic%20Workflow%20Adjustment%2C%20and%20an%20Evolutionary%20Memory%20Module.%20The%20PKB%2C%20comprising%201%2C008%20expert-validated%20workflow%20cases%20across%20162%20practical%20RS%20tasks%2C%20guides%20planning%20and%20substantially%20reduces%20hallucinations%20common%20in%20general-purpose%20agents.%20During%20runtime%20failures%2C%20the%20Dynamic%20Workflow%20Adjustment%20autonomously%20diagnoses%20and%20replans%20recovery%20strategies%2C%20while%20the%20Evolutionary%20Memory%20Module%20continuously%20learns%20from%20these%20events%2C%20iteratively%20enhancing%20the%20agent%27s%20knowledge%20and%20performance.%20This%20synergy%20enables%20CangLing-KnowFlow%20to%20adapt%2C%20learn%2C%20and%20operate%20reliably%20across%20diverse%2C%20complex%20tasks.%20We%20evaluated%20CangLing-KnowFlow%20on%20the%20KnowFlow-Bench%2C%20a%20novel%20benchmark%20of%20324%20workflows%20inspired%20by%20real-world%20applications%2C%20testing%20its%20performance%20across%2013%20top%20Large%20Language%20Model%20%28LLM%29%20backbones%2C%20from%20open-source%20to%20commercial.%20Across%20all%20complex%20tasks%2C%20CangLing-KnowFlow%20surpassed%20the%20Reflexion%20baseline%20by%20at%20least%204%25%20in%20Task%20Success%20Rate.%20As%20the%20first%20most%20comprehensive%20validation%20along%20this%20emerging%20field%2C%20this%20research%20demonstrates%20the%20great%20potential%20of%20CangLing-KnowFlow%20as%20a%20robust%2C%20efficient%2C%20and%20scalable%20automated%20solution%20for%20complex%20EO%20challenges%20by%20leveraging%20expert%20knowledge%20%28Knowledge%29%20into%20adaptive%20and%20verifiable%20procedures%20%28Flow%29.&entry.1838667208=http%3A//arxiv.org/abs/2512.15231v2&entry.124074799=Read"},
{"title": "One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents", "author": "Zhaoxi Zhang and Yitong Duan and Yanzhi Zhang and Yiming Xu and Jiyan He and Yunfang Wu", "abstract": "Locating the files and functions requiring modification in large open-source software (OSS) repositories is challenging due to their scale and structural complexity. Existing large language model (LLM)-based methods typically treat this as a repository-level retrieval task and rely on multiple auxiliary tools, which overlook code execution logic and complicate model control. We propose RepoNavigator, an LLM agent equipped with a single execution-aware tool-jumping to the definition of an invoked symbol. This unified design reflects the actual flow of code execution while simplifying tool manipulation. RepoNavigator is trained end-to-end via Reinforcement Learning (RL) directly from a pretrained model, without any closed-source distillation. Experiments demonstrate that RL-trained RepoNavigator achieves state-of-the-art performance, with the 7B model outperforming 14B baselines, the 14B model surpassing 32B competitors, and even the 32B model exceeding closed-source models such as Claude-3.7. These results confirm that integrating a single, structurally grounded tool with RL training provides an efficient and scalable solution for repository-level issue localization.", "link": "http://arxiv.org/abs/2512.20957v3", "date": "2026-01-05", "relevancy": 1.8717, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4897}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.467}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20Tool%20Is%20Enough%3A%20Reinforcement%20Learning%20for%20Repository-Level%20LLM%20Agents&body=Title%3A%20One%20Tool%20Is%20Enough%3A%20Reinforcement%20Learning%20for%20Repository-Level%20LLM%20Agents%0AAuthor%3A%20Zhaoxi%20Zhang%20and%20Yitong%20Duan%20and%20Yanzhi%20Zhang%20and%20Yiming%20Xu%20and%20Jiyan%20He%20and%20Yunfang%20Wu%0AAbstract%3A%20Locating%20the%20files%20and%20functions%20requiring%20modification%20in%20large%20open-source%20software%20%28OSS%29%20repositories%20is%20challenging%20due%20to%20their%20scale%20and%20structural%20complexity.%20Existing%20large%20language%20model%20%28LLM%29-based%20methods%20typically%20treat%20this%20as%20a%20repository-level%20retrieval%20task%20and%20rely%20on%20multiple%20auxiliary%20tools%2C%20which%20overlook%20code%20execution%20logic%20and%20complicate%20model%20control.%20We%20propose%20RepoNavigator%2C%20an%20LLM%20agent%20equipped%20with%20a%20single%20execution-aware%20tool-jumping%20to%20the%20definition%20of%20an%20invoked%20symbol.%20This%20unified%20design%20reflects%20the%20actual%20flow%20of%20code%20execution%20while%20simplifying%20tool%20manipulation.%20RepoNavigator%20is%20trained%20end-to-end%20via%20Reinforcement%20Learning%20%28RL%29%20directly%20from%20a%20pretrained%20model%2C%20without%20any%20closed-source%20distillation.%20Experiments%20demonstrate%20that%20RL-trained%20RepoNavigator%20achieves%20state-of-the-art%20performance%2C%20with%20the%207B%20model%20outperforming%2014B%20baselines%2C%20the%2014B%20model%20surpassing%2032B%20competitors%2C%20and%20even%20the%2032B%20model%20exceeding%20closed-source%20models%20such%20as%20Claude-3.7.%20These%20results%20confirm%20that%20integrating%20a%20single%2C%20structurally%20grounded%20tool%20with%20RL%20training%20provides%20an%20efficient%20and%20scalable%20solution%20for%20repository-level%20issue%20localization.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20957v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520Tool%2520Is%2520Enough%253A%2520Reinforcement%2520Learning%2520for%2520Repository-Level%2520LLM%2520Agents%26entry.906535625%3DZhaoxi%2520Zhang%2520and%2520Yitong%2520Duan%2520and%2520Yanzhi%2520Zhang%2520and%2520Yiming%2520Xu%2520and%2520Jiyan%2520He%2520and%2520Yunfang%2520Wu%26entry.1292438233%3DLocating%2520the%2520files%2520and%2520functions%2520requiring%2520modification%2520in%2520large%2520open-source%2520software%2520%2528OSS%2529%2520repositories%2520is%2520challenging%2520due%2520to%2520their%2520scale%2520and%2520structural%2520complexity.%2520Existing%2520large%2520language%2520model%2520%2528LLM%2529-based%2520methods%2520typically%2520treat%2520this%2520as%2520a%2520repository-level%2520retrieval%2520task%2520and%2520rely%2520on%2520multiple%2520auxiliary%2520tools%252C%2520which%2520overlook%2520code%2520execution%2520logic%2520and%2520complicate%2520model%2520control.%2520We%2520propose%2520RepoNavigator%252C%2520an%2520LLM%2520agent%2520equipped%2520with%2520a%2520single%2520execution-aware%2520tool-jumping%2520to%2520the%2520definition%2520of%2520an%2520invoked%2520symbol.%2520This%2520unified%2520design%2520reflects%2520the%2520actual%2520flow%2520of%2520code%2520execution%2520while%2520simplifying%2520tool%2520manipulation.%2520RepoNavigator%2520is%2520trained%2520end-to-end%2520via%2520Reinforcement%2520Learning%2520%2528RL%2529%2520directly%2520from%2520a%2520pretrained%2520model%252C%2520without%2520any%2520closed-source%2520distillation.%2520Experiments%2520demonstrate%2520that%2520RL-trained%2520RepoNavigator%2520achieves%2520state-of-the-art%2520performance%252C%2520with%2520the%25207B%2520model%2520outperforming%252014B%2520baselines%252C%2520the%252014B%2520model%2520surpassing%252032B%2520competitors%252C%2520and%2520even%2520the%252032B%2520model%2520exceeding%2520closed-source%2520models%2520such%2520as%2520Claude-3.7.%2520These%2520results%2520confirm%2520that%2520integrating%2520a%2520single%252C%2520structurally%2520grounded%2520tool%2520with%2520RL%2520training%2520provides%2520an%2520efficient%2520and%2520scalable%2520solution%2520for%2520repository-level%2520issue%2520localization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20957v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20Tool%20Is%20Enough%3A%20Reinforcement%20Learning%20for%20Repository-Level%20LLM%20Agents&entry.906535625=Zhaoxi%20Zhang%20and%20Yitong%20Duan%20and%20Yanzhi%20Zhang%20and%20Yiming%20Xu%20and%20Jiyan%20He%20and%20Yunfang%20Wu&entry.1292438233=Locating%20the%20files%20and%20functions%20requiring%20modification%20in%20large%20open-source%20software%20%28OSS%29%20repositories%20is%20challenging%20due%20to%20their%20scale%20and%20structural%20complexity.%20Existing%20large%20language%20model%20%28LLM%29-based%20methods%20typically%20treat%20this%20as%20a%20repository-level%20retrieval%20task%20and%20rely%20on%20multiple%20auxiliary%20tools%2C%20which%20overlook%20code%20execution%20logic%20and%20complicate%20model%20control.%20We%20propose%20RepoNavigator%2C%20an%20LLM%20agent%20equipped%20with%20a%20single%20execution-aware%20tool-jumping%20to%20the%20definition%20of%20an%20invoked%20symbol.%20This%20unified%20design%20reflects%20the%20actual%20flow%20of%20code%20execution%20while%20simplifying%20tool%20manipulation.%20RepoNavigator%20is%20trained%20end-to-end%20via%20Reinforcement%20Learning%20%28RL%29%20directly%20from%20a%20pretrained%20model%2C%20without%20any%20closed-source%20distillation.%20Experiments%20demonstrate%20that%20RL-trained%20RepoNavigator%20achieves%20state-of-the-art%20performance%2C%20with%20the%207B%20model%20outperforming%2014B%20baselines%2C%20the%2014B%20model%20surpassing%2032B%20competitors%2C%20and%20even%20the%2032B%20model%20exceeding%20closed-source%20models%20such%20as%20Claude-3.7.%20These%20results%20confirm%20that%20integrating%20a%20single%2C%20structurally%20grounded%20tool%20with%20RL%20training%20provides%20an%20efficient%20and%20scalable%20solution%20for%20repository-level%20issue%20localization.&entry.1838667208=http%3A//arxiv.org/abs/2512.20957v3&entry.124074799=Read"},
{"title": "A Linear Approach to Data Poisoning", "author": "Donald Flynn and Diego Granziol", "abstract": "Backdoor and data-poisoning attacks can flip predictions with tiny training corruptions, yet a sharp theory linking poisoning strength, overparameterization, and regularization is lacking. We analyze ridge least squares with an unpenalized intercept in the high-dimensional regime \\(p,n\\to\\infty\\), \\(p/n\\to c\\). Targeted poisoning is modelled by shifting a \\(\u03b8\\)-fraction of one class by a direction \\(\\mathbf{v}\\) and relabelling. Using resolvent techniques and deterministic equivalents from random matrix theory, we derive closed-form limits for the poisoned score explicit in the model parameters. The formulas yield scaling laws, recover the interpolation threshold as \\(c\\to1\\) in the ridgeless limit, and show that the weights align with the poisoning direction. Synthetic experiments match theory across sweeps of the parameters and MNIST backdoor tests show qualitatively consistent trends. The results provide a tractable framework for quantifying poisoning in linear models.", "link": "http://arxiv.org/abs/2505.15175v3", "date": "2026-01-05", "relevancy": 1.6534, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4258}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4071}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Linear%20Approach%20to%20Data%20Poisoning&body=Title%3A%20A%20Linear%20Approach%20to%20Data%20Poisoning%0AAuthor%3A%20Donald%20Flynn%20and%20Diego%20Granziol%0AAbstract%3A%20Backdoor%20and%20data-poisoning%20attacks%20can%20flip%20predictions%20with%20tiny%20training%20corruptions%2C%20yet%20a%20sharp%20theory%20linking%20poisoning%20strength%2C%20overparameterization%2C%20and%20regularization%20is%20lacking.%20We%20analyze%20ridge%20least%20squares%20with%20an%20unpenalized%20intercept%20in%20the%20high-dimensional%20regime%20%5C%28p%2Cn%5Cto%5Cinfty%5C%29%2C%20%5C%28p/n%5Cto%20c%5C%29.%20Targeted%20poisoning%20is%20modelled%20by%20shifting%20a%20%5C%28%CE%B8%5C%29-fraction%20of%20one%20class%20by%20a%20direction%20%5C%28%5Cmathbf%7Bv%7D%5C%29%20and%20relabelling.%20Using%20resolvent%20techniques%20and%20deterministic%20equivalents%20from%20random%20matrix%20theory%2C%20we%20derive%20closed-form%20limits%20for%20the%20poisoned%20score%20explicit%20in%20the%20model%20parameters.%20The%20formulas%20yield%20scaling%20laws%2C%20recover%20the%20interpolation%20threshold%20as%20%5C%28c%5Cto1%5C%29%20in%20the%20ridgeless%20limit%2C%20and%20show%20that%20the%20weights%20align%20with%20the%20poisoning%20direction.%20Synthetic%20experiments%20match%20theory%20across%20sweeps%20of%20the%20parameters%20and%20MNIST%20backdoor%20tests%20show%20qualitatively%20consistent%20trends.%20The%20results%20provide%20a%20tractable%20framework%20for%20quantifying%20poisoning%20in%20linear%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2505.15175v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Linear%2520Approach%2520to%2520Data%2520Poisoning%26entry.906535625%3DDonald%2520Flynn%2520and%2520Diego%2520Granziol%26entry.1292438233%3DBackdoor%2520and%2520data-poisoning%2520attacks%2520can%2520flip%2520predictions%2520with%2520tiny%2520training%2520corruptions%252C%2520yet%2520a%2520sharp%2520theory%2520linking%2520poisoning%2520strength%252C%2520overparameterization%252C%2520and%2520regularization%2520is%2520lacking.%2520We%2520analyze%2520ridge%2520least%2520squares%2520with%2520an%2520unpenalized%2520intercept%2520in%2520the%2520high-dimensional%2520regime%2520%255C%2528p%252Cn%255Cto%255Cinfty%255C%2529%252C%2520%255C%2528p/n%255Cto%2520c%255C%2529.%2520Targeted%2520poisoning%2520is%2520modelled%2520by%2520shifting%2520a%2520%255C%2528%25CE%25B8%255C%2529-fraction%2520of%2520one%2520class%2520by%2520a%2520direction%2520%255C%2528%255Cmathbf%257Bv%257D%255C%2529%2520and%2520relabelling.%2520Using%2520resolvent%2520techniques%2520and%2520deterministic%2520equivalents%2520from%2520random%2520matrix%2520theory%252C%2520we%2520derive%2520closed-form%2520limits%2520for%2520the%2520poisoned%2520score%2520explicit%2520in%2520the%2520model%2520parameters.%2520The%2520formulas%2520yield%2520scaling%2520laws%252C%2520recover%2520the%2520interpolation%2520threshold%2520as%2520%255C%2528c%255Cto1%255C%2529%2520in%2520the%2520ridgeless%2520limit%252C%2520and%2520show%2520that%2520the%2520weights%2520align%2520with%2520the%2520poisoning%2520direction.%2520Synthetic%2520experiments%2520match%2520theory%2520across%2520sweeps%2520of%2520the%2520parameters%2520and%2520MNIST%2520backdoor%2520tests%2520show%2520qualitatively%2520consistent%2520trends.%2520The%2520results%2520provide%2520a%2520tractable%2520framework%2520for%2520quantifying%2520poisoning%2520in%2520linear%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15175v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Linear%20Approach%20to%20Data%20Poisoning&entry.906535625=Donald%20Flynn%20and%20Diego%20Granziol&entry.1292438233=Backdoor%20and%20data-poisoning%20attacks%20can%20flip%20predictions%20with%20tiny%20training%20corruptions%2C%20yet%20a%20sharp%20theory%20linking%20poisoning%20strength%2C%20overparameterization%2C%20and%20regularization%20is%20lacking.%20We%20analyze%20ridge%20least%20squares%20with%20an%20unpenalized%20intercept%20in%20the%20high-dimensional%20regime%20%5C%28p%2Cn%5Cto%5Cinfty%5C%29%2C%20%5C%28p/n%5Cto%20c%5C%29.%20Targeted%20poisoning%20is%20modelled%20by%20shifting%20a%20%5C%28%CE%B8%5C%29-fraction%20of%20one%20class%20by%20a%20direction%20%5C%28%5Cmathbf%7Bv%7D%5C%29%20and%20relabelling.%20Using%20resolvent%20techniques%20and%20deterministic%20equivalents%20from%20random%20matrix%20theory%2C%20we%20derive%20closed-form%20limits%20for%20the%20poisoned%20score%20explicit%20in%20the%20model%20parameters.%20The%20formulas%20yield%20scaling%20laws%2C%20recover%20the%20interpolation%20threshold%20as%20%5C%28c%5Cto1%5C%29%20in%20the%20ridgeless%20limit%2C%20and%20show%20that%20the%20weights%20align%20with%20the%20poisoning%20direction.%20Synthetic%20experiments%20match%20theory%20across%20sweeps%20of%20the%20parameters%20and%20MNIST%20backdoor%20tests%20show%20qualitatively%20consistent%20trends.%20The%20results%20provide%20a%20tractable%20framework%20for%20quantifying%20poisoning%20in%20linear%20models.&entry.1838667208=http%3A//arxiv.org/abs/2505.15175v3&entry.124074799=Read"},
{"title": "mHC: Manifold-Constrained Hyper-Connections", "author": "Zhenda Xie and Yixuan Wei and Huanqi Cao and Chenggang Zhao and Chengqi Deng and Jiashi Li and Damai Dai and Huazuo Gao and Jiang Chang and Kuai Yu and Liang Zhao and Shangyan Zhou and Zhean Xu and Zhengyan Zhang and Wangding Zeng and Shengding Hu and Yuqing Wang and Jingyang Yuan and Lean Wang and Wenfeng Liang", "abstract": "Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.", "link": "http://arxiv.org/abs/2512.24880v2", "date": "2026-01-05", "relevancy": 1.7975, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4535}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.447}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20mHC%3A%20Manifold-Constrained%20Hyper-Connections&body=Title%3A%20mHC%3A%20Manifold-Constrained%20Hyper-Connections%0AAuthor%3A%20Zhenda%20Xie%20and%20Yixuan%20Wei%20and%20Huanqi%20Cao%20and%20Chenggang%20Zhao%20and%20Chengqi%20Deng%20and%20Jiashi%20Li%20and%20Damai%20Dai%20and%20Huazuo%20Gao%20and%20Jiang%20Chang%20and%20Kuai%20Yu%20and%20Liang%20Zhao%20and%20Shangyan%20Zhou%20and%20Zhean%20Xu%20and%20Zhengyan%20Zhang%20and%20Wangding%20Zeng%20and%20Shengding%20Hu%20and%20Yuqing%20Wang%20and%20Jingyang%20Yuan%20and%20Lean%20Wang%20and%20Wenfeng%20Liang%0AAbstract%3A%20Recently%2C%20studies%20exemplified%20by%20Hyper-Connections%20%28HC%29%20have%20extended%20the%20ubiquitous%20residual%20connection%20paradigm%20established%20over%20the%20past%20decade%20by%20expanding%20the%20residual%20stream%20width%20and%20diversifying%20connectivity%20patterns.%20While%20yielding%20substantial%20performance%20gains%2C%20this%20diversification%20fundamentally%20compromises%20the%20identity%20mapping%20property%20intrinsic%20to%20the%20residual%20connection%2C%20which%20causes%20severe%20training%20instability%20and%20restricted%20scalability%2C%20and%20additionally%20incurs%20notable%20memory%20access%20overhead.%20To%20address%20these%20challenges%2C%20we%20propose%20Manifold-Constrained%20Hyper-Connections%20%28mHC%29%2C%20a%20general%20framework%20that%20projects%20the%20residual%20connection%20space%20of%20HC%20onto%20a%20specific%20manifold%20to%20restore%20the%20identity%20mapping%20property%2C%20while%20incorporating%20rigorous%20infrastructure%20optimization%20to%20ensure%20efficiency.%20Empirical%20experiments%20demonstrate%20that%20mHC%20is%20effective%20for%20training%20at%20scale%2C%20offering%20tangible%20performance%20improvements%20and%20superior%20scalability.%20We%20anticipate%20that%20mHC%2C%20as%20a%20flexible%20and%20practical%20extension%20of%20HC%2C%20will%20contribute%20to%20a%20deeper%20understanding%20of%20topological%20architecture%20design%20and%20suggest%20promising%20directions%20for%20the%20evolution%20of%20foundational%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2512.24880v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DmHC%253A%2520Manifold-Constrained%2520Hyper-Connections%26entry.906535625%3DZhenda%2520Xie%2520and%2520Yixuan%2520Wei%2520and%2520Huanqi%2520Cao%2520and%2520Chenggang%2520Zhao%2520and%2520Chengqi%2520Deng%2520and%2520Jiashi%2520Li%2520and%2520Damai%2520Dai%2520and%2520Huazuo%2520Gao%2520and%2520Jiang%2520Chang%2520and%2520Kuai%2520Yu%2520and%2520Liang%2520Zhao%2520and%2520Shangyan%2520Zhou%2520and%2520Zhean%2520Xu%2520and%2520Zhengyan%2520Zhang%2520and%2520Wangding%2520Zeng%2520and%2520Shengding%2520Hu%2520and%2520Yuqing%2520Wang%2520and%2520Jingyang%2520Yuan%2520and%2520Lean%2520Wang%2520and%2520Wenfeng%2520Liang%26entry.1292438233%3DRecently%252C%2520studies%2520exemplified%2520by%2520Hyper-Connections%2520%2528HC%2529%2520have%2520extended%2520the%2520ubiquitous%2520residual%2520connection%2520paradigm%2520established%2520over%2520the%2520past%2520decade%2520by%2520expanding%2520the%2520residual%2520stream%2520width%2520and%2520diversifying%2520connectivity%2520patterns.%2520While%2520yielding%2520substantial%2520performance%2520gains%252C%2520this%2520diversification%2520fundamentally%2520compromises%2520the%2520identity%2520mapping%2520property%2520intrinsic%2520to%2520the%2520residual%2520connection%252C%2520which%2520causes%2520severe%2520training%2520instability%2520and%2520restricted%2520scalability%252C%2520and%2520additionally%2520incurs%2520notable%2520memory%2520access%2520overhead.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520Manifold-Constrained%2520Hyper-Connections%2520%2528mHC%2529%252C%2520a%2520general%2520framework%2520that%2520projects%2520the%2520residual%2520connection%2520space%2520of%2520HC%2520onto%2520a%2520specific%2520manifold%2520to%2520restore%2520the%2520identity%2520mapping%2520property%252C%2520while%2520incorporating%2520rigorous%2520infrastructure%2520optimization%2520to%2520ensure%2520efficiency.%2520Empirical%2520experiments%2520demonstrate%2520that%2520mHC%2520is%2520effective%2520for%2520training%2520at%2520scale%252C%2520offering%2520tangible%2520performance%2520improvements%2520and%2520superior%2520scalability.%2520We%2520anticipate%2520that%2520mHC%252C%2520as%2520a%2520flexible%2520and%2520practical%2520extension%2520of%2520HC%252C%2520will%2520contribute%2520to%2520a%2520deeper%2520understanding%2520of%2520topological%2520architecture%2520design%2520and%2520suggest%2520promising%2520directions%2520for%2520the%2520evolution%2520of%2520foundational%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.24880v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=mHC%3A%20Manifold-Constrained%20Hyper-Connections&entry.906535625=Zhenda%20Xie%20and%20Yixuan%20Wei%20and%20Huanqi%20Cao%20and%20Chenggang%20Zhao%20and%20Chengqi%20Deng%20and%20Jiashi%20Li%20and%20Damai%20Dai%20and%20Huazuo%20Gao%20and%20Jiang%20Chang%20and%20Kuai%20Yu%20and%20Liang%20Zhao%20and%20Shangyan%20Zhou%20and%20Zhean%20Xu%20and%20Zhengyan%20Zhang%20and%20Wangding%20Zeng%20and%20Shengding%20Hu%20and%20Yuqing%20Wang%20and%20Jingyang%20Yuan%20and%20Lean%20Wang%20and%20Wenfeng%20Liang&entry.1292438233=Recently%2C%20studies%20exemplified%20by%20Hyper-Connections%20%28HC%29%20have%20extended%20the%20ubiquitous%20residual%20connection%20paradigm%20established%20over%20the%20past%20decade%20by%20expanding%20the%20residual%20stream%20width%20and%20diversifying%20connectivity%20patterns.%20While%20yielding%20substantial%20performance%20gains%2C%20this%20diversification%20fundamentally%20compromises%20the%20identity%20mapping%20property%20intrinsic%20to%20the%20residual%20connection%2C%20which%20causes%20severe%20training%20instability%20and%20restricted%20scalability%2C%20and%20additionally%20incurs%20notable%20memory%20access%20overhead.%20To%20address%20these%20challenges%2C%20we%20propose%20Manifold-Constrained%20Hyper-Connections%20%28mHC%29%2C%20a%20general%20framework%20that%20projects%20the%20residual%20connection%20space%20of%20HC%20onto%20a%20specific%20manifold%20to%20restore%20the%20identity%20mapping%20property%2C%20while%20incorporating%20rigorous%20infrastructure%20optimization%20to%20ensure%20efficiency.%20Empirical%20experiments%20demonstrate%20that%20mHC%20is%20effective%20for%20training%20at%20scale%2C%20offering%20tangible%20performance%20improvements%20and%20superior%20scalability.%20We%20anticipate%20that%20mHC%2C%20as%20a%20flexible%20and%20practical%20extension%20of%20HC%2C%20will%20contribute%20to%20a%20deeper%20understanding%20of%20topological%20architecture%20design%20and%20suggest%20promising%20directions%20for%20the%20evolution%20of%20foundational%20models.&entry.1838667208=http%3A//arxiv.org/abs/2512.24880v2&entry.124074799=Read"},
{"title": "Code for Machines, Not Just Humans: Quantifying AI-Friendliness with Code Health Metrics", "author": "Markus Borg and Nadim Hagatulah and Adam Tornhill and Emma S\u00f6derberg", "abstract": "We are entering a hybrid era in which human developers and AI coding agents work in the same codebases. While industry practice has long optimized code for human comprehension, it is increasingly important to ensure that LLMs with different capabilities can edit code reliably. In this study, we investigate the concept of ``AI-friendly code'' via LLM-based refactoring on a dataset of 5,000 Python files from competitive programming. We find a meaningful association between CodeHealth, a quality metric calibrated for human comprehension, and semantic preservation after AI refactoring. Our findings confirm that human-friendly code is also more compatible with AI tooling. These results suggest that organizations can use CodeHealth to guide where AI interventions are lower risk and where additional human oversight is warranted. Investing in maintainability not only helps humans; it also prepares for large-scale AI adoption.", "link": "http://arxiv.org/abs/2601.02200v1", "date": "2026-01-05", "relevancy": 1.2586, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4519}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4119}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Code%20for%20Machines%2C%20Not%20Just%20Humans%3A%20Quantifying%20AI-Friendliness%20with%20Code%20Health%20Metrics&body=Title%3A%20Code%20for%20Machines%2C%20Not%20Just%20Humans%3A%20Quantifying%20AI-Friendliness%20with%20Code%20Health%20Metrics%0AAuthor%3A%20Markus%20Borg%20and%20Nadim%20Hagatulah%20and%20Adam%20Tornhill%20and%20Emma%20S%C3%B6derberg%0AAbstract%3A%20We%20are%20entering%20a%20hybrid%20era%20in%20which%20human%20developers%20and%20AI%20coding%20agents%20work%20in%20the%20same%20codebases.%20While%20industry%20practice%20has%20long%20optimized%20code%20for%20human%20comprehension%2C%20it%20is%20increasingly%20important%20to%20ensure%20that%20LLMs%20with%20different%20capabilities%20can%20edit%20code%20reliably.%20In%20this%20study%2C%20we%20investigate%20the%20concept%20of%20%60%60AI-friendly%20code%27%27%20via%20LLM-based%20refactoring%20on%20a%20dataset%20of%205%2C000%20Python%20files%20from%20competitive%20programming.%20We%20find%20a%20meaningful%20association%20between%20CodeHealth%2C%20a%20quality%20metric%20calibrated%20for%20human%20comprehension%2C%20and%20semantic%20preservation%20after%20AI%20refactoring.%20Our%20findings%20confirm%20that%20human-friendly%20code%20is%20also%20more%20compatible%20with%20AI%20tooling.%20These%20results%20suggest%20that%20organizations%20can%20use%20CodeHealth%20to%20guide%20where%20AI%20interventions%20are%20lower%20risk%20and%20where%20additional%20human%20oversight%20is%20warranted.%20Investing%20in%20maintainability%20not%20only%20helps%20humans%3B%20it%20also%20prepares%20for%20large-scale%20AI%20adoption.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02200v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCode%2520for%2520Machines%252C%2520Not%2520Just%2520Humans%253A%2520Quantifying%2520AI-Friendliness%2520with%2520Code%2520Health%2520Metrics%26entry.906535625%3DMarkus%2520Borg%2520and%2520Nadim%2520Hagatulah%2520and%2520Adam%2520Tornhill%2520and%2520Emma%2520S%25C3%25B6derberg%26entry.1292438233%3DWe%2520are%2520entering%2520a%2520hybrid%2520era%2520in%2520which%2520human%2520developers%2520and%2520AI%2520coding%2520agents%2520work%2520in%2520the%2520same%2520codebases.%2520While%2520industry%2520practice%2520has%2520long%2520optimized%2520code%2520for%2520human%2520comprehension%252C%2520it%2520is%2520increasingly%2520important%2520to%2520ensure%2520that%2520LLMs%2520with%2520different%2520capabilities%2520can%2520edit%2520code%2520reliably.%2520In%2520this%2520study%252C%2520we%2520investigate%2520the%2520concept%2520of%2520%2560%2560AI-friendly%2520code%2527%2527%2520via%2520LLM-based%2520refactoring%2520on%2520a%2520dataset%2520of%25205%252C000%2520Python%2520files%2520from%2520competitive%2520programming.%2520We%2520find%2520a%2520meaningful%2520association%2520between%2520CodeHealth%252C%2520a%2520quality%2520metric%2520calibrated%2520for%2520human%2520comprehension%252C%2520and%2520semantic%2520preservation%2520after%2520AI%2520refactoring.%2520Our%2520findings%2520confirm%2520that%2520human-friendly%2520code%2520is%2520also%2520more%2520compatible%2520with%2520AI%2520tooling.%2520These%2520results%2520suggest%2520that%2520organizations%2520can%2520use%2520CodeHealth%2520to%2520guide%2520where%2520AI%2520interventions%2520are%2520lower%2520risk%2520and%2520where%2520additional%2520human%2520oversight%2520is%2520warranted.%2520Investing%2520in%2520maintainability%2520not%2520only%2520helps%2520humans%253B%2520it%2520also%2520prepares%2520for%2520large-scale%2520AI%2520adoption.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02200v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Code%20for%20Machines%2C%20Not%20Just%20Humans%3A%20Quantifying%20AI-Friendliness%20with%20Code%20Health%20Metrics&entry.906535625=Markus%20Borg%20and%20Nadim%20Hagatulah%20and%20Adam%20Tornhill%20and%20Emma%20S%C3%B6derberg&entry.1292438233=We%20are%20entering%20a%20hybrid%20era%20in%20which%20human%20developers%20and%20AI%20coding%20agents%20work%20in%20the%20same%20codebases.%20While%20industry%20practice%20has%20long%20optimized%20code%20for%20human%20comprehension%2C%20it%20is%20increasingly%20important%20to%20ensure%20that%20LLMs%20with%20different%20capabilities%20can%20edit%20code%20reliably.%20In%20this%20study%2C%20we%20investigate%20the%20concept%20of%20%60%60AI-friendly%20code%27%27%20via%20LLM-based%20refactoring%20on%20a%20dataset%20of%205%2C000%20Python%20files%20from%20competitive%20programming.%20We%20find%20a%20meaningful%20association%20between%20CodeHealth%2C%20a%20quality%20metric%20calibrated%20for%20human%20comprehension%2C%20and%20semantic%20preservation%20after%20AI%20refactoring.%20Our%20findings%20confirm%20that%20human-friendly%20code%20is%20also%20more%20compatible%20with%20AI%20tooling.%20These%20results%20suggest%20that%20organizations%20can%20use%20CodeHealth%20to%20guide%20where%20AI%20interventions%20are%20lower%20risk%20and%20where%20additional%20human%20oversight%20is%20warranted.%20Investing%20in%20maintainability%20not%20only%20helps%20humans%3B%20it%20also%20prepares%20for%20large-scale%20AI%20adoption.&entry.1838667208=http%3A//arxiv.org/abs/2601.02200v1&entry.124074799=Read"},
{"title": "Meta-Learning Guided Pruning for Few-Shot Plant Pathology on Edge Devices", "author": "Shahnawaz Alam and Mohammed Mudassir Uddin and Mohammed Kaif Pasha", "abstract": "Farmers in remote areas need quick and reliable methods for identifying plant diseases, yet they often lack access to laboratories or high-performance computing resources. Deep learning models can detect diseases from leaf images with high accuracy, but these models are typically too large and computationally expensive to run on low-cost edge devices such as Raspberry Pi. Furthermore, collecting thousands of labeled disease images for training is both expensive and time-consuming. This paper addresses both challenges by combining neural network pruning -- removing unnecessary parts of the model -- with few-shot learning, which enables the model to learn from limited examples. This paper proposes Disease-Aware Channel Importance Scoring (DACIS), a method that identifies which parts of the neural network are most important for distinguishing between different plant diseases, integrated into a three-stage Prune-then-Meta-Learn-then-Prune (PMP) pipeline. Experiments on PlantVillage and PlantDoc datasets demonstrate that the proposed approach reduces model size by 78\\% while maintaining 92.3\\% of the original accuracy, with the compressed model running at 7 frames per second on a Raspberry Pi 4, making real-time field diagnosis practical for smallholder farmers.", "link": "http://arxiv.org/abs/2601.02353v1", "date": "2026-01-05", "relevancy": 1.8466, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4844}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4619}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meta-Learning%20Guided%20Pruning%20for%20Few-Shot%20Plant%20Pathology%20on%20Edge%20Devices&body=Title%3A%20Meta-Learning%20Guided%20Pruning%20for%20Few-Shot%20Plant%20Pathology%20on%20Edge%20Devices%0AAuthor%3A%20Shahnawaz%20Alam%20and%20Mohammed%20Mudassir%20Uddin%20and%20Mohammed%20Kaif%20Pasha%0AAbstract%3A%20Farmers%20in%20remote%20areas%20need%20quick%20and%20reliable%20methods%20for%20identifying%20plant%20diseases%2C%20yet%20they%20often%20lack%20access%20to%20laboratories%20or%20high-performance%20computing%20resources.%20Deep%20learning%20models%20can%20detect%20diseases%20from%20leaf%20images%20with%20high%20accuracy%2C%20but%20these%20models%20are%20typically%20too%20large%20and%20computationally%20expensive%20to%20run%20on%20low-cost%20edge%20devices%20such%20as%20Raspberry%20Pi.%20Furthermore%2C%20collecting%20thousands%20of%20labeled%20disease%20images%20for%20training%20is%20both%20expensive%20and%20time-consuming.%20This%20paper%20addresses%20both%20challenges%20by%20combining%20neural%20network%20pruning%20--%20removing%20unnecessary%20parts%20of%20the%20model%20--%20with%20few-shot%20learning%2C%20which%20enables%20the%20model%20to%20learn%20from%20limited%20examples.%20This%20paper%20proposes%20Disease-Aware%20Channel%20Importance%20Scoring%20%28DACIS%29%2C%20a%20method%20that%20identifies%20which%20parts%20of%20the%20neural%20network%20are%20most%20important%20for%20distinguishing%20between%20different%20plant%20diseases%2C%20integrated%20into%20a%20three-stage%20Prune-then-Meta-Learn-then-Prune%20%28PMP%29%20pipeline.%20Experiments%20on%20PlantVillage%20and%20PlantDoc%20datasets%20demonstrate%20that%20the%20proposed%20approach%20reduces%20model%20size%20by%2078%5C%25%20while%20maintaining%2092.3%5C%25%20of%20the%20original%20accuracy%2C%20with%20the%20compressed%20model%20running%20at%207%20frames%20per%20second%20on%20a%20Raspberry%20Pi%204%2C%20making%20real-time%20field%20diagnosis%20practical%20for%20smallholder%20farmers.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02353v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeta-Learning%2520Guided%2520Pruning%2520for%2520Few-Shot%2520Plant%2520Pathology%2520on%2520Edge%2520Devices%26entry.906535625%3DShahnawaz%2520Alam%2520and%2520Mohammed%2520Mudassir%2520Uddin%2520and%2520Mohammed%2520Kaif%2520Pasha%26entry.1292438233%3DFarmers%2520in%2520remote%2520areas%2520need%2520quick%2520and%2520reliable%2520methods%2520for%2520identifying%2520plant%2520diseases%252C%2520yet%2520they%2520often%2520lack%2520access%2520to%2520laboratories%2520or%2520high-performance%2520computing%2520resources.%2520Deep%2520learning%2520models%2520can%2520detect%2520diseases%2520from%2520leaf%2520images%2520with%2520high%2520accuracy%252C%2520but%2520these%2520models%2520are%2520typically%2520too%2520large%2520and%2520computationally%2520expensive%2520to%2520run%2520on%2520low-cost%2520edge%2520devices%2520such%2520as%2520Raspberry%2520Pi.%2520Furthermore%252C%2520collecting%2520thousands%2520of%2520labeled%2520disease%2520images%2520for%2520training%2520is%2520both%2520expensive%2520and%2520time-consuming.%2520This%2520paper%2520addresses%2520both%2520challenges%2520by%2520combining%2520neural%2520network%2520pruning%2520--%2520removing%2520unnecessary%2520parts%2520of%2520the%2520model%2520--%2520with%2520few-shot%2520learning%252C%2520which%2520enables%2520the%2520model%2520to%2520learn%2520from%2520limited%2520examples.%2520This%2520paper%2520proposes%2520Disease-Aware%2520Channel%2520Importance%2520Scoring%2520%2528DACIS%2529%252C%2520a%2520method%2520that%2520identifies%2520which%2520parts%2520of%2520the%2520neural%2520network%2520are%2520most%2520important%2520for%2520distinguishing%2520between%2520different%2520plant%2520diseases%252C%2520integrated%2520into%2520a%2520three-stage%2520Prune-then-Meta-Learn-then-Prune%2520%2528PMP%2529%2520pipeline.%2520Experiments%2520on%2520PlantVillage%2520and%2520PlantDoc%2520datasets%2520demonstrate%2520that%2520the%2520proposed%2520approach%2520reduces%2520model%2520size%2520by%252078%255C%2525%2520while%2520maintaining%252092.3%255C%2525%2520of%2520the%2520original%2520accuracy%252C%2520with%2520the%2520compressed%2520model%2520running%2520at%25207%2520frames%2520per%2520second%2520on%2520a%2520Raspberry%2520Pi%25204%252C%2520making%2520real-time%2520field%2520diagnosis%2520practical%2520for%2520smallholder%2520farmers.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02353v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meta-Learning%20Guided%20Pruning%20for%20Few-Shot%20Plant%20Pathology%20on%20Edge%20Devices&entry.906535625=Shahnawaz%20Alam%20and%20Mohammed%20Mudassir%20Uddin%20and%20Mohammed%20Kaif%20Pasha&entry.1292438233=Farmers%20in%20remote%20areas%20need%20quick%20and%20reliable%20methods%20for%20identifying%20plant%20diseases%2C%20yet%20they%20often%20lack%20access%20to%20laboratories%20or%20high-performance%20computing%20resources.%20Deep%20learning%20models%20can%20detect%20diseases%20from%20leaf%20images%20with%20high%20accuracy%2C%20but%20these%20models%20are%20typically%20too%20large%20and%20computationally%20expensive%20to%20run%20on%20low-cost%20edge%20devices%20such%20as%20Raspberry%20Pi.%20Furthermore%2C%20collecting%20thousands%20of%20labeled%20disease%20images%20for%20training%20is%20both%20expensive%20and%20time-consuming.%20This%20paper%20addresses%20both%20challenges%20by%20combining%20neural%20network%20pruning%20--%20removing%20unnecessary%20parts%20of%20the%20model%20--%20with%20few-shot%20learning%2C%20which%20enables%20the%20model%20to%20learn%20from%20limited%20examples.%20This%20paper%20proposes%20Disease-Aware%20Channel%20Importance%20Scoring%20%28DACIS%29%2C%20a%20method%20that%20identifies%20which%20parts%20of%20the%20neural%20network%20are%20most%20important%20for%20distinguishing%20between%20different%20plant%20diseases%2C%20integrated%20into%20a%20three-stage%20Prune-then-Meta-Learn-then-Prune%20%28PMP%29%20pipeline.%20Experiments%20on%20PlantVillage%20and%20PlantDoc%20datasets%20demonstrate%20that%20the%20proposed%20approach%20reduces%20model%20size%20by%2078%5C%25%20while%20maintaining%2092.3%5C%25%20of%20the%20original%20accuracy%2C%20with%20the%20compressed%20model%20running%20at%207%20frames%20per%20second%20on%20a%20Raspberry%20Pi%204%2C%20making%20real-time%20field%20diagnosis%20practical%20for%20smallholder%20farmers.&entry.1838667208=http%3A//arxiv.org/abs/2601.02353v1&entry.124074799=Read"},
{"title": "AFTER: Mitigating the Object Hallucination of LVLM via Adaptive Factual-Guided Activation Editing", "author": "Tianbo Wang and Yuqing Ma and Kewei Liao and Zhange Zhang and Simin Li and Jinyang Guo and Xianglong Liu", "abstract": "Large Vision-Language Models (LVLMs) have achieved substantial progress in cross-modal tasks. However, due to language bias, LVLMs are susceptible to object hallucination, which can be primarily divided into category, attribute, and relation hallucination, significantly impeding the trustworthy AI applications. Editing the internal activations of LVLMs has shown promising effectiveness in mitigating hallucinations with minimal cost. However, previous editing approaches neglect the effective guidance offered by factual textual semantics, thereby struggling to explicitly mitigate language bias. To address these issues, we propose Adaptive Factual-guided Visual-Textual Editing for hallucination mitigation (AFTER), which comprises Factual-Augmented Activation Steering (FAS) and Query-Adaptive Offset Optimization (QAO), to adaptively guides the original biased activations towards factual semantics. Specifically, FAS is proposed to provide factual and general guidance for activation editing, thereby explicitly modeling the precise visual-textual associations. Subsequently, QAO introduces a query-aware offset estimator to establish query-specific editing from the general steering vector, enhancing the diversity and granularity of editing. Extensive experiments on standard hallucination benchmarks across three widely adopted LVLMs validate the efficacy of the proposed AFTER, notably achieving up to a 16.3% reduction of hallucination over baseline on the AMBER benchmark. Our code and data will be released for reproducibility.", "link": "http://arxiv.org/abs/2601.01957v1", "date": "2026-01-05", "relevancy": 1.5142, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5208}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5041}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AFTER%3A%20Mitigating%20the%20Object%20Hallucination%20of%20LVLM%20via%20Adaptive%20Factual-Guided%20Activation%20Editing&body=Title%3A%20AFTER%3A%20Mitigating%20the%20Object%20Hallucination%20of%20LVLM%20via%20Adaptive%20Factual-Guided%20Activation%20Editing%0AAuthor%3A%20Tianbo%20Wang%20and%20Yuqing%20Ma%20and%20Kewei%20Liao%20and%20Zhange%20Zhang%20and%20Simin%20Li%20and%20Jinyang%20Guo%20and%20Xianglong%20Liu%0AAbstract%3A%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20achieved%20substantial%20progress%20in%20cross-modal%20tasks.%20However%2C%20due%20to%20language%20bias%2C%20LVLMs%20are%20susceptible%20to%20object%20hallucination%2C%20which%20can%20be%20primarily%20divided%20into%20category%2C%20attribute%2C%20and%20relation%20hallucination%2C%20significantly%20impeding%20the%20trustworthy%20AI%20applications.%20Editing%20the%20internal%20activations%20of%20LVLMs%20has%20shown%20promising%20effectiveness%20in%20mitigating%20hallucinations%20with%20minimal%20cost.%20However%2C%20previous%20editing%20approaches%20neglect%20the%20effective%20guidance%20offered%20by%20factual%20textual%20semantics%2C%20thereby%20struggling%20to%20explicitly%20mitigate%20language%20bias.%20To%20address%20these%20issues%2C%20we%20propose%20Adaptive%20Factual-guided%20Visual-Textual%20Editing%20for%20hallucination%20mitigation%20%28AFTER%29%2C%20which%20comprises%20Factual-Augmented%20Activation%20Steering%20%28FAS%29%20and%20Query-Adaptive%20Offset%20Optimization%20%28QAO%29%2C%20to%20adaptively%20guides%20the%20original%20biased%20activations%20towards%20factual%20semantics.%20Specifically%2C%20FAS%20is%20proposed%20to%20provide%20factual%20and%20general%20guidance%20for%20activation%20editing%2C%20thereby%20explicitly%20modeling%20the%20precise%20visual-textual%20associations.%20Subsequently%2C%20QAO%20introduces%20a%20query-aware%20offset%20estimator%20to%20establish%20query-specific%20editing%20from%20the%20general%20steering%20vector%2C%20enhancing%20the%20diversity%20and%20granularity%20of%20editing.%20Extensive%20experiments%20on%20standard%20hallucination%20benchmarks%20across%20three%20widely%20adopted%20LVLMs%20validate%20the%20efficacy%20of%20the%20proposed%20AFTER%2C%20notably%20achieving%20up%20to%20a%2016.3%25%20reduction%20of%20hallucination%20over%20baseline%20on%20the%20AMBER%20benchmark.%20Our%20code%20and%20data%20will%20be%20released%20for%20reproducibility.%0ALink%3A%20http%3A//arxiv.org/abs/2601.01957v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAFTER%253A%2520Mitigating%2520the%2520Object%2520Hallucination%2520of%2520LVLM%2520via%2520Adaptive%2520Factual-Guided%2520Activation%2520Editing%26entry.906535625%3DTianbo%2520Wang%2520and%2520Yuqing%2520Ma%2520and%2520Kewei%2520Liao%2520and%2520Zhange%2520Zhang%2520and%2520Simin%2520Li%2520and%2520Jinyang%2520Guo%2520and%2520Xianglong%2520Liu%26entry.1292438233%3DLarge%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520achieved%2520substantial%2520progress%2520in%2520cross-modal%2520tasks.%2520However%252C%2520due%2520to%2520language%2520bias%252C%2520LVLMs%2520are%2520susceptible%2520to%2520object%2520hallucination%252C%2520which%2520can%2520be%2520primarily%2520divided%2520into%2520category%252C%2520attribute%252C%2520and%2520relation%2520hallucination%252C%2520significantly%2520impeding%2520the%2520trustworthy%2520AI%2520applications.%2520Editing%2520the%2520internal%2520activations%2520of%2520LVLMs%2520has%2520shown%2520promising%2520effectiveness%2520in%2520mitigating%2520hallucinations%2520with%2520minimal%2520cost.%2520However%252C%2520previous%2520editing%2520approaches%2520neglect%2520the%2520effective%2520guidance%2520offered%2520by%2520factual%2520textual%2520semantics%252C%2520thereby%2520struggling%2520to%2520explicitly%2520mitigate%2520language%2520bias.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520Adaptive%2520Factual-guided%2520Visual-Textual%2520Editing%2520for%2520hallucination%2520mitigation%2520%2528AFTER%2529%252C%2520which%2520comprises%2520Factual-Augmented%2520Activation%2520Steering%2520%2528FAS%2529%2520and%2520Query-Adaptive%2520Offset%2520Optimization%2520%2528QAO%2529%252C%2520to%2520adaptively%2520guides%2520the%2520original%2520biased%2520activations%2520towards%2520factual%2520semantics.%2520Specifically%252C%2520FAS%2520is%2520proposed%2520to%2520provide%2520factual%2520and%2520general%2520guidance%2520for%2520activation%2520editing%252C%2520thereby%2520explicitly%2520modeling%2520the%2520precise%2520visual-textual%2520associations.%2520Subsequently%252C%2520QAO%2520introduces%2520a%2520query-aware%2520offset%2520estimator%2520to%2520establish%2520query-specific%2520editing%2520from%2520the%2520general%2520steering%2520vector%252C%2520enhancing%2520the%2520diversity%2520and%2520granularity%2520of%2520editing.%2520Extensive%2520experiments%2520on%2520standard%2520hallucination%2520benchmarks%2520across%2520three%2520widely%2520adopted%2520LVLMs%2520validate%2520the%2520efficacy%2520of%2520the%2520proposed%2520AFTER%252C%2520notably%2520achieving%2520up%2520to%2520a%252016.3%2525%2520reduction%2520of%2520hallucination%2520over%2520baseline%2520on%2520the%2520AMBER%2520benchmark.%2520Our%2520code%2520and%2520data%2520will%2520be%2520released%2520for%2520reproducibility.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.01957v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AFTER%3A%20Mitigating%20the%20Object%20Hallucination%20of%20LVLM%20via%20Adaptive%20Factual-Guided%20Activation%20Editing&entry.906535625=Tianbo%20Wang%20and%20Yuqing%20Ma%20and%20Kewei%20Liao%20and%20Zhange%20Zhang%20and%20Simin%20Li%20and%20Jinyang%20Guo%20and%20Xianglong%20Liu&entry.1292438233=Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20achieved%20substantial%20progress%20in%20cross-modal%20tasks.%20However%2C%20due%20to%20language%20bias%2C%20LVLMs%20are%20susceptible%20to%20object%20hallucination%2C%20which%20can%20be%20primarily%20divided%20into%20category%2C%20attribute%2C%20and%20relation%20hallucination%2C%20significantly%20impeding%20the%20trustworthy%20AI%20applications.%20Editing%20the%20internal%20activations%20of%20LVLMs%20has%20shown%20promising%20effectiveness%20in%20mitigating%20hallucinations%20with%20minimal%20cost.%20However%2C%20previous%20editing%20approaches%20neglect%20the%20effective%20guidance%20offered%20by%20factual%20textual%20semantics%2C%20thereby%20struggling%20to%20explicitly%20mitigate%20language%20bias.%20To%20address%20these%20issues%2C%20we%20propose%20Adaptive%20Factual-guided%20Visual-Textual%20Editing%20for%20hallucination%20mitigation%20%28AFTER%29%2C%20which%20comprises%20Factual-Augmented%20Activation%20Steering%20%28FAS%29%20and%20Query-Adaptive%20Offset%20Optimization%20%28QAO%29%2C%20to%20adaptively%20guides%20the%20original%20biased%20activations%20towards%20factual%20semantics.%20Specifically%2C%20FAS%20is%20proposed%20to%20provide%20factual%20and%20general%20guidance%20for%20activation%20editing%2C%20thereby%20explicitly%20modeling%20the%20precise%20visual-textual%20associations.%20Subsequently%2C%20QAO%20introduces%20a%20query-aware%20offset%20estimator%20to%20establish%20query-specific%20editing%20from%20the%20general%20steering%20vector%2C%20enhancing%20the%20diversity%20and%20granularity%20of%20editing.%20Extensive%20experiments%20on%20standard%20hallucination%20benchmarks%20across%20three%20widely%20adopted%20LVLMs%20validate%20the%20efficacy%20of%20the%20proposed%20AFTER%2C%20notably%20achieving%20up%20to%20a%2016.3%25%20reduction%20of%20hallucination%20over%20baseline%20on%20the%20AMBER%20benchmark.%20Our%20code%20and%20data%20will%20be%20released%20for%20reproducibility.&entry.1838667208=http%3A//arxiv.org/abs/2601.01957v1&entry.124074799=Read"},
{"title": "Heterogeneous Low-Bandwidth Pre-Training of LLMs", "author": "Yazan Obeidi and Amir Sarfi and Joel Lidin and Paul Janson and Eugene Belilovsky", "abstract": "Pre-training large language models (LLMs) increasingly requires distributed compute, yet bandwidth constraints make it difficult to scale beyond well-provisioned datacenters-especially when model parallelism forces frequent, large inter-device communications. We study whether SparseLoCo, a low-communication data parallel method based on infrequent synchronization and sparse pseudo-gradient exchange, can be combined with low-bandwidth pipeline model parallelism via activation and activation-gradient compression. We introduce a heterogeneous distributed training framework where some participants host full replicas on high-bandwidth interconnects, while resource-limited participants are grouped to jointly instantiate a replica using pipeline parallelism with subspace-projected inter-stage communication. To make the recently introduced subspace pipeline compression compatible with SparseLoCo, we study a number of adaptations. Across large-scale language modeling experiments (178M-1B parameters) on standard pretraining corpora, we find that activation compression composes with SparseLoCo at modest cost, while selective (heterogeneous) compression consistently improves the loss-communication tradeoff relative to compressing all replicas-especially at aggressive compression ratios. These results suggest a practical path to incorporating low-bandwidth model parallelism and heterogeneous participants into LLM pre-training.", "link": "http://arxiv.org/abs/2601.02360v1", "date": "2026-01-05", "relevancy": 1.8795, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4838}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4667}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4573}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heterogeneous%20Low-Bandwidth%20Pre-Training%20of%20LLMs&body=Title%3A%20Heterogeneous%20Low-Bandwidth%20Pre-Training%20of%20LLMs%0AAuthor%3A%20Yazan%20Obeidi%20and%20Amir%20Sarfi%20and%20Joel%20Lidin%20and%20Paul%20Janson%20and%20Eugene%20Belilovsky%0AAbstract%3A%20Pre-training%20large%20language%20models%20%28LLMs%29%20increasingly%20requires%20distributed%20compute%2C%20yet%20bandwidth%20constraints%20make%20it%20difficult%20to%20scale%20beyond%20well-provisioned%20datacenters-especially%20when%20model%20parallelism%20forces%20frequent%2C%20large%20inter-device%20communications.%20We%20study%20whether%20SparseLoCo%2C%20a%20low-communication%20data%20parallel%20method%20based%20on%20infrequent%20synchronization%20and%20sparse%20pseudo-gradient%20exchange%2C%20can%20be%20combined%20with%20low-bandwidth%20pipeline%20model%20parallelism%20via%20activation%20and%20activation-gradient%20compression.%20We%20introduce%20a%20heterogeneous%20distributed%20training%20framework%20where%20some%20participants%20host%20full%20replicas%20on%20high-bandwidth%20interconnects%2C%20while%20resource-limited%20participants%20are%20grouped%20to%20jointly%20instantiate%20a%20replica%20using%20pipeline%20parallelism%20with%20subspace-projected%20inter-stage%20communication.%20To%20make%20the%20recently%20introduced%20subspace%20pipeline%20compression%20compatible%20with%20SparseLoCo%2C%20we%20study%20a%20number%20of%20adaptations.%20Across%20large-scale%20language%20modeling%20experiments%20%28178M-1B%20parameters%29%20on%20standard%20pretraining%20corpora%2C%20we%20find%20that%20activation%20compression%20composes%20with%20SparseLoCo%20at%20modest%20cost%2C%20while%20selective%20%28heterogeneous%29%20compression%20consistently%20improves%20the%20loss-communication%20tradeoff%20relative%20to%20compressing%20all%20replicas-especially%20at%20aggressive%20compression%20ratios.%20These%20results%20suggest%20a%20practical%20path%20to%20incorporating%20low-bandwidth%20model%20parallelism%20and%20heterogeneous%20participants%20into%20LLM%20pre-training.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02360v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeterogeneous%2520Low-Bandwidth%2520Pre-Training%2520of%2520LLMs%26entry.906535625%3DYazan%2520Obeidi%2520and%2520Amir%2520Sarfi%2520and%2520Joel%2520Lidin%2520and%2520Paul%2520Janson%2520and%2520Eugene%2520Belilovsky%26entry.1292438233%3DPre-training%2520large%2520language%2520models%2520%2528LLMs%2529%2520increasingly%2520requires%2520distributed%2520compute%252C%2520yet%2520bandwidth%2520constraints%2520make%2520it%2520difficult%2520to%2520scale%2520beyond%2520well-provisioned%2520datacenters-especially%2520when%2520model%2520parallelism%2520forces%2520frequent%252C%2520large%2520inter-device%2520communications.%2520We%2520study%2520whether%2520SparseLoCo%252C%2520a%2520low-communication%2520data%2520parallel%2520method%2520based%2520on%2520infrequent%2520synchronization%2520and%2520sparse%2520pseudo-gradient%2520exchange%252C%2520can%2520be%2520combined%2520with%2520low-bandwidth%2520pipeline%2520model%2520parallelism%2520via%2520activation%2520and%2520activation-gradient%2520compression.%2520We%2520introduce%2520a%2520heterogeneous%2520distributed%2520training%2520framework%2520where%2520some%2520participants%2520host%2520full%2520replicas%2520on%2520high-bandwidth%2520interconnects%252C%2520while%2520resource-limited%2520participants%2520are%2520grouped%2520to%2520jointly%2520instantiate%2520a%2520replica%2520using%2520pipeline%2520parallelism%2520with%2520subspace-projected%2520inter-stage%2520communication.%2520To%2520make%2520the%2520recently%2520introduced%2520subspace%2520pipeline%2520compression%2520compatible%2520with%2520SparseLoCo%252C%2520we%2520study%2520a%2520number%2520of%2520adaptations.%2520Across%2520large-scale%2520language%2520modeling%2520experiments%2520%2528178M-1B%2520parameters%2529%2520on%2520standard%2520pretraining%2520corpora%252C%2520we%2520find%2520that%2520activation%2520compression%2520composes%2520with%2520SparseLoCo%2520at%2520modest%2520cost%252C%2520while%2520selective%2520%2528heterogeneous%2529%2520compression%2520consistently%2520improves%2520the%2520loss-communication%2520tradeoff%2520relative%2520to%2520compressing%2520all%2520replicas-especially%2520at%2520aggressive%2520compression%2520ratios.%2520These%2520results%2520suggest%2520a%2520practical%2520path%2520to%2520incorporating%2520low-bandwidth%2520model%2520parallelism%2520and%2520heterogeneous%2520participants%2520into%2520LLM%2520pre-training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02360v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heterogeneous%20Low-Bandwidth%20Pre-Training%20of%20LLMs&entry.906535625=Yazan%20Obeidi%20and%20Amir%20Sarfi%20and%20Joel%20Lidin%20and%20Paul%20Janson%20and%20Eugene%20Belilovsky&entry.1292438233=Pre-training%20large%20language%20models%20%28LLMs%29%20increasingly%20requires%20distributed%20compute%2C%20yet%20bandwidth%20constraints%20make%20it%20difficult%20to%20scale%20beyond%20well-provisioned%20datacenters-especially%20when%20model%20parallelism%20forces%20frequent%2C%20large%20inter-device%20communications.%20We%20study%20whether%20SparseLoCo%2C%20a%20low-communication%20data%20parallel%20method%20based%20on%20infrequent%20synchronization%20and%20sparse%20pseudo-gradient%20exchange%2C%20can%20be%20combined%20with%20low-bandwidth%20pipeline%20model%20parallelism%20via%20activation%20and%20activation-gradient%20compression.%20We%20introduce%20a%20heterogeneous%20distributed%20training%20framework%20where%20some%20participants%20host%20full%20replicas%20on%20high-bandwidth%20interconnects%2C%20while%20resource-limited%20participants%20are%20grouped%20to%20jointly%20instantiate%20a%20replica%20using%20pipeline%20parallelism%20with%20subspace-projected%20inter-stage%20communication.%20To%20make%20the%20recently%20introduced%20subspace%20pipeline%20compression%20compatible%20with%20SparseLoCo%2C%20we%20study%20a%20number%20of%20adaptations.%20Across%20large-scale%20language%20modeling%20experiments%20%28178M-1B%20parameters%29%20on%20standard%20pretraining%20corpora%2C%20we%20find%20that%20activation%20compression%20composes%20with%20SparseLoCo%20at%20modest%20cost%2C%20while%20selective%20%28heterogeneous%29%20compression%20consistently%20improves%20the%20loss-communication%20tradeoff%20relative%20to%20compressing%20all%20replicas-especially%20at%20aggressive%20compression%20ratios.%20These%20results%20suggest%20a%20practical%20path%20to%20incorporating%20low-bandwidth%20model%20parallelism%20and%20heterogeneous%20participants%20into%20LLM%20pre-training.&entry.1838667208=http%3A//arxiv.org/abs/2601.02360v1&entry.124074799=Read"},
{"title": "Interconnection and Damping Assignment Passivity-Based Control using Sparse Neural ODEs", "author": "Nicol\u00f2 Botteghi and Owen Brook and Urban Fasel and Federico Califano", "abstract": "Interconnection and Damping Assignment Passivity-Based Control (IDA-PBC) is a nonlinear control technique that assigns a port-Hamiltonian (pH) structure to a controlled system using a state-feedback law. While IDA-PBC has been extensively studied and applied to many systems, its practical implementation often remains confined to academic examples and, almost exclusively, to stabilization tasks. The main limitation of IDA-PBC stems from the complexity of analytically solving a set of partial differential equations (PDEs), referred to as the matching conditions, which enforce the pH structure of the closed-loop system. However, this is extremely challenging, especially for complex physical systems and tasks.\n  In this work, we propose a novel numerical approach for designing IDA-PBC controllers without solving the matching PDEs exactly. We cast the IDA-PBC problem as the learning of a neural ordinary differential equation. In particular, we rely on sparse dictionary learning to parametrize the desired closed-loop system as a sparse linear combination of nonlinear state-dependent functions. Optimization of the controller parameters is achieved by solving a multi-objective optimization problem whose cost function is composed of a generic task-dependent cost and a matching condition-dependent cost. Our numerical results show that the proposed method enables (i) IDA-PBC to be applicable to complex tasks beyond stabilization, such as the discovery of periodic oscillatory behaviors, (ii) the derivation of closed-form expressions of the controlled system, including residual terms in case of approximate matching, and (iii) stability analysis of the learned controller.", "link": "http://arxiv.org/abs/2512.06935v2", "date": "2026-01-05", "relevancy": 1.8968, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4844}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4796}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interconnection%20and%20Damping%20Assignment%20Passivity-Based%20Control%20using%20Sparse%20Neural%20ODEs&body=Title%3A%20Interconnection%20and%20Damping%20Assignment%20Passivity-Based%20Control%20using%20Sparse%20Neural%20ODEs%0AAuthor%3A%20Nicol%C3%B2%20Botteghi%20and%20Owen%20Brook%20and%20Urban%20Fasel%20and%20Federico%20Califano%0AAbstract%3A%20Interconnection%20and%20Damping%20Assignment%20Passivity-Based%20Control%20%28IDA-PBC%29%20is%20a%20nonlinear%20control%20technique%20that%20assigns%20a%20port-Hamiltonian%20%28pH%29%20structure%20to%20a%20controlled%20system%20using%20a%20state-feedback%20law.%20While%20IDA-PBC%20has%20been%20extensively%20studied%20and%20applied%20to%20many%20systems%2C%20its%20practical%20implementation%20often%20remains%20confined%20to%20academic%20examples%20and%2C%20almost%20exclusively%2C%20to%20stabilization%20tasks.%20The%20main%20limitation%20of%20IDA-PBC%20stems%20from%20the%20complexity%20of%20analytically%20solving%20a%20set%20of%20partial%20differential%20equations%20%28PDEs%29%2C%20referred%20to%20as%20the%20matching%20conditions%2C%20which%20enforce%20the%20pH%20structure%20of%20the%20closed-loop%20system.%20However%2C%20this%20is%20extremely%20challenging%2C%20especially%20for%20complex%20physical%20systems%20and%20tasks.%0A%20%20In%20this%20work%2C%20we%20propose%20a%20novel%20numerical%20approach%20for%20designing%20IDA-PBC%20controllers%20without%20solving%20the%20matching%20PDEs%20exactly.%20We%20cast%20the%20IDA-PBC%20problem%20as%20the%20learning%20of%20a%20neural%20ordinary%20differential%20equation.%20In%20particular%2C%20we%20rely%20on%20sparse%20dictionary%20learning%20to%20parametrize%20the%20desired%20closed-loop%20system%20as%20a%20sparse%20linear%20combination%20of%20nonlinear%20state-dependent%20functions.%20Optimization%20of%20the%20controller%20parameters%20is%20achieved%20by%20solving%20a%20multi-objective%20optimization%20problem%20whose%20cost%20function%20is%20composed%20of%20a%20generic%20task-dependent%20cost%20and%20a%20matching%20condition-dependent%20cost.%20Our%20numerical%20results%20show%20that%20the%20proposed%20method%20enables%20%28i%29%20IDA-PBC%20to%20be%20applicable%20to%20complex%20tasks%20beyond%20stabilization%2C%20such%20as%20the%20discovery%20of%20periodic%20oscillatory%20behaviors%2C%20%28ii%29%20the%20derivation%20of%20closed-form%20expressions%20of%20the%20controlled%20system%2C%20including%20residual%20terms%20in%20case%20of%20approximate%20matching%2C%20and%20%28iii%29%20stability%20analysis%20of%20the%20learned%20controller.%0ALink%3A%20http%3A//arxiv.org/abs/2512.06935v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterconnection%2520and%2520Damping%2520Assignment%2520Passivity-Based%2520Control%2520using%2520Sparse%2520Neural%2520ODEs%26entry.906535625%3DNicol%25C3%25B2%2520Botteghi%2520and%2520Owen%2520Brook%2520and%2520Urban%2520Fasel%2520and%2520Federico%2520Califano%26entry.1292438233%3DInterconnection%2520and%2520Damping%2520Assignment%2520Passivity-Based%2520Control%2520%2528IDA-PBC%2529%2520is%2520a%2520nonlinear%2520control%2520technique%2520that%2520assigns%2520a%2520port-Hamiltonian%2520%2528pH%2529%2520structure%2520to%2520a%2520controlled%2520system%2520using%2520a%2520state-feedback%2520law.%2520While%2520IDA-PBC%2520has%2520been%2520extensively%2520studied%2520and%2520applied%2520to%2520many%2520systems%252C%2520its%2520practical%2520implementation%2520often%2520remains%2520confined%2520to%2520academic%2520examples%2520and%252C%2520almost%2520exclusively%252C%2520to%2520stabilization%2520tasks.%2520The%2520main%2520limitation%2520of%2520IDA-PBC%2520stems%2520from%2520the%2520complexity%2520of%2520analytically%2520solving%2520a%2520set%2520of%2520partial%2520differential%2520equations%2520%2528PDEs%2529%252C%2520referred%2520to%2520as%2520the%2520matching%2520conditions%252C%2520which%2520enforce%2520the%2520pH%2520structure%2520of%2520the%2520closed-loop%2520system.%2520However%252C%2520this%2520is%2520extremely%2520challenging%252C%2520especially%2520for%2520complex%2520physical%2520systems%2520and%2520tasks.%250A%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520numerical%2520approach%2520for%2520designing%2520IDA-PBC%2520controllers%2520without%2520solving%2520the%2520matching%2520PDEs%2520exactly.%2520We%2520cast%2520the%2520IDA-PBC%2520problem%2520as%2520the%2520learning%2520of%2520a%2520neural%2520ordinary%2520differential%2520equation.%2520In%2520particular%252C%2520we%2520rely%2520on%2520sparse%2520dictionary%2520learning%2520to%2520parametrize%2520the%2520desired%2520closed-loop%2520system%2520as%2520a%2520sparse%2520linear%2520combination%2520of%2520nonlinear%2520state-dependent%2520functions.%2520Optimization%2520of%2520the%2520controller%2520parameters%2520is%2520achieved%2520by%2520solving%2520a%2520multi-objective%2520optimization%2520problem%2520whose%2520cost%2520function%2520is%2520composed%2520of%2520a%2520generic%2520task-dependent%2520cost%2520and%2520a%2520matching%2520condition-dependent%2520cost.%2520Our%2520numerical%2520results%2520show%2520that%2520the%2520proposed%2520method%2520enables%2520%2528i%2529%2520IDA-PBC%2520to%2520be%2520applicable%2520to%2520complex%2520tasks%2520beyond%2520stabilization%252C%2520such%2520as%2520the%2520discovery%2520of%2520periodic%2520oscillatory%2520behaviors%252C%2520%2528ii%2529%2520the%2520derivation%2520of%2520closed-form%2520expressions%2520of%2520the%2520controlled%2520system%252C%2520including%2520residual%2520terms%2520in%2520case%2520of%2520approximate%2520matching%252C%2520and%2520%2528iii%2529%2520stability%2520analysis%2520of%2520the%2520learned%2520controller.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.06935v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interconnection%20and%20Damping%20Assignment%20Passivity-Based%20Control%20using%20Sparse%20Neural%20ODEs&entry.906535625=Nicol%C3%B2%20Botteghi%20and%20Owen%20Brook%20and%20Urban%20Fasel%20and%20Federico%20Califano&entry.1292438233=Interconnection%20and%20Damping%20Assignment%20Passivity-Based%20Control%20%28IDA-PBC%29%20is%20a%20nonlinear%20control%20technique%20that%20assigns%20a%20port-Hamiltonian%20%28pH%29%20structure%20to%20a%20controlled%20system%20using%20a%20state-feedback%20law.%20While%20IDA-PBC%20has%20been%20extensively%20studied%20and%20applied%20to%20many%20systems%2C%20its%20practical%20implementation%20often%20remains%20confined%20to%20academic%20examples%20and%2C%20almost%20exclusively%2C%20to%20stabilization%20tasks.%20The%20main%20limitation%20of%20IDA-PBC%20stems%20from%20the%20complexity%20of%20analytically%20solving%20a%20set%20of%20partial%20differential%20equations%20%28PDEs%29%2C%20referred%20to%20as%20the%20matching%20conditions%2C%20which%20enforce%20the%20pH%20structure%20of%20the%20closed-loop%20system.%20However%2C%20this%20is%20extremely%20challenging%2C%20especially%20for%20complex%20physical%20systems%20and%20tasks.%0A%20%20In%20this%20work%2C%20we%20propose%20a%20novel%20numerical%20approach%20for%20designing%20IDA-PBC%20controllers%20without%20solving%20the%20matching%20PDEs%20exactly.%20We%20cast%20the%20IDA-PBC%20problem%20as%20the%20learning%20of%20a%20neural%20ordinary%20differential%20equation.%20In%20particular%2C%20we%20rely%20on%20sparse%20dictionary%20learning%20to%20parametrize%20the%20desired%20closed-loop%20system%20as%20a%20sparse%20linear%20combination%20of%20nonlinear%20state-dependent%20functions.%20Optimization%20of%20the%20controller%20parameters%20is%20achieved%20by%20solving%20a%20multi-objective%20optimization%20problem%20whose%20cost%20function%20is%20composed%20of%20a%20generic%20task-dependent%20cost%20and%20a%20matching%20condition-dependent%20cost.%20Our%20numerical%20results%20show%20that%20the%20proposed%20method%20enables%20%28i%29%20IDA-PBC%20to%20be%20applicable%20to%20complex%20tasks%20beyond%20stabilization%2C%20such%20as%20the%20discovery%20of%20periodic%20oscillatory%20behaviors%2C%20%28ii%29%20the%20derivation%20of%20closed-form%20expressions%20of%20the%20controlled%20system%2C%20including%20residual%20terms%20in%20case%20of%20approximate%20matching%2C%20and%20%28iii%29%20stability%20analysis%20of%20the%20learned%20controller.&entry.1838667208=http%3A//arxiv.org/abs/2512.06935v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


