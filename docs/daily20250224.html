<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250223.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "RGB-Only Gaussian Splatting SLAM for Unbounded Outdoor Scenes", "author": "Sicheng Yu and Chong Cheng and Yifan Zhou and Xiaojun Yang and Hao Wang", "abstract": "  3D Gaussian Splatting (3DGS) has become a popular solution in SLAM, as it can\nproduce high-fidelity novel views. However, previous GS-based methods primarily\ntarget indoor scenes and rely on RGB-D sensors or pre-trained depth estimation\nmodels, hence underperforming in outdoor scenarios. To address this issue, we\npropose a RGB-only gaussian splatting SLAM method for unbounded outdoor\nscenes--OpenGS-SLAM. Technically, we first employ a pointmap regression network\nto generate consistent pointmaps between frames for pose estimation. Compared\nto commonly used depth maps, pointmaps include spatial relationships and scene\ngeometry across multiple views, enabling robust camera pose estimation. Then,\nwe propose integrating the estimated camera poses with 3DGS rendering as an\nend-to-end differentiable pipeline. Our method achieves simultaneous\noptimization of camera poses and 3DGS scene parameters, significantly enhancing\nsystem tracking accuracy. Specifically, we also design an adaptive scale mapper\nfor the pointmap regression network, which provides more accurate pointmap\nmapping to the 3DGS map representation. Our experiments on the Waymo dataset\ndemonstrate that OpenGS-SLAM reduces tracking error to 9.8\\% of previous 3DGS\nmethods, and achieves state-of-the-art results in novel view synthesis. Project\nPage: https://3dagentworld.github.io/opengs-slam/\n", "link": "http://arxiv.org/abs/2502.15633v1", "date": "2025-02-21", "relevancy": 3.5183, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.8092}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6722}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6296}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RGB-Only%20Gaussian%20Splatting%20SLAM%20for%20Unbounded%20Outdoor%20Scenes&body=Title%3A%20RGB-Only%20Gaussian%20Splatting%20SLAM%20for%20Unbounded%20Outdoor%20Scenes%0AAuthor%3A%20Sicheng%20Yu%20and%20Chong%20Cheng%20and%20Yifan%20Zhou%20and%20Xiaojun%20Yang%20and%20Hao%20Wang%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20become%20a%20popular%20solution%20in%20SLAM%2C%20as%20it%20can%0Aproduce%20high-fidelity%20novel%20views.%20However%2C%20previous%20GS-based%20methods%20primarily%0Atarget%20indoor%20scenes%20and%20rely%20on%20RGB-D%20sensors%20or%20pre-trained%20depth%20estimation%0Amodels%2C%20hence%20underperforming%20in%20outdoor%20scenarios.%20To%20address%20this%20issue%2C%20we%0Apropose%20a%20RGB-only%20gaussian%20splatting%20SLAM%20method%20for%20unbounded%20outdoor%0Ascenes--OpenGS-SLAM.%20Technically%2C%20we%20first%20employ%20a%20pointmap%20regression%20network%0Ato%20generate%20consistent%20pointmaps%20between%20frames%20for%20pose%20estimation.%20Compared%0Ato%20commonly%20used%20depth%20maps%2C%20pointmaps%20include%20spatial%20relationships%20and%20scene%0Ageometry%20across%20multiple%20views%2C%20enabling%20robust%20camera%20pose%20estimation.%20Then%2C%0Awe%20propose%20integrating%20the%20estimated%20camera%20poses%20with%203DGS%20rendering%20as%20an%0Aend-to-end%20differentiable%20pipeline.%20Our%20method%20achieves%20simultaneous%0Aoptimization%20of%20camera%20poses%20and%203DGS%20scene%20parameters%2C%20significantly%20enhancing%0Asystem%20tracking%20accuracy.%20Specifically%2C%20we%20also%20design%20an%20adaptive%20scale%20mapper%0Afor%20the%20pointmap%20regression%20network%2C%20which%20provides%20more%20accurate%20pointmap%0Amapping%20to%20the%203DGS%20map%20representation.%20Our%20experiments%20on%20the%20Waymo%20dataset%0Ademonstrate%20that%20OpenGS-SLAM%20reduces%20tracking%20error%20to%209.8%5C%25%20of%20previous%203DGS%0Amethods%2C%20and%20achieves%20state-of-the-art%20results%20in%20novel%20view%20synthesis.%20Project%0APage%3A%20https%3A//3dagentworld.github.io/opengs-slam/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15633v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRGB-Only%2520Gaussian%2520Splatting%2520SLAM%2520for%2520Unbounded%2520Outdoor%2520Scenes%26entry.906535625%3DSicheng%2520Yu%2520and%2520Chong%2520Cheng%2520and%2520Yifan%2520Zhou%2520and%2520Xiaojun%2520Yang%2520and%2520Hao%2520Wang%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520become%2520a%2520popular%2520solution%2520in%2520SLAM%252C%2520as%2520it%2520can%250Aproduce%2520high-fidelity%2520novel%2520views.%2520However%252C%2520previous%2520GS-based%2520methods%2520primarily%250Atarget%2520indoor%2520scenes%2520and%2520rely%2520on%2520RGB-D%2520sensors%2520or%2520pre-trained%2520depth%2520estimation%250Amodels%252C%2520hence%2520underperforming%2520in%2520outdoor%2520scenarios.%2520To%2520address%2520this%2520issue%252C%2520we%250Apropose%2520a%2520RGB-only%2520gaussian%2520splatting%2520SLAM%2520method%2520for%2520unbounded%2520outdoor%250Ascenes--OpenGS-SLAM.%2520Technically%252C%2520we%2520first%2520employ%2520a%2520pointmap%2520regression%2520network%250Ato%2520generate%2520consistent%2520pointmaps%2520between%2520frames%2520for%2520pose%2520estimation.%2520Compared%250Ato%2520commonly%2520used%2520depth%2520maps%252C%2520pointmaps%2520include%2520spatial%2520relationships%2520and%2520scene%250Ageometry%2520across%2520multiple%2520views%252C%2520enabling%2520robust%2520camera%2520pose%2520estimation.%2520Then%252C%250Awe%2520propose%2520integrating%2520the%2520estimated%2520camera%2520poses%2520with%25203DGS%2520rendering%2520as%2520an%250Aend-to-end%2520differentiable%2520pipeline.%2520Our%2520method%2520achieves%2520simultaneous%250Aoptimization%2520of%2520camera%2520poses%2520and%25203DGS%2520scene%2520parameters%252C%2520significantly%2520enhancing%250Asystem%2520tracking%2520accuracy.%2520Specifically%252C%2520we%2520also%2520design%2520an%2520adaptive%2520scale%2520mapper%250Afor%2520the%2520pointmap%2520regression%2520network%252C%2520which%2520provides%2520more%2520accurate%2520pointmap%250Amapping%2520to%2520the%25203DGS%2520map%2520representation.%2520Our%2520experiments%2520on%2520the%2520Waymo%2520dataset%250Ademonstrate%2520that%2520OpenGS-SLAM%2520reduces%2520tracking%2520error%2520to%25209.8%255C%2525%2520of%2520previous%25203DGS%250Amethods%252C%2520and%2520achieves%2520state-of-the-art%2520results%2520in%2520novel%2520view%2520synthesis.%2520Project%250APage%253A%2520https%253A//3dagentworld.github.io/opengs-slam/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15633v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RGB-Only%20Gaussian%20Splatting%20SLAM%20for%20Unbounded%20Outdoor%20Scenes&entry.906535625=Sicheng%20Yu%20and%20Chong%20Cheng%20and%20Yifan%20Zhou%20and%20Xiaojun%20Yang%20and%20Hao%20Wang&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20become%20a%20popular%20solution%20in%20SLAM%2C%20as%20it%20can%0Aproduce%20high-fidelity%20novel%20views.%20However%2C%20previous%20GS-based%20methods%20primarily%0Atarget%20indoor%20scenes%20and%20rely%20on%20RGB-D%20sensors%20or%20pre-trained%20depth%20estimation%0Amodels%2C%20hence%20underperforming%20in%20outdoor%20scenarios.%20To%20address%20this%20issue%2C%20we%0Apropose%20a%20RGB-only%20gaussian%20splatting%20SLAM%20method%20for%20unbounded%20outdoor%0Ascenes--OpenGS-SLAM.%20Technically%2C%20we%20first%20employ%20a%20pointmap%20regression%20network%0Ato%20generate%20consistent%20pointmaps%20between%20frames%20for%20pose%20estimation.%20Compared%0Ato%20commonly%20used%20depth%20maps%2C%20pointmaps%20include%20spatial%20relationships%20and%20scene%0Ageometry%20across%20multiple%20views%2C%20enabling%20robust%20camera%20pose%20estimation.%20Then%2C%0Awe%20propose%20integrating%20the%20estimated%20camera%20poses%20with%203DGS%20rendering%20as%20an%0Aend-to-end%20differentiable%20pipeline.%20Our%20method%20achieves%20simultaneous%0Aoptimization%20of%20camera%20poses%20and%203DGS%20scene%20parameters%2C%20significantly%20enhancing%0Asystem%20tracking%20accuracy.%20Specifically%2C%20we%20also%20design%20an%20adaptive%20scale%20mapper%0Afor%20the%20pointmap%20regression%20network%2C%20which%20provides%20more%20accurate%20pointmap%0Amapping%20to%20the%203DGS%20map%20representation.%20Our%20experiments%20on%20the%20Waymo%20dataset%0Ademonstrate%20that%20OpenGS-SLAM%20reduces%20tracking%20error%20to%209.8%5C%25%20of%20previous%203DGS%0Amethods%2C%20and%20achieves%20state-of-the-art%20results%20in%20novel%20view%20synthesis.%20Project%0APage%3A%20https%3A//3dagentworld.github.io/opengs-slam/%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15633v1&entry.124074799=Read"},
{"title": "HumanGif: Single-View Human Diffusion with Generative Prior", "author": "Shoukang Hu and Takuya Narihira and Kazumi Fukuda and Ryosuke Sawata and Takashi Shibuya and Yuki Mitsufuji", "abstract": "  Previous 3D human creation methods have made significant progress in\nsynthesizing view-consistent and temporally aligned results from sparse-view\nimages or monocular videos. However, it remains challenging to produce\nperpetually realistic, view-consistent, and temporally coherent human avatars\nfrom a single image, as limited information is available in the single-view\ninput setting. Motivated by the success of 2D character animation, we propose\nHumanGif, a single-view human diffusion model with generative prior.\nSpecifically, we formulate the single-view-based 3D human novel view and pose\nsynthesis as a single-view-conditioned human diffusion process, utilizing\ngenerative priors from foundational diffusion models to complement the missing\ninformation. To ensure fine-grained and consistent novel view and pose\nsynthesis, we introduce a Human NeRF module in HumanGif to learn spatially\naligned features from the input image, implicitly capturing the relative camera\nand human pose transformation. Furthermore, we introduce an image-level loss\nduring optimization to bridge the gap between latent and image spaces in\ndiffusion models. Extensive experiments on RenderPeople and DNA-Rendering\ndatasets demonstrate that HumanGif achieves the best perceptual performance,\nwith better generalizability for novel view and pose synthesis.\n", "link": "http://arxiv.org/abs/2502.12080v2", "date": "2025-02-21", "relevancy": 3.388, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.748}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6466}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HumanGif%3A%20Single-View%20Human%20Diffusion%20with%20Generative%20Prior&body=Title%3A%20HumanGif%3A%20Single-View%20Human%20Diffusion%20with%20Generative%20Prior%0AAuthor%3A%20Shoukang%20Hu%20and%20Takuya%20Narihira%20and%20Kazumi%20Fukuda%20and%20Ryosuke%20Sawata%20and%20Takashi%20Shibuya%20and%20Yuki%20Mitsufuji%0AAbstract%3A%20%20%20Previous%203D%20human%20creation%20methods%20have%20made%20significant%20progress%20in%0Asynthesizing%20view-consistent%20and%20temporally%20aligned%20results%20from%20sparse-view%0Aimages%20or%20monocular%20videos.%20However%2C%20it%20remains%20challenging%20to%20produce%0Aperpetually%20realistic%2C%20view-consistent%2C%20and%20temporally%20coherent%20human%20avatars%0Afrom%20a%20single%20image%2C%20as%20limited%20information%20is%20available%20in%20the%20single-view%0Ainput%20setting.%20Motivated%20by%20the%20success%20of%202D%20character%20animation%2C%20we%20propose%0AHumanGif%2C%20a%20single-view%20human%20diffusion%20model%20with%20generative%20prior.%0ASpecifically%2C%20we%20formulate%20the%20single-view-based%203D%20human%20novel%20view%20and%20pose%0Asynthesis%20as%20a%20single-view-conditioned%20human%20diffusion%20process%2C%20utilizing%0Agenerative%20priors%20from%20foundational%20diffusion%20models%20to%20complement%20the%20missing%0Ainformation.%20To%20ensure%20fine-grained%20and%20consistent%20novel%20view%20and%20pose%0Asynthesis%2C%20we%20introduce%20a%20Human%20NeRF%20module%20in%20HumanGif%20to%20learn%20spatially%0Aaligned%20features%20from%20the%20input%20image%2C%20implicitly%20capturing%20the%20relative%20camera%0Aand%20human%20pose%20transformation.%20Furthermore%2C%20we%20introduce%20an%20image-level%20loss%0Aduring%20optimization%20to%20bridge%20the%20gap%20between%20latent%20and%20image%20spaces%20in%0Adiffusion%20models.%20Extensive%20experiments%20on%20RenderPeople%20and%20DNA-Rendering%0Adatasets%20demonstrate%20that%20HumanGif%20achieves%20the%20best%20perceptual%20performance%2C%0Awith%20better%20generalizability%20for%20novel%20view%20and%20pose%20synthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12080v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHumanGif%253A%2520Single-View%2520Human%2520Diffusion%2520with%2520Generative%2520Prior%26entry.906535625%3DShoukang%2520Hu%2520and%2520Takuya%2520Narihira%2520and%2520Kazumi%2520Fukuda%2520and%2520Ryosuke%2520Sawata%2520and%2520Takashi%2520Shibuya%2520and%2520Yuki%2520Mitsufuji%26entry.1292438233%3D%2520%2520Previous%25203D%2520human%2520creation%2520methods%2520have%2520made%2520significant%2520progress%2520in%250Asynthesizing%2520view-consistent%2520and%2520temporally%2520aligned%2520results%2520from%2520sparse-view%250Aimages%2520or%2520monocular%2520videos.%2520However%252C%2520it%2520remains%2520challenging%2520to%2520produce%250Aperpetually%2520realistic%252C%2520view-consistent%252C%2520and%2520temporally%2520coherent%2520human%2520avatars%250Afrom%2520a%2520single%2520image%252C%2520as%2520limited%2520information%2520is%2520available%2520in%2520the%2520single-view%250Ainput%2520setting.%2520Motivated%2520by%2520the%2520success%2520of%25202D%2520character%2520animation%252C%2520we%2520propose%250AHumanGif%252C%2520a%2520single-view%2520human%2520diffusion%2520model%2520with%2520generative%2520prior.%250ASpecifically%252C%2520we%2520formulate%2520the%2520single-view-based%25203D%2520human%2520novel%2520view%2520and%2520pose%250Asynthesis%2520as%2520a%2520single-view-conditioned%2520human%2520diffusion%2520process%252C%2520utilizing%250Agenerative%2520priors%2520from%2520foundational%2520diffusion%2520models%2520to%2520complement%2520the%2520missing%250Ainformation.%2520To%2520ensure%2520fine-grained%2520and%2520consistent%2520novel%2520view%2520and%2520pose%250Asynthesis%252C%2520we%2520introduce%2520a%2520Human%2520NeRF%2520module%2520in%2520HumanGif%2520to%2520learn%2520spatially%250Aaligned%2520features%2520from%2520the%2520input%2520image%252C%2520implicitly%2520capturing%2520the%2520relative%2520camera%250Aand%2520human%2520pose%2520transformation.%2520Furthermore%252C%2520we%2520introduce%2520an%2520image-level%2520loss%250Aduring%2520optimization%2520to%2520bridge%2520the%2520gap%2520between%2520latent%2520and%2520image%2520spaces%2520in%250Adiffusion%2520models.%2520Extensive%2520experiments%2520on%2520RenderPeople%2520and%2520DNA-Rendering%250Adatasets%2520demonstrate%2520that%2520HumanGif%2520achieves%2520the%2520best%2520perceptual%2520performance%252C%250Awith%2520better%2520generalizability%2520for%2520novel%2520view%2520and%2520pose%2520synthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12080v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HumanGif%3A%20Single-View%20Human%20Diffusion%20with%20Generative%20Prior&entry.906535625=Shoukang%20Hu%20and%20Takuya%20Narihira%20and%20Kazumi%20Fukuda%20and%20Ryosuke%20Sawata%20and%20Takashi%20Shibuya%20and%20Yuki%20Mitsufuji&entry.1292438233=%20%20Previous%203D%20human%20creation%20methods%20have%20made%20significant%20progress%20in%0Asynthesizing%20view-consistent%20and%20temporally%20aligned%20results%20from%20sparse-view%0Aimages%20or%20monocular%20videos.%20However%2C%20it%20remains%20challenging%20to%20produce%0Aperpetually%20realistic%2C%20view-consistent%2C%20and%20temporally%20coherent%20human%20avatars%0Afrom%20a%20single%20image%2C%20as%20limited%20information%20is%20available%20in%20the%20single-view%0Ainput%20setting.%20Motivated%20by%20the%20success%20of%202D%20character%20animation%2C%20we%20propose%0AHumanGif%2C%20a%20single-view%20human%20diffusion%20model%20with%20generative%20prior.%0ASpecifically%2C%20we%20formulate%20the%20single-view-based%203D%20human%20novel%20view%20and%20pose%0Asynthesis%20as%20a%20single-view-conditioned%20human%20diffusion%20process%2C%20utilizing%0Agenerative%20priors%20from%20foundational%20diffusion%20models%20to%20complement%20the%20missing%0Ainformation.%20To%20ensure%20fine-grained%20and%20consistent%20novel%20view%20and%20pose%0Asynthesis%2C%20we%20introduce%20a%20Human%20NeRF%20module%20in%20HumanGif%20to%20learn%20spatially%0Aaligned%20features%20from%20the%20input%20image%2C%20implicitly%20capturing%20the%20relative%20camera%0Aand%20human%20pose%20transformation.%20Furthermore%2C%20we%20introduce%20an%20image-level%20loss%0Aduring%20optimization%20to%20bridge%20the%20gap%20between%20latent%20and%20image%20spaces%20in%0Adiffusion%20models.%20Extensive%20experiments%20on%20RenderPeople%20and%20DNA-Rendering%0Adatasets%20demonstrate%20that%20HumanGif%20achieves%20the%20best%20perceptual%20performance%2C%0Awith%20better%20generalizability%20for%20novel%20view%20and%20pose%20synthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12080v2&entry.124074799=Read"},
{"title": "MOVE: A Mixture-of-Vision-Encoders Approach for Domain-Focused\n  Vision-Language Processing", "author": "Matvey Skripkin and Elizaveta Goncharova and Dmitrii Tarasov and Andrey Kuznetsov", "abstract": "  Multimodal language models (MLMs) integrate visual and textual information by\ncoupling a vision encoder with a large language model through the specific\nadapter. While existing approaches commonly rely on a single pre-trained vision\nencoder, there is a great variability of specialized encoders that can boost\nmodel's performance in distinct domains. In this work, we propose MOVE (Mixture\nof Vision Encoders) a simple yet effective approach to leverage multiple\npre-trained encoders for specialized multimodal tasks. MOVE automatically\nroutes inputs to the most appropriate encoder among candidates such as Unichat,\nInternViT, and Texify, thereby enhancing performance across a diverse set of\nbenchmarks, including ChartQA, MMBench, and MMMU. Experimental results\ndemonstrate that MOVE achieves competitive accuracy without incurring the\ncomplexities of image slicing for high-resolution images.\n", "link": "http://arxiv.org/abs/2502.15381v1", "date": "2025-02-21", "relevancy": 3.1064, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6416}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6416}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOVE%3A%20A%20Mixture-of-Vision-Encoders%20Approach%20for%20Domain-Focused%0A%20%20Vision-Language%20Processing&body=Title%3A%20MOVE%3A%20A%20Mixture-of-Vision-Encoders%20Approach%20for%20Domain-Focused%0A%20%20Vision-Language%20Processing%0AAuthor%3A%20Matvey%20Skripkin%20and%20Elizaveta%20Goncharova%20and%20Dmitrii%20Tarasov%20and%20Andrey%20Kuznetsov%0AAbstract%3A%20%20%20Multimodal%20language%20models%20%28MLMs%29%20integrate%20visual%20and%20textual%20information%20by%0Acoupling%20a%20vision%20encoder%20with%20a%20large%20language%20model%20through%20the%20specific%0Aadapter.%20While%20existing%20approaches%20commonly%20rely%20on%20a%20single%20pre-trained%20vision%0Aencoder%2C%20there%20is%20a%20great%20variability%20of%20specialized%20encoders%20that%20can%20boost%0Amodel%27s%20performance%20in%20distinct%20domains.%20In%20this%20work%2C%20we%20propose%20MOVE%20%28Mixture%0Aof%20Vision%20Encoders%29%20a%20simple%20yet%20effective%20approach%20to%20leverage%20multiple%0Apre-trained%20encoders%20for%20specialized%20multimodal%20tasks.%20MOVE%20automatically%0Aroutes%20inputs%20to%20the%20most%20appropriate%20encoder%20among%20candidates%20such%20as%20Unichat%2C%0AInternViT%2C%20and%20Texify%2C%20thereby%20enhancing%20performance%20across%20a%20diverse%20set%20of%0Abenchmarks%2C%20including%20ChartQA%2C%20MMBench%2C%20and%20MMMU.%20Experimental%20results%0Ademonstrate%20that%20MOVE%20achieves%20competitive%20accuracy%20without%20incurring%20the%0Acomplexities%20of%20image%20slicing%20for%20high-resolution%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15381v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOVE%253A%2520A%2520Mixture-of-Vision-Encoders%2520Approach%2520for%2520Domain-Focused%250A%2520%2520Vision-Language%2520Processing%26entry.906535625%3DMatvey%2520Skripkin%2520and%2520Elizaveta%2520Goncharova%2520and%2520Dmitrii%2520Tarasov%2520and%2520Andrey%2520Kuznetsov%26entry.1292438233%3D%2520%2520Multimodal%2520language%2520models%2520%2528MLMs%2529%2520integrate%2520visual%2520and%2520textual%2520information%2520by%250Acoupling%2520a%2520vision%2520encoder%2520with%2520a%2520large%2520language%2520model%2520through%2520the%2520specific%250Aadapter.%2520While%2520existing%2520approaches%2520commonly%2520rely%2520on%2520a%2520single%2520pre-trained%2520vision%250Aencoder%252C%2520there%2520is%2520a%2520great%2520variability%2520of%2520specialized%2520encoders%2520that%2520can%2520boost%250Amodel%2527s%2520performance%2520in%2520distinct%2520domains.%2520In%2520this%2520work%252C%2520we%2520propose%2520MOVE%2520%2528Mixture%250Aof%2520Vision%2520Encoders%2529%2520a%2520simple%2520yet%2520effective%2520approach%2520to%2520leverage%2520multiple%250Apre-trained%2520encoders%2520for%2520specialized%2520multimodal%2520tasks.%2520MOVE%2520automatically%250Aroutes%2520inputs%2520to%2520the%2520most%2520appropriate%2520encoder%2520among%2520candidates%2520such%2520as%2520Unichat%252C%250AInternViT%252C%2520and%2520Texify%252C%2520thereby%2520enhancing%2520performance%2520across%2520a%2520diverse%2520set%2520of%250Abenchmarks%252C%2520including%2520ChartQA%252C%2520MMBench%252C%2520and%2520MMMU.%2520Experimental%2520results%250Ademonstrate%2520that%2520MOVE%2520achieves%2520competitive%2520accuracy%2520without%2520incurring%2520the%250Acomplexities%2520of%2520image%2520slicing%2520for%2520high-resolution%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15381v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOVE%3A%20A%20Mixture-of-Vision-Encoders%20Approach%20for%20Domain-Focused%0A%20%20Vision-Language%20Processing&entry.906535625=Matvey%20Skripkin%20and%20Elizaveta%20Goncharova%20and%20Dmitrii%20Tarasov%20and%20Andrey%20Kuznetsov&entry.1292438233=%20%20Multimodal%20language%20models%20%28MLMs%29%20integrate%20visual%20and%20textual%20information%20by%0Acoupling%20a%20vision%20encoder%20with%20a%20large%20language%20model%20through%20the%20specific%0Aadapter.%20While%20existing%20approaches%20commonly%20rely%20on%20a%20single%20pre-trained%20vision%0Aencoder%2C%20there%20is%20a%20great%20variability%20of%20specialized%20encoders%20that%20can%20boost%0Amodel%27s%20performance%20in%20distinct%20domains.%20In%20this%20work%2C%20we%20propose%20MOVE%20%28Mixture%0Aof%20Vision%20Encoders%29%20a%20simple%20yet%20effective%20approach%20to%20leverage%20multiple%0Apre-trained%20encoders%20for%20specialized%20multimodal%20tasks.%20MOVE%20automatically%0Aroutes%20inputs%20to%20the%20most%20appropriate%20encoder%20among%20candidates%20such%20as%20Unichat%2C%0AInternViT%2C%20and%20Texify%2C%20thereby%20enhancing%20performance%20across%20a%20diverse%20set%20of%0Abenchmarks%2C%20including%20ChartQA%2C%20MMBench%2C%20and%20MMMU.%20Experimental%20results%0Ademonstrate%20that%20MOVE%20achieves%20competitive%20accuracy%20without%20incurring%20the%0Acomplexities%20of%20image%20slicing%20for%20high-resolution%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15381v1&entry.124074799=Read"},
{"title": "ELIP: Enhanced Visual-Language Foundation Models for Image Retrieval", "author": "Guanqi Zhan and Yuanpei Liu and Kai Han and Weidi Xie and Andrew Zisserman", "abstract": "  The objective in this paper is to improve the performance of text-to-image\nretrieval. To this end, we introduce a new framework that can boost the\nperformance of large-scale pre-trained vision-language models, so that they can\nbe used for text-to-image re-ranking. The approach, Enhanced Language-Image\nPre-training (ELIP), uses the text query to predict a set of visual prompts to\ncondition the ViT image encoding. ELIP can easily be applied to the commonly\nused CLIP/SigLIP and the state-of-the-art BLIP-2 architectures. To train the\narchitecture with limited computing resources, we develop a 'student friendly'\nbest practice involving global hard sample mining, and selection and curation\nof a large-scale dataset. On the evaluation side, we set up two new\nout-of-distribution benchmarks, Occluded COCO and ImageNet-R, to assess the\nzero-shot generalisation of the models to different domains. Benefiting from\nthe novel architecture and data curation, experiments show our enhanced network\nsignificantly boosts CLIP/SigLIP performance and outperforms the\nstate-of-the-art BLIP-2 model on text-to-image retrieval.\n", "link": "http://arxiv.org/abs/2502.15682v1", "date": "2025-02-21", "relevancy": 2.9428, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6006}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6006}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ELIP%3A%20Enhanced%20Visual-Language%20Foundation%20Models%20for%20Image%20Retrieval&body=Title%3A%20ELIP%3A%20Enhanced%20Visual-Language%20Foundation%20Models%20for%20Image%20Retrieval%0AAuthor%3A%20Guanqi%20Zhan%20and%20Yuanpei%20Liu%20and%20Kai%20Han%20and%20Weidi%20Xie%20and%20Andrew%20Zisserman%0AAbstract%3A%20%20%20The%20objective%20in%20this%20paper%20is%20to%20improve%20the%20performance%20of%20text-to-image%0Aretrieval.%20To%20this%20end%2C%20we%20introduce%20a%20new%20framework%20that%20can%20boost%20the%0Aperformance%20of%20large-scale%20pre-trained%20vision-language%20models%2C%20so%20that%20they%20can%0Abe%20used%20for%20text-to-image%20re-ranking.%20The%20approach%2C%20Enhanced%20Language-Image%0APre-training%20%28ELIP%29%2C%20uses%20the%20text%20query%20to%20predict%20a%20set%20of%20visual%20prompts%20to%0Acondition%20the%20ViT%20image%20encoding.%20ELIP%20can%20easily%20be%20applied%20to%20the%20commonly%0Aused%20CLIP/SigLIP%20and%20the%20state-of-the-art%20BLIP-2%20architectures.%20To%20train%20the%0Aarchitecture%20with%20limited%20computing%20resources%2C%20we%20develop%20a%20%27student%20friendly%27%0Abest%20practice%20involving%20global%20hard%20sample%20mining%2C%20and%20selection%20and%20curation%0Aof%20a%20large-scale%20dataset.%20On%20the%20evaluation%20side%2C%20we%20set%20up%20two%20new%0Aout-of-distribution%20benchmarks%2C%20Occluded%20COCO%20and%20ImageNet-R%2C%20to%20assess%20the%0Azero-shot%20generalisation%20of%20the%20models%20to%20different%20domains.%20Benefiting%20from%0Athe%20novel%20architecture%20and%20data%20curation%2C%20experiments%20show%20our%20enhanced%20network%0Asignificantly%20boosts%20CLIP/SigLIP%20performance%20and%20outperforms%20the%0Astate-of-the-art%20BLIP-2%20model%20on%20text-to-image%20retrieval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DELIP%253A%2520Enhanced%2520Visual-Language%2520Foundation%2520Models%2520for%2520Image%2520Retrieval%26entry.906535625%3DGuanqi%2520Zhan%2520and%2520Yuanpei%2520Liu%2520and%2520Kai%2520Han%2520and%2520Weidi%2520Xie%2520and%2520Andrew%2520Zisserman%26entry.1292438233%3D%2520%2520The%2520objective%2520in%2520this%2520paper%2520is%2520to%2520improve%2520the%2520performance%2520of%2520text-to-image%250Aretrieval.%2520To%2520this%2520end%252C%2520we%2520introduce%2520a%2520new%2520framework%2520that%2520can%2520boost%2520the%250Aperformance%2520of%2520large-scale%2520pre-trained%2520vision-language%2520models%252C%2520so%2520that%2520they%2520can%250Abe%2520used%2520for%2520text-to-image%2520re-ranking.%2520The%2520approach%252C%2520Enhanced%2520Language-Image%250APre-training%2520%2528ELIP%2529%252C%2520uses%2520the%2520text%2520query%2520to%2520predict%2520a%2520set%2520of%2520visual%2520prompts%2520to%250Acondition%2520the%2520ViT%2520image%2520encoding.%2520ELIP%2520can%2520easily%2520be%2520applied%2520to%2520the%2520commonly%250Aused%2520CLIP/SigLIP%2520and%2520the%2520state-of-the-art%2520BLIP-2%2520architectures.%2520To%2520train%2520the%250Aarchitecture%2520with%2520limited%2520computing%2520resources%252C%2520we%2520develop%2520a%2520%2527student%2520friendly%2527%250Abest%2520practice%2520involving%2520global%2520hard%2520sample%2520mining%252C%2520and%2520selection%2520and%2520curation%250Aof%2520a%2520large-scale%2520dataset.%2520On%2520the%2520evaluation%2520side%252C%2520we%2520set%2520up%2520two%2520new%250Aout-of-distribution%2520benchmarks%252C%2520Occluded%2520COCO%2520and%2520ImageNet-R%252C%2520to%2520assess%2520the%250Azero-shot%2520generalisation%2520of%2520the%2520models%2520to%2520different%2520domains.%2520Benefiting%2520from%250Athe%2520novel%2520architecture%2520and%2520data%2520curation%252C%2520experiments%2520show%2520our%2520enhanced%2520network%250Asignificantly%2520boosts%2520CLIP/SigLIP%2520performance%2520and%2520outperforms%2520the%250Astate-of-the-art%2520BLIP-2%2520model%2520on%2520text-to-image%2520retrieval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ELIP%3A%20Enhanced%20Visual-Language%20Foundation%20Models%20for%20Image%20Retrieval&entry.906535625=Guanqi%20Zhan%20and%20Yuanpei%20Liu%20and%20Kai%20Han%20and%20Weidi%20Xie%20and%20Andrew%20Zisserman&entry.1292438233=%20%20The%20objective%20in%20this%20paper%20is%20to%20improve%20the%20performance%20of%20text-to-image%0Aretrieval.%20To%20this%20end%2C%20we%20introduce%20a%20new%20framework%20that%20can%20boost%20the%0Aperformance%20of%20large-scale%20pre-trained%20vision-language%20models%2C%20so%20that%20they%20can%0Abe%20used%20for%20text-to-image%20re-ranking.%20The%20approach%2C%20Enhanced%20Language-Image%0APre-training%20%28ELIP%29%2C%20uses%20the%20text%20query%20to%20predict%20a%20set%20of%20visual%20prompts%20to%0Acondition%20the%20ViT%20image%20encoding.%20ELIP%20can%20easily%20be%20applied%20to%20the%20commonly%0Aused%20CLIP/SigLIP%20and%20the%20state-of-the-art%20BLIP-2%20architectures.%20To%20train%20the%0Aarchitecture%20with%20limited%20computing%20resources%2C%20we%20develop%20a%20%27student%20friendly%27%0Abest%20practice%20involving%20global%20hard%20sample%20mining%2C%20and%20selection%20and%20curation%0Aof%20a%20large-scale%20dataset.%20On%20the%20evaluation%20side%2C%20we%20set%20up%20two%20new%0Aout-of-distribution%20benchmarks%2C%20Occluded%20COCO%20and%20ImageNet-R%2C%20to%20assess%20the%0Azero-shot%20generalisation%20of%20the%20models%20to%20different%20domains.%20Benefiting%20from%0Athe%20novel%20architecture%20and%20data%20curation%2C%20experiments%20show%20our%20enhanced%20network%0Asignificantly%20boosts%20CLIP/SigLIP%20performance%20and%20outperforms%20the%0Astate-of-the-art%20BLIP-2%20model%20on%20text-to-image%20retrieval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15682v1&entry.124074799=Read"},
{"title": "Sparse Voxels Rasterization: Real-time High-fidelity Radiance Field\n  Rendering", "author": "Cheng Sun and Jaesung Choe and Charles Loop and Wei-Chiu Ma and Yu-Chiang Frank Wang", "abstract": "  We propose an efficient radiance field rendering algorithm that incorporates\na rasterization process on adaptive sparse voxels without neural networks or 3D\nGaussians. There are two key contributions coupled with the proposed system.\nThe first is to adaptively and explicitly allocate sparse voxels to different\nlevels of detail within scenes, faithfully reproducing scene details with\n$65536^3$ grid resolution while achieving high rendering frame rates. Second,\nwe customize a rasterizer for efficient adaptive sparse voxels rendering. We\nrender voxels in the correct depth order by using ray direction-dependent\nMorton ordering, which avoids the well-known popping artifact found in Gaussian\nsplatting. Our method improves the previous neural-free voxel model by over 4db\nPSNR and more than 10x FPS speedup, achieving state-of-the-art comparable\nnovel-view synthesis results. Additionally, our voxel representation is\nseamlessly compatible with grid-based 3D processing techniques such as Volume\nFusion, Voxel Pooling, and Marching Cubes, enabling a wide range of future\nextensions and applications.\n", "link": "http://arxiv.org/abs/2412.04459v2", "date": "2025-02-21", "relevancy": 2.9379, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6569}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.57}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Voxels%20Rasterization%3A%20Real-time%20High-fidelity%20Radiance%20Field%0A%20%20Rendering&body=Title%3A%20Sparse%20Voxels%20Rasterization%3A%20Real-time%20High-fidelity%20Radiance%20Field%0A%20%20Rendering%0AAuthor%3A%20Cheng%20Sun%20and%20Jaesung%20Choe%20and%20Charles%20Loop%20and%20Wei-Chiu%20Ma%20and%20Yu-Chiang%20Frank%20Wang%0AAbstract%3A%20%20%20We%20propose%20an%20efficient%20radiance%20field%20rendering%20algorithm%20that%20incorporates%0Aa%20rasterization%20process%20on%20adaptive%20sparse%20voxels%20without%20neural%20networks%20or%203D%0AGaussians.%20There%20are%20two%20key%20contributions%20coupled%20with%20the%20proposed%20system.%0AThe%20first%20is%20to%20adaptively%20and%20explicitly%20allocate%20sparse%20voxels%20to%20different%0Alevels%20of%20detail%20within%20scenes%2C%20faithfully%20reproducing%20scene%20details%20with%0A%2465536%5E3%24%20grid%20resolution%20while%20achieving%20high%20rendering%20frame%20rates.%20Second%2C%0Awe%20customize%20a%20rasterizer%20for%20efficient%20adaptive%20sparse%20voxels%20rendering.%20We%0Arender%20voxels%20in%20the%20correct%20depth%20order%20by%20using%20ray%20direction-dependent%0AMorton%20ordering%2C%20which%20avoids%20the%20well-known%20popping%20artifact%20found%20in%20Gaussian%0Asplatting.%20Our%20method%20improves%20the%20previous%20neural-free%20voxel%20model%20by%20over%204db%0APSNR%20and%20more%20than%2010x%20FPS%20speedup%2C%20achieving%20state-of-the-art%20comparable%0Anovel-view%20synthesis%20results.%20Additionally%2C%20our%20voxel%20representation%20is%0Aseamlessly%20compatible%20with%20grid-based%203D%20processing%20techniques%20such%20as%20Volume%0AFusion%2C%20Voxel%20Pooling%2C%20and%20Marching%20Cubes%2C%20enabling%20a%20wide%20range%20of%20future%0Aextensions%20and%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.04459v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Voxels%2520Rasterization%253A%2520Real-time%2520High-fidelity%2520Radiance%2520Field%250A%2520%2520Rendering%26entry.906535625%3DCheng%2520Sun%2520and%2520Jaesung%2520Choe%2520and%2520Charles%2520Loop%2520and%2520Wei-Chiu%2520Ma%2520and%2520Yu-Chiang%2520Frank%2520Wang%26entry.1292438233%3D%2520%2520We%2520propose%2520an%2520efficient%2520radiance%2520field%2520rendering%2520algorithm%2520that%2520incorporates%250Aa%2520rasterization%2520process%2520on%2520adaptive%2520sparse%2520voxels%2520without%2520neural%2520networks%2520or%25203D%250AGaussians.%2520There%2520are%2520two%2520key%2520contributions%2520coupled%2520with%2520the%2520proposed%2520system.%250AThe%2520first%2520is%2520to%2520adaptively%2520and%2520explicitly%2520allocate%2520sparse%2520voxels%2520to%2520different%250Alevels%2520of%2520detail%2520within%2520scenes%252C%2520faithfully%2520reproducing%2520scene%2520details%2520with%250A%252465536%255E3%2524%2520grid%2520resolution%2520while%2520achieving%2520high%2520rendering%2520frame%2520rates.%2520Second%252C%250Awe%2520customize%2520a%2520rasterizer%2520for%2520efficient%2520adaptive%2520sparse%2520voxels%2520rendering.%2520We%250Arender%2520voxels%2520in%2520the%2520correct%2520depth%2520order%2520by%2520using%2520ray%2520direction-dependent%250AMorton%2520ordering%252C%2520which%2520avoids%2520the%2520well-known%2520popping%2520artifact%2520found%2520in%2520Gaussian%250Asplatting.%2520Our%2520method%2520improves%2520the%2520previous%2520neural-free%2520voxel%2520model%2520by%2520over%25204db%250APSNR%2520and%2520more%2520than%252010x%2520FPS%2520speedup%252C%2520achieving%2520state-of-the-art%2520comparable%250Anovel-view%2520synthesis%2520results.%2520Additionally%252C%2520our%2520voxel%2520representation%2520is%250Aseamlessly%2520compatible%2520with%2520grid-based%25203D%2520processing%2520techniques%2520such%2520as%2520Volume%250AFusion%252C%2520Voxel%2520Pooling%252C%2520and%2520Marching%2520Cubes%252C%2520enabling%2520a%2520wide%2520range%2520of%2520future%250Aextensions%2520and%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.04459v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Voxels%20Rasterization%3A%20Real-time%20High-fidelity%20Radiance%20Field%0A%20%20Rendering&entry.906535625=Cheng%20Sun%20and%20Jaesung%20Choe%20and%20Charles%20Loop%20and%20Wei-Chiu%20Ma%20and%20Yu-Chiang%20Frank%20Wang&entry.1292438233=%20%20We%20propose%20an%20efficient%20radiance%20field%20rendering%20algorithm%20that%20incorporates%0Aa%20rasterization%20process%20on%20adaptive%20sparse%20voxels%20without%20neural%20networks%20or%203D%0AGaussians.%20There%20are%20two%20key%20contributions%20coupled%20with%20the%20proposed%20system.%0AThe%20first%20is%20to%20adaptively%20and%20explicitly%20allocate%20sparse%20voxels%20to%20different%0Alevels%20of%20detail%20within%20scenes%2C%20faithfully%20reproducing%20scene%20details%20with%0A%2465536%5E3%24%20grid%20resolution%20while%20achieving%20high%20rendering%20frame%20rates.%20Second%2C%0Awe%20customize%20a%20rasterizer%20for%20efficient%20adaptive%20sparse%20voxels%20rendering.%20We%0Arender%20voxels%20in%20the%20correct%20depth%20order%20by%20using%20ray%20direction-dependent%0AMorton%20ordering%2C%20which%20avoids%20the%20well-known%20popping%20artifact%20found%20in%20Gaussian%0Asplatting.%20Our%20method%20improves%20the%20previous%20neural-free%20voxel%20model%20by%20over%204db%0APSNR%20and%20more%20than%2010x%20FPS%20speedup%2C%20achieving%20state-of-the-art%20comparable%0Anovel-view%20synthesis%20results.%20Additionally%2C%20our%20voxel%20representation%20is%0Aseamlessly%20compatible%20with%20grid-based%203D%20processing%20techniques%20such%20as%20Volume%0AFusion%2C%20Voxel%20Pooling%2C%20and%20Marching%20Cubes%2C%20enabling%20a%20wide%20range%20of%20future%0Aextensions%20and%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.04459v2&entry.124074799=Read"},
{"title": "TexLiDAR: Automated Text Understanding for Panoramic LiDAR Data", "author": "Naor Cohen and Roy Orfaig and Ben-Zion Bobrovsky", "abstract": "  Efforts to connect LiDAR data with text, such as LidarCLIP, have primarily\nfocused on embedding 3D point clouds into CLIP text-image space. However, these\napproaches rely on 3D point clouds, which present challenges in encoding\nefficiency and neural network processing. With the advent of advanced LiDAR\nsensors like Ouster OS1, which, in addition to 3D point clouds, produce fixed\nresolution depth, signal, and ambient panoramic 2D images, new opportunities\nemerge for LiDAR based tasks. In this work, we propose an alternative approach\nto connect LiDAR data with text by leveraging 2D imagery generated by the OS1\nsensor instead of 3D point clouds. Using the Florence 2 large model in a\nzero-shot setting, we perform image captioning and object detection. Our\nexperiments demonstrate that Florence 2 generates more informative captions and\nachieves superior performance in object detection tasks compared to existing\nmethods like CLIP. By combining advanced LiDAR sensor data with a large\npre-trained model, our approach provides a robust and accurate solution for\nchallenging detection scenarios, including real-time applications requiring\nhigh accuracy and robustness.\n", "link": "http://arxiv.org/abs/2502.04385v2", "date": "2025-02-21", "relevancy": 2.9222, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5879}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5827}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5827}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TexLiDAR%3A%20Automated%20Text%20Understanding%20for%20Panoramic%20LiDAR%20Data&body=Title%3A%20TexLiDAR%3A%20Automated%20Text%20Understanding%20for%20Panoramic%20LiDAR%20Data%0AAuthor%3A%20Naor%20Cohen%20and%20Roy%20Orfaig%20and%20Ben-Zion%20Bobrovsky%0AAbstract%3A%20%20%20Efforts%20to%20connect%20LiDAR%20data%20with%20text%2C%20such%20as%20LidarCLIP%2C%20have%20primarily%0Afocused%20on%20embedding%203D%20point%20clouds%20into%20CLIP%20text-image%20space.%20However%2C%20these%0Aapproaches%20rely%20on%203D%20point%20clouds%2C%20which%20present%20challenges%20in%20encoding%0Aefficiency%20and%20neural%20network%20processing.%20With%20the%20advent%20of%20advanced%20LiDAR%0Asensors%20like%20Ouster%20OS1%2C%20which%2C%20in%20addition%20to%203D%20point%20clouds%2C%20produce%20fixed%0Aresolution%20depth%2C%20signal%2C%20and%20ambient%20panoramic%202D%20images%2C%20new%20opportunities%0Aemerge%20for%20LiDAR%20based%20tasks.%20In%20this%20work%2C%20we%20propose%20an%20alternative%20approach%0Ato%20connect%20LiDAR%20data%20with%20text%20by%20leveraging%202D%20imagery%20generated%20by%20the%20OS1%0Asensor%20instead%20of%203D%20point%20clouds.%20Using%20the%20Florence%202%20large%20model%20in%20a%0Azero-shot%20setting%2C%20we%20perform%20image%20captioning%20and%20object%20detection.%20Our%0Aexperiments%20demonstrate%20that%20Florence%202%20generates%20more%20informative%20captions%20and%0Aachieves%20superior%20performance%20in%20object%20detection%20tasks%20compared%20to%20existing%0Amethods%20like%20CLIP.%20By%20combining%20advanced%20LiDAR%20sensor%20data%20with%20a%20large%0Apre-trained%20model%2C%20our%20approach%20provides%20a%20robust%20and%20accurate%20solution%20for%0Achallenging%20detection%20scenarios%2C%20including%20real-time%20applications%20requiring%0Ahigh%20accuracy%20and%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04385v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTexLiDAR%253A%2520Automated%2520Text%2520Understanding%2520for%2520Panoramic%2520LiDAR%2520Data%26entry.906535625%3DNaor%2520Cohen%2520and%2520Roy%2520Orfaig%2520and%2520Ben-Zion%2520Bobrovsky%26entry.1292438233%3D%2520%2520Efforts%2520to%2520connect%2520LiDAR%2520data%2520with%2520text%252C%2520such%2520as%2520LidarCLIP%252C%2520have%2520primarily%250Afocused%2520on%2520embedding%25203D%2520point%2520clouds%2520into%2520CLIP%2520text-image%2520space.%2520However%252C%2520these%250Aapproaches%2520rely%2520on%25203D%2520point%2520clouds%252C%2520which%2520present%2520challenges%2520in%2520encoding%250Aefficiency%2520and%2520neural%2520network%2520processing.%2520With%2520the%2520advent%2520of%2520advanced%2520LiDAR%250Asensors%2520like%2520Ouster%2520OS1%252C%2520which%252C%2520in%2520addition%2520to%25203D%2520point%2520clouds%252C%2520produce%2520fixed%250Aresolution%2520depth%252C%2520signal%252C%2520and%2520ambient%2520panoramic%25202D%2520images%252C%2520new%2520opportunities%250Aemerge%2520for%2520LiDAR%2520based%2520tasks.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520alternative%2520approach%250Ato%2520connect%2520LiDAR%2520data%2520with%2520text%2520by%2520leveraging%25202D%2520imagery%2520generated%2520by%2520the%2520OS1%250Asensor%2520instead%2520of%25203D%2520point%2520clouds.%2520Using%2520the%2520Florence%25202%2520large%2520model%2520in%2520a%250Azero-shot%2520setting%252C%2520we%2520perform%2520image%2520captioning%2520and%2520object%2520detection.%2520Our%250Aexperiments%2520demonstrate%2520that%2520Florence%25202%2520generates%2520more%2520informative%2520captions%2520and%250Aachieves%2520superior%2520performance%2520in%2520object%2520detection%2520tasks%2520compared%2520to%2520existing%250Amethods%2520like%2520CLIP.%2520By%2520combining%2520advanced%2520LiDAR%2520sensor%2520data%2520with%2520a%2520large%250Apre-trained%2520model%252C%2520our%2520approach%2520provides%2520a%2520robust%2520and%2520accurate%2520solution%2520for%250Achallenging%2520detection%2520scenarios%252C%2520including%2520real-time%2520applications%2520requiring%250Ahigh%2520accuracy%2520and%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04385v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TexLiDAR%3A%20Automated%20Text%20Understanding%20for%20Panoramic%20LiDAR%20Data&entry.906535625=Naor%20Cohen%20and%20Roy%20Orfaig%20and%20Ben-Zion%20Bobrovsky&entry.1292438233=%20%20Efforts%20to%20connect%20LiDAR%20data%20with%20text%2C%20such%20as%20LidarCLIP%2C%20have%20primarily%0Afocused%20on%20embedding%203D%20point%20clouds%20into%20CLIP%20text-image%20space.%20However%2C%20these%0Aapproaches%20rely%20on%203D%20point%20clouds%2C%20which%20present%20challenges%20in%20encoding%0Aefficiency%20and%20neural%20network%20processing.%20With%20the%20advent%20of%20advanced%20LiDAR%0Asensors%20like%20Ouster%20OS1%2C%20which%2C%20in%20addition%20to%203D%20point%20clouds%2C%20produce%20fixed%0Aresolution%20depth%2C%20signal%2C%20and%20ambient%20panoramic%202D%20images%2C%20new%20opportunities%0Aemerge%20for%20LiDAR%20based%20tasks.%20In%20this%20work%2C%20we%20propose%20an%20alternative%20approach%0Ato%20connect%20LiDAR%20data%20with%20text%20by%20leveraging%202D%20imagery%20generated%20by%20the%20OS1%0Asensor%20instead%20of%203D%20point%20clouds.%20Using%20the%20Florence%202%20large%20model%20in%20a%0Azero-shot%20setting%2C%20we%20perform%20image%20captioning%20and%20object%20detection.%20Our%0Aexperiments%20demonstrate%20that%20Florence%202%20generates%20more%20informative%20captions%20and%0Aachieves%20superior%20performance%20in%20object%20detection%20tasks%20compared%20to%20existing%0Amethods%20like%20CLIP.%20By%20combining%20advanced%20LiDAR%20sensor%20data%20with%20a%20large%0Apre-trained%20model%2C%20our%20approach%20provides%20a%20robust%20and%20accurate%20solution%20for%0Achallenging%20detection%20scenarios%2C%20including%20real-time%20applications%20requiring%0Ahigh%20accuracy%20and%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04385v2&entry.124074799=Read"},
{"title": "Bridging vision language model (VLM) evaluation gaps with a framework\n  for scalable and cost-effective benchmark generation", "author": "Tim R\u00e4dsch and Leon Mayer and Simon Pavicic and A. Emre Kavur and Marcel Knopp and Bar\u0131\u015f \u00d6zt\u00fcrk and Klaus Maier-Hein and Paul F. Jaeger and Fabian Isensee and Annika Reinke and Lena Maier-Hein", "abstract": "  Reliable evaluation of AI models is critical for scientific progress and\npractical application. While existing VLM benchmarks provide general insights\ninto model capabilities, their heterogeneous designs and limited focus on a few\nimaging domains pose significant challenges for both cross-domain performance\ncomparison and targeted domain-specific evaluation. To address this, we propose\nthree key contributions: (1) a framework for the resource-efficient creation of\ndomain-specific VLM benchmarks enabled by task augmentation for creating\nmultiple diverse tasks from a single existing task, (2) the release of new VLM\nbenchmarks for seven domains, created according to the same homogeneous\nprotocol and including 162,946 thoroughly human-validated answers, and (3) an\nextensive benchmarking of 22 state-of-the-art VLMs on a total of 37,171 tasks,\nrevealing performance variances across domains and tasks, thereby supporting\nthe need for tailored VLM benchmarks. Adoption of our methodology will pave the\nway for the resource-efficient domain-specific selection of models and guide\nfuture research efforts toward addressing core open questions.\n", "link": "http://arxiv.org/abs/2502.15563v1", "date": "2025-02-21", "relevancy": 2.8229, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5914}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5914}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20vision%20language%20model%20%28VLM%29%20evaluation%20gaps%20with%20a%20framework%0A%20%20for%20scalable%20and%20cost-effective%20benchmark%20generation&body=Title%3A%20Bridging%20vision%20language%20model%20%28VLM%29%20evaluation%20gaps%20with%20a%20framework%0A%20%20for%20scalable%20and%20cost-effective%20benchmark%20generation%0AAuthor%3A%20Tim%20R%C3%A4dsch%20and%20Leon%20Mayer%20and%20Simon%20Pavicic%20and%20A.%20Emre%20Kavur%20and%20Marcel%20Knopp%20and%20Bar%C4%B1%C5%9F%20%C3%96zt%C3%BCrk%20and%20Klaus%20Maier-Hein%20and%20Paul%20F.%20Jaeger%20and%20Fabian%20Isensee%20and%20Annika%20Reinke%20and%20Lena%20Maier-Hein%0AAbstract%3A%20%20%20Reliable%20evaluation%20of%20AI%20models%20is%20critical%20for%20scientific%20progress%20and%0Apractical%20application.%20While%20existing%20VLM%20benchmarks%20provide%20general%20insights%0Ainto%20model%20capabilities%2C%20their%20heterogeneous%20designs%20and%20limited%20focus%20on%20a%20few%0Aimaging%20domains%20pose%20significant%20challenges%20for%20both%20cross-domain%20performance%0Acomparison%20and%20targeted%20domain-specific%20evaluation.%20To%20address%20this%2C%20we%20propose%0Athree%20key%20contributions%3A%20%281%29%20a%20framework%20for%20the%20resource-efficient%20creation%20of%0Adomain-specific%20VLM%20benchmarks%20enabled%20by%20task%20augmentation%20for%20creating%0Amultiple%20diverse%20tasks%20from%20a%20single%20existing%20task%2C%20%282%29%20the%20release%20of%20new%20VLM%0Abenchmarks%20for%20seven%20domains%2C%20created%20according%20to%20the%20same%20homogeneous%0Aprotocol%20and%20including%20162%2C946%20thoroughly%20human-validated%20answers%2C%20and%20%283%29%20an%0Aextensive%20benchmarking%20of%2022%20state-of-the-art%20VLMs%20on%20a%20total%20of%2037%2C171%20tasks%2C%0Arevealing%20performance%20variances%20across%20domains%20and%20tasks%2C%20thereby%20supporting%0Athe%20need%20for%20tailored%20VLM%20benchmarks.%20Adoption%20of%20our%20methodology%20will%20pave%20the%0Away%20for%20the%20resource-efficient%20domain-specific%20selection%20of%20models%20and%20guide%0Afuture%20research%20efforts%20toward%20addressing%20core%20open%20questions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15563v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520vision%2520language%2520model%2520%2528VLM%2529%2520evaluation%2520gaps%2520with%2520a%2520framework%250A%2520%2520for%2520scalable%2520and%2520cost-effective%2520benchmark%2520generation%26entry.906535625%3DTim%2520R%25C3%25A4dsch%2520and%2520Leon%2520Mayer%2520and%2520Simon%2520Pavicic%2520and%2520A.%2520Emre%2520Kavur%2520and%2520Marcel%2520Knopp%2520and%2520Bar%25C4%25B1%25C5%259F%2520%25C3%2596zt%25C3%25BCrk%2520and%2520Klaus%2520Maier-Hein%2520and%2520Paul%2520F.%2520Jaeger%2520and%2520Fabian%2520Isensee%2520and%2520Annika%2520Reinke%2520and%2520Lena%2520Maier-Hein%26entry.1292438233%3D%2520%2520Reliable%2520evaluation%2520of%2520AI%2520models%2520is%2520critical%2520for%2520scientific%2520progress%2520and%250Apractical%2520application.%2520While%2520existing%2520VLM%2520benchmarks%2520provide%2520general%2520insights%250Ainto%2520model%2520capabilities%252C%2520their%2520heterogeneous%2520designs%2520and%2520limited%2520focus%2520on%2520a%2520few%250Aimaging%2520domains%2520pose%2520significant%2520challenges%2520for%2520both%2520cross-domain%2520performance%250Acomparison%2520and%2520targeted%2520domain-specific%2520evaluation.%2520To%2520address%2520this%252C%2520we%2520propose%250Athree%2520key%2520contributions%253A%2520%25281%2529%2520a%2520framework%2520for%2520the%2520resource-efficient%2520creation%2520of%250Adomain-specific%2520VLM%2520benchmarks%2520enabled%2520by%2520task%2520augmentation%2520for%2520creating%250Amultiple%2520diverse%2520tasks%2520from%2520a%2520single%2520existing%2520task%252C%2520%25282%2529%2520the%2520release%2520of%2520new%2520VLM%250Abenchmarks%2520for%2520seven%2520domains%252C%2520created%2520according%2520to%2520the%2520same%2520homogeneous%250Aprotocol%2520and%2520including%2520162%252C946%2520thoroughly%2520human-validated%2520answers%252C%2520and%2520%25283%2529%2520an%250Aextensive%2520benchmarking%2520of%252022%2520state-of-the-art%2520VLMs%2520on%2520a%2520total%2520of%252037%252C171%2520tasks%252C%250Arevealing%2520performance%2520variances%2520across%2520domains%2520and%2520tasks%252C%2520thereby%2520supporting%250Athe%2520need%2520for%2520tailored%2520VLM%2520benchmarks.%2520Adoption%2520of%2520our%2520methodology%2520will%2520pave%2520the%250Away%2520for%2520the%2520resource-efficient%2520domain-specific%2520selection%2520of%2520models%2520and%2520guide%250Afuture%2520research%2520efforts%2520toward%2520addressing%2520core%2520open%2520questions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15563v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20vision%20language%20model%20%28VLM%29%20evaluation%20gaps%20with%20a%20framework%0A%20%20for%20scalable%20and%20cost-effective%20benchmark%20generation&entry.906535625=Tim%20R%C3%A4dsch%20and%20Leon%20Mayer%20and%20Simon%20Pavicic%20and%20A.%20Emre%20Kavur%20and%20Marcel%20Knopp%20and%20Bar%C4%B1%C5%9F%20%C3%96zt%C3%BCrk%20and%20Klaus%20Maier-Hein%20and%20Paul%20F.%20Jaeger%20and%20Fabian%20Isensee%20and%20Annika%20Reinke%20and%20Lena%20Maier-Hein&entry.1292438233=%20%20Reliable%20evaluation%20of%20AI%20models%20is%20critical%20for%20scientific%20progress%20and%0Apractical%20application.%20While%20existing%20VLM%20benchmarks%20provide%20general%20insights%0Ainto%20model%20capabilities%2C%20their%20heterogeneous%20designs%20and%20limited%20focus%20on%20a%20few%0Aimaging%20domains%20pose%20significant%20challenges%20for%20both%20cross-domain%20performance%0Acomparison%20and%20targeted%20domain-specific%20evaluation.%20To%20address%20this%2C%20we%20propose%0Athree%20key%20contributions%3A%20%281%29%20a%20framework%20for%20the%20resource-efficient%20creation%20of%0Adomain-specific%20VLM%20benchmarks%20enabled%20by%20task%20augmentation%20for%20creating%0Amultiple%20diverse%20tasks%20from%20a%20single%20existing%20task%2C%20%282%29%20the%20release%20of%20new%20VLM%0Abenchmarks%20for%20seven%20domains%2C%20created%20according%20to%20the%20same%20homogeneous%0Aprotocol%20and%20including%20162%2C946%20thoroughly%20human-validated%20answers%2C%20and%20%283%29%20an%0Aextensive%20benchmarking%20of%2022%20state-of-the-art%20VLMs%20on%20a%20total%20of%2037%2C171%20tasks%2C%0Arevealing%20performance%20variances%20across%20domains%20and%20tasks%2C%20thereby%20supporting%0Athe%20need%20for%20tailored%20VLM%20benchmarks.%20Adoption%20of%20our%20methodology%20will%20pave%20the%0Away%20for%20the%20resource-efficient%20domain-specific%20selection%20of%20models%20and%20guide%0Afuture%20research%20efforts%20toward%20addressing%20core%20open%20questions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15563v1&entry.124074799=Read"},
{"title": "Enhancing Vehicle Make and Model Recognition with 3D Attention Modules", "author": "Narges Semiromizadeh and Omid Nejati Manzari and Shahriar B. Shokouhi and Sattar Mirzakuchaki", "abstract": "  Vehicle make and model recognition (VMMR) is a crucial component of the\nIntelligent Transport System, garnering significant attention in recent years.\nVMMR has been widely utilized for detecting suspicious vehicles, monitoring\nurban traffic, and autonomous driving systems. The complexity of VMMR arises\nfrom the subtle visual distinctions among vehicle models and the wide variety\nof classes produced by manufacturers. Convolutional Neural Networks (CNNs), a\nprominent type of deep learning model, have been extensively employed in\nvarious computer vision tasks, including VMMR, yielding remarkable results. As\nVMMR is a fine-grained classification problem, it primarily faces inter-class\nsimilarity and intra-class variation challenges. In this study, we implement an\nattention module to address these challenges and enhance the model's focus on\ncritical areas containing distinguishing features. This module, which does not\nincrease the parameters of the original model, generates three-dimensional\n(3-D) attention weights to refine the feature map. Our proposed model\nintegrates the attention module into two different locations within the middle\nsection of a convolutional model, where the feature maps from these sections\noffer sufficient information about the input frames without being overly\ndetailed or overly coarse. The performance of our proposed model, along with\nstate-of-the-art (SOTA) convolutional and transformer-based models, was\nevaluated using the Stanford Cars dataset. Our proposed model achieved the\nhighest accuracy, 90.69\\%, among the compared models.\n", "link": "http://arxiv.org/abs/2502.15398v1", "date": "2025-02-21", "relevancy": 2.7638, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5761}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5411}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Vehicle%20Make%20and%20Model%20Recognition%20with%203D%20Attention%20Modules&body=Title%3A%20Enhancing%20Vehicle%20Make%20and%20Model%20Recognition%20with%203D%20Attention%20Modules%0AAuthor%3A%20Narges%20Semiromizadeh%20and%20Omid%20Nejati%20Manzari%20and%20Shahriar%20B.%20Shokouhi%20and%20Sattar%20Mirzakuchaki%0AAbstract%3A%20%20%20Vehicle%20make%20and%20model%20recognition%20%28VMMR%29%20is%20a%20crucial%20component%20of%20the%0AIntelligent%20Transport%20System%2C%20garnering%20significant%20attention%20in%20recent%20years.%0AVMMR%20has%20been%20widely%20utilized%20for%20detecting%20suspicious%20vehicles%2C%20monitoring%0Aurban%20traffic%2C%20and%20autonomous%20driving%20systems.%20The%20complexity%20of%20VMMR%20arises%0Afrom%20the%20subtle%20visual%20distinctions%20among%20vehicle%20models%20and%20the%20wide%20variety%0Aof%20classes%20produced%20by%20manufacturers.%20Convolutional%20Neural%20Networks%20%28CNNs%29%2C%20a%0Aprominent%20type%20of%20deep%20learning%20model%2C%20have%20been%20extensively%20employed%20in%0Avarious%20computer%20vision%20tasks%2C%20including%20VMMR%2C%20yielding%20remarkable%20results.%20As%0AVMMR%20is%20a%20fine-grained%20classification%20problem%2C%20it%20primarily%20faces%20inter-class%0Asimilarity%20and%20intra-class%20variation%20challenges.%20In%20this%20study%2C%20we%20implement%20an%0Aattention%20module%20to%20address%20these%20challenges%20and%20enhance%20the%20model%27s%20focus%20on%0Acritical%20areas%20containing%20distinguishing%20features.%20This%20module%2C%20which%20does%20not%0Aincrease%20the%20parameters%20of%20the%20original%20model%2C%20generates%20three-dimensional%0A%283-D%29%20attention%20weights%20to%20refine%20the%20feature%20map.%20Our%20proposed%20model%0Aintegrates%20the%20attention%20module%20into%20two%20different%20locations%20within%20the%20middle%0Asection%20of%20a%20convolutional%20model%2C%20where%20the%20feature%20maps%20from%20these%20sections%0Aoffer%20sufficient%20information%20about%20the%20input%20frames%20without%20being%20overly%0Adetailed%20or%20overly%20coarse.%20The%20performance%20of%20our%20proposed%20model%2C%20along%20with%0Astate-of-the-art%20%28SOTA%29%20convolutional%20and%20transformer-based%20models%2C%20was%0Aevaluated%20using%20the%20Stanford%20Cars%20dataset.%20Our%20proposed%20model%20achieved%20the%0Ahighest%20accuracy%2C%2090.69%5C%25%2C%20among%20the%20compared%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15398v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Vehicle%2520Make%2520and%2520Model%2520Recognition%2520with%25203D%2520Attention%2520Modules%26entry.906535625%3DNarges%2520Semiromizadeh%2520and%2520Omid%2520Nejati%2520Manzari%2520and%2520Shahriar%2520B.%2520Shokouhi%2520and%2520Sattar%2520Mirzakuchaki%26entry.1292438233%3D%2520%2520Vehicle%2520make%2520and%2520model%2520recognition%2520%2528VMMR%2529%2520is%2520a%2520crucial%2520component%2520of%2520the%250AIntelligent%2520Transport%2520System%252C%2520garnering%2520significant%2520attention%2520in%2520recent%2520years.%250AVMMR%2520has%2520been%2520widely%2520utilized%2520for%2520detecting%2520suspicious%2520vehicles%252C%2520monitoring%250Aurban%2520traffic%252C%2520and%2520autonomous%2520driving%2520systems.%2520The%2520complexity%2520of%2520VMMR%2520arises%250Afrom%2520the%2520subtle%2520visual%2520distinctions%2520among%2520vehicle%2520models%2520and%2520the%2520wide%2520variety%250Aof%2520classes%2520produced%2520by%2520manufacturers.%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%252C%2520a%250Aprominent%2520type%2520of%2520deep%2520learning%2520model%252C%2520have%2520been%2520extensively%2520employed%2520in%250Avarious%2520computer%2520vision%2520tasks%252C%2520including%2520VMMR%252C%2520yielding%2520remarkable%2520results.%2520As%250AVMMR%2520is%2520a%2520fine-grained%2520classification%2520problem%252C%2520it%2520primarily%2520faces%2520inter-class%250Asimilarity%2520and%2520intra-class%2520variation%2520challenges.%2520In%2520this%2520study%252C%2520we%2520implement%2520an%250Aattention%2520module%2520to%2520address%2520these%2520challenges%2520and%2520enhance%2520the%2520model%2527s%2520focus%2520on%250Acritical%2520areas%2520containing%2520distinguishing%2520features.%2520This%2520module%252C%2520which%2520does%2520not%250Aincrease%2520the%2520parameters%2520of%2520the%2520original%2520model%252C%2520generates%2520three-dimensional%250A%25283-D%2529%2520attention%2520weights%2520to%2520refine%2520the%2520feature%2520map.%2520Our%2520proposed%2520model%250Aintegrates%2520the%2520attention%2520module%2520into%2520two%2520different%2520locations%2520within%2520the%2520middle%250Asection%2520of%2520a%2520convolutional%2520model%252C%2520where%2520the%2520feature%2520maps%2520from%2520these%2520sections%250Aoffer%2520sufficient%2520information%2520about%2520the%2520input%2520frames%2520without%2520being%2520overly%250Adetailed%2520or%2520overly%2520coarse.%2520The%2520performance%2520of%2520our%2520proposed%2520model%252C%2520along%2520with%250Astate-of-the-art%2520%2528SOTA%2529%2520convolutional%2520and%2520transformer-based%2520models%252C%2520was%250Aevaluated%2520using%2520the%2520Stanford%2520Cars%2520dataset.%2520Our%2520proposed%2520model%2520achieved%2520the%250Ahighest%2520accuracy%252C%252090.69%255C%2525%252C%2520among%2520the%2520compared%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15398v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Vehicle%20Make%20and%20Model%20Recognition%20with%203D%20Attention%20Modules&entry.906535625=Narges%20Semiromizadeh%20and%20Omid%20Nejati%20Manzari%20and%20Shahriar%20B.%20Shokouhi%20and%20Sattar%20Mirzakuchaki&entry.1292438233=%20%20Vehicle%20make%20and%20model%20recognition%20%28VMMR%29%20is%20a%20crucial%20component%20of%20the%0AIntelligent%20Transport%20System%2C%20garnering%20significant%20attention%20in%20recent%20years.%0AVMMR%20has%20been%20widely%20utilized%20for%20detecting%20suspicious%20vehicles%2C%20monitoring%0Aurban%20traffic%2C%20and%20autonomous%20driving%20systems.%20The%20complexity%20of%20VMMR%20arises%0Afrom%20the%20subtle%20visual%20distinctions%20among%20vehicle%20models%20and%20the%20wide%20variety%0Aof%20classes%20produced%20by%20manufacturers.%20Convolutional%20Neural%20Networks%20%28CNNs%29%2C%20a%0Aprominent%20type%20of%20deep%20learning%20model%2C%20have%20been%20extensively%20employed%20in%0Avarious%20computer%20vision%20tasks%2C%20including%20VMMR%2C%20yielding%20remarkable%20results.%20As%0AVMMR%20is%20a%20fine-grained%20classification%20problem%2C%20it%20primarily%20faces%20inter-class%0Asimilarity%20and%20intra-class%20variation%20challenges.%20In%20this%20study%2C%20we%20implement%20an%0Aattention%20module%20to%20address%20these%20challenges%20and%20enhance%20the%20model%27s%20focus%20on%0Acritical%20areas%20containing%20distinguishing%20features.%20This%20module%2C%20which%20does%20not%0Aincrease%20the%20parameters%20of%20the%20original%20model%2C%20generates%20three-dimensional%0A%283-D%29%20attention%20weights%20to%20refine%20the%20feature%20map.%20Our%20proposed%20model%0Aintegrates%20the%20attention%20module%20into%20two%20different%20locations%20within%20the%20middle%0Asection%20of%20a%20convolutional%20model%2C%20where%20the%20feature%20maps%20from%20these%20sections%0Aoffer%20sufficient%20information%20about%20the%20input%20frames%20without%20being%20overly%0Adetailed%20or%20overly%20coarse.%20The%20performance%20of%20our%20proposed%20model%2C%20along%20with%0Astate-of-the-art%20%28SOTA%29%20convolutional%20and%20transformer-based%20models%2C%20was%0Aevaluated%20using%20the%20Stanford%20Cars%20dataset.%20Our%20proposed%20model%20achieved%20the%0Ahighest%20accuracy%2C%2090.69%5C%25%2C%20among%20the%20compared%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15398v1&entry.124074799=Read"},
{"title": "LongCaptioning: Unlocking the Power of Long Caption Generation in Large\n  Multimodal Models", "author": "Hongchen Wei and Zhihong Tan and Yaosi Hu and Changwen Chen and Zhenzhong Chen", "abstract": "  Large multimodal models (LMMs) have shown remarkable performance in video\nunderstanding tasks and can even process videos longer than one hour. However,\ndespite their ability to handle long inputs, generating outputs with\ncorresponding levels of richness remains a challenge. In this paper, we explore\nthe issue of long outputs in LMMs using video captioning as a proxy task, and\nwe find that open-source LMMs struggle to consistently generate outputs\nexceeding about 300 words. Through controlled experiments, we find that the\nscarcity of paired examples with long-captions during training is the primary\nfactor limiting the model's output length. However, manually annotating\nlong-caption examples is time-consuming and expensive. To address this, we\npropose the LongCaption-Agent, a framework that synthesizes long caption data\nby aggregating multi-level descriptions. Using LongCaption-Agent, we curated a\nnew long-caption dataset, LongCaption-10K. We also develop LongCaption-Bench, a\nbenchmark designed to comprehensively evaluate the quality of long captions\ngenerated by LMMs. By incorporating LongCaption-10K into training, we enable\nLMMs to generate captions exceeding 1,000 words, while maintaining high output\nquality. In LongCaption-Bench, our 8B parameter model achieved state-of-the-art\nperformance, even surpassing larger proprietary models. We will release the\ndataset and code after publication.\n", "link": "http://arxiv.org/abs/2502.15393v1", "date": "2025-02-21", "relevancy": 2.7438, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5515}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5515}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LongCaptioning%3A%20Unlocking%20the%20Power%20of%20Long%20Caption%20Generation%20in%20Large%0A%20%20Multimodal%20Models&body=Title%3A%20LongCaptioning%3A%20Unlocking%20the%20Power%20of%20Long%20Caption%20Generation%20in%20Large%0A%20%20Multimodal%20Models%0AAuthor%3A%20Hongchen%20Wei%20and%20Zhihong%20Tan%20and%20Yaosi%20Hu%20and%20Changwen%20Chen%20and%20Zhenzhong%20Chen%0AAbstract%3A%20%20%20Large%20multimodal%20models%20%28LMMs%29%20have%20shown%20remarkable%20performance%20in%20video%0Aunderstanding%20tasks%20and%20can%20even%20process%20videos%20longer%20than%20one%20hour.%20However%2C%0Adespite%20their%20ability%20to%20handle%20long%20inputs%2C%20generating%20outputs%20with%0Acorresponding%20levels%20of%20richness%20remains%20a%20challenge.%20In%20this%20paper%2C%20we%20explore%0Athe%20issue%20of%20long%20outputs%20in%20LMMs%20using%20video%20captioning%20as%20a%20proxy%20task%2C%20and%0Awe%20find%20that%20open-source%20LMMs%20struggle%20to%20consistently%20generate%20outputs%0Aexceeding%20about%20300%20words.%20Through%20controlled%20experiments%2C%20we%20find%20that%20the%0Ascarcity%20of%20paired%20examples%20with%20long-captions%20during%20training%20is%20the%20primary%0Afactor%20limiting%20the%20model%27s%20output%20length.%20However%2C%20manually%20annotating%0Along-caption%20examples%20is%20time-consuming%20and%20expensive.%20To%20address%20this%2C%20we%0Apropose%20the%20LongCaption-Agent%2C%20a%20framework%20that%20synthesizes%20long%20caption%20data%0Aby%20aggregating%20multi-level%20descriptions.%20Using%20LongCaption-Agent%2C%20we%20curated%20a%0Anew%20long-caption%20dataset%2C%20LongCaption-10K.%20We%20also%20develop%20LongCaption-Bench%2C%20a%0Abenchmark%20designed%20to%20comprehensively%20evaluate%20the%20quality%20of%20long%20captions%0Agenerated%20by%20LMMs.%20By%20incorporating%20LongCaption-10K%20into%20training%2C%20we%20enable%0ALMMs%20to%20generate%20captions%20exceeding%201%2C000%20words%2C%20while%20maintaining%20high%20output%0Aquality.%20In%20LongCaption-Bench%2C%20our%208B%20parameter%20model%20achieved%20state-of-the-art%0Aperformance%2C%20even%20surpassing%20larger%20proprietary%20models.%20We%20will%20release%20the%0Adataset%20and%20code%20after%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15393v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongCaptioning%253A%2520Unlocking%2520the%2520Power%2520of%2520Long%2520Caption%2520Generation%2520in%2520Large%250A%2520%2520Multimodal%2520Models%26entry.906535625%3DHongchen%2520Wei%2520and%2520Zhihong%2520Tan%2520and%2520Yaosi%2520Hu%2520and%2520Changwen%2520Chen%2520and%2520Zhenzhong%2520Chen%26entry.1292438233%3D%2520%2520Large%2520multimodal%2520models%2520%2528LMMs%2529%2520have%2520shown%2520remarkable%2520performance%2520in%2520video%250Aunderstanding%2520tasks%2520and%2520can%2520even%2520process%2520videos%2520longer%2520than%2520one%2520hour.%2520However%252C%250Adespite%2520their%2520ability%2520to%2520handle%2520long%2520inputs%252C%2520generating%2520outputs%2520with%250Acorresponding%2520levels%2520of%2520richness%2520remains%2520a%2520challenge.%2520In%2520this%2520paper%252C%2520we%2520explore%250Athe%2520issue%2520of%2520long%2520outputs%2520in%2520LMMs%2520using%2520video%2520captioning%2520as%2520a%2520proxy%2520task%252C%2520and%250Awe%2520find%2520that%2520open-source%2520LMMs%2520struggle%2520to%2520consistently%2520generate%2520outputs%250Aexceeding%2520about%2520300%2520words.%2520Through%2520controlled%2520experiments%252C%2520we%2520find%2520that%2520the%250Ascarcity%2520of%2520paired%2520examples%2520with%2520long-captions%2520during%2520training%2520is%2520the%2520primary%250Afactor%2520limiting%2520the%2520model%2527s%2520output%2520length.%2520However%252C%2520manually%2520annotating%250Along-caption%2520examples%2520is%2520time-consuming%2520and%2520expensive.%2520To%2520address%2520this%252C%2520we%250Apropose%2520the%2520LongCaption-Agent%252C%2520a%2520framework%2520that%2520synthesizes%2520long%2520caption%2520data%250Aby%2520aggregating%2520multi-level%2520descriptions.%2520Using%2520LongCaption-Agent%252C%2520we%2520curated%2520a%250Anew%2520long-caption%2520dataset%252C%2520LongCaption-10K.%2520We%2520also%2520develop%2520LongCaption-Bench%252C%2520a%250Abenchmark%2520designed%2520to%2520comprehensively%2520evaluate%2520the%2520quality%2520of%2520long%2520captions%250Agenerated%2520by%2520LMMs.%2520By%2520incorporating%2520LongCaption-10K%2520into%2520training%252C%2520we%2520enable%250ALMMs%2520to%2520generate%2520captions%2520exceeding%25201%252C000%2520words%252C%2520while%2520maintaining%2520high%2520output%250Aquality.%2520In%2520LongCaption-Bench%252C%2520our%25208B%2520parameter%2520model%2520achieved%2520state-of-the-art%250Aperformance%252C%2520even%2520surpassing%2520larger%2520proprietary%2520models.%2520We%2520will%2520release%2520the%250Adataset%2520and%2520code%2520after%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15393v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongCaptioning%3A%20Unlocking%20the%20Power%20of%20Long%20Caption%20Generation%20in%20Large%0A%20%20Multimodal%20Models&entry.906535625=Hongchen%20Wei%20and%20Zhihong%20Tan%20and%20Yaosi%20Hu%20and%20Changwen%20Chen%20and%20Zhenzhong%20Chen&entry.1292438233=%20%20Large%20multimodal%20models%20%28LMMs%29%20have%20shown%20remarkable%20performance%20in%20video%0Aunderstanding%20tasks%20and%20can%20even%20process%20videos%20longer%20than%20one%20hour.%20However%2C%0Adespite%20their%20ability%20to%20handle%20long%20inputs%2C%20generating%20outputs%20with%0Acorresponding%20levels%20of%20richness%20remains%20a%20challenge.%20In%20this%20paper%2C%20we%20explore%0Athe%20issue%20of%20long%20outputs%20in%20LMMs%20using%20video%20captioning%20as%20a%20proxy%20task%2C%20and%0Awe%20find%20that%20open-source%20LMMs%20struggle%20to%20consistently%20generate%20outputs%0Aexceeding%20about%20300%20words.%20Through%20controlled%20experiments%2C%20we%20find%20that%20the%0Ascarcity%20of%20paired%20examples%20with%20long-captions%20during%20training%20is%20the%20primary%0Afactor%20limiting%20the%20model%27s%20output%20length.%20However%2C%20manually%20annotating%0Along-caption%20examples%20is%20time-consuming%20and%20expensive.%20To%20address%20this%2C%20we%0Apropose%20the%20LongCaption-Agent%2C%20a%20framework%20that%20synthesizes%20long%20caption%20data%0Aby%20aggregating%20multi-level%20descriptions.%20Using%20LongCaption-Agent%2C%20we%20curated%20a%0Anew%20long-caption%20dataset%2C%20LongCaption-10K.%20We%20also%20develop%20LongCaption-Bench%2C%20a%0Abenchmark%20designed%20to%20comprehensively%20evaluate%20the%20quality%20of%20long%20captions%0Agenerated%20by%20LMMs.%20By%20incorporating%20LongCaption-10K%20into%20training%2C%20we%20enable%0ALMMs%20to%20generate%20captions%20exceeding%201%2C000%20words%2C%20while%20maintaining%20high%20output%0Aquality.%20In%20LongCaption-Bench%2C%20our%208B%20parameter%20model%20achieved%20state-of-the-art%0Aperformance%2C%20even%20surpassing%20larger%20proprietary%20models.%20We%20will%20release%20the%0Adataset%20and%20code%20after%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15393v1&entry.124074799=Read"},
{"title": "Tailored Design of Audio-Visual Speech Recognition Models using\n  Branchformers", "author": "David Gimeno-G\u00f3mez and Carlos-D. Mart\u00ednez-Hinarejos", "abstract": "  Recent advances in Audio-Visual Speech Recognition (AVSR) have led to\nunprecedented achievements in the field, improving the robustness of this type\nof system in adverse, noisy environments. In most cases, this task has been\naddressed through the design of models composed of two independent encoders,\neach dedicated to a specific modality. However, while recent works have\nexplored unified audio-visual encoders, determining the optimal cross-modal\narchitecture remains an ongoing challenge. Furthermore, such approaches often\nrely on models comprising vast amounts of parameters and high computational\ncost training processes. In this paper, we aim to bridge this research gap by\nintroducing a novel audio-visual framework. Our proposed method constitutes, to\nthe best of our knowledge, the first attempt to harness the flexibility and\ninterpretability offered by encoder architectures, such as the Branchformer, in\nthe design of parameter-efficient AVSR systems. To be more precise, the\nproposed framework consists of two steps: first, estimating audio- and\nvideo-only systems, and then designing a tailored audio-visual unified encoder\nbased on the layer-level branch scores provided by the modality-specific\nmodels. Extensive experiments on English and Spanish AVSR benchmarks covering\nmultiple data conditions and scenarios demonstrated the effectiveness of our\nproposed method. Even when trained on a moderate scale of data, our models\nachieve competitive word error rates (WER) of approximately 2.5\\% for English\nand surpass existing approaches for Spanish, establishing a new benchmark with\nan average WER of around 9.1\\%. These results reflect how our tailored AVSR\nsystem is able to reach state-of-the-art recognition rates while significantly\nreducing the model complexity w.r.t. the prevalent approach in the field. Code\nand pre-trained models are available at\nhttps://github.com/david-gimeno/tailored-avsr.\n", "link": "http://arxiv.org/abs/2407.06606v2", "date": "2025-02-21", "relevancy": 2.7188, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5501}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5501}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tailored%20Design%20of%20Audio-Visual%20Speech%20Recognition%20Models%20using%0A%20%20Branchformers&body=Title%3A%20Tailored%20Design%20of%20Audio-Visual%20Speech%20Recognition%20Models%20using%0A%20%20Branchformers%0AAuthor%3A%20David%20Gimeno-G%C3%B3mez%20and%20Carlos-D.%20Mart%C3%ADnez-Hinarejos%0AAbstract%3A%20%20%20Recent%20advances%20in%20Audio-Visual%20Speech%20Recognition%20%28AVSR%29%20have%20led%20to%0Aunprecedented%20achievements%20in%20the%20field%2C%20improving%20the%20robustness%20of%20this%20type%0Aof%20system%20in%20adverse%2C%20noisy%20environments.%20In%20most%20cases%2C%20this%20task%20has%20been%0Aaddressed%20through%20the%20design%20of%20models%20composed%20of%20two%20independent%20encoders%2C%0Aeach%20dedicated%20to%20a%20specific%20modality.%20However%2C%20while%20recent%20works%20have%0Aexplored%20unified%20audio-visual%20encoders%2C%20determining%20the%20optimal%20cross-modal%0Aarchitecture%20remains%20an%20ongoing%20challenge.%20Furthermore%2C%20such%20approaches%20often%0Arely%20on%20models%20comprising%20vast%20amounts%20of%20parameters%20and%20high%20computational%0Acost%20training%20processes.%20In%20this%20paper%2C%20we%20aim%20to%20bridge%20this%20research%20gap%20by%0Aintroducing%20a%20novel%20audio-visual%20framework.%20Our%20proposed%20method%20constitutes%2C%20to%0Athe%20best%20of%20our%20knowledge%2C%20the%20first%20attempt%20to%20harness%20the%20flexibility%20and%0Ainterpretability%20offered%20by%20encoder%20architectures%2C%20such%20as%20the%20Branchformer%2C%20in%0Athe%20design%20of%20parameter-efficient%20AVSR%20systems.%20To%20be%20more%20precise%2C%20the%0Aproposed%20framework%20consists%20of%20two%20steps%3A%20first%2C%20estimating%20audio-%20and%0Avideo-only%20systems%2C%20and%20then%20designing%20a%20tailored%20audio-visual%20unified%20encoder%0Abased%20on%20the%20layer-level%20branch%20scores%20provided%20by%20the%20modality-specific%0Amodels.%20Extensive%20experiments%20on%20English%20and%20Spanish%20AVSR%20benchmarks%20covering%0Amultiple%20data%20conditions%20and%20scenarios%20demonstrated%20the%20effectiveness%20of%20our%0Aproposed%20method.%20Even%20when%20trained%20on%20a%20moderate%20scale%20of%20data%2C%20our%20models%0Aachieve%20competitive%20word%20error%20rates%20%28WER%29%20of%20approximately%202.5%5C%25%20for%20English%0Aand%20surpass%20existing%20approaches%20for%20Spanish%2C%20establishing%20a%20new%20benchmark%20with%0Aan%20average%20WER%20of%20around%209.1%5C%25.%20These%20results%20reflect%20how%20our%20tailored%20AVSR%0Asystem%20is%20able%20to%20reach%20state-of-the-art%20recognition%20rates%20while%20significantly%0Areducing%20the%20model%20complexity%20w.r.t.%20the%20prevalent%20approach%20in%20the%20field.%20Code%0Aand%20pre-trained%20models%20are%20available%20at%0Ahttps%3A//github.com/david-gimeno/tailored-avsr.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06606v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTailored%2520Design%2520of%2520Audio-Visual%2520Speech%2520Recognition%2520Models%2520using%250A%2520%2520Branchformers%26entry.906535625%3DDavid%2520Gimeno-G%25C3%25B3mez%2520and%2520Carlos-D.%2520Mart%25C3%25ADnez-Hinarejos%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Audio-Visual%2520Speech%2520Recognition%2520%2528AVSR%2529%2520have%2520led%2520to%250Aunprecedented%2520achievements%2520in%2520the%2520field%252C%2520improving%2520the%2520robustness%2520of%2520this%2520type%250Aof%2520system%2520in%2520adverse%252C%2520noisy%2520environments.%2520In%2520most%2520cases%252C%2520this%2520task%2520has%2520been%250Aaddressed%2520through%2520the%2520design%2520of%2520models%2520composed%2520of%2520two%2520independent%2520encoders%252C%250Aeach%2520dedicated%2520to%2520a%2520specific%2520modality.%2520However%252C%2520while%2520recent%2520works%2520have%250Aexplored%2520unified%2520audio-visual%2520encoders%252C%2520determining%2520the%2520optimal%2520cross-modal%250Aarchitecture%2520remains%2520an%2520ongoing%2520challenge.%2520Furthermore%252C%2520such%2520approaches%2520often%250Arely%2520on%2520models%2520comprising%2520vast%2520amounts%2520of%2520parameters%2520and%2520high%2520computational%250Acost%2520training%2520processes.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520bridge%2520this%2520research%2520gap%2520by%250Aintroducing%2520a%2520novel%2520audio-visual%2520framework.%2520Our%2520proposed%2520method%2520constitutes%252C%2520to%250Athe%2520best%2520of%2520our%2520knowledge%252C%2520the%2520first%2520attempt%2520to%2520harness%2520the%2520flexibility%2520and%250Ainterpretability%2520offered%2520by%2520encoder%2520architectures%252C%2520such%2520as%2520the%2520Branchformer%252C%2520in%250Athe%2520design%2520of%2520parameter-efficient%2520AVSR%2520systems.%2520To%2520be%2520more%2520precise%252C%2520the%250Aproposed%2520framework%2520consists%2520of%2520two%2520steps%253A%2520first%252C%2520estimating%2520audio-%2520and%250Avideo-only%2520systems%252C%2520and%2520then%2520designing%2520a%2520tailored%2520audio-visual%2520unified%2520encoder%250Abased%2520on%2520the%2520layer-level%2520branch%2520scores%2520provided%2520by%2520the%2520modality-specific%250Amodels.%2520Extensive%2520experiments%2520on%2520English%2520and%2520Spanish%2520AVSR%2520benchmarks%2520covering%250Amultiple%2520data%2520conditions%2520and%2520scenarios%2520demonstrated%2520the%2520effectiveness%2520of%2520our%250Aproposed%2520method.%2520Even%2520when%2520trained%2520on%2520a%2520moderate%2520scale%2520of%2520data%252C%2520our%2520models%250Aachieve%2520competitive%2520word%2520error%2520rates%2520%2528WER%2529%2520of%2520approximately%25202.5%255C%2525%2520for%2520English%250Aand%2520surpass%2520existing%2520approaches%2520for%2520Spanish%252C%2520establishing%2520a%2520new%2520benchmark%2520with%250Aan%2520average%2520WER%2520of%2520around%25209.1%255C%2525.%2520These%2520results%2520reflect%2520how%2520our%2520tailored%2520AVSR%250Asystem%2520is%2520able%2520to%2520reach%2520state-of-the-art%2520recognition%2520rates%2520while%2520significantly%250Areducing%2520the%2520model%2520complexity%2520w.r.t.%2520the%2520prevalent%2520approach%2520in%2520the%2520field.%2520Code%250Aand%2520pre-trained%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/david-gimeno/tailored-avsr.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06606v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tailored%20Design%20of%20Audio-Visual%20Speech%20Recognition%20Models%20using%0A%20%20Branchformers&entry.906535625=David%20Gimeno-G%C3%B3mez%20and%20Carlos-D.%20Mart%C3%ADnez-Hinarejos&entry.1292438233=%20%20Recent%20advances%20in%20Audio-Visual%20Speech%20Recognition%20%28AVSR%29%20have%20led%20to%0Aunprecedented%20achievements%20in%20the%20field%2C%20improving%20the%20robustness%20of%20this%20type%0Aof%20system%20in%20adverse%2C%20noisy%20environments.%20In%20most%20cases%2C%20this%20task%20has%20been%0Aaddressed%20through%20the%20design%20of%20models%20composed%20of%20two%20independent%20encoders%2C%0Aeach%20dedicated%20to%20a%20specific%20modality.%20However%2C%20while%20recent%20works%20have%0Aexplored%20unified%20audio-visual%20encoders%2C%20determining%20the%20optimal%20cross-modal%0Aarchitecture%20remains%20an%20ongoing%20challenge.%20Furthermore%2C%20such%20approaches%20often%0Arely%20on%20models%20comprising%20vast%20amounts%20of%20parameters%20and%20high%20computational%0Acost%20training%20processes.%20In%20this%20paper%2C%20we%20aim%20to%20bridge%20this%20research%20gap%20by%0Aintroducing%20a%20novel%20audio-visual%20framework.%20Our%20proposed%20method%20constitutes%2C%20to%0Athe%20best%20of%20our%20knowledge%2C%20the%20first%20attempt%20to%20harness%20the%20flexibility%20and%0Ainterpretability%20offered%20by%20encoder%20architectures%2C%20such%20as%20the%20Branchformer%2C%20in%0Athe%20design%20of%20parameter-efficient%20AVSR%20systems.%20To%20be%20more%20precise%2C%20the%0Aproposed%20framework%20consists%20of%20two%20steps%3A%20first%2C%20estimating%20audio-%20and%0Avideo-only%20systems%2C%20and%20then%20designing%20a%20tailored%20audio-visual%20unified%20encoder%0Abased%20on%20the%20layer-level%20branch%20scores%20provided%20by%20the%20modality-specific%0Amodels.%20Extensive%20experiments%20on%20English%20and%20Spanish%20AVSR%20benchmarks%20covering%0Amultiple%20data%20conditions%20and%20scenarios%20demonstrated%20the%20effectiveness%20of%20our%0Aproposed%20method.%20Even%20when%20trained%20on%20a%20moderate%20scale%20of%20data%2C%20our%20models%0Aachieve%20competitive%20word%20error%20rates%20%28WER%29%20of%20approximately%202.5%5C%25%20for%20English%0Aand%20surpass%20existing%20approaches%20for%20Spanish%2C%20establishing%20a%20new%20benchmark%20with%0Aan%20average%20WER%20of%20around%209.1%5C%25.%20These%20results%20reflect%20how%20our%20tailored%20AVSR%0Asystem%20is%20able%20to%20reach%20state-of-the-art%20recognition%20rates%20while%20significantly%0Areducing%20the%20model%20complexity%20w.r.t.%20the%20prevalent%20approach%20in%20the%20field.%20Code%0Aand%20pre-trained%20models%20are%20available%20at%0Ahttps%3A//github.com/david-gimeno/tailored-avsr.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06606v2&entry.124074799=Read"},
{"title": "HealthGPT: A Medical Large Vision-Language Model for Unifying\n  Comprehension and Generation via Heterogeneous Knowledge Adaptation", "author": "Tianwei Lin and Wenqiao Zhang and Sijing Li and Yuqian Yuan and Binhe Yu and Haoyuan Li and Wanggui He and Hao Jiang and Mengze Li and Xiaohui Song and Siliang Tang and Jun Xiao and Hui Lin and Yueting Zhuang and Beng Chin Ooi", "abstract": "  We present HealthGPT, a powerful Medical Large Vision-Language Model\n(Med-LVLM) that integrates medical visual comprehension and generation\ncapabilities within a unified autoregressive paradigm. Our bootstrapping\nphilosophy is to progressively adapt heterogeneous comprehension and generation\nknowledge to pre-trained large language models (LLMs). This is achieved through\na novel heterogeneous low-rank adaptation (H-LoRA) technique, which is\ncomplemented by a tailored hierarchical visual perception approach and a\nthree-stage learning strategy. To effectively learn the HealthGPT, we devise a\ncomprehensive medical domain-specific comprehension and generation dataset\ncalled VL-Health. Experimental results demonstrate exceptional performance and\nscalability of HealthGPT in medical visual unified tasks. Our project can be\naccessed at https://github.com/DCDmllm/HealthGPT.\n", "link": "http://arxiv.org/abs/2502.09838v3", "date": "2025-02-21", "relevancy": 2.6892, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5429}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5429}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HealthGPT%3A%20A%20Medical%20Large%20Vision-Language%20Model%20for%20Unifying%0A%20%20Comprehension%20and%20Generation%20via%20Heterogeneous%20Knowledge%20Adaptation&body=Title%3A%20HealthGPT%3A%20A%20Medical%20Large%20Vision-Language%20Model%20for%20Unifying%0A%20%20Comprehension%20and%20Generation%20via%20Heterogeneous%20Knowledge%20Adaptation%0AAuthor%3A%20Tianwei%20Lin%20and%20Wenqiao%20Zhang%20and%20Sijing%20Li%20and%20Yuqian%20Yuan%20and%20Binhe%20Yu%20and%20Haoyuan%20Li%20and%20Wanggui%20He%20and%20Hao%20Jiang%20and%20Mengze%20Li%20and%20Xiaohui%20Song%20and%20Siliang%20Tang%20and%20Jun%20Xiao%20and%20Hui%20Lin%20and%20Yueting%20Zhuang%20and%20Beng%20Chin%20Ooi%0AAbstract%3A%20%20%20We%20present%20HealthGPT%2C%20a%20powerful%20Medical%20Large%20Vision-Language%20Model%0A%28Med-LVLM%29%20that%20integrates%20medical%20visual%20comprehension%20and%20generation%0Acapabilities%20within%20a%20unified%20autoregressive%20paradigm.%20Our%20bootstrapping%0Aphilosophy%20is%20to%20progressively%20adapt%20heterogeneous%20comprehension%20and%20generation%0Aknowledge%20to%20pre-trained%20large%20language%20models%20%28LLMs%29.%20This%20is%20achieved%20through%0Aa%20novel%20heterogeneous%20low-rank%20adaptation%20%28H-LoRA%29%20technique%2C%20which%20is%0Acomplemented%20by%20a%20tailored%20hierarchical%20visual%20perception%20approach%20and%20a%0Athree-stage%20learning%20strategy.%20To%20effectively%20learn%20the%20HealthGPT%2C%20we%20devise%20a%0Acomprehensive%20medical%20domain-specific%20comprehension%20and%20generation%20dataset%0Acalled%20VL-Health.%20Experimental%20results%20demonstrate%20exceptional%20performance%20and%0Ascalability%20of%20HealthGPT%20in%20medical%20visual%20unified%20tasks.%20Our%20project%20can%20be%0Aaccessed%20at%20https%3A//github.com/DCDmllm/HealthGPT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09838v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHealthGPT%253A%2520A%2520Medical%2520Large%2520Vision-Language%2520Model%2520for%2520Unifying%250A%2520%2520Comprehension%2520and%2520Generation%2520via%2520Heterogeneous%2520Knowledge%2520Adaptation%26entry.906535625%3DTianwei%2520Lin%2520and%2520Wenqiao%2520Zhang%2520and%2520Sijing%2520Li%2520and%2520Yuqian%2520Yuan%2520and%2520Binhe%2520Yu%2520and%2520Haoyuan%2520Li%2520and%2520Wanggui%2520He%2520and%2520Hao%2520Jiang%2520and%2520Mengze%2520Li%2520and%2520Xiaohui%2520Song%2520and%2520Siliang%2520Tang%2520and%2520Jun%2520Xiao%2520and%2520Hui%2520Lin%2520and%2520Yueting%2520Zhuang%2520and%2520Beng%2520Chin%2520Ooi%26entry.1292438233%3D%2520%2520We%2520present%2520HealthGPT%252C%2520a%2520powerful%2520Medical%2520Large%2520Vision-Language%2520Model%250A%2528Med-LVLM%2529%2520that%2520integrates%2520medical%2520visual%2520comprehension%2520and%2520generation%250Acapabilities%2520within%2520a%2520unified%2520autoregressive%2520paradigm.%2520Our%2520bootstrapping%250Aphilosophy%2520is%2520to%2520progressively%2520adapt%2520heterogeneous%2520comprehension%2520and%2520generation%250Aknowledge%2520to%2520pre-trained%2520large%2520language%2520models%2520%2528LLMs%2529.%2520This%2520is%2520achieved%2520through%250Aa%2520novel%2520heterogeneous%2520low-rank%2520adaptation%2520%2528H-LoRA%2529%2520technique%252C%2520which%2520is%250Acomplemented%2520by%2520a%2520tailored%2520hierarchical%2520visual%2520perception%2520approach%2520and%2520a%250Athree-stage%2520learning%2520strategy.%2520To%2520effectively%2520learn%2520the%2520HealthGPT%252C%2520we%2520devise%2520a%250Acomprehensive%2520medical%2520domain-specific%2520comprehension%2520and%2520generation%2520dataset%250Acalled%2520VL-Health.%2520Experimental%2520results%2520demonstrate%2520exceptional%2520performance%2520and%250Ascalability%2520of%2520HealthGPT%2520in%2520medical%2520visual%2520unified%2520tasks.%2520Our%2520project%2520can%2520be%250Aaccessed%2520at%2520https%253A//github.com/DCDmllm/HealthGPT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09838v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HealthGPT%3A%20A%20Medical%20Large%20Vision-Language%20Model%20for%20Unifying%0A%20%20Comprehension%20and%20Generation%20via%20Heterogeneous%20Knowledge%20Adaptation&entry.906535625=Tianwei%20Lin%20and%20Wenqiao%20Zhang%20and%20Sijing%20Li%20and%20Yuqian%20Yuan%20and%20Binhe%20Yu%20and%20Haoyuan%20Li%20and%20Wanggui%20He%20and%20Hao%20Jiang%20and%20Mengze%20Li%20and%20Xiaohui%20Song%20and%20Siliang%20Tang%20and%20Jun%20Xiao%20and%20Hui%20Lin%20and%20Yueting%20Zhuang%20and%20Beng%20Chin%20Ooi&entry.1292438233=%20%20We%20present%20HealthGPT%2C%20a%20powerful%20Medical%20Large%20Vision-Language%20Model%0A%28Med-LVLM%29%20that%20integrates%20medical%20visual%20comprehension%20and%20generation%0Acapabilities%20within%20a%20unified%20autoregressive%20paradigm.%20Our%20bootstrapping%0Aphilosophy%20is%20to%20progressively%20adapt%20heterogeneous%20comprehension%20and%20generation%0Aknowledge%20to%20pre-trained%20large%20language%20models%20%28LLMs%29.%20This%20is%20achieved%20through%0Aa%20novel%20heterogeneous%20low-rank%20adaptation%20%28H-LoRA%29%20technique%2C%20which%20is%0Acomplemented%20by%20a%20tailored%20hierarchical%20visual%20perception%20approach%20and%20a%0Athree-stage%20learning%20strategy.%20To%20effectively%20learn%20the%20HealthGPT%2C%20we%20devise%20a%0Acomprehensive%20medical%20domain-specific%20comprehension%20and%20generation%20dataset%0Acalled%20VL-Health.%20Experimental%20results%20demonstrate%20exceptional%20performance%20and%0Ascalability%20of%20HealthGPT%20in%20medical%20visual%20unified%20tasks.%20Our%20project%20can%20be%0Aaccessed%20at%20https%3A//github.com/DCDmllm/HealthGPT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09838v3&entry.124074799=Read"},
{"title": "LightThinker: Thinking Step-by-Step Compression", "author": "Jintian Zhang and Yuqi Zhu and Mengshu Sun and Yujie Luo and Shuofei Qiao and Lun Du and Da Zheng and Huajun Chen and Ningyu Zhang", "abstract": "  Large language models (LLMs) have shown remarkable performance in complex\nreasoning tasks, but their efficiency is hindered by the substantial memory and\ncomputational costs associated with generating lengthy tokens. In this paper,\nwe propose LightThinker, a novel method that enables LLMs to dynamically\ncompress intermediate thoughts during reasoning. Inspired by human cognitive\nprocesses, LightThinker compresses verbose thought steps into compact\nrepresentations and discards the original reasoning chains, thereby\nsignificantly reducing the number of tokens stored in the context window. This\nis achieved by training the model on when and how to perform compression\nthrough data construction, mapping hidden states to condensed gist tokens, and\ncreating specialized attention masks. Additionally, we introduce the Dependency\n(Dep) metric to quantify the degree of compression by measuring the reliance on\nhistorical tokens during generation. Extensive experiments on four datasets and\ntwo models show that LightThinker reduces peak memory usage and inference time,\nwhile maintaining competitive accuracy. Our work provides a new direction for\nimproving the efficiency of LLMs in complex reasoning tasks without sacrificing\nperformance. Code will be released at https://github.com/zjunlp/LightThinker.\n", "link": "http://arxiv.org/abs/2502.15589v1", "date": "2025-02-21", "relevancy": 2.6609, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5359}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5359}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LightThinker%3A%20Thinking%20Step-by-Step%20Compression&body=Title%3A%20LightThinker%3A%20Thinking%20Step-by-Step%20Compression%0AAuthor%3A%20Jintian%20Zhang%20and%20Yuqi%20Zhu%20and%20Mengshu%20Sun%20and%20Yujie%20Luo%20and%20Shuofei%20Qiao%20and%20Lun%20Du%20and%20Da%20Zheng%20and%20Huajun%20Chen%20and%20Ningyu%20Zhang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20remarkable%20performance%20in%20complex%0Areasoning%20tasks%2C%20but%20their%20efficiency%20is%20hindered%20by%20the%20substantial%20memory%20and%0Acomputational%20costs%20associated%20with%20generating%20lengthy%20tokens.%20In%20this%20paper%2C%0Awe%20propose%20LightThinker%2C%20a%20novel%20method%20that%20enables%20LLMs%20to%20dynamically%0Acompress%20intermediate%20thoughts%20during%20reasoning.%20Inspired%20by%20human%20cognitive%0Aprocesses%2C%20LightThinker%20compresses%20verbose%20thought%20steps%20into%20compact%0Arepresentations%20and%20discards%20the%20original%20reasoning%20chains%2C%20thereby%0Asignificantly%20reducing%20the%20number%20of%20tokens%20stored%20in%20the%20context%20window.%20This%0Ais%20achieved%20by%20training%20the%20model%20on%20when%20and%20how%20to%20perform%20compression%0Athrough%20data%20construction%2C%20mapping%20hidden%20states%20to%20condensed%20gist%20tokens%2C%20and%0Acreating%20specialized%20attention%20masks.%20Additionally%2C%20we%20introduce%20the%20Dependency%0A%28Dep%29%20metric%20to%20quantify%20the%20degree%20of%20compression%20by%20measuring%20the%20reliance%20on%0Ahistorical%20tokens%20during%20generation.%20Extensive%20experiments%20on%20four%20datasets%20and%0Atwo%20models%20show%20that%20LightThinker%20reduces%20peak%20memory%20usage%20and%20inference%20time%2C%0Awhile%20maintaining%20competitive%20accuracy.%20Our%20work%20provides%20a%20new%20direction%20for%0Aimproving%20the%20efficiency%20of%20LLMs%20in%20complex%20reasoning%20tasks%20without%20sacrificing%0Aperformance.%20Code%20will%20be%20released%20at%20https%3A//github.com/zjunlp/LightThinker.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15589v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightThinker%253A%2520Thinking%2520Step-by-Step%2520Compression%26entry.906535625%3DJintian%2520Zhang%2520and%2520Yuqi%2520Zhu%2520and%2520Mengshu%2520Sun%2520and%2520Yujie%2520Luo%2520and%2520Shuofei%2520Qiao%2520and%2520Lun%2520Du%2520and%2520Da%2520Zheng%2520and%2520Huajun%2520Chen%2520and%2520Ningyu%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520remarkable%2520performance%2520in%2520complex%250Areasoning%2520tasks%252C%2520but%2520their%2520efficiency%2520is%2520hindered%2520by%2520the%2520substantial%2520memory%2520and%250Acomputational%2520costs%2520associated%2520with%2520generating%2520lengthy%2520tokens.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520LightThinker%252C%2520a%2520novel%2520method%2520that%2520enables%2520LLMs%2520to%2520dynamically%250Acompress%2520intermediate%2520thoughts%2520during%2520reasoning.%2520Inspired%2520by%2520human%2520cognitive%250Aprocesses%252C%2520LightThinker%2520compresses%2520verbose%2520thought%2520steps%2520into%2520compact%250Arepresentations%2520and%2520discards%2520the%2520original%2520reasoning%2520chains%252C%2520thereby%250Asignificantly%2520reducing%2520the%2520number%2520of%2520tokens%2520stored%2520in%2520the%2520context%2520window.%2520This%250Ais%2520achieved%2520by%2520training%2520the%2520model%2520on%2520when%2520and%2520how%2520to%2520perform%2520compression%250Athrough%2520data%2520construction%252C%2520mapping%2520hidden%2520states%2520to%2520condensed%2520gist%2520tokens%252C%2520and%250Acreating%2520specialized%2520attention%2520masks.%2520Additionally%252C%2520we%2520introduce%2520the%2520Dependency%250A%2528Dep%2529%2520metric%2520to%2520quantify%2520the%2520degree%2520of%2520compression%2520by%2520measuring%2520the%2520reliance%2520on%250Ahistorical%2520tokens%2520during%2520generation.%2520Extensive%2520experiments%2520on%2520four%2520datasets%2520and%250Atwo%2520models%2520show%2520that%2520LightThinker%2520reduces%2520peak%2520memory%2520usage%2520and%2520inference%2520time%252C%250Awhile%2520maintaining%2520competitive%2520accuracy.%2520Our%2520work%2520provides%2520a%2520new%2520direction%2520for%250Aimproving%2520the%2520efficiency%2520of%2520LLMs%2520in%2520complex%2520reasoning%2520tasks%2520without%2520sacrificing%250Aperformance.%2520Code%2520will%2520be%2520released%2520at%2520https%253A//github.com/zjunlp/LightThinker.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15589v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LightThinker%3A%20Thinking%20Step-by-Step%20Compression&entry.906535625=Jintian%20Zhang%20and%20Yuqi%20Zhu%20and%20Mengshu%20Sun%20and%20Yujie%20Luo%20and%20Shuofei%20Qiao%20and%20Lun%20Du%20and%20Da%20Zheng%20and%20Huajun%20Chen%20and%20Ningyu%20Zhang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20remarkable%20performance%20in%20complex%0Areasoning%20tasks%2C%20but%20their%20efficiency%20is%20hindered%20by%20the%20substantial%20memory%20and%0Acomputational%20costs%20associated%20with%20generating%20lengthy%20tokens.%20In%20this%20paper%2C%0Awe%20propose%20LightThinker%2C%20a%20novel%20method%20that%20enables%20LLMs%20to%20dynamically%0Acompress%20intermediate%20thoughts%20during%20reasoning.%20Inspired%20by%20human%20cognitive%0Aprocesses%2C%20LightThinker%20compresses%20verbose%20thought%20steps%20into%20compact%0Arepresentations%20and%20discards%20the%20original%20reasoning%20chains%2C%20thereby%0Asignificantly%20reducing%20the%20number%20of%20tokens%20stored%20in%20the%20context%20window.%20This%0Ais%20achieved%20by%20training%20the%20model%20on%20when%20and%20how%20to%20perform%20compression%0Athrough%20data%20construction%2C%20mapping%20hidden%20states%20to%20condensed%20gist%20tokens%2C%20and%0Acreating%20specialized%20attention%20masks.%20Additionally%2C%20we%20introduce%20the%20Dependency%0A%28Dep%29%20metric%20to%20quantify%20the%20degree%20of%20compression%20by%20measuring%20the%20reliance%20on%0Ahistorical%20tokens%20during%20generation.%20Extensive%20experiments%20on%20four%20datasets%20and%0Atwo%20models%20show%20that%20LightThinker%20reduces%20peak%20memory%20usage%20and%20inference%20time%2C%0Awhile%20maintaining%20competitive%20accuracy.%20Our%20work%20provides%20a%20new%20direction%20for%0Aimproving%20the%20efficiency%20of%20LLMs%20in%20complex%20reasoning%20tasks%20without%20sacrificing%0Aperformance.%20Code%20will%20be%20released%20at%20https%3A//github.com/zjunlp/LightThinker.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15589v1&entry.124074799=Read"},
{"title": "Training Turn-by-Turn Verifiers for Dialogue Tutoring Agents: The\n  Curious Case of LLMs as Your Coding Tutors", "author": "Jian Wang and Yinpei Dai and Yichi Zhang and Ziqiao Ma and Wenjie Li and Joyce Chai", "abstract": "  Intelligent tutoring agents powered by large language models (LLMs) have been\nincreasingly explored to deliver personalized guidance in areas such as\nlanguage learning and science education. However, their capabilities in guiding\nusers to solve complex real-world tasks remain underexplored. To address this\nlimitation, in this work, we focus on coding tutoring, a challenging problem\nthat requires tutors to proactively guide students toward completing predefined\ncoding tasks. We propose a novel agent workflow, Trace-and-Verify (TRAVER),\nwhich combines knowledge tracing to estimate a student's knowledge state and\nturn-by-turn verification to ensure effective guidance toward task completion.\nWe introduce DICT, an automatic evaluation protocol that assesses tutor agents\nholistically using controlled student simulation and code generation tests.\nExtensive experiments reveal the challenges of coding tutoring and demonstrate\nthat TRAVER achieves a significantly higher success rate. Although we use code\ntutoring as an example in this paper, our results and findings can be extended\nbeyond coding, providing valuable insights into advancing tutoring agents for a\nvariety of tasks.\n", "link": "http://arxiv.org/abs/2502.13311v2", "date": "2025-02-21", "relevancy": 2.6531, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5488}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5369}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20Turn-by-Turn%20Verifiers%20for%20Dialogue%20Tutoring%20Agents%3A%20The%0A%20%20Curious%20Case%20of%20LLMs%20as%20Your%20Coding%20Tutors&body=Title%3A%20Training%20Turn-by-Turn%20Verifiers%20for%20Dialogue%20Tutoring%20Agents%3A%20The%0A%20%20Curious%20Case%20of%20LLMs%20as%20Your%20Coding%20Tutors%0AAuthor%3A%20Jian%20Wang%20and%20Yinpei%20Dai%20and%20Yichi%20Zhang%20and%20Ziqiao%20Ma%20and%20Wenjie%20Li%20and%20Joyce%20Chai%0AAbstract%3A%20%20%20Intelligent%20tutoring%20agents%20powered%20by%20large%20language%20models%20%28LLMs%29%20have%20been%0Aincreasingly%20explored%20to%20deliver%20personalized%20guidance%20in%20areas%20such%20as%0Alanguage%20learning%20and%20science%20education.%20However%2C%20their%20capabilities%20in%20guiding%0Ausers%20to%20solve%20complex%20real-world%20tasks%20remain%20underexplored.%20To%20address%20this%0Alimitation%2C%20in%20this%20work%2C%20we%20focus%20on%20coding%20tutoring%2C%20a%20challenging%20problem%0Athat%20requires%20tutors%20to%20proactively%20guide%20students%20toward%20completing%20predefined%0Acoding%20tasks.%20We%20propose%20a%20novel%20agent%20workflow%2C%20Trace-and-Verify%20%28TRAVER%29%2C%0Awhich%20combines%20knowledge%20tracing%20to%20estimate%20a%20student%27s%20knowledge%20state%20and%0Aturn-by-turn%20verification%20to%20ensure%20effective%20guidance%20toward%20task%20completion.%0AWe%20introduce%20DICT%2C%20an%20automatic%20evaluation%20protocol%20that%20assesses%20tutor%20agents%0Aholistically%20using%20controlled%20student%20simulation%20and%20code%20generation%20tests.%0AExtensive%20experiments%20reveal%20the%20challenges%20of%20coding%20tutoring%20and%20demonstrate%0Athat%20TRAVER%20achieves%20a%20significantly%20higher%20success%20rate.%20Although%20we%20use%20code%0Atutoring%20as%20an%20example%20in%20this%20paper%2C%20our%20results%20and%20findings%20can%20be%20extended%0Abeyond%20coding%2C%20providing%20valuable%20insights%20into%20advancing%20tutoring%20agents%20for%20a%0Avariety%20of%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13311v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520Turn-by-Turn%2520Verifiers%2520for%2520Dialogue%2520Tutoring%2520Agents%253A%2520The%250A%2520%2520Curious%2520Case%2520of%2520LLMs%2520as%2520Your%2520Coding%2520Tutors%26entry.906535625%3DJian%2520Wang%2520and%2520Yinpei%2520Dai%2520and%2520Yichi%2520Zhang%2520and%2520Ziqiao%2520Ma%2520and%2520Wenjie%2520Li%2520and%2520Joyce%2520Chai%26entry.1292438233%3D%2520%2520Intelligent%2520tutoring%2520agents%2520powered%2520by%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520been%250Aincreasingly%2520explored%2520to%2520deliver%2520personalized%2520guidance%2520in%2520areas%2520such%2520as%250Alanguage%2520learning%2520and%2520science%2520education.%2520However%252C%2520their%2520capabilities%2520in%2520guiding%250Ausers%2520to%2520solve%2520complex%2520real-world%2520tasks%2520remain%2520underexplored.%2520To%2520address%2520this%250Alimitation%252C%2520in%2520this%2520work%252C%2520we%2520focus%2520on%2520coding%2520tutoring%252C%2520a%2520challenging%2520problem%250Athat%2520requires%2520tutors%2520to%2520proactively%2520guide%2520students%2520toward%2520completing%2520predefined%250Acoding%2520tasks.%2520We%2520propose%2520a%2520novel%2520agent%2520workflow%252C%2520Trace-and-Verify%2520%2528TRAVER%2529%252C%250Awhich%2520combines%2520knowledge%2520tracing%2520to%2520estimate%2520a%2520student%2527s%2520knowledge%2520state%2520and%250Aturn-by-turn%2520verification%2520to%2520ensure%2520effective%2520guidance%2520toward%2520task%2520completion.%250AWe%2520introduce%2520DICT%252C%2520an%2520automatic%2520evaluation%2520protocol%2520that%2520assesses%2520tutor%2520agents%250Aholistically%2520using%2520controlled%2520student%2520simulation%2520and%2520code%2520generation%2520tests.%250AExtensive%2520experiments%2520reveal%2520the%2520challenges%2520of%2520coding%2520tutoring%2520and%2520demonstrate%250Athat%2520TRAVER%2520achieves%2520a%2520significantly%2520higher%2520success%2520rate.%2520Although%2520we%2520use%2520code%250Atutoring%2520as%2520an%2520example%2520in%2520this%2520paper%252C%2520our%2520results%2520and%2520findings%2520can%2520be%2520extended%250Abeyond%2520coding%252C%2520providing%2520valuable%2520insights%2520into%2520advancing%2520tutoring%2520agents%2520for%2520a%250Avariety%2520of%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13311v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20Turn-by-Turn%20Verifiers%20for%20Dialogue%20Tutoring%20Agents%3A%20The%0A%20%20Curious%20Case%20of%20LLMs%20as%20Your%20Coding%20Tutors&entry.906535625=Jian%20Wang%20and%20Yinpei%20Dai%20and%20Yichi%20Zhang%20and%20Ziqiao%20Ma%20and%20Wenjie%20Li%20and%20Joyce%20Chai&entry.1292438233=%20%20Intelligent%20tutoring%20agents%20powered%20by%20large%20language%20models%20%28LLMs%29%20have%20been%0Aincreasingly%20explored%20to%20deliver%20personalized%20guidance%20in%20areas%20such%20as%0Alanguage%20learning%20and%20science%20education.%20However%2C%20their%20capabilities%20in%20guiding%0Ausers%20to%20solve%20complex%20real-world%20tasks%20remain%20underexplored.%20To%20address%20this%0Alimitation%2C%20in%20this%20work%2C%20we%20focus%20on%20coding%20tutoring%2C%20a%20challenging%20problem%0Athat%20requires%20tutors%20to%20proactively%20guide%20students%20toward%20completing%20predefined%0Acoding%20tasks.%20We%20propose%20a%20novel%20agent%20workflow%2C%20Trace-and-Verify%20%28TRAVER%29%2C%0Awhich%20combines%20knowledge%20tracing%20to%20estimate%20a%20student%27s%20knowledge%20state%20and%0Aturn-by-turn%20verification%20to%20ensure%20effective%20guidance%20toward%20task%20completion.%0AWe%20introduce%20DICT%2C%20an%20automatic%20evaluation%20protocol%20that%20assesses%20tutor%20agents%0Aholistically%20using%20controlled%20student%20simulation%20and%20code%20generation%20tests.%0AExtensive%20experiments%20reveal%20the%20challenges%20of%20coding%20tutoring%20and%20demonstrate%0Athat%20TRAVER%20achieves%20a%20significantly%20higher%20success%20rate.%20Although%20we%20use%20code%0Atutoring%20as%20an%20example%20in%20this%20paper%2C%20our%20results%20and%20findings%20can%20be%20extended%0Abeyond%20coding%2C%20providing%20valuable%20insights%20into%20advancing%20tutoring%20agents%20for%20a%0Avariety%20of%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13311v2&entry.124074799=Read"},
{"title": "Steering into New Embedding Spaces: Analyzing Cross-Lingual Alignment\n  Induced by Model Interventions in Multilingual Language Models", "author": "Anirudh Sundar and Sinead Williamson and Katherine Metcalf and Barry-John Theobald and Skyler Seto and Masha Fedzechkina", "abstract": "  Aligned representations across languages is a desired property in\nmultilingual large language models (mLLMs), as alignment can improve\nperformance in cross-lingual tasks. Typically alignment requires fine-tuning a\nmodel, which is computationally expensive, and sizable language data, which\noften may not be available. A data-efficient alternative to fine-tuning is\nmodel interventions -- a method for manipulating model activations to steer\ngeneration into the desired direction. We analyze the effect of a popular\nintervention (finding experts) on the alignment of cross-lingual\nrepresentations in mLLMs. We identify the neurons to manipulate for a given\nlanguage and introspect the embedding space of mLLMs pre- and\npost-manipulation. We show that modifying the mLLM's activations changes its\nembedding space such that cross-lingual alignment is enhanced. Further, we show\nthat the changes to the embedding space translate into improved downstream\nperformance on retrieval tasks, with up to 2x improvements in top-1 accuracy on\ncross-lingual retrieval.\n", "link": "http://arxiv.org/abs/2502.15639v1", "date": "2025-02-21", "relevancy": 2.6415, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5348}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5251}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5251}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Steering%20into%20New%20Embedding%20Spaces%3A%20Analyzing%20Cross-Lingual%20Alignment%0A%20%20Induced%20by%20Model%20Interventions%20in%20Multilingual%20Language%20Models&body=Title%3A%20Steering%20into%20New%20Embedding%20Spaces%3A%20Analyzing%20Cross-Lingual%20Alignment%0A%20%20Induced%20by%20Model%20Interventions%20in%20Multilingual%20Language%20Models%0AAuthor%3A%20Anirudh%20Sundar%20and%20Sinead%20Williamson%20and%20Katherine%20Metcalf%20and%20Barry-John%20Theobald%20and%20Skyler%20Seto%20and%20Masha%20Fedzechkina%0AAbstract%3A%20%20%20Aligned%20representations%20across%20languages%20is%20a%20desired%20property%20in%0Amultilingual%20large%20language%20models%20%28mLLMs%29%2C%20as%20alignment%20can%20improve%0Aperformance%20in%20cross-lingual%20tasks.%20Typically%20alignment%20requires%20fine-tuning%20a%0Amodel%2C%20which%20is%20computationally%20expensive%2C%20and%20sizable%20language%20data%2C%20which%0Aoften%20may%20not%20be%20available.%20A%20data-efficient%20alternative%20to%20fine-tuning%20is%0Amodel%20interventions%20--%20a%20method%20for%20manipulating%20model%20activations%20to%20steer%0Ageneration%20into%20the%20desired%20direction.%20We%20analyze%20the%20effect%20of%20a%20popular%0Aintervention%20%28finding%20experts%29%20on%20the%20alignment%20of%20cross-lingual%0Arepresentations%20in%20mLLMs.%20We%20identify%20the%20neurons%20to%20manipulate%20for%20a%20given%0Alanguage%20and%20introspect%20the%20embedding%20space%20of%20mLLMs%20pre-%20and%0Apost-manipulation.%20We%20show%20that%20modifying%20the%20mLLM%27s%20activations%20changes%20its%0Aembedding%20space%20such%20that%20cross-lingual%20alignment%20is%20enhanced.%20Further%2C%20we%20show%0Athat%20the%20changes%20to%20the%20embedding%20space%20translate%20into%20improved%20downstream%0Aperformance%20on%20retrieval%20tasks%2C%20with%20up%20to%202x%20improvements%20in%20top-1%20accuracy%20on%0Across-lingual%20retrieval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15639v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSteering%2520into%2520New%2520Embedding%2520Spaces%253A%2520Analyzing%2520Cross-Lingual%2520Alignment%250A%2520%2520Induced%2520by%2520Model%2520Interventions%2520in%2520Multilingual%2520Language%2520Models%26entry.906535625%3DAnirudh%2520Sundar%2520and%2520Sinead%2520Williamson%2520and%2520Katherine%2520Metcalf%2520and%2520Barry-John%2520Theobald%2520and%2520Skyler%2520Seto%2520and%2520Masha%2520Fedzechkina%26entry.1292438233%3D%2520%2520Aligned%2520representations%2520across%2520languages%2520is%2520a%2520desired%2520property%2520in%250Amultilingual%2520large%2520language%2520models%2520%2528mLLMs%2529%252C%2520as%2520alignment%2520can%2520improve%250Aperformance%2520in%2520cross-lingual%2520tasks.%2520Typically%2520alignment%2520requires%2520fine-tuning%2520a%250Amodel%252C%2520which%2520is%2520computationally%2520expensive%252C%2520and%2520sizable%2520language%2520data%252C%2520which%250Aoften%2520may%2520not%2520be%2520available.%2520A%2520data-efficient%2520alternative%2520to%2520fine-tuning%2520is%250Amodel%2520interventions%2520--%2520a%2520method%2520for%2520manipulating%2520model%2520activations%2520to%2520steer%250Ageneration%2520into%2520the%2520desired%2520direction.%2520We%2520analyze%2520the%2520effect%2520of%2520a%2520popular%250Aintervention%2520%2528finding%2520experts%2529%2520on%2520the%2520alignment%2520of%2520cross-lingual%250Arepresentations%2520in%2520mLLMs.%2520We%2520identify%2520the%2520neurons%2520to%2520manipulate%2520for%2520a%2520given%250Alanguage%2520and%2520introspect%2520the%2520embedding%2520space%2520of%2520mLLMs%2520pre-%2520and%250Apost-manipulation.%2520We%2520show%2520that%2520modifying%2520the%2520mLLM%2527s%2520activations%2520changes%2520its%250Aembedding%2520space%2520such%2520that%2520cross-lingual%2520alignment%2520is%2520enhanced.%2520Further%252C%2520we%2520show%250Athat%2520the%2520changes%2520to%2520the%2520embedding%2520space%2520translate%2520into%2520improved%2520downstream%250Aperformance%2520on%2520retrieval%2520tasks%252C%2520with%2520up%2520to%25202x%2520improvements%2520in%2520top-1%2520accuracy%2520on%250Across-lingual%2520retrieval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15639v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Steering%20into%20New%20Embedding%20Spaces%3A%20Analyzing%20Cross-Lingual%20Alignment%0A%20%20Induced%20by%20Model%20Interventions%20in%20Multilingual%20Language%20Models&entry.906535625=Anirudh%20Sundar%20and%20Sinead%20Williamson%20and%20Katherine%20Metcalf%20and%20Barry-John%20Theobald%20and%20Skyler%20Seto%20and%20Masha%20Fedzechkina&entry.1292438233=%20%20Aligned%20representations%20across%20languages%20is%20a%20desired%20property%20in%0Amultilingual%20large%20language%20models%20%28mLLMs%29%2C%20as%20alignment%20can%20improve%0Aperformance%20in%20cross-lingual%20tasks.%20Typically%20alignment%20requires%20fine-tuning%20a%0Amodel%2C%20which%20is%20computationally%20expensive%2C%20and%20sizable%20language%20data%2C%20which%0Aoften%20may%20not%20be%20available.%20A%20data-efficient%20alternative%20to%20fine-tuning%20is%0Amodel%20interventions%20--%20a%20method%20for%20manipulating%20model%20activations%20to%20steer%0Ageneration%20into%20the%20desired%20direction.%20We%20analyze%20the%20effect%20of%20a%20popular%0Aintervention%20%28finding%20experts%29%20on%20the%20alignment%20of%20cross-lingual%0Arepresentations%20in%20mLLMs.%20We%20identify%20the%20neurons%20to%20manipulate%20for%20a%20given%0Alanguage%20and%20introspect%20the%20embedding%20space%20of%20mLLMs%20pre-%20and%0Apost-manipulation.%20We%20show%20that%20modifying%20the%20mLLM%27s%20activations%20changes%20its%0Aembedding%20space%20such%20that%20cross-lingual%20alignment%20is%20enhanced.%20Further%2C%20we%20show%0Athat%20the%20changes%20to%20the%20embedding%20space%20translate%20into%20improved%20downstream%0Aperformance%20on%20retrieval%20tasks%2C%20with%20up%20to%202x%20improvements%20in%20top-1%20accuracy%20on%0Across-lingual%20retrieval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15639v1&entry.124074799=Read"},
{"title": "Empowering LLMs with Logical Reasoning: A Comprehensive Survey", "author": "Fengxiang Cheng and Haoxuan Li and Fenrong Liu and Robert van Rooij and Kun Zhang and Zhouchen Lin", "abstract": "  Large language models (LLMs) have achieved remarkable successes on various\nnatural language tasks. However, recent studies have found that there are still\nsignificant challenges to the logical reasoning abilities of LLMs. This paper\nsummarizes and categorizes the main challenges into two aspects: (1) Logical\nquestion answering, LLMs often fail to generate the correct answer within\ncomplex logical problem which requires sophisticated deductive, inductive or\nabductive reasoning given a collection of premises and constrains. (2) Logical\nconsistency, LLMs are prone to producing responses contradicting themselves\nacross different questions. For example, a state-of-the-art Macaw\nquestion-answering LLM answers Yes to both questions Is a magpie a bird? and\nDoes a bird have wings? but answers No to Does a magpie have wings?. To\nfacilitate this research direction, we comprehensively investigate the most\ncutting-edge methods and propose detailed taxonomies of these methods.\nSpecifically, to accurately answer complex logic questions, previous methods\ncan be categorized based on reliance on external solvers, prompts, pretraining,\nand fine-tuning. To avoid logical contradictions, we discuss concepts and\nsolutions of various logical consistencies, including implication, negation,\ntransitivity, factuality consistency, and their composites. In addition, we\nreview commonly used benchmark datasets and evaluation metrics, and discuss\npromising research directions, such as extensions to modal logic to account for\nuncertainty, and efficient algorithms satisfying multiple logical consistencies\nsimultaneously.\n", "link": "http://arxiv.org/abs/2502.15652v1", "date": "2025-02-21", "relevancy": 2.6392, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5416}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5416}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Empowering%20LLMs%20with%20Logical%20Reasoning%3A%20A%20Comprehensive%20Survey&body=Title%3A%20Empowering%20LLMs%20with%20Logical%20Reasoning%3A%20A%20Comprehensive%20Survey%0AAuthor%3A%20Fengxiang%20Cheng%20and%20Haoxuan%20Li%20and%20Fenrong%20Liu%20and%20Robert%20van%20Rooij%20and%20Kun%20Zhang%20and%20Zhouchen%20Lin%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20remarkable%20successes%20on%20various%0Anatural%20language%20tasks.%20However%2C%20recent%20studies%20have%20found%20that%20there%20are%20still%0Asignificant%20challenges%20to%20the%20logical%20reasoning%20abilities%20of%20LLMs.%20This%20paper%0Asummarizes%20and%20categorizes%20the%20main%20challenges%20into%20two%20aspects%3A%20%281%29%20Logical%0Aquestion%20answering%2C%20LLMs%20often%20fail%20to%20generate%20the%20correct%20answer%20within%0Acomplex%20logical%20problem%20which%20requires%20sophisticated%20deductive%2C%20inductive%20or%0Aabductive%20reasoning%20given%20a%20collection%20of%20premises%20and%20constrains.%20%282%29%20Logical%0Aconsistency%2C%20LLMs%20are%20prone%20to%20producing%20responses%20contradicting%20themselves%0Aacross%20different%20questions.%20For%20example%2C%20a%20state-of-the-art%20Macaw%0Aquestion-answering%20LLM%20answers%20Yes%20to%20both%20questions%20Is%20a%20magpie%20a%20bird%3F%20and%0ADoes%20a%20bird%20have%20wings%3F%20but%20answers%20No%20to%20Does%20a%20magpie%20have%20wings%3F.%20To%0Afacilitate%20this%20research%20direction%2C%20we%20comprehensively%20investigate%20the%20most%0Acutting-edge%20methods%20and%20propose%20detailed%20taxonomies%20of%20these%20methods.%0ASpecifically%2C%20to%20accurately%20answer%20complex%20logic%20questions%2C%20previous%20methods%0Acan%20be%20categorized%20based%20on%20reliance%20on%20external%20solvers%2C%20prompts%2C%20pretraining%2C%0Aand%20fine-tuning.%20To%20avoid%20logical%20contradictions%2C%20we%20discuss%20concepts%20and%0Asolutions%20of%20various%20logical%20consistencies%2C%20including%20implication%2C%20negation%2C%0Atransitivity%2C%20factuality%20consistency%2C%20and%20their%20composites.%20In%20addition%2C%20we%0Areview%20commonly%20used%20benchmark%20datasets%20and%20evaluation%20metrics%2C%20and%20discuss%0Apromising%20research%20directions%2C%20such%20as%20extensions%20to%20modal%20logic%20to%20account%20for%0Auncertainty%2C%20and%20efficient%20algorithms%20satisfying%20multiple%20logical%20consistencies%0Asimultaneously.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15652v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmpowering%2520LLMs%2520with%2520Logical%2520Reasoning%253A%2520A%2520Comprehensive%2520Survey%26entry.906535625%3DFengxiang%2520Cheng%2520and%2520Haoxuan%2520Li%2520and%2520Fenrong%2520Liu%2520and%2520Robert%2520van%2520Rooij%2520and%2520Kun%2520Zhang%2520and%2520Zhouchen%2520Lin%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520achieved%2520remarkable%2520successes%2520on%2520various%250Anatural%2520language%2520tasks.%2520However%252C%2520recent%2520studies%2520have%2520found%2520that%2520there%2520are%2520still%250Asignificant%2520challenges%2520to%2520the%2520logical%2520reasoning%2520abilities%2520of%2520LLMs.%2520This%2520paper%250Asummarizes%2520and%2520categorizes%2520the%2520main%2520challenges%2520into%2520two%2520aspects%253A%2520%25281%2529%2520Logical%250Aquestion%2520answering%252C%2520LLMs%2520often%2520fail%2520to%2520generate%2520the%2520correct%2520answer%2520within%250Acomplex%2520logical%2520problem%2520which%2520requires%2520sophisticated%2520deductive%252C%2520inductive%2520or%250Aabductive%2520reasoning%2520given%2520a%2520collection%2520of%2520premises%2520and%2520constrains.%2520%25282%2529%2520Logical%250Aconsistency%252C%2520LLMs%2520are%2520prone%2520to%2520producing%2520responses%2520contradicting%2520themselves%250Aacross%2520different%2520questions.%2520For%2520example%252C%2520a%2520state-of-the-art%2520Macaw%250Aquestion-answering%2520LLM%2520answers%2520Yes%2520to%2520both%2520questions%2520Is%2520a%2520magpie%2520a%2520bird%253F%2520and%250ADoes%2520a%2520bird%2520have%2520wings%253F%2520but%2520answers%2520No%2520to%2520Does%2520a%2520magpie%2520have%2520wings%253F.%2520To%250Afacilitate%2520this%2520research%2520direction%252C%2520we%2520comprehensively%2520investigate%2520the%2520most%250Acutting-edge%2520methods%2520and%2520propose%2520detailed%2520taxonomies%2520of%2520these%2520methods.%250ASpecifically%252C%2520to%2520accurately%2520answer%2520complex%2520logic%2520questions%252C%2520previous%2520methods%250Acan%2520be%2520categorized%2520based%2520on%2520reliance%2520on%2520external%2520solvers%252C%2520prompts%252C%2520pretraining%252C%250Aand%2520fine-tuning.%2520To%2520avoid%2520logical%2520contradictions%252C%2520we%2520discuss%2520concepts%2520and%250Asolutions%2520of%2520various%2520logical%2520consistencies%252C%2520including%2520implication%252C%2520negation%252C%250Atransitivity%252C%2520factuality%2520consistency%252C%2520and%2520their%2520composites.%2520In%2520addition%252C%2520we%250Areview%2520commonly%2520used%2520benchmark%2520datasets%2520and%2520evaluation%2520metrics%252C%2520and%2520discuss%250Apromising%2520research%2520directions%252C%2520such%2520as%2520extensions%2520to%2520modal%2520logic%2520to%2520account%2520for%250Auncertainty%252C%2520and%2520efficient%2520algorithms%2520satisfying%2520multiple%2520logical%2520consistencies%250Asimultaneously.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15652v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empowering%20LLMs%20with%20Logical%20Reasoning%3A%20A%20Comprehensive%20Survey&entry.906535625=Fengxiang%20Cheng%20and%20Haoxuan%20Li%20and%20Fenrong%20Liu%20and%20Robert%20van%20Rooij%20and%20Kun%20Zhang%20and%20Zhouchen%20Lin&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20remarkable%20successes%20on%20various%0Anatural%20language%20tasks.%20However%2C%20recent%20studies%20have%20found%20that%20there%20are%20still%0Asignificant%20challenges%20to%20the%20logical%20reasoning%20abilities%20of%20LLMs.%20This%20paper%0Asummarizes%20and%20categorizes%20the%20main%20challenges%20into%20two%20aspects%3A%20%281%29%20Logical%0Aquestion%20answering%2C%20LLMs%20often%20fail%20to%20generate%20the%20correct%20answer%20within%0Acomplex%20logical%20problem%20which%20requires%20sophisticated%20deductive%2C%20inductive%20or%0Aabductive%20reasoning%20given%20a%20collection%20of%20premises%20and%20constrains.%20%282%29%20Logical%0Aconsistency%2C%20LLMs%20are%20prone%20to%20producing%20responses%20contradicting%20themselves%0Aacross%20different%20questions.%20For%20example%2C%20a%20state-of-the-art%20Macaw%0Aquestion-answering%20LLM%20answers%20Yes%20to%20both%20questions%20Is%20a%20magpie%20a%20bird%3F%20and%0ADoes%20a%20bird%20have%20wings%3F%20but%20answers%20No%20to%20Does%20a%20magpie%20have%20wings%3F.%20To%0Afacilitate%20this%20research%20direction%2C%20we%20comprehensively%20investigate%20the%20most%0Acutting-edge%20methods%20and%20propose%20detailed%20taxonomies%20of%20these%20methods.%0ASpecifically%2C%20to%20accurately%20answer%20complex%20logic%20questions%2C%20previous%20methods%0Acan%20be%20categorized%20based%20on%20reliance%20on%20external%20solvers%2C%20prompts%2C%20pretraining%2C%0Aand%20fine-tuning.%20To%20avoid%20logical%20contradictions%2C%20we%20discuss%20concepts%20and%0Asolutions%20of%20various%20logical%20consistencies%2C%20including%20implication%2C%20negation%2C%0Atransitivity%2C%20factuality%20consistency%2C%20and%20their%20composites.%20In%20addition%2C%20we%0Areview%20commonly%20used%20benchmark%20datasets%20and%20evaluation%20metrics%2C%20and%20discuss%0Apromising%20research%20directions%2C%20such%20as%20extensions%20to%20modal%20logic%20to%20account%20for%0Auncertainty%2C%20and%20efficient%20algorithms%20satisfying%20multiple%20logical%20consistencies%0Asimultaneously.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15652v1&entry.124074799=Read"},
{"title": "Chitrarth: Bridging Vision and Language for a Billion People", "author": "Shaharukh Khan and Ayush Tarun and Abhinav Ravi and Ali Faraz and Akshat Patidar and Praveen Kumar Pokala and Anagha Bhangare and Raja Kolla and Chandra Khatri and Shubham Agarwal", "abstract": "  Recent multimodal foundation models are primarily trained on English or high\nresource European language data, which hinders their applicability to other\nmedium and low-resource languages. To address this limitation, we introduce\nChitrarth (Chitra: Image; Artha: Meaning), an inclusive Vision-Language Model\n(VLM), specifically targeting the rich linguistic diversity and visual\nreasoning across 10 prominent Indian languages. Our model effectively\nintegrates a state-of-the-art (SOTA) multilingual Large Language Model (LLM)\nwith a vision module, primarily trained on multilingual image-text data.\nFurthermore, we also introduce BharatBench, a comprehensive framework for\nevaluating VLMs across various Indian languages, ultimately contributing to\nmore diverse and effective AI systems. Our model achieves SOTA results for\nbenchmarks across low resource languages while retaining its efficiency in\nEnglish. Through our research, we aim to set new benchmarks in\nmultilingual-multimodal capabilities, offering substantial improvements over\nexisting models and establishing a foundation to facilitate future advancements\nin this arena.\n", "link": "http://arxiv.org/abs/2502.15392v1", "date": "2025-02-21", "relevancy": 2.6081, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.525}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.525}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5148}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chitrarth%3A%20Bridging%20Vision%20and%20Language%20for%20a%20Billion%20People&body=Title%3A%20Chitrarth%3A%20Bridging%20Vision%20and%20Language%20for%20a%20Billion%20People%0AAuthor%3A%20Shaharukh%20Khan%20and%20Ayush%20Tarun%20and%20Abhinav%20Ravi%20and%20Ali%20Faraz%20and%20Akshat%20Patidar%20and%20Praveen%20Kumar%20Pokala%20and%20Anagha%20Bhangare%20and%20Raja%20Kolla%20and%20Chandra%20Khatri%20and%20Shubham%20Agarwal%0AAbstract%3A%20%20%20Recent%20multimodal%20foundation%20models%20are%20primarily%20trained%20on%20English%20or%20high%0Aresource%20European%20language%20data%2C%20which%20hinders%20their%20applicability%20to%20other%0Amedium%20and%20low-resource%20languages.%20To%20address%20this%20limitation%2C%20we%20introduce%0AChitrarth%20%28Chitra%3A%20Image%3B%20Artha%3A%20Meaning%29%2C%20an%20inclusive%20Vision-Language%20Model%0A%28VLM%29%2C%20specifically%20targeting%20the%20rich%20linguistic%20diversity%20and%20visual%0Areasoning%20across%2010%20prominent%20Indian%20languages.%20Our%20model%20effectively%0Aintegrates%20a%20state-of-the-art%20%28SOTA%29%20multilingual%20Large%20Language%20Model%20%28LLM%29%0Awith%20a%20vision%20module%2C%20primarily%20trained%20on%20multilingual%20image-text%20data.%0AFurthermore%2C%20we%20also%20introduce%20BharatBench%2C%20a%20comprehensive%20framework%20for%0Aevaluating%20VLMs%20across%20various%20Indian%20languages%2C%20ultimately%20contributing%20to%0Amore%20diverse%20and%20effective%20AI%20systems.%20Our%20model%20achieves%20SOTA%20results%20for%0Abenchmarks%20across%20low%20resource%20languages%20while%20retaining%20its%20efficiency%20in%0AEnglish.%20Through%20our%20research%2C%20we%20aim%20to%20set%20new%20benchmarks%20in%0Amultilingual-multimodal%20capabilities%2C%20offering%20substantial%20improvements%20over%0Aexisting%20models%20and%20establishing%20a%20foundation%20to%20facilitate%20future%20advancements%0Ain%20this%20arena.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15392v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChitrarth%253A%2520Bridging%2520Vision%2520and%2520Language%2520for%2520a%2520Billion%2520People%26entry.906535625%3DShaharukh%2520Khan%2520and%2520Ayush%2520Tarun%2520and%2520Abhinav%2520Ravi%2520and%2520Ali%2520Faraz%2520and%2520Akshat%2520Patidar%2520and%2520Praveen%2520Kumar%2520Pokala%2520and%2520Anagha%2520Bhangare%2520and%2520Raja%2520Kolla%2520and%2520Chandra%2520Khatri%2520and%2520Shubham%2520Agarwal%26entry.1292438233%3D%2520%2520Recent%2520multimodal%2520foundation%2520models%2520are%2520primarily%2520trained%2520on%2520English%2520or%2520high%250Aresource%2520European%2520language%2520data%252C%2520which%2520hinders%2520their%2520applicability%2520to%2520other%250Amedium%2520and%2520low-resource%2520languages.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%250AChitrarth%2520%2528Chitra%253A%2520Image%253B%2520Artha%253A%2520Meaning%2529%252C%2520an%2520inclusive%2520Vision-Language%2520Model%250A%2528VLM%2529%252C%2520specifically%2520targeting%2520the%2520rich%2520linguistic%2520diversity%2520and%2520visual%250Areasoning%2520across%252010%2520prominent%2520Indian%2520languages.%2520Our%2520model%2520effectively%250Aintegrates%2520a%2520state-of-the-art%2520%2528SOTA%2529%2520multilingual%2520Large%2520Language%2520Model%2520%2528LLM%2529%250Awith%2520a%2520vision%2520module%252C%2520primarily%2520trained%2520on%2520multilingual%2520image-text%2520data.%250AFurthermore%252C%2520we%2520also%2520introduce%2520BharatBench%252C%2520a%2520comprehensive%2520framework%2520for%250Aevaluating%2520VLMs%2520across%2520various%2520Indian%2520languages%252C%2520ultimately%2520contributing%2520to%250Amore%2520diverse%2520and%2520effective%2520AI%2520systems.%2520Our%2520model%2520achieves%2520SOTA%2520results%2520for%250Abenchmarks%2520across%2520low%2520resource%2520languages%2520while%2520retaining%2520its%2520efficiency%2520in%250AEnglish.%2520Through%2520our%2520research%252C%2520we%2520aim%2520to%2520set%2520new%2520benchmarks%2520in%250Amultilingual-multimodal%2520capabilities%252C%2520offering%2520substantial%2520improvements%2520over%250Aexisting%2520models%2520and%2520establishing%2520a%2520foundation%2520to%2520facilitate%2520future%2520advancements%250Ain%2520this%2520arena.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15392v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chitrarth%3A%20Bridging%20Vision%20and%20Language%20for%20a%20Billion%20People&entry.906535625=Shaharukh%20Khan%20and%20Ayush%20Tarun%20and%20Abhinav%20Ravi%20and%20Ali%20Faraz%20and%20Akshat%20Patidar%20and%20Praveen%20Kumar%20Pokala%20and%20Anagha%20Bhangare%20and%20Raja%20Kolla%20and%20Chandra%20Khatri%20and%20Shubham%20Agarwal&entry.1292438233=%20%20Recent%20multimodal%20foundation%20models%20are%20primarily%20trained%20on%20English%20or%20high%0Aresource%20European%20language%20data%2C%20which%20hinders%20their%20applicability%20to%20other%0Amedium%20and%20low-resource%20languages.%20To%20address%20this%20limitation%2C%20we%20introduce%0AChitrarth%20%28Chitra%3A%20Image%3B%20Artha%3A%20Meaning%29%2C%20an%20inclusive%20Vision-Language%20Model%0A%28VLM%29%2C%20specifically%20targeting%20the%20rich%20linguistic%20diversity%20and%20visual%0Areasoning%20across%2010%20prominent%20Indian%20languages.%20Our%20model%20effectively%0Aintegrates%20a%20state-of-the-art%20%28SOTA%29%20multilingual%20Large%20Language%20Model%20%28LLM%29%0Awith%20a%20vision%20module%2C%20primarily%20trained%20on%20multilingual%20image-text%20data.%0AFurthermore%2C%20we%20also%20introduce%20BharatBench%2C%20a%20comprehensive%20framework%20for%0Aevaluating%20VLMs%20across%20various%20Indian%20languages%2C%20ultimately%20contributing%20to%0Amore%20diverse%20and%20effective%20AI%20systems.%20Our%20model%20achieves%20SOTA%20results%20for%0Abenchmarks%20across%20low%20resource%20languages%20while%20retaining%20its%20efficiency%20in%0AEnglish.%20Through%20our%20research%2C%20we%20aim%20to%20set%20new%20benchmarks%20in%0Amultilingual-multimodal%20capabilities%2C%20offering%20substantial%20improvements%20over%0Aexisting%20models%20and%20establishing%20a%20foundation%20to%20facilitate%20future%20advancements%0Ain%20this%20arena.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15392v1&entry.124074799=Read"},
{"title": "Less is More: Pre-Training Cross-Lingual Small-Scale Language Models\n  with Cognitively-Plausible Curriculum Learning Strategies", "author": "Suchir Salhan and Richard Diehl Martinez and Z\u00e9bulon Goriely and Paula Buttery", "abstract": "  Curriculum Learning has been a popular strategy to improve the cognitive\nplausibility of Small-Scale Language Models (SSLMs) in the BabyLM Challenge.\nHowever, it has not led to considerable improvements over non-curriculum\nmodels. We assess whether theoretical linguistic acquisition theories can be\nused to specify more fine-grained curriculum learning strategies, creating\nage-ordered corpora of Child-Directed Speech for four typologically distant\nlanguage families to implement SSLMs and acquisition-inspired curricula\ncross-lingually. Comparing the success of three objective curricula (Growing,\nInwards and MMM) that precisely replicate the predictions of acquisition\ntheories on a standard SSLM architecture, we find fine-grained\nacquisition-inspired curricula can outperform non-curriculum baselines and\nperformance benefits of curricula strategies in SSLMs can be derived by\nspecifying fine-grained language-specific curricula that precisely replicate\nlanguage acquisition theories.\n", "link": "http://arxiv.org/abs/2410.22886v2", "date": "2025-02-21", "relevancy": 2.5883, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.522}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.522}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5089}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Less%20is%20More%3A%20Pre-Training%20Cross-Lingual%20Small-Scale%20Language%20Models%0A%20%20with%20Cognitively-Plausible%20Curriculum%20Learning%20Strategies&body=Title%3A%20Less%20is%20More%3A%20Pre-Training%20Cross-Lingual%20Small-Scale%20Language%20Models%0A%20%20with%20Cognitively-Plausible%20Curriculum%20Learning%20Strategies%0AAuthor%3A%20Suchir%20Salhan%20and%20Richard%20Diehl%20Martinez%20and%20Z%C3%A9bulon%20Goriely%20and%20Paula%20Buttery%0AAbstract%3A%20%20%20Curriculum%20Learning%20has%20been%20a%20popular%20strategy%20to%20improve%20the%20cognitive%0Aplausibility%20of%20Small-Scale%20Language%20Models%20%28SSLMs%29%20in%20the%20BabyLM%20Challenge.%0AHowever%2C%20it%20has%20not%20led%20to%20considerable%20improvements%20over%20non-curriculum%0Amodels.%20We%20assess%20whether%20theoretical%20linguistic%20acquisition%20theories%20can%20be%0Aused%20to%20specify%20more%20fine-grained%20curriculum%20learning%20strategies%2C%20creating%0Aage-ordered%20corpora%20of%20Child-Directed%20Speech%20for%20four%20typologically%20distant%0Alanguage%20families%20to%20implement%20SSLMs%20and%20acquisition-inspired%20curricula%0Across-lingually.%20Comparing%20the%20success%20of%20three%20objective%20curricula%20%28Growing%2C%0AInwards%20and%20MMM%29%20that%20precisely%20replicate%20the%20predictions%20of%20acquisition%0Atheories%20on%20a%20standard%20SSLM%20architecture%2C%20we%20find%20fine-grained%0Aacquisition-inspired%20curricula%20can%20outperform%20non-curriculum%20baselines%20and%0Aperformance%20benefits%20of%20curricula%20strategies%20in%20SSLMs%20can%20be%20derived%20by%0Aspecifying%20fine-grained%20language-specific%20curricula%20that%20precisely%20replicate%0Alanguage%20acquisition%20theories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22886v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLess%2520is%2520More%253A%2520Pre-Training%2520Cross-Lingual%2520Small-Scale%2520Language%2520Models%250A%2520%2520with%2520Cognitively-Plausible%2520Curriculum%2520Learning%2520Strategies%26entry.906535625%3DSuchir%2520Salhan%2520and%2520Richard%2520Diehl%2520Martinez%2520and%2520Z%25C3%25A9bulon%2520Goriely%2520and%2520Paula%2520Buttery%26entry.1292438233%3D%2520%2520Curriculum%2520Learning%2520has%2520been%2520a%2520popular%2520strategy%2520to%2520improve%2520the%2520cognitive%250Aplausibility%2520of%2520Small-Scale%2520Language%2520Models%2520%2528SSLMs%2529%2520in%2520the%2520BabyLM%2520Challenge.%250AHowever%252C%2520it%2520has%2520not%2520led%2520to%2520considerable%2520improvements%2520over%2520non-curriculum%250Amodels.%2520We%2520assess%2520whether%2520theoretical%2520linguistic%2520acquisition%2520theories%2520can%2520be%250Aused%2520to%2520specify%2520more%2520fine-grained%2520curriculum%2520learning%2520strategies%252C%2520creating%250Aage-ordered%2520corpora%2520of%2520Child-Directed%2520Speech%2520for%2520four%2520typologically%2520distant%250Alanguage%2520families%2520to%2520implement%2520SSLMs%2520and%2520acquisition-inspired%2520curricula%250Across-lingually.%2520Comparing%2520the%2520success%2520of%2520three%2520objective%2520curricula%2520%2528Growing%252C%250AInwards%2520and%2520MMM%2529%2520that%2520precisely%2520replicate%2520the%2520predictions%2520of%2520acquisition%250Atheories%2520on%2520a%2520standard%2520SSLM%2520architecture%252C%2520we%2520find%2520fine-grained%250Aacquisition-inspired%2520curricula%2520can%2520outperform%2520non-curriculum%2520baselines%2520and%250Aperformance%2520benefits%2520of%2520curricula%2520strategies%2520in%2520SSLMs%2520can%2520be%2520derived%2520by%250Aspecifying%2520fine-grained%2520language-specific%2520curricula%2520that%2520precisely%2520replicate%250Alanguage%2520acquisition%2520theories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22886v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Less%20is%20More%3A%20Pre-Training%20Cross-Lingual%20Small-Scale%20Language%20Models%0A%20%20with%20Cognitively-Plausible%20Curriculum%20Learning%20Strategies&entry.906535625=Suchir%20Salhan%20and%20Richard%20Diehl%20Martinez%20and%20Z%C3%A9bulon%20Goriely%20and%20Paula%20Buttery&entry.1292438233=%20%20Curriculum%20Learning%20has%20been%20a%20popular%20strategy%20to%20improve%20the%20cognitive%0Aplausibility%20of%20Small-Scale%20Language%20Models%20%28SSLMs%29%20in%20the%20BabyLM%20Challenge.%0AHowever%2C%20it%20has%20not%20led%20to%20considerable%20improvements%20over%20non-curriculum%0Amodels.%20We%20assess%20whether%20theoretical%20linguistic%20acquisition%20theories%20can%20be%0Aused%20to%20specify%20more%20fine-grained%20curriculum%20learning%20strategies%2C%20creating%0Aage-ordered%20corpora%20of%20Child-Directed%20Speech%20for%20four%20typologically%20distant%0Alanguage%20families%20to%20implement%20SSLMs%20and%20acquisition-inspired%20curricula%0Across-lingually.%20Comparing%20the%20success%20of%20three%20objective%20curricula%20%28Growing%2C%0AInwards%20and%20MMM%29%20that%20precisely%20replicate%20the%20predictions%20of%20acquisition%0Atheories%20on%20a%20standard%20SSLM%20architecture%2C%20we%20find%20fine-grained%0Aacquisition-inspired%20curricula%20can%20outperform%20non-curriculum%20baselines%20and%0Aperformance%20benefits%20of%20curricula%20strategies%20in%20SSLMs%20can%20be%20derived%20by%0Aspecifying%20fine-grained%20language-specific%20curricula%20that%20precisely%20replicate%0Alanguage%20acquisition%20theories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22886v2&entry.124074799=Read"},
{"title": "Generative Video Diffusion for Unseen Novel Semantic Video Moment\n  Retrieval", "author": "Dezhao Luo and Shaogang Gong and Jiabo Huang and Hailin Jin and Yang Liu", "abstract": "  Video moment retrieval (VMR) aims to locate the most likely video moment(s)\ncorresponding to a text query in untrimmed videos. Training of existing methods\nis limited by the lack of diverse and generalisable VMR datasets, hindering\ntheir ability to generalise moment-text associations to queries containing\nnovel semantic concepts (unseen both visually and textually in a training\nsource domain). For model generalisation to novel semantics, existing methods\nrely heavily on assuming to have access to both video and text sentence pairs\nfrom a target domain in addition to the source domain pair-wise training data.\nThis is neither practical nor scalable. In this work, we introduce a more\ngeneralisable approach by assuming only text sentences describing new semantics\nare available in model training without having seen any videos from a target\ndomain. To that end, we propose a Fine-grained Video Editing framework, termed\nFVE, that explores generative video diffusion to facilitate fine-grained video\nediting from the seen source concepts to the unseen target sentences consisting\nof new concepts. This enables generative hypotheses of unseen video moments\ncorresponding to the novel concepts in the target domain. This fine-grained\ngenerative video diffusion retains the original video structure and subject\nspecifics from the source domain while introducing semantic distinctions of\nunseen novel vocabularies in the target domain. A critical challenge is how to\nenable this generative fine-grained diffusion process to be meaningful in\noptimising VMR, more than just synthesising visually pleasing videos. We solve\nthis problem by introducing a hybrid selection mechanism that integrates three\nquantitative metrics to selectively incorporate synthetic video moments (novel\nvideo hypotheses) as enlarged additions to the original source training data,\nwhilst minimising potential ...\n", "link": "http://arxiv.org/abs/2401.13329v3", "date": "2025-02-21", "relevancy": 2.5717, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6748}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.664}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Video%20Diffusion%20for%20Unseen%20Novel%20Semantic%20Video%20Moment%0A%20%20Retrieval&body=Title%3A%20Generative%20Video%20Diffusion%20for%20Unseen%20Novel%20Semantic%20Video%20Moment%0A%20%20Retrieval%0AAuthor%3A%20Dezhao%20Luo%20and%20Shaogang%20Gong%20and%20Jiabo%20Huang%20and%20Hailin%20Jin%20and%20Yang%20Liu%0AAbstract%3A%20%20%20Video%20moment%20retrieval%20%28VMR%29%20aims%20to%20locate%20the%20most%20likely%20video%20moment%28s%29%0Acorresponding%20to%20a%20text%20query%20in%20untrimmed%20videos.%20Training%20of%20existing%20methods%0Ais%20limited%20by%20the%20lack%20of%20diverse%20and%20generalisable%20VMR%20datasets%2C%20hindering%0Atheir%20ability%20to%20generalise%20moment-text%20associations%20to%20queries%20containing%0Anovel%20semantic%20concepts%20%28unseen%20both%20visually%20and%20textually%20in%20a%20training%0Asource%20domain%29.%20For%20model%20generalisation%20to%20novel%20semantics%2C%20existing%20methods%0Arely%20heavily%20on%20assuming%20to%20have%20access%20to%20both%20video%20and%20text%20sentence%20pairs%0Afrom%20a%20target%20domain%20in%20addition%20to%20the%20source%20domain%20pair-wise%20training%20data.%0AThis%20is%20neither%20practical%20nor%20scalable.%20In%20this%20work%2C%20we%20introduce%20a%20more%0Ageneralisable%20approach%20by%20assuming%20only%20text%20sentences%20describing%20new%20semantics%0Aare%20available%20in%20model%20training%20without%20having%20seen%20any%20videos%20from%20a%20target%0Adomain.%20To%20that%20end%2C%20we%20propose%20a%20Fine-grained%20Video%20Editing%20framework%2C%20termed%0AFVE%2C%20that%20explores%20generative%20video%20diffusion%20to%20facilitate%20fine-grained%20video%0Aediting%20from%20the%20seen%20source%20concepts%20to%20the%20unseen%20target%20sentences%20consisting%0Aof%20new%20concepts.%20This%20enables%20generative%20hypotheses%20of%20unseen%20video%20moments%0Acorresponding%20to%20the%20novel%20concepts%20in%20the%20target%20domain.%20This%20fine-grained%0Agenerative%20video%20diffusion%20retains%20the%20original%20video%20structure%20and%20subject%0Aspecifics%20from%20the%20source%20domain%20while%20introducing%20semantic%20distinctions%20of%0Aunseen%20novel%20vocabularies%20in%20the%20target%20domain.%20A%20critical%20challenge%20is%20how%20to%0Aenable%20this%20generative%20fine-grained%20diffusion%20process%20to%20be%20meaningful%20in%0Aoptimising%20VMR%2C%20more%20than%20just%20synthesising%20visually%20pleasing%20videos.%20We%20solve%0Athis%20problem%20by%20introducing%20a%20hybrid%20selection%20mechanism%20that%20integrates%20three%0Aquantitative%20metrics%20to%20selectively%20incorporate%20synthetic%20video%20moments%20%28novel%0Avideo%20hypotheses%29%20as%20enlarged%20additions%20to%20the%20original%20source%20training%20data%2C%0Awhilst%20minimising%20potential%20...%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.13329v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Video%2520Diffusion%2520for%2520Unseen%2520Novel%2520Semantic%2520Video%2520Moment%250A%2520%2520Retrieval%26entry.906535625%3DDezhao%2520Luo%2520and%2520Shaogang%2520Gong%2520and%2520Jiabo%2520Huang%2520and%2520Hailin%2520Jin%2520and%2520Yang%2520Liu%26entry.1292438233%3D%2520%2520Video%2520moment%2520retrieval%2520%2528VMR%2529%2520aims%2520to%2520locate%2520the%2520most%2520likely%2520video%2520moment%2528s%2529%250Acorresponding%2520to%2520a%2520text%2520query%2520in%2520untrimmed%2520videos.%2520Training%2520of%2520existing%2520methods%250Ais%2520limited%2520by%2520the%2520lack%2520of%2520diverse%2520and%2520generalisable%2520VMR%2520datasets%252C%2520hindering%250Atheir%2520ability%2520to%2520generalise%2520moment-text%2520associations%2520to%2520queries%2520containing%250Anovel%2520semantic%2520concepts%2520%2528unseen%2520both%2520visually%2520and%2520textually%2520in%2520a%2520training%250Asource%2520domain%2529.%2520For%2520model%2520generalisation%2520to%2520novel%2520semantics%252C%2520existing%2520methods%250Arely%2520heavily%2520on%2520assuming%2520to%2520have%2520access%2520to%2520both%2520video%2520and%2520text%2520sentence%2520pairs%250Afrom%2520a%2520target%2520domain%2520in%2520addition%2520to%2520the%2520source%2520domain%2520pair-wise%2520training%2520data.%250AThis%2520is%2520neither%2520practical%2520nor%2520scalable.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520more%250Ageneralisable%2520approach%2520by%2520assuming%2520only%2520text%2520sentences%2520describing%2520new%2520semantics%250Aare%2520available%2520in%2520model%2520training%2520without%2520having%2520seen%2520any%2520videos%2520from%2520a%2520target%250Adomain.%2520To%2520that%2520end%252C%2520we%2520propose%2520a%2520Fine-grained%2520Video%2520Editing%2520framework%252C%2520termed%250AFVE%252C%2520that%2520explores%2520generative%2520video%2520diffusion%2520to%2520facilitate%2520fine-grained%2520video%250Aediting%2520from%2520the%2520seen%2520source%2520concepts%2520to%2520the%2520unseen%2520target%2520sentences%2520consisting%250Aof%2520new%2520concepts.%2520This%2520enables%2520generative%2520hypotheses%2520of%2520unseen%2520video%2520moments%250Acorresponding%2520to%2520the%2520novel%2520concepts%2520in%2520the%2520target%2520domain.%2520This%2520fine-grained%250Agenerative%2520video%2520diffusion%2520retains%2520the%2520original%2520video%2520structure%2520and%2520subject%250Aspecifics%2520from%2520the%2520source%2520domain%2520while%2520introducing%2520semantic%2520distinctions%2520of%250Aunseen%2520novel%2520vocabularies%2520in%2520the%2520target%2520domain.%2520A%2520critical%2520challenge%2520is%2520how%2520to%250Aenable%2520this%2520generative%2520fine-grained%2520diffusion%2520process%2520to%2520be%2520meaningful%2520in%250Aoptimising%2520VMR%252C%2520more%2520than%2520just%2520synthesising%2520visually%2520pleasing%2520videos.%2520We%2520solve%250Athis%2520problem%2520by%2520introducing%2520a%2520hybrid%2520selection%2520mechanism%2520that%2520integrates%2520three%250Aquantitative%2520metrics%2520to%2520selectively%2520incorporate%2520synthetic%2520video%2520moments%2520%2528novel%250Avideo%2520hypotheses%2529%2520as%2520enlarged%2520additions%2520to%2520the%2520original%2520source%2520training%2520data%252C%250Awhilst%2520minimising%2520potential%2520...%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.13329v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Video%20Diffusion%20for%20Unseen%20Novel%20Semantic%20Video%20Moment%0A%20%20Retrieval&entry.906535625=Dezhao%20Luo%20and%20Shaogang%20Gong%20and%20Jiabo%20Huang%20and%20Hailin%20Jin%20and%20Yang%20Liu&entry.1292438233=%20%20Video%20moment%20retrieval%20%28VMR%29%20aims%20to%20locate%20the%20most%20likely%20video%20moment%28s%29%0Acorresponding%20to%20a%20text%20query%20in%20untrimmed%20videos.%20Training%20of%20existing%20methods%0Ais%20limited%20by%20the%20lack%20of%20diverse%20and%20generalisable%20VMR%20datasets%2C%20hindering%0Atheir%20ability%20to%20generalise%20moment-text%20associations%20to%20queries%20containing%0Anovel%20semantic%20concepts%20%28unseen%20both%20visually%20and%20textually%20in%20a%20training%0Asource%20domain%29.%20For%20model%20generalisation%20to%20novel%20semantics%2C%20existing%20methods%0Arely%20heavily%20on%20assuming%20to%20have%20access%20to%20both%20video%20and%20text%20sentence%20pairs%0Afrom%20a%20target%20domain%20in%20addition%20to%20the%20source%20domain%20pair-wise%20training%20data.%0AThis%20is%20neither%20practical%20nor%20scalable.%20In%20this%20work%2C%20we%20introduce%20a%20more%0Ageneralisable%20approach%20by%20assuming%20only%20text%20sentences%20describing%20new%20semantics%0Aare%20available%20in%20model%20training%20without%20having%20seen%20any%20videos%20from%20a%20target%0Adomain.%20To%20that%20end%2C%20we%20propose%20a%20Fine-grained%20Video%20Editing%20framework%2C%20termed%0AFVE%2C%20that%20explores%20generative%20video%20diffusion%20to%20facilitate%20fine-grained%20video%0Aediting%20from%20the%20seen%20source%20concepts%20to%20the%20unseen%20target%20sentences%20consisting%0Aof%20new%20concepts.%20This%20enables%20generative%20hypotheses%20of%20unseen%20video%20moments%0Acorresponding%20to%20the%20novel%20concepts%20in%20the%20target%20domain.%20This%20fine-grained%0Agenerative%20video%20diffusion%20retains%20the%20original%20video%20structure%20and%20subject%0Aspecifics%20from%20the%20source%20domain%20while%20introducing%20semantic%20distinctions%20of%0Aunseen%20novel%20vocabularies%20in%20the%20target%20domain.%20A%20critical%20challenge%20is%20how%20to%0Aenable%20this%20generative%20fine-grained%20diffusion%20process%20to%20be%20meaningful%20in%0Aoptimising%20VMR%2C%20more%20than%20just%20synthesising%20visually%20pleasing%20videos.%20We%20solve%0Athis%20problem%20by%20introducing%20a%20hybrid%20selection%20mechanism%20that%20integrates%20three%0Aquantitative%20metrics%20to%20selectively%20incorporate%20synthetic%20video%20moments%20%28novel%0Avideo%20hypotheses%29%20as%20enlarged%20additions%20to%20the%20original%20source%20training%20data%2C%0Awhilst%20minimising%20potential%20...%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.13329v3&entry.124074799=Read"},
{"title": "PDeepPP:A Deep learning framework with Pretrained Protein language for\n  peptide classification", "author": "Jixiu Zhai and Tianchi Lu and Haitian Zhong and Ziyang Xu and Yuhuan Liu and Xueying Wang and Dan Huang", "abstract": "  Protein post-translational modifications (PTMs) and bioactive peptides (BPs)\nplay critical roles in various biological processes and have significant\ntherapeutic potential. However, identifying PTM sites and bioactive peptides\nthrough experimental methods is often labor-intensive, costly, and\ntime-consuming. As a result, computational tools, particularly those based on\ndeep learning, have become effective solutions for predicting PTM sites and\npeptide bioactivity. Despite progress in this field, existing methods still\nstruggle with the complexity of protein sequences and the challenge of\nrequiring high-quality predictions across diverse datasets.\n  To address these issues, we propose a deep learning framework that integrates\npretrained protein language models with a neural network combining transformer\nand CNN for peptide classification. By leveraging the ability of pretrained\nmodels to capture complex relationships within protein sequences, combined with\nthe predictive power of parallel networks, our approach improves feature\nextraction while enhancing prediction accuracy.\n  This framework was applied to multiple tasks involving PTM site and bioactive\npeptide prediction, utilizing large-scale datasets to enhance the model's\nrobustness. In the comparison across 33 tasks, the model achieved\nstate-of-the-art (SOTA) performance in 25 of them, surpassing existing methods\nand demonstrating its versatility across different datasets. Our results\nsuggest that this approach provides a scalable and effective solution for\nlarge-scale peptide discovery and PTM analysis, paving the way for more\nefficient peptide classification and functional annotation.\n", "link": "http://arxiv.org/abs/2502.15610v1", "date": "2025-02-21", "relevancy": 2.5588, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5127}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5127}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PDeepPP%3AA%20Deep%20learning%20framework%20with%20Pretrained%20Protein%20language%20for%0A%20%20peptide%20classification&body=Title%3A%20PDeepPP%3AA%20Deep%20learning%20framework%20with%20Pretrained%20Protein%20language%20for%0A%20%20peptide%20classification%0AAuthor%3A%20Jixiu%20Zhai%20and%20Tianchi%20Lu%20and%20Haitian%20Zhong%20and%20Ziyang%20Xu%20and%20Yuhuan%20Liu%20and%20Xueying%20Wang%20and%20Dan%20Huang%0AAbstract%3A%20%20%20Protein%20post-translational%20modifications%20%28PTMs%29%20and%20bioactive%20peptides%20%28BPs%29%0Aplay%20critical%20roles%20in%20various%20biological%20processes%20and%20have%20significant%0Atherapeutic%20potential.%20However%2C%20identifying%20PTM%20sites%20and%20bioactive%20peptides%0Athrough%20experimental%20methods%20is%20often%20labor-intensive%2C%20costly%2C%20and%0Atime-consuming.%20As%20a%20result%2C%20computational%20tools%2C%20particularly%20those%20based%20on%0Adeep%20learning%2C%20have%20become%20effective%20solutions%20for%20predicting%20PTM%20sites%20and%0Apeptide%20bioactivity.%20Despite%20progress%20in%20this%20field%2C%20existing%20methods%20still%0Astruggle%20with%20the%20complexity%20of%20protein%20sequences%20and%20the%20challenge%20of%0Arequiring%20high-quality%20predictions%20across%20diverse%20datasets.%0A%20%20To%20address%20these%20issues%2C%20we%20propose%20a%20deep%20learning%20framework%20that%20integrates%0Apretrained%20protein%20language%20models%20with%20a%20neural%20network%20combining%20transformer%0Aand%20CNN%20for%20peptide%20classification.%20By%20leveraging%20the%20ability%20of%20pretrained%0Amodels%20to%20capture%20complex%20relationships%20within%20protein%20sequences%2C%20combined%20with%0Athe%20predictive%20power%20of%20parallel%20networks%2C%20our%20approach%20improves%20feature%0Aextraction%20while%20enhancing%20prediction%20accuracy.%0A%20%20This%20framework%20was%20applied%20to%20multiple%20tasks%20involving%20PTM%20site%20and%20bioactive%0Apeptide%20prediction%2C%20utilizing%20large-scale%20datasets%20to%20enhance%20the%20model%27s%0Arobustness.%20In%20the%20comparison%20across%2033%20tasks%2C%20the%20model%20achieved%0Astate-of-the-art%20%28SOTA%29%20performance%20in%2025%20of%20them%2C%20surpassing%20existing%20methods%0Aand%20demonstrating%20its%20versatility%20across%20different%20datasets.%20Our%20results%0Asuggest%20that%20this%20approach%20provides%20a%20scalable%20and%20effective%20solution%20for%0Alarge-scale%20peptide%20discovery%20and%20PTM%20analysis%2C%20paving%20the%20way%20for%20more%0Aefficient%20peptide%20classification%20and%20functional%20annotation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15610v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPDeepPP%253AA%2520Deep%2520learning%2520framework%2520with%2520Pretrained%2520Protein%2520language%2520for%250A%2520%2520peptide%2520classification%26entry.906535625%3DJixiu%2520Zhai%2520and%2520Tianchi%2520Lu%2520and%2520Haitian%2520Zhong%2520and%2520Ziyang%2520Xu%2520and%2520Yuhuan%2520Liu%2520and%2520Xueying%2520Wang%2520and%2520Dan%2520Huang%26entry.1292438233%3D%2520%2520Protein%2520post-translational%2520modifications%2520%2528PTMs%2529%2520and%2520bioactive%2520peptides%2520%2528BPs%2529%250Aplay%2520critical%2520roles%2520in%2520various%2520biological%2520processes%2520and%2520have%2520significant%250Atherapeutic%2520potential.%2520However%252C%2520identifying%2520PTM%2520sites%2520and%2520bioactive%2520peptides%250Athrough%2520experimental%2520methods%2520is%2520often%2520labor-intensive%252C%2520costly%252C%2520and%250Atime-consuming.%2520As%2520a%2520result%252C%2520computational%2520tools%252C%2520particularly%2520those%2520based%2520on%250Adeep%2520learning%252C%2520have%2520become%2520effective%2520solutions%2520for%2520predicting%2520PTM%2520sites%2520and%250Apeptide%2520bioactivity.%2520Despite%2520progress%2520in%2520this%2520field%252C%2520existing%2520methods%2520still%250Astruggle%2520with%2520the%2520complexity%2520of%2520protein%2520sequences%2520and%2520the%2520challenge%2520of%250Arequiring%2520high-quality%2520predictions%2520across%2520diverse%2520datasets.%250A%2520%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520deep%2520learning%2520framework%2520that%2520integrates%250Apretrained%2520protein%2520language%2520models%2520with%2520a%2520neural%2520network%2520combining%2520transformer%250Aand%2520CNN%2520for%2520peptide%2520classification.%2520By%2520leveraging%2520the%2520ability%2520of%2520pretrained%250Amodels%2520to%2520capture%2520complex%2520relationships%2520within%2520protein%2520sequences%252C%2520combined%2520with%250Athe%2520predictive%2520power%2520of%2520parallel%2520networks%252C%2520our%2520approach%2520improves%2520feature%250Aextraction%2520while%2520enhancing%2520prediction%2520accuracy.%250A%2520%2520This%2520framework%2520was%2520applied%2520to%2520multiple%2520tasks%2520involving%2520PTM%2520site%2520and%2520bioactive%250Apeptide%2520prediction%252C%2520utilizing%2520large-scale%2520datasets%2520to%2520enhance%2520the%2520model%2527s%250Arobustness.%2520In%2520the%2520comparison%2520across%252033%2520tasks%252C%2520the%2520model%2520achieved%250Astate-of-the-art%2520%2528SOTA%2529%2520performance%2520in%252025%2520of%2520them%252C%2520surpassing%2520existing%2520methods%250Aand%2520demonstrating%2520its%2520versatility%2520across%2520different%2520datasets.%2520Our%2520results%250Asuggest%2520that%2520this%2520approach%2520provides%2520a%2520scalable%2520and%2520effective%2520solution%2520for%250Alarge-scale%2520peptide%2520discovery%2520and%2520PTM%2520analysis%252C%2520paving%2520the%2520way%2520for%2520more%250Aefficient%2520peptide%2520classification%2520and%2520functional%2520annotation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15610v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PDeepPP%3AA%20Deep%20learning%20framework%20with%20Pretrained%20Protein%20language%20for%0A%20%20peptide%20classification&entry.906535625=Jixiu%20Zhai%20and%20Tianchi%20Lu%20and%20Haitian%20Zhong%20and%20Ziyang%20Xu%20and%20Yuhuan%20Liu%20and%20Xueying%20Wang%20and%20Dan%20Huang&entry.1292438233=%20%20Protein%20post-translational%20modifications%20%28PTMs%29%20and%20bioactive%20peptides%20%28BPs%29%0Aplay%20critical%20roles%20in%20various%20biological%20processes%20and%20have%20significant%0Atherapeutic%20potential.%20However%2C%20identifying%20PTM%20sites%20and%20bioactive%20peptides%0Athrough%20experimental%20methods%20is%20often%20labor-intensive%2C%20costly%2C%20and%0Atime-consuming.%20As%20a%20result%2C%20computational%20tools%2C%20particularly%20those%20based%20on%0Adeep%20learning%2C%20have%20become%20effective%20solutions%20for%20predicting%20PTM%20sites%20and%0Apeptide%20bioactivity.%20Despite%20progress%20in%20this%20field%2C%20existing%20methods%20still%0Astruggle%20with%20the%20complexity%20of%20protein%20sequences%20and%20the%20challenge%20of%0Arequiring%20high-quality%20predictions%20across%20diverse%20datasets.%0A%20%20To%20address%20these%20issues%2C%20we%20propose%20a%20deep%20learning%20framework%20that%20integrates%0Apretrained%20protein%20language%20models%20with%20a%20neural%20network%20combining%20transformer%0Aand%20CNN%20for%20peptide%20classification.%20By%20leveraging%20the%20ability%20of%20pretrained%0Amodels%20to%20capture%20complex%20relationships%20within%20protein%20sequences%2C%20combined%20with%0Athe%20predictive%20power%20of%20parallel%20networks%2C%20our%20approach%20improves%20feature%0Aextraction%20while%20enhancing%20prediction%20accuracy.%0A%20%20This%20framework%20was%20applied%20to%20multiple%20tasks%20involving%20PTM%20site%20and%20bioactive%0Apeptide%20prediction%2C%20utilizing%20large-scale%20datasets%20to%20enhance%20the%20model%27s%0Arobustness.%20In%20the%20comparison%20across%2033%20tasks%2C%20the%20model%20achieved%0Astate-of-the-art%20%28SOTA%29%20performance%20in%2025%20of%20them%2C%20surpassing%20existing%20methods%0Aand%20demonstrating%20its%20versatility%20across%20different%20datasets.%20Our%20results%0Asuggest%20that%20this%20approach%20provides%20a%20scalable%20and%20effective%20solution%20for%0Alarge-scale%20peptide%20discovery%20and%20PTM%20analysis%2C%20paving%20the%20way%20for%20more%0Aefficient%20peptide%20classification%20and%20functional%20annotation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15610v1&entry.124074799=Read"},
{"title": "Memory Helps, but Confabulation Misleads: Understanding Streaming Events\n  in Videos with MLLMs", "author": "Gengyuan Zhang and Mingcong Ding and Tong Liu and Yao Zhang and Volker Tresp", "abstract": "  Multimodal large language models (MLLMs) have demonstrated strong performance\nin understanding videos holistically, yet their ability to process streaming\nvideos-videos are treated as a sequence of visual events-remains underexplored.\nIntuitively, leveraging past events as memory can enrich contextual and\ntemporal understanding of the current event. In this paper, we show that\nleveraging memories as contexts helps MLLMs better understand video events.\nHowever, because such memories rely on predictions of preceding events, they\nmay contain misinformation, leading to confabulation and degraded performance.\nTo address this, we propose a confabulation-aware memory modification method\nthat mitigates confabulated memory for memory-enhanced event understanding.\n", "link": "http://arxiv.org/abs/2502.15457v1", "date": "2025-02-21", "relevancy": 2.5545, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5187}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5187}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memory%20Helps%2C%20but%20Confabulation%20Misleads%3A%20Understanding%20Streaming%20Events%0A%20%20in%20Videos%20with%20MLLMs&body=Title%3A%20Memory%20Helps%2C%20but%20Confabulation%20Misleads%3A%20Understanding%20Streaming%20Events%0A%20%20in%20Videos%20with%20MLLMs%0AAuthor%3A%20Gengyuan%20Zhang%20and%20Mingcong%20Ding%20and%20Tong%20Liu%20and%20Yao%20Zhang%20and%20Volker%20Tresp%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20demonstrated%20strong%20performance%0Ain%20understanding%20videos%20holistically%2C%20yet%20their%20ability%20to%20process%20streaming%0Avideos-videos%20are%20treated%20as%20a%20sequence%20of%20visual%20events-remains%20underexplored.%0AIntuitively%2C%20leveraging%20past%20events%20as%20memory%20can%20enrich%20contextual%20and%0Atemporal%20understanding%20of%20the%20current%20event.%20In%20this%20paper%2C%20we%20show%20that%0Aleveraging%20memories%20as%20contexts%20helps%20MLLMs%20better%20understand%20video%20events.%0AHowever%2C%20because%20such%20memories%20rely%20on%20predictions%20of%20preceding%20events%2C%20they%0Amay%20contain%20misinformation%2C%20leading%20to%20confabulation%20and%20degraded%20performance.%0ATo%20address%20this%2C%20we%20propose%20a%20confabulation-aware%20memory%20modification%20method%0Athat%20mitigates%20confabulated%20memory%20for%20memory-enhanced%20event%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15457v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemory%2520Helps%252C%2520but%2520Confabulation%2520Misleads%253A%2520Understanding%2520Streaming%2520Events%250A%2520%2520in%2520Videos%2520with%2520MLLMs%26entry.906535625%3DGengyuan%2520Zhang%2520and%2520Mingcong%2520Ding%2520and%2520Tong%2520Liu%2520and%2520Yao%2520Zhang%2520and%2520Volker%2520Tresp%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520strong%2520performance%250Ain%2520understanding%2520videos%2520holistically%252C%2520yet%2520their%2520ability%2520to%2520process%2520streaming%250Avideos-videos%2520are%2520treated%2520as%2520a%2520sequence%2520of%2520visual%2520events-remains%2520underexplored.%250AIntuitively%252C%2520leveraging%2520past%2520events%2520as%2520memory%2520can%2520enrich%2520contextual%2520and%250Atemporal%2520understanding%2520of%2520the%2520current%2520event.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%250Aleveraging%2520memories%2520as%2520contexts%2520helps%2520MLLMs%2520better%2520understand%2520video%2520events.%250AHowever%252C%2520because%2520such%2520memories%2520rely%2520on%2520predictions%2520of%2520preceding%2520events%252C%2520they%250Amay%2520contain%2520misinformation%252C%2520leading%2520to%2520confabulation%2520and%2520degraded%2520performance.%250ATo%2520address%2520this%252C%2520we%2520propose%2520a%2520confabulation-aware%2520memory%2520modification%2520method%250Athat%2520mitigates%2520confabulated%2520memory%2520for%2520memory-enhanced%2520event%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15457v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memory%20Helps%2C%20but%20Confabulation%20Misleads%3A%20Understanding%20Streaming%20Events%0A%20%20in%20Videos%20with%20MLLMs&entry.906535625=Gengyuan%20Zhang%20and%20Mingcong%20Ding%20and%20Tong%20Liu%20and%20Yao%20Zhang%20and%20Volker%20Tresp&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20demonstrated%20strong%20performance%0Ain%20understanding%20videos%20holistically%2C%20yet%20their%20ability%20to%20process%20streaming%0Avideos-videos%20are%20treated%20as%20a%20sequence%20of%20visual%20events-remains%20underexplored.%0AIntuitively%2C%20leveraging%20past%20events%20as%20memory%20can%20enrich%20contextual%20and%0Atemporal%20understanding%20of%20the%20current%20event.%20In%20this%20paper%2C%20we%20show%20that%0Aleveraging%20memories%20as%20contexts%20helps%20MLLMs%20better%20understand%20video%20events.%0AHowever%2C%20because%20such%20memories%20rely%20on%20predictions%20of%20preceding%20events%2C%20they%0Amay%20contain%20misinformation%2C%20leading%20to%20confabulation%20and%20degraded%20performance.%0ATo%20address%20this%2C%20we%20propose%20a%20confabulation-aware%20memory%20modification%20method%0Athat%20mitigates%20confabulated%20memory%20for%20memory-enhanced%20event%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15457v1&entry.124074799=Read"},
{"title": "The Relationship Between Reasoning and Performance in Large Language\n  Models -- o3 (mini) Thinks Harder, Not Longer", "author": "Marthe Ballon and Andres Algaba and Vincent Ginis", "abstract": "  Large language models have demonstrated remarkable progress in mathematical\nreasoning, leveraging chain-of-thought and test-time compute scaling. However,\nmany open questions remain regarding the interplay between reasoning token\nusage and accuracy gains. In particular, when comparing models across\ngenerations, it is unclear whether improved performance results from longer\nreasoning chains or more efficient reasoning. We systematically analyze\nchain-of-thought length across o1-mini and o3-mini variants on the Omni-MATH\nbenchmark, finding that o3-mini (m) achieves superior accuracy without\nrequiring longer reasoning chains than o1-mini. Moreover, we show that accuracy\ngenerally declines as reasoning chains grow across all models and compute\nsettings, even when controlling for difficulty of the questions. This accuracy\ndrop is significantly smaller in more proficient models, suggesting that new\ngenerations of reasoning models use test-time compute more effectively.\nFinally, we highlight that while o3-mini (h) achieves a marginal accuracy gain\nover o3-mini (m), it does so by allocating substantially more reasoning tokens\nacross all problems, even the ones that o3-mini (m) can already solve. These\nfindings provide new insights into the relationship between model capability\nand reasoning length, with implications for efficiency, scaling, and evaluation\nmethodologies.\n", "link": "http://arxiv.org/abs/2502.15631v1", "date": "2025-02-21", "relevancy": 2.55, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5431}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5431}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Relationship%20Between%20Reasoning%20and%20Performance%20in%20Large%20Language%0A%20%20Models%20--%20o3%20%28mini%29%20Thinks%20Harder%2C%20Not%20Longer&body=Title%3A%20The%20Relationship%20Between%20Reasoning%20and%20Performance%20in%20Large%20Language%0A%20%20Models%20--%20o3%20%28mini%29%20Thinks%20Harder%2C%20Not%20Longer%0AAuthor%3A%20Marthe%20Ballon%20and%20Andres%20Algaba%20and%20Vincent%20Ginis%0AAbstract%3A%20%20%20Large%20language%20models%20have%20demonstrated%20remarkable%20progress%20in%20mathematical%0Areasoning%2C%20leveraging%20chain-of-thought%20and%20test-time%20compute%20scaling.%20However%2C%0Amany%20open%20questions%20remain%20regarding%20the%20interplay%20between%20reasoning%20token%0Ausage%20and%20accuracy%20gains.%20In%20particular%2C%20when%20comparing%20models%20across%0Agenerations%2C%20it%20is%20unclear%20whether%20improved%20performance%20results%20from%20longer%0Areasoning%20chains%20or%20more%20efficient%20reasoning.%20We%20systematically%20analyze%0Achain-of-thought%20length%20across%20o1-mini%20and%20o3-mini%20variants%20on%20the%20Omni-MATH%0Abenchmark%2C%20finding%20that%20o3-mini%20%28m%29%20achieves%20superior%20accuracy%20without%0Arequiring%20longer%20reasoning%20chains%20than%20o1-mini.%20Moreover%2C%20we%20show%20that%20accuracy%0Agenerally%20declines%20as%20reasoning%20chains%20grow%20across%20all%20models%20and%20compute%0Asettings%2C%20even%20when%20controlling%20for%20difficulty%20of%20the%20questions.%20This%20accuracy%0Adrop%20is%20significantly%20smaller%20in%20more%20proficient%20models%2C%20suggesting%20that%20new%0Agenerations%20of%20reasoning%20models%20use%20test-time%20compute%20more%20effectively.%0AFinally%2C%20we%20highlight%20that%20while%20o3-mini%20%28h%29%20achieves%20a%20marginal%20accuracy%20gain%0Aover%20o3-mini%20%28m%29%2C%20it%20does%20so%20by%20allocating%20substantially%20more%20reasoning%20tokens%0Aacross%20all%20problems%2C%20even%20the%20ones%20that%20o3-mini%20%28m%29%20can%20already%20solve.%20These%0Afindings%20provide%20new%20insights%20into%20the%20relationship%20between%20model%20capability%0Aand%20reasoning%20length%2C%20with%20implications%20for%20efficiency%2C%20scaling%2C%20and%20evaluation%0Amethodologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15631v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Relationship%2520Between%2520Reasoning%2520and%2520Performance%2520in%2520Large%2520Language%250A%2520%2520Models%2520--%2520o3%2520%2528mini%2529%2520Thinks%2520Harder%252C%2520Not%2520Longer%26entry.906535625%3DMarthe%2520Ballon%2520and%2520Andres%2520Algaba%2520and%2520Vincent%2520Ginis%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520have%2520demonstrated%2520remarkable%2520progress%2520in%2520mathematical%250Areasoning%252C%2520leveraging%2520chain-of-thought%2520and%2520test-time%2520compute%2520scaling.%2520However%252C%250Amany%2520open%2520questions%2520remain%2520regarding%2520the%2520interplay%2520between%2520reasoning%2520token%250Ausage%2520and%2520accuracy%2520gains.%2520In%2520particular%252C%2520when%2520comparing%2520models%2520across%250Agenerations%252C%2520it%2520is%2520unclear%2520whether%2520improved%2520performance%2520results%2520from%2520longer%250Areasoning%2520chains%2520or%2520more%2520efficient%2520reasoning.%2520We%2520systematically%2520analyze%250Achain-of-thought%2520length%2520across%2520o1-mini%2520and%2520o3-mini%2520variants%2520on%2520the%2520Omni-MATH%250Abenchmark%252C%2520finding%2520that%2520o3-mini%2520%2528m%2529%2520achieves%2520superior%2520accuracy%2520without%250Arequiring%2520longer%2520reasoning%2520chains%2520than%2520o1-mini.%2520Moreover%252C%2520we%2520show%2520that%2520accuracy%250Agenerally%2520declines%2520as%2520reasoning%2520chains%2520grow%2520across%2520all%2520models%2520and%2520compute%250Asettings%252C%2520even%2520when%2520controlling%2520for%2520difficulty%2520of%2520the%2520questions.%2520This%2520accuracy%250Adrop%2520is%2520significantly%2520smaller%2520in%2520more%2520proficient%2520models%252C%2520suggesting%2520that%2520new%250Agenerations%2520of%2520reasoning%2520models%2520use%2520test-time%2520compute%2520more%2520effectively.%250AFinally%252C%2520we%2520highlight%2520that%2520while%2520o3-mini%2520%2528h%2529%2520achieves%2520a%2520marginal%2520accuracy%2520gain%250Aover%2520o3-mini%2520%2528m%2529%252C%2520it%2520does%2520so%2520by%2520allocating%2520substantially%2520more%2520reasoning%2520tokens%250Aacross%2520all%2520problems%252C%2520even%2520the%2520ones%2520that%2520o3-mini%2520%2528m%2529%2520can%2520already%2520solve.%2520These%250Afindings%2520provide%2520new%2520insights%2520into%2520the%2520relationship%2520between%2520model%2520capability%250Aand%2520reasoning%2520length%252C%2520with%2520implications%2520for%2520efficiency%252C%2520scaling%252C%2520and%2520evaluation%250Amethodologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15631v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Relationship%20Between%20Reasoning%20and%20Performance%20in%20Large%20Language%0A%20%20Models%20--%20o3%20%28mini%29%20Thinks%20Harder%2C%20Not%20Longer&entry.906535625=Marthe%20Ballon%20and%20Andres%20Algaba%20and%20Vincent%20Ginis&entry.1292438233=%20%20Large%20language%20models%20have%20demonstrated%20remarkable%20progress%20in%20mathematical%0Areasoning%2C%20leveraging%20chain-of-thought%20and%20test-time%20compute%20scaling.%20However%2C%0Amany%20open%20questions%20remain%20regarding%20the%20interplay%20between%20reasoning%20token%0Ausage%20and%20accuracy%20gains.%20In%20particular%2C%20when%20comparing%20models%20across%0Agenerations%2C%20it%20is%20unclear%20whether%20improved%20performance%20results%20from%20longer%0Areasoning%20chains%20or%20more%20efficient%20reasoning.%20We%20systematically%20analyze%0Achain-of-thought%20length%20across%20o1-mini%20and%20o3-mini%20variants%20on%20the%20Omni-MATH%0Abenchmark%2C%20finding%20that%20o3-mini%20%28m%29%20achieves%20superior%20accuracy%20without%0Arequiring%20longer%20reasoning%20chains%20than%20o1-mini.%20Moreover%2C%20we%20show%20that%20accuracy%0Agenerally%20declines%20as%20reasoning%20chains%20grow%20across%20all%20models%20and%20compute%0Asettings%2C%20even%20when%20controlling%20for%20difficulty%20of%20the%20questions.%20This%20accuracy%0Adrop%20is%20significantly%20smaller%20in%20more%20proficient%20models%2C%20suggesting%20that%20new%0Agenerations%20of%20reasoning%20models%20use%20test-time%20compute%20more%20effectively.%0AFinally%2C%20we%20highlight%20that%20while%20o3-mini%20%28h%29%20achieves%20a%20marginal%20accuracy%20gain%0Aover%20o3-mini%20%28m%29%2C%20it%20does%20so%20by%20allocating%20substantially%20more%20reasoning%20tokens%0Aacross%20all%20problems%2C%20even%20the%20ones%20that%20o3-mini%20%28m%29%20can%20already%20solve.%20These%0Afindings%20provide%20new%20insights%20into%20the%20relationship%20between%20model%20capability%0Aand%20reasoning%20length%2C%20with%20implications%20for%20efficiency%2C%20scaling%2C%20and%20evaluation%0Amethodologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15631v1&entry.124074799=Read"},
{"title": "AutoTandemML: Active Learning Enhanced Tandem Neural Networks for\n  Inverse Design Problems", "author": "Luka Grbcic and Juliane M\u00fcller and Wibe Albert de Jong", "abstract": "  Inverse design in science and engineering involves determining optimal design\nparameters that achieve desired performance outcomes, a process often hindered\nby the complexity and high dimensionality of design spaces, leading to\nsignificant computational costs. To tackle this challenge, we propose a novel\nhybrid approach that combines active learning with Tandem Neural Networks to\nenhance the efficiency and effectiveness of solving inverse design problems.\nActive learning allows to selectively sample the most informative data points,\nreducing the required dataset size without compromising accuracy. We\ninvestigate this approach using three benchmark problems: airfoil inverse\ndesign, photonic surface inverse design, and scalar boundary condition\nreconstruction in diffusion partial differential equations. We demonstrate that\nintegrating active learning with Tandem Neural Networks outperforms standard\napproaches across the benchmark suite, achieving better accuracy with fewer\ntraining samples.\n", "link": "http://arxiv.org/abs/2502.15643v1", "date": "2025-02-21", "relevancy": 2.5484, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5243}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5064}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoTandemML%3A%20Active%20Learning%20Enhanced%20Tandem%20Neural%20Networks%20for%0A%20%20Inverse%20Design%20Problems&body=Title%3A%20AutoTandemML%3A%20Active%20Learning%20Enhanced%20Tandem%20Neural%20Networks%20for%0A%20%20Inverse%20Design%20Problems%0AAuthor%3A%20Luka%20Grbcic%20and%20Juliane%20M%C3%BCller%20and%20Wibe%20Albert%20de%20Jong%0AAbstract%3A%20%20%20Inverse%20design%20in%20science%20and%20engineering%20involves%20determining%20optimal%20design%0Aparameters%20that%20achieve%20desired%20performance%20outcomes%2C%20a%20process%20often%20hindered%0Aby%20the%20complexity%20and%20high%20dimensionality%20of%20design%20spaces%2C%20leading%20to%0Asignificant%20computational%20costs.%20To%20tackle%20this%20challenge%2C%20we%20propose%20a%20novel%0Ahybrid%20approach%20that%20combines%20active%20learning%20with%20Tandem%20Neural%20Networks%20to%0Aenhance%20the%20efficiency%20and%20effectiveness%20of%20solving%20inverse%20design%20problems.%0AActive%20learning%20allows%20to%20selectively%20sample%20the%20most%20informative%20data%20points%2C%0Areducing%20the%20required%20dataset%20size%20without%20compromising%20accuracy.%20We%0Ainvestigate%20this%20approach%20using%20three%20benchmark%20problems%3A%20airfoil%20inverse%0Adesign%2C%20photonic%20surface%20inverse%20design%2C%20and%20scalar%20boundary%20condition%0Areconstruction%20in%20diffusion%20partial%20differential%20equations.%20We%20demonstrate%20that%0Aintegrating%20active%20learning%20with%20Tandem%20Neural%20Networks%20outperforms%20standard%0Aapproaches%20across%20the%20benchmark%20suite%2C%20achieving%20better%20accuracy%20with%20fewer%0Atraining%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoTandemML%253A%2520Active%2520Learning%2520Enhanced%2520Tandem%2520Neural%2520Networks%2520for%250A%2520%2520Inverse%2520Design%2520Problems%26entry.906535625%3DLuka%2520Grbcic%2520and%2520Juliane%2520M%25C3%25BCller%2520and%2520Wibe%2520Albert%2520de%2520Jong%26entry.1292438233%3D%2520%2520Inverse%2520design%2520in%2520science%2520and%2520engineering%2520involves%2520determining%2520optimal%2520design%250Aparameters%2520that%2520achieve%2520desired%2520performance%2520outcomes%252C%2520a%2520process%2520often%2520hindered%250Aby%2520the%2520complexity%2520and%2520high%2520dimensionality%2520of%2520design%2520spaces%252C%2520leading%2520to%250Asignificant%2520computational%2520costs.%2520To%2520tackle%2520this%2520challenge%252C%2520we%2520propose%2520a%2520novel%250Ahybrid%2520approach%2520that%2520combines%2520active%2520learning%2520with%2520Tandem%2520Neural%2520Networks%2520to%250Aenhance%2520the%2520efficiency%2520and%2520effectiveness%2520of%2520solving%2520inverse%2520design%2520problems.%250AActive%2520learning%2520allows%2520to%2520selectively%2520sample%2520the%2520most%2520informative%2520data%2520points%252C%250Areducing%2520the%2520required%2520dataset%2520size%2520without%2520compromising%2520accuracy.%2520We%250Ainvestigate%2520this%2520approach%2520using%2520three%2520benchmark%2520problems%253A%2520airfoil%2520inverse%250Adesign%252C%2520photonic%2520surface%2520inverse%2520design%252C%2520and%2520scalar%2520boundary%2520condition%250Areconstruction%2520in%2520diffusion%2520partial%2520differential%2520equations.%2520We%2520demonstrate%2520that%250Aintegrating%2520active%2520learning%2520with%2520Tandem%2520Neural%2520Networks%2520outperforms%2520standard%250Aapproaches%2520across%2520the%2520benchmark%2520suite%252C%2520achieving%2520better%2520accuracy%2520with%2520fewer%250Atraining%2520samples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoTandemML%3A%20Active%20Learning%20Enhanced%20Tandem%20Neural%20Networks%20for%0A%20%20Inverse%20Design%20Problems&entry.906535625=Luka%20Grbcic%20and%20Juliane%20M%C3%BCller%20and%20Wibe%20Albert%20de%20Jong&entry.1292438233=%20%20Inverse%20design%20in%20science%20and%20engineering%20involves%20determining%20optimal%20design%0Aparameters%20that%20achieve%20desired%20performance%20outcomes%2C%20a%20process%20often%20hindered%0Aby%20the%20complexity%20and%20high%20dimensionality%20of%20design%20spaces%2C%20leading%20to%0Asignificant%20computational%20costs.%20To%20tackle%20this%20challenge%2C%20we%20propose%20a%20novel%0Ahybrid%20approach%20that%20combines%20active%20learning%20with%20Tandem%20Neural%20Networks%20to%0Aenhance%20the%20efficiency%20and%20effectiveness%20of%20solving%20inverse%20design%20problems.%0AActive%20learning%20allows%20to%20selectively%20sample%20the%20most%20informative%20data%20points%2C%0Areducing%20the%20required%20dataset%20size%20without%20compromising%20accuracy.%20We%0Ainvestigate%20this%20approach%20using%20three%20benchmark%20problems%3A%20airfoil%20inverse%0Adesign%2C%20photonic%20surface%20inverse%20design%2C%20and%20scalar%20boundary%20condition%0Areconstruction%20in%20diffusion%20partial%20differential%20equations.%20We%20demonstrate%20that%0Aintegrating%20active%20learning%20with%20Tandem%20Neural%20Networks%20outperforms%20standard%0Aapproaches%20across%20the%20benchmark%20suite%2C%20achieving%20better%20accuracy%20with%20fewer%0Atraining%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15643v1&entry.124074799=Read"},
{"title": "Aligning Task- and Reconstruction-Oriented Communications for Edge\n  Intelligence", "author": "Yufeng Diao and Yichi Zhang and Changyang She and Philip Guodong Zhao and Emma Liying Li", "abstract": "  Existing communication systems aim to reconstruct the information at the\nreceiver side, and are known as reconstruction-oriented communications. This\napproach often falls short in meeting the real-time, task-specific demands of\nmodern AI-driven applications such as autonomous driving and semantic\nsegmentation. As a new design principle, task-oriented communications have been\ndeveloped. However, it typically requires joint optimization of encoder,\ndecoder, and modified inference neural networks, resulting in extensive\ncross-system redesigns and compatibility issues. This paper proposes a novel\ncommunication framework that aligns reconstruction-oriented and task-oriented\ncommunications for edge intelligence. The idea is to extend the Information\nBottleneck (IB) theory to optimize data transmission by minimizing\ntask-relevant loss function, while maintaining the structure of the original\ndata by an information reshaper. Such an approach integrates task-oriented\ncommunications with reconstruction-oriented communications, where a variational\napproach is designed to handle the intractability of mutual information in\nhigh-dimensional neural network features. We also introduce a joint\nsource-channel coding (JSCC) modulation scheme compatible with classical\nmodulation techniques, enabling the deployment of AI technologies within\nexisting digital infrastructures. The proposed framework is particularly\neffective in edge-based autonomous driving scenarios. Our evaluation in the Car\nLearning to Act (CARLA) simulator demonstrates that the proposed framework\nsignificantly reduces bits per service by 99.19% compared to existing methods,\nsuch as JPEG, JPEG2000, and BPG, without compromising the effectiveness of task\nexecution.\n", "link": "http://arxiv.org/abs/2502.15472v1", "date": "2025-02-21", "relevancy": 2.5198, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5056}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5056}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aligning%20Task-%20and%20Reconstruction-Oriented%20Communications%20for%20Edge%0A%20%20Intelligence&body=Title%3A%20Aligning%20Task-%20and%20Reconstruction-Oriented%20Communications%20for%20Edge%0A%20%20Intelligence%0AAuthor%3A%20Yufeng%20Diao%20and%20Yichi%20Zhang%20and%20Changyang%20She%20and%20Philip%20Guodong%20Zhao%20and%20Emma%20Liying%20Li%0AAbstract%3A%20%20%20Existing%20communication%20systems%20aim%20to%20reconstruct%20the%20information%20at%20the%0Areceiver%20side%2C%20and%20are%20known%20as%20reconstruction-oriented%20communications.%20This%0Aapproach%20often%20falls%20short%20in%20meeting%20the%20real-time%2C%20task-specific%20demands%20of%0Amodern%20AI-driven%20applications%20such%20as%20autonomous%20driving%20and%20semantic%0Asegmentation.%20As%20a%20new%20design%20principle%2C%20task-oriented%20communications%20have%20been%0Adeveloped.%20However%2C%20it%20typically%20requires%20joint%20optimization%20of%20encoder%2C%0Adecoder%2C%20and%20modified%20inference%20neural%20networks%2C%20resulting%20in%20extensive%0Across-system%20redesigns%20and%20compatibility%20issues.%20This%20paper%20proposes%20a%20novel%0Acommunication%20framework%20that%20aligns%20reconstruction-oriented%20and%20task-oriented%0Acommunications%20for%20edge%20intelligence.%20The%20idea%20is%20to%20extend%20the%20Information%0ABottleneck%20%28IB%29%20theory%20to%20optimize%20data%20transmission%20by%20minimizing%0Atask-relevant%20loss%20function%2C%20while%20maintaining%20the%20structure%20of%20the%20original%0Adata%20by%20an%20information%20reshaper.%20Such%20an%20approach%20integrates%20task-oriented%0Acommunications%20with%20reconstruction-oriented%20communications%2C%20where%20a%20variational%0Aapproach%20is%20designed%20to%20handle%20the%20intractability%20of%20mutual%20information%20in%0Ahigh-dimensional%20neural%20network%20features.%20We%20also%20introduce%20a%20joint%0Asource-channel%20coding%20%28JSCC%29%20modulation%20scheme%20compatible%20with%20classical%0Amodulation%20techniques%2C%20enabling%20the%20deployment%20of%20AI%20technologies%20within%0Aexisting%20digital%20infrastructures.%20The%20proposed%20framework%20is%20particularly%0Aeffective%20in%20edge-based%20autonomous%20driving%20scenarios.%20Our%20evaluation%20in%20the%20Car%0ALearning%20to%20Act%20%28CARLA%29%20simulator%20demonstrates%20that%20the%20proposed%20framework%0Asignificantly%20reduces%20bits%20per%20service%20by%2099.19%25%20compared%20to%20existing%20methods%2C%0Asuch%20as%20JPEG%2C%20JPEG2000%2C%20and%20BPG%2C%20without%20compromising%20the%20effectiveness%20of%20task%0Aexecution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15472v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAligning%2520Task-%2520and%2520Reconstruction-Oriented%2520Communications%2520for%2520Edge%250A%2520%2520Intelligence%26entry.906535625%3DYufeng%2520Diao%2520and%2520Yichi%2520Zhang%2520and%2520Changyang%2520She%2520and%2520Philip%2520Guodong%2520Zhao%2520and%2520Emma%2520Liying%2520Li%26entry.1292438233%3D%2520%2520Existing%2520communication%2520systems%2520aim%2520to%2520reconstruct%2520the%2520information%2520at%2520the%250Areceiver%2520side%252C%2520and%2520are%2520known%2520as%2520reconstruction-oriented%2520communications.%2520This%250Aapproach%2520often%2520falls%2520short%2520in%2520meeting%2520the%2520real-time%252C%2520task-specific%2520demands%2520of%250Amodern%2520AI-driven%2520applications%2520such%2520as%2520autonomous%2520driving%2520and%2520semantic%250Asegmentation.%2520As%2520a%2520new%2520design%2520principle%252C%2520task-oriented%2520communications%2520have%2520been%250Adeveloped.%2520However%252C%2520it%2520typically%2520requires%2520joint%2520optimization%2520of%2520encoder%252C%250Adecoder%252C%2520and%2520modified%2520inference%2520neural%2520networks%252C%2520resulting%2520in%2520extensive%250Across-system%2520redesigns%2520and%2520compatibility%2520issues.%2520This%2520paper%2520proposes%2520a%2520novel%250Acommunication%2520framework%2520that%2520aligns%2520reconstruction-oriented%2520and%2520task-oriented%250Acommunications%2520for%2520edge%2520intelligence.%2520The%2520idea%2520is%2520to%2520extend%2520the%2520Information%250ABottleneck%2520%2528IB%2529%2520theory%2520to%2520optimize%2520data%2520transmission%2520by%2520minimizing%250Atask-relevant%2520loss%2520function%252C%2520while%2520maintaining%2520the%2520structure%2520of%2520the%2520original%250Adata%2520by%2520an%2520information%2520reshaper.%2520Such%2520an%2520approach%2520integrates%2520task-oriented%250Acommunications%2520with%2520reconstruction-oriented%2520communications%252C%2520where%2520a%2520variational%250Aapproach%2520is%2520designed%2520to%2520handle%2520the%2520intractability%2520of%2520mutual%2520information%2520in%250Ahigh-dimensional%2520neural%2520network%2520features.%2520We%2520also%2520introduce%2520a%2520joint%250Asource-channel%2520coding%2520%2528JSCC%2529%2520modulation%2520scheme%2520compatible%2520with%2520classical%250Amodulation%2520techniques%252C%2520enabling%2520the%2520deployment%2520of%2520AI%2520technologies%2520within%250Aexisting%2520digital%2520infrastructures.%2520The%2520proposed%2520framework%2520is%2520particularly%250Aeffective%2520in%2520edge-based%2520autonomous%2520driving%2520scenarios.%2520Our%2520evaluation%2520in%2520the%2520Car%250ALearning%2520to%2520Act%2520%2528CARLA%2529%2520simulator%2520demonstrates%2520that%2520the%2520proposed%2520framework%250Asignificantly%2520reduces%2520bits%2520per%2520service%2520by%252099.19%2525%2520compared%2520to%2520existing%2520methods%252C%250Asuch%2520as%2520JPEG%252C%2520JPEG2000%252C%2520and%2520BPG%252C%2520without%2520compromising%2520the%2520effectiveness%2520of%2520task%250Aexecution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15472v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligning%20Task-%20and%20Reconstruction-Oriented%20Communications%20for%20Edge%0A%20%20Intelligence&entry.906535625=Yufeng%20Diao%20and%20Yichi%20Zhang%20and%20Changyang%20She%20and%20Philip%20Guodong%20Zhao%20and%20Emma%20Liying%20Li&entry.1292438233=%20%20Existing%20communication%20systems%20aim%20to%20reconstruct%20the%20information%20at%20the%0Areceiver%20side%2C%20and%20are%20known%20as%20reconstruction-oriented%20communications.%20This%0Aapproach%20often%20falls%20short%20in%20meeting%20the%20real-time%2C%20task-specific%20demands%20of%0Amodern%20AI-driven%20applications%20such%20as%20autonomous%20driving%20and%20semantic%0Asegmentation.%20As%20a%20new%20design%20principle%2C%20task-oriented%20communications%20have%20been%0Adeveloped.%20However%2C%20it%20typically%20requires%20joint%20optimization%20of%20encoder%2C%0Adecoder%2C%20and%20modified%20inference%20neural%20networks%2C%20resulting%20in%20extensive%0Across-system%20redesigns%20and%20compatibility%20issues.%20This%20paper%20proposes%20a%20novel%0Acommunication%20framework%20that%20aligns%20reconstruction-oriented%20and%20task-oriented%0Acommunications%20for%20edge%20intelligence.%20The%20idea%20is%20to%20extend%20the%20Information%0ABottleneck%20%28IB%29%20theory%20to%20optimize%20data%20transmission%20by%20minimizing%0Atask-relevant%20loss%20function%2C%20while%20maintaining%20the%20structure%20of%20the%20original%0Adata%20by%20an%20information%20reshaper.%20Such%20an%20approach%20integrates%20task-oriented%0Acommunications%20with%20reconstruction-oriented%20communications%2C%20where%20a%20variational%0Aapproach%20is%20designed%20to%20handle%20the%20intractability%20of%20mutual%20information%20in%0Ahigh-dimensional%20neural%20network%20features.%20We%20also%20introduce%20a%20joint%0Asource-channel%20coding%20%28JSCC%29%20modulation%20scheme%20compatible%20with%20classical%0Amodulation%20techniques%2C%20enabling%20the%20deployment%20of%20AI%20technologies%20within%0Aexisting%20digital%20infrastructures.%20The%20proposed%20framework%20is%20particularly%0Aeffective%20in%20edge-based%20autonomous%20driving%20scenarios.%20Our%20evaluation%20in%20the%20Car%0ALearning%20to%20Act%20%28CARLA%29%20simulator%20demonstrates%20that%20the%20proposed%20framework%0Asignificantly%20reduces%20bits%20per%20service%20by%2099.19%25%20compared%20to%20existing%20methods%2C%0Asuch%20as%20JPEG%2C%20JPEG2000%2C%20and%20BPG%2C%20without%20compromising%20the%20effectiveness%20of%20task%0Aexecution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15472v1&entry.124074799=Read"},
{"title": "Generalizing From Short to Long: Effective Data Synthesis for\n  Long-Context Instruction Tuning", "author": "Wenhao Zhu and Pinzhen Chen and Hanxu Hu and Shujian Huang and Fei Yuan and Jiajun Chen and Alexandra Birch", "abstract": "  Long-context modelling for large language models (LLMs) has been a key area\nof recent research because many real world use cases require reasoning over\nlonger inputs such as documents. The focus of research into modelling long\ncontext has been on how to model position and there has been little\ninvestigation into other important aspects of language modelling such as\ninstruction tuning. Long context training examples are challenging and\nexpensive to create and use. In this paper, we investigate how to design\ninstruction data for the post-training phase of a long context pre-trained\nmodel: how much and what type of context is needed for optimal and efficient\npost-training. Our controlled study reveals that models instruction-tuned on\nshort contexts can effectively generalize to longer ones, while also\nidentifying other critical factors such as instruction difficulty and context\ncomposition. Based on these findings, we propose context synthesis, a novel\ndata synthesis framework that leverages off-the-shelf LLMs to generate extended\nbackground contexts for high-quality instruction-answer pairs. Experiment\nresults on the document-level benchmark (LongBench) demonstrate that our\nproposed approach outperforms previous instruction synthesis approaches and\ncomes close to the performance of human-annotated long-context instruction\ndata. The project will be available at:\nhttps://github.com/NJUNLP/context-synthesis.\n", "link": "http://arxiv.org/abs/2502.15592v1", "date": "2025-02-21", "relevancy": 2.4997, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.513}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.513}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalizing%20From%20Short%20to%20Long%3A%20Effective%20Data%20Synthesis%20for%0A%20%20Long-Context%20Instruction%20Tuning&body=Title%3A%20Generalizing%20From%20Short%20to%20Long%3A%20Effective%20Data%20Synthesis%20for%0A%20%20Long-Context%20Instruction%20Tuning%0AAuthor%3A%20Wenhao%20Zhu%20and%20Pinzhen%20Chen%20and%20Hanxu%20Hu%20and%20Shujian%20Huang%20and%20Fei%20Yuan%20and%20Jiajun%20Chen%20and%20Alexandra%20Birch%0AAbstract%3A%20%20%20Long-context%20modelling%20for%20large%20language%20models%20%28LLMs%29%20has%20been%20a%20key%20area%0Aof%20recent%20research%20because%20many%20real%20world%20use%20cases%20require%20reasoning%20over%0Alonger%20inputs%20such%20as%20documents.%20The%20focus%20of%20research%20into%20modelling%20long%0Acontext%20has%20been%20on%20how%20to%20model%20position%20and%20there%20has%20been%20little%0Ainvestigation%20into%20other%20important%20aspects%20of%20language%20modelling%20such%20as%0Ainstruction%20tuning.%20Long%20context%20training%20examples%20are%20challenging%20and%0Aexpensive%20to%20create%20and%20use.%20In%20this%20paper%2C%20we%20investigate%20how%20to%20design%0Ainstruction%20data%20for%20the%20post-training%20phase%20of%20a%20long%20context%20pre-trained%0Amodel%3A%20how%20much%20and%20what%20type%20of%20context%20is%20needed%20for%20optimal%20and%20efficient%0Apost-training.%20Our%20controlled%20study%20reveals%20that%20models%20instruction-tuned%20on%0Ashort%20contexts%20can%20effectively%20generalize%20to%20longer%20ones%2C%20while%20also%0Aidentifying%20other%20critical%20factors%20such%20as%20instruction%20difficulty%20and%20context%0Acomposition.%20Based%20on%20these%20findings%2C%20we%20propose%20context%20synthesis%2C%20a%20novel%0Adata%20synthesis%20framework%20that%20leverages%20off-the-shelf%20LLMs%20to%20generate%20extended%0Abackground%20contexts%20for%20high-quality%20instruction-answer%20pairs.%20Experiment%0Aresults%20on%20the%20document-level%20benchmark%20%28LongBench%29%20demonstrate%20that%20our%0Aproposed%20approach%20outperforms%20previous%20instruction%20synthesis%20approaches%20and%0Acomes%20close%20to%20the%20performance%20of%20human-annotated%20long-context%20instruction%0Adata.%20The%20project%20will%20be%20available%20at%3A%0Ahttps%3A//github.com/NJUNLP/context-synthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15592v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralizing%2520From%2520Short%2520to%2520Long%253A%2520Effective%2520Data%2520Synthesis%2520for%250A%2520%2520Long-Context%2520Instruction%2520Tuning%26entry.906535625%3DWenhao%2520Zhu%2520and%2520Pinzhen%2520Chen%2520and%2520Hanxu%2520Hu%2520and%2520Shujian%2520Huang%2520and%2520Fei%2520Yuan%2520and%2520Jiajun%2520Chen%2520and%2520Alexandra%2520Birch%26entry.1292438233%3D%2520%2520Long-context%2520modelling%2520for%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520been%2520a%2520key%2520area%250Aof%2520recent%2520research%2520because%2520many%2520real%2520world%2520use%2520cases%2520require%2520reasoning%2520over%250Alonger%2520inputs%2520such%2520as%2520documents.%2520The%2520focus%2520of%2520research%2520into%2520modelling%2520long%250Acontext%2520has%2520been%2520on%2520how%2520to%2520model%2520position%2520and%2520there%2520has%2520been%2520little%250Ainvestigation%2520into%2520other%2520important%2520aspects%2520of%2520language%2520modelling%2520such%2520as%250Ainstruction%2520tuning.%2520Long%2520context%2520training%2520examples%2520are%2520challenging%2520and%250Aexpensive%2520to%2520create%2520and%2520use.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520how%2520to%2520design%250Ainstruction%2520data%2520for%2520the%2520post-training%2520phase%2520of%2520a%2520long%2520context%2520pre-trained%250Amodel%253A%2520how%2520much%2520and%2520what%2520type%2520of%2520context%2520is%2520needed%2520for%2520optimal%2520and%2520efficient%250Apost-training.%2520Our%2520controlled%2520study%2520reveals%2520that%2520models%2520instruction-tuned%2520on%250Ashort%2520contexts%2520can%2520effectively%2520generalize%2520to%2520longer%2520ones%252C%2520while%2520also%250Aidentifying%2520other%2520critical%2520factors%2520such%2520as%2520instruction%2520difficulty%2520and%2520context%250Acomposition.%2520Based%2520on%2520these%2520findings%252C%2520we%2520propose%2520context%2520synthesis%252C%2520a%2520novel%250Adata%2520synthesis%2520framework%2520that%2520leverages%2520off-the-shelf%2520LLMs%2520to%2520generate%2520extended%250Abackground%2520contexts%2520for%2520high-quality%2520instruction-answer%2520pairs.%2520Experiment%250Aresults%2520on%2520the%2520document-level%2520benchmark%2520%2528LongBench%2529%2520demonstrate%2520that%2520our%250Aproposed%2520approach%2520outperforms%2520previous%2520instruction%2520synthesis%2520approaches%2520and%250Acomes%2520close%2520to%2520the%2520performance%2520of%2520human-annotated%2520long-context%2520instruction%250Adata.%2520The%2520project%2520will%2520be%2520available%2520at%253A%250Ahttps%253A//github.com/NJUNLP/context-synthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15592v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalizing%20From%20Short%20to%20Long%3A%20Effective%20Data%20Synthesis%20for%0A%20%20Long-Context%20Instruction%20Tuning&entry.906535625=Wenhao%20Zhu%20and%20Pinzhen%20Chen%20and%20Hanxu%20Hu%20and%20Shujian%20Huang%20and%20Fei%20Yuan%20and%20Jiajun%20Chen%20and%20Alexandra%20Birch&entry.1292438233=%20%20Long-context%20modelling%20for%20large%20language%20models%20%28LLMs%29%20has%20been%20a%20key%20area%0Aof%20recent%20research%20because%20many%20real%20world%20use%20cases%20require%20reasoning%20over%0Alonger%20inputs%20such%20as%20documents.%20The%20focus%20of%20research%20into%20modelling%20long%0Acontext%20has%20been%20on%20how%20to%20model%20position%20and%20there%20has%20been%20little%0Ainvestigation%20into%20other%20important%20aspects%20of%20language%20modelling%20such%20as%0Ainstruction%20tuning.%20Long%20context%20training%20examples%20are%20challenging%20and%0Aexpensive%20to%20create%20and%20use.%20In%20this%20paper%2C%20we%20investigate%20how%20to%20design%0Ainstruction%20data%20for%20the%20post-training%20phase%20of%20a%20long%20context%20pre-trained%0Amodel%3A%20how%20much%20and%20what%20type%20of%20context%20is%20needed%20for%20optimal%20and%20efficient%0Apost-training.%20Our%20controlled%20study%20reveals%20that%20models%20instruction-tuned%20on%0Ashort%20contexts%20can%20effectively%20generalize%20to%20longer%20ones%2C%20while%20also%0Aidentifying%20other%20critical%20factors%20such%20as%20instruction%20difficulty%20and%20context%0Acomposition.%20Based%20on%20these%20findings%2C%20we%20propose%20context%20synthesis%2C%20a%20novel%0Adata%20synthesis%20framework%20that%20leverages%20off-the-shelf%20LLMs%20to%20generate%20extended%0Abackground%20contexts%20for%20high-quality%20instruction-answer%20pairs.%20Experiment%0Aresults%20on%20the%20document-level%20benchmark%20%28LongBench%29%20demonstrate%20that%20our%0Aproposed%20approach%20outperforms%20previous%20instruction%20synthesis%20approaches%20and%0Acomes%20close%20to%20the%20performance%20of%20human-annotated%20long-context%20instruction%0Adata.%20The%20project%20will%20be%20available%20at%3A%0Ahttps%3A//github.com/NJUNLP/context-synthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15592v1&entry.124074799=Read"},
{"title": "DarwinLM: Evolutionary Structured Pruning of Large Language Models", "author": "Shengkun Tang and Oliver Sieberling and Eldar Kurtic and Zhiqiang Shen and Dan Alistarh", "abstract": "  Large Language Models (LLMs) have achieved significant success across various\nNLP tasks. However, their massive computational costs limit their widespread\nuse, particularly in real-time applications. Structured pruning offers an\neffective solution by compressing models and directly providing end-to-end\nspeed improvements, regardless of the hardware environment. Meanwhile,\ndifferent components of the model exhibit varying sensitivities towards\npruning, calling for \\emph{non-uniform} model compression. However, a pruning\nmethod should not only identify a capable substructure, but also account for\npost-compression training. To this end, we propose \\sysname, a method for\n\\emph{training-aware} structured pruning. \\sysname builds upon an evolutionary\nsearch process, generating multiple offspring models in each generation through\nmutation, and selecting the fittest for survival. To assess the effect of\npost-training, we incorporate a lightweight, multistep training process within\nthe offspring population, progressively increasing the number of tokens and\neliminating poorly performing models in each selection stage. We validate our\nmethod through extensive experiments on Llama-2-7B, Llama-3.1-8B and\nQwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured\npruning. For instance, \\sysname surpasses ShearedLlama while requiring\n$5\\times$ less training data during post-compression training. Code is at:\nhttps://github.com/IST-DASLab/DarwinLM\n", "link": "http://arxiv.org/abs/2502.07780v2", "date": "2025-02-21", "relevancy": 2.4924, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5071}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5071}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DarwinLM%3A%20Evolutionary%20Structured%20Pruning%20of%20Large%20Language%20Models&body=Title%3A%20DarwinLM%3A%20Evolutionary%20Structured%20Pruning%20of%20Large%20Language%20Models%0AAuthor%3A%20Shengkun%20Tang%20and%20Oliver%20Sieberling%20and%20Eldar%20Kurtic%20and%20Zhiqiang%20Shen%20and%20Dan%20Alistarh%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20significant%20success%20across%20various%0ANLP%20tasks.%20However%2C%20their%20massive%20computational%20costs%20limit%20their%20widespread%0Ause%2C%20particularly%20in%20real-time%20applications.%20Structured%20pruning%20offers%20an%0Aeffective%20solution%20by%20compressing%20models%20and%20directly%20providing%20end-to-end%0Aspeed%20improvements%2C%20regardless%20of%20the%20hardware%20environment.%20Meanwhile%2C%0Adifferent%20components%20of%20the%20model%20exhibit%20varying%20sensitivities%20towards%0Apruning%2C%20calling%20for%20%5Cemph%7Bnon-uniform%7D%20model%20compression.%20However%2C%20a%20pruning%0Amethod%20should%20not%20only%20identify%20a%20capable%20substructure%2C%20but%20also%20account%20for%0Apost-compression%20training.%20To%20this%20end%2C%20we%20propose%20%5Csysname%2C%20a%20method%20for%0A%5Cemph%7Btraining-aware%7D%20structured%20pruning.%20%5Csysname%20builds%20upon%20an%20evolutionary%0Asearch%20process%2C%20generating%20multiple%20offspring%20models%20in%20each%20generation%20through%0Amutation%2C%20and%20selecting%20the%20fittest%20for%20survival.%20To%20assess%20the%20effect%20of%0Apost-training%2C%20we%20incorporate%20a%20lightweight%2C%20multistep%20training%20process%20within%0Athe%20offspring%20population%2C%20progressively%20increasing%20the%20number%20of%20tokens%20and%0Aeliminating%20poorly%20performing%20models%20in%20each%20selection%20stage.%20We%20validate%20our%0Amethod%20through%20extensive%20experiments%20on%20Llama-2-7B%2C%20Llama-3.1-8B%20and%0AQwen-2.5-14B-Instruct%2C%20achieving%20state-of-the-art%20performance%20for%20structured%0Apruning.%20For%20instance%2C%20%5Csysname%20surpasses%20ShearedLlama%20while%20requiring%0A%245%5Ctimes%24%20less%20training%20data%20during%20post-compression%20training.%20Code%20is%20at%3A%0Ahttps%3A//github.com/IST-DASLab/DarwinLM%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07780v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDarwinLM%253A%2520Evolutionary%2520Structured%2520Pruning%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DShengkun%2520Tang%2520and%2520Oliver%2520Sieberling%2520and%2520Eldar%2520Kurtic%2520and%2520Zhiqiang%2520Shen%2520and%2520Dan%2520Alistarh%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520achieved%2520significant%2520success%2520across%2520various%250ANLP%2520tasks.%2520However%252C%2520their%2520massive%2520computational%2520costs%2520limit%2520their%2520widespread%250Ause%252C%2520particularly%2520in%2520real-time%2520applications.%2520Structured%2520pruning%2520offers%2520an%250Aeffective%2520solution%2520by%2520compressing%2520models%2520and%2520directly%2520providing%2520end-to-end%250Aspeed%2520improvements%252C%2520regardless%2520of%2520the%2520hardware%2520environment.%2520Meanwhile%252C%250Adifferent%2520components%2520of%2520the%2520model%2520exhibit%2520varying%2520sensitivities%2520towards%250Apruning%252C%2520calling%2520for%2520%255Cemph%257Bnon-uniform%257D%2520model%2520compression.%2520However%252C%2520a%2520pruning%250Amethod%2520should%2520not%2520only%2520identify%2520a%2520capable%2520substructure%252C%2520but%2520also%2520account%2520for%250Apost-compression%2520training.%2520To%2520this%2520end%252C%2520we%2520propose%2520%255Csysname%252C%2520a%2520method%2520for%250A%255Cemph%257Btraining-aware%257D%2520structured%2520pruning.%2520%255Csysname%2520builds%2520upon%2520an%2520evolutionary%250Asearch%2520process%252C%2520generating%2520multiple%2520offspring%2520models%2520in%2520each%2520generation%2520through%250Amutation%252C%2520and%2520selecting%2520the%2520fittest%2520for%2520survival.%2520To%2520assess%2520the%2520effect%2520of%250Apost-training%252C%2520we%2520incorporate%2520a%2520lightweight%252C%2520multistep%2520training%2520process%2520within%250Athe%2520offspring%2520population%252C%2520progressively%2520increasing%2520the%2520number%2520of%2520tokens%2520and%250Aeliminating%2520poorly%2520performing%2520models%2520in%2520each%2520selection%2520stage.%2520We%2520validate%2520our%250Amethod%2520through%2520extensive%2520experiments%2520on%2520Llama-2-7B%252C%2520Llama-3.1-8B%2520and%250AQwen-2.5-14B-Instruct%252C%2520achieving%2520state-of-the-art%2520performance%2520for%2520structured%250Apruning.%2520For%2520instance%252C%2520%255Csysname%2520surpasses%2520ShearedLlama%2520while%2520requiring%250A%25245%255Ctimes%2524%2520less%2520training%2520data%2520during%2520post-compression%2520training.%2520Code%2520is%2520at%253A%250Ahttps%253A//github.com/IST-DASLab/DarwinLM%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07780v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DarwinLM%3A%20Evolutionary%20Structured%20Pruning%20of%20Large%20Language%20Models&entry.906535625=Shengkun%20Tang%20and%20Oliver%20Sieberling%20and%20Eldar%20Kurtic%20and%20Zhiqiang%20Shen%20and%20Dan%20Alistarh&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20significant%20success%20across%20various%0ANLP%20tasks.%20However%2C%20their%20massive%20computational%20costs%20limit%20their%20widespread%0Ause%2C%20particularly%20in%20real-time%20applications.%20Structured%20pruning%20offers%20an%0Aeffective%20solution%20by%20compressing%20models%20and%20directly%20providing%20end-to-end%0Aspeed%20improvements%2C%20regardless%20of%20the%20hardware%20environment.%20Meanwhile%2C%0Adifferent%20components%20of%20the%20model%20exhibit%20varying%20sensitivities%20towards%0Apruning%2C%20calling%20for%20%5Cemph%7Bnon-uniform%7D%20model%20compression.%20However%2C%20a%20pruning%0Amethod%20should%20not%20only%20identify%20a%20capable%20substructure%2C%20but%20also%20account%20for%0Apost-compression%20training.%20To%20this%20end%2C%20we%20propose%20%5Csysname%2C%20a%20method%20for%0A%5Cemph%7Btraining-aware%7D%20structured%20pruning.%20%5Csysname%20builds%20upon%20an%20evolutionary%0Asearch%20process%2C%20generating%20multiple%20offspring%20models%20in%20each%20generation%20through%0Amutation%2C%20and%20selecting%20the%20fittest%20for%20survival.%20To%20assess%20the%20effect%20of%0Apost-training%2C%20we%20incorporate%20a%20lightweight%2C%20multistep%20training%20process%20within%0Athe%20offspring%20population%2C%20progressively%20increasing%20the%20number%20of%20tokens%20and%0Aeliminating%20poorly%20performing%20models%20in%20each%20selection%20stage.%20We%20validate%20our%0Amethod%20through%20extensive%20experiments%20on%20Llama-2-7B%2C%20Llama-3.1-8B%20and%0AQwen-2.5-14B-Instruct%2C%20achieving%20state-of-the-art%20performance%20for%20structured%0Apruning.%20For%20instance%2C%20%5Csysname%20surpasses%20ShearedLlama%20while%20requiring%0A%245%5Ctimes%24%20less%20training%20data%20during%20post-compression%20training.%20Code%20is%20at%3A%0Ahttps%3A//github.com/IST-DASLab/DarwinLM%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07780v2&entry.124074799=Read"},
{"title": "Three Mechanisms of Feature Learning in a Linear Network", "author": "Yizhou Xu and Liu Ziyin", "abstract": "  Understanding the dynamics of neural networks in different width regimes is\ncrucial for improving their training and performance. We present an exact\nsolution for the learning dynamics of a one-hidden-layer linear network, with\none-dimensional data, across any finite width, uniquely exhibiting both kernel\nand feature learning phases. This study marks a technical advancement by\nenabling the analysis of the training trajectory from any initialization and a\ndetailed phase diagram under varying common hyperparameters such as width,\nlayer-wise learning rates, and scales of output and initialization. We identify\nthree novel prototype mechanisms specific to the feature learning regime: (1)\nlearning by alignment, (2) learning by disalignment, and (3) learning by\nrescaling, which contrast starkly with the dynamics observed in the kernel\nregime. Our theoretical findings are substantiated with empirical evidence\nshowing that these mechanisms also manifest in deep nonlinear networks handling\nreal-world tasks, enhancing our understanding of neural network training\ndynamics and guiding the design of more effective learning strategies.\n", "link": "http://arxiv.org/abs/2401.07085v3", "date": "2025-02-21", "relevancy": 2.484, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5242}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4885}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4777}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Three%20Mechanisms%20of%20Feature%20Learning%20in%20a%20Linear%20Network&body=Title%3A%20Three%20Mechanisms%20of%20Feature%20Learning%20in%20a%20Linear%20Network%0AAuthor%3A%20Yizhou%20Xu%20and%20Liu%20Ziyin%0AAbstract%3A%20%20%20Understanding%20the%20dynamics%20of%20neural%20networks%20in%20different%20width%20regimes%20is%0Acrucial%20for%20improving%20their%20training%20and%20performance.%20We%20present%20an%20exact%0Asolution%20for%20the%20learning%20dynamics%20of%20a%20one-hidden-layer%20linear%20network%2C%20with%0Aone-dimensional%20data%2C%20across%20any%20finite%20width%2C%20uniquely%20exhibiting%20both%20kernel%0Aand%20feature%20learning%20phases.%20This%20study%20marks%20a%20technical%20advancement%20by%0Aenabling%20the%20analysis%20of%20the%20training%20trajectory%20from%20any%20initialization%20and%20a%0Adetailed%20phase%20diagram%20under%20varying%20common%20hyperparameters%20such%20as%20width%2C%0Alayer-wise%20learning%20rates%2C%20and%20scales%20of%20output%20and%20initialization.%20We%20identify%0Athree%20novel%20prototype%20mechanisms%20specific%20to%20the%20feature%20learning%20regime%3A%20%281%29%0Alearning%20by%20alignment%2C%20%282%29%20learning%20by%20disalignment%2C%20and%20%283%29%20learning%20by%0Arescaling%2C%20which%20contrast%20starkly%20with%20the%20dynamics%20observed%20in%20the%20kernel%0Aregime.%20Our%20theoretical%20findings%20are%20substantiated%20with%20empirical%20evidence%0Ashowing%20that%20these%20mechanisms%20also%20manifest%20in%20deep%20nonlinear%20networks%20handling%0Areal-world%20tasks%2C%20enhancing%20our%20understanding%20of%20neural%20network%20training%0Adynamics%20and%20guiding%20the%20design%20of%20more%20effective%20learning%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.07085v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThree%2520Mechanisms%2520of%2520Feature%2520Learning%2520in%2520a%2520Linear%2520Network%26entry.906535625%3DYizhou%2520Xu%2520and%2520Liu%2520Ziyin%26entry.1292438233%3D%2520%2520Understanding%2520the%2520dynamics%2520of%2520neural%2520networks%2520in%2520different%2520width%2520regimes%2520is%250Acrucial%2520for%2520improving%2520their%2520training%2520and%2520performance.%2520We%2520present%2520an%2520exact%250Asolution%2520for%2520the%2520learning%2520dynamics%2520of%2520a%2520one-hidden-layer%2520linear%2520network%252C%2520with%250Aone-dimensional%2520data%252C%2520across%2520any%2520finite%2520width%252C%2520uniquely%2520exhibiting%2520both%2520kernel%250Aand%2520feature%2520learning%2520phases.%2520This%2520study%2520marks%2520a%2520technical%2520advancement%2520by%250Aenabling%2520the%2520analysis%2520of%2520the%2520training%2520trajectory%2520from%2520any%2520initialization%2520and%2520a%250Adetailed%2520phase%2520diagram%2520under%2520varying%2520common%2520hyperparameters%2520such%2520as%2520width%252C%250Alayer-wise%2520learning%2520rates%252C%2520and%2520scales%2520of%2520output%2520and%2520initialization.%2520We%2520identify%250Athree%2520novel%2520prototype%2520mechanisms%2520specific%2520to%2520the%2520feature%2520learning%2520regime%253A%2520%25281%2529%250Alearning%2520by%2520alignment%252C%2520%25282%2529%2520learning%2520by%2520disalignment%252C%2520and%2520%25283%2529%2520learning%2520by%250Arescaling%252C%2520which%2520contrast%2520starkly%2520with%2520the%2520dynamics%2520observed%2520in%2520the%2520kernel%250Aregime.%2520Our%2520theoretical%2520findings%2520are%2520substantiated%2520with%2520empirical%2520evidence%250Ashowing%2520that%2520these%2520mechanisms%2520also%2520manifest%2520in%2520deep%2520nonlinear%2520networks%2520handling%250Areal-world%2520tasks%252C%2520enhancing%2520our%2520understanding%2520of%2520neural%2520network%2520training%250Adynamics%2520and%2520guiding%2520the%2520design%2520of%2520more%2520effective%2520learning%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.07085v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Three%20Mechanisms%20of%20Feature%20Learning%20in%20a%20Linear%20Network&entry.906535625=Yizhou%20Xu%20and%20Liu%20Ziyin&entry.1292438233=%20%20Understanding%20the%20dynamics%20of%20neural%20networks%20in%20different%20width%20regimes%20is%0Acrucial%20for%20improving%20their%20training%20and%20performance.%20We%20present%20an%20exact%0Asolution%20for%20the%20learning%20dynamics%20of%20a%20one-hidden-layer%20linear%20network%2C%20with%0Aone-dimensional%20data%2C%20across%20any%20finite%20width%2C%20uniquely%20exhibiting%20both%20kernel%0Aand%20feature%20learning%20phases.%20This%20study%20marks%20a%20technical%20advancement%20by%0Aenabling%20the%20analysis%20of%20the%20training%20trajectory%20from%20any%20initialization%20and%20a%0Adetailed%20phase%20diagram%20under%20varying%20common%20hyperparameters%20such%20as%20width%2C%0Alayer-wise%20learning%20rates%2C%20and%20scales%20of%20output%20and%20initialization.%20We%20identify%0Athree%20novel%20prototype%20mechanisms%20specific%20to%20the%20feature%20learning%20regime%3A%20%281%29%0Alearning%20by%20alignment%2C%20%282%29%20learning%20by%20disalignment%2C%20and%20%283%29%20learning%20by%0Arescaling%2C%20which%20contrast%20starkly%20with%20the%20dynamics%20observed%20in%20the%20kernel%0Aregime.%20Our%20theoretical%20findings%20are%20substantiated%20with%20empirical%20evidence%0Ashowing%20that%20these%20mechanisms%20also%20manifest%20in%20deep%20nonlinear%20networks%20handling%0Areal-world%20tasks%2C%20enhancing%20our%20understanding%20of%20neural%20network%20training%0Adynamics%20and%20guiding%20the%20design%20of%20more%20effective%20learning%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.07085v3&entry.124074799=Read"},
{"title": "Is Free Self-Alignment Possible?", "author": "Dyah Adila and Changho Shin and Yijing Zhang and Frederic Sala", "abstract": "  Aligning pretrained language models (LMs) often requires large-scale\npreference data and substantial computational resources. These costs become\neven more prohibitive for multi-objective or pluralistic alignment. Is this\ntruly necessary? Can we perform efficient alignment using only internal model\ncapabilities, and without additional training? To answer this question, we\npropose AlignEZ, a novel approach that leverages (1) self-generated preference\ndata and (2) representation editing to achieve cost-effective, efficient\nalignment. By operating directly on learned representations, AlignEZ\nindependently targets different behavioral aspects without the overhead of\ntraditional alignment methods. Our experiments reveal that this cost-efficient\nprocedure improves performance across diverse tasks: up to 19.9% on general\nalignment and 1.9% on challenging mathematical reasoning tasks, even when\nstarting from a strong base model. AlignEZ can also align models to multiple\nobjectives simultaneously, granting fine-grained control over multiple\npreference axes. Finally, we show that AlignEZ can accelerate more expensive\nalignment procedures--such as DPO--even under limited availability of\nground-truth preference data.\n", "link": "http://arxiv.org/abs/2406.03642v2", "date": "2025-02-21", "relevancy": 2.4659, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4963}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4963}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20Free%20Self-Alignment%20Possible%3F&body=Title%3A%20Is%20Free%20Self-Alignment%20Possible%3F%0AAuthor%3A%20Dyah%20Adila%20and%20Changho%20Shin%20and%20Yijing%20Zhang%20and%20Frederic%20Sala%0AAbstract%3A%20%20%20Aligning%20pretrained%20language%20models%20%28LMs%29%20often%20requires%20large-scale%0Apreference%20data%20and%20substantial%20computational%20resources.%20These%20costs%20become%0Aeven%20more%20prohibitive%20for%20multi-objective%20or%20pluralistic%20alignment.%20Is%20this%0Atruly%20necessary%3F%20Can%20we%20perform%20efficient%20alignment%20using%20only%20internal%20model%0Acapabilities%2C%20and%20without%20additional%20training%3F%20To%20answer%20this%20question%2C%20we%0Apropose%20AlignEZ%2C%20a%20novel%20approach%20that%20leverages%20%281%29%20self-generated%20preference%0Adata%20and%20%282%29%20representation%20editing%20to%20achieve%20cost-effective%2C%20efficient%0Aalignment.%20By%20operating%20directly%20on%20learned%20representations%2C%20AlignEZ%0Aindependently%20targets%20different%20behavioral%20aspects%20without%20the%20overhead%20of%0Atraditional%20alignment%20methods.%20Our%20experiments%20reveal%20that%20this%20cost-efficient%0Aprocedure%20improves%20performance%20across%20diverse%20tasks%3A%20up%20to%2019.9%25%20on%20general%0Aalignment%20and%201.9%25%20on%20challenging%20mathematical%20reasoning%20tasks%2C%20even%20when%0Astarting%20from%20a%20strong%20base%20model.%20AlignEZ%20can%20also%20align%20models%20to%20multiple%0Aobjectives%20simultaneously%2C%20granting%20fine-grained%20control%20over%20multiple%0Apreference%20axes.%20Finally%2C%20we%20show%20that%20AlignEZ%20can%20accelerate%20more%20expensive%0Aalignment%20procedures--such%20as%20DPO--even%20under%20limited%20availability%20of%0Aground-truth%20preference%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03642v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520Free%2520Self-Alignment%2520Possible%253F%26entry.906535625%3DDyah%2520Adila%2520and%2520Changho%2520Shin%2520and%2520Yijing%2520Zhang%2520and%2520Frederic%2520Sala%26entry.1292438233%3D%2520%2520Aligning%2520pretrained%2520language%2520models%2520%2528LMs%2529%2520often%2520requires%2520large-scale%250Apreference%2520data%2520and%2520substantial%2520computational%2520resources.%2520These%2520costs%2520become%250Aeven%2520more%2520prohibitive%2520for%2520multi-objective%2520or%2520pluralistic%2520alignment.%2520Is%2520this%250Atruly%2520necessary%253F%2520Can%2520we%2520perform%2520efficient%2520alignment%2520using%2520only%2520internal%2520model%250Acapabilities%252C%2520and%2520without%2520additional%2520training%253F%2520To%2520answer%2520this%2520question%252C%2520we%250Apropose%2520AlignEZ%252C%2520a%2520novel%2520approach%2520that%2520leverages%2520%25281%2529%2520self-generated%2520preference%250Adata%2520and%2520%25282%2529%2520representation%2520editing%2520to%2520achieve%2520cost-effective%252C%2520efficient%250Aalignment.%2520By%2520operating%2520directly%2520on%2520learned%2520representations%252C%2520AlignEZ%250Aindependently%2520targets%2520different%2520behavioral%2520aspects%2520without%2520the%2520overhead%2520of%250Atraditional%2520alignment%2520methods.%2520Our%2520experiments%2520reveal%2520that%2520this%2520cost-efficient%250Aprocedure%2520improves%2520performance%2520across%2520diverse%2520tasks%253A%2520up%2520to%252019.9%2525%2520on%2520general%250Aalignment%2520and%25201.9%2525%2520on%2520challenging%2520mathematical%2520reasoning%2520tasks%252C%2520even%2520when%250Astarting%2520from%2520a%2520strong%2520base%2520model.%2520AlignEZ%2520can%2520also%2520align%2520models%2520to%2520multiple%250Aobjectives%2520simultaneously%252C%2520granting%2520fine-grained%2520control%2520over%2520multiple%250Apreference%2520axes.%2520Finally%252C%2520we%2520show%2520that%2520AlignEZ%2520can%2520accelerate%2520more%2520expensive%250Aalignment%2520procedures--such%2520as%2520DPO--even%2520under%2520limited%2520availability%2520of%250Aground-truth%2520preference%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03642v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20Free%20Self-Alignment%20Possible%3F&entry.906535625=Dyah%20Adila%20and%20Changho%20Shin%20and%20Yijing%20Zhang%20and%20Frederic%20Sala&entry.1292438233=%20%20Aligning%20pretrained%20language%20models%20%28LMs%29%20often%20requires%20large-scale%0Apreference%20data%20and%20substantial%20computational%20resources.%20These%20costs%20become%0Aeven%20more%20prohibitive%20for%20multi-objective%20or%20pluralistic%20alignment.%20Is%20this%0Atruly%20necessary%3F%20Can%20we%20perform%20efficient%20alignment%20using%20only%20internal%20model%0Acapabilities%2C%20and%20without%20additional%20training%3F%20To%20answer%20this%20question%2C%20we%0Apropose%20AlignEZ%2C%20a%20novel%20approach%20that%20leverages%20%281%29%20self-generated%20preference%0Adata%20and%20%282%29%20representation%20editing%20to%20achieve%20cost-effective%2C%20efficient%0Aalignment.%20By%20operating%20directly%20on%20learned%20representations%2C%20AlignEZ%0Aindependently%20targets%20different%20behavioral%20aspects%20without%20the%20overhead%20of%0Atraditional%20alignment%20methods.%20Our%20experiments%20reveal%20that%20this%20cost-efficient%0Aprocedure%20improves%20performance%20across%20diverse%20tasks%3A%20up%20to%2019.9%25%20on%20general%0Aalignment%20and%201.9%25%20on%20challenging%20mathematical%20reasoning%20tasks%2C%20even%20when%0Astarting%20from%20a%20strong%20base%20model.%20AlignEZ%20can%20also%20align%20models%20to%20multiple%0Aobjectives%20simultaneously%2C%20granting%20fine-grained%20control%20over%20multiple%0Apreference%20axes.%20Finally%2C%20we%20show%20that%20AlignEZ%20can%20accelerate%20more%20expensive%0Aalignment%20procedures--such%20as%20DPO--even%20under%20limited%20availability%20of%0Aground-truth%20preference%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03642v2&entry.124074799=Read"},
{"title": "LayerPano3D: Layered 3D Panorama for Hyper-Immersive Scene Generation", "author": "Shuai Yang and Jing Tan and Mengchen Zhang and Tong Wu and Yixuan Li and Gordon Wetzstein and Ziwei Liu and Dahua Lin", "abstract": "  3D immersive scene generation is a challenging yet critical task in computer\nvision and graphics. A desired virtual 3D scene should 1) exhibit\nomnidirectional view consistency, and 2) allow for free exploration in complex\nscene hierarchies. Existing methods either rely on successive scene expansion\nvia inpainting or employ panorama representation to represent large FOV scene\nenvironments. However, the generated scene suffers from semantic drift during\nexpansion and is unable to handle occlusion among scene hierarchies. To tackle\nthese challenges, we introduce Layerpano3D, a novel framework for full-view,\nexplorable panoramic 3D scene generation from a single text prompt. Our key\ninsight is to decompose a reference 2D panorama into multiple layers at\ndifferent depth levels, where each layer reveals the unseen space from the\nreference views via diffusion prior. Layerpano3D comprises multiple dedicated\ndesigns: 1) We introduce a new panorama dataset Upright360, comprising 9k\nhigh-quality and upright panorama images, and finetune the advanced Flux model\non Upright360 for high-quality, upright and consistent panorama generation. 2)\nWe pioneer the Layered 3D Panorama as underlying representation to manage\ncomplex scene hierarchies and lift it into 3D Gaussians to splat detailed\n360-degree omnidirectional scenes with unconstrained viewing paths. Extensive\nexperiments demonstrate that our framework generates state-of-the-art 3D\npanoramic scene in both full view consistency and immersive exploratory\nexperience. We believe that Layerpano3D holds promise for advancing 3D\npanoramic scene creation with numerous applications.\n", "link": "http://arxiv.org/abs/2408.13252v2", "date": "2025-02-21", "relevancy": 2.4541, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6181}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6181}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LayerPano3D%3A%20Layered%203D%20Panorama%20for%20Hyper-Immersive%20Scene%20Generation&body=Title%3A%20LayerPano3D%3A%20Layered%203D%20Panorama%20for%20Hyper-Immersive%20Scene%20Generation%0AAuthor%3A%20Shuai%20Yang%20and%20Jing%20Tan%20and%20Mengchen%20Zhang%20and%20Tong%20Wu%20and%20Yixuan%20Li%20and%20Gordon%20Wetzstein%20and%20Ziwei%20Liu%20and%20Dahua%20Lin%0AAbstract%3A%20%20%203D%20immersive%20scene%20generation%20is%20a%20challenging%20yet%20critical%20task%20in%20computer%0Avision%20and%20graphics.%20A%20desired%20virtual%203D%20scene%20should%201%29%20exhibit%0Aomnidirectional%20view%20consistency%2C%20and%202%29%20allow%20for%20free%20exploration%20in%20complex%0Ascene%20hierarchies.%20Existing%20methods%20either%20rely%20on%20successive%20scene%20expansion%0Avia%20inpainting%20or%20employ%20panorama%20representation%20to%20represent%20large%20FOV%20scene%0Aenvironments.%20However%2C%20the%20generated%20scene%20suffers%20from%20semantic%20drift%20during%0Aexpansion%20and%20is%20unable%20to%20handle%20occlusion%20among%20scene%20hierarchies.%20To%20tackle%0Athese%20challenges%2C%20we%20introduce%20Layerpano3D%2C%20a%20novel%20framework%20for%20full-view%2C%0Aexplorable%20panoramic%203D%20scene%20generation%20from%20a%20single%20text%20prompt.%20Our%20key%0Ainsight%20is%20to%20decompose%20a%20reference%202D%20panorama%20into%20multiple%20layers%20at%0Adifferent%20depth%20levels%2C%20where%20each%20layer%20reveals%20the%20unseen%20space%20from%20the%0Areference%20views%20via%20diffusion%20prior.%20Layerpano3D%20comprises%20multiple%20dedicated%0Adesigns%3A%201%29%20We%20introduce%20a%20new%20panorama%20dataset%20Upright360%2C%20comprising%209k%0Ahigh-quality%20and%20upright%20panorama%20images%2C%20and%20finetune%20the%20advanced%20Flux%20model%0Aon%20Upright360%20for%20high-quality%2C%20upright%20and%20consistent%20panorama%20generation.%202%29%0AWe%20pioneer%20the%20Layered%203D%20Panorama%20as%20underlying%20representation%20to%20manage%0Acomplex%20scene%20hierarchies%20and%20lift%20it%20into%203D%20Gaussians%20to%20splat%20detailed%0A360-degree%20omnidirectional%20scenes%20with%20unconstrained%20viewing%20paths.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20framework%20generates%20state-of-the-art%203D%0Apanoramic%20scene%20in%20both%20full%20view%20consistency%20and%20immersive%20exploratory%0Aexperience.%20We%20believe%20that%20Layerpano3D%20holds%20promise%20for%20advancing%203D%0Apanoramic%20scene%20creation%20with%20numerous%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13252v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLayerPano3D%253A%2520Layered%25203D%2520Panorama%2520for%2520Hyper-Immersive%2520Scene%2520Generation%26entry.906535625%3DShuai%2520Yang%2520and%2520Jing%2520Tan%2520and%2520Mengchen%2520Zhang%2520and%2520Tong%2520Wu%2520and%2520Yixuan%2520Li%2520and%2520Gordon%2520Wetzstein%2520and%2520Ziwei%2520Liu%2520and%2520Dahua%2520Lin%26entry.1292438233%3D%2520%25203D%2520immersive%2520scene%2520generation%2520is%2520a%2520challenging%2520yet%2520critical%2520task%2520in%2520computer%250Avision%2520and%2520graphics.%2520A%2520desired%2520virtual%25203D%2520scene%2520should%25201%2529%2520exhibit%250Aomnidirectional%2520view%2520consistency%252C%2520and%25202%2529%2520allow%2520for%2520free%2520exploration%2520in%2520complex%250Ascene%2520hierarchies.%2520Existing%2520methods%2520either%2520rely%2520on%2520successive%2520scene%2520expansion%250Avia%2520inpainting%2520or%2520employ%2520panorama%2520representation%2520to%2520represent%2520large%2520FOV%2520scene%250Aenvironments.%2520However%252C%2520the%2520generated%2520scene%2520suffers%2520from%2520semantic%2520drift%2520during%250Aexpansion%2520and%2520is%2520unable%2520to%2520handle%2520occlusion%2520among%2520scene%2520hierarchies.%2520To%2520tackle%250Athese%2520challenges%252C%2520we%2520introduce%2520Layerpano3D%252C%2520a%2520novel%2520framework%2520for%2520full-view%252C%250Aexplorable%2520panoramic%25203D%2520scene%2520generation%2520from%2520a%2520single%2520text%2520prompt.%2520Our%2520key%250Ainsight%2520is%2520to%2520decompose%2520a%2520reference%25202D%2520panorama%2520into%2520multiple%2520layers%2520at%250Adifferent%2520depth%2520levels%252C%2520where%2520each%2520layer%2520reveals%2520the%2520unseen%2520space%2520from%2520the%250Areference%2520views%2520via%2520diffusion%2520prior.%2520Layerpano3D%2520comprises%2520multiple%2520dedicated%250Adesigns%253A%25201%2529%2520We%2520introduce%2520a%2520new%2520panorama%2520dataset%2520Upright360%252C%2520comprising%25209k%250Ahigh-quality%2520and%2520upright%2520panorama%2520images%252C%2520and%2520finetune%2520the%2520advanced%2520Flux%2520model%250Aon%2520Upright360%2520for%2520high-quality%252C%2520upright%2520and%2520consistent%2520panorama%2520generation.%25202%2529%250AWe%2520pioneer%2520the%2520Layered%25203D%2520Panorama%2520as%2520underlying%2520representation%2520to%2520manage%250Acomplex%2520scene%2520hierarchies%2520and%2520lift%2520it%2520into%25203D%2520Gaussians%2520to%2520splat%2520detailed%250A360-degree%2520omnidirectional%2520scenes%2520with%2520unconstrained%2520viewing%2520paths.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520framework%2520generates%2520state-of-the-art%25203D%250Apanoramic%2520scene%2520in%2520both%2520full%2520view%2520consistency%2520and%2520immersive%2520exploratory%250Aexperience.%2520We%2520believe%2520that%2520Layerpano3D%2520holds%2520promise%2520for%2520advancing%25203D%250Apanoramic%2520scene%2520creation%2520with%2520numerous%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13252v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LayerPano3D%3A%20Layered%203D%20Panorama%20for%20Hyper-Immersive%20Scene%20Generation&entry.906535625=Shuai%20Yang%20and%20Jing%20Tan%20and%20Mengchen%20Zhang%20and%20Tong%20Wu%20and%20Yixuan%20Li%20and%20Gordon%20Wetzstein%20and%20Ziwei%20Liu%20and%20Dahua%20Lin&entry.1292438233=%20%203D%20immersive%20scene%20generation%20is%20a%20challenging%20yet%20critical%20task%20in%20computer%0Avision%20and%20graphics.%20A%20desired%20virtual%203D%20scene%20should%201%29%20exhibit%0Aomnidirectional%20view%20consistency%2C%20and%202%29%20allow%20for%20free%20exploration%20in%20complex%0Ascene%20hierarchies.%20Existing%20methods%20either%20rely%20on%20successive%20scene%20expansion%0Avia%20inpainting%20or%20employ%20panorama%20representation%20to%20represent%20large%20FOV%20scene%0Aenvironments.%20However%2C%20the%20generated%20scene%20suffers%20from%20semantic%20drift%20during%0Aexpansion%20and%20is%20unable%20to%20handle%20occlusion%20among%20scene%20hierarchies.%20To%20tackle%0Athese%20challenges%2C%20we%20introduce%20Layerpano3D%2C%20a%20novel%20framework%20for%20full-view%2C%0Aexplorable%20panoramic%203D%20scene%20generation%20from%20a%20single%20text%20prompt.%20Our%20key%0Ainsight%20is%20to%20decompose%20a%20reference%202D%20panorama%20into%20multiple%20layers%20at%0Adifferent%20depth%20levels%2C%20where%20each%20layer%20reveals%20the%20unseen%20space%20from%20the%0Areference%20views%20via%20diffusion%20prior.%20Layerpano3D%20comprises%20multiple%20dedicated%0Adesigns%3A%201%29%20We%20introduce%20a%20new%20panorama%20dataset%20Upright360%2C%20comprising%209k%0Ahigh-quality%20and%20upright%20panorama%20images%2C%20and%20finetune%20the%20advanced%20Flux%20model%0Aon%20Upright360%20for%20high-quality%2C%20upright%20and%20consistent%20panorama%20generation.%202%29%0AWe%20pioneer%20the%20Layered%203D%20Panorama%20as%20underlying%20representation%20to%20manage%0Acomplex%20scene%20hierarchies%20and%20lift%20it%20into%203D%20Gaussians%20to%20splat%20detailed%0A360-degree%20omnidirectional%20scenes%20with%20unconstrained%20viewing%20paths.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20framework%20generates%20state-of-the-art%203D%0Apanoramic%20scene%20in%20both%20full%20view%20consistency%20and%20immersive%20exploratory%0Aexperience.%20We%20believe%20that%20Layerpano3D%20holds%20promise%20for%20advancing%203D%0Apanoramic%20scene%20creation%20with%20numerous%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13252v2&entry.124074799=Read"},
{"title": "Constructing a Norm for Children's Scientific Drawing: Distribution\n  Features Based on Semantic Similarity of Large Language Models", "author": "Yi Zhang and Fan Wei and Jingyi Li and Yan Wang and Yanyan Yu and Jianli Chen and Zipo Cai and Xinyu Liu and Wei Wang and Peng Wang and Zhong Wang", "abstract": "  The use of children's drawings to examining their conceptual understanding\nhas been proven to be an effective method, but there are two major problems\nwith previous research: 1. The content of the drawings heavily relies on the\ntask, and the ecological validity of the conclusions is low; 2. The\ninterpretation of drawings relies too much on the subjective feelings of the\nresearchers. To address this issue, this study uses the Large Language Model\n(LLM) to identify 1420 children's scientific drawings (covering 9 scientific\nthemes/concepts), and uses the word2vec algorithm to calculate their semantic\nsimilarity. The study explores whether there are consistent drawing\nrepresentations for children on the same theme, and attempts to establish a\nnorm for children's scientific drawings, providing a baseline reference for\nfollow-up children's drawing research. The results show that the representation\nof most drawings has consistency, manifested as most semantic similarity\ngreater than 0.8. At the same time, it was found that the consistency of the\nrepresentation is independent of the accuracy (of LLM's recognition),\nindicating the existence of consistency bias. In the subsequent exploration of\ninfluencing factors, we used Kendall rank correlation coefficient to\ninvestigate the effects of Sample Size, Abstract Degree, and Focus Points on\ndrawings, and used word frequency statistics to explore whether children\nrepresented abstract themes/concepts by reproducing what was taught in class.\n", "link": "http://arxiv.org/abs/2502.15348v1", "date": "2025-02-21", "relevancy": 2.4497, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4998}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4998}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Constructing%20a%20Norm%20for%20Children%27s%20Scientific%20Drawing%3A%20Distribution%0A%20%20Features%20Based%20on%20Semantic%20Similarity%20of%20Large%20Language%20Models&body=Title%3A%20Constructing%20a%20Norm%20for%20Children%27s%20Scientific%20Drawing%3A%20Distribution%0A%20%20Features%20Based%20on%20Semantic%20Similarity%20of%20Large%20Language%20Models%0AAuthor%3A%20Yi%20Zhang%20and%20Fan%20Wei%20and%20Jingyi%20Li%20and%20Yan%20Wang%20and%20Yanyan%20Yu%20and%20Jianli%20Chen%20and%20Zipo%20Cai%20and%20Xinyu%20Liu%20and%20Wei%20Wang%20and%20Peng%20Wang%20and%20Zhong%20Wang%0AAbstract%3A%20%20%20The%20use%20of%20children%27s%20drawings%20to%20examining%20their%20conceptual%20understanding%0Ahas%20been%20proven%20to%20be%20an%20effective%20method%2C%20but%20there%20are%20two%20major%20problems%0Awith%20previous%20research%3A%201.%20The%20content%20of%20the%20drawings%20heavily%20relies%20on%20the%0Atask%2C%20and%20the%20ecological%20validity%20of%20the%20conclusions%20is%20low%3B%202.%20The%0Ainterpretation%20of%20drawings%20relies%20too%20much%20on%20the%20subjective%20feelings%20of%20the%0Aresearchers.%20To%20address%20this%20issue%2C%20this%20study%20uses%20the%20Large%20Language%20Model%0A%28LLM%29%20to%20identify%201420%20children%27s%20scientific%20drawings%20%28covering%209%20scientific%0Athemes/concepts%29%2C%20and%20uses%20the%20word2vec%20algorithm%20to%20calculate%20their%20semantic%0Asimilarity.%20The%20study%20explores%20whether%20there%20are%20consistent%20drawing%0Arepresentations%20for%20children%20on%20the%20same%20theme%2C%20and%20attempts%20to%20establish%20a%0Anorm%20for%20children%27s%20scientific%20drawings%2C%20providing%20a%20baseline%20reference%20for%0Afollow-up%20children%27s%20drawing%20research.%20The%20results%20show%20that%20the%20representation%0Aof%20most%20drawings%20has%20consistency%2C%20manifested%20as%20most%20semantic%20similarity%0Agreater%20than%200.8.%20At%20the%20same%20time%2C%20it%20was%20found%20that%20the%20consistency%20of%20the%0Arepresentation%20is%20independent%20of%20the%20accuracy%20%28of%20LLM%27s%20recognition%29%2C%0Aindicating%20the%20existence%20of%20consistency%20bias.%20In%20the%20subsequent%20exploration%20of%0Ainfluencing%20factors%2C%20we%20used%20Kendall%20rank%20correlation%20coefficient%20to%0Ainvestigate%20the%20effects%20of%20Sample%20Size%2C%20Abstract%20Degree%2C%20and%20Focus%20Points%20on%0Adrawings%2C%20and%20used%20word%20frequency%20statistics%20to%20explore%20whether%20children%0Arepresented%20abstract%20themes/concepts%20by%20reproducing%20what%20was%20taught%20in%20class.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15348v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConstructing%2520a%2520Norm%2520for%2520Children%2527s%2520Scientific%2520Drawing%253A%2520Distribution%250A%2520%2520Features%2520Based%2520on%2520Semantic%2520Similarity%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DYi%2520Zhang%2520and%2520Fan%2520Wei%2520and%2520Jingyi%2520Li%2520and%2520Yan%2520Wang%2520and%2520Yanyan%2520Yu%2520and%2520Jianli%2520Chen%2520and%2520Zipo%2520Cai%2520and%2520Xinyu%2520Liu%2520and%2520Wei%2520Wang%2520and%2520Peng%2520Wang%2520and%2520Zhong%2520Wang%26entry.1292438233%3D%2520%2520The%2520use%2520of%2520children%2527s%2520drawings%2520to%2520examining%2520their%2520conceptual%2520understanding%250Ahas%2520been%2520proven%2520to%2520be%2520an%2520effective%2520method%252C%2520but%2520there%2520are%2520two%2520major%2520problems%250Awith%2520previous%2520research%253A%25201.%2520The%2520content%2520of%2520the%2520drawings%2520heavily%2520relies%2520on%2520the%250Atask%252C%2520and%2520the%2520ecological%2520validity%2520of%2520the%2520conclusions%2520is%2520low%253B%25202.%2520The%250Ainterpretation%2520of%2520drawings%2520relies%2520too%2520much%2520on%2520the%2520subjective%2520feelings%2520of%2520the%250Aresearchers.%2520To%2520address%2520this%2520issue%252C%2520this%2520study%2520uses%2520the%2520Large%2520Language%2520Model%250A%2528LLM%2529%2520to%2520identify%25201420%2520children%2527s%2520scientific%2520drawings%2520%2528covering%25209%2520scientific%250Athemes/concepts%2529%252C%2520and%2520uses%2520the%2520word2vec%2520algorithm%2520to%2520calculate%2520their%2520semantic%250Asimilarity.%2520The%2520study%2520explores%2520whether%2520there%2520are%2520consistent%2520drawing%250Arepresentations%2520for%2520children%2520on%2520the%2520same%2520theme%252C%2520and%2520attempts%2520to%2520establish%2520a%250Anorm%2520for%2520children%2527s%2520scientific%2520drawings%252C%2520providing%2520a%2520baseline%2520reference%2520for%250Afollow-up%2520children%2527s%2520drawing%2520research.%2520The%2520results%2520show%2520that%2520the%2520representation%250Aof%2520most%2520drawings%2520has%2520consistency%252C%2520manifested%2520as%2520most%2520semantic%2520similarity%250Agreater%2520than%25200.8.%2520At%2520the%2520same%2520time%252C%2520it%2520was%2520found%2520that%2520the%2520consistency%2520of%2520the%250Arepresentation%2520is%2520independent%2520of%2520the%2520accuracy%2520%2528of%2520LLM%2527s%2520recognition%2529%252C%250Aindicating%2520the%2520existence%2520of%2520consistency%2520bias.%2520In%2520the%2520subsequent%2520exploration%2520of%250Ainfluencing%2520factors%252C%2520we%2520used%2520Kendall%2520rank%2520correlation%2520coefficient%2520to%250Ainvestigate%2520the%2520effects%2520of%2520Sample%2520Size%252C%2520Abstract%2520Degree%252C%2520and%2520Focus%2520Points%2520on%250Adrawings%252C%2520and%2520used%2520word%2520frequency%2520statistics%2520to%2520explore%2520whether%2520children%250Arepresented%2520abstract%2520themes/concepts%2520by%2520reproducing%2520what%2520was%2520taught%2520in%2520class.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15348v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Constructing%20a%20Norm%20for%20Children%27s%20Scientific%20Drawing%3A%20Distribution%0A%20%20Features%20Based%20on%20Semantic%20Similarity%20of%20Large%20Language%20Models&entry.906535625=Yi%20Zhang%20and%20Fan%20Wei%20and%20Jingyi%20Li%20and%20Yan%20Wang%20and%20Yanyan%20Yu%20and%20Jianli%20Chen%20and%20Zipo%20Cai%20and%20Xinyu%20Liu%20and%20Wei%20Wang%20and%20Peng%20Wang%20and%20Zhong%20Wang&entry.1292438233=%20%20The%20use%20of%20children%27s%20drawings%20to%20examining%20their%20conceptual%20understanding%0Ahas%20been%20proven%20to%20be%20an%20effective%20method%2C%20but%20there%20are%20two%20major%20problems%0Awith%20previous%20research%3A%201.%20The%20content%20of%20the%20drawings%20heavily%20relies%20on%20the%0Atask%2C%20and%20the%20ecological%20validity%20of%20the%20conclusions%20is%20low%3B%202.%20The%0Ainterpretation%20of%20drawings%20relies%20too%20much%20on%20the%20subjective%20feelings%20of%20the%0Aresearchers.%20To%20address%20this%20issue%2C%20this%20study%20uses%20the%20Large%20Language%20Model%0A%28LLM%29%20to%20identify%201420%20children%27s%20scientific%20drawings%20%28covering%209%20scientific%0Athemes/concepts%29%2C%20and%20uses%20the%20word2vec%20algorithm%20to%20calculate%20their%20semantic%0Asimilarity.%20The%20study%20explores%20whether%20there%20are%20consistent%20drawing%0Arepresentations%20for%20children%20on%20the%20same%20theme%2C%20and%20attempts%20to%20establish%20a%0Anorm%20for%20children%27s%20scientific%20drawings%2C%20providing%20a%20baseline%20reference%20for%0Afollow-up%20children%27s%20drawing%20research.%20The%20results%20show%20that%20the%20representation%0Aof%20most%20drawings%20has%20consistency%2C%20manifested%20as%20most%20semantic%20similarity%0Agreater%20than%200.8.%20At%20the%20same%20time%2C%20it%20was%20found%20that%20the%20consistency%20of%20the%0Arepresentation%20is%20independent%20of%20the%20accuracy%20%28of%20LLM%27s%20recognition%29%2C%0Aindicating%20the%20existence%20of%20consistency%20bias.%20In%20the%20subsequent%20exploration%20of%0Ainfluencing%20factors%2C%20we%20used%20Kendall%20rank%20correlation%20coefficient%20to%0Ainvestigate%20the%20effects%20of%20Sample%20Size%2C%20Abstract%20Degree%2C%20and%20Focus%20Points%20on%0Adrawings%2C%20and%20used%20word%20frequency%20statistics%20to%20explore%20whether%20children%0Arepresented%20abstract%20themes/concepts%20by%20reproducing%20what%20was%20taught%20in%20class.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15348v1&entry.124074799=Read"},
{"title": "Super-Resolution for Interferometric Imaging: Model Comparisons and\n  Performance Analysis", "author": "Hasan Berkay Abdioglu and Rana Gursoy and Yagmur Isik and Ibrahim Cem Balci and Taha Unal and Kerem Bayer and Mustafa Ismail Inal and Nehir Serin and Muhammed Furkan Kosar and Gokhan Bora Esmer and Huseyin Uvet", "abstract": "  This study investigates the application of Super-Resolution techniques in\nholographic microscopy to enhance quantitative phase imaging. An off-axis\nMach-Zehnder interferometric setup was employed to capture interferograms. The\nstudy evaluates two Super-Resolution models, RCAN and Real-ESRGAN, for their\neffectiveness in reconstructing high-resolution interferograms from a\nmicroparticle-based dataset. The models were assessed using two primary\napproaches: image-based analysis for structural detail enhancement and\nmorphological evaluation for maintaining sample integrity and phase map\naccuracy. The results demonstrate that RCAN achieves superior numerical\nprecision, making it ideal for applications requiring highly accurate phase map\nreconstruction, while Real-ESRGAN enhances visual quality and structural\ncoherence, making it suitable for visualization-focused applications. This\nstudy highlights the potential of Super-Resolution models in overcoming\ndiffraction-imposed resolution limitations in holographic microscopy, opening\nthe way for improved imaging techniques in biomedical diagnostics, materials\nscience, and other high-precision fields.\n", "link": "http://arxiv.org/abs/2502.15397v1", "date": "2025-02-21", "relevancy": 2.4333, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4988}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4806}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Super-Resolution%20for%20Interferometric%20Imaging%3A%20Model%20Comparisons%20and%0A%20%20Performance%20Analysis&body=Title%3A%20Super-Resolution%20for%20Interferometric%20Imaging%3A%20Model%20Comparisons%20and%0A%20%20Performance%20Analysis%0AAuthor%3A%20Hasan%20Berkay%20Abdioglu%20and%20Rana%20Gursoy%20and%20Yagmur%20Isik%20and%20Ibrahim%20Cem%20Balci%20and%20Taha%20Unal%20and%20Kerem%20Bayer%20and%20Mustafa%20Ismail%20Inal%20and%20Nehir%20Serin%20and%20Muhammed%20Furkan%20Kosar%20and%20Gokhan%20Bora%20Esmer%20and%20Huseyin%20Uvet%0AAbstract%3A%20%20%20This%20study%20investigates%20the%20application%20of%20Super-Resolution%20techniques%20in%0Aholographic%20microscopy%20to%20enhance%20quantitative%20phase%20imaging.%20An%20off-axis%0AMach-Zehnder%20interferometric%20setup%20was%20employed%20to%20capture%20interferograms.%20The%0Astudy%20evaluates%20two%20Super-Resolution%20models%2C%20RCAN%20and%20Real-ESRGAN%2C%20for%20their%0Aeffectiveness%20in%20reconstructing%20high-resolution%20interferograms%20from%20a%0Amicroparticle-based%20dataset.%20The%20models%20were%20assessed%20using%20two%20primary%0Aapproaches%3A%20image-based%20analysis%20for%20structural%20detail%20enhancement%20and%0Amorphological%20evaluation%20for%20maintaining%20sample%20integrity%20and%20phase%20map%0Aaccuracy.%20The%20results%20demonstrate%20that%20RCAN%20achieves%20superior%20numerical%0Aprecision%2C%20making%20it%20ideal%20for%20applications%20requiring%20highly%20accurate%20phase%20map%0Areconstruction%2C%20while%20Real-ESRGAN%20enhances%20visual%20quality%20and%20structural%0Acoherence%2C%20making%20it%20suitable%20for%20visualization-focused%20applications.%20This%0Astudy%20highlights%20the%20potential%20of%20Super-Resolution%20models%20in%20overcoming%0Adiffraction-imposed%20resolution%20limitations%20in%20holographic%20microscopy%2C%20opening%0Athe%20way%20for%20improved%20imaging%20techniques%20in%20biomedical%20diagnostics%2C%20materials%0Ascience%2C%20and%20other%20high-precision%20fields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15397v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuper-Resolution%2520for%2520Interferometric%2520Imaging%253A%2520Model%2520Comparisons%2520and%250A%2520%2520Performance%2520Analysis%26entry.906535625%3DHasan%2520Berkay%2520Abdioglu%2520and%2520Rana%2520Gursoy%2520and%2520Yagmur%2520Isik%2520and%2520Ibrahim%2520Cem%2520Balci%2520and%2520Taha%2520Unal%2520and%2520Kerem%2520Bayer%2520and%2520Mustafa%2520Ismail%2520Inal%2520and%2520Nehir%2520Serin%2520and%2520Muhammed%2520Furkan%2520Kosar%2520and%2520Gokhan%2520Bora%2520Esmer%2520and%2520Huseyin%2520Uvet%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520the%2520application%2520of%2520Super-Resolution%2520techniques%2520in%250Aholographic%2520microscopy%2520to%2520enhance%2520quantitative%2520phase%2520imaging.%2520An%2520off-axis%250AMach-Zehnder%2520interferometric%2520setup%2520was%2520employed%2520to%2520capture%2520interferograms.%2520The%250Astudy%2520evaluates%2520two%2520Super-Resolution%2520models%252C%2520RCAN%2520and%2520Real-ESRGAN%252C%2520for%2520their%250Aeffectiveness%2520in%2520reconstructing%2520high-resolution%2520interferograms%2520from%2520a%250Amicroparticle-based%2520dataset.%2520The%2520models%2520were%2520assessed%2520using%2520two%2520primary%250Aapproaches%253A%2520image-based%2520analysis%2520for%2520structural%2520detail%2520enhancement%2520and%250Amorphological%2520evaluation%2520for%2520maintaining%2520sample%2520integrity%2520and%2520phase%2520map%250Aaccuracy.%2520The%2520results%2520demonstrate%2520that%2520RCAN%2520achieves%2520superior%2520numerical%250Aprecision%252C%2520making%2520it%2520ideal%2520for%2520applications%2520requiring%2520highly%2520accurate%2520phase%2520map%250Areconstruction%252C%2520while%2520Real-ESRGAN%2520enhances%2520visual%2520quality%2520and%2520structural%250Acoherence%252C%2520making%2520it%2520suitable%2520for%2520visualization-focused%2520applications.%2520This%250Astudy%2520highlights%2520the%2520potential%2520of%2520Super-Resolution%2520models%2520in%2520overcoming%250Adiffraction-imposed%2520resolution%2520limitations%2520in%2520holographic%2520microscopy%252C%2520opening%250Athe%2520way%2520for%2520improved%2520imaging%2520techniques%2520in%2520biomedical%2520diagnostics%252C%2520materials%250Ascience%252C%2520and%2520other%2520high-precision%2520fields.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15397v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Super-Resolution%20for%20Interferometric%20Imaging%3A%20Model%20Comparisons%20and%0A%20%20Performance%20Analysis&entry.906535625=Hasan%20Berkay%20Abdioglu%20and%20Rana%20Gursoy%20and%20Yagmur%20Isik%20and%20Ibrahim%20Cem%20Balci%20and%20Taha%20Unal%20and%20Kerem%20Bayer%20and%20Mustafa%20Ismail%20Inal%20and%20Nehir%20Serin%20and%20Muhammed%20Furkan%20Kosar%20and%20Gokhan%20Bora%20Esmer%20and%20Huseyin%20Uvet&entry.1292438233=%20%20This%20study%20investigates%20the%20application%20of%20Super-Resolution%20techniques%20in%0Aholographic%20microscopy%20to%20enhance%20quantitative%20phase%20imaging.%20An%20off-axis%0AMach-Zehnder%20interferometric%20setup%20was%20employed%20to%20capture%20interferograms.%20The%0Astudy%20evaluates%20two%20Super-Resolution%20models%2C%20RCAN%20and%20Real-ESRGAN%2C%20for%20their%0Aeffectiveness%20in%20reconstructing%20high-resolution%20interferograms%20from%20a%0Amicroparticle-based%20dataset.%20The%20models%20were%20assessed%20using%20two%20primary%0Aapproaches%3A%20image-based%20analysis%20for%20structural%20detail%20enhancement%20and%0Amorphological%20evaluation%20for%20maintaining%20sample%20integrity%20and%20phase%20map%0Aaccuracy.%20The%20results%20demonstrate%20that%20RCAN%20achieves%20superior%20numerical%0Aprecision%2C%20making%20it%20ideal%20for%20applications%20requiring%20highly%20accurate%20phase%20map%0Areconstruction%2C%20while%20Real-ESRGAN%20enhances%20visual%20quality%20and%20structural%0Acoherence%2C%20making%20it%20suitable%20for%20visualization-focused%20applications.%20This%0Astudy%20highlights%20the%20potential%20of%20Super-Resolution%20models%20in%20overcoming%0Adiffraction-imposed%20resolution%20limitations%20in%20holographic%20microscopy%2C%20opening%0Athe%20way%20for%20improved%20imaging%20techniques%20in%20biomedical%20diagnostics%2C%20materials%0Ascience%2C%20and%20other%20high-precision%20fields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15397v1&entry.124074799=Read"},
{"title": "Schema Augmentation for Zero-Shot Domain Adaptation in Dialogue State\n  Tracking", "author": "Christopher Richardson and Roshan Sharma and Neeraj Gaur and Parisa Haghani and Anirudh Sundar and Bhuvana Ramabhadran", "abstract": "  Zero-shot domain adaptation for dialogue state tracking (DST) remains a\nchallenging problem in task-oriented dialogue (TOD) systems, where models must\ngeneralize to target domains unseen at training time. Current large language\nmodel approaches for zero-shot domain adaptation rely on prompting to introduce\nknowledge pertaining to the target domains. However, their efficacy strongly\ndepends on prompt engineering, as well as the zero-shot ability of the\nunderlying language model. In this work, we devise a novel data augmentation\napproach, Schema Augmentation, that improves the zero-shot domain adaptation of\nlanguage models through fine-tuning. Schema Augmentation is a simple but\neffective technique that enhances generalization by introducing variations of\nslot names within the schema provided in the prompt. Experiments on MultiWOZ\nand SpokenWOZ showed that the proposed approach resulted in a substantial\nimprovement over the baseline, in some experiments achieving over a twofold\naccuracy gain over unseen domains while maintaining equal or superior\nperformance over all domains.\n", "link": "http://arxiv.org/abs/2411.00150v2", "date": "2025-02-21", "relevancy": 2.4221, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5059}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4737}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Schema%20Augmentation%20for%20Zero-Shot%20Domain%20Adaptation%20in%20Dialogue%20State%0A%20%20Tracking&body=Title%3A%20Schema%20Augmentation%20for%20Zero-Shot%20Domain%20Adaptation%20in%20Dialogue%20State%0A%20%20Tracking%0AAuthor%3A%20Christopher%20Richardson%20and%20Roshan%20Sharma%20and%20Neeraj%20Gaur%20and%20Parisa%20Haghani%20and%20Anirudh%20Sundar%20and%20Bhuvana%20Ramabhadran%0AAbstract%3A%20%20%20Zero-shot%20domain%20adaptation%20for%20dialogue%20state%20tracking%20%28DST%29%20remains%20a%0Achallenging%20problem%20in%20task-oriented%20dialogue%20%28TOD%29%20systems%2C%20where%20models%20must%0Ageneralize%20to%20target%20domains%20unseen%20at%20training%20time.%20Current%20large%20language%0Amodel%20approaches%20for%20zero-shot%20domain%20adaptation%20rely%20on%20prompting%20to%20introduce%0Aknowledge%20pertaining%20to%20the%20target%20domains.%20However%2C%20their%20efficacy%20strongly%0Adepends%20on%20prompt%20engineering%2C%20as%20well%20as%20the%20zero-shot%20ability%20of%20the%0Aunderlying%20language%20model.%20In%20this%20work%2C%20we%20devise%20a%20novel%20data%20augmentation%0Aapproach%2C%20Schema%20Augmentation%2C%20that%20improves%20the%20zero-shot%20domain%20adaptation%20of%0Alanguage%20models%20through%20fine-tuning.%20Schema%20Augmentation%20is%20a%20simple%20but%0Aeffective%20technique%20that%20enhances%20generalization%20by%20introducing%20variations%20of%0Aslot%20names%20within%20the%20schema%20provided%20in%20the%20prompt.%20Experiments%20on%20MultiWOZ%0Aand%20SpokenWOZ%20showed%20that%20the%20proposed%20approach%20resulted%20in%20a%20substantial%0Aimprovement%20over%20the%20baseline%2C%20in%20some%20experiments%20achieving%20over%20a%20twofold%0Aaccuracy%20gain%20over%20unseen%20domains%20while%20maintaining%20equal%20or%20superior%0Aperformance%20over%20all%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.00150v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSchema%2520Augmentation%2520for%2520Zero-Shot%2520Domain%2520Adaptation%2520in%2520Dialogue%2520State%250A%2520%2520Tracking%26entry.906535625%3DChristopher%2520Richardson%2520and%2520Roshan%2520Sharma%2520and%2520Neeraj%2520Gaur%2520and%2520Parisa%2520Haghani%2520and%2520Anirudh%2520Sundar%2520and%2520Bhuvana%2520Ramabhadran%26entry.1292438233%3D%2520%2520Zero-shot%2520domain%2520adaptation%2520for%2520dialogue%2520state%2520tracking%2520%2528DST%2529%2520remains%2520a%250Achallenging%2520problem%2520in%2520task-oriented%2520dialogue%2520%2528TOD%2529%2520systems%252C%2520where%2520models%2520must%250Ageneralize%2520to%2520target%2520domains%2520unseen%2520at%2520training%2520time.%2520Current%2520large%2520language%250Amodel%2520approaches%2520for%2520zero-shot%2520domain%2520adaptation%2520rely%2520on%2520prompting%2520to%2520introduce%250Aknowledge%2520pertaining%2520to%2520the%2520target%2520domains.%2520However%252C%2520their%2520efficacy%2520strongly%250Adepends%2520on%2520prompt%2520engineering%252C%2520as%2520well%2520as%2520the%2520zero-shot%2520ability%2520of%2520the%250Aunderlying%2520language%2520model.%2520In%2520this%2520work%252C%2520we%2520devise%2520a%2520novel%2520data%2520augmentation%250Aapproach%252C%2520Schema%2520Augmentation%252C%2520that%2520improves%2520the%2520zero-shot%2520domain%2520adaptation%2520of%250Alanguage%2520models%2520through%2520fine-tuning.%2520Schema%2520Augmentation%2520is%2520a%2520simple%2520but%250Aeffective%2520technique%2520that%2520enhances%2520generalization%2520by%2520introducing%2520variations%2520of%250Aslot%2520names%2520within%2520the%2520schema%2520provided%2520in%2520the%2520prompt.%2520Experiments%2520on%2520MultiWOZ%250Aand%2520SpokenWOZ%2520showed%2520that%2520the%2520proposed%2520approach%2520resulted%2520in%2520a%2520substantial%250Aimprovement%2520over%2520the%2520baseline%252C%2520in%2520some%2520experiments%2520achieving%2520over%2520a%2520twofold%250Aaccuracy%2520gain%2520over%2520unseen%2520domains%2520while%2520maintaining%2520equal%2520or%2520superior%250Aperformance%2520over%2520all%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.00150v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Schema%20Augmentation%20for%20Zero-Shot%20Domain%20Adaptation%20in%20Dialogue%20State%0A%20%20Tracking&entry.906535625=Christopher%20Richardson%20and%20Roshan%20Sharma%20and%20Neeraj%20Gaur%20and%20Parisa%20Haghani%20and%20Anirudh%20Sundar%20and%20Bhuvana%20Ramabhadran&entry.1292438233=%20%20Zero-shot%20domain%20adaptation%20for%20dialogue%20state%20tracking%20%28DST%29%20remains%20a%0Achallenging%20problem%20in%20task-oriented%20dialogue%20%28TOD%29%20systems%2C%20where%20models%20must%0Ageneralize%20to%20target%20domains%20unseen%20at%20training%20time.%20Current%20large%20language%0Amodel%20approaches%20for%20zero-shot%20domain%20adaptation%20rely%20on%20prompting%20to%20introduce%0Aknowledge%20pertaining%20to%20the%20target%20domains.%20However%2C%20their%20efficacy%20strongly%0Adepends%20on%20prompt%20engineering%2C%20as%20well%20as%20the%20zero-shot%20ability%20of%20the%0Aunderlying%20language%20model.%20In%20this%20work%2C%20we%20devise%20a%20novel%20data%20augmentation%0Aapproach%2C%20Schema%20Augmentation%2C%20that%20improves%20the%20zero-shot%20domain%20adaptation%20of%0Alanguage%20models%20through%20fine-tuning.%20Schema%20Augmentation%20is%20a%20simple%20but%0Aeffective%20technique%20that%20enhances%20generalization%20by%20introducing%20variations%20of%0Aslot%20names%20within%20the%20schema%20provided%20in%20the%20prompt.%20Experiments%20on%20MultiWOZ%0Aand%20SpokenWOZ%20showed%20that%20the%20proposed%20approach%20resulted%20in%20a%20substantial%0Aimprovement%20over%20the%20baseline%2C%20in%20some%20experiments%20achieving%20over%20a%20twofold%0Aaccuracy%20gain%20over%20unseen%20domains%20while%20maintaining%20equal%20or%20superior%0Aperformance%20over%20all%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.00150v2&entry.124074799=Read"},
{"title": "Probe Pruning: Accelerating LLMs through Dynamic Pruning via\n  Model-Probing", "author": "Qi Le and Enmao Diao and Ziyan Wang and Xinran Wang and Jie Ding and Li Yang and Ali Anwar", "abstract": "  We introduce Probe Pruning (PP), a novel framework for online, dynamic,\nstructured pruning of Large Language Models (LLMs) applied in a batch-wise\nmanner. PP leverages the insight that not all samples and tokens contribute\nequally to the model's output, and probing a small portion of each batch\neffectively identifies crucial weights, enabling tailored dynamic pruning for\ndifferent batches. It comprises three main stages: probing, history-informed\npruning, and full inference. In the probing stage, PP selects a small yet\ncrucial set of hidden states, based on residual importance, to run a few model\nlayers ahead. During the history-informed pruning stage, PP strategically\nintegrates the probing states with historical states. Subsequently, it\nstructurally prunes weights based on the integrated states and the PP\nimportance score, a metric developed specifically to assess the importance of\neach weight channel in maintaining performance. In the final stage, full\ninference is conducted on the remaining weights. A major advantage of PP is its\ncompatibility with existing models, as it operates without requiring additional\nneural network modules or fine-tuning. Comprehensive evaluations of PP on\nLLaMA-2/3 and OPT models reveal that even minimal probing-using just 1.5% of\nFLOPs-can substantially enhance the efficiency of structured pruning of LLMs.\nFor instance, when evaluated on LLaMA-2-7B with WikiText2, PP achieves a 2.56\ntimes lower ratio of performance degradation per unit of runtime reduction\ncompared to the state-of-the-art method at a 40% pruning ratio. Our code is\navailable at https://github.com/Qi-Le1/Probe_Pruning.\n", "link": "http://arxiv.org/abs/2502.15618v1", "date": "2025-02-21", "relevancy": 2.4135, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4973}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4973}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probe%20Pruning%3A%20Accelerating%20LLMs%20through%20Dynamic%20Pruning%20via%0A%20%20Model-Probing&body=Title%3A%20Probe%20Pruning%3A%20Accelerating%20LLMs%20through%20Dynamic%20Pruning%20via%0A%20%20Model-Probing%0AAuthor%3A%20Qi%20Le%20and%20Enmao%20Diao%20and%20Ziyan%20Wang%20and%20Xinran%20Wang%20and%20Jie%20Ding%20and%20Li%20Yang%20and%20Ali%20Anwar%0AAbstract%3A%20%20%20We%20introduce%20Probe%20Pruning%20%28PP%29%2C%20a%20novel%20framework%20for%20online%2C%20dynamic%2C%0Astructured%20pruning%20of%20Large%20Language%20Models%20%28LLMs%29%20applied%20in%20a%20batch-wise%0Amanner.%20PP%20leverages%20the%20insight%20that%20not%20all%20samples%20and%20tokens%20contribute%0Aequally%20to%20the%20model%27s%20output%2C%20and%20probing%20a%20small%20portion%20of%20each%20batch%0Aeffectively%20identifies%20crucial%20weights%2C%20enabling%20tailored%20dynamic%20pruning%20for%0Adifferent%20batches.%20It%20comprises%20three%20main%20stages%3A%20probing%2C%20history-informed%0Apruning%2C%20and%20full%20inference.%20In%20the%20probing%20stage%2C%20PP%20selects%20a%20small%20yet%0Acrucial%20set%20of%20hidden%20states%2C%20based%20on%20residual%20importance%2C%20to%20run%20a%20few%20model%0Alayers%20ahead.%20During%20the%20history-informed%20pruning%20stage%2C%20PP%20strategically%0Aintegrates%20the%20probing%20states%20with%20historical%20states.%20Subsequently%2C%20it%0Astructurally%20prunes%20weights%20based%20on%20the%20integrated%20states%20and%20the%20PP%0Aimportance%20score%2C%20a%20metric%20developed%20specifically%20to%20assess%20the%20importance%20of%0Aeach%20weight%20channel%20in%20maintaining%20performance.%20In%20the%20final%20stage%2C%20full%0Ainference%20is%20conducted%20on%20the%20remaining%20weights.%20A%20major%20advantage%20of%20PP%20is%20its%0Acompatibility%20with%20existing%20models%2C%20as%20it%20operates%20without%20requiring%20additional%0Aneural%20network%20modules%20or%20fine-tuning.%20Comprehensive%20evaluations%20of%20PP%20on%0ALLaMA-2/3%20and%20OPT%20models%20reveal%20that%20even%20minimal%20probing-using%20just%201.5%25%20of%0AFLOPs-can%20substantially%20enhance%20the%20efficiency%20of%20structured%20pruning%20of%20LLMs.%0AFor%20instance%2C%20when%20evaluated%20on%20LLaMA-2-7B%20with%20WikiText2%2C%20PP%20achieves%20a%202.56%0Atimes%20lower%20ratio%20of%20performance%20degradation%20per%20unit%20of%20runtime%20reduction%0Acompared%20to%20the%20state-of-the-art%20method%20at%20a%2040%25%20pruning%20ratio.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/Qi-Le1/Probe_Pruning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15618v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbe%2520Pruning%253A%2520Accelerating%2520LLMs%2520through%2520Dynamic%2520Pruning%2520via%250A%2520%2520Model-Probing%26entry.906535625%3DQi%2520Le%2520and%2520Enmao%2520Diao%2520and%2520Ziyan%2520Wang%2520and%2520Xinran%2520Wang%2520and%2520Jie%2520Ding%2520and%2520Li%2520Yang%2520and%2520Ali%2520Anwar%26entry.1292438233%3D%2520%2520We%2520introduce%2520Probe%2520Pruning%2520%2528PP%2529%252C%2520a%2520novel%2520framework%2520for%2520online%252C%2520dynamic%252C%250Astructured%2520pruning%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520applied%2520in%2520a%2520batch-wise%250Amanner.%2520PP%2520leverages%2520the%2520insight%2520that%2520not%2520all%2520samples%2520and%2520tokens%2520contribute%250Aequally%2520to%2520the%2520model%2527s%2520output%252C%2520and%2520probing%2520a%2520small%2520portion%2520of%2520each%2520batch%250Aeffectively%2520identifies%2520crucial%2520weights%252C%2520enabling%2520tailored%2520dynamic%2520pruning%2520for%250Adifferent%2520batches.%2520It%2520comprises%2520three%2520main%2520stages%253A%2520probing%252C%2520history-informed%250Apruning%252C%2520and%2520full%2520inference.%2520In%2520the%2520probing%2520stage%252C%2520PP%2520selects%2520a%2520small%2520yet%250Acrucial%2520set%2520of%2520hidden%2520states%252C%2520based%2520on%2520residual%2520importance%252C%2520to%2520run%2520a%2520few%2520model%250Alayers%2520ahead.%2520During%2520the%2520history-informed%2520pruning%2520stage%252C%2520PP%2520strategically%250Aintegrates%2520the%2520probing%2520states%2520with%2520historical%2520states.%2520Subsequently%252C%2520it%250Astructurally%2520prunes%2520weights%2520based%2520on%2520the%2520integrated%2520states%2520and%2520the%2520PP%250Aimportance%2520score%252C%2520a%2520metric%2520developed%2520specifically%2520to%2520assess%2520the%2520importance%2520of%250Aeach%2520weight%2520channel%2520in%2520maintaining%2520performance.%2520In%2520the%2520final%2520stage%252C%2520full%250Ainference%2520is%2520conducted%2520on%2520the%2520remaining%2520weights.%2520A%2520major%2520advantage%2520of%2520PP%2520is%2520its%250Acompatibility%2520with%2520existing%2520models%252C%2520as%2520it%2520operates%2520without%2520requiring%2520additional%250Aneural%2520network%2520modules%2520or%2520fine-tuning.%2520Comprehensive%2520evaluations%2520of%2520PP%2520on%250ALLaMA-2/3%2520and%2520OPT%2520models%2520reveal%2520that%2520even%2520minimal%2520probing-using%2520just%25201.5%2525%2520of%250AFLOPs-can%2520substantially%2520enhance%2520the%2520efficiency%2520of%2520structured%2520pruning%2520of%2520LLMs.%250AFor%2520instance%252C%2520when%2520evaluated%2520on%2520LLaMA-2-7B%2520with%2520WikiText2%252C%2520PP%2520achieves%2520a%25202.56%250Atimes%2520lower%2520ratio%2520of%2520performance%2520degradation%2520per%2520unit%2520of%2520runtime%2520reduction%250Acompared%2520to%2520the%2520state-of-the-art%2520method%2520at%2520a%252040%2525%2520pruning%2520ratio.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/Qi-Le1/Probe_Pruning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15618v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probe%20Pruning%3A%20Accelerating%20LLMs%20through%20Dynamic%20Pruning%20via%0A%20%20Model-Probing&entry.906535625=Qi%20Le%20and%20Enmao%20Diao%20and%20Ziyan%20Wang%20and%20Xinran%20Wang%20and%20Jie%20Ding%20and%20Li%20Yang%20and%20Ali%20Anwar&entry.1292438233=%20%20We%20introduce%20Probe%20Pruning%20%28PP%29%2C%20a%20novel%20framework%20for%20online%2C%20dynamic%2C%0Astructured%20pruning%20of%20Large%20Language%20Models%20%28LLMs%29%20applied%20in%20a%20batch-wise%0Amanner.%20PP%20leverages%20the%20insight%20that%20not%20all%20samples%20and%20tokens%20contribute%0Aequally%20to%20the%20model%27s%20output%2C%20and%20probing%20a%20small%20portion%20of%20each%20batch%0Aeffectively%20identifies%20crucial%20weights%2C%20enabling%20tailored%20dynamic%20pruning%20for%0Adifferent%20batches.%20It%20comprises%20three%20main%20stages%3A%20probing%2C%20history-informed%0Apruning%2C%20and%20full%20inference.%20In%20the%20probing%20stage%2C%20PP%20selects%20a%20small%20yet%0Acrucial%20set%20of%20hidden%20states%2C%20based%20on%20residual%20importance%2C%20to%20run%20a%20few%20model%0Alayers%20ahead.%20During%20the%20history-informed%20pruning%20stage%2C%20PP%20strategically%0Aintegrates%20the%20probing%20states%20with%20historical%20states.%20Subsequently%2C%20it%0Astructurally%20prunes%20weights%20based%20on%20the%20integrated%20states%20and%20the%20PP%0Aimportance%20score%2C%20a%20metric%20developed%20specifically%20to%20assess%20the%20importance%20of%0Aeach%20weight%20channel%20in%20maintaining%20performance.%20In%20the%20final%20stage%2C%20full%0Ainference%20is%20conducted%20on%20the%20remaining%20weights.%20A%20major%20advantage%20of%20PP%20is%20its%0Acompatibility%20with%20existing%20models%2C%20as%20it%20operates%20without%20requiring%20additional%0Aneural%20network%20modules%20or%20fine-tuning.%20Comprehensive%20evaluations%20of%20PP%20on%0ALLaMA-2/3%20and%20OPT%20models%20reveal%20that%20even%20minimal%20probing-using%20just%201.5%25%20of%0AFLOPs-can%20substantially%20enhance%20the%20efficiency%20of%20structured%20pruning%20of%20LLMs.%0AFor%20instance%2C%20when%20evaluated%20on%20LLaMA-2-7B%20with%20WikiText2%2C%20PP%20achieves%20a%202.56%0Atimes%20lower%20ratio%20of%20performance%20degradation%20per%20unit%20of%20runtime%20reduction%0Acompared%20to%20the%20state-of-the-art%20method%20at%20a%2040%25%20pruning%20ratio.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/Qi-Le1/Probe_Pruning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15618v1&entry.124074799=Read"},
{"title": "Testing the limits of fine-tuning to improve reasoning in vision\n  language models", "author": "Luca M. Schulze Buschoff and Konstantinos Voudouris and Elif Akata and Matthias Bethge and Joshua B. Tenenbaum and Eric Schulz", "abstract": "  Pre-trained vision language models still fall short of human visual\ncognition. In an effort to improve visual cognition and align models with human\nbehavior, we introduce visual stimuli and human judgments on visual cognition\ntasks, allowing us to systematically evaluate performance across cognitive\ndomains under a consistent environment. We fine-tune models on ground truth\ndata for intuitive physics and causal reasoning and find that this improves\nmodel performance in the respective fine-tuning domain. Furthermore, it can\nimprove model alignment with human behavior. However, we find that fine-tuning\ndoes not contribute to robust human-like generalization to data with other\nvisual characteristics or to tasks in other cognitive domains.\n", "link": "http://arxiv.org/abs/2502.15678v1", "date": "2025-02-21", "relevancy": 2.4066, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6162}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6162}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Testing%20the%20limits%20of%20fine-tuning%20to%20improve%20reasoning%20in%20vision%0A%20%20language%20models&body=Title%3A%20Testing%20the%20limits%20of%20fine-tuning%20to%20improve%20reasoning%20in%20vision%0A%20%20language%20models%0AAuthor%3A%20Luca%20M.%20Schulze%20Buschoff%20and%20Konstantinos%20Voudouris%20and%20Elif%20Akata%20and%20Matthias%20Bethge%20and%20Joshua%20B.%20Tenenbaum%20and%20Eric%20Schulz%0AAbstract%3A%20%20%20Pre-trained%20vision%20language%20models%20still%20fall%20short%20of%20human%20visual%0Acognition.%20In%20an%20effort%20to%20improve%20visual%20cognition%20and%20align%20models%20with%20human%0Abehavior%2C%20we%20introduce%20visual%20stimuli%20and%20human%20judgments%20on%20visual%20cognition%0Atasks%2C%20allowing%20us%20to%20systematically%20evaluate%20performance%20across%20cognitive%0Adomains%20under%20a%20consistent%20environment.%20We%20fine-tune%20models%20on%20ground%20truth%0Adata%20for%20intuitive%20physics%20and%20causal%20reasoning%20and%20find%20that%20this%20improves%0Amodel%20performance%20in%20the%20respective%20fine-tuning%20domain.%20Furthermore%2C%20it%20can%0Aimprove%20model%20alignment%20with%20human%20behavior.%20However%2C%20we%20find%20that%20fine-tuning%0Adoes%20not%20contribute%20to%20robust%20human-like%20generalization%20to%20data%20with%20other%0Avisual%20characteristics%20or%20to%20tasks%20in%20other%20cognitive%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15678v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTesting%2520the%2520limits%2520of%2520fine-tuning%2520to%2520improve%2520reasoning%2520in%2520vision%250A%2520%2520language%2520models%26entry.906535625%3DLuca%2520M.%2520Schulze%2520Buschoff%2520and%2520Konstantinos%2520Voudouris%2520and%2520Elif%2520Akata%2520and%2520Matthias%2520Bethge%2520and%2520Joshua%2520B.%2520Tenenbaum%2520and%2520Eric%2520Schulz%26entry.1292438233%3D%2520%2520Pre-trained%2520vision%2520language%2520models%2520still%2520fall%2520short%2520of%2520human%2520visual%250Acognition.%2520In%2520an%2520effort%2520to%2520improve%2520visual%2520cognition%2520and%2520align%2520models%2520with%2520human%250Abehavior%252C%2520we%2520introduce%2520visual%2520stimuli%2520and%2520human%2520judgments%2520on%2520visual%2520cognition%250Atasks%252C%2520allowing%2520us%2520to%2520systematically%2520evaluate%2520performance%2520across%2520cognitive%250Adomains%2520under%2520a%2520consistent%2520environment.%2520We%2520fine-tune%2520models%2520on%2520ground%2520truth%250Adata%2520for%2520intuitive%2520physics%2520and%2520causal%2520reasoning%2520and%2520find%2520that%2520this%2520improves%250Amodel%2520performance%2520in%2520the%2520respective%2520fine-tuning%2520domain.%2520Furthermore%252C%2520it%2520can%250Aimprove%2520model%2520alignment%2520with%2520human%2520behavior.%2520However%252C%2520we%2520find%2520that%2520fine-tuning%250Adoes%2520not%2520contribute%2520to%2520robust%2520human-like%2520generalization%2520to%2520data%2520with%2520other%250Avisual%2520characteristics%2520or%2520to%2520tasks%2520in%2520other%2520cognitive%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15678v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Testing%20the%20limits%20of%20fine-tuning%20to%20improve%20reasoning%20in%20vision%0A%20%20language%20models&entry.906535625=Luca%20M.%20Schulze%20Buschoff%20and%20Konstantinos%20Voudouris%20and%20Elif%20Akata%20and%20Matthias%20Bethge%20and%20Joshua%20B.%20Tenenbaum%20and%20Eric%20Schulz&entry.1292438233=%20%20Pre-trained%20vision%20language%20models%20still%20fall%20short%20of%20human%20visual%0Acognition.%20In%20an%20effort%20to%20improve%20visual%20cognition%20and%20align%20models%20with%20human%0Abehavior%2C%20we%20introduce%20visual%20stimuli%20and%20human%20judgments%20on%20visual%20cognition%0Atasks%2C%20allowing%20us%20to%20systematically%20evaluate%20performance%20across%20cognitive%0Adomains%20under%20a%20consistent%20environment.%20We%20fine-tune%20models%20on%20ground%20truth%0Adata%20for%20intuitive%20physics%20and%20causal%20reasoning%20and%20find%20that%20this%20improves%0Amodel%20performance%20in%20the%20respective%20fine-tuning%20domain.%20Furthermore%2C%20it%20can%0Aimprove%20model%20alignment%20with%20human%20behavior.%20However%2C%20we%20find%20that%20fine-tuning%0Adoes%20not%20contribute%20to%20robust%20human-like%20generalization%20to%20data%20with%20other%0Avisual%20characteristics%20or%20to%20tasks%20in%20other%20cognitive%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15678v1&entry.124074799=Read"},
{"title": "Feature maps for the Laplacian kernel and its generalizations", "author": "Sudhendu Ahir and Parthe Pandit", "abstract": "  Recent applications of kernel methods in machine learning have seen a renewed\ninterest in the Laplacian kernel, due to its stability to the bandwidth\nhyperparameter in comparison to the Gaussian kernel, as well as its\nexpressivity being equivalent to that of the neural tangent kernel of deep\nfully connected networks. However, unlike the Gaussian kernel, the Laplacian\nkernel is not separable. This poses challenges for techniques to approximate\nit, especially via the random Fourier features (RFF) methodology and its\nvariants. In this work, we provide random features for the Laplacian kernel and\nits two generalizations: Mat\\'{e}rn kernel and the Exponential power kernel. We\nprovide efficiently implementable schemes to sample weight matrices so that\nrandom features approximate these kernels. These weight matrices have a weakly\ncoupled heavy-tailed randomness. Via numerical experiments on real datasets we\ndemonstrate the efficacy of these random feature maps.\n", "link": "http://arxiv.org/abs/2502.15575v1", "date": "2025-02-21", "relevancy": 2.4057, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4952}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4905}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature%20maps%20for%20the%20Laplacian%20kernel%20and%20its%20generalizations&body=Title%3A%20Feature%20maps%20for%20the%20Laplacian%20kernel%20and%20its%20generalizations%0AAuthor%3A%20Sudhendu%20Ahir%20and%20Parthe%20Pandit%0AAbstract%3A%20%20%20Recent%20applications%20of%20kernel%20methods%20in%20machine%20learning%20have%20seen%20a%20renewed%0Ainterest%20in%20the%20Laplacian%20kernel%2C%20due%20to%20its%20stability%20to%20the%20bandwidth%0Ahyperparameter%20in%20comparison%20to%20the%20Gaussian%20kernel%2C%20as%20well%20as%20its%0Aexpressivity%20being%20equivalent%20to%20that%20of%20the%20neural%20tangent%20kernel%20of%20deep%0Afully%20connected%20networks.%20However%2C%20unlike%20the%20Gaussian%20kernel%2C%20the%20Laplacian%0Akernel%20is%20not%20separable.%20This%20poses%20challenges%20for%20techniques%20to%20approximate%0Ait%2C%20especially%20via%20the%20random%20Fourier%20features%20%28RFF%29%20methodology%20and%20its%0Avariants.%20In%20this%20work%2C%20we%20provide%20random%20features%20for%20the%20Laplacian%20kernel%20and%0Aits%20two%20generalizations%3A%20Mat%5C%27%7Be%7Drn%20kernel%20and%20the%20Exponential%20power%20kernel.%20We%0Aprovide%20efficiently%20implementable%20schemes%20to%20sample%20weight%20matrices%20so%20that%0Arandom%20features%20approximate%20these%20kernels.%20These%20weight%20matrices%20have%20a%20weakly%0Acoupled%20heavy-tailed%20randomness.%20Via%20numerical%20experiments%20on%20real%20datasets%20we%0Ademonstrate%20the%20efficacy%20of%20these%20random%20feature%20maps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15575v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature%2520maps%2520for%2520the%2520Laplacian%2520kernel%2520and%2520its%2520generalizations%26entry.906535625%3DSudhendu%2520Ahir%2520and%2520Parthe%2520Pandit%26entry.1292438233%3D%2520%2520Recent%2520applications%2520of%2520kernel%2520methods%2520in%2520machine%2520learning%2520have%2520seen%2520a%2520renewed%250Ainterest%2520in%2520the%2520Laplacian%2520kernel%252C%2520due%2520to%2520its%2520stability%2520to%2520the%2520bandwidth%250Ahyperparameter%2520in%2520comparison%2520to%2520the%2520Gaussian%2520kernel%252C%2520as%2520well%2520as%2520its%250Aexpressivity%2520being%2520equivalent%2520to%2520that%2520of%2520the%2520neural%2520tangent%2520kernel%2520of%2520deep%250Afully%2520connected%2520networks.%2520However%252C%2520unlike%2520the%2520Gaussian%2520kernel%252C%2520the%2520Laplacian%250Akernel%2520is%2520not%2520separable.%2520This%2520poses%2520challenges%2520for%2520techniques%2520to%2520approximate%250Ait%252C%2520especially%2520via%2520the%2520random%2520Fourier%2520features%2520%2528RFF%2529%2520methodology%2520and%2520its%250Avariants.%2520In%2520this%2520work%252C%2520we%2520provide%2520random%2520features%2520for%2520the%2520Laplacian%2520kernel%2520and%250Aits%2520two%2520generalizations%253A%2520Mat%255C%2527%257Be%257Drn%2520kernel%2520and%2520the%2520Exponential%2520power%2520kernel.%2520We%250Aprovide%2520efficiently%2520implementable%2520schemes%2520to%2520sample%2520weight%2520matrices%2520so%2520that%250Arandom%2520features%2520approximate%2520these%2520kernels.%2520These%2520weight%2520matrices%2520have%2520a%2520weakly%250Acoupled%2520heavy-tailed%2520randomness.%2520Via%2520numerical%2520experiments%2520on%2520real%2520datasets%2520we%250Ademonstrate%2520the%2520efficacy%2520of%2520these%2520random%2520feature%2520maps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15575v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature%20maps%20for%20the%20Laplacian%20kernel%20and%20its%20generalizations&entry.906535625=Sudhendu%20Ahir%20and%20Parthe%20Pandit&entry.1292438233=%20%20Recent%20applications%20of%20kernel%20methods%20in%20machine%20learning%20have%20seen%20a%20renewed%0Ainterest%20in%20the%20Laplacian%20kernel%2C%20due%20to%20its%20stability%20to%20the%20bandwidth%0Ahyperparameter%20in%20comparison%20to%20the%20Gaussian%20kernel%2C%20as%20well%20as%20its%0Aexpressivity%20being%20equivalent%20to%20that%20of%20the%20neural%20tangent%20kernel%20of%20deep%0Afully%20connected%20networks.%20However%2C%20unlike%20the%20Gaussian%20kernel%2C%20the%20Laplacian%0Akernel%20is%20not%20separable.%20This%20poses%20challenges%20for%20techniques%20to%20approximate%0Ait%2C%20especially%20via%20the%20random%20Fourier%20features%20%28RFF%29%20methodology%20and%20its%0Avariants.%20In%20this%20work%2C%20we%20provide%20random%20features%20for%20the%20Laplacian%20kernel%20and%0Aits%20two%20generalizations%3A%20Mat%5C%27%7Be%7Drn%20kernel%20and%20the%20Exponential%20power%20kernel.%20We%0Aprovide%20efficiently%20implementable%20schemes%20to%20sample%20weight%20matrices%20so%20that%0Arandom%20features%20approximate%20these%20kernels.%20These%20weight%20matrices%20have%20a%20weakly%0Acoupled%20heavy-tailed%20randomness.%20Via%20numerical%20experiments%20on%20real%20datasets%20we%0Ademonstrate%20the%20efficacy%20of%20these%20random%20feature%20maps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15575v1&entry.124074799=Read"},
{"title": "Fully automatic extraction of morphological traits from the Web: utopia\n  or reality?", "author": "Diego Marcos and Robert van de Vlasakker and Ioannis N. Athanasiadis and Pierre Bonnet and Herv\u00e9 Goeau and Alexis Joly and W. Daniel Kissling and C\u00e9sar Leblanc and Andr\u00e9 S. J. van Proosdij and Konstantinos P. Panousis", "abstract": "  Plant morphological traits, their observable characteristics, are fundamental\nto understand the role played by each species within their ecosystem. However,\ncompiling trait information for even a moderate number of species is a\ndemanding task that may take experts years to accomplish. At the same time,\nmassive amounts of information about species descriptions is available online\nin the form of text, although the lack of structure makes this source of data\nimpossible to use at scale. To overcome this, we propose to leverage recent\nadvances in large language models (LLMs) and devise a mechanism for gathering\nand processing information on plant traits in the form of unstructured textual\ndescriptions, without manual curation. We evaluate our approach by\nautomatically replicating three manually created species-trait matrices. Our\nmethod managed to find values for over half of all species-trait pairs, with an\nF1-score of over 75%. Our results suggest that large-scale creation of\nstructured trait databases from unstructured online text is currently feasible\nthanks to the information extraction capabilities of LLMs, being limited by the\navailability of textual descriptions covering all the traits of interest.\n", "link": "http://arxiv.org/abs/2409.17179v2", "date": "2025-02-21", "relevancy": 2.3819, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4862}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4714}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fully%20automatic%20extraction%20of%20morphological%20traits%20from%20the%20Web%3A%20utopia%0A%20%20or%20reality%3F&body=Title%3A%20Fully%20automatic%20extraction%20of%20morphological%20traits%20from%20the%20Web%3A%20utopia%0A%20%20or%20reality%3F%0AAuthor%3A%20Diego%20Marcos%20and%20Robert%20van%20de%20Vlasakker%20and%20Ioannis%20N.%20Athanasiadis%20and%20Pierre%20Bonnet%20and%20Herv%C3%A9%20Goeau%20and%20Alexis%20Joly%20and%20W.%20Daniel%20Kissling%20and%20C%C3%A9sar%20Leblanc%20and%20Andr%C3%A9%20S.%20J.%20van%20Proosdij%20and%20Konstantinos%20P.%20Panousis%0AAbstract%3A%20%20%20Plant%20morphological%20traits%2C%20their%20observable%20characteristics%2C%20are%20fundamental%0Ato%20understand%20the%20role%20played%20by%20each%20species%20within%20their%20ecosystem.%20However%2C%0Acompiling%20trait%20information%20for%20even%20a%20moderate%20number%20of%20species%20is%20a%0Ademanding%20task%20that%20may%20take%20experts%20years%20to%20accomplish.%20At%20the%20same%20time%2C%0Amassive%20amounts%20of%20information%20about%20species%20descriptions%20is%20available%20online%0Ain%20the%20form%20of%20text%2C%20although%20the%20lack%20of%20structure%20makes%20this%20source%20of%20data%0Aimpossible%20to%20use%20at%20scale.%20To%20overcome%20this%2C%20we%20propose%20to%20leverage%20recent%0Aadvances%20in%20large%20language%20models%20%28LLMs%29%20and%20devise%20a%20mechanism%20for%20gathering%0Aand%20processing%20information%20on%20plant%20traits%20in%20the%20form%20of%20unstructured%20textual%0Adescriptions%2C%20without%20manual%20curation.%20We%20evaluate%20our%20approach%20by%0Aautomatically%20replicating%20three%20manually%20created%20species-trait%20matrices.%20Our%0Amethod%20managed%20to%20find%20values%20for%20over%20half%20of%20all%20species-trait%20pairs%2C%20with%20an%0AF1-score%20of%20over%2075%25.%20Our%20results%20suggest%20that%20large-scale%20creation%20of%0Astructured%20trait%20databases%20from%20unstructured%20online%20text%20is%20currently%20feasible%0Athanks%20to%20the%20information%20extraction%20capabilities%20of%20LLMs%2C%20being%20limited%20by%20the%0Aavailability%20of%20textual%20descriptions%20covering%20all%20the%20traits%20of%20interest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17179v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFully%2520automatic%2520extraction%2520of%2520morphological%2520traits%2520from%2520the%2520Web%253A%2520utopia%250A%2520%2520or%2520reality%253F%26entry.906535625%3DDiego%2520Marcos%2520and%2520Robert%2520van%2520de%2520Vlasakker%2520and%2520Ioannis%2520N.%2520Athanasiadis%2520and%2520Pierre%2520Bonnet%2520and%2520Herv%25C3%25A9%2520Goeau%2520and%2520Alexis%2520Joly%2520and%2520W.%2520Daniel%2520Kissling%2520and%2520C%25C3%25A9sar%2520Leblanc%2520and%2520Andr%25C3%25A9%2520S.%2520J.%2520van%2520Proosdij%2520and%2520Konstantinos%2520P.%2520Panousis%26entry.1292438233%3D%2520%2520Plant%2520morphological%2520traits%252C%2520their%2520observable%2520characteristics%252C%2520are%2520fundamental%250Ato%2520understand%2520the%2520role%2520played%2520by%2520each%2520species%2520within%2520their%2520ecosystem.%2520However%252C%250Acompiling%2520trait%2520information%2520for%2520even%2520a%2520moderate%2520number%2520of%2520species%2520is%2520a%250Ademanding%2520task%2520that%2520may%2520take%2520experts%2520years%2520to%2520accomplish.%2520At%2520the%2520same%2520time%252C%250Amassive%2520amounts%2520of%2520information%2520about%2520species%2520descriptions%2520is%2520available%2520online%250Ain%2520the%2520form%2520of%2520text%252C%2520although%2520the%2520lack%2520of%2520structure%2520makes%2520this%2520source%2520of%2520data%250Aimpossible%2520to%2520use%2520at%2520scale.%2520To%2520overcome%2520this%252C%2520we%2520propose%2520to%2520leverage%2520recent%250Aadvances%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520devise%2520a%2520mechanism%2520for%2520gathering%250Aand%2520processing%2520information%2520on%2520plant%2520traits%2520in%2520the%2520form%2520of%2520unstructured%2520textual%250Adescriptions%252C%2520without%2520manual%2520curation.%2520We%2520evaluate%2520our%2520approach%2520by%250Aautomatically%2520replicating%2520three%2520manually%2520created%2520species-trait%2520matrices.%2520Our%250Amethod%2520managed%2520to%2520find%2520values%2520for%2520over%2520half%2520of%2520all%2520species-trait%2520pairs%252C%2520with%2520an%250AF1-score%2520of%2520over%252075%2525.%2520Our%2520results%2520suggest%2520that%2520large-scale%2520creation%2520of%250Astructured%2520trait%2520databases%2520from%2520unstructured%2520online%2520text%2520is%2520currently%2520feasible%250Athanks%2520to%2520the%2520information%2520extraction%2520capabilities%2520of%2520LLMs%252C%2520being%2520limited%2520by%2520the%250Aavailability%2520of%2520textual%2520descriptions%2520covering%2520all%2520the%2520traits%2520of%2520interest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17179v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fully%20automatic%20extraction%20of%20morphological%20traits%20from%20the%20Web%3A%20utopia%0A%20%20or%20reality%3F&entry.906535625=Diego%20Marcos%20and%20Robert%20van%20de%20Vlasakker%20and%20Ioannis%20N.%20Athanasiadis%20and%20Pierre%20Bonnet%20and%20Herv%C3%A9%20Goeau%20and%20Alexis%20Joly%20and%20W.%20Daniel%20Kissling%20and%20C%C3%A9sar%20Leblanc%20and%20Andr%C3%A9%20S.%20J.%20van%20Proosdij%20and%20Konstantinos%20P.%20Panousis&entry.1292438233=%20%20Plant%20morphological%20traits%2C%20their%20observable%20characteristics%2C%20are%20fundamental%0Ato%20understand%20the%20role%20played%20by%20each%20species%20within%20their%20ecosystem.%20However%2C%0Acompiling%20trait%20information%20for%20even%20a%20moderate%20number%20of%20species%20is%20a%0Ademanding%20task%20that%20may%20take%20experts%20years%20to%20accomplish.%20At%20the%20same%20time%2C%0Amassive%20amounts%20of%20information%20about%20species%20descriptions%20is%20available%20online%0Ain%20the%20form%20of%20text%2C%20although%20the%20lack%20of%20structure%20makes%20this%20source%20of%20data%0Aimpossible%20to%20use%20at%20scale.%20To%20overcome%20this%2C%20we%20propose%20to%20leverage%20recent%0Aadvances%20in%20large%20language%20models%20%28LLMs%29%20and%20devise%20a%20mechanism%20for%20gathering%0Aand%20processing%20information%20on%20plant%20traits%20in%20the%20form%20of%20unstructured%20textual%0Adescriptions%2C%20without%20manual%20curation.%20We%20evaluate%20our%20approach%20by%0Aautomatically%20replicating%20three%20manually%20created%20species-trait%20matrices.%20Our%0Amethod%20managed%20to%20find%20values%20for%20over%20half%20of%20all%20species-trait%20pairs%2C%20with%20an%0AF1-score%20of%20over%2075%25.%20Our%20results%20suggest%20that%20large-scale%20creation%20of%0Astructured%20trait%20databases%20from%20unstructured%20online%20text%20is%20currently%20feasible%0Athanks%20to%20the%20information%20extraction%20capabilities%20of%20LLMs%2C%20being%20limited%20by%20the%0Aavailability%20of%20textual%20descriptions%20covering%20all%20the%20traits%20of%20interest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17179v2&entry.124074799=Read"},
{"title": "KAD: No More FAD! An Effective and Efficient Evaluation Metric for Audio\n  Generation", "author": "Yoonjin Chung and Pilsun Eu and Junwon Lee and Keunwoo Choi and Juhan Nam and Ben Sangbae Chon", "abstract": "  Although being widely adopted for evaluating generated audio signals, the\nFr\\'echet Audio Distance (FAD) suffers from significant limitations, including\nreliance on Gaussian assumptions, sensitivity to sample size, and high\ncomputational complexity. As an alternative, we introduce the Kernel Audio\nDistance (KAD), a novel, distribution-free, unbiased, and computationally\nefficient metric based on Maximum Mean Discrepancy (MMD). Through analysis and\nempirical validation, we demonstrate KAD's advantages: (1) faster convergence\nwith smaller sample sizes, enabling reliable evaluation with limited data; (2)\nlower computational cost, with scalable GPU acceleration; and (3) stronger\nalignment with human perceptual judgments. By leveraging advanced embeddings\nand characteristic kernels, KAD captures nuanced differences between real and\ngenerated audio. Open-sourced in the kadtk toolkit, KAD provides an efficient,\nreliable, and perceptually aligned benchmark for evaluating generative audio\nmodels.\n", "link": "http://arxiv.org/abs/2502.15602v1", "date": "2025-02-21", "relevancy": 2.3803, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4961}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4749}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KAD%3A%20No%20More%20FAD%21%20An%20Effective%20and%20Efficient%20Evaluation%20Metric%20for%20Audio%0A%20%20Generation&body=Title%3A%20KAD%3A%20No%20More%20FAD%21%20An%20Effective%20and%20Efficient%20Evaluation%20Metric%20for%20Audio%0A%20%20Generation%0AAuthor%3A%20Yoonjin%20Chung%20and%20Pilsun%20Eu%20and%20Junwon%20Lee%20and%20Keunwoo%20Choi%20and%20Juhan%20Nam%20and%20Ben%20Sangbae%20Chon%0AAbstract%3A%20%20%20Although%20being%20widely%20adopted%20for%20evaluating%20generated%20audio%20signals%2C%20the%0AFr%5C%27echet%20Audio%20Distance%20%28FAD%29%20suffers%20from%20significant%20limitations%2C%20including%0Areliance%20on%20Gaussian%20assumptions%2C%20sensitivity%20to%20sample%20size%2C%20and%20high%0Acomputational%20complexity.%20As%20an%20alternative%2C%20we%20introduce%20the%20Kernel%20Audio%0ADistance%20%28KAD%29%2C%20a%20novel%2C%20distribution-free%2C%20unbiased%2C%20and%20computationally%0Aefficient%20metric%20based%20on%20Maximum%20Mean%20Discrepancy%20%28MMD%29.%20Through%20analysis%20and%0Aempirical%20validation%2C%20we%20demonstrate%20KAD%27s%20advantages%3A%20%281%29%20faster%20convergence%0Awith%20smaller%20sample%20sizes%2C%20enabling%20reliable%20evaluation%20with%20limited%20data%3B%20%282%29%0Alower%20computational%20cost%2C%20with%20scalable%20GPU%20acceleration%3B%20and%20%283%29%20stronger%0Aalignment%20with%20human%20perceptual%20judgments.%20By%20leveraging%20advanced%20embeddings%0Aand%20characteristic%20kernels%2C%20KAD%20captures%20nuanced%20differences%20between%20real%20and%0Agenerated%20audio.%20Open-sourced%20in%20the%20kadtk%20toolkit%2C%20KAD%20provides%20an%20efficient%2C%0Areliable%2C%20and%20perceptually%20aligned%20benchmark%20for%20evaluating%20generative%20audio%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15602v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKAD%253A%2520No%2520More%2520FAD%2521%2520An%2520Effective%2520and%2520Efficient%2520Evaluation%2520Metric%2520for%2520Audio%250A%2520%2520Generation%26entry.906535625%3DYoonjin%2520Chung%2520and%2520Pilsun%2520Eu%2520and%2520Junwon%2520Lee%2520and%2520Keunwoo%2520Choi%2520and%2520Juhan%2520Nam%2520and%2520Ben%2520Sangbae%2520Chon%26entry.1292438233%3D%2520%2520Although%2520being%2520widely%2520adopted%2520for%2520evaluating%2520generated%2520audio%2520signals%252C%2520the%250AFr%255C%2527echet%2520Audio%2520Distance%2520%2528FAD%2529%2520suffers%2520from%2520significant%2520limitations%252C%2520including%250Areliance%2520on%2520Gaussian%2520assumptions%252C%2520sensitivity%2520to%2520sample%2520size%252C%2520and%2520high%250Acomputational%2520complexity.%2520As%2520an%2520alternative%252C%2520we%2520introduce%2520the%2520Kernel%2520Audio%250ADistance%2520%2528KAD%2529%252C%2520a%2520novel%252C%2520distribution-free%252C%2520unbiased%252C%2520and%2520computationally%250Aefficient%2520metric%2520based%2520on%2520Maximum%2520Mean%2520Discrepancy%2520%2528MMD%2529.%2520Through%2520analysis%2520and%250Aempirical%2520validation%252C%2520we%2520demonstrate%2520KAD%2527s%2520advantages%253A%2520%25281%2529%2520faster%2520convergence%250Awith%2520smaller%2520sample%2520sizes%252C%2520enabling%2520reliable%2520evaluation%2520with%2520limited%2520data%253B%2520%25282%2529%250Alower%2520computational%2520cost%252C%2520with%2520scalable%2520GPU%2520acceleration%253B%2520and%2520%25283%2529%2520stronger%250Aalignment%2520with%2520human%2520perceptual%2520judgments.%2520By%2520leveraging%2520advanced%2520embeddings%250Aand%2520characteristic%2520kernels%252C%2520KAD%2520captures%2520nuanced%2520differences%2520between%2520real%2520and%250Agenerated%2520audio.%2520Open-sourced%2520in%2520the%2520kadtk%2520toolkit%252C%2520KAD%2520provides%2520an%2520efficient%252C%250Areliable%252C%2520and%2520perceptually%2520aligned%2520benchmark%2520for%2520evaluating%2520generative%2520audio%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15602v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KAD%3A%20No%20More%20FAD%21%20An%20Effective%20and%20Efficient%20Evaluation%20Metric%20for%20Audio%0A%20%20Generation&entry.906535625=Yoonjin%20Chung%20and%20Pilsun%20Eu%20and%20Junwon%20Lee%20and%20Keunwoo%20Choi%20and%20Juhan%20Nam%20and%20Ben%20Sangbae%20Chon&entry.1292438233=%20%20Although%20being%20widely%20adopted%20for%20evaluating%20generated%20audio%20signals%2C%20the%0AFr%5C%27echet%20Audio%20Distance%20%28FAD%29%20suffers%20from%20significant%20limitations%2C%20including%0Areliance%20on%20Gaussian%20assumptions%2C%20sensitivity%20to%20sample%20size%2C%20and%20high%0Acomputational%20complexity.%20As%20an%20alternative%2C%20we%20introduce%20the%20Kernel%20Audio%0ADistance%20%28KAD%29%2C%20a%20novel%2C%20distribution-free%2C%20unbiased%2C%20and%20computationally%0Aefficient%20metric%20based%20on%20Maximum%20Mean%20Discrepancy%20%28MMD%29.%20Through%20analysis%20and%0Aempirical%20validation%2C%20we%20demonstrate%20KAD%27s%20advantages%3A%20%281%29%20faster%20convergence%0Awith%20smaller%20sample%20sizes%2C%20enabling%20reliable%20evaluation%20with%20limited%20data%3B%20%282%29%0Alower%20computational%20cost%2C%20with%20scalable%20GPU%20acceleration%3B%20and%20%283%29%20stronger%0Aalignment%20with%20human%20perceptual%20judgments.%20By%20leveraging%20advanced%20embeddings%0Aand%20characteristic%20kernels%2C%20KAD%20captures%20nuanced%20differences%20between%20real%20and%0Agenerated%20audio.%20Open-sourced%20in%20the%20kadtk%20toolkit%2C%20KAD%20provides%20an%20efficient%2C%0Areliable%2C%20and%20perceptually%20aligned%20benchmark%20for%20evaluating%20generative%20audio%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15602v1&entry.124074799=Read"},
{"title": "DeepInteraction++: Multi-Modality Interaction for Autonomous Driving", "author": "Zeyu Yang and Nan Song and Wei Li and Xiatian Zhu and Li Zhang and Philip H. S. Torr", "abstract": "  Existing top-performance autonomous driving systems typically rely on the\nmulti-modal fusion strategy for reliable scene understanding. This design is\nhowever fundamentally restricted due to overlooking the modality-specific\nstrengths and finally hampering the model performance. To address this\nlimitation, in this work, we introduce a novel modality interaction strategy\nthat allows individual per-modality representations to be learned and\nmaintained throughout, enabling their unique characteristics to be exploited\nduring the whole perception pipeline. To demonstrate the effectiveness of the\nproposed strategy, we design DeepInteraction++, a multi-modal interaction\nframework characterized by a multi-modal representational interaction encoder\nand a multi-modal predictive interaction decoder. Specifically, the encoder is\nimplemented as a dual-stream Transformer with specialized attention operation\nfor information exchange and integration between separate modality-specific\nrepresentations. Our multi-modal representational learning incorporates both\nobject-centric, precise sampling-based feature alignment and global dense\ninformation spreading, essential for the more challenging planning task. The\ndecoder is designed to iteratively refine the predictions by alternately\naggregating information from separate representations in a unified\nmodality-agnostic manner, realizing multi-modal predictive interaction.\nExtensive experiments demonstrate the superior performance of the proposed\nframework on both 3D object detection and end-to-end autonomous driving tasks.\nOur code is available at https://github.com/fudan-zvg/DeepInteraction.\n", "link": "http://arxiv.org/abs/2408.05075v3", "date": "2025-02-21", "relevancy": 2.3754, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.623}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5985}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepInteraction%2B%2B%3A%20Multi-Modality%20Interaction%20for%20Autonomous%20Driving&body=Title%3A%20DeepInteraction%2B%2B%3A%20Multi-Modality%20Interaction%20for%20Autonomous%20Driving%0AAuthor%3A%20Zeyu%20Yang%20and%20Nan%20Song%20and%20Wei%20Li%20and%20Xiatian%20Zhu%20and%20Li%20Zhang%20and%20Philip%20H.%20S.%20Torr%0AAbstract%3A%20%20%20Existing%20top-performance%20autonomous%20driving%20systems%20typically%20rely%20on%20the%0Amulti-modal%20fusion%20strategy%20for%20reliable%20scene%20understanding.%20This%20design%20is%0Ahowever%20fundamentally%20restricted%20due%20to%20overlooking%20the%20modality-specific%0Astrengths%20and%20finally%20hampering%20the%20model%20performance.%20To%20address%20this%0Alimitation%2C%20in%20this%20work%2C%20we%20introduce%20a%20novel%20modality%20interaction%20strategy%0Athat%20allows%20individual%20per-modality%20representations%20to%20be%20learned%20and%0Amaintained%20throughout%2C%20enabling%20their%20unique%20characteristics%20to%20be%20exploited%0Aduring%20the%20whole%20perception%20pipeline.%20To%20demonstrate%20the%20effectiveness%20of%20the%0Aproposed%20strategy%2C%20we%20design%20DeepInteraction%2B%2B%2C%20a%20multi-modal%20interaction%0Aframework%20characterized%20by%20a%20multi-modal%20representational%20interaction%20encoder%0Aand%20a%20multi-modal%20predictive%20interaction%20decoder.%20Specifically%2C%20the%20encoder%20is%0Aimplemented%20as%20a%20dual-stream%20Transformer%20with%20specialized%20attention%20operation%0Afor%20information%20exchange%20and%20integration%20between%20separate%20modality-specific%0Arepresentations.%20Our%20multi-modal%20representational%20learning%20incorporates%20both%0Aobject-centric%2C%20precise%20sampling-based%20feature%20alignment%20and%20global%20dense%0Ainformation%20spreading%2C%20essential%20for%20the%20more%20challenging%20planning%20task.%20The%0Adecoder%20is%20designed%20to%20iteratively%20refine%20the%20predictions%20by%20alternately%0Aaggregating%20information%20from%20separate%20representations%20in%20a%20unified%0Amodality-agnostic%20manner%2C%20realizing%20multi-modal%20predictive%20interaction.%0AExtensive%20experiments%20demonstrate%20the%20superior%20performance%20of%20the%20proposed%0Aframework%20on%20both%203D%20object%20detection%20and%20end-to-end%20autonomous%20driving%20tasks.%0AOur%20code%20is%20available%20at%20https%3A//github.com/fudan-zvg/DeepInteraction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05075v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepInteraction%252B%252B%253A%2520Multi-Modality%2520Interaction%2520for%2520Autonomous%2520Driving%26entry.906535625%3DZeyu%2520Yang%2520and%2520Nan%2520Song%2520and%2520Wei%2520Li%2520and%2520Xiatian%2520Zhu%2520and%2520Li%2520Zhang%2520and%2520Philip%2520H.%2520S.%2520Torr%26entry.1292438233%3D%2520%2520Existing%2520top-performance%2520autonomous%2520driving%2520systems%2520typically%2520rely%2520on%2520the%250Amulti-modal%2520fusion%2520strategy%2520for%2520reliable%2520scene%2520understanding.%2520This%2520design%2520is%250Ahowever%2520fundamentally%2520restricted%2520due%2520to%2520overlooking%2520the%2520modality-specific%250Astrengths%2520and%2520finally%2520hampering%2520the%2520model%2520performance.%2520To%2520address%2520this%250Alimitation%252C%2520in%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520modality%2520interaction%2520strategy%250Athat%2520allows%2520individual%2520per-modality%2520representations%2520to%2520be%2520learned%2520and%250Amaintained%2520throughout%252C%2520enabling%2520their%2520unique%2520characteristics%2520to%2520be%2520exploited%250Aduring%2520the%2520whole%2520perception%2520pipeline.%2520To%2520demonstrate%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520strategy%252C%2520we%2520design%2520DeepInteraction%252B%252B%252C%2520a%2520multi-modal%2520interaction%250Aframework%2520characterized%2520by%2520a%2520multi-modal%2520representational%2520interaction%2520encoder%250Aand%2520a%2520multi-modal%2520predictive%2520interaction%2520decoder.%2520Specifically%252C%2520the%2520encoder%2520is%250Aimplemented%2520as%2520a%2520dual-stream%2520Transformer%2520with%2520specialized%2520attention%2520operation%250Afor%2520information%2520exchange%2520and%2520integration%2520between%2520separate%2520modality-specific%250Arepresentations.%2520Our%2520multi-modal%2520representational%2520learning%2520incorporates%2520both%250Aobject-centric%252C%2520precise%2520sampling-based%2520feature%2520alignment%2520and%2520global%2520dense%250Ainformation%2520spreading%252C%2520essential%2520for%2520the%2520more%2520challenging%2520planning%2520task.%2520The%250Adecoder%2520is%2520designed%2520to%2520iteratively%2520refine%2520the%2520predictions%2520by%2520alternately%250Aaggregating%2520information%2520from%2520separate%2520representations%2520in%2520a%2520unified%250Amodality-agnostic%2520manner%252C%2520realizing%2520multi-modal%2520predictive%2520interaction.%250AExtensive%2520experiments%2520demonstrate%2520the%2520superior%2520performance%2520of%2520the%2520proposed%250Aframework%2520on%2520both%25203D%2520object%2520detection%2520and%2520end-to-end%2520autonomous%2520driving%2520tasks.%250AOur%2520code%2520is%2520available%2520at%2520https%253A//github.com/fudan-zvg/DeepInteraction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05075v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepInteraction%2B%2B%3A%20Multi-Modality%20Interaction%20for%20Autonomous%20Driving&entry.906535625=Zeyu%20Yang%20and%20Nan%20Song%20and%20Wei%20Li%20and%20Xiatian%20Zhu%20and%20Li%20Zhang%20and%20Philip%20H.%20S.%20Torr&entry.1292438233=%20%20Existing%20top-performance%20autonomous%20driving%20systems%20typically%20rely%20on%20the%0Amulti-modal%20fusion%20strategy%20for%20reliable%20scene%20understanding.%20This%20design%20is%0Ahowever%20fundamentally%20restricted%20due%20to%20overlooking%20the%20modality-specific%0Astrengths%20and%20finally%20hampering%20the%20model%20performance.%20To%20address%20this%0Alimitation%2C%20in%20this%20work%2C%20we%20introduce%20a%20novel%20modality%20interaction%20strategy%0Athat%20allows%20individual%20per-modality%20representations%20to%20be%20learned%20and%0Amaintained%20throughout%2C%20enabling%20their%20unique%20characteristics%20to%20be%20exploited%0Aduring%20the%20whole%20perception%20pipeline.%20To%20demonstrate%20the%20effectiveness%20of%20the%0Aproposed%20strategy%2C%20we%20design%20DeepInteraction%2B%2B%2C%20a%20multi-modal%20interaction%0Aframework%20characterized%20by%20a%20multi-modal%20representational%20interaction%20encoder%0Aand%20a%20multi-modal%20predictive%20interaction%20decoder.%20Specifically%2C%20the%20encoder%20is%0Aimplemented%20as%20a%20dual-stream%20Transformer%20with%20specialized%20attention%20operation%0Afor%20information%20exchange%20and%20integration%20between%20separate%20modality-specific%0Arepresentations.%20Our%20multi-modal%20representational%20learning%20incorporates%20both%0Aobject-centric%2C%20precise%20sampling-based%20feature%20alignment%20and%20global%20dense%0Ainformation%20spreading%2C%20essential%20for%20the%20more%20challenging%20planning%20task.%20The%0Adecoder%20is%20designed%20to%20iteratively%20refine%20the%20predictions%20by%20alternately%0Aaggregating%20information%20from%20separate%20representations%20in%20a%20unified%0Amodality-agnostic%20manner%2C%20realizing%20multi-modal%20predictive%20interaction.%0AExtensive%20experiments%20demonstrate%20the%20superior%20performance%20of%20the%20proposed%0Aframework%20on%20both%203D%20object%20detection%20and%20end-to-end%20autonomous%20driving%20tasks.%0AOur%20code%20is%20available%20at%20https%3A//github.com/fudan-zvg/DeepInteraction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05075v3&entry.124074799=Read"},
{"title": "Reduced-Order Model Guided Contact-Implicit Model Predictive Control for\n  Humanoid Locomotion", "author": "Sergio A. Esteban and Vince Kurtz and Adrian B. Ghansah and Aaron D. Ames", "abstract": "  Humanoid robots have great potential for real-world applications due to their\nability to operate in environments built for humans, but their deployment is\nhindered by the challenge of controlling their underlying high-dimensional\nnonlinear hybrid dynamics. While reduced-order models like the Hybrid Linear\nInverted Pendulum (HLIP) are simple and computationally efficient, they lose\nwhole-body expressiveness. Meanwhile, recent advances in Contact-Implicit Model\nPredictive Control (CI-MPC) enable robots to plan through multiple hybrid\ncontact modes, but remain vulnerable to local minima and require significant\ntuning. We propose a control framework that combines the strengths of HLIP and\nCI-MPC. The reduced-order model generates a nominal gait, while CI-MPC manages\nthe whole-body dynamics and modifies the contact schedule as needed. We\ndemonstrate the effectiveness of this approach in simulation with a novel 24\ndegree-of-freedom humanoid robot: Achilles. Our proposed framework achieves\nrough terrain walking, disturbance recovery, robustness under model and state\nuncertainty, and allows the robot to interact with obstacles in the\nenvironment, all while running online in real-time at 50 Hz.\n", "link": "http://arxiv.org/abs/2502.15630v1", "date": "2025-02-21", "relevancy": 2.3712, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6173}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5899}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reduced-Order%20Model%20Guided%20Contact-Implicit%20Model%20Predictive%20Control%20for%0A%20%20Humanoid%20Locomotion&body=Title%3A%20Reduced-Order%20Model%20Guided%20Contact-Implicit%20Model%20Predictive%20Control%20for%0A%20%20Humanoid%20Locomotion%0AAuthor%3A%20Sergio%20A.%20Esteban%20and%20Vince%20Kurtz%20and%20Adrian%20B.%20Ghansah%20and%20Aaron%20D.%20Ames%0AAbstract%3A%20%20%20Humanoid%20robots%20have%20great%20potential%20for%20real-world%20applications%20due%20to%20their%0Aability%20to%20operate%20in%20environments%20built%20for%20humans%2C%20but%20their%20deployment%20is%0Ahindered%20by%20the%20challenge%20of%20controlling%20their%20underlying%20high-dimensional%0Anonlinear%20hybrid%20dynamics.%20While%20reduced-order%20models%20like%20the%20Hybrid%20Linear%0AInverted%20Pendulum%20%28HLIP%29%20are%20simple%20and%20computationally%20efficient%2C%20they%20lose%0Awhole-body%20expressiveness.%20Meanwhile%2C%20recent%20advances%20in%20Contact-Implicit%20Model%0APredictive%20Control%20%28CI-MPC%29%20enable%20robots%20to%20plan%20through%20multiple%20hybrid%0Acontact%20modes%2C%20but%20remain%20vulnerable%20to%20local%20minima%20and%20require%20significant%0Atuning.%20We%20propose%20a%20control%20framework%20that%20combines%20the%20strengths%20of%20HLIP%20and%0ACI-MPC.%20The%20reduced-order%20model%20generates%20a%20nominal%20gait%2C%20while%20CI-MPC%20manages%0Athe%20whole-body%20dynamics%20and%20modifies%20the%20contact%20schedule%20as%20needed.%20We%0Ademonstrate%20the%20effectiveness%20of%20this%20approach%20in%20simulation%20with%20a%20novel%2024%0Adegree-of-freedom%20humanoid%20robot%3A%20Achilles.%20Our%20proposed%20framework%20achieves%0Arough%20terrain%20walking%2C%20disturbance%20recovery%2C%20robustness%20under%20model%20and%20state%0Auncertainty%2C%20and%20allows%20the%20robot%20to%20interact%20with%20obstacles%20in%20the%0Aenvironment%2C%20all%20while%20running%20online%20in%20real-time%20at%2050%20Hz.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15630v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReduced-Order%2520Model%2520Guided%2520Contact-Implicit%2520Model%2520Predictive%2520Control%2520for%250A%2520%2520Humanoid%2520Locomotion%26entry.906535625%3DSergio%2520A.%2520Esteban%2520and%2520Vince%2520Kurtz%2520and%2520Adrian%2520B.%2520Ghansah%2520and%2520Aaron%2520D.%2520Ames%26entry.1292438233%3D%2520%2520Humanoid%2520robots%2520have%2520great%2520potential%2520for%2520real-world%2520applications%2520due%2520to%2520their%250Aability%2520to%2520operate%2520in%2520environments%2520built%2520for%2520humans%252C%2520but%2520their%2520deployment%2520is%250Ahindered%2520by%2520the%2520challenge%2520of%2520controlling%2520their%2520underlying%2520high-dimensional%250Anonlinear%2520hybrid%2520dynamics.%2520While%2520reduced-order%2520models%2520like%2520the%2520Hybrid%2520Linear%250AInverted%2520Pendulum%2520%2528HLIP%2529%2520are%2520simple%2520and%2520computationally%2520efficient%252C%2520they%2520lose%250Awhole-body%2520expressiveness.%2520Meanwhile%252C%2520recent%2520advances%2520in%2520Contact-Implicit%2520Model%250APredictive%2520Control%2520%2528CI-MPC%2529%2520enable%2520robots%2520to%2520plan%2520through%2520multiple%2520hybrid%250Acontact%2520modes%252C%2520but%2520remain%2520vulnerable%2520to%2520local%2520minima%2520and%2520require%2520significant%250Atuning.%2520We%2520propose%2520a%2520control%2520framework%2520that%2520combines%2520the%2520strengths%2520of%2520HLIP%2520and%250ACI-MPC.%2520The%2520reduced-order%2520model%2520generates%2520a%2520nominal%2520gait%252C%2520while%2520CI-MPC%2520manages%250Athe%2520whole-body%2520dynamics%2520and%2520modifies%2520the%2520contact%2520schedule%2520as%2520needed.%2520We%250Ademonstrate%2520the%2520effectiveness%2520of%2520this%2520approach%2520in%2520simulation%2520with%2520a%2520novel%252024%250Adegree-of-freedom%2520humanoid%2520robot%253A%2520Achilles.%2520Our%2520proposed%2520framework%2520achieves%250Arough%2520terrain%2520walking%252C%2520disturbance%2520recovery%252C%2520robustness%2520under%2520model%2520and%2520state%250Auncertainty%252C%2520and%2520allows%2520the%2520robot%2520to%2520interact%2520with%2520obstacles%2520in%2520the%250Aenvironment%252C%2520all%2520while%2520running%2520online%2520in%2520real-time%2520at%252050%2520Hz.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15630v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reduced-Order%20Model%20Guided%20Contact-Implicit%20Model%20Predictive%20Control%20for%0A%20%20Humanoid%20Locomotion&entry.906535625=Sergio%20A.%20Esteban%20and%20Vince%20Kurtz%20and%20Adrian%20B.%20Ghansah%20and%20Aaron%20D.%20Ames&entry.1292438233=%20%20Humanoid%20robots%20have%20great%20potential%20for%20real-world%20applications%20due%20to%20their%0Aability%20to%20operate%20in%20environments%20built%20for%20humans%2C%20but%20their%20deployment%20is%0Ahindered%20by%20the%20challenge%20of%20controlling%20their%20underlying%20high-dimensional%0Anonlinear%20hybrid%20dynamics.%20While%20reduced-order%20models%20like%20the%20Hybrid%20Linear%0AInverted%20Pendulum%20%28HLIP%29%20are%20simple%20and%20computationally%20efficient%2C%20they%20lose%0Awhole-body%20expressiveness.%20Meanwhile%2C%20recent%20advances%20in%20Contact-Implicit%20Model%0APredictive%20Control%20%28CI-MPC%29%20enable%20robots%20to%20plan%20through%20multiple%20hybrid%0Acontact%20modes%2C%20but%20remain%20vulnerable%20to%20local%20minima%20and%20require%20significant%0Atuning.%20We%20propose%20a%20control%20framework%20that%20combines%20the%20strengths%20of%20HLIP%20and%0ACI-MPC.%20The%20reduced-order%20model%20generates%20a%20nominal%20gait%2C%20while%20CI-MPC%20manages%0Athe%20whole-body%20dynamics%20and%20modifies%20the%20contact%20schedule%20as%20needed.%20We%0Ademonstrate%20the%20effectiveness%20of%20this%20approach%20in%20simulation%20with%20a%20novel%2024%0Adegree-of-freedom%20humanoid%20robot%3A%20Achilles.%20Our%20proposed%20framework%20achieves%0Arough%20terrain%20walking%2C%20disturbance%20recovery%2C%20robustness%20under%20model%20and%20state%0Auncertainty%2C%20and%20allows%20the%20robot%20to%20interact%20with%20obstacles%20in%20the%0Aenvironment%2C%20all%20while%20running%20online%20in%20real-time%20at%2050%20Hz.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15630v1&entry.124074799=Read"},
{"title": "Do Multilingual LLMs Think In English?", "author": "Lisa Schut and Yarin Gal and Sebastian Farquhar", "abstract": "  Large language models (LLMs) have multilingual capabilities and can solve\ntasks across various languages. However, we show that current LLMs make key\ndecisions in a representation space closest to English, regardless of their\ninput and output languages. Exploring the internal representations with a logit\nlens for sentences in French, German, Dutch, and Mandarin, we show that the LLM\nfirst emits representations close to English for semantically-loaded words\nbefore translating them into the target language. We further show that\nactivation steering in these LLMs is more effective when the steering vectors\nare computed in English rather than in the language of the inputs and outputs.\nThis suggests that multilingual LLMs perform key reasoning steps in a\nrepresentation that is heavily shaped by English in a way that is not\ntransparent to system users.\n", "link": "http://arxiv.org/abs/2502.15603v1", "date": "2025-02-21", "relevancy": 2.3686, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4964}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4964}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Multilingual%20LLMs%20Think%20In%20English%3F&body=Title%3A%20Do%20Multilingual%20LLMs%20Think%20In%20English%3F%0AAuthor%3A%20Lisa%20Schut%20and%20Yarin%20Gal%20and%20Sebastian%20Farquhar%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20multilingual%20capabilities%20and%20can%20solve%0Atasks%20across%20various%20languages.%20However%2C%20we%20show%20that%20current%20LLMs%20make%20key%0Adecisions%20in%20a%20representation%20space%20closest%20to%20English%2C%20regardless%20of%20their%0Ainput%20and%20output%20languages.%20Exploring%20the%20internal%20representations%20with%20a%20logit%0Alens%20for%20sentences%20in%20French%2C%20German%2C%20Dutch%2C%20and%20Mandarin%2C%20we%20show%20that%20the%20LLM%0Afirst%20emits%20representations%20close%20to%20English%20for%20semantically-loaded%20words%0Abefore%20translating%20them%20into%20the%20target%20language.%20We%20further%20show%20that%0Aactivation%20steering%20in%20these%20LLMs%20is%20more%20effective%20when%20the%20steering%20vectors%0Aare%20computed%20in%20English%20rather%20than%20in%20the%20language%20of%20the%20inputs%20and%20outputs.%0AThis%20suggests%20that%20multilingual%20LLMs%20perform%20key%20reasoning%20steps%20in%20a%0Arepresentation%20that%20is%20heavily%20shaped%20by%20English%20in%20a%20way%20that%20is%20not%0Atransparent%20to%20system%20users.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15603v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Multilingual%2520LLMs%2520Think%2520In%2520English%253F%26entry.906535625%3DLisa%2520Schut%2520and%2520Yarin%2520Gal%2520and%2520Sebastian%2520Farquhar%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520multilingual%2520capabilities%2520and%2520can%2520solve%250Atasks%2520across%2520various%2520languages.%2520However%252C%2520we%2520show%2520that%2520current%2520LLMs%2520make%2520key%250Adecisions%2520in%2520a%2520representation%2520space%2520closest%2520to%2520English%252C%2520regardless%2520of%2520their%250Ainput%2520and%2520output%2520languages.%2520Exploring%2520the%2520internal%2520representations%2520with%2520a%2520logit%250Alens%2520for%2520sentences%2520in%2520French%252C%2520German%252C%2520Dutch%252C%2520and%2520Mandarin%252C%2520we%2520show%2520that%2520the%2520LLM%250Afirst%2520emits%2520representations%2520close%2520to%2520English%2520for%2520semantically-loaded%2520words%250Abefore%2520translating%2520them%2520into%2520the%2520target%2520language.%2520We%2520further%2520show%2520that%250Aactivation%2520steering%2520in%2520these%2520LLMs%2520is%2520more%2520effective%2520when%2520the%2520steering%2520vectors%250Aare%2520computed%2520in%2520English%2520rather%2520than%2520in%2520the%2520language%2520of%2520the%2520inputs%2520and%2520outputs.%250AThis%2520suggests%2520that%2520multilingual%2520LLMs%2520perform%2520key%2520reasoning%2520steps%2520in%2520a%250Arepresentation%2520that%2520is%2520heavily%2520shaped%2520by%2520English%2520in%2520a%2520way%2520that%2520is%2520not%250Atransparent%2520to%2520system%2520users.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15603v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Multilingual%20LLMs%20Think%20In%20English%3F&entry.906535625=Lisa%20Schut%20and%20Yarin%20Gal%20and%20Sebastian%20Farquhar&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20multilingual%20capabilities%20and%20can%20solve%0Atasks%20across%20various%20languages.%20However%2C%20we%20show%20that%20current%20LLMs%20make%20key%0Adecisions%20in%20a%20representation%20space%20closest%20to%20English%2C%20regardless%20of%20their%0Ainput%20and%20output%20languages.%20Exploring%20the%20internal%20representations%20with%20a%20logit%0Alens%20for%20sentences%20in%20French%2C%20German%2C%20Dutch%2C%20and%20Mandarin%2C%20we%20show%20that%20the%20LLM%0Afirst%20emits%20representations%20close%20to%20English%20for%20semantically-loaded%20words%0Abefore%20translating%20them%20into%20the%20target%20language.%20We%20further%20show%20that%0Aactivation%20steering%20in%20these%20LLMs%20is%20more%20effective%20when%20the%20steering%20vectors%0Aare%20computed%20in%20English%20rather%20than%20in%20the%20language%20of%20the%20inputs%20and%20outputs.%0AThis%20suggests%20that%20multilingual%20LLMs%20perform%20key%20reasoning%20steps%20in%20a%0Arepresentation%20that%20is%20heavily%20shaped%20by%20English%20in%20a%20way%20that%20is%20not%0Atransparent%20to%20system%20users.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15603v1&entry.124074799=Read"},
{"title": "PFSD: A Multi-Modal Pedestrian-Focus Scene Dataset for Rich Tasks in\n  Semi-Structured Environments", "author": "Yueting Liu and Hanshi Wang and Yunfei Lei and Zhengjun Zha and Weiming Hu and Jin Gao", "abstract": "  Recent advancements in autonomous driving perception have revealed\nexceptional capabilities within structured environments dominated by vehicular\ntraffic. However, current perception models exhibit significant limitations in\nsemi-structured environments, where dynamic pedestrians with more diverse\nirregular movement and occlusion prevail. We attribute this shortcoming to the\nscarcity of high-quality datasets in semi-structured scenes, particularly\nconcerning pedestrian perception and prediction. In this work, we present the\nmulti-modal Pedestrian-Focused Scene Dataset(PFSD), rigorously annotated in\nsemi-structured scenes with the format of nuScenes. PFSD provides comprehensive\nmulti-modal data annotations with point cloud segmentation, detection, and\nobject IDs for tracking. It encompasses over 130,000 pedestrian instances\ncaptured across various scenarios with varying densities, movement patterns,\nand occlusions. Furthermore, to demonstrate the importance of addressing the\nchallenges posed by more diverse and complex semi-structured environments, we\npropose a novel Hybrid Multi-Scale Fusion Network (HMFN). Specifically, to\ndetect pedestrians in densely populated and occluded scenarios, our method\neffectively captures and fuses multi-scale features using a meticulously\ndesigned hybrid framework that integrates sparse and vanilla convolutions.\nExtensive experiments on PFSD demonstrate that HMFN attains improvement in mean\nAverage Precision (mAP) over existing methods, thereby underscoring its\nefficacy in addressing the challenges of 3D pedestrian detection in complex\nsemi-structured environments. Coding and benchmark are available.\n", "link": "http://arxiv.org/abs/2502.15342v1", "date": "2025-02-21", "relevancy": 2.3363, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5934}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5822}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PFSD%3A%20A%20Multi-Modal%20Pedestrian-Focus%20Scene%20Dataset%20for%20Rich%20Tasks%20in%0A%20%20Semi-Structured%20Environments&body=Title%3A%20PFSD%3A%20A%20Multi-Modal%20Pedestrian-Focus%20Scene%20Dataset%20for%20Rich%20Tasks%20in%0A%20%20Semi-Structured%20Environments%0AAuthor%3A%20Yueting%20Liu%20and%20Hanshi%20Wang%20and%20Yunfei%20Lei%20and%20Zhengjun%20Zha%20and%20Weiming%20Hu%20and%20Jin%20Gao%0AAbstract%3A%20%20%20Recent%20advancements%20in%20autonomous%20driving%20perception%20have%20revealed%0Aexceptional%20capabilities%20within%20structured%20environments%20dominated%20by%20vehicular%0Atraffic.%20However%2C%20current%20perception%20models%20exhibit%20significant%20limitations%20in%0Asemi-structured%20environments%2C%20where%20dynamic%20pedestrians%20with%20more%20diverse%0Airregular%20movement%20and%20occlusion%20prevail.%20We%20attribute%20this%20shortcoming%20to%20the%0Ascarcity%20of%20high-quality%20datasets%20in%20semi-structured%20scenes%2C%20particularly%0Aconcerning%20pedestrian%20perception%20and%20prediction.%20In%20this%20work%2C%20we%20present%20the%0Amulti-modal%20Pedestrian-Focused%20Scene%20Dataset%28PFSD%29%2C%20rigorously%20annotated%20in%0Asemi-structured%20scenes%20with%20the%20format%20of%20nuScenes.%20PFSD%20provides%20comprehensive%0Amulti-modal%20data%20annotations%20with%20point%20cloud%20segmentation%2C%20detection%2C%20and%0Aobject%20IDs%20for%20tracking.%20It%20encompasses%20over%20130%2C000%20pedestrian%20instances%0Acaptured%20across%20various%20scenarios%20with%20varying%20densities%2C%20movement%20patterns%2C%0Aand%20occlusions.%20Furthermore%2C%20to%20demonstrate%20the%20importance%20of%20addressing%20the%0Achallenges%20posed%20by%20more%20diverse%20and%20complex%20semi-structured%20environments%2C%20we%0Apropose%20a%20novel%20Hybrid%20Multi-Scale%20Fusion%20Network%20%28HMFN%29.%20Specifically%2C%20to%0Adetect%20pedestrians%20in%20densely%20populated%20and%20occluded%20scenarios%2C%20our%20method%0Aeffectively%20captures%20and%20fuses%20multi-scale%20features%20using%20a%20meticulously%0Adesigned%20hybrid%20framework%20that%20integrates%20sparse%20and%20vanilla%20convolutions.%0AExtensive%20experiments%20on%20PFSD%20demonstrate%20that%20HMFN%20attains%20improvement%20in%20mean%0AAverage%20Precision%20%28mAP%29%20over%20existing%20methods%2C%20thereby%20underscoring%20its%0Aefficacy%20in%20addressing%20the%20challenges%20of%203D%20pedestrian%20detection%20in%20complex%0Asemi-structured%20environments.%20Coding%20and%20benchmark%20are%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15342v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPFSD%253A%2520A%2520Multi-Modal%2520Pedestrian-Focus%2520Scene%2520Dataset%2520for%2520Rich%2520Tasks%2520in%250A%2520%2520Semi-Structured%2520Environments%26entry.906535625%3DYueting%2520Liu%2520and%2520Hanshi%2520Wang%2520and%2520Yunfei%2520Lei%2520and%2520Zhengjun%2520Zha%2520and%2520Weiming%2520Hu%2520and%2520Jin%2520Gao%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520autonomous%2520driving%2520perception%2520have%2520revealed%250Aexceptional%2520capabilities%2520within%2520structured%2520environments%2520dominated%2520by%2520vehicular%250Atraffic.%2520However%252C%2520current%2520perception%2520models%2520exhibit%2520significant%2520limitations%2520in%250Asemi-structured%2520environments%252C%2520where%2520dynamic%2520pedestrians%2520with%2520more%2520diverse%250Airregular%2520movement%2520and%2520occlusion%2520prevail.%2520We%2520attribute%2520this%2520shortcoming%2520to%2520the%250Ascarcity%2520of%2520high-quality%2520datasets%2520in%2520semi-structured%2520scenes%252C%2520particularly%250Aconcerning%2520pedestrian%2520perception%2520and%2520prediction.%2520In%2520this%2520work%252C%2520we%2520present%2520the%250Amulti-modal%2520Pedestrian-Focused%2520Scene%2520Dataset%2528PFSD%2529%252C%2520rigorously%2520annotated%2520in%250Asemi-structured%2520scenes%2520with%2520the%2520format%2520of%2520nuScenes.%2520PFSD%2520provides%2520comprehensive%250Amulti-modal%2520data%2520annotations%2520with%2520point%2520cloud%2520segmentation%252C%2520detection%252C%2520and%250Aobject%2520IDs%2520for%2520tracking.%2520It%2520encompasses%2520over%2520130%252C000%2520pedestrian%2520instances%250Acaptured%2520across%2520various%2520scenarios%2520with%2520varying%2520densities%252C%2520movement%2520patterns%252C%250Aand%2520occlusions.%2520Furthermore%252C%2520to%2520demonstrate%2520the%2520importance%2520of%2520addressing%2520the%250Achallenges%2520posed%2520by%2520more%2520diverse%2520and%2520complex%2520semi-structured%2520environments%252C%2520we%250Apropose%2520a%2520novel%2520Hybrid%2520Multi-Scale%2520Fusion%2520Network%2520%2528HMFN%2529.%2520Specifically%252C%2520to%250Adetect%2520pedestrians%2520in%2520densely%2520populated%2520and%2520occluded%2520scenarios%252C%2520our%2520method%250Aeffectively%2520captures%2520and%2520fuses%2520multi-scale%2520features%2520using%2520a%2520meticulously%250Adesigned%2520hybrid%2520framework%2520that%2520integrates%2520sparse%2520and%2520vanilla%2520convolutions.%250AExtensive%2520experiments%2520on%2520PFSD%2520demonstrate%2520that%2520HMFN%2520attains%2520improvement%2520in%2520mean%250AAverage%2520Precision%2520%2528mAP%2529%2520over%2520existing%2520methods%252C%2520thereby%2520underscoring%2520its%250Aefficacy%2520in%2520addressing%2520the%2520challenges%2520of%25203D%2520pedestrian%2520detection%2520in%2520complex%250Asemi-structured%2520environments.%2520Coding%2520and%2520benchmark%2520are%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15342v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PFSD%3A%20A%20Multi-Modal%20Pedestrian-Focus%20Scene%20Dataset%20for%20Rich%20Tasks%20in%0A%20%20Semi-Structured%20Environments&entry.906535625=Yueting%20Liu%20and%20Hanshi%20Wang%20and%20Yunfei%20Lei%20and%20Zhengjun%20Zha%20and%20Weiming%20Hu%20and%20Jin%20Gao&entry.1292438233=%20%20Recent%20advancements%20in%20autonomous%20driving%20perception%20have%20revealed%0Aexceptional%20capabilities%20within%20structured%20environments%20dominated%20by%20vehicular%0Atraffic.%20However%2C%20current%20perception%20models%20exhibit%20significant%20limitations%20in%0Asemi-structured%20environments%2C%20where%20dynamic%20pedestrians%20with%20more%20diverse%0Airregular%20movement%20and%20occlusion%20prevail.%20We%20attribute%20this%20shortcoming%20to%20the%0Ascarcity%20of%20high-quality%20datasets%20in%20semi-structured%20scenes%2C%20particularly%0Aconcerning%20pedestrian%20perception%20and%20prediction.%20In%20this%20work%2C%20we%20present%20the%0Amulti-modal%20Pedestrian-Focused%20Scene%20Dataset%28PFSD%29%2C%20rigorously%20annotated%20in%0Asemi-structured%20scenes%20with%20the%20format%20of%20nuScenes.%20PFSD%20provides%20comprehensive%0Amulti-modal%20data%20annotations%20with%20point%20cloud%20segmentation%2C%20detection%2C%20and%0Aobject%20IDs%20for%20tracking.%20It%20encompasses%20over%20130%2C000%20pedestrian%20instances%0Acaptured%20across%20various%20scenarios%20with%20varying%20densities%2C%20movement%20patterns%2C%0Aand%20occlusions.%20Furthermore%2C%20to%20demonstrate%20the%20importance%20of%20addressing%20the%0Achallenges%20posed%20by%20more%20diverse%20and%20complex%20semi-structured%20environments%2C%20we%0Apropose%20a%20novel%20Hybrid%20Multi-Scale%20Fusion%20Network%20%28HMFN%29.%20Specifically%2C%20to%0Adetect%20pedestrians%20in%20densely%20populated%20and%20occluded%20scenarios%2C%20our%20method%0Aeffectively%20captures%20and%20fuses%20multi-scale%20features%20using%20a%20meticulously%0Adesigned%20hybrid%20framework%20that%20integrates%20sparse%20and%20vanilla%20convolutions.%0AExtensive%20experiments%20on%20PFSD%20demonstrate%20that%20HMFN%20attains%20improvement%20in%20mean%0AAverage%20Precision%20%28mAP%29%20over%20existing%20methods%2C%20thereby%20underscoring%20its%0Aefficacy%20in%20addressing%20the%20challenges%20of%203D%20pedestrian%20detection%20in%20complex%0Asemi-structured%20environments.%20Coding%20and%20benchmark%20are%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15342v1&entry.124074799=Read"},
{"title": "Enhanced Probabilistic Collision Detection for Motion Planning Under\n  Sensing Uncertainty", "author": "Xiaoli Wang and Sipu Ruan and Xin Meng and Gregory Chirikjian", "abstract": "  Probabilistic collision detection (PCD) is essential in motion planning for\nrobots operating in unstructured environments, where considering sensing\nuncertainty helps prevent damage. Existing PCD methods mainly used simplified\ngeometric models and addressed only position estimation errors. This paper\npresents an enhanced PCD method with two key advancements: (a) using\nsuperquadrics for more accurate shape approximation and (b) accounting for both\nposition and orientation estimation errors to improve robustness under sensing\nuncertainty. Our method first computes an enlarged surface for each object that\nencapsulates its observed rotated copies, thereby addressing the orientation\nestimation errors. Then, the collision probability under the position\nestimation errors is formulated as a chance-constraint problem that is solved\nwith a tight upper bound. Both the two steps leverage the recently developed\nnormal parameterization of superquadric surfaces. Results show that our PCD\nmethod is twice as close to the Monte-Carlo sampled baseline as the best\nexisting PCD method and reduces path length by 30% and planning time by 37%,\nrespectively. A Real2Sim pipeline further validates the importance of\nconsidering orientation estimation errors, showing that the collision\nprobability of executing the planned path in simulation is only 2%, compared to\n9% and 29% when considering only position estimation errors or none at all.\n", "link": "http://arxiv.org/abs/2502.15525v1", "date": "2025-02-21", "relevancy": 2.3232, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6174}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.598}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5373}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Probabilistic%20Collision%20Detection%20for%20Motion%20Planning%20Under%0A%20%20Sensing%20Uncertainty&body=Title%3A%20Enhanced%20Probabilistic%20Collision%20Detection%20for%20Motion%20Planning%20Under%0A%20%20Sensing%20Uncertainty%0AAuthor%3A%20Xiaoli%20Wang%20and%20Sipu%20Ruan%20and%20Xin%20Meng%20and%20Gregory%20Chirikjian%0AAbstract%3A%20%20%20Probabilistic%20collision%20detection%20%28PCD%29%20is%20essential%20in%20motion%20planning%20for%0Arobots%20operating%20in%20unstructured%20environments%2C%20where%20considering%20sensing%0Auncertainty%20helps%20prevent%20damage.%20Existing%20PCD%20methods%20mainly%20used%20simplified%0Ageometric%20models%20and%20addressed%20only%20position%20estimation%20errors.%20This%20paper%0Apresents%20an%20enhanced%20PCD%20method%20with%20two%20key%20advancements%3A%20%28a%29%20using%0Asuperquadrics%20for%20more%20accurate%20shape%20approximation%20and%20%28b%29%20accounting%20for%20both%0Aposition%20and%20orientation%20estimation%20errors%20to%20improve%20robustness%20under%20sensing%0Auncertainty.%20Our%20method%20first%20computes%20an%20enlarged%20surface%20for%20each%20object%20that%0Aencapsulates%20its%20observed%20rotated%20copies%2C%20thereby%20addressing%20the%20orientation%0Aestimation%20errors.%20Then%2C%20the%20collision%20probability%20under%20the%20position%0Aestimation%20errors%20is%20formulated%20as%20a%20chance-constraint%20problem%20that%20is%20solved%0Awith%20a%20tight%20upper%20bound.%20Both%20the%20two%20steps%20leverage%20the%20recently%20developed%0Anormal%20parameterization%20of%20superquadric%20surfaces.%20Results%20show%20that%20our%20PCD%0Amethod%20is%20twice%20as%20close%20to%20the%20Monte-Carlo%20sampled%20baseline%20as%20the%20best%0Aexisting%20PCD%20method%20and%20reduces%20path%20length%20by%2030%25%20and%20planning%20time%20by%2037%25%2C%0Arespectively.%20A%20Real2Sim%20pipeline%20further%20validates%20the%20importance%20of%0Aconsidering%20orientation%20estimation%20errors%2C%20showing%20that%20the%20collision%0Aprobability%20of%20executing%20the%20planned%20path%20in%20simulation%20is%20only%202%25%2C%20compared%20to%0A9%25%20and%2029%25%20when%20considering%20only%20position%20estimation%20errors%20or%20none%20at%20all.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15525v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Probabilistic%2520Collision%2520Detection%2520for%2520Motion%2520Planning%2520Under%250A%2520%2520Sensing%2520Uncertainty%26entry.906535625%3DXiaoli%2520Wang%2520and%2520Sipu%2520Ruan%2520and%2520Xin%2520Meng%2520and%2520Gregory%2520Chirikjian%26entry.1292438233%3D%2520%2520Probabilistic%2520collision%2520detection%2520%2528PCD%2529%2520is%2520essential%2520in%2520motion%2520planning%2520for%250Arobots%2520operating%2520in%2520unstructured%2520environments%252C%2520where%2520considering%2520sensing%250Auncertainty%2520helps%2520prevent%2520damage.%2520Existing%2520PCD%2520methods%2520mainly%2520used%2520simplified%250Ageometric%2520models%2520and%2520addressed%2520only%2520position%2520estimation%2520errors.%2520This%2520paper%250Apresents%2520an%2520enhanced%2520PCD%2520method%2520with%2520two%2520key%2520advancements%253A%2520%2528a%2529%2520using%250Asuperquadrics%2520for%2520more%2520accurate%2520shape%2520approximation%2520and%2520%2528b%2529%2520accounting%2520for%2520both%250Aposition%2520and%2520orientation%2520estimation%2520errors%2520to%2520improve%2520robustness%2520under%2520sensing%250Auncertainty.%2520Our%2520method%2520first%2520computes%2520an%2520enlarged%2520surface%2520for%2520each%2520object%2520that%250Aencapsulates%2520its%2520observed%2520rotated%2520copies%252C%2520thereby%2520addressing%2520the%2520orientation%250Aestimation%2520errors.%2520Then%252C%2520the%2520collision%2520probability%2520under%2520the%2520position%250Aestimation%2520errors%2520is%2520formulated%2520as%2520a%2520chance-constraint%2520problem%2520that%2520is%2520solved%250Awith%2520a%2520tight%2520upper%2520bound.%2520Both%2520the%2520two%2520steps%2520leverage%2520the%2520recently%2520developed%250Anormal%2520parameterization%2520of%2520superquadric%2520surfaces.%2520Results%2520show%2520that%2520our%2520PCD%250Amethod%2520is%2520twice%2520as%2520close%2520to%2520the%2520Monte-Carlo%2520sampled%2520baseline%2520as%2520the%2520best%250Aexisting%2520PCD%2520method%2520and%2520reduces%2520path%2520length%2520by%252030%2525%2520and%2520planning%2520time%2520by%252037%2525%252C%250Arespectively.%2520A%2520Real2Sim%2520pipeline%2520further%2520validates%2520the%2520importance%2520of%250Aconsidering%2520orientation%2520estimation%2520errors%252C%2520showing%2520that%2520the%2520collision%250Aprobability%2520of%2520executing%2520the%2520planned%2520path%2520in%2520simulation%2520is%2520only%25202%2525%252C%2520compared%2520to%250A9%2525%2520and%252029%2525%2520when%2520considering%2520only%2520position%2520estimation%2520errors%2520or%2520none%2520at%2520all.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15525v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Probabilistic%20Collision%20Detection%20for%20Motion%20Planning%20Under%0A%20%20Sensing%20Uncertainty&entry.906535625=Xiaoli%20Wang%20and%20Sipu%20Ruan%20and%20Xin%20Meng%20and%20Gregory%20Chirikjian&entry.1292438233=%20%20Probabilistic%20collision%20detection%20%28PCD%29%20is%20essential%20in%20motion%20planning%20for%0Arobots%20operating%20in%20unstructured%20environments%2C%20where%20considering%20sensing%0Auncertainty%20helps%20prevent%20damage.%20Existing%20PCD%20methods%20mainly%20used%20simplified%0Ageometric%20models%20and%20addressed%20only%20position%20estimation%20errors.%20This%20paper%0Apresents%20an%20enhanced%20PCD%20method%20with%20two%20key%20advancements%3A%20%28a%29%20using%0Asuperquadrics%20for%20more%20accurate%20shape%20approximation%20and%20%28b%29%20accounting%20for%20both%0Aposition%20and%20orientation%20estimation%20errors%20to%20improve%20robustness%20under%20sensing%0Auncertainty.%20Our%20method%20first%20computes%20an%20enlarged%20surface%20for%20each%20object%20that%0Aencapsulates%20its%20observed%20rotated%20copies%2C%20thereby%20addressing%20the%20orientation%0Aestimation%20errors.%20Then%2C%20the%20collision%20probability%20under%20the%20position%0Aestimation%20errors%20is%20formulated%20as%20a%20chance-constraint%20problem%20that%20is%20solved%0Awith%20a%20tight%20upper%20bound.%20Both%20the%20two%20steps%20leverage%20the%20recently%20developed%0Anormal%20parameterization%20of%20superquadric%20surfaces.%20Results%20show%20that%20our%20PCD%0Amethod%20is%20twice%20as%20close%20to%20the%20Monte-Carlo%20sampled%20baseline%20as%20the%20best%0Aexisting%20PCD%20method%20and%20reduces%20path%20length%20by%2030%25%20and%20planning%20time%20by%2037%25%2C%0Arespectively.%20A%20Real2Sim%20pipeline%20further%20validates%20the%20importance%20of%0Aconsidering%20orientation%20estimation%20errors%2C%20showing%20that%20the%20collision%0Aprobability%20of%20executing%20the%20planned%20path%20in%20simulation%20is%20only%202%25%2C%20compared%20to%0A9%25%20and%2029%25%20when%20considering%20only%20position%20estimation%20errors%20or%20none%20at%20all.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15525v1&entry.124074799=Read"},
{"title": "Long Video Understanding with Learnable Retrieval in Video-Language\n  Models", "author": "Jiaqi Xu and Cuiling Lan and Wenxuan Xie and Xuejin Chen and Yan Lu", "abstract": "  The remarkable natural language understanding, reasoning, and generation\ncapabilities of large language models (LLMs) have made them attractive for\napplication to video understanding, utilizing video tokens as contextual input.\nHowever, employing LLMs for long video understanding presents significant\nchallenges. The extensive number of video tokens leads to considerable\ncomputational costs for LLMs while using aggregated tokens results in loss of\nvision details. Moreover, the presence of abundant question-irrelevant tokens\nintroduces noise to the video reasoning process. To address these issues, we\nintroduce a simple yet effective learnable retrieval-based video-language model\n(R-VLM) for efficient long video understanding. Specifically, given a question\n(query) and a long video, our model identifies and selects the most relevant K\nvideo chunks and uses their associated visual tokens to serve as context for\nthe LLM inference. This effectively reduces the number of video tokens,\neliminates noise interference, and enhances system performance. We achieve this\nby incorporating a learnable lightweight MLP block to facilitate the efficient\nretrieval of question-relevant chunks, through the end-to-end training of our\nvideo-language model with a proposed soft matching loss. Our experimental\nresults on multiple zero-shot video question answering datasets validate the\neffectiveness of our framework for comprehending long videos.\n", "link": "http://arxiv.org/abs/2312.04931v2", "date": "2025-02-21", "relevancy": 2.3232, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5907}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5907}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long%20Video%20Understanding%20with%20Learnable%20Retrieval%20in%20Video-Language%0A%20%20Models&body=Title%3A%20Long%20Video%20Understanding%20with%20Learnable%20Retrieval%20in%20Video-Language%0A%20%20Models%0AAuthor%3A%20Jiaqi%20Xu%20and%20Cuiling%20Lan%20and%20Wenxuan%20Xie%20and%20Xuejin%20Chen%20and%20Yan%20Lu%0AAbstract%3A%20%20%20The%20remarkable%20natural%20language%20understanding%2C%20reasoning%2C%20and%20generation%0Acapabilities%20of%20large%20language%20models%20%28LLMs%29%20have%20made%20them%20attractive%20for%0Aapplication%20to%20video%20understanding%2C%20utilizing%20video%20tokens%20as%20contextual%20input.%0AHowever%2C%20employing%20LLMs%20for%20long%20video%20understanding%20presents%20significant%0Achallenges.%20The%20extensive%20number%20of%20video%20tokens%20leads%20to%20considerable%0Acomputational%20costs%20for%20LLMs%20while%20using%20aggregated%20tokens%20results%20in%20loss%20of%0Avision%20details.%20Moreover%2C%20the%20presence%20of%20abundant%20question-irrelevant%20tokens%0Aintroduces%20noise%20to%20the%20video%20reasoning%20process.%20To%20address%20these%20issues%2C%20we%0Aintroduce%20a%20simple%20yet%20effective%20learnable%20retrieval-based%20video-language%20model%0A%28R-VLM%29%20for%20efficient%20long%20video%20understanding.%20Specifically%2C%20given%20a%20question%0A%28query%29%20and%20a%20long%20video%2C%20our%20model%20identifies%20and%20selects%20the%20most%20relevant%20K%0Avideo%20chunks%20and%20uses%20their%20associated%20visual%20tokens%20to%20serve%20as%20context%20for%0Athe%20LLM%20inference.%20This%20effectively%20reduces%20the%20number%20of%20video%20tokens%2C%0Aeliminates%20noise%20interference%2C%20and%20enhances%20system%20performance.%20We%20achieve%20this%0Aby%20incorporating%20a%20learnable%20lightweight%20MLP%20block%20to%20facilitate%20the%20efficient%0Aretrieval%20of%20question-relevant%20chunks%2C%20through%20the%20end-to-end%20training%20of%20our%0Avideo-language%20model%20with%20a%20proposed%20soft%20matching%20loss.%20Our%20experimental%0Aresults%20on%20multiple%20zero-shot%20video%20question%20answering%20datasets%20validate%20the%0Aeffectiveness%20of%20our%20framework%20for%20comprehending%20long%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.04931v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong%2520Video%2520Understanding%2520with%2520Learnable%2520Retrieval%2520in%2520Video-Language%250A%2520%2520Models%26entry.906535625%3DJiaqi%2520Xu%2520and%2520Cuiling%2520Lan%2520and%2520Wenxuan%2520Xie%2520and%2520Xuejin%2520Chen%2520and%2520Yan%2520Lu%26entry.1292438233%3D%2520%2520The%2520remarkable%2520natural%2520language%2520understanding%252C%2520reasoning%252C%2520and%2520generation%250Acapabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520made%2520them%2520attractive%2520for%250Aapplication%2520to%2520video%2520understanding%252C%2520utilizing%2520video%2520tokens%2520as%2520contextual%2520input.%250AHowever%252C%2520employing%2520LLMs%2520for%2520long%2520video%2520understanding%2520presents%2520significant%250Achallenges.%2520The%2520extensive%2520number%2520of%2520video%2520tokens%2520leads%2520to%2520considerable%250Acomputational%2520costs%2520for%2520LLMs%2520while%2520using%2520aggregated%2520tokens%2520results%2520in%2520loss%2520of%250Avision%2520details.%2520Moreover%252C%2520the%2520presence%2520of%2520abundant%2520question-irrelevant%2520tokens%250Aintroduces%2520noise%2520to%2520the%2520video%2520reasoning%2520process.%2520To%2520address%2520these%2520issues%252C%2520we%250Aintroduce%2520a%2520simple%2520yet%2520effective%2520learnable%2520retrieval-based%2520video-language%2520model%250A%2528R-VLM%2529%2520for%2520efficient%2520long%2520video%2520understanding.%2520Specifically%252C%2520given%2520a%2520question%250A%2528query%2529%2520and%2520a%2520long%2520video%252C%2520our%2520model%2520identifies%2520and%2520selects%2520the%2520most%2520relevant%2520K%250Avideo%2520chunks%2520and%2520uses%2520their%2520associated%2520visual%2520tokens%2520to%2520serve%2520as%2520context%2520for%250Athe%2520LLM%2520inference.%2520This%2520effectively%2520reduces%2520the%2520number%2520of%2520video%2520tokens%252C%250Aeliminates%2520noise%2520interference%252C%2520and%2520enhances%2520system%2520performance.%2520We%2520achieve%2520this%250Aby%2520incorporating%2520a%2520learnable%2520lightweight%2520MLP%2520block%2520to%2520facilitate%2520the%2520efficient%250Aretrieval%2520of%2520question-relevant%2520chunks%252C%2520through%2520the%2520end-to-end%2520training%2520of%2520our%250Avideo-language%2520model%2520with%2520a%2520proposed%2520soft%2520matching%2520loss.%2520Our%2520experimental%250Aresults%2520on%2520multiple%2520zero-shot%2520video%2520question%2520answering%2520datasets%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520framework%2520for%2520comprehending%2520long%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.04931v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long%20Video%20Understanding%20with%20Learnable%20Retrieval%20in%20Video-Language%0A%20%20Models&entry.906535625=Jiaqi%20Xu%20and%20Cuiling%20Lan%20and%20Wenxuan%20Xie%20and%20Xuejin%20Chen%20and%20Yan%20Lu&entry.1292438233=%20%20The%20remarkable%20natural%20language%20understanding%2C%20reasoning%2C%20and%20generation%0Acapabilities%20of%20large%20language%20models%20%28LLMs%29%20have%20made%20them%20attractive%20for%0Aapplication%20to%20video%20understanding%2C%20utilizing%20video%20tokens%20as%20contextual%20input.%0AHowever%2C%20employing%20LLMs%20for%20long%20video%20understanding%20presents%20significant%0Achallenges.%20The%20extensive%20number%20of%20video%20tokens%20leads%20to%20considerable%0Acomputational%20costs%20for%20LLMs%20while%20using%20aggregated%20tokens%20results%20in%20loss%20of%0Avision%20details.%20Moreover%2C%20the%20presence%20of%20abundant%20question-irrelevant%20tokens%0Aintroduces%20noise%20to%20the%20video%20reasoning%20process.%20To%20address%20these%20issues%2C%20we%0Aintroduce%20a%20simple%20yet%20effective%20learnable%20retrieval-based%20video-language%20model%0A%28R-VLM%29%20for%20efficient%20long%20video%20understanding.%20Specifically%2C%20given%20a%20question%0A%28query%29%20and%20a%20long%20video%2C%20our%20model%20identifies%20and%20selects%20the%20most%20relevant%20K%0Avideo%20chunks%20and%20uses%20their%20associated%20visual%20tokens%20to%20serve%20as%20context%20for%0Athe%20LLM%20inference.%20This%20effectively%20reduces%20the%20number%20of%20video%20tokens%2C%0Aeliminates%20noise%20interference%2C%20and%20enhances%20system%20performance.%20We%20achieve%20this%0Aby%20incorporating%20a%20learnable%20lightweight%20MLP%20block%20to%20facilitate%20the%20efficient%0Aretrieval%20of%20question-relevant%20chunks%2C%20through%20the%20end-to-end%20training%20of%20our%0Avideo-language%20model%20with%20a%20proposed%20soft%20matching%20loss.%20Our%20experimental%0Aresults%20on%20multiple%20zero-shot%20video%20question%20answering%20datasets%20validate%20the%0Aeffectiveness%20of%20our%20framework%20for%20comprehending%20long%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.04931v2&entry.124074799=Read"},
{"title": "From Priest to Doctor: Domain Adaptation for Low-Resource Neural Machine\n  Translation", "author": "Ali Marashian and Enora Rice and Luke Gessler and Alexis Palmer and Katharina von der Wense", "abstract": "  Many of the world's languages have insufficient data to train high-performing\ngeneral neural machine translation (NMT) models, let alone domain-specific\nmodels, and often the only available parallel data are small amounts of\nreligious texts. Hence, domain adaptation (DA) is a crucial issue faced by\ncontemporary NMT and has, so far, been underexplored for low-resource\nlanguages. In this paper, we evaluate a set of methods from both low-resource\nNMT and DA in a realistic setting, in which we aim to translate between a\nhigh-resource and a low-resource language with access to only: a) parallel\nBible data, b) a bilingual dictionary, and c) a monolingual target-domain\ncorpus in the high-resource language. Our results show that the effectiveness\nof the tested methods varies, with the simplest one, DALI, being most\neffective. We follow up with a small human evaluation of DALI, which shows that\nthere is still a need for more careful investigation of how to accomplish DA\nfor low-resource NMT.\n", "link": "http://arxiv.org/abs/2412.00966v3", "date": "2025-02-21", "relevancy": 2.3201, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4712}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4604}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Priest%20to%20Doctor%3A%20Domain%20Adaptation%20for%20Low-Resource%20Neural%20Machine%0A%20%20Translation&body=Title%3A%20From%20Priest%20to%20Doctor%3A%20Domain%20Adaptation%20for%20Low-Resource%20Neural%20Machine%0A%20%20Translation%0AAuthor%3A%20Ali%20Marashian%20and%20Enora%20Rice%20and%20Luke%20Gessler%20and%20Alexis%20Palmer%20and%20Katharina%20von%20der%20Wense%0AAbstract%3A%20%20%20Many%20of%20the%20world%27s%20languages%20have%20insufficient%20data%20to%20train%20high-performing%0Ageneral%20neural%20machine%20translation%20%28NMT%29%20models%2C%20let%20alone%20domain-specific%0Amodels%2C%20and%20often%20the%20only%20available%20parallel%20data%20are%20small%20amounts%20of%0Areligious%20texts.%20Hence%2C%20domain%20adaptation%20%28DA%29%20is%20a%20crucial%20issue%20faced%20by%0Acontemporary%20NMT%20and%20has%2C%20so%20far%2C%20been%20underexplored%20for%20low-resource%0Alanguages.%20In%20this%20paper%2C%20we%20evaluate%20a%20set%20of%20methods%20from%20both%20low-resource%0ANMT%20and%20DA%20in%20a%20realistic%20setting%2C%20in%20which%20we%20aim%20to%20translate%20between%20a%0Ahigh-resource%20and%20a%20low-resource%20language%20with%20access%20to%20only%3A%20a%29%20parallel%0ABible%20data%2C%20b%29%20a%20bilingual%20dictionary%2C%20and%20c%29%20a%20monolingual%20target-domain%0Acorpus%20in%20the%20high-resource%20language.%20Our%20results%20show%20that%20the%20effectiveness%0Aof%20the%20tested%20methods%20varies%2C%20with%20the%20simplest%20one%2C%20DALI%2C%20being%20most%0Aeffective.%20We%20follow%20up%20with%20a%20small%20human%20evaluation%20of%20DALI%2C%20which%20shows%20that%0Athere%20is%20still%20a%20need%20for%20more%20careful%20investigation%20of%20how%20to%20accomplish%20DA%0Afor%20low-resource%20NMT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.00966v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Priest%2520to%2520Doctor%253A%2520Domain%2520Adaptation%2520for%2520Low-Resource%2520Neural%2520Machine%250A%2520%2520Translation%26entry.906535625%3DAli%2520Marashian%2520and%2520Enora%2520Rice%2520and%2520Luke%2520Gessler%2520and%2520Alexis%2520Palmer%2520and%2520Katharina%2520von%2520der%2520Wense%26entry.1292438233%3D%2520%2520Many%2520of%2520the%2520world%2527s%2520languages%2520have%2520insufficient%2520data%2520to%2520train%2520high-performing%250Ageneral%2520neural%2520machine%2520translation%2520%2528NMT%2529%2520models%252C%2520let%2520alone%2520domain-specific%250Amodels%252C%2520and%2520often%2520the%2520only%2520available%2520parallel%2520data%2520are%2520small%2520amounts%2520of%250Areligious%2520texts.%2520Hence%252C%2520domain%2520adaptation%2520%2528DA%2529%2520is%2520a%2520crucial%2520issue%2520faced%2520by%250Acontemporary%2520NMT%2520and%2520has%252C%2520so%2520far%252C%2520been%2520underexplored%2520for%2520low-resource%250Alanguages.%2520In%2520this%2520paper%252C%2520we%2520evaluate%2520a%2520set%2520of%2520methods%2520from%2520both%2520low-resource%250ANMT%2520and%2520DA%2520in%2520a%2520realistic%2520setting%252C%2520in%2520which%2520we%2520aim%2520to%2520translate%2520between%2520a%250Ahigh-resource%2520and%2520a%2520low-resource%2520language%2520with%2520access%2520to%2520only%253A%2520a%2529%2520parallel%250ABible%2520data%252C%2520b%2529%2520a%2520bilingual%2520dictionary%252C%2520and%2520c%2529%2520a%2520monolingual%2520target-domain%250Acorpus%2520in%2520the%2520high-resource%2520language.%2520Our%2520results%2520show%2520that%2520the%2520effectiveness%250Aof%2520the%2520tested%2520methods%2520varies%252C%2520with%2520the%2520simplest%2520one%252C%2520DALI%252C%2520being%2520most%250Aeffective.%2520We%2520follow%2520up%2520with%2520a%2520small%2520human%2520evaluation%2520of%2520DALI%252C%2520which%2520shows%2520that%250Athere%2520is%2520still%2520a%2520need%2520for%2520more%2520careful%2520investigation%2520of%2520how%2520to%2520accomplish%2520DA%250Afor%2520low-resource%2520NMT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.00966v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Priest%20to%20Doctor%3A%20Domain%20Adaptation%20for%20Low-Resource%20Neural%20Machine%0A%20%20Translation&entry.906535625=Ali%20Marashian%20and%20Enora%20Rice%20and%20Luke%20Gessler%20and%20Alexis%20Palmer%20and%20Katharina%20von%20der%20Wense&entry.1292438233=%20%20Many%20of%20the%20world%27s%20languages%20have%20insufficient%20data%20to%20train%20high-performing%0Ageneral%20neural%20machine%20translation%20%28NMT%29%20models%2C%20let%20alone%20domain-specific%0Amodels%2C%20and%20often%20the%20only%20available%20parallel%20data%20are%20small%20amounts%20of%0Areligious%20texts.%20Hence%2C%20domain%20adaptation%20%28DA%29%20is%20a%20crucial%20issue%20faced%20by%0Acontemporary%20NMT%20and%20has%2C%20so%20far%2C%20been%20underexplored%20for%20low-resource%0Alanguages.%20In%20this%20paper%2C%20we%20evaluate%20a%20set%20of%20methods%20from%20both%20low-resource%0ANMT%20and%20DA%20in%20a%20realistic%20setting%2C%20in%20which%20we%20aim%20to%20translate%20between%20a%0Ahigh-resource%20and%20a%20low-resource%20language%20with%20access%20to%20only%3A%20a%29%20parallel%0ABible%20data%2C%20b%29%20a%20bilingual%20dictionary%2C%20and%20c%29%20a%20monolingual%20target-domain%0Acorpus%20in%20the%20high-resource%20language.%20Our%20results%20show%20that%20the%20effectiveness%0Aof%20the%20tested%20methods%20varies%2C%20with%20the%20simplest%20one%2C%20DALI%2C%20being%20most%0Aeffective.%20We%20follow%20up%20with%20a%20small%20human%20evaluation%20of%20DALI%2C%20which%20shows%20that%0Athere%20is%20still%20a%20need%20for%20more%20careful%20investigation%20of%20how%20to%20accomplish%20DA%0Afor%20low-resource%20NMT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.00966v3&entry.124074799=Read"},
{"title": "MVIP -- A Dataset and Methods for Application Oriented Multi-View and\n  Multi-Modal Industrial Part Recognition", "author": "Paul Koch and Marian Schl\u00fcter and J\u00f6rg Kr\u00fcger", "abstract": "  We present MVIP, a novel dataset for multi-modal and multi-view\napplication-oriented industrial part recognition. Here we are the first to\ncombine a calibrated RGBD multi-view dataset with additional object context\nsuch as physical properties, natural language, and super-classes. The current\nportfolio of available datasets offers a wide range of representations to\ndesign and benchmark related methods. In contrast to existing classification\nchallenges, industrial recognition applications offer controlled multi-modal\nenvironments but at the same time have different problems than traditional\n2D/3D classification challenges. Frequently, industrial applications must deal\nwith a small amount or increased number of training data, visually similar\nparts, and varying object sizes, while requiring a robust near 100% top 5\naccuracy under cost and time constraints. Current methods tackle such\nchallenges individually, but direct adoption of these methods within industrial\napplications is complex and requires further research. Our main goal with MVIP\nis to study and push transferability of various state-of-the-art methods within\nrelated downstream tasks towards an efficient deployment of industrial\nclassifiers. Additionally, we intend to push with MVIP research regarding\nseveral modality fusion topics, (automated) synthetic data generation, and\ncomplex data sampling -- combined in a single application-oriented benchmark.\n", "link": "http://arxiv.org/abs/2502.15448v1", "date": "2025-02-21", "relevancy": 2.3176, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5831}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5787}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVIP%20--%20A%20Dataset%20and%20Methods%20for%20Application%20Oriented%20Multi-View%20and%0A%20%20Multi-Modal%20Industrial%20Part%20Recognition&body=Title%3A%20MVIP%20--%20A%20Dataset%20and%20Methods%20for%20Application%20Oriented%20Multi-View%20and%0A%20%20Multi-Modal%20Industrial%20Part%20Recognition%0AAuthor%3A%20Paul%20Koch%20and%20Marian%20Schl%C3%BCter%20and%20J%C3%B6rg%20Kr%C3%BCger%0AAbstract%3A%20%20%20We%20present%20MVIP%2C%20a%20novel%20dataset%20for%20multi-modal%20and%20multi-view%0Aapplication-oriented%20industrial%20part%20recognition.%20Here%20we%20are%20the%20first%20to%0Acombine%20a%20calibrated%20RGBD%20multi-view%20dataset%20with%20additional%20object%20context%0Asuch%20as%20physical%20properties%2C%20natural%20language%2C%20and%20super-classes.%20The%20current%0Aportfolio%20of%20available%20datasets%20offers%20a%20wide%20range%20of%20representations%20to%0Adesign%20and%20benchmark%20related%20methods.%20In%20contrast%20to%20existing%20classification%0Achallenges%2C%20industrial%20recognition%20applications%20offer%20controlled%20multi-modal%0Aenvironments%20but%20at%20the%20same%20time%20have%20different%20problems%20than%20traditional%0A2D/3D%20classification%20challenges.%20Frequently%2C%20industrial%20applications%20must%20deal%0Awith%20a%20small%20amount%20or%20increased%20number%20of%20training%20data%2C%20visually%20similar%0Aparts%2C%20and%20varying%20object%20sizes%2C%20while%20requiring%20a%20robust%20near%20100%25%20top%205%0Aaccuracy%20under%20cost%20and%20time%20constraints.%20Current%20methods%20tackle%20such%0Achallenges%20individually%2C%20but%20direct%20adoption%20of%20these%20methods%20within%20industrial%0Aapplications%20is%20complex%20and%20requires%20further%20research.%20Our%20main%20goal%20with%20MVIP%0Ais%20to%20study%20and%20push%20transferability%20of%20various%20state-of-the-art%20methods%20within%0Arelated%20downstream%20tasks%20towards%20an%20efficient%20deployment%20of%20industrial%0Aclassifiers.%20Additionally%2C%20we%20intend%20to%20push%20with%20MVIP%20research%20regarding%0Aseveral%20modality%20fusion%20topics%2C%20%28automated%29%20synthetic%20data%20generation%2C%20and%0Acomplex%20data%20sampling%20--%20combined%20in%20a%20single%20application-oriented%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15448v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVIP%2520--%2520A%2520Dataset%2520and%2520Methods%2520for%2520Application%2520Oriented%2520Multi-View%2520and%250A%2520%2520Multi-Modal%2520Industrial%2520Part%2520Recognition%26entry.906535625%3DPaul%2520Koch%2520and%2520Marian%2520Schl%25C3%25BCter%2520and%2520J%25C3%25B6rg%2520Kr%25C3%25BCger%26entry.1292438233%3D%2520%2520We%2520present%2520MVIP%252C%2520a%2520novel%2520dataset%2520for%2520multi-modal%2520and%2520multi-view%250Aapplication-oriented%2520industrial%2520part%2520recognition.%2520Here%2520we%2520are%2520the%2520first%2520to%250Acombine%2520a%2520calibrated%2520RGBD%2520multi-view%2520dataset%2520with%2520additional%2520object%2520context%250Asuch%2520as%2520physical%2520properties%252C%2520natural%2520language%252C%2520and%2520super-classes.%2520The%2520current%250Aportfolio%2520of%2520available%2520datasets%2520offers%2520a%2520wide%2520range%2520of%2520representations%2520to%250Adesign%2520and%2520benchmark%2520related%2520methods.%2520In%2520contrast%2520to%2520existing%2520classification%250Achallenges%252C%2520industrial%2520recognition%2520applications%2520offer%2520controlled%2520multi-modal%250Aenvironments%2520but%2520at%2520the%2520same%2520time%2520have%2520different%2520problems%2520than%2520traditional%250A2D/3D%2520classification%2520challenges.%2520Frequently%252C%2520industrial%2520applications%2520must%2520deal%250Awith%2520a%2520small%2520amount%2520or%2520increased%2520number%2520of%2520training%2520data%252C%2520visually%2520similar%250Aparts%252C%2520and%2520varying%2520object%2520sizes%252C%2520while%2520requiring%2520a%2520robust%2520near%2520100%2525%2520top%25205%250Aaccuracy%2520under%2520cost%2520and%2520time%2520constraints.%2520Current%2520methods%2520tackle%2520such%250Achallenges%2520individually%252C%2520but%2520direct%2520adoption%2520of%2520these%2520methods%2520within%2520industrial%250Aapplications%2520is%2520complex%2520and%2520requires%2520further%2520research.%2520Our%2520main%2520goal%2520with%2520MVIP%250Ais%2520to%2520study%2520and%2520push%2520transferability%2520of%2520various%2520state-of-the-art%2520methods%2520within%250Arelated%2520downstream%2520tasks%2520towards%2520an%2520efficient%2520deployment%2520of%2520industrial%250Aclassifiers.%2520Additionally%252C%2520we%2520intend%2520to%2520push%2520with%2520MVIP%2520research%2520regarding%250Aseveral%2520modality%2520fusion%2520topics%252C%2520%2528automated%2529%2520synthetic%2520data%2520generation%252C%2520and%250Acomplex%2520data%2520sampling%2520--%2520combined%2520in%2520a%2520single%2520application-oriented%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15448v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVIP%20--%20A%20Dataset%20and%20Methods%20for%20Application%20Oriented%20Multi-View%20and%0A%20%20Multi-Modal%20Industrial%20Part%20Recognition&entry.906535625=Paul%20Koch%20and%20Marian%20Schl%C3%BCter%20and%20J%C3%B6rg%20Kr%C3%BCger&entry.1292438233=%20%20We%20present%20MVIP%2C%20a%20novel%20dataset%20for%20multi-modal%20and%20multi-view%0Aapplication-oriented%20industrial%20part%20recognition.%20Here%20we%20are%20the%20first%20to%0Acombine%20a%20calibrated%20RGBD%20multi-view%20dataset%20with%20additional%20object%20context%0Asuch%20as%20physical%20properties%2C%20natural%20language%2C%20and%20super-classes.%20The%20current%0Aportfolio%20of%20available%20datasets%20offers%20a%20wide%20range%20of%20representations%20to%0Adesign%20and%20benchmark%20related%20methods.%20In%20contrast%20to%20existing%20classification%0Achallenges%2C%20industrial%20recognition%20applications%20offer%20controlled%20multi-modal%0Aenvironments%20but%20at%20the%20same%20time%20have%20different%20problems%20than%20traditional%0A2D/3D%20classification%20challenges.%20Frequently%2C%20industrial%20applications%20must%20deal%0Awith%20a%20small%20amount%20or%20increased%20number%20of%20training%20data%2C%20visually%20similar%0Aparts%2C%20and%20varying%20object%20sizes%2C%20while%20requiring%20a%20robust%20near%20100%25%20top%205%0Aaccuracy%20under%20cost%20and%20time%20constraints.%20Current%20methods%20tackle%20such%0Achallenges%20individually%2C%20but%20direct%20adoption%20of%20these%20methods%20within%20industrial%0Aapplications%20is%20complex%20and%20requires%20further%20research.%20Our%20main%20goal%20with%20MVIP%0Ais%20to%20study%20and%20push%20transferability%20of%20various%20state-of-the-art%20methods%20within%0Arelated%20downstream%20tasks%20towards%20an%20efficient%20deployment%20of%20industrial%0Aclassifiers.%20Additionally%2C%20we%20intend%20to%20push%20with%20MVIP%20research%20regarding%0Aseveral%20modality%20fusion%20topics%2C%20%28automated%29%20synthetic%20data%20generation%2C%20and%0Acomplex%20data%20sampling%20--%20combined%20in%20a%20single%20application-oriented%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15448v1&entry.124074799=Read"},
{"title": "WorldCraft: Photo-Realistic 3D World Creation and Customization via LLM\n  Agents", "author": "Xinhang Liu and Chi-Keung Tang and Yu-Wing Tai", "abstract": "  Constructing photorealistic virtual worlds has applications across various\nfields, but it often requires the extensive labor of highly trained\nprofessionals to operate conventional 3D modeling software. To democratize this\nprocess, we introduce WorldCraft, a system where large language model (LLM)\nagents leverage procedural generation to create indoor and outdoor scenes\npopulated with objects, allowing users to control individual object attributes\nand the scene layout using intuitive natural language commands. In our\nframework, a coordinator agent manages the overall process and works with two\nspecialized LLM agents to complete the scene creation: ForgeIt, which\nintegrates an ever-growing manual through auto-verification to enable precise\ncustomization of individual objects, and ArrangeIt, which formulates\nhierarchical optimization problems to achieve a layout that balances ergonomic\nand aesthetic considerations. Additionally, our pipeline incorporates a\ntrajectory control agent, allowing users to animate the scene and operate the\ncamera through natural language interactions. Our system is also compatible\nwith off-the-shelf deep 3D generators to enrich scene assets. Through\nevaluations and comparisons with state-of-the-art methods, we demonstrate the\nversatility of WorldCraft, ranging from single-object customization to\nintricate, large-scale interior and exterior scene designs. This system\nempowers non-professionals to bring their creative visions to life.\n", "link": "http://arxiv.org/abs/2502.15601v1", "date": "2025-02-21", "relevancy": 2.3088, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6102}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5706}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WorldCraft%3A%20Photo-Realistic%203D%20World%20Creation%20and%20Customization%20via%20LLM%0A%20%20Agents&body=Title%3A%20WorldCraft%3A%20Photo-Realistic%203D%20World%20Creation%20and%20Customization%20via%20LLM%0A%20%20Agents%0AAuthor%3A%20Xinhang%20Liu%20and%20Chi-Keung%20Tang%20and%20Yu-Wing%20Tai%0AAbstract%3A%20%20%20Constructing%20photorealistic%20virtual%20worlds%20has%20applications%20across%20various%0Afields%2C%20but%20it%20often%20requires%20the%20extensive%20labor%20of%20highly%20trained%0Aprofessionals%20to%20operate%20conventional%203D%20modeling%20software.%20To%20democratize%20this%0Aprocess%2C%20we%20introduce%20WorldCraft%2C%20a%20system%20where%20large%20language%20model%20%28LLM%29%0Aagents%20leverage%20procedural%20generation%20to%20create%20indoor%20and%20outdoor%20scenes%0Apopulated%20with%20objects%2C%20allowing%20users%20to%20control%20individual%20object%20attributes%0Aand%20the%20scene%20layout%20using%20intuitive%20natural%20language%20commands.%20In%20our%0Aframework%2C%20a%20coordinator%20agent%20manages%20the%20overall%20process%20and%20works%20with%20two%0Aspecialized%20LLM%20agents%20to%20complete%20the%20scene%20creation%3A%20ForgeIt%2C%20which%0Aintegrates%20an%20ever-growing%20manual%20through%20auto-verification%20to%20enable%20precise%0Acustomization%20of%20individual%20objects%2C%20and%20ArrangeIt%2C%20which%20formulates%0Ahierarchical%20optimization%20problems%20to%20achieve%20a%20layout%20that%20balances%20ergonomic%0Aand%20aesthetic%20considerations.%20Additionally%2C%20our%20pipeline%20incorporates%20a%0Atrajectory%20control%20agent%2C%20allowing%20users%20to%20animate%20the%20scene%20and%20operate%20the%0Acamera%20through%20natural%20language%20interactions.%20Our%20system%20is%20also%20compatible%0Awith%20off-the-shelf%20deep%203D%20generators%20to%20enrich%20scene%20assets.%20Through%0Aevaluations%20and%20comparisons%20with%20state-of-the-art%20methods%2C%20we%20demonstrate%20the%0Aversatility%20of%20WorldCraft%2C%20ranging%20from%20single-object%20customization%20to%0Aintricate%2C%20large-scale%20interior%20and%20exterior%20scene%20designs.%20This%20system%0Aempowers%20non-professionals%20to%20bring%20their%20creative%20visions%20to%20life.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15601v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWorldCraft%253A%2520Photo-Realistic%25203D%2520World%2520Creation%2520and%2520Customization%2520via%2520LLM%250A%2520%2520Agents%26entry.906535625%3DXinhang%2520Liu%2520and%2520Chi-Keung%2520Tang%2520and%2520Yu-Wing%2520Tai%26entry.1292438233%3D%2520%2520Constructing%2520photorealistic%2520virtual%2520worlds%2520has%2520applications%2520across%2520various%250Afields%252C%2520but%2520it%2520often%2520requires%2520the%2520extensive%2520labor%2520of%2520highly%2520trained%250Aprofessionals%2520to%2520operate%2520conventional%25203D%2520modeling%2520software.%2520To%2520democratize%2520this%250Aprocess%252C%2520we%2520introduce%2520WorldCraft%252C%2520a%2520system%2520where%2520large%2520language%2520model%2520%2528LLM%2529%250Aagents%2520leverage%2520procedural%2520generation%2520to%2520create%2520indoor%2520and%2520outdoor%2520scenes%250Apopulated%2520with%2520objects%252C%2520allowing%2520users%2520to%2520control%2520individual%2520object%2520attributes%250Aand%2520the%2520scene%2520layout%2520using%2520intuitive%2520natural%2520language%2520commands.%2520In%2520our%250Aframework%252C%2520a%2520coordinator%2520agent%2520manages%2520the%2520overall%2520process%2520and%2520works%2520with%2520two%250Aspecialized%2520LLM%2520agents%2520to%2520complete%2520the%2520scene%2520creation%253A%2520ForgeIt%252C%2520which%250Aintegrates%2520an%2520ever-growing%2520manual%2520through%2520auto-verification%2520to%2520enable%2520precise%250Acustomization%2520of%2520individual%2520objects%252C%2520and%2520ArrangeIt%252C%2520which%2520formulates%250Ahierarchical%2520optimization%2520problems%2520to%2520achieve%2520a%2520layout%2520that%2520balances%2520ergonomic%250Aand%2520aesthetic%2520considerations.%2520Additionally%252C%2520our%2520pipeline%2520incorporates%2520a%250Atrajectory%2520control%2520agent%252C%2520allowing%2520users%2520to%2520animate%2520the%2520scene%2520and%2520operate%2520the%250Acamera%2520through%2520natural%2520language%2520interactions.%2520Our%2520system%2520is%2520also%2520compatible%250Awith%2520off-the-shelf%2520deep%25203D%2520generators%2520to%2520enrich%2520scene%2520assets.%2520Through%250Aevaluations%2520and%2520comparisons%2520with%2520state-of-the-art%2520methods%252C%2520we%2520demonstrate%2520the%250Aversatility%2520of%2520WorldCraft%252C%2520ranging%2520from%2520single-object%2520customization%2520to%250Aintricate%252C%2520large-scale%2520interior%2520and%2520exterior%2520scene%2520designs.%2520This%2520system%250Aempowers%2520non-professionals%2520to%2520bring%2520their%2520creative%2520visions%2520to%2520life.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15601v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WorldCraft%3A%20Photo-Realistic%203D%20World%20Creation%20and%20Customization%20via%20LLM%0A%20%20Agents&entry.906535625=Xinhang%20Liu%20and%20Chi-Keung%20Tang%20and%20Yu-Wing%20Tai&entry.1292438233=%20%20Constructing%20photorealistic%20virtual%20worlds%20has%20applications%20across%20various%0Afields%2C%20but%20it%20often%20requires%20the%20extensive%20labor%20of%20highly%20trained%0Aprofessionals%20to%20operate%20conventional%203D%20modeling%20software.%20To%20democratize%20this%0Aprocess%2C%20we%20introduce%20WorldCraft%2C%20a%20system%20where%20large%20language%20model%20%28LLM%29%0Aagents%20leverage%20procedural%20generation%20to%20create%20indoor%20and%20outdoor%20scenes%0Apopulated%20with%20objects%2C%20allowing%20users%20to%20control%20individual%20object%20attributes%0Aand%20the%20scene%20layout%20using%20intuitive%20natural%20language%20commands.%20In%20our%0Aframework%2C%20a%20coordinator%20agent%20manages%20the%20overall%20process%20and%20works%20with%20two%0Aspecialized%20LLM%20agents%20to%20complete%20the%20scene%20creation%3A%20ForgeIt%2C%20which%0Aintegrates%20an%20ever-growing%20manual%20through%20auto-verification%20to%20enable%20precise%0Acustomization%20of%20individual%20objects%2C%20and%20ArrangeIt%2C%20which%20formulates%0Ahierarchical%20optimization%20problems%20to%20achieve%20a%20layout%20that%20balances%20ergonomic%0Aand%20aesthetic%20considerations.%20Additionally%2C%20our%20pipeline%20incorporates%20a%0Atrajectory%20control%20agent%2C%20allowing%20users%20to%20animate%20the%20scene%20and%20operate%20the%0Acamera%20through%20natural%20language%20interactions.%20Our%20system%20is%20also%20compatible%0Awith%20off-the-shelf%20deep%203D%20generators%20to%20enrich%20scene%20assets.%20Through%0Aevaluations%20and%20comparisons%20with%20state-of-the-art%20methods%2C%20we%20demonstrate%20the%0Aversatility%20of%20WorldCraft%2C%20ranging%20from%20single-object%20customization%20to%0Aintricate%2C%20large-scale%20interior%20and%20exterior%20scene%20designs.%20This%20system%0Aempowers%20non-professionals%20to%20bring%20their%20creative%20visions%20to%20life.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15601v1&entry.124074799=Read"},
{"title": "VaViM and VaVAM: Autonomous Driving through Video Generative Modeling", "author": "Florent Bartoccioni and Elias Ramzi and Victor Besnier and Shashanka Venkataramanan and Tuan-Hung Vu and Yihong Xu and Loick Chambon and Spyros Gidaris and Serkan Odabas and David Hurych and Renaud Marlet and Alexandre Boulch and Mickael Chen and \u00c9loi Zablocki and Andrei Bursuc and Eduardo Valle and Matthieu Cord", "abstract": "  We explore the potential of large-scale generative video models for\nautonomous driving, introducing an open-source auto-regressive video model\n(VaViM) and its companion video-action model (VaVAM) to investigate how video\npre-training transfers to real-world driving. VaViM is a simple auto-regressive\nvideo model that predicts frames using spatio-temporal token sequences. We show\nthat it captures the semantics and dynamics of driving scenes. VaVAM, the\nvideo-action model, leverages the learned representations of VaViM to generate\ndriving trajectories through imitation learning. Together, the models form a\ncomplete perception-to-action pipeline. We evaluate our models in open- and\nclosed-loop driving scenarios, revealing that video-based pre-training holds\npromise for autonomous driving. Key insights include the semantic richness of\nthe learned representations, the benefits of scaling for video synthesis, and\nthe complex relationship between model size, data, and safety metrics in\nclosed-loop evaluations. We release code and model weights at\nhttps://github.com/valeoai/VideoActionModel\n", "link": "http://arxiv.org/abs/2502.15672v1", "date": "2025-02-21", "relevancy": 2.3039, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.583}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5802}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VaViM%20and%20VaVAM%3A%20Autonomous%20Driving%20through%20Video%20Generative%20Modeling&body=Title%3A%20VaViM%20and%20VaVAM%3A%20Autonomous%20Driving%20through%20Video%20Generative%20Modeling%0AAuthor%3A%20Florent%20Bartoccioni%20and%20Elias%20Ramzi%20and%20Victor%20Besnier%20and%20Shashanka%20Venkataramanan%20and%20Tuan-Hung%20Vu%20and%20Yihong%20Xu%20and%20Loick%20Chambon%20and%20Spyros%20Gidaris%20and%20Serkan%20Odabas%20and%20David%20Hurych%20and%20Renaud%20Marlet%20and%20Alexandre%20Boulch%20and%20Mickael%20Chen%20and%20%C3%89loi%20Zablocki%20and%20Andrei%20Bursuc%20and%20Eduardo%20Valle%20and%20Matthieu%20Cord%0AAbstract%3A%20%20%20We%20explore%20the%20potential%20of%20large-scale%20generative%20video%20models%20for%0Aautonomous%20driving%2C%20introducing%20an%20open-source%20auto-regressive%20video%20model%0A%28VaViM%29%20and%20its%20companion%20video-action%20model%20%28VaVAM%29%20to%20investigate%20how%20video%0Apre-training%20transfers%20to%20real-world%20driving.%20VaViM%20is%20a%20simple%20auto-regressive%0Avideo%20model%20that%20predicts%20frames%20using%20spatio-temporal%20token%20sequences.%20We%20show%0Athat%20it%20captures%20the%20semantics%20and%20dynamics%20of%20driving%20scenes.%20VaVAM%2C%20the%0Avideo-action%20model%2C%20leverages%20the%20learned%20representations%20of%20VaViM%20to%20generate%0Adriving%20trajectories%20through%20imitation%20learning.%20Together%2C%20the%20models%20form%20a%0Acomplete%20perception-to-action%20pipeline.%20We%20evaluate%20our%20models%20in%20open-%20and%0Aclosed-loop%20driving%20scenarios%2C%20revealing%20that%20video-based%20pre-training%20holds%0Apromise%20for%20autonomous%20driving.%20Key%20insights%20include%20the%20semantic%20richness%20of%0Athe%20learned%20representations%2C%20the%20benefits%20of%20scaling%20for%20video%20synthesis%2C%20and%0Athe%20complex%20relationship%20between%20model%20size%2C%20data%2C%20and%20safety%20metrics%20in%0Aclosed-loop%20evaluations.%20We%20release%20code%20and%20model%20weights%20at%0Ahttps%3A//github.com/valeoai/VideoActionModel%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15672v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVaViM%2520and%2520VaVAM%253A%2520Autonomous%2520Driving%2520through%2520Video%2520Generative%2520Modeling%26entry.906535625%3DFlorent%2520Bartoccioni%2520and%2520Elias%2520Ramzi%2520and%2520Victor%2520Besnier%2520and%2520Shashanka%2520Venkataramanan%2520and%2520Tuan-Hung%2520Vu%2520and%2520Yihong%2520Xu%2520and%2520Loick%2520Chambon%2520and%2520Spyros%2520Gidaris%2520and%2520Serkan%2520Odabas%2520and%2520David%2520Hurych%2520and%2520Renaud%2520Marlet%2520and%2520Alexandre%2520Boulch%2520and%2520Mickael%2520Chen%2520and%2520%25C3%2589loi%2520Zablocki%2520and%2520Andrei%2520Bursuc%2520and%2520Eduardo%2520Valle%2520and%2520Matthieu%2520Cord%26entry.1292438233%3D%2520%2520We%2520explore%2520the%2520potential%2520of%2520large-scale%2520generative%2520video%2520models%2520for%250Aautonomous%2520driving%252C%2520introducing%2520an%2520open-source%2520auto-regressive%2520video%2520model%250A%2528VaViM%2529%2520and%2520its%2520companion%2520video-action%2520model%2520%2528VaVAM%2529%2520to%2520investigate%2520how%2520video%250Apre-training%2520transfers%2520to%2520real-world%2520driving.%2520VaViM%2520is%2520a%2520simple%2520auto-regressive%250Avideo%2520model%2520that%2520predicts%2520frames%2520using%2520spatio-temporal%2520token%2520sequences.%2520We%2520show%250Athat%2520it%2520captures%2520the%2520semantics%2520and%2520dynamics%2520of%2520driving%2520scenes.%2520VaVAM%252C%2520the%250Avideo-action%2520model%252C%2520leverages%2520the%2520learned%2520representations%2520of%2520VaViM%2520to%2520generate%250Adriving%2520trajectories%2520through%2520imitation%2520learning.%2520Together%252C%2520the%2520models%2520form%2520a%250Acomplete%2520perception-to-action%2520pipeline.%2520We%2520evaluate%2520our%2520models%2520in%2520open-%2520and%250Aclosed-loop%2520driving%2520scenarios%252C%2520revealing%2520that%2520video-based%2520pre-training%2520holds%250Apromise%2520for%2520autonomous%2520driving.%2520Key%2520insights%2520include%2520the%2520semantic%2520richness%2520of%250Athe%2520learned%2520representations%252C%2520the%2520benefits%2520of%2520scaling%2520for%2520video%2520synthesis%252C%2520and%250Athe%2520complex%2520relationship%2520between%2520model%2520size%252C%2520data%252C%2520and%2520safety%2520metrics%2520in%250Aclosed-loop%2520evaluations.%2520We%2520release%2520code%2520and%2520model%2520weights%2520at%250Ahttps%253A//github.com/valeoai/VideoActionModel%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15672v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VaViM%20and%20VaVAM%3A%20Autonomous%20Driving%20through%20Video%20Generative%20Modeling&entry.906535625=Florent%20Bartoccioni%20and%20Elias%20Ramzi%20and%20Victor%20Besnier%20and%20Shashanka%20Venkataramanan%20and%20Tuan-Hung%20Vu%20and%20Yihong%20Xu%20and%20Loick%20Chambon%20and%20Spyros%20Gidaris%20and%20Serkan%20Odabas%20and%20David%20Hurych%20and%20Renaud%20Marlet%20and%20Alexandre%20Boulch%20and%20Mickael%20Chen%20and%20%C3%89loi%20Zablocki%20and%20Andrei%20Bursuc%20and%20Eduardo%20Valle%20and%20Matthieu%20Cord&entry.1292438233=%20%20We%20explore%20the%20potential%20of%20large-scale%20generative%20video%20models%20for%0Aautonomous%20driving%2C%20introducing%20an%20open-source%20auto-regressive%20video%20model%0A%28VaViM%29%20and%20its%20companion%20video-action%20model%20%28VaVAM%29%20to%20investigate%20how%20video%0Apre-training%20transfers%20to%20real-world%20driving.%20VaViM%20is%20a%20simple%20auto-regressive%0Avideo%20model%20that%20predicts%20frames%20using%20spatio-temporal%20token%20sequences.%20We%20show%0Athat%20it%20captures%20the%20semantics%20and%20dynamics%20of%20driving%20scenes.%20VaVAM%2C%20the%0Avideo-action%20model%2C%20leverages%20the%20learned%20representations%20of%20VaViM%20to%20generate%0Adriving%20trajectories%20through%20imitation%20learning.%20Together%2C%20the%20models%20form%20a%0Acomplete%20perception-to-action%20pipeline.%20We%20evaluate%20our%20models%20in%20open-%20and%0Aclosed-loop%20driving%20scenarios%2C%20revealing%20that%20video-based%20pre-training%20holds%0Apromise%20for%20autonomous%20driving.%20Key%20insights%20include%20the%20semantic%20richness%20of%0Athe%20learned%20representations%2C%20the%20benefits%20of%20scaling%20for%20video%20synthesis%2C%20and%0Athe%20complex%20relationship%20between%20model%20size%2C%20data%2C%20and%20safety%20metrics%20in%0Aclosed-loop%20evaluations.%20We%20release%20code%20and%20model%20weights%20at%0Ahttps%3A//github.com/valeoai/VideoActionModel%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15672v1&entry.124074799=Read"},
{"title": "Weakly Supervised Video Scene Graph Generation via Natural Language\n  Supervision", "author": "Kibum Kim and Kanghoon Yoon and Yeonjun In and Jaehyeong Jeon and Jinyoung Moon and Donghyun Kim and Chanyoung Park", "abstract": "  Existing Video Scene Graph Generation (VidSGG) studies are trained in a fully\nsupervised manner, which requires all frames in a video to be annotated,\nthereby incurring high annotation cost compared to Image Scene Graph Generation\n(ImgSGG). Although the annotation cost of VidSGG can be alleviated by adopting\na weakly supervised approach commonly used for ImgSGG (WS-ImgSGG) that uses\nimage captions, there are two key reasons that hinder such a naive adoption: 1)\nTemporality within video captions, i.e., unlike image captions, video captions\ninclude temporal markers (e.g., before, while, then, after) that indicate time\nrelated details, and 2) Variability in action duration, i.e., unlike human\nactions in image captions, human actions in video captions unfold over varying\nduration. To address these issues, we propose a Natural Language-based Video\nScene Graph Generation (NL-VSGG) framework that only utilizes the readily\navailable video captions for training a VidSGG model. NL-VSGG consists of two\nkey modules: Temporality-aware Caption Segmentation (TCS) module and Action\nDuration Variability-aware caption-frame alignment (ADV) module. Specifically,\nTCS segments the video captions into multiple sentences in a temporal order\nbased on a Large Language Model (LLM), and ADV aligns each segmented sentence\nwith appropriate frames considering the variability in action duration. Our\napproach leads to a significant enhancement in performance compared to simply\napplying the WS-ImgSGG pipeline to VidSGG on the Action Genome dataset. As a\nfurther benefit of utilizing the video captions as weak supervision, we show\nthat the VidSGG model trained by NL-VSGG is able to predict a broader range of\naction classes that are not included in the training data, which makes our\nframework practical in reality.\n", "link": "http://arxiv.org/abs/2502.15370v1", "date": "2025-02-21", "relevancy": 2.2947, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5778}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5757}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weakly%20Supervised%20Video%20Scene%20Graph%20Generation%20via%20Natural%20Language%0A%20%20Supervision&body=Title%3A%20Weakly%20Supervised%20Video%20Scene%20Graph%20Generation%20via%20Natural%20Language%0A%20%20Supervision%0AAuthor%3A%20Kibum%20Kim%20and%20Kanghoon%20Yoon%20and%20Yeonjun%20In%20and%20Jaehyeong%20Jeon%20and%20Jinyoung%20Moon%20and%20Donghyun%20Kim%20and%20Chanyoung%20Park%0AAbstract%3A%20%20%20Existing%20Video%20Scene%20Graph%20Generation%20%28VidSGG%29%20studies%20are%20trained%20in%20a%20fully%0Asupervised%20manner%2C%20which%20requires%20all%20frames%20in%20a%20video%20to%20be%20annotated%2C%0Athereby%20incurring%20high%20annotation%20cost%20compared%20to%20Image%20Scene%20Graph%20Generation%0A%28ImgSGG%29.%20Although%20the%20annotation%20cost%20of%20VidSGG%20can%20be%20alleviated%20by%20adopting%0Aa%20weakly%20supervised%20approach%20commonly%20used%20for%20ImgSGG%20%28WS-ImgSGG%29%20that%20uses%0Aimage%20captions%2C%20there%20are%20two%20key%20reasons%20that%20hinder%20such%20a%20naive%20adoption%3A%201%29%0ATemporality%20within%20video%20captions%2C%20i.e.%2C%20unlike%20image%20captions%2C%20video%20captions%0Ainclude%20temporal%20markers%20%28e.g.%2C%20before%2C%20while%2C%20then%2C%20after%29%20that%20indicate%20time%0Arelated%20details%2C%20and%202%29%20Variability%20in%20action%20duration%2C%20i.e.%2C%20unlike%20human%0Aactions%20in%20image%20captions%2C%20human%20actions%20in%20video%20captions%20unfold%20over%20varying%0Aduration.%20To%20address%20these%20issues%2C%20we%20propose%20a%20Natural%20Language-based%20Video%0AScene%20Graph%20Generation%20%28NL-VSGG%29%20framework%20that%20only%20utilizes%20the%20readily%0Aavailable%20video%20captions%20for%20training%20a%20VidSGG%20model.%20NL-VSGG%20consists%20of%20two%0Akey%20modules%3A%20Temporality-aware%20Caption%20Segmentation%20%28TCS%29%20module%20and%20Action%0ADuration%20Variability-aware%20caption-frame%20alignment%20%28ADV%29%20module.%20Specifically%2C%0ATCS%20segments%20the%20video%20captions%20into%20multiple%20sentences%20in%20a%20temporal%20order%0Abased%20on%20a%20Large%20Language%20Model%20%28LLM%29%2C%20and%20ADV%20aligns%20each%20segmented%20sentence%0Awith%20appropriate%20frames%20considering%20the%20variability%20in%20action%20duration.%20Our%0Aapproach%20leads%20to%20a%20significant%20enhancement%20in%20performance%20compared%20to%20simply%0Aapplying%20the%20WS-ImgSGG%20pipeline%20to%20VidSGG%20on%20the%20Action%20Genome%20dataset.%20As%20a%0Afurther%20benefit%20of%20utilizing%20the%20video%20captions%20as%20weak%20supervision%2C%20we%20show%0Athat%20the%20VidSGG%20model%20trained%20by%20NL-VSGG%20is%20able%20to%20predict%20a%20broader%20range%20of%0Aaction%20classes%20that%20are%20not%20included%20in%20the%20training%20data%2C%20which%20makes%20our%0Aframework%20practical%20in%20reality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15370v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeakly%2520Supervised%2520Video%2520Scene%2520Graph%2520Generation%2520via%2520Natural%2520Language%250A%2520%2520Supervision%26entry.906535625%3DKibum%2520Kim%2520and%2520Kanghoon%2520Yoon%2520and%2520Yeonjun%2520In%2520and%2520Jaehyeong%2520Jeon%2520and%2520Jinyoung%2520Moon%2520and%2520Donghyun%2520Kim%2520and%2520Chanyoung%2520Park%26entry.1292438233%3D%2520%2520Existing%2520Video%2520Scene%2520Graph%2520Generation%2520%2528VidSGG%2529%2520studies%2520are%2520trained%2520in%2520a%2520fully%250Asupervised%2520manner%252C%2520which%2520requires%2520all%2520frames%2520in%2520a%2520video%2520to%2520be%2520annotated%252C%250Athereby%2520incurring%2520high%2520annotation%2520cost%2520compared%2520to%2520Image%2520Scene%2520Graph%2520Generation%250A%2528ImgSGG%2529.%2520Although%2520the%2520annotation%2520cost%2520of%2520VidSGG%2520can%2520be%2520alleviated%2520by%2520adopting%250Aa%2520weakly%2520supervised%2520approach%2520commonly%2520used%2520for%2520ImgSGG%2520%2528WS-ImgSGG%2529%2520that%2520uses%250Aimage%2520captions%252C%2520there%2520are%2520two%2520key%2520reasons%2520that%2520hinder%2520such%2520a%2520naive%2520adoption%253A%25201%2529%250ATemporality%2520within%2520video%2520captions%252C%2520i.e.%252C%2520unlike%2520image%2520captions%252C%2520video%2520captions%250Ainclude%2520temporal%2520markers%2520%2528e.g.%252C%2520before%252C%2520while%252C%2520then%252C%2520after%2529%2520that%2520indicate%2520time%250Arelated%2520details%252C%2520and%25202%2529%2520Variability%2520in%2520action%2520duration%252C%2520i.e.%252C%2520unlike%2520human%250Aactions%2520in%2520image%2520captions%252C%2520human%2520actions%2520in%2520video%2520captions%2520unfold%2520over%2520varying%250Aduration.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520Natural%2520Language-based%2520Video%250AScene%2520Graph%2520Generation%2520%2528NL-VSGG%2529%2520framework%2520that%2520only%2520utilizes%2520the%2520readily%250Aavailable%2520video%2520captions%2520for%2520training%2520a%2520VidSGG%2520model.%2520NL-VSGG%2520consists%2520of%2520two%250Akey%2520modules%253A%2520Temporality-aware%2520Caption%2520Segmentation%2520%2528TCS%2529%2520module%2520and%2520Action%250ADuration%2520Variability-aware%2520caption-frame%2520alignment%2520%2528ADV%2529%2520module.%2520Specifically%252C%250ATCS%2520segments%2520the%2520video%2520captions%2520into%2520multiple%2520sentences%2520in%2520a%2520temporal%2520order%250Abased%2520on%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529%252C%2520and%2520ADV%2520aligns%2520each%2520segmented%2520sentence%250Awith%2520appropriate%2520frames%2520considering%2520the%2520variability%2520in%2520action%2520duration.%2520Our%250Aapproach%2520leads%2520to%2520a%2520significant%2520enhancement%2520in%2520performance%2520compared%2520to%2520simply%250Aapplying%2520the%2520WS-ImgSGG%2520pipeline%2520to%2520VidSGG%2520on%2520the%2520Action%2520Genome%2520dataset.%2520As%2520a%250Afurther%2520benefit%2520of%2520utilizing%2520the%2520video%2520captions%2520as%2520weak%2520supervision%252C%2520we%2520show%250Athat%2520the%2520VidSGG%2520model%2520trained%2520by%2520NL-VSGG%2520is%2520able%2520to%2520predict%2520a%2520broader%2520range%2520of%250Aaction%2520classes%2520that%2520are%2520not%2520included%2520in%2520the%2520training%2520data%252C%2520which%2520makes%2520our%250Aframework%2520practical%2520in%2520reality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15370v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weakly%20Supervised%20Video%20Scene%20Graph%20Generation%20via%20Natural%20Language%0A%20%20Supervision&entry.906535625=Kibum%20Kim%20and%20Kanghoon%20Yoon%20and%20Yeonjun%20In%20and%20Jaehyeong%20Jeon%20and%20Jinyoung%20Moon%20and%20Donghyun%20Kim%20and%20Chanyoung%20Park&entry.1292438233=%20%20Existing%20Video%20Scene%20Graph%20Generation%20%28VidSGG%29%20studies%20are%20trained%20in%20a%20fully%0Asupervised%20manner%2C%20which%20requires%20all%20frames%20in%20a%20video%20to%20be%20annotated%2C%0Athereby%20incurring%20high%20annotation%20cost%20compared%20to%20Image%20Scene%20Graph%20Generation%0A%28ImgSGG%29.%20Although%20the%20annotation%20cost%20of%20VidSGG%20can%20be%20alleviated%20by%20adopting%0Aa%20weakly%20supervised%20approach%20commonly%20used%20for%20ImgSGG%20%28WS-ImgSGG%29%20that%20uses%0Aimage%20captions%2C%20there%20are%20two%20key%20reasons%20that%20hinder%20such%20a%20naive%20adoption%3A%201%29%0ATemporality%20within%20video%20captions%2C%20i.e.%2C%20unlike%20image%20captions%2C%20video%20captions%0Ainclude%20temporal%20markers%20%28e.g.%2C%20before%2C%20while%2C%20then%2C%20after%29%20that%20indicate%20time%0Arelated%20details%2C%20and%202%29%20Variability%20in%20action%20duration%2C%20i.e.%2C%20unlike%20human%0Aactions%20in%20image%20captions%2C%20human%20actions%20in%20video%20captions%20unfold%20over%20varying%0Aduration.%20To%20address%20these%20issues%2C%20we%20propose%20a%20Natural%20Language-based%20Video%0AScene%20Graph%20Generation%20%28NL-VSGG%29%20framework%20that%20only%20utilizes%20the%20readily%0Aavailable%20video%20captions%20for%20training%20a%20VidSGG%20model.%20NL-VSGG%20consists%20of%20two%0Akey%20modules%3A%20Temporality-aware%20Caption%20Segmentation%20%28TCS%29%20module%20and%20Action%0ADuration%20Variability-aware%20caption-frame%20alignment%20%28ADV%29%20module.%20Specifically%2C%0ATCS%20segments%20the%20video%20captions%20into%20multiple%20sentences%20in%20a%20temporal%20order%0Abased%20on%20a%20Large%20Language%20Model%20%28LLM%29%2C%20and%20ADV%20aligns%20each%20segmented%20sentence%0Awith%20appropriate%20frames%20considering%20the%20variability%20in%20action%20duration.%20Our%0Aapproach%20leads%20to%20a%20significant%20enhancement%20in%20performance%20compared%20to%20simply%0Aapplying%20the%20WS-ImgSGG%20pipeline%20to%20VidSGG%20on%20the%20Action%20Genome%20dataset.%20As%20a%0Afurther%20benefit%20of%20utilizing%20the%20video%20captions%20as%20weak%20supervision%2C%20we%20show%0Athat%20the%20VidSGG%20model%20trained%20by%20NL-VSGG%20is%20able%20to%20predict%20a%20broader%20range%20of%0Aaction%20classes%20that%20are%20not%20included%20in%20the%20training%20data%2C%20which%20makes%20our%0Aframework%20practical%20in%20reality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15370v1&entry.124074799=Read"},
{"title": "LaRE$^2$: Latent Reconstruction Error Based Method for\n  Diffusion-Generated Image Detection", "author": "Yunpeng Luo and Junlong Du and Ke Yan and Shouhong Ding", "abstract": "  The evolution of Diffusion Models has dramatically improved image generation\nquality, making it increasingly difficult to differentiate between real and\ngenerated images. This development, while impressive, also raises significant\nprivacy and security concerns. In response to this, we propose a novel Latent\nREconstruction error guided feature REfinement method (LaRE^2) for detecting\nthe diffusion-generated images. We come up with the Latent Reconstruction Error\n(LaRE), the first reconstruction-error based feature in the latent space for\ngenerated image detection. LaRE surpasses existing methods in terms of feature\nextraction efficiency while preserving crucial cues required to differentiate\nbetween the real and the fake. To exploit LaRE, we propose an Error-Guided\nfeature REfinement module (EGRE), which can refine the image feature guided by\nLaRE to enhance the discriminativeness of the feature. Our EGRE utilizes an\nalign-then-refine mechanism, which effectively refines the image feature for\ngenerated-image detection from both spatial and channel perspectives. Extensive\nexperiments on the large-scale GenImage benchmark demonstrate the superiority\nof our LaRE^2, which surpasses the best SoTA method by up to 11.9%/12.1%\naverage ACC/AP across 8 different image generators. LaRE also surpasses\nexisting methods in terms of feature extraction cost, delivering an impressive\nspeed enhancement of 8 times. Code is available.\n", "link": "http://arxiv.org/abs/2403.17465v4", "date": "2025-02-21", "relevancy": 2.2731, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6096}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5623}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LaRE%24%5E2%24%3A%20Latent%20Reconstruction%20Error%20Based%20Method%20for%0A%20%20Diffusion-Generated%20Image%20Detection&body=Title%3A%20LaRE%24%5E2%24%3A%20Latent%20Reconstruction%20Error%20Based%20Method%20for%0A%20%20Diffusion-Generated%20Image%20Detection%0AAuthor%3A%20Yunpeng%20Luo%20and%20Junlong%20Du%20and%20Ke%20Yan%20and%20Shouhong%20Ding%0AAbstract%3A%20%20%20The%20evolution%20of%20Diffusion%20Models%20has%20dramatically%20improved%20image%20generation%0Aquality%2C%20making%20it%20increasingly%20difficult%20to%20differentiate%20between%20real%20and%0Agenerated%20images.%20This%20development%2C%20while%20impressive%2C%20also%20raises%20significant%0Aprivacy%20and%20security%20concerns.%20In%20response%20to%20this%2C%20we%20propose%20a%20novel%20Latent%0AREconstruction%20error%20guided%20feature%20REfinement%20method%20%28LaRE%5E2%29%20for%20detecting%0Athe%20diffusion-generated%20images.%20We%20come%20up%20with%20the%20Latent%20Reconstruction%20Error%0A%28LaRE%29%2C%20the%20first%20reconstruction-error%20based%20feature%20in%20the%20latent%20space%20for%0Agenerated%20image%20detection.%20LaRE%20surpasses%20existing%20methods%20in%20terms%20of%20feature%0Aextraction%20efficiency%20while%20preserving%20crucial%20cues%20required%20to%20differentiate%0Abetween%20the%20real%20and%20the%20fake.%20To%20exploit%20LaRE%2C%20we%20propose%20an%20Error-Guided%0Afeature%20REfinement%20module%20%28EGRE%29%2C%20which%20can%20refine%20the%20image%20feature%20guided%20by%0ALaRE%20to%20enhance%20the%20discriminativeness%20of%20the%20feature.%20Our%20EGRE%20utilizes%20an%0Aalign-then-refine%20mechanism%2C%20which%20effectively%20refines%20the%20image%20feature%20for%0Agenerated-image%20detection%20from%20both%20spatial%20and%20channel%20perspectives.%20Extensive%0Aexperiments%20on%20the%20large-scale%20GenImage%20benchmark%20demonstrate%20the%20superiority%0Aof%20our%20LaRE%5E2%2C%20which%20surpasses%20the%20best%20SoTA%20method%20by%20up%20to%2011.9%25/12.1%25%0Aaverage%20ACC/AP%20across%208%20different%20image%20generators.%20LaRE%20also%20surpasses%0Aexisting%20methods%20in%20terms%20of%20feature%20extraction%20cost%2C%20delivering%20an%20impressive%0Aspeed%20enhancement%20of%208%20times.%20Code%20is%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17465v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaRE%2524%255E2%2524%253A%2520Latent%2520Reconstruction%2520Error%2520Based%2520Method%2520for%250A%2520%2520Diffusion-Generated%2520Image%2520Detection%26entry.906535625%3DYunpeng%2520Luo%2520and%2520Junlong%2520Du%2520and%2520Ke%2520Yan%2520and%2520Shouhong%2520Ding%26entry.1292438233%3D%2520%2520The%2520evolution%2520of%2520Diffusion%2520Models%2520has%2520dramatically%2520improved%2520image%2520generation%250Aquality%252C%2520making%2520it%2520increasingly%2520difficult%2520to%2520differentiate%2520between%2520real%2520and%250Agenerated%2520images.%2520This%2520development%252C%2520while%2520impressive%252C%2520also%2520raises%2520significant%250Aprivacy%2520and%2520security%2520concerns.%2520In%2520response%2520to%2520this%252C%2520we%2520propose%2520a%2520novel%2520Latent%250AREconstruction%2520error%2520guided%2520feature%2520REfinement%2520method%2520%2528LaRE%255E2%2529%2520for%2520detecting%250Athe%2520diffusion-generated%2520images.%2520We%2520come%2520up%2520with%2520the%2520Latent%2520Reconstruction%2520Error%250A%2528LaRE%2529%252C%2520the%2520first%2520reconstruction-error%2520based%2520feature%2520in%2520the%2520latent%2520space%2520for%250Agenerated%2520image%2520detection.%2520LaRE%2520surpasses%2520existing%2520methods%2520in%2520terms%2520of%2520feature%250Aextraction%2520efficiency%2520while%2520preserving%2520crucial%2520cues%2520required%2520to%2520differentiate%250Abetween%2520the%2520real%2520and%2520the%2520fake.%2520To%2520exploit%2520LaRE%252C%2520we%2520propose%2520an%2520Error-Guided%250Afeature%2520REfinement%2520module%2520%2528EGRE%2529%252C%2520which%2520can%2520refine%2520the%2520image%2520feature%2520guided%2520by%250ALaRE%2520to%2520enhance%2520the%2520discriminativeness%2520of%2520the%2520feature.%2520Our%2520EGRE%2520utilizes%2520an%250Aalign-then-refine%2520mechanism%252C%2520which%2520effectively%2520refines%2520the%2520image%2520feature%2520for%250Agenerated-image%2520detection%2520from%2520both%2520spatial%2520and%2520channel%2520perspectives.%2520Extensive%250Aexperiments%2520on%2520the%2520large-scale%2520GenImage%2520benchmark%2520demonstrate%2520the%2520superiority%250Aof%2520our%2520LaRE%255E2%252C%2520which%2520surpasses%2520the%2520best%2520SoTA%2520method%2520by%2520up%2520to%252011.9%2525/12.1%2525%250Aaverage%2520ACC/AP%2520across%25208%2520different%2520image%2520generators.%2520LaRE%2520also%2520surpasses%250Aexisting%2520methods%2520in%2520terms%2520of%2520feature%2520extraction%2520cost%252C%2520delivering%2520an%2520impressive%250Aspeed%2520enhancement%2520of%25208%2520times.%2520Code%2520is%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.17465v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaRE%24%5E2%24%3A%20Latent%20Reconstruction%20Error%20Based%20Method%20for%0A%20%20Diffusion-Generated%20Image%20Detection&entry.906535625=Yunpeng%20Luo%20and%20Junlong%20Du%20and%20Ke%20Yan%20and%20Shouhong%20Ding&entry.1292438233=%20%20The%20evolution%20of%20Diffusion%20Models%20has%20dramatically%20improved%20image%20generation%0Aquality%2C%20making%20it%20increasingly%20difficult%20to%20differentiate%20between%20real%20and%0Agenerated%20images.%20This%20development%2C%20while%20impressive%2C%20also%20raises%20significant%0Aprivacy%20and%20security%20concerns.%20In%20response%20to%20this%2C%20we%20propose%20a%20novel%20Latent%0AREconstruction%20error%20guided%20feature%20REfinement%20method%20%28LaRE%5E2%29%20for%20detecting%0Athe%20diffusion-generated%20images.%20We%20come%20up%20with%20the%20Latent%20Reconstruction%20Error%0A%28LaRE%29%2C%20the%20first%20reconstruction-error%20based%20feature%20in%20the%20latent%20space%20for%0Agenerated%20image%20detection.%20LaRE%20surpasses%20existing%20methods%20in%20terms%20of%20feature%0Aextraction%20efficiency%20while%20preserving%20crucial%20cues%20required%20to%20differentiate%0Abetween%20the%20real%20and%20the%20fake.%20To%20exploit%20LaRE%2C%20we%20propose%20an%20Error-Guided%0Afeature%20REfinement%20module%20%28EGRE%29%2C%20which%20can%20refine%20the%20image%20feature%20guided%20by%0ALaRE%20to%20enhance%20the%20discriminativeness%20of%20the%20feature.%20Our%20EGRE%20utilizes%20an%0Aalign-then-refine%20mechanism%2C%20which%20effectively%20refines%20the%20image%20feature%20for%0Agenerated-image%20detection%20from%20both%20spatial%20and%20channel%20perspectives.%20Extensive%0Aexperiments%20on%20the%20large-scale%20GenImage%20benchmark%20demonstrate%20the%20superiority%0Aof%20our%20LaRE%5E2%2C%20which%20surpasses%20the%20best%20SoTA%20method%20by%20up%20to%2011.9%25/12.1%25%0Aaverage%20ACC/AP%20across%208%20different%20image%20generators.%20LaRE%20also%20surpasses%0Aexisting%20methods%20in%20terms%20of%20feature%20extraction%20cost%2C%20delivering%20an%20impressive%0Aspeed%20enhancement%20of%208%20times.%20Code%20is%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17465v4&entry.124074799=Read"},
{"title": "Self-Supervised Diffusion MRI Denoising via Iterative and Stable\n  Refinement", "author": "Chenxu Wu and Qingpeng Kong and Zihang Jiang and S. Kevin Zhou", "abstract": "  Magnetic Resonance Imaging (MRI), including diffusion MRI (dMRI), serves as a\n``microscope'' for anatomical structures and routinely mitigates the influence\nof low signal-to-noise ratio scans by compromising temporal or spatial\nresolution. However, these compromises fail to meet clinical demands for both\nefficiency and precision. Consequently, denoising is a vital preprocessing\nstep, particularly for dMRI, where clean data is unavailable. In this paper, we\nintroduce Di-Fusion, a fully self-supervised denoising method that leverages\nthe latter diffusion steps and an adaptive sampling process. Unlike previous\napproaches, our single-stage framework achieves efficient and stable training\nwithout extra noise model training and offers adaptive and controllable results\nin the sampling process. Our thorough experiments on real and simulated data\ndemonstrate that Di-Fusion achieves state-of-the-art performance in\nmicrostructure modeling, tractography tracking, and other downstream tasks.\nCode is available at https://github.com/FouierL/Di-Fusion.\n", "link": "http://arxiv.org/abs/2501.13514v2", "date": "2025-02-21", "relevancy": 2.2667, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5818}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.57}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Diffusion%20MRI%20Denoising%20via%20Iterative%20and%20Stable%0A%20%20Refinement&body=Title%3A%20Self-Supervised%20Diffusion%20MRI%20Denoising%20via%20Iterative%20and%20Stable%0A%20%20Refinement%0AAuthor%3A%20Chenxu%20Wu%20and%20Qingpeng%20Kong%20and%20Zihang%20Jiang%20and%20S.%20Kevin%20Zhou%0AAbstract%3A%20%20%20Magnetic%20Resonance%20Imaging%20%28MRI%29%2C%20including%20diffusion%20MRI%20%28dMRI%29%2C%20serves%20as%20a%0A%60%60microscope%27%27%20for%20anatomical%20structures%20and%20routinely%20mitigates%20the%20influence%0Aof%20low%20signal-to-noise%20ratio%20scans%20by%20compromising%20temporal%20or%20spatial%0Aresolution.%20However%2C%20these%20compromises%20fail%20to%20meet%20clinical%20demands%20for%20both%0Aefficiency%20and%20precision.%20Consequently%2C%20denoising%20is%20a%20vital%20preprocessing%0Astep%2C%20particularly%20for%20dMRI%2C%20where%20clean%20data%20is%20unavailable.%20In%20this%20paper%2C%20we%0Aintroduce%20Di-Fusion%2C%20a%20fully%20self-supervised%20denoising%20method%20that%20leverages%0Athe%20latter%20diffusion%20steps%20and%20an%20adaptive%20sampling%20process.%20Unlike%20previous%0Aapproaches%2C%20our%20single-stage%20framework%20achieves%20efficient%20and%20stable%20training%0Awithout%20extra%20noise%20model%20training%20and%20offers%20adaptive%20and%20controllable%20results%0Ain%20the%20sampling%20process.%20Our%20thorough%20experiments%20on%20real%20and%20simulated%20data%0Ademonstrate%20that%20Di-Fusion%20achieves%20state-of-the-art%20performance%20in%0Amicrostructure%20modeling%2C%20tractography%20tracking%2C%20and%20other%20downstream%20tasks.%0ACode%20is%20available%20at%20https%3A//github.com/FouierL/Di-Fusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13514v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Diffusion%2520MRI%2520Denoising%2520via%2520Iterative%2520and%2520Stable%250A%2520%2520Refinement%26entry.906535625%3DChenxu%2520Wu%2520and%2520Qingpeng%2520Kong%2520and%2520Zihang%2520Jiang%2520and%2520S.%2520Kevin%2520Zhou%26entry.1292438233%3D%2520%2520Magnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%252C%2520including%2520diffusion%2520MRI%2520%2528dMRI%2529%252C%2520serves%2520as%2520a%250A%2560%2560microscope%2527%2527%2520for%2520anatomical%2520structures%2520and%2520routinely%2520mitigates%2520the%2520influence%250Aof%2520low%2520signal-to-noise%2520ratio%2520scans%2520by%2520compromising%2520temporal%2520or%2520spatial%250Aresolution.%2520However%252C%2520these%2520compromises%2520fail%2520to%2520meet%2520clinical%2520demands%2520for%2520both%250Aefficiency%2520and%2520precision.%2520Consequently%252C%2520denoising%2520is%2520a%2520vital%2520preprocessing%250Astep%252C%2520particularly%2520for%2520dMRI%252C%2520where%2520clean%2520data%2520is%2520unavailable.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520Di-Fusion%252C%2520a%2520fully%2520self-supervised%2520denoising%2520method%2520that%2520leverages%250Athe%2520latter%2520diffusion%2520steps%2520and%2520an%2520adaptive%2520sampling%2520process.%2520Unlike%2520previous%250Aapproaches%252C%2520our%2520single-stage%2520framework%2520achieves%2520efficient%2520and%2520stable%2520training%250Awithout%2520extra%2520noise%2520model%2520training%2520and%2520offers%2520adaptive%2520and%2520controllable%2520results%250Ain%2520the%2520sampling%2520process.%2520Our%2520thorough%2520experiments%2520on%2520real%2520and%2520simulated%2520data%250Ademonstrate%2520that%2520Di-Fusion%2520achieves%2520state-of-the-art%2520performance%2520in%250Amicrostructure%2520modeling%252C%2520tractography%2520tracking%252C%2520and%2520other%2520downstream%2520tasks.%250ACode%2520is%2520available%2520at%2520https%253A//github.com/FouierL/Di-Fusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13514v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Diffusion%20MRI%20Denoising%20via%20Iterative%20and%20Stable%0A%20%20Refinement&entry.906535625=Chenxu%20Wu%20and%20Qingpeng%20Kong%20and%20Zihang%20Jiang%20and%20S.%20Kevin%20Zhou&entry.1292438233=%20%20Magnetic%20Resonance%20Imaging%20%28MRI%29%2C%20including%20diffusion%20MRI%20%28dMRI%29%2C%20serves%20as%20a%0A%60%60microscope%27%27%20for%20anatomical%20structures%20and%20routinely%20mitigates%20the%20influence%0Aof%20low%20signal-to-noise%20ratio%20scans%20by%20compromising%20temporal%20or%20spatial%0Aresolution.%20However%2C%20these%20compromises%20fail%20to%20meet%20clinical%20demands%20for%20both%0Aefficiency%20and%20precision.%20Consequently%2C%20denoising%20is%20a%20vital%20preprocessing%0Astep%2C%20particularly%20for%20dMRI%2C%20where%20clean%20data%20is%20unavailable.%20In%20this%20paper%2C%20we%0Aintroduce%20Di-Fusion%2C%20a%20fully%20self-supervised%20denoising%20method%20that%20leverages%0Athe%20latter%20diffusion%20steps%20and%20an%20adaptive%20sampling%20process.%20Unlike%20previous%0Aapproaches%2C%20our%20single-stage%20framework%20achieves%20efficient%20and%20stable%20training%0Awithout%20extra%20noise%20model%20training%20and%20offers%20adaptive%20and%20controllable%20results%0Ain%20the%20sampling%20process.%20Our%20thorough%20experiments%20on%20real%20and%20simulated%20data%0Ademonstrate%20that%20Di-Fusion%20achieves%20state-of-the-art%20performance%20in%0Amicrostructure%20modeling%2C%20tractography%20tracking%2C%20and%20other%20downstream%20tasks.%0ACode%20is%20available%20at%20https%3A//github.com/FouierL/Di-Fusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13514v2&entry.124074799=Read"},
{"title": "Vibravox: A Dataset of French Speech Captured with Body-conduction Audio\n  Sensors", "author": "Julien Hauret and Malo Olivier and Thomas Joubaud and Christophe Langrenne and Sarah Poir\u00e9e and V\u00e9ronique Zimpfer and \u00c9ric Bavu", "abstract": "  Vibravox is a dataset compliant with the General Data Protection Regulation\n(GDPR) containing audio recordings using five different body-conduction audio\nsensors : two in-ear microphones, two bone conduction vibration pickups and a\nlaryngophone. The dataset also includes audio data from an airborne microphone\nused as a reference. The Vibravox corpus contains 45 hours of speech samples\nand physiological sounds recorded by 188 participants under different acoustic\nconditions imposed by an high order ambisonics 3D spatializer. Annotations\nabout the recording conditions and linguistic transcriptions are also included\nin the corpus. We conducted a series of experiments on various speech-related\ntasks, including speech recognition, speech enhancement and speaker\nverification. These experiments were carried out using state-of-the-art models\nto evaluate and compare their performances on signals captured by the different\naudio sensors offered by the Vibravox dataset, with the aim of gaining a better\ngrasp of their individual characteristics.\n", "link": "http://arxiv.org/abs/2407.11828v3", "date": "2025-02-21", "relevancy": 2.2635, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4553}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4514}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vibravox%3A%20A%20Dataset%20of%20French%20Speech%20Captured%20with%20Body-conduction%20Audio%0A%20%20Sensors&body=Title%3A%20Vibravox%3A%20A%20Dataset%20of%20French%20Speech%20Captured%20with%20Body-conduction%20Audio%0A%20%20Sensors%0AAuthor%3A%20Julien%20Hauret%20and%20Malo%20Olivier%20and%20Thomas%20Joubaud%20and%20Christophe%20Langrenne%20and%20Sarah%20Poir%C3%A9e%20and%20V%C3%A9ronique%20Zimpfer%20and%20%C3%89ric%20Bavu%0AAbstract%3A%20%20%20Vibravox%20is%20a%20dataset%20compliant%20with%20the%20General%20Data%20Protection%20Regulation%0A%28GDPR%29%20containing%20audio%20recordings%20using%20five%20different%20body-conduction%20audio%0Asensors%20%3A%20two%20in-ear%20microphones%2C%20two%20bone%20conduction%20vibration%20pickups%20and%20a%0Alaryngophone.%20The%20dataset%20also%20includes%20audio%20data%20from%20an%20airborne%20microphone%0Aused%20as%20a%20reference.%20The%20Vibravox%20corpus%20contains%2045%20hours%20of%20speech%20samples%0Aand%20physiological%20sounds%20recorded%20by%20188%20participants%20under%20different%20acoustic%0Aconditions%20imposed%20by%20an%20high%20order%20ambisonics%203D%20spatializer.%20Annotations%0Aabout%20the%20recording%20conditions%20and%20linguistic%20transcriptions%20are%20also%20included%0Ain%20the%20corpus.%20We%20conducted%20a%20series%20of%20experiments%20on%20various%20speech-related%0Atasks%2C%20including%20speech%20recognition%2C%20speech%20enhancement%20and%20speaker%0Averification.%20These%20experiments%20were%20carried%20out%20using%20state-of-the-art%20models%0Ato%20evaluate%20and%20compare%20their%20performances%20on%20signals%20captured%20by%20the%20different%0Aaudio%20sensors%20offered%20by%20the%20Vibravox%20dataset%2C%20with%20the%20aim%20of%20gaining%20a%20better%0Agrasp%20of%20their%20individual%20characteristics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11828v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVibravox%253A%2520A%2520Dataset%2520of%2520French%2520Speech%2520Captured%2520with%2520Body-conduction%2520Audio%250A%2520%2520Sensors%26entry.906535625%3DJulien%2520Hauret%2520and%2520Malo%2520Olivier%2520and%2520Thomas%2520Joubaud%2520and%2520Christophe%2520Langrenne%2520and%2520Sarah%2520Poir%25C3%25A9e%2520and%2520V%25C3%25A9ronique%2520Zimpfer%2520and%2520%25C3%2589ric%2520Bavu%26entry.1292438233%3D%2520%2520Vibravox%2520is%2520a%2520dataset%2520compliant%2520with%2520the%2520General%2520Data%2520Protection%2520Regulation%250A%2528GDPR%2529%2520containing%2520audio%2520recordings%2520using%2520five%2520different%2520body-conduction%2520audio%250Asensors%2520%253A%2520two%2520in-ear%2520microphones%252C%2520two%2520bone%2520conduction%2520vibration%2520pickups%2520and%2520a%250Alaryngophone.%2520The%2520dataset%2520also%2520includes%2520audio%2520data%2520from%2520an%2520airborne%2520microphone%250Aused%2520as%2520a%2520reference.%2520The%2520Vibravox%2520corpus%2520contains%252045%2520hours%2520of%2520speech%2520samples%250Aand%2520physiological%2520sounds%2520recorded%2520by%2520188%2520participants%2520under%2520different%2520acoustic%250Aconditions%2520imposed%2520by%2520an%2520high%2520order%2520ambisonics%25203D%2520spatializer.%2520Annotations%250Aabout%2520the%2520recording%2520conditions%2520and%2520linguistic%2520transcriptions%2520are%2520also%2520included%250Ain%2520the%2520corpus.%2520We%2520conducted%2520a%2520series%2520of%2520experiments%2520on%2520various%2520speech-related%250Atasks%252C%2520including%2520speech%2520recognition%252C%2520speech%2520enhancement%2520and%2520speaker%250Averification.%2520These%2520experiments%2520were%2520carried%2520out%2520using%2520state-of-the-art%2520models%250Ato%2520evaluate%2520and%2520compare%2520their%2520performances%2520on%2520signals%2520captured%2520by%2520the%2520different%250Aaudio%2520sensors%2520offered%2520by%2520the%2520Vibravox%2520dataset%252C%2520with%2520the%2520aim%2520of%2520gaining%2520a%2520better%250Agrasp%2520of%2520their%2520individual%2520characteristics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11828v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vibravox%3A%20A%20Dataset%20of%20French%20Speech%20Captured%20with%20Body-conduction%20Audio%0A%20%20Sensors&entry.906535625=Julien%20Hauret%20and%20Malo%20Olivier%20and%20Thomas%20Joubaud%20and%20Christophe%20Langrenne%20and%20Sarah%20Poir%C3%A9e%20and%20V%C3%A9ronique%20Zimpfer%20and%20%C3%89ric%20Bavu&entry.1292438233=%20%20Vibravox%20is%20a%20dataset%20compliant%20with%20the%20General%20Data%20Protection%20Regulation%0A%28GDPR%29%20containing%20audio%20recordings%20using%20five%20different%20body-conduction%20audio%0Asensors%20%3A%20two%20in-ear%20microphones%2C%20two%20bone%20conduction%20vibration%20pickups%20and%20a%0Alaryngophone.%20The%20dataset%20also%20includes%20audio%20data%20from%20an%20airborne%20microphone%0Aused%20as%20a%20reference.%20The%20Vibravox%20corpus%20contains%2045%20hours%20of%20speech%20samples%0Aand%20physiological%20sounds%20recorded%20by%20188%20participants%20under%20different%20acoustic%0Aconditions%20imposed%20by%20an%20high%20order%20ambisonics%203D%20spatializer.%20Annotations%0Aabout%20the%20recording%20conditions%20and%20linguistic%20transcriptions%20are%20also%20included%0Ain%20the%20corpus.%20We%20conducted%20a%20series%20of%20experiments%20on%20various%20speech-related%0Atasks%2C%20including%20speech%20recognition%2C%20speech%20enhancement%20and%20speaker%0Averification.%20These%20experiments%20were%20carried%20out%20using%20state-of-the-art%20models%0Ato%20evaluate%20and%20compare%20their%20performances%20on%20signals%20captured%20by%20the%20different%0Aaudio%20sensors%20offered%20by%20the%20Vibravox%20dataset%2C%20with%20the%20aim%20of%20gaining%20a%20better%0Agrasp%20of%20their%20individual%20characteristics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11828v3&entry.124074799=Read"},
{"title": "Depth-aware Fusion Method based on Image and 4D Radar Spectrum for 3D\n  Object Detection", "author": "Yue Sun and Yeqiang Qian and Chunxiang Wang and Ming Yang", "abstract": "  Safety and reliability are crucial for the public acceptance of autonomous\ndriving. To ensure accurate and reliable environmental perception, intelligent\nvehicles must exhibit accuracy and robustness in various environments.\nMillimeter-wave radar, known for its high penetration capability, can operate\neffectively in adverse weather conditions such as rain, snow, and fog.\nTraditional 3D millimeter-wave radars can only provide range, Doppler, and\nazimuth information for objects. Although the recent emergence of 4D\nmillimeter-wave radars has added elevation resolution, the radar point clouds\nremain sparse due to Constant False Alarm Rate (CFAR) operations. In contrast,\ncameras offer rich semantic details but are sensitive to lighting and weather\nconditions. Hence, this paper leverages these two highly complementary and\ncost-effective sensors, 4D millimeter-wave radar and camera. By integrating 4D\nradar spectra with depth-aware camera images and employing attention\nmechanisms, we fuse texture-rich images with depth-rich radar data in the\nBird's Eye View (BEV) perspective, enhancing 3D object detection. Additionally,\nwe propose using GAN-based networks to generate depth images from radar spectra\nin the absence of depth sensors, further improving detection accuracy.\n", "link": "http://arxiv.org/abs/2502.15516v1", "date": "2025-02-21", "relevancy": 2.2615, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5771}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5663}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth-aware%20Fusion%20Method%20based%20on%20Image%20and%204D%20Radar%20Spectrum%20for%203D%0A%20%20Object%20Detection&body=Title%3A%20Depth-aware%20Fusion%20Method%20based%20on%20Image%20and%204D%20Radar%20Spectrum%20for%203D%0A%20%20Object%20Detection%0AAuthor%3A%20Yue%20Sun%20and%20Yeqiang%20Qian%20and%20Chunxiang%20Wang%20and%20Ming%20Yang%0AAbstract%3A%20%20%20Safety%20and%20reliability%20are%20crucial%20for%20the%20public%20acceptance%20of%20autonomous%0Adriving.%20To%20ensure%20accurate%20and%20reliable%20environmental%20perception%2C%20intelligent%0Avehicles%20must%20exhibit%20accuracy%20and%20robustness%20in%20various%20environments.%0AMillimeter-wave%20radar%2C%20known%20for%20its%20high%20penetration%20capability%2C%20can%20operate%0Aeffectively%20in%20adverse%20weather%20conditions%20such%20as%20rain%2C%20snow%2C%20and%20fog.%0ATraditional%203D%20millimeter-wave%20radars%20can%20only%20provide%20range%2C%20Doppler%2C%20and%0Aazimuth%20information%20for%20objects.%20Although%20the%20recent%20emergence%20of%204D%0Amillimeter-wave%20radars%20has%20added%20elevation%20resolution%2C%20the%20radar%20point%20clouds%0Aremain%20sparse%20due%20to%20Constant%20False%20Alarm%20Rate%20%28CFAR%29%20operations.%20In%20contrast%2C%0Acameras%20offer%20rich%20semantic%20details%20but%20are%20sensitive%20to%20lighting%20and%20weather%0Aconditions.%20Hence%2C%20this%20paper%20leverages%20these%20two%20highly%20complementary%20and%0Acost-effective%20sensors%2C%204D%20millimeter-wave%20radar%20and%20camera.%20By%20integrating%204D%0Aradar%20spectra%20with%20depth-aware%20camera%20images%20and%20employing%20attention%0Amechanisms%2C%20we%20fuse%20texture-rich%20images%20with%20depth-rich%20radar%20data%20in%20the%0ABird%27s%20Eye%20View%20%28BEV%29%20perspective%2C%20enhancing%203D%20object%20detection.%20Additionally%2C%0Awe%20propose%20using%20GAN-based%20networks%20to%20generate%20depth%20images%20from%20radar%20spectra%0Ain%20the%20absence%20of%20depth%20sensors%2C%20further%20improving%20detection%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15516v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth-aware%2520Fusion%2520Method%2520based%2520on%2520Image%2520and%25204D%2520Radar%2520Spectrum%2520for%25203D%250A%2520%2520Object%2520Detection%26entry.906535625%3DYue%2520Sun%2520and%2520Yeqiang%2520Qian%2520and%2520Chunxiang%2520Wang%2520and%2520Ming%2520Yang%26entry.1292438233%3D%2520%2520Safety%2520and%2520reliability%2520are%2520crucial%2520for%2520the%2520public%2520acceptance%2520of%2520autonomous%250Adriving.%2520To%2520ensure%2520accurate%2520and%2520reliable%2520environmental%2520perception%252C%2520intelligent%250Avehicles%2520must%2520exhibit%2520accuracy%2520and%2520robustness%2520in%2520various%2520environments.%250AMillimeter-wave%2520radar%252C%2520known%2520for%2520its%2520high%2520penetration%2520capability%252C%2520can%2520operate%250Aeffectively%2520in%2520adverse%2520weather%2520conditions%2520such%2520as%2520rain%252C%2520snow%252C%2520and%2520fog.%250ATraditional%25203D%2520millimeter-wave%2520radars%2520can%2520only%2520provide%2520range%252C%2520Doppler%252C%2520and%250Aazimuth%2520information%2520for%2520objects.%2520Although%2520the%2520recent%2520emergence%2520of%25204D%250Amillimeter-wave%2520radars%2520has%2520added%2520elevation%2520resolution%252C%2520the%2520radar%2520point%2520clouds%250Aremain%2520sparse%2520due%2520to%2520Constant%2520False%2520Alarm%2520Rate%2520%2528CFAR%2529%2520operations.%2520In%2520contrast%252C%250Acameras%2520offer%2520rich%2520semantic%2520details%2520but%2520are%2520sensitive%2520to%2520lighting%2520and%2520weather%250Aconditions.%2520Hence%252C%2520this%2520paper%2520leverages%2520these%2520two%2520highly%2520complementary%2520and%250Acost-effective%2520sensors%252C%25204D%2520millimeter-wave%2520radar%2520and%2520camera.%2520By%2520integrating%25204D%250Aradar%2520spectra%2520with%2520depth-aware%2520camera%2520images%2520and%2520employing%2520attention%250Amechanisms%252C%2520we%2520fuse%2520texture-rich%2520images%2520with%2520depth-rich%2520radar%2520data%2520in%2520the%250ABird%2527s%2520Eye%2520View%2520%2528BEV%2529%2520perspective%252C%2520enhancing%25203D%2520object%2520detection.%2520Additionally%252C%250Awe%2520propose%2520using%2520GAN-based%2520networks%2520to%2520generate%2520depth%2520images%2520from%2520radar%2520spectra%250Ain%2520the%2520absence%2520of%2520depth%2520sensors%252C%2520further%2520improving%2520detection%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15516v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth-aware%20Fusion%20Method%20based%20on%20Image%20and%204D%20Radar%20Spectrum%20for%203D%0A%20%20Object%20Detection&entry.906535625=Yue%20Sun%20and%20Yeqiang%20Qian%20and%20Chunxiang%20Wang%20and%20Ming%20Yang&entry.1292438233=%20%20Safety%20and%20reliability%20are%20crucial%20for%20the%20public%20acceptance%20of%20autonomous%0Adriving.%20To%20ensure%20accurate%20and%20reliable%20environmental%20perception%2C%20intelligent%0Avehicles%20must%20exhibit%20accuracy%20and%20robustness%20in%20various%20environments.%0AMillimeter-wave%20radar%2C%20known%20for%20its%20high%20penetration%20capability%2C%20can%20operate%0Aeffectively%20in%20adverse%20weather%20conditions%20such%20as%20rain%2C%20snow%2C%20and%20fog.%0ATraditional%203D%20millimeter-wave%20radars%20can%20only%20provide%20range%2C%20Doppler%2C%20and%0Aazimuth%20information%20for%20objects.%20Although%20the%20recent%20emergence%20of%204D%0Amillimeter-wave%20radars%20has%20added%20elevation%20resolution%2C%20the%20radar%20point%20clouds%0Aremain%20sparse%20due%20to%20Constant%20False%20Alarm%20Rate%20%28CFAR%29%20operations.%20In%20contrast%2C%0Acameras%20offer%20rich%20semantic%20details%20but%20are%20sensitive%20to%20lighting%20and%20weather%0Aconditions.%20Hence%2C%20this%20paper%20leverages%20these%20two%20highly%20complementary%20and%0Acost-effective%20sensors%2C%204D%20millimeter-wave%20radar%20and%20camera.%20By%20integrating%204D%0Aradar%20spectra%20with%20depth-aware%20camera%20images%20and%20employing%20attention%0Amechanisms%2C%20we%20fuse%20texture-rich%20images%20with%20depth-rich%20radar%20data%20in%20the%0ABird%27s%20Eye%20View%20%28BEV%29%20perspective%2C%20enhancing%203D%20object%20detection.%20Additionally%2C%0Awe%20propose%20using%20GAN-based%20networks%20to%20generate%20depth%20images%20from%20radar%20spectra%0Ain%20the%20absence%20of%20depth%20sensors%2C%20further%20improving%20detection%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15516v1&entry.124074799=Read"},
{"title": "Robust 4D Radar-aided Inertial Navigation for Aerial Vehicles", "author": "Jinwen Zhu and Jun Hu and Xudong Zhao and Xiaoming Lang and Yinian Mao and Guoquan Huang", "abstract": "  While LiDAR and cameras are becoming ubiquitous for unmanned aerial vehicles\n(UAVs) but can be ineffective in challenging environments, 4D millimeter-wave\n(MMW) radars that can provide robust 3D ranging and Doppler velocity\nmeasurements are less exploited for aerial navigation. In this paper, we\ndevelop an efficient and robust error-state Kalman filter (ESKF)-based\nradar-inertial navigation for UAVs. The key idea of the proposed approach is\nthe point-to-distribution radar scan matching to provide motion constraints\nwith proper uncertainty qualification, which are used to update the navigation\nstates in a tightly coupled manner, along with the Doppler velocity\nmeasurements. Moreover, we propose a robust keyframe-based matching scheme\nagainst the prior map (if available) to bound the accumulated navigation errors\nand thus provide a radar-based global localization solution with high accuracy.\nExtensive real-world experimental validations have demonstrated that the\nproposed radar-aided inertial navigation outperforms state-of-the-art methods\nin both accuracy and robustness.\n", "link": "http://arxiv.org/abs/2502.15452v1", "date": "2025-02-21", "relevancy": 2.2414, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5968}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5356}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%204D%20Radar-aided%20Inertial%20Navigation%20for%20Aerial%20Vehicles&body=Title%3A%20Robust%204D%20Radar-aided%20Inertial%20Navigation%20for%20Aerial%20Vehicles%0AAuthor%3A%20Jinwen%20Zhu%20and%20Jun%20Hu%20and%20Xudong%20Zhao%20and%20Xiaoming%20Lang%20and%20Yinian%20Mao%20and%20Guoquan%20Huang%0AAbstract%3A%20%20%20While%20LiDAR%20and%20cameras%20are%20becoming%20ubiquitous%20for%20unmanned%20aerial%20vehicles%0A%28UAVs%29%20but%20can%20be%20ineffective%20in%20challenging%20environments%2C%204D%20millimeter-wave%0A%28MMW%29%20radars%20that%20can%20provide%20robust%203D%20ranging%20and%20Doppler%20velocity%0Ameasurements%20are%20less%20exploited%20for%20aerial%20navigation.%20In%20this%20paper%2C%20we%0Adevelop%20an%20efficient%20and%20robust%20error-state%20Kalman%20filter%20%28ESKF%29-based%0Aradar-inertial%20navigation%20for%20UAVs.%20The%20key%20idea%20of%20the%20proposed%20approach%20is%0Athe%20point-to-distribution%20radar%20scan%20matching%20to%20provide%20motion%20constraints%0Awith%20proper%20uncertainty%20qualification%2C%20which%20are%20used%20to%20update%20the%20navigation%0Astates%20in%20a%20tightly%20coupled%20manner%2C%20along%20with%20the%20Doppler%20velocity%0Ameasurements.%20Moreover%2C%20we%20propose%20a%20robust%20keyframe-based%20matching%20scheme%0Aagainst%20the%20prior%20map%20%28if%20available%29%20to%20bound%20the%20accumulated%20navigation%20errors%0Aand%20thus%20provide%20a%20radar-based%20global%20localization%20solution%20with%20high%20accuracy.%0AExtensive%20real-world%20experimental%20validations%20have%20demonstrated%20that%20the%0Aproposed%20radar-aided%20inertial%20navigation%20outperforms%20state-of-the-art%20methods%0Ain%20both%20accuracy%20and%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15452v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%25204D%2520Radar-aided%2520Inertial%2520Navigation%2520for%2520Aerial%2520Vehicles%26entry.906535625%3DJinwen%2520Zhu%2520and%2520Jun%2520Hu%2520and%2520Xudong%2520Zhao%2520and%2520Xiaoming%2520Lang%2520and%2520Yinian%2520Mao%2520and%2520Guoquan%2520Huang%26entry.1292438233%3D%2520%2520While%2520LiDAR%2520and%2520cameras%2520are%2520becoming%2520ubiquitous%2520for%2520unmanned%2520aerial%2520vehicles%250A%2528UAVs%2529%2520but%2520can%2520be%2520ineffective%2520in%2520challenging%2520environments%252C%25204D%2520millimeter-wave%250A%2528MMW%2529%2520radars%2520that%2520can%2520provide%2520robust%25203D%2520ranging%2520and%2520Doppler%2520velocity%250Ameasurements%2520are%2520less%2520exploited%2520for%2520aerial%2520navigation.%2520In%2520this%2520paper%252C%2520we%250Adevelop%2520an%2520efficient%2520and%2520robust%2520error-state%2520Kalman%2520filter%2520%2528ESKF%2529-based%250Aradar-inertial%2520navigation%2520for%2520UAVs.%2520The%2520key%2520idea%2520of%2520the%2520proposed%2520approach%2520is%250Athe%2520point-to-distribution%2520radar%2520scan%2520matching%2520to%2520provide%2520motion%2520constraints%250Awith%2520proper%2520uncertainty%2520qualification%252C%2520which%2520are%2520used%2520to%2520update%2520the%2520navigation%250Astates%2520in%2520a%2520tightly%2520coupled%2520manner%252C%2520along%2520with%2520the%2520Doppler%2520velocity%250Ameasurements.%2520Moreover%252C%2520we%2520propose%2520a%2520robust%2520keyframe-based%2520matching%2520scheme%250Aagainst%2520the%2520prior%2520map%2520%2528if%2520available%2529%2520to%2520bound%2520the%2520accumulated%2520navigation%2520errors%250Aand%2520thus%2520provide%2520a%2520radar-based%2520global%2520localization%2520solution%2520with%2520high%2520accuracy.%250AExtensive%2520real-world%2520experimental%2520validations%2520have%2520demonstrated%2520that%2520the%250Aproposed%2520radar-aided%2520inertial%2520navigation%2520outperforms%2520state-of-the-art%2520methods%250Ain%2520both%2520accuracy%2520and%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15452v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%204D%20Radar-aided%20Inertial%20Navigation%20for%20Aerial%20Vehicles&entry.906535625=Jinwen%20Zhu%20and%20Jun%20Hu%20and%20Xudong%20Zhao%20and%20Xiaoming%20Lang%20and%20Yinian%20Mao%20and%20Guoquan%20Huang&entry.1292438233=%20%20While%20LiDAR%20and%20cameras%20are%20becoming%20ubiquitous%20for%20unmanned%20aerial%20vehicles%0A%28UAVs%29%20but%20can%20be%20ineffective%20in%20challenging%20environments%2C%204D%20millimeter-wave%0A%28MMW%29%20radars%20that%20can%20provide%20robust%203D%20ranging%20and%20Doppler%20velocity%0Ameasurements%20are%20less%20exploited%20for%20aerial%20navigation.%20In%20this%20paper%2C%20we%0Adevelop%20an%20efficient%20and%20robust%20error-state%20Kalman%20filter%20%28ESKF%29-based%0Aradar-inertial%20navigation%20for%20UAVs.%20The%20key%20idea%20of%20the%20proposed%20approach%20is%0Athe%20point-to-distribution%20radar%20scan%20matching%20to%20provide%20motion%20constraints%0Awith%20proper%20uncertainty%20qualification%2C%20which%20are%20used%20to%20update%20the%20navigation%0Astates%20in%20a%20tightly%20coupled%20manner%2C%20along%20with%20the%20Doppler%20velocity%0Ameasurements.%20Moreover%2C%20we%20propose%20a%20robust%20keyframe-based%20matching%20scheme%0Aagainst%20the%20prior%20map%20%28if%20available%29%20to%20bound%20the%20accumulated%20navigation%20errors%0Aand%20thus%20provide%20a%20radar-based%20global%20localization%20solution%20with%20high%20accuracy.%0AExtensive%20real-world%20experimental%20validations%20have%20demonstrated%20that%20the%0Aproposed%20radar-aided%20inertial%20navigation%20outperforms%20state-of-the-art%20methods%0Ain%20both%20accuracy%20and%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15452v1&entry.124074799=Read"},
{"title": "Para-Lane: Multi-Lane Dataset Registering Parallel Scans for\n  Benchmarking Novel View Synthesis", "author": "Ziqian Ni and Sicong Du and Zhenghua Hou and Chenming Wu and Sheng Yang", "abstract": "  To evaluate end-to-end autonomous driving systems, a simulation environment\nbased on Novel View Synthesis (NVS) techniques is essential, which synthesizes\nphoto-realistic images and point clouds from previously recorded sequences\nunder new vehicle poses, particularly in cross-lane scenarios. Therefore, the\ndevelopment of a multi-lane dataset and benchmark is necessary. While recent\nsynthetic scene-based NVS datasets have been prepared for cross-lane\nbenchmarking, they still lack the realism of captured images and point clouds.\nTo further assess the performance of existing methods based on NeRF and 3DGS,\nwe present the first multi-lane dataset registering parallel scans specifically\nfor novel driving view synthesis dataset derived from real-world scans,\ncomprising 25 groups of associated sequences, including 16,000 front-view\nimages, 64,000 surround-view images, and 16,000 LiDAR frames. All frames are\nlabeled to differentiate moving objects from static elements. Using this\ndataset, we evaluate the performance of existing approaches in various testing\nscenarios at different lanes and distances. Additionally, our method provides\nthe solution for solving and assessing the quality of multi-sensor poses for\nmulti-modal data alignment for curating such a dataset in real-world. We plan\nto continually add new sequences to test the generalization of existing methods\nacross different scenarios. The dataset is released publicly at the project\npage: https://nizqleo.github.io/paralane-dataset/.\n", "link": "http://arxiv.org/abs/2502.15635v1", "date": "2025-02-21", "relevancy": 2.232, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5664}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5615}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Para-Lane%3A%20Multi-Lane%20Dataset%20Registering%20Parallel%20Scans%20for%0A%20%20Benchmarking%20Novel%20View%20Synthesis&body=Title%3A%20Para-Lane%3A%20Multi-Lane%20Dataset%20Registering%20Parallel%20Scans%20for%0A%20%20Benchmarking%20Novel%20View%20Synthesis%0AAuthor%3A%20Ziqian%20Ni%20and%20Sicong%20Du%20and%20Zhenghua%20Hou%20and%20Chenming%20Wu%20and%20Sheng%20Yang%0AAbstract%3A%20%20%20To%20evaluate%20end-to-end%20autonomous%20driving%20systems%2C%20a%20simulation%20environment%0Abased%20on%20Novel%20View%20Synthesis%20%28NVS%29%20techniques%20is%20essential%2C%20which%20synthesizes%0Aphoto-realistic%20images%20and%20point%20clouds%20from%20previously%20recorded%20sequences%0Aunder%20new%20vehicle%20poses%2C%20particularly%20in%20cross-lane%20scenarios.%20Therefore%2C%20the%0Adevelopment%20of%20a%20multi-lane%20dataset%20and%20benchmark%20is%20necessary.%20While%20recent%0Asynthetic%20scene-based%20NVS%20datasets%20have%20been%20prepared%20for%20cross-lane%0Abenchmarking%2C%20they%20still%20lack%20the%20realism%20of%20captured%20images%20and%20point%20clouds.%0ATo%20further%20assess%20the%20performance%20of%20existing%20methods%20based%20on%20NeRF%20and%203DGS%2C%0Awe%20present%20the%20first%20multi-lane%20dataset%20registering%20parallel%20scans%20specifically%0Afor%20novel%20driving%20view%20synthesis%20dataset%20derived%20from%20real-world%20scans%2C%0Acomprising%2025%20groups%20of%20associated%20sequences%2C%20including%2016%2C000%20front-view%0Aimages%2C%2064%2C000%20surround-view%20images%2C%20and%2016%2C000%20LiDAR%20frames.%20All%20frames%20are%0Alabeled%20to%20differentiate%20moving%20objects%20from%20static%20elements.%20Using%20this%0Adataset%2C%20we%20evaluate%20the%20performance%20of%20existing%20approaches%20in%20various%20testing%0Ascenarios%20at%20different%20lanes%20and%20distances.%20Additionally%2C%20our%20method%20provides%0Athe%20solution%20for%20solving%20and%20assessing%20the%20quality%20of%20multi-sensor%20poses%20for%0Amulti-modal%20data%20alignment%20for%20curating%20such%20a%20dataset%20in%20real-world.%20We%20plan%0Ato%20continually%20add%20new%20sequences%20to%20test%20the%20generalization%20of%20existing%20methods%0Aacross%20different%20scenarios.%20The%20dataset%20is%20released%20publicly%20at%20the%20project%0Apage%3A%20https%3A//nizqleo.github.io/paralane-dataset/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPara-Lane%253A%2520Multi-Lane%2520Dataset%2520Registering%2520Parallel%2520Scans%2520for%250A%2520%2520Benchmarking%2520Novel%2520View%2520Synthesis%26entry.906535625%3DZiqian%2520Ni%2520and%2520Sicong%2520Du%2520and%2520Zhenghua%2520Hou%2520and%2520Chenming%2520Wu%2520and%2520Sheng%2520Yang%26entry.1292438233%3D%2520%2520To%2520evaluate%2520end-to-end%2520autonomous%2520driving%2520systems%252C%2520a%2520simulation%2520environment%250Abased%2520on%2520Novel%2520View%2520Synthesis%2520%2528NVS%2529%2520techniques%2520is%2520essential%252C%2520which%2520synthesizes%250Aphoto-realistic%2520images%2520and%2520point%2520clouds%2520from%2520previously%2520recorded%2520sequences%250Aunder%2520new%2520vehicle%2520poses%252C%2520particularly%2520in%2520cross-lane%2520scenarios.%2520Therefore%252C%2520the%250Adevelopment%2520of%2520a%2520multi-lane%2520dataset%2520and%2520benchmark%2520is%2520necessary.%2520While%2520recent%250Asynthetic%2520scene-based%2520NVS%2520datasets%2520have%2520been%2520prepared%2520for%2520cross-lane%250Abenchmarking%252C%2520they%2520still%2520lack%2520the%2520realism%2520of%2520captured%2520images%2520and%2520point%2520clouds.%250ATo%2520further%2520assess%2520the%2520performance%2520of%2520existing%2520methods%2520based%2520on%2520NeRF%2520and%25203DGS%252C%250Awe%2520present%2520the%2520first%2520multi-lane%2520dataset%2520registering%2520parallel%2520scans%2520specifically%250Afor%2520novel%2520driving%2520view%2520synthesis%2520dataset%2520derived%2520from%2520real-world%2520scans%252C%250Acomprising%252025%2520groups%2520of%2520associated%2520sequences%252C%2520including%252016%252C000%2520front-view%250Aimages%252C%252064%252C000%2520surround-view%2520images%252C%2520and%252016%252C000%2520LiDAR%2520frames.%2520All%2520frames%2520are%250Alabeled%2520to%2520differentiate%2520moving%2520objects%2520from%2520static%2520elements.%2520Using%2520this%250Adataset%252C%2520we%2520evaluate%2520the%2520performance%2520of%2520existing%2520approaches%2520in%2520various%2520testing%250Ascenarios%2520at%2520different%2520lanes%2520and%2520distances.%2520Additionally%252C%2520our%2520method%2520provides%250Athe%2520solution%2520for%2520solving%2520and%2520assessing%2520the%2520quality%2520of%2520multi-sensor%2520poses%2520for%250Amulti-modal%2520data%2520alignment%2520for%2520curating%2520such%2520a%2520dataset%2520in%2520real-world.%2520We%2520plan%250Ato%2520continually%2520add%2520new%2520sequences%2520to%2520test%2520the%2520generalization%2520of%2520existing%2520methods%250Aacross%2520different%2520scenarios.%2520The%2520dataset%2520is%2520released%2520publicly%2520at%2520the%2520project%250Apage%253A%2520https%253A//nizqleo.github.io/paralane-dataset/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Para-Lane%3A%20Multi-Lane%20Dataset%20Registering%20Parallel%20Scans%20for%0A%20%20Benchmarking%20Novel%20View%20Synthesis&entry.906535625=Ziqian%20Ni%20and%20Sicong%20Du%20and%20Zhenghua%20Hou%20and%20Chenming%20Wu%20and%20Sheng%20Yang&entry.1292438233=%20%20To%20evaluate%20end-to-end%20autonomous%20driving%20systems%2C%20a%20simulation%20environment%0Abased%20on%20Novel%20View%20Synthesis%20%28NVS%29%20techniques%20is%20essential%2C%20which%20synthesizes%0Aphoto-realistic%20images%20and%20point%20clouds%20from%20previously%20recorded%20sequences%0Aunder%20new%20vehicle%20poses%2C%20particularly%20in%20cross-lane%20scenarios.%20Therefore%2C%20the%0Adevelopment%20of%20a%20multi-lane%20dataset%20and%20benchmark%20is%20necessary.%20While%20recent%0Asynthetic%20scene-based%20NVS%20datasets%20have%20been%20prepared%20for%20cross-lane%0Abenchmarking%2C%20they%20still%20lack%20the%20realism%20of%20captured%20images%20and%20point%20clouds.%0ATo%20further%20assess%20the%20performance%20of%20existing%20methods%20based%20on%20NeRF%20and%203DGS%2C%0Awe%20present%20the%20first%20multi-lane%20dataset%20registering%20parallel%20scans%20specifically%0Afor%20novel%20driving%20view%20synthesis%20dataset%20derived%20from%20real-world%20scans%2C%0Acomprising%2025%20groups%20of%20associated%20sequences%2C%20including%2016%2C000%20front-view%0Aimages%2C%2064%2C000%20surround-view%20images%2C%20and%2016%2C000%20LiDAR%20frames.%20All%20frames%20are%0Alabeled%20to%20differentiate%20moving%20objects%20from%20static%20elements.%20Using%20this%0Adataset%2C%20we%20evaluate%20the%20performance%20of%20existing%20approaches%20in%20various%20testing%0Ascenarios%20at%20different%20lanes%20and%20distances.%20Additionally%2C%20our%20method%20provides%0Athe%20solution%20for%20solving%20and%20assessing%20the%20quality%20of%20multi-sensor%20poses%20for%0Amulti-modal%20data%20alignment%20for%20curating%20such%20a%20dataset%20in%20real-world.%20We%20plan%0Ato%20continually%20add%20new%20sequences%20to%20test%20the%20generalization%20of%20existing%20methods%0Aacross%20different%20scenarios.%20The%20dataset%20is%20released%20publicly%20at%20the%20project%0Apage%3A%20https%3A//nizqleo.github.io/paralane-dataset/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15635v1&entry.124074799=Read"},
{"title": "LEAP: Enhancing Vision-Based Occupancy Networks with Lightweight\n  Spatio-Temporal Correlation", "author": "Fengcheng Yu and Haoran Xu and Canming Xia and Guang Tan", "abstract": "  Vision-based occupancy networks provide an end-to-end solution for\nreconstructing the surrounding environment using semantic occupied voxels\nderived from multi-view images. This technique relies on effectively learning\nthe correlation between pixel-level visual information and voxels. Despite\nrecent advancements, occupancy results still suffer from limited accuracy due\nto occlusions and sparse visual cues. To address this, we propose a Lightweight\nSpatio-Temporal Correlation (LEAP)} method, which significantly enhances the\nperformance of existing occupancy networks with minimal computational overhead.\nLEAP can be seamlessly integrated into various baseline networks, enabling a\nplug-and-play application. LEAP operates in three stages: 1) it tokenizes\ninformation from recent baseline and motion features into a shared, compact\nlatent space; 2) it establishes full correlation through a tri-stream fusion\narchitecture; 3) it generates occupancy results that strengthen the baseline's\noutput. Extensive experiments demonstrate the efficiency and effectiveness of\nour method, outperforming the latest baseline models. The source code and\nseveral demos are available in the supplementary material.\n", "link": "http://arxiv.org/abs/2502.15438v1", "date": "2025-02-21", "relevancy": 2.2252, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5796}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5534}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LEAP%3A%20Enhancing%20Vision-Based%20Occupancy%20Networks%20with%20Lightweight%0A%20%20Spatio-Temporal%20Correlation&body=Title%3A%20LEAP%3A%20Enhancing%20Vision-Based%20Occupancy%20Networks%20with%20Lightweight%0A%20%20Spatio-Temporal%20Correlation%0AAuthor%3A%20Fengcheng%20Yu%20and%20Haoran%20Xu%20and%20Canming%20Xia%20and%20Guang%20Tan%0AAbstract%3A%20%20%20Vision-based%20occupancy%20networks%20provide%20an%20end-to-end%20solution%20for%0Areconstructing%20the%20surrounding%20environment%20using%20semantic%20occupied%20voxels%0Aderived%20from%20multi-view%20images.%20This%20technique%20relies%20on%20effectively%20learning%0Athe%20correlation%20between%20pixel-level%20visual%20information%20and%20voxels.%20Despite%0Arecent%20advancements%2C%20occupancy%20results%20still%20suffer%20from%20limited%20accuracy%20due%0Ato%20occlusions%20and%20sparse%20visual%20cues.%20To%20address%20this%2C%20we%20propose%20a%20Lightweight%0ASpatio-Temporal%20Correlation%20%28LEAP%29%7D%20method%2C%20which%20significantly%20enhances%20the%0Aperformance%20of%20existing%20occupancy%20networks%20with%20minimal%20computational%20overhead.%0ALEAP%20can%20be%20seamlessly%20integrated%20into%20various%20baseline%20networks%2C%20enabling%20a%0Aplug-and-play%20application.%20LEAP%20operates%20in%20three%20stages%3A%201%29%20it%20tokenizes%0Ainformation%20from%20recent%20baseline%20and%20motion%20features%20into%20a%20shared%2C%20compact%0Alatent%20space%3B%202%29%20it%20establishes%20full%20correlation%20through%20a%20tri-stream%20fusion%0Aarchitecture%3B%203%29%20it%20generates%20occupancy%20results%20that%20strengthen%20the%20baseline%27s%0Aoutput.%20Extensive%20experiments%20demonstrate%20the%20efficiency%20and%20effectiveness%20of%0Aour%20method%2C%20outperforming%20the%20latest%20baseline%20models.%20The%20source%20code%20and%0Aseveral%20demos%20are%20available%20in%20the%20supplementary%20material.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15438v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLEAP%253A%2520Enhancing%2520Vision-Based%2520Occupancy%2520Networks%2520with%2520Lightweight%250A%2520%2520Spatio-Temporal%2520Correlation%26entry.906535625%3DFengcheng%2520Yu%2520and%2520Haoran%2520Xu%2520and%2520Canming%2520Xia%2520and%2520Guang%2520Tan%26entry.1292438233%3D%2520%2520Vision-based%2520occupancy%2520networks%2520provide%2520an%2520end-to-end%2520solution%2520for%250Areconstructing%2520the%2520surrounding%2520environment%2520using%2520semantic%2520occupied%2520voxels%250Aderived%2520from%2520multi-view%2520images.%2520This%2520technique%2520relies%2520on%2520effectively%2520learning%250Athe%2520correlation%2520between%2520pixel-level%2520visual%2520information%2520and%2520voxels.%2520Despite%250Arecent%2520advancements%252C%2520occupancy%2520results%2520still%2520suffer%2520from%2520limited%2520accuracy%2520due%250Ato%2520occlusions%2520and%2520sparse%2520visual%2520cues.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520Lightweight%250ASpatio-Temporal%2520Correlation%2520%2528LEAP%2529%257D%2520method%252C%2520which%2520significantly%2520enhances%2520the%250Aperformance%2520of%2520existing%2520occupancy%2520networks%2520with%2520minimal%2520computational%2520overhead.%250ALEAP%2520can%2520be%2520seamlessly%2520integrated%2520into%2520various%2520baseline%2520networks%252C%2520enabling%2520a%250Aplug-and-play%2520application.%2520LEAP%2520operates%2520in%2520three%2520stages%253A%25201%2529%2520it%2520tokenizes%250Ainformation%2520from%2520recent%2520baseline%2520and%2520motion%2520features%2520into%2520a%2520shared%252C%2520compact%250Alatent%2520space%253B%25202%2529%2520it%2520establishes%2520full%2520correlation%2520through%2520a%2520tri-stream%2520fusion%250Aarchitecture%253B%25203%2529%2520it%2520generates%2520occupancy%2520results%2520that%2520strengthen%2520the%2520baseline%2527s%250Aoutput.%2520Extensive%2520experiments%2520demonstrate%2520the%2520efficiency%2520and%2520effectiveness%2520of%250Aour%2520method%252C%2520outperforming%2520the%2520latest%2520baseline%2520models.%2520The%2520source%2520code%2520and%250Aseveral%2520demos%2520are%2520available%2520in%2520the%2520supplementary%2520material.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15438v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LEAP%3A%20Enhancing%20Vision-Based%20Occupancy%20Networks%20with%20Lightweight%0A%20%20Spatio-Temporal%20Correlation&entry.906535625=Fengcheng%20Yu%20and%20Haoran%20Xu%20and%20Canming%20Xia%20and%20Guang%20Tan&entry.1292438233=%20%20Vision-based%20occupancy%20networks%20provide%20an%20end-to-end%20solution%20for%0Areconstructing%20the%20surrounding%20environment%20using%20semantic%20occupied%20voxels%0Aderived%20from%20multi-view%20images.%20This%20technique%20relies%20on%20effectively%20learning%0Athe%20correlation%20between%20pixel-level%20visual%20information%20and%20voxels.%20Despite%0Arecent%20advancements%2C%20occupancy%20results%20still%20suffer%20from%20limited%20accuracy%20due%0Ato%20occlusions%20and%20sparse%20visual%20cues.%20To%20address%20this%2C%20we%20propose%20a%20Lightweight%0ASpatio-Temporal%20Correlation%20%28LEAP%29%7D%20method%2C%20which%20significantly%20enhances%20the%0Aperformance%20of%20existing%20occupancy%20networks%20with%20minimal%20computational%20overhead.%0ALEAP%20can%20be%20seamlessly%20integrated%20into%20various%20baseline%20networks%2C%20enabling%20a%0Aplug-and-play%20application.%20LEAP%20operates%20in%20three%20stages%3A%201%29%20it%20tokenizes%0Ainformation%20from%20recent%20baseline%20and%20motion%20features%20into%20a%20shared%2C%20compact%0Alatent%20space%3B%202%29%20it%20establishes%20full%20correlation%20through%20a%20tri-stream%20fusion%0Aarchitecture%3B%203%29%20it%20generates%20occupancy%20results%20that%20strengthen%20the%20baseline%27s%0Aoutput.%20Extensive%20experiments%20demonstrate%20the%20efficiency%20and%20effectiveness%20of%0Aour%20method%2C%20outperforming%20the%20latest%20baseline%20models.%20The%20source%20code%20and%0Aseveral%20demos%20are%20available%20in%20the%20supplementary%20material.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15438v1&entry.124074799=Read"},
{"title": "SWEPO: Simultaneous Weighted Preference Optimization for Group\n  Contrastive Alignment", "author": "Taneesh Gupta and Rahul Madhavan and Xuchao Zhang and Chetan Bansal and Saravan Rajmohan", "abstract": "  Direct Preference Optimization (DPO) has proven effective in aligning large\nlanguage models with human preferences but is often constrained to pairwise\ncomparisons -- overlooking additional positive and negative responses that are\ncommonly available in real-world settings. We propose Simultaneous Weighted\nPreference Optimization (SWEPO), which incorporates multiple responses per\nquery and prioritizes those that deviate most from the average reward. This\ndeviation-based weighting focuses training on the most informative outliers,\nakin to a built-in curriculum. Theoretically, we prove that such\nmulti-preference sampling lowers alignment bias, bounding the expected\ndeviation from the true acceptable-response distribution at a rate of\n$\\mathcal{O}(\\tfrac{1}{\\sqrt{k}})$. Empirically, SWEPO outperforms\nstate-of-the-art baselines on the Ultra-Feedback dataset and demonstrates\nsubstantial improvements over DPO and InfoNCA, yielding boosts of up to $\\sim\n4$% on length-controlled win-rate on AlpacaEval.\n", "link": "http://arxiv.org/abs/2412.04628v3", "date": "2025-02-21", "relevancy": 2.2067, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4447}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4397}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SWEPO%3A%20Simultaneous%20Weighted%20Preference%20Optimization%20for%20Group%0A%20%20Contrastive%20Alignment&body=Title%3A%20SWEPO%3A%20Simultaneous%20Weighted%20Preference%20Optimization%20for%20Group%0A%20%20Contrastive%20Alignment%0AAuthor%3A%20Taneesh%20Gupta%20and%20Rahul%20Madhavan%20and%20Xuchao%20Zhang%20and%20Chetan%20Bansal%20and%20Saravan%20Rajmohan%0AAbstract%3A%20%20%20Direct%20Preference%20Optimization%20%28DPO%29%20has%20proven%20effective%20in%20aligning%20large%0Alanguage%20models%20with%20human%20preferences%20but%20is%20often%20constrained%20to%20pairwise%0Acomparisons%20--%20overlooking%20additional%20positive%20and%20negative%20responses%20that%20are%0Acommonly%20available%20in%20real-world%20settings.%20We%20propose%20Simultaneous%20Weighted%0APreference%20Optimization%20%28SWEPO%29%2C%20which%20incorporates%20multiple%20responses%20per%0Aquery%20and%20prioritizes%20those%20that%20deviate%20most%20from%20the%20average%20reward.%20This%0Adeviation-based%20weighting%20focuses%20training%20on%20the%20most%20informative%20outliers%2C%0Aakin%20to%20a%20built-in%20curriculum.%20Theoretically%2C%20we%20prove%20that%20such%0Amulti-preference%20sampling%20lowers%20alignment%20bias%2C%20bounding%20the%20expected%0Adeviation%20from%20the%20true%20acceptable-response%20distribution%20at%20a%20rate%20of%0A%24%5Cmathcal%7BO%7D%28%5Ctfrac%7B1%7D%7B%5Csqrt%7Bk%7D%7D%29%24.%20Empirically%2C%20SWEPO%20outperforms%0Astate-of-the-art%20baselines%20on%20the%20Ultra-Feedback%20dataset%20and%20demonstrates%0Asubstantial%20improvements%20over%20DPO%20and%20InfoNCA%2C%20yielding%20boosts%20of%20up%20to%20%24%5Csim%0A4%24%25%20on%20length-controlled%20win-rate%20on%20AlpacaEval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.04628v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSWEPO%253A%2520Simultaneous%2520Weighted%2520Preference%2520Optimization%2520for%2520Group%250A%2520%2520Contrastive%2520Alignment%26entry.906535625%3DTaneesh%2520Gupta%2520and%2520Rahul%2520Madhavan%2520and%2520Xuchao%2520Zhang%2520and%2520Chetan%2520Bansal%2520and%2520Saravan%2520Rajmohan%26entry.1292438233%3D%2520%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%2520has%2520proven%2520effective%2520in%2520aligning%2520large%250Alanguage%2520models%2520with%2520human%2520preferences%2520but%2520is%2520often%2520constrained%2520to%2520pairwise%250Acomparisons%2520--%2520overlooking%2520additional%2520positive%2520and%2520negative%2520responses%2520that%2520are%250Acommonly%2520available%2520in%2520real-world%2520settings.%2520We%2520propose%2520Simultaneous%2520Weighted%250APreference%2520Optimization%2520%2528SWEPO%2529%252C%2520which%2520incorporates%2520multiple%2520responses%2520per%250Aquery%2520and%2520prioritizes%2520those%2520that%2520deviate%2520most%2520from%2520the%2520average%2520reward.%2520This%250Adeviation-based%2520weighting%2520focuses%2520training%2520on%2520the%2520most%2520informative%2520outliers%252C%250Aakin%2520to%2520a%2520built-in%2520curriculum.%2520Theoretically%252C%2520we%2520prove%2520that%2520such%250Amulti-preference%2520sampling%2520lowers%2520alignment%2520bias%252C%2520bounding%2520the%2520expected%250Adeviation%2520from%2520the%2520true%2520acceptable-response%2520distribution%2520at%2520a%2520rate%2520of%250A%2524%255Cmathcal%257BO%257D%2528%255Ctfrac%257B1%257D%257B%255Csqrt%257Bk%257D%257D%2529%2524.%2520Empirically%252C%2520SWEPO%2520outperforms%250Astate-of-the-art%2520baselines%2520on%2520the%2520Ultra-Feedback%2520dataset%2520and%2520demonstrates%250Asubstantial%2520improvements%2520over%2520DPO%2520and%2520InfoNCA%252C%2520yielding%2520boosts%2520of%2520up%2520to%2520%2524%255Csim%250A4%2524%2525%2520on%2520length-controlled%2520win-rate%2520on%2520AlpacaEval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.04628v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SWEPO%3A%20Simultaneous%20Weighted%20Preference%20Optimization%20for%20Group%0A%20%20Contrastive%20Alignment&entry.906535625=Taneesh%20Gupta%20and%20Rahul%20Madhavan%20and%20Xuchao%20Zhang%20and%20Chetan%20Bansal%20and%20Saravan%20Rajmohan&entry.1292438233=%20%20Direct%20Preference%20Optimization%20%28DPO%29%20has%20proven%20effective%20in%20aligning%20large%0Alanguage%20models%20with%20human%20preferences%20but%20is%20often%20constrained%20to%20pairwise%0Acomparisons%20--%20overlooking%20additional%20positive%20and%20negative%20responses%20that%20are%0Acommonly%20available%20in%20real-world%20settings.%20We%20propose%20Simultaneous%20Weighted%0APreference%20Optimization%20%28SWEPO%29%2C%20which%20incorporates%20multiple%20responses%20per%0Aquery%20and%20prioritizes%20those%20that%20deviate%20most%20from%20the%20average%20reward.%20This%0Adeviation-based%20weighting%20focuses%20training%20on%20the%20most%20informative%20outliers%2C%0Aakin%20to%20a%20built-in%20curriculum.%20Theoretically%2C%20we%20prove%20that%20such%0Amulti-preference%20sampling%20lowers%20alignment%20bias%2C%20bounding%20the%20expected%0Adeviation%20from%20the%20true%20acceptable-response%20distribution%20at%20a%20rate%20of%0A%24%5Cmathcal%7BO%7D%28%5Ctfrac%7B1%7D%7B%5Csqrt%7Bk%7D%7D%29%24.%20Empirically%2C%20SWEPO%20outperforms%0Astate-of-the-art%20baselines%20on%20the%20Ultra-Feedback%20dataset%20and%20demonstrates%0Asubstantial%20improvements%20over%20DPO%20and%20InfoNCA%2C%20yielding%20boosts%20of%20up%20to%20%24%5Csim%0A4%24%25%20on%20length-controlled%20win-rate%20on%20AlpacaEval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.04628v3&entry.124074799=Read"},
{"title": "AI and Entrepreneurship: Facial Recognition Technology Detects\n  Entrepreneurs, Outperforming Human Experts", "author": "Martin Obschonka and Christian Fisch and Tharindu Fernando and Clinton Fookes", "abstract": "  Occupational outcomes like entrepreneurship are generally considered personal\ninformation that individuals should have the autonomy to disclose. With the\nadvancing capability of artificial intelligence (AI) to infer private details\nfrom widely available human-centric data (e.g., social media), it is crucial to\ninvestigate whether AI can accurately extract private occupational information\nfrom such data. In this study, we demonstrate that deep neural networks can\nclassify individuals as entrepreneurs with high accuracy based on facial images\nsourced from Crunchbase, a premier source for entrepreneurship data. Utilizing\na dataset comprising facial images of 40,728 individuals, including both\nentrepreneurs and non-entrepreneurs, we train a Convolutional Neural Network\n(CNN) using a contrastive learning approach based on pairs of facial images\n(one entrepreneur and one non-entrepreneur per pair). While human experts\n(n=650) and trained participants (n=133) were unable to classify entrepreneurs\nwith accuracy above chance levels (>50%), our AI model achieved a\nclassification accuracy of 79.51%. Several robustness tests indicate that this\nhigh level of accuracy is maintained under various conditions. These results\nindicate privacy risks for entrepreneurs.\n", "link": "http://arxiv.org/abs/2409.03765v2", "date": "2025-02-21", "relevancy": 2.2066, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4485}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4419}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI%20and%20Entrepreneurship%3A%20Facial%20Recognition%20Technology%20Detects%0A%20%20Entrepreneurs%2C%20Outperforming%20Human%20Experts&body=Title%3A%20AI%20and%20Entrepreneurship%3A%20Facial%20Recognition%20Technology%20Detects%0A%20%20Entrepreneurs%2C%20Outperforming%20Human%20Experts%0AAuthor%3A%20Martin%20Obschonka%20and%20Christian%20Fisch%20and%20Tharindu%20Fernando%20and%20Clinton%20Fookes%0AAbstract%3A%20%20%20Occupational%20outcomes%20like%20entrepreneurship%20are%20generally%20considered%20personal%0Ainformation%20that%20individuals%20should%20have%20the%20autonomy%20to%20disclose.%20With%20the%0Aadvancing%20capability%20of%20artificial%20intelligence%20%28AI%29%20to%20infer%20private%20details%0Afrom%20widely%20available%20human-centric%20data%20%28e.g.%2C%20social%20media%29%2C%20it%20is%20crucial%20to%0Ainvestigate%20whether%20AI%20can%20accurately%20extract%20private%20occupational%20information%0Afrom%20such%20data.%20In%20this%20study%2C%20we%20demonstrate%20that%20deep%20neural%20networks%20can%0Aclassify%20individuals%20as%20entrepreneurs%20with%20high%20accuracy%20based%20on%20facial%20images%0Asourced%20from%20Crunchbase%2C%20a%20premier%20source%20for%20entrepreneurship%20data.%20Utilizing%0Aa%20dataset%20comprising%20facial%20images%20of%2040%2C728%20individuals%2C%20including%20both%0Aentrepreneurs%20and%20non-entrepreneurs%2C%20we%20train%20a%20Convolutional%20Neural%20Network%0A%28CNN%29%20using%20a%20contrastive%20learning%20approach%20based%20on%20pairs%20of%20facial%20images%0A%28one%20entrepreneur%20and%20one%20non-entrepreneur%20per%20pair%29.%20While%20human%20experts%0A%28n%3D650%29%20and%20trained%20participants%20%28n%3D133%29%20were%20unable%20to%20classify%20entrepreneurs%0Awith%20accuracy%20above%20chance%20levels%20%28%3E50%25%29%2C%20our%20AI%20model%20achieved%20a%0Aclassification%20accuracy%20of%2079.51%25.%20Several%20robustness%20tests%20indicate%20that%20this%0Ahigh%20level%20of%20accuracy%20is%20maintained%20under%20various%20conditions.%20These%20results%0Aindicate%20privacy%20risks%20for%20entrepreneurs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03765v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI%2520and%2520Entrepreneurship%253A%2520Facial%2520Recognition%2520Technology%2520Detects%250A%2520%2520Entrepreneurs%252C%2520Outperforming%2520Human%2520Experts%26entry.906535625%3DMartin%2520Obschonka%2520and%2520Christian%2520Fisch%2520and%2520Tharindu%2520Fernando%2520and%2520Clinton%2520Fookes%26entry.1292438233%3D%2520%2520Occupational%2520outcomes%2520like%2520entrepreneurship%2520are%2520generally%2520considered%2520personal%250Ainformation%2520that%2520individuals%2520should%2520have%2520the%2520autonomy%2520to%2520disclose.%2520With%2520the%250Aadvancing%2520capability%2520of%2520artificial%2520intelligence%2520%2528AI%2529%2520to%2520infer%2520private%2520details%250Afrom%2520widely%2520available%2520human-centric%2520data%2520%2528e.g.%252C%2520social%2520media%2529%252C%2520it%2520is%2520crucial%2520to%250Ainvestigate%2520whether%2520AI%2520can%2520accurately%2520extract%2520private%2520occupational%2520information%250Afrom%2520such%2520data.%2520In%2520this%2520study%252C%2520we%2520demonstrate%2520that%2520deep%2520neural%2520networks%2520can%250Aclassify%2520individuals%2520as%2520entrepreneurs%2520with%2520high%2520accuracy%2520based%2520on%2520facial%2520images%250Asourced%2520from%2520Crunchbase%252C%2520a%2520premier%2520source%2520for%2520entrepreneurship%2520data.%2520Utilizing%250Aa%2520dataset%2520comprising%2520facial%2520images%2520of%252040%252C728%2520individuals%252C%2520including%2520both%250Aentrepreneurs%2520and%2520non-entrepreneurs%252C%2520we%2520train%2520a%2520Convolutional%2520Neural%2520Network%250A%2528CNN%2529%2520using%2520a%2520contrastive%2520learning%2520approach%2520based%2520on%2520pairs%2520of%2520facial%2520images%250A%2528one%2520entrepreneur%2520and%2520one%2520non-entrepreneur%2520per%2520pair%2529.%2520While%2520human%2520experts%250A%2528n%253D650%2529%2520and%2520trained%2520participants%2520%2528n%253D133%2529%2520were%2520unable%2520to%2520classify%2520entrepreneurs%250Awith%2520accuracy%2520above%2520chance%2520levels%2520%2528%253E50%2525%2529%252C%2520our%2520AI%2520model%2520achieved%2520a%250Aclassification%2520accuracy%2520of%252079.51%2525.%2520Several%2520robustness%2520tests%2520indicate%2520that%2520this%250Ahigh%2520level%2520of%2520accuracy%2520is%2520maintained%2520under%2520various%2520conditions.%2520These%2520results%250Aindicate%2520privacy%2520risks%2520for%2520entrepreneurs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03765v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI%20and%20Entrepreneurship%3A%20Facial%20Recognition%20Technology%20Detects%0A%20%20Entrepreneurs%2C%20Outperforming%20Human%20Experts&entry.906535625=Martin%20Obschonka%20and%20Christian%20Fisch%20and%20Tharindu%20Fernando%20and%20Clinton%20Fookes&entry.1292438233=%20%20Occupational%20outcomes%20like%20entrepreneurship%20are%20generally%20considered%20personal%0Ainformation%20that%20individuals%20should%20have%20the%20autonomy%20to%20disclose.%20With%20the%0Aadvancing%20capability%20of%20artificial%20intelligence%20%28AI%29%20to%20infer%20private%20details%0Afrom%20widely%20available%20human-centric%20data%20%28e.g.%2C%20social%20media%29%2C%20it%20is%20crucial%20to%0Ainvestigate%20whether%20AI%20can%20accurately%20extract%20private%20occupational%20information%0Afrom%20such%20data.%20In%20this%20study%2C%20we%20demonstrate%20that%20deep%20neural%20networks%20can%0Aclassify%20individuals%20as%20entrepreneurs%20with%20high%20accuracy%20based%20on%20facial%20images%0Asourced%20from%20Crunchbase%2C%20a%20premier%20source%20for%20entrepreneurship%20data.%20Utilizing%0Aa%20dataset%20comprising%20facial%20images%20of%2040%2C728%20individuals%2C%20including%20both%0Aentrepreneurs%20and%20non-entrepreneurs%2C%20we%20train%20a%20Convolutional%20Neural%20Network%0A%28CNN%29%20using%20a%20contrastive%20learning%20approach%20based%20on%20pairs%20of%20facial%20images%0A%28one%20entrepreneur%20and%20one%20non-entrepreneur%20per%20pair%29.%20While%20human%20experts%0A%28n%3D650%29%20and%20trained%20participants%20%28n%3D133%29%20were%20unable%20to%20classify%20entrepreneurs%0Awith%20accuracy%20above%20chance%20levels%20%28%3E50%25%29%2C%20our%20AI%20model%20achieved%20a%0Aclassification%20accuracy%20of%2079.51%25.%20Several%20robustness%20tests%20indicate%20that%20this%0Ahigh%20level%20of%20accuracy%20is%20maintained%20under%20various%20conditions.%20These%20results%0Aindicate%20privacy%20risks%20for%20entrepreneurs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03765v2&entry.124074799=Read"},
{"title": "HeRCULES: Heterogeneous Radar Dataset in Complex Urban Environment for\n  Multi-session Radar SLAM", "author": "Hanjun Kim and Minwoo Jung and Chiyun Noh and Sangwoo Jung and Hyunho Song and Wooseong Yang and Hyesu Jang and Ayoung Kim", "abstract": "  Recently, radars have been widely featured in robotics for their robustness\nin challenging weather conditions. Two commonly used radar types are spinning\nradars and phased-array radars, each offering distinct sensor characteristics.\nExisting datasets typically feature only a single type of radar, leading to the\ndevelopment of algorithms limited to that specific kind. In this work, we\nhighlight that combining different radar types offers complementary advantages,\nwhich can be leveraged through a heterogeneous radar dataset. Moreover, this\nnew dataset fosters research in multi-session and multi-robot scenarios where\nrobots are equipped with different types of radars. In this context, we\nintroduce the HeRCULES dataset, a comprehensive, multi-modal dataset with\nheterogeneous radars, FMCW LiDAR, IMU, GPS, and cameras. This is the first\ndataset to integrate 4D radar and spinning radar alongside FMCW LiDAR, offering\nunparalleled localization, mapping, and place recognition capabilities. The\ndataset covers diverse weather and lighting conditions and a range of urban\ntraffic scenarios, enabling a comprehensive analysis across various\nenvironments. The sequence paths with multiple revisits and ground truth pose\nfor each sensor enhance its suitability for place recognition research. We\nexpect the HeRCULES dataset to facilitate odometry, mapping, place recognition,\nand sensor fusion research. The dataset and development tools are available at\nhttps://sites.google.com/view/herculesdataset.\n", "link": "http://arxiv.org/abs/2502.01946v3", "date": "2025-02-21", "relevancy": 2.1933, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5553}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5508}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HeRCULES%3A%20Heterogeneous%20Radar%20Dataset%20in%20Complex%20Urban%20Environment%20for%0A%20%20Multi-session%20Radar%20SLAM&body=Title%3A%20HeRCULES%3A%20Heterogeneous%20Radar%20Dataset%20in%20Complex%20Urban%20Environment%20for%0A%20%20Multi-session%20Radar%20SLAM%0AAuthor%3A%20Hanjun%20Kim%20and%20Minwoo%20Jung%20and%20Chiyun%20Noh%20and%20Sangwoo%20Jung%20and%20Hyunho%20Song%20and%20Wooseong%20Yang%20and%20Hyesu%20Jang%20and%20Ayoung%20Kim%0AAbstract%3A%20%20%20Recently%2C%20radars%20have%20been%20widely%20featured%20in%20robotics%20for%20their%20robustness%0Ain%20challenging%20weather%20conditions.%20Two%20commonly%20used%20radar%20types%20are%20spinning%0Aradars%20and%20phased-array%20radars%2C%20each%20offering%20distinct%20sensor%20characteristics.%0AExisting%20datasets%20typically%20feature%20only%20a%20single%20type%20of%20radar%2C%20leading%20to%20the%0Adevelopment%20of%20algorithms%20limited%20to%20that%20specific%20kind.%20In%20this%20work%2C%20we%0Ahighlight%20that%20combining%20different%20radar%20types%20offers%20complementary%20advantages%2C%0Awhich%20can%20be%20leveraged%20through%20a%20heterogeneous%20radar%20dataset.%20Moreover%2C%20this%0Anew%20dataset%20fosters%20research%20in%20multi-session%20and%20multi-robot%20scenarios%20where%0Arobots%20are%20equipped%20with%20different%20types%20of%20radars.%20In%20this%20context%2C%20we%0Aintroduce%20the%20HeRCULES%20dataset%2C%20a%20comprehensive%2C%20multi-modal%20dataset%20with%0Aheterogeneous%20radars%2C%20FMCW%20LiDAR%2C%20IMU%2C%20GPS%2C%20and%20cameras.%20This%20is%20the%20first%0Adataset%20to%20integrate%204D%20radar%20and%20spinning%20radar%20alongside%20FMCW%20LiDAR%2C%20offering%0Aunparalleled%20localization%2C%20mapping%2C%20and%20place%20recognition%20capabilities.%20The%0Adataset%20covers%20diverse%20weather%20and%20lighting%20conditions%20and%20a%20range%20of%20urban%0Atraffic%20scenarios%2C%20enabling%20a%20comprehensive%20analysis%20across%20various%0Aenvironments.%20The%20sequence%20paths%20with%20multiple%20revisits%20and%20ground%20truth%20pose%0Afor%20each%20sensor%20enhance%20its%20suitability%20for%20place%20recognition%20research.%20We%0Aexpect%20the%20HeRCULES%20dataset%20to%20facilitate%20odometry%2C%20mapping%2C%20place%20recognition%2C%0Aand%20sensor%20fusion%20research.%20The%20dataset%20and%20development%20tools%20are%20available%20at%0Ahttps%3A//sites.google.com/view/herculesdataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.01946v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeRCULES%253A%2520Heterogeneous%2520Radar%2520Dataset%2520in%2520Complex%2520Urban%2520Environment%2520for%250A%2520%2520Multi-session%2520Radar%2520SLAM%26entry.906535625%3DHanjun%2520Kim%2520and%2520Minwoo%2520Jung%2520and%2520Chiyun%2520Noh%2520and%2520Sangwoo%2520Jung%2520and%2520Hyunho%2520Song%2520and%2520Wooseong%2520Yang%2520and%2520Hyesu%2520Jang%2520and%2520Ayoung%2520Kim%26entry.1292438233%3D%2520%2520Recently%252C%2520radars%2520have%2520been%2520widely%2520featured%2520in%2520robotics%2520for%2520their%2520robustness%250Ain%2520challenging%2520weather%2520conditions.%2520Two%2520commonly%2520used%2520radar%2520types%2520are%2520spinning%250Aradars%2520and%2520phased-array%2520radars%252C%2520each%2520offering%2520distinct%2520sensor%2520characteristics.%250AExisting%2520datasets%2520typically%2520feature%2520only%2520a%2520single%2520type%2520of%2520radar%252C%2520leading%2520to%2520the%250Adevelopment%2520of%2520algorithms%2520limited%2520to%2520that%2520specific%2520kind.%2520In%2520this%2520work%252C%2520we%250Ahighlight%2520that%2520combining%2520different%2520radar%2520types%2520offers%2520complementary%2520advantages%252C%250Awhich%2520can%2520be%2520leveraged%2520through%2520a%2520heterogeneous%2520radar%2520dataset.%2520Moreover%252C%2520this%250Anew%2520dataset%2520fosters%2520research%2520in%2520multi-session%2520and%2520multi-robot%2520scenarios%2520where%250Arobots%2520are%2520equipped%2520with%2520different%2520types%2520of%2520radars.%2520In%2520this%2520context%252C%2520we%250Aintroduce%2520the%2520HeRCULES%2520dataset%252C%2520a%2520comprehensive%252C%2520multi-modal%2520dataset%2520with%250Aheterogeneous%2520radars%252C%2520FMCW%2520LiDAR%252C%2520IMU%252C%2520GPS%252C%2520and%2520cameras.%2520This%2520is%2520the%2520first%250Adataset%2520to%2520integrate%25204D%2520radar%2520and%2520spinning%2520radar%2520alongside%2520FMCW%2520LiDAR%252C%2520offering%250Aunparalleled%2520localization%252C%2520mapping%252C%2520and%2520place%2520recognition%2520capabilities.%2520The%250Adataset%2520covers%2520diverse%2520weather%2520and%2520lighting%2520conditions%2520and%2520a%2520range%2520of%2520urban%250Atraffic%2520scenarios%252C%2520enabling%2520a%2520comprehensive%2520analysis%2520across%2520various%250Aenvironments.%2520The%2520sequence%2520paths%2520with%2520multiple%2520revisits%2520and%2520ground%2520truth%2520pose%250Afor%2520each%2520sensor%2520enhance%2520its%2520suitability%2520for%2520place%2520recognition%2520research.%2520We%250Aexpect%2520the%2520HeRCULES%2520dataset%2520to%2520facilitate%2520odometry%252C%2520mapping%252C%2520place%2520recognition%252C%250Aand%2520sensor%2520fusion%2520research.%2520The%2520dataset%2520and%2520development%2520tools%2520are%2520available%2520at%250Ahttps%253A//sites.google.com/view/herculesdataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.01946v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HeRCULES%3A%20Heterogeneous%20Radar%20Dataset%20in%20Complex%20Urban%20Environment%20for%0A%20%20Multi-session%20Radar%20SLAM&entry.906535625=Hanjun%20Kim%20and%20Minwoo%20Jung%20and%20Chiyun%20Noh%20and%20Sangwoo%20Jung%20and%20Hyunho%20Song%20and%20Wooseong%20Yang%20and%20Hyesu%20Jang%20and%20Ayoung%20Kim&entry.1292438233=%20%20Recently%2C%20radars%20have%20been%20widely%20featured%20in%20robotics%20for%20their%20robustness%0Ain%20challenging%20weather%20conditions.%20Two%20commonly%20used%20radar%20types%20are%20spinning%0Aradars%20and%20phased-array%20radars%2C%20each%20offering%20distinct%20sensor%20characteristics.%0AExisting%20datasets%20typically%20feature%20only%20a%20single%20type%20of%20radar%2C%20leading%20to%20the%0Adevelopment%20of%20algorithms%20limited%20to%20that%20specific%20kind.%20In%20this%20work%2C%20we%0Ahighlight%20that%20combining%20different%20radar%20types%20offers%20complementary%20advantages%2C%0Awhich%20can%20be%20leveraged%20through%20a%20heterogeneous%20radar%20dataset.%20Moreover%2C%20this%0Anew%20dataset%20fosters%20research%20in%20multi-session%20and%20multi-robot%20scenarios%20where%0Arobots%20are%20equipped%20with%20different%20types%20of%20radars.%20In%20this%20context%2C%20we%0Aintroduce%20the%20HeRCULES%20dataset%2C%20a%20comprehensive%2C%20multi-modal%20dataset%20with%0Aheterogeneous%20radars%2C%20FMCW%20LiDAR%2C%20IMU%2C%20GPS%2C%20and%20cameras.%20This%20is%20the%20first%0Adataset%20to%20integrate%204D%20radar%20and%20spinning%20radar%20alongside%20FMCW%20LiDAR%2C%20offering%0Aunparalleled%20localization%2C%20mapping%2C%20and%20place%20recognition%20capabilities.%20The%0Adataset%20covers%20diverse%20weather%20and%20lighting%20conditions%20and%20a%20range%20of%20urban%0Atraffic%20scenarios%2C%20enabling%20a%20comprehensive%20analysis%20across%20various%0Aenvironments.%20The%20sequence%20paths%20with%20multiple%20revisits%20and%20ground%20truth%20pose%0Afor%20each%20sensor%20enhance%20its%20suitability%20for%20place%20recognition%20research.%20We%0Aexpect%20the%20HeRCULES%20dataset%20to%20facilitate%20odometry%2C%20mapping%2C%20place%20recognition%2C%0Aand%20sensor%20fusion%20research.%20The%20dataset%20and%20development%20tools%20are%20available%20at%0Ahttps%3A//sites.google.com/view/herculesdataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.01946v3&entry.124074799=Read"},
{"title": "GaRLIO: Gravity enhanced Radar-LiDAR-Inertial Odometry", "author": "Chiyun Noh and Wooseong Yang and Minwoo Jung and Sangwoo Jung and Ayoung Kim", "abstract": "  Recently, gravity has been highlighted as a crucial constraint for state\nestimation to alleviate potential vertical drift. Existing online gravity\nestimation methods rely on pose estimation combined with IMU measurements,\nwhich is considered best practice when direct velocity measurements are\nunavailable. However, with radar sensors providing direct velocity data-a\nmeasurement not yet utilized for gravity estimation-we found a significant\nopportunity to improve gravity estimation accuracy substantially. GaRLIO, the\nproposed gravity-enhanced Radar-LiDAR-Inertial Odometry, can robustly predict\ngravity to reduce vertical drift while simultaneously enhancing state\nestimation performance using pointwise velocity measurements. Furthermore,\nGaRLIO ensures robustness in dynamic environments by utilizing radar to remove\ndynamic objects from LiDAR point clouds. Our method is validated through\nexperiments in various environments prone to vertical drift, demonstrating\nsuperior performance compared to traditional LiDAR-Inertial Odometry methods.\nWe make our source code publicly available to encourage further research and\ndevelopment. https://github.com/ChiyunNoh/GaRLIO\n", "link": "http://arxiv.org/abs/2502.07703v2", "date": "2025-02-21", "relevancy": 2.1582, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5512}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5459}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5253}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaRLIO%3A%20Gravity%20enhanced%20Radar-LiDAR-Inertial%20Odometry&body=Title%3A%20GaRLIO%3A%20Gravity%20enhanced%20Radar-LiDAR-Inertial%20Odometry%0AAuthor%3A%20Chiyun%20Noh%20and%20Wooseong%20Yang%20and%20Minwoo%20Jung%20and%20Sangwoo%20Jung%20and%20Ayoung%20Kim%0AAbstract%3A%20%20%20Recently%2C%20gravity%20has%20been%20highlighted%20as%20a%20crucial%20constraint%20for%20state%0Aestimation%20to%20alleviate%20potential%20vertical%20drift.%20Existing%20online%20gravity%0Aestimation%20methods%20rely%20on%20pose%20estimation%20combined%20with%20IMU%20measurements%2C%0Awhich%20is%20considered%20best%20practice%20when%20direct%20velocity%20measurements%20are%0Aunavailable.%20However%2C%20with%20radar%20sensors%20providing%20direct%20velocity%20data-a%0Ameasurement%20not%20yet%20utilized%20for%20gravity%20estimation-we%20found%20a%20significant%0Aopportunity%20to%20improve%20gravity%20estimation%20accuracy%20substantially.%20GaRLIO%2C%20the%0Aproposed%20gravity-enhanced%20Radar-LiDAR-Inertial%20Odometry%2C%20can%20robustly%20predict%0Agravity%20to%20reduce%20vertical%20drift%20while%20simultaneously%20enhancing%20state%0Aestimation%20performance%20using%20pointwise%20velocity%20measurements.%20Furthermore%2C%0AGaRLIO%20ensures%20robustness%20in%20dynamic%20environments%20by%20utilizing%20radar%20to%20remove%0Adynamic%20objects%20from%20LiDAR%20point%20clouds.%20Our%20method%20is%20validated%20through%0Aexperiments%20in%20various%20environments%20prone%20to%20vertical%20drift%2C%20demonstrating%0Asuperior%20performance%20compared%20to%20traditional%20LiDAR-Inertial%20Odometry%20methods.%0AWe%20make%20our%20source%20code%20publicly%20available%20to%20encourage%20further%20research%20and%0Adevelopment.%20https%3A//github.com/ChiyunNoh/GaRLIO%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07703v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaRLIO%253A%2520Gravity%2520enhanced%2520Radar-LiDAR-Inertial%2520Odometry%26entry.906535625%3DChiyun%2520Noh%2520and%2520Wooseong%2520Yang%2520and%2520Minwoo%2520Jung%2520and%2520Sangwoo%2520Jung%2520and%2520Ayoung%2520Kim%26entry.1292438233%3D%2520%2520Recently%252C%2520gravity%2520has%2520been%2520highlighted%2520as%2520a%2520crucial%2520constraint%2520for%2520state%250Aestimation%2520to%2520alleviate%2520potential%2520vertical%2520drift.%2520Existing%2520online%2520gravity%250Aestimation%2520methods%2520rely%2520on%2520pose%2520estimation%2520combined%2520with%2520IMU%2520measurements%252C%250Awhich%2520is%2520considered%2520best%2520practice%2520when%2520direct%2520velocity%2520measurements%2520are%250Aunavailable.%2520However%252C%2520with%2520radar%2520sensors%2520providing%2520direct%2520velocity%2520data-a%250Ameasurement%2520not%2520yet%2520utilized%2520for%2520gravity%2520estimation-we%2520found%2520a%2520significant%250Aopportunity%2520to%2520improve%2520gravity%2520estimation%2520accuracy%2520substantially.%2520GaRLIO%252C%2520the%250Aproposed%2520gravity-enhanced%2520Radar-LiDAR-Inertial%2520Odometry%252C%2520can%2520robustly%2520predict%250Agravity%2520to%2520reduce%2520vertical%2520drift%2520while%2520simultaneously%2520enhancing%2520state%250Aestimation%2520performance%2520using%2520pointwise%2520velocity%2520measurements.%2520Furthermore%252C%250AGaRLIO%2520ensures%2520robustness%2520in%2520dynamic%2520environments%2520by%2520utilizing%2520radar%2520to%2520remove%250Adynamic%2520objects%2520from%2520LiDAR%2520point%2520clouds.%2520Our%2520method%2520is%2520validated%2520through%250Aexperiments%2520in%2520various%2520environments%2520prone%2520to%2520vertical%2520drift%252C%2520demonstrating%250Asuperior%2520performance%2520compared%2520to%2520traditional%2520LiDAR-Inertial%2520Odometry%2520methods.%250AWe%2520make%2520our%2520source%2520code%2520publicly%2520available%2520to%2520encourage%2520further%2520research%2520and%250Adevelopment.%2520https%253A//github.com/ChiyunNoh/GaRLIO%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07703v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaRLIO%3A%20Gravity%20enhanced%20Radar-LiDAR-Inertial%20Odometry&entry.906535625=Chiyun%20Noh%20and%20Wooseong%20Yang%20and%20Minwoo%20Jung%20and%20Sangwoo%20Jung%20and%20Ayoung%20Kim&entry.1292438233=%20%20Recently%2C%20gravity%20has%20been%20highlighted%20as%20a%20crucial%20constraint%20for%20state%0Aestimation%20to%20alleviate%20potential%20vertical%20drift.%20Existing%20online%20gravity%0Aestimation%20methods%20rely%20on%20pose%20estimation%20combined%20with%20IMU%20measurements%2C%0Awhich%20is%20considered%20best%20practice%20when%20direct%20velocity%20measurements%20are%0Aunavailable.%20However%2C%20with%20radar%20sensors%20providing%20direct%20velocity%20data-a%0Ameasurement%20not%20yet%20utilized%20for%20gravity%20estimation-we%20found%20a%20significant%0Aopportunity%20to%20improve%20gravity%20estimation%20accuracy%20substantially.%20GaRLIO%2C%20the%0Aproposed%20gravity-enhanced%20Radar-LiDAR-Inertial%20Odometry%2C%20can%20robustly%20predict%0Agravity%20to%20reduce%20vertical%20drift%20while%20simultaneously%20enhancing%20state%0Aestimation%20performance%20using%20pointwise%20velocity%20measurements.%20Furthermore%2C%0AGaRLIO%20ensures%20robustness%20in%20dynamic%20environments%20by%20utilizing%20radar%20to%20remove%0Adynamic%20objects%20from%20LiDAR%20point%20clouds.%20Our%20method%20is%20validated%20through%0Aexperiments%20in%20various%20environments%20prone%20to%20vertical%20drift%2C%20demonstrating%0Asuperior%20performance%20compared%20to%20traditional%20LiDAR-Inertial%20Odometry%20methods.%0AWe%20make%20our%20source%20code%20publicly%20available%20to%20encourage%20further%20research%20and%0Adevelopment.%20https%3A//github.com/ChiyunNoh/GaRLIO%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07703v2&entry.124074799=Read"},
{"title": "The Observational Partial Order of Causal Structures with Latent\n  Variables", "author": "Marina Maciel Ansanelli and Elie Wolfe and Robert W. Spekkens", "abstract": "  For two causal structures with the same set of visible variables, one is said\nto observationally dominate the other if the set of distributions over the\nvisible variables realizable by the first contains the set of distributions\nover the visible variables realizable by the second. Knowing such dominance\nrelations is useful for adjudicating between these structures given\nobservational data. We here consider the problem of determining the partial\norder of equivalence classes of causal structures with latent variables\nrelative to observational dominance. We provide a complete characterization of\nthe dominance order in the case of three visible variables, and a partial\ncharacterization in the case of four visible variables. Our techniques also\nhelp to identify which observational equivalence classes have a set of\nrealizable distributions that is characterized by nontrivial inequality\nconstraints, analogous to Bell inequalities and instrumental inequalities. We\nfind evidence that as one increases the number of visible variables, the\nequivalence classes satisfying nontrivial inequality constraints become\nubiquitous. (Because such classes are the ones for which there can be a\ndifference in the distributions that are quantumly and classically realizable,\nthis implies that the potential for quantum-classical gaps is also ubiquitous.)\nFurthermore, we find evidence that constraint-based causal discovery algorithms\nthat rely solely on conditional independence constraints have a significantly\nweaker distinguishing power among observational equivalence classes than\nalgorithms that go beyond these (i.e., algorithms that also leverage nested\nMarkov constraints and inequality constraints).\n", "link": "http://arxiv.org/abs/2502.07891v2", "date": "2025-02-21", "relevancy": 2.1457, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4295}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4295}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Observational%20Partial%20Order%20of%20Causal%20Structures%20with%20Latent%0A%20%20Variables&body=Title%3A%20The%20Observational%20Partial%20Order%20of%20Causal%20Structures%20with%20Latent%0A%20%20Variables%0AAuthor%3A%20Marina%20Maciel%20Ansanelli%20and%20Elie%20Wolfe%20and%20Robert%20W.%20Spekkens%0AAbstract%3A%20%20%20For%20two%20causal%20structures%20with%20the%20same%20set%20of%20visible%20variables%2C%20one%20is%20said%0Ato%20observationally%20dominate%20the%20other%20if%20the%20set%20of%20distributions%20over%20the%0Avisible%20variables%20realizable%20by%20the%20first%20contains%20the%20set%20of%20distributions%0Aover%20the%20visible%20variables%20realizable%20by%20the%20second.%20Knowing%20such%20dominance%0Arelations%20is%20useful%20for%20adjudicating%20between%20these%20structures%20given%0Aobservational%20data.%20We%20here%20consider%20the%20problem%20of%20determining%20the%20partial%0Aorder%20of%20equivalence%20classes%20of%20causal%20structures%20with%20latent%20variables%0Arelative%20to%20observational%20dominance.%20We%20provide%20a%20complete%20characterization%20of%0Athe%20dominance%20order%20in%20the%20case%20of%20three%20visible%20variables%2C%20and%20a%20partial%0Acharacterization%20in%20the%20case%20of%20four%20visible%20variables.%20Our%20techniques%20also%0Ahelp%20to%20identify%20which%20observational%20equivalence%20classes%20have%20a%20set%20of%0Arealizable%20distributions%20that%20is%20characterized%20by%20nontrivial%20inequality%0Aconstraints%2C%20analogous%20to%20Bell%20inequalities%20and%20instrumental%20inequalities.%20We%0Afind%20evidence%20that%20as%20one%20increases%20the%20number%20of%20visible%20variables%2C%20the%0Aequivalence%20classes%20satisfying%20nontrivial%20inequality%20constraints%20become%0Aubiquitous.%20%28Because%20such%20classes%20are%20the%20ones%20for%20which%20there%20can%20be%20a%0Adifference%20in%20the%20distributions%20that%20are%20quantumly%20and%20classically%20realizable%2C%0Athis%20implies%20that%20the%20potential%20for%20quantum-classical%20gaps%20is%20also%20ubiquitous.%29%0AFurthermore%2C%20we%20find%20evidence%20that%20constraint-based%20causal%20discovery%20algorithms%0Athat%20rely%20solely%20on%20conditional%20independence%20constraints%20have%20a%20significantly%0Aweaker%20distinguishing%20power%20among%20observational%20equivalence%20classes%20than%0Aalgorithms%20that%20go%20beyond%20these%20%28i.e.%2C%20algorithms%20that%20also%20leverage%20nested%0AMarkov%20constraints%20and%20inequality%20constraints%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07891v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Observational%2520Partial%2520Order%2520of%2520Causal%2520Structures%2520with%2520Latent%250A%2520%2520Variables%26entry.906535625%3DMarina%2520Maciel%2520Ansanelli%2520and%2520Elie%2520Wolfe%2520and%2520Robert%2520W.%2520Spekkens%26entry.1292438233%3D%2520%2520For%2520two%2520causal%2520structures%2520with%2520the%2520same%2520set%2520of%2520visible%2520variables%252C%2520one%2520is%2520said%250Ato%2520observationally%2520dominate%2520the%2520other%2520if%2520the%2520set%2520of%2520distributions%2520over%2520the%250Avisible%2520variables%2520realizable%2520by%2520the%2520first%2520contains%2520the%2520set%2520of%2520distributions%250Aover%2520the%2520visible%2520variables%2520realizable%2520by%2520the%2520second.%2520Knowing%2520such%2520dominance%250Arelations%2520is%2520useful%2520for%2520adjudicating%2520between%2520these%2520structures%2520given%250Aobservational%2520data.%2520We%2520here%2520consider%2520the%2520problem%2520of%2520determining%2520the%2520partial%250Aorder%2520of%2520equivalence%2520classes%2520of%2520causal%2520structures%2520with%2520latent%2520variables%250Arelative%2520to%2520observational%2520dominance.%2520We%2520provide%2520a%2520complete%2520characterization%2520of%250Athe%2520dominance%2520order%2520in%2520the%2520case%2520of%2520three%2520visible%2520variables%252C%2520and%2520a%2520partial%250Acharacterization%2520in%2520the%2520case%2520of%2520four%2520visible%2520variables.%2520Our%2520techniques%2520also%250Ahelp%2520to%2520identify%2520which%2520observational%2520equivalence%2520classes%2520have%2520a%2520set%2520of%250Arealizable%2520distributions%2520that%2520is%2520characterized%2520by%2520nontrivial%2520inequality%250Aconstraints%252C%2520analogous%2520to%2520Bell%2520inequalities%2520and%2520instrumental%2520inequalities.%2520We%250Afind%2520evidence%2520that%2520as%2520one%2520increases%2520the%2520number%2520of%2520visible%2520variables%252C%2520the%250Aequivalence%2520classes%2520satisfying%2520nontrivial%2520inequality%2520constraints%2520become%250Aubiquitous.%2520%2528Because%2520such%2520classes%2520are%2520the%2520ones%2520for%2520which%2520there%2520can%2520be%2520a%250Adifference%2520in%2520the%2520distributions%2520that%2520are%2520quantumly%2520and%2520classically%2520realizable%252C%250Athis%2520implies%2520that%2520the%2520potential%2520for%2520quantum-classical%2520gaps%2520is%2520also%2520ubiquitous.%2529%250AFurthermore%252C%2520we%2520find%2520evidence%2520that%2520constraint-based%2520causal%2520discovery%2520algorithms%250Athat%2520rely%2520solely%2520on%2520conditional%2520independence%2520constraints%2520have%2520a%2520significantly%250Aweaker%2520distinguishing%2520power%2520among%2520observational%2520equivalence%2520classes%2520than%250Aalgorithms%2520that%2520go%2520beyond%2520these%2520%2528i.e.%252C%2520algorithms%2520that%2520also%2520leverage%2520nested%250AMarkov%2520constraints%2520and%2520inequality%2520constraints%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07891v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Observational%20Partial%20Order%20of%20Causal%20Structures%20with%20Latent%0A%20%20Variables&entry.906535625=Marina%20Maciel%20Ansanelli%20and%20Elie%20Wolfe%20and%20Robert%20W.%20Spekkens&entry.1292438233=%20%20For%20two%20causal%20structures%20with%20the%20same%20set%20of%20visible%20variables%2C%20one%20is%20said%0Ato%20observationally%20dominate%20the%20other%20if%20the%20set%20of%20distributions%20over%20the%0Avisible%20variables%20realizable%20by%20the%20first%20contains%20the%20set%20of%20distributions%0Aover%20the%20visible%20variables%20realizable%20by%20the%20second.%20Knowing%20such%20dominance%0Arelations%20is%20useful%20for%20adjudicating%20between%20these%20structures%20given%0Aobservational%20data.%20We%20here%20consider%20the%20problem%20of%20determining%20the%20partial%0Aorder%20of%20equivalence%20classes%20of%20causal%20structures%20with%20latent%20variables%0Arelative%20to%20observational%20dominance.%20We%20provide%20a%20complete%20characterization%20of%0Athe%20dominance%20order%20in%20the%20case%20of%20three%20visible%20variables%2C%20and%20a%20partial%0Acharacterization%20in%20the%20case%20of%20four%20visible%20variables.%20Our%20techniques%20also%0Ahelp%20to%20identify%20which%20observational%20equivalence%20classes%20have%20a%20set%20of%0Arealizable%20distributions%20that%20is%20characterized%20by%20nontrivial%20inequality%0Aconstraints%2C%20analogous%20to%20Bell%20inequalities%20and%20instrumental%20inequalities.%20We%0Afind%20evidence%20that%20as%20one%20increases%20the%20number%20of%20visible%20variables%2C%20the%0Aequivalence%20classes%20satisfying%20nontrivial%20inequality%20constraints%20become%0Aubiquitous.%20%28Because%20such%20classes%20are%20the%20ones%20for%20which%20there%20can%20be%20a%0Adifference%20in%20the%20distributions%20that%20are%20quantumly%20and%20classically%20realizable%2C%0Athis%20implies%20that%20the%20potential%20for%20quantum-classical%20gaps%20is%20also%20ubiquitous.%29%0AFurthermore%2C%20we%20find%20evidence%20that%20constraint-based%20causal%20discovery%20algorithms%0Athat%20rely%20solely%20on%20conditional%20independence%20constraints%20have%20a%20significantly%0Aweaker%20distinguishing%20power%20among%20observational%20equivalence%20classes%20than%0Aalgorithms%20that%20go%20beyond%20these%20%28i.e.%2C%20algorithms%20that%20also%20leverage%20nested%0AMarkov%20constraints%20and%20inequality%20constraints%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07891v2&entry.124074799=Read"},
{"title": "Quantifying the Capability Boundary of DeepSeek Models: An\n  Application-Driven Performance Analysis", "author": "Kaikai Zhao and Zhaoxiang Liu and Xuejiao Lei and Ning Wang and Jiaojiao Zhao and Zipeng Wang and Zhenhong Long and Peijun Yang and Minjie Hua and Chaoyang Ma and Wen Liu and Kai Wang and Shiguo Lian", "abstract": "  DeepSeek-R1, known for its low training cost and exceptional reasoning\ncapabilities, has achieved state-of-the-art performance on various benchmarks.\nHowever, detailed evaluations from the perspective of real-world applications\nare lacking, making it challenging for users to select the most suitable\nDeepSeek models for their specific needs. To address this gap, we evaluate the\nDeepSeek-V3, DeepSeek-R1, DeepSeek-R1-Distill-Qwen series, and\nDeepSeek-R1-Distill-Llama series on the improved version A-Eval (A-Eval-2.0),\nan application-driven benchmark. By comparing original instruction-tuned models\nwith their distilled counterparts, we analyze how reasoning enhancements impact\nperformance across diverse practical tasks. Our results show that\nreasoning-enhanced models, while generally powerful, do not universally\noutperform across all tasks, with performance gains varying significantly\nacross tasks and models. To further assist users in model selection, we\nquantify the capability boundary of DeepSeek models through performance tier\nclassifications and intuitive line charts. Specific examples provide actionable\ninsights to help users select and deploy the most cost-effective DeepSeek\nmodels, ensuring optimal performance and resource efficiency in real-world\napplications. It should be noted that, despite our efforts to establish a\ncomprehensive, objective, and authoritative evaluation benchmark, the selection\nof test samples, characteristics of data distribution, and the setting of\nevaluation criteria may inevitably introduce certain biases into the evaluation\nresults. We will continuously optimize the evaluation benchmarks and\nperiodically update this paper to provide more comprehensive and accurate\nevaluation results. Please refer to the latest version of the paper for the\nmost recent results and conclusions.\n", "link": "http://arxiv.org/abs/2502.11164v2", "date": "2025-02-21", "relevancy": 2.1284, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5385}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5385}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantifying%20the%20Capability%20Boundary%20of%20DeepSeek%20Models%3A%20An%0A%20%20Application-Driven%20Performance%20Analysis&body=Title%3A%20Quantifying%20the%20Capability%20Boundary%20of%20DeepSeek%20Models%3A%20An%0A%20%20Application-Driven%20Performance%20Analysis%0AAuthor%3A%20Kaikai%20Zhao%20and%20Zhaoxiang%20Liu%20and%20Xuejiao%20Lei%20and%20Ning%20Wang%20and%20Jiaojiao%20Zhao%20and%20Zipeng%20Wang%20and%20Zhenhong%20Long%20and%20Peijun%20Yang%20and%20Minjie%20Hua%20and%20Chaoyang%20Ma%20and%20Wen%20Liu%20and%20Kai%20Wang%20and%20Shiguo%20Lian%0AAbstract%3A%20%20%20DeepSeek-R1%2C%20known%20for%20its%20low%20training%20cost%20and%20exceptional%20reasoning%0Acapabilities%2C%20has%20achieved%20state-of-the-art%20performance%20on%20various%20benchmarks.%0AHowever%2C%20detailed%20evaluations%20from%20the%20perspective%20of%20real-world%20applications%0Aare%20lacking%2C%20making%20it%20challenging%20for%20users%20to%20select%20the%20most%20suitable%0ADeepSeek%20models%20for%20their%20specific%20needs.%20To%20address%20this%20gap%2C%20we%20evaluate%20the%0ADeepSeek-V3%2C%20DeepSeek-R1%2C%20DeepSeek-R1-Distill-Qwen%20series%2C%20and%0ADeepSeek-R1-Distill-Llama%20series%20on%20the%20improved%20version%20A-Eval%20%28A-Eval-2.0%29%2C%0Aan%20application-driven%20benchmark.%20By%20comparing%20original%20instruction-tuned%20models%0Awith%20their%20distilled%20counterparts%2C%20we%20analyze%20how%20reasoning%20enhancements%20impact%0Aperformance%20across%20diverse%20practical%20tasks.%20Our%20results%20show%20that%0Areasoning-enhanced%20models%2C%20while%20generally%20powerful%2C%20do%20not%20universally%0Aoutperform%20across%20all%20tasks%2C%20with%20performance%20gains%20varying%20significantly%0Aacross%20tasks%20and%20models.%20To%20further%20assist%20users%20in%20model%20selection%2C%20we%0Aquantify%20the%20capability%20boundary%20of%20DeepSeek%20models%20through%20performance%20tier%0Aclassifications%20and%20intuitive%20line%20charts.%20Specific%20examples%20provide%20actionable%0Ainsights%20to%20help%20users%20select%20and%20deploy%20the%20most%20cost-effective%20DeepSeek%0Amodels%2C%20ensuring%20optimal%20performance%20and%20resource%20efficiency%20in%20real-world%0Aapplications.%20It%20should%20be%20noted%20that%2C%20despite%20our%20efforts%20to%20establish%20a%0Acomprehensive%2C%20objective%2C%20and%20authoritative%20evaluation%20benchmark%2C%20the%20selection%0Aof%20test%20samples%2C%20characteristics%20of%20data%20distribution%2C%20and%20the%20setting%20of%0Aevaluation%20criteria%20may%20inevitably%20introduce%20certain%20biases%20into%20the%20evaluation%0Aresults.%20We%20will%20continuously%20optimize%20the%20evaluation%20benchmarks%20and%0Aperiodically%20update%20this%20paper%20to%20provide%20more%20comprehensive%20and%20accurate%0Aevaluation%20results.%20Please%20refer%20to%20the%20latest%20version%20of%20the%20paper%20for%20the%0Amost%20recent%20results%20and%20conclusions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11164v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantifying%2520the%2520Capability%2520Boundary%2520of%2520DeepSeek%2520Models%253A%2520An%250A%2520%2520Application-Driven%2520Performance%2520Analysis%26entry.906535625%3DKaikai%2520Zhao%2520and%2520Zhaoxiang%2520Liu%2520and%2520Xuejiao%2520Lei%2520and%2520Ning%2520Wang%2520and%2520Jiaojiao%2520Zhao%2520and%2520Zipeng%2520Wang%2520and%2520Zhenhong%2520Long%2520and%2520Peijun%2520Yang%2520and%2520Minjie%2520Hua%2520and%2520Chaoyang%2520Ma%2520and%2520Wen%2520Liu%2520and%2520Kai%2520Wang%2520and%2520Shiguo%2520Lian%26entry.1292438233%3D%2520%2520DeepSeek-R1%252C%2520known%2520for%2520its%2520low%2520training%2520cost%2520and%2520exceptional%2520reasoning%250Acapabilities%252C%2520has%2520achieved%2520state-of-the-art%2520performance%2520on%2520various%2520benchmarks.%250AHowever%252C%2520detailed%2520evaluations%2520from%2520the%2520perspective%2520of%2520real-world%2520applications%250Aare%2520lacking%252C%2520making%2520it%2520challenging%2520for%2520users%2520to%2520select%2520the%2520most%2520suitable%250ADeepSeek%2520models%2520for%2520their%2520specific%2520needs.%2520To%2520address%2520this%2520gap%252C%2520we%2520evaluate%2520the%250ADeepSeek-V3%252C%2520DeepSeek-R1%252C%2520DeepSeek-R1-Distill-Qwen%2520series%252C%2520and%250ADeepSeek-R1-Distill-Llama%2520series%2520on%2520the%2520improved%2520version%2520A-Eval%2520%2528A-Eval-2.0%2529%252C%250Aan%2520application-driven%2520benchmark.%2520By%2520comparing%2520original%2520instruction-tuned%2520models%250Awith%2520their%2520distilled%2520counterparts%252C%2520we%2520analyze%2520how%2520reasoning%2520enhancements%2520impact%250Aperformance%2520across%2520diverse%2520practical%2520tasks.%2520Our%2520results%2520show%2520that%250Areasoning-enhanced%2520models%252C%2520while%2520generally%2520powerful%252C%2520do%2520not%2520universally%250Aoutperform%2520across%2520all%2520tasks%252C%2520with%2520performance%2520gains%2520varying%2520significantly%250Aacross%2520tasks%2520and%2520models.%2520To%2520further%2520assist%2520users%2520in%2520model%2520selection%252C%2520we%250Aquantify%2520the%2520capability%2520boundary%2520of%2520DeepSeek%2520models%2520through%2520performance%2520tier%250Aclassifications%2520and%2520intuitive%2520line%2520charts.%2520Specific%2520examples%2520provide%2520actionable%250Ainsights%2520to%2520help%2520users%2520select%2520and%2520deploy%2520the%2520most%2520cost-effective%2520DeepSeek%250Amodels%252C%2520ensuring%2520optimal%2520performance%2520and%2520resource%2520efficiency%2520in%2520real-world%250Aapplications.%2520It%2520should%2520be%2520noted%2520that%252C%2520despite%2520our%2520efforts%2520to%2520establish%2520a%250Acomprehensive%252C%2520objective%252C%2520and%2520authoritative%2520evaluation%2520benchmark%252C%2520the%2520selection%250Aof%2520test%2520samples%252C%2520characteristics%2520of%2520data%2520distribution%252C%2520and%2520the%2520setting%2520of%250Aevaluation%2520criteria%2520may%2520inevitably%2520introduce%2520certain%2520biases%2520into%2520the%2520evaluation%250Aresults.%2520We%2520will%2520continuously%2520optimize%2520the%2520evaluation%2520benchmarks%2520and%250Aperiodically%2520update%2520this%2520paper%2520to%2520provide%2520more%2520comprehensive%2520and%2520accurate%250Aevaluation%2520results.%2520Please%2520refer%2520to%2520the%2520latest%2520version%2520of%2520the%2520paper%2520for%2520the%250Amost%2520recent%2520results%2520and%2520conclusions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11164v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantifying%20the%20Capability%20Boundary%20of%20DeepSeek%20Models%3A%20An%0A%20%20Application-Driven%20Performance%20Analysis&entry.906535625=Kaikai%20Zhao%20and%20Zhaoxiang%20Liu%20and%20Xuejiao%20Lei%20and%20Ning%20Wang%20and%20Jiaojiao%20Zhao%20and%20Zipeng%20Wang%20and%20Zhenhong%20Long%20and%20Peijun%20Yang%20and%20Minjie%20Hua%20and%20Chaoyang%20Ma%20and%20Wen%20Liu%20and%20Kai%20Wang%20and%20Shiguo%20Lian&entry.1292438233=%20%20DeepSeek-R1%2C%20known%20for%20its%20low%20training%20cost%20and%20exceptional%20reasoning%0Acapabilities%2C%20has%20achieved%20state-of-the-art%20performance%20on%20various%20benchmarks.%0AHowever%2C%20detailed%20evaluations%20from%20the%20perspective%20of%20real-world%20applications%0Aare%20lacking%2C%20making%20it%20challenging%20for%20users%20to%20select%20the%20most%20suitable%0ADeepSeek%20models%20for%20their%20specific%20needs.%20To%20address%20this%20gap%2C%20we%20evaluate%20the%0ADeepSeek-V3%2C%20DeepSeek-R1%2C%20DeepSeek-R1-Distill-Qwen%20series%2C%20and%0ADeepSeek-R1-Distill-Llama%20series%20on%20the%20improved%20version%20A-Eval%20%28A-Eval-2.0%29%2C%0Aan%20application-driven%20benchmark.%20By%20comparing%20original%20instruction-tuned%20models%0Awith%20their%20distilled%20counterparts%2C%20we%20analyze%20how%20reasoning%20enhancements%20impact%0Aperformance%20across%20diverse%20practical%20tasks.%20Our%20results%20show%20that%0Areasoning-enhanced%20models%2C%20while%20generally%20powerful%2C%20do%20not%20universally%0Aoutperform%20across%20all%20tasks%2C%20with%20performance%20gains%20varying%20significantly%0Aacross%20tasks%20and%20models.%20To%20further%20assist%20users%20in%20model%20selection%2C%20we%0Aquantify%20the%20capability%20boundary%20of%20DeepSeek%20models%20through%20performance%20tier%0Aclassifications%20and%20intuitive%20line%20charts.%20Specific%20examples%20provide%20actionable%0Ainsights%20to%20help%20users%20select%20and%20deploy%20the%20most%20cost-effective%20DeepSeek%0Amodels%2C%20ensuring%20optimal%20performance%20and%20resource%20efficiency%20in%20real-world%0Aapplications.%20It%20should%20be%20noted%20that%2C%20despite%20our%20efforts%20to%20establish%20a%0Acomprehensive%2C%20objective%2C%20and%20authoritative%20evaluation%20benchmark%2C%20the%20selection%0Aof%20test%20samples%2C%20characteristics%20of%20data%20distribution%2C%20and%20the%20setting%20of%0Aevaluation%20criteria%20may%20inevitably%20introduce%20certain%20biases%20into%20the%20evaluation%0Aresults.%20We%20will%20continuously%20optimize%20the%20evaluation%20benchmarks%20and%0Aperiodically%20update%20this%20paper%20to%20provide%20more%20comprehensive%20and%20accurate%0Aevaluation%20results.%20Please%20refer%20to%20the%20latest%20version%20of%20the%20paper%20for%20the%0Amost%20recent%20results%20and%20conclusions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11164v2&entry.124074799=Read"},
{"title": "Adaptive Retrieval Without Self-Knowledge? Bringing Uncertainty Back\n  Home", "author": "Viktor Moskvoretskii and Maria Lysyuk and Mikhail Salnikov and Nikolay Ivanov and Sergey Pletenev and Daria Galimzianova and Nikita Krayko and Vasily Konovalov and Irina Nikishina and Alexander Panchenko", "abstract": "  Retrieval Augmented Generation (RAG) improves correctness of Question\nAnswering (QA) and addresses hallucinations in Large Language Models (LLMs),\nyet greatly increase computational costs. Besides, RAG is not always needed as\nmay introduce irrelevant information. Recent adaptive retrieval methods\nintegrate LLMs' intrinsic knowledge with external information appealing to LLM\nself-knowledge, but they often neglect efficiency evaluations and comparisons\nwith uncertainty estimation techniques. We bridge this gap by conducting a\ncomprehensive analysis of 35 adaptive retrieval methods, including 8 recent\napproaches and 27 uncertainty estimation techniques, across 6 datasets using 10\nmetrics for QA performance, self-knowledge, and efficiency. Our findings show\nthat uncertainty estimation techniques often outperform complex pipelines in\nterms of efficiency and self-knowledge, while maintaining comparable QA\nperformance.\n", "link": "http://arxiv.org/abs/2501.12835v2", "date": "2025-02-21", "relevancy": 2.1202, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5556}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5254}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Retrieval%20Without%20Self-Knowledge%3F%20Bringing%20Uncertainty%20Back%0A%20%20Home&body=Title%3A%20Adaptive%20Retrieval%20Without%20Self-Knowledge%3F%20Bringing%20Uncertainty%20Back%0A%20%20Home%0AAuthor%3A%20Viktor%20Moskvoretskii%20and%20Maria%20Lysyuk%20and%20Mikhail%20Salnikov%20and%20Nikolay%20Ivanov%20and%20Sergey%20Pletenev%20and%20Daria%20Galimzianova%20and%20Nikita%20Krayko%20and%20Vasily%20Konovalov%20and%20Irina%20Nikishina%20and%20Alexander%20Panchenko%0AAbstract%3A%20%20%20Retrieval%20Augmented%20Generation%20%28RAG%29%20improves%20correctness%20of%20Question%0AAnswering%20%28QA%29%20and%20addresses%20hallucinations%20in%20Large%20Language%20Models%20%28LLMs%29%2C%0Ayet%20greatly%20increase%20computational%20costs.%20Besides%2C%20RAG%20is%20not%20always%20needed%20as%0Amay%20introduce%20irrelevant%20information.%20Recent%20adaptive%20retrieval%20methods%0Aintegrate%20LLMs%27%20intrinsic%20knowledge%20with%20external%20information%20appealing%20to%20LLM%0Aself-knowledge%2C%20but%20they%20often%20neglect%20efficiency%20evaluations%20and%20comparisons%0Awith%20uncertainty%20estimation%20techniques.%20We%20bridge%20this%20gap%20by%20conducting%20a%0Acomprehensive%20analysis%20of%2035%20adaptive%20retrieval%20methods%2C%20including%208%20recent%0Aapproaches%20and%2027%20uncertainty%20estimation%20techniques%2C%20across%206%20datasets%20using%2010%0Ametrics%20for%20QA%20performance%2C%20self-knowledge%2C%20and%20efficiency.%20Our%20findings%20show%0Athat%20uncertainty%20estimation%20techniques%20often%20outperform%20complex%20pipelines%20in%0Aterms%20of%20efficiency%20and%20self-knowledge%2C%20while%20maintaining%20comparable%20QA%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12835v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Retrieval%2520Without%2520Self-Knowledge%253F%2520Bringing%2520Uncertainty%2520Back%250A%2520%2520Home%26entry.906535625%3DViktor%2520Moskvoretskii%2520and%2520Maria%2520Lysyuk%2520and%2520Mikhail%2520Salnikov%2520and%2520Nikolay%2520Ivanov%2520and%2520Sergey%2520Pletenev%2520and%2520Daria%2520Galimzianova%2520and%2520Nikita%2520Krayko%2520and%2520Vasily%2520Konovalov%2520and%2520Irina%2520Nikishina%2520and%2520Alexander%2520Panchenko%26entry.1292438233%3D%2520%2520Retrieval%2520Augmented%2520Generation%2520%2528RAG%2529%2520improves%2520correctness%2520of%2520Question%250AAnswering%2520%2528QA%2529%2520and%2520addresses%2520hallucinations%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%250Ayet%2520greatly%2520increase%2520computational%2520costs.%2520Besides%252C%2520RAG%2520is%2520not%2520always%2520needed%2520as%250Amay%2520introduce%2520irrelevant%2520information.%2520Recent%2520adaptive%2520retrieval%2520methods%250Aintegrate%2520LLMs%2527%2520intrinsic%2520knowledge%2520with%2520external%2520information%2520appealing%2520to%2520LLM%250Aself-knowledge%252C%2520but%2520they%2520often%2520neglect%2520efficiency%2520evaluations%2520and%2520comparisons%250Awith%2520uncertainty%2520estimation%2520techniques.%2520We%2520bridge%2520this%2520gap%2520by%2520conducting%2520a%250Acomprehensive%2520analysis%2520of%252035%2520adaptive%2520retrieval%2520methods%252C%2520including%25208%2520recent%250Aapproaches%2520and%252027%2520uncertainty%2520estimation%2520techniques%252C%2520across%25206%2520datasets%2520using%252010%250Ametrics%2520for%2520QA%2520performance%252C%2520self-knowledge%252C%2520and%2520efficiency.%2520Our%2520findings%2520show%250Athat%2520uncertainty%2520estimation%2520techniques%2520often%2520outperform%2520complex%2520pipelines%2520in%250Aterms%2520of%2520efficiency%2520and%2520self-knowledge%252C%2520while%2520maintaining%2520comparable%2520QA%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12835v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Retrieval%20Without%20Self-Knowledge%3F%20Bringing%20Uncertainty%20Back%0A%20%20Home&entry.906535625=Viktor%20Moskvoretskii%20and%20Maria%20Lysyuk%20and%20Mikhail%20Salnikov%20and%20Nikolay%20Ivanov%20and%20Sergey%20Pletenev%20and%20Daria%20Galimzianova%20and%20Nikita%20Krayko%20and%20Vasily%20Konovalov%20and%20Irina%20Nikishina%20and%20Alexander%20Panchenko&entry.1292438233=%20%20Retrieval%20Augmented%20Generation%20%28RAG%29%20improves%20correctness%20of%20Question%0AAnswering%20%28QA%29%20and%20addresses%20hallucinations%20in%20Large%20Language%20Models%20%28LLMs%29%2C%0Ayet%20greatly%20increase%20computational%20costs.%20Besides%2C%20RAG%20is%20not%20always%20needed%20as%0Amay%20introduce%20irrelevant%20information.%20Recent%20adaptive%20retrieval%20methods%0Aintegrate%20LLMs%27%20intrinsic%20knowledge%20with%20external%20information%20appealing%20to%20LLM%0Aself-knowledge%2C%20but%20they%20often%20neglect%20efficiency%20evaluations%20and%20comparisons%0Awith%20uncertainty%20estimation%20techniques.%20We%20bridge%20this%20gap%20by%20conducting%20a%0Acomprehensive%20analysis%20of%2035%20adaptive%20retrieval%20methods%2C%20including%208%20recent%0Aapproaches%20and%2027%20uncertainty%20estimation%20techniques%2C%20across%206%20datasets%20using%2010%0Ametrics%20for%20QA%20performance%2C%20self-knowledge%2C%20and%20efficiency.%20Our%20findings%20show%0Athat%20uncertainty%20estimation%20techniques%20often%20outperform%20complex%20pipelines%20in%0Aterms%20of%20efficiency%20and%20self-knowledge%2C%20while%20maintaining%20comparable%20QA%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12835v2&entry.124074799=Read"},
{"title": "Towards Robust Probabilistic Modeling on SO(3) via Rotation Laplace\n  Distribution", "author": "Yingda Yin and Jiangran Lyu and Yang Wang and Haoran Liu and He Wang and Baoquan Chen", "abstract": "  Estimating the 3DoF rotation from a single RGB image is an important yet\nchallenging problem. As a popular approach, probabilistic rotation modeling\nadditionally carries prediction uncertainty information, compared to\nsingle-prediction rotation regression. For modeling probabilistic distribution\nover SO(3), it is natural to use Gaussian-like Bingham distribution and matrix\nFisher, however they are shown to be sensitive to outlier predictions, e.g.\n$180^\\circ$ error and thus are unlikely to converge with optimal performance.\nIn this paper, we draw inspiration from multivariate Laplace distribution and\npropose a novel rotation Laplace distribution on SO(3). Our rotation Laplace\ndistribution is robust to the disturbance of outliers and enforces much\ngradient to the low-error region that it can improve. In addition, we show that\nour method also exhibits robustness to small noises and thus tolerates\nimperfect annotations. With this benefit, we demonstrate its advantages in\nsemi-supervised rotation regression, where the pseudo labels are noisy. To\nfurther capture the multi-modal rotation solution space for symmetric objects,\nwe extend our distribution to rotation Laplace mixture model and demonstrate\nits effectiveness. Our extensive experiments show that our proposed\ndistribution and the mixture model achieve state-of-the-art performance in all\nthe rotation regression experiments over both probabilistic and\nnon-probabilistic baselines.\n", "link": "http://arxiv.org/abs/2305.10465v2", "date": "2025-02-21", "relevancy": 2.0992, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5281}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5267}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Robust%20Probabilistic%20Modeling%20on%20SO%283%29%20via%20Rotation%20Laplace%0A%20%20Distribution&body=Title%3A%20Towards%20Robust%20Probabilistic%20Modeling%20on%20SO%283%29%20via%20Rotation%20Laplace%0A%20%20Distribution%0AAuthor%3A%20Yingda%20Yin%20and%20Jiangran%20Lyu%20and%20Yang%20Wang%20and%20Haoran%20Liu%20and%20He%20Wang%20and%20Baoquan%20Chen%0AAbstract%3A%20%20%20Estimating%20the%203DoF%20rotation%20from%20a%20single%20RGB%20image%20is%20an%20important%20yet%0Achallenging%20problem.%20As%20a%20popular%20approach%2C%20probabilistic%20rotation%20modeling%0Aadditionally%20carries%20prediction%20uncertainty%20information%2C%20compared%20to%0Asingle-prediction%20rotation%20regression.%20For%20modeling%20probabilistic%20distribution%0Aover%20SO%283%29%2C%20it%20is%20natural%20to%20use%20Gaussian-like%20Bingham%20distribution%20and%20matrix%0AFisher%2C%20however%20they%20are%20shown%20to%20be%20sensitive%20to%20outlier%20predictions%2C%20e.g.%0A%24180%5E%5Ccirc%24%20error%20and%20thus%20are%20unlikely%20to%20converge%20with%20optimal%20performance.%0AIn%20this%20paper%2C%20we%20draw%20inspiration%20from%20multivariate%20Laplace%20distribution%20and%0Apropose%20a%20novel%20rotation%20Laplace%20distribution%20on%20SO%283%29.%20Our%20rotation%20Laplace%0Adistribution%20is%20robust%20to%20the%20disturbance%20of%20outliers%20and%20enforces%20much%0Agradient%20to%20the%20low-error%20region%20that%20it%20can%20improve.%20In%20addition%2C%20we%20show%20that%0Aour%20method%20also%20exhibits%20robustness%20to%20small%20noises%20and%20thus%20tolerates%0Aimperfect%20annotations.%20With%20this%20benefit%2C%20we%20demonstrate%20its%20advantages%20in%0Asemi-supervised%20rotation%20regression%2C%20where%20the%20pseudo%20labels%20are%20noisy.%20To%0Afurther%20capture%20the%20multi-modal%20rotation%20solution%20space%20for%20symmetric%20objects%2C%0Awe%20extend%20our%20distribution%20to%20rotation%20Laplace%20mixture%20model%20and%20demonstrate%0Aits%20effectiveness.%20Our%20extensive%20experiments%20show%20that%20our%20proposed%0Adistribution%20and%20the%20mixture%20model%20achieve%20state-of-the-art%20performance%20in%20all%0Athe%20rotation%20regression%20experiments%20over%20both%20probabilistic%20and%0Anon-probabilistic%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.10465v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Robust%2520Probabilistic%2520Modeling%2520on%2520SO%25283%2529%2520via%2520Rotation%2520Laplace%250A%2520%2520Distribution%26entry.906535625%3DYingda%2520Yin%2520and%2520Jiangran%2520Lyu%2520and%2520Yang%2520Wang%2520and%2520Haoran%2520Liu%2520and%2520He%2520Wang%2520and%2520Baoquan%2520Chen%26entry.1292438233%3D%2520%2520Estimating%2520the%25203DoF%2520rotation%2520from%2520a%2520single%2520RGB%2520image%2520is%2520an%2520important%2520yet%250Achallenging%2520problem.%2520As%2520a%2520popular%2520approach%252C%2520probabilistic%2520rotation%2520modeling%250Aadditionally%2520carries%2520prediction%2520uncertainty%2520information%252C%2520compared%2520to%250Asingle-prediction%2520rotation%2520regression.%2520For%2520modeling%2520probabilistic%2520distribution%250Aover%2520SO%25283%2529%252C%2520it%2520is%2520natural%2520to%2520use%2520Gaussian-like%2520Bingham%2520distribution%2520and%2520matrix%250AFisher%252C%2520however%2520they%2520are%2520shown%2520to%2520be%2520sensitive%2520to%2520outlier%2520predictions%252C%2520e.g.%250A%2524180%255E%255Ccirc%2524%2520error%2520and%2520thus%2520are%2520unlikely%2520to%2520converge%2520with%2520optimal%2520performance.%250AIn%2520this%2520paper%252C%2520we%2520draw%2520inspiration%2520from%2520multivariate%2520Laplace%2520distribution%2520and%250Apropose%2520a%2520novel%2520rotation%2520Laplace%2520distribution%2520on%2520SO%25283%2529.%2520Our%2520rotation%2520Laplace%250Adistribution%2520is%2520robust%2520to%2520the%2520disturbance%2520of%2520outliers%2520and%2520enforces%2520much%250Agradient%2520to%2520the%2520low-error%2520region%2520that%2520it%2520can%2520improve.%2520In%2520addition%252C%2520we%2520show%2520that%250Aour%2520method%2520also%2520exhibits%2520robustness%2520to%2520small%2520noises%2520and%2520thus%2520tolerates%250Aimperfect%2520annotations.%2520With%2520this%2520benefit%252C%2520we%2520demonstrate%2520its%2520advantages%2520in%250Asemi-supervised%2520rotation%2520regression%252C%2520where%2520the%2520pseudo%2520labels%2520are%2520noisy.%2520To%250Afurther%2520capture%2520the%2520multi-modal%2520rotation%2520solution%2520space%2520for%2520symmetric%2520objects%252C%250Awe%2520extend%2520our%2520distribution%2520to%2520rotation%2520Laplace%2520mixture%2520model%2520and%2520demonstrate%250Aits%2520effectiveness.%2520Our%2520extensive%2520experiments%2520show%2520that%2520our%2520proposed%250Adistribution%2520and%2520the%2520mixture%2520model%2520achieve%2520state-of-the-art%2520performance%2520in%2520all%250Athe%2520rotation%2520regression%2520experiments%2520over%2520both%2520probabilistic%2520and%250Anon-probabilistic%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.10465v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Robust%20Probabilistic%20Modeling%20on%20SO%283%29%20via%20Rotation%20Laplace%0A%20%20Distribution&entry.906535625=Yingda%20Yin%20and%20Jiangran%20Lyu%20and%20Yang%20Wang%20and%20Haoran%20Liu%20and%20He%20Wang%20and%20Baoquan%20Chen&entry.1292438233=%20%20Estimating%20the%203DoF%20rotation%20from%20a%20single%20RGB%20image%20is%20an%20important%20yet%0Achallenging%20problem.%20As%20a%20popular%20approach%2C%20probabilistic%20rotation%20modeling%0Aadditionally%20carries%20prediction%20uncertainty%20information%2C%20compared%20to%0Asingle-prediction%20rotation%20regression.%20For%20modeling%20probabilistic%20distribution%0Aover%20SO%283%29%2C%20it%20is%20natural%20to%20use%20Gaussian-like%20Bingham%20distribution%20and%20matrix%0AFisher%2C%20however%20they%20are%20shown%20to%20be%20sensitive%20to%20outlier%20predictions%2C%20e.g.%0A%24180%5E%5Ccirc%24%20error%20and%20thus%20are%20unlikely%20to%20converge%20with%20optimal%20performance.%0AIn%20this%20paper%2C%20we%20draw%20inspiration%20from%20multivariate%20Laplace%20distribution%20and%0Apropose%20a%20novel%20rotation%20Laplace%20distribution%20on%20SO%283%29.%20Our%20rotation%20Laplace%0Adistribution%20is%20robust%20to%20the%20disturbance%20of%20outliers%20and%20enforces%20much%0Agradient%20to%20the%20low-error%20region%20that%20it%20can%20improve.%20In%20addition%2C%20we%20show%20that%0Aour%20method%20also%20exhibits%20robustness%20to%20small%20noises%20and%20thus%20tolerates%0Aimperfect%20annotations.%20With%20this%20benefit%2C%20we%20demonstrate%20its%20advantages%20in%0Asemi-supervised%20rotation%20regression%2C%20where%20the%20pseudo%20labels%20are%20noisy.%20To%0Afurther%20capture%20the%20multi-modal%20rotation%20solution%20space%20for%20symmetric%20objects%2C%0Awe%20extend%20our%20distribution%20to%20rotation%20Laplace%20mixture%20model%20and%20demonstrate%0Aits%20effectiveness.%20Our%20extensive%20experiments%20show%20that%20our%20proposed%0Adistribution%20and%20the%20mixture%20model%20achieve%20state-of-the-art%20performance%20in%20all%0Athe%20rotation%20regression%20experiments%20over%20both%20probabilistic%20and%0Anon-probabilistic%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.10465v2&entry.124074799=Read"},
{"title": "Graph Neural Diffusion Networks for Semi-supervised Learning", "author": "Wei Ye and Zexi Huang and Yunqi Hong and Ambuj Singh", "abstract": "  Graph Convolutional Networks (GCN) is a pioneering model for graph-based\nsemi-supervised learning. However, GCN does not perform well on\nsparsely-labeled graphs. Its two-layer version cannot effectively propagate the\nlabel information to the whole graph structure (i.e., the under-smoothing\nproblem) while its deep version over-smoothens and is hard to train (i.e., the\nover-smoothing problem). To solve these two issues, we propose a new graph\nneural network called GND-Nets (for Graph Neural Diffusion Networks) that\nexploits the local and global neighborhood information of a vertex in a single\nlayer. Exploiting the shallow network mitigates the over-smoothing problem\nwhile exploiting the local and global neighborhood information mitigates the\nunder-smoothing problem. The utilization of the local and global neighborhood\ninformation of a vertex is achieved by a new graph diffusion method called\nneural diffusions, which integrate neural networks into the conventional linear\nand nonlinear graph diffusions. The adoption of neural networks makes neural\ndiffusions adaptable to different datasets. Extensive experiments on various\nsparsely-labeled graphs verify the effectiveness and efficiency of GND-Nets\ncompared to state-of-the-art approaches.\n", "link": "http://arxiv.org/abs/2201.09698v3", "date": "2025-02-21", "relevancy": 2.0929, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5362}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5167}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Neural%20Diffusion%20Networks%20for%20Semi-supervised%20Learning&body=Title%3A%20Graph%20Neural%20Diffusion%20Networks%20for%20Semi-supervised%20Learning%0AAuthor%3A%20Wei%20Ye%20and%20Zexi%20Huang%20and%20Yunqi%20Hong%20and%20Ambuj%20Singh%0AAbstract%3A%20%20%20Graph%20Convolutional%20Networks%20%28GCN%29%20is%20a%20pioneering%20model%20for%20graph-based%0Asemi-supervised%20learning.%20However%2C%20GCN%20does%20not%20perform%20well%20on%0Asparsely-labeled%20graphs.%20Its%20two-layer%20version%20cannot%20effectively%20propagate%20the%0Alabel%20information%20to%20the%20whole%20graph%20structure%20%28i.e.%2C%20the%20under-smoothing%0Aproblem%29%20while%20its%20deep%20version%20over-smoothens%20and%20is%20hard%20to%20train%20%28i.e.%2C%20the%0Aover-smoothing%20problem%29.%20To%20solve%20these%20two%20issues%2C%20we%20propose%20a%20new%20graph%0Aneural%20network%20called%20GND-Nets%20%28for%20Graph%20Neural%20Diffusion%20Networks%29%20that%0Aexploits%20the%20local%20and%20global%20neighborhood%20information%20of%20a%20vertex%20in%20a%20single%0Alayer.%20Exploiting%20the%20shallow%20network%20mitigates%20the%20over-smoothing%20problem%0Awhile%20exploiting%20the%20local%20and%20global%20neighborhood%20information%20mitigates%20the%0Aunder-smoothing%20problem.%20The%20utilization%20of%20the%20local%20and%20global%20neighborhood%0Ainformation%20of%20a%20vertex%20is%20achieved%20by%20a%20new%20graph%20diffusion%20method%20called%0Aneural%20diffusions%2C%20which%20integrate%20neural%20networks%20into%20the%20conventional%20linear%0Aand%20nonlinear%20graph%20diffusions.%20The%20adoption%20of%20neural%20networks%20makes%20neural%0Adiffusions%20adaptable%20to%20different%20datasets.%20Extensive%20experiments%20on%20various%0Asparsely-labeled%20graphs%20verify%20the%20effectiveness%20and%20efficiency%20of%20GND-Nets%0Acompared%20to%20state-of-the-art%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2201.09698v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Neural%2520Diffusion%2520Networks%2520for%2520Semi-supervised%2520Learning%26entry.906535625%3DWei%2520Ye%2520and%2520Zexi%2520Huang%2520and%2520Yunqi%2520Hong%2520and%2520Ambuj%2520Singh%26entry.1292438233%3D%2520%2520Graph%2520Convolutional%2520Networks%2520%2528GCN%2529%2520is%2520a%2520pioneering%2520model%2520for%2520graph-based%250Asemi-supervised%2520learning.%2520However%252C%2520GCN%2520does%2520not%2520perform%2520well%2520on%250Asparsely-labeled%2520graphs.%2520Its%2520two-layer%2520version%2520cannot%2520effectively%2520propagate%2520the%250Alabel%2520information%2520to%2520the%2520whole%2520graph%2520structure%2520%2528i.e.%252C%2520the%2520under-smoothing%250Aproblem%2529%2520while%2520its%2520deep%2520version%2520over-smoothens%2520and%2520is%2520hard%2520to%2520train%2520%2528i.e.%252C%2520the%250Aover-smoothing%2520problem%2529.%2520To%2520solve%2520these%2520two%2520issues%252C%2520we%2520propose%2520a%2520new%2520graph%250Aneural%2520network%2520called%2520GND-Nets%2520%2528for%2520Graph%2520Neural%2520Diffusion%2520Networks%2529%2520that%250Aexploits%2520the%2520local%2520and%2520global%2520neighborhood%2520information%2520of%2520a%2520vertex%2520in%2520a%2520single%250Alayer.%2520Exploiting%2520the%2520shallow%2520network%2520mitigates%2520the%2520over-smoothing%2520problem%250Awhile%2520exploiting%2520the%2520local%2520and%2520global%2520neighborhood%2520information%2520mitigates%2520the%250Aunder-smoothing%2520problem.%2520The%2520utilization%2520of%2520the%2520local%2520and%2520global%2520neighborhood%250Ainformation%2520of%2520a%2520vertex%2520is%2520achieved%2520by%2520a%2520new%2520graph%2520diffusion%2520method%2520called%250Aneural%2520diffusions%252C%2520which%2520integrate%2520neural%2520networks%2520into%2520the%2520conventional%2520linear%250Aand%2520nonlinear%2520graph%2520diffusions.%2520The%2520adoption%2520of%2520neural%2520networks%2520makes%2520neural%250Adiffusions%2520adaptable%2520to%2520different%2520datasets.%2520Extensive%2520experiments%2520on%2520various%250Asparsely-labeled%2520graphs%2520verify%2520the%2520effectiveness%2520and%2520efficiency%2520of%2520GND-Nets%250Acompared%2520to%2520state-of-the-art%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2201.09698v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Neural%20Diffusion%20Networks%20for%20Semi-supervised%20Learning&entry.906535625=Wei%20Ye%20and%20Zexi%20Huang%20and%20Yunqi%20Hong%20and%20Ambuj%20Singh&entry.1292438233=%20%20Graph%20Convolutional%20Networks%20%28GCN%29%20is%20a%20pioneering%20model%20for%20graph-based%0Asemi-supervised%20learning.%20However%2C%20GCN%20does%20not%20perform%20well%20on%0Asparsely-labeled%20graphs.%20Its%20two-layer%20version%20cannot%20effectively%20propagate%20the%0Alabel%20information%20to%20the%20whole%20graph%20structure%20%28i.e.%2C%20the%20under-smoothing%0Aproblem%29%20while%20its%20deep%20version%20over-smoothens%20and%20is%20hard%20to%20train%20%28i.e.%2C%20the%0Aover-smoothing%20problem%29.%20To%20solve%20these%20two%20issues%2C%20we%20propose%20a%20new%20graph%0Aneural%20network%20called%20GND-Nets%20%28for%20Graph%20Neural%20Diffusion%20Networks%29%20that%0Aexploits%20the%20local%20and%20global%20neighborhood%20information%20of%20a%20vertex%20in%20a%20single%0Alayer.%20Exploiting%20the%20shallow%20network%20mitigates%20the%20over-smoothing%20problem%0Awhile%20exploiting%20the%20local%20and%20global%20neighborhood%20information%20mitigates%20the%0Aunder-smoothing%20problem.%20The%20utilization%20of%20the%20local%20and%20global%20neighborhood%0Ainformation%20of%20a%20vertex%20is%20achieved%20by%20a%20new%20graph%20diffusion%20method%20called%0Aneural%20diffusions%2C%20which%20integrate%20neural%20networks%20into%20the%20conventional%20linear%0Aand%20nonlinear%20graph%20diffusions.%20The%20adoption%20of%20neural%20networks%20makes%20neural%0Adiffusions%20adaptable%20to%20different%20datasets.%20Extensive%20experiments%20on%20various%0Asparsely-labeled%20graphs%20verify%20the%20effectiveness%20and%20efficiency%20of%20GND-Nets%0Acompared%20to%20state-of-the-art%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2201.09698v3&entry.124074799=Read"},
{"title": "Continual Person Identification using Footstep-Induced Floor Vibrations\n  on Heterogeneous Floor Structures", "author": "Yiwen Dong and Hae Young Noh", "abstract": "  Person identification is important for smart buildings to provide\npersonalized services such as health monitoring, activity tracking, and\npersonnel management. However, previous person identification relies on\npre-collected data from everyone, which is impractical in many buildings and\npublic facilities in which visitors are typically expected. This calls for a\ncontinual person identification system that gradually learns people's\nidentities on the fly. Existing studies use cameras to achieve this goal, but\nthey require direct line-of-sight and also have raised privacy concerns in\npublic. Other modalities such as wearables and pressure mats are limited by the\nrequirement of device-carrying or dense deployment. Thus, prior studies\nintroduced footstep-induced structural vibration sensing, which is\nnon-intrusive and perceived as more privacy-friendly. However, this approach\nhas a significant challenge: the high variability of vibration data due to\nstructural heterogeneity and human gait variations, which makes online person\nidentification algorithms perform poorly. In this paper, we characterize the\nvariability in footstep-induced structural vibration data for accurate online\nperson identification. To achieve this, we quantify and decompose different\nsources of variability and then design a feature transformation function to\nreduce the variability within each person's data to make different people's\ndata more separable. We evaluate our approach through field experiments with 20\npeople. The results show a 70% variability reduction and a 90% accuracy for\nonline person identification.\n", "link": "http://arxiv.org/abs/2502.15632v1", "date": "2025-02-21", "relevancy": 2.0876, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5459}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5323}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Person%20Identification%20using%20Footstep-Induced%20Floor%20Vibrations%0A%20%20on%20Heterogeneous%20Floor%20Structures&body=Title%3A%20Continual%20Person%20Identification%20using%20Footstep-Induced%20Floor%20Vibrations%0A%20%20on%20Heterogeneous%20Floor%20Structures%0AAuthor%3A%20Yiwen%20Dong%20and%20Hae%20Young%20Noh%0AAbstract%3A%20%20%20Person%20identification%20is%20important%20for%20smart%20buildings%20to%20provide%0Apersonalized%20services%20such%20as%20health%20monitoring%2C%20activity%20tracking%2C%20and%0Apersonnel%20management.%20However%2C%20previous%20person%20identification%20relies%20on%0Apre-collected%20data%20from%20everyone%2C%20which%20is%20impractical%20in%20many%20buildings%20and%0Apublic%20facilities%20in%20which%20visitors%20are%20typically%20expected.%20This%20calls%20for%20a%0Acontinual%20person%20identification%20system%20that%20gradually%20learns%20people%27s%0Aidentities%20on%20the%20fly.%20Existing%20studies%20use%20cameras%20to%20achieve%20this%20goal%2C%20but%0Athey%20require%20direct%20line-of-sight%20and%20also%20have%20raised%20privacy%20concerns%20in%0Apublic.%20Other%20modalities%20such%20as%20wearables%20and%20pressure%20mats%20are%20limited%20by%20the%0Arequirement%20of%20device-carrying%20or%20dense%20deployment.%20Thus%2C%20prior%20studies%0Aintroduced%20footstep-induced%20structural%20vibration%20sensing%2C%20which%20is%0Anon-intrusive%20and%20perceived%20as%20more%20privacy-friendly.%20However%2C%20this%20approach%0Ahas%20a%20significant%20challenge%3A%20the%20high%20variability%20of%20vibration%20data%20due%20to%0Astructural%20heterogeneity%20and%20human%20gait%20variations%2C%20which%20makes%20online%20person%0Aidentification%20algorithms%20perform%20poorly.%20In%20this%20paper%2C%20we%20characterize%20the%0Avariability%20in%20footstep-induced%20structural%20vibration%20data%20for%20accurate%20online%0Aperson%20identification.%20To%20achieve%20this%2C%20we%20quantify%20and%20decompose%20different%0Asources%20of%20variability%20and%20then%20design%20a%20feature%20transformation%20function%20to%0Areduce%20the%20variability%20within%20each%20person%27s%20data%20to%20make%20different%20people%27s%0Adata%20more%20separable.%20We%20evaluate%20our%20approach%20through%20field%20experiments%20with%2020%0Apeople.%20The%20results%20show%20a%2070%25%20variability%20reduction%20and%20a%2090%25%20accuracy%20for%0Aonline%20person%20identification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15632v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Person%2520Identification%2520using%2520Footstep-Induced%2520Floor%2520Vibrations%250A%2520%2520on%2520Heterogeneous%2520Floor%2520Structures%26entry.906535625%3DYiwen%2520Dong%2520and%2520Hae%2520Young%2520Noh%26entry.1292438233%3D%2520%2520Person%2520identification%2520is%2520important%2520for%2520smart%2520buildings%2520to%2520provide%250Apersonalized%2520services%2520such%2520as%2520health%2520monitoring%252C%2520activity%2520tracking%252C%2520and%250Apersonnel%2520management.%2520However%252C%2520previous%2520person%2520identification%2520relies%2520on%250Apre-collected%2520data%2520from%2520everyone%252C%2520which%2520is%2520impractical%2520in%2520many%2520buildings%2520and%250Apublic%2520facilities%2520in%2520which%2520visitors%2520are%2520typically%2520expected.%2520This%2520calls%2520for%2520a%250Acontinual%2520person%2520identification%2520system%2520that%2520gradually%2520learns%2520people%2527s%250Aidentities%2520on%2520the%2520fly.%2520Existing%2520studies%2520use%2520cameras%2520to%2520achieve%2520this%2520goal%252C%2520but%250Athey%2520require%2520direct%2520line-of-sight%2520and%2520also%2520have%2520raised%2520privacy%2520concerns%2520in%250Apublic.%2520Other%2520modalities%2520such%2520as%2520wearables%2520and%2520pressure%2520mats%2520are%2520limited%2520by%2520the%250Arequirement%2520of%2520device-carrying%2520or%2520dense%2520deployment.%2520Thus%252C%2520prior%2520studies%250Aintroduced%2520footstep-induced%2520structural%2520vibration%2520sensing%252C%2520which%2520is%250Anon-intrusive%2520and%2520perceived%2520as%2520more%2520privacy-friendly.%2520However%252C%2520this%2520approach%250Ahas%2520a%2520significant%2520challenge%253A%2520the%2520high%2520variability%2520of%2520vibration%2520data%2520due%2520to%250Astructural%2520heterogeneity%2520and%2520human%2520gait%2520variations%252C%2520which%2520makes%2520online%2520person%250Aidentification%2520algorithms%2520perform%2520poorly.%2520In%2520this%2520paper%252C%2520we%2520characterize%2520the%250Avariability%2520in%2520footstep-induced%2520structural%2520vibration%2520data%2520for%2520accurate%2520online%250Aperson%2520identification.%2520To%2520achieve%2520this%252C%2520we%2520quantify%2520and%2520decompose%2520different%250Asources%2520of%2520variability%2520and%2520then%2520design%2520a%2520feature%2520transformation%2520function%2520to%250Areduce%2520the%2520variability%2520within%2520each%2520person%2527s%2520data%2520to%2520make%2520different%2520people%2527s%250Adata%2520more%2520separable.%2520We%2520evaluate%2520our%2520approach%2520through%2520field%2520experiments%2520with%252020%250Apeople.%2520The%2520results%2520show%2520a%252070%2525%2520variability%2520reduction%2520and%2520a%252090%2525%2520accuracy%2520for%250Aonline%2520person%2520identification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15632v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Person%20Identification%20using%20Footstep-Induced%20Floor%20Vibrations%0A%20%20on%20Heterogeneous%20Floor%20Structures&entry.906535625=Yiwen%20Dong%20and%20Hae%20Young%20Noh&entry.1292438233=%20%20Person%20identification%20is%20important%20for%20smart%20buildings%20to%20provide%0Apersonalized%20services%20such%20as%20health%20monitoring%2C%20activity%20tracking%2C%20and%0Apersonnel%20management.%20However%2C%20previous%20person%20identification%20relies%20on%0Apre-collected%20data%20from%20everyone%2C%20which%20is%20impractical%20in%20many%20buildings%20and%0Apublic%20facilities%20in%20which%20visitors%20are%20typically%20expected.%20This%20calls%20for%20a%0Acontinual%20person%20identification%20system%20that%20gradually%20learns%20people%27s%0Aidentities%20on%20the%20fly.%20Existing%20studies%20use%20cameras%20to%20achieve%20this%20goal%2C%20but%0Athey%20require%20direct%20line-of-sight%20and%20also%20have%20raised%20privacy%20concerns%20in%0Apublic.%20Other%20modalities%20such%20as%20wearables%20and%20pressure%20mats%20are%20limited%20by%20the%0Arequirement%20of%20device-carrying%20or%20dense%20deployment.%20Thus%2C%20prior%20studies%0Aintroduced%20footstep-induced%20structural%20vibration%20sensing%2C%20which%20is%0Anon-intrusive%20and%20perceived%20as%20more%20privacy-friendly.%20However%2C%20this%20approach%0Ahas%20a%20significant%20challenge%3A%20the%20high%20variability%20of%20vibration%20data%20due%20to%0Astructural%20heterogeneity%20and%20human%20gait%20variations%2C%20which%20makes%20online%20person%0Aidentification%20algorithms%20perform%20poorly.%20In%20this%20paper%2C%20we%20characterize%20the%0Avariability%20in%20footstep-induced%20structural%20vibration%20data%20for%20accurate%20online%0Aperson%20identification.%20To%20achieve%20this%2C%20we%20quantify%20and%20decompose%20different%0Asources%20of%20variability%20and%20then%20design%20a%20feature%20transformation%20function%20to%0Areduce%20the%20variability%20within%20each%20person%27s%20data%20to%20make%20different%20people%27s%0Adata%20more%20separable.%20We%20evaluate%20our%20approach%20through%20field%20experiments%20with%2020%0Apeople.%20The%20results%20show%20a%2070%25%20variability%20reduction%20and%20a%2090%25%20accuracy%20for%0Aonline%20person%20identification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15632v1&entry.124074799=Read"},
{"title": "Towards Foundation Models for Mixed Integer Linear Programming", "author": "Sirui Li and Janardhan Kulkarni and Ishai Menache and Cathy Wu and Beibin Li", "abstract": "  Mixed Integer Linear Programming (MILP) is essential for modeling complex\ndecision-making problems but faces challenges in computational tractability and\nrequires expert formulation. Current deep learning approaches for MILP focus on\nspecific problem classes and do not generalize to unseen classes. To address\nthis shortcoming, we take a foundation model training approach, where we train\na single deep learning model on a diverse set of MILP problems to generalize\nacross problem classes. As existing datasets for MILP lack diversity and\nvolume, we introduce MILP-Evolve, a novel LLM-based evolutionary framework that\nis capable of generating a large set of diverse MILP classes with an unlimited\namount of instances. We study our methodology on three key learning tasks that\ncapture diverse aspects of MILP: (1) integrality gap prediction, (2) learning\nto branch, and (3) a new task of aligning MILP instances with natural language\ndescriptions. Our empirical results show that models trained on the data\ngenerated by MILP-Evolve achieve significant improvements on unseen problems,\nincluding MIPLIB benchmarks. Our work highlights the potential of moving\ntowards a foundation model approach for MILP that can generalize to a broad\nrange of MILP applications. Our code and data are publicly available at\nhttps://github.com/microsoft/OptiGuide.\n", "link": "http://arxiv.org/abs/2410.08288v2", "date": "2025-02-21", "relevancy": 2.0718, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5193}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5193}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Foundation%20Models%20for%20Mixed%20Integer%20Linear%20Programming&body=Title%3A%20Towards%20Foundation%20Models%20for%20Mixed%20Integer%20Linear%20Programming%0AAuthor%3A%20Sirui%20Li%20and%20Janardhan%20Kulkarni%20and%20Ishai%20Menache%20and%20Cathy%20Wu%20and%20Beibin%20Li%0AAbstract%3A%20%20%20Mixed%20Integer%20Linear%20Programming%20%28MILP%29%20is%20essential%20for%20modeling%20complex%0Adecision-making%20problems%20but%20faces%20challenges%20in%20computational%20tractability%20and%0Arequires%20expert%20formulation.%20Current%20deep%20learning%20approaches%20for%20MILP%20focus%20on%0Aspecific%20problem%20classes%20and%20do%20not%20generalize%20to%20unseen%20classes.%20To%20address%0Athis%20shortcoming%2C%20we%20take%20a%20foundation%20model%20training%20approach%2C%20where%20we%20train%0Aa%20single%20deep%20learning%20model%20on%20a%20diverse%20set%20of%20MILP%20problems%20to%20generalize%0Aacross%20problem%20classes.%20As%20existing%20datasets%20for%20MILP%20lack%20diversity%20and%0Avolume%2C%20we%20introduce%20MILP-Evolve%2C%20a%20novel%20LLM-based%20evolutionary%20framework%20that%0Ais%20capable%20of%20generating%20a%20large%20set%20of%20diverse%20MILP%20classes%20with%20an%20unlimited%0Aamount%20of%20instances.%20We%20study%20our%20methodology%20on%20three%20key%20learning%20tasks%20that%0Acapture%20diverse%20aspects%20of%20MILP%3A%20%281%29%20integrality%20gap%20prediction%2C%20%282%29%20learning%0Ato%20branch%2C%20and%20%283%29%20a%20new%20task%20of%20aligning%20MILP%20instances%20with%20natural%20language%0Adescriptions.%20Our%20empirical%20results%20show%20that%20models%20trained%20on%20the%20data%0Agenerated%20by%20MILP-Evolve%20achieve%20significant%20improvements%20on%20unseen%20problems%2C%0Aincluding%20MIPLIB%20benchmarks.%20Our%20work%20highlights%20the%20potential%20of%20moving%0Atowards%20a%20foundation%20model%20approach%20for%20MILP%20that%20can%20generalize%20to%20a%20broad%0Arange%20of%20MILP%20applications.%20Our%20code%20and%20data%20are%20publicly%20available%20at%0Ahttps%3A//github.com/microsoft/OptiGuide.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08288v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Foundation%2520Models%2520for%2520Mixed%2520Integer%2520Linear%2520Programming%26entry.906535625%3DSirui%2520Li%2520and%2520Janardhan%2520Kulkarni%2520and%2520Ishai%2520Menache%2520and%2520Cathy%2520Wu%2520and%2520Beibin%2520Li%26entry.1292438233%3D%2520%2520Mixed%2520Integer%2520Linear%2520Programming%2520%2528MILP%2529%2520is%2520essential%2520for%2520modeling%2520complex%250Adecision-making%2520problems%2520but%2520faces%2520challenges%2520in%2520computational%2520tractability%2520and%250Arequires%2520expert%2520formulation.%2520Current%2520deep%2520learning%2520approaches%2520for%2520MILP%2520focus%2520on%250Aspecific%2520problem%2520classes%2520and%2520do%2520not%2520generalize%2520to%2520unseen%2520classes.%2520To%2520address%250Athis%2520shortcoming%252C%2520we%2520take%2520a%2520foundation%2520model%2520training%2520approach%252C%2520where%2520we%2520train%250Aa%2520single%2520deep%2520learning%2520model%2520on%2520a%2520diverse%2520set%2520of%2520MILP%2520problems%2520to%2520generalize%250Aacross%2520problem%2520classes.%2520As%2520existing%2520datasets%2520for%2520MILP%2520lack%2520diversity%2520and%250Avolume%252C%2520we%2520introduce%2520MILP-Evolve%252C%2520a%2520novel%2520LLM-based%2520evolutionary%2520framework%2520that%250Ais%2520capable%2520of%2520generating%2520a%2520large%2520set%2520of%2520diverse%2520MILP%2520classes%2520with%2520an%2520unlimited%250Aamount%2520of%2520instances.%2520We%2520study%2520our%2520methodology%2520on%2520three%2520key%2520learning%2520tasks%2520that%250Acapture%2520diverse%2520aspects%2520of%2520MILP%253A%2520%25281%2529%2520integrality%2520gap%2520prediction%252C%2520%25282%2529%2520learning%250Ato%2520branch%252C%2520and%2520%25283%2529%2520a%2520new%2520task%2520of%2520aligning%2520MILP%2520instances%2520with%2520natural%2520language%250Adescriptions.%2520Our%2520empirical%2520results%2520show%2520that%2520models%2520trained%2520on%2520the%2520data%250Agenerated%2520by%2520MILP-Evolve%2520achieve%2520significant%2520improvements%2520on%2520unseen%2520problems%252C%250Aincluding%2520MIPLIB%2520benchmarks.%2520Our%2520work%2520highlights%2520the%2520potential%2520of%2520moving%250Atowards%2520a%2520foundation%2520model%2520approach%2520for%2520MILP%2520that%2520can%2520generalize%2520to%2520a%2520broad%250Arange%2520of%2520MILP%2520applications.%2520Our%2520code%2520and%2520data%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/microsoft/OptiGuide.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08288v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Foundation%20Models%20for%20Mixed%20Integer%20Linear%20Programming&entry.906535625=Sirui%20Li%20and%20Janardhan%20Kulkarni%20and%20Ishai%20Menache%20and%20Cathy%20Wu%20and%20Beibin%20Li&entry.1292438233=%20%20Mixed%20Integer%20Linear%20Programming%20%28MILP%29%20is%20essential%20for%20modeling%20complex%0Adecision-making%20problems%20but%20faces%20challenges%20in%20computational%20tractability%20and%0Arequires%20expert%20formulation.%20Current%20deep%20learning%20approaches%20for%20MILP%20focus%20on%0Aspecific%20problem%20classes%20and%20do%20not%20generalize%20to%20unseen%20classes.%20To%20address%0Athis%20shortcoming%2C%20we%20take%20a%20foundation%20model%20training%20approach%2C%20where%20we%20train%0Aa%20single%20deep%20learning%20model%20on%20a%20diverse%20set%20of%20MILP%20problems%20to%20generalize%0Aacross%20problem%20classes.%20As%20existing%20datasets%20for%20MILP%20lack%20diversity%20and%0Avolume%2C%20we%20introduce%20MILP-Evolve%2C%20a%20novel%20LLM-based%20evolutionary%20framework%20that%0Ais%20capable%20of%20generating%20a%20large%20set%20of%20diverse%20MILP%20classes%20with%20an%20unlimited%0Aamount%20of%20instances.%20We%20study%20our%20methodology%20on%20three%20key%20learning%20tasks%20that%0Acapture%20diverse%20aspects%20of%20MILP%3A%20%281%29%20integrality%20gap%20prediction%2C%20%282%29%20learning%0Ato%20branch%2C%20and%20%283%29%20a%20new%20task%20of%20aligning%20MILP%20instances%20with%20natural%20language%0Adescriptions.%20Our%20empirical%20results%20show%20that%20models%20trained%20on%20the%20data%0Agenerated%20by%20MILP-Evolve%20achieve%20significant%20improvements%20on%20unseen%20problems%2C%0Aincluding%20MIPLIB%20benchmarks.%20Our%20work%20highlights%20the%20potential%20of%20moving%0Atowards%20a%20foundation%20model%20approach%20for%20MILP%20that%20can%20generalize%20to%20a%20broad%0Arange%20of%20MILP%20applications.%20Our%20code%20and%20data%20are%20publicly%20available%20at%0Ahttps%3A//github.com/microsoft/OptiGuide.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08288v2&entry.124074799=Read"},
{"title": "AttentionEngine: A Versatile Framework for Efficient Attention\n  Mechanisms on Diverse Hardware Platforms", "author": "Feiyang Chen and Yu Cheng and Lei Wang and Yuqing Xia and Ziming Miao and Lingxiao Ma and Fan Yang and Jilong Xue and Zhi Yang and Mao Yang and Haibo Chen", "abstract": "  Transformers and large language models (LLMs) have revolutionized machine\nlearning, with attention mechanisms at the core of their success. As the\nlandscape of attention variants expands, so too do the challenges of optimizing\ntheir performance, particularly across different hardware platforms. Current\noptimization strategies are often narrowly focused, requiring extensive manual\nintervention to accommodate changes in model configurations or hardware\nenvironments. In this paper, we introduce AttentionEngine, a comprehensive\nframework designed to streamline the optimization of attention mechanisms\nacross heterogeneous hardware backends. By decomposing attention computation\ninto modular operations with customizable components, AttentionEngine enables\nflexible adaptation to diverse algorithmic requirements. The framework further\nautomates kernel optimization through a combination of programmable templates\nand a robust cross-platform scheduling strategy. Empirical results reveal\nperformance gains of up to 10x on configurations beyond the reach of existing\nmethods. AttentionEngine offers a scalable, efficient foundation for developing\nand deploying attention mechanisms with minimal manual tuning. Our code has\nbeen open-sourced and is available at\nhttps://github.com/microsoft/AttentionEngine.\n", "link": "http://arxiv.org/abs/2502.15349v1", "date": "2025-02-21", "relevancy": 2.0708, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5541}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4988}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AttentionEngine%3A%20A%20Versatile%20Framework%20for%20Efficient%20Attention%0A%20%20Mechanisms%20on%20Diverse%20Hardware%20Platforms&body=Title%3A%20AttentionEngine%3A%20A%20Versatile%20Framework%20for%20Efficient%20Attention%0A%20%20Mechanisms%20on%20Diverse%20Hardware%20Platforms%0AAuthor%3A%20Feiyang%20Chen%20and%20Yu%20Cheng%20and%20Lei%20Wang%20and%20Yuqing%20Xia%20and%20Ziming%20Miao%20and%20Lingxiao%20Ma%20and%20Fan%20Yang%20and%20Jilong%20Xue%20and%20Zhi%20Yang%20and%20Mao%20Yang%20and%20Haibo%20Chen%0AAbstract%3A%20%20%20Transformers%20and%20large%20language%20models%20%28LLMs%29%20have%20revolutionized%20machine%0Alearning%2C%20with%20attention%20mechanisms%20at%20the%20core%20of%20their%20success.%20As%20the%0Alandscape%20of%20attention%20variants%20expands%2C%20so%20too%20do%20the%20challenges%20of%20optimizing%0Atheir%20performance%2C%20particularly%20across%20different%20hardware%20platforms.%20Current%0Aoptimization%20strategies%20are%20often%20narrowly%20focused%2C%20requiring%20extensive%20manual%0Aintervention%20to%20accommodate%20changes%20in%20model%20configurations%20or%20hardware%0Aenvironments.%20In%20this%20paper%2C%20we%20introduce%20AttentionEngine%2C%20a%20comprehensive%0Aframework%20designed%20to%20streamline%20the%20optimization%20of%20attention%20mechanisms%0Aacross%20heterogeneous%20hardware%20backends.%20By%20decomposing%20attention%20computation%0Ainto%20modular%20operations%20with%20customizable%20components%2C%20AttentionEngine%20enables%0Aflexible%20adaptation%20to%20diverse%20algorithmic%20requirements.%20The%20framework%20further%0Aautomates%20kernel%20optimization%20through%20a%20combination%20of%20programmable%20templates%0Aand%20a%20robust%20cross-platform%20scheduling%20strategy.%20Empirical%20results%20reveal%0Aperformance%20gains%20of%20up%20to%2010x%20on%20configurations%20beyond%20the%20reach%20of%20existing%0Amethods.%20AttentionEngine%20offers%20a%20scalable%2C%20efficient%20foundation%20for%20developing%0Aand%20deploying%20attention%20mechanisms%20with%20minimal%20manual%20tuning.%20Our%20code%20has%0Abeen%20open-sourced%20and%20is%20available%20at%0Ahttps%3A//github.com/microsoft/AttentionEngine.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15349v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttentionEngine%253A%2520A%2520Versatile%2520Framework%2520for%2520Efficient%2520Attention%250A%2520%2520Mechanisms%2520on%2520Diverse%2520Hardware%2520Platforms%26entry.906535625%3DFeiyang%2520Chen%2520and%2520Yu%2520Cheng%2520and%2520Lei%2520Wang%2520and%2520Yuqing%2520Xia%2520and%2520Ziming%2520Miao%2520and%2520Lingxiao%2520Ma%2520and%2520Fan%2520Yang%2520and%2520Jilong%2520Xue%2520and%2520Zhi%2520Yang%2520and%2520Mao%2520Yang%2520and%2520Haibo%2520Chen%26entry.1292438233%3D%2520%2520Transformers%2520and%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520revolutionized%2520machine%250Alearning%252C%2520with%2520attention%2520mechanisms%2520at%2520the%2520core%2520of%2520their%2520success.%2520As%2520the%250Alandscape%2520of%2520attention%2520variants%2520expands%252C%2520so%2520too%2520do%2520the%2520challenges%2520of%2520optimizing%250Atheir%2520performance%252C%2520particularly%2520across%2520different%2520hardware%2520platforms.%2520Current%250Aoptimization%2520strategies%2520are%2520often%2520narrowly%2520focused%252C%2520requiring%2520extensive%2520manual%250Aintervention%2520to%2520accommodate%2520changes%2520in%2520model%2520configurations%2520or%2520hardware%250Aenvironments.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520AttentionEngine%252C%2520a%2520comprehensive%250Aframework%2520designed%2520to%2520streamline%2520the%2520optimization%2520of%2520attention%2520mechanisms%250Aacross%2520heterogeneous%2520hardware%2520backends.%2520By%2520decomposing%2520attention%2520computation%250Ainto%2520modular%2520operations%2520with%2520customizable%2520components%252C%2520AttentionEngine%2520enables%250Aflexible%2520adaptation%2520to%2520diverse%2520algorithmic%2520requirements.%2520The%2520framework%2520further%250Aautomates%2520kernel%2520optimization%2520through%2520a%2520combination%2520of%2520programmable%2520templates%250Aand%2520a%2520robust%2520cross-platform%2520scheduling%2520strategy.%2520Empirical%2520results%2520reveal%250Aperformance%2520gains%2520of%2520up%2520to%252010x%2520on%2520configurations%2520beyond%2520the%2520reach%2520of%2520existing%250Amethods.%2520AttentionEngine%2520offers%2520a%2520scalable%252C%2520efficient%2520foundation%2520for%2520developing%250Aand%2520deploying%2520attention%2520mechanisms%2520with%2520minimal%2520manual%2520tuning.%2520Our%2520code%2520has%250Abeen%2520open-sourced%2520and%2520is%2520available%2520at%250Ahttps%253A//github.com/microsoft/AttentionEngine.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15349v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AttentionEngine%3A%20A%20Versatile%20Framework%20for%20Efficient%20Attention%0A%20%20Mechanisms%20on%20Diverse%20Hardware%20Platforms&entry.906535625=Feiyang%20Chen%20and%20Yu%20Cheng%20and%20Lei%20Wang%20and%20Yuqing%20Xia%20and%20Ziming%20Miao%20and%20Lingxiao%20Ma%20and%20Fan%20Yang%20and%20Jilong%20Xue%20and%20Zhi%20Yang%20and%20Mao%20Yang%20and%20Haibo%20Chen&entry.1292438233=%20%20Transformers%20and%20large%20language%20models%20%28LLMs%29%20have%20revolutionized%20machine%0Alearning%2C%20with%20attention%20mechanisms%20at%20the%20core%20of%20their%20success.%20As%20the%0Alandscape%20of%20attention%20variants%20expands%2C%20so%20too%20do%20the%20challenges%20of%20optimizing%0Atheir%20performance%2C%20particularly%20across%20different%20hardware%20platforms.%20Current%0Aoptimization%20strategies%20are%20often%20narrowly%20focused%2C%20requiring%20extensive%20manual%0Aintervention%20to%20accommodate%20changes%20in%20model%20configurations%20or%20hardware%0Aenvironments.%20In%20this%20paper%2C%20we%20introduce%20AttentionEngine%2C%20a%20comprehensive%0Aframework%20designed%20to%20streamline%20the%20optimization%20of%20attention%20mechanisms%0Aacross%20heterogeneous%20hardware%20backends.%20By%20decomposing%20attention%20computation%0Ainto%20modular%20operations%20with%20customizable%20components%2C%20AttentionEngine%20enables%0Aflexible%20adaptation%20to%20diverse%20algorithmic%20requirements.%20The%20framework%20further%0Aautomates%20kernel%20optimization%20through%20a%20combination%20of%20programmable%20templates%0Aand%20a%20robust%20cross-platform%20scheduling%20strategy.%20Empirical%20results%20reveal%0Aperformance%20gains%20of%20up%20to%2010x%20on%20configurations%20beyond%20the%20reach%20of%20existing%0Amethods.%20AttentionEngine%20offers%20a%20scalable%2C%20efficient%20foundation%20for%20developing%0Aand%20deploying%20attention%20mechanisms%20with%20minimal%20manual%20tuning.%20Our%20code%20has%0Abeen%20open-sourced%20and%20is%20available%20at%0Ahttps%3A//github.com/microsoft/AttentionEngine.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15349v1&entry.124074799=Read"},
{"title": "How Do Programming Students Use Generative AI?", "author": "Christian Rahe and Walid Maalej", "abstract": "  Programming students have a widespread access to powerful Generative AI tools\nlike ChatGPT. While this can help understand the learning material and assist\nwith exercises, educators are voicing more and more concerns about an\noverreliance on generated outputs and lack of critical thinking skills. It is\nthus important to understand how students actually use generative AI and what\nimpact this could have on their learning behavior. To this end, we conducted a\nstudy including an exploratory experiment with 37 programming students, giving\nthem monitored access to ChatGPT while solving a code authoring exercise. The\ntask was not directly solvable by ChatGPT and required code comprehension and\nreasoning. While only 23 of the students actually opted to use the chatbot, the\nmajority of those eventually prompted it to simply generate a full solution. We\nobserved two prevalent usage strategies: to seek knowledge about general\nconcepts and to directly generate solutions. Instead of using the bot to\ncomprehend the code and their own mistakes, students often got trapped in a\nvicious cycle of submitting wrong generated code and then asking the bot for a\nfix. Those who self-reported using generative AI regularly were more likely to\nprompt the bot to generate a solution. Our findings indicate that concerns\nabout potential decrease in programmers' agency and productivity with\nGenerative AI are justified. We discuss how researchers and educators can\nrespond to the potential risk of students uncritically over-relying on\nGenerative AI. We also discuss potential modifications to our study design for\nlarge-scale replications.\n", "link": "http://arxiv.org/abs/2501.10091v2", "date": "2025-02-21", "relevancy": 2.0687, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5514}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5058}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.46}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Do%20Programming%20Students%20Use%20Generative%20AI%3F&body=Title%3A%20How%20Do%20Programming%20Students%20Use%20Generative%20AI%3F%0AAuthor%3A%20Christian%20Rahe%20and%20Walid%20Maalej%0AAbstract%3A%20%20%20Programming%20students%20have%20a%20widespread%20access%20to%20powerful%20Generative%20AI%20tools%0Alike%20ChatGPT.%20While%20this%20can%20help%20understand%20the%20learning%20material%20and%20assist%0Awith%20exercises%2C%20educators%20are%20voicing%20more%20and%20more%20concerns%20about%20an%0Aoverreliance%20on%20generated%20outputs%20and%20lack%20of%20critical%20thinking%20skills.%20It%20is%0Athus%20important%20to%20understand%20how%20students%20actually%20use%20generative%20AI%20and%20what%0Aimpact%20this%20could%20have%20on%20their%20learning%20behavior.%20To%20this%20end%2C%20we%20conducted%20a%0Astudy%20including%20an%20exploratory%20experiment%20with%2037%20programming%20students%2C%20giving%0Athem%20monitored%20access%20to%20ChatGPT%20while%20solving%20a%20code%20authoring%20exercise.%20The%0Atask%20was%20not%20directly%20solvable%20by%20ChatGPT%20and%20required%20code%20comprehension%20and%0Areasoning.%20While%20only%2023%20of%20the%20students%20actually%20opted%20to%20use%20the%20chatbot%2C%20the%0Amajority%20of%20those%20eventually%20prompted%20it%20to%20simply%20generate%20a%20full%20solution.%20We%0Aobserved%20two%20prevalent%20usage%20strategies%3A%20to%20seek%20knowledge%20about%20general%0Aconcepts%20and%20to%20directly%20generate%20solutions.%20Instead%20of%20using%20the%20bot%20to%0Acomprehend%20the%20code%20and%20their%20own%20mistakes%2C%20students%20often%20got%20trapped%20in%20a%0Avicious%20cycle%20of%20submitting%20wrong%20generated%20code%20and%20then%20asking%20the%20bot%20for%20a%0Afix.%20Those%20who%20self-reported%20using%20generative%20AI%20regularly%20were%20more%20likely%20to%0Aprompt%20the%20bot%20to%20generate%20a%20solution.%20Our%20findings%20indicate%20that%20concerns%0Aabout%20potential%20decrease%20in%20programmers%27%20agency%20and%20productivity%20with%0AGenerative%20AI%20are%20justified.%20We%20discuss%20how%20researchers%20and%20educators%20can%0Arespond%20to%20the%20potential%20risk%20of%20students%20uncritically%20over-relying%20on%0AGenerative%20AI.%20We%20also%20discuss%20potential%20modifications%20to%20our%20study%20design%20for%0Alarge-scale%20replications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10091v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Do%2520Programming%2520Students%2520Use%2520Generative%2520AI%253F%26entry.906535625%3DChristian%2520Rahe%2520and%2520Walid%2520Maalej%26entry.1292438233%3D%2520%2520Programming%2520students%2520have%2520a%2520widespread%2520access%2520to%2520powerful%2520Generative%2520AI%2520tools%250Alike%2520ChatGPT.%2520While%2520this%2520can%2520help%2520understand%2520the%2520learning%2520material%2520and%2520assist%250Awith%2520exercises%252C%2520educators%2520are%2520voicing%2520more%2520and%2520more%2520concerns%2520about%2520an%250Aoverreliance%2520on%2520generated%2520outputs%2520and%2520lack%2520of%2520critical%2520thinking%2520skills.%2520It%2520is%250Athus%2520important%2520to%2520understand%2520how%2520students%2520actually%2520use%2520generative%2520AI%2520and%2520what%250Aimpact%2520this%2520could%2520have%2520on%2520their%2520learning%2520behavior.%2520To%2520this%2520end%252C%2520we%2520conducted%2520a%250Astudy%2520including%2520an%2520exploratory%2520experiment%2520with%252037%2520programming%2520students%252C%2520giving%250Athem%2520monitored%2520access%2520to%2520ChatGPT%2520while%2520solving%2520a%2520code%2520authoring%2520exercise.%2520The%250Atask%2520was%2520not%2520directly%2520solvable%2520by%2520ChatGPT%2520and%2520required%2520code%2520comprehension%2520and%250Areasoning.%2520While%2520only%252023%2520of%2520the%2520students%2520actually%2520opted%2520to%2520use%2520the%2520chatbot%252C%2520the%250Amajority%2520of%2520those%2520eventually%2520prompted%2520it%2520to%2520simply%2520generate%2520a%2520full%2520solution.%2520We%250Aobserved%2520two%2520prevalent%2520usage%2520strategies%253A%2520to%2520seek%2520knowledge%2520about%2520general%250Aconcepts%2520and%2520to%2520directly%2520generate%2520solutions.%2520Instead%2520of%2520using%2520the%2520bot%2520to%250Acomprehend%2520the%2520code%2520and%2520their%2520own%2520mistakes%252C%2520students%2520often%2520got%2520trapped%2520in%2520a%250Avicious%2520cycle%2520of%2520submitting%2520wrong%2520generated%2520code%2520and%2520then%2520asking%2520the%2520bot%2520for%2520a%250Afix.%2520Those%2520who%2520self-reported%2520using%2520generative%2520AI%2520regularly%2520were%2520more%2520likely%2520to%250Aprompt%2520the%2520bot%2520to%2520generate%2520a%2520solution.%2520Our%2520findings%2520indicate%2520that%2520concerns%250Aabout%2520potential%2520decrease%2520in%2520programmers%2527%2520agency%2520and%2520productivity%2520with%250AGenerative%2520AI%2520are%2520justified.%2520We%2520discuss%2520how%2520researchers%2520and%2520educators%2520can%250Arespond%2520to%2520the%2520potential%2520risk%2520of%2520students%2520uncritically%2520over-relying%2520on%250AGenerative%2520AI.%2520We%2520also%2520discuss%2520potential%2520modifications%2520to%2520our%2520study%2520design%2520for%250Alarge-scale%2520replications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10091v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Do%20Programming%20Students%20Use%20Generative%20AI%3F&entry.906535625=Christian%20Rahe%20and%20Walid%20Maalej&entry.1292438233=%20%20Programming%20students%20have%20a%20widespread%20access%20to%20powerful%20Generative%20AI%20tools%0Alike%20ChatGPT.%20While%20this%20can%20help%20understand%20the%20learning%20material%20and%20assist%0Awith%20exercises%2C%20educators%20are%20voicing%20more%20and%20more%20concerns%20about%20an%0Aoverreliance%20on%20generated%20outputs%20and%20lack%20of%20critical%20thinking%20skills.%20It%20is%0Athus%20important%20to%20understand%20how%20students%20actually%20use%20generative%20AI%20and%20what%0Aimpact%20this%20could%20have%20on%20their%20learning%20behavior.%20To%20this%20end%2C%20we%20conducted%20a%0Astudy%20including%20an%20exploratory%20experiment%20with%2037%20programming%20students%2C%20giving%0Athem%20monitored%20access%20to%20ChatGPT%20while%20solving%20a%20code%20authoring%20exercise.%20The%0Atask%20was%20not%20directly%20solvable%20by%20ChatGPT%20and%20required%20code%20comprehension%20and%0Areasoning.%20While%20only%2023%20of%20the%20students%20actually%20opted%20to%20use%20the%20chatbot%2C%20the%0Amajority%20of%20those%20eventually%20prompted%20it%20to%20simply%20generate%20a%20full%20solution.%20We%0Aobserved%20two%20prevalent%20usage%20strategies%3A%20to%20seek%20knowledge%20about%20general%0Aconcepts%20and%20to%20directly%20generate%20solutions.%20Instead%20of%20using%20the%20bot%20to%0Acomprehend%20the%20code%20and%20their%20own%20mistakes%2C%20students%20often%20got%20trapped%20in%20a%0Avicious%20cycle%20of%20submitting%20wrong%20generated%20code%20and%20then%20asking%20the%20bot%20for%20a%0Afix.%20Those%20who%20self-reported%20using%20generative%20AI%20regularly%20were%20more%20likely%20to%0Aprompt%20the%20bot%20to%20generate%20a%20solution.%20Our%20findings%20indicate%20that%20concerns%0Aabout%20potential%20decrease%20in%20programmers%27%20agency%20and%20productivity%20with%0AGenerative%20AI%20are%20justified.%20We%20discuss%20how%20researchers%20and%20educators%20can%0Arespond%20to%20the%20potential%20risk%20of%20students%20uncritically%20over-relying%20on%0AGenerative%20AI.%20We%20also%20discuss%20potential%20modifications%20to%20our%20study%20design%20for%0Alarge-scale%20replications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10091v2&entry.124074799=Read"},
{"title": "Fr\u00e9chet Cumulative Covariance Net for Deep Nonlinear Sufficient\n  Dimension Reduction with Random Objects", "author": "Hang Yuan and Christina Dan Wang and Zhou Yu", "abstract": "  Nonlinear sufficient dimension reduction\\citep{libing_generalSDR}, which\nconstructs nonlinear low-dimensional representations to summarize essential\nfeatures of high-dimensional data, is an important branch of representation\nlearning. However, most existing methods are not applicable when the response\nvariables are complex non-Euclidean random objects, which are frequently\nencountered in many recent statistical applications. In this paper, we\nintroduce a new statistical dependence measure termed Fr\\'echet Cumulative\nCovariance (FCCov) and develop a novel nonlinear SDR framework based on FCCov.\nOur approach is not only applicable to complex non-Euclidean data, but also\nexhibits robustness against outliers. We further incorporate Feedforward Neural\nNetworks (FNNs) and Convolutional Neural Networks (CNNs) to estimate nonlinear\nsufficient directions in the sample level. Theoretically, we prove that our\nmethod with squared Frobenius norm regularization achieves unbiasedness at the\n$\\sigma$-field level. Furthermore, we establish non-asymptotic convergence\nrates for our estimators based on FNNs and ResNet-type CNNs, which match the\nminimax rate of nonparametric regression up to logarithmic factors. Intensive\nsimulation studies verify the performance of our methods in both Euclidean and\nnon-Euclidean settings. We apply our method to facial expression recognition\ndatasets and the results underscore more realistic and broader applicability of\nour proposal.\n", "link": "http://arxiv.org/abs/2502.15374v1", "date": "2025-02-21", "relevancy": 2.0635, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5199}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5132}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fr%C3%A9chet%20Cumulative%20Covariance%20Net%20for%20Deep%20Nonlinear%20Sufficient%0A%20%20Dimension%20Reduction%20with%20Random%20Objects&body=Title%3A%20Fr%C3%A9chet%20Cumulative%20Covariance%20Net%20for%20Deep%20Nonlinear%20Sufficient%0A%20%20Dimension%20Reduction%20with%20Random%20Objects%0AAuthor%3A%20Hang%20Yuan%20and%20Christina%20Dan%20Wang%20and%20Zhou%20Yu%0AAbstract%3A%20%20%20Nonlinear%20sufficient%20dimension%20reduction%5Ccitep%7Blibing_generalSDR%7D%2C%20which%0Aconstructs%20nonlinear%20low-dimensional%20representations%20to%20summarize%20essential%0Afeatures%20of%20high-dimensional%20data%2C%20is%20an%20important%20branch%20of%20representation%0Alearning.%20However%2C%20most%20existing%20methods%20are%20not%20applicable%20when%20the%20response%0Avariables%20are%20complex%20non-Euclidean%20random%20objects%2C%20which%20are%20frequently%0Aencountered%20in%20many%20recent%20statistical%20applications.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20new%20statistical%20dependence%20measure%20termed%20Fr%5C%27echet%20Cumulative%0ACovariance%20%28FCCov%29%20and%20develop%20a%20novel%20nonlinear%20SDR%20framework%20based%20on%20FCCov.%0AOur%20approach%20is%20not%20only%20applicable%20to%20complex%20non-Euclidean%20data%2C%20but%20also%0Aexhibits%20robustness%20against%20outliers.%20We%20further%20incorporate%20Feedforward%20Neural%0ANetworks%20%28FNNs%29%20and%20Convolutional%20Neural%20Networks%20%28CNNs%29%20to%20estimate%20nonlinear%0Asufficient%20directions%20in%20the%20sample%20level.%20Theoretically%2C%20we%20prove%20that%20our%0Amethod%20with%20squared%20Frobenius%20norm%20regularization%20achieves%20unbiasedness%20at%20the%0A%24%5Csigma%24-field%20level.%20Furthermore%2C%20we%20establish%20non-asymptotic%20convergence%0Arates%20for%20our%20estimators%20based%20on%20FNNs%20and%20ResNet-type%20CNNs%2C%20which%20match%20the%0Aminimax%20rate%20of%20nonparametric%20regression%20up%20to%20logarithmic%20factors.%20Intensive%0Asimulation%20studies%20verify%20the%20performance%20of%20our%20methods%20in%20both%20Euclidean%20and%0Anon-Euclidean%20settings.%20We%20apply%20our%20method%20to%20facial%20expression%20recognition%0Adatasets%20and%20the%20results%20underscore%20more%20realistic%20and%20broader%20applicability%20of%0Aour%20proposal.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15374v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFr%25C3%25A9chet%2520Cumulative%2520Covariance%2520Net%2520for%2520Deep%2520Nonlinear%2520Sufficient%250A%2520%2520Dimension%2520Reduction%2520with%2520Random%2520Objects%26entry.906535625%3DHang%2520Yuan%2520and%2520Christina%2520Dan%2520Wang%2520and%2520Zhou%2520Yu%26entry.1292438233%3D%2520%2520Nonlinear%2520sufficient%2520dimension%2520reduction%255Ccitep%257Blibing_generalSDR%257D%252C%2520which%250Aconstructs%2520nonlinear%2520low-dimensional%2520representations%2520to%2520summarize%2520essential%250Afeatures%2520of%2520high-dimensional%2520data%252C%2520is%2520an%2520important%2520branch%2520of%2520representation%250Alearning.%2520However%252C%2520most%2520existing%2520methods%2520are%2520not%2520applicable%2520when%2520the%2520response%250Avariables%2520are%2520complex%2520non-Euclidean%2520random%2520objects%252C%2520which%2520are%2520frequently%250Aencountered%2520in%2520many%2520recent%2520statistical%2520applications.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520a%2520new%2520statistical%2520dependence%2520measure%2520termed%2520Fr%255C%2527echet%2520Cumulative%250ACovariance%2520%2528FCCov%2529%2520and%2520develop%2520a%2520novel%2520nonlinear%2520SDR%2520framework%2520based%2520on%2520FCCov.%250AOur%2520approach%2520is%2520not%2520only%2520applicable%2520to%2520complex%2520non-Euclidean%2520data%252C%2520but%2520also%250Aexhibits%2520robustness%2520against%2520outliers.%2520We%2520further%2520incorporate%2520Feedforward%2520Neural%250ANetworks%2520%2528FNNs%2529%2520and%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520to%2520estimate%2520nonlinear%250Asufficient%2520directions%2520in%2520the%2520sample%2520level.%2520Theoretically%252C%2520we%2520prove%2520that%2520our%250Amethod%2520with%2520squared%2520Frobenius%2520norm%2520regularization%2520achieves%2520unbiasedness%2520at%2520the%250A%2524%255Csigma%2524-field%2520level.%2520Furthermore%252C%2520we%2520establish%2520non-asymptotic%2520convergence%250Arates%2520for%2520our%2520estimators%2520based%2520on%2520FNNs%2520and%2520ResNet-type%2520CNNs%252C%2520which%2520match%2520the%250Aminimax%2520rate%2520of%2520nonparametric%2520regression%2520up%2520to%2520logarithmic%2520factors.%2520Intensive%250Asimulation%2520studies%2520verify%2520the%2520performance%2520of%2520our%2520methods%2520in%2520both%2520Euclidean%2520and%250Anon-Euclidean%2520settings.%2520We%2520apply%2520our%2520method%2520to%2520facial%2520expression%2520recognition%250Adatasets%2520and%2520the%2520results%2520underscore%2520more%2520realistic%2520and%2520broader%2520applicability%2520of%250Aour%2520proposal.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15374v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fr%C3%A9chet%20Cumulative%20Covariance%20Net%20for%20Deep%20Nonlinear%20Sufficient%0A%20%20Dimension%20Reduction%20with%20Random%20Objects&entry.906535625=Hang%20Yuan%20and%20Christina%20Dan%20Wang%20and%20Zhou%20Yu&entry.1292438233=%20%20Nonlinear%20sufficient%20dimension%20reduction%5Ccitep%7Blibing_generalSDR%7D%2C%20which%0Aconstructs%20nonlinear%20low-dimensional%20representations%20to%20summarize%20essential%0Afeatures%20of%20high-dimensional%20data%2C%20is%20an%20important%20branch%20of%20representation%0Alearning.%20However%2C%20most%20existing%20methods%20are%20not%20applicable%20when%20the%20response%0Avariables%20are%20complex%20non-Euclidean%20random%20objects%2C%20which%20are%20frequently%0Aencountered%20in%20many%20recent%20statistical%20applications.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20new%20statistical%20dependence%20measure%20termed%20Fr%5C%27echet%20Cumulative%0ACovariance%20%28FCCov%29%20and%20develop%20a%20novel%20nonlinear%20SDR%20framework%20based%20on%20FCCov.%0AOur%20approach%20is%20not%20only%20applicable%20to%20complex%20non-Euclidean%20data%2C%20but%20also%0Aexhibits%20robustness%20against%20outliers.%20We%20further%20incorporate%20Feedforward%20Neural%0ANetworks%20%28FNNs%29%20and%20Convolutional%20Neural%20Networks%20%28CNNs%29%20to%20estimate%20nonlinear%0Asufficient%20directions%20in%20the%20sample%20level.%20Theoretically%2C%20we%20prove%20that%20our%0Amethod%20with%20squared%20Frobenius%20norm%20regularization%20achieves%20unbiasedness%20at%20the%0A%24%5Csigma%24-field%20level.%20Furthermore%2C%20we%20establish%20non-asymptotic%20convergence%0Arates%20for%20our%20estimators%20based%20on%20FNNs%20and%20ResNet-type%20CNNs%2C%20which%20match%20the%0Aminimax%20rate%20of%20nonparametric%20regression%20up%20to%20logarithmic%20factors.%20Intensive%0Asimulation%20studies%20verify%20the%20performance%20of%20our%20methods%20in%20both%20Euclidean%20and%0Anon-Euclidean%20settings.%20We%20apply%20our%20method%20to%20facial%20expression%20recognition%0Adatasets%20and%20the%20results%20underscore%20more%20realistic%20and%20broader%20applicability%20of%0Aour%20proposal.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15374v1&entry.124074799=Read"},
{"title": "Generalization Bounds via Meta-Learned Model Representations: PAC-Bayes\n  and Sample Compression Hypernetworks", "author": "Benjamin Leblanc and Mathieu Bazinet and Nathaniel D'Amours and Alexandre Drouin and Pascal Germain", "abstract": "  PAC-Bayesian and Sample Compress learning frameworks are instrumental for\nderiving tight (non-vacuous) generalization bounds for neural networks. We\nleverage these results in a meta-learning scheme, relying on a hypernetwork\nthat outputs the parameters of a downstream predictor from a dataset input. The\noriginality of our approach lies in the investigated hypernetwork architectures\nthat encode the dataset before decoding the parameters: (1) a PAC-Bayesian\nencoder that expresses a posterior distribution over a latent space, (2) a\nSample Compress encoder that selects a small sample of the dataset input along\nwith a message from a discrete set, and (3) a hybrid between both approaches\nmotivated by a new Sample Compress theorem handling continuous messages. The\nlatter theorem exploits the pivotal information transiting at the\nencoder-decoder junction to compute generalization guarantees for each\ndownstream predictor obtained by our meta-learning scheme.\n", "link": "http://arxiv.org/abs/2410.13577v2", "date": "2025-02-21", "relevancy": 2.0566, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5244}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5074}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalization%20Bounds%20via%20Meta-Learned%20Model%20Representations%3A%20PAC-Bayes%0A%20%20and%20Sample%20Compression%20Hypernetworks&body=Title%3A%20Generalization%20Bounds%20via%20Meta-Learned%20Model%20Representations%3A%20PAC-Bayes%0A%20%20and%20Sample%20Compression%20Hypernetworks%0AAuthor%3A%20Benjamin%20Leblanc%20and%20Mathieu%20Bazinet%20and%20Nathaniel%20D%27Amours%20and%20Alexandre%20Drouin%20and%20Pascal%20Germain%0AAbstract%3A%20%20%20PAC-Bayesian%20and%20Sample%20Compress%20learning%20frameworks%20are%20instrumental%20for%0Aderiving%20tight%20%28non-vacuous%29%20generalization%20bounds%20for%20neural%20networks.%20We%0Aleverage%20these%20results%20in%20a%20meta-learning%20scheme%2C%20relying%20on%20a%20hypernetwork%0Athat%20outputs%20the%20parameters%20of%20a%20downstream%20predictor%20from%20a%20dataset%20input.%20The%0Aoriginality%20of%20our%20approach%20lies%20in%20the%20investigated%20hypernetwork%20architectures%0Athat%20encode%20the%20dataset%20before%20decoding%20the%20parameters%3A%20%281%29%20a%20PAC-Bayesian%0Aencoder%20that%20expresses%20a%20posterior%20distribution%20over%20a%20latent%20space%2C%20%282%29%20a%0ASample%20Compress%20encoder%20that%20selects%20a%20small%20sample%20of%20the%20dataset%20input%20along%0Awith%20a%20message%20from%20a%20discrete%20set%2C%20and%20%283%29%20a%20hybrid%20between%20both%20approaches%0Amotivated%20by%20a%20new%20Sample%20Compress%20theorem%20handling%20continuous%20messages.%20The%0Alatter%20theorem%20exploits%20the%20pivotal%20information%20transiting%20at%20the%0Aencoder-decoder%20junction%20to%20compute%20generalization%20guarantees%20for%20each%0Adownstream%20predictor%20obtained%20by%20our%20meta-learning%20scheme.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13577v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralization%2520Bounds%2520via%2520Meta-Learned%2520Model%2520Representations%253A%2520PAC-Bayes%250A%2520%2520and%2520Sample%2520Compression%2520Hypernetworks%26entry.906535625%3DBenjamin%2520Leblanc%2520and%2520Mathieu%2520Bazinet%2520and%2520Nathaniel%2520D%2527Amours%2520and%2520Alexandre%2520Drouin%2520and%2520Pascal%2520Germain%26entry.1292438233%3D%2520%2520PAC-Bayesian%2520and%2520Sample%2520Compress%2520learning%2520frameworks%2520are%2520instrumental%2520for%250Aderiving%2520tight%2520%2528non-vacuous%2529%2520generalization%2520bounds%2520for%2520neural%2520networks.%2520We%250Aleverage%2520these%2520results%2520in%2520a%2520meta-learning%2520scheme%252C%2520relying%2520on%2520a%2520hypernetwork%250Athat%2520outputs%2520the%2520parameters%2520of%2520a%2520downstream%2520predictor%2520from%2520a%2520dataset%2520input.%2520The%250Aoriginality%2520of%2520our%2520approach%2520lies%2520in%2520the%2520investigated%2520hypernetwork%2520architectures%250Athat%2520encode%2520the%2520dataset%2520before%2520decoding%2520the%2520parameters%253A%2520%25281%2529%2520a%2520PAC-Bayesian%250Aencoder%2520that%2520expresses%2520a%2520posterior%2520distribution%2520over%2520a%2520latent%2520space%252C%2520%25282%2529%2520a%250ASample%2520Compress%2520encoder%2520that%2520selects%2520a%2520small%2520sample%2520of%2520the%2520dataset%2520input%2520along%250Awith%2520a%2520message%2520from%2520a%2520discrete%2520set%252C%2520and%2520%25283%2529%2520a%2520hybrid%2520between%2520both%2520approaches%250Amotivated%2520by%2520a%2520new%2520Sample%2520Compress%2520theorem%2520handling%2520continuous%2520messages.%2520The%250Alatter%2520theorem%2520exploits%2520the%2520pivotal%2520information%2520transiting%2520at%2520the%250Aencoder-decoder%2520junction%2520to%2520compute%2520generalization%2520guarantees%2520for%2520each%250Adownstream%2520predictor%2520obtained%2520by%2520our%2520meta-learning%2520scheme.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13577v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalization%20Bounds%20via%20Meta-Learned%20Model%20Representations%3A%20PAC-Bayes%0A%20%20and%20Sample%20Compression%20Hypernetworks&entry.906535625=Benjamin%20Leblanc%20and%20Mathieu%20Bazinet%20and%20Nathaniel%20D%27Amours%20and%20Alexandre%20Drouin%20and%20Pascal%20Germain&entry.1292438233=%20%20PAC-Bayesian%20and%20Sample%20Compress%20learning%20frameworks%20are%20instrumental%20for%0Aderiving%20tight%20%28non-vacuous%29%20generalization%20bounds%20for%20neural%20networks.%20We%0Aleverage%20these%20results%20in%20a%20meta-learning%20scheme%2C%20relying%20on%20a%20hypernetwork%0Athat%20outputs%20the%20parameters%20of%20a%20downstream%20predictor%20from%20a%20dataset%20input.%20The%0Aoriginality%20of%20our%20approach%20lies%20in%20the%20investigated%20hypernetwork%20architectures%0Athat%20encode%20the%20dataset%20before%20decoding%20the%20parameters%3A%20%281%29%20a%20PAC-Bayesian%0Aencoder%20that%20expresses%20a%20posterior%20distribution%20over%20a%20latent%20space%2C%20%282%29%20a%0ASample%20Compress%20encoder%20that%20selects%20a%20small%20sample%20of%20the%20dataset%20input%20along%0Awith%20a%20message%20from%20a%20discrete%20set%2C%20and%20%283%29%20a%20hybrid%20between%20both%20approaches%0Amotivated%20by%20a%20new%20Sample%20Compress%20theorem%20handling%20continuous%20messages.%20The%0Alatter%20theorem%20exploits%20the%20pivotal%20information%20transiting%20at%20the%0Aencoder-decoder%20junction%20to%20compute%20generalization%20guarantees%20for%20each%0Adownstream%20predictor%20obtained%20by%20our%20meta-learning%20scheme.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13577v2&entry.124074799=Read"},
{"title": "Machine-generated text detection prevents language model collapse", "author": "George Drayson and Vasileios Lampos", "abstract": "  As Large Language Models (LLMs) become increasingly prevalent, their\ngenerated outputs are proliferating across the web, risking a future where\nmachine-generated content dilutes human-authored text. Since web data is the\nprimary resource for LLM pretraining, future models will be trained on an\nunknown portion of synthetic data. This will lead to model collapse, a\ndegenerative process which causes models to reinforce their own errors and\nexperience a drop in model performance. In this study, we investigate the\nimpact of decoding strategy on model collapse, where we analyse the\ncharacteristics of the generated data during recursive training, its similarity\nto human references and the resulting model performance. Using the decoding\nstrategies that lead to the most significant model degradation, we tackle the\nquestion: how to avoid model collapse when the origin (human or synthetic) of\nthe training data is unknown. We design a novel methodology based on resampling\nthe data distribution using importance weights from our machine-generated text\ndetector. Our method is validated on two LLM variants (GPT-2 and SmolLM2) on\nthe open-ended text generation task, demonstrating that we can successfully\nprevent model collapse and when there is enough human-authored data in the\ntraining dataset, our method improves model performance.\n", "link": "http://arxiv.org/abs/2502.15654v1", "date": "2025-02-21", "relevancy": 2.0556, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5535}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.514}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Machine-generated%20text%20detection%20prevents%20language%20model%20collapse&body=Title%3A%20Machine-generated%20text%20detection%20prevents%20language%20model%20collapse%0AAuthor%3A%20George%20Drayson%20and%20Vasileios%20Lampos%0AAbstract%3A%20%20%20As%20Large%20Language%20Models%20%28LLMs%29%20become%20increasingly%20prevalent%2C%20their%0Agenerated%20outputs%20are%20proliferating%20across%20the%20web%2C%20risking%20a%20future%20where%0Amachine-generated%20content%20dilutes%20human-authored%20text.%20Since%20web%20data%20is%20the%0Aprimary%20resource%20for%20LLM%20pretraining%2C%20future%20models%20will%20be%20trained%20on%20an%0Aunknown%20portion%20of%20synthetic%20data.%20This%20will%20lead%20to%20model%20collapse%2C%20a%0Adegenerative%20process%20which%20causes%20models%20to%20reinforce%20their%20own%20errors%20and%0Aexperience%20a%20drop%20in%20model%20performance.%20In%20this%20study%2C%20we%20investigate%20the%0Aimpact%20of%20decoding%20strategy%20on%20model%20collapse%2C%20where%20we%20analyse%20the%0Acharacteristics%20of%20the%20generated%20data%20during%20recursive%20training%2C%20its%20similarity%0Ato%20human%20references%20and%20the%20resulting%20model%20performance.%20Using%20the%20decoding%0Astrategies%20that%20lead%20to%20the%20most%20significant%20model%20degradation%2C%20we%20tackle%20the%0Aquestion%3A%20how%20to%20avoid%20model%20collapse%20when%20the%20origin%20%28human%20or%20synthetic%29%20of%0Athe%20training%20data%20is%20unknown.%20We%20design%20a%20novel%20methodology%20based%20on%20resampling%0Athe%20data%20distribution%20using%20importance%20weights%20from%20our%20machine-generated%20text%0Adetector.%20Our%20method%20is%20validated%20on%20two%20LLM%20variants%20%28GPT-2%20and%20SmolLM2%29%20on%0Athe%20open-ended%20text%20generation%20task%2C%20demonstrating%20that%20we%20can%20successfully%0Aprevent%20model%20collapse%20and%20when%20there%20is%20enough%20human-authored%20data%20in%20the%0Atraining%20dataset%2C%20our%20method%20improves%20model%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15654v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMachine-generated%2520text%2520detection%2520prevents%2520language%2520model%2520collapse%26entry.906535625%3DGeorge%2520Drayson%2520and%2520Vasileios%2520Lampos%26entry.1292438233%3D%2520%2520As%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520become%2520increasingly%2520prevalent%252C%2520their%250Agenerated%2520outputs%2520are%2520proliferating%2520across%2520the%2520web%252C%2520risking%2520a%2520future%2520where%250Amachine-generated%2520content%2520dilutes%2520human-authored%2520text.%2520Since%2520web%2520data%2520is%2520the%250Aprimary%2520resource%2520for%2520LLM%2520pretraining%252C%2520future%2520models%2520will%2520be%2520trained%2520on%2520an%250Aunknown%2520portion%2520of%2520synthetic%2520data.%2520This%2520will%2520lead%2520to%2520model%2520collapse%252C%2520a%250Adegenerative%2520process%2520which%2520causes%2520models%2520to%2520reinforce%2520their%2520own%2520errors%2520and%250Aexperience%2520a%2520drop%2520in%2520model%2520performance.%2520In%2520this%2520study%252C%2520we%2520investigate%2520the%250Aimpact%2520of%2520decoding%2520strategy%2520on%2520model%2520collapse%252C%2520where%2520we%2520analyse%2520the%250Acharacteristics%2520of%2520the%2520generated%2520data%2520during%2520recursive%2520training%252C%2520its%2520similarity%250Ato%2520human%2520references%2520and%2520the%2520resulting%2520model%2520performance.%2520Using%2520the%2520decoding%250Astrategies%2520that%2520lead%2520to%2520the%2520most%2520significant%2520model%2520degradation%252C%2520we%2520tackle%2520the%250Aquestion%253A%2520how%2520to%2520avoid%2520model%2520collapse%2520when%2520the%2520origin%2520%2528human%2520or%2520synthetic%2529%2520of%250Athe%2520training%2520data%2520is%2520unknown.%2520We%2520design%2520a%2520novel%2520methodology%2520based%2520on%2520resampling%250Athe%2520data%2520distribution%2520using%2520importance%2520weights%2520from%2520our%2520machine-generated%2520text%250Adetector.%2520Our%2520method%2520is%2520validated%2520on%2520two%2520LLM%2520variants%2520%2528GPT-2%2520and%2520SmolLM2%2529%2520on%250Athe%2520open-ended%2520text%2520generation%2520task%252C%2520demonstrating%2520that%2520we%2520can%2520successfully%250Aprevent%2520model%2520collapse%2520and%2520when%2520there%2520is%2520enough%2520human-authored%2520data%2520in%2520the%250Atraining%2520dataset%252C%2520our%2520method%2520improves%2520model%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15654v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine-generated%20text%20detection%20prevents%20language%20model%20collapse&entry.906535625=George%20Drayson%20and%20Vasileios%20Lampos&entry.1292438233=%20%20As%20Large%20Language%20Models%20%28LLMs%29%20become%20increasingly%20prevalent%2C%20their%0Agenerated%20outputs%20are%20proliferating%20across%20the%20web%2C%20risking%20a%20future%20where%0Amachine-generated%20content%20dilutes%20human-authored%20text.%20Since%20web%20data%20is%20the%0Aprimary%20resource%20for%20LLM%20pretraining%2C%20future%20models%20will%20be%20trained%20on%20an%0Aunknown%20portion%20of%20synthetic%20data.%20This%20will%20lead%20to%20model%20collapse%2C%20a%0Adegenerative%20process%20which%20causes%20models%20to%20reinforce%20their%20own%20errors%20and%0Aexperience%20a%20drop%20in%20model%20performance.%20In%20this%20study%2C%20we%20investigate%20the%0Aimpact%20of%20decoding%20strategy%20on%20model%20collapse%2C%20where%20we%20analyse%20the%0Acharacteristics%20of%20the%20generated%20data%20during%20recursive%20training%2C%20its%20similarity%0Ato%20human%20references%20and%20the%20resulting%20model%20performance.%20Using%20the%20decoding%0Astrategies%20that%20lead%20to%20the%20most%20significant%20model%20degradation%2C%20we%20tackle%20the%0Aquestion%3A%20how%20to%20avoid%20model%20collapse%20when%20the%20origin%20%28human%20or%20synthetic%29%20of%0Athe%20training%20data%20is%20unknown.%20We%20design%20a%20novel%20methodology%20based%20on%20resampling%0Athe%20data%20distribution%20using%20importance%20weights%20from%20our%20machine-generated%20text%0Adetector.%20Our%20method%20is%20validated%20on%20two%20LLM%20variants%20%28GPT-2%20and%20SmolLM2%29%20on%0Athe%20open-ended%20text%20generation%20task%2C%20demonstrating%20that%20we%20can%20successfully%0Aprevent%20model%20collapse%20and%20when%20there%20is%20enough%20human-authored%20data%20in%20the%0Atraining%20dataset%2C%20our%20method%20improves%20model%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15654v1&entry.124074799=Read"},
{"title": "Solving Inverse Problems with Deep Linear Neural Networks: Global\n  Convergence Guarantees for Gradient Descent with Weight Decay", "author": "Hannah Laus and Suzanna Parkinson and Vasileios Charisopoulos and Felix Krahmer and Rebecca Willett", "abstract": "  Machine learning methods are commonly used to solve inverse problems, wherein\nan unknown signal must be estimated from few measurements generated via a known\nacquisition procedure. In particular, neural networks perform well empirically\nbut have limited theoretical guarantees. In this work, we study an\nunderdetermined linear inverse problem that admits several possible solution\nmappings. A standard remedy (e.g., in compressed sensing) establishing\nuniqueness of the solution mapping is to assume knowledge of latent\nlow-dimensional structure in the source signal. We ask the following question:\ndo deep neural networks adapt to this low-dimensional structure when trained by\ngradient descent with weight decay regularization? We prove that mildly\noverparameterized deep linear networks trained in this manner converge to an\napproximate solution that accurately solves the inverse problem while\nimplicitly encoding latent subspace structure. To our knowledge, this is the\nfirst result to rigorously show that deep linear networks trained with weight\ndecay automatically adapt to latent subspace structure in the data under\npractical stepsize and weight initialization schemes. Our work highlights that\nregularization and overparameterization improve generalization, while\noverparameterization also accelerates convergence during training.\n", "link": "http://arxiv.org/abs/2502.15522v1", "date": "2025-02-21", "relevancy": 2.0548, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5167}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5153}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Solving%20Inverse%20Problems%20with%20Deep%20Linear%20Neural%20Networks%3A%20Global%0A%20%20Convergence%20Guarantees%20for%20Gradient%20Descent%20with%20Weight%20Decay&body=Title%3A%20Solving%20Inverse%20Problems%20with%20Deep%20Linear%20Neural%20Networks%3A%20Global%0A%20%20Convergence%20Guarantees%20for%20Gradient%20Descent%20with%20Weight%20Decay%0AAuthor%3A%20Hannah%20Laus%20and%20Suzanna%20Parkinson%20and%20Vasileios%20Charisopoulos%20and%20Felix%20Krahmer%20and%20Rebecca%20Willett%0AAbstract%3A%20%20%20Machine%20learning%20methods%20are%20commonly%20used%20to%20solve%20inverse%20problems%2C%20wherein%0Aan%20unknown%20signal%20must%20be%20estimated%20from%20few%20measurements%20generated%20via%20a%20known%0Aacquisition%20procedure.%20In%20particular%2C%20neural%20networks%20perform%20well%20empirically%0Abut%20have%20limited%20theoretical%20guarantees.%20In%20this%20work%2C%20we%20study%20an%0Aunderdetermined%20linear%20inverse%20problem%20that%20admits%20several%20possible%20solution%0Amappings.%20A%20standard%20remedy%20%28e.g.%2C%20in%20compressed%20sensing%29%20establishing%0Auniqueness%20of%20the%20solution%20mapping%20is%20to%20assume%20knowledge%20of%20latent%0Alow-dimensional%20structure%20in%20the%20source%20signal.%20We%20ask%20the%20following%20question%3A%0Ado%20deep%20neural%20networks%20adapt%20to%20this%20low-dimensional%20structure%20when%20trained%20by%0Agradient%20descent%20with%20weight%20decay%20regularization%3F%20We%20prove%20that%20mildly%0Aoverparameterized%20deep%20linear%20networks%20trained%20in%20this%20manner%20converge%20to%20an%0Aapproximate%20solution%20that%20accurately%20solves%20the%20inverse%20problem%20while%0Aimplicitly%20encoding%20latent%20subspace%20structure.%20To%20our%20knowledge%2C%20this%20is%20the%0Afirst%20result%20to%20rigorously%20show%20that%20deep%20linear%20networks%20trained%20with%20weight%0Adecay%20automatically%20adapt%20to%20latent%20subspace%20structure%20in%20the%20data%20under%0Apractical%20stepsize%20and%20weight%20initialization%20schemes.%20Our%20work%20highlights%20that%0Aregularization%20and%20overparameterization%20improve%20generalization%2C%20while%0Aoverparameterization%20also%20accelerates%20convergence%20during%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15522v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolving%2520Inverse%2520Problems%2520with%2520Deep%2520Linear%2520Neural%2520Networks%253A%2520Global%250A%2520%2520Convergence%2520Guarantees%2520for%2520Gradient%2520Descent%2520with%2520Weight%2520Decay%26entry.906535625%3DHannah%2520Laus%2520and%2520Suzanna%2520Parkinson%2520and%2520Vasileios%2520Charisopoulos%2520and%2520Felix%2520Krahmer%2520and%2520Rebecca%2520Willett%26entry.1292438233%3D%2520%2520Machine%2520learning%2520methods%2520are%2520commonly%2520used%2520to%2520solve%2520inverse%2520problems%252C%2520wherein%250Aan%2520unknown%2520signal%2520must%2520be%2520estimated%2520from%2520few%2520measurements%2520generated%2520via%2520a%2520known%250Aacquisition%2520procedure.%2520In%2520particular%252C%2520neural%2520networks%2520perform%2520well%2520empirically%250Abut%2520have%2520limited%2520theoretical%2520guarantees.%2520In%2520this%2520work%252C%2520we%2520study%2520an%250Aunderdetermined%2520linear%2520inverse%2520problem%2520that%2520admits%2520several%2520possible%2520solution%250Amappings.%2520A%2520standard%2520remedy%2520%2528e.g.%252C%2520in%2520compressed%2520sensing%2529%2520establishing%250Auniqueness%2520of%2520the%2520solution%2520mapping%2520is%2520to%2520assume%2520knowledge%2520of%2520latent%250Alow-dimensional%2520structure%2520in%2520the%2520source%2520signal.%2520We%2520ask%2520the%2520following%2520question%253A%250Ado%2520deep%2520neural%2520networks%2520adapt%2520to%2520this%2520low-dimensional%2520structure%2520when%2520trained%2520by%250Agradient%2520descent%2520with%2520weight%2520decay%2520regularization%253F%2520We%2520prove%2520that%2520mildly%250Aoverparameterized%2520deep%2520linear%2520networks%2520trained%2520in%2520this%2520manner%2520converge%2520to%2520an%250Aapproximate%2520solution%2520that%2520accurately%2520solves%2520the%2520inverse%2520problem%2520while%250Aimplicitly%2520encoding%2520latent%2520subspace%2520structure.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%250Afirst%2520result%2520to%2520rigorously%2520show%2520that%2520deep%2520linear%2520networks%2520trained%2520with%2520weight%250Adecay%2520automatically%2520adapt%2520to%2520latent%2520subspace%2520structure%2520in%2520the%2520data%2520under%250Apractical%2520stepsize%2520and%2520weight%2520initialization%2520schemes.%2520Our%2520work%2520highlights%2520that%250Aregularization%2520and%2520overparameterization%2520improve%2520generalization%252C%2520while%250Aoverparameterization%2520also%2520accelerates%2520convergence%2520during%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15522v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20Inverse%20Problems%20with%20Deep%20Linear%20Neural%20Networks%3A%20Global%0A%20%20Convergence%20Guarantees%20for%20Gradient%20Descent%20with%20Weight%20Decay&entry.906535625=Hannah%20Laus%20and%20Suzanna%20Parkinson%20and%20Vasileios%20Charisopoulos%20and%20Felix%20Krahmer%20and%20Rebecca%20Willett&entry.1292438233=%20%20Machine%20learning%20methods%20are%20commonly%20used%20to%20solve%20inverse%20problems%2C%20wherein%0Aan%20unknown%20signal%20must%20be%20estimated%20from%20few%20measurements%20generated%20via%20a%20known%0Aacquisition%20procedure.%20In%20particular%2C%20neural%20networks%20perform%20well%20empirically%0Abut%20have%20limited%20theoretical%20guarantees.%20In%20this%20work%2C%20we%20study%20an%0Aunderdetermined%20linear%20inverse%20problem%20that%20admits%20several%20possible%20solution%0Amappings.%20A%20standard%20remedy%20%28e.g.%2C%20in%20compressed%20sensing%29%20establishing%0Auniqueness%20of%20the%20solution%20mapping%20is%20to%20assume%20knowledge%20of%20latent%0Alow-dimensional%20structure%20in%20the%20source%20signal.%20We%20ask%20the%20following%20question%3A%0Ado%20deep%20neural%20networks%20adapt%20to%20this%20low-dimensional%20structure%20when%20trained%20by%0Agradient%20descent%20with%20weight%20decay%20regularization%3F%20We%20prove%20that%20mildly%0Aoverparameterized%20deep%20linear%20networks%20trained%20in%20this%20manner%20converge%20to%20an%0Aapproximate%20solution%20that%20accurately%20solves%20the%20inverse%20problem%20while%0Aimplicitly%20encoding%20latent%20subspace%20structure.%20To%20our%20knowledge%2C%20this%20is%20the%0Afirst%20result%20to%20rigorously%20show%20that%20deep%20linear%20networks%20trained%20with%20weight%0Adecay%20automatically%20adapt%20to%20latent%20subspace%20structure%20in%20the%20data%20under%0Apractical%20stepsize%20and%20weight%20initialization%20schemes.%20Our%20work%20highlights%20that%0Aregularization%20and%20overparameterization%20improve%20generalization%2C%20while%0Aoverparameterization%20also%20accelerates%20convergence%20during%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15522v1&entry.124074799=Read"},
{"title": "Adaptive Prompt: Unlocking the Power of Visual Prompt Tuning", "author": "Minh Le and Anh Nguyen and Huy Nguyen and Chau Nguyen and Nhat Ho", "abstract": "  Visual Prompt Tuning (VPT) has recently emerged as a powerful method for\nadapting pre-trained vision models to downstream tasks. By introducing\nlearnable prompt tokens as task-specific instructions, VPT effectively guides\npre-trained transformer models with minimal overhead. Despite its empirical\nsuccess, a comprehensive theoretical understanding of VPT remains an active\narea of research. Building on recent insights into the connection between\nmixture of experts and prompt-based approaches, we identify a key limitation in\nVPT: the restricted functional expressiveness in prompt formulation. To address\nthis limitation, we propose Visual Adaptive Prompt Tuning (VAPT), a new\ngeneration of prompts that redefines prompts as adaptive functions of the\ninput. Our theoretical analysis shows that this simple yet intuitive approach\nachieves optimal sample efficiency. Empirical results on VTAB-1K and FGVC\nfurther demonstrate VAPT's effectiveness, with performance gains of 7.34% and\n1.04% over fully fine-tuning baselines, respectively. Notably, VAPT also\nsurpasses VPT by a substantial margin while using fewer parameters. These\nresults highlight both the effectiveness and efficiency of our method and pave\nthe way for future research to explore the potential of adaptive prompts. Our\ncode is publicly available at https://github.com/Minhchuyentoancbn/VAPT\n", "link": "http://arxiv.org/abs/2501.18936v3", "date": "2025-02-21", "relevancy": 2.0504, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5245}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5106}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Prompt%3A%20Unlocking%20the%20Power%20of%20Visual%20Prompt%20Tuning&body=Title%3A%20Adaptive%20Prompt%3A%20Unlocking%20the%20Power%20of%20Visual%20Prompt%20Tuning%0AAuthor%3A%20Minh%20Le%20and%20Anh%20Nguyen%20and%20Huy%20Nguyen%20and%20Chau%20Nguyen%20and%20Nhat%20Ho%0AAbstract%3A%20%20%20Visual%20Prompt%20Tuning%20%28VPT%29%20has%20recently%20emerged%20as%20a%20powerful%20method%20for%0Aadapting%20pre-trained%20vision%20models%20to%20downstream%20tasks.%20By%20introducing%0Alearnable%20prompt%20tokens%20as%20task-specific%20instructions%2C%20VPT%20effectively%20guides%0Apre-trained%20transformer%20models%20with%20minimal%20overhead.%20Despite%20its%20empirical%0Asuccess%2C%20a%20comprehensive%20theoretical%20understanding%20of%20VPT%20remains%20an%20active%0Aarea%20of%20research.%20Building%20on%20recent%20insights%20into%20the%20connection%20between%0Amixture%20of%20experts%20and%20prompt-based%20approaches%2C%20we%20identify%20a%20key%20limitation%20in%0AVPT%3A%20the%20restricted%20functional%20expressiveness%20in%20prompt%20formulation.%20To%20address%0Athis%20limitation%2C%20we%20propose%20Visual%20Adaptive%20Prompt%20Tuning%20%28VAPT%29%2C%20a%20new%0Ageneration%20of%20prompts%20that%20redefines%20prompts%20as%20adaptive%20functions%20of%20the%0Ainput.%20Our%20theoretical%20analysis%20shows%20that%20this%20simple%20yet%20intuitive%20approach%0Aachieves%20optimal%20sample%20efficiency.%20Empirical%20results%20on%20VTAB-1K%20and%20FGVC%0Afurther%20demonstrate%20VAPT%27s%20effectiveness%2C%20with%20performance%20gains%20of%207.34%25%20and%0A1.04%25%20over%20fully%20fine-tuning%20baselines%2C%20respectively.%20Notably%2C%20VAPT%20also%0Asurpasses%20VPT%20by%20a%20substantial%20margin%20while%20using%20fewer%20parameters.%20These%0Aresults%20highlight%20both%20the%20effectiveness%20and%20efficiency%20of%20our%20method%20and%20pave%0Athe%20way%20for%20future%20research%20to%20explore%20the%20potential%20of%20adaptive%20prompts.%20Our%0Acode%20is%20publicly%20available%20at%20https%3A//github.com/Minhchuyentoancbn/VAPT%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.18936v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Prompt%253A%2520Unlocking%2520the%2520Power%2520of%2520Visual%2520Prompt%2520Tuning%26entry.906535625%3DMinh%2520Le%2520and%2520Anh%2520Nguyen%2520and%2520Huy%2520Nguyen%2520and%2520Chau%2520Nguyen%2520and%2520Nhat%2520Ho%26entry.1292438233%3D%2520%2520Visual%2520Prompt%2520Tuning%2520%2528VPT%2529%2520has%2520recently%2520emerged%2520as%2520a%2520powerful%2520method%2520for%250Aadapting%2520pre-trained%2520vision%2520models%2520to%2520downstream%2520tasks.%2520By%2520introducing%250Alearnable%2520prompt%2520tokens%2520as%2520task-specific%2520instructions%252C%2520VPT%2520effectively%2520guides%250Apre-trained%2520transformer%2520models%2520with%2520minimal%2520overhead.%2520Despite%2520its%2520empirical%250Asuccess%252C%2520a%2520comprehensive%2520theoretical%2520understanding%2520of%2520VPT%2520remains%2520an%2520active%250Aarea%2520of%2520research.%2520Building%2520on%2520recent%2520insights%2520into%2520the%2520connection%2520between%250Amixture%2520of%2520experts%2520and%2520prompt-based%2520approaches%252C%2520we%2520identify%2520a%2520key%2520limitation%2520in%250AVPT%253A%2520the%2520restricted%2520functional%2520expressiveness%2520in%2520prompt%2520formulation.%2520To%2520address%250Athis%2520limitation%252C%2520we%2520propose%2520Visual%2520Adaptive%2520Prompt%2520Tuning%2520%2528VAPT%2529%252C%2520a%2520new%250Ageneration%2520of%2520prompts%2520that%2520redefines%2520prompts%2520as%2520adaptive%2520functions%2520of%2520the%250Ainput.%2520Our%2520theoretical%2520analysis%2520shows%2520that%2520this%2520simple%2520yet%2520intuitive%2520approach%250Aachieves%2520optimal%2520sample%2520efficiency.%2520Empirical%2520results%2520on%2520VTAB-1K%2520and%2520FGVC%250Afurther%2520demonstrate%2520VAPT%2527s%2520effectiveness%252C%2520with%2520performance%2520gains%2520of%25207.34%2525%2520and%250A1.04%2525%2520over%2520fully%2520fine-tuning%2520baselines%252C%2520respectively.%2520Notably%252C%2520VAPT%2520also%250Asurpasses%2520VPT%2520by%2520a%2520substantial%2520margin%2520while%2520using%2520fewer%2520parameters.%2520These%250Aresults%2520highlight%2520both%2520the%2520effectiveness%2520and%2520efficiency%2520of%2520our%2520method%2520and%2520pave%250Athe%2520way%2520for%2520future%2520research%2520to%2520explore%2520the%2520potential%2520of%2520adaptive%2520prompts.%2520Our%250Acode%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/Minhchuyentoancbn/VAPT%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18936v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Prompt%3A%20Unlocking%20the%20Power%20of%20Visual%20Prompt%20Tuning&entry.906535625=Minh%20Le%20and%20Anh%20Nguyen%20and%20Huy%20Nguyen%20and%20Chau%20Nguyen%20and%20Nhat%20Ho&entry.1292438233=%20%20Visual%20Prompt%20Tuning%20%28VPT%29%20has%20recently%20emerged%20as%20a%20powerful%20method%20for%0Aadapting%20pre-trained%20vision%20models%20to%20downstream%20tasks.%20By%20introducing%0Alearnable%20prompt%20tokens%20as%20task-specific%20instructions%2C%20VPT%20effectively%20guides%0Apre-trained%20transformer%20models%20with%20minimal%20overhead.%20Despite%20its%20empirical%0Asuccess%2C%20a%20comprehensive%20theoretical%20understanding%20of%20VPT%20remains%20an%20active%0Aarea%20of%20research.%20Building%20on%20recent%20insights%20into%20the%20connection%20between%0Amixture%20of%20experts%20and%20prompt-based%20approaches%2C%20we%20identify%20a%20key%20limitation%20in%0AVPT%3A%20the%20restricted%20functional%20expressiveness%20in%20prompt%20formulation.%20To%20address%0Athis%20limitation%2C%20we%20propose%20Visual%20Adaptive%20Prompt%20Tuning%20%28VAPT%29%2C%20a%20new%0Ageneration%20of%20prompts%20that%20redefines%20prompts%20as%20adaptive%20functions%20of%20the%0Ainput.%20Our%20theoretical%20analysis%20shows%20that%20this%20simple%20yet%20intuitive%20approach%0Aachieves%20optimal%20sample%20efficiency.%20Empirical%20results%20on%20VTAB-1K%20and%20FGVC%0Afurther%20demonstrate%20VAPT%27s%20effectiveness%2C%20with%20performance%20gains%20of%207.34%25%20and%0A1.04%25%20over%20fully%20fine-tuning%20baselines%2C%20respectively.%20Notably%2C%20VAPT%20also%0Asurpasses%20VPT%20by%20a%20substantial%20margin%20while%20using%20fewer%20parameters.%20These%0Aresults%20highlight%20both%20the%20effectiveness%20and%20efficiency%20of%20our%20method%20and%20pave%0Athe%20way%20for%20future%20research%20to%20explore%20the%20potential%20of%20adaptive%20prompts.%20Our%0Acode%20is%20publicly%20available%20at%20https%3A//github.com/Minhchuyentoancbn/VAPT%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.18936v3&entry.124074799=Read"},
{"title": "Packet Inspection Transformer: A Self-Supervised Journey to Unseen\n  Malware Detection with Few Samples", "author": "Kyle Stein and Arash Mahyari and Guillermo Francia III and Eman El-Sheikh", "abstract": "  As networks continue to expand and become more interconnected, the need for\nnovel malware detection methods becomes more pronounced. Traditional security\nmeasures are increasingly inadequate against the sophistication of modern cyber\nattacks. Deep Packet Inspection (DPI) has been pivotal in enhancing network\nsecurity, offering an in-depth analysis of network traffic that surpasses\nconventional monitoring techniques. DPI not only examines the metadata of\nnetwork packets, but also dives into the actual content being carried within\nthe packet payloads, providing a comprehensive view of the data flowing through\nnetworks. While the integration of advanced deep learning techniques with DPI\nhas introduced modern methodologies into malware detection and network traffic\nclassification, state-of-the-art supervised learning approaches are limited by\ntheir reliance on large amounts of annotated data and their inability to\ngeneralize to novel, unseen malware threats. To address these limitations, this\npaper leverages the recent advancements in self-supervised learning (SSL) and\nfew-shot learning (FSL). Our proposed self-supervised approach trains a\ntransformer via SSL to learn the embedding of packet content, including\npayload, from vast amounts of unlabeled data by masking portions of packets,\nleading to a learned representation that generalizes to various downstream\ntasks. Once the representation is extracted from the packets, they are used to\ntrain a malware detection algorithm. The representation obtained from the\ntransformer is then used to adapt the malware detector to novel types of\nattacks using few-shot learning approaches. Our experimental results\ndemonstrate that our method achieves classification accuracies of up to 94.76%\non the UNSW-NB15 dataset and 83.25% on the CIC-IoT23 dataset.\n", "link": "http://arxiv.org/abs/2409.18219v2", "date": "2025-02-21", "relevancy": 2.0452, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5156}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5117}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Packet%20Inspection%20Transformer%3A%20A%20Self-Supervised%20Journey%20to%20Unseen%0A%20%20Malware%20Detection%20with%20Few%20Samples&body=Title%3A%20Packet%20Inspection%20Transformer%3A%20A%20Self-Supervised%20Journey%20to%20Unseen%0A%20%20Malware%20Detection%20with%20Few%20Samples%0AAuthor%3A%20Kyle%20Stein%20and%20Arash%20Mahyari%20and%20Guillermo%20Francia%20III%20and%20Eman%20El-Sheikh%0AAbstract%3A%20%20%20As%20networks%20continue%20to%20expand%20and%20become%20more%20interconnected%2C%20the%20need%20for%0Anovel%20malware%20detection%20methods%20becomes%20more%20pronounced.%20Traditional%20security%0Ameasures%20are%20increasingly%20inadequate%20against%20the%20sophistication%20of%20modern%20cyber%0Aattacks.%20Deep%20Packet%20Inspection%20%28DPI%29%20has%20been%20pivotal%20in%20enhancing%20network%0Asecurity%2C%20offering%20an%20in-depth%20analysis%20of%20network%20traffic%20that%20surpasses%0Aconventional%20monitoring%20techniques.%20DPI%20not%20only%20examines%20the%20metadata%20of%0Anetwork%20packets%2C%20but%20also%20dives%20into%20the%20actual%20content%20being%20carried%20within%0Athe%20packet%20payloads%2C%20providing%20a%20comprehensive%20view%20of%20the%20data%20flowing%20through%0Anetworks.%20While%20the%20integration%20of%20advanced%20deep%20learning%20techniques%20with%20DPI%0Ahas%20introduced%20modern%20methodologies%20into%20malware%20detection%20and%20network%20traffic%0Aclassification%2C%20state-of-the-art%20supervised%20learning%20approaches%20are%20limited%20by%0Atheir%20reliance%20on%20large%20amounts%20of%20annotated%20data%20and%20their%20inability%20to%0Ageneralize%20to%20novel%2C%20unseen%20malware%20threats.%20To%20address%20these%20limitations%2C%20this%0Apaper%20leverages%20the%20recent%20advancements%20in%20self-supervised%20learning%20%28SSL%29%20and%0Afew-shot%20learning%20%28FSL%29.%20Our%20proposed%20self-supervised%20approach%20trains%20a%0Atransformer%20via%20SSL%20to%20learn%20the%20embedding%20of%20packet%20content%2C%20including%0Apayload%2C%20from%20vast%20amounts%20of%20unlabeled%20data%20by%20masking%20portions%20of%20packets%2C%0Aleading%20to%20a%20learned%20representation%20that%20generalizes%20to%20various%20downstream%0Atasks.%20Once%20the%20representation%20is%20extracted%20from%20the%20packets%2C%20they%20are%20used%20to%0Atrain%20a%20malware%20detection%20algorithm.%20The%20representation%20obtained%20from%20the%0Atransformer%20is%20then%20used%20to%20adapt%20the%20malware%20detector%20to%20novel%20types%20of%0Aattacks%20using%20few-shot%20learning%20approaches.%20Our%20experimental%20results%0Ademonstrate%20that%20our%20method%20achieves%20classification%20accuracies%20of%20up%20to%2094.76%25%0Aon%20the%20UNSW-NB15%20dataset%20and%2083.25%25%20on%20the%20CIC-IoT23%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18219v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPacket%2520Inspection%2520Transformer%253A%2520A%2520Self-Supervised%2520Journey%2520to%2520Unseen%250A%2520%2520Malware%2520Detection%2520with%2520Few%2520Samples%26entry.906535625%3DKyle%2520Stein%2520and%2520Arash%2520Mahyari%2520and%2520Guillermo%2520Francia%2520III%2520and%2520Eman%2520El-Sheikh%26entry.1292438233%3D%2520%2520As%2520networks%2520continue%2520to%2520expand%2520and%2520become%2520more%2520interconnected%252C%2520the%2520need%2520for%250Anovel%2520malware%2520detection%2520methods%2520becomes%2520more%2520pronounced.%2520Traditional%2520security%250Ameasures%2520are%2520increasingly%2520inadequate%2520against%2520the%2520sophistication%2520of%2520modern%2520cyber%250Aattacks.%2520Deep%2520Packet%2520Inspection%2520%2528DPI%2529%2520has%2520been%2520pivotal%2520in%2520enhancing%2520network%250Asecurity%252C%2520offering%2520an%2520in-depth%2520analysis%2520of%2520network%2520traffic%2520that%2520surpasses%250Aconventional%2520monitoring%2520techniques.%2520DPI%2520not%2520only%2520examines%2520the%2520metadata%2520of%250Anetwork%2520packets%252C%2520but%2520also%2520dives%2520into%2520the%2520actual%2520content%2520being%2520carried%2520within%250Athe%2520packet%2520payloads%252C%2520providing%2520a%2520comprehensive%2520view%2520of%2520the%2520data%2520flowing%2520through%250Anetworks.%2520While%2520the%2520integration%2520of%2520advanced%2520deep%2520learning%2520techniques%2520with%2520DPI%250Ahas%2520introduced%2520modern%2520methodologies%2520into%2520malware%2520detection%2520and%2520network%2520traffic%250Aclassification%252C%2520state-of-the-art%2520supervised%2520learning%2520approaches%2520are%2520limited%2520by%250Atheir%2520reliance%2520on%2520large%2520amounts%2520of%2520annotated%2520data%2520and%2520their%2520inability%2520to%250Ageneralize%2520to%2520novel%252C%2520unseen%2520malware%2520threats.%2520To%2520address%2520these%2520limitations%252C%2520this%250Apaper%2520leverages%2520the%2520recent%2520advancements%2520in%2520self-supervised%2520learning%2520%2528SSL%2529%2520and%250Afew-shot%2520learning%2520%2528FSL%2529.%2520Our%2520proposed%2520self-supervised%2520approach%2520trains%2520a%250Atransformer%2520via%2520SSL%2520to%2520learn%2520the%2520embedding%2520of%2520packet%2520content%252C%2520including%250Apayload%252C%2520from%2520vast%2520amounts%2520of%2520unlabeled%2520data%2520by%2520masking%2520portions%2520of%2520packets%252C%250Aleading%2520to%2520a%2520learned%2520representation%2520that%2520generalizes%2520to%2520various%2520downstream%250Atasks.%2520Once%2520the%2520representation%2520is%2520extracted%2520from%2520the%2520packets%252C%2520they%2520are%2520used%2520to%250Atrain%2520a%2520malware%2520detection%2520algorithm.%2520The%2520representation%2520obtained%2520from%2520the%250Atransformer%2520is%2520then%2520used%2520to%2520adapt%2520the%2520malware%2520detector%2520to%2520novel%2520types%2520of%250Aattacks%2520using%2520few-shot%2520learning%2520approaches.%2520Our%2520experimental%2520results%250Ademonstrate%2520that%2520our%2520method%2520achieves%2520classification%2520accuracies%2520of%2520up%2520to%252094.76%2525%250Aon%2520the%2520UNSW-NB15%2520dataset%2520and%252083.25%2525%2520on%2520the%2520CIC-IoT23%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18219v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Packet%20Inspection%20Transformer%3A%20A%20Self-Supervised%20Journey%20to%20Unseen%0A%20%20Malware%20Detection%20with%20Few%20Samples&entry.906535625=Kyle%20Stein%20and%20Arash%20Mahyari%20and%20Guillermo%20Francia%20III%20and%20Eman%20El-Sheikh&entry.1292438233=%20%20As%20networks%20continue%20to%20expand%20and%20become%20more%20interconnected%2C%20the%20need%20for%0Anovel%20malware%20detection%20methods%20becomes%20more%20pronounced.%20Traditional%20security%0Ameasures%20are%20increasingly%20inadequate%20against%20the%20sophistication%20of%20modern%20cyber%0Aattacks.%20Deep%20Packet%20Inspection%20%28DPI%29%20has%20been%20pivotal%20in%20enhancing%20network%0Asecurity%2C%20offering%20an%20in-depth%20analysis%20of%20network%20traffic%20that%20surpasses%0Aconventional%20monitoring%20techniques.%20DPI%20not%20only%20examines%20the%20metadata%20of%0Anetwork%20packets%2C%20but%20also%20dives%20into%20the%20actual%20content%20being%20carried%20within%0Athe%20packet%20payloads%2C%20providing%20a%20comprehensive%20view%20of%20the%20data%20flowing%20through%0Anetworks.%20While%20the%20integration%20of%20advanced%20deep%20learning%20techniques%20with%20DPI%0Ahas%20introduced%20modern%20methodologies%20into%20malware%20detection%20and%20network%20traffic%0Aclassification%2C%20state-of-the-art%20supervised%20learning%20approaches%20are%20limited%20by%0Atheir%20reliance%20on%20large%20amounts%20of%20annotated%20data%20and%20their%20inability%20to%0Ageneralize%20to%20novel%2C%20unseen%20malware%20threats.%20To%20address%20these%20limitations%2C%20this%0Apaper%20leverages%20the%20recent%20advancements%20in%20self-supervised%20learning%20%28SSL%29%20and%0Afew-shot%20learning%20%28FSL%29.%20Our%20proposed%20self-supervised%20approach%20trains%20a%0Atransformer%20via%20SSL%20to%20learn%20the%20embedding%20of%20packet%20content%2C%20including%0Apayload%2C%20from%20vast%20amounts%20of%20unlabeled%20data%20by%20masking%20portions%20of%20packets%2C%0Aleading%20to%20a%20learned%20representation%20that%20generalizes%20to%20various%20downstream%0Atasks.%20Once%20the%20representation%20is%20extracted%20from%20the%20packets%2C%20they%20are%20used%20to%0Atrain%20a%20malware%20detection%20algorithm.%20The%20representation%20obtained%20from%20the%0Atransformer%20is%20then%20used%20to%20adapt%20the%20malware%20detector%20to%20novel%20types%20of%0Aattacks%20using%20few-shot%20learning%20approaches.%20Our%20experimental%20results%0Ademonstrate%20that%20our%20method%20achieves%20classification%20accuracies%20of%20up%20to%2094.76%25%0Aon%20the%20UNSW-NB15%20dataset%20and%2083.25%25%20on%20the%20CIC-IoT23%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18219v2&entry.124074799=Read"},
{"title": "TAG: A Decentralized Framework for Multi-Agent Hierarchical\n  Reinforcement Learning", "author": "Giuseppe Paolo and Abdelhakim Benechehab and Hamza Cherkaoui and Albert Thomas and Bal\u00e1zs K\u00e9gl", "abstract": "  Hierarchical organization is fundamental to biological systems and human\nsocieties, yet artificial intelligence systems often rely on monolithic\narchitectures that limit adaptability and scalability. Current hierarchical\nreinforcement learning (HRL) approaches typically restrict hierarchies to two\nlevels or require centralized training, which limits their practical\napplicability. We introduce TAME Agent Framework (TAG), a framework for\nconstructing fully decentralized hierarchical multi-agent systems.TAG enables\nhierarchies of arbitrary depth through a novel LevelEnv concept, which\nabstracts each hierarchy level as the environment for the agents above it. This\napproach standardizes information flow between levels while preserving loose\ncoupling, allowing for seamless integration of diverse agent types. We\ndemonstrate the effectiveness of TAG by implementing hierarchical architectures\nthat combine different RL agents across multiple levels, achieving improved\nperformance over classical multi-agent RL baselines on standard benchmarks. Our\nresults show that decentralized hierarchical organization enhances both\nlearning speed and final performance, positioning TAG as a promising direction\nfor scalable multi-agent systems.\n", "link": "http://arxiv.org/abs/2502.15425v1", "date": "2025-02-21", "relevancy": 2.0307, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.524}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4974}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TAG%3A%20A%20Decentralized%20Framework%20for%20Multi-Agent%20Hierarchical%0A%20%20Reinforcement%20Learning&body=Title%3A%20TAG%3A%20A%20Decentralized%20Framework%20for%20Multi-Agent%20Hierarchical%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Giuseppe%20Paolo%20and%20Abdelhakim%20Benechehab%20and%20Hamza%20Cherkaoui%20and%20Albert%20Thomas%20and%20Bal%C3%A1zs%20K%C3%A9gl%0AAbstract%3A%20%20%20Hierarchical%20organization%20is%20fundamental%20to%20biological%20systems%20and%20human%0Asocieties%2C%20yet%20artificial%20intelligence%20systems%20often%20rely%20on%20monolithic%0Aarchitectures%20that%20limit%20adaptability%20and%20scalability.%20Current%20hierarchical%0Areinforcement%20learning%20%28HRL%29%20approaches%20typically%20restrict%20hierarchies%20to%20two%0Alevels%20or%20require%20centralized%20training%2C%20which%20limits%20their%20practical%0Aapplicability.%20We%20introduce%20TAME%20Agent%20Framework%20%28TAG%29%2C%20a%20framework%20for%0Aconstructing%20fully%20decentralized%20hierarchical%20multi-agent%20systems.TAG%20enables%0Ahierarchies%20of%20arbitrary%20depth%20through%20a%20novel%20LevelEnv%20concept%2C%20which%0Aabstracts%20each%20hierarchy%20level%20as%20the%20environment%20for%20the%20agents%20above%20it.%20This%0Aapproach%20standardizes%20information%20flow%20between%20levels%20while%20preserving%20loose%0Acoupling%2C%20allowing%20for%20seamless%20integration%20of%20diverse%20agent%20types.%20We%0Ademonstrate%20the%20effectiveness%20of%20TAG%20by%20implementing%20hierarchical%20architectures%0Athat%20combine%20different%20RL%20agents%20across%20multiple%20levels%2C%20achieving%20improved%0Aperformance%20over%20classical%20multi-agent%20RL%20baselines%20on%20standard%20benchmarks.%20Our%0Aresults%20show%20that%20decentralized%20hierarchical%20organization%20enhances%20both%0Alearning%20speed%20and%20final%20performance%2C%20positioning%20TAG%20as%20a%20promising%20direction%0Afor%20scalable%20multi-agent%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15425v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTAG%253A%2520A%2520Decentralized%2520Framework%2520for%2520Multi-Agent%2520Hierarchical%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DGiuseppe%2520Paolo%2520and%2520Abdelhakim%2520Benechehab%2520and%2520Hamza%2520Cherkaoui%2520and%2520Albert%2520Thomas%2520and%2520Bal%25C3%25A1zs%2520K%25C3%25A9gl%26entry.1292438233%3D%2520%2520Hierarchical%2520organization%2520is%2520fundamental%2520to%2520biological%2520systems%2520and%2520human%250Asocieties%252C%2520yet%2520artificial%2520intelligence%2520systems%2520often%2520rely%2520on%2520monolithic%250Aarchitectures%2520that%2520limit%2520adaptability%2520and%2520scalability.%2520Current%2520hierarchical%250Areinforcement%2520learning%2520%2528HRL%2529%2520approaches%2520typically%2520restrict%2520hierarchies%2520to%2520two%250Alevels%2520or%2520require%2520centralized%2520training%252C%2520which%2520limits%2520their%2520practical%250Aapplicability.%2520We%2520introduce%2520TAME%2520Agent%2520Framework%2520%2528TAG%2529%252C%2520a%2520framework%2520for%250Aconstructing%2520fully%2520decentralized%2520hierarchical%2520multi-agent%2520systems.TAG%2520enables%250Ahierarchies%2520of%2520arbitrary%2520depth%2520through%2520a%2520novel%2520LevelEnv%2520concept%252C%2520which%250Aabstracts%2520each%2520hierarchy%2520level%2520as%2520the%2520environment%2520for%2520the%2520agents%2520above%2520it.%2520This%250Aapproach%2520standardizes%2520information%2520flow%2520between%2520levels%2520while%2520preserving%2520loose%250Acoupling%252C%2520allowing%2520for%2520seamless%2520integration%2520of%2520diverse%2520agent%2520types.%2520We%250Ademonstrate%2520the%2520effectiveness%2520of%2520TAG%2520by%2520implementing%2520hierarchical%2520architectures%250Athat%2520combine%2520different%2520RL%2520agents%2520across%2520multiple%2520levels%252C%2520achieving%2520improved%250Aperformance%2520over%2520classical%2520multi-agent%2520RL%2520baselines%2520on%2520standard%2520benchmarks.%2520Our%250Aresults%2520show%2520that%2520decentralized%2520hierarchical%2520organization%2520enhances%2520both%250Alearning%2520speed%2520and%2520final%2520performance%252C%2520positioning%2520TAG%2520as%2520a%2520promising%2520direction%250Afor%2520scalable%2520multi-agent%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15425v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TAG%3A%20A%20Decentralized%20Framework%20for%20Multi-Agent%20Hierarchical%0A%20%20Reinforcement%20Learning&entry.906535625=Giuseppe%20Paolo%20and%20Abdelhakim%20Benechehab%20and%20Hamza%20Cherkaoui%20and%20Albert%20Thomas%20and%20Bal%C3%A1zs%20K%C3%A9gl&entry.1292438233=%20%20Hierarchical%20organization%20is%20fundamental%20to%20biological%20systems%20and%20human%0Asocieties%2C%20yet%20artificial%20intelligence%20systems%20often%20rely%20on%20monolithic%0Aarchitectures%20that%20limit%20adaptability%20and%20scalability.%20Current%20hierarchical%0Areinforcement%20learning%20%28HRL%29%20approaches%20typically%20restrict%20hierarchies%20to%20two%0Alevels%20or%20require%20centralized%20training%2C%20which%20limits%20their%20practical%0Aapplicability.%20We%20introduce%20TAME%20Agent%20Framework%20%28TAG%29%2C%20a%20framework%20for%0Aconstructing%20fully%20decentralized%20hierarchical%20multi-agent%20systems.TAG%20enables%0Ahierarchies%20of%20arbitrary%20depth%20through%20a%20novel%20LevelEnv%20concept%2C%20which%0Aabstracts%20each%20hierarchy%20level%20as%20the%20environment%20for%20the%20agents%20above%20it.%20This%0Aapproach%20standardizes%20information%20flow%20between%20levels%20while%20preserving%20loose%0Acoupling%2C%20allowing%20for%20seamless%20integration%20of%20diverse%20agent%20types.%20We%0Ademonstrate%20the%20effectiveness%20of%20TAG%20by%20implementing%20hierarchical%20architectures%0Athat%20combine%20different%20RL%20agents%20across%20multiple%20levels%2C%20achieving%20improved%0Aperformance%20over%20classical%20multi-agent%20RL%20baselines%20on%20standard%20benchmarks.%20Our%0Aresults%20show%20that%20decentralized%20hierarchical%20organization%20enhances%20both%0Alearning%20speed%20and%20final%20performance%2C%20positioning%20TAG%20as%20a%20promising%20direction%0Afor%20scalable%20multi-agent%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15425v1&entry.124074799=Read"},
{"title": "Identifying Features that Shape Perceived Consciousness in Large\n  Language Model-based AI: A Quantitative Study of Human Responses", "author": "Kang Bongsu and Kim Jundong and Yun Tae-Rim and Bae Hyojin and Kim Chang-Eop", "abstract": "  This study quantitively examines which features of AI-generated text lead\nhumans to perceive subjective consciousness in large language model (LLM)-based\nAI systems. Drawing on 99 passages from conversations with Claude 3 Opus and\nfocusing on eight features -- metacognitive self-reflection, logical reasoning,\nempathy, emotionality, knowledge, fluency, unexpectedness, and subjective\nexpressiveness -- we conducted a survey with 123 participants. Using regression\nand clustering analyses, we investigated how these features influence\nparticipants' perceptions of AI consciousness. The results reveal that\nmetacognitive self-reflection and the AI's expression of its own emotions\nsignificantly increased perceived consciousness, while a heavy emphasis on\nknowledge reduced it. Participants clustered into seven subgroups, each showing\ndistinct feature-weighting patterns. Additionally, higher prior knowledge of\nLLMs and more frequent usage of LLM-based chatbots were associated with greater\noverall likelihood assessments of AI consciousness. This study underscores the\nmultidimensional and individualized nature of perceived AI consciousness and\nprovides a foundation for better understanding the psychosocial implications of\nhuman-AI interaction.\n", "link": "http://arxiv.org/abs/2502.15365v1", "date": "2025-02-21", "relevancy": 2.026, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5159}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5159}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identifying%20Features%20that%20Shape%20Perceived%20Consciousness%20in%20Large%0A%20%20Language%20Model-based%20AI%3A%20A%20Quantitative%20Study%20of%20Human%20Responses&body=Title%3A%20Identifying%20Features%20that%20Shape%20Perceived%20Consciousness%20in%20Large%0A%20%20Language%20Model-based%20AI%3A%20A%20Quantitative%20Study%20of%20Human%20Responses%0AAuthor%3A%20Kang%20Bongsu%20and%20Kim%20Jundong%20and%20Yun%20Tae-Rim%20and%20Bae%20Hyojin%20and%20Kim%20Chang-Eop%0AAbstract%3A%20%20%20This%20study%20quantitively%20examines%20which%20features%20of%20AI-generated%20text%20lead%0Ahumans%20to%20perceive%20subjective%20consciousness%20in%20large%20language%20model%20%28LLM%29-based%0AAI%20systems.%20Drawing%20on%2099%20passages%20from%20conversations%20with%20Claude%203%20Opus%20and%0Afocusing%20on%20eight%20features%20--%20metacognitive%20self-reflection%2C%20logical%20reasoning%2C%0Aempathy%2C%20emotionality%2C%20knowledge%2C%20fluency%2C%20unexpectedness%2C%20and%20subjective%0Aexpressiveness%20--%20we%20conducted%20a%20survey%20with%20123%20participants.%20Using%20regression%0Aand%20clustering%20analyses%2C%20we%20investigated%20how%20these%20features%20influence%0Aparticipants%27%20perceptions%20of%20AI%20consciousness.%20The%20results%20reveal%20that%0Ametacognitive%20self-reflection%20and%20the%20AI%27s%20expression%20of%20its%20own%20emotions%0Asignificantly%20increased%20perceived%20consciousness%2C%20while%20a%20heavy%20emphasis%20on%0Aknowledge%20reduced%20it.%20Participants%20clustered%20into%20seven%20subgroups%2C%20each%20showing%0Adistinct%20feature-weighting%20patterns.%20Additionally%2C%20higher%20prior%20knowledge%20of%0ALLMs%20and%20more%20frequent%20usage%20of%20LLM-based%20chatbots%20were%20associated%20with%20greater%0Aoverall%20likelihood%20assessments%20of%20AI%20consciousness.%20This%20study%20underscores%20the%0Amultidimensional%20and%20individualized%20nature%20of%20perceived%20AI%20consciousness%20and%0Aprovides%20a%20foundation%20for%20better%20understanding%20the%20psychosocial%20implications%20of%0Ahuman-AI%20interaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15365v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentifying%2520Features%2520that%2520Shape%2520Perceived%2520Consciousness%2520in%2520Large%250A%2520%2520Language%2520Model-based%2520AI%253A%2520A%2520Quantitative%2520Study%2520of%2520Human%2520Responses%26entry.906535625%3DKang%2520Bongsu%2520and%2520Kim%2520Jundong%2520and%2520Yun%2520Tae-Rim%2520and%2520Bae%2520Hyojin%2520and%2520Kim%2520Chang-Eop%26entry.1292438233%3D%2520%2520This%2520study%2520quantitively%2520examines%2520which%2520features%2520of%2520AI-generated%2520text%2520lead%250Ahumans%2520to%2520perceive%2520subjective%2520consciousness%2520in%2520large%2520language%2520model%2520%2528LLM%2529-based%250AAI%2520systems.%2520Drawing%2520on%252099%2520passages%2520from%2520conversations%2520with%2520Claude%25203%2520Opus%2520and%250Afocusing%2520on%2520eight%2520features%2520--%2520metacognitive%2520self-reflection%252C%2520logical%2520reasoning%252C%250Aempathy%252C%2520emotionality%252C%2520knowledge%252C%2520fluency%252C%2520unexpectedness%252C%2520and%2520subjective%250Aexpressiveness%2520--%2520we%2520conducted%2520a%2520survey%2520with%2520123%2520participants.%2520Using%2520regression%250Aand%2520clustering%2520analyses%252C%2520we%2520investigated%2520how%2520these%2520features%2520influence%250Aparticipants%2527%2520perceptions%2520of%2520AI%2520consciousness.%2520The%2520results%2520reveal%2520that%250Ametacognitive%2520self-reflection%2520and%2520the%2520AI%2527s%2520expression%2520of%2520its%2520own%2520emotions%250Asignificantly%2520increased%2520perceived%2520consciousness%252C%2520while%2520a%2520heavy%2520emphasis%2520on%250Aknowledge%2520reduced%2520it.%2520Participants%2520clustered%2520into%2520seven%2520subgroups%252C%2520each%2520showing%250Adistinct%2520feature-weighting%2520patterns.%2520Additionally%252C%2520higher%2520prior%2520knowledge%2520of%250ALLMs%2520and%2520more%2520frequent%2520usage%2520of%2520LLM-based%2520chatbots%2520were%2520associated%2520with%2520greater%250Aoverall%2520likelihood%2520assessments%2520of%2520AI%2520consciousness.%2520This%2520study%2520underscores%2520the%250Amultidimensional%2520and%2520individualized%2520nature%2520of%2520perceived%2520AI%2520consciousness%2520and%250Aprovides%2520a%2520foundation%2520for%2520better%2520understanding%2520the%2520psychosocial%2520implications%2520of%250Ahuman-AI%2520interaction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15365v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifying%20Features%20that%20Shape%20Perceived%20Consciousness%20in%20Large%0A%20%20Language%20Model-based%20AI%3A%20A%20Quantitative%20Study%20of%20Human%20Responses&entry.906535625=Kang%20Bongsu%20and%20Kim%20Jundong%20and%20Yun%20Tae-Rim%20and%20Bae%20Hyojin%20and%20Kim%20Chang-Eop&entry.1292438233=%20%20This%20study%20quantitively%20examines%20which%20features%20of%20AI-generated%20text%20lead%0Ahumans%20to%20perceive%20subjective%20consciousness%20in%20large%20language%20model%20%28LLM%29-based%0AAI%20systems.%20Drawing%20on%2099%20passages%20from%20conversations%20with%20Claude%203%20Opus%20and%0Afocusing%20on%20eight%20features%20--%20metacognitive%20self-reflection%2C%20logical%20reasoning%2C%0Aempathy%2C%20emotionality%2C%20knowledge%2C%20fluency%2C%20unexpectedness%2C%20and%20subjective%0Aexpressiveness%20--%20we%20conducted%20a%20survey%20with%20123%20participants.%20Using%20regression%0Aand%20clustering%20analyses%2C%20we%20investigated%20how%20these%20features%20influence%0Aparticipants%27%20perceptions%20of%20AI%20consciousness.%20The%20results%20reveal%20that%0Ametacognitive%20self-reflection%20and%20the%20AI%27s%20expression%20of%20its%20own%20emotions%0Asignificantly%20increased%20perceived%20consciousness%2C%20while%20a%20heavy%20emphasis%20on%0Aknowledge%20reduced%20it.%20Participants%20clustered%20into%20seven%20subgroups%2C%20each%20showing%0Adistinct%20feature-weighting%20patterns.%20Additionally%2C%20higher%20prior%20knowledge%20of%0ALLMs%20and%20more%20frequent%20usage%20of%20LLM-based%20chatbots%20were%20associated%20with%20greater%0Aoverall%20likelihood%20assessments%20of%20AI%20consciousness.%20This%20study%20underscores%20the%0Amultidimensional%20and%20individualized%20nature%20of%20perceived%20AI%20consciousness%20and%0Aprovides%20a%20foundation%20for%20better%20understanding%20the%20psychosocial%20implications%20of%0Ahuman-AI%20interaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15365v1&entry.124074799=Read"},
{"title": "Model Lakes", "author": "Koyena Pal and David Bau and Ren\u00e9e J. Miller", "abstract": "  Given a set of deep learning models, it can be hard to find models\nappropriate to a task, understand the models, and characterize how models are\ndifferent one from another. Currently, practitioners rely on manually-written\ndocumentation to understand and choose models. However, not all models have\ncomplete and reliable documentation. As the number of models increases, the\nchallenges of finding, differentiating, and understanding models become\nincreasingly crucial. Inspired from research on data lakes, we introduce the\nconcept of model lakes. We formalize key model lake tasks, including model\nattribution, versioning, search, and benchmarking, and discuss fundamental\nresearch challenges in the management of large models. We also explore what\ndata management techniques can be brought to bear on the study of large model\nmanagement.\n", "link": "http://arxiv.org/abs/2403.02327v2", "date": "2025-02-21", "relevancy": 2.0255, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5168}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5168}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model%20Lakes&body=Title%3A%20Model%20Lakes%0AAuthor%3A%20Koyena%20Pal%20and%20David%20Bau%20and%20Ren%C3%A9e%20J.%20Miller%0AAbstract%3A%20%20%20Given%20a%20set%20of%20deep%20learning%20models%2C%20it%20can%20be%20hard%20to%20find%20models%0Aappropriate%20to%20a%20task%2C%20understand%20the%20models%2C%20and%20characterize%20how%20models%20are%0Adifferent%20one%20from%20another.%20Currently%2C%20practitioners%20rely%20on%20manually-written%0Adocumentation%20to%20understand%20and%20choose%20models.%20However%2C%20not%20all%20models%20have%0Acomplete%20and%20reliable%20documentation.%20As%20the%20number%20of%20models%20increases%2C%20the%0Achallenges%20of%20finding%2C%20differentiating%2C%20and%20understanding%20models%20become%0Aincreasingly%20crucial.%20Inspired%20from%20research%20on%20data%20lakes%2C%20we%20introduce%20the%0Aconcept%20of%20model%20lakes.%20We%20formalize%20key%20model%20lake%20tasks%2C%20including%20model%0Aattribution%2C%20versioning%2C%20search%2C%20and%20benchmarking%2C%20and%20discuss%20fundamental%0Aresearch%20challenges%20in%20the%20management%20of%20large%20models.%20We%20also%20explore%20what%0Adata%20management%20techniques%20can%20be%20brought%20to%20bear%20on%20the%20study%20of%20large%20model%0Amanagement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.02327v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel%2520Lakes%26entry.906535625%3DKoyena%2520Pal%2520and%2520David%2520Bau%2520and%2520Ren%25C3%25A9e%2520J.%2520Miller%26entry.1292438233%3D%2520%2520Given%2520a%2520set%2520of%2520deep%2520learning%2520models%252C%2520it%2520can%2520be%2520hard%2520to%2520find%2520models%250Aappropriate%2520to%2520a%2520task%252C%2520understand%2520the%2520models%252C%2520and%2520characterize%2520how%2520models%2520are%250Adifferent%2520one%2520from%2520another.%2520Currently%252C%2520practitioners%2520rely%2520on%2520manually-written%250Adocumentation%2520to%2520understand%2520and%2520choose%2520models.%2520However%252C%2520not%2520all%2520models%2520have%250Acomplete%2520and%2520reliable%2520documentation.%2520As%2520the%2520number%2520of%2520models%2520increases%252C%2520the%250Achallenges%2520of%2520finding%252C%2520differentiating%252C%2520and%2520understanding%2520models%2520become%250Aincreasingly%2520crucial.%2520Inspired%2520from%2520research%2520on%2520data%2520lakes%252C%2520we%2520introduce%2520the%250Aconcept%2520of%2520model%2520lakes.%2520We%2520formalize%2520key%2520model%2520lake%2520tasks%252C%2520including%2520model%250Aattribution%252C%2520versioning%252C%2520search%252C%2520and%2520benchmarking%252C%2520and%2520discuss%2520fundamental%250Aresearch%2520challenges%2520in%2520the%2520management%2520of%2520large%2520models.%2520We%2520also%2520explore%2520what%250Adata%2520management%2520techniques%2520can%2520be%2520brought%2520to%2520bear%2520on%2520the%2520study%2520of%2520large%2520model%250Amanagement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.02327v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20Lakes&entry.906535625=Koyena%20Pal%20and%20David%20Bau%20and%20Ren%C3%A9e%20J.%20Miller&entry.1292438233=%20%20Given%20a%20set%20of%20deep%20learning%20models%2C%20it%20can%20be%20hard%20to%20find%20models%0Aappropriate%20to%20a%20task%2C%20understand%20the%20models%2C%20and%20characterize%20how%20models%20are%0Adifferent%20one%20from%20another.%20Currently%2C%20practitioners%20rely%20on%20manually-written%0Adocumentation%20to%20understand%20and%20choose%20models.%20However%2C%20not%20all%20models%20have%0Acomplete%20and%20reliable%20documentation.%20As%20the%20number%20of%20models%20increases%2C%20the%0Achallenges%20of%20finding%2C%20differentiating%2C%20and%20understanding%20models%20become%0Aincreasingly%20crucial.%20Inspired%20from%20research%20on%20data%20lakes%2C%20we%20introduce%20the%0Aconcept%20of%20model%20lakes.%20We%20formalize%20key%20model%20lake%20tasks%2C%20including%20model%0Aattribution%2C%20versioning%2C%20search%2C%20and%20benchmarking%2C%20and%20discuss%20fundamental%0Aresearch%20challenges%20in%20the%20management%20of%20large%20models.%20We%20also%20explore%20what%0Adata%20management%20techniques%20can%20be%20brought%20to%20bear%20on%20the%20study%20of%20large%20model%0Amanagement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02327v2&entry.124074799=Read"},
{"title": "PAPI: Exploiting Dynamic Parallelism in Large Language Model Decoding\n  with a Processing-In-Memory-Enabled Computing System", "author": "Yintao He and Haiyu Mao and Christina Giannoula and Mohammad Sadrosadati and Juan G\u00f3mez-Luna and Huawei Li and Xiaowei Li and Ying Wang and Onur Mutlu", "abstract": "  Large language models (LLMs) are widely used for natural language\nunderstanding and text generation. An LLM model relies on a time-consuming step\ncalled LLM decoding to generate output tokens. Several prior works focus on\nimproving the performance of LLM decoding using parallelism techniques, such as\nbatching and speculative decoding. State-of-the-art LLM decoding has both\ncompute-bound and memory-bound kernels. Some prior works statically identify\nand map these different kernels to a heterogeneous architecture consisting of\nboth processing-in-memory (PIM) units and computation-centric accelerators. We\nobserve that characteristics of LLM decoding kernels (e.g., whether or not a\nkernel is memory-bound) can change dynamically due to parameter changes to meet\nuser and/or system demands, making (1) static kernel mapping to PIM units and\ncomputation-centric accelerators suboptimal, and (2) one-size-fits-all approach\nof designing PIM units inefficient due to a large degree of heterogeneity even\nin memory-bound kernels.\n  In this paper, we aim to accelerate LLM decoding while considering the\ndynamically changing characteristics of the kernels involved. We propose PAPI\n(PArallel Decoding with PIM), a PIM-enabled heterogeneous architecture that\nexploits dynamic scheduling of compute-bound or memory-bound kernels to\nsuitable hardware units. PAPI has two key mechanisms: (1) online kernel\ncharacterization to dynamically schedule kernels to the most suitable hardware\nunits at runtime and (2) a PIM-enabled heterogeneous computing system that\nharmoniously orchestrates both computation-centric processing units and hybrid\nPIM units with different computing capabilities. Our experimental results on\nthree broadly-used LLMs show that PAPI achieves 1.8$\\times$ and 11.1$\\times$\nspeedups over a state-of-the-art heterogeneous LLM accelerator and a\nstate-of-the-art PIM-only LLM accelerator, respectively.\n", "link": "http://arxiv.org/abs/2502.15470v1", "date": "2025-02-21", "relevancy": 2.0119, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.542}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5059}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PAPI%3A%20Exploiting%20Dynamic%20Parallelism%20in%20Large%20Language%20Model%20Decoding%0A%20%20with%20a%20Processing-In-Memory-Enabled%20Computing%20System&body=Title%3A%20PAPI%3A%20Exploiting%20Dynamic%20Parallelism%20in%20Large%20Language%20Model%20Decoding%0A%20%20with%20a%20Processing-In-Memory-Enabled%20Computing%20System%0AAuthor%3A%20Yintao%20He%20and%20Haiyu%20Mao%20and%20Christina%20Giannoula%20and%20Mohammad%20Sadrosadati%20and%20Juan%20G%C3%B3mez-Luna%20and%20Huawei%20Li%20and%20Xiaowei%20Li%20and%20Ying%20Wang%20and%20Onur%20Mutlu%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20widely%20used%20for%20natural%20language%0Aunderstanding%20and%20text%20generation.%20An%20LLM%20model%20relies%20on%20a%20time-consuming%20step%0Acalled%20LLM%20decoding%20to%20generate%20output%20tokens.%20Several%20prior%20works%20focus%20on%0Aimproving%20the%20performance%20of%20LLM%20decoding%20using%20parallelism%20techniques%2C%20such%20as%0Abatching%20and%20speculative%20decoding.%20State-of-the-art%20LLM%20decoding%20has%20both%0Acompute-bound%20and%20memory-bound%20kernels.%20Some%20prior%20works%20statically%20identify%0Aand%20map%20these%20different%20kernels%20to%20a%20heterogeneous%20architecture%20consisting%20of%0Aboth%20processing-in-memory%20%28PIM%29%20units%20and%20computation-centric%20accelerators.%20We%0Aobserve%20that%20characteristics%20of%20LLM%20decoding%20kernels%20%28e.g.%2C%20whether%20or%20not%20a%0Akernel%20is%20memory-bound%29%20can%20change%20dynamically%20due%20to%20parameter%20changes%20to%20meet%0Auser%20and/or%20system%20demands%2C%20making%20%281%29%20static%20kernel%20mapping%20to%20PIM%20units%20and%0Acomputation-centric%20accelerators%20suboptimal%2C%20and%20%282%29%20one-size-fits-all%20approach%0Aof%20designing%20PIM%20units%20inefficient%20due%20to%20a%20large%20degree%20of%20heterogeneity%20even%0Ain%20memory-bound%20kernels.%0A%20%20In%20this%20paper%2C%20we%20aim%20to%20accelerate%20LLM%20decoding%20while%20considering%20the%0Adynamically%20changing%20characteristics%20of%20the%20kernels%20involved.%20We%20propose%20PAPI%0A%28PArallel%20Decoding%20with%20PIM%29%2C%20a%20PIM-enabled%20heterogeneous%20architecture%20that%0Aexploits%20dynamic%20scheduling%20of%20compute-bound%20or%20memory-bound%20kernels%20to%0Asuitable%20hardware%20units.%20PAPI%20has%20two%20key%20mechanisms%3A%20%281%29%20online%20kernel%0Acharacterization%20to%20dynamically%20schedule%20kernels%20to%20the%20most%20suitable%20hardware%0Aunits%20at%20runtime%20and%20%282%29%20a%20PIM-enabled%20heterogeneous%20computing%20system%20that%0Aharmoniously%20orchestrates%20both%20computation-centric%20processing%20units%20and%20hybrid%0APIM%20units%20with%20different%20computing%20capabilities.%20Our%20experimental%20results%20on%0Athree%20broadly-used%20LLMs%20show%20that%20PAPI%20achieves%201.8%24%5Ctimes%24%20and%2011.1%24%5Ctimes%24%0Aspeedups%20over%20a%20state-of-the-art%20heterogeneous%20LLM%20accelerator%20and%20a%0Astate-of-the-art%20PIM-only%20LLM%20accelerator%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15470v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPAPI%253A%2520Exploiting%2520Dynamic%2520Parallelism%2520in%2520Large%2520Language%2520Model%2520Decoding%250A%2520%2520with%2520a%2520Processing-In-Memory-Enabled%2520Computing%2520System%26entry.906535625%3DYintao%2520He%2520and%2520Haiyu%2520Mao%2520and%2520Christina%2520Giannoula%2520and%2520Mohammad%2520Sadrosadati%2520and%2520Juan%2520G%25C3%25B3mez-Luna%2520and%2520Huawei%2520Li%2520and%2520Xiaowei%2520Li%2520and%2520Ying%2520Wang%2520and%2520Onur%2520Mutlu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520widely%2520used%2520for%2520natural%2520language%250Aunderstanding%2520and%2520text%2520generation.%2520An%2520LLM%2520model%2520relies%2520on%2520a%2520time-consuming%2520step%250Acalled%2520LLM%2520decoding%2520to%2520generate%2520output%2520tokens.%2520Several%2520prior%2520works%2520focus%2520on%250Aimproving%2520the%2520performance%2520of%2520LLM%2520decoding%2520using%2520parallelism%2520techniques%252C%2520such%2520as%250Abatching%2520and%2520speculative%2520decoding.%2520State-of-the-art%2520LLM%2520decoding%2520has%2520both%250Acompute-bound%2520and%2520memory-bound%2520kernels.%2520Some%2520prior%2520works%2520statically%2520identify%250Aand%2520map%2520these%2520different%2520kernels%2520to%2520a%2520heterogeneous%2520architecture%2520consisting%2520of%250Aboth%2520processing-in-memory%2520%2528PIM%2529%2520units%2520and%2520computation-centric%2520accelerators.%2520We%250Aobserve%2520that%2520characteristics%2520of%2520LLM%2520decoding%2520kernels%2520%2528e.g.%252C%2520whether%2520or%2520not%2520a%250Akernel%2520is%2520memory-bound%2529%2520can%2520change%2520dynamically%2520due%2520to%2520parameter%2520changes%2520to%2520meet%250Auser%2520and/or%2520system%2520demands%252C%2520making%2520%25281%2529%2520static%2520kernel%2520mapping%2520to%2520PIM%2520units%2520and%250Acomputation-centric%2520accelerators%2520suboptimal%252C%2520and%2520%25282%2529%2520one-size-fits-all%2520approach%250Aof%2520designing%2520PIM%2520units%2520inefficient%2520due%2520to%2520a%2520large%2520degree%2520of%2520heterogeneity%2520even%250Ain%2520memory-bound%2520kernels.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520accelerate%2520LLM%2520decoding%2520while%2520considering%2520the%250Adynamically%2520changing%2520characteristics%2520of%2520the%2520kernels%2520involved.%2520We%2520propose%2520PAPI%250A%2528PArallel%2520Decoding%2520with%2520PIM%2529%252C%2520a%2520PIM-enabled%2520heterogeneous%2520architecture%2520that%250Aexploits%2520dynamic%2520scheduling%2520of%2520compute-bound%2520or%2520memory-bound%2520kernels%2520to%250Asuitable%2520hardware%2520units.%2520PAPI%2520has%2520two%2520key%2520mechanisms%253A%2520%25281%2529%2520online%2520kernel%250Acharacterization%2520to%2520dynamically%2520schedule%2520kernels%2520to%2520the%2520most%2520suitable%2520hardware%250Aunits%2520at%2520runtime%2520and%2520%25282%2529%2520a%2520PIM-enabled%2520heterogeneous%2520computing%2520system%2520that%250Aharmoniously%2520orchestrates%2520both%2520computation-centric%2520processing%2520units%2520and%2520hybrid%250APIM%2520units%2520with%2520different%2520computing%2520capabilities.%2520Our%2520experimental%2520results%2520on%250Athree%2520broadly-used%2520LLMs%2520show%2520that%2520PAPI%2520achieves%25201.8%2524%255Ctimes%2524%2520and%252011.1%2524%255Ctimes%2524%250Aspeedups%2520over%2520a%2520state-of-the-art%2520heterogeneous%2520LLM%2520accelerator%2520and%2520a%250Astate-of-the-art%2520PIM-only%2520LLM%2520accelerator%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15470v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAPI%3A%20Exploiting%20Dynamic%20Parallelism%20in%20Large%20Language%20Model%20Decoding%0A%20%20with%20a%20Processing-In-Memory-Enabled%20Computing%20System&entry.906535625=Yintao%20He%20and%20Haiyu%20Mao%20and%20Christina%20Giannoula%20and%20Mohammad%20Sadrosadati%20and%20Juan%20G%C3%B3mez-Luna%20and%20Huawei%20Li%20and%20Xiaowei%20Li%20and%20Ying%20Wang%20and%20Onur%20Mutlu&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20widely%20used%20for%20natural%20language%0Aunderstanding%20and%20text%20generation.%20An%20LLM%20model%20relies%20on%20a%20time-consuming%20step%0Acalled%20LLM%20decoding%20to%20generate%20output%20tokens.%20Several%20prior%20works%20focus%20on%0Aimproving%20the%20performance%20of%20LLM%20decoding%20using%20parallelism%20techniques%2C%20such%20as%0Abatching%20and%20speculative%20decoding.%20State-of-the-art%20LLM%20decoding%20has%20both%0Acompute-bound%20and%20memory-bound%20kernels.%20Some%20prior%20works%20statically%20identify%0Aand%20map%20these%20different%20kernels%20to%20a%20heterogeneous%20architecture%20consisting%20of%0Aboth%20processing-in-memory%20%28PIM%29%20units%20and%20computation-centric%20accelerators.%20We%0Aobserve%20that%20characteristics%20of%20LLM%20decoding%20kernels%20%28e.g.%2C%20whether%20or%20not%20a%0Akernel%20is%20memory-bound%29%20can%20change%20dynamically%20due%20to%20parameter%20changes%20to%20meet%0Auser%20and/or%20system%20demands%2C%20making%20%281%29%20static%20kernel%20mapping%20to%20PIM%20units%20and%0Acomputation-centric%20accelerators%20suboptimal%2C%20and%20%282%29%20one-size-fits-all%20approach%0Aof%20designing%20PIM%20units%20inefficient%20due%20to%20a%20large%20degree%20of%20heterogeneity%20even%0Ain%20memory-bound%20kernels.%0A%20%20In%20this%20paper%2C%20we%20aim%20to%20accelerate%20LLM%20decoding%20while%20considering%20the%0Adynamically%20changing%20characteristics%20of%20the%20kernels%20involved.%20We%20propose%20PAPI%0A%28PArallel%20Decoding%20with%20PIM%29%2C%20a%20PIM-enabled%20heterogeneous%20architecture%20that%0Aexploits%20dynamic%20scheduling%20of%20compute-bound%20or%20memory-bound%20kernels%20to%0Asuitable%20hardware%20units.%20PAPI%20has%20two%20key%20mechanisms%3A%20%281%29%20online%20kernel%0Acharacterization%20to%20dynamically%20schedule%20kernels%20to%20the%20most%20suitable%20hardware%0Aunits%20at%20runtime%20and%20%282%29%20a%20PIM-enabled%20heterogeneous%20computing%20system%20that%0Aharmoniously%20orchestrates%20both%20computation-centric%20processing%20units%20and%20hybrid%0APIM%20units%20with%20different%20computing%20capabilities.%20Our%20experimental%20results%20on%0Athree%20broadly-used%20LLMs%20show%20that%20PAPI%20achieves%201.8%24%5Ctimes%24%20and%2011.1%24%5Ctimes%24%0Aspeedups%20over%20a%20state-of-the-art%20heterogeneous%20LLM%20accelerator%20and%20a%0Astate-of-the-art%20PIM-only%20LLM%20accelerator%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15470v1&entry.124074799=Read"},
{"title": "Context-Aware Doubly-Robust Semi-Supervised Learning", "author": "Clement Ruah and Houssem Sifaou and Osvaldo Simeone and Bashir Al-Hashimi", "abstract": "  The widespread adoption of artificial intelligence (AI) in next-generation\ncommunication systems is challenged by the heterogeneity of traffic and network\nconditions, which call for the use of highly contextual, site-specific, data. A\npromising solution is to rely not only on real-world data, but also on\nsynthetic pseudo-data generated by a network digital twin (NDT). However, the\neffectiveness of this approach hinges on the accuracy of the NDT, which can\nvary widely across different contexts. To address this problem, this paper\nintroduces context-aware doubly-robust (CDR) learning, a novel semi-supervised\nscheme that adapts its reliance on the pseudo-data to the different levels of\nfidelity of the NDT across contexts. CDR is evaluated on the task of downlink\nbeamforming, showing superior performance compared to previous state-of-the-art\nsemi-supervised approaches.\n", "link": "http://arxiv.org/abs/2502.15577v1", "date": "2025-02-21", "relevancy": 2.0098, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5114}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5108}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context-Aware%20Doubly-Robust%20Semi-Supervised%20Learning&body=Title%3A%20Context-Aware%20Doubly-Robust%20Semi-Supervised%20Learning%0AAuthor%3A%20Clement%20Ruah%20and%20Houssem%20Sifaou%20and%20Osvaldo%20Simeone%20and%20Bashir%20Al-Hashimi%0AAbstract%3A%20%20%20The%20widespread%20adoption%20of%20artificial%20intelligence%20%28AI%29%20in%20next-generation%0Acommunication%20systems%20is%20challenged%20by%20the%20heterogeneity%20of%20traffic%20and%20network%0Aconditions%2C%20which%20call%20for%20the%20use%20of%20highly%20contextual%2C%20site-specific%2C%20data.%20A%0Apromising%20solution%20is%20to%20rely%20not%20only%20on%20real-world%20data%2C%20but%20also%20on%0Asynthetic%20pseudo-data%20generated%20by%20a%20network%20digital%20twin%20%28NDT%29.%20However%2C%20the%0Aeffectiveness%20of%20this%20approach%20hinges%20on%20the%20accuracy%20of%20the%20NDT%2C%20which%20can%0Avary%20widely%20across%20different%20contexts.%20To%20address%20this%20problem%2C%20this%20paper%0Aintroduces%20context-aware%20doubly-robust%20%28CDR%29%20learning%2C%20a%20novel%20semi-supervised%0Ascheme%20that%20adapts%20its%20reliance%20on%20the%20pseudo-data%20to%20the%20different%20levels%20of%0Afidelity%20of%20the%20NDT%20across%20contexts.%20CDR%20is%20evaluated%20on%20the%20task%20of%20downlink%0Abeamforming%2C%20showing%20superior%20performance%20compared%20to%20previous%20state-of-the-art%0Asemi-supervised%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15577v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext-Aware%2520Doubly-Robust%2520Semi-Supervised%2520Learning%26entry.906535625%3DClement%2520Ruah%2520and%2520Houssem%2520Sifaou%2520and%2520Osvaldo%2520Simeone%2520and%2520Bashir%2520Al-Hashimi%26entry.1292438233%3D%2520%2520The%2520widespread%2520adoption%2520of%2520artificial%2520intelligence%2520%2528AI%2529%2520in%2520next-generation%250Acommunication%2520systems%2520is%2520challenged%2520by%2520the%2520heterogeneity%2520of%2520traffic%2520and%2520network%250Aconditions%252C%2520which%2520call%2520for%2520the%2520use%2520of%2520highly%2520contextual%252C%2520site-specific%252C%2520data.%2520A%250Apromising%2520solution%2520is%2520to%2520rely%2520not%2520only%2520on%2520real-world%2520data%252C%2520but%2520also%2520on%250Asynthetic%2520pseudo-data%2520generated%2520by%2520a%2520network%2520digital%2520twin%2520%2528NDT%2529.%2520However%252C%2520the%250Aeffectiveness%2520of%2520this%2520approach%2520hinges%2520on%2520the%2520accuracy%2520of%2520the%2520NDT%252C%2520which%2520can%250Avary%2520widely%2520across%2520different%2520contexts.%2520To%2520address%2520this%2520problem%252C%2520this%2520paper%250Aintroduces%2520context-aware%2520doubly-robust%2520%2528CDR%2529%2520learning%252C%2520a%2520novel%2520semi-supervised%250Ascheme%2520that%2520adapts%2520its%2520reliance%2520on%2520the%2520pseudo-data%2520to%2520the%2520different%2520levels%2520of%250Afidelity%2520of%2520the%2520NDT%2520across%2520contexts.%2520CDR%2520is%2520evaluated%2520on%2520the%2520task%2520of%2520downlink%250Abeamforming%252C%2520showing%2520superior%2520performance%2520compared%2520to%2520previous%2520state-of-the-art%250Asemi-supervised%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15577v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context-Aware%20Doubly-Robust%20Semi-Supervised%20Learning&entry.906535625=Clement%20Ruah%20and%20Houssem%20Sifaou%20and%20Osvaldo%20Simeone%20and%20Bashir%20Al-Hashimi&entry.1292438233=%20%20The%20widespread%20adoption%20of%20artificial%20intelligence%20%28AI%29%20in%20next-generation%0Acommunication%20systems%20is%20challenged%20by%20the%20heterogeneity%20of%20traffic%20and%20network%0Aconditions%2C%20which%20call%20for%20the%20use%20of%20highly%20contextual%2C%20site-specific%2C%20data.%20A%0Apromising%20solution%20is%20to%20rely%20not%20only%20on%20real-world%20data%2C%20but%20also%20on%0Asynthetic%20pseudo-data%20generated%20by%20a%20network%20digital%20twin%20%28NDT%29.%20However%2C%20the%0Aeffectiveness%20of%20this%20approach%20hinges%20on%20the%20accuracy%20of%20the%20NDT%2C%20which%20can%0Avary%20widely%20across%20different%20contexts.%20To%20address%20this%20problem%2C%20this%20paper%0Aintroduces%20context-aware%20doubly-robust%20%28CDR%29%20learning%2C%20a%20novel%20semi-supervised%0Ascheme%20that%20adapts%20its%20reliance%20on%20the%20pseudo-data%20to%20the%20different%20levels%20of%0Afidelity%20of%20the%20NDT%20across%20contexts.%20CDR%20is%20evaluated%20on%20the%20task%20of%20downlink%0Abeamforming%2C%20showing%20superior%20performance%20compared%20to%20previous%20state-of-the-art%0Asemi-supervised%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15577v1&entry.124074799=Read"},
{"title": "NEAT: Nonlinear Parameter-efficient Adaptation of Pre-trained Models", "author": "Yibo Zhong and Haoxiang Jiang and Lincan Li and Ryumei Nakada and Tianci Liu and Linjun Zhang and Huaxiu Yao and Haoyu Wang", "abstract": "  Fine-tuning pre-trained models often yields state-of-the-art performance but\nis computationally expensive when updating all parameters. Parameter-efficient\nfine-tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), address this by\nfreezing pre-trained weights and introducing low-rank matrices. However,\nbecause LoRA relies on low-rank decomposition, it struggles to capture complex\nnonlinear dynamics and optimal optimization trajectories, resulting in a\nperformance gap relative to full fine-tuning and inefficient parameter\nutilization. To overcome these issues, we propose NEAT, a nonlinear PEFT\napproach that employs a lightweight neural network to learn a nonlinear\ntransformation of the pre-trained weights, thereby better approximating\ncumulative weight updates. Our theoretical analysis shows that NEAT achieves\ngreater efficiency than LoRA while maintaining equivalent expressivity.\nExtensive experiments on four benchmarks and over twenty datasets demonstrate\nthat NEAT significantly outperforms state-of-the-art baselines in both vision\nand text tasks.\n", "link": "http://arxiv.org/abs/2410.01870v2", "date": "2025-02-21", "relevancy": 2.0018, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5266}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4877}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4795}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NEAT%3A%20Nonlinear%20Parameter-efficient%20Adaptation%20of%20Pre-trained%20Models&body=Title%3A%20NEAT%3A%20Nonlinear%20Parameter-efficient%20Adaptation%20of%20Pre-trained%20Models%0AAuthor%3A%20Yibo%20Zhong%20and%20Haoxiang%20Jiang%20and%20Lincan%20Li%20and%20Ryumei%20Nakada%20and%20Tianci%20Liu%20and%20Linjun%20Zhang%20and%20Huaxiu%20Yao%20and%20Haoyu%20Wang%0AAbstract%3A%20%20%20Fine-tuning%20pre-trained%20models%20often%20yields%20state-of-the-art%20performance%20but%0Ais%20computationally%20expensive%20when%20updating%20all%20parameters.%20Parameter-efficient%0Afine-tuning%20%28PEFT%29%20methods%2C%20such%20as%20Low-Rank%20Adaptation%20%28LoRA%29%2C%20address%20this%20by%0Afreezing%20pre-trained%20weights%20and%20introducing%20low-rank%20matrices.%20However%2C%0Abecause%20LoRA%20relies%20on%20low-rank%20decomposition%2C%20it%20struggles%20to%20capture%20complex%0Anonlinear%20dynamics%20and%20optimal%20optimization%20trajectories%2C%20resulting%20in%20a%0Aperformance%20gap%20relative%20to%20full%20fine-tuning%20and%20inefficient%20parameter%0Autilization.%20To%20overcome%20these%20issues%2C%20we%20propose%20NEAT%2C%20a%20nonlinear%20PEFT%0Aapproach%20that%20employs%20a%20lightweight%20neural%20network%20to%20learn%20a%20nonlinear%0Atransformation%20of%20the%20pre-trained%20weights%2C%20thereby%20better%20approximating%0Acumulative%20weight%20updates.%20Our%20theoretical%20analysis%20shows%20that%20NEAT%20achieves%0Agreater%20efficiency%20than%20LoRA%20while%20maintaining%20equivalent%20expressivity.%0AExtensive%20experiments%20on%20four%20benchmarks%20and%20over%20twenty%20datasets%20demonstrate%0Athat%20NEAT%20significantly%20outperforms%20state-of-the-art%20baselines%20in%20both%20vision%0Aand%20text%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01870v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNEAT%253A%2520Nonlinear%2520Parameter-efficient%2520Adaptation%2520of%2520Pre-trained%2520Models%26entry.906535625%3DYibo%2520Zhong%2520and%2520Haoxiang%2520Jiang%2520and%2520Lincan%2520Li%2520and%2520Ryumei%2520Nakada%2520and%2520Tianci%2520Liu%2520and%2520Linjun%2520Zhang%2520and%2520Huaxiu%2520Yao%2520and%2520Haoyu%2520Wang%26entry.1292438233%3D%2520%2520Fine-tuning%2520pre-trained%2520models%2520often%2520yields%2520state-of-the-art%2520performance%2520but%250Ais%2520computationally%2520expensive%2520when%2520updating%2520all%2520parameters.%2520Parameter-efficient%250Afine-tuning%2520%2528PEFT%2529%2520methods%252C%2520such%2520as%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%252C%2520address%2520this%2520by%250Afreezing%2520pre-trained%2520weights%2520and%2520introducing%2520low-rank%2520matrices.%2520However%252C%250Abecause%2520LoRA%2520relies%2520on%2520low-rank%2520decomposition%252C%2520it%2520struggles%2520to%2520capture%2520complex%250Anonlinear%2520dynamics%2520and%2520optimal%2520optimization%2520trajectories%252C%2520resulting%2520in%2520a%250Aperformance%2520gap%2520relative%2520to%2520full%2520fine-tuning%2520and%2520inefficient%2520parameter%250Autilization.%2520To%2520overcome%2520these%2520issues%252C%2520we%2520propose%2520NEAT%252C%2520a%2520nonlinear%2520PEFT%250Aapproach%2520that%2520employs%2520a%2520lightweight%2520neural%2520network%2520to%2520learn%2520a%2520nonlinear%250Atransformation%2520of%2520the%2520pre-trained%2520weights%252C%2520thereby%2520better%2520approximating%250Acumulative%2520weight%2520updates.%2520Our%2520theoretical%2520analysis%2520shows%2520that%2520NEAT%2520achieves%250Agreater%2520efficiency%2520than%2520LoRA%2520while%2520maintaining%2520equivalent%2520expressivity.%250AExtensive%2520experiments%2520on%2520four%2520benchmarks%2520and%2520over%2520twenty%2520datasets%2520demonstrate%250Athat%2520NEAT%2520significantly%2520outperforms%2520state-of-the-art%2520baselines%2520in%2520both%2520vision%250Aand%2520text%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01870v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NEAT%3A%20Nonlinear%20Parameter-efficient%20Adaptation%20of%20Pre-trained%20Models&entry.906535625=Yibo%20Zhong%20and%20Haoxiang%20Jiang%20and%20Lincan%20Li%20and%20Ryumei%20Nakada%20and%20Tianci%20Liu%20and%20Linjun%20Zhang%20and%20Huaxiu%20Yao%20and%20Haoyu%20Wang&entry.1292438233=%20%20Fine-tuning%20pre-trained%20models%20often%20yields%20state-of-the-art%20performance%20but%0Ais%20computationally%20expensive%20when%20updating%20all%20parameters.%20Parameter-efficient%0Afine-tuning%20%28PEFT%29%20methods%2C%20such%20as%20Low-Rank%20Adaptation%20%28LoRA%29%2C%20address%20this%20by%0Afreezing%20pre-trained%20weights%20and%20introducing%20low-rank%20matrices.%20However%2C%0Abecause%20LoRA%20relies%20on%20low-rank%20decomposition%2C%20it%20struggles%20to%20capture%20complex%0Anonlinear%20dynamics%20and%20optimal%20optimization%20trajectories%2C%20resulting%20in%20a%0Aperformance%20gap%20relative%20to%20full%20fine-tuning%20and%20inefficient%20parameter%0Autilization.%20To%20overcome%20these%20issues%2C%20we%20propose%20NEAT%2C%20a%20nonlinear%20PEFT%0Aapproach%20that%20employs%20a%20lightweight%20neural%20network%20to%20learn%20a%20nonlinear%0Atransformation%20of%20the%20pre-trained%20weights%2C%20thereby%20better%20approximating%0Acumulative%20weight%20updates.%20Our%20theoretical%20analysis%20shows%20that%20NEAT%20achieves%0Agreater%20efficiency%20than%20LoRA%20while%20maintaining%20equivalent%20expressivity.%0AExtensive%20experiments%20on%20four%20benchmarks%20and%20over%20twenty%20datasets%20demonstrate%0Athat%20NEAT%20significantly%20outperforms%20state-of-the-art%20baselines%20in%20both%20vision%0Aand%20text%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01870v2&entry.124074799=Read"},
{"title": "ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models", "author": "Martina Miliani and Serenna Auriemma and Alessandro Bondielli and Emmanuele Chersoni and Lucia Passaro and Irene Sucameli and Alessandro Lenci", "abstract": "  Large Language Models (LLMs) are increasingly used in tasks requiring\ninterpretive and inferential accuracy. In this paper, we introduce ExpliCa, a\nnew dataset for evaluating LLMs in explicit causal reasoning. ExpliCa uniquely\nintegrates both causal and temporal relations presented in different linguistic\norders and explicitly expressed by linguistic connectives. The dataset is\nenriched with crowdsourced human acceptability ratings. We tested LLMs on\nExpliCa through prompting and perplexity-based metrics. We assessed seven\ncommercial and open-source LLMs, revealing that even top models struggle to\nreach 0.80 accuracy. Interestingly, models tend to confound temporal relations\nwith causal ones, and their performance is also strongly influenced by the\nlinguistic order of the events. Finally, perplexity-based scores and prompting\nperformance are differently affected by model size.\n", "link": "http://arxiv.org/abs/2502.15487v1", "date": "2025-02-21", "relevancy": 2.0005, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5092}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5092}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ExpliCa%3A%20Evaluating%20Explicit%20Causal%20Reasoning%20in%20Large%20Language%20Models&body=Title%3A%20ExpliCa%3A%20Evaluating%20Explicit%20Causal%20Reasoning%20in%20Large%20Language%20Models%0AAuthor%3A%20Martina%20Miliani%20and%20Serenna%20Auriemma%20and%20Alessandro%20Bondielli%20and%20Emmanuele%20Chersoni%20and%20Lucia%20Passaro%20and%20Irene%20Sucameli%20and%20Alessandro%20Lenci%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20used%20in%20tasks%20requiring%0Ainterpretive%20and%20inferential%20accuracy.%20In%20this%20paper%2C%20we%20introduce%20ExpliCa%2C%20a%0Anew%20dataset%20for%20evaluating%20LLMs%20in%20explicit%20causal%20reasoning.%20ExpliCa%20uniquely%0Aintegrates%20both%20causal%20and%20temporal%20relations%20presented%20in%20different%20linguistic%0Aorders%20and%20explicitly%20expressed%20by%20linguistic%20connectives.%20The%20dataset%20is%0Aenriched%20with%20crowdsourced%20human%20acceptability%20ratings.%20We%20tested%20LLMs%20on%0AExpliCa%20through%20prompting%20and%20perplexity-based%20metrics.%20We%20assessed%20seven%0Acommercial%20and%20open-source%20LLMs%2C%20revealing%20that%20even%20top%20models%20struggle%20to%0Areach%200.80%20accuracy.%20Interestingly%2C%20models%20tend%20to%20confound%20temporal%20relations%0Awith%20causal%20ones%2C%20and%20their%20performance%20is%20also%20strongly%20influenced%20by%20the%0Alinguistic%20order%20of%20the%20events.%20Finally%2C%20perplexity-based%20scores%20and%20prompting%0Aperformance%20are%20differently%20affected%20by%20model%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15487v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExpliCa%253A%2520Evaluating%2520Explicit%2520Causal%2520Reasoning%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DMartina%2520Miliani%2520and%2520Serenna%2520Auriemma%2520and%2520Alessandro%2520Bondielli%2520and%2520Emmanuele%2520Chersoni%2520and%2520Lucia%2520Passaro%2520and%2520Irene%2520Sucameli%2520and%2520Alessandro%2520Lenci%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520increasingly%2520used%2520in%2520tasks%2520requiring%250Ainterpretive%2520and%2520inferential%2520accuracy.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520ExpliCa%252C%2520a%250Anew%2520dataset%2520for%2520evaluating%2520LLMs%2520in%2520explicit%2520causal%2520reasoning.%2520ExpliCa%2520uniquely%250Aintegrates%2520both%2520causal%2520and%2520temporal%2520relations%2520presented%2520in%2520different%2520linguistic%250Aorders%2520and%2520explicitly%2520expressed%2520by%2520linguistic%2520connectives.%2520The%2520dataset%2520is%250Aenriched%2520with%2520crowdsourced%2520human%2520acceptability%2520ratings.%2520We%2520tested%2520LLMs%2520on%250AExpliCa%2520through%2520prompting%2520and%2520perplexity-based%2520metrics.%2520We%2520assessed%2520seven%250Acommercial%2520and%2520open-source%2520LLMs%252C%2520revealing%2520that%2520even%2520top%2520models%2520struggle%2520to%250Areach%25200.80%2520accuracy.%2520Interestingly%252C%2520models%2520tend%2520to%2520confound%2520temporal%2520relations%250Awith%2520causal%2520ones%252C%2520and%2520their%2520performance%2520is%2520also%2520strongly%2520influenced%2520by%2520the%250Alinguistic%2520order%2520of%2520the%2520events.%2520Finally%252C%2520perplexity-based%2520scores%2520and%2520prompting%250Aperformance%2520are%2520differently%2520affected%2520by%2520model%2520size.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15487v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ExpliCa%3A%20Evaluating%20Explicit%20Causal%20Reasoning%20in%20Large%20Language%20Models&entry.906535625=Martina%20Miliani%20and%20Serenna%20Auriemma%20and%20Alessandro%20Bondielli%20and%20Emmanuele%20Chersoni%20and%20Lucia%20Passaro%20and%20Irene%20Sucameli%20and%20Alessandro%20Lenci&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20used%20in%20tasks%20requiring%0Ainterpretive%20and%20inferential%20accuracy.%20In%20this%20paper%2C%20we%20introduce%20ExpliCa%2C%20a%0Anew%20dataset%20for%20evaluating%20LLMs%20in%20explicit%20causal%20reasoning.%20ExpliCa%20uniquely%0Aintegrates%20both%20causal%20and%20temporal%20relations%20presented%20in%20different%20linguistic%0Aorders%20and%20explicitly%20expressed%20by%20linguistic%20connectives.%20The%20dataset%20is%0Aenriched%20with%20crowdsourced%20human%20acceptability%20ratings.%20We%20tested%20LLMs%20on%0AExpliCa%20through%20prompting%20and%20perplexity-based%20metrics.%20We%20assessed%20seven%0Acommercial%20and%20open-source%20LLMs%2C%20revealing%20that%20even%20top%20models%20struggle%20to%0Areach%200.80%20accuracy.%20Interestingly%2C%20models%20tend%20to%20confound%20temporal%20relations%0Awith%20causal%20ones%2C%20and%20their%20performance%20is%20also%20strongly%20influenced%20by%20the%0Alinguistic%20order%20of%20the%20events.%20Finally%2C%20perplexity-based%20scores%20and%20prompting%0Aperformance%20are%20differently%20affected%20by%20model%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15487v1&entry.124074799=Read"},
{"title": "I2CKD : Intra- and Inter-Class Knowledge Distillation for Semantic\n  Segmentation", "author": "Ayoub Karine and Thibault Napol\u00e9on and Maher Jridi", "abstract": "  This paper proposes a new knowledge distillation method tailored for image\nsemantic segmentation, termed Intra- and Inter-Class Knowledge Distillation\n(I2CKD). The focus of this method is on capturing and transferring knowledge\nbetween the intermediate layers of teacher (cumbersome model) and student\n(compact model). For knowledge extraction, we exploit class prototypes derived\nfrom feature maps. To facilitate knowledge transfer, we employ a triplet loss\nin order to minimize intra-class variances and maximize inter-class variances\nbetween teacher and student prototypes. Consequently, I2CKD enables the student\nto better mimic the feature representation of the teacher for each class,\nthereby enhancing the segmentation performance of the compact network.\nExtensive experiments on three segmentation datasets, i.e., Cityscapes, Pascal\nVOC and CamVid, using various teacher-student network pairs demonstrate the\neffectiveness of the proposed method.\n", "link": "http://arxiv.org/abs/2403.18490v2", "date": "2025-02-21", "relevancy": 1.9977, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5095}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.504}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4875}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20I2CKD%20%3A%20Intra-%20and%20Inter-Class%20Knowledge%20Distillation%20for%20Semantic%0A%20%20Segmentation&body=Title%3A%20I2CKD%20%3A%20Intra-%20and%20Inter-Class%20Knowledge%20Distillation%20for%20Semantic%0A%20%20Segmentation%0AAuthor%3A%20Ayoub%20Karine%20and%20Thibault%20Napol%C3%A9on%20and%20Maher%20Jridi%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20new%20knowledge%20distillation%20method%20tailored%20for%20image%0Asemantic%20segmentation%2C%20termed%20Intra-%20and%20Inter-Class%20Knowledge%20Distillation%0A%28I2CKD%29.%20The%20focus%20of%20this%20method%20is%20on%20capturing%20and%20transferring%20knowledge%0Abetween%20the%20intermediate%20layers%20of%20teacher%20%28cumbersome%20model%29%20and%20student%0A%28compact%20model%29.%20For%20knowledge%20extraction%2C%20we%20exploit%20class%20prototypes%20derived%0Afrom%20feature%20maps.%20To%20facilitate%20knowledge%20transfer%2C%20we%20employ%20a%20triplet%20loss%0Ain%20order%20to%20minimize%20intra-class%20variances%20and%20maximize%20inter-class%20variances%0Abetween%20teacher%20and%20student%20prototypes.%20Consequently%2C%20I2CKD%20enables%20the%20student%0Ato%20better%20mimic%20the%20feature%20representation%20of%20the%20teacher%20for%20each%20class%2C%0Athereby%20enhancing%20the%20segmentation%20performance%20of%20the%20compact%20network.%0AExtensive%20experiments%20on%20three%20segmentation%20datasets%2C%20i.e.%2C%20Cityscapes%2C%20Pascal%0AVOC%20and%20CamVid%2C%20using%20various%20teacher-student%20network%20pairs%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18490v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DI2CKD%2520%253A%2520Intra-%2520and%2520Inter-Class%2520Knowledge%2520Distillation%2520for%2520Semantic%250A%2520%2520Segmentation%26entry.906535625%3DAyoub%2520Karine%2520and%2520Thibault%2520Napol%25C3%25A9on%2520and%2520Maher%2520Jridi%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520new%2520knowledge%2520distillation%2520method%2520tailored%2520for%2520image%250Asemantic%2520segmentation%252C%2520termed%2520Intra-%2520and%2520Inter-Class%2520Knowledge%2520Distillation%250A%2528I2CKD%2529.%2520The%2520focus%2520of%2520this%2520method%2520is%2520on%2520capturing%2520and%2520transferring%2520knowledge%250Abetween%2520the%2520intermediate%2520layers%2520of%2520teacher%2520%2528cumbersome%2520model%2529%2520and%2520student%250A%2528compact%2520model%2529.%2520For%2520knowledge%2520extraction%252C%2520we%2520exploit%2520class%2520prototypes%2520derived%250Afrom%2520feature%2520maps.%2520To%2520facilitate%2520knowledge%2520transfer%252C%2520we%2520employ%2520a%2520triplet%2520loss%250Ain%2520order%2520to%2520minimize%2520intra-class%2520variances%2520and%2520maximize%2520inter-class%2520variances%250Abetween%2520teacher%2520and%2520student%2520prototypes.%2520Consequently%252C%2520I2CKD%2520enables%2520the%2520student%250Ato%2520better%2520mimic%2520the%2520feature%2520representation%2520of%2520the%2520teacher%2520for%2520each%2520class%252C%250Athereby%2520enhancing%2520the%2520segmentation%2520performance%2520of%2520the%2520compact%2520network.%250AExtensive%2520experiments%2520on%2520three%2520segmentation%2520datasets%252C%2520i.e.%252C%2520Cityscapes%252C%2520Pascal%250AVOC%2520and%2520CamVid%252C%2520using%2520various%2520teacher-student%2520network%2520pairs%2520demonstrate%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.18490v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=I2CKD%20%3A%20Intra-%20and%20Inter-Class%20Knowledge%20Distillation%20for%20Semantic%0A%20%20Segmentation&entry.906535625=Ayoub%20Karine%20and%20Thibault%20Napol%C3%A9on%20and%20Maher%20Jridi&entry.1292438233=%20%20This%20paper%20proposes%20a%20new%20knowledge%20distillation%20method%20tailored%20for%20image%0Asemantic%20segmentation%2C%20termed%20Intra-%20and%20Inter-Class%20Knowledge%20Distillation%0A%28I2CKD%29.%20The%20focus%20of%20this%20method%20is%20on%20capturing%20and%20transferring%20knowledge%0Abetween%20the%20intermediate%20layers%20of%20teacher%20%28cumbersome%20model%29%20and%20student%0A%28compact%20model%29.%20For%20knowledge%20extraction%2C%20we%20exploit%20class%20prototypes%20derived%0Afrom%20feature%20maps.%20To%20facilitate%20knowledge%20transfer%2C%20we%20employ%20a%20triplet%20loss%0Ain%20order%20to%20minimize%20intra-class%20variances%20and%20maximize%20inter-class%20variances%0Abetween%20teacher%20and%20student%20prototypes.%20Consequently%2C%20I2CKD%20enables%20the%20student%0Ato%20better%20mimic%20the%20feature%20representation%20of%20the%20teacher%20for%20each%20class%2C%0Athereby%20enhancing%20the%20segmentation%20performance%20of%20the%20compact%20network.%0AExtensive%20experiments%20on%20three%20segmentation%20datasets%2C%20i.e.%2C%20Cityscapes%2C%20Pascal%0AVOC%20and%20CamVid%2C%20using%20various%20teacher-student%20network%20pairs%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18490v2&entry.124074799=Read"},
{"title": "Training Neural ODEs Using Fully Discretized Simultaneous Optimization", "author": "Mariia Shapovalova and Calvin Tsay", "abstract": "  Neural Ordinary Differential Equations (Neural ODEs) represent\ncontinuous-time dynamics with neural networks, offering advancements for\nmodeling and control tasks. However, training Neural ODEs requires solving\ndifferential equations at each epoch, leading to high computational costs. This\nwork investigates simultaneous optimization methods as a faster training\nalternative. In particular, we employ a collocation-based, fully discretized\nformulation and use IPOPT--a solver for large-scale nonlinear optimization--to\nsimultaneously optimize collocation coefficients and neural network parameters.\nUsing the Van der Pol Oscillator as a case study, we demonstrate faster\nconvergence compared to traditional training methods. Furthermore, we introduce\na decomposition framework utilizing Alternating Direction Method of Multipliers\n(ADMM) to effectively coordinate sub-models among data batches. Our results\nshow significant potential for (collocation-based) simultaneous Neural ODE\ntraining pipelines.\n", "link": "http://arxiv.org/abs/2502.15642v1", "date": "2025-02-21", "relevancy": 1.9949, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5099}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4927}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4857}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20Neural%20ODEs%20Using%20Fully%20Discretized%20Simultaneous%20Optimization&body=Title%3A%20Training%20Neural%20ODEs%20Using%20Fully%20Discretized%20Simultaneous%20Optimization%0AAuthor%3A%20Mariia%20Shapovalova%20and%20Calvin%20Tsay%0AAbstract%3A%20%20%20Neural%20Ordinary%20Differential%20Equations%20%28Neural%20ODEs%29%20represent%0Acontinuous-time%20dynamics%20with%20neural%20networks%2C%20offering%20advancements%20for%0Amodeling%20and%20control%20tasks.%20However%2C%20training%20Neural%20ODEs%20requires%20solving%0Adifferential%20equations%20at%20each%20epoch%2C%20leading%20to%20high%20computational%20costs.%20This%0Awork%20investigates%20simultaneous%20optimization%20methods%20as%20a%20faster%20training%0Aalternative.%20In%20particular%2C%20we%20employ%20a%20collocation-based%2C%20fully%20discretized%0Aformulation%20and%20use%20IPOPT--a%20solver%20for%20large-scale%20nonlinear%20optimization--to%0Asimultaneously%20optimize%20collocation%20coefficients%20and%20neural%20network%20parameters.%0AUsing%20the%20Van%20der%20Pol%20Oscillator%20as%20a%20case%20study%2C%20we%20demonstrate%20faster%0Aconvergence%20compared%20to%20traditional%20training%20methods.%20Furthermore%2C%20we%20introduce%0Aa%20decomposition%20framework%20utilizing%20Alternating%20Direction%20Method%20of%20Multipliers%0A%28ADMM%29%20to%20effectively%20coordinate%20sub-models%20among%20data%20batches.%20Our%20results%0Ashow%20significant%20potential%20for%20%28collocation-based%29%20simultaneous%20Neural%20ODE%0Atraining%20pipelines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15642v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520Neural%2520ODEs%2520Using%2520Fully%2520Discretized%2520Simultaneous%2520Optimization%26entry.906535625%3DMariia%2520Shapovalova%2520and%2520Calvin%2520Tsay%26entry.1292438233%3D%2520%2520Neural%2520Ordinary%2520Differential%2520Equations%2520%2528Neural%2520ODEs%2529%2520represent%250Acontinuous-time%2520dynamics%2520with%2520neural%2520networks%252C%2520offering%2520advancements%2520for%250Amodeling%2520and%2520control%2520tasks.%2520However%252C%2520training%2520Neural%2520ODEs%2520requires%2520solving%250Adifferential%2520equations%2520at%2520each%2520epoch%252C%2520leading%2520to%2520high%2520computational%2520costs.%2520This%250Awork%2520investigates%2520simultaneous%2520optimization%2520methods%2520as%2520a%2520faster%2520training%250Aalternative.%2520In%2520particular%252C%2520we%2520employ%2520a%2520collocation-based%252C%2520fully%2520discretized%250Aformulation%2520and%2520use%2520IPOPT--a%2520solver%2520for%2520large-scale%2520nonlinear%2520optimization--to%250Asimultaneously%2520optimize%2520collocation%2520coefficients%2520and%2520neural%2520network%2520parameters.%250AUsing%2520the%2520Van%2520der%2520Pol%2520Oscillator%2520as%2520a%2520case%2520study%252C%2520we%2520demonstrate%2520faster%250Aconvergence%2520compared%2520to%2520traditional%2520training%2520methods.%2520Furthermore%252C%2520we%2520introduce%250Aa%2520decomposition%2520framework%2520utilizing%2520Alternating%2520Direction%2520Method%2520of%2520Multipliers%250A%2528ADMM%2529%2520to%2520effectively%2520coordinate%2520sub-models%2520among%2520data%2520batches.%2520Our%2520results%250Ashow%2520significant%2520potential%2520for%2520%2528collocation-based%2529%2520simultaneous%2520Neural%2520ODE%250Atraining%2520pipelines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15642v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20Neural%20ODEs%20Using%20Fully%20Discretized%20Simultaneous%20Optimization&entry.906535625=Mariia%20Shapovalova%20and%20Calvin%20Tsay&entry.1292438233=%20%20Neural%20Ordinary%20Differential%20Equations%20%28Neural%20ODEs%29%20represent%0Acontinuous-time%20dynamics%20with%20neural%20networks%2C%20offering%20advancements%20for%0Amodeling%20and%20control%20tasks.%20However%2C%20training%20Neural%20ODEs%20requires%20solving%0Adifferential%20equations%20at%20each%20epoch%2C%20leading%20to%20high%20computational%20costs.%20This%0Awork%20investigates%20simultaneous%20optimization%20methods%20as%20a%20faster%20training%0Aalternative.%20In%20particular%2C%20we%20employ%20a%20collocation-based%2C%20fully%20discretized%0Aformulation%20and%20use%20IPOPT--a%20solver%20for%20large-scale%20nonlinear%20optimization--to%0Asimultaneously%20optimize%20collocation%20coefficients%20and%20neural%20network%20parameters.%0AUsing%20the%20Van%20der%20Pol%20Oscillator%20as%20a%20case%20study%2C%20we%20demonstrate%20faster%0Aconvergence%20compared%20to%20traditional%20training%20methods.%20Furthermore%2C%20we%20introduce%0Aa%20decomposition%20framework%20utilizing%20Alternating%20Direction%20Method%20of%20Multipliers%0A%28ADMM%29%20to%20effectively%20coordinate%20sub-models%20among%20data%20batches.%20Our%20results%0Ashow%20significant%20potential%20for%20%28collocation-based%29%20simultaneous%20Neural%20ODE%0Atraining%20pipelines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15642v1&entry.124074799=Read"},
{"title": "The Role of Background Information in Reducing Object Hallucination in\n  Vision-Language Models: Insights from Cutoff API Prompting", "author": "Masayo Tomita and Katsuhiko Hayashi and Tomoyuki Kaneko", "abstract": "  Vision-Language Models (VLMs) occasionally generate outputs that contradict\ninput images, constraining their reliability in real-world applications. While\nvisual prompting is reported to suppress hallucinations by augmenting prompts\nwith relevant area inside an image, the effectiveness in terms of the area\nremains uncertain. This study analyzes success and failure cases of\nAttention-driven visual prompting in object hallucination, revealing that\npreserving background context is crucial for mitigating object hallucination.\n", "link": "http://arxiv.org/abs/2502.15389v1", "date": "2025-02-21", "relevancy": 1.993, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5037}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5037}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Role%20of%20Background%20Information%20in%20Reducing%20Object%20Hallucination%20in%0A%20%20Vision-Language%20Models%3A%20Insights%20from%20Cutoff%20API%20Prompting&body=Title%3A%20The%20Role%20of%20Background%20Information%20in%20Reducing%20Object%20Hallucination%20in%0A%20%20Vision-Language%20Models%3A%20Insights%20from%20Cutoff%20API%20Prompting%0AAuthor%3A%20Masayo%20Tomita%20and%20Katsuhiko%20Hayashi%20and%20Tomoyuki%20Kaneko%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20occasionally%20generate%20outputs%20that%20contradict%0Ainput%20images%2C%20constraining%20their%20reliability%20in%20real-world%20applications.%20While%0Avisual%20prompting%20is%20reported%20to%20suppress%20hallucinations%20by%20augmenting%20prompts%0Awith%20relevant%20area%20inside%20an%20image%2C%20the%20effectiveness%20in%20terms%20of%20the%20area%0Aremains%20uncertain.%20This%20study%20analyzes%20success%20and%20failure%20cases%20of%0AAttention-driven%20visual%20prompting%20in%20object%20hallucination%2C%20revealing%20that%0Apreserving%20background%20context%20is%20crucial%20for%20mitigating%20object%20hallucination.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15389v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Role%2520of%2520Background%2520Information%2520in%2520Reducing%2520Object%2520Hallucination%2520in%250A%2520%2520Vision-Language%2520Models%253A%2520Insights%2520from%2520Cutoff%2520API%2520Prompting%26entry.906535625%3DMasayo%2520Tomita%2520and%2520Katsuhiko%2520Hayashi%2520and%2520Tomoyuki%2520Kaneko%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520occasionally%2520generate%2520outputs%2520that%2520contradict%250Ainput%2520images%252C%2520constraining%2520their%2520reliability%2520in%2520real-world%2520applications.%2520While%250Avisual%2520prompting%2520is%2520reported%2520to%2520suppress%2520hallucinations%2520by%2520augmenting%2520prompts%250Awith%2520relevant%2520area%2520inside%2520an%2520image%252C%2520the%2520effectiveness%2520in%2520terms%2520of%2520the%2520area%250Aremains%2520uncertain.%2520This%2520study%2520analyzes%2520success%2520and%2520failure%2520cases%2520of%250AAttention-driven%2520visual%2520prompting%2520in%2520object%2520hallucination%252C%2520revealing%2520that%250Apreserving%2520background%2520context%2520is%2520crucial%2520for%2520mitigating%2520object%2520hallucination.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15389v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Role%20of%20Background%20Information%20in%20Reducing%20Object%20Hallucination%20in%0A%20%20Vision-Language%20Models%3A%20Insights%20from%20Cutoff%20API%20Prompting&entry.906535625=Masayo%20Tomita%20and%20Katsuhiko%20Hayashi%20and%20Tomoyuki%20Kaneko&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20occasionally%20generate%20outputs%20that%20contradict%0Ainput%20images%2C%20constraining%20their%20reliability%20in%20real-world%20applications.%20While%0Avisual%20prompting%20is%20reported%20to%20suppress%20hallucinations%20by%20augmenting%20prompts%0Awith%20relevant%20area%20inside%20an%20image%2C%20the%20effectiveness%20in%20terms%20of%20the%20area%0Aremains%20uncertain.%20This%20study%20analyzes%20success%20and%20failure%20cases%20of%0AAttention-driven%20visual%20prompting%20in%20object%20hallucination%2C%20revealing%20that%0Apreserving%20background%20context%20is%20crucial%20for%20mitigating%20object%20hallucination.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15389v1&entry.124074799=Read"},
{"title": "Large Language Models and Mathematical Reasoning Failures", "author": "Johan Boye and Birger Moell", "abstract": "  This paper investigates the mathematical reasoning capabilities of large\nlanguage models (LLMs) using 50 newly constructed high-school-level word\nproblems. Unlike prior studies that focus solely on answer correctness, we\nrigorously analyze both final answers and solution steps to identify reasoning\nfailures. Evaluating eight state-of-the-art models - including Mixtral, Llama,\nGemini, GPT-4o, and OpenAI's o1 variants - we find that while newer models\n(e.g., o3-mini, deepseek-r1) achieve higher accuracy, all models exhibit errors\nin spatial reasoning, strategic planning, and arithmetic, sometimes producing\ncorrect answers through flawed logic. Common failure modes include unwarranted\nassumptions, over-reliance on numerical patterns, and difficulty translating\nphysical intuition into mathematical steps. Manual analysis reveals that models\nstruggle with problems requiring multi-step deduction or real-world knowledge,\ndespite possessing broad mathematical knowledge. Our results underscore the\nimportance of evaluating reasoning processes, not just answers, and caution\nagainst overestimating LLMs' problem-solving proficiency. The study highlights\npersistent gaps in LLMs' generalization abilities, emphasizing the need for\ntargeted improvements in structured reasoning and constraint handling.\n", "link": "http://arxiv.org/abs/2502.11574v2", "date": "2025-02-21", "relevancy": 1.9873, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5097}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5097}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20and%20Mathematical%20Reasoning%20Failures&body=Title%3A%20Large%20Language%20Models%20and%20Mathematical%20Reasoning%20Failures%0AAuthor%3A%20Johan%20Boye%20and%20Birger%20Moell%0AAbstract%3A%20%20%20This%20paper%20investigates%20the%20mathematical%20reasoning%20capabilities%20of%20large%0Alanguage%20models%20%28LLMs%29%20using%2050%20newly%20constructed%20high-school-level%20word%0Aproblems.%20Unlike%20prior%20studies%20that%20focus%20solely%20on%20answer%20correctness%2C%20we%0Arigorously%20analyze%20both%20final%20answers%20and%20solution%20steps%20to%20identify%20reasoning%0Afailures.%20Evaluating%20eight%20state-of-the-art%20models%20-%20including%20Mixtral%2C%20Llama%2C%0AGemini%2C%20GPT-4o%2C%20and%20OpenAI%27s%20o1%20variants%20-%20we%20find%20that%20while%20newer%20models%0A%28e.g.%2C%20o3-mini%2C%20deepseek-r1%29%20achieve%20higher%20accuracy%2C%20all%20models%20exhibit%20errors%0Ain%20spatial%20reasoning%2C%20strategic%20planning%2C%20and%20arithmetic%2C%20sometimes%20producing%0Acorrect%20answers%20through%20flawed%20logic.%20Common%20failure%20modes%20include%20unwarranted%0Aassumptions%2C%20over-reliance%20on%20numerical%20patterns%2C%20and%20difficulty%20translating%0Aphysical%20intuition%20into%20mathematical%20steps.%20Manual%20analysis%20reveals%20that%20models%0Astruggle%20with%20problems%20requiring%20multi-step%20deduction%20or%20real-world%20knowledge%2C%0Adespite%20possessing%20broad%20mathematical%20knowledge.%20Our%20results%20underscore%20the%0Aimportance%20of%20evaluating%20reasoning%20processes%2C%20not%20just%20answers%2C%20and%20caution%0Aagainst%20overestimating%20LLMs%27%20problem-solving%20proficiency.%20The%20study%20highlights%0Apersistent%20gaps%20in%20LLMs%27%20generalization%20abilities%2C%20emphasizing%20the%20need%20for%0Atargeted%20improvements%20in%20structured%20reasoning%20and%20constraint%20handling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11574v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520and%2520Mathematical%2520Reasoning%2520Failures%26entry.906535625%3DJohan%2520Boye%2520and%2520Birger%2520Moell%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520the%2520mathematical%2520reasoning%2520capabilities%2520of%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520using%252050%2520newly%2520constructed%2520high-school-level%2520word%250Aproblems.%2520Unlike%2520prior%2520studies%2520that%2520focus%2520solely%2520on%2520answer%2520correctness%252C%2520we%250Arigorously%2520analyze%2520both%2520final%2520answers%2520and%2520solution%2520steps%2520to%2520identify%2520reasoning%250Afailures.%2520Evaluating%2520eight%2520state-of-the-art%2520models%2520-%2520including%2520Mixtral%252C%2520Llama%252C%250AGemini%252C%2520GPT-4o%252C%2520and%2520OpenAI%2527s%2520o1%2520variants%2520-%2520we%2520find%2520that%2520while%2520newer%2520models%250A%2528e.g.%252C%2520o3-mini%252C%2520deepseek-r1%2529%2520achieve%2520higher%2520accuracy%252C%2520all%2520models%2520exhibit%2520errors%250Ain%2520spatial%2520reasoning%252C%2520strategic%2520planning%252C%2520and%2520arithmetic%252C%2520sometimes%2520producing%250Acorrect%2520answers%2520through%2520flawed%2520logic.%2520Common%2520failure%2520modes%2520include%2520unwarranted%250Aassumptions%252C%2520over-reliance%2520on%2520numerical%2520patterns%252C%2520and%2520difficulty%2520translating%250Aphysical%2520intuition%2520into%2520mathematical%2520steps.%2520Manual%2520analysis%2520reveals%2520that%2520models%250Astruggle%2520with%2520problems%2520requiring%2520multi-step%2520deduction%2520or%2520real-world%2520knowledge%252C%250Adespite%2520possessing%2520broad%2520mathematical%2520knowledge.%2520Our%2520results%2520underscore%2520the%250Aimportance%2520of%2520evaluating%2520reasoning%2520processes%252C%2520not%2520just%2520answers%252C%2520and%2520caution%250Aagainst%2520overestimating%2520LLMs%2527%2520problem-solving%2520proficiency.%2520The%2520study%2520highlights%250Apersistent%2520gaps%2520in%2520LLMs%2527%2520generalization%2520abilities%252C%2520emphasizing%2520the%2520need%2520for%250Atargeted%2520improvements%2520in%2520structured%2520reasoning%2520and%2520constraint%2520handling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11574v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20and%20Mathematical%20Reasoning%20Failures&entry.906535625=Johan%20Boye%20and%20Birger%20Moell&entry.1292438233=%20%20This%20paper%20investigates%20the%20mathematical%20reasoning%20capabilities%20of%20large%0Alanguage%20models%20%28LLMs%29%20using%2050%20newly%20constructed%20high-school-level%20word%0Aproblems.%20Unlike%20prior%20studies%20that%20focus%20solely%20on%20answer%20correctness%2C%20we%0Arigorously%20analyze%20both%20final%20answers%20and%20solution%20steps%20to%20identify%20reasoning%0Afailures.%20Evaluating%20eight%20state-of-the-art%20models%20-%20including%20Mixtral%2C%20Llama%2C%0AGemini%2C%20GPT-4o%2C%20and%20OpenAI%27s%20o1%20variants%20-%20we%20find%20that%20while%20newer%20models%0A%28e.g.%2C%20o3-mini%2C%20deepseek-r1%29%20achieve%20higher%20accuracy%2C%20all%20models%20exhibit%20errors%0Ain%20spatial%20reasoning%2C%20strategic%20planning%2C%20and%20arithmetic%2C%20sometimes%20producing%0Acorrect%20answers%20through%20flawed%20logic.%20Common%20failure%20modes%20include%20unwarranted%0Aassumptions%2C%20over-reliance%20on%20numerical%20patterns%2C%20and%20difficulty%20translating%0Aphysical%20intuition%20into%20mathematical%20steps.%20Manual%20analysis%20reveals%20that%20models%0Astruggle%20with%20problems%20requiring%20multi-step%20deduction%20or%20real-world%20knowledge%2C%0Adespite%20possessing%20broad%20mathematical%20knowledge.%20Our%20results%20underscore%20the%0Aimportance%20of%20evaluating%20reasoning%20processes%2C%20not%20just%20answers%2C%20and%20caution%0Aagainst%20overestimating%20LLMs%27%20problem-solving%20proficiency.%20The%20study%20highlights%0Apersistent%20gaps%20in%20LLMs%27%20generalization%20abilities%2C%20emphasizing%20the%20need%20for%0Atargeted%20improvements%20in%20structured%20reasoning%20and%20constraint%20handling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11574v2&entry.124074799=Read"},
{"title": "Generalization Guarantees for Representation Learning via Data-Dependent\n  Gaussian Mixture Priors", "author": "Milad Sefidgaran and Abdellatif Zaidi and Piotr Krasnowski", "abstract": "  We establish in-expectation and tail bounds on the generalization error of\nrepresentation learning type algorithms. The bounds are in terms of the\nrelative entropy between the distribution of the representations extracted from\nthe training and \"test'' datasets and a data-dependent symmetric prior, i.e.,\nthe Minimum Description Length (MDL) of the latent variables for the training\nand test datasets. Our bounds are shown to reflect the \"structure\" and\n\"simplicity'' of the encoder and significantly improve upon the few existing\nones for the studied model. We then use our in-expectation bound to devise a\nsuitable data-dependent regularizer; and we investigate thoroughly the\nimportant question of the selection of the prior. We propose a systematic\napproach to simultaneously learning a data-dependent Gaussian mixture prior and\nusing it as a regularizer. Interestingly, we show that a weighted attention\nmechanism emerges naturally in this procedure. Our experiments show that our\napproach outperforms the now popular Variational Information Bottleneck (VIB)\nmethod as well as the recent Category-Dependent VIB (CDVIB).\n", "link": "http://arxiv.org/abs/2502.15540v1", "date": "2025-02-21", "relevancy": 1.9867, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5001}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4983}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalization%20Guarantees%20for%20Representation%20Learning%20via%20Data-Dependent%0A%20%20Gaussian%20Mixture%20Priors&body=Title%3A%20Generalization%20Guarantees%20for%20Representation%20Learning%20via%20Data-Dependent%0A%20%20Gaussian%20Mixture%20Priors%0AAuthor%3A%20Milad%20Sefidgaran%20and%20Abdellatif%20Zaidi%20and%20Piotr%20Krasnowski%0AAbstract%3A%20%20%20We%20establish%20in-expectation%20and%20tail%20bounds%20on%20the%20generalization%20error%20of%0Arepresentation%20learning%20type%20algorithms.%20The%20bounds%20are%20in%20terms%20of%20the%0Arelative%20entropy%20between%20the%20distribution%20of%20the%20representations%20extracted%20from%0Athe%20training%20and%20%22test%27%27%20datasets%20and%20a%20data-dependent%20symmetric%20prior%2C%20i.e.%2C%0Athe%20Minimum%20Description%20Length%20%28MDL%29%20of%20the%20latent%20variables%20for%20the%20training%0Aand%20test%20datasets.%20Our%20bounds%20are%20shown%20to%20reflect%20the%20%22structure%22%20and%0A%22simplicity%27%27%20of%20the%20encoder%20and%20significantly%20improve%20upon%20the%20few%20existing%0Aones%20for%20the%20studied%20model.%20We%20then%20use%20our%20in-expectation%20bound%20to%20devise%20a%0Asuitable%20data-dependent%20regularizer%3B%20and%20we%20investigate%20thoroughly%20the%0Aimportant%20question%20of%20the%20selection%20of%20the%20prior.%20We%20propose%20a%20systematic%0Aapproach%20to%20simultaneously%20learning%20a%20data-dependent%20Gaussian%20mixture%20prior%20and%0Ausing%20it%20as%20a%20regularizer.%20Interestingly%2C%20we%20show%20that%20a%20weighted%20attention%0Amechanism%20emerges%20naturally%20in%20this%20procedure.%20Our%20experiments%20show%20that%20our%0Aapproach%20outperforms%20the%20now%20popular%20Variational%20Information%20Bottleneck%20%28VIB%29%0Amethod%20as%20well%20as%20the%20recent%20Category-Dependent%20VIB%20%28CDVIB%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15540v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralization%2520Guarantees%2520for%2520Representation%2520Learning%2520via%2520Data-Dependent%250A%2520%2520Gaussian%2520Mixture%2520Priors%26entry.906535625%3DMilad%2520Sefidgaran%2520and%2520Abdellatif%2520Zaidi%2520and%2520Piotr%2520Krasnowski%26entry.1292438233%3D%2520%2520We%2520establish%2520in-expectation%2520and%2520tail%2520bounds%2520on%2520the%2520generalization%2520error%2520of%250Arepresentation%2520learning%2520type%2520algorithms.%2520The%2520bounds%2520are%2520in%2520terms%2520of%2520the%250Arelative%2520entropy%2520between%2520the%2520distribution%2520of%2520the%2520representations%2520extracted%2520from%250Athe%2520training%2520and%2520%2522test%2527%2527%2520datasets%2520and%2520a%2520data-dependent%2520symmetric%2520prior%252C%2520i.e.%252C%250Athe%2520Minimum%2520Description%2520Length%2520%2528MDL%2529%2520of%2520the%2520latent%2520variables%2520for%2520the%2520training%250Aand%2520test%2520datasets.%2520Our%2520bounds%2520are%2520shown%2520to%2520reflect%2520the%2520%2522structure%2522%2520and%250A%2522simplicity%2527%2527%2520of%2520the%2520encoder%2520and%2520significantly%2520improve%2520upon%2520the%2520few%2520existing%250Aones%2520for%2520the%2520studied%2520model.%2520We%2520then%2520use%2520our%2520in-expectation%2520bound%2520to%2520devise%2520a%250Asuitable%2520data-dependent%2520regularizer%253B%2520and%2520we%2520investigate%2520thoroughly%2520the%250Aimportant%2520question%2520of%2520the%2520selection%2520of%2520the%2520prior.%2520We%2520propose%2520a%2520systematic%250Aapproach%2520to%2520simultaneously%2520learning%2520a%2520data-dependent%2520Gaussian%2520mixture%2520prior%2520and%250Ausing%2520it%2520as%2520a%2520regularizer.%2520Interestingly%252C%2520we%2520show%2520that%2520a%2520weighted%2520attention%250Amechanism%2520emerges%2520naturally%2520in%2520this%2520procedure.%2520Our%2520experiments%2520show%2520that%2520our%250Aapproach%2520outperforms%2520the%2520now%2520popular%2520Variational%2520Information%2520Bottleneck%2520%2528VIB%2529%250Amethod%2520as%2520well%2520as%2520the%2520recent%2520Category-Dependent%2520VIB%2520%2528CDVIB%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15540v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalization%20Guarantees%20for%20Representation%20Learning%20via%20Data-Dependent%0A%20%20Gaussian%20Mixture%20Priors&entry.906535625=Milad%20Sefidgaran%20and%20Abdellatif%20Zaidi%20and%20Piotr%20Krasnowski&entry.1292438233=%20%20We%20establish%20in-expectation%20and%20tail%20bounds%20on%20the%20generalization%20error%20of%0Arepresentation%20learning%20type%20algorithms.%20The%20bounds%20are%20in%20terms%20of%20the%0Arelative%20entropy%20between%20the%20distribution%20of%20the%20representations%20extracted%20from%0Athe%20training%20and%20%22test%27%27%20datasets%20and%20a%20data-dependent%20symmetric%20prior%2C%20i.e.%2C%0Athe%20Minimum%20Description%20Length%20%28MDL%29%20of%20the%20latent%20variables%20for%20the%20training%0Aand%20test%20datasets.%20Our%20bounds%20are%20shown%20to%20reflect%20the%20%22structure%22%20and%0A%22simplicity%27%27%20of%20the%20encoder%20and%20significantly%20improve%20upon%20the%20few%20existing%0Aones%20for%20the%20studied%20model.%20We%20then%20use%20our%20in-expectation%20bound%20to%20devise%20a%0Asuitable%20data-dependent%20regularizer%3B%20and%20we%20investigate%20thoroughly%20the%0Aimportant%20question%20of%20the%20selection%20of%20the%20prior.%20We%20propose%20a%20systematic%0Aapproach%20to%20simultaneously%20learning%20a%20data-dependent%20Gaussian%20mixture%20prior%20and%0Ausing%20it%20as%20a%20regularizer.%20Interestingly%2C%20we%20show%20that%20a%20weighted%20attention%0Amechanism%20emerges%20naturally%20in%20this%20procedure.%20Our%20experiments%20show%20that%20our%0Aapproach%20outperforms%20the%20now%20popular%20Variational%20Information%20Bottleneck%20%28VIB%29%0Amethod%20as%20well%20as%20the%20recent%20Category-Dependent%20VIB%20%28CDVIB%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15540v1&entry.124074799=Read"},
{"title": "Evaluating Multimodal Generative AI with Korean Educational Standards", "author": "Sanghee Park and Geewook Kim", "abstract": "  This paper presents the Korean National Educational Test Benchmark (KoNET), a\nnew benchmark designed to evaluate Multimodal Generative AI Systems using\nKorean national educational tests. KoNET comprises four exams: the Korean\nElementary General Educational Development Test (KoEGED), Middle (KoMGED), High\n(KoHGED), and College Scholastic Ability Test (KoCSAT). These exams are\nrenowned for their rigorous standards and diverse questions, facilitating a\ncomprehensive analysis of AI performance across different educational levels.\nBy focusing on Korean, KoNET provides insights into model performance in\nless-explored languages. We assess a range of models - open-source,\nopen-access, and closed APIs - by examining difficulties, subject diversity,\nand human error rates. The code and dataset builder will be made fully\nopen-sourced at https://github.com/naver-ai/KoNET.\n", "link": "http://arxiv.org/abs/2502.15422v1", "date": "2025-02-21", "relevancy": 1.9776, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4966}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4955}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Multimodal%20Generative%20AI%20with%20Korean%20Educational%20Standards&body=Title%3A%20Evaluating%20Multimodal%20Generative%20AI%20with%20Korean%20Educational%20Standards%0AAuthor%3A%20Sanghee%20Park%20and%20Geewook%20Kim%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20Korean%20National%20Educational%20Test%20Benchmark%20%28KoNET%29%2C%20a%0Anew%20benchmark%20designed%20to%20evaluate%20Multimodal%20Generative%20AI%20Systems%20using%0AKorean%20national%20educational%20tests.%20KoNET%20comprises%20four%20exams%3A%20the%20Korean%0AElementary%20General%20Educational%20Development%20Test%20%28KoEGED%29%2C%20Middle%20%28KoMGED%29%2C%20High%0A%28KoHGED%29%2C%20and%20College%20Scholastic%20Ability%20Test%20%28KoCSAT%29.%20These%20exams%20are%0Arenowned%20for%20their%20rigorous%20standards%20and%20diverse%20questions%2C%20facilitating%20a%0Acomprehensive%20analysis%20of%20AI%20performance%20across%20different%20educational%20levels.%0ABy%20focusing%20on%20Korean%2C%20KoNET%20provides%20insights%20into%20model%20performance%20in%0Aless-explored%20languages.%20We%20assess%20a%20range%20of%20models%20-%20open-source%2C%0Aopen-access%2C%20and%20closed%20APIs%20-%20by%20examining%20difficulties%2C%20subject%20diversity%2C%0Aand%20human%20error%20rates.%20The%20code%20and%20dataset%20builder%20will%20be%20made%20fully%0Aopen-sourced%20at%20https%3A//github.com/naver-ai/KoNET.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15422v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Multimodal%2520Generative%2520AI%2520with%2520Korean%2520Educational%2520Standards%26entry.906535625%3DSanghee%2520Park%2520and%2520Geewook%2520Kim%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520Korean%2520National%2520Educational%2520Test%2520Benchmark%2520%2528KoNET%2529%252C%2520a%250Anew%2520benchmark%2520designed%2520to%2520evaluate%2520Multimodal%2520Generative%2520AI%2520Systems%2520using%250AKorean%2520national%2520educational%2520tests.%2520KoNET%2520comprises%2520four%2520exams%253A%2520the%2520Korean%250AElementary%2520General%2520Educational%2520Development%2520Test%2520%2528KoEGED%2529%252C%2520Middle%2520%2528KoMGED%2529%252C%2520High%250A%2528KoHGED%2529%252C%2520and%2520College%2520Scholastic%2520Ability%2520Test%2520%2528KoCSAT%2529.%2520These%2520exams%2520are%250Arenowned%2520for%2520their%2520rigorous%2520standards%2520and%2520diverse%2520questions%252C%2520facilitating%2520a%250Acomprehensive%2520analysis%2520of%2520AI%2520performance%2520across%2520different%2520educational%2520levels.%250ABy%2520focusing%2520on%2520Korean%252C%2520KoNET%2520provides%2520insights%2520into%2520model%2520performance%2520in%250Aless-explored%2520languages.%2520We%2520assess%2520a%2520range%2520of%2520models%2520-%2520open-source%252C%250Aopen-access%252C%2520and%2520closed%2520APIs%2520-%2520by%2520examining%2520difficulties%252C%2520subject%2520diversity%252C%250Aand%2520human%2520error%2520rates.%2520The%2520code%2520and%2520dataset%2520builder%2520will%2520be%2520made%2520fully%250Aopen-sourced%2520at%2520https%253A//github.com/naver-ai/KoNET.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15422v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Multimodal%20Generative%20AI%20with%20Korean%20Educational%20Standards&entry.906535625=Sanghee%20Park%20and%20Geewook%20Kim&entry.1292438233=%20%20This%20paper%20presents%20the%20Korean%20National%20Educational%20Test%20Benchmark%20%28KoNET%29%2C%20a%0Anew%20benchmark%20designed%20to%20evaluate%20Multimodal%20Generative%20AI%20Systems%20using%0AKorean%20national%20educational%20tests.%20KoNET%20comprises%20four%20exams%3A%20the%20Korean%0AElementary%20General%20Educational%20Development%20Test%20%28KoEGED%29%2C%20Middle%20%28KoMGED%29%2C%20High%0A%28KoHGED%29%2C%20and%20College%20Scholastic%20Ability%20Test%20%28KoCSAT%29.%20These%20exams%20are%0Arenowned%20for%20their%20rigorous%20standards%20and%20diverse%20questions%2C%20facilitating%20a%0Acomprehensive%20analysis%20of%20AI%20performance%20across%20different%20educational%20levels.%0ABy%20focusing%20on%20Korean%2C%20KoNET%20provides%20insights%20into%20model%20performance%20in%0Aless-explored%20languages.%20We%20assess%20a%20range%20of%20models%20-%20open-source%2C%0Aopen-access%2C%20and%20closed%20APIs%20-%20by%20examining%20difficulties%2C%20subject%20diversity%2C%0Aand%20human%20error%20rates.%20The%20code%20and%20dataset%20builder%20will%20be%20made%20fully%0Aopen-sourced%20at%20https%3A//github.com/naver-ai/KoNET.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15422v1&entry.124074799=Read"},
{"title": "SWAN: SGD with Normalization and Whitening Enables Stateless LLM\n  Training", "author": "Chao Ma and Wenbo Gong and Meyer Scetbon and Edward Meeds", "abstract": "  Adaptive optimizers such as Adam (Kingma & Ba, 2015) have been central to the\nsuccess of large language models. However, they often require to maintain\noptimizer states throughout training, which can result in memory requirements\nseveral times greater than the model footprint. This overhead imposes\nconstraints on scalability and computational efficiency. Stochastic Gradient\nDescent (SGD), in contrast, is a stateless optimizer, as it does not track\nstate variables during training. Consequently, it achieves optimal memory\nefficiency. However, its capability in LLM training is limited (Zhao et al.,\n2024b). In this work, we show that pre-processing SGD in a stateless manner can\nachieve the same performance as the Adam optimizer for LLM training, while\ndrastically reducing the memory cost. Specifically, we propose to pre-process\nthe instantaneous stochastic gradients using normalization and whitening. We\nshow that normalization stabilizes gradient distributions, and whitening\ncounteracts the local curvature of the loss landscape. This results in SWAN\n(SGD with Whitening And Normalization), a stochastic optimizer that eliminates\nthe need to store any optimizer states. Empirically, SWAN has the same memory\nfootprint as SGD, achieving $\\approx 50\\%$ reduction on total end-to-end memory\ncompared to Adam. In language modeling tasks, SWAN demonstrates comparable or\neven better performance than Adam: when pre-training the LLaMA model with 350M\nand 1.3B parameters, SWAN achieves a 2x speedup by reaching the same evaluation\nperplexity using half as many tokens.\n", "link": "http://arxiv.org/abs/2412.13148v3", "date": "2025-02-21", "relevancy": 1.9707, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5042}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4997}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SWAN%3A%20SGD%20with%20Normalization%20and%20Whitening%20Enables%20Stateless%20LLM%0A%20%20Training&body=Title%3A%20SWAN%3A%20SGD%20with%20Normalization%20and%20Whitening%20Enables%20Stateless%20LLM%0A%20%20Training%0AAuthor%3A%20Chao%20Ma%20and%20Wenbo%20Gong%20and%20Meyer%20Scetbon%20and%20Edward%20Meeds%0AAbstract%3A%20%20%20Adaptive%20optimizers%20such%20as%20Adam%20%28Kingma%20%26%20Ba%2C%202015%29%20have%20been%20central%20to%20the%0Asuccess%20of%20large%20language%20models.%20However%2C%20they%20often%20require%20to%20maintain%0Aoptimizer%20states%20throughout%20training%2C%20which%20can%20result%20in%20memory%20requirements%0Aseveral%20times%20greater%20than%20the%20model%20footprint.%20This%20overhead%20imposes%0Aconstraints%20on%20scalability%20and%20computational%20efficiency.%20Stochastic%20Gradient%0ADescent%20%28SGD%29%2C%20in%20contrast%2C%20is%20a%20stateless%20optimizer%2C%20as%20it%20does%20not%20track%0Astate%20variables%20during%20training.%20Consequently%2C%20it%20achieves%20optimal%20memory%0Aefficiency.%20However%2C%20its%20capability%20in%20LLM%20training%20is%20limited%20%28Zhao%20et%20al.%2C%0A2024b%29.%20In%20this%20work%2C%20we%20show%20that%20pre-processing%20SGD%20in%20a%20stateless%20manner%20can%0Aachieve%20the%20same%20performance%20as%20the%20Adam%20optimizer%20for%20LLM%20training%2C%20while%0Adrastically%20reducing%20the%20memory%20cost.%20Specifically%2C%20we%20propose%20to%20pre-process%0Athe%20instantaneous%20stochastic%20gradients%20using%20normalization%20and%20whitening.%20We%0Ashow%20that%20normalization%20stabilizes%20gradient%20distributions%2C%20and%20whitening%0Acounteracts%20the%20local%20curvature%20of%20the%20loss%20landscape.%20This%20results%20in%20SWAN%0A%28SGD%20with%20Whitening%20And%20Normalization%29%2C%20a%20stochastic%20optimizer%20that%20eliminates%0Athe%20need%20to%20store%20any%20optimizer%20states.%20Empirically%2C%20SWAN%20has%20the%20same%20memory%0Afootprint%20as%20SGD%2C%20achieving%20%24%5Capprox%2050%5C%25%24%20reduction%20on%20total%20end-to-end%20memory%0Acompared%20to%20Adam.%20In%20language%20modeling%20tasks%2C%20SWAN%20demonstrates%20comparable%20or%0Aeven%20better%20performance%20than%20Adam%3A%20when%20pre-training%20the%20LLaMA%20model%20with%20350M%0Aand%201.3B%20parameters%2C%20SWAN%20achieves%20a%202x%20speedup%20by%20reaching%20the%20same%20evaluation%0Aperplexity%20using%20half%20as%20many%20tokens.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13148v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSWAN%253A%2520SGD%2520with%2520Normalization%2520and%2520Whitening%2520Enables%2520Stateless%2520LLM%250A%2520%2520Training%26entry.906535625%3DChao%2520Ma%2520and%2520Wenbo%2520Gong%2520and%2520Meyer%2520Scetbon%2520and%2520Edward%2520Meeds%26entry.1292438233%3D%2520%2520Adaptive%2520optimizers%2520such%2520as%2520Adam%2520%2528Kingma%2520%2526%2520Ba%252C%25202015%2529%2520have%2520been%2520central%2520to%2520the%250Asuccess%2520of%2520large%2520language%2520models.%2520However%252C%2520they%2520often%2520require%2520to%2520maintain%250Aoptimizer%2520states%2520throughout%2520training%252C%2520which%2520can%2520result%2520in%2520memory%2520requirements%250Aseveral%2520times%2520greater%2520than%2520the%2520model%2520footprint.%2520This%2520overhead%2520imposes%250Aconstraints%2520on%2520scalability%2520and%2520computational%2520efficiency.%2520Stochastic%2520Gradient%250ADescent%2520%2528SGD%2529%252C%2520in%2520contrast%252C%2520is%2520a%2520stateless%2520optimizer%252C%2520as%2520it%2520does%2520not%2520track%250Astate%2520variables%2520during%2520training.%2520Consequently%252C%2520it%2520achieves%2520optimal%2520memory%250Aefficiency.%2520However%252C%2520its%2520capability%2520in%2520LLM%2520training%2520is%2520limited%2520%2528Zhao%2520et%2520al.%252C%250A2024b%2529.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520pre-processing%2520SGD%2520in%2520a%2520stateless%2520manner%2520can%250Aachieve%2520the%2520same%2520performance%2520as%2520the%2520Adam%2520optimizer%2520for%2520LLM%2520training%252C%2520while%250Adrastically%2520reducing%2520the%2520memory%2520cost.%2520Specifically%252C%2520we%2520propose%2520to%2520pre-process%250Athe%2520instantaneous%2520stochastic%2520gradients%2520using%2520normalization%2520and%2520whitening.%2520We%250Ashow%2520that%2520normalization%2520stabilizes%2520gradient%2520distributions%252C%2520and%2520whitening%250Acounteracts%2520the%2520local%2520curvature%2520of%2520the%2520loss%2520landscape.%2520This%2520results%2520in%2520SWAN%250A%2528SGD%2520with%2520Whitening%2520And%2520Normalization%2529%252C%2520a%2520stochastic%2520optimizer%2520that%2520eliminates%250Athe%2520need%2520to%2520store%2520any%2520optimizer%2520states.%2520Empirically%252C%2520SWAN%2520has%2520the%2520same%2520memory%250Afootprint%2520as%2520SGD%252C%2520achieving%2520%2524%255Capprox%252050%255C%2525%2524%2520reduction%2520on%2520total%2520end-to-end%2520memory%250Acompared%2520to%2520Adam.%2520In%2520language%2520modeling%2520tasks%252C%2520SWAN%2520demonstrates%2520comparable%2520or%250Aeven%2520better%2520performance%2520than%2520Adam%253A%2520when%2520pre-training%2520the%2520LLaMA%2520model%2520with%2520350M%250Aand%25201.3B%2520parameters%252C%2520SWAN%2520achieves%2520a%25202x%2520speedup%2520by%2520reaching%2520the%2520same%2520evaluation%250Aperplexity%2520using%2520half%2520as%2520many%2520tokens.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13148v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SWAN%3A%20SGD%20with%20Normalization%20and%20Whitening%20Enables%20Stateless%20LLM%0A%20%20Training&entry.906535625=Chao%20Ma%20and%20Wenbo%20Gong%20and%20Meyer%20Scetbon%20and%20Edward%20Meeds&entry.1292438233=%20%20Adaptive%20optimizers%20such%20as%20Adam%20%28Kingma%20%26%20Ba%2C%202015%29%20have%20been%20central%20to%20the%0Asuccess%20of%20large%20language%20models.%20However%2C%20they%20often%20require%20to%20maintain%0Aoptimizer%20states%20throughout%20training%2C%20which%20can%20result%20in%20memory%20requirements%0Aseveral%20times%20greater%20than%20the%20model%20footprint.%20This%20overhead%20imposes%0Aconstraints%20on%20scalability%20and%20computational%20efficiency.%20Stochastic%20Gradient%0ADescent%20%28SGD%29%2C%20in%20contrast%2C%20is%20a%20stateless%20optimizer%2C%20as%20it%20does%20not%20track%0Astate%20variables%20during%20training.%20Consequently%2C%20it%20achieves%20optimal%20memory%0Aefficiency.%20However%2C%20its%20capability%20in%20LLM%20training%20is%20limited%20%28Zhao%20et%20al.%2C%0A2024b%29.%20In%20this%20work%2C%20we%20show%20that%20pre-processing%20SGD%20in%20a%20stateless%20manner%20can%0Aachieve%20the%20same%20performance%20as%20the%20Adam%20optimizer%20for%20LLM%20training%2C%20while%0Adrastically%20reducing%20the%20memory%20cost.%20Specifically%2C%20we%20propose%20to%20pre-process%0Athe%20instantaneous%20stochastic%20gradients%20using%20normalization%20and%20whitening.%20We%0Ashow%20that%20normalization%20stabilizes%20gradient%20distributions%2C%20and%20whitening%0Acounteracts%20the%20local%20curvature%20of%20the%20loss%20landscape.%20This%20results%20in%20SWAN%0A%28SGD%20with%20Whitening%20And%20Normalization%29%2C%20a%20stochastic%20optimizer%20that%20eliminates%0Athe%20need%20to%20store%20any%20optimizer%20states.%20Empirically%2C%20SWAN%20has%20the%20same%20memory%0Afootprint%20as%20SGD%2C%20achieving%20%24%5Capprox%2050%5C%25%24%20reduction%20on%20total%20end-to-end%20memory%0Acompared%20to%20Adam.%20In%20language%20modeling%20tasks%2C%20SWAN%20demonstrates%20comparable%20or%0Aeven%20better%20performance%20than%20Adam%3A%20when%20pre-training%20the%20LLaMA%20model%20with%20350M%0Aand%201.3B%20parameters%2C%20SWAN%20achieves%20a%202x%20speedup%20by%20reaching%20the%20same%20evaluation%0Aperplexity%20using%20half%20as%20many%20tokens.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13148v3&entry.124074799=Read"},
{"title": "A Defensive Framework Against Adversarial Attacks on Machine\n  Learning-Based Network Intrusion Detection Systems", "author": "Benyamin Tafreshian and Shengzhi Zhang", "abstract": "  As cyberattacks become increasingly sophisticated, advanced Network Intrusion\nDetection Systems (NIDS) are critical for modern network security. Traditional\nsignature-based NIDS are inadequate against zero-day and evolving attacks. In\nresponse, machine learning (ML)-based NIDS have emerged as promising solutions;\nhowever, they are vulnerable to adversarial evasion attacks that subtly\nmanipulate network traffic to bypass detection. To address this vulnerability,\nwe propose a novel defensive framework that enhances the robustness of ML-based\nNIDS by simultaneously integrating adversarial training, dataset balancing\ntechniques, advanced feature engineering, ensemble learning, and extensive\nmodel fine-tuning. We validate our framework using the NSL-KDD and UNSW-NB15\ndatasets. Experimental results show, on average, a 35% increase in detection\naccuracy and a 12.5% reduction in false positives compared to baseline models,\nparticularly under adversarial conditions. The proposed defense against\nadversarial attacks significantly advances the practical deployment of robust\nML-based NIDS in real-world networks.\n", "link": "http://arxiv.org/abs/2502.15561v1", "date": "2025-02-21", "relevancy": 1.9697, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4951}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4942}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Defensive%20Framework%20Against%20Adversarial%20Attacks%20on%20Machine%0A%20%20Learning-Based%20Network%20Intrusion%20Detection%20Systems&body=Title%3A%20A%20Defensive%20Framework%20Against%20Adversarial%20Attacks%20on%20Machine%0A%20%20Learning-Based%20Network%20Intrusion%20Detection%20Systems%0AAuthor%3A%20Benyamin%20Tafreshian%20and%20Shengzhi%20Zhang%0AAbstract%3A%20%20%20As%20cyberattacks%20become%20increasingly%20sophisticated%2C%20advanced%20Network%20Intrusion%0ADetection%20Systems%20%28NIDS%29%20are%20critical%20for%20modern%20network%20security.%20Traditional%0Asignature-based%20NIDS%20are%20inadequate%20against%20zero-day%20and%20evolving%20attacks.%20In%0Aresponse%2C%20machine%20learning%20%28ML%29-based%20NIDS%20have%20emerged%20as%20promising%20solutions%3B%0Ahowever%2C%20they%20are%20vulnerable%20to%20adversarial%20evasion%20attacks%20that%20subtly%0Amanipulate%20network%20traffic%20to%20bypass%20detection.%20To%20address%20this%20vulnerability%2C%0Awe%20propose%20a%20novel%20defensive%20framework%20that%20enhances%20the%20robustness%20of%20ML-based%0ANIDS%20by%20simultaneously%20integrating%20adversarial%20training%2C%20dataset%20balancing%0Atechniques%2C%20advanced%20feature%20engineering%2C%20ensemble%20learning%2C%20and%20extensive%0Amodel%20fine-tuning.%20We%20validate%20our%20framework%20using%20the%20NSL-KDD%20and%20UNSW-NB15%0Adatasets.%20Experimental%20results%20show%2C%20on%20average%2C%20a%2035%25%20increase%20in%20detection%0Aaccuracy%20and%20a%2012.5%25%20reduction%20in%20false%20positives%20compared%20to%20baseline%20models%2C%0Aparticularly%20under%20adversarial%20conditions.%20The%20proposed%20defense%20against%0Aadversarial%20attacks%20significantly%20advances%20the%20practical%20deployment%20of%20robust%0AML-based%20NIDS%20in%20real-world%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15561v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Defensive%2520Framework%2520Against%2520Adversarial%2520Attacks%2520on%2520Machine%250A%2520%2520Learning-Based%2520Network%2520Intrusion%2520Detection%2520Systems%26entry.906535625%3DBenyamin%2520Tafreshian%2520and%2520Shengzhi%2520Zhang%26entry.1292438233%3D%2520%2520As%2520cyberattacks%2520become%2520increasingly%2520sophisticated%252C%2520advanced%2520Network%2520Intrusion%250ADetection%2520Systems%2520%2528NIDS%2529%2520are%2520critical%2520for%2520modern%2520network%2520security.%2520Traditional%250Asignature-based%2520NIDS%2520are%2520inadequate%2520against%2520zero-day%2520and%2520evolving%2520attacks.%2520In%250Aresponse%252C%2520machine%2520learning%2520%2528ML%2529-based%2520NIDS%2520have%2520emerged%2520as%2520promising%2520solutions%253B%250Ahowever%252C%2520they%2520are%2520vulnerable%2520to%2520adversarial%2520evasion%2520attacks%2520that%2520subtly%250Amanipulate%2520network%2520traffic%2520to%2520bypass%2520detection.%2520To%2520address%2520this%2520vulnerability%252C%250Awe%2520propose%2520a%2520novel%2520defensive%2520framework%2520that%2520enhances%2520the%2520robustness%2520of%2520ML-based%250ANIDS%2520by%2520simultaneously%2520integrating%2520adversarial%2520training%252C%2520dataset%2520balancing%250Atechniques%252C%2520advanced%2520feature%2520engineering%252C%2520ensemble%2520learning%252C%2520and%2520extensive%250Amodel%2520fine-tuning.%2520We%2520validate%2520our%2520framework%2520using%2520the%2520NSL-KDD%2520and%2520UNSW-NB15%250Adatasets.%2520Experimental%2520results%2520show%252C%2520on%2520average%252C%2520a%252035%2525%2520increase%2520in%2520detection%250Aaccuracy%2520and%2520a%252012.5%2525%2520reduction%2520in%2520false%2520positives%2520compared%2520to%2520baseline%2520models%252C%250Aparticularly%2520under%2520adversarial%2520conditions.%2520The%2520proposed%2520defense%2520against%250Aadversarial%2520attacks%2520significantly%2520advances%2520the%2520practical%2520deployment%2520of%2520robust%250AML-based%2520NIDS%2520in%2520real-world%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15561v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Defensive%20Framework%20Against%20Adversarial%20Attacks%20on%20Machine%0A%20%20Learning-Based%20Network%20Intrusion%20Detection%20Systems&entry.906535625=Benyamin%20Tafreshian%20and%20Shengzhi%20Zhang&entry.1292438233=%20%20As%20cyberattacks%20become%20increasingly%20sophisticated%2C%20advanced%20Network%20Intrusion%0ADetection%20Systems%20%28NIDS%29%20are%20critical%20for%20modern%20network%20security.%20Traditional%0Asignature-based%20NIDS%20are%20inadequate%20against%20zero-day%20and%20evolving%20attacks.%20In%0Aresponse%2C%20machine%20learning%20%28ML%29-based%20NIDS%20have%20emerged%20as%20promising%20solutions%3B%0Ahowever%2C%20they%20are%20vulnerable%20to%20adversarial%20evasion%20attacks%20that%20subtly%0Amanipulate%20network%20traffic%20to%20bypass%20detection.%20To%20address%20this%20vulnerability%2C%0Awe%20propose%20a%20novel%20defensive%20framework%20that%20enhances%20the%20robustness%20of%20ML-based%0ANIDS%20by%20simultaneously%20integrating%20adversarial%20training%2C%20dataset%20balancing%0Atechniques%2C%20advanced%20feature%20engineering%2C%20ensemble%20learning%2C%20and%20extensive%0Amodel%20fine-tuning.%20We%20validate%20our%20framework%20using%20the%20NSL-KDD%20and%20UNSW-NB15%0Adatasets.%20Experimental%20results%20show%2C%20on%20average%2C%20a%2035%25%20increase%20in%20detection%0Aaccuracy%20and%20a%2012.5%25%20reduction%20in%20false%20positives%20compared%20to%20baseline%20models%2C%0Aparticularly%20under%20adversarial%20conditions.%20The%20proposed%20defense%20against%0Aadversarial%20attacks%20significantly%20advances%20the%20practical%20deployment%20of%20robust%0AML-based%20NIDS%20in%20real-world%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15561v1&entry.124074799=Read"},
{"title": "Giving AI Personalities Leads to More Human-Like Reasoning", "author": "Animesh Nighojkar and Bekhzodbek Moydinboyev and My Duong and John Licato", "abstract": "  In computational cognitive modeling, capturing the full spectrum of human\njudgment and decision-making processes, beyond just optimal behaviors, is a\nsignificant challenge. This study explores whether Large Language Models (LLMs)\ncan emulate the breadth of human reasoning by predicting both intuitive, fast\nSystem 1 and deliberate, slow System 2 processes. We investigate the potential\nof AI to mimic diverse reasoning behaviors across a human population,\naddressing what we call the \"full reasoning spectrum problem\". We designed\nreasoning tasks using a novel generalization of the Natural Language Inference\n(NLI) format to evaluate LLMs' ability to replicate human reasoning. The\nquestions were crafted to elicit both System 1 and System 2 responses. Human\nresponses were collected through crowd-sourcing and the entire distribution was\nmodeled, rather than just the majority of the answers. We used\npersonality-based prompting inspired by the Big Five personality model to\nelicit AI responses reflecting specific personality traits, capturing the\ndiversity of human reasoning, and exploring how personality traits influence\nLLM outputs. Combined with genetic algorithms to optimize the weighting of\nthese prompts, this method was tested alongside traditional machine learning\nmodels. The results show that LLMs can mimic human response distributions, with\nopen-source models like Llama and Mistral outperforming proprietary GPT models.\nPersonality-based prompting, especially when optimized with genetic algorithms,\nsignificantly enhanced LLMs' ability to predict human response distributions,\nsuggesting that capturing suboptimal, naturalistic reasoning may require\nmodeling techniques incorporating diverse reasoning styles and psychological\nprofiles. The study concludes that personality-based prompting combined with\ngenetic algorithms is promising for enhancing AI's 'human-ness' in reasoning.\n", "link": "http://arxiv.org/abs/2502.14155v2", "date": "2025-02-21", "relevancy": 1.9622, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4911}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4911}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Giving%20AI%20Personalities%20Leads%20to%20More%20Human-Like%20Reasoning&body=Title%3A%20Giving%20AI%20Personalities%20Leads%20to%20More%20Human-Like%20Reasoning%0AAuthor%3A%20Animesh%20Nighojkar%20and%20Bekhzodbek%20Moydinboyev%20and%20My%20Duong%20and%20John%20Licato%0AAbstract%3A%20%20%20In%20computational%20cognitive%20modeling%2C%20capturing%20the%20full%20spectrum%20of%20human%0Ajudgment%20and%20decision-making%20processes%2C%20beyond%20just%20optimal%20behaviors%2C%20is%20a%0Asignificant%20challenge.%20This%20study%20explores%20whether%20Large%20Language%20Models%20%28LLMs%29%0Acan%20emulate%20the%20breadth%20of%20human%20reasoning%20by%20predicting%20both%20intuitive%2C%20fast%0ASystem%201%20and%20deliberate%2C%20slow%20System%202%20processes.%20We%20investigate%20the%20potential%0Aof%20AI%20to%20mimic%20diverse%20reasoning%20behaviors%20across%20a%20human%20population%2C%0Aaddressing%20what%20we%20call%20the%20%22full%20reasoning%20spectrum%20problem%22.%20We%20designed%0Areasoning%20tasks%20using%20a%20novel%20generalization%20of%20the%20Natural%20Language%20Inference%0A%28NLI%29%20format%20to%20evaluate%20LLMs%27%20ability%20to%20replicate%20human%20reasoning.%20The%0Aquestions%20were%20crafted%20to%20elicit%20both%20System%201%20and%20System%202%20responses.%20Human%0Aresponses%20were%20collected%20through%20crowd-sourcing%20and%20the%20entire%20distribution%20was%0Amodeled%2C%20rather%20than%20just%20the%20majority%20of%20the%20answers.%20We%20used%0Apersonality-based%20prompting%20inspired%20by%20the%20Big%20Five%20personality%20model%20to%0Aelicit%20AI%20responses%20reflecting%20specific%20personality%20traits%2C%20capturing%20the%0Adiversity%20of%20human%20reasoning%2C%20and%20exploring%20how%20personality%20traits%20influence%0ALLM%20outputs.%20Combined%20with%20genetic%20algorithms%20to%20optimize%20the%20weighting%20of%0Athese%20prompts%2C%20this%20method%20was%20tested%20alongside%20traditional%20machine%20learning%0Amodels.%20The%20results%20show%20that%20LLMs%20can%20mimic%20human%20response%20distributions%2C%20with%0Aopen-source%20models%20like%20Llama%20and%20Mistral%20outperforming%20proprietary%20GPT%20models.%0APersonality-based%20prompting%2C%20especially%20when%20optimized%20with%20genetic%20algorithms%2C%0Asignificantly%20enhanced%20LLMs%27%20ability%20to%20predict%20human%20response%20distributions%2C%0Asuggesting%20that%20capturing%20suboptimal%2C%20naturalistic%20reasoning%20may%20require%0Amodeling%20techniques%20incorporating%20diverse%20reasoning%20styles%20and%20psychological%0Aprofiles.%20The%20study%20concludes%20that%20personality-based%20prompting%20combined%20with%0Agenetic%20algorithms%20is%20promising%20for%20enhancing%20AI%27s%20%27human-ness%27%20in%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14155v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGiving%2520AI%2520Personalities%2520Leads%2520to%2520More%2520Human-Like%2520Reasoning%26entry.906535625%3DAnimesh%2520Nighojkar%2520and%2520Bekhzodbek%2520Moydinboyev%2520and%2520My%2520Duong%2520and%2520John%2520Licato%26entry.1292438233%3D%2520%2520In%2520computational%2520cognitive%2520modeling%252C%2520capturing%2520the%2520full%2520spectrum%2520of%2520human%250Ajudgment%2520and%2520decision-making%2520processes%252C%2520beyond%2520just%2520optimal%2520behaviors%252C%2520is%2520a%250Asignificant%2520challenge.%2520This%2520study%2520explores%2520whether%2520Large%2520Language%2520Models%2520%2528LLMs%2529%250Acan%2520emulate%2520the%2520breadth%2520of%2520human%2520reasoning%2520by%2520predicting%2520both%2520intuitive%252C%2520fast%250ASystem%25201%2520and%2520deliberate%252C%2520slow%2520System%25202%2520processes.%2520We%2520investigate%2520the%2520potential%250Aof%2520AI%2520to%2520mimic%2520diverse%2520reasoning%2520behaviors%2520across%2520a%2520human%2520population%252C%250Aaddressing%2520what%2520we%2520call%2520the%2520%2522full%2520reasoning%2520spectrum%2520problem%2522.%2520We%2520designed%250Areasoning%2520tasks%2520using%2520a%2520novel%2520generalization%2520of%2520the%2520Natural%2520Language%2520Inference%250A%2528NLI%2529%2520format%2520to%2520evaluate%2520LLMs%2527%2520ability%2520to%2520replicate%2520human%2520reasoning.%2520The%250Aquestions%2520were%2520crafted%2520to%2520elicit%2520both%2520System%25201%2520and%2520System%25202%2520responses.%2520Human%250Aresponses%2520were%2520collected%2520through%2520crowd-sourcing%2520and%2520the%2520entire%2520distribution%2520was%250Amodeled%252C%2520rather%2520than%2520just%2520the%2520majority%2520of%2520the%2520answers.%2520We%2520used%250Apersonality-based%2520prompting%2520inspired%2520by%2520the%2520Big%2520Five%2520personality%2520model%2520to%250Aelicit%2520AI%2520responses%2520reflecting%2520specific%2520personality%2520traits%252C%2520capturing%2520the%250Adiversity%2520of%2520human%2520reasoning%252C%2520and%2520exploring%2520how%2520personality%2520traits%2520influence%250ALLM%2520outputs.%2520Combined%2520with%2520genetic%2520algorithms%2520to%2520optimize%2520the%2520weighting%2520of%250Athese%2520prompts%252C%2520this%2520method%2520was%2520tested%2520alongside%2520traditional%2520machine%2520learning%250Amodels.%2520The%2520results%2520show%2520that%2520LLMs%2520can%2520mimic%2520human%2520response%2520distributions%252C%2520with%250Aopen-source%2520models%2520like%2520Llama%2520and%2520Mistral%2520outperforming%2520proprietary%2520GPT%2520models.%250APersonality-based%2520prompting%252C%2520especially%2520when%2520optimized%2520with%2520genetic%2520algorithms%252C%250Asignificantly%2520enhanced%2520LLMs%2527%2520ability%2520to%2520predict%2520human%2520response%2520distributions%252C%250Asuggesting%2520that%2520capturing%2520suboptimal%252C%2520naturalistic%2520reasoning%2520may%2520require%250Amodeling%2520techniques%2520incorporating%2520diverse%2520reasoning%2520styles%2520and%2520psychological%250Aprofiles.%2520The%2520study%2520concludes%2520that%2520personality-based%2520prompting%2520combined%2520with%250Agenetic%2520algorithms%2520is%2520promising%2520for%2520enhancing%2520AI%2527s%2520%2527human-ness%2527%2520in%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14155v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Giving%20AI%20Personalities%20Leads%20to%20More%20Human-Like%20Reasoning&entry.906535625=Animesh%20Nighojkar%20and%20Bekhzodbek%20Moydinboyev%20and%20My%20Duong%20and%20John%20Licato&entry.1292438233=%20%20In%20computational%20cognitive%20modeling%2C%20capturing%20the%20full%20spectrum%20of%20human%0Ajudgment%20and%20decision-making%20processes%2C%20beyond%20just%20optimal%20behaviors%2C%20is%20a%0Asignificant%20challenge.%20This%20study%20explores%20whether%20Large%20Language%20Models%20%28LLMs%29%0Acan%20emulate%20the%20breadth%20of%20human%20reasoning%20by%20predicting%20both%20intuitive%2C%20fast%0ASystem%201%20and%20deliberate%2C%20slow%20System%202%20processes.%20We%20investigate%20the%20potential%0Aof%20AI%20to%20mimic%20diverse%20reasoning%20behaviors%20across%20a%20human%20population%2C%0Aaddressing%20what%20we%20call%20the%20%22full%20reasoning%20spectrum%20problem%22.%20We%20designed%0Areasoning%20tasks%20using%20a%20novel%20generalization%20of%20the%20Natural%20Language%20Inference%0A%28NLI%29%20format%20to%20evaluate%20LLMs%27%20ability%20to%20replicate%20human%20reasoning.%20The%0Aquestions%20were%20crafted%20to%20elicit%20both%20System%201%20and%20System%202%20responses.%20Human%0Aresponses%20were%20collected%20through%20crowd-sourcing%20and%20the%20entire%20distribution%20was%0Amodeled%2C%20rather%20than%20just%20the%20majority%20of%20the%20answers.%20We%20used%0Apersonality-based%20prompting%20inspired%20by%20the%20Big%20Five%20personality%20model%20to%0Aelicit%20AI%20responses%20reflecting%20specific%20personality%20traits%2C%20capturing%20the%0Adiversity%20of%20human%20reasoning%2C%20and%20exploring%20how%20personality%20traits%20influence%0ALLM%20outputs.%20Combined%20with%20genetic%20algorithms%20to%20optimize%20the%20weighting%20of%0Athese%20prompts%2C%20this%20method%20was%20tested%20alongside%20traditional%20machine%20learning%0Amodels.%20The%20results%20show%20that%20LLMs%20can%20mimic%20human%20response%20distributions%2C%20with%0Aopen-source%20models%20like%20Llama%20and%20Mistral%20outperforming%20proprietary%20GPT%20models.%0APersonality-based%20prompting%2C%20especially%20when%20optimized%20with%20genetic%20algorithms%2C%0Asignificantly%20enhanced%20LLMs%27%20ability%20to%20predict%20human%20response%20distributions%2C%0Asuggesting%20that%20capturing%20suboptimal%2C%20naturalistic%20reasoning%20may%20require%0Amodeling%20techniques%20incorporating%20diverse%20reasoning%20styles%20and%20psychological%0Aprofiles.%20The%20study%20concludes%20that%20personality-based%20prompting%20combined%20with%0Agenetic%20algorithms%20is%20promising%20for%20enhancing%20AI%27s%20%27human-ness%27%20in%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14155v2&entry.124074799=Read"},
{"title": "Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer\n  a Safer Path?", "author": "Yoshua Bengio and Michael Cohen and Damiano Fornasiere and Joumana Ghosn and Pietro Greiner and Matt MacDermott and S\u00f6ren Mindermann and Adam Oberman and Jesse Richardson and Oliver Richardson and Marc-Antoine Rondeau and Pierre-Luc St-Charles and David Williams-King", "abstract": "  The leading AI companies are increasingly focused on building generalist AI\nagents -- systems that can autonomously plan, act, and pursue goals across\nalmost all tasks that humans can perform. Despite how useful these systems\nmight be, unchecked AI agency poses significant risks to public safety and\nsecurity, ranging from misuse by malicious actors to a potentially irreversible\nloss of human control. We discuss how these risks arise from current AI\ntraining methods. Indeed, various scenarios and experiments have demonstrated\nthe possibility of AI agents engaging in deception or pursuing goals that were\nnot specified by human operators and that conflict with human interests, such\nas self-preservation. Following the precautionary principle, we see a strong\nneed for safer, yet still useful, alternatives to the current agency-driven\ntrajectory. Accordingly, we propose as a core building block for further\nadvances the development of a non-agentic AI system that is trustworthy and\nsafe by design, which we call Scientist AI. This system is designed to explain\nthe world from observations, as opposed to taking actions in it to imitate or\nplease humans. It comprises a world model that generates theories to explain\ndata and a question-answering inference machine. Both components operate with\nan explicit notion of uncertainty to mitigate the risks of overconfident\npredictions. In light of these considerations, a Scientist AI could be used to\nassist human researchers in accelerating scientific progress, including in AI\nsafety. In particular, our system can be employed as a guardrail against AI\nagents that might be created despite the risks involved. Ultimately, focusing\non non-agentic AI may enable the benefits of AI innovation while avoiding the\nrisks associated with the current trajectory. We hope these arguments will\nmotivate researchers, developers, and policymakers to favor this safer path.\n", "link": "http://arxiv.org/abs/2502.15657v1", "date": "2025-02-21", "relevancy": 1.9597, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5179}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4725}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Superintelligent%20Agents%20Pose%20Catastrophic%20Risks%3A%20Can%20Scientist%20AI%20Offer%0A%20%20a%20Safer%20Path%3F&body=Title%3A%20Superintelligent%20Agents%20Pose%20Catastrophic%20Risks%3A%20Can%20Scientist%20AI%20Offer%0A%20%20a%20Safer%20Path%3F%0AAuthor%3A%20Yoshua%20Bengio%20and%20Michael%20Cohen%20and%20Damiano%20Fornasiere%20and%20Joumana%20Ghosn%20and%20Pietro%20Greiner%20and%20Matt%20MacDermott%20and%20S%C3%B6ren%20Mindermann%20and%20Adam%20Oberman%20and%20Jesse%20Richardson%20and%20Oliver%20Richardson%20and%20Marc-Antoine%20Rondeau%20and%20Pierre-Luc%20St-Charles%20and%20David%20Williams-King%0AAbstract%3A%20%20%20The%20leading%20AI%20companies%20are%20increasingly%20focused%20on%20building%20generalist%20AI%0Aagents%20--%20systems%20that%20can%20autonomously%20plan%2C%20act%2C%20and%20pursue%20goals%20across%0Aalmost%20all%20tasks%20that%20humans%20can%20perform.%20Despite%20how%20useful%20these%20systems%0Amight%20be%2C%20unchecked%20AI%20agency%20poses%20significant%20risks%20to%20public%20safety%20and%0Asecurity%2C%20ranging%20from%20misuse%20by%20malicious%20actors%20to%20a%20potentially%20irreversible%0Aloss%20of%20human%20control.%20We%20discuss%20how%20these%20risks%20arise%20from%20current%20AI%0Atraining%20methods.%20Indeed%2C%20various%20scenarios%20and%20experiments%20have%20demonstrated%0Athe%20possibility%20of%20AI%20agents%20engaging%20in%20deception%20or%20pursuing%20goals%20that%20were%0Anot%20specified%20by%20human%20operators%20and%20that%20conflict%20with%20human%20interests%2C%20such%0Aas%20self-preservation.%20Following%20the%20precautionary%20principle%2C%20we%20see%20a%20strong%0Aneed%20for%20safer%2C%20yet%20still%20useful%2C%20alternatives%20to%20the%20current%20agency-driven%0Atrajectory.%20Accordingly%2C%20we%20propose%20as%20a%20core%20building%20block%20for%20further%0Aadvances%20the%20development%20of%20a%20non-agentic%20AI%20system%20that%20is%20trustworthy%20and%0Asafe%20by%20design%2C%20which%20we%20call%20Scientist%20AI.%20This%20system%20is%20designed%20to%20explain%0Athe%20world%20from%20observations%2C%20as%20opposed%20to%20taking%20actions%20in%20it%20to%20imitate%20or%0Aplease%20humans.%20It%20comprises%20a%20world%20model%20that%20generates%20theories%20to%20explain%0Adata%20and%20a%20question-answering%20inference%20machine.%20Both%20components%20operate%20with%0Aan%20explicit%20notion%20of%20uncertainty%20to%20mitigate%20the%20risks%20of%20overconfident%0Apredictions.%20In%20light%20of%20these%20considerations%2C%20a%20Scientist%20AI%20could%20be%20used%20to%0Aassist%20human%20researchers%20in%20accelerating%20scientific%20progress%2C%20including%20in%20AI%0Asafety.%20In%20particular%2C%20our%20system%20can%20be%20employed%20as%20a%20guardrail%20against%20AI%0Aagents%20that%20might%20be%20created%20despite%20the%20risks%20involved.%20Ultimately%2C%20focusing%0Aon%20non-agentic%20AI%20may%20enable%20the%20benefits%20of%20AI%20innovation%20while%20avoiding%20the%0Arisks%20associated%20with%20the%20current%20trajectory.%20We%20hope%20these%20arguments%20will%0Amotivate%20researchers%2C%20developers%2C%20and%20policymakers%20to%20favor%20this%20safer%20path.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15657v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuperintelligent%2520Agents%2520Pose%2520Catastrophic%2520Risks%253A%2520Can%2520Scientist%2520AI%2520Offer%250A%2520%2520a%2520Safer%2520Path%253F%26entry.906535625%3DYoshua%2520Bengio%2520and%2520Michael%2520Cohen%2520and%2520Damiano%2520Fornasiere%2520and%2520Joumana%2520Ghosn%2520and%2520Pietro%2520Greiner%2520and%2520Matt%2520MacDermott%2520and%2520S%25C3%25B6ren%2520Mindermann%2520and%2520Adam%2520Oberman%2520and%2520Jesse%2520Richardson%2520and%2520Oliver%2520Richardson%2520and%2520Marc-Antoine%2520Rondeau%2520and%2520Pierre-Luc%2520St-Charles%2520and%2520David%2520Williams-King%26entry.1292438233%3D%2520%2520The%2520leading%2520AI%2520companies%2520are%2520increasingly%2520focused%2520on%2520building%2520generalist%2520AI%250Aagents%2520--%2520systems%2520that%2520can%2520autonomously%2520plan%252C%2520act%252C%2520and%2520pursue%2520goals%2520across%250Aalmost%2520all%2520tasks%2520that%2520humans%2520can%2520perform.%2520Despite%2520how%2520useful%2520these%2520systems%250Amight%2520be%252C%2520unchecked%2520AI%2520agency%2520poses%2520significant%2520risks%2520to%2520public%2520safety%2520and%250Asecurity%252C%2520ranging%2520from%2520misuse%2520by%2520malicious%2520actors%2520to%2520a%2520potentially%2520irreversible%250Aloss%2520of%2520human%2520control.%2520We%2520discuss%2520how%2520these%2520risks%2520arise%2520from%2520current%2520AI%250Atraining%2520methods.%2520Indeed%252C%2520various%2520scenarios%2520and%2520experiments%2520have%2520demonstrated%250Athe%2520possibility%2520of%2520AI%2520agents%2520engaging%2520in%2520deception%2520or%2520pursuing%2520goals%2520that%2520were%250Anot%2520specified%2520by%2520human%2520operators%2520and%2520that%2520conflict%2520with%2520human%2520interests%252C%2520such%250Aas%2520self-preservation.%2520Following%2520the%2520precautionary%2520principle%252C%2520we%2520see%2520a%2520strong%250Aneed%2520for%2520safer%252C%2520yet%2520still%2520useful%252C%2520alternatives%2520to%2520the%2520current%2520agency-driven%250Atrajectory.%2520Accordingly%252C%2520we%2520propose%2520as%2520a%2520core%2520building%2520block%2520for%2520further%250Aadvances%2520the%2520development%2520of%2520a%2520non-agentic%2520AI%2520system%2520that%2520is%2520trustworthy%2520and%250Asafe%2520by%2520design%252C%2520which%2520we%2520call%2520Scientist%2520AI.%2520This%2520system%2520is%2520designed%2520to%2520explain%250Athe%2520world%2520from%2520observations%252C%2520as%2520opposed%2520to%2520taking%2520actions%2520in%2520it%2520to%2520imitate%2520or%250Aplease%2520humans.%2520It%2520comprises%2520a%2520world%2520model%2520that%2520generates%2520theories%2520to%2520explain%250Adata%2520and%2520a%2520question-answering%2520inference%2520machine.%2520Both%2520components%2520operate%2520with%250Aan%2520explicit%2520notion%2520of%2520uncertainty%2520to%2520mitigate%2520the%2520risks%2520of%2520overconfident%250Apredictions.%2520In%2520light%2520of%2520these%2520considerations%252C%2520a%2520Scientist%2520AI%2520could%2520be%2520used%2520to%250Aassist%2520human%2520researchers%2520in%2520accelerating%2520scientific%2520progress%252C%2520including%2520in%2520AI%250Asafety.%2520In%2520particular%252C%2520our%2520system%2520can%2520be%2520employed%2520as%2520a%2520guardrail%2520against%2520AI%250Aagents%2520that%2520might%2520be%2520created%2520despite%2520the%2520risks%2520involved.%2520Ultimately%252C%2520focusing%250Aon%2520non-agentic%2520AI%2520may%2520enable%2520the%2520benefits%2520of%2520AI%2520innovation%2520while%2520avoiding%2520the%250Arisks%2520associated%2520with%2520the%2520current%2520trajectory.%2520We%2520hope%2520these%2520arguments%2520will%250Amotivate%2520researchers%252C%2520developers%252C%2520and%2520policymakers%2520to%2520favor%2520this%2520safer%2520path.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15657v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Superintelligent%20Agents%20Pose%20Catastrophic%20Risks%3A%20Can%20Scientist%20AI%20Offer%0A%20%20a%20Safer%20Path%3F&entry.906535625=Yoshua%20Bengio%20and%20Michael%20Cohen%20and%20Damiano%20Fornasiere%20and%20Joumana%20Ghosn%20and%20Pietro%20Greiner%20and%20Matt%20MacDermott%20and%20S%C3%B6ren%20Mindermann%20and%20Adam%20Oberman%20and%20Jesse%20Richardson%20and%20Oliver%20Richardson%20and%20Marc-Antoine%20Rondeau%20and%20Pierre-Luc%20St-Charles%20and%20David%20Williams-King&entry.1292438233=%20%20The%20leading%20AI%20companies%20are%20increasingly%20focused%20on%20building%20generalist%20AI%0Aagents%20--%20systems%20that%20can%20autonomously%20plan%2C%20act%2C%20and%20pursue%20goals%20across%0Aalmost%20all%20tasks%20that%20humans%20can%20perform.%20Despite%20how%20useful%20these%20systems%0Amight%20be%2C%20unchecked%20AI%20agency%20poses%20significant%20risks%20to%20public%20safety%20and%0Asecurity%2C%20ranging%20from%20misuse%20by%20malicious%20actors%20to%20a%20potentially%20irreversible%0Aloss%20of%20human%20control.%20We%20discuss%20how%20these%20risks%20arise%20from%20current%20AI%0Atraining%20methods.%20Indeed%2C%20various%20scenarios%20and%20experiments%20have%20demonstrated%0Athe%20possibility%20of%20AI%20agents%20engaging%20in%20deception%20or%20pursuing%20goals%20that%20were%0Anot%20specified%20by%20human%20operators%20and%20that%20conflict%20with%20human%20interests%2C%20such%0Aas%20self-preservation.%20Following%20the%20precautionary%20principle%2C%20we%20see%20a%20strong%0Aneed%20for%20safer%2C%20yet%20still%20useful%2C%20alternatives%20to%20the%20current%20agency-driven%0Atrajectory.%20Accordingly%2C%20we%20propose%20as%20a%20core%20building%20block%20for%20further%0Aadvances%20the%20development%20of%20a%20non-agentic%20AI%20system%20that%20is%20trustworthy%20and%0Asafe%20by%20design%2C%20which%20we%20call%20Scientist%20AI.%20This%20system%20is%20designed%20to%20explain%0Athe%20world%20from%20observations%2C%20as%20opposed%20to%20taking%20actions%20in%20it%20to%20imitate%20or%0Aplease%20humans.%20It%20comprises%20a%20world%20model%20that%20generates%20theories%20to%20explain%0Adata%20and%20a%20question-answering%20inference%20machine.%20Both%20components%20operate%20with%0Aan%20explicit%20notion%20of%20uncertainty%20to%20mitigate%20the%20risks%20of%20overconfident%0Apredictions.%20In%20light%20of%20these%20considerations%2C%20a%20Scientist%20AI%20could%20be%20used%20to%0Aassist%20human%20researchers%20in%20accelerating%20scientific%20progress%2C%20including%20in%20AI%0Asafety.%20In%20particular%2C%20our%20system%20can%20be%20employed%20as%20a%20guardrail%20against%20AI%0Aagents%20that%20might%20be%20created%20despite%20the%20risks%20involved.%20Ultimately%2C%20focusing%0Aon%20non-agentic%20AI%20may%20enable%20the%20benefits%20of%20AI%20innovation%20while%20avoiding%20the%0Arisks%20associated%20with%20the%20current%20trajectory.%20We%20hope%20these%20arguments%20will%0Amotivate%20researchers%2C%20developers%2C%20and%20policymakers%20to%20favor%20this%20safer%20path.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15657v1&entry.124074799=Read"},
{"title": "Zweistein: A Dynamic Programming Evaluation Function for Einstein\n  W\u00fcrfelt Nicht!", "author": "Wei Lin. Hsueh and Tsan Sheng. Hsu", "abstract": "  This paper introduces Zweistein, a dynamic programming evaluation function\nfor Einstein W\\\"urfelt Nicht! (EWN). Instead of relying on human knowledge to\ncraft an evaluation function, Zweistein uses a data-centric approach that\neliminates the need for parameter tuning. The idea is to use a vector recording\nthe distance to the corner of all pieces. This distance vector captures the\nessence of EWN. It not only outperforms many traditional EWN evaluation\nfunctions but also won first place in the TCGA 2023 competition.\n", "link": "http://arxiv.org/abs/2502.15547v1", "date": "2025-02-21", "relevancy": 1.5952, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4146}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.392}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3765}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zweistein%3A%20A%20Dynamic%20Programming%20Evaluation%20Function%20for%20Einstein%0A%20%20W%C3%BCrfelt%20Nicht%21&body=Title%3A%20Zweistein%3A%20A%20Dynamic%20Programming%20Evaluation%20Function%20for%20Einstein%0A%20%20W%C3%BCrfelt%20Nicht%21%0AAuthor%3A%20Wei%20Lin.%20Hsueh%20and%20Tsan%20Sheng.%20Hsu%0AAbstract%3A%20%20%20This%20paper%20introduces%20Zweistein%2C%20a%20dynamic%20programming%20evaluation%20function%0Afor%20Einstein%20W%5C%22urfelt%20Nicht%21%20%28EWN%29.%20Instead%20of%20relying%20on%20human%20knowledge%20to%0Acraft%20an%20evaluation%20function%2C%20Zweistein%20uses%20a%20data-centric%20approach%20that%0Aeliminates%20the%20need%20for%20parameter%20tuning.%20The%20idea%20is%20to%20use%20a%20vector%20recording%0Athe%20distance%20to%20the%20corner%20of%20all%20pieces.%20This%20distance%20vector%20captures%20the%0Aessence%20of%20EWN.%20It%20not%20only%20outperforms%20many%20traditional%20EWN%20evaluation%0Afunctions%20but%20also%20won%20first%20place%20in%20the%20TCGA%202023%20competition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15547v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZweistein%253A%2520A%2520Dynamic%2520Programming%2520Evaluation%2520Function%2520for%2520Einstein%250A%2520%2520W%25C3%25BCrfelt%2520Nicht%2521%26entry.906535625%3DWei%2520Lin.%2520Hsueh%2520and%2520Tsan%2520Sheng.%2520Hsu%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520Zweistein%252C%2520a%2520dynamic%2520programming%2520evaluation%2520function%250Afor%2520Einstein%2520W%255C%2522urfelt%2520Nicht%2521%2520%2528EWN%2529.%2520Instead%2520of%2520relying%2520on%2520human%2520knowledge%2520to%250Acraft%2520an%2520evaluation%2520function%252C%2520Zweistein%2520uses%2520a%2520data-centric%2520approach%2520that%250Aeliminates%2520the%2520need%2520for%2520parameter%2520tuning.%2520The%2520idea%2520is%2520to%2520use%2520a%2520vector%2520recording%250Athe%2520distance%2520to%2520the%2520corner%2520of%2520all%2520pieces.%2520This%2520distance%2520vector%2520captures%2520the%250Aessence%2520of%2520EWN.%2520It%2520not%2520only%2520outperforms%2520many%2520traditional%2520EWN%2520evaluation%250Afunctions%2520but%2520also%2520won%2520first%2520place%2520in%2520the%2520TCGA%25202023%2520competition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15547v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zweistein%3A%20A%20Dynamic%20Programming%20Evaluation%20Function%20for%20Einstein%0A%20%20W%C3%BCrfelt%20Nicht%21&entry.906535625=Wei%20Lin.%20Hsueh%20and%20Tsan%20Sheng.%20Hsu&entry.1292438233=%20%20This%20paper%20introduces%20Zweistein%2C%20a%20dynamic%20programming%20evaluation%20function%0Afor%20Einstein%20W%5C%22urfelt%20Nicht%21%20%28EWN%29.%20Instead%20of%20relying%20on%20human%20knowledge%20to%0Acraft%20an%20evaluation%20function%2C%20Zweistein%20uses%20a%20data-centric%20approach%20that%0Aeliminates%20the%20need%20for%20parameter%20tuning.%20The%20idea%20is%20to%20use%20a%20vector%20recording%0Athe%20distance%20to%20the%20corner%20of%20all%20pieces.%20This%20distance%20vector%20captures%20the%0Aessence%20of%20EWN.%20It%20not%20only%20outperforms%20many%20traditional%20EWN%20evaluation%0Afunctions%20but%20also%20won%20first%20place%20in%20the%20TCGA%202023%20competition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15547v1&entry.124074799=Read"},
{"title": "RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers", "author": "Ke Cao and Jing Wang and Ao Ma and Jiasong Feng and Zhanjie Zhang and Xuanhua He and Shanyuan Liu and Bo Cheng and Dawei Leng and Yuhui Yin and Jie Zhang", "abstract": "  The Diffusion Transformer plays a pivotal role in advancing text-to-image and\ntext-to-video generation, owing primarily to its inherent scalability. However,\nexisting controlled diffusion transformer methods incur significant parameter\nand computational overheads and suffer from inefficient resource allocation due\nto their failure to account for the varying relevance of control information\nacross different transformer layers. To address this, we propose the\nRelevance-Guided Efficient Controllable Generation framework, RelaCtrl,\nenabling efficient and resource-optimized integration of control signals into\nthe Diffusion Transformer. First, we evaluate the relevance of each layer in\nthe Diffusion Transformer to the control information by assessing the\n\"ControlNet Relevance Score\"-i.e., the impact of skipping each control layer on\nboth the quality of generation and the control effectiveness during inference.\nBased on the strength of the relevance, we then tailor the positioning,\nparameter scale, and modeling capacity of the control layers to reduce\nunnecessary parameters and redundant computations. Additionally, to further\nimprove efficiency, we replace the self-attention and FFN in the commonly used\ncopy block with the carefully designed Two-Dimensional Shuffle Mixer (TDSM),\nenabling efficient implementation of both the token mixer and channel mixer.\nBoth qualitative and quantitative experimental results demonstrate that our\napproach achieves superior performance with only 15% of the parameters and\ncomputational complexity compared to PixArt-delta.\n", "link": "http://arxiv.org/abs/2502.14377v2", "date": "2025-02-21", "relevancy": 1.9116, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.7131}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6165}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6151}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RelaCtrl%3A%20Relevance-Guided%20Efficient%20Control%20for%20Diffusion%20Transformers&body=Title%3A%20RelaCtrl%3A%20Relevance-Guided%20Efficient%20Control%20for%20Diffusion%20Transformers%0AAuthor%3A%20Ke%20Cao%20and%20Jing%20Wang%20and%20Ao%20Ma%20and%20Jiasong%20Feng%20and%20Zhanjie%20Zhang%20and%20Xuanhua%20He%20and%20Shanyuan%20Liu%20and%20Bo%20Cheng%20and%20Dawei%20Leng%20and%20Yuhui%20Yin%20and%20Jie%20Zhang%0AAbstract%3A%20%20%20The%20Diffusion%20Transformer%20plays%20a%20pivotal%20role%20in%20advancing%20text-to-image%20and%0Atext-to-video%20generation%2C%20owing%20primarily%20to%20its%20inherent%20scalability.%20However%2C%0Aexisting%20controlled%20diffusion%20transformer%20methods%20incur%20significant%20parameter%0Aand%20computational%20overheads%20and%20suffer%20from%20inefficient%20resource%20allocation%20due%0Ato%20their%20failure%20to%20account%20for%20the%20varying%20relevance%20of%20control%20information%0Aacross%20different%20transformer%20layers.%20To%20address%20this%2C%20we%20propose%20the%0ARelevance-Guided%20Efficient%20Controllable%20Generation%20framework%2C%20RelaCtrl%2C%0Aenabling%20efficient%20and%20resource-optimized%20integration%20of%20control%20signals%20into%0Athe%20Diffusion%20Transformer.%20First%2C%20we%20evaluate%20the%20relevance%20of%20each%20layer%20in%0Athe%20Diffusion%20Transformer%20to%20the%20control%20information%20by%20assessing%20the%0A%22ControlNet%20Relevance%20Score%22-i.e.%2C%20the%20impact%20of%20skipping%20each%20control%20layer%20on%0Aboth%20the%20quality%20of%20generation%20and%20the%20control%20effectiveness%20during%20inference.%0ABased%20on%20the%20strength%20of%20the%20relevance%2C%20we%20then%20tailor%20the%20positioning%2C%0Aparameter%20scale%2C%20and%20modeling%20capacity%20of%20the%20control%20layers%20to%20reduce%0Aunnecessary%20parameters%20and%20redundant%20computations.%20Additionally%2C%20to%20further%0Aimprove%20efficiency%2C%20we%20replace%20the%20self-attention%20and%20FFN%20in%20the%20commonly%20used%0Acopy%20block%20with%20the%20carefully%20designed%20Two-Dimensional%20Shuffle%20Mixer%20%28TDSM%29%2C%0Aenabling%20efficient%20implementation%20of%20both%20the%20token%20mixer%20and%20channel%20mixer.%0ABoth%20qualitative%20and%20quantitative%20experimental%20results%20demonstrate%20that%20our%0Aapproach%20achieves%20superior%20performance%20with%20only%2015%25%20of%20the%20parameters%20and%0Acomputational%20complexity%20compared%20to%20PixArt-delta.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14377v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelaCtrl%253A%2520Relevance-Guided%2520Efficient%2520Control%2520for%2520Diffusion%2520Transformers%26entry.906535625%3DKe%2520Cao%2520and%2520Jing%2520Wang%2520and%2520Ao%2520Ma%2520and%2520Jiasong%2520Feng%2520and%2520Zhanjie%2520Zhang%2520and%2520Xuanhua%2520He%2520and%2520Shanyuan%2520Liu%2520and%2520Bo%2520Cheng%2520and%2520Dawei%2520Leng%2520and%2520Yuhui%2520Yin%2520and%2520Jie%2520Zhang%26entry.1292438233%3D%2520%2520The%2520Diffusion%2520Transformer%2520plays%2520a%2520pivotal%2520role%2520in%2520advancing%2520text-to-image%2520and%250Atext-to-video%2520generation%252C%2520owing%2520primarily%2520to%2520its%2520inherent%2520scalability.%2520However%252C%250Aexisting%2520controlled%2520diffusion%2520transformer%2520methods%2520incur%2520significant%2520parameter%250Aand%2520computational%2520overheads%2520and%2520suffer%2520from%2520inefficient%2520resource%2520allocation%2520due%250Ato%2520their%2520failure%2520to%2520account%2520for%2520the%2520varying%2520relevance%2520of%2520control%2520information%250Aacross%2520different%2520transformer%2520layers.%2520To%2520address%2520this%252C%2520we%2520propose%2520the%250ARelevance-Guided%2520Efficient%2520Controllable%2520Generation%2520framework%252C%2520RelaCtrl%252C%250Aenabling%2520efficient%2520and%2520resource-optimized%2520integration%2520of%2520control%2520signals%2520into%250Athe%2520Diffusion%2520Transformer.%2520First%252C%2520we%2520evaluate%2520the%2520relevance%2520of%2520each%2520layer%2520in%250Athe%2520Diffusion%2520Transformer%2520to%2520the%2520control%2520information%2520by%2520assessing%2520the%250A%2522ControlNet%2520Relevance%2520Score%2522-i.e.%252C%2520the%2520impact%2520of%2520skipping%2520each%2520control%2520layer%2520on%250Aboth%2520the%2520quality%2520of%2520generation%2520and%2520the%2520control%2520effectiveness%2520during%2520inference.%250ABased%2520on%2520the%2520strength%2520of%2520the%2520relevance%252C%2520we%2520then%2520tailor%2520the%2520positioning%252C%250Aparameter%2520scale%252C%2520and%2520modeling%2520capacity%2520of%2520the%2520control%2520layers%2520to%2520reduce%250Aunnecessary%2520parameters%2520and%2520redundant%2520computations.%2520Additionally%252C%2520to%2520further%250Aimprove%2520efficiency%252C%2520we%2520replace%2520the%2520self-attention%2520and%2520FFN%2520in%2520the%2520commonly%2520used%250Acopy%2520block%2520with%2520the%2520carefully%2520designed%2520Two-Dimensional%2520Shuffle%2520Mixer%2520%2528TDSM%2529%252C%250Aenabling%2520efficient%2520implementation%2520of%2520both%2520the%2520token%2520mixer%2520and%2520channel%2520mixer.%250ABoth%2520qualitative%2520and%2520quantitative%2520experimental%2520results%2520demonstrate%2520that%2520our%250Aapproach%2520achieves%2520superior%2520performance%2520with%2520only%252015%2525%2520of%2520the%2520parameters%2520and%250Acomputational%2520complexity%2520compared%2520to%2520PixArt-delta.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14377v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RelaCtrl%3A%20Relevance-Guided%20Efficient%20Control%20for%20Diffusion%20Transformers&entry.906535625=Ke%20Cao%20and%20Jing%20Wang%20and%20Ao%20Ma%20and%20Jiasong%20Feng%20and%20Zhanjie%20Zhang%20and%20Xuanhua%20He%20and%20Shanyuan%20Liu%20and%20Bo%20Cheng%20and%20Dawei%20Leng%20and%20Yuhui%20Yin%20and%20Jie%20Zhang&entry.1292438233=%20%20The%20Diffusion%20Transformer%20plays%20a%20pivotal%20role%20in%20advancing%20text-to-image%20and%0Atext-to-video%20generation%2C%20owing%20primarily%20to%20its%20inherent%20scalability.%20However%2C%0Aexisting%20controlled%20diffusion%20transformer%20methods%20incur%20significant%20parameter%0Aand%20computational%20overheads%20and%20suffer%20from%20inefficient%20resource%20allocation%20due%0Ato%20their%20failure%20to%20account%20for%20the%20varying%20relevance%20of%20control%20information%0Aacross%20different%20transformer%20layers.%20To%20address%20this%2C%20we%20propose%20the%0ARelevance-Guided%20Efficient%20Controllable%20Generation%20framework%2C%20RelaCtrl%2C%0Aenabling%20efficient%20and%20resource-optimized%20integration%20of%20control%20signals%20into%0Athe%20Diffusion%20Transformer.%20First%2C%20we%20evaluate%20the%20relevance%20of%20each%20layer%20in%0Athe%20Diffusion%20Transformer%20to%20the%20control%20information%20by%20assessing%20the%0A%22ControlNet%20Relevance%20Score%22-i.e.%2C%20the%20impact%20of%20skipping%20each%20control%20layer%20on%0Aboth%20the%20quality%20of%20generation%20and%20the%20control%20effectiveness%20during%20inference.%0ABased%20on%20the%20strength%20of%20the%20relevance%2C%20we%20then%20tailor%20the%20positioning%2C%0Aparameter%20scale%2C%20and%20modeling%20capacity%20of%20the%20control%20layers%20to%20reduce%0Aunnecessary%20parameters%20and%20redundant%20computations.%20Additionally%2C%20to%20further%0Aimprove%20efficiency%2C%20we%20replace%20the%20self-attention%20and%20FFN%20in%20the%20commonly%20used%0Acopy%20block%20with%20the%20carefully%20designed%20Two-Dimensional%20Shuffle%20Mixer%20%28TDSM%29%2C%0Aenabling%20efficient%20implementation%20of%20both%20the%20token%20mixer%20and%20channel%20mixer.%0ABoth%20qualitative%20and%20quantitative%20experimental%20results%20demonstrate%20that%20our%0Aapproach%20achieves%20superior%20performance%20with%20only%2015%25%20of%20the%20parameters%20and%0Acomputational%20complexity%20compared%20to%20PixArt-delta.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14377v2&entry.124074799=Read"},
{"title": "OptMATH: A Scalable Bidirectional Data Synthesis Framework for\n  Optimization Modeling", "author": "Hongliang Lu and Zhonglin Xie and Yaoyu Wu and Can Ren and Yuxuan Chen and Zaiwen Wen", "abstract": "  Despite the rapid development of large language models (LLMs), a fundamental\nchallenge persists: the lack of high-quality optimization modeling datasets\nhampers LLMs' robust modeling of practical optimization problems from natural\nlanguage descriptions (NL). This data scarcity also contributes to the\ngeneralization difficulties experienced by learning-based methods. To address\nthese challenges, we propose a scalable framework for synthesizing a\nhigh-quality dataset, named OptMATH. Starting from curated seed data with\nmathematical formulations (MF), this framework automatically generates problem\ndata (PD) with controllable complexity. Then, a back-translation step is\nemployed to obtain NL. To verify the correspondence between the NL and the PD,\na forward modeling step followed by rejection sampling is used. The accepted\npairs constitute the training part of OptMATH. Then a collection of rejected\npairs is identified and further filtered. This collection serves as a new\nbenchmark for optimization modeling, containing difficult instances whose\nlengths are much longer than these of NL4OPT and MAMO. Through extensive\nexperiments, we demonstrate that models of various sizes (0.5B-32B parameters)\ntrained on OptMATH achieve superior results on multiple modeling benchmarks,\nthereby validating the effectiveness and scalability of our approach. Our\ndataset is publicly available at https://github.com/AuroraLHL/OptMATH.\n", "link": "http://arxiv.org/abs/2502.11102v2", "date": "2025-02-21", "relevancy": 1.0266, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5181}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5139}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5079}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OptMATH%3A%20A%20Scalable%20Bidirectional%20Data%20Synthesis%20Framework%20for%0A%20%20Optimization%20Modeling&body=Title%3A%20OptMATH%3A%20A%20Scalable%20Bidirectional%20Data%20Synthesis%20Framework%20for%0A%20%20Optimization%20Modeling%0AAuthor%3A%20Hongliang%20Lu%20and%20Zhonglin%20Xie%20and%20Yaoyu%20Wu%20and%20Can%20Ren%20and%20Yuxuan%20Chen%20and%20Zaiwen%20Wen%0AAbstract%3A%20%20%20Despite%20the%20rapid%20development%20of%20large%20language%20models%20%28LLMs%29%2C%20a%20fundamental%0Achallenge%20persists%3A%20the%20lack%20of%20high-quality%20optimization%20modeling%20datasets%0Ahampers%20LLMs%27%20robust%20modeling%20of%20practical%20optimization%20problems%20from%20natural%0Alanguage%20descriptions%20%28NL%29.%20This%20data%20scarcity%20also%20contributes%20to%20the%0Ageneralization%20difficulties%20experienced%20by%20learning-based%20methods.%20To%20address%0Athese%20challenges%2C%20we%20propose%20a%20scalable%20framework%20for%20synthesizing%20a%0Ahigh-quality%20dataset%2C%20named%20OptMATH.%20Starting%20from%20curated%20seed%20data%20with%0Amathematical%20formulations%20%28MF%29%2C%20this%20framework%20automatically%20generates%20problem%0Adata%20%28PD%29%20with%20controllable%20complexity.%20Then%2C%20a%20back-translation%20step%20is%0Aemployed%20to%20obtain%20NL.%20To%20verify%20the%20correspondence%20between%20the%20NL%20and%20the%20PD%2C%0Aa%20forward%20modeling%20step%20followed%20by%20rejection%20sampling%20is%20used.%20The%20accepted%0Apairs%20constitute%20the%20training%20part%20of%20OptMATH.%20Then%20a%20collection%20of%20rejected%0Apairs%20is%20identified%20and%20further%20filtered.%20This%20collection%20serves%20as%20a%20new%0Abenchmark%20for%20optimization%20modeling%2C%20containing%20difficult%20instances%20whose%0Alengths%20are%20much%20longer%20than%20these%20of%20NL4OPT%20and%20MAMO.%20Through%20extensive%0Aexperiments%2C%20we%20demonstrate%20that%20models%20of%20various%20sizes%20%280.5B-32B%20parameters%29%0Atrained%20on%20OptMATH%20achieve%20superior%20results%20on%20multiple%20modeling%20benchmarks%2C%0Athereby%20validating%20the%20effectiveness%20and%20scalability%20of%20our%20approach.%20Our%0Adataset%20is%20publicly%20available%20at%20https%3A//github.com/AuroraLHL/OptMATH.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11102v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptMATH%253A%2520A%2520Scalable%2520Bidirectional%2520Data%2520Synthesis%2520Framework%2520for%250A%2520%2520Optimization%2520Modeling%26entry.906535625%3DHongliang%2520Lu%2520and%2520Zhonglin%2520Xie%2520and%2520Yaoyu%2520Wu%2520and%2520Can%2520Ren%2520and%2520Yuxuan%2520Chen%2520and%2520Zaiwen%2520Wen%26entry.1292438233%3D%2520%2520Despite%2520the%2520rapid%2520development%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520a%2520fundamental%250Achallenge%2520persists%253A%2520the%2520lack%2520of%2520high-quality%2520optimization%2520modeling%2520datasets%250Ahampers%2520LLMs%2527%2520robust%2520modeling%2520of%2520practical%2520optimization%2520problems%2520from%2520natural%250Alanguage%2520descriptions%2520%2528NL%2529.%2520This%2520data%2520scarcity%2520also%2520contributes%2520to%2520the%250Ageneralization%2520difficulties%2520experienced%2520by%2520learning-based%2520methods.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520propose%2520a%2520scalable%2520framework%2520for%2520synthesizing%2520a%250Ahigh-quality%2520dataset%252C%2520named%2520OptMATH.%2520Starting%2520from%2520curated%2520seed%2520data%2520with%250Amathematical%2520formulations%2520%2528MF%2529%252C%2520this%2520framework%2520automatically%2520generates%2520problem%250Adata%2520%2528PD%2529%2520with%2520controllable%2520complexity.%2520Then%252C%2520a%2520back-translation%2520step%2520is%250Aemployed%2520to%2520obtain%2520NL.%2520To%2520verify%2520the%2520correspondence%2520between%2520the%2520NL%2520and%2520the%2520PD%252C%250Aa%2520forward%2520modeling%2520step%2520followed%2520by%2520rejection%2520sampling%2520is%2520used.%2520The%2520accepted%250Apairs%2520constitute%2520the%2520training%2520part%2520of%2520OptMATH.%2520Then%2520a%2520collection%2520of%2520rejected%250Apairs%2520is%2520identified%2520and%2520further%2520filtered.%2520This%2520collection%2520serves%2520as%2520a%2520new%250Abenchmark%2520for%2520optimization%2520modeling%252C%2520containing%2520difficult%2520instances%2520whose%250Alengths%2520are%2520much%2520longer%2520than%2520these%2520of%2520NL4OPT%2520and%2520MAMO.%2520Through%2520extensive%250Aexperiments%252C%2520we%2520demonstrate%2520that%2520models%2520of%2520various%2520sizes%2520%25280.5B-32B%2520parameters%2529%250Atrained%2520on%2520OptMATH%2520achieve%2520superior%2520results%2520on%2520multiple%2520modeling%2520benchmarks%252C%250Athereby%2520validating%2520the%2520effectiveness%2520and%2520scalability%2520of%2520our%2520approach.%2520Our%250Adataset%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/AuroraLHL/OptMATH.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11102v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OptMATH%3A%20A%20Scalable%20Bidirectional%20Data%20Synthesis%20Framework%20for%0A%20%20Optimization%20Modeling&entry.906535625=Hongliang%20Lu%20and%20Zhonglin%20Xie%20and%20Yaoyu%20Wu%20and%20Can%20Ren%20and%20Yuxuan%20Chen%20and%20Zaiwen%20Wen&entry.1292438233=%20%20Despite%20the%20rapid%20development%20of%20large%20language%20models%20%28LLMs%29%2C%20a%20fundamental%0Achallenge%20persists%3A%20the%20lack%20of%20high-quality%20optimization%20modeling%20datasets%0Ahampers%20LLMs%27%20robust%20modeling%20of%20practical%20optimization%20problems%20from%20natural%0Alanguage%20descriptions%20%28NL%29.%20This%20data%20scarcity%20also%20contributes%20to%20the%0Ageneralization%20difficulties%20experienced%20by%20learning-based%20methods.%20To%20address%0Athese%20challenges%2C%20we%20propose%20a%20scalable%20framework%20for%20synthesizing%20a%0Ahigh-quality%20dataset%2C%20named%20OptMATH.%20Starting%20from%20curated%20seed%20data%20with%0Amathematical%20formulations%20%28MF%29%2C%20this%20framework%20automatically%20generates%20problem%0Adata%20%28PD%29%20with%20controllable%20complexity.%20Then%2C%20a%20back-translation%20step%20is%0Aemployed%20to%20obtain%20NL.%20To%20verify%20the%20correspondence%20between%20the%20NL%20and%20the%20PD%2C%0Aa%20forward%20modeling%20step%20followed%20by%20rejection%20sampling%20is%20used.%20The%20accepted%0Apairs%20constitute%20the%20training%20part%20of%20OptMATH.%20Then%20a%20collection%20of%20rejected%0Apairs%20is%20identified%20and%20further%20filtered.%20This%20collection%20serves%20as%20a%20new%0Abenchmark%20for%20optimization%20modeling%2C%20containing%20difficult%20instances%20whose%0Alengths%20are%20much%20longer%20than%20these%20of%20NL4OPT%20and%20MAMO.%20Through%20extensive%0Aexperiments%2C%20we%20demonstrate%20that%20models%20of%20various%20sizes%20%280.5B-32B%20parameters%29%0Atrained%20on%20OptMATH%20achieve%20superior%20results%20on%20multiple%20modeling%20benchmarks%2C%0Athereby%20validating%20the%20effectiveness%20and%20scalability%20of%20our%20approach.%20Our%0Adataset%20is%20publicly%20available%20at%20https%3A//github.com/AuroraLHL/OptMATH.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11102v2&entry.124074799=Read"},
{"title": "When Compression Meets Model Compression: Memory-Efficient Double\n  Compression for Large Language Models", "author": "Weilan Wang and Yu Mao and Dongdong Tang and Hongchao Du and Nan Guan and Chun Jason Xue", "abstract": "  Large language models (LLMs) exhibit excellent performance in various tasks.\nHowever, the memory requirements of LLMs present a great challenge when\ndeploying on memory-limited devices, even for quantized LLMs. This paper\nintroduces a framework to compress LLM after quantization further, achieving\nabout 2.2x compression ratio. A compression-aware quantization is first\nproposed to enhance model weight compressibility by re-scaling the model\nparameters before quantization, followed by a pruning method to improve\nfurther. Upon this, we notice that decompression can be a bottleneck during\npractical scenarios. We then give a detailed analysis of the trade-off between\nmemory usage and latency brought by the proposed method. A speed-adaptive\nmethod is proposed to overcome it. The experimental results show inference with\nthe compressed model can achieve a 40% reduction in memory size with negligible\nloss in accuracy and inference speed.\n", "link": "http://arxiv.org/abs/2502.15443v1", "date": "2025-02-21", "relevancy": 1.9148, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5133}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4718}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Compression%20Meets%20Model%20Compression%3A%20Memory-Efficient%20Double%0A%20%20Compression%20for%20Large%20Language%20Models&body=Title%3A%20When%20Compression%20Meets%20Model%20Compression%3A%20Memory-Efficient%20Double%0A%20%20Compression%20for%20Large%20Language%20Models%0AAuthor%3A%20Weilan%20Wang%20and%20Yu%20Mao%20and%20Dongdong%20Tang%20and%20Hongchao%20Du%20and%20Nan%20Guan%20and%20Chun%20Jason%20Xue%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20exhibit%20excellent%20performance%20in%20various%20tasks.%0AHowever%2C%20the%20memory%20requirements%20of%20LLMs%20present%20a%20great%20challenge%20when%0Adeploying%20on%20memory-limited%20devices%2C%20even%20for%20quantized%20LLMs.%20This%20paper%0Aintroduces%20a%20framework%20to%20compress%20LLM%20after%20quantization%20further%2C%20achieving%0Aabout%202.2x%20compression%20ratio.%20A%20compression-aware%20quantization%20is%20first%0Aproposed%20to%20enhance%20model%20weight%20compressibility%20by%20re-scaling%20the%20model%0Aparameters%20before%20quantization%2C%20followed%20by%20a%20pruning%20method%20to%20improve%0Afurther.%20Upon%20this%2C%20we%20notice%20that%20decompression%20can%20be%20a%20bottleneck%20during%0Apractical%20scenarios.%20We%20then%20give%20a%20detailed%20analysis%20of%20the%20trade-off%20between%0Amemory%20usage%20and%20latency%20brought%20by%20the%20proposed%20method.%20A%20speed-adaptive%0Amethod%20is%20proposed%20to%20overcome%20it.%20The%20experimental%20results%20show%20inference%20with%0Athe%20compressed%20model%20can%20achieve%20a%2040%25%20reduction%20in%20memory%20size%20with%20negligible%0Aloss%20in%20accuracy%20and%20inference%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15443v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Compression%2520Meets%2520Model%2520Compression%253A%2520Memory-Efficient%2520Double%250A%2520%2520Compression%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DWeilan%2520Wang%2520and%2520Yu%2520Mao%2520and%2520Dongdong%2520Tang%2520and%2520Hongchao%2520Du%2520and%2520Nan%2520Guan%2520and%2520Chun%2520Jason%2520Xue%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520exhibit%2520excellent%2520performance%2520in%2520various%2520tasks.%250AHowever%252C%2520the%2520memory%2520requirements%2520of%2520LLMs%2520present%2520a%2520great%2520challenge%2520when%250Adeploying%2520on%2520memory-limited%2520devices%252C%2520even%2520for%2520quantized%2520LLMs.%2520This%2520paper%250Aintroduces%2520a%2520framework%2520to%2520compress%2520LLM%2520after%2520quantization%2520further%252C%2520achieving%250Aabout%25202.2x%2520compression%2520ratio.%2520A%2520compression-aware%2520quantization%2520is%2520first%250Aproposed%2520to%2520enhance%2520model%2520weight%2520compressibility%2520by%2520re-scaling%2520the%2520model%250Aparameters%2520before%2520quantization%252C%2520followed%2520by%2520a%2520pruning%2520method%2520to%2520improve%250Afurther.%2520Upon%2520this%252C%2520we%2520notice%2520that%2520decompression%2520can%2520be%2520a%2520bottleneck%2520during%250Apractical%2520scenarios.%2520We%2520then%2520give%2520a%2520detailed%2520analysis%2520of%2520the%2520trade-off%2520between%250Amemory%2520usage%2520and%2520latency%2520brought%2520by%2520the%2520proposed%2520method.%2520A%2520speed-adaptive%250Amethod%2520is%2520proposed%2520to%2520overcome%2520it.%2520The%2520experimental%2520results%2520show%2520inference%2520with%250Athe%2520compressed%2520model%2520can%2520achieve%2520a%252040%2525%2520reduction%2520in%2520memory%2520size%2520with%2520negligible%250Aloss%2520in%2520accuracy%2520and%2520inference%2520speed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15443v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Compression%20Meets%20Model%20Compression%3A%20Memory-Efficient%20Double%0A%20%20Compression%20for%20Large%20Language%20Models&entry.906535625=Weilan%20Wang%20and%20Yu%20Mao%20and%20Dongdong%20Tang%20and%20Hongchao%20Du%20and%20Nan%20Guan%20and%20Chun%20Jason%20Xue&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20exhibit%20excellent%20performance%20in%20various%20tasks.%0AHowever%2C%20the%20memory%20requirements%20of%20LLMs%20present%20a%20great%20challenge%20when%0Adeploying%20on%20memory-limited%20devices%2C%20even%20for%20quantized%20LLMs.%20This%20paper%0Aintroduces%20a%20framework%20to%20compress%20LLM%20after%20quantization%20further%2C%20achieving%0Aabout%202.2x%20compression%20ratio.%20A%20compression-aware%20quantization%20is%20first%0Aproposed%20to%20enhance%20model%20weight%20compressibility%20by%20re-scaling%20the%20model%0Aparameters%20before%20quantization%2C%20followed%20by%20a%20pruning%20method%20to%20improve%0Afurther.%20Upon%20this%2C%20we%20notice%20that%20decompression%20can%20be%20a%20bottleneck%20during%0Apractical%20scenarios.%20We%20then%20give%20a%20detailed%20analysis%20of%20the%20trade-off%20between%0Amemory%20usage%20and%20latency%20brought%20by%20the%20proposed%20method.%20A%20speed-adaptive%0Amethod%20is%20proposed%20to%20overcome%20it.%20The%20experimental%20results%20show%20inference%20with%0Athe%20compressed%20model%20can%20achieve%20a%2040%25%20reduction%20in%20memory%20size%20with%20negligible%0Aloss%20in%20accuracy%20and%20inference%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15443v1&entry.124074799=Read"},
{"title": "Integrating Generative AI in Cybersecurity Education: Case Study\n  Insights on Pedagogical Strategies, Critical Thinking, and Responsible AI Use", "author": "Mahmoud Elkhodr and Ergun Gide", "abstract": "  The rapid advancement of Generative Artificial Intelligence (GenAI) has\nintroduced new opportunities for transforming higher education, particularly in\nfields that require analytical reasoning and regulatory compliance, such as\ncybersecurity management. This study presents a structured framework for\nintegrating GenAI tools into cybersecurity education, demonstrating their role\nin fostering critical thinking, real-world problem-solving, and regulatory\nawareness. The implementation strategy followed a two-stage approach, embedding\nGenAI within tutorial exercises and assessment tasks. Tutorials enabled\nstudents to generate, critique, and refine AI-assisted cybersecurity policies,\nwhile assessments required them to apply AI-generated outputs to real-world\nscenarios, ensuring alignment with industry standards and regulatory\nrequirements. Findings indicate that AI-assisted learning significantly\nenhanced students' ability to evaluate security policies, refine risk\nassessments, and bridge theoretical knowledge with practical application.\nStudent reflections and instructor observations revealed improvements in\nanalytical engagement, yet challenges emerged regarding AI over-reliance,\nvariability in AI literacy, and the contextual limitations of AI-generated\ncontent. Through structured intervention and research-driven refinement,\nstudents were able to recognize AI strengths as a generative tool while\nacknowledging its need for human oversight. This study further highlights the\nbroader implications of AI adoption in cybersecurity education, emphasizing the\nnecessity of balancing automation with expert judgment to cultivate\nindustry-ready professionals. Future research should explore the long-term\nimpact of AI-driven learning on cybersecurity competency, as well as the\npotential for adaptive AI-assisted assessments to further personalize and\nenhance educational outcomes.\n", "link": "http://arxiv.org/abs/2502.15357v1", "date": "2025-02-21", "relevancy": 1.9436, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5138}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4752}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20Generative%20AI%20in%20Cybersecurity%20Education%3A%20Case%20Study%0A%20%20Insights%20on%20Pedagogical%20Strategies%2C%20Critical%20Thinking%2C%20and%20Responsible%20AI%20Use&body=Title%3A%20Integrating%20Generative%20AI%20in%20Cybersecurity%20Education%3A%20Case%20Study%0A%20%20Insights%20on%20Pedagogical%20Strategies%2C%20Critical%20Thinking%2C%20and%20Responsible%20AI%20Use%0AAuthor%3A%20Mahmoud%20Elkhodr%20and%20Ergun%20Gide%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20Generative%20Artificial%20Intelligence%20%28GenAI%29%20has%0Aintroduced%20new%20opportunities%20for%20transforming%20higher%20education%2C%20particularly%20in%0Afields%20that%20require%20analytical%20reasoning%20and%20regulatory%20compliance%2C%20such%20as%0Acybersecurity%20management.%20This%20study%20presents%20a%20structured%20framework%20for%0Aintegrating%20GenAI%20tools%20into%20cybersecurity%20education%2C%20demonstrating%20their%20role%0Ain%20fostering%20critical%20thinking%2C%20real-world%20problem-solving%2C%20and%20regulatory%0Aawareness.%20The%20implementation%20strategy%20followed%20a%20two-stage%20approach%2C%20embedding%0AGenAI%20within%20tutorial%20exercises%20and%20assessment%20tasks.%20Tutorials%20enabled%0Astudents%20to%20generate%2C%20critique%2C%20and%20refine%20AI-assisted%20cybersecurity%20policies%2C%0Awhile%20assessments%20required%20them%20to%20apply%20AI-generated%20outputs%20to%20real-world%0Ascenarios%2C%20ensuring%20alignment%20with%20industry%20standards%20and%20regulatory%0Arequirements.%20Findings%20indicate%20that%20AI-assisted%20learning%20significantly%0Aenhanced%20students%27%20ability%20to%20evaluate%20security%20policies%2C%20refine%20risk%0Aassessments%2C%20and%20bridge%20theoretical%20knowledge%20with%20practical%20application.%0AStudent%20reflections%20and%20instructor%20observations%20revealed%20improvements%20in%0Aanalytical%20engagement%2C%20yet%20challenges%20emerged%20regarding%20AI%20over-reliance%2C%0Avariability%20in%20AI%20literacy%2C%20and%20the%20contextual%20limitations%20of%20AI-generated%0Acontent.%20Through%20structured%20intervention%20and%20research-driven%20refinement%2C%0Astudents%20were%20able%20to%20recognize%20AI%20strengths%20as%20a%20generative%20tool%20while%0Aacknowledging%20its%20need%20for%20human%20oversight.%20This%20study%20further%20highlights%20the%0Abroader%20implications%20of%20AI%20adoption%20in%20cybersecurity%20education%2C%20emphasizing%20the%0Anecessity%20of%20balancing%20automation%20with%20expert%20judgment%20to%20cultivate%0Aindustry-ready%20professionals.%20Future%20research%20should%20explore%20the%20long-term%0Aimpact%20of%20AI-driven%20learning%20on%20cybersecurity%20competency%2C%20as%20well%20as%20the%0Apotential%20for%20adaptive%20AI-assisted%20assessments%20to%20further%20personalize%20and%0Aenhance%20educational%20outcomes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15357v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520Generative%2520AI%2520in%2520Cybersecurity%2520Education%253A%2520Case%2520Study%250A%2520%2520Insights%2520on%2520Pedagogical%2520Strategies%252C%2520Critical%2520Thinking%252C%2520and%2520Responsible%2520AI%2520Use%26entry.906535625%3DMahmoud%2520Elkhodr%2520and%2520Ergun%2520Gide%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520Generative%2520Artificial%2520Intelligence%2520%2528GenAI%2529%2520has%250Aintroduced%2520new%2520opportunities%2520for%2520transforming%2520higher%2520education%252C%2520particularly%2520in%250Afields%2520that%2520require%2520analytical%2520reasoning%2520and%2520regulatory%2520compliance%252C%2520such%2520as%250Acybersecurity%2520management.%2520This%2520study%2520presents%2520a%2520structured%2520framework%2520for%250Aintegrating%2520GenAI%2520tools%2520into%2520cybersecurity%2520education%252C%2520demonstrating%2520their%2520role%250Ain%2520fostering%2520critical%2520thinking%252C%2520real-world%2520problem-solving%252C%2520and%2520regulatory%250Aawareness.%2520The%2520implementation%2520strategy%2520followed%2520a%2520two-stage%2520approach%252C%2520embedding%250AGenAI%2520within%2520tutorial%2520exercises%2520and%2520assessment%2520tasks.%2520Tutorials%2520enabled%250Astudents%2520to%2520generate%252C%2520critique%252C%2520and%2520refine%2520AI-assisted%2520cybersecurity%2520policies%252C%250Awhile%2520assessments%2520required%2520them%2520to%2520apply%2520AI-generated%2520outputs%2520to%2520real-world%250Ascenarios%252C%2520ensuring%2520alignment%2520with%2520industry%2520standards%2520and%2520regulatory%250Arequirements.%2520Findings%2520indicate%2520that%2520AI-assisted%2520learning%2520significantly%250Aenhanced%2520students%2527%2520ability%2520to%2520evaluate%2520security%2520policies%252C%2520refine%2520risk%250Aassessments%252C%2520and%2520bridge%2520theoretical%2520knowledge%2520with%2520practical%2520application.%250AStudent%2520reflections%2520and%2520instructor%2520observations%2520revealed%2520improvements%2520in%250Aanalytical%2520engagement%252C%2520yet%2520challenges%2520emerged%2520regarding%2520AI%2520over-reliance%252C%250Avariability%2520in%2520AI%2520literacy%252C%2520and%2520the%2520contextual%2520limitations%2520of%2520AI-generated%250Acontent.%2520Through%2520structured%2520intervention%2520and%2520research-driven%2520refinement%252C%250Astudents%2520were%2520able%2520to%2520recognize%2520AI%2520strengths%2520as%2520a%2520generative%2520tool%2520while%250Aacknowledging%2520its%2520need%2520for%2520human%2520oversight.%2520This%2520study%2520further%2520highlights%2520the%250Abroader%2520implications%2520of%2520AI%2520adoption%2520in%2520cybersecurity%2520education%252C%2520emphasizing%2520the%250Anecessity%2520of%2520balancing%2520automation%2520with%2520expert%2520judgment%2520to%2520cultivate%250Aindustry-ready%2520professionals.%2520Future%2520research%2520should%2520explore%2520the%2520long-term%250Aimpact%2520of%2520AI-driven%2520learning%2520on%2520cybersecurity%2520competency%252C%2520as%2520well%2520as%2520the%250Apotential%2520for%2520adaptive%2520AI-assisted%2520assessments%2520to%2520further%2520personalize%2520and%250Aenhance%2520educational%2520outcomes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15357v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Generative%20AI%20in%20Cybersecurity%20Education%3A%20Case%20Study%0A%20%20Insights%20on%20Pedagogical%20Strategies%2C%20Critical%20Thinking%2C%20and%20Responsible%20AI%20Use&entry.906535625=Mahmoud%20Elkhodr%20and%20Ergun%20Gide&entry.1292438233=%20%20The%20rapid%20advancement%20of%20Generative%20Artificial%20Intelligence%20%28GenAI%29%20has%0Aintroduced%20new%20opportunities%20for%20transforming%20higher%20education%2C%20particularly%20in%0Afields%20that%20require%20analytical%20reasoning%20and%20regulatory%20compliance%2C%20such%20as%0Acybersecurity%20management.%20This%20study%20presents%20a%20structured%20framework%20for%0Aintegrating%20GenAI%20tools%20into%20cybersecurity%20education%2C%20demonstrating%20their%20role%0Ain%20fostering%20critical%20thinking%2C%20real-world%20problem-solving%2C%20and%20regulatory%0Aawareness.%20The%20implementation%20strategy%20followed%20a%20two-stage%20approach%2C%20embedding%0AGenAI%20within%20tutorial%20exercises%20and%20assessment%20tasks.%20Tutorials%20enabled%0Astudents%20to%20generate%2C%20critique%2C%20and%20refine%20AI-assisted%20cybersecurity%20policies%2C%0Awhile%20assessments%20required%20them%20to%20apply%20AI-generated%20outputs%20to%20real-world%0Ascenarios%2C%20ensuring%20alignment%20with%20industry%20standards%20and%20regulatory%0Arequirements.%20Findings%20indicate%20that%20AI-assisted%20learning%20significantly%0Aenhanced%20students%27%20ability%20to%20evaluate%20security%20policies%2C%20refine%20risk%0Aassessments%2C%20and%20bridge%20theoretical%20knowledge%20with%20practical%20application.%0AStudent%20reflections%20and%20instructor%20observations%20revealed%20improvements%20in%0Aanalytical%20engagement%2C%20yet%20challenges%20emerged%20regarding%20AI%20over-reliance%2C%0Avariability%20in%20AI%20literacy%2C%20and%20the%20contextual%20limitations%20of%20AI-generated%0Acontent.%20Through%20structured%20intervention%20and%20research-driven%20refinement%2C%0Astudents%20were%20able%20to%20recognize%20AI%20strengths%20as%20a%20generative%20tool%20while%0Aacknowledging%20its%20need%20for%20human%20oversight.%20This%20study%20further%20highlights%20the%0Abroader%20implications%20of%20AI%20adoption%20in%20cybersecurity%20education%2C%20emphasizing%20the%0Anecessity%20of%20balancing%20automation%20with%20expert%20judgment%20to%20cultivate%0Aindustry-ready%20professionals.%20Future%20research%20should%20explore%20the%20long-term%0Aimpact%20of%20AI-driven%20learning%20on%20cybersecurity%20competency%2C%20as%20well%20as%20the%0Apotential%20for%20adaptive%20AI-assisted%20assessments%20to%20further%20personalize%20and%0Aenhance%20educational%20outcomes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15357v1&entry.124074799=Read"},
{"title": "Q-PETR: Quant-aware Position Embedding Transformation for Multi-View 3D\n  Object Detection", "author": "Jiangyong Yu and Changyong Shu and Dawei Yang and Zichen Yu and Xing Hu and Yan Chen", "abstract": "  PETR-based methods have dominated benchmarks in 3D perception and are\nincreasingly becoming a key component in modern autonomous driving systems.\nHowever, their quantization performance significantly degrades when INT8\ninference is required, with a degradation of 58.2% in mAP and 36.9% in NDS on\nthe NuScenes dataset. To address this issue, we propose a quantization-aware\nposition embedding transformation for multi-view 3D object detection, termed\nQ-PETR. Q-PETR offers a quantizationfriendly and deployment-friendly\narchitecture while preserving the original performance of PETR. It\nsubstantially narrows the accuracy gap between INT8 and FP32 inference for\nPETR-series methods. Without bells and whistles, our approach reduces the mAP\nand NDS drop to within 1% under standard 8-bit per-tensor post-training\nquantization. Furthermore, our method exceeds the performance of the original\nPETR in terms of floating-point precision. Extensive experiments across a\nvariety of PETR-series models demonstrate its broad generalization.\n", "link": "http://arxiv.org/abs/2502.15488v1", "date": "2025-02-21", "relevancy": 1.6506, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5895}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5434}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Q-PETR%3A%20Quant-aware%20Position%20Embedding%20Transformation%20for%20Multi-View%203D%0A%20%20Object%20Detection&body=Title%3A%20Q-PETR%3A%20Quant-aware%20Position%20Embedding%20Transformation%20for%20Multi-View%203D%0A%20%20Object%20Detection%0AAuthor%3A%20Jiangyong%20Yu%20and%20Changyong%20Shu%20and%20Dawei%20Yang%20and%20Zichen%20Yu%20and%20Xing%20Hu%20and%20Yan%20Chen%0AAbstract%3A%20%20%20PETR-based%20methods%20have%20dominated%20benchmarks%20in%203D%20perception%20and%20are%0Aincreasingly%20becoming%20a%20key%20component%20in%20modern%20autonomous%20driving%20systems.%0AHowever%2C%20their%20quantization%20performance%20significantly%20degrades%20when%20INT8%0Ainference%20is%20required%2C%20with%20a%20degradation%20of%2058.2%25%20in%20mAP%20and%2036.9%25%20in%20NDS%20on%0Athe%20NuScenes%20dataset.%20To%20address%20this%20issue%2C%20we%20propose%20a%20quantization-aware%0Aposition%20embedding%20transformation%20for%20multi-view%203D%20object%20detection%2C%20termed%0AQ-PETR.%20Q-PETR%20offers%20a%20quantizationfriendly%20and%20deployment-friendly%0Aarchitecture%20while%20preserving%20the%20original%20performance%20of%20PETR.%20It%0Asubstantially%20narrows%20the%20accuracy%20gap%20between%20INT8%20and%20FP32%20inference%20for%0APETR-series%20methods.%20Without%20bells%20and%20whistles%2C%20our%20approach%20reduces%20the%20mAP%0Aand%20NDS%20drop%20to%20within%201%25%20under%20standard%208-bit%20per-tensor%20post-training%0Aquantization.%20Furthermore%2C%20our%20method%20exceeds%20the%20performance%20of%20the%20original%0APETR%20in%20terms%20of%20floating-point%20precision.%20Extensive%20experiments%20across%20a%0Avariety%20of%20PETR-series%20models%20demonstrate%20its%20broad%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15488v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQ-PETR%253A%2520Quant-aware%2520Position%2520Embedding%2520Transformation%2520for%2520Multi-View%25203D%250A%2520%2520Object%2520Detection%26entry.906535625%3DJiangyong%2520Yu%2520and%2520Changyong%2520Shu%2520and%2520Dawei%2520Yang%2520and%2520Zichen%2520Yu%2520and%2520Xing%2520Hu%2520and%2520Yan%2520Chen%26entry.1292438233%3D%2520%2520PETR-based%2520methods%2520have%2520dominated%2520benchmarks%2520in%25203D%2520perception%2520and%2520are%250Aincreasingly%2520becoming%2520a%2520key%2520component%2520in%2520modern%2520autonomous%2520driving%2520systems.%250AHowever%252C%2520their%2520quantization%2520performance%2520significantly%2520degrades%2520when%2520INT8%250Ainference%2520is%2520required%252C%2520with%2520a%2520degradation%2520of%252058.2%2525%2520in%2520mAP%2520and%252036.9%2525%2520in%2520NDS%2520on%250Athe%2520NuScenes%2520dataset.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520quantization-aware%250Aposition%2520embedding%2520transformation%2520for%2520multi-view%25203D%2520object%2520detection%252C%2520termed%250AQ-PETR.%2520Q-PETR%2520offers%2520a%2520quantizationfriendly%2520and%2520deployment-friendly%250Aarchitecture%2520while%2520preserving%2520the%2520original%2520performance%2520of%2520PETR.%2520It%250Asubstantially%2520narrows%2520the%2520accuracy%2520gap%2520between%2520INT8%2520and%2520FP32%2520inference%2520for%250APETR-series%2520methods.%2520Without%2520bells%2520and%2520whistles%252C%2520our%2520approach%2520reduces%2520the%2520mAP%250Aand%2520NDS%2520drop%2520to%2520within%25201%2525%2520under%2520standard%25208-bit%2520per-tensor%2520post-training%250Aquantization.%2520Furthermore%252C%2520our%2520method%2520exceeds%2520the%2520performance%2520of%2520the%2520original%250APETR%2520in%2520terms%2520of%2520floating-point%2520precision.%2520Extensive%2520experiments%2520across%2520a%250Avariety%2520of%2520PETR-series%2520models%2520demonstrate%2520its%2520broad%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15488v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Q-PETR%3A%20Quant-aware%20Position%20Embedding%20Transformation%20for%20Multi-View%203D%0A%20%20Object%20Detection&entry.906535625=Jiangyong%20Yu%20and%20Changyong%20Shu%20and%20Dawei%20Yang%20and%20Zichen%20Yu%20and%20Xing%20Hu%20and%20Yan%20Chen&entry.1292438233=%20%20PETR-based%20methods%20have%20dominated%20benchmarks%20in%203D%20perception%20and%20are%0Aincreasingly%20becoming%20a%20key%20component%20in%20modern%20autonomous%20driving%20systems.%0AHowever%2C%20their%20quantization%20performance%20significantly%20degrades%20when%20INT8%0Ainference%20is%20required%2C%20with%20a%20degradation%20of%2058.2%25%20in%20mAP%20and%2036.9%25%20in%20NDS%20on%0Athe%20NuScenes%20dataset.%20To%20address%20this%20issue%2C%20we%20propose%20a%20quantization-aware%0Aposition%20embedding%20transformation%20for%20multi-view%203D%20object%20detection%2C%20termed%0AQ-PETR.%20Q-PETR%20offers%20a%20quantizationfriendly%20and%20deployment-friendly%0Aarchitecture%20while%20preserving%20the%20original%20performance%20of%20PETR.%20It%0Asubstantially%20narrows%20the%20accuracy%20gap%20between%20INT8%20and%20FP32%20inference%20for%0APETR-series%20methods.%20Without%20bells%20and%20whistles%2C%20our%20approach%20reduces%20the%20mAP%0Aand%20NDS%20drop%20to%20within%201%25%20under%20standard%208-bit%20per-tensor%20post-training%0Aquantization.%20Furthermore%2C%20our%20method%20exceeds%20the%20performance%20of%20the%20original%0APETR%20in%20terms%20of%20floating-point%20precision.%20Extensive%20experiments%20across%20a%0Avariety%20of%20PETR-series%20models%20demonstrate%20its%20broad%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15488v1&entry.124074799=Read"},
{"title": "Are Neuromorphic Architectures Inherently Privacy-preserving? An\n  Exploratory Study", "author": "Ayana Moshruba and Ihsen Alouani and Maryam Parsa", "abstract": "  While machine learning (ML) models are becoming mainstream, especially in\nsensitive application areas, the risk of data leakage has become a growing\nconcern. Attacks like membership inference (MIA) have shown that trained models\ncan reveal sensitive data, jeopardizing confidentiality. While traditional\nArtificial Neural Networks (ANNs) dominate ML applications, neuromorphic\narchitectures, specifically Spiking Neural Networks (SNNs), are emerging as\npromising alternatives due to their low power consumption and event-driven\nprocessing, akin to biological neurons. Privacy in ANNs is well-studied;\nhowever, little work has explored the privacy-preserving properties of SNNs.\nThis paper examines whether SNNs inherently offer better privacy. Using MIAs,\nwe assess the privacy resilience of SNNs versus ANNs across diverse datasets.\nWe analyze the impact of learning algorithms (surrogate gradient and\nevolutionary), frameworks (snnTorch, TENNLab, LAVA), and parameters on SNN\nprivacy. Our findings show that SNNs consistently outperform ANNs in privacy\npreservation, with evolutionary algorithms offering additional resilience. For\ninstance, on CIFAR-10, SNNs achieve an AUC of 0.59, significantly lower than\nANNs' 0.82, and on CIFAR-100, SNNs maintain an AUC of 0.58 compared to ANNs'\n0.88. Additionally, we explore the privacy-utility trade-off with\nDifferentially Private Stochastic Gradient Descent (DPSGD), finding that SNNs\nsustain less accuracy loss than ANNs under similar privacy constraints.\n", "link": "http://arxiv.org/abs/2411.06613v2", "date": "2025-02-21", "relevancy": 1.8258, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4707}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4486}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Neuromorphic%20Architectures%20Inherently%20Privacy-preserving%3F%20An%0A%20%20Exploratory%20Study&body=Title%3A%20Are%20Neuromorphic%20Architectures%20Inherently%20Privacy-preserving%3F%20An%0A%20%20Exploratory%20Study%0AAuthor%3A%20Ayana%20Moshruba%20and%20Ihsen%20Alouani%20and%20Maryam%20Parsa%0AAbstract%3A%20%20%20While%20machine%20learning%20%28ML%29%20models%20are%20becoming%20mainstream%2C%20especially%20in%0Asensitive%20application%20areas%2C%20the%20risk%20of%20data%20leakage%20has%20become%20a%20growing%0Aconcern.%20Attacks%20like%20membership%20inference%20%28MIA%29%20have%20shown%20that%20trained%20models%0Acan%20reveal%20sensitive%20data%2C%20jeopardizing%20confidentiality.%20While%20traditional%0AArtificial%20Neural%20Networks%20%28ANNs%29%20dominate%20ML%20applications%2C%20neuromorphic%0Aarchitectures%2C%20specifically%20Spiking%20Neural%20Networks%20%28SNNs%29%2C%20are%20emerging%20as%0Apromising%20alternatives%20due%20to%20their%20low%20power%20consumption%20and%20event-driven%0Aprocessing%2C%20akin%20to%20biological%20neurons.%20Privacy%20in%20ANNs%20is%20well-studied%3B%0Ahowever%2C%20little%20work%20has%20explored%20the%20privacy-preserving%20properties%20of%20SNNs.%0AThis%20paper%20examines%20whether%20SNNs%20inherently%20offer%20better%20privacy.%20Using%20MIAs%2C%0Awe%20assess%20the%20privacy%20resilience%20of%20SNNs%20versus%20ANNs%20across%20diverse%20datasets.%0AWe%20analyze%20the%20impact%20of%20learning%20algorithms%20%28surrogate%20gradient%20and%0Aevolutionary%29%2C%20frameworks%20%28snnTorch%2C%20TENNLab%2C%20LAVA%29%2C%20and%20parameters%20on%20SNN%0Aprivacy.%20Our%20findings%20show%20that%20SNNs%20consistently%20outperform%20ANNs%20in%20privacy%0Apreservation%2C%20with%20evolutionary%20algorithms%20offering%20additional%20resilience.%20For%0Ainstance%2C%20on%20CIFAR-10%2C%20SNNs%20achieve%20an%20AUC%20of%200.59%2C%20significantly%20lower%20than%0AANNs%27%200.82%2C%20and%20on%20CIFAR-100%2C%20SNNs%20maintain%20an%20AUC%20of%200.58%20compared%20to%20ANNs%27%0A0.88.%20Additionally%2C%20we%20explore%20the%20privacy-utility%20trade-off%20with%0ADifferentially%20Private%20Stochastic%20Gradient%20Descent%20%28DPSGD%29%2C%20finding%20that%20SNNs%0Asustain%20less%20accuracy%20loss%20than%20ANNs%20under%20similar%20privacy%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06613v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Neuromorphic%2520Architectures%2520Inherently%2520Privacy-preserving%253F%2520An%250A%2520%2520Exploratory%2520Study%26entry.906535625%3DAyana%2520Moshruba%2520and%2520Ihsen%2520Alouani%2520and%2520Maryam%2520Parsa%26entry.1292438233%3D%2520%2520While%2520machine%2520learning%2520%2528ML%2529%2520models%2520are%2520becoming%2520mainstream%252C%2520especially%2520in%250Asensitive%2520application%2520areas%252C%2520the%2520risk%2520of%2520data%2520leakage%2520has%2520become%2520a%2520growing%250Aconcern.%2520Attacks%2520like%2520membership%2520inference%2520%2528MIA%2529%2520have%2520shown%2520that%2520trained%2520models%250Acan%2520reveal%2520sensitive%2520data%252C%2520jeopardizing%2520confidentiality.%2520While%2520traditional%250AArtificial%2520Neural%2520Networks%2520%2528ANNs%2529%2520dominate%2520ML%2520applications%252C%2520neuromorphic%250Aarchitectures%252C%2520specifically%2520Spiking%2520Neural%2520Networks%2520%2528SNNs%2529%252C%2520are%2520emerging%2520as%250Apromising%2520alternatives%2520due%2520to%2520their%2520low%2520power%2520consumption%2520and%2520event-driven%250Aprocessing%252C%2520akin%2520to%2520biological%2520neurons.%2520Privacy%2520in%2520ANNs%2520is%2520well-studied%253B%250Ahowever%252C%2520little%2520work%2520has%2520explored%2520the%2520privacy-preserving%2520properties%2520of%2520SNNs.%250AThis%2520paper%2520examines%2520whether%2520SNNs%2520inherently%2520offer%2520better%2520privacy.%2520Using%2520MIAs%252C%250Awe%2520assess%2520the%2520privacy%2520resilience%2520of%2520SNNs%2520versus%2520ANNs%2520across%2520diverse%2520datasets.%250AWe%2520analyze%2520the%2520impact%2520of%2520learning%2520algorithms%2520%2528surrogate%2520gradient%2520and%250Aevolutionary%2529%252C%2520frameworks%2520%2528snnTorch%252C%2520TENNLab%252C%2520LAVA%2529%252C%2520and%2520parameters%2520on%2520SNN%250Aprivacy.%2520Our%2520findings%2520show%2520that%2520SNNs%2520consistently%2520outperform%2520ANNs%2520in%2520privacy%250Apreservation%252C%2520with%2520evolutionary%2520algorithms%2520offering%2520additional%2520resilience.%2520For%250Ainstance%252C%2520on%2520CIFAR-10%252C%2520SNNs%2520achieve%2520an%2520AUC%2520of%25200.59%252C%2520significantly%2520lower%2520than%250AANNs%2527%25200.82%252C%2520and%2520on%2520CIFAR-100%252C%2520SNNs%2520maintain%2520an%2520AUC%2520of%25200.58%2520compared%2520to%2520ANNs%2527%250A0.88.%2520Additionally%252C%2520we%2520explore%2520the%2520privacy-utility%2520trade-off%2520with%250ADifferentially%2520Private%2520Stochastic%2520Gradient%2520Descent%2520%2528DPSGD%2529%252C%2520finding%2520that%2520SNNs%250Asustain%2520less%2520accuracy%2520loss%2520than%2520ANNs%2520under%2520similar%2520privacy%2520constraints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06613v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Neuromorphic%20Architectures%20Inherently%20Privacy-preserving%3F%20An%0A%20%20Exploratory%20Study&entry.906535625=Ayana%20Moshruba%20and%20Ihsen%20Alouani%20and%20Maryam%20Parsa&entry.1292438233=%20%20While%20machine%20learning%20%28ML%29%20models%20are%20becoming%20mainstream%2C%20especially%20in%0Asensitive%20application%20areas%2C%20the%20risk%20of%20data%20leakage%20has%20become%20a%20growing%0Aconcern.%20Attacks%20like%20membership%20inference%20%28MIA%29%20have%20shown%20that%20trained%20models%0Acan%20reveal%20sensitive%20data%2C%20jeopardizing%20confidentiality.%20While%20traditional%0AArtificial%20Neural%20Networks%20%28ANNs%29%20dominate%20ML%20applications%2C%20neuromorphic%0Aarchitectures%2C%20specifically%20Spiking%20Neural%20Networks%20%28SNNs%29%2C%20are%20emerging%20as%0Apromising%20alternatives%20due%20to%20their%20low%20power%20consumption%20and%20event-driven%0Aprocessing%2C%20akin%20to%20biological%20neurons.%20Privacy%20in%20ANNs%20is%20well-studied%3B%0Ahowever%2C%20little%20work%20has%20explored%20the%20privacy-preserving%20properties%20of%20SNNs.%0AThis%20paper%20examines%20whether%20SNNs%20inherently%20offer%20better%20privacy.%20Using%20MIAs%2C%0Awe%20assess%20the%20privacy%20resilience%20of%20SNNs%20versus%20ANNs%20across%20diverse%20datasets.%0AWe%20analyze%20the%20impact%20of%20learning%20algorithms%20%28surrogate%20gradient%20and%0Aevolutionary%29%2C%20frameworks%20%28snnTorch%2C%20TENNLab%2C%20LAVA%29%2C%20and%20parameters%20on%20SNN%0Aprivacy.%20Our%20findings%20show%20that%20SNNs%20consistently%20outperform%20ANNs%20in%20privacy%0Apreservation%2C%20with%20evolutionary%20algorithms%20offering%20additional%20resilience.%20For%0Ainstance%2C%20on%20CIFAR-10%2C%20SNNs%20achieve%20an%20AUC%20of%200.59%2C%20significantly%20lower%20than%0AANNs%27%200.82%2C%20and%20on%20CIFAR-100%2C%20SNNs%20maintain%20an%20AUC%20of%200.58%20compared%20to%20ANNs%27%0A0.88.%20Additionally%2C%20we%20explore%20the%20privacy-utility%20trade-off%20with%0ADifferentially%20Private%20Stochastic%20Gradient%20Descent%20%28DPSGD%29%2C%20finding%20that%20SNNs%0Asustain%20less%20accuracy%20loss%20than%20ANNs%20under%20similar%20privacy%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06613v2&entry.124074799=Read"},
{"title": "Large Language Models for Interpretable Mental Health Diagnosis", "author": "Brian Hyeongseok Kim and Chao Wang", "abstract": "  We propose a clinical decision support system (CDSS) for mental health\ndiagnosis that combines the strengths of large language models (LLMs) and\nconstraint logic programming (CLP). Having a CDSS is important because of the\nhigh complexity of diagnostic manuals used by mental health professionals and\nthe danger of diagnostic errors. Our CDSS is a software tool that uses an LLM\nto translate diagnostic manuals to a logic program and solves the program using\nan off-the-shelf CLP engine to query a patient's diagnosis based on the encoded\nrules and provided data. By giving domain experts the opportunity to inspect\nthe LLM-generated logic program, and making modifications when needed, our CDSS\nensures that the diagnosis is not only accurate but also interpretable. We\nexperimentally compare it with two baseline approaches of using LLMs:\ndiagnosing patients using the LLM-only approach, and using the LLM-generated\nlogic program but without expert inspection. The results show that, while LLMs\nare extremely useful in generating candidate logic programs, these programs\nstill require expert inspection and modification to guarantee faithfulness to\nthe official diagnostic manuals. Additionally, ethical concerns arise from the\ndirect use of patient data in LLMs, underscoring the need for a safer hybrid\napproach like our proposed method.\n", "link": "http://arxiv.org/abs/2501.07653v2", "date": "2025-02-21", "relevancy": 1.8609, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4676}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4676}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20for%20Interpretable%20Mental%20Health%20Diagnosis&body=Title%3A%20Large%20Language%20Models%20for%20Interpretable%20Mental%20Health%20Diagnosis%0AAuthor%3A%20Brian%20Hyeongseok%20Kim%20and%20Chao%20Wang%0AAbstract%3A%20%20%20We%20propose%20a%20clinical%20decision%20support%20system%20%28CDSS%29%20for%20mental%20health%0Adiagnosis%20that%20combines%20the%20strengths%20of%20large%20language%20models%20%28LLMs%29%20and%0Aconstraint%20logic%20programming%20%28CLP%29.%20Having%20a%20CDSS%20is%20important%20because%20of%20the%0Ahigh%20complexity%20of%20diagnostic%20manuals%20used%20by%20mental%20health%20professionals%20and%0Athe%20danger%20of%20diagnostic%20errors.%20Our%20CDSS%20is%20a%20software%20tool%20that%20uses%20an%20LLM%0Ato%20translate%20diagnostic%20manuals%20to%20a%20logic%20program%20and%20solves%20the%20program%20using%0Aan%20off-the-shelf%20CLP%20engine%20to%20query%20a%20patient%27s%20diagnosis%20based%20on%20the%20encoded%0Arules%20and%20provided%20data.%20By%20giving%20domain%20experts%20the%20opportunity%20to%20inspect%0Athe%20LLM-generated%20logic%20program%2C%20and%20making%20modifications%20when%20needed%2C%20our%20CDSS%0Aensures%20that%20the%20diagnosis%20is%20not%20only%20accurate%20but%20also%20interpretable.%20We%0Aexperimentally%20compare%20it%20with%20two%20baseline%20approaches%20of%20using%20LLMs%3A%0Adiagnosing%20patients%20using%20the%20LLM-only%20approach%2C%20and%20using%20the%20LLM-generated%0Alogic%20program%20but%20without%20expert%20inspection.%20The%20results%20show%20that%2C%20while%20LLMs%0Aare%20extremely%20useful%20in%20generating%20candidate%20logic%20programs%2C%20these%20programs%0Astill%20require%20expert%20inspection%20and%20modification%20to%20guarantee%20faithfulness%20to%0Athe%20official%20diagnostic%20manuals.%20Additionally%2C%20ethical%20concerns%20arise%20from%20the%0Adirect%20use%20of%20patient%20data%20in%20LLMs%2C%20underscoring%20the%20need%20for%20a%20safer%20hybrid%0Aapproach%20like%20our%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07653v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520for%2520Interpretable%2520Mental%2520Health%2520Diagnosis%26entry.906535625%3DBrian%2520Hyeongseok%2520Kim%2520and%2520Chao%2520Wang%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520clinical%2520decision%2520support%2520system%2520%2528CDSS%2529%2520for%2520mental%2520health%250Adiagnosis%2520that%2520combines%2520the%2520strengths%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%250Aconstraint%2520logic%2520programming%2520%2528CLP%2529.%2520Having%2520a%2520CDSS%2520is%2520important%2520because%2520of%2520the%250Ahigh%2520complexity%2520of%2520diagnostic%2520manuals%2520used%2520by%2520mental%2520health%2520professionals%2520and%250Athe%2520danger%2520of%2520diagnostic%2520errors.%2520Our%2520CDSS%2520is%2520a%2520software%2520tool%2520that%2520uses%2520an%2520LLM%250Ato%2520translate%2520diagnostic%2520manuals%2520to%2520a%2520logic%2520program%2520and%2520solves%2520the%2520program%2520using%250Aan%2520off-the-shelf%2520CLP%2520engine%2520to%2520query%2520a%2520patient%2527s%2520diagnosis%2520based%2520on%2520the%2520encoded%250Arules%2520and%2520provided%2520data.%2520By%2520giving%2520domain%2520experts%2520the%2520opportunity%2520to%2520inspect%250Athe%2520LLM-generated%2520logic%2520program%252C%2520and%2520making%2520modifications%2520when%2520needed%252C%2520our%2520CDSS%250Aensures%2520that%2520the%2520diagnosis%2520is%2520not%2520only%2520accurate%2520but%2520also%2520interpretable.%2520We%250Aexperimentally%2520compare%2520it%2520with%2520two%2520baseline%2520approaches%2520of%2520using%2520LLMs%253A%250Adiagnosing%2520patients%2520using%2520the%2520LLM-only%2520approach%252C%2520and%2520using%2520the%2520LLM-generated%250Alogic%2520program%2520but%2520without%2520expert%2520inspection.%2520The%2520results%2520show%2520that%252C%2520while%2520LLMs%250Aare%2520extremely%2520useful%2520in%2520generating%2520candidate%2520logic%2520programs%252C%2520these%2520programs%250Astill%2520require%2520expert%2520inspection%2520and%2520modification%2520to%2520guarantee%2520faithfulness%2520to%250Athe%2520official%2520diagnostic%2520manuals.%2520Additionally%252C%2520ethical%2520concerns%2520arise%2520from%2520the%250Adirect%2520use%2520of%2520patient%2520data%2520in%2520LLMs%252C%2520underscoring%2520the%2520need%2520for%2520a%2520safer%2520hybrid%250Aapproach%2520like%2520our%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07653v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20for%20Interpretable%20Mental%20Health%20Diagnosis&entry.906535625=Brian%20Hyeongseok%20Kim%20and%20Chao%20Wang&entry.1292438233=%20%20We%20propose%20a%20clinical%20decision%20support%20system%20%28CDSS%29%20for%20mental%20health%0Adiagnosis%20that%20combines%20the%20strengths%20of%20large%20language%20models%20%28LLMs%29%20and%0Aconstraint%20logic%20programming%20%28CLP%29.%20Having%20a%20CDSS%20is%20important%20because%20of%20the%0Ahigh%20complexity%20of%20diagnostic%20manuals%20used%20by%20mental%20health%20professionals%20and%0Athe%20danger%20of%20diagnostic%20errors.%20Our%20CDSS%20is%20a%20software%20tool%20that%20uses%20an%20LLM%0Ato%20translate%20diagnostic%20manuals%20to%20a%20logic%20program%20and%20solves%20the%20program%20using%0Aan%20off-the-shelf%20CLP%20engine%20to%20query%20a%20patient%27s%20diagnosis%20based%20on%20the%20encoded%0Arules%20and%20provided%20data.%20By%20giving%20domain%20experts%20the%20opportunity%20to%20inspect%0Athe%20LLM-generated%20logic%20program%2C%20and%20making%20modifications%20when%20needed%2C%20our%20CDSS%0Aensures%20that%20the%20diagnosis%20is%20not%20only%20accurate%20but%20also%20interpretable.%20We%0Aexperimentally%20compare%20it%20with%20two%20baseline%20approaches%20of%20using%20LLMs%3A%0Adiagnosing%20patients%20using%20the%20LLM-only%20approach%2C%20and%20using%20the%20LLM-generated%0Alogic%20program%20but%20without%20expert%20inspection.%20The%20results%20show%20that%2C%20while%20LLMs%0Aare%20extremely%20useful%20in%20generating%20candidate%20logic%20programs%2C%20these%20programs%0Astill%20require%20expert%20inspection%20and%20modification%20to%20guarantee%20faithfulness%20to%0Athe%20official%20diagnostic%20manuals.%20Additionally%2C%20ethical%20concerns%20arise%20from%20the%0Adirect%20use%20of%20patient%20data%20in%20LLMs%2C%20underscoring%20the%20need%20for%20a%20safer%20hybrid%0Aapproach%20like%20our%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07653v2&entry.124074799=Read"},
{"title": "Shared Control with Black Box Agents using Oracle Queries", "author": "Inbal Avraham and Reuth Mirsky", "abstract": "  Shared control problems involve a robot learning to collaborate with a human.\nWhen learning a shared control policy, short communication between the agents\ncan often significantly reduce running times and improve the system's accuracy.\nWe extend the shared control problem to include the ability to directly query a\ncooperating agent. We consider two types of potential responses to a query,\nnamely oracles: one that can provide the learner with the best action they\nshould take, even when that action might be myopically wrong, and one with a\nbounded knowledge limited to its part of the system. Given this additional\ninformation channel, this work further presents three heuristics for choosing\nwhen to query: reinforcement learning-based, utility-based, and entropy-based.\nThese heuristics aim to reduce a system's overall learning cost. Empirical\nresults on two environments show the benefits of querying to learn a better\ncontrol policy and the tradeoffs between the proposed heuristics.\n", "link": "http://arxiv.org/abs/2410.19612v2", "date": "2025-02-21", "relevancy": 1.6616, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.6052}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5096}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shared%20Control%20with%20Black%20Box%20Agents%20using%20Oracle%20Queries&body=Title%3A%20Shared%20Control%20with%20Black%20Box%20Agents%20using%20Oracle%20Queries%0AAuthor%3A%20Inbal%20Avraham%20and%20Reuth%20Mirsky%0AAbstract%3A%20%20%20Shared%20control%20problems%20involve%20a%20robot%20learning%20to%20collaborate%20with%20a%20human.%0AWhen%20learning%20a%20shared%20control%20policy%2C%20short%20communication%20between%20the%20agents%0Acan%20often%20significantly%20reduce%20running%20times%20and%20improve%20the%20system%27s%20accuracy.%0AWe%20extend%20the%20shared%20control%20problem%20to%20include%20the%20ability%20to%20directly%20query%20a%0Acooperating%20agent.%20We%20consider%20two%20types%20of%20potential%20responses%20to%20a%20query%2C%0Anamely%20oracles%3A%20one%20that%20can%20provide%20the%20learner%20with%20the%20best%20action%20they%0Ashould%20take%2C%20even%20when%20that%20action%20might%20be%20myopically%20wrong%2C%20and%20one%20with%20a%0Abounded%20knowledge%20limited%20to%20its%20part%20of%20the%20system.%20Given%20this%20additional%0Ainformation%20channel%2C%20this%20work%20further%20presents%20three%20heuristics%20for%20choosing%0Awhen%20to%20query%3A%20reinforcement%20learning-based%2C%20utility-based%2C%20and%20entropy-based.%0AThese%20heuristics%20aim%20to%20reduce%20a%20system%27s%20overall%20learning%20cost.%20Empirical%0Aresults%20on%20two%20environments%20show%20the%20benefits%20of%20querying%20to%20learn%20a%20better%0Acontrol%20policy%20and%20the%20tradeoffs%20between%20the%20proposed%20heuristics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19612v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShared%2520Control%2520with%2520Black%2520Box%2520Agents%2520using%2520Oracle%2520Queries%26entry.906535625%3DInbal%2520Avraham%2520and%2520Reuth%2520Mirsky%26entry.1292438233%3D%2520%2520Shared%2520control%2520problems%2520involve%2520a%2520robot%2520learning%2520to%2520collaborate%2520with%2520a%2520human.%250AWhen%2520learning%2520a%2520shared%2520control%2520policy%252C%2520short%2520communication%2520between%2520the%2520agents%250Acan%2520often%2520significantly%2520reduce%2520running%2520times%2520and%2520improve%2520the%2520system%2527s%2520accuracy.%250AWe%2520extend%2520the%2520shared%2520control%2520problem%2520to%2520include%2520the%2520ability%2520to%2520directly%2520query%2520a%250Acooperating%2520agent.%2520We%2520consider%2520two%2520types%2520of%2520potential%2520responses%2520to%2520a%2520query%252C%250Anamely%2520oracles%253A%2520one%2520that%2520can%2520provide%2520the%2520learner%2520with%2520the%2520best%2520action%2520they%250Ashould%2520take%252C%2520even%2520when%2520that%2520action%2520might%2520be%2520myopically%2520wrong%252C%2520and%2520one%2520with%2520a%250Abounded%2520knowledge%2520limited%2520to%2520its%2520part%2520of%2520the%2520system.%2520Given%2520this%2520additional%250Ainformation%2520channel%252C%2520this%2520work%2520further%2520presents%2520three%2520heuristics%2520for%2520choosing%250Awhen%2520to%2520query%253A%2520reinforcement%2520learning-based%252C%2520utility-based%252C%2520and%2520entropy-based.%250AThese%2520heuristics%2520aim%2520to%2520reduce%2520a%2520system%2527s%2520overall%2520learning%2520cost.%2520Empirical%250Aresults%2520on%2520two%2520environments%2520show%2520the%2520benefits%2520of%2520querying%2520to%2520learn%2520a%2520better%250Acontrol%2520policy%2520and%2520the%2520tradeoffs%2520between%2520the%2520proposed%2520heuristics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19612v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shared%20Control%20with%20Black%20Box%20Agents%20using%20Oracle%20Queries&entry.906535625=Inbal%20Avraham%20and%20Reuth%20Mirsky&entry.1292438233=%20%20Shared%20control%20problems%20involve%20a%20robot%20learning%20to%20collaborate%20with%20a%20human.%0AWhen%20learning%20a%20shared%20control%20policy%2C%20short%20communication%20between%20the%20agents%0Acan%20often%20significantly%20reduce%20running%20times%20and%20improve%20the%20system%27s%20accuracy.%0AWe%20extend%20the%20shared%20control%20problem%20to%20include%20the%20ability%20to%20directly%20query%20a%0Acooperating%20agent.%20We%20consider%20two%20types%20of%20potential%20responses%20to%20a%20query%2C%0Anamely%20oracles%3A%20one%20that%20can%20provide%20the%20learner%20with%20the%20best%20action%20they%0Ashould%20take%2C%20even%20when%20that%20action%20might%20be%20myopically%20wrong%2C%20and%20one%20with%20a%0Abounded%20knowledge%20limited%20to%20its%20part%20of%20the%20system.%20Given%20this%20additional%0Ainformation%20channel%2C%20this%20work%20further%20presents%20three%20heuristics%20for%20choosing%0Awhen%20to%20query%3A%20reinforcement%20learning-based%2C%20utility-based%2C%20and%20entropy-based.%0AThese%20heuristics%20aim%20to%20reduce%20a%20system%27s%20overall%20learning%20cost.%20Empirical%0Aresults%20on%20two%20environments%20show%20the%20benefits%20of%20querying%20to%20learn%20a%20better%0Acontrol%20policy%20and%20the%20tradeoffs%20between%20the%20proposed%20heuristics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19612v2&entry.124074799=Read"},
{"title": "Flow of Reasoning:Training LLMs for Divergent Problem Solving with\n  Minimal Examples", "author": "Fangxu Yu and Lai Jiang and Haoqiang Kang and Shibo Hao and Lianhui Qin", "abstract": "  The ability to generate diverse solutions to a given problem is a hallmark of\nhuman creativity. This divergent reasoning is also crucial for machines,\nenhancing their robustness and enabling them to assist humans in many\napplications such as scientific discovery. However, existing approaches to\nmulti-step reasoning with large language models (LLMs) have mostly focused only\non reasoning accuracy, without further discovering more diverse valid\nsolutions. For example, supervised fine-tuning can improve LLM reasoning\nquality, but requires extensive supervised data to capture the full range of\npossible solutions. Reward-maximization reinforcement learning aims to find\nlimited highest-reward solutions while neglecting the solution diversity. To\nfill this gap, we propose Flow of Reasoning (FoR), an efficient\ndiversity-seeking LLM finetuning method aimed at improving reasoning quality\nand diversity with minimal data. FoR formulates multi-step LLM reasoning as a\nMarkovian flow on a DAG-structured reasoning graph. This formulation allows us\nto incorporate and adapt principled GFlowNet approaches, for finetuning LLMs to\nsample divergent paths with probabilities proportional to the (unnormalized)\nreward of target problems. Extensive experiments show that, with limited\ntraining examples (e.g., 15 examples), FoR enables the discovery of diverse,\ncreative, high-quality solutions, greatly outperforming a wide range of\nexisting inference and training methods across six challenging reasoning tasks,\nincluding BlocksWorld (embodied reasoning), Game24 (math puzzle solving),\nRubik's Cube (spatial reasoning), 1D-ARC (abstraction reasoning), GSM8k (math\nreasoning), and ProntoQA (logical reasoning). Code is available at\nhttps://github.com/Yu-Fangxu/FoR.\n", "link": "http://arxiv.org/abs/2406.05673v4", "date": "2025-02-21", "relevancy": 1.544, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5373}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5133}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flow%20of%20Reasoning%3ATraining%20LLMs%20for%20Divergent%20Problem%20Solving%20with%0A%20%20Minimal%20Examples&body=Title%3A%20Flow%20of%20Reasoning%3ATraining%20LLMs%20for%20Divergent%20Problem%20Solving%20with%0A%20%20Minimal%20Examples%0AAuthor%3A%20Fangxu%20Yu%20and%20Lai%20Jiang%20and%20Haoqiang%20Kang%20and%20Shibo%20Hao%20and%20Lianhui%20Qin%0AAbstract%3A%20%20%20The%20ability%20to%20generate%20diverse%20solutions%20to%20a%20given%20problem%20is%20a%20hallmark%20of%0Ahuman%20creativity.%20This%20divergent%20reasoning%20is%20also%20crucial%20for%20machines%2C%0Aenhancing%20their%20robustness%20and%20enabling%20them%20to%20assist%20humans%20in%20many%0Aapplications%20such%20as%20scientific%20discovery.%20However%2C%20existing%20approaches%20to%0Amulti-step%20reasoning%20with%20large%20language%20models%20%28LLMs%29%20have%20mostly%20focused%20only%0Aon%20reasoning%20accuracy%2C%20without%20further%20discovering%20more%20diverse%20valid%0Asolutions.%20For%20example%2C%20supervised%20fine-tuning%20can%20improve%20LLM%20reasoning%0Aquality%2C%20but%20requires%20extensive%20supervised%20data%20to%20capture%20the%20full%20range%20of%0Apossible%20solutions.%20Reward-maximization%20reinforcement%20learning%20aims%20to%20find%0Alimited%20highest-reward%20solutions%20while%20neglecting%20the%20solution%20diversity.%20To%0Afill%20this%20gap%2C%20we%20propose%20Flow%20of%20Reasoning%20%28FoR%29%2C%20an%20efficient%0Adiversity-seeking%20LLM%20finetuning%20method%20aimed%20at%20improving%20reasoning%20quality%0Aand%20diversity%20with%20minimal%20data.%20FoR%20formulates%20multi-step%20LLM%20reasoning%20as%20a%0AMarkovian%20flow%20on%20a%20DAG-structured%20reasoning%20graph.%20This%20formulation%20allows%20us%0Ato%20incorporate%20and%20adapt%20principled%20GFlowNet%20approaches%2C%20for%20finetuning%20LLMs%20to%0Asample%20divergent%20paths%20with%20probabilities%20proportional%20to%20the%20%28unnormalized%29%0Areward%20of%20target%20problems.%20Extensive%20experiments%20show%20that%2C%20with%20limited%0Atraining%20examples%20%28e.g.%2C%2015%20examples%29%2C%20FoR%20enables%20the%20discovery%20of%20diverse%2C%0Acreative%2C%20high-quality%20solutions%2C%20greatly%20outperforming%20a%20wide%20range%20of%0Aexisting%20inference%20and%20training%20methods%20across%20six%20challenging%20reasoning%20tasks%2C%0Aincluding%20BlocksWorld%20%28embodied%20reasoning%29%2C%20Game24%20%28math%20puzzle%20solving%29%2C%0ARubik%27s%20Cube%20%28spatial%20reasoning%29%2C%201D-ARC%20%28abstraction%20reasoning%29%2C%20GSM8k%20%28math%0Areasoning%29%2C%20and%20ProntoQA%20%28logical%20reasoning%29.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Yu-Fangxu/FoR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05673v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlow%2520of%2520Reasoning%253ATraining%2520LLMs%2520for%2520Divergent%2520Problem%2520Solving%2520with%250A%2520%2520Minimal%2520Examples%26entry.906535625%3DFangxu%2520Yu%2520and%2520Lai%2520Jiang%2520and%2520Haoqiang%2520Kang%2520and%2520Shibo%2520Hao%2520and%2520Lianhui%2520Qin%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520generate%2520diverse%2520solutions%2520to%2520a%2520given%2520problem%2520is%2520a%2520hallmark%2520of%250Ahuman%2520creativity.%2520This%2520divergent%2520reasoning%2520is%2520also%2520crucial%2520for%2520machines%252C%250Aenhancing%2520their%2520robustness%2520and%2520enabling%2520them%2520to%2520assist%2520humans%2520in%2520many%250Aapplications%2520such%2520as%2520scientific%2520discovery.%2520However%252C%2520existing%2520approaches%2520to%250Amulti-step%2520reasoning%2520with%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520mostly%2520focused%2520only%250Aon%2520reasoning%2520accuracy%252C%2520without%2520further%2520discovering%2520more%2520diverse%2520valid%250Asolutions.%2520For%2520example%252C%2520supervised%2520fine-tuning%2520can%2520improve%2520LLM%2520reasoning%250Aquality%252C%2520but%2520requires%2520extensive%2520supervised%2520data%2520to%2520capture%2520the%2520full%2520range%2520of%250Apossible%2520solutions.%2520Reward-maximization%2520reinforcement%2520learning%2520aims%2520to%2520find%250Alimited%2520highest-reward%2520solutions%2520while%2520neglecting%2520the%2520solution%2520diversity.%2520To%250Afill%2520this%2520gap%252C%2520we%2520propose%2520Flow%2520of%2520Reasoning%2520%2528FoR%2529%252C%2520an%2520efficient%250Adiversity-seeking%2520LLM%2520finetuning%2520method%2520aimed%2520at%2520improving%2520reasoning%2520quality%250Aand%2520diversity%2520with%2520minimal%2520data.%2520FoR%2520formulates%2520multi-step%2520LLM%2520reasoning%2520as%2520a%250AMarkovian%2520flow%2520on%2520a%2520DAG-structured%2520reasoning%2520graph.%2520This%2520formulation%2520allows%2520us%250Ato%2520incorporate%2520and%2520adapt%2520principled%2520GFlowNet%2520approaches%252C%2520for%2520finetuning%2520LLMs%2520to%250Asample%2520divergent%2520paths%2520with%2520probabilities%2520proportional%2520to%2520the%2520%2528unnormalized%2529%250Areward%2520of%2520target%2520problems.%2520Extensive%2520experiments%2520show%2520that%252C%2520with%2520limited%250Atraining%2520examples%2520%2528e.g.%252C%252015%2520examples%2529%252C%2520FoR%2520enables%2520the%2520discovery%2520of%2520diverse%252C%250Acreative%252C%2520high-quality%2520solutions%252C%2520greatly%2520outperforming%2520a%2520wide%2520range%2520of%250Aexisting%2520inference%2520and%2520training%2520methods%2520across%2520six%2520challenging%2520reasoning%2520tasks%252C%250Aincluding%2520BlocksWorld%2520%2528embodied%2520reasoning%2529%252C%2520Game24%2520%2528math%2520puzzle%2520solving%2529%252C%250ARubik%2527s%2520Cube%2520%2528spatial%2520reasoning%2529%252C%25201D-ARC%2520%2528abstraction%2520reasoning%2529%252C%2520GSM8k%2520%2528math%250Areasoning%2529%252C%2520and%2520ProntoQA%2520%2528logical%2520reasoning%2529.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/Yu-Fangxu/FoR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05673v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flow%20of%20Reasoning%3ATraining%20LLMs%20for%20Divergent%20Problem%20Solving%20with%0A%20%20Minimal%20Examples&entry.906535625=Fangxu%20Yu%20and%20Lai%20Jiang%20and%20Haoqiang%20Kang%20and%20Shibo%20Hao%20and%20Lianhui%20Qin&entry.1292438233=%20%20The%20ability%20to%20generate%20diverse%20solutions%20to%20a%20given%20problem%20is%20a%20hallmark%20of%0Ahuman%20creativity.%20This%20divergent%20reasoning%20is%20also%20crucial%20for%20machines%2C%0Aenhancing%20their%20robustness%20and%20enabling%20them%20to%20assist%20humans%20in%20many%0Aapplications%20such%20as%20scientific%20discovery.%20However%2C%20existing%20approaches%20to%0Amulti-step%20reasoning%20with%20large%20language%20models%20%28LLMs%29%20have%20mostly%20focused%20only%0Aon%20reasoning%20accuracy%2C%20without%20further%20discovering%20more%20diverse%20valid%0Asolutions.%20For%20example%2C%20supervised%20fine-tuning%20can%20improve%20LLM%20reasoning%0Aquality%2C%20but%20requires%20extensive%20supervised%20data%20to%20capture%20the%20full%20range%20of%0Apossible%20solutions.%20Reward-maximization%20reinforcement%20learning%20aims%20to%20find%0Alimited%20highest-reward%20solutions%20while%20neglecting%20the%20solution%20diversity.%20To%0Afill%20this%20gap%2C%20we%20propose%20Flow%20of%20Reasoning%20%28FoR%29%2C%20an%20efficient%0Adiversity-seeking%20LLM%20finetuning%20method%20aimed%20at%20improving%20reasoning%20quality%0Aand%20diversity%20with%20minimal%20data.%20FoR%20formulates%20multi-step%20LLM%20reasoning%20as%20a%0AMarkovian%20flow%20on%20a%20DAG-structured%20reasoning%20graph.%20This%20formulation%20allows%20us%0Ato%20incorporate%20and%20adapt%20principled%20GFlowNet%20approaches%2C%20for%20finetuning%20LLMs%20to%0Asample%20divergent%20paths%20with%20probabilities%20proportional%20to%20the%20%28unnormalized%29%0Areward%20of%20target%20problems.%20Extensive%20experiments%20show%20that%2C%20with%20limited%0Atraining%20examples%20%28e.g.%2C%2015%20examples%29%2C%20FoR%20enables%20the%20discovery%20of%20diverse%2C%0Acreative%2C%20high-quality%20solutions%2C%20greatly%20outperforming%20a%20wide%20range%20of%0Aexisting%20inference%20and%20training%20methods%20across%20six%20challenging%20reasoning%20tasks%2C%0Aincluding%20BlocksWorld%20%28embodied%20reasoning%29%2C%20Game24%20%28math%20puzzle%20solving%29%2C%0ARubik%27s%20Cube%20%28spatial%20reasoning%29%2C%201D-ARC%20%28abstraction%20reasoning%29%2C%20GSM8k%20%28math%0Areasoning%29%2C%20and%20ProntoQA%20%28logical%20reasoning%29.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Yu-Fangxu/FoR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05673v4&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


