<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250601.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional\n  Multiview Diffusion", "author": "Yangyi Huang and Ye Yuan and Xueting Li and Jan Kautz and Umar Iqbal", "abstract": "  Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.\n", "link": "http://arxiv.org/abs/2505.24877v1", "date": "2025-05-30", "relevancy": 3.4445, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7224}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6721}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaHuman%3A%20Animatable%20Detailed%203D%20Human%20Generation%20with%20Compositional%0A%20%20Multiview%20Diffusion&body=Title%3A%20AdaHuman%3A%20Animatable%20Detailed%203D%20Human%20Generation%20with%20Compositional%0A%20%20Multiview%20Diffusion%0AAuthor%3A%20Yangyi%20Huang%20and%20Ye%20Yuan%20and%20Xueting%20Li%20and%20Jan%20Kautz%20and%20Umar%20Iqbal%0AAbstract%3A%20%20%20Existing%20methods%20for%20image-to-3D%20avatar%20generation%20struggle%20to%20produce%20highly%0Adetailed%2C%20animation-ready%20avatars%20suitable%20for%20real-world%20applications.%20We%0Aintroduce%20AdaHuman%2C%20a%20novel%20framework%20that%20generates%20high-fidelity%20animatable%0A3D%20avatars%20from%20a%20single%20in-the-wild%20image.%20AdaHuman%20incorporates%20two%20key%0Ainnovations%3A%20%281%29%20A%20pose-conditioned%203D%20joint%20diffusion%20model%20that%20synthesizes%0Aconsistent%20multi-view%20images%20in%20arbitrary%20poses%20alongside%20corresponding%203D%0AGaussian%20Splats%20%283DGS%29%20reconstruction%20at%20each%20diffusion%20step%3B%20%282%29%20A%0Acompositional%203DGS%20refinement%20module%20that%20enhances%20the%20details%20of%20local%20body%0Aparts%20through%20image-to-image%20refinement%20and%20seamlessly%20integrates%20them%20using%20a%0Anovel%20crop-aware%20camera%20ray%20map%2C%20producing%20a%20cohesive%20detailed%203D%20avatar.%20These%0Acomponents%20allow%20AdaHuman%20to%20generate%20highly%20realistic%20standardized%20A-pose%0Aavatars%20with%20minimal%20self-occlusion%2C%20enabling%20rigging%20and%20animation%20with%20any%0Ainput%20motion.%20Extensive%20evaluation%20on%20public%20benchmarks%20and%20in-the-wild%20images%0Ademonstrates%20that%20AdaHuman%20significantly%20outperforms%20state-of-the-art%20methods%0Ain%20both%20avatar%20reconstruction%20and%20reposing.%20Code%20and%20models%20will%20be%20publicly%0Aavailable%20for%20research%20purposes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24877v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaHuman%253A%2520Animatable%2520Detailed%25203D%2520Human%2520Generation%2520with%2520Compositional%250A%2520%2520Multiview%2520Diffusion%26entry.906535625%3DYangyi%2520Huang%2520and%2520Ye%2520Yuan%2520and%2520Xueting%2520Li%2520and%2520Jan%2520Kautz%2520and%2520Umar%2520Iqbal%26entry.1292438233%3D%2520%2520Existing%2520methods%2520for%2520image-to-3D%2520avatar%2520generation%2520struggle%2520to%2520produce%2520highly%250Adetailed%252C%2520animation-ready%2520avatars%2520suitable%2520for%2520real-world%2520applications.%2520We%250Aintroduce%2520AdaHuman%252C%2520a%2520novel%2520framework%2520that%2520generates%2520high-fidelity%2520animatable%250A3D%2520avatars%2520from%2520a%2520single%2520in-the-wild%2520image.%2520AdaHuman%2520incorporates%2520two%2520key%250Ainnovations%253A%2520%25281%2529%2520A%2520pose-conditioned%25203D%2520joint%2520diffusion%2520model%2520that%2520synthesizes%250Aconsistent%2520multi-view%2520images%2520in%2520arbitrary%2520poses%2520alongside%2520corresponding%25203D%250AGaussian%2520Splats%2520%25283DGS%2529%2520reconstruction%2520at%2520each%2520diffusion%2520step%253B%2520%25282%2529%2520A%250Acompositional%25203DGS%2520refinement%2520module%2520that%2520enhances%2520the%2520details%2520of%2520local%2520body%250Aparts%2520through%2520image-to-image%2520refinement%2520and%2520seamlessly%2520integrates%2520them%2520using%2520a%250Anovel%2520crop-aware%2520camera%2520ray%2520map%252C%2520producing%2520a%2520cohesive%2520detailed%25203D%2520avatar.%2520These%250Acomponents%2520allow%2520AdaHuman%2520to%2520generate%2520highly%2520realistic%2520standardized%2520A-pose%250Aavatars%2520with%2520minimal%2520self-occlusion%252C%2520enabling%2520rigging%2520and%2520animation%2520with%2520any%250Ainput%2520motion.%2520Extensive%2520evaluation%2520on%2520public%2520benchmarks%2520and%2520in-the-wild%2520images%250Ademonstrates%2520that%2520AdaHuman%2520significantly%2520outperforms%2520state-of-the-art%2520methods%250Ain%2520both%2520avatar%2520reconstruction%2520and%2520reposing.%2520Code%2520and%2520models%2520will%2520be%2520publicly%250Aavailable%2520for%2520research%2520purposes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24877v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaHuman%3A%20Animatable%20Detailed%203D%20Human%20Generation%20with%20Compositional%0A%20%20Multiview%20Diffusion&entry.906535625=Yangyi%20Huang%20and%20Ye%20Yuan%20and%20Xueting%20Li%20and%20Jan%20Kautz%20and%20Umar%20Iqbal&entry.1292438233=%20%20Existing%20methods%20for%20image-to-3D%20avatar%20generation%20struggle%20to%20produce%20highly%0Adetailed%2C%20animation-ready%20avatars%20suitable%20for%20real-world%20applications.%20We%0Aintroduce%20AdaHuman%2C%20a%20novel%20framework%20that%20generates%20high-fidelity%20animatable%0A3D%20avatars%20from%20a%20single%20in-the-wild%20image.%20AdaHuman%20incorporates%20two%20key%0Ainnovations%3A%20%281%29%20A%20pose-conditioned%203D%20joint%20diffusion%20model%20that%20synthesizes%0Aconsistent%20multi-view%20images%20in%20arbitrary%20poses%20alongside%20corresponding%203D%0AGaussian%20Splats%20%283DGS%29%20reconstruction%20at%20each%20diffusion%20step%3B%20%282%29%20A%0Acompositional%203DGS%20refinement%20module%20that%20enhances%20the%20details%20of%20local%20body%0Aparts%20through%20image-to-image%20refinement%20and%20seamlessly%20integrates%20them%20using%20a%0Anovel%20crop-aware%20camera%20ray%20map%2C%20producing%20a%20cohesive%20detailed%203D%20avatar.%20These%0Acomponents%20allow%20AdaHuman%20to%20generate%20highly%20realistic%20standardized%20A-pose%0Aavatars%20with%20minimal%20self-occlusion%2C%20enabling%20rigging%20and%20animation%20with%20any%0Ainput%20motion.%20Extensive%20evaluation%20on%20public%20benchmarks%20and%20in-the-wild%20images%0Ademonstrates%20that%20AdaHuman%20significantly%20outperforms%20state-of-the-art%20methods%0Ain%20both%20avatar%20reconstruction%20and%20reposing.%20Code%20and%20models%20will%20be%20publicly%0Aavailable%20for%20research%20purposes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24877v1&entry.124074799=Read"},
{"title": "Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision\n  Geometry Priors", "author": "Duo Zheng and Shijia Huang and Yanyang Li and Liwei Wang", "abstract": "  Previous research has investigated the application of Multimodal Large\nLanguage Models (MLLMs) in understanding 3D scenes by interpreting them as\nvideos. These approaches generally depend on comprehensive 3D data inputs, such\nas point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research,\nwe advance this field by enhancing the capability of MLLMs to understand and\nreason in 3D spaces directly from video data, without the need for additional\n3D input. We propose a novel and efficient method, the Video-3D Geometry Large\nLanguage Model (VG LLM). Our approach employs a 3D visual geometry encoder that\nextracts 3D prior information from video sequences. This information is\nintegrated with visual tokens and fed into the MLLM. Extensive experiments have\nshown that our method has achieved substantial improvements in various tasks\nrelated to 3D scene understanding and spatial reasoning, all directly learned\nfrom video sources. Impressively, our 4B model, which does not rely on explicit\n3D data inputs, achieves competitive results compared to existing\nstate-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the\nVSI-Bench evaluations.\n", "link": "http://arxiv.org/abs/2505.24625v1", "date": "2025-05-30", "relevancy": 3.2799, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6739}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6739}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Videos%20for%203D%20World%3A%20Enhancing%20MLLMs%20with%203D%20Vision%0A%20%20Geometry%20Priors&body=Title%3A%20Learning%20from%20Videos%20for%203D%20World%3A%20Enhancing%20MLLMs%20with%203D%20Vision%0A%20%20Geometry%20Priors%0AAuthor%3A%20Duo%20Zheng%20and%20Shijia%20Huang%20and%20Yanyang%20Li%20and%20Liwei%20Wang%0AAbstract%3A%20%20%20Previous%20research%20has%20investigated%20the%20application%20of%20Multimodal%20Large%0ALanguage%20Models%20%28MLLMs%29%20in%20understanding%203D%20scenes%20by%20interpreting%20them%20as%0Avideos.%20These%20approaches%20generally%20depend%20on%20comprehensive%203D%20data%20inputs%2C%20such%0Aas%20point%20clouds%20or%20reconstructed%20Bird%27s-Eye%20View%20%28BEV%29%20maps.%20In%20our%20research%2C%0Awe%20advance%20this%20field%20by%20enhancing%20the%20capability%20of%20MLLMs%20to%20understand%20and%0Areason%20in%203D%20spaces%20directly%20from%20video%20data%2C%20without%20the%20need%20for%20additional%0A3D%20input.%20We%20propose%20a%20novel%20and%20efficient%20method%2C%20the%20Video-3D%20Geometry%20Large%0ALanguage%20Model%20%28VG%20LLM%29.%20Our%20approach%20employs%20a%203D%20visual%20geometry%20encoder%20that%0Aextracts%203D%20prior%20information%20from%20video%20sequences.%20This%20information%20is%0Aintegrated%20with%20visual%20tokens%20and%20fed%20into%20the%20MLLM.%20Extensive%20experiments%20have%0Ashown%20that%20our%20method%20has%20achieved%20substantial%20improvements%20in%20various%20tasks%0Arelated%20to%203D%20scene%20understanding%20and%20spatial%20reasoning%2C%20all%20directly%20learned%0Afrom%20video%20sources.%20Impressively%2C%20our%204B%20model%2C%20which%20does%20not%20rely%20on%20explicit%0A3D%20data%20inputs%2C%20achieves%20competitive%20results%20compared%20to%20existing%0Astate-of-the-art%20methods%2C%20and%20even%20surpasses%20the%20Gemini-1.5-Pro%20in%20the%0AVSI-Bench%20evaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24625v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520Videos%2520for%25203D%2520World%253A%2520Enhancing%2520MLLMs%2520with%25203D%2520Vision%250A%2520%2520Geometry%2520Priors%26entry.906535625%3DDuo%2520Zheng%2520and%2520Shijia%2520Huang%2520and%2520Yanyang%2520Li%2520and%2520Liwei%2520Wang%26entry.1292438233%3D%2520%2520Previous%2520research%2520has%2520investigated%2520the%2520application%2520of%2520Multimodal%2520Large%250ALanguage%2520Models%2520%2528MLLMs%2529%2520in%2520understanding%25203D%2520scenes%2520by%2520interpreting%2520them%2520as%250Avideos.%2520These%2520approaches%2520generally%2520depend%2520on%2520comprehensive%25203D%2520data%2520inputs%252C%2520such%250Aas%2520point%2520clouds%2520or%2520reconstructed%2520Bird%2527s-Eye%2520View%2520%2528BEV%2529%2520maps.%2520In%2520our%2520research%252C%250Awe%2520advance%2520this%2520field%2520by%2520enhancing%2520the%2520capability%2520of%2520MLLMs%2520to%2520understand%2520and%250Areason%2520in%25203D%2520spaces%2520directly%2520from%2520video%2520data%252C%2520without%2520the%2520need%2520for%2520additional%250A3D%2520input.%2520We%2520propose%2520a%2520novel%2520and%2520efficient%2520method%252C%2520the%2520Video-3D%2520Geometry%2520Large%250ALanguage%2520Model%2520%2528VG%2520LLM%2529.%2520Our%2520approach%2520employs%2520a%25203D%2520visual%2520geometry%2520encoder%2520that%250Aextracts%25203D%2520prior%2520information%2520from%2520video%2520sequences.%2520This%2520information%2520is%250Aintegrated%2520with%2520visual%2520tokens%2520and%2520fed%2520into%2520the%2520MLLM.%2520Extensive%2520experiments%2520have%250Ashown%2520that%2520our%2520method%2520has%2520achieved%2520substantial%2520improvements%2520in%2520various%2520tasks%250Arelated%2520to%25203D%2520scene%2520understanding%2520and%2520spatial%2520reasoning%252C%2520all%2520directly%2520learned%250Afrom%2520video%2520sources.%2520Impressively%252C%2520our%25204B%2520model%252C%2520which%2520does%2520not%2520rely%2520on%2520explicit%250A3D%2520data%2520inputs%252C%2520achieves%2520competitive%2520results%2520compared%2520to%2520existing%250Astate-of-the-art%2520methods%252C%2520and%2520even%2520surpasses%2520the%2520Gemini-1.5-Pro%2520in%2520the%250AVSI-Bench%2520evaluations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24625v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Videos%20for%203D%20World%3A%20Enhancing%20MLLMs%20with%203D%20Vision%0A%20%20Geometry%20Priors&entry.906535625=Duo%20Zheng%20and%20Shijia%20Huang%20and%20Yanyang%20Li%20and%20Liwei%20Wang&entry.1292438233=%20%20Previous%20research%20has%20investigated%20the%20application%20of%20Multimodal%20Large%0ALanguage%20Models%20%28MLLMs%29%20in%20understanding%203D%20scenes%20by%20interpreting%20them%20as%0Avideos.%20These%20approaches%20generally%20depend%20on%20comprehensive%203D%20data%20inputs%2C%20such%0Aas%20point%20clouds%20or%20reconstructed%20Bird%27s-Eye%20View%20%28BEV%29%20maps.%20In%20our%20research%2C%0Awe%20advance%20this%20field%20by%20enhancing%20the%20capability%20of%20MLLMs%20to%20understand%20and%0Areason%20in%203D%20spaces%20directly%20from%20video%20data%2C%20without%20the%20need%20for%20additional%0A3D%20input.%20We%20propose%20a%20novel%20and%20efficient%20method%2C%20the%20Video-3D%20Geometry%20Large%0ALanguage%20Model%20%28VG%20LLM%29.%20Our%20approach%20employs%20a%203D%20visual%20geometry%20encoder%20that%0Aextracts%203D%20prior%20information%20from%20video%20sequences.%20This%20information%20is%0Aintegrated%20with%20visual%20tokens%20and%20fed%20into%20the%20MLLM.%20Extensive%20experiments%20have%0Ashown%20that%20our%20method%20has%20achieved%20substantial%20improvements%20in%20various%20tasks%0Arelated%20to%203D%20scene%20understanding%20and%20spatial%20reasoning%2C%20all%20directly%20learned%0Afrom%20video%20sources.%20Impressively%2C%20our%204B%20model%2C%20which%20does%20not%20rely%20on%20explicit%0A3D%20data%20inputs%2C%20achieves%20competitive%20results%20compared%20to%20existing%0Astate-of-the-art%20methods%2C%20and%20even%20surpasses%20the%20Gemini-1.5-Pro%20in%20the%0AVSI-Bench%20evaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24625v1&entry.124074799=Read"},
{"title": "Tackling View-Dependent Semantics in 3D Language Gaussian Splatting", "author": "Jiazhong Cen and Xudong Zhou and Jiemin Fang and Changsong Wen and Lingxi Xie and Xiaopeng Zhang and Wei Shen and Qi Tian", "abstract": "  Recent advancements in 3D Gaussian Splatting (3D-GS) enable high-quality 3D\nscene reconstruction from RGB images. Many studies extend this paradigm for\nlanguage-driven open-vocabulary scene understanding. However, most of them\nsimply project 2D semantic features onto 3D Gaussians and overlook a\nfundamental gap between 2D and 3D understanding: a 3D object may exhibit\nvarious semantics from different viewpoints--a phenomenon we term\nview-dependent semantics. To address this challenge, we propose LaGa (Language\nGaussians), which establishes cross-view semantic connections by decomposing\nthe 3D scene into objects. Then, it constructs view-aggregated semantic\nrepresentations by clustering semantic descriptors and reweighting them based\non multi-view semantics. Extensive experiments demonstrate that LaGa\neffectively captures key information from view-dependent semantics, enabling a\nmore comprehensive understanding of 3D scenes. Notably, under the same\nsettings, LaGa achieves a significant improvement of +18.7% mIoU over the\nprevious SOTA on the LERF-OVS dataset. Our code is available at:\nhttps://github.com/SJTU-DeepVisionLab/LaGa.\n", "link": "http://arxiv.org/abs/2505.24746v1", "date": "2025-05-30", "relevancy": 3.2711, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6964}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6502}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tackling%20View-Dependent%20Semantics%20in%203D%20Language%20Gaussian%20Splatting&body=Title%3A%20Tackling%20View-Dependent%20Semantics%20in%203D%20Language%20Gaussian%20Splatting%0AAuthor%3A%20Jiazhong%20Cen%20and%20Xudong%20Zhou%20and%20Jiemin%20Fang%20and%20Changsong%20Wen%20and%20Lingxi%20Xie%20and%20Xiaopeng%20Zhang%20and%20Wei%20Shen%20and%20Qi%20Tian%0AAbstract%3A%20%20%20Recent%20advancements%20in%203D%20Gaussian%20Splatting%20%283D-GS%29%20enable%20high-quality%203D%0Ascene%20reconstruction%20from%20RGB%20images.%20Many%20studies%20extend%20this%20paradigm%20for%0Alanguage-driven%20open-vocabulary%20scene%20understanding.%20However%2C%20most%20of%20them%0Asimply%20project%202D%20semantic%20features%20onto%203D%20Gaussians%20and%20overlook%20a%0Afundamental%20gap%20between%202D%20and%203D%20understanding%3A%20a%203D%20object%20may%20exhibit%0Avarious%20semantics%20from%20different%20viewpoints--a%20phenomenon%20we%20term%0Aview-dependent%20semantics.%20To%20address%20this%20challenge%2C%20we%20propose%20LaGa%20%28Language%0AGaussians%29%2C%20which%20establishes%20cross-view%20semantic%20connections%20by%20decomposing%0Athe%203D%20scene%20into%20objects.%20Then%2C%20it%20constructs%20view-aggregated%20semantic%0Arepresentations%20by%20clustering%20semantic%20descriptors%20and%20reweighting%20them%20based%0Aon%20multi-view%20semantics.%20Extensive%20experiments%20demonstrate%20that%20LaGa%0Aeffectively%20captures%20key%20information%20from%20view-dependent%20semantics%2C%20enabling%20a%0Amore%20comprehensive%20understanding%20of%203D%20scenes.%20Notably%2C%20under%20the%20same%0Asettings%2C%20LaGa%20achieves%20a%20significant%20improvement%20of%20%2B18.7%25%20mIoU%20over%20the%0Aprevious%20SOTA%20on%20the%20LERF-OVS%20dataset.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/SJTU-DeepVisionLab/LaGa.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24746v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTackling%2520View-Dependent%2520Semantics%2520in%25203D%2520Language%2520Gaussian%2520Splatting%26entry.906535625%3DJiazhong%2520Cen%2520and%2520Xudong%2520Zhou%2520and%2520Jiemin%2520Fang%2520and%2520Changsong%2520Wen%2520and%2520Lingxi%2520Xie%2520and%2520Xiaopeng%2520Zhang%2520and%2520Wei%2520Shen%2520and%2520Qi%2520Tian%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%25203D%2520Gaussian%2520Splatting%2520%25283D-GS%2529%2520enable%2520high-quality%25203D%250Ascene%2520reconstruction%2520from%2520RGB%2520images.%2520Many%2520studies%2520extend%2520this%2520paradigm%2520for%250Alanguage-driven%2520open-vocabulary%2520scene%2520understanding.%2520However%252C%2520most%2520of%2520them%250Asimply%2520project%25202D%2520semantic%2520features%2520onto%25203D%2520Gaussians%2520and%2520overlook%2520a%250Afundamental%2520gap%2520between%25202D%2520and%25203D%2520understanding%253A%2520a%25203D%2520object%2520may%2520exhibit%250Avarious%2520semantics%2520from%2520different%2520viewpoints--a%2520phenomenon%2520we%2520term%250Aview-dependent%2520semantics.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520LaGa%2520%2528Language%250AGaussians%2529%252C%2520which%2520establishes%2520cross-view%2520semantic%2520connections%2520by%2520decomposing%250Athe%25203D%2520scene%2520into%2520objects.%2520Then%252C%2520it%2520constructs%2520view-aggregated%2520semantic%250Arepresentations%2520by%2520clustering%2520semantic%2520descriptors%2520and%2520reweighting%2520them%2520based%250Aon%2520multi-view%2520semantics.%2520Extensive%2520experiments%2520demonstrate%2520that%2520LaGa%250Aeffectively%2520captures%2520key%2520information%2520from%2520view-dependent%2520semantics%252C%2520enabling%2520a%250Amore%2520comprehensive%2520understanding%2520of%25203D%2520scenes.%2520Notably%252C%2520under%2520the%2520same%250Asettings%252C%2520LaGa%2520achieves%2520a%2520significant%2520improvement%2520of%2520%252B18.7%2525%2520mIoU%2520over%2520the%250Aprevious%2520SOTA%2520on%2520the%2520LERF-OVS%2520dataset.%2520Our%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/SJTU-DeepVisionLab/LaGa.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24746v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tackling%20View-Dependent%20Semantics%20in%203D%20Language%20Gaussian%20Splatting&entry.906535625=Jiazhong%20Cen%20and%20Xudong%20Zhou%20and%20Jiemin%20Fang%20and%20Changsong%20Wen%20and%20Lingxi%20Xie%20and%20Xiaopeng%20Zhang%20and%20Wei%20Shen%20and%20Qi%20Tian&entry.1292438233=%20%20Recent%20advancements%20in%203D%20Gaussian%20Splatting%20%283D-GS%29%20enable%20high-quality%203D%0Ascene%20reconstruction%20from%20RGB%20images.%20Many%20studies%20extend%20this%20paradigm%20for%0Alanguage-driven%20open-vocabulary%20scene%20understanding.%20However%2C%20most%20of%20them%0Asimply%20project%202D%20semantic%20features%20onto%203D%20Gaussians%20and%20overlook%20a%0Afundamental%20gap%20between%202D%20and%203D%20understanding%3A%20a%203D%20object%20may%20exhibit%0Avarious%20semantics%20from%20different%20viewpoints--a%20phenomenon%20we%20term%0Aview-dependent%20semantics.%20To%20address%20this%20challenge%2C%20we%20propose%20LaGa%20%28Language%0AGaussians%29%2C%20which%20establishes%20cross-view%20semantic%20connections%20by%20decomposing%0Athe%203D%20scene%20into%20objects.%20Then%2C%20it%20constructs%20view-aggregated%20semantic%0Arepresentations%20by%20clustering%20semantic%20descriptors%20and%20reweighting%20them%20based%0Aon%20multi-view%20semantics.%20Extensive%20experiments%20demonstrate%20that%20LaGa%0Aeffectively%20captures%20key%20information%20from%20view-dependent%20semantics%2C%20enabling%20a%0Amore%20comprehensive%20understanding%20of%203D%20scenes.%20Notably%2C%20under%20the%20same%0Asettings%2C%20LaGa%20achieves%20a%20significant%20improvement%20of%20%2B18.7%25%20mIoU%20over%20the%0Aprevious%20SOTA%20on%20the%20LERF-OVS%20dataset.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/SJTU-DeepVisionLab/LaGa.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24746v1&entry.124074799=Read"},
{"title": "Structured 3D Latents for Scalable and Versatile 3D Generation", "author": "Jianfeng Xiang and Zelong Lv and Sicheng Xu and Yu Deng and Ruicheng Wang and Bowen Zhang and Dong Chen and Xin Tong and Jiaolong Yang", "abstract": "  We introduce a novel 3D generation method for versatile and high-quality 3D\nasset creation. The cornerstone is a unified Structured LATent (SLAT)\nrepresentation which allows decoding to different output formats, such as\nRadiance Fields, 3D Gaussians, and meshes. This is achieved by integrating a\nsparsely-populated 3D grid with dense multiview visual features extracted from\na powerful vision foundation model, comprehensively capturing both structural\n(geometry) and textural (appearance) information while maintaining flexibility\nduring decoding. We employ rectified flow transformers tailored for SLAT as our\n3D generation models and train models with up to 2 billion parameters on a\nlarge 3D asset dataset of 500K diverse objects. Our model generates\nhigh-quality results with text or image conditions, significantly surpassing\nexisting methods, including recent ones at similar scales. We showcase flexible\noutput format selection and local 3D editing capabilities which were not\noffered by previous models. Code, model, and data will be released.\n", "link": "http://arxiv.org/abs/2412.01506v3", "date": "2025-05-30", "relevancy": 3.1488, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6443}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6225}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structured%203D%20Latents%20for%20Scalable%20and%20Versatile%203D%20Generation&body=Title%3A%20Structured%203D%20Latents%20for%20Scalable%20and%20Versatile%203D%20Generation%0AAuthor%3A%20Jianfeng%20Xiang%20and%20Zelong%20Lv%20and%20Sicheng%20Xu%20and%20Yu%20Deng%20and%20Ruicheng%20Wang%20and%20Bowen%20Zhang%20and%20Dong%20Chen%20and%20Xin%20Tong%20and%20Jiaolong%20Yang%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%203D%20generation%20method%20for%20versatile%20and%20high-quality%203D%0Aasset%20creation.%20The%20cornerstone%20is%20a%20unified%20Structured%20LATent%20%28SLAT%29%0Arepresentation%20which%20allows%20decoding%20to%20different%20output%20formats%2C%20such%20as%0ARadiance%20Fields%2C%203D%20Gaussians%2C%20and%20meshes.%20This%20is%20achieved%20by%20integrating%20a%0Asparsely-populated%203D%20grid%20with%20dense%20multiview%20visual%20features%20extracted%20from%0Aa%20powerful%20vision%20foundation%20model%2C%20comprehensively%20capturing%20both%20structural%0A%28geometry%29%20and%20textural%20%28appearance%29%20information%20while%20maintaining%20flexibility%0Aduring%20decoding.%20We%20employ%20rectified%20flow%20transformers%20tailored%20for%20SLAT%20as%20our%0A3D%20generation%20models%20and%20train%20models%20with%20up%20to%202%20billion%20parameters%20on%20a%0Alarge%203D%20asset%20dataset%20of%20500K%20diverse%20objects.%20Our%20model%20generates%0Ahigh-quality%20results%20with%20text%20or%20image%20conditions%2C%20significantly%20surpassing%0Aexisting%20methods%2C%20including%20recent%20ones%20at%20similar%20scales.%20We%20showcase%20flexible%0Aoutput%20format%20selection%20and%20local%203D%20editing%20capabilities%20which%20were%20not%0Aoffered%20by%20previous%20models.%20Code%2C%20model%2C%20and%20data%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.01506v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructured%25203D%2520Latents%2520for%2520Scalable%2520and%2520Versatile%25203D%2520Generation%26entry.906535625%3DJianfeng%2520Xiang%2520and%2520Zelong%2520Lv%2520and%2520Sicheng%2520Xu%2520and%2520Yu%2520Deng%2520and%2520Ruicheng%2520Wang%2520and%2520Bowen%2520Zhang%2520and%2520Dong%2520Chen%2520and%2520Xin%2520Tong%2520and%2520Jiaolong%2520Yang%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%25203D%2520generation%2520method%2520for%2520versatile%2520and%2520high-quality%25203D%250Aasset%2520creation.%2520The%2520cornerstone%2520is%2520a%2520unified%2520Structured%2520LATent%2520%2528SLAT%2529%250Arepresentation%2520which%2520allows%2520decoding%2520to%2520different%2520output%2520formats%252C%2520such%2520as%250ARadiance%2520Fields%252C%25203D%2520Gaussians%252C%2520and%2520meshes.%2520This%2520is%2520achieved%2520by%2520integrating%2520a%250Asparsely-populated%25203D%2520grid%2520with%2520dense%2520multiview%2520visual%2520features%2520extracted%2520from%250Aa%2520powerful%2520vision%2520foundation%2520model%252C%2520comprehensively%2520capturing%2520both%2520structural%250A%2528geometry%2529%2520and%2520textural%2520%2528appearance%2529%2520information%2520while%2520maintaining%2520flexibility%250Aduring%2520decoding.%2520We%2520employ%2520rectified%2520flow%2520transformers%2520tailored%2520for%2520SLAT%2520as%2520our%250A3D%2520generation%2520models%2520and%2520train%2520models%2520with%2520up%2520to%25202%2520billion%2520parameters%2520on%2520a%250Alarge%25203D%2520asset%2520dataset%2520of%2520500K%2520diverse%2520objects.%2520Our%2520model%2520generates%250Ahigh-quality%2520results%2520with%2520text%2520or%2520image%2520conditions%252C%2520significantly%2520surpassing%250Aexisting%2520methods%252C%2520including%2520recent%2520ones%2520at%2520similar%2520scales.%2520We%2520showcase%2520flexible%250Aoutput%2520format%2520selection%2520and%2520local%25203D%2520editing%2520capabilities%2520which%2520were%2520not%250Aoffered%2520by%2520previous%2520models.%2520Code%252C%2520model%252C%2520and%2520data%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01506v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structured%203D%20Latents%20for%20Scalable%20and%20Versatile%203D%20Generation&entry.906535625=Jianfeng%20Xiang%20and%20Zelong%20Lv%20and%20Sicheng%20Xu%20and%20Yu%20Deng%20and%20Ruicheng%20Wang%20and%20Bowen%20Zhang%20and%20Dong%20Chen%20and%20Xin%20Tong%20and%20Jiaolong%20Yang&entry.1292438233=%20%20We%20introduce%20a%20novel%203D%20generation%20method%20for%20versatile%20and%20high-quality%203D%0Aasset%20creation.%20The%20cornerstone%20is%20a%20unified%20Structured%20LATent%20%28SLAT%29%0Arepresentation%20which%20allows%20decoding%20to%20different%20output%20formats%2C%20such%20as%0ARadiance%20Fields%2C%203D%20Gaussians%2C%20and%20meshes.%20This%20is%20achieved%20by%20integrating%20a%0Asparsely-populated%203D%20grid%20with%20dense%20multiview%20visual%20features%20extracted%20from%0Aa%20powerful%20vision%20foundation%20model%2C%20comprehensively%20capturing%20both%20structural%0A%28geometry%29%20and%20textural%20%28appearance%29%20information%20while%20maintaining%20flexibility%0Aduring%20decoding.%20We%20employ%20rectified%20flow%20transformers%20tailored%20for%20SLAT%20as%20our%0A3D%20generation%20models%20and%20train%20models%20with%20up%20to%202%20billion%20parameters%20on%20a%0Alarge%203D%20asset%20dataset%20of%20500K%20diverse%20objects.%20Our%20model%20generates%0Ahigh-quality%20results%20with%20text%20or%20image%20conditions%2C%20significantly%20surpassing%0Aexisting%20methods%2C%20including%20recent%20ones%20at%20similar%20scales.%20We%20showcase%20flexible%0Aoutput%20format%20selection%20and%20local%203D%20editing%20capabilities%20which%20were%20not%0Aoffered%20by%20previous%20models.%20Code%2C%20model%2C%20and%20data%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.01506v3&entry.124074799=Read"},
{"title": "DreamDance: Animating Character Art via Inpainting Stable Gaussian\n  Worlds", "author": "Jiaxu Zhang and Xianfang Zeng and Xin Chen and Wei Zuo and Gang Yu and Guosheng Lin and Zhigang Tu", "abstract": "  This paper presents DreamDance, a novel character art animation framework\ncapable of producing stable, consistent character and scene motion conditioned\non precise camera trajectories. To achieve this, we re-formulate the animation\ntask as two inpainting-based steps: Camera-aware Scene Inpainting and\nPose-aware Video Inpainting. The first step leverages a pre-trained image\ninpainting model to generate multi-view scene images from the reference art and\noptimizes a stable large-scale Gaussian field, which enables coarse background\nvideo rendering with camera trajectories. However, the rendered video is rough\nand only conveys scene motion. To resolve this, the second step trains a\npose-aware video inpainting model that injects the dynamic character into the\nscene video while enhancing background quality. Specifically, this model is a\nDiT-based video generation model with a gating strategy that adaptively\nintegrates the character's appearance and pose information into the base\nbackground video. Through extensive experiments, we demonstrate the\neffectiveness and generalizability of DreamDance, producing high-quality and\nconsistent character animations with remarkable camera dynamics.\n", "link": "http://arxiv.org/abs/2505.24733v1", "date": "2025-05-30", "relevancy": 3.1358, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6475}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6384}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamDance%3A%20Animating%20Character%20Art%20via%20Inpainting%20Stable%20Gaussian%0A%20%20Worlds&body=Title%3A%20DreamDance%3A%20Animating%20Character%20Art%20via%20Inpainting%20Stable%20Gaussian%0A%20%20Worlds%0AAuthor%3A%20Jiaxu%20Zhang%20and%20Xianfang%20Zeng%20and%20Xin%20Chen%20and%20Wei%20Zuo%20and%20Gang%20Yu%20and%20Guosheng%20Lin%20and%20Zhigang%20Tu%0AAbstract%3A%20%20%20This%20paper%20presents%20DreamDance%2C%20a%20novel%20character%20art%20animation%20framework%0Acapable%20of%20producing%20stable%2C%20consistent%20character%20and%20scene%20motion%20conditioned%0Aon%20precise%20camera%20trajectories.%20To%20achieve%20this%2C%20we%20re-formulate%20the%20animation%0Atask%20as%20two%20inpainting-based%20steps%3A%20Camera-aware%20Scene%20Inpainting%20and%0APose-aware%20Video%20Inpainting.%20The%20first%20step%20leverages%20a%20pre-trained%20image%0Ainpainting%20model%20to%20generate%20multi-view%20scene%20images%20from%20the%20reference%20art%20and%0Aoptimizes%20a%20stable%20large-scale%20Gaussian%20field%2C%20which%20enables%20coarse%20background%0Avideo%20rendering%20with%20camera%20trajectories.%20However%2C%20the%20rendered%20video%20is%20rough%0Aand%20only%20conveys%20scene%20motion.%20To%20resolve%20this%2C%20the%20second%20step%20trains%20a%0Apose-aware%20video%20inpainting%20model%20that%20injects%20the%20dynamic%20character%20into%20the%0Ascene%20video%20while%20enhancing%20background%20quality.%20Specifically%2C%20this%20model%20is%20a%0ADiT-based%20video%20generation%20model%20with%20a%20gating%20strategy%20that%20adaptively%0Aintegrates%20the%20character%27s%20appearance%20and%20pose%20information%20into%20the%20base%0Abackground%20video.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20the%0Aeffectiveness%20and%20generalizability%20of%20DreamDance%2C%20producing%20high-quality%20and%0Aconsistent%20character%20animations%20with%20remarkable%20camera%20dynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24733v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamDance%253A%2520Animating%2520Character%2520Art%2520via%2520Inpainting%2520Stable%2520Gaussian%250A%2520%2520Worlds%26entry.906535625%3DJiaxu%2520Zhang%2520and%2520Xianfang%2520Zeng%2520and%2520Xin%2520Chen%2520and%2520Wei%2520Zuo%2520and%2520Gang%2520Yu%2520and%2520Guosheng%2520Lin%2520and%2520Zhigang%2520Tu%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520DreamDance%252C%2520a%2520novel%2520character%2520art%2520animation%2520framework%250Acapable%2520of%2520producing%2520stable%252C%2520consistent%2520character%2520and%2520scene%2520motion%2520conditioned%250Aon%2520precise%2520camera%2520trajectories.%2520To%2520achieve%2520this%252C%2520we%2520re-formulate%2520the%2520animation%250Atask%2520as%2520two%2520inpainting-based%2520steps%253A%2520Camera-aware%2520Scene%2520Inpainting%2520and%250APose-aware%2520Video%2520Inpainting.%2520The%2520first%2520step%2520leverages%2520a%2520pre-trained%2520image%250Ainpainting%2520model%2520to%2520generate%2520multi-view%2520scene%2520images%2520from%2520the%2520reference%2520art%2520and%250Aoptimizes%2520a%2520stable%2520large-scale%2520Gaussian%2520field%252C%2520which%2520enables%2520coarse%2520background%250Avideo%2520rendering%2520with%2520camera%2520trajectories.%2520However%252C%2520the%2520rendered%2520video%2520is%2520rough%250Aand%2520only%2520conveys%2520scene%2520motion.%2520To%2520resolve%2520this%252C%2520the%2520second%2520step%2520trains%2520a%250Apose-aware%2520video%2520inpainting%2520model%2520that%2520injects%2520the%2520dynamic%2520character%2520into%2520the%250Ascene%2520video%2520while%2520enhancing%2520background%2520quality.%2520Specifically%252C%2520this%2520model%2520is%2520a%250ADiT-based%2520video%2520generation%2520model%2520with%2520a%2520gating%2520strategy%2520that%2520adaptively%250Aintegrates%2520the%2520character%2527s%2520appearance%2520and%2520pose%2520information%2520into%2520the%2520base%250Abackground%2520video.%2520Through%2520extensive%2520experiments%252C%2520we%2520demonstrate%2520the%250Aeffectiveness%2520and%2520generalizability%2520of%2520DreamDance%252C%2520producing%2520high-quality%2520and%250Aconsistent%2520character%2520animations%2520with%2520remarkable%2520camera%2520dynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24733v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamDance%3A%20Animating%20Character%20Art%20via%20Inpainting%20Stable%20Gaussian%0A%20%20Worlds&entry.906535625=Jiaxu%20Zhang%20and%20Xianfang%20Zeng%20and%20Xin%20Chen%20and%20Wei%20Zuo%20and%20Gang%20Yu%20and%20Guosheng%20Lin%20and%20Zhigang%20Tu&entry.1292438233=%20%20This%20paper%20presents%20DreamDance%2C%20a%20novel%20character%20art%20animation%20framework%0Acapable%20of%20producing%20stable%2C%20consistent%20character%20and%20scene%20motion%20conditioned%0Aon%20precise%20camera%20trajectories.%20To%20achieve%20this%2C%20we%20re-formulate%20the%20animation%0Atask%20as%20two%20inpainting-based%20steps%3A%20Camera-aware%20Scene%20Inpainting%20and%0APose-aware%20Video%20Inpainting.%20The%20first%20step%20leverages%20a%20pre-trained%20image%0Ainpainting%20model%20to%20generate%20multi-view%20scene%20images%20from%20the%20reference%20art%20and%0Aoptimizes%20a%20stable%20large-scale%20Gaussian%20field%2C%20which%20enables%20coarse%20background%0Avideo%20rendering%20with%20camera%20trajectories.%20However%2C%20the%20rendered%20video%20is%20rough%0Aand%20only%20conveys%20scene%20motion.%20To%20resolve%20this%2C%20the%20second%20step%20trains%20a%0Apose-aware%20video%20inpainting%20model%20that%20injects%20the%20dynamic%20character%20into%20the%0Ascene%20video%20while%20enhancing%20background%20quality.%20Specifically%2C%20this%20model%20is%20a%0ADiT-based%20video%20generation%20model%20with%20a%20gating%20strategy%20that%20adaptively%0Aintegrates%20the%20character%27s%20appearance%20and%20pose%20information%20into%20the%20base%0Abackground%20video.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20the%0Aeffectiveness%20and%20generalizability%20of%20DreamDance%2C%20producing%20high-quality%20and%0Aconsistent%20character%20animations%20with%20remarkable%20camera%20dynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24733v1&entry.124074799=Read"},
{"title": "CraftsMan3D: High-fidelity Mesh Generation with 3D Native Generation and\n  Interactive Geometry Refiner", "author": "Weiyu Li and Jiarui Liu and Hongyu Yan and Rui Chen and Yixun Liang and Xuelin Chen and Ping Tan and Xiaoxiao Long", "abstract": "  We present a novel generative 3D modeling system, coined CraftsMan, which can\ngenerate high-fidelity 3D geometries with highly varied shapes, regular mesh\ntopologies, and detailed surfaces, and, notably, allows for refining the\ngeometry in an interactive manner. Despite the significant advancements in 3D\ngeneration, existing methods still struggle with lengthy optimization\nprocesses, irregular mesh topologies, noisy surfaces, and difficulties in\naccommodating user edits, consequently impeding their widespread adoption and\nimplementation in 3D modeling software. Our work is inspired by the craftsman,\nwho usually roughs out the holistic figure of the work first and elaborates the\nsurface details subsequently. Specifically, we employ a 3D native diffusion\nmodel, which operates on latent space learned from latent set-based 3D\nrepresentations, to generate coarse geometries with regular mesh topology in\nseconds. In particular, this process takes as input a text prompt or a\nreference image and leverages a powerful multi-view (MV) diffusion model to\ngenerate multiple views of the coarse geometry, which are fed into our\nMV-conditioned 3D diffusion model for generating the 3D geometry, significantly\nimproving robustness and generalizability. Following that, a normal-based\ngeometry refiner is used to significantly enhance the surface details. This\nrefinement can be performed automatically, or interactively with user-supplied\nedits. Extensive experiments demonstrate that our method achieves high efficacy\nin producing superior-quality 3D assets compared to existing methods. HomePage:\nhttps://craftsman3d.github.io/, Code: https://github.com/wyysf-98/CraftsMan\n", "link": "http://arxiv.org/abs/2405.14979v4", "date": "2025-05-30", "relevancy": 3.1315, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6456}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6456}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CraftsMan3D%3A%20High-fidelity%20Mesh%20Generation%20with%203D%20Native%20Generation%20and%0A%20%20Interactive%20Geometry%20Refiner&body=Title%3A%20CraftsMan3D%3A%20High-fidelity%20Mesh%20Generation%20with%203D%20Native%20Generation%20and%0A%20%20Interactive%20Geometry%20Refiner%0AAuthor%3A%20Weiyu%20Li%20and%20Jiarui%20Liu%20and%20Hongyu%20Yan%20and%20Rui%20Chen%20and%20Yixun%20Liang%20and%20Xuelin%20Chen%20and%20Ping%20Tan%20and%20Xiaoxiao%20Long%0AAbstract%3A%20%20%20We%20present%20a%20novel%20generative%203D%20modeling%20system%2C%20coined%20CraftsMan%2C%20which%20can%0Agenerate%20high-fidelity%203D%20geometries%20with%20highly%20varied%20shapes%2C%20regular%20mesh%0Atopologies%2C%20and%20detailed%20surfaces%2C%20and%2C%20notably%2C%20allows%20for%20refining%20the%0Ageometry%20in%20an%20interactive%20manner.%20Despite%20the%20significant%20advancements%20in%203D%0Ageneration%2C%20existing%20methods%20still%20struggle%20with%20lengthy%20optimization%0Aprocesses%2C%20irregular%20mesh%20topologies%2C%20noisy%20surfaces%2C%20and%20difficulties%20in%0Aaccommodating%20user%20edits%2C%20consequently%20impeding%20their%20widespread%20adoption%20and%0Aimplementation%20in%203D%20modeling%20software.%20Our%20work%20is%20inspired%20by%20the%20craftsman%2C%0Awho%20usually%20roughs%20out%20the%20holistic%20figure%20of%20the%20work%20first%20and%20elaborates%20the%0Asurface%20details%20subsequently.%20Specifically%2C%20we%20employ%20a%203D%20native%20diffusion%0Amodel%2C%20which%20operates%20on%20latent%20space%20learned%20from%20latent%20set-based%203D%0Arepresentations%2C%20to%20generate%20coarse%20geometries%20with%20regular%20mesh%20topology%20in%0Aseconds.%20In%20particular%2C%20this%20process%20takes%20as%20input%20a%20text%20prompt%20or%20a%0Areference%20image%20and%20leverages%20a%20powerful%20multi-view%20%28MV%29%20diffusion%20model%20to%0Agenerate%20multiple%20views%20of%20the%20coarse%20geometry%2C%20which%20are%20fed%20into%20our%0AMV-conditioned%203D%20diffusion%20model%20for%20generating%20the%203D%20geometry%2C%20significantly%0Aimproving%20robustness%20and%20generalizability.%20Following%20that%2C%20a%20normal-based%0Ageometry%20refiner%20is%20used%20to%20significantly%20enhance%20the%20surface%20details.%20This%0Arefinement%20can%20be%20performed%20automatically%2C%20or%20interactively%20with%20user-supplied%0Aedits.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%20high%20efficacy%0Ain%20producing%20superior-quality%203D%20assets%20compared%20to%20existing%20methods.%20HomePage%3A%0Ahttps%3A//craftsman3d.github.io/%2C%20Code%3A%20https%3A//github.com/wyysf-98/CraftsMan%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14979v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCraftsMan3D%253A%2520High-fidelity%2520Mesh%2520Generation%2520with%25203D%2520Native%2520Generation%2520and%250A%2520%2520Interactive%2520Geometry%2520Refiner%26entry.906535625%3DWeiyu%2520Li%2520and%2520Jiarui%2520Liu%2520and%2520Hongyu%2520Yan%2520and%2520Rui%2520Chen%2520and%2520Yixun%2520Liang%2520and%2520Xuelin%2520Chen%2520and%2520Ping%2520Tan%2520and%2520Xiaoxiao%2520Long%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520generative%25203D%2520modeling%2520system%252C%2520coined%2520CraftsMan%252C%2520which%2520can%250Agenerate%2520high-fidelity%25203D%2520geometries%2520with%2520highly%2520varied%2520shapes%252C%2520regular%2520mesh%250Atopologies%252C%2520and%2520detailed%2520surfaces%252C%2520and%252C%2520notably%252C%2520allows%2520for%2520refining%2520the%250Ageometry%2520in%2520an%2520interactive%2520manner.%2520Despite%2520the%2520significant%2520advancements%2520in%25203D%250Ageneration%252C%2520existing%2520methods%2520still%2520struggle%2520with%2520lengthy%2520optimization%250Aprocesses%252C%2520irregular%2520mesh%2520topologies%252C%2520noisy%2520surfaces%252C%2520and%2520difficulties%2520in%250Aaccommodating%2520user%2520edits%252C%2520consequently%2520impeding%2520their%2520widespread%2520adoption%2520and%250Aimplementation%2520in%25203D%2520modeling%2520software.%2520Our%2520work%2520is%2520inspired%2520by%2520the%2520craftsman%252C%250Awho%2520usually%2520roughs%2520out%2520the%2520holistic%2520figure%2520of%2520the%2520work%2520first%2520and%2520elaborates%2520the%250Asurface%2520details%2520subsequently.%2520Specifically%252C%2520we%2520employ%2520a%25203D%2520native%2520diffusion%250Amodel%252C%2520which%2520operates%2520on%2520latent%2520space%2520learned%2520from%2520latent%2520set-based%25203D%250Arepresentations%252C%2520to%2520generate%2520coarse%2520geometries%2520with%2520regular%2520mesh%2520topology%2520in%250Aseconds.%2520In%2520particular%252C%2520this%2520process%2520takes%2520as%2520input%2520a%2520text%2520prompt%2520or%2520a%250Areference%2520image%2520and%2520leverages%2520a%2520powerful%2520multi-view%2520%2528MV%2529%2520diffusion%2520model%2520to%250Agenerate%2520multiple%2520views%2520of%2520the%2520coarse%2520geometry%252C%2520which%2520are%2520fed%2520into%2520our%250AMV-conditioned%25203D%2520diffusion%2520model%2520for%2520generating%2520the%25203D%2520geometry%252C%2520significantly%250Aimproving%2520robustness%2520and%2520generalizability.%2520Following%2520that%252C%2520a%2520normal-based%250Ageometry%2520refiner%2520is%2520used%2520to%2520significantly%2520enhance%2520the%2520surface%2520details.%2520This%250Arefinement%2520can%2520be%2520performed%2520automatically%252C%2520or%2520interactively%2520with%2520user-supplied%250Aedits.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520achieves%2520high%2520efficacy%250Ain%2520producing%2520superior-quality%25203D%2520assets%2520compared%2520to%2520existing%2520methods.%2520HomePage%253A%250Ahttps%253A//craftsman3d.github.io/%252C%2520Code%253A%2520https%253A//github.com/wyysf-98/CraftsMan%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14979v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CraftsMan3D%3A%20High-fidelity%20Mesh%20Generation%20with%203D%20Native%20Generation%20and%0A%20%20Interactive%20Geometry%20Refiner&entry.906535625=Weiyu%20Li%20and%20Jiarui%20Liu%20and%20Hongyu%20Yan%20and%20Rui%20Chen%20and%20Yixun%20Liang%20and%20Xuelin%20Chen%20and%20Ping%20Tan%20and%20Xiaoxiao%20Long&entry.1292438233=%20%20We%20present%20a%20novel%20generative%203D%20modeling%20system%2C%20coined%20CraftsMan%2C%20which%20can%0Agenerate%20high-fidelity%203D%20geometries%20with%20highly%20varied%20shapes%2C%20regular%20mesh%0Atopologies%2C%20and%20detailed%20surfaces%2C%20and%2C%20notably%2C%20allows%20for%20refining%20the%0Ageometry%20in%20an%20interactive%20manner.%20Despite%20the%20significant%20advancements%20in%203D%0Ageneration%2C%20existing%20methods%20still%20struggle%20with%20lengthy%20optimization%0Aprocesses%2C%20irregular%20mesh%20topologies%2C%20noisy%20surfaces%2C%20and%20difficulties%20in%0Aaccommodating%20user%20edits%2C%20consequently%20impeding%20their%20widespread%20adoption%20and%0Aimplementation%20in%203D%20modeling%20software.%20Our%20work%20is%20inspired%20by%20the%20craftsman%2C%0Awho%20usually%20roughs%20out%20the%20holistic%20figure%20of%20the%20work%20first%20and%20elaborates%20the%0Asurface%20details%20subsequently.%20Specifically%2C%20we%20employ%20a%203D%20native%20diffusion%0Amodel%2C%20which%20operates%20on%20latent%20space%20learned%20from%20latent%20set-based%203D%0Arepresentations%2C%20to%20generate%20coarse%20geometries%20with%20regular%20mesh%20topology%20in%0Aseconds.%20In%20particular%2C%20this%20process%20takes%20as%20input%20a%20text%20prompt%20or%20a%0Areference%20image%20and%20leverages%20a%20powerful%20multi-view%20%28MV%29%20diffusion%20model%20to%0Agenerate%20multiple%20views%20of%20the%20coarse%20geometry%2C%20which%20are%20fed%20into%20our%0AMV-conditioned%203D%20diffusion%20model%20for%20generating%20the%203D%20geometry%2C%20significantly%0Aimproving%20robustness%20and%20generalizability.%20Following%20that%2C%20a%20normal-based%0Ageometry%20refiner%20is%20used%20to%20significantly%20enhance%20the%20surface%20details.%20This%0Arefinement%20can%20be%20performed%20automatically%2C%20or%20interactively%20with%20user-supplied%0Aedits.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%20high%20efficacy%0Ain%20producing%20superior-quality%203D%20assets%20compared%20to%20existing%20methods.%20HomePage%3A%0Ahttps%3A//craftsman3d.github.io/%2C%20Code%3A%20https%3A//github.com/wyysf-98/CraftsMan%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14979v4&entry.124074799=Read"},
{"title": "TC-GS: A Faster Gaussian Splatting Module Utilizing Tensor Cores", "author": "Zimu Liao and Jifeng Ding and Rong Fu and Siwei Cui and Ruixuan Gong and Li Wang and Boni Hu and Yi Wang and Hengjie Li and XIngcheng Zhang and Hui Wang", "abstract": "  3D Gaussian Splatting (3DGS) renders pixels by rasterizing Gaussian\nprimitives, where conditional alpha-blending dominates the time cost in the\nrendering pipeline. This paper proposes TC-GS, an algorithm-independent\nuniversal module that expands Tensor Core (TCU) applicability for 3DGS, leading\nto substantial speedups and seamless integration into existing 3DGS\noptimization frameworks. The key innovation lies in mapping alpha computation\nto matrix multiplication, fully utilizing otherwise idle TCUs in existing 3DGS\nimplementations. TC-GS provides plug-and-play acceleration for existing\ntop-tier acceleration algorithms tightly coupled with rendering pipeline\ndesigns, like Gaussian compression and redundancy elimination algorithms.\nAdditionally, we introduce a global-to-local coordinate transformation to\nmitigate rounding errors from quadratic terms of pixel coordinates caused by\nTensor Core half-precision computation. Extensive experiments demonstrate that\nour method maintains rendering quality while providing an additional 2.18x\nspeedup over existing Gaussian acceleration algorithms, thus reaching up to a\ntotal 5.6x acceleration. The code is currently available at anonymous\n\\href{https://github.com/TensorCore3DGS/3DGSTensorCore}\n", "link": "http://arxiv.org/abs/2505.24796v1", "date": "2025-05-30", "relevancy": 3.0876, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6557}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6211}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TC-GS%3A%20A%20Faster%20Gaussian%20Splatting%20Module%20Utilizing%20Tensor%20Cores&body=Title%3A%20TC-GS%3A%20A%20Faster%20Gaussian%20Splatting%20Module%20Utilizing%20Tensor%20Cores%0AAuthor%3A%20Zimu%20Liao%20and%20Jifeng%20Ding%20and%20Rong%20Fu%20and%20Siwei%20Cui%20and%20Ruixuan%20Gong%20and%20Li%20Wang%20and%20Boni%20Hu%20and%20Yi%20Wang%20and%20Hengjie%20Li%20and%20XIngcheng%20Zhang%20and%20Hui%20Wang%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20renders%20pixels%20by%20rasterizing%20Gaussian%0Aprimitives%2C%20where%20conditional%20alpha-blending%20dominates%20the%20time%20cost%20in%20the%0Arendering%20pipeline.%20This%20paper%20proposes%20TC-GS%2C%20an%20algorithm-independent%0Auniversal%20module%20that%20expands%20Tensor%20Core%20%28TCU%29%20applicability%20for%203DGS%2C%20leading%0Ato%20substantial%20speedups%20and%20seamless%20integration%20into%20existing%203DGS%0Aoptimization%20frameworks.%20The%20key%20innovation%20lies%20in%20mapping%20alpha%20computation%0Ato%20matrix%20multiplication%2C%20fully%20utilizing%20otherwise%20idle%20TCUs%20in%20existing%203DGS%0Aimplementations.%20TC-GS%20provides%20plug-and-play%20acceleration%20for%20existing%0Atop-tier%20acceleration%20algorithms%20tightly%20coupled%20with%20rendering%20pipeline%0Adesigns%2C%20like%20Gaussian%20compression%20and%20redundancy%20elimination%20algorithms.%0AAdditionally%2C%20we%20introduce%20a%20global-to-local%20coordinate%20transformation%20to%0Amitigate%20rounding%20errors%20from%20quadratic%20terms%20of%20pixel%20coordinates%20caused%20by%0ATensor%20Core%20half-precision%20computation.%20Extensive%20experiments%20demonstrate%20that%0Aour%20method%20maintains%20rendering%20quality%20while%20providing%20an%20additional%202.18x%0Aspeedup%20over%20existing%20Gaussian%20acceleration%20algorithms%2C%20thus%20reaching%20up%20to%20a%0Atotal%205.6x%20acceleration.%20The%20code%20is%20currently%20available%20at%20anonymous%0A%5Chref%7Bhttps%3A//github.com/TensorCore3DGS/3DGSTensorCore%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24796v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTC-GS%253A%2520A%2520Faster%2520Gaussian%2520Splatting%2520Module%2520Utilizing%2520Tensor%2520Cores%26entry.906535625%3DZimu%2520Liao%2520and%2520Jifeng%2520Ding%2520and%2520Rong%2520Fu%2520and%2520Siwei%2520Cui%2520and%2520Ruixuan%2520Gong%2520and%2520Li%2520Wang%2520and%2520Boni%2520Hu%2520and%2520Yi%2520Wang%2520and%2520Hengjie%2520Li%2520and%2520XIngcheng%2520Zhang%2520and%2520Hui%2520Wang%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520renders%2520pixels%2520by%2520rasterizing%2520Gaussian%250Aprimitives%252C%2520where%2520conditional%2520alpha-blending%2520dominates%2520the%2520time%2520cost%2520in%2520the%250Arendering%2520pipeline.%2520This%2520paper%2520proposes%2520TC-GS%252C%2520an%2520algorithm-independent%250Auniversal%2520module%2520that%2520expands%2520Tensor%2520Core%2520%2528TCU%2529%2520applicability%2520for%25203DGS%252C%2520leading%250Ato%2520substantial%2520speedups%2520and%2520seamless%2520integration%2520into%2520existing%25203DGS%250Aoptimization%2520frameworks.%2520The%2520key%2520innovation%2520lies%2520in%2520mapping%2520alpha%2520computation%250Ato%2520matrix%2520multiplication%252C%2520fully%2520utilizing%2520otherwise%2520idle%2520TCUs%2520in%2520existing%25203DGS%250Aimplementations.%2520TC-GS%2520provides%2520plug-and-play%2520acceleration%2520for%2520existing%250Atop-tier%2520acceleration%2520algorithms%2520tightly%2520coupled%2520with%2520rendering%2520pipeline%250Adesigns%252C%2520like%2520Gaussian%2520compression%2520and%2520redundancy%2520elimination%2520algorithms.%250AAdditionally%252C%2520we%2520introduce%2520a%2520global-to-local%2520coordinate%2520transformation%2520to%250Amitigate%2520rounding%2520errors%2520from%2520quadratic%2520terms%2520of%2520pixel%2520coordinates%2520caused%2520by%250ATensor%2520Core%2520half-precision%2520computation.%2520Extensive%2520experiments%2520demonstrate%2520that%250Aour%2520method%2520maintains%2520rendering%2520quality%2520while%2520providing%2520an%2520additional%25202.18x%250Aspeedup%2520over%2520existing%2520Gaussian%2520acceleration%2520algorithms%252C%2520thus%2520reaching%2520up%2520to%2520a%250Atotal%25205.6x%2520acceleration.%2520The%2520code%2520is%2520currently%2520available%2520at%2520anonymous%250A%255Chref%257Bhttps%253A//github.com/TensorCore3DGS/3DGSTensorCore%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24796v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TC-GS%3A%20A%20Faster%20Gaussian%20Splatting%20Module%20Utilizing%20Tensor%20Cores&entry.906535625=Zimu%20Liao%20and%20Jifeng%20Ding%20and%20Rong%20Fu%20and%20Siwei%20Cui%20and%20Ruixuan%20Gong%20and%20Li%20Wang%20and%20Boni%20Hu%20and%20Yi%20Wang%20and%20Hengjie%20Li%20and%20XIngcheng%20Zhang%20and%20Hui%20Wang&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20renders%20pixels%20by%20rasterizing%20Gaussian%0Aprimitives%2C%20where%20conditional%20alpha-blending%20dominates%20the%20time%20cost%20in%20the%0Arendering%20pipeline.%20This%20paper%20proposes%20TC-GS%2C%20an%20algorithm-independent%0Auniversal%20module%20that%20expands%20Tensor%20Core%20%28TCU%29%20applicability%20for%203DGS%2C%20leading%0Ato%20substantial%20speedups%20and%20seamless%20integration%20into%20existing%203DGS%0Aoptimization%20frameworks.%20The%20key%20innovation%20lies%20in%20mapping%20alpha%20computation%0Ato%20matrix%20multiplication%2C%20fully%20utilizing%20otherwise%20idle%20TCUs%20in%20existing%203DGS%0Aimplementations.%20TC-GS%20provides%20plug-and-play%20acceleration%20for%20existing%0Atop-tier%20acceleration%20algorithms%20tightly%20coupled%20with%20rendering%20pipeline%0Adesigns%2C%20like%20Gaussian%20compression%20and%20redundancy%20elimination%20algorithms.%0AAdditionally%2C%20we%20introduce%20a%20global-to-local%20coordinate%20transformation%20to%0Amitigate%20rounding%20errors%20from%20quadratic%20terms%20of%20pixel%20coordinates%20caused%20by%0ATensor%20Core%20half-precision%20computation.%20Extensive%20experiments%20demonstrate%20that%0Aour%20method%20maintains%20rendering%20quality%20while%20providing%20an%20additional%202.18x%0Aspeedup%20over%20existing%20Gaussian%20acceleration%20algorithms%2C%20thus%20reaching%20up%20to%20a%0Atotal%205.6x%20acceleration.%20The%20code%20is%20currently%20available%20at%20anonymous%0A%5Chref%7Bhttps%3A//github.com/TensorCore3DGS/3DGSTensorCore%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24796v1&entry.124074799=Read"},
{"title": "Behind the Magic, MERLIM: Multi-modal Evaluation Benchmark for Large\n  Image-Language Models", "author": "Andr\u00e9s Villa and Juan Carlos Le\u00f3n Alc\u00e1zar and Alvaro Soto and Bernard Ghanem", "abstract": "  Large Vision and Language Models have enabled significant advances in fully\nsupervised and zero-shot visual tasks. These large architectures serve as the\nbaseline to what is currently known as Instruction Tuning Large Vision and\nLanguage models (IT-LVLMs). IT-LVLMs are general-purpose multi-modal assistants\nwhose responses are modulated by natural language instructions and visual data.\nDespite this versatility, IT-LVLM effectiveness in fundamental computer vision\nproblems remains unclear, primarily due to the absence of a standardized\nevaluation benchmark. This paper introduces a Multi-modal Evaluation Benchmark\nnamed MERLIM, a scalable test-bed to assess the capabilities of IT-LVLMs on\nfundamental computer vision tasks. MERLIM contains over 300K image-question\npairs and has a strong focus on detecting cross-modal \"hallucination\" events in\nIT-LVLMs. Our results bring important insights on the performance of\nstate-of-the-art IT-LVLMs including limitations at identifying fine-grained\nvisual concepts, object hallucinations across tasks, and biases towards the\nlanguage query. Our findings also suggest that these models have weak visual\ngrounding, but manage to make adequate guesses from global visual patterns or\nlanguage biases contained in the LLM component. We name this phenomenon of\ncorrect answers with no visual grounding as hidden hallucinations.\n", "link": "http://arxiv.org/abs/2312.02219v3", "date": "2025-05-30", "relevancy": 2.973, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5993}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5993}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Behind%20the%20Magic%2C%20MERLIM%3A%20Multi-modal%20Evaluation%20Benchmark%20for%20Large%0A%20%20Image-Language%20Models&body=Title%3A%20Behind%20the%20Magic%2C%20MERLIM%3A%20Multi-modal%20Evaluation%20Benchmark%20for%20Large%0A%20%20Image-Language%20Models%0AAuthor%3A%20Andr%C3%A9s%20Villa%20and%20Juan%20Carlos%20Le%C3%B3n%20Alc%C3%A1zar%20and%20Alvaro%20Soto%20and%20Bernard%20Ghanem%0AAbstract%3A%20%20%20Large%20Vision%20and%20Language%20Models%20have%20enabled%20significant%20advances%20in%20fully%0Asupervised%20and%20zero-shot%20visual%20tasks.%20These%20large%20architectures%20serve%20as%20the%0Abaseline%20to%20what%20is%20currently%20known%20as%20Instruction%20Tuning%20Large%20Vision%20and%0ALanguage%20models%20%28IT-LVLMs%29.%20IT-LVLMs%20are%20general-purpose%20multi-modal%20assistants%0Awhose%20responses%20are%20modulated%20by%20natural%20language%20instructions%20and%20visual%20data.%0ADespite%20this%20versatility%2C%20IT-LVLM%20effectiveness%20in%20fundamental%20computer%20vision%0Aproblems%20remains%20unclear%2C%20primarily%20due%20to%20the%20absence%20of%20a%20standardized%0Aevaluation%20benchmark.%20This%20paper%20introduces%20a%20Multi-modal%20Evaluation%20Benchmark%0Anamed%20MERLIM%2C%20a%20scalable%20test-bed%20to%20assess%20the%20capabilities%20of%20IT-LVLMs%20on%0Afundamental%20computer%20vision%20tasks.%20MERLIM%20contains%20over%20300K%20image-question%0Apairs%20and%20has%20a%20strong%20focus%20on%20detecting%20cross-modal%20%22hallucination%22%20events%20in%0AIT-LVLMs.%20Our%20results%20bring%20important%20insights%20on%20the%20performance%20of%0Astate-of-the-art%20IT-LVLMs%20including%20limitations%20at%20identifying%20fine-grained%0Avisual%20concepts%2C%20object%20hallucinations%20across%20tasks%2C%20and%20biases%20towards%20the%0Alanguage%20query.%20Our%20findings%20also%20suggest%20that%20these%20models%20have%20weak%20visual%0Agrounding%2C%20but%20manage%20to%20make%20adequate%20guesses%20from%20global%20visual%20patterns%20or%0Alanguage%20biases%20contained%20in%20the%20LLM%20component.%20We%20name%20this%20phenomenon%20of%0Acorrect%20answers%20with%20no%20visual%20grounding%20as%20hidden%20hallucinations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02219v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBehind%2520the%2520Magic%252C%2520MERLIM%253A%2520Multi-modal%2520Evaluation%2520Benchmark%2520for%2520Large%250A%2520%2520Image-Language%2520Models%26entry.906535625%3DAndr%25C3%25A9s%2520Villa%2520and%2520Juan%2520Carlos%2520Le%25C3%25B3n%2520Alc%25C3%25A1zar%2520and%2520Alvaro%2520Soto%2520and%2520Bernard%2520Ghanem%26entry.1292438233%3D%2520%2520Large%2520Vision%2520and%2520Language%2520Models%2520have%2520enabled%2520significant%2520advances%2520in%2520fully%250Asupervised%2520and%2520zero-shot%2520visual%2520tasks.%2520These%2520large%2520architectures%2520serve%2520as%2520the%250Abaseline%2520to%2520what%2520is%2520currently%2520known%2520as%2520Instruction%2520Tuning%2520Large%2520Vision%2520and%250ALanguage%2520models%2520%2528IT-LVLMs%2529.%2520IT-LVLMs%2520are%2520general-purpose%2520multi-modal%2520assistants%250Awhose%2520responses%2520are%2520modulated%2520by%2520natural%2520language%2520instructions%2520and%2520visual%2520data.%250ADespite%2520this%2520versatility%252C%2520IT-LVLM%2520effectiveness%2520in%2520fundamental%2520computer%2520vision%250Aproblems%2520remains%2520unclear%252C%2520primarily%2520due%2520to%2520the%2520absence%2520of%2520a%2520standardized%250Aevaluation%2520benchmark.%2520This%2520paper%2520introduces%2520a%2520Multi-modal%2520Evaluation%2520Benchmark%250Anamed%2520MERLIM%252C%2520a%2520scalable%2520test-bed%2520to%2520assess%2520the%2520capabilities%2520of%2520IT-LVLMs%2520on%250Afundamental%2520computer%2520vision%2520tasks.%2520MERLIM%2520contains%2520over%2520300K%2520image-question%250Apairs%2520and%2520has%2520a%2520strong%2520focus%2520on%2520detecting%2520cross-modal%2520%2522hallucination%2522%2520events%2520in%250AIT-LVLMs.%2520Our%2520results%2520bring%2520important%2520insights%2520on%2520the%2520performance%2520of%250Astate-of-the-art%2520IT-LVLMs%2520including%2520limitations%2520at%2520identifying%2520fine-grained%250Avisual%2520concepts%252C%2520object%2520hallucinations%2520across%2520tasks%252C%2520and%2520biases%2520towards%2520the%250Alanguage%2520query.%2520Our%2520findings%2520also%2520suggest%2520that%2520these%2520models%2520have%2520weak%2520visual%250Agrounding%252C%2520but%2520manage%2520to%2520make%2520adequate%2520guesses%2520from%2520global%2520visual%2520patterns%2520or%250Alanguage%2520biases%2520contained%2520in%2520the%2520LLM%2520component.%2520We%2520name%2520this%2520phenomenon%2520of%250Acorrect%2520answers%2520with%2520no%2520visual%2520grounding%2520as%2520hidden%2520hallucinations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.02219v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Behind%20the%20Magic%2C%20MERLIM%3A%20Multi-modal%20Evaluation%20Benchmark%20for%20Large%0A%20%20Image-Language%20Models&entry.906535625=Andr%C3%A9s%20Villa%20and%20Juan%20Carlos%20Le%C3%B3n%20Alc%C3%A1zar%20and%20Alvaro%20Soto%20and%20Bernard%20Ghanem&entry.1292438233=%20%20Large%20Vision%20and%20Language%20Models%20have%20enabled%20significant%20advances%20in%20fully%0Asupervised%20and%20zero-shot%20visual%20tasks.%20These%20large%20architectures%20serve%20as%20the%0Abaseline%20to%20what%20is%20currently%20known%20as%20Instruction%20Tuning%20Large%20Vision%20and%0ALanguage%20models%20%28IT-LVLMs%29.%20IT-LVLMs%20are%20general-purpose%20multi-modal%20assistants%0Awhose%20responses%20are%20modulated%20by%20natural%20language%20instructions%20and%20visual%20data.%0ADespite%20this%20versatility%2C%20IT-LVLM%20effectiveness%20in%20fundamental%20computer%20vision%0Aproblems%20remains%20unclear%2C%20primarily%20due%20to%20the%20absence%20of%20a%20standardized%0Aevaluation%20benchmark.%20This%20paper%20introduces%20a%20Multi-modal%20Evaluation%20Benchmark%0Anamed%20MERLIM%2C%20a%20scalable%20test-bed%20to%20assess%20the%20capabilities%20of%20IT-LVLMs%20on%0Afundamental%20computer%20vision%20tasks.%20MERLIM%20contains%20over%20300K%20image-question%0Apairs%20and%20has%20a%20strong%20focus%20on%20detecting%20cross-modal%20%22hallucination%22%20events%20in%0AIT-LVLMs.%20Our%20results%20bring%20important%20insights%20on%20the%20performance%20of%0Astate-of-the-art%20IT-LVLMs%20including%20limitations%20at%20identifying%20fine-grained%0Avisual%20concepts%2C%20object%20hallucinations%20across%20tasks%2C%20and%20biases%20towards%20the%0Alanguage%20query.%20Our%20findings%20also%20suggest%20that%20these%20models%20have%20weak%20visual%0Agrounding%2C%20but%20manage%20to%20make%20adequate%20guesses%20from%20global%20visual%20patterns%20or%0Alanguage%20biases%20contained%20in%20the%20LLM%20component.%20We%20name%20this%20phenomenon%20of%0Acorrect%20answers%20with%20no%20visual%20grounding%20as%20hidden%20hallucinations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02219v3&entry.124074799=Read"},
{"title": "Bridging 3D Anomaly Localization and Repair via High-Quality Continuous\n  Geometric Representation", "author": "Bozhong Zheng and Jinye Gan and Xiaohao Xu and Wenqiao Li and Xiaonan Huang and Na Ni and Yingna Wu", "abstract": "  3D point cloud anomaly detection is essential for robust vision systems but\nis challenged by pose variations and complex geometric anomalies. Existing\npatch-based methods often suffer from geometric fidelity issues due to discrete\nvoxelization or projection-based representations, limiting fine-grained anomaly\nlocalization. We introduce Pose-Aware Signed Distance Field (PASDF), a novel\nframework that integrates 3D anomaly detection and repair by learning a\ncontinuous, pose-invariant shape representation. PASDF leverages a Pose\nAlignment Module for canonicalization and a SDF Network to dynamically\nincorporate pose, enabling implicit learning of high-fidelity anomaly repair\ntemplates from the continuous SDF. This facilitates precise pixel-level anomaly\nlocalization through an Anomaly-Aware Scoring Module. Crucially, the continuous\n3D representation in PASDF extends beyond detection, facilitating in-situ\nanomaly repair. Experiments on Real3D-AD and Anomaly-ShapeNet demonstrate\nstate-of-the-art performance, achieving high object-level AUROC scores of 80.2%\nand 90.0%, respectively. These results highlight the effectiveness of\ncontinuous geometric representations in advancing 3D anomaly detection and\nfacilitating practical anomaly region repair. The code is available at\nhttps://github.com/ZZZBBBZZZ/PASDF to support further research.\n", "link": "http://arxiv.org/abs/2505.24431v1", "date": "2025-05-30", "relevancy": 2.9416, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6129}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5763}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%203D%20Anomaly%20Localization%20and%20Repair%20via%20High-Quality%20Continuous%0A%20%20Geometric%20Representation&body=Title%3A%20Bridging%203D%20Anomaly%20Localization%20and%20Repair%20via%20High-Quality%20Continuous%0A%20%20Geometric%20Representation%0AAuthor%3A%20Bozhong%20Zheng%20and%20Jinye%20Gan%20and%20Xiaohao%20Xu%20and%20Wenqiao%20Li%20and%20Xiaonan%20Huang%20and%20Na%20Ni%20and%20Yingna%20Wu%0AAbstract%3A%20%20%203D%20point%20cloud%20anomaly%20detection%20is%20essential%20for%20robust%20vision%20systems%20but%0Ais%20challenged%20by%20pose%20variations%20and%20complex%20geometric%20anomalies.%20Existing%0Apatch-based%20methods%20often%20suffer%20from%20geometric%20fidelity%20issues%20due%20to%20discrete%0Avoxelization%20or%20projection-based%20representations%2C%20limiting%20fine-grained%20anomaly%0Alocalization.%20We%20introduce%20Pose-Aware%20Signed%20Distance%20Field%20%28PASDF%29%2C%20a%20novel%0Aframework%20that%20integrates%203D%20anomaly%20detection%20and%20repair%20by%20learning%20a%0Acontinuous%2C%20pose-invariant%20shape%20representation.%20PASDF%20leverages%20a%20Pose%0AAlignment%20Module%20for%20canonicalization%20and%20a%20SDF%20Network%20to%20dynamically%0Aincorporate%20pose%2C%20enabling%20implicit%20learning%20of%20high-fidelity%20anomaly%20repair%0Atemplates%20from%20the%20continuous%20SDF.%20This%20facilitates%20precise%20pixel-level%20anomaly%0Alocalization%20through%20an%20Anomaly-Aware%20Scoring%20Module.%20Crucially%2C%20the%20continuous%0A3D%20representation%20in%20PASDF%20extends%20beyond%20detection%2C%20facilitating%20in-situ%0Aanomaly%20repair.%20Experiments%20on%20Real3D-AD%20and%20Anomaly-ShapeNet%20demonstrate%0Astate-of-the-art%20performance%2C%20achieving%20high%20object-level%20AUROC%20scores%20of%2080.2%25%0Aand%2090.0%25%2C%20respectively.%20These%20results%20highlight%20the%20effectiveness%20of%0Acontinuous%20geometric%20representations%20in%20advancing%203D%20anomaly%20detection%20and%0Afacilitating%20practical%20anomaly%20region%20repair.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/ZZZBBBZZZ/PASDF%20to%20support%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24431v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%25203D%2520Anomaly%2520Localization%2520and%2520Repair%2520via%2520High-Quality%2520Continuous%250A%2520%2520Geometric%2520Representation%26entry.906535625%3DBozhong%2520Zheng%2520and%2520Jinye%2520Gan%2520and%2520Xiaohao%2520Xu%2520and%2520Wenqiao%2520Li%2520and%2520Xiaonan%2520Huang%2520and%2520Na%2520Ni%2520and%2520Yingna%2520Wu%26entry.1292438233%3D%2520%25203D%2520point%2520cloud%2520anomaly%2520detection%2520is%2520essential%2520for%2520robust%2520vision%2520systems%2520but%250Ais%2520challenged%2520by%2520pose%2520variations%2520and%2520complex%2520geometric%2520anomalies.%2520Existing%250Apatch-based%2520methods%2520often%2520suffer%2520from%2520geometric%2520fidelity%2520issues%2520due%2520to%2520discrete%250Avoxelization%2520or%2520projection-based%2520representations%252C%2520limiting%2520fine-grained%2520anomaly%250Alocalization.%2520We%2520introduce%2520Pose-Aware%2520Signed%2520Distance%2520Field%2520%2528PASDF%2529%252C%2520a%2520novel%250Aframework%2520that%2520integrates%25203D%2520anomaly%2520detection%2520and%2520repair%2520by%2520learning%2520a%250Acontinuous%252C%2520pose-invariant%2520shape%2520representation.%2520PASDF%2520leverages%2520a%2520Pose%250AAlignment%2520Module%2520for%2520canonicalization%2520and%2520a%2520SDF%2520Network%2520to%2520dynamically%250Aincorporate%2520pose%252C%2520enabling%2520implicit%2520learning%2520of%2520high-fidelity%2520anomaly%2520repair%250Atemplates%2520from%2520the%2520continuous%2520SDF.%2520This%2520facilitates%2520precise%2520pixel-level%2520anomaly%250Alocalization%2520through%2520an%2520Anomaly-Aware%2520Scoring%2520Module.%2520Crucially%252C%2520the%2520continuous%250A3D%2520representation%2520in%2520PASDF%2520extends%2520beyond%2520detection%252C%2520facilitating%2520in-situ%250Aanomaly%2520repair.%2520Experiments%2520on%2520Real3D-AD%2520and%2520Anomaly-ShapeNet%2520demonstrate%250Astate-of-the-art%2520performance%252C%2520achieving%2520high%2520object-level%2520AUROC%2520scores%2520of%252080.2%2525%250Aand%252090.0%2525%252C%2520respectively.%2520These%2520results%2520highlight%2520the%2520effectiveness%2520of%250Acontinuous%2520geometric%2520representations%2520in%2520advancing%25203D%2520anomaly%2520detection%2520and%250Afacilitating%2520practical%2520anomaly%2520region%2520repair.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/ZZZBBBZZZ/PASDF%2520to%2520support%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24431v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%203D%20Anomaly%20Localization%20and%20Repair%20via%20High-Quality%20Continuous%0A%20%20Geometric%20Representation&entry.906535625=Bozhong%20Zheng%20and%20Jinye%20Gan%20and%20Xiaohao%20Xu%20and%20Wenqiao%20Li%20and%20Xiaonan%20Huang%20and%20Na%20Ni%20and%20Yingna%20Wu&entry.1292438233=%20%203D%20point%20cloud%20anomaly%20detection%20is%20essential%20for%20robust%20vision%20systems%20but%0Ais%20challenged%20by%20pose%20variations%20and%20complex%20geometric%20anomalies.%20Existing%0Apatch-based%20methods%20often%20suffer%20from%20geometric%20fidelity%20issues%20due%20to%20discrete%0Avoxelization%20or%20projection-based%20representations%2C%20limiting%20fine-grained%20anomaly%0Alocalization.%20We%20introduce%20Pose-Aware%20Signed%20Distance%20Field%20%28PASDF%29%2C%20a%20novel%0Aframework%20that%20integrates%203D%20anomaly%20detection%20and%20repair%20by%20learning%20a%0Acontinuous%2C%20pose-invariant%20shape%20representation.%20PASDF%20leverages%20a%20Pose%0AAlignment%20Module%20for%20canonicalization%20and%20a%20SDF%20Network%20to%20dynamically%0Aincorporate%20pose%2C%20enabling%20implicit%20learning%20of%20high-fidelity%20anomaly%20repair%0Atemplates%20from%20the%20continuous%20SDF.%20This%20facilitates%20precise%20pixel-level%20anomaly%0Alocalization%20through%20an%20Anomaly-Aware%20Scoring%20Module.%20Crucially%2C%20the%20continuous%0A3D%20representation%20in%20PASDF%20extends%20beyond%20detection%2C%20facilitating%20in-situ%0Aanomaly%20repair.%20Experiments%20on%20Real3D-AD%20and%20Anomaly-ShapeNet%20demonstrate%0Astate-of-the-art%20performance%2C%20achieving%20high%20object-level%20AUROC%20scores%20of%2080.2%25%0Aand%2090.0%25%2C%20respectively.%20These%20results%20highlight%20the%20effectiveness%20of%0Acontinuous%20geometric%20representations%20in%20advancing%203D%20anomaly%20detection%20and%0Afacilitating%20practical%20anomaly%20region%20repair.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/ZZZBBBZZZ/PASDF%20to%20support%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24431v1&entry.124074799=Read"},
{"title": "Video-MME: The First-Ever Comprehensive Evaluation Benchmark of\n  Multi-modal LLMs in Video Analysis", "author": "Chaoyou Fu and Yuhan Dai and Yongdong Luo and Lei Li and Shuhuai Ren and Renrui Zhang and Zihan Wang and Chenyu Zhou and Yunhang Shen and Mengdan Zhang and Peixian Chen and Yanwei Li and Shaohui Lin and Sirui Zhao and Ke Li and Tong Xu and Xiawu Zheng and Enhong Chen and Caifeng Shan and Ran He and Xing Sun", "abstract": "  In the quest for artificial general intelligence, Multi-modal Large Language\nModels (MLLMs) have emerged as a focal point in recent advancements. However,\nthe predominant focus remains on developing their capabilities in static image\nunderstanding. The potential of MLLMs in processing sequential visual data is\nstill insufficiently explored, highlighting the absence of a comprehensive,\nhigh-quality assessment of their performance. In this paper, we introduce\nVideo-MME, the first-ever full-spectrum, Multi-Modal Evaluation benchmark of\nMLLMs in Video analysis. Our work distinguishes from existing benchmarks\nthrough four key features: 1) Diversity in video types, spanning 6 primary\nvisual domains with 30 subfields to ensure broad scenario generalizability; 2)\nDuration in temporal dimension, encompassing both short-, medium-, and\nlong-term videos, ranging from 11 seconds to 1 hour, for robust contextual\ndynamics; 3) Breadth in data modalities, integrating multi-modal inputs besides\nvideo frames, including subtitles and audios, to unveil the all-round\ncapabilities of MLLMs; 4) Quality in annotations, utilizing rigorous manual\nlabeling by expert annotators to facilitate precise and reliable model\nassessment. 900 videos with a total of 254 hours are manually selected and\nannotated by repeatedly viewing all the video content, resulting in 2,700\nquestion-answer pairs. With Video-MME, we extensively evaluate various\nstate-of-the-art MLLMs, including GPT-4 series and Gemini 1.5 Pro, as well as\nopen-source image models like InternVL-Chat-V1.5 and video models like\nLLaVA-NeXT-Video. Our experiments reveal that Gemini 1.5 Pro is the\nbest-performing commercial model, significantly outperforming the open-source\nmodels. Our dataset along with these findings underscores the need for further\nimprovements in handling longer sequences and multi-modal data. Project Page:\nhttps://video-mme.github.io\n", "link": "http://arxiv.org/abs/2405.21075v3", "date": "2025-05-30", "relevancy": 2.928, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5913}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5913}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video-MME%3A%20The%20First-Ever%20Comprehensive%20Evaluation%20Benchmark%20of%0A%20%20Multi-modal%20LLMs%20in%20Video%20Analysis&body=Title%3A%20Video-MME%3A%20The%20First-Ever%20Comprehensive%20Evaluation%20Benchmark%20of%0A%20%20Multi-modal%20LLMs%20in%20Video%20Analysis%0AAuthor%3A%20Chaoyou%20Fu%20and%20Yuhan%20Dai%20and%20Yongdong%20Luo%20and%20Lei%20Li%20and%20Shuhuai%20Ren%20and%20Renrui%20Zhang%20and%20Zihan%20Wang%20and%20Chenyu%20Zhou%20and%20Yunhang%20Shen%20and%20Mengdan%20Zhang%20and%20Peixian%20Chen%20and%20Yanwei%20Li%20and%20Shaohui%20Lin%20and%20Sirui%20Zhao%20and%20Ke%20Li%20and%20Tong%20Xu%20and%20Xiawu%20Zheng%20and%20Enhong%20Chen%20and%20Caifeng%20Shan%20and%20Ran%20He%20and%20Xing%20Sun%0AAbstract%3A%20%20%20In%20the%20quest%20for%20artificial%20general%20intelligence%2C%20Multi-modal%20Large%20Language%0AModels%20%28MLLMs%29%20have%20emerged%20as%20a%20focal%20point%20in%20recent%20advancements.%20However%2C%0Athe%20predominant%20focus%20remains%20on%20developing%20their%20capabilities%20in%20static%20image%0Aunderstanding.%20The%20potential%20of%20MLLMs%20in%20processing%20sequential%20visual%20data%20is%0Astill%20insufficiently%20explored%2C%20highlighting%20the%20absence%20of%20a%20comprehensive%2C%0Ahigh-quality%20assessment%20of%20their%20performance.%20In%20this%20paper%2C%20we%20introduce%0AVideo-MME%2C%20the%20first-ever%20full-spectrum%2C%20Multi-Modal%20Evaluation%20benchmark%20of%0AMLLMs%20in%20Video%20analysis.%20Our%20work%20distinguishes%20from%20existing%20benchmarks%0Athrough%20four%20key%20features%3A%201%29%20Diversity%20in%20video%20types%2C%20spanning%206%20primary%0Avisual%20domains%20with%2030%20subfields%20to%20ensure%20broad%20scenario%20generalizability%3B%202%29%0ADuration%20in%20temporal%20dimension%2C%20encompassing%20both%20short-%2C%20medium-%2C%20and%0Along-term%20videos%2C%20ranging%20from%2011%20seconds%20to%201%20hour%2C%20for%20robust%20contextual%0Adynamics%3B%203%29%20Breadth%20in%20data%20modalities%2C%20integrating%20multi-modal%20inputs%20besides%0Avideo%20frames%2C%20including%20subtitles%20and%20audios%2C%20to%20unveil%20the%20all-round%0Acapabilities%20of%20MLLMs%3B%204%29%20Quality%20in%20annotations%2C%20utilizing%20rigorous%20manual%0Alabeling%20by%20expert%20annotators%20to%20facilitate%20precise%20and%20reliable%20model%0Aassessment.%20900%20videos%20with%20a%20total%20of%20254%20hours%20are%20manually%20selected%20and%0Aannotated%20by%20repeatedly%20viewing%20all%20the%20video%20content%2C%20resulting%20in%202%2C700%0Aquestion-answer%20pairs.%20With%20Video-MME%2C%20we%20extensively%20evaluate%20various%0Astate-of-the-art%20MLLMs%2C%20including%20GPT-4%20series%20and%20Gemini%201.5%20Pro%2C%20as%20well%20as%0Aopen-source%20image%20models%20like%20InternVL-Chat-V1.5%20and%20video%20models%20like%0ALLaVA-NeXT-Video.%20Our%20experiments%20reveal%20that%20Gemini%201.5%20Pro%20is%20the%0Abest-performing%20commercial%20model%2C%20significantly%20outperforming%20the%20open-source%0Amodels.%20Our%20dataset%20along%20with%20these%20findings%20underscores%20the%20need%20for%20further%0Aimprovements%20in%20handling%20longer%20sequences%20and%20multi-modal%20data.%20Project%20Page%3A%0Ahttps%3A//video-mme.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.21075v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo-MME%253A%2520The%2520First-Ever%2520Comprehensive%2520Evaluation%2520Benchmark%2520of%250A%2520%2520Multi-modal%2520LLMs%2520in%2520Video%2520Analysis%26entry.906535625%3DChaoyou%2520Fu%2520and%2520Yuhan%2520Dai%2520and%2520Yongdong%2520Luo%2520and%2520Lei%2520Li%2520and%2520Shuhuai%2520Ren%2520and%2520Renrui%2520Zhang%2520and%2520Zihan%2520Wang%2520and%2520Chenyu%2520Zhou%2520and%2520Yunhang%2520Shen%2520and%2520Mengdan%2520Zhang%2520and%2520Peixian%2520Chen%2520and%2520Yanwei%2520Li%2520and%2520Shaohui%2520Lin%2520and%2520Sirui%2520Zhao%2520and%2520Ke%2520Li%2520and%2520Tong%2520Xu%2520and%2520Xiawu%2520Zheng%2520and%2520Enhong%2520Chen%2520and%2520Caifeng%2520Shan%2520and%2520Ran%2520He%2520and%2520Xing%2520Sun%26entry.1292438233%3D%2520%2520In%2520the%2520quest%2520for%2520artificial%2520general%2520intelligence%252C%2520Multi-modal%2520Large%2520Language%250AModels%2520%2528MLLMs%2529%2520have%2520emerged%2520as%2520a%2520focal%2520point%2520in%2520recent%2520advancements.%2520However%252C%250Athe%2520predominant%2520focus%2520remains%2520on%2520developing%2520their%2520capabilities%2520in%2520static%2520image%250Aunderstanding.%2520The%2520potential%2520of%2520MLLMs%2520in%2520processing%2520sequential%2520visual%2520data%2520is%250Astill%2520insufficiently%2520explored%252C%2520highlighting%2520the%2520absence%2520of%2520a%2520comprehensive%252C%250Ahigh-quality%2520assessment%2520of%2520their%2520performance.%2520In%2520this%2520paper%252C%2520we%2520introduce%250AVideo-MME%252C%2520the%2520first-ever%2520full-spectrum%252C%2520Multi-Modal%2520Evaluation%2520benchmark%2520of%250AMLLMs%2520in%2520Video%2520analysis.%2520Our%2520work%2520distinguishes%2520from%2520existing%2520benchmarks%250Athrough%2520four%2520key%2520features%253A%25201%2529%2520Diversity%2520in%2520video%2520types%252C%2520spanning%25206%2520primary%250Avisual%2520domains%2520with%252030%2520subfields%2520to%2520ensure%2520broad%2520scenario%2520generalizability%253B%25202%2529%250ADuration%2520in%2520temporal%2520dimension%252C%2520encompassing%2520both%2520short-%252C%2520medium-%252C%2520and%250Along-term%2520videos%252C%2520ranging%2520from%252011%2520seconds%2520to%25201%2520hour%252C%2520for%2520robust%2520contextual%250Adynamics%253B%25203%2529%2520Breadth%2520in%2520data%2520modalities%252C%2520integrating%2520multi-modal%2520inputs%2520besides%250Avideo%2520frames%252C%2520including%2520subtitles%2520and%2520audios%252C%2520to%2520unveil%2520the%2520all-round%250Acapabilities%2520of%2520MLLMs%253B%25204%2529%2520Quality%2520in%2520annotations%252C%2520utilizing%2520rigorous%2520manual%250Alabeling%2520by%2520expert%2520annotators%2520to%2520facilitate%2520precise%2520and%2520reliable%2520model%250Aassessment.%2520900%2520videos%2520with%2520a%2520total%2520of%2520254%2520hours%2520are%2520manually%2520selected%2520and%250Aannotated%2520by%2520repeatedly%2520viewing%2520all%2520the%2520video%2520content%252C%2520resulting%2520in%25202%252C700%250Aquestion-answer%2520pairs.%2520With%2520Video-MME%252C%2520we%2520extensively%2520evaluate%2520various%250Astate-of-the-art%2520MLLMs%252C%2520including%2520GPT-4%2520series%2520and%2520Gemini%25201.5%2520Pro%252C%2520as%2520well%2520as%250Aopen-source%2520image%2520models%2520like%2520InternVL-Chat-V1.5%2520and%2520video%2520models%2520like%250ALLaVA-NeXT-Video.%2520Our%2520experiments%2520reveal%2520that%2520Gemini%25201.5%2520Pro%2520is%2520the%250Abest-performing%2520commercial%2520model%252C%2520significantly%2520outperforming%2520the%2520open-source%250Amodels.%2520Our%2520dataset%2520along%2520with%2520these%2520findings%2520underscores%2520the%2520need%2520for%2520further%250Aimprovements%2520in%2520handling%2520longer%2520sequences%2520and%2520multi-modal%2520data.%2520Project%2520Page%253A%250Ahttps%253A//video-mme.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.21075v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video-MME%3A%20The%20First-Ever%20Comprehensive%20Evaluation%20Benchmark%20of%0A%20%20Multi-modal%20LLMs%20in%20Video%20Analysis&entry.906535625=Chaoyou%20Fu%20and%20Yuhan%20Dai%20and%20Yongdong%20Luo%20and%20Lei%20Li%20and%20Shuhuai%20Ren%20and%20Renrui%20Zhang%20and%20Zihan%20Wang%20and%20Chenyu%20Zhou%20and%20Yunhang%20Shen%20and%20Mengdan%20Zhang%20and%20Peixian%20Chen%20and%20Yanwei%20Li%20and%20Shaohui%20Lin%20and%20Sirui%20Zhao%20and%20Ke%20Li%20and%20Tong%20Xu%20and%20Xiawu%20Zheng%20and%20Enhong%20Chen%20and%20Caifeng%20Shan%20and%20Ran%20He%20and%20Xing%20Sun&entry.1292438233=%20%20In%20the%20quest%20for%20artificial%20general%20intelligence%2C%20Multi-modal%20Large%20Language%0AModels%20%28MLLMs%29%20have%20emerged%20as%20a%20focal%20point%20in%20recent%20advancements.%20However%2C%0Athe%20predominant%20focus%20remains%20on%20developing%20their%20capabilities%20in%20static%20image%0Aunderstanding.%20The%20potential%20of%20MLLMs%20in%20processing%20sequential%20visual%20data%20is%0Astill%20insufficiently%20explored%2C%20highlighting%20the%20absence%20of%20a%20comprehensive%2C%0Ahigh-quality%20assessment%20of%20their%20performance.%20In%20this%20paper%2C%20we%20introduce%0AVideo-MME%2C%20the%20first-ever%20full-spectrum%2C%20Multi-Modal%20Evaluation%20benchmark%20of%0AMLLMs%20in%20Video%20analysis.%20Our%20work%20distinguishes%20from%20existing%20benchmarks%0Athrough%20four%20key%20features%3A%201%29%20Diversity%20in%20video%20types%2C%20spanning%206%20primary%0Avisual%20domains%20with%2030%20subfields%20to%20ensure%20broad%20scenario%20generalizability%3B%202%29%0ADuration%20in%20temporal%20dimension%2C%20encompassing%20both%20short-%2C%20medium-%2C%20and%0Along-term%20videos%2C%20ranging%20from%2011%20seconds%20to%201%20hour%2C%20for%20robust%20contextual%0Adynamics%3B%203%29%20Breadth%20in%20data%20modalities%2C%20integrating%20multi-modal%20inputs%20besides%0Avideo%20frames%2C%20including%20subtitles%20and%20audios%2C%20to%20unveil%20the%20all-round%0Acapabilities%20of%20MLLMs%3B%204%29%20Quality%20in%20annotations%2C%20utilizing%20rigorous%20manual%0Alabeling%20by%20expert%20annotators%20to%20facilitate%20precise%20and%20reliable%20model%0Aassessment.%20900%20videos%20with%20a%20total%20of%20254%20hours%20are%20manually%20selected%20and%0Aannotated%20by%20repeatedly%20viewing%20all%20the%20video%20content%2C%20resulting%20in%202%2C700%0Aquestion-answer%20pairs.%20With%20Video-MME%2C%20we%20extensively%20evaluate%20various%0Astate-of-the-art%20MLLMs%2C%20including%20GPT-4%20series%20and%20Gemini%201.5%20Pro%2C%20as%20well%20as%0Aopen-source%20image%20models%20like%20InternVL-Chat-V1.5%20and%20video%20models%20like%0ALLaVA-NeXT-Video.%20Our%20experiments%20reveal%20that%20Gemini%201.5%20Pro%20is%20the%0Abest-performing%20commercial%20model%2C%20significantly%20outperforming%20the%20open-source%0Amodels.%20Our%20dataset%20along%20with%20these%20findings%20underscores%20the%20need%20for%20further%0Aimprovements%20in%20handling%20longer%20sequences%20and%20multi-modal%20data.%20Project%20Page%3A%0Ahttps%3A//video-mme.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.21075v3&entry.124074799=Read"},
{"title": "Category-Level 6D Object Pose Estimation in Agricultural Settings Using\n  a Lattice-Deformation Framework and Diffusion-Augmented Synthetic Data", "author": "Marios Glytsos and Panagiotis P. Filntisis and George Retsinas and Petros Maragos", "abstract": "  Accurate 6D object pose estimation is essential for robotic grasping and\nmanipulation, particularly in agriculture, where fruits and vegetables exhibit\nhigh intra-class variability in shape, size, and texture. The vast majority of\nexisting methods rely on instance-specific CAD models or require depth sensors\nto resolve geometric ambiguities, making them impractical for real-world\nagricultural applications. In this work, we introduce PLANTPose, a novel\nframework for category-level 6D pose estimation that operates purely on RGB\ninput. PLANTPose predicts both the 6D pose and deformation parameters relative\nto a base mesh, allowing a single category-level CAD model to adapt to unseen\ninstances. This enables accurate pose estimation across varying shapes without\nrelying on instance-specific data. To enhance realism and improve\ngeneralization, we also leverage Stable Diffusion to refine synthetic training\nimages with realistic texturing, mimicking variations due to ripeness and\nenvironmental factors and bridging the domain gap between synthetic data and\nthe real world. Our evaluations on a challenging benchmark that includes\nbananas of various shapes, sizes, and ripeness status demonstrate the\neffectiveness of our framework in handling large intraclass variations while\nmaintaining accurate 6D pose predictions, significantly outperforming the\nstate-of-the-art RGB-based approach MegaPose.\n", "link": "http://arxiv.org/abs/2505.24636v1", "date": "2025-05-30", "relevancy": 2.8969, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6059}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5752}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Category-Level%206D%20Object%20Pose%20Estimation%20in%20Agricultural%20Settings%20Using%0A%20%20a%20Lattice-Deformation%20Framework%20and%20Diffusion-Augmented%20Synthetic%20Data&body=Title%3A%20Category-Level%206D%20Object%20Pose%20Estimation%20in%20Agricultural%20Settings%20Using%0A%20%20a%20Lattice-Deformation%20Framework%20and%20Diffusion-Augmented%20Synthetic%20Data%0AAuthor%3A%20Marios%20Glytsos%20and%20Panagiotis%20P.%20Filntisis%20and%20George%20Retsinas%20and%20Petros%20Maragos%0AAbstract%3A%20%20%20Accurate%206D%20object%20pose%20estimation%20is%20essential%20for%20robotic%20grasping%20and%0Amanipulation%2C%20particularly%20in%20agriculture%2C%20where%20fruits%20and%20vegetables%20exhibit%0Ahigh%20intra-class%20variability%20in%20shape%2C%20size%2C%20and%20texture.%20The%20vast%20majority%20of%0Aexisting%20methods%20rely%20on%20instance-specific%20CAD%20models%20or%20require%20depth%20sensors%0Ato%20resolve%20geometric%20ambiguities%2C%20making%20them%20impractical%20for%20real-world%0Aagricultural%20applications.%20In%20this%20work%2C%20we%20introduce%20PLANTPose%2C%20a%20novel%0Aframework%20for%20category-level%206D%20pose%20estimation%20that%20operates%20purely%20on%20RGB%0Ainput.%20PLANTPose%20predicts%20both%20the%206D%20pose%20and%20deformation%20parameters%20relative%0Ato%20a%20base%20mesh%2C%20allowing%20a%20single%20category-level%20CAD%20model%20to%20adapt%20to%20unseen%0Ainstances.%20This%20enables%20accurate%20pose%20estimation%20across%20varying%20shapes%20without%0Arelying%20on%20instance-specific%20data.%20To%20enhance%20realism%20and%20improve%0Ageneralization%2C%20we%20also%20leverage%20Stable%20Diffusion%20to%20refine%20synthetic%20training%0Aimages%20with%20realistic%20texturing%2C%20mimicking%20variations%20due%20to%20ripeness%20and%0Aenvironmental%20factors%20and%20bridging%20the%20domain%20gap%20between%20synthetic%20data%20and%0Athe%20real%20world.%20Our%20evaluations%20on%20a%20challenging%20benchmark%20that%20includes%0Abananas%20of%20various%20shapes%2C%20sizes%2C%20and%20ripeness%20status%20demonstrate%20the%0Aeffectiveness%20of%20our%20framework%20in%20handling%20large%20intraclass%20variations%20while%0Amaintaining%20accurate%206D%20pose%20predictions%2C%20significantly%20outperforming%20the%0Astate-of-the-art%20RGB-based%20approach%20MegaPose.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24636v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCategory-Level%25206D%2520Object%2520Pose%2520Estimation%2520in%2520Agricultural%2520Settings%2520Using%250A%2520%2520a%2520Lattice-Deformation%2520Framework%2520and%2520Diffusion-Augmented%2520Synthetic%2520Data%26entry.906535625%3DMarios%2520Glytsos%2520and%2520Panagiotis%2520P.%2520Filntisis%2520and%2520George%2520Retsinas%2520and%2520Petros%2520Maragos%26entry.1292438233%3D%2520%2520Accurate%25206D%2520object%2520pose%2520estimation%2520is%2520essential%2520for%2520robotic%2520grasping%2520and%250Amanipulation%252C%2520particularly%2520in%2520agriculture%252C%2520where%2520fruits%2520and%2520vegetables%2520exhibit%250Ahigh%2520intra-class%2520variability%2520in%2520shape%252C%2520size%252C%2520and%2520texture.%2520The%2520vast%2520majority%2520of%250Aexisting%2520methods%2520rely%2520on%2520instance-specific%2520CAD%2520models%2520or%2520require%2520depth%2520sensors%250Ato%2520resolve%2520geometric%2520ambiguities%252C%2520making%2520them%2520impractical%2520for%2520real-world%250Aagricultural%2520applications.%2520In%2520this%2520work%252C%2520we%2520introduce%2520PLANTPose%252C%2520a%2520novel%250Aframework%2520for%2520category-level%25206D%2520pose%2520estimation%2520that%2520operates%2520purely%2520on%2520RGB%250Ainput.%2520PLANTPose%2520predicts%2520both%2520the%25206D%2520pose%2520and%2520deformation%2520parameters%2520relative%250Ato%2520a%2520base%2520mesh%252C%2520allowing%2520a%2520single%2520category-level%2520CAD%2520model%2520to%2520adapt%2520to%2520unseen%250Ainstances.%2520This%2520enables%2520accurate%2520pose%2520estimation%2520across%2520varying%2520shapes%2520without%250Arelying%2520on%2520instance-specific%2520data.%2520To%2520enhance%2520realism%2520and%2520improve%250Ageneralization%252C%2520we%2520also%2520leverage%2520Stable%2520Diffusion%2520to%2520refine%2520synthetic%2520training%250Aimages%2520with%2520realistic%2520texturing%252C%2520mimicking%2520variations%2520due%2520to%2520ripeness%2520and%250Aenvironmental%2520factors%2520and%2520bridging%2520the%2520domain%2520gap%2520between%2520synthetic%2520data%2520and%250Athe%2520real%2520world.%2520Our%2520evaluations%2520on%2520a%2520challenging%2520benchmark%2520that%2520includes%250Abananas%2520of%2520various%2520shapes%252C%2520sizes%252C%2520and%2520ripeness%2520status%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520framework%2520in%2520handling%2520large%2520intraclass%2520variations%2520while%250Amaintaining%2520accurate%25206D%2520pose%2520predictions%252C%2520significantly%2520outperforming%2520the%250Astate-of-the-art%2520RGB-based%2520approach%2520MegaPose.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24636v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Category-Level%206D%20Object%20Pose%20Estimation%20in%20Agricultural%20Settings%20Using%0A%20%20a%20Lattice-Deformation%20Framework%20and%20Diffusion-Augmented%20Synthetic%20Data&entry.906535625=Marios%20Glytsos%20and%20Panagiotis%20P.%20Filntisis%20and%20George%20Retsinas%20and%20Petros%20Maragos&entry.1292438233=%20%20Accurate%206D%20object%20pose%20estimation%20is%20essential%20for%20robotic%20grasping%20and%0Amanipulation%2C%20particularly%20in%20agriculture%2C%20where%20fruits%20and%20vegetables%20exhibit%0Ahigh%20intra-class%20variability%20in%20shape%2C%20size%2C%20and%20texture.%20The%20vast%20majority%20of%0Aexisting%20methods%20rely%20on%20instance-specific%20CAD%20models%20or%20require%20depth%20sensors%0Ato%20resolve%20geometric%20ambiguities%2C%20making%20them%20impractical%20for%20real-world%0Aagricultural%20applications.%20In%20this%20work%2C%20we%20introduce%20PLANTPose%2C%20a%20novel%0Aframework%20for%20category-level%206D%20pose%20estimation%20that%20operates%20purely%20on%20RGB%0Ainput.%20PLANTPose%20predicts%20both%20the%206D%20pose%20and%20deformation%20parameters%20relative%0Ato%20a%20base%20mesh%2C%20allowing%20a%20single%20category-level%20CAD%20model%20to%20adapt%20to%20unseen%0Ainstances.%20This%20enables%20accurate%20pose%20estimation%20across%20varying%20shapes%20without%0Arelying%20on%20instance-specific%20data.%20To%20enhance%20realism%20and%20improve%0Ageneralization%2C%20we%20also%20leverage%20Stable%20Diffusion%20to%20refine%20synthetic%20training%0Aimages%20with%20realistic%20texturing%2C%20mimicking%20variations%20due%20to%20ripeness%20and%0Aenvironmental%20factors%20and%20bridging%20the%20domain%20gap%20between%20synthetic%20data%20and%0Athe%20real%20world.%20Our%20evaluations%20on%20a%20challenging%20benchmark%20that%20includes%0Abananas%20of%20various%20shapes%2C%20sizes%2C%20and%20ripeness%20status%20demonstrate%20the%0Aeffectiveness%20of%20our%20framework%20in%20handling%20large%20intraclass%20variations%20while%0Amaintaining%20accurate%206D%20pose%20predictions%2C%20significantly%20outperforming%20the%0Astate-of-the-art%20RGB-based%20approach%20MegaPose.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24636v1&entry.124074799=Read"},
{"title": "GARLIC: GAussian Representation LearnIng for spaCe partitioning", "author": "Panagiotis Rigas and Panagiotis Drivas and Charalambos Tzamos and Ioannis Chamodrakas and George Ioannakis and Leonidas J. Guibas and Ioannis Z. Emiris", "abstract": "  We introduce GARLIC (GAussian Representation LearnIng for spaCe\npartitioning), a novel indexing structure based on \\(N\\)-dimensional Gaussians\nfor efficiently learning high-dimensional vector spaces. Our approach is\ninspired from Gaussian splatting techniques, typically used in 3D rendering,\nwhich we adapt for high-dimensional search and classification. We optimize\nGaussian parameters using information-theoretic objectives that balance\ncoverage, assignment confidence, and structural and semantic consistency. A key\ncontribution is to progressively refine the representation through split and\nclone operations, handling hundreds of dimensions, thus handling varying data\ndensities. GARLIC offers the fast building times of traditional space\npartitioning methods (e.g., under \\(\\sim5\\) min build time for SIFT1M) while\nachieving \\(\\sim50\\%\\) Recall10@10 in low-candidate regimes. Experimental\nresults on standard benchmarks demonstrate our method's consistency in (a)\n\\(k\\)-NN retrieval, outperforming methods, such as Faiss-IVF, in fast-recall by\nusing about half their probes for the same Recall10@10 in Fashion-MNIST, and\n(b) in classification tasks, beating by \\(\\sim15\\%\\) accuracy other majority\nvoting methods. Further, we show strong generalization capabilities,\nmaintaining high accuracy even with downsampled training data: using just\n\\(1\\%\\) of the training data returns \\(\\sim 45\\%\\) Recall@1, thus making GARLIC\nquite powerful for applications requiring both speed and accuracy.\n", "link": "http://arxiv.org/abs/2505.24608v1", "date": "2025-05-30", "relevancy": 2.8943, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.599}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5762}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GARLIC%3A%20GAussian%20Representation%20LearnIng%20for%20spaCe%20partitioning&body=Title%3A%20GARLIC%3A%20GAussian%20Representation%20LearnIng%20for%20spaCe%20partitioning%0AAuthor%3A%20Panagiotis%20Rigas%20and%20Panagiotis%20Drivas%20and%20Charalambos%20Tzamos%20and%20Ioannis%20Chamodrakas%20and%20George%20Ioannakis%20and%20Leonidas%20J.%20Guibas%20and%20Ioannis%20Z.%20Emiris%0AAbstract%3A%20%20%20We%20introduce%20GARLIC%20%28GAussian%20Representation%20LearnIng%20for%20spaCe%0Apartitioning%29%2C%20a%20novel%20indexing%20structure%20based%20on%20%5C%28N%5C%29-dimensional%20Gaussians%0Afor%20efficiently%20learning%20high-dimensional%20vector%20spaces.%20Our%20approach%20is%0Ainspired%20from%20Gaussian%20splatting%20techniques%2C%20typically%20used%20in%203D%20rendering%2C%0Awhich%20we%20adapt%20for%20high-dimensional%20search%20and%20classification.%20We%20optimize%0AGaussian%20parameters%20using%20information-theoretic%20objectives%20that%20balance%0Acoverage%2C%20assignment%20confidence%2C%20and%20structural%20and%20semantic%20consistency.%20A%20key%0Acontribution%20is%20to%20progressively%20refine%20the%20representation%20through%20split%20and%0Aclone%20operations%2C%20handling%20hundreds%20of%20dimensions%2C%20thus%20handling%20varying%20data%0Adensities.%20GARLIC%20offers%20the%20fast%20building%20times%20of%20traditional%20space%0Apartitioning%20methods%20%28e.g.%2C%20under%20%5C%28%5Csim5%5C%29%20min%20build%20time%20for%20SIFT1M%29%20while%0Aachieving%20%5C%28%5Csim50%5C%25%5C%29%20Recall10%4010%20in%20low-candidate%20regimes.%20Experimental%0Aresults%20on%20standard%20benchmarks%20demonstrate%20our%20method%27s%20consistency%20in%20%28a%29%0A%5C%28k%5C%29-NN%20retrieval%2C%20outperforming%20methods%2C%20such%20as%20Faiss-IVF%2C%20in%20fast-recall%20by%0Ausing%20about%20half%20their%20probes%20for%20the%20same%20Recall10%4010%20in%20Fashion-MNIST%2C%20and%0A%28b%29%20in%20classification%20tasks%2C%20beating%20by%20%5C%28%5Csim15%5C%25%5C%29%20accuracy%20other%20majority%0Avoting%20methods.%20Further%2C%20we%20show%20strong%20generalization%20capabilities%2C%0Amaintaining%20high%20accuracy%20even%20with%20downsampled%20training%20data%3A%20using%20just%0A%5C%281%5C%25%5C%29%20of%20the%20training%20data%20returns%20%5C%28%5Csim%2045%5C%25%5C%29%20Recall%401%2C%20thus%20making%20GARLIC%0Aquite%20powerful%20for%20applications%20requiring%20both%20speed%20and%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24608v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGARLIC%253A%2520GAussian%2520Representation%2520LearnIng%2520for%2520spaCe%2520partitioning%26entry.906535625%3DPanagiotis%2520Rigas%2520and%2520Panagiotis%2520Drivas%2520and%2520Charalambos%2520Tzamos%2520and%2520Ioannis%2520Chamodrakas%2520and%2520George%2520Ioannakis%2520and%2520Leonidas%2520J.%2520Guibas%2520and%2520Ioannis%2520Z.%2520Emiris%26entry.1292438233%3D%2520%2520We%2520introduce%2520GARLIC%2520%2528GAussian%2520Representation%2520LearnIng%2520for%2520spaCe%250Apartitioning%2529%252C%2520a%2520novel%2520indexing%2520structure%2520based%2520on%2520%255C%2528N%255C%2529-dimensional%2520Gaussians%250Afor%2520efficiently%2520learning%2520high-dimensional%2520vector%2520spaces.%2520Our%2520approach%2520is%250Ainspired%2520from%2520Gaussian%2520splatting%2520techniques%252C%2520typically%2520used%2520in%25203D%2520rendering%252C%250Awhich%2520we%2520adapt%2520for%2520high-dimensional%2520search%2520and%2520classification.%2520We%2520optimize%250AGaussian%2520parameters%2520using%2520information-theoretic%2520objectives%2520that%2520balance%250Acoverage%252C%2520assignment%2520confidence%252C%2520and%2520structural%2520and%2520semantic%2520consistency.%2520A%2520key%250Acontribution%2520is%2520to%2520progressively%2520refine%2520the%2520representation%2520through%2520split%2520and%250Aclone%2520operations%252C%2520handling%2520hundreds%2520of%2520dimensions%252C%2520thus%2520handling%2520varying%2520data%250Adensities.%2520GARLIC%2520offers%2520the%2520fast%2520building%2520times%2520of%2520traditional%2520space%250Apartitioning%2520methods%2520%2528e.g.%252C%2520under%2520%255C%2528%255Csim5%255C%2529%2520min%2520build%2520time%2520for%2520SIFT1M%2529%2520while%250Aachieving%2520%255C%2528%255Csim50%255C%2525%255C%2529%2520Recall10%254010%2520in%2520low-candidate%2520regimes.%2520Experimental%250Aresults%2520on%2520standard%2520benchmarks%2520demonstrate%2520our%2520method%2527s%2520consistency%2520in%2520%2528a%2529%250A%255C%2528k%255C%2529-NN%2520retrieval%252C%2520outperforming%2520methods%252C%2520such%2520as%2520Faiss-IVF%252C%2520in%2520fast-recall%2520by%250Ausing%2520about%2520half%2520their%2520probes%2520for%2520the%2520same%2520Recall10%254010%2520in%2520Fashion-MNIST%252C%2520and%250A%2528b%2529%2520in%2520classification%2520tasks%252C%2520beating%2520by%2520%255C%2528%255Csim15%255C%2525%255C%2529%2520accuracy%2520other%2520majority%250Avoting%2520methods.%2520Further%252C%2520we%2520show%2520strong%2520generalization%2520capabilities%252C%250Amaintaining%2520high%2520accuracy%2520even%2520with%2520downsampled%2520training%2520data%253A%2520using%2520just%250A%255C%25281%255C%2525%255C%2529%2520of%2520the%2520training%2520data%2520returns%2520%255C%2528%255Csim%252045%255C%2525%255C%2529%2520Recall%25401%252C%2520thus%2520making%2520GARLIC%250Aquite%2520powerful%2520for%2520applications%2520requiring%2520both%2520speed%2520and%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24608v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GARLIC%3A%20GAussian%20Representation%20LearnIng%20for%20spaCe%20partitioning&entry.906535625=Panagiotis%20Rigas%20and%20Panagiotis%20Drivas%20and%20Charalambos%20Tzamos%20and%20Ioannis%20Chamodrakas%20and%20George%20Ioannakis%20and%20Leonidas%20J.%20Guibas%20and%20Ioannis%20Z.%20Emiris&entry.1292438233=%20%20We%20introduce%20GARLIC%20%28GAussian%20Representation%20LearnIng%20for%20spaCe%0Apartitioning%29%2C%20a%20novel%20indexing%20structure%20based%20on%20%5C%28N%5C%29-dimensional%20Gaussians%0Afor%20efficiently%20learning%20high-dimensional%20vector%20spaces.%20Our%20approach%20is%0Ainspired%20from%20Gaussian%20splatting%20techniques%2C%20typically%20used%20in%203D%20rendering%2C%0Awhich%20we%20adapt%20for%20high-dimensional%20search%20and%20classification.%20We%20optimize%0AGaussian%20parameters%20using%20information-theoretic%20objectives%20that%20balance%0Acoverage%2C%20assignment%20confidence%2C%20and%20structural%20and%20semantic%20consistency.%20A%20key%0Acontribution%20is%20to%20progressively%20refine%20the%20representation%20through%20split%20and%0Aclone%20operations%2C%20handling%20hundreds%20of%20dimensions%2C%20thus%20handling%20varying%20data%0Adensities.%20GARLIC%20offers%20the%20fast%20building%20times%20of%20traditional%20space%0Apartitioning%20methods%20%28e.g.%2C%20under%20%5C%28%5Csim5%5C%29%20min%20build%20time%20for%20SIFT1M%29%20while%0Aachieving%20%5C%28%5Csim50%5C%25%5C%29%20Recall10%4010%20in%20low-candidate%20regimes.%20Experimental%0Aresults%20on%20standard%20benchmarks%20demonstrate%20our%20method%27s%20consistency%20in%20%28a%29%0A%5C%28k%5C%29-NN%20retrieval%2C%20outperforming%20methods%2C%20such%20as%20Faiss-IVF%2C%20in%20fast-recall%20by%0Ausing%20about%20half%20their%20probes%20for%20the%20same%20Recall10%4010%20in%20Fashion-MNIST%2C%20and%0A%28b%29%20in%20classification%20tasks%2C%20beating%20by%20%5C%28%5Csim15%5C%25%5C%29%20accuracy%20other%20majority%0Avoting%20methods.%20Further%2C%20we%20show%20strong%20generalization%20capabilities%2C%0Amaintaining%20high%20accuracy%20even%20with%20downsampled%20training%20data%3A%20using%20just%0A%5C%281%5C%25%5C%29%20of%20the%20training%20data%20returns%20%5C%28%5Csim%2045%5C%25%5C%29%20Recall%401%2C%20thus%20making%20GARLIC%0Aquite%20powerful%20for%20applications%20requiring%20both%20speed%20and%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24608v1&entry.124074799=Read"},
{"title": "NUC-Net: Non-uniform Cylindrical Partition Network for Efficient LiDAR\n  Semantic Segmentation", "author": "Xuzhi Wang and Wei Feng and Lingdong Kong and Liang Wan", "abstract": "  LiDAR semantic segmentation plays a vital role in autonomous driving.\nExisting voxel-based methods for LiDAR semantic segmentation apply uniform\npartition to the 3D LiDAR point cloud to form a structured representation based\non cartesian/cylindrical coordinates. Although these methods show impressive\nperformance, the drawback of existing voxel-based methods remains in two\naspects: (1) it requires a large enough input voxel resolution, which brings a\nlarge amount of computation cost and memory consumption. (2) it does not well\nhandle the unbalanced point distribution of LiDAR point cloud. In this paper,\nwe propose a non-uniform cylindrical partition network named NUC-Net to tackle\nthe above challenges. Specifically, we propose the Arithmetic Progression of\nInterval (API) method to non-uniformly partition the radial axis and generate\nthe voxel representation which is representative and efficient. Moreover, we\npropose a non-uniform multi-scale aggregation method to improve contextual\ninformation. Our method achieves state-of-the-art performance on SemanticKITTI\nand nuScenes datasets with much faster speed and much less training time. And\nour method can be a general component for LiDAR semantic segmentation, which\nsignificantly improves both the accuracy and efficiency of the uniform\ncounterpart by $4 \\times$ training faster and $2 \\times$ GPU memory reduction\nand $3 \\times$ inference speedup. We further provide theoretical analysis\ntowards understanding why NUC is effective and how point distribution affects\nperformance. Code is available at\n\\href{https://github.com/alanWXZ/NUC-Net}{https://github.com/alanWXZ/NUC-Net}.\n", "link": "http://arxiv.org/abs/2505.24634v1", "date": "2025-05-30", "relevancy": 2.8807, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6111}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5669}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NUC-Net%3A%20Non-uniform%20Cylindrical%20Partition%20Network%20for%20Efficient%20LiDAR%0A%20%20Semantic%20Segmentation&body=Title%3A%20NUC-Net%3A%20Non-uniform%20Cylindrical%20Partition%20Network%20for%20Efficient%20LiDAR%0A%20%20Semantic%20Segmentation%0AAuthor%3A%20Xuzhi%20Wang%20and%20Wei%20Feng%20and%20Lingdong%20Kong%20and%20Liang%20Wan%0AAbstract%3A%20%20%20LiDAR%20semantic%20segmentation%20plays%20a%20vital%20role%20in%20autonomous%20driving.%0AExisting%20voxel-based%20methods%20for%20LiDAR%20semantic%20segmentation%20apply%20uniform%0Apartition%20to%20the%203D%20LiDAR%20point%20cloud%20to%20form%20a%20structured%20representation%20based%0Aon%20cartesian/cylindrical%20coordinates.%20Although%20these%20methods%20show%20impressive%0Aperformance%2C%20the%20drawback%20of%20existing%20voxel-based%20methods%20remains%20in%20two%0Aaspects%3A%20%281%29%20it%20requires%20a%20large%20enough%20input%20voxel%20resolution%2C%20which%20brings%20a%0Alarge%20amount%20of%20computation%20cost%20and%20memory%20consumption.%20%282%29%20it%20does%20not%20well%0Ahandle%20the%20unbalanced%20point%20distribution%20of%20LiDAR%20point%20cloud.%20In%20this%20paper%2C%0Awe%20propose%20a%20non-uniform%20cylindrical%20partition%20network%20named%20NUC-Net%20to%20tackle%0Athe%20above%20challenges.%20Specifically%2C%20we%20propose%20the%20Arithmetic%20Progression%20of%0AInterval%20%28API%29%20method%20to%20non-uniformly%20partition%20the%20radial%20axis%20and%20generate%0Athe%20voxel%20representation%20which%20is%20representative%20and%20efficient.%20Moreover%2C%20we%0Apropose%20a%20non-uniform%20multi-scale%20aggregation%20method%20to%20improve%20contextual%0Ainformation.%20Our%20method%20achieves%20state-of-the-art%20performance%20on%20SemanticKITTI%0Aand%20nuScenes%20datasets%20with%20much%20faster%20speed%20and%20much%20less%20training%20time.%20And%0Aour%20method%20can%20be%20a%20general%20component%20for%20LiDAR%20semantic%20segmentation%2C%20which%0Asignificantly%20improves%20both%20the%20accuracy%20and%20efficiency%20of%20the%20uniform%0Acounterpart%20by%20%244%20%5Ctimes%24%20training%20faster%20and%20%242%20%5Ctimes%24%20GPU%20memory%20reduction%0Aand%20%243%20%5Ctimes%24%20inference%20speedup.%20We%20further%20provide%20theoretical%20analysis%0Atowards%20understanding%20why%20NUC%20is%20effective%20and%20how%20point%20distribution%20affects%0Aperformance.%20Code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/alanWXZ/NUC-Net%7D%7Bhttps%3A//github.com/alanWXZ/NUC-Net%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24634v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNUC-Net%253A%2520Non-uniform%2520Cylindrical%2520Partition%2520Network%2520for%2520Efficient%2520LiDAR%250A%2520%2520Semantic%2520Segmentation%26entry.906535625%3DXuzhi%2520Wang%2520and%2520Wei%2520Feng%2520and%2520Lingdong%2520Kong%2520and%2520Liang%2520Wan%26entry.1292438233%3D%2520%2520LiDAR%2520semantic%2520segmentation%2520plays%2520a%2520vital%2520role%2520in%2520autonomous%2520driving.%250AExisting%2520voxel-based%2520methods%2520for%2520LiDAR%2520semantic%2520segmentation%2520apply%2520uniform%250Apartition%2520to%2520the%25203D%2520LiDAR%2520point%2520cloud%2520to%2520form%2520a%2520structured%2520representation%2520based%250Aon%2520cartesian/cylindrical%2520coordinates.%2520Although%2520these%2520methods%2520show%2520impressive%250Aperformance%252C%2520the%2520drawback%2520of%2520existing%2520voxel-based%2520methods%2520remains%2520in%2520two%250Aaspects%253A%2520%25281%2529%2520it%2520requires%2520a%2520large%2520enough%2520input%2520voxel%2520resolution%252C%2520which%2520brings%2520a%250Alarge%2520amount%2520of%2520computation%2520cost%2520and%2520memory%2520consumption.%2520%25282%2529%2520it%2520does%2520not%2520well%250Ahandle%2520the%2520unbalanced%2520point%2520distribution%2520of%2520LiDAR%2520point%2520cloud.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520a%2520non-uniform%2520cylindrical%2520partition%2520network%2520named%2520NUC-Net%2520to%2520tackle%250Athe%2520above%2520challenges.%2520Specifically%252C%2520we%2520propose%2520the%2520Arithmetic%2520Progression%2520of%250AInterval%2520%2528API%2529%2520method%2520to%2520non-uniformly%2520partition%2520the%2520radial%2520axis%2520and%2520generate%250Athe%2520voxel%2520representation%2520which%2520is%2520representative%2520and%2520efficient.%2520Moreover%252C%2520we%250Apropose%2520a%2520non-uniform%2520multi-scale%2520aggregation%2520method%2520to%2520improve%2520contextual%250Ainformation.%2520Our%2520method%2520achieves%2520state-of-the-art%2520performance%2520on%2520SemanticKITTI%250Aand%2520nuScenes%2520datasets%2520with%2520much%2520faster%2520speed%2520and%2520much%2520less%2520training%2520time.%2520And%250Aour%2520method%2520can%2520be%2520a%2520general%2520component%2520for%2520LiDAR%2520semantic%2520segmentation%252C%2520which%250Asignificantly%2520improves%2520both%2520the%2520accuracy%2520and%2520efficiency%2520of%2520the%2520uniform%250Acounterpart%2520by%2520%25244%2520%255Ctimes%2524%2520training%2520faster%2520and%2520%25242%2520%255Ctimes%2524%2520GPU%2520memory%2520reduction%250Aand%2520%25243%2520%255Ctimes%2524%2520inference%2520speedup.%2520We%2520further%2520provide%2520theoretical%2520analysis%250Atowards%2520understanding%2520why%2520NUC%2520is%2520effective%2520and%2520how%2520point%2520distribution%2520affects%250Aperformance.%2520Code%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/alanWXZ/NUC-Net%257D%257Bhttps%253A//github.com/alanWXZ/NUC-Net%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24634v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NUC-Net%3A%20Non-uniform%20Cylindrical%20Partition%20Network%20for%20Efficient%20LiDAR%0A%20%20Semantic%20Segmentation&entry.906535625=Xuzhi%20Wang%20and%20Wei%20Feng%20and%20Lingdong%20Kong%20and%20Liang%20Wan&entry.1292438233=%20%20LiDAR%20semantic%20segmentation%20plays%20a%20vital%20role%20in%20autonomous%20driving.%0AExisting%20voxel-based%20methods%20for%20LiDAR%20semantic%20segmentation%20apply%20uniform%0Apartition%20to%20the%203D%20LiDAR%20point%20cloud%20to%20form%20a%20structured%20representation%20based%0Aon%20cartesian/cylindrical%20coordinates.%20Although%20these%20methods%20show%20impressive%0Aperformance%2C%20the%20drawback%20of%20existing%20voxel-based%20methods%20remains%20in%20two%0Aaspects%3A%20%281%29%20it%20requires%20a%20large%20enough%20input%20voxel%20resolution%2C%20which%20brings%20a%0Alarge%20amount%20of%20computation%20cost%20and%20memory%20consumption.%20%282%29%20it%20does%20not%20well%0Ahandle%20the%20unbalanced%20point%20distribution%20of%20LiDAR%20point%20cloud.%20In%20this%20paper%2C%0Awe%20propose%20a%20non-uniform%20cylindrical%20partition%20network%20named%20NUC-Net%20to%20tackle%0Athe%20above%20challenges.%20Specifically%2C%20we%20propose%20the%20Arithmetic%20Progression%20of%0AInterval%20%28API%29%20method%20to%20non-uniformly%20partition%20the%20radial%20axis%20and%20generate%0Athe%20voxel%20representation%20which%20is%20representative%20and%20efficient.%20Moreover%2C%20we%0Apropose%20a%20non-uniform%20multi-scale%20aggregation%20method%20to%20improve%20contextual%0Ainformation.%20Our%20method%20achieves%20state-of-the-art%20performance%20on%20SemanticKITTI%0Aand%20nuScenes%20datasets%20with%20much%20faster%20speed%20and%20much%20less%20training%20time.%20And%0Aour%20method%20can%20be%20a%20general%20component%20for%20LiDAR%20semantic%20segmentation%2C%20which%0Asignificantly%20improves%20both%20the%20accuracy%20and%20efficiency%20of%20the%20uniform%0Acounterpart%20by%20%244%20%5Ctimes%24%20training%20faster%20and%20%242%20%5Ctimes%24%20GPU%20memory%20reduction%0Aand%20%243%20%5Ctimes%24%20inference%20speedup.%20We%20further%20provide%20theoretical%20analysis%0Atowards%20understanding%20why%20NUC%20is%20effective%20and%20how%20point%20distribution%20affects%0Aperformance.%20Code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/alanWXZ/NUC-Net%7D%7Bhttps%3A//github.com/alanWXZ/NUC-Net%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24634v1&entry.124074799=Read"},
{"title": "VideoGameBench: Can Vision-Language Models complete popular video games?", "author": "Alex L. Zhang and Thomas L. Griffiths and Karthik R. Narasimhan and Ofir Press", "abstract": "  Vision-language models (VLMs) have achieved strong results on coding and math\nbenchmarks that are challenging for humans, yet their ability to perform tasks\nthat come naturally to humans--such as perception, spatial navigation, and\nmemory management--remains understudied. Real video games are crafted to be\nintuitive for humans to learn and master by leveraging innate inductive biases,\nmaking them an ideal testbed for evaluating such capabilities in VLMs. To this\nend, we introduce VideoGameBench, a benchmark consisting of 10 popular video\ngames from the 1990s that VLMs directly interact with in real-time.\nVideoGameBench challenges models to complete entire games with access to only\nraw visual inputs and a high-level description of objectives and controls, a\nsignificant departure from existing setups that rely on game-specific\nscaffolding and auxiliary information. We keep three of the games secret to\nencourage solutions that generalize to unseen environments. Our experiments\nshow that frontier vision-language models struggle to progress beyond the\nbeginning of each game. We find inference latency to be a major limitation of\nfrontier models in the real-time setting; therefore, we introduce\nVideoGameBench Lite, a setting where the game pauses while waiting for the LM's\nnext action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of\nVideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization\nof the human skills mentioned above into this benchmark motivates progress in\nthese research directions.\n", "link": "http://arxiv.org/abs/2505.18134v2", "date": "2025-05-30", "relevancy": 2.8712, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6089}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6089}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoGameBench%3A%20Can%20Vision-Language%20Models%20complete%20popular%20video%20games%3F&body=Title%3A%20VideoGameBench%3A%20Can%20Vision-Language%20Models%20complete%20popular%20video%20games%3F%0AAuthor%3A%20Alex%20L.%20Zhang%20and%20Thomas%20L.%20Griffiths%20and%20Karthik%20R.%20Narasimhan%20and%20Ofir%20Press%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20have%20achieved%20strong%20results%20on%20coding%20and%20math%0Abenchmarks%20that%20are%20challenging%20for%20humans%2C%20yet%20their%20ability%20to%20perform%20tasks%0Athat%20come%20naturally%20to%20humans--such%20as%20perception%2C%20spatial%20navigation%2C%20and%0Amemory%20management--remains%20understudied.%20Real%20video%20games%20are%20crafted%20to%20be%0Aintuitive%20for%20humans%20to%20learn%20and%20master%20by%20leveraging%20innate%20inductive%20biases%2C%0Amaking%20them%20an%20ideal%20testbed%20for%20evaluating%20such%20capabilities%20in%20VLMs.%20To%20this%0Aend%2C%20we%20introduce%20VideoGameBench%2C%20a%20benchmark%20consisting%20of%2010%20popular%20video%0Agames%20from%20the%201990s%20that%20VLMs%20directly%20interact%20with%20in%20real-time.%0AVideoGameBench%20challenges%20models%20to%20complete%20entire%20games%20with%20access%20to%20only%0Araw%20visual%20inputs%20and%20a%20high-level%20description%20of%20objectives%20and%20controls%2C%20a%0Asignificant%20departure%20from%20existing%20setups%20that%20rely%20on%20game-specific%0Ascaffolding%20and%20auxiliary%20information.%20We%20keep%20three%20of%20the%20games%20secret%20to%0Aencourage%20solutions%20that%20generalize%20to%20unseen%20environments.%20Our%20experiments%0Ashow%20that%20frontier%20vision-language%20models%20struggle%20to%20progress%20beyond%20the%0Abeginning%20of%20each%20game.%20We%20find%20inference%20latency%20to%20be%20a%20major%20limitation%20of%0Afrontier%20models%20in%20the%20real-time%20setting%3B%20therefore%2C%20we%20introduce%0AVideoGameBench%20Lite%2C%20a%20setting%20where%20the%20game%20pauses%20while%20waiting%20for%20the%20LM%27s%0Anext%20action.%20The%20best%20performing%20model%2C%20Gemini%202.5%20Pro%2C%20completes%20only%200.48%25%20of%0AVideoGameBench%20and%201.6%25%20of%20VideoGameBench%20Lite.%20We%20hope%20that%20the%20formalization%0Aof%20the%20human%20skills%20mentioned%20above%20into%20this%20benchmark%20motivates%20progress%20in%0Athese%20research%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18134v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoGameBench%253A%2520Can%2520Vision-Language%2520Models%2520complete%2520popular%2520video%2520games%253F%26entry.906535625%3DAlex%2520L.%2520Zhang%2520and%2520Thomas%2520L.%2520Griffiths%2520and%2520Karthik%2520R.%2520Narasimhan%2520and%2520Ofir%2520Press%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520have%2520achieved%2520strong%2520results%2520on%2520coding%2520and%2520math%250Abenchmarks%2520that%2520are%2520challenging%2520for%2520humans%252C%2520yet%2520their%2520ability%2520to%2520perform%2520tasks%250Athat%2520come%2520naturally%2520to%2520humans--such%2520as%2520perception%252C%2520spatial%2520navigation%252C%2520and%250Amemory%2520management--remains%2520understudied.%2520Real%2520video%2520games%2520are%2520crafted%2520to%2520be%250Aintuitive%2520for%2520humans%2520to%2520learn%2520and%2520master%2520by%2520leveraging%2520innate%2520inductive%2520biases%252C%250Amaking%2520them%2520an%2520ideal%2520testbed%2520for%2520evaluating%2520such%2520capabilities%2520in%2520VLMs.%2520To%2520this%250Aend%252C%2520we%2520introduce%2520VideoGameBench%252C%2520a%2520benchmark%2520consisting%2520of%252010%2520popular%2520video%250Agames%2520from%2520the%25201990s%2520that%2520VLMs%2520directly%2520interact%2520with%2520in%2520real-time.%250AVideoGameBench%2520challenges%2520models%2520to%2520complete%2520entire%2520games%2520with%2520access%2520to%2520only%250Araw%2520visual%2520inputs%2520and%2520a%2520high-level%2520description%2520of%2520objectives%2520and%2520controls%252C%2520a%250Asignificant%2520departure%2520from%2520existing%2520setups%2520that%2520rely%2520on%2520game-specific%250Ascaffolding%2520and%2520auxiliary%2520information.%2520We%2520keep%2520three%2520of%2520the%2520games%2520secret%2520to%250Aencourage%2520solutions%2520that%2520generalize%2520to%2520unseen%2520environments.%2520Our%2520experiments%250Ashow%2520that%2520frontier%2520vision-language%2520models%2520struggle%2520to%2520progress%2520beyond%2520the%250Abeginning%2520of%2520each%2520game.%2520We%2520find%2520inference%2520latency%2520to%2520be%2520a%2520major%2520limitation%2520of%250Afrontier%2520models%2520in%2520the%2520real-time%2520setting%253B%2520therefore%252C%2520we%2520introduce%250AVideoGameBench%2520Lite%252C%2520a%2520setting%2520where%2520the%2520game%2520pauses%2520while%2520waiting%2520for%2520the%2520LM%2527s%250Anext%2520action.%2520The%2520best%2520performing%2520model%252C%2520Gemini%25202.5%2520Pro%252C%2520completes%2520only%25200.48%2525%2520of%250AVideoGameBench%2520and%25201.6%2525%2520of%2520VideoGameBench%2520Lite.%2520We%2520hope%2520that%2520the%2520formalization%250Aof%2520the%2520human%2520skills%2520mentioned%2520above%2520into%2520this%2520benchmark%2520motivates%2520progress%2520in%250Athese%2520research%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18134v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoGameBench%3A%20Can%20Vision-Language%20Models%20complete%20popular%20video%20games%3F&entry.906535625=Alex%20L.%20Zhang%20and%20Thomas%20L.%20Griffiths%20and%20Karthik%20R.%20Narasimhan%20and%20Ofir%20Press&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20have%20achieved%20strong%20results%20on%20coding%20and%20math%0Abenchmarks%20that%20are%20challenging%20for%20humans%2C%20yet%20their%20ability%20to%20perform%20tasks%0Athat%20come%20naturally%20to%20humans--such%20as%20perception%2C%20spatial%20navigation%2C%20and%0Amemory%20management--remains%20understudied.%20Real%20video%20games%20are%20crafted%20to%20be%0Aintuitive%20for%20humans%20to%20learn%20and%20master%20by%20leveraging%20innate%20inductive%20biases%2C%0Amaking%20them%20an%20ideal%20testbed%20for%20evaluating%20such%20capabilities%20in%20VLMs.%20To%20this%0Aend%2C%20we%20introduce%20VideoGameBench%2C%20a%20benchmark%20consisting%20of%2010%20popular%20video%0Agames%20from%20the%201990s%20that%20VLMs%20directly%20interact%20with%20in%20real-time.%0AVideoGameBench%20challenges%20models%20to%20complete%20entire%20games%20with%20access%20to%20only%0Araw%20visual%20inputs%20and%20a%20high-level%20description%20of%20objectives%20and%20controls%2C%20a%0Asignificant%20departure%20from%20existing%20setups%20that%20rely%20on%20game-specific%0Ascaffolding%20and%20auxiliary%20information.%20We%20keep%20three%20of%20the%20games%20secret%20to%0Aencourage%20solutions%20that%20generalize%20to%20unseen%20environments.%20Our%20experiments%0Ashow%20that%20frontier%20vision-language%20models%20struggle%20to%20progress%20beyond%20the%0Abeginning%20of%20each%20game.%20We%20find%20inference%20latency%20to%20be%20a%20major%20limitation%20of%0Afrontier%20models%20in%20the%20real-time%20setting%3B%20therefore%2C%20we%20introduce%0AVideoGameBench%20Lite%2C%20a%20setting%20where%20the%20game%20pauses%20while%20waiting%20for%20the%20LM%27s%0Anext%20action.%20The%20best%20performing%20model%2C%20Gemini%202.5%20Pro%2C%20completes%20only%200.48%25%20of%0AVideoGameBench%20and%201.6%25%20of%20VideoGameBench%20Lite.%20We%20hope%20that%20the%20formalization%0Aof%20the%20human%20skills%20mentioned%20above%20into%20this%20benchmark%20motivates%20progress%20in%0Athese%20research%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18134v2&entry.124074799=Read"},
{"title": "AnimeGamer: Infinite Anime Life Simulation with Next Game State\n  Prediction", "author": "Junhao Cheng and Yuying Ge and Yixiao Ge and Jing Liao and Ying Shan", "abstract": "  Recent advancements in image and video synthesis have opened up new promise\nin generative games. One particularly intriguing application is transforming\ncharacters from anime films into interactive, playable entities. This allows\nplayers to immerse themselves in the dynamic anime world as their favorite\ncharacters for life simulation through language instructions. Such games are\ndefined as infinite game since they eliminate predetermined boundaries and\nfixed gameplay rules, where players can interact with the game world through\nopen-ended language and experience ever-evolving storylines and environments.\nRecently, a pioneering approach for infinite anime life simulation employs\nlarge language models (LLMs) to translate multi-turn text dialogues into\nlanguage instructions for image generation. However, it neglects historical\nvisual context, leading to inconsistent gameplay. Furthermore, it only\ngenerates static images, failing to incorporate the dynamics necessary for an\nengaging gaming experience. In this work, we propose AnimeGamer, which is built\nupon Multimodal Large Language Models (MLLMs) to generate each game state,\nincluding dynamic animation shots that depict character movements and updates\nto character states, as illustrated in Figure 1. We introduce novel\naction-aware multimodal representations to represent animation shots, which can\nbe decoded into high-quality video clips using a video diffusion model. By\ntaking historical animation shot representations as context and predicting\nsubsequent representations, AnimeGamer can generate games with contextual\nconsistency and satisfactory dynamics. Extensive evaluations using both\nautomated metrics and human evaluations demonstrate that AnimeGamer outperforms\nexisting methods in various aspects of the gaming experience. Codes and\ncheckpoints are available at https://github.com/TencentARC/AnimeGamer.\n", "link": "http://arxiv.org/abs/2504.01014v2", "date": "2025-05-30", "relevancy": 2.8601, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5902}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5646}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnimeGamer%3A%20Infinite%20Anime%20Life%20Simulation%20with%20Next%20Game%20State%0A%20%20Prediction&body=Title%3A%20AnimeGamer%3A%20Infinite%20Anime%20Life%20Simulation%20with%20Next%20Game%20State%0A%20%20Prediction%0AAuthor%3A%20Junhao%20Cheng%20and%20Yuying%20Ge%20and%20Yixiao%20Ge%20and%20Jing%20Liao%20and%20Ying%20Shan%0AAbstract%3A%20%20%20Recent%20advancements%20in%20image%20and%20video%20synthesis%20have%20opened%20up%20new%20promise%0Ain%20generative%20games.%20One%20particularly%20intriguing%20application%20is%20transforming%0Acharacters%20from%20anime%20films%20into%20interactive%2C%20playable%20entities.%20This%20allows%0Aplayers%20to%20immerse%20themselves%20in%20the%20dynamic%20anime%20world%20as%20their%20favorite%0Acharacters%20for%20life%20simulation%20through%20language%20instructions.%20Such%20games%20are%0Adefined%20as%20infinite%20game%20since%20they%20eliminate%20predetermined%20boundaries%20and%0Afixed%20gameplay%20rules%2C%20where%20players%20can%20interact%20with%20the%20game%20world%20through%0Aopen-ended%20language%20and%20experience%20ever-evolving%20storylines%20and%20environments.%0ARecently%2C%20a%20pioneering%20approach%20for%20infinite%20anime%20life%20simulation%20employs%0Alarge%20language%20models%20%28LLMs%29%20to%20translate%20multi-turn%20text%20dialogues%20into%0Alanguage%20instructions%20for%20image%20generation.%20However%2C%20it%20neglects%20historical%0Avisual%20context%2C%20leading%20to%20inconsistent%20gameplay.%20Furthermore%2C%20it%20only%0Agenerates%20static%20images%2C%20failing%20to%20incorporate%20the%20dynamics%20necessary%20for%20an%0Aengaging%20gaming%20experience.%20In%20this%20work%2C%20we%20propose%20AnimeGamer%2C%20which%20is%20built%0Aupon%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20to%20generate%20each%20game%20state%2C%0Aincluding%20dynamic%20animation%20shots%20that%20depict%20character%20movements%20and%20updates%0Ato%20character%20states%2C%20as%20illustrated%20in%20Figure%201.%20We%20introduce%20novel%0Aaction-aware%20multimodal%20representations%20to%20represent%20animation%20shots%2C%20which%20can%0Abe%20decoded%20into%20high-quality%20video%20clips%20using%20a%20video%20diffusion%20model.%20By%0Ataking%20historical%20animation%20shot%20representations%20as%20context%20and%20predicting%0Asubsequent%20representations%2C%20AnimeGamer%20can%20generate%20games%20with%20contextual%0Aconsistency%20and%20satisfactory%20dynamics.%20Extensive%20evaluations%20using%20both%0Aautomated%20metrics%20and%20human%20evaluations%20demonstrate%20that%20AnimeGamer%20outperforms%0Aexisting%20methods%20in%20various%20aspects%20of%20the%20gaming%20experience.%20Codes%20and%0Acheckpoints%20are%20available%20at%20https%3A//github.com/TencentARC/AnimeGamer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.01014v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnimeGamer%253A%2520Infinite%2520Anime%2520Life%2520Simulation%2520with%2520Next%2520Game%2520State%250A%2520%2520Prediction%26entry.906535625%3DJunhao%2520Cheng%2520and%2520Yuying%2520Ge%2520and%2520Yixiao%2520Ge%2520and%2520Jing%2520Liao%2520and%2520Ying%2520Shan%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520image%2520and%2520video%2520synthesis%2520have%2520opened%2520up%2520new%2520promise%250Ain%2520generative%2520games.%2520One%2520particularly%2520intriguing%2520application%2520is%2520transforming%250Acharacters%2520from%2520anime%2520films%2520into%2520interactive%252C%2520playable%2520entities.%2520This%2520allows%250Aplayers%2520to%2520immerse%2520themselves%2520in%2520the%2520dynamic%2520anime%2520world%2520as%2520their%2520favorite%250Acharacters%2520for%2520life%2520simulation%2520through%2520language%2520instructions.%2520Such%2520games%2520are%250Adefined%2520as%2520infinite%2520game%2520since%2520they%2520eliminate%2520predetermined%2520boundaries%2520and%250Afixed%2520gameplay%2520rules%252C%2520where%2520players%2520can%2520interact%2520with%2520the%2520game%2520world%2520through%250Aopen-ended%2520language%2520and%2520experience%2520ever-evolving%2520storylines%2520and%2520environments.%250ARecently%252C%2520a%2520pioneering%2520approach%2520for%2520infinite%2520anime%2520life%2520simulation%2520employs%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520to%2520translate%2520multi-turn%2520text%2520dialogues%2520into%250Alanguage%2520instructions%2520for%2520image%2520generation.%2520However%252C%2520it%2520neglects%2520historical%250Avisual%2520context%252C%2520leading%2520to%2520inconsistent%2520gameplay.%2520Furthermore%252C%2520it%2520only%250Agenerates%2520static%2520images%252C%2520failing%2520to%2520incorporate%2520the%2520dynamics%2520necessary%2520for%2520an%250Aengaging%2520gaming%2520experience.%2520In%2520this%2520work%252C%2520we%2520propose%2520AnimeGamer%252C%2520which%2520is%2520built%250Aupon%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520to%2520generate%2520each%2520game%2520state%252C%250Aincluding%2520dynamic%2520animation%2520shots%2520that%2520depict%2520character%2520movements%2520and%2520updates%250Ato%2520character%2520states%252C%2520as%2520illustrated%2520in%2520Figure%25201.%2520We%2520introduce%2520novel%250Aaction-aware%2520multimodal%2520representations%2520to%2520represent%2520animation%2520shots%252C%2520which%2520can%250Abe%2520decoded%2520into%2520high-quality%2520video%2520clips%2520using%2520a%2520video%2520diffusion%2520model.%2520By%250Ataking%2520historical%2520animation%2520shot%2520representations%2520as%2520context%2520and%2520predicting%250Asubsequent%2520representations%252C%2520AnimeGamer%2520can%2520generate%2520games%2520with%2520contextual%250Aconsistency%2520and%2520satisfactory%2520dynamics.%2520Extensive%2520evaluations%2520using%2520both%250Aautomated%2520metrics%2520and%2520human%2520evaluations%2520demonstrate%2520that%2520AnimeGamer%2520outperforms%250Aexisting%2520methods%2520in%2520various%2520aspects%2520of%2520the%2520gaming%2520experience.%2520Codes%2520and%250Acheckpoints%2520are%2520available%2520at%2520https%253A//github.com/TencentARC/AnimeGamer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.01014v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnimeGamer%3A%20Infinite%20Anime%20Life%20Simulation%20with%20Next%20Game%20State%0A%20%20Prediction&entry.906535625=Junhao%20Cheng%20and%20Yuying%20Ge%20and%20Yixiao%20Ge%20and%20Jing%20Liao%20and%20Ying%20Shan&entry.1292438233=%20%20Recent%20advancements%20in%20image%20and%20video%20synthesis%20have%20opened%20up%20new%20promise%0Ain%20generative%20games.%20One%20particularly%20intriguing%20application%20is%20transforming%0Acharacters%20from%20anime%20films%20into%20interactive%2C%20playable%20entities.%20This%20allows%0Aplayers%20to%20immerse%20themselves%20in%20the%20dynamic%20anime%20world%20as%20their%20favorite%0Acharacters%20for%20life%20simulation%20through%20language%20instructions.%20Such%20games%20are%0Adefined%20as%20infinite%20game%20since%20they%20eliminate%20predetermined%20boundaries%20and%0Afixed%20gameplay%20rules%2C%20where%20players%20can%20interact%20with%20the%20game%20world%20through%0Aopen-ended%20language%20and%20experience%20ever-evolving%20storylines%20and%20environments.%0ARecently%2C%20a%20pioneering%20approach%20for%20infinite%20anime%20life%20simulation%20employs%0Alarge%20language%20models%20%28LLMs%29%20to%20translate%20multi-turn%20text%20dialogues%20into%0Alanguage%20instructions%20for%20image%20generation.%20However%2C%20it%20neglects%20historical%0Avisual%20context%2C%20leading%20to%20inconsistent%20gameplay.%20Furthermore%2C%20it%20only%0Agenerates%20static%20images%2C%20failing%20to%20incorporate%20the%20dynamics%20necessary%20for%20an%0Aengaging%20gaming%20experience.%20In%20this%20work%2C%20we%20propose%20AnimeGamer%2C%20which%20is%20built%0Aupon%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20to%20generate%20each%20game%20state%2C%0Aincluding%20dynamic%20animation%20shots%20that%20depict%20character%20movements%20and%20updates%0Ato%20character%20states%2C%20as%20illustrated%20in%20Figure%201.%20We%20introduce%20novel%0Aaction-aware%20multimodal%20representations%20to%20represent%20animation%20shots%2C%20which%20can%0Abe%20decoded%20into%20high-quality%20video%20clips%20using%20a%20video%20diffusion%20model.%20By%0Ataking%20historical%20animation%20shot%20representations%20as%20context%20and%20predicting%0Asubsequent%20representations%2C%20AnimeGamer%20can%20generate%20games%20with%20contextual%0Aconsistency%20and%20satisfactory%20dynamics.%20Extensive%20evaluations%20using%20both%0Aautomated%20metrics%20and%20human%20evaluations%20demonstrate%20that%20AnimeGamer%20outperforms%0Aexisting%20methods%20in%20various%20aspects%20of%20the%20gaming%20experience.%20Codes%20and%0Acheckpoints%20are%20available%20at%20https%3A//github.com/TencentARC/AnimeGamer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.01014v2&entry.124074799=Read"},
{"title": "Mixpert: Mitigating Multimodal Learning Conflicts with Efficient\n  Mixture-of-Vision-Experts", "author": "Xin He and Xumeng Han and Longhui Wei and Lingxi Xie and Qi Tian", "abstract": "  Multimodal large language models (MLLMs) require a nuanced interpretation of\ncomplex image information, typically leveraging a vision encoder to perceive\nvarious visual scenarios. However, relying solely on a single vision encoder to\nhandle diverse task domains proves difficult and inevitably leads to conflicts.\nRecent work enhances data perception by directly integrating multiple\ndomain-specific vision encoders, yet this structure adds complexity and limits\nthe potential for joint optimization. In this paper, we introduce Mixpert, an\nefficient mixture-of-vision-experts architecture that inherits the joint\nlearning advantages from a single vision encoder while being restructured into\na multi-expert paradigm for task-specific fine-tuning across different visual\ntasks. Additionally, we design a dynamic routing mechanism that allocates input\nimages to the most suitable visual expert. Mixpert effectively alleviates\ndomain conflicts encountered by a single vision encoder in multi-task learning\nwith minimal additional computational cost, making it more efficient than\nmultiple encoders. Furthermore, Mixpert integrates seamlessly into any MLLM,\nwith experimental results demonstrating substantial performance gains across\nvarious tasks.\n", "link": "http://arxiv.org/abs/2505.24541v1", "date": "2025-05-30", "relevancy": 2.8507, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5729}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5707}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mixpert%3A%20Mitigating%20Multimodal%20Learning%20Conflicts%20with%20Efficient%0A%20%20Mixture-of-Vision-Experts&body=Title%3A%20Mixpert%3A%20Mitigating%20Multimodal%20Learning%20Conflicts%20with%20Efficient%0A%20%20Mixture-of-Vision-Experts%0AAuthor%3A%20Xin%20He%20and%20Xumeng%20Han%20and%20Longhui%20Wei%20and%20Lingxi%20Xie%20and%20Qi%20Tian%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20require%20a%20nuanced%20interpretation%20of%0Acomplex%20image%20information%2C%20typically%20leveraging%20a%20vision%20encoder%20to%20perceive%0Avarious%20visual%20scenarios.%20However%2C%20relying%20solely%20on%20a%20single%20vision%20encoder%20to%0Ahandle%20diverse%20task%20domains%20proves%20difficult%20and%20inevitably%20leads%20to%20conflicts.%0ARecent%20work%20enhances%20data%20perception%20by%20directly%20integrating%20multiple%0Adomain-specific%20vision%20encoders%2C%20yet%20this%20structure%20adds%20complexity%20and%20limits%0Athe%20potential%20for%20joint%20optimization.%20In%20this%20paper%2C%20we%20introduce%20Mixpert%2C%20an%0Aefficient%20mixture-of-vision-experts%20architecture%20that%20inherits%20the%20joint%0Alearning%20advantages%20from%20a%20single%20vision%20encoder%20while%20being%20restructured%20into%0Aa%20multi-expert%20paradigm%20for%20task-specific%20fine-tuning%20across%20different%20visual%0Atasks.%20Additionally%2C%20we%20design%20a%20dynamic%20routing%20mechanism%20that%20allocates%20input%0Aimages%20to%20the%20most%20suitable%20visual%20expert.%20Mixpert%20effectively%20alleviates%0Adomain%20conflicts%20encountered%20by%20a%20single%20vision%20encoder%20in%20multi-task%20learning%0Awith%20minimal%20additional%20computational%20cost%2C%20making%20it%20more%20efficient%20than%0Amultiple%20encoders.%20Furthermore%2C%20Mixpert%20integrates%20seamlessly%20into%20any%20MLLM%2C%0Awith%20experimental%20results%20demonstrating%20substantial%20performance%20gains%20across%0Avarious%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24541v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixpert%253A%2520Mitigating%2520Multimodal%2520Learning%2520Conflicts%2520with%2520Efficient%250A%2520%2520Mixture-of-Vision-Experts%26entry.906535625%3DXin%2520He%2520and%2520Xumeng%2520Han%2520and%2520Longhui%2520Wei%2520and%2520Lingxi%2520Xie%2520and%2520Qi%2520Tian%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520require%2520a%2520nuanced%2520interpretation%2520of%250Acomplex%2520image%2520information%252C%2520typically%2520leveraging%2520a%2520vision%2520encoder%2520to%2520perceive%250Avarious%2520visual%2520scenarios.%2520However%252C%2520relying%2520solely%2520on%2520a%2520single%2520vision%2520encoder%2520to%250Ahandle%2520diverse%2520task%2520domains%2520proves%2520difficult%2520and%2520inevitably%2520leads%2520to%2520conflicts.%250ARecent%2520work%2520enhances%2520data%2520perception%2520by%2520directly%2520integrating%2520multiple%250Adomain-specific%2520vision%2520encoders%252C%2520yet%2520this%2520structure%2520adds%2520complexity%2520and%2520limits%250Athe%2520potential%2520for%2520joint%2520optimization.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Mixpert%252C%2520an%250Aefficient%2520mixture-of-vision-experts%2520architecture%2520that%2520inherits%2520the%2520joint%250Alearning%2520advantages%2520from%2520a%2520single%2520vision%2520encoder%2520while%2520being%2520restructured%2520into%250Aa%2520multi-expert%2520paradigm%2520for%2520task-specific%2520fine-tuning%2520across%2520different%2520visual%250Atasks.%2520Additionally%252C%2520we%2520design%2520a%2520dynamic%2520routing%2520mechanism%2520that%2520allocates%2520input%250Aimages%2520to%2520the%2520most%2520suitable%2520visual%2520expert.%2520Mixpert%2520effectively%2520alleviates%250Adomain%2520conflicts%2520encountered%2520by%2520a%2520single%2520vision%2520encoder%2520in%2520multi-task%2520learning%250Awith%2520minimal%2520additional%2520computational%2520cost%252C%2520making%2520it%2520more%2520efficient%2520than%250Amultiple%2520encoders.%2520Furthermore%252C%2520Mixpert%2520integrates%2520seamlessly%2520into%2520any%2520MLLM%252C%250Awith%2520experimental%2520results%2520demonstrating%2520substantial%2520performance%2520gains%2520across%250Avarious%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24541v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixpert%3A%20Mitigating%20Multimodal%20Learning%20Conflicts%20with%20Efficient%0A%20%20Mixture-of-Vision-Experts&entry.906535625=Xin%20He%20and%20Xumeng%20Han%20and%20Longhui%20Wei%20and%20Lingxi%20Xie%20and%20Qi%20Tian&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20require%20a%20nuanced%20interpretation%20of%0Acomplex%20image%20information%2C%20typically%20leveraging%20a%20vision%20encoder%20to%20perceive%0Avarious%20visual%20scenarios.%20However%2C%20relying%20solely%20on%20a%20single%20vision%20encoder%20to%0Ahandle%20diverse%20task%20domains%20proves%20difficult%20and%20inevitably%20leads%20to%20conflicts.%0ARecent%20work%20enhances%20data%20perception%20by%20directly%20integrating%20multiple%0Adomain-specific%20vision%20encoders%2C%20yet%20this%20structure%20adds%20complexity%20and%20limits%0Athe%20potential%20for%20joint%20optimization.%20In%20this%20paper%2C%20we%20introduce%20Mixpert%2C%20an%0Aefficient%20mixture-of-vision-experts%20architecture%20that%20inherits%20the%20joint%0Alearning%20advantages%20from%20a%20single%20vision%20encoder%20while%20being%20restructured%20into%0Aa%20multi-expert%20paradigm%20for%20task-specific%20fine-tuning%20across%20different%20visual%0Atasks.%20Additionally%2C%20we%20design%20a%20dynamic%20routing%20mechanism%20that%20allocates%20input%0Aimages%20to%20the%20most%20suitable%20visual%20expert.%20Mixpert%20effectively%20alleviates%0Adomain%20conflicts%20encountered%20by%20a%20single%20vision%20encoder%20in%20multi-task%20learning%0Awith%20minimal%20additional%20computational%20cost%2C%20making%20it%20more%20efficient%20than%0Amultiple%20encoders.%20Furthermore%2C%20Mixpert%20integrates%20seamlessly%20into%20any%20MLLM%2C%0Awith%20experimental%20results%20demonstrating%20substantial%20performance%20gains%20across%0Avarious%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24541v1&entry.124074799=Read"},
{"title": "OpenUni: A Simple Baseline for Unified Multimodal Understanding and\n  Generation", "author": "Size Wu and Zhonghua Wu and Zerui Gong and Qingyi Tao and Sheng Jin and Qinyue Li and Wei Li and Chen Change Loy", "abstract": "  In this report, we present OpenUni, a simple, lightweight, and fully\nopen-source baseline for unifying multimodal understanding and generation.\nInspired by prevailing practices in unified model learning, we adopt an\nefficient training strategy that minimizes the training complexity and overhead\nby bridging the off-the-shelf multimodal large language models (LLMs) and\ndiffusion models through a set of learnable queries and a light-weight\ntransformer-based connector. With a minimalist choice of architecture, we\ndemonstrate that OpenUni can: 1) generate high-quality and instruction-aligned\nimages, and 2) achieve exceptional performance on standard benchmarks such as\nGenEval, DPG- Bench, and WISE, with only 1.1B and 3.1B activated parameters. To\nsupport open research and community advancement, we release all model weights,\ntraining code, and our curated training datasets (including 23M image-text\npairs) at https://github.com/wusize/OpenUni.\n", "link": "http://arxiv.org/abs/2505.23661v2", "date": "2025-05-30", "relevancy": 2.8222, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5765}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5584}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenUni%3A%20A%20Simple%20Baseline%20for%20Unified%20Multimodal%20Understanding%20and%0A%20%20Generation&body=Title%3A%20OpenUni%3A%20A%20Simple%20Baseline%20for%20Unified%20Multimodal%20Understanding%20and%0A%20%20Generation%0AAuthor%3A%20Size%20Wu%20and%20Zhonghua%20Wu%20and%20Zerui%20Gong%20and%20Qingyi%20Tao%20and%20Sheng%20Jin%20and%20Qinyue%20Li%20and%20Wei%20Li%20and%20Chen%20Change%20Loy%0AAbstract%3A%20%20%20In%20this%20report%2C%20we%20present%20OpenUni%2C%20a%20simple%2C%20lightweight%2C%20and%20fully%0Aopen-source%20baseline%20for%20unifying%20multimodal%20understanding%20and%20generation.%0AInspired%20by%20prevailing%20practices%20in%20unified%20model%20learning%2C%20we%20adopt%20an%0Aefficient%20training%20strategy%20that%20minimizes%20the%20training%20complexity%20and%20overhead%0Aby%20bridging%20the%20off-the-shelf%20multimodal%20large%20language%20models%20%28LLMs%29%20and%0Adiffusion%20models%20through%20a%20set%20of%20learnable%20queries%20and%20a%20light-weight%0Atransformer-based%20connector.%20With%20a%20minimalist%20choice%20of%20architecture%2C%20we%0Ademonstrate%20that%20OpenUni%20can%3A%201%29%20generate%20high-quality%20and%20instruction-aligned%0Aimages%2C%20and%202%29%20achieve%20exceptional%20performance%20on%20standard%20benchmarks%20such%20as%0AGenEval%2C%20DPG-%20Bench%2C%20and%20WISE%2C%20with%20only%201.1B%20and%203.1B%20activated%20parameters.%20To%0Asupport%20open%20research%20and%20community%20advancement%2C%20we%20release%20all%20model%20weights%2C%0Atraining%20code%2C%20and%20our%20curated%20training%20datasets%20%28including%2023M%20image-text%0Apairs%29%20at%20https%3A//github.com/wusize/OpenUni.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23661v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenUni%253A%2520A%2520Simple%2520Baseline%2520for%2520Unified%2520Multimodal%2520Understanding%2520and%250A%2520%2520Generation%26entry.906535625%3DSize%2520Wu%2520and%2520Zhonghua%2520Wu%2520and%2520Zerui%2520Gong%2520and%2520Qingyi%2520Tao%2520and%2520Sheng%2520Jin%2520and%2520Qinyue%2520Li%2520and%2520Wei%2520Li%2520and%2520Chen%2520Change%2520Loy%26entry.1292438233%3D%2520%2520In%2520this%2520report%252C%2520we%2520present%2520OpenUni%252C%2520a%2520simple%252C%2520lightweight%252C%2520and%2520fully%250Aopen-source%2520baseline%2520for%2520unifying%2520multimodal%2520understanding%2520and%2520generation.%250AInspired%2520by%2520prevailing%2520practices%2520in%2520unified%2520model%2520learning%252C%2520we%2520adopt%2520an%250Aefficient%2520training%2520strategy%2520that%2520minimizes%2520the%2520training%2520complexity%2520and%2520overhead%250Aby%2520bridging%2520the%2520off-the-shelf%2520multimodal%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%250Adiffusion%2520models%2520through%2520a%2520set%2520of%2520learnable%2520queries%2520and%2520a%2520light-weight%250Atransformer-based%2520connector.%2520With%2520a%2520minimalist%2520choice%2520of%2520architecture%252C%2520we%250Ademonstrate%2520that%2520OpenUni%2520can%253A%25201%2529%2520generate%2520high-quality%2520and%2520instruction-aligned%250Aimages%252C%2520and%25202%2529%2520achieve%2520exceptional%2520performance%2520on%2520standard%2520benchmarks%2520such%2520as%250AGenEval%252C%2520DPG-%2520Bench%252C%2520and%2520WISE%252C%2520with%2520only%25201.1B%2520and%25203.1B%2520activated%2520parameters.%2520To%250Asupport%2520open%2520research%2520and%2520community%2520advancement%252C%2520we%2520release%2520all%2520model%2520weights%252C%250Atraining%2520code%252C%2520and%2520our%2520curated%2520training%2520datasets%2520%2528including%252023M%2520image-text%250Apairs%2529%2520at%2520https%253A//github.com/wusize/OpenUni.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23661v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenUni%3A%20A%20Simple%20Baseline%20for%20Unified%20Multimodal%20Understanding%20and%0A%20%20Generation&entry.906535625=Size%20Wu%20and%20Zhonghua%20Wu%20and%20Zerui%20Gong%20and%20Qingyi%20Tao%20and%20Sheng%20Jin%20and%20Qinyue%20Li%20and%20Wei%20Li%20and%20Chen%20Change%20Loy&entry.1292438233=%20%20In%20this%20report%2C%20we%20present%20OpenUni%2C%20a%20simple%2C%20lightweight%2C%20and%20fully%0Aopen-source%20baseline%20for%20unifying%20multimodal%20understanding%20and%20generation.%0AInspired%20by%20prevailing%20practices%20in%20unified%20model%20learning%2C%20we%20adopt%20an%0Aefficient%20training%20strategy%20that%20minimizes%20the%20training%20complexity%20and%20overhead%0Aby%20bridging%20the%20off-the-shelf%20multimodal%20large%20language%20models%20%28LLMs%29%20and%0Adiffusion%20models%20through%20a%20set%20of%20learnable%20queries%20and%20a%20light-weight%0Atransformer-based%20connector.%20With%20a%20minimalist%20choice%20of%20architecture%2C%20we%0Ademonstrate%20that%20OpenUni%20can%3A%201%29%20generate%20high-quality%20and%20instruction-aligned%0Aimages%2C%20and%202%29%20achieve%20exceptional%20performance%20on%20standard%20benchmarks%20such%20as%0AGenEval%2C%20DPG-%20Bench%2C%20and%20WISE%2C%20with%20only%201.1B%20and%203.1B%20activated%20parameters.%20To%0Asupport%20open%20research%20and%20community%20advancement%2C%20we%20release%20all%20model%20weights%2C%0Atraining%20code%2C%20and%20our%20curated%20training%20datasets%20%28including%2023M%20image-text%0Apairs%29%20at%20https%3A//github.com/wusize/OpenUni.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23661v2&entry.124074799=Read"},
{"title": "RedundancyLens: Revealing and Exploiting Visual Token Processing\n  Redundancy for Efficient Decoder-Only MLLMs", "author": "Hongliang Li and Jiaxin Zhang and Wenhui Liao and Dezhi Peng and Kai Ding and Lianwen Jin", "abstract": "  Current Multimodal Large Language Model (MLLM) architectures face a critical\ntradeoff between performance and efficiency: decoder-only architectures achieve\nhigher performance but lower efficiency, while cross-attention-based\narchitectures offer greater efficiency but lower performance. The key\ndistinction lies in how visual tokens are processed. Decoder-only architectures\napply self-attention and FFN operations on visual tokens, while cross-attention\narchitectures skip these computations. To investigate whether redundancy exists\nin this computationally expensive process, we propose a training-free framework\nfor analyzing trained MLLMs. It consists of Probe-Activated Dynamic FFN and\nHollow Attention, which enable adjustable reductions in computations for visual\ntokens, as well as a Layer Ranking Algorithm that prioritizes layers for these\nreductions. Extensive experiments demonstrate substantial, structured, and\nclustered redundancy unique to decoder-only MLLMs, offering valuable insights\nfor future MLLM architecture design. Furthermore, by leveraging our reduction\nframework as a training-free inference acceleration approach, we achieve\nperformance comparable to or better than state-of-the-art methods while\nremaining compatible with them. Code will be publicly available at\nhttps://github.com/L-Hugh/RedundancyLens.\n", "link": "http://arxiv.org/abs/2501.19036v3", "date": "2025-05-30", "relevancy": 2.8129, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5687}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5595}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RedundancyLens%3A%20Revealing%20and%20Exploiting%20Visual%20Token%20Processing%0A%20%20Redundancy%20for%20Efficient%20Decoder-Only%20MLLMs&body=Title%3A%20RedundancyLens%3A%20Revealing%20and%20Exploiting%20Visual%20Token%20Processing%0A%20%20Redundancy%20for%20Efficient%20Decoder-Only%20MLLMs%0AAuthor%3A%20Hongliang%20Li%20and%20Jiaxin%20Zhang%20and%20Wenhui%20Liao%20and%20Dezhi%20Peng%20and%20Kai%20Ding%20and%20Lianwen%20Jin%0AAbstract%3A%20%20%20Current%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20architectures%20face%20a%20critical%0Atradeoff%20between%20performance%20and%20efficiency%3A%20decoder-only%20architectures%20achieve%0Ahigher%20performance%20but%20lower%20efficiency%2C%20while%20cross-attention-based%0Aarchitectures%20offer%20greater%20efficiency%20but%20lower%20performance.%20The%20key%0Adistinction%20lies%20in%20how%20visual%20tokens%20are%20processed.%20Decoder-only%20architectures%0Aapply%20self-attention%20and%20FFN%20operations%20on%20visual%20tokens%2C%20while%20cross-attention%0Aarchitectures%20skip%20these%20computations.%20To%20investigate%20whether%20redundancy%20exists%0Ain%20this%20computationally%20expensive%20process%2C%20we%20propose%20a%20training-free%20framework%0Afor%20analyzing%20trained%20MLLMs.%20It%20consists%20of%20Probe-Activated%20Dynamic%20FFN%20and%0AHollow%20Attention%2C%20which%20enable%20adjustable%20reductions%20in%20computations%20for%20visual%0Atokens%2C%20as%20well%20as%20a%20Layer%20Ranking%20Algorithm%20that%20prioritizes%20layers%20for%20these%0Areductions.%20Extensive%20experiments%20demonstrate%20substantial%2C%20structured%2C%20and%0Aclustered%20redundancy%20unique%20to%20decoder-only%20MLLMs%2C%20offering%20valuable%20insights%0Afor%20future%20MLLM%20architecture%20design.%20Furthermore%2C%20by%20leveraging%20our%20reduction%0Aframework%20as%20a%20training-free%20inference%20acceleration%20approach%2C%20we%20achieve%0Aperformance%20comparable%20to%20or%20better%20than%20state-of-the-art%20methods%20while%0Aremaining%20compatible%20with%20them.%20Code%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/L-Hugh/RedundancyLens.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19036v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRedundancyLens%253A%2520Revealing%2520and%2520Exploiting%2520Visual%2520Token%2520Processing%250A%2520%2520Redundancy%2520for%2520Efficient%2520Decoder-Only%2520MLLMs%26entry.906535625%3DHongliang%2520Li%2520and%2520Jiaxin%2520Zhang%2520and%2520Wenhui%2520Liao%2520and%2520Dezhi%2520Peng%2520and%2520Kai%2520Ding%2520and%2520Lianwen%2520Jin%26entry.1292438233%3D%2520%2520Current%2520Multimodal%2520Large%2520Language%2520Model%2520%2528MLLM%2529%2520architectures%2520face%2520a%2520critical%250Atradeoff%2520between%2520performance%2520and%2520efficiency%253A%2520decoder-only%2520architectures%2520achieve%250Ahigher%2520performance%2520but%2520lower%2520efficiency%252C%2520while%2520cross-attention-based%250Aarchitectures%2520offer%2520greater%2520efficiency%2520but%2520lower%2520performance.%2520The%2520key%250Adistinction%2520lies%2520in%2520how%2520visual%2520tokens%2520are%2520processed.%2520Decoder-only%2520architectures%250Aapply%2520self-attention%2520and%2520FFN%2520operations%2520on%2520visual%2520tokens%252C%2520while%2520cross-attention%250Aarchitectures%2520skip%2520these%2520computations.%2520To%2520investigate%2520whether%2520redundancy%2520exists%250Ain%2520this%2520computationally%2520expensive%2520process%252C%2520we%2520propose%2520a%2520training-free%2520framework%250Afor%2520analyzing%2520trained%2520MLLMs.%2520It%2520consists%2520of%2520Probe-Activated%2520Dynamic%2520FFN%2520and%250AHollow%2520Attention%252C%2520which%2520enable%2520adjustable%2520reductions%2520in%2520computations%2520for%2520visual%250Atokens%252C%2520as%2520well%2520as%2520a%2520Layer%2520Ranking%2520Algorithm%2520that%2520prioritizes%2520layers%2520for%2520these%250Areductions.%2520Extensive%2520experiments%2520demonstrate%2520substantial%252C%2520structured%252C%2520and%250Aclustered%2520redundancy%2520unique%2520to%2520decoder-only%2520MLLMs%252C%2520offering%2520valuable%2520insights%250Afor%2520future%2520MLLM%2520architecture%2520design.%2520Furthermore%252C%2520by%2520leveraging%2520our%2520reduction%250Aframework%2520as%2520a%2520training-free%2520inference%2520acceleration%2520approach%252C%2520we%2520achieve%250Aperformance%2520comparable%2520to%2520or%2520better%2520than%2520state-of-the-art%2520methods%2520while%250Aremaining%2520compatible%2520with%2520them.%2520Code%2520will%2520be%2520publicly%2520available%2520at%250Ahttps%253A//github.com/L-Hugh/RedundancyLens.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19036v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RedundancyLens%3A%20Revealing%20and%20Exploiting%20Visual%20Token%20Processing%0A%20%20Redundancy%20for%20Efficient%20Decoder-Only%20MLLMs&entry.906535625=Hongliang%20Li%20and%20Jiaxin%20Zhang%20and%20Wenhui%20Liao%20and%20Dezhi%20Peng%20and%20Kai%20Ding%20and%20Lianwen%20Jin&entry.1292438233=%20%20Current%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20architectures%20face%20a%20critical%0Atradeoff%20between%20performance%20and%20efficiency%3A%20decoder-only%20architectures%20achieve%0Ahigher%20performance%20but%20lower%20efficiency%2C%20while%20cross-attention-based%0Aarchitectures%20offer%20greater%20efficiency%20but%20lower%20performance.%20The%20key%0Adistinction%20lies%20in%20how%20visual%20tokens%20are%20processed.%20Decoder-only%20architectures%0Aapply%20self-attention%20and%20FFN%20operations%20on%20visual%20tokens%2C%20while%20cross-attention%0Aarchitectures%20skip%20these%20computations.%20To%20investigate%20whether%20redundancy%20exists%0Ain%20this%20computationally%20expensive%20process%2C%20we%20propose%20a%20training-free%20framework%0Afor%20analyzing%20trained%20MLLMs.%20It%20consists%20of%20Probe-Activated%20Dynamic%20FFN%20and%0AHollow%20Attention%2C%20which%20enable%20adjustable%20reductions%20in%20computations%20for%20visual%0Atokens%2C%20as%20well%20as%20a%20Layer%20Ranking%20Algorithm%20that%20prioritizes%20layers%20for%20these%0Areductions.%20Extensive%20experiments%20demonstrate%20substantial%2C%20structured%2C%20and%0Aclustered%20redundancy%20unique%20to%20decoder-only%20MLLMs%2C%20offering%20valuable%20insights%0Afor%20future%20MLLM%20architecture%20design.%20Furthermore%2C%20by%20leveraging%20our%20reduction%0Aframework%20as%20a%20training-free%20inference%20acceleration%20approach%2C%20we%20achieve%0Aperformance%20comparable%20to%20or%20better%20than%20state-of-the-art%20methods%20while%0Aremaining%20compatible%20with%20them.%20Code%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/L-Hugh/RedundancyLens.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19036v3&entry.124074799=Read"},
{"title": "Advancing Compositional Awareness in CLIP with Efficient Fine-Tuning", "author": "Amit Peleg and Naman Deep Singh and Matthias Hein", "abstract": "  Vision-language models like CLIP have demonstrated remarkable zero-shot\ncapabilities in classification and retrieval. However, these models often\nstruggle with compositional reasoning - the ability to understand the\nrelationships between concepts. A recent benchmark, SugarCrepe++, reveals that\nprevious works on improving compositionality have mainly improved lexical\nsensitivity but neglected semantic understanding. In addition, downstream\nretrieval performance often deteriorates, although one would expect that\nimproving compositionality should enhance retrieval. In this work, we introduce\nCLIC (Compositionally-aware Learning in CLIP), a fine-tuning method based on a\nnovel training technique combining multiple images and their associated\ncaptions. CLIC improves compositionality across architectures as well as\ndifferently pre-trained CLIP models, both in terms of lexical and semantic\nunderstanding, and achieves consistent gains in retrieval performance. This\neven applies to the recent CLIPS, which achieves SOTA retrieval performance.\nNevertheless, the short fine-tuning with CLIC leads to an improvement in\nretrieval and to the best compositional CLIP model on SugarCrepe++. All our\nmodels and code are available at https://clic-compositional-clip.github.io\n", "link": "http://arxiv.org/abs/2505.24424v1", "date": "2025-05-30", "relevancy": 2.8084, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5689}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.558}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Compositional%20Awareness%20in%20CLIP%20with%20Efficient%20Fine-Tuning&body=Title%3A%20Advancing%20Compositional%20Awareness%20in%20CLIP%20with%20Efficient%20Fine-Tuning%0AAuthor%3A%20Amit%20Peleg%20and%20Naman%20Deep%20Singh%20and%20Matthias%20Hein%0AAbstract%3A%20%20%20Vision-language%20models%20like%20CLIP%20have%20demonstrated%20remarkable%20zero-shot%0Acapabilities%20in%20classification%20and%20retrieval.%20However%2C%20these%20models%20often%0Astruggle%20with%20compositional%20reasoning%20-%20the%20ability%20to%20understand%20the%0Arelationships%20between%20concepts.%20A%20recent%20benchmark%2C%20SugarCrepe%2B%2B%2C%20reveals%20that%0Aprevious%20works%20on%20improving%20compositionality%20have%20mainly%20improved%20lexical%0Asensitivity%20but%20neglected%20semantic%20understanding.%20In%20addition%2C%20downstream%0Aretrieval%20performance%20often%20deteriorates%2C%20although%20one%20would%20expect%20that%0Aimproving%20compositionality%20should%20enhance%20retrieval.%20In%20this%20work%2C%20we%20introduce%0ACLIC%20%28Compositionally-aware%20Learning%20in%20CLIP%29%2C%20a%20fine-tuning%20method%20based%20on%20a%0Anovel%20training%20technique%20combining%20multiple%20images%20and%20their%20associated%0Acaptions.%20CLIC%20improves%20compositionality%20across%20architectures%20as%20well%20as%0Adifferently%20pre-trained%20CLIP%20models%2C%20both%20in%20terms%20of%20lexical%20and%20semantic%0Aunderstanding%2C%20and%20achieves%20consistent%20gains%20in%20retrieval%20performance.%20This%0Aeven%20applies%20to%20the%20recent%20CLIPS%2C%20which%20achieves%20SOTA%20retrieval%20performance.%0ANevertheless%2C%20the%20short%20fine-tuning%20with%20CLIC%20leads%20to%20an%20improvement%20in%0Aretrieval%20and%20to%20the%20best%20compositional%20CLIP%20model%20on%20SugarCrepe%2B%2B.%20All%20our%0Amodels%20and%20code%20are%20available%20at%20https%3A//clic-compositional-clip.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24424v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Compositional%2520Awareness%2520in%2520CLIP%2520with%2520Efficient%2520Fine-Tuning%26entry.906535625%3DAmit%2520Peleg%2520and%2520Naman%2520Deep%2520Singh%2520and%2520Matthias%2520Hein%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520like%2520CLIP%2520have%2520demonstrated%2520remarkable%2520zero-shot%250Acapabilities%2520in%2520classification%2520and%2520retrieval.%2520However%252C%2520these%2520models%2520often%250Astruggle%2520with%2520compositional%2520reasoning%2520-%2520the%2520ability%2520to%2520understand%2520the%250Arelationships%2520between%2520concepts.%2520A%2520recent%2520benchmark%252C%2520SugarCrepe%252B%252B%252C%2520reveals%2520that%250Aprevious%2520works%2520on%2520improving%2520compositionality%2520have%2520mainly%2520improved%2520lexical%250Asensitivity%2520but%2520neglected%2520semantic%2520understanding.%2520In%2520addition%252C%2520downstream%250Aretrieval%2520performance%2520often%2520deteriorates%252C%2520although%2520one%2520would%2520expect%2520that%250Aimproving%2520compositionality%2520should%2520enhance%2520retrieval.%2520In%2520this%2520work%252C%2520we%2520introduce%250ACLIC%2520%2528Compositionally-aware%2520Learning%2520in%2520CLIP%2529%252C%2520a%2520fine-tuning%2520method%2520based%2520on%2520a%250Anovel%2520training%2520technique%2520combining%2520multiple%2520images%2520and%2520their%2520associated%250Acaptions.%2520CLIC%2520improves%2520compositionality%2520across%2520architectures%2520as%2520well%2520as%250Adifferently%2520pre-trained%2520CLIP%2520models%252C%2520both%2520in%2520terms%2520of%2520lexical%2520and%2520semantic%250Aunderstanding%252C%2520and%2520achieves%2520consistent%2520gains%2520in%2520retrieval%2520performance.%2520This%250Aeven%2520applies%2520to%2520the%2520recent%2520CLIPS%252C%2520which%2520achieves%2520SOTA%2520retrieval%2520performance.%250ANevertheless%252C%2520the%2520short%2520fine-tuning%2520with%2520CLIC%2520leads%2520to%2520an%2520improvement%2520in%250Aretrieval%2520and%2520to%2520the%2520best%2520compositional%2520CLIP%2520model%2520on%2520SugarCrepe%252B%252B.%2520All%2520our%250Amodels%2520and%2520code%2520are%2520available%2520at%2520https%253A//clic-compositional-clip.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24424v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Compositional%20Awareness%20in%20CLIP%20with%20Efficient%20Fine-Tuning&entry.906535625=Amit%20Peleg%20and%20Naman%20Deep%20Singh%20and%20Matthias%20Hein&entry.1292438233=%20%20Vision-language%20models%20like%20CLIP%20have%20demonstrated%20remarkable%20zero-shot%0Acapabilities%20in%20classification%20and%20retrieval.%20However%2C%20these%20models%20often%0Astruggle%20with%20compositional%20reasoning%20-%20the%20ability%20to%20understand%20the%0Arelationships%20between%20concepts.%20A%20recent%20benchmark%2C%20SugarCrepe%2B%2B%2C%20reveals%20that%0Aprevious%20works%20on%20improving%20compositionality%20have%20mainly%20improved%20lexical%0Asensitivity%20but%20neglected%20semantic%20understanding.%20In%20addition%2C%20downstream%0Aretrieval%20performance%20often%20deteriorates%2C%20although%20one%20would%20expect%20that%0Aimproving%20compositionality%20should%20enhance%20retrieval.%20In%20this%20work%2C%20we%20introduce%0ACLIC%20%28Compositionally-aware%20Learning%20in%20CLIP%29%2C%20a%20fine-tuning%20method%20based%20on%20a%0Anovel%20training%20technique%20combining%20multiple%20images%20and%20their%20associated%0Acaptions.%20CLIC%20improves%20compositionality%20across%20architectures%20as%20well%20as%0Adifferently%20pre-trained%20CLIP%20models%2C%20both%20in%20terms%20of%20lexical%20and%20semantic%0Aunderstanding%2C%20and%20achieves%20consistent%20gains%20in%20retrieval%20performance.%20This%0Aeven%20applies%20to%20the%20recent%20CLIPS%2C%20which%20achieves%20SOTA%20retrieval%20performance.%0ANevertheless%2C%20the%20short%20fine-tuning%20with%20CLIC%20leads%20to%20an%20improvement%20in%0Aretrieval%20and%20to%20the%20best%20compositional%20CLIP%20model%20on%20SugarCrepe%2B%2B.%20All%20our%0Amodels%20and%20code%20are%20available%20at%20https%3A//clic-compositional-clip.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24424v1&entry.124074799=Read"},
{"title": "Vision LLMs Are Bad at Hierarchical Visual Understanding, and LLMs Are\n  the Bottleneck", "author": "Yuwen Tan and Yuan Qing and Boqing Gong", "abstract": "  This paper reveals that many state-of-the-art large language models (LLMs)\nlack hierarchical knowledge about our visual world, unaware of even\nwell-established biology taxonomies. This shortcoming makes LLMs a bottleneck\nfor vision LLMs' hierarchical visual understanding (e.g., recognizing Anemone\nFish but not Vertebrate). We arrive at these findings using about one million\nfour-choice visual question answering (VQA) tasks constructed from six\ntaxonomies and four image datasets. Interestingly, finetuning a vision LLM\nusing our VQA tasks reaffirms LLMs' bottleneck effect to some extent because\nthe VQA tasks improve the LLM's hierarchical consistency more than the vision\nLLM's. We conjecture that one cannot make vision LLMs understand visual\nconcepts fully hierarchical until LLMs possess corresponding taxonomy\nknowledge.\n", "link": "http://arxiv.org/abs/2505.24840v1", "date": "2025-05-30", "relevancy": 2.8029, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5916}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5916}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20LLMs%20Are%20Bad%20at%20Hierarchical%20Visual%20Understanding%2C%20and%20LLMs%20Are%0A%20%20the%20Bottleneck&body=Title%3A%20Vision%20LLMs%20Are%20Bad%20at%20Hierarchical%20Visual%20Understanding%2C%20and%20LLMs%20Are%0A%20%20the%20Bottleneck%0AAuthor%3A%20Yuwen%20Tan%20and%20Yuan%20Qing%20and%20Boqing%20Gong%0AAbstract%3A%20%20%20This%20paper%20reveals%20that%20many%20state-of-the-art%20large%20language%20models%20%28LLMs%29%0Alack%20hierarchical%20knowledge%20about%20our%20visual%20world%2C%20unaware%20of%20even%0Awell-established%20biology%20taxonomies.%20This%20shortcoming%20makes%20LLMs%20a%20bottleneck%0Afor%20vision%20LLMs%27%20hierarchical%20visual%20understanding%20%28e.g.%2C%20recognizing%20Anemone%0AFish%20but%20not%20Vertebrate%29.%20We%20arrive%20at%20these%20findings%20using%20about%20one%20million%0Afour-choice%20visual%20question%20answering%20%28VQA%29%20tasks%20constructed%20from%20six%0Ataxonomies%20and%20four%20image%20datasets.%20Interestingly%2C%20finetuning%20a%20vision%20LLM%0Ausing%20our%20VQA%20tasks%20reaffirms%20LLMs%27%20bottleneck%20effect%20to%20some%20extent%20because%0Athe%20VQA%20tasks%20improve%20the%20LLM%27s%20hierarchical%20consistency%20more%20than%20the%20vision%0ALLM%27s.%20We%20conjecture%20that%20one%20cannot%20make%20vision%20LLMs%20understand%20visual%0Aconcepts%20fully%20hierarchical%20until%20LLMs%20possess%20corresponding%20taxonomy%0Aknowledge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24840v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520LLMs%2520Are%2520Bad%2520at%2520Hierarchical%2520Visual%2520Understanding%252C%2520and%2520LLMs%2520Are%250A%2520%2520the%2520Bottleneck%26entry.906535625%3DYuwen%2520Tan%2520and%2520Yuan%2520Qing%2520and%2520Boqing%2520Gong%26entry.1292438233%3D%2520%2520This%2520paper%2520reveals%2520that%2520many%2520state-of-the-art%2520large%2520language%2520models%2520%2528LLMs%2529%250Alack%2520hierarchical%2520knowledge%2520about%2520our%2520visual%2520world%252C%2520unaware%2520of%2520even%250Awell-established%2520biology%2520taxonomies.%2520This%2520shortcoming%2520makes%2520LLMs%2520a%2520bottleneck%250Afor%2520vision%2520LLMs%2527%2520hierarchical%2520visual%2520understanding%2520%2528e.g.%252C%2520recognizing%2520Anemone%250AFish%2520but%2520not%2520Vertebrate%2529.%2520We%2520arrive%2520at%2520these%2520findings%2520using%2520about%2520one%2520million%250Afour-choice%2520visual%2520question%2520answering%2520%2528VQA%2529%2520tasks%2520constructed%2520from%2520six%250Ataxonomies%2520and%2520four%2520image%2520datasets.%2520Interestingly%252C%2520finetuning%2520a%2520vision%2520LLM%250Ausing%2520our%2520VQA%2520tasks%2520reaffirms%2520LLMs%2527%2520bottleneck%2520effect%2520to%2520some%2520extent%2520because%250Athe%2520VQA%2520tasks%2520improve%2520the%2520LLM%2527s%2520hierarchical%2520consistency%2520more%2520than%2520the%2520vision%250ALLM%2527s.%2520We%2520conjecture%2520that%2520one%2520cannot%2520make%2520vision%2520LLMs%2520understand%2520visual%250Aconcepts%2520fully%2520hierarchical%2520until%2520LLMs%2520possess%2520corresponding%2520taxonomy%250Aknowledge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24840v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20LLMs%20Are%20Bad%20at%20Hierarchical%20Visual%20Understanding%2C%20and%20LLMs%20Are%0A%20%20the%20Bottleneck&entry.906535625=Yuwen%20Tan%20and%20Yuan%20Qing%20and%20Boqing%20Gong&entry.1292438233=%20%20This%20paper%20reveals%20that%20many%20state-of-the-art%20large%20language%20models%20%28LLMs%29%0Alack%20hierarchical%20knowledge%20about%20our%20visual%20world%2C%20unaware%20of%20even%0Awell-established%20biology%20taxonomies.%20This%20shortcoming%20makes%20LLMs%20a%20bottleneck%0Afor%20vision%20LLMs%27%20hierarchical%20visual%20understanding%20%28e.g.%2C%20recognizing%20Anemone%0AFish%20but%20not%20Vertebrate%29.%20We%20arrive%20at%20these%20findings%20using%20about%20one%20million%0Afour-choice%20visual%20question%20answering%20%28VQA%29%20tasks%20constructed%20from%20six%0Ataxonomies%20and%20four%20image%20datasets.%20Interestingly%2C%20finetuning%20a%20vision%20LLM%0Ausing%20our%20VQA%20tasks%20reaffirms%20LLMs%27%20bottleneck%20effect%20to%20some%20extent%20because%0Athe%20VQA%20tasks%20improve%20the%20LLM%27s%20hierarchical%20consistency%20more%20than%20the%20vision%0ALLM%27s.%20We%20conjecture%20that%20one%20cannot%20make%20vision%20LLMs%20understand%20visual%0Aconcepts%20fully%20hierarchical%20until%20LLMs%20possess%20corresponding%20taxonomy%0Aknowledge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24840v1&entry.124074799=Read"},
{"title": "GPT4Point: A Unified Framework for Point-Language Understanding and\n  Generation", "author": "Zhangyang Qi and Ye Fang and Zeyi Sun and Xiaoyang Wu and Tong Wu and Jiaqi Wang and Dahua Lin and Hengshuang Zhao", "abstract": "  Multimodal Large Language Models (MLLMs) have excelled in 2D image-text\ncomprehension and image generation, but their understanding of the 3D world is\nnotably deficient, limiting progress in 3D language understanding and\ngeneration. To solve this problem, we introduce GPT4Point, an innovative\ngroundbreaking point-language multimodal model designed specifically for\nunified 3D object understanding and generation within the MLLM framework.\nGPT4Point as a powerful 3D MLLM seamlessly can execute a variety of point-text\nreference tasks such as point-cloud captioning and Q&A. Additionally, GPT4Point\nis equipped with advanced capabilities for controllable 3D generation, it can\nget high-quality results through a low-quality point-text feature maintaining\nthe geometric shapes and colors. To support the expansive needs of 3D\nobject-text pairs, we develop Pyramid-XL, a point-language dataset annotation\nengine. It constructs a large-scale database over 1M objects of varied text\ngranularity levels from the Objaverse-XL dataset, essential for training\nGPT4Point. A comprehensive benchmark has been proposed to evaluate 3D\npoint-language understanding capabilities. In extensive evaluations, GPT4Point\nhas demonstrated superior performance in understanding and generation.\n", "link": "http://arxiv.org/abs/2312.02980v2", "date": "2025-05-30", "relevancy": 2.7902, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5622}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5622}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GPT4Point%3A%20A%20Unified%20Framework%20for%20Point-Language%20Understanding%20and%0A%20%20Generation&body=Title%3A%20GPT4Point%3A%20A%20Unified%20Framework%20for%20Point-Language%20Understanding%20and%0A%20%20Generation%0AAuthor%3A%20Zhangyang%20Qi%20and%20Ye%20Fang%20and%20Zeyi%20Sun%20and%20Xiaoyang%20Wu%20and%20Tong%20Wu%20and%20Jiaqi%20Wang%20and%20Dahua%20Lin%20and%20Hengshuang%20Zhao%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20excelled%20in%202D%20image-text%0Acomprehension%20and%20image%20generation%2C%20but%20their%20understanding%20of%20the%203D%20world%20is%0Anotably%20deficient%2C%20limiting%20progress%20in%203D%20language%20understanding%20and%0Ageneration.%20To%20solve%20this%20problem%2C%20we%20introduce%20GPT4Point%2C%20an%20innovative%0Agroundbreaking%20point-language%20multimodal%20model%20designed%20specifically%20for%0Aunified%203D%20object%20understanding%20and%20generation%20within%20the%20MLLM%20framework.%0AGPT4Point%20as%20a%20powerful%203D%20MLLM%20seamlessly%20can%20execute%20a%20variety%20of%20point-text%0Areference%20tasks%20such%20as%20point-cloud%20captioning%20and%20Q%26A.%20Additionally%2C%20GPT4Point%0Ais%20equipped%20with%20advanced%20capabilities%20for%20controllable%203D%20generation%2C%20it%20can%0Aget%20high-quality%20results%20through%20a%20low-quality%20point-text%20feature%20maintaining%0Athe%20geometric%20shapes%20and%20colors.%20To%20support%20the%20expansive%20needs%20of%203D%0Aobject-text%20pairs%2C%20we%20develop%20Pyramid-XL%2C%20a%20point-language%20dataset%20annotation%0Aengine.%20It%20constructs%20a%20large-scale%20database%20over%201M%20objects%20of%20varied%20text%0Agranularity%20levels%20from%20the%20Objaverse-XL%20dataset%2C%20essential%20for%20training%0AGPT4Point.%20A%20comprehensive%20benchmark%20has%20been%20proposed%20to%20evaluate%203D%0Apoint-language%20understanding%20capabilities.%20In%20extensive%20evaluations%2C%20GPT4Point%0Ahas%20demonstrated%20superior%20performance%20in%20understanding%20and%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02980v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGPT4Point%253A%2520A%2520Unified%2520Framework%2520for%2520Point-Language%2520Understanding%2520and%250A%2520%2520Generation%26entry.906535625%3DZhangyang%2520Qi%2520and%2520Ye%2520Fang%2520and%2520Zeyi%2520Sun%2520and%2520Xiaoyang%2520Wu%2520and%2520Tong%2520Wu%2520and%2520Jiaqi%2520Wang%2520and%2520Dahua%2520Lin%2520and%2520Hengshuang%2520Zhao%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520excelled%2520in%25202D%2520image-text%250Acomprehension%2520and%2520image%2520generation%252C%2520but%2520their%2520understanding%2520of%2520the%25203D%2520world%2520is%250Anotably%2520deficient%252C%2520limiting%2520progress%2520in%25203D%2520language%2520understanding%2520and%250Ageneration.%2520To%2520solve%2520this%2520problem%252C%2520we%2520introduce%2520GPT4Point%252C%2520an%2520innovative%250Agroundbreaking%2520point-language%2520multimodal%2520model%2520designed%2520specifically%2520for%250Aunified%25203D%2520object%2520understanding%2520and%2520generation%2520within%2520the%2520MLLM%2520framework.%250AGPT4Point%2520as%2520a%2520powerful%25203D%2520MLLM%2520seamlessly%2520can%2520execute%2520a%2520variety%2520of%2520point-text%250Areference%2520tasks%2520such%2520as%2520point-cloud%2520captioning%2520and%2520Q%2526A.%2520Additionally%252C%2520GPT4Point%250Ais%2520equipped%2520with%2520advanced%2520capabilities%2520for%2520controllable%25203D%2520generation%252C%2520it%2520can%250Aget%2520high-quality%2520results%2520through%2520a%2520low-quality%2520point-text%2520feature%2520maintaining%250Athe%2520geometric%2520shapes%2520and%2520colors.%2520To%2520support%2520the%2520expansive%2520needs%2520of%25203D%250Aobject-text%2520pairs%252C%2520we%2520develop%2520Pyramid-XL%252C%2520a%2520point-language%2520dataset%2520annotation%250Aengine.%2520It%2520constructs%2520a%2520large-scale%2520database%2520over%25201M%2520objects%2520of%2520varied%2520text%250Agranularity%2520levels%2520from%2520the%2520Objaverse-XL%2520dataset%252C%2520essential%2520for%2520training%250AGPT4Point.%2520A%2520comprehensive%2520benchmark%2520has%2520been%2520proposed%2520to%2520evaluate%25203D%250Apoint-language%2520understanding%2520capabilities.%2520In%2520extensive%2520evaluations%252C%2520GPT4Point%250Ahas%2520demonstrated%2520superior%2520performance%2520in%2520understanding%2520and%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.02980v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPT4Point%3A%20A%20Unified%20Framework%20for%20Point-Language%20Understanding%20and%0A%20%20Generation&entry.906535625=Zhangyang%20Qi%20and%20Ye%20Fang%20and%20Zeyi%20Sun%20and%20Xiaoyang%20Wu%20and%20Tong%20Wu%20and%20Jiaqi%20Wang%20and%20Dahua%20Lin%20and%20Hengshuang%20Zhao&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20excelled%20in%202D%20image-text%0Acomprehension%20and%20image%20generation%2C%20but%20their%20understanding%20of%20the%203D%20world%20is%0Anotably%20deficient%2C%20limiting%20progress%20in%203D%20language%20understanding%20and%0Ageneration.%20To%20solve%20this%20problem%2C%20we%20introduce%20GPT4Point%2C%20an%20innovative%0Agroundbreaking%20point-language%20multimodal%20model%20designed%20specifically%20for%0Aunified%203D%20object%20understanding%20and%20generation%20within%20the%20MLLM%20framework.%0AGPT4Point%20as%20a%20powerful%203D%20MLLM%20seamlessly%20can%20execute%20a%20variety%20of%20point-text%0Areference%20tasks%20such%20as%20point-cloud%20captioning%20and%20Q%26A.%20Additionally%2C%20GPT4Point%0Ais%20equipped%20with%20advanced%20capabilities%20for%20controllable%203D%20generation%2C%20it%20can%0Aget%20high-quality%20results%20through%20a%20low-quality%20point-text%20feature%20maintaining%0Athe%20geometric%20shapes%20and%20colors.%20To%20support%20the%20expansive%20needs%20of%203D%0Aobject-text%20pairs%2C%20we%20develop%20Pyramid-XL%2C%20a%20point-language%20dataset%20annotation%0Aengine.%20It%20constructs%20a%20large-scale%20database%20over%201M%20objects%20of%20varied%20text%0Agranularity%20levels%20from%20the%20Objaverse-XL%20dataset%2C%20essential%20for%20training%0AGPT4Point.%20A%20comprehensive%20benchmark%20has%20been%20proposed%20to%20evaluate%203D%0Apoint-language%20understanding%20capabilities.%20In%20extensive%20evaluations%2C%20GPT4Point%0Ahas%20demonstrated%20superior%20performance%20in%20understanding%20and%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02980v2&entry.124074799=Read"},
{"title": "A Cross Branch Fusion-Based Contrastive Learning Framework for Point\n  Cloud Self-supervised Learning", "author": "Chengzhi Wu and Qianliang Huang and Kun Jin and Julius Pfrommer and J\u00fcrgen Beyerer", "abstract": "  Contrastive learning is an essential method in self-supervised learning. It\nprimarily employs a multi-branch strategy to compare latent representations\nobtained from different branches and train the encoder. In the case of\nmulti-modal input, diverse modalities of the same object are fed into distinct\nbranches. When using single-modal data, the same input undergoes various\naugmentations before being fed into different branches. However, all existing\ncontrastive learning frameworks have so far only performed contrastive\noperations on the learned features at the final loss end, with no information\nexchange between different branches prior to this stage. In this paper, for\npoint cloud unsupervised learning without the use of extra training data, we\npropose a Contrastive Cross-branch Attention-based framework for Point cloud\ndata (termed PoCCA), to learn rich 3D point cloud representations. By\nintroducing sub-branches, PoCCA allows information exchange between different\nbranches before the loss end. Experimental results demonstrate that in the case\nof using no extra training data, the representations learned with our\nself-supervised model achieve state-of-the-art performances when used for\ndownstream tasks on point clouds.\n", "link": "http://arxiv.org/abs/2505.24641v1", "date": "2025-05-30", "relevancy": 2.7826, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5824}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5466}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5406}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Cross%20Branch%20Fusion-Based%20Contrastive%20Learning%20Framework%20for%20Point%0A%20%20Cloud%20Self-supervised%20Learning&body=Title%3A%20A%20Cross%20Branch%20Fusion-Based%20Contrastive%20Learning%20Framework%20for%20Point%0A%20%20Cloud%20Self-supervised%20Learning%0AAuthor%3A%20Chengzhi%20Wu%20and%20Qianliang%20Huang%20and%20Kun%20Jin%20and%20Julius%20Pfrommer%20and%20J%C3%BCrgen%20Beyerer%0AAbstract%3A%20%20%20Contrastive%20learning%20is%20an%20essential%20method%20in%20self-supervised%20learning.%20It%0Aprimarily%20employs%20a%20multi-branch%20strategy%20to%20compare%20latent%20representations%0Aobtained%20from%20different%20branches%20and%20train%20the%20encoder.%20In%20the%20case%20of%0Amulti-modal%20input%2C%20diverse%20modalities%20of%20the%20same%20object%20are%20fed%20into%20distinct%0Abranches.%20When%20using%20single-modal%20data%2C%20the%20same%20input%20undergoes%20various%0Aaugmentations%20before%20being%20fed%20into%20different%20branches.%20However%2C%20all%20existing%0Acontrastive%20learning%20frameworks%20have%20so%20far%20only%20performed%20contrastive%0Aoperations%20on%20the%20learned%20features%20at%20the%20final%20loss%20end%2C%20with%20no%20information%0Aexchange%20between%20different%20branches%20prior%20to%20this%20stage.%20In%20this%20paper%2C%20for%0Apoint%20cloud%20unsupervised%20learning%20without%20the%20use%20of%20extra%20training%20data%2C%20we%0Apropose%20a%20Contrastive%20Cross-branch%20Attention-based%20framework%20for%20Point%20cloud%0Adata%20%28termed%20PoCCA%29%2C%20to%20learn%20rich%203D%20point%20cloud%20representations.%20By%0Aintroducing%20sub-branches%2C%20PoCCA%20allows%20information%20exchange%20between%20different%0Abranches%20before%20the%20loss%20end.%20Experimental%20results%20demonstrate%20that%20in%20the%20case%0Aof%20using%20no%20extra%20training%20data%2C%20the%20representations%20learned%20with%20our%0Aself-supervised%20model%20achieve%20state-of-the-art%20performances%20when%20used%20for%0Adownstream%20tasks%20on%20point%20clouds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24641v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Cross%2520Branch%2520Fusion-Based%2520Contrastive%2520Learning%2520Framework%2520for%2520Point%250A%2520%2520Cloud%2520Self-supervised%2520Learning%26entry.906535625%3DChengzhi%2520Wu%2520and%2520Qianliang%2520Huang%2520and%2520Kun%2520Jin%2520and%2520Julius%2520Pfrommer%2520and%2520J%25C3%25BCrgen%2520Beyerer%26entry.1292438233%3D%2520%2520Contrastive%2520learning%2520is%2520an%2520essential%2520method%2520in%2520self-supervised%2520learning.%2520It%250Aprimarily%2520employs%2520a%2520multi-branch%2520strategy%2520to%2520compare%2520latent%2520representations%250Aobtained%2520from%2520different%2520branches%2520and%2520train%2520the%2520encoder.%2520In%2520the%2520case%2520of%250Amulti-modal%2520input%252C%2520diverse%2520modalities%2520of%2520the%2520same%2520object%2520are%2520fed%2520into%2520distinct%250Abranches.%2520When%2520using%2520single-modal%2520data%252C%2520the%2520same%2520input%2520undergoes%2520various%250Aaugmentations%2520before%2520being%2520fed%2520into%2520different%2520branches.%2520However%252C%2520all%2520existing%250Acontrastive%2520learning%2520frameworks%2520have%2520so%2520far%2520only%2520performed%2520contrastive%250Aoperations%2520on%2520the%2520learned%2520features%2520at%2520the%2520final%2520loss%2520end%252C%2520with%2520no%2520information%250Aexchange%2520between%2520different%2520branches%2520prior%2520to%2520this%2520stage.%2520In%2520this%2520paper%252C%2520for%250Apoint%2520cloud%2520unsupervised%2520learning%2520without%2520the%2520use%2520of%2520extra%2520training%2520data%252C%2520we%250Apropose%2520a%2520Contrastive%2520Cross-branch%2520Attention-based%2520framework%2520for%2520Point%2520cloud%250Adata%2520%2528termed%2520PoCCA%2529%252C%2520to%2520learn%2520rich%25203D%2520point%2520cloud%2520representations.%2520By%250Aintroducing%2520sub-branches%252C%2520PoCCA%2520allows%2520information%2520exchange%2520between%2520different%250Abranches%2520before%2520the%2520loss%2520end.%2520Experimental%2520results%2520demonstrate%2520that%2520in%2520the%2520case%250Aof%2520using%2520no%2520extra%2520training%2520data%252C%2520the%2520representations%2520learned%2520with%2520our%250Aself-supervised%2520model%2520achieve%2520state-of-the-art%2520performances%2520when%2520used%2520for%250Adownstream%2520tasks%2520on%2520point%2520clouds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24641v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Cross%20Branch%20Fusion-Based%20Contrastive%20Learning%20Framework%20for%20Point%0A%20%20Cloud%20Self-supervised%20Learning&entry.906535625=Chengzhi%20Wu%20and%20Qianliang%20Huang%20and%20Kun%20Jin%20and%20Julius%20Pfrommer%20and%20J%C3%BCrgen%20Beyerer&entry.1292438233=%20%20Contrastive%20learning%20is%20an%20essential%20method%20in%20self-supervised%20learning.%20It%0Aprimarily%20employs%20a%20multi-branch%20strategy%20to%20compare%20latent%20representations%0Aobtained%20from%20different%20branches%20and%20train%20the%20encoder.%20In%20the%20case%20of%0Amulti-modal%20input%2C%20diverse%20modalities%20of%20the%20same%20object%20are%20fed%20into%20distinct%0Abranches.%20When%20using%20single-modal%20data%2C%20the%20same%20input%20undergoes%20various%0Aaugmentations%20before%20being%20fed%20into%20different%20branches.%20However%2C%20all%20existing%0Acontrastive%20learning%20frameworks%20have%20so%20far%20only%20performed%20contrastive%0Aoperations%20on%20the%20learned%20features%20at%20the%20final%20loss%20end%2C%20with%20no%20information%0Aexchange%20between%20different%20branches%20prior%20to%20this%20stage.%20In%20this%20paper%2C%20for%0Apoint%20cloud%20unsupervised%20learning%20without%20the%20use%20of%20extra%20training%20data%2C%20we%0Apropose%20a%20Contrastive%20Cross-branch%20Attention-based%20framework%20for%20Point%20cloud%0Adata%20%28termed%20PoCCA%29%2C%20to%20learn%20rich%203D%20point%20cloud%20representations.%20By%0Aintroducing%20sub-branches%2C%20PoCCA%20allows%20information%20exchange%20between%20different%0Abranches%20before%20the%20loss%20end.%20Experimental%20results%20demonstrate%20that%20in%20the%20case%0Aof%20using%20no%20extra%20training%20data%2C%20the%20representations%20learned%20with%20our%0Aself-supervised%20model%20achieve%20state-of-the-art%20performances%20when%20used%20for%0Adownstream%20tasks%20on%20point%20clouds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24641v1&entry.124074799=Read"},
{"title": "SAMBLE: Shape-Specific Point Cloud Sampling for an Optimal Trade-Off\n  Between Local Detail and Global Uniformity", "author": "Chengzhi Wu and Yuxin Wan and Hao Fu and Julius Pfrommer and Zeyun Zhong and Junwei Zheng and Jiaming Zhang and J\u00fcrgen Beyerer", "abstract": "  Driven by the increasing demand for accurate and efficient representation of\n3D data in various domains, point cloud sampling has emerged as a pivotal\nresearch topic in 3D computer vision. Recently, learning-to-sample methods have\ngarnered growing interest from the community, particularly for their ability to\nbe jointly trained with downstream tasks. However, previous learning-based\nsampling methods either lead to unrecognizable sampling patterns by generating\na new point cloud or biased sampled results by focusing excessively on sharp\nedge details. Moreover, they all overlook the natural variations in point\ndistribution across different shapes, applying a similar sampling strategy to\nall point clouds. In this paper, we propose a Sparse Attention Map and\nBin-based Learning method (termed SAMBLE) to learn shape-specific sampling\nstrategies for point cloud shapes. SAMBLE effectively achieves an improved\nbalance between sampling edge points for local details and preserving\nuniformity in the global shape, resulting in superior performance across\nmultiple common point cloud downstream tasks, even in scenarios with few-point\nsampling.\n", "link": "http://arxiv.org/abs/2504.19581v2", "date": "2025-05-30", "relevancy": 2.7767, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5751}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5459}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAMBLE%3A%20Shape-Specific%20Point%20Cloud%20Sampling%20for%20an%20Optimal%20Trade-Off%0A%20%20Between%20Local%20Detail%20and%20Global%20Uniformity&body=Title%3A%20SAMBLE%3A%20Shape-Specific%20Point%20Cloud%20Sampling%20for%20an%20Optimal%20Trade-Off%0A%20%20Between%20Local%20Detail%20and%20Global%20Uniformity%0AAuthor%3A%20Chengzhi%20Wu%20and%20Yuxin%20Wan%20and%20Hao%20Fu%20and%20Julius%20Pfrommer%20and%20Zeyun%20Zhong%20and%20Junwei%20Zheng%20and%20Jiaming%20Zhang%20and%20J%C3%BCrgen%20Beyerer%0AAbstract%3A%20%20%20Driven%20by%20the%20increasing%20demand%20for%20accurate%20and%20efficient%20representation%20of%0A3D%20data%20in%20various%20domains%2C%20point%20cloud%20sampling%20has%20emerged%20as%20a%20pivotal%0Aresearch%20topic%20in%203D%20computer%20vision.%20Recently%2C%20learning-to-sample%20methods%20have%0Agarnered%20growing%20interest%20from%20the%20community%2C%20particularly%20for%20their%20ability%20to%0Abe%20jointly%20trained%20with%20downstream%20tasks.%20However%2C%20previous%20learning-based%0Asampling%20methods%20either%20lead%20to%20unrecognizable%20sampling%20patterns%20by%20generating%0Aa%20new%20point%20cloud%20or%20biased%20sampled%20results%20by%20focusing%20excessively%20on%20sharp%0Aedge%20details.%20Moreover%2C%20they%20all%20overlook%20the%20natural%20variations%20in%20point%0Adistribution%20across%20different%20shapes%2C%20applying%20a%20similar%20sampling%20strategy%20to%0Aall%20point%20clouds.%20In%20this%20paper%2C%20we%20propose%20a%20Sparse%20Attention%20Map%20and%0ABin-based%20Learning%20method%20%28termed%20SAMBLE%29%20to%20learn%20shape-specific%20sampling%0Astrategies%20for%20point%20cloud%20shapes.%20SAMBLE%20effectively%20achieves%20an%20improved%0Abalance%20between%20sampling%20edge%20points%20for%20local%20details%20and%20preserving%0Auniformity%20in%20the%20global%20shape%2C%20resulting%20in%20superior%20performance%20across%0Amultiple%20common%20point%20cloud%20downstream%20tasks%2C%20even%20in%20scenarios%20with%20few-point%0Asampling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19581v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAMBLE%253A%2520Shape-Specific%2520Point%2520Cloud%2520Sampling%2520for%2520an%2520Optimal%2520Trade-Off%250A%2520%2520Between%2520Local%2520Detail%2520and%2520Global%2520Uniformity%26entry.906535625%3DChengzhi%2520Wu%2520and%2520Yuxin%2520Wan%2520and%2520Hao%2520Fu%2520and%2520Julius%2520Pfrommer%2520and%2520Zeyun%2520Zhong%2520and%2520Junwei%2520Zheng%2520and%2520Jiaming%2520Zhang%2520and%2520J%25C3%25BCrgen%2520Beyerer%26entry.1292438233%3D%2520%2520Driven%2520by%2520the%2520increasing%2520demand%2520for%2520accurate%2520and%2520efficient%2520representation%2520of%250A3D%2520data%2520in%2520various%2520domains%252C%2520point%2520cloud%2520sampling%2520has%2520emerged%2520as%2520a%2520pivotal%250Aresearch%2520topic%2520in%25203D%2520computer%2520vision.%2520Recently%252C%2520learning-to-sample%2520methods%2520have%250Agarnered%2520growing%2520interest%2520from%2520the%2520community%252C%2520particularly%2520for%2520their%2520ability%2520to%250Abe%2520jointly%2520trained%2520with%2520downstream%2520tasks.%2520However%252C%2520previous%2520learning-based%250Asampling%2520methods%2520either%2520lead%2520to%2520unrecognizable%2520sampling%2520patterns%2520by%2520generating%250Aa%2520new%2520point%2520cloud%2520or%2520biased%2520sampled%2520results%2520by%2520focusing%2520excessively%2520on%2520sharp%250Aedge%2520details.%2520Moreover%252C%2520they%2520all%2520overlook%2520the%2520natural%2520variations%2520in%2520point%250Adistribution%2520across%2520different%2520shapes%252C%2520applying%2520a%2520similar%2520sampling%2520strategy%2520to%250Aall%2520point%2520clouds.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520Sparse%2520Attention%2520Map%2520and%250ABin-based%2520Learning%2520method%2520%2528termed%2520SAMBLE%2529%2520to%2520learn%2520shape-specific%2520sampling%250Astrategies%2520for%2520point%2520cloud%2520shapes.%2520SAMBLE%2520effectively%2520achieves%2520an%2520improved%250Abalance%2520between%2520sampling%2520edge%2520points%2520for%2520local%2520details%2520and%2520preserving%250Auniformity%2520in%2520the%2520global%2520shape%252C%2520resulting%2520in%2520superior%2520performance%2520across%250Amultiple%2520common%2520point%2520cloud%2520downstream%2520tasks%252C%2520even%2520in%2520scenarios%2520with%2520few-point%250Asampling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19581v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAMBLE%3A%20Shape-Specific%20Point%20Cloud%20Sampling%20for%20an%20Optimal%20Trade-Off%0A%20%20Between%20Local%20Detail%20and%20Global%20Uniformity&entry.906535625=Chengzhi%20Wu%20and%20Yuxin%20Wan%20and%20Hao%20Fu%20and%20Julius%20Pfrommer%20and%20Zeyun%20Zhong%20and%20Junwei%20Zheng%20and%20Jiaming%20Zhang%20and%20J%C3%BCrgen%20Beyerer&entry.1292438233=%20%20Driven%20by%20the%20increasing%20demand%20for%20accurate%20and%20efficient%20representation%20of%0A3D%20data%20in%20various%20domains%2C%20point%20cloud%20sampling%20has%20emerged%20as%20a%20pivotal%0Aresearch%20topic%20in%203D%20computer%20vision.%20Recently%2C%20learning-to-sample%20methods%20have%0Agarnered%20growing%20interest%20from%20the%20community%2C%20particularly%20for%20their%20ability%20to%0Abe%20jointly%20trained%20with%20downstream%20tasks.%20However%2C%20previous%20learning-based%0Asampling%20methods%20either%20lead%20to%20unrecognizable%20sampling%20patterns%20by%20generating%0Aa%20new%20point%20cloud%20or%20biased%20sampled%20results%20by%20focusing%20excessively%20on%20sharp%0Aedge%20details.%20Moreover%2C%20they%20all%20overlook%20the%20natural%20variations%20in%20point%0Adistribution%20across%20different%20shapes%2C%20applying%20a%20similar%20sampling%20strategy%20to%0Aall%20point%20clouds.%20In%20this%20paper%2C%20we%20propose%20a%20Sparse%20Attention%20Map%20and%0ABin-based%20Learning%20method%20%28termed%20SAMBLE%29%20to%20learn%20shape-specific%20sampling%0Astrategies%20for%20point%20cloud%20shapes.%20SAMBLE%20effectively%20achieves%20an%20improved%0Abalance%20between%20sampling%20edge%20points%20for%20local%20details%20and%20preserving%0Auniformity%20in%20the%20global%20shape%2C%20resulting%20in%20superior%20performance%20across%0Amultiple%20common%20point%20cloud%20downstream%20tasks%2C%20even%20in%20scenarios%20with%20few-point%0Asampling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19581v2&entry.124074799=Read"},
{"title": "VITA: Towards Open-Source Interactive Omni Multimodal LLM", "author": "Chaoyou Fu and Haojia Lin and Zuwei Long and Yunhang Shen and Yuhang Dai and Meng Zhao and Yi-Fan Zhang and Shaoqi Dong and Yangze Li and Xiong Wang and Haoyu Cao and Di Yin and Long Ma and Xiawu Zheng and Rongrong Ji and Yunsheng Wu and Ran He and Caifeng Shan and Xing Sun", "abstract": "  The remarkable multimodal capabilities and interactive experience of GPT-4o\nunderscore their necessity in practical applications, yet open-source models\nrarely excel in both areas. In this paper, we introduce VITA, the first-ever\nopen-source Multimodal Large Language Model (MLLM) adept at simultaneous\nprocessing and analysis of Video, Image, Text, and Audio modalities, and\nmeanwhile has an advanced multimodal interactive experience. Starting from\nMixtral 8x7B as a language foundation, we expand its Chinese vocabulary\nfollowed by bilingual instruction tuning. We further endow the language model\nwith visual and audio capabilities through two-stage multi-task learning of\nmultimodal alignment and instruction tuning. VITA demonstrates robust\nfoundational capabilities of multilingual, vision, and audio understanding, as\nevidenced by its strong performance across a range of both unimodal and\nmultimodal benchmarks. Beyond foundational capabilities, we have made\nconsiderable progress in enhancing the natural multimodal human-computer\ninteraction experience. VITA is the first step for the open-source community to\nexplore the seamless integration of multimodal understanding and interaction.\nWhile there is still lots of work to be done on VITA to get close to\nclose-source counterparts, we hope that its role as a pioneer can serve as a\ncornerstone for subsequent research. Project Page: https://vita-home.github.io.\n", "link": "http://arxiv.org/abs/2408.05211v3", "date": "2025-05-30", "relevancy": 2.7719, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5761}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5447}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VITA%3A%20Towards%20Open-Source%20Interactive%20Omni%20Multimodal%20LLM&body=Title%3A%20VITA%3A%20Towards%20Open-Source%20Interactive%20Omni%20Multimodal%20LLM%0AAuthor%3A%20Chaoyou%20Fu%20and%20Haojia%20Lin%20and%20Zuwei%20Long%20and%20Yunhang%20Shen%20and%20Yuhang%20Dai%20and%20Meng%20Zhao%20and%20Yi-Fan%20Zhang%20and%20Shaoqi%20Dong%20and%20Yangze%20Li%20and%20Xiong%20Wang%20and%20Haoyu%20Cao%20and%20Di%20Yin%20and%20Long%20Ma%20and%20Xiawu%20Zheng%20and%20Rongrong%20Ji%20and%20Yunsheng%20Wu%20and%20Ran%20He%20and%20Caifeng%20Shan%20and%20Xing%20Sun%0AAbstract%3A%20%20%20The%20remarkable%20multimodal%20capabilities%20and%20interactive%20experience%20of%20GPT-4o%0Aunderscore%20their%20necessity%20in%20practical%20applications%2C%20yet%20open-source%20models%0Ararely%20excel%20in%20both%20areas.%20In%20this%20paper%2C%20we%20introduce%20VITA%2C%20the%20first-ever%0Aopen-source%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20adept%20at%20simultaneous%0Aprocessing%20and%20analysis%20of%20Video%2C%20Image%2C%20Text%2C%20and%20Audio%20modalities%2C%20and%0Ameanwhile%20has%20an%20advanced%20multimodal%20interactive%20experience.%20Starting%20from%0AMixtral%208x7B%20as%20a%20language%20foundation%2C%20we%20expand%20its%20Chinese%20vocabulary%0Afollowed%20by%20bilingual%20instruction%20tuning.%20We%20further%20endow%20the%20language%20model%0Awith%20visual%20and%20audio%20capabilities%20through%20two-stage%20multi-task%20learning%20of%0Amultimodal%20alignment%20and%20instruction%20tuning.%20VITA%20demonstrates%20robust%0Afoundational%20capabilities%20of%20multilingual%2C%20vision%2C%20and%20audio%20understanding%2C%20as%0Aevidenced%20by%20its%20strong%20performance%20across%20a%20range%20of%20both%20unimodal%20and%0Amultimodal%20benchmarks.%20Beyond%20foundational%20capabilities%2C%20we%20have%20made%0Aconsiderable%20progress%20in%20enhancing%20the%20natural%20multimodal%20human-computer%0Ainteraction%20experience.%20VITA%20is%20the%20first%20step%20for%20the%20open-source%20community%20to%0Aexplore%20the%20seamless%20integration%20of%20multimodal%20understanding%20and%20interaction.%0AWhile%20there%20is%20still%20lots%20of%20work%20to%20be%20done%20on%20VITA%20to%20get%20close%20to%0Aclose-source%20counterparts%2C%20we%20hope%20that%20its%20role%20as%20a%20pioneer%20can%20serve%20as%20a%0Acornerstone%20for%20subsequent%20research.%20Project%20Page%3A%20https%3A//vita-home.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05211v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVITA%253A%2520Towards%2520Open-Source%2520Interactive%2520Omni%2520Multimodal%2520LLM%26entry.906535625%3DChaoyou%2520Fu%2520and%2520Haojia%2520Lin%2520and%2520Zuwei%2520Long%2520and%2520Yunhang%2520Shen%2520and%2520Yuhang%2520Dai%2520and%2520Meng%2520Zhao%2520and%2520Yi-Fan%2520Zhang%2520and%2520Shaoqi%2520Dong%2520and%2520Yangze%2520Li%2520and%2520Xiong%2520Wang%2520and%2520Haoyu%2520Cao%2520and%2520Di%2520Yin%2520and%2520Long%2520Ma%2520and%2520Xiawu%2520Zheng%2520and%2520Rongrong%2520Ji%2520and%2520Yunsheng%2520Wu%2520and%2520Ran%2520He%2520and%2520Caifeng%2520Shan%2520and%2520Xing%2520Sun%26entry.1292438233%3D%2520%2520The%2520remarkable%2520multimodal%2520capabilities%2520and%2520interactive%2520experience%2520of%2520GPT-4o%250Aunderscore%2520their%2520necessity%2520in%2520practical%2520applications%252C%2520yet%2520open-source%2520models%250Ararely%2520excel%2520in%2520both%2520areas.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520VITA%252C%2520the%2520first-ever%250Aopen-source%2520Multimodal%2520Large%2520Language%2520Model%2520%2528MLLM%2529%2520adept%2520at%2520simultaneous%250Aprocessing%2520and%2520analysis%2520of%2520Video%252C%2520Image%252C%2520Text%252C%2520and%2520Audio%2520modalities%252C%2520and%250Ameanwhile%2520has%2520an%2520advanced%2520multimodal%2520interactive%2520experience.%2520Starting%2520from%250AMixtral%25208x7B%2520as%2520a%2520language%2520foundation%252C%2520we%2520expand%2520its%2520Chinese%2520vocabulary%250Afollowed%2520by%2520bilingual%2520instruction%2520tuning.%2520We%2520further%2520endow%2520the%2520language%2520model%250Awith%2520visual%2520and%2520audio%2520capabilities%2520through%2520two-stage%2520multi-task%2520learning%2520of%250Amultimodal%2520alignment%2520and%2520instruction%2520tuning.%2520VITA%2520demonstrates%2520robust%250Afoundational%2520capabilities%2520of%2520multilingual%252C%2520vision%252C%2520and%2520audio%2520understanding%252C%2520as%250Aevidenced%2520by%2520its%2520strong%2520performance%2520across%2520a%2520range%2520of%2520both%2520unimodal%2520and%250Amultimodal%2520benchmarks.%2520Beyond%2520foundational%2520capabilities%252C%2520we%2520have%2520made%250Aconsiderable%2520progress%2520in%2520enhancing%2520the%2520natural%2520multimodal%2520human-computer%250Ainteraction%2520experience.%2520VITA%2520is%2520the%2520first%2520step%2520for%2520the%2520open-source%2520community%2520to%250Aexplore%2520the%2520seamless%2520integration%2520of%2520multimodal%2520understanding%2520and%2520interaction.%250AWhile%2520there%2520is%2520still%2520lots%2520of%2520work%2520to%2520be%2520done%2520on%2520VITA%2520to%2520get%2520close%2520to%250Aclose-source%2520counterparts%252C%2520we%2520hope%2520that%2520its%2520role%2520as%2520a%2520pioneer%2520can%2520serve%2520as%2520a%250Acornerstone%2520for%2520subsequent%2520research.%2520Project%2520Page%253A%2520https%253A//vita-home.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05211v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VITA%3A%20Towards%20Open-Source%20Interactive%20Omni%20Multimodal%20LLM&entry.906535625=Chaoyou%20Fu%20and%20Haojia%20Lin%20and%20Zuwei%20Long%20and%20Yunhang%20Shen%20and%20Yuhang%20Dai%20and%20Meng%20Zhao%20and%20Yi-Fan%20Zhang%20and%20Shaoqi%20Dong%20and%20Yangze%20Li%20and%20Xiong%20Wang%20and%20Haoyu%20Cao%20and%20Di%20Yin%20and%20Long%20Ma%20and%20Xiawu%20Zheng%20and%20Rongrong%20Ji%20and%20Yunsheng%20Wu%20and%20Ran%20He%20and%20Caifeng%20Shan%20and%20Xing%20Sun&entry.1292438233=%20%20The%20remarkable%20multimodal%20capabilities%20and%20interactive%20experience%20of%20GPT-4o%0Aunderscore%20their%20necessity%20in%20practical%20applications%2C%20yet%20open-source%20models%0Ararely%20excel%20in%20both%20areas.%20In%20this%20paper%2C%20we%20introduce%20VITA%2C%20the%20first-ever%0Aopen-source%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20adept%20at%20simultaneous%0Aprocessing%20and%20analysis%20of%20Video%2C%20Image%2C%20Text%2C%20and%20Audio%20modalities%2C%20and%0Ameanwhile%20has%20an%20advanced%20multimodal%20interactive%20experience.%20Starting%20from%0AMixtral%208x7B%20as%20a%20language%20foundation%2C%20we%20expand%20its%20Chinese%20vocabulary%0Afollowed%20by%20bilingual%20instruction%20tuning.%20We%20further%20endow%20the%20language%20model%0Awith%20visual%20and%20audio%20capabilities%20through%20two-stage%20multi-task%20learning%20of%0Amultimodal%20alignment%20and%20instruction%20tuning.%20VITA%20demonstrates%20robust%0Afoundational%20capabilities%20of%20multilingual%2C%20vision%2C%20and%20audio%20understanding%2C%20as%0Aevidenced%20by%20its%20strong%20performance%20across%20a%20range%20of%20both%20unimodal%20and%0Amultimodal%20benchmarks.%20Beyond%20foundational%20capabilities%2C%20we%20have%20made%0Aconsiderable%20progress%20in%20enhancing%20the%20natural%20multimodal%20human-computer%0Ainteraction%20experience.%20VITA%20is%20the%20first%20step%20for%20the%20open-source%20community%20to%0Aexplore%20the%20seamless%20integration%20of%20multimodal%20understanding%20and%20interaction.%0AWhile%20there%20is%20still%20lots%20of%20work%20to%20be%20done%20on%20VITA%20to%20get%20close%20to%0Aclose-source%20counterparts%2C%20we%20hope%20that%20its%20role%20as%20a%20pioneer%20can%20serve%20as%20a%0Acornerstone%20for%20subsequent%20research.%20Project%20Page%3A%20https%3A//vita-home.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05211v3&entry.124074799=Read"},
{"title": "Are MLMs Trapped in the Visual Room?", "author": "Yazhou Zhang and Chunwang Zou and Qimeng Liu and Lu Rong and Ben Yao and Zheng Lian and Qiuchi Li and Peng Zhang and Jing Qin", "abstract": "  Can multi-modal large models (MLMs) that can ``see'' an image be said to\n``understand'' it? Drawing inspiration from Searle's Chinese Room, we propose\nthe \\textbf{Visual Room} argument: a system may process and describe every\ndetail of visual inputs by following algorithmic rules, without genuinely\ncomprehending the underlying intention. This dilemma challenges the prevailing\nassumption that perceptual mastery implies genuine understanding. In\nimplementation, we introduce a two-tier evaluation framework spanning\nperception and cognition. The perception component evaluates whether MLMs can\naccurately capture the surface-level details of visual contents, where the\ncognitive component examines their ability to infer sarcasm polarity. To\nsupport this framework, We further introduce a high-quality multi-modal sarcasm\ndataset comprising both 924 static images and 100 dynamic videos. All sarcasm\nlabels are annotated by the original authors and verified by independent\nreviewers to ensure clarity and consistency. We evaluate eight state-of-the-art\n(SoTA) MLMs. Our results highlight three key findings: (1) MLMs demonstrate\nhigh accuracy in visual perception; (2) even with correct perception, MLMs\nexhibit an average error rate of ~17.1\\% in sarcasm understanding, revealing a\nsignificant gap between seeing and understanding; (3) this gap stems from\nweaknesses in context integration, emotional reasoning, and pragmatic\ninference. This work provides empirical grounding for the proposed Visual Room\nargument and offers a new evaluation paradigm for MLMs.\n", "link": "http://arxiv.org/abs/2505.23272v2", "date": "2025-05-30", "relevancy": 2.7508, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5635}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5635}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5235}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20MLMs%20Trapped%20in%20the%20Visual%20Room%3F&body=Title%3A%20Are%20MLMs%20Trapped%20in%20the%20Visual%20Room%3F%0AAuthor%3A%20Yazhou%20Zhang%20and%20Chunwang%20Zou%20and%20Qimeng%20Liu%20and%20Lu%20Rong%20and%20Ben%20Yao%20and%20Zheng%20Lian%20and%20Qiuchi%20Li%20and%20Peng%20Zhang%20and%20Jing%20Qin%0AAbstract%3A%20%20%20Can%20multi-modal%20large%20models%20%28MLMs%29%20that%20can%20%60%60see%27%27%20an%20image%20be%20said%20to%0A%60%60understand%27%27%20it%3F%20Drawing%20inspiration%20from%20Searle%27s%20Chinese%20Room%2C%20we%20propose%0Athe%20%5Ctextbf%7BVisual%20Room%7D%20argument%3A%20a%20system%20may%20process%20and%20describe%20every%0Adetail%20of%20visual%20inputs%20by%20following%20algorithmic%20rules%2C%20without%20genuinely%0Acomprehending%20the%20underlying%20intention.%20This%20dilemma%20challenges%20the%20prevailing%0Aassumption%20that%20perceptual%20mastery%20implies%20genuine%20understanding.%20In%0Aimplementation%2C%20we%20introduce%20a%20two-tier%20evaluation%20framework%20spanning%0Aperception%20and%20cognition.%20The%20perception%20component%20evaluates%20whether%20MLMs%20can%0Aaccurately%20capture%20the%20surface-level%20details%20of%20visual%20contents%2C%20where%20the%0Acognitive%20component%20examines%20their%20ability%20to%20infer%20sarcasm%20polarity.%20To%0Asupport%20this%20framework%2C%20We%20further%20introduce%20a%20high-quality%20multi-modal%20sarcasm%0Adataset%20comprising%20both%20924%20static%20images%20and%20100%20dynamic%20videos.%20All%20sarcasm%0Alabels%20are%20annotated%20by%20the%20original%20authors%20and%20verified%20by%20independent%0Areviewers%20to%20ensure%20clarity%20and%20consistency.%20We%20evaluate%20eight%20state-of-the-art%0A%28SoTA%29%20MLMs.%20Our%20results%20highlight%20three%20key%20findings%3A%20%281%29%20MLMs%20demonstrate%0Ahigh%20accuracy%20in%20visual%20perception%3B%20%282%29%20even%20with%20correct%20perception%2C%20MLMs%0Aexhibit%20an%20average%20error%20rate%20of%20~17.1%5C%25%20in%20sarcasm%20understanding%2C%20revealing%20a%0Asignificant%20gap%20between%20seeing%20and%20understanding%3B%20%283%29%20this%20gap%20stems%20from%0Aweaknesses%20in%20context%20integration%2C%20emotional%20reasoning%2C%20and%20pragmatic%0Ainference.%20This%20work%20provides%20empirical%20grounding%20for%20the%20proposed%20Visual%20Room%0Aargument%20and%20offers%20a%20new%20evaluation%20paradigm%20for%20MLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23272v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520MLMs%2520Trapped%2520in%2520the%2520Visual%2520Room%253F%26entry.906535625%3DYazhou%2520Zhang%2520and%2520Chunwang%2520Zou%2520and%2520Qimeng%2520Liu%2520and%2520Lu%2520Rong%2520and%2520Ben%2520Yao%2520and%2520Zheng%2520Lian%2520and%2520Qiuchi%2520Li%2520and%2520Peng%2520Zhang%2520and%2520Jing%2520Qin%26entry.1292438233%3D%2520%2520Can%2520multi-modal%2520large%2520models%2520%2528MLMs%2529%2520that%2520can%2520%2560%2560see%2527%2527%2520an%2520image%2520be%2520said%2520to%250A%2560%2560understand%2527%2527%2520it%253F%2520Drawing%2520inspiration%2520from%2520Searle%2527s%2520Chinese%2520Room%252C%2520we%2520propose%250Athe%2520%255Ctextbf%257BVisual%2520Room%257D%2520argument%253A%2520a%2520system%2520may%2520process%2520and%2520describe%2520every%250Adetail%2520of%2520visual%2520inputs%2520by%2520following%2520algorithmic%2520rules%252C%2520without%2520genuinely%250Acomprehending%2520the%2520underlying%2520intention.%2520This%2520dilemma%2520challenges%2520the%2520prevailing%250Aassumption%2520that%2520perceptual%2520mastery%2520implies%2520genuine%2520understanding.%2520In%250Aimplementation%252C%2520we%2520introduce%2520a%2520two-tier%2520evaluation%2520framework%2520spanning%250Aperception%2520and%2520cognition.%2520The%2520perception%2520component%2520evaluates%2520whether%2520MLMs%2520can%250Aaccurately%2520capture%2520the%2520surface-level%2520details%2520of%2520visual%2520contents%252C%2520where%2520the%250Acognitive%2520component%2520examines%2520their%2520ability%2520to%2520infer%2520sarcasm%2520polarity.%2520To%250Asupport%2520this%2520framework%252C%2520We%2520further%2520introduce%2520a%2520high-quality%2520multi-modal%2520sarcasm%250Adataset%2520comprising%2520both%2520924%2520static%2520images%2520and%2520100%2520dynamic%2520videos.%2520All%2520sarcasm%250Alabels%2520are%2520annotated%2520by%2520the%2520original%2520authors%2520and%2520verified%2520by%2520independent%250Areviewers%2520to%2520ensure%2520clarity%2520and%2520consistency.%2520We%2520evaluate%2520eight%2520state-of-the-art%250A%2528SoTA%2529%2520MLMs.%2520Our%2520results%2520highlight%2520three%2520key%2520findings%253A%2520%25281%2529%2520MLMs%2520demonstrate%250Ahigh%2520accuracy%2520in%2520visual%2520perception%253B%2520%25282%2529%2520even%2520with%2520correct%2520perception%252C%2520MLMs%250Aexhibit%2520an%2520average%2520error%2520rate%2520of%2520~17.1%255C%2525%2520in%2520sarcasm%2520understanding%252C%2520revealing%2520a%250Asignificant%2520gap%2520between%2520seeing%2520and%2520understanding%253B%2520%25283%2529%2520this%2520gap%2520stems%2520from%250Aweaknesses%2520in%2520context%2520integration%252C%2520emotional%2520reasoning%252C%2520and%2520pragmatic%250Ainference.%2520This%2520work%2520provides%2520empirical%2520grounding%2520for%2520the%2520proposed%2520Visual%2520Room%250Aargument%2520and%2520offers%2520a%2520new%2520evaluation%2520paradigm%2520for%2520MLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23272v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20MLMs%20Trapped%20in%20the%20Visual%20Room%3F&entry.906535625=Yazhou%20Zhang%20and%20Chunwang%20Zou%20and%20Qimeng%20Liu%20and%20Lu%20Rong%20and%20Ben%20Yao%20and%20Zheng%20Lian%20and%20Qiuchi%20Li%20and%20Peng%20Zhang%20and%20Jing%20Qin&entry.1292438233=%20%20Can%20multi-modal%20large%20models%20%28MLMs%29%20that%20can%20%60%60see%27%27%20an%20image%20be%20said%20to%0A%60%60understand%27%27%20it%3F%20Drawing%20inspiration%20from%20Searle%27s%20Chinese%20Room%2C%20we%20propose%0Athe%20%5Ctextbf%7BVisual%20Room%7D%20argument%3A%20a%20system%20may%20process%20and%20describe%20every%0Adetail%20of%20visual%20inputs%20by%20following%20algorithmic%20rules%2C%20without%20genuinely%0Acomprehending%20the%20underlying%20intention.%20This%20dilemma%20challenges%20the%20prevailing%0Aassumption%20that%20perceptual%20mastery%20implies%20genuine%20understanding.%20In%0Aimplementation%2C%20we%20introduce%20a%20two-tier%20evaluation%20framework%20spanning%0Aperception%20and%20cognition.%20The%20perception%20component%20evaluates%20whether%20MLMs%20can%0Aaccurately%20capture%20the%20surface-level%20details%20of%20visual%20contents%2C%20where%20the%0Acognitive%20component%20examines%20their%20ability%20to%20infer%20sarcasm%20polarity.%20To%0Asupport%20this%20framework%2C%20We%20further%20introduce%20a%20high-quality%20multi-modal%20sarcasm%0Adataset%20comprising%20both%20924%20static%20images%20and%20100%20dynamic%20videos.%20All%20sarcasm%0Alabels%20are%20annotated%20by%20the%20original%20authors%20and%20verified%20by%20independent%0Areviewers%20to%20ensure%20clarity%20and%20consistency.%20We%20evaluate%20eight%20state-of-the-art%0A%28SoTA%29%20MLMs.%20Our%20results%20highlight%20three%20key%20findings%3A%20%281%29%20MLMs%20demonstrate%0Ahigh%20accuracy%20in%20visual%20perception%3B%20%282%29%20even%20with%20correct%20perception%2C%20MLMs%0Aexhibit%20an%20average%20error%20rate%20of%20~17.1%5C%25%20in%20sarcasm%20understanding%2C%20revealing%20a%0Asignificant%20gap%20between%20seeing%20and%20understanding%3B%20%283%29%20this%20gap%20stems%20from%0Aweaknesses%20in%20context%20integration%2C%20emotional%20reasoning%2C%20and%20pragmatic%0Ainference.%20This%20work%20provides%20empirical%20grounding%20for%20the%20proposed%20Visual%20Room%0Aargument%20and%20offers%20a%20new%20evaluation%20paradigm%20for%20MLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23272v2&entry.124074799=Read"},
{"title": "MSVCOD:A Large-Scale Multi-Scene Dataset for Video Camouflage Object\n  Detection", "author": "Shuyong Gao and Yu'ang Feng and Qishan Wang and Lingyi Hong and Xinyu Zhou and Liu Fei and Yan Wang and Wenqiang Zhang", "abstract": "  Video Camouflaged Object Detection (VCOD) is a challenging task which aims to\nidentify objects that seamlessly concealed within the background in videos. The\ndynamic properties of video enable detection of camouflaged objects through\nmotion cues or varied perspectives. Previous VCOD datasets primarily contain\nanimal objects, limiting the scope of research to wildlife scenarios. However,\nthe applications of VCOD extend beyond wildlife and have significant\nimplications in security, art, and medical fields. Addressing this problem, we\nconstruct a new large-scale multi-domain VCOD dataset MSVCOD. To achieve\nhigh-quality annotations, we design a semi-automatic iterative annotation\npipeline that reduces costs while maintaining annotation accuracy. Our MSVCOD\nis the largest VCOD dataset to date, introducing multiple object categories\nincluding human, animal, medical, and vehicle objects for the first time, while\nalso expanding background diversity across various environments. This expanded\nscope increases the practical applicability of the VCOD task in camouflaged\nobject detection. Alongside this dataset, we introduce a one-steam video\ncamouflage object detection model that performs both feature extraction and\ninformation fusion without additional motion feature fusion modules. Our\nframework achieves state-of-the-art results on the existing VCOD animal dataset\nand the proposed MSVCOD. The dataset and code will be made publicly available.\n", "link": "http://arxiv.org/abs/2502.13859v2", "date": "2025-05-30", "relevancy": 2.7167, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.548}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.541}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MSVCOD%3AA%20Large-Scale%20Multi-Scene%20Dataset%20for%20Video%20Camouflage%20Object%0A%20%20Detection&body=Title%3A%20MSVCOD%3AA%20Large-Scale%20Multi-Scene%20Dataset%20for%20Video%20Camouflage%20Object%0A%20%20Detection%0AAuthor%3A%20Shuyong%20Gao%20and%20Yu%27ang%20Feng%20and%20Qishan%20Wang%20and%20Lingyi%20Hong%20and%20Xinyu%20Zhou%20and%20Liu%20Fei%20and%20Yan%20Wang%20and%20Wenqiang%20Zhang%0AAbstract%3A%20%20%20Video%20Camouflaged%20Object%20Detection%20%28VCOD%29%20is%20a%20challenging%20task%20which%20aims%20to%0Aidentify%20objects%20that%20seamlessly%20concealed%20within%20the%20background%20in%20videos.%20The%0Adynamic%20properties%20of%20video%20enable%20detection%20of%20camouflaged%20objects%20through%0Amotion%20cues%20or%20varied%20perspectives.%20Previous%20VCOD%20datasets%20primarily%20contain%0Aanimal%20objects%2C%20limiting%20the%20scope%20of%20research%20to%20wildlife%20scenarios.%20However%2C%0Athe%20applications%20of%20VCOD%20extend%20beyond%20wildlife%20and%20have%20significant%0Aimplications%20in%20security%2C%20art%2C%20and%20medical%20fields.%20Addressing%20this%20problem%2C%20we%0Aconstruct%20a%20new%20large-scale%20multi-domain%20VCOD%20dataset%20MSVCOD.%20To%20achieve%0Ahigh-quality%20annotations%2C%20we%20design%20a%20semi-automatic%20iterative%20annotation%0Apipeline%20that%20reduces%20costs%20while%20maintaining%20annotation%20accuracy.%20Our%20MSVCOD%0Ais%20the%20largest%20VCOD%20dataset%20to%20date%2C%20introducing%20multiple%20object%20categories%0Aincluding%20human%2C%20animal%2C%20medical%2C%20and%20vehicle%20objects%20for%20the%20first%20time%2C%20while%0Aalso%20expanding%20background%20diversity%20across%20various%20environments.%20This%20expanded%0Ascope%20increases%20the%20practical%20applicability%20of%20the%20VCOD%20task%20in%20camouflaged%0Aobject%20detection.%20Alongside%20this%20dataset%2C%20we%20introduce%20a%20one-steam%20video%0Acamouflage%20object%20detection%20model%20that%20performs%20both%20feature%20extraction%20and%0Ainformation%20fusion%20without%20additional%20motion%20feature%20fusion%20modules.%20Our%0Aframework%20achieves%20state-of-the-art%20results%20on%20the%20existing%20VCOD%20animal%20dataset%0Aand%20the%20proposed%20MSVCOD.%20The%20dataset%20and%20code%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13859v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMSVCOD%253AA%2520Large-Scale%2520Multi-Scene%2520Dataset%2520for%2520Video%2520Camouflage%2520Object%250A%2520%2520Detection%26entry.906535625%3DShuyong%2520Gao%2520and%2520Yu%2527ang%2520Feng%2520and%2520Qishan%2520Wang%2520and%2520Lingyi%2520Hong%2520and%2520Xinyu%2520Zhou%2520and%2520Liu%2520Fei%2520and%2520Yan%2520Wang%2520and%2520Wenqiang%2520Zhang%26entry.1292438233%3D%2520%2520Video%2520Camouflaged%2520Object%2520Detection%2520%2528VCOD%2529%2520is%2520a%2520challenging%2520task%2520which%2520aims%2520to%250Aidentify%2520objects%2520that%2520seamlessly%2520concealed%2520within%2520the%2520background%2520in%2520videos.%2520The%250Adynamic%2520properties%2520of%2520video%2520enable%2520detection%2520of%2520camouflaged%2520objects%2520through%250Amotion%2520cues%2520or%2520varied%2520perspectives.%2520Previous%2520VCOD%2520datasets%2520primarily%2520contain%250Aanimal%2520objects%252C%2520limiting%2520the%2520scope%2520of%2520research%2520to%2520wildlife%2520scenarios.%2520However%252C%250Athe%2520applications%2520of%2520VCOD%2520extend%2520beyond%2520wildlife%2520and%2520have%2520significant%250Aimplications%2520in%2520security%252C%2520art%252C%2520and%2520medical%2520fields.%2520Addressing%2520this%2520problem%252C%2520we%250Aconstruct%2520a%2520new%2520large-scale%2520multi-domain%2520VCOD%2520dataset%2520MSVCOD.%2520To%2520achieve%250Ahigh-quality%2520annotations%252C%2520we%2520design%2520a%2520semi-automatic%2520iterative%2520annotation%250Apipeline%2520that%2520reduces%2520costs%2520while%2520maintaining%2520annotation%2520accuracy.%2520Our%2520MSVCOD%250Ais%2520the%2520largest%2520VCOD%2520dataset%2520to%2520date%252C%2520introducing%2520multiple%2520object%2520categories%250Aincluding%2520human%252C%2520animal%252C%2520medical%252C%2520and%2520vehicle%2520objects%2520for%2520the%2520first%2520time%252C%2520while%250Aalso%2520expanding%2520background%2520diversity%2520across%2520various%2520environments.%2520This%2520expanded%250Ascope%2520increases%2520the%2520practical%2520applicability%2520of%2520the%2520VCOD%2520task%2520in%2520camouflaged%250Aobject%2520detection.%2520Alongside%2520this%2520dataset%252C%2520we%2520introduce%2520a%2520one-steam%2520video%250Acamouflage%2520object%2520detection%2520model%2520that%2520performs%2520both%2520feature%2520extraction%2520and%250Ainformation%2520fusion%2520without%2520additional%2520motion%2520feature%2520fusion%2520modules.%2520Our%250Aframework%2520achieves%2520state-of-the-art%2520results%2520on%2520the%2520existing%2520VCOD%2520animal%2520dataset%250Aand%2520the%2520proposed%2520MSVCOD.%2520The%2520dataset%2520and%2520code%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13859v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MSVCOD%3AA%20Large-Scale%20Multi-Scene%20Dataset%20for%20Video%20Camouflage%20Object%0A%20%20Detection&entry.906535625=Shuyong%20Gao%20and%20Yu%27ang%20Feng%20and%20Qishan%20Wang%20and%20Lingyi%20Hong%20and%20Xinyu%20Zhou%20and%20Liu%20Fei%20and%20Yan%20Wang%20and%20Wenqiang%20Zhang&entry.1292438233=%20%20Video%20Camouflaged%20Object%20Detection%20%28VCOD%29%20is%20a%20challenging%20task%20which%20aims%20to%0Aidentify%20objects%20that%20seamlessly%20concealed%20within%20the%20background%20in%20videos.%20The%0Adynamic%20properties%20of%20video%20enable%20detection%20of%20camouflaged%20objects%20through%0Amotion%20cues%20or%20varied%20perspectives.%20Previous%20VCOD%20datasets%20primarily%20contain%0Aanimal%20objects%2C%20limiting%20the%20scope%20of%20research%20to%20wildlife%20scenarios.%20However%2C%0Athe%20applications%20of%20VCOD%20extend%20beyond%20wildlife%20and%20have%20significant%0Aimplications%20in%20security%2C%20art%2C%20and%20medical%20fields.%20Addressing%20this%20problem%2C%20we%0Aconstruct%20a%20new%20large-scale%20multi-domain%20VCOD%20dataset%20MSVCOD.%20To%20achieve%0Ahigh-quality%20annotations%2C%20we%20design%20a%20semi-automatic%20iterative%20annotation%0Apipeline%20that%20reduces%20costs%20while%20maintaining%20annotation%20accuracy.%20Our%20MSVCOD%0Ais%20the%20largest%20VCOD%20dataset%20to%20date%2C%20introducing%20multiple%20object%20categories%0Aincluding%20human%2C%20animal%2C%20medical%2C%20and%20vehicle%20objects%20for%20the%20first%20time%2C%20while%0Aalso%20expanding%20background%20diversity%20across%20various%20environments.%20This%20expanded%0Ascope%20increases%20the%20practical%20applicability%20of%20the%20VCOD%20task%20in%20camouflaged%0Aobject%20detection.%20Alongside%20this%20dataset%2C%20we%20introduce%20a%20one-steam%20video%0Acamouflage%20object%20detection%20model%20that%20performs%20both%20feature%20extraction%20and%0Ainformation%20fusion%20without%20additional%20motion%20feature%20fusion%20modules.%20Our%0Aframework%20achieves%20state-of-the-art%20results%20on%20the%20existing%20VCOD%20animal%20dataset%0Aand%20the%20proposed%20MSVCOD.%20The%20dataset%20and%20code%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13859v2&entry.124074799=Read"},
{"title": "Reading Recognition in the Wild", "author": "Charig Yang and Samiul Alam and Shakhrul Iman Siam and Michael J. Proulx and Lambert Mathias and Kiran Somasundaram and Luis Pesqueira and James Fort and Sheroze Sheriffdeen and Omkar Parkhi and Carl Ren and Mi Zhang and Yuning Chai and Richard Newcombe and Hyo Jin Kim", "abstract": "  To enable egocentric contextual AI in always-on smart glasses, it is crucial\nto be able to keep a record of the user's interactions with the world,\nincluding during reading. In this paper, we introduce a new task of reading\nrecognition to determine when the user is reading. We first introduce the\nfirst-of-its-kind large-scale multimodal Reading in the Wild dataset,\ncontaining 100 hours of reading and non-reading videos in diverse and realistic\nscenarios. We then identify three modalities (egocentric RGB, eye gaze, head\npose) that can be used to solve the task, and present a flexible transformer\nmodel that performs the task using these modalities, either individually or\ncombined. We show that these modalities are relevant and complementary to the\ntask, and investigate how to efficiently and effectively encode each modality.\nAdditionally, we show the usefulness of this dataset towards classifying types\nof reading, extending current reading understanding studies conducted in\nconstrained settings to larger scale, diversity and realism. Code, model, and\ndata will be public.\n", "link": "http://arxiv.org/abs/2505.24848v1", "date": "2025-05-30", "relevancy": 2.7141, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5442}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5421}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reading%20Recognition%20in%20the%20Wild&body=Title%3A%20Reading%20Recognition%20in%20the%20Wild%0AAuthor%3A%20Charig%20Yang%20and%20Samiul%20Alam%20and%20Shakhrul%20Iman%20Siam%20and%20Michael%20J.%20Proulx%20and%20Lambert%20Mathias%20and%20Kiran%20Somasundaram%20and%20Luis%20Pesqueira%20and%20James%20Fort%20and%20Sheroze%20Sheriffdeen%20and%20Omkar%20Parkhi%20and%20Carl%20Ren%20and%20Mi%20Zhang%20and%20Yuning%20Chai%20and%20Richard%20Newcombe%20and%20Hyo%20Jin%20Kim%0AAbstract%3A%20%20%20To%20enable%20egocentric%20contextual%20AI%20in%20always-on%20smart%20glasses%2C%20it%20is%20crucial%0Ato%20be%20able%20to%20keep%20a%20record%20of%20the%20user%27s%20interactions%20with%20the%20world%2C%0Aincluding%20during%20reading.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20task%20of%20reading%0Arecognition%20to%20determine%20when%20the%20user%20is%20reading.%20We%20first%20introduce%20the%0Afirst-of-its-kind%20large-scale%20multimodal%20Reading%20in%20the%20Wild%20dataset%2C%0Acontaining%20100%20hours%20of%20reading%20and%20non-reading%20videos%20in%20diverse%20and%20realistic%0Ascenarios.%20We%20then%20identify%20three%20modalities%20%28egocentric%20RGB%2C%20eye%20gaze%2C%20head%0Apose%29%20that%20can%20be%20used%20to%20solve%20the%20task%2C%20and%20present%20a%20flexible%20transformer%0Amodel%20that%20performs%20the%20task%20using%20these%20modalities%2C%20either%20individually%20or%0Acombined.%20We%20show%20that%20these%20modalities%20are%20relevant%20and%20complementary%20to%20the%0Atask%2C%20and%20investigate%20how%20to%20efficiently%20and%20effectively%20encode%20each%20modality.%0AAdditionally%2C%20we%20show%20the%20usefulness%20of%20this%20dataset%20towards%20classifying%20types%0Aof%20reading%2C%20extending%20current%20reading%20understanding%20studies%20conducted%20in%0Aconstrained%20settings%20to%20larger%20scale%2C%20diversity%20and%20realism.%20Code%2C%20model%2C%20and%0Adata%20will%20be%20public.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24848v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReading%2520Recognition%2520in%2520the%2520Wild%26entry.906535625%3DCharig%2520Yang%2520and%2520Samiul%2520Alam%2520and%2520Shakhrul%2520Iman%2520Siam%2520and%2520Michael%2520J.%2520Proulx%2520and%2520Lambert%2520Mathias%2520and%2520Kiran%2520Somasundaram%2520and%2520Luis%2520Pesqueira%2520and%2520James%2520Fort%2520and%2520Sheroze%2520Sheriffdeen%2520and%2520Omkar%2520Parkhi%2520and%2520Carl%2520Ren%2520and%2520Mi%2520Zhang%2520and%2520Yuning%2520Chai%2520and%2520Richard%2520Newcombe%2520and%2520Hyo%2520Jin%2520Kim%26entry.1292438233%3D%2520%2520To%2520enable%2520egocentric%2520contextual%2520AI%2520in%2520always-on%2520smart%2520glasses%252C%2520it%2520is%2520crucial%250Ato%2520be%2520able%2520to%2520keep%2520a%2520record%2520of%2520the%2520user%2527s%2520interactions%2520with%2520the%2520world%252C%250Aincluding%2520during%2520reading.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520new%2520task%2520of%2520reading%250Arecognition%2520to%2520determine%2520when%2520the%2520user%2520is%2520reading.%2520We%2520first%2520introduce%2520the%250Afirst-of-its-kind%2520large-scale%2520multimodal%2520Reading%2520in%2520the%2520Wild%2520dataset%252C%250Acontaining%2520100%2520hours%2520of%2520reading%2520and%2520non-reading%2520videos%2520in%2520diverse%2520and%2520realistic%250Ascenarios.%2520We%2520then%2520identify%2520three%2520modalities%2520%2528egocentric%2520RGB%252C%2520eye%2520gaze%252C%2520head%250Apose%2529%2520that%2520can%2520be%2520used%2520to%2520solve%2520the%2520task%252C%2520and%2520present%2520a%2520flexible%2520transformer%250Amodel%2520that%2520performs%2520the%2520task%2520using%2520these%2520modalities%252C%2520either%2520individually%2520or%250Acombined.%2520We%2520show%2520that%2520these%2520modalities%2520are%2520relevant%2520and%2520complementary%2520to%2520the%250Atask%252C%2520and%2520investigate%2520how%2520to%2520efficiently%2520and%2520effectively%2520encode%2520each%2520modality.%250AAdditionally%252C%2520we%2520show%2520the%2520usefulness%2520of%2520this%2520dataset%2520towards%2520classifying%2520types%250Aof%2520reading%252C%2520extending%2520current%2520reading%2520understanding%2520studies%2520conducted%2520in%250Aconstrained%2520settings%2520to%2520larger%2520scale%252C%2520diversity%2520and%2520realism.%2520Code%252C%2520model%252C%2520and%250Adata%2520will%2520be%2520public.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24848v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reading%20Recognition%20in%20the%20Wild&entry.906535625=Charig%20Yang%20and%20Samiul%20Alam%20and%20Shakhrul%20Iman%20Siam%20and%20Michael%20J.%20Proulx%20and%20Lambert%20Mathias%20and%20Kiran%20Somasundaram%20and%20Luis%20Pesqueira%20and%20James%20Fort%20and%20Sheroze%20Sheriffdeen%20and%20Omkar%20Parkhi%20and%20Carl%20Ren%20and%20Mi%20Zhang%20and%20Yuning%20Chai%20and%20Richard%20Newcombe%20and%20Hyo%20Jin%20Kim&entry.1292438233=%20%20To%20enable%20egocentric%20contextual%20AI%20in%20always-on%20smart%20glasses%2C%20it%20is%20crucial%0Ato%20be%20able%20to%20keep%20a%20record%20of%20the%20user%27s%20interactions%20with%20the%20world%2C%0Aincluding%20during%20reading.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20task%20of%20reading%0Arecognition%20to%20determine%20when%20the%20user%20is%20reading.%20We%20first%20introduce%20the%0Afirst-of-its-kind%20large-scale%20multimodal%20Reading%20in%20the%20Wild%20dataset%2C%0Acontaining%20100%20hours%20of%20reading%20and%20non-reading%20videos%20in%20diverse%20and%20realistic%0Ascenarios.%20We%20then%20identify%20three%20modalities%20%28egocentric%20RGB%2C%20eye%20gaze%2C%20head%0Apose%29%20that%20can%20be%20used%20to%20solve%20the%20task%2C%20and%20present%20a%20flexible%20transformer%0Amodel%20that%20performs%20the%20task%20using%20these%20modalities%2C%20either%20individually%20or%0Acombined.%20We%20show%20that%20these%20modalities%20are%20relevant%20and%20complementary%20to%20the%0Atask%2C%20and%20investigate%20how%20to%20efficiently%20and%20effectively%20encode%20each%20modality.%0AAdditionally%2C%20we%20show%20the%20usefulness%20of%20this%20dataset%20towards%20classifying%20types%0Aof%20reading%2C%20extending%20current%20reading%20understanding%20studies%20conducted%20in%0Aconstrained%20settings%20to%20larger%20scale%2C%20diversity%20and%20realism.%20Code%2C%20model%2C%20and%0Adata%20will%20be%20public.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24848v1&entry.124074799=Read"},
{"title": "SORCE: Small Object Retrieval in Complex Environments", "author": "Chunxu Liu and Chi Xie and Xiaxu Chen and Wei Li and Feng Zhu and Rui Zhao and Limin Wang", "abstract": "  Text-to-Image Retrieval (T2IR) is a highly valuable task that aims to match a\ngiven textual query to images in a gallery. Existing benchmarks primarily focus\non textual queries describing overall image semantics or foreground salient\nobjects, possibly overlooking inconspicuous small objects, especially in\ncomplex environments. Such small object retrieval is crucial, as in real-world\napplications, the targets of interest are not always prominent in the image.\nThus, we introduce SORCE (Small Object Retrieval in Complex Environments), a\nnew subfield of T2IR, focusing on retrieving small objects in complex images\nwith textual queries. We propose a new benchmark, SORCE-1K, consisting of\nimages with complex environments and textual queries describing less\nconspicuous small objects with minimal contextual cues from other salient\nobjects. Preliminary analysis on SORCE-1K finds that existing T2IR methods\nstruggle to capture small objects and encode all the semantics into a single\nembedding, leading to poor retrieval performance on SORCE-1K. Therefore, we\npropose to represent each image with multiple distinctive embeddings. We\nleverage Multimodal Large Language Models (MLLMs) to extract multiple\nembeddings for each image instructed by a set of Regional Prompts (ReP).\nExperimental results show that our multi-embedding approach through MLLM and\nReP significantly outperforms existing T2IR methods on SORCE-1K. Our\nexperiments validate the effectiveness of SORCE-1K for benchmarking SORCE\nperformances, highlighting the potential of multi-embedding representation and\ntext-customized MLLM features for addressing this task.\n", "link": "http://arxiv.org/abs/2505.24441v1", "date": "2025-05-30", "relevancy": 2.7127, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5656}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5656}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4964}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SORCE%3A%20Small%20Object%20Retrieval%20in%20Complex%20Environments&body=Title%3A%20SORCE%3A%20Small%20Object%20Retrieval%20in%20Complex%20Environments%0AAuthor%3A%20Chunxu%20Liu%20and%20Chi%20Xie%20and%20Xiaxu%20Chen%20and%20Wei%20Li%20and%20Feng%20Zhu%20and%20Rui%20Zhao%20and%20Limin%20Wang%0AAbstract%3A%20%20%20Text-to-Image%20Retrieval%20%28T2IR%29%20is%20a%20highly%20valuable%20task%20that%20aims%20to%20match%20a%0Agiven%20textual%20query%20to%20images%20in%20a%20gallery.%20Existing%20benchmarks%20primarily%20focus%0Aon%20textual%20queries%20describing%20overall%20image%20semantics%20or%20foreground%20salient%0Aobjects%2C%20possibly%20overlooking%20inconspicuous%20small%20objects%2C%20especially%20in%0Acomplex%20environments.%20Such%20small%20object%20retrieval%20is%20crucial%2C%20as%20in%20real-world%0Aapplications%2C%20the%20targets%20of%20interest%20are%20not%20always%20prominent%20in%20the%20image.%0AThus%2C%20we%20introduce%20SORCE%20%28Small%20Object%20Retrieval%20in%20Complex%20Environments%29%2C%20a%0Anew%20subfield%20of%20T2IR%2C%20focusing%20on%20retrieving%20small%20objects%20in%20complex%20images%0Awith%20textual%20queries.%20We%20propose%20a%20new%20benchmark%2C%20SORCE-1K%2C%20consisting%20of%0Aimages%20with%20complex%20environments%20and%20textual%20queries%20describing%20less%0Aconspicuous%20small%20objects%20with%20minimal%20contextual%20cues%20from%20other%20salient%0Aobjects.%20Preliminary%20analysis%20on%20SORCE-1K%20finds%20that%20existing%20T2IR%20methods%0Astruggle%20to%20capture%20small%20objects%20and%20encode%20all%20the%20semantics%20into%20a%20single%0Aembedding%2C%20leading%20to%20poor%20retrieval%20performance%20on%20SORCE-1K.%20Therefore%2C%20we%0Apropose%20to%20represent%20each%20image%20with%20multiple%20distinctive%20embeddings.%20We%0Aleverage%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20to%20extract%20multiple%0Aembeddings%20for%20each%20image%20instructed%20by%20a%20set%20of%20Regional%20Prompts%20%28ReP%29.%0AExperimental%20results%20show%20that%20our%20multi-embedding%20approach%20through%20MLLM%20and%0AReP%20significantly%20outperforms%20existing%20T2IR%20methods%20on%20SORCE-1K.%20Our%0Aexperiments%20validate%20the%20effectiveness%20of%20SORCE-1K%20for%20benchmarking%20SORCE%0Aperformances%2C%20highlighting%20the%20potential%20of%20multi-embedding%20representation%20and%0Atext-customized%20MLLM%20features%20for%20addressing%20this%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24441v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSORCE%253A%2520Small%2520Object%2520Retrieval%2520in%2520Complex%2520Environments%26entry.906535625%3DChunxu%2520Liu%2520and%2520Chi%2520Xie%2520and%2520Xiaxu%2520Chen%2520and%2520Wei%2520Li%2520and%2520Feng%2520Zhu%2520and%2520Rui%2520Zhao%2520and%2520Limin%2520Wang%26entry.1292438233%3D%2520%2520Text-to-Image%2520Retrieval%2520%2528T2IR%2529%2520is%2520a%2520highly%2520valuable%2520task%2520that%2520aims%2520to%2520match%2520a%250Agiven%2520textual%2520query%2520to%2520images%2520in%2520a%2520gallery.%2520Existing%2520benchmarks%2520primarily%2520focus%250Aon%2520textual%2520queries%2520describing%2520overall%2520image%2520semantics%2520or%2520foreground%2520salient%250Aobjects%252C%2520possibly%2520overlooking%2520inconspicuous%2520small%2520objects%252C%2520especially%2520in%250Acomplex%2520environments.%2520Such%2520small%2520object%2520retrieval%2520is%2520crucial%252C%2520as%2520in%2520real-world%250Aapplications%252C%2520the%2520targets%2520of%2520interest%2520are%2520not%2520always%2520prominent%2520in%2520the%2520image.%250AThus%252C%2520we%2520introduce%2520SORCE%2520%2528Small%2520Object%2520Retrieval%2520in%2520Complex%2520Environments%2529%252C%2520a%250Anew%2520subfield%2520of%2520T2IR%252C%2520focusing%2520on%2520retrieving%2520small%2520objects%2520in%2520complex%2520images%250Awith%2520textual%2520queries.%2520We%2520propose%2520a%2520new%2520benchmark%252C%2520SORCE-1K%252C%2520consisting%2520of%250Aimages%2520with%2520complex%2520environments%2520and%2520textual%2520queries%2520describing%2520less%250Aconspicuous%2520small%2520objects%2520with%2520minimal%2520contextual%2520cues%2520from%2520other%2520salient%250Aobjects.%2520Preliminary%2520analysis%2520on%2520SORCE-1K%2520finds%2520that%2520existing%2520T2IR%2520methods%250Astruggle%2520to%2520capture%2520small%2520objects%2520and%2520encode%2520all%2520the%2520semantics%2520into%2520a%2520single%250Aembedding%252C%2520leading%2520to%2520poor%2520retrieval%2520performance%2520on%2520SORCE-1K.%2520Therefore%252C%2520we%250Apropose%2520to%2520represent%2520each%2520image%2520with%2520multiple%2520distinctive%2520embeddings.%2520We%250Aleverage%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520to%2520extract%2520multiple%250Aembeddings%2520for%2520each%2520image%2520instructed%2520by%2520a%2520set%2520of%2520Regional%2520Prompts%2520%2528ReP%2529.%250AExperimental%2520results%2520show%2520that%2520our%2520multi-embedding%2520approach%2520through%2520MLLM%2520and%250AReP%2520significantly%2520outperforms%2520existing%2520T2IR%2520methods%2520on%2520SORCE-1K.%2520Our%250Aexperiments%2520validate%2520the%2520effectiveness%2520of%2520SORCE-1K%2520for%2520benchmarking%2520SORCE%250Aperformances%252C%2520highlighting%2520the%2520potential%2520of%2520multi-embedding%2520representation%2520and%250Atext-customized%2520MLLM%2520features%2520for%2520addressing%2520this%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24441v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SORCE%3A%20Small%20Object%20Retrieval%20in%20Complex%20Environments&entry.906535625=Chunxu%20Liu%20and%20Chi%20Xie%20and%20Xiaxu%20Chen%20and%20Wei%20Li%20and%20Feng%20Zhu%20and%20Rui%20Zhao%20and%20Limin%20Wang&entry.1292438233=%20%20Text-to-Image%20Retrieval%20%28T2IR%29%20is%20a%20highly%20valuable%20task%20that%20aims%20to%20match%20a%0Agiven%20textual%20query%20to%20images%20in%20a%20gallery.%20Existing%20benchmarks%20primarily%20focus%0Aon%20textual%20queries%20describing%20overall%20image%20semantics%20or%20foreground%20salient%0Aobjects%2C%20possibly%20overlooking%20inconspicuous%20small%20objects%2C%20especially%20in%0Acomplex%20environments.%20Such%20small%20object%20retrieval%20is%20crucial%2C%20as%20in%20real-world%0Aapplications%2C%20the%20targets%20of%20interest%20are%20not%20always%20prominent%20in%20the%20image.%0AThus%2C%20we%20introduce%20SORCE%20%28Small%20Object%20Retrieval%20in%20Complex%20Environments%29%2C%20a%0Anew%20subfield%20of%20T2IR%2C%20focusing%20on%20retrieving%20small%20objects%20in%20complex%20images%0Awith%20textual%20queries.%20We%20propose%20a%20new%20benchmark%2C%20SORCE-1K%2C%20consisting%20of%0Aimages%20with%20complex%20environments%20and%20textual%20queries%20describing%20less%0Aconspicuous%20small%20objects%20with%20minimal%20contextual%20cues%20from%20other%20salient%0Aobjects.%20Preliminary%20analysis%20on%20SORCE-1K%20finds%20that%20existing%20T2IR%20methods%0Astruggle%20to%20capture%20small%20objects%20and%20encode%20all%20the%20semantics%20into%20a%20single%0Aembedding%2C%20leading%20to%20poor%20retrieval%20performance%20on%20SORCE-1K.%20Therefore%2C%20we%0Apropose%20to%20represent%20each%20image%20with%20multiple%20distinctive%20embeddings.%20We%0Aleverage%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20to%20extract%20multiple%0Aembeddings%20for%20each%20image%20instructed%20by%20a%20set%20of%20Regional%20Prompts%20%28ReP%29.%0AExperimental%20results%20show%20that%20our%20multi-embedding%20approach%20through%20MLLM%20and%0AReP%20significantly%20outperforms%20existing%20T2IR%20methods%20on%20SORCE-1K.%20Our%0Aexperiments%20validate%20the%20effectiveness%20of%20SORCE-1K%20for%20benchmarking%20SORCE%0Aperformances%2C%20highlighting%20the%20potential%20of%20multi-embedding%20representation%20and%0Atext-customized%20MLLM%20features%20for%20addressing%20this%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24441v1&entry.124074799=Read"},
{"title": "Decoding Knowledge Attribution in Mixture-of-Experts: A Framework of\n  Basic-Refinement Collaboration and Efficiency Analysis", "author": "Junzhuo Li and Bo Wang and Xiuze Zhou and Peijie Jiang and Jia Liu and Xuming Hu", "abstract": "  The interpretability of Mixture-of-Experts (MoE) models, especially those\nwith heterogeneous designs, remains underexplored. Existing attribution methods\nfor dense models fail to capture dynamic routing-expert interactions in sparse\nMoE architectures. To address this issue, we propose a cross-level attribution\nalgorithm to analyze sparse MoE architectures (Qwen 1.5-MoE, OLMoE,\nMixtral-8x7B) against dense models (Qwen 1.5-7B, Llama-7B, Mixtral-7B). Results\nshow MoE models achieve 37% higher per-layer efficiency via a \"mid-activation,\nlate-amplification\" pattern: early layers screen experts, while late layers\nrefine knowledge collaboratively. Ablation studies reveal a \"basic-refinement\"\nframework--shared experts handle general tasks (entity recognition), while\nrouted experts specialize in domain-specific processing (geographic\nattributes). Semantic-driven routing is evidenced by strong correlations\nbetween attention heads and experts (r=0.68), enabling task-aware coordination.\nNotably, architectural depth dictates robustness: deep Qwen 1.5-MoE mitigates\nexpert failures (e.g., 43% MRR drop in geographic tasks when blocking top-10\nexperts) through shared expert redundancy, whereas shallow OLMoE suffers severe\ndegradation (76% drop). Task sensitivity further guides design: core-sensitive\ntasks (geography) require concentrated expertise, while distributed-tolerant\ntasks (object attributes) leverage broader participation. These insights\nadvance MoE interpretability, offering principles to balance efficiency,\nspecialization, and robustness.\n", "link": "http://arxiv.org/abs/2505.24593v1", "date": "2025-05-30", "relevancy": 2.7018, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5564}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5564}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoding%20Knowledge%20Attribution%20in%20Mixture-of-Experts%3A%20A%20Framework%20of%0A%20%20Basic-Refinement%20Collaboration%20and%20Efficiency%20Analysis&body=Title%3A%20Decoding%20Knowledge%20Attribution%20in%20Mixture-of-Experts%3A%20A%20Framework%20of%0A%20%20Basic-Refinement%20Collaboration%20and%20Efficiency%20Analysis%0AAuthor%3A%20Junzhuo%20Li%20and%20Bo%20Wang%20and%20Xiuze%20Zhou%20and%20Peijie%20Jiang%20and%20Jia%20Liu%20and%20Xuming%20Hu%0AAbstract%3A%20%20%20The%20interpretability%20of%20Mixture-of-Experts%20%28MoE%29%20models%2C%20especially%20those%0Awith%20heterogeneous%20designs%2C%20remains%20underexplored.%20Existing%20attribution%20methods%0Afor%20dense%20models%20fail%20to%20capture%20dynamic%20routing-expert%20interactions%20in%20sparse%0AMoE%20architectures.%20To%20address%20this%20issue%2C%20we%20propose%20a%20cross-level%20attribution%0Aalgorithm%20to%20analyze%20sparse%20MoE%20architectures%20%28Qwen%201.5-MoE%2C%20OLMoE%2C%0AMixtral-8x7B%29%20against%20dense%20models%20%28Qwen%201.5-7B%2C%20Llama-7B%2C%20Mixtral-7B%29.%20Results%0Ashow%20MoE%20models%20achieve%2037%25%20higher%20per-layer%20efficiency%20via%20a%20%22mid-activation%2C%0Alate-amplification%22%20pattern%3A%20early%20layers%20screen%20experts%2C%20while%20late%20layers%0Arefine%20knowledge%20collaboratively.%20Ablation%20studies%20reveal%20a%20%22basic-refinement%22%0Aframework--shared%20experts%20handle%20general%20tasks%20%28entity%20recognition%29%2C%20while%0Arouted%20experts%20specialize%20in%20domain-specific%20processing%20%28geographic%0Aattributes%29.%20Semantic-driven%20routing%20is%20evidenced%20by%20strong%20correlations%0Abetween%20attention%20heads%20and%20experts%20%28r%3D0.68%29%2C%20enabling%20task-aware%20coordination.%0ANotably%2C%20architectural%20depth%20dictates%20robustness%3A%20deep%20Qwen%201.5-MoE%20mitigates%0Aexpert%20failures%20%28e.g.%2C%2043%25%20MRR%20drop%20in%20geographic%20tasks%20when%20blocking%20top-10%0Aexperts%29%20through%20shared%20expert%20redundancy%2C%20whereas%20shallow%20OLMoE%20suffers%20severe%0Adegradation%20%2876%25%20drop%29.%20Task%20sensitivity%20further%20guides%20design%3A%20core-sensitive%0Atasks%20%28geography%29%20require%20concentrated%20expertise%2C%20while%20distributed-tolerant%0Atasks%20%28object%20attributes%29%20leverage%20broader%20participation.%20These%20insights%0Aadvance%20MoE%20interpretability%2C%20offering%20principles%20to%20balance%20efficiency%2C%0Aspecialization%2C%20and%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24593v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoding%2520Knowledge%2520Attribution%2520in%2520Mixture-of-Experts%253A%2520A%2520Framework%2520of%250A%2520%2520Basic-Refinement%2520Collaboration%2520and%2520Efficiency%2520Analysis%26entry.906535625%3DJunzhuo%2520Li%2520and%2520Bo%2520Wang%2520and%2520Xiuze%2520Zhou%2520and%2520Peijie%2520Jiang%2520and%2520Jia%2520Liu%2520and%2520Xuming%2520Hu%26entry.1292438233%3D%2520%2520The%2520interpretability%2520of%2520Mixture-of-Experts%2520%2528MoE%2529%2520models%252C%2520especially%2520those%250Awith%2520heterogeneous%2520designs%252C%2520remains%2520underexplored.%2520Existing%2520attribution%2520methods%250Afor%2520dense%2520models%2520fail%2520to%2520capture%2520dynamic%2520routing-expert%2520interactions%2520in%2520sparse%250AMoE%2520architectures.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520cross-level%2520attribution%250Aalgorithm%2520to%2520analyze%2520sparse%2520MoE%2520architectures%2520%2528Qwen%25201.5-MoE%252C%2520OLMoE%252C%250AMixtral-8x7B%2529%2520against%2520dense%2520models%2520%2528Qwen%25201.5-7B%252C%2520Llama-7B%252C%2520Mixtral-7B%2529.%2520Results%250Ashow%2520MoE%2520models%2520achieve%252037%2525%2520higher%2520per-layer%2520efficiency%2520via%2520a%2520%2522mid-activation%252C%250Alate-amplification%2522%2520pattern%253A%2520early%2520layers%2520screen%2520experts%252C%2520while%2520late%2520layers%250Arefine%2520knowledge%2520collaboratively.%2520Ablation%2520studies%2520reveal%2520a%2520%2522basic-refinement%2522%250Aframework--shared%2520experts%2520handle%2520general%2520tasks%2520%2528entity%2520recognition%2529%252C%2520while%250Arouted%2520experts%2520specialize%2520in%2520domain-specific%2520processing%2520%2528geographic%250Aattributes%2529.%2520Semantic-driven%2520routing%2520is%2520evidenced%2520by%2520strong%2520correlations%250Abetween%2520attention%2520heads%2520and%2520experts%2520%2528r%253D0.68%2529%252C%2520enabling%2520task-aware%2520coordination.%250ANotably%252C%2520architectural%2520depth%2520dictates%2520robustness%253A%2520deep%2520Qwen%25201.5-MoE%2520mitigates%250Aexpert%2520failures%2520%2528e.g.%252C%252043%2525%2520MRR%2520drop%2520in%2520geographic%2520tasks%2520when%2520blocking%2520top-10%250Aexperts%2529%2520through%2520shared%2520expert%2520redundancy%252C%2520whereas%2520shallow%2520OLMoE%2520suffers%2520severe%250Adegradation%2520%252876%2525%2520drop%2529.%2520Task%2520sensitivity%2520further%2520guides%2520design%253A%2520core-sensitive%250Atasks%2520%2528geography%2529%2520require%2520concentrated%2520expertise%252C%2520while%2520distributed-tolerant%250Atasks%2520%2528object%2520attributes%2529%2520leverage%2520broader%2520participation.%2520These%2520insights%250Aadvance%2520MoE%2520interpretability%252C%2520offering%2520principles%2520to%2520balance%2520efficiency%252C%250Aspecialization%252C%2520and%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24593v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoding%20Knowledge%20Attribution%20in%20Mixture-of-Experts%3A%20A%20Framework%20of%0A%20%20Basic-Refinement%20Collaboration%20and%20Efficiency%20Analysis&entry.906535625=Junzhuo%20Li%20and%20Bo%20Wang%20and%20Xiuze%20Zhou%20and%20Peijie%20Jiang%20and%20Jia%20Liu%20and%20Xuming%20Hu&entry.1292438233=%20%20The%20interpretability%20of%20Mixture-of-Experts%20%28MoE%29%20models%2C%20especially%20those%0Awith%20heterogeneous%20designs%2C%20remains%20underexplored.%20Existing%20attribution%20methods%0Afor%20dense%20models%20fail%20to%20capture%20dynamic%20routing-expert%20interactions%20in%20sparse%0AMoE%20architectures.%20To%20address%20this%20issue%2C%20we%20propose%20a%20cross-level%20attribution%0Aalgorithm%20to%20analyze%20sparse%20MoE%20architectures%20%28Qwen%201.5-MoE%2C%20OLMoE%2C%0AMixtral-8x7B%29%20against%20dense%20models%20%28Qwen%201.5-7B%2C%20Llama-7B%2C%20Mixtral-7B%29.%20Results%0Ashow%20MoE%20models%20achieve%2037%25%20higher%20per-layer%20efficiency%20via%20a%20%22mid-activation%2C%0Alate-amplification%22%20pattern%3A%20early%20layers%20screen%20experts%2C%20while%20late%20layers%0Arefine%20knowledge%20collaboratively.%20Ablation%20studies%20reveal%20a%20%22basic-refinement%22%0Aframework--shared%20experts%20handle%20general%20tasks%20%28entity%20recognition%29%2C%20while%0Arouted%20experts%20specialize%20in%20domain-specific%20processing%20%28geographic%0Aattributes%29.%20Semantic-driven%20routing%20is%20evidenced%20by%20strong%20correlations%0Abetween%20attention%20heads%20and%20experts%20%28r%3D0.68%29%2C%20enabling%20task-aware%20coordination.%0ANotably%2C%20architectural%20depth%20dictates%20robustness%3A%20deep%20Qwen%201.5-MoE%20mitigates%0Aexpert%20failures%20%28e.g.%2C%2043%25%20MRR%20drop%20in%20geographic%20tasks%20when%20blocking%20top-10%0Aexperts%29%20through%20shared%20expert%20redundancy%2C%20whereas%20shallow%20OLMoE%20suffers%20severe%0Adegradation%20%2876%25%20drop%29.%20Task%20sensitivity%20further%20guides%20design%3A%20core-sensitive%0Atasks%20%28geography%29%20require%20concentrated%20expertise%2C%20while%20distributed-tolerant%0Atasks%20%28object%20attributes%29%20leverage%20broader%20participation.%20These%20insights%0Aadvance%20MoE%20interpretability%2C%20offering%20principles%20to%20balance%20efficiency%2C%0Aspecialization%2C%20and%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24593v1&entry.124074799=Read"},
{"title": "Reinforcing Video Reasoning with Focused Thinking", "author": "Jisheng Dang and Jingze Wu and Teng Wang and Xuanhui Lin and Nannan Zhu and Hongbo Chen and Wei-Shi Zheng and Meng Wang and Tat-Seng Chua", "abstract": "  Recent advancements in reinforcement learning, particularly through Group\nRelative Policy Optimization (GRPO), have significantly improved multimodal\nlarge language models for complex reasoning tasks. However, two critical\nlimitations persist: 1) they often produce unfocused, verbose reasoning chains\nthat obscure salient spatiotemporal cues and 2) binary rewarding fails to\naccount for partially correct answers, resulting in high reward variance and\ninefficient learning. In this paper, we propose TW-GRPO, a novel framework that\nenhances visual reasoning with focused thinking and dense reward granularity.\nSpecifically, we employs a token weighting mechanism that prioritizes tokens\nwith high informational density (estimated by intra-group variance),\nsuppressing redundant tokens like generic reasoning prefixes. Furthermore, we\nreformulate RL training by shifting from single-choice to multi-choice QA\ntasks, where soft rewards enable finer-grained gradient estimation by\ndistinguishing partial correctness. Additionally, we propose question-answer\ninversion, a data augmentation strategy to generate diverse multi-choice\nsamples from existing benchmarks. Experiments demonstrate state-of-the-art\nperformance on several video reasoning and general understanding benchmarks.\nNotably, TW-GRPO achieves 50.4\\% accuracy on CLEVRER (18.8\\% improvement over\nVideo-R1) and 65.8\\% on MMVU. Our codes are available at\n\\href{https://github.com/longmalongma/TW-GRPO}{https://github.com/longmalongma/TW-GRPO}.\n", "link": "http://arxiv.org/abs/2505.24718v1", "date": "2025-05-30", "relevancy": 2.6972, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5459}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5459}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcing%20Video%20Reasoning%20with%20Focused%20Thinking&body=Title%3A%20Reinforcing%20Video%20Reasoning%20with%20Focused%20Thinking%0AAuthor%3A%20Jisheng%20Dang%20and%20Jingze%20Wu%20and%20Teng%20Wang%20and%20Xuanhui%20Lin%20and%20Nannan%20Zhu%20and%20Hongbo%20Chen%20and%20Wei-Shi%20Zheng%20and%20Meng%20Wang%20and%20Tat-Seng%20Chua%0AAbstract%3A%20%20%20Recent%20advancements%20in%20reinforcement%20learning%2C%20particularly%20through%20Group%0ARelative%20Policy%20Optimization%20%28GRPO%29%2C%20have%20significantly%20improved%20multimodal%0Alarge%20language%20models%20for%20complex%20reasoning%20tasks.%20However%2C%20two%20critical%0Alimitations%20persist%3A%201%29%20they%20often%20produce%20unfocused%2C%20verbose%20reasoning%20chains%0Athat%20obscure%20salient%20spatiotemporal%20cues%20and%202%29%20binary%20rewarding%20fails%20to%0Aaccount%20for%20partially%20correct%20answers%2C%20resulting%20in%20high%20reward%20variance%20and%0Ainefficient%20learning.%20In%20this%20paper%2C%20we%20propose%20TW-GRPO%2C%20a%20novel%20framework%20that%0Aenhances%20visual%20reasoning%20with%20focused%20thinking%20and%20dense%20reward%20granularity.%0ASpecifically%2C%20we%20employs%20a%20token%20weighting%20mechanism%20that%20prioritizes%20tokens%0Awith%20high%20informational%20density%20%28estimated%20by%20intra-group%20variance%29%2C%0Asuppressing%20redundant%20tokens%20like%20generic%20reasoning%20prefixes.%20Furthermore%2C%20we%0Areformulate%20RL%20training%20by%20shifting%20from%20single-choice%20to%20multi-choice%20QA%0Atasks%2C%20where%20soft%20rewards%20enable%20finer-grained%20gradient%20estimation%20by%0Adistinguishing%20partial%20correctness.%20Additionally%2C%20we%20propose%20question-answer%0Ainversion%2C%20a%20data%20augmentation%20strategy%20to%20generate%20diverse%20multi-choice%0Asamples%20from%20existing%20benchmarks.%20Experiments%20demonstrate%20state-of-the-art%0Aperformance%20on%20several%20video%20reasoning%20and%20general%20understanding%20benchmarks.%0ANotably%2C%20TW-GRPO%20achieves%2050.4%5C%25%20accuracy%20on%20CLEVRER%20%2818.8%5C%25%20improvement%20over%0AVideo-R1%29%20and%2065.8%5C%25%20on%20MMVU.%20Our%20codes%20are%20available%20at%0A%5Chref%7Bhttps%3A//github.com/longmalongma/TW-GRPO%7D%7Bhttps%3A//github.com/longmalongma/TW-GRPO%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24718v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcing%2520Video%2520Reasoning%2520with%2520Focused%2520Thinking%26entry.906535625%3DJisheng%2520Dang%2520and%2520Jingze%2520Wu%2520and%2520Teng%2520Wang%2520and%2520Xuanhui%2520Lin%2520and%2520Nannan%2520Zhu%2520and%2520Hongbo%2520Chen%2520and%2520Wei-Shi%2520Zheng%2520and%2520Meng%2520Wang%2520and%2520Tat-Seng%2520Chua%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520reinforcement%2520learning%252C%2520particularly%2520through%2520Group%250ARelative%2520Policy%2520Optimization%2520%2528GRPO%2529%252C%2520have%2520significantly%2520improved%2520multimodal%250Alarge%2520language%2520models%2520for%2520complex%2520reasoning%2520tasks.%2520However%252C%2520two%2520critical%250Alimitations%2520persist%253A%25201%2529%2520they%2520often%2520produce%2520unfocused%252C%2520verbose%2520reasoning%2520chains%250Athat%2520obscure%2520salient%2520spatiotemporal%2520cues%2520and%25202%2529%2520binary%2520rewarding%2520fails%2520to%250Aaccount%2520for%2520partially%2520correct%2520answers%252C%2520resulting%2520in%2520high%2520reward%2520variance%2520and%250Ainefficient%2520learning.%2520In%2520this%2520paper%252C%2520we%2520propose%2520TW-GRPO%252C%2520a%2520novel%2520framework%2520that%250Aenhances%2520visual%2520reasoning%2520with%2520focused%2520thinking%2520and%2520dense%2520reward%2520granularity.%250ASpecifically%252C%2520we%2520employs%2520a%2520token%2520weighting%2520mechanism%2520that%2520prioritizes%2520tokens%250Awith%2520high%2520informational%2520density%2520%2528estimated%2520by%2520intra-group%2520variance%2529%252C%250Asuppressing%2520redundant%2520tokens%2520like%2520generic%2520reasoning%2520prefixes.%2520Furthermore%252C%2520we%250Areformulate%2520RL%2520training%2520by%2520shifting%2520from%2520single-choice%2520to%2520multi-choice%2520QA%250Atasks%252C%2520where%2520soft%2520rewards%2520enable%2520finer-grained%2520gradient%2520estimation%2520by%250Adistinguishing%2520partial%2520correctness.%2520Additionally%252C%2520we%2520propose%2520question-answer%250Ainversion%252C%2520a%2520data%2520augmentation%2520strategy%2520to%2520generate%2520diverse%2520multi-choice%250Asamples%2520from%2520existing%2520benchmarks.%2520Experiments%2520demonstrate%2520state-of-the-art%250Aperformance%2520on%2520several%2520video%2520reasoning%2520and%2520general%2520understanding%2520benchmarks.%250ANotably%252C%2520TW-GRPO%2520achieves%252050.4%255C%2525%2520accuracy%2520on%2520CLEVRER%2520%252818.8%255C%2525%2520improvement%2520over%250AVideo-R1%2529%2520and%252065.8%255C%2525%2520on%2520MMVU.%2520Our%2520codes%2520are%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/longmalongma/TW-GRPO%257D%257Bhttps%253A//github.com/longmalongma/TW-GRPO%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24718v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcing%20Video%20Reasoning%20with%20Focused%20Thinking&entry.906535625=Jisheng%20Dang%20and%20Jingze%20Wu%20and%20Teng%20Wang%20and%20Xuanhui%20Lin%20and%20Nannan%20Zhu%20and%20Hongbo%20Chen%20and%20Wei-Shi%20Zheng%20and%20Meng%20Wang%20and%20Tat-Seng%20Chua&entry.1292438233=%20%20Recent%20advancements%20in%20reinforcement%20learning%2C%20particularly%20through%20Group%0ARelative%20Policy%20Optimization%20%28GRPO%29%2C%20have%20significantly%20improved%20multimodal%0Alarge%20language%20models%20for%20complex%20reasoning%20tasks.%20However%2C%20two%20critical%0Alimitations%20persist%3A%201%29%20they%20often%20produce%20unfocused%2C%20verbose%20reasoning%20chains%0Athat%20obscure%20salient%20spatiotemporal%20cues%20and%202%29%20binary%20rewarding%20fails%20to%0Aaccount%20for%20partially%20correct%20answers%2C%20resulting%20in%20high%20reward%20variance%20and%0Ainefficient%20learning.%20In%20this%20paper%2C%20we%20propose%20TW-GRPO%2C%20a%20novel%20framework%20that%0Aenhances%20visual%20reasoning%20with%20focused%20thinking%20and%20dense%20reward%20granularity.%0ASpecifically%2C%20we%20employs%20a%20token%20weighting%20mechanism%20that%20prioritizes%20tokens%0Awith%20high%20informational%20density%20%28estimated%20by%20intra-group%20variance%29%2C%0Asuppressing%20redundant%20tokens%20like%20generic%20reasoning%20prefixes.%20Furthermore%2C%20we%0Areformulate%20RL%20training%20by%20shifting%20from%20single-choice%20to%20multi-choice%20QA%0Atasks%2C%20where%20soft%20rewards%20enable%20finer-grained%20gradient%20estimation%20by%0Adistinguishing%20partial%20correctness.%20Additionally%2C%20we%20propose%20question-answer%0Ainversion%2C%20a%20data%20augmentation%20strategy%20to%20generate%20diverse%20multi-choice%0Asamples%20from%20existing%20benchmarks.%20Experiments%20demonstrate%20state-of-the-art%0Aperformance%20on%20several%20video%20reasoning%20and%20general%20understanding%20benchmarks.%0ANotably%2C%20TW-GRPO%20achieves%2050.4%5C%25%20accuracy%20on%20CLEVRER%20%2818.8%5C%25%20improvement%20over%0AVideo-R1%29%20and%2065.8%5C%25%20on%20MMVU.%20Our%20codes%20are%20available%20at%0A%5Chref%7Bhttps%3A//github.com/longmalongma/TW-GRPO%7D%7Bhttps%3A//github.com/longmalongma/TW-GRPO%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24718v1&entry.124074799=Read"},
{"title": "Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges\n  and Future Perspectives", "author": "Sara Sarto and Marcella Cornia and Rita Cucchiara", "abstract": "  The evaluation of machine-generated image captions is a complex and evolving\nchallenge. With the advent of Multimodal Large Language Models (MLLMs), image\ncaptioning has become a core task, increasing the need for robust and reliable\nevaluation metrics. This survey provides a comprehensive overview of\nadvancements in image captioning evaluation, analyzing the evolution,\nstrengths, and limitations of existing metrics. We assess these metrics across\nmultiple dimensions, including correlation with human judgment, ranking\naccuracy, and sensitivity to hallucinations. Additionally, we explore the\nchallenges posed by the longer and more detailed captions generated by MLLMs\nand examine the adaptability of current metrics to these stylistic variations.\nOur analysis highlights some limitations of standard evaluation approaches and\nsuggests promising directions for future research in image captioning\nassessment.\n", "link": "http://arxiv.org/abs/2503.14604v2", "date": "2025-05-30", "relevancy": 2.6793, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5406}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5406}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20Captioning%20Evaluation%20in%20the%20Age%20of%20Multimodal%20LLMs%3A%20Challenges%0A%20%20and%20Future%20Perspectives&body=Title%3A%20Image%20Captioning%20Evaluation%20in%20the%20Age%20of%20Multimodal%20LLMs%3A%20Challenges%0A%20%20and%20Future%20Perspectives%0AAuthor%3A%20Sara%20Sarto%20and%20Marcella%20Cornia%20and%20Rita%20Cucchiara%0AAbstract%3A%20%20%20The%20evaluation%20of%20machine-generated%20image%20captions%20is%20a%20complex%20and%20evolving%0Achallenge.%20With%20the%20advent%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20image%0Acaptioning%20has%20become%20a%20core%20task%2C%20increasing%20the%20need%20for%20robust%20and%20reliable%0Aevaluation%20metrics.%20This%20survey%20provides%20a%20comprehensive%20overview%20of%0Aadvancements%20in%20image%20captioning%20evaluation%2C%20analyzing%20the%20evolution%2C%0Astrengths%2C%20and%20limitations%20of%20existing%20metrics.%20We%20assess%20these%20metrics%20across%0Amultiple%20dimensions%2C%20including%20correlation%20with%20human%20judgment%2C%20ranking%0Aaccuracy%2C%20and%20sensitivity%20to%20hallucinations.%20Additionally%2C%20we%20explore%20the%0Achallenges%20posed%20by%20the%20longer%20and%20more%20detailed%20captions%20generated%20by%20MLLMs%0Aand%20examine%20the%20adaptability%20of%20current%20metrics%20to%20these%20stylistic%20variations.%0AOur%20analysis%20highlights%20some%20limitations%20of%20standard%20evaluation%20approaches%20and%0Asuggests%20promising%20directions%20for%20future%20research%20in%20image%20captioning%0Aassessment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.14604v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520Captioning%2520Evaluation%2520in%2520the%2520Age%2520of%2520Multimodal%2520LLMs%253A%2520Challenges%250A%2520%2520and%2520Future%2520Perspectives%26entry.906535625%3DSara%2520Sarto%2520and%2520Marcella%2520Cornia%2520and%2520Rita%2520Cucchiara%26entry.1292438233%3D%2520%2520The%2520evaluation%2520of%2520machine-generated%2520image%2520captions%2520is%2520a%2520complex%2520and%2520evolving%250Achallenge.%2520With%2520the%2520advent%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520image%250Acaptioning%2520has%2520become%2520a%2520core%2520task%252C%2520increasing%2520the%2520need%2520for%2520robust%2520and%2520reliable%250Aevaluation%2520metrics.%2520This%2520survey%2520provides%2520a%2520comprehensive%2520overview%2520of%250Aadvancements%2520in%2520image%2520captioning%2520evaluation%252C%2520analyzing%2520the%2520evolution%252C%250Astrengths%252C%2520and%2520limitations%2520of%2520existing%2520metrics.%2520We%2520assess%2520these%2520metrics%2520across%250Amultiple%2520dimensions%252C%2520including%2520correlation%2520with%2520human%2520judgment%252C%2520ranking%250Aaccuracy%252C%2520and%2520sensitivity%2520to%2520hallucinations.%2520Additionally%252C%2520we%2520explore%2520the%250Achallenges%2520posed%2520by%2520the%2520longer%2520and%2520more%2520detailed%2520captions%2520generated%2520by%2520MLLMs%250Aand%2520examine%2520the%2520adaptability%2520of%2520current%2520metrics%2520to%2520these%2520stylistic%2520variations.%250AOur%2520analysis%2520highlights%2520some%2520limitations%2520of%2520standard%2520evaluation%2520approaches%2520and%250Asuggests%2520promising%2520directions%2520for%2520future%2520research%2520in%2520image%2520captioning%250Aassessment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.14604v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20Captioning%20Evaluation%20in%20the%20Age%20of%20Multimodal%20LLMs%3A%20Challenges%0A%20%20and%20Future%20Perspectives&entry.906535625=Sara%20Sarto%20and%20Marcella%20Cornia%20and%20Rita%20Cucchiara&entry.1292438233=%20%20The%20evaluation%20of%20machine-generated%20image%20captions%20is%20a%20complex%20and%20evolving%0Achallenge.%20With%20the%20advent%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20image%0Acaptioning%20has%20become%20a%20core%20task%2C%20increasing%20the%20need%20for%20robust%20and%20reliable%0Aevaluation%20metrics.%20This%20survey%20provides%20a%20comprehensive%20overview%20of%0Aadvancements%20in%20image%20captioning%20evaluation%2C%20analyzing%20the%20evolution%2C%0Astrengths%2C%20and%20limitations%20of%20existing%20metrics.%20We%20assess%20these%20metrics%20across%0Amultiple%20dimensions%2C%20including%20correlation%20with%20human%20judgment%2C%20ranking%0Aaccuracy%2C%20and%20sensitivity%20to%20hallucinations.%20Additionally%2C%20we%20explore%20the%0Achallenges%20posed%20by%20the%20longer%20and%20more%20detailed%20captions%20generated%20by%20MLLMs%0Aand%20examine%20the%20adaptability%20of%20current%20metrics%20to%20these%20stylistic%20variations.%0AOur%20analysis%20highlights%20some%20limitations%20of%20standard%20evaluation%20approaches%20and%0Asuggests%20promising%20directions%20for%20future%20research%20in%20image%20captioning%0Aassessment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.14604v2&entry.124074799=Read"},
{"title": "Good Keypoints for the Two-View Geometry Estimation Problem", "author": "Konstantin Pakulev and Alexander Vakhitov and Gonzalo Ferrer", "abstract": "  Local features are essential to many modern downstream applications.\nTherefore, it is of interest to determine the properties of local features that\ncontribute to the downstream performance for a better design of feature\ndetectors and descriptors. In our work, we propose a new theoretical model for\nscoring feature points (keypoints) in the context of the two-view geometry\nestimation problem. The model determines two properties that a good keypoint\nfor solving the homography estimation problem should have: be repeatable and\nhave a small expected measurement error. This result provides key insights into\nwhy maximizing the number of correspondences doesn't always lead to better\nhomography estimation accuracy. We use the developed model to design a method\nthat detects keypoints that benefit the homography estimation and introduce the\nBounded NeSS-ST (BoNeSS-ST) keypoint detector. The novelty of BoNeSS-ST comes\nfrom strong theoretical foundations, a more accurate keypoint scoring due to\nsubpixel refinement and a cost designed for superior robustness to low saliency\nkeypoints. As a result, BoNeSS-ST outperforms prior self-supervised local\nfeature detectors on the planar homography estimation task and is on par with\nthem on the epipolar geometry estimation task.\n", "link": "http://arxiv.org/abs/2503.18767v2", "date": "2025-05-30", "relevancy": 2.6726, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6161}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4958}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Good%20Keypoints%20for%20the%20Two-View%20Geometry%20Estimation%20Problem&body=Title%3A%20Good%20Keypoints%20for%20the%20Two-View%20Geometry%20Estimation%20Problem%0AAuthor%3A%20Konstantin%20Pakulev%20and%20Alexander%20Vakhitov%20and%20Gonzalo%20Ferrer%0AAbstract%3A%20%20%20Local%20features%20are%20essential%20to%20many%20modern%20downstream%20applications.%0ATherefore%2C%20it%20is%20of%20interest%20to%20determine%20the%20properties%20of%20local%20features%20that%0Acontribute%20to%20the%20downstream%20performance%20for%20a%20better%20design%20of%20feature%0Adetectors%20and%20descriptors.%20In%20our%20work%2C%20we%20propose%20a%20new%20theoretical%20model%20for%0Ascoring%20feature%20points%20%28keypoints%29%20in%20the%20context%20of%20the%20two-view%20geometry%0Aestimation%20problem.%20The%20model%20determines%20two%20properties%20that%20a%20good%20keypoint%0Afor%20solving%20the%20homography%20estimation%20problem%20should%20have%3A%20be%20repeatable%20and%0Ahave%20a%20small%20expected%20measurement%20error.%20This%20result%20provides%20key%20insights%20into%0Awhy%20maximizing%20the%20number%20of%20correspondences%20doesn%27t%20always%20lead%20to%20better%0Ahomography%20estimation%20accuracy.%20We%20use%20the%20developed%20model%20to%20design%20a%20method%0Athat%20detects%20keypoints%20that%20benefit%20the%20homography%20estimation%20and%20introduce%20the%0ABounded%20NeSS-ST%20%28BoNeSS-ST%29%20keypoint%20detector.%20The%20novelty%20of%20BoNeSS-ST%20comes%0Afrom%20strong%20theoretical%20foundations%2C%20a%20more%20accurate%20keypoint%20scoring%20due%20to%0Asubpixel%20refinement%20and%20a%20cost%20designed%20for%20superior%20robustness%20to%20low%20saliency%0Akeypoints.%20As%20a%20result%2C%20BoNeSS-ST%20outperforms%20prior%20self-supervised%20local%0Afeature%20detectors%20on%20the%20planar%20homography%20estimation%20task%20and%20is%20on%20par%20with%0Athem%20on%20the%20epipolar%20geometry%20estimation%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.18767v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGood%2520Keypoints%2520for%2520the%2520Two-View%2520Geometry%2520Estimation%2520Problem%26entry.906535625%3DKonstantin%2520Pakulev%2520and%2520Alexander%2520Vakhitov%2520and%2520Gonzalo%2520Ferrer%26entry.1292438233%3D%2520%2520Local%2520features%2520are%2520essential%2520to%2520many%2520modern%2520downstream%2520applications.%250ATherefore%252C%2520it%2520is%2520of%2520interest%2520to%2520determine%2520the%2520properties%2520of%2520local%2520features%2520that%250Acontribute%2520to%2520the%2520downstream%2520performance%2520for%2520a%2520better%2520design%2520of%2520feature%250Adetectors%2520and%2520descriptors.%2520In%2520our%2520work%252C%2520we%2520propose%2520a%2520new%2520theoretical%2520model%2520for%250Ascoring%2520feature%2520points%2520%2528keypoints%2529%2520in%2520the%2520context%2520of%2520the%2520two-view%2520geometry%250Aestimation%2520problem.%2520The%2520model%2520determines%2520two%2520properties%2520that%2520a%2520good%2520keypoint%250Afor%2520solving%2520the%2520homography%2520estimation%2520problem%2520should%2520have%253A%2520be%2520repeatable%2520and%250Ahave%2520a%2520small%2520expected%2520measurement%2520error.%2520This%2520result%2520provides%2520key%2520insights%2520into%250Awhy%2520maximizing%2520the%2520number%2520of%2520correspondences%2520doesn%2527t%2520always%2520lead%2520to%2520better%250Ahomography%2520estimation%2520accuracy.%2520We%2520use%2520the%2520developed%2520model%2520to%2520design%2520a%2520method%250Athat%2520detects%2520keypoints%2520that%2520benefit%2520the%2520homography%2520estimation%2520and%2520introduce%2520the%250ABounded%2520NeSS-ST%2520%2528BoNeSS-ST%2529%2520keypoint%2520detector.%2520The%2520novelty%2520of%2520BoNeSS-ST%2520comes%250Afrom%2520strong%2520theoretical%2520foundations%252C%2520a%2520more%2520accurate%2520keypoint%2520scoring%2520due%2520to%250Asubpixel%2520refinement%2520and%2520a%2520cost%2520designed%2520for%2520superior%2520robustness%2520to%2520low%2520saliency%250Akeypoints.%2520As%2520a%2520result%252C%2520BoNeSS-ST%2520outperforms%2520prior%2520self-supervised%2520local%250Afeature%2520detectors%2520on%2520the%2520planar%2520homography%2520estimation%2520task%2520and%2520is%2520on%2520par%2520with%250Athem%2520on%2520the%2520epipolar%2520geometry%2520estimation%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.18767v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Good%20Keypoints%20for%20the%20Two-View%20Geometry%20Estimation%20Problem&entry.906535625=Konstantin%20Pakulev%20and%20Alexander%20Vakhitov%20and%20Gonzalo%20Ferrer&entry.1292438233=%20%20Local%20features%20are%20essential%20to%20many%20modern%20downstream%20applications.%0ATherefore%2C%20it%20is%20of%20interest%20to%20determine%20the%20properties%20of%20local%20features%20that%0Acontribute%20to%20the%20downstream%20performance%20for%20a%20better%20design%20of%20feature%0Adetectors%20and%20descriptors.%20In%20our%20work%2C%20we%20propose%20a%20new%20theoretical%20model%20for%0Ascoring%20feature%20points%20%28keypoints%29%20in%20the%20context%20of%20the%20two-view%20geometry%0Aestimation%20problem.%20The%20model%20determines%20two%20properties%20that%20a%20good%20keypoint%0Afor%20solving%20the%20homography%20estimation%20problem%20should%20have%3A%20be%20repeatable%20and%0Ahave%20a%20small%20expected%20measurement%20error.%20This%20result%20provides%20key%20insights%20into%0Awhy%20maximizing%20the%20number%20of%20correspondences%20doesn%27t%20always%20lead%20to%20better%0Ahomography%20estimation%20accuracy.%20We%20use%20the%20developed%20model%20to%20design%20a%20method%0Athat%20detects%20keypoints%20that%20benefit%20the%20homography%20estimation%20and%20introduce%20the%0ABounded%20NeSS-ST%20%28BoNeSS-ST%29%20keypoint%20detector.%20The%20novelty%20of%20BoNeSS-ST%20comes%0Afrom%20strong%20theoretical%20foundations%2C%20a%20more%20accurate%20keypoint%20scoring%20due%20to%0Asubpixel%20refinement%20and%20a%20cost%20designed%20for%20superior%20robustness%20to%20low%20saliency%0Akeypoints.%20As%20a%20result%2C%20BoNeSS-ST%20outperforms%20prior%20self-supervised%20local%0Afeature%20detectors%20on%20the%20planar%20homography%20estimation%20task%20and%20is%20on%20par%20with%0Athem%20on%20the%20epipolar%20geometry%20estimation%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.18767v2&entry.124074799=Read"},
{"title": "Chameleon: A Flexible Data-mixing Framework for Language Model\n  Pretraining and Finetuning", "author": "Wanyun Xie and Francesco Tonin and Volkan Cevher", "abstract": "  Training data mixtures greatly impact the generalization performance of large\nlanguage models. Existing domain reweighting methods often rely on costly\nweight computations and require retraining when new data is introduced. To this\nend, we introduce a flexible and efficient data mixing framework, Chameleon,\nthat employs leverage scores to quantify domain importance within a learned\nembedding space. We first construct a domain affinity matrix over domain\nembeddings. The induced leverage scores determine a mixture that upweights\ndomains sharing common representations in embedding space. This formulation\nallows direct transfer to new data by computing the new domain embeddings. In\nexperiments, we demonstrate improvements over three key scenarios: (i) our\ncomputed weights improve performance on pretraining domains with a fraction of\nthe compute of existing methods; (ii) Chameleon can adapt to data changes\nwithout proxy retraining, boosting few-shot reasoning accuracies when\ntransferred to new data; (iii) our method enables efficient domain reweighting\nin finetuning, consistently improving test perplexity on all finetuning domains\nover uniform mixture. Our code is available at\nhttps://github.com/LIONS-EPFL/Chameleon.\n", "link": "http://arxiv.org/abs/2505.24844v1", "date": "2025-05-30", "relevancy": 2.6598, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5714}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5166}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5079}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chameleon%3A%20A%20Flexible%20Data-mixing%20Framework%20for%20Language%20Model%0A%20%20Pretraining%20and%20Finetuning&body=Title%3A%20Chameleon%3A%20A%20Flexible%20Data-mixing%20Framework%20for%20Language%20Model%0A%20%20Pretraining%20and%20Finetuning%0AAuthor%3A%20Wanyun%20Xie%20and%20Francesco%20Tonin%20and%20Volkan%20Cevher%0AAbstract%3A%20%20%20Training%20data%20mixtures%20greatly%20impact%20the%20generalization%20performance%20of%20large%0Alanguage%20models.%20Existing%20domain%20reweighting%20methods%20often%20rely%20on%20costly%0Aweight%20computations%20and%20require%20retraining%20when%20new%20data%20is%20introduced.%20To%20this%0Aend%2C%20we%20introduce%20a%20flexible%20and%20efficient%20data%20mixing%20framework%2C%20Chameleon%2C%0Athat%20employs%20leverage%20scores%20to%20quantify%20domain%20importance%20within%20a%20learned%0Aembedding%20space.%20We%20first%20construct%20a%20domain%20affinity%20matrix%20over%20domain%0Aembeddings.%20The%20induced%20leverage%20scores%20determine%20a%20mixture%20that%20upweights%0Adomains%20sharing%20common%20representations%20in%20embedding%20space.%20This%20formulation%0Aallows%20direct%20transfer%20to%20new%20data%20by%20computing%20the%20new%20domain%20embeddings.%20In%0Aexperiments%2C%20we%20demonstrate%20improvements%20over%20three%20key%20scenarios%3A%20%28i%29%20our%0Acomputed%20weights%20improve%20performance%20on%20pretraining%20domains%20with%20a%20fraction%20of%0Athe%20compute%20of%20existing%20methods%3B%20%28ii%29%20Chameleon%20can%20adapt%20to%20data%20changes%0Awithout%20proxy%20retraining%2C%20boosting%20few-shot%20reasoning%20accuracies%20when%0Atransferred%20to%20new%20data%3B%20%28iii%29%20our%20method%20enables%20efficient%20domain%20reweighting%0Ain%20finetuning%2C%20consistently%20improving%20test%20perplexity%20on%20all%20finetuning%20domains%0Aover%20uniform%20mixture.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/LIONS-EPFL/Chameleon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24844v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChameleon%253A%2520A%2520Flexible%2520Data-mixing%2520Framework%2520for%2520Language%2520Model%250A%2520%2520Pretraining%2520and%2520Finetuning%26entry.906535625%3DWanyun%2520Xie%2520and%2520Francesco%2520Tonin%2520and%2520Volkan%2520Cevher%26entry.1292438233%3D%2520%2520Training%2520data%2520mixtures%2520greatly%2520impact%2520the%2520generalization%2520performance%2520of%2520large%250Alanguage%2520models.%2520Existing%2520domain%2520reweighting%2520methods%2520often%2520rely%2520on%2520costly%250Aweight%2520computations%2520and%2520require%2520retraining%2520when%2520new%2520data%2520is%2520introduced.%2520To%2520this%250Aend%252C%2520we%2520introduce%2520a%2520flexible%2520and%2520efficient%2520data%2520mixing%2520framework%252C%2520Chameleon%252C%250Athat%2520employs%2520leverage%2520scores%2520to%2520quantify%2520domain%2520importance%2520within%2520a%2520learned%250Aembedding%2520space.%2520We%2520first%2520construct%2520a%2520domain%2520affinity%2520matrix%2520over%2520domain%250Aembeddings.%2520The%2520induced%2520leverage%2520scores%2520determine%2520a%2520mixture%2520that%2520upweights%250Adomains%2520sharing%2520common%2520representations%2520in%2520embedding%2520space.%2520This%2520formulation%250Aallows%2520direct%2520transfer%2520to%2520new%2520data%2520by%2520computing%2520the%2520new%2520domain%2520embeddings.%2520In%250Aexperiments%252C%2520we%2520demonstrate%2520improvements%2520over%2520three%2520key%2520scenarios%253A%2520%2528i%2529%2520our%250Acomputed%2520weights%2520improve%2520performance%2520on%2520pretraining%2520domains%2520with%2520a%2520fraction%2520of%250Athe%2520compute%2520of%2520existing%2520methods%253B%2520%2528ii%2529%2520Chameleon%2520can%2520adapt%2520to%2520data%2520changes%250Awithout%2520proxy%2520retraining%252C%2520boosting%2520few-shot%2520reasoning%2520accuracies%2520when%250Atransferred%2520to%2520new%2520data%253B%2520%2528iii%2529%2520our%2520method%2520enables%2520efficient%2520domain%2520reweighting%250Ain%2520finetuning%252C%2520consistently%2520improving%2520test%2520perplexity%2520on%2520all%2520finetuning%2520domains%250Aover%2520uniform%2520mixture.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/LIONS-EPFL/Chameleon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24844v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chameleon%3A%20A%20Flexible%20Data-mixing%20Framework%20for%20Language%20Model%0A%20%20Pretraining%20and%20Finetuning&entry.906535625=Wanyun%20Xie%20and%20Francesco%20Tonin%20and%20Volkan%20Cevher&entry.1292438233=%20%20Training%20data%20mixtures%20greatly%20impact%20the%20generalization%20performance%20of%20large%0Alanguage%20models.%20Existing%20domain%20reweighting%20methods%20often%20rely%20on%20costly%0Aweight%20computations%20and%20require%20retraining%20when%20new%20data%20is%20introduced.%20To%20this%0Aend%2C%20we%20introduce%20a%20flexible%20and%20efficient%20data%20mixing%20framework%2C%20Chameleon%2C%0Athat%20employs%20leverage%20scores%20to%20quantify%20domain%20importance%20within%20a%20learned%0Aembedding%20space.%20We%20first%20construct%20a%20domain%20affinity%20matrix%20over%20domain%0Aembeddings.%20The%20induced%20leverage%20scores%20determine%20a%20mixture%20that%20upweights%0Adomains%20sharing%20common%20representations%20in%20embedding%20space.%20This%20formulation%0Aallows%20direct%20transfer%20to%20new%20data%20by%20computing%20the%20new%20domain%20embeddings.%20In%0Aexperiments%2C%20we%20demonstrate%20improvements%20over%20three%20key%20scenarios%3A%20%28i%29%20our%0Acomputed%20weights%20improve%20performance%20on%20pretraining%20domains%20with%20a%20fraction%20of%0Athe%20compute%20of%20existing%20methods%3B%20%28ii%29%20Chameleon%20can%20adapt%20to%20data%20changes%0Awithout%20proxy%20retraining%2C%20boosting%20few-shot%20reasoning%20accuracies%20when%0Atransferred%20to%20new%20data%3B%20%28iii%29%20our%20method%20enables%20efficient%20domain%20reweighting%0Ain%20finetuning%2C%20consistently%20improving%20test%20perplexity%20on%20all%20finetuning%20domains%0Aover%20uniform%20mixture.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/LIONS-EPFL/Chameleon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24844v1&entry.124074799=Read"},
{"title": "Re-ttention: Ultra Sparse Visual Generation via Attention Statistical\n  Reshape", "author": "Ruichen Chen and Keith G. Mills and Liyao Jiang and Chao Gao and Di Niu", "abstract": "  Diffusion Transformers (DiT) have become the de-facto model for generating\nhigh-quality visual content like videos and images. A huge bottleneck is the\nattention mechanism where complexity scales quadratically with resolution and\nvideo length. One logical way to lessen this burden is sparse attention, where\nonly a subset of tokens or patches are included in the calculation. However,\nexisting techniques fail to preserve visual quality at extremely high sparsity\nlevels and might even incur non-negligible compute overheads. % To address this\nconcern, we propose Re-ttention, which implements very high sparse attention\nfor visual generation models by leveraging the temporal redundancy of Diffusion\nModels to overcome the probabilistic normalization shift within the attention\nmechanism. Specifically, Re-ttention reshapes attention scores based on the\nprior softmax distribution history in order to preserve the visual quality of\nthe full quadratic attention at very high sparsity levels. % Experimental\nresults on T2V/T2I models such as CogVideoX and the PixArt DiTs demonstrate\nthat Re-ttention requires as few as 3.1\\% of the tokens during inference,\noutperforming contemporary methods like FastDiTAttn, Sparse VideoGen and\nMInference. Further, we measure latency to show that our method can attain over\n45\\% end-to-end % and over 92\\% self-attention latency reduction on an H100 GPU\nat negligible overhead cost.\n  Code available online here:\n\\href{https://github.com/cccrrrccc/Re-ttention}{https://github.com/cccrrrccc/Re-ttention}\n", "link": "http://arxiv.org/abs/2505.22918v2", "date": "2025-05-30", "relevancy": 2.6572, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.7064}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.647}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Re-ttention%3A%20Ultra%20Sparse%20Visual%20Generation%20via%20Attention%20Statistical%0A%20%20Reshape&body=Title%3A%20Re-ttention%3A%20Ultra%20Sparse%20Visual%20Generation%20via%20Attention%20Statistical%0A%20%20Reshape%0AAuthor%3A%20Ruichen%20Chen%20and%20Keith%20G.%20Mills%20and%20Liyao%20Jiang%20and%20Chao%20Gao%20and%20Di%20Niu%0AAbstract%3A%20%20%20Diffusion%20Transformers%20%28DiT%29%20have%20become%20the%20de-facto%20model%20for%20generating%0Ahigh-quality%20visual%20content%20like%20videos%20and%20images.%20A%20huge%20bottleneck%20is%20the%0Aattention%20mechanism%20where%20complexity%20scales%20quadratically%20with%20resolution%20and%0Avideo%20length.%20One%20logical%20way%20to%20lessen%20this%20burden%20is%20sparse%20attention%2C%20where%0Aonly%20a%20subset%20of%20tokens%20or%20patches%20are%20included%20in%20the%20calculation.%20However%2C%0Aexisting%20techniques%20fail%20to%20preserve%20visual%20quality%20at%20extremely%20high%20sparsity%0Alevels%20and%20might%20even%20incur%20non-negligible%20compute%20overheads.%20%25%20To%20address%20this%0Aconcern%2C%20we%20propose%20Re-ttention%2C%20which%20implements%20very%20high%20sparse%20attention%0Afor%20visual%20generation%20models%20by%20leveraging%20the%20temporal%20redundancy%20of%20Diffusion%0AModels%20to%20overcome%20the%20probabilistic%20normalization%20shift%20within%20the%20attention%0Amechanism.%20Specifically%2C%20Re-ttention%20reshapes%20attention%20scores%20based%20on%20the%0Aprior%20softmax%20distribution%20history%20in%20order%20to%20preserve%20the%20visual%20quality%20of%0Athe%20full%20quadratic%20attention%20at%20very%20high%20sparsity%20levels.%20%25%20Experimental%0Aresults%20on%20T2V/T2I%20models%20such%20as%20CogVideoX%20and%20the%20PixArt%20DiTs%20demonstrate%0Athat%20Re-ttention%20requires%20as%20few%20as%203.1%5C%25%20of%20the%20tokens%20during%20inference%2C%0Aoutperforming%20contemporary%20methods%20like%20FastDiTAttn%2C%20Sparse%20VideoGen%20and%0AMInference.%20Further%2C%20we%20measure%20latency%20to%20show%20that%20our%20method%20can%20attain%20over%0A45%5C%25%20end-to-end%20%25%20and%20over%2092%5C%25%20self-attention%20latency%20reduction%20on%20an%20H100%20GPU%0Aat%20negligible%20overhead%20cost.%0A%20%20Code%20available%20online%20here%3A%0A%5Chref%7Bhttps%3A//github.com/cccrrrccc/Re-ttention%7D%7Bhttps%3A//github.com/cccrrrccc/Re-ttention%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22918v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRe-ttention%253A%2520Ultra%2520Sparse%2520Visual%2520Generation%2520via%2520Attention%2520Statistical%250A%2520%2520Reshape%26entry.906535625%3DRuichen%2520Chen%2520and%2520Keith%2520G.%2520Mills%2520and%2520Liyao%2520Jiang%2520and%2520Chao%2520Gao%2520and%2520Di%2520Niu%26entry.1292438233%3D%2520%2520Diffusion%2520Transformers%2520%2528DiT%2529%2520have%2520become%2520the%2520de-facto%2520model%2520for%2520generating%250Ahigh-quality%2520visual%2520content%2520like%2520videos%2520and%2520images.%2520A%2520huge%2520bottleneck%2520is%2520the%250Aattention%2520mechanism%2520where%2520complexity%2520scales%2520quadratically%2520with%2520resolution%2520and%250Avideo%2520length.%2520One%2520logical%2520way%2520to%2520lessen%2520this%2520burden%2520is%2520sparse%2520attention%252C%2520where%250Aonly%2520a%2520subset%2520of%2520tokens%2520or%2520patches%2520are%2520included%2520in%2520the%2520calculation.%2520However%252C%250Aexisting%2520techniques%2520fail%2520to%2520preserve%2520visual%2520quality%2520at%2520extremely%2520high%2520sparsity%250Alevels%2520and%2520might%2520even%2520incur%2520non-negligible%2520compute%2520overheads.%2520%2525%2520To%2520address%2520this%250Aconcern%252C%2520we%2520propose%2520Re-ttention%252C%2520which%2520implements%2520very%2520high%2520sparse%2520attention%250Afor%2520visual%2520generation%2520models%2520by%2520leveraging%2520the%2520temporal%2520redundancy%2520of%2520Diffusion%250AModels%2520to%2520overcome%2520the%2520probabilistic%2520normalization%2520shift%2520within%2520the%2520attention%250Amechanism.%2520Specifically%252C%2520Re-ttention%2520reshapes%2520attention%2520scores%2520based%2520on%2520the%250Aprior%2520softmax%2520distribution%2520history%2520in%2520order%2520to%2520preserve%2520the%2520visual%2520quality%2520of%250Athe%2520full%2520quadratic%2520attention%2520at%2520very%2520high%2520sparsity%2520levels.%2520%2525%2520Experimental%250Aresults%2520on%2520T2V/T2I%2520models%2520such%2520as%2520CogVideoX%2520and%2520the%2520PixArt%2520DiTs%2520demonstrate%250Athat%2520Re-ttention%2520requires%2520as%2520few%2520as%25203.1%255C%2525%2520of%2520the%2520tokens%2520during%2520inference%252C%250Aoutperforming%2520contemporary%2520methods%2520like%2520FastDiTAttn%252C%2520Sparse%2520VideoGen%2520and%250AMInference.%2520Further%252C%2520we%2520measure%2520latency%2520to%2520show%2520that%2520our%2520method%2520can%2520attain%2520over%250A45%255C%2525%2520end-to-end%2520%2525%2520and%2520over%252092%255C%2525%2520self-attention%2520latency%2520reduction%2520on%2520an%2520H100%2520GPU%250Aat%2520negligible%2520overhead%2520cost.%250A%2520%2520Code%2520available%2520online%2520here%253A%250A%255Chref%257Bhttps%253A//github.com/cccrrrccc/Re-ttention%257D%257Bhttps%253A//github.com/cccrrrccc/Re-ttention%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22918v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Re-ttention%3A%20Ultra%20Sparse%20Visual%20Generation%20via%20Attention%20Statistical%0A%20%20Reshape&entry.906535625=Ruichen%20Chen%20and%20Keith%20G.%20Mills%20and%20Liyao%20Jiang%20and%20Chao%20Gao%20and%20Di%20Niu&entry.1292438233=%20%20Diffusion%20Transformers%20%28DiT%29%20have%20become%20the%20de-facto%20model%20for%20generating%0Ahigh-quality%20visual%20content%20like%20videos%20and%20images.%20A%20huge%20bottleneck%20is%20the%0Aattention%20mechanism%20where%20complexity%20scales%20quadratically%20with%20resolution%20and%0Avideo%20length.%20One%20logical%20way%20to%20lessen%20this%20burden%20is%20sparse%20attention%2C%20where%0Aonly%20a%20subset%20of%20tokens%20or%20patches%20are%20included%20in%20the%20calculation.%20However%2C%0Aexisting%20techniques%20fail%20to%20preserve%20visual%20quality%20at%20extremely%20high%20sparsity%0Alevels%20and%20might%20even%20incur%20non-negligible%20compute%20overheads.%20%25%20To%20address%20this%0Aconcern%2C%20we%20propose%20Re-ttention%2C%20which%20implements%20very%20high%20sparse%20attention%0Afor%20visual%20generation%20models%20by%20leveraging%20the%20temporal%20redundancy%20of%20Diffusion%0AModels%20to%20overcome%20the%20probabilistic%20normalization%20shift%20within%20the%20attention%0Amechanism.%20Specifically%2C%20Re-ttention%20reshapes%20attention%20scores%20based%20on%20the%0Aprior%20softmax%20distribution%20history%20in%20order%20to%20preserve%20the%20visual%20quality%20of%0Athe%20full%20quadratic%20attention%20at%20very%20high%20sparsity%20levels.%20%25%20Experimental%0Aresults%20on%20T2V/T2I%20models%20such%20as%20CogVideoX%20and%20the%20PixArt%20DiTs%20demonstrate%0Athat%20Re-ttention%20requires%20as%20few%20as%203.1%5C%25%20of%20the%20tokens%20during%20inference%2C%0Aoutperforming%20contemporary%20methods%20like%20FastDiTAttn%2C%20Sparse%20VideoGen%20and%0AMInference.%20Further%2C%20we%20measure%20latency%20to%20show%20that%20our%20method%20can%20attain%20over%0A45%5C%25%20end-to-end%20%25%20and%20over%2092%5C%25%20self-attention%20latency%20reduction%20on%20an%20H100%20GPU%0Aat%20negligible%20overhead%20cost.%0A%20%20Code%20available%20online%20here%3A%0A%5Chref%7Bhttps%3A//github.com/cccrrrccc/Re-ttention%7D%7Bhttps%3A//github.com/cccrrrccc/Re-ttention%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22918v2&entry.124074799=Read"},
{"title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with\n  Foundation Models", "author": "Adam Stein and Aaditya Naik and Neelay Velingker and Mayur Naik and Eric Wong", "abstract": "  Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.\n", "link": "http://arxiv.org/abs/2505.24874v1", "date": "2025-05-30", "relevancy": 2.6529, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5481}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5481}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4955}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Road%20to%20Generalizable%20Neuro-Symbolic%20Learning%20Should%20be%20Paved%20with%0A%20%20Foundation%20Models&body=Title%3A%20The%20Road%20to%20Generalizable%20Neuro-Symbolic%20Learning%20Should%20be%20Paved%20with%0A%20%20Foundation%20Models%0AAuthor%3A%20Adam%20Stein%20and%20Aaditya%20Naik%20and%20Neelay%20Velingker%20and%20Mayur%20Naik%20and%20Eric%20Wong%0AAbstract%3A%20%20%20Neuro-symbolic%20learning%20was%20proposed%20to%20address%20challenges%20with%20training%0Aneural%20networks%20for%20complex%20reasoning%20tasks%20with%20the%20added%20benefits%20of%0Ainterpretability%2C%20reliability%2C%20and%20efficiency.%20Neuro-symbolic%20learning%20methods%0Atraditionally%20train%20neural%20models%20in%20conjunction%20with%20symbolic%20programs%2C%20but%0Athey%20face%20significant%20challenges%20that%20limit%20them%20to%20simplistic%20problems.%20On%20the%0Aother%20hand%2C%20purely-neural%20foundation%20models%20now%20reach%20state-of-the-art%0Aperformance%20through%20prompting%20rather%20than%20training%2C%20but%20they%20are%20often%0Aunreliable%20and%20lack%20interpretability.%20Supplementing%20foundation%20models%20with%0Asymbolic%20programs%2C%20which%20we%20call%20neuro-symbolic%20prompting%2C%20provides%20a%20way%20to%0Ause%20these%20models%20for%20complex%20reasoning%20tasks.%20Doing%20so%20raises%20the%20question%3A%0AWhat%20role%20does%20specialized%20model%20training%20as%20part%20of%20neuro-symbolic%20learning%0Ahave%20in%20the%20age%20of%20foundation%20models%3F%20To%20explore%20this%20question%2C%20we%20highlight%0Athree%20pitfalls%20of%20traditional%20neuro-symbolic%20learning%20with%20respect%20to%20the%0Acompute%2C%20data%2C%20and%20programs%20leading%20to%20generalization%20problems.%20This%20position%0Apaper%20argues%20that%20foundation%20models%20enable%20generalizable%20neuro-symbolic%0Asolutions%2C%20offering%20a%20path%20towards%20achieving%20the%20original%20goals%20of%0Aneuro-symbolic%20learning%20without%20the%20downsides%20of%20training%20from%20scratch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24874v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Road%2520to%2520Generalizable%2520Neuro-Symbolic%2520Learning%2520Should%2520be%2520Paved%2520with%250A%2520%2520Foundation%2520Models%26entry.906535625%3DAdam%2520Stein%2520and%2520Aaditya%2520Naik%2520and%2520Neelay%2520Velingker%2520and%2520Mayur%2520Naik%2520and%2520Eric%2520Wong%26entry.1292438233%3D%2520%2520Neuro-symbolic%2520learning%2520was%2520proposed%2520to%2520address%2520challenges%2520with%2520training%250Aneural%2520networks%2520for%2520complex%2520reasoning%2520tasks%2520with%2520the%2520added%2520benefits%2520of%250Ainterpretability%252C%2520reliability%252C%2520and%2520efficiency.%2520Neuro-symbolic%2520learning%2520methods%250Atraditionally%2520train%2520neural%2520models%2520in%2520conjunction%2520with%2520symbolic%2520programs%252C%2520but%250Athey%2520face%2520significant%2520challenges%2520that%2520limit%2520them%2520to%2520simplistic%2520problems.%2520On%2520the%250Aother%2520hand%252C%2520purely-neural%2520foundation%2520models%2520now%2520reach%2520state-of-the-art%250Aperformance%2520through%2520prompting%2520rather%2520than%2520training%252C%2520but%2520they%2520are%2520often%250Aunreliable%2520and%2520lack%2520interpretability.%2520Supplementing%2520foundation%2520models%2520with%250Asymbolic%2520programs%252C%2520which%2520we%2520call%2520neuro-symbolic%2520prompting%252C%2520provides%2520a%2520way%2520to%250Ause%2520these%2520models%2520for%2520complex%2520reasoning%2520tasks.%2520Doing%2520so%2520raises%2520the%2520question%253A%250AWhat%2520role%2520does%2520specialized%2520model%2520training%2520as%2520part%2520of%2520neuro-symbolic%2520learning%250Ahave%2520in%2520the%2520age%2520of%2520foundation%2520models%253F%2520To%2520explore%2520this%2520question%252C%2520we%2520highlight%250Athree%2520pitfalls%2520of%2520traditional%2520neuro-symbolic%2520learning%2520with%2520respect%2520to%2520the%250Acompute%252C%2520data%252C%2520and%2520programs%2520leading%2520to%2520generalization%2520problems.%2520This%2520position%250Apaper%2520argues%2520that%2520foundation%2520models%2520enable%2520generalizable%2520neuro-symbolic%250Asolutions%252C%2520offering%2520a%2520path%2520towards%2520achieving%2520the%2520original%2520goals%2520of%250Aneuro-symbolic%2520learning%2520without%2520the%2520downsides%2520of%2520training%2520from%2520scratch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24874v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Road%20to%20Generalizable%20Neuro-Symbolic%20Learning%20Should%20be%20Paved%20with%0A%20%20Foundation%20Models&entry.906535625=Adam%20Stein%20and%20Aaditya%20Naik%20and%20Neelay%20Velingker%20and%20Mayur%20Naik%20and%20Eric%20Wong&entry.1292438233=%20%20Neuro-symbolic%20learning%20was%20proposed%20to%20address%20challenges%20with%20training%0Aneural%20networks%20for%20complex%20reasoning%20tasks%20with%20the%20added%20benefits%20of%0Ainterpretability%2C%20reliability%2C%20and%20efficiency.%20Neuro-symbolic%20learning%20methods%0Atraditionally%20train%20neural%20models%20in%20conjunction%20with%20symbolic%20programs%2C%20but%0Athey%20face%20significant%20challenges%20that%20limit%20them%20to%20simplistic%20problems.%20On%20the%0Aother%20hand%2C%20purely-neural%20foundation%20models%20now%20reach%20state-of-the-art%0Aperformance%20through%20prompting%20rather%20than%20training%2C%20but%20they%20are%20often%0Aunreliable%20and%20lack%20interpretability.%20Supplementing%20foundation%20models%20with%0Asymbolic%20programs%2C%20which%20we%20call%20neuro-symbolic%20prompting%2C%20provides%20a%20way%20to%0Ause%20these%20models%20for%20complex%20reasoning%20tasks.%20Doing%20so%20raises%20the%20question%3A%0AWhat%20role%20does%20specialized%20model%20training%20as%20part%20of%20neuro-symbolic%20learning%0Ahave%20in%20the%20age%20of%20foundation%20models%3F%20To%20explore%20this%20question%2C%20we%20highlight%0Athree%20pitfalls%20of%20traditional%20neuro-symbolic%20learning%20with%20respect%20to%20the%0Acompute%2C%20data%2C%20and%20programs%20leading%20to%20generalization%20problems.%20This%20position%0Apaper%20argues%20that%20foundation%20models%20enable%20generalizable%20neuro-symbolic%0Asolutions%2C%20offering%20a%20path%20towards%20achieving%20the%20original%20goals%20of%0Aneuro-symbolic%20learning%20without%20the%20downsides%20of%20training%20from%20scratch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24874v1&entry.124074799=Read"},
{"title": "ENACT: Entropy-based Clustering of Attention Input for Reducing the\n  Computational Needs of Object Detection Transformers", "author": "Giorgos Savathrakis and Antonis Argyros", "abstract": "  Transformers demonstrate competitive performance in terms of precision on the\nproblem of vision-based object detection. However, they require considerable\ncomputational resources due to the quadratic size of the attention weights. In\nthis work, we propose to cluster the transformer input on the basis of its\nentropy, due to its similarity between same object pixels. This is expected to\nreduce GPU usage during training, while maintaining reasonable accuracy. This\nidea is realized with an implemented module that is called ENtropy-based\nAttention Clustering for detection Transformers (ENACT), which serves as a\nplug-in to any multi-head self-attention based transformer network. Experiments\non the COCO object detection dataset and three detection transformers\ndemonstrate that the requirements on memory are reduced, while the detection\naccuracy is degraded only slightly. The code of the ENACT module is available\nat https://github.com/GSavathrakis/ENACT.\n", "link": "http://arxiv.org/abs/2409.07541v2", "date": "2025-05-30", "relevancy": 2.6264, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.528}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5272}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5206}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ENACT%3A%20Entropy-based%20Clustering%20of%20Attention%20Input%20for%20Reducing%20the%0A%20%20Computational%20Needs%20of%20Object%20Detection%20Transformers&body=Title%3A%20ENACT%3A%20Entropy-based%20Clustering%20of%20Attention%20Input%20for%20Reducing%20the%0A%20%20Computational%20Needs%20of%20Object%20Detection%20Transformers%0AAuthor%3A%20Giorgos%20Savathrakis%20and%20Antonis%20Argyros%0AAbstract%3A%20%20%20Transformers%20demonstrate%20competitive%20performance%20in%20terms%20of%20precision%20on%20the%0Aproblem%20of%20vision-based%20object%20detection.%20However%2C%20they%20require%20considerable%0Acomputational%20resources%20due%20to%20the%20quadratic%20size%20of%20the%20attention%20weights.%20In%0Athis%20work%2C%20we%20propose%20to%20cluster%20the%20transformer%20input%20on%20the%20basis%20of%20its%0Aentropy%2C%20due%20to%20its%20similarity%20between%20same%20object%20pixels.%20This%20is%20expected%20to%0Areduce%20GPU%20usage%20during%20training%2C%20while%20maintaining%20reasonable%20accuracy.%20This%0Aidea%20is%20realized%20with%20an%20implemented%20module%20that%20is%20called%20ENtropy-based%0AAttention%20Clustering%20for%20detection%20Transformers%20%28ENACT%29%2C%20which%20serves%20as%20a%0Aplug-in%20to%20any%20multi-head%20self-attention%20based%20transformer%20network.%20Experiments%0Aon%20the%20COCO%20object%20detection%20dataset%20and%20three%20detection%20transformers%0Ademonstrate%20that%20the%20requirements%20on%20memory%20are%20reduced%2C%20while%20the%20detection%0Aaccuracy%20is%20degraded%20only%20slightly.%20The%20code%20of%20the%20ENACT%20module%20is%20available%0Aat%20https%3A//github.com/GSavathrakis/ENACT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07541v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DENACT%253A%2520Entropy-based%2520Clustering%2520of%2520Attention%2520Input%2520for%2520Reducing%2520the%250A%2520%2520Computational%2520Needs%2520of%2520Object%2520Detection%2520Transformers%26entry.906535625%3DGiorgos%2520Savathrakis%2520and%2520Antonis%2520Argyros%26entry.1292438233%3D%2520%2520Transformers%2520demonstrate%2520competitive%2520performance%2520in%2520terms%2520of%2520precision%2520on%2520the%250Aproblem%2520of%2520vision-based%2520object%2520detection.%2520However%252C%2520they%2520require%2520considerable%250Acomputational%2520resources%2520due%2520to%2520the%2520quadratic%2520size%2520of%2520the%2520attention%2520weights.%2520In%250Athis%2520work%252C%2520we%2520propose%2520to%2520cluster%2520the%2520transformer%2520input%2520on%2520the%2520basis%2520of%2520its%250Aentropy%252C%2520due%2520to%2520its%2520similarity%2520between%2520same%2520object%2520pixels.%2520This%2520is%2520expected%2520to%250Areduce%2520GPU%2520usage%2520during%2520training%252C%2520while%2520maintaining%2520reasonable%2520accuracy.%2520This%250Aidea%2520is%2520realized%2520with%2520an%2520implemented%2520module%2520that%2520is%2520called%2520ENtropy-based%250AAttention%2520Clustering%2520for%2520detection%2520Transformers%2520%2528ENACT%2529%252C%2520which%2520serves%2520as%2520a%250Aplug-in%2520to%2520any%2520multi-head%2520self-attention%2520based%2520transformer%2520network.%2520Experiments%250Aon%2520the%2520COCO%2520object%2520detection%2520dataset%2520and%2520three%2520detection%2520transformers%250Ademonstrate%2520that%2520the%2520requirements%2520on%2520memory%2520are%2520reduced%252C%2520while%2520the%2520detection%250Aaccuracy%2520is%2520degraded%2520only%2520slightly.%2520The%2520code%2520of%2520the%2520ENACT%2520module%2520is%2520available%250Aat%2520https%253A//github.com/GSavathrakis/ENACT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07541v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ENACT%3A%20Entropy-based%20Clustering%20of%20Attention%20Input%20for%20Reducing%20the%0A%20%20Computational%20Needs%20of%20Object%20Detection%20Transformers&entry.906535625=Giorgos%20Savathrakis%20and%20Antonis%20Argyros&entry.1292438233=%20%20Transformers%20demonstrate%20competitive%20performance%20in%20terms%20of%20precision%20on%20the%0Aproblem%20of%20vision-based%20object%20detection.%20However%2C%20they%20require%20considerable%0Acomputational%20resources%20due%20to%20the%20quadratic%20size%20of%20the%20attention%20weights.%20In%0Athis%20work%2C%20we%20propose%20to%20cluster%20the%20transformer%20input%20on%20the%20basis%20of%20its%0Aentropy%2C%20due%20to%20its%20similarity%20between%20same%20object%20pixels.%20This%20is%20expected%20to%0Areduce%20GPU%20usage%20during%20training%2C%20while%20maintaining%20reasonable%20accuracy.%20This%0Aidea%20is%20realized%20with%20an%20implemented%20module%20that%20is%20called%20ENtropy-based%0AAttention%20Clustering%20for%20detection%20Transformers%20%28ENACT%29%2C%20which%20serves%20as%20a%0Aplug-in%20to%20any%20multi-head%20self-attention%20based%20transformer%20network.%20Experiments%0Aon%20the%20COCO%20object%20detection%20dataset%20and%20three%20detection%20transformers%0Ademonstrate%20that%20the%20requirements%20on%20memory%20are%20reduced%2C%20while%20the%20detection%0Aaccuracy%20is%20degraded%20only%20slightly.%20The%20code%20of%20the%20ENACT%20module%20is%20available%0Aat%20https%3A//github.com/GSavathrakis/ENACT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07541v2&entry.124074799=Read"},
{"title": "Logits-Based Finetuning", "author": "Jingyao Li and Senqiao Yang and Sitong Wu and Han Shi and Chuanyang Zheng and Hong Xu and Jiaya Jia", "abstract": "  The core of out-of-distribution (OOD) detection is to learn the\nin-distribution (ID) representation, which is distinguishable from OOD samples.\nPrevious work applied recognition-based methods to learn the ID features, which\ntend to learn shortcuts instead of comprehensive representations. In this work,\nwe find surprisingly that simply using reconstruction-based methods could boost\nthe performance of OOD detection significantly. We deeply explore the main\ncontributors of OOD detection and find that reconstruction-based pretext tasks\nhave the potential to provide a generally applicable and efficacious prior,\nwhich benefits the model in learning intrinsic data distributions of the ID\ndataset. Specifically, we take Masked Image Modeling as a pretext task for our\nOOD detection framework (MOOD). Without bells and whistles, MOOD outperforms\nprevious SOTA of one-class OOD detection by 5.7%, multi-class OOD detection by\n3.0%, and near-distribution OOD detection by 2.1%. It even defeats the\n10-shot-per-class outlier exposure OOD detection, although we do not include\nany OOD samples for our detection. Codes are available at\nhttps://github.com/JulietLJY/MOOD.\n", "link": "http://arxiv.org/abs/2505.24461v1", "date": "2025-05-30", "relevancy": 2.6198, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.531}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.524}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Logits-Based%20Finetuning&body=Title%3A%20Logits-Based%20Finetuning%0AAuthor%3A%20Jingyao%20Li%20and%20Senqiao%20Yang%20and%20Sitong%20Wu%20and%20Han%20Shi%20and%20Chuanyang%20Zheng%20and%20Hong%20Xu%20and%20Jiaya%20Jia%0AAbstract%3A%20%20%20The%20core%20of%20out-of-distribution%20%28OOD%29%20detection%20is%20to%20learn%20the%0Ain-distribution%20%28ID%29%20representation%2C%20which%20is%20distinguishable%20from%20OOD%20samples.%0APrevious%20work%20applied%20recognition-based%20methods%20to%20learn%20the%20ID%20features%2C%20which%0Atend%20to%20learn%20shortcuts%20instead%20of%20comprehensive%20representations.%20In%20this%20work%2C%0Awe%20find%20surprisingly%20that%20simply%20using%20reconstruction-based%20methods%20could%20boost%0Athe%20performance%20of%20OOD%20detection%20significantly.%20We%20deeply%20explore%20the%20main%0Acontributors%20of%20OOD%20detection%20and%20find%20that%20reconstruction-based%20pretext%20tasks%0Ahave%20the%20potential%20to%20provide%20a%20generally%20applicable%20and%20efficacious%20prior%2C%0Awhich%20benefits%20the%20model%20in%20learning%20intrinsic%20data%20distributions%20of%20the%20ID%0Adataset.%20Specifically%2C%20we%20take%20Masked%20Image%20Modeling%20as%20a%20pretext%20task%20for%20our%0AOOD%20detection%20framework%20%28MOOD%29.%20Without%20bells%20and%20whistles%2C%20MOOD%20outperforms%0Aprevious%20SOTA%20of%20one-class%20OOD%20detection%20by%205.7%25%2C%20multi-class%20OOD%20detection%20by%0A3.0%25%2C%20and%20near-distribution%20OOD%20detection%20by%202.1%25.%20It%20even%20defeats%20the%0A10-shot-per-class%20outlier%20exposure%20OOD%20detection%2C%20although%20we%20do%20not%20include%0Aany%20OOD%20samples%20for%20our%20detection.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/JulietLJY/MOOD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24461v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLogits-Based%2520Finetuning%26entry.906535625%3DJingyao%2520Li%2520and%2520Senqiao%2520Yang%2520and%2520Sitong%2520Wu%2520and%2520Han%2520Shi%2520and%2520Chuanyang%2520Zheng%2520and%2520Hong%2520Xu%2520and%2520Jiaya%2520Jia%26entry.1292438233%3D%2520%2520The%2520core%2520of%2520out-of-distribution%2520%2528OOD%2529%2520detection%2520is%2520to%2520learn%2520the%250Ain-distribution%2520%2528ID%2529%2520representation%252C%2520which%2520is%2520distinguishable%2520from%2520OOD%2520samples.%250APrevious%2520work%2520applied%2520recognition-based%2520methods%2520to%2520learn%2520the%2520ID%2520features%252C%2520which%250Atend%2520to%2520learn%2520shortcuts%2520instead%2520of%2520comprehensive%2520representations.%2520In%2520this%2520work%252C%250Awe%2520find%2520surprisingly%2520that%2520simply%2520using%2520reconstruction-based%2520methods%2520could%2520boost%250Athe%2520performance%2520of%2520OOD%2520detection%2520significantly.%2520We%2520deeply%2520explore%2520the%2520main%250Acontributors%2520of%2520OOD%2520detection%2520and%2520find%2520that%2520reconstruction-based%2520pretext%2520tasks%250Ahave%2520the%2520potential%2520to%2520provide%2520a%2520generally%2520applicable%2520and%2520efficacious%2520prior%252C%250Awhich%2520benefits%2520the%2520model%2520in%2520learning%2520intrinsic%2520data%2520distributions%2520of%2520the%2520ID%250Adataset.%2520Specifically%252C%2520we%2520take%2520Masked%2520Image%2520Modeling%2520as%2520a%2520pretext%2520task%2520for%2520our%250AOOD%2520detection%2520framework%2520%2528MOOD%2529.%2520Without%2520bells%2520and%2520whistles%252C%2520MOOD%2520outperforms%250Aprevious%2520SOTA%2520of%2520one-class%2520OOD%2520detection%2520by%25205.7%2525%252C%2520multi-class%2520OOD%2520detection%2520by%250A3.0%2525%252C%2520and%2520near-distribution%2520OOD%2520detection%2520by%25202.1%2525.%2520It%2520even%2520defeats%2520the%250A10-shot-per-class%2520outlier%2520exposure%2520OOD%2520detection%252C%2520although%2520we%2520do%2520not%2520include%250Aany%2520OOD%2520samples%2520for%2520our%2520detection.%2520Codes%2520are%2520available%2520at%250Ahttps%253A//github.com/JulietLJY/MOOD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24461v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Logits-Based%20Finetuning&entry.906535625=Jingyao%20Li%20and%20Senqiao%20Yang%20and%20Sitong%20Wu%20and%20Han%20Shi%20and%20Chuanyang%20Zheng%20and%20Hong%20Xu%20and%20Jiaya%20Jia&entry.1292438233=%20%20The%20core%20of%20out-of-distribution%20%28OOD%29%20detection%20is%20to%20learn%20the%0Ain-distribution%20%28ID%29%20representation%2C%20which%20is%20distinguishable%20from%20OOD%20samples.%0APrevious%20work%20applied%20recognition-based%20methods%20to%20learn%20the%20ID%20features%2C%20which%0Atend%20to%20learn%20shortcuts%20instead%20of%20comprehensive%20representations.%20In%20this%20work%2C%0Awe%20find%20surprisingly%20that%20simply%20using%20reconstruction-based%20methods%20could%20boost%0Athe%20performance%20of%20OOD%20detection%20significantly.%20We%20deeply%20explore%20the%20main%0Acontributors%20of%20OOD%20detection%20and%20find%20that%20reconstruction-based%20pretext%20tasks%0Ahave%20the%20potential%20to%20provide%20a%20generally%20applicable%20and%20efficacious%20prior%2C%0Awhich%20benefits%20the%20model%20in%20learning%20intrinsic%20data%20distributions%20of%20the%20ID%0Adataset.%20Specifically%2C%20we%20take%20Masked%20Image%20Modeling%20as%20a%20pretext%20task%20for%20our%0AOOD%20detection%20framework%20%28MOOD%29.%20Without%20bells%20and%20whistles%2C%20MOOD%20outperforms%0Aprevious%20SOTA%20of%20one-class%20OOD%20detection%20by%205.7%25%2C%20multi-class%20OOD%20detection%20by%0A3.0%25%2C%20and%20near-distribution%20OOD%20detection%20by%202.1%25.%20It%20even%20defeats%20the%0A10-shot-per-class%20outlier%20exposure%20OOD%20detection%2C%20although%20we%20do%20not%20include%0Aany%20OOD%20samples%20for%20our%20detection.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/JulietLJY/MOOD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24461v1&entry.124074799=Read"},
{"title": "PCIE_Pose Solution for EgoExo4D Pose and Proficiency Estimation\n  Challenge", "author": "Feng Chen and Kanokphan Lertniphonphan and Qiancheng Yan and Xiaohui Fan and Jun Xie and Tao Zhang and Zhepeng Wang", "abstract": "  This report introduces our team's (PCIE_EgoPose) solutions for the EgoExo4D\nPose and Proficiency Estimation Challenges at CVPR2025. Focused on the\nintricate task of estimating 21 3D hand joints from RGB egocentric videos,\nwhich are complicated by subtle movements and frequent occlusions, we developed\nthe Hand Pose Vision Transformer (HP-ViT+). This architecture synergizes a\nVision Transformer and a CNN backbone, using weighted fusion to refine the hand\npose predictions. For the EgoExo4D Body Pose Challenge, we adopted a multimodal\nspatio-temporal feature integration strategy to address the complexities of\nbody pose estimation across dynamic contexts. Our methods achieved remarkable\nperformance: 8.31 PA-MPJPE in the Hand Pose Challenge and 11.25 MPJPE in the\nBody Pose Challenge, securing championship titles in both competitions. We\nextended our pose estimation solutions to the Proficiency Estimation task,\napplying core technologies such as transformer-based architectures. This\nextension enabled us to achieve a top-1 accuracy of 0.53, a SOTA result, in the\nDemonstrator Proficiency Estimation competition.\n", "link": "http://arxiv.org/abs/2505.24411v1", "date": "2025-05-30", "relevancy": 2.6102, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5294}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5263}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PCIE_Pose%20Solution%20for%20EgoExo4D%20Pose%20and%20Proficiency%20Estimation%0A%20%20Challenge&body=Title%3A%20PCIE_Pose%20Solution%20for%20EgoExo4D%20Pose%20and%20Proficiency%20Estimation%0A%20%20Challenge%0AAuthor%3A%20Feng%20Chen%20and%20Kanokphan%20Lertniphonphan%20and%20Qiancheng%20Yan%20and%20Xiaohui%20Fan%20and%20Jun%20Xie%20and%20Tao%20Zhang%20and%20Zhepeng%20Wang%0AAbstract%3A%20%20%20This%20report%20introduces%20our%20team%27s%20%28PCIE_EgoPose%29%20solutions%20for%20the%20EgoExo4D%0APose%20and%20Proficiency%20Estimation%20Challenges%20at%20CVPR2025.%20Focused%20on%20the%0Aintricate%20task%20of%20estimating%2021%203D%20hand%20joints%20from%20RGB%20egocentric%20videos%2C%0Awhich%20are%20complicated%20by%20subtle%20movements%20and%20frequent%20occlusions%2C%20we%20developed%0Athe%20Hand%20Pose%20Vision%20Transformer%20%28HP-ViT%2B%29.%20This%20architecture%20synergizes%20a%0AVision%20Transformer%20and%20a%20CNN%20backbone%2C%20using%20weighted%20fusion%20to%20refine%20the%20hand%0Apose%20predictions.%20For%20the%20EgoExo4D%20Body%20Pose%20Challenge%2C%20we%20adopted%20a%20multimodal%0Aspatio-temporal%20feature%20integration%20strategy%20to%20address%20the%20complexities%20of%0Abody%20pose%20estimation%20across%20dynamic%20contexts.%20Our%20methods%20achieved%20remarkable%0Aperformance%3A%208.31%20PA-MPJPE%20in%20the%20Hand%20Pose%20Challenge%20and%2011.25%20MPJPE%20in%20the%0ABody%20Pose%20Challenge%2C%20securing%20championship%20titles%20in%20both%20competitions.%20We%0Aextended%20our%20pose%20estimation%20solutions%20to%20the%20Proficiency%20Estimation%20task%2C%0Aapplying%20core%20technologies%20such%20as%20transformer-based%20architectures.%20This%0Aextension%20enabled%20us%20to%20achieve%20a%20top-1%20accuracy%20of%200.53%2C%20a%20SOTA%20result%2C%20in%20the%0ADemonstrator%20Proficiency%20Estimation%20competition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24411v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPCIE_Pose%2520Solution%2520for%2520EgoExo4D%2520Pose%2520and%2520Proficiency%2520Estimation%250A%2520%2520Challenge%26entry.906535625%3DFeng%2520Chen%2520and%2520Kanokphan%2520Lertniphonphan%2520and%2520Qiancheng%2520Yan%2520and%2520Xiaohui%2520Fan%2520and%2520Jun%2520Xie%2520and%2520Tao%2520Zhang%2520and%2520Zhepeng%2520Wang%26entry.1292438233%3D%2520%2520This%2520report%2520introduces%2520our%2520team%2527s%2520%2528PCIE_EgoPose%2529%2520solutions%2520for%2520the%2520EgoExo4D%250APose%2520and%2520Proficiency%2520Estimation%2520Challenges%2520at%2520CVPR2025.%2520Focused%2520on%2520the%250Aintricate%2520task%2520of%2520estimating%252021%25203D%2520hand%2520joints%2520from%2520RGB%2520egocentric%2520videos%252C%250Awhich%2520are%2520complicated%2520by%2520subtle%2520movements%2520and%2520frequent%2520occlusions%252C%2520we%2520developed%250Athe%2520Hand%2520Pose%2520Vision%2520Transformer%2520%2528HP-ViT%252B%2529.%2520This%2520architecture%2520synergizes%2520a%250AVision%2520Transformer%2520and%2520a%2520CNN%2520backbone%252C%2520using%2520weighted%2520fusion%2520to%2520refine%2520the%2520hand%250Apose%2520predictions.%2520For%2520the%2520EgoExo4D%2520Body%2520Pose%2520Challenge%252C%2520we%2520adopted%2520a%2520multimodal%250Aspatio-temporal%2520feature%2520integration%2520strategy%2520to%2520address%2520the%2520complexities%2520of%250Abody%2520pose%2520estimation%2520across%2520dynamic%2520contexts.%2520Our%2520methods%2520achieved%2520remarkable%250Aperformance%253A%25208.31%2520PA-MPJPE%2520in%2520the%2520Hand%2520Pose%2520Challenge%2520and%252011.25%2520MPJPE%2520in%2520the%250ABody%2520Pose%2520Challenge%252C%2520securing%2520championship%2520titles%2520in%2520both%2520competitions.%2520We%250Aextended%2520our%2520pose%2520estimation%2520solutions%2520to%2520the%2520Proficiency%2520Estimation%2520task%252C%250Aapplying%2520core%2520technologies%2520such%2520as%2520transformer-based%2520architectures.%2520This%250Aextension%2520enabled%2520us%2520to%2520achieve%2520a%2520top-1%2520accuracy%2520of%25200.53%252C%2520a%2520SOTA%2520result%252C%2520in%2520the%250ADemonstrator%2520Proficiency%2520Estimation%2520competition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24411v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PCIE_Pose%20Solution%20for%20EgoExo4D%20Pose%20and%20Proficiency%20Estimation%0A%20%20Challenge&entry.906535625=Feng%20Chen%20and%20Kanokphan%20Lertniphonphan%20and%20Qiancheng%20Yan%20and%20Xiaohui%20Fan%20and%20Jun%20Xie%20and%20Tao%20Zhang%20and%20Zhepeng%20Wang&entry.1292438233=%20%20This%20report%20introduces%20our%20team%27s%20%28PCIE_EgoPose%29%20solutions%20for%20the%20EgoExo4D%0APose%20and%20Proficiency%20Estimation%20Challenges%20at%20CVPR2025.%20Focused%20on%20the%0Aintricate%20task%20of%20estimating%2021%203D%20hand%20joints%20from%20RGB%20egocentric%20videos%2C%0Awhich%20are%20complicated%20by%20subtle%20movements%20and%20frequent%20occlusions%2C%20we%20developed%0Athe%20Hand%20Pose%20Vision%20Transformer%20%28HP-ViT%2B%29.%20This%20architecture%20synergizes%20a%0AVision%20Transformer%20and%20a%20CNN%20backbone%2C%20using%20weighted%20fusion%20to%20refine%20the%20hand%0Apose%20predictions.%20For%20the%20EgoExo4D%20Body%20Pose%20Challenge%2C%20we%20adopted%20a%20multimodal%0Aspatio-temporal%20feature%20integration%20strategy%20to%20address%20the%20complexities%20of%0Abody%20pose%20estimation%20across%20dynamic%20contexts.%20Our%20methods%20achieved%20remarkable%0Aperformance%3A%208.31%20PA-MPJPE%20in%20the%20Hand%20Pose%20Challenge%20and%2011.25%20MPJPE%20in%20the%0ABody%20Pose%20Challenge%2C%20securing%20championship%20titles%20in%20both%20competitions.%20We%0Aextended%20our%20pose%20estimation%20solutions%20to%20the%20Proficiency%20Estimation%20task%2C%0Aapplying%20core%20technologies%20such%20as%20transformer-based%20architectures.%20This%0Aextension%20enabled%20us%20to%20achieve%20a%20top-1%20accuracy%20of%200.53%2C%20a%20SOTA%20result%2C%20in%20the%0ADemonstrator%20Proficiency%20Estimation%20competition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24411v1&entry.124074799=Read"},
{"title": "LLaMAs Have Feelings Too: Unveiling Sentiment and Emotion\n  Representations in LLaMA Models Through Probing", "author": "Dario Di Palma and Alessandro De Bellis and Giovanni Servedio and Vito Walter Anelli and Fedelucio Narducci and Tommaso Di Noia", "abstract": "  Large Language Models (LLMs) have rapidly become central to NLP,\ndemonstrating their ability to adapt to various tasks through prompting\ntechniques, including sentiment analysis. However, we still have a limited\nunderstanding of how these models capture sentiment-related information. This\nstudy probes the hidden layers of Llama models to pinpoint where sentiment\nfeatures are most represented and to assess how this affects sentiment\nanalysis.\n  Using probe classifiers, we analyze sentiment encoding across layers and\nscales, identifying the layers and pooling methods that best capture sentiment\nsignals. Our results show that sentiment information is most concentrated in\nmid-layers for binary polarity tasks, with detection accuracy increasing up to\n14% over prompting techniques. Additionally, we find that in decoder-only\nmodels, the last token is not consistently the most informative for sentiment\nencoding. Finally, this approach enables sentiment tasks to be performed with\nmemory requirements reduced by an average of 57%.\n  These insights contribute to a broader understanding of sentiment in LLMs,\nsuggesting layer-specific probing as an effective approach for sentiment tasks\nbeyond prompting, with potential to enhance model utility and reduce memory\nrequirements.\n", "link": "http://arxiv.org/abs/2505.16491v2", "date": "2025-05-30", "relevancy": 2.6043, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5421}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5421}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLaMAs%20Have%20Feelings%20Too%3A%20Unveiling%20Sentiment%20and%20Emotion%0A%20%20Representations%20in%20LLaMA%20Models%20Through%20Probing&body=Title%3A%20LLaMAs%20Have%20Feelings%20Too%3A%20Unveiling%20Sentiment%20and%20Emotion%0A%20%20Representations%20in%20LLaMA%20Models%20Through%20Probing%0AAuthor%3A%20Dario%20Di%20Palma%20and%20Alessandro%20De%20Bellis%20and%20Giovanni%20Servedio%20and%20Vito%20Walter%20Anelli%20and%20Fedelucio%20Narducci%20and%20Tommaso%20Di%20Noia%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20rapidly%20become%20central%20to%20NLP%2C%0Ademonstrating%20their%20ability%20to%20adapt%20to%20various%20tasks%20through%20prompting%0Atechniques%2C%20including%20sentiment%20analysis.%20However%2C%20we%20still%20have%20a%20limited%0Aunderstanding%20of%20how%20these%20models%20capture%20sentiment-related%20information.%20This%0Astudy%20probes%20the%20hidden%20layers%20of%20Llama%20models%20to%20pinpoint%20where%20sentiment%0Afeatures%20are%20most%20represented%20and%20to%20assess%20how%20this%20affects%20sentiment%0Aanalysis.%0A%20%20Using%20probe%20classifiers%2C%20we%20analyze%20sentiment%20encoding%20across%20layers%20and%0Ascales%2C%20identifying%20the%20layers%20and%20pooling%20methods%20that%20best%20capture%20sentiment%0Asignals.%20Our%20results%20show%20that%20sentiment%20information%20is%20most%20concentrated%20in%0Amid-layers%20for%20binary%20polarity%20tasks%2C%20with%20detection%20accuracy%20increasing%20up%20to%0A14%25%20over%20prompting%20techniques.%20Additionally%2C%20we%20find%20that%20in%20decoder-only%0Amodels%2C%20the%20last%20token%20is%20not%20consistently%20the%20most%20informative%20for%20sentiment%0Aencoding.%20Finally%2C%20this%20approach%20enables%20sentiment%20tasks%20to%20be%20performed%20with%0Amemory%20requirements%20reduced%20by%20an%20average%20of%2057%25.%0A%20%20These%20insights%20contribute%20to%20a%20broader%20understanding%20of%20sentiment%20in%20LLMs%2C%0Asuggesting%20layer-specific%20probing%20as%20an%20effective%20approach%20for%20sentiment%20tasks%0Abeyond%20prompting%2C%20with%20potential%20to%20enhance%20model%20utility%20and%20reduce%20memory%0Arequirements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16491v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLaMAs%2520Have%2520Feelings%2520Too%253A%2520Unveiling%2520Sentiment%2520and%2520Emotion%250A%2520%2520Representations%2520in%2520LLaMA%2520Models%2520Through%2520Probing%26entry.906535625%3DDario%2520Di%2520Palma%2520and%2520Alessandro%2520De%2520Bellis%2520and%2520Giovanni%2520Servedio%2520and%2520Vito%2520Walter%2520Anelli%2520and%2520Fedelucio%2520Narducci%2520and%2520Tommaso%2520Di%2520Noia%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520rapidly%2520become%2520central%2520to%2520NLP%252C%250Ademonstrating%2520their%2520ability%2520to%2520adapt%2520to%2520various%2520tasks%2520through%2520prompting%250Atechniques%252C%2520including%2520sentiment%2520analysis.%2520However%252C%2520we%2520still%2520have%2520a%2520limited%250Aunderstanding%2520of%2520how%2520these%2520models%2520capture%2520sentiment-related%2520information.%2520This%250Astudy%2520probes%2520the%2520hidden%2520layers%2520of%2520Llama%2520models%2520to%2520pinpoint%2520where%2520sentiment%250Afeatures%2520are%2520most%2520represented%2520and%2520to%2520assess%2520how%2520this%2520affects%2520sentiment%250Aanalysis.%250A%2520%2520Using%2520probe%2520classifiers%252C%2520we%2520analyze%2520sentiment%2520encoding%2520across%2520layers%2520and%250Ascales%252C%2520identifying%2520the%2520layers%2520and%2520pooling%2520methods%2520that%2520best%2520capture%2520sentiment%250Asignals.%2520Our%2520results%2520show%2520that%2520sentiment%2520information%2520is%2520most%2520concentrated%2520in%250Amid-layers%2520for%2520binary%2520polarity%2520tasks%252C%2520with%2520detection%2520accuracy%2520increasing%2520up%2520to%250A14%2525%2520over%2520prompting%2520techniques.%2520Additionally%252C%2520we%2520find%2520that%2520in%2520decoder-only%250Amodels%252C%2520the%2520last%2520token%2520is%2520not%2520consistently%2520the%2520most%2520informative%2520for%2520sentiment%250Aencoding.%2520Finally%252C%2520this%2520approach%2520enables%2520sentiment%2520tasks%2520to%2520be%2520performed%2520with%250Amemory%2520requirements%2520reduced%2520by%2520an%2520average%2520of%252057%2525.%250A%2520%2520These%2520insights%2520contribute%2520to%2520a%2520broader%2520understanding%2520of%2520sentiment%2520in%2520LLMs%252C%250Asuggesting%2520layer-specific%2520probing%2520as%2520an%2520effective%2520approach%2520for%2520sentiment%2520tasks%250Abeyond%2520prompting%252C%2520with%2520potential%2520to%2520enhance%2520model%2520utility%2520and%2520reduce%2520memory%250Arequirements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16491v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLaMAs%20Have%20Feelings%20Too%3A%20Unveiling%20Sentiment%20and%20Emotion%0A%20%20Representations%20in%20LLaMA%20Models%20Through%20Probing&entry.906535625=Dario%20Di%20Palma%20and%20Alessandro%20De%20Bellis%20and%20Giovanni%20Servedio%20and%20Vito%20Walter%20Anelli%20and%20Fedelucio%20Narducci%20and%20Tommaso%20Di%20Noia&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20rapidly%20become%20central%20to%20NLP%2C%0Ademonstrating%20their%20ability%20to%20adapt%20to%20various%20tasks%20through%20prompting%0Atechniques%2C%20including%20sentiment%20analysis.%20However%2C%20we%20still%20have%20a%20limited%0Aunderstanding%20of%20how%20these%20models%20capture%20sentiment-related%20information.%20This%0Astudy%20probes%20the%20hidden%20layers%20of%20Llama%20models%20to%20pinpoint%20where%20sentiment%0Afeatures%20are%20most%20represented%20and%20to%20assess%20how%20this%20affects%20sentiment%0Aanalysis.%0A%20%20Using%20probe%20classifiers%2C%20we%20analyze%20sentiment%20encoding%20across%20layers%20and%0Ascales%2C%20identifying%20the%20layers%20and%20pooling%20methods%20that%20best%20capture%20sentiment%0Asignals.%20Our%20results%20show%20that%20sentiment%20information%20is%20most%20concentrated%20in%0Amid-layers%20for%20binary%20polarity%20tasks%2C%20with%20detection%20accuracy%20increasing%20up%20to%0A14%25%20over%20prompting%20techniques.%20Additionally%2C%20we%20find%20that%20in%20decoder-only%0Amodels%2C%20the%20last%20token%20is%20not%20consistently%20the%20most%20informative%20for%20sentiment%0Aencoding.%20Finally%2C%20this%20approach%20enables%20sentiment%20tasks%20to%20be%20performed%20with%0Amemory%20requirements%20reduced%20by%20an%20average%20of%2057%25.%0A%20%20These%20insights%20contribute%20to%20a%20broader%20understanding%20of%20sentiment%20in%20LLMs%2C%0Asuggesting%20layer-specific%20probing%20as%20an%20effective%20approach%20for%20sentiment%20tasks%0Abeyond%20prompting%2C%20with%20potential%20to%20enhance%20model%20utility%20and%20reduce%20memory%0Arequirements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16491v2&entry.124074799=Read"},
{"title": "SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual\n  Reasoning Self-Improvement", "author": "Xiyao Wang and Zhengyuan Yang and Chao Feng and Hongjin Lu and Linjie Li and Chung-Ching Lin and Kevin Lin and Furong Huang and Lijuan Wang", "abstract": "  We introduce ThinkLite-VL, a family of visual reasoning models that achieve\nstate-of-the-art (SoTA) performance using an order of magnitude fewer training\nsamples, relying purely on reinforcement fine-tuning (RFT) self-improvement\nwithout any knowledge distillation. Our central insight is that sample\ndifficulty critically influences RFT effectiveness: appropriately challenging\nexamples can drive substantial reasoning improvements, even in low-data\nregimes. However, quantifying sample difficulty in a reliable and scalable\nmanner remains non-trivial. To address this, we repurpose Monte Carlo Tree\nSearch (MCTS) to measure sample difficulty via the number of reasoning\niterations a vision-language model (VLM) requires to solve each instance. This\nMCTS-based selection procedure identifies samples that induce deeper reasoning\nwhile remaining solvable, allowing us to filter a high-quality subset from 70k\nopen-source examples spanning math, natural image understanding, and chart\ncomprehension. Using this approach, we select just 11k challenging samples for\nRFT on Qwen2.5-VL-7B-Instruct and 7.5k samples for Qwen2.5-VL-72B-Instruct. The\nresulting models, ThinkLite-VL-7B and ThinkLite-VL-72B, significantly\noutperform their respective base models across eight visual reasoning\nbenchmarks. In particular, ThinkLite-VL-7B improves the average performance of\nQwen2.5-VL-7B-Instruct by 7\\% and surpasses all existing 7B-level models, as\nwell as much larger models such as GPT-4o, O1 and Qwen2.5-VL-72B, achieving a\nnew SoTA score of 75.1 on MathVista. ThinkLite-VL-72B further advances the SoTA\nfrontier, achieving an accuracy of 79.7 on MathVista and an average benchmark\nimprovement of 4.42 over the open-source SOTA. These results demonstrate that\nMCTS-guided difficulty filtering provides a scalable and effective path toward\ndata-efficient self-improvement in multimodal reasoning.\n", "link": "http://arxiv.org/abs/2504.07934v3", "date": "2025-05-30", "relevancy": 2.6013, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5321}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5143}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SoTA%20with%20Less%3A%20MCTS-Guided%20Sample%20Selection%20for%20Data-Efficient%20Visual%0A%20%20Reasoning%20Self-Improvement&body=Title%3A%20SoTA%20with%20Less%3A%20MCTS-Guided%20Sample%20Selection%20for%20Data-Efficient%20Visual%0A%20%20Reasoning%20Self-Improvement%0AAuthor%3A%20Xiyao%20Wang%20and%20Zhengyuan%20Yang%20and%20Chao%20Feng%20and%20Hongjin%20Lu%20and%20Linjie%20Li%20and%20Chung-Ching%20Lin%20and%20Kevin%20Lin%20and%20Furong%20Huang%20and%20Lijuan%20Wang%0AAbstract%3A%20%20%20We%20introduce%20ThinkLite-VL%2C%20a%20family%20of%20visual%20reasoning%20models%20that%20achieve%0Astate-of-the-art%20%28SoTA%29%20performance%20using%20an%20order%20of%20magnitude%20fewer%20training%0Asamples%2C%20relying%20purely%20on%20reinforcement%20fine-tuning%20%28RFT%29%20self-improvement%0Awithout%20any%20knowledge%20distillation.%20Our%20central%20insight%20is%20that%20sample%0Adifficulty%20critically%20influences%20RFT%20effectiveness%3A%20appropriately%20challenging%0Aexamples%20can%20drive%20substantial%20reasoning%20improvements%2C%20even%20in%20low-data%0Aregimes.%20However%2C%20quantifying%20sample%20difficulty%20in%20a%20reliable%20and%20scalable%0Amanner%20remains%20non-trivial.%20To%20address%20this%2C%20we%20repurpose%20Monte%20Carlo%20Tree%0ASearch%20%28MCTS%29%20to%20measure%20sample%20difficulty%20via%20the%20number%20of%20reasoning%0Aiterations%20a%20vision-language%20model%20%28VLM%29%20requires%20to%20solve%20each%20instance.%20This%0AMCTS-based%20selection%20procedure%20identifies%20samples%20that%20induce%20deeper%20reasoning%0Awhile%20remaining%20solvable%2C%20allowing%20us%20to%20filter%20a%20high-quality%20subset%20from%2070k%0Aopen-source%20examples%20spanning%20math%2C%20natural%20image%20understanding%2C%20and%20chart%0Acomprehension.%20Using%20this%20approach%2C%20we%20select%20just%2011k%20challenging%20samples%20for%0ARFT%20on%20Qwen2.5-VL-7B-Instruct%20and%207.5k%20samples%20for%20Qwen2.5-VL-72B-Instruct.%20The%0Aresulting%20models%2C%20ThinkLite-VL-7B%20and%20ThinkLite-VL-72B%2C%20significantly%0Aoutperform%20their%20respective%20base%20models%20across%20eight%20visual%20reasoning%0Abenchmarks.%20In%20particular%2C%20ThinkLite-VL-7B%20improves%20the%20average%20performance%20of%0AQwen2.5-VL-7B-Instruct%20by%207%5C%25%20and%20surpasses%20all%20existing%207B-level%20models%2C%20as%0Awell%20as%20much%20larger%20models%20such%20as%20GPT-4o%2C%20O1%20and%20Qwen2.5-VL-72B%2C%20achieving%20a%0Anew%20SoTA%20score%20of%2075.1%20on%20MathVista.%20ThinkLite-VL-72B%20further%20advances%20the%20SoTA%0Afrontier%2C%20achieving%20an%20accuracy%20of%2079.7%20on%20MathVista%20and%20an%20average%20benchmark%0Aimprovement%20of%204.42%20over%20the%20open-source%20SOTA.%20These%20results%20demonstrate%20that%0AMCTS-guided%20difficulty%20filtering%20provides%20a%20scalable%20and%20effective%20path%20toward%0Adata-efficient%20self-improvement%20in%20multimodal%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.07934v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoTA%2520with%2520Less%253A%2520MCTS-Guided%2520Sample%2520Selection%2520for%2520Data-Efficient%2520Visual%250A%2520%2520Reasoning%2520Self-Improvement%26entry.906535625%3DXiyao%2520Wang%2520and%2520Zhengyuan%2520Yang%2520and%2520Chao%2520Feng%2520and%2520Hongjin%2520Lu%2520and%2520Linjie%2520Li%2520and%2520Chung-Ching%2520Lin%2520and%2520Kevin%2520Lin%2520and%2520Furong%2520Huang%2520and%2520Lijuan%2520Wang%26entry.1292438233%3D%2520%2520We%2520introduce%2520ThinkLite-VL%252C%2520a%2520family%2520of%2520visual%2520reasoning%2520models%2520that%2520achieve%250Astate-of-the-art%2520%2528SoTA%2529%2520performance%2520using%2520an%2520order%2520of%2520magnitude%2520fewer%2520training%250Asamples%252C%2520relying%2520purely%2520on%2520reinforcement%2520fine-tuning%2520%2528RFT%2529%2520self-improvement%250Awithout%2520any%2520knowledge%2520distillation.%2520Our%2520central%2520insight%2520is%2520that%2520sample%250Adifficulty%2520critically%2520influences%2520RFT%2520effectiveness%253A%2520appropriately%2520challenging%250Aexamples%2520can%2520drive%2520substantial%2520reasoning%2520improvements%252C%2520even%2520in%2520low-data%250Aregimes.%2520However%252C%2520quantifying%2520sample%2520difficulty%2520in%2520a%2520reliable%2520and%2520scalable%250Amanner%2520remains%2520non-trivial.%2520To%2520address%2520this%252C%2520we%2520repurpose%2520Monte%2520Carlo%2520Tree%250ASearch%2520%2528MCTS%2529%2520to%2520measure%2520sample%2520difficulty%2520via%2520the%2520number%2520of%2520reasoning%250Aiterations%2520a%2520vision-language%2520model%2520%2528VLM%2529%2520requires%2520to%2520solve%2520each%2520instance.%2520This%250AMCTS-based%2520selection%2520procedure%2520identifies%2520samples%2520that%2520induce%2520deeper%2520reasoning%250Awhile%2520remaining%2520solvable%252C%2520allowing%2520us%2520to%2520filter%2520a%2520high-quality%2520subset%2520from%252070k%250Aopen-source%2520examples%2520spanning%2520math%252C%2520natural%2520image%2520understanding%252C%2520and%2520chart%250Acomprehension.%2520Using%2520this%2520approach%252C%2520we%2520select%2520just%252011k%2520challenging%2520samples%2520for%250ARFT%2520on%2520Qwen2.5-VL-7B-Instruct%2520and%25207.5k%2520samples%2520for%2520Qwen2.5-VL-72B-Instruct.%2520The%250Aresulting%2520models%252C%2520ThinkLite-VL-7B%2520and%2520ThinkLite-VL-72B%252C%2520significantly%250Aoutperform%2520their%2520respective%2520base%2520models%2520across%2520eight%2520visual%2520reasoning%250Abenchmarks.%2520In%2520particular%252C%2520ThinkLite-VL-7B%2520improves%2520the%2520average%2520performance%2520of%250AQwen2.5-VL-7B-Instruct%2520by%25207%255C%2525%2520and%2520surpasses%2520all%2520existing%25207B-level%2520models%252C%2520as%250Awell%2520as%2520much%2520larger%2520models%2520such%2520as%2520GPT-4o%252C%2520O1%2520and%2520Qwen2.5-VL-72B%252C%2520achieving%2520a%250Anew%2520SoTA%2520score%2520of%252075.1%2520on%2520MathVista.%2520ThinkLite-VL-72B%2520further%2520advances%2520the%2520SoTA%250Afrontier%252C%2520achieving%2520an%2520accuracy%2520of%252079.7%2520on%2520MathVista%2520and%2520an%2520average%2520benchmark%250Aimprovement%2520of%25204.42%2520over%2520the%2520open-source%2520SOTA.%2520These%2520results%2520demonstrate%2520that%250AMCTS-guided%2520difficulty%2520filtering%2520provides%2520a%2520scalable%2520and%2520effective%2520path%2520toward%250Adata-efficient%2520self-improvement%2520in%2520multimodal%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.07934v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SoTA%20with%20Less%3A%20MCTS-Guided%20Sample%20Selection%20for%20Data-Efficient%20Visual%0A%20%20Reasoning%20Self-Improvement&entry.906535625=Xiyao%20Wang%20and%20Zhengyuan%20Yang%20and%20Chao%20Feng%20and%20Hongjin%20Lu%20and%20Linjie%20Li%20and%20Chung-Ching%20Lin%20and%20Kevin%20Lin%20and%20Furong%20Huang%20and%20Lijuan%20Wang&entry.1292438233=%20%20We%20introduce%20ThinkLite-VL%2C%20a%20family%20of%20visual%20reasoning%20models%20that%20achieve%0Astate-of-the-art%20%28SoTA%29%20performance%20using%20an%20order%20of%20magnitude%20fewer%20training%0Asamples%2C%20relying%20purely%20on%20reinforcement%20fine-tuning%20%28RFT%29%20self-improvement%0Awithout%20any%20knowledge%20distillation.%20Our%20central%20insight%20is%20that%20sample%0Adifficulty%20critically%20influences%20RFT%20effectiveness%3A%20appropriately%20challenging%0Aexamples%20can%20drive%20substantial%20reasoning%20improvements%2C%20even%20in%20low-data%0Aregimes.%20However%2C%20quantifying%20sample%20difficulty%20in%20a%20reliable%20and%20scalable%0Amanner%20remains%20non-trivial.%20To%20address%20this%2C%20we%20repurpose%20Monte%20Carlo%20Tree%0ASearch%20%28MCTS%29%20to%20measure%20sample%20difficulty%20via%20the%20number%20of%20reasoning%0Aiterations%20a%20vision-language%20model%20%28VLM%29%20requires%20to%20solve%20each%20instance.%20This%0AMCTS-based%20selection%20procedure%20identifies%20samples%20that%20induce%20deeper%20reasoning%0Awhile%20remaining%20solvable%2C%20allowing%20us%20to%20filter%20a%20high-quality%20subset%20from%2070k%0Aopen-source%20examples%20spanning%20math%2C%20natural%20image%20understanding%2C%20and%20chart%0Acomprehension.%20Using%20this%20approach%2C%20we%20select%20just%2011k%20challenging%20samples%20for%0ARFT%20on%20Qwen2.5-VL-7B-Instruct%20and%207.5k%20samples%20for%20Qwen2.5-VL-72B-Instruct.%20The%0Aresulting%20models%2C%20ThinkLite-VL-7B%20and%20ThinkLite-VL-72B%2C%20significantly%0Aoutperform%20their%20respective%20base%20models%20across%20eight%20visual%20reasoning%0Abenchmarks.%20In%20particular%2C%20ThinkLite-VL-7B%20improves%20the%20average%20performance%20of%0AQwen2.5-VL-7B-Instruct%20by%207%5C%25%20and%20surpasses%20all%20existing%207B-level%20models%2C%20as%0Awell%20as%20much%20larger%20models%20such%20as%20GPT-4o%2C%20O1%20and%20Qwen2.5-VL-72B%2C%20achieving%20a%0Anew%20SoTA%20score%20of%2075.1%20on%20MathVista.%20ThinkLite-VL-72B%20further%20advances%20the%20SoTA%0Afrontier%2C%20achieving%20an%20accuracy%20of%2079.7%20on%20MathVista%20and%20an%20average%20benchmark%0Aimprovement%20of%204.42%20over%20the%20open-source%20SOTA.%20These%20results%20demonstrate%20that%0AMCTS-guided%20difficulty%20filtering%20provides%20a%20scalable%20and%20effective%20path%20toward%0Adata-efficient%20self-improvement%20in%20multimodal%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.07934v3&entry.124074799=Read"},
{"title": "Directed Homophily-Aware Graph Neural Network", "author": "Aihu Zhang and Jiaxing Xu and Mengcheng Lan and Shili Xiang and Yiping Ke", "abstract": "  Graph Neural Networks (GNNs) have achieved significant success in various\nlearning tasks on graph-structured data. Nevertheless, most GNNs struggle to\ngeneralize to heterophilic neighborhoods. Additionally, many GNNs ignore the\ndirectional nature of real-world graphs, resulting in suboptimal performance on\ndirected graphs with asymmetric structures. In this work, we propose Directed\nHomophily-aware Graph Neural Network (DHGNN), a novel framework that addresses\nthese limitations by incorporating homophily-aware and direction-sensitive\ncomponents. DHGNN employs a resettable gating mechanism to adaptively modulate\nmessage contributions based on homophily levels and informativeness, and a\nstructure-aware noise-tolerant fusion module to effectively integrate node\nrepresentations from the original and reverse directions. Extensive experiments\non both homophilic and heterophilic directed graph datasets demonstrate that\nDHGNN outperforms state-of-the-art methods in node classification and link\nprediction. In particular, DHGNN improves over the best baseline by up to\n15.07% in link prediction. Our analysis further shows that the gating mechanism\ncaptures directional homophily gaps and fluctuating homophily across layers,\nproviding deeper insights into message-passing behavior on complex graph\nstructures.\n", "link": "http://arxiv.org/abs/2505.22362v2", "date": "2025-05-30", "relevancy": 2.5991, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5496}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5095}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Directed%20Homophily-Aware%20Graph%20Neural%20Network&body=Title%3A%20Directed%20Homophily-Aware%20Graph%20Neural%20Network%0AAuthor%3A%20Aihu%20Zhang%20and%20Jiaxing%20Xu%20and%20Mengcheng%20Lan%20and%20Shili%20Xiang%20and%20Yiping%20Ke%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20achieved%20significant%20success%20in%20various%0Alearning%20tasks%20on%20graph-structured%20data.%20Nevertheless%2C%20most%20GNNs%20struggle%20to%0Ageneralize%20to%20heterophilic%20neighborhoods.%20Additionally%2C%20many%20GNNs%20ignore%20the%0Adirectional%20nature%20of%20real-world%20graphs%2C%20resulting%20in%20suboptimal%20performance%20on%0Adirected%20graphs%20with%20asymmetric%20structures.%20In%20this%20work%2C%20we%20propose%20Directed%0AHomophily-aware%20Graph%20Neural%20Network%20%28DHGNN%29%2C%20a%20novel%20framework%20that%20addresses%0Athese%20limitations%20by%20incorporating%20homophily-aware%20and%20direction-sensitive%0Acomponents.%20DHGNN%20employs%20a%20resettable%20gating%20mechanism%20to%20adaptively%20modulate%0Amessage%20contributions%20based%20on%20homophily%20levels%20and%20informativeness%2C%20and%20a%0Astructure-aware%20noise-tolerant%20fusion%20module%20to%20effectively%20integrate%20node%0Arepresentations%20from%20the%20original%20and%20reverse%20directions.%20Extensive%20experiments%0Aon%20both%20homophilic%20and%20heterophilic%20directed%20graph%20datasets%20demonstrate%20that%0ADHGNN%20outperforms%20state-of-the-art%20methods%20in%20node%20classification%20and%20link%0Aprediction.%20In%20particular%2C%20DHGNN%20improves%20over%20the%20best%20baseline%20by%20up%20to%0A15.07%25%20in%20link%20prediction.%20Our%20analysis%20further%20shows%20that%20the%20gating%20mechanism%0Acaptures%20directional%20homophily%20gaps%20and%20fluctuating%20homophily%20across%20layers%2C%0Aproviding%20deeper%20insights%20into%20message-passing%20behavior%20on%20complex%20graph%0Astructures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22362v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDirected%2520Homophily-Aware%2520Graph%2520Neural%2520Network%26entry.906535625%3DAihu%2520Zhang%2520and%2520Jiaxing%2520Xu%2520and%2520Mengcheng%2520Lan%2520and%2520Shili%2520Xiang%2520and%2520Yiping%2520Ke%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520achieved%2520significant%2520success%2520in%2520various%250Alearning%2520tasks%2520on%2520graph-structured%2520data.%2520Nevertheless%252C%2520most%2520GNNs%2520struggle%2520to%250Ageneralize%2520to%2520heterophilic%2520neighborhoods.%2520Additionally%252C%2520many%2520GNNs%2520ignore%2520the%250Adirectional%2520nature%2520of%2520real-world%2520graphs%252C%2520resulting%2520in%2520suboptimal%2520performance%2520on%250Adirected%2520graphs%2520with%2520asymmetric%2520structures.%2520In%2520this%2520work%252C%2520we%2520propose%2520Directed%250AHomophily-aware%2520Graph%2520Neural%2520Network%2520%2528DHGNN%2529%252C%2520a%2520novel%2520framework%2520that%2520addresses%250Athese%2520limitations%2520by%2520incorporating%2520homophily-aware%2520and%2520direction-sensitive%250Acomponents.%2520DHGNN%2520employs%2520a%2520resettable%2520gating%2520mechanism%2520to%2520adaptively%2520modulate%250Amessage%2520contributions%2520based%2520on%2520homophily%2520levels%2520and%2520informativeness%252C%2520and%2520a%250Astructure-aware%2520noise-tolerant%2520fusion%2520module%2520to%2520effectively%2520integrate%2520node%250Arepresentations%2520from%2520the%2520original%2520and%2520reverse%2520directions.%2520Extensive%2520experiments%250Aon%2520both%2520homophilic%2520and%2520heterophilic%2520directed%2520graph%2520datasets%2520demonstrate%2520that%250ADHGNN%2520outperforms%2520state-of-the-art%2520methods%2520in%2520node%2520classification%2520and%2520link%250Aprediction.%2520In%2520particular%252C%2520DHGNN%2520improves%2520over%2520the%2520best%2520baseline%2520by%2520up%2520to%250A15.07%2525%2520in%2520link%2520prediction.%2520Our%2520analysis%2520further%2520shows%2520that%2520the%2520gating%2520mechanism%250Acaptures%2520directional%2520homophily%2520gaps%2520and%2520fluctuating%2520homophily%2520across%2520layers%252C%250Aproviding%2520deeper%2520insights%2520into%2520message-passing%2520behavior%2520on%2520complex%2520graph%250Astructures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22362v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Directed%20Homophily-Aware%20Graph%20Neural%20Network&entry.906535625=Aihu%20Zhang%20and%20Jiaxing%20Xu%20and%20Mengcheng%20Lan%20and%20Shili%20Xiang%20and%20Yiping%20Ke&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20achieved%20significant%20success%20in%20various%0Alearning%20tasks%20on%20graph-structured%20data.%20Nevertheless%2C%20most%20GNNs%20struggle%20to%0Ageneralize%20to%20heterophilic%20neighborhoods.%20Additionally%2C%20many%20GNNs%20ignore%20the%0Adirectional%20nature%20of%20real-world%20graphs%2C%20resulting%20in%20suboptimal%20performance%20on%0Adirected%20graphs%20with%20asymmetric%20structures.%20In%20this%20work%2C%20we%20propose%20Directed%0AHomophily-aware%20Graph%20Neural%20Network%20%28DHGNN%29%2C%20a%20novel%20framework%20that%20addresses%0Athese%20limitations%20by%20incorporating%20homophily-aware%20and%20direction-sensitive%0Acomponents.%20DHGNN%20employs%20a%20resettable%20gating%20mechanism%20to%20adaptively%20modulate%0Amessage%20contributions%20based%20on%20homophily%20levels%20and%20informativeness%2C%20and%20a%0Astructure-aware%20noise-tolerant%20fusion%20module%20to%20effectively%20integrate%20node%0Arepresentations%20from%20the%20original%20and%20reverse%20directions.%20Extensive%20experiments%0Aon%20both%20homophilic%20and%20heterophilic%20directed%20graph%20datasets%20demonstrate%20that%0ADHGNN%20outperforms%20state-of-the-art%20methods%20in%20node%20classification%20and%20link%0Aprediction.%20In%20particular%2C%20DHGNN%20improves%20over%20the%20best%20baseline%20by%20up%20to%0A15.07%25%20in%20link%20prediction.%20Our%20analysis%20further%20shows%20that%20the%20gating%20mechanism%0Acaptures%20directional%20homophily%20gaps%20and%20fluctuating%20homophily%20across%20layers%2C%0Aproviding%20deeper%20insights%20into%20message-passing%20behavior%20on%20complex%20graph%0Astructures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22362v2&entry.124074799=Read"},
{"title": "Deep Augmentation: Dropout as Augmentation for Self-Supervised Learning", "author": "Rickard Br\u00fcel-Gabrielsson and Tongzhou Wang and Manel Baradad and Justin Solomon", "abstract": "  Despite dropout's ubiquity in machine learning, its effectiveness as a form\nof data augmentation remains under-explored. We address two key questions: (i)\nWhen is dropout effective as an augmentation strategy? (ii) Is dropout uniquely\neffective under these conditions? To explore these questions, we propose Deep\nAugmentation, a network- and modality-agnostic method that applies dropout or\nPCA transformations to targeted layers in neural networks. Through extensive\nexperiments on contrastive learning tasks in NLP, computer vision, and graph\nlearning, we find that uniformly applying dropout across layers does not\nconsistently improve performance. Instead, dropout proves most beneficial in\ndeeper layers and can be matched by alternative augmentations (e.g., PCA). We\nalso show that a stop-gradient operation is critical for ensuring dropout\nfunctions effectively as an augmentation, and that performance trends invert\nwhen moving from contrastive tasks to supervised tasks. Our analysis suggests\nthat Deep Augmentation helps mitigate inter-layer co-adaptation -- a notable\nissue in self-supervised learning due to the absence of labeled data. Drawing\non these insights, we outline a procedure for selecting the optimal\naugmentation layer and demonstrate that Deep Augmentation can outperform\ntraditional input-level augmentations. This simple yet powerful approach can be\nseamlessly integrated into a wide range of architectures and modalities,\nyielding notable gains in both performance and generalization.\n", "link": "http://arxiv.org/abs/2303.14537v5", "date": "2025-05-30", "relevancy": 2.5945, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5381}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5099}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Augmentation%3A%20Dropout%20as%20Augmentation%20for%20Self-Supervised%20Learning&body=Title%3A%20Deep%20Augmentation%3A%20Dropout%20as%20Augmentation%20for%20Self-Supervised%20Learning%0AAuthor%3A%20Rickard%20Br%C3%BCel-Gabrielsson%20and%20Tongzhou%20Wang%20and%20Manel%20Baradad%20and%20Justin%20Solomon%0AAbstract%3A%20%20%20Despite%20dropout%27s%20ubiquity%20in%20machine%20learning%2C%20its%20effectiveness%20as%20a%20form%0Aof%20data%20augmentation%20remains%20under-explored.%20We%20address%20two%20key%20questions%3A%20%28i%29%0AWhen%20is%20dropout%20effective%20as%20an%20augmentation%20strategy%3F%20%28ii%29%20Is%20dropout%20uniquely%0Aeffective%20under%20these%20conditions%3F%20To%20explore%20these%20questions%2C%20we%20propose%20Deep%0AAugmentation%2C%20a%20network-%20and%20modality-agnostic%20method%20that%20applies%20dropout%20or%0APCA%20transformations%20to%20targeted%20layers%20in%20neural%20networks.%20Through%20extensive%0Aexperiments%20on%20contrastive%20learning%20tasks%20in%20NLP%2C%20computer%20vision%2C%20and%20graph%0Alearning%2C%20we%20find%20that%20uniformly%20applying%20dropout%20across%20layers%20does%20not%0Aconsistently%20improve%20performance.%20Instead%2C%20dropout%20proves%20most%20beneficial%20in%0Adeeper%20layers%20and%20can%20be%20matched%20by%20alternative%20augmentations%20%28e.g.%2C%20PCA%29.%20We%0Aalso%20show%20that%20a%20stop-gradient%20operation%20is%20critical%20for%20ensuring%20dropout%0Afunctions%20effectively%20as%20an%20augmentation%2C%20and%20that%20performance%20trends%20invert%0Awhen%20moving%20from%20contrastive%20tasks%20to%20supervised%20tasks.%20Our%20analysis%20suggests%0Athat%20Deep%20Augmentation%20helps%20mitigate%20inter-layer%20co-adaptation%20--%20a%20notable%0Aissue%20in%20self-supervised%20learning%20due%20to%20the%20absence%20of%20labeled%20data.%20Drawing%0Aon%20these%20insights%2C%20we%20outline%20a%20procedure%20for%20selecting%20the%20optimal%0Aaugmentation%20layer%20and%20demonstrate%20that%20Deep%20Augmentation%20can%20outperform%0Atraditional%20input-level%20augmentations.%20This%20simple%20yet%20powerful%20approach%20can%20be%0Aseamlessly%20integrated%20into%20a%20wide%20range%20of%20architectures%20and%20modalities%2C%0Ayielding%20notable%20gains%20in%20both%20performance%20and%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.14537v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Augmentation%253A%2520Dropout%2520as%2520Augmentation%2520for%2520Self-Supervised%2520Learning%26entry.906535625%3DRickard%2520Br%25C3%25BCel-Gabrielsson%2520and%2520Tongzhou%2520Wang%2520and%2520Manel%2520Baradad%2520and%2520Justin%2520Solomon%26entry.1292438233%3D%2520%2520Despite%2520dropout%2527s%2520ubiquity%2520in%2520machine%2520learning%252C%2520its%2520effectiveness%2520as%2520a%2520form%250Aof%2520data%2520augmentation%2520remains%2520under-explored.%2520We%2520address%2520two%2520key%2520questions%253A%2520%2528i%2529%250AWhen%2520is%2520dropout%2520effective%2520as%2520an%2520augmentation%2520strategy%253F%2520%2528ii%2529%2520Is%2520dropout%2520uniquely%250Aeffective%2520under%2520these%2520conditions%253F%2520To%2520explore%2520these%2520questions%252C%2520we%2520propose%2520Deep%250AAugmentation%252C%2520a%2520network-%2520and%2520modality-agnostic%2520method%2520that%2520applies%2520dropout%2520or%250APCA%2520transformations%2520to%2520targeted%2520layers%2520in%2520neural%2520networks.%2520Through%2520extensive%250Aexperiments%2520on%2520contrastive%2520learning%2520tasks%2520in%2520NLP%252C%2520computer%2520vision%252C%2520and%2520graph%250Alearning%252C%2520we%2520find%2520that%2520uniformly%2520applying%2520dropout%2520across%2520layers%2520does%2520not%250Aconsistently%2520improve%2520performance.%2520Instead%252C%2520dropout%2520proves%2520most%2520beneficial%2520in%250Adeeper%2520layers%2520and%2520can%2520be%2520matched%2520by%2520alternative%2520augmentations%2520%2528e.g.%252C%2520PCA%2529.%2520We%250Aalso%2520show%2520that%2520a%2520stop-gradient%2520operation%2520is%2520critical%2520for%2520ensuring%2520dropout%250Afunctions%2520effectively%2520as%2520an%2520augmentation%252C%2520and%2520that%2520performance%2520trends%2520invert%250Awhen%2520moving%2520from%2520contrastive%2520tasks%2520to%2520supervised%2520tasks.%2520Our%2520analysis%2520suggests%250Athat%2520Deep%2520Augmentation%2520helps%2520mitigate%2520inter-layer%2520co-adaptation%2520--%2520a%2520notable%250Aissue%2520in%2520self-supervised%2520learning%2520due%2520to%2520the%2520absence%2520of%2520labeled%2520data.%2520Drawing%250Aon%2520these%2520insights%252C%2520we%2520outline%2520a%2520procedure%2520for%2520selecting%2520the%2520optimal%250Aaugmentation%2520layer%2520and%2520demonstrate%2520that%2520Deep%2520Augmentation%2520can%2520outperform%250Atraditional%2520input-level%2520augmentations.%2520This%2520simple%2520yet%2520powerful%2520approach%2520can%2520be%250Aseamlessly%2520integrated%2520into%2520a%2520wide%2520range%2520of%2520architectures%2520and%2520modalities%252C%250Ayielding%2520notable%2520gains%2520in%2520both%2520performance%2520and%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.14537v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Augmentation%3A%20Dropout%20as%20Augmentation%20for%20Self-Supervised%20Learning&entry.906535625=Rickard%20Br%C3%BCel-Gabrielsson%20and%20Tongzhou%20Wang%20and%20Manel%20Baradad%20and%20Justin%20Solomon&entry.1292438233=%20%20Despite%20dropout%27s%20ubiquity%20in%20machine%20learning%2C%20its%20effectiveness%20as%20a%20form%0Aof%20data%20augmentation%20remains%20under-explored.%20We%20address%20two%20key%20questions%3A%20%28i%29%0AWhen%20is%20dropout%20effective%20as%20an%20augmentation%20strategy%3F%20%28ii%29%20Is%20dropout%20uniquely%0Aeffective%20under%20these%20conditions%3F%20To%20explore%20these%20questions%2C%20we%20propose%20Deep%0AAugmentation%2C%20a%20network-%20and%20modality-agnostic%20method%20that%20applies%20dropout%20or%0APCA%20transformations%20to%20targeted%20layers%20in%20neural%20networks.%20Through%20extensive%0Aexperiments%20on%20contrastive%20learning%20tasks%20in%20NLP%2C%20computer%20vision%2C%20and%20graph%0Alearning%2C%20we%20find%20that%20uniformly%20applying%20dropout%20across%20layers%20does%20not%0Aconsistently%20improve%20performance.%20Instead%2C%20dropout%20proves%20most%20beneficial%20in%0Adeeper%20layers%20and%20can%20be%20matched%20by%20alternative%20augmentations%20%28e.g.%2C%20PCA%29.%20We%0Aalso%20show%20that%20a%20stop-gradient%20operation%20is%20critical%20for%20ensuring%20dropout%0Afunctions%20effectively%20as%20an%20augmentation%2C%20and%20that%20performance%20trends%20invert%0Awhen%20moving%20from%20contrastive%20tasks%20to%20supervised%20tasks.%20Our%20analysis%20suggests%0Athat%20Deep%20Augmentation%20helps%20mitigate%20inter-layer%20co-adaptation%20--%20a%20notable%0Aissue%20in%20self-supervised%20learning%20due%20to%20the%20absence%20of%20labeled%20data.%20Drawing%0Aon%20these%20insights%2C%20we%20outline%20a%20procedure%20for%20selecting%20the%20optimal%0Aaugmentation%20layer%20and%20demonstrate%20that%20Deep%20Augmentation%20can%20outperform%0Atraditional%20input-level%20augmentations.%20This%20simple%20yet%20powerful%20approach%20can%20be%0Aseamlessly%20integrated%20into%20a%20wide%20range%20of%20architectures%20and%20modalities%2C%0Ayielding%20notable%20gains%20in%20both%20performance%20and%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.14537v5&entry.124074799=Read"},
{"title": "Learning to Reason Over Time: Timeline Self-Reflection for Improved\n  Temporal Reasoning in Language Models", "author": "Adri\u00e1n Bazaga and Rexhina Blloshmi and Bill Byrne and Adri\u00e0 de Gispert", "abstract": "  Large Language Models (LLMs) have emerged as powerful tools for generating\ncoherent text, understanding context, and performing reasoning tasks. However,\nthey struggle with temporal reasoning, which requires processing time-related\ninformation such as event sequencing, durations, and inter-temporal\nrelationships. These capabilities are critical for applications including\nquestion answering, scheduling, and historical analysis. In this paper, we\nintroduce TISER, a novel framework that enhances the temporal reasoning\nabilities of LLMs through a multi-stage process that combines timeline\nconstruction with iterative self-reflection. Our approach leverages test-time\nscaling to extend the length of reasoning traces, enabling models to capture\ncomplex temporal dependencies more effectively. This strategy not only boosts\nreasoning accuracy but also improves the traceability of the inference process.\nExperimental results demonstrate state-of-the-art performance across multiple\nbenchmarks, including out-of-distribution test sets, and reveal that TISER\nenables smaller open-source models to surpass larger closed-weight models on\nchallenging temporal reasoning tasks.\n", "link": "http://arxiv.org/abs/2504.05258v2", "date": "2025-05-30", "relevancy": 2.5844, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5193}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5157}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5157}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Reason%20Over%20Time%3A%20Timeline%20Self-Reflection%20for%20Improved%0A%20%20Temporal%20Reasoning%20in%20Language%20Models&body=Title%3A%20Learning%20to%20Reason%20Over%20Time%3A%20Timeline%20Self-Reflection%20for%20Improved%0A%20%20Temporal%20Reasoning%20in%20Language%20Models%0AAuthor%3A%20Adri%C3%A1n%20Bazaga%20and%20Rexhina%20Blloshmi%20and%20Bill%20Byrne%20and%20Adri%C3%A0%20de%20Gispert%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20emerged%20as%20powerful%20tools%20for%20generating%0Acoherent%20text%2C%20understanding%20context%2C%20and%20performing%20reasoning%20tasks.%20However%2C%0Athey%20struggle%20with%20temporal%20reasoning%2C%20which%20requires%20processing%20time-related%0Ainformation%20such%20as%20event%20sequencing%2C%20durations%2C%20and%20inter-temporal%0Arelationships.%20These%20capabilities%20are%20critical%20for%20applications%20including%0Aquestion%20answering%2C%20scheduling%2C%20and%20historical%20analysis.%20In%20this%20paper%2C%20we%0Aintroduce%20TISER%2C%20a%20novel%20framework%20that%20enhances%20the%20temporal%20reasoning%0Aabilities%20of%20LLMs%20through%20a%20multi-stage%20process%20that%20combines%20timeline%0Aconstruction%20with%20iterative%20self-reflection.%20Our%20approach%20leverages%20test-time%0Ascaling%20to%20extend%20the%20length%20of%20reasoning%20traces%2C%20enabling%20models%20to%20capture%0Acomplex%20temporal%20dependencies%20more%20effectively.%20This%20strategy%20not%20only%20boosts%0Areasoning%20accuracy%20but%20also%20improves%20the%20traceability%20of%20the%20inference%20process.%0AExperimental%20results%20demonstrate%20state-of-the-art%20performance%20across%20multiple%0Abenchmarks%2C%20including%20out-of-distribution%20test%20sets%2C%20and%20reveal%20that%20TISER%0Aenables%20smaller%20open-source%20models%20to%20surpass%20larger%20closed-weight%20models%20on%0Achallenging%20temporal%20reasoning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.05258v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Reason%2520Over%2520Time%253A%2520Timeline%2520Self-Reflection%2520for%2520Improved%250A%2520%2520Temporal%2520Reasoning%2520in%2520Language%2520Models%26entry.906535625%3DAdri%25C3%25A1n%2520Bazaga%2520and%2520Rexhina%2520Blloshmi%2520and%2520Bill%2520Byrne%2520and%2520Adri%25C3%25A0%2520de%2520Gispert%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520emerged%2520as%2520powerful%2520tools%2520for%2520generating%250Acoherent%2520text%252C%2520understanding%2520context%252C%2520and%2520performing%2520reasoning%2520tasks.%2520However%252C%250Athey%2520struggle%2520with%2520temporal%2520reasoning%252C%2520which%2520requires%2520processing%2520time-related%250Ainformation%2520such%2520as%2520event%2520sequencing%252C%2520durations%252C%2520and%2520inter-temporal%250Arelationships.%2520These%2520capabilities%2520are%2520critical%2520for%2520applications%2520including%250Aquestion%2520answering%252C%2520scheduling%252C%2520and%2520historical%2520analysis.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520TISER%252C%2520a%2520novel%2520framework%2520that%2520enhances%2520the%2520temporal%2520reasoning%250Aabilities%2520of%2520LLMs%2520through%2520a%2520multi-stage%2520process%2520that%2520combines%2520timeline%250Aconstruction%2520with%2520iterative%2520self-reflection.%2520Our%2520approach%2520leverages%2520test-time%250Ascaling%2520to%2520extend%2520the%2520length%2520of%2520reasoning%2520traces%252C%2520enabling%2520models%2520to%2520capture%250Acomplex%2520temporal%2520dependencies%2520more%2520effectively.%2520This%2520strategy%2520not%2520only%2520boosts%250Areasoning%2520accuracy%2520but%2520also%2520improves%2520the%2520traceability%2520of%2520the%2520inference%2520process.%250AExperimental%2520results%2520demonstrate%2520state-of-the-art%2520performance%2520across%2520multiple%250Abenchmarks%252C%2520including%2520out-of-distribution%2520test%2520sets%252C%2520and%2520reveal%2520that%2520TISER%250Aenables%2520smaller%2520open-source%2520models%2520to%2520surpass%2520larger%2520closed-weight%2520models%2520on%250Achallenging%2520temporal%2520reasoning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.05258v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Reason%20Over%20Time%3A%20Timeline%20Self-Reflection%20for%20Improved%0A%20%20Temporal%20Reasoning%20in%20Language%20Models&entry.906535625=Adri%C3%A1n%20Bazaga%20and%20Rexhina%20Blloshmi%20and%20Bill%20Byrne%20and%20Adri%C3%A0%20de%20Gispert&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20emerged%20as%20powerful%20tools%20for%20generating%0Acoherent%20text%2C%20understanding%20context%2C%20and%20performing%20reasoning%20tasks.%20However%2C%0Athey%20struggle%20with%20temporal%20reasoning%2C%20which%20requires%20processing%20time-related%0Ainformation%20such%20as%20event%20sequencing%2C%20durations%2C%20and%20inter-temporal%0Arelationships.%20These%20capabilities%20are%20critical%20for%20applications%20including%0Aquestion%20answering%2C%20scheduling%2C%20and%20historical%20analysis.%20In%20this%20paper%2C%20we%0Aintroduce%20TISER%2C%20a%20novel%20framework%20that%20enhances%20the%20temporal%20reasoning%0Aabilities%20of%20LLMs%20through%20a%20multi-stage%20process%20that%20combines%20timeline%0Aconstruction%20with%20iterative%20self-reflection.%20Our%20approach%20leverages%20test-time%0Ascaling%20to%20extend%20the%20length%20of%20reasoning%20traces%2C%20enabling%20models%20to%20capture%0Acomplex%20temporal%20dependencies%20more%20effectively.%20This%20strategy%20not%20only%20boosts%0Areasoning%20accuracy%20but%20also%20improves%20the%20traceability%20of%20the%20inference%20process.%0AExperimental%20results%20demonstrate%20state-of-the-art%20performance%20across%20multiple%0Abenchmarks%2C%20including%20out-of-distribution%20test%20sets%2C%20and%20reveal%20that%20TISER%0Aenables%20smaller%20open-source%20models%20to%20surpass%20larger%20closed-weight%20models%20on%0Achallenging%20temporal%20reasoning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.05258v2&entry.124074799=Read"},
{"title": "TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning\n  for Enhancing LLMs' Social Intelligence", "author": "Guiyang Hou and Xing Gao and Yuchuan Wu and Xiang Huang and Wenqi Zhang and Zhe Zheng and Yongliang Shen and Jialu Du and Fei Huang and Yongbin Li and Weiming Lu", "abstract": "  Recently, Large Language Models (LLMs) have made significant progress in\nIQ-related domains that require careful thinking, such as mathematics and\ncoding. However, enhancing LLMs' cognitive development in social domains,\nparticularly from a post-training perspective, remains underexplored.\nRecognizing that the social world follows a distinct timeline and requires a\nricher blend of cognitive modes (from intuitive reactions (System 1) and\nsurface-level thinking to deliberate thinking (System 2)) than mathematics,\nwhich primarily relies on System 2 cognition (careful, step-by-step reasoning),\nwe introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning\n(TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we\nsystematically explore improving LLMs' social intelligence and validate the\neffectiveness of the TimeHC-RL method, through five other post-training\nparadigms and two test-time intervention paradigms on eight datasets with\ndiverse data patterns. Experimental results reveal the superiority of our\nproposed TimeHC-RL method compared to the widely adopted System 2 RL method. It\ngives the 7B backbone model wings, enabling it to rival the performance of\nadvanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic\nexploration from post-training and test-time interventions perspectives to\nimprove LLMs' social intelligence has uncovered several valuable insights.\n", "link": "http://arxiv.org/abs/2505.24500v1", "date": "2025-05-30", "relevancy": 2.5786, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5366}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5053}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TimeHC-RL%3A%20Temporal-aware%20Hierarchical%20Cognitive%20Reinforcement%20Learning%0A%20%20for%20Enhancing%20LLMs%27%20Social%20Intelligence&body=Title%3A%20TimeHC-RL%3A%20Temporal-aware%20Hierarchical%20Cognitive%20Reinforcement%20Learning%0A%20%20for%20Enhancing%20LLMs%27%20Social%20Intelligence%0AAuthor%3A%20Guiyang%20Hou%20and%20Xing%20Gao%20and%20Yuchuan%20Wu%20and%20Xiang%20Huang%20and%20Wenqi%20Zhang%20and%20Zhe%20Zheng%20and%20Yongliang%20Shen%20and%20Jialu%20Du%20and%20Fei%20Huang%20and%20Yongbin%20Li%20and%20Weiming%20Lu%0AAbstract%3A%20%20%20Recently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20made%20significant%20progress%20in%0AIQ-related%20domains%20that%20require%20careful%20thinking%2C%20such%20as%20mathematics%20and%0Acoding.%20However%2C%20enhancing%20LLMs%27%20cognitive%20development%20in%20social%20domains%2C%0Aparticularly%20from%20a%20post-training%20perspective%2C%20remains%20underexplored.%0ARecognizing%20that%20the%20social%20world%20follows%20a%20distinct%20timeline%20and%20requires%20a%0Aricher%20blend%20of%20cognitive%20modes%20%28from%20intuitive%20reactions%20%28System%201%29%20and%0Asurface-level%20thinking%20to%20deliberate%20thinking%20%28System%202%29%29%20than%20mathematics%2C%0Awhich%20primarily%20relies%20on%20System%202%20cognition%20%28careful%2C%20step-by-step%20reasoning%29%2C%0Awe%20introduce%20Temporal-aware%20Hierarchical%20Cognitive%20Reinforcement%20Learning%0A%28TimeHC-RL%29%20for%20enhancing%20LLMs%27%20social%20intelligence.%20In%20our%20experiments%2C%20we%0Asystematically%20explore%20improving%20LLMs%27%20social%20intelligence%20and%20validate%20the%0Aeffectiveness%20of%20the%20TimeHC-RL%20method%2C%20through%20five%20other%20post-training%0Aparadigms%20and%20two%20test-time%20intervention%20paradigms%20on%20eight%20datasets%20with%0Adiverse%20data%20patterns.%20Experimental%20results%20reveal%20the%20superiority%20of%20our%0Aproposed%20TimeHC-RL%20method%20compared%20to%20the%20widely%20adopted%20System%202%20RL%20method.%20It%0Agives%20the%207B%20backbone%20model%20wings%2C%20enabling%20it%20to%20rival%20the%20performance%20of%0Aadvanced%20models%20like%20DeepSeek-R1%20and%20OpenAI-O3.%20Additionally%2C%20the%20systematic%0Aexploration%20from%20post-training%20and%20test-time%20interventions%20perspectives%20to%0Aimprove%20LLMs%27%20social%20intelligence%20has%20uncovered%20several%20valuable%20insights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24500v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTimeHC-RL%253A%2520Temporal-aware%2520Hierarchical%2520Cognitive%2520Reinforcement%2520Learning%250A%2520%2520for%2520Enhancing%2520LLMs%2527%2520Social%2520Intelligence%26entry.906535625%3DGuiyang%2520Hou%2520and%2520Xing%2520Gao%2520and%2520Yuchuan%2520Wu%2520and%2520Xiang%2520Huang%2520and%2520Wenqi%2520Zhang%2520and%2520Zhe%2520Zheng%2520and%2520Yongliang%2520Shen%2520and%2520Jialu%2520Du%2520and%2520Fei%2520Huang%2520and%2520Yongbin%2520Li%2520and%2520Weiming%2520Lu%26entry.1292438233%3D%2520%2520Recently%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520made%2520significant%2520progress%2520in%250AIQ-related%2520domains%2520that%2520require%2520careful%2520thinking%252C%2520such%2520as%2520mathematics%2520and%250Acoding.%2520However%252C%2520enhancing%2520LLMs%2527%2520cognitive%2520development%2520in%2520social%2520domains%252C%250Aparticularly%2520from%2520a%2520post-training%2520perspective%252C%2520remains%2520underexplored.%250ARecognizing%2520that%2520the%2520social%2520world%2520follows%2520a%2520distinct%2520timeline%2520and%2520requires%2520a%250Aricher%2520blend%2520of%2520cognitive%2520modes%2520%2528from%2520intuitive%2520reactions%2520%2528System%25201%2529%2520and%250Asurface-level%2520thinking%2520to%2520deliberate%2520thinking%2520%2528System%25202%2529%2529%2520than%2520mathematics%252C%250Awhich%2520primarily%2520relies%2520on%2520System%25202%2520cognition%2520%2528careful%252C%2520step-by-step%2520reasoning%2529%252C%250Awe%2520introduce%2520Temporal-aware%2520Hierarchical%2520Cognitive%2520Reinforcement%2520Learning%250A%2528TimeHC-RL%2529%2520for%2520enhancing%2520LLMs%2527%2520social%2520intelligence.%2520In%2520our%2520experiments%252C%2520we%250Asystematically%2520explore%2520improving%2520LLMs%2527%2520social%2520intelligence%2520and%2520validate%2520the%250Aeffectiveness%2520of%2520the%2520TimeHC-RL%2520method%252C%2520through%2520five%2520other%2520post-training%250Aparadigms%2520and%2520two%2520test-time%2520intervention%2520paradigms%2520on%2520eight%2520datasets%2520with%250Adiverse%2520data%2520patterns.%2520Experimental%2520results%2520reveal%2520the%2520superiority%2520of%2520our%250Aproposed%2520TimeHC-RL%2520method%2520compared%2520to%2520the%2520widely%2520adopted%2520System%25202%2520RL%2520method.%2520It%250Agives%2520the%25207B%2520backbone%2520model%2520wings%252C%2520enabling%2520it%2520to%2520rival%2520the%2520performance%2520of%250Aadvanced%2520models%2520like%2520DeepSeek-R1%2520and%2520OpenAI-O3.%2520Additionally%252C%2520the%2520systematic%250Aexploration%2520from%2520post-training%2520and%2520test-time%2520interventions%2520perspectives%2520to%250Aimprove%2520LLMs%2527%2520social%2520intelligence%2520has%2520uncovered%2520several%2520valuable%2520insights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24500v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TimeHC-RL%3A%20Temporal-aware%20Hierarchical%20Cognitive%20Reinforcement%20Learning%0A%20%20for%20Enhancing%20LLMs%27%20Social%20Intelligence&entry.906535625=Guiyang%20Hou%20and%20Xing%20Gao%20and%20Yuchuan%20Wu%20and%20Xiang%20Huang%20and%20Wenqi%20Zhang%20and%20Zhe%20Zheng%20and%20Yongliang%20Shen%20and%20Jialu%20Du%20and%20Fei%20Huang%20and%20Yongbin%20Li%20and%20Weiming%20Lu&entry.1292438233=%20%20Recently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20made%20significant%20progress%20in%0AIQ-related%20domains%20that%20require%20careful%20thinking%2C%20such%20as%20mathematics%20and%0Acoding.%20However%2C%20enhancing%20LLMs%27%20cognitive%20development%20in%20social%20domains%2C%0Aparticularly%20from%20a%20post-training%20perspective%2C%20remains%20underexplored.%0ARecognizing%20that%20the%20social%20world%20follows%20a%20distinct%20timeline%20and%20requires%20a%0Aricher%20blend%20of%20cognitive%20modes%20%28from%20intuitive%20reactions%20%28System%201%29%20and%0Asurface-level%20thinking%20to%20deliberate%20thinking%20%28System%202%29%29%20than%20mathematics%2C%0Awhich%20primarily%20relies%20on%20System%202%20cognition%20%28careful%2C%20step-by-step%20reasoning%29%2C%0Awe%20introduce%20Temporal-aware%20Hierarchical%20Cognitive%20Reinforcement%20Learning%0A%28TimeHC-RL%29%20for%20enhancing%20LLMs%27%20social%20intelligence.%20In%20our%20experiments%2C%20we%0Asystematically%20explore%20improving%20LLMs%27%20social%20intelligence%20and%20validate%20the%0Aeffectiveness%20of%20the%20TimeHC-RL%20method%2C%20through%20five%20other%20post-training%0Aparadigms%20and%20two%20test-time%20intervention%20paradigms%20on%20eight%20datasets%20with%0Adiverse%20data%20patterns.%20Experimental%20results%20reveal%20the%20superiority%20of%20our%0Aproposed%20TimeHC-RL%20method%20compared%20to%20the%20widely%20adopted%20System%202%20RL%20method.%20It%0Agives%20the%207B%20backbone%20model%20wings%2C%20enabling%20it%20to%20rival%20the%20performance%20of%0Aadvanced%20models%20like%20DeepSeek-R1%20and%20OpenAI-O3.%20Additionally%2C%20the%20systematic%0Aexploration%20from%20post-training%20and%20test-time%20interventions%20perspectives%20to%0Aimprove%20LLMs%27%20social%20intelligence%20has%20uncovered%20several%20valuable%20insights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24500v1&entry.124074799=Read"},
{"title": "RaaS: Reasoning-Aware Attention Sparsity for Efficient LLM Reasoning", "author": "Junhao Hu and Wenrui Huang and Weidong Wang and Zhenwen Li and Tiancheng Hu and Zhixia Liu and Xusheng Chen and Tao Xie and Yizhou Shan", "abstract": "  Large Language Models (LLMs) have demonstrated strong capabilities across\nvarious domains, with recent advancements in challenging reasoning tasks such\nas mathematics and programming. However, solving reasoning tasks often requires\nan LLM to generate long sequences, incurring $O(N)$ time and memory\ncomplexities per token, where $N$ is the current sequence length. To reduce\ncomplexities, existing sparsity-based algorithms propose to retain Key-Value\n(KV) vectors, the intermediate representations of only the most critical\ntokens. However, these algorithms struggle with the \"impossible trinity\" of\naccuracy, time, and memory. For example, the state-of-the-art algorithm, Quest,\nachieves high accuracy with $O(L)$ time but $O(N)$ memory ($L$ is the cache\nbudget, $L \\ll N$). To address the \"impossible trinity\", in this paper, we\nidentify a new attention pattern during the decode stage of reasoning tasks,\nwhere milestone tokens (analogous to lemmas in mathematical proofs) emerge, are\nutilized, and then become unimportant afterward. Based on this pattern, we\npropose a new algorithm RaaS that identifies milestone tokens and retains their\nKV vectors until they are no longer needed, achieving high accuracy with $O(L)$\ntime and $O(L)$ memory complexities.\n", "link": "http://arxiv.org/abs/2502.11147v2", "date": "2025-05-30", "relevancy": 2.5755, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5373}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RaaS%3A%20Reasoning-Aware%20Attention%20Sparsity%20for%20Efficient%20LLM%20Reasoning&body=Title%3A%20RaaS%3A%20Reasoning-Aware%20Attention%20Sparsity%20for%20Efficient%20LLM%20Reasoning%0AAuthor%3A%20Junhao%20Hu%20and%20Wenrui%20Huang%20and%20Weidong%20Wang%20and%20Zhenwen%20Li%20and%20Tiancheng%20Hu%20and%20Zhixia%20Liu%20and%20Xusheng%20Chen%20and%20Tao%20Xie%20and%20Yizhou%20Shan%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20strong%20capabilities%20across%0Avarious%20domains%2C%20with%20recent%20advancements%20in%20challenging%20reasoning%20tasks%20such%0Aas%20mathematics%20and%20programming.%20However%2C%20solving%20reasoning%20tasks%20often%20requires%0Aan%20LLM%20to%20generate%20long%20sequences%2C%20incurring%20%24O%28N%29%24%20time%20and%20memory%0Acomplexities%20per%20token%2C%20where%20%24N%24%20is%20the%20current%20sequence%20length.%20To%20reduce%0Acomplexities%2C%20existing%20sparsity-based%20algorithms%20propose%20to%20retain%20Key-Value%0A%28KV%29%20vectors%2C%20the%20intermediate%20representations%20of%20only%20the%20most%20critical%0Atokens.%20However%2C%20these%20algorithms%20struggle%20with%20the%20%22impossible%20trinity%22%20of%0Aaccuracy%2C%20time%2C%20and%20memory.%20For%20example%2C%20the%20state-of-the-art%20algorithm%2C%20Quest%2C%0Aachieves%20high%20accuracy%20with%20%24O%28L%29%24%20time%20but%20%24O%28N%29%24%20memory%20%28%24L%24%20is%20the%20cache%0Abudget%2C%20%24L%20%5Cll%20N%24%29.%20To%20address%20the%20%22impossible%20trinity%22%2C%20in%20this%20paper%2C%20we%0Aidentify%20a%20new%20attention%20pattern%20during%20the%20decode%20stage%20of%20reasoning%20tasks%2C%0Awhere%20milestone%20tokens%20%28analogous%20to%20lemmas%20in%20mathematical%20proofs%29%20emerge%2C%20are%0Autilized%2C%20and%20then%20become%20unimportant%20afterward.%20Based%20on%20this%20pattern%2C%20we%0Apropose%20a%20new%20algorithm%20RaaS%20that%20identifies%20milestone%20tokens%20and%20retains%20their%0AKV%20vectors%20until%20they%20are%20no%20longer%20needed%2C%20achieving%20high%20accuracy%20with%20%24O%28L%29%24%0Atime%20and%20%24O%28L%29%24%20memory%20complexities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11147v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRaaS%253A%2520Reasoning-Aware%2520Attention%2520Sparsity%2520for%2520Efficient%2520LLM%2520Reasoning%26entry.906535625%3DJunhao%2520Hu%2520and%2520Wenrui%2520Huang%2520and%2520Weidong%2520Wang%2520and%2520Zhenwen%2520Li%2520and%2520Tiancheng%2520Hu%2520and%2520Zhixia%2520Liu%2520and%2520Xusheng%2520Chen%2520and%2520Tao%2520Xie%2520and%2520Yizhou%2520Shan%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520strong%2520capabilities%2520across%250Avarious%2520domains%252C%2520with%2520recent%2520advancements%2520in%2520challenging%2520reasoning%2520tasks%2520such%250Aas%2520mathematics%2520and%2520programming.%2520However%252C%2520solving%2520reasoning%2520tasks%2520often%2520requires%250Aan%2520LLM%2520to%2520generate%2520long%2520sequences%252C%2520incurring%2520%2524O%2528N%2529%2524%2520time%2520and%2520memory%250Acomplexities%2520per%2520token%252C%2520where%2520%2524N%2524%2520is%2520the%2520current%2520sequence%2520length.%2520To%2520reduce%250Acomplexities%252C%2520existing%2520sparsity-based%2520algorithms%2520propose%2520to%2520retain%2520Key-Value%250A%2528KV%2529%2520vectors%252C%2520the%2520intermediate%2520representations%2520of%2520only%2520the%2520most%2520critical%250Atokens.%2520However%252C%2520these%2520algorithms%2520struggle%2520with%2520the%2520%2522impossible%2520trinity%2522%2520of%250Aaccuracy%252C%2520time%252C%2520and%2520memory.%2520For%2520example%252C%2520the%2520state-of-the-art%2520algorithm%252C%2520Quest%252C%250Aachieves%2520high%2520accuracy%2520with%2520%2524O%2528L%2529%2524%2520time%2520but%2520%2524O%2528N%2529%2524%2520memory%2520%2528%2524L%2524%2520is%2520the%2520cache%250Abudget%252C%2520%2524L%2520%255Cll%2520N%2524%2529.%2520To%2520address%2520the%2520%2522impossible%2520trinity%2522%252C%2520in%2520this%2520paper%252C%2520we%250Aidentify%2520a%2520new%2520attention%2520pattern%2520during%2520the%2520decode%2520stage%2520of%2520reasoning%2520tasks%252C%250Awhere%2520milestone%2520tokens%2520%2528analogous%2520to%2520lemmas%2520in%2520mathematical%2520proofs%2529%2520emerge%252C%2520are%250Autilized%252C%2520and%2520then%2520become%2520unimportant%2520afterward.%2520Based%2520on%2520this%2520pattern%252C%2520we%250Apropose%2520a%2520new%2520algorithm%2520RaaS%2520that%2520identifies%2520milestone%2520tokens%2520and%2520retains%2520their%250AKV%2520vectors%2520until%2520they%2520are%2520no%2520longer%2520needed%252C%2520achieving%2520high%2520accuracy%2520with%2520%2524O%2528L%2529%2524%250Atime%2520and%2520%2524O%2528L%2529%2524%2520memory%2520complexities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11147v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RaaS%3A%20Reasoning-Aware%20Attention%20Sparsity%20for%20Efficient%20LLM%20Reasoning&entry.906535625=Junhao%20Hu%20and%20Wenrui%20Huang%20and%20Weidong%20Wang%20and%20Zhenwen%20Li%20and%20Tiancheng%20Hu%20and%20Zhixia%20Liu%20and%20Xusheng%20Chen%20and%20Tao%20Xie%20and%20Yizhou%20Shan&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20strong%20capabilities%20across%0Avarious%20domains%2C%20with%20recent%20advancements%20in%20challenging%20reasoning%20tasks%20such%0Aas%20mathematics%20and%20programming.%20However%2C%20solving%20reasoning%20tasks%20often%20requires%0Aan%20LLM%20to%20generate%20long%20sequences%2C%20incurring%20%24O%28N%29%24%20time%20and%20memory%0Acomplexities%20per%20token%2C%20where%20%24N%24%20is%20the%20current%20sequence%20length.%20To%20reduce%0Acomplexities%2C%20existing%20sparsity-based%20algorithms%20propose%20to%20retain%20Key-Value%0A%28KV%29%20vectors%2C%20the%20intermediate%20representations%20of%20only%20the%20most%20critical%0Atokens.%20However%2C%20these%20algorithms%20struggle%20with%20the%20%22impossible%20trinity%22%20of%0Aaccuracy%2C%20time%2C%20and%20memory.%20For%20example%2C%20the%20state-of-the-art%20algorithm%2C%20Quest%2C%0Aachieves%20high%20accuracy%20with%20%24O%28L%29%24%20time%20but%20%24O%28N%29%24%20memory%20%28%24L%24%20is%20the%20cache%0Abudget%2C%20%24L%20%5Cll%20N%24%29.%20To%20address%20the%20%22impossible%20trinity%22%2C%20in%20this%20paper%2C%20we%0Aidentify%20a%20new%20attention%20pattern%20during%20the%20decode%20stage%20of%20reasoning%20tasks%2C%0Awhere%20milestone%20tokens%20%28analogous%20to%20lemmas%20in%20mathematical%20proofs%29%20emerge%2C%20are%0Autilized%2C%20and%20then%20become%20unimportant%20afterward.%20Based%20on%20this%20pattern%2C%20we%0Apropose%20a%20new%20algorithm%20RaaS%20that%20identifies%20milestone%20tokens%20and%20retains%20their%0AKV%20vectors%20until%20they%20are%20no%20longer%20needed%2C%20achieving%20high%20accuracy%20with%20%24O%28L%29%24%0Atime%20and%20%24O%28L%29%24%20memory%20complexities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11147v2&entry.124074799=Read"},
{"title": "CL-LoRA: Continual Low-Rank Adaptation for Rehearsal-Free\n  Class-Incremental Learning", "author": "Jiangpeng He and Zhihao Duan and Fengqing Zhu", "abstract": "  Class-Incremental Learning (CIL) aims to learn new classes sequentially while\nretaining the knowledge of previously learned classes. Recently, pre-trained\nmodels (PTMs) combined with parameter-efficient fine-tuning (PEFT) have shown\nremarkable performance in rehearsal-free CIL without requiring exemplars from\nprevious tasks. However, existing adapter-based methods, which incorporate\nlightweight learnable modules into PTMs for CIL, create new adapters for each\nnew task, leading to both parameter redundancy and failure to leverage shared\nknowledge across tasks. In this work, we propose ContinuaL Low-Rank Adaptation\n(CL-LoRA), which introduces a novel dual-adapter architecture combining\n\\textbf{task-shared adapters} to learn cross-task knowledge and\n\\textbf{task-specific adapters} to capture unique features of each new task.\nSpecifically, the shared adapters utilize random orthogonal matrices and\nleverage knowledge distillation with gradient reassignment to preserve\nessential shared knowledge. In addition, we introduce learnable block-wise\nweights for task-specific adapters, which mitigate inter-task interference\nwhile maintaining the model's plasticity. We demonstrate CL-LoRA consistently\nachieves promising performance under multiple benchmarks with reduced training\nand inference computation, establishing a more efficient and scalable paradigm\nfor continual learning with pre-trained models.\n", "link": "http://arxiv.org/abs/2505.24816v1", "date": "2025-05-30", "relevancy": 2.5694, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5755}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4909}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CL-LoRA%3A%20Continual%20Low-Rank%20Adaptation%20for%20Rehearsal-Free%0A%20%20Class-Incremental%20Learning&body=Title%3A%20CL-LoRA%3A%20Continual%20Low-Rank%20Adaptation%20for%20Rehearsal-Free%0A%20%20Class-Incremental%20Learning%0AAuthor%3A%20Jiangpeng%20He%20and%20Zhihao%20Duan%20and%20Fengqing%20Zhu%0AAbstract%3A%20%20%20Class-Incremental%20Learning%20%28CIL%29%20aims%20to%20learn%20new%20classes%20sequentially%20while%0Aretaining%20the%20knowledge%20of%20previously%20learned%20classes.%20Recently%2C%20pre-trained%0Amodels%20%28PTMs%29%20combined%20with%20parameter-efficient%20fine-tuning%20%28PEFT%29%20have%20shown%0Aremarkable%20performance%20in%20rehearsal-free%20CIL%20without%20requiring%20exemplars%20from%0Aprevious%20tasks.%20However%2C%20existing%20adapter-based%20methods%2C%20which%20incorporate%0Alightweight%20learnable%20modules%20into%20PTMs%20for%20CIL%2C%20create%20new%20adapters%20for%20each%0Anew%20task%2C%20leading%20to%20both%20parameter%20redundancy%20and%20failure%20to%20leverage%20shared%0Aknowledge%20across%20tasks.%20In%20this%20work%2C%20we%20propose%20ContinuaL%20Low-Rank%20Adaptation%0A%28CL-LoRA%29%2C%20which%20introduces%20a%20novel%20dual-adapter%20architecture%20combining%0A%5Ctextbf%7Btask-shared%20adapters%7D%20to%20learn%20cross-task%20knowledge%20and%0A%5Ctextbf%7Btask-specific%20adapters%7D%20to%20capture%20unique%20features%20of%20each%20new%20task.%0ASpecifically%2C%20the%20shared%20adapters%20utilize%20random%20orthogonal%20matrices%20and%0Aleverage%20knowledge%20distillation%20with%20gradient%20reassignment%20to%20preserve%0Aessential%20shared%20knowledge.%20In%20addition%2C%20we%20introduce%20learnable%20block-wise%0Aweights%20for%20task-specific%20adapters%2C%20which%20mitigate%20inter-task%20interference%0Awhile%20maintaining%20the%20model%27s%20plasticity.%20We%20demonstrate%20CL-LoRA%20consistently%0Aachieves%20promising%20performance%20under%20multiple%20benchmarks%20with%20reduced%20training%0Aand%20inference%20computation%2C%20establishing%20a%20more%20efficient%20and%20scalable%20paradigm%0Afor%20continual%20learning%20with%20pre-trained%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24816v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCL-LoRA%253A%2520Continual%2520Low-Rank%2520Adaptation%2520for%2520Rehearsal-Free%250A%2520%2520Class-Incremental%2520Learning%26entry.906535625%3DJiangpeng%2520He%2520and%2520Zhihao%2520Duan%2520and%2520Fengqing%2520Zhu%26entry.1292438233%3D%2520%2520Class-Incremental%2520Learning%2520%2528CIL%2529%2520aims%2520to%2520learn%2520new%2520classes%2520sequentially%2520while%250Aretaining%2520the%2520knowledge%2520of%2520previously%2520learned%2520classes.%2520Recently%252C%2520pre-trained%250Amodels%2520%2528PTMs%2529%2520combined%2520with%2520parameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520have%2520shown%250Aremarkable%2520performance%2520in%2520rehearsal-free%2520CIL%2520without%2520requiring%2520exemplars%2520from%250Aprevious%2520tasks.%2520However%252C%2520existing%2520adapter-based%2520methods%252C%2520which%2520incorporate%250Alightweight%2520learnable%2520modules%2520into%2520PTMs%2520for%2520CIL%252C%2520create%2520new%2520adapters%2520for%2520each%250Anew%2520task%252C%2520leading%2520to%2520both%2520parameter%2520redundancy%2520and%2520failure%2520to%2520leverage%2520shared%250Aknowledge%2520across%2520tasks.%2520In%2520this%2520work%252C%2520we%2520propose%2520ContinuaL%2520Low-Rank%2520Adaptation%250A%2528CL-LoRA%2529%252C%2520which%2520introduces%2520a%2520novel%2520dual-adapter%2520architecture%2520combining%250A%255Ctextbf%257Btask-shared%2520adapters%257D%2520to%2520learn%2520cross-task%2520knowledge%2520and%250A%255Ctextbf%257Btask-specific%2520adapters%257D%2520to%2520capture%2520unique%2520features%2520of%2520each%2520new%2520task.%250ASpecifically%252C%2520the%2520shared%2520adapters%2520utilize%2520random%2520orthogonal%2520matrices%2520and%250Aleverage%2520knowledge%2520distillation%2520with%2520gradient%2520reassignment%2520to%2520preserve%250Aessential%2520shared%2520knowledge.%2520In%2520addition%252C%2520we%2520introduce%2520learnable%2520block-wise%250Aweights%2520for%2520task-specific%2520adapters%252C%2520which%2520mitigate%2520inter-task%2520interference%250Awhile%2520maintaining%2520the%2520model%2527s%2520plasticity.%2520We%2520demonstrate%2520CL-LoRA%2520consistently%250Aachieves%2520promising%2520performance%2520under%2520multiple%2520benchmarks%2520with%2520reduced%2520training%250Aand%2520inference%2520computation%252C%2520establishing%2520a%2520more%2520efficient%2520and%2520scalable%2520paradigm%250Afor%2520continual%2520learning%2520with%2520pre-trained%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24816v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CL-LoRA%3A%20Continual%20Low-Rank%20Adaptation%20for%20Rehearsal-Free%0A%20%20Class-Incremental%20Learning&entry.906535625=Jiangpeng%20He%20and%20Zhihao%20Duan%20and%20Fengqing%20Zhu&entry.1292438233=%20%20Class-Incremental%20Learning%20%28CIL%29%20aims%20to%20learn%20new%20classes%20sequentially%20while%0Aretaining%20the%20knowledge%20of%20previously%20learned%20classes.%20Recently%2C%20pre-trained%0Amodels%20%28PTMs%29%20combined%20with%20parameter-efficient%20fine-tuning%20%28PEFT%29%20have%20shown%0Aremarkable%20performance%20in%20rehearsal-free%20CIL%20without%20requiring%20exemplars%20from%0Aprevious%20tasks.%20However%2C%20existing%20adapter-based%20methods%2C%20which%20incorporate%0Alightweight%20learnable%20modules%20into%20PTMs%20for%20CIL%2C%20create%20new%20adapters%20for%20each%0Anew%20task%2C%20leading%20to%20both%20parameter%20redundancy%20and%20failure%20to%20leverage%20shared%0Aknowledge%20across%20tasks.%20In%20this%20work%2C%20we%20propose%20ContinuaL%20Low-Rank%20Adaptation%0A%28CL-LoRA%29%2C%20which%20introduces%20a%20novel%20dual-adapter%20architecture%20combining%0A%5Ctextbf%7Btask-shared%20adapters%7D%20to%20learn%20cross-task%20knowledge%20and%0A%5Ctextbf%7Btask-specific%20adapters%7D%20to%20capture%20unique%20features%20of%20each%20new%20task.%0ASpecifically%2C%20the%20shared%20adapters%20utilize%20random%20orthogonal%20matrices%20and%0Aleverage%20knowledge%20distillation%20with%20gradient%20reassignment%20to%20preserve%0Aessential%20shared%20knowledge.%20In%20addition%2C%20we%20introduce%20learnable%20block-wise%0Aweights%20for%20task-specific%20adapters%2C%20which%20mitigate%20inter-task%20interference%0Awhile%20maintaining%20the%20model%27s%20plasticity.%20We%20demonstrate%20CL-LoRA%20consistently%0Aachieves%20promising%20performance%20under%20multiple%20benchmarks%20with%20reduced%20training%0Aand%20inference%20computation%2C%20establishing%20a%20more%20efficient%20and%20scalable%20paradigm%0Afor%20continual%20learning%20with%20pre-trained%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24816v1&entry.124074799=Read"},
{"title": "Digital twins enable full-reference quality assessment of photoacoustic\n  image reconstructions", "author": "Janek Gr\u00f6hl and Leonid Kunyansky and Jenni Poimala and Thomas R. Else and Francesca Di Cecio and Sarah E. Bohndiek and Ben T. Cox and Andreas Hauptmann", "abstract": "  Quantitative comparison of the quality of photoacoustic image reconstruction\nalgorithms remains a major challenge. No-reference image quality measures are\noften inadequate, but full-reference measures require access to an ideal\nreference image. While the ground truth is known in simulations, it is unknown\nin vivo, or in phantom studies, as the reference depends on both the phantom\nproperties and the imaging system. We tackle this problem by using numerical\ndigital twins of tissue-mimicking phantoms and the imaging system to perform a\nquantitative calibration to reduce the simulation gap. The contributions of\nthis paper are two-fold: First, we use this digital-twin framework to compare\nmultiple state-of-the-art reconstruction algorithms. Second, among these is a\nFourier transform-based reconstruction algorithm for circular detection\ngeometries, which we test on experimental data for the first time. Our results\ndemonstrate the usefulness of digital phantom twins by enabling assessment of\nthe accuracy of the numerical forward model and enabling comparison of image\nreconstruction schemes with full-reference image quality assessment. We show\nthat the Fourier transform-based algorithm yields results comparable to those\nof iterative time reversal, but at a lower computational cost. All data and\ncode are publicly available on Zenodo: https://doi.org/10.5281/zenodo.15388429.\n", "link": "http://arxiv.org/abs/2505.24514v1", "date": "2025-05-30", "relevancy": 2.5668, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5152}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5152}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Digital%20twins%20enable%20full-reference%20quality%20assessment%20of%20photoacoustic%0A%20%20image%20reconstructions&body=Title%3A%20Digital%20twins%20enable%20full-reference%20quality%20assessment%20of%20photoacoustic%0A%20%20image%20reconstructions%0AAuthor%3A%20Janek%20Gr%C3%B6hl%20and%20Leonid%20Kunyansky%20and%20Jenni%20Poimala%20and%20Thomas%20R.%20Else%20and%20Francesca%20Di%20Cecio%20and%20Sarah%20E.%20Bohndiek%20and%20Ben%20T.%20Cox%20and%20Andreas%20Hauptmann%0AAbstract%3A%20%20%20Quantitative%20comparison%20of%20the%20quality%20of%20photoacoustic%20image%20reconstruction%0Aalgorithms%20remains%20a%20major%20challenge.%20No-reference%20image%20quality%20measures%20are%0Aoften%20inadequate%2C%20but%20full-reference%20measures%20require%20access%20to%20an%20ideal%0Areference%20image.%20While%20the%20ground%20truth%20is%20known%20in%20simulations%2C%20it%20is%20unknown%0Ain%20vivo%2C%20or%20in%20phantom%20studies%2C%20as%20the%20reference%20depends%20on%20both%20the%20phantom%0Aproperties%20and%20the%20imaging%20system.%20We%20tackle%20this%20problem%20by%20using%20numerical%0Adigital%20twins%20of%20tissue-mimicking%20phantoms%20and%20the%20imaging%20system%20to%20perform%20a%0Aquantitative%20calibration%20to%20reduce%20the%20simulation%20gap.%20The%20contributions%20of%0Athis%20paper%20are%20two-fold%3A%20First%2C%20we%20use%20this%20digital-twin%20framework%20to%20compare%0Amultiple%20state-of-the-art%20reconstruction%20algorithms.%20Second%2C%20among%20these%20is%20a%0AFourier%20transform-based%20reconstruction%20algorithm%20for%20circular%20detection%0Ageometries%2C%20which%20we%20test%20on%20experimental%20data%20for%20the%20first%20time.%20Our%20results%0Ademonstrate%20the%20usefulness%20of%20digital%20phantom%20twins%20by%20enabling%20assessment%20of%0Athe%20accuracy%20of%20the%20numerical%20forward%20model%20and%20enabling%20comparison%20of%20image%0Areconstruction%20schemes%20with%20full-reference%20image%20quality%20assessment.%20We%20show%0Athat%20the%20Fourier%20transform-based%20algorithm%20yields%20results%20comparable%20to%20those%0Aof%20iterative%20time%20reversal%2C%20but%20at%20a%20lower%20computational%20cost.%20All%20data%20and%0Acode%20are%20publicly%20available%20on%20Zenodo%3A%20https%3A//doi.org/10.5281/zenodo.15388429.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24514v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDigital%2520twins%2520enable%2520full-reference%2520quality%2520assessment%2520of%2520photoacoustic%250A%2520%2520image%2520reconstructions%26entry.906535625%3DJanek%2520Gr%25C3%25B6hl%2520and%2520Leonid%2520Kunyansky%2520and%2520Jenni%2520Poimala%2520and%2520Thomas%2520R.%2520Else%2520and%2520Francesca%2520Di%2520Cecio%2520and%2520Sarah%2520E.%2520Bohndiek%2520and%2520Ben%2520T.%2520Cox%2520and%2520Andreas%2520Hauptmann%26entry.1292438233%3D%2520%2520Quantitative%2520comparison%2520of%2520the%2520quality%2520of%2520photoacoustic%2520image%2520reconstruction%250Aalgorithms%2520remains%2520a%2520major%2520challenge.%2520No-reference%2520image%2520quality%2520measures%2520are%250Aoften%2520inadequate%252C%2520but%2520full-reference%2520measures%2520require%2520access%2520to%2520an%2520ideal%250Areference%2520image.%2520While%2520the%2520ground%2520truth%2520is%2520known%2520in%2520simulations%252C%2520it%2520is%2520unknown%250Ain%2520vivo%252C%2520or%2520in%2520phantom%2520studies%252C%2520as%2520the%2520reference%2520depends%2520on%2520both%2520the%2520phantom%250Aproperties%2520and%2520the%2520imaging%2520system.%2520We%2520tackle%2520this%2520problem%2520by%2520using%2520numerical%250Adigital%2520twins%2520of%2520tissue-mimicking%2520phantoms%2520and%2520the%2520imaging%2520system%2520to%2520perform%2520a%250Aquantitative%2520calibration%2520to%2520reduce%2520the%2520simulation%2520gap.%2520The%2520contributions%2520of%250Athis%2520paper%2520are%2520two-fold%253A%2520First%252C%2520we%2520use%2520this%2520digital-twin%2520framework%2520to%2520compare%250Amultiple%2520state-of-the-art%2520reconstruction%2520algorithms.%2520Second%252C%2520among%2520these%2520is%2520a%250AFourier%2520transform-based%2520reconstruction%2520algorithm%2520for%2520circular%2520detection%250Ageometries%252C%2520which%2520we%2520test%2520on%2520experimental%2520data%2520for%2520the%2520first%2520time.%2520Our%2520results%250Ademonstrate%2520the%2520usefulness%2520of%2520digital%2520phantom%2520twins%2520by%2520enabling%2520assessment%2520of%250Athe%2520accuracy%2520of%2520the%2520numerical%2520forward%2520model%2520and%2520enabling%2520comparison%2520of%2520image%250Areconstruction%2520schemes%2520with%2520full-reference%2520image%2520quality%2520assessment.%2520We%2520show%250Athat%2520the%2520Fourier%2520transform-based%2520algorithm%2520yields%2520results%2520comparable%2520to%2520those%250Aof%2520iterative%2520time%2520reversal%252C%2520but%2520at%2520a%2520lower%2520computational%2520cost.%2520All%2520data%2520and%250Acode%2520are%2520publicly%2520available%2520on%2520Zenodo%253A%2520https%253A//doi.org/10.5281/zenodo.15388429.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24514v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Digital%20twins%20enable%20full-reference%20quality%20assessment%20of%20photoacoustic%0A%20%20image%20reconstructions&entry.906535625=Janek%20Gr%C3%B6hl%20and%20Leonid%20Kunyansky%20and%20Jenni%20Poimala%20and%20Thomas%20R.%20Else%20and%20Francesca%20Di%20Cecio%20and%20Sarah%20E.%20Bohndiek%20and%20Ben%20T.%20Cox%20and%20Andreas%20Hauptmann&entry.1292438233=%20%20Quantitative%20comparison%20of%20the%20quality%20of%20photoacoustic%20image%20reconstruction%0Aalgorithms%20remains%20a%20major%20challenge.%20No-reference%20image%20quality%20measures%20are%0Aoften%20inadequate%2C%20but%20full-reference%20measures%20require%20access%20to%20an%20ideal%0Areference%20image.%20While%20the%20ground%20truth%20is%20known%20in%20simulations%2C%20it%20is%20unknown%0Ain%20vivo%2C%20or%20in%20phantom%20studies%2C%20as%20the%20reference%20depends%20on%20both%20the%20phantom%0Aproperties%20and%20the%20imaging%20system.%20We%20tackle%20this%20problem%20by%20using%20numerical%0Adigital%20twins%20of%20tissue-mimicking%20phantoms%20and%20the%20imaging%20system%20to%20perform%20a%0Aquantitative%20calibration%20to%20reduce%20the%20simulation%20gap.%20The%20contributions%20of%0Athis%20paper%20are%20two-fold%3A%20First%2C%20we%20use%20this%20digital-twin%20framework%20to%20compare%0Amultiple%20state-of-the-art%20reconstruction%20algorithms.%20Second%2C%20among%20these%20is%20a%0AFourier%20transform-based%20reconstruction%20algorithm%20for%20circular%20detection%0Ageometries%2C%20which%20we%20test%20on%20experimental%20data%20for%20the%20first%20time.%20Our%20results%0Ademonstrate%20the%20usefulness%20of%20digital%20phantom%20twins%20by%20enabling%20assessment%20of%0Athe%20accuracy%20of%20the%20numerical%20forward%20model%20and%20enabling%20comparison%20of%20image%0Areconstruction%20schemes%20with%20full-reference%20image%20quality%20assessment.%20We%20show%0Athat%20the%20Fourier%20transform-based%20algorithm%20yields%20results%20comparable%20to%20those%0Aof%20iterative%20time%20reversal%2C%20but%20at%20a%20lower%20computational%20cost.%20All%20data%20and%0Acode%20are%20publicly%20available%20on%20Zenodo%3A%20https%3A//doi.org/10.5281/zenodo.15388429.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24514v1&entry.124074799=Read"},
{"title": "Advancing Molecular Graph-Text Pre-training via Fine-grained Alignment", "author": "Yibo Li and Yuan Fang and Mengmei Zhang and Chuan Shi", "abstract": "  Understanding molecular structure and related knowledge is crucial for\nscientific research. Recent studies integrate molecular graphs with their\ntextual descriptions to enhance molecular representation learning. However,\nthey focus on the whole molecular graph and neglect frequently occurring\nsubgraphs, known as motifs, which are essential for determining molecular\nproperties. Without such fine-grained knowledge, these models struggle to\ngeneralize to unseen molecules and tasks that require motif-level insights. To\nbridge this gap, we propose FineMolTex, a novel Fine-grained Molecular\ngraph-Text pre-training framework to jointly learn coarse-grained\nmolecule-level knowledge and fine-grained motif-level knowledge. Specifically,\nFineMolTex consists of two pre-training tasks: a contrastive alignment task for\ncoarse-grained matching and a masked multi-modal modeling task for fine-grained\nmatching. In particular, the latter predicts the labels of masked motifs and\nwords, which are selected based on their importance. By leveraging insights\nfrom both modalities, FineMolTex is able to understand the fine-grained\nmatching between motifs and words. Finally, we conduct extensive experiments\nacross three downstream tasks, achieving up to 230% improvement in the\ntext-based molecule editing task. Additionally, our case studies reveal that\nFineMolTex successfully captures fine-grained knowledge, potentially offering\nvaluable insights for drug discovery and catalyst design.\n", "link": "http://arxiv.org/abs/2409.14106v5", "date": "2025-05-30", "relevancy": 2.5643, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5654}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.49}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Molecular%20Graph-Text%20Pre-training%20via%20Fine-grained%20Alignment&body=Title%3A%20Advancing%20Molecular%20Graph-Text%20Pre-training%20via%20Fine-grained%20Alignment%0AAuthor%3A%20Yibo%20Li%20and%20Yuan%20Fang%20and%20Mengmei%20Zhang%20and%20Chuan%20Shi%0AAbstract%3A%20%20%20Understanding%20molecular%20structure%20and%20related%20knowledge%20is%20crucial%20for%0Ascientific%20research.%20Recent%20studies%20integrate%20molecular%20graphs%20with%20their%0Atextual%20descriptions%20to%20enhance%20molecular%20representation%20learning.%20However%2C%0Athey%20focus%20on%20the%20whole%20molecular%20graph%20and%20neglect%20frequently%20occurring%0Asubgraphs%2C%20known%20as%20motifs%2C%20which%20are%20essential%20for%20determining%20molecular%0Aproperties.%20Without%20such%20fine-grained%20knowledge%2C%20these%20models%20struggle%20to%0Ageneralize%20to%20unseen%20molecules%20and%20tasks%20that%20require%20motif-level%20insights.%20To%0Abridge%20this%20gap%2C%20we%20propose%20FineMolTex%2C%20a%20novel%20Fine-grained%20Molecular%0Agraph-Text%20pre-training%20framework%20to%20jointly%20learn%20coarse-grained%0Amolecule-level%20knowledge%20and%20fine-grained%20motif-level%20knowledge.%20Specifically%2C%0AFineMolTex%20consists%20of%20two%20pre-training%20tasks%3A%20a%20contrastive%20alignment%20task%20for%0Acoarse-grained%20matching%20and%20a%20masked%20multi-modal%20modeling%20task%20for%20fine-grained%0Amatching.%20In%20particular%2C%20the%20latter%20predicts%20the%20labels%20of%20masked%20motifs%20and%0Awords%2C%20which%20are%20selected%20based%20on%20their%20importance.%20By%20leveraging%20insights%0Afrom%20both%20modalities%2C%20FineMolTex%20is%20able%20to%20understand%20the%20fine-grained%0Amatching%20between%20motifs%20and%20words.%20Finally%2C%20we%20conduct%20extensive%20experiments%0Aacross%20three%20downstream%20tasks%2C%20achieving%20up%20to%20230%25%20improvement%20in%20the%0Atext-based%20molecule%20editing%20task.%20Additionally%2C%20our%20case%20studies%20reveal%20that%0AFineMolTex%20successfully%20captures%20fine-grained%20knowledge%2C%20potentially%20offering%0Avaluable%20insights%20for%20drug%20discovery%20and%20catalyst%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.14106v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Molecular%2520Graph-Text%2520Pre-training%2520via%2520Fine-grained%2520Alignment%26entry.906535625%3DYibo%2520Li%2520and%2520Yuan%2520Fang%2520and%2520Mengmei%2520Zhang%2520and%2520Chuan%2520Shi%26entry.1292438233%3D%2520%2520Understanding%2520molecular%2520structure%2520and%2520related%2520knowledge%2520is%2520crucial%2520for%250Ascientific%2520research.%2520Recent%2520studies%2520integrate%2520molecular%2520graphs%2520with%2520their%250Atextual%2520descriptions%2520to%2520enhance%2520molecular%2520representation%2520learning.%2520However%252C%250Athey%2520focus%2520on%2520the%2520whole%2520molecular%2520graph%2520and%2520neglect%2520frequently%2520occurring%250Asubgraphs%252C%2520known%2520as%2520motifs%252C%2520which%2520are%2520essential%2520for%2520determining%2520molecular%250Aproperties.%2520Without%2520such%2520fine-grained%2520knowledge%252C%2520these%2520models%2520struggle%2520to%250Ageneralize%2520to%2520unseen%2520molecules%2520and%2520tasks%2520that%2520require%2520motif-level%2520insights.%2520To%250Abridge%2520this%2520gap%252C%2520we%2520propose%2520FineMolTex%252C%2520a%2520novel%2520Fine-grained%2520Molecular%250Agraph-Text%2520pre-training%2520framework%2520to%2520jointly%2520learn%2520coarse-grained%250Amolecule-level%2520knowledge%2520and%2520fine-grained%2520motif-level%2520knowledge.%2520Specifically%252C%250AFineMolTex%2520consists%2520of%2520two%2520pre-training%2520tasks%253A%2520a%2520contrastive%2520alignment%2520task%2520for%250Acoarse-grained%2520matching%2520and%2520a%2520masked%2520multi-modal%2520modeling%2520task%2520for%2520fine-grained%250Amatching.%2520In%2520particular%252C%2520the%2520latter%2520predicts%2520the%2520labels%2520of%2520masked%2520motifs%2520and%250Awords%252C%2520which%2520are%2520selected%2520based%2520on%2520their%2520importance.%2520By%2520leveraging%2520insights%250Afrom%2520both%2520modalities%252C%2520FineMolTex%2520is%2520able%2520to%2520understand%2520the%2520fine-grained%250Amatching%2520between%2520motifs%2520and%2520words.%2520Finally%252C%2520we%2520conduct%2520extensive%2520experiments%250Aacross%2520three%2520downstream%2520tasks%252C%2520achieving%2520up%2520to%2520230%2525%2520improvement%2520in%2520the%250Atext-based%2520molecule%2520editing%2520task.%2520Additionally%252C%2520our%2520case%2520studies%2520reveal%2520that%250AFineMolTex%2520successfully%2520captures%2520fine-grained%2520knowledge%252C%2520potentially%2520offering%250Avaluable%2520insights%2520for%2520drug%2520discovery%2520and%2520catalyst%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.14106v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Molecular%20Graph-Text%20Pre-training%20via%20Fine-grained%20Alignment&entry.906535625=Yibo%20Li%20and%20Yuan%20Fang%20and%20Mengmei%20Zhang%20and%20Chuan%20Shi&entry.1292438233=%20%20Understanding%20molecular%20structure%20and%20related%20knowledge%20is%20crucial%20for%0Ascientific%20research.%20Recent%20studies%20integrate%20molecular%20graphs%20with%20their%0Atextual%20descriptions%20to%20enhance%20molecular%20representation%20learning.%20However%2C%0Athey%20focus%20on%20the%20whole%20molecular%20graph%20and%20neglect%20frequently%20occurring%0Asubgraphs%2C%20known%20as%20motifs%2C%20which%20are%20essential%20for%20determining%20molecular%0Aproperties.%20Without%20such%20fine-grained%20knowledge%2C%20these%20models%20struggle%20to%0Ageneralize%20to%20unseen%20molecules%20and%20tasks%20that%20require%20motif-level%20insights.%20To%0Abridge%20this%20gap%2C%20we%20propose%20FineMolTex%2C%20a%20novel%20Fine-grained%20Molecular%0Agraph-Text%20pre-training%20framework%20to%20jointly%20learn%20coarse-grained%0Amolecule-level%20knowledge%20and%20fine-grained%20motif-level%20knowledge.%20Specifically%2C%0AFineMolTex%20consists%20of%20two%20pre-training%20tasks%3A%20a%20contrastive%20alignment%20task%20for%0Acoarse-grained%20matching%20and%20a%20masked%20multi-modal%20modeling%20task%20for%20fine-grained%0Amatching.%20In%20particular%2C%20the%20latter%20predicts%20the%20labels%20of%20masked%20motifs%20and%0Awords%2C%20which%20are%20selected%20based%20on%20their%20importance.%20By%20leveraging%20insights%0Afrom%20both%20modalities%2C%20FineMolTex%20is%20able%20to%20understand%20the%20fine-grained%0Amatching%20between%20motifs%20and%20words.%20Finally%2C%20we%20conduct%20extensive%20experiments%0Aacross%20three%20downstream%20tasks%2C%20achieving%20up%20to%20230%25%20improvement%20in%20the%0Atext-based%20molecule%20editing%20task.%20Additionally%2C%20our%20case%20studies%20reveal%20that%0AFineMolTex%20successfully%20captures%20fine-grained%20knowledge%2C%20potentially%20offering%0Avaluable%20insights%20for%20drug%20discovery%20and%20catalyst%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.14106v5&entry.124074799=Read"},
{"title": "Zero-Shot Chinese Character Recognition with Hierarchical\n  Multi-Granularity Image-Text Aligning", "author": "Yinglian Zhu and Haiyang Yu and Qizao Wang and Wei Lu and Xiangyang Xue and Bin Li", "abstract": "  Chinese Character Recognition (CCR) is a fundamental technology for\nintelligent document processing. Unlike Latin characters, Chinese characters\nexhibit unique spatial structures and compositional rules, allowing for the use\nof fine-grained semantic information in representation. However, existing\napproaches are usually based on auto-regressive as well as edit distance\npost-process and typically rely on a single-level character representation. In\nthis paper, we propose a Hierarchical Multi-Granularity Image-Text Aligning\n(Hi-GITA) framework based on a contrastive paradigm. To leverage the abundant\nfine-grained semantic information of Chinese characters, we propose\nmulti-granularity encoders on both image and text sides. Specifically, the\nImage Multi-Granularity Encoder extracts hierarchical image representations\nfrom character images, capturing semantic cues from localized strokes to\nholistic structures. The Text Multi-Granularity Encoder extracts stroke and\nradical sequence representations at different levels of granularity. To better\ncapture the relationships between strokes and radicals, we introduce\nMulti-Granularity Fusion Modules on the image and text sides, respectively.\nFurthermore, to effectively bridge the two modalities, we further introduce a\nFine-Grained Decoupled Image-Text Contrastive loss, which aligns image and text\nrepresentations across multiple granularities. Extensive experiments\ndemonstrate that our proposed Hi-GITA significantly outperforms existing\nzero-shot CCR methods. For instance, it brings about 20% accuracy improvement\nin handwritten character and radical zero-shot settings. Code and models will\nbe released soon.\n", "link": "http://arxiv.org/abs/2505.24837v1", "date": "2025-05-30", "relevancy": 2.5632, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5554}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4996}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4829}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Chinese%20Character%20Recognition%20with%20Hierarchical%0A%20%20Multi-Granularity%20Image-Text%20Aligning&body=Title%3A%20Zero-Shot%20Chinese%20Character%20Recognition%20with%20Hierarchical%0A%20%20Multi-Granularity%20Image-Text%20Aligning%0AAuthor%3A%20Yinglian%20Zhu%20and%20Haiyang%20Yu%20and%20Qizao%20Wang%20and%20Wei%20Lu%20and%20Xiangyang%20Xue%20and%20Bin%20Li%0AAbstract%3A%20%20%20Chinese%20Character%20Recognition%20%28CCR%29%20is%20a%20fundamental%20technology%20for%0Aintelligent%20document%20processing.%20Unlike%20Latin%20characters%2C%20Chinese%20characters%0Aexhibit%20unique%20spatial%20structures%20and%20compositional%20rules%2C%20allowing%20for%20the%20use%0Aof%20fine-grained%20semantic%20information%20in%20representation.%20However%2C%20existing%0Aapproaches%20are%20usually%20based%20on%20auto-regressive%20as%20well%20as%20edit%20distance%0Apost-process%20and%20typically%20rely%20on%20a%20single-level%20character%20representation.%20In%0Athis%20paper%2C%20we%20propose%20a%20Hierarchical%20Multi-Granularity%20Image-Text%20Aligning%0A%28Hi-GITA%29%20framework%20based%20on%20a%20contrastive%20paradigm.%20To%20leverage%20the%20abundant%0Afine-grained%20semantic%20information%20of%20Chinese%20characters%2C%20we%20propose%0Amulti-granularity%20encoders%20on%20both%20image%20and%20text%20sides.%20Specifically%2C%20the%0AImage%20Multi-Granularity%20Encoder%20extracts%20hierarchical%20image%20representations%0Afrom%20character%20images%2C%20capturing%20semantic%20cues%20from%20localized%20strokes%20to%0Aholistic%20structures.%20The%20Text%20Multi-Granularity%20Encoder%20extracts%20stroke%20and%0Aradical%20sequence%20representations%20at%20different%20levels%20of%20granularity.%20To%20better%0Acapture%20the%20relationships%20between%20strokes%20and%20radicals%2C%20we%20introduce%0AMulti-Granularity%20Fusion%20Modules%20on%20the%20image%20and%20text%20sides%2C%20respectively.%0AFurthermore%2C%20to%20effectively%20bridge%20the%20two%20modalities%2C%20we%20further%20introduce%20a%0AFine-Grained%20Decoupled%20Image-Text%20Contrastive%20loss%2C%20which%20aligns%20image%20and%20text%0Arepresentations%20across%20multiple%20granularities.%20Extensive%20experiments%0Ademonstrate%20that%20our%20proposed%20Hi-GITA%20significantly%20outperforms%20existing%0Azero-shot%20CCR%20methods.%20For%20instance%2C%20it%20brings%20about%2020%25%20accuracy%20improvement%0Ain%20handwritten%20character%20and%20radical%20zero-shot%20settings.%20Code%20and%20models%20will%0Abe%20released%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24837v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Chinese%2520Character%2520Recognition%2520with%2520Hierarchical%250A%2520%2520Multi-Granularity%2520Image-Text%2520Aligning%26entry.906535625%3DYinglian%2520Zhu%2520and%2520Haiyang%2520Yu%2520and%2520Qizao%2520Wang%2520and%2520Wei%2520Lu%2520and%2520Xiangyang%2520Xue%2520and%2520Bin%2520Li%26entry.1292438233%3D%2520%2520Chinese%2520Character%2520Recognition%2520%2528CCR%2529%2520is%2520a%2520fundamental%2520technology%2520for%250Aintelligent%2520document%2520processing.%2520Unlike%2520Latin%2520characters%252C%2520Chinese%2520characters%250Aexhibit%2520unique%2520spatial%2520structures%2520and%2520compositional%2520rules%252C%2520allowing%2520for%2520the%2520use%250Aof%2520fine-grained%2520semantic%2520information%2520in%2520representation.%2520However%252C%2520existing%250Aapproaches%2520are%2520usually%2520based%2520on%2520auto-regressive%2520as%2520well%2520as%2520edit%2520distance%250Apost-process%2520and%2520typically%2520rely%2520on%2520a%2520single-level%2520character%2520representation.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520a%2520Hierarchical%2520Multi-Granularity%2520Image-Text%2520Aligning%250A%2528Hi-GITA%2529%2520framework%2520based%2520on%2520a%2520contrastive%2520paradigm.%2520To%2520leverage%2520the%2520abundant%250Afine-grained%2520semantic%2520information%2520of%2520Chinese%2520characters%252C%2520we%2520propose%250Amulti-granularity%2520encoders%2520on%2520both%2520image%2520and%2520text%2520sides.%2520Specifically%252C%2520the%250AImage%2520Multi-Granularity%2520Encoder%2520extracts%2520hierarchical%2520image%2520representations%250Afrom%2520character%2520images%252C%2520capturing%2520semantic%2520cues%2520from%2520localized%2520strokes%2520to%250Aholistic%2520structures.%2520The%2520Text%2520Multi-Granularity%2520Encoder%2520extracts%2520stroke%2520and%250Aradical%2520sequence%2520representations%2520at%2520different%2520levels%2520of%2520granularity.%2520To%2520better%250Acapture%2520the%2520relationships%2520between%2520strokes%2520and%2520radicals%252C%2520we%2520introduce%250AMulti-Granularity%2520Fusion%2520Modules%2520on%2520the%2520image%2520and%2520text%2520sides%252C%2520respectively.%250AFurthermore%252C%2520to%2520effectively%2520bridge%2520the%2520two%2520modalities%252C%2520we%2520further%2520introduce%2520a%250AFine-Grained%2520Decoupled%2520Image-Text%2520Contrastive%2520loss%252C%2520which%2520aligns%2520image%2520and%2520text%250Arepresentations%2520across%2520multiple%2520granularities.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520our%2520proposed%2520Hi-GITA%2520significantly%2520outperforms%2520existing%250Azero-shot%2520CCR%2520methods.%2520For%2520instance%252C%2520it%2520brings%2520about%252020%2525%2520accuracy%2520improvement%250Ain%2520handwritten%2520character%2520and%2520radical%2520zero-shot%2520settings.%2520Code%2520and%2520models%2520will%250Abe%2520released%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24837v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Chinese%20Character%20Recognition%20with%20Hierarchical%0A%20%20Multi-Granularity%20Image-Text%20Aligning&entry.906535625=Yinglian%20Zhu%20and%20Haiyang%20Yu%20and%20Qizao%20Wang%20and%20Wei%20Lu%20and%20Xiangyang%20Xue%20and%20Bin%20Li&entry.1292438233=%20%20Chinese%20Character%20Recognition%20%28CCR%29%20is%20a%20fundamental%20technology%20for%0Aintelligent%20document%20processing.%20Unlike%20Latin%20characters%2C%20Chinese%20characters%0Aexhibit%20unique%20spatial%20structures%20and%20compositional%20rules%2C%20allowing%20for%20the%20use%0Aof%20fine-grained%20semantic%20information%20in%20representation.%20However%2C%20existing%0Aapproaches%20are%20usually%20based%20on%20auto-regressive%20as%20well%20as%20edit%20distance%0Apost-process%20and%20typically%20rely%20on%20a%20single-level%20character%20representation.%20In%0Athis%20paper%2C%20we%20propose%20a%20Hierarchical%20Multi-Granularity%20Image-Text%20Aligning%0A%28Hi-GITA%29%20framework%20based%20on%20a%20contrastive%20paradigm.%20To%20leverage%20the%20abundant%0Afine-grained%20semantic%20information%20of%20Chinese%20characters%2C%20we%20propose%0Amulti-granularity%20encoders%20on%20both%20image%20and%20text%20sides.%20Specifically%2C%20the%0AImage%20Multi-Granularity%20Encoder%20extracts%20hierarchical%20image%20representations%0Afrom%20character%20images%2C%20capturing%20semantic%20cues%20from%20localized%20strokes%20to%0Aholistic%20structures.%20The%20Text%20Multi-Granularity%20Encoder%20extracts%20stroke%20and%0Aradical%20sequence%20representations%20at%20different%20levels%20of%20granularity.%20To%20better%0Acapture%20the%20relationships%20between%20strokes%20and%20radicals%2C%20we%20introduce%0AMulti-Granularity%20Fusion%20Modules%20on%20the%20image%20and%20text%20sides%2C%20respectively.%0AFurthermore%2C%20to%20effectively%20bridge%20the%20two%20modalities%2C%20we%20further%20introduce%20a%0AFine-Grained%20Decoupled%20Image-Text%20Contrastive%20loss%2C%20which%20aligns%20image%20and%20text%0Arepresentations%20across%20multiple%20granularities.%20Extensive%20experiments%0Ademonstrate%20that%20our%20proposed%20Hi-GITA%20significantly%20outperforms%20existing%0Azero-shot%20CCR%20methods.%20For%20instance%2C%20it%20brings%20about%2020%25%20accuracy%20improvement%0Ain%20handwritten%20character%20and%20radical%20zero-shot%20settings.%20Code%20and%20models%20will%0Abe%20released%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24837v1&entry.124074799=Read"},
{"title": "ViStoryBench: Comprehensive Benchmark Suite for Story Visualization", "author": "Cailin Zhuang and Ailin Huang and Wei Cheng and Jingwei Wu and Yaoqi Hu and Jiaqi Liao and Zhewei Huang and Hongyuan Wang and Xinyao Liao and Weiwei Cai and Hengyuan Xu and Xuanyang Zhang and Xianfang Zeng and Gang Yu and Chi Zhang", "abstract": "  Story visualization, which aims to generate a sequence of visually coherent\nimages aligning with a given narrative and reference images, has seen\nsignificant progress with recent advancements in generative models. To further\nenhance the performance of story visualization frameworks in real-world\nscenarios, we introduce a comprehensive evaluation benchmark, ViStoryBench. We\ncollect a diverse dataset encompassing various story types and artistic styles,\nensuring models are evaluated across multiple dimensions such as different\nplots (e.g., comedy, horror) and visual aesthetics (e.g., anime, 3D\nrenderings). ViStoryBench is carefully curated to balance narrative structures\nand visual elements, featuring stories with single and multiple protagonists to\ntest models' ability to maintain character consistency. Additionally, it\nincludes complex plots and intricate world-building to challenge models in\ngenerating accurate visuals. To ensure comprehensive comparisons, our benchmark\nincorporates a wide range of evaluation metrics assessing critical aspects.\nThis structured and multifaceted framework enables researchers to thoroughly\nidentify both the strengths and weaknesses of different models, fostering\ntargeted improvements.\n", "link": "http://arxiv.org/abs/2505.24862v1", "date": "2025-05-30", "relevancy": 2.5578, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5346}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5346}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViStoryBench%3A%20Comprehensive%20Benchmark%20Suite%20for%20Story%20Visualization&body=Title%3A%20ViStoryBench%3A%20Comprehensive%20Benchmark%20Suite%20for%20Story%20Visualization%0AAuthor%3A%20Cailin%20Zhuang%20and%20Ailin%20Huang%20and%20Wei%20Cheng%20and%20Jingwei%20Wu%20and%20Yaoqi%20Hu%20and%20Jiaqi%20Liao%20and%20Zhewei%20Huang%20and%20Hongyuan%20Wang%20and%20Xinyao%20Liao%20and%20Weiwei%20Cai%20and%20Hengyuan%20Xu%20and%20Xuanyang%20Zhang%20and%20Xianfang%20Zeng%20and%20Gang%20Yu%20and%20Chi%20Zhang%0AAbstract%3A%20%20%20Story%20visualization%2C%20which%20aims%20to%20generate%20a%20sequence%20of%20visually%20coherent%0Aimages%20aligning%20with%20a%20given%20narrative%20and%20reference%20images%2C%20has%20seen%0Asignificant%20progress%20with%20recent%20advancements%20in%20generative%20models.%20To%20further%0Aenhance%20the%20performance%20of%20story%20visualization%20frameworks%20in%20real-world%0Ascenarios%2C%20we%20introduce%20a%20comprehensive%20evaluation%20benchmark%2C%20ViStoryBench.%20We%0Acollect%20a%20diverse%20dataset%20encompassing%20various%20story%20types%20and%20artistic%20styles%2C%0Aensuring%20models%20are%20evaluated%20across%20multiple%20dimensions%20such%20as%20different%0Aplots%20%28e.g.%2C%20comedy%2C%20horror%29%20and%20visual%20aesthetics%20%28e.g.%2C%20anime%2C%203D%0Arenderings%29.%20ViStoryBench%20is%20carefully%20curated%20to%20balance%20narrative%20structures%0Aand%20visual%20elements%2C%20featuring%20stories%20with%20single%20and%20multiple%20protagonists%20to%0Atest%20models%27%20ability%20to%20maintain%20character%20consistency.%20Additionally%2C%20it%0Aincludes%20complex%20plots%20and%20intricate%20world-building%20to%20challenge%20models%20in%0Agenerating%20accurate%20visuals.%20To%20ensure%20comprehensive%20comparisons%2C%20our%20benchmark%0Aincorporates%20a%20wide%20range%20of%20evaluation%20metrics%20assessing%20critical%20aspects.%0AThis%20structured%20and%20multifaceted%20framework%20enables%20researchers%20to%20thoroughly%0Aidentify%20both%20the%20strengths%20and%20weaknesses%20of%20different%20models%2C%20fostering%0Atargeted%20improvements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24862v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViStoryBench%253A%2520Comprehensive%2520Benchmark%2520Suite%2520for%2520Story%2520Visualization%26entry.906535625%3DCailin%2520Zhuang%2520and%2520Ailin%2520Huang%2520and%2520Wei%2520Cheng%2520and%2520Jingwei%2520Wu%2520and%2520Yaoqi%2520Hu%2520and%2520Jiaqi%2520Liao%2520and%2520Zhewei%2520Huang%2520and%2520Hongyuan%2520Wang%2520and%2520Xinyao%2520Liao%2520and%2520Weiwei%2520Cai%2520and%2520Hengyuan%2520Xu%2520and%2520Xuanyang%2520Zhang%2520and%2520Xianfang%2520Zeng%2520and%2520Gang%2520Yu%2520and%2520Chi%2520Zhang%26entry.1292438233%3D%2520%2520Story%2520visualization%252C%2520which%2520aims%2520to%2520generate%2520a%2520sequence%2520of%2520visually%2520coherent%250Aimages%2520aligning%2520with%2520a%2520given%2520narrative%2520and%2520reference%2520images%252C%2520has%2520seen%250Asignificant%2520progress%2520with%2520recent%2520advancements%2520in%2520generative%2520models.%2520To%2520further%250Aenhance%2520the%2520performance%2520of%2520story%2520visualization%2520frameworks%2520in%2520real-world%250Ascenarios%252C%2520we%2520introduce%2520a%2520comprehensive%2520evaluation%2520benchmark%252C%2520ViStoryBench.%2520We%250Acollect%2520a%2520diverse%2520dataset%2520encompassing%2520various%2520story%2520types%2520and%2520artistic%2520styles%252C%250Aensuring%2520models%2520are%2520evaluated%2520across%2520multiple%2520dimensions%2520such%2520as%2520different%250Aplots%2520%2528e.g.%252C%2520comedy%252C%2520horror%2529%2520and%2520visual%2520aesthetics%2520%2528e.g.%252C%2520anime%252C%25203D%250Arenderings%2529.%2520ViStoryBench%2520is%2520carefully%2520curated%2520to%2520balance%2520narrative%2520structures%250Aand%2520visual%2520elements%252C%2520featuring%2520stories%2520with%2520single%2520and%2520multiple%2520protagonists%2520to%250Atest%2520models%2527%2520ability%2520to%2520maintain%2520character%2520consistency.%2520Additionally%252C%2520it%250Aincludes%2520complex%2520plots%2520and%2520intricate%2520world-building%2520to%2520challenge%2520models%2520in%250Agenerating%2520accurate%2520visuals.%2520To%2520ensure%2520comprehensive%2520comparisons%252C%2520our%2520benchmark%250Aincorporates%2520a%2520wide%2520range%2520of%2520evaluation%2520metrics%2520assessing%2520critical%2520aspects.%250AThis%2520structured%2520and%2520multifaceted%2520framework%2520enables%2520researchers%2520to%2520thoroughly%250Aidentify%2520both%2520the%2520strengths%2520and%2520weaknesses%2520of%2520different%2520models%252C%2520fostering%250Atargeted%2520improvements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24862v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViStoryBench%3A%20Comprehensive%20Benchmark%20Suite%20for%20Story%20Visualization&entry.906535625=Cailin%20Zhuang%20and%20Ailin%20Huang%20and%20Wei%20Cheng%20and%20Jingwei%20Wu%20and%20Yaoqi%20Hu%20and%20Jiaqi%20Liao%20and%20Zhewei%20Huang%20and%20Hongyuan%20Wang%20and%20Xinyao%20Liao%20and%20Weiwei%20Cai%20and%20Hengyuan%20Xu%20and%20Xuanyang%20Zhang%20and%20Xianfang%20Zeng%20and%20Gang%20Yu%20and%20Chi%20Zhang&entry.1292438233=%20%20Story%20visualization%2C%20which%20aims%20to%20generate%20a%20sequence%20of%20visually%20coherent%0Aimages%20aligning%20with%20a%20given%20narrative%20and%20reference%20images%2C%20has%20seen%0Asignificant%20progress%20with%20recent%20advancements%20in%20generative%20models.%20To%20further%0Aenhance%20the%20performance%20of%20story%20visualization%20frameworks%20in%20real-world%0Ascenarios%2C%20we%20introduce%20a%20comprehensive%20evaluation%20benchmark%2C%20ViStoryBench.%20We%0Acollect%20a%20diverse%20dataset%20encompassing%20various%20story%20types%20and%20artistic%20styles%2C%0Aensuring%20models%20are%20evaluated%20across%20multiple%20dimensions%20such%20as%20different%0Aplots%20%28e.g.%2C%20comedy%2C%20horror%29%20and%20visual%20aesthetics%20%28e.g.%2C%20anime%2C%203D%0Arenderings%29.%20ViStoryBench%20is%20carefully%20curated%20to%20balance%20narrative%20structures%0Aand%20visual%20elements%2C%20featuring%20stories%20with%20single%20and%20multiple%20protagonists%20to%0Atest%20models%27%20ability%20to%20maintain%20character%20consistency.%20Additionally%2C%20it%0Aincludes%20complex%20plots%20and%20intricate%20world-building%20to%20challenge%20models%20in%0Agenerating%20accurate%20visuals.%20To%20ensure%20comprehensive%20comparisons%2C%20our%20benchmark%0Aincorporates%20a%20wide%20range%20of%20evaluation%20metrics%20assessing%20critical%20aspects.%0AThis%20structured%20and%20multifaceted%20framework%20enables%20researchers%20to%20thoroughly%0Aidentify%20both%20the%20strengths%20and%20weaknesses%20of%20different%20models%2C%20fostering%0Atargeted%20improvements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24862v1&entry.124074799=Read"},
{"title": "MuSC: Improving Complex Instruction Following with Multi-granularity\n  Self-Contrastive Training", "author": "Hui Huang and Jiaheng Liu and Yancheng He and Shilong Li and Bing Xu and Conghui Zhu and Muyun Yang and Tiejun Zhao", "abstract": "  Complex instruction-following with elaborate constraints is imperative for\nLarge Language Models (LLMs). While existing methods have constructed data for\ncomplex instruction alignment, they all rely on a more advanced model,\nespecially GPT-4, limiting their application. In this paper, we propose a\nMulti-granularity Self-Contrastive Training (MuSC) framework, to improve the\ncomplex instruction alignment without relying on a stronger model. Our method\nis conducted on both coarse and fine granularity. On coarse-granularity, we\nconstruct constraint-aware preference data based on instruction decomposition\nand recombination. On fine-granularity, we perform token-aware preference\noptimization with dynamic token-level supervision. Our method is evaluated on\nopen-sourced models, and experiment results show our method achieves\nsignificant improvement on both complex and general instruction-following\nbenchmarks, surpassing previous self-alignment methods.\n", "link": "http://arxiv.org/abs/2502.11541v3", "date": "2025-05-30", "relevancy": 2.5575, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5158}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5158}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MuSC%3A%20Improving%20Complex%20Instruction%20Following%20with%20Multi-granularity%0A%20%20Self-Contrastive%20Training&body=Title%3A%20MuSC%3A%20Improving%20Complex%20Instruction%20Following%20with%20Multi-granularity%0A%20%20Self-Contrastive%20Training%0AAuthor%3A%20Hui%20Huang%20and%20Jiaheng%20Liu%20and%20Yancheng%20He%20and%20Shilong%20Li%20and%20Bing%20Xu%20and%20Conghui%20Zhu%20and%20Muyun%20Yang%20and%20Tiejun%20Zhao%0AAbstract%3A%20%20%20Complex%20instruction-following%20with%20elaborate%20constraints%20is%20imperative%20for%0ALarge%20Language%20Models%20%28LLMs%29.%20While%20existing%20methods%20have%20constructed%20data%20for%0Acomplex%20instruction%20alignment%2C%20they%20all%20rely%20on%20a%20more%20advanced%20model%2C%0Aespecially%20GPT-4%2C%20limiting%20their%20application.%20In%20this%20paper%2C%20we%20propose%20a%0AMulti-granularity%20Self-Contrastive%20Training%20%28MuSC%29%20framework%2C%20to%20improve%20the%0Acomplex%20instruction%20alignment%20without%20relying%20on%20a%20stronger%20model.%20Our%20method%0Ais%20conducted%20on%20both%20coarse%20and%20fine%20granularity.%20On%20coarse-granularity%2C%20we%0Aconstruct%20constraint-aware%20preference%20data%20based%20on%20instruction%20decomposition%0Aand%20recombination.%20On%20fine-granularity%2C%20we%20perform%20token-aware%20preference%0Aoptimization%20with%20dynamic%20token-level%20supervision.%20Our%20method%20is%20evaluated%20on%0Aopen-sourced%20models%2C%20and%20experiment%20results%20show%20our%20method%20achieves%0Asignificant%20improvement%20on%20both%20complex%20and%20general%20instruction-following%0Abenchmarks%2C%20surpassing%20previous%20self-alignment%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11541v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMuSC%253A%2520Improving%2520Complex%2520Instruction%2520Following%2520with%2520Multi-granularity%250A%2520%2520Self-Contrastive%2520Training%26entry.906535625%3DHui%2520Huang%2520and%2520Jiaheng%2520Liu%2520and%2520Yancheng%2520He%2520and%2520Shilong%2520Li%2520and%2520Bing%2520Xu%2520and%2520Conghui%2520Zhu%2520and%2520Muyun%2520Yang%2520and%2520Tiejun%2520Zhao%26entry.1292438233%3D%2520%2520Complex%2520instruction-following%2520with%2520elaborate%2520constraints%2520is%2520imperative%2520for%250ALarge%2520Language%2520Models%2520%2528LLMs%2529.%2520While%2520existing%2520methods%2520have%2520constructed%2520data%2520for%250Acomplex%2520instruction%2520alignment%252C%2520they%2520all%2520rely%2520on%2520a%2520more%2520advanced%2520model%252C%250Aespecially%2520GPT-4%252C%2520limiting%2520their%2520application.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250AMulti-granularity%2520Self-Contrastive%2520Training%2520%2528MuSC%2529%2520framework%252C%2520to%2520improve%2520the%250Acomplex%2520instruction%2520alignment%2520without%2520relying%2520on%2520a%2520stronger%2520model.%2520Our%2520method%250Ais%2520conducted%2520on%2520both%2520coarse%2520and%2520fine%2520granularity.%2520On%2520coarse-granularity%252C%2520we%250Aconstruct%2520constraint-aware%2520preference%2520data%2520based%2520on%2520instruction%2520decomposition%250Aand%2520recombination.%2520On%2520fine-granularity%252C%2520we%2520perform%2520token-aware%2520preference%250Aoptimization%2520with%2520dynamic%2520token-level%2520supervision.%2520Our%2520method%2520is%2520evaluated%2520on%250Aopen-sourced%2520models%252C%2520and%2520experiment%2520results%2520show%2520our%2520method%2520achieves%250Asignificant%2520improvement%2520on%2520both%2520complex%2520and%2520general%2520instruction-following%250Abenchmarks%252C%2520surpassing%2520previous%2520self-alignment%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11541v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MuSC%3A%20Improving%20Complex%20Instruction%20Following%20with%20Multi-granularity%0A%20%20Self-Contrastive%20Training&entry.906535625=Hui%20Huang%20and%20Jiaheng%20Liu%20and%20Yancheng%20He%20and%20Shilong%20Li%20and%20Bing%20Xu%20and%20Conghui%20Zhu%20and%20Muyun%20Yang%20and%20Tiejun%20Zhao&entry.1292438233=%20%20Complex%20instruction-following%20with%20elaborate%20constraints%20is%20imperative%20for%0ALarge%20Language%20Models%20%28LLMs%29.%20While%20existing%20methods%20have%20constructed%20data%20for%0Acomplex%20instruction%20alignment%2C%20they%20all%20rely%20on%20a%20more%20advanced%20model%2C%0Aespecially%20GPT-4%2C%20limiting%20their%20application.%20In%20this%20paper%2C%20we%20propose%20a%0AMulti-granularity%20Self-Contrastive%20Training%20%28MuSC%29%20framework%2C%20to%20improve%20the%0Acomplex%20instruction%20alignment%20without%20relying%20on%20a%20stronger%20model.%20Our%20method%0Ais%20conducted%20on%20both%20coarse%20and%20fine%20granularity.%20On%20coarse-granularity%2C%20we%0Aconstruct%20constraint-aware%20preference%20data%20based%20on%20instruction%20decomposition%0Aand%20recombination.%20On%20fine-granularity%2C%20we%20perform%20token-aware%20preference%0Aoptimization%20with%20dynamic%20token-level%20supervision.%20Our%20method%20is%20evaluated%20on%0Aopen-sourced%20models%2C%20and%20experiment%20results%20show%20our%20method%20achieves%0Asignificant%20improvement%20on%20both%20complex%20and%20general%20instruction-following%0Abenchmarks%2C%20surpassing%20previous%20self-alignment%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11541v3&entry.124074799=Read"},
{"title": "The Gaussian Mixing Mechanism: Renyi Differential Privacy via Gaussian\n  Sketches", "author": "Omri Lev and Vishwak Srinivasan and Moshe Shenfeld and Katrina Ligett and Ayush Sekhari and Ashia C. Wilson", "abstract": "  Gaussian sketching, which consists of pre-multiplying the data with a random\nGaussian matrix, is a widely used technique for multiple problems in data\nscience and machine learning, with applications spanning computationally\nefficient optimization, coded computing, and federated learning. This operation\nalso provides differential privacy guarantees due to its inherent randomness.\nIn this work, we revisit this operation through the lens of Renyi Differential\nPrivacy (RDP), providing a refined privacy analysis that yields significantly\ntighter bounds than prior results. We then demonstrate how this improved\nanalysis leads to performance improvement in different linear regression\nsettings, establishing theoretical utility guarantees. Empirically, our methods\nimprove performance across multiple datasets and, in several cases, reduce\nruntime.\n", "link": "http://arxiv.org/abs/2505.24603v1", "date": "2025-05-30", "relevancy": 2.5555, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5336}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5129}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Gaussian%20Mixing%20Mechanism%3A%20Renyi%20Differential%20Privacy%20via%20Gaussian%0A%20%20Sketches&body=Title%3A%20The%20Gaussian%20Mixing%20Mechanism%3A%20Renyi%20Differential%20Privacy%20via%20Gaussian%0A%20%20Sketches%0AAuthor%3A%20Omri%20Lev%20and%20Vishwak%20Srinivasan%20and%20Moshe%20Shenfeld%20and%20Katrina%20Ligett%20and%20Ayush%20Sekhari%20and%20Ashia%20C.%20Wilson%0AAbstract%3A%20%20%20Gaussian%20sketching%2C%20which%20consists%20of%20pre-multiplying%20the%20data%20with%20a%20random%0AGaussian%20matrix%2C%20is%20a%20widely%20used%20technique%20for%20multiple%20problems%20in%20data%0Ascience%20and%20machine%20learning%2C%20with%20applications%20spanning%20computationally%0Aefficient%20optimization%2C%20coded%20computing%2C%20and%20federated%20learning.%20This%20operation%0Aalso%20provides%20differential%20privacy%20guarantees%20due%20to%20its%20inherent%20randomness.%0AIn%20this%20work%2C%20we%20revisit%20this%20operation%20through%20the%20lens%20of%20Renyi%20Differential%0APrivacy%20%28RDP%29%2C%20providing%20a%20refined%20privacy%20analysis%20that%20yields%20significantly%0Atighter%20bounds%20than%20prior%20results.%20We%20then%20demonstrate%20how%20this%20improved%0Aanalysis%20leads%20to%20performance%20improvement%20in%20different%20linear%20regression%0Asettings%2C%20establishing%20theoretical%20utility%20guarantees.%20Empirically%2C%20our%20methods%0Aimprove%20performance%20across%20multiple%20datasets%20and%2C%20in%20several%20cases%2C%20reduce%0Aruntime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24603v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Gaussian%2520Mixing%2520Mechanism%253A%2520Renyi%2520Differential%2520Privacy%2520via%2520Gaussian%250A%2520%2520Sketches%26entry.906535625%3DOmri%2520Lev%2520and%2520Vishwak%2520Srinivasan%2520and%2520Moshe%2520Shenfeld%2520and%2520Katrina%2520Ligett%2520and%2520Ayush%2520Sekhari%2520and%2520Ashia%2520C.%2520Wilson%26entry.1292438233%3D%2520%2520Gaussian%2520sketching%252C%2520which%2520consists%2520of%2520pre-multiplying%2520the%2520data%2520with%2520a%2520random%250AGaussian%2520matrix%252C%2520is%2520a%2520widely%2520used%2520technique%2520for%2520multiple%2520problems%2520in%2520data%250Ascience%2520and%2520machine%2520learning%252C%2520with%2520applications%2520spanning%2520computationally%250Aefficient%2520optimization%252C%2520coded%2520computing%252C%2520and%2520federated%2520learning.%2520This%2520operation%250Aalso%2520provides%2520differential%2520privacy%2520guarantees%2520due%2520to%2520its%2520inherent%2520randomness.%250AIn%2520this%2520work%252C%2520we%2520revisit%2520this%2520operation%2520through%2520the%2520lens%2520of%2520Renyi%2520Differential%250APrivacy%2520%2528RDP%2529%252C%2520providing%2520a%2520refined%2520privacy%2520analysis%2520that%2520yields%2520significantly%250Atighter%2520bounds%2520than%2520prior%2520results.%2520We%2520then%2520demonstrate%2520how%2520this%2520improved%250Aanalysis%2520leads%2520to%2520performance%2520improvement%2520in%2520different%2520linear%2520regression%250Asettings%252C%2520establishing%2520theoretical%2520utility%2520guarantees.%2520Empirically%252C%2520our%2520methods%250Aimprove%2520performance%2520across%2520multiple%2520datasets%2520and%252C%2520in%2520several%2520cases%252C%2520reduce%250Aruntime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24603v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Gaussian%20Mixing%20Mechanism%3A%20Renyi%20Differential%20Privacy%20via%20Gaussian%0A%20%20Sketches&entry.906535625=Omri%20Lev%20and%20Vishwak%20Srinivasan%20and%20Moshe%20Shenfeld%20and%20Katrina%20Ligett%20and%20Ayush%20Sekhari%20and%20Ashia%20C.%20Wilson&entry.1292438233=%20%20Gaussian%20sketching%2C%20which%20consists%20of%20pre-multiplying%20the%20data%20with%20a%20random%0AGaussian%20matrix%2C%20is%20a%20widely%20used%20technique%20for%20multiple%20problems%20in%20data%0Ascience%20and%20machine%20learning%2C%20with%20applications%20spanning%20computationally%0Aefficient%20optimization%2C%20coded%20computing%2C%20and%20federated%20learning.%20This%20operation%0Aalso%20provides%20differential%20privacy%20guarantees%20due%20to%20its%20inherent%20randomness.%0AIn%20this%20work%2C%20we%20revisit%20this%20operation%20through%20the%20lens%20of%20Renyi%20Differential%0APrivacy%20%28RDP%29%2C%20providing%20a%20refined%20privacy%20analysis%20that%20yields%20significantly%0Atighter%20bounds%20than%20prior%20results.%20We%20then%20demonstrate%20how%20this%20improved%0Aanalysis%20leads%20to%20performance%20improvement%20in%20different%20linear%20regression%0Asettings%2C%20establishing%20theoretical%20utility%20guarantees.%20Empirically%2C%20our%20methods%0Aimprove%20performance%20across%20multiple%20datasets%20and%2C%20in%20several%20cases%2C%20reduce%0Aruntime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24603v1&entry.124074799=Read"},
{"title": "Graph-Based Representation Learning of Neuronal Dynamics and Behavior", "author": "Moein Khajehnejad and Forough Habibollahi and Ahmad Khajehnejad and Chris French and Brett J. Kagan and Adeel Razi", "abstract": "  Understanding how neuronal networks reorganize in response to external\nstimuli and give rise to behavior is a central challenge in neuroscience and\nartificial intelligence. However, existing methods often fail to capture the\nevolving structure of neural connectivity in ways that capture its relationship\nto behavior, especially in dynamic, uncertain, or high-dimensional settings\nwith sufficient resolution or interpretability. We introduce the Temporal\nAttention-enhanced Variational Graph Recurrent Neural Network (TAVRNN), a novel\nframework that models time-varying neuronal connectivity by integrating\nprobabilistic graph learning with temporal attention mechanisms. TAVRNN learns\nlatent dynamics at the single-unit level while maintaining interpretable\npopulation-level representations, to identify key connectivity patterns linked\nto behavior. TAVRNN generalizes across diverse neural systems and modalities,\ndemonstrating state-of-the-art classification and clustering performance. We\nvalidate TAVRNN on three diverse datasets: (1) electrophysiological data from a\nfreely behaving rat, (2) primate somatosensory cortex recordings during a\nreaching task, and (3) biological neurons in the DishBrain platform interacting\nwith a virtual game environment. Our method outperforms state-of-the-art\ndynamic embedding techniques, revealing previously unreported relationships\nbetween adaptive behavior and the evolving topological organization of neural\nnetworks. These findings demonstrate that TAVRNN offers a powerful and\ngeneralizable approach for modeling neural dynamics across experimental and\nsynthetic biological systems. Its architecture is modality-agnostic and\nscalable, making it applicable across a wide range of neural recording\nplatforms and behavioral paradigms.\n", "link": "http://arxiv.org/abs/2410.00665v2", "date": "2025-05-30", "relevancy": 2.5555, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5187}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5075}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph-Based%20Representation%20Learning%20of%20Neuronal%20Dynamics%20and%20Behavior&body=Title%3A%20Graph-Based%20Representation%20Learning%20of%20Neuronal%20Dynamics%20and%20Behavior%0AAuthor%3A%20Moein%20Khajehnejad%20and%20Forough%20Habibollahi%20and%20Ahmad%20Khajehnejad%20and%20Chris%20French%20and%20Brett%20J.%20Kagan%20and%20Adeel%20Razi%0AAbstract%3A%20%20%20Understanding%20how%20neuronal%20networks%20reorganize%20in%20response%20to%20external%0Astimuli%20and%20give%20rise%20to%20behavior%20is%20a%20central%20challenge%20in%20neuroscience%20and%0Aartificial%20intelligence.%20However%2C%20existing%20methods%20often%20fail%20to%20capture%20the%0Aevolving%20structure%20of%20neural%20connectivity%20in%20ways%20that%20capture%20its%20relationship%0Ato%20behavior%2C%20especially%20in%20dynamic%2C%20uncertain%2C%20or%20high-dimensional%20settings%0Awith%20sufficient%20resolution%20or%20interpretability.%20We%20introduce%20the%20Temporal%0AAttention-enhanced%20Variational%20Graph%20Recurrent%20Neural%20Network%20%28TAVRNN%29%2C%20a%20novel%0Aframework%20that%20models%20time-varying%20neuronal%20connectivity%20by%20integrating%0Aprobabilistic%20graph%20learning%20with%20temporal%20attention%20mechanisms.%20TAVRNN%20learns%0Alatent%20dynamics%20at%20the%20single-unit%20level%20while%20maintaining%20interpretable%0Apopulation-level%20representations%2C%20to%20identify%20key%20connectivity%20patterns%20linked%0Ato%20behavior.%20TAVRNN%20generalizes%20across%20diverse%20neural%20systems%20and%20modalities%2C%0Ademonstrating%20state-of-the-art%20classification%20and%20clustering%20performance.%20We%0Avalidate%20TAVRNN%20on%20three%20diverse%20datasets%3A%20%281%29%20electrophysiological%20data%20from%20a%0Afreely%20behaving%20rat%2C%20%282%29%20primate%20somatosensory%20cortex%20recordings%20during%20a%0Areaching%20task%2C%20and%20%283%29%20biological%20neurons%20in%20the%20DishBrain%20platform%20interacting%0Awith%20a%20virtual%20game%20environment.%20Our%20method%20outperforms%20state-of-the-art%0Adynamic%20embedding%20techniques%2C%20revealing%20previously%20unreported%20relationships%0Abetween%20adaptive%20behavior%20and%20the%20evolving%20topological%20organization%20of%20neural%0Anetworks.%20These%20findings%20demonstrate%20that%20TAVRNN%20offers%20a%20powerful%20and%0Ageneralizable%20approach%20for%20modeling%20neural%20dynamics%20across%20experimental%20and%0Asynthetic%20biological%20systems.%20Its%20architecture%20is%20modality-agnostic%20and%0Ascalable%2C%20making%20it%20applicable%20across%20a%20wide%20range%20of%20neural%20recording%0Aplatforms%20and%20behavioral%20paradigms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.00665v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph-Based%2520Representation%2520Learning%2520of%2520Neuronal%2520Dynamics%2520and%2520Behavior%26entry.906535625%3DMoein%2520Khajehnejad%2520and%2520Forough%2520Habibollahi%2520and%2520Ahmad%2520Khajehnejad%2520and%2520Chris%2520French%2520and%2520Brett%2520J.%2520Kagan%2520and%2520Adeel%2520Razi%26entry.1292438233%3D%2520%2520Understanding%2520how%2520neuronal%2520networks%2520reorganize%2520in%2520response%2520to%2520external%250Astimuli%2520and%2520give%2520rise%2520to%2520behavior%2520is%2520a%2520central%2520challenge%2520in%2520neuroscience%2520and%250Aartificial%2520intelligence.%2520However%252C%2520existing%2520methods%2520often%2520fail%2520to%2520capture%2520the%250Aevolving%2520structure%2520of%2520neural%2520connectivity%2520in%2520ways%2520that%2520capture%2520its%2520relationship%250Ato%2520behavior%252C%2520especially%2520in%2520dynamic%252C%2520uncertain%252C%2520or%2520high-dimensional%2520settings%250Awith%2520sufficient%2520resolution%2520or%2520interpretability.%2520We%2520introduce%2520the%2520Temporal%250AAttention-enhanced%2520Variational%2520Graph%2520Recurrent%2520Neural%2520Network%2520%2528TAVRNN%2529%252C%2520a%2520novel%250Aframework%2520that%2520models%2520time-varying%2520neuronal%2520connectivity%2520by%2520integrating%250Aprobabilistic%2520graph%2520learning%2520with%2520temporal%2520attention%2520mechanisms.%2520TAVRNN%2520learns%250Alatent%2520dynamics%2520at%2520the%2520single-unit%2520level%2520while%2520maintaining%2520interpretable%250Apopulation-level%2520representations%252C%2520to%2520identify%2520key%2520connectivity%2520patterns%2520linked%250Ato%2520behavior.%2520TAVRNN%2520generalizes%2520across%2520diverse%2520neural%2520systems%2520and%2520modalities%252C%250Ademonstrating%2520state-of-the-art%2520classification%2520and%2520clustering%2520performance.%2520We%250Avalidate%2520TAVRNN%2520on%2520three%2520diverse%2520datasets%253A%2520%25281%2529%2520electrophysiological%2520data%2520from%2520a%250Afreely%2520behaving%2520rat%252C%2520%25282%2529%2520primate%2520somatosensory%2520cortex%2520recordings%2520during%2520a%250Areaching%2520task%252C%2520and%2520%25283%2529%2520biological%2520neurons%2520in%2520the%2520DishBrain%2520platform%2520interacting%250Awith%2520a%2520virtual%2520game%2520environment.%2520Our%2520method%2520outperforms%2520state-of-the-art%250Adynamic%2520embedding%2520techniques%252C%2520revealing%2520previously%2520unreported%2520relationships%250Abetween%2520adaptive%2520behavior%2520and%2520the%2520evolving%2520topological%2520organization%2520of%2520neural%250Anetworks.%2520These%2520findings%2520demonstrate%2520that%2520TAVRNN%2520offers%2520a%2520powerful%2520and%250Ageneralizable%2520approach%2520for%2520modeling%2520neural%2520dynamics%2520across%2520experimental%2520and%250Asynthetic%2520biological%2520systems.%2520Its%2520architecture%2520is%2520modality-agnostic%2520and%250Ascalable%252C%2520making%2520it%2520applicable%2520across%2520a%2520wide%2520range%2520of%2520neural%2520recording%250Aplatforms%2520and%2520behavioral%2520paradigms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.00665v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph-Based%20Representation%20Learning%20of%20Neuronal%20Dynamics%20and%20Behavior&entry.906535625=Moein%20Khajehnejad%20and%20Forough%20Habibollahi%20and%20Ahmad%20Khajehnejad%20and%20Chris%20French%20and%20Brett%20J.%20Kagan%20and%20Adeel%20Razi&entry.1292438233=%20%20Understanding%20how%20neuronal%20networks%20reorganize%20in%20response%20to%20external%0Astimuli%20and%20give%20rise%20to%20behavior%20is%20a%20central%20challenge%20in%20neuroscience%20and%0Aartificial%20intelligence.%20However%2C%20existing%20methods%20often%20fail%20to%20capture%20the%0Aevolving%20structure%20of%20neural%20connectivity%20in%20ways%20that%20capture%20its%20relationship%0Ato%20behavior%2C%20especially%20in%20dynamic%2C%20uncertain%2C%20or%20high-dimensional%20settings%0Awith%20sufficient%20resolution%20or%20interpretability.%20We%20introduce%20the%20Temporal%0AAttention-enhanced%20Variational%20Graph%20Recurrent%20Neural%20Network%20%28TAVRNN%29%2C%20a%20novel%0Aframework%20that%20models%20time-varying%20neuronal%20connectivity%20by%20integrating%0Aprobabilistic%20graph%20learning%20with%20temporal%20attention%20mechanisms.%20TAVRNN%20learns%0Alatent%20dynamics%20at%20the%20single-unit%20level%20while%20maintaining%20interpretable%0Apopulation-level%20representations%2C%20to%20identify%20key%20connectivity%20patterns%20linked%0Ato%20behavior.%20TAVRNN%20generalizes%20across%20diverse%20neural%20systems%20and%20modalities%2C%0Ademonstrating%20state-of-the-art%20classification%20and%20clustering%20performance.%20We%0Avalidate%20TAVRNN%20on%20three%20diverse%20datasets%3A%20%281%29%20electrophysiological%20data%20from%20a%0Afreely%20behaving%20rat%2C%20%282%29%20primate%20somatosensory%20cortex%20recordings%20during%20a%0Areaching%20task%2C%20and%20%283%29%20biological%20neurons%20in%20the%20DishBrain%20platform%20interacting%0Awith%20a%20virtual%20game%20environment.%20Our%20method%20outperforms%20state-of-the-art%0Adynamic%20embedding%20techniques%2C%20revealing%20previously%20unreported%20relationships%0Abetween%20adaptive%20behavior%20and%20the%20evolving%20topological%20organization%20of%20neural%0Anetworks.%20These%20findings%20demonstrate%20that%20TAVRNN%20offers%20a%20powerful%20and%0Ageneralizable%20approach%20for%20modeling%20neural%20dynamics%20across%20experimental%20and%0Asynthetic%20biological%20systems.%20Its%20architecture%20is%20modality-agnostic%20and%0Ascalable%2C%20making%20it%20applicable%20across%20a%20wide%20range%20of%20neural%20recording%0Aplatforms%20and%20behavioral%20paradigms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.00665v2&entry.124074799=Read"},
{"title": "No Task Left Behind: Isotropic Model Merging with Common and\n  Task-Specific Subspaces", "author": "Daniel Marczak and Simone Magistri and Sebastian Cygert and Bart\u0142omiej Twardowski and Andrew D. Bagdanov and Joost van de Weijer", "abstract": "  Model merging integrates the weights of multiple task-specific models into a\nsingle multi-task model. Despite recent interest in the problem, a significant\nperformance gap between the combined and single-task models remains. In this\npaper, we investigate the key characteristics of task matrices -- weight update\nmatrices applied to a pre-trained model -- that enable effective merging. We\nshow that alignment between singular components of task-specific and merged\nmatrices strongly correlates with performance improvement over the pre-trained\nmodel. Based on this, we propose an isotropic merging framework that flattens\nthe singular value spectrum of task matrices, enhances alignment, and reduces\nthe performance gap. Additionally, we incorporate both common and task-specific\nsubspaces to further improve alignment and performance. Our proposed approach\nachieves state-of-the-art performance on vision and language tasks across\nvarious sets of tasks and model scales. This work advances the understanding of\nmodel merging dynamics, offering an effective methodology to merge models\nwithout requiring additional training. Code is available at\nhttps://github.com/danielm1405/iso-merging .\n", "link": "http://arxiv.org/abs/2502.04959v2", "date": "2025-05-30", "relevancy": 2.5432, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5351}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5003}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20No%20Task%20Left%20Behind%3A%20Isotropic%20Model%20Merging%20with%20Common%20and%0A%20%20Task-Specific%20Subspaces&body=Title%3A%20No%20Task%20Left%20Behind%3A%20Isotropic%20Model%20Merging%20with%20Common%20and%0A%20%20Task-Specific%20Subspaces%0AAuthor%3A%20Daniel%20Marczak%20and%20Simone%20Magistri%20and%20Sebastian%20Cygert%20and%20Bart%C5%82omiej%20Twardowski%20and%20Andrew%20D.%20Bagdanov%20and%20Joost%20van%20de%20Weijer%0AAbstract%3A%20%20%20Model%20merging%20integrates%20the%20weights%20of%20multiple%20task-specific%20models%20into%20a%0Asingle%20multi-task%20model.%20Despite%20recent%20interest%20in%20the%20problem%2C%20a%20significant%0Aperformance%20gap%20between%20the%20combined%20and%20single-task%20models%20remains.%20In%20this%0Apaper%2C%20we%20investigate%20the%20key%20characteristics%20of%20task%20matrices%20--%20weight%20update%0Amatrices%20applied%20to%20a%20pre-trained%20model%20--%20that%20enable%20effective%20merging.%20We%0Ashow%20that%20alignment%20between%20singular%20components%20of%20task-specific%20and%20merged%0Amatrices%20strongly%20correlates%20with%20performance%20improvement%20over%20the%20pre-trained%0Amodel.%20Based%20on%20this%2C%20we%20propose%20an%20isotropic%20merging%20framework%20that%20flattens%0Athe%20singular%20value%20spectrum%20of%20task%20matrices%2C%20enhances%20alignment%2C%20and%20reduces%0Athe%20performance%20gap.%20Additionally%2C%20we%20incorporate%20both%20common%20and%20task-specific%0Asubspaces%20to%20further%20improve%20alignment%20and%20performance.%20Our%20proposed%20approach%0Aachieves%20state-of-the-art%20performance%20on%20vision%20and%20language%20tasks%20across%0Avarious%20sets%20of%20tasks%20and%20model%20scales.%20This%20work%20advances%20the%20understanding%20of%0Amodel%20merging%20dynamics%2C%20offering%20an%20effective%20methodology%20to%20merge%20models%0Awithout%20requiring%20additional%20training.%20Code%20is%20available%20at%0Ahttps%3A//github.com/danielm1405/iso-merging%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04959v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNo%2520Task%2520Left%2520Behind%253A%2520Isotropic%2520Model%2520Merging%2520with%2520Common%2520and%250A%2520%2520Task-Specific%2520Subspaces%26entry.906535625%3DDaniel%2520Marczak%2520and%2520Simone%2520Magistri%2520and%2520Sebastian%2520Cygert%2520and%2520Bart%25C5%2582omiej%2520Twardowski%2520and%2520Andrew%2520D.%2520Bagdanov%2520and%2520Joost%2520van%2520de%2520Weijer%26entry.1292438233%3D%2520%2520Model%2520merging%2520integrates%2520the%2520weights%2520of%2520multiple%2520task-specific%2520models%2520into%2520a%250Asingle%2520multi-task%2520model.%2520Despite%2520recent%2520interest%2520in%2520the%2520problem%252C%2520a%2520significant%250Aperformance%2520gap%2520between%2520the%2520combined%2520and%2520single-task%2520models%2520remains.%2520In%2520this%250Apaper%252C%2520we%2520investigate%2520the%2520key%2520characteristics%2520of%2520task%2520matrices%2520--%2520weight%2520update%250Amatrices%2520applied%2520to%2520a%2520pre-trained%2520model%2520--%2520that%2520enable%2520effective%2520merging.%2520We%250Ashow%2520that%2520alignment%2520between%2520singular%2520components%2520of%2520task-specific%2520and%2520merged%250Amatrices%2520strongly%2520correlates%2520with%2520performance%2520improvement%2520over%2520the%2520pre-trained%250Amodel.%2520Based%2520on%2520this%252C%2520we%2520propose%2520an%2520isotropic%2520merging%2520framework%2520that%2520flattens%250Athe%2520singular%2520value%2520spectrum%2520of%2520task%2520matrices%252C%2520enhances%2520alignment%252C%2520and%2520reduces%250Athe%2520performance%2520gap.%2520Additionally%252C%2520we%2520incorporate%2520both%2520common%2520and%2520task-specific%250Asubspaces%2520to%2520further%2520improve%2520alignment%2520and%2520performance.%2520Our%2520proposed%2520approach%250Aachieves%2520state-of-the-art%2520performance%2520on%2520vision%2520and%2520language%2520tasks%2520across%250Avarious%2520sets%2520of%2520tasks%2520and%2520model%2520scales.%2520This%2520work%2520advances%2520the%2520understanding%2520of%250Amodel%2520merging%2520dynamics%252C%2520offering%2520an%2520effective%2520methodology%2520to%2520merge%2520models%250Awithout%2520requiring%2520additional%2520training.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/danielm1405/iso-merging%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04959v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=No%20Task%20Left%20Behind%3A%20Isotropic%20Model%20Merging%20with%20Common%20and%0A%20%20Task-Specific%20Subspaces&entry.906535625=Daniel%20Marczak%20and%20Simone%20Magistri%20and%20Sebastian%20Cygert%20and%20Bart%C5%82omiej%20Twardowski%20and%20Andrew%20D.%20Bagdanov%20and%20Joost%20van%20de%20Weijer&entry.1292438233=%20%20Model%20merging%20integrates%20the%20weights%20of%20multiple%20task-specific%20models%20into%20a%0Asingle%20multi-task%20model.%20Despite%20recent%20interest%20in%20the%20problem%2C%20a%20significant%0Aperformance%20gap%20between%20the%20combined%20and%20single-task%20models%20remains.%20In%20this%0Apaper%2C%20we%20investigate%20the%20key%20characteristics%20of%20task%20matrices%20--%20weight%20update%0Amatrices%20applied%20to%20a%20pre-trained%20model%20--%20that%20enable%20effective%20merging.%20We%0Ashow%20that%20alignment%20between%20singular%20components%20of%20task-specific%20and%20merged%0Amatrices%20strongly%20correlates%20with%20performance%20improvement%20over%20the%20pre-trained%0Amodel.%20Based%20on%20this%2C%20we%20propose%20an%20isotropic%20merging%20framework%20that%20flattens%0Athe%20singular%20value%20spectrum%20of%20task%20matrices%2C%20enhances%20alignment%2C%20and%20reduces%0Athe%20performance%20gap.%20Additionally%2C%20we%20incorporate%20both%20common%20and%20task-specific%0Asubspaces%20to%20further%20improve%20alignment%20and%20performance.%20Our%20proposed%20approach%0Aachieves%20state-of-the-art%20performance%20on%20vision%20and%20language%20tasks%20across%0Avarious%20sets%20of%20tasks%20and%20model%20scales.%20This%20work%20advances%20the%20understanding%20of%0Amodel%20merging%20dynamics%2C%20offering%20an%20effective%20methodology%20to%20merge%20models%0Awithout%20requiring%20additional%20training.%20Code%20is%20available%20at%0Ahttps%3A//github.com/danielm1405/iso-merging%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04959v2&entry.124074799=Read"},
{"title": "SparQLe: Speech Queries to Text Translation Through LLMs", "author": "Amirbek Djanibekov and Hanan Aldarmaki", "abstract": "  With the growing influence of Large Language Models (LLMs), there is\nincreasing interest in integrating speech representations with them to enable\nmore seamless multi-modal processing and speech understanding. This study\nintroduces a novel approach that combines self-supervised speech\nrepresentations with instruction-tuned LLMs for speech-to-text translation. The\nproposed approach leverages a modality adapter to align extracted speech\nfeatures with instruction-tuned LLMs using English speech data. Our experiments\ndemonstrate that this method effectively preserves the semantic content of the\ninput speech and serves as an effective bridge between self-supervised speech\nmodels and instruction-tuned LLMs, offering a promising approach for various\nspeech understanding applications.\n", "link": "http://arxiv.org/abs/2502.09284v3", "date": "2025-05-30", "relevancy": 2.538, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5254}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4987}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SparQLe%3A%20Speech%20Queries%20to%20Text%20Translation%20Through%20LLMs&body=Title%3A%20SparQLe%3A%20Speech%20Queries%20to%20Text%20Translation%20Through%20LLMs%0AAuthor%3A%20Amirbek%20Djanibekov%20and%20Hanan%20Aldarmaki%0AAbstract%3A%20%20%20With%20the%20growing%20influence%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20there%20is%0Aincreasing%20interest%20in%20integrating%20speech%20representations%20with%20them%20to%20enable%0Amore%20seamless%20multi-modal%20processing%20and%20speech%20understanding.%20This%20study%0Aintroduces%20a%20novel%20approach%20that%20combines%20self-supervised%20speech%0Arepresentations%20with%20instruction-tuned%20LLMs%20for%20speech-to-text%20translation.%20The%0Aproposed%20approach%20leverages%20a%20modality%20adapter%20to%20align%20extracted%20speech%0Afeatures%20with%20instruction-tuned%20LLMs%20using%20English%20speech%20data.%20Our%20experiments%0Ademonstrate%20that%20this%20method%20effectively%20preserves%20the%20semantic%20content%20of%20the%0Ainput%20speech%20and%20serves%20as%20an%20effective%20bridge%20between%20self-supervised%20speech%0Amodels%20and%20instruction-tuned%20LLMs%2C%20offering%20a%20promising%20approach%20for%20various%0Aspeech%20understanding%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09284v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparQLe%253A%2520Speech%2520Queries%2520to%2520Text%2520Translation%2520Through%2520LLMs%26entry.906535625%3DAmirbek%2520Djanibekov%2520and%2520Hanan%2520Aldarmaki%26entry.1292438233%3D%2520%2520With%2520the%2520growing%2520influence%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520there%2520is%250Aincreasing%2520interest%2520in%2520integrating%2520speech%2520representations%2520with%2520them%2520to%2520enable%250Amore%2520seamless%2520multi-modal%2520processing%2520and%2520speech%2520understanding.%2520This%2520study%250Aintroduces%2520a%2520novel%2520approach%2520that%2520combines%2520self-supervised%2520speech%250Arepresentations%2520with%2520instruction-tuned%2520LLMs%2520for%2520speech-to-text%2520translation.%2520The%250Aproposed%2520approach%2520leverages%2520a%2520modality%2520adapter%2520to%2520align%2520extracted%2520speech%250Afeatures%2520with%2520instruction-tuned%2520LLMs%2520using%2520English%2520speech%2520data.%2520Our%2520experiments%250Ademonstrate%2520that%2520this%2520method%2520effectively%2520preserves%2520the%2520semantic%2520content%2520of%2520the%250Ainput%2520speech%2520and%2520serves%2520as%2520an%2520effective%2520bridge%2520between%2520self-supervised%2520speech%250Amodels%2520and%2520instruction-tuned%2520LLMs%252C%2520offering%2520a%2520promising%2520approach%2520for%2520various%250Aspeech%2520understanding%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09284v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SparQLe%3A%20Speech%20Queries%20to%20Text%20Translation%20Through%20LLMs&entry.906535625=Amirbek%20Djanibekov%20and%20Hanan%20Aldarmaki&entry.1292438233=%20%20With%20the%20growing%20influence%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20there%20is%0Aincreasing%20interest%20in%20integrating%20speech%20representations%20with%20them%20to%20enable%0Amore%20seamless%20multi-modal%20processing%20and%20speech%20understanding.%20This%20study%0Aintroduces%20a%20novel%20approach%20that%20combines%20self-supervised%20speech%0Arepresentations%20with%20instruction-tuned%20LLMs%20for%20speech-to-text%20translation.%20The%0Aproposed%20approach%20leverages%20a%20modality%20adapter%20to%20align%20extracted%20speech%0Afeatures%20with%20instruction-tuned%20LLMs%20using%20English%20speech%20data.%20Our%20experiments%0Ademonstrate%20that%20this%20method%20effectively%20preserves%20the%20semantic%20content%20of%20the%0Ainput%20speech%20and%20serves%20as%20an%20effective%20bridge%20between%20self-supervised%20speech%0Amodels%20and%20instruction-tuned%20LLMs%2C%20offering%20a%20promising%20approach%20for%20various%0Aspeech%20understanding%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09284v3&entry.124074799=Read"},
{"title": "Using Knowledge Graphs to harvest datasets for efficient CLIP model\n  training", "author": "Simon Ging and Sebastian Walter and Jelena Bratuli\u0107 and Johannes Dienert and Hannah Bast and Thomas Brox", "abstract": "  Training high-quality CLIP models typically requires enormous datasets, which\nlimits the development of domain-specific models -- especially in areas that\neven the largest CLIP models do not cover well -- and drives up training costs.\nThis poses challenges for scientific research that needs fine-grained control\nover the training procedure of CLIP models. In this work, we show that by\nemploying smart web search strategies enhanced with knowledge graphs, a robust\nCLIP model can be trained from scratch with considerably less data.\nSpecifically, we demonstrate that an expert foundation model for living\norganisms can be built using just 10M images. Moreover, we introduce EntityNet,\na dataset comprising 33M images paired with 46M text descriptions, which\nenables the training of a generic CLIP model in significantly reduced time.\n", "link": "http://arxiv.org/abs/2505.02746v2", "date": "2025-05-30", "relevancy": 2.5205, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5043}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20Knowledge%20Graphs%20to%20harvest%20datasets%20for%20efficient%20CLIP%20model%0A%20%20training&body=Title%3A%20Using%20Knowledge%20Graphs%20to%20harvest%20datasets%20for%20efficient%20CLIP%20model%0A%20%20training%0AAuthor%3A%20Simon%20Ging%20and%20Sebastian%20Walter%20and%20Jelena%20Bratuli%C4%87%20and%20Johannes%20Dienert%20and%20Hannah%20Bast%20and%20Thomas%20Brox%0AAbstract%3A%20%20%20Training%20high-quality%20CLIP%20models%20typically%20requires%20enormous%20datasets%2C%20which%0Alimits%20the%20development%20of%20domain-specific%20models%20--%20especially%20in%20areas%20that%0Aeven%20the%20largest%20CLIP%20models%20do%20not%20cover%20well%20--%20and%20drives%20up%20training%20costs.%0AThis%20poses%20challenges%20for%20scientific%20research%20that%20needs%20fine-grained%20control%0Aover%20the%20training%20procedure%20of%20CLIP%20models.%20In%20this%20work%2C%20we%20show%20that%20by%0Aemploying%20smart%20web%20search%20strategies%20enhanced%20with%20knowledge%20graphs%2C%20a%20robust%0ACLIP%20model%20can%20be%20trained%20from%20scratch%20with%20considerably%20less%20data.%0ASpecifically%2C%20we%20demonstrate%20that%20an%20expert%20foundation%20model%20for%20living%0Aorganisms%20can%20be%20built%20using%20just%2010M%20images.%20Moreover%2C%20we%20introduce%20EntityNet%2C%0Aa%20dataset%20comprising%2033M%20images%20paired%20with%2046M%20text%20descriptions%2C%20which%0Aenables%20the%20training%20of%20a%20generic%20CLIP%20model%20in%20significantly%20reduced%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02746v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520Knowledge%2520Graphs%2520to%2520harvest%2520datasets%2520for%2520efficient%2520CLIP%2520model%250A%2520%2520training%26entry.906535625%3DSimon%2520Ging%2520and%2520Sebastian%2520Walter%2520and%2520Jelena%2520Bratuli%25C4%2587%2520and%2520Johannes%2520Dienert%2520and%2520Hannah%2520Bast%2520and%2520Thomas%2520Brox%26entry.1292438233%3D%2520%2520Training%2520high-quality%2520CLIP%2520models%2520typically%2520requires%2520enormous%2520datasets%252C%2520which%250Alimits%2520the%2520development%2520of%2520domain-specific%2520models%2520--%2520especially%2520in%2520areas%2520that%250Aeven%2520the%2520largest%2520CLIP%2520models%2520do%2520not%2520cover%2520well%2520--%2520and%2520drives%2520up%2520training%2520costs.%250AThis%2520poses%2520challenges%2520for%2520scientific%2520research%2520that%2520needs%2520fine-grained%2520control%250Aover%2520the%2520training%2520procedure%2520of%2520CLIP%2520models.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520by%250Aemploying%2520smart%2520web%2520search%2520strategies%2520enhanced%2520with%2520knowledge%2520graphs%252C%2520a%2520robust%250ACLIP%2520model%2520can%2520be%2520trained%2520from%2520scratch%2520with%2520considerably%2520less%2520data.%250ASpecifically%252C%2520we%2520demonstrate%2520that%2520an%2520expert%2520foundation%2520model%2520for%2520living%250Aorganisms%2520can%2520be%2520built%2520using%2520just%252010M%2520images.%2520Moreover%252C%2520we%2520introduce%2520EntityNet%252C%250Aa%2520dataset%2520comprising%252033M%2520images%2520paired%2520with%252046M%2520text%2520descriptions%252C%2520which%250Aenables%2520the%2520training%2520of%2520a%2520generic%2520CLIP%2520model%2520in%2520significantly%2520reduced%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02746v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20Knowledge%20Graphs%20to%20harvest%20datasets%20for%20efficient%20CLIP%20model%0A%20%20training&entry.906535625=Simon%20Ging%20and%20Sebastian%20Walter%20and%20Jelena%20Bratuli%C4%87%20and%20Johannes%20Dienert%20and%20Hannah%20Bast%20and%20Thomas%20Brox&entry.1292438233=%20%20Training%20high-quality%20CLIP%20models%20typically%20requires%20enormous%20datasets%2C%20which%0Alimits%20the%20development%20of%20domain-specific%20models%20--%20especially%20in%20areas%20that%0Aeven%20the%20largest%20CLIP%20models%20do%20not%20cover%20well%20--%20and%20drives%20up%20training%20costs.%0AThis%20poses%20challenges%20for%20scientific%20research%20that%20needs%20fine-grained%20control%0Aover%20the%20training%20procedure%20of%20CLIP%20models.%20In%20this%20work%2C%20we%20show%20that%20by%0Aemploying%20smart%20web%20search%20strategies%20enhanced%20with%20knowledge%20graphs%2C%20a%20robust%0ACLIP%20model%20can%20be%20trained%20from%20scratch%20with%20considerably%20less%20data.%0ASpecifically%2C%20we%20demonstrate%20that%20an%20expert%20foundation%20model%20for%20living%0Aorganisms%20can%20be%20built%20using%20just%2010M%20images.%20Moreover%2C%20we%20introduce%20EntityNet%2C%0Aa%20dataset%20comprising%2033M%20images%20paired%20with%2046M%20text%20descriptions%2C%20which%0Aenables%20the%20training%20of%20a%20generic%20CLIP%20model%20in%20significantly%20reduced%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02746v2&entry.124074799=Read"},
{"title": "Generative Knowledge Production Pipeline Driven by Academic Influencers", "author": "Katalin Feher and Marton Demeter", "abstract": "  Generative AI transforms knowledge production, validation, and dissemination,\nraising academic integrity and credibility concerns. This study examines 53\nacademic influencer videos that reached 5.3 million viewers to identify an\nemerging, structured, implementation-ready pipeline balancing originality,\nethical compliance, and human-AI collaboration despite the disruptive impacts.\nFindings highlight generative AI's potential to automate publication workflows\nand democratize participation in knowledge production while challenging\ntraditional scientific norms. Academic influencers emerge as key intermediaries\nin this paradigm shift, connecting bottom-up practices with institutional\npolicies to improve adaptability. Accordingly, the study proposes a generative\npublication production pipeline and a policy framework for co-intelligence\nadaptation and reinforcing credibility-centered standards in AI-powered\nresearch. These insights support scholars, educators, and policymakers in\nunderstanding AI's transformative impact by advocating responsible and\ninnovation-driven knowledge production. Additionally, they reveal pathways for\nautomating best practices, optimizing scholarly workflows, and fostering\ncreativity in academic research and publication.\n", "link": "http://arxiv.org/abs/2505.24681v1", "date": "2025-05-30", "relevancy": 2.5176, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5316}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4907}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4884}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Knowledge%20Production%20Pipeline%20Driven%20by%20Academic%20Influencers&body=Title%3A%20Generative%20Knowledge%20Production%20Pipeline%20Driven%20by%20Academic%20Influencers%0AAuthor%3A%20Katalin%20Feher%20and%20Marton%20Demeter%0AAbstract%3A%20%20%20Generative%20AI%20transforms%20knowledge%20production%2C%20validation%2C%20and%20dissemination%2C%0Araising%20academic%20integrity%20and%20credibility%20concerns.%20This%20study%20examines%2053%0Aacademic%20influencer%20videos%20that%20reached%205.3%20million%20viewers%20to%20identify%20an%0Aemerging%2C%20structured%2C%20implementation-ready%20pipeline%20balancing%20originality%2C%0Aethical%20compliance%2C%20and%20human-AI%20collaboration%20despite%20the%20disruptive%20impacts.%0AFindings%20highlight%20generative%20AI%27s%20potential%20to%20automate%20publication%20workflows%0Aand%20democratize%20participation%20in%20knowledge%20production%20while%20challenging%0Atraditional%20scientific%20norms.%20Academic%20influencers%20emerge%20as%20key%20intermediaries%0Ain%20this%20paradigm%20shift%2C%20connecting%20bottom-up%20practices%20with%20institutional%0Apolicies%20to%20improve%20adaptability.%20Accordingly%2C%20the%20study%20proposes%20a%20generative%0Apublication%20production%20pipeline%20and%20a%20policy%20framework%20for%20co-intelligence%0Aadaptation%20and%20reinforcing%20credibility-centered%20standards%20in%20AI-powered%0Aresearch.%20These%20insights%20support%20scholars%2C%20educators%2C%20and%20policymakers%20in%0Aunderstanding%20AI%27s%20transformative%20impact%20by%20advocating%20responsible%20and%0Ainnovation-driven%20knowledge%20production.%20Additionally%2C%20they%20reveal%20pathways%20for%0Aautomating%20best%20practices%2C%20optimizing%20scholarly%20workflows%2C%20and%20fostering%0Acreativity%20in%20academic%20research%20and%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24681v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Knowledge%2520Production%2520Pipeline%2520Driven%2520by%2520Academic%2520Influencers%26entry.906535625%3DKatalin%2520Feher%2520and%2520Marton%2520Demeter%26entry.1292438233%3D%2520%2520Generative%2520AI%2520transforms%2520knowledge%2520production%252C%2520validation%252C%2520and%2520dissemination%252C%250Araising%2520academic%2520integrity%2520and%2520credibility%2520concerns.%2520This%2520study%2520examines%252053%250Aacademic%2520influencer%2520videos%2520that%2520reached%25205.3%2520million%2520viewers%2520to%2520identify%2520an%250Aemerging%252C%2520structured%252C%2520implementation-ready%2520pipeline%2520balancing%2520originality%252C%250Aethical%2520compliance%252C%2520and%2520human-AI%2520collaboration%2520despite%2520the%2520disruptive%2520impacts.%250AFindings%2520highlight%2520generative%2520AI%2527s%2520potential%2520to%2520automate%2520publication%2520workflows%250Aand%2520democratize%2520participation%2520in%2520knowledge%2520production%2520while%2520challenging%250Atraditional%2520scientific%2520norms.%2520Academic%2520influencers%2520emerge%2520as%2520key%2520intermediaries%250Ain%2520this%2520paradigm%2520shift%252C%2520connecting%2520bottom-up%2520practices%2520with%2520institutional%250Apolicies%2520to%2520improve%2520adaptability.%2520Accordingly%252C%2520the%2520study%2520proposes%2520a%2520generative%250Apublication%2520production%2520pipeline%2520and%2520a%2520policy%2520framework%2520for%2520co-intelligence%250Aadaptation%2520and%2520reinforcing%2520credibility-centered%2520standards%2520in%2520AI-powered%250Aresearch.%2520These%2520insights%2520support%2520scholars%252C%2520educators%252C%2520and%2520policymakers%2520in%250Aunderstanding%2520AI%2527s%2520transformative%2520impact%2520by%2520advocating%2520responsible%2520and%250Ainnovation-driven%2520knowledge%2520production.%2520Additionally%252C%2520they%2520reveal%2520pathways%2520for%250Aautomating%2520best%2520practices%252C%2520optimizing%2520scholarly%2520workflows%252C%2520and%2520fostering%250Acreativity%2520in%2520academic%2520research%2520and%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24681v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Knowledge%20Production%20Pipeline%20Driven%20by%20Academic%20Influencers&entry.906535625=Katalin%20Feher%20and%20Marton%20Demeter&entry.1292438233=%20%20Generative%20AI%20transforms%20knowledge%20production%2C%20validation%2C%20and%20dissemination%2C%0Araising%20academic%20integrity%20and%20credibility%20concerns.%20This%20study%20examines%2053%0Aacademic%20influencer%20videos%20that%20reached%205.3%20million%20viewers%20to%20identify%20an%0Aemerging%2C%20structured%2C%20implementation-ready%20pipeline%20balancing%20originality%2C%0Aethical%20compliance%2C%20and%20human-AI%20collaboration%20despite%20the%20disruptive%20impacts.%0AFindings%20highlight%20generative%20AI%27s%20potential%20to%20automate%20publication%20workflows%0Aand%20democratize%20participation%20in%20knowledge%20production%20while%20challenging%0Atraditional%20scientific%20norms.%20Academic%20influencers%20emerge%20as%20key%20intermediaries%0Ain%20this%20paradigm%20shift%2C%20connecting%20bottom-up%20practices%20with%20institutional%0Apolicies%20to%20improve%20adaptability.%20Accordingly%2C%20the%20study%20proposes%20a%20generative%0Apublication%20production%20pipeline%20and%20a%20policy%20framework%20for%20co-intelligence%0Aadaptation%20and%20reinforcing%20credibility-centered%20standards%20in%20AI-powered%0Aresearch.%20These%20insights%20support%20scholars%2C%20educators%2C%20and%20policymakers%20in%0Aunderstanding%20AI%27s%20transformative%20impact%20by%20advocating%20responsible%20and%0Ainnovation-driven%20knowledge%20production.%20Additionally%2C%20they%20reveal%20pathways%20for%0Aautomating%20best%20practices%2C%20optimizing%20scholarly%20workflows%2C%20and%20fostering%0Acreativity%20in%20academic%20research%20and%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24681v1&entry.124074799=Read"},
{"title": "QGAN-based data augmentation for hybrid quantum-classical neural\n  networks", "author": "Run-Ze He and Jun-Jian Su and Su-Juan Qin and Zheng-Ping Jin and Fei Gao", "abstract": "  Quantum neural networks converge faster and achieve higher accuracy than\nclassical models. However, data augmentation in quantum machine learning\nremains underexplored. To tackle data scarcity, we integrate quantum generative\nadversarial networks (QGANs) with hybrid quantum-classical neural networks\n(HQCNNs) to develop an augmentation framework. We propose two strategies: a\ngeneral approach to enhance data processing and classification across HQCNNs,\nand a customized strategy that dynamically generates samples tailored to the\nHQCNN's performance on specific data categories, improving its ability to learn\nfrom complex datasets. Simulation experiments on the MNIST dataset demonstrate\nthat QGAN outperforms traditional data augmentation methods and classical GANs.\nCompared to baseline DCGAN, QGAN achieves comparable performance with half the\nparameters, balancing efficiency and effectiveness. This suggests that QGANs\ncan simplify models and generate high-quality data, enhancing HQCNN accuracy\nand performance. These findings pave the way for applying quantum data\naugmentation techniques in machine learning.\n", "link": "http://arxiv.org/abs/2505.24780v1", "date": "2025-05-30", "relevancy": 2.5103, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5093}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5032}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QGAN-based%20data%20augmentation%20for%20hybrid%20quantum-classical%20neural%0A%20%20networks&body=Title%3A%20QGAN-based%20data%20augmentation%20for%20hybrid%20quantum-classical%20neural%0A%20%20networks%0AAuthor%3A%20Run-Ze%20He%20and%20Jun-Jian%20Su%20and%20Su-Juan%20Qin%20and%20Zheng-Ping%20Jin%20and%20Fei%20Gao%0AAbstract%3A%20%20%20Quantum%20neural%20networks%20converge%20faster%20and%20achieve%20higher%20accuracy%20than%0Aclassical%20models.%20However%2C%20data%20augmentation%20in%20quantum%20machine%20learning%0Aremains%20underexplored.%20To%20tackle%20data%20scarcity%2C%20we%20integrate%20quantum%20generative%0Aadversarial%20networks%20%28QGANs%29%20with%20hybrid%20quantum-classical%20neural%20networks%0A%28HQCNNs%29%20to%20develop%20an%20augmentation%20framework.%20We%20propose%20two%20strategies%3A%20a%0Ageneral%20approach%20to%20enhance%20data%20processing%20and%20classification%20across%20HQCNNs%2C%0Aand%20a%20customized%20strategy%20that%20dynamically%20generates%20samples%20tailored%20to%20the%0AHQCNN%27s%20performance%20on%20specific%20data%20categories%2C%20improving%20its%20ability%20to%20learn%0Afrom%20complex%20datasets.%20Simulation%20experiments%20on%20the%20MNIST%20dataset%20demonstrate%0Athat%20QGAN%20outperforms%20traditional%20data%20augmentation%20methods%20and%20classical%20GANs.%0ACompared%20to%20baseline%20DCGAN%2C%20QGAN%20achieves%20comparable%20performance%20with%20half%20the%0Aparameters%2C%20balancing%20efficiency%20and%20effectiveness.%20This%20suggests%20that%20QGANs%0Acan%20simplify%20models%20and%20generate%20high-quality%20data%2C%20enhancing%20HQCNN%20accuracy%0Aand%20performance.%20These%20findings%20pave%20the%20way%20for%20applying%20quantum%20data%0Aaugmentation%20techniques%20in%20machine%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24780v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQGAN-based%2520data%2520augmentation%2520for%2520hybrid%2520quantum-classical%2520neural%250A%2520%2520networks%26entry.906535625%3DRun-Ze%2520He%2520and%2520Jun-Jian%2520Su%2520and%2520Su-Juan%2520Qin%2520and%2520Zheng-Ping%2520Jin%2520and%2520Fei%2520Gao%26entry.1292438233%3D%2520%2520Quantum%2520neural%2520networks%2520converge%2520faster%2520and%2520achieve%2520higher%2520accuracy%2520than%250Aclassical%2520models.%2520However%252C%2520data%2520augmentation%2520in%2520quantum%2520machine%2520learning%250Aremains%2520underexplored.%2520To%2520tackle%2520data%2520scarcity%252C%2520we%2520integrate%2520quantum%2520generative%250Aadversarial%2520networks%2520%2528QGANs%2529%2520with%2520hybrid%2520quantum-classical%2520neural%2520networks%250A%2528HQCNNs%2529%2520to%2520develop%2520an%2520augmentation%2520framework.%2520We%2520propose%2520two%2520strategies%253A%2520a%250Ageneral%2520approach%2520to%2520enhance%2520data%2520processing%2520and%2520classification%2520across%2520HQCNNs%252C%250Aand%2520a%2520customized%2520strategy%2520that%2520dynamically%2520generates%2520samples%2520tailored%2520to%2520the%250AHQCNN%2527s%2520performance%2520on%2520specific%2520data%2520categories%252C%2520improving%2520its%2520ability%2520to%2520learn%250Afrom%2520complex%2520datasets.%2520Simulation%2520experiments%2520on%2520the%2520MNIST%2520dataset%2520demonstrate%250Athat%2520QGAN%2520outperforms%2520traditional%2520data%2520augmentation%2520methods%2520and%2520classical%2520GANs.%250ACompared%2520to%2520baseline%2520DCGAN%252C%2520QGAN%2520achieves%2520comparable%2520performance%2520with%2520half%2520the%250Aparameters%252C%2520balancing%2520efficiency%2520and%2520effectiveness.%2520This%2520suggests%2520that%2520QGANs%250Acan%2520simplify%2520models%2520and%2520generate%2520high-quality%2520data%252C%2520enhancing%2520HQCNN%2520accuracy%250Aand%2520performance.%2520These%2520findings%2520pave%2520the%2520way%2520for%2520applying%2520quantum%2520data%250Aaugmentation%2520techniques%2520in%2520machine%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24780v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QGAN-based%20data%20augmentation%20for%20hybrid%20quantum-classical%20neural%0A%20%20networks&entry.906535625=Run-Ze%20He%20and%20Jun-Jian%20Su%20and%20Su-Juan%20Qin%20and%20Zheng-Ping%20Jin%20and%20Fei%20Gao&entry.1292438233=%20%20Quantum%20neural%20networks%20converge%20faster%20and%20achieve%20higher%20accuracy%20than%0Aclassical%20models.%20However%2C%20data%20augmentation%20in%20quantum%20machine%20learning%0Aremains%20underexplored.%20To%20tackle%20data%20scarcity%2C%20we%20integrate%20quantum%20generative%0Aadversarial%20networks%20%28QGANs%29%20with%20hybrid%20quantum-classical%20neural%20networks%0A%28HQCNNs%29%20to%20develop%20an%20augmentation%20framework.%20We%20propose%20two%20strategies%3A%20a%0Ageneral%20approach%20to%20enhance%20data%20processing%20and%20classification%20across%20HQCNNs%2C%0Aand%20a%20customized%20strategy%20that%20dynamically%20generates%20samples%20tailored%20to%20the%0AHQCNN%27s%20performance%20on%20specific%20data%20categories%2C%20improving%20its%20ability%20to%20learn%0Afrom%20complex%20datasets.%20Simulation%20experiments%20on%20the%20MNIST%20dataset%20demonstrate%0Athat%20QGAN%20outperforms%20traditional%20data%20augmentation%20methods%20and%20classical%20GANs.%0ACompared%20to%20baseline%20DCGAN%2C%20QGAN%20achieves%20comparable%20performance%20with%20half%20the%0Aparameters%2C%20balancing%20efficiency%20and%20effectiveness.%20This%20suggests%20that%20QGANs%0Acan%20simplify%20models%20and%20generate%20high-quality%20data%2C%20enhancing%20HQCNN%20accuracy%0Aand%20performance.%20These%20findings%20pave%20the%20way%20for%20applying%20quantum%20data%0Aaugmentation%20techniques%20in%20machine%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24780v1&entry.124074799=Read"},
{"title": "In Dialogue with Intelligence: Rethinking Large Language Models as\n  Collective Knowledge", "author": "Eleni Vasilaki", "abstract": "  Large Language Models (LLMs) are typically analysed through architectural,\nbehavioural, or training-data lenses. This article offers a theoretical and\nexperiential re-framing: LLMs as dynamic instantiations of Collective human\nKnowledge (CK), where intelligence is evoked through dialogue rather than\nstored statically. Drawing on concepts from neuroscience and AI, and grounded\nin sustained interaction with ChatGPT-4, I examine emergent dialogue patterns,\nthe implications of fine-tuning, and the notion of co-augmentation: mutual\nenhancement between human and machine cognition. This perspective offers a new\nlens for understanding interaction, representation, and agency in contemporary\nAI systems.\n", "link": "http://arxiv.org/abs/2505.22767v2", "date": "2025-05-30", "relevancy": 2.4966, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5076}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5076}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4827}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In%20Dialogue%20with%20Intelligence%3A%20Rethinking%20Large%20Language%20Models%20as%0A%20%20Collective%20Knowledge&body=Title%3A%20In%20Dialogue%20with%20Intelligence%3A%20Rethinking%20Large%20Language%20Models%20as%0A%20%20Collective%20Knowledge%0AAuthor%3A%20Eleni%20Vasilaki%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20typically%20analysed%20through%20architectural%2C%0Abehavioural%2C%20or%20training-data%20lenses.%20This%20article%20offers%20a%20theoretical%20and%0Aexperiential%20re-framing%3A%20LLMs%20as%20dynamic%20instantiations%20of%20Collective%20human%0AKnowledge%20%28CK%29%2C%20where%20intelligence%20is%20evoked%20through%20dialogue%20rather%20than%0Astored%20statically.%20Drawing%20on%20concepts%20from%20neuroscience%20and%20AI%2C%20and%20grounded%0Ain%20sustained%20interaction%20with%20ChatGPT-4%2C%20I%20examine%20emergent%20dialogue%20patterns%2C%0Athe%20implications%20of%20fine-tuning%2C%20and%20the%20notion%20of%20co-augmentation%3A%20mutual%0Aenhancement%20between%20human%20and%20machine%20cognition.%20This%20perspective%20offers%20a%20new%0Alens%20for%20understanding%20interaction%2C%20representation%2C%20and%20agency%20in%20contemporary%0AAI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22767v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn%2520Dialogue%2520with%2520Intelligence%253A%2520Rethinking%2520Large%2520Language%2520Models%2520as%250A%2520%2520Collective%2520Knowledge%26entry.906535625%3DEleni%2520Vasilaki%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520typically%2520analysed%2520through%2520architectural%252C%250Abehavioural%252C%2520or%2520training-data%2520lenses.%2520This%2520article%2520offers%2520a%2520theoretical%2520and%250Aexperiential%2520re-framing%253A%2520LLMs%2520as%2520dynamic%2520instantiations%2520of%2520Collective%2520human%250AKnowledge%2520%2528CK%2529%252C%2520where%2520intelligence%2520is%2520evoked%2520through%2520dialogue%2520rather%2520than%250Astored%2520statically.%2520Drawing%2520on%2520concepts%2520from%2520neuroscience%2520and%2520AI%252C%2520and%2520grounded%250Ain%2520sustained%2520interaction%2520with%2520ChatGPT-4%252C%2520I%2520examine%2520emergent%2520dialogue%2520patterns%252C%250Athe%2520implications%2520of%2520fine-tuning%252C%2520and%2520the%2520notion%2520of%2520co-augmentation%253A%2520mutual%250Aenhancement%2520between%2520human%2520and%2520machine%2520cognition.%2520This%2520perspective%2520offers%2520a%2520new%250Alens%2520for%2520understanding%2520interaction%252C%2520representation%252C%2520and%2520agency%2520in%2520contemporary%250AAI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22767v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In%20Dialogue%20with%20Intelligence%3A%20Rethinking%20Large%20Language%20Models%20as%0A%20%20Collective%20Knowledge&entry.906535625=Eleni%20Vasilaki&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20typically%20analysed%20through%20architectural%2C%0Abehavioural%2C%20or%20training-data%20lenses.%20This%20article%20offers%20a%20theoretical%20and%0Aexperiential%20re-framing%3A%20LLMs%20as%20dynamic%20instantiations%20of%20Collective%20human%0AKnowledge%20%28CK%29%2C%20where%20intelligence%20is%20evoked%20through%20dialogue%20rather%20than%0Astored%20statically.%20Drawing%20on%20concepts%20from%20neuroscience%20and%20AI%2C%20and%20grounded%0Ain%20sustained%20interaction%20with%20ChatGPT-4%2C%20I%20examine%20emergent%20dialogue%20patterns%2C%0Athe%20implications%20of%20fine-tuning%2C%20and%20the%20notion%20of%20co-augmentation%3A%20mutual%0Aenhancement%20between%20human%20and%20machine%20cognition.%20This%20perspective%20offers%20a%20new%0Alens%20for%20understanding%20interaction%2C%20representation%2C%20and%20agency%20in%20contemporary%0AAI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22767v2&entry.124074799=Read"},
{"title": "DemoShapley: Valuation of Demonstrations for In-Context Learning", "author": "Shan Xie and Man Luo and Chadly Daniel Stern and Mengnan Du and Lu Cheng", "abstract": "  Large language models (LLMs) using in-context learning (ICL) excel in many\ntasks without task-specific fine-tuning. However, demonstration selection and\nordering greatly impact ICL effectiveness. To address this, we propose\nDemoShapley and Beta-DemoShapley, inspired by Data Shapley and Beta Shapley, to\nassess the influence of individual demonstrations. DemoShapley captures how\neach example influences performance in different contexts, unlike other\ninfluence-based methods that rely on a fixed number of demonstrations.\nBeta-DemoShapley further enhances this framework by incorporating the Beta\ndistribution, allowing users to assign higher weights to smaller cardinalities,\nwhich aligns with ICL's prompt length and computational constraints. Our\nfindings show that the proposed algorithms improve model performance by\nselecting quality demonstrations, and enhancing generalization to\nout-of-distribution tasks. It also identifies noise-compromised data and\npromotes fairness in LLMs, protecting model performance and ensuring robustness\nacross various scenarios.\n", "link": "http://arxiv.org/abs/2410.07523v2", "date": "2025-05-30", "relevancy": 2.4954, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4994}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4989}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4989}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DemoShapley%3A%20Valuation%20of%20Demonstrations%20for%20In-Context%20Learning&body=Title%3A%20DemoShapley%3A%20Valuation%20of%20Demonstrations%20for%20In-Context%20Learning%0AAuthor%3A%20Shan%20Xie%20and%20Man%20Luo%20and%20Chadly%20Daniel%20Stern%20and%20Mengnan%20Du%20and%20Lu%20Cheng%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20using%20in-context%20learning%20%28ICL%29%20excel%20in%20many%0Atasks%20without%20task-specific%20fine-tuning.%20However%2C%20demonstration%20selection%20and%0Aordering%20greatly%20impact%20ICL%20effectiveness.%20To%20address%20this%2C%20we%20propose%0ADemoShapley%20and%20Beta-DemoShapley%2C%20inspired%20by%20Data%20Shapley%20and%20Beta%20Shapley%2C%20to%0Aassess%20the%20influence%20of%20individual%20demonstrations.%20DemoShapley%20captures%20how%0Aeach%20example%20influences%20performance%20in%20different%20contexts%2C%20unlike%20other%0Ainfluence-based%20methods%20that%20rely%20on%20a%20fixed%20number%20of%20demonstrations.%0ABeta-DemoShapley%20further%20enhances%20this%20framework%20by%20incorporating%20the%20Beta%0Adistribution%2C%20allowing%20users%20to%20assign%20higher%20weights%20to%20smaller%20cardinalities%2C%0Awhich%20aligns%20with%20ICL%27s%20prompt%20length%20and%20computational%20constraints.%20Our%0Afindings%20show%20that%20the%20proposed%20algorithms%20improve%20model%20performance%20by%0Aselecting%20quality%20demonstrations%2C%20and%20enhancing%20generalization%20to%0Aout-of-distribution%20tasks.%20It%20also%20identifies%20noise-compromised%20data%20and%0Apromotes%20fairness%20in%20LLMs%2C%20protecting%20model%20performance%20and%20ensuring%20robustness%0Aacross%20various%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07523v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDemoShapley%253A%2520Valuation%2520of%2520Demonstrations%2520for%2520In-Context%2520Learning%26entry.906535625%3DShan%2520Xie%2520and%2520Man%2520Luo%2520and%2520Chadly%2520Daniel%2520Stern%2520and%2520Mengnan%2520Du%2520and%2520Lu%2520Cheng%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520using%2520in-context%2520learning%2520%2528ICL%2529%2520excel%2520in%2520many%250Atasks%2520without%2520task-specific%2520fine-tuning.%2520However%252C%2520demonstration%2520selection%2520and%250Aordering%2520greatly%2520impact%2520ICL%2520effectiveness.%2520To%2520address%2520this%252C%2520we%2520propose%250ADemoShapley%2520and%2520Beta-DemoShapley%252C%2520inspired%2520by%2520Data%2520Shapley%2520and%2520Beta%2520Shapley%252C%2520to%250Aassess%2520the%2520influence%2520of%2520individual%2520demonstrations.%2520DemoShapley%2520captures%2520how%250Aeach%2520example%2520influences%2520performance%2520in%2520different%2520contexts%252C%2520unlike%2520other%250Ainfluence-based%2520methods%2520that%2520rely%2520on%2520a%2520fixed%2520number%2520of%2520demonstrations.%250ABeta-DemoShapley%2520further%2520enhances%2520this%2520framework%2520by%2520incorporating%2520the%2520Beta%250Adistribution%252C%2520allowing%2520users%2520to%2520assign%2520higher%2520weights%2520to%2520smaller%2520cardinalities%252C%250Awhich%2520aligns%2520with%2520ICL%2527s%2520prompt%2520length%2520and%2520computational%2520constraints.%2520Our%250Afindings%2520show%2520that%2520the%2520proposed%2520algorithms%2520improve%2520model%2520performance%2520by%250Aselecting%2520quality%2520demonstrations%252C%2520and%2520enhancing%2520generalization%2520to%250Aout-of-distribution%2520tasks.%2520It%2520also%2520identifies%2520noise-compromised%2520data%2520and%250Apromotes%2520fairness%2520in%2520LLMs%252C%2520protecting%2520model%2520performance%2520and%2520ensuring%2520robustness%250Aacross%2520various%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07523v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DemoShapley%3A%20Valuation%20of%20Demonstrations%20for%20In-Context%20Learning&entry.906535625=Shan%20Xie%20and%20Man%20Luo%20and%20Chadly%20Daniel%20Stern%20and%20Mengnan%20Du%20and%20Lu%20Cheng&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20using%20in-context%20learning%20%28ICL%29%20excel%20in%20many%0Atasks%20without%20task-specific%20fine-tuning.%20However%2C%20demonstration%20selection%20and%0Aordering%20greatly%20impact%20ICL%20effectiveness.%20To%20address%20this%2C%20we%20propose%0ADemoShapley%20and%20Beta-DemoShapley%2C%20inspired%20by%20Data%20Shapley%20and%20Beta%20Shapley%2C%20to%0Aassess%20the%20influence%20of%20individual%20demonstrations.%20DemoShapley%20captures%20how%0Aeach%20example%20influences%20performance%20in%20different%20contexts%2C%20unlike%20other%0Ainfluence-based%20methods%20that%20rely%20on%20a%20fixed%20number%20of%20demonstrations.%0ABeta-DemoShapley%20further%20enhances%20this%20framework%20by%20incorporating%20the%20Beta%0Adistribution%2C%20allowing%20users%20to%20assign%20higher%20weights%20to%20smaller%20cardinalities%2C%0Awhich%20aligns%20with%20ICL%27s%20prompt%20length%20and%20computational%20constraints.%20Our%0Afindings%20show%20that%20the%20proposed%20algorithms%20improve%20model%20performance%20by%0Aselecting%20quality%20demonstrations%2C%20and%20enhancing%20generalization%20to%0Aout-of-distribution%20tasks.%20It%20also%20identifies%20noise-compromised%20data%20and%0Apromotes%20fairness%20in%20LLMs%2C%20protecting%20model%20performance%20and%20ensuring%20robustness%0Aacross%20various%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07523v2&entry.124074799=Read"},
{"title": "UniGeo: Taming Video Diffusion for Unified Consistent Geometry\n  Estimation", "author": "Yang-Tian Sun and Xin Yu and Zehuan Huang and Yi-Hua Huang and Yuan-Chen Guo and Ziyi Yang and Yan-Pei Cao and Xiaojuan Qi", "abstract": "  Recently, methods leveraging diffusion model priors to assist monocular\ngeometric estimation (e.g., depth and normal) have gained significant attention\ndue to their strong generalization ability. However, most existing works focus\non estimating geometric properties within the camera coordinate system of\nindividual video frames, neglecting the inherent ability of diffusion models to\ndetermine inter-frame correspondence. In this work, we demonstrate that,\nthrough appropriate design and fine-tuning, the intrinsic consistency of video\ngeneration models can be effectively harnessed for consistent geometric\nestimation. Specifically, we 1) select geometric attributes in the global\ncoordinate system that share the same correspondence with video frames as the\nprediction targets, 2) introduce a novel and efficient conditioning method by\nreusing positional encodings, and 3) enhance performance through joint training\non multiple geometric attributes that share the same correspondence. Our\nresults achieve superior performance in predicting global geometric attributes\nin videos and can be directly applied to reconstruction tasks. Even when\ntrained solely on static video data, our approach exhibits the potential to\ngeneralize to dynamic video scenes.\n", "link": "http://arxiv.org/abs/2505.24521v1", "date": "2025-05-30", "relevancy": 2.4915, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6387}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6369}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6025}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniGeo%3A%20Taming%20Video%20Diffusion%20for%20Unified%20Consistent%20Geometry%0A%20%20Estimation&body=Title%3A%20UniGeo%3A%20Taming%20Video%20Diffusion%20for%20Unified%20Consistent%20Geometry%0A%20%20Estimation%0AAuthor%3A%20Yang-Tian%20Sun%20and%20Xin%20Yu%20and%20Zehuan%20Huang%20and%20Yi-Hua%20Huang%20and%20Yuan-Chen%20Guo%20and%20Ziyi%20Yang%20and%20Yan-Pei%20Cao%20and%20Xiaojuan%20Qi%0AAbstract%3A%20%20%20Recently%2C%20methods%20leveraging%20diffusion%20model%20priors%20to%20assist%20monocular%0Ageometric%20estimation%20%28e.g.%2C%20depth%20and%20normal%29%20have%20gained%20significant%20attention%0Adue%20to%20their%20strong%20generalization%20ability.%20However%2C%20most%20existing%20works%20focus%0Aon%20estimating%20geometric%20properties%20within%20the%20camera%20coordinate%20system%20of%0Aindividual%20video%20frames%2C%20neglecting%20the%20inherent%20ability%20of%20diffusion%20models%20to%0Adetermine%20inter-frame%20correspondence.%20In%20this%20work%2C%20we%20demonstrate%20that%2C%0Athrough%20appropriate%20design%20and%20fine-tuning%2C%20the%20intrinsic%20consistency%20of%20video%0Ageneration%20models%20can%20be%20effectively%20harnessed%20for%20consistent%20geometric%0Aestimation.%20Specifically%2C%20we%201%29%20select%20geometric%20attributes%20in%20the%20global%0Acoordinate%20system%20that%20share%20the%20same%20correspondence%20with%20video%20frames%20as%20the%0Aprediction%20targets%2C%202%29%20introduce%20a%20novel%20and%20efficient%20conditioning%20method%20by%0Areusing%20positional%20encodings%2C%20and%203%29%20enhance%20performance%20through%20joint%20training%0Aon%20multiple%20geometric%20attributes%20that%20share%20the%20same%20correspondence.%20Our%0Aresults%20achieve%20superior%20performance%20in%20predicting%20global%20geometric%20attributes%0Ain%20videos%20and%20can%20be%20directly%20applied%20to%20reconstruction%20tasks.%20Even%20when%0Atrained%20solely%20on%20static%20video%20data%2C%20our%20approach%20exhibits%20the%20potential%20to%0Ageneralize%20to%20dynamic%20video%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24521v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniGeo%253A%2520Taming%2520Video%2520Diffusion%2520for%2520Unified%2520Consistent%2520Geometry%250A%2520%2520Estimation%26entry.906535625%3DYang-Tian%2520Sun%2520and%2520Xin%2520Yu%2520and%2520Zehuan%2520Huang%2520and%2520Yi-Hua%2520Huang%2520and%2520Yuan-Chen%2520Guo%2520and%2520Ziyi%2520Yang%2520and%2520Yan-Pei%2520Cao%2520and%2520Xiaojuan%2520Qi%26entry.1292438233%3D%2520%2520Recently%252C%2520methods%2520leveraging%2520diffusion%2520model%2520priors%2520to%2520assist%2520monocular%250Ageometric%2520estimation%2520%2528e.g.%252C%2520depth%2520and%2520normal%2529%2520have%2520gained%2520significant%2520attention%250Adue%2520to%2520their%2520strong%2520generalization%2520ability.%2520However%252C%2520most%2520existing%2520works%2520focus%250Aon%2520estimating%2520geometric%2520properties%2520within%2520the%2520camera%2520coordinate%2520system%2520of%250Aindividual%2520video%2520frames%252C%2520neglecting%2520the%2520inherent%2520ability%2520of%2520diffusion%2520models%2520to%250Adetermine%2520inter-frame%2520correspondence.%2520In%2520this%2520work%252C%2520we%2520demonstrate%2520that%252C%250Athrough%2520appropriate%2520design%2520and%2520fine-tuning%252C%2520the%2520intrinsic%2520consistency%2520of%2520video%250Ageneration%2520models%2520can%2520be%2520effectively%2520harnessed%2520for%2520consistent%2520geometric%250Aestimation.%2520Specifically%252C%2520we%25201%2529%2520select%2520geometric%2520attributes%2520in%2520the%2520global%250Acoordinate%2520system%2520that%2520share%2520the%2520same%2520correspondence%2520with%2520video%2520frames%2520as%2520the%250Aprediction%2520targets%252C%25202%2529%2520introduce%2520a%2520novel%2520and%2520efficient%2520conditioning%2520method%2520by%250Areusing%2520positional%2520encodings%252C%2520and%25203%2529%2520enhance%2520performance%2520through%2520joint%2520training%250Aon%2520multiple%2520geometric%2520attributes%2520that%2520share%2520the%2520same%2520correspondence.%2520Our%250Aresults%2520achieve%2520superior%2520performance%2520in%2520predicting%2520global%2520geometric%2520attributes%250Ain%2520videos%2520and%2520can%2520be%2520directly%2520applied%2520to%2520reconstruction%2520tasks.%2520Even%2520when%250Atrained%2520solely%2520on%2520static%2520video%2520data%252C%2520our%2520approach%2520exhibits%2520the%2520potential%2520to%250Ageneralize%2520to%2520dynamic%2520video%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24521v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniGeo%3A%20Taming%20Video%20Diffusion%20for%20Unified%20Consistent%20Geometry%0A%20%20Estimation&entry.906535625=Yang-Tian%20Sun%20and%20Xin%20Yu%20and%20Zehuan%20Huang%20and%20Yi-Hua%20Huang%20and%20Yuan-Chen%20Guo%20and%20Ziyi%20Yang%20and%20Yan-Pei%20Cao%20and%20Xiaojuan%20Qi&entry.1292438233=%20%20Recently%2C%20methods%20leveraging%20diffusion%20model%20priors%20to%20assist%20monocular%0Ageometric%20estimation%20%28e.g.%2C%20depth%20and%20normal%29%20have%20gained%20significant%20attention%0Adue%20to%20their%20strong%20generalization%20ability.%20However%2C%20most%20existing%20works%20focus%0Aon%20estimating%20geometric%20properties%20within%20the%20camera%20coordinate%20system%20of%0Aindividual%20video%20frames%2C%20neglecting%20the%20inherent%20ability%20of%20diffusion%20models%20to%0Adetermine%20inter-frame%20correspondence.%20In%20this%20work%2C%20we%20demonstrate%20that%2C%0Athrough%20appropriate%20design%20and%20fine-tuning%2C%20the%20intrinsic%20consistency%20of%20video%0Ageneration%20models%20can%20be%20effectively%20harnessed%20for%20consistent%20geometric%0Aestimation.%20Specifically%2C%20we%201%29%20select%20geometric%20attributes%20in%20the%20global%0Acoordinate%20system%20that%20share%20the%20same%20correspondence%20with%20video%20frames%20as%20the%0Aprediction%20targets%2C%202%29%20introduce%20a%20novel%20and%20efficient%20conditioning%20method%20by%0Areusing%20positional%20encodings%2C%20and%203%29%20enhance%20performance%20through%20joint%20training%0Aon%20multiple%20geometric%20attributes%20that%20share%20the%20same%20correspondence.%20Our%0Aresults%20achieve%20superior%20performance%20in%20predicting%20global%20geometric%20attributes%0Ain%20videos%20and%20can%20be%20directly%20applied%20to%20reconstruction%20tasks.%20Even%20when%0Atrained%20solely%20on%20static%20video%20data%2C%20our%20approach%20exhibits%20the%20potential%20to%0Ageneralize%20to%20dynamic%20video%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24521v1&entry.124074799=Read"},
{"title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "author": "Zehan Wang and Jiayang Xu and Ziang Zhang and Tianyu Pan and Chao Du and Hengshuang Zhao and Zhou Zhao", "abstract": "  Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.\n", "link": "http://arxiv.org/abs/2505.24870v1", "date": "2025-05-30", "relevancy": 2.4828, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6229}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6229}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenSpace%3A%20Benchmarking%20Spatially-Aware%20Image%20Generation&body=Title%3A%20GenSpace%3A%20Benchmarking%20Spatially-Aware%20Image%20Generation%0AAuthor%3A%20Zehan%20Wang%20and%20Jiayang%20Xu%20and%20Ziang%20Zhang%20and%20Tianyu%20Pan%20and%20Chao%20Du%20and%20Hengshuang%20Zhao%20and%20Zhou%20Zhao%0AAbstract%3A%20%20%20Humans%20can%20intuitively%20compose%20and%20arrange%20scenes%20in%20the%203D%20space%20for%0Aphotography.%20However%2C%20can%20advanced%20AI%20image%20generators%20plan%20scenes%20with%20similar%0A3D%20spatial%20awareness%20when%20creating%20images%20from%20text%20or%20image%20prompts%3F%20We%0Apresent%20GenSpace%2C%20a%20novel%20benchmark%20and%20evaluation%20pipeline%20to%20comprehensively%0Aassess%20the%20spatial%20awareness%20of%20current%20image%20generation%20models.%20Furthermore%2C%0Astandard%20evaluations%20using%20general%20Vision-Language%20Models%20%28VLMs%29%20frequently%0Afail%20to%20capture%20the%20detailed%20spatial%20errors.%20To%20handle%20this%20challenge%2C%20we%0Apropose%20a%20specialized%20evaluation%20pipeline%20and%20metric%2C%20which%20reconstructs%203D%0Ascene%20geometry%20using%20multiple%20visual%20foundation%20models%20and%20provides%20a%20more%0Aaccurate%20and%20human-aligned%20metric%20of%20spatial%20faithfulness.%20Our%20findings%20show%0Athat%20while%20AI%20models%20create%20visually%20appealing%20images%20and%20can%20follow%20general%0Ainstructions%2C%20they%20struggle%20with%20specific%203D%20details%20like%20object%20placement%2C%0Arelationships%2C%20and%20measurements.%20We%20summarize%20three%20core%20limitations%20in%20the%0Aspatial%20perception%20of%20current%20state-of-the-art%20image%20generation%20models%3A%201%29%0AObject%20Perspective%20Understanding%2C%202%29%20Egocentric-Allocentric%20Transformation%20and%0A3%29%20Metric%20Measurement%20Adherence%2C%20highlighting%20possible%20directions%20for%20improving%0Aspatial%20intelligence%20in%20image%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24870v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenSpace%253A%2520Benchmarking%2520Spatially-Aware%2520Image%2520Generation%26entry.906535625%3DZehan%2520Wang%2520and%2520Jiayang%2520Xu%2520and%2520Ziang%2520Zhang%2520and%2520Tianyu%2520Pan%2520and%2520Chao%2520Du%2520and%2520Hengshuang%2520Zhao%2520and%2520Zhou%2520Zhao%26entry.1292438233%3D%2520%2520Humans%2520can%2520intuitively%2520compose%2520and%2520arrange%2520scenes%2520in%2520the%25203D%2520space%2520for%250Aphotography.%2520However%252C%2520can%2520advanced%2520AI%2520image%2520generators%2520plan%2520scenes%2520with%2520similar%250A3D%2520spatial%2520awareness%2520when%2520creating%2520images%2520from%2520text%2520or%2520image%2520prompts%253F%2520We%250Apresent%2520GenSpace%252C%2520a%2520novel%2520benchmark%2520and%2520evaluation%2520pipeline%2520to%2520comprehensively%250Aassess%2520the%2520spatial%2520awareness%2520of%2520current%2520image%2520generation%2520models.%2520Furthermore%252C%250Astandard%2520evaluations%2520using%2520general%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520frequently%250Afail%2520to%2520capture%2520the%2520detailed%2520spatial%2520errors.%2520To%2520handle%2520this%2520challenge%252C%2520we%250Apropose%2520a%2520specialized%2520evaluation%2520pipeline%2520and%2520metric%252C%2520which%2520reconstructs%25203D%250Ascene%2520geometry%2520using%2520multiple%2520visual%2520foundation%2520models%2520and%2520provides%2520a%2520more%250Aaccurate%2520and%2520human-aligned%2520metric%2520of%2520spatial%2520faithfulness.%2520Our%2520findings%2520show%250Athat%2520while%2520AI%2520models%2520create%2520visually%2520appealing%2520images%2520and%2520can%2520follow%2520general%250Ainstructions%252C%2520they%2520struggle%2520with%2520specific%25203D%2520details%2520like%2520object%2520placement%252C%250Arelationships%252C%2520and%2520measurements.%2520We%2520summarize%2520three%2520core%2520limitations%2520in%2520the%250Aspatial%2520perception%2520of%2520current%2520state-of-the-art%2520image%2520generation%2520models%253A%25201%2529%250AObject%2520Perspective%2520Understanding%252C%25202%2529%2520Egocentric-Allocentric%2520Transformation%2520and%250A3%2529%2520Metric%2520Measurement%2520Adherence%252C%2520highlighting%2520possible%2520directions%2520for%2520improving%250Aspatial%2520intelligence%2520in%2520image%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24870v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenSpace%3A%20Benchmarking%20Spatially-Aware%20Image%20Generation&entry.906535625=Zehan%20Wang%20and%20Jiayang%20Xu%20and%20Ziang%20Zhang%20and%20Tianyu%20Pan%20and%20Chao%20Du%20and%20Hengshuang%20Zhao%20and%20Zhou%20Zhao&entry.1292438233=%20%20Humans%20can%20intuitively%20compose%20and%20arrange%20scenes%20in%20the%203D%20space%20for%0Aphotography.%20However%2C%20can%20advanced%20AI%20image%20generators%20plan%20scenes%20with%20similar%0A3D%20spatial%20awareness%20when%20creating%20images%20from%20text%20or%20image%20prompts%3F%20We%0Apresent%20GenSpace%2C%20a%20novel%20benchmark%20and%20evaluation%20pipeline%20to%20comprehensively%0Aassess%20the%20spatial%20awareness%20of%20current%20image%20generation%20models.%20Furthermore%2C%0Astandard%20evaluations%20using%20general%20Vision-Language%20Models%20%28VLMs%29%20frequently%0Afail%20to%20capture%20the%20detailed%20spatial%20errors.%20To%20handle%20this%20challenge%2C%20we%0Apropose%20a%20specialized%20evaluation%20pipeline%20and%20metric%2C%20which%20reconstructs%203D%0Ascene%20geometry%20using%20multiple%20visual%20foundation%20models%20and%20provides%20a%20more%0Aaccurate%20and%20human-aligned%20metric%20of%20spatial%20faithfulness.%20Our%20findings%20show%0Athat%20while%20AI%20models%20create%20visually%20appealing%20images%20and%20can%20follow%20general%0Ainstructions%2C%20they%20struggle%20with%20specific%203D%20details%20like%20object%20placement%2C%0Arelationships%2C%20and%20measurements.%20We%20summarize%20three%20core%20limitations%20in%20the%0Aspatial%20perception%20of%20current%20state-of-the-art%20image%20generation%20models%3A%201%29%0AObject%20Perspective%20Understanding%2C%202%29%20Egocentric-Allocentric%20Transformation%20and%0A3%29%20Metric%20Measurement%20Adherence%2C%20highlighting%20possible%20directions%20for%20improving%0Aspatial%20intelligence%20in%20image%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24870v1&entry.124074799=Read"},
{"title": "Efficient Text Encoders for Labor Market Analysis", "author": "Jens-Joris Decorte and Jeroen Van Hautte and Chris Develder and Thomas Demeester", "abstract": "  Labor market analysis relies on extracting insights from job advertisements,\nwhich provide valuable yet unstructured information on job titles and\ncorresponding skill requirements. While state-of-the-art methods for skill\nextraction achieve strong performance, they depend on large language models\n(LLMs), which are computationally expensive and slow. In this paper, we propose\n\\textbf{ConTeXT-match}, a novel contrastive learning approach with token-level\nattention that is well-suited for the extreme multi-label classification task\nof skill classification. \\textbf{ConTeXT-match} significantly improves skill\nextraction efficiency and performance, achieving state-of-the-art results with\na lightweight bi-encoder model. To support robust evaluation, we introduce\n\\textbf{Skill-XL}, a new benchmark with exhaustive, sentence-level skill\nannotations that explicitly address the redundancy in the large label space.\nFinally, we present \\textbf{JobBERT V2}, an improved job title normalization\nmodel that leverages extracted skills to produce high-quality job title\nrepresentations. Experiments demonstrate that our models are efficient,\naccurate, and scalable, making them ideal for large-scale, real-time labor\nmarket analysis.\n", "link": "http://arxiv.org/abs/2505.24640v1", "date": "2025-05-30", "relevancy": 2.4743, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5004}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5004}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Text%20Encoders%20for%20Labor%20Market%20Analysis&body=Title%3A%20Efficient%20Text%20Encoders%20for%20Labor%20Market%20Analysis%0AAuthor%3A%20Jens-Joris%20Decorte%20and%20Jeroen%20Van%20Hautte%20and%20Chris%20Develder%20and%20Thomas%20Demeester%0AAbstract%3A%20%20%20Labor%20market%20analysis%20relies%20on%20extracting%20insights%20from%20job%20advertisements%2C%0Awhich%20provide%20valuable%20yet%20unstructured%20information%20on%20job%20titles%20and%0Acorresponding%20skill%20requirements.%20While%20state-of-the-art%20methods%20for%20skill%0Aextraction%20achieve%20strong%20performance%2C%20they%20depend%20on%20large%20language%20models%0A%28LLMs%29%2C%20which%20are%20computationally%20expensive%20and%20slow.%20In%20this%20paper%2C%20we%20propose%0A%5Ctextbf%7BConTeXT-match%7D%2C%20a%20novel%20contrastive%20learning%20approach%20with%20token-level%0Aattention%20that%20is%20well-suited%20for%20the%20extreme%20multi-label%20classification%20task%0Aof%20skill%20classification.%20%5Ctextbf%7BConTeXT-match%7D%20significantly%20improves%20skill%0Aextraction%20efficiency%20and%20performance%2C%20achieving%20state-of-the-art%20results%20with%0Aa%20lightweight%20bi-encoder%20model.%20To%20support%20robust%20evaluation%2C%20we%20introduce%0A%5Ctextbf%7BSkill-XL%7D%2C%20a%20new%20benchmark%20with%20exhaustive%2C%20sentence-level%20skill%0Aannotations%20that%20explicitly%20address%20the%20redundancy%20in%20the%20large%20label%20space.%0AFinally%2C%20we%20present%20%5Ctextbf%7BJobBERT%20V2%7D%2C%20an%20improved%20job%20title%20normalization%0Amodel%20that%20leverages%20extracted%20skills%20to%20produce%20high-quality%20job%20title%0Arepresentations.%20Experiments%20demonstrate%20that%20our%20models%20are%20efficient%2C%0Aaccurate%2C%20and%20scalable%2C%20making%20them%20ideal%20for%20large-scale%2C%20real-time%20labor%0Amarket%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24640v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Text%2520Encoders%2520for%2520Labor%2520Market%2520Analysis%26entry.906535625%3DJens-Joris%2520Decorte%2520and%2520Jeroen%2520Van%2520Hautte%2520and%2520Chris%2520Develder%2520and%2520Thomas%2520Demeester%26entry.1292438233%3D%2520%2520Labor%2520market%2520analysis%2520relies%2520on%2520extracting%2520insights%2520from%2520job%2520advertisements%252C%250Awhich%2520provide%2520valuable%2520yet%2520unstructured%2520information%2520on%2520job%2520titles%2520and%250Acorresponding%2520skill%2520requirements.%2520While%2520state-of-the-art%2520methods%2520for%2520skill%250Aextraction%2520achieve%2520strong%2520performance%252C%2520they%2520depend%2520on%2520large%2520language%2520models%250A%2528LLMs%2529%252C%2520which%2520are%2520computationally%2520expensive%2520and%2520slow.%2520In%2520this%2520paper%252C%2520we%2520propose%250A%255Ctextbf%257BConTeXT-match%257D%252C%2520a%2520novel%2520contrastive%2520learning%2520approach%2520with%2520token-level%250Aattention%2520that%2520is%2520well-suited%2520for%2520the%2520extreme%2520multi-label%2520classification%2520task%250Aof%2520skill%2520classification.%2520%255Ctextbf%257BConTeXT-match%257D%2520significantly%2520improves%2520skill%250Aextraction%2520efficiency%2520and%2520performance%252C%2520achieving%2520state-of-the-art%2520results%2520with%250Aa%2520lightweight%2520bi-encoder%2520model.%2520To%2520support%2520robust%2520evaluation%252C%2520we%2520introduce%250A%255Ctextbf%257BSkill-XL%257D%252C%2520a%2520new%2520benchmark%2520with%2520exhaustive%252C%2520sentence-level%2520skill%250Aannotations%2520that%2520explicitly%2520address%2520the%2520redundancy%2520in%2520the%2520large%2520label%2520space.%250AFinally%252C%2520we%2520present%2520%255Ctextbf%257BJobBERT%2520V2%257D%252C%2520an%2520improved%2520job%2520title%2520normalization%250Amodel%2520that%2520leverages%2520extracted%2520skills%2520to%2520produce%2520high-quality%2520job%2520title%250Arepresentations.%2520Experiments%2520demonstrate%2520that%2520our%2520models%2520are%2520efficient%252C%250Aaccurate%252C%2520and%2520scalable%252C%2520making%2520them%2520ideal%2520for%2520large-scale%252C%2520real-time%2520labor%250Amarket%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24640v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Text%20Encoders%20for%20Labor%20Market%20Analysis&entry.906535625=Jens-Joris%20Decorte%20and%20Jeroen%20Van%20Hautte%20and%20Chris%20Develder%20and%20Thomas%20Demeester&entry.1292438233=%20%20Labor%20market%20analysis%20relies%20on%20extracting%20insights%20from%20job%20advertisements%2C%0Awhich%20provide%20valuable%20yet%20unstructured%20information%20on%20job%20titles%20and%0Acorresponding%20skill%20requirements.%20While%20state-of-the-art%20methods%20for%20skill%0Aextraction%20achieve%20strong%20performance%2C%20they%20depend%20on%20large%20language%20models%0A%28LLMs%29%2C%20which%20are%20computationally%20expensive%20and%20slow.%20In%20this%20paper%2C%20we%20propose%0A%5Ctextbf%7BConTeXT-match%7D%2C%20a%20novel%20contrastive%20learning%20approach%20with%20token-level%0Aattention%20that%20is%20well-suited%20for%20the%20extreme%20multi-label%20classification%20task%0Aof%20skill%20classification.%20%5Ctextbf%7BConTeXT-match%7D%20significantly%20improves%20skill%0Aextraction%20efficiency%20and%20performance%2C%20achieving%20state-of-the-art%20results%20with%0Aa%20lightweight%20bi-encoder%20model.%20To%20support%20robust%20evaluation%2C%20we%20introduce%0A%5Ctextbf%7BSkill-XL%7D%2C%20a%20new%20benchmark%20with%20exhaustive%2C%20sentence-level%20skill%0Aannotations%20that%20explicitly%20address%20the%20redundancy%20in%20the%20large%20label%20space.%0AFinally%2C%20we%20present%20%5Ctextbf%7BJobBERT%20V2%7D%2C%20an%20improved%20job%20title%20normalization%0Amodel%20that%20leverages%20extracted%20skills%20to%20produce%20high-quality%20job%20title%0Arepresentations.%20Experiments%20demonstrate%20that%20our%20models%20are%20efficient%2C%0Aaccurate%2C%20and%20scalable%2C%20making%20them%20ideal%20for%20large-scale%2C%20real-time%20labor%0Amarket%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24640v1&entry.124074799=Read"},
{"title": "LPASS: Linear Probes as Stepping Stones for vulnerability detection\n  using compressed LLMs", "author": "Luis Ibanez-Lissen and Lorena Gonzalez-Manzano and Jose Maria de Fuentes and Nicolas Anciaux", "abstract": "  Large Language Models (LLMs) are being extensively used for cybersecurity\npurposes. One of them is the detection of vulnerable codes. For the sake of\nefficiency and effectiveness, compression and fine-tuning techniques are being\ndeveloped, respectively. However, they involve spending substantial\ncomputational efforts. In this vein, we analyse how Linear Probes (LPs) can be\nused to provide an estimation on the performance of a compressed LLM at an\nearly phase -- before fine-tuning. We also show their suitability to set the\ncut-off point when applying layer pruning compression. Our approach, dubbed\n$LPASS$, is applied in BERT and Gemma for the detection of 12 of MITRE's Top 25\nmost dangerous vulnerabilities on 480k C/C++ samples. LPs can be computed in\n142.97 s. and provide key findings: (1) 33.3 \\% and 72.2\\% of layers can be\nremoved, respectively, with no precision loss; (2) they provide an early\nestimate of the post-fine-tuning and post-compression model effectiveness, with\n3\\% and 8.68\\% as the lowest and average precision errors, respectively.\n$LPASS$-based LLMs outperform the state of the art, reaching 86.9\\% of accuracy\nin multi-class vulnerability detection. Interestingly, $LPASS$-based compressed\nversions of Gemma outperform the original ones by 1.6\\% of F1-score at a\nmaximum while saving 29.4 \\% and 23.8\\% of training and inference time and\n42.98\\% of model size.\n", "link": "http://arxiv.org/abs/2505.24451v1", "date": "2025-05-30", "relevancy": 2.4644, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4988}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4988}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LPASS%3A%20Linear%20Probes%20as%20Stepping%20Stones%20for%20vulnerability%20detection%0A%20%20using%20compressed%20LLMs&body=Title%3A%20LPASS%3A%20Linear%20Probes%20as%20Stepping%20Stones%20for%20vulnerability%20detection%0A%20%20using%20compressed%20LLMs%0AAuthor%3A%20Luis%20Ibanez-Lissen%20and%20Lorena%20Gonzalez-Manzano%20and%20Jose%20Maria%20de%20Fuentes%20and%20Nicolas%20Anciaux%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20being%20extensively%20used%20for%20cybersecurity%0Apurposes.%20One%20of%20them%20is%20the%20detection%20of%20vulnerable%20codes.%20For%20the%20sake%20of%0Aefficiency%20and%20effectiveness%2C%20compression%20and%20fine-tuning%20techniques%20are%20being%0Adeveloped%2C%20respectively.%20However%2C%20they%20involve%20spending%20substantial%0Acomputational%20efforts.%20In%20this%20vein%2C%20we%20analyse%20how%20Linear%20Probes%20%28LPs%29%20can%20be%0Aused%20to%20provide%20an%20estimation%20on%20the%20performance%20of%20a%20compressed%20LLM%20at%20an%0Aearly%20phase%20--%20before%20fine-tuning.%20We%20also%20show%20their%20suitability%20to%20set%20the%0Acut-off%20point%20when%20applying%20layer%20pruning%20compression.%20Our%20approach%2C%20dubbed%0A%24LPASS%24%2C%20is%20applied%20in%20BERT%20and%20Gemma%20for%20the%20detection%20of%2012%20of%20MITRE%27s%20Top%2025%0Amost%20dangerous%20vulnerabilities%20on%20480k%20C/C%2B%2B%20samples.%20LPs%20can%20be%20computed%20in%0A142.97%20s.%20and%20provide%20key%20findings%3A%20%281%29%2033.3%20%5C%25%20and%2072.2%5C%25%20of%20layers%20can%20be%0Aremoved%2C%20respectively%2C%20with%20no%20precision%20loss%3B%20%282%29%20they%20provide%20an%20early%0Aestimate%20of%20the%20post-fine-tuning%20and%20post-compression%20model%20effectiveness%2C%20with%0A3%5C%25%20and%208.68%5C%25%20as%20the%20lowest%20and%20average%20precision%20errors%2C%20respectively.%0A%24LPASS%24-based%20LLMs%20outperform%20the%20state%20of%20the%20art%2C%20reaching%2086.9%5C%25%20of%20accuracy%0Ain%20multi-class%20vulnerability%20detection.%20Interestingly%2C%20%24LPASS%24-based%20compressed%0Aversions%20of%20Gemma%20outperform%20the%20original%20ones%20by%201.6%5C%25%20of%20F1-score%20at%20a%0Amaximum%20while%20saving%2029.4%20%5C%25%20and%2023.8%5C%25%20of%20training%20and%20inference%20time%20and%0A42.98%5C%25%20of%20model%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24451v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLPASS%253A%2520Linear%2520Probes%2520as%2520Stepping%2520Stones%2520for%2520vulnerability%2520detection%250A%2520%2520using%2520compressed%2520LLMs%26entry.906535625%3DLuis%2520Ibanez-Lissen%2520and%2520Lorena%2520Gonzalez-Manzano%2520and%2520Jose%2520Maria%2520de%2520Fuentes%2520and%2520Nicolas%2520Anciaux%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520being%2520extensively%2520used%2520for%2520cybersecurity%250Apurposes.%2520One%2520of%2520them%2520is%2520the%2520detection%2520of%2520vulnerable%2520codes.%2520For%2520the%2520sake%2520of%250Aefficiency%2520and%2520effectiveness%252C%2520compression%2520and%2520fine-tuning%2520techniques%2520are%2520being%250Adeveloped%252C%2520respectively.%2520However%252C%2520they%2520involve%2520spending%2520substantial%250Acomputational%2520efforts.%2520In%2520this%2520vein%252C%2520we%2520analyse%2520how%2520Linear%2520Probes%2520%2528LPs%2529%2520can%2520be%250Aused%2520to%2520provide%2520an%2520estimation%2520on%2520the%2520performance%2520of%2520a%2520compressed%2520LLM%2520at%2520an%250Aearly%2520phase%2520--%2520before%2520fine-tuning.%2520We%2520also%2520show%2520their%2520suitability%2520to%2520set%2520the%250Acut-off%2520point%2520when%2520applying%2520layer%2520pruning%2520compression.%2520Our%2520approach%252C%2520dubbed%250A%2524LPASS%2524%252C%2520is%2520applied%2520in%2520BERT%2520and%2520Gemma%2520for%2520the%2520detection%2520of%252012%2520of%2520MITRE%2527s%2520Top%252025%250Amost%2520dangerous%2520vulnerabilities%2520on%2520480k%2520C/C%252B%252B%2520samples.%2520LPs%2520can%2520be%2520computed%2520in%250A142.97%2520s.%2520and%2520provide%2520key%2520findings%253A%2520%25281%2529%252033.3%2520%255C%2525%2520and%252072.2%255C%2525%2520of%2520layers%2520can%2520be%250Aremoved%252C%2520respectively%252C%2520with%2520no%2520precision%2520loss%253B%2520%25282%2529%2520they%2520provide%2520an%2520early%250Aestimate%2520of%2520the%2520post-fine-tuning%2520and%2520post-compression%2520model%2520effectiveness%252C%2520with%250A3%255C%2525%2520and%25208.68%255C%2525%2520as%2520the%2520lowest%2520and%2520average%2520precision%2520errors%252C%2520respectively.%250A%2524LPASS%2524-based%2520LLMs%2520outperform%2520the%2520state%2520of%2520the%2520art%252C%2520reaching%252086.9%255C%2525%2520of%2520accuracy%250Ain%2520multi-class%2520vulnerability%2520detection.%2520Interestingly%252C%2520%2524LPASS%2524-based%2520compressed%250Aversions%2520of%2520Gemma%2520outperform%2520the%2520original%2520ones%2520by%25201.6%255C%2525%2520of%2520F1-score%2520at%2520a%250Amaximum%2520while%2520saving%252029.4%2520%255C%2525%2520and%252023.8%255C%2525%2520of%2520training%2520and%2520inference%2520time%2520and%250A42.98%255C%2525%2520of%2520model%2520size.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24451v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LPASS%3A%20Linear%20Probes%20as%20Stepping%20Stones%20for%20vulnerability%20detection%0A%20%20using%20compressed%20LLMs&entry.906535625=Luis%20Ibanez-Lissen%20and%20Lorena%20Gonzalez-Manzano%20and%20Jose%20Maria%20de%20Fuentes%20and%20Nicolas%20Anciaux&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20being%20extensively%20used%20for%20cybersecurity%0Apurposes.%20One%20of%20them%20is%20the%20detection%20of%20vulnerable%20codes.%20For%20the%20sake%20of%0Aefficiency%20and%20effectiveness%2C%20compression%20and%20fine-tuning%20techniques%20are%20being%0Adeveloped%2C%20respectively.%20However%2C%20they%20involve%20spending%20substantial%0Acomputational%20efforts.%20In%20this%20vein%2C%20we%20analyse%20how%20Linear%20Probes%20%28LPs%29%20can%20be%0Aused%20to%20provide%20an%20estimation%20on%20the%20performance%20of%20a%20compressed%20LLM%20at%20an%0Aearly%20phase%20--%20before%20fine-tuning.%20We%20also%20show%20their%20suitability%20to%20set%20the%0Acut-off%20point%20when%20applying%20layer%20pruning%20compression.%20Our%20approach%2C%20dubbed%0A%24LPASS%24%2C%20is%20applied%20in%20BERT%20and%20Gemma%20for%20the%20detection%20of%2012%20of%20MITRE%27s%20Top%2025%0Amost%20dangerous%20vulnerabilities%20on%20480k%20C/C%2B%2B%20samples.%20LPs%20can%20be%20computed%20in%0A142.97%20s.%20and%20provide%20key%20findings%3A%20%281%29%2033.3%20%5C%25%20and%2072.2%5C%25%20of%20layers%20can%20be%0Aremoved%2C%20respectively%2C%20with%20no%20precision%20loss%3B%20%282%29%20they%20provide%20an%20early%0Aestimate%20of%20the%20post-fine-tuning%20and%20post-compression%20model%20effectiveness%2C%20with%0A3%5C%25%20and%208.68%5C%25%20as%20the%20lowest%20and%20average%20precision%20errors%2C%20respectively.%0A%24LPASS%24-based%20LLMs%20outperform%20the%20state%20of%20the%20art%2C%20reaching%2086.9%5C%25%20of%20accuracy%0Ain%20multi-class%20vulnerability%20detection.%20Interestingly%2C%20%24LPASS%24-based%20compressed%0Aversions%20of%20Gemma%20outperform%20the%20original%20ones%20by%201.6%5C%25%20of%20F1-score%20at%20a%0Amaximum%20while%20saving%2029.4%20%5C%25%20and%2023.8%5C%25%20of%20training%20and%20inference%20time%20and%0A42.98%5C%25%20of%20model%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24451v1&entry.124074799=Read"},
{"title": "Can Classic GNNs Be Strong Baselines for Graph-level Tasks? Simple\n  Architectures Meet Excellence", "author": "Yuankai Luo and Lei Shi and Xiao-Ming Wu", "abstract": "  Message-passing Graph Neural Networks (GNNs) are often criticized for their\nlimited expressiveness, issues like over-smoothing and over-squashing, and\nchallenges in capturing long-range dependencies. Conversely, Graph Transformers\n(GTs) are regarded as superior due to their employment of global attention\nmechanisms, which potentially mitigate these challenges. Literature frequently\nsuggests that GTs outperform GNNs in graph-level tasks, especially for graph\nclassification and regression on small molecular graphs. In this study, we\nexplore the untapped potential of GNNs through an enhanced framework, GNN+,\nwhich integrates six widely used techniques: edge feature integration,\nnormalization, dropout, residual connections, feed-forward networks, and\npositional encoding, to effectively tackle graph-level tasks. We conduct a\nsystematic re-evaluation of three classic GNNs (GCN, GIN, and GatedGCN)\nenhanced by the GNN+ framework across 14 well-known graph-level datasets. Our\nresults reveal that, contrary to prevailing beliefs, these classic GNNs\nconsistently match or surpass the performance of GTs, securing top-three\nrankings across all datasets and achieving first place in eight. Furthermore,\nthey demonstrate greater efficiency, running several times faster than GTs on\nmany datasets. This highlights the potential of simple GNN architectures,\nchallenging the notion that complex mechanisms in GTs are essential for\nsuperior graph-level performance. Our source code is available at\nhttps://github.com/LUOyk1999/GNNPlus.\n", "link": "http://arxiv.org/abs/2502.09263v2", "date": "2025-05-30", "relevancy": 2.4605, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5246}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4764}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Classic%20GNNs%20Be%20Strong%20Baselines%20for%20Graph-level%20Tasks%3F%20Simple%0A%20%20Architectures%20Meet%20Excellence&body=Title%3A%20Can%20Classic%20GNNs%20Be%20Strong%20Baselines%20for%20Graph-level%20Tasks%3F%20Simple%0A%20%20Architectures%20Meet%20Excellence%0AAuthor%3A%20Yuankai%20Luo%20and%20Lei%20Shi%20and%20Xiao-Ming%20Wu%0AAbstract%3A%20%20%20Message-passing%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20often%20criticized%20for%20their%0Alimited%20expressiveness%2C%20issues%20like%20over-smoothing%20and%20over-squashing%2C%20and%0Achallenges%20in%20capturing%20long-range%20dependencies.%20Conversely%2C%20Graph%20Transformers%0A%28GTs%29%20are%20regarded%20as%20superior%20due%20to%20their%20employment%20of%20global%20attention%0Amechanisms%2C%20which%20potentially%20mitigate%20these%20challenges.%20Literature%20frequently%0Asuggests%20that%20GTs%20outperform%20GNNs%20in%20graph-level%20tasks%2C%20especially%20for%20graph%0Aclassification%20and%20regression%20on%20small%20molecular%20graphs.%20In%20this%20study%2C%20we%0Aexplore%20the%20untapped%20potential%20of%20GNNs%20through%20an%20enhanced%20framework%2C%20GNN%2B%2C%0Awhich%20integrates%20six%20widely%20used%20techniques%3A%20edge%20feature%20integration%2C%0Anormalization%2C%20dropout%2C%20residual%20connections%2C%20feed-forward%20networks%2C%20and%0Apositional%20encoding%2C%20to%20effectively%20tackle%20graph-level%20tasks.%20We%20conduct%20a%0Asystematic%20re-evaluation%20of%20three%20classic%20GNNs%20%28GCN%2C%20GIN%2C%20and%20GatedGCN%29%0Aenhanced%20by%20the%20GNN%2B%20framework%20across%2014%20well-known%20graph-level%20datasets.%20Our%0Aresults%20reveal%20that%2C%20contrary%20to%20prevailing%20beliefs%2C%20these%20classic%20GNNs%0Aconsistently%20match%20or%20surpass%20the%20performance%20of%20GTs%2C%20securing%20top-three%0Arankings%20across%20all%20datasets%20and%20achieving%20first%20place%20in%20eight.%20Furthermore%2C%0Athey%20demonstrate%20greater%20efficiency%2C%20running%20several%20times%20faster%20than%20GTs%20on%0Amany%20datasets.%20This%20highlights%20the%20potential%20of%20simple%20GNN%20architectures%2C%0Achallenging%20the%20notion%20that%20complex%20mechanisms%20in%20GTs%20are%20essential%20for%0Asuperior%20graph-level%20performance.%20Our%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/LUOyk1999/GNNPlus.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09263v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Classic%2520GNNs%2520Be%2520Strong%2520Baselines%2520for%2520Graph-level%2520Tasks%253F%2520Simple%250A%2520%2520Architectures%2520Meet%2520Excellence%26entry.906535625%3DYuankai%2520Luo%2520and%2520Lei%2520Shi%2520and%2520Xiao-Ming%2520Wu%26entry.1292438233%3D%2520%2520Message-passing%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520are%2520often%2520criticized%2520for%2520their%250Alimited%2520expressiveness%252C%2520issues%2520like%2520over-smoothing%2520and%2520over-squashing%252C%2520and%250Achallenges%2520in%2520capturing%2520long-range%2520dependencies.%2520Conversely%252C%2520Graph%2520Transformers%250A%2528GTs%2529%2520are%2520regarded%2520as%2520superior%2520due%2520to%2520their%2520employment%2520of%2520global%2520attention%250Amechanisms%252C%2520which%2520potentially%2520mitigate%2520these%2520challenges.%2520Literature%2520frequently%250Asuggests%2520that%2520GTs%2520outperform%2520GNNs%2520in%2520graph-level%2520tasks%252C%2520especially%2520for%2520graph%250Aclassification%2520and%2520regression%2520on%2520small%2520molecular%2520graphs.%2520In%2520this%2520study%252C%2520we%250Aexplore%2520the%2520untapped%2520potential%2520of%2520GNNs%2520through%2520an%2520enhanced%2520framework%252C%2520GNN%252B%252C%250Awhich%2520integrates%2520six%2520widely%2520used%2520techniques%253A%2520edge%2520feature%2520integration%252C%250Anormalization%252C%2520dropout%252C%2520residual%2520connections%252C%2520feed-forward%2520networks%252C%2520and%250Apositional%2520encoding%252C%2520to%2520effectively%2520tackle%2520graph-level%2520tasks.%2520We%2520conduct%2520a%250Asystematic%2520re-evaluation%2520of%2520three%2520classic%2520GNNs%2520%2528GCN%252C%2520GIN%252C%2520and%2520GatedGCN%2529%250Aenhanced%2520by%2520the%2520GNN%252B%2520framework%2520across%252014%2520well-known%2520graph-level%2520datasets.%2520Our%250Aresults%2520reveal%2520that%252C%2520contrary%2520to%2520prevailing%2520beliefs%252C%2520these%2520classic%2520GNNs%250Aconsistently%2520match%2520or%2520surpass%2520the%2520performance%2520of%2520GTs%252C%2520securing%2520top-three%250Arankings%2520across%2520all%2520datasets%2520and%2520achieving%2520first%2520place%2520in%2520eight.%2520Furthermore%252C%250Athey%2520demonstrate%2520greater%2520efficiency%252C%2520running%2520several%2520times%2520faster%2520than%2520GTs%2520on%250Amany%2520datasets.%2520This%2520highlights%2520the%2520potential%2520of%2520simple%2520GNN%2520architectures%252C%250Achallenging%2520the%2520notion%2520that%2520complex%2520mechanisms%2520in%2520GTs%2520are%2520essential%2520for%250Asuperior%2520graph-level%2520performance.%2520Our%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/LUOyk1999/GNNPlus.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09263v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Classic%20GNNs%20Be%20Strong%20Baselines%20for%20Graph-level%20Tasks%3F%20Simple%0A%20%20Architectures%20Meet%20Excellence&entry.906535625=Yuankai%20Luo%20and%20Lei%20Shi%20and%20Xiao-Ming%20Wu&entry.1292438233=%20%20Message-passing%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20often%20criticized%20for%20their%0Alimited%20expressiveness%2C%20issues%20like%20over-smoothing%20and%20over-squashing%2C%20and%0Achallenges%20in%20capturing%20long-range%20dependencies.%20Conversely%2C%20Graph%20Transformers%0A%28GTs%29%20are%20regarded%20as%20superior%20due%20to%20their%20employment%20of%20global%20attention%0Amechanisms%2C%20which%20potentially%20mitigate%20these%20challenges.%20Literature%20frequently%0Asuggests%20that%20GTs%20outperform%20GNNs%20in%20graph-level%20tasks%2C%20especially%20for%20graph%0Aclassification%20and%20regression%20on%20small%20molecular%20graphs.%20In%20this%20study%2C%20we%0Aexplore%20the%20untapped%20potential%20of%20GNNs%20through%20an%20enhanced%20framework%2C%20GNN%2B%2C%0Awhich%20integrates%20six%20widely%20used%20techniques%3A%20edge%20feature%20integration%2C%0Anormalization%2C%20dropout%2C%20residual%20connections%2C%20feed-forward%20networks%2C%20and%0Apositional%20encoding%2C%20to%20effectively%20tackle%20graph-level%20tasks.%20We%20conduct%20a%0Asystematic%20re-evaluation%20of%20three%20classic%20GNNs%20%28GCN%2C%20GIN%2C%20and%20GatedGCN%29%0Aenhanced%20by%20the%20GNN%2B%20framework%20across%2014%20well-known%20graph-level%20datasets.%20Our%0Aresults%20reveal%20that%2C%20contrary%20to%20prevailing%20beliefs%2C%20these%20classic%20GNNs%0Aconsistently%20match%20or%20surpass%20the%20performance%20of%20GTs%2C%20securing%20top-three%0Arankings%20across%20all%20datasets%20and%20achieving%20first%20place%20in%20eight.%20Furthermore%2C%0Athey%20demonstrate%20greater%20efficiency%2C%20running%20several%20times%20faster%20than%20GTs%20on%0Amany%20datasets.%20This%20highlights%20the%20potential%20of%20simple%20GNN%20architectures%2C%0Achallenging%20the%20notion%20that%20complex%20mechanisms%20in%20GTs%20are%20essential%20for%0Asuperior%20graph-level%20performance.%20Our%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/LUOyk1999/GNNPlus.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09263v2&entry.124074799=Read"},
{"title": "DexMachina: Functional Retargeting for Bimanual Dexterous Manipulation", "author": "Zhao Mandi and Yifan Hou and Dieter Fox and Yashraj Narang and Ajay Mandlekar and Shuran Song", "abstract": "  We study the problem of functional retargeting: learning dexterous\nmanipulation policies to track object states from human hand-object\ndemonstrations. We focus on long-horizon, bimanual tasks with articulated\nobjects, which is challenging due to large action space, spatiotemporal\ndiscontinuities, and embodiment gap between human and robot hands. We propose\nDexMachina, a novel curriculum-based algorithm: the key idea is to use virtual\nobject controllers with decaying strength: an object is first driven\nautomatically towards its target states, such that the policy can gradually\nlearn to take over under motion and contact guidance. We release a simulation\nbenchmark with a diverse set of tasks and dexterous hands, and show that\nDexMachina significantly outperforms baseline methods. Our algorithm and\nbenchmark enable a functional comparison for hardware designs, and we present\nkey findings informed by quantitative and qualitative results. With the recent\nsurge in dexterous hand development, we hope this work will provide a useful\nplatform for identifying desirable hardware capabilities and lower the barrier\nfor contributing to future research. Videos and more at\nhttps://project-dexmachina.github.io/\n", "link": "http://arxiv.org/abs/2505.24853v1", "date": "2025-05-30", "relevancy": 2.4577, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6331}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6067}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DexMachina%3A%20Functional%20Retargeting%20for%20Bimanual%20Dexterous%20Manipulation&body=Title%3A%20DexMachina%3A%20Functional%20Retargeting%20for%20Bimanual%20Dexterous%20Manipulation%0AAuthor%3A%20Zhao%20Mandi%20and%20Yifan%20Hou%20and%20Dieter%20Fox%20and%20Yashraj%20Narang%20and%20Ajay%20Mandlekar%20and%20Shuran%20Song%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20functional%20retargeting%3A%20learning%20dexterous%0Amanipulation%20policies%20to%20track%20object%20states%20from%20human%20hand-object%0Ademonstrations.%20We%20focus%20on%20long-horizon%2C%20bimanual%20tasks%20with%20articulated%0Aobjects%2C%20which%20is%20challenging%20due%20to%20large%20action%20space%2C%20spatiotemporal%0Adiscontinuities%2C%20and%20embodiment%20gap%20between%20human%20and%20robot%20hands.%20We%20propose%0ADexMachina%2C%20a%20novel%20curriculum-based%20algorithm%3A%20the%20key%20idea%20is%20to%20use%20virtual%0Aobject%20controllers%20with%20decaying%20strength%3A%20an%20object%20is%20first%20driven%0Aautomatically%20towards%20its%20target%20states%2C%20such%20that%20the%20policy%20can%20gradually%0Alearn%20to%20take%20over%20under%20motion%20and%20contact%20guidance.%20We%20release%20a%20simulation%0Abenchmark%20with%20a%20diverse%20set%20of%20tasks%20and%20dexterous%20hands%2C%20and%20show%20that%0ADexMachina%20significantly%20outperforms%20baseline%20methods.%20Our%20algorithm%20and%0Abenchmark%20enable%20a%20functional%20comparison%20for%20hardware%20designs%2C%20and%20we%20present%0Akey%20findings%20informed%20by%20quantitative%20and%20qualitative%20results.%20With%20the%20recent%0Asurge%20in%20dexterous%20hand%20development%2C%20we%20hope%20this%20work%20will%20provide%20a%20useful%0Aplatform%20for%20identifying%20desirable%20hardware%20capabilities%20and%20lower%20the%20barrier%0Afor%20contributing%20to%20future%20research.%20Videos%20and%20more%20at%0Ahttps%3A//project-dexmachina.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24853v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDexMachina%253A%2520Functional%2520Retargeting%2520for%2520Bimanual%2520Dexterous%2520Manipulation%26entry.906535625%3DZhao%2520Mandi%2520and%2520Yifan%2520Hou%2520and%2520Dieter%2520Fox%2520and%2520Yashraj%2520Narang%2520and%2520Ajay%2520Mandlekar%2520and%2520Shuran%2520Song%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520functional%2520retargeting%253A%2520learning%2520dexterous%250Amanipulation%2520policies%2520to%2520track%2520object%2520states%2520from%2520human%2520hand-object%250Ademonstrations.%2520We%2520focus%2520on%2520long-horizon%252C%2520bimanual%2520tasks%2520with%2520articulated%250Aobjects%252C%2520which%2520is%2520challenging%2520due%2520to%2520large%2520action%2520space%252C%2520spatiotemporal%250Adiscontinuities%252C%2520and%2520embodiment%2520gap%2520between%2520human%2520and%2520robot%2520hands.%2520We%2520propose%250ADexMachina%252C%2520a%2520novel%2520curriculum-based%2520algorithm%253A%2520the%2520key%2520idea%2520is%2520to%2520use%2520virtual%250Aobject%2520controllers%2520with%2520decaying%2520strength%253A%2520an%2520object%2520is%2520first%2520driven%250Aautomatically%2520towards%2520its%2520target%2520states%252C%2520such%2520that%2520the%2520policy%2520can%2520gradually%250Alearn%2520to%2520take%2520over%2520under%2520motion%2520and%2520contact%2520guidance.%2520We%2520release%2520a%2520simulation%250Abenchmark%2520with%2520a%2520diverse%2520set%2520of%2520tasks%2520and%2520dexterous%2520hands%252C%2520and%2520show%2520that%250ADexMachina%2520significantly%2520outperforms%2520baseline%2520methods.%2520Our%2520algorithm%2520and%250Abenchmark%2520enable%2520a%2520functional%2520comparison%2520for%2520hardware%2520designs%252C%2520and%2520we%2520present%250Akey%2520findings%2520informed%2520by%2520quantitative%2520and%2520qualitative%2520results.%2520With%2520the%2520recent%250Asurge%2520in%2520dexterous%2520hand%2520development%252C%2520we%2520hope%2520this%2520work%2520will%2520provide%2520a%2520useful%250Aplatform%2520for%2520identifying%2520desirable%2520hardware%2520capabilities%2520and%2520lower%2520the%2520barrier%250Afor%2520contributing%2520to%2520future%2520research.%2520Videos%2520and%2520more%2520at%250Ahttps%253A//project-dexmachina.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24853v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DexMachina%3A%20Functional%20Retargeting%20for%20Bimanual%20Dexterous%20Manipulation&entry.906535625=Zhao%20Mandi%20and%20Yifan%20Hou%20and%20Dieter%20Fox%20and%20Yashraj%20Narang%20and%20Ajay%20Mandlekar%20and%20Shuran%20Song&entry.1292438233=%20%20We%20study%20the%20problem%20of%20functional%20retargeting%3A%20learning%20dexterous%0Amanipulation%20policies%20to%20track%20object%20states%20from%20human%20hand-object%0Ademonstrations.%20We%20focus%20on%20long-horizon%2C%20bimanual%20tasks%20with%20articulated%0Aobjects%2C%20which%20is%20challenging%20due%20to%20large%20action%20space%2C%20spatiotemporal%0Adiscontinuities%2C%20and%20embodiment%20gap%20between%20human%20and%20robot%20hands.%20We%20propose%0ADexMachina%2C%20a%20novel%20curriculum-based%20algorithm%3A%20the%20key%20idea%20is%20to%20use%20virtual%0Aobject%20controllers%20with%20decaying%20strength%3A%20an%20object%20is%20first%20driven%0Aautomatically%20towards%20its%20target%20states%2C%20such%20that%20the%20policy%20can%20gradually%0Alearn%20to%20take%20over%20under%20motion%20and%20contact%20guidance.%20We%20release%20a%20simulation%0Abenchmark%20with%20a%20diverse%20set%20of%20tasks%20and%20dexterous%20hands%2C%20and%20show%20that%0ADexMachina%20significantly%20outperforms%20baseline%20methods.%20Our%20algorithm%20and%0Abenchmark%20enable%20a%20functional%20comparison%20for%20hardware%20designs%2C%20and%20we%20present%0Akey%20findings%20informed%20by%20quantitative%20and%20qualitative%20results.%20With%20the%20recent%0Asurge%20in%20dexterous%20hand%20development%2C%20we%20hope%20this%20work%20will%20provide%20a%20useful%0Aplatform%20for%20identifying%20desirable%20hardware%20capabilities%20and%20lower%20the%20barrier%0Afor%20contributing%20to%20future%20research.%20Videos%20and%20more%20at%0Ahttps%3A//project-dexmachina.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24853v1&entry.124074799=Read"},
{"title": "EL-AGHF: Extended Lagrangian Affine Geometric Heat Flow", "author": "Sangmin Kim and Hae-Won Park", "abstract": "  We propose a constrained Affine Geometric Heat Flow (AGHF) method that\nevolves so as to suppress the dynamics gaps associated with inadmissible\ncontrol directions. AGHF provides a unified framework applicable to a wide\nrange of motion planning problems, including both holonomic and non-holonomic\nsystems. However, to generate admissible trajectories, it requires assigning\ninfinite penalties to inadmissible control directions. This design choice,\nwhile theoretically valid, often leads to high computational cost or numerical\ninstability when the penalty becomes excessively large. To overcome this\nlimitation, we extend AGHF in an Augmented Lagrangian method approach by\nintroducing a dual trajectory related to dynamics gaps in inadmissible control\ndirections. This method solves the constrained variational problem as an\nextended parabolic partial differential equation defined over both the state\nand dual trajectorys, ensuring the admissibility of the resulting trajectory.\nWe demonstrate the effectiveness of our algorithm through simulation examples.\n", "link": "http://arxiv.org/abs/2505.24751v1", "date": "2025-05-30", "relevancy": 2.4572, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5159}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5044}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EL-AGHF%3A%20Extended%20Lagrangian%20Affine%20Geometric%20Heat%20Flow&body=Title%3A%20EL-AGHF%3A%20Extended%20Lagrangian%20Affine%20Geometric%20Heat%20Flow%0AAuthor%3A%20Sangmin%20Kim%20and%20Hae-Won%20Park%0AAbstract%3A%20%20%20We%20propose%20a%20constrained%20Affine%20Geometric%20Heat%20Flow%20%28AGHF%29%20method%20that%0Aevolves%20so%20as%20to%20suppress%20the%20dynamics%20gaps%20associated%20with%20inadmissible%0Acontrol%20directions.%20AGHF%20provides%20a%20unified%20framework%20applicable%20to%20a%20wide%0Arange%20of%20motion%20planning%20problems%2C%20including%20both%20holonomic%20and%20non-holonomic%0Asystems.%20However%2C%20to%20generate%20admissible%20trajectories%2C%20it%20requires%20assigning%0Ainfinite%20penalties%20to%20inadmissible%20control%20directions.%20This%20design%20choice%2C%0Awhile%20theoretically%20valid%2C%20often%20leads%20to%20high%20computational%20cost%20or%20numerical%0Ainstability%20when%20the%20penalty%20becomes%20excessively%20large.%20To%20overcome%20this%0Alimitation%2C%20we%20extend%20AGHF%20in%20an%20Augmented%20Lagrangian%20method%20approach%20by%0Aintroducing%20a%20dual%20trajectory%20related%20to%20dynamics%20gaps%20in%20inadmissible%20control%0Adirections.%20This%20method%20solves%20the%20constrained%20variational%20problem%20as%20an%0Aextended%20parabolic%20partial%20differential%20equation%20defined%20over%20both%20the%20state%0Aand%20dual%20trajectorys%2C%20ensuring%20the%20admissibility%20of%20the%20resulting%20trajectory.%0AWe%20demonstrate%20the%20effectiveness%20of%20our%20algorithm%20through%20simulation%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24751v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEL-AGHF%253A%2520Extended%2520Lagrangian%2520Affine%2520Geometric%2520Heat%2520Flow%26entry.906535625%3DSangmin%2520Kim%2520and%2520Hae-Won%2520Park%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520constrained%2520Affine%2520Geometric%2520Heat%2520Flow%2520%2528AGHF%2529%2520method%2520that%250Aevolves%2520so%2520as%2520to%2520suppress%2520the%2520dynamics%2520gaps%2520associated%2520with%2520inadmissible%250Acontrol%2520directions.%2520AGHF%2520provides%2520a%2520unified%2520framework%2520applicable%2520to%2520a%2520wide%250Arange%2520of%2520motion%2520planning%2520problems%252C%2520including%2520both%2520holonomic%2520and%2520non-holonomic%250Asystems.%2520However%252C%2520to%2520generate%2520admissible%2520trajectories%252C%2520it%2520requires%2520assigning%250Ainfinite%2520penalties%2520to%2520inadmissible%2520control%2520directions.%2520This%2520design%2520choice%252C%250Awhile%2520theoretically%2520valid%252C%2520often%2520leads%2520to%2520high%2520computational%2520cost%2520or%2520numerical%250Ainstability%2520when%2520the%2520penalty%2520becomes%2520excessively%2520large.%2520To%2520overcome%2520this%250Alimitation%252C%2520we%2520extend%2520AGHF%2520in%2520an%2520Augmented%2520Lagrangian%2520method%2520approach%2520by%250Aintroducing%2520a%2520dual%2520trajectory%2520related%2520to%2520dynamics%2520gaps%2520in%2520inadmissible%2520control%250Adirections.%2520This%2520method%2520solves%2520the%2520constrained%2520variational%2520problem%2520as%2520an%250Aextended%2520parabolic%2520partial%2520differential%2520equation%2520defined%2520over%2520both%2520the%2520state%250Aand%2520dual%2520trajectorys%252C%2520ensuring%2520the%2520admissibility%2520of%2520the%2520resulting%2520trajectory.%250AWe%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520algorithm%2520through%2520simulation%2520examples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24751v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EL-AGHF%3A%20Extended%20Lagrangian%20Affine%20Geometric%20Heat%20Flow&entry.906535625=Sangmin%20Kim%20and%20Hae-Won%20Park&entry.1292438233=%20%20We%20propose%20a%20constrained%20Affine%20Geometric%20Heat%20Flow%20%28AGHF%29%20method%20that%0Aevolves%20so%20as%20to%20suppress%20the%20dynamics%20gaps%20associated%20with%20inadmissible%0Acontrol%20directions.%20AGHF%20provides%20a%20unified%20framework%20applicable%20to%20a%20wide%0Arange%20of%20motion%20planning%20problems%2C%20including%20both%20holonomic%20and%20non-holonomic%0Asystems.%20However%2C%20to%20generate%20admissible%20trajectories%2C%20it%20requires%20assigning%0Ainfinite%20penalties%20to%20inadmissible%20control%20directions.%20This%20design%20choice%2C%0Awhile%20theoretically%20valid%2C%20often%20leads%20to%20high%20computational%20cost%20or%20numerical%0Ainstability%20when%20the%20penalty%20becomes%20excessively%20large.%20To%20overcome%20this%0Alimitation%2C%20we%20extend%20AGHF%20in%20an%20Augmented%20Lagrangian%20method%20approach%20by%0Aintroducing%20a%20dual%20trajectory%20related%20to%20dynamics%20gaps%20in%20inadmissible%20control%0Adirections.%20This%20method%20solves%20the%20constrained%20variational%20problem%20as%20an%0Aextended%20parabolic%20partial%20differential%20equation%20defined%20over%20both%20the%20state%0Aand%20dual%20trajectorys%2C%20ensuring%20the%20admissibility%20of%20the%20resulting%20trajectory.%0AWe%20demonstrate%20the%20effectiveness%20of%20our%20algorithm%20through%20simulation%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24751v1&entry.124074799=Read"},
{"title": "Towards Scalable Schema Mapping using Large Language Models", "author": "Christopher Buss and Mahdis Safari and Arash Termehchy and Stefan Lee and David Maier", "abstract": "  The growing need to integrate information from a large number of diverse\nsources poses significant scalability challenges for data integration systems.\nThese systems often rely on manually written schema mappings, which are\ncomplex, source-specific, and costly to maintain as sources evolve. While\nrecent advances suggest that large language models (LLMs) can assist in\nautomating schema matching by leveraging both structural and natural language\ncues, key challenges remain. In this paper, we identify three core issues with\nusing LLMs for schema mapping: (1) inconsistent outputs due to sensitivity to\ninput phrasing and structure, which we propose methods to address through\nsampling and aggregation techniques; (2) the need for more expressive mappings\n(e.g., GLaV), which strain the limited context windows of LLMs; and (3) the\ncomputational cost of repeated LLM calls, which we propose to mitigate through\nstrategies like data type prefiltering.\n", "link": "http://arxiv.org/abs/2505.24716v1", "date": "2025-05-30", "relevancy": 2.4551, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4988}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4988}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Scalable%20Schema%20Mapping%20using%20Large%20Language%20Models&body=Title%3A%20Towards%20Scalable%20Schema%20Mapping%20using%20Large%20Language%20Models%0AAuthor%3A%20Christopher%20Buss%20and%20Mahdis%20Safari%20and%20Arash%20Termehchy%20and%20Stefan%20Lee%20and%20David%20Maier%0AAbstract%3A%20%20%20The%20growing%20need%20to%20integrate%20information%20from%20a%20large%20number%20of%20diverse%0Asources%20poses%20significant%20scalability%20challenges%20for%20data%20integration%20systems.%0AThese%20systems%20often%20rely%20on%20manually%20written%20schema%20mappings%2C%20which%20are%0Acomplex%2C%20source-specific%2C%20and%20costly%20to%20maintain%20as%20sources%20evolve.%20While%0Arecent%20advances%20suggest%20that%20large%20language%20models%20%28LLMs%29%20can%20assist%20in%0Aautomating%20schema%20matching%20by%20leveraging%20both%20structural%20and%20natural%20language%0Acues%2C%20key%20challenges%20remain.%20In%20this%20paper%2C%20we%20identify%20three%20core%20issues%20with%0Ausing%20LLMs%20for%20schema%20mapping%3A%20%281%29%20inconsistent%20outputs%20due%20to%20sensitivity%20to%0Ainput%20phrasing%20and%20structure%2C%20which%20we%20propose%20methods%20to%20address%20through%0Asampling%20and%20aggregation%20techniques%3B%20%282%29%20the%20need%20for%20more%20expressive%20mappings%0A%28e.g.%2C%20GLaV%29%2C%20which%20strain%20the%20limited%20context%20windows%20of%20LLMs%3B%20and%20%283%29%20the%0Acomputational%20cost%20of%20repeated%20LLM%20calls%2C%20which%20we%20propose%20to%20mitigate%20through%0Astrategies%20like%20data%20type%20prefiltering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24716v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Scalable%2520Schema%2520Mapping%2520using%2520Large%2520Language%2520Models%26entry.906535625%3DChristopher%2520Buss%2520and%2520Mahdis%2520Safari%2520and%2520Arash%2520Termehchy%2520and%2520Stefan%2520Lee%2520and%2520David%2520Maier%26entry.1292438233%3D%2520%2520The%2520growing%2520need%2520to%2520integrate%2520information%2520from%2520a%2520large%2520number%2520of%2520diverse%250Asources%2520poses%2520significant%2520scalability%2520challenges%2520for%2520data%2520integration%2520systems.%250AThese%2520systems%2520often%2520rely%2520on%2520manually%2520written%2520schema%2520mappings%252C%2520which%2520are%250Acomplex%252C%2520source-specific%252C%2520and%2520costly%2520to%2520maintain%2520as%2520sources%2520evolve.%2520While%250Arecent%2520advances%2520suggest%2520that%2520large%2520language%2520models%2520%2528LLMs%2529%2520can%2520assist%2520in%250Aautomating%2520schema%2520matching%2520by%2520leveraging%2520both%2520structural%2520and%2520natural%2520language%250Acues%252C%2520key%2520challenges%2520remain.%2520In%2520this%2520paper%252C%2520we%2520identify%2520three%2520core%2520issues%2520with%250Ausing%2520LLMs%2520for%2520schema%2520mapping%253A%2520%25281%2529%2520inconsistent%2520outputs%2520due%2520to%2520sensitivity%2520to%250Ainput%2520phrasing%2520and%2520structure%252C%2520which%2520we%2520propose%2520methods%2520to%2520address%2520through%250Asampling%2520and%2520aggregation%2520techniques%253B%2520%25282%2529%2520the%2520need%2520for%2520more%2520expressive%2520mappings%250A%2528e.g.%252C%2520GLaV%2529%252C%2520which%2520strain%2520the%2520limited%2520context%2520windows%2520of%2520LLMs%253B%2520and%2520%25283%2529%2520the%250Acomputational%2520cost%2520of%2520repeated%2520LLM%2520calls%252C%2520which%2520we%2520propose%2520to%2520mitigate%2520through%250Astrategies%2520like%2520data%2520type%2520prefiltering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24716v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Scalable%20Schema%20Mapping%20using%20Large%20Language%20Models&entry.906535625=Christopher%20Buss%20and%20Mahdis%20Safari%20and%20Arash%20Termehchy%20and%20Stefan%20Lee%20and%20David%20Maier&entry.1292438233=%20%20The%20growing%20need%20to%20integrate%20information%20from%20a%20large%20number%20of%20diverse%0Asources%20poses%20significant%20scalability%20challenges%20for%20data%20integration%20systems.%0AThese%20systems%20often%20rely%20on%20manually%20written%20schema%20mappings%2C%20which%20are%0Acomplex%2C%20source-specific%2C%20and%20costly%20to%20maintain%20as%20sources%20evolve.%20While%0Arecent%20advances%20suggest%20that%20large%20language%20models%20%28LLMs%29%20can%20assist%20in%0Aautomating%20schema%20matching%20by%20leveraging%20both%20structural%20and%20natural%20language%0Acues%2C%20key%20challenges%20remain.%20In%20this%20paper%2C%20we%20identify%20three%20core%20issues%20with%0Ausing%20LLMs%20for%20schema%20mapping%3A%20%281%29%20inconsistent%20outputs%20due%20to%20sensitivity%20to%0Ainput%20phrasing%20and%20structure%2C%20which%20we%20propose%20methods%20to%20address%20through%0Asampling%20and%20aggregation%20techniques%3B%20%282%29%20the%20need%20for%20more%20expressive%20mappings%0A%28e.g.%2C%20GLaV%29%2C%20which%20strain%20the%20limited%20context%20windows%20of%20LLMs%3B%20and%20%283%29%20the%0Acomputational%20cost%20of%20repeated%20LLM%20calls%2C%20which%20we%20propose%20to%20mitigate%20through%0Astrategies%20like%20data%20type%20prefiltering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24716v1&entry.124074799=Read"},
{"title": "Knockoff-Guided Compressive Sensing: A Statistical Machine Learning\n  Framework for Support-Assured Signal Recovery", "author": "Xiaochen Zhang and Haoyi Xiong", "abstract": "  This paper introduces a novel Knockoff-guided compressive sensing framework,\nreferred to as \\TheName{}, which enhances signal recovery by leveraging precise\nfalse discovery rate (FDR) control during the support identification phase.\nUnlike LASSO, which jointly performs support selection and signal estimation\nwithout explicit error control, our method guarantees FDR control in finite\nsamples, enabling more reliable identification of the true signal support. By\nseparating and controlling the support recovery process through statistical\nKnockoff filters, our framework achieves more accurate signal reconstruction,\nespecially in challenging scenarios where traditional methods fail. We\nestablish theoretical guarantees demonstrating how FDR control directly ensures\nrecovery performance under weaker conditions than traditional $\\ell_1$-based\ncompressive sensing methods, while maintaining accurate signal reconstruction.\nExtensive numerical experiments demonstrate that our proposed Knockoff-based\nmethod consistently outperforms LASSO-based and other state-of-the-art\ncompressive sensing techniques. In simulation studies, our method improves\nF1-score by up to 3.9x over baseline methods, attributed to principled false\ndiscovery rate (FDR) control and enhanced support recovery. The method also\nconsistently yields lower reconstruction and relative errors. We further\nvalidate the framework on real-world datasets, where it achieves top downstream\npredictive performance across both regression and classification tasks, often\nnarrowing or even surpassing the performance gap relative to uncompressed\nsignals. These results establish \\TheName{} as a robust and practical\nalternative to existing approaches, offering both theoretical guarantees and\nstrong empirical performance through statistically grounded support selection.\n", "link": "http://arxiv.org/abs/2505.24727v1", "date": "2025-05-30", "relevancy": 2.4484, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4958}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4887}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knockoff-Guided%20Compressive%20Sensing%3A%20A%20Statistical%20Machine%20Learning%0A%20%20Framework%20for%20Support-Assured%20Signal%20Recovery&body=Title%3A%20Knockoff-Guided%20Compressive%20Sensing%3A%20A%20Statistical%20Machine%20Learning%0A%20%20Framework%20for%20Support-Assured%20Signal%20Recovery%0AAuthor%3A%20Xiaochen%20Zhang%20and%20Haoyi%20Xiong%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20Knockoff-guided%20compressive%20sensing%20framework%2C%0Areferred%20to%20as%20%5CTheName%7B%7D%2C%20which%20enhances%20signal%20recovery%20by%20leveraging%20precise%0Afalse%20discovery%20rate%20%28FDR%29%20control%20during%20the%20support%20identification%20phase.%0AUnlike%20LASSO%2C%20which%20jointly%20performs%20support%20selection%20and%20signal%20estimation%0Awithout%20explicit%20error%20control%2C%20our%20method%20guarantees%20FDR%20control%20in%20finite%0Asamples%2C%20enabling%20more%20reliable%20identification%20of%20the%20true%20signal%20support.%20By%0Aseparating%20and%20controlling%20the%20support%20recovery%20process%20through%20statistical%0AKnockoff%20filters%2C%20our%20framework%20achieves%20more%20accurate%20signal%20reconstruction%2C%0Aespecially%20in%20challenging%20scenarios%20where%20traditional%20methods%20fail.%20We%0Aestablish%20theoretical%20guarantees%20demonstrating%20how%20FDR%20control%20directly%20ensures%0Arecovery%20performance%20under%20weaker%20conditions%20than%20traditional%20%24%5Cell_1%24-based%0Acompressive%20sensing%20methods%2C%20while%20maintaining%20accurate%20signal%20reconstruction.%0AExtensive%20numerical%20experiments%20demonstrate%20that%20our%20proposed%20Knockoff-based%0Amethod%20consistently%20outperforms%20LASSO-based%20and%20other%20state-of-the-art%0Acompressive%20sensing%20techniques.%20In%20simulation%20studies%2C%20our%20method%20improves%0AF1-score%20by%20up%20to%203.9x%20over%20baseline%20methods%2C%20attributed%20to%20principled%20false%0Adiscovery%20rate%20%28FDR%29%20control%20and%20enhanced%20support%20recovery.%20The%20method%20also%0Aconsistently%20yields%20lower%20reconstruction%20and%20relative%20errors.%20We%20further%0Avalidate%20the%20framework%20on%20real-world%20datasets%2C%20where%20it%20achieves%20top%20downstream%0Apredictive%20performance%20across%20both%20regression%20and%20classification%20tasks%2C%20often%0Anarrowing%20or%20even%20surpassing%20the%20performance%20gap%20relative%20to%20uncompressed%0Asignals.%20These%20results%20establish%20%5CTheName%7B%7D%20as%20a%20robust%20and%20practical%0Aalternative%20to%20existing%20approaches%2C%20offering%20both%20theoretical%20guarantees%20and%0Astrong%20empirical%20performance%20through%20statistically%20grounded%20support%20selection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24727v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnockoff-Guided%2520Compressive%2520Sensing%253A%2520A%2520Statistical%2520Machine%2520Learning%250A%2520%2520Framework%2520for%2520Support-Assured%2520Signal%2520Recovery%26entry.906535625%3DXiaochen%2520Zhang%2520and%2520Haoyi%2520Xiong%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520Knockoff-guided%2520compressive%2520sensing%2520framework%252C%250Areferred%2520to%2520as%2520%255CTheName%257B%257D%252C%2520which%2520enhances%2520signal%2520recovery%2520by%2520leveraging%2520precise%250Afalse%2520discovery%2520rate%2520%2528FDR%2529%2520control%2520during%2520the%2520support%2520identification%2520phase.%250AUnlike%2520LASSO%252C%2520which%2520jointly%2520performs%2520support%2520selection%2520and%2520signal%2520estimation%250Awithout%2520explicit%2520error%2520control%252C%2520our%2520method%2520guarantees%2520FDR%2520control%2520in%2520finite%250Asamples%252C%2520enabling%2520more%2520reliable%2520identification%2520of%2520the%2520true%2520signal%2520support.%2520By%250Aseparating%2520and%2520controlling%2520the%2520support%2520recovery%2520process%2520through%2520statistical%250AKnockoff%2520filters%252C%2520our%2520framework%2520achieves%2520more%2520accurate%2520signal%2520reconstruction%252C%250Aespecially%2520in%2520challenging%2520scenarios%2520where%2520traditional%2520methods%2520fail.%2520We%250Aestablish%2520theoretical%2520guarantees%2520demonstrating%2520how%2520FDR%2520control%2520directly%2520ensures%250Arecovery%2520performance%2520under%2520weaker%2520conditions%2520than%2520traditional%2520%2524%255Cell_1%2524-based%250Acompressive%2520sensing%2520methods%252C%2520while%2520maintaining%2520accurate%2520signal%2520reconstruction.%250AExtensive%2520numerical%2520experiments%2520demonstrate%2520that%2520our%2520proposed%2520Knockoff-based%250Amethod%2520consistently%2520outperforms%2520LASSO-based%2520and%2520other%2520state-of-the-art%250Acompressive%2520sensing%2520techniques.%2520In%2520simulation%2520studies%252C%2520our%2520method%2520improves%250AF1-score%2520by%2520up%2520to%25203.9x%2520over%2520baseline%2520methods%252C%2520attributed%2520to%2520principled%2520false%250Adiscovery%2520rate%2520%2528FDR%2529%2520control%2520and%2520enhanced%2520support%2520recovery.%2520The%2520method%2520also%250Aconsistently%2520yields%2520lower%2520reconstruction%2520and%2520relative%2520errors.%2520We%2520further%250Avalidate%2520the%2520framework%2520on%2520real-world%2520datasets%252C%2520where%2520it%2520achieves%2520top%2520downstream%250Apredictive%2520performance%2520across%2520both%2520regression%2520and%2520classification%2520tasks%252C%2520often%250Anarrowing%2520or%2520even%2520surpassing%2520the%2520performance%2520gap%2520relative%2520to%2520uncompressed%250Asignals.%2520These%2520results%2520establish%2520%255CTheName%257B%257D%2520as%2520a%2520robust%2520and%2520practical%250Aalternative%2520to%2520existing%2520approaches%252C%2520offering%2520both%2520theoretical%2520guarantees%2520and%250Astrong%2520empirical%2520performance%2520through%2520statistically%2520grounded%2520support%2520selection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24727v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knockoff-Guided%20Compressive%20Sensing%3A%20A%20Statistical%20Machine%20Learning%0A%20%20Framework%20for%20Support-Assured%20Signal%20Recovery&entry.906535625=Xiaochen%20Zhang%20and%20Haoyi%20Xiong&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20Knockoff-guided%20compressive%20sensing%20framework%2C%0Areferred%20to%20as%20%5CTheName%7B%7D%2C%20which%20enhances%20signal%20recovery%20by%20leveraging%20precise%0Afalse%20discovery%20rate%20%28FDR%29%20control%20during%20the%20support%20identification%20phase.%0AUnlike%20LASSO%2C%20which%20jointly%20performs%20support%20selection%20and%20signal%20estimation%0Awithout%20explicit%20error%20control%2C%20our%20method%20guarantees%20FDR%20control%20in%20finite%0Asamples%2C%20enabling%20more%20reliable%20identification%20of%20the%20true%20signal%20support.%20By%0Aseparating%20and%20controlling%20the%20support%20recovery%20process%20through%20statistical%0AKnockoff%20filters%2C%20our%20framework%20achieves%20more%20accurate%20signal%20reconstruction%2C%0Aespecially%20in%20challenging%20scenarios%20where%20traditional%20methods%20fail.%20We%0Aestablish%20theoretical%20guarantees%20demonstrating%20how%20FDR%20control%20directly%20ensures%0Arecovery%20performance%20under%20weaker%20conditions%20than%20traditional%20%24%5Cell_1%24-based%0Acompressive%20sensing%20methods%2C%20while%20maintaining%20accurate%20signal%20reconstruction.%0AExtensive%20numerical%20experiments%20demonstrate%20that%20our%20proposed%20Knockoff-based%0Amethod%20consistently%20outperforms%20LASSO-based%20and%20other%20state-of-the-art%0Acompressive%20sensing%20techniques.%20In%20simulation%20studies%2C%20our%20method%20improves%0AF1-score%20by%20up%20to%203.9x%20over%20baseline%20methods%2C%20attributed%20to%20principled%20false%0Adiscovery%20rate%20%28FDR%29%20control%20and%20enhanced%20support%20recovery.%20The%20method%20also%0Aconsistently%20yields%20lower%20reconstruction%20and%20relative%20errors.%20We%20further%0Avalidate%20the%20framework%20on%20real-world%20datasets%2C%20where%20it%20achieves%20top%20downstream%0Apredictive%20performance%20across%20both%20regression%20and%20classification%20tasks%2C%20often%0Anarrowing%20or%20even%20surpassing%20the%20performance%20gap%20relative%20to%20uncompressed%0Asignals.%20These%20results%20establish%20%5CTheName%7B%7D%20as%20a%20robust%20and%20practical%0Aalternative%20to%20existing%20approaches%2C%20offering%20both%20theoretical%20guarantees%20and%0Astrong%20empirical%20performance%20through%20statistically%20grounded%20support%20selection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24727v1&entry.124074799=Read"},
{"title": "Post-hoc Probabilistic Vision-Language Models", "author": "Anton Baumann and Rui Li and Marcus Klasson and Santeri Mentu and Shyamgopal Karthik and Zeynep Akata and Arno Solin and Martin Trapp", "abstract": "  Vision-language models (VLMs), such as CLIP and SigLIP, have found remarkable\nsuccess in classification, retrieval, and generative tasks. For this, VLMs\ndeterministically map images and text descriptions to a joint latent space in\nwhich their similarity is assessed using the cosine similarity. However, a\ndeterministic mapping of inputs fails to capture uncertainties over concepts\narising from domain shifts when used in downstream tasks. In this work, we\npropose post-hoc uncertainty estimation in VLMs that does not require\nadditional training. Our method leverages a Bayesian posterior approximation\nover the last layers in VLMs and analytically quantifies uncertainties over\ncosine similarities. We demonstrate its effectiveness for uncertainty\nquantification and support set selection in active learning. Compared to\nbaselines, we obtain improved and well-calibrated predictive uncertainties,\ninterpretable uncertainty estimates, and sample-efficient active learning. Our\nresults show promise for safety-critical applications of large-scale models.\n", "link": "http://arxiv.org/abs/2412.06014v3", "date": "2025-05-30", "relevancy": 2.4445, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6238}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.6061}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5922}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Post-hoc%20Probabilistic%20Vision-Language%20Models&body=Title%3A%20Post-hoc%20Probabilistic%20Vision-Language%20Models%0AAuthor%3A%20Anton%20Baumann%20and%20Rui%20Li%20and%20Marcus%20Klasson%20and%20Santeri%20Mentu%20and%20Shyamgopal%20Karthik%20and%20Zeynep%20Akata%20and%20Arno%20Solin%20and%20Martin%20Trapp%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%2C%20such%20as%20CLIP%20and%20SigLIP%2C%20have%20found%20remarkable%0Asuccess%20in%20classification%2C%20retrieval%2C%20and%20generative%20tasks.%20For%20this%2C%20VLMs%0Adeterministically%20map%20images%20and%20text%20descriptions%20to%20a%20joint%20latent%20space%20in%0Awhich%20their%20similarity%20is%20assessed%20using%20the%20cosine%20similarity.%20However%2C%20a%0Adeterministic%20mapping%20of%20inputs%20fails%20to%20capture%20uncertainties%20over%20concepts%0Aarising%20from%20domain%20shifts%20when%20used%20in%20downstream%20tasks.%20In%20this%20work%2C%20we%0Apropose%20post-hoc%20uncertainty%20estimation%20in%20VLMs%20that%20does%20not%20require%0Aadditional%20training.%20Our%20method%20leverages%20a%20Bayesian%20posterior%20approximation%0Aover%20the%20last%20layers%20in%20VLMs%20and%20analytically%20quantifies%20uncertainties%20over%0Acosine%20similarities.%20We%20demonstrate%20its%20effectiveness%20for%20uncertainty%0Aquantification%20and%20support%20set%20selection%20in%20active%20learning.%20Compared%20to%0Abaselines%2C%20we%20obtain%20improved%20and%20well-calibrated%20predictive%20uncertainties%2C%0Ainterpretable%20uncertainty%20estimates%2C%20and%20sample-efficient%20active%20learning.%20Our%0Aresults%20show%20promise%20for%20safety-critical%20applications%20of%20large-scale%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.06014v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPost-hoc%2520Probabilistic%2520Vision-Language%2520Models%26entry.906535625%3DAnton%2520Baumann%2520and%2520Rui%2520Li%2520and%2520Marcus%2520Klasson%2520and%2520Santeri%2520Mentu%2520and%2520Shyamgopal%2520Karthik%2520and%2520Zeynep%2520Akata%2520and%2520Arno%2520Solin%2520and%2520Martin%2520Trapp%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%252C%2520such%2520as%2520CLIP%2520and%2520SigLIP%252C%2520have%2520found%2520remarkable%250Asuccess%2520in%2520classification%252C%2520retrieval%252C%2520and%2520generative%2520tasks.%2520For%2520this%252C%2520VLMs%250Adeterministically%2520map%2520images%2520and%2520text%2520descriptions%2520to%2520a%2520joint%2520latent%2520space%2520in%250Awhich%2520their%2520similarity%2520is%2520assessed%2520using%2520the%2520cosine%2520similarity.%2520However%252C%2520a%250Adeterministic%2520mapping%2520of%2520inputs%2520fails%2520to%2520capture%2520uncertainties%2520over%2520concepts%250Aarising%2520from%2520domain%2520shifts%2520when%2520used%2520in%2520downstream%2520tasks.%2520In%2520this%2520work%252C%2520we%250Apropose%2520post-hoc%2520uncertainty%2520estimation%2520in%2520VLMs%2520that%2520does%2520not%2520require%250Aadditional%2520training.%2520Our%2520method%2520leverages%2520a%2520Bayesian%2520posterior%2520approximation%250Aover%2520the%2520last%2520layers%2520in%2520VLMs%2520and%2520analytically%2520quantifies%2520uncertainties%2520over%250Acosine%2520similarities.%2520We%2520demonstrate%2520its%2520effectiveness%2520for%2520uncertainty%250Aquantification%2520and%2520support%2520set%2520selection%2520in%2520active%2520learning.%2520Compared%2520to%250Abaselines%252C%2520we%2520obtain%2520improved%2520and%2520well-calibrated%2520predictive%2520uncertainties%252C%250Ainterpretable%2520uncertainty%2520estimates%252C%2520and%2520sample-efficient%2520active%2520learning.%2520Our%250Aresults%2520show%2520promise%2520for%2520safety-critical%2520applications%2520of%2520large-scale%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.06014v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Post-hoc%20Probabilistic%20Vision-Language%20Models&entry.906535625=Anton%20Baumann%20and%20Rui%20Li%20and%20Marcus%20Klasson%20and%20Santeri%20Mentu%20and%20Shyamgopal%20Karthik%20and%20Zeynep%20Akata%20and%20Arno%20Solin%20and%20Martin%20Trapp&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%2C%20such%20as%20CLIP%20and%20SigLIP%2C%20have%20found%20remarkable%0Asuccess%20in%20classification%2C%20retrieval%2C%20and%20generative%20tasks.%20For%20this%2C%20VLMs%0Adeterministically%20map%20images%20and%20text%20descriptions%20to%20a%20joint%20latent%20space%20in%0Awhich%20their%20similarity%20is%20assessed%20using%20the%20cosine%20similarity.%20However%2C%20a%0Adeterministic%20mapping%20of%20inputs%20fails%20to%20capture%20uncertainties%20over%20concepts%0Aarising%20from%20domain%20shifts%20when%20used%20in%20downstream%20tasks.%20In%20this%20work%2C%20we%0Apropose%20post-hoc%20uncertainty%20estimation%20in%20VLMs%20that%20does%20not%20require%0Aadditional%20training.%20Our%20method%20leverages%20a%20Bayesian%20posterior%20approximation%0Aover%20the%20last%20layers%20in%20VLMs%20and%20analytically%20quantifies%20uncertainties%20over%0Acosine%20similarities.%20We%20demonstrate%20its%20effectiveness%20for%20uncertainty%0Aquantification%20and%20support%20set%20selection%20in%20active%20learning.%20Compared%20to%0Abaselines%2C%20we%20obtain%20improved%20and%20well-calibrated%20predictive%20uncertainties%2C%0Ainterpretable%20uncertainty%20estimates%2C%20and%20sample-efficient%20active%20learning.%20Our%0Aresults%20show%20promise%20for%20safety-critical%20applications%20of%20large-scale%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.06014v3&entry.124074799=Read"},
{"title": "LoBAM: LoRA-Based Backdoor Attack on Model Merging", "author": "Ming Yin and Jingyang Zhang and Jingwei Sun and Minghong Fang and Hai Li and Yiran Chen", "abstract": "  Model merging is an emerging technique that integrates multiple models\nfine-tuned on different tasks to create a versatile model that excels in\nmultiple domains. This scheme, in the meantime, may open up backdoor attack\nopportunities where one single malicious model can jeopardize the integrity of\nthe merged model. Existing works try to demonstrate the risk of such attacks by\nassuming substantial computational resources, focusing on cases where the\nattacker can fully fine-tune the pre-trained model. Such an assumption,\nhowever, may not be feasible given the increasing size of machine learning\nmodels. In practice where resources are limited and the attacker can only\nemploy techniques like Low-Rank Adaptation (LoRA) to produce the malicious\nmodel, it remains unclear whether the attack can still work and pose threats.\nIn this work, we first identify that the attack efficacy is significantly\ndiminished when using LoRA for fine-tuning. Then, we propose LoBAM, a method\nthat yields high attack success rate with minimal training resources. The key\nidea of LoBAM is to amplify the malicious weights in an intelligent way that\neffectively enhances the attack efficacy. We demonstrate that our design can\nlead to improved attack success rate through extensive empirical experiments\nacross various model merging scenarios. Moreover, we show that our method is\nhighly stealthy and is difficult to detect and defend against.\n", "link": "http://arxiv.org/abs/2411.16746v4", "date": "2025-05-30", "relevancy": 2.4372, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5098}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4797}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoBAM%3A%20LoRA-Based%20Backdoor%20Attack%20on%20Model%20Merging&body=Title%3A%20LoBAM%3A%20LoRA-Based%20Backdoor%20Attack%20on%20Model%20Merging%0AAuthor%3A%20Ming%20Yin%20and%20Jingyang%20Zhang%20and%20Jingwei%20Sun%20and%20Minghong%20Fang%20and%20Hai%20Li%20and%20Yiran%20Chen%0AAbstract%3A%20%20%20Model%20merging%20is%20an%20emerging%20technique%20that%20integrates%20multiple%20models%0Afine-tuned%20on%20different%20tasks%20to%20create%20a%20versatile%20model%20that%20excels%20in%0Amultiple%20domains.%20This%20scheme%2C%20in%20the%20meantime%2C%20may%20open%20up%20backdoor%20attack%0Aopportunities%20where%20one%20single%20malicious%20model%20can%20jeopardize%20the%20integrity%20of%0Athe%20merged%20model.%20Existing%20works%20try%20to%20demonstrate%20the%20risk%20of%20such%20attacks%20by%0Aassuming%20substantial%20computational%20resources%2C%20focusing%20on%20cases%20where%20the%0Aattacker%20can%20fully%20fine-tune%20the%20pre-trained%20model.%20Such%20an%20assumption%2C%0Ahowever%2C%20may%20not%20be%20feasible%20given%20the%20increasing%20size%20of%20machine%20learning%0Amodels.%20In%20practice%20where%20resources%20are%20limited%20and%20the%20attacker%20can%20only%0Aemploy%20techniques%20like%20Low-Rank%20Adaptation%20%28LoRA%29%20to%20produce%20the%20malicious%0Amodel%2C%20it%20remains%20unclear%20whether%20the%20attack%20can%20still%20work%20and%20pose%20threats.%0AIn%20this%20work%2C%20we%20first%20identify%20that%20the%20attack%20efficacy%20is%20significantly%0Adiminished%20when%20using%20LoRA%20for%20fine-tuning.%20Then%2C%20we%20propose%20LoBAM%2C%20a%20method%0Athat%20yields%20high%20attack%20success%20rate%20with%20minimal%20training%20resources.%20The%20key%0Aidea%20of%20LoBAM%20is%20to%20amplify%20the%20malicious%20weights%20in%20an%20intelligent%20way%20that%0Aeffectively%20enhances%20the%20attack%20efficacy.%20We%20demonstrate%20that%20our%20design%20can%0Alead%20to%20improved%20attack%20success%20rate%20through%20extensive%20empirical%20experiments%0Aacross%20various%20model%20merging%20scenarios.%20Moreover%2C%20we%20show%20that%20our%20method%20is%0Ahighly%20stealthy%20and%20is%20difficult%20to%20detect%20and%20defend%20against.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16746v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoBAM%253A%2520LoRA-Based%2520Backdoor%2520Attack%2520on%2520Model%2520Merging%26entry.906535625%3DMing%2520Yin%2520and%2520Jingyang%2520Zhang%2520and%2520Jingwei%2520Sun%2520and%2520Minghong%2520Fang%2520and%2520Hai%2520Li%2520and%2520Yiran%2520Chen%26entry.1292438233%3D%2520%2520Model%2520merging%2520is%2520an%2520emerging%2520technique%2520that%2520integrates%2520multiple%2520models%250Afine-tuned%2520on%2520different%2520tasks%2520to%2520create%2520a%2520versatile%2520model%2520that%2520excels%2520in%250Amultiple%2520domains.%2520This%2520scheme%252C%2520in%2520the%2520meantime%252C%2520may%2520open%2520up%2520backdoor%2520attack%250Aopportunities%2520where%2520one%2520single%2520malicious%2520model%2520can%2520jeopardize%2520the%2520integrity%2520of%250Athe%2520merged%2520model.%2520Existing%2520works%2520try%2520to%2520demonstrate%2520the%2520risk%2520of%2520such%2520attacks%2520by%250Aassuming%2520substantial%2520computational%2520resources%252C%2520focusing%2520on%2520cases%2520where%2520the%250Aattacker%2520can%2520fully%2520fine-tune%2520the%2520pre-trained%2520model.%2520Such%2520an%2520assumption%252C%250Ahowever%252C%2520may%2520not%2520be%2520feasible%2520given%2520the%2520increasing%2520size%2520of%2520machine%2520learning%250Amodels.%2520In%2520practice%2520where%2520resources%2520are%2520limited%2520and%2520the%2520attacker%2520can%2520only%250Aemploy%2520techniques%2520like%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520to%2520produce%2520the%2520malicious%250Amodel%252C%2520it%2520remains%2520unclear%2520whether%2520the%2520attack%2520can%2520still%2520work%2520and%2520pose%2520threats.%250AIn%2520this%2520work%252C%2520we%2520first%2520identify%2520that%2520the%2520attack%2520efficacy%2520is%2520significantly%250Adiminished%2520when%2520using%2520LoRA%2520for%2520fine-tuning.%2520Then%252C%2520we%2520propose%2520LoBAM%252C%2520a%2520method%250Athat%2520yields%2520high%2520attack%2520success%2520rate%2520with%2520minimal%2520training%2520resources.%2520The%2520key%250Aidea%2520of%2520LoBAM%2520is%2520to%2520amplify%2520the%2520malicious%2520weights%2520in%2520an%2520intelligent%2520way%2520that%250Aeffectively%2520enhances%2520the%2520attack%2520efficacy.%2520We%2520demonstrate%2520that%2520our%2520design%2520can%250Alead%2520to%2520improved%2520attack%2520success%2520rate%2520through%2520extensive%2520empirical%2520experiments%250Aacross%2520various%2520model%2520merging%2520scenarios.%2520Moreover%252C%2520we%2520show%2520that%2520our%2520method%2520is%250Ahighly%2520stealthy%2520and%2520is%2520difficult%2520to%2520detect%2520and%2520defend%2520against.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16746v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoBAM%3A%20LoRA-Based%20Backdoor%20Attack%20on%20Model%20Merging&entry.906535625=Ming%20Yin%20and%20Jingyang%20Zhang%20and%20Jingwei%20Sun%20and%20Minghong%20Fang%20and%20Hai%20Li%20and%20Yiran%20Chen&entry.1292438233=%20%20Model%20merging%20is%20an%20emerging%20technique%20that%20integrates%20multiple%20models%0Afine-tuned%20on%20different%20tasks%20to%20create%20a%20versatile%20model%20that%20excels%20in%0Amultiple%20domains.%20This%20scheme%2C%20in%20the%20meantime%2C%20may%20open%20up%20backdoor%20attack%0Aopportunities%20where%20one%20single%20malicious%20model%20can%20jeopardize%20the%20integrity%20of%0Athe%20merged%20model.%20Existing%20works%20try%20to%20demonstrate%20the%20risk%20of%20such%20attacks%20by%0Aassuming%20substantial%20computational%20resources%2C%20focusing%20on%20cases%20where%20the%0Aattacker%20can%20fully%20fine-tune%20the%20pre-trained%20model.%20Such%20an%20assumption%2C%0Ahowever%2C%20may%20not%20be%20feasible%20given%20the%20increasing%20size%20of%20machine%20learning%0Amodels.%20In%20practice%20where%20resources%20are%20limited%20and%20the%20attacker%20can%20only%0Aemploy%20techniques%20like%20Low-Rank%20Adaptation%20%28LoRA%29%20to%20produce%20the%20malicious%0Amodel%2C%20it%20remains%20unclear%20whether%20the%20attack%20can%20still%20work%20and%20pose%20threats.%0AIn%20this%20work%2C%20we%20first%20identify%20that%20the%20attack%20efficacy%20is%20significantly%0Adiminished%20when%20using%20LoRA%20for%20fine-tuning.%20Then%2C%20we%20propose%20LoBAM%2C%20a%20method%0Athat%20yields%20high%20attack%20success%20rate%20with%20minimal%20training%20resources.%20The%20key%0Aidea%20of%20LoBAM%20is%20to%20amplify%20the%20malicious%20weights%20in%20an%20intelligent%20way%20that%0Aeffectively%20enhances%20the%20attack%20efficacy.%20We%20demonstrate%20that%20our%20design%20can%0Alead%20to%20improved%20attack%20success%20rate%20through%20extensive%20empirical%20experiments%0Aacross%20various%20model%20merging%20scenarios.%20Moreover%2C%20we%20show%20that%20our%20method%20is%0Ahighly%20stealthy%20and%20is%20difficult%20to%20detect%20and%20defend%20against.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16746v4&entry.124074799=Read"},
{"title": "Time Blindness: Why Video-Language Models Can't See What Humans Can?", "author": "Ujjwal Upadhyay and Mukul Ranjan and Zhiqiang Shen and Mohamed Elhoseiny", "abstract": "  Recent advances in vision-language models (VLMs) have made impressive strides\nin understanding spatio-temporal relationships in videos. However, when spatial\ninformation is obscured, these models struggle to capture purely temporal\npatterns. We introduce $\\textbf{SpookyBench}$, a benchmark where information is\nencoded solely in temporal sequences of noise-like frames, mirroring natural\nphenomena from biological signaling to covert communication. Interestingly,\nwhile humans can recognize shapes, text, and patterns in these sequences with\nover 98% accuracy, state-of-the-art VLMs achieve 0% accuracy. This performance\ngap highlights a critical limitation: an over-reliance on frame-level spatial\nfeatures and an inability to extract meaning from temporal cues. Furthermore,\nwhen trained in data sets with low spatial signal-to-noise ratios (SNR),\ntemporal understanding of models degrades more rapidly than human perception,\nespecially in tasks requiring fine-grained temporal reasoning. Overcoming this\nlimitation will require novel architectures or training paradigms that decouple\nspatial dependencies from temporal processing. Our systematic analysis shows\nthat this issue persists across model scales and architectures. We release\nSpookyBench to catalyze research in temporal pattern recognition and bridge the\ngap between human and machine video understanding. Dataset and code has been\nmade available on our project website: https://timeblindness.github.io/.\n", "link": "http://arxiv.org/abs/2505.24867v1", "date": "2025-05-30", "relevancy": 2.4367, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6152}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6152}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5789}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Time%20Blindness%3A%20Why%20Video-Language%20Models%20Can%27t%20See%20What%20Humans%20Can%3F&body=Title%3A%20Time%20Blindness%3A%20Why%20Video-Language%20Models%20Can%27t%20See%20What%20Humans%20Can%3F%0AAuthor%3A%20Ujjwal%20Upadhyay%20and%20Mukul%20Ranjan%20and%20Zhiqiang%20Shen%20and%20Mohamed%20Elhoseiny%0AAbstract%3A%20%20%20Recent%20advances%20in%20vision-language%20models%20%28VLMs%29%20have%20made%20impressive%20strides%0Ain%20understanding%20spatio-temporal%20relationships%20in%20videos.%20However%2C%20when%20spatial%0Ainformation%20is%20obscured%2C%20these%20models%20struggle%20to%20capture%20purely%20temporal%0Apatterns.%20We%20introduce%20%24%5Ctextbf%7BSpookyBench%7D%24%2C%20a%20benchmark%20where%20information%20is%0Aencoded%20solely%20in%20temporal%20sequences%20of%20noise-like%20frames%2C%20mirroring%20natural%0Aphenomena%20from%20biological%20signaling%20to%20covert%20communication.%20Interestingly%2C%0Awhile%20humans%20can%20recognize%20shapes%2C%20text%2C%20and%20patterns%20in%20these%20sequences%20with%0Aover%2098%25%20accuracy%2C%20state-of-the-art%20VLMs%20achieve%200%25%20accuracy.%20This%20performance%0Agap%20highlights%20a%20critical%20limitation%3A%20an%20over-reliance%20on%20frame-level%20spatial%0Afeatures%20and%20an%20inability%20to%20extract%20meaning%20from%20temporal%20cues.%20Furthermore%2C%0Awhen%20trained%20in%20data%20sets%20with%20low%20spatial%20signal-to-noise%20ratios%20%28SNR%29%2C%0Atemporal%20understanding%20of%20models%20degrades%20more%20rapidly%20than%20human%20perception%2C%0Aespecially%20in%20tasks%20requiring%20fine-grained%20temporal%20reasoning.%20Overcoming%20this%0Alimitation%20will%20require%20novel%20architectures%20or%20training%20paradigms%20that%20decouple%0Aspatial%20dependencies%20from%20temporal%20processing.%20Our%20systematic%20analysis%20shows%0Athat%20this%20issue%20persists%20across%20model%20scales%20and%20architectures.%20We%20release%0ASpookyBench%20to%20catalyze%20research%20in%20temporal%20pattern%20recognition%20and%20bridge%20the%0Agap%20between%20human%20and%20machine%20video%20understanding.%20Dataset%20and%20code%20has%20been%0Amade%20available%20on%20our%20project%20website%3A%20https%3A//timeblindness.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24867v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTime%2520Blindness%253A%2520Why%2520Video-Language%2520Models%2520Can%2527t%2520See%2520What%2520Humans%2520Can%253F%26entry.906535625%3DUjjwal%2520Upadhyay%2520and%2520Mukul%2520Ranjan%2520and%2520Zhiqiang%2520Shen%2520and%2520Mohamed%2520Elhoseiny%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520vision-language%2520models%2520%2528VLMs%2529%2520have%2520made%2520impressive%2520strides%250Ain%2520understanding%2520spatio-temporal%2520relationships%2520in%2520videos.%2520However%252C%2520when%2520spatial%250Ainformation%2520is%2520obscured%252C%2520these%2520models%2520struggle%2520to%2520capture%2520purely%2520temporal%250Apatterns.%2520We%2520introduce%2520%2524%255Ctextbf%257BSpookyBench%257D%2524%252C%2520a%2520benchmark%2520where%2520information%2520is%250Aencoded%2520solely%2520in%2520temporal%2520sequences%2520of%2520noise-like%2520frames%252C%2520mirroring%2520natural%250Aphenomena%2520from%2520biological%2520signaling%2520to%2520covert%2520communication.%2520Interestingly%252C%250Awhile%2520humans%2520can%2520recognize%2520shapes%252C%2520text%252C%2520and%2520patterns%2520in%2520these%2520sequences%2520with%250Aover%252098%2525%2520accuracy%252C%2520state-of-the-art%2520VLMs%2520achieve%25200%2525%2520accuracy.%2520This%2520performance%250Agap%2520highlights%2520a%2520critical%2520limitation%253A%2520an%2520over-reliance%2520on%2520frame-level%2520spatial%250Afeatures%2520and%2520an%2520inability%2520to%2520extract%2520meaning%2520from%2520temporal%2520cues.%2520Furthermore%252C%250Awhen%2520trained%2520in%2520data%2520sets%2520with%2520low%2520spatial%2520signal-to-noise%2520ratios%2520%2528SNR%2529%252C%250Atemporal%2520understanding%2520of%2520models%2520degrades%2520more%2520rapidly%2520than%2520human%2520perception%252C%250Aespecially%2520in%2520tasks%2520requiring%2520fine-grained%2520temporal%2520reasoning.%2520Overcoming%2520this%250Alimitation%2520will%2520require%2520novel%2520architectures%2520or%2520training%2520paradigms%2520that%2520decouple%250Aspatial%2520dependencies%2520from%2520temporal%2520processing.%2520Our%2520systematic%2520analysis%2520shows%250Athat%2520this%2520issue%2520persists%2520across%2520model%2520scales%2520and%2520architectures.%2520We%2520release%250ASpookyBench%2520to%2520catalyze%2520research%2520in%2520temporal%2520pattern%2520recognition%2520and%2520bridge%2520the%250Agap%2520between%2520human%2520and%2520machine%2520video%2520understanding.%2520Dataset%2520and%2520code%2520has%2520been%250Amade%2520available%2520on%2520our%2520project%2520website%253A%2520https%253A//timeblindness.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24867v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Time%20Blindness%3A%20Why%20Video-Language%20Models%20Can%27t%20See%20What%20Humans%20Can%3F&entry.906535625=Ujjwal%20Upadhyay%20and%20Mukul%20Ranjan%20and%20Zhiqiang%20Shen%20and%20Mohamed%20Elhoseiny&entry.1292438233=%20%20Recent%20advances%20in%20vision-language%20models%20%28VLMs%29%20have%20made%20impressive%20strides%0Ain%20understanding%20spatio-temporal%20relationships%20in%20videos.%20However%2C%20when%20spatial%0Ainformation%20is%20obscured%2C%20these%20models%20struggle%20to%20capture%20purely%20temporal%0Apatterns.%20We%20introduce%20%24%5Ctextbf%7BSpookyBench%7D%24%2C%20a%20benchmark%20where%20information%20is%0Aencoded%20solely%20in%20temporal%20sequences%20of%20noise-like%20frames%2C%20mirroring%20natural%0Aphenomena%20from%20biological%20signaling%20to%20covert%20communication.%20Interestingly%2C%0Awhile%20humans%20can%20recognize%20shapes%2C%20text%2C%20and%20patterns%20in%20these%20sequences%20with%0Aover%2098%25%20accuracy%2C%20state-of-the-art%20VLMs%20achieve%200%25%20accuracy.%20This%20performance%0Agap%20highlights%20a%20critical%20limitation%3A%20an%20over-reliance%20on%20frame-level%20spatial%0Afeatures%20and%20an%20inability%20to%20extract%20meaning%20from%20temporal%20cues.%20Furthermore%2C%0Awhen%20trained%20in%20data%20sets%20with%20low%20spatial%20signal-to-noise%20ratios%20%28SNR%29%2C%0Atemporal%20understanding%20of%20models%20degrades%20more%20rapidly%20than%20human%20perception%2C%0Aespecially%20in%20tasks%20requiring%20fine-grained%20temporal%20reasoning.%20Overcoming%20this%0Alimitation%20will%20require%20novel%20architectures%20or%20training%20paradigms%20that%20decouple%0Aspatial%20dependencies%20from%20temporal%20processing.%20Our%20systematic%20analysis%20shows%0Athat%20this%20issue%20persists%20across%20model%20scales%20and%20architectures.%20We%20release%0ASpookyBench%20to%20catalyze%20research%20in%20temporal%20pattern%20recognition%20and%20bridge%20the%0Agap%20between%20human%20and%20machine%20video%20understanding.%20Dataset%20and%20code%20has%20been%0Amade%20available%20on%20our%20project%20website%3A%20https%3A//timeblindness.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24867v1&entry.124074799=Read"},
{"title": "Disentangling Granularity: An Implicit Inductive Bias in Factorized VAEs", "author": "Zihao Chen and Yu Xiang and Wenyong Wang", "abstract": "  Despite the success in learning semantically meaningful, unsupervised\ndisentangled representations, variational autoencoders (VAEs) and their\nvariants face a fundamental theoretical challenge: substantial evidence\nindicates that unsupervised disentanglement is unattainable without implicit\ninductive bias, yet such bias remains elusive. In this work, we focus on\nexploring the implicit inductive bias that drive disentanglement in VAEs with\nfactorization priors. By analyzing the total correlation in \\b{eta}-TCVAE, we\nuncover a crucial implicit inductive bias called disentangling granularity,\nwhich leads to the discovery of an interesting \"V\"-shaped optimal Evidence\nLower Bound (ELBO) trajectory within the parameter space. This finding is\nvalidated through over 100K experiments using factorized VAEs and our newly\nproposed model, \\b{eta}-STCVAE. Notably, experimental results reveal that\nconventional factorized VAEs, constrained by fixed disentangling granularity,\ninherently tend to disentangle low-complexity feature. Whereas, appropriately\ntuning disentangling granularity, as enabled by \\b{eta}-STCVAE, broadens the\nrange of disentangled representations, allowing for the disentanglement of\nhigh-complexity features. Our findings unveil that disentangling granularity as\nan implicit inductive bias in factorized VAEs influence both disentanglement\nperformance and the inference of the ELBO, offering fresh insights into the\ninterpretability and inherent biases of VAEs.\n", "link": "http://arxiv.org/abs/2505.24684v1", "date": "2025-05-30", "relevancy": 2.4301, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5115}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4733}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangling%20Granularity%3A%20An%20Implicit%20Inductive%20Bias%20in%20Factorized%20VAEs&body=Title%3A%20Disentangling%20Granularity%3A%20An%20Implicit%20Inductive%20Bias%20in%20Factorized%20VAEs%0AAuthor%3A%20Zihao%20Chen%20and%20Yu%20Xiang%20and%20Wenyong%20Wang%0AAbstract%3A%20%20%20Despite%20the%20success%20in%20learning%20semantically%20meaningful%2C%20unsupervised%0Adisentangled%20representations%2C%20variational%20autoencoders%20%28VAEs%29%20and%20their%0Avariants%20face%20a%20fundamental%20theoretical%20challenge%3A%20substantial%20evidence%0Aindicates%20that%20unsupervised%20disentanglement%20is%20unattainable%20without%20implicit%0Ainductive%20bias%2C%20yet%20such%20bias%20remains%20elusive.%20In%20this%20work%2C%20we%20focus%20on%0Aexploring%20the%20implicit%20inductive%20bias%20that%20drive%20disentanglement%20in%20VAEs%20with%0Afactorization%20priors.%20By%20analyzing%20the%20total%20correlation%20in%20%5Cb%7Beta%7D-TCVAE%2C%20we%0Auncover%20a%20crucial%20implicit%20inductive%20bias%20called%20disentangling%20granularity%2C%0Awhich%20leads%20to%20the%20discovery%20of%20an%20interesting%20%22V%22-shaped%20optimal%20Evidence%0ALower%20Bound%20%28ELBO%29%20trajectory%20within%20the%20parameter%20space.%20This%20finding%20is%0Avalidated%20through%20over%20100K%20experiments%20using%20factorized%20VAEs%20and%20our%20newly%0Aproposed%20model%2C%20%5Cb%7Beta%7D-STCVAE.%20Notably%2C%20experimental%20results%20reveal%20that%0Aconventional%20factorized%20VAEs%2C%20constrained%20by%20fixed%20disentangling%20granularity%2C%0Ainherently%20tend%20to%20disentangle%20low-complexity%20feature.%20Whereas%2C%20appropriately%0Atuning%20disentangling%20granularity%2C%20as%20enabled%20by%20%5Cb%7Beta%7D-STCVAE%2C%20broadens%20the%0Arange%20of%20disentangled%20representations%2C%20allowing%20for%20the%20disentanglement%20of%0Ahigh-complexity%20features.%20Our%20findings%20unveil%20that%20disentangling%20granularity%20as%0Aan%20implicit%20inductive%20bias%20in%20factorized%20VAEs%20influence%20both%20disentanglement%0Aperformance%20and%20the%20inference%20of%20the%20ELBO%2C%20offering%20fresh%20insights%20into%20the%0Ainterpretability%20and%20inherent%20biases%20of%20VAEs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangling%2520Granularity%253A%2520An%2520Implicit%2520Inductive%2520Bias%2520in%2520Factorized%2520VAEs%26entry.906535625%3DZihao%2520Chen%2520and%2520Yu%2520Xiang%2520and%2520Wenyong%2520Wang%26entry.1292438233%3D%2520%2520Despite%2520the%2520success%2520in%2520learning%2520semantically%2520meaningful%252C%2520unsupervised%250Adisentangled%2520representations%252C%2520variational%2520autoencoders%2520%2528VAEs%2529%2520and%2520their%250Avariants%2520face%2520a%2520fundamental%2520theoretical%2520challenge%253A%2520substantial%2520evidence%250Aindicates%2520that%2520unsupervised%2520disentanglement%2520is%2520unattainable%2520without%2520implicit%250Ainductive%2520bias%252C%2520yet%2520such%2520bias%2520remains%2520elusive.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%250Aexploring%2520the%2520implicit%2520inductive%2520bias%2520that%2520drive%2520disentanglement%2520in%2520VAEs%2520with%250Afactorization%2520priors.%2520By%2520analyzing%2520the%2520total%2520correlation%2520in%2520%255Cb%257Beta%257D-TCVAE%252C%2520we%250Auncover%2520a%2520crucial%2520implicit%2520inductive%2520bias%2520called%2520disentangling%2520granularity%252C%250Awhich%2520leads%2520to%2520the%2520discovery%2520of%2520an%2520interesting%2520%2522V%2522-shaped%2520optimal%2520Evidence%250ALower%2520Bound%2520%2528ELBO%2529%2520trajectory%2520within%2520the%2520parameter%2520space.%2520This%2520finding%2520is%250Avalidated%2520through%2520over%2520100K%2520experiments%2520using%2520factorized%2520VAEs%2520and%2520our%2520newly%250Aproposed%2520model%252C%2520%255Cb%257Beta%257D-STCVAE.%2520Notably%252C%2520experimental%2520results%2520reveal%2520that%250Aconventional%2520factorized%2520VAEs%252C%2520constrained%2520by%2520fixed%2520disentangling%2520granularity%252C%250Ainherently%2520tend%2520to%2520disentangle%2520low-complexity%2520feature.%2520Whereas%252C%2520appropriately%250Atuning%2520disentangling%2520granularity%252C%2520as%2520enabled%2520by%2520%255Cb%257Beta%257D-STCVAE%252C%2520broadens%2520the%250Arange%2520of%2520disentangled%2520representations%252C%2520allowing%2520for%2520the%2520disentanglement%2520of%250Ahigh-complexity%2520features.%2520Our%2520findings%2520unveil%2520that%2520disentangling%2520granularity%2520as%250Aan%2520implicit%2520inductive%2520bias%2520in%2520factorized%2520VAEs%2520influence%2520both%2520disentanglement%250Aperformance%2520and%2520the%2520inference%2520of%2520the%2520ELBO%252C%2520offering%2520fresh%2520insights%2520into%2520the%250Ainterpretability%2520and%2520inherent%2520biases%2520of%2520VAEs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangling%20Granularity%3A%20An%20Implicit%20Inductive%20Bias%20in%20Factorized%20VAEs&entry.906535625=Zihao%20Chen%20and%20Yu%20Xiang%20and%20Wenyong%20Wang&entry.1292438233=%20%20Despite%20the%20success%20in%20learning%20semantically%20meaningful%2C%20unsupervised%0Adisentangled%20representations%2C%20variational%20autoencoders%20%28VAEs%29%20and%20their%0Avariants%20face%20a%20fundamental%20theoretical%20challenge%3A%20substantial%20evidence%0Aindicates%20that%20unsupervised%20disentanglement%20is%20unattainable%20without%20implicit%0Ainductive%20bias%2C%20yet%20such%20bias%20remains%20elusive.%20In%20this%20work%2C%20we%20focus%20on%0Aexploring%20the%20implicit%20inductive%20bias%20that%20drive%20disentanglement%20in%20VAEs%20with%0Afactorization%20priors.%20By%20analyzing%20the%20total%20correlation%20in%20%5Cb%7Beta%7D-TCVAE%2C%20we%0Auncover%20a%20crucial%20implicit%20inductive%20bias%20called%20disentangling%20granularity%2C%0Awhich%20leads%20to%20the%20discovery%20of%20an%20interesting%20%22V%22-shaped%20optimal%20Evidence%0ALower%20Bound%20%28ELBO%29%20trajectory%20within%20the%20parameter%20space.%20This%20finding%20is%0Avalidated%20through%20over%20100K%20experiments%20using%20factorized%20VAEs%20and%20our%20newly%0Aproposed%20model%2C%20%5Cb%7Beta%7D-STCVAE.%20Notably%2C%20experimental%20results%20reveal%20that%0Aconventional%20factorized%20VAEs%2C%20constrained%20by%20fixed%20disentangling%20granularity%2C%0Ainherently%20tend%20to%20disentangle%20low-complexity%20feature.%20Whereas%2C%20appropriately%0Atuning%20disentangling%20granularity%2C%20as%20enabled%20by%20%5Cb%7Beta%7D-STCVAE%2C%20broadens%20the%0Arange%20of%20disentangled%20representations%2C%20allowing%20for%20the%20disentanglement%20of%0Ahigh-complexity%20features.%20Our%20findings%20unveil%20that%20disentangling%20granularity%20as%0Aan%20implicit%20inductive%20bias%20in%20factorized%20VAEs%20influence%20both%20disentanglement%0Aperformance%20and%20the%20inference%20of%20the%20ELBO%2C%20offering%20fresh%20insights%20into%20the%0Ainterpretability%20and%20inherent%20biases%20of%20VAEs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24684v1&entry.124074799=Read"},
{"title": "NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization", "author": "Hyuntak Kim and Byung-Hak Kim", "abstract": "  Summarizing long-form narratives--such as books, movies, and TV\nscripts--requires capturing intricate plotlines, character interactions, and\nthematic coherence, a task that remains challenging for existing LLMs. We\nintroduce NexusSum, a multi-agent LLM framework for narrative summarization\nthat processes long-form text through a structured, sequential\npipeline--without requiring fine-tuning. Our approach introduces two key\ninnovations: (1) Dialogue-to-Description Transformation: A narrative-specific\npreprocessing method that standardizes character dialogue and descriptive text\ninto a unified format, improving coherence. (2) Hierarchical Multi-LLM\nSummarization: A structured summarization pipeline that optimizes chunk\nprocessing and controls output length for accurate, high-quality summaries. Our\nmethod establishes a new state-of-the-art in narrative summarization, achieving\nup to a 30.0% improvement in BERTScore (F1) across books, movies, and TV\nscripts. These results demonstrate the effectiveness of multi-agent LLMs in\nhandling long-form content, offering a scalable approach for structured\nsummarization in diverse storytelling domains.\n", "link": "http://arxiv.org/abs/2505.24575v1", "date": "2025-05-30", "relevancy": 2.415, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.489}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.489}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NexusSum%3A%20Hierarchical%20LLM%20Agents%20for%20Long-Form%20Narrative%20Summarization&body=Title%3A%20NexusSum%3A%20Hierarchical%20LLM%20Agents%20for%20Long-Form%20Narrative%20Summarization%0AAuthor%3A%20Hyuntak%20Kim%20and%20Byung-Hak%20Kim%0AAbstract%3A%20%20%20Summarizing%20long-form%20narratives--such%20as%20books%2C%20movies%2C%20and%20TV%0Ascripts--requires%20capturing%20intricate%20plotlines%2C%20character%20interactions%2C%20and%0Athematic%20coherence%2C%20a%20task%20that%20remains%20challenging%20for%20existing%20LLMs.%20We%0Aintroduce%20NexusSum%2C%20a%20multi-agent%20LLM%20framework%20for%20narrative%20summarization%0Athat%20processes%20long-form%20text%20through%20a%20structured%2C%20sequential%0Apipeline--without%20requiring%20fine-tuning.%20Our%20approach%20introduces%20two%20key%0Ainnovations%3A%20%281%29%20Dialogue-to-Description%20Transformation%3A%20A%20narrative-specific%0Apreprocessing%20method%20that%20standardizes%20character%20dialogue%20and%20descriptive%20text%0Ainto%20a%20unified%20format%2C%20improving%20coherence.%20%282%29%20Hierarchical%20Multi-LLM%0ASummarization%3A%20A%20structured%20summarization%20pipeline%20that%20optimizes%20chunk%0Aprocessing%20and%20controls%20output%20length%20for%20accurate%2C%20high-quality%20summaries.%20Our%0Amethod%20establishes%20a%20new%20state-of-the-art%20in%20narrative%20summarization%2C%20achieving%0Aup%20to%20a%2030.0%25%20improvement%20in%20BERTScore%20%28F1%29%20across%20books%2C%20movies%2C%20and%20TV%0Ascripts.%20These%20results%20demonstrate%20the%20effectiveness%20of%20multi-agent%20LLMs%20in%0Ahandling%20long-form%20content%2C%20offering%20a%20scalable%20approach%20for%20structured%0Asummarization%20in%20diverse%20storytelling%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24575v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNexusSum%253A%2520Hierarchical%2520LLM%2520Agents%2520for%2520Long-Form%2520Narrative%2520Summarization%26entry.906535625%3DHyuntak%2520Kim%2520and%2520Byung-Hak%2520Kim%26entry.1292438233%3D%2520%2520Summarizing%2520long-form%2520narratives--such%2520as%2520books%252C%2520movies%252C%2520and%2520TV%250Ascripts--requires%2520capturing%2520intricate%2520plotlines%252C%2520character%2520interactions%252C%2520and%250Athematic%2520coherence%252C%2520a%2520task%2520that%2520remains%2520challenging%2520for%2520existing%2520LLMs.%2520We%250Aintroduce%2520NexusSum%252C%2520a%2520multi-agent%2520LLM%2520framework%2520for%2520narrative%2520summarization%250Athat%2520processes%2520long-form%2520text%2520through%2520a%2520structured%252C%2520sequential%250Apipeline--without%2520requiring%2520fine-tuning.%2520Our%2520approach%2520introduces%2520two%2520key%250Ainnovations%253A%2520%25281%2529%2520Dialogue-to-Description%2520Transformation%253A%2520A%2520narrative-specific%250Apreprocessing%2520method%2520that%2520standardizes%2520character%2520dialogue%2520and%2520descriptive%2520text%250Ainto%2520a%2520unified%2520format%252C%2520improving%2520coherence.%2520%25282%2529%2520Hierarchical%2520Multi-LLM%250ASummarization%253A%2520A%2520structured%2520summarization%2520pipeline%2520that%2520optimizes%2520chunk%250Aprocessing%2520and%2520controls%2520output%2520length%2520for%2520accurate%252C%2520high-quality%2520summaries.%2520Our%250Amethod%2520establishes%2520a%2520new%2520state-of-the-art%2520in%2520narrative%2520summarization%252C%2520achieving%250Aup%2520to%2520a%252030.0%2525%2520improvement%2520in%2520BERTScore%2520%2528F1%2529%2520across%2520books%252C%2520movies%252C%2520and%2520TV%250Ascripts.%2520These%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520multi-agent%2520LLMs%2520in%250Ahandling%2520long-form%2520content%252C%2520offering%2520a%2520scalable%2520approach%2520for%2520structured%250Asummarization%2520in%2520diverse%2520storytelling%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24575v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NexusSum%3A%20Hierarchical%20LLM%20Agents%20for%20Long-Form%20Narrative%20Summarization&entry.906535625=Hyuntak%20Kim%20and%20Byung-Hak%20Kim&entry.1292438233=%20%20Summarizing%20long-form%20narratives--such%20as%20books%2C%20movies%2C%20and%20TV%0Ascripts--requires%20capturing%20intricate%20plotlines%2C%20character%20interactions%2C%20and%0Athematic%20coherence%2C%20a%20task%20that%20remains%20challenging%20for%20existing%20LLMs.%20We%0Aintroduce%20NexusSum%2C%20a%20multi-agent%20LLM%20framework%20for%20narrative%20summarization%0Athat%20processes%20long-form%20text%20through%20a%20structured%2C%20sequential%0Apipeline--without%20requiring%20fine-tuning.%20Our%20approach%20introduces%20two%20key%0Ainnovations%3A%20%281%29%20Dialogue-to-Description%20Transformation%3A%20A%20narrative-specific%0Apreprocessing%20method%20that%20standardizes%20character%20dialogue%20and%20descriptive%20text%0Ainto%20a%20unified%20format%2C%20improving%20coherence.%20%282%29%20Hierarchical%20Multi-LLM%0ASummarization%3A%20A%20structured%20summarization%20pipeline%20that%20optimizes%20chunk%0Aprocessing%20and%20controls%20output%20length%20for%20accurate%2C%20high-quality%20summaries.%20Our%0Amethod%20establishes%20a%20new%20state-of-the-art%20in%20narrative%20summarization%2C%20achieving%0Aup%20to%20a%2030.0%25%20improvement%20in%20BERTScore%20%28F1%29%20across%20books%2C%20movies%2C%20and%20TV%0Ascripts.%20These%20results%20demonstrate%20the%20effectiveness%20of%20multi-agent%20LLMs%20in%0Ahandling%20long-form%20content%2C%20offering%20a%20scalable%20approach%20for%20structured%0Asummarization%20in%20diverse%20storytelling%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24575v1&entry.124074799=Read"},
{"title": "Multiple LLM Agents Debate for Equitable Cultural Alignment", "author": "Dayeon Ki and Rachel Rudinger and Tianyi Zhou and Marine Carpuat", "abstract": "  Large Language Models (LLMs) need to adapt their predictions to diverse\ncultural contexts to benefit diverse communities across the world. While\nprevious efforts have focused on single-LLM, single-turn approaches, we propose\nto exploit the complementary strengths of multiple LLMs to promote cultural\nadaptability. We introduce a Multi-Agent Debate framework, where two LLM-based\nagents debate over a cultural scenario and collaboratively reach a final\ndecision. We propose two variants: one where either LLM agents exclusively\ndebate and another where they dynamically choose between self-reflection and\ndebate during their turns. We evaluate these approaches on 7 open-weight LLMs\n(and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette\nnorms in 75 countries. Experiments show that debate improves both overall\naccuracy and cultural group parity over single-LLM baselines. Notably,\nmulti-agent debate enables relatively small LLMs (7-9B) to achieve accuracies\ncomparable to that of a much larger model (27B parameters).\n", "link": "http://arxiv.org/abs/2505.24671v1", "date": "2025-05-30", "relevancy": 2.4111, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4934}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4767}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multiple%20LLM%20Agents%20Debate%20for%20Equitable%20Cultural%20Alignment&body=Title%3A%20Multiple%20LLM%20Agents%20Debate%20for%20Equitable%20Cultural%20Alignment%0AAuthor%3A%20Dayeon%20Ki%20and%20Rachel%20Rudinger%20and%20Tianyi%20Zhou%20and%20Marine%20Carpuat%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20need%20to%20adapt%20their%20predictions%20to%20diverse%0Acultural%20contexts%20to%20benefit%20diverse%20communities%20across%20the%20world.%20While%0Aprevious%20efforts%20have%20focused%20on%20single-LLM%2C%20single-turn%20approaches%2C%20we%20propose%0Ato%20exploit%20the%20complementary%20strengths%20of%20multiple%20LLMs%20to%20promote%20cultural%0Aadaptability.%20We%20introduce%20a%20Multi-Agent%20Debate%20framework%2C%20where%20two%20LLM-based%0Aagents%20debate%20over%20a%20cultural%20scenario%20and%20collaboratively%20reach%20a%20final%0Adecision.%20We%20propose%20two%20variants%3A%20one%20where%20either%20LLM%20agents%20exclusively%0Adebate%20and%20another%20where%20they%20dynamically%20choose%20between%20self-reflection%20and%0Adebate%20during%20their%20turns.%20We%20evaluate%20these%20approaches%20on%207%20open-weight%20LLMs%0A%28and%2021%20LLM%20combinations%29%20using%20the%20NormAd-ETI%20benchmark%20for%20social%20etiquette%0Anorms%20in%2075%20countries.%20Experiments%20show%20that%20debate%20improves%20both%20overall%0Aaccuracy%20and%20cultural%20group%20parity%20over%20single-LLM%20baselines.%20Notably%2C%0Amulti-agent%20debate%20enables%20relatively%20small%20LLMs%20%287-9B%29%20to%20achieve%20accuracies%0Acomparable%20to%20that%20of%20a%20much%20larger%20model%20%2827B%20parameters%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24671v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiple%2520LLM%2520Agents%2520Debate%2520for%2520Equitable%2520Cultural%2520Alignment%26entry.906535625%3DDayeon%2520Ki%2520and%2520Rachel%2520Rudinger%2520and%2520Tianyi%2520Zhou%2520and%2520Marine%2520Carpuat%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520need%2520to%2520adapt%2520their%2520predictions%2520to%2520diverse%250Acultural%2520contexts%2520to%2520benefit%2520diverse%2520communities%2520across%2520the%2520world.%2520While%250Aprevious%2520efforts%2520have%2520focused%2520on%2520single-LLM%252C%2520single-turn%2520approaches%252C%2520we%2520propose%250Ato%2520exploit%2520the%2520complementary%2520strengths%2520of%2520multiple%2520LLMs%2520to%2520promote%2520cultural%250Aadaptability.%2520We%2520introduce%2520a%2520Multi-Agent%2520Debate%2520framework%252C%2520where%2520two%2520LLM-based%250Aagents%2520debate%2520over%2520a%2520cultural%2520scenario%2520and%2520collaboratively%2520reach%2520a%2520final%250Adecision.%2520We%2520propose%2520two%2520variants%253A%2520one%2520where%2520either%2520LLM%2520agents%2520exclusively%250Adebate%2520and%2520another%2520where%2520they%2520dynamically%2520choose%2520between%2520self-reflection%2520and%250Adebate%2520during%2520their%2520turns.%2520We%2520evaluate%2520these%2520approaches%2520on%25207%2520open-weight%2520LLMs%250A%2528and%252021%2520LLM%2520combinations%2529%2520using%2520the%2520NormAd-ETI%2520benchmark%2520for%2520social%2520etiquette%250Anorms%2520in%252075%2520countries.%2520Experiments%2520show%2520that%2520debate%2520improves%2520both%2520overall%250Aaccuracy%2520and%2520cultural%2520group%2520parity%2520over%2520single-LLM%2520baselines.%2520Notably%252C%250Amulti-agent%2520debate%2520enables%2520relatively%2520small%2520LLMs%2520%25287-9B%2529%2520to%2520achieve%2520accuracies%250Acomparable%2520to%2520that%2520of%2520a%2520much%2520larger%2520model%2520%252827B%2520parameters%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24671v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multiple%20LLM%20Agents%20Debate%20for%20Equitable%20Cultural%20Alignment&entry.906535625=Dayeon%20Ki%20and%20Rachel%20Rudinger%20and%20Tianyi%20Zhou%20and%20Marine%20Carpuat&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20need%20to%20adapt%20their%20predictions%20to%20diverse%0Acultural%20contexts%20to%20benefit%20diverse%20communities%20across%20the%20world.%20While%0Aprevious%20efforts%20have%20focused%20on%20single-LLM%2C%20single-turn%20approaches%2C%20we%20propose%0Ato%20exploit%20the%20complementary%20strengths%20of%20multiple%20LLMs%20to%20promote%20cultural%0Aadaptability.%20We%20introduce%20a%20Multi-Agent%20Debate%20framework%2C%20where%20two%20LLM-based%0Aagents%20debate%20over%20a%20cultural%20scenario%20and%20collaboratively%20reach%20a%20final%0Adecision.%20We%20propose%20two%20variants%3A%20one%20where%20either%20LLM%20agents%20exclusively%0Adebate%20and%20another%20where%20they%20dynamically%20choose%20between%20self-reflection%20and%0Adebate%20during%20their%20turns.%20We%20evaluate%20these%20approaches%20on%207%20open-weight%20LLMs%0A%28and%2021%20LLM%20combinations%29%20using%20the%20NormAd-ETI%20benchmark%20for%20social%20etiquette%0Anorms%20in%2075%20countries.%20Experiments%20show%20that%20debate%20improves%20both%20overall%0Aaccuracy%20and%20cultural%20group%20parity%20over%20single-LLM%20baselines.%20Notably%2C%0Amulti-agent%20debate%20enables%20relatively%20small%20LLMs%20%287-9B%29%20to%20achieve%20accuracies%0Acomparable%20to%20that%20of%20a%20much%20larger%20model%20%2827B%20parameters%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24671v1&entry.124074799=Read"},
{"title": "You need to MIMIC to get FAME: Solving Meeting Transcript Scarcity with\n  a Multi-Agent Conversations", "author": "Frederic Kirstein and Muneeb Khan and Jan Philip Wahle and Terry Ruas and Bela Gipp", "abstract": "  Meeting summarization suffers from limited high-quality data, mainly due to\nprivacy restrictions and expensive collection processes. We address this gap\nwith FAME, a dataset of 500 meetings in English and 300 in German produced by\nMIMIC, our new multi-agent meeting synthesis framework that generates meeting\ntranscripts on a given knowledge source by defining psychologically grounded\nparticipant profiles, outlining the conversation, and orchestrating a large\nlanguage model (LLM) debate. A modular post-processing step refines these\noutputs, mitigating potential repetitiveness and overly formal tones, ensuring\ncoherent, credible dialogues at scale. We also propose a psychologically\ngrounded evaluation framework assessing naturalness, social behavior\nauthenticity, and transcript difficulties. Human assessments show that FAME\napproximates real-meeting spontaneity (4.5/5 in naturalness), preserves\nspeaker-centric challenges (3/5 in spoken language), and introduces richer\ninformation-oriented difficulty (4/5 in difficulty). These findings highlight\nthat FAME is a good and scalable proxy for real-world meeting conditions. It\nenables new test scenarios for meeting summarization research and other\nconversation-centric applications in tasks requiring conversation data or\nsimulating social scenarios under behavioral constraints.\n", "link": "http://arxiv.org/abs/2502.13001v2", "date": "2025-05-30", "relevancy": 2.403, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4862}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4778}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20You%20need%20to%20MIMIC%20to%20get%20FAME%3A%20Solving%20Meeting%20Transcript%20Scarcity%20with%0A%20%20a%20Multi-Agent%20Conversations&body=Title%3A%20You%20need%20to%20MIMIC%20to%20get%20FAME%3A%20Solving%20Meeting%20Transcript%20Scarcity%20with%0A%20%20a%20Multi-Agent%20Conversations%0AAuthor%3A%20Frederic%20Kirstein%20and%20Muneeb%20Khan%20and%20Jan%20Philip%20Wahle%20and%20Terry%20Ruas%20and%20Bela%20Gipp%0AAbstract%3A%20%20%20Meeting%20summarization%20suffers%20from%20limited%20high-quality%20data%2C%20mainly%20due%20to%0Aprivacy%20restrictions%20and%20expensive%20collection%20processes.%20We%20address%20this%20gap%0Awith%20FAME%2C%20a%20dataset%20of%20500%20meetings%20in%20English%20and%20300%20in%20German%20produced%20by%0AMIMIC%2C%20our%20new%20multi-agent%20meeting%20synthesis%20framework%20that%20generates%20meeting%0Atranscripts%20on%20a%20given%20knowledge%20source%20by%20defining%20psychologically%20grounded%0Aparticipant%20profiles%2C%20outlining%20the%20conversation%2C%20and%20orchestrating%20a%20large%0Alanguage%20model%20%28LLM%29%20debate.%20A%20modular%20post-processing%20step%20refines%20these%0Aoutputs%2C%20mitigating%20potential%20repetitiveness%20and%20overly%20formal%20tones%2C%20ensuring%0Acoherent%2C%20credible%20dialogues%20at%20scale.%20We%20also%20propose%20a%20psychologically%0Agrounded%20evaluation%20framework%20assessing%20naturalness%2C%20social%20behavior%0Aauthenticity%2C%20and%20transcript%20difficulties.%20Human%20assessments%20show%20that%20FAME%0Aapproximates%20real-meeting%20spontaneity%20%284.5/5%20in%20naturalness%29%2C%20preserves%0Aspeaker-centric%20challenges%20%283/5%20in%20spoken%20language%29%2C%20and%20introduces%20richer%0Ainformation-oriented%20difficulty%20%284/5%20in%20difficulty%29.%20These%20findings%20highlight%0Athat%20FAME%20is%20a%20good%20and%20scalable%20proxy%20for%20real-world%20meeting%20conditions.%20It%0Aenables%20new%20test%20scenarios%20for%20meeting%20summarization%20research%20and%20other%0Aconversation-centric%20applications%20in%20tasks%20requiring%20conversation%20data%20or%0Asimulating%20social%20scenarios%20under%20behavioral%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13001v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYou%2520need%2520to%2520MIMIC%2520to%2520get%2520FAME%253A%2520Solving%2520Meeting%2520Transcript%2520Scarcity%2520with%250A%2520%2520a%2520Multi-Agent%2520Conversations%26entry.906535625%3DFrederic%2520Kirstein%2520and%2520Muneeb%2520Khan%2520and%2520Jan%2520Philip%2520Wahle%2520and%2520Terry%2520Ruas%2520and%2520Bela%2520Gipp%26entry.1292438233%3D%2520%2520Meeting%2520summarization%2520suffers%2520from%2520limited%2520high-quality%2520data%252C%2520mainly%2520due%2520to%250Aprivacy%2520restrictions%2520and%2520expensive%2520collection%2520processes.%2520We%2520address%2520this%2520gap%250Awith%2520FAME%252C%2520a%2520dataset%2520of%2520500%2520meetings%2520in%2520English%2520and%2520300%2520in%2520German%2520produced%2520by%250AMIMIC%252C%2520our%2520new%2520multi-agent%2520meeting%2520synthesis%2520framework%2520that%2520generates%2520meeting%250Atranscripts%2520on%2520a%2520given%2520knowledge%2520source%2520by%2520defining%2520psychologically%2520grounded%250Aparticipant%2520profiles%252C%2520outlining%2520the%2520conversation%252C%2520and%2520orchestrating%2520a%2520large%250Alanguage%2520model%2520%2528LLM%2529%2520debate.%2520A%2520modular%2520post-processing%2520step%2520refines%2520these%250Aoutputs%252C%2520mitigating%2520potential%2520repetitiveness%2520and%2520overly%2520formal%2520tones%252C%2520ensuring%250Acoherent%252C%2520credible%2520dialogues%2520at%2520scale.%2520We%2520also%2520propose%2520a%2520psychologically%250Agrounded%2520evaluation%2520framework%2520assessing%2520naturalness%252C%2520social%2520behavior%250Aauthenticity%252C%2520and%2520transcript%2520difficulties.%2520Human%2520assessments%2520show%2520that%2520FAME%250Aapproximates%2520real-meeting%2520spontaneity%2520%25284.5/5%2520in%2520naturalness%2529%252C%2520preserves%250Aspeaker-centric%2520challenges%2520%25283/5%2520in%2520spoken%2520language%2529%252C%2520and%2520introduces%2520richer%250Ainformation-oriented%2520difficulty%2520%25284/5%2520in%2520difficulty%2529.%2520These%2520findings%2520highlight%250Athat%2520FAME%2520is%2520a%2520good%2520and%2520scalable%2520proxy%2520for%2520real-world%2520meeting%2520conditions.%2520It%250Aenables%2520new%2520test%2520scenarios%2520for%2520meeting%2520summarization%2520research%2520and%2520other%250Aconversation-centric%2520applications%2520in%2520tasks%2520requiring%2520conversation%2520data%2520or%250Asimulating%2520social%2520scenarios%2520under%2520behavioral%2520constraints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13001v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=You%20need%20to%20MIMIC%20to%20get%20FAME%3A%20Solving%20Meeting%20Transcript%20Scarcity%20with%0A%20%20a%20Multi-Agent%20Conversations&entry.906535625=Frederic%20Kirstein%20and%20Muneeb%20Khan%20and%20Jan%20Philip%20Wahle%20and%20Terry%20Ruas%20and%20Bela%20Gipp&entry.1292438233=%20%20Meeting%20summarization%20suffers%20from%20limited%20high-quality%20data%2C%20mainly%20due%20to%0Aprivacy%20restrictions%20and%20expensive%20collection%20processes.%20We%20address%20this%20gap%0Awith%20FAME%2C%20a%20dataset%20of%20500%20meetings%20in%20English%20and%20300%20in%20German%20produced%20by%0AMIMIC%2C%20our%20new%20multi-agent%20meeting%20synthesis%20framework%20that%20generates%20meeting%0Atranscripts%20on%20a%20given%20knowledge%20source%20by%20defining%20psychologically%20grounded%0Aparticipant%20profiles%2C%20outlining%20the%20conversation%2C%20and%20orchestrating%20a%20large%0Alanguage%20model%20%28LLM%29%20debate.%20A%20modular%20post-processing%20step%20refines%20these%0Aoutputs%2C%20mitigating%20potential%20repetitiveness%20and%20overly%20formal%20tones%2C%20ensuring%0Acoherent%2C%20credible%20dialogues%20at%20scale.%20We%20also%20propose%20a%20psychologically%0Agrounded%20evaluation%20framework%20assessing%20naturalness%2C%20social%20behavior%0Aauthenticity%2C%20and%20transcript%20difficulties.%20Human%20assessments%20show%20that%20FAME%0Aapproximates%20real-meeting%20spontaneity%20%284.5/5%20in%20naturalness%29%2C%20preserves%0Aspeaker-centric%20challenges%20%283/5%20in%20spoken%20language%29%2C%20and%20introduces%20richer%0Ainformation-oriented%20difficulty%20%284/5%20in%20difficulty%29.%20These%20findings%20highlight%0Athat%20FAME%20is%20a%20good%20and%20scalable%20proxy%20for%20real-world%20meeting%20conditions.%20It%0Aenables%20new%20test%20scenarios%20for%20meeting%20summarization%20research%20and%20other%0Aconversation-centric%20applications%20in%20tasks%20requiring%20conversation%20data%20or%0Asimulating%20social%20scenarios%20under%20behavioral%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13001v2&entry.124074799=Read"},
{"title": "AutoStudio: Crafting Consistent Subjects in Multi-turn Interactive Image\n  Generation", "author": "Junhao Cheng and Xi Lu and Hanhui Li and Khun Loun Zai and Baiqiao Yin and Yuhao Cheng and Yiqiang Yan and Xiaodan Liang", "abstract": "  As cutting-edge Text-to-Image (T2I) generation models already excel at\nproducing remarkable single images, an even more challenging task, i.e.,\nmulti-turn interactive image generation begins to attract the attention of\nrelated research communities. This task requires models to interact with users\nover multiple turns to generate a coherent sequence of images. However, since\nusers may switch subjects frequently, current efforts struggle to maintain\nsubject consistency while generating diverse images. To address this issue, we\nintroduce a training-free multi-agent framework called AutoStudio. AutoStudio\nemploys three agents based on large language models (LLMs) to handle\ninteractions, along with a stable diffusion (SD) based agent for generating\nhigh-quality images. Specifically, AutoStudio consists of (i) a subject manager\nto interpret interaction dialogues and manage the context of each subject, (ii)\na layout generator to generate fine-grained bounding boxes to control subject\nlocations, (iii) a supervisor to provide suggestions for layout refinements,\nand (iv) a drawer to complete image generation. Furthermore, we introduce a\nParallel-UNet to replace the original UNet in the drawer, which employs two\nparallel cross-attention modules for exploiting subject-aware features. We also\nintroduce a subject-initialized generation method to better preserve small\nsubjects. Our AutoStudio hereby can generate a sequence of multi-subject images\ninteractively and consistently. Extensive experiments on the public CMIGBench\nbenchmark and human evaluations show that AutoStudio maintains multi-subject\nconsistency across multiple turns well, and it also raises the state-of-the-art\nperformance by 13.65% in average Frechet Inception Distance and 2.83% in\naverage character-character similarity.\n", "link": "http://arxiv.org/abs/2406.01388v3", "date": "2025-05-30", "relevancy": 2.4015, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6077}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5952}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoStudio%3A%20Crafting%20Consistent%20Subjects%20in%20Multi-turn%20Interactive%20Image%0A%20%20Generation&body=Title%3A%20AutoStudio%3A%20Crafting%20Consistent%20Subjects%20in%20Multi-turn%20Interactive%20Image%0A%20%20Generation%0AAuthor%3A%20Junhao%20Cheng%20and%20Xi%20Lu%20and%20Hanhui%20Li%20and%20Khun%20Loun%20Zai%20and%20Baiqiao%20Yin%20and%20Yuhao%20Cheng%20and%20Yiqiang%20Yan%20and%20Xiaodan%20Liang%0AAbstract%3A%20%20%20As%20cutting-edge%20Text-to-Image%20%28T2I%29%20generation%20models%20already%20excel%20at%0Aproducing%20remarkable%20single%20images%2C%20an%20even%20more%20challenging%20task%2C%20i.e.%2C%0Amulti-turn%20interactive%20image%20generation%20begins%20to%20attract%20the%20attention%20of%0Arelated%20research%20communities.%20This%20task%20requires%20models%20to%20interact%20with%20users%0Aover%20multiple%20turns%20to%20generate%20a%20coherent%20sequence%20of%20images.%20However%2C%20since%0Ausers%20may%20switch%20subjects%20frequently%2C%20current%20efforts%20struggle%20to%20maintain%0Asubject%20consistency%20while%20generating%20diverse%20images.%20To%20address%20this%20issue%2C%20we%0Aintroduce%20a%20training-free%20multi-agent%20framework%20called%20AutoStudio.%20AutoStudio%0Aemploys%20three%20agents%20based%20on%20large%20language%20models%20%28LLMs%29%20to%20handle%0Ainteractions%2C%20along%20with%20a%20stable%20diffusion%20%28SD%29%20based%20agent%20for%20generating%0Ahigh-quality%20images.%20Specifically%2C%20AutoStudio%20consists%20of%20%28i%29%20a%20subject%20manager%0Ato%20interpret%20interaction%20dialogues%20and%20manage%20the%20context%20of%20each%20subject%2C%20%28ii%29%0Aa%20layout%20generator%20to%20generate%20fine-grained%20bounding%20boxes%20to%20control%20subject%0Alocations%2C%20%28iii%29%20a%20supervisor%20to%20provide%20suggestions%20for%20layout%20refinements%2C%0Aand%20%28iv%29%20a%20drawer%20to%20complete%20image%20generation.%20Furthermore%2C%20we%20introduce%20a%0AParallel-UNet%20to%20replace%20the%20original%20UNet%20in%20the%20drawer%2C%20which%20employs%20two%0Aparallel%20cross-attention%20modules%20for%20exploiting%20subject-aware%20features.%20We%20also%0Aintroduce%20a%20subject-initialized%20generation%20method%20to%20better%20preserve%20small%0Asubjects.%20Our%20AutoStudio%20hereby%20can%20generate%20a%20sequence%20of%20multi-subject%20images%0Ainteractively%20and%20consistently.%20Extensive%20experiments%20on%20the%20public%20CMIGBench%0Abenchmark%20and%20human%20evaluations%20show%20that%20AutoStudio%20maintains%20multi-subject%0Aconsistency%20across%20multiple%20turns%20well%2C%20and%20it%20also%20raises%20the%20state-of-the-art%0Aperformance%20by%2013.65%25%20in%20average%20Frechet%20Inception%20Distance%20and%202.83%25%20in%0Aaverage%20character-character%20similarity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01388v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoStudio%253A%2520Crafting%2520Consistent%2520Subjects%2520in%2520Multi-turn%2520Interactive%2520Image%250A%2520%2520Generation%26entry.906535625%3DJunhao%2520Cheng%2520and%2520Xi%2520Lu%2520and%2520Hanhui%2520Li%2520and%2520Khun%2520Loun%2520Zai%2520and%2520Baiqiao%2520Yin%2520and%2520Yuhao%2520Cheng%2520and%2520Yiqiang%2520Yan%2520and%2520Xiaodan%2520Liang%26entry.1292438233%3D%2520%2520As%2520cutting-edge%2520Text-to-Image%2520%2528T2I%2529%2520generation%2520models%2520already%2520excel%2520at%250Aproducing%2520remarkable%2520single%2520images%252C%2520an%2520even%2520more%2520challenging%2520task%252C%2520i.e.%252C%250Amulti-turn%2520interactive%2520image%2520generation%2520begins%2520to%2520attract%2520the%2520attention%2520of%250Arelated%2520research%2520communities.%2520This%2520task%2520requires%2520models%2520to%2520interact%2520with%2520users%250Aover%2520multiple%2520turns%2520to%2520generate%2520a%2520coherent%2520sequence%2520of%2520images.%2520However%252C%2520since%250Ausers%2520may%2520switch%2520subjects%2520frequently%252C%2520current%2520efforts%2520struggle%2520to%2520maintain%250Asubject%2520consistency%2520while%2520generating%2520diverse%2520images.%2520To%2520address%2520this%2520issue%252C%2520we%250Aintroduce%2520a%2520training-free%2520multi-agent%2520framework%2520called%2520AutoStudio.%2520AutoStudio%250Aemploys%2520three%2520agents%2520based%2520on%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520handle%250Ainteractions%252C%2520along%2520with%2520a%2520stable%2520diffusion%2520%2528SD%2529%2520based%2520agent%2520for%2520generating%250Ahigh-quality%2520images.%2520Specifically%252C%2520AutoStudio%2520consists%2520of%2520%2528i%2529%2520a%2520subject%2520manager%250Ato%2520interpret%2520interaction%2520dialogues%2520and%2520manage%2520the%2520context%2520of%2520each%2520subject%252C%2520%2528ii%2529%250Aa%2520layout%2520generator%2520to%2520generate%2520fine-grained%2520bounding%2520boxes%2520to%2520control%2520subject%250Alocations%252C%2520%2528iii%2529%2520a%2520supervisor%2520to%2520provide%2520suggestions%2520for%2520layout%2520refinements%252C%250Aand%2520%2528iv%2529%2520a%2520drawer%2520to%2520complete%2520image%2520generation.%2520Furthermore%252C%2520we%2520introduce%2520a%250AParallel-UNet%2520to%2520replace%2520the%2520original%2520UNet%2520in%2520the%2520drawer%252C%2520which%2520employs%2520two%250Aparallel%2520cross-attention%2520modules%2520for%2520exploiting%2520subject-aware%2520features.%2520We%2520also%250Aintroduce%2520a%2520subject-initialized%2520generation%2520method%2520to%2520better%2520preserve%2520small%250Asubjects.%2520Our%2520AutoStudio%2520hereby%2520can%2520generate%2520a%2520sequence%2520of%2520multi-subject%2520images%250Ainteractively%2520and%2520consistently.%2520Extensive%2520experiments%2520on%2520the%2520public%2520CMIGBench%250Abenchmark%2520and%2520human%2520evaluations%2520show%2520that%2520AutoStudio%2520maintains%2520multi-subject%250Aconsistency%2520across%2520multiple%2520turns%2520well%252C%2520and%2520it%2520also%2520raises%2520the%2520state-of-the-art%250Aperformance%2520by%252013.65%2525%2520in%2520average%2520Frechet%2520Inception%2520Distance%2520and%25202.83%2525%2520in%250Aaverage%2520character-character%2520similarity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01388v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoStudio%3A%20Crafting%20Consistent%20Subjects%20in%20Multi-turn%20Interactive%20Image%0A%20%20Generation&entry.906535625=Junhao%20Cheng%20and%20Xi%20Lu%20and%20Hanhui%20Li%20and%20Khun%20Loun%20Zai%20and%20Baiqiao%20Yin%20and%20Yuhao%20Cheng%20and%20Yiqiang%20Yan%20and%20Xiaodan%20Liang&entry.1292438233=%20%20As%20cutting-edge%20Text-to-Image%20%28T2I%29%20generation%20models%20already%20excel%20at%0Aproducing%20remarkable%20single%20images%2C%20an%20even%20more%20challenging%20task%2C%20i.e.%2C%0Amulti-turn%20interactive%20image%20generation%20begins%20to%20attract%20the%20attention%20of%0Arelated%20research%20communities.%20This%20task%20requires%20models%20to%20interact%20with%20users%0Aover%20multiple%20turns%20to%20generate%20a%20coherent%20sequence%20of%20images.%20However%2C%20since%0Ausers%20may%20switch%20subjects%20frequently%2C%20current%20efforts%20struggle%20to%20maintain%0Asubject%20consistency%20while%20generating%20diverse%20images.%20To%20address%20this%20issue%2C%20we%0Aintroduce%20a%20training-free%20multi-agent%20framework%20called%20AutoStudio.%20AutoStudio%0Aemploys%20three%20agents%20based%20on%20large%20language%20models%20%28LLMs%29%20to%20handle%0Ainteractions%2C%20along%20with%20a%20stable%20diffusion%20%28SD%29%20based%20agent%20for%20generating%0Ahigh-quality%20images.%20Specifically%2C%20AutoStudio%20consists%20of%20%28i%29%20a%20subject%20manager%0Ato%20interpret%20interaction%20dialogues%20and%20manage%20the%20context%20of%20each%20subject%2C%20%28ii%29%0Aa%20layout%20generator%20to%20generate%20fine-grained%20bounding%20boxes%20to%20control%20subject%0Alocations%2C%20%28iii%29%20a%20supervisor%20to%20provide%20suggestions%20for%20layout%20refinements%2C%0Aand%20%28iv%29%20a%20drawer%20to%20complete%20image%20generation.%20Furthermore%2C%20we%20introduce%20a%0AParallel-UNet%20to%20replace%20the%20original%20UNet%20in%20the%20drawer%2C%20which%20employs%20two%0Aparallel%20cross-attention%20modules%20for%20exploiting%20subject-aware%20features.%20We%20also%0Aintroduce%20a%20subject-initialized%20generation%20method%20to%20better%20preserve%20small%0Asubjects.%20Our%20AutoStudio%20hereby%20can%20generate%20a%20sequence%20of%20multi-subject%20images%0Ainteractively%20and%20consistently.%20Extensive%20experiments%20on%20the%20public%20CMIGBench%0Abenchmark%20and%20human%20evaluations%20show%20that%20AutoStudio%20maintains%20multi-subject%0Aconsistency%20across%20multiple%20turns%20well%2C%20and%20it%20also%20raises%20the%20state-of-the-art%0Aperformance%20by%2013.65%25%20in%20average%20Frechet%20Inception%20Distance%20and%202.83%25%20in%0Aaverage%20character-character%20similarity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01388v3&entry.124074799=Read"},
{"title": "U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound\n  Understanding", "author": "Anjie Le and Henan Liu and Yue Wang and Zhenyu Liu and Rongkun Zhu and Taohan Weng and Jinze Yu and Boyang Wang and Yalun Wu and Kaiwen Yan and Quanlin Sun and Meirui Jiang and Jialun Pei and Siya Liu and Haoyun Zheng and Zhoujun Li and Alison Noble and Jacques Souquet and Xiaoqing Guo and Manxi Lin and Hongcheng Guo", "abstract": "  Ultrasound is a widely-used imaging modality critical to global healthcare,\nyet its interpretation remains challenging due to its varying image quality on\noperators, noises, and anatomical structures. Although large vision-language\nmodels (LVLMs) have demonstrated impressive multimodal capabilities across\nnatural and medical domains, their performance on ultrasound remains largely\nunexplored. We introduce U2-BENCH, the first comprehensive benchmark to\nevaluate LVLMs on ultrasound understanding across classification, detection,\nregression, and text generation tasks. U2-BENCH aggregates 7,241 cases spanning\n15 anatomical regions and defines 8 clinically inspired tasks, such as\ndiagnosis, view recognition, lesion localization, clinical value estimation,\nand report generation, across 50 ultrasound application scenarios. We evaluate\n20 state-of-the-art LVLMs, both open- and closed-source, general-purpose and\nmedical-specific. Our results reveal strong performance on image-level\nclassification, but persistent challenges in spatial reasoning and clinical\nlanguage generation. U2-BENCH establishes a rigorous and unified testbed to\nassess and accelerate LVLM research in the uniquely multimodal domain of\nmedical ultrasound imaging.\n", "link": "http://arxiv.org/abs/2505.17779v2", "date": "2025-05-30", "relevancy": 2.398, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6076}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6076}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20U2-BENCH%3A%20Benchmarking%20Large%20Vision-Language%20Models%20on%20Ultrasound%0A%20%20Understanding&body=Title%3A%20U2-BENCH%3A%20Benchmarking%20Large%20Vision-Language%20Models%20on%20Ultrasound%0A%20%20Understanding%0AAuthor%3A%20Anjie%20Le%20and%20Henan%20Liu%20and%20Yue%20Wang%20and%20Zhenyu%20Liu%20and%20Rongkun%20Zhu%20and%20Taohan%20Weng%20and%20Jinze%20Yu%20and%20Boyang%20Wang%20and%20Yalun%20Wu%20and%20Kaiwen%20Yan%20and%20Quanlin%20Sun%20and%20Meirui%20Jiang%20and%20Jialun%20Pei%20and%20Siya%20Liu%20and%20Haoyun%20Zheng%20and%20Zhoujun%20Li%20and%20Alison%20Noble%20and%20Jacques%20Souquet%20and%20Xiaoqing%20Guo%20and%20Manxi%20Lin%20and%20Hongcheng%20Guo%0AAbstract%3A%20%20%20Ultrasound%20is%20a%20widely-used%20imaging%20modality%20critical%20to%20global%20healthcare%2C%0Ayet%20its%20interpretation%20remains%20challenging%20due%20to%20its%20varying%20image%20quality%20on%0Aoperators%2C%20noises%2C%20and%20anatomical%20structures.%20Although%20large%20vision-language%0Amodels%20%28LVLMs%29%20have%20demonstrated%20impressive%20multimodal%20capabilities%20across%0Anatural%20and%20medical%20domains%2C%20their%20performance%20on%20ultrasound%20remains%20largely%0Aunexplored.%20We%20introduce%20U2-BENCH%2C%20the%20first%20comprehensive%20benchmark%20to%0Aevaluate%20LVLMs%20on%20ultrasound%20understanding%20across%20classification%2C%20detection%2C%0Aregression%2C%20and%20text%20generation%20tasks.%20U2-BENCH%20aggregates%207%2C241%20cases%20spanning%0A15%20anatomical%20regions%20and%20defines%208%20clinically%20inspired%20tasks%2C%20such%20as%0Adiagnosis%2C%20view%20recognition%2C%20lesion%20localization%2C%20clinical%20value%20estimation%2C%0Aand%20report%20generation%2C%20across%2050%20ultrasound%20application%20scenarios.%20We%20evaluate%0A20%20state-of-the-art%20LVLMs%2C%20both%20open-%20and%20closed-source%2C%20general-purpose%20and%0Amedical-specific.%20Our%20results%20reveal%20strong%20performance%20on%20image-level%0Aclassification%2C%20but%20persistent%20challenges%20in%20spatial%20reasoning%20and%20clinical%0Alanguage%20generation.%20U2-BENCH%20establishes%20a%20rigorous%20and%20unified%20testbed%20to%0Aassess%20and%20accelerate%20LVLM%20research%20in%20the%20uniquely%20multimodal%20domain%20of%0Amedical%20ultrasound%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17779v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DU2-BENCH%253A%2520Benchmarking%2520Large%2520Vision-Language%2520Models%2520on%2520Ultrasound%250A%2520%2520Understanding%26entry.906535625%3DAnjie%2520Le%2520and%2520Henan%2520Liu%2520and%2520Yue%2520Wang%2520and%2520Zhenyu%2520Liu%2520and%2520Rongkun%2520Zhu%2520and%2520Taohan%2520Weng%2520and%2520Jinze%2520Yu%2520and%2520Boyang%2520Wang%2520and%2520Yalun%2520Wu%2520and%2520Kaiwen%2520Yan%2520and%2520Quanlin%2520Sun%2520and%2520Meirui%2520Jiang%2520and%2520Jialun%2520Pei%2520and%2520Siya%2520Liu%2520and%2520Haoyun%2520Zheng%2520and%2520Zhoujun%2520Li%2520and%2520Alison%2520Noble%2520and%2520Jacques%2520Souquet%2520and%2520Xiaoqing%2520Guo%2520and%2520Manxi%2520Lin%2520and%2520Hongcheng%2520Guo%26entry.1292438233%3D%2520%2520Ultrasound%2520is%2520a%2520widely-used%2520imaging%2520modality%2520critical%2520to%2520global%2520healthcare%252C%250Ayet%2520its%2520interpretation%2520remains%2520challenging%2520due%2520to%2520its%2520varying%2520image%2520quality%2520on%250Aoperators%252C%2520noises%252C%2520and%2520anatomical%2520structures.%2520Although%2520large%2520vision-language%250Amodels%2520%2528LVLMs%2529%2520have%2520demonstrated%2520impressive%2520multimodal%2520capabilities%2520across%250Anatural%2520and%2520medical%2520domains%252C%2520their%2520performance%2520on%2520ultrasound%2520remains%2520largely%250Aunexplored.%2520We%2520introduce%2520U2-BENCH%252C%2520the%2520first%2520comprehensive%2520benchmark%2520to%250Aevaluate%2520LVLMs%2520on%2520ultrasound%2520understanding%2520across%2520classification%252C%2520detection%252C%250Aregression%252C%2520and%2520text%2520generation%2520tasks.%2520U2-BENCH%2520aggregates%25207%252C241%2520cases%2520spanning%250A15%2520anatomical%2520regions%2520and%2520defines%25208%2520clinically%2520inspired%2520tasks%252C%2520such%2520as%250Adiagnosis%252C%2520view%2520recognition%252C%2520lesion%2520localization%252C%2520clinical%2520value%2520estimation%252C%250Aand%2520report%2520generation%252C%2520across%252050%2520ultrasound%2520application%2520scenarios.%2520We%2520evaluate%250A20%2520state-of-the-art%2520LVLMs%252C%2520both%2520open-%2520and%2520closed-source%252C%2520general-purpose%2520and%250Amedical-specific.%2520Our%2520results%2520reveal%2520strong%2520performance%2520on%2520image-level%250Aclassification%252C%2520but%2520persistent%2520challenges%2520in%2520spatial%2520reasoning%2520and%2520clinical%250Alanguage%2520generation.%2520U2-BENCH%2520establishes%2520a%2520rigorous%2520and%2520unified%2520testbed%2520to%250Aassess%2520and%2520accelerate%2520LVLM%2520research%2520in%2520the%2520uniquely%2520multimodal%2520domain%2520of%250Amedical%2520ultrasound%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17779v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=U2-BENCH%3A%20Benchmarking%20Large%20Vision-Language%20Models%20on%20Ultrasound%0A%20%20Understanding&entry.906535625=Anjie%20Le%20and%20Henan%20Liu%20and%20Yue%20Wang%20and%20Zhenyu%20Liu%20and%20Rongkun%20Zhu%20and%20Taohan%20Weng%20and%20Jinze%20Yu%20and%20Boyang%20Wang%20and%20Yalun%20Wu%20and%20Kaiwen%20Yan%20and%20Quanlin%20Sun%20and%20Meirui%20Jiang%20and%20Jialun%20Pei%20and%20Siya%20Liu%20and%20Haoyun%20Zheng%20and%20Zhoujun%20Li%20and%20Alison%20Noble%20and%20Jacques%20Souquet%20and%20Xiaoqing%20Guo%20and%20Manxi%20Lin%20and%20Hongcheng%20Guo&entry.1292438233=%20%20Ultrasound%20is%20a%20widely-used%20imaging%20modality%20critical%20to%20global%20healthcare%2C%0Ayet%20its%20interpretation%20remains%20challenging%20due%20to%20its%20varying%20image%20quality%20on%0Aoperators%2C%20noises%2C%20and%20anatomical%20structures.%20Although%20large%20vision-language%0Amodels%20%28LVLMs%29%20have%20demonstrated%20impressive%20multimodal%20capabilities%20across%0Anatural%20and%20medical%20domains%2C%20their%20performance%20on%20ultrasound%20remains%20largely%0Aunexplored.%20We%20introduce%20U2-BENCH%2C%20the%20first%20comprehensive%20benchmark%20to%0Aevaluate%20LVLMs%20on%20ultrasound%20understanding%20across%20classification%2C%20detection%2C%0Aregression%2C%20and%20text%20generation%20tasks.%20U2-BENCH%20aggregates%207%2C241%20cases%20spanning%0A15%20anatomical%20regions%20and%20defines%208%20clinically%20inspired%20tasks%2C%20such%20as%0Adiagnosis%2C%20view%20recognition%2C%20lesion%20localization%2C%20clinical%20value%20estimation%2C%0Aand%20report%20generation%2C%20across%2050%20ultrasound%20application%20scenarios.%20We%20evaluate%0A20%20state-of-the-art%20LVLMs%2C%20both%20open-%20and%20closed-source%2C%20general-purpose%20and%0Amedical-specific.%20Our%20results%20reveal%20strong%20performance%20on%20image-level%0Aclassification%2C%20but%20persistent%20challenges%20in%20spatial%20reasoning%20and%20clinical%0Alanguage%20generation.%20U2-BENCH%20establishes%20a%20rigorous%20and%20unified%20testbed%20to%0Aassess%20and%20accelerate%20LVLM%20research%20in%20the%20uniquely%20multimodal%20domain%20of%0Amedical%20ultrasound%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17779v2&entry.124074799=Read"},
{"title": "Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic\n  Tasks", "author": "Tajamul Ashraf and Amal Saqib and Hanan Ghani and Muhra AlMahri and Yuhao Li and Noor Ahsan and Umair Nawaz and Jean Lahoud and Hisham Cholakkal and Mubarak Shah and Philip Torr and Fahad Shahbaz Khan and Rao Muhammad Anwer and Salman Khan", "abstract": "  Deep reasoning is fundamental for solving complex tasks, especially in\nvision-centric scenarios that demand sequential, multimodal understanding.\nHowever, existing benchmarks typically evaluate agents with fully synthetic,\nsingle-turn queries, limited visual modalities, and lack a framework to assess\nreasoning quality over multiple steps as required in real-world settings. To\naddress this, we introduce Agent-X, a large-scale benchmark for evaluating\nvision-centric agents multi-step and deep reasoning capabilities in real-world,\nmultimodal settings. Agent- X features 828 agentic tasks with authentic visual\ncontexts, including images, multi-image comparisons, videos, and instructional\ntext. These tasks span six major agentic environments: general visual\nreasoning, web browsing, security and surveillance, autonomous driving, sports,\nand math reasoning. Our benchmark requires agents to integrate tool use with\nexplicit, stepwise decision-making in these diverse settings. In addition, we\npropose a fine-grained, step-level evaluation framework that assesses the\ncorrectness and logical coherence of each reasoning step and the effectiveness\nof tool usage throughout the task. Our results reveal that even the\nbest-performing models, including GPT, Gemini, and Qwen families, struggle to\nsolve multi-step vision tasks, achieving less than 50% full-chain success.\nThese findings highlight key bottlenecks in current LMM reasoning and tool-use\ncapabilities and identify future research directions in vision-centric agentic\nreasoning models. Our data and code are publicly available at\nhttps://github.com/mbzuai-oryx/Agent-X\n", "link": "http://arxiv.org/abs/2505.24876v1", "date": "2025-05-30", "relevancy": 2.3953, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.603}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.603}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5782}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agent-X%3A%20Evaluating%20Deep%20Multimodal%20Reasoning%20in%20Vision-Centric%20Agentic%0A%20%20Tasks&body=Title%3A%20Agent-X%3A%20Evaluating%20Deep%20Multimodal%20Reasoning%20in%20Vision-Centric%20Agentic%0A%20%20Tasks%0AAuthor%3A%20Tajamul%20Ashraf%20and%20Amal%20Saqib%20and%20Hanan%20Ghani%20and%20Muhra%20AlMahri%20and%20Yuhao%20Li%20and%20Noor%20Ahsan%20and%20Umair%20Nawaz%20and%20Jean%20Lahoud%20and%20Hisham%20Cholakkal%20and%20Mubarak%20Shah%20and%20Philip%20Torr%20and%20Fahad%20Shahbaz%20Khan%20and%20Rao%20Muhammad%20Anwer%20and%20Salman%20Khan%0AAbstract%3A%20%20%20Deep%20reasoning%20is%20fundamental%20for%20solving%20complex%20tasks%2C%20especially%20in%0Avision-centric%20scenarios%20that%20demand%20sequential%2C%20multimodal%20understanding.%0AHowever%2C%20existing%20benchmarks%20typically%20evaluate%20agents%20with%20fully%20synthetic%2C%0Asingle-turn%20queries%2C%20limited%20visual%20modalities%2C%20and%20lack%20a%20framework%20to%20assess%0Areasoning%20quality%20over%20multiple%20steps%20as%20required%20in%20real-world%20settings.%20To%0Aaddress%20this%2C%20we%20introduce%20Agent-X%2C%20a%20large-scale%20benchmark%20for%20evaluating%0Avision-centric%20agents%20multi-step%20and%20deep%20reasoning%20capabilities%20in%20real-world%2C%0Amultimodal%20settings.%20Agent-%20X%20features%20828%20agentic%20tasks%20with%20authentic%20visual%0Acontexts%2C%20including%20images%2C%20multi-image%20comparisons%2C%20videos%2C%20and%20instructional%0Atext.%20These%20tasks%20span%20six%20major%20agentic%20environments%3A%20general%20visual%0Areasoning%2C%20web%20browsing%2C%20security%20and%20surveillance%2C%20autonomous%20driving%2C%20sports%2C%0Aand%20math%20reasoning.%20Our%20benchmark%20requires%20agents%20to%20integrate%20tool%20use%20with%0Aexplicit%2C%20stepwise%20decision-making%20in%20these%20diverse%20settings.%20In%20addition%2C%20we%0Apropose%20a%20fine-grained%2C%20step-level%20evaluation%20framework%20that%20assesses%20the%0Acorrectness%20and%20logical%20coherence%20of%20each%20reasoning%20step%20and%20the%20effectiveness%0Aof%20tool%20usage%20throughout%20the%20task.%20Our%20results%20reveal%20that%20even%20the%0Abest-performing%20models%2C%20including%20GPT%2C%20Gemini%2C%20and%20Qwen%20families%2C%20struggle%20to%0Asolve%20multi-step%20vision%20tasks%2C%20achieving%20less%20than%2050%25%20full-chain%20success.%0AThese%20findings%20highlight%20key%20bottlenecks%20in%20current%20LMM%20reasoning%20and%20tool-use%0Acapabilities%20and%20identify%20future%20research%20directions%20in%20vision-centric%20agentic%0Areasoning%20models.%20Our%20data%20and%20code%20are%20publicly%20available%20at%0Ahttps%3A//github.com/mbzuai-oryx/Agent-X%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24876v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgent-X%253A%2520Evaluating%2520Deep%2520Multimodal%2520Reasoning%2520in%2520Vision-Centric%2520Agentic%250A%2520%2520Tasks%26entry.906535625%3DTajamul%2520Ashraf%2520and%2520Amal%2520Saqib%2520and%2520Hanan%2520Ghani%2520and%2520Muhra%2520AlMahri%2520and%2520Yuhao%2520Li%2520and%2520Noor%2520Ahsan%2520and%2520Umair%2520Nawaz%2520and%2520Jean%2520Lahoud%2520and%2520Hisham%2520Cholakkal%2520and%2520Mubarak%2520Shah%2520and%2520Philip%2520Torr%2520and%2520Fahad%2520Shahbaz%2520Khan%2520and%2520Rao%2520Muhammad%2520Anwer%2520and%2520Salman%2520Khan%26entry.1292438233%3D%2520%2520Deep%2520reasoning%2520is%2520fundamental%2520for%2520solving%2520complex%2520tasks%252C%2520especially%2520in%250Avision-centric%2520scenarios%2520that%2520demand%2520sequential%252C%2520multimodal%2520understanding.%250AHowever%252C%2520existing%2520benchmarks%2520typically%2520evaluate%2520agents%2520with%2520fully%2520synthetic%252C%250Asingle-turn%2520queries%252C%2520limited%2520visual%2520modalities%252C%2520and%2520lack%2520a%2520framework%2520to%2520assess%250Areasoning%2520quality%2520over%2520multiple%2520steps%2520as%2520required%2520in%2520real-world%2520settings.%2520To%250Aaddress%2520this%252C%2520we%2520introduce%2520Agent-X%252C%2520a%2520large-scale%2520benchmark%2520for%2520evaluating%250Avision-centric%2520agents%2520multi-step%2520and%2520deep%2520reasoning%2520capabilities%2520in%2520real-world%252C%250Amultimodal%2520settings.%2520Agent-%2520X%2520features%2520828%2520agentic%2520tasks%2520with%2520authentic%2520visual%250Acontexts%252C%2520including%2520images%252C%2520multi-image%2520comparisons%252C%2520videos%252C%2520and%2520instructional%250Atext.%2520These%2520tasks%2520span%2520six%2520major%2520agentic%2520environments%253A%2520general%2520visual%250Areasoning%252C%2520web%2520browsing%252C%2520security%2520and%2520surveillance%252C%2520autonomous%2520driving%252C%2520sports%252C%250Aand%2520math%2520reasoning.%2520Our%2520benchmark%2520requires%2520agents%2520to%2520integrate%2520tool%2520use%2520with%250Aexplicit%252C%2520stepwise%2520decision-making%2520in%2520these%2520diverse%2520settings.%2520In%2520addition%252C%2520we%250Apropose%2520a%2520fine-grained%252C%2520step-level%2520evaluation%2520framework%2520that%2520assesses%2520the%250Acorrectness%2520and%2520logical%2520coherence%2520of%2520each%2520reasoning%2520step%2520and%2520the%2520effectiveness%250Aof%2520tool%2520usage%2520throughout%2520the%2520task.%2520Our%2520results%2520reveal%2520that%2520even%2520the%250Abest-performing%2520models%252C%2520including%2520GPT%252C%2520Gemini%252C%2520and%2520Qwen%2520families%252C%2520struggle%2520to%250Asolve%2520multi-step%2520vision%2520tasks%252C%2520achieving%2520less%2520than%252050%2525%2520full-chain%2520success.%250AThese%2520findings%2520highlight%2520key%2520bottlenecks%2520in%2520current%2520LMM%2520reasoning%2520and%2520tool-use%250Acapabilities%2520and%2520identify%2520future%2520research%2520directions%2520in%2520vision-centric%2520agentic%250Areasoning%2520models.%2520Our%2520data%2520and%2520code%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/mbzuai-oryx/Agent-X%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24876v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agent-X%3A%20Evaluating%20Deep%20Multimodal%20Reasoning%20in%20Vision-Centric%20Agentic%0A%20%20Tasks&entry.906535625=Tajamul%20Ashraf%20and%20Amal%20Saqib%20and%20Hanan%20Ghani%20and%20Muhra%20AlMahri%20and%20Yuhao%20Li%20and%20Noor%20Ahsan%20and%20Umair%20Nawaz%20and%20Jean%20Lahoud%20and%20Hisham%20Cholakkal%20and%20Mubarak%20Shah%20and%20Philip%20Torr%20and%20Fahad%20Shahbaz%20Khan%20and%20Rao%20Muhammad%20Anwer%20and%20Salman%20Khan&entry.1292438233=%20%20Deep%20reasoning%20is%20fundamental%20for%20solving%20complex%20tasks%2C%20especially%20in%0Avision-centric%20scenarios%20that%20demand%20sequential%2C%20multimodal%20understanding.%0AHowever%2C%20existing%20benchmarks%20typically%20evaluate%20agents%20with%20fully%20synthetic%2C%0Asingle-turn%20queries%2C%20limited%20visual%20modalities%2C%20and%20lack%20a%20framework%20to%20assess%0Areasoning%20quality%20over%20multiple%20steps%20as%20required%20in%20real-world%20settings.%20To%0Aaddress%20this%2C%20we%20introduce%20Agent-X%2C%20a%20large-scale%20benchmark%20for%20evaluating%0Avision-centric%20agents%20multi-step%20and%20deep%20reasoning%20capabilities%20in%20real-world%2C%0Amultimodal%20settings.%20Agent-%20X%20features%20828%20agentic%20tasks%20with%20authentic%20visual%0Acontexts%2C%20including%20images%2C%20multi-image%20comparisons%2C%20videos%2C%20and%20instructional%0Atext.%20These%20tasks%20span%20six%20major%20agentic%20environments%3A%20general%20visual%0Areasoning%2C%20web%20browsing%2C%20security%20and%20surveillance%2C%20autonomous%20driving%2C%20sports%2C%0Aand%20math%20reasoning.%20Our%20benchmark%20requires%20agents%20to%20integrate%20tool%20use%20with%0Aexplicit%2C%20stepwise%20decision-making%20in%20these%20diverse%20settings.%20In%20addition%2C%20we%0Apropose%20a%20fine-grained%2C%20step-level%20evaluation%20framework%20that%20assesses%20the%0Acorrectness%20and%20logical%20coherence%20of%20each%20reasoning%20step%20and%20the%20effectiveness%0Aof%20tool%20usage%20throughout%20the%20task.%20Our%20results%20reveal%20that%20even%20the%0Abest-performing%20models%2C%20including%20GPT%2C%20Gemini%2C%20and%20Qwen%20families%2C%20struggle%20to%0Asolve%20multi-step%20vision%20tasks%2C%20achieving%20less%20than%2050%25%20full-chain%20success.%0AThese%20findings%20highlight%20key%20bottlenecks%20in%20current%20LMM%20reasoning%20and%20tool-use%0Acapabilities%20and%20identify%20future%20research%20directions%20in%20vision-centric%20agentic%0Areasoning%20models.%20Our%20data%20and%20code%20are%20publicly%20available%20at%0Ahttps%3A//github.com/mbzuai-oryx/Agent-X%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24876v1&entry.124074799=Read"},
{"title": "Evaluating Gemini in an arena for learning", "author": " LearnLM Team and Abhinit Modi and Aditya Srikanth Veerubhotla and Aliya Rysbek and Andrea Huber and Ankit Anand and Avishkar Bhoopchand and Brett Wiltshire and Daniel Gillick and Daniel Kasenberg and Eleni Sgouritsa and Gal Elidan and Hengrui Liu and Holger Winnemoeller and Irina Jurenka and James Cohan and Jennifer She and Julia Wilkowski and Kaiz Alarakyia and Kevin R. McKee and Komal Singh and Lisa Wang and Markus Kunesch and Miruna P\u00eeslar and Niv Efron and Parsa Mahmoudieh and Pierre-Alexandre Kamienny and Sara Wiltberger and Shakir Mohamed and Shashank Agarwal and Shubham Milind Phal and Sun Jae Lee and Theofilos Strinopoulos and Wei-Jen Ko and Yael Gold-Zamir and Yael Haramaty and Yannis Assael", "abstract": "  Artificial intelligence (AI) is poised to transform education, but the\nresearch community lacks a robust, general benchmark to evaluate AI models for\nlearning. To assess state-of-the-art support for educational use cases, we ran\nan \"arena for learning\" where educators and pedagogy experts conduct blind,\nhead-to-head, multi-turn comparisons of leading AI models. In particular, $N =\n189$ educators drew from their experience to role-play realistic learning use\ncases, interacting with two models sequentially, after which $N = 206$ experts\njudged which model better supported the user's learning goals. The arena\nevaluated a slate of state-of-the-art models: Gemini 2.5 Pro, Claude 3.7\nSonnet, GPT-4o, and OpenAI o3. Excluding ties, experts preferred Gemini 2.5 Pro\nin 73.2% of these match-ups -- ranking it first overall in the arena. Gemini\n2.5 Pro also demonstrated markedly higher performance across key principles of\ngood pedagogy. Altogether, these results position Gemini 2.5 Pro as a leading\nmodel for learning.\n", "link": "http://arxiv.org/abs/2505.24477v1", "date": "2025-05-30", "relevancy": 2.3791, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5052}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4611}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Gemini%20in%20an%20arena%20for%20learning&body=Title%3A%20Evaluating%20Gemini%20in%20an%20arena%20for%20learning%0AAuthor%3A%20%20LearnLM%20Team%20and%20Abhinit%20Modi%20and%20Aditya%20Srikanth%20Veerubhotla%20and%20Aliya%20Rysbek%20and%20Andrea%20Huber%20and%20Ankit%20Anand%20and%20Avishkar%20Bhoopchand%20and%20Brett%20Wiltshire%20and%20Daniel%20Gillick%20and%20Daniel%20Kasenberg%20and%20Eleni%20Sgouritsa%20and%20Gal%20Elidan%20and%20Hengrui%20Liu%20and%20Holger%20Winnemoeller%20and%20Irina%20Jurenka%20and%20James%20Cohan%20and%20Jennifer%20She%20and%20Julia%20Wilkowski%20and%20Kaiz%20Alarakyia%20and%20Kevin%20R.%20McKee%20and%20Komal%20Singh%20and%20Lisa%20Wang%20and%20Markus%20Kunesch%20and%20Miruna%20P%C3%AEslar%20and%20Niv%20Efron%20and%20Parsa%20Mahmoudieh%20and%20Pierre-Alexandre%20Kamienny%20and%20Sara%20Wiltberger%20and%20Shakir%20Mohamed%20and%20Shashank%20Agarwal%20and%20Shubham%20Milind%20Phal%20and%20Sun%20Jae%20Lee%20and%20Theofilos%20Strinopoulos%20and%20Wei-Jen%20Ko%20and%20Yael%20Gold-Zamir%20and%20Yael%20Haramaty%20and%20Yannis%20Assael%0AAbstract%3A%20%20%20Artificial%20intelligence%20%28AI%29%20is%20poised%20to%20transform%20education%2C%20but%20the%0Aresearch%20community%20lacks%20a%20robust%2C%20general%20benchmark%20to%20evaluate%20AI%20models%20for%0Alearning.%20To%20assess%20state-of-the-art%20support%20for%20educational%20use%20cases%2C%20we%20ran%0Aan%20%22arena%20for%20learning%22%20where%20educators%20and%20pedagogy%20experts%20conduct%20blind%2C%0Ahead-to-head%2C%20multi-turn%20comparisons%20of%20leading%20AI%20models.%20In%20particular%2C%20%24N%20%3D%0A189%24%20educators%20drew%20from%20their%20experience%20to%20role-play%20realistic%20learning%20use%0Acases%2C%20interacting%20with%20two%20models%20sequentially%2C%20after%20which%20%24N%20%3D%20206%24%20experts%0Ajudged%20which%20model%20better%20supported%20the%20user%27s%20learning%20goals.%20The%20arena%0Aevaluated%20a%20slate%20of%20state-of-the-art%20models%3A%20Gemini%202.5%20Pro%2C%20Claude%203.7%0ASonnet%2C%20GPT-4o%2C%20and%20OpenAI%20o3.%20Excluding%20ties%2C%20experts%20preferred%20Gemini%202.5%20Pro%0Ain%2073.2%25%20of%20these%20match-ups%20--%20ranking%20it%20first%20overall%20in%20the%20arena.%20Gemini%0A2.5%20Pro%20also%20demonstrated%20markedly%20higher%20performance%20across%20key%20principles%20of%0Agood%20pedagogy.%20Altogether%2C%20these%20results%20position%20Gemini%202.5%20Pro%20as%20a%20leading%0Amodel%20for%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24477v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Gemini%2520in%2520an%2520arena%2520for%2520learning%26entry.906535625%3D%2520LearnLM%2520Team%2520and%2520Abhinit%2520Modi%2520and%2520Aditya%2520Srikanth%2520Veerubhotla%2520and%2520Aliya%2520Rysbek%2520and%2520Andrea%2520Huber%2520and%2520Ankit%2520Anand%2520and%2520Avishkar%2520Bhoopchand%2520and%2520Brett%2520Wiltshire%2520and%2520Daniel%2520Gillick%2520and%2520Daniel%2520Kasenberg%2520and%2520Eleni%2520Sgouritsa%2520and%2520Gal%2520Elidan%2520and%2520Hengrui%2520Liu%2520and%2520Holger%2520Winnemoeller%2520and%2520Irina%2520Jurenka%2520and%2520James%2520Cohan%2520and%2520Jennifer%2520She%2520and%2520Julia%2520Wilkowski%2520and%2520Kaiz%2520Alarakyia%2520and%2520Kevin%2520R.%2520McKee%2520and%2520Komal%2520Singh%2520and%2520Lisa%2520Wang%2520and%2520Markus%2520Kunesch%2520and%2520Miruna%2520P%25C3%25AEslar%2520and%2520Niv%2520Efron%2520and%2520Parsa%2520Mahmoudieh%2520and%2520Pierre-Alexandre%2520Kamienny%2520and%2520Sara%2520Wiltberger%2520and%2520Shakir%2520Mohamed%2520and%2520Shashank%2520Agarwal%2520and%2520Shubham%2520Milind%2520Phal%2520and%2520Sun%2520Jae%2520Lee%2520and%2520Theofilos%2520Strinopoulos%2520and%2520Wei-Jen%2520Ko%2520and%2520Yael%2520Gold-Zamir%2520and%2520Yael%2520Haramaty%2520and%2520Yannis%2520Assael%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520%2528AI%2529%2520is%2520poised%2520to%2520transform%2520education%252C%2520but%2520the%250Aresearch%2520community%2520lacks%2520a%2520robust%252C%2520general%2520benchmark%2520to%2520evaluate%2520AI%2520models%2520for%250Alearning.%2520To%2520assess%2520state-of-the-art%2520support%2520for%2520educational%2520use%2520cases%252C%2520we%2520ran%250Aan%2520%2522arena%2520for%2520learning%2522%2520where%2520educators%2520and%2520pedagogy%2520experts%2520conduct%2520blind%252C%250Ahead-to-head%252C%2520multi-turn%2520comparisons%2520of%2520leading%2520AI%2520models.%2520In%2520particular%252C%2520%2524N%2520%253D%250A189%2524%2520educators%2520drew%2520from%2520their%2520experience%2520to%2520role-play%2520realistic%2520learning%2520use%250Acases%252C%2520interacting%2520with%2520two%2520models%2520sequentially%252C%2520after%2520which%2520%2524N%2520%253D%2520206%2524%2520experts%250Ajudged%2520which%2520model%2520better%2520supported%2520the%2520user%2527s%2520learning%2520goals.%2520The%2520arena%250Aevaluated%2520a%2520slate%2520of%2520state-of-the-art%2520models%253A%2520Gemini%25202.5%2520Pro%252C%2520Claude%25203.7%250ASonnet%252C%2520GPT-4o%252C%2520and%2520OpenAI%2520o3.%2520Excluding%2520ties%252C%2520experts%2520preferred%2520Gemini%25202.5%2520Pro%250Ain%252073.2%2525%2520of%2520these%2520match-ups%2520--%2520ranking%2520it%2520first%2520overall%2520in%2520the%2520arena.%2520Gemini%250A2.5%2520Pro%2520also%2520demonstrated%2520markedly%2520higher%2520performance%2520across%2520key%2520principles%2520of%250Agood%2520pedagogy.%2520Altogether%252C%2520these%2520results%2520position%2520Gemini%25202.5%2520Pro%2520as%2520a%2520leading%250Amodel%2520for%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24477v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Gemini%20in%20an%20arena%20for%20learning&entry.906535625=%20LearnLM%20Team%20and%20Abhinit%20Modi%20and%20Aditya%20Srikanth%20Veerubhotla%20and%20Aliya%20Rysbek%20and%20Andrea%20Huber%20and%20Ankit%20Anand%20and%20Avishkar%20Bhoopchand%20and%20Brett%20Wiltshire%20and%20Daniel%20Gillick%20and%20Daniel%20Kasenberg%20and%20Eleni%20Sgouritsa%20and%20Gal%20Elidan%20and%20Hengrui%20Liu%20and%20Holger%20Winnemoeller%20and%20Irina%20Jurenka%20and%20James%20Cohan%20and%20Jennifer%20She%20and%20Julia%20Wilkowski%20and%20Kaiz%20Alarakyia%20and%20Kevin%20R.%20McKee%20and%20Komal%20Singh%20and%20Lisa%20Wang%20and%20Markus%20Kunesch%20and%20Miruna%20P%C3%AEslar%20and%20Niv%20Efron%20and%20Parsa%20Mahmoudieh%20and%20Pierre-Alexandre%20Kamienny%20and%20Sara%20Wiltberger%20and%20Shakir%20Mohamed%20and%20Shashank%20Agarwal%20and%20Shubham%20Milind%20Phal%20and%20Sun%20Jae%20Lee%20and%20Theofilos%20Strinopoulos%20and%20Wei-Jen%20Ko%20and%20Yael%20Gold-Zamir%20and%20Yael%20Haramaty%20and%20Yannis%20Assael&entry.1292438233=%20%20Artificial%20intelligence%20%28AI%29%20is%20poised%20to%20transform%20education%2C%20but%20the%0Aresearch%20community%20lacks%20a%20robust%2C%20general%20benchmark%20to%20evaluate%20AI%20models%20for%0Alearning.%20To%20assess%20state-of-the-art%20support%20for%20educational%20use%20cases%2C%20we%20ran%0Aan%20%22arena%20for%20learning%22%20where%20educators%20and%20pedagogy%20experts%20conduct%20blind%2C%0Ahead-to-head%2C%20multi-turn%20comparisons%20of%20leading%20AI%20models.%20In%20particular%2C%20%24N%20%3D%0A189%24%20educators%20drew%20from%20their%20experience%20to%20role-play%20realistic%20learning%20use%0Acases%2C%20interacting%20with%20two%20models%20sequentially%2C%20after%20which%20%24N%20%3D%20206%24%20experts%0Ajudged%20which%20model%20better%20supported%20the%20user%27s%20learning%20goals.%20The%20arena%0Aevaluated%20a%20slate%20of%20state-of-the-art%20models%3A%20Gemini%202.5%20Pro%2C%20Claude%203.7%0ASonnet%2C%20GPT-4o%2C%20and%20OpenAI%20o3.%20Excluding%20ties%2C%20experts%20preferred%20Gemini%202.5%20Pro%0Ain%2073.2%25%20of%20these%20match-ups%20--%20ranking%20it%20first%20overall%20in%20the%20arena.%20Gemini%0A2.5%20Pro%20also%20demonstrated%20markedly%20higher%20performance%20across%20key%20principles%20of%0Agood%20pedagogy.%20Altogether%2C%20these%20results%20position%20Gemini%202.5%20Pro%20as%20a%20leading%0Amodel%20for%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24477v1&entry.124074799=Read"},
{"title": "A Survey on Text-Driven 360-Degree Panorama Generation", "author": "Hai Wang and Xiaoyu Xiang and Weihao Xia and Jing-Hao Xue", "abstract": "  The advent of text-driven 360-degree panorama generation, enabling the\nsynthesis of 360-degree panoramic images directly from textual descriptions,\nmarks a transformative advancement in immersive visual content creation. This\ninnovation significantly simplifies the traditionally complex process of\nproducing such content. Recent progress in text-to-image diffusion models has\naccelerated the rapid development in this emerging field. This survey presents\na comprehensive review of text-driven 360-degree panorama generation, offering\nan in-depth analysis of state-of-the-art algorithms and their expanding\napplications in 360-degree 3D scene generation. Furthermore, we critically\nexamine current limitations and propose promising directions for future\nresearch. A curated project page with relevant resources and research papers is\navailable at https://littlewhitesea.github.io/Text-Driven-Pano-Gen/.\n", "link": "http://arxiv.org/abs/2502.14799v2", "date": "2025-05-30", "relevancy": 2.3766, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6028}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6028}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Text-Driven%20360-Degree%20Panorama%20Generation&body=Title%3A%20A%20Survey%20on%20Text-Driven%20360-Degree%20Panorama%20Generation%0AAuthor%3A%20Hai%20Wang%20and%20Xiaoyu%20Xiang%20and%20Weihao%20Xia%20and%20Jing-Hao%20Xue%0AAbstract%3A%20%20%20The%20advent%20of%20text-driven%20360-degree%20panorama%20generation%2C%20enabling%20the%0Asynthesis%20of%20360-degree%20panoramic%20images%20directly%20from%20textual%20descriptions%2C%0Amarks%20a%20transformative%20advancement%20in%20immersive%20visual%20content%20creation.%20This%0Ainnovation%20significantly%20simplifies%20the%20traditionally%20complex%20process%20of%0Aproducing%20such%20content.%20Recent%20progress%20in%20text-to-image%20diffusion%20models%20has%0Aaccelerated%20the%20rapid%20development%20in%20this%20emerging%20field.%20This%20survey%20presents%0Aa%20comprehensive%20review%20of%20text-driven%20360-degree%20panorama%20generation%2C%20offering%0Aan%20in-depth%20analysis%20of%20state-of-the-art%20algorithms%20and%20their%20expanding%0Aapplications%20in%20360-degree%203D%20scene%20generation.%20Furthermore%2C%20we%20critically%0Aexamine%20current%20limitations%20and%20propose%20promising%20directions%20for%20future%0Aresearch.%20A%20curated%20project%20page%20with%20relevant%20resources%20and%20research%20papers%20is%0Aavailable%20at%20https%3A//littlewhitesea.github.io/Text-Driven-Pano-Gen/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14799v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Text-Driven%2520360-Degree%2520Panorama%2520Generation%26entry.906535625%3DHai%2520Wang%2520and%2520Xiaoyu%2520Xiang%2520and%2520Weihao%2520Xia%2520and%2520Jing-Hao%2520Xue%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520text-driven%2520360-degree%2520panorama%2520generation%252C%2520enabling%2520the%250Asynthesis%2520of%2520360-degree%2520panoramic%2520images%2520directly%2520from%2520textual%2520descriptions%252C%250Amarks%2520a%2520transformative%2520advancement%2520in%2520immersive%2520visual%2520content%2520creation.%2520This%250Ainnovation%2520significantly%2520simplifies%2520the%2520traditionally%2520complex%2520process%2520of%250Aproducing%2520such%2520content.%2520Recent%2520progress%2520in%2520text-to-image%2520diffusion%2520models%2520has%250Aaccelerated%2520the%2520rapid%2520development%2520in%2520this%2520emerging%2520field.%2520This%2520survey%2520presents%250Aa%2520comprehensive%2520review%2520of%2520text-driven%2520360-degree%2520panorama%2520generation%252C%2520offering%250Aan%2520in-depth%2520analysis%2520of%2520state-of-the-art%2520algorithms%2520and%2520their%2520expanding%250Aapplications%2520in%2520360-degree%25203D%2520scene%2520generation.%2520Furthermore%252C%2520we%2520critically%250Aexamine%2520current%2520limitations%2520and%2520propose%2520promising%2520directions%2520for%2520future%250Aresearch.%2520A%2520curated%2520project%2520page%2520with%2520relevant%2520resources%2520and%2520research%2520papers%2520is%250Aavailable%2520at%2520https%253A//littlewhitesea.github.io/Text-Driven-Pano-Gen/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14799v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Text-Driven%20360-Degree%20Panorama%20Generation&entry.906535625=Hai%20Wang%20and%20Xiaoyu%20Xiang%20and%20Weihao%20Xia%20and%20Jing-Hao%20Xue&entry.1292438233=%20%20The%20advent%20of%20text-driven%20360-degree%20panorama%20generation%2C%20enabling%20the%0Asynthesis%20of%20360-degree%20panoramic%20images%20directly%20from%20textual%20descriptions%2C%0Amarks%20a%20transformative%20advancement%20in%20immersive%20visual%20content%20creation.%20This%0Ainnovation%20significantly%20simplifies%20the%20traditionally%20complex%20process%20of%0Aproducing%20such%20content.%20Recent%20progress%20in%20text-to-image%20diffusion%20models%20has%0Aaccelerated%20the%20rapid%20development%20in%20this%20emerging%20field.%20This%20survey%20presents%0Aa%20comprehensive%20review%20of%20text-driven%20360-degree%20panorama%20generation%2C%20offering%0Aan%20in-depth%20analysis%20of%20state-of-the-art%20algorithms%20and%20their%20expanding%0Aapplications%20in%20360-degree%203D%20scene%20generation.%20Furthermore%2C%20we%20critically%0Aexamine%20current%20limitations%20and%20propose%20promising%20directions%20for%20future%0Aresearch.%20A%20curated%20project%20page%20with%20relevant%20resources%20and%20research%20papers%20is%0Aavailable%20at%20https%3A//littlewhitesea.github.io/Text-Driven-Pano-Gen/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14799v2&entry.124074799=Read"},
{"title": "Adversarial Pruning: A Survey and Benchmark of Pruning Methods for\n  Adversarial Robustness", "author": "Giorgio Piras and Maura Pintor and Ambra Demontis and Battista Biggio and Giorgio Giacinto and Fabio Roli", "abstract": "  Recent work has proposed neural network pruning techniques to reduce the size\nof a network while preserving robustness against adversarial examples, i.e.,\nwell-crafted inputs inducing a misclassification. These methods, which we refer\nto as adversarial pruning methods, involve complex and articulated designs,\nmaking it difficult to analyze the differences and establish a fair and\naccurate comparison. In this work, we overcome these issues by surveying\ncurrent adversarial pruning methods and proposing a novel taxonomy to\ncategorize them based on two main dimensions: the pipeline, defining when to\nprune; and the specifics, defining how to prune. We then highlight the\nlimitations of current empirical analyses and propose a novel, fair evaluation\nbenchmark to address them. We finally conduct an empirical re-evaluation of\ncurrent adversarial pruning methods and discuss the results, highlighting the\nshared traits of top-performing adversarial pruning methods, as well as common\nissues. We welcome contributions in our publicly-available benchmark at\nhttps://github.com/pralab/AdversarialPruningBenchmark\n", "link": "http://arxiv.org/abs/2409.01249v2", "date": "2025-05-30", "relevancy": 2.3701, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4824}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4767}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Pruning%3A%20A%20Survey%20and%20Benchmark%20of%20Pruning%20Methods%20for%0A%20%20Adversarial%20Robustness&body=Title%3A%20Adversarial%20Pruning%3A%20A%20Survey%20and%20Benchmark%20of%20Pruning%20Methods%20for%0A%20%20Adversarial%20Robustness%0AAuthor%3A%20Giorgio%20Piras%20and%20Maura%20Pintor%20and%20Ambra%20Demontis%20and%20Battista%20Biggio%20and%20Giorgio%20Giacinto%20and%20Fabio%20Roli%0AAbstract%3A%20%20%20Recent%20work%20has%20proposed%20neural%20network%20pruning%20techniques%20to%20reduce%20the%20size%0Aof%20a%20network%20while%20preserving%20robustness%20against%20adversarial%20examples%2C%20i.e.%2C%0Awell-crafted%20inputs%20inducing%20a%20misclassification.%20These%20methods%2C%20which%20we%20refer%0Ato%20as%20adversarial%20pruning%20methods%2C%20involve%20complex%20and%20articulated%20designs%2C%0Amaking%20it%20difficult%20to%20analyze%20the%20differences%20and%20establish%20a%20fair%20and%0Aaccurate%20comparison.%20In%20this%20work%2C%20we%20overcome%20these%20issues%20by%20surveying%0Acurrent%20adversarial%20pruning%20methods%20and%20proposing%20a%20novel%20taxonomy%20to%0Acategorize%20them%20based%20on%20two%20main%20dimensions%3A%20the%20pipeline%2C%20defining%20when%20to%0Aprune%3B%20and%20the%20specifics%2C%20defining%20how%20to%20prune.%20We%20then%20highlight%20the%0Alimitations%20of%20current%20empirical%20analyses%20and%20propose%20a%20novel%2C%20fair%20evaluation%0Abenchmark%20to%20address%20them.%20We%20finally%20conduct%20an%20empirical%20re-evaluation%20of%0Acurrent%20adversarial%20pruning%20methods%20and%20discuss%20the%20results%2C%20highlighting%20the%0Ashared%20traits%20of%20top-performing%20adversarial%20pruning%20methods%2C%20as%20well%20as%20common%0Aissues.%20We%20welcome%20contributions%20in%20our%20publicly-available%20benchmark%20at%0Ahttps%3A//github.com/pralab/AdversarialPruningBenchmark%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.01249v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Pruning%253A%2520A%2520Survey%2520and%2520Benchmark%2520of%2520Pruning%2520Methods%2520for%250A%2520%2520Adversarial%2520Robustness%26entry.906535625%3DGiorgio%2520Piras%2520and%2520Maura%2520Pintor%2520and%2520Ambra%2520Demontis%2520and%2520Battista%2520Biggio%2520and%2520Giorgio%2520Giacinto%2520and%2520Fabio%2520Roli%26entry.1292438233%3D%2520%2520Recent%2520work%2520has%2520proposed%2520neural%2520network%2520pruning%2520techniques%2520to%2520reduce%2520the%2520size%250Aof%2520a%2520network%2520while%2520preserving%2520robustness%2520against%2520adversarial%2520examples%252C%2520i.e.%252C%250Awell-crafted%2520inputs%2520inducing%2520a%2520misclassification.%2520These%2520methods%252C%2520which%2520we%2520refer%250Ato%2520as%2520adversarial%2520pruning%2520methods%252C%2520involve%2520complex%2520and%2520articulated%2520designs%252C%250Amaking%2520it%2520difficult%2520to%2520analyze%2520the%2520differences%2520and%2520establish%2520a%2520fair%2520and%250Aaccurate%2520comparison.%2520In%2520this%2520work%252C%2520we%2520overcome%2520these%2520issues%2520by%2520surveying%250Acurrent%2520adversarial%2520pruning%2520methods%2520and%2520proposing%2520a%2520novel%2520taxonomy%2520to%250Acategorize%2520them%2520based%2520on%2520two%2520main%2520dimensions%253A%2520the%2520pipeline%252C%2520defining%2520when%2520to%250Aprune%253B%2520and%2520the%2520specifics%252C%2520defining%2520how%2520to%2520prune.%2520We%2520then%2520highlight%2520the%250Alimitations%2520of%2520current%2520empirical%2520analyses%2520and%2520propose%2520a%2520novel%252C%2520fair%2520evaluation%250Abenchmark%2520to%2520address%2520them.%2520We%2520finally%2520conduct%2520an%2520empirical%2520re-evaluation%2520of%250Acurrent%2520adversarial%2520pruning%2520methods%2520and%2520discuss%2520the%2520results%252C%2520highlighting%2520the%250Ashared%2520traits%2520of%2520top-performing%2520adversarial%2520pruning%2520methods%252C%2520as%2520well%2520as%2520common%250Aissues.%2520We%2520welcome%2520contributions%2520in%2520our%2520publicly-available%2520benchmark%2520at%250Ahttps%253A//github.com/pralab/AdversarialPruningBenchmark%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.01249v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Pruning%3A%20A%20Survey%20and%20Benchmark%20of%20Pruning%20Methods%20for%0A%20%20Adversarial%20Robustness&entry.906535625=Giorgio%20Piras%20and%20Maura%20Pintor%20and%20Ambra%20Demontis%20and%20Battista%20Biggio%20and%20Giorgio%20Giacinto%20and%20Fabio%20Roli&entry.1292438233=%20%20Recent%20work%20has%20proposed%20neural%20network%20pruning%20techniques%20to%20reduce%20the%20size%0Aof%20a%20network%20while%20preserving%20robustness%20against%20adversarial%20examples%2C%20i.e.%2C%0Awell-crafted%20inputs%20inducing%20a%20misclassification.%20These%20methods%2C%20which%20we%20refer%0Ato%20as%20adversarial%20pruning%20methods%2C%20involve%20complex%20and%20articulated%20designs%2C%0Amaking%20it%20difficult%20to%20analyze%20the%20differences%20and%20establish%20a%20fair%20and%0Aaccurate%20comparison.%20In%20this%20work%2C%20we%20overcome%20these%20issues%20by%20surveying%0Acurrent%20adversarial%20pruning%20methods%20and%20proposing%20a%20novel%20taxonomy%20to%0Acategorize%20them%20based%20on%20two%20main%20dimensions%3A%20the%20pipeline%2C%20defining%20when%20to%0Aprune%3B%20and%20the%20specifics%2C%20defining%20how%20to%20prune.%20We%20then%20highlight%20the%0Alimitations%20of%20current%20empirical%20analyses%20and%20propose%20a%20novel%2C%20fair%20evaluation%0Abenchmark%20to%20address%20them.%20We%20finally%20conduct%20an%20empirical%20re-evaluation%20of%0Acurrent%20adversarial%20pruning%20methods%20and%20discuss%20the%20results%2C%20highlighting%20the%0Ashared%20traits%20of%20top-performing%20adversarial%20pruning%20methods%2C%20as%20well%20as%20common%0Aissues.%20We%20welcome%20contributions%20in%20our%20publicly-available%20benchmark%20at%0Ahttps%3A//github.com/pralab/AdversarialPruningBenchmark%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.01249v2&entry.124074799=Read"},
{"title": "Object Centric Concept Bottlenecks", "author": "David Steinmann and Wolfgang Stammer and Antonia W\u00fcst and Kristian Kersting", "abstract": "  Developing high-performing, yet interpretable models remains a critical\nchallenge in modern AI. Concept-based models (CBMs) attempt to address this by\nextracting human-understandable concepts from a global encoding (e.g., image\nencoding) and then applying a linear classifier on the resulting concept\nactivations, enabling transparent decision-making. However, their reliance on\nholistic image encodings limits their expressiveness in object-centric\nreal-world settings and thus hinders their ability to solve complex vision\ntasks beyond single-label classification. To tackle these challenges, we\nintroduce Object-Centric Concept Bottlenecks (OCB), a framework that combines\nthe strengths of CBMs and pre-trained object-centric foundation models,\nboosting performance and interpretability. We evaluate OCB on complex image\ndatasets and conduct a comprehensive ablation study to analyze key components\nof the framework, such as strategies for aggregating object-concept encodings.\nThe results show that OCB outperforms traditional CBMs and allows one to make\ninterpretable decisions for complex visual tasks.\n", "link": "http://arxiv.org/abs/2505.24492v1", "date": "2025-05-30", "relevancy": 2.3665, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.593}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.593}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Object%20Centric%20Concept%20Bottlenecks&body=Title%3A%20Object%20Centric%20Concept%20Bottlenecks%0AAuthor%3A%20David%20Steinmann%20and%20Wolfgang%20Stammer%20and%20Antonia%20W%C3%BCst%20and%20Kristian%20Kersting%0AAbstract%3A%20%20%20Developing%20high-performing%2C%20yet%20interpretable%20models%20remains%20a%20critical%0Achallenge%20in%20modern%20AI.%20Concept-based%20models%20%28CBMs%29%20attempt%20to%20address%20this%20by%0Aextracting%20human-understandable%20concepts%20from%20a%20global%20encoding%20%28e.g.%2C%20image%0Aencoding%29%20and%20then%20applying%20a%20linear%20classifier%20on%20the%20resulting%20concept%0Aactivations%2C%20enabling%20transparent%20decision-making.%20However%2C%20their%20reliance%20on%0Aholistic%20image%20encodings%20limits%20their%20expressiveness%20in%20object-centric%0Areal-world%20settings%20and%20thus%20hinders%20their%20ability%20to%20solve%20complex%20vision%0Atasks%20beyond%20single-label%20classification.%20To%20tackle%20these%20challenges%2C%20we%0Aintroduce%20Object-Centric%20Concept%20Bottlenecks%20%28OCB%29%2C%20a%20framework%20that%20combines%0Athe%20strengths%20of%20CBMs%20and%20pre-trained%20object-centric%20foundation%20models%2C%0Aboosting%20performance%20and%20interpretability.%20We%20evaluate%20OCB%20on%20complex%20image%0Adatasets%20and%20conduct%20a%20comprehensive%20ablation%20study%20to%20analyze%20key%20components%0Aof%20the%20framework%2C%20such%20as%20strategies%20for%20aggregating%20object-concept%20encodings.%0AThe%20results%20show%20that%20OCB%20outperforms%20traditional%20CBMs%20and%20allows%20one%20to%20make%0Ainterpretable%20decisions%20for%20complex%20visual%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24492v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObject%2520Centric%2520Concept%2520Bottlenecks%26entry.906535625%3DDavid%2520Steinmann%2520and%2520Wolfgang%2520Stammer%2520and%2520Antonia%2520W%25C3%25BCst%2520and%2520Kristian%2520Kersting%26entry.1292438233%3D%2520%2520Developing%2520high-performing%252C%2520yet%2520interpretable%2520models%2520remains%2520a%2520critical%250Achallenge%2520in%2520modern%2520AI.%2520Concept-based%2520models%2520%2528CBMs%2529%2520attempt%2520to%2520address%2520this%2520by%250Aextracting%2520human-understandable%2520concepts%2520from%2520a%2520global%2520encoding%2520%2528e.g.%252C%2520image%250Aencoding%2529%2520and%2520then%2520applying%2520a%2520linear%2520classifier%2520on%2520the%2520resulting%2520concept%250Aactivations%252C%2520enabling%2520transparent%2520decision-making.%2520However%252C%2520their%2520reliance%2520on%250Aholistic%2520image%2520encodings%2520limits%2520their%2520expressiveness%2520in%2520object-centric%250Areal-world%2520settings%2520and%2520thus%2520hinders%2520their%2520ability%2520to%2520solve%2520complex%2520vision%250Atasks%2520beyond%2520single-label%2520classification.%2520To%2520tackle%2520these%2520challenges%252C%2520we%250Aintroduce%2520Object-Centric%2520Concept%2520Bottlenecks%2520%2528OCB%2529%252C%2520a%2520framework%2520that%2520combines%250Athe%2520strengths%2520of%2520CBMs%2520and%2520pre-trained%2520object-centric%2520foundation%2520models%252C%250Aboosting%2520performance%2520and%2520interpretability.%2520We%2520evaluate%2520OCB%2520on%2520complex%2520image%250Adatasets%2520and%2520conduct%2520a%2520comprehensive%2520ablation%2520study%2520to%2520analyze%2520key%2520components%250Aof%2520the%2520framework%252C%2520such%2520as%2520strategies%2520for%2520aggregating%2520object-concept%2520encodings.%250AThe%2520results%2520show%2520that%2520OCB%2520outperforms%2520traditional%2520CBMs%2520and%2520allows%2520one%2520to%2520make%250Ainterpretable%2520decisions%2520for%2520complex%2520visual%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24492v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Object%20Centric%20Concept%20Bottlenecks&entry.906535625=David%20Steinmann%20and%20Wolfgang%20Stammer%20and%20Antonia%20W%C3%BCst%20and%20Kristian%20Kersting&entry.1292438233=%20%20Developing%20high-performing%2C%20yet%20interpretable%20models%20remains%20a%20critical%0Achallenge%20in%20modern%20AI.%20Concept-based%20models%20%28CBMs%29%20attempt%20to%20address%20this%20by%0Aextracting%20human-understandable%20concepts%20from%20a%20global%20encoding%20%28e.g.%2C%20image%0Aencoding%29%20and%20then%20applying%20a%20linear%20classifier%20on%20the%20resulting%20concept%0Aactivations%2C%20enabling%20transparent%20decision-making.%20However%2C%20their%20reliance%20on%0Aholistic%20image%20encodings%20limits%20their%20expressiveness%20in%20object-centric%0Areal-world%20settings%20and%20thus%20hinders%20their%20ability%20to%20solve%20complex%20vision%0Atasks%20beyond%20single-label%20classification.%20To%20tackle%20these%20challenges%2C%20we%0Aintroduce%20Object-Centric%20Concept%20Bottlenecks%20%28OCB%29%2C%20a%20framework%20that%20combines%0Athe%20strengths%20of%20CBMs%20and%20pre-trained%20object-centric%20foundation%20models%2C%0Aboosting%20performance%20and%20interpretability.%20We%20evaluate%20OCB%20on%20complex%20image%0Adatasets%20and%20conduct%20a%20comprehensive%20ablation%20study%20to%20analyze%20key%20components%0Aof%20the%20framework%2C%20such%20as%20strategies%20for%20aggregating%20object-concept%20encodings.%0AThe%20results%20show%20that%20OCB%20outperforms%20traditional%20CBMs%20and%20allows%20one%20to%20make%0Ainterpretable%20decisions%20for%20complex%20visual%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24492v1&entry.124074799=Read"},
{"title": "Train One Sparse Autoencoder Across Multiple Sparsity Budgets to\n  Preserve Interpretability and Accuracy", "author": "Nikita Balagansky and Yaroslav Aksenov and Daniil Laptev and Vadim Kurochkin and Gleb Gerasimov and Nikita Koryagin and Daniil Gavrilov", "abstract": "  Sparse Autoencoders (SAEs) have proven to be powerful tools for interpreting\nneural networks by decomposing hidden representations into disentangled,\ninterpretable features via sparsity constraints. However, conventional SAEs are\nconstrained by the fixed sparsity level chosen during training; meeting\ndifferent sparsity requirements therefore demands separate models and increases\nthe computational footprint during both training and evaluation. We introduce a\nnovel training objective, \\emph{HierarchicalTopK}, which trains a single SAE to\noptimise reconstructions across multiple sparsity levels simultaneously.\nExperiments with Gemma-2 2B demonstrate that our approach achieves\nPareto-optimal trade-offs between sparsity and explained variance,\noutperforming traditional SAEs trained at individual sparsity levels. Further\nanalysis shows that HierarchicalTopK preserves high interpretability scores\neven at higher sparsity. The proposed objective thus closes an important gap\nbetween flexibility and interpretability in SAE design.\n", "link": "http://arxiv.org/abs/2505.24473v1", "date": "2025-05-30", "relevancy": 2.3629, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4873}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4707}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Train%20One%20Sparse%20Autoencoder%20Across%20Multiple%20Sparsity%20Budgets%20to%0A%20%20Preserve%20Interpretability%20and%20Accuracy&body=Title%3A%20Train%20One%20Sparse%20Autoencoder%20Across%20Multiple%20Sparsity%20Budgets%20to%0A%20%20Preserve%20Interpretability%20and%20Accuracy%0AAuthor%3A%20Nikita%20Balagansky%20and%20Yaroslav%20Aksenov%20and%20Daniil%20Laptev%20and%20Vadim%20Kurochkin%20and%20Gleb%20Gerasimov%20and%20Nikita%20Koryagin%20and%20Daniil%20Gavrilov%0AAbstract%3A%20%20%20Sparse%20Autoencoders%20%28SAEs%29%20have%20proven%20to%20be%20powerful%20tools%20for%20interpreting%0Aneural%20networks%20by%20decomposing%20hidden%20representations%20into%20disentangled%2C%0Ainterpretable%20features%20via%20sparsity%20constraints.%20However%2C%20conventional%20SAEs%20are%0Aconstrained%20by%20the%20fixed%20sparsity%20level%20chosen%20during%20training%3B%20meeting%0Adifferent%20sparsity%20requirements%20therefore%20demands%20separate%20models%20and%20increases%0Athe%20computational%20footprint%20during%20both%20training%20and%20evaluation.%20We%20introduce%20a%0Anovel%20training%20objective%2C%20%5Cemph%7BHierarchicalTopK%7D%2C%20which%20trains%20a%20single%20SAE%20to%0Aoptimise%20reconstructions%20across%20multiple%20sparsity%20levels%20simultaneously.%0AExperiments%20with%20Gemma-2%202B%20demonstrate%20that%20our%20approach%20achieves%0APareto-optimal%20trade-offs%20between%20sparsity%20and%20explained%20variance%2C%0Aoutperforming%20traditional%20SAEs%20trained%20at%20individual%20sparsity%20levels.%20Further%0Aanalysis%20shows%20that%20HierarchicalTopK%20preserves%20high%20interpretability%20scores%0Aeven%20at%20higher%20sparsity.%20The%20proposed%20objective%20thus%20closes%20an%20important%20gap%0Abetween%20flexibility%20and%20interpretability%20in%20SAE%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24473v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrain%2520One%2520Sparse%2520Autoencoder%2520Across%2520Multiple%2520Sparsity%2520Budgets%2520to%250A%2520%2520Preserve%2520Interpretability%2520and%2520Accuracy%26entry.906535625%3DNikita%2520Balagansky%2520and%2520Yaroslav%2520Aksenov%2520and%2520Daniil%2520Laptev%2520and%2520Vadim%2520Kurochkin%2520and%2520Gleb%2520Gerasimov%2520and%2520Nikita%2520Koryagin%2520and%2520Daniil%2520Gavrilov%26entry.1292438233%3D%2520%2520Sparse%2520Autoencoders%2520%2528SAEs%2529%2520have%2520proven%2520to%2520be%2520powerful%2520tools%2520for%2520interpreting%250Aneural%2520networks%2520by%2520decomposing%2520hidden%2520representations%2520into%2520disentangled%252C%250Ainterpretable%2520features%2520via%2520sparsity%2520constraints.%2520However%252C%2520conventional%2520SAEs%2520are%250Aconstrained%2520by%2520the%2520fixed%2520sparsity%2520level%2520chosen%2520during%2520training%253B%2520meeting%250Adifferent%2520sparsity%2520requirements%2520therefore%2520demands%2520separate%2520models%2520and%2520increases%250Athe%2520computational%2520footprint%2520during%2520both%2520training%2520and%2520evaluation.%2520We%2520introduce%2520a%250Anovel%2520training%2520objective%252C%2520%255Cemph%257BHierarchicalTopK%257D%252C%2520which%2520trains%2520a%2520single%2520SAE%2520to%250Aoptimise%2520reconstructions%2520across%2520multiple%2520sparsity%2520levels%2520simultaneously.%250AExperiments%2520with%2520Gemma-2%25202B%2520demonstrate%2520that%2520our%2520approach%2520achieves%250APareto-optimal%2520trade-offs%2520between%2520sparsity%2520and%2520explained%2520variance%252C%250Aoutperforming%2520traditional%2520SAEs%2520trained%2520at%2520individual%2520sparsity%2520levels.%2520Further%250Aanalysis%2520shows%2520that%2520HierarchicalTopK%2520preserves%2520high%2520interpretability%2520scores%250Aeven%2520at%2520higher%2520sparsity.%2520The%2520proposed%2520objective%2520thus%2520closes%2520an%2520important%2520gap%250Abetween%2520flexibility%2520and%2520interpretability%2520in%2520SAE%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24473v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Train%20One%20Sparse%20Autoencoder%20Across%20Multiple%20Sparsity%20Budgets%20to%0A%20%20Preserve%20Interpretability%20and%20Accuracy&entry.906535625=Nikita%20Balagansky%20and%20Yaroslav%20Aksenov%20and%20Daniil%20Laptev%20and%20Vadim%20Kurochkin%20and%20Gleb%20Gerasimov%20and%20Nikita%20Koryagin%20and%20Daniil%20Gavrilov&entry.1292438233=%20%20Sparse%20Autoencoders%20%28SAEs%29%20have%20proven%20to%20be%20powerful%20tools%20for%20interpreting%0Aneural%20networks%20by%20decomposing%20hidden%20representations%20into%20disentangled%2C%0Ainterpretable%20features%20via%20sparsity%20constraints.%20However%2C%20conventional%20SAEs%20are%0Aconstrained%20by%20the%20fixed%20sparsity%20level%20chosen%20during%20training%3B%20meeting%0Adifferent%20sparsity%20requirements%20therefore%20demands%20separate%20models%20and%20increases%0Athe%20computational%20footprint%20during%20both%20training%20and%20evaluation.%20We%20introduce%20a%0Anovel%20training%20objective%2C%20%5Cemph%7BHierarchicalTopK%7D%2C%20which%20trains%20a%20single%20SAE%20to%0Aoptimise%20reconstructions%20across%20multiple%20sparsity%20levels%20simultaneously.%0AExperiments%20with%20Gemma-2%202B%20demonstrate%20that%20our%20approach%20achieves%0APareto-optimal%20trade-offs%20between%20sparsity%20and%20explained%20variance%2C%0Aoutperforming%20traditional%20SAEs%20trained%20at%20individual%20sparsity%20levels.%20Further%0Aanalysis%20shows%20that%20HierarchicalTopK%20preserves%20high%20interpretability%20scores%0Aeven%20at%20higher%20sparsity.%20The%20proposed%20objective%20thus%20closes%20an%20important%20gap%0Abetween%20flexibility%20and%20interpretability%20in%20SAE%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24473v1&entry.124074799=Read"},
{"title": "ReasonGen-R1: CoT for Autoregressive Image generation models through SFT\n  and RL", "author": "Yu Zhang and Yunqi Li and Yifan Yang and Rui Wang and Yuqing Yang and Dai Qi and Jianmin Bao and Dongdong Chen and Chong Luo and Lili Qiu", "abstract": "  Although chain-of-thought reasoning and reinforcement learning (RL) have\ndriven breakthroughs in NLP, their integration into generative vision models\nremains underexplored. We introduce ReasonGen-R1, a two-stage framework that\nfirst imbues an autoregressive image generator with explicit text-based\n\"thinking\" skills via supervised fine-tuning on a newly generated reasoning\ndataset of written rationales, and then refines its outputs using Group\nRelative Policy Optimization. To enable the model to reason through text before\ngenerating images, We automatically generate and release a corpus of model\ncrafted rationales paired with visual prompts, enabling controlled planning of\nobject layouts, styles, and scene compositions. Our GRPO algorithm uses reward\nsignals from a pretrained vision language model to assess overall visual\nquality, optimizing the policy in each update. Evaluations on GenEval, DPG, and\nthe T2I benchmark demonstrate that ReasonGen-R1 consistently outperforms strong\nbaselines and prior state-of-the-art models. More: aka.ms/reasongen.\n", "link": "http://arxiv.org/abs/2505.24875v1", "date": "2025-05-30", "relevancy": 2.3504, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6089}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5744}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReasonGen-R1%3A%20CoT%20for%20Autoregressive%20Image%20generation%20models%20through%20SFT%0A%20%20and%20RL&body=Title%3A%20ReasonGen-R1%3A%20CoT%20for%20Autoregressive%20Image%20generation%20models%20through%20SFT%0A%20%20and%20RL%0AAuthor%3A%20Yu%20Zhang%20and%20Yunqi%20Li%20and%20Yifan%20Yang%20and%20Rui%20Wang%20and%20Yuqing%20Yang%20and%20Dai%20Qi%20and%20Jianmin%20Bao%20and%20Dongdong%20Chen%20and%20Chong%20Luo%20and%20Lili%20Qiu%0AAbstract%3A%20%20%20Although%20chain-of-thought%20reasoning%20and%20reinforcement%20learning%20%28RL%29%20have%0Adriven%20breakthroughs%20in%20NLP%2C%20their%20integration%20into%20generative%20vision%20models%0Aremains%20underexplored.%20We%20introduce%20ReasonGen-R1%2C%20a%20two-stage%20framework%20that%0Afirst%20imbues%20an%20autoregressive%20image%20generator%20with%20explicit%20text-based%0A%22thinking%22%20skills%20via%20supervised%20fine-tuning%20on%20a%20newly%20generated%20reasoning%0Adataset%20of%20written%20rationales%2C%20and%20then%20refines%20its%20outputs%20using%20Group%0ARelative%20Policy%20Optimization.%20To%20enable%20the%20model%20to%20reason%20through%20text%20before%0Agenerating%20images%2C%20We%20automatically%20generate%20and%20release%20a%20corpus%20of%20model%0Acrafted%20rationales%20paired%20with%20visual%20prompts%2C%20enabling%20controlled%20planning%20of%0Aobject%20layouts%2C%20styles%2C%20and%20scene%20compositions.%20Our%20GRPO%20algorithm%20uses%20reward%0Asignals%20from%20a%20pretrained%20vision%20language%20model%20to%20assess%20overall%20visual%0Aquality%2C%20optimizing%20the%20policy%20in%20each%20update.%20Evaluations%20on%20GenEval%2C%20DPG%2C%20and%0Athe%20T2I%20benchmark%20demonstrate%20that%20ReasonGen-R1%20consistently%20outperforms%20strong%0Abaselines%20and%20prior%20state-of-the-art%20models.%20More%3A%20aka.ms/reasongen.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24875v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasonGen-R1%253A%2520CoT%2520for%2520Autoregressive%2520Image%2520generation%2520models%2520through%2520SFT%250A%2520%2520and%2520RL%26entry.906535625%3DYu%2520Zhang%2520and%2520Yunqi%2520Li%2520and%2520Yifan%2520Yang%2520and%2520Rui%2520Wang%2520and%2520Yuqing%2520Yang%2520and%2520Dai%2520Qi%2520and%2520Jianmin%2520Bao%2520and%2520Dongdong%2520Chen%2520and%2520Chong%2520Luo%2520and%2520Lili%2520Qiu%26entry.1292438233%3D%2520%2520Although%2520chain-of-thought%2520reasoning%2520and%2520reinforcement%2520learning%2520%2528RL%2529%2520have%250Adriven%2520breakthroughs%2520in%2520NLP%252C%2520their%2520integration%2520into%2520generative%2520vision%2520models%250Aremains%2520underexplored.%2520We%2520introduce%2520ReasonGen-R1%252C%2520a%2520two-stage%2520framework%2520that%250Afirst%2520imbues%2520an%2520autoregressive%2520image%2520generator%2520with%2520explicit%2520text-based%250A%2522thinking%2522%2520skills%2520via%2520supervised%2520fine-tuning%2520on%2520a%2520newly%2520generated%2520reasoning%250Adataset%2520of%2520written%2520rationales%252C%2520and%2520then%2520refines%2520its%2520outputs%2520using%2520Group%250ARelative%2520Policy%2520Optimization.%2520To%2520enable%2520the%2520model%2520to%2520reason%2520through%2520text%2520before%250Agenerating%2520images%252C%2520We%2520automatically%2520generate%2520and%2520release%2520a%2520corpus%2520of%2520model%250Acrafted%2520rationales%2520paired%2520with%2520visual%2520prompts%252C%2520enabling%2520controlled%2520planning%2520of%250Aobject%2520layouts%252C%2520styles%252C%2520and%2520scene%2520compositions.%2520Our%2520GRPO%2520algorithm%2520uses%2520reward%250Asignals%2520from%2520a%2520pretrained%2520vision%2520language%2520model%2520to%2520assess%2520overall%2520visual%250Aquality%252C%2520optimizing%2520the%2520policy%2520in%2520each%2520update.%2520Evaluations%2520on%2520GenEval%252C%2520DPG%252C%2520and%250Athe%2520T2I%2520benchmark%2520demonstrate%2520that%2520ReasonGen-R1%2520consistently%2520outperforms%2520strong%250Abaselines%2520and%2520prior%2520state-of-the-art%2520models.%2520More%253A%2520aka.ms/reasongen.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24875v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReasonGen-R1%3A%20CoT%20for%20Autoregressive%20Image%20generation%20models%20through%20SFT%0A%20%20and%20RL&entry.906535625=Yu%20Zhang%20and%20Yunqi%20Li%20and%20Yifan%20Yang%20and%20Rui%20Wang%20and%20Yuqing%20Yang%20and%20Dai%20Qi%20and%20Jianmin%20Bao%20and%20Dongdong%20Chen%20and%20Chong%20Luo%20and%20Lili%20Qiu&entry.1292438233=%20%20Although%20chain-of-thought%20reasoning%20and%20reinforcement%20learning%20%28RL%29%20have%0Adriven%20breakthroughs%20in%20NLP%2C%20their%20integration%20into%20generative%20vision%20models%0Aremains%20underexplored.%20We%20introduce%20ReasonGen-R1%2C%20a%20two-stage%20framework%20that%0Afirst%20imbues%20an%20autoregressive%20image%20generator%20with%20explicit%20text-based%0A%22thinking%22%20skills%20via%20supervised%20fine-tuning%20on%20a%20newly%20generated%20reasoning%0Adataset%20of%20written%20rationales%2C%20and%20then%20refines%20its%20outputs%20using%20Group%0ARelative%20Policy%20Optimization.%20To%20enable%20the%20model%20to%20reason%20through%20text%20before%0Agenerating%20images%2C%20We%20automatically%20generate%20and%20release%20a%20corpus%20of%20model%0Acrafted%20rationales%20paired%20with%20visual%20prompts%2C%20enabling%20controlled%20planning%20of%0Aobject%20layouts%2C%20styles%2C%20and%20scene%20compositions.%20Our%20GRPO%20algorithm%20uses%20reward%0Asignals%20from%20a%20pretrained%20vision%20language%20model%20to%20assess%20overall%20visual%0Aquality%2C%20optimizing%20the%20policy%20in%20each%20update.%20Evaluations%20on%20GenEval%2C%20DPG%2C%20and%0Athe%20T2I%20benchmark%20demonstrate%20that%20ReasonGen-R1%20consistently%20outperforms%20strong%0Abaselines%20and%20prior%20state-of-the-art%20models.%20More%3A%20aka.ms/reasongen.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24875v1&entry.124074799=Read"},
{"title": "Scaling Large Motion Models with Million-Level Human Motions", "author": "Ye Wang and Sipeng Zheng and Bin Cao and Qianshan Wei and Weishuai Zeng and Qin Jin and Zongqing Lu", "abstract": "  Inspired by the recent success of LLMs, the field of human motion\nunderstanding has increasingly shifted toward developing large motion models.\nDespite some progress, current efforts remain far from achieving truly\ngeneralist models, primarily due to the lack of massive high-quality data. To\naddress this gap, we present MotionLib, the first million-level dataset for\nmotion generation, which is at least 15$\\times$ larger than existing\ncounterparts and enriched with hierarchical text descriptions. Using MotionLib,\nwe train a large motion model named \\projname, demonstrating robust performance\nacross a wide range of human activities, including unseen ones. Through\nsystematic investigation, for the first time, we highlight the importance of\nscaling both data and model size for advancing motion generation, along with\nkey insights to achieve this goal. To better integrate the motion modality, we\npropose Motionbook, an innovative motion encoding approach including (1) a\ncompact yet lossless feature to represent motions; (2) a novel 2D lookup-free\nmotion tokenizer that preserves fine-grained motion details while expanding\ncodebook capacity, significantly enhancing the representational power of motion\ntokens. We believe this work lays the groundwork for developing more versatile\nand powerful motion generation models in the future. For further details, visit\nhttps://beingbeyond.github.io/Being-M0/.\n", "link": "http://arxiv.org/abs/2410.03311v3", "date": "2025-05-30", "relevancy": 2.3492, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6486}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5616}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Large%20Motion%20Models%20with%20Million-Level%20Human%20Motions&body=Title%3A%20Scaling%20Large%20Motion%20Models%20with%20Million-Level%20Human%20Motions%0AAuthor%3A%20Ye%20Wang%20and%20Sipeng%20Zheng%20and%20Bin%20Cao%20and%20Qianshan%20Wei%20and%20Weishuai%20Zeng%20and%20Qin%20Jin%20and%20Zongqing%20Lu%0AAbstract%3A%20%20%20Inspired%20by%20the%20recent%20success%20of%20LLMs%2C%20the%20field%20of%20human%20motion%0Aunderstanding%20has%20increasingly%20shifted%20toward%20developing%20large%20motion%20models.%0ADespite%20some%20progress%2C%20current%20efforts%20remain%20far%20from%20achieving%20truly%0Ageneralist%20models%2C%20primarily%20due%20to%20the%20lack%20of%20massive%20high-quality%20data.%20To%0Aaddress%20this%20gap%2C%20we%20present%20MotionLib%2C%20the%20first%20million-level%20dataset%20for%0Amotion%20generation%2C%20which%20is%20at%20least%2015%24%5Ctimes%24%20larger%20than%20existing%0Acounterparts%20and%20enriched%20with%20hierarchical%20text%20descriptions.%20Using%20MotionLib%2C%0Awe%20train%20a%20large%20motion%20model%20named%20%5Cprojname%2C%20demonstrating%20robust%20performance%0Aacross%20a%20wide%20range%20of%20human%20activities%2C%20including%20unseen%20ones.%20Through%0Asystematic%20investigation%2C%20for%20the%20first%20time%2C%20we%20highlight%20the%20importance%20of%0Ascaling%20both%20data%20and%20model%20size%20for%20advancing%20motion%20generation%2C%20along%20with%0Akey%20insights%20to%20achieve%20this%20goal.%20To%20better%20integrate%20the%20motion%20modality%2C%20we%0Apropose%20Motionbook%2C%20an%20innovative%20motion%20encoding%20approach%20including%20%281%29%20a%0Acompact%20yet%20lossless%20feature%20to%20represent%20motions%3B%20%282%29%20a%20novel%202D%20lookup-free%0Amotion%20tokenizer%20that%20preserves%20fine-grained%20motion%20details%20while%20expanding%0Acodebook%20capacity%2C%20significantly%20enhancing%20the%20representational%20power%20of%20motion%0Atokens.%20We%20believe%20this%20work%20lays%20the%20groundwork%20for%20developing%20more%20versatile%0Aand%20powerful%20motion%20generation%20models%20in%20the%20future.%20For%20further%20details%2C%20visit%0Ahttps%3A//beingbeyond.github.io/Being-M0/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03311v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Large%2520Motion%2520Models%2520with%2520Million-Level%2520Human%2520Motions%26entry.906535625%3DYe%2520Wang%2520and%2520Sipeng%2520Zheng%2520and%2520Bin%2520Cao%2520and%2520Qianshan%2520Wei%2520and%2520Weishuai%2520Zeng%2520and%2520Qin%2520Jin%2520and%2520Zongqing%2520Lu%26entry.1292438233%3D%2520%2520Inspired%2520by%2520the%2520recent%2520success%2520of%2520LLMs%252C%2520the%2520field%2520of%2520human%2520motion%250Aunderstanding%2520has%2520increasingly%2520shifted%2520toward%2520developing%2520large%2520motion%2520models.%250ADespite%2520some%2520progress%252C%2520current%2520efforts%2520remain%2520far%2520from%2520achieving%2520truly%250Ageneralist%2520models%252C%2520primarily%2520due%2520to%2520the%2520lack%2520of%2520massive%2520high-quality%2520data.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520present%2520MotionLib%252C%2520the%2520first%2520million-level%2520dataset%2520for%250Amotion%2520generation%252C%2520which%2520is%2520at%2520least%252015%2524%255Ctimes%2524%2520larger%2520than%2520existing%250Acounterparts%2520and%2520enriched%2520with%2520hierarchical%2520text%2520descriptions.%2520Using%2520MotionLib%252C%250Awe%2520train%2520a%2520large%2520motion%2520model%2520named%2520%255Cprojname%252C%2520demonstrating%2520robust%2520performance%250Aacross%2520a%2520wide%2520range%2520of%2520human%2520activities%252C%2520including%2520unseen%2520ones.%2520Through%250Asystematic%2520investigation%252C%2520for%2520the%2520first%2520time%252C%2520we%2520highlight%2520the%2520importance%2520of%250Ascaling%2520both%2520data%2520and%2520model%2520size%2520for%2520advancing%2520motion%2520generation%252C%2520along%2520with%250Akey%2520insights%2520to%2520achieve%2520this%2520goal.%2520To%2520better%2520integrate%2520the%2520motion%2520modality%252C%2520we%250Apropose%2520Motionbook%252C%2520an%2520innovative%2520motion%2520encoding%2520approach%2520including%2520%25281%2529%2520a%250Acompact%2520yet%2520lossless%2520feature%2520to%2520represent%2520motions%253B%2520%25282%2529%2520a%2520novel%25202D%2520lookup-free%250Amotion%2520tokenizer%2520that%2520preserves%2520fine-grained%2520motion%2520details%2520while%2520expanding%250Acodebook%2520capacity%252C%2520significantly%2520enhancing%2520the%2520representational%2520power%2520of%2520motion%250Atokens.%2520We%2520believe%2520this%2520work%2520lays%2520the%2520groundwork%2520for%2520developing%2520more%2520versatile%250Aand%2520powerful%2520motion%2520generation%2520models%2520in%2520the%2520future.%2520For%2520further%2520details%252C%2520visit%250Ahttps%253A//beingbeyond.github.io/Being-M0/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03311v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Large%20Motion%20Models%20with%20Million-Level%20Human%20Motions&entry.906535625=Ye%20Wang%20and%20Sipeng%20Zheng%20and%20Bin%20Cao%20and%20Qianshan%20Wei%20and%20Weishuai%20Zeng%20and%20Qin%20Jin%20and%20Zongqing%20Lu&entry.1292438233=%20%20Inspired%20by%20the%20recent%20success%20of%20LLMs%2C%20the%20field%20of%20human%20motion%0Aunderstanding%20has%20increasingly%20shifted%20toward%20developing%20large%20motion%20models.%0ADespite%20some%20progress%2C%20current%20efforts%20remain%20far%20from%20achieving%20truly%0Ageneralist%20models%2C%20primarily%20due%20to%20the%20lack%20of%20massive%20high-quality%20data.%20To%0Aaddress%20this%20gap%2C%20we%20present%20MotionLib%2C%20the%20first%20million-level%20dataset%20for%0Amotion%20generation%2C%20which%20is%20at%20least%2015%24%5Ctimes%24%20larger%20than%20existing%0Acounterparts%20and%20enriched%20with%20hierarchical%20text%20descriptions.%20Using%20MotionLib%2C%0Awe%20train%20a%20large%20motion%20model%20named%20%5Cprojname%2C%20demonstrating%20robust%20performance%0Aacross%20a%20wide%20range%20of%20human%20activities%2C%20including%20unseen%20ones.%20Through%0Asystematic%20investigation%2C%20for%20the%20first%20time%2C%20we%20highlight%20the%20importance%20of%0Ascaling%20both%20data%20and%20model%20size%20for%20advancing%20motion%20generation%2C%20along%20with%0Akey%20insights%20to%20achieve%20this%20goal.%20To%20better%20integrate%20the%20motion%20modality%2C%20we%0Apropose%20Motionbook%2C%20an%20innovative%20motion%20encoding%20approach%20including%20%281%29%20a%0Acompact%20yet%20lossless%20feature%20to%20represent%20motions%3B%20%282%29%20a%20novel%202D%20lookup-free%0Amotion%20tokenizer%20that%20preserves%20fine-grained%20motion%20details%20while%20expanding%0Acodebook%20capacity%2C%20significantly%20enhancing%20the%20representational%20power%20of%20motion%0Atokens.%20We%20believe%20this%20work%20lays%20the%20groundwork%20for%20developing%20more%20versatile%0Aand%20powerful%20motion%20generation%20models%20in%20the%20future.%20For%20further%20details%2C%20visit%0Ahttps%3A//beingbeyond.github.io/Being-M0/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03311v3&entry.124074799=Read"},
{"title": "Supervised Quadratic Feature Analysis: Information Geometry Approach for\n  Dimensionality Reduction", "author": "Daniel Herrera-Esposito and Johannes Burge", "abstract": "  Supervised dimensionality reduction aims to map labeled data to a\nlow-dimensional feature space while maximizing class discriminability. Directly\ncomputing discriminability is often impractical, so an alternative approach is\nto learn features that maximize a distance or dissimilarity measure between\nclasses. The Fisher-Rao distance is an important information geometry distance\nin statistical manifolds. It is induced by the Fisher information metric, a\ntool widely used for understanding neural representations. Despite its\ntheoretical and pratical appeal, Fisher-Rao distances between classes have not\nbeen used as a maximization objective in supervised feature learning. Here, we\npresent Supervised Quadratic Feature Analysis (SQFA), a linear dimensionality\nreduction method that maximizes Fisher-Rao distances between class\ndistributions, by exploiting the information geometry of the symmetric positive\ndefinite manifold. SQFA maximizes distances using first- and second-order\nstatistics, and its features allow for quadratic discriminability (i.e. QDA\nperformance) matching or surpassing state-of-the-art methods on real-world\ndatasets. We theoretically motivate Fisher-Rao distances as a proxy for\nquadratic discriminability, and compare its performance to other popular\ndistances (e.g. Wasserstein distances). SQFA provides a flexible\nstate-of-the-art method for dimensionality reduction. Its successful use of\nFisher-Rao distances between classes motivates future research directions.\n", "link": "http://arxiv.org/abs/2502.00168v3", "date": "2025-05-30", "relevancy": 2.3422, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4908}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4653}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Supervised%20Quadratic%20Feature%20Analysis%3A%20Information%20Geometry%20Approach%20for%0A%20%20Dimensionality%20Reduction&body=Title%3A%20Supervised%20Quadratic%20Feature%20Analysis%3A%20Information%20Geometry%20Approach%20for%0A%20%20Dimensionality%20Reduction%0AAuthor%3A%20Daniel%20Herrera-Esposito%20and%20Johannes%20Burge%0AAbstract%3A%20%20%20Supervised%20dimensionality%20reduction%20aims%20to%20map%20labeled%20data%20to%20a%0Alow-dimensional%20feature%20space%20while%20maximizing%20class%20discriminability.%20Directly%0Acomputing%20discriminability%20is%20often%20impractical%2C%20so%20an%20alternative%20approach%20is%0Ato%20learn%20features%20that%20maximize%20a%20distance%20or%20dissimilarity%20measure%20between%0Aclasses.%20The%20Fisher-Rao%20distance%20is%20an%20important%20information%20geometry%20distance%0Ain%20statistical%20manifolds.%20It%20is%20induced%20by%20the%20Fisher%20information%20metric%2C%20a%0Atool%20widely%20used%20for%20understanding%20neural%20representations.%20Despite%20its%0Atheoretical%20and%20pratical%20appeal%2C%20Fisher-Rao%20distances%20between%20classes%20have%20not%0Abeen%20used%20as%20a%20maximization%20objective%20in%20supervised%20feature%20learning.%20Here%2C%20we%0Apresent%20Supervised%20Quadratic%20Feature%20Analysis%20%28SQFA%29%2C%20a%20linear%20dimensionality%0Areduction%20method%20that%20maximizes%20Fisher-Rao%20distances%20between%20class%0Adistributions%2C%20by%20exploiting%20the%20information%20geometry%20of%20the%20symmetric%20positive%0Adefinite%20manifold.%20SQFA%20maximizes%20distances%20using%20first-%20and%20second-order%0Astatistics%2C%20and%20its%20features%20allow%20for%20quadratic%20discriminability%20%28i.e.%20QDA%0Aperformance%29%20matching%20or%20surpassing%20state-of-the-art%20methods%20on%20real-world%0Adatasets.%20We%20theoretically%20motivate%20Fisher-Rao%20distances%20as%20a%20proxy%20for%0Aquadratic%20discriminability%2C%20and%20compare%20its%20performance%20to%20other%20popular%0Adistances%20%28e.g.%20Wasserstein%20distances%29.%20SQFA%20provides%20a%20flexible%0Astate-of-the-art%20method%20for%20dimensionality%20reduction.%20Its%20successful%20use%20of%0AFisher-Rao%20distances%20between%20classes%20motivates%20future%20research%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.00168v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSupervised%2520Quadratic%2520Feature%2520Analysis%253A%2520Information%2520Geometry%2520Approach%2520for%250A%2520%2520Dimensionality%2520Reduction%26entry.906535625%3DDaniel%2520Herrera-Esposito%2520and%2520Johannes%2520Burge%26entry.1292438233%3D%2520%2520Supervised%2520dimensionality%2520reduction%2520aims%2520to%2520map%2520labeled%2520data%2520to%2520a%250Alow-dimensional%2520feature%2520space%2520while%2520maximizing%2520class%2520discriminability.%2520Directly%250Acomputing%2520discriminability%2520is%2520often%2520impractical%252C%2520so%2520an%2520alternative%2520approach%2520is%250Ato%2520learn%2520features%2520that%2520maximize%2520a%2520distance%2520or%2520dissimilarity%2520measure%2520between%250Aclasses.%2520The%2520Fisher-Rao%2520distance%2520is%2520an%2520important%2520information%2520geometry%2520distance%250Ain%2520statistical%2520manifolds.%2520It%2520is%2520induced%2520by%2520the%2520Fisher%2520information%2520metric%252C%2520a%250Atool%2520widely%2520used%2520for%2520understanding%2520neural%2520representations.%2520Despite%2520its%250Atheoretical%2520and%2520pratical%2520appeal%252C%2520Fisher-Rao%2520distances%2520between%2520classes%2520have%2520not%250Abeen%2520used%2520as%2520a%2520maximization%2520objective%2520in%2520supervised%2520feature%2520learning.%2520Here%252C%2520we%250Apresent%2520Supervised%2520Quadratic%2520Feature%2520Analysis%2520%2528SQFA%2529%252C%2520a%2520linear%2520dimensionality%250Areduction%2520method%2520that%2520maximizes%2520Fisher-Rao%2520distances%2520between%2520class%250Adistributions%252C%2520by%2520exploiting%2520the%2520information%2520geometry%2520of%2520the%2520symmetric%2520positive%250Adefinite%2520manifold.%2520SQFA%2520maximizes%2520distances%2520using%2520first-%2520and%2520second-order%250Astatistics%252C%2520and%2520its%2520features%2520allow%2520for%2520quadratic%2520discriminability%2520%2528i.e.%2520QDA%250Aperformance%2529%2520matching%2520or%2520surpassing%2520state-of-the-art%2520methods%2520on%2520real-world%250Adatasets.%2520We%2520theoretically%2520motivate%2520Fisher-Rao%2520distances%2520as%2520a%2520proxy%2520for%250Aquadratic%2520discriminability%252C%2520and%2520compare%2520its%2520performance%2520to%2520other%2520popular%250Adistances%2520%2528e.g.%2520Wasserstein%2520distances%2529.%2520SQFA%2520provides%2520a%2520flexible%250Astate-of-the-art%2520method%2520for%2520dimensionality%2520reduction.%2520Its%2520successful%2520use%2520of%250AFisher-Rao%2520distances%2520between%2520classes%2520motivates%2520future%2520research%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00168v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Supervised%20Quadratic%20Feature%20Analysis%3A%20Information%20Geometry%20Approach%20for%0A%20%20Dimensionality%20Reduction&entry.906535625=Daniel%20Herrera-Esposito%20and%20Johannes%20Burge&entry.1292438233=%20%20Supervised%20dimensionality%20reduction%20aims%20to%20map%20labeled%20data%20to%20a%0Alow-dimensional%20feature%20space%20while%20maximizing%20class%20discriminability.%20Directly%0Acomputing%20discriminability%20is%20often%20impractical%2C%20so%20an%20alternative%20approach%20is%0Ato%20learn%20features%20that%20maximize%20a%20distance%20or%20dissimilarity%20measure%20between%0Aclasses.%20The%20Fisher-Rao%20distance%20is%20an%20important%20information%20geometry%20distance%0Ain%20statistical%20manifolds.%20It%20is%20induced%20by%20the%20Fisher%20information%20metric%2C%20a%0Atool%20widely%20used%20for%20understanding%20neural%20representations.%20Despite%20its%0Atheoretical%20and%20pratical%20appeal%2C%20Fisher-Rao%20distances%20between%20classes%20have%20not%0Abeen%20used%20as%20a%20maximization%20objective%20in%20supervised%20feature%20learning.%20Here%2C%20we%0Apresent%20Supervised%20Quadratic%20Feature%20Analysis%20%28SQFA%29%2C%20a%20linear%20dimensionality%0Areduction%20method%20that%20maximizes%20Fisher-Rao%20distances%20between%20class%0Adistributions%2C%20by%20exploiting%20the%20information%20geometry%20of%20the%20symmetric%20positive%0Adefinite%20manifold.%20SQFA%20maximizes%20distances%20using%20first-%20and%20second-order%0Astatistics%2C%20and%20its%20features%20allow%20for%20quadratic%20discriminability%20%28i.e.%20QDA%0Aperformance%29%20matching%20or%20surpassing%20state-of-the-art%20methods%20on%20real-world%0Adatasets.%20We%20theoretically%20motivate%20Fisher-Rao%20distances%20as%20a%20proxy%20for%0Aquadratic%20discriminability%2C%20and%20compare%20its%20performance%20to%20other%20popular%0Adistances%20%28e.g.%20Wasserstein%20distances%29.%20SQFA%20provides%20a%20flexible%0Astate-of-the-art%20method%20for%20dimensionality%20reduction.%20Its%20successful%20use%20of%0AFisher-Rao%20distances%20between%20classes%20motivates%20future%20research%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.00168v3&entry.124074799=Read"},
{"title": "Emotion-Qwen: Training Hybrid Experts for Unified Emotion and General\n  Vision-Language Understanding", "author": "Dawei Huang and Qing Li and Chuan Yan and Zebang Cheng and Yurong Huang and Xiang Li and Bin Li and Xiaohui Wang and Zheng Lian and Xiaojiang Peng", "abstract": "  Emotion understanding in videos aims to accurately recognize and interpret\nindividuals' emotional states by integrating contextual, visual, textual, and\nauditory cues. While Large Multimodal Models (LMMs) have demonstrated\nsignificant progress in general vision-language (VL) tasks, their performance\nin emotion-specific scenarios remains limited. Moreover, fine-tuning LMMs on\nemotion-related tasks often leads to catastrophic forgetting, hindering their\nability to generalize across diverse tasks. To address these challenges, we\npresent Emotion-Qwen, a tailored multimodal framework designed to enhance both\nemotion understanding and general VL reasoning. Emotion-Qwen incorporates a\nsophisticated Hybrid Compressor based on the Mixture of Experts (MoE) paradigm,\nwhich dynamically routes inputs to balance emotion-specific and general-purpose\nprocessing. The model is pre-trained in a three-stage pipeline on large-scale\ngeneral and emotional image datasets to support robust multimodal\nrepresentations. Furthermore, we construct the Video Emotion Reasoning (VER)\ndataset, comprising more than 40K bilingual video clips with fine-grained\ndescriptive annotations, to further enrich Emotion-Qwen's emotional reasoning\ncapability. Experimental results demonstrate that Emotion-Qwen achieves\nstate-of-the-art performance on multiple emotion recognition benchmarks, while\nmaintaining competitive results on general VL tasks. Code and models are\navailable at https://github.com/24DavidHuang/Emotion-Qwen.\n", "link": "http://arxiv.org/abs/2505.06685v2", "date": "2025-05-30", "relevancy": 2.3341, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5904}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5904}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emotion-Qwen%3A%20Training%20Hybrid%20Experts%20for%20Unified%20Emotion%20and%20General%0A%20%20Vision-Language%20Understanding&body=Title%3A%20Emotion-Qwen%3A%20Training%20Hybrid%20Experts%20for%20Unified%20Emotion%20and%20General%0A%20%20Vision-Language%20Understanding%0AAuthor%3A%20Dawei%20Huang%20and%20Qing%20Li%20and%20Chuan%20Yan%20and%20Zebang%20Cheng%20and%20Yurong%20Huang%20and%20Xiang%20Li%20and%20Bin%20Li%20and%20Xiaohui%20Wang%20and%20Zheng%20Lian%20and%20Xiaojiang%20Peng%0AAbstract%3A%20%20%20Emotion%20understanding%20in%20videos%20aims%20to%20accurately%20recognize%20and%20interpret%0Aindividuals%27%20emotional%20states%20by%20integrating%20contextual%2C%20visual%2C%20textual%2C%20and%0Aauditory%20cues.%20While%20Large%20Multimodal%20Models%20%28LMMs%29%20have%20demonstrated%0Asignificant%20progress%20in%20general%20vision-language%20%28VL%29%20tasks%2C%20their%20performance%0Ain%20emotion-specific%20scenarios%20remains%20limited.%20Moreover%2C%20fine-tuning%20LMMs%20on%0Aemotion-related%20tasks%20often%20leads%20to%20catastrophic%20forgetting%2C%20hindering%20their%0Aability%20to%20generalize%20across%20diverse%20tasks.%20To%20address%20these%20challenges%2C%20we%0Apresent%20Emotion-Qwen%2C%20a%20tailored%20multimodal%20framework%20designed%20to%20enhance%20both%0Aemotion%20understanding%20and%20general%20VL%20reasoning.%20Emotion-Qwen%20incorporates%20a%0Asophisticated%20Hybrid%20Compressor%20based%20on%20the%20Mixture%20of%20Experts%20%28MoE%29%20paradigm%2C%0Awhich%20dynamically%20routes%20inputs%20to%20balance%20emotion-specific%20and%20general-purpose%0Aprocessing.%20The%20model%20is%20pre-trained%20in%20a%20three-stage%20pipeline%20on%20large-scale%0Ageneral%20and%20emotional%20image%20datasets%20to%20support%20robust%20multimodal%0Arepresentations.%20Furthermore%2C%20we%20construct%20the%20Video%20Emotion%20Reasoning%20%28VER%29%0Adataset%2C%20comprising%20more%20than%2040K%20bilingual%20video%20clips%20with%20fine-grained%0Adescriptive%20annotations%2C%20to%20further%20enrich%20Emotion-Qwen%27s%20emotional%20reasoning%0Acapability.%20Experimental%20results%20demonstrate%20that%20Emotion-Qwen%20achieves%0Astate-of-the-art%20performance%20on%20multiple%20emotion%20recognition%20benchmarks%2C%20while%0Amaintaining%20competitive%20results%20on%20general%20VL%20tasks.%20Code%20and%20models%20are%0Aavailable%20at%20https%3A//github.com/24DavidHuang/Emotion-Qwen.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06685v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmotion-Qwen%253A%2520Training%2520Hybrid%2520Experts%2520for%2520Unified%2520Emotion%2520and%2520General%250A%2520%2520Vision-Language%2520Understanding%26entry.906535625%3DDawei%2520Huang%2520and%2520Qing%2520Li%2520and%2520Chuan%2520Yan%2520and%2520Zebang%2520Cheng%2520and%2520Yurong%2520Huang%2520and%2520Xiang%2520Li%2520and%2520Bin%2520Li%2520and%2520Xiaohui%2520Wang%2520and%2520Zheng%2520Lian%2520and%2520Xiaojiang%2520Peng%26entry.1292438233%3D%2520%2520Emotion%2520understanding%2520in%2520videos%2520aims%2520to%2520accurately%2520recognize%2520and%2520interpret%250Aindividuals%2527%2520emotional%2520states%2520by%2520integrating%2520contextual%252C%2520visual%252C%2520textual%252C%2520and%250Aauditory%2520cues.%2520While%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520have%2520demonstrated%250Asignificant%2520progress%2520in%2520general%2520vision-language%2520%2528VL%2529%2520tasks%252C%2520their%2520performance%250Ain%2520emotion-specific%2520scenarios%2520remains%2520limited.%2520Moreover%252C%2520fine-tuning%2520LMMs%2520on%250Aemotion-related%2520tasks%2520often%2520leads%2520to%2520catastrophic%2520forgetting%252C%2520hindering%2520their%250Aability%2520to%2520generalize%2520across%2520diverse%2520tasks.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apresent%2520Emotion-Qwen%252C%2520a%2520tailored%2520multimodal%2520framework%2520designed%2520to%2520enhance%2520both%250Aemotion%2520understanding%2520and%2520general%2520VL%2520reasoning.%2520Emotion-Qwen%2520incorporates%2520a%250Asophisticated%2520Hybrid%2520Compressor%2520based%2520on%2520the%2520Mixture%2520of%2520Experts%2520%2528MoE%2529%2520paradigm%252C%250Awhich%2520dynamically%2520routes%2520inputs%2520to%2520balance%2520emotion-specific%2520and%2520general-purpose%250Aprocessing.%2520The%2520model%2520is%2520pre-trained%2520in%2520a%2520three-stage%2520pipeline%2520on%2520large-scale%250Ageneral%2520and%2520emotional%2520image%2520datasets%2520to%2520support%2520robust%2520multimodal%250Arepresentations.%2520Furthermore%252C%2520we%2520construct%2520the%2520Video%2520Emotion%2520Reasoning%2520%2528VER%2529%250Adataset%252C%2520comprising%2520more%2520than%252040K%2520bilingual%2520video%2520clips%2520with%2520fine-grained%250Adescriptive%2520annotations%252C%2520to%2520further%2520enrich%2520Emotion-Qwen%2527s%2520emotional%2520reasoning%250Acapability.%2520Experimental%2520results%2520demonstrate%2520that%2520Emotion-Qwen%2520achieves%250Astate-of-the-art%2520performance%2520on%2520multiple%2520emotion%2520recognition%2520benchmarks%252C%2520while%250Amaintaining%2520competitive%2520results%2520on%2520general%2520VL%2520tasks.%2520Code%2520and%2520models%2520are%250Aavailable%2520at%2520https%253A//github.com/24DavidHuang/Emotion-Qwen.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06685v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emotion-Qwen%3A%20Training%20Hybrid%20Experts%20for%20Unified%20Emotion%20and%20General%0A%20%20Vision-Language%20Understanding&entry.906535625=Dawei%20Huang%20and%20Qing%20Li%20and%20Chuan%20Yan%20and%20Zebang%20Cheng%20and%20Yurong%20Huang%20and%20Xiang%20Li%20and%20Bin%20Li%20and%20Xiaohui%20Wang%20and%20Zheng%20Lian%20and%20Xiaojiang%20Peng&entry.1292438233=%20%20Emotion%20understanding%20in%20videos%20aims%20to%20accurately%20recognize%20and%20interpret%0Aindividuals%27%20emotional%20states%20by%20integrating%20contextual%2C%20visual%2C%20textual%2C%20and%0Aauditory%20cues.%20While%20Large%20Multimodal%20Models%20%28LMMs%29%20have%20demonstrated%0Asignificant%20progress%20in%20general%20vision-language%20%28VL%29%20tasks%2C%20their%20performance%0Ain%20emotion-specific%20scenarios%20remains%20limited.%20Moreover%2C%20fine-tuning%20LMMs%20on%0Aemotion-related%20tasks%20often%20leads%20to%20catastrophic%20forgetting%2C%20hindering%20their%0Aability%20to%20generalize%20across%20diverse%20tasks.%20To%20address%20these%20challenges%2C%20we%0Apresent%20Emotion-Qwen%2C%20a%20tailored%20multimodal%20framework%20designed%20to%20enhance%20both%0Aemotion%20understanding%20and%20general%20VL%20reasoning.%20Emotion-Qwen%20incorporates%20a%0Asophisticated%20Hybrid%20Compressor%20based%20on%20the%20Mixture%20of%20Experts%20%28MoE%29%20paradigm%2C%0Awhich%20dynamically%20routes%20inputs%20to%20balance%20emotion-specific%20and%20general-purpose%0Aprocessing.%20The%20model%20is%20pre-trained%20in%20a%20three-stage%20pipeline%20on%20large-scale%0Ageneral%20and%20emotional%20image%20datasets%20to%20support%20robust%20multimodal%0Arepresentations.%20Furthermore%2C%20we%20construct%20the%20Video%20Emotion%20Reasoning%20%28VER%29%0Adataset%2C%20comprising%20more%20than%2040K%20bilingual%20video%20clips%20with%20fine-grained%0Adescriptive%20annotations%2C%20to%20further%20enrich%20Emotion-Qwen%27s%20emotional%20reasoning%0Acapability.%20Experimental%20results%20demonstrate%20that%20Emotion-Qwen%20achieves%0Astate-of-the-art%20performance%20on%20multiple%20emotion%20recognition%20benchmarks%2C%20while%0Amaintaining%20competitive%20results%20on%20general%20VL%20tasks.%20Code%20and%20models%20are%0Aavailable%20at%20https%3A//github.com/24DavidHuang/Emotion-Qwen.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06685v2&entry.124074799=Read"},
{"title": "AFLoRA: Adaptive Federated Fine-Tuning of Large Language Models with\n  Resource-Aware Low-Rank Adaption", "author": "Yajie Zhou and Xiaoyi Pang and Zhibo Wang", "abstract": "  Federated fine-tuning has emerged as a promising approach to adapt foundation\nmodels to downstream tasks using decentralized data. However, real-world\ndeployment remains challenging due to the high computational and communication\ndemands of fine-tuning Large Language Models (LLMs) on clients with data and\nsystem resources that are heterogeneous and constrained. In such settings, the\nglobal model's performance is often bottlenecked by the weakest clients and\nfurther degraded by the non-IID nature of local data. Although existing methods\nleverage parameter-efficient techniques such as Low-Rank Adaptation (LoRA) to\nreduce communication and computation overhead, they often fail to\nsimultaneously ensure accurate aggregation of low-rank updates and maintain low\nsystem costs, thereby hindering overall performance. To address these\nchallenges, we propose AFLoRA, an adaptive and lightweight federated\nfine-tuning framework for LLMs. AFLoRA decouples shared and client-specific\nupdates to reduce overhead and improve aggregation accuracy, incorporates\ndiagonal matrix-based rank pruning to better utilize local resources, and\nemploys rank-aware aggregation with public data refinement to strengthen\ngeneralization under data heterogeneity. Extensive experiments demonstrate that\nAFLoRA outperforms state-of-the-art methods in both accuracy and efficiency,\nproviding a practical solution for efficient LLM adaptation in heterogeneous\nenvironments in the real world.\n", "link": "http://arxiv.org/abs/2505.24773v1", "date": "2025-05-30", "relevancy": 2.3332, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4713}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4684}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AFLoRA%3A%20Adaptive%20Federated%20Fine-Tuning%20of%20Large%20Language%20Models%20with%0A%20%20Resource-Aware%20Low-Rank%20Adaption&body=Title%3A%20AFLoRA%3A%20Adaptive%20Federated%20Fine-Tuning%20of%20Large%20Language%20Models%20with%0A%20%20Resource-Aware%20Low-Rank%20Adaption%0AAuthor%3A%20Yajie%20Zhou%20and%20Xiaoyi%20Pang%20and%20Zhibo%20Wang%0AAbstract%3A%20%20%20Federated%20fine-tuning%20has%20emerged%20as%20a%20promising%20approach%20to%20adapt%20foundation%0Amodels%20to%20downstream%20tasks%20using%20decentralized%20data.%20However%2C%20real-world%0Adeployment%20remains%20challenging%20due%20to%20the%20high%20computational%20and%20communication%0Ademands%20of%20fine-tuning%20Large%20Language%20Models%20%28LLMs%29%20on%20clients%20with%20data%20and%0Asystem%20resources%20that%20are%20heterogeneous%20and%20constrained.%20In%20such%20settings%2C%20the%0Aglobal%20model%27s%20performance%20is%20often%20bottlenecked%20by%20the%20weakest%20clients%20and%0Afurther%20degraded%20by%20the%20non-IID%20nature%20of%20local%20data.%20Although%20existing%20methods%0Aleverage%20parameter-efficient%20techniques%20such%20as%20Low-Rank%20Adaptation%20%28LoRA%29%20to%0Areduce%20communication%20and%20computation%20overhead%2C%20they%20often%20fail%20to%0Asimultaneously%20ensure%20accurate%20aggregation%20of%20low-rank%20updates%20and%20maintain%20low%0Asystem%20costs%2C%20thereby%20hindering%20overall%20performance.%20To%20address%20these%0Achallenges%2C%20we%20propose%20AFLoRA%2C%20an%20adaptive%20and%20lightweight%20federated%0Afine-tuning%20framework%20for%20LLMs.%20AFLoRA%20decouples%20shared%20and%20client-specific%0Aupdates%20to%20reduce%20overhead%20and%20improve%20aggregation%20accuracy%2C%20incorporates%0Adiagonal%20matrix-based%20rank%20pruning%20to%20better%20utilize%20local%20resources%2C%20and%0Aemploys%20rank-aware%20aggregation%20with%20public%20data%20refinement%20to%20strengthen%0Ageneralization%20under%20data%20heterogeneity.%20Extensive%20experiments%20demonstrate%20that%0AAFLoRA%20outperforms%20state-of-the-art%20methods%20in%20both%20accuracy%20and%20efficiency%2C%0Aproviding%20a%20practical%20solution%20for%20efficient%20LLM%20adaptation%20in%20heterogeneous%0Aenvironments%20in%20the%20real%20world.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24773v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAFLoRA%253A%2520Adaptive%2520Federated%2520Fine-Tuning%2520of%2520Large%2520Language%2520Models%2520with%250A%2520%2520Resource-Aware%2520Low-Rank%2520Adaption%26entry.906535625%3DYajie%2520Zhou%2520and%2520Xiaoyi%2520Pang%2520and%2520Zhibo%2520Wang%26entry.1292438233%3D%2520%2520Federated%2520fine-tuning%2520has%2520emerged%2520as%2520a%2520promising%2520approach%2520to%2520adapt%2520foundation%250Amodels%2520to%2520downstream%2520tasks%2520using%2520decentralized%2520data.%2520However%252C%2520real-world%250Adeployment%2520remains%2520challenging%2520due%2520to%2520the%2520high%2520computational%2520and%2520communication%250Ademands%2520of%2520fine-tuning%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520on%2520clients%2520with%2520data%2520and%250Asystem%2520resources%2520that%2520are%2520heterogeneous%2520and%2520constrained.%2520In%2520such%2520settings%252C%2520the%250Aglobal%2520model%2527s%2520performance%2520is%2520often%2520bottlenecked%2520by%2520the%2520weakest%2520clients%2520and%250Afurther%2520degraded%2520by%2520the%2520non-IID%2520nature%2520of%2520local%2520data.%2520Although%2520existing%2520methods%250Aleverage%2520parameter-efficient%2520techniques%2520such%2520as%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520to%250Areduce%2520communication%2520and%2520computation%2520overhead%252C%2520they%2520often%2520fail%2520to%250Asimultaneously%2520ensure%2520accurate%2520aggregation%2520of%2520low-rank%2520updates%2520and%2520maintain%2520low%250Asystem%2520costs%252C%2520thereby%2520hindering%2520overall%2520performance.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520AFLoRA%252C%2520an%2520adaptive%2520and%2520lightweight%2520federated%250Afine-tuning%2520framework%2520for%2520LLMs.%2520AFLoRA%2520decouples%2520shared%2520and%2520client-specific%250Aupdates%2520to%2520reduce%2520overhead%2520and%2520improve%2520aggregation%2520accuracy%252C%2520incorporates%250Adiagonal%2520matrix-based%2520rank%2520pruning%2520to%2520better%2520utilize%2520local%2520resources%252C%2520and%250Aemploys%2520rank-aware%2520aggregation%2520with%2520public%2520data%2520refinement%2520to%2520strengthen%250Ageneralization%2520under%2520data%2520heterogeneity.%2520Extensive%2520experiments%2520demonstrate%2520that%250AAFLoRA%2520outperforms%2520state-of-the-art%2520methods%2520in%2520both%2520accuracy%2520and%2520efficiency%252C%250Aproviding%2520a%2520practical%2520solution%2520for%2520efficient%2520LLM%2520adaptation%2520in%2520heterogeneous%250Aenvironments%2520in%2520the%2520real%2520world.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24773v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AFLoRA%3A%20Adaptive%20Federated%20Fine-Tuning%20of%20Large%20Language%20Models%20with%0A%20%20Resource-Aware%20Low-Rank%20Adaption&entry.906535625=Yajie%20Zhou%20and%20Xiaoyi%20Pang%20and%20Zhibo%20Wang&entry.1292438233=%20%20Federated%20fine-tuning%20has%20emerged%20as%20a%20promising%20approach%20to%20adapt%20foundation%0Amodels%20to%20downstream%20tasks%20using%20decentralized%20data.%20However%2C%20real-world%0Adeployment%20remains%20challenging%20due%20to%20the%20high%20computational%20and%20communication%0Ademands%20of%20fine-tuning%20Large%20Language%20Models%20%28LLMs%29%20on%20clients%20with%20data%20and%0Asystem%20resources%20that%20are%20heterogeneous%20and%20constrained.%20In%20such%20settings%2C%20the%0Aglobal%20model%27s%20performance%20is%20often%20bottlenecked%20by%20the%20weakest%20clients%20and%0Afurther%20degraded%20by%20the%20non-IID%20nature%20of%20local%20data.%20Although%20existing%20methods%0Aleverage%20parameter-efficient%20techniques%20such%20as%20Low-Rank%20Adaptation%20%28LoRA%29%20to%0Areduce%20communication%20and%20computation%20overhead%2C%20they%20often%20fail%20to%0Asimultaneously%20ensure%20accurate%20aggregation%20of%20low-rank%20updates%20and%20maintain%20low%0Asystem%20costs%2C%20thereby%20hindering%20overall%20performance.%20To%20address%20these%0Achallenges%2C%20we%20propose%20AFLoRA%2C%20an%20adaptive%20and%20lightweight%20federated%0Afine-tuning%20framework%20for%20LLMs.%20AFLoRA%20decouples%20shared%20and%20client-specific%0Aupdates%20to%20reduce%20overhead%20and%20improve%20aggregation%20accuracy%2C%20incorporates%0Adiagonal%20matrix-based%20rank%20pruning%20to%20better%20utilize%20local%20resources%2C%20and%0Aemploys%20rank-aware%20aggregation%20with%20public%20data%20refinement%20to%20strengthen%0Ageneralization%20under%20data%20heterogeneity.%20Extensive%20experiments%20demonstrate%20that%0AAFLoRA%20outperforms%20state-of-the-art%20methods%20in%20both%20accuracy%20and%20efficiency%2C%0Aproviding%20a%20practical%20solution%20for%20efficient%20LLM%20adaptation%20in%20heterogeneous%0Aenvironments%20in%20the%20real%20world.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24773v1&entry.124074799=Read"},
{"title": "MiniMax-Remover: Taming Bad Noise Helps Video Object Removal", "author": "Bojia Zi and Weixuan Peng and Xianbiao Qi and Jianan Wang and Shihao Zhao and Rong Xiao and Kam-Fai Wong", "abstract": "  Recent advances in video diffusion models have driven rapid progress in video\nediting techniques. However, video object removal, a critical subtask of video\nediting, remains challenging due to issues such as hallucinated objects and\nvisual artifacts. Furthermore, existing methods often rely on computationally\nexpensive sampling procedures and classifier-free guidance (CFG), resulting in\nslow inference. To address these limitations, we propose MiniMax-Remover, a\nnovel two-stage video object removal approach. Motivated by the observation\nthat text condition is not best suited for this task, we simplify the\npretrained video generation model by removing textual input and cross-attention\nlayers, resulting in a more lightweight and efficient model architecture in the\nfirst stage. In the second stage, we distilled our remover on successful videos\nproduced by the stage-1 model and curated by human annotators, using a minimax\noptimization strategy to further improve editing quality and inference speed.\nSpecifically, the inner maximization identifies adversarial input noise (\"bad\nnoise\") that makes failure removals, while the outer minimization step trains\nthe model to generate high-quality removal results even under such challenging\nconditions. As a result, our method achieves a state-of-the-art video object\nremoval results with as few as 6 sampling steps and doesn't rely on CFG,\nsignificantly improving inference efficiency. Extensive experiments demonstrate\nthe effectiveness and superiority of MiniMax-Remover compared to existing\nmethods. Codes and Videos are available at: https://minimax-remover.github.io.\n", "link": "http://arxiv.org/abs/2505.24873v1", "date": "2025-05-30", "relevancy": 2.3301, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5952}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5851}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MiniMax-Remover%3A%20Taming%20Bad%20Noise%20Helps%20Video%20Object%20Removal&body=Title%3A%20MiniMax-Remover%3A%20Taming%20Bad%20Noise%20Helps%20Video%20Object%20Removal%0AAuthor%3A%20Bojia%20Zi%20and%20Weixuan%20Peng%20and%20Xianbiao%20Qi%20and%20Jianan%20Wang%20and%20Shihao%20Zhao%20and%20Rong%20Xiao%20and%20Kam-Fai%20Wong%0AAbstract%3A%20%20%20Recent%20advances%20in%20video%20diffusion%20models%20have%20driven%20rapid%20progress%20in%20video%0Aediting%20techniques.%20However%2C%20video%20object%20removal%2C%20a%20critical%20subtask%20of%20video%0Aediting%2C%20remains%20challenging%20due%20to%20issues%20such%20as%20hallucinated%20objects%20and%0Avisual%20artifacts.%20Furthermore%2C%20existing%20methods%20often%20rely%20on%20computationally%0Aexpensive%20sampling%20procedures%20and%20classifier-free%20guidance%20%28CFG%29%2C%20resulting%20in%0Aslow%20inference.%20To%20address%20these%20limitations%2C%20we%20propose%20MiniMax-Remover%2C%20a%0Anovel%20two-stage%20video%20object%20removal%20approach.%20Motivated%20by%20the%20observation%0Athat%20text%20condition%20is%20not%20best%20suited%20for%20this%20task%2C%20we%20simplify%20the%0Apretrained%20video%20generation%20model%20by%20removing%20textual%20input%20and%20cross-attention%0Alayers%2C%20resulting%20in%20a%20more%20lightweight%20and%20efficient%20model%20architecture%20in%20the%0Afirst%20stage.%20In%20the%20second%20stage%2C%20we%20distilled%20our%20remover%20on%20successful%20videos%0Aproduced%20by%20the%20stage-1%20model%20and%20curated%20by%20human%20annotators%2C%20using%20a%20minimax%0Aoptimization%20strategy%20to%20further%20improve%20editing%20quality%20and%20inference%20speed.%0ASpecifically%2C%20the%20inner%20maximization%20identifies%20adversarial%20input%20noise%20%28%22bad%0Anoise%22%29%20that%20makes%20failure%20removals%2C%20while%20the%20outer%20minimization%20step%20trains%0Athe%20model%20to%20generate%20high-quality%20removal%20results%20even%20under%20such%20challenging%0Aconditions.%20As%20a%20result%2C%20our%20method%20achieves%20a%20state-of-the-art%20video%20object%0Aremoval%20results%20with%20as%20few%20as%206%20sampling%20steps%20and%20doesn%27t%20rely%20on%20CFG%2C%0Asignificantly%20improving%20inference%20efficiency.%20Extensive%20experiments%20demonstrate%0Athe%20effectiveness%20and%20superiority%20of%20MiniMax-Remover%20compared%20to%20existing%0Amethods.%20Codes%20and%20Videos%20are%20available%20at%3A%20https%3A//minimax-remover.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24873v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMiniMax-Remover%253A%2520Taming%2520Bad%2520Noise%2520Helps%2520Video%2520Object%2520Removal%26entry.906535625%3DBojia%2520Zi%2520and%2520Weixuan%2520Peng%2520and%2520Xianbiao%2520Qi%2520and%2520Jianan%2520Wang%2520and%2520Shihao%2520Zhao%2520and%2520Rong%2520Xiao%2520and%2520Kam-Fai%2520Wong%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520video%2520diffusion%2520models%2520have%2520driven%2520rapid%2520progress%2520in%2520video%250Aediting%2520techniques.%2520However%252C%2520video%2520object%2520removal%252C%2520a%2520critical%2520subtask%2520of%2520video%250Aediting%252C%2520remains%2520challenging%2520due%2520to%2520issues%2520such%2520as%2520hallucinated%2520objects%2520and%250Avisual%2520artifacts.%2520Furthermore%252C%2520existing%2520methods%2520often%2520rely%2520on%2520computationally%250Aexpensive%2520sampling%2520procedures%2520and%2520classifier-free%2520guidance%2520%2528CFG%2529%252C%2520resulting%2520in%250Aslow%2520inference.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520MiniMax-Remover%252C%2520a%250Anovel%2520two-stage%2520video%2520object%2520removal%2520approach.%2520Motivated%2520by%2520the%2520observation%250Athat%2520text%2520condition%2520is%2520not%2520best%2520suited%2520for%2520this%2520task%252C%2520we%2520simplify%2520the%250Apretrained%2520video%2520generation%2520model%2520by%2520removing%2520textual%2520input%2520and%2520cross-attention%250Alayers%252C%2520resulting%2520in%2520a%2520more%2520lightweight%2520and%2520efficient%2520model%2520architecture%2520in%2520the%250Afirst%2520stage.%2520In%2520the%2520second%2520stage%252C%2520we%2520distilled%2520our%2520remover%2520on%2520successful%2520videos%250Aproduced%2520by%2520the%2520stage-1%2520model%2520and%2520curated%2520by%2520human%2520annotators%252C%2520using%2520a%2520minimax%250Aoptimization%2520strategy%2520to%2520further%2520improve%2520editing%2520quality%2520and%2520inference%2520speed.%250ASpecifically%252C%2520the%2520inner%2520maximization%2520identifies%2520adversarial%2520input%2520noise%2520%2528%2522bad%250Anoise%2522%2529%2520that%2520makes%2520failure%2520removals%252C%2520while%2520the%2520outer%2520minimization%2520step%2520trains%250Athe%2520model%2520to%2520generate%2520high-quality%2520removal%2520results%2520even%2520under%2520such%2520challenging%250Aconditions.%2520As%2520a%2520result%252C%2520our%2520method%2520achieves%2520a%2520state-of-the-art%2520video%2520object%250Aremoval%2520results%2520with%2520as%2520few%2520as%25206%2520sampling%2520steps%2520and%2520doesn%2527t%2520rely%2520on%2520CFG%252C%250Asignificantly%2520improving%2520inference%2520efficiency.%2520Extensive%2520experiments%2520demonstrate%250Athe%2520effectiveness%2520and%2520superiority%2520of%2520MiniMax-Remover%2520compared%2520to%2520existing%250Amethods.%2520Codes%2520and%2520Videos%2520are%2520available%2520at%253A%2520https%253A//minimax-remover.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24873v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MiniMax-Remover%3A%20Taming%20Bad%20Noise%20Helps%20Video%20Object%20Removal&entry.906535625=Bojia%20Zi%20and%20Weixuan%20Peng%20and%20Xianbiao%20Qi%20and%20Jianan%20Wang%20and%20Shihao%20Zhao%20and%20Rong%20Xiao%20and%20Kam-Fai%20Wong&entry.1292438233=%20%20Recent%20advances%20in%20video%20diffusion%20models%20have%20driven%20rapid%20progress%20in%20video%0Aediting%20techniques.%20However%2C%20video%20object%20removal%2C%20a%20critical%20subtask%20of%20video%0Aediting%2C%20remains%20challenging%20due%20to%20issues%20such%20as%20hallucinated%20objects%20and%0Avisual%20artifacts.%20Furthermore%2C%20existing%20methods%20often%20rely%20on%20computationally%0Aexpensive%20sampling%20procedures%20and%20classifier-free%20guidance%20%28CFG%29%2C%20resulting%20in%0Aslow%20inference.%20To%20address%20these%20limitations%2C%20we%20propose%20MiniMax-Remover%2C%20a%0Anovel%20two-stage%20video%20object%20removal%20approach.%20Motivated%20by%20the%20observation%0Athat%20text%20condition%20is%20not%20best%20suited%20for%20this%20task%2C%20we%20simplify%20the%0Apretrained%20video%20generation%20model%20by%20removing%20textual%20input%20and%20cross-attention%0Alayers%2C%20resulting%20in%20a%20more%20lightweight%20and%20efficient%20model%20architecture%20in%20the%0Afirst%20stage.%20In%20the%20second%20stage%2C%20we%20distilled%20our%20remover%20on%20successful%20videos%0Aproduced%20by%20the%20stage-1%20model%20and%20curated%20by%20human%20annotators%2C%20using%20a%20minimax%0Aoptimization%20strategy%20to%20further%20improve%20editing%20quality%20and%20inference%20speed.%0ASpecifically%2C%20the%20inner%20maximization%20identifies%20adversarial%20input%20noise%20%28%22bad%0Anoise%22%29%20that%20makes%20failure%20removals%2C%20while%20the%20outer%20minimization%20step%20trains%0Athe%20model%20to%20generate%20high-quality%20removal%20results%20even%20under%20such%20challenging%0Aconditions.%20As%20a%20result%2C%20our%20method%20achieves%20a%20state-of-the-art%20video%20object%0Aremoval%20results%20with%20as%20few%20as%206%20sampling%20steps%20and%20doesn%27t%20rely%20on%20CFG%2C%0Asignificantly%20improving%20inference%20efficiency.%20Extensive%20experiments%20demonstrate%0Athe%20effectiveness%20and%20superiority%20of%20MiniMax-Remover%20compared%20to%20existing%0Amethods.%20Codes%20and%20Videos%20are%20available%20at%3A%20https%3A//minimax-remover.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24873v1&entry.124074799=Read"},
{"title": "Bi-Manual Joint Camera Calibration and Scene Representation", "author": "Haozhan Tang and Tianyi Zhang and Matthew Johnson-Roberson and Weiming Zhi", "abstract": "  Robot manipulation, especially bimanual manipulation, often requires setting\nup multiple cameras on multiple robot manipulators. Before robot manipulators\ncan generate motion or even build representations of their environments, the\ncameras rigidly mounted to the robot need to be calibrated. Camera calibration\nis a cumbersome process involving collecting a set of images, with each\ncapturing a pre-determined marker. In this work, we introduce the Bi-Manual\nJoint Calibration and Representation Framework (Bi-JCR). Bi-JCR enables\nmultiple robot manipulators, each with cameras mounted, to circumvent taking\nimages of calibration markers. By leveraging 3D foundation models for dense,\nmarker-free multi-view correspondence, Bi-JCR jointly estimates: (i) the\nextrinsic transformation from each camera to its end-effector, (ii) the\ninter-arm relative poses between manipulators, and (iii) a unified,\nscale-consistent 3D representation of the shared workspace, all from the same\ncaptured RGB image sets. The representation, jointly constructed from images\ncaptured by cameras on both manipulators, lives in a common coordinate frame\nand supports collision checking and semantic segmentation to facilitate\ndownstream bimanual coordination tasks. We empirically evaluate the robustness\nof Bi-JCR on a variety of tabletop environments, and demonstrate its\napplicability on a variety of downstream tasks.\n", "link": "http://arxiv.org/abs/2505.24819v1", "date": "2025-05-30", "relevancy": 1.1723, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6012}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5835}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bi-Manual%20Joint%20Camera%20Calibration%20and%20Scene%20Representation&body=Title%3A%20Bi-Manual%20Joint%20Camera%20Calibration%20and%20Scene%20Representation%0AAuthor%3A%20Haozhan%20Tang%20and%20Tianyi%20Zhang%20and%20Matthew%20Johnson-Roberson%20and%20Weiming%20Zhi%0AAbstract%3A%20%20%20Robot%20manipulation%2C%20especially%20bimanual%20manipulation%2C%20often%20requires%20setting%0Aup%20multiple%20cameras%20on%20multiple%20robot%20manipulators.%20Before%20robot%20manipulators%0Acan%20generate%20motion%20or%20even%20build%20representations%20of%20their%20environments%2C%20the%0Acameras%20rigidly%20mounted%20to%20the%20robot%20need%20to%20be%20calibrated.%20Camera%20calibration%0Ais%20a%20cumbersome%20process%20involving%20collecting%20a%20set%20of%20images%2C%20with%20each%0Acapturing%20a%20pre-determined%20marker.%20In%20this%20work%2C%20we%20introduce%20the%20Bi-Manual%0AJoint%20Calibration%20and%20Representation%20Framework%20%28Bi-JCR%29.%20Bi-JCR%20enables%0Amultiple%20robot%20manipulators%2C%20each%20with%20cameras%20mounted%2C%20to%20circumvent%20taking%0Aimages%20of%20calibration%20markers.%20By%20leveraging%203D%20foundation%20models%20for%20dense%2C%0Amarker-free%20multi-view%20correspondence%2C%20Bi-JCR%20jointly%20estimates%3A%20%28i%29%20the%0Aextrinsic%20transformation%20from%20each%20camera%20to%20its%20end-effector%2C%20%28ii%29%20the%0Ainter-arm%20relative%20poses%20between%20manipulators%2C%20and%20%28iii%29%20a%20unified%2C%0Ascale-consistent%203D%20representation%20of%20the%20shared%20workspace%2C%20all%20from%20the%20same%0Acaptured%20RGB%20image%20sets.%20The%20representation%2C%20jointly%20constructed%20from%20images%0Acaptured%20by%20cameras%20on%20both%20manipulators%2C%20lives%20in%20a%20common%20coordinate%20frame%0Aand%20supports%20collision%20checking%20and%20semantic%20segmentation%20to%20facilitate%0Adownstream%20bimanual%20coordination%20tasks.%20We%20empirically%20evaluate%20the%20robustness%0Aof%20Bi-JCR%20on%20a%20variety%20of%20tabletop%20environments%2C%20and%20demonstrate%20its%0Aapplicability%20on%20a%20variety%20of%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24819v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBi-Manual%2520Joint%2520Camera%2520Calibration%2520and%2520Scene%2520Representation%26entry.906535625%3DHaozhan%2520Tang%2520and%2520Tianyi%2520Zhang%2520and%2520Matthew%2520Johnson-Roberson%2520and%2520Weiming%2520Zhi%26entry.1292438233%3D%2520%2520Robot%2520manipulation%252C%2520especially%2520bimanual%2520manipulation%252C%2520often%2520requires%2520setting%250Aup%2520multiple%2520cameras%2520on%2520multiple%2520robot%2520manipulators.%2520Before%2520robot%2520manipulators%250Acan%2520generate%2520motion%2520or%2520even%2520build%2520representations%2520of%2520their%2520environments%252C%2520the%250Acameras%2520rigidly%2520mounted%2520to%2520the%2520robot%2520need%2520to%2520be%2520calibrated.%2520Camera%2520calibration%250Ais%2520a%2520cumbersome%2520process%2520involving%2520collecting%2520a%2520set%2520of%2520images%252C%2520with%2520each%250Acapturing%2520a%2520pre-determined%2520marker.%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%2520Bi-Manual%250AJoint%2520Calibration%2520and%2520Representation%2520Framework%2520%2528Bi-JCR%2529.%2520Bi-JCR%2520enables%250Amultiple%2520robot%2520manipulators%252C%2520each%2520with%2520cameras%2520mounted%252C%2520to%2520circumvent%2520taking%250Aimages%2520of%2520calibration%2520markers.%2520By%2520leveraging%25203D%2520foundation%2520models%2520for%2520dense%252C%250Amarker-free%2520multi-view%2520correspondence%252C%2520Bi-JCR%2520jointly%2520estimates%253A%2520%2528i%2529%2520the%250Aextrinsic%2520transformation%2520from%2520each%2520camera%2520to%2520its%2520end-effector%252C%2520%2528ii%2529%2520the%250Ainter-arm%2520relative%2520poses%2520between%2520manipulators%252C%2520and%2520%2528iii%2529%2520a%2520unified%252C%250Ascale-consistent%25203D%2520representation%2520of%2520the%2520shared%2520workspace%252C%2520all%2520from%2520the%2520same%250Acaptured%2520RGB%2520image%2520sets.%2520The%2520representation%252C%2520jointly%2520constructed%2520from%2520images%250Acaptured%2520by%2520cameras%2520on%2520both%2520manipulators%252C%2520lives%2520in%2520a%2520common%2520coordinate%2520frame%250Aand%2520supports%2520collision%2520checking%2520and%2520semantic%2520segmentation%2520to%2520facilitate%250Adownstream%2520bimanual%2520coordination%2520tasks.%2520We%2520empirically%2520evaluate%2520the%2520robustness%250Aof%2520Bi-JCR%2520on%2520a%2520variety%2520of%2520tabletop%2520environments%252C%2520and%2520demonstrate%2520its%250Aapplicability%2520on%2520a%2520variety%2520of%2520downstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24819v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bi-Manual%20Joint%20Camera%20Calibration%20and%20Scene%20Representation&entry.906535625=Haozhan%20Tang%20and%20Tianyi%20Zhang%20and%20Matthew%20Johnson-Roberson%20and%20Weiming%20Zhi&entry.1292438233=%20%20Robot%20manipulation%2C%20especially%20bimanual%20manipulation%2C%20often%20requires%20setting%0Aup%20multiple%20cameras%20on%20multiple%20robot%20manipulators.%20Before%20robot%20manipulators%0Acan%20generate%20motion%20or%20even%20build%20representations%20of%20their%20environments%2C%20the%0Acameras%20rigidly%20mounted%20to%20the%20robot%20need%20to%20be%20calibrated.%20Camera%20calibration%0Ais%20a%20cumbersome%20process%20involving%20collecting%20a%20set%20of%20images%2C%20with%20each%0Acapturing%20a%20pre-determined%20marker.%20In%20this%20work%2C%20we%20introduce%20the%20Bi-Manual%0AJoint%20Calibration%20and%20Representation%20Framework%20%28Bi-JCR%29.%20Bi-JCR%20enables%0Amultiple%20robot%20manipulators%2C%20each%20with%20cameras%20mounted%2C%20to%20circumvent%20taking%0Aimages%20of%20calibration%20markers.%20By%20leveraging%203D%20foundation%20models%20for%20dense%2C%0Amarker-free%20multi-view%20correspondence%2C%20Bi-JCR%20jointly%20estimates%3A%20%28i%29%20the%0Aextrinsic%20transformation%20from%20each%20camera%20to%20its%20end-effector%2C%20%28ii%29%20the%0Ainter-arm%20relative%20poses%20between%20manipulators%2C%20and%20%28iii%29%20a%20unified%2C%0Ascale-consistent%203D%20representation%20of%20the%20shared%20workspace%2C%20all%20from%20the%20same%0Acaptured%20RGB%20image%20sets.%20The%20representation%2C%20jointly%20constructed%20from%20images%0Acaptured%20by%20cameras%20on%20both%20manipulators%2C%20lives%20in%20a%20common%20coordinate%20frame%0Aand%20supports%20collision%20checking%20and%20semantic%20segmentation%20to%20facilitate%0Adownstream%20bimanual%20coordination%20tasks.%20We%20empirically%20evaluate%20the%20robustness%0Aof%20Bi-JCR%20on%20a%20variety%20of%20tabletop%20environments%2C%20and%20demonstrate%20its%0Aapplicability%20on%20a%20variety%20of%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24819v1&entry.124074799=Read"},
{"title": "Scene-Adaptive Motion Planning with Explicit Mixture of Experts and\n  Interaction-Oriented Optimization", "author": "Hongbiao Zhu and Liulong Ma and Xian Wu and Xin Deng and Xiaoyao Liang", "abstract": "  Despite over a decade of development, autonomous driving trajectory planning\nin complex urban environments continues to encounter significant challenges.\nThese challenges include the difficulty in accommodating the multi-modal nature\nof trajectories, the limitations of single expert model in managing diverse\nscenarios, and insufficient consideration of environmental interactions. To\naddress these issues, this paper introduces the EMoE-Planner, which\nincorporates three innovative approaches. Firstly, the Explicit MoE (Mixture of\nExperts) dynamically selects specialized experts based on scenario-specific\ninformation through a shared scene router. Secondly, the planner utilizes\nscene-specific queries to provide multi-modal priors, directing the model's\nfocus towards relevant target areas. Lastly, it enhances the prediction model\nand loss calculation by considering the interactions between the ego vehicle\nand other agents, thereby significantly boosting planning performance.\nComparative experiments were conducted on the Nuplan dataset against the\nstate-of-the-art methods. The simulation results demonstrate that our model\nconsistently outperforms SOTA models across nearly all test scenarios. Our\nmodel is the first pure learning model to achieve performance surpassing\nrule-based algorithms in almost all Nuplan closed-loop simulations.\n", "link": "http://arxiv.org/abs/2505.12311v2", "date": "2025-05-30", "relevancy": 1.6677, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5602}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5575}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scene-Adaptive%20Motion%20Planning%20with%20Explicit%20Mixture%20of%20Experts%20and%0A%20%20Interaction-Oriented%20Optimization&body=Title%3A%20Scene-Adaptive%20Motion%20Planning%20with%20Explicit%20Mixture%20of%20Experts%20and%0A%20%20Interaction-Oriented%20Optimization%0AAuthor%3A%20Hongbiao%20Zhu%20and%20Liulong%20Ma%20and%20Xian%20Wu%20and%20Xin%20Deng%20and%20Xiaoyao%20Liang%0AAbstract%3A%20%20%20Despite%20over%20a%20decade%20of%20development%2C%20autonomous%20driving%20trajectory%20planning%0Ain%20complex%20urban%20environments%20continues%20to%20encounter%20significant%20challenges.%0AThese%20challenges%20include%20the%20difficulty%20in%20accommodating%20the%20multi-modal%20nature%0Aof%20trajectories%2C%20the%20limitations%20of%20single%20expert%20model%20in%20managing%20diverse%0Ascenarios%2C%20and%20insufficient%20consideration%20of%20environmental%20interactions.%20To%0Aaddress%20these%20issues%2C%20this%20paper%20introduces%20the%20EMoE-Planner%2C%20which%0Aincorporates%20three%20innovative%20approaches.%20Firstly%2C%20the%20Explicit%20MoE%20%28Mixture%20of%0AExperts%29%20dynamically%20selects%20specialized%20experts%20based%20on%20scenario-specific%0Ainformation%20through%20a%20shared%20scene%20router.%20Secondly%2C%20the%20planner%20utilizes%0Ascene-specific%20queries%20to%20provide%20multi-modal%20priors%2C%20directing%20the%20model%27s%0Afocus%20towards%20relevant%20target%20areas.%20Lastly%2C%20it%20enhances%20the%20prediction%20model%0Aand%20loss%20calculation%20by%20considering%20the%20interactions%20between%20the%20ego%20vehicle%0Aand%20other%20agents%2C%20thereby%20significantly%20boosting%20planning%20performance.%0AComparative%20experiments%20were%20conducted%20on%20the%20Nuplan%20dataset%20against%20the%0Astate-of-the-art%20methods.%20The%20simulation%20results%20demonstrate%20that%20our%20model%0Aconsistently%20outperforms%20SOTA%20models%20across%20nearly%20all%20test%20scenarios.%20Our%0Amodel%20is%20the%20first%20pure%20learning%20model%20to%20achieve%20performance%20surpassing%0Arule-based%20algorithms%20in%20almost%20all%20Nuplan%20closed-loop%20simulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12311v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScene-Adaptive%2520Motion%2520Planning%2520with%2520Explicit%2520Mixture%2520of%2520Experts%2520and%250A%2520%2520Interaction-Oriented%2520Optimization%26entry.906535625%3DHongbiao%2520Zhu%2520and%2520Liulong%2520Ma%2520and%2520Xian%2520Wu%2520and%2520Xin%2520Deng%2520and%2520Xiaoyao%2520Liang%26entry.1292438233%3D%2520%2520Despite%2520over%2520a%2520decade%2520of%2520development%252C%2520autonomous%2520driving%2520trajectory%2520planning%250Ain%2520complex%2520urban%2520environments%2520continues%2520to%2520encounter%2520significant%2520challenges.%250AThese%2520challenges%2520include%2520the%2520difficulty%2520in%2520accommodating%2520the%2520multi-modal%2520nature%250Aof%2520trajectories%252C%2520the%2520limitations%2520of%2520single%2520expert%2520model%2520in%2520managing%2520diverse%250Ascenarios%252C%2520and%2520insufficient%2520consideration%2520of%2520environmental%2520interactions.%2520To%250Aaddress%2520these%2520issues%252C%2520this%2520paper%2520introduces%2520the%2520EMoE-Planner%252C%2520which%250Aincorporates%2520three%2520innovative%2520approaches.%2520Firstly%252C%2520the%2520Explicit%2520MoE%2520%2528Mixture%2520of%250AExperts%2529%2520dynamically%2520selects%2520specialized%2520experts%2520based%2520on%2520scenario-specific%250Ainformation%2520through%2520a%2520shared%2520scene%2520router.%2520Secondly%252C%2520the%2520planner%2520utilizes%250Ascene-specific%2520queries%2520to%2520provide%2520multi-modal%2520priors%252C%2520directing%2520the%2520model%2527s%250Afocus%2520towards%2520relevant%2520target%2520areas.%2520Lastly%252C%2520it%2520enhances%2520the%2520prediction%2520model%250Aand%2520loss%2520calculation%2520by%2520considering%2520the%2520interactions%2520between%2520the%2520ego%2520vehicle%250Aand%2520other%2520agents%252C%2520thereby%2520significantly%2520boosting%2520planning%2520performance.%250AComparative%2520experiments%2520were%2520conducted%2520on%2520the%2520Nuplan%2520dataset%2520against%2520the%250Astate-of-the-art%2520methods.%2520The%2520simulation%2520results%2520demonstrate%2520that%2520our%2520model%250Aconsistently%2520outperforms%2520SOTA%2520models%2520across%2520nearly%2520all%2520test%2520scenarios.%2520Our%250Amodel%2520is%2520the%2520first%2520pure%2520learning%2520model%2520to%2520achieve%2520performance%2520surpassing%250Arule-based%2520algorithms%2520in%2520almost%2520all%2520Nuplan%2520closed-loop%2520simulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12311v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scene-Adaptive%20Motion%20Planning%20with%20Explicit%20Mixture%20of%20Experts%20and%0A%20%20Interaction-Oriented%20Optimization&entry.906535625=Hongbiao%20Zhu%20and%20Liulong%20Ma%20and%20Xian%20Wu%20and%20Xin%20Deng%20and%20Xiaoyao%20Liang&entry.1292438233=%20%20Despite%20over%20a%20decade%20of%20development%2C%20autonomous%20driving%20trajectory%20planning%0Ain%20complex%20urban%20environments%20continues%20to%20encounter%20significant%20challenges.%0AThese%20challenges%20include%20the%20difficulty%20in%20accommodating%20the%20multi-modal%20nature%0Aof%20trajectories%2C%20the%20limitations%20of%20single%20expert%20model%20in%20managing%20diverse%0Ascenarios%2C%20and%20insufficient%20consideration%20of%20environmental%20interactions.%20To%0Aaddress%20these%20issues%2C%20this%20paper%20introduces%20the%20EMoE-Planner%2C%20which%0Aincorporates%20three%20innovative%20approaches.%20Firstly%2C%20the%20Explicit%20MoE%20%28Mixture%20of%0AExperts%29%20dynamically%20selects%20specialized%20experts%20based%20on%20scenario-specific%0Ainformation%20through%20a%20shared%20scene%20router.%20Secondly%2C%20the%20planner%20utilizes%0Ascene-specific%20queries%20to%20provide%20multi-modal%20priors%2C%20directing%20the%20model%27s%0Afocus%20towards%20relevant%20target%20areas.%20Lastly%2C%20it%20enhances%20the%20prediction%20model%0Aand%20loss%20calculation%20by%20considering%20the%20interactions%20between%20the%20ego%20vehicle%0Aand%20other%20agents%2C%20thereby%20significantly%20boosting%20planning%20performance.%0AComparative%20experiments%20were%20conducted%20on%20the%20Nuplan%20dataset%20against%20the%0Astate-of-the-art%20methods.%20The%20simulation%20results%20demonstrate%20that%20our%20model%0Aconsistently%20outperforms%20SOTA%20models%20across%20nearly%20all%20test%20scenarios.%20Our%0Amodel%20is%20the%20first%20pure%20learning%20model%20to%20achieve%20performance%20surpassing%0Arule-based%20algorithms%20in%20almost%20all%20Nuplan%20closed-loop%20simulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12311v2&entry.124074799=Read"},
{"title": "CAE-Net: Generalized Deepfake Image Detection using Convolution and\n  Attention Mechanisms with Spatial and Frequency Domain Features", "author": "Kafi Anan and Anindya Bhattacharjee and Ashir Intesher and Kaidul Islam and Abrar Assaeem Fuad and Utsab Saha and Hafiz Imtiaz", "abstract": "  Effective deepfake detection tools are becoming increasingly essential to the\ngrowing usage of deepfakes in unethical practices. There exists a wide range of\ndeepfake generation techniques, which makes it challenging to develop an\naccurate universal detection mechanism. The 2025 IEEE Signal Processing Cup\n(\\textit{DFWild-Cup} competition) provided a diverse dataset of deepfake images\ncontaining significant class imbalance. The images in the dataset are generated\nfrom multiple deepfake image generators, for training machine learning model(s)\nto emphasize the generalization of deepfake detection. To this end, we proposed\na disjoint set-based multistage training method to address the class imbalance\nand devised an ensemble-based architecture \\emph{CAE-Net}. Our architecture\nconsists of a convolution- and attention-based ensemble network, and employs\nthree different neural network architectures: EfficientNet, Data-Efficient\nImage Transformer (DeiT), and ConvNeXt with wavelet transform to capture both\nlocal and global features of deepfakes. We visualize the specific regions that\nthese models focus on for classification using Grad-CAM, and empirically\ndemonstrate the effectiveness of these models in grouping real and fake images\ninto cohesive clusters using t-SNE plots. Individually, the EfficientNet B0\narchitecture has achieved 90.79\\% accuracy, whereas the ConvNeXt and the DeiT\narchitecture have achieved 89.49\\% and 89.32\\% accuracy, respectively. With\nthese networks, our weighted ensemble model achieves an excellent accuracy of\n94.63\\% on the validation dataset of the SP Cup 2025 competition. The equal\nerror rate of 4.72\\% and the Area Under the ROC curve of 97.37\\% further\nconfirm the stability of our proposed method. Finally, the robustness of our\nproposed model against adversarial perturbation attacks is tested as well,\nshowing the inherent defensive properties of the ensemble approach.\n", "link": "http://arxiv.org/abs/2502.10682v2", "date": "2025-05-30", "relevancy": 2.1815, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.561}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.542}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAE-Net%3A%20Generalized%20Deepfake%20Image%20Detection%20using%20Convolution%20and%0A%20%20Attention%20Mechanisms%20with%20Spatial%20and%20Frequency%20Domain%20Features&body=Title%3A%20CAE-Net%3A%20Generalized%20Deepfake%20Image%20Detection%20using%20Convolution%20and%0A%20%20Attention%20Mechanisms%20with%20Spatial%20and%20Frequency%20Domain%20Features%0AAuthor%3A%20Kafi%20Anan%20and%20Anindya%20Bhattacharjee%20and%20Ashir%20Intesher%20and%20Kaidul%20Islam%20and%20Abrar%20Assaeem%20Fuad%20and%20Utsab%20Saha%20and%20Hafiz%20Imtiaz%0AAbstract%3A%20%20%20Effective%20deepfake%20detection%20tools%20are%20becoming%20increasingly%20essential%20to%20the%0Agrowing%20usage%20of%20deepfakes%20in%20unethical%20practices.%20There%20exists%20a%20wide%20range%20of%0Adeepfake%20generation%20techniques%2C%20which%20makes%20it%20challenging%20to%20develop%20an%0Aaccurate%20universal%20detection%20mechanism.%20The%202025%20IEEE%20Signal%20Processing%20Cup%0A%28%5Ctextit%7BDFWild-Cup%7D%20competition%29%20provided%20a%20diverse%20dataset%20of%20deepfake%20images%0Acontaining%20significant%20class%20imbalance.%20The%20images%20in%20the%20dataset%20are%20generated%0Afrom%20multiple%20deepfake%20image%20generators%2C%20for%20training%20machine%20learning%20model%28s%29%0Ato%20emphasize%20the%20generalization%20of%20deepfake%20detection.%20To%20this%20end%2C%20we%20proposed%0Aa%20disjoint%20set-based%20multistage%20training%20method%20to%20address%20the%20class%20imbalance%0Aand%20devised%20an%20ensemble-based%20architecture%20%5Cemph%7BCAE-Net%7D.%20Our%20architecture%0Aconsists%20of%20a%20convolution-%20and%20attention-based%20ensemble%20network%2C%20and%20employs%0Athree%20different%20neural%20network%20architectures%3A%20EfficientNet%2C%20Data-Efficient%0AImage%20Transformer%20%28DeiT%29%2C%20and%20ConvNeXt%20with%20wavelet%20transform%20to%20capture%20both%0Alocal%20and%20global%20features%20of%20deepfakes.%20We%20visualize%20the%20specific%20regions%20that%0Athese%20models%20focus%20on%20for%20classification%20using%20Grad-CAM%2C%20and%20empirically%0Ademonstrate%20the%20effectiveness%20of%20these%20models%20in%20grouping%20real%20and%20fake%20images%0Ainto%20cohesive%20clusters%20using%20t-SNE%20plots.%20Individually%2C%20the%20EfficientNet%20B0%0Aarchitecture%20has%20achieved%2090.79%5C%25%20accuracy%2C%20whereas%20the%20ConvNeXt%20and%20the%20DeiT%0Aarchitecture%20have%20achieved%2089.49%5C%25%20and%2089.32%5C%25%20accuracy%2C%20respectively.%20With%0Athese%20networks%2C%20our%20weighted%20ensemble%20model%20achieves%20an%20excellent%20accuracy%20of%0A94.63%5C%25%20on%20the%20validation%20dataset%20of%20the%20SP%20Cup%202025%20competition.%20The%20equal%0Aerror%20rate%20of%204.72%5C%25%20and%20the%20Area%20Under%20the%20ROC%20curve%20of%2097.37%5C%25%20further%0Aconfirm%20the%20stability%20of%20our%20proposed%20method.%20Finally%2C%20the%20robustness%20of%20our%0Aproposed%20model%20against%20adversarial%20perturbation%20attacks%20is%20tested%20as%20well%2C%0Ashowing%20the%20inherent%20defensive%20properties%20of%20the%20ensemble%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10682v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAE-Net%253A%2520Generalized%2520Deepfake%2520Image%2520Detection%2520using%2520Convolution%2520and%250A%2520%2520Attention%2520Mechanisms%2520with%2520Spatial%2520and%2520Frequency%2520Domain%2520Features%26entry.906535625%3DKafi%2520Anan%2520and%2520Anindya%2520Bhattacharjee%2520and%2520Ashir%2520Intesher%2520and%2520Kaidul%2520Islam%2520and%2520Abrar%2520Assaeem%2520Fuad%2520and%2520Utsab%2520Saha%2520and%2520Hafiz%2520Imtiaz%26entry.1292438233%3D%2520%2520Effective%2520deepfake%2520detection%2520tools%2520are%2520becoming%2520increasingly%2520essential%2520to%2520the%250Agrowing%2520usage%2520of%2520deepfakes%2520in%2520unethical%2520practices.%2520There%2520exists%2520a%2520wide%2520range%2520of%250Adeepfake%2520generation%2520techniques%252C%2520which%2520makes%2520it%2520challenging%2520to%2520develop%2520an%250Aaccurate%2520universal%2520detection%2520mechanism.%2520The%25202025%2520IEEE%2520Signal%2520Processing%2520Cup%250A%2528%255Ctextit%257BDFWild-Cup%257D%2520competition%2529%2520provided%2520a%2520diverse%2520dataset%2520of%2520deepfake%2520images%250Acontaining%2520significant%2520class%2520imbalance.%2520The%2520images%2520in%2520the%2520dataset%2520are%2520generated%250Afrom%2520multiple%2520deepfake%2520image%2520generators%252C%2520for%2520training%2520machine%2520learning%2520model%2528s%2529%250Ato%2520emphasize%2520the%2520generalization%2520of%2520deepfake%2520detection.%2520To%2520this%2520end%252C%2520we%2520proposed%250Aa%2520disjoint%2520set-based%2520multistage%2520training%2520method%2520to%2520address%2520the%2520class%2520imbalance%250Aand%2520devised%2520an%2520ensemble-based%2520architecture%2520%255Cemph%257BCAE-Net%257D.%2520Our%2520architecture%250Aconsists%2520of%2520a%2520convolution-%2520and%2520attention-based%2520ensemble%2520network%252C%2520and%2520employs%250Athree%2520different%2520neural%2520network%2520architectures%253A%2520EfficientNet%252C%2520Data-Efficient%250AImage%2520Transformer%2520%2528DeiT%2529%252C%2520and%2520ConvNeXt%2520with%2520wavelet%2520transform%2520to%2520capture%2520both%250Alocal%2520and%2520global%2520features%2520of%2520deepfakes.%2520We%2520visualize%2520the%2520specific%2520regions%2520that%250Athese%2520models%2520focus%2520on%2520for%2520classification%2520using%2520Grad-CAM%252C%2520and%2520empirically%250Ademonstrate%2520the%2520effectiveness%2520of%2520these%2520models%2520in%2520grouping%2520real%2520and%2520fake%2520images%250Ainto%2520cohesive%2520clusters%2520using%2520t-SNE%2520plots.%2520Individually%252C%2520the%2520EfficientNet%2520B0%250Aarchitecture%2520has%2520achieved%252090.79%255C%2525%2520accuracy%252C%2520whereas%2520the%2520ConvNeXt%2520and%2520the%2520DeiT%250Aarchitecture%2520have%2520achieved%252089.49%255C%2525%2520and%252089.32%255C%2525%2520accuracy%252C%2520respectively.%2520With%250Athese%2520networks%252C%2520our%2520weighted%2520ensemble%2520model%2520achieves%2520an%2520excellent%2520accuracy%2520of%250A94.63%255C%2525%2520on%2520the%2520validation%2520dataset%2520of%2520the%2520SP%2520Cup%25202025%2520competition.%2520The%2520equal%250Aerror%2520rate%2520of%25204.72%255C%2525%2520and%2520the%2520Area%2520Under%2520the%2520ROC%2520curve%2520of%252097.37%255C%2525%2520further%250Aconfirm%2520the%2520stability%2520of%2520our%2520proposed%2520method.%2520Finally%252C%2520the%2520robustness%2520of%2520our%250Aproposed%2520model%2520against%2520adversarial%2520perturbation%2520attacks%2520is%2520tested%2520as%2520well%252C%250Ashowing%2520the%2520inherent%2520defensive%2520properties%2520of%2520the%2520ensemble%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10682v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAE-Net%3A%20Generalized%20Deepfake%20Image%20Detection%20using%20Convolution%20and%0A%20%20Attention%20Mechanisms%20with%20Spatial%20and%20Frequency%20Domain%20Features&entry.906535625=Kafi%20Anan%20and%20Anindya%20Bhattacharjee%20and%20Ashir%20Intesher%20and%20Kaidul%20Islam%20and%20Abrar%20Assaeem%20Fuad%20and%20Utsab%20Saha%20and%20Hafiz%20Imtiaz&entry.1292438233=%20%20Effective%20deepfake%20detection%20tools%20are%20becoming%20increasingly%20essential%20to%20the%0Agrowing%20usage%20of%20deepfakes%20in%20unethical%20practices.%20There%20exists%20a%20wide%20range%20of%0Adeepfake%20generation%20techniques%2C%20which%20makes%20it%20challenging%20to%20develop%20an%0Aaccurate%20universal%20detection%20mechanism.%20The%202025%20IEEE%20Signal%20Processing%20Cup%0A%28%5Ctextit%7BDFWild-Cup%7D%20competition%29%20provided%20a%20diverse%20dataset%20of%20deepfake%20images%0Acontaining%20significant%20class%20imbalance.%20The%20images%20in%20the%20dataset%20are%20generated%0Afrom%20multiple%20deepfake%20image%20generators%2C%20for%20training%20machine%20learning%20model%28s%29%0Ato%20emphasize%20the%20generalization%20of%20deepfake%20detection.%20To%20this%20end%2C%20we%20proposed%0Aa%20disjoint%20set-based%20multistage%20training%20method%20to%20address%20the%20class%20imbalance%0Aand%20devised%20an%20ensemble-based%20architecture%20%5Cemph%7BCAE-Net%7D.%20Our%20architecture%0Aconsists%20of%20a%20convolution-%20and%20attention-based%20ensemble%20network%2C%20and%20employs%0Athree%20different%20neural%20network%20architectures%3A%20EfficientNet%2C%20Data-Efficient%0AImage%20Transformer%20%28DeiT%29%2C%20and%20ConvNeXt%20with%20wavelet%20transform%20to%20capture%20both%0Alocal%20and%20global%20features%20of%20deepfakes.%20We%20visualize%20the%20specific%20regions%20that%0Athese%20models%20focus%20on%20for%20classification%20using%20Grad-CAM%2C%20and%20empirically%0Ademonstrate%20the%20effectiveness%20of%20these%20models%20in%20grouping%20real%20and%20fake%20images%0Ainto%20cohesive%20clusters%20using%20t-SNE%20plots.%20Individually%2C%20the%20EfficientNet%20B0%0Aarchitecture%20has%20achieved%2090.79%5C%25%20accuracy%2C%20whereas%20the%20ConvNeXt%20and%20the%20DeiT%0Aarchitecture%20have%20achieved%2089.49%5C%25%20and%2089.32%5C%25%20accuracy%2C%20respectively.%20With%0Athese%20networks%2C%20our%20weighted%20ensemble%20model%20achieves%20an%20excellent%20accuracy%20of%0A94.63%5C%25%20on%20the%20validation%20dataset%20of%20the%20SP%20Cup%202025%20competition.%20The%20equal%0Aerror%20rate%20of%204.72%5C%25%20and%20the%20Area%20Under%20the%20ROC%20curve%20of%2097.37%5C%25%20further%0Aconfirm%20the%20stability%20of%20our%20proposed%20method.%20Finally%2C%20the%20robustness%20of%20our%0Aproposed%20model%20against%20adversarial%20perturbation%20attacks%20is%20tested%20as%20well%2C%0Ashowing%20the%20inherent%20defensive%20properties%20of%20the%20ensemble%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10682v2&entry.124074799=Read"},
{"title": "Distributed gradient methods under heavy-tailed communication noise", "author": "Manojlo Vukovic and Dusan Jakovetic and Dragana Bajovic and Soummya Kar", "abstract": "  We consider a standard distributed optimization problem in which networked\nnodes collaboratively minimize the sum of their locally known convex costs. For\nthis setting, we address for the first time the fundamental problem of design\nand analysis of distributed methods to solve the above problem when inter-node\ncommunication is subject to \\emph{heavy-tailed} noise. Heavy-tailed noise is\nhighly relevant and frequently arises in densely deployed wireless sensor and\nInternet of Things (IoT) networks. Specifically, we design a distributed\ngradient-type method that features a carefully balanced mixed time-scale\ntime-varying consensus and gradient contribution step sizes and a bounded\nnonlinear operator on the consensus update to limit the effect of heavy-tailed\nnoise. Assuming heterogeneous strongly convex local costs with mutually\ndifferent minimizers that are arbitrarily far apart, we show that the proposed\nmethod converges to a neighborhood of the network-wide problem solution in the\nmean squared error (MSE) sense, and we also characterize the corresponding\nconvergence rate. We further show that the asymptotic MSE can be made\narbitrarily small through consensus step-size tuning, possibly at the cost of\nslowing down the transient error decay. Numerical experiments corroborate our\nfindings and demonstrate the resilience of the proposed method to heavy-tailed\n(and infinite variance) communication noise. They also show that existing\ndistributed methods, designed for finite-communication-noise-variance settings,\nfail in the presence of infinite variance noise.\n", "link": "http://arxiv.org/abs/2505.24464v1", "date": "2025-05-30", "relevancy": 1.8122, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4681}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4479}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4401}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distributed%20gradient%20methods%20under%20heavy-tailed%20communication%20noise&body=Title%3A%20Distributed%20gradient%20methods%20under%20heavy-tailed%20communication%20noise%0AAuthor%3A%20Manojlo%20Vukovic%20and%20Dusan%20Jakovetic%20and%20Dragana%20Bajovic%20and%20Soummya%20Kar%0AAbstract%3A%20%20%20We%20consider%20a%20standard%20distributed%20optimization%20problem%20in%20which%20networked%0Anodes%20collaboratively%20minimize%20the%20sum%20of%20their%20locally%20known%20convex%20costs.%20For%0Athis%20setting%2C%20we%20address%20for%20the%20first%20time%20the%20fundamental%20problem%20of%20design%0Aand%20analysis%20of%20distributed%20methods%20to%20solve%20the%20above%20problem%20when%20inter-node%0Acommunication%20is%20subject%20to%20%5Cemph%7Bheavy-tailed%7D%20noise.%20Heavy-tailed%20noise%20is%0Ahighly%20relevant%20and%20frequently%20arises%20in%20densely%20deployed%20wireless%20sensor%20and%0AInternet%20of%20Things%20%28IoT%29%20networks.%20Specifically%2C%20we%20design%20a%20distributed%0Agradient-type%20method%20that%20features%20a%20carefully%20balanced%20mixed%20time-scale%0Atime-varying%20consensus%20and%20gradient%20contribution%20step%20sizes%20and%20a%20bounded%0Anonlinear%20operator%20on%20the%20consensus%20update%20to%20limit%20the%20effect%20of%20heavy-tailed%0Anoise.%20Assuming%20heterogeneous%20strongly%20convex%20local%20costs%20with%20mutually%0Adifferent%20minimizers%20that%20are%20arbitrarily%20far%20apart%2C%20we%20show%20that%20the%20proposed%0Amethod%20converges%20to%20a%20neighborhood%20of%20the%20network-wide%20problem%20solution%20in%20the%0Amean%20squared%20error%20%28MSE%29%20sense%2C%20and%20we%20also%20characterize%20the%20corresponding%0Aconvergence%20rate.%20We%20further%20show%20that%20the%20asymptotic%20MSE%20can%20be%20made%0Aarbitrarily%20small%20through%20consensus%20step-size%20tuning%2C%20possibly%20at%20the%20cost%20of%0Aslowing%20down%20the%20transient%20error%20decay.%20Numerical%20experiments%20corroborate%20our%0Afindings%20and%20demonstrate%20the%20resilience%20of%20the%20proposed%20method%20to%20heavy-tailed%0A%28and%20infinite%20variance%29%20communication%20noise.%20They%20also%20show%20that%20existing%0Adistributed%20methods%2C%20designed%20for%20finite-communication-noise-variance%20settings%2C%0Afail%20in%20the%20presence%20of%20infinite%20variance%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24464v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistributed%2520gradient%2520methods%2520under%2520heavy-tailed%2520communication%2520noise%26entry.906535625%3DManojlo%2520Vukovic%2520and%2520Dusan%2520Jakovetic%2520and%2520Dragana%2520Bajovic%2520and%2520Soummya%2520Kar%26entry.1292438233%3D%2520%2520We%2520consider%2520a%2520standard%2520distributed%2520optimization%2520problem%2520in%2520which%2520networked%250Anodes%2520collaboratively%2520minimize%2520the%2520sum%2520of%2520their%2520locally%2520known%2520convex%2520costs.%2520For%250Athis%2520setting%252C%2520we%2520address%2520for%2520the%2520first%2520time%2520the%2520fundamental%2520problem%2520of%2520design%250Aand%2520analysis%2520of%2520distributed%2520methods%2520to%2520solve%2520the%2520above%2520problem%2520when%2520inter-node%250Acommunication%2520is%2520subject%2520to%2520%255Cemph%257Bheavy-tailed%257D%2520noise.%2520Heavy-tailed%2520noise%2520is%250Ahighly%2520relevant%2520and%2520frequently%2520arises%2520in%2520densely%2520deployed%2520wireless%2520sensor%2520and%250AInternet%2520of%2520Things%2520%2528IoT%2529%2520networks.%2520Specifically%252C%2520we%2520design%2520a%2520distributed%250Agradient-type%2520method%2520that%2520features%2520a%2520carefully%2520balanced%2520mixed%2520time-scale%250Atime-varying%2520consensus%2520and%2520gradient%2520contribution%2520step%2520sizes%2520and%2520a%2520bounded%250Anonlinear%2520operator%2520on%2520the%2520consensus%2520update%2520to%2520limit%2520the%2520effect%2520of%2520heavy-tailed%250Anoise.%2520Assuming%2520heterogeneous%2520strongly%2520convex%2520local%2520costs%2520with%2520mutually%250Adifferent%2520minimizers%2520that%2520are%2520arbitrarily%2520far%2520apart%252C%2520we%2520show%2520that%2520the%2520proposed%250Amethod%2520converges%2520to%2520a%2520neighborhood%2520of%2520the%2520network-wide%2520problem%2520solution%2520in%2520the%250Amean%2520squared%2520error%2520%2528MSE%2529%2520sense%252C%2520and%2520we%2520also%2520characterize%2520the%2520corresponding%250Aconvergence%2520rate.%2520We%2520further%2520show%2520that%2520the%2520asymptotic%2520MSE%2520can%2520be%2520made%250Aarbitrarily%2520small%2520through%2520consensus%2520step-size%2520tuning%252C%2520possibly%2520at%2520the%2520cost%2520of%250Aslowing%2520down%2520the%2520transient%2520error%2520decay.%2520Numerical%2520experiments%2520corroborate%2520our%250Afindings%2520and%2520demonstrate%2520the%2520resilience%2520of%2520the%2520proposed%2520method%2520to%2520heavy-tailed%250A%2528and%2520infinite%2520variance%2529%2520communication%2520noise.%2520They%2520also%2520show%2520that%2520existing%250Adistributed%2520methods%252C%2520designed%2520for%2520finite-communication-noise-variance%2520settings%252C%250Afail%2520in%2520the%2520presence%2520of%2520infinite%2520variance%2520noise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24464v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributed%20gradient%20methods%20under%20heavy-tailed%20communication%20noise&entry.906535625=Manojlo%20Vukovic%20and%20Dusan%20Jakovetic%20and%20Dragana%20Bajovic%20and%20Soummya%20Kar&entry.1292438233=%20%20We%20consider%20a%20standard%20distributed%20optimization%20problem%20in%20which%20networked%0Anodes%20collaboratively%20minimize%20the%20sum%20of%20their%20locally%20known%20convex%20costs.%20For%0Athis%20setting%2C%20we%20address%20for%20the%20first%20time%20the%20fundamental%20problem%20of%20design%0Aand%20analysis%20of%20distributed%20methods%20to%20solve%20the%20above%20problem%20when%20inter-node%0Acommunication%20is%20subject%20to%20%5Cemph%7Bheavy-tailed%7D%20noise.%20Heavy-tailed%20noise%20is%0Ahighly%20relevant%20and%20frequently%20arises%20in%20densely%20deployed%20wireless%20sensor%20and%0AInternet%20of%20Things%20%28IoT%29%20networks.%20Specifically%2C%20we%20design%20a%20distributed%0Agradient-type%20method%20that%20features%20a%20carefully%20balanced%20mixed%20time-scale%0Atime-varying%20consensus%20and%20gradient%20contribution%20step%20sizes%20and%20a%20bounded%0Anonlinear%20operator%20on%20the%20consensus%20update%20to%20limit%20the%20effect%20of%20heavy-tailed%0Anoise.%20Assuming%20heterogeneous%20strongly%20convex%20local%20costs%20with%20mutually%0Adifferent%20minimizers%20that%20are%20arbitrarily%20far%20apart%2C%20we%20show%20that%20the%20proposed%0Amethod%20converges%20to%20a%20neighborhood%20of%20the%20network-wide%20problem%20solution%20in%20the%0Amean%20squared%20error%20%28MSE%29%20sense%2C%20and%20we%20also%20characterize%20the%20corresponding%0Aconvergence%20rate.%20We%20further%20show%20that%20the%20asymptotic%20MSE%20can%20be%20made%0Aarbitrarily%20small%20through%20consensus%20step-size%20tuning%2C%20possibly%20at%20the%20cost%20of%0Aslowing%20down%20the%20transient%20error%20decay.%20Numerical%20experiments%20corroborate%20our%0Afindings%20and%20demonstrate%20the%20resilience%20of%20the%20proposed%20method%20to%20heavy-tailed%0A%28and%20infinite%20variance%29%20communication%20noise.%20They%20also%20show%20that%20existing%0Adistributed%20methods%2C%20designed%20for%20finite-communication-noise-variance%20settings%2C%0Afail%20in%20the%20presence%20of%20infinite%20variance%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24464v1&entry.124074799=Read"},
{"title": "Optimizing the Interface Between Knowledge Graphs and LLMs for Complex\n  Reasoning", "author": "Vasilije Markovic and Lazar Obradovic and Laszlo Hajdu and Jovan Pavlovic", "abstract": "  Integrating Large Language Models (LLMs) with Knowledge Graphs (KGs) results\nin complex systems with numerous hyperparameters that directly affect\nperformance. While such systems are increasingly common in retrieval-augmented\ngeneration, the role of systematic hyperparameter optimization remains\nunderexplored. In this paper, we study this problem in the context of Cognee, a\nmodular framework for end-to-end KG construction and retrieval. Using three\nmulti-hop QA benchmarks (HotPotQA, TwoWikiMultiHop, and MuSiQue) we optimize\nparameters related to chunking, graph construction, retrieval, and prompting.\nEach configuration is scored using established metrics (exact match, F1, and\nDeepEval's LLM-based correctness metric). Our results demonstrate that\nmeaningful gains can be achieved through targeted tuning. While the gains are\nconsistent, they are not uniform, with performance varying across datasets and\nmetrics. This variability highlights both the value of tuning and the\nlimitations of standard evaluation measures. While demonstrating the immediate\npotential of hyperparameter tuning, we argue that future progress will depend\nnot only on architectural advances but also on clearer frameworks for\noptimization and evaluation in complex, modular systems.\n", "link": "http://arxiv.org/abs/2505.24478v1", "date": "2025-05-30", "relevancy": 2.0948, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5293}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5293}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4957}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20the%20Interface%20Between%20Knowledge%20Graphs%20and%20LLMs%20for%20Complex%0A%20%20Reasoning&body=Title%3A%20Optimizing%20the%20Interface%20Between%20Knowledge%20Graphs%20and%20LLMs%20for%20Complex%0A%20%20Reasoning%0AAuthor%3A%20Vasilije%20Markovic%20and%20Lazar%20Obradovic%20and%20Laszlo%20Hajdu%20and%20Jovan%20Pavlovic%0AAbstract%3A%20%20%20Integrating%20Large%20Language%20Models%20%28LLMs%29%20with%20Knowledge%20Graphs%20%28KGs%29%20results%0Ain%20complex%20systems%20with%20numerous%20hyperparameters%20that%20directly%20affect%0Aperformance.%20While%20such%20systems%20are%20increasingly%20common%20in%20retrieval-augmented%0Ageneration%2C%20the%20role%20of%20systematic%20hyperparameter%20optimization%20remains%0Aunderexplored.%20In%20this%20paper%2C%20we%20study%20this%20problem%20in%20the%20context%20of%20Cognee%2C%20a%0Amodular%20framework%20for%20end-to-end%20KG%20construction%20and%20retrieval.%20Using%20three%0Amulti-hop%20QA%20benchmarks%20%28HotPotQA%2C%20TwoWikiMultiHop%2C%20and%20MuSiQue%29%20we%20optimize%0Aparameters%20related%20to%20chunking%2C%20graph%20construction%2C%20retrieval%2C%20and%20prompting.%0AEach%20configuration%20is%20scored%20using%20established%20metrics%20%28exact%20match%2C%20F1%2C%20and%0ADeepEval%27s%20LLM-based%20correctness%20metric%29.%20Our%20results%20demonstrate%20that%0Ameaningful%20gains%20can%20be%20achieved%20through%20targeted%20tuning.%20While%20the%20gains%20are%0Aconsistent%2C%20they%20are%20not%20uniform%2C%20with%20performance%20varying%20across%20datasets%20and%0Ametrics.%20This%20variability%20highlights%20both%20the%20value%20of%20tuning%20and%20the%0Alimitations%20of%20standard%20evaluation%20measures.%20While%20demonstrating%20the%20immediate%0Apotential%20of%20hyperparameter%20tuning%2C%20we%20argue%20that%20future%20progress%20will%20depend%0Anot%20only%20on%20architectural%20advances%20but%20also%20on%20clearer%20frameworks%20for%0Aoptimization%20and%20evaluation%20in%20complex%2C%20modular%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24478v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520the%2520Interface%2520Between%2520Knowledge%2520Graphs%2520and%2520LLMs%2520for%2520Complex%250A%2520%2520Reasoning%26entry.906535625%3DVasilije%2520Markovic%2520and%2520Lazar%2520Obradovic%2520and%2520Laszlo%2520Hajdu%2520and%2520Jovan%2520Pavlovic%26entry.1292438233%3D%2520%2520Integrating%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520Knowledge%2520Graphs%2520%2528KGs%2529%2520results%250Ain%2520complex%2520systems%2520with%2520numerous%2520hyperparameters%2520that%2520directly%2520affect%250Aperformance.%2520While%2520such%2520systems%2520are%2520increasingly%2520common%2520in%2520retrieval-augmented%250Ageneration%252C%2520the%2520role%2520of%2520systematic%2520hyperparameter%2520optimization%2520remains%250Aunderexplored.%2520In%2520this%2520paper%252C%2520we%2520study%2520this%2520problem%2520in%2520the%2520context%2520of%2520Cognee%252C%2520a%250Amodular%2520framework%2520for%2520end-to-end%2520KG%2520construction%2520and%2520retrieval.%2520Using%2520three%250Amulti-hop%2520QA%2520benchmarks%2520%2528HotPotQA%252C%2520TwoWikiMultiHop%252C%2520and%2520MuSiQue%2529%2520we%2520optimize%250Aparameters%2520related%2520to%2520chunking%252C%2520graph%2520construction%252C%2520retrieval%252C%2520and%2520prompting.%250AEach%2520configuration%2520is%2520scored%2520using%2520established%2520metrics%2520%2528exact%2520match%252C%2520F1%252C%2520and%250ADeepEval%2527s%2520LLM-based%2520correctness%2520metric%2529.%2520Our%2520results%2520demonstrate%2520that%250Ameaningful%2520gains%2520can%2520be%2520achieved%2520through%2520targeted%2520tuning.%2520While%2520the%2520gains%2520are%250Aconsistent%252C%2520they%2520are%2520not%2520uniform%252C%2520with%2520performance%2520varying%2520across%2520datasets%2520and%250Ametrics.%2520This%2520variability%2520highlights%2520both%2520the%2520value%2520of%2520tuning%2520and%2520the%250Alimitations%2520of%2520standard%2520evaluation%2520measures.%2520While%2520demonstrating%2520the%2520immediate%250Apotential%2520of%2520hyperparameter%2520tuning%252C%2520we%2520argue%2520that%2520future%2520progress%2520will%2520depend%250Anot%2520only%2520on%2520architectural%2520advances%2520but%2520also%2520on%2520clearer%2520frameworks%2520for%250Aoptimization%2520and%2520evaluation%2520in%2520complex%252C%2520modular%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24478v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20the%20Interface%20Between%20Knowledge%20Graphs%20and%20LLMs%20for%20Complex%0A%20%20Reasoning&entry.906535625=Vasilije%20Markovic%20and%20Lazar%20Obradovic%20and%20Laszlo%20Hajdu%20and%20Jovan%20Pavlovic&entry.1292438233=%20%20Integrating%20Large%20Language%20Models%20%28LLMs%29%20with%20Knowledge%20Graphs%20%28KGs%29%20results%0Ain%20complex%20systems%20with%20numerous%20hyperparameters%20that%20directly%20affect%0Aperformance.%20While%20such%20systems%20are%20increasingly%20common%20in%20retrieval-augmented%0Ageneration%2C%20the%20role%20of%20systematic%20hyperparameter%20optimization%20remains%0Aunderexplored.%20In%20this%20paper%2C%20we%20study%20this%20problem%20in%20the%20context%20of%20Cognee%2C%20a%0Amodular%20framework%20for%20end-to-end%20KG%20construction%20and%20retrieval.%20Using%20three%0Amulti-hop%20QA%20benchmarks%20%28HotPotQA%2C%20TwoWikiMultiHop%2C%20and%20MuSiQue%29%20we%20optimize%0Aparameters%20related%20to%20chunking%2C%20graph%20construction%2C%20retrieval%2C%20and%20prompting.%0AEach%20configuration%20is%20scored%20using%20established%20metrics%20%28exact%20match%2C%20F1%2C%20and%0ADeepEval%27s%20LLM-based%20correctness%20metric%29.%20Our%20results%20demonstrate%20that%0Ameaningful%20gains%20can%20be%20achieved%20through%20targeted%20tuning.%20While%20the%20gains%20are%0Aconsistent%2C%20they%20are%20not%20uniform%2C%20with%20performance%20varying%20across%20datasets%20and%0Ametrics.%20This%20variability%20highlights%20both%20the%20value%20of%20tuning%20and%20the%0Alimitations%20of%20standard%20evaluation%20measures.%20While%20demonstrating%20the%20immediate%0Apotential%20of%20hyperparameter%20tuning%2C%20we%20argue%20that%20future%20progress%20will%20depend%0Anot%20only%20on%20architectural%20advances%20but%20also%20on%20clearer%20frameworks%20for%0Aoptimization%20and%20evaluation%20in%20complex%2C%20modular%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24478v1&entry.124074799=Read"},
{"title": "Why is plausibility surprisingly problematic as an XAI criterion?", "author": "Weina Jin and Xiaoxiao Li and Ghassan Hamarneh", "abstract": "  Explainable artificial intelligence (XAI) is motivated by the problem of\nmaking AI predictions understandable, transparent, and responsible, as AI\nbecomes increasingly impactful in society and high-stakes domains. The\nevaluation and optimization criteria of XAI are gatekeepers for XAI algorithms\nto achieve their expected goals and should withstand rigorous inspection. To\nimprove the scientific rigor of XAI, we conduct a critical examination of a\ncommon XAI criterion: plausibility. Plausibility assesses how convincing the AI\nexplanation is to humans, and is usually quantified by metrics of feature\nlocalization or feature correlation. Our examination shows that plausibility is\ninvalid to measure explainability, and human explanations are not the ground\ntruth for XAI, because doing so ignores the necessary assumptions underpinning\nan explanation. Our examination further reveals the consequences of using\nplausibility as an XAI criterion, including increasing misleading explanations\nthat manipulate users, deteriorating users' trust in the AI system, undermining\nhuman autonomy, being unable to achieve complementary human-AI task\nperformance, and abandoning other possible approaches of enhancing\nunderstandability. Due to the invalidity of measurements and the unethical\nissues, this position paper argues that the community should stop using\nplausibility as a criterion for the evaluation and optimization of XAI\nalgorithms. We also delineate new research approaches to improve XAI in\ntrustworthiness, understandability, and utility to users, including\ncomplementary human-AI task performance.\n", "link": "http://arxiv.org/abs/2303.17707v4", "date": "2025-05-30", "relevancy": 1.675, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4203}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4185}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Why%20is%20plausibility%20surprisingly%20problematic%20as%20an%20XAI%20criterion%3F&body=Title%3A%20Why%20is%20plausibility%20surprisingly%20problematic%20as%20an%20XAI%20criterion%3F%0AAuthor%3A%20Weina%20Jin%20and%20Xiaoxiao%20Li%20and%20Ghassan%20Hamarneh%0AAbstract%3A%20%20%20Explainable%20artificial%20intelligence%20%28XAI%29%20is%20motivated%20by%20the%20problem%20of%0Amaking%20AI%20predictions%20understandable%2C%20transparent%2C%20and%20responsible%2C%20as%20AI%0Abecomes%20increasingly%20impactful%20in%20society%20and%20high-stakes%20domains.%20The%0Aevaluation%20and%20optimization%20criteria%20of%20XAI%20are%20gatekeepers%20for%20XAI%20algorithms%0Ato%20achieve%20their%20expected%20goals%20and%20should%20withstand%20rigorous%20inspection.%20To%0Aimprove%20the%20scientific%20rigor%20of%20XAI%2C%20we%20conduct%20a%20critical%20examination%20of%20a%0Acommon%20XAI%20criterion%3A%20plausibility.%20Plausibility%20assesses%20how%20convincing%20the%20AI%0Aexplanation%20is%20to%20humans%2C%20and%20is%20usually%20quantified%20by%20metrics%20of%20feature%0Alocalization%20or%20feature%20correlation.%20Our%20examination%20shows%20that%20plausibility%20is%0Ainvalid%20to%20measure%20explainability%2C%20and%20human%20explanations%20are%20not%20the%20ground%0Atruth%20for%20XAI%2C%20because%20doing%20so%20ignores%20the%20necessary%20assumptions%20underpinning%0Aan%20explanation.%20Our%20examination%20further%20reveals%20the%20consequences%20of%20using%0Aplausibility%20as%20an%20XAI%20criterion%2C%20including%20increasing%20misleading%20explanations%0Athat%20manipulate%20users%2C%20deteriorating%20users%27%20trust%20in%20the%20AI%20system%2C%20undermining%0Ahuman%20autonomy%2C%20being%20unable%20to%20achieve%20complementary%20human-AI%20task%0Aperformance%2C%20and%20abandoning%20other%20possible%20approaches%20of%20enhancing%0Aunderstandability.%20Due%20to%20the%20invalidity%20of%20measurements%20and%20the%20unethical%0Aissues%2C%20this%20position%20paper%20argues%20that%20the%20community%20should%20stop%20using%0Aplausibility%20as%20a%20criterion%20for%20the%20evaluation%20and%20optimization%20of%20XAI%0Aalgorithms.%20We%20also%20delineate%20new%20research%20approaches%20to%20improve%20XAI%20in%0Atrustworthiness%2C%20understandability%2C%20and%20utility%20to%20users%2C%20including%0Acomplementary%20human-AI%20task%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.17707v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhy%2520is%2520plausibility%2520surprisingly%2520problematic%2520as%2520an%2520XAI%2520criterion%253F%26entry.906535625%3DWeina%2520Jin%2520and%2520Xiaoxiao%2520Li%2520and%2520Ghassan%2520Hamarneh%26entry.1292438233%3D%2520%2520Explainable%2520artificial%2520intelligence%2520%2528XAI%2529%2520is%2520motivated%2520by%2520the%2520problem%2520of%250Amaking%2520AI%2520predictions%2520understandable%252C%2520transparent%252C%2520and%2520responsible%252C%2520as%2520AI%250Abecomes%2520increasingly%2520impactful%2520in%2520society%2520and%2520high-stakes%2520domains.%2520The%250Aevaluation%2520and%2520optimization%2520criteria%2520of%2520XAI%2520are%2520gatekeepers%2520for%2520XAI%2520algorithms%250Ato%2520achieve%2520their%2520expected%2520goals%2520and%2520should%2520withstand%2520rigorous%2520inspection.%2520To%250Aimprove%2520the%2520scientific%2520rigor%2520of%2520XAI%252C%2520we%2520conduct%2520a%2520critical%2520examination%2520of%2520a%250Acommon%2520XAI%2520criterion%253A%2520plausibility.%2520Plausibility%2520assesses%2520how%2520convincing%2520the%2520AI%250Aexplanation%2520is%2520to%2520humans%252C%2520and%2520is%2520usually%2520quantified%2520by%2520metrics%2520of%2520feature%250Alocalization%2520or%2520feature%2520correlation.%2520Our%2520examination%2520shows%2520that%2520plausibility%2520is%250Ainvalid%2520to%2520measure%2520explainability%252C%2520and%2520human%2520explanations%2520are%2520not%2520the%2520ground%250Atruth%2520for%2520XAI%252C%2520because%2520doing%2520so%2520ignores%2520the%2520necessary%2520assumptions%2520underpinning%250Aan%2520explanation.%2520Our%2520examination%2520further%2520reveals%2520the%2520consequences%2520of%2520using%250Aplausibility%2520as%2520an%2520XAI%2520criterion%252C%2520including%2520increasing%2520misleading%2520explanations%250Athat%2520manipulate%2520users%252C%2520deteriorating%2520users%2527%2520trust%2520in%2520the%2520AI%2520system%252C%2520undermining%250Ahuman%2520autonomy%252C%2520being%2520unable%2520to%2520achieve%2520complementary%2520human-AI%2520task%250Aperformance%252C%2520and%2520abandoning%2520other%2520possible%2520approaches%2520of%2520enhancing%250Aunderstandability.%2520Due%2520to%2520the%2520invalidity%2520of%2520measurements%2520and%2520the%2520unethical%250Aissues%252C%2520this%2520position%2520paper%2520argues%2520that%2520the%2520community%2520should%2520stop%2520using%250Aplausibility%2520as%2520a%2520criterion%2520for%2520the%2520evaluation%2520and%2520optimization%2520of%2520XAI%250Aalgorithms.%2520We%2520also%2520delineate%2520new%2520research%2520approaches%2520to%2520improve%2520XAI%2520in%250Atrustworthiness%252C%2520understandability%252C%2520and%2520utility%2520to%2520users%252C%2520including%250Acomplementary%2520human-AI%2520task%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.17707v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Why%20is%20plausibility%20surprisingly%20problematic%20as%20an%20XAI%20criterion%3F&entry.906535625=Weina%20Jin%20and%20Xiaoxiao%20Li%20and%20Ghassan%20Hamarneh&entry.1292438233=%20%20Explainable%20artificial%20intelligence%20%28XAI%29%20is%20motivated%20by%20the%20problem%20of%0Amaking%20AI%20predictions%20understandable%2C%20transparent%2C%20and%20responsible%2C%20as%20AI%0Abecomes%20increasingly%20impactful%20in%20society%20and%20high-stakes%20domains.%20The%0Aevaluation%20and%20optimization%20criteria%20of%20XAI%20are%20gatekeepers%20for%20XAI%20algorithms%0Ato%20achieve%20their%20expected%20goals%20and%20should%20withstand%20rigorous%20inspection.%20To%0Aimprove%20the%20scientific%20rigor%20of%20XAI%2C%20we%20conduct%20a%20critical%20examination%20of%20a%0Acommon%20XAI%20criterion%3A%20plausibility.%20Plausibility%20assesses%20how%20convincing%20the%20AI%0Aexplanation%20is%20to%20humans%2C%20and%20is%20usually%20quantified%20by%20metrics%20of%20feature%0Alocalization%20or%20feature%20correlation.%20Our%20examination%20shows%20that%20plausibility%20is%0Ainvalid%20to%20measure%20explainability%2C%20and%20human%20explanations%20are%20not%20the%20ground%0Atruth%20for%20XAI%2C%20because%20doing%20so%20ignores%20the%20necessary%20assumptions%20underpinning%0Aan%20explanation.%20Our%20examination%20further%20reveals%20the%20consequences%20of%20using%0Aplausibility%20as%20an%20XAI%20criterion%2C%20including%20increasing%20misleading%20explanations%0Athat%20manipulate%20users%2C%20deteriorating%20users%27%20trust%20in%20the%20AI%20system%2C%20undermining%0Ahuman%20autonomy%2C%20being%20unable%20to%20achieve%20complementary%20human-AI%20task%0Aperformance%2C%20and%20abandoning%20other%20possible%20approaches%20of%20enhancing%0Aunderstandability.%20Due%20to%20the%20invalidity%20of%20measurements%20and%20the%20unethical%0Aissues%2C%20this%20position%20paper%20argues%20that%20the%20community%20should%20stop%20using%0Aplausibility%20as%20a%20criterion%20for%20the%20evaluation%20and%20optimization%20of%20XAI%0Aalgorithms.%20We%20also%20delineate%20new%20research%20approaches%20to%20improve%20XAI%20in%0Atrustworthiness%2C%20understandability%2C%20and%20utility%20to%20users%2C%20including%0Acomplementary%20human-AI%20task%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.17707v4&entry.124074799=Read"},
{"title": "Statistical mechanics of extensive-width Bayesian neural networks near\n  interpolation", "author": "Jean Barbier and Francesco Camilli and Minh-Toan Nguyen and Mauro Pastore and Rudy Skerk", "abstract": "  For three decades statistical mechanics has been providing a framework to\nanalyse neural networks. However, the theoretically tractable models, e.g.,\nperceptrons, random features models and kernel machines, or multi-index models\nand committee machines with few neurons, remained simple compared to those used\nin applications. In this paper we help reducing the gap between practical\nnetworks and their theoretical understanding through a statistical physics\nanalysis of the supervised learning of a two-layer fully connected network with\ngeneric weight distribution and activation function, whose hidden layer is\nlarge but remains proportional to the inputs dimension. This makes it more\nrealistic than infinitely wide networks where no feature learning occurs, but\nalso more expressive than narrow ones or with fixed inner weights. We focus on\nthe Bayes-optimal learning in the teacher-student scenario, i.e., with a\ndataset generated by another network with the same architecture. We operate\naround interpolation, where the number of trainable parameters and of data are\ncomparable and feature learning emerges. Our analysis uncovers a rich\nphenomenology with various learning transitions as the number of data\nincreases. In particular, the more strongly the features (i.e., hidden neurons\nof the target) contribute to the observed responses, the less data is needed to\nlearn them. Moreover, when the data is scarce, the model only learns non-linear\ncombinations of the teacher weights, rather than \"specialising\" by aligning its\nweights with the teacher's. Specialisation occurs only when enough data becomes\navailable, but it can be hard to find for practical training algorithms,\npossibly due to statistical-to-computational~gaps.\n", "link": "http://arxiv.org/abs/2505.24849v1", "date": "2025-05-30", "relevancy": 1.4974, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5348}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5047}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Statistical%20mechanics%20of%20extensive-width%20Bayesian%20neural%20networks%20near%0A%20%20interpolation&body=Title%3A%20Statistical%20mechanics%20of%20extensive-width%20Bayesian%20neural%20networks%20near%0A%20%20interpolation%0AAuthor%3A%20Jean%20Barbier%20and%20Francesco%20Camilli%20and%20Minh-Toan%20Nguyen%20and%20Mauro%20Pastore%20and%20Rudy%20Skerk%0AAbstract%3A%20%20%20For%20three%20decades%20statistical%20mechanics%20has%20been%20providing%20a%20framework%20to%0Aanalyse%20neural%20networks.%20However%2C%20the%20theoretically%20tractable%20models%2C%20e.g.%2C%0Aperceptrons%2C%20random%20features%20models%20and%20kernel%20machines%2C%20or%20multi-index%20models%0Aand%20committee%20machines%20with%20few%20neurons%2C%20remained%20simple%20compared%20to%20those%20used%0Ain%20applications.%20In%20this%20paper%20we%20help%20reducing%20the%20gap%20between%20practical%0Anetworks%20and%20their%20theoretical%20understanding%20through%20a%20statistical%20physics%0Aanalysis%20of%20the%20supervised%20learning%20of%20a%20two-layer%20fully%20connected%20network%20with%0Ageneric%20weight%20distribution%20and%20activation%20function%2C%20whose%20hidden%20layer%20is%0Alarge%20but%20remains%20proportional%20to%20the%20inputs%20dimension.%20This%20makes%20it%20more%0Arealistic%20than%20infinitely%20wide%20networks%20where%20no%20feature%20learning%20occurs%2C%20but%0Aalso%20more%20expressive%20than%20narrow%20ones%20or%20with%20fixed%20inner%20weights.%20We%20focus%20on%0Athe%20Bayes-optimal%20learning%20in%20the%20teacher-student%20scenario%2C%20i.e.%2C%20with%20a%0Adataset%20generated%20by%20another%20network%20with%20the%20same%20architecture.%20We%20operate%0Aaround%20interpolation%2C%20where%20the%20number%20of%20trainable%20parameters%20and%20of%20data%20are%0Acomparable%20and%20feature%20learning%20emerges.%20Our%20analysis%20uncovers%20a%20rich%0Aphenomenology%20with%20various%20learning%20transitions%20as%20the%20number%20of%20data%0Aincreases.%20In%20particular%2C%20the%20more%20strongly%20the%20features%20%28i.e.%2C%20hidden%20neurons%0Aof%20the%20target%29%20contribute%20to%20the%20observed%20responses%2C%20the%20less%20data%20is%20needed%20to%0Alearn%20them.%20Moreover%2C%20when%20the%20data%20is%20scarce%2C%20the%20model%20only%20learns%20non-linear%0Acombinations%20of%20the%20teacher%20weights%2C%20rather%20than%20%22specialising%22%20by%20aligning%20its%0Aweights%20with%20the%20teacher%27s.%20Specialisation%20occurs%20only%20when%20enough%20data%20becomes%0Aavailable%2C%20but%20it%20can%20be%20hard%20to%20find%20for%20practical%20training%20algorithms%2C%0Apossibly%20due%20to%20statistical-to-computational~gaps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24849v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStatistical%2520mechanics%2520of%2520extensive-width%2520Bayesian%2520neural%2520networks%2520near%250A%2520%2520interpolation%26entry.906535625%3DJean%2520Barbier%2520and%2520Francesco%2520Camilli%2520and%2520Minh-Toan%2520Nguyen%2520and%2520Mauro%2520Pastore%2520and%2520Rudy%2520Skerk%26entry.1292438233%3D%2520%2520For%2520three%2520decades%2520statistical%2520mechanics%2520has%2520been%2520providing%2520a%2520framework%2520to%250Aanalyse%2520neural%2520networks.%2520However%252C%2520the%2520theoretically%2520tractable%2520models%252C%2520e.g.%252C%250Aperceptrons%252C%2520random%2520features%2520models%2520and%2520kernel%2520machines%252C%2520or%2520multi-index%2520models%250Aand%2520committee%2520machines%2520with%2520few%2520neurons%252C%2520remained%2520simple%2520compared%2520to%2520those%2520used%250Ain%2520applications.%2520In%2520this%2520paper%2520we%2520help%2520reducing%2520the%2520gap%2520between%2520practical%250Anetworks%2520and%2520their%2520theoretical%2520understanding%2520through%2520a%2520statistical%2520physics%250Aanalysis%2520of%2520the%2520supervised%2520learning%2520of%2520a%2520two-layer%2520fully%2520connected%2520network%2520with%250Ageneric%2520weight%2520distribution%2520and%2520activation%2520function%252C%2520whose%2520hidden%2520layer%2520is%250Alarge%2520but%2520remains%2520proportional%2520to%2520the%2520inputs%2520dimension.%2520This%2520makes%2520it%2520more%250Arealistic%2520than%2520infinitely%2520wide%2520networks%2520where%2520no%2520feature%2520learning%2520occurs%252C%2520but%250Aalso%2520more%2520expressive%2520than%2520narrow%2520ones%2520or%2520with%2520fixed%2520inner%2520weights.%2520We%2520focus%2520on%250Athe%2520Bayes-optimal%2520learning%2520in%2520the%2520teacher-student%2520scenario%252C%2520i.e.%252C%2520with%2520a%250Adataset%2520generated%2520by%2520another%2520network%2520with%2520the%2520same%2520architecture.%2520We%2520operate%250Aaround%2520interpolation%252C%2520where%2520the%2520number%2520of%2520trainable%2520parameters%2520and%2520of%2520data%2520are%250Acomparable%2520and%2520feature%2520learning%2520emerges.%2520Our%2520analysis%2520uncovers%2520a%2520rich%250Aphenomenology%2520with%2520various%2520learning%2520transitions%2520as%2520the%2520number%2520of%2520data%250Aincreases.%2520In%2520particular%252C%2520the%2520more%2520strongly%2520the%2520features%2520%2528i.e.%252C%2520hidden%2520neurons%250Aof%2520the%2520target%2529%2520contribute%2520to%2520the%2520observed%2520responses%252C%2520the%2520less%2520data%2520is%2520needed%2520to%250Alearn%2520them.%2520Moreover%252C%2520when%2520the%2520data%2520is%2520scarce%252C%2520the%2520model%2520only%2520learns%2520non-linear%250Acombinations%2520of%2520the%2520teacher%2520weights%252C%2520rather%2520than%2520%2522specialising%2522%2520by%2520aligning%2520its%250Aweights%2520with%2520the%2520teacher%2527s.%2520Specialisation%2520occurs%2520only%2520when%2520enough%2520data%2520becomes%250Aavailable%252C%2520but%2520it%2520can%2520be%2520hard%2520to%2520find%2520for%2520practical%2520training%2520algorithms%252C%250Apossibly%2520due%2520to%2520statistical-to-computational~gaps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24849v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Statistical%20mechanics%20of%20extensive-width%20Bayesian%20neural%20networks%20near%0A%20%20interpolation&entry.906535625=Jean%20Barbier%20and%20Francesco%20Camilli%20and%20Minh-Toan%20Nguyen%20and%20Mauro%20Pastore%20and%20Rudy%20Skerk&entry.1292438233=%20%20For%20three%20decades%20statistical%20mechanics%20has%20been%20providing%20a%20framework%20to%0Aanalyse%20neural%20networks.%20However%2C%20the%20theoretically%20tractable%20models%2C%20e.g.%2C%0Aperceptrons%2C%20random%20features%20models%20and%20kernel%20machines%2C%20or%20multi-index%20models%0Aand%20committee%20machines%20with%20few%20neurons%2C%20remained%20simple%20compared%20to%20those%20used%0Ain%20applications.%20In%20this%20paper%20we%20help%20reducing%20the%20gap%20between%20practical%0Anetworks%20and%20their%20theoretical%20understanding%20through%20a%20statistical%20physics%0Aanalysis%20of%20the%20supervised%20learning%20of%20a%20two-layer%20fully%20connected%20network%20with%0Ageneric%20weight%20distribution%20and%20activation%20function%2C%20whose%20hidden%20layer%20is%0Alarge%20but%20remains%20proportional%20to%20the%20inputs%20dimension.%20This%20makes%20it%20more%0Arealistic%20than%20infinitely%20wide%20networks%20where%20no%20feature%20learning%20occurs%2C%20but%0Aalso%20more%20expressive%20than%20narrow%20ones%20or%20with%20fixed%20inner%20weights.%20We%20focus%20on%0Athe%20Bayes-optimal%20learning%20in%20the%20teacher-student%20scenario%2C%20i.e.%2C%20with%20a%0Adataset%20generated%20by%20another%20network%20with%20the%20same%20architecture.%20We%20operate%0Aaround%20interpolation%2C%20where%20the%20number%20of%20trainable%20parameters%20and%20of%20data%20are%0Acomparable%20and%20feature%20learning%20emerges.%20Our%20analysis%20uncovers%20a%20rich%0Aphenomenology%20with%20various%20learning%20transitions%20as%20the%20number%20of%20data%0Aincreases.%20In%20particular%2C%20the%20more%20strongly%20the%20features%20%28i.e.%2C%20hidden%20neurons%0Aof%20the%20target%29%20contribute%20to%20the%20observed%20responses%2C%20the%20less%20data%20is%20needed%20to%0Alearn%20them.%20Moreover%2C%20when%20the%20data%20is%20scarce%2C%20the%20model%20only%20learns%20non-linear%0Acombinations%20of%20the%20teacher%20weights%2C%20rather%20than%20%22specialising%22%20by%20aligning%20its%0Aweights%20with%20the%20teacher%27s.%20Specialisation%20occurs%20only%20when%20enough%20data%20becomes%0Aavailable%2C%20but%20it%20can%20be%20hard%20to%20find%20for%20practical%20training%20algorithms%2C%0Apossibly%20due%20to%20statistical-to-computational~gaps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24849v1&entry.124074799=Read"},
{"title": "A Generalization Result for Convergence in Learning-to-Optimize", "author": "Michael Sucker and Peter Ochs", "abstract": "  Learning-to-optimize leverages machine learning to accelerate optimization\nalgorithms. While empirical results show tremendous improvements compared to\nclassical optimization algorithms, theoretical guarantees are mostly lacking,\nsuch that the outcome cannot be reliably assured. Especially, convergence is\nhardly studied in learning-to-optimize, because conventional convergence\nguarantees in optimization are based on geometric arguments, which cannot be\napplied easily to learned algorithms. Thus, we develop a probabilistic\nframework that resembles classical optimization and allows for transferring\ngeometric arguments into learning-to-optimize. Based on our new proof-strategy,\nour main theorem is a generalization result for parametric classes of\npotentially non-smooth, non-convex loss functions and establishes the\nconvergence of learned optimization algorithms to critical points with high\nprobability. This effectively generalizes the results of a worst-case analysis\ninto a probabilistic framework, and frees the design of the learned algorithm\nfrom using safeguards.\n", "link": "http://arxiv.org/abs/2410.07704v2", "date": "2025-05-30", "relevancy": 1.8215, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4618}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.456}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Generalization%20Result%20for%20Convergence%20in%20Learning-to-Optimize&body=Title%3A%20A%20Generalization%20Result%20for%20Convergence%20in%20Learning-to-Optimize%0AAuthor%3A%20Michael%20Sucker%20and%20Peter%20Ochs%0AAbstract%3A%20%20%20Learning-to-optimize%20leverages%20machine%20learning%20to%20accelerate%20optimization%0Aalgorithms.%20While%20empirical%20results%20show%20tremendous%20improvements%20compared%20to%0Aclassical%20optimization%20algorithms%2C%20theoretical%20guarantees%20are%20mostly%20lacking%2C%0Asuch%20that%20the%20outcome%20cannot%20be%20reliably%20assured.%20Especially%2C%20convergence%20is%0Ahardly%20studied%20in%20learning-to-optimize%2C%20because%20conventional%20convergence%0Aguarantees%20in%20optimization%20are%20based%20on%20geometric%20arguments%2C%20which%20cannot%20be%0Aapplied%20easily%20to%20learned%20algorithms.%20Thus%2C%20we%20develop%20a%20probabilistic%0Aframework%20that%20resembles%20classical%20optimization%20and%20allows%20for%20transferring%0Ageometric%20arguments%20into%20learning-to-optimize.%20Based%20on%20our%20new%20proof-strategy%2C%0Aour%20main%20theorem%20is%20a%20generalization%20result%20for%20parametric%20classes%20of%0Apotentially%20non-smooth%2C%20non-convex%20loss%20functions%20and%20establishes%20the%0Aconvergence%20of%20learned%20optimization%20algorithms%20to%20critical%20points%20with%20high%0Aprobability.%20This%20effectively%20generalizes%20the%20results%20of%20a%20worst-case%20analysis%0Ainto%20a%20probabilistic%20framework%2C%20and%20frees%20the%20design%20of%20the%20learned%20algorithm%0Afrom%20using%20safeguards.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07704v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Generalization%2520Result%2520for%2520Convergence%2520in%2520Learning-to-Optimize%26entry.906535625%3DMichael%2520Sucker%2520and%2520Peter%2520Ochs%26entry.1292438233%3D%2520%2520Learning-to-optimize%2520leverages%2520machine%2520learning%2520to%2520accelerate%2520optimization%250Aalgorithms.%2520While%2520empirical%2520results%2520show%2520tremendous%2520improvements%2520compared%2520to%250Aclassical%2520optimization%2520algorithms%252C%2520theoretical%2520guarantees%2520are%2520mostly%2520lacking%252C%250Asuch%2520that%2520the%2520outcome%2520cannot%2520be%2520reliably%2520assured.%2520Especially%252C%2520convergence%2520is%250Ahardly%2520studied%2520in%2520learning-to-optimize%252C%2520because%2520conventional%2520convergence%250Aguarantees%2520in%2520optimization%2520are%2520based%2520on%2520geometric%2520arguments%252C%2520which%2520cannot%2520be%250Aapplied%2520easily%2520to%2520learned%2520algorithms.%2520Thus%252C%2520we%2520develop%2520a%2520probabilistic%250Aframework%2520that%2520resembles%2520classical%2520optimization%2520and%2520allows%2520for%2520transferring%250Ageometric%2520arguments%2520into%2520learning-to-optimize.%2520Based%2520on%2520our%2520new%2520proof-strategy%252C%250Aour%2520main%2520theorem%2520is%2520a%2520generalization%2520result%2520for%2520parametric%2520classes%2520of%250Apotentially%2520non-smooth%252C%2520non-convex%2520loss%2520functions%2520and%2520establishes%2520the%250Aconvergence%2520of%2520learned%2520optimization%2520algorithms%2520to%2520critical%2520points%2520with%2520high%250Aprobability.%2520This%2520effectively%2520generalizes%2520the%2520results%2520of%2520a%2520worst-case%2520analysis%250Ainto%2520a%2520probabilistic%2520framework%252C%2520and%2520frees%2520the%2520design%2520of%2520the%2520learned%2520algorithm%250Afrom%2520using%2520safeguards.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07704v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Generalization%20Result%20for%20Convergence%20in%20Learning-to-Optimize&entry.906535625=Michael%20Sucker%20and%20Peter%20Ochs&entry.1292438233=%20%20Learning-to-optimize%20leverages%20machine%20learning%20to%20accelerate%20optimization%0Aalgorithms.%20While%20empirical%20results%20show%20tremendous%20improvements%20compared%20to%0Aclassical%20optimization%20algorithms%2C%20theoretical%20guarantees%20are%20mostly%20lacking%2C%0Asuch%20that%20the%20outcome%20cannot%20be%20reliably%20assured.%20Especially%2C%20convergence%20is%0Ahardly%20studied%20in%20learning-to-optimize%2C%20because%20conventional%20convergence%0Aguarantees%20in%20optimization%20are%20based%20on%20geometric%20arguments%2C%20which%20cannot%20be%0Aapplied%20easily%20to%20learned%20algorithms.%20Thus%2C%20we%20develop%20a%20probabilistic%0Aframework%20that%20resembles%20classical%20optimization%20and%20allows%20for%20transferring%0Ageometric%20arguments%20into%20learning-to-optimize.%20Based%20on%20our%20new%20proof-strategy%2C%0Aour%20main%20theorem%20is%20a%20generalization%20result%20for%20parametric%20classes%20of%0Apotentially%20non-smooth%2C%20non-convex%20loss%20functions%20and%20establishes%20the%0Aconvergence%20of%20learned%20optimization%20algorithms%20to%20critical%20points%20with%20high%0Aprobability.%20This%20effectively%20generalizes%20the%20results%20of%20a%20worst-case%20analysis%0Ainto%20a%20probabilistic%20framework%2C%20and%20frees%20the%20design%20of%20the%20learned%20algorithm%0Afrom%20using%20safeguards.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07704v2&entry.124074799=Read"},
{"title": "PDE-Transformer: Efficient and Versatile Transformers for Physics\n  Simulations", "author": "Benjamin Holzschuh and Qiang Liu and Georg Kohl and Nils Thuerey", "abstract": "  We introduce PDE-Transformer, an improved transformer-based architecture for\nsurrogate modeling of physics simulations on regular grids. We combine recent\narchitectural improvements of diffusion transformers with adjustments specific\nfor large-scale simulations to yield a more scalable and versatile\ngeneral-purpose transformer architecture, which can be used as the backbone for\nbuilding large-scale foundation models in physical sciences. We demonstrate\nthat our proposed architecture outperforms state-of-the-art transformer\narchitectures for computer vision on a large dataset of 16 different types of\nPDEs. We propose to embed different physical channels individually as\nspatio-temporal tokens, which interact via channel-wise self-attention. This\nhelps to maintain a consistent information density of tokens when learning\nmultiple types of PDEs simultaneously. We demonstrate that our pre-trained\nmodels achieve improved performance on several challenging downstream tasks\ncompared to training from scratch and also beat other foundation model\narchitectures for physics simulations.\n", "link": "http://arxiv.org/abs/2505.24717v1", "date": "2025-05-30", "relevancy": 1.7156, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6706}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5444}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PDE-Transformer%3A%20Efficient%20and%20Versatile%20Transformers%20for%20Physics%0A%20%20Simulations&body=Title%3A%20PDE-Transformer%3A%20Efficient%20and%20Versatile%20Transformers%20for%20Physics%0A%20%20Simulations%0AAuthor%3A%20Benjamin%20Holzschuh%20and%20Qiang%20Liu%20and%20Georg%20Kohl%20and%20Nils%20Thuerey%0AAbstract%3A%20%20%20We%20introduce%20PDE-Transformer%2C%20an%20improved%20transformer-based%20architecture%20for%0Asurrogate%20modeling%20of%20physics%20simulations%20on%20regular%20grids.%20We%20combine%20recent%0Aarchitectural%20improvements%20of%20diffusion%20transformers%20with%20adjustments%20specific%0Afor%20large-scale%20simulations%20to%20yield%20a%20more%20scalable%20and%20versatile%0Ageneral-purpose%20transformer%20architecture%2C%20which%20can%20be%20used%20as%20the%20backbone%20for%0Abuilding%20large-scale%20foundation%20models%20in%20physical%20sciences.%20We%20demonstrate%0Athat%20our%20proposed%20architecture%20outperforms%20state-of-the-art%20transformer%0Aarchitectures%20for%20computer%20vision%20on%20a%20large%20dataset%20of%2016%20different%20types%20of%0APDEs.%20We%20propose%20to%20embed%20different%20physical%20channels%20individually%20as%0Aspatio-temporal%20tokens%2C%20which%20interact%20via%20channel-wise%20self-attention.%20This%0Ahelps%20to%20maintain%20a%20consistent%20information%20density%20of%20tokens%20when%20learning%0Amultiple%20types%20of%20PDEs%20simultaneously.%20We%20demonstrate%20that%20our%20pre-trained%0Amodels%20achieve%20improved%20performance%20on%20several%20challenging%20downstream%20tasks%0Acompared%20to%20training%20from%20scratch%20and%20also%20beat%20other%20foundation%20model%0Aarchitectures%20for%20physics%20simulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24717v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPDE-Transformer%253A%2520Efficient%2520and%2520Versatile%2520Transformers%2520for%2520Physics%250A%2520%2520Simulations%26entry.906535625%3DBenjamin%2520Holzschuh%2520and%2520Qiang%2520Liu%2520and%2520Georg%2520Kohl%2520and%2520Nils%2520Thuerey%26entry.1292438233%3D%2520%2520We%2520introduce%2520PDE-Transformer%252C%2520an%2520improved%2520transformer-based%2520architecture%2520for%250Asurrogate%2520modeling%2520of%2520physics%2520simulations%2520on%2520regular%2520grids.%2520We%2520combine%2520recent%250Aarchitectural%2520improvements%2520of%2520diffusion%2520transformers%2520with%2520adjustments%2520specific%250Afor%2520large-scale%2520simulations%2520to%2520yield%2520a%2520more%2520scalable%2520and%2520versatile%250Ageneral-purpose%2520transformer%2520architecture%252C%2520which%2520can%2520be%2520used%2520as%2520the%2520backbone%2520for%250Abuilding%2520large-scale%2520foundation%2520models%2520in%2520physical%2520sciences.%2520We%2520demonstrate%250Athat%2520our%2520proposed%2520architecture%2520outperforms%2520state-of-the-art%2520transformer%250Aarchitectures%2520for%2520computer%2520vision%2520on%2520a%2520large%2520dataset%2520of%252016%2520different%2520types%2520of%250APDEs.%2520We%2520propose%2520to%2520embed%2520different%2520physical%2520channels%2520individually%2520as%250Aspatio-temporal%2520tokens%252C%2520which%2520interact%2520via%2520channel-wise%2520self-attention.%2520This%250Ahelps%2520to%2520maintain%2520a%2520consistent%2520information%2520density%2520of%2520tokens%2520when%2520learning%250Amultiple%2520types%2520of%2520PDEs%2520simultaneously.%2520We%2520demonstrate%2520that%2520our%2520pre-trained%250Amodels%2520achieve%2520improved%2520performance%2520on%2520several%2520challenging%2520downstream%2520tasks%250Acompared%2520to%2520training%2520from%2520scratch%2520and%2520also%2520beat%2520other%2520foundation%2520model%250Aarchitectures%2520for%2520physics%2520simulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24717v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PDE-Transformer%3A%20Efficient%20and%20Versatile%20Transformers%20for%20Physics%0A%20%20Simulations&entry.906535625=Benjamin%20Holzschuh%20and%20Qiang%20Liu%20and%20Georg%20Kohl%20and%20Nils%20Thuerey&entry.1292438233=%20%20We%20introduce%20PDE-Transformer%2C%20an%20improved%20transformer-based%20architecture%20for%0Asurrogate%20modeling%20of%20physics%20simulations%20on%20regular%20grids.%20We%20combine%20recent%0Aarchitectural%20improvements%20of%20diffusion%20transformers%20with%20adjustments%20specific%0Afor%20large-scale%20simulations%20to%20yield%20a%20more%20scalable%20and%20versatile%0Ageneral-purpose%20transformer%20architecture%2C%20which%20can%20be%20used%20as%20the%20backbone%20for%0Abuilding%20large-scale%20foundation%20models%20in%20physical%20sciences.%20We%20demonstrate%0Athat%20our%20proposed%20architecture%20outperforms%20state-of-the-art%20transformer%0Aarchitectures%20for%20computer%20vision%20on%20a%20large%20dataset%20of%2016%20different%20types%20of%0APDEs.%20We%20propose%20to%20embed%20different%20physical%20channels%20individually%20as%0Aspatio-temporal%20tokens%2C%20which%20interact%20via%20channel-wise%20self-attention.%20This%0Ahelps%20to%20maintain%20a%20consistent%20information%20density%20of%20tokens%20when%20learning%0Amultiple%20types%20of%20PDEs%20simultaneously.%20We%20demonstrate%20that%20our%20pre-trained%0Amodels%20achieve%20improved%20performance%20on%20several%20challenging%20downstream%20tasks%0Acompared%20to%20training%20from%20scratch%20and%20also%20beat%20other%20foundation%20model%0Aarchitectures%20for%20physics%20simulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24717v1&entry.124074799=Read"},
{"title": "Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking\n  Through Options", "author": "Lakshmi Nair and Ian Trase and Mark Kim", "abstract": "  We present a novel reasoning approach called Flow-of-Options (FoO), designed\nto address intrinsic biases in Large Language Models (LLMs). Flow-of-Options\nenables LLMs to systematically explore a diverse range of possibilities in\ntheir reasoning, as demonstrated by an FoO-based agentic framework developed\nfor autonomously solving Machine Learning (ML) tasks. FoO enforces diversity in\nLLM solutions through compressed and interpretable task representations,\nresulting in improvements of 38.2% - 69.2% on standard data science tasks, and\n37.4% - 47.9% on therapeutic chemistry tasks, as compared to state-of-the-art\nbaselines. With an overall operation cost under $1 per task, our framework is\nwell-suited for cost-sensitive applications. Going beyond tabular\nclassification and regression, we show the broader applicability of our\nFoO-based agentic system to tasks such as reinforcement learning and image\ngeneration. Our code is open-sourced at:\nhttps://github.com/flagshippioneering/Flow-of-Options.\n", "link": "http://arxiv.org/abs/2502.12929v2", "date": "2025-05-30", "relevancy": 1.4213, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4876}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4762}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flow-of-Options%3A%20Diversified%20and%20Improved%20LLM%20Reasoning%20by%20Thinking%0A%20%20Through%20Options&body=Title%3A%20Flow-of-Options%3A%20Diversified%20and%20Improved%20LLM%20Reasoning%20by%20Thinking%0A%20%20Through%20Options%0AAuthor%3A%20Lakshmi%20Nair%20and%20Ian%20Trase%20and%20Mark%20Kim%0AAbstract%3A%20%20%20We%20present%20a%20novel%20reasoning%20approach%20called%20Flow-of-Options%20%28FoO%29%2C%20designed%0Ato%20address%20intrinsic%20biases%20in%20Large%20Language%20Models%20%28LLMs%29.%20Flow-of-Options%0Aenables%20LLMs%20to%20systematically%20explore%20a%20diverse%20range%20of%20possibilities%20in%0Atheir%20reasoning%2C%20as%20demonstrated%20by%20an%20FoO-based%20agentic%20framework%20developed%0Afor%20autonomously%20solving%20Machine%20Learning%20%28ML%29%20tasks.%20FoO%20enforces%20diversity%20in%0ALLM%20solutions%20through%20compressed%20and%20interpretable%20task%20representations%2C%0Aresulting%20in%20improvements%20of%2038.2%25%20-%2069.2%25%20on%20standard%20data%20science%20tasks%2C%20and%0A37.4%25%20-%2047.9%25%20on%20therapeutic%20chemistry%20tasks%2C%20as%20compared%20to%20state-of-the-art%0Abaselines.%20With%20an%20overall%20operation%20cost%20under%20%241%20per%20task%2C%20our%20framework%20is%0Awell-suited%20for%20cost-sensitive%20applications.%20Going%20beyond%20tabular%0Aclassification%20and%20regression%2C%20we%20show%20the%20broader%20applicability%20of%20our%0AFoO-based%20agentic%20system%20to%20tasks%20such%20as%20reinforcement%20learning%20and%20image%0Ageneration.%20Our%20code%20is%20open-sourced%20at%3A%0Ahttps%3A//github.com/flagshippioneering/Flow-of-Options.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12929v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlow-of-Options%253A%2520Diversified%2520and%2520Improved%2520LLM%2520Reasoning%2520by%2520Thinking%250A%2520%2520Through%2520Options%26entry.906535625%3DLakshmi%2520Nair%2520and%2520Ian%2520Trase%2520and%2520Mark%2520Kim%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520reasoning%2520approach%2520called%2520Flow-of-Options%2520%2528FoO%2529%252C%2520designed%250Ato%2520address%2520intrinsic%2520biases%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520Flow-of-Options%250Aenables%2520LLMs%2520to%2520systematically%2520explore%2520a%2520diverse%2520range%2520of%2520possibilities%2520in%250Atheir%2520reasoning%252C%2520as%2520demonstrated%2520by%2520an%2520FoO-based%2520agentic%2520framework%2520developed%250Afor%2520autonomously%2520solving%2520Machine%2520Learning%2520%2528ML%2529%2520tasks.%2520FoO%2520enforces%2520diversity%2520in%250ALLM%2520solutions%2520through%2520compressed%2520and%2520interpretable%2520task%2520representations%252C%250Aresulting%2520in%2520improvements%2520of%252038.2%2525%2520-%252069.2%2525%2520on%2520standard%2520data%2520science%2520tasks%252C%2520and%250A37.4%2525%2520-%252047.9%2525%2520on%2520therapeutic%2520chemistry%2520tasks%252C%2520as%2520compared%2520to%2520state-of-the-art%250Abaselines.%2520With%2520an%2520overall%2520operation%2520cost%2520under%2520%25241%2520per%2520task%252C%2520our%2520framework%2520is%250Awell-suited%2520for%2520cost-sensitive%2520applications.%2520Going%2520beyond%2520tabular%250Aclassification%2520and%2520regression%252C%2520we%2520show%2520the%2520broader%2520applicability%2520of%2520our%250AFoO-based%2520agentic%2520system%2520to%2520tasks%2520such%2520as%2520reinforcement%2520learning%2520and%2520image%250Ageneration.%2520Our%2520code%2520is%2520open-sourced%2520at%253A%250Ahttps%253A//github.com/flagshippioneering/Flow-of-Options.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12929v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flow-of-Options%3A%20Diversified%20and%20Improved%20LLM%20Reasoning%20by%20Thinking%0A%20%20Through%20Options&entry.906535625=Lakshmi%20Nair%20and%20Ian%20Trase%20and%20Mark%20Kim&entry.1292438233=%20%20We%20present%20a%20novel%20reasoning%20approach%20called%20Flow-of-Options%20%28FoO%29%2C%20designed%0Ato%20address%20intrinsic%20biases%20in%20Large%20Language%20Models%20%28LLMs%29.%20Flow-of-Options%0Aenables%20LLMs%20to%20systematically%20explore%20a%20diverse%20range%20of%20possibilities%20in%0Atheir%20reasoning%2C%20as%20demonstrated%20by%20an%20FoO-based%20agentic%20framework%20developed%0Afor%20autonomously%20solving%20Machine%20Learning%20%28ML%29%20tasks.%20FoO%20enforces%20diversity%20in%0ALLM%20solutions%20through%20compressed%20and%20interpretable%20task%20representations%2C%0Aresulting%20in%20improvements%20of%2038.2%25%20-%2069.2%25%20on%20standard%20data%20science%20tasks%2C%20and%0A37.4%25%20-%2047.9%25%20on%20therapeutic%20chemistry%20tasks%2C%20as%20compared%20to%20state-of-the-art%0Abaselines.%20With%20an%20overall%20operation%20cost%20under%20%241%20per%20task%2C%20our%20framework%20is%0Awell-suited%20for%20cost-sensitive%20applications.%20Going%20beyond%20tabular%0Aclassification%20and%20regression%2C%20we%20show%20the%20broader%20applicability%20of%20our%0AFoO-based%20agentic%20system%20to%20tasks%20such%20as%20reinforcement%20learning%20and%20image%0Ageneration.%20Our%20code%20is%20open-sourced%20at%3A%0Ahttps%3A//github.com/flagshippioneering/Flow-of-Options.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12929v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


